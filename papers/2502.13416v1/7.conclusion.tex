\section{Conclusion}
%In conclusion, this paper presents a significant leap forward in addressing the challenge of FCHs in LLMs. By developing a systematic framework based on logic programming and integrating it into \tool, we have effectively tackled the limitations of current detection methodologies. Our approach, grounded in transforming a comprehensive factual knowledge base sourced from Wikipedia through advanced logic reasoning methods, has demonstrated superior performance in detecting factual inaccuracies across various LLMs. The automation of this process marks a notable advancement in scalability, reducing reliance on manual intervention. Furthermore, the release of our enriched dataset as a benchmark contributes to the broader research community, paving the way for future innovations in hallucination detection. This work not only enhances the reliability and usability of LLMs but also sets a new standard for research in this critical area of language processing technology. 
We target the critical challenge of FCH in LLM, where they generate outputs contradicting established facts. We developed a novel automated testing framework that combines logic programming and metamorphic testing to systematically detect FCH issues in LLMs. Our novel approach constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, then applies innovative logic reasoning rules to transform this knowledge into a large set of test cases with ground truth answers. 
These reasoning rules are either predefined relations or automatically generated from randomly sampled temporal formulae. 
LLMs are evaluated on these test cases through template prompts, with two semantic-aware oracles analyzing the similarity between the logical/semantic structures of the LLM outputs and ground truth to validate reasoning and pinpoint FCHs. 

Across diverse subjects and LLM architectures, our framework automatically generated over 9,000 useful test cases, uncovering hallucination rates as high as 59.8\% and identifying lack of logical reasoning as a key contributor to FCH issues. This work pioneers automated FCH testing capabilities, providing a comprehensive benchmark, data augmentation techniques, and answer validation methods. The implications are far-reaching --- enhancing LLM reliability and trustworthiness for high-stakes applications by exposing critical weaknesses while advancing systematic evaluation methodologies.
