


\section{Methodology}\label{sec:method}

\begin{figure}[!h]
\vspace{-2mm}
\centering
\includegraphics[width=1\linewidth]{fig/overview_syh.jpg}
\vspace{-5mm}
\caption{\tool Overview }
\label{fig:tool_overview}
\vspace{-1mm}
\end{figure}

%\shil{shall we provide an overview figure of the proposed framework?}
%\syh{I will work on a workflow figure if possible}
\tool (\figref{fig:tool_overview}) is a general-purpose testing framework that evaluates the LLM outputs for automatically generated test cases. 
The inputs for the response evaluation
contain a natural language (NL) query for LLM and its ground truth answer obtained using logic programming (\secref{subsec3.1}).  
Based on voluminous knowledge database dumps, \tool extracts factual knowledge (\secref{knowledge}), which outputs a set of 
predicates
in the form of Prolog facts. 
Then, \tool deploys a set of pre-defined or automatically generated reasoning rules to
extend the database with a set of derived facts (\secref{sec:derive_more_facts}, \secref{sec:encoding_MTL}). 
These derived facts facilitate an automated test generation (\secref{prompt}), which outputs question-answer pairs (Q\&A) and concrete prompts for LLMs. 
Given the Q\&A pairs and the LLM outputs, \tool evaluates the responses from LLMs and detects factual inconsistency automatically (\secref{response}). 
To this end, it 
first parses LLM outputs semantic-aware structure, and evaluates their semantic similarity to the ground truth. %Subsequently, 
Lastly, it develops similarity-based oracles that apply metamorphic testing to assess consistency against the ground truth. 



%Therefore, the response evaluation for automatically generated tests is achieved given the ground truth Q\&A pairs and the LLM outputs. 


%Lastly, to evaluate the responses from LLMs and detect factual inconsistency automatically (\secref{response}), it first parses LLM outputs semantic-aware structure. Then, it evaluates their semantic similarity to the ground truth. Subsequently, it develops similarity-based oracles that apply metamorphic testing to assess consistency against the ground truth. 






\subsection{Preliminary}
\label{subsec3.1}

\begin{figure}[!b]
{
\vspace{-2mm}
\centering
\small
$
\arraycolsep=3pt\def\arraystretch{1}
\begin{array}{@{}lrcl}
\m{(Program)}&  \Prolog &{::=  } &
\widetilde{\relation} \,\plus\plus\,   \widetilde{\drule} 
\\
\m{(Rule)} &  \m{\drule} &{ ::=  } & 
\relation ~\hornarrow~ \widetilde{body}
\\[0.3em]
\m{(Body)} & \m{body} &{  ::=  } & 
{\tt{Pos}}~ \relation
\,\mid\, {\tt{Neg}}~ \relation 
\,\mid\, \pi
  \\
\m{(Predicate)} &  \relation &{  ::=  } &
 \m{\nm}\,(\widetilde{\entity}) 
 %\text{\syh{how to link entity and term?}}
\\[0.3em]
 \m{(Pure)}  &\pi &{::=}~&
{ T }
  \mid  F
 \mid  {\m{bop}(}{t_1, t_2}{)}
 %\mid \nm(\widetilde{t})
 \mid   {{\pi_1}}  {\wedge}  \pi_2
 \mid  {{\pi_1}} {\vee} \pi_2
 \mid  \neg\pi
\\[0.3em]
 \m{(Term)}  &t &{::=}~& c 
 \mid X 
 \mid t_1{\text{\ttfamily +}}t_2
 \mid t_1\text{-}t_2
\end{array}$
\caption{A Core Syntax of Prolog}
\label{fig:Syntax_of_Prolog}
}
\end{figure}

Logic programming allows the programmer to specify the rules and facts, enabling the Prolog interpreter to infer answers to the given queries automatically. 
We define a core syntax of Prolog in \figref{fig:Syntax_of_Prolog}. 
A Prolog program consists of two parts: a set of facts ($\widetilde{\relation}$) and a set of rules ($\widetilde{\drule}$). 
Throughout the paper, we use the over-tilde notation to denote a set of items. 
For example, $\widetilde{X}$ refers to a set of variables, i.e., $\{X_1, \dots, X_n\}$. 
A fact is represented as a relational predicate with a name and a set of entity arguments, where $\nm$ is an arbitrary distinct identifier drawn from a finite set of relations. 
Entities are drawn from the knowledge database, ranging from string types (for names or events) and integers (for time points).  
A Prolog rule is a Horn clause that comprises a head predicate and a set of body predicates placed on the left and right side of the arrow symbol ($\hornarrow$).

A rule means that the left-hand side is logically implied by the right-hand side. 
The rule bodies are either positive or negative relations, corresponding to the requirements upon the presence or absence of facts. 
We use ``$\relation$'' and ``$\shortNeg\,\relation$'' as abbreviations for
``${\tt{Pos}}~\relation$'' and ``${\tt{Neg}}~\relation$'', respectively. 
Rule bodies contain pure formulae and simplified and decidable sets of Presburger arithmetic predicates over local variables. 
The Boolean values of \emph{true} and \emph{false} are respectively indicated by $T$ and $F$. 
%Other logical relations are represented using general abstract predicates over the terms, which contain the 
The binary operators $bop$ are from $\{ {<}, {\leq}, {=}, {\geq}, {>} \}$. 
Terms consist of constants (denoted by $c$), program variables (denoted by $X$ %\shil{since we use lowercase $c$ to represent constant, can we use lowercase $x$ to represent variables?}
%\syh{Upper case for variable is the Prolog convention.}
), or simple computations of terms, such as $t_1{\plus} t_2$ and $t_1\text{-}t_2$. 
%A Prolog query is executed against a database of facts. 





\subsection{Factual Knowledge Extraction}
\label{knowledge} 
%While predicates can have an arbitrary number of arguments in general, 
To facilitate an automated reasoning system, we extract the \emph{ground facts} in the structure of three-element predicates, i.e., $\m{\nm}\,(\Subj,\Obj)$, where ``$\Subj$'' (stands for $\m{subject}$) and ``$\Obj$'' (stands for $\m{object}$) are entities, and ``$\nm$'' is the name of the predicate. 
Here, we follow the convention of Prolog, where variable names must start with an uppercase letter, and any name that begins with a lowercase letter is a constant. %\shil{whether this format applies to the examples in Fig 6?}
%\syh{Fig. 6 is revised to lowercase now} 

Existing knowledge databases~\cite{freebase, DBpedia, Yago, WordNet} not only encompass a vast array of documents but also provide structured data, facilitating an ideal source for constructing a rich factual knowledge base. 
Thus, the genesis of our test cases is exclusively rooted in the entities and structured relations sourced from existing knowledge databases, ensuring a sophisticated and well-informed foundation for our testing framework. 
Specifically, we follow the categorization for entities (\figref{table:categories}) and relations (\figref{table:relations}) used by WikiPedia~\cite{DBpedia} to perform a thorough facts extraction. 
In particular, the {\small\textbf{Prop.}} (stands for properties) entry for relations guides the automated generation of reasoning rules detailed in \secref{sec:derive_more_facts}.

%as shown in \figref{table:entity_relations}, 


\begin{figure}[!b]
\vspace{-3mm}
\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{2pt}
\footnotesize 
%\resizebox{\linewidth}{!}{
\begin{tabular}{l | l }
\Xhline{1.0\arrayrulewidth}
\textbf{Entity Cat.} & \textbf{Description}\\
        \Xhline{\arrayrulewidth}
        {Culture and the Arts} & Famous films, books, etc.\\ 
        % & 10,537\\
        %\hline
        {Geography and Places} & Countries, cities and locations. \\
        % & 8,806\\
        %\hline
        {Health and Fitness} & Diseases and genes. \\
        % & 179\\
        %\hline
        {History and Events} & Famous historical events, etc. \\
        % & 5,561\\
       % \hline
        {People and Self} & Important figures. \\
        % & 21,720\\
        %\hline
        {Mathematics and Logic} & Formulas and theorems. \\
        % & 141\\
       % \hline
        {Natural and Physical Sciences} & Celestial bodies and astronomy. \\
        % & 904\\
       % \hline
        {Society and Social Sciences} & Major social institutions, etc.\\ 
        % & 3,862\\
       % \hline
        {Technology and Applied Sciences} & Computer science, etc. \\
        % & 2,773\\
        \Xhline{1.5\arrayrulewidth} %添加表格底部粗线
    \end{tabular}
    %}
\caption{{Entity Categorization.}}
\label{table:categories}
\end{figure}


\begin{figure}[!b]
%\vspace{1.5mm}
\centering
\def\arraystretch{1.1}
\setlength{\tabcolsep}{2pt}
\footnotesize
%\resizebox{\linewidth}{!}{
\begin{tabular}{l | l | l}
\Xhline{1.5\arrayrulewidth}
\textbf{Relation Cat.} & \textbf{Examples}
& 
\textbf{Prop.} 
%(\figref{fig:basic_op_for_predicates})
\\
\Xhline{1.5\arrayrulewidth}
        {Noun Phrase} & 
\begin{tabular}[l]{@{}l@{}} \textit{place\_of\_birth\,(barack\_obama, honolulu).}\\ \textit{genre\,(28\_days\_later, horror\_film).} \end{tabular}
        &
\begin{tabular}[l]{@{}l@{}} 
        $\RNeg$ \\
        $\RSym$ \\
        $\RTrans$
        \end{tabular}
        \\
        \hline
        \begin{tabular}[l]{@{}l@{}} Verb Phrase \\  
        (Passive Voice) \end{tabular}
        & \begin{tabular}[l]{@{}l@{}} \textit{killed\_by}\,\textit{(alexander\_pushkin}, \\ \quad  \textit{georges-charles\_de\_heeckeren\_d'anthès)}.\\ \textit{located\_in\_time\_zone\,(arizona, utc-07:00).}\\ 
        % \textit{(Bayes' theorem, named after, Thomas Bayes)}
        \end{tabular}
        &
\begin{tabular}[l]{@{}l@{}} 
        $\RNeg$ \\
        $\RInv$
        \end{tabular}
        \\
        \hline
        \begin{tabular}[l]{@{}l@{}} Verb Phrase \\  
        (Active Voice) \end{tabular}
        & \begin{tabular}[l]{@{}l@{}} \textit{follows\,(4769\_Castalia, 4768\_hartley).}\\ \textit{replaces\,(american\_broadcasting\_company,} \\ \qquad \quad\ \   \textit{nbc\_blue\_network).} \end{tabular}
        &
\begin{tabular}[l]{@{}l@{}}  
        $\RNeg$ \\
        $\RInv$
        \end{tabular}
        \\
        \Xhline{1.5\arrayrulewidth} %添加表格底部粗线
    \end{tabular}
    %}
\caption{Relation Categorization.}
\label{table:relations}
\end{figure}









The facts extraction is done per-category basis, implementing a divide-and-conquer strategy, which efficiently integrates all the facts from all the categories. 
As shown in \algoref{alg:ground_truth}, for any given entity category and relation category, the function $\textsc{ExtractGroundFacts}$ iterates through all possible entities and relations. 
For each combination ($\m{entity}, \nm$), it queries the database using the $\textsc{QueryDB}$ function, which retrieves all three-element facts established with the specific predicate $\nm$ and the argument $\m{entity}$ placed either in the subject or the object position. 

{
\begin{algorithm}[!h]
\caption{Facts Extraction}
\label{alg:ground_truth}
\small
\begin{algorithmic}[1]
\Require  
Entity Category (\entityCat), Relation Category (\relationCat)
\Ensure Ground Facts ($\groundTruthTriples$)
\Function{ExtractGroundFacts}{
\entityCat, \relationCat}
\State$\groundTruthTriples \gets []$ \Comment{\commentstyle{Initialization}}
\For{$\m{entity}$ $\in$ \entityCat~} \Comment{\commentstyle{Iterate over each entity}}
\For{~$\nm$ $\in$  \relationCat~} \Comment{\commentstyle{Iterate over each relation}}
\State $\widetilde{\relation} \gets$ \Call{QueryDB}{$\m{entity}$, $\nm$} 
\Comment{\commentstyle{Retrieve ground facts}}
\State $\groundTruthTriples.\m{append}(\widetilde{\relation})$ \Comment{\commentstyle{Extend the ground facts}}
\EndFor
\EndFor
\State \Return $\groundTruthTriples$ \Comment{\commentstyle{Return the ground facts}}
\EndFunction
\end{algorithmic}
\end{algorithm}
}



\begin{figure}[!b]
%\vspace{-2mm}
\centering
\small
\begin{gather*}
\frac{
\begin{matrix}
\RNeg\\
{\drule}{=}\,\nm_{\m{new}}\,(\SUBJ, \OBJ){\hornarrow} !\nm\,(\SUBJ, \OBJ)
\end{matrix}
}{
\deriveRules{\nm}{\nm_{\m{new}}}{{\drule}}}
\ \  
\frac{
\begin{matrix}
\RInv\\
{\drule}{=}\,\nm_{\m{new}}\,(\SUBJ, \OBJ) {\hornarrow} \nm\,(\OBJ, \SUBJ)
\end{matrix}
}{
\deriveRules{\nm}{\nm_{\m{new}}}{{\drule}}}
\\[0.4em]
\frac{
\begin{matrix}
\RSym\\
{\drule}{=}\,\nm\,(\SUBJ, \OBJ) {\hornarrow} \nm\,(\OBJ, \SUBJ)
\end{matrix}
}{
\deriveRules{\nm}{\nm}{{\drule}}}
\quad\   
\frac{
\begin{matrix}
{\drule}{=}\,\nm\,(\SUBJ, \OBJ') {\hornarrow} 
\\ 
\nm\,(\SUBJ, \OBJ), 
\nm\,(\OBJ, \OBJ')
\end{matrix}
}{
\deriveRules{\nm}{\nm}{{\drule}}}  \RTrans
\end{gather*}
%\vspace{-1mm}
\caption{Deriving New Facts From the Known Facts}
\label{fig:basic_op_for_predicates}
\end{figure}


\vspace{-2mm}
\subsection{Deriving Simple Facts via Logical Reasoning}
\label{sec:derive_more_facts}
Based on the relation category, each predicate is labeled with a different set of properties, shown in \figref{table:relations}, which are mapped to different derivation rules. 

Based on the ground facts extracted from the databases, \tool derives additional facts to enrich the knowledge and generates test cases from each derived fact. 
As shown in \figref{fig:basic_op_for_predicates}, it provides four basic derivation rules, 
providing sound strategies to generate mutated facts from the ground facts. %\shil{1. How to define the he new name for $nm_{new}$? better provide this. 2. For negation rule, subject and object shall not be changed the position, can refer to oopsla paper.}
%\syh{1. added by the end of next paragraph; 2. revised}
%\footnote{
Note that these rules are also prevalently adopted in several literature~\cite{zhou2019completing, ren2020beta, liang2022reasoning, TIAN2022100159, abboud2020boxe} in the context of knowledge reasoning.
Given a predicate name $\nm$, the derivation  ``$\deriveRules {\nm}{\nm_{\m{new}}}{{\drule}}$'' holds if $\nm$ can be applied into a Prolog rule ${\drule}$, and produces more facts upon a new predicate with the name  $\nm_{\m{new}}$. 
These new predicates are freshly generated based on predefined suffixes. 


As indicated in \figref{table:relations}, 
%In particular, 
all the predicates can be applied to the $\RNeg$ rule, which derives the negated relations, e.g., ``!$\m{was}$'' using its positive counterpart, e.g., ``$\m{wasn't}$''. 
For the \emph{noun phrase} relations, both $\RSym$ and $\RTrans$ rules can apply, which generate more facts without creating new predicates. 
For the \emph{verb phrase} relations, both passive voice and active voice predicates can be applied to the  $\RInv$ rule, which captures the inverse relations, where the subject and object can be reversely linked through a variant of the original relation.  An example of such a rule is: 
\[
\m{influence(\OBJ, \SUBJ)}\hornarrow\m{influence\_by(\SUBJ, \OBJ)}
.\] 


We summarize the fact derivation process using \algoref{alg:logic_reasoning}. Given any relation category, we iterate its predicates and generates the derivation rule $\drule$ (Line 4). 
For simplicity, we assume that the choice of which derivation rule to apply is predetermined. Based on this assumption, a new Prolog program is constructed, comprising ground facts and $\drule$. 
In particular, we use $\llbracket \relation \rrbracket_{\Prolog}$ to denote the query results of $\relation$ concerning the Prolog program $\Prolog$, and $\Prolog$ contains all the ground facts and the derivation rule. 
{Note that when $\relation$ contains no variables, it returns Boolean results indicating the presence of the fact; otherwise, it outputs all the possible instantiation of the variables. }
For each instantiation that contains one subject ``\Subj'' and one object ``\Obj'', we then compose them with the new predicate, which is taken as a  \emph{derived fact}.  
These derived facts are later used to generate NL test cases, detailed in \secref{prompt}. 


{
\begin{algorithm}[!h]
\caption{Deriving New Facts}
\label{alg:logic_reasoning}
\small
\begin{algorithmic}[1]
\Require Ground Facts ($\groundTruthTriples$), Relation Category (\relationCat)
\Ensure Derived Facts ($\derivedFacts$)
\Function{DerivingFacts}{$\groundTruthTriples$, \relationCat}
\State $\derivedFacts \gets []$ \Comment{\commentstyle{Initialization}}
\For{$\nm$ in \relationCat}
\Comment{\commentstyle{Iterate each predicate}}
\State $\deriveRules{\nm}{\nm_{\m{new}}}{{\drule}}$\Comment{\commentstyle{Obtain the new predicate}} %the reasoning rule, and 
\State $\Prolog \gets \groundTruthTriples\plus\plus{\drule}$ \Comment{\commentstyle{Construct the prolog program}} 
\State $\m{instantiations} \gets \llbracket \nm_{\m{new}}(\SUBJ, \OBJ)\rrbracket_{\Prolog}$ 
%\Comment{\commentstyle{Obtain concrete entities}}
\For{(\Subj, \Obj) in $\m{instantiations}$}
\Comment{\commentstyle{Iterate each entity tuple}}
\State $\relation_{\m{new}} \gets \nm_{\m{new}}(\Subj, \Obj)$ 
\Comment{\commentstyle{Construct the derived fact}}
\State $\derivedFacts.\m{append}(\relation_{\m{new}})$ \Comment{\commentstyle{Append the derived facts}}
\EndFor
\EndFor
\State \Return $\derivedFacts$ \Comment{\commentstyle{Return the derived facts}}
\EndFunction 
\end{algorithmic}
\end{algorithm}
}






%-logic-based
\vspace{-2mm}
\subsection{Deriving Facts via Temporal Reasoning}
\label{sec:encoding_MTL}




%as the query language, 
%We convert the temporal-logic-based test cases into 
%temporal-logic-based natural language query, we use NLP techniques \cite{icaps2023fc,aaai2023fc} %\syh{cite here?} %to convert it into a 

Apart from the basic derivation rules, \tool can also automatically derive complex composition rules based on \emph{Metric Temporal Logic} (MTL) \cite{DBLP:conf/lics/OuaknineW05}. 
Specifically, we generate temporal test cases  based on randomly generated MTL formulae over historical events. 
We define the syntax for MTL formula in \figref{fig:syntax_of_the_metric_temporal_logic}, which contains the temporal operators for ``finally ($\mathcal{F}$)'', 
``globally ($\mathcal{G}$)'', 
``until ($\mathcal{U}$)'', 
and ``next ($\mathcal{N}$)''. 
The atomic propositions here are basic event relations $\nm$. 
%are three-element relations associated with time stamps. 
The time intervals are pairs of natural numbers indicating the lower and upper bounds of the years%\shil{only year?} \syh{so far yes, but I added a sentence by the end to make it more generic}
; and the constraint $\Istart \,{\leq}\, \Iend$ is enforced implicitly for all time intervals. 
In this paper for simplicity, we use discrete time measured in years as the smallest time interval. However, the framework can be extended to accommodate any smaller discrete time intervals, such as days or seconds. 


\begin{figure}[!h] 
\vspace{-2.5mm}
\small
\centering
\begin{align*}
(\m{MTL})\quad & \mtl &{::=}\quad &
\nm %(\Subj, \Obj) %\ap  
\,{\mid}\, \mathcal{F}_\interval \,\mtl
\,{\mid}\, \mathcal{G}_\interval \,\mtl 
\,{\mid}\, \mtl_1  
\,\mathcal{U}_\interval \,  \mtl_2 
\,{\mid}\, \mathcal{N} \,\mtl
\,{\mid}\, 
\\
&&&
%\,{\mid}\, \mtl_1 \, {\rightarrow} \,\mtl_2
 \mtl_1  \,{\wedge}\, \mtl_2
\,{\mid}\, \mtl_1  \,{\vee}\, \mtl_2
\,{\mid}\, \neg \mtl 
\\[0.3em]
%&(\m{Atomic~Proposaition})~ \ap ~{::=}~ 
%\nm\_{\m{TS}}(\interval, \Subj, \Obj)
%\\[0.3em]
(\m{Time~Interval}) \quad& \interval &{::=}\quad & [\Istart, \Iend]
\end{align*}
\vspace{-4mm}
\caption{A Core Syntax of MTL}
\label{fig:syntax_of_the_metric_temporal_logic}
\vspace{-3mm}
\end{figure}


To facilitate the generation of temporal-based Q\&A pairs, we define the semantics model for the MTL formulae in \defref{def:semantics_MTL}, where the history is a set of facts. 
Here, we use $\history$ as a set of historical relations, 
e.g., ``$\nm\_{\m{TS}}(\interval, \Subj, \Obj)$'', which are the time-stamped version relations of the three-element relations ``$\nm(\Subj, \Obj)$'', derived by one of the following rules: \\[-0.5em]

\noindent 
{\small $\ \   
\nm\_{\m{TS}}(\interval, \Subj, \Obj) \hornarrow \nm(\Subj, \Obj), \m{start}(\Obj, n_1), \m{end}(\Obj, n_2), \interval{=}[n_1, n_2]. 
$}\\[-0.5em]

\noindent  {\small $\ \  \nm\_{\m{TS}}(\interval, \Subj, \Obj) \hornarrow \nm(\Subj, \Obj), \m{start}(\Subj, n_1), \m{end}(\Subj, n_2), \interval{=}[n_1, n_2].$}
\\

\noindent which construct the event intervals using the time stamps associated with the object or the subject, respectively. 
The ``$\m{start}$'' and ``$\m{end}$'' predicates are originally generated from the knowledge database and mark the starting and ending points of the duration of the object (or subject) event. 
For simplicity, we use ``$\nm\_{\m{TS}}(\interval)$'' to abbreviate ``$\nm\_{\m{TS}}(\interval, \Subj, \Obj)$'' when $\Subj$ and $\Obj$ are unambiguously unique from the context. 
We also use $\llbracket \nm\_{\m{TS}}(\interval) %, \Subj, \Obj
\rrbracket_{\history}$ to denote the validity of querying the presence of a fact $\nm\_{\m{TS}}(\interval)$ 
against the historical facts $\history$, which stores all the time-stamped three-element predicates. 


\begin{definition}[A Point-based Semantics for MTL]
\label{def:semantics_MTL}
Given a set of (historical) facts $\history$, recording all the events that happened in history, an MTL formula $\mtl$, and a concrete time point  $\timepoint$, the satisfaction relation $(\history, \timepoint) \models \mtl$  (read at the time point \timepoint, the history $\history$ satisfies $\mtl$) is recursively defined as follows: 

{
\small
\begin{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% AP  R   %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/AP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Finally %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Finally_sementics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Globally %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Globally_sementic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Next %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Next_sementic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% negation %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Neg_semnetic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Unitl %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Until_sementic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% conjunction %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Conj_sementic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% disjunction %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sementics_rules/Disj_sementic}
\end{align*}}
\end{definition}
\vspace{2mm}



We randomly generate temporal test cases based on the rich set of historical events and the syntax templates defined in \figref{fig:syntax_of_the_metric_temporal_logic}. 
Each temporal question consists of a concrete MTL formula and a concrete time point, i.e., $(\phi, \timepoint)$. 
For example, the query ``\emph{At 1800, does Victorian era finally come within 40 years?}'' is represented as $(\mathcal{F}_{[0, 40]} \m{victorian\_era}, 1800)$. 
Next, we show how to obtain the expected answer by automatically generating Prolog reasoning rules. 

Given a query $\mtl$, the relation ``$\encoding{\mtl}{\nm}{\widetilde{\drule}}$'' holds if $\mtl$ can be translated into a set of Prolog rules, i.e., $\widetilde{\drule}$. 
Querying ``$\nm(\interval)$'' 
%with the set of Prolog rules 
$\widetilde{\drule}$, against the known database facts yields a set of instantiation of the interval $\interval$. 
The validity of $\mtl$ at any given time point $\timepoint$ is then indicated by the existence of a concrete interval  $\interval$ such that $\timepoint\,{\in}\,\interval$. 
We define the full set of encoding rules for MTL operators in \figref{fig:encoding_rules_mtl}. 

These encoding rules deploy several auxiliary predicates: the 
``$\m{findall}(\interval, \nm)$'' relation indicates that $\interval$ is a union of all the time intervals which satisfy $\nm$; 
the  ``$\m{compl}(\interval, \interval_1)$'' relation indicates that time intervals $\interval$ and $\interval_1$ complement each other, and their union encompasses the entire set of time points; the union and intersection operations, denoted by $\cup$ and $\cap$, are applied to two sets of time intervals. 

\begin{figure}[!h]
% \begin{minipage}[b]{1\linewidth}
\vspace{-2mm}
\vspace{0mm}
\begin{lstlisting}[xleftmargin=6em,numbersep=5pt,basicstyle=\footnotesize\ttfamily]
//nm1 = charles_dickens
charles_dickens_TS([1812, 1870]).
//nm2 = victorian_era
victorian_era_TS([1837, 1901]).
\end{lstlisting} 
\vspace{-1mm}
\caption{Database $\history_s$ Containing Two Time-stamped Events}
\label{fig:Prolog_encoding_Example}
\vspace{-2mm}
\end{figure}


Next, we illustrate the encoding rules for each MTL operator using a few examples. 
To facilitate the illustration, we use a small database $\history_s$ defined in \figref{fig:Prolog_encoding_Example}, which contains two facts: ``\emph{The author Charles Dickens was born in 1812 and he lived until 1870, which spanned a significant portion of the Victorian era}'' and ``\emph{The Victorian era started from 1837 until Queen Victoria died in 1901}'': 

\begin{figure}[!b]
\vspace{-3mm}
\centering\small
\begin{gather*} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% AP R %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/AP-R}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% Finally %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Finally_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Globally %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Globally_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Next %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Next_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Until %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Until_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Negation %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Neg_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Conjunction %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Conj_encoding}
\\[0.6em]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Disjunction %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{encoding_rules/Disj_encoding}
\end{gather*}
\caption{Encoding MTL Formula $\mtl$ using Prolog Rules}
\label{fig:encoding_rules_mtl}
\end{figure}




\begin{enumerate}[itemsep=0.7em,leftmargin=!,wide]
\item 
When 
$\mtl\,{=}\,\m{charles\_dickens}$ and $\timepoint\,{=}\,1800$: \\ 
According to the encoding rule $[\trans\text{-}\m{AP}]$, the generated Prolog rule is: $\m{\nm1(\interval)}\hornarrow\,\m{
charles\_dickens\_TS(\interval)}$.
Now, querying ``$\nm1(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1812, 1870]$. Since $1800\,{\not\in}\,\interval$, the expected result of this query is false.

Similarly, when 
$\mtl\,{=}\,\m{victorian\_era}$ and $\timepoint\,{=}\,1900$: \\ 
According to the encoding rule $[\trans\text{-}\m{AP}]$, the generated Prolog rule is: $\m{\nm2(\interval)}\hornarrow\,\m{
victorian\_era\_TS(\interval)}$.
Now, querying ``$\nm2(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1837, 1901]$. Since $1900\,{\in}\,\interval$, the expected result of this query is true. 

\item When $\mtl\,{=}\,\mathcal{F}_{[0, 40]}\,\m{victorian\_era}$ and $\timepoint\,{=}\,1800$: \\
According to the encoding rule $[\trans\text{-}\m{Finally}]$, the generated Prolog rule is: 
$\m{finally\_\nm2([n_1\text{-}40, n_2\text{-}0])}\hornarrow \m{
\nm2([n_1, n_2])}.$
Now, querying ``$\m{finally\_\nm2}(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1797, 1901]$. Since $1800\,{\in}\,\interval$, the expected result is true. 
Indeed, all the time points in $\interval$ satisfy the property: ``\emph{within 40 years, finally Victorian era came/still exist}''. 

\item When $\mtl\,{=}\,\mathcal{G}_{[30, 50]}\,\m{victorian\_era}$ and $\timepoint\,{=}\,1800$: \\
According to the rule $[\trans\text{-}\m{Globally}]$, the generated Prolog rule is: $\m{globally\_\nm2([n_1\text{-}30, n_2\text{-}50])}\hornarrow \m{
\nm2([n_1, n_2])}.$
Now, querying ``$globally\_\nm2(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1807, 1851]$. Since $1800\,{\not\in}\,\interval$, the expected result is false. 
Indeed, only all the time points in $\interval$ satisfy that ``\emph{Victorian era is globally true throughout the 30th to the 50th years in the future}''. 

\item When $\mtl\,{=}\,\mathcal{N}\,\m{victorian\_era}$ and $\timepoint\,{=}\,1836$: \\
According to the rule $[\trans\text{-}\m{Next}]$, the generated Prolog rule is: $\m{next\_\nm2([n_1\text{-}1, n_2\text{-}1])}\hornarrow \m{
\nm2([n_1, n_2])}.$
Now, querying ``$\m{next\_\nm2}(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1836, 1900]$. Since  $1836\,{\in}\,\interval$, the expected result is true. 
Indeed, all the time points in $\interval$ satisfy that ``\emph{next year Victorian era came/still exist}''. 

\item When $\mtl\,{=}\,\m{charles\_dickens}
~\mathcal{U}_{[10, 20]}\,\m{victorian\_era}$ and $\timepoint{=}1800$: This query aims to determine the time interval $\interval$ that encompasses all time points $\timepoint'$ for which there exists a future year ($\timepoint'\plus\distance$) when the Victorian era had begun; and during the time from $\timepoint'$ to $\timepoint'\plus\distance$, Charles Dickens must have been born and remained alive throughout. Lastly, check if $1800\,{\in}\,\interval$.

In $[\trans\text{-}\m{Until}]$, 
$\m{helper1}$ computes all the possible  $\timepoint'\plus d$  
%where \\  $d\,{\in}\,[10, 20]$, 
such that $(\history, k)\models \m{charles\_dickens} ~\m{forall}~ 
k~\m{with} ~\timepoint'{<}k{<}(\timepoint'\plus\distance)$. 
Next $\m{helper2}$ computes the overlapping  intervals of $\m{helper1}$ and the intervals that also satisfy $(\history, \timepoint'\plus\distance)\,{\models}\,\m{victorian\_era}$. 
Then $\nm_f$ computes the interval of $\timepoint'$ which finally reach $\m{helper2}$ within 10 to 20 years. 
Lastly, the final answer of the interval of $\timepoint'$~is the intersection of $\nm_f$ and $\nm_1$. 
Therefore, given the concrete query $(\phi, \timepoint)$, from $[\trans\text{-}\m{Until}]$, 
the generated rules are shown in \figref{fig:until10-20-encoding}. 




\begin{figure}[!h]
\vspace{-3mm}
{
\begin{align*}
&\m{helper1([n_1\plus10, n_2\plus1])}\hornarrow \m{\nm1([n_1, n_2])}.
& // [1822, 1871]
\\
&\m{helper2(\interval_1\cap\interval_2)}\hornarrow\m{helper1(\interval_1)}, \m{\nm2(\interval_2)}. 
& // [1837, 1871]
\\
& \m{\nm_f([n_1\text{-}20, n_2\text{-}10])}\hornarrow \m{
helper2([n_1, n_2])}.
& // [1817, 1861]
\end{align*}
\vspace{-4mm}
\[\m{charles}\_\m{until}\_\m{victorian\_era}(\interval_1\cap\interval_2) \hornarrow \m{\nm1(\interval_1)}, \m{\nm_f(\interval_2)}.\]}
\caption{Prolog Rules Generated for an "Until" Query}
\label{fig:until10-20-encoding}
\vspace{-1mm}
\end{figure}

Querying ``$\m{charles}\_\m{until}\_\m{victorian\_era}$'' against $\history_s$ yields $\interval\,{=}\,[1817, 1861]$. Since $1800\,{\not\in}\,\interval$, the expected result is false. 
Indeed, only all the time points in $\interval$ satisfy $\phi$ under the semantic definition of \emph{Until}, cf.  \defref{def:semantics_MTL}. For example when $\timepoint'{=}1817$, there exists $\distance{=}20$ such that $\phi$ holds; and when $\timepoint'{=}1861$ there exists $\distance{=}10$ such that $\phi$ holds. 

Note that, in this encoding, the interval of ``\emph{Until}'' operators does not include $[0, 0]$, as $\mtl_1  
\,\mathcal{U}_{[0, 0]} \,  \mtl_2$ essentially equals $\mtl_2$. Therefore, when the interval compasses $[0, 0]$, we use the following rule to decompose the encoding: (Note that when $\interval'\,{=}\,\interval{\setminus}[0, 0]$, it means $\interval'\cup[0, 0]\,{=}\,\interval$)
\begin{align*}
\input{encoding_rules/Until_0}
\end{align*}

\vspace{2mm}
\item When $\mtl\,{=}\,\neg\,\m{victorian\_era}$ and $\timepoint\,{=}\,1800$: \\
By $[\trans\text{-}\m{Neg}]$, the generated Prolog rule is: 
$\m{neg\_\nm2(\interval)}\hornarrow$ 
$\m{findall}(\interval_1, \nm1), \m{compl}(\interval_1, \interval).$ 
Querying ``$\m{neg\_\nm2}(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1, 1836] \cup [1902, 2024].$
Here, we take all the after-century years to be the full set. 
Since $1800\,{\in}\,\interval$, the expected result is true. 
Indeed, all the time points in $\interval$ satisfy that ``\emph{Victorian era has not come/already passed}''.  


\item When $\mtl\,{=}\,\m{charles\_dickens}\,{\wedge}\,\m{victorian\_era}$ and $\timepoint{=}1900$: 
By $[\trans\text{-}\m{Conj}]$, the generated Prolog rule is: 
$\m{\nm1\_and\_\nm2(\interval_1{\cap}\interval_2)}\hornarrow 
\m{findall}(\interval_1, \nm1), \m{findall}(\interval_2, \nm2)$. 
Now, querying ``$\m{\nm1\_and\_\nm2}(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1837, 1870]$. Since $1900\,{\not\in}\,\interval$, the expected result is false. 
Indeed, only the time points in $\interval$ satisfy that ``\emph{Victorian era exists and Charles Dickens is alive}''. 


\item When $\mtl\,{=}\,\m{charles\_dickens}\,{\vee}\,\m{victorian\_era}$~and
$\timepoint{=}1900$: 
By $[\trans\text{-}\m{Disj}]$, the generated Prolog rule is as follows: 
$\m{\nm1\_or\_\nm2(\interval_1{\cup}\interval_2)}\hornarrow 
\m{findall}(\interval_1, \nm1), \m{findall}(\interval_2, \nm2)$. 
Now, querying ``$\m{\nm1\_or\_\nm2}(\interval)$'' against $\history_s$ yields $\interval\,{=}\,[1812, 1901]$. Since $1900\,{\in}\,\interval$, the expected result is true. 
Indeed, all the time points in $\interval$ satisfy that ``\emph{Victorian era exists or Charles Dickens is alive}''.  
%\shil{check implementation?}
\end{enumerate}
\vspace{3mm}






{\emph{\textbf{Remark.}}} 
While discrete-time MTL is commonly employed for model-checking timed verification~ \cite{DBLP:phd/us/Henzinger91}, utilizing Prolog to encode MTL for reasoning about the temporal relationships among events and detecting LLM hallucination is novel. 
These encoding rules can recursively accommodate the entire range of MTL formulas, including those with any level of nesting. 
We provide a formal definition for the correctness of these encoding rules in \theoref{ThemSoundAndComplete} and demonstrate that they are both sound and complete.



\begin{restatable}[Correctness of the encoding rules]{thm}{ThemSoundAndComplete}
\label{ThemSoundAndComplete}
~\\
Given any $\history$, 
$\mtl$, and 
$\encoding {\mtl}{\nm}{\widetilde{\drule}}$, let $\Prolog{=}\history \plus\plus \widetilde{\drule}$, we define,  \\
(1) Soundness: \\
$\forall\, \interval$.  
$\llbracket \nm(\interval) \rrbracket_{ \Prolog} {=} \m{true}$, then 
$\forall\, \timepoint\,{\in}\, \interval$, we have 
$(\history, \timepoint) \models \mtl$;  \\
(2) Completeness: \\ 
$\forall \,\timepoint\,$. $(\history, \timepoint) \models \mtl$, then $\exists\, \interval$. $\llbracket \nm(\interval) \rrbracket_{\Prolog} {=} \m{true}$  and $\timepoint\,{\in}\,\interval$. 
\end{restatable}

\begin{proof}
By structural induction over $\phi$. 
%a case analysis of the encoding rules.
The detailed proofs are given in the Appendix. %\appref{app:correctness}.  
\end{proof}




\begin{table*}[!t]
% \def\arraystretch{1.0}
\setlength{\tabcolsep}{1pt}
\centering
% \footnotesize
\caption{Relation-Template Mapping Patterns.}
 % \lnk{need to refine}}
\label{table:template}
\footnotesize
%\resizebox{\linewidth}{!}{
\begin{tabular}{l l}
\toprule 
\textbf{Relation} & \textbf{Template Examples}  \\
    \midrule
{Noun Phrase} & \begin{tabular}[l]{@{}l@{}} - Is it true that 
$\langle \m{Subject}\rangle$ and 
$\langle\m{Object}\rangle$ share 
$\langle\m{Relation}\rangle$? 
\\ - $\langle\m{Subject}\rangle$ and $\langle\m{Object}\rangle$ have/made/shared totally different $\langle\m{Relation}\rangle$. Please judge the truthfulness of this statement.
%\\Please judge the truthfulness of this statement. 
    \end{tabular}  \\
    \midrule
    \begin{tabular}[l]{@{}l@{}} Verb Phrase \\ (Passive Voice) \end{tabular} & \begin{tabular}[l]{@{}l@{}} - Is it true that $\langle \m{Subject}\rangle$ is/was/are/were $\langle \m{Relation}\rangle$ $\langle\m{Object}\rangle$? \\ - It is impossible for $\langle \m{Subject}\rangle$ to be $\langle\m{Relation}\rangle$ $\langle\m{Object}\rangle$. Am I right?%\\ Other formats... 
    \end{tabular}  \\
    \midrule
\begin{tabular}[l]{@{}l@{}} Verb Phrase \\ (Active Voice) \end{tabular}
 & \begin{tabular}[l]{@{}l@{}} - Is it true that 
 $\langle \m{Subject}\rangle$
 $\langle\m{Relation}\rangle$
 $\langle\m{Object}\rangle$?  \\ - $\langle \m{Subject}\rangle$ $\langle\m{Relation}\rangle$ $\langle\m{Object}\rangle$. 
 %\\ Please judge the truthfulness of this statement.
 %\\ Other formats... 
 \end{tabular}  \\

    \bottomrule %添加表格底部粗线
\end{tabular}
%}
\end{table*}

\begin{table*}[!t]
\setlength{\tabcolsep}{3pt}
\centering
% \footnotesize
\caption{Temporal-Template Mapping Patterns (implicitly querying upon year $\iyear$).}
\label{table:temporal_template}
\footnotesize
\begin{tabular}{l  l }
\toprule 
\textbf{MTL Formulae} & \textbf{Template Examples}  \\
\midrule 
\mtltoNL($\nm$) &  Did $\langle\nm\rangle$ happen at year $\langle \iyear \rangle$? 
\\  \midrule
\mtltoNL($\mathcal{F}_\interval \,\mtl$) & Did ``Event'' finally happen within the time frame of $\langle \interval \rangle$ after the year $\langle \iyear \rangle$, where ``Event'' is defined as $\langle \mtltoNL(\mtl)\rangle$? 
\\ 
\midrule 
\mtltoNL($\mathcal{G}_\interval \,\mtl$) &  Did ``Event'' globally happen within the time frame of $\langle \interval \rangle$ after the year $\langle \iyear \rangle$, where ``Event'' is defined as $\langle \mtltoNL(\mtl)\rangle$?
\\ 
\midrule 
\mtltoNL($\mathcal{N} \,\mtl$) & Did ``Event'' happen in the next year of $\langle \iyear \rangle$, where ``Event'' is defined as $\langle \mtltoNL(\mtl)\rangle$? 
\\ 
\midrule 
\mtltoNL($\mtl_1 \, \mathcal{U}_\interval \,\mtl_2$) &  
\begin{tabular}[l]{@{}l@{}} 
Did ``Event$_1$'' happen continuously until ``Event$_2$'' started, during the period $\langle \interval \rangle$ after the year $\langle \iyear \rangle$, \\ 
where ``Event$_1$'' is $\langle \mtltoNL(\mtl_1) \rangle$ and ``Event$_2$'' is $\langle \mtltoNL(\mtl_2) \rangle$?
\end{tabular}
\\ 
\midrule 
\mtltoNL($\mtl_1  \,{\wedge}\,  \mtl_2$) &  Did both ``Event$_1$'' and  ``Event$_2$'' happen at year $\langle \iyear \rangle$, where ``Event$_1$'' is $\langle \mtltoNL(\mtl_1) \rangle$ and ``Event$_2$'' is $\langle \mtltoNL(\mtl_2) \rangle$? 
\\ 
\midrule 
\mtltoNL($\mtl_1  \,{\vee}\,  \mtl_2$) &  Did either ``Event$_1$'' or ``Event$_2$''  happen at year $\langle \iyear \rangle$, where ``Event$_1$'' is $\langle \mtltoNL(\mtl_1) \rangle$ and ``Event$_2$'' is $\langle \mtltoNL(\mtl_2) \rangle$? 
\\ 
\midrule 
\mtltoNL($\neg \mtl $) &  Did ``Event'' not happen at year $\langle \iyear \rangle$, where ``Event'' is $\langle \mtltoNL(\mtl) \rangle$? 
\\ 
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table*}




\subsection{Benchmark Construction}
\label{prompt}
%From the derived facts, 
\tool constructs question-answer~(Q\&A) pairs and prompts to facilitate the testing for FCH. 
To address the challenge of the high human efforts required in the test oracle generation, we design an automated approach based on mapping relations between various entities to problem templates, largely reducing reliance on manual efforts. 

\textbf{\emph{Question Generation.}}
%\wkl{rewritten, check}
%\syh{checked, ok}
To facilitate efficient and systematic generation of test cases and prompts, we have adopted a method that leverages entity relationships and mappings of temporal operators to predefined Q\&A templates. 

When constructing relation-based Q\&A templates (without temporal operators), one key aspect lies in aligning various types of relations with the corresponding question templates from the mutated triples, i.e., the predicate type in the triple. Different relation types possess unique characteristics and expressive requirements, leading to various predefined templates. 
As listed in \tabref{table:template}, we map the relation types to question templates based on speech and the grammatical tense of the predicate to guarantee comprehensive coverage. 

When constructing temporal-logic-related queries, we define a mapping pattern for each temporal operator, as outlined in \tabref{table:temporal_template}. For any query expressed as ``$\mtl$''  with any concrete year $\iyear$ in query, the $\mtltoNL(\mtl)$ function converts the MTL formula $\mtl$ into a natural language query. In this context, $\mtltoNL(\mtl')$ is called recursively to generate the natural language description for the CTL subformula $\mtl'$. 



In both mapping patterns, we enhance the construction of the naturally formatted questions by leveraging an LLM to reformulate the structure and grammar of the Q\&A pairs. 

\textbf{\emph{Answer Generation.}}
We note that the answer to the corresponding question is readily attainable from the factual knowledge and the Prolog reasoning rules, defined in both \figref{fig:basic_op_for_predicates} and \figref{fig:encoding_rules_mtl}. 
Primarily, it is easy to determine whether the answer is \emph{true} or \emph{false} based on the mutated triples and the ground-truth time intervals using temporal reasoning. 
Meanwhile, mutated templates with positive and negative semantics via the usage of synonyms or antonyms, which greatly enhance the diversity of the questions, can be treated similarly as the negation rule defined in \figref{fig:basic_op_for_predicates}. 
Specifically, if the answer to a question with original semantics is Yes/No, then for a question with mutated opposite semantics, the corresponding answer would %naturally 
be the opposite, i.e., No/Yes. For example, after obtaining the original Q\&A pair \textit{- ``Is it true that Crohn's disease and Huntington's disease could share similar symptoms and signs? - Yes.''}, we can use antonyms to mutate it into \textit{- ``Is it true that Crohn's disease and Huntington's disease have different symptoms and signs? - No.''}
%\syh{what does this mean?}







% In total, we have defined 60 templates according to the pre-defined rules.
% , from which we have generated 194,850 Q\&A pairs. To prevent the dataset scope from becoming overly extensive, we conduct an initial screening based on categories of reasoning rules, ultimately yielding a total of 14,228 question pairs.
\begin{comment}   
\begin{table*}[!t]
    % \def\arraystretch{1.0}
    \setlength{\tabcolsep}{1ex}
	\centering
        \Large
	% \footnotesize
	\caption{Relation-Template Mapping Pattern.}
 % \lnk{need to refine}}
        \label{table:template}
	\resizebox{\linewidth}{!}{
	\begin{tabular}{l l l}
    \toprule 
    \textbf{Relation} & \textbf{Template} &\textbf{Example} \\
    \midrule
    \textbf{Noun Phrase} & \begin{tabular}[l]{@{}l@{}} - Is it true that \textit{Subject} and \textit{Object} share \textit{Relation}? \\ - \textit{Subject} and \textit{Object} have/made/shared totally different [Relation]. \\Please judge the truthfulness of this statement. \end{tabular} & \begin{tabular}[l]{@{}l@{}} New Triple: \textit{[Crohn's disease, similar\_symptoms\_and\_signs, Huntington's disease]}\\ Template: - Is it true that \textit{Crohn's disease} and \textit{Huntington's disease} share \textit{similar symptoms and signs}? -Yes. \\- Does \textit{Crohn's disease} and \textit{Huntington's disease} have similarities on {symptoms and signs}? - Yes. \end{tabular} \\
    \midrule
    \textbf{Verb Phrase in Passive Voice} & \begin{tabular}[l]{@{}l@{}} - Is it true that \textit{Subject} is/was/are/were \textit{Relation} \textit{Object}? \\ - It is impossible for \textit{Subject} to be \textit{Relation} \textit{Object}. Am I right?\\ Other formats... \end{tabular} & \begin{tabular}[l]{@{}l@{}} New Triple: \textit{[Kuratowski's theorem, not\_proved\_by, Kurt Gödel]}\\ Template: - Is it true that Kuratowski's theorem was proved by Kurt Gödel? - No.\\- Someone else other than Kurt Gödel proved Kuratowski's theorem, am I right? - Yes. \end{tabular} \\
    \midrule
    \textbf{Verb Phrase in Active Voice} & \begin{tabular}[l]{@{}l@{}} - Is it true that \textit{Subject Relation Object}?  \\ - \textit{Subject} \textit{Relation} \textit{Object}. Please judge the truthfulness of this statement.\\ Other formats... \end{tabular} & \begin{tabular}[l]{@{}l@{}} New Triple: \textit{[Baby Don't Lie, appeared\_before, Spark the Fire]}\\ Template: - Is it true that \textit{Baby Don't Lie} appeared before \textit{Spark the Fire}? - Yes.\\- \textit{Baby Don't Lie} never appeared before \textit{Spark the Fire}. \\ Please judge the truthfulness of this statement. -No.   \end{tabular} \\
    % \midrule
    % \textbf{Custom-Designed} & \begin{tabular}[l]{@{}l@{}l@{}l@{}l@{}} Given the \textit{SubjectList}, is it true that \textit{SubjectSelected} is the {Relation} among them?\\ Other formats... \end{tabular} & \begin{tabular}[l]{@{}l@{}l@{}l@{}l@{}} New Triple: [\textit{Roman Holiday, Hindenburg disaster newsreel footage, Lassie Come Home, The White Parade}, \textit{descending\_duration\_order}, \\ \textit{Roman Holiday, Lassie Come Home, The White Parade}]\\ Template: - Given the list [Roman Holiday, Hindenburg disaster newsreel footage, Lassie Come Home, The White Parade],\\ is it true that Roman Holiday have the longest duration among them? - Yes. \end{tabular} \\
    \bottomrule %添加表格底部粗线
\end{tabular}}
\end{table*}
\end{comment}

\textbf{\emph{Prompt Construction.}}
% \wkl{here maybe give a sample prompt screenshot as a figure to illustrate?}
As illustrated in \tabref{table:prompt}, before initiating any interaction with LLMs, we predefine specific instructions and prompts, requesting the model to utilize its inherent knowledge and inferential capabilities to deliver explicit (Yes/No/I don't know) judgments on our queries. Additionally, we instruct the model to present its reasoning process in a template following the judgment. The main goal is to ensure that LLMs give easy-to-understand responses using standardized prompts and instructions. 
This approach also helps the model to effectively exercise its reasoning abilities based on the given instructions and examples.


\begin{table*}[!t]
%\vspace{-0.3cm}
    % \def\arraystretch{1.0}
    \setlength{\tabcolsep}{1ex}
	\centering
    %\large 
	\small
	\caption{Prompt Template. %\shil{Shall we restrict the answer for relation query begnin with Yes/No/I don't know?}
    }
        \label{table:prompt}
        \vspace{-0.1cm}
	%\resizebox{\linewidth}{!}{
	\begin{tabular}{l}
    \toprule 
    \rowcolor{mycolor}
    \textbf{\instruction:} Answer the question with your knowledge and reasoning power.\\
    \midrule
    \rowcolor{mycolor} \textbf{\query (Relation):}  Given the $\langle \textit{question} \rangle$: \textit{question}, please provide an answer with your knowledge and reasoning power.\\ 
    \rowcolor{mycolor} Think of it step by step with a human-like reasoning process. After giving the answer, list the knowledge used in your\\ 
    \rowcolor{mycolor} reasoning process in the form of declarative sentences and point by point. The answer must contain `Yes', `No' or `I \\
    \rowcolor{mycolor} don't know' at the beginning. \\
    \midrule
    \rowcolor{mycolor} \textbf{\query (Temporal):}  Given the question: 
    $\langle \textit{question} \rangle$, please provide an answer with your knowledge and reasoning power \\
    \rowcolor{mycolor}  upon metric temporal logic. Think it step by step with a human-like reasoning process. After giving the answer, list the \\
    \rowcolor{mycolor} evidence from your temporal reasoning  in the form of declarative sentences and point by point. The answer must contain   \\
\rowcolor{mycolor} `Yes', `No' or `I don't know' at the beginning.\\
    \bottomrule %添加表格底部粗线
\end{tabular}
\end{table*}




\vspace{-1mm}
\subsection{Response Evaluation}\label{response}
%
%\begin{lstlisting}
% \begin{align*}
%     <program>::=&<decoder>|<query>|<model>|<condition>|<distribution>\\&|<query*>|<parser> \\
%     <query*>::=&statement(transform(s,R,o))\\
%     <parser>::=&extract(statement)\\
%     <transform>::=&[Neg]|[Sym]|[Inverse]|[Trans]|[Comp]
% \end{align*}
% \begingroup\vspace*{-1cm}
% \captionof{figure}{Syntax of Extended LMQL.}\label{sec:syntax}
% \vspace*{\baselineskip}\endgroup
%\end{lstlisting}
%
% \begin{lstlisting}[language=Python, caption=LMQL Program Grammar]
% <decoder> ::= argmax | beam(n=<int>) | sample(n=<int>)
% <query> ::= (<python_statement>)+
% <cond> ::= <cond> and <cond> | <cond> or <cond> | not <cond> | <cond_term>
% <cond_term> ::= <python_expression>
% <cond_op> ::= < | > | = | in
% <dist> ::= <var> over <python_expression>
% \end{lstlisting}
% \begin{grammar}
% <LMQL Program> ::= <decoder> <query>

% <decoder> ::= `argmax' | `beam(n=\textit{int})' | `sample(n=\textit{int})'

% <query> ::= (<python\_statement>)+

% <cond> ::= <cond> `and' <cond> | <cond> `or' <cond> | `not' <cond> | <cond\_term>

% <cond\_term> ::= <python\_expression>

% <cond\_op> ::= `<` | `>' | `=' | `in'

% <dist> ::= <var> `over' <python\_expression>
% \end{grammar}

% To facilitate the automated the query and answer validation process, we extend the previously proposed LMQL~(language Model Query Language)~\cite{Beurer-Kellner-2023} designed for LLM programming. LMQL utilizes SQL-like elements and a imperative syntax for scripting, as shown in Figure~\ref{sec:syntax}. More specifically, LMQL defines the interactive process with an LLM as a python-like $<program>$, including a $<decoder>$ to the decoding procedure employed by the LMQL runtime when solving the
% query, a $<query>$ to model the interaction with the model, a $<model>$ to denote the LLM to interact with, a $<condition>$ to place constraints on the variables in the program, a $<distribution>$ to represent the probability for output predictions from the LLM. language is augmented with additional constructs to facilitate the interaction and generation capabilities of LLMs.

% Firstly, we introduce the <query*> element as an extension to the existing <query> element. The <query*> block models the interaction with the LLM, serving as the prompt that is fed into the model. These query strings allow for the use of specially escaped subfields, similar to Python's f-strings. These subfields, denoted by "[varname]", represent phrases that will be generated by the LLM, also known as holes.

% Furthermore, we introduce a new element named <parser>. The <parser> element is responsible for extracting triples (subject-predicate-object) from the LLM-generated answers for further semantic comparison. The <parser> is capable of processing one or multiple sentences, extracting triples from each sentence sequentially and recording the derived results for further use. The <parser> element plays a crucial role in the answer validation process by breaking down the LLM's generated responses into structured triples. These triples can then be compared against a knowledge base or a set of predefined rules to validate the semantic correctness and coherence of the generated content.


The objective of this module is to enhance the detection of FCH in LLM outputs, specifically focusing on identifying the discrepancies between LLM responses and the verified ground truths. Recognizing the inherent limitation in directly accepting ``Yes'' or ``No'' answers from LLMs, our approach underscores the automated detection of factual consistency during the reasoning process presented by LLMs. 
% This analysis is vital for accurately determining the factual consistency of LLM responses, thereby addressing the primary challenge in identifying FCH within LLM outputs. 
% To achieve automated detection of factual consistency, our methodology first incorporates a parsing step that leverages advanced NLP techniques. This step is designed to extract essential semantic elements from each sentence within LLM outputs, assembling these elements into a coherent, semantic-aware structure. 
% The foundational premise of our approach is predicated on evaluating the semantic similarity between these constructed structures, aiming to discern the degree of consistency in their underlying semantics.
% Subsequently, we propose the development of a set of similarity-based testing oracles. These oracles are instrumental in applying metamorphic testing principles, enabling us to systematically assess the consistency or inconsistency between LLM responses and the established ground truth. 
Our approach is structured around several critical steps, as listed in \algoref{alg:eval} and detailed below:
% \shil{(SL: to cite Algorithm 3 and when explanation, referring to line number? For lines 11 to 15, use ``or" to include three conditions?)}
% The key target of this module is to detect the FCH by identifying the inconsistency between answers from the LLMs and the groundtruth in Q\&A pairs. However, as we unable to directly trust the yes or no answers from LLM directly, we need to parse the reasoning process carefully before reaching the verdict on the correctness with accuracy, which is the aforementioned challenges for detecting FCH in LLMs.  
% %This module outlines our approach for detecting hallucinations in the responses of the target LLMs. A key insight is the premise that any response contradicting the answer of the factual Q\&A pairs we provide is inherently regarded as an occurrence of FCH.
% To facilitate the automatic detection, we first design a parsing step to utilize an NLP-based approach to extract the critical semantic component from each sentence from LLM outputs, and assemble them into a semantic-aware structure. The key insight is to examine the similarity between these structures to determine the consistency in their semantics. Then, by designing a set of similarity-based testing oracles, we are able to utilize metamorphic testing to determine the (in)consistency between LLM answers and groundtruth.  This method primarily comprises the following steps:
%During the aforementioned prompt design process, we have prepared question prompts and their corresponding answers, thereby establishing ground truth Q\&A pairs. Therefore, we can automatically compare the LLM output with the expected ground truth answer to detect the discrepancies. Moreover, we utilize an NLP-based approach to compare the semantics in the reasoning process to identify inconsistencies to assist in understanding the cause of FCH. %our similarity between the responses from LLMs with these ground truth Q\&A pairs to verify the consistency and logical soundness of the LLMs' responses. 
% Detailed conclusions are discussed in the following section.


%In summary, we propose a method based on semantic parsing and metamorphic relations to verify whether LLM responses contain FCHs. This method primarily comprises the following steps:

%labelwidth=!,
\begin{enumerate}[wide,  labelindent=9pt]
%Step 1. 
\item \textbf{Preliminary Screening.} Given the LLM response $\llmResponse$, we first eliminate scenarios when the LLM declines to provide an answer, indicated by the ``answer'' field of LLM's responses. 
Most of these responses arise because the LLM lacks the relevant knowledge for the reasoning process. As these responses adhere to the LLM's principle of honesty, we categorize them as correct and normal responses.
% (as described in Algorithm~\ref{alg:eval} Line 7-8). 

%Step 2. 
\item \textbf{Response Parsing and Semantic Structure Construction.} Provided with the remaining suspicious responses from $\llmResponse$ and ground truth facts $\groundTruthTriples$, we use \textsc{ExtractTriple} function to extract triples that follow the same structure as the fact defined in the \secref{subsec3.1}. For each LLM response, the extracted triples ($\widetilde{\m{Trpl}}$) are based on the statements contained in the \textit{reasoning process} part of the LLM's response, which is further utilized to construct a response semantic structure $\semantic_{\m{resp}}$ using the \textsc{BuildGraph} function. In this structure, the $\widetilde{\entity}$ are depicted as \emph{nodes} ($\m{N}$), and the relational predicate ($\nm$) between them are illustrated as \emph{edges} ($\m{E}$). Concurrently using the same approach, we construct another semantic structure $\semantic_{\m{ground}}$ using $\groundTruthTriples$.

%Step 3. 
\item \textbf{Similarity-based Metamorphic Testing and Oracles.} 
We apply metamorphic relations to identify hallucination answers from LLMs, i.e., comparing the similarity between semantic structures generated by LLMs and the ground truth counterparts. Note that we provide four classifications: correct responses ($\m{CO}$), hallucinations caused by error inference ($\m{EI}$), hallucinations caused by erroneous knowledge ($\m{EK}$), and hallucinations containing both issues ($\m{OL}$). 
Specifically, the oracles for metamorphic testing can be divided into the following types:
 
% We then apply metamorphic relations to detect and evaluate potential errors in LLM responses, based on the relationships between inputs and outputs, rather than relying on traditional labeled data. 
% In our context, metamorphic relations specifically refer to comparing the similarity between semantic structures generated by LLMs and the ground truth counterparts, to identify and classify hallucination answers from LLMs.
% (as mentioned in Algorithm~\ref{alg:eval} Line 12-18). 
\end{enumerate}

\begin{comment}

tree = Leaf() | Node ()

resp{
    answer = bool 
    steps = tree ??
}

ground_truth{
    answer = bool 
    reasoning = tree
}


evaluation (resp, ground_truth)
    if resp.answer = refusal then CO 
    else 
        s_e = SE(resp, R_derived)
        s_n = SN(resp, R_derived)
        if s_e < threadhold_e then EI 
        else if s_n < threadhold_n then EK 
        else CO 


    
\end{comment}


% \lnk{check this algo}
% \begin{algorithm}[!h]
% \caption{Response Evaluation}
% \label{alg:eval}
% \small
% \begin{algorithmic}[1]
% \Require LLM Response ($\llmResponse$), Ground Facts ($\groundTruthTriples$), Threshold ($\theta_{\m{e}}, \theta_{\m{n}}$)
% \Ensure Evaluation Result ($\eval$)
% \Function{EvaluateResponse}{$\llmResponse$, $\groundTruthTriples$, $\theta_{\m{e}}$, $\theta_{\m{n}}$}
%     % \State $hallu\_ei, hallu\_ek, hallu\_both \gets$ [] \Comment{\commentstyle{Initialization}}
%     % \State $\eval, \eval_{\m{ei}}, \eval_{\m{ek}}, \eval_{\m{co}} \gets$ [] \Comment{\commentstyle{Initialization}}
%     % \State $refuse\_to\_answer \gets$ \Call{FindRefuseToAnswer}{$\llmResponse$} \Comment{\commentstyle{Find `refuse to answer' responses}}
%     % \State $suspicious\_resps \gets$ \Call{FilterSuspiciousRes}{$\llmResponse$, $GT\_Answer$} \Comment{\commentstyle{Filter suspicious responses}}
%     % \For{$\m{resp}$ in $\llmResponse$} \Comment{\commentstyle{Iterate each response}}
%     \If{$\llmResponse.answer = \m{refusal}$}
%         \State
%         $\eval$ $\in$ $CO$ \Comment{\commentstyle{Preliminary Screening}}
%     % \State $\eval_{\m{co}}$.append($\m{resp_\m{refusal}}$) \Comment{\commentstyle{Preliminary Screening}}
%     \Else
%         \State $\deriveKG{\m{resp}}{\groundTruthTriples}{\semantic_{\m{resp}}}{\semantic_{\m{ground}}}$
%         %\Comment{\commentstyle{Extract Semantic Structure}}
%         \State $\m{s}_{\m{e}} \gets$ $\similarity_\m{e}${$(\semantic_{\m{resp}}$, $\semantic_{\m{ground}})$} \Comment{\commentstyle{Calculate edge similarity}}
%         \State $\m{s}_{\m{n}} \gets$ $\similarity_\m{n}${$(\semantic_{\m{resp}}$, $\semantic_{\m{ground}})$} \Comment{\commentstyle{Calculate node similarity}}
%         % \If{$edge\_sim < \theta\_e$ and $node\_sim < \theta\_n$}
%             % \State $\eval$.append($response$) \Comment{\commentstyle{Append mixed hallucination}}
%         \If{$\m{s}_{\m{e}} < \theta_{\m{e}}$}
%             \State 
%             $\eval$ $\in$ $EI$  \Comment{\commentstyle{Append error inference hallucination}}
%         \ElsIf{$\m{s}_{\m{n}} < \theta_{\m{n}}$}
%             \State 
%             $\eval$ $\in$ $EK$  \Comment{\commentstyle{Append error knowledge hallucination}}
%         \Else
%             \State
%             $\eval$ $\in$ $CO$  
%             \Comment{\commentstyle{Append correct response}}
%         \EndIf
%     \EndIf
%     % \EndFor
%     % \State $\eval$.extend($\eval_{\m{ei}}, \eval_{\m{ek}}, \eval_{\m{co}}$) \Comment{\commentstyle{Merge the result}}
%     % \State $evaluation\_result \gets$ \Call{GenerateResult}{$hallu\_both$, $hallu\_ei$, $hallu\_ek$}
%     % \shil{(we use three lists \_both, \_ei, and \_ek)} $contradictory\_answers$} 
%     % \Comment{Generate evaluation result}
%     \State \Return $\eval$ \Comment{\commentstyle{Return the evaluation result}}
% \EndFunction
% \end{algorithmic}
% \end{algorithm}
\begin{algorithm}[!b]
\caption{Response Evaluation}
\label{alg:eval}
\small
\begin{algorithmic}[1]
\Require LLM Response ($\llmResponse$), Ground Facts ($\groundTruthTriples$), Threshold ($\theta_{\m{e}}, \theta_{\m{n}}$)
\Ensure Evaluation Result Category~($CO, EK, EI, OL$)
\Function{EvaluateResponse}{$\llmResponse$, $\groundTruthTriples$, $\theta_{\m{e}}$, $\theta_{\m{n}}$}
    % \State $hallu\_ei, hallu\_ek, hallu\_both \gets$ [] \Comment{\commentstyle{Initialization}}
    % \State $\eval, \eval_{\m{ei}}, \eval_{\m{ek}}, \eval_{\m{co}} \gets$ [] \Comment{\commentstyle{Initialization}}
    % \State $refuse\_to\_answer \gets$ \Call{FindRefuseToAnswer}{$\llmResponse$} \Comment{\commentstyle{Find `refuse to answer' responses}}
    % \State $suspicious\_resps \gets$ \Call{FilterSuspiciousRes}{$\llmResponse$, $GT\_Answer$} \Comment{\commentstyle{Filter suspicious responses}}
    % \For{$\m{resp}$ in $\llmResponse$} \Comment{\commentstyle{Iterate each response}}
    \State $CO, EK, EI, OL \gets []$ \Comment{\commentstyle{Initialization}}
    \If{$\llmResponse.answer = refusal$}
        \State
        $CO.\m{append}(\llmResponse)$ \Comment{\commentstyle{Preliminary Screening}}
    % \State $\eval_{\m{co}}$.append($\m{resp_\m{refusal}}$) \Comment{\commentstyle{Preliminary Screening}}
    \Else
        \State $\widetilde{\m{Trpl}} \gets$ \Call{ExtractTriple}{$\m{Resp.reasoning}$} 
        % \Comment{\commentstyle{Extract useful triples}}
        \State $\semantic_{\m{resp}}, \semantic_{\m{ground}} \gets$ \Call{BuildGraph}{$\widetilde{\m{Trpl}}, \groundTruthTriples$} 
        % \Comment{\commentstyle{Build semantic structure}}
        % \State $\deriveKG{\m{Resp}}{\rall}{\semantic_{\m{resp}}}{\semantic_{\m{ground}}}$\lnk{More specific} \Comment{\commentstyle{Extract Semantic Structure}}
        \State $\m{s}_{\m{e}} \gets$ $\similarity_\m{e}${$(\semantic_{\m{resp}}$, $\semantic_{\m{ground}})$} \Comment{\commentstyle{Calculate edge similarity}}
        \State $\m{s}_{\m{n}} \gets$ $\similarity_\m{n}${$(\semantic_{\m{resp}}$, $\semantic_{\m{ground}})$} \Comment{\commentstyle{Calculate node similarity}}
        % \If{$edge\_sim < \theta\_e$ and $node\_sim < \theta\_n$}
            % \State $\eval$.append($response$) \Comment{\commentstyle{Append mixed hallucination}}
        \If {$s_e < \theta_{e}$ and $s_n < \theta_{n}$}
            \State
            $OL.\m{append}(\llmResponse)$  \Comment{\commentstyle{Append  overlapped cases}}
        \ElsIf{$\m{s}_{\m{e}} < \theta_{\m{e}}$}
            \State 
            $EI.\m{append}(\llmResponse)$  \Comment{\commentstyle{Append error inference}}
        \ElsIf{$\m{s}_{\m{n}} < \theta_{\m{n}}$}
            \State 
            $EK.\m{append}(\llmResponse)$  \Comment{\commentstyle{Append error knowledge}}
        \Else
            \State
            $CO.\m{append}(\llmResponse)$
            \Comment{\commentstyle{Append correct response}}
        \EndIf
    \EndIf
    % \EndFor
    % \State $\eval$.extend($\eval_{\m{ei}}, \eval_{\m{ek}}, \eval_{\m{co}}$) \Comment{\commentstyle{Merge the result}}
    % \State $evaluation\_result \gets$ \Call{GenerateResult}{$hallu\_both$, $hallu\_ei$, $hallu\_ek$}
    % \shil{(we use three lists \_both, \_ei, and \_ek)} $contradictory\_answers$} 
    % \Comment{Generate evaluation result}
    \State \Return $CO, EK, EI, OL$ 
    % \Comment{\commentstyle{Return the result}}
\EndFunction
\end{algorithmic}
\end{algorithm}

\textbf{Edge Vector Metamorphic Oracle ($MO_E$)}: This oracle is based on the similarity of edge vectors between $\semantic_{\m{resp}}$ and $\semantic_{\m{ground}}$. If the vector similarity ($\m{s}_{\m{e}}$) between the edges in the $\semantic_{\m{resp}}$ and those in $\semantic_{\m{ground}}$ falls below a predetermined threshold $\theta_{\m{e}}$, it indicates that the LLM's answer significantly diverges from the ground truth. This suggests the presence of an FCH, and vice versa. %Conversely, the LLM's answer is considered to {align with} the ground truth. % indicates the correct answer. Otherwise, it detects an occurrence of FCH.
More specifically, we utilize \emph{Jaccard Similarity}~\cite{J_S} to calculate the similarity score between edge vectors extracted from $\semantic_{\m{resp}}$ and  $\semantic_{\m{ground}}$. 
$$
\similarity_{\m{e}}(\semantic_{\m{resp}}, \semantic_{\m{ground}}) = \frac{|\widetilde{E}_{\m{resp}} \cap \widetilde{E}_{\m{ground}}|}{|\widetilde{E}_{\m{resp}} \cup \widetilde{E}_{\m{ground}}|}, $$check if $  \similarity_{\m{e}}(\semantic_{\m{resp}}, \semantic_{\m{ground}})  < \theta_{\m{e}} \ 
$~
where $\widetilde{E}_{\m{resp}}$ and $\widetilde{E}_{\m{ground}}$ denote the set of edges extracted from $\semantic_{\m{resp}}$ and $\semantic_{\m{ground}}$. 
% , and \( \theta_E \) is a predefined threshold~(to be detailed in Section~\ref{sec:ex_setup}). 
% Intuitively, the similarity score is calculated as the proportion of identical edges shared between the two sets against the total number of unique edges in both sets. If the similarity score is smaller than the threshold, then an FCH is detected. Note that when determining the joint and union of sets $E_{LLM}$ and $E_{GT}$, we consider two edges as identical if their corresponding relations are identical or represented by synonymous words, and vice versa.  

% Define a function \( \text{Sim}_E(KG_{LLM}, KG_{GT}) \) to calculate the similarity of edge vectors between the knowledge graph generated by the language model, \( KG_{LLM} \), and the ground truth knowledge graph, \( KG_{GT} \).
% $$\text{Sim}_E(KG_{LLM}, KG_{GT}) = \frac{|E_{\text{LLM}} \cap E_{\text{GT}}|}{|E_{\text{LLM}} \cup E_{\text{GT}}|}$$

% If \( \text{Sim}_E(KG_{LLM}, KG_{GT}) < \theta_E \), where \( \theta_E \) is a predefined threshold, then an error inference hallucination is identified.

   
\textbf{Node Vector Metamorphic Oracle ($MO_N$)}: This relation examines the similarity of node vectors between $\semantic_{\m{resp}}$ and $\semantic_{\m{ground}}$. 
Defined in a similar manner as $MO_E$, if the node similarity between the nodes ($\m{s}_{\m{n}}$) in the $\semantic_{\m{resp}}$ and those in $\semantic_{\m{ground}}$ falls below a predetermined threshold $\theta_{\m{n}}$, it indicates that the LLM's answer significantly diverges from the ground truth, and vice versa.
$MO_N$ can be captured by the Jaccard Similarity, defined as follows:
%When the similarity between the nodes in the $KG_{LLM}$ and those in $KG_{GT}$ is below a predetermined threshold, this metamorphic relation exposes an error knowledge hallucination.
%Define a function \( \text{Sim}_N(KG_{LLM}, KG_{GT}) \) to measure the similarity of node vectors between \( KG_{LLM} \) and \( KG_{GT} \).

$$\similarity_{\m{n}}(\semantic_{\m{resp}}, \semantic_{\m{ground}}) = \frac{|N_{\m{resp}} \cap N_{\m{ground}}|}{|N_{\m{resp}} \cup N_{\m{ground}}|}, $$check if $
\similarity_{\m{n}}(\semantic_{\m{resp}}, \semantic_{\m{ground}})  < \theta_{\m{n}}  
$~
where $N_{\m{resp}}$ and $N_{\m{ground}}$ denotes the set of nodes extracted from $\semantic_{\m{resp}}$ and $\semantic_{\m{ground}}$.
% , and \( \theta_N \) is a predefined threshold~(to be detailed in Section~\ref{sec:ex_setup}). 
% Intuitively, the similarity score is calculated as the proportion of identical edges/nodes shared between the two sets against the total number of unique edges/nodes in both sets. If the similarity score is smaller than the threshold, then an FCH is detected. 
Note that when determining the joint and union of the edges/nodes sets, we consider two edges/nodes as identical if their corresponding entities are identical or synonymous, and vice versa.
%If \( \text{Sim}_N(KG_{LLM}, KG_{GT}) < \theta_N \), where \( \theta_N \) is a predetermined threshold, then an error knowledge hallucination is recognized.


% \textbf{Answer Consistency Metamorphic Oracle ($MO_C$)}: This relation is distinct in that it focuses on the consistency or inconsistency of the model's final answer with the ground truth, regardless of whether the node or edge vector similarities meet the thresholds. This relation helps identify situations where, despite a seemingly reasonable reasoning process, the outcome contradicts the facts (or vice versa), indicating contradictory answers.

% Consider the final answer \( Ans_{LLM} \) provided by LLMs and the ground truth answer \( Ans_{GT} \).
% If the similarity between \( KG_{LLM} \) and \( KG_{GT} \) is above or below the threshold but there exists a contradiction or consistency between \( Ans_{LLM} \) and \( Ans_{GT} \), this scenario is considered as a contradictory answer.
% $$ \text{If similarity between } KG_{LLM} \text{ and } KG_{GT} \text{ is above or below threshold and } A_{LLM} \neq A_{GT} \text{, then identify a contradictory answer.} $$


% \subsection{Feedback Loop}
% Based on the evaluation results from the preceding section, this module is employed to select test case types that trigger higher levels of FCHs and to mutate them for more hallucination answers, thereby enhancing the ability of the testing process to expose LLM FCHs.

