\section{Introduction}
% Large Language Models~(LLMs) represent a transformative advancement in the field of language processing, demonstrating an unparalleled capacity for text generation and comprehension, which can be further applied in a wide variety of applications.  
% %Large language models (LLMs) have risen to prominence in various fields, offering endless possibilities for artificial intelligence applications. 
% Despite their significant prevalence in recent years, LLMs are frequently challenged with security and privacy issues, such as poor explainability~\cite{}, poor robustness~\cite{}, data dependency~\cite{}, etc. Among them, a specific and notable concern that has garnered increasing attention is the phenomenon of `hallucination', where models generate plausible but factually inaccurate or irrelevant content when employed for specific tasks such as problem-solving.  
% %In particular, the hallucination issue is when these large models are employed for problem-solving, users frequently voice concerns regarding being misled or deceived by the models' nonsensical and erratic outputs. 
% The tendency of these models to produce inaccurate outputs and fabricate facts has severely undermined the safety and usability of LLM applications, which calls for immediate attention in LLM research. 
% %Hallucination in large language models (LLMs) is a critical issue that needs immediate attention in LLM research. The tendency of these models to produce inaccurate outputs and fabricate facts has severely undermined the safety and usability of LLM applications. 
%exceptional 
%including limited explainability, compromised robustness, and a heavy reliance on data, each 
%However, d
Large Language Models (LLMs) have revolutionized language processing, demonstrating impressive text generation and comprehension capabilities with diverse applications. However, despite their growing use, LLMs face significant security and privacy challenges~\cite{siddiq2023generate, hou2023large, kaddour2023challenges, li2024model, 10.1145/3691620.3695510}, which affect their overall effectiveness and reliability. A critical issue is the phenomenon of \emph{hallucination}, where LLMs generate outputs that are coherent but factually incorrect or irrelevant. This tendency to produce misleading information compromises the safety and usability of LLM-based systems. This paper focuses on \emph{fact-conflicting hallucina}tion (FCH), the most prominent form of hallucination in LLMs. FCH occurs when LLMs generate content that directly contradicts established facts. For instance, as illustrated in \figref{fig:example1}, an LLM incorrectly asserts that ``\emph{Haruki Murakami won the Nobel Prize in Literature in 2016}'', whereas the fact is that ``\emph{Haruki Murakami has not won the Nobel Prize, though he has received numerous other literary awards}''. 
Such inaccuracies can significantly lead to user confusion and undermine the trust and reliability that are crucial for LLM applications.

% Large Language Models~(LLMs) have brought transformative advancements to language processing and beyond, showcasing text generation and comprehension abilities with wide-ranging applications. 
% Despite the increasing prevalence, LLMs face critical challenges in security and privacy aspects~\cite{siddiq2023generate, hou2023large, kaddour2023challenges}, heavily impacting their effectiveness and reliability. 
% One notable issue is the phenomenon of \emph{hallucination}, where LLMs produce coherent but factually inaccurate or irrelevant outputs during problem-solving. 
% Such a tendency to generate misleading information jeopardizes the safety and usability of LLM-based applications. 
% This paper concerns the \emph{fact-conflicting hallucination}~(FCH), which is the primary form of hallucinations in LLMs. 
% FCH occurs when LLMs generate content that directly contradicts the well-established facts, as exemplified in \figref{fig:example1}, where an LLM incorrectly believes 
% ``\emph{Haruki Murakami won the Nobel Prize in Literature in 2016}'', deviating from the fact that ``\emph{Haruki Murakami has not won the Nobel Prize but other numerous awards for his work in Literature}''. Such misinformation can cause significant user confusion and undermine the trust and reliability that are essential in various LLM applications. 

%correct answer of 

%is manifested by
%Such misinformation dissemination leads to significant user confusion, eroding the trust and reliability that are crucial in various LLM applications. 

%Large Language Models~(LLMs) represent a transformative advancement in the field of language processing, demonstrating an unparalleled capacity for text generation and comprehension, which can be further applied in a wide variety of applications. Despite their growing prevalence, LLMs encounter critical challenges, particularly in aspects of security and privacy. These include concerns such as limited explainability~\cite{}, compromised robustness~\cite{}, and heavy reliance on data~\cite{}, each posing distinct challenges to their efficacy and reliability. Among these, the phenomenon of ``hallucination'' stands out as a notable concern. This occurs when LLMs, while employed in tasks like problem-solving, generate outputs that are coherent yet factually inaccurate or irrelevant. Such a tendency to produce misleading information not only compromises the safety of LLM applications but also raises urgent questions regarding their usability. 

% Hallucinations in LLMs manifest in several distinct forms, each contributing differently to the challenges identified in their growing applications. 
% %The first, known as ``Input-conflicting hallucination'', arises when there is a discrepancy between the model's output and the user's initial input, reflecting a potential misunderstanding of the task at hand. On the other hand, ``Context-conflicting hallucination'' represents the second type, occurring when LLMs produce inconsistent responses in prolonged or multi-turn interactions, indicative of their limitations in maintaining coherent context. 
% Among the three types categorized in the literature~\cite{huang2023survey,zhang2023hallucination}, ``Fact-conflicting hallucination~(FCH)'' poses a particularly serious concern which is the primary focus of this paper. This phenomenon generates content in direct opposition to established factual knowledge. As illustrated in the example shown in Figure~\ref{fig:example1}, when an LLM was asked about the first person to walk on the moon, it incorrectly answered ``Charles Lindbergh in 1951'', a clear deviation from the factual answer of Neil Armstrong in 1969. This type of hallucination can lead to the dissemination of incorrect information and cause significant confusion among users, undermining the trust and reliability critical in various LLM applications. %Addressing fact-conflicting hallucinations is therefore essential for the advancement of LLMs, ensuring they not only function effectively but also responsibly in their diverse roles.


% According to \cite{huang2023survey} and \cite{zhang2023hallucination}, hallucinations in large language models can be categorized into types such as factual hallucinations and contextual hallucinations. Current benchmark assessments tend to focus on evaluating the propensity of LLMs to generate erroneous facts. The origin of these issues can be traced back to multiple deficiencies, including flaws in training data, training algorithms, and the inference process.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{fig/example1-cropped.pdf}\\
%     \caption{A Hallucination Output Example.}
%     %\vspace{-0.5cm}
%     \label{fig:example1}
% \end{figure}

\begin{figure}[t]
\centering
\vspace{3mm}
\hspace{-3mm}
\includegraphics[width=\linewidth]{fig/drowzee-example.pdf}
\\[0.5em]
\caption{A Hallucination Output Example}
\label{fig:example1}
\vspace{-4mm}
\end{figure}
%\lnk{Factual Hallucination and LLM inference current status}

Recent studies have introduced various methods to detect LLM hallucinations. A common approach involves developing specialized benchmarks, such as TruthfulQA~\cite{lin-etal-2022-truthfulqa}, HaluEval~\cite{HaluEval}, and KoLA~\cite{yu2023kola}, to assess hallucinations in tasks like question-answering, summarization, and knowledge graphs. 
While manually labeled datasets provide valuable insights, current methods often rely on simplistic or semi-automated techniques such as string matching, manual validation, or verification through another language model. These approaches reveal significant gaps in automatically and effectively detecting fact-conflicting hallucinations (FCH). 
The primary challenges in FCH detection arise from the lack of dedicated ground truth datasets, the absence of comprehensive test cases designed to trigger FCH, and the lack of a robust testing framework.  
Unlike other types of hallucinations, such as input-conflicting or context-conflicting hallucinations~\cite{ji-etal-2023-rho, shi2023large}, which can often be identified through semantic consistency checks, detecting FCH requires the verification of factual accuracy against external knowledge sources/databases. This process is particularly challenging and resource-intensive, especially for tasks that involve complex logical relationships~\cite{zhang2024fusion}. We identify three primary challenges in addressing this research gap:


% Recent studies have introduced a range of methods for detecting 
% hallucinations. One common approach involves creating comprehensive benchmarks tailored for this purpose. 
% Datasets such as TruthfulQA~\cite{lin-etal-2022-truthfulqa}, HaluEval~\cite{HaluEval}, and KoLA~\cite{yu2023kola} have been designed to evaluate hallucinations across different contexts, including question-answering, summarization, and knowledge graphs. 
% Despite the value of these manually labeled datasets, the current techniques heavily rely on naive and semi-automatic methods, such as string matching, manual validation, or utilizing another LLM for confirmation. 
% Therefore, there is a gap 
% in automatically and effectively testing FCHs, and the primary obstacle in testing FCH is the absence of dedicated ground truth datasets and an extensive testing framework.  
% Unlike other types of hallucinations, e.g., input-conflicting or context-conflicting 
% \cite{ji-etal-2023-rho, shi2023large}, 
% which can be identified through checks for semantic consistency, 
% detecting FCH
% requires the verification of the content's factual accuracy against external sources of knowledge or databases. This makes the process particularly arduous and resource-intensive, especially for tasks processing content with complex logical connections. 
% Here, we highlight three concrete challenges in filling up the identified research gap: 




%The main obstacle in testing for FCH is the absence of dedicated ground truth datasets and specific testing frameworks. Unlike other types of hallucinations~(e.g., input-conflicting and context-conflicting hallucinations, to be detailed in Section~\ref{subsec:cat}) which can be identified through checks for semantic consistency, FCH demands the verification of the content's factual accuracy against external sources of knowledge or databases. This requirement makes the process particularly challenging and resource-intensive, especially for tasks processing contents with inherent logical connections.

% \shil{(I feel the transition is not smooth, we first introducing datasets, and not explaining how they use these datasets to test llm. after these, we can state these methods are not automatic.)}


% To tackle FCH, recent works have developed various techniques for testing and detecting hallucination~\citep{yu2023kola,HaluEval}. The typical and intuitive solution is to develop comprehensive benchmarks for detection. This is done through a process of sampling, filtering, and enhancing ground-truth answers to identify the best and correct answers from given candidates. For example, a well-known hallucination evaluation benchmark HaluEval~\cite{HaluEval} constructs scenarios where LLMs are tested on their ability to select the most factually accurate answers from a set of provided options, with a focus on filtering out hallucinated responses. %\yi{ also talk about the construction of benchmark?}
% Additionally, human annotation plays a critical role in identifying hallucinations in LLM outputs~\cite{Alpaca}. This involves humans determining whether responses contain hallucinated information and considering aspects such as unverifiability, non-factuality, and irrelevance. 



% \lnk{Key challenge: lack of hallucination testing when faced with logic reasoning related problems}
%Bridging the identified research gap in the literature necessitates exploring the inherent challenges faced in detecting FCHs, which are crucial for advancing and enhancing the reliability of LLMs. 

\begin{enumerate}[itemsep=1mm, wide,  labelindent=9pt]
%[itemsep=0ex,leftmargin=0.35cm]
%Challenge\#1: 
%While these benchmarks effectively detect certain hallucinations, they 
\item {\textbf{Automatically constructing and updating benchmark datasets.}} Existing methodologies mainly rely on manually curated benchmarks for detecting specific hallucinations, which fail to encompass the broad and dynamic spectrum of fact-conflicting scenarios in LLMs. 
Meanwhile, due to the ever-evolving nature of knowledge, the need for frequent updates to benchmark data imposes a substantial and continuous maintenance effort.
The reliance on benchmark datasets thus restricts the FCH detection techniques' adaptability, scalability, and  %more importantly, 
detection capability;  
%Challenge\#2:
% in existing test cases. 
\item {\textbf{Efficiently generating FCH test cases.}}
LLMs often answer correctly to simple, straightforward questions due to their extensive training on vast datasets. However, to effectively assess their reasoning capabilities, it is important to generate more complex questions, such as those involving intricate temporal characteristics, that require reasoning rather than just recalling facts. However, constructing such test cases is non-trivial. The challenge lies in designing questions that use familiar knowledge but involve reasoning patterns the LLM may not have been explicitly trained on. Creating such test cases efficiently while ensuring they probe reasoning skills in ways the model has not previously encountered is essential to uncovering latent hallucinations;
% queries that involve temporal concepts, such as ``\emph{Does the human population finally reach six billion by the year 2000?}'' may often be used in applications. However, the correctness of the LLM outputs cannot be guaranteed, potentially leading to misleading information. Currently, there are no satisfactory approaches to automatically verify LLM outputs in such test cases; 
%errors even before the occurrence of large model hallucinations; 
%However, it is known that 
%Another critical issue lies in the verification of temporal logic in existing test cases. 
%It is well known that test cases involving temporal-related questions often face difficulties in automatically verifying the soundness and completeness of these issues. Consequently, the correctness of these test cases cannot be guaranteed, potentially introducing errors even before the occurrence of large model hallucinations;
%Challenge\#3: 
\item {\textbf{Validating the reasoning steps from LLM outputs.}} Even when LLMs finally produce correct answers, the outputs may not indicate an accurate reasoning process, potentially masking false understanding -- a source of FCH. Additionally, the quality of manual validation can differ based on human expertise. As a result, automatically validating reasoning processes, particularly those involving complex logical relationships, is inherently challenging. 
\vspace{1mm}
\end{enumerate}







% \lnk{Key challenge: factual knowledge exploring and new facts generation}
%\yi{we should focus on testing, addressing is a little bit vague.}
% The current research landscape in LLM presents a critical gap in automatically testing FCHs. Predominantly, existing methodologies are anchored to manual benchmarks. %\yi{this sentence is quite chinglish.}
% While these benchmarks are effective in detecting certain types of hallucinations, such as those in Figure~\ref{fig:example1}, they fall short in encompassing the broad and dynamic spectrum of fact-conflicting scenarios inherent to LLMs. %\yi{again, this sentence is not very clear}
% Meanwhile, the need for frequent updates to benchmark data, due to the ever-evolving nature of knowledge, imposes a significant and continuous maintenance effort.
% The reliance on benchmark datasets thus restricts the detection techniques’ adaptability, scalability, and worse, detection capability. 
% From a second perspective, the consistency in the quality of benchmark questions can vary, reflecting the differing levels of experience and skill among the human experts responsible for creating them. This is particularly reflected in the stages such as data labeling and results validation. Additionally, it is important to acknowledge that humans are prone to errors.
% %the scalability and the deof these existing methods are also significantly challenged by their dependency on extensive human intervention, particularly in stages such as data labeling and results validation. %This heavy reliance on manual efforts not only limits the scalability of such approaches but also questions their feasibility in efficiently handling the extensive and intricate datasets characteristic of LLMs.
% Thus, the development of more autonomous, agile, and scalable testing techniques is imperative to effectively identify and tackle FCHs in LLMs.%\yi{in this paper, we focus on testing, but until this paragraph, no terms about ``testing'' explicitly occur.}

% \lnk{Solution to Challenge1: comprehensive logic reasoning based testing framework}

% \lnk{Solution to Challenge2: wiki factual knowledge extraction and prolog rules inference for scalability.}
% \lnk{Key challenge: }

%\textbf{Our Work.}
%To address limitations in the existing techniques, 
%we are the first, to the best of our knowledge, to introduce 
To address the problems outlined above, this paper presents a novel automatic end-to-end metamorphic testing technique based on temporal logic for detecting FCH. To the best of our knowledge, we are the first to create a comprehensive FCH testing framework that utilizes factual knowledge reasoning and metamorphic testing, all seamlessly integrated into the fully automated tool, \tool. 

%\shil{(which four methods?)}
\tool begins by establishing a comprehensive factual knowledge base sourced through crawling information from accessible knowledge bases such as Wikipedia. Each piece of this knowledge acts as a ``seed'' for subsequent transformations. Leveraging logical operators to automatically generate temporal reasoning rules, we transform and augment these seeds and expand factual knowledge into a well-established set of question-answer pairs.
%\yi{into xx}. 
Using the questions and answers in the knowledge set as test cases and ground truth, respectively, we construct a reliable and robust FCH testing benchmark. 


The experiment uses a series of carefully designed template-based prompts to test for FCHs in LLMs. To thoroughly evaluate the reasoning behind the responses, we instruct the LLMs not only to generate answers to the test cases but also to provide detailed justifications for their answers. To reliably identify FCH, we introduce two semantic-aware, similarity-based metamorphic oracles. These oracles extract the key semantic elements from each sentence and map out the logical relationships between them. By comparing the logical and semantic structures of the LLM's responses with the ground truth, the oracles can detect FCH by identifying significant deviations in the LLM's answers from the correct information.




%well-crafted prompts\yi{how prompts generated?} to engage LLMs, testing the alignment of their generated content with our enhanced ground truth. Disparities between LLM outputs and the ground truth signal potential hallucinations. 
%Additionally, in our commitment to fostering collaborative research, we have released our constructed dataset as a benchmark~\cite{drowzee}.

%Our approach directly addresses the need for a comprehensive and flexible testing method by transforming structural factual data into a diverse range of scenarios that LLMs may encounter. This method not only improves the reliability of detection but also enhances its adaptability to various factual contexts.
%Furthermore, we address the scalability challenge by automating the transformation and enlargement of our knowledge base, significantly reducing the dependency on human effort. The well-designed prompts used to test LLMs further streamline the process, making it more efficient in identifying potential hallucinations by comparing LLM outputs with our extended ground truth.

%\textbf{Results and Findings.}
%In evaluating our proposed FCH testing framework and \tool, 
%we undertake 
%to evaluate their effectiveness 
We demonstrate the effectiveness of our approach through comprehensive experiments in multiple contexts. First, our evaluation involves deploying \tool across a wide range of topics drawn from a diverse selection of Wikipedia articles. Second, we test our framework on various open-source and commercial LLMs, thoroughly assessing its applicability and performance across different model architectures. 
Our key findings indicate that \tool succeeds in automatically generating practical test cases and identifying hallucination issues of nine LLMs across nine domains. 
Using these test sets, our experiments show that the rate of hallucination responses produced by various LLMs ranges from 24.7\% to 59.8\% for cases unrelated to temporal reasoning and 16.7\% to 39.2\% for cases requiring temporal reasoning. 
%\shil{shall we differentiate the number for non-temporal and temporal one?}.  
We then categorize these hallucination responses into \emph{erroneous knowledge hallucination} and \emph{erroneous inference hallucination}. 
%\syh{four types?}. 
Through an in-depth analysis, we unveil that the lack of logical reasoning capabilities contributes the most to the FCH issues in LLMs. 
Additionally, we observe that LLMs are particularly prone to generating hallucinations in test cases involving temporal concepts and out-of-distribution knowledge. 
Such an evaluation demonstrates that the 
%Furthermore, we confirm that 
test cases generated using %our 
logical reasoning rules can effectively trigger and detect LLM hallucinations.  %issues in . 


This paper builds upon the earlier version~\cite{DBLP:journals/pacmpl/LiL0SW024} by incorporating hallucination detection through temporal-logic-guided test generation. It includes additional motivational examples (\secref{sec:motivating}), a comprehensive set of reasoning rules for encoding \emph{Metric Temporal Logic} (MTL)~\cite{DBLP:conf/lics/OuaknineW05} formulae (\secref{sec:encoding_MTL}) and automatically generating temporal-logic-related question-answer pairs (\secref{prompt}), and further experimental studies (the {RQ4} at \secref{sec:eval}) that detect hallucinations due to insufficient temporal reasoning capabilities. The main contributions of this work are summarized as follows: 
%We summarize the main contributions of this paper below:
\begin{itemize}[itemsep=1mm,leftmargin=0.35cm]
\item 
%Development of 
\textbf{A novel FCH testing framework.} 
To the best of our knowledge, 
we are the first to develop a novel testing framework based on logic programming and metamorphic testing to automatically detect FCH issues in LLMs. %\yi{hanging sentence}This framework represents a significant advancement over current methodologies, providing a more systematic, comprehensive approach to detection.
%Construction and Release of
\item \textbf{An extensive benchmark based on factual knowledge.} 
To facilitate collaborative efforts and future advances in identifying FCH, 
the source code of \tool and constructed benchmark dataset are publicly available  \cite{drowzee}. 
\item \textbf{Test generation via temporal reasoning.} 
Our tool automatically generates test cases that provide a more comprehensive evaluation of LLMs in handling reasoning tasks and identifying factual inconsistencies. By applying temporal logic-based reasoning rules, we expand the initial seed data from our knowledge base, enhancing the diversity and complexity of the test scenarios. 

\item \textbf{Semantic-aware oracles for LLM answer validation.} We propose and implement two automated verification mechanisms, i.e., the oracles, that analyze the semantic structure similarity between sentences. These oracles are designed to validate the reasoning logic behind the answers generated by LLMs, hereby reliably detecting the occurrence of FCHs. 

\end{itemize}


