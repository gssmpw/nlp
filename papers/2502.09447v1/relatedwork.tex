\section{Related Work}
\vspace{-5pt}
\subsection{Datasets for Reasoning Segmentation}
\vspace{-5pt}
Current reasoning segmentation datasets \cite{lai2023lisa,rasheed2024glamm,yuan2024osprey} mainly focus on region-level segmentation and single-step reasoning, which fail to meet the comprehensive requirements of the pixel-level RS task.
Traditional region-level segmentation datasets \cite{yu2016modeling,krishna2017visual} rely on simple and explicit instructions, lacking complex user intents understanding. To bridge this gap, ReasonSeg \cite{lai2023lisa} is the first to propose a segmentation dataset based on complex queries, which is a small scale and does not support multi-turn interactions. While recent datasets \cite{yuan2024osprey,rasheed2024glamm,ren2024pixellm} have expanded in size and diversity, they remain focused on multi-object segmentation and provide limitations for multi-step reasoning and fine-grained grounding. In contrast, our PRIST dataset utilizes conversation to simulate a human-like multi-step reasoning process, which innovatively combines multi-turn reasoning with pixel-level segmentation. %PRIST provides a new solution for reasoning segmentation tasks. 
Detailed dataset comparisons are shown in Table \ref{tab:compare_benchmark} in Appendix \ref{dataappendix}.
\vspace{-5pt}
\subsection{MLLMs for Region-level Segmentation} 
\vspace{-5pt}
MLLMs have advanced vision-language perception tasks, with recent works 
\cite{peng2023kosmos,you2023ferret,zhang2023gpt4roi} focusing on image-level visual dialogue. Some \cite{chen2023shikra,peng2023kosmos,pi2023detgpt} achieve region-level understanding by incorporating positional information and boundary boxes, mainly relying on LLMs for region interpretation. Several models \cite{lai2023lisa,ren2024pixellm,rasheed2024glamm,zhang2024omg} integrate segmentation-specific modules with LLMs for end-to-end training, enabling a more comprehensive understanding of regions. While these methods address pixel-level grounding, they still face limitations in complex reasoning. More comparison between models in Appendix \ref{sec:compare}. Our proposed MIRAS overcomes these challenges by enhancing segmentation accuracy through interactive reasoning, progressively refining the boundaries. 
\vspace{-2pt}