% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[preprint]{acl} 
%\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{array}
\usepackage{xcolor}
\usepackage{float}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{soul}

\usepackage{booktabs}  % For professional table rules
\usepackage{tabularx}  % For flexible-width tables
\usepackage{listings}  % For inline code formatting
\usepackage{array}     % For better column customization
\usepackage{comment}
\usepackage{enumitem}
%\setlist[itemize]{leftmargin=0pt, labelindent=0pt}

\begin{document}

%\title{SAL-SQL: Self augmentation and refinement Learning in Text-to-SQL}
\title{SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL }


\author{Jimin Lee, ~Ingeol Baek, ~Byeongjeong Kim, ~Hwanhee Lee\thanks{Corresponding author.}\\
{Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea} \\
\texttt{\{ljm1690, ingeolbaek, michael97k, hwanheelee\}@cau.ac.kr} \\
}


\input{0_abstract}
\input{1_introduction}
\input{2_related_work}
\input{3_method}
\input{4_experiment}
\input{5_ablation_study}
\input{6_case_study}
\input{7_conclusion}
\input{8_limitations}

\section*{Acknowledgement}
This research was supported by Institute for Information \& Communications Technology Planning \& Evaluation (IITP) through the Korea government (MSIT) under Grant No. 2021-0-01341 (Artificial Intelligence Graduate School Program (Chung-Ang University)).
%\bibliographystyle{plain}
\bibliography{10_ref}
\input{9_Appendix}


\end{document}
\EOD


% \maketitle
% \begin{abstract}
% Text-to-SQL is the task of transforming natural language questions into executable SQL queries, enabling seamless database interaction. Existing approaches, such as skeleton-masked selection, rely on retrieving similar examples from the training set to guide query generation. However, such methods struggle when no similar examples exist in the training data, which is a common challenge in real-world scenarios. To address this limitation, we propose SAL-SQL, a novel approach that enables large language models (LLMs) to self-correct and refine their SQL generation process without relying on example similarity. SAL-SQL incorporates predefined error warnings and self-teaching signals, guiding LLMs to avoid common pitfalls in Text-to-SQL tasks. Our approach improves model robustness and accuracy, particularly in unseen or complex cases where traditional methods fail. 
% \end{abstract}




% \section{Introduction}


% Text-to-SQL generation, the process of translating natural language questions into SQL queries, plays a crucial role in enabling intuitive database interactions. Traditional methods, such as skeleton-masked selection, rely heavily on retrieving similar examples from training data to guide query generation. However, these methods face significant challenges in real-world scenarios where similar examples are often unavailable in the training set.
% To overcome these limitations, we introduce SAL-SQL, an approach that leverages the generative capabilities of large language models (LLMs) to create synthetic examples, consisting of SQL queries, their corresponding natural language questions, and reasoning paths. Unlike traditional methods, It enables the model to generate and learn from its own examples, guided by predefined error warnings and self-teaching signals. This mechanism allows LLMs to iteratively improve their inference capabilities, enhancing both robustness and accuracy. By relying on LLM-generated examples, our method demonstrates superior performance, particularly in complex or unseen scenarios where traditional retrieval-based approaches fail.

% \begin{figure}[t]
% \centerline{\includegraphics[scale=0.3]{Pictures/ss1.png}}
% \caption{Our proposed SQL example generation flowchart.}
% \vspace{-5mm}
% \end{figure}

% \begin{figure*}[!ht]
% \centerline{\includegraphics[scale=0.2]{Pictures/ss.png}}
% \caption{Our proposed SAL-SQL flowchart.}
% \vspace{-5mm}
% \end{figure*}


% \section{Related work}

% \subsection{Rule based system}
% Early approaches to Text-to-SQL relied on rule-based systems and semantic parsing frameworks, which required extensive domain expertise and manual feature engineering. Rule-based methods involved designing syntactic and semantic rules to map natural language to SQL, often utilizing predefined templates or handcrafted grammars. Systems focusing on translating restricted natural language inputs into SQL queries. 


% \subsection{Supervised Fine-Tuning}
% Supervised Fine-Tuning enables models to learn domain-specific nuances and schema alignments, significantly improving performance on specialized tasks. Techniques such as schema linking, constraint-based decoding, and execution-guided generation have further enhanced the robustness of fine-tuned models in handling domain-specific challenges. There is a method that synthesizes text-to-SQL data from weak and strong LLMs~\cite{synthesize}. This method utilizes preference learning from the weak data from small LLMs and strong data from Large LLMs. Supervised fine-tuning in Text-to-SQL is a time-consuming task and requires an enormous amount of computational resources.



% \subsection{In-context learning}
% In-context learning has emerged as another influential method, leveraging the ability of large language models to perform text to sql task by conditioning on a few examples provided in the input prompt, without requiring explicit parameter updates.

% \subsubsection{Schema Linking}
% Schema Linking is a crucial process for learning associations between database schema elements (e.g., tables, columns, and values) and natural language questions. This technique identifies keywords or concepts in a question and links them to specific schema components in the database, clarifying which parts of the schema the question refers to. It typically employs word embeddings, edit distances, or pre-trained language models to measure the similarity between question tokens and schema elements. Schema Linking plays a vital role in improving the accuracy of Text-to-SQL models, particularly by interpreting complex references in user queries and mapping them to the correct SQL components.
% DIN-SQL~\cite{din} utilizes pre-sql with schema linking which related tables and column entities which boosts the execution accuracy compared with the traditional method.  



% \subsubsection{Skeleton masked similarity}
% AST-sql~\cite{ast} introduces using an abstract syntax tree algorithm to select similar examples.
% Skeleton Masked Similarity is an approach that emphasizes structural similarity between natural language questions and SQL queries. This method involves extracting the skeleton of a SQL query from the given question and masking unnecessary details to focus on its essential structure. By preserving key structural patterns, such as SELECT-FROM-WHERE clauses, this approach facilitates learning the correspondence between natural language expressions and SQL query elements. It moves beyond simple word-level matching to capture deeper structural correlations, which is particularly effective in handling complex SQL queries or questions with diverse linguistic expressions. This method is not applicable without a training set.  
% \subsubsection{Classification and decomposition}
% The Classification and Decomposition Method simplifies the generation of SQL queries from natural language questions by breaking down the task into sequential steps. This process begins with classifying the input question based on its structure or intent (e.g., single-table queries, multi-table joins, or nested queries). Following this, the question is decomposed into smaller subtasks, such as identifying specific clauses and resolving their individual components. PTD-SQL~\cite{ptd} decomposes and categorizes SQL questions to enhance the LLM inference. This step-wise approach allows models to address each sub-problem independently, reducing complexity and improving overall accuracy.
% By modularizing the SQL generation process, the Classification and Decomposition Method enables better handling of complex, multi-intent queries or those involving nested and hierarchical relationships. This method also enhances robustness when dealing with ambiguous questions by isolating and resolving individual components before combining them into a cohesive SQL query. However, this method is hard to apply when the SQL question is not in the predefined categories. 

% \subsubsection{Self correction}
% MAGIC~\cite{magic} introduces self-correction guidelines for in-context Text-to-SQL. Magic consists of three agents(manager, feedback, and correction agents) to self-correct the generated SQL queries. This method also requires training set to define the self-correction guideline. Furthermore, it requires more than five LLM iterations to generate one final SQL query. Text-to-SQL needs to be more simple and precise.
% \section{SAL-SQL}
% SAL-SQL introduces a novel framework for enabling large language models (LLMs) to autonomously improve SQL query generation. Unlike prior methods that depend on retrieving similar questions or predefined structures (e.g., skeleton-masked selection), SAL-SQL focuses on self-generating relevant examples and reasoning paths dynamically to guide the model toward producing correct SQL queries.


% \subsection{Self generating example}
% Given a test question and database table, SAL-SQL generates a set of three similar SQL questions and their corresponding queries. These similar examples are generated by:
% Utilizing the LLM's capability to reason about question intent and database schema.
% Adjusting entities, conditions, and structures of the test question while maintaining a comparable reasoning path.
% \subsection{Reasoning Path generation}
% For each of the generated questions and SQL queries, SAL-SQL produces a reasoning path that explains the thought process behind mapping the question to the SQL query. The reasoning path includes:
% Question Analysis: Identifying the intent, target columns, tables, and filters.
% Schema Alignment: Mapping question components to database schema elements. 
% Query Construction Steps: Breaking down the SQL query into logical steps.
% \subsection{Predifined error warning}
% SAL-SQL further improves accuracy by incorporating predefined error warnings that address common pitfalls in Text-to-SQL tasks, such as Missing clauses.
% Incorrect column or table selection.
% Logical inconsistencies (e.g., mismatched aggregations and filters).
% These warnings serve as self-learning signals, enabling the model to detect and correct errors autonomously during the generation process.

% \begin{table*}[t]
% \centering
% \begin{tabular}{llccccc}
% \toprule
% \textbf{Methods} & \textbf{Model} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Extra Hard} & \textbf{All} \\  
% \midrule
% \textit{Few-shot} & CodeX-davinci & 84.7\% & 67.3\% & 47.1\% & 26.5\% & 61.5\% \\  
% \textit{Few-shot} & GPT-4o         & 86.7\% & 73.1\% & 59.2\% & 31.9\% & 67.4\% \\  
% \textit{DIN-SQL}  & CodeX-davinci & 89.1\% & 75.6\% & 58.0\% & 38.6\% & 69.9\% \\  
% \textit{DIN-SQL}  & GPT-4o         & 91.1\% & 79.8\% & 63.9\% & 43.4\% & 74.2\% \\  
% \textit{DAIL-SQL}  & CodeX-davinci & 90.3\% & 79.6\% & 62.0\% & 38.6\% & 69.9\% \\  
% \textit{DAIL-SQL}  & GPT-4o         & 92.1\% & 80.8\% & 63.9\% & 48.4\% & 75.2\% \\  
% \textit{MAGIC-SQL}  & GPT-4o          & 93.1\% & \textbf{85.8\%} & 69.9\% & 49.4\% & 77.2\% \\  
% \textit{\textbf{ \textit{SAL-SQL}}} & GPT-4o-mini & \textbf{93.6\%} & 87.5\% & \textbf{90.08\%}& \textbf{74.7\%} & \textbf{87.4\%} \\  
% \bottomrule
% \end{tabular}
% \caption{Execution accuracy performance of different methods and models on Text-to-SQL tasks across difficulty levels.}
% \label{tab:sql_comparison}
% \end{table*}

% \subsection{Final SQL query generation}
% The LLM then examines the generated SQL examples and their reasoning paths to self-reflect and correct its initial SQL query. By analyzing patterns and logical consistencies across the generated examples, the model identifies and rectifies any errors in its own reasoning process.

% \subsection{Cross consistency}
% PET-SQL~\cite{pet} introduces cross consistency which leverages outputs from multiple models to select the final SQL query. We implement three independent models (e.g., GPT-4, Codex, and Llama3.1) to generate SQL queries for the same input question. The outputs from the three models are compared to identify the most frequent SQL query. Cross-consistency is determined by matching the generated SQL queries at the structural level rather than exact token-level matching. If two or more models produce identical or highly similar SQL queries, that query is selected as the final output. In case of conflicts or divergence, a majority-voting mechanism is applied based on logical consistency and execution outcomes.


% % Bibliography entries for the entire Anthology, followed by custom entries
% %\bibliography{anthology,custom}
% % Custom bibliography entries only

% \section{Experiment}

% \begin{table}[t]
%     \centering
%     \begin{tabular}{lcc}
%         \toprule
%         Models & EX & VES \\
%         \midrule
%         GPT-4o + SAL & 37.94 & 42.15 \\
%         w/o 3 examples & 35.21 (-2.73) & 40.03 (-1.95) \\
%         w/o R path & 37.23 (-0.71) & 41.30 (-0.85) \\
%         w/o P warning & 36.25 (-1.69) & 40.46 (-2.07) \\
%         w/o cross consist & 36.91 (-1.03) & 41.12 (-1.55) \\
%         \bottomrule
%     \end{tabular}
%     \caption{This table shows the performance with different methods}
%     \label{tab:models}
% \end{table}



% \subsection{Setting}
% For our experiments, we utilized GPT-4o as the backbone model to evaluate the performance of the Text-to-SQL task. The evaluation was conducted on the Spider Dev Dataset, which is a widely used benchmark for Text-to-SQL systems.
% The Spider dataset is a large-scale, cross-domain benchmark specifically designed to assess the generalization capabilities of Text-to-SQL models. It contains 7,000 training samples spanning 166 databases across various domains and 
% 1,034 evaluation samples (referred to as the “Dev split”) from 20 databases.
% This dataset was chosen for its diversity and ability to evaluate the model’s performance in generating SQL queries across unseen database schemas, making it a suitable testbed for assessing the generalizability and robustness of the proposed approach.The experiments were conducted under controlled conditions to ensure consistency across all evaluations.
% \subsection{Evaluation}
% Execution Accuracy(EX) measures whether the SQL query generated by the model produces the same results as the ground truth query when executed on a database. This metric is sensitive to the state and schema of the database, making it essential to maintain a consistent testing environment for reliable evaluation. Valid Efficient Score(VES) considering both validity and efficiency. 

% \subsection{Performance for different SQL difficulty level}
% In our analysis, we evaluate the efficacy of SQL-PaLM against a spectrum of SQL difficulty levels, which are categorized based on the number of SQL keywords used, the presence of nested subqueries, and the application of column selections or aggregations. The results in Table 2 highlight STL performance in comparison with the standard few-shot prompting approach using GPT-4 and CodeX-Davinci,
% as well as the advanced prompting approach DIN-SQL. Our findings reveal that
% SAL-SQL overall surpasses the other approaches across all evaluated difficulty levels.

% \section{Ablation study}
% To analyze the contribution of key components in our model, we conducted an ablation study by removing the Reasoning Path, Predefined Warning, and Cross Consistency modules individually. The results, evaluated using Execution Accuracy (EX) and VES, are summarized in the table above. The removal of the Reasoning Path resulted in a decrease in Execution Accuracy (EX) by 0.71 and VES by 0.85. This demonstrates the importance of reasoning pathways for guiding the model in generating accurate SQL queries. Without the Predefined Warning mechanism, EX dropped by 1.69 and VES decreased by 2.07. This indicates that predefined warnings help the model avoid common pitfalls or ambiguities during SQL generation, leading to improved performance. Eliminating the Cross Consistency mechanism caused EX to drop by 1.03 and VES by 1.55. Cross Consistency plays a significant role in ensuring that the generated SQL queries remain coherent and accurate across varying conditions. These results highlight that all three components—Reasoning Path, Predefined Warning, and Cross Consistency—are critical for achieving optimal performance. The full model (GPT-4o + SAL-SQL) outperforms its ablated versions, demonstrating the synergistic effect of these components in improving Execution Accuracy and VES scores.



% \section{Conclusion}
% In this paper, we propose SAL-SQL which enhances SQL execution accuracy with self-augmented examples and reasoning paths. Through extensive experiments and an ablation study, we show that critical components such as Reasoning Path, Predefined Warning, and cross-consistency contribute significantly to the overall performance. Specifically, the full model (GPT-4o + SAL) achieves state-of-the-art results, with notable improvements over ablated versions, highlighting the importance of these modules in generating accurate and semantically valid SQL queries. Our findings underscore the capability of large language models when carefully structured and enhanced, to address complex Text-to-SQL tasks. Future work will focus on further improving performance in low-resource settings, handling more complex SQL queries, and expanding evaluation to larger and more diverse real-world datasets.

