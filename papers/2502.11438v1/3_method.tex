




%\section{SAFE-SQL}
\section{Fine-grained Self-Augmentation for Text-to-SQL}
%We introduce SAFE-SQL, a novel framework that autonomously generates examples for in-context learning in the Text-to-SQL. 
We propose SAFE-SQL, a framework that automatically generates high-quality examples for in-context learning in Text-to-SQL tasks. 
%Unlike traditional methods that depend on retrieving similar questions or predefined query templates, SAFE-SQL leverages self-generated examples to enhance query generation. 
Unlike traditional methods that rely on retrieving similar questions or using predefined templates, SAFE-SQL uses LLMs to create synthetic examples tailored to the given database schema.
%filter the examples based on the threshold computed by the reasoning path and the semantic similarity.
These examples are then filtered based on their semantic similarity, structural similarity, and the quality of their reasoning paths.
Finally, we predict final SQL query for the test input using the self-generated examples via in-context learning.

\subsection{Schema Linking}

The first step in SAFE-SQL is schema linking, which identifies and extracts relevant schema elements from the database to reduce noise and improve performance in Text-to-SQL tasks~\cite{robust_schema_linking}.
As shown in Figure~\ref{fig:main}, the schema linking step involves analyzing the test question to detect keywords and phrases that correspond to schema elements such as tables, columns, rows, and foreign keys within the database schema.
This mapping narrows the focus to the most pertinent parts of the schema and provides the necessary context for generating examples that are both meaningful and grounded in the database structure. 
%This schema linking step establishes connections between the test question and the schema elements. 
%This mapping not only narrows down the relevant parts of the schema but also provides the foundational context required for the example generation phase in the subsequent steps of SAFE-SQL. It ensures that the examples generated are meaningful and grounded in the schema elements most relevant to the test question.
\subsection{Example Generation}

%After we identify relevant relationships among test questions, tables, foreign keys, and columns by performing schema linking, we generate similar examples. 
%Inference performance can be significantly enhanced by employing only synthetic training data without human-annotated data~\cite{selftaughtevaluators}. Moreover, generating diverse and semantically relevant examples enhance the robustness of Text-to-SQL models~\cite{diverse_example}. Accurately identifying relationships among database schema elements improve SQL query generation~\cite{importance_schema_linking}.
%Utilizing schema linking information as prior information of bayesian algorithm~\cite{bayesian_inference}, we generate diverse and semantically relevant examples for each test question, ensuring a richer representation of database interactions. 
Using the information obtained from schema linking, the LLM generates multiple synthetic examples for each test question. As illustrated in Figure~\ref{fig:main}, for each test question, we generate ten examples—each comprising a similar question, its corresponding SQL query, and a detailed reasoning path. These examples maintain structural similarity while varying elements such as numerical values, table names, and key attributes. This controlled variation ensures that the generated examples remain relevant while encouraging the model to generalize beyond surface-level patterns. By observing these modified instances, the model can infer the correct SQL query even when faced with unseen but structurally similar questions. In particular, the reasoning path outlines the logical steps required to derive the correct SQL query result, providing a comprehensive explanation of the query execution process. We provide the full prompt used for Large Language Models in Appendix~\ref{appen:example_prompt}.
%This approach enhances the diversity and informativeness of the generated examples by incorporating relevant schema elements, thereby facilitating a more robust understanding of question-to-SQL query mapping.

\subsection{Relevance Scoring}
\begin{comment}
Following example generation, SAFE-SQL prioritizes the selection of highly pertinent examples to inform the final inference stage.This relevance evaluation strategy aligns with established principles in query understanding and information retrieval, aiming to favor examples that accurately capture the core intent of the request while minimizing irrelevant details~\cite{trust_sql} in text-to-SQL. To quantify relevance, each example is assigned a relevance score \( Rel \) on a scale from 0 to 10. The score leveraging three factors: reasoning path, the example question, and the corresponding SQL query. Formally, the relevance score \( R(e) \) for an example \( e \) is defined as:
\[
Rel =  S_q(e, Q_t) + S_k(e, Q_t) + S_r(e, Q_t) 
\]
where:
\begin{itemize}
    \item \( S_q(e, Q_t) \): Embedding similarity between the example question and \( Q_t \).
    \item \( S_k(e, Q_t) \): Key word \& Structural similarity of the SQL question associated with \( e \) to the expected schema context derived from \( Q_t \).
    \item \( S_r(e, Q_t) \): Completeness and Reasonability of the reasoning path of the example \( e \) to the test question \( Q_t \).
\end{itemize}
Each similarity component \( S \) is calculated using a combination of Reasonalbility, embedding-based similarity measures (cosine similarity), and keyword \& structural comparisons. The relevance scdore is composed of three parts: embedding similarity (max 3 points), keyword \& structural similarity (max 3 points), and the completeness and reasonability of the reasoning path (max 4 points). The reasonability component is assigned an additional point to include several sub-factors:  Logical Consistency, Schema Relevance of Reasoning Steps, Completeness of Reasoning, and Faithfulness to Question, each of which contributes to a more comprehensive evaluation of the reasoning path.
\end{comment}
After generating synthetic examples, SAFE-SQL evaluates examples, as shown in Figure~\ref{fig:main}. This evaluation process capture the core intent of the test question. To achieve this, each example \( e \) is assigned a composite relevance score \( Rel\) on a scale from 0 to 10, which is calculated as follows:
\begin{equation}\resizebox{0.42\textwidth}{!}{
$ Rel = \alpha \cdot S(Q_e, Q_t) + \beta \cdot A(Q_e, Q_t) + \gamma \cdot R$}
\end{equation}


%  알파 감마 베타 합이 1임. 
Here, \( Q_t \) denotes the test question, \( Q_e \) denotes the generated example question, \( R \) denotes the reasoning path of the generated example, and the three components are defined as:
\begin{itemize}
    \item \textbf{\( S(Q_e, Q_t) \):} Semantic similarity between the generated question and test questions.
    \item \textbf{\( A(Q_e, Q_t) \):} Structural alignment between the generated question and test questions.
    \item \textbf{\( R \):} Evaluates the quality of the reasoning path accompanying the example. This score measures the completeness, logical consistency, and alignment of the reasoning steps with both the test question and the relevant schema elements.
\end{itemize}
The weighting factors $\alpha$, $\beta$, and $\gamma$ sum to 1, allowing SAFE-SQL to select examples. Only examples that surpass a predefined threshold are retained for guiding SQL query  generation. We provide the full prompt used for LLMs in Appendix~\ref{appen:filtering_examples}.
The semantic similarity score measures whether the generated question accurately preserves the intent of the test question, even if its wording differs. As shown in Figure~\ref{fig:main}, a generated question "Retrieve the list of countries where one singer is older than 40 and another is younger than 30." has a different structure but correctly maintains the meaning, resulting in a high semantic similarity score. Another generated question, "Show cities where a singer above age 40 and a singer below 30 are from." shares the same SQL structure as the test question, differing only in the geographical entity, resulting in a high structural similarity score. For reasoning path similarity, examples that follow a similar step-by-step logical process to derive the SQL query receive a higher score. Queries that require the same table joins, filtering conditions, and aggregation methods are more likely to align with the reasoning process of the test question, leading to a stronger match. These factors are complementary, and carefully evaluating these factors is crucial for effective example selection, ensuring that the retrieved examples are highly relevant to the test question. 
\subsection{Threshold Selection}




 % However, a question like "Show countries with singers who are either above 40 or below 30." alters the logical condition by changing "and" to "or," which deviates from the original intent, leading to a lower semantic similarity score.
\begin{comment}
To ensure high-quality examples, only examples with a relevance score above a predefined threshold \( T \) are retained:
\[
E_{\text{selected}} = \{ e \in E \mid R(e) \geq 
\theta\ \},
\]
where \( E_{\text{selected}} \) is the set of selected examples, \( E \) is the set of all generated examples, and \(\theta\)  is the threshold value (e.g., \( T = 8 \)). By filtering out low-scoring examples and prioritizing those with the highest scores, SAFE-SQL ensures that the final SQL query generation process is guided by high-quality, contextually appropriate examples. The relevance scoring mechanism acts as a self-assessment tool, enabling the framework to dynamically adapt to the complexity of the test question and maintain consistency and accuracy.
\end{comment}
To further ensure quality, SAFE-SQL retains only those examples with a relevance score above a predefined threshold \( \theta \). Formally, the set of selected examples is defined as:
\begin{equation}\resizebox{0.30\textwidth}{!}{
$E_{\text{selected}} = \{ e \in E \mid Rel \geq \theta \}$}
\end{equation}
where \( E \) represents all generated examples. This thresholding step filters out low-quality examples and ensures that only the most informative and contextually appropriate examples are used in the final inference. The threshold is set to 8, as Figure~\ref{tab:diff_thres} demonstrates that this value provides an optimal balance between preserving high-quality examples and maintaining sufficient diversity for robust SQL generation.


\subsection{Final Inference}
%In the final stage of the SAFE-SQL framework, the system generates an SQL query by integrating the test question with the most relevant examples. Each example consists of similar questions, an SQL query, and a reasoning paths, which links schema elements to SQL components. This contextualized input is then fed into an LLM, which generates the final SQL query. By leveraging schema linking, example generation, and relevance scoring, SAFE-SQL ensures syntactically correct and semantically aligned SQL query generation. The inclusion of explicit reasoning paths enhances interpretability, making the process more transparent and reliable.
In the final stage, the high-quality examples generated in previous steps are combined with the test question to construct a comprehensive prompt for the LLM. These examples, enriched with similar questions, corresponding SQL queries, and detailed reasoning paths, guide the LLM in generating the final SQL query. By integrating schema linking, synthetic example generation, relevance scoring, and threshold-based filtering, SAFE-SQL produces SQL queries that are both syntactically correct and semantically aligned with the intended database operations, while also providing an interpretable reasoning process.


