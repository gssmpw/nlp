

\section{Experiment}
% ~\footnote{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}
% ~\footnote{https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct}
% ~\footnote{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}
% ~\footnote{https://huggingface.co/bigcode/starcoder2-7b}
\subsection{Experimental Setup}
For our experiments, we employ six models for comparison purposes: GPT-4o~\cite{hurst2024gpt}, GPT-4o-mini~\cite{hurst2024gpt}, GPT-4~\cite{achiam2023gpt}, Llama-3.1-8B-Instruct~\cite{dubey2024llama}, Deepseek-coder-6.7b-instruct~\cite{guo2024deepseek}, Qwen2.5-7B-Instruct~\cite{yang2024qwen2}, and starcoder2-7b~\cite{lozhkov2024starcoder}. The evaluation is conducted on the Spider~\cite{spider} dev dataset, which is a widely used benchmark for Text-to-SQL systems. The Spider dev set is a large-scale, cross-domain benchmark specifically designed to assess the generalization capabilities of Text-to-SQL models. It contains 7,000 training samples covering 166 databases in various domains and 1,034 evaluation samples from 20 databases. Our analysis parts conduct with gpt-4o-2024-08-06 version model. 


% Our second dataset, Spider-Syn~\cite{spider_syn}, is a variant of the original Spider dataset where schema-related terms are replaced with synonyms, and explicit references that connect natural language queries (NLQs) to the database schema are removed.


\subsection{Baselines}
We use the following baseline text to SQL methods: \textbf{Supervised fine tuning}, which fine-tune open source model, \textbf{Zero-shot}, which inference without examples, \textbf{Few-shot}, which inference with few examples. Synthesizes text-to-SQL data from weak and strong LLMs~\cite{synthesize} utilizes preference learning from the weak data from small LLMs and strong data from LLMs. SQL-Palm~\cite{palm-sql} introduces synthetic data augmentation to fine-tune open source models. Din SQL~\cite{din} breaking down the task into smaller sub-tasks, allowing large language models to iteratively improve their reasoning process through self-correction. C3 SQL~\cite{c3_zeroshot} comprises  Clear Prompting, Calibration with Hints, and Consistent Output, which systematically addresses model input, bias, and output to enhance performance using zero-shot prompt. Dail SQL~\cite{dail_sql} introduces effective few-shot learning which significantly reduces the number of tokens required per question. ACT-SQL~\cite{act_sql} enhances Text-to-SQL performance by automatically generating chain-of-thought exemplars, eliminating the need for manual labeling. PTD SQL~\cite{ptd} categorizing queries into subproblems and focusing on targeted drilling to improve large language models' reasoning capabilities. Unlike traditional few-shot methods that depend on human-selected examples, our Self-Augmented In-Context Learning method allowing the model to generate its own in-context examples, and used for final inference. This self-augmented approach removing the need for manually provided exemples while still leveraging the benefits of in-context learning, leading to improved adaptability and performance in Text-to-SQL tasks.

\input{Tables/main_table}

\subsection{Evaluation Metrics}
We use Execution Accuracy (EX) and Exact Match (EM) to evaluate the performance of our model. EX measures whether the SQL query generated by the model produces the same results as the ground truth query when executed on a database. Exact Match (EM), on the other hand, assesses whether the predicted SQL query exactly matches the ground truth query in its structure and syntax. By combining these two metrics, we ensure a comprehensive evaluation of both the correctness and execution reliability of the generated SQL queries.

% This metric is sensitive to the state and schema of the database, making it essential to maintain a consistent testing environment for reliable evaluation.
\input{Tables/1_ablation}

\subsection{Performance among SQL difficulty level}
We analyze the performance of SAFE-SQL across different SQL difficulty levels and compare it with zero-shot, few-shot prompting methods, and supervised fine-tuning approaches. The results, presented in Table~\ref{tab:sql_comparison}, demonstrate that SAFE-SQL achieves overall superior performance, with particularly strong improvements in hard and extra hard categories. Few-shot methods exhibit higher accuracy in Easy and Medium categories, which can be attributed to skeleton-masked selection which retrieves answers directly from the training set, leading to an inflated performance in simpler queries. SAFE-SQL excels in hard and extra hard categories, achieving significantly higher EX. This improvement is notably influenced by the inclusion of reasoning paths, which provide explicit guidance in SQL generation and enhance the modelâ€™s ability to construct complex queries, as well as the filtering of misleading examples, which reduces potential confusion and prevents error propagation. While multiple factors contribute to SAFE-SQL's effectiveness, these mechanisms play a crucial role in enabling the model to generate more accurate and structurally sound SQL queries, especially in challenging scenarios where other approaches struggle.


