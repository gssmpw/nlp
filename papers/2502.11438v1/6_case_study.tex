\subsection{Analysis}

\input{Tables/number_of_filtered_ex} 

\begin{figure*}[t]
\centerline{\includegraphics[scale=0.48]{Pictures/corr_bin.pdf}}

\caption{(Left) Correlation between question embedding similarity and average EX, (Right) Average EX across embedding similarity bins}
% \vspace{-4mm}
\label{fig:corr_bin}
\end{figure*}

\paragraph{Number of generated and filtered examples per score, along with an embedding similarity analysis of the filtered examples}
For each test question in the Spider dev set, 10 examples are generated, resulting in a total of 10,340 examples. The quality of these examples is assessed using a relevance score ranging from 0 to 10. As shown in Table~\ref{tab:number_of_generated}, the 65.71\% of examples are assigned a score of 10, while the 0.59\% of examples are received a score of 0. This trend suggests that the LLM tends to assign high relevance to its own generated examples. The similarity is computed using cosine similarity, where higher scores indicate greater semantic alignment between the test questions and the retained examples. As the filtering threshold increases, the embedding similarity also increases, suggesting that higher-relevance examples exhibit stronger semantic consistency with the test questions. However, we also observe that overly strict filtering—selecting only examples with a perfect score of 10—leads to a decline in performance. This drop occurs because an excessively high threshold significantly reduces the number of available examples, limiting the diversity.


\paragraph{Effect of question embedding similarity on Execution Accuracy.}
In Figure~\ref{fig:corr_bin}, the left graph illustrates the correlation between embedding similarity and EX. Each point represents one of the 11 data points obtained by filtering examples based on different threshold scores (0 to 10). The data points follow an upward trend, suggesting that higher similarity tends to result in better EX. The red line indicates the overall correlation, with a coefficient of 0.82, showing a relatively strong positive relationship. Building on this analysis, the right graph provides a more fine-grained view by examining the execution accuracy of individual generated examples based on their embedding similarity with test questions. The x-axis represents the normalized similarity between the test question and the generated question, and the y-axis indicates EX. The results show that EX is lowest in the 0.0-0.1 similarity range, suggesting that examples with very low similarity to test questions tend to be less useful. As similarity increases, EX generally improves, peaking in the 0.7-0.8 range. This suggests that examples with a moderate to high similarity to test questions are more effective in generating executable SQL queries. However, accuracy drops slightly in the 0.8-0.9 range before rising again in the 0.9-1.0 range. This indicates that excessively high similarity can reduce diversity, potentially limiting the model’s generalization ability. 


\begin{figure}[t]
\centerline{\includegraphics[scale=0.36]{Pictures/Diff_threshold_GPT4o.png}}
\caption{Performance of GPT-4o at different relevance score thresholds.}
% \vspace{-5mm}
\label{tab:diff_thres}
\end{figure}


\paragraph{Effect of Relevance Scoring Thresholds on Performance.}

To further evaluate the effectiveness of SAFE-SQL, we conduct a detailed case study using varying thresholds for the relevance scoring mechanism as shown in Figure~\ref{tab:diff_thres}.  The self-generated examples are filtered based on relevance scores, with thresholds ranging from 0 to 10. For each test question, the number of high-scoring examples varied due to the specific content and schema structure (e.g., some test questions had six examples with scores $\geq 8$, while others had three). The selected examples are then used during the final inference stage to generate SQL queries. The $\geq 8$ threshold consistently produced the best results, validating the robustness of SAFE-SQL’s relevance score filtering. The results demonstrate that selecting high-quality examples plays a critical role in guiding LLMs to generate accurate SQL queries, regardless of the underlying model.


\input{Tables/generated_example}

%This experiment is performed across multiple models, including GPT 4o Mini, Deepseek Coder 6.7B, %Llama 3.1 8B, and Starcoder 7B.
\input{Tables/filtering_score_ablation} 

\paragraph{Effect of three measuring components on Performance.}

To assess the impact of the three measuring components—semantic similarity ($\alpha$), keyword \& structural similarity ($\beta$), and reasoning path quality ($\gamma$)—on EX, we conduct experiments by varying their respective weightings. The results, presented in Table~\ref{tab:filtering_score_ablation}, highlight distinct performance trends across different difficulty levels. Notably, the exclusion of reasoning path quality leads to a drop in EX, particularly in the Hard and Extra Hard. This suggests that a well-structured reasoning path is crucial for handling complex queries, as it provides essential logical steps that bridge the gap between natural language understanding and SQL formulation. Conversely, semantic similarity and structural SQL query similarity have a greater influence on the Easy and Medium levels. This is because these queries tend to be relatively straightforward, meaning that having structurally similar SQL questions in the example set often provides sufficient guidance for generating correct queries. In these cases, direct pattern matching and schema alignment play a larger role. Overall, the findings demonstrate that a balanced combination of all three components is essential for optimizing performance across different levels of query complexity.

% Simply maximizing similarity may not always yield the best results, and a balanced approach that considers both relevance and diversity could be more effective.


%: Qwen2.5-3B-instruct, Qwen2.5-7B-instruct, and Qwen2.5-14B-instruct 


\subsection{Case Study}
As shown in Table~\ref{tab:sql_examples}, test questions from the Spider dev set alongside their generated similar examples, evaluated based on semantic similarity, structural similarity, and the reasoning path score, which together determine the relevance score. The first example achieves a perfect relevance score of 10, as the generated question closely aligns with the original in meaning, structure, and reasoning. The SQL formulation remains nearly identical, and the reasoning path explicitly details each step, ensuring full alignment. The second example receives a relevance score of 8, with semantic similarity of 7 due to minor differences in terminology ("documents" vs. "articles" and "letter 'w'" vs. "word 'data'"). However, its structural similarity remains high, as the SQL structure is nearly identical. The reasoning path score of 8 reflects a clear explanation of query formulation, though slightly less detailed than the first example. The third example has the lowest relevance score due to significant differences. The generated question shifts focus from counting car models to listing IT employees, resulting in semantic similarity of 6 and structural similarity of 3. These results emphasize the importance of fine-grained example selection due to the varing quality of generated examples.
% Table 6번 언급되는 곳이 하나도 없었습니다. 맨뒤로 빼고 Case Study 만들어서 설명할 필요가 있습니다. 또한 Relevance Score 변경했는데 확인해주셔야합니다