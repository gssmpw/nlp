\appendix
\newpage
\clearpage
\section{Appendix}

% \subsection{Number of generated examples per score.}
% \label{appen:Full_number_of_generated}
% Number of generated examples per score.
% \input{Tables/Full_number_of_generated_ex(APP)}





%\section{Performance based on generated examples across different model size}
%\input{Tables/Qwen2.5_generated_examples}

\section{Prompts for SAFE-SQL}
\subsection{Prompt for example generation.}
\label{appen:example_prompt}
For example generation, we use zero shot prompt as shown in the figure~\ref{tab:example_generation}. 
\input{others/example_prompt}



\subsection{Prompt for filtering examples.}
\label{appen:filtering_examples}
For example generation, we use zero shot prompt as shown in figure~\ref{tab:filtering_exmaples}. 
\input{others/relevance_prompt}

\subsection{Prompt for final inference.}
\label{appen:final_inference}
For final inference, we use zero shot prompt as shown in figure~\ref{tab:final_prompt}. 
\input{others/Final_inference}
\label{appen:Impact_size}





\section{Impact of model size}

\input{Tables/Qwen2.5_generated_examples}

\paragraph{Performance based on generated examples across different model size}
As shown in Table~\ref{tab:qwen_ab_models}, We investigate the impact of model size on example generation with different variants of the Qwen2.5 Models. The results demonstrate that the 14B model achieves the highest overall performance, followed by the 7B and the 3B. This trend is consistent across all difficulty levels, with large model size generating higher-quality examples that lead to more accurate SQL query generation. The performance improvement with increasing model size can be attributed to the enhanced capacity of larger models to capture SQL question patterns and semantic relationships. Moreover, larger models possess more extensive information, allowing them to generate more appropriate questions and construct detailed reasoning paths, which contribute to the overall accuracy of SQL query generation.


\section{Spider dev training set embedding clusters.}
% \begin{figure}[h]
% \centerline{\includegraphics[scale=0.35]{Pictures/tsne_embedding_train.pdf}}
% \caption{t-SNE embedding clusters of the Spider dev training dataset categorized into 15 groups.}
% \end{figure}
\begin{figure}[h]
\centerline{\includegraphics[scale=0.35]{Pictures/tsne_embedding_train.pdf}}
\caption{Embedding of spider dev set training questions.}
\vspace{-5mm}
\label{tab:spider_dev_embedding}
\end{figure}
 Although questions within the same category share semantic similarities, they may belong to different clusters, leading to inconsistencies when retrieving examples from the training set. This highlights the limitations of training set retrieval in Text-to-SQL tasks.
 

\input{Tables/another_main}
\subsection{Additional model performance} 
To evaluate the impact of example generation quality on Text-to-SQL performance, we conducted experiments using different models for final inference. Examples generated by GPT-4o, followed by inference using the target model. Large language models', such as Qwen 2.5-7B and Deepseek-coder-6.7B, ability to generate high-quality, semantically relevant in-context examples is limited. To mitigate this, we first used GPT-4o to generate in-context examples and filtering examples, then performed final inference using the selected model. Our results show that leveraging GPT-4o for example generation and scoring improved overall execution accuracy by 6.9 points compared to fully relying on Qwen 2.5-7B for the entire process as shown in Table~\ref{tab:qwen_ab_models}. This confirms that high-quality, well-aligned in-context examples play a crucial role in enhancing Text-to-SQL performance, especially in complex queries.