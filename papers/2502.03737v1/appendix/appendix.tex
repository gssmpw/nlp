

\section{Omitted Proofs}
\label{sec:apx}

\begin{proof}[Proof of ~\Cref{lem:lower}]
Consider a linear combination information structure $\theta=\theta_1$ or $\theta_2$ with equal probability. Now we compute its best response $f'(\hat{\rvx})=\E\left[\frac{1}{n}\sum_i \rx_i|\hat{\rvx}\right]$, which is the posterior given the observed ratings. Given $s=\sum_i \mathbbm{1}(\hat{x}_i=1),t=\sum_i \mathbbm{1}(\hat{x}_i=m)$. We have

\begin{footnotesize}
\begin{align*}
    &\quad \ \Pr[\theta=\theta_1|\hat{\rvx}]\\
    &=\frac{\Pr[\hat{\rvx}|\theta_1]}{\Pr[\hat{\rvx}|\theta_1]+\Pr[\hat{\rvx}|\theta_2]}\\
    &=\frac{\binom{n}{t}\binom{n-t}{s}a^{n-t}(1-a)^tq^s(1-q)^{n-s-t}}{\binom{n}{t}\binom{n-t}{s}a^{n-t}(1-a)^tq^s(1-q)^{n-s-t}+\binom{n}{s}\binom{n-s}{t}a^{n-s}(1-a)^sq^t(1-q)^{n-s-t}}\\
    &=\frac{(aq)^{s-t}}{(aq)^{s-t}+(1-a)^{s-t}}.\\
    &=\frac{1}{1+(\frac{1-a}{aq})^{s-t}}
\end{align*}
\end{footnotesize}

Then  

\begin{align*}
    &\quad \ \E[\rx|\hat{\rx}=0]\\
    &=\Pr[\theta=\theta_1|\hat{\rvx}]\E[\rx|\hat{\rx}=0,\theta_1]+\Pr[\theta=\theta_2|\hat{\rvx}]\E[\rx|\hat{\rx}=0,\theta_2]\\
    &=1*\frac{1}{1+(\frac{1-a}{aq})^{s-t}}+m*(1-\frac{1}{1+(\frac{1-a}{aq})^{s-t}})\\
    &=m-(m-1)\frac{1}{1+(\frac{1-a}{aq})^{s-t}}.
\end{align*}

Thus the best response is $f'(\hat{\rvx})=\frac{\sum_i \hat{\rx}_i+(m-(m-1)\frac{1}{1+(\frac{1-a}{aq})^{s-t}})*(n-s-t)}{n}$. By simple calculation we obtain that for any $f$,
\begin{footnotesize}
\begin{align*}
    R(f,\Theta)&\ge R(f,\theta)\ge R(f',\theta)\\
    &=\sum_{s,t}\binom{n}{t}\binom{n-t}{s}a^{n-t}(1-a)^tq^s(1-q)^{n-s-t}\left(\frac{(n-s-t)(m-1)(1-\mu)}{n}\right)^2\\
\end{align*}
\end{footnotesize}
\end{proof}


\begin{proof}[Proof of ~\Cref{thm:PAA}]
When $n\to\infty$, the empirical distribution is propositional to the element-wise product of the true distribution $\vp$ and the participation probabilities $\vg$: $\hat{\vp}\propto \vp\circ\vg$. We want to minimize $\max_{\vp,\vg}(f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2$.

Given $\hat{\vp}$, define the lower bound of the expectation of $\vp$ which satisfy the propositional constraint $l^*(\hat{\vp})=\min_{\vp,\vg:\hat{p}\propto \vp\circ\vg}\E_{\rx\sim\vp}[\rx]$ and the upper bound $u^*(\hat{\vp})=\max_{\vp,\vg:\hat{p}\propto \vp\circ\vg}\E_{\rx\sim\vp}[\rx]$. Since $(f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2$ is a quadratic function about $\E_{\rx\sim\vp}[\rx]$, we have 
% \begin{small}
%     $$\max_{\vp,g}(f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2=\max\{(f(\hat{\vp})-l^*(\hat{\vp}))^2,(f(\hat{\vp})-u^*(\hat{\vp}))^2\}\geq (u^*(\hat{\vp})-l^*(\hat{\vp}))^2/4$$
% \end{small}
\begin{align*}
    \max_{\vp,g}(f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2&=\max\{(f(\hat{\vp})-l^*(\hat{\vp}))^2,(f(\hat{\vp})-u^*(\hat{\vp}))^2\}\\
    &\geq (u^*(\hat{\vp})-l^*(\hat{\vp}))^2/4.
\end{align*}

The equality holds if and only if $f(\hat{\vp})=(u^*(\hat{\vp})+l^*(\hat{\vp}))/2$, so the best aggregator is $f(\hat{\vp})=(u^*(\hat{\vp})+l^*(\hat{\vp}))/2$. 
It's left to show that $l^*(\hat{\vp})=l(\hat{\vp})$ and $u^*(\hat{\vp})=u(\hat{\vp})$ where $l(\hat{\vp})$ and $u(\hat{\vp})$ are described in the definition of PAA (\Cref{def:paa}).
% We only need to prove $l=l^*(\hat{\vp})$ and $u=u^*(\hat{\vp})$.

% Let $\vg'$ is the solution to $u^*(\hat{\vp})$(or $l^*(\hat{\vp})$), first we characterize $\vg'$. Given $\hat{\vp}$, $\E_{\rx\sim\vp}[\rx]$ can be viewed as a function( let it be $F$ ) of $\vg$.
Given $\hat{\vp}$, we start to find $(\vp',\vg')$ to minimize or maximize $\E_{\rx\sim\vp}[\rx]$, conditional on $\hat{\vp}\propto \vp\circ\vg$. We first characterize $\vg'$. Here are two properties of the optimal $\vg'$. 
\begin{itemize}
    \item $\vg'$ is extreme: $g_r=1$ or $q$ for any rating $r$.
    \item $\vg'$ is monotonic: $g_r\le g_{r+1}$ or $g_r\ge g_{r+1}$ for any rating $r$.
\end{itemize}

% The reason is, given $\hat{\vp}$, $\E_{\rx\sim\vp}[\rx]$ can be viewed as a function( let it be $F$ ) of $\vg$. 
Given $\hat{\vp}$, because $\hat{\vp}\propto \vp\circ\vg$, $\E_{\rx\sim\vp}[\rx]$ can be viewed as a function, denoted as $F$, of $\vg$. By calculating the partial derivative of $F$,  we have
    \begin{align*}
        \frac{dF}{dg_r}&=\frac{p_r\hat{g}_r^{-2}}{(\sum_j \hat{p}_{j}/g_j)^2}*(\sum_j j\frac{\hat{p}_{j}}{g_j}-r\sum_j \frac{\hat{p}_{j}}{g_j})\\
        &=\frac{p_r\hat{g}_r^{-2}}{(\sum_j \hat{p}_{j}/g_j)^2}*(\sum_{j=1}^{r-1}(j-r)\frac{\hat{p}_{j}}{g_j}+\sum_{j=r+1}^{m}(j-r)\frac{\hat{p}_{j}}{g_j})\\
        &=\frac{p_r\hat{g}_r^{-2}}{\sum_j \hat{p}_{j}/g_j}*(F(\vg)-r).
    \end{align*}

First Notice that the sign of $\frac{dF}{dg_r}$ is independent with $g_r$ as long as $g_r$ is positive, so the optimal $\vg$ is extreme: $g_r=1$ or $g_r=q$. Then Notice the sign of $\frac{dF}{dg_r}$ is determined by $F(\vg)-r$ where $F(\vg)=\E_{\rx \sim \vp}[\rx] \in [1,m]$, so the optimal $\vg$ is monotonic.

Using these two properties of $\vg'$, $\vg'$ can be given by an index strategy: 

for the minimum, there exists an index $k_1(\hat{\vp})$ such that 
\begin{equation*}
    g'_r=\left\{\begin{aligned}
        &q&\quad r\le k_1(\hat{\vp})\\
        &1&\quad r> k_1(\hat{\vp})\\
    \end{aligned}
    \right
    .
\end{equation*}
while for the maximum, there exists an index $k_2(\hat{\vp})$ such that
\begin{equation*}
    g'_r=\left\{\begin{aligned}
        &1&\quad r\le k_2(\hat{\vp})\\
        &q&\quad r> k_2(\hat{\vp})\\
    \end{aligned}
    \right
    .
\end{equation*}

% Using these two properties of $\vg'$, $\vg'$ can be given by an index strategy: 

% for the minimum, there exists an index $k_1$ such that 
% \begin{equation}
%     g'_r=\left\{\begin{aligned}
%         &q&\quad r\le k_1\\
%         &1&\quad r> k_1\\
%     \end{aligned}
%     \right
%     .
% \end{equation}
% while for the maximum, there exists an index $k_2$ such that
% \begin{equation}
%     g'_r=\left\{\begin{aligned}
%         &1&\quad r\le k_2\\
%         &q&\quad r> k_2\\
%     \end{aligned}
%     \right
%     .
% \end{equation}

To find the optimal index, we can simply calculate all the $m$ values, but here we make a more careful analysis. 

% Take the minimum for example. We use $\vg^{(k)}$ to denote a $1\times m$ vector with the first $k$ elements being $q$ and the other elements being $1$. We claim $F(\vg^{(k)})$ has a single-bottom shape regarding the index $k$, or equivalently, $F(\vg^{(k)})-F(\vg^{(k-1)})>0 \implies F(\vg^{(k+1)})-F(\vg^{(k)})>0$.\\
Take the minimum for example. We use $\vg^{(k)}$ to denote a $1\times m$ vector with the first $k$ elements being $q$ and the other elements being $1$. We claim $F(\vg^{(k)})$ has a single-bottom shape regarding the index $k$, or equivalently, $$F(\vg^{(k)})-F(\vg^{(k-1)})>0 \implies \forall k^{'}\geq k, \ F(\vg^{(k')})-F(\vg^{(k'-1)})\geq 0.$$

Let $D_k=\frac{1}{q}\sum_{i=1}^k{\hat{p}_{i}}+\sum_{i=k+1}^m\hat{p}_{i}$. We have $$
F(\vg^{(k)})-F(\vg^{(k-1)})=(\frac{1}{q}-1)\hat{p}_kD_k(k-F(g^k)).$$

We first prove $F(\vg^{(k)})< k \implies \forall k^{'}\geq k, \ F(\vg^{(k')}) < k^{'}$. Notice 
% which implies $F(\vg^{(k)})-F(\vg^{(k-1)})>0 \iff F(\vg^{(k)})<k$. Also, we have

$$F(\vg^{(k+1)})-F(\vg^{(k)})=(\frac{1}{q}-1)\hat{p}_{k+1}D_{k+1}((k+1)-F(g^{k+1})).$$

So
$$F(\vg^{(k+1)})=\frac{F(k)+(\frac{1}{q}-1)(k+1)\hat{p}_{k+1}D_{k+1}}{1+(\frac{1}{q}-1)\hat{p}_{k+1}D_{k+1}}.$$

If $F(\vg^{(k)}) < k$, we have 
\begin{align*}
    F(g^{k+1})&=\frac{F(k)+(\frac{1}{q}-1)(k+1)\hat{p}_{k+1}D_{k+1}}{1+(\frac{1}{q}-1)\hat{p}_{k+1}D_{k+1}}\\
    &<\frac{k+(\frac{1}{q}-1)(k+1)\hat{p}_{k+1}D_{k+1}}{1+(\frac{1}{q}-1)\hat{p}_{k+1}D_{k+1}}\\
    &<\frac{(k+1)+(\frac{1}{q}-1)(k+1)\hat{p}_{k+1}D_{k+1}}{1+(\frac{1}{q}-1)\hat{p}_{k+1}D_{k+1}}\\
    &=k+1
\end{align*}
By induction, we have $\forall k^{'}\geq k, \ F(\vg^{(k')}) < k^{'}$. So 
\begin{align*}
    F(\vg^{(k)})-F(\vg^{(k-1)})>0 &\implies F(\vg^{(k)})< k \\
    &\implies \forall k^{'}\geq k, \ F(\vg^{(k')}) < k^{'} \\
    &\implies \forall k^{'}\geq k, \ F(\vg^{(k')})-F(\vg^{(k'-1)})\geq 0.
\end{align*}


So $k_1(\hat{\vp})$ is the optimal index for minimum $\iff F(\vg^{(k_1(\hat{\vp}))})-k_1(\hat{\vp})>0, F(\vg^{(k_1(\hat{\vp})+1)})-(k_1(\hat{\vp})+1)<0$ ( Since $\hat{\vp}$ can have zero entries, there might be some consecutive $k$ that all of them is "optimal". For clarity, we take this definition). 
i.e. $$k_1(\hat{\vp})=\max\{k:1\leq k\leq m,\sum_{i=1}^{k-1}(i-k)\frac{\hat{p}_{i}}{q}+\sum_{i=k+1}^{m}(i-k)\hat{p}_{i}\geq 0\}.$$

Similarly, $$k_2(\hat{\vp})=\max\{k:1\leq k\leq m,\sum_{i=1}^{k-1}(i-k)\hat{p}_{i}+\sum_{i=k+1}^{m}(i-k)\frac{\hat{p}_{i}}{q}\geq 0\}.$$


% So $k_1$ is the optimal index for minimum $\iff F(\vg^{(k_1)})-k_1>0, F(\vg^{(k_1+1)})-(k_1+1)<0$ (Since $\hat{\vp}$ can have zero entries, there might be some consecutive $k$ that all of them is "optimal". For clarity, we take this definition). 
% i.e. $$k_1=\max\{k:1\leq k\leq m,\sum_{i=1}^{k-1}(i-k)\frac{\hat{p}_{i}}{q}+\sum_{i=k+1}^{m}(i-k)\hat{p}_{i}\geq 0\}.$$

% Similarly, $$k_2=\max\{k:1\leq k\leq m,\sum_{i=1}^{k-1}(i-k)\hat{p}_{i}+\sum_{i=k+1}^{m}(i-k)\frac{\hat{p}_{i}}{q}\geq 0\}.$$

Notice the optimal index is also a rough approximation of the optimal value. Since $k_1(\hat{\vp})<l(\hat{\vp})<k_1(\hat{\vp})+1,k_2(\hat{\vp})<u(\hat{\vp})<k_2(\hat{\vp})+1$ and $l(\hat{\vp})<u(\hat{\vp})$, we have $k_1(\hat{\vp})\leq k_2(\hat{\vp})$.



\end{proof}



\begin{proof}[Proof of ~\Cref{prop:PAA}]
Fix the optimal aggregator PAA, we aim to find the worst information structure $(\vp^*,\vg^*)$ that maximizes the loss/regret $(f^{PAA}(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2$.

We have proved that the optimal aggregator is the midpoint of the extremes $l(\hat{\vp})$ and $u(\hat{\vp})$. The maximal loss is $\left(u(\hat{\vp})-l(\hat{\vp})\right)^2/4$. To obtain the maximal loss, we aim to find $\hat{\vp}^*$ to maximize $u(\hat{\vp})-l(\hat{\vp})$. We analyze the maximizer in the following four steps.

% We reduce this lemma in four steps. 
First we prove $\hat{\vp}^*$ has at most three non-zero entries. Then we prove $\hat{\vp}^*$ has exactly two non-zero entries. Next we prove the $\hat{\vp}^*$ is $(\frac{1}{2},0,\cdots,0,\frac{1}{2})$. Finally, we calculate the worst information structures $(\vp^*,\vg^*)$ given $\hat{\vp}^*$ by computing the maximizer and the minimizer of $\E_{\rx\sim\vp}[\rx]$ given $\hat{\vp}=\hat{\vp}^*$.




\textbf{First Step:}  For any $\hat{\vp}$, consider the optimal index for minimum $k_1(\hat{\vp})$ and the optimal index for maximum $k_2(\hat{\vp})$. Since $k_1(\hat{\vp})<l(\hat{\vp})<k_1(\hat{\vp})+1,k_2(\hat{\vp})<u(\hat{\vp})<k_2(\hat{\vp})+1$ and $l(\hat{\vp})<u(\hat{\vp})$, we have $k_1(\hat{\vp})\leq k_2(\hat{\vp})$. Define $a=\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}$ and
$$\hat{\vp}_1=(a,0,\cdots,0,\hat{p}_{k_1(\hat{\vp})+1},\hat{p}_{k_1(\hat{\vp})+2},\cdots,\hat{p}_m).$$ 



For short, we denote $R(f,\hat{\vp})=\max_{\vp,\vg:\hat{\vp}\propto \vp\circ\vg} (f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2$, which is the maximum regret of $f$ given the empirical distribution $\hat{\vp}$. We will show $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}_1)$.

Note for $\hat{\vp}_1$, we have
$$l(\hat{\vp}_1)\leq \frac{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}\hat{p}_{j}},
$$
and 
$$u(\hat{\vp}_1)\geq \frac{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}\hat{p}_{j}}.$$

So \begin{align*}
    & \quad \, (u(\hat{\vp}_1)-l(\hat{\vp}_1))-(u(\hat{\vp})-l(\hat{\vp}))\\
    &=(l(\hat{\vp})-l(\hat{\vp}_1))-(u(\hat{\vp})-u(\hat{\vp}_1))\\
    &\geq (\frac{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}j\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}\hat{p}_{j}}-\frac{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}\hat{p}_{j}})\\&-(\frac{\sum_{j=1}^{k_2(\hat{\vp})}j\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}\hat{p}_{j}})\\
    &=\frac{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}(j-1)\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+\sum_{j=k_1(\hat{\vp})+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2(\hat{\vp})}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}\hat{p}_{j}}\\
    &=\frac{\sum_{j=1}^{k_1(\hat{\vp})}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_1(\hat{\vp})}\hat{p}_{j}+q\sum_{j=k_1(\hat{\vp})+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2(\hat{\vp})}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_2(\hat{\vp})}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2(\hat{\vp})+1}^{m}\hat{p}_{j}}\\
    &\geq 0.  
\end{align*}

which implies $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}_1)$.

We also use $\vp(1)$ to denote the first element of $\vp$. Define $b=\sum_{j=k_2(\hat{\vp}_1)+1}^{m}\hat{\vp}_{1}(j)$ and
$\hat{\vp}_2=(\hat{\vp}_1(1),\hat{\vp}_1(2),\cdots,\hat{\vp}_1(k_2(\hat{\vp}_1)),0,\cdots,0,b)$.


Similarly, we can prove $R(f^{PAA},\hat{\vp}_1)\leq R(f^{PAA},\hat{\vp}_2)$. 

We call this one iteration. Note the non-zero entries will not increase during the iteration. We iterate until the non-zero entries do not decrease. We use $\hat{\vp}'$ to denote the stable distribution after iterations. $\hat{\vp}'$ has the format of $(\hat{p}_1',0,\cdots,0,\hat{p}_l',\hat{p}_{l+1}',\cdots,\hat{p}_r',0,\cdots,0,\hat{p}_m')$, where $k_1(\hat{\vp}') < l$ and $k_2(\hat{\vp}') > r$.


Define $\hat{\vp}_3=(\hat{p}_1',0,\cdots,0,1-\hat{p}_1'-\hat{p}_m',0,\cdots,0,\hat{p}_m')$ where the $l$-th entry is $1-\hat{p}_1'-\hat{p}_m'$. Define $\hat{\vp}_4=(\hat{p}_1',0,\cdots,0,1-\hat{p}_1'-\hat{p}_m',0,\cdots,0,\hat{p}_m')$ where the $r$-th entry is $1-\hat{p}_1'-\hat{p}_m'$. 


$$l(\hat{\vp}_3)\leq \frac{\frac{1}{q}\sum_{j=1}^{l-1}j\hat{\vp}_{3}(j)+\sum_{j=l}^{m}j\hat{\vp}_{3}(j)}{\frac{1}{q}\sum_{j=1}^{l-1}\hat{\vp}_{3}(j)+\sum_{j=l}^{m}\hat{\vp}_{3}(j)}
=\frac{\frac{1}{q}\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+m\hat{p}_m'}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')}
$$

$$u(\hat{\vp}_3)\geq \frac{\sum_{j=1}^{r}j\hat{\vp}_{3}(j)+\frac{1}{q}\sum_{j=r+1}^{m}j\hat{\vp}_{3}(j)}{\sum_{j=1}^{r}\hat{\vp}_{3}(j)+\frac{1}{q}\sum_{j=r+1}^{m}\hat{\vp}_{3}(j)}
=\frac{\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+\frac{m}{q}\hat{p}_m'}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'}
$$
If $\hat{p}_1' \leq \hat{p}_m'$, then
\begin{align*}
    & \quad \, (u(\hat{\vp}_3)-l(\hat{\vp}_3))-(u(\hat{\vp}')-l(\hat{\vp})')\\
    &=(l(\hat{\vp}')-l(\hat{\vp}_3))-(u(\hat{\vp}')-u(\hat{\vp}_3))\\
    &\geq (\frac{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp}')}j\hat{p}_{j}'+\sum_{j=k_1(\hat{\vp}')+1}^{m}j\hat{p}_{j}'}{\frac{1}{q}\sum_{j=1}^{k_1(\hat{\vp}')}\hat{p}_{j}'+\sum_{j=k_1(\hat{\vp}')+1}^{m}\hat{p}_{j}'}
    -\frac{\frac{1}{q}\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+m\hat{p}_m'}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')})\\
    &-(\frac{\sum_{j=1}^{k_2(\hat{\vp}')}j\hat{p}_{j}'+\frac{1}{q}\sum_{j=k_2(\hat{\vp}')+1}^{m}j\hat{p}_{j}'}{\sum_{j=1}^{k_2(\hat{\vp}'}\hat{p}_{j}'+\frac{1}{q}\sum_{j=k_2(\hat{\vp}')+1}^{m}\hat{p}_{j}'}
    -\frac{\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+\frac{m}{q}\hat{p}_m'}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'})\\
    &=(\frac{\frac{1}{q}\hat{p}_1'+\sum_{j=l}^{r}j\hat{p}_j'+m\hat{p}_m'}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')}
    -\frac{\frac{1}{q}\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+m\hat{p}_m'}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')}
    )\\
    &-(\frac{\hat{p}_1'+\sum_{j=l}^{r}j\hat{p}_j'+\frac{m}{q}\hat{p}_m'}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'}
    -\frac{\hat{p}_1'+l(1-\hat{p}_1'-\hat{p}_m')+\frac{m}{q}\hat{p}_m'}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'})\\
    &= \left(\sum_{j=l}^{r}j\hat{p}_j'-l(1-\hat{p}_1'-\hat{p}_m')\right)\left(\frac{1}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')}-\frac{1}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'}\right)\\
    &= \sum_{j=l}^{r}(j-l)\hat{p}_j'\left(\frac{1}{\frac{1}{q}\hat{p}_1'+(1-\hat{p}_1')}-\frac{1}{(1-\hat{p}_m')+\frac{1}{q}\hat{p}_m'}\right)\\
    &\geq 0.
\end{align*}

Similarly, if $\hat{p}_1' > \hat{p}_m'$, we have $$(u(\hat{\vp}_4)-l(\hat{\vp}_4))-(u(\hat{\vp}')-l(\hat{\vp})')\geq 0.$$

So $(u(\hat{\vp}')-l(\hat{\vp})')\leq \max\{(u(\hat{\vp}_3)-l(\hat{\vp}_3)),(u(\hat{\vp}_4)-l(\hat{\vp}_4))\}$, which implies $R(f^{PAA},\hat{\vp}')\leq \max\{R(f^{PAA},\hat{\vp}_3),R(f^{PAA},\hat{\vp}_4)\}$. So $\hat{\vp}^*$ has at most three non-zero entries.


% \textbf{First Step:}  For any $\hat{\vp}$, suppose the optimal index for minimum is $k_1$ and the optimal index for maximum is $k_2$. Since $k_1<l(\hat{\vp})<k_1+1,k_2<u(\hat{\vp})<k_2+1$ and $l(\hat{\vp})<u(\hat{\vp})$, we have $k_1\leq k_2$. Define $a=\sum_{j=1}^{k_1}\hat{p}_{j}, \ b=\sum_{j=k_2+1}^{m}\hat{p}_{j}$ and
% $$\hat{\vp}_1=(a,0,\cdots,0,\hat{p}_{k_1+1},\hat{p}_{k_1+2},\cdots,\hat{p}_m)$$ $$\hat{\vp}_2=(a,0,\cdots,0,\hat{p}_{k_1+1},\cdots,\hat{p}_{k_2},0,\cdots,0,b)$$  


% For short, we denote $R(f,\hat{\vp})=\max_{\vp,\vg:\hat{\vp}\propto \vp\circ\vg} (f(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2$, which is the maximum regret of $f$ given the empirical distribution $\hat{\vp}$.

% We will show $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}_1) \leq R(f^{PAA},\hat{\vp}_2)$.


% Notice for $\hat{\vp}_1$, we have $$l(\hat{\vp}_1)\leq \frac{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}}, u(\hat{\vp}_1)\geq \frac{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}\hat{p}_{j}}$$

% So \begin{align*}
%     & \quad \, (u(\hat{\vp}_1)-l(\hat{\vp}_1))-(u(\hat{\vp})-l(\hat{\vp}))\\
%     &=(l(\hat{\vp})-l(\hat{\vp}_1))-(u(\hat{\vp})-u(\hat{\vp}_1))\\
%     &\geq (\frac{\frac{1}{q}\sum_{j=1}^{k_1}j\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}}-\frac{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}})\\&-(\frac{\sum_{j=1}^{k_2}j\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}j\hat{p}_{j}}{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}\hat{p}_{j}})\\
%     &=\frac{\frac{1}{q}\sum_{j=1}^{k_1}(j-1)\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}\hat{p}_{j}}\\
%     &=\frac{\sum_{j=1}^{k_1}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_1}\hat{p}_{j}+q\sum_{j=k_1+1}^{m}\hat{p}_{j}}-\frac{\sum_{j=1}^{k_2}(j-1)\hat{p}_{j}}{\sum_{j=1}^{k_2}\hat{p}_{j}+\frac{1}{q}\sum_{j=k_2+1}^{m}\hat{p}_{j}}\\
%     &\geq 0   
% \end{align*}


% which implies $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}_1)$. Similarly, we can prove $R(f^{PAA},\hat{\vp}_1)\leq R(f^{PAA},\hat{\vp}_2)$. Define $\hat{\vp}_3=(a,0,\cdots,0,1-a-b,0,\cdots,0,b)$ where the $(k_1+1)$-th entry is $1-a-b$. Define $\hat{\vp}_4=(a,0,\cdots,0,1-a-b,0,\cdots,0,b)$ where the $k_2$-th entry is $1-a-b$. Using the same method, we can prove $R(f^{PAA},\hat{\vp}_2)\leq \max\{R(f^{PAA},\hat{\vp}_3),R(f^{PAA},\hat{\vp}_4)\}$. So $\hat{\vp}^*$ has at most three non-zero entries.

\textbf{Second Step:} Now we prove  $\hat{\vp}^*$ has exactly two non-zero entries. Without loss of generality, we now suppose there are only three ratings: $1,k,m$, where $1<k<m$. For $\hat{\vp}=(a,1-a-b,b)$, suppose the optimal index for minimum is $k_1(\hat{\vp}) \in \{1,2\}$ and the optimal index for maximum is $k_2(\hat{\vp}) \in \{1,2\}$. Still we have $k_1(\hat{\vp})\leq k_2(\hat{\vp})$. Define $\hat{\vp}_5=(a,0,1-a),\hat{\vp}_6=(1-b,0,b)$. There are three cases.

\textbf{Case 1: }
We have
$k_1(\hat{\vp})=k_2(\hat{\vp})=1$. $$l(\hat{\vp})=\frac{a}{a+q(1-a)}+\frac{q(1-a-b)}{a+q(1-a)}\times k+\frac{qb}{a+q(1-a)}\times m$$
$$u(\hat{\vp})=\frac{qa}{qa+1-a}+\frac{1-a-b}{qa+(1-a)}\times k+\frac{b}{qa+(1-a)}\times m$$ $$l(\hat{\vp}_5)=\frac{a}{a+q(1-a)}+\frac{q(1-a)}{a+q(1-a)}\times m$$ 
$$u(\hat{\vp}_5)=\frac{qa}{qa+(1-a)}+\frac{1-a}{qa+(1-a)}\times m$$

Then
\begin{align*}
    & \quad \ (u(\hat{\vp}_5)-l(\hat{\vp}_5))-(u(\hat{\vp})-l(\hat{\vp}))\\
    &=(u(\hat{\vp}_5)-u(\hat{\vp}))-(l(\hat{\vp}_5)-l(\hat{\vp}))\\
    &=\frac{1-a-b}{qa+(1-a)}\times k- \frac{q(1-a-b)}{a+q(1-a)}\times k\\
    &=k(1-a-b)(\frac{1}{qa+(1-a)}-\frac{1}{\frac{a}{q}+(1-a)})\\
    &\geq 0.
\end{align*}
So $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}^{5})$.

\textbf{Case 2: }$k_1(\hat{\vp})=k_2(\hat{\vp})=2$. Similarly, we can prove $R(f^{PAA},\hat{\vp})\leq R(f^{PAA},\hat{\vp}_{6})$.

\textbf{Case 3: }$k_1(\hat{\vp})=1, k_2(\hat{\vp})=2$. $$l(\hat{\vp})=\frac{a}{a+q(1-a)}+\frac{q(1-a-b)}{a+q(1-a)}\times k+\frac{qb}{a+q(1-a)}\times m$$
$$u(\hat{\vp})=\frac{qa}{b+q(1-b)}+\frac{q(1-a-b)}{b+q(1-b)}\times k+\frac{b}{b+q(1-b))}\times m$$
Define $$A=\frac{q(1-b)}{b+q(1-b)}+\frac{bm}{b+q(1-b)}-(1+\frac{qb(m-1)}{a+q(1-a)}),$$
$$B=1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)}-(\frac{a}{a+q(1-a)}+\frac{q(1-a)m}{a+q(1-a)}).$$

Since $u(\hat{\vp})-l(\hat{\vp})$ is linear on $k$, we have $$u(\hat{\vp})-l(\hat{\vp})\leq \max\{A,B\}.$$

Notice $$u(\hat{\vp}_6)=\frac{q(1-b)}{b+q(1-b)}+\frac{bm}{b+q(1-b)},\ l(\hat{\vp}_5)=\frac{a}{a+q(1-a)}+\frac{q(1-a)m}{a+q(1-a)}.$$ 

So
\begin{comment}
    \begin{footnotesize}
    $$l(\hat{\vp}^{'})-u(\hat{\vp}^{'})\leq \max\{u(\hat{\vp}_6)-(1+\frac{qb(m-1)}{a+q(1-a)}),\ 1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)}-l(\hat{\vp}_5)\}$$
\end{footnotesize}
\end{comment}

$$A=u(\hat{\vp}_6)-(1+\frac{qb(m-1)}{a+q(1-a)}),\ B=1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)}-l(\hat{\vp}_5)$$



    


\begin{align*}
    &\quad \; l(\hat{\vp}_6)-(1+\frac{qb(m-1)}{a+q(1-a)})\\
    &=\frac{1-b}{qb+1-b}+\frac{qbm}{qb+1-b}-(1+\frac{qb(m-1)}{a+q(1-a)})\\
    &=1+\frac{qb(m-1)}{qb+1-b}-((1+\frac{qb(m-1)}{a+q(1-a)}))\\
    &=qb(m-1)(\frac{1}{qb+1-b}-\frac{1}{a+q(1-a)})\\
    &=\frac{qb(m-1)}{(qb+1-b)(a+q(1-a))}(a+q(1-a)-(qb+1-b))\\
    &=\frac{qb(m-1)}{(qb+1-b)(a+q(1-a))}(q-1)(1-a-b)\\
    &\leq 0
\end{align*}
\begin{align*}
    &\quad \; u(\hat{\vp}_5)-(1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)})\\
    &=\frac{qa}{qa+(1-a)}+\frac{(1-a)m}{qa+(1-a)}-\left(1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)}\right)\\
    &=1+\frac{(1-a)(m-1)}{qa+(1-a)}-\left(1+\frac{(q(1-a-b)+b)(m-1)}{b+q(1-b)}\right)\\
    &=(m-1)(\frac{1-a}{qa+(1-a)}-\frac{q(1-a-b)+b}{b+q(1-b)})\\
    &=(m-1)\left(1-\frac{qa}{qa+(1-a)}-(1-\frac{qa}{b+q(1-b)})\right)\\
    &=qa(m-1)(\frac{1}{b+q(1-b)}-\frac{1}{qa+(1-a)})\\
    &=\frac{qa(m-1)}{(qa+(1-a))(b+q(1-b))}(qa+(1-a)-b+q(1-b))\\
    &=\frac{qa(m-1)}{(qa+(1-a))(b+q(1-b))}(1-q)(1-a-b)\\
    &\geq 0.
\end{align*}
So $A \leq u(\hat{\vp}_6)-l(\hat{\vp}_6), \ B \leq u(\hat{\vp}_5)-l(\hat{\vp}_5)$, which implies
$$u(\hat{\vp})-l(\hat{\vp})\leq \max\{u(\hat{\vp}_6)-l(\hat{\vp}_6),u(\hat{\vp}_5)-l(\hat{\vp}_5)\},$$ i.e., $$R(f^{PAA},\hat{\vp})\leq \max\{R(f^{PAA},\hat{\vp}_5),R(f^{PAA},\hat{\vp}_6)\}.$$

Putting the three pieces together, we have $$R(f^{PAA},\hat{\vp})\leq \max\{R(f^{PAA},\hat{\vp}_5),R(f^{PAA},\hat{\vp}_6)\},$$
which implies $\hat{\vp}^{*}$ has exactly two non-zero entries.

\textbf{Third Step:} Next, we prove  $\hat{\vp}^* = (\frac{1}{2},0,\cdots,0,\frac{1}{2})$. For $\hat{\vp}=(a,0,\cdots,0,1-a),0\leq a \leq 1$, $l(\hat{\vp})=\frac{a}{a+q(1-a)}+\frac{q(1-a)}{a+q(1-a)}\times m,\ u(\hat{\vp})=\frac{qa}{qa+(1-a)}+\frac{1-a}{qa+(1-a)}\times m$, so \begin{align*}
    u(\hat{\vp})-l(\hat{\vp})&=(m-1)\times \left(\frac{1-a}{qa+(1-a)}-\frac{q(1-a)}{a+q(1-a)}\right)\\
    &=(m-1)\times \frac{(1-q^2)(a-a^2)}{(1-q)^2(a-a^2)+q}\\
    &=(m-1)\times \frac{(1+q)(a-a^2)}{(1-q)(a-a^2)+\frac{q}{1-q}}.
\end{align*}

Let $x=a-a^2 \in [0,\frac{1}{4}],\ F(x)=\frac{u(\hat{\vp})-l(\hat{\vp})}{m-1}=\frac{(1+q)x}{(1-q)x+\frac{q}{1-q}}$. Then \begin{align*}
    F'(x)&=\frac{(1+q)\left((1-q)x+\frac{q}{1-q}\right)-(1-q)(1+q)x}{\left((1-q)x+\frac{q}{1-q}\right)^2}\\
    &=\frac{q(1+q)}{(1-q)\left((1-q)x+\frac{q}{1-q}\right)^2}\\
    &>0.
\end{align*}

So $x=\frac{1}{4}$ uniquely maximizes the regret. Since $x=\frac{1}{4} \iff a=\frac{1}{2}$, we prove $\hat{\vp}^{*} = (\frac{1}{2},0,\cdots,0,\frac{1}{2})$.

\textbf{Fourth Step:} Finally, it is easy to figure out the optimal index for minimum $k_1(\hat{\vp}^{*})$ and the optimal index for maximum $k_2(\hat{\vp}^{*})$ can both be any value in $[m-1]$. Without loss of generality, we set $k_1(\hat{\vp}^{*})=1, k_2(\hat{\vp}^{*})=m-1$. So the corresponding worst pair of information structure is 
    \begin{itemize}
        \item $\theta_1=(\vp_1=[\frac{1}{q+1},0,\cdots,0,\frac{q}{q+1}],\vg_1=[q,1,\cdots,1,1])$
        \item $\theta_2=(\vp_2=[\frac{q}{q+1},0,\cdots,0,\frac{1}{q+1}],\vg_2=[1,1,\cdots, 1,q])$
    \end{itemize}
\end{proof}


\begin{comment}
    \begin{theorem}
When $m=o\left((\frac{n}{\ln n})^{\frac{1}{4}}\right)$,
$$\ R(f^{PAA}^{PAA},\Theta)\le R(f^{PAA},\Theta)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$ 
for any aggregator $f$.
\end{theorem}
\end{comment}


\begin{proof}[Proof of ~\Cref{thm:finite}]
    For clarity, we define $R_n(f^{PAA},\vp,\vg)$, which is the regret of PAA when the sample size is $n$. Suppose $f^*_n$ is the optimal aggregator when sample size is $n$. It is equivalent to prove $$\max_{\vp,\vg} R_n(f^{PAA},\vp,\vg)\le \max_{\vp,\vg} R_n(f^*_n,\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$

    Use $R_n(f^*)$ to denote the optimal regret when the sample size is $n$, and $R^*$ is the optimal regret when $n \to \infty$. First Notice $R_n(f^*)$ is a non-increasing function with respect to $n$ since we can always choose the aggregator which only uses part of the ratings, so $R_n(f^*)\geq R^*$. Then We bound the difference between $R_n(f^{PAA},\vp,\vg)$ and $R^*$ by concentration inequality. 
    
    For any $(\vp,\vg)$, when $n \to \infty$, we obtain $\hat{\vp}=\frac{\vp \vg}{\sum_j{p_jg_j}}$ and  $$R_{\infty}(f^{PAA},\vp,\vg)=(f^{PAA}(\hat{\vp})-\E_{\rx\sim\vp}[\rx])^2=(\frac{u(\hat{\vp})+l(\hat{\vp})}{2}-\E_{\rx\sim\vp}[\rx])^2$$
    
    In the finite case, we get a noisy distribution $\hat{\vp}^{'}$. We use $n_r$ to denote the number of people whose rating is $r$ and $\hat{n}_r$ to denote the number of people who actually report $r$. 
    \begin{small}
        $$R_n(f^{PAA},\vp,\vg)=\E[(f^{PAA}(\hat{\vp}^{'})-\frac{\sum_r{rn_r}}{n}]=\E[(\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\frac{\sum_r{rn_r}}{n})^2]$$
    \end{small}
    
    
    By chernoff bound, we have $$\Pr[|n_r-E[n_r]|\geq \sqrt{n\ln n}]\leq 2e^{-\frac{2n\ln n}{n}}=\frac{2}{n^2}$$
    $$\Pr[|\hat{n}_r-E[\hat{n}_r]|\geq \sqrt{n\ln n}]\leq 2e^{-\frac{2n\ln n}{n}}=\frac{2}{n^2}$$
    $$\Pr[|\sum_r \hat{n}_r-E[\sum_r \hat{n}_r]|\geq \sqrt{n\ln n}]\leq 2e^{-\frac{2n\ln n}{n}}=\frac{2}{n^2}$$
    
    Define event $A_r=\{|n_r-E[n_r]|\geq \sqrt{n\ln n}\}$, event $A=\bigvee_{r=1}^{m}A_r$, event $B_r=\{|\hat{n_r}-E[\hat{n}_r]|\geq \sqrt{n\ln n}\}$, event $B=\bigvee_{r=1}^{m}B_r$, event $C=\{|\sum_r \hat{n}_r-E[\sum_r \hat{n}_r]|\geq \sqrt{n\ln n}\}$.  By union bound, we have $$
    \Pr[A\vee B \vee C]\leq \frac{4m+2}{n^2} \leq \frac{1}{n}
    $$

    \textbf{Case 1: } $A$ happens or $B$ happens or $C$ happens. $$\E\left[(\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\frac{\sum_r{rn_r}}{n})^2 \ | \ A\vee B \vee C\right]=O(m^2)$$

    \textbf{Case 2: } None of $A,B,C$ happens. We have \begin{align*}
        \left|\hat{p}_{r}^{'}-\hat{p}_{r}\right|&=\left|\frac{\hat{n}_r}{\sum_j{\hat{n}_j}}-\frac{p_rg_r}{\sum_j{p_jg_j}}\right|\\
        &=\left|\frac{E[\hat{n}_r]+O(\sqrt{n\ln n})}{E[\sum_j{\hat{n}_j}]+O(\sqrt{n\ln n})}-\frac{p_rg_r}{\sum_j{p_jg_j}}\right|\\
        &=\left|\frac{p_rg_rn+O(\sqrt{n\ln n})}{\sum_j{p_jg_j}n+O(\sqrt{n\ln n})}-\frac{p_rg_r}{\sum_j{p_jg_j}}\right|\\
        &=O\left(\sqrt{\frac{\ln n}{n}}\right)
    \end{align*}

    When $n$ is large enough, this error will not influence the optimal index. So
    \begin{small}
        \begin{align*}
        &\quad  \left|l(\hat{\vp}^{'})-l(\hat{\vp})\right|\\
        &=\left| \frac{\frac{1}{q}\sum_{j=1}^{k_1}j\hat{p}_{j}^{'}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}^{'}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}^{'}+\sum_{j=k_1+1}^{m}\hat{p}_{j}^{'}}-\frac{\frac{1}{q}\sum_{j=1}^{k_1}j\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}}\right|\\
        &=\left|\frac{\frac{1}{q}\sum_{j=1}^{k_1}j\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}+O\left(m\sqrt{\frac{\ln n}{n}}\right)}-\frac{\frac{1}{q}\sum_{j=1}^{k_1}j\hat{p}_{j}+\sum_{j=k_1+1}^{m}j\hat{p}_{j}}{\frac{1}{q}\sum_{j=1}^{k_1}\hat{p}_{j}+\sum_{j=k_1+1}^{m}\hat{p}_{j}}\right|\\
        &=O\left(m^2\sqrt{\frac{\ln n}{n}}\right)
    \end{align*}
    \end{small}
    

    Similarly, $\left|u(\hat{\vp}^{'})-u(\hat{\vp})\right|=O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$, so $$\left|\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\frac{u(\hat{\vp})+l(\hat{\vp})}{2}\right|=O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$
    
    Notice 
    \begin{footnotesize}
    $$\left|\frac{\sum_r rn_r}{n}-\E_{\rx\sim\vp}[\rx]\right|=\left|\frac{\sum_r rn_r}{n}-\frac{\sum_r r\E[n_r]}{n}\right|=\left|\frac{\sum_r r(n_r-\E[n_r])}{n}\right|=O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$ 
    \end{footnotesize}
    
    Then we have 
    \begin{align*}
        & \quad \E\left[(\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\sum_r{rn_r})^2 \ | \  \neg (A\vee B \vee C)\right]\\
        &=\left(\frac{u(\hat{\vp})+l(\hat{\vp})}{2}-\E_{\rx\sim\vp}[\rx]+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\right)^2\\
        &= R_{\infty}(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)
    \end{align*}
    

    Putting the two pieces together, we have
    \begin{footnotesize}
        \begin{align*}
        &\quad R_n(f^{PAA},\vp,\vg)\\
        &= \Pr[A\vee B \vee C] \ \E\left[(\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\frac{\sum_r{rn_r}}{n})^2 \ | \ A\vee B \vee C\right]\\
        &+ \Pr[\neg(A\vee B \vee C)] \ \E\left[(\frac{u(\hat{\vp}^{'})+l(\hat{\vp}^{'})}{2}-\frac{\sum_r{rn_r}}{n})^2 \ | \ \neg(A\vee B \vee C)\right]\\
        &=\Pr[A\vee B \vee C] \ O(m^2)+ \Pr[\neg(A\vee B \vee C)] \left(R_n(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\right)\\
        &\leq \frac{m^2}{n}+(1-\frac{1}{n})\left(R_n(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\right)\\
        &=R_{\infty}(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)
        \end{align*}
    \end{footnotesize}
        
    
    
Since for any $\vp,\vg$ the inequality holds, we have $$\max_{\vp,\vg} R_n(f^{PAA},\vp,\vg)\leq \max_{\vp,\vg} R_{\infty}(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$

So for any sample size $n$,
\begin{align*}
    \max_{\vp,\vg} R_n(f^{PAA},\vp,\vg)&\leq \max_{\vp,\vg} R_{\infty}(f^{PAA},\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\\
    &=R^*+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\\
    &\leq R_n(f^*)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\\
    &= \max_{\vp,\vg } R(f_n^*,\vp,\vg)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)
\end{align*}

The first equality holds since $f^{PAA}$ is the optimal aggregator when $n \to \infty$.

\begin{comment}
    Since for any $\vp,\vg$ the inequality holds, we have $$R_n(f^{PAA},\Theta)\leq R_{\infty}(f^{PAA},\Theta)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)$$.
    
    So
    \begin{align*}
        R(f^{PAA},\Theta)&\leq R_{\infty}(f^{PAA},\Theta)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\\
        &\leq R_{\infty}(f^*,\Theta)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)\\
        &\leq R(f^*,\Theta)+O\left(m^2\sqrt{\frac{\ln n}{n}}\right)
    \end{align*}

    The second inequality holds since $f^{PAA}$ is the optimal aggregator when $n \to \infty$.
\end{comment}
    
\end{proof}



\begin{comment}
    \begin{lemma}
    Fix $(x,y)$, $F(q)=\left|f_{q}^{PAA}(x,y)-\frac{m+1}{2}\right|$ is a monotonically increasing function with respect to $q \ (0 \leq q\leq 1)$ where $x=\sum_i \mathbbm{1}(\hat{x}_i=1),y=\sum_i \mathbbm{1}(\hat{x}_i=m)$.
\end{lemma}
    

\begin{proof}
    $$f_{q}^{PAA}(x,y)=1+\frac{m-1}{2}(\frac{y}{\frac{x}{q}+y}+\frac{y}{qx+y})$$
    
    \begin{align*}
        f_{q}^{PAA}(x,y)-\frac{m+1}{2}&=\frac{m-1}{2}(\frac{y}{\frac{x}{q}+y}+\frac{y}{qx+y}-1)\\
        &=\frac{m-1}{2}\left(\frac{(1/q+q)xy+2y^2}{x^2+y^2+(1/q+q)xy}-1\right)\\
        &=\frac{m-1}{2}\left(\frac{y^2-x^2}{x^2+y^2+(1/q+q)xy}\right)\\
    \end{align*}
    
    When $x\leq y, f_{q}^{PAA}(x,y)-\frac{m+1}{2}\geq 0, F(q)=f_{q}^{PAA}(x,y)-\frac{m+1}{2}$.\\
    
    $$\frac{dF}{dq}=\frac{m-1}{2}\frac{(\frac{1}{q^2}-1)(y^2-x^2)xy}{(x^2+y^2+txy)^2}\geq 0$$

    Similarly, when $x > y, f_{q}^{PAA}(x,y)-\frac{m+1}{2} < 0, F(q)=\frac{m+1}{2}-f_{q}^{PAA}(x,y)$.\\
    
    $$\frac{dF}{dq}=\frac{m-1}{2}\frac{(\frac{1}{q^2}-1)(x^2-y^2)xy}{(x^2+y^2+txy)^2}\geq 0$$

    So $F(q)=\left|f_{q}^{PAA}(x,y)-\frac{m+1}{2}\right|$ is a monotonically increasing function with respect to $q \ (0 \leq q\leq 1)$.
\end{proof}

\begin{corollary}
    When $n\to\infty$, the regret of simple average aggregator is $\left(\frac{(m-1)(1-\sqrt{q})}{(1+\sqrt{q})}\right)^2$. In addition, the corresponding worst pair of information structure is 
    \begin{itemize}
        \item $\theta_1=(\vp_1=[\frac{1}{\sqrt{q}+1},0,\cdots,0,\frac{\sqrt{q}}{\sqrt{q}+1}],\vg_1=[q,1,\cdots,1,1])$
        \item $\theta_2=(\vp_2=[\frac{\sqrt{q}}{\sqrt{q}+1},0,\cdots,0,\frac{1}{\sqrt{q}+1}],\vg_2=[1,1,\cdots, 1,q])$
    \end{itemize}
    
    %That is, $$R(f^{SA},\theta_1)=R(f^{SA},\theta_2)=\left(\frac{(m-1)(1-\sqrt{q})}{(1+\sqrt{q})}\right)^2.$$
\end{corollary}

The proof is similar to \Cref{lem:PAA}.
\end{comment}








\begin{comment}
    $$sa(x,y)=\frac{x+my}{x+y}=1+\frac{m-1}{2}\frac{2y}{x+y}$$
    \begin{align*}
        f(x,y)-sa(x,y)&=\frac{m-1}{2}(\frac{y}{\frac{x}{q}+y}+\frac{y}{qx+y}-\frac{2y}{x+y})\\
        &=\frac{m-1}{2}\left(\frac{(1/q+q)x+2y}{x^2+y^2+(1/q+q)xy}-\frac{2y}{x+y}\right)
    \end{align*}
    \begin{align*}
        F(t)&=f_{q}^{PAA}(x,y)-\frac{m+1}{2}\\
        &=\frac{m-1}{2}\left(\frac{y^2-x^2}{x^2+y^2+txy}\right)\\
    \end{align*}
\end{comment}

\section{Omitted Figures}
\label{sec:fig}


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/m=3/N=20_full.pdf}
        \caption{$n=20$, $m=3$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/m=3/N=10_full.pdf}
        \caption{$n=10$, $m=3$}
    \end{subfigure}
    
    \vspace{0.01\textheight} 
    
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/m=5/N=20_full.pdf}
        \caption{$n=20$, $m=5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{picture/m=5/N=10_full.pdf}
        \caption{$n=10$, $m=5$}
    \end{subfigure}
    
    \caption{Simple averaging vs. BEA for different sample size $n$ and the number of rating categories $m$. The x-axis is the lower bound of the participation probability, $q$, and the y-axis is the natural logarithm of the regret. The regret of BEA almost matches the theoretical lower bound for a wide range of $q$.}
    \label{fig:full}
\end{figure}



\begin{comment}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.47\textwidth,keepaspectratio]{picture/a.pdf}
  \caption{The value of $a^*$ for different participation parameter $q$. The x-axis is $q$ and the y-axis is the value of $a^*$. We assume $a^*\ge 1/2$ since $a*$ and $1-a^*$ are both optimal due to the symmetry. $a^*$ increases as $q$ and $n$ decreases.}
  \label{fig:a}
\end{figure}
\end{comment}

\Cref{fig:full} are the regret of different aggregators with $q$ ranging from $0.01$ to $0.99$ with step $0.01$. Notice when $q$ is large (the threshold is around $0.82$ for $n=10$ and $0.91$ for $n=20$), the regret of BEA deviates from the theory lower bound and even exceeds the regret of simple average aggregator when $q$ is larger. The reason may be that the information structure we construct in \Cref{lem:lower}is not worst for BEA. Instead, the worst information structure will become $$\theta=(\vp=[b,0,\cdots,0,1-b],\vg_1=[q,q,\cdots, q,q])$$ where $b$ is a parameter. 



\begin{figure}[H]
  \centering
  \includegraphics[width=0.45\textwidth,keepaspectratio]{picture/linear.pdf}
  \caption{How the regret decreases with $n$ when $m=2,q=0.5,p=(0.67,0.33)$.}
  \label{fig:linear}
\end{figure}

\Cref{fig:linear} shows the regret of BEA and PAA with different number of agents $n$, ranging from $20$ to $100$. We set $m=2$, the rating distribution $p=(0.67,0.33)$ and lower bound of participation probability $q=0.5$. The regret is approximately equal to $k/n+b$ where $k,b$ are two constants. Both the worst-case regrets of BEA and PAA are proportional to $(m-1)^2$ when $n\to\infty$.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth,keepaspectratio]{picture/data.pdf}
  \caption{Distribution of 96,646 survey/true ratings and 48,826 posted/observed ratings.}
  \label{fig:data}
\end{figure}



