\section{Introduction}

Suppose you come back from a hotel stay that left you frustrated. The service was terrible, and the noise kept you up all night. The experience was bad enough that, as soon as you got home, you felt compelled to go online and leave a negative review. This is not something you normally do—in fact, the last time you stayed at a hotel that was just okay, you did not even bother to rate it. 

Now, imagine you're on the other side, trying to decide between two hotels on an online platform. Both have identical average ratings, but as shown in \Cref{fig:hotel}, their rating distributions are quite different. Which one would you pick?

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{picture/hotel.pdf}
    \caption{The observed ratings of two hotels.}
    \label{fig:hotel}
\end{figure}

While many people rely on average ratings to make their choices, these scores can be misleading due to biases in the way they are collected. One such bias is participation bias, where the ratings do not represent the views of all guests—e.g., only the ones who felt strongly enough to leave feedback \cite{bhole2017effectiveness,zhu2022bias}. 

%Looking at the figure, empirical studies suggest that guests who either loved or hated their experience are more likely to leave a review, while those who found their stay to be just average may skip rating it altogether \gyk{add citation here}.

At Hotel A, reviews are polarized: some guests, possibly those staying in the premium rooms, report excellent stays with 5-star ratings, while others leave very negative feedback. In contrast, Hotel B’s reviews are consistently around 4 stars, indicating generally positive but not outstanding experiences. This suggests that Hotel A's displayed rating might be inflated, as guests with neutral opinions may have skipped reviewing, whereas Hotel B’s balanced ratings offer a more accurate reflection. \Cref{fig:hotel2} shows a possible way of the true distribution.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{picture/hotel2.pdf}
    \caption{The true ratings of two hotels.}
    \label{fig:hotel2}
\end{figure}

Such participation bias affects other areas too, like course evaluations, movie reviews, and product ratings on e-commerce platforms, where only extreme opinions tend to be represented \cite{hu2009online,koh2010online,bhole2017effectiveness,zhu2022bias}.


If we could estimate the likelihood of participation across different rater groups, we could apply traditional methods like importance sampling to approximate the true average rater ratings \cite{tokdar2010importance,tabandeh2022review,vogel2020weighted}. However, without data from non-participants, quantifying the precise extent of this bias remains challenging. Therefore, we need to use alternative metrics to evaluate the aggregation methods without full knowledge of the participation bias. Following the field of robustness analysis \cite{arieli2018robust}, we employ a robust aggregation paradigm aimed at minimizing the worst-case error to mitigate the participation bias. 

\subsection{Problem Statement}
For all positive integer $k$, $[k]$ denotes the set $\{1,2,\cdots,k\}$.\ 

We aim to find a robust function $f$ to aggregate the discrete ratings from $n$ raters. Each rating $\rx_i\in [m]$ follows an underlying distribution $\vp=(p_1,p_2,\cdots,p_m)$ and we want to estimate the expectation $\mu=\E_{\rx\sim\vp}[\rx]$. We assume the ratings are independent. Otherwise, the correlations between ratings may make it impossible for any aggregator to recover the true expectation. Under this assumption, the best estimator for $\mu$ is the empirical average of all ratings $\frac{1}{n}\sum_i x_i$. We denote all the reports by $\rvx=(x_1,\cdots,x_n)$.

However, due to the participation bias, raters will report their ratings with probabilities that depend on the rating values. Let \(\vg = (g_1, g_2, \dots, g_m)\) represent these probabilities, where \(g_{r}\) is the probability of an rater reporting her rating when it is \(r\). We assume raters are homogeneous and have the same participation bias. As a result, only a subset of elements in \(\rvx\) is observed, which we denote by \(\hat{\rvx}\). For each rater \(i\):
\[
\hat{\rx}_i = 
\begin{cases}
{\rx}_i & \text{with probability } g_{{\rx}_i} \\
0 & \text{otherwise}
\end{cases}
\]
where \(0\) indicates that the rating is unobserved. Let $D_{\vp}$ denote the distribution generating $\rvx$ and $D_{\vp,\vg}$ denote the distribution generating $\hat{\rvx}$. We also call $D_{\vp,\vg}$ the information structure.

Without any information on the likelihood of reporting, the worst-case scenario would be no one reports, resulting in a lack of any useful aggregator. Thus, we assume there exists a known parameter $q$, indicating the lowest reporting probability among raters. That is, $g_r\in [q,1]$ for any rating $r$. When $q=1$, there is no participation bias and as $q$ decreases, the bias may increase accordingly. We use an example to illustrate our model.

\begin{example}[Why direct average is not good]
Consider a binary rating system where \( x_i \in \{1, 2\} \) with equal probability. Here, let \( g_1 = 1 \) and \( g_2 = q \), meaning that negative reviews (\( x_i = 1 \)) are always observed, while positive reviews (\( x_i = 2 \)) are observed only with probability \( q \).

Under this setup, even with a large sample size \( n \), the direct average of observed ratings will tend toward \( \frac{1+2q}{1+q} \), which significantly underestimates the true expectation of \( 3/2 \) when \( q \) is small.


\end{example}

%Consider the ideal aggregator $\frac{1}{n}\sum_i x_i$, which fully observes the ratings. its error is $\frac{1}{n}\mathrm{Var}(\vp)$, indicating that for small $n$, even the ideal aggregator performs bad. Thus we will optimize the regret compared to the aggregator in the worst case:

We measure the performance of an aggregator by its expected squared error, defined as \(L(f,\vp,\vg)=\E_{\hat{\rvx} \sim D_{\vp, \vg}} \left[(f(\hat{\rvx}) - \mu)^2\right]\). One objective could be finding \(f\) that minimizes the worst-case error: $\max_{\vp,\vg}L(f,\vp,\vg)$. However, when the sample size $n$ is small, selecting $\vp$ with high variances can lead to large errors even for the ideal aggregator who observes \(\vx\). Since we focus on addressing the errors attributable to uncertainties in $\vg$, we adopt a robust approach and instead minimize the regret relative to the ideal aggregator who observes the full data \(\vx\), and outputs \(\frac{1}{n} \sum_i x_i\): $$\min_f\max_{\vp,\vg}\E_{\rvx\sim D_{\vp}, \hat{\rvx}\sim D_{\vp,\vg}}[(f(\hat{\rvx})-\mu)^2-(\frac{1}{n}\sum_i {\rx}_i-\mu)^2].$$

%\E_{\hat{\rvx}\sim D_{\vp,\vg}}[(f(\hat{\rvx})-\frac{1}{n}\sum_i {\rx}_i)^2]

For short, we denote $R(f,\vp,\vg)=\E_{\rvx\sim D_{\vp}, \hat{\rvx}\sim D_{\vp,\vg}}[(f(\hat{\rvx})-\mu)^2-(\frac{1}{n}\sum_i {\rx}_i-\mu)^2]$.

Because we have assumed the raters are homogeneous, observing $\hat{\vx}$ is equivalent as observing the histograms $n_u, n_1,n_2,\cdots,n_m$ where $n_r$ counts the number of ratings of $r$ for all $r\in\{1,\cdots,m\}$, and $n_u$ indicates the number of unobserved ratings. That is, the aggregator's input is $n_u, n_1, n_2,\cdots,n_m$. This is applicable in the scenarios when the sample size $n$ is known. For example, in the teaching evaluation, the number of students is known to the instructors but not all students will give feedback.

\paragraph{A variant with unknown $n$} However, in practice, we may not know $n$. For example, in the movie rating platforms, we cannot obtain the number of people watching the movies. Thus, we consider a variant of the above problem where $n$ is unknown. In this case, the aggregation problem remains the same except that the aggregator's input is $n_1,n_2,\cdots,n_m$. 

\subsection{Summary of Results}

We explore the optimal aggregator in two settings, depending on the knowledge of the sample size $n$ (i.e., the number of raters).

\paragraph{The sample size $n$ is known} In \Cref{sec:known}, we assume $n$ is known. We construct a lower bound for the regret by considering a mixture of two specific information structures. Based on the lower bound, we provide a new aggregator, the Balanced Extreme Aggregator (BEA), which is the best response to those two specific information structures. Intuitively, BEA estimates the expected ratings of those unreported raters based on the difference between $n_1$ and $n_m$, the counts of extreme ratings. Then BEA adjusts the observed rating average with the estimated unobserved rating average. \Cref{fig:finite} illustrates the whole process. We illustrate its near-optimal performance numerically in various cases.

\paragraph{The sample size $n$ is unknown} In \Cref{sec:unknown}, we assume $n$ is unknown. Since we do not know how many unobserved ratings there are, the above aggregator is not applicable. Instead, we obtain a new aggregator, the Polarizing-Averaging Aggregator (PAA), and show its optimality when $n$ goes to infinite. Furthermore, we numerically show that PAA performs well for a finite sample size. \Cref{fig:infinite} illustrates the whole process. We create two modified histograms from the original observed data. For the first histogram, we identify a threshold $k_1$, and only keep $q$ fraction of the counts for ratings above $k_1$. In the second histogram, we identify a threshold $k_2$, and only keep $q$ fraction of the counts for ratings below $k_2$. We then calculate the empirical mean for each of these adjusted histograms and output their average. 

\Cref{sec:vir} shows the virtualization of our aggregators, which helps to establish some insights about the aggregators.
We validate our aggregators using both real-world and numerical datasets in \Cref{sec:exp}. The results demonstrate the superiority of our method.



\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth,keepaspectratio]{picture/agg1.pdf}
  \caption{Illustration of the Balanced Extreme Aggregator (BEA) when the sample size $n$ is known. BEA estimates the expected ratings of those unreported raters based on the difference between $n_1$ and $n_m$, the counts of extreme ratings. Then BEA adjusts the observed rating average with the estimated unobserved rating average.}
  \label{fig:finite}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=0.47\textwidth,keepaspectratio]{picture/agg2.pdf}
  \caption{Illustration of the Polarizing-Averaging Aggregator (PAA) when the sample size $n$ is unknown. We create two modified histograms from the original observed data. For the first histogram, we identify a threshold $k_1$, and only keep $q$ fraction of the counts for ratings above $k_1$. In the second histogram, we identify a threshold $k_2$, and only keep $q$ fraction of the counts for ratings below $k_2$. We then calculate the empirical mean for each of these adjusted histograms and output their average. }
  \label{fig:infinite}
\end{figure}