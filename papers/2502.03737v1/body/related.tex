\section{Related Literature}

\paragraph{Rating Aggregation}

Rating aggregation can be defined as the problem of integrating different ratings into a single representative value. Previous works aim to resist individual disturbance from random or malicious spammers \cite{chirita2005preventing,benevenuto2009detecting} by building a reputation
system \cite{resnick2000reputation,fujimura2003reputation,josang2007survey}. The basic idea of reputation system is weighted average where a rater's weight is measured by his reputation score which is calculated by using the rater-object weighed bipartite network. The quality-based methods measure a raters’s reputation by the difference between the rating values and the objects’ weighted average rating values  \cite{laureti2006information,zhou2011robust,liao2014ranking,zhu2023robust}. Unlike quality-based methods, raters are grouped based on their rating similarities and their reputation is calculated by the corresponding group sizes in group-based methods
\cite{gao2015group,gao2017evaluating,fu2021iterative}. Another branch of literature considers how to resist collusive disturbance where malicious raters simultaneously promote or demote the qualities of the targeted objects by giving consistent rating score \cite{mukherjee2012spotting,wang2016detecting,wang2018graph,zhang2020label,zhu2024robust}.

Our work focuses on the simple setting where there is a single object. Thus, we can not use the rater-object weighed bipartite network to learn raters' rating behavioral patterns. We primarily address participation bias, excluding other forms of malicious behavior. Lastly, we adopt an adversarial approach to evaluate the performance of an aggregation scheme.


% Rating aggregation can be defined as the problem of integrating different ratings into a single representative value. Though the simple average method has been performed as an effective way, it can be easily influenced by individual disturbance from random or malicious spammers \cite{chirita2005preventing,benevenuto2009detecting}. To solve this problem, building a reputation
% system is a good way \cite{resnick2000reputation,fujimura2003reputation,josang2007survey}.

% \begin{comment}

% are mostly based on the principle that raters whose ratings often differ
% from the ratings of other raters are assigned less weight.

% with an underlying assumption that every object is associated with a most objective rating that best reflect its quality. 

% group-based methods assume that a rating is reasonable if it follows the most choices in the group.
% \end{comment}

% Previous methods can be divided into two categories: the quality-based methods and the group-based methods. The quality-based methods measure a rater’s reputation by the difference between the rating values and the estimated objects’ quality values. Laureti et al. proposed an iterative refinement (IR) method \cite{laureti2006information}, where a rater’s reputation is inversely proportional to the difference between his ratings and corresponding objects’ weighted average rating vector. Weighted rating of all objects and
% reputation of all raters are recalculated at each step, until both of them become stable. Zhou et al. applied the Pearson correlation to calculate the differences between rater rating and the weighted average rating \cite{zhou2011robust}. Liao et al. further improved the method by introducing a reputation redistribution process and two penalty factors \cite{liao2014ranking}. Zhu et al. propose a new reputation iterative algorithm based on Z-statistics (ZS) to eliminate the effect of thorny
% objects which are difficult to evaluate because of their intrinsic uncertainty and complexity \cite{zhu2023robust}.

% Unlike quality-based methods, raters are grouped based on their rating similarities and their reputation is calculated by the corresponding group sizes in group-based methods.
% Gao et al. proposed a group-based ranking (GR) method that evaluates raters’ reputations based on their grouping behaviors and then then improved the GR method by introducing an iterative reputation-allocation process (IGR) \cite{gao2015group,gao2017evaluating}. raters with higher reputation have higher weights in dominating the corresponding group sizes. Fu et al. also considered the characteristics of the individual ratings based on IGR by giving a higher reputation score to raters with a smaller rating deviation \cite{fu2021iterative}. 


% Another branch of literature considers how to resist collusive disturbance where malicious raters simultaneously promote or demote the qualities of the targeted objects by giving consistent rating score \cite{mukherjee2012spotting,wang2016detecting,wang2018graph,zhang2020label,zhu2024robust}.




\paragraph{Participation Bias}

Participation bias is a well-studied topic within the causal inference community which arises when participants disproportionately possess certain traits that affect participation \cite{elston2021participation}. It is a common source of error in clinical trials and survey studies \cite{dahrouge2019high,schoeler2023participation,schoeler2022correction,fakhouri2020investigation,coon2020evaluating}. 

When participation bias is only related to some observed (measured) variables and other representative samples are available (e.g. the whole population data from government), weighting 
strategies make it possible to create a pseudo-sample
representative for these variables \cite{robins1994estimation,schoeler2022correction} under the assumption that non-participants have equivalent behaviours to participants in the same socio-demographic category. An alternative way is multiple imputation (MI), which is viable when data are missing at random \cite{sterne2009multiple,peytchev2012multiple,alanya2015comparing,gray2013use,gorman2017adjustment}. If the data are thought to be
missing-not-at-random then methods involving sensitivity analyses like pattern mixture modelling \cite{little1993pattern} and NARFCS \cite{tompsett2018use} can be helpful. These methods can also be combined to correct participation bias \cite{gray2020correcting}.
 


Methods used to correct sample selection bias can also be used to correct participation bias \cite{verger2021online}. The first solution to sample selection bias was suggested by \citet{heckman1974shadow} who proposed a maximum likelihood estimator which heavily relies on the normality assumption. One way to relax normality while remaining in the maximum likelihood framework was suggested by \citet{lee1982some,lee1983generalized}. Two-Step estimators are frequently employed in empirical work including parametric models \cite{heckman1976common,heckman1979sample} and semi-parametric models \cite{heckman1985alternative,ahn1993semiparametric,lee1994semiparametric}. Recent works on sample selection models have aimed to address robust alternatives to the
Heckman model \cite{marchenko2012heckman, zhelonkin2016robust, de2022generalized}.

Our work significantly differs from the above literature in a few aspects. First, we are only interested in recovering the average rating rather than the causal effect. Second, we consider a setting where we only know the reported rating and the lower bound of participation rate instead of various socio-demographic data. Finally, we adopt a robust approach to evaluate the aggregation scheme.

\paragraph{Robust Aggregation}
\citet{arieli2018robust} first introduced a robust paradigm for forecast aggregation, aiming to minimize the aggregator's regret compared to an omniscient aggregator. Later, \citet{de2021robust} explored a robust absolute paradigm focused on minimizing the aggregator's own loss. Since then, a growing number of researches have addressed robust information aggregation under various information structures, including the projective substitutes condition~\citep{neyman2022you}, conditional independent setting~\citep{arieli2018robust}, and scenarios involving second-order information~\citep{pan2023robust}. Furthermore, \citet{guo2024algorithmic} provide an algorithmic approach for computing a near-optimal aggregator across general information structures. \citet{kong2024surprising} tackle robust aggregation in the context of base rate neglect, a specific bias in forecast aggregation.

We consider a different setting from the above literature. We focus on addressing participation bias in rating aggregation.



\begin{comment}
    Another weighted

Selection bias is a well-studied topic within the casual inference community. We focus on the sample selection bias that arises when an investigator does not observe a random sample of a population of interest\cite{winship1992models}. 

\subsection{Early Works}
Most Early works aim to recover casual effect under mild conditions. Maximum likelihood estimation and two-Step estimation are the most representative.

\subsubsection{Maximum Likelihood Estimation}
The first solution was suggested by Heckman \cite{heckman1974shadow} who proposed a maximum likelihood estimator which heavily relies on the normality assumption. One way to relax normality while remaining in the maximum likelihood framework was suggested by Lee\cite{lee1982some,lee1983generalized}. To avoid the distributional assumptions required by Lee, Gallant and Nychka\cite{gallant1987semi} employ the general estimation strategy.

\subsubsection{Two-Step Estimation}
Two-Step estimators are frequently employed in empirical work. They can be categorized into three "generations" in considering the two-step procedures\cite{vella1998estimating}. The first fully exploits the parametric assumptions\cite{heckman1976common,heckman1979sample}. The second relaxes the distributional assumptions in at least one stage of estimation\cite{olsen1980least}. The third is semi-parametric in that it relaxes the distributional assumptions\cite{heckman1985alternative,robinson1988root,ahn1993semiparametric,lee1994semiparametric}.

\subsection{Recent Works}
\subsubsection{Regression under Sample Selection Bias}
Recent works on sample selection models have aimed to address robust alternatives to the
Heckman model. \cite{marchenko2012heckman} proposed a Student-t sample selection model for dealing with the robustness to the normal assumption in the Heckman model. \cite{zhelonkin2016robust} proposed a modified robust semi-parametric alternative based on Heckman’s two-step estimation method with asymptotic normality. \cite{ogundimu2016sample} introduced the skew-normal sample selection model to mitigate the remaining effect of skewness after applying a logarithmic transformation to the outcome variable. \cite{de2022generalized} propose a generalization of the Heckman sample selection model by allowing the sample selection bias and dispersion parameters to depend on covariates.


\subsubsection{Classification under Sample Selection Bias}

\subsection{}
Our work significantly differs from that extensive literature in a few aspects. First we are only interested in recovering the average score rather than casual effect and we drop regression since we only know the reported score and the lower bound of participation rate. Finally, instead of making extra assumptions, we adopt an adversarial or robust approach to evaluate the performance of an aggregation scheme.
\end{comment}
