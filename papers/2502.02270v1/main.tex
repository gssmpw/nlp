\documentclass[11pt,a4paper]{amsart}
\pdfoutput=1
\usepackage[margin=3.25cm]{geometry}

\input{src}

\title[{Exact Sequence Classification with Hardmax Transformers}]{Exact Sequence Classification with \\ Hardmax Transformers}

\author[A. Alcalde]{Albert Alcalde\thankssymb{2}}
\email{albert.alcalde@fau.de}

\author[G. Fantuzzi]{Giovanni Fantuzzi\thankssymb{2}}
\email{giovanni.fantuzzi@fau.de}

\author[E. Zuazua]{Enrique Zuazua\thankssymb{1}\thankssymb{2}\thankssymb{3}}
\email{enrique.zuazua@fau.de}

\thanks{\thankssymb{2}Chair for Dynamics, Control, Machine Learning, and Numerics (Alexander von Humboldt Professorship), Department of Mathematics, Friedrich--Alexander--Universit\"at Erlangen--N\"urnberg, 91058 Erlangen, Germany.
}

\thanks{\thankssymb{1}Departamento de Matem\'{a}ticas,
Universidad Aut\'{o}noma de Madrid, 28049 Madrid, Spain.
}

\thanks{\thankssymb{3}Chair of Computational Mathematics, Fundaci\'{o}n Deusto. Av. de las Universidades, 24, 48007 Bilbao, Basque Country, Spain.
}

\begin{document}
\date{\today}
\begin{abstract}
    We prove that hardmax attention transformers perfectly classify datasets of $N$ labeled sequences in $\R^d$, $d\geq 2$. Specifically, given $N$ sequences with an arbitrary but finite length in $\R^d$, we construct a transformer with $\mathcal{O}(N)$ blocks and $\mathcal{O}(Nd)$ parameters perfectly classifying this dataset. Our construction achieves the best complexity estimate to date, independent of the length of the sequences, by innovatively alternating feed-forward and self-attention layers and by capitalizing on the clustering effect inherent to the latter. Our novel constructive method also uses low-rank parameter matrices within the attention mechanism, a common practice in real-life transformer implementations. Consequently, our analysis holds twofold significance: it substantially advances the mathematical theory of transformers and it rigorously justifies their exceptional real-world performance in sequence classification tasks.
\end{abstract}
\maketitle
%
\section{Introduction}\label{sec:introduction}
Transformers \cite{vaswaniAttentionAllYou2017} have revolutionized machine learning by outperforming traditional residual networks (ResNets) in applications such as natural language processing (NLP) \cite{openai2024gpt4technicalreport} and computer vision \cite{dosovitskiy2021imageworth16x16words}, where the input data is a finite sequence of $d$-dimensional vectors. Their practical success relies on their universal approximation properties \cite{yunAreTransformersUniversal2020, alberti2023sumformer} and, most importantly, on the effectiveness of the self-attention layers, which act between feed-forward layers to inject information of the sequence to each component.

The approximation power of feed-forward layers---the heart of ResNets since their inception \cite{kaimanHe_2016}---has been studied extensively, especially in the context of classification problems. For example, \citet{geshkovski2022turnpike} and \citet{domenec2023NODES} established \emph{simultaneous controllability} results showing that sufficiently deep fully-connected ResNets can exactly classify inputs in Euclidean spaces of dimension $d\geq 2$. They also provide explicit upper bounds on the number of ResNet parameters and layers as a function of the input dimension $d$. 

Since the parameters of self-attention layers can always be chosen such that they act as the identity map, these perfect classification results carry over to transformers. However, the parameter estimates obtained in this way depend strongly on the sequence length $n$, whereas in practice transformers have good performance independently of $n$. It is therefore necessary to develop new theory specific to transformers that can elucidate the role of their self-attention layers and, consequently, justify their outstanding practical performance. 

In this work, we take a step in this direction by proving that transformers equipped with hardmax self-attention layers can perfectly classify sequences with a significant reduction in the number of parameters compared to ResNets (see \Cref{sec:main-thm} for a quantitative statement). Specifically, through careful explicit choices of the self-attention parameters, we show that the self-attention layers serve as a dimensionality reduction mechanism that allows transformers to classify sequences using a number of parameters independent of the sequence length. Our constructive strategy alternates self-attention with feed-forward layers and works with rank-1 parameter matrices in the self-attention mechanism, justifying why practical implementations that maintain a low-rank factorization in terms of `key' and `query' matrices work well. Our results potentially explain why transformers outperform ResNets in sequence-based classification tasks.
%
\subsection{Related Work}
Mathematical insight into the inner workings of transformers can be gained by looking at self-attention layers through the lens of interacting particle systems \cite{lu2019understanding,sanderSinkformers2022}. Specifically, one can view the elements of an input sequence as particles that interact through a kernel determined by the self-attention mechanism. \citet{geshkovski2023emergence, geshkovski_mathematical_2023} adopted this perspective to prove asymptotic clustering results for simplified attention-only transformers with shared weights. \citet{alcalde2024clustering} proved further clustering results for a hardmax formulation of self-attention, which allows for a more geometric understanding of the attention mechanism.

\citet{geshkovski2024measure} recently exploited the clustering effect of self-attention layers to prove approximate interpolation results for a class of transformers acting as measure-to-measure mappings. In these transformers, a softmax self-attention acts as a particular interaction mechanism for $d$-dimensional particles on the unit sphere $\mathbb{S}^{d-1}$. This formulation uses a continuous-time interpretation of transformers similar to the so-called neural ODE interpretation of ResNets \cite{Weinan2017APO,NODES2018}. The resulting transformer model is a nonlocal transport equation for measures, with the transformer parameters acting as control inputs. Approximate interpolation of measures in the Wasserstein-2 distance is achieved using explicit piecewise-constant control inputs with a number of switches (corresponding to the number of transformer blocks needed in practice) proportional to $Nd$, where $N$ is the number of measures \citep[Theorem~1.1]{geshkovski2024measure}. Moreover, if the input and target measures are atomic with the same number of atoms, then one can perfectly solve the classification task stated in \Cref{pb:textClass} below using $\mathcal{O}(Nd)$ transformer blocks (see \cite[\S1.4.2]{geshkovski2024measure}).
%
\subsection{Novelty of this Work}
The theoretical results proved in this paper are similar in spirit to those of \citet{geshkovski2024measure}, but with two key differences. First, we analyze a different transformer architecture with a discrete sequence of transformer blocks and no need for a continuous-time model (see \Cref{ss:transArch} for details). In particular, our transformer is a sequence-to-sequence map and uses a hardmax self-attention mechanism. Second, and more important, we use a different strategy to choose the transformer parameters, reducing the number of blocks required to achieve perfect sequence classification to $\mathcal{O}(N)$ independently of the dimension $d$ of the sequence elements. 
%
\section{Transformer-Based Classification}
Our goal is to prove that a simple transformer architecture can perfectly solve the supervised learning problem of sequence classification. This section gives a mathematical statement of this problem, describes the transformer architecture we use to solve it, and presents our main result.
%
\subsection{Problem Statement}\label{ss:pb_statement}
Fix $d,M,N \in \N$ with $d\geq 2$. Let $(\R^d)^n$ be the set of sequences in $\R^d$ with $n$ elements. The set of finite-length sequences in $\R^d$ is
\begin{equation}
    \mathcal{Z} = \bigcup_{n \in \N} (\R^d)^n.
\end{equation}
Let $Z^1,\ldots,Z^N \in \mathcal{Z}$ be sequences to be classified, let $\smash{S^1,\ldots,S^M}$ be disjoint subsets of $\R^d$ representing classification categories or \textit{labels}, and let $c : [N] \to [M]$ be a map that assigns a category $\smash{S^{c(j)}}$ to every sequence $Z^j$. (Here and throughout the paper, $[k]$ denotes the set $\{1,\ldots,k\}$.) The problem we study in this paper can be stated as follows.
\begin{problem}\label{pb:textClass}
    Construct a function $\phi : \mathcal{Z} \to \mathbb{R}^d$ such that $\phi(Z^j) \in S^{c(j)}$ for all $j \in [N]$.
\end{problem}
Two classical examples fit the framework of \Cref{pb:textClass}. One is text sentiment analysis, where the positive and negative sentiment classes are represented by disjoint open sets $S^1$ and $S^2$, each sequence $Z^j$ is a collection of sentences, and its elements (also called \emph{tokens}) are words or sub-words encoded as points in $\R^d$. Another example is exact next-token prediction, where each sequence $Z^j$ represents an incomplete sentence and the sets $S^{i}=\{y^{i}\}$ are singletons encoding words that can be used to complete them. Both problems can be solved exactly with the transformers described in \Cref{ss:transArch}. In contrast, the continuous-time measure-theoretic transformers of \citet{geshkovski2024measure} can solve the sentiment analysis problem, but not the exact next-token prediction one unless the target tokens $\{y^{c(j)}\}_{j\in[N]}$ are pairwise distinct (see \cite[\S1.4.2]{geshkovski2024measure}).
%
\subsection{Transformer Architecture}\label{ss:transArch}
We solve \Cref{pb:textClass} by taking $\phi = \mathrm{R} \circ \T$ to be the composition of a transformer $\T: \mathcal{Z} \to \mathcal{Z}$ and a so-called `readout map' $\mathrm{R}: \mathcal{Z} \to \mathbb{R}^d$, which we describe below. We denote by $\len(Z)$ the length of a sequence $Z$.

The readout map is defined as
\begin{equation}\label{eq:readout}
    \mathrm{R}(Z) = \frac{1}{\len(Z)} \sum_{i=1}^{\len(Z)} z_i.
\end{equation}
The transformer is instead a composition of \emph{transformer blocks}, in which a feed-forward (FF) layer with a residual connection is followed by a self-attention (SA) layer with a hardmax attention mechanism that may or may not include a residual connection. We define these layers in detail next.
\vspace{3mm}

\noindent\textit{Feed-Forward Layers.} Given $d'\in \N$, feed-forward layers are functions $\FF : \mathcal{Z} \to \mathcal{Z}$ parametrized by $W \in \R^{d \times d'}$, $U \in \R^{d'\times d}$, $b\in \R^{d'}$, and an activation function $\sigma$ acting element-wise on vectors. The $i$-th component of the sequence $\FF(Z)=\{\FF_1(Z),\ldots,\FF_{\len(Z)}(Z)\}$ is given by
\begin{equation}
\FF_i(Z) = z_i + W\sigma(U z_i + b).
\end{equation}
In this work, we fix $\sigma(x) = \max (0,x)$ (the ReLU function).
\begin{remark}
    ResNets can also be used to solve \Cref{pb:textClass} if all input sequences $Z^j$ have the same length $n$. However, the FF layers in these ResNets act on a sequence $Z$ after `flattening' it to a vector in $\R^{nd}$. Therefore, they require a number of parameters linearly proportional to the sequence length $n$. In contrast, the FF layers used in transformers act independently on each token in the sequence $Z$ and require a number of parameters independent of the sequence length. In fact, they can act on sequences of arbitrary (but finite) length. This complexity reduction is a distinguishing feature of transformers and is key to our analysis.
\end{remark}
\begin{figure}
    \centering
    \includegraphics[width=0.25\linewidth]{figures/hardmax-attention.pdf}
    \caption{Geometric interpretation of \Cref{eq:selfatt_c} for $i=1$ with $A=I$. Tokens $z_2$ and $z_3$ have the largest orthogonal projection on the direction of $Az_1 = z_1$, so $\mathcal{C}_i(Z,A) = \{ 2,3 \}$.}
    \label{fig:hm_att}
\end{figure}
\vspace{3mm}

\noindent\textit{Self-Attention Layers.} 
Self-attention layers are functions $\mathrm{SA} :\mathcal{Z} \to \mathcal{Z}$ parametrized by $\rho \in \R$ and matrices $V, A \in \R^{d \times d}$ as follows. For every token index $i \in \len(Z)$, set 
\begin{subequations}\label{eq:selfatt}
\begin{gather}
\label{eq:selfatt_c}
    \mathcal{C}_i(Z,A) = \left\{ j :\;  \langle A z_i, z_j \rangle = \max_{\ell \in \len(Z)} \langle A z_i, z_\ell \rangle \right\}
    \\
\label{eq:selfatt_b}
\text{and}\quad
    \Lambda_{i\ell}(Z, A) = 
    \begin{cases}
        \frac{1}{\abs{ \mathcal{C}_i(Z,A) }} &\text{if } z_\ell \in \mathcal{C}_i(Z,A),\\
        \;\, 0 &\text{otherwise}.
    \end{cases}
\end{gather}
Then, the sequence $\mathrm{SA}(Z) = \{\mathrm{SA}_1(Z),\dots, \mathrm{SA}_{\len(Z)}(Z)\}$ is determined component-wise using the formula
\begin{equation}
\label{eq:selfatt_a}
    \mathrm{SA}_i(Z) = \rho z_i + V \sum_{\ell = 1}^{\len(Z)} \Lambda_{i\ell}(Z,A)z_\ell.
\end{equation}
\end{subequations}
\begin{remark}
    We choose the hardmax attention mechanism because of its simple geometric interpretation: a token $z_i$ is influenced by the tokens with the largest orthogonal projection onto the direction of $A z_i$ (cf. \Cref{fig:hm_att}). 
    A regularized `softmax' attention is usually preferred in practice because it allows for training with standard gradient-based algorithms, but one expects the two models to behave very similarly when the regularization parameter in the softmax attention is small enough. \citet{alcalde2024clustering} verified this expectation for the classification problem of sentiment analysis.
\end{remark}
\vspace{3mm}

\noindent\textit{The Transformer.}
We are now ready to combine FF layers and SA layers into a transformer. Fix a depth $L\in \N$, FF layers $\{ \FF^k \}_{k=1}^L$ with parameters $\{ W^k, U^k, b^k \}_{k=1}^L$, and SA layers $\{\SA^k \}_{k=1}^L$ with parameters $\{ \rho^k, V^k, A^k \}_{k=1}^L$.
A transformer is a map $\T : \mathcal{Z} \rightarrow \mathcal{Z}$ defined by
\begin{equation}\label{eq:trans_model}
\T = \left( \SA^L \circ \FF^L \right) \circ \dots \circ \left(\SA^1 \circ \FF^1 \right).
\end{equation}
For each $k\in [L]$, we call the function $\TB^k = \SA^k \circ \FF^k$ a transformer block and say that $\T$ in \Cref{eq:trans_model} has $L$ blocks.
%
\subsection{Main Result}\label{sec:main-thm}
Our main result states that \Cref{pb:textClass} can be solved exactly with the transformer architecture described in \Cref{ss:transArch}.
\begin{theorem}\label{thm:mainResult}
    Fix $d,M,N \in \N$ with $d\geq 2$, disjoint sets $\smash{S^1,\ldots,S^M} \subset \R^d$, sequences $Z^1,\ldots,Z^N \in \mathcal{Z}$,  and a map $c : [N] \to [M]$. Assume that:
    \begin{enumerate}[noitemsep, topsep=0pt]
    \item[i)] The sequences $Z^1,\ldots, Z^N$ are pairwise distinct;
    \item[ii)] Tokens within each sequence are pairwise distinct.
    \end{enumerate}
Let $\mathrm{R}$ be the readout map in \Cref{eq:readout}. There exists a transformer $\T$ of the form \Cref{eq:trans_model} with $L \leq 8N + 4$ blocks and $P = \mathcal{O}(Nd)$ parameters such that
\begin{equation}
    \left( \mathrm{R} \circ \mathrm{T} \right)(Z^j) \in S^{c(j)}\quad \text{for all}\quad j \in [N].
\end{equation}
\end{theorem}
\begin{remark}
    Assumption \textit{i)} is very mild. Indeed, if $Z^i=Z^j$ and $c(i) = c(j)$, then one of the sequences can be dropped without changing \Cref{pb:textClass}. If instead, $Z^i=Z^j$ and $c(i) \neq c(j)$, then \Cref{pb:textClass} trivially has no solution.
\end{remark}
\begin{remark}
    Assumption \textit{ii)} is mild since positional encoding \cite{press2022trainshorttestlong} usually ensures that tokens in a sequence are distinct. However, it can also be removed with relatively straightforward modifications to the proof, but more cumbersome notation. Indeed, repeated tokens in an input sequence $Z^j$ can be replaced by a single token with more `mass'. This can be handled by replacing the constant $1/|\mathcal{C}_i(Z,A)|$ in \Cref{eq:selfatt_b} with 
    \begin{equation}
    \frac{w_\ell}{\sum_{k \in \mathcal{C}_i(Z,A)} w_k},
    \end{equation}
    where $w_\ell$ counts the number of times token $z_\ell$ is repeated.
\end{remark}
\begin{remark}
    Our transformer has $\mathcal{O}(Nd)$ parameters, rather than the $\mathcal{O}(Nd^2)$ one might expect at first, because in the self-attention layers we always choose $V$ as a multiple of the identity and $A = vv^\top$ for some $v\in \R^d$. Moreover, the feed-forward layers use $d'= 1$ as hidden dimension, so $W$ and $U$ are always described by vectors in $\R^d$.
\end{remark}
\Cref{thm:mainResult} highlights the efficiency of transformers for sequence classification since this is achieved perfectly with a number of parameters independent of sequence length, which is arbitrary in our setting. This is a key advantage compared to ResNets, which require sequences of fixed length $n$ and simply view them as vectors in $\mathbb{R}^{nd}$. Indeed, analysis by \citet{domenec2023NODES} shows that ResNets can perfectly classify sequences, but require $\mathcal{O}(nNd)$ parameters. 
Our analysis, therefore, justifies why transformers outperform ResNets in sequence classification tasks.
%
\section{Proof of \Cref{thm:mainResult}}
We now prove \Cref{thm:mainResult} by prescribing explicit transformer parameters. The construction is technical but follows an intuitive strategy that illuminates how transformers work. To better highlight it, we outline only the main steps of the proof in \Cref{sec:proof-outline} and postpone all technicalities to~\Cref{sec:lemmas}.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/sketch-thm.pdf}
    \caption{Proof strategy of \Cref{thm:mainResult} applied to $N=3$ sequences (denoted with circles, squares and stars) in $\R^2$, with $M=2$ different categories (blue and orange). The initial sequences' tokens are shown in panel $(a)$. Panels $(b)$--$(f)$ are obtained by applying a transformer block to the previous panel. Red, pink, blue and green arrows correspond to the separation, clustering, matching labels and classification steps, respectively. In panels $(e)$ and $(f)$, the blue circle and blue square overlap.}
    \label{fig:thm_sketch}
\end{figure}

%
\subsection{Proof Strategy}\label{sec:proof-outline}
Following \citet{geshkovski2024measure}, our proof proceeds in steps, illustrated in \Cref{fig:thm_sketch} for a simple sequence classification problem with $N=3$ input sequences in $\R^2$ and $M=2$ categories represented by the upper and lower half-planes. The first step uses $\mathcal{O}(N)$ transformer blocks to separate overlapping tokens from different input sequences (see panels \textit{(b)} and \textit{(c)} in \Cref{fig:thm_sketch}). This is the most delicate step and requires alternation of feed-forward layers and self-attention layers. In the second step (\Cref{fig:thm_sketch}\textit{(d)}) a single transformer block collapses the sequences into a single token using the clustering properties of self-attention layers. The third step (\Cref{fig:thm_sketch}\textit{(e)}) drives sequences with the same labels to the same point using $\mathcal{O}(N)$ blocks. In the final step, the collapsed sequences are driven to their target set (\Cref{fig:thm_sketch}\textit{(f)}). This requires at most $\mathcal{O}(N)$ transformer blocks thanks to existing classification results for ResNets~\cite{domenec2023NODES}.
We now make these steps rigorous to prove \Cref{thm:mainResult}, but still relegate all technical constructions to~\Cref{sec:lemmas} for clarity.
%
\begin{proof}[Proof of \Cref{thm:mainResult}]
We combine transformers $\T_{\text{cla}}$, $\T_{\text{lab}}$, $\T_{\text{clu}}$, $\T_{\text{sep}}$, $\T_{\text{pre}}$ of the form \Cref{eq:trans_model} to preprocess, separate, cluster, match up, and classify the input sequences. The final transformer is 
\begin{equation}
\T = \T_{\text{cla}} \circ \T_{\text{lab}} \circ \T_{\text{clu}} \circ \T_{\text{sep}} \circ \T_{\text{pre}}.
\end{equation}

\noindent\textit{Preprocessing.} The transformer $\T_{\text{pre}}$ has a single block ensuring that $\T_{\text{pre}}(z_\ell^j) \neq 0$ for all $\ell \in [\len (Z^j)]$ and all $j\in [N]$. This is achieved by setting $U = 0$, $b=1$ and $W = w \in \R^d$ any non-zero vector in the FF layer, and $\rho = 1$, $V = 0$, and $A=0$ in the self-attention layer. With these choices, the FF layer shifts tokens by $w$ and the self-attention layer acts as the identity. No tokens in $\T_{\text{pre}}(Z^1),\ldots,\T_{\text{pre}}(Z^N)$ are zero if $\| w\|$ is sufficiently small, which is easily ensured.
\vspace{3mm}

\noindent\textit{Separation.} The transformer $\T_{\text{sep}}$ has $2(N-1)$ blocks and is built using technical steps in \Cref{lem:splitOverlappingSequences} to ensure that 
\begin{equation}
\T_{\text{sep}}(\T_{\text{pre}}(Z^j)) \cap \T_{\text{sep}}(\T_{\text{pre}}(Z^{j'})) = \emptyset \quad \text{for all} \quad j\neq j'.
\end{equation}

\noindent\textit{Clustering.} The transformer $\T_{\text{clu}}$ has a single block that collapses each of the sequences $\T_{\text{sep}}(\T_{\text{pre}}(Z^j))$. This block has a FF layer constructed as in \Cref{lem:chooseLeader} and a SA layer with $\rho = 0$, $V = I$ and $A=v v^\top$, where $v \in \R^d$ is also given by \Cref{lem:chooseLeader}. Then, for every $j\in [N]$, the sequence $\T_{\text{clu}}(\T_{\text{sep}}(\T_{\text{pre}}(Z^j)))$ has a single token $\T_{\text{sep}}(\T_{\text{pre}}(z_{i_j}^j))$ repeated $\len(Z^j)$ times. The index $i_j$ is given by \Cref{lem:chooseLeader}. In what follows, we identify the sequence $\T_{\textrm{clu}}(\T_{\text{sep}}(\T_{\text{pre}}(Z^j)))$ with the token $\Tilde{z}^j \coloneqq \T_{\text{sep}}(\T_{\text{pre}}(z_{i_j}^j))$.
\vspace{3mm}

\noindent\textit{Matching Labels.} We now build a transformer $\T_{\text{lab}}$ with $N-1$ blocks such that $\T_{\text{lab}}(\Tilde{z}^i) = \T_{\text{lab}}(\Tilde{z}^j)$ if $S^{c(i)}=S^{c(j)}$. The blocks have self-attention layers acting as the identity ($\rho = 1$, $V = 0$, $A = 0$) and $\FF$ layers built as follows. Fix distinct $i_0, i_1 \in [N]$ such that $\Tilde{z}^{i_0}, \Tilde{z}^{i_1}\in \ext \co (\Tilde{z}^1,\dots,\Tilde{z}^N)$. By the hyperplane separation theorem, for each $\ell\in\{0,1\}$ there exist $u_\ell\in\R^d$ and $b_\ell\in \R$ such that
\begin{equation}
\begin{cases} 
   \ip{u_\ell}{\Tilde{z}^{i_\ell}} + b_\ell >0, \\ 
   \ip{u_\ell}{\Tilde{z}^j} + b_\ell <0  &\text{if} \quad j \neq i_\ell.  
\end{cases}
\end{equation}
To choose the parameters $U$, $b$ and $W$ of the first FF layer of $\T_{\textrm{lab}}$ we distinguish two cases depending on whether there are other input sequences $Z^j$ with the same label $S^{c(i_1)}$.

\textit{Case 1:} $S^{c(j)} \neq S^{c(i_1)}$  $\forall j \in [N] \setminus\{i_1\}$. Then, we set 
\begin{equation}
    U = u_1,\quad b = b_1, \quad \text{and} \quad W = (\langle u_1, \Tilde{z}^{i_1} \rangle + b_1)^{-1}(y - \Tilde{z}^{i_1}),
\end{equation} where $y\neq \Tilde{z}^{i_0}$ is any point such that $\ip{u_0}{y} + b_0 > 0$. This gives $\ip{u_0}{\FF (\Tilde{z}^{i_1})} + b_0 > 0$ and $\FF (\Tilde{z}^{i_1}) \neq \Tilde{z}^{i_0}$.

\textit{Case 2:} $\exists j\in[N] \setminus\{i_1\}$ such that $S^{c(j)} = S^{c(i_1)}$. Then, we set 
\begin{equation}
    U = u_1, \quad b = b_1, \quad \text{and} \quad W = (\ip{u_1}{\Tilde{z}^{i_1}} + b_1)^{-1}(\Tilde{z}^j - \Tilde{z}^{i_1}).
\end{equation}
This gives $\FF (\Tilde{z}^{i_1}) = \Tilde{z}^j$.

The FF layers for blocks $k\in\{2,\ldots,N-1\}$ are constructed similarly. Precisely, with a small abuse of notation, let $\Tilde{z}^{1},\ldots,\Tilde{z}^{N}$ now be the tokens returned by block $k-1$. One picks $i_k \in [N]\setminus\{i_0,\ldots,i_{k-1}\}$ such that $\Tilde{z}^{i_k} \in \ext\co(\Tilde{z}^1,\dots,\Tilde{z}^N)$ satisfies $\ip{u_0}{\Tilde{z}^{i_k}} + b_0 < 0$, and sets the FF parameters according to cases~1 and~2 above using $u_k \in \R^d$ and $b_k \in \R$ such that the hyperplane $\ip{u_k}{z}+b_k=0$ separates $\Tilde{z}^{i_k}$ from the other tokens.
\vspace{3mm}

\noindent\textit{Classification.} 
For every $j \in [N]$, the sequence $(\T_{\text{lab}}\circ\T_{\text{clu}}\circ \T_{\text{sep}}\circ \T_{\text{pre}})(Z^j)$ has collapsed to one of $M$ distinct points $x^1,\ldots,x^m \in \R^d$. We now only need to map each point $x^{c(j)}$ to its corresponding label $\{ S^{c(j)}\}$. We do this with a transformer $\T_{\text{cla}}$ with at most $5(N+1)$ in which the self-attention layers act as the identity ($\rho = 1$, $V = 0$, $A = 0$) and the FF layers are tuned as described by \cite[Theorem~4.1]{domenec2023NODES}. These layers have inputs in $\R^d$ and hidden dimension $d' = 1$, giving $\mathcal{O}(d)$ parameters per transformer block.
The resulting transformer satisfies 
\begin{equation}
    (\T_{\text{cla}} \circ \T_{\text{lab}}\circ \T_{\text{clu}} \circ \T_{\text{sep}} \circ \T_{\text{pre}})(Z^j) \in S^{c(j)} \quad \text{for all} \quad j\in [N].
\end{equation}

\noindent\textit{Complexity of the Construction.}
Finally, we count the number of blocks and parameters in our transformer. It is clear that at most $1 + 2(N-1) + 1 + N-1 + 5(N+1) = 8N + 4$ blocks are needed. Moreover, the parameters in each block have been chosen as vectors in $\R^d$ or constants, giving $\mathcal{O}(d)$ parameters per block. The total number of non-zero parameters is therefore $\mathcal{O}(Nd)$.
\end{proof}
%
\subsection{Technical Lemmas}\label{sec:lemmas}
The proof of \Cref{thm:mainResult} requires understanding how different parameter choices in a transformer block affect tokens. We start by asking if a self-attention layer can leave a specific token $z_p^j$ from sequence $Z^j$ unchanged. This helps us determine which tokens can become clusters for their sequence in the clustering step of our proof. 

It is not hard to see that $z_p^j$ will not be changed by a self-attention layer if $\mathcal{C}_p(Z,A) = \{ p \}$. In such a case, we say that token $z_p\in Z$ is a \textit{leader}. The following lemma asserts that a token can be a leader if and only if it is extreme for the convex hull of the tokens in the sequence $Z$.
\begin{lemma}\label{lem:howToLeader}
   Let $Z$ be a sequence of non-zero tokens. There exists $A\in \R^{d\times d}$ such that $z_p \in Z$ is a leader if and only if $z_p \in \ext \co (Z)$.
\end{lemma}
\begin{proof}
Suppose $A\in \R^{d\times d}$ is such that $z_p$ is a leader. Then,
\begin{equation}
    \langle A z_p, z_r \rangle < \langle A z_p, z_p\rangle \quad \forall r\neq p.
\end{equation}
It is easy to verify that this implies that $z_p$ is the unique maximizer of the linear function $z \mapsto \langle Az_p, z\rangle$ over $\co(Z)$. Since linear functions on convex sets are always maximized by at least one extreme point of the set, we conclude that $z_p \in \ext \co (Z)$. This proves the `only if' part of the lemma.

For the reverse implication, we fix $z_p\in\ext\co (Z)$ and explicitly construct $A$ for which $z_p$ is a leader. Since $z_p\in\ext\co (Z)$, then $z_p\notin \co (Z \setminus \{z_p \})$, which is a closed and convex set. By the hyperplane separation theorem, there exists a non-zero vector $v\in \R^d$ and a constant $\alpha\in \R$ such that
\begin{equation}
\begin{cases} 
   \ip{v}{z_p} > \alpha, \\ 
   \ip{v}{z_\ell} < \alpha  &\forall \ell\neq p. 
\end{cases}
\end{equation}
Since the maps $v \mapsto \ip{v}{z_\ell}$ are continuous for all $\ell \in [\len (Z)]$, we can assume without loss of generality that $\ip{v}{z_p} \neq 0$. Then, if $\ip{v}{z_p} > 0$ we set $A = vv^\top$ and verify that
\begin{equation}
    \begin{aligned}
    \langle A z_p, z_p\rangle &= \ip{v}{z_p} \ip{v}{z_p} \\
    &> \ip{v}{z_p}\ip{v}{z_\ell} \\
    &= \langle Az_p, z_\ell\rangle \quad \forall \ell \neq p,
    \end{aligned}
\end{equation}
because $\ip{v}{z_p} > \alpha > \ip{v}{z_\ell}$. This yields $\mathcal{C}_p(Z,A) = \{ p \}$, as desired. If, instead, $\ip{v}{z_p} < 0$, then we set $A = -vv^\top$ and obtain
\begin{equation}\label{eq:lemleader}
\begin{aligned}
    \langle A z_p, z_p\rangle &= -\ip{v}{z_p} \ip{v}{z_p} \\
    &> -\ip{v}{z_p}\ip{v}{z_\ell} \\
    &= \langle Az_p, z_\ell\rangle \quad \forall \ell \neq p. 
\end{aligned}
\end{equation}
This yields $\mathcal{C}_p(Z,A) = \{ p \}$ and finishes the proof.
\end{proof}
The next two lemmas ensure that all tokens in a sequence $Z^j$ can be attracted by a single token in that sequence. This is a crucial result for the clustering step in the proof of \Cref{thm:mainResult}, since it ensures that the self-attention layer can collapse all sequences into a single token. Moreover, the lemmas allow prescribing the attracting token $z_p^q$ in a sequence $Z^q$. This is used in the separation step, for which the result is iterated to separate all sequences one by one.
\begin{lemma}\label{lem:auxLemmaSingleLeader}
Given sequences $Z^1, \dots, Z^N$ of non-zero tokens, fix $q\in [N]$ and $p\in [\len (Z^q)]$ such that $z_p^q \in \ext\co(Z^q)$. There exists $v\in \R^d$ such that:
\begin{enumerate}[topsep=0pt,itemsep=0pt]
    \item[i)] $\ip{v}{z_p^q} \neq 0$.
    \item[ii)] For every $j \in [N]$, there exists $i_j\in [\len (Z^j)]$ such that $\ip{v}{z_\ell^j} < \ip{v}{z_{i_j}^j}$ $\forall \ell \in [\len (Z^j)].$
    \item[iii)] $i_q = p$.
\end{enumerate}
\end{lemma}
\begin{proof}
We assume without loss of generality that $q = 1$, and proceed by induction on $N$. For $N=1$, a vector $v_1\in \R^d$ such that conditions \textit{i)--iii)} hold can be chosen using the same arguments in \Cref{lem:howToLeader}. 

Next, we assume that $v_{N-1}\in \R^d$ is such that \textit{i)--iii)} hold for the first $N-1$ sequences, and construct $v_{N}\in \R^d$ such that they also hold when the last sequence $Z^N$ is considered. Since only condition \textit{ii)} can fail when $Z^N$ is added to the problem, we need only consider two cases.

\textit{Case 1: Condition \textit{ii)} holds also for $j=N$.} In this case, we can simply set $v_{N} = v_{N-1}$.

\textit{Case 2: Condition \textit{ii)} does not hold for $j=N$.} In this case, the quantity $\ip{v}{z_\ell^{N}}$ is maximized at $s>1$ tokens, which we may take to be $z_1^{N}, \dots, z_s^{N}$ without loss of generality after reordering the sequence if necessary. We may also assume without loss of generality that $z_1^{N}$ is extreme for $\co(z_1^{N}, \dots, z_s^{N})$. Then, by the hyperplane separation theorem, there exists $u\in\R^d$ such that 
\begin{equation}
    \ip{u}{z_r^{N}} < \ip{u}{z_1^{N}} \quad \text{for all} \quad r \in \{2,\dots,s\}.
\end{equation}
Set $v_{N} = v_{N-1} + \varepsilon u$ for some $\varepsilon>0$ to be specified below. Then, we obtain
\begin{equation}
\begin{aligned}
    \ip{v_{N}}{z_r^{N}} &= \ip{v_{N-1}}{z_r^{N}} + \varepsilon \ip{u}{z_r^{N}}  \\
    &< \ip{v_{N-1}}{z_1^{N}} + \varepsilon \ip{u}{z_1^{N}}  \\
    &= \ip{v_{N}}{z_1^{N}}
\end{aligned}
\end{equation}
for every $r \in \{2,\dots, s\}$. We now fix $\varepsilon$ small enough that $\ip{v_{N}}{z_r^{N}} < \ip{v_{N}}{z_1^{N}}$ also for $r\in\{s+1,\ldots,\len(Z^N)\}$ and that conditions \textit{i)--iii)} remain true for sequence indices $j\in[N-1]$. This is possible because the maps $v\mapsto \ip{v}{z}$ are continuous. \qedhere
\end{proof}
\begin{lemma}\label{lem:chooseLeader}
Given sequences $Z^1, \dots, Z^N$ of non-zero tokens, fix $q\in [N]$ and $p\in [\len (Z^q)]$ such that $z_p^q \in \ext\co(Z^q)$. There exist parameters $\{ U, W, b\}$ of a feed-forward layer $\FF$, a vector $v\in \R^d$, and indices $\{i_j\}_{j\in [N]}$ with $i_q = p$ such that, for all $\{\ell, j\}\in [\len (Z^j)]\times [N]$,
\begin{equation}\label{eq:chooseLeaderLemma}
   \FF(z_\ell^j) \neq 0 
   \quad\text{and}\quad
    \C_\ell(\FF(Z^j), vv^\top) = \{ i_j \}.
\end{equation}
\end{lemma}
\begin{proof}
Choose $v\in\R^d$ as in \Cref{lem:auxLemmaSingleLeader}. We will choose the parameters of the FF layer such that \Cref{eq:chooseLeaderLemma} holds. For this, we first observe that adding a constant vector to every token of every sequence does not change properties \textit{ii)--iii)} of \Cref{lem:auxLemmaSingleLeader}. We then set the parameters of $\FF$ as $W = v / \| v \|^2$, $U = 0$ and 
\begin{equation}
    b = \max_{k\in [N]} \max_{r\in [\len (Z^k)]}| \ip{v}{z_r^k}| + \varepsilon,
\end{equation} 
where $\varepsilon > 0$. For this choice, $\sigma(Uz + b) = \sigma (b) = b$, so that $\FF(z_\ell^j) = z_\ell^j + b/ {\| v \|^2} v$. It is clear that, for all $\varepsilon$ sufficiently small, $\FF(z_\ell^j) \neq 0$ for all $\{\ell, j\}\in [\len (Z^j)]\times [N]$, giving the first condition in \Cref{eq:chooseLeaderLemma}. For the second one, note that
\begin{align}\label{eq:lemPosTerms}
    \ip{v}{\FF({z}_\ell^j)} &= \ip{v}{z_\ell^j} +  \frac{\ip{v}{v}}{\|v\|^2} b \geq \varepsilon > 0
\end{align}
for all $\{ \ell, j\} \in [\len (Z^j)] \times [N]$. With this inequality we find that $i\in \C_\ell (\FF(Z^j), vv^\top)$ if and only if
\begin{equation}
\begin{aligned}
    i &\overset{\phantom{\text{by \Cref{eq:lemPosTerms}}}}{\in}\argmax_{r \in [\len (Z^j)]} \ip{v}{\FF(z^j_\ell)} \ip{v}{\FF(z_r^j)} \\
    &\overset{\text{by \Cref{eq:lemPosTerms}}}{=} \argmax_{r \in [\len (Z^j)]}  \ip{v}{\FF(z_r^j)}.
\end{aligned}
\end{equation}
This fact, combined with property \textit{ii)} of \Cref{lem:auxLemmaSingleLeader} implies that $\C_\ell(\FF(Z^j), vv^\top) = \{ i_j \}$ for all $\ell \in [\len (Z^j)]$ and all $j\in [N]$. Finally, $i_q = p$ by property \textit{iii)} of \Cref{lem:auxLemmaSingleLeader}. 
\end{proof}
We now give results related to the separation step in the proof of \Cref{thm:mainResult}. First, we show that a FF layer can split the centers of mass of two sequences. We denote the center of mass of a sequence $Z^j$ and of $\FF(Z^j)$ by
\begin{subequations}
    \begin{align}
    m^{j} &:= \frac{1}{\len (Z^j)}\sum_{\ell=1}^{\len (Z^j)} z_\ell^j,\\
    \Tilde{m}^j &:= \frac{1}{\len (Z^j)}\sum_{\ell=1}^{\len (Z^j)} \FF(z_\ell^j).
\end{align}
\end{subequations}
\begin{lemma}\label{lem:splitCentersOfMass}
Fix $j, j'\in [N]$, $j\neq j'$. There exist parameters $\{ U, W, b\}$ of a feed-forward layer $\FF$ such that $\Tilde{m}^j \neq \Tilde{m}^{j'}$. Moreover, for any token $z_i^j\in Z^j$, $\FF(z_i^j)\in \ext \co (\FF(Z^j))$ if and only if $z_i^j\in \ext \co (Z^j)$. 
\end{lemma}
\begin{proof}
We first show the result for the case in which the centers of mass are different, that is, $m^j \neq m^{j'}$. Then, we set $W = 0$, $U=0$, $b=0$ to obtain $\FF(Z) = Z$ and thus $\Tilde{m}^j \neq \Tilde{m}^{j'}$.

We now deal with the more delicate case $m^j = m^{j'}$. Possibly after relabeling, denote by $Z^j \cap Z^{j'} = \{ z_1^j, \dots, z_p^j\} = \{ z_1^{j'}, \dots, z_p^{j'}\}$ for some $p \leq \min (\len (Z^j), \len (Z^{j'}))$ the set of tokens appearing in both sequences. Then, possibly after relabeling, there exists $J \in \{ j, j'\}$ such that $z_{p+1}^J \in \ext \co (\mathcal{S})$ where $\mathcal{S} = (Z^j \cup Z^{j'}) \setminus (Z^j \cap Z^{j'})$. Assume without loss of generality that $J = j$ and note that the set $\mathcal{S} \setminus \{ z_{p+1}^j\}$ cannot be empty since we assumed that $m^j = m^{j'}$. Note also that $\{ z_{p+1}^j \}$ is disjoint from the closed convex set $\co(\mathcal{S} \setminus \{ z_{p+1}^j\})$, so there exists $u\in \R^d$ and $\beta \in \R$ such that
\begin{equation}\label{e:hyperplane}
\begin{cases} 
    \ip{u}{z_{p+1}^j} + \beta > 0, \\ 
    \ip{u}{z} + \beta < 0  &\forall z\in \co (\mathcal{S}\setminus \{ z_{p+1}^j \}).  
\end{cases}
\end{equation}
Now, let the parameters in $\FF$ be $W = w \in \R^{d\times 1}$, for $w \neq 0$, $U = u^\top \in \R^{1 \times d}$ and $b = \beta \in \R$. By \Cref{e:hyperplane}, this means that $\sigma (\ip{u}{z_\ell^{j}} + b) = \sigma (\ip{u}{z_\ell^{j'}} + b) = 0$ for all $\ell > p+1$. Then, recalling that $m^j = m^{j'}$ by assumption and that $\sigma(\ip{u}{z_\ell^j} + b) = \sigma(\ip{u}{z_\ell^{j'}} + b)$ for all $\ell\in [p]$ by construction, we can calculate
\begin{equation}
\begin{aligned}
    \Tilde{m}^j - \Tilde{m}^{j'} &= w\frac{1}{n} \Big[ \sum_{\ell = 1}^p \sigma(\ip{u}{z_\ell^j} + b) - \sum_{\ell = 1}^p \sigma(\ip{u}{z_\ell^{j'}} + b)
    + \ip{u}{z_{p+1}^j} + b \Big] \\
    &= w\frac{1}{n} \Big[\ip{u}{z_{p+1}^j} + b\Big].
\end{aligned}
\end{equation}
The last term is positive by \Cref{e:hyperplane}, giving $\Tilde{m}^j \neq \Tilde{m}^{j'}$. Moreover, $\|w \|$ can be chosen small enough that $\FF(z_i^j) \in \ext\co(\FF(Z^j))$ if and only if $z_i^j \in \ext \co (Z^j)$.
\end{proof}
Next, we ensure that a family of $N$ sequences can be made pairwise disjoint through the action of suitably chosen transformer blocks. This is the key construction that enables the separation step in the proof of \Cref{thm:mainResult}.

\begin{lemma}\label{lem:splitOverlappingSequences}
Given sequences $Z^1,\ldots,Z^N$ with nonzero tokens, there exists a transformer $\T$ with $2 (N-1)$ blocks such that $\T({Z}^j) \cap \T({Z}^{j'}) = \emptyset$ for all $j\neq j'\in [N]$.
\end{lemma}
\begin{proof}
We prove the result constructively by induction on~$N$. Throughout the proof, $B_\delta(x)$ denotes the open ball of radius $\delta > 0$ centered at $x\in \R^d$.
\vspace{3mm}

\noindent{$\bm{N = 2}$.} By relabeling tokens if necessary, we may assume without loss of generality that $z_1^1 \in \ext\co(Z^1)$. We then consider three cases, which can be handled using at most two transformer blocks.

\textit{Case 1: $z_1^1 \cap Z^2 = \emptyset$ or $z_1^1 = z_1^2$ for $z_1^2 \notin \ext\co (Z^2)$.} In this case, the extreme token $z_1^1 \in Z^1$ does not coincide with any extreme token for $Z^2$, which allows for sequence separation using a single transformer block. For the FF layer of this block, we apply \Cref{lem:chooseLeader} with $p = q = 1$ to construct a feed-forward layer $\FF$ and a vector $v_1\in \R^d$ such that
\begin{subequations}
    \begin{align}
    \C_{\ell} (\FF (Z^1), v_1 v_1^\top) = \{ 1 \} \quad \forall \ell \in [\len (Z^1)],
    \\ 
    \C_r (\FF (Z^2), v_1 v_1^\top) = \{ i_2 \} \quad \forall r \in [\len (Z^2)].
\end{align}
\end{subequations}
Note that $z_{i_2}^2\in \ext \co (Z^2)$ by  \Cref{lem:howToLeader}, so $z_{i_2}^2\neq z_1^1$ by the assumption defining our Case 1. For the self-attention layer, instead, we take $\alpha > 0$ to be specified precisely below and set $\rho = 1 -\alpha$, $V = \alpha I$ and $A=vv^\top$ to obtain
\begin{subequations}
    \begin{align}
    \SA_{\ell}(\FF (Z^1)) &=  {z}_{\ell}^1 + \alpha ({z}_1^1 -{z}_{\ell}^1)  + b\frac{v_1}{\| v_1 \|^2}, \\ 
    \SA_{r}(\FF (Z^2)) &= {z}_r^2 + \alpha ({z}_{i_2}^2 - {z}_r^2) + b\frac{v_1}{\| v_1 \|^2}.    
\end{align}
\end{subequations}
(The last term in these two equations comes from the shift introduced by the FF layer.) These quantities are different for all $\{\ell, r\} \in [\len (Z^1)] \times [\len (Z^2)]$ if and only if
\begin{equation}\label{eq:lemConditionOnAlpha}
(1 - \alpha) (z_\ell^1 - z_r^2) + \alpha (z_1^1 - z_{i_2}^2) \neq 0.
\end{equation}
This condition fails if and only if there exists $c_{\ell r} \in \R$ such that $z_\ell^1 - z_r^2 = c_{\ell r} (z_1^1 - z_{i_2}^2)$ 
and $\alpha$ is the unique solution to
\begin{equation}
    \frac{\alpha}{\alpha - 1} = c_{\ell r}.
\end{equation}
This means there are at most finitely many choices of $\alpha$ for which \eqref{eq:lemConditionOnAlpha} fails, so it suffices to pick $\alpha$ not from this set. \Cref{fig:case1} illustrates the construction with a simple example.
\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/case1.pdf}
    \caption{Illustration of Case 1 in \Cref{lem:splitOverlappingSequences} for $N=2$ sequences in $\R^2$. The initial tokens in $Z^1$ (stars) and $Z^2$ (circles) are shown in panel $(a)$. In panel $(b)$, a $\FF$ layer has shifted the tokens in the direction of $v\in \R^2$ such that, in the following self-attention layer, the token in each sequence are attracted by a single leader (marked by a red arrow). In panel $(c)$, the sequences have been split by a $\SA$ layer.}
    \label{fig:case1}
\end{figure}

\textit{Case 2: $z_1^1 \in \ext\co (Z^2)$ and $m^1 \neq m^2$.} 
In this case, the extreme token $z_1^1 \in Z^1$ coincides with an extreme token for $Z^2$, which we take to be $z_1^2$ without loss of generality. Nevertheless, we can separate the sequences using their distinct centers of mass through two transformer blocks: one to reduce the problem to Case~1 (cf. \Cref{fig:case2}), and one to handle that case. 

In the first block, we take a FF layer acting as the identity ($W=0$, $U=0$, $b=0$). For the self-attention layer, we fix a constant $\beta > 0$ to be specified later and a nonzero vector $v_2\in \R^d$ such that $\ip{v_2}{z_1^1} = 0 = \ip{v_2}{z_1^2}$. We then set $A = v_2 v_2^\top$, $\rho = 1-\beta$ and $V = \beta I$. This gives
\begin{subequations}
    \begin{align}
    \mathrm{SA}_1(Z^1) &= z_1^1 +  \beta (m^1 - z_1^1), \\ 
    \label{eq:SA_case2}
    \mathrm{SA}_1(Z^2) &= z_1^2 + \beta (m^2 - z_1^2).
\end{align}
\end{subequations}
Since $m^{1} \neq m^{2}$, $\SA_1(Z^1) \neq \SA_1(Z^2)$ for any $\beta>0$. We now pick $\beta$ to ensure that 
\begin{equation}
   \SA_1(Z^1) \in \ext\co (\SA(Z^1)) \quad \text{and} \quad \SA_1(Z^1) \cap \SA (Z^2) = \emptyset, 
\end{equation}
reducing Case~2 to Case~1 as desired.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/case2.pdf}
    \caption{Illustration of Case 2 in \Cref{lem:splitOverlappingSequences} for $N=2$ sequences in $\R^2$. Panel $(a)$ shows the sequences' initial tokens and centers of mass. In panel $(b)$, a $\SA$ layer separates $z_1^1\in Z^1$ from $z_1^2\in Z^2$.}
    \label{fig:case2}
\end{figure}
For that, notice that the right-hand side of \Cref{eq:SA_case2} is a perturbation of the identity and is continuous in $\beta$. The same is true for the expressions for $\mathrm{SA}_\ell(Z^2)$, $\ell \in [\len (Z^2)]$. Therefore, for all $\delta > 0$, there exists $\beta > 0$ such that 
\begin{equation}
    \SA_\ell (Z^2) \in B_\delta (z_\ell^2) \quad \text{for all} \quad \ell \in [\len (Z^2)]. 
\end{equation}
We can then take $\delta > 0$ small enough such that the balls $\{B_\delta(z_\ell^2)\}_{\ell\in [\len (Z^2)]}$ are disjoint and $\SA_1(Z^1) \cap \SA (Z^2) = \emptyset$. Similar continuity arguments and the assumption that $z_1^1 \in \ext \co (Z^1)$ show that $\SA_1(Z^1) \in \ext\co (\SA(Z^1))$.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/case3.pdf}
    \caption{Illustration of Case 3 in \Cref{lem:splitOverlappingSequences} for $N=2$ sequences in $\R^2$. Panel $(a)$ shows the sequences' initial tokens and centers of mass. In panel $(b)$, a $\FF$ layer separates $m^1$ from $m^2$.}
    \label{fig:case3}
\end{figure}

\textit{Case 3: $z_1^1 \in \ext\co (Z^2)$ and $m^1 = m^2$.} 
In this case, the extreme token $z_1^1 \in Z^1$ coincides with an extreme token for $Z^2$ and the sequences' centers of mass also coincide. We handle this using two transformer blocks, one to reduce the problem to Case 1, and one as described in that case.

To construct the FF layer of the first block, we apply \Cref{lem:splitCentersOfMass} to construct feed-forward layer $\FF$ such that $\Tilde{m}^1 \neq \Tilde{m}^2$ (cf. \Cref{fig:case3}). After this transformation, which ensures that $\FF (z_1^1) \in \ext \co (\FF(Z^1))$, the problem reduces to Case~2 if $\FF (z_1^1) \in \ext \co (\FF (Z^2))$, and to Case~1 if not. In the former case, we take the self-attention layer of the first transformer block as described in Case~2. Otherwise, we use a self-attention layer $\SA$ with $\rho = 1$, $V = 0$ and $A=0$ to have $\SA(Z) = Z$. Either way, we obtain a transformer block that reduces Case~3 to Case~1, as desired. 
\vspace{3mm}

\noindent\textbf{Induction Step.} Assume there exists a transformer $\T_{N-1}$ with at most $2(N-2)$ blocks such that 
\begin{equation}
    \T_{N-1}(Z^j) \cap \T_{N-1}(Z^{j'}) = \emptyset \quad \forall j\neq j'\in [N-1].
\end{equation}
Set $\Hat{Z} \coloneqq \T_{N-1}(Z)$ to ease the notation. Fix $\Hat{z}_i^N \in \ext \co (\Hat{Z}^N)$, assuming without loss of generality that $i=1$.

If $\Hat{z}_1^N \cap \Hat{Z}^j = \emptyset$ for all $j\in [N]$, then we apply the same argument in Case 1 above, but now choose $\alpha > 0$ such that
\begin{equation}\label{eq:lemConditionOnAlphaInduction}
(1 - \alpha) (\Hat{z}_\ell^N - \Hat{z}_r^j) + \alpha (\Hat{z}_1^N - \Hat{z}_{i_j}^j) \neq 0 \quad \forall j\in [N-1]. 
\end{equation}
Again, there are at most finitely many choices of $\alpha$ for which \Cref{eq:lemConditionOnAlphaInduction} fails and we choose $\alpha$ outside of this set.
    
If $\Hat{z}_1^N \cap \Hat{Z}^j \neq \emptyset$ for some $j\in [N]$, we can assume (upon relabeling if necessary) that $\Hat{z}_1^N = \Hat{z}_1^{N-1}$. Then, since since $\hat{Z}^{N-1} \cap \Hat{Z}^j = \emptyset$ for all $j\in [N-2]$ by the induction hypothesis, we also have
\begin{equation}
    \Hat{z}_1^N \cap \Hat{Z}^j = \emptyset \quad \text{for all} \quad j\in [N-2].
\end{equation}
We are therefore back in one of the Cases 1, 2 or 3 above, depending on whether $\Hat{z}_1^{N-1}$ is extreme for the convex hull of its sequence and whether the sequences $\Hat{Z}^{N-1}$ and $\Hat{Z}^N$ have the same center of mass.

Irrespective of which case arises, we construct a transformer $\T'$ with at most $2$ blocks ensuring that $\T' (\Hat{Z}^N) \cap \T' (\Hat{Z}^{N-1}) = \emptyset$ by applying the arguments used for $N=2$. Moreover, by the induction hypothesis, there exists $\delta_1 > 0$ such that the balls $\{ B_{\delta_1} (\hat{z}_\ell^j )\}_{\ell, j}$ are disjoint for all $\ell \in [\len (Z^j)]$ and all $j\in [N-1]$. By taking the parameters $\alpha$ and $\beta$ in the blocks of $\T'$ small enough, we can ensure also that the balls $\{ B_{\delta_1} (\T'(\hat{z}_\ell^j)) \}_{\ell, j}$ remain disjoint for all $\ell \in [\len (Z^j)]$ and all $j\in [N-1]$, whence $\T' (\Hat{Z}^N) \cap \T' (\Hat{Z}^{j}) = \emptyset$ for all $j\in [N - 1]$.

In summary, since $\Hat{Z}^j = \T_{N-1}(Z^j)$ for all $j\in[N]$, we have constructed a transformer $\T = \T' \circ \T_{N-1}$ with at most $2(N-1)$ blocks achieving $\T({Z}^j) \cap \T({Z}^{j'}) = \emptyset$ for all $j\neq j'\in [N]$. The proof is complete.
\end{proof}
%
\section{Conclusion}\label{sec:conclusion}
The results in this paper advance the mathematical understanding of transformers in the context of classification problems. We provide an explicit construction showing that transformers can perform sequence classification and, in doing so, we uncover two key roles played by their self-attention layers. First, they reduce dimensionality thanks to their clustering effect, which allows transformers to classify sequences with a total number of parameters independent of the sequence length. Second, self-attention layers separate tokens shared by different sequences, thanks to the non-locality of the self-attention mechanism as a transformation in the token space. The construction also incorporates two common features of real-life transformers: the alternation of self-attention and feed-forward layers, and the low-rank structure of the matrix inside the inner product of self-attention layers. Our theoretical results, therefore, offer an explanation for the remarkable practical efficiency of transformers when solving sequence classification tasks. In particular, we have shown that self-attention allows for a significant reduction in the number of parameters compared to traditional ResNet architectures used to solve the same tasks, with the number of parameters defining our transformer being independent of the input sequence length. 

Our work leaves room for several future improvements. An interesting question is whether our arguments can be extended to transformers with multi-head attention. In particular, even though empirical work suggests that a single head might suffice~\cite{michel2019sixteen}, we wonder if multiple heads can reduce the number of parameters and blocks required to achieve perfect sequence classification. For this, one should replace \Cref{eq:selfatt_a} with
\begin{equation}
    \mathrm{SA}_i(Z) = \rho z_i + \sum_{h=1}^H V^h \sum_{\ell = 1}^{\len(Z)} \Lambda_{i\ell}(Z,A^h)z_\ell,
\end{equation}
for a given number of heads $H\in \N$, and check, for example, whether the combination of multiple heads can reduce the number of blocks required by the separation step. 

On the other hand, it would be interesting to consider masked self-attention layers. Masking is a common feature of self-attention layers for classification tasks in NLP, where a token $z_i$ should only be influenced by those preceding it. To model this, one should replace \Cref{eq:selfatt_a} with
\begin{equation}
    \mathrm{SA}_i(Z) = \rho z_i + V \sum_{\ell = 1}^i \Lambda_{i\ell}(Z,A)z_\ell.
\end{equation}
Notice that the sum now runs only up to $i\in [\len (Z)]$, forcing tokens ahead of $z_i$ not to interact with it. 
Extending our present analysis to such a model of masked transformers remains an open challenge.
%
\section*{Acknowledgments}
This work was funded by the Humboldt Research
Fellowship for postdoctoral researchers, the Alexander von Humboldt-Professorship program, the European Union (Horizon Europe MSCA project ModConFlex, grant number 101073558), 24IOE027 of AFOSR, the COST Action MAT-DYN-NET, the Transregio 154 Project of the DFG, grants PID2020-112617GB-C22 and TED2021-131390B-I00 of MINECO (Spain), and the Madrid Government--UAM Agreement for the Excellence of the University Research Staff in the context of the V PRICIT (Regional Programme of Research and Technological Innovation). The authors thank Martín Hernández for valuable conversations.
%
\bibliographystyle{abbrvnat}  
\bibliography{refs}
\vfill
\end{document}