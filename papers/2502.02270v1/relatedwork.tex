\section{Related Work}
Mathematical insight into the inner workings of transformers can be gained by looking at self-attention layers through the lens of interacting particle systems \cite{lu2019understanding,sanderSinkformers2022}. Specifically, one can view the elements of an input sequence as particles that interact through a kernel determined by the self-attention mechanism. \citet{geshkovski2023emergence, geshkovski_mathematical_2023} adopted this perspective to prove asymptotic clustering results for simplified attention-only transformers with shared weights. \citet{alcalde2024clustering} proved further clustering results for a hardmax formulation of self-attention, which allows for a more geometric understanding of the attention mechanism.

\citet{geshkovski2024measure} recently exploited the clustering effect of self-attention layers to prove approximate interpolation results for a class of transformers acting as measure-to-measure mappings. In these transformers, a softmax self-attention acts as a particular interaction mechanism for $d$-dimensional particles on the unit sphere $\mathbb{S}^{d-1}$. This formulation uses a continuous-time interpretation of transformers similar to the so-called neural ODE interpretation of ResNets \cite{Weinan2017APO,NODES2018}. The resulting transformer model is a nonlocal transport equation for measures, with the transformer parameters acting as control inputs. Approximate interpolation of measures in the Wasserstein-2 distance is achieved using explicit piecewise-constant control inputs with a number of switches (corresponding to the number of transformer blocks needed in practice) proportional to $Nd$, where $N$ is the number of measures \citep[Theorem~1.1]{geshkovski2024measure}. Moreover, if the input and target measures are atomic with the same number of atoms, then one can perfectly solve the classification task stated in \Cref{pb:textClass} below using $\mathcal{O}(Nd)$ transformer blocks (see \cite[\S1.4.2]{geshkovski2024measure}).
%