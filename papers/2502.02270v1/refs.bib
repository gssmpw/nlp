@misc{alcalde2024clustering,
    title={Clustering in pure-attention hardmax transformers and its role in sentiment analysis}, 
    author={Albert Alcalde and Giovanni Fantuzzi and Enrique Zuazua},
    year={2024},
    howpublished = {\href{https://arxiv.org/abs/2407.01602}{arXiv:2407.01602 [cs.CL]}}
}

@article{domenec2023NODES,
    author = {Ruiz-Balet, Dom\`{e}nec and Zuazua, Enrique},
    title = {Neural ODE Control for Classification, Approximation, and Transport},
    journal = {SIAM Review},
    volume = {65},
    number = {3},
    pages = {735-773},
    year = {2023}
}

@article{geshkovski2022turnpike,
    title={Turnpike in optimal control of PDEs, ResNets, and beyond},
    author={Geshkovski, Borjan and Zuazua, Enrique},
    journal={Acta Numerica},
    volume={31},
    pages={135--263},
    year={2022},
    publisher={Cambridge University Press}
}

@inproceedings{yunAreTransformersUniversal2020,
    author = {Yun, C. and Bhojanapalli, S. and Rawat, Ankit S. and Reddi, S. J. and Kumar, S.},
    title = {Are Transformers Universal Approximators of Sequence-to-Sequence Functions?},
    booktitle = {International Conference on Learning Representations},
    year = {2020}
}

@inproceedings{alberti2023sumformer,
    title={Sumformer: Universal approximation for efficient transformers},
    author={Alberti, Silas and Dern, Niclas and Thesing, Laura and Kutyniok, Gitta},
    booktitle={Topological, Algebraic and Geometric Learning Workshops 2023},
    pages={72--86},
    year={2023},
    organization={PMLR}
}

@misc{geshkovski2023emergence,
    title={The emergence of clusters in self-attention dynamics}, 
    author = {Geshkovski, B. and Letrouit, C. and Polyanskiy, Y. and Rigollet, P.},
    year={2023},
    howpublished = {\href{https://arxiv.org/abs/2305.05465}{arXiv:2305.05465 [cs.LG]}}
}

@misc{geshkovski_mathematical_2023,
    title = {A mathematical perspective on {Transformers}},
    author = {Geshkovski, B. and Letrouit, C. and Polyanskiy, Y. and Rigollet, P.},
    year = {2023},
    howpublished = {\href{https://arxiv.org/abs/2312.10794}{arXiv:2312.10794 [cs.LG]}}
}

@misc{geshkovski2024measure,
    title={Measure-to-measure interpolation using Transformers}, 
    author={Borjan Geshkovski and Philippe Rigollet and Dom√®nec Ruiz-Balet},
    year={2024},
    howpublished = {\href{https://arxiv.org/abs/2411.04551}{arXiv:2411.04551 [math.OC]}}
}

@article{michel2019sixteen,
    title={Are sixteen heads really better than one?},
    author={Michel, Paul and Levy, Omer and Neubig, Graham},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

@inproceedings{sanderSinkformers2022,
    title = {Sinkformers: {{Transformers}} with {{Doubly Stochastic Attention}}},
    booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
    author = {Sander, M. E. and Ablin, P. and Blondel, M. and Peyr{\'e}, G.},
    year = {2022},
    pages = {3515--3530}
}

@inproceedings{lu2019understanding,
    title={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},
    author={Yiping Lu and others},
    booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
    year={2019}
}

@misc{press2022trainshorttestlong,
    title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
    author={Ofir Press and Noah A. Smith and Mike Lewis},
    year={2022},
    howpublished = {\href{https://arxiv.org/abs/2108.12409}{arXiv:2108.12409 [cs.CL]}}
}

@inproceedings{kaimanHe_2016,
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Deep Residual Learning for Image Recognition}, 
    year={2016},
    pages={770-778}
}

@misc{openai2024gpt4technicalreport,
    title={GPT-4 Technical Report}, 
    author={OpenAI},
    year={2024},
    howpublished = {\href{https://arxiv.org/abs/2303.08774}{arXiv:2303.08774 [cs.CL]}}
}

@inproceedings{vaswaniAttentionAllYou2017,
    title = {Attention Is {{All}} You {{Need}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, {\L}. and Polosukhin, I.},
    year = {2017},
    volume = {30}
}

@misc{dosovitskiy2021imageworth16x16words,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
    author={Alexey Dosovitskiy and others},
    year={2021},
    howpublished = {\href{https://arxiv.org/abs/2010.11929}{arXiv:2010.11929 [cs.CV]}}
}

@inproceedings{NODES2018,
    author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
    booktitle = {Advances in Neural Information Processing Systems},
    title = {Neural Ordinary Differential Equations},
    volume = {31},
    year = {2018}
}

@article{Weinan2017APO,
    author = {E, Weinan},
    title = {A proposal on machine learning via dynamical systems},
    journal = {Communications in Mathematics and Statistics},
    volume = {5},
    year = {2017},
    number = {1},
    pages = {1--11}
}