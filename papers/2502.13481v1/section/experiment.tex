\begin{table*}
% \caption{Performance comparison of different methods. The best performance on each dataset and metric are highlighted. "RI" indicates the relative improvements of LLM4Tag over the corresponding baseline.}
\caption{Performance comparison of different methods. Note that different tasks, multi-tag tasks (Brower News) and single-tag tasks (Advertisement Creatives and Search Query), have different metrics. The best result is given in bold, and the second-best value is underlined. "RI" indicates the relative improvements of LLM4Tag over the corresponding baseline.}
\label{table:all}
%  \resizebox{0.99\textwidth}{!}{
 \renewcommand\arraystretch{1.1}
\begin{threeparttable}[htbp]{
\begin{tabular}{c|cccc|cccc|cccc}
\midrule
 {\multirow{2}{*}{{Model}}} & \multicolumn{4}{c|}{{Browser News}} & \multicolumn{4}{c|}{{Advertisement Creatives}} & \multicolumn{4}{c}{{Search Query}} \\ 
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
  & Acc@1 & Acc@2 & Acc@3 & RI & Precision & Recall & F1 & RI & Precision & Recall & F1 & RI \\ 
\midrule
  BGE & 0.7427 & 0.6584 & 0.5976 & 29.8\% & 0.7817 & 0.7396   & 0.7601 & 18.5\% & 0.6364 & 0.5122  & 0.5676&  56.2\% \\
  GTE & 0.7292 & 0.6507 & 0.5941 & 31.3\% & 0.7369 & 0.7026  & 0.7194& 25.0\%  & 0.6129 & 0.4634 & 0.5278 & 67.9\% \\
 CONAN  & 0.7568 & 0.6814 & 0.6266 & 25.5\%  & 0.7491  & 0.7194 & 0.7339& 22.6\% & 0.6056 & 0.5244 & 0.5621& 57.9\% \\
\midrule
 TagGPT & 0.8351 & 0.7813 & 0.7424 &   9.5\%  & 0.8454 & 0.7997  & 0.8219& 9.4\% & 0.8421 & 0.7805  & 0.8101 & 9.7\% \\
 ICXML & 0.8398 & 0.7883 & 0.7560 & 8.4\%  & 0.8492  & 0.8025  & 0.8252 & 9.0\% & 0.8600 & 0.7840 & 0.8202& 8.3\% \\
 LLM4TC & \underline{0.8602} & \underline{0.8069} & \underline{0.8235} & 3.7\% & \underline{0.8726} & \underline{0.8245}  & \underline{0.8479} & 6.1\% & \underline{0.9028} & \underline{0.8025} & \underline{0.8497} & 4.5\% \\
\midrule
 \textbf{LLM4Tag} & \textbf{0.9041}  & \textbf{0.8511} & \textbf{0.8273} & - & \textbf{0.9138} & \textbf{0.8857} & \textbf{0.8995} &-  & \textbf{0.9325} & \textbf{0.8485} & \textbf{0.8885} & -\\  
  \midrule          
\end{tabular}
}
\end{threeparttable}
% }
\end{table*}

% \begin{table*}
% % \caption{Performance comparison of different methods. The best performance on each dataset and metric are highlighted. "RI" indicates the relative improvements of LLM4Tag over the corresponding baseline.}
% \caption{Performance comparison of different methods. The best performance on each dataset and metric are highlighted. Note that we have two different tasks, multi-tag task (Brower) and single-tag task (Advertisement Creatives and Search Query), thus they have different metrics. "RI" indicates the relative improvements of LLM4Tag over the corresponding baseline.}
% \label{table:all}
%  \resizebox{0.99\textwidth}{!}{
%  \renewcommand\arraystretch{1.1}
% \begin{threeparttable}[htbp]{
% \begin{tabular}{c|c|cccc|cccc|cccc}
% \midrule
% {\multirow{2}{*}{{Category}}} & {\multirow{2}{*}{{Model}}} & \multicolumn{4}{c|}{{Browser News}} & \multicolumn{4}{|c|}{{Advertisement Creatives}} & \multicolumn{4}{c}{{Search Query}} \\ 
% % {\multirow{3}{*}{{Category}}} & {\multirow{3}{*}{{Model}}} & \multicolumn{4}{c|}{{Multi-Tag Task}} & \multicolumn{8}{|c|}{{Single-Tag Task}}  \\ 
% % \cmidrule(lr){3-6}\cmidrule(lr){7-14}
% % & & \multicolumn{4}{c|}{{Browser News Dataset}} & \multicolumn{4}{|c|}{{Advertisement Creatives Dataset}} & \multicolumn{4}{c}{{Search Query Dataset}} \\ 
% \cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-14}
%  & & Acc@1 & Acc@2 & Acc@3 & RI & Precision & Recall & F1 & RI & Precision & Recall & F1 & RI \\ 
% \midrule
% {\multirow{3}{*}{\makecell{Soft-Matching\quad }}} &  BGE & 0.7427 & 0.6584 & 0.5976 & 29.8\% & 0.7817 & 0.7396   & 0.7601 & 18.5\% & 0.6364 & 0.5122  & 0.5676&  56.2\% \\
%  & GTE & 0.7292 & 0.6507 & 0.5941 & 31.3\% & 0.7369 & 0.7026  & 0.7194& 25.0\%  & 0.6129 & 0.4634 & 0.5278 & 67.9\% \\
% & CONAN  & 0.7568 & 0.6814 & 0.6266 & 25.5\%  & 0.7491  & 0.7194 & 0.7339& 22.6\% & 0.6056 & 0.5244 & 0.5621& 57.9\% \\
% \midrule
% {\multirow{4}{*}{\makecell{LLM-Enhanced\quad }}} & TagGPT & 0.8351 & 0.7813 & 0.7424 &   9.5\%  & 0.8454 & 0.7997  & 0.8219& 9.4\% & 0.8421 & 0.7805  & 0.8101 & 9.7\% \\
% & ICXML & 0.8398 & 0.7883 & 0.7560 & 8.4\%  & 0.8492  & 0.8025  & 0.8252 & 9.0\% & 0.8600 & 0.7840 & 0.8202& 8.3\% \\
% & LLM4TC & 0.8602 & 0.8069 & 0.8235& 3.7\% & 0.8726 & 0.8245  & 0.8479 & 6.1\% & 0.9028 & 0.8025 & 0.8497 & 4.5\% \\
% % \midrule
% & \textbf{LLM4Tag} & \textbf{0.9041}  & \textbf{0.8511} & \textbf{0.8273} & - & \textbf{0.9138} & \textbf{0.8857} & \textbf{0.8995} &-  & \textbf{0.9325} & \textbf{0.8485} & \textbf{0.8885} & -\\  
%   \midrule          
% \end{tabular}
% }
% \end{threeparttable}
% }
% \end{table*}



\section{EXPERIMENTS}\label{sec:exp}
In this section, we conduct extensive experiments to answer the following research questions:
\begin{itemize}
    \item[\textbf{RQ1}] How does LLM4Tag perform in comparison to existing tagging algorithms?
    \item[\textbf{RQ2}] How effective is the graph-based tag recall module?
    \item[\textbf{RQ3}] Does the injection of domain-specific knowledge enhance the tagging performance?
    \item[\textbf{RQ4}] What is the impact of the tag confidence calibration module?
    % \item[\textbf{RQ5}] Could LLM4Tag provide a more intuitive understanding to show the performance? 
\end{itemize}


\subsection{Experimental Settings}
\subsubsection{Dataset}
We conducted experiments on a mainstream information distribution platform with hundreds of millions of users and sampled three representative industrial datasets  from online logs to ensure consistency in data distribution, containing two types of tasks: (1) multi-tag task (\textit{Browser News}), and (2) single-tag task (\textit{Advertisement Creatives} and \textit{Search Query}).

\begin{itemize}[leftmargin = 12 pt]
\item \textbf{Browser News} dataset includes popular news articles and user-generated videos, primarily in the form of text, images, and short videos. This is a multi-tag task, wherein the objective is to select multiple appropriate tags for each content from a massive tag repository (more than 100,000 tags). Around 30,000 contents are randomly collected as the testing dataset through expert annotations.

\item \textbf{Advertisement Creatives} dataset includes ad creatives, including cover images, copywriting, and product descriptions from advertisers.
The task for this dataset is a single-tag task, where we need to select the most relevant tag to the advertisement from a well-designed tag repository (more than 1,000 tags) and we collect around 10,000 advertisements randomly as the testing dataset through expert annotation.
\item \textbf{Search Query} dataset primarily consists of user search queries from a web search engine, used for user intent classification.
The task for this dataset is also a single-tag task, where the most probable intent needs to be selected as the tag for each query. The size of the tag repository is about 1,000, and 2,000 queries are collected and manually tagged as the testing dataset.
 \end{itemize}
 
 \subsubsection{Baselines}
To evaluate the superiority and effectiveness of our proposed model, we compare LLM4Tag with two classes of existing models: 
\begin{itemize}[leftmargin = 12 pt]
\item \textbf{Traditional Methods} that encode the contents and tags by leveraging pre-trained language models and select the most relevant tags according to cosine distance for each content as the result. Here we compare three different pre-trained language models.
\textbf{BGE}~\cite{bge} pre-trains the models with retromae on large-scale pairs data using contrastive learning. \textbf{GTE}~\cite{gte} further proposes multi-stage contrastive learning to train the text embedding. \textbf{CONAN}~\cite{conan} maximizes the utilization of more and higher-quality negative examples to pre-train the model.
\item \textbf{LLM-Enhanced Methods} that utilize large language models to assist the tag generation. \textbf{TagGPT}~\cite{li2023taggpt} proposes a zero-shot automated tag extraction system through prompt engineering via LLMs. \textbf{ICXML}~\cite{zhu2023icxml} introduces a two-stage tag generation framework, involving generation-based label shortlisting and label reranking through in-context learning. \textbf{LLM4TC}~\cite{chae2023large} further leverages fine-tuning using domain knowledge to improve the performance of tag generation.
 \end{itemize}



\subsubsection{Evaluation Metrics}
For multi-tag tasks, due to the excessive number of tags (millions), we can not annotate all the correct tags and thus only directly judge whether the results generated by the model are correct or not. In this case, we define Acc@k to evaluate the performance:
\begin{equation}
\begin{aligned}
\operatorname{Acc@k} &=\frac{1}{N'}\sum_{i=1}^{N'}\sum_{j=1}^{k'}  \frac{\mathbb{I}\,{\left(T_i[j]\right)}}{k'}~, \\
k' &= \min(k, len(T_i))~,\\
\mathbb{I}\,{\left(T_i[j]\right)}&= \begin{cases}
1, & \text{$T_i[j]$ is right}~,\\
0, &  \text{otherwise}~,
\end{cases}
\end{aligned}
\end{equation}
% \begin{equation}
% \begin{aligned}
% \operatorname{Acc@k} &=\frac{1}{N'}\sum_{i=1}^{N'}\sum_{j=1}^{k'}  \frac{\operatorname{check}(T_i[j])}{k'}~, \\
% k' &= \min(k, len(T_i))~,\\
% \operatorname{check}(T_i[j])&= \begin{cases}
% 1, & \text{$T_i[j]$ is right}~,\\
% 0, &  \text{otherwise}~,
% \end{cases}
% \end{aligned}
% \end{equation}
where $T_i[j]$ is the $j$-th generated tag of the $i$-th content and $N'$ is the size of test dataset. It is worth noticing there exists contents that do not have $k$ proper tags, thus we allow the number of generated tags to be less than $k$. 

For the single-tag task, we adopt Precision, Recall, and F1 following previous works~\cite{li2023taggpt,chae2023large}. Higher values of these metrics indicate better performance.

Moreover, we report Relative Improvement (RI) to represent the relative improvement our model achieves over the compared models. Here we calculate the average RI of the above all metrics.

\subsubsection{Implementation Details} 
In the selection of LLMs, we select Huawei's large languge model PanGu-7B~\cite{zeng2021pangu,wang2023pangu}.
For the graph-based tag recall module, we choose BGE~\cite{bge} as the encoder model. $\delta_{c-t}$ and $\delta_{c-c}$ are set as $0.5$ and $0.8$, respectively. Besides, we set maximum recall numbers for different meta-paths, $15$ for C2T meta-path and $5$ for C2C2T meta-path.
For knowledge-enhanced tag generation module, the size of the training dataset in long-term supervised knowledge injection contains approximately $10,000$ annotated samples and the tuning is performed every two weeks. As for the short-term retrieved knowledge injection, the retrievable database is updated in real-time and we retrieve at most $3$ relevant samples/segments for in-context learning injection and augmented generation injection, respectively.
For the tag confidence calibration module, we eliminate tags with confidence scores less than $0.5$ and rank the remaining tags in order of confidence scores as the result.


% $5$ most similar tagged contents and 

% the model of choice for the small language model as encoder in graph-based tag recall module is BGE~\cite{bge}. 
% The $\delta_{c-t}$ and $\delta_{c-c}$ are set as $0.5$ and $0.8$, respectively. Besides, we set maximum recall numbers for different meta-paths, $15$ for C2T meta-path and $5$ for C2C2T meta-path.
% For the 
% For the tag confidence calibration, we eliminate tags with confidence scores less than $0.5$ and rank the remaining tags in order of confidence scores as the result.
% \cb{more datails? SFT and RAG}






% \subsubsection{Implementation Details}


\subsection{Result Comparison \& Deployment (RQ1)}
Table~\ref{table:all}  summarizes the performance of different methods on three industrial datasets, from which we have the following observations:

\begin{itemize}[leftmargin = 12 pt]
    \item \textbf{Leveraging large language models (LLMs) brings benefit to model performance.} TagGPT, ICXML, and LLM4TC, utilize LLMs to assist the tag generation, achieving better performances than other small language models (SLMs), such as BGE, GTE, and CONAN.
    This phenomenon indicates that the world knowledge and reasoning capabilities of LLMs enable better content understanding and tag generation, significantly improving tagging effectiveness.
    
    
    % However, the models using language models (LMs) with parameters less than one billion, such as BGE, GTE, and CONAN, tend to exhibit poorer performances. These results demonstrate the large ability gap between the LMs and LLMs.
    \item \textbf{Introducing domain knowledge can significantly improve performance.} Although LLMs benefit from general world knowledge, there remains a significant gap compared with domain-specific knowledge. Therefore, LLM4TC injects domain knowledge by fine-tuning the LLMs and achieves better performance than other baselines in all metrics, which validates the importance of domain knowledge injection.
    \item \textbf{The superior performance of LLM4Tag.} We can observe from Table~\ref{table:all} that LLM4Tag yields the best performance on all datasets consistently and significantly, validating the superior effectiveness of our proposed LLM4Tag. Concretely, LLM4Tag beats the best baseline by \textbf{3.7\%}, \textbf{6.1\%}, and \textbf{4.5\%} on three datasets, respectively. This performance improvement is attributed to the advanced nature of our LLM4Tag, including more comprehensive graph-based tag recall, deeper domain-specific knowledge injection, and more reliable confidence calibration.
    \item \textbf{Notably, LLM4Tag has been deployed online and covers all the traffic.} We randomly resampled the online data, and the online report shows consistency between the improvements in the online metrics and those observed in the offline evaluation. Now, LLM4Tag has been deployed in the content tagging system of these three online applications, serving hundreds of millions of users daily.
    
    
\end{itemize}

% 1. 用了llm比使用soft-matching好
% 2. LLM4TC第二好，说明训练数据引入的必要性
% 3. 我们方法最好，balabala


\subsection{The Effectiveness of Graph-based Tag Recall Module (RQ2)}
\label{sec:exp_retrieval}
% 一跳二跳有效性分析
In this subsection, we compare our proposed graph-based tag recall module with match-based recall to validate the effectiveness of candidate tags retrieval over the Browser News Dataset. For fairness, both methods use the same pre-trained language model BGE to encode contents and tags, and the number of candidate tags is fixed as 20.
We define two metrics to evaluate the performance: \textit{\#Right} means the average number of correct tags in candidate tags, and \textit{HR\#k} means the proportion of cases where at least $k$ correct tags are hit in the candidate tag set.


\begin{table}[htbp]
	\caption{Performance comparison between different recall types over the Browser News Dataset.}
	\label{tab:recall}
	\centering
	\resizebox{0.4\textwidth}{!}{
	 \renewcommand\arraystretch{1.1}
		\begin{tabular}{c|cccc}
			\midrule
            Recall Type &  \#Right &  HR\#1 & HR\#2 & HR\#3   \\ \midrule
            Match-based &  4.48 & 0.9586 & 0.8841 & 0.7643 \\
            Ours & 5.37 & 0.9745 & 0.9212 & 0.8425  \\ 
            % \midrule
            % RI &  20.1\%  & 1.6\% & 4.2\%  &  10.2\% \\
			\midrule
		\end{tabular}
		} 
\end{table}

As shown in Table~\ref{tab:recall}, we can find that our graph-based recall method can significantly improve the quality of candidate tags. The metrics \#Right and  HR\#3 increase by $19.8\%$ and $10.2\%$ respectively, which demonstrates that our method yields a more complete and comprehensive candidate tag set via a meta-path-based multi-hop recall mechanism. Moreover, the lifting of HR\#1 illustrates that our method can recall the correct tags when the match-based method encounters challenges in hard cases and fails to select the relevant tags.



\begin{figure}[h]
  \centering
  \vspace{-1.0em}
  \includegraphics[width=0.98\linewidth]{figure/case_study/case_retrieval.pdf}
  \vspace{-1.0em}
  \caption{The online cases for verifying the effectiveness of graph-based tag recall.}
  \label{fig:case_retrieval}
\end{figure}

Besides, to verify the effectiveness and interpretability of our proposed graph-based tag recall, we randomly select some cases in our deployed tagging scenario and visualize the recall results in Figure~\ref{fig:case_retrieval}.
It can be observed that, when match-based recall fails to select the correct tags for some challenging cases, our method effectively retrieves accurate tags by C2C2T meta-path multi-hop traversal in the graph, thus avoiding missing correct tags due to the limited capabilities of SLMs.


% As demonstrated in Figure~\ref{fig:case_retrieval}, the results of match-based recall and graph-based recall for three cases observed in our deployment scenario are presented. 

% Despite the failure of embedding-based retrieval to select the correct tags, graph-based retrieval retrieval successfully identified similar items with accurate tags. This outcome underscores the efficacy of the proposed method.

\subsection{The Effectiveness of Knowledge-enhanced Tag Generation (RQ3)}
In order to systematically evaluate the contribution of domain knowledge-enhanced tag generation (KETG) in our framework, we have designed the following variants:

\begin{itemize}[leftmargin = 12 pt]
\item \textbf{LLM4Tag (w/o KETG)} removes both long-term supervised knowledge injection (LSKI) and short-term retrieved knowledge injection (SRKI), and selects tags using native LLMs.
\item \textbf{LLM4Tag (w/o LSKI)} removes LSKI and only maintains SRKI to inject the short-term domain-specific knowledge.
\item \textbf{LLM4Tag (w/o SRKI)} removes SRKI and only maintains LSKI to inject the long-term domain-specific knowledge.
\item \textbf{LLM4Tag (Ours)} incorporates both LSKI and SRKI to inject the long/short-term domain-specific knowledge.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.36\textwidth]{figure/domain_knowledge_enhancement.pdf}
    \caption{Ablation study about the effectiveness of knowledge-enhanced tag generation module in LLM4Tag.}
    \label{fig:dke}
\end{figure}
Figure~\ref{fig:dke} presents the comparative results on Browser News Dataset, revealing three key findings:

\begin{itemize}[leftmargin = 12 pt]
\item The complete framework achieves optimal performance, demonstrating the synergistic value of combining supervised fine-tuning in long-term supervised knowledge injection with non-parametric short-term retrieved knowledge injection.
\item The removal of either component in knowledge-enhanced tag generation module causes measurable degradation.
Among them, the removal of long-term knowledge results in a greater decline, indicating that long-term knowledge may cover a broader range of domain-specific knowledge and highlighting the importance of SFT in model knowledge injection.

\item The most basic variant (w/o KETG) exhibits the lowest performance, highlighting the crucial role of domain adaptation in specialized tagging tasks within information retrieval systems.
\end{itemize}


% FT facilitates effective assimilation of domain-specific linguistic patterns, while RAG enables dynamic access to contextual knowledge. The integration of these mechanisms results in a complementary reinforcement effect.


% \begin{table*}[htbp]
% 	\caption{the effectiveness of RAG/FT}
% 	\label{tab:recall}
% 	\centering
% 	% \resizebox{0.43\textwidth}{!}{
% 		\begin{tabular}{c|ccc}
% 			\toprule
%             Model &  Acc@1 & Acc@2 & Acc@3 \\ \midrule
%             ours-RAG-FT & 0.8527 & 0.8067 & 0.7780 \\
%             ours-FT & 0.8676 & 0.8147&0.7869\\
%             ours-RAG & 0.8948 & 0.8392 & 0.8051\\
%             ours & 0.9041&0.8511&0.8273\\
% 			\bottomrule
% 		\end{tabular}
% 		% }
% \end{table*}


\subsection{The Effectiveness of Tag Confidence Calibration (RQ4)}
To validate the effectiveness of the tag confidence calibration module, we evaluate model performance on the Browser News Dataset and use different confidence thresholds to achieve different pruning rates. Here we define a metric \textit{Coverage@k} to evaluate the cover rate of final results as:
\begin{equation}
\begin{aligned}
\text{Coverage}@k &= \frac{1}{N'} \sum_{i=1}^{N'} \mathbb{I}\,{\left(|T_i| \geq k\right)}~,\\
% \math\operatorname{check}(|T_i| \geq k)~,\\
\mathbb{I}\,{\left(|T_i| \geq k\right)}&= \begin{cases}
1, & |T_i| \geq k~,\\
0, &  \text{otherwise}~,
\end{cases}
\end{aligned}
\end{equation}
where $T_i$ is the result tags of the $i$-th content and $N'$ is the size of testing dataset. 

% \begin{equation}
% \begin{aligned}
% \operatorname{Acc@k} &=\frac{1}{N'}\sum_{i=1}^{N'}\sum_{j=1}^{k'}  \frac{\operatorname{check}(T_i[j])}{k'}~, \\
% k' &= \min(k, len(T_i))~,\\
% \operatorname{check}(T_i[j])&= \begin{cases}
% 1, & \text{$T_i[j]$ is right}~,\\
% 0, &  \text{otherwise}~,
% \end{cases}
% \end{aligned}
% \end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=0.38\textwidth]{figure/logit.pdf}
\caption{Model Accuracy vs. Tag Coverage for Different Pruning Rates.}
\label{fig:logit}
\end{figure}

% The cover rate in \figurename{\ref{fig:logit}} is calculated as follows:
% \begin{equation}
% \text{Cove Rate}@k = \frac{1}{n} \sum_{i=1}^{n} \delta(s_i \geq k)
% \end{equation}
% where $s_i$ is the number of retained tags for sample 
% $i$ after pruning, and $\delta (\cdot)$ denotes the indicator function. 

% \figurename{\ref{fig:logit}} shows a positive correlation between model performance and tag pruning rate as the threshold $t$ increases, demonstrating that our confidence scores effectively identify hallucinated tags for removal.

% The results show that stricter confidence thresholds (higher $t$) reduce coverage rates due to more aggressive tag elimination. This trade-off between model accuracy and tag coverage necessitates threshold selection based on specific application requirements for LLM4Tag implementations.

As shown in Figure~\ref{fig:logit}, our experimental results indicate that when we increase the pruning rate by setting a larger confidence threshold, the Acc@k is significantly boosted while the Coverage@k continues to decrease, which demonstrates the effectiveness of our proposed tag confidence calibration module. 
Additionally, as the pruning rate increases, the accuracy gains gradually slow down.
This characteristic allows us to set an appropriate confidence threshold in practical deployment scenarios to achieve a balance between prediction accuracy and tag coverage.

% However, as the pruning rate increases, the enhancement gradually decreases, and in addition, the Coverage@k continues to decrease, which suggests the necessity of setting an appropriate confidence threshold to achieve a balance between prediction accuracy and tag coverage in practical deployment scenarios.


% As demonstrated in Figure~\ref{fig:logit}, our experimental findings indicate that:
% \begin{itemize}[leftmargin = 12 pt]
% \item Acc@k is significantly boosted when we increase the pruning rate by turning up the confidence threshold. However, the rate of lifting gradually decreases, 

% While Acc@k steadily improves with stricter confidence thresholds, the gains exhibit diminishing returns, indicating saturation effects in confidence filtering. Initially, higher thresholds effectively prune low-confidence ambiguous tags near decision boundaries, thus driving significant accuracy gains.
% \item 
% \item 
% \end{itemize}

% As demonstrated in Figure~\ref{fig:logit}, our experimental findings indicate a substantial correlation between the rate of tag pruning and the performance of the model. We observe that the coverage rate experiences a linear decline as the prune rate is increased.

% % confidence threshold $t$ is increased. 

% Notably, while accuracy steadily improves with stricter confidence thresholds, the gains exhibit diminishing returns, indicating saturation effects in confidence filtering. Initially, higher thresholds effectively prune low-confidence ambiguous tags near decision boundaries, thus driving significant accuracy gains. However, as thresholds are further tightened, the retained high-confidence tags already approach peak performance levels. Beyond a critical threshold, excessive filtering begins removing correct tags more frequently than errors, yielding minimal accuracy improvements despite sharp prune rate increases. This underscores the necessity of calibrating confidence thresholds to achieve a balance between prediction reliability and coverage in practical deployment scenarios.

% \begin{table}[ht]
% \centering
% \caption{Case Study of Tag-level Confidence Alignment}
% \label{tab:retrieval_case}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|c|l|}
% \hline
% \textbf{Sample Image} & \textbf{Sample Title} & \multicolumn{1}{c|}{\textbf{Tags and According Confidence Scores}} \\
% \hline
% \includegraphics[height=3cm]{figure/case_study/lego.png} & Island Lego City Update & (Lego: 0.93046), (Freight Train: 0.04380), (Building Blocks: 0.97737)\\
% \hline
% \includegraphics[height=3cm]{figure/case_study/bull.png} & Bullfight Winner & (Buffalo: 0.88720), (Bullfight: 0.84662), (Bulldog: 0.13784)\\
% \hline
% \includegraphics[height=3cm]{figure/case_study/tires.png} & Why are all abandoned tires piled up in the desert? & (Tires: 0.92117), (Desert: 0.89528), (Desert Prevention: 0.03445)\\
% \hline
% \includegraphics[height=3cm]{figure/case_study/whale.png} & Orca herd preys & (Orca: 0.90375), (Predatory Behavior: 0.89805), (Seals: 0.97737)\\
% \hline
% \end{tabular}}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Case Study of Tag-level Confidence Alignment}
% \label{tab:logit_case}
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Case} & \multicolumn{1}{c|}{\textbf{Tags and Confidence Scores}} \\
% \hline
% \begin{minipage}[c]{0.2\textwidth}
% \centering
% Island Lego City Update\\
% \includegraphics[height=4cm]{figure/case_study/lego.png}
% \end{minipage} &
% \parbox[t]{0.2\textwidth}{
% Lego: 0.93046\\
% Freight Train: 0.04380\\
% Building Blocks: 0.97737
% }\\
% \hline
% \begin{minipage}[c]{0.2\textwidth}
% \centering
% Bullfight Winner \\
% \includegraphics[height=4cm]{figure/case_study/bull.png}
% \end{minipage} &
% \parbox[t]{0.2\textwidth}{
% Buffalo: 0.88720\\
% Bullfight: 0.84662\\
% Bulldog: 0.13784
% }\\
% \hline
% \begin{minipage}[c]{0.2\textwidth}
% \centering
% Why are all abandoned tires piled up in the desert?\\
% \includegraphics[height=4cm]{figure/case_study/tires.png}
% \end{minipage} &
% \parbox[t]{0.2\textwidth}{
% Tires: 0.92117\\
% Desert: 0.89528\\
% Desert Prevention: 0.03445
% }\\
% \hline
% \begin{minipage}[c]{0.2\textwidth}
% \centering
% Orca herd preys\\
% \includegraphics[height=4cm]{figure/case_study/whale.png}

% \end{minipage} &
% \parbox[t]{0.2\textwidth}{
% Orca: 0.90375\\
% Predatory Behavior: 0.89805\\
% Seals: 0.55549
% }\\
% \hline
% \end{tabular}
% \end{table}

\begin{figure}[h]
  \centering
%   \vspace{-0.8em}
  \includegraphics[width=0.48\textwidth]{figure/case_study/case_logit2.pdf}
%   \vspace{-1.0em}
  \caption{The online cases of tag confidence calibration module. Tags with low confidence are highlighted in red.}
%   \vspace{-1.0em}
  \label{fig:case_logit}
\end{figure}

Furthermore, we randomly select some cases in our deployed tagging scenario and visualize them with confidence scores in Figure~\ref{fig:case_logit}. 
We find that in Cases A, B, and C, irrelevant tags such as "Freight Train," "Bulldog," and "Religious Culture" receive low confidence scores and will be calibrated by our model, and in Case D, the weak-relevant tag, "Seals", which is a non-primary entity in the figure, receive a medium confidence score and will be ranked low in the final results, which further demonstrates the superiority of tag confidence calibration module.

% . All these cases further demonstrates the superiority of tag confidence calibration module.

% As illustrated in \figurename\ref{fig:case_logit}, the generated tags and their corresponding confidence scores are displayed for four specific cases from the deployment scenario. Tags with low confidence scores are highlighted in red.

% For Cases A, B, and C, irrelevant tags such as "Freight Train," "Bulldog," and "Religious Culture" receive low confidence scores from the model. This demonstrates the efficacy of the method in filtering out incorrect tags. In Case D, despite the presence of a seal on the left side of the scene, the primary focus of the content is on a group of orcas hunting. Consequently, "seal" is identified as a non-primary entity, deemed not sufficiently important to be selected as a key tag.This further demonstrates that our confidence alignment approach accurately identifies main entities and retains pertinent tags while eliminating those deemed less relevant.

% \begin{table*}[htbp]
% 	\caption{the effectiveness of Logits}
% 	\centering
% 	% \resizebox{0.43\textwidth}{!}{
% 		\begin{tabular}{c|ccc}
% 			\toprule
%             Remain Ratio &  Acc@1 & Acc@2 & Acc@3 \\ \midrule
%             100\% & 0.8961 & 0.8354 & 0.8007 \\
%             99\% & 0.8982 & 0.8388  & 0.8063 \\
%             98\% & 0.8994 & 0.8411 & 0.8094  \\
%             95\% & 0.9015  & 0.8457 & 0.8155 \\
%             90\% & 0.9054 & 0.8497 & 0.8241 \\
%             80\% & 0.9113 & 0.8606 & 0.8394 \\
%             70\% & 0.9128 & 0.8644 & 0.8450 \\
%             60\% & 0.9203 & 0.8735 & 0.8565 \\
%             50\% & 0.9223 & 0.8777 & 0.8636 \\
% 			\bottomrule
% 		\end{tabular}
% 		% }
% \end{table*}

% \subsection{Case Study}

