
\section{INTRODUCTION}\label{sec:intro}
% intro：
% 	1. 打标是什么？标签对于推搜广链路很重要，打标的准确率、覆盖率等会影响线上的推荐效果。面向推荐的打标和其他作用的内容标签的区别
% 	2. 传统的方式包括（ DL-based/BERT/ERNIE）
% 		a. 缺点：小模型能力弱，效果差；预测效果取决于任务相关的训练集泛化性很差，扩展性很差
% 	3. 大模型的兴起，具有世界知识、强大的内容理解和推理能力，大幅提升打标的效果。
% 		a. 列一下工作
% 			i. Large Language Models Are Zero-Shot Text Classifier
% 				1) 直接大模型打标
% 			ii. Generative AI Text Classification using Ensemble LLM Approaches
% 				1) 通过ensemble实现分类
% 			iii. Text Classification via Large Language Models（ACL 23）
% 				1) In-context learning+ COT +投票
% 			iv. TagGPT
% 				1) 多模态打标，先召回再llm选择；或先生成再找相似的
% 			v. ICXML: An in-context learning framework for zero-shot extreme multi-label classification（ACL 24）
% 				a. LLM生成相似的样本，然后通过zero-shot retriever为没条样本生成标签，当demonstration；为高相关的label生成示例样本
% 				b. LLM根据1,2作上下文生成开域标签再去映射最相关的n个（其实就是LLM生成in-context文本）
% 				c. 再reranking
% 		b. 优点：
% 			i. 效果好于小模型
% 			ii. 可以不需要任务相关的训练集或者只需要少量的训练集，具有极强的泛化性
%           iii. 强大的语义理解和世界知识
% 		c. 缺点：
% 			i. 候选标签集感知能力不强；海量标签仅通过召回实现，但不够完整；（1,5可以，但召回得不够完整，针对多模态场景）？？？
% 			ii. 下游推搜广新领域知识理解不太够（虽然有一些，但不够，2,5做了context learning；4做了sft）
% 			iii. 无法量化标签置信度，这个对于推搜广领域至关重要（此外llm存在幻觉问题）
% 	4. 我们的方案
% 		a. 基于图的多路召回（文本，图片，相似内容） （针对缺点1  1跳+2跳）
% 		b. （垂域知识学习框架）Prompt learning + SFT 选择相关的标签（针对缺点2）
% 			i. In-context learning：Few-shot 相似样本应该打上什么样的标签
% 			ii. RAG增强：（1）内容、标签补充说明；（2）人工规则
% 			iii. SFT
% 		c. logits技术+SFT（针对缺点3）
% 	5. Contribution（三点）
% 		a. 我们提出了一个大模型作自动标签生成的框架，具备拓展性（标签可拓展）、持续学习性（可用垂域知识学习框架和SFT修正效果）、量化性（分数可度量）
% 		b. AutoTag包括三个阶段（多路召回，LLM打标，logit分）来自动地为内容生成合适的标签。
% 		c. 实验。。。

Tagging is the process of assigning tags, such as keywords or labels, to digital content, products, or users to facilitate organization, retrieval, and analysis. Tags serve as descriptors that summarize key attributes or themes, enabling efficient categorization and searchability, which play a crucial role in information retrieval systems, such as search engines, recommender systems, content management, and social networks~\cite{zhang2011tag,gupta2010survey,bischoff2008can,li2008tag}.
For information retrieval systems, tags are widely used in various stages, including content distribution strategies, ranking algorithms, and operational decision-making processes~\cite{dattolo2010role,ahmadian2022deep}.
Therefore, tagging systems must not only require high accuracy and coverage, but also provide interpretability and strong confidence.

Before the era of Large Language Models (LLMs), the mainstream tagging methods mainly included statistics-based (\textit{i.e.}, TF-IDF-based~\cite{qaiser2018text}, LDA-based~\cite{diaz2010lda}), supervised classification-based (\textit{i.e.}, CNN-based~\cite{zhang2015sensitivity,elnagar2019automatic}, RNN-based~\cite{liu2016recurrent,wang2015unified}), pre-trained model-based methods (\textit{i.e.}, BERT-based~\cite{hasegawa2021bert,bge,ozan2021auto}), 
\textit{etc}.
However, limited by the model capacity, these methods cannot achieve satisfactory results, especially for complex contents. Besides, they heavily rely on annotated training data, resulting in limited generalization and transferability.

The rise of LLMs, with their extensive world knowledge, powerful semantic understanding, and reasoning capabilities, has significantly enhanced the effectiveness of tagging systems. 
LLM4TC~\cite{chae2023large} employs LLMs directly as tagging classifiers and leverages annotated data to fine-tune LLMs. TagGPT~\cite{li2023taggpt} further introduces a match-based recall to filter out a small-scale tag set to address the limited input length of LLMs. ICXML~\cite{zhu2023icxml} proposes an in-context learning algorithm to guide LLMs to further improve performance.

\begin{figure}[h]
\centering
\includegraphics[width=0.98\linewidth]{figure/Tag_intro.pdf}
\caption{LLM-enhanced tagging systems and their three limitations. (L1) Simple match-based recall is prone to missing relevant tags; (L2) The emerging domain-specific knowledge may not align with the pre-trained knowledge of LLMs; (L3) LLMs cannot accurately quantify tag confidence.}
\label{fig:tag_intro}
\end{figure}


However, existing LLM-enhanced tagging algorithms exhibit several critical limitations that require improvement (shown in Figure~\ref{fig:tag_intro}):
\begin{enumerate}[label=(L\arabic*),leftmargin=22pt]
    \item Constrained by the input length and inference efficiency of LLMs, existing methods adopt simple match-based recall to filter out a small-scale candidate tag set~\cite{li2023taggpt,zhu2023icxml}, which is prone to missing relevant tags, thereby reducing accuracy.
    \item General purpose LLMs pre-trained in publicly available corpora exhibit limitations in comprehending emerging domain-specific knowledge within information retrieval, leading to lower accuracy in challenging cases~\cite{chae2023large,li2024ecomgpt}.
    \item Due to the hallucination and uncertainty~\cite{ji2023survey,huang2023look}, LLMs cannot accurately quantify tag confidence, which is crucial for information retrieval applications.
\end{enumerate}



To address the three limitations of existing approaches, we propose an automatic tagging system called LLM4Tag, which consists of three key modules.
Specifically, to improve the completeness of candidate tags (L1), we propose a graph-based tag recall module designed to construct small-scale, highly relevant candidate tags from a massive tag repository efficiently and comprehensively.
To enhance domain-specific knowledge and adaptability to emerging information of general-purpose LLMs (L2), a knowledge-enhanced tag generation module that integrates long-term supervised knowledge injection and short-term retrieved knowledge injection is designed to generate accurate tags.
Moreover, a tag confidence calibration module is introduced to generates reliable tag confidence scores, ensuring more robust and trustworthy results (L3). 

% 	5. Contribution（三点）
% 		a. 我们提出了一个大模型作自动标签生成的框架，具备拓展性（标签可拓展）、持续学习性（可用垂域知识学习框架和SFT修正效果）、量化性（分数可度量）
% 		b. AutoTag包括三个阶段（多路召回，LLM打标，logit分）来自动地为内容生成合适的标签。
% 		c. 实验。。。

% Our main contributions are summarized as follows:
To summarize, the main contributions of this paper can be highlighted as follows:
\begin{itemize}[leftmargin=12pt]
\item We propose an LLM-enhanced tagging framework LLM4Tag, characterized by completeness, continuous knowledge evolution, and quantifiability.
\item  To address the limitations of existing approaches, LLM4Tag integrates three key modules: graph-based tag recall, knowledge-enhanced tag generation, and tag confidence calibration, ensuring the generation of accurate and reliable tags.
\item LLM4Tag achieves state-of-the-art in three large-scale industrial datasets with detailed analysis that provides a deeper understanding of model performance.
Moreover, LLM4Tag has been deployed online for content tagging, serving hundreds of millions of users. 
\end{itemize}

% The rest of the paper is organized as follows. We discuss related work in Section~2 and introduce a detailed description of our proposed framework in Section~3. The experimental settings and corresponding results are shown in Section~4. Finally, we conclude the paper in Section~5.












