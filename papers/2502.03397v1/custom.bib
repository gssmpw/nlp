@inproceedings{
    zhan_2024_reappraisal,
    title={Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided},
    author={Hongli Zhan and Allen Zheng and Yoon Kyung Lee and Jina Suh and Junyi Jessy Li and Desmond Ong},
    booktitle={Proceedings of the First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=yK8MT91dQY}
}

@inproceedings{hu2022lora,
    title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
    author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{zhan-etal-2023-evaluating,
    title = "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
    author = "Zhan, Hongli  and
      Ong, Desmond  and
      Li, Junyi Jessy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.962/",
    doi = "10.18653/v1/2023.findings-emnlp.962",
    pages = "14418--14446",
    abstract = "The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models {---} excelling at a wide range of NLP tasks {---} to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public."
}

@article{guan2025deliberativealignmentreasoningenables,
      title={Deliberative Alignment: Reasoning Enables Safer Language Models}, 
      author={Melody Y. Guan and Manas Joglekar and Eric Wallace and Saachi Jain and Boaz Barak and Alec Helyar and Rachel Dias and Andrea Vallone and Hongyu Ren and Jason Wei and Hyung Won Chung and Sam Toyer and Johannes Heidecke and Alex Beutel and Amelia Glaese},
      year={2025},
      eprint={2412.16339},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.16339}, 
}

@inproceedings{khan2024debating,
    title={Debating with More Persuasive {LLM}s Leads to More Truthful Answers},
    author={Akbir Khan and John Hughes and Dan Valentine and Laura Ruis and Kshitij Sachan and Ansh Radhakrishnan and Edward Grefenstette and Samuel R. Bowman and Tim Rockt{\"a}schel and Ethan Perez},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=iLCZtl7FTa}
}

@article{donyehiya2024futureopenhumanfeedback,
      title={The Future of Open Human Feedback}, 
      author={Shachar Don-Yehiya and Ben Burtenshaw and Ramon Fernandez Astudillo and Cailean Osborne and Mimansa Jaiswal and Tzu-Sheng Kuo and Wenting Zhao and Idan Shenfeld and Andi Peng and Mikhail Yurochkin and Atoosa Kasirzadeh and Yangsibo Huang and Tatsunori Hashimoto and Yacine Jernite and Daniel Vila-Suero and Omri Abend and Jennifer Ding and Sara Hooker and Hannah Rose Kirk and Leshem Choshen},
      year={2024},
      eprint={2408.16961},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2408.16961}, 
}


@article{bakker2022fine,
  title={Fine-tuning language models to find agreement among humans with diverse preferences},
  author={Bakker, Michiel and Chadwick, Martin and Sheahan, Hannah and Tessler, Michael and Campbell-Gillingham, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matt and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38176--38189},
  year={2022}
}

@article{bai2022traininghelpfulharmlessassistant,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.05862}, 
}

@inproceedings{Lee2024RLAIFVR,
    title={{RLAIF} vs. {RLHF}: Scaling Reinforcement Learning from Human Feedback with {AI} Feedback},
    author={Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Ren Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=uydQ2W41KO}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{ganguli2022redteaminglanguagemodels,
      title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned}, 
      author={Deep Ganguli and Liane Lovitt and Jackson Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Ben Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zac Hatfield-Dodds and Tom Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom Brown and Nicholas Joseph and Sam McCandlish and Chris Olah and Jared Kaplan and Jack Clark},
      year={2022},
      eprint={2209.07858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.07858}, 
}

@inproceedings{roit-etal-2023-factually,
    title = "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback",
    author = "Roit, Paul  and
      Ferret, Johan  and
      Shani, Lior  and
      Aharoni, Roee  and
      Cideron, Geoffrey  and
      Dadashi, Robert  and
      Geist, Matthieu  and
      Girgin, Sertan  and
      Hussenot, Leonard  and
      Keller, Orgad  and
      Momchev, Nikola  and
      Ramos Garea, Sabela  and
      Stanczyk, Piotr  and
      Vieillard, Nino  and
      Bachem, Olivier  and
      Elidan, Gal  and
      Hassidim, Avinatan  and
      Pietquin, Olivier  and
      Szpektor, Idan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.344/",
    doi = "10.18653/v1/2023.acl-long.344",
    pages = "6252--6272",
    abstract = "Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual-entailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience and conciseness of the generated summaries."
}

@inproceedings{yang2024rlcd,
    title={{RLCD}: Reinforcement Learning from Contrastive Distillation for {LM} Alignment},
    author={Kevin Yang and Dan Klein and Asli Celikyilmaz and Nanyun Peng and Yuandong Tian},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=v3XXtxWKi6}
}

@inproceedings{kwon2023reward,
    title={Reward Design with Language Models},
    author={Minae Kwon and Sang Michael Xie and Kalesha Bullard and Dorsa Sadigh},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=10uNUgI5Kl}
}

@inproceedings{wang-etal-2022-super,
    title = "Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.340/",
    doi = "10.18653/v1/2022.emnlp-main.340",
    pages = "5085--5109",
    abstract = "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions{---}training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9{\%} on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models."
}

@book{laclau2005populist,
  title={On populist reason},
  author={Laclau, Ernesto},
  year={2005},
  publisher={Verso}
}

@book{levi2013introduction,
  title={Introduction to the work of Marcel Mauss},
  author={L{\'e}vi-Strauss, Claude},
  year={2013},
  publisher={Routledge}
}

@inproceedings{kirk-etal-2023-past,
    title = "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
    author = {Kirk, Hannah Rose  and
      Bean, Andrew M.  and
      Vidgen, Bertie  and
      R{\"o}ttger, Paul  and
      Hale, Scott A.},
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.148/",
    doi = "10.18653/v1/2023.emnlp-main.148",
    pages = "2409--2430",
    abstract = "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges."
}

@article{kirk2023empty,
      title={The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ``Alignment'' in Large Language Models}, 
      author={Hannah Rose Kirk and Bertie Vidgen and Paul Röttger and Scott A. Hale},
      year={2023},
      eprint={2310.02457},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02457}, 
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{groeneveld-etal-2024-olmo,
    title = "{OLM}o: Accelerating the Science of Language Models",
    author = "Groeneveld, Dirk  and
      Beltagy, Iz  and
      Walsh, Evan  and
      Bhagia, Akshita  and
      Kinney, Rodney  and
      Tafjord, Oyvind  and
      Jha, Ananya  and
      Ivison, Hamish  and
      Magnusson, Ian  and
      Wang, Yizhong  and
      Arora, Shane  and
      Atkinson, David  and
      Authur, Russell  and
      Chandu, Khyathi  and
      Cohan, Arman  and
      Dumas, Jennifer  and
      Elazar, Yanai  and
      Gu, Yuling  and
      Hessel, Jack  and
      Khot, Tushar  and
      Merrill, William  and
      Morrison, Jacob  and
      Muennighoff, Niklas  and
      Naik, Aakanksha  and
      Nam, Crystal  and
      Peters, Matthew  and
      Pyatkin, Valentina  and
      Ravichander, Abhilasha  and
      Schwenk, Dustin  and
      Shah, Saurabh  and
      Smith, William  and
      Strubell, Emma  and
      Subramani, Nishant  and
      Wortsman, Mitchell  and
      Dasigi, Pradeep  and
      Lambert, Nathan  and
      Richardson, Kyle  and
      Zettlemoyer, Luke  and
      Dodge, Jesse  and
      Lo, Kyle  and
      Soldaini, Luca  and
      Smith, Noah  and
      Hajishirzi, Hannaneh",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.841/",
    doi = "10.18653/v1/2024.acl-long.841",
    pages = "15789--15809",
    abstract = "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation."
}

@inproceedings{
    zhao2024selfguide,
    title={Self-Guide: Better Task-Specific Instruction Following via Self-Synthetic Finetuning},
    author={Chenyang Zhao and Xueying Jia and Vijay Viswanathan and Graham Neubig and Tongshuang Wu},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=Dt6qXZsgaU}
}

@article{xu2024magpie,
  title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing},
  author={Zhangchen Xu and Fengqing Jiang and Luyao Niu and Yuntian Deng and Radha Poovendran and Yejin Choi and Bill Yuchen Lin},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.08464},
  url={https://api.semanticscholar.org/CorpusID:270391432}
}


@article{openai2024openaio1card,
      title={OpenAI o1 System Card},
      author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
      year={2024},
      eprint={2412.16720},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.16720}, 
}

@inproceedings{liu2024what,
    title={What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
    author={Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=BTKAeLqLMw}
}

@article{openai2024gpt4ocard,
      title={GPT-4o System Card}, 
      author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@inproceedings{ye2024flask,
    title={{FLASK}: Fine-grained Language Model Evaluation based on Alignment Skill Sets},
    author={Seonghyeon Ye and Doyoung Kim and Sungdong Kim and Hyeonbin Hwang and Seungone Kim and Yongrae Jo and James Thorne and Juho Kim and Minjoon Seo},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=CYmF38ysDa}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{yang2024qwen2,
  title={Qwen2.5 Technical Report}, 
      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@inproceedings{sprague2024musr,
    title={Mu{SR}: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning},
    author={Zayne Rea Sprague and Xi Ye and Kaj Bostrom and Swarat Chaudhuri and Greg Durrett},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=jenyYQzue1}
}

@inproceedings{suzgun-etal-2023-challenging,
    title = "Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
    author = {Suzgun, Mirac  and
      Scales, Nathan  and
      Sch{\"a}rli, Nathanael  and
      Gehrmann, Sebastian  and
      Tay, Yi  and
      Chung, Hyung Won  and
      Chowdhery, Aakanksha  and
      Le, Quoc  and
      Chi, Ed  and
      Zhou, Denny  and
      Wei, Jason},
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.824/",
    doi = "10.18653/v1/2023.findings-acl.824",
    pages = "13003--13051",
    abstract = "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65{\%} of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
}

@inproceedings{wang2024mmlupro,
    title={{MMLU}-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark},
    author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen},
    booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
    year={2024},
    url={https://openreview.net/forum?id=y10DM6R2r3}
}

@inproceedings{rein2024gpqa,
    title={{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
    author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=Ti67584b98}
}

@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472/",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
    abstract = "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as {\textquotedblleft}A woman sits at a piano,{\textquotedblright} a machine must select the most likely followup: {\textquotedblleft}She sets her fingers on the keys.{\textquotedblright} With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\ensuremath{>}}95{\%} accuracy), state-of-the-art models struggle ({\ensuremath{<}}48{\%}). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical {\textquoteleft}Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges."
}

@inproceedings{lee2024LLM,
      title={Large Language Models Produce Responses Perceived to be Empathic}, 
      author={Yoon Kyung Lee and Jina Suh and Hongli Zhan and Junyi Jessy Li and Desmond C Ong},
      year={2024},
      booktitle={Proceedings of the 12th IEEE International Conference on Affective Computing and Intelligent Interaction}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@article{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@inproceedings{
    huang2024large,
    title={Large Language Models Cannot Self-Correct Reasoning Yet},
    author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=IkmD3fKBPQ}
}

@article{cui2024efficienteffectivetextencoding,
      title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca}, 
      author={Yiming Cui and Ziqing Yang and Xin Yao},
      year={2024},
      eprint={2304.08177},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08177}, 
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{zhou2023instructionfollowingevaluationlargelanguage,
      title={Instruction-Following Evaluation for Large Language Models}, 
      author={Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and Siddhartha Brahma and Sujoy Basu and Yi Luan and Denny Zhou and Le Hou},
      year={2023},
      eprint={2311.07911},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07911}, 
}

@article{jiang2024mixtralexperts,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04088}, 
}

@article{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}


@article{shi2024wildfeedbackaligningllmsinsitu,
      title={WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback}, 
      author={Taiwei Shi and Zhuoer Wang and Longqi Yang and Ying-Chun Lin and Zexue He and Mengting Wan and Pei Zhou and Sujay Jauhar and Xiaofeng Xu and Xia Song and Jennifer Neville},
      year={2024},
      eprint={2408.15549},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.15549}, 
}

@article{Demszky2023UsingLL,
  title={Using large language models in psychology},
  author={Dorottya Demszky and Diyi Yang and David S. Yeager and Christopher J. Bryan and Margarett Clapper and Susannah Chandhok and Johannes C. Eichstaedt and Cameron A. Hecht and Jeremy P. Jamieson and Meghann Johnson and Michaela Jones and Danielle Krettek-Cobb and Leslie Lai and Nirel JonesMitchell and Desmond C. Ong and Carol S. Dweck and James J. Gross and James W. Pennebaker},
  journal={Nature Reviews Psychology},
  year={2023},
  volume={2},
  pages={688 - 701},
  url={https://api.semanticscholar.org/CorpusID:264107446}
}

@article{kim2024biggenbenchprincipledbenchmark,
      title={The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models}, 
      author={Seungone Kim and Juyoung Suk and Ji Yong Cho and Shayne Longpre and Chaeeun Kim and Dongkeun Yoon and Guijin Son and Yejin Cho and Sheikh Shafayat and Jinheon Baek and Sue Hyun Park and Hyeonbin Hwang and Jinkyung Jo and Hyowon Cho and Haebin Shin and Seongyun Lee and Hanseok Oh and Noah Lee and Namgyu Ho and Se June Joo and Miyoung Ko and Yoonjoo Lee and Hyungjoo Chae and Jamin Shin and Joel Jang and Seonghyeon Ye and Bill Yuchen Lin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2406.05761},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05761}, 
}

@inproceedings{ConstitutionMaker-2024,
    author = {Petridis, Savvas and Wedin, Benjamin D and Wexler, James and Pushkarna, Mahima and Donsbach, Aaron and Goyal, Nitesh and Cai, Carrie J and Terry, Michael},
    title = {ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles},
    year = {2024},
    isbn = {9798400705083},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3640543.3645144},
    doi = {10.1145/3640543.3645144},
    abstract = {Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot’s outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model’s outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model’s behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot’s response; each mode of feedback automatically generates a principle that is inserted into the chatbot’s prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.},
    booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
    pages = {853–868},
    numpages = {16},
    keywords = {Conversational AI, Feedback, Generative AI, Interactive Critique, Large Language Models},
    location = {<conf-loc>, <city>Greenville</city>, <state>SC</state>, <country>USA</country>, </conf-loc>},
    series = {IUI '24}
}

@inproceedings{petridis-etal-2024-constitutionalexperts,
    title = "{C}onstitutional{E}xperts: Training a Mixture of Principle-based Prompts",
    author = "Petridis, Savvas  and
      Wedin, Ben  and
      Yuan, Ann  and
      Wexler, James  and
      Thain, Nithum",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.52/",
    doi = "10.18653/v1/2024.acl-short.52",
    pages = "574--582",
    abstract = "Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization techniques by 10.9{\%} (F1) and that mixture-of-experts improves all techniques, suggesting its broad applicability."
}

@article{Li2024SyntheticD,
  title={Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models},
  author={Haoran Li and Qingxiu Dong and Zhengyang Tang and Chaojun Wang and Xingxing Zhang and Haoyang Huang and Shaohan Huang and Xiaolong Huang and Zeqiang Huang and Dongdong Zhang and Yuxian Gu and Xin Cheng and Xun Wang and Si-Qing Chen and Li Dong and Wei Lu and Zhifang Sui and Benyou Wang and Wai Lam and Furu Wei},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.13064},
  url={https://api.semanticscholar.org/CorpusID:267759981}
}

@article{bai2022constitutional,
  title={Constitutional AI: Harmlessness from AI Feedback},
  author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.08073},
  url={https://api.semanticscholar.org/CorpusID:254823489}
}

@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@inproceedings{NEURIPS2023_4dbb61cb,
 author = {Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {24678--24704},
 publisher = {Curran Associates, Inc.},
 title = {BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}

@article{askell2021generallanguageassistantlaboratory,
      title={A General Language Assistant as a Laboratory for Alignment}, 
      author={Amanda Askell and Yuntao Bai and Anna Chen and Dawn Drain and Deep Ganguli and Tom Henighan and Andy Jones and Nicholas Joseph and Ben Mann and Nova DasSarma and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Jackson Kernion and Kamal Ndousse and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2021},
      eprint={2112.00861},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.00861}, 
}

@inproceedings{zhao-etal-2021-ethical,
    title = "Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?",
    author = "Zhao, Jieyu  and
      Khashabi, Daniel  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Chang, Kai-Wei",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.364/",
    doi = "10.18653/v1/2021.findings-acl.364",
    pages = "4158--4164"
}

@article{jin2022make,
  title={When to make exceptions: Exploring language models as accounts of human moral judgment},
  author={Jin, Zhijing and Levine, Sydney and Gonzalez Adauto, Fernando and Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and Mihalcea, Rada and Tenenbaum, Josh and Sch{\"o}lkopf, Bernhard},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={28458--28473},
  year={2022}
}

@inproceedings{liu2024chain,
    title={Chain of Hindsight aligns Language Models with Feedback},
    author={Hao Liu and Carmelo Sferrazza and Pieter Abbeel},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=6xfe4IVcOu}
}

@inproceedings{hancock-etal-2019-learning,
    title = "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",
    author = "Hancock, Braden  and
      Bordes, Antoine  and
      Mazare, Pierre-Emmanuel  and
      Weston, Jason",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1358/",
    doi = "10.18653/v1/P19-1358",
    pages = "3667--3684",
    abstract = "The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user`s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot`s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision."
}

@inproceedings{korbak2023pretraining,
  title={Pretraining language models with human preferences},
  author={Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika Vinayak and Buckley, Christopher and Phang, Jason and Bowman, Samuel R and Perez, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={17506--17533},
  year={2023},
  organization={PMLR}
}

@inproceedings{yang-etal-2022-re3,
    title = "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    author = "Yang, Kevin  and
      Tian, Yuandong  and
      Peng, Nanyun  and
      Klein, Dan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.296/",
    doi = "10.18653/v1/2022.emnlp-main.296",
    pages = "4393--4479",
    abstract = "We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3`s stories as having a coherent overarching plot (by 14{\%} absolute increase), and relevant to the given initial premise (by 20{\%})."
}

@inproceedings{fu-etal-2024-gptscore,
    title = "{GPTS}core: Evaluate as You Desire",
    author = "Fu, Jinlan  and
      Ng, See-Kiong  and
      Jiang, Zhengbao  and
      Liu, Pengfei",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.365/",
    doi = "10.18653/v1/2024.naacl-long.365",
    pages = "6556--6576",
    abstract = "Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation{--}how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available."
}

@article{zhou2021narle,
  title={NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback}, 
      author={Ruijie Zhou and Soham Deshmukh and Jeremiah Greer and Charles Lee},
      year={2021},
      eprint={2110.02148},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.02148}, 
}

@inproceedings{
    liu2022second,
    title={Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits},
    author={Ruibo Liu and Chenyan Jia and Ge Zhang and Ziyu Zhuang and Tony X Liu and Soroush Vosoughi},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=u6OfmaGIya1}
}

@article{liu2023trustworthy,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment}, 
      author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo and Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hang Li},
      year={2024},
      eprint={2308.05374},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.05374}, 
}


@article{gross1998emerging,
  title={The emerging field of emotion regulation: An integrative review},
  author={Gross, James J},
  journal={Review of general psychology},
  volume={2},
  number={3},
  pages={271--299},
  year={1998},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{mcrae2016cognitive,
  title={Cognitive emotion regulation: A review of theory and scientific findings},
  author={McRae, Kateri},
  journal={Current Opinion in Behavioral Sciences},
  volume={10},
  pages={119--124},
  year={2016},
  publisher={Elsevier}
}

@article{goldin2008neural,
  title={The neural bases of emotion regulation: reappraisal and suppression of negative emotion},
  author={Goldin, Philippe R and McRae, Kateri and Ramel, Wiveka and Gross, James J},
  journal={Biological psychiatry},
  volume={63},
  number={6},
  pages={577--586},
  year={2008},
  publisher={Elsevier}
}

@article{YeoOng2023Appraisals,
  title={A meta-analytic review of the associations between cognitive appraisals and emotions in cognitive appraisal theory},
  author={Gerard Yeo and Desmond C. Ong},
  journal={PsyArXiv},
  url={https://psyarxiv.com/ystxc},
  year={2023}
}

@article{moors2013appraisal,
  title={Appraisal theories of emotion: State of the art and future development},
  author={Moors, Agnes and Ellsworth, Phoebe C and Scherer, Klaus R and Frijda, Nico H},
  journal={Emotion review},
  volume={5},
  number={2},
  pages={119--124},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}

@book{ortony2022cognitive,
  title={The cognitive structure of emotions},
  author={Ortony, Andrew and Clore, Gerald L and Collins, Allan},
  year={2022},
  publisher={Cambridge university press}
}

@book{lazarus1966psychological,
      title={Psychological stress and the coping process.},
      author={Lazarus, Richard S},
      year={1966},
      publisher={McGraw-Hill}
}

@incollection{ellsworth2003appraisal,
  title={Appraisal processes in emotion},
  author={Ellsworth, Phoebe C and Scherer, Klaus R},
  booktitle={Handbook of Affective Sciences},
  editor={R. J. Davidson and K. R. Scherer and H. H. Goldsmith},
  year={2003},
  pages={572-595},
  publisher={Oxford University Press}
}

@book{arnold1960emotion,
      title={Emotion and personality.},
      author={Arnold, Magda B},
      year={1960},
      publisher={Columbia University Press}
}

@article{waugh2016emotion,
  title={Emotion regulation changes the duration of the BOLD response to emotional stimuli},
  author={Waugh, Christian E and Zarolia, Pareezad and Mauss, Iris B and Lumian, Daniel S and Ford, Brett Q and Davis, Tchikima S and Ciesielski, Bethany G and Sams, Katherine V and McRae, Kateri},
  journal={Social Cognitive and Affective Neuroscience},
  volume={11},
  number={10},
  pages={1550--1559},
  year={2016},
  publisher={Oxford University Press}
}

@article{ray2010cognitive,
  title={Cognitive reappraisal of negative affect: converging evidence from EMG and self-report.},
  author={Ray, Rebecca D and McRae, Kateri and Ochsner, Kevin N and Gross, James J},
  journal={Emotion},
  volume={10},
  number={4},
  pages={587},
  year={2010},
  publisher={American Psychological Association}
}

@article{buhle2014cognitive,
  title={Cognitive reappraisal of emotion: a meta-analysis of human neuroimaging studies},
  author={Buhle, Jason T and Silvers, Jennifer A and Wager, Tor D and Lopez, Richard and Onyemekwu, Chukwudi and Kober, Hedy and Weber, Jochen and Ochsner, Kevin N},
  journal={Cerebral cortex},
  volume={24},
  number={11},
  pages={2981--2990},
  year={2014},
  publisher={Oxford University Press}
}

@article{ochsner2002rethinking,
  title={Rethinking feelings: an FMRI study of the cognitive regulation of emotion},
  author={Ochsner, Kevin N and Bunge, Silvia A and Gross, James J and Gabrieli, John DE},
  journal={Journal of cognitive neuroscience},
  volume={14},
  number={8},
  pages={1215--1229},
  year={2002},
  publisher={MIT Press}
}

@article{gross2003individual,
  title={Individual differences in two emotion regulation processes: implications for affect, relationships, and well-being.},
  author={Gross, James J and John, Oliver P},
  journal={Journal of personality and social psychology},
  volume={85},
  number={2},
  pages={348},
  year={2003},
  publisher={American Psychological Association}
}

@article{gross1998antecedent,
  title={Antecedent-and response-focused emotion regulation: divergent consequences for experience, expression, and physiology.},
  author={Gross, James J},
  journal={Journal of personality and social psychology},
  volume={74},
  number={1},
  pages={224},
  year={1998},
  publisher={American Psychological Association}
}

@inproceedings{chen-etal-2024-iteralign,
    title = "{I}ter{A}lign: Iterative Constitutional Alignment of Large Language Models",
    author = "Chen, Xiusi  and
      Wen, Hongzhi  and
      Nag, Sreyashi  and
      Luo, Chen  and
      Yin, Qingyu  and
      Li, Ruirui  and
      Li, Zheng  and
      Wang, Wei",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.78/",
    doi = "10.18653/v1/2024.naacl-long.78",
    pages = "1423--1433",
    abstract = "With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to 13.5{\%} in harmlessness."
}

@inproceedings{cui2024ultrafeedback,
    title={{ULTRAFEEDBACK}: Boosting Language Models with Scaled {AI} Feedback},
    author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Bingxiang He and Wei Zhu and Yuan Ni and Guotong Xie and Ruobing Xie and Yankai Lin and Zhiyuan Liu and Maosong Sun},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=BOorDpKHiJ}
}

@inproceedings{
    madaan2023selfrefine,
    title={Self-Refine: Iterative Refinement with Self-Feedback},
    author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=S37hOerQLB}
}

@article{leike2018scalableagentalignmentreward,
      title={Scalable agent alignment via reward modeling: a research direction}, 
      author={Jan Leike and David Krueger and Tom Everitt and Miljan Martic and Vishal Maini and Shane Legg},
      year={2018},
      eprint={1811.07871},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.07871}, 
}

@inproceedings{lin2024the,
    title={The Unlocking Spell on Base {LLM}s:  Rethinking Alignment via In-Context Learning},
    author={Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=wxJ0eXwwda}
}

@inproceedings{li2024rain,
    title={{RAIN}: Your Language Models Can Align Themselves without Finetuning},
    author={Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=pETSfWMUzy}
}

@inproceedings{zhou2023lima,
    title={{LIMA}: Less Is More for Alignment},
    author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=KBMOKmX2he}
}

@article{ji2024aialignmentcomprehensivesurvey,
      title={AI Alignment: A Comprehensive Survey}, 
      author={Jiaming Ji and Tianyi Qiu and Boyuan Chen and Borong Zhang and Hantao Lou and Kaile Wang and Yawen Duan and Zhonghao He and Jiayi Zhou and Zhaowei Zhang and Fanzhi Zeng and Kwan Yee Ng and Juntao Dai and Xuehai Pan and Aidan O'Gara and Yingshan Lei and Hua Xu and Brian Tse and Jie Fu and Stephen McAleer and Yaodong Yang and Yizhou Wang and Song-Chun Zhu and Yike Guo and Wen Gao},
      year={2024},
      eprint={2310.19852},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.19852}, 
}

@inproceedings{sorensen2024position,
    title={Position: A Roadmap to Pluralistic Alignment},
    author={Taylor Sorensen and Jared Moore and Jillian Fisher and Mitchell L Gordon and Niloofar Mireshghallah and Christopher Michael Rytting and Andre Ye and Liwei Jiang and Ximing Lu and Nouha Dziri and Tim Althoff and Yejin Choi},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=gQpBnRHwxM}
}

@article{Kundu2023SpecificVG,
  title={Specific versus General Principles for Constitutional AI},
  author={Sandipan Kundu and Yuntao Bai and Saurav Kadavath and Amanda Askell and Andrew Callahan and Anna Chen and Anna Goldie and Avital Balwit and Azalia Mirhoseini and Brayden McLean and Catherine Olsson and Cassie Evraets and Eli Tran-Johnson and Esin Durmus and Ethan Perez and John Kernion and Jamie Kerr and Kamal Ndousse and Karina Nguyen and Nelson Elhage and Newton Cheng and Nicholas Schiefer and Nova Dassarma and Oliver Rausch and Robin Larson and Shannon Yang and Shauna Kravec and Timothy Telleen-Lawton and Thomas I. Liao and Tom Henighan and Tristan Hume and Zac Hatfield-Dodds and S{\"o}ren Mindermann and Nicholas Joseph and Sam McCandlish and Jared Kaplan},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.13798},
  url={https://api.semanticscholar.org/CorpusID:264426105}
}

@InProceedings{pmlr-v202-zhou23g,
  title = 	 {Controlled Text Generation with Natural Language Instructions},
  author =       {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Wilcox, Ethan and Cotterell, Ryan and Sachan, Mrinmaya},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {42602--42613},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/zhou23g/zhou23g.pdf},
  url = 	 {https://proceedings.mlr.press/v202/zhou23g.html},
  abstract = 	 {Large language models can be prompted to pro- duce fluent output for a wide range of tasks without being specifically trained to do so. Nevertheless, it is notoriously difficult to control their generation in such a way that it satisfies user-specified constraints. In this paper, we present InstructCTG, a simple controlled text generation framework that incorporates different constraints by verbalizing them as natural language instructions. We annotate natural texts through a combination of off-the-shelf NLP tools and simple heuristics with the linguistic and extra-linguistic constraints they satisfy. Then, we verbalize the constraints into natural language instructions to form weakly supervised training data, i.e., we prepend the natural language verbalizations of the constraints in front of their corresponding natural language sentences. Next, we fine-tune a pre-trained language model on the augmented corpus. Compared to existing methods, InstructCTG is more flexible in terms of the types of constraints it allows the practitioner to use. It also does not require any modification of the decoding procedure. Finally, InstructCTG allows the model to adapt to new constraints without re-training through the use of in-context learning.}
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@article{anwar2024foundational,
  title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models},
  author={Usman Anwar and Abulhair Saparov and Javier Rando and Daniel Paleka and Miles Turpin and Peter Hase and Ekdeep Singh Lubana and Erik Jenner and Stephen Casper and Oliver Sourbut and Benjamin L. Edelman and Zhaowei Zhang and Mario Günther and Anton Korinek and Jose Hernandez-Orallo and Lewis Hammond and Eric Bigelow and Alexander Pan and Lauro Langosco and Tomasz Korbak and Heidi Zhang and Ruiqi Zhong and Seán O hÉigeartaigh and Gabriel Recchia and Giulio Corsi and Alan Chan and Markus Anderljung and Lilian Edwards and Yoshua Bengio and Danqi Chen and Samuel Albanie and Tegan Maharaj and Jakob Foerster and Florian Tramer and He He and Atoosa Kasirzadeh and Yejin Choi and David Krueger},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.09932},
  url={https://api.semanticscholar.org/CorpusID:269149478}
}

@inproceedings{wei2022chain,
    title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
    author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{Kim2024Prometheus2A,
  title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models},
  author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.01535},
  url={https://api.semanticscholar.org/CorpusID:269502688}
}

@inproceedings{zheng2023judging,
title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=uccHPGDlao}
}

@article{Gunasekar2023TextbooksAA,
  title={Textbooks Are All You Need},
  author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio C'esar Teodoro Mendes and Allison Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and S. Shah and Harkirat Singh Behl and Xin Wang and S{\'e}bastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuan-Fang Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.11644},
  url={https://api.semanticscholar.org/CorpusID:259203998}
}

@article{Li2023TextbooksAA,
  title={Textbooks Are All You Need II: phi-1.5 technical report},
  author={Yuan-Fang Li and S{\'e}bastien Bubeck and Ronen Eldan and Allison Del Giorno and Suriya Gunasekar and Yin Tat Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.05463},
  url={https://api.semanticscholar.org/CorpusID:261696657}
}

@inproceedings{kulkarni-etal-2024-synthdst,
    title = "{S}ynth{DST}: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
    author = "Kulkarni, Atharva  and
      Tseng, Bo-Hsiang  and
      Moniz, Joel  and
      Piraviperumal, Dhivya  and
      Yu, Hong  and
      Bhargava, Shruti",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.120",
    pages = "1988--2001",
    abstract = "In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, {`}\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose , a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from results in $4-5\%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly 98{\%} of the performance compared to the few-shot setup using human-annotated training data.",
}

@inproceedings{xu2024wizardlm,
    title={Wizard{LM}: Empowering Large Pre-Trained Language Models to Follow Complex Instructions},
    author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Qingwei Lin and Daxin Jiang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=CfXh93NDgH}
}

@article{Sudalairaj2024LABLA,
  title={LAB: Large-Scale Alignment for ChatBots},
  author={Shivchander Sudalairaj and Abhishek Bhandwaldar and Aldo Pareja and Kai Xu and David D. Cox and Akash Srivastava},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.01081},
  url={https://api.semanticscholar.org/CorpusID:268230871}
}

@article{Gandhi2024BetterSD,
  title={Better Synthetic Data by Retrieving and Transforming Existing Datasets},
  author={Saumya Gandhi and Ritu Gala and Vijay Viswanathan and Tongshuang Wu and Graham Neubig},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.14361},
  url={https://api.semanticscholar.org/CorpusID:269292964}
}

@inproceedings{jiang-etal-2023-llm,
    title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
    author = "Jiang, Dongfu  and
      Ren, Xiang  and
      Lin, Bill Yuchen",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.792/",
    doi = "10.18653/v1/2023.acl-long.792",
    pages = "14165--14178",
    abstract = "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap."
}

@article{DatabricksBlog2023DollyV2,
  title={Free dolly: Introducing the world’s first truly open instruction-tuned llm},
  author={Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  journal={Company Blog of Databricks},
  year={2023}
}

@inproceedings{an2024automatic,
    title={Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models},
    author={Bang An and Sicheng Zhu and Ruiyi Zhang and Michael-Andrei Panaitescu-Liess and Yuancheng Xu and Furong Huang},
    booktitle={First Conference on Language Modeling},
    year={2024},
    url={https://openreview.net/forum?id=ljFgX6A8NL}
}

@inproceedings{wang-etal-2023-self-instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}

@inproceedings{honovich-etal-2023-unnatural,
    title = "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
    author = "Honovich, Or  and
      Scialom, Thomas  and
      Levy, Omer  and
      Schick, Timo",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.806",
    doi = "10.18653/v1/2023.acl-long.806",
    pages = "14409--14428",
    abstract = "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.",
}

@inproceedings{qian-etal-2022-controllable,
    title = "Controllable Natural Language Generation with Contrastive Prefixes",
    author = "Qian, Jing  and
      Dong, Li  and
      Shen, Yelong  and
      Wei, Furu  and
      Chen, Weizhu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.229",
    doi = "10.18653/v1/2022.findings-acl.229",
    pages = "2912--2924",
    abstract = "To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to steer natural language generation. Different from Li and Liang (2021), where each prefix is trained independently, we take the relationship among prefixes into consideration and train multiple prefixes simultaneously. We propose a novel supervised method and also an unsupervised method to train the prefixes for single-aspect control while the combination of these two methods can achieve multi-aspect control. Experimental results on both single-aspect and multi-aspect control show that our methods can guide generation towards the desired attributes while keeping high linguistic quality.",
}

@inproceedings{deng-raffel-2023-reward,
    title = "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    author = "Deng, Haikang  and
      Raffel, Colin",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.721",
    doi = "10.18653/v1/2023.emnlp-main.721",
    pages = "11781--11791",
    abstract = "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.",
}

@article{Jiang2024MixtralOE,
  title={Mixtral of Experts},
  author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and L'elio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Th{\'e}ophile Gervet and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.04088},
  url={https://api.semanticscholar.org/CorpusID:266844877}
}


@inproceedings{sun2023principledriven,
    title={Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
    author={Zhiqing Sun and Yikang Shen and Qinhong Zhou and Hongxin Zhang and Zhenfang Chen and David Daniel Cox and Yiming Yang and Chuang Gan},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=p40XRfBX96}
}

@inproceedings{kim-etal-2023-critic,
    title = "Critic-Guided Decoding for Controlled Text Generation",
    author = "Kim, Minbeom  and
      Lee, Hwanhee  and
      Yoo, Kang Min  and
      Park, Joonsuk  and
      Lee, Hwaran  and
      Jung, Kyomin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.281",
    doi = "10.18653/v1/2023.findings-acl.281",
    pages = "4598--4612",
    abstract = "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
}

@article{yu2023skill,
  title={Skill-Mix: A flexible and expandable family of evaluations for AI models},
  author={Yu, Dingli and Kaur, Simran and Gupta, Arushi and Brown-Cohen, Jonah and Goyal, Anirudh and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2310.17567},
  year={2023}
}

@article{hashemi2024llm,
  title={LLM-rubric: A multidimensional, calibrated approach to automated evaluation of natural language texts},
  author={Hashemi, Helia and Eisner, Jason and Rosset, Corby and Van Durme, Benjamin and Kedzie, Chris},
  journal={arXiv preprint arXiv:2501.00274},
  year={2024}
}