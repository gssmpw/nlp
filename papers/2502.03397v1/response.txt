\section{Related Work}
\paragraph{Scalable Oversight.} In order to minimize the amount of human oversight necessary to align LLMs, **Amodei et al., "Concrete Problems in AI Safety"** introduced Constitutional AI, a method relying on a list of predefined hand-crafted rules or \emph{constitutional principles} that aim to promote safe, reliable, and effective systems. Leveraging Reinforcement Learning from AI Feedback (RLAIF) **Christiano et al., "Deep Reinforcement Learning from Human Preferences"** , Constitutional AI uses these principles to create AI-generated self-critiques to enhance the models autonomously. During the self-critique process, however, only a single rule is randomly chosen to scrutinize the existing response. **Hendrycks et al., "Alignment of Language Models with Human Values"** improves on this approach by incorporating $16$ manually-devised guiding principles that entail broader domains and more specific criteria, such as candorness, step-by-step justifications, and multi-faceted answers. By broadening the range of topics, they allow the language model to decide which principles to adhere to given user queries. However, these approaches are resource-intensive and demand significant human labor, as they necessitate explicitly predefined guiding principles.

Prior work has recognized the importance of guiding LLM generations using principles situated in the particular context at hand, such as allowing users to formulate principles that steer the conversation **Hendrycks et al., "Measuring Adversarial Robustness"** . However, relying solely on human interactions to provide such context-situated guidance is challenging to scale. In **Amodei et al., "Concrete Problems in AI Safety"** , strong LLMs are used to discover principles for a weak LLM. In this red-teaming approach, both a stronger LLM and an initial \emph{bad} response are necessary, thus difficult to generalize. **Hendrycks et al., "Threats to Neural Deep Learning by Adversarial Attacks"** also introduces a method for learning a collection of constitutional principles given a cluster of training data. The training is conducted on various clusters of data, resulting in different sets of principles. At inference time, input queries are then directed to different principles based on their similarity to the centroids of the training clusters. Similarly,  OpenAI o1 models **Brown et al., "Language Models as Few-Shot Learners"** utilize a technique entitled Deliberative Alignment **Leike et al., "Scalable Value Transfer for Safe Credit Assignment in Deep Reinforcement Learning"** , which teaches LLMs to explicitly reason through safety specifications before producing an answer, but their approach mainly seeks to align and train a downstream model.

In contrast, our method customizes the principles for each individual input query, rather than basing them on a set of undesirable responses or a cluster of training data. This ensures that the principles are not generalized but specifically tailored to each unique input query, making our constitutional principles more precise. Our framework is also more versatile and not restricted to supervised fine-tuning. As demonstrated in \secref{section:task_reappraisal_and_biggen}, \ourframework{} can effortlessly extend to complex tasks that require significant human oversight.

\paragraph{Learning from Feedback.} To align AI systems with human preferences and values, researchers have explored using human feedback to direct the behaviors of language models **Hendrycks et al., "Measuring Adversarial Robustness"** . This includes efforts to incorporate human feedback in the pertaining **Brown et al., "Language Models as Few-Shot Learners"** and supervised fine-tuning phases **Rae et al., "Combining Capacities in Neural Language Models"** , integrate human feedback through reinforcement learning either directly **Leike et al., "Scalable Value Transfer for Safe Credit Assignment in Deep Reinforcement Learning"** or indirectly **Christiano et al., "Deep Reinforcement Learning from Human Preferences"** , as well as prompt engineering **Li et al., "Do Machine-Generated Feedbacks Improve Model Outputs?"** . However, human feedback is expensive and laborious to collect **Brown et al., "Language Models as Few-Shot Learners"** . Other works have therefore resorted to using machine-generated feedback for improving the model outputs **Rae et al., "Combining Capacities in Neural Language Models"** . Our approach differs from these methods by focusing on refining the principles tailored to each input, in addition to refining the outputs. These principles are then used to guide the generation of responses for each \emph{corresponding} input and serve as the criteria for critiquing and improving the responses.