\section{Related Work}
\paragraph{Scalable Oversight.} In order to minimize the amount of human oversight necessary to align LLMs, \citet{bai2022constitutional} introduced Constitutional AI, a method relying on a list of predefined hand-crafted rules or \emph{constitutional principles} that aim to promote safe, reliable, and effective systems. Leveraging Reinforcement Learning from AI Feedback (RLAIF) \cite{Lee2024RLAIFVR}, Constitutional AI uses these principles to create AI-generated self-critiques to enhance the models autonomously. During the self-critique process, however, only a single rule is randomly chosen to scrutinize the existing response. \citet{sun2023principledriven} improves on this approach by incorporating $16$ manually-devised guiding principles that entail broader domains and more specific criteria, such as candorness, step-by-step justifications, and multi-faceted answers. By broadening the range of topics, they allow the language model to decide which principles to adhere to given user queries. However, these approaches are resource-intensive and demand significant human labor, as they necessitate explicitly predefined guiding principles.

Prior work has recognized the importance of guiding LLM generations using principles situated in the particular context at hand, such as allowing users to formulate principles that steer the conversation \cite{ConstitutionMaker-2024}. However, relying solely on human interactions to provide such context-situated guidance is challenging to scale. In \citet{chen-etal-2024-iteralign}, strong LLMs are used to discover principles for a weak LLM. In this red-teaming approach, both a stronger LLM and an initial \emph{bad} response are necessary, thus difficult to generalize. \citet{petridis-etal-2024-constitutionalexperts} also introduces a method for learning a collection of constitutional principles given a cluster of training data. The training is conducted on various clusters of data, resulting in different sets of principles. At inference time, input queries are then directed to different principles based on their similarity to the centroids of the training clusters. Similarly,  OpenAI o1 models \cite{openai2024openaio1card} utilize a technique entitled Deliberative Alignment \citep{guan2025deliberativealignmentreasoningenables}, which teaches LLMs to explicitly reason through safety specifications before producing an answer, but their approach mainly seeks to align and train a downstream model.

In contrast, our method customizes the principles for each individual input query, rather than basing them on a set of undesirable responses or a cluster of training data. This ensures that the principles are not generalized but specifically tailored to each unique input query, making our constitutional principles more precise. Our framework is also more versatile and not restricted to supervised fine-tuning. As demonstrated in \secref{section:task_reappraisal_and_biggen}, \ourframework{} can effortlessly extend to complex tasks that require significant human oversight.

\paragraph{Learning from Feedback.} To align AI systems with human preferences and values, researchers have explored using human feedback to direct the behaviors of language models \cite{kirk-etal-2023-past}. This includes efforts to incorporate human feedback in the pertaining \cite{korbak2023pretraining} and supervised fine-tuning phases \cite{hancock-etal-2019-learning, liu2024chain}, integrate human feedback through reinforcement learning either directly \cite{stiennon2020learning, bai2022traininghelpfulharmlessassistant, bakker2022fine, ouyang2022training, liu2022second} or indirectly \cite{zhou2021narle, korbak2023pretraining}, as well as prompt engineering \cite{jin2022make, zhao-etal-2021-ethical, askell2021generallanguageassistantlaboratory}. However, human feedback is expensive and laborious to collect \cite{Lee2024RLAIFVR}. Other works have therefore resorted to using machine-generated feedback for improving the model outputs \cite{bai2022constitutional, yang-etal-2022-re3, Lee2024RLAIFVR, fu-etal-2024-gptscore, cui2024ultrafeedback, madaan2023selfrefine}. Our approach differs from these methods by focusing on refining the principles tailored to each input, in addition to refining the outputs. These principles are then used to guide the generation of responses for each \emph{corresponding} input and serve as the criteria for critiquing and improving the responses.