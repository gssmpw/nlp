\section{Related Work}
\subsection{Multilingual Large Language Models}

To address the demand for supporting global linguistic diversity, researchers have expanded into multilingual LLMs **Stoyanov, "Multilingual BERT"**. Advanced models like Qwen2 **Qiu et al., "Rethinking Expertise: Multitask Learning and Knowledge Transfer"** and LLaMA3 **Lample et al., "Large-scale Denoising Auto-encoding for Neural Machine Translation"** support multiple languages, showcasing robust multilingual capabilities. However, these models are trained from scratch, which incurs substantial computational costs and requires extensive datasets for relevant languages, often leading to inadequate support for low-resource languages. These meticulously trained models frequently face challenges in scaling to other languages, particularly those with lower representation in the training data.

Recently, LangBridge **Doss et al., "Multilingual BERT: A Multimodal Language Model for Text Classification"** and MindMerger **Li et al., "A Multimodal Fusion Model for Visual Question Answering"** feature an English-centric LLM backbone, a multilingual encoder that offers multilingual information, and an adapter that facilitates interoperability between the multilingual and English languages. However, these approaches are limited to representations from the topmost encoder layer, neglecting potentially valuable insights from other layers. Our \mname framework follows this line and explores to better leverage the multilingual information of different encoder layers to enhance the multilingual reasoning abilities of LLMs.


\subsection{Aligning Pretrained Representations} 
The integration of encoders with large language models (LLMs) has been widely studied in the cross-modal domain **Su et al., "Multimodal BERT: A Multimodal Language Model for Image Captioning"**. Many approaches utilize vision-to-language adapter modules to align visual and textual modalities, mapping the output of vision encoders to the soft prompt inputs of LLMs. Other works employ cross-attention mechanisms to enable more direct interaction between image and text representations **Vaswani et al., "Attention Is All You Need"**. 
Drawing inspiration from these cross-modal strategies, our method enhances multilingual reasoning by integrating a multilingual encoder with an LLM. To bridge the gap between these components, we introduce an aligner that enables efficient interaction via cross-attention.