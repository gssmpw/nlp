@misc{evlm,
      title={EVLM: An Efficient Vision-Language Model for Visual Understanding}, 
      author={Kaibing Chen and Dong Shen and Hanwen Zhong and Huasong Zhong and Kui Xia and Di Xu and Wei Yuan and Yifei Hu and Bin Wen and Tianke Zhang and Changyi Liu and Dewen Fan and Huihui Xiao and Jiahong Wu and Fan Yang and Size Li and Di Zhang},
      year={2024},
      eprint={2407.14177},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.14177}, 
}

@inproceedings{langbridge,
    title = "{L}ang{B}ridge: Multilingual Reasoning Without Multilingual Supervision",
    author = "Yoon, Dongkeun  and
      Jang, Joel  and
      others",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.405",
    pages = "7502--7522",
    abstract = "We introduce LangBridge, a $\textit{zero-shot}$ approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., MetaMath). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.",
}

@article{llama3,

title={Llama 3 Model Card}

@misc{mindmerger,
      title={MindMerger: Efficient Boosting LLM Reasoning in non-English Languages}, 
      author={Zixian Huang and Wenhao Zhu and Gong Cheng and Lei Li and Fei Yuan},
      year={2024},
      eprint={2405.17386},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17386}, 
}

@inproceedings{multi-model-1,
    title = "An Empirical Study on Parameter-Efficient Fine-Tuning for {M}ulti{M}odal Large Language Models",
    author = "Zhou, Xiongtao  and
      He, Jie  and
      Ke, Yuhua  and
      Zhu, Guangyao  and
      Gutierrez Basulto, Victor  and
      Pan, Jeff",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.598",
    pages = "10057--10084",
    abstract = "Multimodal Large Language Models (MLLMs) fine-tuned with multimodal instruction-following data have demonstrated formidable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging due to the rapid growth of the overall model{'}s parameters. To address this issue, we study Parameter-Efficient Fine-Tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing performance in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies that employ four widely used PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of PEFT module, fine-tuning data scale, model stability based on PEFT method, MLLM{'}s generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories, unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method in various aspects. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs.",
}

@article{qin2025survey,
  title={A survey of multilingual large language models},
  author={Qin, Libo and Chen, Qiguang and Zhou, Yuhang and Chen, Zhi and Li, Yinghui and Liao, Lizi and Li, Min and Che, Wanxiang and Philip, S Yu},
  journal={Patterns},
  volume={6},
  number={1},
  year={2025},
  publisher={Elsevier}
}

@misc{qwen2,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and others},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

