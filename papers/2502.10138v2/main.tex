\documentclass[vlined,cleveref,final,12pt]{colt2025} %
\usepackage{lipsum}

\AtBeginEnvironment{appendices}{\crefalias{section}{appendix}}

\title[Provably Efficient RL under Episode-Wise Safety in Linear CMDPs]
{Provably Efficient RL under Episode-Wise Safety in\\ Constrained MDPs with Linear Function Approximation}

\usepackage{color, colortbl}
\definecolor{LightGray}{gray}{0.9}

\usepackage{cancel}
\usepackage{multirow}
\usepackage{times}
\usepackage{threeparttable}
\usepackage{rl-theory}
\input{commands}

\newcount\isdebug
\isdebug=0  %

\ifnum\isdebug=1
\newcommand{\arnob}[1]{\textcolor{cyan}{\textbf{[Arnob: }#1\textbf{]}}}
\newcommand{\toshinori}[1]{\textcolor{orange}{\textbf{[Toshinori: }#1\textbf{]}}}
\newcommand{\tadashi}[1]{\textcolor{purple}{\textbf{[Tadashi: }#1\textbf{]}}}
\else
\newcommand{\arnob}[1]{}
\newcommand{\toshinori}[1]{}
\newcommand{\tadashi}[1]{}
\newcommand{\yourname}[1]{}
\fi





\coltauthor{%
 \Name{Toshinori Kitamura} \Email{toshinori-k@weblab.t.u-tokyo.ac.jp}\\
 \addr The University of Tokyo
 \AND
 \Name{Arnob Ghosh} \Email{arnob.ghosh@njit.edu}\\
 \addr New Jersey Institute of Technology
 \AND
 \Name{Tadashi Kozuno} \Email{tadashi.kozuno@sinicx.com}\\
 \addr OMRON SINIC X, Osaka University
 \AND
 \Name{Wataru Kumagai} \Email{wataru.kumagai@sinicx.com}\\
 \Name{Kazumi Kasaura} \Email{kazumi.kasaura@sinicx.com}\\
 \addr OMRON SINIC X
 \AND
 \Name{Kenta Hoshino} \Email{hoshino@i.kyoto-u.ac.jp}\\
 \Name{Yohei Hosoe} \Email{hosoe@kuee.kyoto-u.ac.jp}\\
 \addr Kyoto University 
 \AND
 \Name{Yutaka Matsuo} \Email{matsuo@weblab.t.u-tokyo.ac.jp}\\
 \addr The University of Tokyo\\
}

\begin{document}

\maketitle

\vspace{-1cm}
\begin{abstract}%
\looseness=-1
We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\widetilde{\mathcal{O}}(\sqrt{K})$ regret with an {\em episode-wise} zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.
\end{abstract}

\begin{keywords}%
Constrained Markov decision process, linear function approximation%
\end{keywords}

\input{section/intro}
\input{section/preliminary}
\input{section/bandit}
\input{section/MDP}
\input{section/conclusion}

\acks{This work is supported by JST Moonshot R\&D Program Grant Number JPMJMS2236.
}

\bibliography{mybib}

\newpage
\appendix

\tableofcontents
\newpage

\crefalias{section}{appendix} %

\input{section/relatedwork}
\input{appendix/auxiliary_lemma}
\input{appendix/bandit-analysis}
\input{appendix/CMDP-analysis}

\end{document}
