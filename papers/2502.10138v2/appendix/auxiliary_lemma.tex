\section{Useful Lemmas}\label{appendix:useful-lemma}

\begin{definition}[Distance metrics]
Let $\dist_{\infty}$ be the distance metric such that, for two functions $Q, Q': \SA \to \R$, $\dist_\infty(Q, Q') = \sup_{(s, a) \in \SA} \abs*{Q(s, a) - Q'(s, a)}$. 
Similarly, for two functions $V, V': \S \to \R$, $\dist_\infty(V, V') = \sup_{s \in \S} \abs*{V(s) - V'(s)}$. 
Finally, $\dist_1$ denotes the distance metric such that, for two functions $\pi, \pi': \S \to \sP(\A)$, $\dist_1(\pi, \pi') = \sup_{s \in \S} \norm*{\pi\paren{\cdot \given s} - \pi'\paren{\cdot \given s}}_1$. 
\end{definition}

\begin{lemma}[\textbf{Lemma 5.2} in \citet{vershynin2010introduction}]\label{lemma:ball-covering-bound}
The $\varepsilon$-covering number of the ball \\    
$\Theta=\left\{\theta \in \R^d:\|\theta\|_2 \leq R \right\}$ with the distance metric $\norm*{\cdot}_2$ is upper bounded by $(1+2 R / \varepsilon)^d$.
\end{lemma}


\begin{lemma}[Danskin's Theorem \citep{bertsekas1997nonlinear}]\label{lemma:danskin's theorem}
    \looseness=-1
    Let $f: \R^n \times \cZ \to \R$ be a continuous function where $\cZ \in \R^m$ is a compact set and $g(x) \df \max_{z\in \cZ} f(x, z)$.\\
    Let $\cZ_0(x) \df \brace*{\bar{z} \given f(x, \bar{z})=\max_{z\in \cZ}f(x, z)}$ be the maximizing points of $f(x, z)$.
    Assume that $f(x, z)$ is convex in $x$ for every $z\in \cZ$. 
    Then, $g(x)$ is convex.
    Furthermore, if $Z_0(x)$ consists of a single element $\bar{z}$, i.e., $\cZ_0(x)=\brace{\bar{z}}$, it holds that
    $\frac{\partial g(x)}{\partial x}=\frac{\partial f(x, \bar{z})}{\partial x}$.
\end{lemma}

\begin{lemma}[\textbf{Lemma D.4} in \citet{rosenberg2020near}]\label{lemma:exp-to-concrete}
Let $\left(X^{(k)}\right)_{k=1}^{\infty}$ be a sequence of random variables with expectation adapted to the filtration $\left(\mathcal{F}^{(k)}\right)_{k=0}^{\infty}$. 
Suppose that $0 \leq X^{(k)} \leq B$ almost surely. 
Then, with probability at least $1-\delta$, the following holds for all $k \geq 1$ simultaneously:    
$$
\sum_{i=1}^k \mathbb{E}\left[X^{(i)} \mid \mathcal{F}^{(i-1)}\right] \leq 2 \sum_{i=1}^k X^{(i)}+4 B \ln \frac{2 k}{\delta}
$$
\end{lemma}

\begin{lemma}[\textbf{Lemma 11} in \citet{abbasi2011improved}]\label{lemma:elliptical potential}
Let \(\left\{\bx^{(k)} \right\}_{k=1}^K\) be a sequence in $\R^d$.
Let \(\bLambda^{(k)}=\rho \bI+\sum_{i=1}^{k-1} \bx^{(i)} \paren*{\bx^{(i)}}^{\top}.
\)
If \(\norm*{\bx^{(k)}}_2 \leq B\) for all $k$, 
$$
\sum_{k=1}^K \min \left\{1,\left\|\bx^{(k)}\right\|_{\paren*{\bLambda^{(k)}}^{-1}}^2\right\} \leq 2 d \ln \left(\frac{\rho d+K B^2}{\rho d}\right)\;.
$$
Additionally, if \(\norm*{\bx^{(k)}}_2 \leq 1\) for all $k$ and $\rho \geq 1$\footnote{The second argument follows since $\left\|\bx\right\|_{\bLambda^{-1}}^2 \leq \sigma_{\max }\left(\bLambda^{-1}\right)\left\|\bx\right\|^2 \leq \rho^{-1} \leq 1$, where $\sigma_{\max}(\bLambda^{-1})$ denotes the maximum eigen value of $\bLambda^{-1}$.}, we have
$$
\sum_{k=1}^K \left\|\bx^{(k)}\right\|_{\paren*{\bLambda^{(k)}}^{-1}}^2 \leq 2 d \ln \left(\frac{\rho d+K}{\rho d}\right)\;.
$$
\end{lemma}

\begin{lemma}[\textbf{Theorem 2} in \citet{abbasi2011improved}]\label{lemma:confidence-ellipsoid}
Let $\left\{\cF^{(k)}\right\}_{k=0}^{\infty}$ be a filtration. 
Let $\left\{\varepsilon^{(k)}\right\}_{k=1}^{\infty}$ be a real-valued stochastic process such that $\varepsilon^{(k)}$ is $\cF^{(k)}$-measurable and $\varepsilon^{(k)}$ is conditionally $R$-sub-Gaussian for some $R \geq 0$. 
Let $\left\{\bphi^{(k)}\right\}_{k=1}^{\infty}$ be an $\R^d$-valued stochastic process such that $\bphi^{(k)}$ is $\cF^{(k-1)}$ measurable and $\|\bphi^{(k)}\|_2 \leq L$ for all $k$. 
For any $k \geq 0$, define $Y_k \df \btheta^\top \bphi^{(k)}+ \varepsilon_t$ for some $\btheta \in \R^d$ such that $\norm*{\btheta}_2 \leq B$,
$\bLambda^{(k)}\df\rho \bI+\sum_{i=1}^k \bphi^{(i)} \paren*{\bphi^{(i)}}^\top$,
and  \(\widehat{\btheta}^{(k)} \df \paren*{\bLambda^{(k)}}^{-1}\sum_{i =1}^k \bphi^{(i)} Y^{(i)} \).
Then for any $\delta>0$, with probability at least $1-\delta$, for all $k \geq 0$, we have
\begin{align*}
\norm*{\widehat{\btheta}^{(k)}-\btheta}_{\bLambda^{(k)}} \leq 
\rho^{1 / 2} B + 
R \sqrt{d \ln \left(\frac{1+k L^2 / \rho}{\delta}\right)}\;.
\end{align*}
\end{lemma}


\begin{lemma}[\textbf{Lemma D.4} in \citet{jin2020provably}]\label{lemma:fixed-V-bound}
Let $\left\{s^{(k)}\right\}_{k=1}^{\infty}$ be a stochastic process on state space $\mathcal{S}$ with corresponding filtration $\left\{\cF^{(k)}\right\}_{k=0}^{\infty}$.
Let $\left\{\bphi^{(k)}\right\}_{k=0}^{\infty}$ be an $\mathbb{R}^d$-valued stochastic process where $\bphi^{(k)}$ is $\cF^{(k-1)}$-measurable and $\left\|\bphi^{(k)}\right\| \leq 1$.
Let $\bLambda^{(k)}=\rho \bI+\sum_{k=1}^k \bphi^{(k)} \paren*{\bphi^{(k)}}^{\top}$ and let $\cV$ be a class of real-valued function over the state space $\S$ such that $\sup_s|V(s)| \leq B$ for a $B > 0$.
Let $\mathcal{N}^\cV_{\varepsilon}$ be the $\varepsilon$-cover of $\cV$ with respect to the distance $\dist_\infty$. 
Then for any $\delta>0$, with probability at least $1-\delta$, for all $K \geq 0$, and any $V \in \cV$, we have:    
$$
\left\|\sum_{k=1}^K \bphi^{(k)}\paren*{V\left(s^{(k)}\right)-\mathbb{E}\left[V\left(s^{(k)}\right) \mid \mathcal{F}^{(k-1)}\right]}\right\|_{\paren*{\bLambda^{(k)}}^{-1}}^2 \leq 4 B^2\paren*{\frac{d}{2} \ln \left(\frac{K+\rho}{\rho}\right)+\ln \frac{|\cN^\cV_{\varepsilon}|}{\delta}}+\frac{8 K^2 \varepsilon^2}{\rho}\;.
$$
\end{lemma}




\begin{lemma}[\textbf{Lemma A.1} in \citet{shalev2014understanding}]\label{lemma:alogx-inequality}
Let $a>0$. Then, $x \geq 2 a \ln (a)$ yields $x \geq a \ln (x)$. 
It follows that a necessary condition for the inequality $x\leq a \ln (x)$ to hold is that $x\leq 2 a \ln (a)$.    
\end{lemma}

\begin{lemma}\label{lemma:sum of sqrt}
For any positive real numbers $x_1, x_2, \dots, x_n$, 
$\sum_{i=1}^n \sqrt{x_i} \leq \sqrt{n}\sqrt{\sum^n_{i=1}x_i}$.
\end{lemma}
\begin{proof}
Due to the Cauchy-Schwarz inequality, we have
$
\left(\frac{\sum_{i=1}^n \sqrt{x_i}}{n}\right)^2 \leq \frac{\sum_{i=1}^n x_i}{n}
$.
Taking the square root of the inequality proves the claim.
\end{proof}

\begin{lemma}[\textbf{Lemma 1} in \citet{shani2020optimistic}]\label{lemma:extended value difference}
Let $\widetilde{\pi}, \pi$ be two policies, $P$ be a transition kernel, and $g$ be a reward function.
Let $\tvf{\pi}_h: \S \to \R$ be a function such that 
$$
\tvf{\pi}_h(s) = \sum_{a\in \A}\widetilde{\pi}_h\paren{a\given s} \widetilde{Q}_h(s, a)\;,$$ 
for all $h \in \bbrack{1, H}$ with some function $\widetilde{Q}_h: \HSA \to \R$.
Then, for any $(h, s) \in \bbrack{1, H}\times \S$
\begin{align*}
\tvf{\widetilde{\pi}}_h(s)-\vf{\pi, g}_{P, h}(s) 
= 
\vf{\pi, g^1}_{P, h}(s) 
+ \vf{\pi, g^2}_{P, h}(s)\;,
\end{align*}
where $g^1$ and $g^2$ are reward functions such that
\begin{align*}
g^1_h(s, a) = \sum_{a \in \A} \paren*{\tpi_h\paren*{a\mid s}-\pi_h\paren*{a \mid s}}\widetilde{Q}_h(s, a)\;
\text{ and }\;
g^2_h(s, a) = 
\widetilde{Q}_h\left(s, a \right)-g_h\left(s, a\right)-\paren*{P_h\tvf{\widetilde{\pi}}_{h+1}}(s, a)\;.
\end{align*}
\end{lemma}

\begin{lemma}[Regularized value difference lemma]\label{lemma:regularized value difference}
Let $\kappa \geq 0$ be a non-negative value, $\pi, \pi'$ be two policies, $P$ be a transition kernel, and $g$ be a reward function.
Let $\tvf{\tpi}_h[\kappa]: \S \to \R$ be a function such that 
$$
\tvf{\tpi}_h[\kappa](s) = \sum_{a \in \A}\tpi_h\paren{a\given s} \paren*{\widetilde{Q}_h(s, a) - \kappa \ln \tpi_h\paren{a\given s}}\;,$$ 
for all $h \in \bbrack{1, H}$ with some function $\widetilde{Q}_h: \HSA \to \R$.
Then, for any $(h, s) \in \bbrack{1, H}\times \S$
\begin{align*}
\tvf{\tpi}_h[\kappa](s)-\vf{\pi, g}_{P, h}[\kappa](s) 
= 
\vf{\pi, f^1}_{P, h}(s) + \vf{\pi, f^2}_{P, h}(s)\;,
\end{align*}
where $f^1$ and $f^2$ are reward functions such that
\begin{align*}
f^1_h(s, a) 
&= \sum_{a \in \A}\tpi_h\paren*{a\mid s}\paren*{\widetilde{Q}_h(s, a) - \kappa \ln \tpi_h\paren{a\given s}}
- \pi_h\paren*{a \mid s} \paren*{\widetilde{Q}_h(s, a) - \kappa \ln \pi_h\paren{a\given s}}\\
\text{ and }\;
f^2_h(s, a) &= 
\widetilde{Q}_h(s, a) 
-g_h\left(s, a\right)
-\paren*{P_h\tvf{\widetilde{\pi}}_{h+1}[\kappa]}(s, a)\;.
\end{align*}
\end{lemma}
\begin{proof}
Since 
$$
\tvf{\tpi}_h[\kappa](s) = \sum_{a \in \A}\tpi_h\paren{a\given s} 
\paren*{\widetilde{Q}_h(s, a) - \kappa \ln \tpi_h\paren{a\given s}}
\; \text{ and }\;
\vf{\pi, g}_{P, h}[\kappa](s) = 
\vf{\pi, g - \kappa \ln \pi}_{P, h}(s) \;,
$$ 
using \Cref{lemma:extended value difference}, we have
\begin{align*}
\tvf{\tpi}_1[\kappa](s_1)  -\vf{\pi, g}_{P, 1}[\kappa](s_1) 
= 
\vf{\pi, g^1}_{P, 1}(s_1) + \vf{\pi, g^2}_{P, 1}(s_1)\;,
\end{align*}
where $g^1$ and $g^2$ are reward functions such that
\begin{align*}
g^1_h(s, a) 
&= \sum_{a \in \A}\paren*{\tpi_h\paren*{a\mid s}-\pi_h\paren*{a \mid s}} \paren*{\widetilde{Q}_h(s, a) - \kappa \ln \tpi_h\paren{a\given s}}\\
&= \sum_{a \in \A}\tpi_h\paren*{a\mid s}\paren*{\widetilde{Q}_h(s, a) - \kappa \ln \tpi_h\paren{a\given s}}
- 
\pi_h\paren*{a \mid s} 
\paren*{\widetilde{Q}_h(s, a) - \kappa \ln \pi_h\paren{a\given s}}
\\
&\underbrace{+ \sum_{a \in \A}\pi_h\paren{a \given s}
\paren*{\kappa \ln \tpi_h\paren{a\given s} - \kappa \ln \pi_h\paren{a\given s}}}_{\text{(a)}}\\
\text{ and }\;&g^2_h(s, a) = 
\widetilde{Q}_h(s, a) 
-g_h\left(s, a\right)
-\paren*{P_h\tvf{\widetilde{\pi}}_{h+1}[\kappa]}(s, a)
\underbrace{- \kappa \ln \tpi_h\paren{a\given s}
+ \kappa \ln \pi_h\paren{a \given s}}_{(b)}
\;.
\end{align*}
The claim holds since the terms (a) and (b) are canceled out in
\(
\vf{\pi, g^1}_{P, h}(s) + \vf{\pi, g^2}_{P, h}(s)
\).
\end{proof}

\begin{lemma}\label{lemma:softmax-policy-bound}
\looseness=-1    
Let $Q, \widetilde{Q} : \A \to \R$ be two functions.
Let $\kappa > 0$ be a positive constant.
Define two softmax distributions $\pi, \widetilde{\pi} \in \sP(\A)$ such that
\(
\pi = \softmax\paren*{\frac{Q}{\kappa}} 
\) and 
\(
\widetilde{\pi} = \softmax\paren*{\frac{\widetilde{Q}}{\kappa}}
\).
Then, 
\(
\norm*{\pi - \widetilde{\pi}}_1 \leq 
\frac{8}{\kappa}
\norm*{Q -\widetilde{Q}}_\infty
\).
\end{lemma}
\begin{proof}
It holds that
\begin{align*}
\frac{1}{2}\norm*{\pi - \widetilde{\pi}}_{1}
&\numeq{\leq}{a} 2\sum_{a \in \A} \pi\paren*{a} \abs*{\ln \pi\paren*{a} - \ln \widetilde{\pi}\paren*{a}}
\leq 2\max_a \abs*{\ln \pi\paren*{a} - \ln \widetilde{\pi}\paren*{a}}\\
&= 2\max_a \abs*{\frac{1}{\kappa} Q\paren*{a} - \frac{1}{\kappa}\widetilde{Q}\paren*{a} - \ln \sum_{a}\exp\paren*{\frac{1}{\kappa}Q(a)} + \ln \sum_{a}\exp\paren*{\frac{1}{\kappa}\widetilde{Q}(a)}}\\
&\leq 
2\max_a \abs*{\frac{1}{\kappa} Q\paren*{a} - \frac{1}{\kappa}\widetilde{Q}\paren*{a}}+ 2\abs*{\ln \sum_{a}\exp\paren*{\frac{1}{\kappa}Q(a)} - \ln \sum_{a}\exp\paren*{\frac{1}{\kappa}\widetilde{Q}(a)}}\\
&\numeq{\leq}{b} 4\max_a \abs*{\frac{1}{\kappa} Q\paren*{a} - \frac{1}{\kappa}\widetilde{Q}\paren*{a}}\;,
\end{align*}
where (a) uses \textbf{Theorem 17} in \citet{sason2016f} and (b) uses the fact that $\ln \sum_i \exp(\bx_i) - \ln \sum_i \exp(\by_i) \leq \max_i \paren*{\bx_i - \by_i}$ (see, e.g., \textbf{Theorem 1} in \citet{dutta2024log}).
This concludes the proof.
\end{proof}
