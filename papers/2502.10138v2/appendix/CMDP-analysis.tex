\section{Regret Analysis (Linear CMDP)}\label{appendix:MDP-regret-analysis}
\subsection{Definitions and Useful Lemmas}

\begin{definition}
For a set of positive values $\brace*{a_n}_{n=1}^N$, we write $x=\polylog\left(a_1, \dots, a_N\right)$ if there exists an absolute constants $\brace*{b_n}_{n=0}^N >0$ and $\brace*{c_n}_{n=1}^N >0$ such that 
$
x \leq b_0 + b_1 \paren*{\ln a_1}^{c_1} + \dots + b_N \paren*{\ln a_N}^{c_N}
$.
\end{definition}

\begin{definition}[$\varepsilon$-cover]
Let $\Theta=\left\{\btheta \in \R^d:\|\btheta\|_2 \leq R \right\}$ be a ball with radius $R$.
Fix an $\varepsilon$.
An $\varepsilon$-net $\cN_\varepsilon \subset \Theta$ is a finite set such that for any $\btheta \in \Theta$, there exists a $\btheta' \in \cN_\varepsilon$ such that $\dist\paren*{\btheta, \btheta'} \leq \varepsilon$ for some distance metric $\dist(\cdot, \cdot)$.
The smallest $\varepsilon$-net is called $\varepsilon$-cover, and the size of $\varepsilon$-net is called $\varepsilon$-covering number.
\end{definition}



\begin{definition}[$\bmu$-estimator]\label{def:mu-estimation}
Let \(\be(s) \in \R^\S\) denote a one-hot vector such that only the element at \(s \in \S\) is \(1\) and otherwise \(0\).
In \Cref{algo:zero-vio-linear MDP}, for all $h$ and $k$, define $\bmu^{(k)}_h \in \R^{S\times d}$ and $\bepsilon_h^{(k)} \in \R^\S$ such that
\begin{align}
\bmu^{(k)}_h \df 
\sum^{k-1}_{i=1} \be\paren*{s^{(i)}_{h+1}} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1}
\;\text{ and }\; \bepsilon_h^{(k)} \df \be\paren*{s^{(k)}_{h+1}} - P\paren*{\cdot \given s^{(k)}_h, a^{(k)}_h} \;.
\end{align}
We remark that 
$\paren*{\hP^{(k)}_h V}(s, a) = \bphi(s, a)^\top \paren*{\bmu_h^{(k)}}^\top V$ for any $V \in \R^\S$.
\end{definition}

\begin{lemma}\label{lemma:mu-diff}
For all $k$ and $h$, it holds that:
\begin{align*}
\bmu_h^{(k)}-\bmu_h=-\rho \bmu_h\left(\bLambda_h^{(k)}\right)^{-1}+\sum_{i=1}^{k-1} \bepsilon_h^{(i)} \bphi\left(s_h^{(i)}, a_h^{(i)}\right)^{\top}\left(\bLambda_h^{(k)}\right)^{-1}
\end{align*}
\end{lemma}
\begin{proof}
Due to the definition of $\bmu^{(k)}_h$, we have
\begin{align*}
\bmu_h^{(k)} & 
=\sum^{k-1}_{i=1} \be\paren*{s^{(i)}_{h+1}} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1}
=\sum^{k-1}_{i=1} \paren*{P\paren*{\cdot \given s^{(k)}_h, a^{(k)}_h} +  \bepsilon_h^{(k)}} 
\bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1}\\
&=\sum^{k-1}_{i=1} \paren*{\bmu_h \bphi\paren*{s^{(k)}_h, a^{(k)}_h} +  \bepsilon_h^{(k)}} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1}\\
&=\sum^{k-1}_{i=1} \bmu_h \bphi\paren*{s^{(k)}_h, a^{(k)}_h} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1} + \sum^{k-1}_{i=1} \bepsilon_h^{(k)} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1} \\
&=\bmu_h \paren*{\bLambda^{(k)}_h - \rho \bI} \paren*{\bLambda^{(k)}_h}^{-1} + \sum^{k-1}_{i=1} \bepsilon_h^{(k)} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1} \\
&=\bmu_h - \rho \bmu_h \paren*{\bLambda^{(k)}_h}^{-1} + \sum^{k-1}_{i=1} \bepsilon_h^{(k)} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1} \;.
\end{align*}
\end{proof}


\begin{lemma}\label{lemma:hP-P-V bound}
Let $\cV$ be a class of real-valued function over the state space $\S$ such that $\sup_s|V(s)| \leq B$ for a $B > 0$.
Let $\mathcal{N}_{\varepsilon}$ be the $\varepsilon$-cover of $\cV$ with respect to the distance $\dist_\infty$. 
In \Cref{algo:zero-vio-linear MDP}, for all $k, h, s, a$, for any $V \in \cV$, with probability at least $1-\delta$, we have
\begin{align*}
&\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}V}(s, a)} \\
\leq 
&\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }
\paren*{
\sqrt{d\rho}B
+ 
2 B\sqrt{\frac{d}{2} \ln \paren*{\frac{k + \rho}{\rho}}}
+2B \sqrt{\ln \frac{|\cN_{\varepsilon}|}{\delta}}
+\frac{4 k \varepsilon}{\sqrt{\rho}}
}\;.
\end{align*}
\end{lemma}
\begin{proof}
Using \Cref{lemma:fixed-V-bound} and due to the definition of $\bLambda^{(k)}$ in \Cref{algo:zero-vio-linear MDP}, with probability at least \(1-\delta\), for all \(k, h\), we have
\begin{align*}
\left\|\sum_{i=1}^{k-1} \bphi\left(s_h^{(k)}, a_h^{(k)}\right)\left(V^{\top} \bepsilon_h^{(i)}\right)\right\|_{\left(\bLambda^{(k)}\right)^{-1}} 
&\leq 
\sqrt{4 B^2\paren*{\frac{d}{2} \ln \paren*{\frac{k + \rho}{\rho}}+\ln \frac{|\cN_{\varepsilon}|}{\delta}}+\frac{8 k^2 \varepsilon^2}{\rho}}\\
&\leq 
2 B\sqrt{\frac{d}{2} \ln \paren*{\frac{k + \rho}{\rho}}}
+2B \sqrt{\ln \frac{|\cN_{\varepsilon}|}{\delta}}
+\frac{4 k \varepsilon}{\sqrt{\rho}}\;,
\end{align*}
where the second inequality uses $\sqrt{a + b} \leq \sqrt{a} + \sqrt{b}$.
By inserting this to \Cref{def:mu-estimation}, we have
\begin{align*}
&\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}V}(s, a)}\\
=&\abs*{\bphi(s, a)^\top \paren*{\bmu_h^{(k)} - \bmu_h}^\top V}\\
=&\abs*{\bphi(s, a)^\top 
\paren*{
-\rho \bmu_h\left(\bLambda_h^{(k)}\right)^{-1}+\sum_{i=1}^{k-1} \bepsilon_h^{(i)} \bphi\left(s_h^{(i)}, a_h^{(i)}\right)^{\top}\left(\bLambda_h^{(k)}\right)^{-1} }^\top  V}\\
\leq &
\rho \abs*{
\bphi(s, a)^\top \paren*{\bLambda_h^{(k)}}^{-1} \paren*{\bmu_h}^\top V}
+\abs*{
\bphi(s, a)^\top \paren*{\bLambda_h^{(k)}}^{-1} 
\sum_{i=1}^{k-1} 
\bphi\left(s_h^{(i)}, a_h^{(i)}\right)
\paren*{V^\top \bepsilon_h^{(i)}}}\\
\leq &
\rho \norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1}}
\underbrace{\norm*{\paren*{\bmu_h}^\top V}_{\paren*{\bLambda_h^{(k)}}^{-1}}}_{\leq B \sqrt{d / \rho}\; \text{ by \Cref{assumption:linear mdp}}}
+
\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }
\norm*{
\sum_{i=1}^{k-1} 
\bphi\left(s_h^{(i)}, a_h^{(i)}\right)
\paren*{\bepsilon_h^{(i)}}^{\top}V}_{\paren*{\bLambda_h^{(k)}}^{-1} }\\
\leq &
\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }
\paren*{
\sqrt{d\rho}B
+ 
2 B\sqrt{\frac{d}{2} \ln \paren*{\frac{k + \rho}{\rho}}}
+2B \sqrt{\ln \frac{|\cN_{\varepsilon}|}{\delta}}
+\frac{4 k \varepsilon}{\sqrt{\rho}}
}\;.
\end{align*}
\end{proof}


\subsection{Function Classes and Covering Argument }

\begin{definition}[$Q$ function class]\label{def:abs-Q-function}
For any $h$ and for a pair of $(\bw, \bLambda)$, where $\bw \in \mathbb{R}^d$ and $\bLambda \in \R^{d\times d}$, define 
$\qf{(\bw, \bLambda), r}_{h} : \SA \to \R$, 
$\qf{(\bw, \bLambda), u}_{h}: \SA \to \R$,
and $\qf{(\bw, \bLambda), \dagger}_{h} : \SA \to \R$ such that
\begin{align*}
\qf{(\bw, \bLambda), r}_{h}(s, a) &=
r_h(s, a) + \clip\brace*{\co \norm*{\bphi(s, a)}_{\bLambda^{-1}} + \bw^{\top} \bphi(s, a),\; 0,\; H_\kappa-h_\kappa}\\
\qf{(\bw, \bLambda), u}_{h} (s, a) &= u_h(s, a) + \clip\brace*{-\cp \norm*{\bphi(s, a)}_{\bLambda^{-1}} + \bw^{\top} \bphi(s, a),\; 0,\; H-h}\\
\qf{(\bw, \bLambda), \dagger}_{h} (s, a) &= 
B_\dagger \norm*{\bphi(s, a)}_{\bLambda^{-1}} + \clip\brace*{C_\dagger \norm*{\bphi(s, a)}_{\bLambda^{-1}} + \bw^{\top} \bphi(s, a),\; 0,\; B_\dagger(H-h)}\;,
\end{align*}
where $\kappa, \co, \cp, B_\dagger, C_\dagger \geq 0$.
We denoted \(h_\kappa \df h(1 + \kappa \ln A)\) for $h \in \bbrack{1, H}$.
Let $\cQr_h$, $\cQu_h$, $\cQc_h$ denote function classes such that
\begin{align*}
\cQr_h&\df \brace*{\qf{(\bw, \bLambda), r}_{h}\given \|\bw\|_2 \leq KH_\kappa,\; \sigma_{\min }(\bLambda) \geq 1}\;,\\
\cQu_h&\df \brace*{\qf{(\bw, \bLambda), u}_h\given \|\bw\|_2 \leq KH,\; \sigma_{\min }(\bLambda) \geq 1}\;,\\
\text{ and }\; \cQc_h&\df \brace*{\qf{(\bw, \bLambda), \dagger}_h\given \|\bw\|_2 \leq KHB_\dagger ,\; \sigma_{\min }(\bLambda) \geq 1}\;.
\end{align*}
We let $\cN^{\cQr_h}_{\varepsilon}$, 
$\cN^{\cQu_h}_{\varepsilon}$, 
and $\cN^{\cQc_h}_{\varepsilon}$,
be the $\varepsilon$-covers of $\cQr_h$, $\cQu_h$, and $\cQc_h$ with the distance metric $\dist_\infty$.
\end{definition}

\begin{lemma}[$Q$ covers]\label{lemma:Q eps covers}
\looseness=-1
When \Cref{algo:zero-vio-linear MDP} is run with $\rho =1$, it hold that:
\begin{itemize}
\item[$(\mathrm{i})$] 
For all $k, h$ and for any $\pi \in \Pi$, 
\(\oqf{\pi, r}_{(k), h}[\kappa] \in \cQr_h\),
\(\pqf{\pi, u}_{(k), h} \in \cQu_h \),
and \(\oqf{\pi, \dagger}_{(k), h} \in \cQc_h \) 
\item[$(\mathrm{ii})$]
\(\ln |\cN^{\cQr_h}_{\varepsilon}| \leq d \ln \paren*{1+\frac{4 KH_\kappa }{\varepsilon}}+d^2 \ln \paren*{1+\frac{8 \sqrt{d} \co^2}{\varepsilon^2}} 
= \cO\paren*{d^2}\polylog\paren*{d,K,H_\kappa,\co,\varepsilon^{-1}}
\),\\
\(\ln |\cN^{\cQu_h}_{\varepsilon}| 
\leq d \ln \paren*{1+\frac{4 KH}{\varepsilon}}+d^2 \ln \paren*{1+\frac{8 \sqrt{d} \cp^2}{\varepsilon^2}}= \cO\paren*{d^2}\polylog\paren*{d,K,H,\cp,\varepsilon^{-1}}
\), \\
and \(\ln |\cN^{\cQc_h}_{\varepsilon}| 
\leq d \ln \paren*{1+\frac{4 KB_\dagger H}{\varepsilon}}+d^2 \ln \paren*{1+\frac{8 \sqrt{d} C_\dagger^2}{\varepsilon^2}}= \cO\paren*{d^2}\polylog\paren*{d, K, H, B_\dagger, C_\dagger,\varepsilon^{-1}}
\)
\end{itemize}
\end{lemma}
\begin{proof}
\looseness=-1
The statements in ($\mathrm{ii}$) immediately follow from the proof of \textbf{Lemma D.6} in \citet{jin2020provably}. 

\looseness=-1
We prove the first claim ($\mathrm{i}$).
For $\oqf{\pi, r}_{(k), h}$, we have
\begin{align*}
\oqf{\pi, r}_{(k), h}[\kappa]
=
&r_h + \clip\brace*{\co\beta^{(k)} + \hP^{(k)}\ovf{\pi, r}_{(k), h+1}[\kappa],\; 0,\; (H-h)(1+\kappa \ln A)} \\
= &
r_h + \clip\brace*{\co \sqrt{\bphi(s, a)^{\top} \paren*{\bLambda^{(k)}_h}^{-1} \bphi(s, a)}
+ \bphi(s, a)^\top 
\paren*{\bmu^{(k)}_h}^\top \ovf{\pi, r}_{(k), h+1}[\kappa],\; 0, \; (H-h)(1 + \kappa \ln A)}\;.
\end{align*}
According to the definition of $Q^{(\bw, \bLambda), r}_h$ (\Cref{def:abs-Q-function}), the claim immediately holds by showing the L2 bound of \( \left(\bmu_h^{(k)}\right)^{\top} \ovf{\pi, r}_{(k), h+1}[\kappa]\).
For any $h \in \bbrack{1, H}$ and $k \in \bbrack{1, K}$, we have
\begin{align*}
\norm*{\paren*{\bmu^{(k)}_h}^\top \ovf{\pi, r}_{(k), h+1}[\kappa]}_2
= & \norm*{
\sum^{k-1}_{i=1} 
\ovf{\pi, r}_{(k), h+1}[\kappa] \paren*{s^{(i)}_{h+1}} \bphi\paren*{s^{(i)}_h, a_h^{(i)}}^\top \paren*{\bLambda^{(k)}_h}^{-1}
}_2\\
\numeq{\leq}{a} & H_\kappa\norm*{
\paren*{\bLambda^{(k)}_h}^{-1}
\sum^{k-1}_{i=1} 
\bphi\paren*{s^{(i)}_h, a_h^{(i)}}
}_2
\leq KH_\kappa\;.
\end{align*}
where (a) uses $\norm*{\bphi}_2 \leq 1$ with $\rho=1$ and $0 \leq \ovf{\pi, r}_{(k), h+1}[\kappa] \leq H_\kappa$.

\looseness=-1
The remaining claims for \(\pqf{\pi, u}_{(k), h}(s, a) \in \cQu_h \) and \(\oqf{\pi, \dagger}_{(k), h}(s, a) \in \cQc_h \) can be similarly proven.
\end{proof}

\begin{definition}[Composite $Q$ function class]\label{def:composite-Q-function}
For each $h$, let $\Qcomp_h$ denote a function class such that
\begin{align*}
\Qcomp_h\df \brace*{
Q^{\dagger} + Q^{r} + \lambda Q^{u}\given 
Q^{\dagger} \in \cQc_h,\;  Q^{r} \in \cQr_h, \; Q^{u} \in \cQu_h,\; \text{ and } \; \lambda \in [0, C_\lambda]}\;.
\end{align*}
where $C_\lambda > 0$.
We let ${\cN}^{\Qcomp_h}_{\varepsilon}$ be the $\varepsilon$-cover of $\Qcomp_h$ with the distance metric $\dist_\infty$.
\end{definition}

\begin{lemma}[Composite Q cover]\label{lemma:composite-Q-cover}
When \Cref{algo:zero-vio-linear MDP} is run with $\rho =1$, the following statements hold:
\begin{itemize}
    \item[$(\mathrm{i})$] For all $(k, h)$, for any $\pi \in \Pi$, and for any $\lambda \in [0, C_\lambda]$,  
\(
\oqf{\pi, \dagger}_{(k), h} + \oqf{\pi, r}_{(k), h}[\kappa] + \lambda \pqf{\pi, u}_{(k), h} \in \Qcomp_h
\)
    \item[$(\mathrm{ii})$]
    \(
\ln \abs*{\cN^{\Qcomp_h}_{\varepsilon}} = 
\cO\paren*{d^2} 
\polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}}
\)
\end{itemize}
\end{lemma}
\begin{proof}
\looseness=-1
The claim $(\mathrm{i})$ clearly holds by \Cref{lemma:Q eps covers} and \Cref{def:composite-Q-function}.

\looseness=-1
We prove the second claim $(\mathrm{ii})$.
Let $\cN^\lambda_\varepsilon$ be the $\varepsilon$-cover of a set $\brace*{\lambda \given \lambda \in [0, C_\lambda]}$ with the distance metric $\norm*{\cdot}_2$.
Let $\varepsilon_\dagger, \varepsilon_{r}, \varepsilon_{u}, \varepsilon_{\lambda} > 0$ be positive scalars.
Consider 
$\widetilde{Q}^\dagger\in \cN^{\cQc_h}_{\varepsilon_\dagger}$,
$\widetilde{Q}^r\in \cN^{\cQr_h}_{\varepsilon_r}$,
$\widetilde{Q}^u \in \cN^{\cQu_h}_{\varepsilon_u}$, 
and $\widetilde{\lambda} \in \cN^\lambda_{\varepsilon_\lambda}$.
For any $Q^\dagger \in \cQc_h$, $Q^r \in \cQr_h$, $Q^u \in \cQu_h$, and $\lambda \in [0, C_\lambda]$, we have
\begin{align*}
\begin{aligned}
& \dist_\infty\paren*{
Q^\dagger + Q^r+\lambda Q^u, 
\widetilde{Q}^\dagger + \widetilde{Q}^r +\widetilde{\lambda} \widetilde{Q}^u}\\
&\leq
\underbrace{\sup _{s, a} \abs*{Q^\dagger(s, a) - \widetilde{Q}^\dagger(s, a)}}_{\leq \varepsilon_\dagger}
+ \underbrace{\sup _{s, a} \abs*{Q^r(s, a) - \widetilde{Q}^r(s, a)}}_{\leq \varepsilon_r}\\
&\quad + \underbrace{\lambda\sup _{s, a}\abs*{\paren*{Q^u(s, a) - \widetilde{Q}^u(s, a)}}}_{\leq C_\lambda \varepsilon_u} 
+ \underbrace{\sup _{s, a}
\abs*{
\paren*{\lambda - \widetilde{\lambda}}Q^u(s, a)}}_{\varepsilon_\lambda H}\\
&\numeq{\leq}{a}
\varepsilon_\dagger +  \varepsilon_r + C_\lambda \varepsilon_u + \varepsilon_\lambda H\;,
\end{aligned}    
\end{align*}
where 
(a) appropriately chooses $\widetilde{Q}^\dagger, \widetilde{Q}^r, \widetilde{Q}^u, \widetilde{\lambda}$.
By replacing 
$\varepsilon_\dagger$ with $\varepsilon / 4$, 
$\varepsilon_r$ with $\varepsilon / 4$, 
$\varepsilon_u$ with $1 / 4 C_\lambda$, and 
$\varepsilon_\lambda$ with $\varepsilon / 4 H$, the above inequality is upper bounded by $\varepsilon$.
Thus, 
\begin{align*}
\ln \abs*{\cN^{\Qcomp_h}_{\varepsilon}} 
\leq &
\ln \abs*{\cN^{\lambda}_{\varepsilon / 4 H}} 
+
\ln \abs*{\cN^{\cQu_h}_{\varepsilon / 4 C_\lambda}} 
+ \ln \abs*{\cN^{\cQr_h}_{\varepsilon / 4}}
+ \ln \abs*{\cN^{\cQc_h}_{\varepsilon / 4}} \\
\leq &
\cO\paren*{d^2} 
\polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}}
\;.
\end{align*}
where the second inequality uses \Cref{lemma:ball-covering-bound} and \Cref{lemma:Q eps covers}.
\end{proof}

\begin{definition}[Policy class]\label{def:policy-class}
$\tiPi \df \widetilde{\Pi}_1 \times \dots \times \widetilde{\Pi}_H$ denotes a softmax policy class such that 
\begin{align*}
\tiPi_h \df
\brace*{
\pi_Q \in \Pi
\given 
Q \in \Qcomp_h
}
\;\text{ where }\;
\pi_Q\paren{\cdot \given s}=\softmax\paren*{\frac{1}{\kappa}
Q(s, \cdot)
} \; \forall s \in \S\;
\;,
\end{align*}
where $\kappa > 0$.
We let ${\cN}^{\tiPi_h}_{\varepsilon}$ be the $\varepsilon$-cover of $\tiPi_h$ with the distance metric $\dist_1$.
\end{definition}


\begin{lemma}[$\pi^{(k), \lambda}$ cover]\label{lemma:policy-cover}
When \Cref{algo:zero-vio-linear MDP} is run with $\rho =1$ and $\kappa > 0$, for all $h$, the following statements hold:
\begin{itemize}
\item[$(\mathrm{i})$] For all $(k, h)$ and $\lambda \in [0, C_\lambda]$ in \Cref{algo:zero-vio-linear MDP}, 
\(\pi^{(k), \lambda}_h \in \tiPi_h\)
\item[$(\mathrm{ii})$]
$
\ln \abs*{\cN^{\tiPi_h}_\varepsilon}=
\cO\paren*{d^2} 
\polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
$
\end{itemize}
\end{lemma}
\begin{proof}
The claim $(\mathrm{i})$ immediately follows from \Cref{lemma:composite-Q-cover} and \Cref{def:composite-softmax-policy}.

\looseness=-1
We prove the second claim.
For a $Q: \SA \to \R$, let $\pi_Q$ be a softmax policy such that $\pi_Q\paren{\cdot \given s} = \softmax\paren*{\frac
{Q(s, \cdot)}{\kappa}}$. 
Consider $\widetilde{Q}$ from $\cN^{\Qcomp_h}_{\varepsilon}$.
Then, for any $Q \in \Qcomp_h$, we have
\begin{align*}
\dist_1\paren*{\pi_{Q}, \pi_{\widetilde{Q}}}
\numeq{\leq}{a} 
\frac{8}{\kappa}
\dist_\infty\paren*{Q, \widetilde{Q}}
\numeq{\leq}{b} \frac{8\varepsilon}{\kappa}\;,
\end{align*}
where (a) uses \Cref{lemma:softmax-policy-bound} and (b) appropriately chooses $\widetilde{Q}$ from $\cN^{\Qcomp_h}_\varepsilon$.
Therefore, 
\begin{align*}
\ln \abs*{\cN^{\tiPi_h}_\varepsilon}
\leq 
\ln \abs*{\cN^{\Qcomp_h}_{\kappa\varepsilon / 8}}
\leq
\cO\paren*{d^2} 
\polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
\end{align*}
where the second inequality uses \Cref{lemma:composite-Q-cover}.
\end{proof}

\begin{definition}[$V$ function class]\label{def:V-cover}
Let $\cVr_h$, $\cVu_h$, and $\cVc_h$ denote value function classes such that
\begin{align*}
&\cVr_h\df \brace*{V^\pi_Q[\kappa]: \S \to \R \given 
\pi \in \tiPi_h \cup \brace{\pisafe_h}\;
\text{ and }\; Q \in \cQr_h
}\;,\\
&\cVu_h\df \brace*{V^\pi_Q[0]: \S \to \R \given 
\pi \in \tiPi_h \cup \brace{\pisafe_h}\;
\text{ and }\; Q \in \cQu_h
}\;,\\
\text{ and }\;
&\cVc_h\df \brace*{V^\pi_Q[0]: \S \to \R \given 
\pi \in \tiPi_h \cup \brace{\pisafe_h}\;
\text{ and }\; Q \in \cQc_h
}\;,\\
\text{ where }\;
&V^\pi_Q[\kappa](s) \df \sum_{a \in \A} \pi\paren{a \given s}\paren*{Q(s, a) - \kappa \ln \pi\paren{a \given s}}\; \forall s \in \S \;.
\end{align*}
We let $\cN^{\cVr_h}_{\varepsilon}$, $\cN^{\cVu_h}_{\varepsilon}$, and $\cN^{\cVc_h}_{\varepsilon}$ be the $\varepsilon$-covers of $\cVr_h$, $\cVu_h$, and $\cVc_h$ with the distance metric $\dist_\infty$.
\end{definition}

\begin{lemma}[$V$ covers]\label{lemma:V-cover}
\looseness=-1
When \Cref{algo:zero-vio-linear MDP} is run with $\rho =1$ and $\kappa > 0$, for all $h$, the following statements hold:
\begin{itemize}
\item[$(\mathrm{i})$] 
For all $(k, h)$, for any $\lambda \in [0, C_\lambda]$, and for both $\pi=\pi^{(k),\lambda}$ and $\pi=\pisafe$, we have:\\
\(\ovf{\pi, r}_{(k), h}[\kappa] \in \cVr_h\),
\(\pvf{\pi, u}_{(k), h} \in \cVu_h\),
and \(\ovf{\pi, \dagger}_{(k), h} \in \cVc_h\)  
\item[$(\mathrm{ii})$]
\(\ln \abs*{\cN^{\cVr_h}_{\varepsilon}} =
\cO\paren*{d^2} \polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
\),\\
\(\ln \abs*{\cN^{\cVu_h}_{\varepsilon}} =
\cO\paren*{d^2} \polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
\),\\
and \(\ln \abs*{\cN^{\cVc_h}_{\varepsilon}} =
\cO\paren*{d^2} \polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
\)
\end{itemize}
\end{lemma}
\begin{proof}
The condition $(\mathrm{i})$ immediately follow from \Cref{lemma:Q eps covers} and \Cref{lemma:policy-cover} with \Cref{def:V-cover} and \Cref{def:composite-softmax-policy}.

\looseness=-1
We prove the second claim $(\mathrm{ii})$.
Let $Q \in \cQr_h$ and $\widetilde{Q} \in \cN^{\cQr_h}_{\varepsilon^r}$ where $\varepsilon^r > 0$.
For any two $\pi, \tpi : \S \to \sP(\A)$, for any $s$, we have
\begin{align*}
&\abs*{
\sum_{a \in \A}\pi\paren{a \given s} \paren*{Q(s, a) -\kappa \ln \pi\paren{a \given s}}
- \sum_{a \in \A}\widetilde{\pi}\paren{a \given s} \paren*{\widetilde{Q}(s, a) - \kappa \ln \pi \paren{a \given s}}
}\\
\leq &
\abs*{
\sum_{a \in \A}\pi\paren{a \given s} Q(s, a) 
- \sum_{a \in \A}\pi\paren{a \given s} \widetilde{Q}(s, a)    
+ \sum_{a \in \A}\pi\paren{a \given s} \widetilde{Q}(s, a)    
- \sum_{a \in \A}\widetilde{\pi}\paren{a \given s} \widetilde{Q}(s, a)    
}\\
&+ \kappa \underbrace{\abs*{\sum_{a \in \A}\pi\paren{a \given s}\ln \pi\paren{a \given s} - \tpi\paren{a \given s}\ln \tpi\paren{a \given s}}}_{\fd \sH(\pi) - \sH(\tpi)}
\\
\leq &
\sum_{a \in \A}\pi\paren{a \given s} \underbrace{\abs*{Q(s, a) - \widetilde{Q}(s, a)}}_{\leq \varepsilon^r}
+ 
\norm*{\pi\paren{\cdot \given s} - \widetilde{\pi}\paren{\cdot \given s}}_1 \underbrace{\norm*{\widetilde{Q}(\cdot, s)}_\infty}_{\leq H_\kappa} + \kappa \paren*{\sH(\pi) - \sH(\tpi)}
\\
\leq & \varepsilon^r + 
H_\kappa \norm*{\pi\paren{\cdot \given s} - \widetilde{\pi}\paren{\cdot \given s}}_1  
+ \kappa \paren*{\sH(\pi) - \sH(\tpi)}
\end{align*}
where the second inequality chooses appropriate $\widetilde{Q}$.
We defined entropies of $\pi$ and $\tpi$ as $\sH(\pi) \df \sum_{a \in \A}\pi\paren{a \given s}\ln \pi\paren{a \given s}$ and $\sH(\tpi) \df \sum_{a \in \A}\tpi\paren{a \given s}\ln \tpi\paren{a \given s}$, respectively.

\looseness=-1
The remaining task is to bound $H_\kappa \norm*{\pi\paren{\cdot \given s} - \widetilde{\pi}\paren{\cdot \given s}}_1 + \kappa \paren*{\sH(\pi) - \sH(\tpi)}$.
When $\pi = \pisafe$, choosing $\tpi=\pisafe$ trivially bounds this term by $0$.
Thus, we only consider the case when $\pi \in \tiPi_h$, i.e., $\pi\paren{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}Q^\circ(s, \cdot)}$ with $Q^\circ \in \Qcomp_h$.
We also consider $\tpi\paren{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}\widetilde{Q}^\circ(s, \cdot)}$ with $\widetilde{Q}^\circ \in \cN^{\Qcomp_h}_{\varepsilon^\circ}$, where $\varepsilon^\circ > 0$.
For the entropy gap, we have
\begin{align*}
&\sH(\pi) - \sH(\tpi)\\
=&\abs*{\sum_{a \in \A}\pi\paren{a \given s}\ln \pi\paren{a \given s} - \tpi\paren{a \given s}\ln \tpi\paren{a \given s}} \\
=&\abs*{\sum_{a \in \A}\paren*{\pi\paren{a \given s} - \tpi\paren{a \given s}}\ln \pi\paren{a \given s} + \sum_{a \in \A}\tpi\paren{a \given s}\paren*{\ln \pi\paren{a \given s} - \ln \tpi\paren{a \given s}}} \\
\leq&
\norm*{\pi\paren{\cdot \given s} - \tpi\paren{\cdot \given s}}_1 \max_a \ln \pi\paren{a \given s}
+ 
\max_a \abs*{\ln \pi\paren{a \given s} - \ln \tpi\paren{a \given s}}\\
\numeq{\leq}{a}&
\underbrace{\norm*{\pi\paren{\cdot \given s} - \tpi\paren{\cdot \given s}}_1}_{\leq \frac{8}{\kappa} \max_{a} \abs*{Q^\circ(s, a) - \widetilde{Q}^\circ(s, a)} \;\text{ by \Cref{lemma:softmax-policy-bound}}} \max_a \ln \pi\paren{a \given s}
+ 
\frac{2}{\kappa}\underbrace{\max_a \abs*{Q^\circ\paren{s, a} - \widetilde{Q}^\circ\paren{s, a}}}_{\leq \varepsilon^\circ}\\
\numeq{\leq}{b}& 
\frac{\varepsilon^\circ}{\kappa}
\paren*{
8\max_a \ln \pi\paren{a \given s}
+ 2 
}\;,
\end{align*}
where (a) utilizes a decomposition similar to the proof of \Cref{lemma:softmax-policy-bound}, and (b) chooses an appropriate $\widetilde{Q}^\circ$.
Finally, $\ln \pi\paren{a \given s}$ can be bounded as
\begin{align*}
\max_a \ln \pi\paren{a \given s}   
= 
\max_a \frac{1}{\kappa}Q^\circ(s, a) - \ln \sum_{a'} \exp\paren*{\frac{1}{\kappa}Q^\circ(s, a')}
\leq \frac{B_\dagger H + H_\kappa + C_\lambda H}{\kappa}\;,
\end{align*}
where the last inequality is due to \Cref{def:composite-Q-function}.

\looseness=-1
Therefore, we have
\begin{align*}
&H_\kappa 
\norm*{\pi\paren{\cdot \given s} - \widetilde{\pi}\paren{\cdot \given s}}_1+ \kappa \paren*{\sH(\pi) - \sH(\tpi)} \leq  \varepsilon^\circ \underbrace{\paren*{
2 + \frac{8}{\kappa}\paren*{B_\dagger H + 2H_\kappa + C_\lambda H}}}_{\fd Z}\;.
\end{align*}

\looseness=-1
Finally, by setting $\varepsilon^r = \varepsilon / 2H_\kappa$ and $\varepsilon^\circ = \varepsilon / 2Z$, $\ln \abs*{\cN^{\cVr_h}_{\varepsilon}}$ is bounded as:
\begin{align*}
\ln \abs*{\cN^{\cVr_h}_{\varepsilon}}
\leq 
\ln \paren*{\abs*{\cN^{\Qcomp_h}_{\varepsilon / 2 Z}} + 1}
+ \ln |\cN^{\cQr_h}_{\varepsilon / 2 H_\kappa}|
=
\cO\paren*{d^2} 
\polylog\paren*{d, K, H_\kappa, \co, \cp, B_\dagger, C_\dagger,C_\lambda,\varepsilon^{-1}, \kappa^{-1}}
\;,
\end{align*}
where the second inequality is due to \Cref{lemma:Q eps covers} and \Cref{lemma:composite-Q-cover}.
The claims for $\ln \abs*{\cN^{\cVu_h}_\varepsilon}$ and $\ln \abs*{\cN^{\cVc_h}_\varepsilon}$ can be similarly proven. 
\end{proof}








\subsection{Good Events and Value Confidence Bounds for \Cref{lemma:opt-pes-MDP-main} Proof}

\begin{lemma}[Good event 1]\label{lemma:good-event1-MDP}
Define $\rmvEevent$ as the event where the following inequality holds: 
\begin{align*}
&\sum_{k=1}^K \sum_{h=1}^H
\E\brack*{\norm*{\bphi(s_h^{(k)}, a_h^{(k)})}^2_{\paren*{\bLambda_h^{(k)}}^{-1}}\given s^{(k)}_h, a^{(k)}_h \sim \pi^{(k)}_h}\\
\leq&
2 \sum_{k=1}^K \sum_{h=1}^H
\norm*{\bphi(s_h^{(k)}, a_h^{(k)})}^2_{\paren*{\bLambda_h^{(k)}}^{-1}}
+ 4H \ln \frac{2KH}{\delta} \;.
\end{align*}
If \Cref{algo:zero-vio-linear MDP} is run with $\rho = 1$, $\P(\rmvEevent) \geq 1 - \delta$.
\end{lemma}
\begin{proof}
\looseness=-1
The claim immediately follows from \Cref{lemma:exp-to-concrete} with $\norm*{\phi}_2\leq 1$ and $\rho=1$.
\end{proof}

\begin{lemma}[Good event 2]\label{lemma:good-event2-MDP}
Define $\confevent$ as the event where the following condition holds: \\
For all $k, h$ and for any $V^r \in \cVr_{h+1}$, $V^u \in \cVu_{h+1}$, and $V^\dagger \in \cVc_{h+1}$
\begin{align*}
&\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}
V^r}(s, a)} 
\leq 
\co\beta^{(k)}_h(s, a)\quad \forall (h, s, a) \in \HSA\\
&\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}
V^u}(s, a)} 
\leq 
\cp\beta^{(k)}_h(s, a)\quad \forall (h, s, a) \in \HSA\\
\text{and } &\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}
V^\dagger}(s, a)} 
\leq 
C_\dagger\beta^{(k)}_h(s, a)\quad \forall (h, s, a) \in \HSA\;.
\end{align*}
\looseness=-1
If \Cref{algo:zero-vio-linear MDP} is run with $\rho =1$, $\co = \tiO(dH_\kappa)$, $\cp=\tiO(dH)$, and $C_\dagger=\tiO(dHB_\dagger)$, we have $\P(\confevent) \geq 1 - 2\delta$.
\end{lemma}
\begin{proof}
Using \Cref{lemma:hP-P-V bound} with $\cN^{\cVr_{h+1}}_{1 / K}$, with probability at least $1-\delta$, for any $(k, h, s, a)$, 
\begin{align*}
&\abs*{\paren*{\paren*{\hP_h^{(k)}-P_h}
V^r}(s, a)} \\
\numeq{\leq}{a}
&\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }
\paren*{
\sqrt{d}H_\kappa
+ 
2 H_\kappa\sqrt{\frac{d}{2} \ln \paren*{2K}}
+2H_\kappa \sqrt{\ln \frac{\abs*{\cN^{\cVr_{h+1}}_{1/K}}}{\delta}}
+4
}\\
\numeq{\leq}{b}
&\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }
\tiO \paren*{dH_\kappa}\ln \co
\numeq{\leq}{c}
\norm*{\bphi(s, a)}_{\paren*{\bLambda_h^{(k)}}^{-1} }\co
\end{align*}
where (a) sets $\varepsilon=1/K$ to $\cN^{\cVu_h}_{\varepsilon}$ and uses \cref{lemma:hP-P-V bound}, (b) uses \Cref{lemma:Q eps covers}, and (c) set sufficiently large $\co=\tiO\paren*{dH_\kappa}$ and uses \cref{lemma:alogx-inequality}.
The claim for $\cVu_{h+1}$ and $\cVc_{h+1}$ can be similarly proven.
\end{proof}


\begin{lemma}[Remove clipping one-side]\label{lemma:no-clip}
Under $\confevent$, for any $(k, h, s, a)$, and for any $\lambda \in [0, C_\lambda]$, for both $\pi = \pi^{(k), \lambda}$ and $\pi=\pisafe$, we have 
\begin{align*}
&\co\beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a) \geq \paren*{P_h \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a) \geq 0\;,\\
&-\cp \beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a) \leq  \paren*{P_h \pvf{\pi, u}_{(k), h+1}}(s, a) \leq H-h\;,\\
\text{ and }\;
&C_\dagger\beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \ovf{\pi, \dagger}_{(k), h+1}}(s, a) \geq \paren*{P_h \ovf{\pi, \dagger}_{(k), h+1}}(s, a) \geq 0\\
\end{align*}
\end{lemma}
\begin{proof}
We have
\begin{align*}
&\co\beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a)\\
\numeq{\geq}{a} &\abs*{\paren*{P_h - \hP^{(k)}_h} \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a) + \paren*{\hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a)\\
\geq &\paren*{P_h - \hP^{(k)}_h} \ovf{\pi, r}_{(k), h+1}[\kappa](s, a) + \paren*{\hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa]}(s, a)\\
= &
P_h \ovf{\pi, r}_{(k), h+1}[\kappa](s, a) \numeq{\geq}{b} 0\;,
\end{align*}   
where (a) is due to $\confevent$ with \Cref{lemma:V-cover} and (b) is due to $r \geq 0$ and by the definition of $\ovf{\pi, r}_{(k), h+1}[\kappa]$.
The claim for $\ovf{\pi, \dagger}_{(k), h+1}$ can be similarly proven.

\looseness=-1
For $\pvf{\pi, u}_{(k), h+1}$, we have
\begin{align*}
&-\cp \beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a)\\
\numeq{\leq}{a} &-\abs*{\paren*{\hP^{(k)}_h - P_h} \pvf{\pi, u}_{(k), h+1}}(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a)\\
\leq &-\paren*{\hP^{(k)}_h - P_h} \pvf{\pi, u}_{(k), h+1}(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a)\\
= &
P_h \pvf{\pi, u}_{(k), h+1}(s, a) \numeq{\leq}{b} H-h\;,
\end{align*}   
where (a) is due to $\confevent$ with \Cref{lemma:V-cover} and (b) is due to $u \leq \bone$ and by the definition of $\pvf{\pi, u}_{(k), h+1}$.
\end{proof}

\begin{definition}[$Q$ estimation gap]\label{def:delta-Q}
For any $h, k$ and $\pi \in \Pi$, define 
$\delta^{\pi, r}_{(k), h},
\delta^{\pi, u}_{(k), h}, \delta^{\pi, \dagger}_{(k), h}:\SA \to \R$
be functions such that:
\begin{align*}
&\delta^{\pi, r}_{(k), h} =\clip\brace*{\co \beta^{(k)}_h + \paren*{\hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa]}, 0, H_\kappa-h_\kappa}
- \paren*{P_h \ovf{\pi, r}_{(k), h+1}[\kappa]}\;, \\
&\delta^{\pi, u}_{(k), h} =\paren*{P_h \pvf{\pi, u}_{(k), h+1}} - \clip\brace*{-\cp\beta^{(k)}_h + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}, 0, H-h}\;,\\
\text{ and }\; 
&\delta^{\pi, \dagger}_{(k), h} =\clip\brace*{C_\dagger \beta^{(k)}_h + \paren*{\hP^{(k)}_h \ovf{\pi, \dagger}_{(k), h+1}}, 0, B_\dagger (H-h)}
- \paren*{P_h \ovf{\pi, \dagger}_{(k), h+1}}\;, \\
\end{align*}
It is clear that these functions satisfy, for any $(\pi, k, h)$,  %
\begin{equation}\label{eq:Q-Q-diff}
\begin{aligned}
\oqf{\pi, r}_{(k), h}[\kappa] = \qf{\pi, r + \delta^{\pi, r}_{(k)}}_{P, h}[\kappa],\quad
\pqf{\pi, u}_{(k), 1} = \qf{\pi, u - \delta^{\pi, u}_{(k)}}_{P, h},\;
\text{ and }\;
\oqf{\pi, \dagger}_{(k), h} = \qf{\pi, B_\dagger \beta^{(k)} + \delta^{\pi, \dagger}_{(k)}}_{P, h}\;.
\end{aligned}
\end{equation}
Additionally, let $\Deltar$, $\Deltau$, and $\Deltac$ be function classes such that:
\begin{align*}
&\Deltar \df  
\brace*{\delta: \HSA\to \R \given 
\bzero \leq \delta_h \leq \min\brace*{2\co \beta^{(k)}_h, H_\kappa-h_\kappa} \; \forall h \in \bbrack{1, H}}\\
&\Deltau \df 
\brace*{\delta: \HSA\to \R \given 
\bzero \leq \delta_h \leq \min \brace*{2\cp \beta^{(k)}_h, H-h} \; \forall h \in \bbrack{1, H}}\\
\text{ and }\;&\Deltac \df 
\brace*{\delta: \HSA\to \R \given 
\bzero \leq \delta_h \leq \min \brace*{2C_\dagger \beta^{(k)}_h, B_\dagger(H-h)} \; \forall h \in \bbrack{1, H}}\;.
\end{align*}
\end{definition}


\begin{lemma}\label{lemma:delta-bound}
Under $\confevent$, for any $k$ and for any $\lambda \in [0, C_\lambda]$, for both $\pi = \pi^{(k), \lambda}$ and $\pi=\pisafe$, it holds that
\(\delta^{\pi, r}_{(k), \cdot} \in \Deltar\),
\(\delta^{\pi, u}_{(k), \cdot} \in \Deltau\),
and  \(\delta^{\pi, \dagger}_{(k), \cdot} \in \Deltac\).
\end{lemma}
\begin{proof}
\looseness=-1
$\delta^{\pi, u}_{(k), h}(s, a) \leq H-h$ clearly holds.
Additionally, we have
\begin{align*}
\delta^{\pi, u}_{(k), h}(s, a) 
\numeq{=}{a}&\paren*{P_h \pvf{\pi, u}_{(k), h+1}}(s, a) - \max\brace*{-\cp \beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a), 0}\\
\leq &\paren*{P_h \pvf{\pi, u}_{(k), h+1}}(s, a) +\cp\beta^{(k)}_h(s, a) - \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a) \\
\leq &\cp \beta^{(k)}_h(s, a) + \abs*{\paren*{P_h - \hP^{(k)}_h} \pvf{\pi, u}_{(k), h+1}}(s, a) \numeq{\leq}{b} 2\cp\beta^{(k)}_h(s, a)\;,
\end{align*}
where (a) is due to \Cref{lemma:no-clip} and (b) is due to $\confevent$.
Finally, note that 
\begin{align*}
\delta^{\pi, u}_{(k), h}(s, a)
=&\paren*{P_h \pvf{\pi, u}_{(k), h+1}}(s, a) - \max\brace*{-\cp \beta^{(k)}_h(s, a) + \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a), 0}\\
\geq&
\underbrace{\cp \beta^{(k)}_h(s, a) + \paren*{P_h \pvf{\pi, u}_{(k), h+1}}(s, a) - \paren*{\hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}}(s, a)}_{\geq 0 \;\text{ by }\; \confevent}
\geq 0\;.
\end{align*}
This concludes the proof for $\delta^{\pi, u}_{(k), h}$.
The claims for $\delta^{\pi, r}_{(k), h}$ and $\delta^{\pi, \dagger}_{(k), h}$ can be similarly proven.
\end{proof}

\begin{lemma}[Restatement of \Cref{lemma:opt-pes-MDP-main}]\label{lemma:opt-pes-MDP}
Suppose $\confevent$ holds. 
For any $k$ and for any $\lambda \in [0, C_\lambda]$, for both $\pi = \pi^{(k), \lambda}$ and $\pi=\pisafe$, we have 
\begin{align*}
&\vf{\pi,r}_{P, h} \leq \ovf{\pi,r}_{(k), h} \leq  \vf{\pi,r+ 2\co \beta^{(k)}}_{P, h},
&&\qf{\pi,r}_{P, h} \leq \oqf{\pi,r}_{(k), h} \leq  \qf{\pi,r+ 2\co \beta^{(k)}}_{P, h}  \\
&\vf{\pi,B_\dagger\beta^{(k)}}_{P, h} \leq \ovf{\pi,\dagger}_{(k), h} \leq  \vf{\pi,B_\dagger\beta^{(k)} + 2C_\dagger \beta^{(k)}}_{P, h},
&&\qf{\pi,B_\dagger\beta^{(k)}}_{P, h} \leq \oqf{\pi,\dagger}_{(k), h} \leq  \qf{\pi,B_\dagger\beta^{(k)} + 2C_\dagger \beta^{(k)}}_{P, h},\\
&\vf{\pi,u-2\cp \beta^{(k)}}_{P, h} \leq \pvf{\pi,u}_{(k), h} \leq \vf{\pi,u}_{P, h},
&&\qf{\pi,u-2\cp \beta^{(k)}}_{P, h} \leq \pqf{\pi,u}_{(k), h} \leq \qf{\pi,u}_{P, h}\;.
\end{align*}
\end{lemma}
\begin{proof}
The inequalities for $Q$ functions directly hold by \Cref{eq:Q-Q-diff} and \Cref{lemma:delta-bound}.

\looseness=-1
For the utility $V$ function,
\begin{align*}
\pvf{\pi^{(k)},u}_{(k), h}(s) - \vf{\pi^{(k)},u}_{P, h}(s)
&= 
\sum_{a \in \A} \pi_h\paren{a \given s}
\paren*{
\pqf{\pi,u}_{(k), h}(s, a) - \qf{\pi,u}_{P, h}(s, a)
}\\
&\numeq{=}{a} 
\sum_{a \in \A} \pi_h\paren{a \given s} \qf{\pi, -\delta^{\pi, u}_{(k)}}_{P, h}(s)\numeq{\leq}{b} 0\;,
\end{align*}
where (a) uses \Cref{eq:Q-Q-diff} and (b) uses \Cref{lemma:delta-bound}.
Similarly,
\begin{align*}
\pvf{\pi,u}_{(k), h}(s) - \vf{\pi,u - 2\cp \beta^{(k)}}_{P, h}(s)
&= 
\sum_{a \in \A} \pi_h\paren{a \given s}
\paren*{
\pqf{\pi,u}_{(k), h}(s, a) - \qf{\pi,u-2\cp \beta^{(k)}}_{P, h}(s, a)
}\\
&\numeq{=}{a} 
\sum_{a \in \A} \pi_h\paren{a \given s} \qf{\pi, -\delta^{\pi, u}_{(k)} + 2\cp \beta^{(k)}}_{P, h}(s)\numeq{\geq}{b} 0\;,
\end{align*}
where (a) uses \Cref{eq:Q-Q-diff} and (b) uses \Cref{lemma:delta-bound}.
The claims for $r$ and $\dagger$ can be similarly proven.
\end{proof}


\subsection{Proofs for Zero-Violation Guarantee (\Cref{subsec:MDP-zero-vio})}

\subsubsection{Proof of \Cref{lemma:trigger-condition-main} and \Cref{lemma:softmax-value-monotonicity-main}}\label{subsec:safe-softmax-policy-exists-proof}

\begin{lemma}[Restatement of \Cref{lemma:softmax-value-monotonicity-main}]\label{lemma:softmax-value-monotonicity}
Let $f, g: \HSA \to \R$ be functions and let $\kappa > 0$. 
Given $\lambda \geq 0$, let $\pi^\lambda$ be a softmax policy such that 
\begin{align*}
\pi^\lambda_h \paren*{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}\paren*{
    \qf{\pi, f}_{P, h}[\kappa](s, \cdot)
    + \lambda \qf{\pi, g}_{P, h}(s, \cdot)}}\;.
\end{align*}
Then, \(\vf{\pi^\lambda, g}_{P, 1}(s_1)\) is monotonically increasing in $\lambda$.
\end{lemma}
\begin{proof}
Let $\cW \df \brace*{\occ{\pi}_{P, \cdot}: \HSA \to [0, 1] \given \pi \in \Pi}$ be the set of all the occupancy measures.
Let $\sL: \R\times \cW \to \R$ be a function such that:
\begin{align*}
\sL(\lambda, w) = &\sum_{h, s, a \in \HSA} w_h(s, a)\paren*{f_h(s, a) + \lambda g_h(s, a)}
- \kappa w_h(s, a)\ln \frac{w_h(s, a)}{\sum_{a' \in \A}w_h(s, a')}\;.
\end{align*}

\looseness=-1
We first show that $\sL$ is strictly concave in $\cW$.
Let 
$$\sH: w \in \cW \mapsto \sum_{h, s, a \in \HSA} - w_h(s, a)\ln \frac{w_h(s, a)}{\sum_{a' \in \A}w_h(s, a')}$$ 
be the function representing the second term of $\sL$.
Then, \footnote{This proof is based of \textbf{Lemma 14} from \citet{ding2023last}}
\begin{align*}
& \sH\left(\alpha w^1+(1-\alpha) w^2\right) \\
& =-\sum_{h, s, a}\left(\alpha w_h^1(s, a)+(1-\alpha) w_h^2(s, a)\right) \log \frac{\alpha w_h^1(s, a)+(1-\alpha) w_h^2(s, a)}{\alpha \sum_{a^{\prime}} w_h^1\left(s, a^{\prime}\right)+(1-\alpha) \sum_{a^{\prime}} w_h^2\left(s, a^{\prime}\right)} \\
& \numeq{\geq}{a}-\sum_{h, s, a} \alpha w_h^1(s, a) \log \frac{\alpha w_h^1(s, a)}{\alpha \sum_{a^{\prime}} w_h^1\left(s, a^{\prime}\right)}-\sum_{h, s, a}(1-\alpha) w_h^2(s, a) \log \frac{(1-\alpha) w_h^2(s, a)}{(1-\alpha) \sum_{a^{\prime}} w_h^2\left(s, a^{\prime}\right)} \\
& =\alpha \sH\left(w_h^1\right)+(1-\alpha) \sH\left(w_h^2\right)\;,
\end{align*}
for any $w^1, w^2 \in \cW$ and $\alpha \in [0, 1]$, where (a) is due to the log sum inequality 
$\left(\sum_i \bx_i\right) \ln \frac{\sum_i \bx_i}{\sum_i \by_i} \leq \sum_i \bx_i \ln \frac{\bx_i}{\by_i}$
for non-negative $\bx_i$ and $\by_i$.
Since (a) takes equality if and only if $w^1 = w^2$, $\sH$ is strictly concave.
Consequently, 
\(
\sL(\lambda, w) = \sum_{h, s, a \in \HSA} w_h(s, a)\paren*{f_h(s, a) + \lambda g_h(s, a)}
- \kappa \sH(w)
\) is also strictly concave in $\cW$.

\looseness=-1
Let \(w^\lambda = \argmax_{w \in \cW} \sL(\lambda, w)\), which is a unique maximizer due to the strict concavity. 
Define $\sL(\lambda) \df \max_{w \in \cW} \sL(\lambda, w)$. 
Using Danskin's theorem (\Cref{lemma:danskin's theorem}), 
$\sL(\lambda)$ is convex and $\frac{\partial\sL (\lambda)}{\partial \lambda} = \sum_{h, s, a \in \HSA} w^\lambda_h(s, a)g_h(s, a)$.
Since $\sL(\lambda)$ is convex, its derivative is non-decreasing. Therefore,
\begin{align}\label{eq:L-lambda-increasing}
\frac{\partial^2\sL (\lambda)}{\partial \lambda^2} =   
\frac{\partial}{\partial \lambda}
\sum_{h, s, a \in \HSA} w^\lambda_h(s, a)g_h(s, a) \geq 0\;.
\end{align}

\looseness=-1
Since $\pi^\lambda$ is the softmax policy, combined with the one-to-one mapping between occupancy measure and policy \citep{puterman1990markov}, the well-known analytical solution of regularized MDP \citep{geist2019theory} indicates that 
$w^\lambda$ corresponds to the occupancy measure of $\pi^\lambda$. 
Thus, due to \Cref{eq:L-lambda-increasing}, it holds that
$$0 \leq 
\frac{\partial}{\partial \lambda}
\sum_{h, s, a \in \HSA} w^\lambda_h(s, a)g_h(s, a)
=
\frac{\partial}{\partial \lambda}\vf{\pi^\lambda, g}_{P, 1}(s_1)\;.$$
This concludes the proof.
\end{proof}








\begin{definition}[Softmax policy with fixed $\bdelta$]\label{def:fixed-delta-softmax}
For any $k \in \unconfMDP^c$, $\bdelta \df (\delta^r, \delta^u, \delta^\dagger) \in \Deltar \times \Deltau \times \Deltac$ and $\lambda \geq 0$, let $\pi^{\bdelta, \lambda}\in \Pi$ be a policy such that 
\begin{align*}
\pi^{\bdelta, \lambda}_h \paren*{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}\paren*{
    \qf{\pi^{\bdelta, \lambda}, B_\dagger \beta^{(k)} + \delta^\dagger}_{P, h}(s, \cdot)
    + \qf{\pi^{\bdelta, \lambda}, r + \delta^r}_{P, h}[\kappa](s, \cdot) + \lambda \qf{\pi^{\bdelta, \lambda}, u - \delta^u}_{P, h}(s, \cdot)}}\;.
\end{align*}
\end{definition}

\begin{lemma}[Existence of feasible $\lambda$]\label{lemma:feasible-lambda-exists}
Suppose $\kappa \leq \frac{\bslt^2}{32H_\kappa^2 (B_\dagger + 1)}$.
For any $k$ and for any $\bdelta \in \Deltac\times \Deltar \times \Deltau$, there exists a $\lambda^{\bdelta} \in \brack*{0, \frac{8H_\kappa^2(B_\dagger + 1)}{\bslt}}$ such that, 
\(
\vf{\pi^{\bdelta, \lambda}, u - \delta^u}_{P, 1}(s_1) \geq b
\) holds for any $\lambda \geq \lambda^{\bdelta}$.
\end{lemma}
\begin{proof}
\looseness=-1
Throughout the proof, we use a shorthand $r^\bdelta \df B_\dagger \beta^{(k)} + \delta^\dagger + r + \delta^r$.
Consider the following entropy-regularized max-min optimization problem:
\begin{align}
&\max_{\pi \in \Pi}\min_{\lambda \geq 0} \vf{\pi, r^\delta}_{P, 1}[\kappa](s_1)  + \lambda \paren*{\vf{\pi, u - \delta^u}_{P, 1}(s_1) - b - \frac{\bslt}{4}} + \frac{\kappa}{2} \lambda^2\nonumber\\
=&\min_{\lambda \geq 0}\max_{\pi \in \Pi} \vf{\pi, r^\delta}_{P, 1}[\kappa](s_1)  + \lambda \paren*{\vf{\pi, u - \delta^u}_{P, 1}(s_1) - b - \frac{\bslt}{4}} + \frac{\kappa}{2} \lambda^2 \;. \label{eq:min-max-duality}
\end{align}
where the equality holds by the strong duality of regularized CMDPs (see, e.g., \textbf{Appendix C.1} in  \citet{ding2023last}).
Let $(\widetilde{\pi}, \widetilde{\lambda})$ be a saddle point of the problem, which is ensured to be unique thanks to the regularization. 
We first show the analytical forms of $(\widetilde{\pi}, \widetilde{\lambda})$.

\paragraph{Analytical forms of $(\widetilde{\pi}, \widetilde{\lambda})$.}
Due to the strong duality, we have
\begin{align*}
\max_{\pi \in \Pi} \vf{\pi, r^\bdelta \widetilde{\lambda} (u - \delta^u)}_{P, 1}[\kappa](s_1)
= \vf{\widetilde{\pi}, r^\bdelta + \widetilde{\lambda}(u - \delta^u)}_{P, 1}[\kappa](s_1)\;.
\end{align*}
Since the left-hand side is an entropy-regularized optimization problem in an MDP, the well-known analytical solution of regularized MDP indicates that \citep{geist2019theory}:
\begin{align}\label{eq:pilambda-analytical}
\widetilde{\pi}_{h}\paren*{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}\paren*{
\qf{\widetilde{\pi}, r^\bdelta}_{P, h}[\kappa](s, \cdot) + \widetilde{\lambda} \qf{\widetilde{\pi}, u - \delta^u}_{P, h}(s, \cdot)}}
=\pi^{\bdelta, \widetilde{\lambda}}_h\;, 
\end{align}
where the last equality is due to the definition of $\pi^{\bdelta, \lambda}_h$.
Additionally, due to the strong duality,
\begin{align*}
\widetilde{\lambda} \in \argmin_{\lambda \geq 0}\vf{\widetilde{\pi}, r^\bdelta}_{P, 1}[\kappa](s_1)  + \lambda \paren*{\vf{\widetilde{\pi}, u - \delta^u}_{P, 1}(s_1) - b - \frac{\bslt}{4}} + \frac{\kappa}{2} \lambda^2 \;.
\end{align*}
Since the right-hand side is a quadratic equation on $\lambda$, we have
\begin{align}
\widetilde{\lambda} = \frac{1}{\kappa} \brack*{b + \frac{\bslt}{4} - \vf{\widetilde{\pi}, u - \delta^u}_{P, 1}(s_1)}_+\;. \label{eq:lambda-analytical}
\end{align}

\looseness=-1
\paragraph{$\widetilde{\lambda}$ upper bound.}
Next, we will show that $\widetilde{\lambda}$ is upper bounded by constant.
We have
\begin{align*}
2H^2_\kappa (B_\dagger + 1)
&\numeq{\geq}{a} 
\vf{\widetilde{\pi}, r^\bdelta}_{P, 1}[\kappa](s_1)  - \underbrace{\frac{1}{2\kappa}\brack*{b + \frac{\bslt}{4} - \vf{\widetilde{\pi}, u - \delta^u}_{P, 1}(s_1)}^2_+}_{\geq 0}\\
&\numeq{=}{b} \vf{\widetilde{\pi}, r^\bdelta}_{P, 1}[\kappa](s_1)  + \widetilde{\lambda} \paren*{\vf{\widetilde{\pi}, u - \delta^u}_{P, 1}(s_1) - b - \frac{\bslt}{4}} + \frac{\kappa}{2} \widetilde{\lambda}^2\\
&\numeq{\geq}{c} \vf{\pisafe, r^\bdelta}_{P, 1}[\kappa](s_1) 
+ \widetilde{\lambda} \paren*{\vf{\pisafe, u - \delta^u}_{P, 1}(s_1) - b - \frac{\bslt}{4}} + \frac{\kappa}{2} \widetilde{\lambda}^2\\
&\geq \widetilde{\lambda} \paren*{\underbrace{\vf{\pisafe, u}_{P, 1}(s_1) - b - \frac{\bslt}{4}}_{\geq 3\bslt / 4} 
-\underbrace{\vf{\pisafe,2C_\beta\beta^{(k)}}_{P, 1}(s_1)}_{\leq \bslt / 2 \;\text{ since } k \in \unconfMDP^c}
} \geq \widetilde{\lambda} \frac{\bslt}{4}\;,
\end{align*}
where (a) is since $\norm*{r^\bdelta}_\infty = \norm*{B_\dagger \beta^{(k)} + \delta^\dagger + r + \delta^r}_\infty 
\leq B_\dagger + B_\dagger H + 1 + H
= (H + 1)(B_\dagger + 1)$, (b) is due to \Cref{eq:lambda-analytical}, (c) uses \Cref{eq:min-max-duality}.
By reformulating the inequality, 
\begin{align}\label{eq:lambda-upper-bound}
\widetilde{\lambda}  \leq  \frac{8H_\kappa^2(B_\dagger + 1)}{\bslt}\;.  
\end{align}


\paragraph{Constraint violation of $\pi^{\bdelta, \lambda}$}
Finally, we will show that for any $\lambda \geq \widetilde{\lambda}$, $\pi^{\bdelta, \lambda}$ guarantees zero constraint violation.
Due to \Cref{eq:pilambda-analytical,,eq:lambda-analytical,,eq:lambda-upper-bound}, we have
\begin{align*}
\kappa \widetilde{\lambda} = \brack*{b + \frac{\bslt}{4}- \vf{\pi^{\bdelta, \widetilde{\lambda}}, u - \delta^u}_{P, 1}(s_1)}_+ \leq 
\frac{8\kappa H^2_\kappa (B_\dagger + 1)}{\bslt}\;,
\end{align*}
which ensures the small violation of $\pi^{\bdelta, \widetilde{\lambda}}$ when $\kappa \ll 1$. 
Since $\vf{\pi^{\bdelta, \lambda}, u - \delta^u}_{P, 1}(s_1)$ is monotonically increasing in $\lambda$ due to \Cref{lemma:softmax-value-monotonicity}, for any $\lambda \geq \widetilde{\lambda}$, $\vf{\pi^{\bdelta, \lambda}, u - \delta^u}_{P, 1}(s_1) \geq b + \frac{\bslt}{4}- \frac{8\kappa H^2_\kappa (B_\dagger + 1)}{\bslt}$. 
Therefore, by setting 
$
\kappa \leq \frac{\bslt^2}{32H_\kappa^2 (B_\dagger + 1)}
$, we have 
$\vf{\pi^{\bdelta, \lambda}, u - \delta^u}_{P, 1}(s_1) \geq b$.
\end{proof}

\begin{lemma}[Restatement of \Cref{lemma:trigger-condition-main}]
If \Cref{algo:zero-vio-linear MDP} is run with $\rho = 1$, $C_\lambda \geq \frac{8H_\kappa^2 (B_\dagger + 1)}{\bslt}$, and $\kappa \leq \frac{\bslt^2}{32H_\kappa^2 (B_\dagger + 1)}$,
under $\confevent$, 
it holds $\pvf{\pi^{(k), C_\lambda}, u}_{(k), 1}(s_1) \geq b$ for any $k \in \unconfMDP^c$.
\end{lemma}
\begin{proof}
Due to $\confevent$, it holds that
$$
\bdelta \df \paren*{\delta^{\pi^{(k), C_\lambda}, r}_{(k), \cdot},
\delta^{\pi^{(k), C_\lambda}, u}_{(k), \cdot}, 
\delta^{\pi^{(k), C_\lambda}, \dagger}_{(k), \cdot}} \in \Deltac \times \Deltar \times \Deltau\;.
$$
\looseness=-1
According to \Cref{eq:Q-Q-diff}, this $\bdelta$ satisfies
$\pi^{\bdelta, C_\lambda} = \pi^{(k), C_\lambda}$ where $\pi^{\bdelta, C_\lambda}$ is defined in \Cref{def:fixed-delta-softmax}.
Therefore, using \Cref{lemma:feasible-lambda-exists}, 
$\pvf{\pi^{(k), C_\lambda}, u}_{(k), 1}(s_1) \geq b$.
This concludes the proof.
\end{proof}


\subsubsection{Proof of \Cref{lemma:Ck-bound-MDP-main}}
\begin{lemma}[Bonus summation bound]\label{lemma:elliptical-potential-MDP}
\looseness=-1
If \Cref{algo:zero-vio-linear MDP} is run with $\rho = 1$, under $\rmvEevent$ and $\confevent$, it holds that
\begin{align*}
&\sum_{k=1}^K
\paren*{\vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)}^2   
\leq 2 H^2 d\ln\paren*{1 + \frac{K}{d}}+ 4 H^2 \ln \frac{2KH}{\delta}
= \tiO\paren*{H^2d}\\
\text{and }\;
&\sum_{k=1}^K
\paren*{\vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)} \leq 
H\sqrt{K}
\sqrt{
2 d\ln\paren*{1 + \frac{K}{d}}+ 4 \ln \frac{2KH}{\delta}
}
=\tiO\paren*{H\sqrt{d K}}\;.\\
\end{align*}
\end{lemma}
\begin{proof}
\looseness=-1
We have 
\begin{align*}
\sum_{k=1}^K
\paren*{\vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)}^2
&=
\sum_{k=1}^K
\paren*{
\sum_{h=1}^H
\E\brack*{\beta^{(k)}_h(s_h, a_h)\given s_h, a_h \sim \pi^{(k)}}}^2\\
&\numeq{\leq}{a}
H
\sum_{k=1}^K
\sum_{h=1}^H
\paren*{
\E\brack*{\beta^{(k)}_h(s_h, a_h)\given s_h, a_h \sim \pi^{(k)}}}^2\\
&\numeq{\leq}{a}
H \sum_{k=1}^K
\sum_{h=1}^H
\E\brack*{\norm*{\bphi(s_h, a_h)}^2_{\paren*{\bLambda_h^{(k)}}^{-1}}\given s_h, a_h \sim \pi^{(k)}}\\
&\numeq{\leq}{b}
2H\sum_{k=1}^K
\sum_{h=1}^H
\norm*{\bphi(s_h^{(k)}, a_h^{(k)})}^2_{\paren*{\bLambda_h^{(k)}}^{-1}}
+ 4 H^2\ln \frac{2KH}{\delta} \\
&\numeq{\leq}{c} 2 H^2 d\ln\paren*{1 + \frac{K}{d}}+ 4 H^2 \ln \frac{2KH}{\delta} \;,
\end{align*}
where (a) is due to Jensen's inequality, (b) is due to $\rmvEevent$, and (c) uses \Cref{lemma:elliptical potential}.   
The second claim follows by:
\begin{align*}
\sum_{k=1}^K  \vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)
\numeq{\leq}{a} \sqrt{K}\sqrt{\sum_{k=1}^K \paren*{\vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)}^2}
\numeq{\leq}{b} 
H\sqrt{K}
\sqrt{
2 d\ln\paren*{1 + \frac{K}{d}}+ 4 \ln \frac{2KH}{\delta}
}\;,
\end{align*}
where (a) uses Cauchyâ€“Schwarz inequality and (b) uses the first claim.
\end{proof}

\begin{lemma}[Restatement of \Cref{lemma:Ck-bound-MDP-main}]\label{lemma:Ck-bound-MDP}
Suppose \Cref{algo:zero-vio-linear MDP} is run with $\rho = 1$ and $\rmvEevent$ and $\confevent$ hold.
Then, 
\begin{align*}
|\unconfMDP|
\leq
\frac{64\cp^2 H^2 d}{\bslt^2}\ln\paren*{\frac{2KH}{\delta}}
= \tiO\paren*{\bslt^{-2} H^4 d^3}\;,
\end{align*}
where the last equality sets $\cp = \tiO\paren*{dH}$.
\end{lemma}
\begin{proof}
\looseness=-1
Using \Cref{lemma:elliptical-potential-MDP} and \Cref{def:unconf-set MDP}, we have 
\begin{align*}
\abs{\unconfMDP} \paren*{\frac{\bslt}{2}}^2
\leq
\sum_{k\in \unconfMDP}
\paren*{\vf{\pisafe, 2\cp\beta^{(k)}}_{P, 1}(s_1)}^2
\leq 
8\cp^2 H^2 d\ln\paren*{1 + \frac{K}{d}}+ 16\cp^2 H^2 \ln \frac{2KH}{\delta} \;.
\end{align*}
Therefore, we have
\begin{align*}
\abs{\unconfMDP}
\leq
\frac{32\cp^2 H^2 d}{\bslt^2}\ln\paren*{1 + \frac{K}{d}}
+ \frac{64 \cp^2 H^2}{\bslt^2}\ln \frac{2KH}{\delta} 
\leq
\frac{64\cp^2 H^2 d}{\bslt^2}\ln\paren*{\frac{2KH}{\delta}}
\;.
\end{align*}
\end{proof}

\subsection{Proofs for Sublinear Regret Guarantee (\Cref{subsec:MDP-regret-analysis})}

\looseness=-1
Suppose the good events $\rmvEevent\cap \confevent$ hold.
We decompose the regret as follows:
\begin{align}
&\regret(K) \nonumber \\
=& \sum_{k=1}^K \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pi^{(k)}, r}_{P, 1}(s_1)} \nonumber \\
=& 
\sum_{k\in \unconfMDP} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pi^{(k)}, r}_{P, 1}(s_1)}  
+ 
\sum_{k \in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pi^{(k)}, r}_{P, 1}(s_1)} \nonumber\\
\leq &
|\unconfMDP| H
+ 
\sum_{k \in \unconfMDP^c} \paren*{\ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1) - \vf{\pi^{(k)}, r}_{P, 1}(s_1)}
+
\sum_{k\in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}\nonumber
\\
\numeq{\leq}{a} &
\tiO\paren*{d^3H^4 \bslt^{-2}}
+ 
\sum_{k \in \unconfMDP^c} \paren*{\ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1) - \vf{\pi^{(k)}, r}_{P, 1}[\kappa](s_1)}
+
\sum_{k\in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}
+ \kappa KH \ln A\nonumber\\
\numeq{\leq}{b} &
\tiO\paren*{d^3H^4 \bslt^{-2}}
+ 
\underbrace{
2\co \sum_{k \in \unconfMDP^c} \vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)
}_{\circled{1}}
+
\underbrace{\sum_{k\in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}}_{\circled{2}}
+ \kappa KH \ln A
\;, \label{eq:regret-decomposition}
\end{align}   
where (a) uses \Cref{lemma:Ck-bound-MDP} and (b) is due to \Cref{lemma:opt-pes-MDP} with $\confevent$.
Under $\rmvEevent \cap \confevent$, $\circled{1}$ can be easily bounded by \Cref{lemma:elliptical-potential-MDP}
\begin{align}\label{eq:martingale-bound}
\circled{1} \leq 
\co \tiO(H\sqrt{dK})
\leq \tiO\paren*{H_\kappa^{2} d^{3/2}\sqrt{K}}\;,
\end{align}
where the last equality inserts $\co = \tiO\paren*{dH_\kappa}$.

\subsubsection{Mixture Policy Decomposition}

\looseness=-1
We upper bound $\circled{2}$ in \Cref{eq:regret-decomposition} by the mixture policy technique.

\begin{lemma}[Mixture policy's feasibility]\label{lemma:mixture-feasibility}
Let $\alpha^{(k)} \df \frac{\bslt}{\bslt + 2 \vf{\pi^\star, 2\cp \beta^{(k)}}_{P, 1}(s_1)}$.
For any $k \in \unconfMDP^c$ and $\alpha \in [0, \alpha^{(k)}]$, $\pi^\alpha$ defined in \Cref{def:mixture-policy} satisfies
\(
\vf{\pi^\alpha, u-2\cp \beta^{(k)}}_{P, 1}(s_1) \geq b
\).
\end{lemma}
\begin{proof}
We have    
\begin{align*}
&\vf{\pi^\alpha, u-2\cp \beta^{(k)}}_{P, 1}(s_1) - b\\
=& 
(1-\alpha)\paren*{\vf{\pisafe, u-2\cp \beta^{(k)}}_{P, 1}(s_1) - b}
+ \alpha\paren*{\vf{\pi^\star, u-2\cp \beta^{(k)}}_{P, 1}(s_1) - b}\\
\geq&
(1-\alpha)\frac{\bslt}{2}
+ \alpha\paren*{\vf{\pi^\star, -2\cp \beta^{(k)}}_{P, 1}(s_1)}\;,
\end{align*}
where the last inequality holds because $\vf{\pisafe, 2\cp\beta^{(k)}}_{P, 1}(s_1) \leq \frac{\bslt}{2}$ due to $k \in \unconfMDP^c$.
Thus, $\vf{\pi^\alpha, u-2\cp \beta^{(k)}}_{P, 1}(s_1) - b \geq 0$ holds when 
\begin{align*}
\alpha \leq \frac{\bslt}{\bslt + 2 \vf{\pi^\star, 2\cp \beta^{(k)}}_{P, 1}(s_1)}\;.
\end{align*}
\end{proof}

\begin{lemma}[Mixture policy's optimism]\label{lemma:mixture-optimism}
Let $B_\dagger \geq \frac{4\cp H}{\bslt}$.
For any $k \in \unconfMDP^c$, $\pi^{\alpha^{(k)}}$ with $\alpha^{(k)}$ from \Cref{lemma:mixture-feasibility} satisfies,
\begin{align*}
\vf{\pi^{\alpha^{(k)}}, r+B_\dagger \beta^{(k)}}_{P, 1}(s_1)  \geq 
\vf{\pi^\star, r}_{P, 1}(s_1) 
\;\text{ and }\;
\vf{\pi^{\alpha^{(k)}}, u-2\cp \beta^{(k)}}_{P, 1}(s_1) \geq b\;.
\end{align*}
\end{lemma}
\begin{proof}
The sufficient condition that
\(
\vf{\pi^\alpha, r+B_\dagger \beta^{(k)}}_{P, 1}(s_1) 
\geq
\vf{\pi^\star, r}_{P, 1}(s_1)
\) to hold is 
\begin{align*}
B_\dagger
&\geq 
\frac{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pi^\alpha, r}_{P, 1}(s_1)}{\vf{\pi^\alpha, \beta^{(k)}}_{P, 1}(s_1)}
= 
\frac{(1-\alpha)\paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pisafe, r}_{P, 1}(s_1)}}
{(1-\alpha)\vf{\pisafe, \beta^{(k)}}_{P, 1}(s_1)
+ \alpha\vf{\pi^\star, \beta^{(k)}}_{P, 1}(s_1)}\\
&= 
\frac{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pisafe, r}_{P, 1}(s_1)}
{\vf{\pisafe, \beta^{(k)}}_{P, 1}(s_1)
+ \frac{\alpha}{1-\alpha}\vf{\pi^\star, \beta^{(k)}}_{P, 1}(s_1)}\;.
\end{align*} 
By inserting $\alpha^{(k)} = \frac{\bslt}{\bslt + 2 \vf{\pi^\star, 2\cp \beta^{(k)}}_{P, 1}(s_1)}$ into $\alpha$, i.e., 
$\frac{\alpha}{1-\alpha} = \frac{\bslt}{2 \vf{\pi^\star, 2\cp \beta^{(k)}}_{P, 1}(s_1)}$, 
\begin{align*}
B_\dagger
\geq
\frac{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pisafe, r}_{P, 1}(s_1)}
{\vf{\pisafe, \beta^{(k)}}_{P, 1}(s_1)
+ \frac{\bslt}{4\cp \vf{\pi^\star, \beta^{(k)}}_{P, 1}(s_1)}\vf{\pi^\star, \beta^{(k)}}_{P, 1}(s_1)}
= 
\frac{4\cp\paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \vf{\pisafe, r}_{P, 1}(s_1)}}
{2\vf{\pisafe, 2\cp \beta^{(k)}}_{P, 1}(s_1) + \bslt}\;.
\end{align*} 
Thus, when $B_\dagger \geq \frac{4\cp H}{\bslt}$, 
it holds that \(\vf{\pi^{\alpha^{(k)}}, r+B_\dagger \beta^{(k)}}_{P, 1}(s_1)  \geq \vf{\pi^\star, r}_{P, 1}(s_1)\).
The second claim follows from \Cref{lemma:mixture-feasibility}.
\end{proof}

\looseness=-1
We are now ready to decompose $\circled{2}$.
Using \Cref{lemma:mixture-feasibility,,lemma:mixture-optimism}, we have
\begin{equation}\label{eq:optimism-decomposition}
\begin{aligned}
\circled{2}
=& \sum_{k\in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}\\
\leq& \sum_{k\in \unconfMDP^c} \paren*{
\vf{\pi^{\alpha^{(k)}}, B_\dagger \beta^{(k)}}_{P, 1}(s_1)  
+ \vf{\pi^{\alpha^{(k)}}, r}_{P, 1}[\kappa](s_1)  
- \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}\\
=&
\sum_{k\in \unconfMDP^c} 
\Big(\vf{\pi^{\alpha^{(k)}}, B_\dagger \beta^{(k)}}_{P, 1}(s_1) 
+ \vf{\pi^{\alpha^{(k)}}, r}_{P, 1}[\kappa](s_1)
+ \widebar{\lambda}^{(k, T)} \vf{\pi^{\alpha^{(k)}}, u - 2\cp \beta^{(k)}}_{P, 1}(s_1)\\
&
\underbrace{\quad\quad
- \ovf{\pi^{(k)}, \dagger}_{(k), 1}(s_1)
- \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)
- \widebar{\lambda}^{(k, T)} \pvf{\pi^{(k)}, u}_{(k), 1}(s_1)
\Big)\quad \quad\quad\quad\quad}_{\circled{3}}\\
&+ 
\underbrace{\sum_{k\in \unconfMDP^c}  \ovf{\pi^{(k)}, \dagger}_{(k), 1}(s_1) }_{\circled{4}}
+ 
\underbrace{\sum_{k\in \unconfMDP^c} 
\widebar{\lambda}^{(k, T)}
\paren*{
\pvf{\pi^{(k)}, u}_{(k), 1}(s_1) - \vf{\pi^{\alpha^{(k)}}, u - 2\cp\beta^{(k)}}_{P, 1}(s_1)
}}_{\circled{5}} \;,
\end{aligned}
\end{equation}
where $\widebar{\lambda}^{(k, T)}$ is defined in \Cref{line:pik-deploy}.
Using \Cref{lemma:opt-pes-MDP}, the term $\circled{4}$ is bounded as
\begin{align*}
\circled{4} \leq \vf{\pi^{(k)}, (B_\dagger + 2C_\dagger)\beta^{(k)}}_{P, 1}(s_1)\;.
\end{align*}
Using \Cref{lemma:elliptical-potential-MDP}, it holds that
\begin{align}\label{eq:additional bonus bound}
\circled{4} \leq 
(B_\dagger + 2C_\dagger) \tiO\paren*{H\sqrt{dK}}
= \tiO\paren*{H^{4} d^{5/2} \bslt^{-1}\sqrt{K}}\;,
\end{align}
where the last equality inserts $B_\dagger = 4\bslt^{-1}\cp H$, $\cp = \tiO(dH)$, and $C_\dagger=\tiO(dHB_\dagger)$.
We will bound $\circled{3}$ and $\circled{5}$ separately.

\subsubsection{Optimistic Bounds}

\begin{lemma}[Optimism in composite value function]\label{lemma:optimism-composite}
Suppose $\confevent$ holds. Then,
\begin{align*}
\circled{3} = 
&\sum_{k\in \unconfMDP^c} 
\Big(\vf{\pi^{\alpha^{(k)}}, B_\dagger \beta^{(k)}}_{P, 1}(s_1) 
+ \vf{\pi^{\alpha^{(k)}}, r}_{P, 1}[\kappa](s_1)
+ \widebar{\lambda}^{(k, T)} \vf{\pi^{\alpha^{(k)}}, u - 2\cp \beta^{(k)}}_{P, 1}(s_1)\\
&
\quad\quad
- \ovf{\pi^{(k)}, B_\dagger \beta^{(k)}}_{(k), 1}(s_1)
- \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)
- \widebar{\lambda}^{(k, T)} \pvf{\pi^{(k)}, u}_{(k), 1}(s_1)
\Big) \leq 0\;.
\end{align*}
\end{lemma}
\begin{proof}
Using \Cref{lemma:regularized value difference}, for any $k \in \unconfMDP^c$, we have
\begin{align*}
& \ovf{\pi^{(k)}, B_\dagger \beta^{(k)}}_{(k), 1}(s_1)
+\ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)
+\widebar{\lambda}^{(k, T)} \pvf{\pi^{(k)}, u}_{(k), 1}(s_1) \\
& -\vf{\pi^{\alpha^{(k)}}, B_\dagger \beta^{(k)}}_{P, 1}(s_1) 
- \vf{\pi^{\alpha^{(k)}}, r}_{P, 1}[\kappa](s_1)
- \widebar{\lambda}^{(k, T)} \vf{\pi^{\alpha^{(k)}}, u - 2\cp \beta^{(k)}}_{P, 1}(s_1)\\
=&
\vf{\pi^{\alpha^{(k)}}, f^1}_{P, 1}(s_1) 
+ \vf{\pi^{\alpha^{(k)}}, f^2}_{P, 1}(s_1) 
+ \widebar{\lambda}^{(k, T)}\vf{\pi^{\alpha^{(k)}}, 2\cp \beta^{(k)}}_{P, 1}(s_1) 
\end{align*}
where $f^1: \HSA\to \R$ and $f^2: \HSA\to \R$ are functions such that
\begin{align*}
f^1_h(s, a) &= 
\sum_{a \in \A}
\paren*{
\pi^{(k)}_h\paren{a \given s} 
\paren*{\oqf{\pi^{(k)}, r}_{(k), h}[\kappa](s, a) + \widebar{\lambda}^{(k, T)} \pqf{\pi^{(k)}, u}_{(k), h}(s, a)
- \kappa \ln \pi^{(k)}_h\paren{a \given s}}
}\\
&\quad - 
\sum_{a \in \A}
\paren*{
\pi^{\alpha^{(k)}}_h\paren{a \given s} 
\paren*{\oqf{\pi^{(k)}, r}_{(k), h}(s, a) + \widebar{\lambda}^{(k, T)} \pqf{\pi^{(k)}, u}_{(k), h}(s, a)
- \kappa \ln \pi^{\alpha^{(k)}}_h\paren{a \given s}}
}\\
f^2_h(s, a) &= \delta^{\pi^{(k)}, r}_{(k)} - \widebar{\lambda}^{(k, T)}\delta^{\pi^{(k)}, u}_{(k)}\;.
\end{align*}

It is well-known that the analytical maximizer of 
\(
\max_{\pi \in \sP(\A)}
\sum_{a \in \A}
\pi\paren{a} \paren*{\bx(a) - \kappa \ln \pi\paren{a}}
\) is $\softmax\paren*{\frac{1}{\kappa} \bx(\cdot)}$. Therefore, the function $f^1$ is non-negative and thus $\vf{\pi^{\alpha^{(k)}}, f^1}_{P, 1}(s_1) \geq 0$.

\looseness=-1
On the other hand, using \Cref{lemma:delta-bound}, we have 
\begin{align*}
f^2_h(s, a) 
= \delta^{\pi^{(k)}, r}_{(k), h} - \widebar{\lambda}^{(k, T)}\delta^{\pi^{(k)}, u}_{(k), h}
\numeq{\geq}{a}
- \widebar{\lambda}^{(k, T)} 2\cp \beta^{(k)}_h
\end{align*}
Therefore, it holds that 
\begin{align*}
\vf{\pi^{\alpha^{(k)}}, f^2}_{P, 1}(s_1) 
+ \widebar{\lambda}^{(k, T)}\vf{\pi^{\alpha^{(k)}}, 2\cp \beta^{(k)}}_{P, 1}(s_1) 
\geq 0\;.
\end{align*}

\looseness=-1
By combining all the results, we have \(\circled{3} \leq 0\).
\end{proof}

\subsubsection{Bounds for Bisection Search}

\looseness=-1
Using \Cref{lemma:mixture-feasibility}, $\circled{5}$ is further bounded by 
\begin{align*}
\circled{5} &= \sum_{k\in \unconfMDP^c}
\widebar{\lambda}^{(k, T)}
\paren*{
\pvf{\pi^{(k)}, u}_{(k), 1}(s_1) - \vf{\pi^{\alpha^{(k)}}, u - 2\cp\beta^{(k)}}_{P, 1}(s_1)
}\\
& \leq 
\sum_{k\in \unconfMDP^c}
\widebar{\lambda}^{(k, T)}
\paren*{
\pvf{\pi^{(k)}, u}_{(k), 1}(s_1) - b
}
\leq 
C_\lambda
\sum_{k\in \unconfMDP^c}
\paren*{
\pvf{\pi^{(k)}, u}_{(k), 1}(s_1) - b
}
\;.
\end{align*}
We bound the last term using the bisection search in \Cref{algo:zero-vio-linear MDP}.
Note that we focus only the case $\pvf{\pi^{(k), 0}, u}_{(k), 1}(s_1) < b$ and $\pvf{\pi^{(k), C_\lambda}, u}_{(k), 1}(s_1) \geq b$ due to \Cref{line:pizero-deploy} and \Cref{line:pisafe-deploy} in \Cref{algo:zero-vio-linear MDP}. 
Due to the definitions of $\widebar{\lambda}^{(k, t)}$ and $\underline{\lambda}^{(k, t)}$ in \Cref{algo:zero-vio-linear MDP}, 
$$
\pvf{\pi^{(k), \underline{\lambda}^{(k, t)}}, u}_{(k), 1}(s_1) < b
  \;\text{ and }\;
\pvf{\pi^{(k), \widebar{\lambda}^{(k, t)}}, u}_{(k), 1}(s_1) \geq b\;
$$
hold for any $t \in \bbrack{1, T}$. 
Therefore, 
\begin{align*}
\circled{5}
\leq &C_\lambda \sum_{k \in \unconfMDP^c} 
\paren*{
\pvf{\pi^{(k), \widebar{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1) - 
\pvf{\pi^{(k), \underline{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1)
}
\end{align*}
To bound the right-hand side, we derive the sensitivity of $\vf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1)$ with respect to $\lambda$.

\begin{lemma}[Restatement of \Cref{lemma:lambda-sensitive-main}]\label{lemma:lambda-sensitive}
Let $X \df K \paren*{1 + \frac{8(1+C_\lambda)(H_\kappa + B_\dagger H + H)}{\kappa}}$ 
and 
$Y \df \frac{8(H_\kappa + B_\dagger H + H)}{\kappa}$.
For any $k$ and $\lambda \in [0, C_\lambda]$, it holds that
\begin{align*}
\abs*{\pvf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1)
- \pvf{\pi^{(k), \lambda + \varepsilon}, u}_{(k), 1}(s_1)}
\leq X^H H^2 Y \varepsilon\;.
\end{align*}   
\end{lemma}
\begin{proof}
\looseness=-1
The proof is based on \textbf{Lemma 2} from \citet{ghosh2024towards}.
For notational simplicity, we denote $\pi \df \pi^{(k), \lambda}$ and $\pi' \df \pi^{(k), \lambda + \varepsilon}$.
Additionally, we use shorthand:
\begin{align*}
&v_h^r \df \norm*{\ovf{\pi, r}_{(k), h}[\kappa] - \ovf{\pi', r}_{(k), h}[\kappa]}_\infty\;,
&&q_h^r \df \norm*{\oqf{\pi, r}_{(k), h}[\kappa] - \oqf{\pi', r}_{(k), h}[\kappa]}_\infty\;,
\\
&v_h^\dagger \df \norm*{\ovf{\pi, \dagger}_{(k), h} - \ovf{\pi', \dagger}_{(k), h}}_\infty\;,
&&q_h^\dagger \df \norm*{\oqf{\pi, \dagger}_{(k), h} - \oqf{\pi', \dagger}_{(k), h}}_\infty\;,\\
&v_h^u \df \norm*{\pvf{\pi, u}_{(k), h} - \pvf{\pi', u}_{(k), h}}_\infty \;,
&&q_h^u \df \norm*{\pqf{\pi, u}_{(k), h} - \pqf{\pi', u}_{(k), h}}_\infty \;.
\end{align*}

\looseness=-1
For any $h$, we have
\begin{align*}
&v_h^r = \norm*{ \pi_h\oqf{\pi, r}_{(k), h}[\kappa] - \pi'_h \oqf{\pi', r}_{(k), h}[\kappa]}_\infty \leq H_\kappa\norm*{\pi_h - \pi'_h}_1 + q^r_h \\
&v_h^\dagger \leq B_\dagger H\norm*{\pi_h - \pi'_h}_1 + q_h^\dagger\\
&v_h^u \leq H\norm*{\pi_h - \pi'_h}_1 + q_h^u\;.
\end{align*}

\looseness=-1
Since $\pi_h$ and $\pi'_h$ are softmax policies, using \Cref{lemma:softmax-policy-bound},
\begin{align*}
\norm*{\pi_h - \pi'_h}_1
&\leq 
\frac{8}{\kappa}\norm*{
\oqf{\pi, \dagger}_{(k), h} 
+ \oqf{\pi, r}_{(k), h}[\kappa] 
+ \lambda \pqf{\pi, u}_{(k), h}
- \oqf{\pi', \dagger}_{(k), h}
-\oqf{\pi', r}_{(k), h}[\kappa]
- (\lambda+\varepsilon) \pqf{\pi', u}_{(k), h}}_\infty\\
&\leq 
\frac{8}{\kappa}
\paren*{
q_h^\dagger
+ q_h^r
+C_\lambda q_h^u
+ \varepsilon H
}
\end{align*}

\looseness=-1
Additionally, 
\begin{align*}
&q_h^r \leq \norm*{ \hP^{(k)}_h \paren*{\ovf{\pi, r}_{(k), h+1}[\kappa]  - \ovf{\pi', r}_{(k), h+1}[\kappa]} }_\infty \leq K v_{h+1}^r\\
&q_h^\dagger \leq \norm*{ \hP^{(k)}_h \paren*{\ovf{\pi, \dagger}_{(k), h+1}  - \ovf{\pi', \dagger}_{(k), h+1}} }_\infty \leq K v_{h+1}^\dagger\\
&q_h^u \leq \norm*{ \hP^{(k)}_h \paren*{\pvf{\pi, u}_{(k), h+1}  - \pvf{\pi', u}_{(k), h+1}} }_\infty \leq K v_{h+1}^u\;,
\end{align*}
where we used the fact that, for any $V: \S \to \R$,
\begin{align*}
\abs*{\hP^{(k)}_h V}(s, a)
&= 
\abs*{{\bphi(s, a)^{\top}\paren{\bLambda^{(k)}_h}^{-1} \sum_{i=1}^{k-1}\bphi\paren{s_h^{(i)}, a_h^{(i)}} V\paren{s_{h+1}^{(i)}}}}\\
&\leq \norm*{{\paren{\bLambda^{(k)}_h}^{-1} \sum_{i=1}^{k-1}\bphi\paren{s_h^{(i)}, a_h^{(i)}} }}_2 \norm*{V}_\infty
\leq K \norm*{V}_\infty\;.
\end{align*}

\looseness=-1
By combining all the results, 
\begin{align*}
&v_h^r \leq 
K\paren*{\frac{8H_\kappa}{\kappa} + 1} v_{h+1}^r
&&+K\frac{8H_\kappa}{\kappa} v_{h+1}^\dagger
&&&+ K\frac{8H_\kappa C_\lambda}{\kappa} v_{h+1}^u
&&&&+ \frac{8H_\kappa}{\kappa} \varepsilon H\\
&v_h^\dagger \leq 
K\frac{8B_\dagger H}{\kappa} v_{h+1}^r
&&+K\paren*{\frac{8B_\dagger H}{\kappa} + 1}  v_{h+1}^\dagger
&&&+ K\frac{8B_\dagger H C_\lambda}{\kappa} v_{h+1}^u
&&&&+ \frac{8B_\dagger H}{\kappa} \varepsilon H\\
&v_h^u \leq 
K\frac{8 H}{\kappa} v_{h+1}^r
&&+K\frac{8 H}{\kappa}  v_{h+1}^\dagger
&&&+ K\paren*{\frac{8H}{\kappa} + 1}C_\lambda v_{h+1}^u
&&&&+ \frac{8H}{\kappa} \varepsilon H\;.
\end{align*}
Let 
$X \df K \paren*{1 + \frac{8(1+C_\lambda)(H_\kappa + B_\dagger H + H)}{\kappa}}$ 
and 
$Y \df \frac{8(H_\kappa + B_\dagger H + H)}{\kappa}$.
Then,
\begin{align*}
v_h^r + v_h^\dagger + v_h^u 
&\leq  X(v_{h+1}^r + v_{h+1}^\dagger + v_{h+1}^u) + Y H \varepsilon\\
&\leq  X^2(v_{h+2}^r + v_{h+2}^\dagger + v_{h+2}^u) + XY H\varepsilon + Y H \varepsilon \\
&\leq \dots\\
&\leq \paren*{X^H+ \dots + X+ 1}Y H \varepsilon \;. 
\end{align*}
\end{proof}

\looseness=-1
We are now ready to bound $\circled{5}$
Applying \Cref{lemma:lambda-sensitive} to $\circled{5}$, we obtain the following lemma.
\begin{lemma}\label{lemma:binary-search-bound}
\looseness=-1
When $T = \tiO(H)$, it holds that 
$$\circled{5} \leq 
C_\lambda \sum_{k \in \unconfMDP^c} 
\paren*{
\pvf{\pi^{(k), \widebar{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1) - 
\pvf{\pi^{(k), \underline{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1)
}
\leq \tiO(1)\;.$$
\end{lemma}
\begin{proof}
\looseness=-1
Due to the bisection search update rule, $\widebar{\lambda}^{(k, T)} - \underline{\lambda}^{(k, T)} = 2^{-T}$.
Thus, 
\begin{align*}
\circled{5} 
&\leq 
C_\lambda \sum_{k \in \unconfMDP^c} 
\paren*{
\pvf{\pi^{(k), \widebar{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1) - 
\pvf{\pi^{(k), \underline{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1)
}
\leq
X^H C_\lambda K H^2 Y 2^{-T}
\end{align*}    
where the inequality uses \Cref{lemma:lambda-sensitive} with $X$ and $Y$ defined in \Cref{lemma:lambda-sensitive}.
Thus, $\circled{5}\leq \tiO\paren*{1}$ holds by setting $T = H \polylog(X, H, Y)$. 
This concludes the proof.
\end{proof}

\looseness=-1
We are now ready to prove \Cref{theorem:MDP-regret-main}.
The proof is under the parameters of: 
$\rho = 1$,
$\co=\tiO(dH)$, 
$\cp=\tiO(dH)$,
$C_\dagger=\tiO(d^2H^3\bslt^{-1})$,
$B_\dagger = \tiO\paren*{dH^2\bslt^{-1}}$,
$\kappa = \widetilde{\Omega}\paren*{\bslt^3 H^{-4}d^{-1}K^{-0.5}}$, 
$T=\tiO(H)$, and
$C_\lambda = \tiO\paren*{dH^4\bslt^{-2}}$.

\subsubsection{Proof of \Cref{theorem:MDP-regret-main}}

\looseness=-1
We condition the proof with the good events $\rmvEevent \cap \confevent$, which holds with probability at least $1-3\delta$ by \Cref{lemma:good-event1-MDP,,lemma:good-event2-MDP}.

\looseness=-1
In \Cref{algo:zero-vio-linear MDP}, the deployed policy switches between $\pisafe \in \Pisafe$ and the softmax policies.
Since \Cref{algo:zero-vio-linear MDP} deploys the softmax policies only when $\pvf{\pi^{(k), 0}, u}_{(k), 1}(s_1) \geq b$, due to \Cref{lemma:delta-bound} and the good events, all the deployed policies satisfy $\pi^{(k)} \in \Pisafe$ for all $k \in \bbrack{1, K}$.
This concludes the proof of the zero-violation guarantee.

\looseness=-1
Next, we derive the regret bound.
Recall from \Cref{eq:regret-decomposition} that
\begin{align*}
\regret(K) \leq     
\tiO\paren*{d^3H^4 \bslt^{-2}}
+ 
\circled{1}
+
\circled{2}
+ \kappa KH \ln A
\leq     
\tiO\paren*{d^3H^4 \bslt^{-2}}
+ 
\circled{1}
+
\circled{2}
+ \tiO(\sqrt{K})\;,
\end{align*}
where the second inequality is due to the value of $\kappa$.

\looseness=-1
Using \Cref{eq:martingale-bound}, 
$$
\circled{1} \leq 
\tiO\paren*{H^{2} d^{3/2}\sqrt{K}}.
$$

\looseness=-1
Using \Cref{eq:optimism-decomposition}, $\circled{2}$ can be decomposed as:
\begin{align*}
\circled{2} &\leq \circled{3} + \circled{4} + \circled{5}\;.
\end{align*}
Each term can be bounded as:
\begin{itemize}
\item $\circled{3} \leq 0$ by \Cref{lemma:optimism-composite}
\item $\circled{4} \leq \tiO\paren*{H^{4} d^{5/2} \bslt^{-1}\sqrt{K}}$ by \Cref{eq:additional bonus bound},
\item $\circled{5} \leq \tiO(1)$ by \Cref{lemma:binary-search-bound}
\end{itemize}

\looseness=-1
Finally, by combining all the results, we have 
\begin{align*}
\regret(K) &\leq 
\tiO\paren*{d^3H^4 \bslt^{-2}}
+ 
\tiO\paren*{H^{2} d^{3/2}\sqrt{K}}
+ 
\tiO\paren*{H^{4} d^{5/2} \bslt^{-1}\sqrt{K}}\;.
\end{align*}
This concludes the proof of the sublinear regret guarantee.
