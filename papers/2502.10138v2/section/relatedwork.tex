\section{Related Work}\label{sec:related work}

\subsection{Related Algorithms}
\looseness=-1
Building on the seminal work of \citet{efroni2020exploration}, numerous safe RL algorithms for CMDPs have been developed, broadly categorized into linear programming (LP) approaches and Lagrangian-based approaches.

\looseness=-1
\paragraph{Linear programming.}
LP approaches formulate CMDPs as linear optimization problems using an estimated transition kernel \citep{altman1999constrained}.
\citet{efroni2020exploration} introduced a basic sublinear regret algorithm, while \citet{hasanzadezonuzy2021learning} provided $(\varepsilon,\delta)$-PAC guarantees, ensuring the algorithm outputs a near-optimal policy.
However, these methods permit constraint violations during exploration, making them unsuitable for safety-critical applications.
\citet{liu2021learning} and \citet{bura2022dope} developed LP-based algorithms that achieve sublinear regret while maintaining episode-wise zero-violation guarantees. 
The key is to incorporate optimistic-pessimistic value estimation into the LP formulation.

\looseness=-1
LP-based approaches in tabular settings, however, suffer from computational costs that scale with the size of the state space, making them impractical for linear CMDPs. 
While several studies propose LP algorithms for linear MDPs \citep{neu2020unifying,bas2021logistic,neu2023efficient,lakshminarayanan2017linearly,gabbianelli2024offline}, these methods either use occupancy measures as decision variables---which can be exponentially large for large state spaces---or require a set of feature vectors that sufficiently cover the state space, which may not be feasible in our exploration settings.
Moreover, as described in \Cref{subsec:technical-challenge-MDP}, the estimated value functions in linear CMDPs with exploration require clipping operators, further complicating the use of occupancy-measure-based approaches like LP methods in our setting.

\looseness=-1
\paragraph{Lagrangian approach.}
Lagrangian approaches reformulate the constrained optimization\\ $\max_\pi \brace*{f(\pi) \given h(\pi) \geq 0}$ as a min-max optimization $\min_{\lambda \geq 0} \max_{\pi}\brace*{f(\pi) + \lambda h(\pi)}$, and simultaneously optimize both $\pi$ and $\lambda$.
When an algorithm gradually updates \(\pi\) and then adjusts \(\lambda\) incrementally, it is referred to as a \textbf{primal-dual (PD)} algorithm \citep{ding2023last}.  
In contrast, if \(\lambda\) is updated only after fully optimizing \(\pi\) in the inner maximization, it is known as a \textbf{dual} approach \citep{ying2022dual}.
Since the inner maximization reduces to standard policy optimization, Lagrangian methods integrate naturally with scalable methods such as policy gradient and value iteration.

\looseness=-1
For the tabular settings, \citet{wei2021provably,muller2024truly} develop model-free primal-dual algorithms with sublinear regret, while \citet{wei2022provably} extends this approach to the average-reward setting. 
\citet{zeng2022finite,kitamura2024policy} propose \((\varepsilon,\delta)\)-PAC primal-dual algorithms, and \citet{vaswani2022near} achieved the PAC guarantee via dual approach.

\looseness=-1
Beyond tabular settings, \citet{ding2021provably} propose PD algorithms with linear function approximation, achieving sublinear regret guarantees.
\citet{gosh2023achiving} extend this to the average-reward linear CMDPs.
\citet{ghosh2022provably} take a dual approach, also attaining sublinear regret in the finite-horizon settings.

\looseness=-1
These PD and dual algorithms, however, do not ensure episode-wise zero violation.
Intuitively, the key issue lies in their $\lambda$-adjustment strategy, which updates $\lambda$ only incrementally.
For example, the basic PD and dual algorithms by \citet{efroni2020exploration} updates $\lambda$ using $\lambda^{(k+1)} \leftarrow \lambda^{(k)} + \alpha \cdot [\text{violation}]$, where $\alpha$ is a small learning rate.
Since $\lambda$ controls constraint satisfaction, if the current policy fails to satisfy constraints adequately, $\lambda$ should be increased sufficiently before the next policy deployment.

\looseness=-1
Following this principle, \citet{ghosh2024towards} propose a dual approach that searches for an appropriate $\lambda$ within each episode, leading to a tighter violation regret guarantee than \citet{ghosh2022provably}.
However, due to the lack of pessimistic constraint estimation, their method does not ensure episode-wise safety and allows constraint violations.
Like \citet{ghosh2024towards}, our \MDPalgo searches for the best $\lambda$ in each episode.
However, unlike their approach, \MDPalgo controls $\lambda$ with pessimism, ensuring zero violation, and guarantees the existence of a feasible $\lambda$ by deploying a sufficient number of $\pisafe$.

\subsection{Related Safety Types}

\paragraph{Instantaneous safety.}
\looseness=-1  
Unlike our episode-wise safety, instantaneous safety defines exploration as safe if it satisfies \( u_h(s^{(k)}_h, a^{(k)}_h) \geq b \) for all \( h \) and \( k \) \citep{pacchiano2021stochastic, pacchiano2024contextual, hutchinson2024directional, shi2023near, amani2021safe}.  
In other words, states and actions must belong to predefined safe sets, \( \mathcal{S}_\safe \times \mathcal{A}_\safe \).  
Instantaneous safety is a special case of the episode-wise constraint. Indeed, by defining \( u_h(s, a) = -\mathbb{I}\{(s, a) \in \mathcal{S}_\safe \times \mathcal{A}_\safe\} \) and setting \( b = 0 \), an episode-wise safe algorithm safeties the instantaneous constraint for all \( h \) and \( k \).

\paragraph{Cancel Safety.}
Cancel safety is another common safety measure in CMDP literature \citet{wei2021provably,ghosh2022provably}.
It allows a strict constraint satisfaction in one episode to compensate for a violation in another. 
Formally, cancel safety ensures that the following cumulative \textbf{cancel violation regret} remains non-positive:
$$\violation_{\mathrm{cancel}}(K) \df \sum^K_{k=1} b - \vf{\pi^{(k)}, u}_{P, 1}(s_1)\;.$$
Note that the ``hard'' violation regret $\violation_{\mathrm{hard}}(K) \df \sum^K_{k=1} \max\brace*{b - \vf{\pi^{(k)}, u}_{P, 1}(s_1), 0}$ which considers violations in each individual episode \citep{ghosh2024towards, efroni2020exploration, muller2024truly}, always upper-bounds the cancel regret. This means cancel regret is a weaker measure.
Since episode-wise safety ensures $\violation_{\mathrm{hard}}=0$, our \MDPalgo always satisfies cancel safety, but cancel safety does not necessarily guarantee episode-wise safety.



