\section{Introduction}

\looseness=-1
Safe decision-making is essential in real-world applications such as autonomous driving, plant control, and finance \citep{gu2022review}. 
The constrained Markov decision process (CMDP) provides a powerful mathematical framework for developing decision-making algorithms with formal safety guarantees \citep{altman1999constrained}. 
This paper studies the reinforcement learning (RL) problem in finite-horizon CMDPs, where an agent seeks to maximize the expected cumulative rewards while satisfying a single constraint on the expected total utility value.
Since the system dynamics are unknown, the agent must explore the environment to gather information while ensuring constraint satisfaction.

\looseness=-1
The safe RL problem has been extensively studied in the tabular CMDP literature.
The seminal work by \citet{efroni2020exploration} achieves sublinear regret {\em but allows constraint violations}, making it unsuitable for safety-critical settings.
Subsequent works \citep{liu2021learning,bura2022dope} achieve \textbf{episode-wise} zero-violation RL with $\tiO(\sqrt{K})$ regret for $K$ number of episodes, ensuring cumulative utility constraint satisfaction in every episode.
Their approach consists of two phases: 
deploying a known strictly safe policy $\pisafe$ and then updating policies via linear programs (LPs) that optimize an optimistically estimated objective while satisfying a pessimistic constraint.
Deploying $\pisafe$ is necessary to guarantee feasible solutions for the LPs once enough environmental information is collected.

\begin{table*}[tb]
\small
\caption{
\looseness=-1
\small
Comparison of tabular/linear CMDP results. 
$|\S|$, $|\A|$, $d$, $H$, $K$, and $\bslt$ denote state space size, action space size, feature dimension, episode horizon, number of episodes, and a safety-related parameter, respectively (see \Cref{sec:MDP} for details).
}\label{table:algorithms}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
&Paper & Epi.-Wise Safe? & Comp. Efficient? & Regret \\\hline 
\multirow{2}{*}{Tabular} 
& \citet{liu2021learning} & Yes & $|\S|$ dependent & $\tiO\paren{\bslt^{-1} \sqrt{|\S|^3 |\A| H^6 K}}$  \\ 
& \citet{bura2022dope} & Yes & $|\S|$ dependent & $\tiO\paren{\bslt^{-1} \sqrt{|\S|^2 |\A| H^6 K}}$  \\ \hline
\multirow{6}{*}{Linear} 
& \citet{amani2021safe} & Instantaneous & Yes & $\tiO\paren{\sqrt{d^3 H^4 K}}$ \\ 
& \citet{ghosh2022provably} & No & Yes & $\tiO\paren{\sqrt{d^3 H^3 K}}$  \\ 
& \citet{yang2022reduction} & No & N/A & $\tiO\paren{\sqrt{d^2 H^3 K}}$  \\ 
& \citet{ghosh2024towards} & No & No & $\tiO\paren{\sqrt{d^3 H^4 K}}$  \\ 
& \citet{wei2024safe} & No & Yes & $\tiO\paren{\sqrt{d^3 H^4 K}}$  \\ 
\rowcolor{LightGray} &\textbf{\MDPalgo (Ours)} & \textbf{Yes} & \textbf{Yes} & $\tiO\paren{\bslt^{-1} \sqrt{d^5 H^8 K}}$\\
\hline
\end{tabular}
\end{table*}


\looseness=-1
While efficient exploration under safety is well-established in tabular CMDPs, extending it to large-scale CMDPs with function approximation remains a major challenge.
LP-based methods are impractical for large-scale problems due to their state-dependent computational cost.\footnote{While some literature proposed LP methods for unconstrained linear MDPs (e.g., \citet{neu2023efficient}), they remain unsuitable for our exploration setting or still incur state-dependent computational costs.
See \Cref{sec:related work} for details.} 
As a result, even in linear CMDPs, where value functions have a linear structure, episode-wise safe RL remains unresolved. 
\citet{ghosh2022provably,ghosh2024towards,yang2022reduction,wei2024safe} propose linear CMDP algorithms but allow constraint violations in each episode.
Worse still, the state-of-the-art linear CMDP algorithm \citep{ghosh2024towards}, which achieves the best $\tiO(\sqrt{K})$ violation regret,\footnote{Violation regret denotes the total amount of constraint violation during exploration.} suffers from an exponential computational cost of $K^H$, where $H$ is the horizon length.
\citet{amani2021safe} achieve safe RL but focuses only on instantaneous constraints,\footnote{Using notations from \Cref{sec:MDP}, instantaneous constraint ensures $u_h(s^{(k)}_h, a^{(k)}_h) \geq b$ for every $h, k \in \bbrack{1,H}\times\bbrack{1,K}$.} a special subclass of the episode-wise constraint that can be overly conservative (e.g., in drone control, temporary high energy consumption is tolerable, but full battery depletion is not).
\Cref{table:algorithms} summarizes representative algorithms, with additional related work in \Cref{sec:related work}.
In short, a fundamental open question remains:
\begin{center}
\emph{Can we develop a computationally efficient\footnote{An algorithm is comp. efficient if its cost scales polynomially with problem parameters, excluding state space size.} linear CMDP algorithm with \\sublinear regret and zero episode-wise constraint violation?}
\end{center}

\looseness=-1
\paragraph{Contributions.}
We propose \textbf{O}ptimistic-\textbf{P}essimistic \textbf{S}oftmax \textbf{E}xploration for \textbf{L}inear \textbf{CMDP} (\MDPalgo), the first algorithm for linear CMDPs that achieves \textbf{\(\boldsymbol{\tiO\paren{\sqrt{K}}}\)-regret and episode-wise safety}.
Our approach builds on the optimistic-pessimistic exploration framework with two key innovations for large-scale state-space problems:
($\mathrm{i}$) a new \textbf{deployment rule for $\boldsymbol{\pisafe}$},
and ($\mathrm{ii}$) a \textbf{computationally efficient} method to implement optimism for the objective and pessimism for the constraint within the softmax policy framework \citep{ghosh2022provably,ghosh2024towards}.

\looseness=-1
\Cref{sec:zero-vio bandit} first analyzes the linear constrained bandit problem as a ``warm-up'' for linear CMDPs ($H=1$ with an instantaneous constraint), highlighting the key role of the $\pisafe$ deployment rule in bounding its usage and avoiding linear regret.
When the constraint is instantaneous, prior work limits $\pisafe$ deployments by assigning a vector representation to the safe action $\basafe \in \R^d$ \citep{pacchiano2021stochastic,pacchiano2024contextual,hutchinson2024directional,amani2019linear,amani2021safe}. 
However, extending this approach to episode-wise safety is non-trivial, as the constraint is imposed on policies rather than actions, and policies may be nonlinear functions (e.g., softmax mapping from value functions) rather than single vectors.
We overcome this challenge by showing that \textbf{if $\boldsymbol{\pisafe}$ is deployed only when the agent is less confident in $\boldsymbol{\pisafe}$'s safety}, the number of deployments is logarithmically bounded (\Cref{lemma:Ck-bound-main}).


\looseness=-1  
\Cref{sec:MDP} then extends the bandit result to RL in CMDPs.
To enable optimistic-pessimistic exploration in linear CMDPs, \MDPalgo employs the \textbf{composite softmax policy} (\Cref{def:composite-softmax-policy}), which adjusts optimism and pessimism by controlling a variable \(\lambda\).  
\MDPalgo efficiently searches for the best \(\lambda\) through \textbf{bisection search}, achieving a \textbf{polynomial computational cost} in problem parameters, independent of state-space cardinality (\Cref{remark:computational cost}). 
Overall, our techniques---the novel $\pisafe$ deployment rule and softmax-based optimistic-pessimistic exploration---achieve the first episode-wise safe RL with sublinear regret and computational efficiency in linear CMDPs.


