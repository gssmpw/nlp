\section{Warm Up: Safe Exploration in Linear Constrained Bandit}\label{sec:zero-vio bandit}

\looseness=-1
To better illustrate the core ideas of our CMDP algorithm, this section introduces its variant for a contextual linear bandit problem.
All the formal theorems and proofs in this section are provided in \Cref{sec:regret-analysis-bandit}.
Let $\A \subset \R^d$ be the action space, a compact set of bounded $d$-dimensional vectors. 
Without loss of generality, we assume $\norm{\ba}_2 \leq 1$ for any $\ba \in \A$.
At each round $k$, the agent selects a policy $\pi^{(k)} \in \sP\paren*{\A}$ to sample an action $\ba^{(k)} \sim \pi^{(k)}$, and then it observes the reward $r^{(k)} = \btheta_r^\top \ba^{(k)}+ \varepsilon^{(k)}_r$ and utility $u^{(k)} = \btheta_u^\top \ba^{(k)}+ \varepsilon^{(k)}_u$.
Here, $\btheta_r, \btheta_u\in \R^d$ are vectors unknown to the agent such that \(\norm{\btheta_r}_2, \norm{\btheta_u}_2 \leq B\) for some $B > 0$, and $\varepsilon^{(k)}_r, \varepsilon^{(k)}_u$ are $R$-sub-Gaussian random noises that satisfy:
$\mathbb{E}\brack{e^{\alpha \varepsilon^{(k)}_g} \given \cF^{(k-1)}} \leq \exp \left(\alpha^2 R^2 / 2\right)$ for all $k \in \mathbb{N}$, $g \in \brace{r, u}$, and $\alpha \in \mathbb{R}$,
where $\cF^{(k)} \df \sigma\paren{\ba^{(1)}, \dots, \ba^{(k+1)}, 
\brace{\varepsilon^{(1)}_g}_{g\in \brace{r, u}},
\dots,
\brace{\varepsilon^{(k)}_g}_{g\in \brace{r,u}}
}$ is the $\sigma$-algebra generated by the interaction.
For any policy $\pi$ and $g \in \brace{r, u}$, let $g_{\pi} \df \E_{\ba \sim \pi}\brack*{\left\langle\btheta_g, \ba\right\rangle}$ be the expected reward and utility.
We consider a constraint such that the expected utility must be above the threshold $b \in \R$. 
Formally, let $\Pisafe \df \brace*{\pi \given u_{\pi} \geq b}$ denote the set of safe policies. 
The agent's goal is to achieve sublinear regret while satisfying the \textbf{instantaneous} constraints defined as follows:
\begin{align}\label{eq:const-bandit}
{\textstyle
\regret (K) \df 
\sum^K_{k=1} 
r_{\pi^\star} - r_{\pi^{(k)}}
= o\paren*{K}
\;\text{ such that }\;
{\pi^{(k)}} \in \Pisafe \; \forall k \in \bbrack{1, K}\;,
}
\end{align}
where $\pi^\star \in \argmax_{\pi \in \Pisafe} r_{\pi}$.
A sublinear regret exploration is efficient, as its averaged reward approaches the optimal value, i.e.,  
\(\lim_{K\to \infty }\frac{1}{K}r_{\pi^{(K)}} \to  r_{\pi^\star}\).
Finally, we assume access to a strictly safe policy in $\Pisafe$, as deploying arbitrary policies without this assumption risks violating constraints.
\begin{assumption}[Safe policy]\label{assumption:slater-bandit}
We have access to $\pisafe \in \Pisafe$ and $\bslt > 0$ such that $u_{\pisafe} - b \geq \bslt\;$\footnote{The knowledge of $\bslt$ is for simplicity. If unknown, we can estimate it by deploying $\pisafe$ and it requires a little overhead.}.
\end{assumption}

\subsection{Technical Challenge: Zero-Violation with a Safe Policy}\label{subsec:bandit-challenge}

\looseness=-1
The key to efficient and safe exploration is the \textbf{optimistic-pessimistic} exploration, which constructs an optimistic reward $\widebar{r}^{(k)}_\pi \geq r_\pi$ and a pessimistic utility $\underline{u}^{(k)}_\pi \leq u_\pi$, and then computes a policy by:
\begin{align}\label{eq:optimistic-pessimistic-bandit}
\max_{\pi \in \sP(\A)} 
\widebar{r}^{(k)}_\pi
\quad \text{ such that }\quad \underline{u}_\pi \geq b\;.
\end{align}
$\widebar{r}^{(k)}_\pi$ and $\underline{u}^{(k)}_\pi$ are designed to converge sufficiently quickly to $r_\pi$ and $u_\pi$ as more data is collected, enabling efficient exploration while satisfying the constraint \citep{abbasi2011improved}.
However, although \Cref{eq:optimistic-pessimistic-bandit} is expected to have feasible solutions when $\underline{u}_\pi \approx u_\pi$, the pessimistic constraint may not have any feasible solution in the early stages of exploration.

\looseness=-1
To ensure that \Cref{eq:optimistic-pessimistic-bandit} always has a solution, a common bandit approach assumes access to a safe action $\basafe \in \A$ such that $\btheta_u^\top \basafe \geq b + \bslt$, and then ensures the feasibility of \Cref{eq:optimistic-pessimistic-bandit} by leveraging the \textbf{vector representation} of $\basafe \in \R^d$. 
For example, \citet{pacchiano2021stochastic,pacchiano2024contextual,amani2019linear} designed $\underline{u}^{(k)}_\pi$ using the orthogonal direction $\paren*{\basafe}^\bot \df \basafe - \basafe / \norm*{\basafe}_2$, while \citet{hutchinson2024directional} assume $\basafe = \bzero \in \A$ with a negative constraint threshold $b < 0$. 
Both approaches ensure that a policy playing $\basafe$ with probability $1$ is always feasible in \Cref{eq:optimistic-pessimistic-bandit}.

\looseness=-1
However, extending this safe action technique to our goal of ``episode-wise safe RL'' is non-trivial, as the episode-wise constraint is imposed on policies rather than actions, and policies in linear CMDPs may be nonlinear functions (e.g., softmax mappings from value functions) rather than single vectors.
To address this challenge, we first develop a new bandit algorithm that ensures the feasibility of \Cref{eq:optimistic-pessimistic-bandit} without relying on safe action techniques.


\begin{algorithm2e}[t!]
\caption{Optimistic-Pessimistic Linear Bandit with Safe Policy}
\label{algo:zero-vio-bandit}
\DontPrintSemicolon
\LinesNumbered
    \KwIn{\small Regression coefficient $\rho = 1$, bonus scalers $\cp = B + R\sqrt{d \ln 4K\delta^{-1}}$ and $\co = \cp \paren{1 + 2B\bslt^{-1}}$, safe policy $\pisafe$, and iteration length $K \in \N$}
    \For{$k = 1, \dots, K$}{
    Let \(\beta^{(k)}_\pi\), \(\widehat{r}^{(k)}_\pi\), and \(\widehat{u}^{(k)}_\pi\) be bonus, estimated reward and utility, respectively (see \Cref{sec:bandit-algo-analysis})\;
    {\color{blue}\tcc{Switch policy based on the confidence level of $\pisafe$}}
    \lIf{\(\cp \beta^{(k)}_{\pisafe} > \frac{\bslt}{2}\)}{
    \(\pi^{(k)}\df \pisafe\)}\label{line:safe-policy-bandit}
    \lElse{
    \(\pi^{(k)} \in 
    \argmax_{\pi \in \sP(\A)}\; \widehat{r}^{(k)}_\pi + \co \beta^{(k)}_\pi \; \text { such that }\; \widehat{u}^{(k)}_\pi - \cp \beta^{(k)}_\pi \geq b \)
    }\label{line:op-ps-policy-bandit}
    Sample an action $\ba^{(k)} \sim \pi^{(k)}$ and observe reward $r^{(k)}$ and utility $u^{(k)}$.
    }
\end{algorithm2e}

\subsection{Algorithm and Analysis}\label{sec:bandit-algo-analysis}

\looseness=-1
We summarize the proposed \textbf{O}ptimistic-\textbf{P}essimistic \textbf{L}inear \textbf{B}andit with \textbf{S}afe \textbf{P}olicy (\Banditalgo) in \Cref{algo:zero-vio-bandit}, which follows the standard linear bandit framework (see \citet{abbasi2011improved}).
Throughout this section, we analyze \Cref{algo:zero-vio-bandit} under the parameters listed in its \hyperref[algo:zero-vio-bandit]{\textbf{Input}} line.
Let $\hbtheta^{(k)}_r \df \paren{\bLambda^{(k)}}^{-1}\sum_{i=1}^{k-1} \ba^{(i)} r^{(i)}$ and $\hbtheta^{(k)}_u \df \paren{\bLambda^{(k)}}^{-1}\sum_{i=1}^{k-1} \ba^{(i)} u^{(i)} $ denote the regularized least-squares estimates of $\btheta_r$ and $\btheta_u$, respectively.
Let \(\widehat{r}^{(k)}_\pi\df \E_{\ba \sim \pi}\brack{\ba^\top \hbtheta_r^{(k)}}\) and \(\widehat{u}^{(k)}_\pi \df \E_{\ba \sim \pi}\brack{\ba^\top \hbtheta_u^{(k)}}\) be the estimated reward and utility functions.
Using the bonus function 
\(\beta^{(k)}_\pi \df \E_{\ba \sim \pi}\norm*{\ba}_{\paren*{\bLambda^{(k)}}^{-1}}\) where \(\bLambda^{(k)} \df \rho \bI + \sum_{i=1}^{k-1} \ba^{(i)} \paren{\ba^{(i)}}^\top \), with the well-established elliptical confidence bound argument for linear bandits \citep{abbasi2011improved}, the following confidence bounds hold:
\begin{lemma}[Confidence bounds]\label{lemma:bandit-opt-pes-main}
For any $\pi$ and $k$, with probability (w.p.) at least $1-\delta$,
$$
r_{\pi} + 2\co \beta^{(k)}_{\pi} \geq 
\widehat{r}_{\pi}^{(k)} + \co \beta^{(k)}_{\pi} \geq r_{\pi} \quad \text{ and }\quad
u_{\pi} \geq \widehat{u}_{\pi}^{(k)} - \cp \beta^{(k)}_{\pi}
\geq u_{\pi} - 2\cp \beta^{(k)}_{\pi}\;.
$$
\end{lemma}
\Cref{algo:zero-vio-bandit} updates policies by solving the following optimistic-pessimistic (\optpes) problem:
\begin{align}\label{eq:op-ps-bandit-propose}
\textbf{Opt-Pes (\Cref{line:op-ps-policy-bandit})}
\quad
\pi^{(k)} \in 
\argmax_{\pi \in \sP(\A)}\; \underbrace{\widehat{r}^{(k)}_\pi + \co \beta^{(k)}_\pi}_{\geq r_\pi \text{ by \Cref{lemma:bandit-opt-pes-main}}} \; \text { such that }\; \underbrace{\widehat{u}^{(k)}_\pi - \cp \beta^{(k)}_\pi }_{\leq u_\pi \text{ by \Cref{lemma:bandit-opt-pes-main}}} \geq b \;.
\end{align}

\subsubsection{Zero-Violation and Logarithmic Number of $\pisafe$ Deployments}\label{subsec:zero-vio-bandit}

\looseness=-1
Since $\pi^{(k)}$ is either $\pisafe$ or the solution to \optpes (if feasible), all deployed policies in \Cref{algo:zero-vio-bandit} satisfy the constraint with high probability due to the pessimistic constraint.
However, as noted in \Cref{subsec:bandit-challenge}, the pessimistic constraint may render \optpes infeasible, requiring \Cref{line:op-ps-policy-bandit} to wait until the bonus \(\beta^{(k)}_\pi\) shrinks sufficiently.  
Yet, waiting too long leads to repeated deployments of \(\pisafe\), resulting in poor regret since $\pisafe$ may be sub-optimal. Therefore, efficient exploration must ensure that the number of iterations where \Cref{eq:optimistic-pessimistic-bandit} is infeasible remains bounded.

\looseness=-1
The core technique of \Cref{algo:zero-vio-bandit} lies in the \textbf{$\boldsymbol{\pisafe}$ deployment trigger} based on the confidence of $\pisafe$.
Specifically, we solve the optimistic-pessimistic optimization whenever 
$\beta^{(k)}_{\pisafe} \leq \frac{\bslt}{2\cp}$; otherwise, we correct the data by deploying $\pisafe$ (see \Cref{line:safe-policy-bandit}).
Under this trigger, the following \Cref{lemma:Ck-bound-main} ensures that the number of $\pisafe$ deployments grows \textbf{logarithmically} with the iteration length $K$.
\begin{definition}[$\pisafe$ unconfident iterations]\label{def:unconf-set}
Let $\unconfBandit$ be the set of iterations when \Cref{algo:zero-vio-bandit} is unconfident in $\pisafe$, i.e., 
\(
\unconfBandit \df 
\brace{k \in \bbrack{1, K} \given 
\beta^{(k)}_{\pisafe} > \bslt / (2\cp)
}
\).
Let $\unconfBandit^c \df \bbrack{1, K} \setminus \unconfBandit$ be its complement.
\end{definition}
\begin{lemma}[Logarithmic $\abs{\unconfBandit}$ bound]\label{lemma:Ck-bound-main}
It holds w.p. at least $1-\delta$ that
\(|\unconfBandit| \leq \cO\paren*{d\cp^2\bslt^{-2}\ln\paren*{K\delta^{-1}}}\).
\end{lemma}
The proof utilizes the well-known elliptical potential lemma \citep{abbasi2011improved}.
Intuitively, it ensures that the confidence bounds shrink on average, thereby limiting the number of iterations where the algorithm remains unconfident in $\pisafe$.
\citet{he2021uniform,zhang2023interplay} employed a similar technique in linear bandits to ensure the suboptimality of policies after sufficient iterations.

\looseness=-1
Moreover, combined with \Cref{lemma:bandit-opt-pes-main}, the following \Cref{lemma:alpha-feasibility-main} ensures that, after logarithmic iterations, \textbf{policies around $\boldsymbol{\pisafe}$ will become feasible solutions to \optpes and \Cref{line:op-ps-policy-bandit}}.
\begin{lemma}[Mixture policy feasibility]\label{lemma:alpha-feasibility-main}
Consider $k \in \unconfBandit^c$.
Let $\alpha^{(k)} \df \frac{\bslt - 2\cp \beta^{(k)}_{\pisafe}}{\bslt - 2\cp \beta^{(k)}_{\pisafe} + 2\cp \beta^{(k)}_{\pi^\star}}$.
For any $\alpha \in \brack*{0, \alpha^{(k)}}$, the mixture policy \(\pi_\alpha \df (1-\alpha) \pisafe + \alpha \pi^\star\) satisfies
$u_{\pi_\alpha} - 2\cp \beta^{(k)}_{\pi_\alpha} \geq b$.
\end{lemma}
\Cref{lemma:bandit-opt-pes-main} and \Cref{lemma:alpha-feasibility-main} directly imply the following zero-violation guarantee:
\begin{corollary}[Zero-violation]
W.p. at least $1-\delta$, \Cref{algo:zero-vio-bandit} satisfies $\pi^{(k)} \in \Pisafe$ for any $k$.
\end{corollary}

\subsubsection{Regret Analysis}
\looseness=-1
The remaining task is to ensure sublinear regret. By \Cref{lemma:Ck-bound-main,,lemma:bandit-opt-pes-main}, the regret is decomposed as:
\begin{align*}
{\textstyle
\regret(K) 
\leq \tiO\paren*{d B \cp^2\bslt^{-2}}
+ \underline{3\co \sum_{k \in \unconfBandit^c}\beta^{(k)}_\pi}_{\;\circled{1}}
+ \underline{\sum_{k \in \unconfBandit^c} \paren*{r_{\pi^\star} - \widehat{r}^{(k)}_{\pi^{(k)}} - \co \beta^{(k)}_\pi}}_{\;\circled{2}}\;.
}
\end{align*}
Using the elliptical potential lemma \citep{abbasi2011improved}, we can bound $\circled{1} \leq \tiO\paren{\co \sqrt{dK}}$.

\looseness=-1
For the term $\circled{2}$, when there is no constraint in \optpes, the common strategy is bounding $\circled{2}$ using $r_{\pi^\star} - \widehat{r}^{(k)}_{\pi^{(k)}} - \co \beta^{(k)}_\pi \leq 0$, leveraging the optimism due to \Cref{lemma:bandit-opt-pes-main} with the maximality of $\pi^{(k)}$ in \optpes (see, e.g., \citet{abbasi2011improved}).
However, due to the pessimistic constraint in \optpes, $\pi^\star$ may not be a solution to \optpes, necessitating a modification to this approach.

\looseness=-1
Recall from \Cref{lemma:alpha-feasibility-main} that, for $k \in \unconfBandit^c$, the mixture policy $\pi_{\alpha^{(k)}} \df (1-\alpha^{(k)})\pisafe + \alpha^{(k)}\pi^\star$ satisfies $u_{\pi_{\alpha^{(k)}}} - 2\cp \beta^{(k)}_{\pi_{\alpha^{(k)}}} \geq b$.
For this $\pi_{\alpha^{(k)}}$, the following optimism with respect to $\pi^\star$ holds:
\begin{lemma}[$\pi_{\alpha^{(k)}}$ optimism]\label{lemma:optimism-main}
If $\co \geq 2B\cp\bslt^{-1}$, for any $k \in \unconfBandit^c$, it holds 
\(r_{\pi_{\alpha^{(k)}}} + \co \beta^{(k)}_{\pi_{\alpha^{(k)}}} \geq r_{\pi^{\star}}\).
\end{lemma}
Since $\pi_{\alpha^{(k)}}$ is a feasible solution to \optpes, and $\pi^{(k)}$ is its maximizer, when $\co\geq \cp\paren{1+2B \bslt^{-1}}$,
\begin{align}\label{eq:mixture-policy-optimism}
{\textstyle
\circled{2}
\numeq{\leq}{a}
\sum_{k\notin \unconfBandit}
r_{\pi_{\alpha^{(k)}}} + \co \beta^{(k)}_{\pi_{\alpha^{(k)}}}
- \widehat{r}^{(k)}_{\pi^{(k)}} - \co\beta^{(k)}_{\pi^{(k)}}
\numeq{\leq}{b} \sum_{k\notin \unconfBandit}
\co \beta^{(k)}_{\pi_{\alpha^{(k)}}}
\numeq{\leq}{c} \tiO\paren{\co \sqrt{dK}}\;,
}
\end{align}
where (a) uses \Cref{lemma:optimism-main}, 
(b) uses \Cref{lemma:bandit-opt-pes-main},
and (c) is bounded similarly to $\circled{1}$.
This optimism via a mixture policy technique is adapted from tabular CMDPs to the linear bandit setup \citep{liu2021learning, bura2022dope}.
By combining all the results, \Cref{algo:zero-vio-bandit} archives the following guarantees:
\begin{theorem}\label{theorem:bandit-regret-main}
\looseness=-1
If \Banditalgo is run with the parameters listed in its \hyperref[algo:zero-vio-bandit]{\textbf{Input}} line, w.p. at least $1-\delta$,
\begin{align*}
\pi^{(k)} \in \Pisafe \;\text{ for any }\; k \in \bbrack{1, K} 
\quad \text{ and }\quad 
\regret (K) \leq 
\tiO\paren{dB \cp^2\bslt^{-2}
+ \co \sqrt{dK}
}\;.
\end{align*}
\end{theorem}

\looseness=-1
In summary, the zero-violation and regret guarantees rely on three key components:
($\mathrm{i}$) optimistic-pessimistic policy updates (\optpes),
($\mathrm{ii}$) a logarithmic number of $\pisafe$ deployments (\Cref{lemma:Ck-bound-main}), and
($\mathrm{iii}$) compensation for the pessimistic constraint via a linear mixture of policies (\Cref{lemma:optimism-main}).
In the next section, we develop a linear CMDP algorithm that builds upon these three components.
