\section{Safe Reinforcement Learning in Linear Constrained MDP} \label{sec:MDP}

\looseness=-1
We now consider the linear CMDP setting, a general framework that encompasses the linear constrained bandit as a special case with minor modifications.

\looseness=-1
A finite-horizon and episodic CMDP is defined as a tuple $(\S, \A, H, P, r, u, b, s_1)$, where
$\S$ is the finite but potentially exponentially large state space,
$\A$ is the finite action space ($\abs{\A}=\aA$),
$H \in \N$ is the episode horizon,
$b \in [0, H]$ is the constrained threshold, 
and $s_1$ is the fixed initial state.
The reward and utility functions $r, u: \HSA \to [0, 1]$ specify the reward $r_h(s, a)$ and constraint utility $u_h(s, a)$ when taking action $a$ at state $s$ in step $h$.
Finally, $P_\cdot\paren*{\cdot\given \cdot, \cdot}: \HSA\times \S \to [0, 1]$ denotes the transition kernel, where $P_{h}\paren{s' \given s, a}$ denotes the state transition probability to a new state $s'$ from a state $s$ when taking an action $a$ in step $h$.
With a slight abuse of notation, for functions $V : \S \to \R$ and $P_h$, we write $(P_h V)(x, a) = \sum_{y\in \S} V(y) P_h \paren{y \given x, a}$.

\looseness=-1
\paragraph{Policy and (regularized) value functions.}
A policy is defined as $\pi_{\cdot}\paren*{\cdot\given\cdot} : \HSA \to [0,1]$, where $\pi_h \paren{a \given s}$ gives the probability of taking an action $a$ at state $s$ in step $h$.
The set of all the policies is denoted as $\Pi$.
With an abuse of notation, for any policy $\pi $ and $Q: \SA \to \R$, let $\pi_h$ be an operator such that $(\pi_h Q) (s) = \sum_{a \in \A} \pi_h\paren{a\given s} Q (s, a)$.
For a policy $\pi$, transition kernel $P$, reward function $g: \HSA \to \R$, and entropy coefficient $\kappa \geq 0$, let $\qf{\pi, g}_{P, h}[\kappa]: \SA \to \R$ and $\vf{\pi, g}_{P, h}[\kappa]: \S \to \R$ denote the entropy-regularized value functions at step $h$ satisfying:
\begin{align*}
\qf{\pi, g}_{P,h}[\kappa] = g_h + \paren{P_h\vf{\pi, g}_{h+1, P}[\kappa]},\; 
\vf{\pi, g}_{P, h}[\kappa] = \pi_h
    \paren{\qf{\pi, g}_{P,h}[\kappa] - \kappa \ln \pi_h },
\;\text{ and }\; \vf{\pi, g}_{H+1, P}[\kappa] = \bzero\;.
\end{align*}
For \(\kappa = 0\), we omit \(\kappa\), e.g., \(\qf{\pi, g}_{P, h} \df \qf{\pi, g}_{P, h}[0]\). We denote \(h_\kappa \df h(1 + \kappa \ln A)\) for $h \in \bbrack{1, H}$.

\looseness=-1
For $h \in \bbrack{1, H}$, let $\occ{\pi}_{P, h}\in \Delta(\SA)$ denote the occupancy measure of $\pi$ in $P$ at step $h$ such that
\begin{equation}\label{eq:occupancy measure}
\occ{\pi}_{P, h}(s, a) = \P\paren*{s_h=s, a_h=a \given \pi, P}\quad \forall\; (h, s, a) \in \HSA\;,
\end{equation}
where the expectation is taken over all possible trajectories, in which $a_h \sim \pi_h \paren{\cdot \given s_h}$ and $s_{h+1} \sim P_h \paren{\cdot \given s_h, a_h}$.
With a slight abuse of notation, we write $\occ{\pi}_{P, h}(s) = \sum_{a \in \A} \occ{\pi}_{P, h}(s, a)$.

\looseness=-1
\paragraph{Learning Setup.}
An agent interacts with the CMDP over $K$ episodes using policies $\pi^{(1)}, \dots, \pi^{(K)} \in \Pi$. 
Each episode $k$ starts from $s_1$.
At step $h$ in episode $k$, the agent observes a state $s_h^{(k)}$, selects an action $a_h^{(k)} \sim \pi^{(k)}_h(\cdot\mid s_h^{(k)})$, and transitions to $s_{h+1}^{(k)} \sim P_h\paren{\cdot\given s_h^{(k)}, a_h^{(k)}}$.
The algorithm lacks prior knowledge of the transition kernel $P$, while $r$ and $u$ are known for simplicity. 
Extending our setting to unknown stochastic reward and utility is straightforward (see, e.g., \citet{efroni2020exploration}).

\looseness=-1
To handle a potentially large state space, we consider the following linear MDP assumption:
\begin{assumption}[Linear MDP]
\label{assumption:linear mdp}
We have access to a known feature map $\bphi: \SA \to \R^d$ satisfying:
there exist $d$ (signed) measures
$\bmu_h \df \paren{\bmu^1_h, \ldots, \bmu_h^d} \in \R^{S\times d}$ such that
\(P_h \paren*{s' \given s, a} = \bmu_h(s')^\top \bphi (s, a)\), and vectors $\btheta_h^r, \btheta_h^u \in \R^d$ such that 
$r_h(s, a) = \paren*{\btheta_h^r}^\top \bphi(s, a)$ and $u_h(s, a) = \paren*{\btheta_h^u}^\top \bphi(s, a)$.
$\bmu_h$ is unknown, but $\btheta^r_h$ and $\btheta^u_h$ are known to the algorithm.
We assume that $\sup_{s, a}\norm*{\bphi(s, a)}_2 \leq 1$ 
and $\norm*{V^\top \bmu_h}_2 \leq \sqrt{d}$ for any $V \in \R^\S$ such that $\norm*{V}_\infty \leq 1$.
\end{assumption}

\looseness=-1
Let $\pi^\star \in \argmax_{\pi \in \Pisafe} \vf{\pi, r}_{P, 1}(s_1)$ be the optimal policy, where $\Pisafe \df \brace{\pi \given \vf{\pi, u}_{P, 1}(s_1) \geq b}$ is the set of safe policies. The goal is to achieve sublinear regret under \textbf{episode-wise} constraints:
\begin{align}\label{eq:CMDP-goal}
{\textstyle
\regret (K) 
\df 
\sum^K_{k=1} 
\vf{\pi^\star, r}_{P, 1}(s_1) - 
\vf{\pi^{(k)}, r}_{P, 1}(s_1)
= o\paren*{K}
\;\text{ such that }\;
\pi^{(k)} \in \Pisafe \quad \forall k \in [K]\;.
}
\end{align}
\looseness=-1
Unlike most linear CMDP literature (except for instantaneous safety, see \Cref{table:algorithms}), this requires $\pi^{(k)}$ to be safe in every episode $k$.
Finally, we assume the strictly safe policy similar to \Cref{sec:zero-vio bandit}.
\begin{assumption}[Safe policy]\label{assumption:slater}
We have access to $\pisafe \in \Pisafe$ and $\bslt > 0$ such that $\vf{\pisafe, u}_{P, 1}(s_1) - b \geq \bslt\;$.
\end{assumption}



\subsection{Technical Challenge: Optimistic-Pessimistic Optimization in Linear CMDP}\label{subsec:technical-challenge-MDP}

\looseness=-1
Our linear CMDP algorithm builds on \Banditalgo in \Cref{sec:zero-vio bandit}: deploying an optimistic-pessimistic policy when confident in \(\pisafe\); otherwise, it uses \(\pisafe\). 
We will logarithmically bound the number of \(\pisafe\) deployments, similar to \Cref{lemma:Ck-bound-main}, and ensure optimism through a linear mixture of policies, as in \Cref{lemma:alpha-feasibility-main}.
However, computing an optimistic-pessimistic policy in the linear CMDP setting, as in \optpes, presents a non-trivial challenge.
This section outlines the difficulties.

\looseness=-1
Following standard linear MDP algorithm frameworks (e.g., \citet{jin2020provably,lykouris2021corruption}), for each $h, k$, let 
\(\beta^{(k)}_h: (s, a) \mapsto \norm{\bphi(s, a)}_{\paren{\bLambda^{(k)}_h}^{-1}}\) be the bonus, where \(\bLambda^{(k)}_h \df \rho \bI + \sum_{i=1}^{k-1} \bphi\paren{s_h^{(i)}, a_h^{(i)}} \bphi\paren{s_h^{(i)}, a_h^{(i)}}^\top\) and $\rho > 0$.
For any $V: \S \to \R$, let $\hP^{(k)}_h V$ be the next-step value estimation defined as:
$
\paren{\hP^{(k)}_h V}(s, a) \df \bphi(s, a)^{\top}\paren{\bLambda^{(k)}_h}^{-1} \sum_{i=1}^{k-1}\bphi\paren{s_h^{(i)}, a_h^{(i)}} V\paren{s_{h+1}^{(i)}}
$.
We construct the following optimistic and pessimistic value functions for reward and utility, respectively:
\begin{definition}[Clipped value functions]\label{def:clipped value functions}
\looseness=-1
Let $\co, \cp, C_\dagger, B_\dagger > 0$.
For each $k, h$, $\pi$, and $\kappa \geq 0$, 
define \(\oqf{\pi, r}_{(k), h}[\kappa], \oqf{\pi, \dagger}_{(k), h}, \pqf{\pi, u}_{(k), h}: \SA\to \R\) 
and \(\ovf{\pi, r}_{(k), h}[\kappa], \ovf{\pi, \dagger}_{(k), h}, \pvf{\pi, u}_{(k), h}: \S \to \R\) such that:
\begin{align*}
&\oqf{\pi, r}_{(k), h}[\kappa] \df r_h + \clip\brace{\co \beta^{(k)}_h + \hP^{(k)}_h \ovf{\pi, r}_{(k), h+1}[\kappa], \; 0, \; H_\kappa-h_\kappa},\;
\ovf{\pi, r}_{(k), h}[\kappa] \df \pi_h\paren{\oqf{\pi, r}_{(k), h}[\kappa] - \kappa \ln \pi_h}\;,\\
&\oqf{\pi, \dagger}_{(k), h} \df B_\dagger \beta^{(k)}_h + \clip\brace{C_\dagger \beta^{(k)}_h + \hP^{(k)}_h \ovf{\pi, \dagger}_{(k), h+1}, \; 0, \; B_\dagger (H-h)},\;
\ovf{\pi, \dagger}_{(k), h} \df \pi_h\oqf{\pi, \dagger}_{(k), h}\;,\\
&\pqf{\pi, u}_{(k), h} \df u_h + \clip\brace{-\cp \beta^{(k)}_h + \hP^{(k)}_h \pvf{\pi, u}_{(k), h+1}, \; 0, \; H-h},\; \text{ and }\;
\pvf{\pi, u}_{(k), h} \df \pi_h\pqf{\pi, u}_{(k), h}\;.
\end{align*}
We set \(\ovf{\pi, r}_{(k), H+1}[\kappa] = \ovf{\pi, \dagger}_{(k), H+1} = \pvf{\pi, u}_{(k), H+1} = \bzero\).
For \(\kappa = 0\), omit \(\kappa\), e.g., \(\oqf{\pi, r}_{(k), h} \df \oqf{\pi, r}_{(k), h}[0]\).
\end{definition}
Similar to the bandit proof (\Cref{eq:mixture-policy-optimism}), we need an additional optimistic bonus in the objective to compensate for the pessimistic constraint. 
We will utilize \(\oqf{\pi, \dagger}_{(k), h}\) and \(\ovf{\pi, \dagger}_{(k), h}\) for the compensation in this MDP setup.\footnote{Increasing $C_r$ and clip-threshold could offer similar compensation, but separate value functions simplify analysis.}
Entropy regularization in $\oqf{\pi, r}_{(k), h}[\kappa]$ is for the later analysis.
The clipping operators are essential to avoid the propagation of unreasonable value estimates \citep{zanette2020frequentist}.

\looseness=-1
Using these optimistic and pessimistic value functions, one might consider extending \optpes to linear CMDPs by solving the following optimization problem:
\begin{align}\label{eq:opt-pes-CMDP-informal}
\max_{\pi \in \Pi} \ovf{\pi, r}_{(k), 1}(s_1) + \ovf{\pi, \dagger}_{(k), 1}(s_1) 
\; \text{ such that }\; \pvf{\pi, u}_{(k), 1}(s_1) \geq b\;.
\end{align}
However, solving \Cref{eq:opt-pes-CMDP-informal} is challenging due to (\(\mathrm{i}\)) \textbf{the large state space} in the linear CMDP setting (\(|\S| \gg 1\)) and (\(\mathrm{ii}\)) \textbf{the clipping operators} in \(\oqf{\pi, r}_{(k), h}\), \(\oqf{\pi, \dagger}_{(k), h}\), and \(\pqf{\pi, u}_{(k), h}\).

\looseness=-1
In tabular CMDPs with small $|\S|$, \citet{liu2021learning,bura2022dope} 
used linear programming (LP) to solve similar optimistic-pessimistic optimization problems, achieving zero violation. However, the computational cost of LP scales with $|\S|$, making it impractical for linear CMDPs.


\looseness=-1
Another option is the Lagrangian method for CMDPs \citep{altman1999constrained}.
Essentially, it transforms the constrained optimization into a max-min optimization, and then swaps the max-min as follows:
$
\min_{\lambda \geq 0} 
\max_{\pi \in \Pi} \ovf{\pi, r}_{(k), 1}(s_1) + \ovf{\pi, \dagger}_{(k), 1}(s_1)  
+ 
\lambda\paren{\pvf{\pi, u}_{(k), 1}(s_1) - b}
$.
When the value functions are exact, i.e., $\ovf{\pi, \dagger}_{(k), h}+ \ovf{\pi, r}_{(k), h} + \pvf{\pi, u}_{(k), h} = \vf{\pi, r+B_\dagger \beta^{(k)}+\lambda u}_{P, h}$, 
the min-max formulation is equivalent to \Cref{eq:opt-pes-CMDP-informal} \citep{altman1999constrained}.
Moreover, the inner maximization becomes tractable since it reduces to policy optimization over 
$\vf{\pi, r +B_\dagger \beta^{(k)} + \lambda u}_{P, 1}(s_1)$.
Both favorable properties arise due to the linearity of the value function with respect to the occupancy measure (see, e.g., \citet{paternain2019constrained}).

\looseness=-1
However, due to the clipping operators, the value functions in \Cref{def:clipped value functions} may not admit an occupancy measure representation, making it non-trivial to guarantee that the Lagrangian approach can successfully solve \Cref{eq:opt-pes-CMDP-informal}.
To address this optimization challenge, our algorithm avoids directly solving \Cref{eq:opt-pes-CMDP-informal}. 
Instead, it realizes optimism and pessimism through a novel adaptation of the recent \textbf{softmax policy} technique for linear CMDPs \citep{ghosh2024towards,ghosh2022provably}, combined with \textbf{an extension of the $\boldsymbol{\pisafe}$ deployment technique} from \Cref{sec:zero-vio bandit} to the linear CMDP setting.

\begin{algorithm2e}[t!]
\caption{
Optimistic-Pessimistic Softmax Exploration for Linear CMDP}
\label{algo:zero-vio-linear MDP}
\DontPrintSemicolon
\LinesNumbered
\KwIn{\small Regr. coeff. $\rho = 1$, bonus scalers 
$\co=\tiO(dH)$, 
$\cp=\tiO(dH)$,
$C_\dagger=\tiO(d^2H^3\bslt^{-1})$,
$B_\dagger = \tiO\paren*{dH^2\bslt^{-1}}$,
entropy coeff. $\kappa = \widetilde{\Omega}\paren*{\bslt^3 H^{-4}d^{-1}K^{-0.5}}$, 
search length $T=\tiO(H)$, 
$\lambda$-threshold $C_\lambda = \tiO\paren*{dH^4\bslt^{-2}}$,
safe policy $\pisafe$, and iter. length $K \in \N$}
\For{$k = 1, \dots, K$}{
    Let \(\pvf{\pi, u}_{(k), h}\) be value function (\Cref{def:clipped value functions}) and $\pi^{(k), \lambda}$ be softmax policy (\Cref{def:composite-softmax-policy})\;
    {\color{blue}\tcc{Trigger is implicitly tied to $\pisafe$ confidence (\Cref{lemma:trigger-condition-main})}}
    \lIf{\(\pvf{\pi^{(k), C_\lambda}, u}_{(k), 1}(s_1) < b\)}{Set \(\pi^{(k)}\df \pisafe\)
    } \label{line:pisafe-deploy}
    \lElseIf{\(\pvf{\pi^{(k), 0}, u}_{(k), 1}(s_1) \geq b\)}{Set \(\pi^{(k)}\df \pi^{(k), 0}\)} \label{line:pizero-deploy}
    \Else({\tcc*[h]{\color{blue} Do bisection-search to find safe $\pi^{(k), \lambda}$ with small $\lambda$}})
    {
    Set $\underline{\lambda}^{(k, 1)} \df 0$ and $\widebar{\lambda}^{(k, 1)} \df C_\lambda$.
    Let $\lambda^{(k, t)} \df \paren{\underline{\lambda}^{(k, t)} + \widebar{\lambda}^{(k, t)}} / 2$\;
    \For{$t = 1, \dots, T$ \label{line:binary-search}}{
        \lIf{\(\pvf{\pi^{(k), \lambda^{(k, t)}},u}_{(k), 1}(s_1) \geq b\;\)}{
        \(\underline{\lambda}^{(k, t+1)} \df \underline{\lambda}^{(k, t)}\; \) 
        and \(\;\widebar{\lambda}^{(k, t+1)} \df \lambda^{(k, t)}\)}
        \lElse{
        \(\underline{\lambda}^{(k, t+1)} \df \lambda^{(k, t)}\; \) 
        and \(\;\widebar{\lambda}^{(k, t+1)} \df \widebar{\lambda}^{(k, t)}\)
        }
    }
    Set $\pi^{(k)} \df \pi^{(k), \widebar{\lambda}^{(k, T)}}$\label{line:pik-deploy}
    }
    Sample a trajectory $(s^{(k)}_1, a^{(k)}_1, \dots, s^{(k)}_H, a^{(k)}_H)$ by deploying $\pi^{(k)}$\;
}
\end{algorithm2e}


\subsection{Algorithm and Analysis}

\looseness=-1
We summarize the proposed \MDPalgo in \Cref{algo:zero-vio-linear MDP}.
All formal theorems and proofs in this section are provided in \Cref{appendix:MDP-regret-analysis}.
Throughout this section, we analyze \Cref{algo:zero-vio-linear MDP} under the parameters listed in its \hyperref[algo:zero-vio-linear MDP]{\textbf{Input}} line.
\looseness=-1
A key component of our algorithm is the \textbf{composite softmax policy}, which balances optimistic exploration and pessimistic constraint satisfaction via $\lambda \geq 0$:
\begin{definition}[Composite softmax policy]\label{def:composite-softmax-policy}
For $\lambda \geq 0$, $\kappa > 0$, let $\pi^{(k), \lambda}\in \Pi$ be a policy such that 
\begin{align*}
\pi^{(k), \lambda}_{h}\paren*{\cdot \given s} = \softmax\paren*{\frac{1}{\kappa}\paren*{
\oqf{\pi^{(k), \lambda}, \dagger}_{(k), h}(s, \cdot) + 
\oqf{\pi^{(k), \lambda}, r}_{(k), h}[\kappa](s, \cdot) + 
\lambda \pqf{\pi^{(k), \lambda}, u}_{(k), h}(s, \cdot)}}\;.
\end{align*}
We remark that $\pi^{(k), \lambda}$ can be computed iteratively in a backward manner for $h=H, \dots, 1$.
\end{definition}
Additionally, this softmax policy is essential for establishing the concentration bounds in linear CMDP (see \citet{ghosh2022provably} for details).
Given two softmax distributions $\pi = \softmax(\frac{\bq}{\kappa})$ and $\tpi = \softmax(\frac{\widetilde{\bq}}{\kappa})$ where $\bq, \widetilde{\bq}\in \R^\aA$, it holds that $\norm*{\pi - \widetilde{\pi}}_1 \leq \frac{8}{\kappa} \norm*{\bq - \widetilde{\bq}}_\infty$ (see \Cref{lemma:softmax-policy-bound}).
Leveraging this Lipschitz continuity, we derive the following confidence bounds:
\begin{lemma}[Confidence bounds]\label{lemma:opt-pes-MDP-main}
For any $(k, h)$, for any $\lambda \in [0, C_\lambda]$, and for both $\pi = \pi^{(k), \lambda}$ and $\pi=\pisafe$, w.p. at least $1-\delta$, it holds that
\begin{align*}
\vf{\pi,r}_{P, h} \leq \ovf{\pi,r}_{(k), h} \leq  \vf{\pi,r+ 2\co \beta^{(k)}}_{P, h},\;
\vf{\pi,B_\dagger\beta^{(k)}}_{P, h} \leq \ovf{\pi,\dagger}_{(k), h} \leq  \vf{\pi,(B_\dagger + 2C_\dagger) \beta^{(k)}}_{P, h},\;
\vf{\pi,u-2\cp \beta^{(k)}}_{P, h} \leq \pvf{\pi,u}_{(k), h} \leq \vf{\pi,u}_{P, h}.
\end{align*}
\end{lemma}
Using \Cref{lemma:opt-pes-MDP-main}, analogous to \Cref{subsec:zero-vio-bandit}, we next establish the zero-violation guarantee.



\subsubsection{Zero-Violation and Logarithmic Number of $\pisafe$ Deployments}\label{subsec:MDP-zero-vio}

\looseness=-1
In the softmax policy (\Cref{def:composite-softmax-policy}), $\lambda$ balances optimism and pessimism: a small $\lambda$ promotes exploration, while a large $\lambda$ prioritizes constraint satisfaction.
Building on this, \Cref{algo:zero-vio-linear MDP} conducts a \textbf{bisection search} to find the smallest feasible $\lambda$ while ensuring the pessimistic constraint holds (\Cref{line:pizero-deploy} to \Cref{line:pik-deploy}).
If even a large $\lambda = C_\lambda$ fails to satisfy the constraint, the algorithm assumes no feasible pessimistic policy exists and deploys $\pisafe$ (\Cref{line:pisafe-deploy}).
Since the softmax policy is only deployed for $\lambda$ satisfying $\pvf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1) \geq b$, \Cref{lemma:opt-pes-MDP-main} implies the following zero-violation guarantees:
\begin{corollary}[Zero-violation]\label{corollary:zero-violation-MDP}
W.p. at least $1-\delta$, \Cref{algo:zero-vio-linear MDP} satisfies $\pi^{(k)} \in \Pisafe$ for any $k$.
\end{corollary}


\looseness=-1  
Next, we bound the number of $\pisafe$ deployments.
To this end, similar to the bandit warm-up (\Cref{sec:zero-vio bandit}), \textbf{we relate $\boldsymbol{\pisafe}$ deployment to $\boldsymbol{\pisafe}$ uncertainty level} and logarithmically bound the number of uncertain iterations.
The following \Cref{lemma:trigger-condition-main} ensures that, if \Cref{algo:zero-vio-linear MDP} is confident in $\pisafe$ and runs with appropriate $C_\lambda$ and $\kappa$, then $\pisafe$ is not deployed.
\begin{definition}[$\pisafe$ unconfident iterations]\label{def:unconf-set MDP}
Let $\unconfMDP$ be the iterations when \Cref{algo:zero-vio-linear MDP} is unconfident in $\pisafe$, i.e., 
$
\unconfMDP \df 
\brace*{k \in \bbrack{1, K} \given 
\vf{\pisafe, \beta^{(k)}}_{P, 1}(s_1) > \frac{\bslt}{4\cp}
}
$.
Let $\unconfMDP^c \df \bbrack{1, K} \setminus \unconfMDP$ be its complement.
\end{definition}
\begin{lemma}[Implicit $\pisafe$ deployment trigger]\label{lemma:trigger-condition-main}
When $C_\lambda \geq \frac{8H_\kappa^2 (B_\dagger + 1)}{\bslt}$ and $\kappa \leq \frac{\bslt^2}{32H_\kappa^2 (B_\dagger + 1)}$, then w.p. at least $1-\delta$, it holds that $\pvf{\pi^{(k), C_\lambda}, u}_{(k), 1}(s_1) \geq b$ for all $k \in \unconfMDP^c$.
\end{lemma}
\looseness=-1
Essentially, the proof of \Cref{lemma:trigger-condition-main} relies on the following monotonic property of the value function for the softmax policy: if the value estimation is exact, increasing $\lambda$ monotonically improves safety.
\begin{lemma}[Softmax value monotonicity]\label{lemma:softmax-value-monotonicity-main}
\looseness=-1
For $\lambda \geq 0$, let $\pi^\lambda$ be a softmax policy such that 
\(
\pi^\lambda_h \paren{\cdot \given s} = \softmax\paren{\frac{1}{\kappa}\paren{
    \qf{\pi, r}_{P, h}[\kappa](s, \cdot)
    + \lambda \qf{\pi, u}_{P, h}(s, \cdot)}}
\).
Then, \(\vf{\pi^\lambda, u}_{P, 1}(s_1)\) is monotonically increasing in $\lambda$.
\end{lemma}
While the true value function enjoys this monotonicity, the estimated value $\pvf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1)$ may not, as $\hP^{(k)}_h V$ can take negative values even when $V$ is positive. 
This makes the proof of \Cref{lemma:trigger-condition-main} non-trivial.
To overcome this challenge, we leverage \Cref{lemma:opt-pes-MDP-main}, which ensures that the estimated values are sandwiched between certain true value functions.
We prove \Cref{lemma:trigger-condition-main} by showing that, for sufficiently large $C_\lambda$, any sandwiched value satisfies the constraint under pessimism, implying that the estimated value also satisfies it.
This novel result enables bisection search to adjust $\lambda$, making \MDPalgo more computationally efficient than \citet{ghosh2024towards}.
The detailed proofs of \Cref{lemma:trigger-condition-main,,lemma:softmax-value-monotonicity-main} are provided in \Cref{subsec:safe-softmax-policy-exists-proof}.



\looseness=-1
Finally, the following lemma ensures that the number of $\pisafe$ deployment scales logarithmic to $K$, as in \Cref{lemma:Ck-bound-main}.
The proof follows from extending the banditâ€™s proof of \Cref{lemma:Ck-bound-main} to CMDPs.
\begin{lemma}[Logarithmic $|\unconfMDP|$ bound]\label{lemma:Ck-bound-MDP-main}
It holds w.p. at least $1-\delta$ that
\(
\abs{\unconfMDP}
\leq
\cO\paren*{d^3 H^4\bslt^{-2} \ln KH \delta^{-1}}
\).
\end{lemma}

\subsubsection{Regret Analysis}\label{subsec:MDP-regret-analysis}
\looseness=-1
The remaining task is to ensure sublinear regret. By \Cref{lemma:Ck-bound-MDP-main,,lemma:opt-pes-MDP-main}, the regret is decomposed as:
\begin{align*}
\regret(K) 
\leq
\tiO\paren*{\frac{d^3 H^4}{\xi^{2}}}
+ 
\underline{
 \sum_{k \in \unconfMDP^c} \vf{\pi^{(k)}, 2\co\beta^{(k)}}_{P, 1}(s_1)
}_{\;\circled{1}}
+
\underline{\sum_{k\in \unconfMDP^c} \paren*{\vf{\pi^\star, r}_{P, 1}(s_1) - \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)}}_{\;\circled{2}}
+ \kappa KH \ln A 
\;,
\end{align*}
where the last term arises from the entropy regularization ($\vf{\pi, r}_{P, 1}(s_1)[\kappa] - \vf{\pi, r}_{P, 1}(s_1) \leq \kappa H \ln \aA$), which is controlled by the value of $\kappa$.  
Using the elliptical potential lemma for linear MDPs \citep{jin2020provably}, we obtain $\circled{1} \leq \tiO\paren{\co H\sqrt{d K}}$.

\looseness=-1
We now bound $\circled{2}$. Note that for any $k \in \unconfMDP^c$, due to \Cref{lemma:trigger-condition-main}, $\pi^{(k)}$ is the softmax policy by \Cref{line:pik-deploy}.
To bound $\circled{2}$, following a similar approach to \Cref{lemma:optimism-main}, we replace $\pi^\star$ with a mixture policy that satisfies the pessimistic constraint.
To this end, we utilize the following lemmas.
\begin{definition}[Mixture policy]\label{def:mixture-policy}
For $\alpha \in [0, 1]$, let $\pi^\alpha$ be a mixture policy such that, for any $h$, 
\(\occ{\pi^\alpha}_{P, h} = (1 - \alpha) \occ{\pisafe}_{P, h} + \alpha \occ{\pi^\star}_{P, h}\).
Such a $\pi^\alpha$ is ensured to exists for any $\alpha \in [0, 1]$ \citep{borkar1988convex}.
\end{definition}

\begin{lemma}[Safe and optimistic mixture policy]\label{lemma:mixture-safe-optimism-MDP-main}
Let $\alpha^{(k)} \df \frac{\bslt}{\bslt + 2 \vf{\pi^\star, 2\cp \beta^{(k)}}_{P, 1}(s_1)}$.
If $B_\dagger \geq \frac{4\cp H}{\bslt}$, then for any $k \in \unconfMDP^c$, it holds
($\mathrm{i}$)
\(
\vf{\pi^{\alpha^{(k)}}, u-2\cp \beta^{(k)}}_{P, 1}(s_1) \geq b
\) and 
($\mathrm{ii}$)
\(
\vf{\pi^{\alpha^{(k)}}, r+B_\dagger\beta^{(k)}}_{P, 1}(s_1) \geq
\vf{\pi^\star, r}_{P, 1}(s_1) 
\).
\end{lemma}

\looseness=-1
In \Cref{algo:zero-vio-linear MDP}, $\widebar{\lambda}^{(k, T)}$ is always chosen to satisfy $\pvf{\pi^{(k)}, u}_{(k), 1}(s_1) < b$.
Since $b \leq \vf{\pi^{\alpha^{(k)}}, u - 2\cp \beta^{(k)}}_{P, 1}(s_1)$ holds due to \Cref{lemma:mixture-safe-optimism-MDP-main}, $\circled{2}$ is bounded by
\begin{align*}
&
\begin{rcases}
\circled{2} \leq
&\sum_{k\in \unconfMDP^c} 
\Big(\vf{\pi^{\alpha^{(k)}}, B_\dagger \beta^{(k)}}_{P, 1}(s_1) 
+ \vf{\pi^{\alpha^{(k)}}, r}_{P, 1}[\kappa](s_1)
+ \widebar{\lambda}^{(k, T)} \vf{\pi^{\alpha^{(k)}}, u - 2\cp \beta^{(k)}}_{P, 1}(s_1)\\
&
\quad\quad
- \ovf{\pi^{(k)}, \dagger}_{(k), 1}(s_1)
- \ovf{\pi^{(k)}, r}_{(k), 1}[\kappa](s_1)
- \widebar{\lambda}^{(k, T)} \pvf{\pi^{(k)}, u}_{(k), 1}(s_1)
\Big)
\end{rcases}\; \circled{3}\\
&
\quad\quad\quad\quad
{\textstyle
+
\underline{\sum_{k\in \unconfMDP^c}  \ovf{\pi^{(k)}, \dagger}_{(k), 1}(s_1) }_{\;\circled{4}}
+ 
\underline{
C_\lambda \sum_{k \in \unconfMDP^c} 
\paren*{
\pvf{\pi^{(k), \widebar{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1) - 
\pvf{\pi^{(k), \underline{\lambda}^{(k, T)}}, u}_{(k), 1}(s_1)
}
}_{\;\circled{5}} \;.
}
\end{align*}

\looseness=-1
The second term $\circled{4}$ can be bounded similarly to $\circled{1}$. 
Using \Cref{lemma:opt-pes-MDP-main}, we obtain
\(
\circled{4} \leq   
(B_\dagger + 2C_\dagger)\sum_{k\in \unconfMDP^c}  \vf{\pi^{(k)}, \beta^{(k)}}_{P, 1}(s_1)
\leq \tiO\paren*{(B_\dagger + C_\dagger)H\sqrt{d K}}\;.
\)

\looseness=-1
The third term $\circled{5}$ is controlled by the width of the bisection search space ($\widebar{\lambda}^{(k, T)} - \underline{\lambda}^{(k, T)}$) and the following sensitivity result for $\pvf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1)$ with respect to $\lambda$.
\begin{lemma}\label{lemma:lambda-sensitive-main}
For any $k$ and $\lambda \in [0, C_\lambda]$, it holds that
\(
\abs*{\pvf{\pi^{(k), \lambda}, u}_{(k), 1}(s_1)
- \pvf{\pi^{(k), \lambda + \varepsilon}, u}_{(k), 1}(s_1)}
\leq \cO\paren*{X^H}\varepsilon
\) where $X \df K \paren*{1 + 8(1+C_\lambda)(H_\kappa + B_\dagger H + H)\kappa^{-1}}$.
\end{lemma}
\citet{ghosh2024towards} also derived a similar exponential bound (see their \textbf{Appendix C}).
Due to the update rule of the bisection search, setting the search iteration to $T = \tiO(H)$ ensures that \(\circled{5} \leq \tiO\paren{1}\).

\looseness=-1
For $\circled{3}$, using a modification of the so-called value-difference lemma \citep{shani2020optimistic},
\begin{align}\label{eq:third-term-decompose}
\circled{3} =
\sum_{k \in \unconfMDP^c}
\vf{\pi^{\alpha^{(k)}}, f^1}_{P, 1}(s_1) 
- \vf{\pi^{\alpha^{(k)}}, f^2}_{P, 1}(s_1) 
- \widebar{\lambda}^{(k, T)}\vf{\pi^{\alpha^{(k)}}, 2\cp \beta^{(k)}}_{P, 1}(s_1)\;, 
\end{align}
where $f^1: \HSA\to \R$ and $f^2: \HSA\to \R$ are functions such that, for any $h$,
\begin{align*}
&
f^1_h = 
\paren*{\pi^{\alpha^{(k)}}_h - \pi^{(k)}_h}
\paren*{
\oqf{\pi^{(k)}, \dagger}_{(k), h} +
\oqf{\pi^{(k)}, r}_{(k), h}[\kappa] + \widebar{\lambda}^{(k, T)} 
\pqf{\pi^{(k)}, u}_{(k), h}
}
- \kappa\pi^{\alpha^{(k)}}_h\ln \pi^{\alpha^{(k)}}_h 
+ \kappa \pi^{(k)}_h \ln \pi^{(k)}_h
\\
\text{ and }\; &
f^2_h = 
\paren*{\oqf{\pi^{(k)}, r}_{(k), h}[\kappa]
- r_h - P_h \ovf{\pi^{(k)}, r}_{(k), h+1}[\kappa]}
+ \widebar{\lambda}^{(k, T)}\paren*{u_h + P_h \pvf{\pi^{(k)}, u}_{(k), h+1} - \pqf{\pi^{(k)}, u}_{(k), h}}\\
&\quad \quad+ \paren*{\oqf{\pi^{(k)}, \dagger}_{(k), h} - B_\dagger \beta^{(k)} - P_h \ovf{\pi^{(k)}, \dagger}_{(k), h+1}}\;.
\end{align*}
\looseness=-1
\textbf{Our use of the softmax policy with entropy regularization is crucial for bounding $\circled{3}$.}
Since the analytical maximizer of the regularized optimization 
\(
\max_{\pi \in \sP(\A)}
\sum_{a \in \A}
\pi\paren{a} \paren*{\bx(a) - \kappa \ln \pi\paren{a}}
\) is given by  $\softmax\paren*{\frac{1}{\kappa} \bx(\cdot)}$, it follows that $f^1$ is non-positive, implying $\vf{\pi^{\alpha^{(k)}}, f^1}_{P, 1}(s_1) \leq 0$.
Additionally, applying \Cref{lemma:opt-pes-MDP-main}, we derive 
\(
f^2_h \geq -\widebar{\lambda}^{(k, T)} 2\cp \beta^{(k)}_h
\), which leads to
\(-\vf{\pi^{\alpha^{(k)}}, f^2}_{P, 1}(s_1) 
-\widebar{\lambda}^{(k, T)}\vf{\pi^{\alpha^{(k)}}, 2\cp \beta^{(k)}}_{P, 1}(s_1) 
\leq 0\).
By substituting these bounds into \Cref{eq:third-term-decompose}, we obtain $\circled{3} \leq 0$.

\looseness=-1
By combining all the results, \Cref{algo:zero-vio-linear MDP} achieves the following guarantees:
\begin{theorem}\label{theorem:MDP-regret-main}
\looseness=-1
If \MDPalgo is run with the parameters listed in its \hyperref[algo:zero-vio-linear MDP]{\textbf{Input}} line, w.p. at least $1-\delta$,
\begin{align*}
\pi^{(k)} \in \Pisafe \; \forall k \in \bbrack{1, K} 
\; \text{ and }\; 
\regret (K) \leq 
\underline{
\tiO\paren{H^{2} \sqrt{d^3 K}}
}_{\;(\mathrm{i})}
+ 
\underline{
\tiO\paren{d^3H^4 \bslt^{-2}}
}_{\;(\mathrm{ii})}
+ 
\underline{
\tiO\paren{H^{4} \bslt^{-1}\sqrt{d^5 K}}
}_{\;(\mathrm{iii})}\;.
\end{align*}
\end{theorem}

\begin{remark}[Regret bound]\label{remark:regret}  
\looseness=-1
In the regret bound, term \((\mathrm{i})\) arises from unconstrained exploration by \(\oqf{\pi, r}_{(k), h}\),
\((\mathrm{ii})\) accounts for \(\pisafe\) deployment within \(\abs{\unconfMDP}\), 
and \((\mathrm{iii})\) compensates for the pessimistic constraint using \(\oqf{\pi, \dagger}_{(k), h}\). 
Without the constraint---i.e., removing \((\mathrm{ii})\) and \((\mathrm{iii})\)---our bound simplifies to \(\tiO\paren{H^{2} \sqrt{d^3 K}}\), matching the regret bound of the fundamental LSVI-UCB algorithm by \citet{jin2020provably}.
The presence of $\xi^{-2}$ in $(\mathrm{ii})$ is unavoidable \citep{pacchiano2021stochastic}.
Compared to $(\mathrm{i})$, $(\mathrm{iii})$ has a worse dependence on $H$ and $d$, but the analysis by \citet{vaswani2022near} suggests that such dependence may be inherent rather than an artifact of our analysis.
Determining whether the bound can be improved is beyond the scope of this paper.
Nonetheless, \textbf{\MDPalgo establishes the first result achieving zero episode-wise constraint violations and sublinear regret in linear CMDPs.}
As a by-product, since linear CMDPs generalize tabular CMDPs \citep{jin2020provably}, \MDPalgo is also the first episode-wise safe algorithm for tabular CMDPs that operates without solving LPs.
\end{remark}

\begin{remark}[Computational cost]\label{remark:computational cost}  
\Cref{algo:zero-vio-linear MDP} requires up to \( T \) evaluations of the clipped value functions (\Cref{def:clipped value functions}) and the softmax policy (\Cref{def:composite-softmax-policy}), yielding a per-iteration cost of \( \cO(T \times \text{[value \& policy computation]})\).  
Using the bisection search, we bound \( T = \tiO(H) \), reducing the cost to \( \tiO(H \times \text{[value \& policy computation]}) \).  
Since these computations scale polynomially with \( \aA, H, \) and \( d \) \citep{lykouris2021corruption}, \textbf{\MDPalgo runs in polynomial time}---an improvement over recent \citet{ghosh2024towards}, which achieves $\tiO(\sqrt{K})$ violation regret but incurs an exponential \( K^H \) cost.
\end{remark}
