\section{Conclusion}\label{sec:conclusion}

\looseness=-1
This paper proposed \MDPalgo, the first RL algorithm achieving both sublinear regret and episode-wise constraint satisfaction in linear CMDPs (\Cref{theorem:MDP-regret-main}). 
Our approach builds on optimistic-pessimistic exploration with two key innovations: ($\mathrm{i}$) a novel deployment rule for $\pisafe$ and ($\mathrm{ii}$) a softmax-based approach for efficiently implementing optimistic-pessimistic policies in linear CMDPs.

\paragraph{Limitation.}
\looseness=-1
\MDPalgo achieves computational efficiency by performing a bisection search over $\lambda \in [0, C_\lambda]$. 
This approach is feasible in the single-constraint setting, where increasing $\lambda$ monotonically improves safety (\Cref{lemma:softmax-value-monotonicity-main}).
However, extending our method to the \textbf{multi-constraint setting} is non-trivial, as $\lambda$ becomes a vector, requiring a vectorized version of the softmax monotonicity property.
Nonetheless, we note that all theoretical results in \Cref{table:algorithms} are also limited to single-constraint settings, meaning our work still advances the state of the art in safety.
Developing a computationally efficient algorithm for multi-constrained linear CMDPs remains an open challenge for future research.
