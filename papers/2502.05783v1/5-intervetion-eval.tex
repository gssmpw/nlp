\section{Intervention Evaluation}
\label{sec:intervention_evaluation}
The promising model performance in Sec.~\ref{sub:model_evaluation:pipeline_evaluation} has validated the effectiveness of our few-shot learning pipeline.
Building upon the pipeline, we further conducted a user study to evaluate the effectiveness of \projectname and compared it against a rule-based baseline intervention system.

\subsection{Participants}
\label{sub:intervention_evaluation:participants}
With IRB approval, we recruited the same set of participants in Sec.~\ref{sub:model_evaluation:data_collection} for a follow-up intervention study. 
In the previous data collection, participants performed five per-determined actions and a self-defined action. In this study, they were asked to select one of the six actions that they had the strongest need for intervention.
This action was set as the target action for intervention during the study.
Among the 26 participants, 5 of them did not follow the study protocol. Their results were removed as outliers.  This section focused on the findings based on the remaining 21 participants.

\subsection{Intervention Setting}
\label{sub:intervention_evaluation:setting}
Since personal undesirable actions are inherently difficult to predict or control, we designed an intervention experience that closely mirrors real-life contexts to enhance ecological validity, encouraging participants to perform these actions under more natural conditions.
Our initial conversation with participants indicated two common scenarios where they tended to perform these actions: when they were in an engaging task with a relaxing state (\eg watching an interesting movie or a reality show with dramatic twists and turns); and when they were bored or disengaged (\eg mindlessly scrolling through social media or watching a tedious video) \footnote{Several participants also mentioned the scenarios under pressure or stress. Considering the feasibility and ethics of a multi-hour intervention study, we did not provide this option.}.
Therefore, we set up two types of video-watching tasks and allowed participants to pick the type in which they tended to perform more undesirable actions.

The first type included \textit{engaging} videos. We prepare a set of multi-hour videos for participants to choose from, such as the Harry Potter movie series, sports competitions, and mystery/detective shows.
The second type was watching \textit{disengaging} videos. Examples include cycling or driving route videos, math problem explanations, and public health lecture videos.
Participants sat in a quiet room with a laptop on the table and watched the video they selected, as shown in \autoref{fig:intervention_setup}(a) and (b).
During the video-watching, participants were not interrupted by the experimenter, simulating the real-life setting. 

\input{fig_tab_alg/fig_intervention_setup}

\subsection{Study Design and Procedure}
\label{sub:intervention_evaluation:design}
We adopted a within-subject design and compared our AI-powered \projectname against a rule-based intervention system.
In the rule-based system, a regular notification (the same interface as \autoref{subfig:interface_design:intervention}) was delivered every 10 minutes, regardless of whether the user did the action.
To mitigate the effect of the two systems outputting different numbers of notifications, we further added restrictions in \projectname so that the number of delivered notifications would be in the range of $\times 0.5$ to $\times 2$ as the baseline system.
This was achieved by forcefully delivering a notification if there was no intervention by the end of each 20-minute window ($\times 0.5$ times of interventions in minimum).
With the 5-min cool-down setup, \projectname can only deliver up to one intervention every 5 minutes, which would be no more than $\times 2$ times of interventions as the baseline.

Our study procedure was designed as follows. After selecting the personal target undesirable action and the task type (engaging vs disengaging), participants would calibrate and familiarize themselves with the intervention system and study setup. They then attended two intervention sessions in total, one session per day. We counterbalanced the order between \projectname and the baseline system, and participants were blind to the order of the two systems.
After familiarizing themselves with the room environment and setup, participants went through each intervention session with three stages (in total 130 minutes): (1) a 30-minute \textit{pre-intervention stage}, where there was no intervention delivered; (2) a 90-minute \textit{intervention stage}, where \projectname or the baseline system would deliver interventions as designed; and (3) a 10-minute \textit{post-intervention} stage, where no more intervention was delivered to observe any lasting effect \footnote{Due to the restrictions of the room booking time and device battery, we regretfully could not do a longer post-intervention stage. We recognize this as a limitation of our study in discussion.}.

The whole intervention session was video-recorded by a camera from the ceiling, positioned at an angle to capture participants' micro-actions and collect ground truth (see \autoref{fig:intervention_setup}(c)). We manually annotated the video and calculated the number and duration of the target actions during the three stages.
We collected participants' Self-Report of Habit Strength of the target action~\cite{verplanken2003reflections} before and after each session.
After the post-intervention stage, we further collected quantitative data from participants with a questionnaire that includes System Usability Scale (SUS) survey~\cite{bangor2008empirical} and Working Alliance Inventory (WAI, short revision)~\cite{munder2010working}.
In addition, we conducted a brief semi-structured interview to collect qualitative feedback on the intervention experience from each participant.

In total, the two sessions took around 5 hours for each participant. To reduce user fatigue, the two sessions were scheduled on two different days within a week. Participants were compensated with \$50 for the intervention study.
% \orson{@Yancheng to double-check and fill in the content in this section.}

\subsection{Intervention Results}
\label{sub:intervention_evaluation:intervention_results}
We first summarize the quantitative results from our study.
We coded the recorded videos by documenting the duration of target actions performed by participants every 10 minutes across the three stages.
Since participants had diverse behavior patterns, we normalized the results with each individual's target action duration in the pre-intervention stage as the reference.
The \textit{relative duration} was calculated by dividing the average duration of target actions per 10 minutes in both the intervention and post-intervention stages by that of the pre-intervention stage. A lower relative duration means more reduction of the target actions compared to the pre-intervention stage.

% Overall, the real-time pipeline of \projectname had a consistent performance as Sec.~\ref{sec:model_evaluation}.
% , with an overall accuracy of XX.X\% and a false positive rate of XX.

% \subsubsection{Controlled Effects on the Number of Interventions.}
% As the frequency of reminders could be one of the factors affecting intervention effectiveness, we compared the number of interventions between AI-based and rule-based intervention modes. A significant difference was found between the two modes. The mean number of interventions in the AI-based group was 11.76 and in the rule-based group was 9.00. This difference was statistically significant according to an independent-samples t-test (t(20) = 2.97, p < .01). As a result, we included the number of interventions as a covariate in our statistical model to control for its effect.

% \orson{@Yancheeng, check whether there is a significant difference between the number of interventions. Hopefully no. If it is, we would say that we will add the number of interventions in the statistical model to control its effect. I don't think we would need a figure here.}
\input{fig_tab_alg/fig_intervention_result}

\subsubsection{Reduction of the Duration of Target Actions by Intervention.}
% After the study, we coded the recorded videos by documenting the start and end times of target actions performed by all participants during the last three stages.
We compare the relative duration between \projectname and the baseline in both intervention and post-intervention stages.
Since participants received slightly more notifications in \projectname during the intervention stage (on average 11.8 \vs 9.0 times per session), we controlled the effect of the number of notifications by using generalized linear mixed models (GLMMs).
Specifically, a GLMM had relative duration as the dependent variable, with the intervention method (AI-based in \projectname \vs rule-based in baseline) and the number of notifications as the main factors.

As shown in the left of Fig.\ref{fig:duration_result}(a), during the intervention stage, \projectname resulted in 36.0 $\pm$ 22.6\% of the duration compared to the pre-intervention stage (\ie a reduction of 64.0\% of the target undesirable action), and the baseline system led to 65.0 $\pm$ 47.5\% of the duration (\ie a reduction of 35.0\%).
We fitted a GLMM to compare the two intervention methods.
Our results revealed the significant difference between the two methods: \projectname significantly outperformed the baseline by 29.0\% more reduction of the target undesirable action ($\chi^2_1=6.32, p < .05$). Meanwhile, the number of notifications does not show significance ($\chi^2_1=0.53, p = 0.47$). These results suggest that the advantage of \projectname was mainly attributed to the AI-based intervention method.

In addition, although our post-intervention stage was short, both methods showed promising signals of a potential lasting effect when the intervention was gone (13.9 $\pm$ 16.8\% for the \projectname; 37.7 $\pm$ 37.2\% for the baseline), as shown in the right of Fig.\ref{fig:duration_result}(a).
We fitted another GLMM on the post-intervention data. The results also indicate the significance of the intervention method ($\chi^2_1=10.04, p<0.01$), but not the number of notifications ($\chi^2_1=0.12, p=0.73$).
This is consistent with the result of the intervention stage, further demonstrating the superior performance of \projectname over the baseline method.


% The model also included a non-significant intercept (b = 0.38, OR = 1.46, 95\% CI [0.90, 2.37], p = .131) and intervention times (b = 0.00, OR = 1.00, 95\% CI [0.96, 1.04], p = .933).
% These results indicate that \projectname was more effective in reducing participants' target actions compared to scheduled reminders, and this effect was not significantly associated with the number of intervention times. The GLMM analysis also showed that the relative duration of target actions during the post-intervention stage remained significantly lower in the AI-based condition compared to rule-based intervention (b = 0.25, OR = 1.28, 95\% CI [1.05, 1.55], p < .05). The model included a non-significant intercept (b = 0.12, OR = 1.13, 95\% CI [0.78, 1.63], p = .537) and intervention times (b = 0.00, OR = 1.00, 95\% CI [0.97, 1.03], p = .910). These findings suggest that \projectname demonstrated superior lasting effects in reducing target actions compared to the rule-based condition during the short-term post-intervention period.

% \orson{@Yancheng to add the results figure and write up the findings}

\subsubsection{Intervention Effectiveness over Time.}
To investigate changes in the duration of target action during the study session, we visualize the change of participants' target action duration throughout the study (see \autoref{fig:duration_result}(b)).
Both intervention methods showed a clear and significant decreasing trend once participants entered the intervention stage.
The fitted lines in \autoref{fig:duration_result}(b) indicate that \projectname achieved more duration reduction ($m=-4.8\%$ per 10-minute) compared to the baseline ($m=-4.1\%$) over the intervention session.
In particular, \projectname had a more rapid initial decrease and maintained consistently lower levels throughout the rest of the session compared to the rule-based baseline.
Overall, \projectname demonstrated stronger cumulative effects.

\subsubsection{Difference across Task Types.}
During the study, we asked participants to pick their own preferred task types between watching engaging (N=11) or disengaging videos (N=10).
\autoref{fig:duration_result_breakdown} presents the breakdown of the task type in \autoref{fig:duration_result}(a).
We fitted GLMMs with task type as another main factor and observed a marginal significance of the interaction between the intervention method and the task type ($\chi^2_1=3.27, p=0.07<0.1$). This was only during the intervention stage, but not the post-intervention stage.
\autoref{fig:duration_result_breakdown}(a) and (b) indicate that the advantage of \projectname during the intervention stage was more salient when participants were watching engaging videos ($\Delta=42.3\pm49.6\%$) compared to when they were watching disengaging videos ($\Delta=14.2\pm22.5\%$).
This could be due to the fact that participants were more interruptable or receptive in less engaging tasks~\cite{pielot2017beyond,mishra_detecting_2021,choi_multi-stage_2019}, thus even a basic rule-based intervention could effectively reduce the target actions. However, in more engaging tasks, accurate and just-in-time reminders are more effective than basic ones.

\input{fig_tab_alg/fig_intervention_breakdown}

\subsubsection{Survey Outcomes.}
In addition to the objective measurement, we also compare participants' subjective reports on the SUS, WAI, and the change of the habit strength.
Overall, participants reported that \projectname had better usability (SUS: 73.3$\pm$12.8) than the baseline (66.8$\pm$15.9), with significance through a Wilcoxon rank sum test ($p<0.05$).
\projectname achieved a SUS score over 70, indicating acceptable usability. In both methods, false positive notifications were inevitable and could introduce participants' confusion or surprise, which could explain the subpar SUS scores in general.

Interestingly, the results of WAI and habit strength did not indicate such a difference. Participants had similar reports of the relationship with the system (WAI score: 42.1$\pm$7.1 for the \projectname \vs 41.9$\pm$9.3 for the baseline, $p=0.58$).
The change of habit strength between the pre- and post-intervention stages is also minimal ($\Delta$ of habit strength score: $-4.2\pm5.5$ for the \projectname \vs $-5.5\pm6.6$ for the baseline, $p=0.25$).
This was probably due to the fact that the intervention sessions were not long enough to form a long-term alliance between users and the system, or to influence longitudinal behaviors or habits.
Our qualitative results from semi-structured interviews provide more nuanced insights into these results.


\subsection{Qualitative Results}
\label{sub:intervention_evaluation:qualitative_results}
% Moreover, our interview results also revealed interesting insights into \projectname that enables a human-AI collaborative intervention experience.
All interviews were recorded and transcribed. We adopted a simple content analysis framework~\cite{prior2014content}. One author took extensive notes during the interviews, went through the scripts to categorize themes and count their frequency, and discussed with two other authors until convergence.
We summarize our key findings below.

% \orson{@yancheng to add more details and quotes.}

\subsubsection{Perception of AI-powered Intervention}
Multiple participants reported that the AI-powered system possessed a sense of presence or "\textit{having a soul}". For instance, P10 noted, "\textit{[\projectname{}] resembles a habit instructor, or even like my mom... who would gently remind me when I scratch my head.}"
P18 remarked, "\textit{This system seems to read my mind, anticipating when I'm about to bite my lips and reminding me just in time. Sometimes I felt like I was sneaking around when making these actions.}"
Compared to the rule-based condition, \projectname's interventions appeared to foster greater self-reflection among users.
Notably, P19 even perceived the AI's reminders as rewards: "\textit{After being caught [touching my face] several times initially, I managed to control myself for a while. Then, even if the system reminded me again, I felt it was affirming my progress, like receiving a reward.}"
In contrast, the rule-based condition yielded opposite effects, "\textit{This mode of notification felt random to me - it was just like a machine}" (P02).

However, some participants also had a negative experience with \projectname, especially when it did not detect the actions accurately (mostly false positive). For example, P08 mentioned that \projectname had limited impact, and that they also felt a sense of distrust. "\textit{At first, when it reported errors a few times, I tried to look for reasons elsewhere. But it kept making mistakes, which became frustrating. When it occasionally got something right, I thought it was just luck!}" Participants could lose trust in \projectname when the system made mistakes at the beginning of their interaction.
This is supported by prior research in other human-AI interaction systems~\cite{swaroop2024accuracy,jacobs2021designing}.


\subsubsection{Illusory Amplification of Intervention Strength}
We noticed a surprisingly interesting phenomenon: 
Several users (P09, P10, and P16) reported that the vibration strength of the AI-based intervention in \projectname felt stronger than that of the rule-based intervention. However, the vibration setup was identical in the two sessions.
Even after we explained the specific intervention methods after the two study sessions, P16 stated, "\textit{Not only did I subjectively feel that Mode B [our \projectname method] gave me a stronger sense of motion restraint, but it also seemed to vibrate more intensely. Are you sure it's really the same setting?}"
This indicated that participants might develop an illusory or distorted perception of the intervention's strength when the interventions were delivered just-in-time.
We discuss this more in Sec.~\ref{sub:discussion:distorted}.

\subsubsection{Diverse Patterns of Human-AI Collaborative Relationship}
Users exhibited diverse patterns of engagement with the AI system. Some participants demonstrated adaptive behavioral modification in response to \projectname's reminders. As P14 described, "\textit{Every time I shook my leg, it would remind me, which made me increasingly hesitant to move}". This was aligned with our original design goal of introducing AI-powered JITI.

Other than reducing the target actions, we also observed other behavior patterns. One pattern emerged where participants developed an interesting competitive relationship with the AI for user agency. For instance, P8 articulated this sentiment: "\textit{I wanted to compete with it - I tried to resist the urge just so it wouldn't catch me.}" This competitive spirit evolved into experimental behavior for some users, who attempted to understand and control the system's underlying logic. P18's experience exemplified this progression: "\textit{Initially, I felt caught red-handed with every reminder. Later, I noticed it wouldn't always detect my subtle movements, so I started experimenting with the notification logic, trying to gain control over the reminders. Eventually, though, I made peace with it and lost the urge to perform the action altogether.}" These participants wanted to gain better agency in this human-AI relationship.

In addition, some participants developed playful interactions with the system, treating it as an engaging companion rather than a mere monitoring tool. For example, P17 shared: "\textit{When the video was boring... I just wanted to goof around a bit. This thing was actually keeping an eye on me, so I'd mess with it for fun, play around with it, and boom - it would react right away. Kinda helped wake me up a bit? It was basically like playing a game.}"
Overall, these diverse patterns between users and \projectname suggest a set of potential collaborative relationships between the two sides. We discuss this finding in Sec.~\ref{sub:discussion:collaboration}.