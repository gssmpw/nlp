\section{\projectname Design}
\label{sec:methods}
We designed a few-shot learning pipeline to enable users to define their own undesirable actions with a small number of examples.
We introduce our technical pipeline (Sec.~\ref{sub:methods:system}), the interface and intervention experience design (Sec.~\ref{sub:methods:interface}), as well as the implementation details of \projectname (Sec.~\ref{sub:methods:implementation}).

\subsection{Few-shot Learning Pipeline}
\label{sub:methods:system}
% model
To achieve the goal of learning user-defined undesirable actions with few-shot samples, we designed and implemented a three-stage pipeline building on public models and datasets.
\autoref{fig:customization_pipeline} visualizes the overall structure of the pipeline.
To ensure compatibility with existing public models and datasets, we used tri-axial accelerometer data from the IMU, sampled at 30 Hz.

\subsubsection{Stage 1: Pre-trained SSL Model}
\label{subsub:methods:system:pretrain}

One of the primary challenges in deep learning for human activity recognition (HAR) is the lack of large labeled datasets \cite{yuan2024self, leng2024imugpt}.
Although there exist multiple public human activity recognition (HAR) datasets with ground truth labels, they often have limited size and different sensing modalities, data sizes, task definitions, and collection protocols (e.g., \cite{hu2020fine,chen2015utd,opportunity_activity_recognition_226,matey2023dataset,ada_alevizaki_2022_7092553,mallol_ragolta_2022_6517688,alexander_holzemann_2024_7654684,herrera2019monitoring,ofli2013berkeley,de2009guide,kirmizis_2021_4507451,kyritsis_2021_4421861,vaizman2017recognizing, hu_2022_7058383}). Therefore, it is challenging to unify these datasets into a single large-scale dataset for pre-training a supervised-learning based model.
SSL addresses these challenges by leveraging vast amounts of unlabeled data to learn meaningful representations through pretext tasks, making it particularly well-suited for HAR tasks \cite{saeed2019multi}.

We adopted a pre-trained model developed by \citet{yuan2024self}, which was trained using multi-task self-supervised learning on the UK Biobank activity tracker dataset \cite{doherty2017large}. The dataset contains over 700,000 person-days of unlabeled wearable sensor data collected from free-living activities via a wrist-worn accelerometer.
This dataset setup fits well into our target application scenarios.
\autoref{fig:customization_pipeline}(A) provides a high-level overview of the architecture of the pre-trained model, which includes a five-layer ResNet-based feature extractor \cite{he2016deep} followed by two fully connected layers.
The model was pre-trained on three fixed-length signals (\ie 5 sec, 10 sec, 30 sec) at 30 Hz.
Three data augmentation techniques \cite{um2017data}, including Arrow of time (AoT), Permutation, and Time warping (TW), were used in pre-training process as three self-supervised tasks \cite{saeed2019multi}.
This pre-trained model is publicly available\footnote{See the model and implementation at: \hyperlink{https://github.com/OxWearables/ssl-wearables}{https://github.com/OxWearables/ssl-wearables}}.

\input{fig_tab_alg/fig_customization_pipeline}

\subsubsection{Stage 2: Model Finetuning}
\label{subsub:methods:system:finetune}

While direct feature extraction from IMU signals using the pre-trained model has demonstrated improved performance in downstream classification tasks~\cite{yuan2024self}, there is a significant gap between the pre-trained tasks (\ie mostly coarse-grained human activities that involves large range of motion) and our target customization tasks (\ie actions with fine-grained activity).
This brings up the need for better model adaptation to bridge this discrepancy.
To address this gap, we finetuned the pre-trained SSL model using two datasets curated by \citet{hu_2022_7058383} and \citet{bhattacharya2022leveraging} that are closer to our use cases in a supervised manner\footnote{The two datasets are available at \hyperlink{https://zenodo.org/records/7058383}{https://zenodo.org/records/7058383} and \hyperlink{https://doi.org/10.18738/T8/NNDFQD}{https://doi.org/10.18738/T8/NNDFQD}}.
These datasets contain labeled, fine-grained, hand-specific human activity data.
% : the \textit{Vibration and IMU Sensing Human Activity Dataset}  and \textit{IEHA} 
% dataset and preprocess

We pre-processed the datasets before merging them together. We first unified their units, resampled them to 30 Hz to maintain uniformity across datasets, and then applied z-score normalization to standardize the signal. After these steps, we adopted a 5-second sliding window with a step size of 0.1 second, because a 5-second window fits one of the pre-training signal lengths in the SSL model in Sec.~\ref{subsub:methods:system:pretrain} and is appropriate for our use cases (the same window size as described in Sec.~\ref{subsub:methods:system:fewshot}).
As for the labels, we manually unified activities with the same semantics (\eg handwriting in \cite{hu_2022_7058383} and writing in \cite{bhattacharya2022leveraging}), resulting in a combined dataset with 26 activity classes plus one negative class (no target action).


To establish a more comprehensive and diverse representation of the negative class, we recruited a small group of participants (N=10), each performing 30 minutes of regular indoor daily activities, such as sleeping, playing computer games, studying, and cooking. Participants were specifically instructed and supervised to minimize potentially undesirable micro-actions to ensure the quality of negative data. We manually remove the improper data episodes from the data. This data was then sampled for the subsequence training process. This step enables the model to learn from more comprehensive and diverse negative class samples, thereby reducing false positives.
% add negative data description collected by cyc

% training process
Combining all these datasets together resulted in a dataset with about 12.5~hours of positive signals and 5 hours of negative signals (before sampling) in total. Using this combined dataset, we performed finetuning on the pre-trained model, with all layers activated (see \autoref{fig:customization_pipeline}(B)).
% This decision to activate all layers for finetuning was achieved through an pre-experiment ablation study using only the training dataset, which showed that it would lead to better performance.
We adopted weighted cross-entropy as the loss function to address the class imbalance problem.
% - while the positive classes were relatively balanced, the negative class was highly overrepresented, accounting for approximately 11 times the smallest positive class. This weighting strategy helped the model learn balanced representations across all classes, improving its ability to generalize to underrepresented categories.


\subsubsection{Stage 3: Few-Shot Model Customization}  
\label{subsub:methods:system:fewshot}

% gap & solution
As mentioned earlier, in real-world applications, it is often impractical for users to provide a large number of samples of a self-defined action for model training.
% To address this limitation, our approach combines transfer learning and few-shot learning, with the goal of distinguishing each customized action from negative actions using no more than 10 samples provided for training.
% algorithm
% Building on the existing seven models with good performance, we create a new classifier after the feature embedding extraction layers for this new tasks. 
To achieve individual customization, our few-shot learning procedure was designed to train a new prediction head with lightweight layers built upon the finetuned model from Sec.~\ref{subsub:methods:system:finetune}.
For easy understanding, we will explain our pipeline with the case of adding one intervention action (\ie binary classification) in detail, starting with data collection and then the few-shot learning process. The scenarios with multiple actions adopt the same method.

We implemented a simple, user-friendly data collection process for customization, where a user would follow instructions on the wearable device to repeat the target action several times (N shots, 10 seconds each time), with a short period of 5-second pause or other activities (negative class) between the two repetitions, as shown in \autoref{fig:customization_pipeline}(C).
Given such a signal sequence, we applied the same sliding window process as in Sec.~\ref{subsub:methods:system:finetune} (5-second width and 0.1-second step).
Each window was labeled as positive if it contained more than 3 seconds of the target action; otherwise, it was annotated as negative.

We then introduced a signal processing procedure, including both data augmentation and data synthesis, to enhance our data for model training.
First, we adopted six data augmentation techniques~\cite{iglesias2023data}:
% zooming, scaling, time warping, adding noise, time reversal, and frequency domain transform.
1) zooming, to simulate variations in action speed, randomly selected from $\times0.9$ to $\times1$;
2) scaling, to represent variations in action intensity, with the scaling factor $s \sim \mathcal{N}(1,0.2^2), s \in [0,2]$;
3) time warping, to simulate action temporal variance, using 2 interpolation knots and a warping randomness $w \sim \mathcal{N}(1,0.05^2), w \in [0,2]$;
4) time reversal, to simulate temporal variation, by reversing the action's time sequence;
5) time-domain noise, to simulate sensor inaccuracies or environmental disturbances, Gaussian noise with a noise level of 0.01 is added to the original data;
and 6) frequency-domain noise, to simulate frequency variation or external interferences. We add the random noise to the frequency components of the signal (after Fourier transform) and then transformed the signal back to the time domain (with inverse Fourier transform).
We went through all combinations of these six augmentation technique steps, which increased the size of the data by $2^6-1$ times.

Next, we designed a data synthesis step. To create additional samples that simulate short episodes of target undesirable actions, we artificially concatenated short segments of the target undesirable action with negative episodes (no target undesirable actions).
The positive segments (of the target undesirable actions) were chosen randomly from the 10-second continuous recordings and would have varying lengths sampled from [3, 4.9] seconds.  
The starting position of the positive episode was also randomized within a 5-second window, with negative episodes padding at the beginning and the end.
This step further increased the sample size by about 80 times.
In total, our data augmentation and data synthesis steps enlarged the original few-shot samples by about 143 times.

Finally, to customize the model to recognize a new target undesirable action, we further trained the finetuned model with a new classification head with the total set of data. The head was trained for a binary classification when adding one intervention action, and N+1-class classification when adding N new actions.

In the real-time system, the final classification model followed the same sliding window setup and performed classification at 10 Hz (0.1-second step).
To improve the system robustness, we added a smoothing step with the threshold as 3 based on grid search, \ie the system will only recognize a target action if there is a consecutive sequence of positive outcomes from 3 windows.


\subsection{Intervention Design}
\label{sub:methods:interface}
Building upon the customized model, we then developed a real-time intervention system on the smartwatch.
As introduced in Sec.~\ref{subsub:methods:system:fewshot}, in the initial customization process, a user would go through a simple data recording to collect a few samples of the undesirable action.
\autoref{subfig:interface_design:data_collection} shows the interface on the smartwatch to select or define an undesirable action and set up the number of shots for customization.
% Add a figure of the watch interface of data collection (where users can select the number of shots?);

Once a model was trained following the 3-stage pipeline, the user could enter the live-stream mode to receive interventions.
Whenever the target action was detected, the watch would send a reminder notification with vibration. \autoref{subfig:interface_design:intervention} shows the interface of the intervention.
The user could click a button to dismiss the reminder.
% To avoid delivering overwhelming interventions, we set a 5-minute cool-down time, \ie once an intervention was delivered, the system would remain silent for 5 minutes.
% Cool down time
To avoid delivering overwhelming interventions, we set a 5-minute cool-down time, \ie at most, one intervention can be delivered during this interval.
% Additionally, a mandatory 20-minute interval was established: if the system remains silent - failing to detect any targeted actions - throughout this period, it will automatically send an intervention, both to reinforce the interventions effect and to compensate for occasional algorithmic inaccuracies (\ie false negatives). 

\input{fig_tab_alg/fig_interface_design}

\subsection{System Implementation}
\label{sub:methods:implementation}
% We adopted a client-server setup for the system implementation. We implemented the interface on the Google Pixel Watch 2, which acted as a client and streamed the 30~Hz accelerometer data to a server in real time.
% The server had completed the Stage 1 and 2 in Sec.~\ref{sub:methods:system} beforehand. 
% Once the customization data was collected, we used an A100 GPU for few-shot model training.
% The final model was then used for real-time inference.

We adopted a client-server architecture for the system implementation to enable efficient data transmission and processing. The interface was implemented on the Google Pixel Watch 2, which acted as the client device. It continuously streamed the accelerometer data, collected at 30 Hz, to a dedicated server in real time via a socket communication protocol. 

Before real-time data transmission, the server had already completed the initial setup stages, namely Stage 1 and Stage 2, as described in Sec.~\ref{sub:methods:system}. Therefore, once the customization data from the client was collected, we utilized an A100 GPU to perform the few-shot custom model training, enabling rapid adaptation to new data with minimal samples. 

After training the model on the server, we deployed the final model for real-time inference. The inference process ran on the server, and the results were transmitted back to the client for immediate feedback, enabling efficient and responsive action recognition and the delivery of JITI. 

