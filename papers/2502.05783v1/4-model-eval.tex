\section{Model Evaluation}
\label{sec:model_evaluation}
In this section, we report the evaluation of \projectname's few-shot learning pipeline offline performance. We will further elaborate on the evaluation of \projectname's intervention effectiveness in section \ref{sec:intervention_evaluation}.

\subsection{Data Collection}
\label{sub:model_evaluation:data_collection}
\subsubsection{Participants}
We recruited 26 users (14 females, 12 males, age 22$\pm$2) for data collection via social media platforms.
We focused on users who were aware of their own undesirable actions and had the intention to reduce these actions. These are the target users of our intervention system.
Our study was IRB-approved by the local institution, and participants were compensated with \$10 for this data collection study (around 45 minutes).

\subsubsection{Personal Undesirable Action Customization}

Participants were asked to record five pre-determined target actions that are commonly recognized as undesirable actions~\cite{teng2002body,oshio2018shake}, including \textit{Face Scratching}, \textit{Nail Biting}, \textit{Eye Rubbing}, \textit{Lip Tearing}, and \textit{Leg Shaking}. The first five figures in \autoref{fig:evaluation_actions} illustrate these actions.

Moreover, each participant was asked to define a new undesirable action tailored to their own personal needs.
In total, 26 participants designed an additional set of 12 actions, including \textit{Finger Lipping} (designed by N=5 participants), \textit{Head Scratching} (N=5), \textit{Nose Rubbing} (N=4), \textit{Finger Picking} (N=3), \textit{Hair Scratching} (N=2), \textit{Face Rubbing} (N=1), \textit{Finger Biting} (N=1), \textit{Hair Pulling} (N=1), \textit{Hair Rubbing} (N=1), \textit{Lip Biting} (N=1), \textit{Nail Picking} (N=1), and \textit{Neck Scar Scratching} (N=1).
We only grouped identical actions and distinguished actions as long as they differed slightly. For instance, \textit{Head Scratching} and  \textit{Hair Scratching} were similar, but one involved contacts between fingers and 
scalp, while the other one did not.
Similarly, \textit{Finger Picking} and \textit{Nail Picking} were also quite close, yet one solely focused on the skin on the finger, while the other focused on nails.
These actions were visualized in the second half of \autoref{fig:evaluation_actions}.


\input{fig_tab_alg/fig_evaluation_actions}

\subsubsection{Data Collection Procedure}
For each action, participants followed a consistent protocol (briefly mentioned in Sec.~\ref{subsub:methods:system:fewshot}) comprising two phases per shot: a 5-second \textit{free mode} and a 10-second \textit{record mode}. In the free mode, participants were free to rest or perform natural daily activities (negative data). Once entering the record mode, they performed the target actions (positive data).
This process was repeated across five rounds, with each round consisting of five consecutive shots.
Participants took a short break between two rounds to prevent physical fatigue and were asked to freely adjust the watch position between each round to increase data variance.
In total, we collected 25 shots for each target action.
Moreover, we leveraged the onboarding process at the beginning of the data collection to passively record participants' natural activities (about 5 minutes). This was used as additional data to augment the negative class\footnote{In real-world applications, we envision that such negative data can also be passively collected and implicitly embedded in the instruction process, thereby introducing minimal additional workload for the user}.

The \textit{free mode} segment was labeled as negative data, while the \textit{record mode} segment was labeled as positive data.
To prevent data contamination, the first two seconds during the record mode were excluded from training because these recordings were mixed with postural changes and arm movement.


\subsection{Offline Performance Evaluation}
\label{sub:model_evaluation:pipeline_evaluation}
We evaluated our pipeline by adding one or more actions as target actions.
For each action, we randomly selected two rounds of recordings as the training set (up to 10 shots), one round as the validation set (5 shots), and the remaining two rounds as the test set (10 shots). We repeated the training three times and calculated the average performance.

It is noteworthy that the model performance has two aspects: the window level and the action level. 
For the window level, each sliding window is counted as a binary classification data point (same as the model training process).
For the action level, windows are aggregated with a smoothing threshold of 3 (Sec.~\ref{subsub:methods:system:fewshot}) and represent a closer experience as real-life applications. Such aggregation significantly reduces the false negative and false positive.

\subsubsection{Prediction Performance with Different Number of Shots and Actions}
\label{subsub:model_evaluation:pipeline_evaluation:shots and action}
We evaluated the model performance by training on one to ten shots of the data.
For action recognition, we started by adding one action for each participant (\ie training binary classification models).
To evaluate the performance of multi-class classification models, we also experimented with customizing multiple actions (up to six, as each participant recorded five pre-designed actions and one custom action). This led to a total number of 63 combinations from one to six actions (\(\sum_{k=1}^6 \binom{6}{k}\)).
% We conducted this evaluation by systematically exploring all possible combinations of actions, ranging from one new action to six new actions. Subsequently, we trained models using different numbers of training samples, with the number of shots varying from one to ten.
In total, we trained and evaluated 49,140 models = 10 shot numbers $\times$ 63 action combinations $\times$ 26 participants $\times$ 3 repetitions. 
% We keep the original recognition results on these sliding window samples as  window level

We mainly focused on the action-level performance. \autoref{tab:action_shot_study} presents both the window-level and action-level results.
As shown in \autoref{fig:all_result_ges_num_action}, when using only one shot to add a new action (\ie the user performs the action only once), our framework achieved an average accuracy of 76.8\% and an F1 score of 74.8\%.
The recognition performance became better with more shots for training the model. With five shots of a new action, our framework attained an average accuracy of 84.7\% and an F1 score of 84.2\%. When using ten shots, our model's performance achieved 87.7\% and 87.2\%, respectively.

Recognizing multiple new actions simultaneously presented a greater challenge. However, compared to the performance of adding one action with five shots (84.7\% and 84.2\%), introducing three new actions (\ie four-class classification) with five shots each, the framework maintained a good average accuracy of 79.1\% and an F1 score of 78.1\%.
Even with six additional new actions and five shots each, the framework still achieved an average accuracy of 73.7\% and an F1 score of 72.3\%.
These results demonstrated the robustness and effectiveness of our pipeline for data-efficient action recognition.

\input{fig_tab_alg/fig_model_evaluation_action_shot}

% We report both window level and action level results in \autoref{tab:action_shot_study}
\input{fig_tab_alg/tab_action_shot}

\subsubsection{Prediction Performance of Each New Gesture with Different Number of Shots}
\label{subsub:model_evaluation:pipeline_evaluation:one new gesture}


We further compared the recognition performance across actions.
As shown in \autoref{fig:all_result_shot_num_action}, most of the 17 actions exhibited good performance. Using only one shot, about half of the actions achieved an F1 score above 75\%. When the number of shots increased to five, 14 out of 17 actions surpassed this threshold. With ten shots, performance improved further for most actions, with 12 out of 17 actions achieving an F1 score above 85\%.
\textit{Hair Pulling} appeared to be an exception. Its performance did not improve with more samples after five shots. This was probably due to the overly large variance of the \textit{Hair Pulling} action, even performed by the same individual, and it was challenging for a model to achieve reliable performance even with a limited amount of additional data.

Overall, these results indicate our framework has good learning ability for new actions.

\input{fig_tab_alg/fig_model_evaluation_each_action}

% \subsubsection{Ablation Studies of Pipeline Stages}
% \label{subsub:model_evaluation:pipeline_evaluation:ablation}
% \orson{@Ying to add more details}

% To measure the effectiveness of different stages in our few-shot pipeline, we further conducted an ablation study by removing different stages in our pipeline.
% \autoref{tab:ablation_study} indicates xxxx. \orson{@Ying to add more details of the results interpretation.}

% \input{fig_tab_alg/tab_ablation}