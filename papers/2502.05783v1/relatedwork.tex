\section{Related Work}
\label{sec:background}

In this section, we first provide a general overview of just-in-time behavior intervention, and then a review of prior work in hand gesture recognition based on wearable devices. 

\subsection{Sensing-based Just-in-Time Intervention (JITI)}

% Something like: The concept of just-in-time intervention was introduced in .... with the goal to achieve xxxx....
Advances in mobile sensing technologies enable the unobtrusive real-time monitoring of individual states and environmental contexts, while delivering proactive cues and user-specific information \cite{choi2019multi}.
Such advances facilitate the implementation of just-in-time intervention (JITI) \cite{nahum2018just}, with the goal of delivering timely and appropriate support for users.  
% Early work are rule-based to achieve "just-in-time". For example, xxx
Early research has applied JITI to address a variety of health-related issues using rule-based approaches \cite{choi2019multi, sun2020beactive, haliburton2023exploring, luo2018time, kim2019lockntype, kim2019goalkeeper, howe2022design, raether2022evaluating, hsu2014persuasive}.
These approaches usually depended on predefined sets of rules and conditions to trigger interventions, which are typically defined by domain experts. Examples include event-based rules \cite{kim2019lockntype, kim2019goalkeeper, raether2022evaluating}, time-based rules \cite{sun2020beactive, haliburton2023exploring, luo2018time}, combinations of multiple rules \cite{howe2022design}, multi-stage rules \cite{choi2019multi}, to name a few. 

% More recently, with the development of AI techniques, a growing number of studies have started to apply AI for JITI. For instance, xxx
Recently, with the advancement of AI techniques, an increasing number of studies have started to apply AI for JITI \cite{orzikulova2024time2stop, wu2024mindshift, xu2022typeout, li2024stayfocused, sarker2014assessing, lee2017itchtector, liao2020personalized, kim2022prediction, rojas2021scalable, alharbi2023smokemon}. In contrast to rule-based JITI, AI-based approaches utilize large-scale user behavior data and trained AI/ML models to determine optimal intervention timing and personalized interventions.
For instance, Time2Stop \cite{orzikulova2024time2stop} employs machine learning to develop an adaptive, explainable intervention system for smartphone overuse that determines optimal timings, offers transparent AI explanations, and integrates user feedback to improve the model over time. Rabbi et al. \cite{rabbi2015mybehavior} and Liao et al. \cite{liao2020personalized} incorporated reinforcement learning algorithms into JITI systems to personalize the model for each user, enhancing the effectiveness of physical activity interventions.

% However, they mainly focused on pre-determined "common" behaviors.
% ours: personalized gestures

However, these studies primarily focused on predefined ``common'' health behaviors that are broadly applicable to large populations, failing to address personal/idiosyncratic undesirable behaviors that are specific and unique to individual users \cite{stein1998phenomenology, snorrason2012skin,teng2002body,oshio2018shake,stein2008trichotillomania}. 
% Meanwhile, these behaviors often vary widely across individuals \cite{xu2022enabling}, influenced by diverse physical, psychological, social, and environmental factors \cite{trapp2015individual}.
Idiosyncratic behaviors naturally mean that the sample size (from a single individual) would be much more limited than other common health behaviors, posing challenges to training AI models for intelligent intervention systems.
The limitation of existing solutions reduces the intervention systems' ability to adapt to personal behaviors.
% influenced by various physical, psychological, social, and environmental factors \cite{trapp2015individual}.
To bridge this gap, our work proposes a personalized intervention approach to deliver customized JITI for user-defined undesirable actions.

% A potential section: Behavior Change Theory behind JITI (maybe Randy could be helpful here)


\subsection{Wearables for Hand Gesture Recognition and Customization}
% You can merge Sec 2.1 and 2.2 in the CHI 2022 Apple paper, pretending that there is no the CHI'22 paper.
% Then something like: Building on top of our prior work of gesture customization (masked now for anonymization), we developed an open-sourced few-shot learning pipeline that is built on public datasets. Our system enables xxxx.

% The area of hand gesture recognition using wearable technologies has been explored extensively. A wide range of sensing technologies are leveraged to capture hand gestures. Examples include cameras~\cite{gong_wristwhirl_2016,kim_digits_2012,wu_back-hand-pose_2020,yeo_opisthenar_2019,hu_fingertrak_2020}, IR sensors~\cite{mcintosh_sensir_2017,georgi_recognizing_2015,fukui_hand_2011}, microphones~\cite{nandakumar_fingerio_2016,laput_sensing_2019, harrison_skinput_2010,iravantchi_beamband_2019,iravantchi_interferi_2019}, electromyography (EMG) sensors~\cite{saponas_enabling_2009,saponas_demonstrating_2008}, electrical impedance tomography~\cite{zhang_tomo_2015}, pressure sensors~\cite{dementyev_wristflex_2014,jung_wearable_2015}, stretch sensors~\cite{strohmeier_flick_2012}, magnetic sensors~\cite{chen_finexus_2016,chen_utrack_2013,parizi_auraring_2019,yang_magic_2012,kienzle_lightring_2014}, and bio-capacitive sensors~\cite{rekimoto_gesturewrist_2001,truong_capband_2018,sato_touche_2012}.
% Among these sensors, IMU is arguably one of the most low-cost and widely available sensors embedded in nowadays' commodity wearable devices. It is often employed to capture dynamic hand gestures that involve arm or hand motion~\cite{laput_viband_2016,wen_serendipity_2016,xu_finger-writing_2015,akl_novel_2011,kim_imu_2019}. We focused on the IMU sensor in our research to ensure scalability and generalizability.

% Early trajectory-based gesture recognition methods, such as dynamic time warping (DTW)~\cite{liu_uwave_2009} and hidden Markov model (HMM)~\cite{mckenna_comparison_2004}, can recognize gesture trajectories (\eg line, square, circle, star~\cite{mckenna_comparison_2004}) using few samples and achieve high accuracy.
% However, for gestures that are more sophisticated and fine-grained, the methods become more complicated: A modern ML-based gesture recognition system typically starts from defining a set of gestures and implementing sensors, if needed. Then, researchers conduct user studies with multiple participants to collect data of the gesture set, and train an ML model.
% The dataset usually contains hundreds or thousands of gesture samples. Depending on its size, traditional models such as support vector machine (SVM) and random forest, \eg \cite{georgi_recognizing_2015,iravantchi_beamband_2019}, or deep learning models such as CNN, \eg \cite{hu_fingertrak_2020,yeo_opisthenar_2019}, are learned to recognize the gestures. 

The field of hand gesture recognition or activity recognition using wearable technologies has been extensively studied, utilizing a range of sensing techniques (\eg vision \cite{gong_wristwhirl_2016,kim_digits_2012,wu_back-hand-pose_2020,yeo_opisthenar_2019,hu_fingertrak_2020, nguyen2023hand, qi2024computer,xia2024ts2act}, sound wave \cite{nandakumar_fingerio_2016,laput_sensing_2019,harrison_skinput_2010,iravantchi_beamband_2019,iravantchi_interferi_2019, lee2024echowrist, li2023enabling}, electromyography \cite{saponas_enabling_2009,saponas_demonstrating_2008, meng2022user,chamberland2023novel}, pressure or stretch \cite{dementyev_wristflex_2014,jung_wearable_2015,strohmeier_flick_2012, si2022flexible, delpreto2022wearable}, magnetism \cite{chen_finexus_2016,chen_utrack_2013,parizi_auraring_2019,yang_magic_2012,kienzle_lightring_2014,sluyters2022hand,byberi2023glovesense}). 
Among them, motion data (\eg acceleration, angular velocity) collected by IMUs are particularly notable for their effectiveness in capturing dynamic hand gestures ~\cite{laput_viband_2016,wen_serendipity_2016,xu_finger-writing_2015,akl_novel_2011,kim_imu_2019, li2023signring,sharma2023sparseimu}. Coupled with their cost-effectiveness and widespread availability in commercial wearable devices, IMUs' are the best choice of sensor for a JITI system to ensure effectiveness, ubiquity and generalizability.

Existing gesture recognition approaches can be broadly categorized into trajectory-based and ML-based. 
Early trajectory-based methods \cite{liu_uwave_2009, mckenna_comparison_2004} achieve high accuracy with relatively few samples, but they are limited in recognizing more complex gesture trajectories \cite{mckenna_comparison_2004}. 
For more sophisticated and fine-grained gestures, ML-based methods are more suitable, but are typically heavily data-driven, requiring a large number of samples of a pre-defined gesture set to train either a traditional model \cite{georgi_recognizing_2015, iravantchi_beamband_2019,hu2020fine,chen2015utd} or a deep learning model \cite{hu_fingertrak_2020,yeo_opisthenar_2019,li2023signring,leng2024imugpt,shen2024mousering} depending on the dataset size. 

In addition to recognizing gestures from predefined sets, some systems support the customization of user-defined gestures, which enhances memorability~\cite{nacenta_memorability_2013}, interaction efficiency~\cite{ouyang_bootstrapping_2012}, and accessibility for individuals with physical disabilities~\cite{anthony_analyzing_2013}. Most notably, incorporating gesture customization in our system enables users to define their own undesirable gestures for intervention.  
However, to ensure a seamless user experience, the data collection process for new gestures must be efficient and limited in scale (\eg no more than 10 samples). Existing approaches (\eg rule-based methods~\cite{avrahami_guided_2001,doring_gestural_2011} and computational techniques~\cite{lou_personalized_2017,anthony_lightweight_2010, mckenna_comparison_2004,ouyang_bootstrapping_2012}) meet this requirement, but are limited to recognizing hand gestures with significant motion, where IMU signals are distinct, and the recognition task is relatively straightforward. 
To identify more sophisticated and fine-grained gestures or actions, there are two common approaches: 1) collect more data of the new gesture to further train models, or 2) fine-tune a pre-trained base model use a limited amount of new samples. Since the first option would impact user experience in real-life applications, the fine-tuning approach is more appropriate. However, fine-tuning a robust fine-grained gesture recognition model with a small number of samples remains a challenging task \cite{stewart_online_2020,wu_one_2012,rahimian_few-shot_2021, xu2022enabling, rahimian2021fs,zou2024pregesnet}. Most related to our work, Xu et al.~\cite{xu2022enabling} collected extensive gesture data to pre-train a robust model and used few-shot samples to fine-tune new gestures, but this work focuses on short-duration gestures (less than one second), which may limit its potential to support users in defining their own undesirable actions for intervention. Furthermore, the work is closed-sourced, with both its dataset and pre-trained model unavailable for public use, hindering reproducibility and broader applicability.

% The advantages of supporting customized, user-defined gestures include but are not limited to greater memorability~\cite{nacenta_memorability_2013}, higher interaction efficiency~\cite{ouyang_bootstrapping_2012}, and better accessibility for people with physical disabilities~\cite{anthony_analyzing_2013}. 
% Some prior works explored and summarized customized gesture sets through user elicitation studies (\eg \cite{wobbrock_user-defined_2009,ruiz_user-defined_2011,piumsomboon_user-defined_2013}), and some built tools that facilitate the creation of new customized gestures (\eg \cite{yang_magic_2012,bau_octopocus_2008,oh_challenges_2013}).

% To enable gesture customization user experience, the data collection process of the new gestures needs to be quick and small-scale.
% Several systems have included the support for gesture customization. Some is simply rule-based ~\cite{avrahami_guided_2001,doring_gestural_2011}, others used computational methods~\cite{lou_personalized_2017,anthony_lightweight_2010, mckenna_comparison_2004,ouyang_bootstrapping_2012}.
% More related to our work, uWave~\cite{liu_uwave_2009} stored a piece of template hand-hold accelerometer signals of the new gesture, and used DTW to compare against the new data stream to detect new gestures.
% Bigdelou \etal~\cite{bigdelou_adaptive_2012} applied Laplacian Eigenmaps and kernel regression on arm-worn IMU signals to achieve the recognition of personalized gesture sets. 
% Mezari \etal~\cite{mezari_easily_2018} processed new gestures' accelerometer data with fast Fourier transformation and symbolic aggregate approximation, and then used simple distance metrics to recognize new gestures.

% However, although these systems only need a small training dataset, they can only work with hand gestures that involve significant hand motion, where IMU signals are distinguishable and the recognition tasks are easy.
% More importantly, none of them explored the problem of extending a trained model to recognize new gestures.
% As we will show in Section~\ref{sec:model-evaluation}, traditional methods have poor performance when they are applied to complex fine-grained gestures.

% A straightforward solution is to collect a good amount of the new gestures' data and re-trained the model. However, the time- and energy-consuming data collection process introduce prohibitively high cost to an end-user.
% Another solution is to fine-tune the base model on the new small training set. 
% However, training a high-performance dynamic gesture recognition model with a small amount of data (\eg less than 10 samples) is challenging yet a key step towards gesture customization.
% Very few works explored the feasibility of recognizing hand gestures with few samples using camera-based data~\cite{stewart_online_2020,wu_one_2012}, EMG signals~\cite{rahimian_few-shot_2021}. 

Building on top of prior work to address the challenges of gesture customization, we developed an open-sourced few-shot learning pipeline using public models and datasets. Our system enables the model to quickly adapt to new target gestures or actions with high accuracy using only a few samples.