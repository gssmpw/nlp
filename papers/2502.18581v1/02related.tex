% \section{Related Works}
% \subsection{Inference Time Scaling and Reward Models}

% Evaluating the quality of outputs from Large Language Models (LLMs) and deriving appropriate rewards for these responses is a crucial aspect in improving their performance. DeepSeek-R1 \cite{guo2025deepseek} enhance the base model through large-scale reinforcement learning by leveraging rewards obtained from rule-based evaluations and reward models. \citet{snell2024scaling} demonstrate that utilizing rewards from a PRM enables effective exploration of multiple reasoning paths, outperforming scaling model parameters under certain conditions. 

% Due to the inherent complexity, ambiguity, and contextual variability of natural language, rule-based evaluation methods are often insufficient for assessing response quality. As a result, reward models are commonly used to fill this gap. However, reward models are typically task-specific and highly sensitive to the choice of the base model \cite{wang2024math}.  Over time, reinforcement learning with a reward model may lead to reward hacking, necessitating frequent adjustments to maintain effectiveness, which complicates the maintenance of an active reward model. Similarly, while scaling test-time compute through path traversal and PRM selection shows advantages in computational cost compared to scaling model parameters, the high training cost of reward models can undermine these benefits. 

%  Researchers have explored various alternatives to replace reward models, such as directly utilizing human preference data (DPO), using larger models to evaluate smaller ones, or employing internal inspection methods. Our research falls into the latter category.

% \subsection{Self-Consistency and Internal Consistency Driven Algorithms}
% It is often assumed that after sufficient training, a model has gained a reasonable level of understanding regarding the correctness of its outputs \cite{liang2024internal}. Self-consistency, which leverages the consensus among multiple model outputs, has shown significant improvements by demonstrating that the aggregated judgment of the model is more reliable than that of a single sample. This suggests that models may not always require an external evaluator to judge the quality of their responses. However, self-consistency requires assessing whether two outputs reach the same final answer. This stringent requirement makes it difficult to generalize to open-ended generation. While USC \cite{chen2023universal} has been extended to other tasks, its poor scalability, and inability to provide certainty measures render it less suitable as a universal method for reward evaluation.

% Average log probability and perplexity are commonly used as measures of model confidence. However, the information they provide does not always lead to improvements. For example, on the GSM8K dataset, a 'normalized weighted sum'  is outperformed by 'unweighted sum',  and using these measures in beam search yields worse results than greedy decoding or sampling \cite{wang2022self}. This suggests that while confidence is often assumed to correlate with response quality, conventional confidence measures fail to capture this aspect effectively. 

% We argue that this failure stems from an oversimplification in measuring confidence, which often reduces it to the assigned probabilities of sampled tokens, neglecting the broader distributional context. The idea of incorporating information from the entire distribution generated by LLMs for certainty evaluation is rarely explored.  Although \citet{luo2024sed} utilize the entropy of the distribution in their work, they apply this information solely for "chaotic point detection" and do not test a variety of formulas. And they evaluate the responses of models by prompting them to self-evaluate. 

% In contrast, our approach to measuring certainty focuses on the full distribution generated by the model, providing a more comprehensive and accurate evaluation of response quality. As we demonstrate in this paper, this method overcomes the limitations of traditional confidence measures, offering a deeper understanding of model output quality.

\section{Related Works}
\paragraph{Reward Models for Response Reranking and Selection.}
Evaluating the quality of LLM outputs is crucial for improving performance on complex reasoning tasks. One common method is using external models, such as verifiers or reward models, to rerank and select responses based on predefined criteria. Reward models, like Outcome Reward Models (ORMs) and Process Reward Models (PRMs), help guide this selection. Research shows that incorporating reward models enhances reasoning and enables selecting the best samples from multiple candidates~\citep{lightman2023let, wang2024math}. However, reward models come with several limitations. They tend to be task-specific and are highly sensitive to the base model, making them less generalizable across different tasks~\citep{eisenstein2023helping}. Moreover, training reward models is computationally expensive, as recent studies suggest that reward models often require a similar number of parameters as the generating models to be effective~\citep{wang2024math}. In contrast, our approach does not require additional training. Self-certainty, our proposed measure, leverages the logits generated by the LLM itself to assess the quality of responses quickly and efficiently, without the need for externally trained models.

\paragraph{Consistency-Based Response Selection.}
It is often assumed that after sufficient training, a model gains a reasonable level of understanding regarding the correctness of its outputs~\citep{liang2024internal}. One approach to leveraging this understanding is self-consistency \cite{wang2022self}, which aggregates multiple model outputs to select the most common response. This method has demonstrated significant improvements, showing that the aggregated judgment of the model can be more reliable than a single output. This suggests that models may not always require an external evaluator to assess the quality of their responses. However, self-consistency has limitations. It requires that multiple outputs converge on the same final answer, making it difficult to generalize to tasks involving open-ended generation. Additionally, while universal self-consistency (USC)~\citep{chen2023universal} has been extended to a variety of tasks, it struggles with scalability and lacks a measure of certainty, making it less suitable as a universal method for evaluating responses.
Self-certainty, in contrast, overcomes these limitations by directly measuring the confidence of each response based on its token distribution. It can both handle open-ended generation tasks and scale efficiently, providing a robust evaluation without the need for identical outputs or external models.

\paragraph{Confidence Estimation for Model Responses.}
Confidence estimation is crucial for enhancing the quality of generated responses. Several approaches have been proposed to assess the reliability of model outputs. Self-Evaluation~\citep{ren2023self} prompts models to evaluate their own responses, using the probability assigned to a yes or no token as a confidence measure. BSDetector~\citep{chen2024quantifying} quantifies confidence by measuring the similarity between responses generated from the same prompt and then prompting the model to verify its own correctness. TrustScore~\citep{zheng2024trustscore} estimates confidence by computing the likelihood of the model selecting its original response when hidden among modified-prompt distractors. While these methods provide valuable confidence measures, they require multiple prompt evaluations, making them less scalable and challenging to apply in Best-of-N selection. In contrast, self-certainty takes full advantage of the generated token distribution during sampling, eliminating the need for additional prompts. This makes our approach more efficient, scalable, and well-suited for Best-of-N selection.