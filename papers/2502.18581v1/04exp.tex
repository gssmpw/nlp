\section{Experiment Setup}
This section presents the experimental setup for comparing various confidence measures in selecting reliable responses for reasoning tasks. We also extend the evaluation to additional datasets and explore combining self-certainty with voting methods for improved response selection.

% from experiment xxx.
\subsection{Comparison of Confidence Measures} \label{sec:experiment_setup}  
To assess the effectiveness of different candidate formulations in Section~\ref{sec:candidate}, we employ them to select the most confident response from a set of \( N \) outputs generated by our base model, Llama-3.1-8B-Instruct \cite{dubey2024llama}. To mitigate the potential bias and data contamination arising from the model's training on publicly available datasets, we evaluate their performance using the LiveBench-Math dataset \cite{white2024livebench}, which was released after the model's deployment.

We begin by sampling 64 responses using \(\text{temperature} = 0.6\) and \(\text{top-p} = 0.9\), and subsequently create subsets comprising the first \(N = 4,8,16,32,64\) to perform Best-of-N selection. To ensure fairness, we test different measures' performance using the same set of samples. We mask out responses for which an answer cannot be extracted-primarily because these outputs do not adhere to the format instructions to facilitate latter comparisons with majority voting. We also include a baseline, FirstAns, which simply selects the first extractable answer from the 
\(N\) outputs. This baseline serves as a reference point for quantifying the performance improvements achieved by our candidate expression-based selection strategy. The evaluation is implemented based on the ZeroEval framework \cite{zeroeval}, which provides a unified structure for performance evaluation. We repeat the experiment five times and report the average accuracy as the overall performance. 

% As shown in Figure~\ref{fig:expression_selection}, the KL-divergence-inspired distributional confidence (self-certainty) achieves the highest accuracy on LiveBench-Math, particularly excelling at larger $N$, whereas other measures plateau or degrade beyond $N=16$.

\subsection{Validation on Additional Datasets and Combined Voting Methods} \label{sec:borda_setup}
% After confirming that self-certainty outperforms other confidence measures, we combine it with voting methods to further enhance performance and evaluate its effectiveness on a broader set of datasets. 
We conduct a series of experiments to evaluate the proposed self-certainty and Borda Voting methods against self-consistency, universal self-consistency (USC), greedy decoding, and FirstAns across various reasoning tasks.


The sampling strategy follows the procedures outlined in Section~\ref{sec:experiment_setup}. For USC, we use the template from the original paper \cite{chen2023universal} (with minor wording modifications, as shown in Appendix \ref{sec:usc_example}). To ensure a fair comparison, we assist USC in selecting the first valid response when it fails to choose one with an extractable answer. 

We evaluate different methods using the Llama-3.1-8B-Instruct model across the following benchmarks:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]
    \item \textbf{Mathematical Reasoning}: We utilize the LiveBench-Math dataset \cite{white2024livebench}, the validation set of GSM8K dataset \cite{gsm} and the test set of MATH dataset \cite{MATH}.
    \item \textbf{Code Reasoning}: The CRUXEval-O benchmark \cite{gu2024cruxeval} is employed, which involves predicting the output of Python codes.
    \item \textbf{Code Generation}: We adopt the LiveCodeBench code generation benchmark \cite{jain2024livecodebench} to assess the improvements introduced by our methods.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/section1.pdf}
    % \vspace{-1.5em}
    \caption{Evaluation of Best-of-\(N\) selection accuracy on LiveBench-MATH across multiple confidence measures. KL achieves the best performance at larger \(N\), while other measures plateau or decline after \(N = 16\).} 
    \label{fig:expression_selection}
\end{figure}

For all test models and datasets, we employ Chain-of-Thought reasoning \cite{wei2022chain}, except for the code generation dataset. To evaluate the generalization of our measure across different training methodologies, particularly for the recent R1-series large reasoning models \cite{guo2025deepseek}, we test our approach on DeepSeek-R1-Distill-Llama-8B using the MATH dataset (Level 3). Given the increased reasoning time required by this model, we conduct a single trial for this experiment. To further validate and assess generalizability, we apply both USC and self-certainty to the Qwen-2.5-Coder-32B-Instruct model \cite{hui2024qwen2}, in addition to Llama-3.1-8B-Instruct, on the LiveCodeBench dataset.


\section{Results and Analysis}\label{sec:result}

\subsection{Self-Certainty}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/combined_confidence_new.png}
    \vspace{-1.em}
    \caption{Scatter plot showing various confidence measures against response length (measured in number of characters) in the LiveBench-Math dataset, using the Llama-3.1-8B-Instruct model with 64 samples per question. The figure demonstrates that, with the exception of self-certainty, all other measures exhibit a bias towards longer responses.} 
    \label{fig:scatter}
\end{figure*}

\paragraph{KL-Divergence-Inspired Distributional Confidence Outperforms Other Measures in Best-of-N Selection.}

The results, shown in Figure \ref{fig:expression_selection}, demonstrate that distributional confidence measures generally outperform perplexity when \(N \geq 16\). Among all candidate methods, KL divergence is the only measure that consistently improves as N increases to 32 and 64. This indicates that KL divergence serves as a more robust measure of confidence, offering better insight into the accuracy of responses. Equation~\ref{eq:sc} defines tokenwise self-certainty as the KL divergence from a uniform distribution, with an alternative empirical distribution evaluated in Appendix~\ref{sec:diff_dis}. The results confirm that KL with a uniform distribution, our original design, generalizes better.



\paragraph{Self-Certainty's Robustness to Reasoning Length in Response Selection.}
To understand why self-certainty outperforms other confidence measures in selecting better responses, we examine the relationship between reasoning length and confidence scores across different measures, as shown in Figure~\ref{fig:scatter}. The scatter plots reveal that longer reasoning lengths correlate with higher confidence scores in most metrics except self-certainty. This indicates that while other measures tend to favor samples with extended reasoning, self-certainty remains largely invariant to response length. This finding aligns with \citet{basu2020mirostat}'s observation that, under low \(p\) values, perplexity decreases as the output length increases. Unlike other metrics that may conflate verbosity with correctness, self-certainty provides a more unbiased assessment of response quality. This robustness ensures that models cannot manipulate the confidence measure by simply generating more extended but meaningless reasoning paths.

\paragraph{Self-Certainty Effectively Separates Correct and Incorrect Responses.}
We analyze the distribution of self-certainty and negative perplexity across correct, incorrect, and no-answer responses using Level 4 of the MATH dataset for a balanced comparison. Figure~\ref{fig:freq} presents our findings. The histogram shows that self-certainty in both correct and incorrect responses follows an approximately normal distribution, with the correct group consistently exhibiting a higher mean. In contrast, while perplexity is able to identify better results when \(N\) is small (also in Figure~\ref{fig:expression_selection}), it fails to distinguish between correct and incorrect responses when applied to the full dataset with multiple outputs per question. \citet{zhang2020trading} demonstrates that as perplexity declines, the quality of responses improves initially, then experiences a significant drop -- an observation consistent with our findings. Notably, perplexity tends to assign higher confidence to no-answer responses, which often arise from self-repetition, early stopping, or excessively long reasoning chains that fail to follow prompt instructions. Given that Llama-3.1-8B-Instruct has a relatively low no-answer rate (\(< 2\% \)) in simpler Level 1 MATH problems, we attribute the no-answer rate primarily to the limited capacity of the model. It is unsurprising that negative perplexity favors these outputs, as avoiding difficult questions or repeating oneself is a common failure mode, even for humans -- consistent with \citet{basu2020mirostat}, who showed that maximizing perplexity increases self-repetition. In contrast, self-certainty reliably assigns lower confidence scores to no-answer responses, effectively distinguishing them from correct answers. These observations are further strong evidence that self-certainty is a more effective measure of the certainty of a model as it is more closely correlated with the quality of responses.

\begin{table}[t]
\centering
\caption{Accuracy of different voting methods on the test set of MATH dataset using Llama-3.1-8B-Instruct. Self-certainty-based Borda voting outperforms other voting methods.}
\vspace{0.5em}
\label{tab:voting}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Method} & \textbf{\(\boldsymbol{N = 8}\)} & \textbf{\(\boldsymbol{N = 64}\)} \\ 
\midrule
Majority & 58.60 & 63.40  \\
Average             & 46.92 & 32.94\\
Sum                 &  59.06 & 63.51 \\
Borda (\(p=0.5\))   & \textbf{59.08}  & 63.71  \\
Borda (\(p=1.2\))   & 58.86  & \textbf{64.10}  \\
\bottomrule
\end{tabular}
% \vspace{-1em}
\end{table}

\begin{table*}[ht]
    \centering
    \caption{Performance comparison of various methods across different datasets using Llama-3.1-8B-Instruct. Some USC results are omitted due to over 20\% of the data exceeding context window limits under the settings. Self-certainty consistently outperforms sampling, greedy decoding, and USC, while Borda Voting with the optimal parameter \(p\) delivers the best performance across all methods.
    }
    \vspace{0.5em}
    \label{tab:results}
    \footnotesize
    \begin{tabular}{lcc  cc  cc  cc c}
        \toprule
        \multirow{2}{*}{\textbf{Method}}  & \multicolumn{2}{c}{\textbf{LiveBench-Math}} & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{Crux}} & \multirow{2}{*}{\textbf{Avg.}}\\ 
       & $N=8$ & $N=32$ & $N=8$ & $N=64$ & $N=8$ & $N=64$ & $N=8$ & $N=64$ & \\
        \midrule
        Greedy            & \multicolumn{2}{c}{12.23} & \multicolumn{2}{c}{84.00} & \multicolumn{2}{c}{47.96} & \multicolumn{2}{c}{39.88} & 46.02\\
        FirstAns          & 17.66 & 17.66 & 82.08 & 82.08 & 49.08 & 49.09 & 42.93 & 42.93 & 47.94 \\
        \midrule
        USC                  & 21.08 &  -    & 87.32 & 85.65 & 54.66 & - &  43.78 & 41.25 & 51.19\\
        Self-consistency     & 22.50 & 26.25 & 89.42 & 90.99 & 58.60 & 63.40 & 47.58 & 50.42 & 56.15 \\
        \midrule            
        Self-certainty       & 20.87 & 22.01 & 87.32 & 88.90 & 54.63 & 56.70 & 45.38 & 45.83 & 52.71 \\
        - Borda ($p=0.3$)    & \textbf{23.69} & 26.47 & \textbf{89.57} & \textbf{91.07} & \textbf{59.04} & 63.60 & \textbf{47.94} & 50.42 & 56.48\\
        - Borda ($p=0.7$)    & 23.59 & 26.36 & 89.51 & 91.04 & \textbf{59.04} & 63.85 & 47.85 & 50.65 & 56.49\\
        - Borda ($p=1.2$)    & 23.21 & \textbf{26.69} & 89.51 & 90.95 & 58.86 & \textbf{64.10} & 47.93 & 50.85 & \textbf{56.51}\\
        - Borda ($p=2.0$)      & 22.45 & 26.41 & 89.13 & 90.90 & 57.94 & 60.02 & 47.25 & \textbf{51.23} & 55.67 \\ 
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/four_datasets_line_plots_new.pdf}
    % \vspace{-1.5em}
    \caption{Performance evaluation across four datasets employing different strategies with Llama-3.1-8B-Instruct. These line graphs demonstrates the strong scaling ability of both self-certainty and Borda voting.}
    \label{fig:four_datasets}
\end{figure*}

\subsection{Self-Certainty and Voting}\label{sec:exp_voting}
\paragraph{Borda Voting in Combination with Self-Certainty.}
As discussed in Section \ref{sec:vote}, when responses contain explicit answers, self-certainty can be integrated with voting methods to enhance overall accuracy. We evaluate the effectiveness of Borda voting for combination voting in such cases, comparing it against majority voting, average self-certainty, and sum self-certainty on the MATH dataset, as shown in Table~\ref{tab:voting}. Our results indicate that self-certainty-based Borda voting outperforms other voting methods.



\paragraph{Performance Comparison Across Four Datasets.}
We examine the scaling properties of self-certainty and self-certainty-based Borda voting in Figure~\ref{fig:four_datasets}. The results indicate that self-certainty significantly outperforms both regular sampling and greedy decoding. Moreover, its performance improves considerably as $N$ increases, demonstrating that self-certainty, as a measure of the model’s confidence in its responses, provides valuable insight into output correctness.
Additionally, Borda voting demonstrates better performance compared to self-consistency under various settings of \(p\) and \(N\) across all four datasets. This suggests that the self-certainty measure enhances the accuracy of the final-answer-based voting method by providing useful ranking information. 



\paragraph{Optimizing the Borda Parameter \(p\) for Different \(N\).}
To investigate the relationship between the Borda parameter \(p\) in Equation~\ref{eq:borda_vote} and the efficiency of the selection method, we plot line charts in Figure~\ref{fig:math_p}, showing the performance of different selection methods across varying sample sizes \(N\). The result reveals that the optimal \(p\) increases from 0.5 to 1.2 as \(N\) increases from 8 to 64, suggesting that stronger control from self-certainty is needed with a larger \(N\). For general use cases, grid search remains the most effective approach for determining the optimal $p$. Alternatively, a simple heuristic is to set \(p = 0.3\) when \(N\leq 16\) and \( p = 1.2\) when \(N\geq 32\), though this rule of thumb may vary depending on the model and the complexity of the questions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/math_p.pdf}
    \vspace{-1em}
    \caption{Performance of Borda voting on the MATH dataset using Llama-3.1-8B-Instruct, with varying \( p \) and \( N \). For each $N$, accuracy initially increases with $p$ reaches a peak, and then declines. The optimal \( p \) depends on \( N \). Note that self-consistency corresponds to Borda voting with \( p = 0 \).}
    \label{fig:math_p}
\end{figure}

\subsection{Generalization}
\paragraph{Generalization of Self-Certainty on Open-Ended Generation Tasks.}
Self-consistency struggles with creative, open-ended tasks such as code generation, where each sample produces a unique answer. In such cases, it defaults to standard sampling. USC and our method self-certainty offer solutions to this limitation. We compare self-certainty with USC on the code generation task of LiveCodeBench (Figure~\ref{fig:livecode}). Our findings reveal that USC underperforms compared to greedy decoding on the Llama-3.1-8B-Instruct model, likely due to the model’s constrained ability to recognize consistency. This observation is reinforced by results from the larger Qwen model, where USC successfully outperforms greedy decoding. In contrast, self-certainty consistently outperforms greedy decoding on both models and surpasses USC on the larger Qwen-2.5-Coder-32B-Ins. Additionally, the performance of self-certainty improves as \(N\) increases.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/livecode.pdf}
    % \vspace{-1em}
    \caption{Comparison of self-certainty and USC on the LiveCodeBench code generation task. The results show that self-certainty outperforms USC and greedy decoding on both Llama-3.1-8B-Instruct and Qwen-2.5-Coder-32B-Ins models, with performance improving as \(N\) increases.}
    \label{fig:livecode}
\end{figure}

\paragraph{Generalization of Self-Certainty on Reasoning Models.}
Recent research on DeepSeek-R1 \citep{guo2025deepseek} demonstrates that rule-based reinforcement learning and long-chain-of-thought (CoT) can significantly enhance the reasoning capabilities of LLMs. We evaluate the generalization of self-certainty on such reasoning models, with results for DeepSeek-R1-Distill-Llama-8B presented in Table~\ref{tab:distill}. Our findings show that self-certainty consistently outperforms both greedy decoding and sampling, with performance further improving as $N$ increases on reasoning models. Additionally, Borda voting with self-certainty surpasses the performance of self-consistency with proper \(p\). These results reinforce previous observations, highlighting the robustness of our methods across various fine-tuning techniques.

\begin{table}[t]
\centering
\caption{Accuracy of various methods on the Level 3 test set of the MATH dataset using DeepSeek-R1-Distill-Llama-8B (single trial). Self-certainty outperforms Greedy and FirstAns, while Borda Voting with an appropriate \(p\) surpasses self-consistency.}
\label{tab:distill}
\vspace{0.5em}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{\(\boldsymbol{N = 4}\)} & \textbf{\(\boldsymbol{N = 16} \)} & \textbf{\(\boldsymbol{N = 64}\)} \\
\midrule
Greedy    & 77.54  &  77.54 & 77.54 \\
FirstAns            & 81.17    & 81.43 & 81.43\\
\midrule
Self-consistency    & 83.64    & 86.47 & 87.62\\
\midrule
Self-certainty      & 83.29 & 83.73 & 84.08  \\
- Borda ($p=0.3$)   & 84.79  & 87.00 & 87.80  \\
- Borda ($p=0.7$)   & 84.70 & 86.91  & 87.62\\
- Borda ($p=1.2$)   & 84.62  & 87.00 & 88.06\\
- Borda ($p=2.0$)   & 83.29  & 87.00 & 87.98\\
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}

% \xuandong{zhexian figure without self-consistency. Greedy, first answer, upper-bound, self-certainty, borada for five datasets}

\section{Discussion and Future Research}
While self-certainty proves effective in evaluating model outputs for open-ended generation tasks and demonstrates stable scalability, it has several limitations.

First, self-certainty alone underperforms self-consistency on questions with unique answers (Section~\ref{sec:result}). Although self-consistency generally achieves better performance, recent studies suggest that properly trained ORMs and PRMs can surpass it \cite{lightman2023let,uesato2022solving}. Current reward model training fine-tunes a base model to rate sentences based on token probabilities \cite{wang2024math}. A key insight from our findings is that treating the softmaxed logits of LLMs as a full probability distribution, rather than relying solely on individual token probabilities, leads to more robust certainty measurements. In self-certainty, using KL divergence between the output distribution and a uniform distribution provides greater stability than averaging log probabilities, suggesting that integrating this approach could enhance reward model effectiveness.

Second, this study explores a limited set of formulations for distributional confidence and Borda voting. The default choice of the averaging function for $F$ in Equation~\ref{eq:DC} may not be optimal, and a broader selection of $F$ could further improve self-certainty’s accuracy. Likewise, the power function used for vote distribution in Equation~\ref{eq:borda_vote} for Borda voting is intuitive but may not be the most effective formulation.

Beyond these limitations, self-certainty presents exciting opportunities for future research. It not only encourages rethinking reward model designs but also offers potential applications in test-time scaling techniques \cite{snell2024scaling}, potentially reducing computational costs. Additionally, self-certainty could be leveraged for data labeling and reinforcement learning tasks \cite{bai2022training,ouyang2022training}. By maximizing token-wise self-certainty, we may enable more autonomous and efficient learning systems, paving the way for advancements in both model performance and computational efficiency.

% Beyond encouraging a remodeling of the reward model, self-certainty opens up diverse future research directions. Some test-time compute scaling techniques rely on reward models to select superior reasoning and planning paths \cite{snell2024scaling}. Replacing the reword model with self-certainty can substantially reduce the heavy computational cost of training rewards without significantly compromising performance. Moreover, it is also possible to apply Self-certainty to data labeling and reinforcement learning tasks. Given the robustness of self-certainty and the fact that the assigned probability of selected tokens does not directly determine it, it creates new opportunities for developing a fully autonomous learning system. For instance, by defining the loss function to minimize the token-wise self-certainty of selected tokens and iteratively optimizing this measure, self-certainty offers a promising approach to enhancing model performance while ensuring greater autonomy and computational efficiency.



\section{Conclusion}
In this paper, we introduce self-certainty and self-certainty-based Borda voting as novel approaches for evaluating and enhancing model response performance. Self-certainty functions as an internal measure of response quality, demonstrating robustness in several key aspects. Compared to traditional scoring methods, such as average log probability and perplexity, it offers superior scalability when applied to Best-of-N selection. Additionally, the ranking information provided by self-certainty improves chain-of-thought reasoning and outperforms universal self-consistency (USC) in code generation tasks. Its stability, flexibility, and generalizability make it applicable across a wide range of domains, with the potential to enhance the autonomous learning capabilities of LLMs. 

% \section*{Impact Statement}
% This research introduces self-certainty, a novel confidence measure that exhibits a stronger correlation with response quality than traditional probability-based metrics, such as perplexity, by capturing distributional-level information. By leveraging the inherent probabilistic structure of LLMs, self-certainty offers a lightweight, scalable, and effective alternative to reward models for inference-time response selection, reducing the computational and annotation burdens associated with existing Best-of-N selection techniques.  

% Beyond its immediate application in improving reasoning accuracy, self-certainty has the potential to transform self-evaluation and autonomous learning in large language models. Unlike reward models, which require extensive fine-tuning and human supervision, self-certainty enables models to assess their own outputs with minimal additional computation. This advancement paves the way for more efficient self-improving AI systems that can refine their responses without explicit external feedback.  

% Furthermore, self-certainty generalizes beyond structured problem-solving tasks, offering a robust mechanism for quality assessment in open-ended text generation, including creative writing, dialogue generation, and code synthesis. By challenging conventional reward model-based selection frameworks, our approach not only enhances the reliability of LLM outputs but also lays the groundwork for more interpretable and transparent AI decision-making. The adoption of self-certainty in AI research and deployment has the potential to make advanced reasoning capabilities more accessible, reducing reliance on computationally expensive techniques while maintaining high levels of accuracy and adaptability.

\section*{Acknowledgment}  

We appreciate the valuable discussions with Kexun Zhang and Danqing Wang. We also thank the Center for AI Safety for providing substantial compute resources for this project.