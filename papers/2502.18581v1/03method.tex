% \section{Methodology}
\section{Measuring Confidence of LLMs} \label{sec:candidate}
This section explores and evaluates several candidate metrics for quantifying the confidence of LLMs' predictions. We assess traditional probabilistic measures like average log probability and perplexity, alongside distributional measures such as KL divergence and entropy. By comparing these methods, we aim to determine the most effective confidence measure for selecting the most reliable model outputs. 

\subsection{LLM Background}
To understand how these confidence metrics are derived, we first briefly review the basics of LLM output generation.  LLMs are typically based on the Transformer architecture and employ an autoregressive approach \cite{vaswani2017attention}. Input sentences or prompts are tokenized into a sequence of discrete tokens  $
x = (x_1, x_2, \dots, x_n), x_i \in \mathcal{V}
$
where $\mathcal{V}$ is the vocabulary. The LLM processes $x$ to produce a sequence of logits:  
$
L_{x} = (\ell_1, \ell_2, \dots, \ell_n), \ell_i \in \mathbb{R}^{V}
$
where $V = |\mathcal{V}|$ is the vocabulary size. Each logit vector $\ell_i$ represents the model's prediction for the $i$-th token, conditional on preceding tokens. For each position $i$, applying the softmax function to $\ell_i$ yields a probability distribution over the vocabulary:  
$
p(\cdot | x_1, \dots, x_{i-1}) \in [0,1]^{V}.
$
For the last input token $x_n$, the logits $\ell_n$ produce the probability distribution:  
$
p(\cdot | x) \in [0,1]^{V},
$
representing the likelihood of each token in $\mathcal{V}$ being the next token after $x$. After sampling an output sequence $y = (y_1, y_2, \dots, y_m)$, the probability distribution for generating the $i$-th token $y_i$, conditional on input $x$ and prior outputs $y_{<i} = (y_1, \dots, y_{i-1})$, is:  
$
p(\cdot | x, y_{<i}) \in [0,1]^{V}.
$
This distribution reflects the model’s belief about the next token given the prompt and generated sequence so far.

\subsection{Sentence-Level Probabilistic Confidence}
Probabilistic confidence quantifies a model's certainty in its predictions by directly leveraging the probabilities assigned to sampled tokens.

\textbf{Average log-probability.} A common confidence measure is the average log-probability (AvgLogP) of the sampled tokens:
\begin{equation}
    \text{AvgLogP} \coloneqq \frac{1}{n}\sum_{i=1}^n\log\left[p(y_i|x,y_{<i})\right] \nonumber
\end{equation}
where \(p(y_i|x,y_{<i})\) is the probability assigned to token \(y_i\). A higher AvgLogP indicates that the model consistently assigns higher probabilities to its generated tokens, reflecting greater confidence.


\textbf{Perplexity.} Perplexity is one of the most common metrics for evaluating language models and is closely related to uncertainty. It is defined as the exponentiated average negative log-likelihood of a sequence:
\begin{equation}\label{eq:perplexity}
    \text{Perplexity} \coloneqq \exp\left(-\frac{1}{n}\sum_{i=1}^n\log\left[p(y_i|x,y_{<i})\right]\right) 
\end{equation}
Since \(\text{Perplexity} = \exp(-\text{AvgLogP})\), both measures are equivalent when selecting the most certain response. In the following sections, we use negative perplexity for Best-of-N selection to assess the effectiveness of these confidence measures. However, while perplexity is widely used in LLM evaluations, it has been shown \cite{hu2024can} to fail in capturing a model’s ability to understand long contexts, highlighting the need for alternative measures.




\subsection{Distributional Confidence}
Distributional confidence measures broaden the scope beyond just the probabilities of sampled tokens. They consider the entire probability distribution over the vocabulary at each generation step, aiming to capture a more holistic view of the model's certainty.

A sentence-level distributional confidence measure can be defined as:
\begin{equation}
    \text{Distributional-Confidence} :=  F(f(P_{y|x})) \nonumber
\end{equation}
where \(P_{y|x} = \left(p(\cdot|x), p(\cdot|x,y_{1}), \dots, p(\cdot|x,y_{< n})\right)\) represents the sequence of token-level probability distributions, \(f: \mathbb{R}^{n\times V} \to \mathbb{R}^n\) is a function that takes the generated distribution and produces a confidence score for each token, and \(F: \mathbb{R}^{n}
\to \mathbb{R}\), aggregates these token-level confidences into a sentence-level confidence. Here, \(n\) is the output sequence length and \(V\) is the vocabulary size. In this work, We define \(F\) as the average of the confidence values across all positions:
\begin{equation}\label{eq:DC}
    F(C_1, \dots, C_n) = \frac{1}{n} \sum_{i=1}^{n} C_{i}, \quad C_{i} = f(p(\cdot|x,y_{<i}))  
\end{equation}

For the distribution confidence function $f$, we explore metrics that quantify how ``peaked'' or ``concentrated'' the probability distribution is. A more concentrated distribution suggests higher model certainty. We consider the following metrics, inspired by statistical measures:

\textbf{Kullback-Leibler (KL) Divergence.}
% \xuandong{rewrite it to be a KL divergence against another distribution. We choose to use uniform distribution for simplicity.}
Drawing upon the view of neural networks as Maximum Likelihood Estimators \cite{lecun2015deep}, we hypothesize that higher confidence corresponds to output distributions that are further from a uniform distribution \(U\)(representing maximum uncertainty). KL Divergence quantifies this difference, which effectively measures how much the predicted distribution deviates from randomness:
\begin{equation}\label{eq:sc}
    \begin{split}
        C_i^{\text{KL}} &\coloneqq \mathrm{KL}(U\parallel p(\cdot|x,y_{<i})) =  \sum_{j=1}^{V} \frac{1}{V} \log\left(\frac{1/V}{p(j|x,y_{<i})}\right) \\
        &= -\frac{1}{V}\sum_{j=1}^{V} \log\left(V\cdot p(j|x,y_{<i})\right) 
    \end{split}
\end{equation}
\textbf{Gini Impurity.}
Originally introduced in decision trees \citep{breiman2017classification}, Gini Impurity quantifies the probability that two randomly sampled tokens belong to different classes. A more concentrated output distribution indicates higher model confidence, as the likelihood of selecting tokens from different classes decreases:
\begin{equation}
     C_i^{\text{Gini}}\coloneqq 1-I_G(p(\cdot|x,y_{<i})) = \sum_{j=1}^{V} (p(j|x,y_{<i}))^2 \nonumber 
\end{equation}

\textbf{Entropy.}
Entropy measures the degree of disorder in a probability distribution. Higher entropy signifies greater uncertainty, and thus, lower confidence. To use entropy as a confidence measure, we take the negative of entropy:
\begin{equation}
     C_i^{\text{Entropy}} \coloneqq \sum_{j=1}^{V} p(j|x,y_{<i}) \log(p(j|x,y_{<i})) \nonumber
\end{equation}

\textbf{Distributional Perplexity (DP).} 
Similar to entropy, we apply a negative sign to the perplexity measure in order to interpret it as confidence. To distinguish distributional perplexity from the standard perplexity defined in Equation~\ref{eq:perplexity}, we denote it as DP.
\begin{equation}
     C_i^{\text{DP}}\coloneqq -\exp{\bigg(-\sum_{j=1}^{V} p(j|x,y_{<i}) \log(p(j|x,y_{<i})\bigg)}\nonumber
\end{equation}

\subsection{Our Primary Metric: Self-certainty}
Our empirical evaluations, as shown in Figure~\ref{fig:freq} and~\ref{fig:expression_selection}, reveal that the KL-divergence-inspired distributional confidence is more effective in distinguishing correct samples from incorrect ones and has the best performance of accuracy at larger N. Based on this finding, we define self-certainty as our primary confidence metric for best-of-\(N\) selection, and the sentence-level self-certainty can be formulated as:
\begin{equation}
    \textbf{Self-certainty} = -\frac{1}{nV}\sum_{i=1}^n\sum_{j=1}^{V} \log\left(V\cdot p(j|x,y_{<i})\right) \nonumber
\end{equation}

One may use the cross entropy between the predicted distribution and a uniform distribution as an equivalent confidence measure. In fact, this measure differs from the corresponding KL-divergence only by an additive constant. Specifically, the self-certainty based on cross-entropy is given by:
$
\text{Self-certainty (CE)} = -\frac{1}{n V} \sum_{i=1}^{n} \sum_{j=1}^{V} \log \bigl( p(j\mid x, y_{<i}) \bigr),
$

\subsection{Analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/example_0.pdf}
    % \vspace{-1.em}
    \caption{Comparison of reasoning paths in solving a quadratic equation for the given problem using self-certainty and negative perplexity. Sample I factors the quadratic equation directly, while Sample II applies the quadratic formula. The figure illustrates an example of how the two measures assign confidence scores at each reasoning step, showing that self-certainty distinguishes between correct and incorrect reasoning more effectively than negative perplexity.}
    \label{fig:example0}
\end{figure*}

Reward Models, such as PRMs and ORMs, usually assign the overall response rating based on the minimum reward across all steps \cite{lightman2023let,wang2024math}, which emphasizes the mistakes made by the model rather than its progress. The reason self-certainty methods that use the average as an aggregation function are also effective in identifying mistakes is that an error at an early stage reduces overall confidence, affecting both the current and subsequent steps. In the example shown in Figure~\ref{fig:example0}, sample I contains a mistake in the first step. Although subsequent calculations are flawless, self-certainty assigns lower confidence for the follow-up reasoning compared to the correct reasoning path in sample II. In contrast, negative perplexity assigns a similar confidence score to the correct follow-up, regardless of whether it follows accurate or flawed reasoning. Moreover, distributional confidence can identify the correct reasoning path from the starting token, whereas negative perplexity only recognizes it later.

\section{Self-Certainty with Voting Method}\label{sec:vote}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{fig/vote.pdf}
    % \vspace{-2em}
    \vspace{-1.em}
    \caption{Example of Borda Voting correctly identifying the answer when confidence-driven selection and self-consistency fail. The figure illustrates how Borda Voting aggregates confidence scores and ranks to select the correct answer.}
    \label{fig:example1}
    % \vspace{-1em}
\end{figure*}
While self-certainty is more robust than other confidence measures, it still suffers from the risk of being skewed by samples with spuriously high confidence scores. We discover that self-certainty-driven Best-of-N selection performs worse than self-consistency in terms of accuracy on the math dataset with clear final answers, when using the same value of N, as shown in Table~\ref{tab:results}. However, this does not necessarily make it inferior to self-consistency. Self-consistency focuses on response-layer consistency in LLMs, whereas self-certainty aggregates information from decoding-layer consistency. By integrating both layers, we can potentially extract more reliable and consistent responses from multiple outputs with \emph{explicit answers}.

A common approach to combining majority voting with score-based selection is summing the scores over samples with the same answer. However, this method is sensitive to the scaling of the scores. Similarly, using the average confidence of the outputs may not adequately account for answers that are sampled more frequently, potentially underrepresenting the confidence in those answers. To address these limitations, and drawing inspiration from the Borda count, we propose the following combined method: 

First, we rank \(N\) outputs of models by confidence, obtaining a ranking \([r_1, r_2, \dots, r_N]\). We then assign votes to these ranked outputs using the following formula:
\begin{equation} \label{eq:borda_vote}
    v(r) = (N - r + 1)^p
\end{equation}
where \(r\) is the rank of the output (\(1 \le r \le N\)). Each valid response casts votes for its final answer, with total number of votes corresponding to its rank. The answer receiving the highest total votes will be considered the consensus answer among the given samples. When \(p = 0\), Equation~\eqref{eq:borda_vote} reduces to majority voting (all ranked outputs receive an equal vote of 1). As \(p\) approaches positive infinity, the highest-ranked output dominates, making the selection equivalent to using distributional confidence alone.

Figure~\ref{fig:example1} demonstrates how Borda Voting correctly selects the answer by considering both confidence ranking and answer frequency, addressing the limitations of confidence-driven selection and self-consistency. The parameter \(p\), which controls the ranking influence,  is a tunable hyperparameter and will be discussed in Section \ref{sec:exp_voting}.