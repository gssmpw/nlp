\section{Introduction}

% \xuandong{
% 1. organize and clean the code.
% 2. R1 experiment
% 3. revise experiment/results and method part
% 3. add a conclusion in each table and figure
% 4. add examples and prompts in the appendix and reference in the main paper.
% }

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/combined_confidence_histograms.pdf}
    \vspace{-1.5em}
    \caption{Distribution of self-certainty and negative perplexity for correct, incorrect, and no-answer responses on the MATH dataset~(Level 4)~\citep{MATH} using the Llama-3.1-8B-Instruct model with 64 samples per question. Self-certainty exhibits a roughly normal distribution for both correct and incorrect responses, with correct answers having a higher mean. In contrast, negative perplexity does not clearly separate correct from incorrect outputs and disproportionately favors no-answer responses. This underscores self-certainty’s potential for effectively distinguishing response quality.} 
    \label{fig:freq}
    % \vspace{-1.2em}
\end{figure}

Large Language Models (LLMs) have achieved impressive reasoning abilities, yet reliably producing accurate outputs for complex tasks often requires techniques to enhance inference-time performance~\citep{wu2024inference, xiang2025towards}.  \emph{Best-of-N} selection, generating and selecting from multiple candidate responses, has emerged as a powerful paradigm for significantly improving reasoning accuracy~\citep{snell2024scaling}.  Current Best-of-N methods frequently rely on reward models, such as Outcome Reward Models (ORMs) \cite{cobbe2021training} and Process Reward Models (PRMs) \cite{lightman2023let,uesato2022solving,wang2022self}, not only for output selection but also for data annotation to further refine LLM reasoning capabilities~\citep{uesato2022solving,wang2022self}.

However, reward models introduce substantial computational and practical challenges. First, they are computationally expensive to train or fine-tune, often requiring as many parameters as the LLM itself~\citep{wang2024math}.  Second, reward models are vulnerable to distribution shifts, leading to unreliable predictions, and to ``reward hacking,'' where models exploit reward function weaknesses to achieve artificially high scores and degraded generalization~\citep{eisenstein2023helping}.  While techniques such as reward model ensembles~\citep{coste2023reward} provide partial mitigation, they further increase annotation and computational overhead.

As a lighter-weight alternative, Self-Consistency~\citep{wang2022self} aggregates multiple outputs using majority voting, obviating the need for explicit reward scoring. However, it is applicable only to tasks where answers can be directly compared through string matching, making it unable to differentiate different reasoning paths that yield the same final answer.  
Furthermore, it cannot be applied to open-ended generation tasks. Universal Self-Consistency (USC)~\citep{chen2023universal} attempts to address this limitation by prompting the LLM to choose the most consistent response among the samples. Yet, USC’s performance gains are constrained by the model’s context length and inherent reasoning ability, with empirical evidence showing a decline when the sample size increases beyond a certain threshold (e.g., from $N = 8$ to $N = 16$ on the GSM8K dataset \cite{gsm}). Our research further confirms that USC can be ineffective for small models. Moreover, self-consistency and USC lack a direct quality score for responses, limiting their applicability in tasks such as candidate ranking.

To overcome these limitations, we propose leveraging the LLM's inherent probabilistic output for a more practical, general, and robust approach to Best-of-N selection and self-evaluation.  We hypothesize that an LLM’s probability distribution naturally encodes its \emph{certainty} in a response. We introduce \textbf{self-certainty}, a novel metric that quantifies this confidence by measuring the divergence of the predicted token distribution from a uniform distribution. A distribution that diverges significantly from uniform indicates a more peaked—and thus more certain—prediction. As shown in  Figure~\ref{fig:freq}, self-certainty demonstrates a stronger signal for distinguishing correct responses. Notably, it incurs almost no computational overhead, as the token distribution is generated alongside the tokens during inference. Inspired by Borda Voting, a method for aggregating ranked preferences, we enhance self-consistency by incorporating self-certainty-based ranking.  Our method assigns weighted votes based on self-certainty rank, using a scaling factor of $(N-\text{ranking}+1)^p$, where $p$ is a hyperparameter, effectively prioritizing more confident responses.

We rigorously evaluate our methods across diverse reasoning benchmarks, including LiveBench-Math \cite{white2024livebench}, GSM8K \cite{gsm}, MATH \cite{MATH}, CRUXEval \cite{gu2024cruxeval} and LiveCodeBench \cite{jain2024livecodebench}, spanning mathematical reasoning, code reasoning, and code generation. Our experiments reveal that self-certainty-based voting consistently outperforms self-consistency in Best-of-N selection of reasoning tasks, effectively adapting to varying sample sizes and question difficulties.

The key advantages of self-certainty are:
\begin{itemize}[leftmargin=*, itemsep=0pt, topsep=0pt]  
\item \textbf{Scalability}: Self-certainty scales efficiently with increasing sample size \(N\), mirroring reward models in scalability but without their computational burden, enabling more reliable selection from larger candidate pools.
\item \textbf{Orthogonal Enhancement to Chain-of-Thought}:  Self-certainty complements chain-of-thought reasoning \cite{wei2022chain}. It integrates seamlessly with powerful reasoning models, outperforms CoT baselines, and beats self-consistency through weighted voting beyond simple majority rule.
\item \textbf{Generalizability to Open-Ended Tasks}: Self-certainty generalizes effectively to open-ended responses, where self-consistency is inapplicable. Our confidence-based Best-of-N strategy significantly surpasses both greedy decoding and USC in open-ended scenarios, while retaining USC’s simplicity and universal applicability.
\end{itemize}
