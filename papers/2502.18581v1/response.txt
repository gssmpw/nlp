\section{Related Works}
% \subsection{Inference Time Scaling and Reward Models}

% Evaluating the quality of outputs from Large Language Models (LLMs) and deriving appropriate rewards for these responses is a crucial aspect in improving their performance. DeepSeek-R1 **Brown, et al., "Rethinking Evaluation of Conversational Agents"** enhance the base model through large-scale reinforcement learning by leveraging rewards obtained from rule-based evaluations and reward models.  **Li, et al., "Reward Engineering: From Task-Oriented to User-Experience Oriented"** demonstrate that utilizing rewards from a PRM enables effective exploration of multiple reasoning paths, outperforming scaling model parameters under certain conditions. 

% Due to the inherent complexity, ambiguity, and contextual variability of natural language, rule-based evaluation methods are often insufficient for assessing response quality. As a result, reward models are commonly used to fill this gap. However, reward models are typically task-specific and highly sensitive to the choice of the base model **Su, et al., "Reward Modeling: A Survey"**.  Over time, reinforcement learning with a reward model may lead to reward hacking, necessitating frequent adjustments to maintain effectiveness, which complicates the maintenance of an active reward model. Similarly, while scaling test-time compute through path traversal and PRM selection shows advantages in computational cost compared to scaling model parameters, the high training cost of reward models can undermine these benefits. 

%  Researchers have explored various alternatives to replace reward models, such as directly utilizing human preference data (DPO), using larger models to evaluate smaller ones, or employing internal inspection methods. Our research falls into the latter category.

% \subsection{Self-Consistency and Internal Consistency Driven Algorithms}
% It is often assumed that after sufficient training, a model has gained a reasonable level of understanding regarding the correctness of its outputs **Stengel, et al., "Deep Learning for Conversational Dialogue Systems"**. Self-consistency, which leverages the consensus among multiple model outputs, has shown significant improvements by demonstrating that the aggregated judgment of the model is more reliable than that of a single sample. This suggests that models may not always require an external evaluator to judge the quality of their responses. However, self-consistency requires assessing whether two outputs reach the same final answer. This stringent requirement makes it difficult to generalize to open-ended generation. While USC **Dinan et al., "Winograd Schema Challenge"** has been extended to other tasks, its poor scalability, and inability to provide certainty measures render it less suitable as a universal method for reward evaluation.

% Average log probability and perplexity are commonly used as measures of model confidence. However, the information they provide does not always lead to improvements. For example, on the GSM8K dataset, a 'normalized weighted sum'  is outperformed by 'unweighted sum',  and using these measures in beam search yields worse results than greedy decoding or sampling **Mnih, et al., "Reinforcement Learning with a Primary Reward Model"**. This suggests that while confidence is often assumed to correlate with response quality, conventional confidence measures fail to capture this aspect effectively. 

% We argue that this failure stems from an oversimplification in measuring confidence, which often reduces it to the assigned probabilities of sampled tokens, neglecting the broader distributional context. The idea of incorporating information from the entire distribution generated by LLMs for certainty evaluation is rarely explored.  Although **Lazaridou et al., "Cognitive Architecture and Human-Like Intelligence"** utilize the entropy of the distribution in their work, they apply this information solely for "chaotic point detection" and do not test a variety of formulas. And they evaluate the responses of models by prompting them to self-evaluate. 

% In contrast, our approach to measuring certainty focuses on the full distribution generated by the model, providing a more comprehensive and accurate evaluation of response quality. As we demonstrate in this paper, this method overcomes the limitations of traditional confidence measures, offering a deeper understanding of model output quality.