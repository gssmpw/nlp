\section{Related Work}
\paragraph{Story Generation.}
The task of long-form story generation has received significant attention in recent years due to advancements in LLMs.
Early approaches primarily focused on developing new modules to enhance narrative coherence and consistency.
For example, ____ leveraged graph structures to organize events more effectively and improve narrative consistency. 
% Similarly, ____ introduced a gated multi-scale self-attention mechanism to better model long-range context in stories. 
Another example is ____, which introduced an interface for human-computer interaction to generate personalized stories and applied it to RNN-based models for controlling story endings and storylines.
However, these methods often struggled to maintain coherence and consistency over extended~sequences.
More recently, prompt engineering techniques have been adopted to tap into the generative power of LLMs____.
For instance, ____ proposed a director-actor agent collaboration framework for controllable and interactive drama script generation, while ____ explored dynamic beam sizing and affective reranking to generate engaging narratives.
% On the dataset side, ____ introduced the Storium dataset, which contains 6,000 lengthy stories with fine-grained natural language annotations.

Despite these advancements, existing methods lack a framework to emulate complex narratives and distinct authorial styles, essential for real-world long-form novel writing.



\paragraph{Parameter-Efficient Fine-Tuning.}

Parameter-efficient fine-tuning (PEFT)____ reduces the computational costs of fine-tuning LLMs by introducing additional modules, avoiding direct updates to the large-scale pretrained weights.
Adapters____ insert extra feature transformations between model blocks, while prefix tuning____ optimizes parameters through learnable prefixed embeddings without modifying the pretrained weights.
More recently, LoRA____ introduces low-rank matrix decomposition to efficiently fine-tune models by updating small, trainable parameter matrices. 
Extensions of LoRA have further explored task-specific adapters, enabling specialization in distinct aspects of text generation____.
For example, MoELoRA____ adopts a mixture-of-experts (MoE) approach, dynamically selecting different LoRA experts based on the input to improve adaptability. 
Unlike these works, our WriterLoRA combines advantages of a mixture-of-LoRA architecture but is specifically designed to handle the challenges of pastiche novel generation.


\paragraph{Personalized LLMs.}
Personalization is crucial for enabling LLMs to adapt to individual user preferences and requirements. 
Existing approaches to personalization can be broadly categorized into prompt-based methods____ and PEFT-based methods____.
Prompt-based methods encode user history and behaviors as contextual examples to guide the modelâ€™s response generation____. In contrast, PEFT-based methods focus on efficiently integrating user-specific preferences into LLMs. For example, ____ introduced PEFT modules to capture and store user-specific behavior patterns and preferences. Further research has explored improving generalization and efficiency using techniques such as MoE-style gating____, parameter merging strategies____, and iterative learning____.
Building on these foundations, our work extends personalization to stylish novel generation, enabling the model to adapt to complex narrative styles and authorial preferences.



\begin{figure*}[htb]
\centering
\includegraphics[scale=1.1]{figs/model.pdf}
\caption{
The entire training process can be divided into two parts: the pretraining phase and the fine-tuning phase. During the fine-tuning phase, tasks are divided into three stages of increasing complexity: world-building learning, plot structure learning, and stylish writing learning. These stages are integrated using curriculum learning.
}
\label{fig:framework}
\end{figure*}