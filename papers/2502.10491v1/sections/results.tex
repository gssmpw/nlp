\section{Results and Discussion}

\begin{table}[b]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccc}
\hline
\textbf{Method} & \textbf{Additional Parameters} & \textbf{Runtime Space Complexity} \\ \hline \hline
S S-RPE~\cite{agarwal_structure_2024}  &  $\mathcal{O} ( \mathcal{s} (\mathcal{h} \mathcal{d})^2 + \ell (\mathcal{h} \mathcal{d})^2 )$  &  $\mathcal{O}( \ell \mathcal{t}^2 \mathcal{h} \mathcal{d} )$  \\
SPE~\cite{liutkus_relative_2021}  & $\mathcal{O} (\mathcal{h} \mathcal{d} N_f)$ & $\mathcal{O}(\ell \mathcal{t} \mathcal{h} \mathcal{d} N_f)$ \\ 
F-StrIPE & $\mathcal{O} (\mathcal{s} \mathcal{h} \mathcal{d} N_f)$ & $\mathcal{O}(\mathcal{s} \ell \mathcal{t} \mathcal{h} \mathcal{d} N_f)$ \\ \hline                         
\end{tabular}%
}
\caption{Complexity analysis for different methods with $\ell$ layers, $\mathcal{h}$ heads, $\mathcal{d}$ head dimension, $\mathcal{s}$ structures, and $\mathcal{t}$ sequence length. A typical order for size is $\mathcal{s} < (\mathcal{h}, \ell) << \mathcal{d} <<< \mathcal{t} $, with $\mathcal{s}$ contributing the least and $\mathcal{t}$ contributing the most, assuming equal growth rate.}
\label{tab:complexity}
\end{table}

In Table \ref{tab:results_unrounded}, we report the mean and standard deviation on 3 seeds for each metric.

\subsection{Utility of structural information in PE}

When we compare PEs without structure (NoPE, SPE) against PEs with structure (S S-RPE, F-StrIPE), we observe that the latter perform better. This matches previous findings reported in the literature~\cite{agarwal_structure_2024} which argued that using structure in PE boosts performance, particularly in underdetermined problems such as melody harmonization.

\subsection{Influence of different random features}

F-StrIPE:SFF adds structural information to SPE and F-StrIPE improves on this by using RFF in place of SFF. F-StrIPE:SFF yields marginal improvements over the performance of SPE in the (16, 16) scenario, in particular, on CS. This lends some additional support to our previous observation that structural information used in PE is useful. However, F-StrIPE, which uses a noise-free estimate of the positional matrix $\mathbf{P}_d$, gives us significant boosts over the performance of F-StrIPE:SFF and, by extension, that of SPE, on both (16, 16) and (16, 64). These improvements are particularly noticeable in CS, GS and NDD and are especially pronounced in the (16, 16) setting. This emphasizes that the correct approximation techniques can strongly enhance the effect of augmenting our models with prior knowledge.

\subsection{Ablations on F-StrIPE and F-StrIPE:SFF}

As described in Section \ref{sssection:baselines}, we perform ablations on structural levels used during training, resulting in models that are specialized to use only melody (F-StrIPE:M), only chord (F-StrIPE:C) or only phrase (F-StrIPE:P) as structural information. When we compare these models against F-StrIPE, which uses all three structures, we see first that F-StrIPE:M and F-StrIPE:P do worse than F-StrIPE on all metrics and both task settings. In fact, their performance drops lower than even NoPE. In contrast, F-StrIPE:C brings a clear advantage over F-StrIPE, with significant improvements on all metrics and both task settings. This fits our intuition that the accompaniment in pop songs can be nicely characterized by chord progressions~\cite{paiement_probabilistic_2005,zhu_xiaoice_2018}. Our results show that using only chord information is better than using all structures simultaneously. This shows that while task-specific structural information can boost performance, ill-founded and generic priors can prove counterproductive. Thus, how prior knowledge is selected and incorporated into a deep-learning model should be an important consideration while designing such systems.

\subsection{Comparing complexities}

These results should be viewed in the context of the complexity analysis presented in Table \ref{tab:complexity}. Compared to S S-RPE, SPE and F-StrIPE have linear complexity in sequence length, which makes a sizeable dent in the requirement for computational resources. Moreover, scaling up from SPE to F-StrIPE only adds a factor of $\mathcal{s}$, corresponding to the number of structures we use in PE, which grows the slowest compared to all other variables. In fact, since the ablations use only a single structure at a time $(\mathcal{s} = 1)$, the complexity of our best-performing method, F-StrIPE:C, matches that of SPE.

Thus, on the one hand, in the worst case where multiple structures are needed, it only costs a small amount of additional computational resources. On the other hand, if we already know which structure is best-suited to our task, we can benefit from F-StrIPE and leverage prior knowledge in our model without needing any extra resources.

\subsection{Length Generalization}

Finally, we see that models that are trained on 16 bars of music but tested on 64 bars of music reflect the same trends as seen in the models that were tested on 16 bars of music: structural information combined with RFF provides a sizeable improvement over baselines that either do not use structure or use structure but with SFF.

On CS and GS, the PEs that use SFF (SPE, F-StrIPE:SFF and F-StrIPE:SFF:C) show large improvements on the (16, 64) setting compared to the (16, 16) setting. This does not hold true for the RFF-based PEs, where some methods show small improvements and others show small deteriorations. Nevertheless, F-StrIPE:C outperforms all other PEs in the (16, 64) situation. This suggests that an in-depth comparative investigation of the characteristics of different approximation techniques is needed. Specifically, it would be interesting to understand which approximation is suited to what learning scenario and whether we can combine the strengths of different approximations to obtain a more robust, fast, structure-informed PE. Interestingly, in two metrics - SSMD and NDD - all the methods in Table \ref{tab:results_unrounded} do marginally better in the (16, 64) scenario compared to the (16, 16) one.

The transfer of performance from the (16, 16) setting to the (16, 64) setting can be partly attributed to the presence of stereotypical structure and a high degree of repetition in pop songs~\cite{sargent_estimating_2017,dai_missing_2022}. Thus, 16 measures of music could potentially contain much of the necessary information to generate much longer sequences. It would be interesting to quantatively assess whether this hypothesis is true.
% To assess this hypothesis, we first assumed that an $\mathcal{N}$-measure sample in our dataset consists of $(n_\text{tracks} \times 128) \times |\mathcal{N}|$ independent variables, where $|\mathcal{N}|$ is the number of timesteps in $\mathcal{N}$ measures of music at half-beat resolution following our convention in Section \ref{sssection:data_input}. We plot the average Shannon entropy over all such variables, with the distribution for a given variable being computed over the full dataset. The results are plotted in Fig. \ref{fig:entropy}. We see that samples with 16 bars of music have the same informational content as samples with 64 bars of music. %Although a more detailed investigation is needed to understand this phenomenon, ...

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/entropy.png}
%     \caption{entropy}
%     \label{fig:entropy}
% \end{figure}
% \textbf{Appropriate to point this out? Doesn't highlight our method in particular even though it does the best out of all metrics.}