\section{Introduction}
\label{sec:intro}


% \begin{figure*}[h!]
%     \begin{subfigure}{2.05\columnwidth}
%     \centering
%     \includegraphics[scale=0.65]{figs/ismir_2024_sff.png}
%     % \caption{top: SVD 4x4 matrix}
%     % \label{subfig:a}
%     \end{subfigure}\\
%     \begin{subfigure}{2.05\columnwidth}
%     \centering
%     \includegraphics[scale=0.65]{figs/ismir_2024_rff.png}
%     % \caption{bottom: SVD 4x4 matrix calculation}
%     % \label{subfig:b}
%     \end{subfigure}
%     \caption{Approximating Method 3~\cite{huang_improve_2020} (left column, corresponding to Equation \ref{eq:huang_m3}) with two random feature methods (right column). The top row conceptually expresses Proposition 1, while the bottom row demonstrates how Random Fourier Features is connected to Stochastic Fourier Features. The variables referenced here are detailed in Section \ref{section:prelim}.}
%     \label{fig:visualize_matrices}
% \end{figure*}


Owing to their remarkable ability to produce realistic, high-quality samples, deep generative models are attracting significant interest. Two interdependent factors have been clearly established as being important for their superior performance: voluminous data and ever-increasing parameter counts~\cite{kaplan_scaling_2020}. Domains with abundant data -- text, vision and speech -- have successfully exploited these factors. In comparison, the limited size of publicly-available music datasets places an implicit limit on the size of the models that can be used, making music a challenging data domain. 

% Despite the advances brought about by the introduction of Transformers and attention, music generated by such models often lacks long-term coherence and organization~\cite{wu_jazz_2020}. 
The introduction of Transformers and attention has accelerated the advances in deep generative models and music has been no stranger to this phenomenon. However, generated music often lacks long-term coherence and organization, which are hallmarks of real music~\cite{wu_jazz_2020}.
% One way of improving music generation is to embed prior knowledge about musical structure into Transformers~\cite{richard_model_2024,bhandari_motifs_2024} through the positional encoding (PE) module~\cite{agarwal_structure_2024,yi_popmag_2020,guo_domain_2023,liu2022symphony}. 
One way of improving music generation is to embed prior knowledge about musical structure into data-driven models~\cite{ji_survey_2023,richard_model_2024,bhandari_motifs_2024}, for example, through the positional encoding (PE) module of Transformers~\cite{agarwal_structure_2024,yi_popmag_2020,guo_domain_2023,liu2022symphony}. 
This is an attractive option that provides a drop-in replacement for vanilla, structure-free PE without added complexity or training pipeline changes.

Despite their successes, Transformers bear a quadratic complexity in sequence length, which restricts their use on long sequences. Kernel approximations can be used to mitigate this cost~\cite{tay_efficient_2022, tsai_transformer_2019}.

In this work, we unite these two strands of research, one of which aims to improve Transformers for music generation by using informative priors, and the other that employs kernel approximations to achieve low-complexity Transformers that are able to process long sequences.
In particular, we propose F-StrIPE, a \textbf{f}ast \textbf{str}ucture-\textbf{i}nformed \textbf{p}ositional \textbf{e}ncoding method that works in linear complexity. We show that F-StrIPE is a generalization of Stochastic Positional Encoding (SPE)~\cite{liutkus_relative_2021}, an existing structure-free positional encoding technique, as sketched in Figure \ref{fig:contributions}. We do this by using Random Fourier Features~\cite{rahimi_random_2007}, thereby drawing on and providing a connection to previous work on kernel approximation for efficient attention. We empirically evaluate F-StrIPE on the symbolic music generation task of melody harmonization and show that, compared to the features used by SPE, Random Fourier Features are better-suited to structure-informed PE. We demonstrate how structure can be efficiently used in Transformers for music generation, giving us the coveted twin benefits of better performance and lower computational cost.
%\textit{In particular, we first generalize Stochastic Positional Encoding (SPE)~\cite{liutkus_relative_2021} to leverage structure. We draw on previous work on random features for kernel approximation and show that Random Fourier Features~\cite{rahimi_random_2007} is better for structure-informed PE than the features used by SPE. In this way, we demonstrate how structure can be efficiently used in Transformers for music generation, giving us the coveted twin benefits of better performance and lower computational cost.}