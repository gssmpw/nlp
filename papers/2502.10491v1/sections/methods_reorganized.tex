\section{Methods} \label{section:methods}

\subsection{Structure-informed positional encoding}

As we hinted in Section \ref{section:background}, positional encoding depends on a sequence $\mathcal{P} = [ p_1, p_2, ... , p_T ]$ of positional indices. In standard PE, which does not utilize structure, $p_i = i$, which makes $\mathcal{P}$ a linear grid. When structure is included in PE, $p_i = s_\ell (i)$, where $s_\ell (i)$ gives the structural label at level $\ell$ (e.g. chord) for timestep $i$. In this case, $s_\ell (i) = s_\ell (i^{\prime})$ is possible for $i \neq i^{\prime}$, making $\mathcal{P}$ a non-linear grid. In fact, we can consider vectorial positional indices with multiple resolutions of structural organization. Consequently, we obtain $p_i = \mathbf{s} (i)$, where $\mathbf{s} (i) = [ s_1 (i) , ..., s_\ell (i), ..., s_L (i) ]$ is a vector of $L$ structural labels. Viewed in this way, PEs with structure can simply be used as a drop-in replacement for PEs without structure by replacing the form of $p_i$, giving us richer positional information. This is a way to flexibly represent domain-specific prior knowledge about the underlying data domain.

\subsection{Adding structure to SPE: F-StrIPE:SFF}

In order to use rich positional information in SPE, we can augment the sinusoidal feature matrix $\boldsymbol{\Omega}$ from (\ref{eq:sff:sine_features}) to be:
\begin{equation} \label{eq:pos_sine_features}
    [\boldsymbol{\Omega}(\mathcal{P}, \boldsymbol{f}, \boldsymbol{\theta})]_{i j}= \begin{cases}\cos \left(2 \pi \mathbf{f}[\omega, :]^\top p_i+\boldsymbol{\theta}[\omega]\right) & \text { if } j = 2\omega \\ \sin \left(2 \pi \mathbf{f}[\omega, :]^\top p_i+\boldsymbol{\theta}[\omega] \right) & \text { else }\end{cases}
\end{equation}
Here, we use the vectorial formulation of structure-aware positional indices $p_i = \mathbf{s}(i)$, unlike (\ref{eq:sff:sine_features}) where $p_i = i$ was a sequence of structure-free positional indices linked solely to the passage of time. Whereas in (\ref{eq:sff:sine_features}), $\mathbf{f}[\omega]$ was a single frequency, $\mathbf{f}[\omega, :]$ in (\ref{eq:pos_sine_features}) is a vector of frequencies. Therefore, each frequency $\mathbf{f}[\omega, \ell]$ in this vector acts on the $\ell^{\text{th}}$ structural label at timestep $i$. We can combine (\ref{eq:spe:ohyeah}), (\ref{eq:sff}) and (\ref{eq:pos_sine_features}) to obtain a fast structure-aware PE technique that uses Stochastic Fourier Features. We call this method \textit{F-StrIPE:SFF}.

% Here, unlike (\ref{eq:sff:sine_features}) where $\mathbf{f}[\omega]$ was a single frequency, $\mathbf{f}[\omega, :]$ is a vector of frequencies. Each frequency $\mathbf{f}[\omega, \ell]$ in this vector acts on the $\ell^{\text{th}}$ structural label at timestep $i$, where we can use the vectorial formulation of structure-aware positional indices $p_i = \mathbf{s}(i)$. In contrast, $p_i = i$ in (\ref{eq:sff}).

\subsection{Asymptotic case of SFF: Random Fourier Features} \label{ssection:asymptotic_rff}

Using Equations (\ref{eq:spe:ohyeah}) and (\ref{eq:sff}), we can express the SFF approximation of the positional matrix $\mathbf{P}_d$ for arbitrary timesteps $m$ and $n$ as:
\begin{equation} \label{eq:sff_mn}
    \mathbf{P}_d[m, n] \approx \Big[ \Omega_\mathcal{Q}^d [m, :] (\mathbf{Z}_d \mathbf{Z}_d^\top) \Omega_\mathcal{K}^{d^\top} [:, n] \Big] / R \\
\end{equation}

where we use the abbreviation $\Omega_\mathcal{A}^d = \boldsymbol{\Omega}\left(\mathcal{P}_A, \boldsymbol{f}_d, \boldsymbol{\theta}_d^A \right) \text{diag}\left(\ddot{\boldsymbol{\lambda}_d}\right)$. We observe that $\mathbf{Z}_d \mathbf{Z}_d^\top = \widehat{\mathbf{C}}_d$ acts as an empirical covariance matrix for the features $\Omega_\mathcal{Q}^d$ and $\Omega_\mathcal{K}^d$. Since $\mathbf{Z}_d$ has zero mean and unit variance, as $R \to \infty$, $\widehat{\mathbf{C}}_d$ approaches the theoretical covariance matrix $\mathbf{C}_d = \mathbf{I}_{2N_{f}}$.
In the ideal case of $\mathbf{C}_d$, (\ref{eq:sff_mn}) simplifies to $\mathbf{P}_d[m, n] \approx \Omega_\mathcal{Q}^d [m, :] \Omega_\mathcal{K}^{d^\top} [:, n]$, giving:
\begin{equation} \label{eq:ideal_C}
    \mathbf{P}_d[m, n] \approx  \frac{1}{N_f}  \sum_{\omega = 1}^{N_f} \Lambda_\omega \cos \Big( f_{\omega} ( \mathcal{P}_Q[m] - \mathcal{P}_K[n] ) + \Theta_\omega \Big)
\end{equation}
where $\Lambda_\omega$ is the gain contributed by the matrices $\text{diag}\left(\ddot{\boldsymbol{\lambda}_d}\right)$ and $\Theta_\omega$ is the phase-shift contributed by $\boldsymbol{\theta}^Q_d$ and $\boldsymbol{\theta}^K_d$. This representation has been studied in previous work, where it is called \textit{Random Fourier Features} (RFF) \cite{rahimi_random_2007, sutherland_error_2015}.

\subsection{Generalizing F-StrIPE:SFF to F-StrIPE}

Using this insight, we can redesign the positional feature matrices from (\ref{eq:sff}) to be:
\begin{equation} \label{eq:rff}
    \mathbf{P}^{Q/K}_d = \boldsymbol{\Omega}\left(\mathcal{P}_{Q/K}, \boldsymbol{f}_d, \boldsymbol{\theta}_d^{Q/K}\right) \text{diag}\left(\ddot{\boldsymbol{\lambda}_d}\right) / \sqrt{N_f}
\end{equation}
where the sinusoidal features $\boldsymbol{\Omega}$ uses structure-aware positional indices as given in (\ref{eq:pos_sine_features}). With this, we can now modify (\ref{eq:spe:ohyeah}) to use $\mathbf{P}^{Q/K}_d$ in place of $\thicktilde{\mathbf{P}}^{Q/K}_d$, giving us $\mathbf{Q}^{\text{RFF}}/\mathbf{K}^{\text{RFF}}$ in place of $\mathbf{Q}^{\text{SFF}}/\mathbf{K}^{\text{SFF}}$. To signify that such a PE technique generalizes F-StrIPE:SFF to use RFF in place of SFF, we call this method \textit{F-StrIPE}.

In the second row of Figure \ref{fig:visualize_matrices}, similar to SFF, we show the different components of RFF in the case where gains are 1 and phase shifts are 0. RFF can be understood as the ideal case of SFF where $R \to \infty$. Seen in this way, RFF gives us a noiseless estimate of $\mathbf{P}_d$ with direct access to the theoretical covariance matrix $\mathbf{C}_d$.

% In addition, if we pair $\mathbf{Q}^{\text{SFF}}/\mathbf{K}^{\text{SFF}}$ with the sinusoidal features given in Equation \ref{eq:sff:sinusoidal_features}, in place of those given in Equation \ref{eq:sff:sine_features}, we obtain a richer version of SPE which accepts multi-dimensional structural information instead of time indices. To distinguish it from SPE, we name this variant F-StrIPE:SFF.

\section{Experiments}
% To test the efficacy of our method, we use the task of melody harmonization for symbolic music. 
We assess the merits of our approaches on the task of melody harmonization for symbolic music.

\subsection{Dataset and Input Representation} \label{sssection:data_input}
We use the Chinese POP909 dataset~\cite{wang_pop909_2020} and three levels of structural labels with different resolutions~\cite{dai_automatic_2020}: melodic pitch ($16^{th}$-note), chord (quarter-note) and phrase (measure). Each MIDI file in this dataset consists of three tracks: melody, bridge (second melody) and piano (accompaniment). 
We use the POP909 alignment dataset~\cite{agarwal_structure_2024} to correctly match the structural labels with the input. We convert the MIDI files to binary pianorolls $\mathbf{X} \in \mathbb{B}^{(n_\text{tracks} \times 128) \times n_\text{time}}, \mathbb{B} = \{0, 1\}$, where $n_\text{tracks}$ is the number of tracks and $n_\text{time}$ is the number of timesteps in the pianoroll.

\subsection{Task Setup}
Given the sequence for the melody and bridge tracks $\big[ \mathbf{x}_n \in \mathbb{B}^{(n_\text{tracks} - 1) \times 128} \big]$ as input, with $n \in \{ 1, ..., n_\text{time}\}$, the model must predict all tracks $\big[ \mathbf{y}_n \in \mathbb{B}^{n_\text{tracks} \times 128} \big]$. We expect the model to produce the complete accompaniment track for all timesteps at once, without conditioning later predictions on earlier predictions. 
% We use three settings: (16, 16), (16, 64) and (64, 64), where the first number is the sequence length (in bars of music) used for training and the second number is that used for testing.
We use two settings: (16, 16) and (16, 64). The first number is the sequence length (in measures) for training and the second is that used for testing.

\subsection{Model and Training}
We use a 2-layer causal encoder Transformer with 4 heads and 512 model dimension. Training for 15 epochs with a batch size of 8, we use gradient clipping and curriculum learning~\cite{bengio_curriculum_2009}. We use two learning rate schedulers: a linear warmup and an epoch-wise decay.
We do a grid-search for two hyperparameters: learning rate (choices: $\{ 1, 5, 10 \} \times 0.0001$) and post-processing binarization strategy~\cite{agarwal_structure_2024} (choices: thresholding, thresholding with merge). While the first binarization strategy uses a fixed threshold, the second additionally fills the gap between notes if the gap is less than a minimum distance.

\subsection{Baselines and Our Methods} \label{sssection:baselines}
We consider three types of baselines: (i) Transformers without PE (NoPE~\cite{tsai_transformer_2019,haviv_transformer_2022}), (ii) Transformers with efficient, approximate atttention but no structural information in PE (SPE~\cite{liutkus_relative_2021}), and (iii) Transformers with structural information in PE but using inefficient, exact attention (S S-RPE~\cite{agarwal_structure_2024}).
From our methods, we use F-StrIPE with the three structural levels described in Section \ref{sssection:data_input}. We also assess the influence of different random features with F-StrIPE:SFF using all structural levels. 
We perform ablations on F-StrIPE by selecting one level at a time during training. Finally, we use the best-performing structural level from the F-StrIPE ablations to additionally do an ablation study with F-StrIPE:SFF.

\subsection{Evaluation}
We choose a collection of musically-motivated metrics from the literature, guided by four criteria. 

To assess large- and small-scale structural properties, we use Self-Similarity Matrix Distance (SSMD)~\cite{wu_musemorphose_2021}. For both the target and the prediction, we calculate chroma vectors, giving us the number of onset occurrences per chroma in every half-measure. We then compose a self-similarity matrix (SSM) for each chroma vector by taking the pairwise cosine similarities between all elements of the vector. The SSMD is the mean absolute difference between the SSM of the target and the SSM of the prediction.

For melodic consistency, we use Chroma Similarity (CS)~\cite{wu_musemorphose_2021}. Using the aforementioned method of constructing chroma vectors, we compute the CS as the mean cosine similarity between corresponding entries of the target chroma vector and the prediction chroma vector.

For rhythmic consistency, we use Grooving pattern Similarity (GS)~\cite{wu_jazz_2020}. The grooving pattern of a piece of music is a vector that encodes a 1 for the quarter-notes where onsets occur and 0 for those where no onsets occur. After obtaining the grooving patterns of the target and prediction, we compute the GS as the percentage of quarter-notes where the corresponding pattern values match.

To gauge polyphonicity, we use Note Density Distance (NDD)~\cite{agarwal_structure_2024, haki_real_2022}. We calculate the total number of pitches in each $16^{\text{th}}$-note of the target and prediction. The NDD is the average percentage of missing pitches in the prediction, with the number of pitches in the target giving us the maximum possible value.