\section{Background}
\label{section:background}

\subsection{Transformers and Positional Encoding}

The Transformer~\cite{vaswani_attention_2017} is a sequence-to-sequence architecture that processes all timesteps in a sequence parallely using \textit{attention}. Given input sequence $[\mathbf{x}_1 , ... , \mathbf{x}_T]$, each element of the output sequence is:
\begin{equation}
    \mathbf{y}_m = \frac{\sum_n \mathsf{a}_{m n} \mathbf{v}_n}{\sum_n \mathsf{a}_{m n}} \text{ with } \mathsf{a}_{mn} = \text{exp} \bigg( \frac{a_{mn}}{\sqrt{D}} \bigg)
\end{equation}

%where $a_{mn} = \mathbf{q}_m \mathbf{k}_n^\top$ and $m, n \in \{1, ..., T\}$ are timesteps. 
where $a_{mn} = \mathbf{q}_m \mathbf{k}_n^\top$ is the attention coefficient or ``similarity score" for a pair of timesteps $(m,n)$, with $m, n \in \{1, ..., T\}$.
The query, key and value vectors $\mathbf{q}_m, \mathbf{k}_m, \mathbf{v}_m$ are obtained by linearly transforming input $\mathbf{x}_m$. Since attention executes pairwise computation among all timesteps, Transformers are invariant to permutations in the temporal order of inputs. Hence, positional encoding (PE) is applied to provide the model with a sense of time. Positional information can be incorporated in two places: at the input or during attention computation. The latter is called Relative Positional Encoding (RPE) and we focus on this approach here.

\subsection{Efficient Attention with Positional Information}

Attention has quadratic complexity in sequence length. To address this, a kernelized form of attention was introduced:
% \begin{align} 
%     \mathsf{a}_{mn} &= \mathcal{K} ( \mathbf{q}_m , \mathbf{k}_n ) = \mathbb{E}\left[ \phi(\mathbf{q}_m) \phi(\mathbf{k}_n)^{\top}\right] \label{eq:attention_as_expectation} \\
%     \boldsymbol{\mathsf{A}} &= \left[ \mathsf{a}_{mn} \right] \approx \phi(\mathbf{Q})\phi(\mathbf{K})^{\top} \label{eq:attention_approx}
% \end{align} 
\begin{equation} \label{eq:attention_approx}
    \mathsf{a}_{mn} = \mathcal{K} ( \mathbf{q}_m , \mathbf{k}_n ) = \mathbb{E} \Big[ \phi(\mathbf{q}_m) \phi(\mathbf{k}_n)^{\top} \Big]
\end{equation}

where $\mathcal{K}$ is a positive (semi)definite kernel and $\phi(\mathbf{x}): \mathbb{R}^D \to \mathbb{R}^{D_\phi}$ defines a \textit{randomized feature map} for $\mathbf{x}$~\cite{tsai_transformer_2019, choromanski_rethinking_2021}. With multiple instantiations, $\phi$ captures, on average, the relationship between $\mathbf{q}_m$ and $\mathbf{k}_n$, typified by $\mathcal{K}$. Thus, coefficients $\mathsf{a}_{mn}$ need not be computed explicitly, which produces linear-complexity Transformers.
%Because RPE requires explicit computation of $a_{mn}$, it was not possible to use RPE with the efficient formulation described above.

The efficient formulation described above is not directly applicable to RPE which, as introduced in \cite{shaw_rpe_2018}, requires the explicit computation of attention coefficients $a_{mn}$. This gap was addressed by Stochastic Positional Encoding~\cite{liutkus_relative_2021}, which approximates attention as:

\begin{equation} \label{eq:spe:ohyeah}
a_{mn} \approx \left[ \sum_{d=1}^D
    \lefteqn{\overbrace{\phantom{
    \text{diag}(\mathbf{Q}_{:,d})
    \frac{
        \thicktilde{\mathbf{P}}^Q_d
    }{\sqrt{R}}
    }}^{
    \mathbf{Q}^{\text{SFF}}
    }}
    \text{diag}(\mathbf{Q}_{:,d})
    \underbrace{
    \frac{
        \thicktilde{\mathbf{P}}^Q_d
    }{\sqrt{R}}
    \lefteqn{\overbrace{\phantom{
    \frac{
        % {\mathbf{P}_d^{K}}^\top
        \thicktilde{\mathbf{P}}^K_d{}^\top
    }{\sqrt{R}}
    \text{diag}(\mathbf{K}^\top_{:,d})
    }}^{
    \mathbf{K}^{\text{SFF}}
    }}
    \frac{
        \thicktilde{\mathbf{P}}^K_d{}^\top
    }{\sqrt{R}}
    }_{
    \approx \mathbf{P}_d
    }
    \text{diag}(\mathbf{K}^\top_{:,d})
    \right]_{mn}
\end{equation}

%where $\mathbf{Q}_{:, d}\slash\mathbf{K}_{:, d}^\top$ extracts a $T_{Q/K}$-dimensional vector containing the $d^{th}$ dimension  for all timesteps of the query/key matrix.
% \gael{where $\mathbf{Q}_{:, d}$ (respectively $\mathbf{K}_{:, d})$ extracts a $T_Q$ (resp. $T_K$)-dimensional vector containing the $d^{th}$ dimension for all timesteps of the query (resp. key) matrix. Note that, for the sake of brevity, we use $\mathbf{Q}$/$\mathbf{K}$ to refer respectively to the $\mathbf{Q}$ (resp. $\mathbf{K}$) matrices in the following formulations. }
% where $\mathbf{Q}_{:, d}/\mathbf{K}_{:, d}$ extracts a $T_Q/T_K$-dimensional vector containing the $d^{th}$ dimension for all timesteps of the query/key matrix. Note that, for the sake of brevity, we use the convention `$\mathbf{Q}$/$\mathbf{K}$' as a shorthand to mean `$\mathbf{Q}$ (respectively $\mathbf{K}$)' in this paper.
% The positional matrix $\mathbf{P}_d$ captures the relationship between all pairs $(m, n)$ of timesteps that come from the positional index sequences $\mathcal{P}_Q = \{ 1, ..., m, ..., T_Q \}$ and $\mathcal{P}_K = \{ 1, ..., n, ..., T_K \}$. $\mathbf{P}_d$ is approximated by using $R$ feature realizations of $\mathcal{P}_Q$ and $\mathcal{P}_K$. These realizations are collected in the positional representation matrices $\thicktilde{\mathbf{P}}^Q_d$ and $\thicktilde{\mathbf{P}}^K_d$. SPE uses the feature maps $\phi$ proposed in earlier work~\cite{katharopoulos_transformers_2020,zhuoran_efficient_2021}, but applies them to $\mathbf{Q}^{\text{SFF}}$ and $\mathbf{K}^{\text{SFF}}$ (\ref{eq:spe:ohyeah}) instead of $\mathbf{Q}$ and $\mathbf{K}$ (\ref{eq:attention_approx}). As can be seen in (\ref{eq:spe:ohyeah}), $\mathbf{Q}^{\text{SFF}}$ and $\mathbf{K}^{\text{SFF}}$ combine content information ($\mathbf{Q}$/$\mathbf{K}$) with context information ($\thicktilde{\mathbf{P}}^Q_d$/$\thicktilde{\mathbf{P}}^K_d$). The positional representation matrices $\thicktilde{\mathbf{P}}^{Q/K}_d$ are given as:
In SPE, content information ($\mathbf{Q}$/$\mathbf{K}$) is combined with context information ($\thicktilde{\mathbf{P}}^Q_d$/$\thicktilde{\mathbf{P}}^K_d$) in the matrices $\mathbf{Q}^{\text{SFF}}$ and $\mathbf{K}^{\text{SFF}}$. Note that, for the sake of brevity, we use the convention `$\mathbf{Q}$/$\mathbf{K}$' as a shorthand to mean `$\mathbf{Q}$ (respectively $\mathbf{K}$)' in this paper. SPE uses the feature maps $\phi$ proposed in earlier work~\cite{katharopoulos_transformers_2020,zhuoran_efficient_2021}, but applies them to $\mathbf{Q}^{\text{SFF}}$ and $\mathbf{K}^{\text{SFF}}$ (\ref{eq:spe:ohyeah}) instead of $\mathbf{Q}$ and $\mathbf{K}$ (\ref{eq:attention_approx}).
In (\ref{eq:spe:ohyeah}), $\mathbf{Q}_{:, d}/\mathbf{K}_{:, d}$ extracts a $T_Q/T_K$-dimensional vector containing the $d^{th}$ dimension for all timesteps of the query/key matrix. The positional matrix $\mathbf{P}_d$ captures the relationship between all pairs $(m, n)$ of timesteps that come from the positional index sequences $\mathcal{P}_Q = \{ 1, ..., m, ..., T_Q \}$ and $\mathcal{P}_K = \{ 1, ..., n, ..., T_K \}$. $\mathbf{P}_d$ is approximated by using $R$ feature realizations of $\mathcal{P}_Q$ and $\mathcal{P}_K$. These realizations are collected in the positional representation matrices $\thicktilde{\mathbf{P}}^Q_d$ and $\thicktilde{\mathbf{P}}^K_d$, which are given as:
\begin{equation} \label{eq:sff}
    \thicktilde{\mathbf{P}}^{Q/K}_d = \frac{\boldsymbol{\Omega}\left(\mathcal{P}_{Q/K}, \boldsymbol{f}_d, \boldsymbol{\theta}_d^{Q/K}\right) \text{diag}\left(\ddot{\boldsymbol{\lambda}_d}\right) \mathbf{Z}_d}{\sqrt{2 N_f}}
\end{equation}
The first term, $\boldsymbol{\Omega}\left(\mathcal{P}_{Q/K}, \boldsymbol{f}_d, \boldsymbol{\theta}_d^{Q/K}\right)$, represents sinusoidal features for the index sequence $\mathcal{P}_{Q/K}$. These features are parameterized by $N_f$ frequencies, collected in $\boldsymbol{f}_d$, and their phase shifts, collected in $\boldsymbol{\theta}_d$. The second term, $\text{diag}\left(\ddot{\boldsymbol{\lambda}_d}\right)$, consists of gains that apply to the sinusoidal features. Finally, the last term, $\mathbf{Z}_d$, consists of i.i.d. entries from a zero-mean, unit-variance Gaussian distribution. The sinusoidal features are given by:

% where $\mathbf{Z}_d$ consists of i.i.d. entries from a zero-mean, unit-variance Gaussian distribution and $\ddot{\boldsymbol{\lambda}_d}$ are gains that are applied to the sinusoidal feature matrix $\boldsymbol{\Omega}$. Phase-shifts $\boldsymbol{\theta}_d^K$ are set to $\boldsymbol{0}$, unlike $\boldsymbol{\theta}_d^Q$. The matrix $\boldsymbol{\Omega}$ represents a sequence of positions $\mathcal{P}$, parameterized by $N_f$ frequencies, collected in $\boldsymbol{f}$, and their phase shifts, collected in $\boldsymbol{\theta}$, as:

\begin{equation} \label{eq:sff:sine_features}
[\boldsymbol{\Omega}(\mathcal{P}, \boldsymbol{f}, \boldsymbol{\theta})]_{i j}= \begin{cases}\cos \left(2 \pi \mathbf{f}[\omega] p_i+\boldsymbol{\theta}[\omega]\right) & \text { if } j = 2\omega \\ \sin \left(2 \pi \mathbf{f}[\omega] p_i+\boldsymbol{\theta}[\omega] \right) & \text { else }\end{cases}
\end{equation}

where $i$ runs over timesteps, such that $p_i$ is the $i^{\text{th}}$ timestep of index sequence $\mathcal{P}$ and $\omega$ runs over frequencies. The variable $j$ allows us to apply the same frequency $\mathbf{f}[\omega]$ to a pair of cosine and sine features.

We call this feature representation technique, which uses (\ref{eq:sff}) and (\ref{eq:sff:sine_features}), \textit{Stochastic Fourier Features} (SFF). We visualize the different components of SFF in the first row of Figure \ref{fig:visualize_matrices}. In particular, we illustrate the features $\thicktilde{\mathbf{P}}^Q_d$ and $\thicktilde{\mathbf{P}}^K_d$ when all gains are 1 and all phase shifts are 0. 