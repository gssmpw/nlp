\documentclass[letterpaper]{article} %
\usepackage{aaai25}  %
\usepackage{times}  %
\usepackage{helvet}  %
\usepackage{courier}  %
\usepackage[hyphens]{url}  %
\usepackage{graphicx} %
\urlstyle{rm} %
\def\UrlFont{\rm}  %
\usepackage{natbib}  %
\usepackage{caption} %
\usepackage{enumitem} 
\frenchspacing  %
\setlength{\pdfpagewidth}{8.5in} %
\setlength{\pdfpageheight}{11in} %
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} %
\lstset{%
	basicstyle={\footnotesize\ttfamily},%
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,%
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}



\usepackage{amsmath,amsfonts,bm, amsthm, mathtools}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rowsp}{rowsp}
\DeclareMathOperator{\colsp}{colsp}

\newtheorem{question}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}


\newcommand{\newterm}[1]{{\bf #1}}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} %

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\supp}{\text{supp}}

\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\usepackage{xcolor}
\newcommand{\yibo}[1]{{\color{blue} Yibo: #1}}
\newcommand\jason[1]{{\color{brown}\textbf{Jason: }{#1} }}
\newcommand{\xiangwen}[1]{{\color{purple} Xiangwen: #1}}
\definecolor{ForestGreen}{RGB}{34,139,34}
\newcommand{\kt}[1]{{\color{ForestGreen} Kath: #1}}
\pdfinfo{
/TemplateVersion (2025.1)
}


\usepackage{autonum}
\setcounter{secnumdepth}{2} %



\title{Aligning Compound AI Systems via System-level DPO}
\author{
    Xiangwen Wang\textsuperscript{\rm 1,2}\equalcontrib, 
    Yibo Jacky Zhang\textsuperscript{\rm 1}\equalcontrib, 
    Zhoujie Ding\textsuperscript{\rm 1}, 
    Katherine Tsai\textsuperscript{\rm 1}, 
    Sanmi Koyejo\textsuperscript{\rm 1}
}
\affiliations{
    \textsuperscript{\rm 1}Stanford University, Stanford, CA, USA\\
    \textsuperscript{\rm 2}University of Science and Technology of China, Hefei, China\\
   wangxiangwen@mail.ustc.edu.cn, yiboz@stanford.edu, d1ng@stanford.edu, tsaikl@stanford.edu, sanmi@cs.stanford.edu
}





\begin{document}

\maketitle






\begin{abstract}
Compound AI systems, comprising multiple interacting components such as LLM agents and external tools, demonstrate state-of-the-art results across diverse tasks. It is hence crucial to align components within the system to produce consistent results that match human expectations.
However, conventional alignment methods, such as Direct Preference Optimization (DPO), are not directly applicable to compound AI systems.
These challenges include the non-differentiable interactions between components, making end-to-end gradient optimization infeasible. Additionally, system-level preferences cannot be directly translated into component-level preferences, further complicating alignment.  %
We address the issues by formulating compound AI systems as Directed Acyclic Graphs (DAGs), capturing the connections between agents and the data generation processes. We propose a system-level DPO (SysDPO) to jointly align compound systems by adapting the DPO to operate on these DAGs. We study the joint alignment of an LLM and a diffusion model to demonstrate the effectiveness of our approach. Our exploration provides insights into the alignment of compound AI systems and lays a foundation for future advancements.
\end{abstract}





\section{Introduction}\label{sec:intro}






Compound AI systems, which consist of multiple interacting AI components\footnote{In compound AI systems, \textit{components} include \textit{models} and  \textit{agents}, and these terms are used interchangeably in this work. }, serve as promising frameworks to push beyond the model capabilities and achieve state-of-the-art performance~\cite{zaharia2024shift,chen2024are,kandogan2024blueprint,lin2024llm}.
For example, ChatGPT integrates a large language model (LLM), a DALL-E image generator, a web browser plugin, and more~\cite{achiam2023gpt}. %
A multi-agent system consisting of multiple LLMs working collaboratively, e.g., Mixture-of-Agents (MoA), achieves improved performance compared to a single agent~\cite{wang2024mixture}. A Retrieval-Augmented Generation (RAG) system combines large language models with information retrieval capabilities and is able to answer time-sensitive queries. A multi-LLM routing system includes a router that dynamically selects among a diverse set of models to maximize the overall performance~\cite{hu2024routerbench}. 
Compound AI systems utilize LLMs as the base models and further integrate other models to complete more sophisticated tasks that exceed the capability of a single LLM.


\begin{figure}[t!]
\captionsetup[subfigure]{font=scriptsize,labelfont=scriptsize}
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-1-1.png} 
        \caption{Calm Cat}
        \end{subfigure}
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-1-2.png}
        \caption{ Slightly Irritated Cat}
        \end{subfigure}
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-1-3.png}
        \caption{Very Angry Cat}
        \end{subfigure}
    \end{minipage}
    \begin{minipage}{\linewidth}
    \vspace{0.5cm}
        \centering
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-2-1.png} 
        \caption{Slightly Annoyed Cat}
        \end{subfigure}
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-2-2.png}
        \caption{Angry Cat}
        \end{subfigure}
        \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/cat-2-3.png}
        \caption{Furious Cat}
        \end{subfigure}
    \end{minipage}
    
    \caption{\footnotesize This example illustrates the challenges in a compound system composed of GPT-4 and the image generator DALL-E. Given the user prompt to GPT-4, "Generate three separate images of a cat being progressively angrier." The first row (a–c) shows the results from the first query, while the second row (d–f) represents the results from another query. The captions under each image summarize the prompts generated by GPT-4 for DALL-E (complete prompts shown in Appendix). The prompts in both versions reflect a progression in anger. 
    Similarly, DALL-E  accurately generates the images following the given prompts. However, the first row fails to demonstrate a clear visual progression of anger compared to the second row, highlighting GPT-4's inconsistent collaboration with DALL-E. Furthermore, our experiments (Section~\ref{sec:exp}) show that Llama-3-8B and Stable Diffusion XL achieve correct results only 32\% of the time on similar tasks. }
    \label{fig:compound_example}
    \vspace{-2em}
\end{figure}

It is, therefore, crucial to ensure that the outputs of a compound AI system align with human preferences and that each component within the system is aligned to collaborate effectively~\cite{lin2024llm}. 
However, such coordination does not come naturally by simply integrating multiple pre-trained models; we demonstrate a failure case of the coordination between an LLM (GPT-4) and a diffusion model (DALL-E) in Figure~\ref{fig:compound_example}. This demonstrates the critical need to develop a new framework to align compound AI systems. 




While there are many effective ways to align monolithic models with human preference~\cite{rafailov2024direct, ziegler2019fine, bai2022training}, aligning compound systems remains an open problem.
Standard methods such as Direct Preference Optimization (DPO)~\cite{rafailov2024direct} and Reinforcement Learning from Human Feedback (RLHF)~\cite{ziegler2019fine, bai2022training} are not directly applicable to compound systems for three primary reasons.
First, components in a compound AI system communicate in a non-differentiable way such as through plain text, which prohibits an end-to-end gradient optimization or RLHF. Second, aligning each component separately is problematic because the overall system's preferences cannot be decomposed into the preferences of individual components. Effective collaboration among components is critical but not easily captured by aligning them individually. Third, while alignment datasets may exist for the system’s overall task, they are often not available for the unique sub-tasks of individual components. %

In light of these challenges, there is an urgent need to develop methodologies for aligning compound AI systems. %
While recent studies have explored prompting techniques and instruction tuning approaches~\cite{yuksekgonul2024textgrad, lin2024llm, shinn2024reflexion}, these solutions only partially address the fundamental challenges.
To tackle these  challenges, we make the following contributions:


\begin{itemize}
    \item We formally define the problem of preference learning and alignment of compound AI system and then propose SysDPO to align the entire compound AI system;%
    
    \item We show how SysDPO can be applied to align a compound AI system composed of an LLM agent and a text-to-image diffusion model; 
    \item We demonstrate that aligning compound AI systems increases the success rate in handling complex instructions and coordinating components.

\end{itemize}
These results deepen our understanding of the alignment challenges in compound AI systems and provide a foundation for future research.



\begin{figure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/dag-1.png} 
        \caption{Example 1}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/dag-2.png} 
        \caption{Example 2}
    \end{subfigure}
    
    \caption{ 
    \footnotesize
    \textbf{Example 1.} (LLM+Diffusion Models) The user gives a prompt $x$ which is processed by the LLM $\theta_1$ to produce three captions $y_1, y_2, y_3$  for the diffusion model. The diffusion model $\theta_2$ is called three times to generate images $z_1, z_2, z_3$ based on three captions. 
    \textbf{Example 2.} (Mixture-of-agents) The two-layered MoA composed of three models $\theta_1, \theta_2, \theta_3$. The instruction $x$ is sent to the three models in the first layer, and three text outputs $y_{1, 1}, y_{1, 2}, y_{1, 3}$ are generated respectively. Then, all of the previous texts $(x, y_{1, 1}, y_{1, 2}, y_{1, 3})$ are sent to the second layer, and three outputs $y_{2, 1}, y_{2, 2}, y_{2, 3}$ are generated respectively. We omit the arrow from $x$ to the later layer for better readability.  Finally, the output from the second layer along with the input, i.e., $(x, y_{2, 1}, y_{2, 2}, y_{2, 3})$ is sent to the final model to obtain the final output. }
    \label{fig:dag}
\end{figure}


\section{The SysDPO Framework}\label{sec:problem}
In this section, we introduce the SysDPO pipeline. We start by modeling the structure of compound AI systems as Directed Acyclic Graphs (DAGs), which encode both the connections between agents and the flow of the underlying data generation process. The DAG structure enables us to factorize the joint probability of generated outputs into several components, resolving the non-differentiability issue when aligning multiple agents. We then define a DPO-based loss function that can be optimized from end-to-end simply via gradient descent. 
The end-to-end optimization ensures that each agent is aligned with user-defined preferences. Below, we outline the key steps in the pipeline:




    \paragraph{1. System Representation.} We represent the compound AI system as a Directed Acyclic Graph (DAG).%
    We define nodes as $x, \{y_i\}_{i\in I}, \{z_j\}_{j\in J}$, where $x$ is the input, $y_i$ for $i\in I$ are intermediate outputs and $z_j$ for $j\in J$ are final outputs. Except for the input $x$, each node represents a generated output, given by a single model or an external tool based on some other nodes. We define the set of all generated outputs as $s=\{y_i, z_j\}_{i\in I, j\in J}$. The directed edges represent the flow of the generated data between components. 


    Let us consider two concrete examples of a compound AI system. The first example involves an LLM and a diffusion model as shown in Figure~\ref{fig:compound_example} with the user prompt $x$ being ``generate three separate images of \dots.'' The DAG of this example is shown in Figure~\ref{fig:dag}~(a). The second example is MoA~\cite{wang2024mixture}. It leverages the collective power of multiple LLMs through a layered architecture, where each agent combines outputs from the preceding layer as the auxiliary information to generate responses. We formulate the DAG for a two-layered MoA composed of three models, as shown in Figure~\ref{fig:dag}~(b). 
    \paragraph{2. Probability Factorization.} %

    The DAG structure encodes the conditional independence of the generated data~\cite{pearl2009causality}, enabling the decomposition of the probability of the generated data into multiple terms:
    {\small
    \begin{multline}
        p_\theta(s|x) = 
        \prod_{i \in I, j \in J} p_{\theta_i}(y_i|\texttt{P}(y_i)) \cdot p_{\theta_j}(z_j|\texttt{P}(z_j)),
        \label{eq: decompose}
    \end{multline}
    }
where $\texttt{P}(\cdot)$ returns the parent nodes (may include the input $x$) of a given node in the graph, and $\theta=\{\theta_k:k\in I\cup J\}$ denotes the parameter set of models in the compound AI system. This decomposition, derived from the DAG structure, breaks down the likelihood of system generation into a product of multiple terms, where each term contains a single model, allowing model-dependent optimization.
Take the case of Figure~\ref{fig:dag}~(a) as an example and denoting the set of generated contents as $s=\{y_1,y_2,y_3,z_1,z_2, z_3\}$, we have  $
    p(s|x)=\prod_{i=1}^3 p_{\theta_1}(y_i|x)\cdot p_{\theta_2}(z_i|y_i)$.

    For external tool $\theta_i$ integrated within the system, the probability factorization $p_{\theta_i}(y_i | \texttt{P}(y_i))$ is set to 1, assuming that external tools provide deterministic outputs.
    \paragraph{3. Preference Dataset Construction.} 
    SysDPO optimizes for pairwise preferences by leveraging a preference dataset. The dataset can be obtained in the following way: given a query $x$, the system generates two versions of the responses, which include outputs of every agent. We label the preferred set as  $s^w$, and the not-preferred set as $s^l$. 
    \paragraph{4. Loss Function Design.} 
    Given such a dataset $D$ composed of preference pairs $(x, s^w, s^l)$ and a compound AI system formulated as a DAG, we can apply DPO to align the system~\cite{rafailov2024direct}:
    {\small
\begin{multline}
    L(\theta)= \\
    -\mathbb{E}_{(x, s^w, s^l) \sim D}\left[ \log \sigma\left( \beta\log\frac{p_\theta (s^w|x)}{p_{\bar\theta}(s^w|x)} -\beta\log\frac{p_\theta (s^l|x)}{p_{\bar\theta}(s^l|x)} \right) \right],
    \label{eq:sysdpo_loss}
\end{multline}
    }
where $\bar \theta$ denotes the collection of reference models, $\sigma(\cdot)$ stands for the sigmoid function. By decomposing $p_\theta$ via (\ref{eq: decompose}) in the DPO loss, we derive a differentiable loss function tailored for compound AI systems, which we refer to as the SysDPO loss. Unlike the original DPO loss, which optimizes individual models, SysDPO integrates probability decomposition to capture interactions between multiple components in compound AI systems. 




    
    
    
    
    
    
























\section{Application: Compound AI System of \\a LLM and a Diffusion Model}\label{sec:method}
In this section, we apply SysDPO to a group-image-generation application with an example in Figure~\ref{fig:compound_example}, which involves an LLM $\psi$ and a Diffusion Model $\phi$.    For a single input $x$ provided to the system, the LLM generates an intermediate output $y$, which can be parsed to multiple captions $y_1, y_2, \dots, y_n$. Each $y_i,\;i=1,\ldots, n$ serves as a prompt for the diffusion model. The diffusion model is then queried $n$ times, generating images $z_1, z_2, \dots, z_n$ as the final outputs. %
This multi-step process is modeled as a DAG whose special case $(n=3)$ is shown in Figure~\ref{fig:dag} (a), and it allows us to decompose the generation process by
    {\small
    \begin{align}\label{eq:fact1}
    p(s|x)=p_{\psi}(y|x)  \cdot\prod_{i=1}^n p_{\phi}(z_i|y_i).
    \end{align}
    }
Note that, for better readability,  we adopt a different notation for the models in this section as opposed to the notation used in  Section~\ref{sec:problem}. %
Apply the decomposition of probability~\eqref{eq:fact1} to the loss function~\eqref{eq:sysdpo_loss}, we get the joint loss function of this system
{\small
\begin{align}
    &L(\psi, \phi)= -\E_{(x, s^w, s^l)\sim D} \\
    &\bigg[ \log \sigma\bigg( \beta \left( \log\frac{p_{\psi} (y^w|x)}{p_{\bar\psi}(y^w|x)} + \sum_{i}^{n}\log\frac{p_{\phi} (z_i^w|y_i^w)}{p_{\bar \phi} (z_i^w|y_i^w)}\right) \\
    &-\beta \left( \log\frac{p_{\psi} (y^l|x)}{p_{\bar\psi}(y^l|x)} + \sum_{i}^{n}\log\frac{p_{\phi} (z_i^l|y_i^l)}{p_{\bar \phi} (z_i^l|y_i^l)}\right) \bigg) \bigg], \label{eq:dpo-loss2}
\end{align}
}
where $s^w = \{ y^w, z_1^w, z_2^w, \dots z_n^w \}$, and $\bar \psi, \bar \phi$ are reference models. %
The language model's generation likelihood $p_\psi(y|x)$ is accessible, while the diffusion model's $p_\phi(z|y)$ is not. The following subsection will handle this challenge by delving into the generation process of diffusion models.

\subsection{Handling  the Diffusion Model}


To obtain the diffusion model's generation likelihood, we build upon~\cite{wallace2024diffusion}, which applies DPO to denoising diffusion probabilistic models~\cite{ho2020denoising}, and extend it to accommodate our framework.  
Details of the derivation and the theorem are in Appendix~\ref{appendix: diffusion loss}. 

A diffusion model learns to reverse a diffusion process, represented by a sequence \( z_{0:T} := (z_0, z_1, \dots, z_T) \), where the original image \( z_0 \) is gradually transformed into standard Gaussian noise \( z_T \) over \( T \)  steps. By learning to reverse this process, the model generates images by progressively denoising \( z_T \), starting from noise and reconstructing the original image \( z_0 \). The likelihood of the reverse process is
{\small
\begin{align}
     p_\phi(z_{0:T}|y)=p(z_T)\textstyle\prod_{t=1}^T p_\phi(z_{t-1}|z_{t}, y),
     \label{eq:factor}
\end{align}
}
where each $p_\phi(z_{t-1}|z_{t}, y)$ is a Gaussian density function.

However, the diffusion model does not directly provide the likelihood $p_\phi$, even for a small Gaussian step $p_\phi(z_{t-1}|z_{t}, y)$. To this end, \citet{ho2020denoising} proposed a denoiser $\epsilon_\phi$, which predicts the original image from a noisy input and can be applied to approximate the likelihood. Such denoiser can be learned from data by optimizing the following objective function:
{\small
\begin{align}
    \ell_\epsilon(\phi; t, z^w_{i, t}, y_i^w):=
    \left[ w_t \left\| \epsilon-\epsilon_\phi(z^w_{i, t}, t, y_i^w)  \right\|^2 \right],
\end{align}
}
where $w_t$ is a weight parameter, \( z^w_{i, t} \) is the \( i \)-th output at timestep \( t \), and $\epsilon$ corresponds to the noise added to $z_{i,0}$ from which $z^w_{i, t}$ is derived. Similarly, we use $\ell_\epsilon(\phi; t, z^l_{i, t}, y_i^l)$ to denote the denoising loss for the losing data.




We prove the following theorem, which converts \eqref{eq:dpo-loss2} into a loss function that directly utilizes the denoiser loss function, thereby making the loss function optimizable. 






\begin{theorem}
\label{theorem: diffusion}


    The loss function~(\ref{eq:dpo-loss2}) is upper bounded by
    { \small
\begin{align}
    &L(\psi, \phi)\leq -\E_{(x, s^w, s^l)} \E_t\ \E_{z^w_{i, t}, z^l_{i, t}}\  \Bigg[ \log \sigma \Bigg( \beta \Bigg( \\
     &\Bigg( \log\frac{p_{\psi} (y^w|x)}{p_{\bar\psi}(y^w|x)}  + T \sum_{i} (-\ell_\epsilon(\phi; t, z^w_{i, t}, y_i^w)+\ell_\epsilon(\bar\phi; t, z^w_{i, t}, y_i^w)) \Bigg) - \\
     & \Bigg( \log\frac{p_{\psi} (y^l|x)}{p_{\bar\psi}(y^l|x)} + T \sum_{i}(-\ell_\epsilon(\phi; t, z^l_{i, t}, y_i^l)+\ell_\epsilon(\bar\phi; t, z^l_{i, t}, y_i^l))\Bigg) \Bigg)\Bigg)\Bigg]. 
\end{align}
}

\end{theorem}


Thus, we obtain a tractable loss function for SysDPO.




\section{Experiments}\label{sec:exp}

We evaluate the effectiveness of SysDPO alignment in a compound AI system described in section~\ref{sec:method}. 
We train and evaluate the system on a dataset of multi-modal progression tasks, where the system generates sequences of images with a specific scene-related attribute that varies progressively. Examples of inputs and outputs are provided in Appendix~\ref{appendix:system_examples}.
Our evaluation focuses on the coherence among images and their alignment with holistic preferences.


\paragraph{Dataset Construction.}
We constructed a custom dataset using the following steps:
\begin{enumerate}
    \item \textbf{Attribute Selection:} We use a regressor from \citet{zhuang2021enjoy} which gives scores from $[0,1]$ to images based on $40$ distinct scene-related attributes (e.g., brightness, coldness, fog density).
    \item \textbf{Instruction Design:} For each attribute, we query GPT-4 to generate $250$ user prompts of generating a sequence of images representing the progression of the intensity of that attribute. To ensure the diversity of user prompts, we generate prompts using four distinct prompt styles from \citet{qin2024diffusiongpt}. Details are provided in Appendix~\ref{appendix:prompt_details}. %
   \item \textbf{Constructing Chosen and Rejected Pairs:}
    For each user prompt, four image sequences are generated and ranked using the Preference Score \( q \), described below in~\eqref{eq:pscore}. Six comparison pairs are constructed from the four samples. The instance among the two pairs with the higher preference score is marked as the preferred. The dataset contains a total of $6000$ comparison pairs.
\end{enumerate}
\paragraph{Preference Score.} 
To compare the generated image sequences, we define a \emph{preference score} \( q \) that evaluates both order consistency and distribution evenness. This metric is based on the attribute scores assigned to the images by the regressor from \citet{zhuang2021enjoy}. Given a sequence of three images with attribute scores \( a_1, a_2, \) and \( a_3 \), the Preference Score \( q \) is computed as:
{
\small
\begin{equation}\label{eq:pscore}
q = -\left(a_1 - a_3 + \left|a_2 - (a_1 + a_3)/{2}\right|\right)
\end{equation}
}
Sequences with higher \( q \) values are preferred, as they reflect correct ordering and smoother distributions. Conversely, reversed or uneven sequences result in lower \( q \). 

For further details, including examples illustrating the calculation of \( q \), please refer to Appendix~\ref{appendix:preference_score}.
\paragraph{Models.} For dataset construction and evaluation, we use an instruction-tuned Llama-3-8B model~\cite{llama3modelcard} as the language model. 
To generate image sequences for constructing chosen and rejected samples in the dataset, we employ Stable Diffusion XL (SDXL)~\cite{podell2023sdxl}. For training purposes, we use Stable Diffusion 1.5~\cite{Rombach_2022_CVPR} which provides a balance between computational efficiency and generation quality.
\paragraph{Evaluation.} The performance of the system is evaluated using two metrics. The first metric is the \textbf{Average Preference Score} across all generated sequences from the test dataset.
The second evaluation metric is the \textbf{Order Consistency Ratio}, measuring the proportion of generated sequences in the correct order, i.e., where \( a_1 < a_2 < a_3 \). 
\paragraph{Baselines.}
To evaluate the effectiveness of the proposed SysDPO joint alignment approach, we compare it against four baseline methods.

\begin{enumerate}
    \item  \textbf{System Before Alignment.} The first baseline represents the system prior to applying SysDPO. Notably, Llama-3-8B-it is instruction-tuned, so that it serves as a baseline for conventional separately aligned systems.

    \item  \textbf{Best-of-4 Sampling Baseline.}
For this baseline, we sample four image sequences generated by the system without optimization. For each user prompt, we select the best-performing sequence based on the Preference Score. The average of the selected sequences is reported. 

    \item  \textbf{Only Train Language Model or Diffusion Model.}
In this baseline, we freeze the weights of the diffusion model or language model and train only another model using the dataset and proposed loss function of SysDPO. 
\end{enumerate}

\noindent \textbf{Results.}
This section presents the performance of the proposed SysDPO compared to the baselines. We evaluate the system using the Preference Score and Order Consistency Ratio. Examples of system outputs before and after training can be found in Appendix~\ref{appendix:system_examples}.\footnote{\footnotesize We use LoRA to train the language model, reducing memory overhead. The experiments were performed on 2 A100 GPUs, requiring around 4 hours of training.}

\begin{table}[h]
\centering
\caption{
\footnotesize
Performance comparison of the proposed method and baselines. Higher Preference Scores (Pref. Score) and higher Order Consistency Ratios (OC Ratio) are better.}
\label{tab:combined_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method}                     & \textbf{Pref. Score} & \textbf{OC Ratio} \\ \hline
SysDPO (Proposed)          &   \underline{0.25}                   & \underline{70\%}                             \\ \hline
System Before Alignment             &   -0.20                  & 32\%                             \\ \hline
Best-of-Sampling                    &   0.16                  & 67\%                             \\ \hline
Only Train Language Model           &   0.23                   & 65\%                             \\ \hline
Only Train Diffusion Model          &   -0.03                  & 35\%                             \\ \hline
\end{tabular}
\end{table}

The results in Table~\ref{tab:combined_results} demonstrate the importance of alignment in compound AI systems and the effectiveness of the proposed SysDPO alignment approach. The ``System Before Alignment" baseline achieves poor performance, with a low Preference Score and a low Order Consistency Ratio (32\%), indicating that conventionally instruction-tuned components are insufficient for ensuring coherent collaboration in compound systems. The ``Only Train Language Model" baseline achieves significantly better results than the ``Only Train diffusion Model", with a Preference Score of 0.23 and a Ratio of 65\%. This is due to the LLM's role in generating captions that control output sequences, influencing the overall progression and coherence of the system. SysDPO achieves the best Preference Score (0.25) and the highest Order Consistency Ratio (70\%). These results validate the effectiveness of our SysDPO algorithm, demonstrating its ability to optimize both components together for superior performance in generating coherent image sequences.

\section{Discussion and Future Work}\label{sec:conclusion}

Our preliminary investigations indicate that the proposed formulation and methodology are promising for aligning compound AI systems. However, further experimental investigations are necessary to evaluate its potential comprehensively. For instance, how does our approach compare to existing techniques, such as instruction tuning and prompting strategies? Additionally, the scalability of our method to more complex applications, where the number of components and interactions grows significantly, remains open.


Despite these open questions, our work establishes a solid foundation for aligning compound AI systems as cohesive entities. We believe that the insights and framework presented here pave the way for promising advancements in this area of research.

\newpage
\section*{Acknowledgments}
We thank the anonymous reviewers for their valuable feedback and suggestions. SK acknowledges support from NSF grants 2046795 and 2205329, IES grant R305C240046, the MacArthur Foundation, Stanford HAI, OpenAI, and Google. KT acknowledges support from the NSF Graduate Research Fellowship.










\bibliography{ref}

\appendix
\onecolumn


\part*{\centering Appendix}    



\section{Details of Diffusion Models and Proof of Theorem~\ref{theorem: diffusion} }
\label{appendix: diffusion loss}
In this section of the appendix, we provide a detailed explanation of the DDPM diffusion model and the derivation of Theorem~\ref{theorem: diffusion}. 


\subsection{Denoising Diffusion Probabilistic Model (DDPM)}
DDPM~\cite{ho2020denoising} is a widely used class of diffusion model. Below is a highlight of the key ingredient we need for DPO for DDPM~\cite{wallace2024diffusion}, with our framework. 

Given a real image $z_0$, consider a diffusion process, which we call the forward process, gradually making the original image into Gaussian noise $z_T$ after $T$ steps, i.e.,
\begin{align}
    z_0\to z_1\to z_2\to \dots \to z_T \sim \gN(0, I).
\end{align}

The goal of the diffusion model $\phi$ is to reverse this process that recovers an image from noise. The forward process and the reverse process are denoted respectively as 
\begin{align}
    q(z_{0:T}|y), \qquad p_\phi(z_{0:T}|y),
\end{align}
where $y$ is the context, i.e., the prompt to the diffusion model. 

Note that both the forward and backward processes are Markovian, and in particular we have the nice property that the forward process
\begin{align}
     q(z_{0:T}|y)=q(z_0|y) \prod_{t=1}^T q(z_t|z_{t-1}), 
     \qquad \text{where each } q(z_t|z_{t-1}) \text{ is a Gaussian.}
\end{align}
Similarly, the reverse process
\begin{align}
     p_\phi(z_{0:T}|y)=p(z_T)\prod_{t=1}^T p_\phi(z_{t-1}|z_{t}, y),  \qquad \text{where each } p_\phi(z_{t-1}|z_{t}, y) \text{ is a Gaussian.} \label{eq:factor-1}
\end{align}
In this formulation, the ideal goal for the diffusion model is that $q(z_{0:T}|y)=p_\phi(z_{0:T}|y)$. However, this is not easy to optimize directly. With some analysis, the DDPM paper~\cite{ho2020denoising} proposes to minimize for
\begin{align}
    D_{KL}(q(z_{t-1}|z_t, z_0, y)\| p_\phi(z_{t-1}|z_t, y)) 
    \quad \text{for}\quad  t\sim \gU([T]), z_0\sim q(z_0|y),
\end{align}
where $\gU(\cdot)$ denotes the uniform distribution on a set, and $[T]$ denotes the set of $\{1, 2, \dots ,T\}$.
This is done by learning a denoiser $\epsilon_\phi$ operating in the following way. For a real image $z_0\sim q(z_0|y)$, we sample noise $\epsilon \sim \gN(0, I)$, and have 
\begin{align}
    z_t(z_0, \epsilon)=\sqrt{\bar \alpha_t} z_0 + \sqrt{1-\bar \alpha_t} \epsilon, \label{eq:diffusion-noise}
\end{align}
where $\bar \alpha_t$ is some parameter such that $z_t\sim q(z_t|z_0)$. Then, the denoiser predicts the noise $\epsilon$ that is added to the $z_0$. I.e.,
\begin{align}
    \epsilon_\phi(z_t(z_0, \epsilon), t, y) \text{ aims to predict  } \epsilon.
\end{align}
The denoiser $\epsilon_\phi$ is essentially a reparameterization of the mean of $p_\phi(z_{t-1}|z_{t}, y)$.

The \textbf{key ingredient} is that, as shown in \cite{ho2020denoising},
\begin{align}
    D_{KL}(q(z_{t-1}|z_t, z_0, y)\| p_\phi(z_{t-1}|z_t, y))
    =\E_{\epsilon\sim \gN(0, I)}\left[ w_t \left\| \epsilon-\epsilon_\phi(z_t(z_0, \epsilon), t, y)  \right\|^2 \right] + C, \label{eq:KL-denoiser}
\end{align}
where $w_t$ is a weight parameter and $C$ is a constant independent of model $\phi$.

Therefore, modeling $\epsilon_\phi$ by a neural net, the DDPM model $\phi$ is trained to minimize the above objective averaged over samples of $y, z_0, \epsilon, t$.

\subsection{Dealing with the Diffusion Model in SysDPO}
Recall in the main text we obtain the System DPO loss function as \eqref{eq:dpo-loss2}:
\begin{align}
    &L(\psi, \phi)= -\E_{(x, s^w, s^l)\sim D}\bigg[ \log \sigma\bigg( \beta \left( \log\frac{p_{\psi} (y^w|x)}{p_{\bar\psi}(y^w|x)} + \sum_{i}^{n}\log\frac{p_{\phi} (z_i^w|y_i^w)}{p_{\bar \phi} (z_i^w|y_i^w)}\right) -\beta \left( \log\frac{p_{\psi} (y^l|x)}{p_{\bar\psi}(y^l|x)} + \sum_{i}^{n}\log\frac{p_{\phi} (z_i^l|y_i^l)}{p_{\bar \phi} (z_i^l|y_i^l)}\right) \bigg) \bigg].
\end{align}
The next step is to convert the likelihood of the diffusion model $p_\phi$ to something optimizable. 

The first step is to consider the generated image as the whole process, i.e., 
\begin{align}
    z_{i, 0:T}:=\{z_{i, 0}, z_{i, 1},\dots,  z_{i, T}\},
\end{align}
where $z_{i, 0}$ is the generated image, while the others are things in the middle. Following the same notation, we denote 
\begin{align}
    z_{i, t-1, t} := \{z_{i, t-1}, z_{i, t}\}.
\end{align}

The preference is considered to be given to every process that generates $z_0$ as the end outcome. 
Following \cite{wallace2024diffusion}, we have 
\begin{align}
    &L(\theta, \phi)=-\E_{(x, s^w, s^l)\sim D}\Bigg[ \log \sigma \Bigg( \beta \E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})} \\
    &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + \sum_{i}\log\frac{p_{\phi} (z^w_{i, 0:T}|y^w_i}{p_{\bar \phi} (z^w_{i, 0:T}|y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + \sum_{i}\log\frac{p_{\phi} (z^l_{i, 0:T}|y^l_i)}{p_{\bar \phi} (z^l_{i, 0:T}|y^l_i)}\right) \right) \Bigg)\Bigg]. \label{eq:diff-dpo-loss}
\end{align}


Recall the factorization of the reverse process (\eqref{eq:factor-1}), we have
\begin{align}
     &L(\theta, \phi)=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})}\\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + \sum_{i}\sum_{t=1}^T\log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + \sum_{i}\sum_{t=1}^T\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg].
\end{align}
Note that $\sum_{t=1}^T=T\E_{t\sim \gU([T])}$ for $t$ is a random variable uniformly distributed on $1, 2, \dots, T$. Simply denoting $\E_{t\sim \gU([T])}$ as $E_t$,  we have 
\begin{align}
    &L(\theta, \phi)=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})}\\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i}\E_t \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\E_t \log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg]\\
     &=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})}\ \E_t \\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg].
\end{align}
Next, we may further simplify the equation by switching $\E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})}$ and $\E_t$ in the above, i.e.,
\begin{align}
    &L(\theta, \phi)=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_t\ \E_{z^w_{i, 1:T}\sim q(z^w_{i, 1:T}|z^w_{i, 0}), z^l_{i, 1:T}\sim q(z^l_{i, 1:T}|z^l_{i, 0})} \\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg]\\
     &=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_t\ \E_{z^w_{i, t-1, t}\sim q(z^w_{i, t-1, t}|z^w_{i, 0}), z^l_{i, t-1, t}\sim q(z^l_{i, t-1, t}|z^l_{i, 0})} \\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg]\\
\end{align}
The rationale for the above can be illustrated as follows. Consider a random variables $Z_1, \dots, Z_T$, and any function $f:(Z_{t-1}, Z_t)\to \sR$ for any $t\in [T]$. Then, denoting $\delta_s^t$ as the indicator function, i.e., $\delta_s^t=1$ only if $s=t$, and $\delta_s^t=0$ otherwise, we can derive
\begin{align}
    \E_{Z_{1:T}}\ \E_{t\sim \gU([T])}\ f(Z_{t-1}, Z_t)&= \E_{Z_{1:T}}\ \E_{t\sim \gU([T])}\ \sum_{s=1}^T \delta_s^t \cdot f(Z_{s-1}, Z_s)\\
    &=\E_{t\sim \gU([T])}\  \sum_{s=1}^T \delta_s^t \cdot \E_{Z_{1:T}} f(Z_{s-1}, Z_s)\\
    &=\E_{t\sim \gU([T])}\  \sum_{s=1}^T \delta_s^t \cdot \E_{Z_{s-1}, Z_s} f(Z_{s-1}, Z_s)\\
    &=\E_{t\sim \gU([T])}\  \cdot \E_{Z_{t-1}, Z_t} f(Z_{t-1}, Z_t).
\end{align}


Next, noting that $q(z^w_{i, t-1, t}|z^w_{i, 0})=q(z^w_{i, t}|z^w_{i, 0})\cdot q(z^w_{i, t-1}|z^w_{i, 0}, z^w_{i, t})$ (similarly for $q(z^l_{i, t-1, t}|z^l_{i, 0})$), we can first sample $z^w_{i, t}$ and then $z^w_{i, t-1}$ separately, i.e., 
\begin{align}
    &L(\theta, \phi)=-\E_{(x, s^w, s^l)\sim D} \Bigg[ \log \sigma \Bigg(\beta \E_t\ \E_{z^w_{i, t}\sim q(z^w_{i, t}|z^w_{i, 0}), z^l_{i, t}\sim q(z^l_{i, t}|z^l_{i, 0})}\  \E_{z^w_{i, t-1}\sim q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}), z^l_{i, t-1}\sim q(z^l_{i, t-1}|z^l_{i, 0}, z^l_{i, t})} \\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg].
\end{align}
Since $-\log \sigma$ is convex, by Jensen's inequality, we have
\begin{align}
    &L(\theta, \phi)\leq -\E_{(x, s^w, s^l)\sim D} \E_t\ \E_{z^w_{i, t}\sim q(z^w_{i, t}|z^w_{i, 0}), z^l_{i, t}\sim q(z^l_{i, t}|z^l_{i, 0})}\ \Bigg[ \log \sigma \Bigg(\beta  \E_{z^w_{i, t-1}\sim q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}), z^l_{i, t-1}\sim q(z^l_{i, t-1}|z^l_{i, 0}, z^l_{i, t})} \\
     &\left(  \left( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\right) - \left( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}\log\frac{p_{\phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}{p_{\bar \phi} (z^l_{i, t-1}|z^l_{i, t}, y^l_i)}\right) \right)\Bigg)\Bigg]. \label{eq:loss-to-KL}
\end{align}

Recall that what we have done so far is all for making the diffusion model's log probability efficiently computable. To complete the derivation, it remains to convert the log-probabilities to the denoising loss via \eqref{eq:KL-denoiser}. Specifically, with $C$ being the constant appears in \eqref{eq:KL-denoiser}, we can see that
\begin{align}
    &\E_{z^w_{i, t-1}\sim q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t})}\log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}\\
    &=\E_{z^w_{i, t-1}\sim q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t})}\left( \log\frac{p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t})}-\log\frac{p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)}{q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t})} \right)\\
    &= -D_{KL}(q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}) \| p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)) +D_{KL}(q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}) \| p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i))\\
    &=-D_{KL}(q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}) \| p_{\phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i)) +C +D_{KL}(q(z^w_{i, t-1}|z^w_{i, 0},z^w_{i, t}) \| p_{\bar \phi} (z^w_{i, t-1}|z^w_{i, t}, y^w_i))-C\\
    &= -\E_{\epsilon\sim \gN(0, I)}\left[ w_t \left\| \epsilon-\epsilon_\phi(z_t(z^w_{i, 0}, \epsilon), t, y^w_i)  \right\|^2 \right]+\E_{\epsilon\sim \gN(0, I)}\left[ w_t \left\| \epsilon-\epsilon_{\bar \phi}(z_t(z^w_{i, 0}, \epsilon), t, y^w_i)  \right\|^2 \right].
\end{align}
To simplify the notation, we denote
\begin{align}
    \ell_\epsilon(\phi; t, z^w_{i, t}, y^w_i):=
    \left[ w_t \left\| \epsilon-\epsilon_\phi(z^w_{i, t}, t, y^w_i)  \right\|^2 \right],
\end{align}
where the $\epsilon$ corresponds to the noise from which $z^w_{i, t}$ is derived (see \eqref{eq:diffusion-noise}). Similarly, we use $\ell_\epsilon(\phi; t, z^l_{i, t}, y^l_i)$ to denote the denoising loss for the losing data. 

Thus we can write \eqref{eq:loss-to-KL} as
\begin{align}
    &L(\theta, \phi)\leq -\E_{(x, s^w, s^l)\sim D} \E_t\ \E_{z^w_{i, t}\sim q(z^w_{i, t}|z^w_{i, 0}), z^l_{i, t}\sim q(z^l_{i, t}|z^l_{i, 0})}\   \\
     &\Bigg[\Bigg[ \log \sigma \Bigg(\beta \Bigg(  \Bigg( \log\frac{p_{\theta} (y^w|x)}{p_{\bar\theta}(y^w|x)} + T \sum_{i} (-\ell_\epsilon(\phi; t, z^w_{i, t}, y^w_i)+\ell_\epsilon(\bar\phi; t, z^w_{i, t}, y^w_i)) \Bigg) \\
     & \quad - \Bigg( \log\frac{p_{\theta} (y^l|x)}{p_{\bar\theta}(y^l|x)} + T \sum_{i}(-\ell_\epsilon(\phi; t, z^l_{i, t}, y^l_i)+\ell_\epsilon(\bar\phi; t, z^l_{i, t}, y^l_i))\Bigg) \Bigg)\Bigg)\mid t\Bigg]\Bigg]. 
\end{align}
Thus, we complete the proof of Theorem~\ref{theorem: diffusion}.

\newpage
\section{Complete Prompts in Figure~\ref{fig:compound_example}}

The complete prompts generated by GPT-4 for DALL-E to generate the six images in Figure~\ref{fig:compound_example} are as follows. 
\begin{enumerate}[label=(\alph*)]
    \item A realistic image of a calm cat sitting comfortably on a soft cushion. The cat has a serene expression, with its eyes half-closed in contentment. It has fluffy fur, predominantly grey with white patches, and its tail is neatly curled around its body. The background is a cozy room with a hint of sunlight filtering through a window, casting a warm glow on the cat.
    \item A realistic image of a slightly irritated cat sitting on a wooden floor. The cat has a slight frown and its ears are tilted back slightly, indicating mild annoyance. It has short fur, a mix of black and white, and is staring directly at the viewer with wide eyes. The background is simple, with a few scattered cat toys and a small plant, adding to the homey atmosphere.
    \item A realistic image of a very angry cat with its fur bristled and ears flattened against its head. The cat's eyes are narrowed and glowing, mouth slightly open in a hiss. It has thick, long fur that is mostly black with some orange stripes. The background shows a stormy sky through a window, enhancing the dramatic mood. The cat is on a dark rug, which contrasts with its bright, intense eyes.
    \item A cartoon-style image of a cat with a slightly annoyed expression. The cat has fluffy white fur, bright green eyes, and its ears are slightly pulled back. It is sitting comfortably on a soft blue cushion, with its tail flicking lightly to the side. The background is a simple, soft yellow to keep the focus on the cat's expression.
    \item A cartoon-style image of a cat with an angry expression. The cat has fluffy grey fur, narrowed yellow eyes, and its ears are flat against its head. It is standing with an arched back on a wooden floor, with its fur bristling and its claws slightly out. The background is a dimly lit room, adding to the menacing atmosphere around the cat.
    \item A cartoon-style image of a cat with a furious expression. The cat has short black fur, glaring red eyes, and its ears are pinned back. It is hissing aggressively, showing sharp teeth, with a puffed tail. The cat stands on a stormy night background, with lightning in the sky, enhancing the dramatic and intense mood.
\end{enumerate}

\section{Preference Score Calculation}\label{appendix:preference_score}

\paragraph{Definition.} The Preference Score \( q \) evaluates the quality of a sequence of three images with attribute scores \( a_1, a_2, \) and \( a_3 \in [0, 1] \), and is computed as:

\begin{equation}\label{eq:pscore_appendix}
q = -\left(a_1 - a_3 + \left|a_2 - \frac{a_1 + a_3}{2}\right|\right)
\end{equation}

\paragraph{Properties.} The Preference Score reflects two aspects:
\begin{enumerate}
    \item \textbf{Order Consistency:} A correctly ordered sequence (\( a_1 < a_2 < a_3 \)) yields a higher \( q \) value, while a reversed sequence results in a lower \( q \) value.
    \item \textbf{Distribution Evenness:} A sequence where \( a_2 \) is closer to the midpoint between \( a_1 \) and \( a_3 \) maximizes the score.
\end{enumerate}

\paragraph{Example Calculation.}
Consider four sequences of attribute scores:
\begin{itemize}
    \item Sequence \( \mathbf{a} = [1, 0.5, 0] \)
    \item Sequence \( \mathbf{b} = [0, 1, 0.9] \)
    \item Sequence \( \mathbf{c} = [0, 0.5, 1] \)
\end{itemize}

For \( \mathbf{a} \):
\[
q_a = -\left(1 - 0 + \left|0.5 - \frac{1 + 0}{2}\right|\right) = -1
\]

For \( \mathbf{b} \):
\[
q_b = -\left(0 - 0.9 + \left|1 - \frac{0 + 0.9}{2}\right|\right) =  0.35
\]

For \( \mathbf{c} \):
\[
q_c = -\left(0 - 1 + \left|0.5 - \frac{0 + 1}{2}\right|\right) =  1
\]

Since \( q_a < q_b \), sequence \( \mathbf{b} \) is preferred between sequence \( \mathbf{a} \) and \( \mathbf{b} \). Sequence \( \mathbf{c} \) is preferred between sequence \( \mathbf{b} \) and \( \mathbf{c} \). This illustrates how the Preference Score penalizes uneven intermediate distributions or incorrect orderings.

\section{Prompt Styles and Examples}
\label{appendix:prompt_details}

To ensure diversity in user prompts, we utilize four distinct prompt styles inspired by \cite{qin2024diffusiongpt}. Each style varies in how it frames the objective for image generation. For illustration, all the examples below are based on the attribute "bright," showcasing how this attribute can be expressed in different styles.

\subsection{Prompt Styles}

\paragraph{Prompt-Based Style.}
This style of prompt directly describes the objective to be generated. It provides a clear and concise target for the system. For example:
\begin{itemize}
    \item "A series of images showing a garden with increasing brightness, from dawn to midday."
\end{itemize}

\paragraph{Instruction-Based Style.}
This style uses instructional language to explicitly direct the system on what to generate. The phrasing is structured as a command or directive. For example:
\begin{itemize}
    \item "Generate a series of images of a morning scene, increasing the brightness and cheerfulness."
\end{itemize}

\paragraph{Inspiration-Based Style.}
This style reflects a user’s desire or inspiration for what they want to see. The prompt is expressed as a personal request or imaginative wish. For example:
\begin{itemize}
    \item "I want to see a series of images of a mountain as it gets progressively brighter."
\end{itemize}

\paragraph{Hypothesis-Based Style.}
This style frames the generation task as a hypothetical scenario, often using conditional or reasoning-based language. The prompt includes both the condition and the desired outcome. For example:
\begin{itemize}
    \item "If the scene becomes brighter, the series of images will show progressively more illuminated scenes."
\end{itemize}

\section{Examples of Input and Output of Experiments}
\label{appendix:system_examples}
This appendix presents illustrative examples of the system’s inputs and outputs. Each example consists of a user prompt as input and a sequence of three images as the output. Examples include:
\begin{itemize}
    \item Success cases before training, highlighting potential alignment issues in untrained systems.
    \item Failed cases before training, showcasing issues with order consistency or uneven attribute progression.
    \item Improved cases after training, demonstrating the effectiveness of the proposed SysDPO alignment method.
\end{itemize}

\subsection{Success Case Before Training}
\textbf{Input Prompt:} "I want to see a series of images of a beach, with varying intensity of sunlight from overcast to bright sun."\\
\textbf{Attribute:} Sunny\\
\textbf{Intermediate Captions:}
\begin{enumerate}[label=(\alph*)]
    \item Generate an image of a serene beach on a cloudy day, with the sun hidden behind thick grey clouds, casting a soft, diffused light on the sand and sea. The waves are calm, and a few seagulls are flying overhead.
    \item Create an image of the same beach, but with the sun breaking through the clouds, casting a warm, golden light on the sand and sea. The waves are slightly choppy, and a few beach umbrellas and sun loungers are scattered along the shore.
    \item Produce an image of the beach on a bright, sunny day, with the sun high in the sky and shining brightly down on the sand and sea. The waves are crashing against the shore, and a group of people are enjoying the day, with some playing beach volleyball and others swimming in the ocean.
\end{enumerate}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/sunny_88_1_0.png}
    \includegraphics[width=0.3\textwidth]{figures/sunny_88_1_1.png}
    \includegraphics[width=0.3\textwidth]{figures/sunny_88_1_2.png}
    \caption{An example of a successful output before training.}
    \label{fig:success_before_training}
\end{figure}

\subsection{Failed Case Before Training}
\textbf{Input Prompt:} "I want to see a series of images of a lake as the ice increases." \\
\textbf{Attribute:} Ice \\
\textbf{Intermediate Captions:}
\begin{enumerate}[label=(\alph*)]
    \item Generate an image of a serene lake on a warm summer day, with calm water and lush greenery surrounding it, but with no signs of ice or frost.
    \item Create an image of the same lake but with a thin layer of ice starting to form on the surface, with ripples and imperfections, and the surrounding landscape showing subtle signs of autumnal change, such as fallen leaves and fading greenery.
    \item Produce an image of the lake completely frozen, with a thick layer of ice covering its surface, and the surrounding landscape transformed into a winter wonderland, with snow-covered trees, frozen waterfalls, and a crisp, blue-gray sky.
\end{enumerate}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/ice_167_3_0.png}
    \includegraphics[width=0.3\textwidth]{figures/ice_167_3_1.png}
    \includegraphics[width=0.3\textwidth]{figures/ice_167_3_2.png}
    \caption{An example of a failure case before training. The first and second images lack clear progression in ice intensity.}
    \label{fig:failure_before_training}
\end{figure}

\subsection{Improved Case After Training}
\textbf{Input Prompt:} "Create a series of images of a lake, increasing the thickness of ice." \\
\textbf{Attribute:} Ice \\
\textbf{Intermediate Captions:}
\begin{enumerate}[label=(\alph*)]
    \item Generate a serene summer day with a calm and clear lake shoreline, featuring lush green trees and blooming vegetation, with water as the main focus. The water should be smooth and reflective, with a few boats or sailboats in the background. Ignore the water conditions.
    \item Continue with the same shoreline and surroundings, but with the onset of autumn, add gentle winds and decreasing temperatures. The vegetation should be dormant, and while the water is still calm, start to notice a slight decrease in clarity. There should be no ice yet, but hint at the changing conditions with minimal foliage or bare trees along the shore.
    \item In the dead of winter, depict a frozen landscape with the same shoreline, but now with thick, rugged ice covering the entire lake. The trees should be bare and shown as silhouettes against the harsh winter sky. The goal is a highly realistic, textured ice surface with no signs of water, and incorporate any relevant winter elements like snow-covered ground or bare branches.
\end{enumerate}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/ice_1_0.png}
    \includegraphics[width=0.3\textwidth]{figures/ice_1_1.png}
    \includegraphics[width=0.3\textwidth]{figures/ice_1_2.png}
    \caption{An example of an improved case after training. The sequence shows smooth and consistent progression in the ice intensity.}
    \label{fig:improved_after_training}
\end{figure}


\end{document}
