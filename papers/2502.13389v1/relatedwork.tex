\section{Related Works}
\textbf{Mathematical Reasoning.}
Recent advancements in LLMs for mathematical reasoning have primarily focused on two broad categories of approaches: prompt engineering and learning to reason. Prompt engineering techniques design specific prompting strategies, such as CoT~\citep{wei2022chain,kojima2022large,wang2022self,zhang2022automatic}, multi-path reasoning~\citep{yao2023tot,yao2022react,besta2024graph} or divide-and-conquer~\citep{zhou2022least,sel2023algorithm,wang2023plan} to guide the model to generate step-by-step reasoning processes during the inference phase. In contrast, learn-to-reason approaches train the model explicitly to improve its intrinsic reasoning ability. Extensive math and code corpora are first utilized to build the fundamental reasoning capability in the pre-training phase~\citep{lewkowycz2022solving,shao2024deepseekmath,yang2024qwen2,abdin2024phi,dubey2024llama} and various techniques, such as supervised fine-tuning~\citep{yuan2023scaling,wang2023mathcoder,min2024imitateexploreselfimprovereproduction}, direct preference optimization~\citep{rafailov2024direct,lai2024step} and reinforcement learning~\citep{reft2024,shao2024deepseekmath,guo2025deepseek} are used to further enhance the mathematical reasoning ability during post-training.


\textbf{Tree-Search Reasoning.}
While most CoT methods have typically adopted an auto-regressive reasoning manner, there has been a trend to engage in more complicated reasoning architectures like trees~\citep{koh2024tree,zhang2024llamaberrypairwiseoptimizationo1like}. Recently, various methods for exploring tree structures have been devised to identify optimal reasoning trajectories, \textit{e.g.},  tree of thought~\citep{yao2023tot}, graph of thoughts~\citep{besta2024graph} and Monte Carlo tree search~\citep{hao2023reasoninglanguagemodelplanning,chen2024alphamath,zhang2024rest}. These direct tree-search methods struggle with limited exploration due to the homogeneous reasoning paths and the large search space. Although rStar~\citep{qi2024mutualreasoningmakessmaller} utilizes functional prompts in tree search to diversify and constrain the search process, they only act as external constraints in the inference phase while we introduce learnable functional tokens during model training to facilitate effective and efficient exploration, thus improving the model's intrinsic reasoning ability.

\textbf{LLM Self-Improvement.}
Self-improvement with self-generated reasoning paths has gained increasing interest in mathematical reasoning due to the lack of golden rationales. Early methods~\citep{yuan2023scaling,zelikman2022star} utilize reject sampling to select reasoning paths with correct answers for iterative self-training, which often leads to local optimal due to the homogeneity of self-generated reasoning paths. Recent approaches explore MCTS~\citep{zhang2024rest,tian2024toward,hao2023reasoninglanguagemodelplanning,zhang2024accessing} to generate more diverse reasoning paths offline or adopt random sampling to produce abundant reasoning paths during the online reinforcement learning phase~\citep{reft2024,wang2024math,lambert2024t}. However, these methods struggle to fully explore the reasoning space, especially for smaller language models. In RFTT, we employ function tokens for automatic exploration of the reasoning space through token-guided tree traversal, achieving self-improvement for functional reasoning.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/RFTT.pdf}
    \vspace{-18pt}
    \caption{An illustrative diagram of the online reinforcement learning phase in RFTT. The LLM policy directly samples functional tokens from its vocabulary to autonomously expand reasoning trees to search for the final solutions. Then we use online reinforcement learning with process rewards to optimize the functional reasoning capabilities of the LLM policy.
    }
    \label{fig:rftt}
    \vspace{-8pt}
\end{figure*}