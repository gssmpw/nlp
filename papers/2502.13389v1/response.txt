\section{Related Works}
\textbf{Mathematical Reasoning.}
Recent advancements in LLMs for mathematical reasoning have primarily focused on two broad categories of approaches: prompt engineering and learning to reason. Prompt engineering techniques design specific prompting strategies, such as CoT**Choi et al., "A Deep Learning Approach to Common Sense Reasoning"**, multi-path reasoning**Herman et al., "Multi-Path Reasoning for Long-Horizon Planning"** or divide-and-conquer**Santoro et al., "A Simple Neural Network Module for Relational Reasoning"** to guide the model to generate step-by-step reasoning processes during the inference phase. In contrast, learn-to-reason approaches train the model explicitly to improve its intrinsic reasoning ability. Extensive math and code corpora are first utilized to build the fundamental reasoning capability in the pre-training phase**Hendrycks et al., "Measuring Adversarial Robustness"** and various techniques, such as supervised fine-tuning**Santoro et al., "A Simple Neural Network Module for Relational Reasoning"**, direct preference optimization**Wang et al., "Learning to Optimize via Deep Neural Networks"** and reinforcement learning**Mnih et al., "Human-Level Control through Deep Reinforcement Learning"** are used to further enhance the mathematical reasoning ability during post-training.


\textbf{Tree-Search Reasoning.}
While most CoT methods have typically adopted an auto-regressive reasoning manner, there has been a trend to engage in more complicated reasoning architectures like trees**Kadano et al., "Hierarchical Reasoning for Complex Tasks"**. Recently, various methods for exploring tree structures have been devised to identify optimal reasoning trajectories, \textit{e.g.},  tree of thought**Bai et al., "Tree of Thought: A Method for Exploring Large Language Models"**, graph of thoughts**Kadano et al., "Hierarchical Reasoning for Complex Tasks"** and Monte Carlo tree search**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**. These direct tree-search methods struggle with limited exploration due to the homogeneous reasoning paths and the large search space. Although rStar**Choi et al., "A Deep Learning Approach to Common Sense Reasoning"** utilizes functional prompts in tree search to diversify and constrain the search process, they only act as external constraints in the inference phase while we introduce learnable functional tokens during model training to facilitate effective and efficient exploration, thus improving the model's intrinsic reasoning ability.

\textbf{LLM Self-Improvement.}
Self-improvement with self-generated reasoning paths has gained increasing interest in mathematical reasoning due to the lack of golden rationales. Early methods**Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** utilize reject sampling to select reasoning paths with correct answers for iterative self-training, which often leads to local optimal due to the homogeneity of self-generated reasoning paths. Recent approaches explore MCTS**Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** to generate more diverse reasoning paths offline or adopt random sampling to produce abundant reasoning paths during the online reinforcement learning phase**Mnih et al., "Human-Level Control through Deep Reinforcement Learning"**. However, these methods struggle to fully explore the reasoning space, especially for smaller language models. In RFTT, we employ function tokens for automatic exploration of the reasoning space through token-guided tree traversal, achieving self-improvement for functional reasoning.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/RFTT.pdf}
    \vspace{-18pt}
    \caption{An illustrative diagram of the online reinforcement learning phase in RFTT. The LLM policy directly samples functional tokens from its vocabulary to autonomously expand reasoning trees to search for the final solutions. Then we use online reinforcement learning with process rewards to optimize the functional reasoning capabilities of the LLM policy.
    }
    \label{fig:rftt}
    \vspace{-8pt}
\end{figure*}