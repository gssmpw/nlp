@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@inproceedings{hao2023reasoninglanguagemodelplanning,
  author       = {Shibo Hao and
                  Yi Gu and
                  Haodi Ma and
                  Joshua Jiahua Hong and
                  Zhen Wang and
                  Daisy Zhe Wang and
                  Zhiting Hu},
  title        = {Reasoning with Language Model is Planning with World Model},
  booktitle    = {Conference on Empirical Methods in Natural Language Processing},
  year         = {2023},
}

@article{koh2024tree,
  title={Tree search for language model agents},
  author={Koh, Jing Yu and McAleer, Stephen and Fried, Daniel and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2407.01476},
  year={2024}
}

@inproceedings{kojima2022large,
  author       = {Takeshi Kojima and
                  Shixiang Shane Gu and
                  Machel Reid and
                  Yutaka Matsuo and
                  Yusuke Iwasawa},
  title        = {Large Language Models are Zero-Shot Reasoners},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@article{lai2024step,
  title={Step-dpo: Step-wise preference optimization for long-chain reasoning of llms},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya},
  journal={arXiv preprint arXiv:2406.18629},
  year={2024}
}

@article{lambert2024t,
  title={T$\backslash$" ulu 3: Pushing frontiers in open language model post-training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}

@inproceedings{lewkowycz2022solving,
  author       = {Aitor Lewkowycz and
                  Anders Andreassen and
                  David Dohan and
                  Ethan Dyer and
                  Henryk Michalewski and
                  Vinay V. Ramasesh and
                  Ambrose Slone and
                  Cem Anil and
                  Imanol Schlag and
                  Theo Gutman{-}Solo and
                  Yuhuai Wu and
                  Behnam Neyshabur and
                  Guy Gur{-}Ari and
                  Vedant Misra},
  title        = {Solving Quantitative Reasoning Problems with Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@article{min2024imitateexploreselfimprovereproduction,
  title={Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems},
  author={Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others},
  journal={arXiv preprint arXiv:2412.09413},
  year={2024}
}

@article{qi2024mutualreasoningmakessmaller,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@inproceedings{rafailov2024direct,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Christopher D. Manning and
                  Stefano Ermon and
                  Chelsea Finn},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2023},
}

@inproceedings{reft2024,
  author       = {Luong Quoc Trung and
                  Xinbo Zhang and
                  Zhanming Jie and
                  Peng Sun and
                  Xiaoran Jin and
                  Hang Li},
  title        = {ReFT: Reasoning with Reinforced Fine-Tuning},
  booktitle    = {Annual Meeting of the Association for Computational Linguistics},
  year         = {2024},
}

@inproceedings{sel2023algorithm,
  author       = {Bilgehan Sel and
                  Ahmad Al{-}Tawaha and
                  Vanshaj Khattar and
                  Ruoxi Jia and
                  Ming Jin},
  title        = {Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},
  booktitle    = {International Conference on Machine Learning},
  year         = {2024},
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@inproceedings{tian2024toward,
  author       = {Ye Tian and
                  Baolin Peng and
                  Linfeng Song and
                  Lifeng Jin and
                  Dian Yu and
                  Lei Han and
                  Haitao Mi and
                  Dong Yu},
  title        = {Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2024},
}

@inproceedings{wang2022self,
  author       = {Xuezhi Wang and
                  Jason Wei and
                  Dale Schuurmans and
                  Quoc V. Le and
                  Ed H. Chi and
                  Sharan Narang and
                  Aakanksha Chowdhery and
                  Denny Zhou},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{wang2023mathcoder,
  author       = {Ke Wang and
                  Houxing Ren and
                  Aojun Zhou and
                  Zimu Lu and
                  Sichun Luo and
                  Weikang Shi and
                  Renrui Zhang and
                  Linqi Song and
                  Mingjie Zhan and
                  Hongsheng Li},
  title        = {MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical
                  Reasoning},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024},
}

@inproceedings{wang2023plan,
  author       = {Lei Wang and
                  Wanyu Xu and
                  Yihuai Lan and
                  Zhiqiang Hu and
                  Yunshi Lan and
                  Roy Ka{-}Wei Lee and
                  Ee{-}Peng Lim},
  title        = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
  booktitle    = {Annual Meeting of the Association for Computational Linguistics},
  year         = {2023},
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

@inproceedings{wei2022chain,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@inproceedings{yao2022react,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{yao2023tot,
  author       = {Shunyu Yao and
                  Dian Yu and
                  Jeffrey Zhao and
                  Izhak Shafran and
                  Tom Griffiths and
                  Yuan Cao and
                  Karthik Narasimhan},
  title        = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2023},
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@inproceedings{zelikman2022star,
  author       = {Eric Zelikman and
                  Yuhuai Wu and
                  Jesse Mu and
                  Noah D. Goodman},
  title        = {STaR: Bootstrapping Reasoning With Reasoning},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@inproceedings{zhang2022automatic,
  author       = {Zhuosheng Zhang and
                  Aston Zhang and
                  Mu Li and
                  Alex Smola},
  title        = {Automatic Chain of Thought Prompting in Large Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@article{zhang2024accessing,
  title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
  author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.07394},
  year={2024}
}

@misc{zhang2024llamaberrypairwiseoptimizationo1like,
      title={LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning}, 
      author={Di Zhang and Jianbo Wu and Jingdi Lei and Tong Che and Jiatong Li and Tong Xie and Xiaoshui Huang and Shufei Zhang and Marco Pavone and Yuqiang Li and Wanli Ouyang and Dongzhan Zhou},
      year={2024},
      eprint={2410.02884},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{zhang2024rest,
  author       = {Dan Zhang and
                  Sining Zhoubian and
                  Ziniu Hu and
                  Yisong Yue and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {ReST-MCTS*: {LLM} Self-Training via Process Reward Guided Tree Search},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2024},
}

@inproceedings{zhou2022least,
  author       = {Denny Zhou and
                  Nathanael Sch{\"{a}}rli and
                  Le Hou and
                  Jason Wei and
                  Nathan Scales and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Claire Cui and
                  Olivier Bousquet and
                  Quoc V. Le and
                  Ed H. Chi},
  title        = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

