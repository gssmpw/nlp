% begin my citation
@misc{zeng2024scalingsearchlearningroadmap,
      title={Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective}, 
      author={Zhiyuan Zeng and Qinyuan Cheng and Zhangyue Yin and Bo Wang and Shimin Li and Yunhua Zhou and Qipeng Guo and Xuanjing Huang and Xipeng Qiu},
      year={2024},
      eprint={2412.14135},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.14135}, 
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}



@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anthropic2024claude,
  title={Claude 3.5 sonnet model card addendum},
  author={Anthropic, AI},
  journal={Claude-3.5 Model Card},
  volume={3},
  number={6},
  year={2024}
}

@misc{feng2024faragillmsneed,
      title={How Far Are We From AGI: Are LLMs All We Need?}, 
      author={Tao Feng and Chuanyang Jin and Jingyu Liu and Kunlun Zhu and Haoqin Tu and Zirui Cheng and Guanyu Lin and Jiaxuan You},
      year={2024},
      eprint={2405.10313},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.10313}, 
}

@misc{kim20242reasoninggeneralityadaptation,
      title={System 2 Reasoning via Generality and Adaptation}, 
      author={Sejin Kim and Sundong Kim},
      year={2024},
      eprint={2410.07866},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.07866}, 
}

@inproceedings{weng2023large,
	title={Large Language Models are Better Reasoners with Self-Verification}, 
	author={Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
	year={2023},
	eprint={2212.09561},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@misc{yang2024supercorrectsupervisingcorrectinglanguage,
      title={SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights}, 
      author={Ling Yang and Zhaochen Yu and Tianjun Zhang and Minkai Xu and Joseph E. Gonzalez and Bin Cui and Shuicheng Yan},
      year={2024},
      eprint={2410.09008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09008}, 
}

@article{qi2024mutualreasoningmakessmaller,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  year={2024}
}

@inproceedings{wei2022chain,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@inproceedings{rafailov2024direct,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Christopher D. Manning and
                  Stefano Ermon and
                  Chelsea Finn},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2023},
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{rso2024liu,
  author       = {Tianqi Liu and
                  Yao Zhao and
                  Rishabh Joshi and
                  Misha Khalman and
                  Mohammad Saleh and
                  Peter J. Liu and
                  Jialu Liu},
  title        = {Statistical Rejection Sampling Improves Preference Optimization},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024},
}

@inproceedings{reft2024,
  author       = {Luong Quoc Trung and
                  Xinbo Zhang and
                  Zhanming Jie and
                  Peng Sun and
                  Xiaoran Jin and
                  Hang Li},
  title        = {ReFT: Reasoning with Reinforced Fine-Tuning},
  booktitle    = {Annual Meeting of the Association for Computational Linguistics},
  year         = {2024},
}

@inproceedings{zhang2024rest,
  author       = {Dan Zhang and
                  Sining Zhoubian and
                  Ziniu Hu and
                  Yisong Yue and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {ReST-MCTS*: {LLM} Self-Training via Process Reward Guided Tree Search},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2024},
}

@article{feng2023alphazero,
  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
  author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:2309.17179},
  year={2023}
}

@article{huang2024o1,
  title={O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?},
  author={Huang, Zhen and Zou, Haoyang and Li, Xuefeng and Liu, Yixiu and Zheng, Yuxiang and Chern, Ethan and Xia, Shijie and Qin, Yiwei and Yuan, Weizhe and Liu, Pengfei},
  journal={arXiv preprint arXiv:2411.16489},
  year={2024}
}

@article{min2024imitateexploreselfimprovereproduction,
  title={Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems},
  author={Min, Yingqian and Chen, Zhipeng and Jiang, Jinhao and Chen, Jie and Deng, Jia and Hu, Yiwen and Tang, Yiru and Wang, Jiapeng and Cheng, Xiaoxue and Song, Huatong and others},
  journal={arXiv preprint arXiv:2412.09413},
  year={2024}
}

@inproceedings{tyen2024llms,
  title={LLMs cannot find reasoning errors, but can correct them given the error location},
  author={Tyen, Gladys and Mansoor, Hassan and C{\u{a}}rbune, Victor and Chen, Yuanzhu Peter and Mak, Tony},
  booktitle={Findings of the Association for Computational Linguistics},
  year={2024}
}

@misc{xia2024lessselectinginfluentialdata,
      title={LESS: Selecting Influential Data for Targeted Instruction Tuning}, 
      author={Mengzhou Xia and Sadhika Malladi and Suchin Gururangan and Sanjeev Arora and Danqi Chen},
      year={2024},
      eprint={2402.04333},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04333}, 
}

@misc{zhou2023limaalignment,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11206}, 
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017},
  url={https://arxiv.org/abs/1707.06347}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@misc{zhang2024llamaberrypairwiseoptimizationo1like,
      title={LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning}, 
      author={Di Zhang and Jianbo Wu and Jingdi Lei and Tong Che and Jiatong Li and Tong Xie and Xiaoshui Huang and Shufei Zhang and Marco Pavone and Yuqiang Li and Wanli Ouyang and Dongzhan Zhou},
      year={2024},
      eprint={2410.02884},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{yao2023tot,
  author       = {Shunyu Yao and
                  Dian Yu and
                  Jeffrey Zhao and
                  Izhak Shafran and
                  Tom Griffiths and
                  Yuan Cao and
                  Karthik Narasimhan},
  title        = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2023},
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2024}
}

@inproceedings{hao2023reasoninglanguagemodelplanning,
  author       = {Shibo Hao and
                  Yi Gu and
                  Haodi Ma and
                  Joshua Jiahua Hong and
                  Zhen Wang and
                  Daisy Zhe Wang and
                  Zhiting Hu},
  title        = {Reasoning with Language Model is Planning with World Model},
  booktitle    = {Conference on Empirical Methods in Natural Language Processing},
  year         = {2023},
}

@inproceedings{
chen2024alphamath,
title={AlphaMath Almost Zero: Process Supervision without Process},
author={Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan},
booktitle={Conference on Neural Information Processing Systems},
year={2024},
}

@techreport{ethayarajh2023halos,
  author = {Ethayarajh, Kawin and Xu, Winnie and Jurafsky, Dan and Kiela, Douwe},
  title = {Human-Centered Loss Functions (HALOs)},
  institution = {Contextual AI},
  year = {2023}
}

@misc{song2024preferencerankingoptimizationhuman,
      title={Preference Ranking Optimization for Human Alignment}, 
      author={Feifan Song and Bowen Yu and Minghao Li and Haiyang Yu and Fei Huang and Yongbin Li and Houfeng Wang},
      year={2024},
      eprint={2306.17492},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Proceedings of NeurIPS},
  year={2022}
}

@article{wu2024self,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}

@article{zhang2024accessing,
  title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
  author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.07394},
  year={2024}
}

@inproceedings{kojima2022large,
  author       = {Takeshi Kojima and
                  Shixiang Shane Gu and
                  Machel Reid and
                  Yutaka Matsuo and
                  Yusuke Iwasawa},
  title        = {Large Language Models are Zero-Shot Reasoners},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@inproceedings{wang2022self,
  author       = {Xuezhi Wang and
                  Jason Wei and
                  Dale Schuurmans and
                  Quoc V. Le and
                  Ed H. Chi and
                  Sharan Narang and
                  Aakanksha Chowdhery and
                  Denny Zhou},
  title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{zhang2022automatic,
  author       = {Zhuosheng Zhang and
                  Aston Zhang and
                  Mu Li and
                  Alex Smola},
  title        = {Automatic Chain of Thought Prompting in Large Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{yao2022react,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{zhou2022least,
  author       = {Denny Zhou and
                  Nathanael Sch{\"{a}}rli and
                  Le Hou and
                  Jason Wei and
                  Nathan Scales and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Claire Cui and
                  Olivier Bousquet and
                  Quoc V. Le and
                  Ed H. Chi},
  title        = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  booktitle    = {International Conference on Learning Representations},
  year         = {2023},
}

@inproceedings{sel2023algorithm,
  author       = {Bilgehan Sel and
                  Ahmad Al{-}Tawaha and
                  Vanshaj Khattar and
                  Ruoxi Jia and
                  Ming Jin},
  title        = {Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},
  booktitle    = {International Conference on Machine Learning},
  year         = {2024},
}

@inproceedings{wang2023plan,
  author       = {Lei Wang and
                  Wanyu Xu and
                  Yihuai Lan and
                  Zhiqiang Hu and
                  Yunshi Lan and
                  Roy Ka{-}Wei Lee and
                  Ee{-}Peng Lim},
  title        = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
  booktitle    = {Annual Meeting of the Association for Computational Linguistics},
  year         = {2023},
}

@inproceedings{lewkowycz2022solving,
  author       = {Aitor Lewkowycz and
                  Anders Andreassen and
                  David Dohan and
                  Ethan Dyer and
                  Henryk Michalewski and
                  Vinay V. Ramasesh and
                  Ambrose Slone and
                  Cem Anil and
                  Imanol Schlag and
                  Theo Gutman{-}Solo and
                  Yuhuai Wu and
                  Behnam Neyshabur and
                  Guy Gur{-}Ari and
                  Vedant Misra},
  title        = {Solving Quantitative Reasoning Problems with Language Models},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{toshniwal2024openmathinstruct,
  title={Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data},
  author={Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Kisacanin, Branislav and Ayrapetyan, Alexan and Gitman, Igor},
  journal={arXiv preprint arXiv:2410.01560},
  year={2024}
}

@article{patil2025advancingreasoninglargelanguage,
      title={Advancing Reasoning in Large Language Models: Promising Methods and Approaches}, 
      author={Avinash Patil},
      year={2025},
      journal={arXiv preprint arXiv:2410.01560},
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@inproceedings{wang2023mathcoder,
  author       = {Ke Wang and
                  Houxing Ren and
                  Aojun Zhou and
                  Zimu Lu and
                  Sichun Luo and
                  Weikang Shi and
                  Renrui Zhang and
                  Linqi Song and
                  Mingjie Zhan and
                  Hongsheng Li},
  title        = {MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical
                  Reasoning},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024},
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{yang2024qwen25,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{koh2024tree,
  title={Tree search for language model agents},
  author={Koh, Jing Yu and McAleer, Stephen and Fried, Daniel and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2407.01476},
  year={2024}
}

@inproceedings{hendrycks2021measuring,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2021},
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{patel2021nlp,
  author       = {Arkil Patel and
                  Satwik Bhattamishra and
                  Navin Goyal},
  title        = {Are {NLP} Models really able to Solve Simple Math Word Problems?},
  booktitle    = {North American Chapter of the Association for Computational Linguistics},
  pages        = {2080--2094},
  year         = {2021},
}

@inproceedings{he2024olympiadbench,
  author       = {Chaoqun He and
                  Renjie Luo and
                  Yuzhuo Bai and
                  Shengding Hu and
                  Zhen Leng Thai and
                  Junhao Shen and
                  Jinyi Hu and
                  Xu Han and
                  Yujie Huang and
                  Yuxiang Zhang and
                  Jie Liu and
                  Lei Qi and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {OlympiadBench: {A} Challenging Benchmark for Promoting {AGI} with
                  Olympiad-Level Bilingual Multimodal Scientific Problems},
  booktitle    = {Annual Meeting of the Association for Computational Linguistics},
  year         = {2024},
}

@inproceedings{lightman2023let,
  author       = {Hunter Lightman and
                  Vineet Kosaraju and
                  Yuri Burda and
                  Harrison Edwards and
                  Bowen Baker and
                  Teddy Lee and
                  Jan Leike and
                  John Schulman and
                  Ilya Sutskever and
                  Karl Cobbe},
  title        = {Let's Verify Step by Step},
  booktitle    = {International Conference on Learning Representations},
  year         = {2024},
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{zhuo2024bigcodebench,
  title={Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={arXiv preprint arXiv:2406.15877},
  year={2024}
}

@article{guan2025rstar,
  title={rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},
  author={Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2501.04519},
  year={2025}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{li2023coannotating,
  author       = {Minzhi Li and
                  Taiwei Shi and
                  Caleb Ziems and
                  Min{-}Yen Kan and
                  Nancy F. Chen and
                  Zhengyuan Liu and
                  Diyi Yang},
  title        = {CoAnnotating: Uncertainty-Guided Work Allocation between Human and
                  Large Language Models for Data Annotation},
  booktitle    = {Conference on Empirical Methods in Natural Language Processing},
  year         = {2023},
}

@inproceedings{ahn2024large,
  author       = {Janice Ahn and
                  Rishu Verma and
                  Renze Lou and
                  Di Liu and
                  Rui Zhang and
                  Wenpeng Yin},
  title        = {Large Language Models for Mathematical Reasoning: Progresses and Challenges},
  booktitle    = {Conference of the European Chapter of the Association for Computational Linguistics},
  year         = {2024},
}

@article{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  journal={Farrar, Straus and Giroux},
  year={2011}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={ACM Symposium on Operating Systems Principles},
  year={2023}
}

@article{zheng2024llamafactory,
  title={Llamafactory: Unified efficient fine-tuning of 100+ language models},
  author={Zheng, Yaowei and Zhang, Richong and Zhang, Junhao and Ye, Yanhan and Luo, Zheyan and Feng, Zhangchi and Ma, Yongqiang},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024}
}

@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Hu, Jian and Wu, Xibin and Wang, Weixun and Zhang, Dehao and Cao, Yu and others},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}

@inproceedings{aminabadi2022deepspeed,
  title={Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE}
}

@article{li2023quantity,
  title={From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
  author={Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  journal={arXiv preprint arXiv:2308.12032},
  year={2023}
}

@inproceedings{tian2024toward,
  author       = {Ye Tian and
                  Baolin Peng and
                  Linfeng Song and
                  Lifeng Jin and
                  Dian Yu and
                  Lei Han and
                  Haitao Mi and
                  Dong Yu},
  title        = {Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2024},
}

@article{lai2024step,
  title={Step-dpo: Step-wise preference optimization for long-chain reasoning of llms},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya},
  journal={arXiv preprint arXiv:2406.18629},
  year={2024}
}

@inproceedings{zelikman2022star,
  author       = {Eric Zelikman and
                  Yuhuai Wu and
                  Jesse Mu and
                  Noah D. Goodman},
  title        = {STaR: Bootstrapping Reasoning With Reasoning},
  booktitle    = {Conference on Neural Information Processing Systems},
  year         = {2022},
}

@article{lambert2024t,
  title={T$\backslash$" ulu 3: Pushing frontiers in open language model post-training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}

@article{xu2025towards,
  title={Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models},
  author={Xu, Fengli and Hao, Qianyue and Zong, Zefang and Wang, Jingwei and Zhang, Yunke and Wang, Jingyi and Lan, Xiaochong and Gong, Jiahui and Ouyang, Tianjian and Meng, Fanjin and others},
  journal={arXiv preprint arXiv:2501.09686},
  year={2025}
}

@article{hu2025reinforce++,
  title={REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}

@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
}