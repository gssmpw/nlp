\section{Related Works}
\textbf{Mathematical Reasoning.}
Recent advancements in LLMs for mathematical reasoning have primarily focused on two broad categories of approaches: prompt engineering and learning to reason. Prompt engineering techniques design specific prompting strategies, such as CoT____, multi-path reasoning____ or divide-and-conquer____ to guide the model to generate step-by-step reasoning processes during the inference phase. In contrast, learn-to-reason approaches train the model explicitly to improve its intrinsic reasoning ability. Extensive math and code corpora are first utilized to build the fundamental reasoning capability in the pre-training phase____ and various techniques, such as supervised fine-tuning____, direct preference optimization____ and reinforcement learning____ are used to further enhance the mathematical reasoning ability during post-training.


\textbf{Tree-Search Reasoning.}
While most CoT methods have typically adopted an auto-regressive reasoning manner, there has been a trend to engage in more complicated reasoning architectures like trees____. Recently, various methods for exploring tree structures have been devised to identify optimal reasoning trajectories, \textit{e.g.},  tree of thought____, graph of thoughts____ and Monte Carlo tree search____. These direct tree-search methods struggle with limited exploration due to the homogeneous reasoning paths and the large search space. Although rStar____ utilizes functional prompts in tree search to diversify and constrain the search process, they only act as external constraints in the inference phase while we introduce learnable functional tokens during model training to facilitate effective and efficient exploration, thus improving the model's intrinsic reasoning ability.

\textbf{LLM Self-Improvement.}
Self-improvement with self-generated reasoning paths has gained increasing interest in mathematical reasoning due to the lack of golden rationales. Early methods____ utilize reject sampling to select reasoning paths with correct answers for iterative self-training, which often leads to local optimal due to the homogeneity of self-generated reasoning paths. Recent approaches explore MCTS____ to generate more diverse reasoning paths offline or adopt random sampling to produce abundant reasoning paths during the online reinforcement learning phase____. However, these methods struggle to fully explore the reasoning space, especially for smaller language models. In RFTT, we employ function tokens for automatic exploration of the reasoning space through token-guided tree traversal, achieving self-improvement for functional reasoning.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/RFTT.pdf}
    \vspace{-18pt}
    \caption{An illustrative diagram of the online reinforcement learning phase in RFTT. The LLM policy directly samples functional tokens from its vocabulary to autonomously expand reasoning trees to search for the final solutions. Then we use online reinforcement learning with process rewards to optimize the functional reasoning capabilities of the LLM policy.
    }
    \label{fig:rftt}
    \vspace{-8pt}
\end{figure*}