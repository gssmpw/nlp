\section{Model Setup}
\subsection{Characterizing Patients and Providers}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\subsection{Uniform Choice Model}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat.

\subsection{Assortment Optimization}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider an instance of the assortment optimization problem with ordering $\pi_{1} \cdots \pi{N}$, so that $\pi_{i}$ indicates the i$^{\mathrm{th}}$ patient that is seen. 
    Let $\mathbf{Z}_{i}$ denotes the available providers for patient $\pi_{i}$, so that $\mathbf{Z}_{\mathbf{M},i} = f(\mathbf{Y}_{\pi_{i-1}},\mathbf{Y}_{\pi_{i-2}},\cdots,\mathbf{Y}_{\pi_{1}},\mathbf{M}_{i})$. 
    Here, $\mathbf{Y}_{\pi_{i}}$ denotes the random variable (implicitly dependent on $p$) representing the provider chosen by the ith patient, written as a 0-1 vector over providers, and $M_{\pi_i}$ is a 0-1 vector which encodes the menu for patient $\pi_{i}$. 
    Suppose the budget, $B \geq P$. Then let the greedy menu be $\mathbf{\hat{M}}_{\pi_i} = \mathbf{1}$, where $\mathbf{1}$ is the vector of all 1s. 
    Let $\mathrm{ALG} =   
        \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{M}},i})]$ and let     
    \begin{equation}
        \mathrm{OPT} = \max_{M_{1} \cdots M_{N}} \mathbb{E}_{\pi}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]
    \end{equation}
    Then, for any $p$ and $\epsilon$, there exists some $\theta$, so that $\mathrm{ALG} \leq \epsilon \mathrm{OPT}$
  
\end{lemma}

\begin{proof}
\textbf{Proof:}
    We construct a problem instance where the greedy algorithm performs an $\epsilon$ fraction of the optimal algorithm; that is $\mathrm{ALG} \leq \epsilon \mathrm{OPT}$. 
    To do so, we consider an instance of the assortment optimization problem with $N=P$ patients and providers. 
    Let $\theta_{1,1} = 1$, while $\theta_{i,1} = 2 \delta$ for $i \neq 1$. 
    Additionally, let $\theta_{i,j} = \delta$ for $j \neq 1$. 

    In this scenario, when $\delta \leq \frac{1}{2}$, the optimal selection is to let $\mathbf{m}_{i} = \mathbf{e}_{i}$, so that each patient gets a menu of size one, with only provider $i$ being available. 
    This results in an expected total utility of $\mathrm{OPT} = p (\delta (N-1) + 1)$. 

    Next, consider the greedy algorithm, where $\mathbf{m}_{i} = \mathbf{1}$. 
    Note that all patients prefer provider $j=1$. 
    By symmetry, each patient has an equal chance at receiving provider $j=1$. 
    Therefore, with probability at most $\frac{1}{N}$, patient $i$ is first (that is $\pi_{1} = i$), and $i=1$ is assigned to provider $j=1$, resulting in a utility of $1$, while with probability at most $1-\frac{1}{N}$, we receive a utility of $2 \delta$. 
    For all the $N-1$ other providers, we receive a utility of $\delta$, with a probability $p$ of accepting each.  
    Combining gives that our expected total utility is $\frac{1}{N} + \frac{2\delta(N-1)}{N} + p(N-1) \delta$. 

    Therefore, we get the following: 
    \begin{equation}
        \frac{\mathrm{ALG}}{\mathrm{OPT}} = \frac{\frac{1}{N} + \frac{2\delta(N-1)}{N} + p(N-1) \delta}{p (\delta (N-1) + 1)} \leq \frac{\frac{1}{N} + 2\delta + p(N-1) \delta}{p(\delta (N-1) + 1)} \leq \frac{\frac{1}{N} + 2\delta + p(N-1) \delta}{p} = \frac{1}{Np} + 2\frac{\delta}{p} + N \delta 
    \end{equation}
    We let $\frac{1}{Np} \leq \frac{\epsilon}{3}$, so we let $N =\frac{3}{\epsilon p}$. 
    Additionally, we let $N \delta \leq \frac{\epsilon}{3}$, so $\delta \leq \frac{\epsilon}{3N}$, and $2\frac{\delta}{p} \leq \frac{\epsilon}{3} = \delta \leq \frac{2p \epsilon}{3}$. 
    Letting $\delta = \min(\frac{2p \epsilon}{3},\frac{\epsilon}{3N})$, shows that 
    \begin{equation}
        \frac{\mathrm{ALG}}{\mathrm{OPT}} \leq \frac{1}{Np} + 2\frac{\delta}{p} + N \delta \leq \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} = \epsilon   
    \end{equation}
    Therefore, for any choice of $\epsilon$ and $p$, there exists a choice of $N$ and $\theta$ (implicitly chosen through $\delta$), so that $\mathrm{ALG} \leq \epsilon \mathrm{OPT}$. 
\end{proof}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. 


\subsection{Optimization under Instance Response}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider an instance of the assortment planning problem, with some fixed $\theta$, $p$, and $\pi$. 
    Let the offline set of menus, $M_{1}, M_{2}, \cdots M_{N}$ be  
    \begin{align}
        & M_{1}, M_{2}, \cdots, M_{N} \\
        & = \argmax_{M_{1} \cdots M_{N}} \mathbb{E}_{Y_{1} \cdots Y_{N}}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]
    \end{align}
    Let the online instance of menus be $\bar{M}_{1}, \bar{M}_{2}, \cdots \bar{M}_{N}$, and let this be
    \begin{align}
        & \bar{M}_{i} =  \\ 
        & \argmax_{\bar{M}_{i}} \max_{\bar{M}_{i+1}, \cdots, M_{N}}  \mathbb{E}_{Y_{i} \cdots Y_{N}}[\sum_{i'=i}^{N} \sum_{j=1}^{P} \theta_{\pi_{i'},j} Y_{\pi_{i'},j}(\mathbf{Z}_{i'})]  
    \end{align}
    Then 
    \begin{align}
        & \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{M}},i})] \\ 
        & \leq \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{\bar{M}}},i})] \\
    \end{align}
\end{lemma}
\begin{proof}
    \textbf{Proof:} We first note that $\bar{M}_{\pi_{1}]} = \hat{M}_{\pi_{1}}$, as both have no information on chosen providers from previous patients (due to the lack of previous patients). 
    To demonstrate the inequality, we first show that using $\bar{M}_{\pi_{2}}$ instead of $\hat{M}_{\pi_{2}}$ can only increase reward. 
    We repeat this process up to $\pi_{N}$ to show that using the online instances of menus can only improve reward through a recursive argument, showing that using online menus improves overall reward. 

    After patient $\pi_{1}$ selects their provider, we aim to select a set of menus, $M_{\pi_{2}} \cdots M_{\pi_{N}}$ to maximize 
    \begin{equation}
        \max_{\bar{M}_{\pi_{2}}, M_{\pi_{3}} \cdots, M_{\pi_{N}}}  \mathbb{E}_{Y_{2} \cdots Y_{N}}[\sum_{i'=2}^{N} \sum_{j=1}^{P} \theta_{\pi_{i'},j} Y_{\pi_{i'},j}(\mathbf{Z}_{i'})]  
    \end{equation}

    By the definition of the maximum, we note that the assortment $\bar{M}_{\pi_{2}}, M_{\pi_{3}}, \cdots M_{\pi_{N}}$ results in a higher objective value than the assortment $\hat{M}_{\pi_{2}} \cdots \hat{M}_{\pi_{N}}$. 

    Next, we note that for $i=3$, through a similar argument, that $\bar{M}_{\pi_{3}}, M_{\pi_{4}}, \cdots M_{\pi_{N}}$ results in a higher objective value than that of $\hat{M}_{\pi_{3}} \cdots \hat{M}_{\pi_{N}}$. 
    We note that our selection of menus only improves the objectives at each timestep; if we let $\bar{M}_{\pi_{2}}, M_{\pi_{3}}, \cdots M_{\pi_{N}}$ be the set of menus computed at timestep $i=2$, then such menus are still an option at timestep $i=3$. 
    However, by definition of the maximum, such menus are either equivalent to $\bar{M}_{\pi_{3}}, M_{\pi_{4}}, \cdots M_{\pi_{N}}$. 
    Therefore, at each timestep, our choice of $\bar{M}_{i}$ only improves the expected utility when compared with the default, $\hat{M}_{i}$, and so 
    \begin{align}
        & \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{M}},i})] \\ 
        & \leq \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{\bar{M}}},i})] \\
    \end{align}
    \nrcomment{Make this more rigorous}
\end{proof}

\begin{lemma}
    Consider a problem setting with some $\theta$ and let $\pi$ be a random ordering. 
    Let $M_{i}$ be a menu that is computed as $M_{i} = f(\mathbf{Y}_{i-1},\ldots,\mathbf{Y}_{1})$. Let $M_{i,j} = 1$ where $j = \argmax_{j |  \mathbf{Z}_{\mathbf{1},i,j} = 1} \theta_{i,j}$; in the event of ties, $j$ is chosen arbitrarily. Note here this corresponds to the greedy choice in an online response setting. Let $\mathrm{OPT}$ correspond to the underlying linear-program corresponding to this problem; that is, $\mathrm{OPT}$ is the optimal value for
    \begin{equation}
        \mathrm{OPT} = \max_{x} \sum_{i,j} \theta_{i,j} x_{i,j}
    \end{equation}
    Finally, let $\mathrm{ALG} =  \mathbb{E}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{\hat{\mathbf{M}},i})]$. 
    Then $\mathrm{ALG} \geq \frac{OPT}{2}$
 
\end{lemma}

\begin{proof}
    \textbf{Proof:} We reduce this problem to the online bipartite matching problem, and relate this to prior work which demonstrates that we can achieve a 2-approximation to this problem. 

    First, assume that the randomization is chosen independent of the algorithm. 
    That is, let $x_{i}$ be a random variable for patient $i$ that is $1$ with probability $p$, and $0$ with probability $1-p$. 
    $x_{i}$ corresponds to whether patient $i$ will successfully match with a provider, assuming their menu is non-empty. 

    Next, note that when $x_{i}=0$, the choice of menu does not impact the expected objective value, as patients match with no providers in this situation. 
    Equivalently, we can view $x_{i}=0$ as corresponding to situations where patients match with providers who provide $0$ utility. 

    To translate this to an online bipartite matching problem, consider a modified instance of the problem with $N$ patients and $P+N$ providers. 
    Here $\bar{\theta}_{i,j} = \theta_{i,j}$ when $j \leq P$ and $\bar{\theta}_{i,j} = 0$ otherwise. 
    Additionally, let $\bar{\theta}_{i,j} = -1$ if $x_{i} = 0$ and $j \leq P$. 

    In the greedy selection for the online response setting, $B=1$, as available providers are known, and so we can pre-compute which provider will be selected (if any is selected at all). 

    Then the online assortment problem corresponds to the online bipartite matching problem on the graph with edges $\bar{\theta}_{i,j}$. 
    Here, we note that if $x_{i}=0$, then patients will select a provider $j>P$, as all $j \leq P$ provide negative value. 
    \nrcomment{Add a bit more here}

    We then note that online edge-weighted bipartite matching can be 2-approximated from prior work (INSERT PRIOR WORK). 
    Using this same algorithm, we demonstrate that our online response is a 2-approximation against the optimal, which is an LP solution with knowledge of which patients will actually match. 
\end{proof}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. 

\subsection{Optimization under Known Ordering}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{theorem}
\nrcomment{Add some assumption here}
    Let $\pi$ be some ordering of patients known to the algorithm, and let $\theta$ be the matrix of patient-provider match qualities. 
    Let $N=P$ so that the number of providers and patients are equal. 
    Let $v_{i}$ be the solutions to the linear program that maximize the 1-1 bipartite matching problem with edge weights $\theta_{i,j}$. 
    Let $v^{i} = j$ so that $v^{-1}_{j} = i$. 
    Let $G_{i,j} = \max(0,\theta_{i,j}-\theta_{i,v_{i}})$. 
    \nrcomment{This is the ith person in order, not hte ith patient}
    Let $M_{i,j} = \mathbbm{1}[G_{i,j} \geq \log(\frac{1}{1-p}) \sum_{r=i+1}^{N} (M_{r,j} (1-p)^{\sum_{r'=i}^{r-1} M_{r',j}})]*\mathbbm{1}[v^{-1}_{j} \leq i]$ and let $M_{i,v_{i}} = 1$. 
    If cross-provider interference is ignored \nrcomment{Formalize this}, then $M_{1} \cdots M_{N}$ is the optimal algorithm under the one-shot response model. 
\end{theorem}

\begin{proof}
    \textbf{Proof:} We first discuss the structure of the menus offered for any optimal algorithm, then discuss how we can parlay this into an upper bound. 
    First, we note that, when the order is known, each patient must be offered at most one provider which is not seen by the previous patients. 
    Because the ordering is known, when offering two new providers, one provider must have higher utility than the other, and so the less preferred provider will never be selected. 
    Therefore, when constructing the optimal menu, two questions must be answered: what new provider (if any) to offer each patient, and which of the previously seen providers should be visible by a patient. 
    
    \nrcomment{Somehow justify the use of the Linear Program here; talk to Holly about this}

    Let the linear program matches be denoted $v_{\pi_{i}}$, so that $v_{\pi_{i}}$ is the provider allocated to patient $\pi_{i}$, and $v^{-1}_{j}$ is the patient corresponding to provider j. 
    For provider $j$, let $x_{i,j}$ denote whether provider $j$ is shown to patient $i$. 
    The maximum possible reward corresponding from provider $j$ is then the reward corresponding to each patient, combined with the probability that provider $j$ is available. 
    Such a statement is an upper bound because it does not account for situations where provider $j'$ is selected by provider $j$ for some future patient. 
    The maximum possible reward for provider $j$ is then 
    \begin{equation}
        \sum_{i=v^{-1}_{j}}^{N} x_{i,j} (1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i',j}} p \theta_{i,j}
    \end{equation}

    We normalize this by considering the presence of the default option, $v_{\pi_{i}}$ for each patient $i$. 
    The maximum improvement is then written as a function of $G_{i} = \max(\theta_{i,j} - \theta_{i,v_{i}},0)
    Therefore, the maximum possible reward for provider $j$ is
    \begin{equation}
        p (\theta_{v^{-1}_{j},j} + \sum_{i=v^{-1}_{j}}^{N} x_{i,j} (1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i',j}} \G_{i})
    \end{equation}

    The challenge is to determine the selection of $x_{i,j}$ to maximize this sum. 
    We compute such an $x_{i,j}$ through KKT conditions. 
    \nrcomment{Look into whether this is convex; talk to Holly about this}
    
    We optimize $x_{i,j}$ for each $j$ individually (due to independence across values of $j$), so we denote the variables as $x_{i}$. 
    We first note that $x_{i} \in \{0,1\}$, which we relax to $x_{i} \in [0,1]$. 
    The KKT conditions can then be stated as 
    \begin{equation}
        \grad_{x}(p (\theta_{v^{-1}_{j},j} + \sum_{i=v^{-1}_{j}}^{N} x_{i,j} (1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i',j}} \G_{i}) + \sum_{i=v^{-1}_{j}}^{N} u_{i} (-x_{i}) + v_{i}(1-x_{i})) = 0
    \end{equation}
    subject to
    \begin{equation}
        u_{i} x_{i} = 0, v_{i}(1-x_{i}) = 0, u_{i} \geq, v_{i} \geq 0
    \end{equation}
    For patient $i$ we then get  
    \begin{equation}
        p(-(1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i'}} - \sum_{i'=i+1}^{N} x_{i'} G_{i'} \log(1-p) (1-p)^{\sum_{i'=v^{-1}_{j}}^{i'-1}}) - u_{i} + v_{i} = 0
    \end{equation}
    We solve such an equation by noting the sign of the first term; if $-(1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i'}} - \sum_{i'=i+1}^{N} x_{i'} G_{i'} \log(1-p) (1-p)^{\sum_{i'=v^{-1}_{j}}^{i'-1}}$ is positive, then $u_{i}>0$, while the converse implies $v_{i} > 0$. 
    When such a term is 0, $u_{i}=v_{i}$. 
    Because $u_{i} x_{i} = 0$ and $v_{i}(1-x_{i}) = 0$, this implies $u_{i}=v_{i}=0$. 
    
    To compute the sign of the first term, we aim to find out if 
    \begin{equation}
        G_{i} (1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1} x_{i'}}  > \sum_{i'=i+1}^{N} x_{i'} G_{i'} \log(\frac{1}{1-p}) (1-p)^{\sum_{i'=v^{-1}_{j}}^{i'-1}}
    \end{equation}

    Dividing by $(1-p)^{\sum_{i'=v^{-1}_{j}}^{i-1}} > 0$ gives 
    \begin{equation}
        G_{i} > \sum_{i'=i+1}^{N} x_{i'} G_{i'} \log(\frac{1}{1-p}) (1-p)^{\sum_{i'=i+1}^{i'-1} x_{i'}}
    \end{equation}

    Such a sum only relies on terms greater than $i$; if these terms are known, then we can compute the sign, and therefore, determine $x_{i}$. 
    We note that if the first term is 0, then any choice of $x_{i}$ will satisfy the equation; we let $x_{i}=1$ arbitrarily. 
    \nrcomment{Check the KKT condition here carefully; talk with Holly}
    We additionally note that when $i=N$, that the right-side becomes $0$, and so $x_{N}=1$. 

    Therefore, we can recursively compute the optimal $x_{i}$ based on backsolving. 
    This leads to an upper bound on the maximum possible solution. 

\end{proof}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. 