\section{Designing Algorithms for Matching}
 \subsection{Analyzing P=1}
 \begin{figure*}
     \centering 
     \includegraphics[width=0.5 \textwidth]{example-image-a}
     \caption{Figure for $P=1$}
 \end{figure*}

 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
\begin{lemma}
    The optimal algorithm in this scenario is (when ordering is known)...
\end{lemma}
\begin{proof}
    \textbf{Proof:} When the ordering is known, we reduce the problem to a decision of whether to offer the single provider $x_{i}$. 
    Our objective is to select the $x_{i}$ that maximizes 

    \begin{equation}
        \sum_{i=1}^{N} x_{i} (1-p)^{\sum_{j=1}^{i-1} x_{j}} p \theta_{i}
    \end{equation}

    We use the same proof as before, and leverage KKT conditions, along with boundedness of $x_{i}$ to find that $x_{i}$. 
    Taking the derivative yields for element $i$ that 

    \begin{equation}
        \sum_{j=i+1}^{N} (x_{j} \log(1-p) (1-p)^{\sum_{k=1}^{j-1} x_{k}}) + p \theta_{i} (1-p)^{\sum_{j=1}^{i-1} x_{j}}  - u_{i} + v_{i} = 0 
    \end{equation}

    Dividing by $(1-p)^{\sum_{k=1}^{i-1}}$ and analyzing the sign of the first two terms yields: 

    \begin{equation}
        \sum_{j=i+1}^{N} (x_{j} \log(1-p) (1-p)^{\sum_{k=i}^{j-1} x_{k}}) + p \theta_{i} > 0
    \end{equation}

    We note that this can be computed based only using $x_{i+1} \cdots x_{N}$, and that $x_{N}=1$. 
    If the first term is positive, then $u_{i}>0$, which implies that $x_{i} = 0$; otherwise, $x_{i} = 1$. 
    Therefore, we can optimally compute the best policy in this situation by iteratively computing values of $x_{i}$ based on the sign above. 

\end{proof}

 Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
\begin{theorem}
    Let $\mathrm{OPT}$ be the optimal algorithm under known ordering: 
    \begin{equation}
        \mathrm{OPT} = \max_{M_{1} \cdots M_{N}} \mathbb{E}_{\mathbf{Y}}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]
    \end{equation}
    Let $\mathrm{ALG}$ be any algorithm with unknown ordering; that is
    \begin{equation}
        \mathrm{ALG} = \max_{M_{1} \cdots M_{N}} \mathbb{E}_{\pi} \mathbb{E}_{\mathbf{Y}}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]
    \end{equation}

    When $P=1$, $\mathrm{ALG} \leq \frac{p}{p+({1-p})^2} \mathrm{OPT}$. 
\end{theorem}
\begin{proof}
    \textbf{Proof:} We provide a scenario where any deterministic algorithm performs $\frac{p}{p+(1-p)^2}$ worse than the optimal algorithm which responds in an online manner with knowledge of the order. 
    We note that such an algorithm performs worse than the linear programming upper bound; therefore, no deterministic algorithm can perform better than a $\frac{p}{p+(1-p)^2}$ approximation. 
    We note that by Yao's principle, such a claim also implies that any random algorithm is also a $\frac{p}{p+(1-p)^2}$. 

    To prove this, we construct values of $\theta$. 
    We let $\theta_{N} = 1$ and let $\theta_{1} = \theta_{2} \ldots \theta_{N-1} = \frac{1}{P}$.
    Because of symmetry between patients 1 to $N-1$, we can characterize any deterministic algorithm as offering the single provider to $K$ out of $N-1$ of the patients and whether it is offered to patient $N$. 
    We note that not offering it to patient $N$ results in a reward of at most $p$. 

    Next, if an algorithm offers the single provider to $k$ of the $N-1$ patients and also to patient $N$, then we can compute an upper bound on the reward. 
    Suppose we have an ordering with $r$ of the $k$ patients before patient $N$; this occurs with probability $\frac{1}{k+1}$. 
    Then we incur an expected reward of $\sum_{i=0}^{r-1} (p^2(1-p)^i) + p(1-p)^r + \sum_{i=r+1}^{k} (p^2(1-p)^i) \leq p(1-p)^r + p^{2} \sum_{i=0}^{k-1} (1-p)^{i} = p ((1-p)^{r} + 1-(1-p)^{k})$. 
    We note that $r$ is uniformly distributed between $0$ and $k$, and therefore, the expected reward for such a policy is 
    \begin{equation}
        \frac{1}{k+1} (1-p)^{0}*p + (1-p)^{1}*p*\alpha + (1-p)^{2}*p*\alpha \cdots + (1-p)^{0}*p \alpha + (1-p)^{1}*p + \cdots = p (\sum_{i=1}^{K} (1-p)^{i}) \frac{\alpha k + 1}{k+1} = \frac{p(\alpha k + 1)}{k+1} \frac{1-(1-p)^{k+1}}{p}        
    \end{equation}

    \nrcomment{Make this rigorous}
    Visual inspection reveals that the maximum value is either $K=N$ or $K=0$, with the changeover point occurring at 
    \begin{equation}
        \alpha = \frac{-1+(1-p)^{N}+p+Np-(1-p)^{N}p}{N(1-(1-p)^{N} + (1-p)^{N}p}
    \end{equation}

    We then note that the optimal algorithm, with awareness of the order, is to offer first to patient $N$, wherever $N$ is in the order, and to all patients after $N$. 
    Such a policy results in a reward of
    \begin{equation}
        \frac{1}{N+1} ((1-p)^{0}*p + (1-p)^{0}*p+(1-p)^{1}*p\alpha \cdots) = p(N+1)+ p \alpha \sum_{k=1}^{N} \sum_{i=1}^{k} (1-p)^{i} = p(N+1) \sum_{k=1}^{N} \frac{(1-p)(1-(1-p)^{k})}{p} p \alpha = \frac{p(N+1) + \alpha (1-p)(N-(1-p)(1-(1-p)^{k})/p}{N+1}
    \end{equation}

    We then select $\alpha$ at the changeover point, so that the selection of $K$ is immaterial, giving a ratio of 
    \begin{equation}
        \frac{p}{\frac{1}{N+1}*\left(p*\left(N+1\right)+\frac{\left(-1+\left(1-p\right)^{N}+p+p-\left(1-p\right)^{N}p\right)}{N*\left(1-\left(1-p\right)^{N}+\left(1-p\right)^{N}p\right)}*\left(1-p\right)*\left(N-\left(1-p\right)*\frac{\left(1-\left(1-p\right)^{N}\right)}{p}\right)\right)}
    \end{equation}

    Note that for large $N$ and $p$ near 0, this term approaches $\frac{1}{2}$, which implies that the best deterministic algorithm is at most a 2-approximation. 
\end{proof}

 Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\subsection{General Algorithm Formulation}
 \begin{figure*}
     \centering 
     \includegraphics[width=0.75 \textwidth]{example-image-a}
     \caption{Figure for general algorithm}
 \end{figure*}


 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\subsection{Algorithm Guarantees}
 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sed lobortis neque. Vivamus rhoncus erat sapien. Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{theorem}
    Let $\pi$ be unknown, so that we aim to maximize: 
    \begin{equation}
        \max_{M_{1} \cdots M_{N}} \mathbb{E}_{\pi}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]
    \end{equation}
    Under a delayed response scenario, where $M_{i}$ are all chosen initially, letting $X_{i,j} = \mathbbm{1}[j = v_{i}]$ results in a $p$-approximation to the problem. 
    That is, when $X_{i,j} = \mathbbm{1}[j = v_{i}]$, $\mathrm{ALG} = \mathbb{E}_{\pi}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]$, and $\mathrm{OPT} = \max_{M_{1} \cdots M_{N}} \mathbb{E}_{\pi}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]$, then $\mathrm{ALG} \geq p \mathrm{OPT}$. 
    Moreover, such analysis is tight, as there exists $Y_{i,j}$ such that $\mathrm{ALG} = p \mathrm{OPT}$
\end{theorem}
\begin{proof}
    \textbf{Proof:} We prove this in two parts. First we demonstrate that our algorithm achieves a $p$-fraction of the linear-program solution to the problem, in expectation. 
    Next, we show that no algorithm can achieve more than the value from the linear program. 

    First, in our offering of the linear program, we note that each patient is only offered one provider, and no provider is offered to more than one patient. 
    Under this scenario, the results of each patient are independent Bernoulli variables, with probability of success $p$, scaled by the appropriate $\theta$ values. 
    That is, the expected reward is $\sum_{i=1}^{N} \theta_{i,v_{i}} p$

    Next, suppose that there exists some allocation of menus so that the rewards under such an assortment are higher than that of the corresponding linear program. 
    Let the matches from such an allocation be denoted $u_{i}$. 
    Then $\sum_{i=1}^{N} \theta_{i,u_{i}} > \sum_{i=1}^{N} \theta_{i,v_{i}}$. 
    However, such a statement is contradiction, as by the definition of $v$, it maximizes $\sum_{i=1}^{N} \theta_{i,v_{i}}$. 
    Therefore, no solution can improve upon the reward of the linear program, which implies that our algorithm achieves a reward of $p \mathrm{OPT}$. 

    We next demonstrate that such analysis is tight. 
    Let $\theta_{i,1} = 1$ for all $i$, and let $\theta_{i,j} = 0$ otherwise. 
    With the linear program solution, we get patient $i$ is assigned to provider $1$ for some $i$; this is the only patient who achieves a non-zero reward, and so our expected reward is $p$. 
    Note that the linear program solution to this problem achieves a reward of $1$. 
    Moreover, the optimal solution to this situation is to offer provider $1$ to all patients; then with probability $1-(1-p)^{N}$, some patient will be matched with provider $1$. 
    Therefore, our approximation ratio in this scenario is $\frac{p}{1-(1-p)^{N}}$; for large $N$, this approaches a $p$-fraction of the optimal reward. 
\end{proof}

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider an initial set of assortments, denoted by $X_{i,j}$, so that $X_{i,\pi(i)} = 1$ for some function $\pi: \{1,2,\cdots,N\} \rightarrow \{1,2,\cdots,P\}$. 
    Next, consider an algorithm that augments such an assortment through the following: at each step, a clique is chosen, so that for any $i,i'$ in the clique, $X_{i,\pi(i')} = X_{i',\pi(i)} = 1$. Then, for any patient $i$, for any random ordering, the resulting menu, $\mathbf{Z}_{i}$ is non-empty. 
\end{lemma}

\begin{proof}
    \textbf{Proof:} We will first prove that such a clique structure guarantees that all menus are non-empty.  
    Consider patient $i$ in a clique of size $k$, so that the size of the menu for patient $i$ is also $k$. 
    Each time a member of the clique, $i'$ is chosen before $i$ in the ordering $\pi$, the set of available options in the menu decreases by $1$. 
    Because there are $k$ members in the clique, at most $k-1$ things can come before $i$ in the ordering, and so the menu size is at least $k-(k-1) = 1$, which implies the menu is non-empty. 

    Next, we will prove that only such a clique structure can guarantee non-empty menus. 
    Consider a non-clique graph, and let $p=1$.
    Consider some minimum degree node $u$ with degree $r$ and its neighbor $v$. 
    Let $u = \pi_{N}$ in the ordering, and let $\pi$ be ordered inversely proportional to the degree of the corresponding node. 
    Suppose the menu for $u$ is non-empty, then either a) $u$ itself is avaialble or b) one of u's neighbors is available. 
    
    Option a implies that none of $u$'s neighbors selected $u$, despite $u$ being the most preferred node. 
    This implies that $u$ is a 1-clique, which breaks the assumption. 
    Next, suppose that some neighbor $v$ is available. 
    Suppose $v$ had more than $r-1$ neighbors; then by the pigeonhole principle, one of $v$'s neighbors is not a neighbor of $u$ , and therefore, $v$ is the most preferred option. 
    That is, this implies that there are at least $r$ other nodes which are just as favorable as $v$; when factoring in $u$, this implies that there are at least $r+1$ nodes of distance $0$ or $1$ from $u$, which breaks the assumption that $u$ is of degree $r$. 

    Therefore, $v$ must also be of degree $r$ and is also a minimum degree node. 
    Moreover, if $v$ is available, this implies that $v$ did not select itself, and therefore, $v$ must have selected $u$. 
    If none of $v$'s neighbors selected $v$, then all of $v$'s neighbors must also be adjacent to another neighbor of $u$. 
    Additionally, this implies that $v$ must be before all of $v$'s neighbors in the graph, as otherwise, $v$ would have been unselected. 

    Now consider the ordering $\pi$, so that it is ordered inversely to the distance to $u$. 
    For some neighbors $v$ to be unselected, each of its $r-1$ neighbors must not have selected it. 
    Suppose those $r-1$ neighbors come before $v$ in the ordering. 
    This implies that each of the neighbors was able to select $u$ or one of its $r-1$ non-$v$ neighbors. 
    This implies that each of $v$'s neighbors was able to select themselves. 
    This implies that $v$'s neighbors have no other neighbors but themselves. 
    Note that each of $v$'s neighbors must have at least $r$ neighbors. 
    This implies that the graph is a clique, which is a contradiction. 
\end{proof}

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
\begin{corollary}
    Suppose whether each patient successfully matched with some provider is chosen a prior. Then the simple choice algorithm is optimal for the match rate. 
\end{corollary}
\begin{proof}
    \textbf{Proof:} We note that if menus are always non-empty for any ordering of patients, then all patients who will get matched in the optimal algorithm will also get matched in this algorithm. 
    From Lemma (INSERT LEMMA NUMBER), we showed that, for our method, all menus are non-empty. 
    This implies that we achieve the maximum possible match rate of $p$. 
\end{proof}

\subsection{Skewed Market Dynamics}
\subsubsection{More Providers than Patients}
 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider a scenario with $P>N$, with fixed $\theta_{i,j}$ and $p$. 
    Let providers $N+1,N+2,\cdots,P$ be unmatched in the linear program according to $v_{i}$. 
    Denote the set of cliques as $S_{1} \cdots S_{K}$ so that $S_{1} \{s_{1,1} \cdots s_{1,n_{1}}$. 
    Let $\bar{\theta}_{i}$ be a sorted list of providers, in descending order, for provider $i$, such that $M_{i,j} = 1$. 
    Let $n_{j,l}$ denote the number of patients such that provider $j$ is one of the $l$ largest values in $\bar{theta}_{i}$. 
    Finally, construct a set of weights $w_{i,j}$ where $j \geq N+1$, such that $w_{i,j} = \sum_{k \in \bar{theta}_{i}} \max(\theta_{i,j}-\theta_{i,k},0) P_{i,k}$, and where 
    \begin{equation}
        P_{i,k} = \frac{1}{N} \sum_{l=1}^{N-1} \prod_{j'=1}^{k-1} 1-(1-p)^{n_{j',l}}
    \end{equation}
    When there's more providers than patients, then assigning excess providers according to the 1-1 matching problem with weights $w_{i,j}$ maximizes the upper bound on utility gained. 
\end{lemma}
\begin{proof}
    \textbf{Proof:} We first demonstrate that adding excess providers during the linear program is not useful. 
    This is because, during ``stage 1'', if both $j$ and $j'$ are matched to patient $i$, then either $\theta_{i,j} \geq \theta_{i,j'}$ or $\theta_{i,j'} \geq \theta_{i,j}$, and so either provider $j$ or $j'$ is redundant. 

    To assess how excess patients should be allocated when patients have non-unit menus, we compute an upper bound on the improvement when adding provider $j'$. 
    Let $b=\theta_{i,j'}$, and let patient $i$'s menu consist of patients $b_{1} \cdots b_{k}$, where $\theta_{i,b_{1}} \geq \theta_{i,b_{2}}, \ldots, \theta_{i,b_{k}}$. 
    We then compute our upper bound as 
    \begin{equation}
        \sum_{j=1}^{k} (b-b_{k}) \sum_{i=1}^{N-1} \frac{1}{N} \sum_{r=j-1}^{i} p^{r} (1-p)^{i-k-1} {i \choose r}
    \end{equation}
    We optimize this upper bound through a linear program, and note that we can compute this in $O(NP + BP^{2}) = O(P(N+BP))$ time. 
\end{proof}

\subsubsection{More Patients than Providers}
 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider an assortment planning scenario with $N > P$. 
    Consider the 1-1 matching problem with coefficients $v_{i}$. 
    For each provider $j$, let $M_{j}$ be the ratio of $\theta_{i,j}$ to $\theta_{i',j}$ where $\theta_{i',j}$ is the next largest value of $\theta_{i,j}$. 
    Then adding providers $i$ with $(1-\frac{p}{2})(1+M) \geq 1$ adds extra value. 
\end{lemma}
\begin{proof}
    \textbf{Proof:} Let the linear programming solution be $v_{i}$ and $v^{-1}_{j}$. 
    Consider the impact of adding an additional patient for provider $j$. 
    In particular, suppose that we aim to match patient $i'$ in addition to patient $\{i_{1}, i_{2}, \cdots i_{r}\}$ to provider $j$. 
    By symmetry, each of the $r$ patients have an equal probability of being selected; therefore, our expected reward when successfully matching is $\sum_{k=1}^{r} \frac{\theta_{i_{k},j}}{r}$. 
    The probability of matching is $1-(1-p)^{r}$, and so in total, our expected reward is $((1-(1-p)^{r})) \sum_{k=1}^{r} \frac{\theta_{i_{k},j}}{r}$. 

    Let $M_{j} = \frac{\sum_{k=1}^{r+1} \frac{\theta_{i_{k},j}}{r+1}}{\sum_{k=1}^{r} \frac{\theta_{i_{k},j}}{r}}$, which is the change in average reward when adding $i_{r+1}$. 
    Then we should add patient $i_{r+1}$ if 
    \begin{equation}
        M_{j} \geq \frac{1-(1-p)^{r}}{1-(1-p)^{r+1}}
    \end{equation}

    Each patient is used at most once, and there are $P$ total patients, leading to $O(NP)$ time for computation. 
\end{proof}

\subsection{Batching and Ordering Patients}
 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

\begin{lemma}
    Consider a scenario with known $\theta$ and fixed $p$. 
    Let $v_{i}$ be the solution to the 1-1 bipartite matching problem with edge coefficients $\theta_{i,j}$. 
    Construct a graph $G=(V,E)$ so that there are $N$ nodes. 
    Let node $i$ be connected to node $i'$ if $\theta_{i,v_{i}} \leq \theta_{i,v_{i'}}$; that is edges exist only for pairs with $\theta_{i,v_{i}} \leq \theta_{i,v_{i'}}$. 
    Let $\pi$ be some reverse topological ordering ordering over such a graph, so that $\pi_{1}$ has no outgoing edges. 
    Finally, let $M_{i,j} = \mathbbm{1}[\theta_{i,v_{i}} \leq \theta_{i,j}]$. 
    Then $M_{i,j}$ is optimal under ordering $\pi$. Moreover, let $\mathrm{ALG} = \mathbb{E}_{\mathbf{Y}}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi_{i},j} Y_{\pi_{i},j}(\mathbf{Z}_{i})]$. 
    If $\mathrm{OPT} = \max_{\pi'} \max_{M_{i}} \mathbb{E}_{\mathbf{Y}}[\sum_{i=1}^{N} \sum_{j=1}^{P} \theta_{\pi'_{i},j} Y_{\pi'_{i},j}(\mathbf{Z}_{i})]$, then $\mathrm{OPT} = \mathrm{ALG}$.  
    \nrcomment{N=P}
\end{lemma}
\begin{prooof}
    \textbf{Proof}: Let $G=(V,E)$ be a directed graph with nodes corresponding to patients and an edge between $i$ and $i'$ indicating that $\theta_{i,v_{i'}} \geq \theta_{i,v_{i}}$. 
    Note that such a graph is necessarily acyclic; any cycle indicates that there exists a pareto dominant allocation of providers to patients in the linear program, which contradicts the optimality of the linear program. 

    Next note that for a sink node (no outgoing edges), when offered a menu that is a subset of $[P]$, it is immaterial what providers have been selected previously, assuming that $v_{i}$ is selected. 
    This is because $v_{i}$ is the most provider for patient $i$, and therefore unless $v_{i}$ is selected, patient $i$ will not select any other provider. 

    Note that our online response algorithm will give patient $i$ first access to provider $v_{i}$, and therefore, it always achieves $p \theta_{i,v_{i}}$. 

    We next consider a non-sink node, corresponding to patient $i$. 
    For such a node, there exists some other patient, $i'$, such that, if $i'$ is unselected prior to $i$, then with some probability, if $i'$ is unselected, then it will increase the score of $i$. 
    In otherwords, placing provider $i'$ before patient $i$ increases the reward over placing $i'$ after $i$. 

    We will finally demonstrate that the reverse topological ordering is the optimal ordering; that is, any ordering that is not this ordering can be modified to improve the expected reward. 
    Moroever, through a series of steps, any ordering can be transformed into the reverse topological ordering while not decreasing the expected reward. 

    For any ordering ,if there exists a node such that $i'$ is after $i$, with no nodes in between, then swapping $i'$ and $i$ can improve the expected reward. 
    Note that for the reverse topological ordering, no such swap exists. 

    For any non-topological ordering, where some forward facing arrow exists from $i$ to $i'$, consider the following: 
    \begin{enumerate}
        \item If no arrows exists from $i$ to another node in between $i'$ and $i$, and no arrow exists from $i'$ to any node between $i$ and $i'$, then the nodes can be swapped as needed. 
        Moreover, we can place $i$ and $i'$ in sequence without impacting the results from the other nodes. 
        \item If there exists some arrow in between, then if the arrow is forward facing from $i$ or facing towards $i'$, then we can recurse on this. Otherwise, we must have that the arrows match those of the reverse topological ordering. 
        Note that, because the graph is acyclic, there exists no node $r$ that is connected to both $i$ and $i'$. 
        Moreover, there exists no other path from $i'$ to $i$. 
        To remove the forward facing edge, we move $i'$, and all nodes connected from $i'$, before $i$. 
        Note that $i$ cannot connect to any of these nodes due to acyclic property, and so this only has an impact on the $i-i'$ pattern. 
        We next note that $i$ and $i'$ are adjacent now; swapping them, thereby swapping the arrow, can only improve performance and expected reward. 
    \end{enumerate}
\end{prooof}

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat

\begin{lemma}
    Consider an application of our approximation algorithm to an assortment planning problem with preferences $\theta$. 
    Denote the set of cliques as $S_{1} \cdots S_{K}$ so that $S_{1} \{s_{1,1} \cdots s_{1,n_{1}}$. 
    Construct a $G=(V,E)$ so that there are $n_{r}$ nodes. 
    For two nodes $i,i' \in S_{r}$, let node $i$ be connected to node $i'$ if $\theta_{i,v_{i}} \leq \theta_{i,v_{i'}}$; that is edges exist only for pairs with $\theta_{i,v_{i}} \leq \theta_{i,v_{i'}}$. 
    Then the optimal batching strategy is to partition $G$ such that each partition is continuous in the topological ordering. 
    Moreover, the optimal such ordering is to have $n_{r}$ batches, each of unit size, essenetially reducing it to the online assortment problem.
\end{lemma}
\begin{proof}
    Consider a partitioning of nodes, such that, within each partition, ordering is random, but partitions can be ordered. 
    This corresponds to selecting clusters of patients, where the ordering within the cluster of patients is random .

    We first note that menus are only shown up to a budget $B$. 
    This means that with $B$ partitions, we can fully control the order. 

    We next note that $B$ partitions are not necesary to fully control the ordering. 
    If, within a partition, there exists no nodes between nodes, then any ordering of the nodes produces the same expected rewards. 
    The question then becomes how many such partitions are needed so that there are no edges within a partition. 
    To compute this, we solve this using dynamic programming. 

    Let $v_{a,b}$ be the dynamic programming solution of whether there exists a parittion where we use only $b$ partitions, starting with patient $i$ (in the reverse topological ordering). 
    We aim to compute $v_{*,N}$, then finding the minimum value where this is 1.  
    Our base cases are $v_{N,*} = 1$ and $v_{-N,0} = 0$. 
    Recursively, we let $v_{a,b} = v_{a+1,b-1} \lor v_{a+2,b-1},\ldots,v_{a+j,b-1}$, where $j$ is the smallest number so that $a$ to $a+j$ contain no edges between nodes. 
    This can be computed in $O(N^{3})$ time in the worst case, and allows us to find the optimal ordering using the minimum number of partitions. 
\end{proof}

 \subsection{Accounting for Provider Balance}
 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.
 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec dignissim. Sed vulputate mattis arcu, sed vehicula sem interdum quis. Aenean faucibus elit sed lectus congue, a pretium tellus convallis. Nunc sagittis cursus tincidunt. Morbi ut tincidunt nulla.

 Aenean ut tempor metus, et semper lorem. Integer suscipit finibus dolor eu porttitor. Suspendisse sagittis, nisl ac elementum convallis, leo mauris imperdiet velit, et blandit ante leo eget turpis. Aenean ultrices est sed ante posuere placerat. Nullam porttitor faucibus nulla commodo molestie. Quisque ultricies et tortor nec