\section{Background and Related Work}
\subsection{Transfomer-based LLM Training}

LLMs like OPT: Brown et al., "I Am a Large Language Model I Can Give You a Referral"__GPT series: Radford et al., "Improving Language Understanding by Generative Pre-Training"

and fine-tuning on domain-specific tasks____ enables broader participation but still faces significant memory challenges.

GPU memory during training is consumed by activations and model data (parameters, gradients, optimizer states).
Using mixed-precision training with ADAM optimizer____, activations and parameters are stored in FP16 while maintaining FP32 precision for optimizer states, which include momentum, variance, and FP32 parameter copies.
This requires 16\emph{M} bytes for a model with \emph{M} parameters, meaning a 2-billion parameter model would exceed a single V100 GPU's memory capacity even before accounting for activations, which scale with batch size and sequence length. Memory fragmentation further constrains effective utilization, making these resources extremely scarce.
In response, research efforts are increasingly focused on enhancing LLM training through multi-GPU parallelism and single-GPU memory optimization techniques.


\subsection{Parallel Training}

\textbf{Data Parallelism (DP)}____ distributes inputs across GPUs while maintaining full model copies. However, DP alone cannot handle LLM training due to single-Gpu memory constraints. ZeRO: Ho et al., "ZeRO: Memory Optimizations Towards Training Large Neural Models"__
addresses this by partitioning model states across GPUs, using efficient reduce-scatter and all-gather operations to minimize communication overhead.

% \textbf{Data Parallelism.} Data parallelism (DP)____ received a lot of attention to speed up traditional DNN training. It distributes inputs and activations across GPUs but retains a full model copy in each GPU. However, DP alone is infeasible for LLM training, as the model data of an LLM typically exceeds the memory capacity of a single GPU. ZeRO: Ho et al., "ZeRO: Memory Optimizations Towards Training Large Neural Models"__
tackles this limitation by further partitioning optimizer states, gradients, and parameters across GPUs. The all-reduce operation is decomposed into reduce-scatter and all-gather operations before and after the optimizer update phase, effectively avoiding additional communication overhead.

\textbf{Model Parallelism (MP)} ____ divides model parameter across GPUs. Mesh-Tensorflow: Meng et al., "Mesh-TensorFlow: A Library for High-Performance Machine Learning on Heterogeneous Hardware"__ enables flexible tensor partitioning, while Megatron-LM: Shoeybi et al., "Megatron-LM: Training Multi-Billion Parameter Models Using Model Parallelism"__
optimizes communication through strategic row and column parallelism for transformer-based LLMs.

% \textbf{Model Parallelism.} Motivated by the usage of aggregated GPU memory to store model data, model parallelism (MP) is re-exploited____. It divides the parameter matrices of each layer throughout the GPU cluster. Mesh-Tensorflow: Meng et al., "Mesh-TensorFlow: A Library for High-Performance Machine Learning on Heterogeneous Hardware"__ provides a high-level interface to support tensor partitioning along any dimensions, where users can design custom MP policies. Megatron-LM: Shoeybi et al., "Megatron-LM: Training Multi-Billion Parameter Models Using Model Parallelism"__
is the first tailored MP design for transformer-based LLMs, which mitigates communication requirements by crossover row-parallel and column-parallel in each module.

\textbf{Pipeline Parallelism (PP)} ____ segments models into stages, processing micro-batches in a pipelined manner. Efficient scheduling strategies____ are crucial for balanced GPU utilization and throughput.

% \textbf{Pipeline Parallelism.} Pipeline parallelism (PP)____ partitions the layer sequence of the model into distinct stages, with activations divided into multiple micro-batch and processed in a pipelined manner across these stages. A well-crafted placement and scheduling strategy____ is crucial to facilitate training throughput and load-balanced memory utilization among the different GPUs.


\textbf{Hybrid Parallelism}____, combining DP, MP, and PP, has also been developed to achieve optimal distributed training efficiency.
Nevertheless, these parallelism solutions necessitate sufficient aggregated GPU memory to accommodate model data, presenting a significant affordability challenge.


\subsection{Memory-Saving Training}

% \textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations at checkpoints while trading additional computation.
\textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations as checkpoints while recomputing discarded activations from these checkpoints during the backward pass.
A typical approach segments an \emph{N}-layer model into $\sqrt{N}$ parts____. Various strategies have been proposed, from optimal but computationally intensive solutions____ to faster but sub-optimal approaches____, each balancing memory savings against recomputation costs.


% \textbf{Activation Checkpointing.} Activation checkpointing trades additional recomputation for memory savings by segmenting an \emph{N} layer model into $\sqrt{N}$ parts, retaining only the input activations at each checkpoint. Due to its simplicity and effectiveness, this method has been widely adopted in various deep learning toolkits. The choice of checkpointing strategies varies significantly, influenced by different computational graph structures and the balance between recomputation costs and memory savings. Solutions range from those requiring extensive search time but yielding optimal results____, to quicker methods that produce sub-optimal sequences____.

% The activation checkpointing technique introduces additional recomputing in exchange for memory savings on activations during training. Chen et al.____ divided the \emph{N} layer model into $\sqrt{N}$ segments, with each segment retaining only the input activations as a checkpoint. This method has been integrated into many deep learning toolkits due to its simplicity and effectiveness. In fact, the strategy space of activation checkpointing is exponential due to varying recomputing overheads and memory savings of each operator. Considering different computing graph structures of models, different solvers for activation checkpointing are exploited. One requires expensive search time but obtains optimality____, and others solve faster but obtain sub-optimal recomputing sequences____________.




\textbf{Offloading} leverages CPU memory to supplement GPU memory during training. While initially developed for CNNs to manage activations____, LLM training shifts focus to handling model data. L2L: Huang et al., "LLM-Adapter: Adapting Pre-Trained Language Models with a Single Adapter Layer"__
pioneered heterogeneous LLM training by keeping only one transformer layer on GPU. ZeRO-Offload:  Zhang et al., "ZeRO-Offload: Decentralized Training of Large Neural Networks through Efficient Offloading and Asynchronous Reduction"__
enables training of 13-billion parameter models on a single V100 GPU by offloading gradients and optimizer states to CPU, though its static approach limits efficiency. 
% PatrickStar____ introduces dynamic resource management but lacks effective overlap between computation and data transfer.
PatrickStar: Liu et al., "PatrickStar: A Dynamic Resource Management Framework for Heterogeneous Training"__
and StrongHold: Liu et al., "StrongHold: Asynchronous Execution of Model Parameters in Distributed Deep Learning"__
introduced dynamic memory allocation methods for model data using synchronous and asynchronous execution, respectively. However, the absence of comprehensive analyses of memory usage and execution overhead during training results in suboptimal performance.

% \textbf{Offloading.} The offloading technique uses CPU memory to supplement GPU memory for heterogeneous training, initially researched extensively for convolutional neural networks where activations are the main memory consumers____________. In LLM training, the emphasis shifts to managing model data, including parameters, gradients, and optimizer states. L2L: Huang et al., "LLM-Adapter: Adapting Pre-Trained Language Models with a Single Adapter Layer"__
, the first heterogeneous training approach for LLMs, keeps only one transformer layer on the GPU, offloading the rest to CPU memory. Despite its innovations, L2L struggles with the memory demands of optimizer states that require significantly more space than parameters when using a mixed-precision ADAM optimizer. ZeRO-Offload:  Zhang et al., "ZeRO-Offload: Decentralized Training of Large Neural Networks through Efficient Offloading and Asynchronous Reduction"__
advances this approach by offloading gradients and optimizer states to the CPU, enabling the training of 13-billion parameter models on a single V100 GPU. Nonetheless, its static strategy does not account for the actual memory needs and capacities, resulting in inefficient GPU usage and increased workload. PatrickStar: Liu et al., "PatrickStar: A Dynamic Resource Management Framework for Heterogeneous Training"__
improves resource allocation by dynamically managing model data between CPU and GPU, but its performance is hindered by the lack of overlapping between GPU computing, data transfer, and CPU processing.

This paper focuses on GPU memory-saving training to make LLM training more accessible.
Existing approaches lack flexibility and do not jointly consider activation checkpointing, offloading, and hybrid optimizer strategies.
Furthermore, the possibility of operator scheduling remains unexplored in LLM heterogenous training, thus leaving room for significant performance improvements.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Fig/overview.pdf}
    \caption{The overview of AutoHete.}
    \label{figmethod}
\end{figure*}