



@inproceedings{reed2022torchfx,
  title={torch. fx: Practical program capture and transformation for deep learning in python},
  author={Reed, James and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={638--651},
  year={2022}
}

@inproceedings{Attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@article{exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  journal={OpenAI}
}

@inproceedings{xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}




@article{scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
journal={hal-03850124},
year={2023}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{wall,
  title={AI and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  publisher={IEEE}
}

@article{pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@inproceedings{unified,
  title={A unified architecture for accelerating distributed {DNN} training in heterogeneous {GPU/CPU} clusters},
  author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
  booktitle={Proceedings of the USENIX Symposium on Operating Systems Design and Implementation},
  pages={463--479},
  year={2020},
}

@inproceedings{can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the  International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}


@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{pipedream,
  title={PipeDream: generalized pipeline parallelism for {DNN} training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@inproceedings{chimera,
  title={Chimera: efficiently training large-scale neural networks with bidirectional pipelines},
  author={Li, Shigang and Hoefler, Torsten},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2021}
}


@inproceedings{alpa,
  title={Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={Proceedings of the  USENIX Symposium on Operating Systems Design and Implementation},
  pages={559--578},
  year={2022}
}

@inproceedings{generic,
  title={A generic communication scheduler for distributed {DNN} training acceleration},
  author={Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
  booktitle={Proceedings of the ACM Symposium on Operating Systems Principles},
  pages={16--29},
  year={2019}
}

@inproceedings{sparcml,
  title={SparCML: High-performance sparse communication for machine learning},
  author={Renggli, C{\`e}dric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2019}
}

@article{acctfm,
  title={Acctfm: An effective intra-layer model parallelization strategy for training large-scale transformer-based models},
  author={Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Li, Kenli and Li, Keqin},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={12},
  pages={4326--4338},
  year={2022},
  publisher={IEEE}
}


@inproceedings{efftra,
  title={Training acceleration for deep neural networks: A hybrid parallelization strategy},
  author={Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Chang, Wanli and Li, Kenli},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)},
  pages={1165--1170},
  year={2021},
  organization={IEEE}
}

@article{chen2016,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  booktitle={Proceedings of the Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}

@inproceedings{rockmate,
  title={Rockmate: an efficient, fast, automatic and generic tool for re-materialization in pytorch},
  author={Zhao, Xunyi and Le Hellard, Th{\'e}otime and Eyraud-Dubois, Lionel and Gusak, Julia and Beaumont, Olivier},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={42018--42045},
  year={2023},
  organization={PMLR}
}

@article{dtr,
  title={Dynamic tensor rematerialization},
  author={Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary},
  journal={arXiv preprint arXiv:2006.09616},
  year={2020}
}

@article{l2l,
  title={Training large neural networks with constant memory using a new execution algorithm},
  author={Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  journal={arXiv preprint arXiv:2002.05645},
  year={2020}
}

@inproceedings{ZeRO-Offload,
  title={Zero-offload: Democratizing billion-scale model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={Proceedings of the  USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}


@article{patrickstar,
  title={Parallel training of pre-trained models via chunk-based dynamic memory management},
  author={Fang, Jiarui and Zhu, Zilin and Li, Shenggui and Su, Hui and Yu, Yang and Zhou, Jie and You, Yang},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={1},
  pages={304--315},
  year={2022},
  publisher={IEEE}
}

@inproceedings{stronghold,
  title={Stronghold: fast and affordable billion-scale deep learning model training},
  author={Sun, Xiaoyang and Wang, Wei and Qiu, Shenghao and Yang, Renyu and Huang, Songfang and Xu, Jie and Wang, Zheng},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--17},
  year={2022},
  organization={IEEE}
}


@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}



@article{horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@inproceedings{dapple,
  title={DAPPLE: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the  ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021}
}


@inproceedings{combination,
  title={Efficient combination of rematerialization and offloading for training dnns},
  author={Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={34},
  pages={23844--23857},
  year={2021}
}


@inproceedings{vdnn,
  title={vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design},
  author={Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W},
  booktitle={Proceedings of the Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={1--13},
  year={2016},
  organization={IEEE}
}

@inproceedings{swapadvisor,
  title={Swapadvisor: Pushing deep learning beyond the {GPU} memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}


@inproceedings{capuchin,
  title={Capuchin: Tensor-based {GPU} memory management for deep learning},
  author={Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
  booktitle={Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={891--905},
  year={2020}
}


@inproceedings{AdaPipe,
  title={AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning},
  author={Sun, Zhenbo and Cao, Huanqi and Wang, Yuanwei and Feng, Guanyu and Chen, Shengqi and Wang, Haojie and Chen, Wenguang},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={86--100},
  year={2024}
}


@inproceedings{yuliang,
  title={Accelerating the training of large language models using efficient activation rematerialization and optimal hybrid parallelism},
  author={Yuan, Tailing and Liu, Yuliang and Ye, Xucheng and Zhang, Shenglong and Tan, Jianchao and Chen, Bin and Song, Chengru and Zhang, Di},
  booktitle={2024 USENIX Annual Technical Conference (USENIX ATC 24)},
  pages={545--561},
  year={2024}
}

@inproceedings{megascale,
  title={$\{$MegaScale$\}$: Scaling large language model training to more than 10,000 $\{$GPUs$\}$},
  author={Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and others},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={745--760},
  year={2024}
}