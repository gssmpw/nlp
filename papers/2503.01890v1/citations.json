[
  {
    "index": 0,
    "papers": [
      {
        "key": "opt",
        "author": "Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others",
        "title": "Opt: Open pre-trained transformer language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "gpt1",
        "author": "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others",
        "title": "Improving language understanding by generative pre-training"
      },
      {
        "key": "gpt2",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "gpt3",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "gpt4",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ZeRO-Offload",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Zero-offload: Democratizing billion-scale model training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pytorch",
        "author": "Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others",
        "title": "Pytorch distributed: Experiences on accelerating data parallel training"
      },
      {
        "key": "unified",
        "author": "Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong",
        "title": "A unified architecture for accelerating distributed {DNN} training in heterogeneous {GPU/CPU} clusters"
      },
      {
        "key": "generic",
        "author": "Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong",
        "title": "A generic communication scheduler for distributed {DNN} training acceleration"
      },
      {
        "key": "sparcml",
        "author": "Renggli, C{\\`e}dric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten",
        "title": "SparCML: High-performance sparse communication for machine learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zero",
        "author": "Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong",
        "title": "Zero: Memory optimizations toward training trillion parameter models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "towards",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "pytorch",
        "author": "Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others",
        "title": "Pytorch distributed: Experiences on accelerating data parallel training"
      },
      {
        "key": "unified",
        "author": "Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong",
        "title": "A unified architecture for accelerating distributed {DNN} training in heterogeneous {GPU/CPU} clusters"
      },
      {
        "key": "generic",
        "author": "Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong",
        "title": "A generic communication scheduler for distributed {DNN} training acceleration"
      },
      {
        "key": "sparcml",
        "author": "Renggli, C{\\`e}dric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten",
        "title": "SparCML: High-performance sparse communication for machine learning"
      },
      {
        "key": "pipesgd",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zero",
        "author": "Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong",
        "title": "Zero: Memory optimizations toward training trillion parameter models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      },
      {
        "key": "acctfm",
        "author": "Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Li, Kenli and Li, Keqin",
        "title": "Acctfm: An effective intra-layer model parallelization strategy for training large-scale transformer-based models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mesh",
        "author": "Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others",
        "title": "Mesh-tensorflow: Deep learning for supercomputers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      },
      {
        "key": "acctfm",
        "author": "Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Li, Kenli and Li, Keqin",
        "title": "Acctfm: An effective intra-layer model parallelization strategy for training large-scale transformer-based models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "mesh",
        "author": "Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others",
        "title": "Mesh-tensorflow: Deep learning for supercomputers"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gpipe",
        "author": "Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others",
        "title": "Gpipe: Efficient training of giant neural networks using pipeline parallelism"
      },
      {
        "key": "pipedream",
        "author": "Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei",
        "title": "PipeDream: generalized pipeline parallelism for {DNN} training"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "dapple",
        "author": "Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others",
        "title": "DAPPLE: A pipelined data parallel approach for training large models"
      },
      {
        "key": "chimera",
        "author": "Li, Shigang and Hoefler, Torsten",
        "title": "Chimera: efficiently training large-scale neural networks with bidirectional pipelines"
      },
      {
        "key": "AdaPipe",
        "author": "Sun, Zhenbo and Cao, Huanqi and Wang, Yuanwei and Feng, Guanyu and Chen, Shengqi and Wang, Haojie and Chen, Wenguang",
        "title": "AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gpipe",
        "author": "Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others",
        "title": "Gpipe: Efficient training of giant neural networks using pipeline parallelism"
      },
      {
        "key": "pipedream",
        "author": "Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei",
        "title": "PipeDream: generalized pipeline parallelism for {DNN} training"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "dapple",
        "author": "Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others",
        "title": "DAPPLE: A pipelined data parallel approach for training large models"
      },
      {
        "key": "chimera",
        "author": "Li, Shigang and Hoefler, Torsten",
        "title": "Chimera: efficiently training large-scale neural networks with bidirectional pipelines"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "alpa",
        "author": "Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others",
        "title": "Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning"
      },
      {
        "key": "yuliang",
        "author": "Yuan, Tailing and Liu, Yuliang and Ye, Xucheng and Zhang, Shenglong and Tan, Jianchao and Chen, Bin and Song, Chengru and Zhang, Di",
        "title": "Accelerating the training of large language models using efficient activation rematerialization and optimal hybrid parallelism"
      },
      {
        "key": "megascale",
        "author": "Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and others",
        "title": "$\\{$MegaScale$\\}$: Scaling large language model training to more than 10,000 $\\{$GPUs$\\}$"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "chen2016",
        "author": "Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos",
        "title": "Training deep nets with sublinear memory cost"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "checkmate",
        "author": "Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion",
        "title": "Checkmate: Breaking the memory wall with optimal tensor rematerialization"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "dtr",
        "author": "Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary",
        "title": "Dynamic tensor rematerialization"
      },
      {
        "key": "rockmate",
        "author": "Zhao, Xunyi and Le Hellard, Th{\\'e}otime and Eyraud-Dubois, Lionel and Gusak, Julia and Beaumont, Olivier",
        "title": "Rockmate: an efficient, fast, automatic and generic tool for re-materialization in pytorch"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "checkmate",
        "author": "Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion",
        "title": "Checkmate: Breaking the memory wall with optimal tensor rematerialization"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "combination",
        "author": "Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena",
        "title": "Efficient combination of rematerialization and offloading for training dnns"
      },
      {
        "key": "dtr",
        "author": "Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary",
        "title": "Dynamic tensor rematerialization"
      },
      {
        "key": "rockmate",
        "author": "Zhao, Xunyi and Le Hellard, Th{\\'e}otime and Eyraud-Dubois, Lionel and Gusak, Julia and Beaumont, Olivier",
        "title": "Rockmate: an efficient, fast, automatic and generic tool for re-materialization in pytorch"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "chen2016",
        "author": "Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos",
        "title": "Training deep nets with sublinear memory cost"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "checkmate",
        "author": "Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion",
        "title": "Checkmate: Breaking the memory wall with optimal tensor rematerialization"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "combination",
        "author": "Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena",
        "title": "Efficient combination of rematerialization and offloading for training dnns"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "dtr",
        "author": "Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary",
        "title": "Dynamic tensor rematerialization"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "rockmate",
        "author": "Zhao, Xunyi and Le Hellard, Th{\\'e}otime and Eyraud-Dubois, Lionel and Gusak, Julia and Beaumont, Olivier",
        "title": "Rockmate: an efficient, fast, automatic and generic tool for re-materialization in pytorch"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "vdnn",
        "author": "Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W",
        "title": "vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design"
      },
      {
        "key": "swapadvisor",
        "author": "Huang, Chien-Chin and Jin, Gu and Li, Jinyang",
        "title": "Swapadvisor: Pushing deep learning beyond the {GPU} memory limit via smart swapping"
      },
      {
        "key": "capuchin",
        "author": "Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai",
        "title": "Capuchin: Tensor-based {GPU} memory management for deep learning"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "l2l",
        "author": "Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth",
        "title": "Training large neural networks with constant memory using a new execution algorithm"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "ZeRO-Offload",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Zero-offload: Democratizing billion-scale model training"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "patrickstar",
        "author": "Fang, Jiarui and Zhu, Zilin and Li, Shenggui and Su, Hui and Yu, Yang and Zhou, Jie and You, Yang",
        "title": "Parallel training of pre-trained models via chunk-based dynamic memory management"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "patrickstar",
        "author": "Fang, Jiarui and Zhu, Zilin and Li, Shenggui and Su, Hui and Yu, Yang and Zhou, Jie and You, Yang",
        "title": "Parallel training of pre-trained models via chunk-based dynamic memory management"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "stronghold",
        "author": "Sun, Xiaoyang and Wang, Wei and Qiu, Shenghao and Yang, Renyu and Huang, Songfang and Xu, Jie and Wang, Zheng",
        "title": "Stronghold: fast and affordable billion-scale deep learning model training"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "vdnn",
        "author": "Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W",
        "title": "vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "swapadvisor",
        "author": "Huang, Chien-Chin and Jin, Gu and Li, Jinyang",
        "title": "Swapadvisor: Pushing deep learning beyond the {GPU} memory limit via smart swapping"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "capuchin",
        "author": "Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai",
        "title": "Capuchin: Tensor-based {GPU} memory management for deep learning"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "l2l",
        "author": "Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth",
        "title": "Training large neural networks with constant memory using a new execution algorithm"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "ZeRO-Offload",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "Zero-offload: Democratizing billion-scale model training"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "patrickstar",
        "author": "Fang, Jiarui and Zhu, Zilin and Li, Shenggui and Su, Hui and Yu, Yang and Zhou, Jie and You, Yang",
        "title": "Parallel training of pre-trained models via chunk-based dynamic memory management"
      }
    ]
  }
]