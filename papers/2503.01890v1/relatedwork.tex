\section{Background and Related Work}
\subsection{Transfomer-based LLM Training}

LLMs like OPT~\cite{opt} and GPT series~\cite{gpt1,gpt2,gpt3,gpt4} stack multiple transformer blocks for powerful sequence modeling and representation learning. While pre-training requires vast computational resources dominated by tech companies, fine-tuning on domain-specific tasks~\cite{llama2} enables broader participation but still faces significant memory challenges.

GPU memory during training is consumed by activations and model data (parameters, gradients, optimizer states).
Using mixed-precision training with ADAM optimizer~\cite{ZeRO-Offload}, activations and parameters are stored in FP16 while maintaining FP32 precision for optimizer states, which include momentum, variance, and FP32 parameter copies.
This requires 16\emph{M} bytes for a model with \emph{M} parameters, meaning a 2-billion parameter model would exceed a single V100 GPU's memory capacity even before accounting for activations, which scale with batch size and sequence length. Memory fragmentation further constrains effective utilization, making these resources extremely scarce.
In response, research efforts are increasingly focused on enhancing LLM training through multi-GPU parallelism and single-GPU memory optimization techniques.


\subsection{Parallel Training}

\textbf{Data Parallelism (DP)}~\cite{pytorch,unified,generic,sparcml} distributes inputs across GPUs while maintaining full model copies. However, DP alone cannot handle LLM training due to single-GPU memory constraints. ZeRO~\cite{zero} addresses this by partitioning model states across GPUs, using efficient reduce-scatter and all-gather operations to minimize communication overhead.

% \textbf{Data Parallelism.} Data parallelism (DP)~\cite{towards,pytorch,unified,generic,sparcml,pipesgd} received a lot of attention to speed up traditional DNN training. It distributes inputs and activations across GPUs but retains a full model copy in each GPU. However, DP alone is infeasible for LLM training, as the model data of an LLM typically exceeds the memory capacity of a single GPU. ZeRO~\cite{zero} tackles this limitation by further partitioning optimizer states, gradients, and parameters across GPUs. The all-reduce operation is decomposed into reduce-scatter and all-gather operations before and after the optimizer update phase, effectively avoiding additional communication overhead.

\textbf{Model Parallelism (MP)} ~\cite{megatron,acctfm} divides model parameter across GPUs. Mesh-Tensorflow~\cite{mesh} enables flexible tensor partitioning, while Megatron-LM~\cite{megatron} optimizes communication through strategic row and column parallelism for transformer-based LLMs.

% \textbf{Model Parallelism.} Motivated by the usage of aggregated GPU memory to store model data, model parallelism (MP) is re-exploited~\cite{megatron,acctfm}. It divides the parameter matrices of each layer throughout the GPU cluster. Mesh-Tensorflow~\cite{mesh} provides a high-level interface to support tensor partitioning along any dimensions, where users can design custom MP policies. Megatron-LM~\cite{megatron} is the first tailored MP design for transformer-based LLMs, which mitigates communication requirements by crossover row-parallel and column-parallel in each module.

\textbf{Pipeline Parallelism (PP)} ~\cite{gpipe,pipedream} segments models into stages, processing micro-batches in a pipelined manner. Efficient scheduling strategies~\cite{dapple,chimera,AdaPipe} are crucial for balanced GPU utilization and throughput.

% \textbf{Pipeline Parallelism.} Pipeline parallelism (PP)~\cite{gpipe,pipedream} partitions the layer sequence of the model into distinct stages, with activations divided into multiple micro-batch and processed in a pipelined manner across these stages. A well-crafted placement and scheduling strategy~\cite{dapple,chimera} is crucial to facilitate training throughput and load-balanced memory utilization among the different GPUs.


\textbf{Hybrid Parallelism}~\cite{alpa,yuliang,megascale}, combining DP, MP, and PP, has also been developed to achieve optimal distributed training efficiency.
Nevertheless, these parallelism solutions necessitate sufficient aggregated GPU memory to accommodate model data, presenting a significant affordability challenge.


\subsection{Memory-Saving Training}

% \textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations at checkpoints while trading additional computation.
\textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations as checkpoints while recomputing discarded activations from these checkpoints during the backward pass.
A typical approach segments an \emph{N}-layer model into $\sqrt{N}$ parts~\cite{chen2016}. Various strategies have been proposed, from optimal but computationally intensive solutions~\cite{checkmate} to faster but sub-optimal approaches~\cite{dtr,rockmate}, each balancing memory savings against recomputation costs.


% \textbf{Activation Checkpointing.} Activation checkpointing trades additional recomputation for memory savings by segmenting an \emph{N} layer model into $\sqrt{N}$ parts, retaining only the input activations at each checkpoint. Due to its simplicity and effectiveness, this method has been widely adopted in various deep learning toolkits. The choice of checkpointing strategies varies significantly, influenced by different computational graph structures and the balance between recomputation costs and memory savings. Solutions range from those requiring extensive search time but yielding optimal results~\cite{checkmate}, to quicker methods that produce sub-optimal sequences~\cite{combination,dtr,rockmate}.

% The activation checkpointing technique introduces additional recomputing in exchange for memory savings on activations during training. Chen et al.~\cite{chen2016} divided the \emph{N} layer model into $\sqrt{N}$ segments, with each segment retaining only the input activations as a checkpoint. This method has been integrated into many deep learning toolkits due to its simplicity and effectiveness. In fact, the strategy space of activation checkpointing is exponential due to varying recomputing overheads and memory savings of each operator. Considering different computing graph structures of models, different solvers for activation checkpointing are exploited. One requires expensive search time but obtains optimality~\cite{checkmate}, and others solve faster but obtain sub-optimal recomputing sequences~\cite{combination}~\cite{dtr}~\cite{rockmate}.




\textbf{Offloading} leverages CPU memory to supplement GPU memory during training. While initially developed for CNNs to manage activations~\cite{vdnn,swapadvisor,capuchin}, LLM training shifts focus to handling model data. L2L~\cite{l2l} pioneered heterogeneous LLM training by keeping only one transformer layer on GPU. ZeRO-Offload~\cite{ZeRO-Offload} enables training of 13-billion parameter models on a single V100 GPU by offloading gradients and optimizer states to CPU, though its static approach limits efficiency. 
% PatrickStar~\cite{patrickstar} introduces dynamic resource management but lacks effective overlap between computation and data transfer.
PatrickStar~\cite{patrickstar} and StrongHold~\cite{stronghold} introduced dynamic memory allocation methods for model data using synchronous and asynchronous execution, respectively. However, the absence of comprehensive analyses of memory usage and execution overhead during training results in suboptimal performance.

% \textbf{Offloading.} The offloading technique uses CPU memory to supplement GPU memory for heterogeneous training, initially researched extensively for convolutional neural networks where activations are the main memory consumers~\cite{vdnn}\cite{swapadvisor}\cite{capuchin}. In LLM training, the emphasis shifts to managing model data, including parameters, gradients, and optimizer states. L2L~\cite{l2l}, the first heterogeneous training approach for LLMs, keeps only one transformer layer on the GPU, offloading the rest to CPU memory. Despite its innovations, L2L struggles with the memory demands of optimizer states that require significantly more space than parameters when using a mixed-precision ADAM optimizer. ZeRO-Offload~\cite{ZeRO-Offload} advances this approach by offloading gradients and optimizer states to the CPU, enabling the training of 13-billion parameter models on a single V100 GPU. Nonetheless, its static strategy does not account for the actual memory needs and capacities, resulting in inefficient GPU usage and increased workload. PatrickStar~\cite{patrickstar} improves resource allocation by dynamically managing model data between CPU and GPU, but its performance is hindered by the lack of overlapping between GPU computing, data transfer, and CPU processing.

This paper focuses on GPU memory-saving training to make LLM training more accessible.
Existing approaches lack flexibility and do not jointly consider activation checkpointing, offloading, and hybrid optimizer strategies.
Furthermore, the possibility of operator scheduling remains unexplored in LLM heterogenous training, thus leaving room for significant performance improvements.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Fig/overview.pdf}
    \caption{The overview of AutoHete.}
    \label{figmethod}
\end{figure*}