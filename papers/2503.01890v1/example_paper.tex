%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{AutoHete: An Automatic and Efficient Heterogeneous Training System for LLMs}

\begin{document}

\twocolumn[
\icmltitle{AutoHete: An Automatic and Efficient Heterogeneous Training System for LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zihao Zeng}{ntu}
\icmlauthor{Chubo Liu}{hnu}
\icmlauthor{Xin He}{astar}
\icmlauthor{Juan Hu}{nus}
\icmlauthor{Yong Jiang}{ali}
\icmlauthor{Fei Huang}{ali}
\icmlauthor{Kenli Li}{hnu}
\icmlauthor{Wei Yang Bryan Lim}{ntu}
\end{icmlauthorlist}

\icmlaffiliation{ntu}{Nanyang Technological University}
\icmlaffiliation{hnu}{Hunan University}
\icmlaffiliation{astar}{Agency for Science, Technology and Research}
\icmlaffiliation{nus}{National University of Singapore}
\icmlaffiliation{ali}{Alibaba Group}

\icmlcorrespondingauthor{Zihao Zeng}{zihao.zeng@ntu.edu.sg}
\icmlcorrespondingauthor{Wei Yang Bryan Lim}{bryan.limwy@ntu.edu.sg}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Transformer-based large language models (LLMs) have demonstrated exceptional capabilities in sequence modeling and text generation, with improvements scaling proportionally with model size. However, the limitations of GPU memory have restricted LLM training accessibility for many researchers. Existing heterogeneous training methods significantly expand the scale of trainable models but introduce substantial communication overheads and CPU workloads. In this work, we propose AutoHete, an automatic and efficient heterogeneous training system compatible with both single-GPU and multi-GPU environments.  AutoHete dynamically adjusts activation checkpointing, parameter offloading, and optimizer offloading based on the specific hardware configuration and LLM training needs.
Additionally, we design a priority-based scheduling mechanism that maximizes the overlap between operations across training iterations, enhancing throughput.
Compared to state-of-the-art heterogeneous training systems, AutoHete delivers a 1.32x$\sim$1.91x throughput improvement across various model sizes and training configurations.
\end{abstract}

\section{Introduction}

The Transformer architecture~\cite{Attention} has become the foundation of Large Language Models (LLMs) due to its excellent performance across various natural language processing (NLP) tasks \cite{exploring,bert,xlnet}. Scaling laws~\cite{scaling} suggests that model quality increases in model size, leading to the development of ever-larger models like OPT~\cite{opt}, BLOOM~\cite{bloom}, and the GPT series~\cite{gpt1,gpt2,gpt3,gpt4}.
However, GPU memory capacity has increased only 2.5-fold, from NVIDIA's 32 GB V100 to the 80 GB H100, a minor increase compared to the thousand-fold growth in model sizes over the past five years.
This ``GPU memory wall''~\cite{wall} restricts the accessibility of LLM training for most researchers.
Efforts to tackle this issue mainly involve parallelism and memory-saving strategies for LLM training.

Leveraging data~\cite{pytorch}, model~\cite{megatron}, pipeline~\cite{gpipe}, and hybrid parallelism~\cite{alpa}, we can distribute activations, parameters, and optimizer states across multiple GPUs, enabling scalable LLM training with improved efficiency.
However, this requires many GPUs, often too costly for most academic teams and small businesses, as training a 10-billion parameter model needs 16 NVIDIA V100 GPUs~\cite{ZeRO-Offload}.
% Conversely, memory-saving techniques like activation checkpointing~\cite{chen2016,checkmate,dtr,rockmate} and heterogeneous training~\cite{l2l,ZeRO-Offload,patrickstar,stronghold} allow significant scaling on a single GPU by using cheaper CPU memory to store model parameters and optimizer states.
Conversely, memory-saving techniques like activation checkpointing~\cite{dtr,rockmate} and heterogeneous training~\cite{ZeRO-Offload,patrickstar,stronghold} allow significant scaling on a single GPU.


% To democratize transformer-based LLM training, we focus on the exploration of the latter memory-saving training approach, which often exhibits compatibility with distributed training methods to further augment the model scale and accelerate training. However, while activation checkpointing reduces GPU memory consumption by discarding activation tensors, it does not sufficiently address the substantial memory demands of model parameters and optimizer states essential for LLM training. ZeRO-Offload~\cite{ZeRO-Offload} and PatrickStar~\cite{patrickstar} have emerged as prominent heterogeneous training approaches for transformer-based LLMs. ZeRO-Offload keeps all model parameters resident on the GPU while offloading all gradients, optimizer states, and corresponding parameter updates process to the CPU. It allows for 10x larger models on a single NVIDIA V100 GPU but incurs substantial communication and CPU workload. For example, our experiments showed that using ZeRO-Offload for 10B model training on a single 40GB A100 GPU, over 80\% of time was spent on communication operations and CPU computation. PatrickStar dynamically place model parameters and optimizer states across CPU and GPU memory. It improves GPU memory utilization but still suffers from performance limitations due to the absence of any overlaps between GPU computation, communication, and CPU-side parameter updates.

To democratize transformer-based LLM training, we focus on memory-saving approaches that complement distributed training methods to enhance model scalability and speed up training.
Although activation checkpointing reduces GPU memory usage by discarding and recomputing activation tensors, it fails to adequately address the extensive memory needs for model data, which include parameters, gradients, and optimizer states that are crucial for LLM training. 
% Prominent heterogeneous training methods like ZeRO-Offload~\cite{ZeRO-Offload} and PatrickStar~\cite{patrickstar} have been developed for these models.
Heterogeneous training presents a promising solution for LLMs by offloading model data to cheaper CPU memory.
ZeRO-Offload~\cite{ZeRO-Offload} enables training of models 10x larger on a single NVIDIA V100 GPU by offloading all optimizer states to the CPU, albeit at the cost of significant CPU workload and communication overhead. 
% For instance, our tests with ZeRO-Offload on a 40GB A100 GPU for a 10-billion parameter model showed that over 80\% of the time was consumed by communication and CPU processing.
% PatrickStar, on the other hand, dynamically distributes model parameters and optimizer states across CPU and GPU memory, improving memory utilization but facing performance bottlenecks due to the lack of overlap between GPU computations, communication, and CPU parameter updates.
PatrickStar~\cite{patrickstar} and StrongHold~\cite{stronghold} dynamically allocate model data across heterogeneous memory spaces, achieving a better trade-off.
However, existing heterogeneous training systems exhibit two key limitations.
First, they fail to comprehensively analyze the memory requirements and execution costs of all data involved in training, including activations, parameters, gradients, and optimizer states.
% Second, the overlap of asynchronous operations is restricted to a single training iteration, ignoring opportunities for overlap across iterations.
Second, significant resource idle periods arise between consecutive training iterations, caused by lagged gradient offloading and CPU optimizer updates. Prior works limit operations overlap to individual iterations, ignoring opportunities for overlap across iterations.

This paper introduces AutoHete, an automatic and efficient heterogeneous training system that dynamically integrates activation checkpointing, parameter offloading, and optimizer offloading.
Achieving both flexibility and efficiency poses two challenges for the system: (1) Given a specific LLM architecture, batch size, and GPU-CPU configuration, the system must decide which intermediate activations to recompute, which parameters to offload, and which optimizer states to place on the CPU.
(2) Another challenge lies in scheduling operations across different execution streams—including GPU computation, offloading, prefetching, and CPU optimizer updates—to maximize the overlap while preserving data dependencies.


To tackle these challenges, AutoHete involves a two-stage optimization strategy.
Firstly, we construct a cost model to accurately capture GPU peak memory consumption and execution time during heterogeneous training.
We then formalize the heterogeneous training optimization problem as an integer linear program, deriving the optimal plan in a few seconds.
Instead of uniform post-backward parameter updates, our cost model considers asynchronous layer-wise optimizer steps during the backward pass, opening up more granular scheduling opportunities for optimal operation overlap.
% The second stage introduces a priority-based scheduling strategy to maximize overlap between concurrent operations. 
% Specifically, our observations indicate a progressive increase in processing time for gradient generation (i.e., backward computation), offloading, and consumption (i.e., optimizer step).
% The scheduling strategy prioritizes gradients offloading and optimizer updates for earlier layers within the LLM, thereby initiating subsequent training iterations in advance.
The second stage introduces a priority-based scheduling strategy to minimize resource idle periods between successive training iterations. By prioritizing gradient offloading and CPU optimizer updates for earlier LLM layers, it facilitates the early initiation of subsequent training iterations.
This work makes the following key contributions:
\begin{itemize}
\item We formulate the heterogeneous training optimization problem as an integer linear program, which combines activation checkpointing, parameter offloading, and optimizer offloading.
\item We introduce a priority-based scheduling strategy, which achieves operators overlapping across training iterations without increasing peak GPU memory usage.
\item We evaluate AutoHete's  efficiency and adaptability in both single-GPU and multi-GPU environments, demonstrating a 1.32x$\sim$1.91x performance improvement over state-of-the-art heterogeneous training solutions.
\end{itemize}


\section{Background and Related Work}
\subsection{Transfomer-based LLM Training}

LLMs like OPT~\cite{opt} and GPT series~\cite{gpt1,gpt2,gpt3,gpt4} stack multiple transformer blocks for powerful sequence modeling and representation learning. While pre-training requires vast computational resources dominated by tech companies, fine-tuning on domain-specific tasks~\cite{llama2} enables broader participation but still faces significant memory challenges.

GPU memory during training is consumed by activations and model data (parameters, gradients, optimizer states).
Using mixed-precision training with ADAM optimizer~\cite{ZeRO-Offload}, activations and parameters are stored in FP16 while maintaining FP32 precision for optimizer states, which include momentum, variance, and FP32 parameter copies.
This requires 16\emph{M} bytes for a model with \emph{M} parameters, meaning a 2-billion parameter model would exceed a single V100 GPU's memory capacity even before accounting for activations, which scale with batch size and sequence length. Memory fragmentation further constrains effective utilization, making these resources extremely scarce.
In response, research efforts are increasingly focused on enhancing LLM training through multi-GPU parallelism and single-GPU memory optimization techniques.


\subsection{Parallel Training}

\textbf{Data Parallelism (DP)}~\cite{pytorch,unified,generic,sparcml} distributes inputs across GPUs while maintaining full model copies. However, DP alone cannot handle LLM training due to single-GPU memory constraints. ZeRO~\cite{zero} addresses this by partitioning model states across GPUs, using efficient reduce-scatter and all-gather operations to minimize communication overhead.

% \textbf{Data Parallelism.} Data parallelism (DP)~\cite{towards,pytorch,unified,generic,sparcml,pipesgd} received a lot of attention to speed up traditional DNN training. It distributes inputs and activations across GPUs but retains a full model copy in each GPU. However, DP alone is infeasible for LLM training, as the model data of an LLM typically exceeds the memory capacity of a single GPU. ZeRO~\cite{zero} tackles this limitation by further partitioning optimizer states, gradients, and parameters across GPUs. The all-reduce operation is decomposed into reduce-scatter and all-gather operations before and after the optimizer update phase, effectively avoiding additional communication overhead.

\textbf{Model Parallelism (MP)} ~\cite{megatron,acctfm} divides model parameter across GPUs. Mesh-Tensorflow~\cite{mesh} enables flexible tensor partitioning, while Megatron-LM~\cite{megatron} optimizes communication through strategic row and column parallelism for transformer-based LLMs.

% \textbf{Model Parallelism.} Motivated by the usage of aggregated GPU memory to store model data, model parallelism (MP) is re-exploited~\cite{megatron,acctfm}. It divides the parameter matrices of each layer throughout the GPU cluster. Mesh-Tensorflow~\cite{mesh} provides a high-level interface to support tensor partitioning along any dimensions, where users can design custom MP policies. Megatron-LM~\cite{megatron} is the first tailored MP design for transformer-based LLMs, which mitigates communication requirements by crossover row-parallel and column-parallel in each module.

\textbf{Pipeline Parallelism (PP)} ~\cite{gpipe,pipedream} segments models into stages, processing micro-batches in a pipelined manner. Efficient scheduling strategies~\cite{dapple,chimera,AdaPipe} are crucial for balanced GPU utilization and throughput.

% \textbf{Pipeline Parallelism.} Pipeline parallelism (PP)~\cite{gpipe,pipedream} partitions the layer sequence of the model into distinct stages, with activations divided into multiple micro-batch and processed in a pipelined manner across these stages. A well-crafted placement and scheduling strategy~\cite{dapple,chimera} is crucial to facilitate training throughput and load-balanced memory utilization among the different GPUs.


\textbf{Hybrid Parallelism}~\cite{alpa,yuliang,megascale}, combining DP, MP, and PP, has also been developed to achieve optimal distributed training efficiency.
Nevertheless, these parallelism solutions necessitate sufficient aggregated GPU memory to accommodate model data, presenting a significant affordability challenge.


\subsection{Memory-Saving Training}

% \textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations at checkpoints while trading additional computation.
\textbf{Activation Checkpointing} reduces memory usage by retaining only essential activations as checkpoints while recomputing discarded activations from these checkpoints during the backward pass.
A typical approach segments an \emph{N}-layer model into $\sqrt{N}$ parts~\cite{chen2016}. Various strategies have been proposed, from optimal but computationally intensive solutions~\cite{checkmate} to faster but sub-optimal approaches~\cite{dtr,rockmate}, each balancing memory savings against recomputation costs.


% \textbf{Activation Checkpointing.} Activation checkpointing trades additional recomputation for memory savings by segmenting an \emph{N} layer model into $\sqrt{N}$ parts, retaining only the input activations at each checkpoint. Due to its simplicity and effectiveness, this method has been widely adopted in various deep learning toolkits. The choice of checkpointing strategies varies significantly, influenced by different computational graph structures and the balance between recomputation costs and memory savings. Solutions range from those requiring extensive search time but yielding optimal results~\cite{checkmate}, to quicker methods that produce sub-optimal sequences~\cite{combination,dtr,rockmate}.

% The activation checkpointing technique introduces additional recomputing in exchange for memory savings on activations during training. Chen et al.~\cite{chen2016} divided the \emph{N} layer model into $\sqrt{N}$ segments, with each segment retaining only the input activations as a checkpoint. This method has been integrated into many deep learning toolkits due to its simplicity and effectiveness. In fact, the strategy space of activation checkpointing is exponential due to varying recomputing overheads and memory savings of each operator. Considering different computing graph structures of models, different solvers for activation checkpointing are exploited. One requires expensive search time but obtains optimality~\cite{checkmate}, and others solve faster but obtain sub-optimal recomputing sequences~\cite{combination}~\cite{dtr}~\cite{rockmate}.




\textbf{Offloading} leverages CPU memory to supplement GPU memory during training. While initially developed for CNNs to manage activations~\cite{vdnn,swapadvisor,capuchin}, LLM training shifts focus to handling model data. L2L~\cite{l2l} pioneered heterogeneous LLM training by keeping only one transformer layer on GPU. ZeRO-Offload~\cite{ZeRO-Offload} enables training of 13-billion parameter models on a single V100 GPU by offloading gradients and optimizer states to CPU, though its static approach limits efficiency. 
% PatrickStar~\cite{patrickstar} introduces dynamic resource management but lacks effective overlap between computation and data transfer.
PatrickStar~\cite{patrickstar} and StrongHold~\cite{stronghold} introduced dynamic memory allocation methods for model data using synchronous and asynchronous execution, respectively. However, the absence of comprehensive analyses of memory usage and execution overhead during training results in suboptimal performance.

% \textbf{Offloading.} The offloading technique uses CPU memory to supplement GPU memory for heterogeneous training, initially researched extensively for convolutional neural networks where activations are the main memory consumers~\cite{vdnn}\cite{swapadvisor}\cite{capuchin}. In LLM training, the emphasis shifts to managing model data, including parameters, gradients, and optimizer states. L2L~\cite{l2l}, the first heterogeneous training approach for LLMs, keeps only one transformer layer on the GPU, offloading the rest to CPU memory. Despite its innovations, L2L struggles with the memory demands of optimizer states that require significantly more space than parameters when using a mixed-precision ADAM optimizer. ZeRO-Offload~\cite{ZeRO-Offload} advances this approach by offloading gradients and optimizer states to the CPU, enabling the training of 13-billion parameter models on a single V100 GPU. Nonetheless, its static strategy does not account for the actual memory needs and capacities, resulting in inefficient GPU usage and increased workload. PatrickStar~\cite{patrickstar} improves resource allocation by dynamically managing model data between CPU and GPU, but its performance is hindered by the lack of overlapping between GPU computing, data transfer, and CPU processing.

This paper focuses on GPU memory-saving training to make LLM training more accessible.
Existing approaches lack flexibility and do not jointly consider activation checkpointing, offloading, and hybrid optimizer strategies.
Furthermore, the possibility of operator scheduling remains unexplored in LLM heterogenous training, thus leaving room for significant performance improvements.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Fig/overview.pdf}
    \caption{The overview of AutoHete.}
    \label{figmethod}
\end{figure*}

\section{AutoHete}\label{sec3}
% \textcolor{red}{Fig. \ref{figmethod} shows the overview of AutoHete.
% Given the LLM and hardware platform configuration, AutoHete automatically identifies an efficient execution strategy combining activation checkpointing, parameters offloading, and optimizer offloading.
% AutoHete achieves this by formulating heterogeneous training optimization problems as integer linear programming.
% Considering the disparities in execution times among concurrent operators, a priority-based scheduling mechanism is implemented to maximize overlaps between GPU computing, prefetching, offloading, and CPU workloads.
% Notably, our heterogeneous training strategy strictly maintains the data dependency of all operators so without affecting training convergence.}
Fig. \ref{figmethod} shows the overview of AutoHete.
Given the LLM and hardware configuration, we construct a Profiler module to capture the memory consumption and execution time of all operators.
The collected information is then fed into the Solver module, which formulates the heterogeneous training optimization problem as integer linear programming to derive optimal strategies for activation checkpointing, parameter offloading, and optimizer offloading.
% The Solver generates optimal strategies for activation checkpointing, parameter offloading, and optimizer offloading. 
A priority-based scheduler further orchestrates GPU computation, CPU-GPU communication, and CPU optimizer updates to maximize overlap among these operations.
Finally, the resulting execution strategy is configured into the runtime to enable efficient heterogeneous training.
Notably, our heterogeneous training system strictly maintains the data dependency of all operators so without affecting training convergence.


\subsection{Computation and Memory Profiler}
To guide the exploration of heterogeneous training strategies, we first need to collect the execution time and memory footprints of all operators.
A computational graph is constructed by the \emph{torch.fx}~\cite{reed2022torchfx} module, which utilizes symbolic execution to capture operations invoked on a given input throughout the program.
Based on the symbolic tracing mechanism, we use fake input tensors containing solely meta-data without actual values to infer the output shape of each node within the computational graph, consequently revealing the computational demands and memory allocation of each operator.

Instead of focusing on individual nodes within the computational graph, we simplify the decision-making process by considering the entire transformer block as the fundamental planning unit.
Within a transformer block, there are three binary switches to consider whether to delete and recompute all intermediate activations, offload all parameters, and offload all corresponding optimizer states.
This coarse-grained decision-making provides three main advantages.
Firstly, the policy search space is significantly reduced, as a transformer block encompasses hundreds of nodes.
Secondly, the CPU-GPU data transfer operations that aggregate multiple tensors make more efficient utilization of PCIe bandwidth.
Thirdly, the coarse-grained execution time evaluation is obviously more accurate than the evaluation on individual nodes.
Therefore, we partition the computational graph, where each partition represents a transformer block.
The embedding layer is treated as a distinct partition.



We use $M_{a}$ and $M_{a}^{'}$ to denote the total activations and input activations of a transformer block, respectively.
Note that nodes involved in in-place operations are skipped, as they are not allocated new memory footprint for their output activations.
The number of parameters $M_{p}$ within the partition can be directly derived from the hidden dimension.
The forward and backward computation overheads ($t_{fp}$ and $t_{bp}$) of the partition are evaluated by directly executing the corresponding operators on the GPU.
Rather than executing on the original model, evaluation is conducted by a tiny model containing only one transformer block with identical configuration.
Similar profiling is employed to evaluate the cost of CPU optimizer updates $t_{optim}^{cpu}$, GPU optimizer updates $t_{optim}^{gpu}$, parameters prefetching $t_{h2d}$, and parameter or gradients offloading $t_{d2h}$ for the transformer block.




\subsection{Solver}
\subsubsection{Analysis}
Before solving the heterogeneous training strategy, we analyze the GPU memory savings and the extra overheads incurred by diverse planning. For an LLM with $L$ transformer blocks, $F_{i}$ and $B_{i}$ represent the forward and backward execution of the $i$-th ($i\in \{1, ..., L\}$) transformer block, respectively.
The strategy space contains three binary decision sequences: $C, P, O \in \{0, 1\}^{L}$, 
where $C[i]$, $P[i]$, and $O[i]$ represent whether to employ activation checkpointing, parameters offloading, and optimizer states offloading, respectively, for the $i$-th transformer block.


If checkpointing is applied to the $i$-th transformer block, it will reduce $2*(M_{a}-M_{a}^{'})$ bytes GPU memory usage during the execution interval $[F_{i+1}, ..., F_{L}, B_{L}, ..., B_{i+1}]$, concurrently incurring an additional recomputation cost of $t_{fp}$.
Similarly, offloading parameters of the $i$-th transformer block frees GPU memory footprint by $2*M_{p}$ bytes\footnote{The multiplication factor of 2 stems from storing parameters and activations in FP16 format during forward and backward computation.} and adds extra CPU-GPU communication overhead of $t_{d2h} + t_{h2d}$ within the same execution interval.

\textbf{Insight 1: } 
\textit{For activation checkpointing and parameter offloading, prioritizing earlier transformer blocks is advantageous, as it enables GPU memory savings over a longer execution duration while incurring similar costs to later blocks. This observation can be formalized as $C[i] \geq C[j]$ and $P[i] \geq P[j]$, $\forall i, j \in \{1, ..., L\}, i < j$.}


Placing the optimizer states of the $i$-th transformer block on the CPU involves 1) prefetching FP16 parameters before $F_{i}$, 2) offloading FP16 gradients after $B_{i}$, and 3) updating CPU FP32 parameters.
It reduces GPU memory footprint by $12*M_{p}$ bytes during the entire training iteration with a total cost of $t_{h2d} + t_{d2h} + t_{optim}^{cpu}$.
Consistent with ZeRO-Offload, we avoid moving the optimizer states between the GPU and CPU since it will introduce excessive communication costs, roughly $6*(t_{h2d} + t_{d2h})$.
Despite the equivalent cost, GPU memory savings, and the duration of memory savings, disparities exist in optimizer offloading between various transformer blocks.
Considering offloading optimizer states for the $i$-th block, its FP16 parameter prefetching can be overlapped with earlier forward computations $[F_{1}, ..., F_{i-1}]$, while its FP16 gradient offloading and CPU parameter updates can be overlapped with later backward computations $[B_{i-1}, ..., B_{1}]$.
Appendix \ref{allc_example} provides an example of the memory allocation process.

\textbf{Insight 2:}
\textit{For optimizer offloading, the later transformer blocks should be prioritized to improve overlaps.
We formulate it as $O[i] \leq O[j], \forall i, j \in \{1, ..., L\}, i < j$.}


Based on the above analysis, the heterogeneous training strategy space can be reduced to three decision variables: $\hat{c}, \hat{p}, \hat{o} \in \{0, ..., L\}$, which represent the number of sequential transformer blocks for applying activation checkpointing, parameters offloading, and optimizer states offloading, respectively.
We further evaluate the peak GPU memory consumption and total execution time of a training iteration. 


\subsubsection{Cost Model and Strategy Search}
\textbf{Modeling peak memory footprint.}
GPU memory usage increases during the forward pass due to activation generation and parameter prefetching. Conversely, memory footprint decreases during the backward pass as activations, parameters, and gradients are deallocated.
Peak GPU memory allocation occurs at the transition between these two passes. We also employed a trick to optimize memory efficiency\footnote{If the optimizer states of the $i$-th transformer block reside on the GPU, its FP16 parameters are not allocated until $F_{i}$ or $B_{i}$ execution, temporarily converted through FP32 parameters, and immediately released after computation.
The overhead of precision conversion on the GPU is negligible.
In this context, offloading planning for FP16 parameters is unnecessary, which can be formulated as $\hat{p} \leq \hat{o}$.}.
Overall, given the GPU memory capacity $M_{gpu}$, we have:
\begin{align}
    & 2M_{a}^{'}*\hat{c} + 2M_{a}*(L-\hat{c}+1) + 2M_{p}*(L-\hat{p}+1) \nonumber \\
    & + 12M_{p}*(L-\hat{o}) + M_{gc} \nonumber \\
    & \leq M_{gpu},
\end{align}
where $M_{gc}$ is constant GPU storage requirements (e.g., activations) outside the transformer model.


The CPU memory is used to store offloaded model data, including optimizer states (FP32 momentum, FP32 variance, and FP32 parameters), FP16 parameters, and FP16 gradients. 
Notably, FP16 parameters and FP16 gradients of a transformer block can share the same memory space since they do not coexist.
We define $M_{cc}$ as the CPU storage requirements beyond the transformer model (e.g., the embedding layer). 
Thus, given the CPU memory limitation $M_{cpu}$, we have:
\begin{equation}
    14M_{p}*\hat{o} + M_{cc} \leq M_{cpu}.
\end{equation}
\textbf{Modeling execution time.}
In the forward phase, GPU computations and  CPU-GPU communication for FP16 parameters can proceed asynchronously via CUDA Stream, with computation and prefetch streams overlapping to enhance efficiency. 
Synchronization operations are inserted into the computation stream to ensure forward computations utilize up-to-date FP16 parameters. FP16 parameters can be released immediately after computations are completed, without data transfer, as the CPU retains a copy. 
Thus, the critical path for forward execution is either parameter prefetching or the GPU computation stream, represented as:
\begin{equation}
    T_{fwd} = \max \left(t_{fp}*L,\ \  t_{h2d}*\hat{o}+t_{fp}\right).
\end{equation}
In the backward phase, operations such as gradients computation, activations recomputation, parameters prefetching, gradients offloading, and optimizer updates are conducted.
Unlike the forward pass, the early backward pass is located at the peak duration of GPU memory usage.
Premature parameter prefetching in this context carries the risk of out of GPU memory.
Therefore, we consider a relatively conservative execution, prefetching parameters only one transformer block in advance.
There are two cases for evaluating the synchronization overhead in the backward computation stream, i.e., whether parameter prefetching overlaps also with recomputation. 
We introduce $\hat{v}$ to represent the number of transformer blocks applying both activation checkpointing and parameter offloading, which can be derived by $\hat{c}$, $\hat{p}$, and $\hat{o}$.
The synchronization overhead is denoted as $t_{sync}$, then:
\begin{align}
    t_{sync} = & \ \hat{v}*\max\left(t_{h2d}-t_{fp}-t_{bp}, \ 0\right) \nonumber \\
    & + (\hat{p}-\hat{v})*\max\left(t_{h2d}-t_{bp}, \ 0\right).
\end{align}
Gradient offloading and optimizer execution are immediate, occurring as soon as gradients are generated or offloaded, without waiting for subsequent backward execution\footnote{For flexibility, we define $n$ optimizers, each responsible for performing parameter updates of individual transformer blocks.}. 
The critical path during the backward phase lies in either the CPU workflow or the backward computation stream that may be delayed by parameter prefetching.
Therefore, we have:
\begin{align}
    T_{bwd} = \max \left( \right. & t_{bp}*L + t_{fp}*\hat{c} + t_{optim}^{gpu}*(L-\hat{o}) + t_{sync}, \nonumber \\
    & \left. t_{bp} + t_{d2h} + t_{optim}^{cpu}*\hat{o}\right).
\end{align}
\textbf{Strategy Search.}
With the heterogeneous memory space constraints, finding the optimal execution strategy $s = (\hat{c}, \hat{p}, \hat{o})$ becomes an integer linear programming (ILP) problem:
\begin{equation}
    \mathop{\min}_{s} \ \ T_{fwd} + T_{bwd},\ \  s.t. (1), (2).
\end{equation}
This ILP problem can be swiftly solved by commodity solvers, as it involves only three integer variables.
Given a feasible solution from ILP, we fine-tune the parameter prefetching in the backward phase to reduce the synchronization overhead. 
A simulator of heterogeneous training is constructed to capture dynamic GPU memory allocation.
We will advance a parameter prefetching operation if enough GPU memory is available before it.
% In practice, a slight discrepancy exists between the cost model and runtime, sometimes causing a solution from ILP to run out of GPU memory.
% We apply an error factor multiplied by the GPU memory capacity to impose further constraints.
% Based on our experimental evaluation, setting this factor to 0.93 is sufficient to cover all cases.



\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{\textwidth} % 
        \centering
        \includegraphics[width=0.85\textwidth]{Fig/ps_a.pdf} % 

        \caption{Without priority-based scheduling.}
        \label{sub1}
    \end{subfigure}
    \hfill % 水平间距
    \begin{subfigure}[b]{\textwidth} % 
        \centering
        \includegraphics[width=0.85\textwidth]{Fig/ps_b.pdf} % 

        \caption{With priority-based scheduling.}
        \label{sub2}
    \end{subfigure}
    \caption{Execution examples of AutoHete for an LLM comprising 5 transformer blocks, with and without priority-based scheduling strategy. Here, $\hat{c}=3$, $\hat{p}=1$, and $\hat{o}=5$, indicating blocks 1-3 adopts activation checkpointing, block 1 offloads parameters, and blocks 1-5 offload optimizer states. $pq_{d2h}$ and $pq_{opt}$ denote priority queues that manage the order of blocks for gradient offloading and CPU optimizer updates, respectively. The queue states before each operation are shown in the dashed boxes.}
    \label{priority}
\end{figure*}


\subsection{Priority-based Scheduler}
While ILP offers efficient solutions for single training iterations, there remains potential to enhance efficiency by overlapping operations across iterations.
Fig. \ref{priority} illustrates the execution of AutoHete, spanning from the backward phase of the previous training iteration to the forward phase of the subsequent iteration.
% The four parallel execution streams (from top to bottom) represent the GPU computation stream, CPU-to-GPU data transfer stream, GPU-to-CPU data transfer stream, and CPU computation stream, respectively.
As shown in Fig. \ref{sub1}, there are notable discrepancies in the execution speeds across streams within a training iteration: the GPU computation finishes the fastest, followed by the GPU-to-CPU communication, and finally, the CPU workflow.
% This phenomenon is particularly pronounced when training larger models, as it necessitates offloading a substantial volume of optimizer states to the CPU.
% However, the initial computation $F_{1}$ for the next iteration must await the completion of the CPU optimizer updates from the previous iteration, which naturally finishes last, leaving a significant gap.
This gap is more pronounced in larger models due to extensive offloading of optimizer states, creating a bottleneck where the next iteration's initial computation $F_{1}$ must wait for the CPU optimizer updates of block 1, which naturally finishes last.
Analogously, for operators that start the forward pass earlier, their dependent CPU optimizer updates are completed later.

\textbf{Insight 3: } 
\textit{
% The barriers between iterations can be eliminated without breaking operator dependencies.
By prioritizing the execution of gradient offloading and CPU optimizer updates for earlier blocks, the idle period can be significantly reduced.}


We introduce a priority-based scheduling mechanism.
% The indices of the transformer blocks indicate their priority, with lower values signifying higher priority.
Higher priority is assigned to earlier blocks in the model.
Specifically, we manage two dynamic priority queues ($pq_{d2h}$ and $pq_{opt}$) to determine the execution order for gradient offloading and CPU parameter updates.
Initially, both queues are empty, and the $L$-th block is naturally added as the first element. 
Along the backward pass, we track the execution status of the gradient computation and offloading for each transformer block. 
It is achieved by inserting CUDA Events.


Upon the backward computation of a transformer block is completed, we append its index to the priority queue $pq_{d2h}$ of gradient offloading.
Instead of offloading gradients in the backward order, we perform the next by dequeue from $pq_{d2h}$ when an offloading operation concludes. 
If the queues are empty, we must await the completion of the latest gradient computation.
Similarly, we add the indices of completed gradient offloading to the optimizer execution queue $pq_{opt}$. 
Rather than following the gradient offloading sequence, we continually dequeue the transformer block indices from $pq_{opt}$ to conduct CPU optimizer updates.
Notable, priority-based scheduling neither diminishes nor extends the critical path of original backward pass.
It overlaps the critical path of the backward and forward pass by enabling parameter prefetching and forward computation to commence earlier.



In the subsequent iteration, we schedule parameter prefetching in the forward computation order. 
Necessary synchronization operations are employed to ensure data dependencies.
Parameter prefetching can be launched once the CPU optimizer updates are complete.
Ideally, the early initiation of parameter prefetching does not augment the peak GPU memory consumption, as both the latter span of the backward phase and the initial span of the forward phase exhibit lower GPU memory footprints.
Furthermore, they demonstrate complementary GPU memory allocation trends, wherein memory utilization gradually diminishes along the backward propagation while escalating during the forward pass.
To strictly avoid running out of GPU memory, we evaluate the GPU memory footprint before scheduling parameter prefetching.
If numerous gradients await offloading in the queue, which results in not enough GPU memory available, we defer parameter prefetching by one step.

\subsection{Implementation}
We implemented the AutoHete system prototype based on PyTorch. AutoHete is user-friendly and designed as a wrapper without modifying the original model definition. 
% With just a few lines of code, it enables efficient heterogeneous training. 
AutoHete leverages \emph{torch.fx} module to capture the computational graph of the model.
The system implements activation recomputation by annotating each node in the graph with checkpoint information.
New nodes are inserted after each transformer block, inheriting from the \emph{torch.autograd.Function} class with customized forward and backward functions to orchestrate parameter prefetching, gradient offloading, and optimizer updates. 
% The parameters within each block are merged into a single chunk to optimize PCIe bandwidth utilization. 
Additionally, AutoHete employs a parameter pre-allocation mechanism, mapping parameters to GPU memory in advance to minimize memory fragmentation.
To maintain data dependencies, \emph{torch.cuda.Event} is used to record and synchronize operations.
Finally, the modified computational graph is recompiled to enforce the planned optimizations at runtime.



\section{Evaluation}
\subsection{Evaluation Methodology}\label{expset}

\textbf{Testbed.}
We evaluate AutoHete on the National Supercomputing Centre Singapore (NSCC), applying for a node configured with 4 NVIDIA 40GB A100 GPU, AMD EPYC 7713 64-Core CPU, and 440GB DDR4 RAM.
The GPUs are connected with NVLink, and data transfer between the GPU and CPU occurs over the PCIe 4.0 interface.


\textbf{Workloads.}
Consistent with prior research, we utilized GPT-like models for performance evaluation.
Models with varying scales are obtained by adjusting the hidden dimension and the number of transformer blocks, as shown in Table \ref{m_config}.
The sequence length is 1024 for all cases.


\begin{table}
\renewcommand{\tabcolsep}{1.2mm} % enlarge column spacing
  \caption{Model configuration in evaluation.}
  \label{m_config}
  \centering
  \begin{tabular}{ccc}
    \toprule
    \# Params (billion)     & \# Layers     & Hidden Size \\
    \midrule
    2    & 42    & 2048 \\
    8, 10, 12, 14, 16   & 20, 25, 30, 35, 40    & 6144   \\
    4, 6   & 21, 32    & 4096  \\
    18, 20, 22   & 23, 26, 29    & 8192      \\
    % 10   & 34    & 5120        \\
    \bottomrule
  \end{tabular}
\end{table}



\textbf{Baselines.}
We compare the effectiveness of AutoHete with three advanced heterogeneous training solutions for LLMs. 
ZeRO-Offload~\cite{ZeRO-Offload} statically keeps all parameters on the GPU but offloads gradients and optimizers to the CPU. 
% PatrickStar~\cite{patrickstar} dynamically determines the placement of parameters and optimizers across heterogeneous memory spaces. 
PatrickStar~\cite{patrickstar} performs on-demand placement of model data across heterogeneous memory spaces during runtime.
StrongHold~\cite{stronghold} maintains a dynamic working window on the GPU to store model data, which moves along the computation direction.
Optimal performance for ZeRO-Offload, PatrickStar, and StrongHold was evaluated with and without full activation checkpointing.
For distributed training, we utilized the ZeRO-3 stage to partition model data.

\subsection{Experimental Results}\label{exp}
We first evaluate the overall performance of AutoHete, ZeRO-Offload, PatrickStar, and StrongHold across various model scales.
The performance of AutoHete without the priority-based scheduling strategy (w/o PS) is also evaluated to promote ablation studies.
The global batch size defaults to 8 for both single-GPU and multi-GPU environments.
Subsequently, we further analyze the performance across varying batch sizes and GPU memory budgets to demonstrate the flexibility of AutoHete.


\begin{figure}[!t]
\centering

\begin{subfigure}[b]{0.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/perf_single.png}
  \caption{Single GPU}
  \label{gpu1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/perf_four.png}
  \caption{Four GPUs}
  \label{gpu4}

\end{subfigure}
\caption{The training throughput of ZeRO-Offload, PatrickStar, StrongHold, and AutoHete across various model sizes on single and multiple GPUs.}
\label{resultgpu}
\end{figure}



\textbf{Single GPU.}
Fig. \ref{gpu1} shows the training throughput (TFLOPS) of AutoHete, ZeRO-Offload, PatrickStar, and StrongHold on a single GPU.
Due to keeping all FP16 parameters on GPU, ZeRO-Offload runs out of GPU memory when the model size exceeds 12B parameters. 
AutoHete, PatrickStar, and StrongHold enable the training of larger models by further offloading FP16 parameters to the CPU.

Compared to ZeRO-Offload, we achieved 1.85x on average (up to 2.08x) performance improvement.
GPU memory remained underutilized when training models with 2 billion to 8 billion parameters using ZeRO-Offload.
Rather than offloading the entire optimizer states, AutoHete offloads only the requisite portion based on actual memory demands and available GPU memory, directly reducing the overheads from parameter prefetching, gradient offloading, and CPU optimizer execution.
The performance gap is more pronounced for smaller models.
Taking a 4B model as an example, AutoHete offloaded the optimizer states of only the last 13 out of 21 transformer blocks to the CPU.


AutoHete achieves 1.63x on average (up to 1.91x) training throughput over PatrickStar.
Although PatrickStar is also memory-aware, its performance remains constrained by communication overheads and CPU workloads due to synchronous execution. 
% Our cost model accounts for asynchronous execution between operations and further maximizes overlaps through a priority-based scheduling strategy.
Our cost model accounts for asynchronous execution between operators and focuses on maximizing overlaps.
Performance gains are relatively modest for smaller models, where computational overhead dominates.
The throughput improvements become more pronounced with relatively larger models.
Despite considerable overheads from parameter prefetching, gradient offloading, and CPU workloads, the highly overlapped execution across the four streams effectively counteracts these costs.

StrongHold leverages data prefetching to overlap data movement with GPU computation, outperforming PatrickStar in training throughput.
However, its performance remains suboptimal due to the lack of integrated consideration for the memory requirements and execution overhead associated with activations, parameters, and optimizer states.
Moreover, operators overlapping in StrongHold is limited to a single training iteration, whereas AutoHete's priority-based scheduling strategy enables overlapping across training iterations.
AutoHete achieves an average of 1.32x (up to 1.41x) throughput improvement compared to StrongHold.


The priority-based scheduling (PS) mechanism contributes to a throughput improvement of 1.16x on average (up to 1.24x) for AutoHete. 
For 2B and 4B models, the performance of AutoHete with and without PS is comparable. 
This is because only the optimizer states of the initial few transformer blocks in the backward pass are offloaded to the CPU, and the introduced overhead is largely overlapped with subsequent backward computations even without PS. 
When more optimizers are offloaded to the CPU, CPU workflow becomes significantly behind the GPU computation stream.
PS fills this gap by achieving overlaps across training iterations, i.e.,  scheduling parameter prefetching and forward computation of the next iteration in advance.



\textbf{Multi-GPUs.}
% Fig. \ref{gpu4} shows the multi-GPU (4 GPUs) training performance comparison of AutoHete, ZeRO-Offload, PatrickStar, and StrongHold.
Fig. \ref{gpu4} shows the training performance comparison in a multi-GPU (4 GPUs) environment.
ZeRO-Offload runs out of CPU memory instead of GPU memory when the model scale reaches 22 billion parameters.
The CPU memory capacity is insufficient to accommodate all optimizer states and gradients.
AutoHete, PatrickStar, and StrongHold leverage the aggregated GPU memory to store a portion of the optimizer states.

AutoHete achieves an average of 2.23x (up to 2.67x) performance improvement compared to ZeRO-Offload.
Similarly, more performance gains are observed for smaller models due to reduced communication cost and CPU workload. 
AutoHete demonstrates performance improvement of 1.65x on average (up to 1.79x) over PatrickStar and 1.44x on average (up to 1.52x) over StrongHold.
Although distributed training requires additional inter-GPU communication overheads (i.e., all-gather and reduce-scatter operations), these overheads are effectively overlapped in AutoHete.
Furthermore, the priority scheduling mechanism provides AutoHete with an average of 1.21x (up to 1.3x) performance benefit.


\begin{figure}[!htbp]
\begin{minipage}[t]{0.5\textwidth}
\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/results_bs_6b.png}
  \caption{The 6B model}
  \label{fig:sub1a}
\end{subfigure}
\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/results_bs_10b.png}
  \caption{The 10B model} 
  \label{fig:sub1b}
\end{subfigure}
\caption{Performance on various batch sizes.}
\label{bs}
\end{minipage}  

\hfill
\begin{minipage}[t]{0.5\textwidth}
\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/results_mem_4b.png}
  \caption{The 4B model} 
  \label{fig:sub2a}
\end{subfigure}
\begin{subfigure}[b]{0.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/results_mem_6b.png} 
  \caption{The 6B model} 
  \label{fig:sub2b}
\end{subfigure}
\caption{Performance on different GPU memory budgets.}
\label{gpumem}
\end{minipage}
\end{figure}


\subsubsection{Further Analysis}
\textbf{Impact of batch size.}
Fig. \ref{bs} shows the throughput variations across different batch sizes for training the 6B and 10B models on a single GPU.
Larger batch sizes incur higher computational load and activation storage demands.
The throughput of AutoHete, ZeRO-Offload, PatrickStar, and StrongHold increases with larger batch sizes due to the continuously rising proportion of computation overhead.
ZeRO-Offload runs out of GPU memory when the batch size exceeds 24 for the 6B model and 12 for the 10B model.

AutoHete initially exhibits a more rapid throughput increase, as the additional computation from larger batch sizes does not fully translate into an increase in total execution time.
The backward computation overhead, including recomputation cost, is still completely hidden by communication and CPU optimizer execution.
However, continuously increasing the batch size forces AutoHete to place more model data on the CPU, and the additional communication and CPU workload leads to a slowdown in throughput growth.


\textbf{Different GPU memory budgets.}
Fig. \ref{gpumem} shows performance under varying GPU memory budgets for the 4B and 6B models.  
The throughput of AutoHete, PatrickStar, and StrongHold increases with a larger GPU memory budget, while ZeRO-Offload remains nearly constant due to its static placement strategy.
Additionally, ZeRO-Offload fails to sustain 4B model training when the GPU memory budget is less than 16GB, and training the 6B model requires a minimum of 24GB of GPU memory.
It is worth noting that AutoHete maintains high performance while reducing hardware costs.
With merely 12GB of GPU memory consumption, AutoHete achieves higher training throughput than StrongHold, PatrickStar, and ZeRO-Offload running under the 40GB GPU memory budget.



\section{Conclusion}
This paper presents AutoHete, an automatic and efficient heterogeneous training system for LLMs. 
It automatically identifies an effective asynchronous execution strategy that combines activation checkpointing, parameter offloading, and optimizer offloading.
A priority-based scheduling strategy is introduced to facilitate operation overlaps across training iterations.
AutoHete significantly outperforms state-of-the-art heterogeneous training systems, enhancing the accessibility of LLM training.







% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.




\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Memory Allocation Analysis}\label{allc_example}
\begin{figure*}[!htbp]
    \centering
    \begin{subfigure}[b]{\textwidth} % 
        \centering
        \includegraphics[width=0.9\textwidth]{Fig/appendix_a.pdf} % 

        \caption{The memory allocation process for activations, where only activation checkpointing is applied to the first two transformer blocks.}
        \label{appendix_a}
    \end{subfigure}
    \hfill % 水平间距
    \begin{subfigure}[b]{\textwidth} % 
        \centering
        \includegraphics[width=0.9\textwidth]{Fig/appendix_b.pdf} % 

        \caption{The memory allocation process for parameters, where only parameter offloading is applied to the first two transformer blocks.}
        \label{appendix_b}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth} % 
        \centering
        \includegraphics[width=0.9\textwidth]{Fig/appendix_c.pdf} % 

        \caption{The memory allocation process for parameters, gradients, and optimizer states, where only optimizer offloading is applied to the last two transformer blocks.}
        \label{appendix_c}
    \end{subfigure}
    \caption{Example of the memory allocation process during a training iteration for an LLM comprising 5 transformer blocks.}
    \label{appendix}
\end{figure*}

Fig. \ref{appendix} shows an example of the memory allocation process during a training iteration, individually applying activation checkpointing, parameter offloading, and optimizer offloading. 
Here, solid boxes represent executed operations, while hollow boxes indicate memory allocation along the execution flow.

Activations are generated during the forward pass and consumed during the backward pass.
As shown in Fig. \ref{appendix_a}, applying activation checkpointing to block 1 reduces $2*(M_{a}-M_{a}^{'})$ bytes GPU memory usage during the execution of $[F_2, F_3, F_4, F_5, B_5, B_4, B_3, B_2]$.
Similarly, for block 2, GPU memory savings occur during $[F_3, F_4, F_5, B_5, B_4, B_3]$.
Activation checkpointing should be prioritized for earlier blocks.
The same principle applies to parameter offloading, as observed in Fig. \ref{appendix_b}, where earlier offloading results in a longer duration of memory savings.
Due to asynchronous execution, GPU memory allocated for parameters is released only after offloading completes, while memory allocation occurs when the prefetching kernel is launched.

The memory footprint of optimizer states remains constant during execution, as they are accessed only once for optimizer updates and do not require CPU-GPU transfers.
Optimizer offloading affects parameters and gradients memory allocation due to data dependencies.
Blocks employing optimizer offloading prefetch the latest parameters from the CPU for forward and backward computation on the GPU, while gradients from the backward pass are offloaded to the CPU for optimizer updates.
From Fig. \ref{appendix_c}, it can be inferred that optimizer offloading should prioritize later blocks, as this allows for greater overlap of parameter prefetching, gradient offloading, and CPU optimizer updates.

GPU memory allocation for activations and parameters increases during the forward pass and decreases during the backward pass, with peak memory usage occurring at the transition between the two.



\subsection{Supplemental Experiments}




\begin{figure}[!htbp]
% \vspace{-0.3cm}
\centering

\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/appendix_perf_single_bs_4.png}
  \caption{Single GPU}
  \label{app2_gpu1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/appendix_perf_four_bs_4.png}
  \caption{Four GPUs}
  \label{app2_gpu4}

\end{subfigure}
% \vspace{-0.1cm}
\caption{The training throughput of ZeRO-Offload, PatrickStar, StrongHold, and AutoHete with a global batch size of 4.}
\label{app2_resultgpu}
% \vspace{-0.3cm}
\end{figure}

\begin{figure}[!htbp]
% \vspace{-0.3cm}
\centering

\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/appendix_perf_single_bs_16.png}
  \caption{Single GPU}
  \label{app1_gpu1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{Fig/appendix_perf_four_bs_16.png}
  \caption{Four GPUs}
  \label{app1_gpu4}

\end{subfigure}
\caption{The training throughput of ZeRO-Offload, PatrickStar, StrongHold, and AutoHete with a global batch size of 16.}
\label{app1_resultgpu}
% \vspace{-0.3cm}
\end{figure}

Fig. \ref{app2_resultgpu} and Fig. \ref{app1_resultgpu} depict the training throughput for AutoHete, ZeRO-Offload, PatrickStar, and StrongHold with global batch sizes of 4 and 16, respectively.
Compared to ZeRO-Offload, AutoHete exhibits a 1.39x$\sim$3.1x throughput improvement. The performance gap between AutoHete and ZeRO-Offload is more pronounced for smaller models and batch sizes, where ZeRO-Offload's overhead is dominated by CPU workload and communication.
AutoHete automatically places more optimizer states on the GPU, avoiding this overhead.
% AutoHete demonstrates a 1.08x$\sim$1.91x performance enhancement over PatrickStar, with larger disparities observed for relatively larger models due to judicious operation overlapping in AutoHete runtime.
AutoHete demonstrates a performance enhancement of 1.08x$\sim$1.91x over PatrickStar and 1.04x$\sim$1.48x over StrongHold due to judicious operation overlap in AutoHete runtime.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
