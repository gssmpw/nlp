@inproceedings{AdaPipe,
  title={AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning},
  author={Sun, Zhenbo and Cao, Huanqi and Wang, Yuanwei and Feng, Guanyu and Chen, Shengqi and Wang, Haojie and Chen, Wenguang},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={86--100},
  year={2024}
}

@inproceedings{ZeRO-Offload,
  title={Zero-offload: Democratizing billion-scale model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={Proceedings of the  USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}

@article{acctfm,
  title={Acctfm: An effective intra-layer model parallelization strategy for training large-scale transformer-based models},
  author={Zeng, Zihao and Liu, Chubo and Tang, Zhuo and Li, Kenli and Li, Keqin},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={12},
  pages={4326--4338},
  year={2022},
  publisher={IEEE}
}

@inproceedings{alpa,
  title={Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={Proceedings of the  USENIX Symposium on Operating Systems Design and Implementation},
  pages={559--578},
  year={2022}
}

@inproceedings{capuchin,
  title={Capuchin: Tensor-based {GPU} memory management for deep learning},
  author={Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
  booktitle={Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={891--905},
  year={2020}
}

@inproceedings{checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  booktitle={Proceedings of the Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}

@article{chen2016,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{chimera,
  title={Chimera: efficiently training large-scale neural networks with bidirectional pipelines},
  author={Li, Shigang and Hoefler, Torsten},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2021}
}

@inproceedings{combination,
  title={Efficient combination of rematerialization and offloading for training dnns},
  author={Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={34},
  pages={23844--23857},
  year={2021}
}

@inproceedings{dapple,
  title={DAPPLE: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the  ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021}
}

@article{dtr,
  title={Dynamic tensor rematerialization},
  author={Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary},
  journal={arXiv preprint arXiv:2006.09616},
  year={2020}
}

@inproceedings{generic,
  title={A generic communication scheduler for distributed {DNN} training acceleration},
  author={Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
  booktitle={Proceedings of the ACM Symposium on Operating Systems Principles},
  pages={16--29},
  year={2019}
}

@inproceedings{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  journal={OpenAI}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{l2l,
  title={Training large neural networks with constant memory using a new execution algorithm},
  author={Pudipeddi, Bharadwaj and Mesmakhosroshahi, Maral and Xi, Jinwen and Bharadwaj, Sujeeth},
  journal={arXiv preprint arXiv:2002.05645},
  year={2020}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{megascale,
  title={$\{$MegaScale$\}$: Scaling large language model training to more than 10,000 $\{$GPUs$\}$},
  author={Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and others},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={745--760},
  year={2024}
}

@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  booktitle={Proceedings of the Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{patrickstar,
  title={Parallel training of pre-trained models via chunk-based dynamic memory management},
  author={Fang, Jiarui and Zhu, Zilin and Li, Shenggui and Su, Hui and Yu, Yang and Zhou, Jie and You, Yang},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={1},
  pages={304--315},
  year={2022},
  publisher={IEEE}
}

@inproceedings{pipedream,
  title={PipeDream: generalized pipeline parallelism for {DNN} training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@article{pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@inproceedings{rockmate,
  title={Rockmate: an efficient, fast, automatic and generic tool for re-materialization in pytorch},
  author={Zhao, Xunyi and Le Hellard, Th{\'e}otime and Eyraud-Dubois, Lionel and Gusak, Julia and Beaumont, Olivier},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={42018--42045},
  year={2023},
  organization={PMLR}
}

@inproceedings{sparcml,
  title={SparCML: High-performance sparse communication for machine learning},
  author={Renggli, C{\`e}dric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2019}
}

@inproceedings{stronghold,
  title={Stronghold: fast and affordable billion-scale deep learning model training},
  author={Sun, Xiaoyang and Wang, Wei and Qiu, Shenghao and Yang, Renyu and Huang, Songfang and Xu, Jie and Wang, Zheng},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--17},
  year={2022},
  organization={IEEE}
}

@inproceedings{swapadvisor,
  title={Swapadvisor: Pushing deep learning beyond the {GPU} memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}

@inproceedings{unified,
  title={A unified architecture for accelerating distributed {DNN} training in heterogeneous {GPU/CPU} clusters},
  author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
  booktitle={Proceedings of the USENIX Symposium on Operating Systems Design and Implementation},
  pages={463--479},
  year={2020},
}

@inproceedings{vdnn,
  title={vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design},
  author={Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W},
  booktitle={Proceedings of the Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={1--13},
  year={2016},
  organization={IEEE}
}

@inproceedings{yuliang,
  title={Accelerating the training of large language models using efficient activation rematerialization and optimal hybrid parallelism},
  author={Yuan, Tailing and Liu, Yuliang and Ye, Xucheng and Zhang, Shenglong and Tan, Jianchao and Chen, Bin and Song, Chengru and Zhang, Di},
  booktitle={2024 USENIX Annual Technical Conference (USENIX ATC 24)},
  pages={545--561},
  year={2024}
}

@inproceedings{zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the  International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

