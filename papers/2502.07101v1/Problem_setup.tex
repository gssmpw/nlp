\label{MAB framework}
We introduce a two-layer Multi-Arm Bandit-based sensitivity estimation framework, followed by multiple ways to calculate the reward. We also summarize the steps in Algorithm \ref{multi-arm}.
% \subsection{Multi-Arm Bandit Sensitivity Framework}
We propose a nested Multi-Arm Bandit Framework where there are two layers or levels of sampling. \\\noindent
In the \underline{Outer Layer} MAB, the individual arms correspond to the words in the dataset. We acquire these words by using appropriate \textit{tokenization techniques} available for a particular language. Each Outer arm/word $w$ is associated with a reward value which we call the \textbf{Global Sensitivity value} for that particular outer arm/word.
\\\noindent
The \underline{Inner Layer} is comprised of the sentences from the dataset in which a particular word (Outer arm) $w$ is present. We denote $\textbf{S}^{w}$ as the set of all the sentences in which a particular outer arm or word (say $w$) is present. Similar to the Outer arm, each inner arm/sentence is linked with a reward value which we call the \textbf{Local Sensitivity value} for that particular inner arm/sentence.

\paragraph{Calculating Global Sensitivity:} Global sensitivity value (represented as $G^{w}_{t}$) for a particular word $w$ at a particular step/iteration $t$ is calculated as follows:
\begin{equation*}
    G^{w}_{t} = \frac{(N^w \:* \:G^{w}_{t - 1}\: + \: L^s_{w})}{ 1 \:+ \:N^w},
\end{equation*}
where $N^w$ represents the number of times the word $w$ has been picked up so far and $L^w_s$ represents the Local sensitivity of a sentence $s \in \textbf{S}^{w}$. $G^{w}_{t}$ lies between 0 and 1.
    
We employ the UCB1 (\cite{10.1023/A:1013689704352}) sampling strategy ito choose a specific word $w$ at each step $s$ using the following equation:
\begin{equation}
    w^*_{t + 1} = \argmax_{w \in \textbf{W}} \left(\:G^w_t \:+\: \sqrt{\qfrac{2 \: * \: \log (1 + t)}{1 \:+ \:N^w}}\right),
\end{equation}
where $\textbf{W}$ is the set of all the words/ outer-arms.

\paragraph{Calculating Local Sensitivity: } We calculate the Local Sensitivity $L^w_s$ for a word $w$ at each step $t$ using the following way. We estimate the effect of the word by creating a set of perturbed sentences using \textit{sample-replace-predict} strategy. 

First, we \underline{sample} a word $w$  using the UCB1 sampling strategy. For this word $w$, a sentence $s \in \textbf{S}^{w}$ is further selected randomly \textit{with replacement} strategy. Then, we \underline{replace} the word $w$ in the selected sentence $s$ using predictions of a multilingual model (here we use \texttt{XLM-Roberta-Large}), ensuring that the resulting sentence with the new word $w^{'}$ remains coherent and semantically sound. This process is repeated $N$ times (here $N=10$). If, across the $N$ replacements, the new word $w^{'}$ matches the original word $w$, then we discard that instance. Let $\textbf{P}_w$ denote the total number of valid instances i.e. instances that have not been discarded. Lastly, we use the target model to \underline{predict} the labels of the newly constructed ${P}_w$ sentences. By utilizing the target model predictions of the ${P}_w$ sentences, we  estimate the local sensitivity ($L^w_s$) of a sentence $s \in \textbf{S}^{w}$ for a particular (outer arm) word $w$ is using two methods:\\\noindent
    $\bullet$~~\textbf{With Gold Labels:} We compare the predicted label of each of the 10 sentences with that of the \textsc{Gold} label of the original sentence. If they matches, we give a reward of $0$, otherwise we give a reward of $1$.\\\noindent
    $\bullet$~~\textbf{Without Gold Labels:} Here, we calculate the local senstivity as: 
        $L^w_s = 1 \: - \: \qfrac[2.5pt]{f_{mode}}{{P}_w}$, 
    where $f_{mode}$ represents the frequency of the mode of the predictions of the ${P}_w$ valid instances.
    This definition has been borrowed from \cite{lu2023prompts} and slightly modified.

    

    \paragraph{Estimating the Total Regret.} In the Multi-arm Bandit framework, Total Regret $R_t$ is defined as the total loss we get by not selecting the optimal action up to the step or iteration $t$. 
    Let the outer arm or word $w$ be picked up at the step or iteration $t$. Now, in turn, we will pick up a sentence $s \in \textbf{S}^{w}$. Let $L^w_s$ be the local sensitivity of sentence $s$. Hence, the Total Regret $R_t$ up to the iteration $t$ is defined as:
\begin{equation*}
    R_t = R_{t-1} \: + \: ([L^{w*}\: - \: L^w_s] \: * \: G^w_t),
\end{equation*}
    where $G^w_t$ is the Global sensitivity value of (the outer arm) the word $w$ that was picked and $L^{W*}$ is the optimal/ highest value of Local sensitivity that can be obtained out of the set $\textbf{S}^{w}$.
    
    \paragraph{Initialize the Global sensitivity Values.} We set the initial global sensitivity values for each word or outer arm by assigning values drawn from a Normal distribution with a mean of $0$ and a standard deviation of $1$, clipped from a = $0$ to b = $0.1$.


\begin{algorithm}
    \caption{Multi Arm Bandit Algorithm}\label{multi-arm}
    \textbf{Input: } A set of words/ outer-arms \textbf{W}, Dictionary \textbf{D} containing the set $\textbf{S}^{w}$ of sentences as a \textit{value} for every \textit{key} i.e. word $w \in \textbf{W}$ and total number of iterations $\textbf{T} \gets 100000$.\\

    \textbf{Output: } The set \textbf{G} containing final global-sensitivity values for every word $w \in \textbf{W}$, and the Total Regret \textbf{R}.
    \vspace{0.1cm}
    \hrule % Line after Output
    \vspace{0.1cm}

    \begin{algorithmic}[1]
        \State Initialize the set \textbf{G} as the initial values of the global sensitivities of the words. Here, $|\textbf{G}| = |\textbf{W}|$

        \State Initialize the set \textbf{N} ($|\textbf{N}| = |\textbf{W}|$) to zero. \textbf{N} represent the count of every word $w \in \textbf{W}$.

        \State $t \gets 0$
        \State \textbf{Repeat steps 5 to 9 until $t \neq \textbf{T}$ :}
        \State Select a word $w \in \textbf{W}$ such that 
        $w^*_{t + 1} = \argmax_{w \in \textbf{W}} \left(\:G^w_t \:+\: \sqrt{\qfrac{2 \: * \: \log (1 + t)}{1\: + \:N^w}}\right) $

        \State $\textbf{S}^{w} \gets \textbf{D}[w^{*}]$, $N^w \gets N^w + 1$
        \State Randomly select a sentence $s \in \textbf{S}^{w}$ and calculate Local sensitivity of $s$ as $L^w_s = 1 \: - \: \qfrac[2.5pt]{f_{mode}}{P}$

        \State Update Global Sensitivity as: $G^{w}_{t} = \qfrac[2.5pt]{(N^w \:* \:G^{w}_{t - 1} \: + \: L^w_{s})}{ 1 \:+ \:N^w}$, $\textbf{G}[w] \gets G^{w}_{t}$ 

        \State Total Regret $R_t = R_{t-1} \: + \: ([L^{w*}\: - \: L^w_s] \: * \: G^w_t)$
    \end{algorithmic}
\end{algorithm}

