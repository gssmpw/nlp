To understand the complexity of sequence classification tasks, \citet{hahn-etal-2021-sensitivity} proposed sensitivity as the number of disjoint subsets of the input sequence that can each be individually changed to change the output. Though effective, calculating sensitivity at scale using this framework is costly because of exponential time complexity. Therefore, we introduce a \textbf{S}ensitivity-based \textbf{M}ulti \textbf{A}rmed \textbf{B}andit framework (\textbf{SMAB}), which provides a scalable approach for calculating \textit{word}-level \textit{local} (sentence-level) and \textit{global} (aggregated) sensitivities concerning an underlying text classifier for any dataset. We establish the effectiveness of our approach through various applications. We perform a case study on \textsc{CheckList} generated sentiment analysis dataset where we show that our algorithm indeed captures intuitively high and low-sensitive words. Through experiments on multiple tasks and languages, we show that sensitivity can serve as a proxy for accuracy in the absence of gold data. Lastly, we show that guiding perturbation prompts using sensitivity values in adversarial example generation improves attack success rate by 13.61\%, whereas using sensitivity as an additional reward in adversarial paraphrase generation gives a 12.00\% improvement over SOTA approaches. \textcolor{red}{\textit{Warning: Contains potentially offensive content.}}