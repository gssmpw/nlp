% ------------- Figure -----------------%
\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/SMAB_0410.png}
\caption{Overview of our \textbf{SMAB} framework. The outer arm consists of all words in the corpus, each linked to a set of sentences in the inner arm. $w_1$ is a word in the outer arm, and $S_{w_1}$ is the set of sentences in the inner arm that contains $w_1$. $G_t^{w_1}$ is the Global Sensitivity of word $w_1$ at step $t$. We utilize a \textbf{sample-replace-predict} strategy to estimate local sensitivity values $L_{w_1}$ for a word $w_1$. Here, $P_{w_1}$ is the set of predicted labels obtained after perturbing $S_{w_1}$ sentences and using the target classifier. $s_1$ is chosen randomly from $P_{w_1}$ while $s_2$ is chosen such that it has the highest reward. The local sensitivity values of a word help to update its Global Sensitivity values, which helps in better outer arm selection in the next time step.
}
\label{fig:MABframework}
\end{figure*}
% ------------- Figure -----------------%

Classifiers leveraging pre-trained Language Models (\textit{PLMs}) while being empirically successful, are often opaque. Identifying input subspaces where the models classify correctly (or incorrectly) or the input patterns the model is \textit{sensitive} towards, is not straightforward. In the presence of model weights (white box), it may be feasible to explore such spaces but computationally intractable. On the other hand, methods attempting to explain black-box models resort to robust diagnostic tests (such as \textsc{CheckList}; \citet{ribeiro-etal-2020-beyond}) at scale, which requires human input and may not be sufficient. Visualization techniques such as LIME \cite{ribeiro2016whyitrustyou}, SHAP \cite{lundberg2017unifiedapproachinterpretingmodel} attempt to explain \textit{local} causes towards a prediction but do not provide a global view of these models.
Therefore, a systematic framework is required to understand the model's weaknesses and strengths related to the input space without assuming \textit{access to model weights}. 

\citet{hahn-etal-2021-sensitivity} proposed a theoretical framework for understanding the complexity of sequence classification tasks using {\em sensitivity}, where sensitivity is the number of disjoint subsets of the input sequence that can each be individually changed to change the output. High-sensitivity functions are complex because a single change in the input can alter the output, whereas low-sensitivity functions are simple. Sensitivity helps predict the complexity of the tasks for various machine learning methods. While the proposed method effectively captures the task complexity, it requires iterating exhaustively over all possible independent subsequences in the input, resulting in a time complexity that is exponential to the number of input tokens. 
\textit{In this work, we extend this definition of sensitivity} and \textit{propose a scalable framework based on multi-armed bandits to calculate word-level sensitivities on sequence classification tasks, without assuming access to model weights and gold labels}. 

Specifically, our framework provides an effective tool for \textit{local} attributions of the model output to individual segments of the input (sentence level \textit{local sensitivities}) as well as dataset-level sensitivities to specific words/phrases, which is independent of any context (\textit{global sensitivities}). We introduce \textbf{SMAB}, a sensitivity-based multi-armed bandit framework, which utilizes masked language modeling \textit{(MLM)} to compute word-level sensitivities in the dataset through effective exploration-exploitation strategies in a multi-armed bandit setup. Using these word-level sensitivities, we prove the effectiveness of the SMAB framework on three different tasks for three different application scenarios: (1) \textbf{Case Study on \textsc{CheckList}}, where we show that for template generated datasets, the global sensitivity values obtained from SMAB can help us identify high and low sensitive words across different test types. (2) \textbf{Sensitivity as a Proxy for Accuracy}, we show that sensitivity can serve as an unsupervised proxy for accuracy for a given classifier when the gold labels are unavailable. (3) \textbf{Adversarial Example Generation}, where we propose perturbation instructions that use global sensitivities obtained from the SMAB framework along with perturbation instructions proposed in \citet{xu2023llmfoolitselfpromptbased} to attack LLMs like GPT-3.5 \cite{brown2020languagemodelsfewshotlearners} and Llama-2-7B \cite{touvron2023llama2openfoundation}. We also demonstrate that using local sensitivities as an additional reward helps design highly accurate paraphrase attacks for LMs using the adversarial attack generation method proposed in \citet{roth2024constraintenforcingrewardadversarialattacks}. The two primary  contributions\footnote{Code: \href{https://github.com/skp1999/SMAB}{https://github.com/skp1999/SMAB}} of this work are summarized as follows.
\begin{compactitem}
    \item We propose an efficient, scalable algorithm, SMAB, to estimate the \textit{local} (sentence\-level) and \textit{global} (dataset\-level) sensitivities of words concerning an underlying classifier without \textit{access to model weights and gold labels}.
    % \item We propose and empirically establish the usefulness of word sensitivity in three different applications across several classification tasks, proving both the generalizability and effectiveness of our approach.
    \item We propose and empirically establish the usefulness of SMAB by (i) a case study on a templated dataset (using \textsc{CheckList}) to identify high and low-sensitive words, (ii) proposing sensitivity as an unsupervised proxy for accuracy (drops), and (iii) adversarial example generation using \textit{local} (sentence-level) and \textit{global} (dataset-level) word sensitivities. 
    % \item Through a case study on the CheckList-templated dataset, we show that the sensitivity estimates are well-suited to identify globally high and low-sensitive words.
    % \item We further show that sensitivity can serve as an unsupervised proxy for accuracy in the absence of gold labels
    % \item We propose perturbation instructions that include word-level sensitivities to attack LLMs in a zero-shot manner. We also show that local sensitivities obtained from SMAB can be used as an additional reward to generate adversarial examples through finetuning.
   % \item We propose perturbation prompts that include words with high sensitivity values obtained from SMAB and also show that, when used as an additional reward, sensitivities of a word can boost the adversarial attack success rate of a known attack method.
\end{compactitem}