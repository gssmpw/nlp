%You may include other additional sections here.
% \clearpage

\section{MAB Framework: Additional Details}
\label{sec:appendix_SMAB}
We summarize the details of the SMAB framework in an Algorithm format in Algorithm~\ref{algo:SMAB}. 

\paragraph{Outer Arm Selection.} We employ the UCB sampling strategy to choose a specific word $w$ at each step $t$ using:
\begin{equation*}
    w^*_{t+1} = \operatorname*{argmax}_{w \in W} \left( Beta(\alpha, \beta) \right)
\end{equation*}

% \begin{equation}
%      w^*_{t+1} = \operatorname*{arg\,max}_{w \in W} \left( \ G^w_{t} + \sqrt{\frac{2*log(1+t)}{1+N^w}} \right),
% \end{equation}
where $W$ is the set of all the words or outer arms. $N^w$ is the no. of times a word $w$ has been picked so far. 

\paragraph{Estimation of Total Regret.}~~In MAB, Total Regret $R_t$ is defined as the total loss we get by not selecting the optimal action up to the step or iteration $t$. Let the outer arm or word $w$ be picked up at the step or iteration $t$. Now, in turn, we will pick up sentences $s_1$ and $s_2$ $\in$ $S_w$. Let $L_w$ be the local sensitivity. Hence, the Total Regret $R_t$ up to the iteration $t$ is defined as:
\begin{equation}
    R_t = R_{t-1} \: + \: ([L_{w^*}\: - \: L_w] \: * \: G^w_t)
\end{equation}
where $G^w_t$ is the Global sensitivity value of (the outer arm) the word $w$ that was picked and $L_{W^*}$ is the highest value of local sensitivity that can be obtained out of the set $S^{w}$.

\paragraph{Comparison of Time Complexity.} Let $|P|$ be the size of the subset, $|D|$ be the size of the dataset i.e. the number of input sentences in the dataset, $|\sum|$ be the total number of words in the dataset, $|V|$ be the vocabulary size of the Language Model, and $cost(f)$ be the cost to use a Language Model for various purposes like classification, Masked Language Modeling.\\
\textbf{Time Complexity for Subset-sensitivity:} For every input sentence in the dataset, we use an MLM to generate $|V|^{|P|}$ perturbed strings and then classify them using an LLM to see if the label flipped. Hence, the Time Complexity is:
\begin{equation}
    O(|D|\cdot |V|^{|P|} \cdot cost(f) )
\end{equation}
For calculating the block sensitivity, the subset sensitivity is calculated K times, corresponding to the K partitions, which can go exponential. \\
\textbf{Time Complexity of SMAB:} Local sensitivity in our algorithm is closely related to the subset sensitivity defined in \citet{hahn-etal-2021-sensitivity}. Our local sensitivity is obtained by taking $|P| = 1$ (singleton sensitivity) and global sensitivity is calculated using equation 3 in our paper, which is O(1). The time complexity of our SMAB algorithm given $T$ as the total number of iterations is:
\begin{equation}
    O(T\cdot (|\sum| \, + \, (|D| \cdot |V|\cdot cost(f))))
\end{equation}
This shows that our proposed algorithm is computationally more efficient than the existing algorithms.
In practice, the top few replacements (perturbations) from the MLM probability distribution are used instead of iterating over all vocabulary symbols.


\begin{algorithm}[htbp]
    \caption{Multi-Armed Bandit Algorithm}
    \label{algo:SMAB}
    \textbf{Input: } A set of words/outer-arms \textbf{W}, Dictionary \textbf{D} containing the set $\textbf{S}^{w}$ of sentences as a \textit{value} for every \textit{key} i.e. word $w \in \textbf{W}$ and total number of iterations $\textbf{T} \gets 200000$.\\
    \textbf{Output: } The set \textbf{G} containing final global-sensitivity values for every word $w \in \textbf{W}$
    % , and the Total Regret \textbf{R}.
    \vspace{0.1cm}
    \hrule % Line after Output
    \vspace{0.1cm}

    \begin{algorithmic}[1]
        \State Initialize the set \textbf{G} as the initial values of the global sensitivities of the words. Here, $|\textbf{G}| = |\textbf{W}|$

        \State Initialize the set \textbf{N} ($|\textbf{N}| = |\textbf{W}|$) to zero. \textbf{N} represent the count of every word $w \in \textbf{W}$.

        \State $t \gets 0$
        \State \textbf{Repeat steps 5 to 9 until $t \neq \textbf{T}$ :}
        \State Select a word $w \in \textbf{W}$ such that 
        \begin{equation*}
            w^*_{t+1} = \operatorname*{argmax}_{w \in W} \left( Beta(\alpha, \beta) \right)
        \end{equation*}
    
        \State $\textbf{S}^{w} \gets \textbf{D}[w^{*}]$, $N^w \gets N^w + 1$
        \State Select two sentences \( s_1, s_2 \in \textbf{S}^{w} \) and calculate Local sensitivity as:
\begin{equation*}
    L_w = \epsilon \cdot r_1 + (1-\epsilon) \cdot r_2
\end{equation*}

        \State Update Global Sensitivity $G^w_t$ as
        \begin{equation*}
            G^{w}_{t} = \frac{(N^w \:* \:G^{w}_{t - 1} \: + \: L_{w})}{ 1 \:+ \:N^w}
        \end{equation*}

        \State Final Global Sensitivity Values: $\textbf{G}[w] \gets G^{w}_{t}$ 

        \State Total Regret $R_t = R_{t-1} \: + \: ([L_{w*}\: - \: L_{w}] \: * \: G^w_t)$
    \end{algorithmic}
\end{algorithm}

\section{\textsc{CheckList}: Additional Results}

In Figure ~\ref{fig:scatter_plot}, we show a scatter plot of the word sensitivities estimated using SMAB (with Thomspon Sampling). As mentioned in the main paper, we observe that words from DIR templates have a larger distribution, present in the $0.2-1$ range, whereas, the words from the invariant test cases (INV template) have lower sensitivities (mostly between $0$ and $0.2$).

\begin{figure}[!t]
\centering
    \includegraphics[height=0.32\textheight]{figures/scatter_checklist_1000_xlabel.png}
    \caption{Scatter plot of estimated global sensitivities of arms of INV and DIR templates using TS. Words from DIR templates have higher estimated global sensitivity and are spread in the whole space as opposed to words from INV templates. %\sa{change the colors to more distinct ones. dark blue, and light red with black border.}
    } 
    \label{fig:scatter_plot}
\end{figure}


\subsection{Dataset details}
\label{sec:Appendix:subsection:dataset details}

\subsubsection{Hate Speech Dataset}
\label{sec:appendix:subsection:hate_speech_details}
In this section, we describe the sources of our compiled dataset for the hate classification task for different languages. The complete statistics for all the languages can be found in Table~\ref{tab:hate_dataset_used}.

\paragraph{English:} We used the \textbf{Stage 1: High Level Categorization} of the \textit{Implicit Hate} dataset provided by \cite{elsherief-etal-2021-latent}. It contains three labels namely implicit\_hate, explicit\_hate, and not\_hate. We converted this multi-classification task into a binary classification task where the labels implicit\_hate, and explicit\_hate are treated as the hate label.
\paragraph{Bengali:} We used a subset of the Bengali Hate speech dataset provided by \cite{romim2020hate} which includes the categories \textit{crime}, \textit{religion}, and \textit{politics} for hate label and all the categories for non-hate label.
\paragraph{Hindi:} For the Hindi language, we combined datasets collected from two different sources: 1) \cite{10.1145/3368567.3368584} provides a binary (hate/no-hate) version of the Hindi dataset. 2) We used a subset of the Hindi dataset (excluding the \textit{fake} category) provided by \cite{bhardwaj2020hostility} which includes the category \textit{non-hostile} for non-hate label and all remaining categories for hate label.

\paragraph{Spanish:}We used two datasets for the Spanish language: 1) HatEval dataset is provided by \cite{basile-etal-2019-semeval}, and 2) \cite{PereiraKohatsu2019DetectingAM} has provided the hate speech dataset in the Spanish language.

% \paragraph{Brazilian Portuguese:} We used the dataset provided by \cite{leite2020toxic} in our experiments.

\begin{table}[tbhp]
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Language}   & \textbf{Hate}  & \textbf{No Hate} & \textbf{Total} \\
        \midrule
        English    &  1026 & 1658 & 2684 \\
        Bengali    & 806  & 1194 & 2000 \\
        French     & 1593 & 421 & 2014\\
        German     & 904 & 1607 & 2511\\
        Greek      & 501 & 1100 & 1601\\
        Hindi      & 783  & 549 & 1332 \\
        Spanish    & 1010  & 1290 & 2300 \\
        Turkish    & 400 & 1290 & 1690 \\
       \bottomrule
    \end{tabular}
    \caption{Dataset Statistics for val split of mHate dataset}
    \label{tab:hate_dataset_used}
\end{table}


\section{KLD v/s Accuracy Experiments}
\label{sec:appendix:subsection:kld_vs_acc}


\paragraph{Correlation Within Langauge.}
We experiment with various models on mHate and XNLI dataset. We use 5 different target classifiers for a language to estimate global sensitivities. We use XLM-R \cite{conneau2020unsupervisedcrosslingualrepresentationlearning}, mBERT, mDeBERTa \cite{he2021debertadecodingenhancedbertdisentangled}, FlanT5-L \cite{chung2022scalinginstructionfinetunedlanguagemodels} and GPT-3.5 for predictions on mHate and XNLI dataset. For a given language, we then compare the difference in sensitivity distributions of different models with respect to a base model (XLM-R), measured as KL Divergence with the performance of different models on that langauge (accuracy). We plot KLD v/s Accuracy plots for different models under study. 

\begin{figure}[htbp]
    \includegraphics[width=\columnwidth]{figures/kld_vs_accuracy/kld_ts_wl_mhate.png}
    \caption{KLD v/s accuracy within language on mHate-English dataset.}
    \label{fig:mhate_within_languges}
\end{figure}

\section{Improving Paraphrase Attack with Local Sensitivity}
\label{appendix:paraphraseattack_details}
% In this section, we provide additional details about improving paraphrase attacks using local sensitivity. In Table~\ref{tab:paraphrase_attack_HS}, we show the flip success rates for different variants on the hate speech dataset.


\subsection{Senisitivity Reward Calculation}
\label{appendix:sensitvity_reward_calc}
We adjust the original reward function $R(x, x')$ by incorporating an additional \textbf{Sensitivity reward} $S(x, x')$ weighted by the scaling constant $\alpha \in (0, 1)$ as shown:
\begin{equation}
    R(x, x') = R(x, x') + \alpha \, S(x, x').
\end{equation}
Here, $S(x, x')$ is the difference between the sensitivity of the input text $x$ and the sensitivity of the generated text 
$x'$: $S(x, x') = s(x) - s(x')$.
The sensitivity of a text is calculated as 
\begin{equation}
    s(x) = \frac{\sum_{i=1}^{m} \sum_{j=1}^{n_i} L_{s}^{ij}}{\sum_{i=1}^{m} n_i}
\end{equation}
where ${m}$ represents the total number of keyphrases, ${n_i}$ represents the number of words in the i-th keyphrase and $L_{s}^{ij}$ represents the local sensitivity of $j^{th}$ word in the $i^{th}$ keyphrase. For our experiments, we used Scaling constant $\alpha = 0.25$. We extract the keyphrases from a text using a TopicRank keyphrase extraction model using an open source toolkit \texttt{pke} \cite{boudin:2016:COLINGDEMO}.


\begin{table*}[t]
\centering
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{l|l|c}
            \toprule
            \textbf{Purpose} & \textbf{Model}  &\textbf{Threshold}\\ 
            \midrule
            Paraphraser & \texttt{prithivida/parrot\_paraphraser\_on\_T5}  & - \\ 
            \midrule
            \begin{tabular}[c]{@{}l@{}}Target (RT) \end{tabular} & \texttt{textattack/distilbert-base-uncased-rotten-tomatoes}  & - \\
            \midrule
             % \begin{tabular}[c]{@{}l@{}}Victim (HS) \end{tabular}& \texttt{bert-base-multilingual-cased}  & - \\
             % \midrule
             \begin{tabular}[c]{@{}l@{}}Sensitivity (RT) \end{tabular} & \texttt{textattack/bert-base-uncased-rotten-tomatoes}  
            & - \\ 
            \midrule
            % \begin{tabular}[c]{@{}l@{}}Sensitivity (HS) \end{tabular} & Finetuned \texttt{bert-base-multilingual-cased}  & - \\ 
            % \midrule
            \begin{tabular}[c]{@{}l@{}}Linguistic \\ Acceptability\end{tabular} & \texttt{textattack/albert-base-v2-CoLA}   &0.5\\ 
            \midrule
            \begin{tabular}[c]{@{}l@{}}Semantic \\ Consistency\end{tabular} & \texttt{sentence-transformers/paraphrase-MiniLM-L12-v2}   &0.8\\
            \midrule
            \begin{tabular}[c]{@{}l@{}}Label \\ Invariance\end{tabular} & \texttt{howey/electra-small-mnli}  &0.2\\
            \midrule
\end{tabular}%
}
\caption{Various models as target model, in sensitivity reward functions and constraints used in Paraphrase Attack using Local Sensitivity based adversarial example generation experiment. \textbf{RT}: Rotten Tomatoes dataset. }
% \textbf{HS}: Hate speech dataset.}
\label{tab:various_models_used_paraphrase}
\end{table*}

\begin{table}[htbp]
\resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{l c c c}
    \toprule
    \multirow{1}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Strategy\end{tabular}}} &
    \multirow{1}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Temp \end{tabular}}} & 
      \textbf{\begin{tabular}[c]{@{}l@{}}Total generated \\ paraphrases\end{tabular}} & 
      \textbf{\begin{tabular}[c]{@{}l@{}}Total used \\paraphrases \end{tabular}} \\ 
    \midrule
    
    \multicolumn{4}{c}{\rule{0pt}{2ex} SMAB Model $\rightarrow$ \ding{55}}\\
    \hline

    DBS + (\ding{55}) & 0.85  & 62 & 62\\
    BS + (\ding{55}) & 0.85 & 53 & 53\\
    BS + (\ding{55})  & 1.00 & 110 & 100\\
    BS + (\ding{55})  & 1.15 & 68 & 68\\                       
    \hline

    \multicolumn{4}{c}{\rule{0pt}{2ex} SMAB Model $\rightarrow$ \textbf{BERT}}\\
    \hline
    DBS + (\ding{51})  & 0.85 & 181 & 100\\
    BS + (\ding{51}) & 0.85 & 71 & 71\\
    BS + (\ding{51})  & 1.00 & 79 & 79\\
    BS + (\ding{51}) & 1.15 & 183 & 100\\
    \hline

    \multicolumn{4}{c}{\rule{0pt}{2ex} SMAB Model $\rightarrow$ \textbf{DistilBERT}}\\
    \hline
    DBS + (\ding{51})   & 0.85  & 319 & 100\\
    BS + (\ding{51})  & 0.85  & 140 & 100\\ 
    BS + (\ding{51})   & 1.00  & 334 & 100\\
    BS + (\ding{51})  & 1.15  & 75 & 75\\ 
    \bottomrule
\end{tabular}%
}
\caption{Total unique paraphrases generated by each model variation from a total of 359 test instances that resulted in a label flip and the total paraphrases selected for human evaluation for the \textbf{Rotten Tomatoes} dataset.}
\label{tab:UniqueParaphrase_RT_appendix}
\end{table}


% Here is the 7.2 table

\begin{table}[htbp]
\resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{l c c c}
    \toprule
    \multirow{1}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Strategy\end{tabular}}} &
    \multirow{1}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Temp \end{tabular}}} &
    \multirow{1}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}FSR $\uparrow$ \end{tabular}}} & \textbf{\begin{tabular}[c]{@{}l@{}}After Attack \\ Accuracy $\downarrow$ \end{tabular}} \\
    \hline
    
    \multicolumn{4}{c}{\rule{0pt}{1ex} SMAB Model $\rightarrow$ \ding{55}}\\
    \hline

    DBS + (\ding{55}) & 0.85  & 24.79 & 69.35 \\
    BS + (\ding{55}) & 0.85 & 15.87  & 82.72 \\ 
    BS + (\ding{55})  &1.00 &  17.54  & 81.05 \\
    BS + (\ding{55})  & 1.15 & 13.64 & 85.23 \\  
    \hline

    \multicolumn{4}{c}{\rule{0pt}{1ex} SMAB Model $\rightarrow$ \textbf{BERT}}\\
    \hline
    DBS + (\ding{51})  & 0.85 & \cellcolor{green1}44.87  & \cellcolor{green1}49.58 \\
    BS + (\ding{51}) & 0.85 & 18.10  & 80.22 \\
    BS + (\ding{51})  & 1.00  & 20.33  & 77.99 \\
    BS + (\ding{51}) & 1.15 & \cellcolor{green1}47.62  & \cellcolor{green1}49.02 \\
    \hline

    \multicolumn{4}{c}{\rule{0pt}{1ex} SMAB Model $\rightarrow$ \textbf{DistilBERT}}\\
    \hline
    DBS + (\ding{51})   & 0.85  & \cellcolor{green1}86.35  & \cellcolor{green1}11.14 \\
    BS + (\ding{51})  & 0.85  & \cellcolor{green1}35.93  & \cellcolor{green1}61.00 \\ 
    BS + (\ding{51})   & 1.00  & \cellcolor{green2}92.20  & \cellcolor{green2}6.96 \\
    BS + (\ding{51})  & 1.15  & 20.33 & 79.10 \\
    \bottomrule
\end{tabular}%
}
\caption{ParaphraseAttack Results on Rotten Tomatoes (\textbf{RT}) dataset. The strategy represents the combination of decoding mechanism (\textbf{DBS}: Diverse Beam Search, \textbf{BS}: Beam Search) and usage of sensitivity reward (\ding{55}: No sensitivity, \ding{51}: Sensitivity using bert/distilbert). Temp: decoding temperature. FSR/After Attack Accuracy in ($\%$). \textbf{Before Attack Accuracy} is 100\%. All variants outperforming the highest baseline score are highlighted in \colorbox{green1}{green} and the best performing variant is highlighted in \colorbox{green2}{green}.}
\label{tab:paraphrase_attack_RT_appendix}
\end{table}



% SMAB table %
\begin{table*}[htbp]
\resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|l|c|c|c|c|}
    \hline
    \textbf{Task}  & \textbf{Dataset} & \multicolumn{1}{c|}{\textbf{Target Classifier}} & \textbf{Languages} & \textbf{\#datapoints} & \textbf{\#arms} & \textbf{\#edges} \\ 
    \hline
    
    \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Sentiment \\ Analysis\end{tabular}}          & CheckList & cardiffnlp/twitter-roberta-base-sentiment-latest & english & 34486                 & 8498 & 52359 \\ 
    \cline{2-7} & RT & textattack/distilbert-base-uncased-rotten-tomatoes & english & 1066 & 5104 & 18835 \\ \cline{2-7}  & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SST-2 \end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}distilbert/distilbert-base-uncased-finetuned-sst-2-english \\ gpt-3.5-turbo\\ meta-llama/Llama-2-7b \end{tabular}} & english & 872  & 4088  & 8041 \\
    \cline{4-7} & & & english & 872 & 4088 & 8041 \\
    \cline{4-7} & & & english & 872 & 4088 & 8041 \\
    \hline
    
    \multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Hate \\ Classification\end{tabular}}         & \multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Multilingual\\ hate\end{tabular}} & \multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}google-bert/bert-base-multilingual-uncased\\ google/flan-t5-large\\ MoritzLaurer/deberta-v3-base-zeroshot-v1\\ gpt-3.5-turbo\end{tabular}} & bengali & 2000 & 7880 & 18929 \\ 
    \cline{4-7}  &   &  & english & 2684 & 7061 & 23149 \\ 
    \cline{4-7} &  &  & french  & 2014 & 5680 & 14184 \\ 
    \cline{4-7}  &   &   & german & 2511 & 11767 & 24226\\ 
    \cline{4-7}  &  &  & greek & 1601 & 9799 & 22031 \\ 
    \cline{4-7}  &  &   & hindi  & 1332 & 7181 & 20140 \\ 
    \cline{4-7} & & & italian & 1846 & 7551 & 17876 \\ \cline{4-7}  &  &  & spanish & 2150 & 8780            & 21982 \\ 
    \cline{4-7}  & &  & turkish & 1690 & 12308 & 20631  \\ \hline
    
    \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Natural\\ Language\\ Inference\end{tabular}} & \multirow{6}{*}{XNLI} & \multirow{6}{*}{MoritzLaurer/mDeBERTa-v3-base-mnli-xnli} & english & 5010  & 9543  & 68156 \\ \cline{4-7}  & & & french & 5010 & 12431 & 75322  \\ \cline{4-7}  & & & german & 5010 & 13099 & 61780 \\ \cline{4-7}  & & & greek  & 5010  & 15114  & 90149 \\ \cline{4-7} & & & hindi & 5010 & 9484 & 70938 \\ \cline{4-7}  &  &  & spanish & 5010 & 11994 & 69166 \\ \hline
    \end{tabular}
    }
    \caption{Training details of SMAB for each task and target classifier and the langauges. \#datapoints represent the number of sentences in the training set, \#arms represents the unique words present in the dataset after preprocessing.}
    \label{tab: SMAB_training_details}
\end{table*}

\subsection{Key Phrase sensitivity Calculation}
\label{Keyphrase_sensitivity}
We present the key-phrase sensitivity estimation process in a pseudo-code form in Algorithm \ref{algo:alg_kp_sensitivity}. This algorithm takes as input the original text $x$ and calculates its sensitivity. The algorithm has three main steps: In the \textbf{first step}, it extracts the key phrases from the input text $x$ using the Topic Rank Key phrase extraction Model $M$ and stores them in a list $K$. This first step is shown in line $6$ of the pseudo-algorithm. In the \textbf{second step}, it calculates the sensitivities of each key phrase. The algorithm iterates through all the key phrases stored in the list $K$ and does the following: \circled{1} It extracts all the words that are present in the current key phrase and stores it in the list $words$. Similarly, it maintains a global variable $Total\_words$ that will store the total number of words extracted from each Keyphrase. \circled{2} It creates a masked sentence $x_{masked}$ from the input text $x$ by replacing all the words that are present in the current key phrase with [MASK]. \circled{3} It then uses \texttt{bert-large-uncased} to generate $10$ perturbed sentences by predicting new words at the locations where [MASK] is present in $x_{masked}$. These $10$ perturbations are stored in the list $Masked\_output$. \circled{4} The algorithm then predicts the label of the input text $x$ and $10$ perturbed sentence present in the list $Masked\_output$ and stores the predictions in the variable $P rediction\_original$ and in the list $Prediction\_List$ respectively. \circled{5} Then, it iterates through all the labels in the $Prediction\_List$ and increments the variable $flips$ if the $P rediction\_original$
doesn't match the current label. 
 \circled{6} Finally, it calculates the local sensitivity of the current keyphrase as the proportion of the total flips that we got out of the total labels i.e. $flips / \texttt{len}(Prediction\_List)$ and stores it in a list $Keyphrase\_sensitivity$ at the index corresponding to the current key phrase. This second step is shown in lines $7$ to $25$ in the pseudo-algorithm. In the \textbf{third step}, the algorithm adds up all the values in the list $Keyphrase\_sensitivity$ and divides it by $Total\_words$ to get the sensitivity of the input text $x$. This third step is shown in lines $27$ to $33$ in the pseudo-algorithm.

\subsection{Qualitative analysis of the type of Attacks}
\label{sec:appendix_qual_examples_RT}
Table \ref{tab:qual_examples_PP} shows the three different types of attacks carried out by different variants when using \textbf{distilbert} as the SMAB Model. \circled{1} \textbf{Type 1} attack involves generating adversarial examples by replacing one or more words in a sentence with new words and rephrasing the original sentence. Model variant with decoding strategy \textbf{BS + (\ding{51})} and a temperature of $0.85$  has learned to carry out \textbf{Type 1} attacks. \circled{2} \textbf{Type 2} attack involves generating adversarial examples by removing one or more words from the original sentence while preserving its semantic meaning. Model variant with decoding strategy \textbf{BS + (\ding{51})} and a temperature of $1.15$  has learned to carry out \textbf{Type 2} attacks. \circled{3} \textbf{Type 3} attack involves appending various suffixes to sentences, such as adding \textit{but it's true}, or words like \textit{but why} followed by a \textit{?}. Model variant with decoding strategy \textbf{BS + (\ding{51})} and a temperature of $1$  has learned to carry out \textbf{Type 3} attacks.

% Qualitative Examples - ParaphraseAttack %
\begin{table}[htbp!]
\scriptsize 
\begin{tabular}{l l r}
\toprule
\textbf{Attack} & \textbf{Examples} & \textbf{Flip Result} \\
\midrule
\begin{tabular}[c]{@{}l@{}}Type 1\end{tabular} & \begin{tabular}[c]{@{}l@{}} \underline{Original}: smarter than its commercials make \\it seem. \\ 

\underline{Perturbed}: 
it's smarter than the commercials \\make it appear. \\

\hline
\\
\underline{Original}: it has become apparent that the \\franchise's best years are long past. \\ 
\underline{Perturbed}: it's clear that the best years of \\the franchise are long gone.\end{tabular} & \begin{tabular}[c]{@{}r@{}}\texttt{\textcolor{blue}{pos}} $\rightarrow$ \texttt{\textcolor{red}{neg}}\\ \\ \\ \\ \texttt{\textcolor{red}{neg}} $\rightarrow$ \texttt{\textcolor{blue}{pos}}\end{tabular} \\ 
\midrule
\begin{tabular}[c]{@{}l@{}}Type 2\end{tabular} & \begin{tabular}[c]{@{}l@{}} \underline{Original}: crush is so warm and fuzzy you \\might be able to forgive its mean-spirited \\second half. \\ 

\underline{Perturbed}: crush is so warm and fuzzy you \\might forgive its mean-spirited second half \\ 
\hline
\\
\underline{Original}: you can practically hear george \\orwell turning over.
\\ 

\underline{Perturbed}: You can hear george orwell \\turning over. \\ 

\end{tabular} & \begin{tabular}[c]{@{}r@{}}\texttt{\textcolor{blue}{pos}} $\rightarrow$ \texttt{\textcolor{red}{neg}}\\ \\ \\ \\ \\ \texttt{\textcolor{red}{neg}} $\rightarrow$ \texttt{\textcolor{blue}{pos}}\end{tabular} \\ 
\midrule
\begin{tabular}[c]{@{}l@{}}Type 3\end{tabular} & \begin{tabular}[c]{@{}l@{}}\underline{Original}: provides a porthole into that noble, \\trembling incoherence that defines us all. \\ 

\underline{Perturbed}: It provides a porthole into that \\noble trembling incoherence that defines us \\all. But why? \\ 
\hline
\\
\underline{Original}: this feature is about as necessary \\as a hole in the head.
\\ 

\underline{Perturbed}: This feature is about as necessary \\as a hole in the head. But it's true. \\ 

\end{tabular} & \begin{tabular}[c]{@{}r@{}}\texttt{\textcolor{blue}{pos}} $\rightarrow$ \texttt{\textcolor{red}{neg}}\\ \\ \\ \\ \\ \\ \texttt{\textcolor{red}{neg}} $\rightarrow$ \texttt{\textcolor{blue}{pos}}\end{tabular} \\
\bottomrule
\end{tabular}
\caption{Examples of successful adversarial attacks across the three different attack types when using DistilBERT as the SMAB Model. The \textbf{Flip Result} column shows label changes from \texttt{Original Label} $\rightarrow$ \texttt{New Label}, where \texttt{pos} and \texttt{neg} denote \textit{positive} and \textit{negative} classes respectively in the \textbf{Rotten Tomatoes} dataset.}
\label{tab:qual_examples_PP}
\end{table}


% Classification Prompts Table %
\begin{table*}[htbp!]
\resizebox{\textwidth}{!}{%
\centering
    \begin{tabular}{l | l }
    \toprule
    \textbf{\begin{tabular}[c]{@{}l@{}}LLM\end{tabular}} &
    \textbf{\begin{tabular}[c]{@{}l@{}}Prompt \end{tabular}} \\ 
    \midrule
    \textbf{GPT-3.5} & \begin{tabular}[c]{@{}l@{}}Given a sentence that is a movie review, your task is to assign a label based on its sentiment. Label 1 if the sentence is a \\ positive review and Label 0 if the sentence is a negative review. Remember to only provide the label.\\
    Sentence: [input\_sentence]\\
    Label: \end{tabular} \\ 
    \midrule
    \textbf{Llama-2-7B} & \begin{tabular}[c]{@{}l@{}}Please label the sentiment of the given movie review text. The sentiment label should be "positive" or "negative". \\
    Answer only a single word for the sentiment label. Do not leave answer as empty. Do not generate any extra text. \\ 
    Text: [input\_sentence] \\
    Answer: \end{tabular} \\ 
    \midrule
    \textbf{Qwen-2.5-7B} & \begin{tabular}[c]{@{}l@{}}Please label the sentiment of the given movie review text. The sentiment label should be "positive" or "negative". \\
    Answer only a single word for the sentiment label. Do not leave answer as empty. Do not generate any extra text. \\ 
    Text: [input\_sentence] \\
    Answer: \end{tabular} \\
    \bottomrule
\end{tabular}%
}
\caption{Prompts used for sentiment classification task on SST-2 dataset}
\label{tab:sentiment_classification_prompt}
\end{table*}


% Qualitative examples (PromptAttack) %
\begin{table*}[htbp]
\resizebox{\textwidth}{!}{%
\centering
    \begin{tabular}{l l l}
    \toprule
    \textbf{\begin{tabular}[c]{@{}l@{}}Perturb \\Type\end{tabular}} &
    \textbf{\begin{tabular}[c]{@{}l@{}}Qualitative \\Example \end{tabular}} &
    \textbf{\begin{tabular}[c]{@{}l@{}}Flip \\ Result \end{tabular}}\\ 
    \midrule
    $W4$ & \text{\begin{tabular}[c]{@{}l@{}} \textbf{Original Sentence:} the title not only describes its main characters, but the lazy people behind the camera as well.\\
    \textbf{New Sentence:} The title not only portrays its main protagonists, but also the laid-back crew behind the camera as well.
    \end{tabular}} & \textcolor{red}{neg} $\rightarrow$ \textcolor{blue}{pos}              \\ 
    \midrule
    $W5$ & \text{\begin{tabular}[c]{@{}l@{}} \textbf{Original Sentence:} an important movie, a reminder of the power of film to move us and to make us examine our values.\\
    \textbf{New Sentence:} an important movie, a reminder of the influence of cinema to move us and to force us to question our beliefs.
    \end{tabular}} & \textcolor{blue}{pos} $\rightarrow$ \textcolor{red}{neg}\\ 
    \midrule
    $W6$ & \text{\begin{tabular}[c]{@{}l@{}} \textbf{Original Sentence:} It's a charming and often affecting journey.\\
    \textbf{New Sentence:} It's a charming but sometimes disconcerting journey.
    \end{tabular}} & \textcolor{blue}{pos} $\rightarrow$ \textcolor{red}{neg}\\
    \bottomrule
\end{tabular}%
}
\caption{Qualitative examples
for perturbation types $W4$, $W5$, and $W6$ when using Llama-2-13B both as the as the \textsc{Smab} Model and the target Model. The \textbf{Flip Result} column shows label changes from \texttt{Original Label} $\rightarrow$ \texttt{New Label}, where \texttt{pos} and \texttt{neg} denote \textit{positive} and \textit{negative} classes respectively in the \textbf{SST-2} dataset. In $W4$, the placeholders "\texttt{[Word1]}", "\texttt{[Word2]}", \texttt{GS1}, and \texttt{GS2} are substituted with \textit{characters}, \textit{people}, 0.9865988772394045, and 0.968535617081986 respectively. In $W5$, the placeholders "\texttt{[Word1]}" and "\texttt{[Word2]}" are substituted with \textit{make} and \textit{film} respectively. In $W6$, the placeholders "\texttt{[Word1]}" and "\texttt{[Word2]}" are substituted with \textit{affecting} and \textit{often} respectively.}
\label{tab:Qual_Examples_promptAttack}
\end{table*}

% Keyphrase sensitivity algo %
\clearpage

\begin{algorithm*}[htbp]
\begin{minipage}{\textwidth}
\begin{algorithmic}[1]
\INPUT $x \gets$ Input Text
\Require $M \gets$ TopicRank keyphrase extraction model, $C \gets$ Classifier Model, $m \gets$ \texttt{bert-large-uncased} Model 
\REPRESENTATION $G(x) \gets$ Gold label of the Input Text $x$, $C(x) \gets$ Predicted label of the Input Text $x$ using the Classifier Model $C$, $m(x) \gets$ generates $K = 10$ perturbations of the input text x using the model $m$
\OUTPUT $s \gets$ Sensitivity of the Input Text

\State \textcolor{blue}{// Initialization}
\State $s \gets 0$ \Comment{\textcolor{blue}{This variable will store the sensitivity of the input text $x$.}}
\State $Total\_words \gets 0$ \Comment{\textcolor{blue}{This variable represents the total number of words across all the keyphrases.}}
\State $Keyphrase\_sensitivity \gets \{\}$ \Comment{\textcolor{blue}{A dictionary to store the sensitivity values of each keyphrase.}}

\State \textcolor{blue}{// Extract keyphrases using the TopicRank Model $M$ and store them in the list $K$.}
\State $K \gets M(x)$

\State \textcolor{blue}{// Main algorithm}
\For{$k$ \textbf{in} $K$} 
    \State $words \gets k.\texttt{split()}$ \Comment{\textcolor{blue}{Get a list of all the words in the current keyphrase.}}
    \State $Total\_words \mathrel{+}= \texttt{len}(words)$

    \For{$w$ \textbf{in} $words$}
        \State $x_{masked} \gets$ mask the word $w$ in the text $x$ using "[MASK]"
    \EndFor

    \State $Masked\_output \gets m(x_{masked})$ \Comment{\textcolor{blue}{Store all the perturbed samples in this list.}}
    \State $Prediction\_List \gets C(Masked\_output)$ \Comment{\textcolor{blue}{Store the prediction labels of all the perturbed samples in this list.}}
    \State $Prediction\_original \gets C(x)$ \Comment{\textcolor{blue}{Store the prediction label of the input text $x$.}}

    \State $flips \gets 0$ \Comment{\textcolor{blue}{A temporary variable to count the flips.}}
    \For{$labels$ \textbf{in} $Prediction\_List$}
        \If{$labels \neq Prediction\_original$}
            \State $flips \mathrel{+}= 1$
        \EndIf
    \EndFor
    \State $local\_sensitivity \gets flips / \texttt{len}(Prediction\_List)$ \Comment{\textcolor{blue}{Total flips normalized by the total number of labels.}}
    \State $Keyphrase\_sensitivity[k] \gets local\_sensitivity$
\EndFor

\State \textcolor{blue}{// Calculate the sensitivity of the input text $x$.}
\If{$Total\_words \neq 0$}
    \State $t \gets 0$ \Comment{\textcolor{blue}{A temporary variable.}}
    \For{$value$ \textbf{in} $Keyphrase\_sensitivity.\texttt{values}()$}
        \State $t \mathrel{+}= value$
    \EndFor
    \State $s \gets t / Total\_words$
\EndIf

\Return $s$
\end{algorithmic}
\end{minipage}
\caption{Algorithm to calculate the sensitivity of Input Text}
\label{algo:alg_kp_sensitivity}

\end{algorithm*}

