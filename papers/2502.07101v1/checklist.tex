\paragraph{Dataset.}Evaluating the global sensitivity values of words is challenging as they depend on a specific task and classifier combination. There is no straightforward way to determine the ground truth global sensitivity of different words. Therefore, we start with a template-generated dataset such as \textsc{CheckList} \cite{ribeiro-etal-2020-beyond}, where for a template, we know that changing specific keywords may cause a label flip while changing others should not have any effect on the label. We note that this still provides only an approximation of sensitivity values, as the global sensitivity value of a word (as defined in \S\ref{sec:definitions}) also depends on the target classifier.

\paragraph{Method.}The \textsc{CheckList} framework creates targeted test cases inspired by standard software engineering practices. Each test belongs to one of the categories -- MFT (\textit{Minimum Functionality Test}), INV (\textit{Invariance Test}), and DIR (\textit{Directional Expectation Test}). INV applies perturbations that preserve the original label, whereas DIR tests if the confidence of a label changes in a specific direction. These tests (INV \& DIR) consist of templates that vary according to the test type in consideration. For example, \textit{change names} test suite of \textit{INV} test type has sentences where we only vary a single word (name) in the complete sentence. In the texts shown below, only the name (\texttt{Alicia}) changes in the newly created example.

\begin{tcolorbox}[colback=white,colframe=red!75!black,left=1pt,right=1pt,top=0pt,bottom=0pt]
\textbf{\underline{Example: }}\\
    \-@JetBlue Thank you {\textcolor{blue}{Alicia}}!Exceptional Service\\
    \-@JetBlue Thank you {\textcolor{blue}{Haley}}!Exceptional Service 
\end{tcolorbox}
We utilize test types from INV and DIR to identify words with low and high sensitivity using our \textit{SMAB} framework. We experiment with two outer arm sampling strategies, UCB and TS. 
We use the perturbed Twitter US airline sentiment dataset \footnote{https://github.com/marcotcr/checklist} obtained from \textsc{Checklist}. We sample $\sim35k$ sentences from all the test types covering \textbf{38 test types}. We extract \textbf{8498} arms from the above sentences after some initial preprocessing (stopwords removal, lemmatization). Finally, we run our SMAB algorithm (\S\ref{sec:SMAB}) on the above set of arms and sentences to obtain the global sensitivity of all the words.

\begin{table}[t!]
    \resizebox{\columnwidth}{!}{%
    \large
    \begin{tabular}{c|l|c} 
        \hline
        \textbf{\textsc{Type}} & \multicolumn{1}{c|}{\textbf{\textsc{Example}}} & 
        \textbf{$\textbf{G}_s^w$}  \\ 
        \hline
        \textbf{INV}   & \begin{tabular}[c]{@{}l@{}}@united happens every time in and out of\\\textcolor{blue}{\textbf{<Newark>}}\end{tabular} & 0.0397          \\ 
        \hline
        \textbf{INV} & \begin{tabular}[c]{@{}l@{}}@JetBlue and of course that was supposed to say\\\textcolor{blue}{\textbf{<Jeremy>}}, not login.\end{tabular} & 0.0883          \\ 
        \hline
        \textbf{DIR}   & \begin{tabular}[c]{@{}l@{}}Thanks @JetBlue. Next up we will see how\\the slog from JFK to the city goes. You are\\\textcolor{red}{\textbf{<exceptional>}}.\end{tabular} & 0.6185          \\ 
        \hline
        \textbf{DIR}  & \begin{tabular}[c]{@{}l@{}}@USAirways Delays due to faulty engine light.\\Great work guys. Coming up on 2 hrs sitting\\on the plane. WorstAirlineInAmerica. You are\\\textcolor{red}{\textbf{<creepy>}}.\end{tabular} & 0.7574          \\ 
        \hline
        \textbf{INV} & \begin{tabular}[c]{@{}l@{}}@SouthwestAir I did ....itâ€™s just been such a\\\textcolor{red}{\textbf{disheartening}} experience for me and my family\\...and a lot of taxi money wasted. \textbf{<@MZ0ql9>}\end{tabular} & 0.9311   \\
        \hline
    \end{tabular}
    }
    \caption{A few examples from the \textsc{CheckList} test suite showing the highlighted words in the template and their respective estimated global sensitivity ($G_s^w$) using SMAB. The templated words are enclosed in <word>. The low and high sensitivity words are highlighted in \textcolor{blue}{blue} and \textcolor{red}{red} respectively. %\skp{I think we should put this instead of scatter plot}
    } 
    \label{tab:template_examples}
\end{table}

\paragraph{Observations.} We observe that the perturbation of words (arms) present in \textbf{INV} templates does not tend to change the label of the original sentence. In contrast, the words in \textbf{DIR} templates are more prone to flipping the label since they contribute to confidence score manipulation. Further analysis (details in Figure \ref{fig:scatter_plot} in the Appendix) shows that the words from \textit{DIR} templates have higher estimated global sensitivity and have a much wider spread of values ranging from 0 to 1. In contrast, as expected, the words from \textit{INV} are concentrated in the low-sensitivity range of (0-0.2).
Further, in Table \ref{tab:template_examples}, we present some qualitative examples from various test types, words, and their estimated global sensitivities.

\paragraph{Evaluation.} Sensitivity threshold is the value above which a word present in a sentence if perturbed, is highly likely to change the predicted label. To evaluate the performance of our framework, we propose a metric, \textbf{Sensitivity Attack Success Rate (SASR)}, calculated as follows. Given a test dataset, a word, $w$, from the set of all the words present in the dataset, $S_w$, the set of sentences in which the word is present,   and $G_s^w$, its estimated global sensitivity from our SMAB framework, if the word is above the sensitivity threshold and replacing the word with the predictions of a masked language model flips the predicted label in any one of the sentences from $S_w$, it is called a success. SASR is the fraction of all the words in a dataset above the sensitivity threshold that can flip the predicted label.

\begin{figure}[t]
    \includegraphics[width=\columnwidth, height=0.25\textheight]{figures/checklist_SASR_UCB_TS.png}
    \caption{Variation of SASR with sensitivity threshold on CheckList test dataset for UCB and TS. For UCB, words are only present in bins (0-0.1) and (0.9-1.0), hence SASR becomes constant after 0.1. It shows that Thompson Sampling proves to be a better sampling strategy for this task as compared to UCB.}
    \label{fig:checklist_SASR}
\end{figure}

We calculate SASR for a test set from \textsc{CheckList} templated dataset of 1800 sampled data points and plot SASR for different sensitivity thresholds for both the algorithms, UCB and TS, as shown in Figure \ref{fig:checklist_SASR}. We observe that the SASR\_TS increases as we increase the sensitivity threshold, which signifies that the words in the high sensitivity region ($0.7$-$1.0$) are responsible for the change in the predicted label. SASR\_UCB remains constant after threshold of 0.1 since the estimated sensitivity values from UCB lie only in two bins, ($0.0$-$0.1$) and ($0.9$-$1.0$). Additionally, we plot the number of words obtained at different sensitivity thresholds to ensure sufficient words are present to calculate SASR. For TS, we get $104$ words even above the sensitivity threshold of $0.9$, with around $83$ of these words able to produce a flip. Hence, it shows that the estimated global sensitivities from our SMAB framework, in a true sense, capture the impact of various words in a sentence for a particular task in a given dataset.
