Sensitivity has been commonly used as a measure for the \textit{complexity} of sequence classification tasks~\cite{hahn-etal-2021-sensitivity}. Sensitivity has also been used for the understanding and optimization of prompts in related paradigms such as in-context learning. \citet{lu-etal-2024-prompts} performs an analysis of accuracy and sensitivity, for different prompts in an \textit{ICL} setting and observes a negative correlation between them. \textit{FormatSpread} \cite{sclar2024quantifying} also presents a Multi-armed bandit framework for a model-agnostic evaluation of performance spread across different prompt formats. 
% SMAB distinguishes itself from these works by focusing on the study and comparison of model decision boundaries through the lens of word-level global sensitivities, across all NLP tasks.
Multiple research work have focused on the paradigm of adversarial text generation by perturbing safe input examples through gradient-based approaches \cite{ebrahimi-etal-2018-hotflip,cheng2020seq2sickevaluatingrobustnesssequencetosequence}. \citet{wallace-etal-2019-universal} deployed a gradient guided search over all tokens to extract \textit{universal adversarial triggers}, which are input-agnostic tokens to trigger a model. \citet{ribeiro-etal-2018-semantically} followed a similar approach by presenting simple and universal \textit{semantically equivalent adversarial rules} (SEARs) that create adversaries on safe inputs. Other approaches such as \citet{iyyer-etal-2018-adversarial} and \citet{roth2024constraintenforcingrewardadversarialattacks}, have delved into the training of paraphrase networks for controlled generation of attack examples, whereas \citet{xu2023llmfoolitselfpromptbased} proposed prompt-based adversarial attack to audit LLMs. 
% Our \textit{SMAB} framework builds on the paraphrase network training approach, by enforcing multiple constraints for ensuring label in-variance and semantic equivalence. Unlike previous work, we utilize the notion of \textit{global sensitivity} of individual tokens in the input example, to guide our adversarial model training.   
% Based on the notion of Boolean function sensitivity, \cite{hahn2021sensitivity} introduced a theoretical framework to measure the complexity of sequence classification tasks. The sensitivity of a function measures the count of distinct subsets within the input sequence that, when modified individually, lead to a change in the output. In sequence classification tasks, it measures the non-linearity of the decision boundary. Tasks with low sensitivity favor simple functions like linear classifiers, while high-sensitivity tasks demand more complex methods. The level of information in the input also plays a crucial role in sensitivity; insufficient information leads to high sensitivity, where a minor input change drastically alters the output. Conversely, redundant input information signifies stability and low sensitivity in the output. \\



% % There is a growing body of research that studies the correlation between sensitivity and accuracy under the paradigms of In-context learning (ICL), a popular few-shot learning method used with Large language models. \cite{lu2023prompts} analyzed the effect of prompts across different models and tasks and determined that some prompts work better than others because they reduce the level of sensitivity. They also demonstrate that sensitivity can function as an unsupervised proxy for model performance, and hence can be used to evaluate model performance without the need for labeled data. \cite{chen2024relation} finds that label bias in ICL distorts the true sensitivity. They observe that if a prediction is sensitive to small changes in the prompt, then it is likely to be incorrect, and hence, sensitivity can be used as a proxy to abstain from making predictions on such prompts.