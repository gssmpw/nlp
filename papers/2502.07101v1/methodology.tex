We formalize the definitions of the global sensitivity of a word for a given text classifier. Subsequently, we explain our proposed sensitivity estimation framework with examples.

\subsection{Problem Formulation}
Given an input space $X$ containing the input sentences and output space of possible labels $Y$, we have a pre-trained classifier \(f_\theta: X \rightarrow Y \) that maps the input text $ x = [w_1 \cdot w_2 \cdot w_3 \cdots w_n]  \in  X $ to a class $\hat{y} \in Y$. We are interested in finding the minimal subset of words to replace in $x$ (by contextually relevant words), such that for the new sentence $x'$, $f_\theta(x') \neq f_\theta(x)$.

\subsection{Definitions}
\label{sec:definitions}
\textbf{Local Sensitivity}:~~  We define the local sensitivity of a word for a specific input text ($x$). For an underlying classifier ($f_\theta$), local sensitivity estimates the relative importance of a word towards the predicted label ($f_\theta(x)$). We estimate singleton sensitivity \cite{hahn-etal-2021-sensitivity}\footnote{For subset sensitivity, \citet{hahn-etal-2021-sensitivity} captures the variance among predicted labels, we capture mean over flips of the predicted label from the original label.}, which is proportional to the number of flips of the predicted label when we replace a target word, say $w_i$, with contextually relevant words. \\
\\
\textbf{Global Sensitivity}: {\it Consider a text containing \(m\) words $W_x = \{w_1, w_2, \ldots w_m \}$. For an underlying classifier ($f_\theta$), we assume there exists a minimal subset of words $W_k \subseteq W_x$ that can be replaced to change the predicted label. The global sensitivity of a word provides a greedy heuristic to discover such a minimal subset. The higher the global sensitivity, the higher the chance that the word belongs to the minimal subset.} We estimate the global sensitivity of a word by aggregating the local sensitivity of words per sentence.  

\subsection{SMAB Framework}
\label{sec:SMAB}
Multi-armed bandits offer a simple yet powerful framework for algorithms to optimize decision-making over a given period of events. Our proposed framework SMAB can interpret the importance of all words (\textit{global sensitivity}) present in a given dataset for a particular task from a language modeling perspective. The framework is described below.\\
\\
\textbf{Multi-armed Bandit.} Our use of multi-armed bandits has two levels -- with words at the outer arms and sentences in the inner arm. The \textbf{outer arm} consists of all the words present in a dataset obtained after applying preprocessing techniques like removal of stopwords, removal of random URLs, and lemmatization. 
The outer arm is associated with a reward value termed as \textit{Global Sensitivity} of a word $w$, which provides a greedy heuristic to discover a minimal subset of words in a sentence that needs to be changed to flip the predicted label. The \textbf{inner arm} comprises all the sentences from the dataset in which a particular word \(w\) from the outer arm is present. We denote ${S}^{w}$ as the set of all the sentences in which a particular
outer arm or word (say \(w\)) is present. Each inner arm also has a reward value, defined as \textit{Local Sensitivity} for the inner arm.\\
\\
\textbf{Calculating Local Sensitivity: } Local Sensitivity ${L}_w$ for a word $w$ at each step $t$ is calculated by perturbing sentences using \textit{sample-replace-predict} strategy. First, we \underline{sample} a word from all the words present in the outer arm using either \textbf{Upper Confidence Bound1 (UCB)} \cite{auer2002finite} or \textbf{Thompson Sampling (TS)} \cite{thompson1933likelihood}. We use \textbf{TS} to draw a sample with maximum value from the Beta distribution of sensitivity of all words using:
\begin{equation}
    w^*_{t+1} = \operatorname*{argmax}_{w \in W} \left( Beta(\alpha, \beta) \right),
\end{equation}
where $\alpha$ $\in$ (0,1) and $\beta = 1-\alpha$. Then, we  \underline{replace} the word $w$ in all the sentences of the inner arm using predictions of a masked language model, ensuring that the resulting sentence with the new word $w'$ remains coherent and semantically sound. This process is repeated $N$ times (here $N=10$). If, across the $N$ replacements, the new word $w'$ matches the original word $w$, we discard that instance. Let $P_w$ denote the set of all valid instances i.e., instances that have not been discarded. Lastly, we use the target model to \underline{predict} the labels of the newly constructed ${P}_w$ sentences. By utilizing the target model predictions of the ${P}_w$ sentences, we select a randomly sampled sentence $s_1$ $\in$ $P_w$ with reward $r_1$ (sentence-level local sensitivity) and a sentence $s_2$ $\in$ $P_w$ with the highest reward $r_2$ is selected. The local sensitivity for a word $L_w$ is calculated as the convex combination of rewards from $s_1$ and $s_2$,  $\epsilon$ \(\in\) (0, 1) \begin{equation}
    L_w = \epsilon\:*r_1 + (1-\epsilon)\:*r_2,
\end{equation} 
\textbf{Calculating Global Sensitivity:} Global sensitivity value (represented as $G^{w}_{t}$) for a particular word $w$ at a particular step/iteration $t$ is calculated as follows: \begin{equation}
    G^{w}_{t} = \frac{(N^w\:*\:G^{w}_{t - 1}\: + \: L_{w})}{ 1 \:+ \:N^w},
\end{equation}
where $N^w$ represents the number of times the word $w$ has been picked up till now. $G^{w}_{t}$ \(\in\) (0,1). We assign $L_{w}$ to $1$ if $L_{w}>0$. We minimize the total regret $R_t$ over the total number of iterations. The estimation of total regret and the pseudocode of the complete algorithm is presented in Algorithm~\ref{algo:SMAB}.

\subsection{SMAB Training Details}
We initialize the global sensitivity values of all the words (arms) present in a dataset with $Beta(\alpha, \beta)$, where $\alpha = Random(0,0.5)$ and $\beta = (1 - \alpha)$. We use $\epsilon$ in the convex addition function as 0.9. We iterate with the total number of steps, $N = 2,00,000$. We report the  \textit{\#datapoints} used for training, inner and outer arm details in Table~\ref{tab: SMAB_training_details}. 
%Details of the target classifier being used for each dataset/task in subsequent sections can be found in Table~\ref{tab: SMAB_training_details}