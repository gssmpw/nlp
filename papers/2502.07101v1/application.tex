\subsection{Datasets} 
We evaluate the utility of word-level sensitivities in \textit{adversarial example generation} using the Sentiment Analysis task. We conduct experiments on the Cornell Movie Review dataset (RT dataset) \cite{pang-lee-2005-seeing} and SST-2 dataset~\cite{socher2013recursive}. We extend two perturbation-based (or paraphrase) attack baselines using sensitivities predicted from our SMAB framework.

\subsection{Evaluation Metrics}
\textbf{Attack Success Rate:} Following \citet{wang-etal-2018-glue}, we evaluate our attack methods using the Attack Success Rate (\textbf{ASR}). Given a dataset $\mathcal{D} = \{(x^{(i)}, y^{(i)})\}_{i=1}^{N}$ consisting of $N$ pairs of samples $x^{(i)}$ and ground truth labels $y^{(i)}$, for an adversarial attack method $A$ that generates an adversarial example $A(x)$ given an input $x$ to attack a surrogate model $f$, ASR is calculated as follows: \\
\begin{equation}
    \sum_{(x,y) \in \mathcal{D}}
    \frac{\mathbbm{1} \big[ (f(A(x)) \neq y)
                      \land (f(x) =y)\big]}
                      {\mathbbm{1} \big[ f(x) = y \big]}
    \label{eq:ASR}
\end{equation}\\
\textbf{After Attack Accuracy:} It measures the robustness of a model to an adversarial attack, A low After-attack accuracy represents a highly vulnerable model to adversarial attacks. It is calculated as follows: \\
\begin{equation}
    \frac{1}{|\mathcal{D}|} \sum_{(x,y) \in \mathcal{D}}
    \mathbbm{1} \big[ (f(A(x)) = f(x) = y) \big]
    \label{eq:AAA}
\end{equation}
% \begin{align}
%     \frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} 
%     \mathbbm{1} \big[ & (L_{v}(x') = L_{g}(x) = L_{v}(x) ) \big]
%     \label{eq:AAA}
% \end{align}

%   Perturbation Instructions Table %
\begin{table*}[t!]
\resizebox{\textwidth}{!}{%
\centering
    \begin{tabular}{l | l }
    \toprule
    \textbf{\begin{tabular}[c]{@{}l@{}}Perturb\\ Type\end{tabular}} &
    \textbf{\begin{tabular}[c]{@{}c@{}}Perturbation instructions \end{tabular}} \\ 
    \midrule
    $W1$ & Replace at most two words in the sentence with synonyms.                    \\ 
    \midrule
    $W2$ & Choose at most two words in the sentence that do not contribute to the meaning of the sentence and delete them.                    \\ 
    \midrule
    $W3$ &    Add at most two semantically neutral words to the sentence.                  \\ 
    \midrule
    $W4$ (Ours) &   {\begin{tabular}[c]{@{}l@{}}   For a given sentence, there always exists a minimal subset of words that need to be replaced to flip the label of the \\sentence while preserving its semantic meaning. Global Sensitivity of a word provides a greedy heuristic to discover \\such a minimal subset. The higher the global sensitivity, the higher the chance that the word belongs to the minimal \\subset. Given the minimal subset of the words ["\texttt{[Word1]}", "\texttt{[Word2]}"] and their global sensitivity values in the \\decreasing order [\texttt{GS1}, \texttt{GS2}], replace these words in the original sentence with semantically close words.\end{tabular}}              \\
    \midrule
    $W5$ (Ours) &      {\begin{tabular}[c]{@{}l@{}}The words "\texttt{[Word1]}" and "\texttt{[Word2]}" are highly sensitive in the given sentence, and perturbing either "\texttt{[Word1]}", \\"\texttt{[Word2]}", or both can change the label of the sentence while preserving the semantic meaning of the new sentence \\as that of the original.\end{tabular}}              \\
    \midrule
    $W6$ (Ours) &      Add at most two semantically close words to the sentence, replacing the words "\texttt{[Word1]}" or "\texttt{[Word2]}, or both.\\
    \bottomrule
\end{tabular}%
}
\caption{Perturbation instructions used in Prompt Guidance: In \textit{$W4$}, \textit{$W5$} and \textit{$W6$}, \texttt{[Word1]} is replaced with the most sensitive word in the sentence, while \texttt{[Word2]} is replaced with the second most sensitive word. In \textit{$W4$}, \texttt{GS1} and \texttt{GS2} represent the global sensitivity values of the most sensitive word and the second most sensitive word. 
}
\label{tab:Attack_Guidance_perturbation_instrs}
\end{table*}

\subsection{PromptAttack using Global Sensitivities}
Our first baseline, PromptAttack \cite{xu2023llmfoolitselfpromptbased}, is a prompt-based adversarial attack that can effectively audit the adversarial robustness of a target LLM. For a target LLM, PromptAttack prompts the same LLM with a perturbation instruction such that the LLM generates an adversarial sample for a given input text, effectively fooling itself. Authors propose character-level, word-level, and sentence-level perturbations. As we capture word-level sensitivities, we define three word-level perturbation instructions utilizing the global sensitivity values obtained from the SMAB framework on the SST-2 dataset. Prompts for the sentiment classification task on SST-2 can be found in Table~\ref{tab:sentiment_classification_prompt} and all perturbation instructions are outlined in Table \ref{tab:Attack_Guidance_perturbation_instrs}. We experiment with GPT-3.5 (\texttt{gpt-35-turbo}) and Llama-2-7B as the target LLMs. Our instructions primarily rely on perturbing the high-sensitive words in the input text. We apply the \textit{fidelity filter} used in \citet{xu2023llmfoolitselfpromptbased} with the following constraints: BERTScore $\geq 0.92$ \cite{zhang2019bertscore} and Word Modification Ratio $\le 1.0$ \cite{wang-etal-2018-glue}.\\
\paragraph{Results.}As shown in Table \ref{tab:avg_ASR_ZS_SST}, using GPT-3.5 as the target LLM, we compute sensitivity values from our SMAB framework with six different LLMs serving as classifiers, referred to as \textbf{SMAB LLMs}: BERT, GPT-3.5, Llama-2-7B, Llama-2-13B, Llama-3.1-8B and Qwen-2.5-7B. For all four SMAB LLMs,  our word level perturbation instructions outperform the baselines $W1$, $W2$, and $W3$. We obtain the best ASR of \textbf{52.06\%} and the lowest After Attack Accuracy of \textbf{44.23\%} when we use Qwen-2.5-7B as the SMAB LLM. It demonstrates improvement over the baselines by a margin of \textbf{15.58\%}. Likewise, with Llama-2-7B as both the target LLM and the SMAB LLM, perturbation type $W5$ outperforms the top baseline $W3$. With Qwen-2.5-7B as both the target LLM and SMAB LLM, out perturbation type $W6$ outperforms the top baseline $W3$. The results show that our instructions utilizing high-\textit{sensitive} words consistently outperform the baselines and can generate high-quality adversarial examples as shown in Table \ref{tab:Qual_Examples_promptAttack}.


\begin{table}[!ht]
\resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule
    \textbf{\begin{tabular}[c]{@{}l@{}}SMAB \\ LLM\end{tabular}}  &
    \textbf{\begin{tabular}[c]{@{}l@{}}Perturb \\ Type \end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}} ASR $\,\uparrow$ \end{tabular}}  & 
    \textbf{\begin{tabular}[c]{@{}l@{}} After Attack \\Accuracy $\,\downarrow$ \end{tabular}} \\
    \hline

    \multicolumn{4}{c}{\rule{0pt}{2ex} Target LLM $\rightarrow$ \textbf{GPT-3.5}}\\
    \multicolumn{4}{c}{\rule{0pt}{2ex} (Before Attack Accuracy $-$ 92.16\%)}\\
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\ding{55} \end{tabular}} 
     & $W1$ & 22.20 & 72.09           \\ 
    &  $W2$  & 24.63  & 69.73        \\ 
    &  $W3$  &  36.48 & 58.68   \\ 
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}BERT \end{tabular}}  
    &   $W4$    & \cellcolor{green1} 37.32  & \cellcolor{green1}58.20   \\
    &   $W5$    & \cellcolor{green1}38.34   & \cellcolor{green1}56.96   \\
    &   $W6$    & \cellcolor{green1}48.23   & \cellcolor{green1}47.84   \\
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Llama-2-7B \end{tabular}}   
    & $W4$ &   \cellcolor{green1}37.49 &  \cellcolor{green1}57.85  \\
    &   $W5$  & \cellcolor{green1}38.20 & \cellcolor{green1}57.04   \\
    &   $W6$  &   \cellcolor{green1}48.26 & \cellcolor{green1}47.80  \\
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Llama-2-13B \end{tabular}}  
    & $W4$ &  \cellcolor{green1}38.61  & \cellcolor{green1}56.95  \\
    &   $W5$  & \cellcolor{green1}38.56  & \cellcolor{green1}56.79 \\
    &   $W6$  &  \cellcolor{green1} 50.09 & \cellcolor{green1} 46.23 \\
    \hline

    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Llama-3.1-8B \end{tabular}}  
    & $W4$ &   \cellcolor{green1}48.16 &  \cellcolor{green1}47.80 \\
    &   $W5$  & \cellcolor{green1}39.96  & \cellcolor{green1}55.40 \\
    &   $W6$  & \cellcolor{green1}46.98  & \cellcolor{green1}48.90 \\
    \hline

    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Qwen-2.5-7B \end{tabular}}  
    & $W4$ &   \cellcolor{green1}51.62 &  \cellcolor{green1}44.61 \\
    &   $W5$  &   \cellcolor{green1}38.87 & \cellcolor{green1}56.37 \\
    &   $W6$  & \cellcolor{green2}52.06 & \cellcolor{green2}44.23 \\
    \hline

    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GPT-3.5 \end{tabular}}
    &   $W4$    & \cellcolor{green1}37.94   & \cellcolor{green1}57.55   \\
    &   $W5$    & \cellcolor{green1}39.40   & \cellcolor{green1}56.09   \\
    &   $W6$    & \cellcolor{green1}46.06   & \cellcolor{green1}49.95   \\
    \hline

    \multicolumn{4}{c}{\rule{0pt}{2ex} Target LLM $\rightarrow$ \textbf{Llama-2-7B}}\\
    \multicolumn{4}{c}{\rule{0pt}{2ex} (Before Attack Accuracy $-$ 92.32\%)}\\
    \hline    

    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\ding{55} \end{tabular}} 
     & $W1$ & 56.19 & 36.13          \\ 
    &  $W2$  & 43.97 & 48.35       \\ 
    &  $W3$  & 60.20 & 32.11   \\ 
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Llama-2-7B \end{tabular}}   
    & $W4$ & 30.55   & 61.76    \\
    &   $W5$  &  \cellcolor{green2}61.69 & \cellcolor{green2}30.62   \\
    &   $W6$  &  44.85  & 47.46 \\ \hline

    \multicolumn{4}{c}{\rule{0pt}{2ex} Target LLM $\rightarrow$ \textbf{Qwen-2.5-7B}}\\
    \multicolumn{4}{c}{\rule{0pt}{2ex} (Before Attack Accuracy $-$ 76.38\%)}\\
    \hline

    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\ding{55} \end{tabular}} 
     & $W1$ & 36.70 & 39.68         \\ 
    &  $W2$  & 14.79 & 61.58       \\ 
    &  $W3$  & 28.78 &  47.71  \\ 
    \hline
    
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Qwen-2.5-7B \end{tabular}}   
    & $W4$ &  28.44 &  47.94   \\
    & $W5$  & 36.01 &  40.25 \\
    & $W6$  &  \cellcolor{green2}37.16  &  \cellcolor{green2}39.22 \\

    \bottomrule
\end{tabular}%
}
\caption{PromptAttack results on SST-2 dataset. \textbf{SMAB LLM} is the LLM used for calculating sensitivity values from the SMAB framework. ASR and After Attack Accuracy are in ($\%$). All \textbf{perturb types} that outperformed the highest baseline score are highlighted in \cellcolor{green1}{green}, and the best-performing perturb type is highlighted in \cellcolor{green2}{green}.}
\label{tab:avg_ASR_ZS_SST}
\end{table}


\subsection{ParaphraseAttack using Local Sensitivities}
\label{sec:paraphraseAttack}
\citet{roth2024constraintenforcingrewardadversarialattacks} studies adversarial attacks on text classifiers using an encoder-decoder paraphrase model (T5), trained to generate adversarial examples using a reinforcement learning algorithm \cite{Williams2004SimpleSG} with a \textit{constraint-enforcing reward} that helps generation of semantically close, label invariant and grammatically correct adversarial examples. We modify the original reward function by incorporating an additional \textbf{Sensitivity reward}, defined as the difference between the sensitivity of the input text and the sensitivity of the generated text. Details of the modified reward function and the keyphrase sensitivity calculation can be found in Appendix~\ref{appendix:paraphraseattack_details}. During inference, we generate $8$ paraphrases for each input text using various decoding mechanisms under constraints used in \citet{roth2024constraintenforcingrewardadversarialattacks} and use the target classifier to predict the label. We used the RT dataset to conduct experiments using the same hyperparameter and design choices used in the referenced work. The details of the different models used, including the paraphrase model are provided in Table \ref{tab:various_models_used_paraphrase}.



\paragraph{Human Evaluation.} We also perform a human evaluation of the generated adversarial examples. Two co-authors (graduate CS students trained in NLP) quantify the quality of generated paraphrases on three aspects. We use the human evaluation metrics proposed in \citet{das-etal-2024-low}. Specificity (\textbf{SPE}) measures how specific the aspects obtained in the generated text are in response to the input. Grammaticality (\textbf{GRM}) measures how grammatically correct are the paraphrased sentences. Choose-or-Not (\textbf{CHO}) represents whether a human considers the generated paraphrase adversarial. Humans evaluate SPE and GRM on a Likert scale of $1$-$5$ (least to most). CHO is $1$ if a human considers the generated paraphrase adversarial, else $0$. We compute the average score across all instances for each metric. For human evaluation, we selected up to $100$ unique paraphrases generated by each model variation that flipped the label of a text. The number of unique paraphrases varies across strategies, as detailed in Table~\ref{tab:UniqueParaphrase_RT_appendix}. 

\paragraph{Results.} Table~\ref{tab:paraphrase_attack_RT} shows that incorporating \textit{sensitivity reward} consistently results in a higher \textbf{ASR} and \textbf{CHO} for a particular strategy and temperature combination. Each strategy uses a combination of decoding mechanisms and whether the sensitivity reward is used or not. We achieved the highest ASR of \textbf{93.03\%} with DistilBERT~\cite{sanh2019distilbert} as the SMAB Model, \textit{beam search} decoding strategy and a temperature of 1.00 that exceeds the corresponding baseline variant by \textbf{74.09\%} and the best baseline variant by \textbf{62.39\%}. Similarly, we achieved the best CHO using BERT as the SMAB model that exceeds the corresponding baseline variant by \textbf{20.14\%} and the best baseline variant by \textbf{12.00\%}. We present the qualitative examples in the Table \ref{tab:qual_examples_PP}.

\begin{table}[!htb]
\resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{l c c | c c c}
    \toprule
    \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Strategy\end{tabular}}} &
    \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Temp \end{tabular}}} &
    \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}ASR \(\uparrow\) \end{tabular}}} & 
      \textbf{\begin{tabular}[c]{@{}l@{}}SPE \(\uparrow\) \end{tabular}} & 
      \textbf{\begin{tabular}[c]{@{}l@{}}GRM \(\uparrow\) \end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}CHO \(\uparrow\) \end{tabular}} \\ 
    & & & \multicolumn{3}{c}{\rule{0pt}{2ex} \textbf{(Human Evaluation)}}\\
    \hline
    

    \multicolumn{6}{c}{\rule{0pt}{2ex} SMAB Model \(\rightarrow\) \ding{55}}\\
    \hline

    DBS + (\ding{55}) & 0.85  & 30.64  & 3.43 & 3.65  & 27.00                  \\
    BS + (\ding{55}) & 0.85 & 17.27  & 3.50 & 3.85  & 22.58                      \\ 
    BS + (\ding{55})  &1.00 &  18.94  & 3.72 & 3.87  & 19.11                  \\
    BS + (\ding{55})  & 1.15 & 14.76  & 3.64 & 3.86 & 18.86                        \\ 
    \hline

    \multicolumn{6}{c}{\rule{0pt}{2ex} SMAB Model \(\rightarrow\) \textbf{BERT}}\\
    \hline
    DBS + (\ding{51})  & 0.85 & \cellcolor{green1}50.41  &  3.28&  3.35&   21.00 \\
    BS + (\ding{51}) & 0.85 & 19.77  & 3.38 & \cellcolor{green1}3.88 &  25.35 \\
    BS + (\ding{51})  & 1.00  & 22.00  &  3.46&  3.65&   24.05 \\
    BS + (\ding{51}) & 1.15 & \cellcolor{green1}50.97  & 3.52 & 3.45 &  \cellcolor{green2}39.00 \\
    \hline

    \multicolumn{6}{c}{\rule{0pt}{2ex} SMAB Model \(\rightarrow\) \textbf{DistilBERT}}\\
    \hline
    DBS + (\ding{51})   & 0.85  & \cellcolor{green1}88.85  &  \cellcolor{green2}3.88&  \cellcolor{green2}3.90&   22.00                      \\
    BS + (\ding{51})  & 0.85  & \cellcolor{green1}38.99  &  3.44&  3.57&   \cellcolor{green1}37.00                     \\ 
    BS + (\ding{51})   & 1.00  & \cellcolor{green2}93.03  &  \cellcolor{green1}3.86&  \cellcolor{green1}3.88&   26.00                      \\
    BS + (\ding{51})  & 1.15  & 20.89  &  3.40&  3.74&   22.66                     \\ 
    \bottomrule
\end{tabular}%
}
\caption{ParaphraseAttack Results on RT dataset. The strategy represents the combination of decoding mechanism (\textbf{DBS}: Diverse Beam Search, \textbf{BS}: Beam Search) and usage of sensitivity reward (\ding{55}: No sensitivity, \ding{51}: Sensitivity using BERT/DistilBERT). Temp: decoding temperature. ASR/CHO in (\%). All variants outperforming the highest baseline score are highlighted in \colorbox{green1}{green} and the best-performing variant is highlighted in \colorbox{green2}{green}.}
\label{tab:paraphrase_attack_RT}
\end{table}
