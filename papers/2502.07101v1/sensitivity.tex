Using our multi-armed bandit framework (in \S\ref{sec:SMAB}), we calculate the word sensitivities for each word in a given dataset based on the model predictions (in an unsupervised way). Here, we aim to quantify the correlation between the accuracy of various models and the difference between their corresponding sensitivity distributions obtained from the SMAB framework. %We compare the relative drop in accuracy between the two models and the KL divergence of the sensitivity distributions from the models. 
We experiment with two different settings - (i) Correlation across languages (same model) and (ii) Correlation within language (different models). We compare the KL divergence of the sensitivity distributions from two different models (/languages) with the relative drop in accuracy between models (languages). We compute
$
\label{eq:kld}
D_{\text{KL}}(P \parallel Q) = \sum_{i=1}^{N} P(i) \log \frac{P(i)}{Q(i)}
$
, where $P$ and $Q$ represent sensitivity distributions obtained from two different runs of SMAB. $N$ represents the number of sensitivity bins (here $10$). We calculate accuracy on the same dataset using ground truth labels.
We hypothesize that KL divergence is negatively correlated with accuracy drop for both the settings, which signifies that the sensitivity distributions may serve as an \textit{unsupervised proxy for accuracy} for a given target classifier when gold labels are absent.

\subsection{Tasks \& Datasets}
\textbf{Hate Speech Classification Task.}~~~
Hate Classification is a challenging task that contains many words spanning different sensitivity bins (contains highly-sensitive target words). We selected hate speech classification datasets from various sources, covering nine languages - English, Bengali, French, German, Greek, Hindi, Italian, Spanish, and Turkish. Hereafter, we refer to this dataset as the mHate dataset.
\\\noindent
\textbf{Natural Language Inference Task.}~~~
We use the \textit{XNLI} \cite{conneau-etal-2018-xnli} dataset, a cross-lingual NLI dataset for this task, which expands upon the English-based \textit{MultiNLI} dataset \cite{williams-etal-2018-broad} by translation into 14 languages. We select five languages - English, French, Greek, Hindi, and Spanish to evaluate our hypothesis.

\subsection{Correlation Across Languages}
Robust evaluation and benchmarking of low-resource languages have always been challenging because of the lack of sufficient and reliable evaluation datasets~\cite{ahuja2022beyond}, \cite{ahuja2022multi}. SMAB may be highly effective in the evaluation and benchmarking of low-resource languages. We experiment with various languages of mHate and XNLI datasets. We quantify the relative zero-shot drop in accuracy and attempt to correlate it with the KLD. We utilize the mBERT~\cite{devlin2019bert} classifier for mHate and mDeBERTa~\cite{he2021debertadecodingenhancedbertdisentangled} for XNLI. Given a classifier, we get the predictions on the mentioned language split of the dataset. Then, we compare the KLD between sensitivity distributions of different languages from a base language (English, in our case) and the model's accuracy in various languages. We plot KLD v/s accuracy to study nine languages for a particular target classifier under study. 

\subsection{Results}
From Figures \ref{fig:mbert_acorss_languges} and \ref{fig:mdeberta_acorss_languges}, we observe a negative correlation between KL Divergence and Accuracy on the test set. For mHate, we calculate KLD between sensitivity distributions obtained from our SMAB framework across eight languages and sensitivity distribution for English. We observe a negative correlation with Pearson Correlation Coefficient(R) of $-0.75$ (statistically significant with \textit{p-value} of $0.03$). Similarly, for XNLI, we perform it for $5$ different languages against English and obtain a Correlation Coefficient of $-0.91$ (statistically significant with \textit{p-value} of $0.03$). We also carry out experiments for \textbf{within the language} setting using various pre-trained classifiers and obtain a similar correlation (Appendix \ref{sec:appendix:subsection:kld_vs_acc}). The results show that KLD between sensitivity distributions is \textit{negatively correlated} with the accuracy of a target classifier. Hence, the sensitivity values obtained from SMAB act as an \textit{unsupervised proxy for accuracy} for a given target classifier on a dataset when the gold labels are absent.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/kld_vs_accuracy/kld_ts_mbert_mhate.png}
        \caption{KL Divergence v/s accuracy across languages of mHate dataset using mBERT.}
        \label{fig:mbert_acorss_languges}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/kld_vs_accuracy/kld_ts_mdeberta_xnli.png}
    \caption{KL Divergence v/s accuracy across languages of XNLI dataset using mDeBERTa.}
    \label{fig:mdeberta_acorss_languges}
\end{figure}

