\documentclass[12pt]{article}



\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathrsfs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{microtype} 
\usepackage{color}
\usepackage{hhline}
\usepackage{authblk}



% \usepackage[]{footmisc}
%\usepackage[symbol]{footmisc}
% \usepackage{ulem}

%----margin
\topmargin 0.1cm \oddsidemargin 0.01cm
\textwidth 15.9cm

%\flushbottom
\textheight 22.2cm
%------------------------------------

\usepackage{titletoc}

%\usepackage[notref,notcite]{showkeys}
%\usepackage{citeref}
\usepackage{indentfirst}
%\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{txfonts}
\usepackage{bbm}
\usepackage{braket}
\usepackage{tabularx}
\textwidth=15.54truecm
\textheight=21truecm
\oddsidemargin 0cm
\evensidemargin 0cm

\def\cma{\color{magenta}}%color
\def\cbl{\color{blue}}
\def\crd{\color{red}}
\def\ct{\color{teal}}
\def\cog{\color{orange}}
\def\cyg{\color{yellowgreen}}

\usepackage{natbib,tcolorbox}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{cancel}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\input{macro}
\hypersetup{
    colorlinks=true,
    linkcolor=linkcolor,
    citecolor=blue,
    filecolor=magenta,      
    urlcolor=black,
}

\begin{document}


\title{{Towards a  Learning Theory of Representation Alignment}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Francesco Insulla$^1$,
% Shuo Huang$^2$,
% Lorenzo Rosasco$^{2,3,4}$
%  }

\author{Francesco Insulla \thanks{Institute of Computational and Mathematical Engineering Stanford University, Stanford, CA, USA. Email:\href{mailto:franinsu@stanford.edu}{franinsu@stanford.edu} }$\quad$
Shuo Huang \thanks{Istituto Italiano di Tecnologia, Genoa, Italy. Email: \href{mailto:shuo.huang@iit.it}{shuo.huang@iit.it}}$\quad$
Lorenzo Rosasco \thanks{MaLGa Center – DIBRIS – Università di Genova, Genoa, Italy; also CBMM – Massachusets Institute of Technology,
USA; also Istituto Italiano di Tecnologia, Genoa, Italy. Email: \href{mailto:lrosasco@mit.edu}{lrosasco@mit.edu} }
}

\date{}


% \thanks{

% }

% \date{$^1$ Institute of Computational and Mathematical Engineering\\ Stanford University, Stanford, CA, USA \\
% $^2$ IIT$@$MIT, Istituto Italiano di Tecnologia, Genoa, Italy\\
% $^3$ MaLGa Center - DIBRIS - Universit\`a di Genova, Genoa, Italy\\
% $^4$ CBMM - Massachusets Institute of Technology\\ Cambridge, MA, USA\\}

\maketitle

\begin{abstract}
It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations in the context of a task. Our main contribution here is relating properties of stitching to the kernel alignment of the underlying representation. 
% Numerical results are provided to illustrate these theoretical findings. 
Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.
% It has recently been argued that representations in AI models might be becoming aligned over time and across multiple domains. Empirical analysis have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations. Our main contribution here is relating the generalization properties of stitching to the kernel alignment of the underlying representation. Numerical results are provided to illustrate these theoretical findings. Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.
%In recent years, there has been increasing interest in multi-modal learning, which integrates information from different sources such as vision, language, and audio to achieve better results. While multi-modal learning has shown promising outcomes, mathematically measuring and evaluating representation alignment across different modalities remains an open challenge. In this paper, we derive several representation alignment matrices using definitions from kernel alignment, distance alignment, independence testing, and learning theory, demonstrating their equivalence and correlations. Additionally, we introduce the concept of stitching mathematically, a technique used in (uni/multi-modal) transfer learning to improve model efficiency. We further introduce the stitching error between different modalities and, to the best of our knowledge, present the first theoretical stitching error bound based on misalignment error. Finally, we present heuristic empirical results demonstrating the relationship between stitching error and alignment error.
\end{abstract}
\section{Introduction}
% In recent years, as AI systems have grown in scale and performance, the ability to learn across different modalities such as language and vision is likely to play a central role in building more capable, flexible, and generalizable systems, including CLIP \citep{radford2021learning}, VinVL \citep{zhang2021vinvl}, FLAVA \citep{singh2022flava}, OpenAI's GPT4 \citep{achiam2023gpt}, Google's Gemini \citep{team2023gemini} and so on. These architectures are trained on diverse datasets containing both images and text to produce embeddings that can be used for downstream tasks in either modality, or tasks that require both.  Ensuring that features from, say, images and text are aligned in a meaningful way is essential for models to effectively reason about both.  This engineering choice poses interesting questions about the tradeoff between unimodal and multimodal modelling. \textbf{(1) while multi-modal may give access to greater scale through dataset size and computational efficiency, to what extent could the alignment of representations across modalities be a bottleneck? How well do features learned from different modalities correspond to eachother? How do we mathematically measure and evaluate this alignment and feature learning across modalities?}


In recent years, as AI systems have grown in scale and performance, attention has moved towards universal models that share architecture across modalities. Examples of such systems include CLIP \citep{radford2021learning}, VinVL \citep{zhang2021vinvl}, FLAVA \citep{singh2022flava}, OpenAI's GPT-4 \citep{achiam2023gpt}, and Google's Gemini \citep{team2023gemini}. These models are trained on diverse datasets containing both images and text and yield embeddings that can be used for downstream tasks in either modality or for tasks that require both modalities. The emergence of this new class of multimodal models poses interesting questions regarding alignment and the trade-offs between unimodal and multimodal modeling. While multimodal models may provide access to greater scale through dataset size and computational efficiency, how well do features learned from different modalities correspond to each other? How do we mathematically quantify and evaluate this alignment and feature learning across modalities?

Regarding alignment, \cite{huh2024position} observed that as the scale and performance of deep networks increase, the models' representations tend to align. They further conjectured that the limiting representations accurately describe reality - known as \textit{Platonic representation hypothesis}. Their analysis also suggests that alignment correlates with performance, implying that improving the alignment of learned features across different modalities could enhance a model’s generalization ability. However, alignment across modalities has yet to be evaluated in a more interpretable manner, and theoretical guarantees of alignment under realistic assumptions are still lacking.

{
% There exist many measures of representation alignment. \cite{cristianini2001kernel} introduced the concept of kernel alignment by comparing the covariance of two kernel matrices $K_{1,n}, K_{2,n}$ with frobenius norm

\begin{figure}[h]
{
\usetikzlibrary{positioning, shapes.geometric, arrows.meta}
\begin{center}
 \begin{tikzpicture}[
    node distance=0.5cm and 0.5cm,
    every node/.style={align=center},
    arrow/.style={-Stealth, thick},
    thick,
    % ellipse/.style={ellipse, draw, minimum width=2.5cm, minimum height=1.5cm} % Set fixed size for ellipses
    ]

% Nodes
\node (reality) [ellipse, draw] {Reality\\($\Xi$, $\xi$)};
\node (data1) [ellipse, draw, below left=of reality] {Objects/data\\($\cX_1$, $\mu_1$)};
\node (data2) [ellipse, draw, below right=of reality] {Objects/data\\($\cX_2$, $\mu_2$)};
% \node (rep1) [ellipse, draw, below=of data1] {Representation \\ space \,$\mathcal{F}_1$};
% \node (rep2) [ellipse, draw, below=of data2] {Representation\\ space\,$\mathcal{F}_2$};
\node (rep1) [ellipse, draw, below=of data1] {Representation \\ \,$(\mathcal{Z}_1,\lambda_1)$};
\node (rep2) [ellipse, draw, below=of data2] {Representation \\ \,$(\mathcal{Z}_2,\lambda_2)$};
\node (output1) [ellipse, draw, below=of rep1] {Output\\($\mathcal{Y}_1$, $\nu_1$)};
\node (output2) [ellipse, draw, below=of rep2] {Output\\($\mathcal{Y}_2$, $\nu_2$)};
% Arrows
\draw[->, thick] (reality) -- node[left] {$m_1$} (data1);
\draw[->, thick] (reality) -- node[right] {$m_2$} (data2);
\draw[->, thick] (data1) -- node[left] {$f_1$} (rep1);
\draw[->, thick] (data2) -- node[right] {$f_2$} (rep2);
\draw[->, thick] (rep1) -- node[left] {$g_1$} (output1);
\draw[->, thick] (rep2) -- node[right] {$g_2$} (output2);
% Curved arrow from rep1 to rep2
\draw[arrow] (rep1) to[out=20, in=160] node[above] {{$s_{1,2}$}} (rep2);
% Curved arrow from rep2 to rep1
\draw[arrow] (rep2) to[out=200, in=-20] node[below] {{$s_{2,1}$}} (rep1);
\draw[arrow] (data1.west) to[ looseness=1.5, bend right] node[left] {{$h_1$}} (output1.west);
\draw[arrow] (data2.east) to[looseness=1.5, bend left] node[right] {{$h_2$}} (output2.east);
\end{tikzpicture}
\end{center}
}
\caption{Diagram illustrating the process of multi-modal learning. It contains spaces and measures of reality, objects/data, representation, and outputs as well as the functions connecting them. A detailed explanation of these symbols is in Section \ref{sec:settings}.}
\label{fig:wholePic}
\end{figure}


One way to quantify alignment is by kernel alignment, introduced by \cite{cristianini2001kernel}, which evaluates the correlation of two kernel matrices $K_{1,n}, K_{2,n}$ through Frobenius norms 

\[\widehat{A}(K_{1,n},K_{2,n}) = \frac{\langle K_{1,n}, K_{2,n}\rangle_F}{\sqrt{\langle K_{1,n}, K_{1,n}\rangle_F\langle K_{2,n}, K_{2,n}\rangle_F}}.\] Following this direction, methods like Centered Kernel Alignment (CKA) \citep{kornblith2019similarity} and Singular Vector Canonical Correlation Analysis (SVCCA) \citep{raghu2017svcca} were developed to compare learned representations. 
% Another natural choice for quantifying alignment given two metrics is
% \[
%     D(d_1,d_2) = \int  (d_1^2(x,x') - d_2^2(x,x'))^2 d\mu(x)d\mu(x').
% \]
Another class of metrics is derived from independence testing, including the Hilbert-Schmidt Independence Criterion (HSIC) \citep{gretton2005measuring} and Mutual Information (MI) \citep{hjelm2018learning}. 
% Additional methods include Maximum Mean Discrepancy (MMD) \citep{gretton2012kernel}, as well as distance-based methods like the Earth Mover (EM) distance or Wasserstein distance \citep{arjovsky2017wasserstein}. 
However, further research is needed to clarify the relationships among these methods and other frameworks for assessing alignment.}


% Another class of metrics come from independence testing. For example, the Hilbert-Schmidt Independence Criterion (HSIC) \citep{gretton2005measuring}
% \[
%     \text{HSIC}(\mu, \cH_1, \cH_2) = \|C_{1,2}\|_{HS}^2
% \]
% with $C_{1,2}$ being the cross-covariance operator of $\cH_1, \cH_2$  and mutual information (MI) \citep{hjelm2018learning} are commonly used to measure the dependence between two feature spaces. Additional alternatives include Maximum Mean Discrepancy (MMD) \citep{gretton2012kernel}, as well as distance-based methods like the Earth Mover (EM) distance or Wasserstein distance \citep{arjovsky2017wasserstein}. 
% However, the relationships among these methods for measuring alignment remain unclear, we will connect these different notions in this paper.}


% {\red Put some references to say about typical alignment/similarity metic, such as CKA, CCA, MI... to help state about our contributions. } 



To quantify the alignment of representation conditioned on a task, one approach is to use the stitching method \citep{lenc2015understanding}. \cite{bansal2021revisiting} revisited this technique and used it to highlight that good models trained on different objectives (supervised vs self-supervised) have similar representations. By evaluating how well one representation integrates into another model, stitching provides a more interpretable framework for assessing alignment. To describe the setup, we use $h_{1,2}:=g_2 \circ s_{1,2}\circ f_1$ to represent the function after stitching from model $1$ to model $2$ (Figure \ref{fig:wholePic} gives a detailed illustration of the whole process). Here $g_q$ and $f_q$ are parts of model $\cH_q: \cX_q \to \cY_q$ with $q =1,2$, and $s_{1,2}$ is the stitcher. We consider the generalization error after stitching between two models:
$$\cR(g_2 \circ s_{1,2}\circ f_1)=\Eof{\ell(h_{1,2}(x), y)}. $$
We can use the risk of the stitched model in excess of the risk of model 2
\[\min_{s_{1,2}} \cR(h_{1,2}) -\min_{h_2 \in \cH_2} \cR(h_2)\]
to quantify the impact of using different representations, fixing $g_2$.

In this paper, we aim to formalize and refine some of these questions, and our contributions are summarized as follows:
\begin{enumerate}[label=(\alph*),left=0pt]
    \item We compile different definitions of alignment from various communities, demonstrate their connections, and give spectral interpretations.
    \begin{itemize}[left=0pt]
        \item Starting from the empirical Kernel Alignment (KA), we reformulate empirical KA and population version of KA using feature/representation maps, operators in Reproducing Kernel Hilbert Space (RKHS), and spectral interpretation. In addition, we discuss the statistical properties of KA.
        \item  We integrate various notions of alignment, ranging from kernel alignment in independence testing and learning theory to measure and metric alignment, and demonstrate their relationships and correlations. This comprehensive exploration provides a deeper understanding for practical applications.
    \end{itemize}
    \item We provide the generalization error bound of linear stitching with the kernel alignment of the underlying representation.
        \begin{itemize}[left=0pt]
            \item A linear  $g_q$  results in the stitching error being equivalent to the risk from the  model  $\cH_q$. This occurs, for example, when  $\cH_q$  represents RKHSs or neural networks, then $ g_q$  is a linear combination of features in RKHSs or the output linear layers of neural networks.
            \item The excess stitching risk can be bounded by kernel alignment when  $g_q $ are nonlinear functions with the Lipschitz property. A typical scenario is stitching across the intermediate layers of neural networks.
            % \item We get a general lower bound for $\min_{s_{1,2}}\cR(h_{1,2})$ by $\min_{h_1} \cR(h_1)$ with the assumption that $g_2 \circ s_{1,2}\subseteq \cG_1=\{g_1\}$. In practice, for models involve several compositions such as deep networks, this can occur given similar networks if we stitch from a layer further from output to a layer closer to the output (stitching forward, assuming indices are aligned at the end).
            \item For models involving several compositions such as deep networks, if we stitch from a layer further from the output to a layer closer to the output (stitching forward) and $g_q$ is Lipschitz, the difference in risk can be bounded by stitching.
            % \item {\red If stitch forward + lipschitz then, difference in risk is bounded by stitching.}
            % \item \textcolor{red}{Performance can be bounded by misalignment (across modalities, not KTA)}
        \end{itemize}
    % \item What do algorithms converge to 
\end{enumerate}


\paragraph{Structure of the paper} In the following of this paper, we introduce the problem settings and some notations in Section \ref{sec:settings}. Different definitions for representation alignment from different communities and the relationship among them will be derived in Section \ref{sec:alignment}. Section \ref{sec:stitching} demonstrates that the stitching error could be bounded by the kernel alignment metric. And the conclusion is given in Section \ref{sec:conclusion}.
% Some simulations are done in Section \ref{sec:experiment} to verify the relationship of stitching and kernel alignment.



\section{Preliminaries}\label{sec:settings}
Empirical results demonstrate that well-aligned features significantly enhance task performance. However, there is a pressing need for more rigorous mathematical tools to formalize and quantify these concepts in uni/multi-modal settings. In this section, we provide a mathematical formalization of uni/ multi-modal learning, introducing key notations to facilitate a deeper understanding of the underlying processes.




% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\linewidth]{diagram.png}
%     \caption{Diagram illustrating spaces and measures of reality, objects/data, representation, and outputs as well as the functions connecting them. }
%     \label{fig:wholePic}
% \end{figure}


\paragraph{Setup} Without loss of generality, we focus on the case of two modalities, as illustrated in Figure~\ref{fig:wholePic}, which outlines the corresponding process. For \(q = 1, 2\), let \((\mathcal{X}_q, \mu_q)\) and \((\mathcal{Z}_q, \lambda_q)\) be probability spaces, and let \(\mathcal{F}_q\) be the space of functions \(f_q : \mathcal{X}_q \to \mathcal{Z}_q = \mathbb{R}^{d_q}\). We regard \(\mathcal{X}_q\) as the space of \textbf{objects} (or data), \(\mathcal{F}_q\) as the space of representation (or embedding) maps, and \(\mathcal{Z}_q\) as the space of \textbf{representations}. We relate \(\mu_q\) and \(\lambda_q\) by assuming \( \lambda_q = (f_q)_{\#}\mu_q \)\footnote{\( (f_q)_\#\mu_q \) is the pushforward measure of \(\mu_q\) defined as \( (f_q)_\#\mu_q(A)=\mu_q(f_q^{-1}(A)) \). In terms of random variables \(X_q\) and \(Z_q\) with measures \(\mu_q\) and \(\lambda_q\), respectively, this is equivalent to \(f_q(X_q)\) and \(Z_q\) being equal in law.}. We also assume that \(\mu_1\) and \(\mu_2\) are the marginals of a joint probability space \((\mathcal{X}, \mu)\) with \( \mathcal{X} = \mathcal{X}_1 \times \mathcal{X}_2,\quad \mu_q = (\pi_q)_{\#}\mu \), where \(\pi_q : \mathcal{X} \to \mathcal{X}_q\) is the projection map. Moreover, let \((\mathcal{Y}_q, \nu_q)\) be the task-based \textbf{output} spaces and define \(\mathcal{G}_q = \{g_q: \mathcal{Z}_q \to \mathcal{Y}_q\}\) with \(\nu_q = (g_q)_{\#}\lambda_q\). Each overall model is generated by \( \mathcal{H}_q := \{h_q: \mathcal{X}_q \to \mathcal{Y}_q \mid h_q = g_q \circ f_q\} \).


\paragraph{Reality}
Consider a space of abstract objects, called the \emph{reality space} and denoted by \(\Xi\), which generates the observed data in various modalities through maps \(m_q : \Xi \to \cX_q\). These maps may be bijective, lossy, or stochastic. Reality can be modeled as a probability space \((\Xi, \xi)\). Alternatively, one may define reality as the joint distribution over modalities by setting \(m_q = \pi_q\).

% We'll assume that there is a shared probability space called reality $(\Xi,\xi)$ from which data is generated by $m_q:\Xi\to \cX_q$ with $\mu_q = (m_q)_\# \xi$. 


\paragraph{Uni/Multi-modal}
We may want to consider the case of a single modality, where only one data space exists, versus multiple modalities, where several such spaces are present. Two modalities are deemed equal if \(\pi_1 = \pi_2\).



\paragraph{Representation alignment}
A representation mapping is a function $f : \cX \to \R^{d}$  that assigns a latent feature vector in  $\R^{d}$  to each input in the data domain $ \cX$. Alignment provides a metric to evaluate how well the latent feature spaces obtained from different representation mappings, whether from uni-modal or multi-modal data, are aligned or similar.  Commonly used metrics for measuring alignment include those derived from kernel alignment, contrastive learning, mutual information, canonical correlation analysis, and cross-modal mechanisms, among others. However, they are introduced in a very fragmented manner, without an integrated or unified concept. A detailed introduction and analysis of these methods will be provided in Section \ref{sec:alignment}.






\section{Frameworks for Representation Alignment}\label{sec:alignment}

In this section, we describe various definitions of representation alignment from different communities and demonstrate the relationship among them. We begin with a detailed presentation of empirical and population Kernel Alignment and its statistical properties. We then cover other notions of alignment coming from metrics, independence testing, and probability measures, as well as their spectral interpretations. We draw connections to kernel alignment which emerges as a central object.
% {\red some sentence to summarize what we have, what's new here?}

\subsection{Kernel alignment (KA)}
Based on the work of \citet{cristianini2001kernel}, who introduced the definition of kernel alignment using empirical kernel matrices, we propose different perspectives to understand kernel alignment in both empirical and population settings and derive its statistical properties accordingly.

A reproducing positive definite kernel \(K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) captures the notion of similarity between objects by inducing an inner product in the associated reproducing kernel Hilbert space (RKHS) $\mathcal{H}$. Specifically, \( K(x, x') = \langle f(x), f(x') \rangle \) for any representation (feature) map $f \in \mathcal{H}$, and \( x, x' \in \mathcal{X} \).
For the multi-modal case, we define 
\( K_q(x, x') := \tilde{K}_q(\pi_q(x), \pi_q(x')) = \tilde{K}_q(x_q, x_q') \),  where $\tilde{K}_q$ is the reproducing kernel associated with $\mathcal{H}_q$. In other words, \( K_q \) acts on \( x = (x_1, x_2) \) by first applying the projection \( \pi_q(x) = x_q \). In the following, the subscript \( x_q \) denotes the \( q \)-th modality, and the superscript \( x^i \) indicates the \( i \)-th sample.



\subsubsection{Empirical KA}
 
 From \cite{cristianini2001kernel}, we adopt the following formulation for kernel alignment for kernel matrix $K_{q,n} \in \R^{n \times n}$ with samples $\{x^i\}_{i=1}^n $ drawing according to the probability measure $ \mu$
\begin{equation*}\label{equ:empiricalKA}
     \widehat{A}(K_{1,n},K_{2,n}) = \frac{\langle K_{1,n}, K_{2,n}\rangle_F}{\sqrt{\langle K_{1,n}, K_{1,n}\rangle_F\langle K_{2,n}, K_{2,n}\rangle_F}},
\end{equation*}
where  $\langle K_{1,n},K_{2,n} \rangle_F = \sum_{i,j=1}^n K_{1,n}(x^i,x^j)K_{2,n}(x^i,x^j)$. One modification is to first demean the kernel by applying a matrix $H=I_n-\tfrac{1}{n}1_n1_n^T$ on the left and right of each $K_{q,n}$ with $I \in \R^{n \times n}$ being the identity matrix and $1_n$ being the ones vectors. This results in Centered Kernel Alignment (CKA).
% And also, if we consider the alignment with output $\mathbf{y} = (y^1, \ldots, y^n)^T$, we refer to $\widehat{A}(K_{1,n},\mathbf{y}\mathbf{y}^T)$ as Kernel Target/Task Alignment (KTA).

\paragraph{Representation interpretation of KA} Denote the empirical cross-covariance matrix between the representation maps $f_1$ and $f_2$ as $\widehat{\Sigma}_{1,2}=\Esub{n}{f_1(x)f_2(x)^T }=\frac{1}{n}\sum_{i=1}^n f_1(x^i)f_2(x^i)^T \in \R^{d_1 \times d_2}$. Then the empirical KA will become
\begin{equation}\label{equ:repreKA}
    \widehat{A}(K_{1,n},K_{2,n})=\frac{\|\widehat{\Sigma}_{1,2}\|_F^2}{\|\widehat{\Sigma}_{1,1}\|_F\|\widehat{\Sigma}_{2,2}\|_F}.
\end{equation}

\paragraph{RKHS operator interpretation of KA} { Inspired by the \eqref{equ:repreKA}, we construct a consistent definition of Kernel Alignment using tools of RKHS, where it suffices to consider output in one dimension\footnote{ We can generalize the definition to vector-valued functions by recasting $h_q:\cX_q \to \R^{t_q}$ as $h_q : \cX_q \times [t_q] \to \R$ i.e. with kernels of the form $K_q(x_q,i,x_q',i')$ for integers $1\leq i,i'\leq t_q$.}. Consider RKHS $\cH_q$ containing functions $h_q:\cX_q\to\R$ with kernel $K_q$.} Given evaluation (sampling) operators $\widehat{S}_{q}:\cH_q\to\R^n$ defined by $(\widehat{S}_q h_q)^i=h_q(x_q^i)=\langle h_q, K_{q, x_q^i}\rangle$. It is not hard to check that the adjoint operator $\widehat{S}_q^* : \R^n \to \cH_q$ can be written as $\widehat{S}_q^*(w^1, \ldots, w^n )= \sum_{i=1}^n w^i K_{q}(x_q^i, \cdot)$ and the empirical kernels can be written as $K_{q,n}/n = \widehat{S}_q\widehat{S}_q^*$ \citep{smale2004shannon,de2005learning}. Then the empirical KA may be written as
\[
    \widehat{A}(K_{1,n}, K_{2,n}) = \frac{\langle \widehat{S}_1\widehat{S}_1^*, \widehat{S}_2\widehat{S}_2^*\rangle_F }{\|\widehat{S}_1\widehat{S}_1^*\|_F\|\widehat{S}_2\widehat{S}_2^*\|_F} = \frac{\| \widehat{S}_1^* \widehat{S}_2\|_F^2 }{\|\widehat{S}_1^*\widehat{S}_1\|_F\|\widehat{S}_2^*\widehat{S}_2\|_F},
\]
where $\widehat{S}_1^*\widehat{S}_2=\tfrac{1}{n}\sum_i K_{1,x_1^i}\otimes K_{2,x_2^i}$ and it coincides with the literature about learning theory with RKHS. 
% {\color{red} We can generalize the definition to vector valued functions by recasting $h_q:\cX_q \to \R^{m_q}$ as $h_q : \cX_q \times [m_q] \to \R$ i.e. with kernels of the form $K_q(x_q,i,x_q',i')$ for integers $1\leq i,i'\leq m_q$.}

% Centering can be done by replacing $\widehat{S}_q$ with $H \widehat{S}_q$ with $H = I - \frac{1}{n}11^T$. 

% \tcolorboxenvironment{theorem}{
%   colframe=gray,        % Frame color (gray)
%   colback=gray,      % Background color (light gray)
%   colbacktitle=gray,       % Title text color
%   fonttitle=\bfseries,   % Title font style
%   sharp corners,         % Remove rounded corners
%   boxrule=0.5mm,         % Border thickness
%   colbacktitle=gray!10     % Title background color
% }

% % Generate the framework
% \begin{tcolorbox}[title=The Capacity Hypothesis]
%   Bigger models are more likely to converge to a shared representation than smaller models.
% \end{tcolorbox}

\subsubsection{Population version of KA}

For the population setting  (infinite data limit of the evaluation operator) in $L^2$, the restriction operator $S_q: \cH_q \to L^2(\cX_q,\mu)$ is defined by $S_q h_q (x)= \langle h_q, K_{q}(x, \cdot) \rangle_{K_q}$ and its adjoint $S_q^*: L^2(\cX_q,\mu)\to \cH_q$ is given by $S_q^* g = \int_\cX g(x) K_q(x, \cdot) dx$. Then the integral operator $L_{K_q}=S_qS_q^*: L^2(\cX_q,\mu) \to L^2(\cX_q,\mu)$ is given by $L_{K_q} g(x) = \int_{\cX} K_q(x,x') g(x') d\mu(x')$ and the operator $\Sigma_q=S_q^* S_q :\cH_q \to \cH_q$ can be written as $\Sigma_q = \int_\cX K_q(x, \cdot) \otimes  K_q(x, \cdot) d\mu(x)$ \citep{de2005learning,rosasco2010learning}.
Similarly, the population KA between two kernels $K_1, K_2$ can be defined by 
\[A(K_1,K_2)= \frac{\text{Tr}\left(L_{K_1}L_{K_2}\right)}{ \sqrt{\text{Tr}\left(L_{K_1}^2\right) \text{Tr}\left(L_{K_2}^2\right)}},\]
where the summation in $\langle K_{1,n},K_{2,n} \rangle_F$ becomes the integration as
$$\text{Tr}\left(L_{K_1}L_{K_2}\right)= \int d\mu(x_1,x_2)d\mu(x_1',x_2')K_1(x_1,x_1')K_2(x_2,x_2').$$
If $K_q(x,x')=\langle f_q(x),f_q(x')\rangle$, then $S_{q}^*S_{q}$ is a projection onto the span of coordinates of $f_q$. The population version of CKA is KA with $S_{q}$ replaced with $H S_{q}$. 


\paragraph{Spectral Interpretation of KA}\label{para:spectral}
Furthermore, the understanding of kernel alignment (KA) can be deepened via the spectral decomposition of the associated integral operator. The mercer kernel \(K\) can be decomposed as
\(
K = \sum_i \eta_i\, \phi_i \otimes \phi_i,
\)
where \(\eta_i\) are the eigenvalues and \(\phi_i\) are the eigenfunctions of the integral operator \(L_K\) \citep{cucker2002mathematical,scholkopf2002learning}. Defining the features as \(f_i = \sqrt{\eta_i}\phi_i\) and expressing the target function as \(h = \sum_i w_i f_i\), we obtain
\[
A(K, h \otimes h) = \frac{\sum_i \eta_i^2 w_i^2}{\sqrt{\sum_i \eta_i^2}\, \sum_i \eta_i w_i^2}.
\]
Similarly, given two kernels $K_q=\sum_i \eta_{q,i} \phi_{q,i }\otimes \phi_{q,i}$ with $f_{q,i}=\sqrt{\eta_{q,i}}\phi_{q,i}$, we have
\[
A(K_1,K_2)=\frac{\sum_{i,j}\langle f_{1,i}, f_{2,j}\rangle}{\sqrt{\sum_i \eta^2_{1,i}\sum_i \eta^2_{2,i}}} = \frac{\sum_{i,j}\eta_{1,i}\eta_{2,j} \langle \phi_{1,i}, \phi_{2,j}\rangle^2}{\sqrt{\sum_i \eta^2_{1,i}\sum_i \eta^2_{2,i}}} .
\]
Letting \([C_{1,2}]_{i,j} = \langle \phi_{1,i}, \phi_{2,j} \rangle\) and defining \(\hat{\eta}_i = \eta_i/\|\eta_i\|\), we can equivalently write
\[
A(K_1,K_2) = \operatorname{Tr}\Bigl[C_{1,2}\,\operatorname{diag}(\hat{\eta}_2)\, C_{1,2}^T\,\operatorname{diag}(\hat{\eta}_1)\Bigr] = \langle \hat{\eta}_1, (C_{1,2}\odot C_{1,2})\, \hat{\eta}_2\rangle = \langle \hat{\eta}_1\hat{\eta}_2^T, C_{1,2}\odot C_{1,2} \rangle
\]
with $\odot$ as the Hadamard product. This formulation provides insight into kernel alignment by relating it to the similarity between the eigenfunctions of the two integral operators. In particular, if \(\eta_1\) and \(\eta_2\) are constant, then \(A(K_1,K_2) \propto \|C_{1,2}\|^2\); and if \(C_{1,2} = I\), then \(A(K_1,K_2) = \langle \hat{\eta}_1, \hat{\eta}_2 \rangle\).


% \br[Alignment vs Prediction] Notice, if independent $\mu = \mu_1\times \mu_2$ then CKA is zero. If determined $\mu(x_1,x_2) = \delta(x_2-f_{2|1}(x_1))\mu_1(x_1)$ and $K_2(f_{2|1}(x_1),f_{2|1}(x_1'))\propto K_1(x_1,x_1')$, then alignment is 1.\er

\subsubsection{Statistical properties of KA.}

Having introduced both the empirical and population versions of KA, we now explore its statistical properties. 
\citet{Cristianini2006} shows that empirical KA concentrates to its expectation by McDiarmid’s
inequality and gives an $O(1/\sqrt{n})$ bound. For completeness, we state the following lemma summarizing this statistical property and the proof is provided in Appendix~\ref{additionalproofs}.

% The following lemma shows the concentration inequality for the empirical KA  and KA \citep{Cristianini2006} and the proof can be found in Appendix.
\begin{lem}\label{lem:statsKA}
    Let $K_1, K_2$ be two kernels for different representations  and $\widehat{K}_{1,n}, \widehat{K}_{2,n} \in \R^{n \times n}$ be kernel matrices generated by $n$ samples, then with probability at least $1-\delta$, we have
    \[\widehat{A}(K_{1,n},K_{2,n}) - A(K_1,K_2) \leq  \sqrt{(32/n)\log (2/\delta)}. \]
\end{lem}

% \br
% We can improve the rate to be $O(n^{-1})$ as \citet{Cristianini2006} shows that empirical KA concentrates to its expectation by McDiarmid’s
% inequality and gives an $O(n^{-1} \log n)$ bound on the difference to population KA after adjusting for diagonal terms which contribute $O(n^{-1})$.
% \er

\subsection{Alignment from distance alignment}

\paragraph{Distance alignment (DA)}  Given distances $d_q: \cX_q \times \cX_q \to \R$,  then we can compare the difference of two spaces by
\[
    D(d_1,d_2) = \int  (d_1^2(x,x') - d_2^2(x,x'))^2 d\mu(x)d\mu(x').
\]
% Generalizes in the same way as KA to multiple modes with distance having projections.


% The following lemma provides the equivalence between KA and DA if we choose 
% \begin{equation}\label{equ:d_q}
%     d_q^2=2(1-K_q).
% \end{equation}

% The following lemma demonstrates that distance alignment is equivalent to kernel alignment and the proof can be found in {\red Appendix}.

% \begin{lem}\label{lem:KADA}
%    Suppose $K_q(x_q,x_q)=1$ and $\Tr(L_{K_q})=C$, then by choosing the metric $d_q$ as \eqref{equ:d_q}, we have that
%    \[D(d_1,d_2)=8C(1-A(K_1,K_2)).\]
% \end{lem}

% \begin{proof}
    
% \end{proof}
% \begin{proof}
%     \begin{equation*}
%     \begin{split}
%       & \int_{\cX \times \cX}( d_1(x,x')-d_2(x,x'))^2 d \mu(x)d \mu(x') \\
%        = & \frac{1}{2}\int_{\cX \times \cX}\left(\frac{K_2(x,x')}{\sqrt{\text{Tr} (L_{K_2}^2)}}-\frac{K_1(x,x')}{\sqrt{\text{Tr} (L_{K_1}^2)}}\right)^2 d \mu(x)d \mu(x')\\
%        =&  \frac{1}{2} \left( 2-2\frac{\Tr(L_{K_1} L_{K_2})}{\sqrt{\text{Tr} (L_{K_1}^2)}\sqrt{\text{Tr} (L_{K_2}^2)}}\right)= 1- A(K_1,K_2).
%     \end{split}
%     \end{equation*}
% \end{proof}
{\paragraph{Equivalence between KA and DA}
Suppose \(d_q^2 = 2(1-K_q)\) \citep{igel2007gradient} and \(K_q(x_q,x_q)=1\), which emerge naturally from assuming 
\(
K_q(x,x')=\langle f_q(x),f_q(x')\rangle, \|f_q(x)\|=1,
\)
and 
\(
d_q^2(x_q,x_q')=\|f_q(x_q)-f_q(x_q')\|^2,
\)
(i.e., \(K_q\) represents a mapping onto a ball). Also assume \(\|K_q\|=C\). Then,
\(
D(d_1,d_2)=8C(1-A(K_1,K_2)),
\)
hence the two paradigms are equivalent.}




\subsection{Alignment from independence testing}
Independence testing is a statistical method used to assess the degree of dependence between variables. It often involves examining the covariance and correlations between random variables and can also be applied to quantify kernel-based independence. In this section, we outline several approaches from independence testing within the alignment framework and investigate their connections to the kernel alignment method discussed earlier.

\paragraph{Hilbert-Schmidt Independence Criterion (HSIC)} The cross-covariance operator for two functions \citep{baker1973joint} is given by ${C_{\1,2}}[h_1,h_2]=\E_{x_1,x_2}[(h_1(x_1)-\E_{x_1}(h_1(x_1))(h_2(x_2)-\E_{x_2}(h_2(x_2))]$ for $ h_1\in \cH_1, h_2\in \cH_2$.
 From \cite{gretton2005measuring}
\[
    \text{HSIC}(\mu, \cH_1, \cH_2) = \|C_{1,2}\|_{HS}^2,
\]
where $\mu$ is the joint distribution of $\cX_1$ and $\cX_2$. We can also note that 
\[
\text{HSIC}(\mu, \cH_1, \cH_2)=\|\Eof{K_{x_1} \otimes K_{x_2}}\|^2= \|\Sigma_{1,2}\|_{HS}^{2}.
\]
Hence HSIC is effectively and unnormalized version of CKA, or, more explicitly, 
\[
\text{CKA}(K_1,K_2) = \frac{\text{HSIC}(\cH_1, \cH_2)}{\sqrt{\text{HSIC}(\cH_1, \cH_1)\text{HSIC}(\cH_2, \cH_2)}}.
\]

\paragraph{Statistical property of HSIC}
 { \cite{gretton2005kernel} shows that, excluding the $O(n^{-1})$ diagonal bias, centered empirical HSIC concentrates to population and \cite{song2012feature} provides an unbiased estimator of HSIC and shows its concentration, both by U-statistic arguments.}

 \br[Other notions from independence testing]
There are other concepts of independence testing for alignment such us Constrained Covariance (COCO) \citep{gretton2005measuring}, Kernel Canonical Correlation (KCC), Kernel Mutual Information (KMI)  \citep{bach2002kernel}. They are also related to kernel alignment and more detailed explanations can be found in Appendix \ref{subsec:def_it}.

 \er

\subsection{Alignment from measure alignment}

There are several methods for comparing measures on the same space. One can then quantify independence by comparing a joint measure with the product of its marginals. This principle allows us to interpret HSIC as test for independence given two function classes.

\paragraph{MMD to HSIC}
Following \cite{gretton2012kernel}, we start by introducing the so-called Maximum Mean Discrepancy (MMD). Let $\cH$ be a class of functions $h:\cX\to\R$ and let $\mu_q$ be different measures on $\cX$. Then, letting $x_q\sim \mu_q$,
\[
\text{MMD}(\mu_1,\mu_2;\cH) = \sup_{h\in \cH} \Eof{h(x_1)-h(x_2)}.
\]
Let $\cH$ be an RKHS and restrict to a ball of radius 1, then 
\[
\text{MMD}(\mu_1,\mu_2;\cH)^2 = \| \Eof{K_{x_1} - K_{x_2}}\|_{\cH}^2 = \Eof{K(x_1,x_1')+K(x_2,x_2')-2 K(x_1,x_2)}.
\] 
Now we construct a measure of independence by applying MMD on $\mu$ versus $\mu_1\otimes \mu_2$ where $\cH$ is replaced with $\cH_1\times \cH_2$ and get HSIC 
\[
\text{MMD}(\mu,\mu_1\otimes \mu_2; \cH_1\otimes \cH_2)^2 = \text{HSIC}(\mu, \cH_1, \cH_2) = \|\Sigma_{1,2}\|^{2} = \sum_i \rho_i^2
\]
where $\left\{\rho_i^2\right\}$ is the spectrum of $\Sigma_{1,2}\Sigma_{2,1}$.

We can also use tests of independence that don't explicitly depend on a function class, such as mutual information, by letting $\mu$ be a Gaussian Process measure on two functions in their respective RKHS with covariance defined by their kernels. 

\paragraph{KL Divergence to Mutual Information}
 Given KL divergence
\[
D_{\text{KL}}(\mu||\nu)  = \int d\mu(x) \log \left( \frac{d\mu}{d\nu}(x)\right),
\]
we can define mutual information as
\[
I(\mu)=D_{\text{KL}}(\mu||\mu_1\otimes\mu_2)=\int d\mu(x_1,x_2) \log\left(\frac{\mu(x_1,x_2)}{\mu_1(x_1)\mu_2(x_2)}\right)=\int d\mu(x_1,x_2) \log\left(\frac{\mu(x_2|x_1)}{\mu_2(x_2)}\right).
\]

For multivariate Gaussian $\mu$, with marginals $\mu_q = \cN(0,\Sigma_q)$, 
\[\text{MI}(\nu)=\frac{1}{2}\log\left(\frac{|\Sigma_1||\Sigma_2|}{|\Sigma|}\right)=\frac{1}{2}\log \left(\frac{|\Sigma_2|}{|\Sigma_2 - \Sigma_{2,1}\Sigma_{1}^{-1}\Sigma_{1,2}|}\right).
\]
For the simplest case of $\Sigma_q=I$, then this simplifies to 
\[
\text{MI}(\nu)=-\tfrac{1}{2}\log(|I - \Sigma_{1,2}\Sigma_{2,1}|)= -\tfrac{1}{2}\sum_i \log (1 - \rho_i^2).
\]

% \color{red}\textbf{Need to read well KMI first}
% \br[KA vs MI]  For multivariate Gaussian $\nu$, with $\nu_q = \cN(0,I)$, \[\text{MI}(\nu)=\tfrac{1}{2}\log\left(\frac{|\Sigma_1||\Sigma_2|}{|\Sigma|}\right)=-\tfrac{1}{2}\log(|I - \Sigma_{1,2}\Sigma_{2,1}|) \approx \tfrac{1}{2}\|\Sigma_{1,2}\|^2=\tfrac{\sqrt{d_1d_2}}{2}A(K_1,K_2)\]
% where the approximation is valid for small $\|\Sigma_{1,2}\|$.
% \er

\paragraph{Wasserstein distance}
For the Wasserstein distance 
\[
W_2(\mu,\nu) = \inf\{\Esub{(x,y)\sim\gamma}{\|x - y\|^2}: \gamma_1 = \mu, \gamma_2=\nu\},
\]
applying  $\mu$ and $\mu_1\otimes \mu_2$ to measure independence, we have
\[
W_2(\mu,\mu_1\otimes\mu_2) = \inf\{\Esub{((x_1,x_2),(x_1',x_2'))\sim\gamma}{\|x_1 - x_1'\|^2+\|x_2 - x_2'\|^2}: \gamma_1 = \mu, \gamma_2=\mu_1\otimes \mu_2\}.
\]
For mean zero Gaussians
\[
W_2(\mu_1,\mu_2) = \text{Tr}[\Sigma_1 + \Sigma_2 - 2(\Sigma_1^{1/2}\Sigma_2\Sigma_1^{1/2})^{1/2}]
\]
and as a measure of independence with $\Sigma_q=I$
\[
W_2(\mu,\mu_1\otimes\mu_2) 
% = 2\text{Tr}[\Sigma_2 - (\Sigma_2^{1/2}(\Sigma_2/\Sigma_1)\Sigma_{2}^{1/2})^{1/2}] 
= 2\text{Tr}[I - (I - \Sigma_{1,2}\Sigma_{2,1})^{1/2}] = 2 \sum_i \left(1 - \sqrt{1 - \rho_i^2}\right).
\]

% \paragraph{Task Alignment}
%  {\color{red} While we've been discussing alignment between two representations, similar notions can be used to measure alignment between a model and a task to estimate generalization error.}  \cite{cristianini2001kernel,Cristianini2006,cortes2012algorithms} introduced Kernel Task Alignment (KTA),  $A(K, yy^T)$ , to bound generalization error and generate predictors. Similarly, several works \citep{atanasov2022neural, paccolat2021geometric, kopitkov2020neural, fort2020deep, shan2021theory} used KTA to study feature learning and Neural Tangent Kernel (NTK) evolution. Related ideas appear in Kernel Alignment Risk Estimator (KARE) \citep{jacot2020kernel} and Spectral Task-Model Alignment \citep{canatar2021spectral}. Additionally, the source condition assumption in kernel ridge regression generalization error \citep{rosasco2005spectral} can be linked to spectral task alignment. More details are provided in the Appendix \ref{subsec:def_al_task}.

{
In summary, we've introduced several popular metrics for alignment between two representations and related them via spectral decompositions to a central notion of kernel alignment generalized for RKHS. Moreover, similar notions can be used to quantify alignment between a model and a task to estimate generalization error, and more details are provided in the Appendix \ref{subsec:def_al_task}. 
% \cite{cristianini2001kernel,Cristianini2006,cortes2012algorithms} introduced Kernel Task Alignment (KTA),  $A(K, yy^T)$ , to bound generalization error and generate predictors. Similarly, several works \citep{atanasov2022neural, paccolat2021geometric, kopitkov2020neural, fort2020deep, shan2021theory} used KTA to study feature learning and Neural Tangent Kernel (NTK) evolution. Related ideas appear in Kernel Alignment Risk Estimator (KARE) \citep{jacot2020kernel} and Spectral Task-Model Alignment \citep{canatar2021spectral}. Additionally, the source condition assumption in kernel ridge regression generalization error \citep{rosasco2005spectral} can be linked to spectral task alignment. More details are provided 

\section{Stitching: Task Aware Representation Alignment}\label{sec:stitching}
{ Building on our understanding of kernel alignment—a fundamental metric for evaluating the alignment of representations detailed in the previous section—we now explore stitching, a task-aware concept of alignment.} {
Stitching involves combining layers or components from various models to create a new model which can be used to  understand of how different parts contribute to overall performance or to compare the learned features for a task.} 
% \cite{bansal2021revisiting} use stitching to show that good models trained on different objectives (supervised vs self-supervised) have similar representations, supporting their ``Anna Karenina'' hypothesis. They also show that ``More is better'', meaning representations from good models can be plugged into weaker models to boost their performance and further conjecture that SGD has an implicit bias towards such stitching connected models.
% {\color{violet} Stitching involves combining layers or components from various models to create a new model, enabling an understanding of how different parts contribute to overall performance and improving model efficiency.  This technique not only facilitates transfer learning but also provides a way to evaluate feature learning quality by measuring the representation alignment of different models with the task-specific objectives and the generalization error after stitching.}
In this section, we mathematically formulate this process and {provide some intuition by demonstrating}  that the generalization error after stitching can be bounded by kernel alignment { using spectral arguments.}

\subsection{Stitching error between models}
In the following, we focus on stitching between two modalities. Figure \ref{fig:wholePic} provides a detailed illustration of the functions, spaces, and compositions in question.
Denote the function space for task learning as $\cH_q := \{h_q: \cX_q \to \cY_q | h_q = g_q \circ f_q, \; g_q \in \cG_q, f_q \in \cF_q\}$ with $q = 1,2$. Here $\cF_q:\cX_q \to \cZ_q$ and $\cG_q:\cZ_q \to \cY_q$.
Denote  $\cS_{1,2} :=\{s_{1,2}: \cZ_1 \to \cZ_2\}$ as the stitching map from $\cZ_1 $ to $ \cZ_2$  and $\cS_{2,1} :=\{s_{2,1}: \cZ_2 \to \cZ_1\}$ reversely. Define the risk concerning the least squares loss as
\begin{equation*}
    \cR_q(h_q) =\Eof{\|h_q(x)-y\|^2} = \int_{\cX_q \times \cY_q} \|h_q(x)-y\|^2 \md \rho_q(x,y), \quad h_q \in \cH_q.
\end{equation*}
Here, $\rho_q(x,y)$ is the joint distribution of $\cX_q$ and $\cY_q$ and we use the notation $\|\cdot\|$ to represent $\|\cdot\|_{\cY_q}$ associated with space $\cY_q$ for simplicity, i.e. absolute value for $\cY_q=\R$, $l_2$ norm for $\cY_q = \R^{t_q}$ and $L_2$ norm for $\cY_q$ being the function space.
For $h_q \in \cH_q$, denote any minimizer of $\cR(h_q)$ among $\cH_q$ as $h_q^*$, that is,
\begin{equation*}
    \cR_q(\cH_q):=\cR_q(h_q^*)=\min_{h \in \cH_q}\cR_q(h), \quad q=1,2.
\end{equation*}

Moreover, denote the function spaces generated after stitching from $\cZ_1$ to $\cZ_2$ as
\[\cH_{1,2}=\{h_{1,2}=g_2\circ s_{1,2}\circ f_1:s_{1,2}\in\cS_{1,2}\}\]
and conversely as $\cH_{2,1}$.



\citet{lenc2015understanding} proposed to describe the similarity between two representations by measuring how usable a representation $f_1$ is when stitching with $g_2$ through a function $s_{1,2}:\cZ_1\to\cZ_2$ or oppositely through $s_{2,1} \in \cS_{2,1}$. To quantify the similarity, we provide a detailed definition of the stitching error.
 \paragraph{Stitching error}
Define the stitching error as $$\cR^{\text{stitch}}_{1,2}(s_{1,2}):=\cR_2(g_2 \circ s_{1,2}\circ f_1)=\cR_2(h_{1,2})$$ 
 and the minimum as
 \[\cR^{\text{stitch}}_{1,2}(\cS_{1,2}):=\min_{s_{1,2}\in \cS_{1,2}}\cR_2(h_{1,2})=\cR_2(\cH_{1,2}). \]
 To quantify the difference in the use of stitching, we define the \textbf{excess stitching risk} as
 \begin{equation*}
     \cR_{1,2}^{\text{stitch}}(\cS_{1,2})-\cR_2(\cH_2).
 \end{equation*}

Note that $\cR_{1,2}^{\text{stitch}}(\cS_{1,2})-\cR_2(\cH_2)$ quantifies a difference in use of representation (fix $g_2$, compare $s_{1,2}\circ f_1$ vs $f_2$), while if $\cY_1=\cY_2$ then $\cR_{1,2}^{\text{stitch}}(\cS_{1,2})-\cR_1(\cH_1)$ quantifies difference between $g_2\circ s_{1,2}$ and $g_1 $ (fix $f_1$). 

{The functions in $\cS_{1,2}$ are typically simple maps such as linear layers or convolutions of size one, to avoid introducing any learning, as emphasized in \cite{bansal2021revisiting}. 
The aim is to measure the compatibility of two given representations without fitting a representation to another. One perspective inspired by \citet{lenc2015understanding} is that we should not penalize certain symmetries, such as rotations, scaling, or translations, which do not alter the information content of the representations. Furthermore, the amount of unwanted learning may be quantified by stitching from a randomly initialized network.
}

\subsection{Stitching error bounds with kernel alignment}
% {\color{purple} To avoid outweighing the significance of the original representations}, 
% {\color{green} To avoid outweighing the significance of the original representations}, 
% the family of stitchers $\cS_{1,2}$ is chosen to be simple. 
In this section, we focus on a simplified setting where $s_{1,2}:\cZ_1\to\cZ_2$ is a \textbf{linear stitching}, that is, $s_{1,2}(z_1)=S_{1,2} z_1$ with $S_{1,2}\in \R^{d_2\times d_1}, z_q \in 
\R^{d_q}$. Additionally, we assume $\cY_1=\R^{t_1}, \cY_2=\R^{t_2}$.
% Additionally, we assume {$\cY_1=\cY_2=\cY=\R^d$}, meaning both focus on the same task. 
In this section, we quantify the stitching error and excess stitching risk using kernel alignment and provide a lower bound for the stitching error when stitching forward.

% {\color{red}
% \br[Linearity Assumption]  
% We consider linear stitching (linear layer) to align the dimensionality of two models before connecting them together, which is also commonly used in practice \cite{}. The primary objective here is to evaluate the alignment between the two feature spaces (or layers). By adopting this simple linear stitching approach, we aim to preserve the significance of the original representations.
% \er
% }

The following lemma shows that when $\cG_q$ are linear,  stitching error only measures the difference in risk of $\cH_1$ versus $\cH_2$.

\begin{lem} \label{lem:stit}
Suppose $\text{dim}(\cY_1)=\text{dim}(\cY_2)=d$ and $\cR_1=\cR_2$. Let $g_q\in \cG_q$ be linear with $g_q(z_q) = W_q z_q$ and $W_q\in\R^{d \times d_q}$. Let $s_{1,2}:\cZ_1\to\cZ_2$ be linear with $s_{1,2}(z_1)=S_{1,2} z_1$ and $S_{1,2}\in \R^{d_2\times d_1}$. Then $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})=\cR_1(\cH_1)$.
\end{lem}

% \begin{proof}
% Assume a minimizer is $h_1^* = g_1^* \circ f_1^*$ with $g_1^*(z_1) = w_1^* \cdot z_1$.  Let $h_{1,2}=g_2\circ s_{1,2}\circ f_1^* \in \cH_{1,2}$. By linearity of $g_2\circ s_{1,2}$, we have $
% h_{1,2} = (w_2^T S_{1,2}) \cdot f_1^* \in \cH_1$. Hence, $\cR^{\text{stitch}}(\cS_{1,2})\geq \cR(\cH_1)$.  Note too that the minimizer $h_1^*\in \cH_{1,2}$ because choosing $S_{1,2}=w_2^\dagger w_1$ results in $h_1=h_{1,2}$, which implies that $\cR^{\text{stitch}}(\cS_{1,2})\leq \cR(\cH_1)$. 
% \end{proof}

% \begin{proof}
%     For the linear case, there exists a vector $W_q \in \R^{d \times d_q}$, such that $g_q(z_q) = W_q z_q, z_q \in \R^{d_q}$. We can write the error of stitching as
% \begin{align*}
% \cR^{\text{stitch}}_{1,2}(s_{1,2}) 
% &= \Eof{\|W_2 S_{1,2}f_1-y\|^2}\\
% &=\Eof{\|(W_2 S_{1,2} - W_1)f_1\|^2}+\Eof{\|W_1 f_1(x)-y\|^2} \\
% &=\|W_2 S_{1,2} - W_1\|_{\eta_1}^2 + \cR_1(h_1),
% \end{align*}

% where we used that for $W_1$ to be optimal, we require $\partial_{W_1}\cR_1(h_1)=\Eof{(W_1 f_1-y)f_1^T}=0$.
% Minimizing with respect to $S_{1,2}$ yields
% \begin{align*}
%     \cR^{\text{stitch}}_{1,2}(\cS_{1,2}) &= \|\Pi_2^\perp W_1\|_{\eta_1}^2+\cR_1(\cH_1),
% \end{align*}
% where we use $\Pi_2=I - (W_2^T \text{diag}(\eta_1) W_2)^{\dagger}W_2^T \text{diag}(\eta_1)$ to denote the residual of the generalized $\eta_1$-projection onto (column) span of $W_2$. We note that in general, as long as { $d\leq d_2$}, we have $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})=\cR_1(\cH_1)$.
% \end{proof}

\br
% Note that no assumptions were made regarding  $\cF_q$, so 
The lemma applies when  $\cH_q $ represents a neural network with  $\cG_q$  as the output linear layer, as well as when  $\cH_q $ is an RKHS with a Mercer kernel and  $\cG_q $ is the linear map of representations
\footnote{{ More explicitly, if the RKHS kernel $K_q$ is a sum of separable kernels, then }by Mercer's theorem we can decompose it as $K_q=\sum_{\rho=1}^{d_q} \eta_{q,\rho} \phi_{q,\rho} \otimes \phi_{q,\rho}$ where   $\eta_{q,\rho} \geq 0$  are the eigenvalues, and  $\phi_{q,\rho}: \R^{D_q} \to\R^{d_q}$  are the orthonormal eigenfunctions of the integral operator associated with the kernel $K_q$. Then any $h_q \in \cH_q$ can be decomposed as $h_q=g_q\circ f_q$, where $f_q\in \cF_q$ is the feature map $f_q(\cX_q)_\rho=\sqrt{\eta_\rho}\phi_{q,\rho}(\cX_q)$ and $g_q\in \cG_q$ is linear $g_q(z_q) = w_q\cdot z_q$.}.
\er



The next theorem shows the case when $\cG_q$ are nonlinear with the $\kappa$-Lipschitz property, $\|g(z)-g(z')\|\leq \kappa \|z-z'\|$. One intermediate example is the stitching between the middle layers of neural networks.

\begin{thm}\label{thm2}
    Suppose $g_2$ is $\kappa_2$-Lipschitz. Again let $s_{1,2}$ be linear, identified with matrix $S_{1,2}$. With the spectral interpretations of  $\Sigma_{1,2}=\Eof{f_1f_2^T} = \text{diag}(\eta_1)^{1/2}C_{1,2}\text{diag}(\eta_2)^{1/2}$ and  $\tilde A_2 = \|I\|_{\eta_2} - \|C_{1,2}\|_{\eta_2}^2$ as Paragraph \ref{para:spectral}, we have
    \begin{equation}\label{equ:thm1}
        \cR^{\text{stitch}}_{1,2}(\cS_{1,2})\leq \cR_2(\cH_2) + \kappa_2^2 \tilde A_{2} + 2\kappa_2 (\tilde A_{2} \cR_2(\cH_2))^{1/2}.
    \end{equation}

    % Let $A_{1,2}=A(K_1,K_2)$ be kernel alignment and define kernel misalignment as $\tilde A_{1,2}=1-A_{1,2}$.
    % \[
    % \cR^{\text{stitch}}_{1,2}(\cS_{1,2})\leq \cR(\cH_2) + C_1 \tilde A_{1,2} + C_2 (\tilde A_{1,2} \cR(\cH_2))^{1/2}
    % \]
\end{thm}


\begin{proof}
Breaking $\cR^{\text{stitch}}_{1,2}(s_{1,2})$ into two parts and using Cauchy-Schwarz we get    
\begin{equation*}
\begin{split}
    % \cR^{\text{stitch}}_{1,2}(s_{1,2})
    & \Eof{\|g_2(S_{1,2} f_1)(x)-y\|^2}\\
    =& \Eof{\|(g_2(S_{1,2} f_1)(x)- g_2(f_2)(x))-(y-g_2(f_2)(x))\|^2} \\
    \leq & \cR_2(h_2) + \Eof{\|g_2(S_{1,2}f_1)(x)-g_2(f_2)(x)\|^2} + 2(\Eof{\|g_2(S_{1,2}f_1)(x)-g_2(f_2)(x)\|^2}\cR_2(h_2))^{1/2}.
\end{split}
\end{equation*}

% \[
% |\cR^{\text{stitch}}_{1,2}(s_{1,2})-\cR(h_2)| \leq \Eof{\|g_2(S_{1,2}f_1)-g_2(f_2)\|^2} + 2(\Eof{\|g_2(S_{1,2}f_1)-g_2(f_2)\|^2}\cR(h_2))^{1/2}
% \]
{ As $g_2$ is $\kappa_2$-Lipschitz, we can bound with the error from linearly regressing $f_2$ on $f_1$}
\begin{align*}
\Eof{\left\|g_2(S_{1,2}f_1)(x)-g_2(f_2)(x)\right\|^2} 
&\leq \kappa_2^2 \Eof{\|S_{1,2}f_1(x) - f_2(x)\|^2} \\
&= \kappa_2^2(\|S_{1,2}\|_{\eta_1}^2 + \|I\|_{\eta_2}^2 - 2\langle S_{1,2}, \Sigma_{1,2}^T\rangle)
\end{align*}
with $\|M\|_{\eta}^2 = \langle M, M\text{diag}(\eta)  \rangle$. Taking derivatives, we note that the minimizer of the RHS is $S_{1,2}=\Sigma_{1,2}^T \text{diag}(\eta_1)^{-1}$. Plugging in, the RHS reduces to $\kappa_2^2\tilde A_2$. Thus
\begin{align*}
    \cR_{1,2}^{\text{stitch}}(\cS_{1,2}) 
    &\leq \cR_{1,2}^{\text{stitch}}(\Sigma_{1,2}) \\
    &\leq \cR_2(\cH_2) + \kappa_2^2 \tilde A_{2} + 2\kappa_2 (\tilde A_{2} \cR_2(\cH_2))^{1/2}.
\end{align*}
\end{proof}

\br
In arguing that kernel alignment bounds stitching error for Theorem \ref{thm2}, we made several simplifying assumptions, which we now assess. Firstly, we restricted the stitching $\cS_{1,2}$ to linear maps, following the transformations commonly used in practice \citep{bansal2021revisiting,csiszarik2021similarity}, and to preserve the significance of the original representations. If we relax this assumption, we observe that a similar result holds, with $\tilde A_2 = \inf_{s_{1,2}\in \cS_{1,2}} \E[\| s_{1,2}(f_1(x)) - f_2(x) \|^2 ].$
Interestingly, for $s_{1,2}$ to use only information about the covariance of $f_1,f_2$, similarly to kernel alignment, $s_{1,2}$ must be linear. Furthermore, we note that for stitching classes that include all linear maps, the linear result remains valid.
\er

\br
Note that the notion of alignment that appears here, namely $\|I\|_{\eta_2}^2-\tilde A_2 = \|C_{1,2}\|_{\eta_2}^2=\|C_{1,2}\text{diag}(\eta_2)\|^2$, is similar to, yet distinct from, kernel alignment, which is  given by $\|\Sigma_{1,2}\|^2=\|\text{diag}(\eta_1)^{1/2} C_{1,2}\text{diag}(\eta_2)^{1/2}\|^2$. In particular, the spectrum $\eta_1$ is irrelevant for the bound. However, this does not hold if regularization is added to $S_{1,2}$ {by analogy to linear regression}.
\er

\br
If two representations are similar in the alignment sense, they are also similar in the stitching sense; however, the converse does not necessarily hold. By loose analogy to topology, this suggests that kernel alignment is a stronger notion of similarity. 
\er

Excess stitching risk can also serve as an intermediate result to bound the difference in risk. Let $\cY_1=\cY_2$ and $\cR_1=\cR_2$. To obtain a lower bound for $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})$ in a practical setting, we can assume that $\cS_{1,2} \circ \cG_2\subseteq \cG_1$. For models involving several compositions, such as deep networks, this condition can hold when stitching from a layer further from the output to a layer closer to the output (i.e., stitching forward), provided that the networks are similar and the layer indices are aligned at the end.
\begin{lem}\label{lem:lowerbound}
    Let $\cY_1=\cY_2=\cY$ and $\cR_1=\cR_2=\cR$. If $\cS_{1,2} \circ \cG_2 \subseteq \cG_1$ then $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})\geq \cR_1(\cH_1)$.
\end{lem}


The following theorem derives directly from  \eqref{equ:thm1} and Lemma \ref{lem:lowerbound}.

\begin{thm}
Let $\cY_1=\cY_2$ and $\cR_1=\cR_2=\cR$. Let $\cS_{1,2} \circ \cG_2 \subseteq \cG_1$ and let $g_q$ be $\kappa_q$-Lipschitz for $q=1,2$. Then
\[
\cR(\cH_1) - \cR(\cH_2)\leq \cR^{\text{stitch}}_{1,2}(\cS_{1,2}) - \cR(\cH_2)\leq \kappa_2^2 \tilde A_{2} + 2\kappa_2 (\tilde A_{2} \cR(\cH_2))^{1/2}.
\]
\end{thm}

\br
If we consider deep models and keep the $\cH_1,\cH_2$ the same but iterate over layers $j$ stitching forward, then
\[
\cR(\cH_1) - \cR(\cH_2)\leq \min_j \left\{(\kappa_2^{(j)})^2 \tilde A_{2}^{(j)} + 2\kappa_2^{(j)} (\tilde A_{2}^{(j)} \cR(\cH_2))^{1/2}\right\}.
\]


Alternatively, by making similar assumptions and swapping the index $1\leftrightarrow 2$, which requires $\cG_1=\cG_2$ up to a linear layer (due to the $\cS_{1,2} \circ \cG_2 \subseteq \cG_1$ condition), we get

\begin{align*}
    |\cR(\cH_1)-\cR(\cH_2)|
    &\leq \max_{i\in\{1,2\}}\left\{\kappa_i^2 \tilde A_{i} + 2\kappa_i (\tilde A_{i}\cR(\cH_q))^{1/2}\right\}.
\end{align*}


The above result can be stated informally as ``alignment at similar depth (measured backward from the output) bounds differences in risk".
\er


% We consider linear stitching (linear layer) to align the dimensionality of two models before connecting them together, which is also commonly used in practice \cite{}. The primary objective here is to evaluate the alignment between the two feature spaces (or layers). By adopting this simple linear stitching approach, we aim to preserve the significance of the original representations.


% First of all, we consider linear stitching (linear layer) to align the dimensionality of two models before connecting them together, which is also commonly used in practice \citep{bansal2021revisiting,csiszarik2021similarity}. The primary objective here is to evaluate the alignment between the two feature spaces (or layers). By adopting this simple linear stitching approach, we aim to preserve the significance of the original representations.


% \subsection{Practical Implications}
The results presented have several practical implications. First, we build on the experiments from \cite{huh2024position}, which provide evidence for the alignment of deep networks at a large scale using measures similar to kernel alignment. By establishing a connection between kernel alignment and stitching, our work supports building universal models that share architectures across modalities as scale increases. Second, we can elucidate the experiments from \cite{bansal2021revisiting}, which suggest that typical SGD minima have low stitching costs (stitching connectivity). This aligns with works that argue feature learning under SGD can be understood through the lens of adaptive kernels \citep{radhakrishnan2022mechanism, atanasov2022neural}.


\section{Conclusion}\label{sec:conclusion}
 In this paper, we review and unify several representation alignment metrics, including kernel alignment, distance alignment, and independence testing, demonstrating their equivalence and interrelationships. Additionally, we formalize the concept of stitching, a technique used in uni/multi-modal settings to quantify alignment in relation to a given task. Furthermore, we establish bounds on stitching error across different modalities and derive stitching error bounds based on misalignment, along with their generalizations and implications.
 
\section*{Acknowledgment}
L.R. is thankful to the CBMM-Hebrew University workshop organizers and Philipp Isola for the talk that  inspired this work.  L. R. acknowledges the financial support of the European Research Council (grant SLING 819789), the European Commission (Horizon Europe grant ELIAS 101120237), the US Air Force Office of Scientific Research (FA8655-22-1-7034), the Ministry of Education, University and Research (FARE grant ML4IP R205T7J2KP; grant BAC FAIR PE00000013 funded by the EU - NGEU). This work represents only the view of the authors. The European Commission and the other organizations are not responsible for any use that may be made of the information it contains.

 
 % One practical implication for the results relates to the experiments from \cite{huh2024position}. The authors show evidence for the alignment of deep networks at large scale using measures similar to kernel alignment and suggest multimodal alignment could be used as a proxy for performance. Our results connect kernel alignment to stitching and support building universal models sharing architecture across modalities as scale increases.
 

 % using measures similar to kernel alignment and suggests multimodal alignment could be used as a proxy for performance. 
 
  % {\color{red} Finally we proved a practical implication for the results. Recall \cite{huh2024position} shows evidence for the alignment of deep networks at large scale using measures similar to kernel alignment and suggests multimodal alignment could be used as a proxy for performance. Our result connecting kernel alignment to stitching would then support universal models sharing architecture across modalities.} 

 % , to the best of our knowledge, presented the first theoretical stitching error bound based on misalignment error. 
 
 % In practice, the scenario may occur when stitching across layers of the neural networks.
 % The study of representation alignment has several practical implications. Firstly, as described previously, \cite{huh2024position} shows evidence for the alignment of deep networks at large scale using measures similar to kernel alignment and suggests multimodal alignment could be used as a proxy for performance. Our result connecting kernel alignment to stitching would then support universal models sharing architecture across modalities. 

% Secondly,  measuring alignment can be seen as a proxy for transferability. Linear stitching assumes that the alignment of feature spaces can be measured using a linear transformation.  If it achieves high alignment, it indicates that the two feature spaces are well-aligned, suggesting better transferability. Conversely, poor alignment suggests that the representations differ significantly in structure or distribution or more advanced stitching transformations might be necessary in real practice. {\red For example, non-linear transformation layers are added between the encoder and decoder in certain image processing tasks and domain-specific layers or modules are designed to process and align features from different modalities before combining them \citep{vinyals2015show,radford2021learning}.}

% One more special case. Suppose $S$ is a superset of linear maps but $g_q$ are linear. Then $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})\leq \cR(\cH_1)$ and bound above replaces $\tilde A_{1,2}$ with $w_2^T(I - \Sigma_{1,2}^T\Sigma_{1,2})w_2$.

% {\red experiment: the key points are to evaluate $\cR(\cH_1), \cR(\cH_2)$ and $A(K_1,K_2)$ for what models.} \\
% (1)From Isola's experiments, they have "extract-features.py". Is this the same as our $f_q$ or $K_i$? Then can we get the $\hat{A}(K_1,K_2)$ based on that? If yes, the problem will be how to get $\cR(\cH_q)$ or $\widehat{\cR}(\cH_q)$. Then stitching $s_{1,2}$...\\
% {\blue yes, can be done but solving for stitching may be a little tricky since model is $h = g_2\circ s_{1,2}\circ f_1$ hence $\nabla_{s_{1,2}}h$ also involves $\nabla_z g_2$}

% (2) Is it possible to generate simple data using simple functions? The first Q maybe how to describe the data in different modalities? The benefits maybe it's easier to know $\cR(\cH_q)$....

% \section{Experiments}\label{sec:experiment}

% In this section we provide numerical experiments highlighting properties of kernel alignment and stitching.

% \section*{}
% \subsubsection*{Author Contributions}
% \subsubsection*{Acknowledgments}
% L. R. acknowledges the financial support of the European Research Council (grant SLING 819789), the European Commission (Horizon Europe grant ELIAS 101120237), the US Air Force Office of Scientific Research (FA8655-22-1-7034), the Ministry of Education, University and Research (FARE grant ML4IP R205T7J2KP; grant BAC FAIR PE00000013 funded by the EU - NGEU) and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. This work represents only the view of the authors. The European Commission and the other organizations are not responsible for any use that may be made of the information it contains.									
% \subsubsection*{Ethics Statement}
% \subsubsection*{Reproducibility Statement}


\bibliography{arxiv_align}
\bibliographystyle{apalike}

\section{Appendix}


\subsection{Alignment to task}\label{subsec:def_al_task}

Here we mention ideas of alignment between a representation and task used to estimate generalization error and characterize spectral contributions to sample complexity.

\paragraph{Kernel alignment risk estimator (KARE)}

In \cite{jacot2020kernel} we have the following definition for KARE which is an estimator for risk.
\[
\rho(\lambda, y_n, K_n)  = \frac{\frac{1}{n}\langle (K_n/n + \lambda I)^{-2}, y_ny_n^T\rangle}{(\frac{1}{n}\text{Tr}[(K_n/n + \lambda I)^{-1}])^2}
\]
This was also obtained in \cite{golub1979generalized}, \cite{wei2022more}, \cite{craven1978smoothing}.


\paragraph{Spectral task-model alignment}

From \cite{canatar2021spectral}, we have a definition for the cumulative power distribution which quantifies task-model alignment.  
\[
 C(n) = \frac{\sum_{i \leq n}\eta_i w_i^2}{\sum_{i}\eta_i  w_i^2}
\]
Here $K=\sum_i \eta_i \phi_i \otimes \phi_i$, $\langle \phi_i, \phi_j\rangle =\delta_{i,j}$, and target $h_\mu = \sum_i w_i \sqrt{\eta_i} \phi_i$. $C(n)$ can be interpreted as fraction of variance of $h_\mu$ explained by first $n$ features. The faster $C(n)$ goes to 1, the higher the alignment. 


% Spectral bias, task-model alignment and noise explain generalization in kernel regression.
\paragraph{Source Condition}
From \cite{rosasco2005spectral} we have bounds on generalization of kernel ridge assuming some regularity of $h_\mu$, called source condition
\[
h_\mu \in \Omega_{r,R} = \left\{h\in L^2(X,\rho): h= L_K^r v,  \|v\|_K\leq R \right\}
\]
Assuming $h_\mu = \sum_i w_i \sqrt{\eta_i} \phi_i$, then the statement can be rewritten as 
\[
\sum_{i=1}^\infty \frac{\eta_i w_i^2 }{\eta_i^{2r}} < \infty
\] 


\br
KTA appears in several theoretical applications. \cite{cristianini2001kernel} bounds generalization error of Parzen window classifier \footnote[1]{ \cite{cortes2012algorithms} notes error in proof since implicitly assumes $\max_x \Esub{x'}{K^2(x,x')}/\Esub{x,x'}{K^2(x,x')} = 1$ making kernel constant. However proof can be saved with an additional assumption.}.  \cite{Cristianini2006,cortes2012algorithms} show that there exist predictors for which kernel target alignment (KTA) $A(K,yy^T)$ bounds risk.
    \[
    h(x)=\frac{\Esub{x',y'}{K(x,x')y'}}{\Esub{x',x}{K(x,x')^2}} \Rightarrow \cR(h) \leq  2(1 - A(K,yy^T))
    \]
{ Furthermore, several authors including \cite{atanasov2022neural, paccolat2021geometric, kopitkov2020neural, fort2020deep, shan2021theory} use KTA to study feature learning and Neural Tangent Kernel evolution.}
\er




\subsection{Other notions for alignment from independence testing}\label{subsec:def_it}

\paragraph{Constrained Covariance (COCO)}  Then  \cite{gretton2005measuring} proposed the concept of constrained covariance as the largest singular value of the cross-covariance operator,

\[
    \text{COCO}(\mu, \cH_1, \cH_2) = \sup \{ \text{cov}[h_1(x_1),h_2(x_2)]: h_1\in \cH_1, h_2\in \cH_2\} %{\red ?=} \|\Sigma_{1,2}\|_{2}
\]



\paragraph{Kernel Canonical Correlation (KCC)}

From \cite{bach2002kernel}
\[
    \text{KCC}(\mu, \cH_1, \cH_2, \kappa) = \sup \left\{ \frac{\text{cov}[h_1(x_1),h_2(x_2)]}{(\text{var}(h_1(x_1)) + \kappa \|h_1\|^2_{\cH_1})^{1/2}(\text{var}(h_2(x_2)) + \kappa \|h_2\|^2_{\cH_2})^{1/2}}: h_1\in \cH_1, h_2\in \cH_2\right\}
\]


The next two are bounds on mutual information from correlation and covariance respectively


\paragraph{Kernel Mutual Information (KMI)}
From \cite{bach2002kernel}
\[
    \text{KMI}(\cH_1, \cH_2) =   -\frac{1}{2}\log(|I -(\kappa_{1,n}\kappa_{2,n})K_{1,n}K_{2,n}|)
\]
where kernels are centered and $\kappa_{q,n} = \min_i\sum_j K_q(x_{q,i},x_{q,j})$ but empirically $\kappa = 1/n$ suffices.

% \paragraph{Kernel Generalized Variance (KGV)}
%  From \cite{bach2002kernel}
% \[
%     \text{KGV}(\cH_1, \cH_2) =   -\frac{1}{2}\log\frac{\cK_n}{\cD_n}
% \]





\subsection{Additional Proofs}\label{additionalproofs}
In this section, we provide the detailed proofs of Lemmas presented in and Section \ref{sec:alignment} and Section \ref{sec:stitching}.


We begin with the proof of Lemma \ref{lem:statsKA}. For completeness, we first restate the lemma below.

\begin{lem}\label{lem:statsKAproof}
    Assume $|K_q(x_q,x_q')|\leq C_q$. Let $\hat A_{1,2}(X) = \hat A_{1,2}((x_1^1,x_2^1),\ldots,(x_1^n,x_2^n))=\frac{1}{n^2}\langle K_1, K_2\rangle_F$. Let $A_{1,2}=\E{\hat A_{1,2}}$, $\hat A = \frac{\hat A_{1,2}}{\sqrt{\hat A_{1,1}\hat A_{2,2}}}$, and $A =  \frac{ A_{1,2}}{\sqrt{ A_{1,1} A_{2,2}}}$. Then with probability at least $1-\delta$, and $\eps =\sqrt{(32/n)\log (2/\delta)}$, we have $|\hat A-A|\leq C(X) \eps$, where $C(X)$ is non-trivial function.
\end{lem}
\begin{proof} Let $({x_1^i}',{x_2^i}')=(x_1^i,x_2^i)$ for all $i=1,\ldots n$ except  $k$. Then

$D_{ij} = K_1(x_1^i,x_1^j) K_2(x_2^i,x_2^j) - K_1( {x_1^i}',{x_1^j}') K_2({x_2^i}',{x_2^j}')$ and note $|D_{ij}|\leq 4C_1C_2$. Then
\[
|\hat A_{1,2}(X)-\hat A_{1,2}(X')| = n^{-2}\left(2\sum_{j\neq i} |D_{ij}| + |D_{ii}|\right) \leq 4C_1C_2\frac{2n - 1}{n^2}\leq \frac{8C}{n}
\]

Applying McDiarmid, we get
\[
P\{|\hat A_{1,2}- A_{1,2}|\geq \eps\} \leq 2\exp{\left(\frac{-\eps^2n}{32C^2}\right)}
\]
which can also be read as, with probability at least $1-\delta$, $|\hat A_{1,2}- A_{1,2}|\leq \epsilon = \sqrt{(32/n)\log (2/\delta)}$

Finally, we show that $|\hat A_{i,j}-A_{i,j}|\leq \epsilon$ for $i,j\in \{1,2\}$ gives $|\hat A - A|\leq C(X) \epsilon$.

\begin{align*}
    |\hat A -  A |
    =& \left|\hat A_{1,2}(\hat A_{1,1}\hat A_{2,2})^{-1/2} - A_{1,2}(A_{1,1} A_{2,2})^{-1/2}\right| \\
    =& |\hat A_{1,2} - A_{1,2}|(\hat A_{1,1}\hat A_{2,2})^{-1/2} + A_{1,2}\left|(\hat A_{1,1}\hat A_{2,2})^{-1/2}-( A_{1,1} A_{2,2})^{-1/2}\right|\\
    =& |\hat A_{1,2} - A_{1,2}|(\hat A_{1,1}\hat A_{2,2})^{-1/2} \\
    &+ A_{1,2}\left(\left|(\hat A_{1,1}\hat A_{2,2})^{-1/2}-( A_{1,1} \hat A_{2,2})^{-1/2}\right|+ \left|(A_{1,1}\hat A_{2,2})^{-1/2}-( A_{1,1}  A_{2,2})^{-1/2}\right|\right)
\end{align*}

Lastly, we can use \[
(x^{-1/2}-y^{-1/2})=\frac{y^{1/2}-x^{1/2}}{(xy)^{1/2}}=\frac{y-x}{(xy)^{1/2}(y^{-1/2}+x^{-1/2})}
\]

\end{proof}

Now we are in the position to prove Lemma \ref{lem:stit}. To complete this, we first restate the lemma below.

\begin{lem} Suppose $\text{dim}(\cY_1)=\text{dim}(\cY_2)=d$ and $\cR_1=\cR_2$. Let $g_q\in \cG_q$ be linear with $g_q(z_q) = W_q z_q$ and $W_q\in\R^{d \times d_q}$. Let $s_{1,2}:\cZ_1\to\cZ_2$ be linear with $s_{1,2}(z_1)=S_{1,2} z_1$ and $S_{1,2}\in \R^{d_2\times d_1}$. Then $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})=\cR_1(\cH_1)$.
\end{lem}

% \begin{proof}
% Assume a minimizer is $h_1^* = g_1^* \circ f_1^*$ with $g_1^*(z_1) = w_1^* \cdot z_1$.  Let $h_{1,2}=g_2\circ s_{1,2}\circ f_1^* \in \cH_{1,2}$. By linearity of $g_2\circ s_{1,2}$, we have $
% h_{1,2} = (w_2^T S_{1,2}) \cdot f_1^* \in \cH_1$. Hence, $\cR^{\text{stitch}}(\cS_{1,2})\geq \cR(\cH_1)$.  Note too that the minimizer $h_1^*\in \cH_{1,2}$ because choosing $S_{1,2}=w_2^\dagger w_1$ results in $h_1=h_{1,2}$, which implies that $\cR^{\text{stitch}}(\cS_{1,2})\leq \cR(\cH_1)$. 
% \end{proof}

\begin{proof}
    For the linear case, there exists a vector $W_q \in \R^{d \times d_q}$, such that $g_q(z_q) = W_q z_q, z_q \in \R^{d_q}$. We can write the error of stitching as
\begin{align*}
\cR^{\text{stitch}}_{1,2}(s_{1,2}) 
&= \Eof{\|W_2 S_{1,2}f_1-y\|^2}\\
&=\Eof{\|(W_2 S_{1,2} - W_1)f_1\|^2}+\Eof{\|W_1 f_1(x)-y\|^2} \\
&=\|W_2 S_{1,2} - W_1\|_{\eta_1}^2 + \cR_1(h_1),
\end{align*}

where we used that for $W_1$ to be optimal, we require $\partial_{W_1}\cR_1(h_1)=\Eof{(W_1 f_1-y)f_1^T}=0$.
Minimizing with respect to $S_{1,2}$ yields
\begin{align*}
    \cR^{\text{stitch}}_{1,2}(\cS_{1,2}) &= \|\Pi_2^\perp W_1\|_{\eta_1}^2+\cR_1(\cH_1),
\end{align*}
where we use $\Pi_2=I - (W_2^T \text{diag}(\eta_1) W_2)^{\dagger}W_2^T \text{diag}(\eta_1)$ to denote the residual of the generalized $\eta_1$-projection onto (column) span of $W_2$. We note that in general, as long as { $d\leq d_2$}, we have $\cR^{\text{stitch}}_{1,2}(\cS_{1,2})=\cR_1(\cH_1)$.
\end{proof}



\end{document}
