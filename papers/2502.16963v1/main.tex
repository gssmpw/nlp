\documentclass[10pt,conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{marvosym}


% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,fill, inner sep=0pt, minimum width=0.3cm] (char) {\textcolor{white}{#1}};}}

\newcommand{\hpcayear}{2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% -- UPDATE -- %%%%%%%%%%%%%%%
\newcommand{\hpcasubmissionnumber}{306}
\title{Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% -- ONLY FOR CAMERA READY -- %%%%%%%%
\def\hpcacameraready{} % Uncomment to build camera-ready version
\newcommand{\hpcapubid}{0000--0000/00\$00.00}
\newcommand\hpcaauthors{Lian Liu\textsuperscript{$1, 2, 3, \dagger$}, Shixin Zhao\textsuperscript{$1, 2, \dagger$}, Bing Li\textsuperscript{4}, Haimeng Ren\textsuperscript{1,5}, Zhaohui Xu\textsuperscript{1,5}, \\ Mengdi Wang\textsuperscript{1,2}, Xiaowei Li\textsuperscript{1,2,3}, Yinhe Han\textsuperscript{1,2}, and Ying Wang\textsuperscript{1,2, \Letter}}
\newcommand\hpcaaffiliation{Institute of Computing Technology, Chinese Academic of Sciences\textsuperscript{$1$}, \\ University of Chinese Academy of Sciences \textsuperscript{$2$}, Zhongguancun Laboratory\textsuperscript{$3$}, \\
Institute of Microelectronics, Chinese Academy of Sciences\textsuperscript{$4$}, \\
School of Information Science and Technology, ShanghaiTech University\textsuperscript{$5$}}
\newcommand\hpcaemail{ \{liulian211, zhaoshixin18\}@mails.ucas.ac.cn \quad libing2024@ime.ac.cn \quad \{renhm2022, xuzhh12022\}@shanghaitech.edu.cn \\ \{wangmengdi, lxw, yinhes, \textcolor{blue}{wangying2009}\}@ict.ac.cn}

%%%%% -- ARTEFACT EVALUATION RESULTS -- %%%%%%
% Uncomment the following based on the badges that were awarded to this paper1
%\def\aeopen{}           % The artifact is publically available
%\def\aereviewed{}     % The artefact has been reviewed
%\def\aereproduced{} % The results have been reproduced
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Text/macro}
\input{hpca-template}

\begin{abstract}
The billion-scale Large Language Models (LLMs) necessitate deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability. As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the current trend. This has sparked extensive research into relocating LLM parameters from expensive GPUs to external host memory. However, the restricted bandwidth between the host and GPU memory limits the inference performance of existing solutions.

This work introduces Hermes, a budget-friendly system that leverages the near-data processing units (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference. We recognize that the inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed ``hot" and ``cold" neurons, respectively. Hot neurons, which consist of only approximately 20\% of all weight parameters, account for 80\% of the total computational load. In contrast, cold neurons make up the other 80\% of parameters but are responsible for just 20\% of the computational workload. Leveraging this observation, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU without large-capacity HBMs, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities. In addition, the dynamic nature of activation sparsity necessitates a real-time partition of hot and cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules. To tackle these issues, we introduce a lightweight predictor that ensures optimal real-time neuron partition and adjustment between GPU and NDP-DIMMs. Furthermore, we utilize a window-based online scheduling mechanism to maintain load balance among multiple NDP-DIMM modules. In summary, Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at a rate of 13.75 tokens/s and realizes an average 75.24$\times$ speedup over the state-of-the-art offloading-based inference system on popular LLMs.

\end{abstract}

\maketitle % should come after the abstract
\thispagestyle{empty}
\pagestyle{empty}

% \pagestyle{plain} % should come right after \maketitle
\def\thefootnote{$\dagger$}\footnotetext{Both authors contributed equally to this research}\def\thefootnote{\arabic{footnote}}
\def\thefootnote{\Letter}\footnotetext{Corresponding author}\def\thefootnote{\arabic{footnote}}


\input{Text/1-Introduction}
\input{Text/2-Background}
\input{Text/3-Motivation}
\input{Text/4-System}
\input{Text/5-Evaluation}
\input{Text/6-Related-works}
\input{Text/7-Conclusion}

\section{Acknowledgments}
We sincerely thank the anonymous reviewers for their insightful suggestions. This work was partially supported by the National Key R\&D Program of China (Grant No. 2023YFB4404400) and the National Natural Science Foundation of China (Grant No. 62222411, 62204164). Ying Wang is the corresponding author (wangying2009@ict.ac.cn).

\bibliographystyle{plain}
\bibliography{references}

\end{document}

