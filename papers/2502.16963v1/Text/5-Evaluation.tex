\section{Evaluation}

\begin{table}
    \centering
    \caption{Configuration details of NDP-DIMM.}
    \vspace{-0.3cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c}
    \hline
    \multicolumn{3}{c}{\textbf{NDP core}} \\
    \hline
    \multicolumn{3}{c}{Configuration: 256 multipliers, reduction tree-based accumulator, Buffer size: 256KB}\\ 
    \hline
    One NDP core per DIMM & Frequency: @ 1 GHz  & area overhead: $1.23mm^2$ per core\\
    \hline
    \multicolumn{3}{c}{\textbf{DIMM Parameters}} \\
    \hline
     \multicolumn{3}{c}{DDR4-3200, 32GB/DIMM$\times$8, \update{2 DIMMs/channel}}\\
     \multicolumn{3}{c}{4 rank/DIMM, 2 bank groups/rank, 4 bank/BG}\\
     \hline
    \multicolumn{3}{c}{\textbf{DIMM Timing}} \\
    \hline
     \multicolumn{3}{c}{tRC=76, tRCD=24, tCL=24, tRP=24, tBL=4}\\
     \multicolumn{3}{c}{tCCD S=4, tCCD L=8,tRRD S=4, tRRD L=6, tFAW=26}\\
    \hline
    \multicolumn{3}{c}{\textbf{DIMM-Link Parameters}} \\
     \hline
    \multicolumn{3}{c}{25Gb/s/Lane, 1.17 pJ/b, 8 $\times$ Lanes (25GB/s per Link)} \\
    \hline
    \end{tabular}
    }
    \label{tab:dimmcfg}
\vspace{-0.3cm}
\end{table}

\subsection{Experimental Setup}\label{sec:experimental-setup}

\subsubsection{\name~System}
The proposed \name~system integrates a single NVIDIA RTX 4090 GPU with 24GB of graphic memory \update{and 330 tensor TOPS (FP16)} to process hot neurons. Additionally, we provide 8 NDP-DIMMs, each including 32GB DDR4 memory as the extension of GPU memory. We use PCIe 4.0 to support data interaction between NDP-DIMMs and GPU memory with a bandwidth of 64GB/s. The kernel performance of the NVIDIA RTX 4090 is measured using NVIDIA Nsight Compute~\cite{nsight}. Furthermore, we develop an in-house simulator by modifying Ramulator 2.0~\cite{luo2023ramulator, ramulator2.0} to evaluate the performance efficiency of NDP-DIMM devices. For the NDP core, we implemented it in RTL and synthesized it using the Synopsys Design Compiler~\cite{synopsys.org} with the TSMC 7nm technology. \tab \ref{tab:dimmcfg} shows the configuration details of adopted NDP-DIMMs.

\subsubsection{Baseline Systems}
We selected several offloading-based inference systems, such as Huggingface Accelerate~\cite{jain2022hugging, huggingface-accelerate}, FlexGen~\cite{sheng2023flexgen}, and Deja Vu~\cite{liu2023deja}, as the baselines. FlexGen and Deja Vu are restricted to OPT models. Moreover, Deja Vu, initially optimized for LLM activation sparsity within high-performance distributed systems, has been adapted to support offloading-based serving systems. In contrast to \name, these methods depend solely on the basic host memory to expand capacity without offering additional computational resources. \update{We also provided a system (Hermes-host) that offloads cold neurons to the host CPU while handling hot neurons on GPU, demonstrating the necessity of NDP-DIMMs. Hermes-host follows the configuration in~\cite{song2023powerinfer}, which equips an Intel i9-13900K processor as the host CPU (providing a maximum bandwidth of 89.6 GB/s), and also uses a single NVIDIA RTX 4090 as the GPU for hot neurons.} Additionally, to highlight the significance of activation sparsity in boosting \name~system efficiency, we also compare \name~against a straightforward NDP-DIMM extended system (referred to as Hermes-base) that does not leverage activation sparsity in LLMs.

\begin{figure}
    \centering
    \includegraphics[width=.98\linewidth]{Fig/end1_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{Performance comparison with existing offloading-based systems.}}
    \label{fig:offloading-performance}
\vspace{-0.3cm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Fig/end2_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{The effectiveness of activation sparsity and NDP design on \name.}}
    \label{fig:base-hermes-performance}
\vspace{-0.3cm}
\end{figure}

\subsubsection{Workloads}
We chose OPT-13B, OPT-30B, OPT-66B~\cite{zhang2022opt}, LLaMA2-13B, LLaMA2-70B~\cite{touvron2023llama2}, and Falcon-40B~\cite{almazrouei2023falcon} as target models. For the OPT series models, we utilized their native ReLU activations to achieve activation sparsity. For the LLaMA2 and Falcon models, we use the open-source models\footnote{The modified LLMs can be found at \href{https://huggingface.co/SparseLLM}{https://huggingface.co/SparseLLM}, including both LLaMA2 and Falcon models} that substituted their original activation functions with ReLU~\cite{mirzadeh2023relu, zhang2024relu}. Furthermore, we added additional ReLU functions before generating QKV to achieve activation sparsity in self-attention blocks. Evaluation results show that these alterations result in negligible accuracy loss (under 1\%). \update{Furthermore, we adopt ChatGPT prompts~\cite{gpt-prompts} and Alpaca~\cite{alpaca} as the datasets to evaluate the end-to-end performance, following configurations in \cite{xue2024powerinfer, song2023powerinfer}.}
% \todo{The dataset used for evaluation with description of the distribution of the prompt and generated output lengths}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Fig/batching_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{End-to-end performance on different batch sizes (ranging from 1 to 16). N.P. denotes the model is not supported by the current inference system.}}
    \label{fig:batching-inference}
\vspace{-0.3cm}
\end{figure*}

\subsubsection{Evaluation Metric}
Given our focus on local deployment scenarios, we primarily optimized LLM inference with small batch sizes. We concentrated on the average number of tokens generated per second (tokens/s) to evaluate model inference efficiency. Hereafter, the number above each bar in each figure indicates the end-to-end generation speed (tokens/s). In our experiments, we used batch sizes between 1 and 16, and kept the lengths of both input and output sequences fixed at 128.


% Moreover, 

% Moreover, we concentrated on token-to-token latency (ms/token) and the average number of tokens generated per second (tokens/s) to evaluate model inference efficiency.

\subsection{\name~Performance}\label{sec:end-to-end}

\subsubsection{End-to-End Performance}
We begin by evaluating the end-to-end inference performance of \name~and baseline systems at a batch size of 1, which is commonly used for local deployments~\cite{cai2023medusa}. Noting that FlexGen and Deja Vu are limited to support OPT family models, we first compare \name~against existing offload-based inference systems on OPT models. \update{Additionally, we evaluate the Hermes-host and Hermes-base systems' performance across various LLMs to illustrate the necessity of NDP-DIMMs design and activation sparsity in Hermes, respectively.}
% 这里需要包含两个部分，分别是 end-to-end performance 和 token-to-token latency

\textbf{Comparison with Offloading-based Systems. } \fig \ref{fig:offloading-performance} presents the end-to-end performances on OPT family models. Compared with the Accelerate and FlexGen systems, \name~can achieve an average $578.42 \times$ and $247.25 \times$ speedup, respectively. \name~is capable of achieving a rate of $20.37$ tokens/s for OPT-66B, which substantially surpasses current inference systems. In contrast, Deja Vu only attains an average speedup of $2.12 \times$ over FlexGen due to the necessity of loading cold neurons. The frequent data transfer on PCIe compromises the performance improvement of activation sparsity, while the expensive MLP-based predictor used in Deja Vu further diminishes its benefits. Compared to OPT-13B, \name~achieves greater performance gains on OPT-66B. This is because 80\% of the parameters in OPT-13B can be stored in GPU memory, whereas only 15\% of parameters in OPT-66B can be stored in GPU memory. This further exacerbates the data transfer overhead between host memory and GPU memory. 
% Fortunately, \name~can effectively utilize NDP-DIMMs to process the offloaded parameters without introducing significant data movement.

\textbf{Necessity of Activation Sparsity. } We further compare \name~with the Hermes-base system, which only adopts a na\"ive NDP-DIMM extended system without utilizing activation sparsity, as shown in \fig \ref{fig:base-hermes-performance}. 
The Hermes-based system processes the FC layers on the GPU when their parameters are available, switches to NDP-DIMMs when their parameters are stored in those modules, and offloads all attention computations to NDP-DIMMs.
% The Hermes-based system activates nearby processing units based on the location of parameters to compute FC operators and offloads all attention computations to NDP-DIMMs. 
This approach leverages the high internal bandwidth of NDP-DIMMs and
reduces data transfer between DIMMs and GPU memory. In comparison to Huggingface Accelerate, the Hermes-base system can achieve $53.89 \times$ speedup on average, as it greatly reduces the data transfer on PCIe. By effectively leveraging activation sparsity in LLMs, Hermes outperforms the Hermes-base system with average speedups of $5.17 \times$, specifically for large models such as Falcon-40B and
LLaMA2-70B. This is due to when running large models, most layers are offloaded on the computation-limited NDP-DIMMs for the Hermes-base system. 

\update{\textbf{NDP-DIMMs instead of host CPU. } 
Experimental results in Figure \ref{fig:offloading-performance}, \ref{fig:base-hermes-performance} demonstrate the necessity of NDP-DIMMs. Hermes achieves $4.79\times$ - $7.75\times$ speedup when compared to Hermes-host. Specifically, the Hermes-host system also utilizes the hot/cold neuron partition, but computes the cold neurons on the host CPU. This approach effectively alleviates the burdensome data loading on PCIe for existing offloading-based systems. In comparison to Huggingface Accelerate and FlexGen, the Hermes-host system can achieve $62.00 \times$ and $44.96\times$ speedup on average, respectively. However, the memory bandwidth on the CPU side is significantly lower than that of NDP-DIMMs, making the Hermes-host system still far less efficient than our proposed Hermes system. }



\subsubsection{Batching Inference}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Fig/breakdown.pdf}
    \vspace{-0.3cm}
    \caption{Evaluating the performance breakdown on Deja Vu, \name, and \name-base (H-base) on various LLMs with different batch sizes. }
    \label{fig:performance-breakdown}
\vspace{-0.3cm}
\end{figure*}

We also evaluate the end-to-end performance of \name~with different batch sizes. As shown in the \fig \ref{fig:batching-inference}, \name~demonstrates consistent performance improvement with the batch sizes varying from 1 to 16. Hermes attains average speedups of $148.98\times$ and $75.24\times$ for various batch sizes when compared to FlexGen and Deja Vu, respectively, offering promising support for larger batch sizes. \update{Furthermore, \name~achieves an average $7.17 \times$ speedup over \name-host for various batch sizes. As the batch size increases, the performance gap between Hermes-host and Hermes becomes more pronounced. This occurs as the consumer-grade GPU with sufficient computation capability is minimally impacted by larger batch sizes, whereas the dynamic loading overhead of cold neurons is closely tied to bandwidth. Consequently, as batch sizes grow, the limited memory bandwidth on the CPU side increasingly affects overall system performance.} The performance gap between \name~and the Hermes-base system is the smallest when the batch size is 2. This is because for \name-base, the computation capability of the NDP core can still effectively handle the corresponding computational load, and larger batches can effectively amortize the DRAM cell access overhead as weight parameters are reused by the two batches. At other batch sizes, \name~demonstrates a significant performance advantage over Hermes-base. First, at a batch size of 1, Hermes can utilize activation sparsity to significantly reduce the number of neurons that need to be activated, thereby lowering data access overhead. Second, as the batch size increases, Hermes is not constrained by the computation capability of NDP-DIMMs due to the presence of activation sparsity. 

\subsection{Ablation Studies}\label{sec:ablation-study}
% 要包括这样几种: offline modeling and mapping; online placement; load balance optimization 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/ab_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{Ablation study on proposed offline and online scheduling strategies.}}
    \label{fig:ablation-study}
\vspace{-0.3cm}
\end{figure}



To evaluate the scheduling strategies proposed in Section \ref{sec:hermes-system}, we compare the normalized inference latency on MLP block for different LLMs with various scheduling settings. Specifically, \name-random denotes utilizing a random offline mapper to achieve neuron placement, \name-partition denotes that it only considers the optimal offline neuron placement, \name-adjustment denotes the system that further uses online adjustment for hot/cold neuron partition, and \name~is the one that integrates all the scheduling strategies proposed in Section \ref{sec:hermes-system}. \update{Furthermore, we also explore when only adopting token-wise prediction or layer-wise prediction to guide the online adjustment of hot/cold partition, denoted as Hermes-token-adjustment and Hermes-layer-adjustment, respectively.} 

\textbf{Load Balancing with Multi-level Optimization. } \fig \ref{fig:ablation-study} shows the contributions of each component in \name~. Utilizing the offline mapper can effectively identify the frequent hot neurons, reducing the computation cost of NDP-DIMMs. As a result, \name-partition can achieve $1.63 \times$ speedup than \name-random. However, the input-specific nature of activation sparsity challenges the offline partition approach. Therefore, further adopting online adjustment for hot/cold partition (\name-adjustment) achieves $1.33 \times$ performance gains over \name-partition. Despite this, the overall execution efficiency is still constrained by the NDP-DIMMs, which possess limited computation capability. Thus, the performance of the resource-constrained NDP-DIMMs can be improved by tackling the load imbalance issues in several NDP-DIMMs. The introduced online remapping method successfully addresses this problem. As a consequence,
the fully optimized Hermes system demonstrates a $1.29 \times$ boost in performance when compared with \name-adjustment.
% 这里就是分析每一部分的优势

\update{\textbf{Benefits of Token-wise and Layer-wise Prediction.}
Compared to \name-partition which only considers the optimal offline neuron placement, \name-token-adjustment and \name-layer-adjustment can achieve $1.08\times$ and $1.11\times$ speedup, respectively, demonstrating the benefits of online adjustment. However, token-wise prediction cannot address fluctuations in neuron activity, making it inaccurate for frequent changes in hot/cold neurons. Simultaneously, layer-wise prediction only relies on the static sampled neuron correlation table to guide the online adjustment, inefficient for constant changes of online adjustment. As a result, using token-wise or layer-wise prediction only cannot effectively unleash the benefits of prediction-based online adjustment.  
}

\subsection{Performance Breakdown}\label{sec:breakdown}

% 这里要分析几个点，首先是 deja vu 中的load weight 的影响，communication 的开销，以及predictor 的开销；然后是 prefill 的开销占比；然后是计算的开销

\fig \ref{fig:performance-breakdown} illustrates the performance breakdown of Deja Vu, \name-base, and \name~on various LLMs. It provides detailed insights into the efficiency sources of \name.

Figure \ref{fig:performance-breakdown}a shows that while Deja Vu benefits from activation sparsity, it still requires loading cold neurons when activated, resulting in communication costs—especially PCIe data transfer—comprising about 89\% of the execution time. On the right side of Figure \ref{fig:performance-breakdown}a, we disregard the effect of communication on performance. The MLP-based predictor in Deja Vu consumes roughly 18.1\% of computation time, further reducing the gains from activation sparsity. Our lightweight predictor, in contrast, contributes less than 0.1\% to runtime overhead. Even with communication costs lowered through reusable neurons at large batch sizes, Deja Vu's performance remains inferior to Hermes.

Figure \ref{fig:performance-breakdown}b compares \name-base and \name. Without activation sparsity, \name-base incurs higher computation costs, especially as batch sizes increase, due to intensive computation on NDP-DIMMs. For example, running LLaMA2-70B offloads over 80\% of computation to NDP-DIMMs, leading to a substantial portion of the execution time being occupied by FC computation. In Hermes, token generation takes 66.40\% of execution time at batch size 1. After optimizing token generation, the prompting stage becomes the bottleneck, accounting for about 33.01\% of the overhead, limiting further inference efficiency improvements.

% \fig \ref{fig:performance-breakdown} presents the performance breakdown of Deja Vu, \name-base and \name~on OPT-30B, OPT-66B, Falcon-40B and LLaMA2-70B. 
% It effectively describes in detail the sources of efficiency of \name. 

% As \fig \ref{fig:performance-breakdown}a shows, despite benefiting from activation sparsity, Deja Vu still needs to load cold neurons when they are activated. Consequently, the communication cost, particularly data transfer on PCIe, makes up about 89\% of the total execution time. On the right side of Figure \ref{fig:performance-breakdown}a, we disregard the effect of communication on performance. The MLP-based predictor used in Deja Vu consumes approximately 18.1\% of the overall computation time, diminishing the gains from activation sparsity. In contrast, our proposed lightweight predictor contributes to less than 0.1\% of the total runtime overhead. The proportion of communication in Deja Vu reduces due to reusable neurons when running at large batch sizes. Nonetheless, the overall performance of Deja Vu is still significantly inferior to Hermes.

% \fig \ref{fig:performance-breakdown}b provides a comparative analysis of the performance breakdown between \name-base and \name. The absence of activation sparsity in the Hermes-base results in considerably higher computation costs compared to \name, especially as the batch size increases. This is primarily due to the intensive computation on NDP-DIMMs, which significantly impacts overall execution efficiency. For instance, when running LLaMA2-70B, over 80\% of the computation is offloaded to NDP-DIMMs, leading to a substantial portion of the execution time being occupied by FC computation. In contrast, in Hermes, the token generation time occupies 66.40\% of the total execution time when at batch size is 1. With the token generation stage fully optimized, the prompting stage becomes the bottleneck, accounting for approximately 33.01\% of the overhead, thus limiting further improvements in inference efficiency.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/dimm_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{Throughput of four typical LLMs with different numbers of NDP-DIMMs. N.P. denotes the model is not supported by current system.}}
    \label{fig:dimm}
\vspace{-0.3cm}
\end{figure}

\update{\subsection{Sensitivity Studies}}

\subsubsection{\update{Sensitivity analysis of the number of DIMMs}}

\update{\fig~\ref{fig:dimm} illustrates the improvement in LLM throughput as the number of NDP-DIMMs increases. We evaluated four distinct LLM models using a single batch to understand the impact of varying numbers of NDP-DIMMs, while mitigating the effect of limited computation capability. An increase in NDP-DIMMs enhances both memory size and internal bandwidth. Larger memory capacity facilitates the deployment of more extensive models; for instance, deploying Falcon-40B on Hermes necessitates a minimum of four NDP-DIMMs. Additionally, higher internal bandwidth significantly enhances end-to-end performance, addressing the bandwidth limitations that bottleneck current offloading-based systems. However, once sufficient bandwidth is achieved, further increases in the number of NDP-DIMMs do not proportionally boost throughput. For example, LLaMA2-70B exhibits similar throughput with both 8 and 16 NDP-DIMMs. Once the NDP-DIMMs surpass the GPU in performance, additional NDP-DIMMs do not yield further performance gains.}

% \update{
% \fig~\ref{fig:dimm} shows that LLM throughput improves as the number of NDP-DIMMs increases. We evaluated four different LLM models with a single batch to assess the impact on different numbers of NDP-DIMMs, avoiding the effect of limited computation capacity. More NDP-DIMMs provide larger memory size as well as higher internal bandwidth. Abundant memory size allows the deployment of larger models. For example, deploying Falcon-40B on Hermes needs at least 4 NDP-DIMMs. Furthermore, higher internal bandwidth can effectively boost the end-to-end performance, as the limited bandwidth is the bottleneck of existing offloading-based system. However, when sufficient bandwidth is provided, the end-to-end throughput will not be further improved proportionally with the increasing number of NDP-DIMMs. For instance, LLaMA2-70B shows similar throughput with 8 and 16 NDP-DIMMs. Once the NDP-DIMMs outperform the GPU, adding more NDP-DIMMs no longer impacts performance.
% }

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/GPU_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{Throughput of OPT-13B and OPT-30B with various GPUs, including RTX 4090, RTX 3090 and Tesla T4.}}
    \label{fig:gpu}
\vspace{-0.3cm}
\end{figure}

\subsubsection{\update{Sensitivity analysis of various GPUs}}

\update{\fig~\ref{fig:gpu} illustrates the significant impact of different GPUs on the end-to-end throughput of LLM execution. We have included two additional consumer-grade GPUs, Tesla T4 and RTX 3090, in our evaluation. Specifically, Tesla T4 offers 16GB of graphic memory, 320GB/s memory bandwidth, and 65 tensor TOPS (FP16), whereas RTX 3090 provides almost the same graphic memory and bandwidth as RTX 4090, but with 142 tensor TOPS (FP16). Overall, \name~with RTX 4090 achieves an average throughput improvement of $2.02\times$ and $1.34\times$ compared to \name~with Tesla T4 and RTX 3090, respectively. The data loading cost for RTX 3090 is nearly identical to that of RTX 4090. However, RTX 3090 spends more time on prefill and hot neuron computations due to its weaker computation capability. Tesla T4, with its smaller graphic memory and lower memory bandwidth compared to RTX 3090, is inefficient for data loading. Consequently, the choice of GPU device is crucial for optimizing \name~performance.}


\subsubsection{\update{Design Space Exploration for NDP-DIMMs}}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/gemv_rebuttal.pdf}
    \vspace{-0.3cm}
    \caption{\update{Design Space Exploration for NDP-DIMMs with different number of multipliers in each GEMV unit.}}
    \label{fig:gemv}
\vspace{-0.3cm}
\end{figure}

\update{
\fig~\ref{fig:gemv} highlights the impact of increasing the number of multipliers within a GEMV unit per DIMM on LLM inference performance, especially with larger batch sizes. We varied the number of multipliers within a GEMV unit from 32 to 512, thereby enhancing computation capability by 16$\times$. For OPT-13B with a batch size of 1, performance stabilizes once 64 multipliers are reached, as further computation capability yields minimal gains. In contrast, with a batch size of 16, performance continuously improves with additional multipliers, achieving up to a $3.86\times$ speedup. This difference arises because memory bandwidth limits performance for smaller batch sizes due to lower arithmetic intensity, while computation capability becomes the bottleneck with larger batch sizes. To optimize the balance between hardware overhead and performance across various batch sizes, we selected 256 multipliers within the GEMV unit per DIMM.
}

\subsection{Comparison with High-Performance System}\label{sec:comparison-high-performance}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Fig/comparison.pdf}
    \vspace{-0.3cm}
    \caption{Comparison with TensorRT-LLM on LLaMA2-70B.}
    \label{fig:trt-llm-comparison}
\vspace{-0.3cm}
\end{figure}

% 这里可以考虑一下系统的开销和对应的结果
This section discusses the performance gap between our budget-friendly LLM inference system Hermes and state-of-the-art high-performance serving system TensorRT-LLM~\cite{tensorrt-llm}. We kept the input and output sequence lengths set at 128. To handle LLaMA2-70B with a batch size of 16, TensorRT-LLM requires five NVIDIA A100-40GB-SXM4 GPUs. In contrast, 
\name~operates with only one NVIDIA RTX-4090 GPU and affordable NDP-DIMMs. Figure \ref{fig:trt-llm-comparison} displays the performance comparison between TensorRT-LLM and Hermes. For a batch size of 1, Hermes achieves 79.1\% inference efficiency of TensorRT-LLM. Even at a batch size of 16, Hermes retains 24.4\% inference efficiency of TensorRT-LLM. Despite this, Hermes is far more economical than TensorRT-LLM, which is equipped with 5 NVIDIA A100-40GB-SMX4 GPUs. Specifically, Hermes only costs approximately \$2,500, whereas TensorRT-LLM requires \$50000 to support LLaMA2-70B. Hermes provides efficient and low-budget LLM inference for local deployments. 

% Despite this, \name~is far more economical, costing approximately \$2,500 compared to the \$50,000 needed to build a high-performance system with 5 NVIDIA A100-40GB-SMX4 GPUs, making it highly efficient for local deployments with smaller batch sizes.