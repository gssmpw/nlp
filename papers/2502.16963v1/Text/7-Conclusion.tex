\section{Conclusion}\label{sec:conclusion}


% Our insight is that the activation sparsity within LLMs effectively partitions the billion-scale weight parameters into two distinct parts, denoted as hot and cold neurons. Among them, hot neurons occupy a small amount of parameters but have high computation intensity, whereas cold neurons only require minimal computation.

In this paper, we propose an innovative and affordable inference system, Hermes, that utilizes NDP-DIMMs to enhance both the memory capacity and processing capability of consumer-grade GPUs. We partition the billion-scale weight parameters within LLMs into hot/cold neurons. Specifically, we map hot neurons to computation-efficient but storage-limited consumer-grade GPUs, while offloading cold neurons to storage-ample but computation-limited NDP-DIMMs, to fully leverage their advantages. To further improve the inference efficiency on \name, we propose a lightweight predictor to assist the online partition for hot/cold neurons and adopt window-based online scheduling to achieve load balance across multiple NDP-DIMMs. Compared with existing high-performance inference systems, \name~can achieve competitive inference efficiency with approximately 5\% budget. 