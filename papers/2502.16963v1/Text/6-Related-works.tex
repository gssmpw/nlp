\section{Related Works}\label{sec:related-works}

\subsection{LLM Inference with PIM}\label{sec:related-works-PIM}

% It discusses the placement of computation units within HBM and identifies that GEMV units in the near bank of HBM are the optimal PIM placement for LLMs.

% Given that LLM inference is primarily memory bandwidth-bound, using PIM to accelerate LLM inference is a natural choice. AttAcc!~\cite{park2024attacc} utilizes a hybrid architecture of HBM-PIM and xPU (GPU/TPU), offloading the attention computation to HBM-PIM. It explores pipeline parallelism at the attention head level to further improve the inference efficiency. NeuPIMs~\cite{heo2024neupims} and IANUS~\cite{seo2024ianus} address the compatibility issue between PIM functionality and regular memory access by adopting dual buffers and incorporating additional control units, respectively. They optimize the design of HBM-PIM to support both processing and memory access simultaneously, utilizing PIM and xPU collaboration for LLM inference acceleration. These approaches adopt a fixed offloading strategy, delegating specific operators such as attention to PIM. SpecPIM~\cite{li2024specpim}, on the other hand, targets speculative LLM models with a multi-device architecture, where each device includes a xPU and multiple HBM-PIM chips. SpecPIM formalizes the mapping and scheduling of various models during speculative inference as a design space exploration problem, achieving flexible resource allocation based on actual model requirements. However, these works are all designed for server-grade devices (such as H100) and rely on expensive HBM-PIM for LLM inference acceleration, making them unsuitable for local deployment with limited budget. 

Given that LLM inference is primarily memory bandwidth-bound, accelerating it with processing in memory (PIM) is a natural choice~\cite{li2020hitm,zhai2023star,zhu2023processing}. AttAcc!~\cite{park2024attacc} utilizes a hybrid architecture of HBM-PIM and xPU (GPU/TPU), offloading the attention computation to HBM-PIM. NeuPIMs~\cite{heo2024neupims} and IANUS~\cite{seo2024ianus} address the compatibility issue between PIM functionality and regular memory access by adopting dual buffers and incorporating additional control units, respectively. They optimize the design of HBM-PIM to support both processing and memory access simultaneously, utilizing PIM and xPU collaboration for LLM inference acceleration. SpecPIM~\cite{li2024specpim}, on the other hand, targets speculative LLM models with a multi-device architecture, where each device includes an xPU and multiple HBM-PIM chips. However, these works are all designed for server-grade devices (such as H100) and rely on expensive HBM-PIM for LLM inference acceleration, making them unsuitable for local deployment with a limited budget. 

\subsection{LLM Acceleration with Activation Sparsity}\label{sec:related-works-sparsity}

% The promising activation sparsity in deep learning models motivates researches~\cite{zheng2023pit, cui2023optimizing} to further improve their inference efficiency, especially for LLMs. Deja Vu~\cite{liu2023deja} utilizes the activation sparsity to reduce the memory access on the unified memory of multiple server-grade GPUs. However, it it still requires storing all parameter data in GPU memory, failing to reduce GPU storage overhead, and thus is not suitable for local deployment scenarios. Powerinfer~\cite{song2023powerinfer} introduces a CPU-GPU hybrid system to achieve activation sparsity-based LLM inference. It stores hot neurons in GPU memory and uses GPU tensor cores for the corresponding computations while storing cold neurons in CPU memory and utilizing the CPU as a computing unit. However, the CPU-side memory bandwidth is significantly lower than that in the GPU, making CPU-side computation a bottleneck. Additionally, Powerinfer primarily targets single-batch computation, resulting in suboptimal utilization of GPU computation capability. Overall, existing systems do not fully exploit the advantages of activation sparsity. It is necessary to utilize processing units with high bandwidth and large storage to fully leverage activation sparsity. 

The promising activation sparsity in deep learning models motivates researchers~\cite{zheng2023pit, cui2023optimizing,liu2024drift} to further improve their inference efficiency, especially for LLMs. Deja Vu~\cite{liu2023deja} utilizes the activation sparsity to reduce the memory access on the unified memory of multiple server-grade GPUs. However, it still requires storing all parameter data in GPU memory, failing to reduce GPU storage overhead. Powerinfer~\cite{song2023powerinfer} introduces a CPU-GPU hybrid system to achieve activation sparsity-based LLM inference. It stores hot neurons in GPU memory and uses GPU tensor cores for the corresponding computations while offloading cold neurons in CPU memory and utilizing the CPU as a computing unit. However, the CPU-side memory bandwidth is significantly lower than that in the GPU, making CPU-side computation a bottleneck. Overall, existing systems do not fully exploit the advantages of activation sparsity.