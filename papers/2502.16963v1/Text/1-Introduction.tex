
\section{Introduction}

Large Language Models (LLMs) have gained significant importance and widespread attention. Open-source models like OPT, LLaMA, and Qwen series~\cite{zhang2022opt, qwen2, touvron2023llama2}, as well as proprietary models such as GPT-4 and Claude~\cite{achiam2023gpt, claude3}, exhibit remarkable performance in a variety of tasks including code generation~\cite{chen2021evaluating, Copilot}, machine translation~\cite{le2023bloom, jiang2023mistral}, and chatbots~\cite{chatgpt2023, bard2023}, etc. Nevertheless, extremely powerful LLMs with billions of parameters often require server-grade GPUs with large-capacity HBMs, making them cost-prohibitive for many applications. For example, deploying LLaMA2-70B locally using TensorRT-LLM~\cite{tensorrt-llm} requires five NVIDIA A100-40GB-SXM4 GPUs, totaling over \$50,000.

To investigate the development of cost-effective LLM inference systems, researchers have shifted their focus to more budget-friendly hardware, such as consumer-grade GPUs. Despite these GPUs' significant computation capability, such as 1321 Tensor TOPS in NVIDIA RTX 4090, they suffer from limited graphic memory size. This limitation renders them unsuitable for deploying LLMs with billions of parameters. To this end, researchers use offloading strategies~\cite{jain2022hugging, rasley2020deepspeed, sheng2023flexgen}, transferring large portions of LLM parameters to DIMM (Dual-Inline Memory Module)-based host memory. As depicted in \fig \ref{fig:offloading}a, existing offloading solutions view host memory as the augmented memory space for GPUs to enable LLMs, and parameters stored in host memory need to be accessed via PCIe. This results in substantial data transfers on PCIe. However, due to more than $15 \times$ bandwidth gap between the PCIe and the internal GPU memory, about 99\% of the overall LLM runtime in these offloading solutions is attributed to the data transfers on PCIe. 

% TODO
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Fig/intro.pdf}
    \vspace{-0.3cm}
    \caption{(a) Existing offloading solutions view host memory as the augmented memory, but cause burdensome data transfer on PCIe. (b) Partitioning the weight matrix in each layer, and utilizing NDP-DIMMs to handle poor computation intensity parts, only introduces negligible data transfer.}
    \label{fig:offloading}
\vspace{-0.3cm}
\end{figure}

It is essential to minimize the data loading for weight parameters to ease the burden on PCIe. Thus, existing works~\cite{liu2023deja, song2023powerinfer, xue2024powerinfer} utilize the activation sparsity to reduce the required data loading.
Since the activation functions such as ReLU in LLMs can zero out specific activation values, the corresponding parameters that are expected to be computed with these zero activations do not need to be loaded either, as illustrated in \fig \ref{fig:pruning-network}.  According to the activation sparsity, weight parameters in LLMs can be further categorized into hot and cold neurons\footnote{This paper defines a neuron as a specific row/column in a weight matrix, and neurons will not be activated when associated with zero activations.}. Our evaluation indicates that around 20\% of neurons, referred to as ``hot neurons'', are responsible for 80\% of the computations, whereas the remaining 80\% of neurons, known as ``cold neurons'', account for only 20\% of the computations. This suggests that the computation intensity of hot neurons is $16 \times$ higher than that of cold neurons. Consequently, it is natural to store hot neurons in GPU memory and offload cold neurons to host memory to effectively mitigate data loading costs~\cite{liu2023deja}. Despite these optimizations, data transfers on PCIe still dominate the inference procedure, accounting for 90\% of the total inference latency of OPT-66B, as they constitute a large part of the total LLM work-set.

According to our observation, the cold neurons offloaded on host memory require large storage but have poor computation intensity. As a result, we are motivated to utilize near-data processing (NDP) units based on DRAM DIMMs to provide the least-required computation capability for cold neurons to avoid their movement. As illustrated in \fig \ref{fig:offloading}b, we can leverage the NDP units and GPU cores to conduct computations for cold and hot neurons, respectively. As the computation results only take a few KBs, the data transfer cost in step \circled{2} is negligible. Note that we use NDP-DIMMs, instead of high-performance but expensive alternatives such as HBM-PIM and AiM~\cite{park2024attacc, heo2024neupims, gao2017tetris, cong2017aim}, as the augmented memory to build the budget-friendly system for local deployment. 

% and render the offloading solutions feasible
% Thus, it becomes essential to minimize the data loaded from the host memory to ease the burden on the PCIe bus. Previous works~\cite{zheng2024learn, song2024prosparse, mirzadeh2023relu} have pointed out that not all the weight parameters need to be loaded. As the activation functions such as ReLU in LLMs can zero out specific activation values, the corresponding weight parameters that are expected to be computed with these zero activations do not need to be loaded either, as illustrated in \fig \ref{fig:pruning-network}. For example, Deja Vu~\cite{liu2023deja} utilizes an MLP-based predictor to prefetch the required neurons\footnote{This paper defines a neuron as a specific row/column in a weight matrix, and neurons will not be activated when associated with zero activations.} only to GPU memory during LLM inference, effectively reducing the PCIe data transferring. Despite these optimizations, PCIe data transfers still dominate the inference process, accounting for over 95\% of the total inference time. The reason is that only moving the activated neurons to GPU memory still induces high pressure on the PCIe bus. 

% 整体感觉这一段的表述问题挺大，过于啰嗦，后面好好讨论一下这一段
% According to our discovery, weight parameters in LLMs can also be categorized into hot and cold neurons. Our evaluation indicates that around 20\% of neurons, referred to as ``hot neurons,'' are responsible for 80\% of the computations, whereas the remaining 80\% of neurons, known as ``cold neurons,'' account for only 20\% of the computations. This suggests that the computation intensity of hot neurons is $16 \times$ higher than that of cold neurons. However, in previous methods, the infrequently activated cold neurons, unlike inactive sparse neurons, also need to be moved to the GPU memory since they contribute to computation anyway. Transferring these rarely activated cold neurons to GPU memory via PCIe results in a $5 \times$ decrease in inference performance, as they constitute a large part of the total LLM work-set. This motivates us to find a method to keep them in DRAM DIMMs and place hot neurons solely into GPU memory. Although cold neurons require much less computational intensity compared to hot neurons, they require substantial memory space and high memory bandwidth, making them a perfect workload for NDP architectures. Therefore, this work aims to utilize near-data processing (NDP) units based on DRAM DIMMs to provide the least-required computation capability for cold neurons to avoid their movement. Since the proposed offloading NDP solution still needs to be compatible with the memory interface of the CPU host, NDP-DIMM, which functions as both DDR memory and a simple near-data processor, is more feasible for CPU-side memory than costly high-performance alternatives such as HBM-PIM and AiM~\cite{park2024attacc, heo2024neupims, gao2017tetris, cong2017aim}.

Yet, attaining high-performance but affordable LLM inference using a basic NDP-DIMM enhanced GPU system is challenging due to the limited computational resources in NDP-DIMMs. Two primary challenges must be resolved:

% hot/cold 的划分， 首先是为什么要划分，然后是划分的挑战，挑战主要是 首先找到最优划分，并且online 预测并调整 划分 

%  of the proposed offloading strategy
\textbf{1. Deciding the optimal neuron partition. }
First, the criteria for dividing hot and cold neurons between GPU and NDP-DIMMs are crucial for computational efficiency. For instance, if only the least active neurons are predicted as ``hot", this will stress the limited GPU memory size. Conversely, allocating frequently activated neurons to the ``cold" region will burden the computation-limited NDP-DIMMs with excessive computation. Therefore, determining the optimal neuron partition strategy is essential. However, due to the input-specific nature, the hot/cold neuron partition cannot be completely predetermined. It necessitates an accurate but lightweight online prediction to achieve real-time adjustment for hot/cold neuron partition with minimal migration cost. 

% Furthermore, since the ideal partition of ``cold" and ``hot" neurons depends on both the model and the input context, it needs to be dynamically predicted without degrading inference performance, necessitating an accurate but lightweight online prediction. Moreover, real-time adjustment of the hot and cold data partition is required to accommodate the rapid changes in context, which calls for an efficient online migration policy for NDP-DIMM and GPU memory.

%  and does not have the infrastructure to communicate with other DNP engines
\textbf{2. Exploiting the limited computation capability of multiple NDP-DIMMs. }
% While the low computation intensity cold neurons are allocated to the NDP-DIMMs, they still create some computational workload for the NDP-DIMM, which has very limited computation capacity, e.g., hundreds of GFLOPS \cite{asghari2016chameleon,zhou2023dimm,devaux2019true,kim2021aquabolt}, in contrast to the hundreds of TFLOPS of GPUs.
In contrast to the provided hundreds of TFLOPS of a single GPU, the computation capability is constrained to hundreds of GFLOPS~\cite{asghari2016chameleon,zhou2023dimm,devaux2019true,kim2021aquabolt} on NDP-DIMMs. Consequently, even are used to process the infrequently activated neurons, NDP-DIMMs still bottleneck the inference performance. Thus, it is crucial to fully unleash NDP units for efficient computing. Specifically, as we need to use multiple DIMMs together to support the large-scale LLMs, computational loads on each NDP-DIMM are expected to be balanced. However, due to the dynamics of activated neurons, some NDP-DIMMs are overburdened while others remain underutilized during inference. Therefore, the main challenge is to achieve online scheduling for computational load balance among NDP-DIMMs.


% Designing an efficient scheduler within DIMMs to fully unleash NDP engines is crucial. This utility becomes especially important when multiple gigabyte-scale DIMMs must be used together to support LLMs. In such a scenario, each NDP can only process the data stored on its respective DIMM. Consequently, because cold neurons are activated sporadically and spread randomly across different DIMMs, this may lead to the phenomenon that some NDP-DIMMs are overburdened while others remain underutilized. This imbalance in the use of NDP's limited computational resources may hinder the processing efficiency of cold neurons. Therefore, the main challenge is to achieve computational load balancing among NDP-DIMMs.

% 这一段还需要重新改一下，需要和 challenges 分别对应上
To address the aforementioned challenges, we introduce \name, an innovative and budget-friendly inference system that uses NDP-DIMMs to enhance both the memory capacity and processing capability of a single consumer-grade GPU. On one hand, we address the optimal neuron partition in two phases. First, we formalize the problem as an integer linear programming (ILP) issue and employ an offline solver to help determine the optimal partition based on the profiled data. Then, utilizing the distinct distribution patterns of hot and cold neurons, we develop a lightweight online predictor to manage online cold/hot neuron partition. This approach bypasses the expensive MLP-based predictor used in prior studies~\cite{song2024prosparse, song2024turbo, xue2024powerinfer}, enabling real-time migration of hot and cold neurons. On the other hand, to address load imbalance issues among multiple NDP-DIMMs, we exploit the token-wise similarity inherent in LLM. In detail, we propose a window-based online scheduling strategy, which utilizes the neuron activity of adjacent tokens to online remap cold neurons across multiple NDP-DIMMs, achieving load balance.

% In detail, we propose a window-based online scheduling strategy to online remap cold neurons onto appropriate DIMMs. This strategy considers a group of adjacent tokens as a historical window, refers to the access patterns shown within this window, and employs a greedy algorithm to online remap cold neurons across multiple NDP-DIMMs, thereby achieving load balance.

In a nutshell, our contributions are as follows:
% contributions 也要随之而修改
\begin{enumerate}
    \item We propose a novel system, \name, which takes advantage of the cold/hot distribution in LLM inference and augments consumer-grade GPU with NDP-DIMMs to achieve high-performance and economic LLM inference.
    \item We propose a two-step solution to achieve the optimal cold/hot neuron partition for \name. We first formulate an ILP problem and utilize an offline solver to find the original optimal partition, and further implement a lightweight online predictor to guide the online migration of hot and cold neurons during LLM inference. 
    \item We develop a window-based online scheduling strategy to achieve load balance among multiple computation-limited NDP-DIMMs, effectively improving the overall hardware utilization.
    \item Compared to existing offloading-based inference systems FlexGen and Deja Vu, \name~achieves a speedup of $148.98 \times$ and $75.24 \times$, respectively. 
\end{enumerate}
