\section{Motivations \& Challenges}

\subsection{Why NDP-DIMM Enhanced GPU?}
% 这里再跟 powerinfer 比，以及说明 CPU 的扩展的问题。

% 1. offloading 不行 -> 哪怕有activation sparsity, 也不行；但是我们进一步分析 activation sparsity 的特点，说明 hot/cold 的划分特点； 根据特点，选择硬件。 (low cost ?)

% 首先要说明为什么选择 NDP-DIMM
Offloading is essential for LLM inference on low-budget systems with a single consumer-grade GPU. However, as noted in Section \ref{sec:background-offloading}, even utilizing activation sparsity to reduce weight parameter access, the PCIe bandwidth remains the bottleneck. Thus, costly data transfers between extended memory and GPU must be minimized. However, simply offloading the corresponding computation of cold neurons on the host CPU~\cite{llama.cpp, song2023powerinfer} can only achieve a limited performance improvement, \update{as the host CPU can only access DRAM with limited improved bandwidth than PCIe (e.g., 89.6 GB/s vs. 64 GB/s)}. To this end, we choose to employ multiple NDP-DIMMs as the extended memory, as they offer comparable bandwidth and larger storage capacity than a single consumer-grade GPU. Need to mention that as a budget-friendly host memory solution, we do not consider high-performance but expensive HBM-PIM and AiM~\cite{cong2017aim, park2024attacc} in this study. Given the limited computation capability, only utilizing the processing units in NDP-DIMMs cannot boost the inference efficiency~\cite{wu2024pim}. Consequently, we are motivated to use NDP-DIMMs to enhance GPU for efficient LLM inference.

% neurons 划分来协助相应的计算
Our observation indicates that the activation sparsity within LLMs effectively partitions weight parameters into two distinct regions, which are ideally suited to consumer-grade GPU and NDP-DIMMs, respectively. Specifically, activation sparsity in LLMs follows a power-law distribution~\cite{xue2024powerinfer, song2023powerinfer}. About 20\% of neurons (\textit{hot neurons}) account for 80\% of computations, while 80\% (\textit{cold neurons}) handle only 20\%. Hot neurons, with $16 \times$ higher computation intensity, fit GPU memory, while cold neurons suit NDP-DIMMs. During inference, GPU can provide high computation capability for hot neurons and NDP-DIMMs enable the cold neurons computation in memory.

\subsection{Necessity of Hot/Cold Neuron Partition} \label{sec:similar}

% 1. 首先需要说明为什么要进行 hot/cold neurons partition, 然后说明对应的 challenges，以及目前的预测的缺陷
% 根据我们的评估，在LLaMA-70B 中，有约 52% 的 hot neurons 在推理过程中会发生变化，导致the predetermined hot/cold neurons partition 相比于 oracle 的划分产生 1.63x 的performance 下降。 
Hot/cold neuron partition impacts the computational load on GPU/NDP-DIMMs, affecting the inference performance of the heterogeneous system. Due to the input-specific nature of activation sparsity, solely relying on the offline partition is insufficient. Our evaluation on LLaMA2-70B reveals significant dynamics in when the neuron will be activated (hereafter, neuron activity patterns) during inference. Approximately 52\% of the initialized hot neurons exhibit varied activity during inference. This variability in neuron behavior results in suboptimal performance with a fixed hot/cold partition, causing a $1.63\times$ degradation compared to an oracle (the theoretically optimal partition) scheme. Thus, we must dynamically predict and adjust the hot/cold neuron partition.

However, typical MLP-based predictors~\cite{liu2023deja, song2024prosparse, zhang2024relu, mirzadeh2023relu} for activation sparsity in LLMs are costly. For example, predicting the activated neurons in LLaMA-7B needs per-layer MLP-based predictors, requiring an extra 2GB storage and inducing 10\%-25\% inference runtime. Fortunately, the inherent locality of activation sparsity leads us to design a lightweight and accurate predictor for efficient online partition adjustments. To be specific, we found that activation sparsity in LLM inference shows considerable token-wise similarity and layer-wise correlation, worth exploiting.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{Fig/prediction-motivation.pdf}
    \vspace{-0.3cm}
   \caption{Distribution patterns for activation sparsity. (a) \update{The adjacent tokens enjoy high similarity on activated neurons for various models and datasets.} (b) The activated neurons between consecutive layers are highly correlated.}
    \label{fig:similarity}
\vspace{-0.3cm}
\end{figure}

\subsubsection{Token-wise Similarity}
% As mentioned in the previous section, the dynamically sparse neurons exhibit a power-law distribution, resulting in high locality among activated neurons. This indicates that we can divide all neurons into hot neurons and cold neurons. However, this division is not fixed and requires dynamic adjustment. 
We analyzed the similarity between tokens to explore the distribution characteristics of activation sparsity. \update{As shown in \fig \ref{fig:similarity}a, we evaluate the token-wise similarity for LLaMA-13B and Falcon-40B with multiple widely adopted datasets, including COPA~\cite{roemmele2011choice}, Wikitext2~\cite{merity2016pointer} and PIQA~\cite{bisk2020piqa}. As one can notice, the adjacent tokens have a higher distribution similarity than distant tokens. Specifically, the similarity between adjacent tokens exceeds 90\% (95\% for Falcon-40B), but drops to 70\% once the tokens' distance exceeds 10.} This indicates that in context, adjacent tokens often express similar meanings, leading to high similarity in their activity distribution. Additionally, we observe that when the distance between tokens exceeds 25, the distribution similarity almost no longer decreases, indicating that beyond a certain window size, the semantic correlation becomes weak and has less impact on the overall distribution.

\subsubsection{Layer-wise Correlation} 
We further observed that the distribution of activated neurons in two consecutive layers is highly correlated. As shown in \fig \ref{fig:similarity}b, when the 6th neuron in layer-30 of LLaMA-13B is activated, the probability of neurons 0 and 5 being activated in layer-31 exceeds 90\%. This suggests that we can use the results of the preceding layer to predict the distribution of activated neurons in the current layer.

Overall, the token-wise similarity and layer-wise correlation motivate us to design a lightweight online predictor based on historical activation information. According to the prediction results, we can online adjust the hot/cold neurons partition to effectively exploit the processing advantages of the consumer-grade GPU and NDP-DIMMs, respectively.

% to predict whether to compute neurons of the current token in a specific layer. 
% Compared to costly MLP predictors, this historical activation-based strategy introduces negligible overhead.

\subsection{Load Imbalance across Multiple NDP-DIMMs}
% When initially distributing cold neurons across multiple DIMMs, we analyze past computational patterns to estimate the potential workload of these neurons. This allows us to distribute their corresponding weights across DIMMs in a manner that aims to evenly distribute the expected computational load among them. However, due to the inherently stochastic nature of cold neuron computations, relying on a static weight distribution strategy can result in imbalanced workload distribution across DIMMs.
% imbalance的来源+数据imbanlance有多大 -> 会很影响性能，所以我们要去做load balance的事情，DIMM间也需要有个简易的数据通路
Due to the storage limitation of a single DIMM, multiple DIMMs are required to store all the neurons (weight parameters) in LLM. Specifically, one DIMM only stores portions of the neurons and the corresponding processing unit can only directly assess neurons in the DIMM with high internal bandwidth. However, due to the input-specific nature of activation sparsity, the computational load on each NDP-DIMM can be diverse. For example, when fixing the cold neuron distribution on multiple DIMMs for LLaMA-13B, the most overloaded NDP-DIMM will have 1.2-2.5$\times$ more computational load than others.
% Furthermore, given the limited computational capacity of individual DIMMs, an overloaded DIMM can potentially bottleneck the entire system.

%是不是也可以在这里说，使用center buffer的DIMM，通过在buffer中添加很少的单元就可以实现DIMM间高效的通信而无需CPU参与，且每个DIMM中的PU都可以获得对应DIMM上的所有数据，所以我们选择了center-buffer 的NDP-DIMM
Therefore, we need an online scheduling strategy to remap the cold neuron across DIMMs to achieve load balance. Meanwhile, an efficient data transmission pathway among DIMMs is essential to help adjust the neuron placement. By optimizing neuron computation scheduling, we can minimize data transfers across NDP-DIMMs while ensuring balanced computational loads across DIMMs. This ensures that all parts of the system can maximize their performance.

In summary, the NDP-DIMM enhanced GPU approach effectively addresses the substantial data transfer overhead in offloading processes, providing a promising solution to improve LLM inference efficiency by leveraging the activation sparsity patterns inherent in LLMs.
