**Response to The Meta Review**

We express our gratitude for the professional and constructive reviews provided. Here, we provide the revision information for the paper, with a particular emphasis on the sections closely aligned with your reviews. The revised submissions are provided in the attached PDF document, 'hpca-381.pdf'.

We provide more details of FlexGen in the second paragraph of Section II.C to make the paper self-contained. Specifically, we described the zig-zag scheduling strategy in FlexGen, which is used to overlap the computation of one layer and the corresponding weight loading for the next layer. We also highlighted that the scheduling strategy in FlexGen is quite useful for prefill, but not for the token generation phase.

Furthermore, we have also included the following revised details to address each reviewer's concerns. 

**Response to the Individual Reviewer**

**Reviewer-A:**

**A.1 Online remapping cost.**

We illustrated the remapping process in Fig. 6a. Online remapping will not be performed concurrently with NDP operations in our current design. We complete the remapping process during the projection computation carried out by the GPU when the NDP-DIMM remains idle during that time.

**A.2 The programming implications of splitting hot and cold pages on heterogeneous hardware.**

We newly included a programming interface in Section-IV.A.1 and a programming example in Section-IV.A.2, to illustrate the programming details. Specifically, we use a standard programming model, PIM-SYCL [1], to compile the heterogeneous platform. Unified memory programming [2,3] allows data to be transferred implicitly between devices, enabling cooperative processing between the GPU and NDP. Additionally, we provide a set of extra commands, such as MAC and softmax, for the NDP-DIMM to support various operators in LLMs. Taking GEMV computation as an example, the NDP-DIMM computations can be invoked through the memory command interface by sending a series of MAC commands. On the GPU side, the corresponding computation tasks are triggered through APIs like cudaLaunchKernel.

**Reviewer-B:**

**B.1 The dataset used for evaluation.**

We described the dataset we used in Section V.A.3. We use two typical datasets, ChatGPT prompts and Alpaca, to evaluate the end-to-end performance of Hermes. 

**B.2 How is the scheduler implemented? Is it software or hardware? how is the remapping scheme implemented?**

We added the description in Section-IV.A.1 that both the scheduler and the remapping scheme are implemented in software on the host.

**B.3 The internal DIMM bandwidth.**

We detailed the configuration of NDP-DIMM in TABLE II. The internal bandwidth increases with the number of DIMMs per channel and ranks per DIMM [1]. Consequently, with one DIMM per channel, NDP-DIMM can also provide 4 $\times$ higher internal bandwidth following the configuration in Table II, achieving an average 2.94 $\times$ end-to-end speedup for OPT-13B than running LLM on host CPU. In our evaluation, each channel has two DIMMs, which ensures 8 $\times$ bandwidth improvement. Experimental results indicate that our design effectively achieves 4.89 $\times$ end-to-end performance improvement.

**B.4 Why formulated neuron placement as an integer linear programming problem?**

The varying sparsity in different layers [4] indicates that it's sub-optimal to place the same number of neurons across all layers. Consequently, we formulate the neuron placement as an ILP problem to acquire a globally optimal solution for overall performance. Our formulation considers the overheads of each computing device (including both NDP-DIMMs and GPU), aiming to identify the best allocation scheme under the constraints rather than simply placing the same number of neurons in the GPU across all layers.

**B.5 How many samples should be used to obtain representative values for the $f_i$ parameters?**

We added that we used 128 samples from popular datasets C4 to obtain representative values for the $f_i$ parameters in Section-IV.B.

**Reviewer-C:**

**C.1 More explanation for the size of the neuron state table.**

We clarified the size of the neuron state table in Section IV.C. The storage overhead of these tables is usually less than 1MB. For example, LLaMA-7B consists of 32 layers, with each layer containing 4K neurons in the self-attention block and 10.5K neurons in the MLP block. In our implementation, only 4 bits of data are used to record the state of each neuron. As a result, the neuron state table for LLaMA-7B requires only 232 KB of storage.

**Reviewer-D:**

**D.1 How cold and hot neurons are differentiated lacks sufficient detail.**

We formulated the definition of cold and hot neurons in Section-IV.B. The hot/cold partition depends on the activity of each neuron and the graphic memory size. The most frequently activated neurons mapped on the GPU side, are viewed as hot neurons, while others are cold neurons. Furthermore, the ratio of hot neurons depends on the available graphic memory size and model size. For example, when running LLaMA2-70B on Hermes with RTX 4090 (24GB), only approximately 10% of the whole neurons can be partitioned to hot neurons.

**Reviewer-E:**

**E.1 What is the hot/cold accuracy of offline neuron mapping, and how much can it be improved through online prediction and the load imbalance of the NDP-DIMM?**

As indicated in Section III.B, approximately 52% of the initialized hot neurons exhibit varied activity, making the accuracy of offline neuron mapping less than 60%. Fortunately, our proposed online prediction improves this accuracy to about 98%. Fig.13 demonstrates the improvements achieved through online remapping. By using the proposed techniques, Hermes effectively reduces the overhead gap of different NDP-DIMMs from an average of 1.8 to 1.1. We also added the additional sensitivity studies in Section-V.E. The results indicated that each computing device of our system exhibits high utilization, demonstrating the effectiveness of the proposed techniques.


[1] Samsung PIM/PNM for Transformer based AI, HotChips'23. 

[2] IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System, ASPLOS'24.

[3] NVIDIA Unified Memory Programming, [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd)

[4] ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models, ICLR'24. 