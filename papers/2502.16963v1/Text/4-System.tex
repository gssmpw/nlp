\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{Fig/overview.pdf}
    \vspace{-0.3cm}
    \caption{Overview of our proposed Hermes System. (a) Hermes augments GPU memory with NDP-DIMMs, and utilizes a scheduler to control the inference workflow. (b) Multiple NDP-DIMMs are connected to support LLM inference and inter-DIMM communication. }
    \label{fig:system-overview}
\vspace{-0.3cm}
\end{figure*}

\section{\name~System}\label{sec:hermes-system}
\subsection{System \& Architecture Overview}

\subsubsection{Architecture}

\fig\ref{fig:system-overview}a illustrates the overview of \name. \name~augments consumer-grade GPU with NDP-DIMMs to achieve low-budget, high-performance inference system for LLMs. 

% We elaborate on the architecture detail of each component as follows.

\textbf{Consumer-grade GPU:} For LLM inference, we only use one accessible and budget-friendly consumer-grade GPU. Despite limited graphic memory, it has ample computing units, like tensor cores, for high-performance parallel processing. It also features high-speed GDDR memory with superior bandwidth. For instance, the NVIDIA RTX 4090 with 24GB GDDR6 provides 82.6 TFLOPS, 1321 Tensor TOPS, and 936 GB/s bandwidth, making it suitable for LLM inference. Hermes uses a single GPU to efficiently execute hot neurons.
%[726] 不确定下面这句还要不要 : liu 暂时觉得可以不要，后面会主要说workflow，
% Given the abundant computational power of the GPU and the high-speed bandwidth but limited storage space of GDDR memory, during inference, we load only the hot neurons into GDDR and utilize the GPU's tensor cores to efficiently compute the hot neurons in parallel.

\textbf{NDP-DIMM:}
Given that cold neurons are randomly activated, all data stored on each DIMM should be accessible to its own NDP units. \update{Meanwhile, DIMM is required to support the normal data access from GPU for hot neuron transmission.} Therefore, as illustrated in \fig \ref{fig:system-overview}b, we have chosen the center buffer-based NDP-DIMM design\cite{alian2018application,cong2017aim,ke2020recnmp,kwon2019tensordimm}, which allows the processing unit to access all data in its own DIMM. 
\update{The center buffer-based NDP DIMM design also complies with the normal memory access as the newly added units will not influence the memory access function supported by the local memory controller \cite{alian2018application, ke2020recnmp}.}
Here, we detail the microarchitecture of our NDP-DIMM design. To facilitate typical operations in LLMs and potential inter-DIMM data moving, each NDP-DIMM is equipped with GEMV units, activation units, and DIMM-links~\cite{zhou2023dimm}.

\textit{GEMV Unit}: 
The GEMV unit reads data from the DRAM cell and the center buffer, performing the GEMV computation associated with cold neurons. 
\update{To support batched inference and fully utilize the bandwidth achievable within the DIMM center buffer, each GEMV unit contains 256 multipliers.}
Each multiplier is responsible for 128-bit multiplication \update{in a typical bit-serial manner~\cite{devaux2019true}}, a reduction tree-based accumulator, and a 256 KB buffer. During computation, each multiplier computes eight FP16 values simultaneously, and the accumulator is responsible for the addition of partial sums with data dependencies. The buffer stores the intermediate result generated by LLM layers. 

% \todo{add details on why GEMV design, partly done}
% Q: do we need to add some data to support our design, like the analysis of NDP-DIMM accessible data amount in one cycle and the batched inference requirement}

\textit{Activation Unit}: The activation unit is designed to support the necessary non-linear functions, such as softmax and ReLU operation for LLM inference. 
This unit is composed of 256 FP16 exponentiation units, 256 FP16 addition units, and 256 FP16 multiplication units, in addition to a comparator tree, an adder tree, and a divider. 

% The setup of the activation units is consistent with the throughput needs of GEMV units. Moreover, the activation unit repurposes the comparator tree to support both softmax and other activation functions.
% By incorporating an activation unit in the central buffer, we can reduce unnecessary data transfers. For example, softmax requires the `score' output, and having the activation unit and score computation unit both in DIMM allows direct data access from the same center buffer, avoiding additional data transfer to the GPU.

\textit{DIMM-link}: 
Due to the input-specific nature of the activated neurons, it is necessary to adjust the neuron mapping in multiple NDP-DIMMs to further ensure the load balance of computation in the DIMMs. Therefore, we adopt DIMM-link~\cite{zhou2023dimm} to achieve inter-DIMM communication with a bandwidth of 25 GB/s. Each DIMM-link employs bidirectional external data links between DIMMs, facilitating efficient point-to-point data transfers. The DIMM-link controller and bridge enable high-speed neuron redistribution between DIMMs.
\update{Compared to relying on the host for inter-DIMM data movement, using DIMM links provides over a 62$\times$ speedup for data transfer with negligible hardware overhead.
For example, when running OPT-66B, the introduction of DIMM-link effectively reduces the migration overhead for cold neurons from 5.3\% of total time to below 0.2\% .}
% For example, DIMM0 and DIMM1 store weights for N channels, but if no computations are needed for DIMM0 and x channels need computation in DIMM1, we can exchange x/2 channels' weights between DIMM0 and DIMM1 to balance the load, thereby reducing computation latency.

\textbf{Scheduler}:
During LLM inference, the scheduler in the host CPU redistributes neuron computation tasks to the GPU and NDP-DIMMs. The scheduler primarily comprises two components: a lightweight predictor and a neuron mapper, \update{which are both implemented by software}. In addition, the scheduler includes a monitor that gathers runtime information to assist the predictor and an instruction queue that triggers instructions for the GPU and NDP-DIMMs. With the help of the monitor, the lightweight predictor leverages token-wise similarity and layer-wise correlation patterns to accurately predict neuron activity. Based on the prediction results, the neuron mapper assigns hot and cold neurons to DIMMs and GPU memory, respectively, and it also dynamically adjusts the neurons' placement to ensure efficient inference on both the GPU and NDP-DIMMs. The subsequent sections will provide detailed descriptions of these two components. 

\textbf{\update{Programming Interface}}:
\update{We use a standard programming model, PIM-SYCL \cite{kim2023samsung}, to compile the heterogeneous platform. Unified memory programming \cite{zhao2024pim, nvidia_unified_memory} allows data to be transferred implicitly between heterogeneous memory devices, enabling cooperative processing on GPU and NDP-DIMMs. Additionally, \name~provides a set of extra NDP commands, such as MAC and softmax, to support various operators in LLMs. Taking GEMV computation as an example, the NDP-DIMM computations can be invoked through the memory command interface by sending a series of MAC commands. On the GPU side, the corresponding computations are triggered through APIs like cudaLaunchKernel. }
 % \update{For instance, the instruction queue issues the GEMV operator on GPU using APIs like cudaLaunchKernel. Conversely, an additional GEMV instruction, which contains the MAC command with the data address, is defined to support the GEMV operator on the NDP-DIMM side. The instruction queue can invoke the corresponding computation through the memory command interface~\cite{cong2017aim, park2024attacc, heo2024neupims}.}


% \update{Once the scheduler determines the computation tasks assigned to the GPU and NDP-DIMM, it invokes the corresponding instructions for each device. For instance, in the case of a GEMV computation, the scheduler will issue a CUDA instruction when the GPU is required. Conversely, when leveraging the NDP-DIMM, it sends the NDP computation instruction, which contains the MAC command with the data address, through the memory command interface.}
% 感觉上面这个programming interface感觉确实可要可不要？如果说的话，还得说我们在DIMM里面的controller里添加了对于计算指令解析的通路。

% 流水线，两段，上面总结，下面详细说MLP（参考ATTACC
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Fig/workflow.pdf}
    \vspace{-0.3cm}
    \caption{Workflow of Hermes system (a) The whole workflow of LLMs inference on the \name~system. (b) Illustration of computation process for FC layers with activation sparsity. The block with a number in the Mem. means one neuron's weight.}
    \label{fig:workflow}
\vspace{-0.3cm}
\end{figure}

\subsubsection{Workflow}
% During this stage, the KV matrices are transferred to the NDP-DIMMs as soon as they are created.

The workflow for LLM inference within the \name~system is depicted in \fig \ref{fig:workflow}a. \update{Given the significant computational demands, the entire prompting stage is processed on the GPU, adhering to a traditional offloading strategy~\cite{sheng2023flexgen}.} During this stage, the host scheduler records neuron activity for future scheduling optimization. Upon completing the prompting stage, only the selected hot neurons are loaded back into GPU memory. The offline partition of hot and cold neurons will be further detailed in Section \ref{sec:offline}.
In the token generation stage, for each transformer layer, the QKV generation is collaboratively completed by GPU and NDP-DIMMs. The output of QKV generation will be collected in the NDP-DIMMs for further attention computation. 
The memory bandwidth-intensive nature of attention computation~\cite{yu2022orca, park2024attacc} makes it ideal for execution on NDP-DIMMs, which benefit from the abundant internal bandwidth. Additionally, transferring attention computation to NDP-DIMMs helps save the limited GPU memory by eliminating the need for storing KV cache.
Since the projection layer cannot utilize the activation sparsity, it is handled solely by the computation-efficient GPU. During the projection computation, as the DIMMs are entirely idle, the host takes advantage of this period to dynamically reconfigure the hot/cold partitions and redistribute neurons across DIMMs based on the prediction results, which will be detailed in the \sec \ref{sec:partition-design} and \ref{sec:cold-neuron-mapping}. Then, similar to the QKV generation, MLP is offloaded to both GPU and NDP-DIMMs. Finally, the output of each transformer layer is reduced in the NDP-DIMMs. 

% Once the result of attention is obtained, the projection step requires substantial computational resources, since it lacks the feature of sparsity of activation. To avoid extra synchronization overhead from concurrent execution on both GPU and NDP-DIMM, the projection computation is handled solely by the GPU.


% The Par denotes the predetermined or offline profiled parameters. The Var denotes the variables that need to be solved.

\fig \ref{fig:workflow}b illustrates the computation process for FC layers (for both QKV generation and MLP block) with activation sparsity. Specifically, it includes three steps.
After completing the related prediction, the host CPU determines the computation allocation for both the GPU and NDP-DIMMs based on the location of the activated neurons.
\update{Once the neuron mapping is determined, the host CPU invokes APIs for both the GPU and NDP-DIMMs to load data and perform computation. For example, the host CPU uses ``cudaLaunchKernel" to launch GPU kernels for GEMM and GEMV operations. To ensure correctness, the host CPU inserts barriers for the GPU and NDP-DIMMs to synchronize their computations. Once the DIMMs and GPU complete their computations, a merge kernel is invoked on the NDP-DIMMs side to gather the results from both sources.} This method is advantageous for two reasons. Firstly, as the GPU generally finishes computation tasks more quickly owing to its superior computation capability, the latency in transferring data from the GPU to DIMMs can be hidden by the DIMMs' computation, thus not penalizing the overall system runtime. Secondly, with the attention computation occurring on NDP-DIMMs, merging the QKV generation outcomes on the NDP-DIMMs side minimizes the additional data transfer overhead.
% In the next sections, the important components of Hermes will be introduced in detail.

\begin{table}
\vspace{-0.3cm}
\caption{Terminology for the offline partition solver. } 
\label{tab:terminology}
\scriptsize{
\centering
\begin{tabular}{c|p{7.2cm}}
\hline
% \textbf{Symbol} & \textbf{Description} \\[1ex]
% \hline
% \Xhline{3\arrayrulewidth}
\rowcolor{black!10}
\multicolumn{2}{c}{\textbf{\textit{Parameters - predetermined or offline profiled}} }\\
\hline
$\mathbb{L}$ & All layers \\
$\mathbb{N}$ & All neurons \\
$\mathbb{D}$ & All NDP-DIMMs \\ 
$f_{i}$ & Activation frequency of neuron $i$ \\
$N_{l}$ & Neuron in layer $l$ \\
$M_{i}$ & The memory space required by neuron $i$ \\
$T_{sync}$ & The time required for one synchronization \\
$T_{l}^{j}$ & The time for computing one neuron in layer $l$ on processing $j$ \\
$S_{j}$ & The storage size for processing unit $j$ \\
\hline
\rowcolor{black!10}
\multicolumn{2}{c}{\textbf{\textit{Binary Variables - needed to be solved}} } \\
\hline
\multirow{2}{*}{$x_{il}^{j}$} & Whether neuron $i$ in layer $l$ is placed on processing unit $j$ \\
& $x_{il}^{j}=1$ means the neuron $i$ in layer $l$ is placed on processing unit $j$\\
\hline 
\end{tabular}
}
\vspace{-0.3cm}
\end{table}

\subsection{Offline Neuron Mapping} \label{sec:offline}
% Since both NDP-DIMMs and GPU cooperate to process the same operator, 
Since NDP-DIMMs and GPU are responsible for the computational load of the neurons stored in them, the predetermined mapping for each neuron's location greatly influences the inference efficiency. However, due to the huge neuron mapping space (e.g., more than $2^{1000}$ for LLaMA-7B), solely relying on online mapping solutions is impractical and will contribute to considerable performance degradation. Therefore, in the belief that ``hot" and ``cold" neurons are partly attributed to the pretrained LLM's nature~\cite{song2023powerinfer, song2024prosparse, zheng2024learn, song2024turbo}, we utilize the offline profiled information to deduce the initial offline neuron mapping. It alleviates the adjustment cost of subsequent online partition and scheduling during inference. Please note that the optimal initial mapping denotes the mapping that can be found during the offline stage, which will be adjusted during runtime.

% To fully unleash the computational capabilities of both NDP-DIMMs and GPU, we need to effectively partition the hot and cold neurons. Therefore, we first propose an offline neuron partition scheme to optimize data layout. 

% \subsubsection{Offline Parameter Placement}


% 感觉这里要不要加一句说，我们的将该问题抽象为ILP的最终目标是求解是系统时延最小的方案，然后再说求解的过程考虑了XXX，最后说具体的建模如下
To determine the optimal location for each neuron \update{that minimizes the inference latency using our heterogeneous system}, we formalize the mapping issue as an integer linear programming problem (ILP). In particular, we analyze several factors, including each neuron's activated frequency, computational overhead, memory usage, and synchronization delays, to model the inference performance of the \name~system. \update{To gather these factors accurately, we test the model on popular datasets such as C4~\cite{raffel2020exploring} and Pile~\cite{gao2020pile} with 128 samples}, and also employ an execution monitor in the host CPU to record during inference. The notation for solving the optimal offline neuron placement problem is summarized in Table \ref{tab:terminology}.

\textbf{Objective function.}
The objective of the optimal neuron mapping is to minimize the total inference latency, as shown in Equation \ref{eq1}. Since the execution of each layer involves both GPU and NDP-DIMMs, the total execution time is determined by the longer duration of the GPU and NDP-DIMMs execution times. For NDP-DIMMs, the single-layer execution time is the longest execution time among all DIMM modules, as shown in Equation \ref{eq2}. For the GPU, the single-layer execution time includes both computation time and extra synchronization overhead, while the synchronization overhead includes that of fetching input activation data from the DIMM and sending the computation results back to the DIMM to trigger the merge kernel. Hence, as illustrated in Equation \ref{eq3}, the total GPU execution time also includes twice the single-direction synchronization overhead. 

% 整体的求解目标 for layer l 
{
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{0pt}
\begin{equation}
    \min \textstyle \sum_{l} max (T_{GPU-l}, T_{DIMM-l}), \quad \forall l \in \mathbb{L} \label{eq1}
\end{equation}
\begin{equation}
     T_{DIMM-l} = max (T_{dimm-jl}), \quad \forall j \in \mathbb{D} \label{eq2}
\end{equation}
\begin{equation}
    T_{GPU-l} = T_{compute-GPU-l} + 2\cdot T_{sync} \label{eq3}
\end{equation}
}


The computation times for a single layer on both the GPU and NDP-DIMMs depend on the number of activated neurons located in each device. Let $T_{l}^{GPU}$ represent the time required to compute a single neuron on the GPU. Consequently, the computation time for a single layer on the GPU is the product of the number of activated neurons in the GPU memory and the time taken to compute each neuron, as illustrated in Equation \ref{eq4}. Similarly, the single-layer computation time for each NDP-DIMM is demonstrated in Equation \ref{eq5}.% Similarly, the NDP-DIMM's single-layer computation time is the product of the activation counts of all neurons stored in a single DIMM module and the computation time per neuron, as shown in \eq \ref{eq5}.


% GPU & NDP-DIMM 计算时间
{
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{5pt}
\begin{align}
    T_{compute-GPU-l} = T_{l}^{GPU} \cdot \textstyle \sum_{i} f_{i}\cdot x_{il}^{GPU}, \quad \forall i \in \mathbb{N} \label{eq4}\\ 
    T_{dimm-jl} = T_{l}^{DIMM} \cdot \textstyle \sum_{i} f_{i} \cdot x_{il}^{dimm-j}, \quad \forall i \in \mathbb{N} \label{eq5}
\end{align}
}


\textbf{Constraints.}
The offline optimal neuron placement issue must adhere to the conditions listed in \eq \ref{eq6} and \ref{eq7}, which limit the memory space occupied by neurons not to exceed the available memory size of each DIMM and GPU.

% GPU & DIMM 存储约束
{
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{5pt}
\begin{align}
   \textstyle \sum_{l} M_{i} \cdot x_{il}^{GPU} \le S_{GPU}, \quad \forall l \in \mathbb{L} \label{eq6}\\ 
   \textstyle \sum_{l} M_{i} \cdot x_{il}^{dimm-j} \le S_{dimm-j}, \quad \forall l \in \mathbb{L} \label{eq7}
\end{align}
}

Consequently, we employ the open-sourced optimization solver, PulP~\cite{pulp-solver}, to determine the optimal offline neuron mapping.
% Given that ILP problems are inherently NP-complete, directly solving them for an LLM with hundreds of billions of parameters presents a considerable computational challenge. 
Based on our assessment, it takes about 110 seconds to solve for the optimal neuron mapping, making it appropriate for a single offline compilation process. Before LLM inference, we initially transfer relevant hot neurons to GPU memory based on the mapping outcomes and further adjust the mapping during runtime to improve efficiency.
% However, the activation sparsity causes the neuron activation distribution to be non-static, 


% 准备把 C 和 D 两个小节的逻辑都换一下，重点应该是 hot/cold partition 以及 partition 之后的对应操作导致的影响，predictor 对 inference 的影响要弱化，不要去过多强调这个，是一个附加的。后续重点去考虑那部分，而非当前的predictor 的内容带来的实质性影响。 然后 C 的话，重点就是 hot/cold neurons 的预测，跟 D 的关系就没有那么大
\subsection{Online Adjustment for Hot/Cold Neuron Partition}\label{sec:partition-design}  

Although the optimal offline neuron mapping provides an effective hot/cold partition, the input-specific nature of activation sparsity makes the hot/cold neuron partition change dynamically in practice. Our evaluation indicates that about 52\% of the initialized hot neurons exhibit varied activity during inference. Therefore, it is necessary to adjust the hot/cold neuron partition online to improve inference efficiency before neuron computation, which requires an in-advance prediction of the neuron partition. In this section, we leverage the distribution patterns of activation sparsity to create a novel lightweight predictor to guide the online adjustment of the hot/cold neuron partition.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Fig/predictor.pdf}
    \vspace{-0.3cm}
    \caption{The predictor design in Hermes. (a) We are motivated to utilize the temporal locality of token generation for prediction. (b) The layer-wise correlation effectively predicts activated neurons.}
    \label{fig:predictor}
\vspace{-0.3cm}
\end{figure}

\subsubsection{Predictor Design}\label{sec:predictor-design}
% 一方面，workflow中需要提前决定the computation loads for both GPU and NDP-DIMMs to utilize the activation sparsity; 另一方面，提前将 hot neurons 映射到 GPU 上有助于充分发挥 GPU 的算力，缓解NDP-DIMMs的负载。
Accurately forecasting activated neurons and the hot/cold neuron partition is crucial for improving inference performance. On one hand, to effectively harness activation sparsity, the \name~workflow necessitates predetermining the computation loads for both the GPU and NDP-DIMMs. On the other hand, assigning hot neurons to the GPU before computation can fully utilize the GPU’s computation capability and ease the burden on NDP-DIMMs. Nevertheless, existing MLP-based predictors~\cite{song2023powerinfer, song2024prosparse, liu2023deja} incur considerable storage and computation overhead, reducing inference efficiency. To address it, we introduce a lightweight predictor that exploits token-wise similarity and layer-wise correlation (discussed in Section \ref{sec:similar}) for accurate predictions.

\textbf{Token-wise Prediction. } The token-wise similarity suggests that the distribution of activated neurons is similar among adjacent tokens. Given that tokens are generated one by one during the token generation stage, token-wise similarity can be considered as a temporal locality of activated neurons. Inspired by well-known branch prediction strategies~\cite{smith1981study, yeh1991two, mcfarling1993combining} that also benefit from temporal locality, we propose a novel prediction strategy. As shown in \fig \ref{fig:predictor}a, we establish a neuron state table where each neuron has a 4-bit state, ranging from 0 to 15, used to predict whether the neuron will be activated. After the prefill stage, we initialize each neuron's state based on the activated frequency in the whole prefill stage. Specifically, we divide the distribution of the activated frequency into 16 stages and initialize each state accordingly. For example, if a neuron's activated frequency exceeds 90\%, its state is initialized as `15', whereas if the ratio is below 2\%, the state is set as `0'.

We update each neuron's state based on the actual activated neurons during each token generation step using a finite state machine. If a neuron is not activated, its state decreases by 1; if it is activated, its state increases by \( s \), which is set to 4 in this paper. The left part of \fig \ref{fig:predictor}a shows that, when neuron 6 is activated, the state is updated from $7$ to $11$, while the state of neuron 5 is updated from 10 to 9 as it is not activated.  

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Fig/mapper.pdf}
    \vspace{-0.3cm}
    \caption{Neuron mapper design. (a) The mapper utilizes the information in the neuron state table to adjust the hot/cold neuron partition. (b) Cold neurons are remapped based on the neuron activity within a window.}
    \label{fig:mapper}
\vspace{-0.3cm}
\end{figure}

\textbf{Layer-wise Prediction. } 
Token-wise similarity alone cannot address fluctuations in neuron activity between tokens~\cite{zhang2024relu, zheng2024learn}. Therefore, we further employ layer-wise correlation to improve prediction accuracy. Insights from Section \ref{sec:similar} suggest that if neurons with high correlation in the preceding layer are activated, the activated probability for the current neuron is significantly increased. Consequently, we create a neuron correlation table to boost layer-wise prediction. As depicted in Figure \ref{fig:predictor}b, we initially offline sampled the top 2 correlated neurons from the previous layer and documented their relationships in the neuron correlation table.
% During token generation, to determine whether it needs activation, we record the activation number of its top 2 correlated neurons. If the number exceeds a threshold \( T2 \), the neuron is predicted to be activated; otherwise, it is not.

% if a neuron has a state $s_1$ from the neuron state table and a number of $s_2$ highly-correlated neurons from the neuron correlation table are activated, its activation prediction follows: $s_1 + \lambda\cdot s_2> T $.

Finally, we combine the token-wise and layer-wise prediction strategies to achieve accurate prediction for activated neurons during token generation. Specifically, we use $s_1$ to denote the state in the neuron state table for one neuron, and use $s_2$ to indicate the activated number of the highly correlated neurons for one neuron. To predict the activation state for such a neuron, we examine the inequation: $s_1 + \lambda\cdot s_2> T $. In this paper, we set $\lambda$ as 6, and the threshold $T$ as 15. As Figure \ref{fig:predictor} shows, following the prediction criterion, we finally activate neurons 3, 6 and 9 for subsequent computation. During context switches, token similarity may vanish, but layer-wise correlation is still available for effective prediction. Conversely, even if correlated neurons are not activated, observing neighboring tokens' activation states still helps achieve accurate prediction. Experimental result shows that the accuracy of our proposed predictor achieves 98\% using less than 1MB of memory. \update{For instance, LLaMA-7B occupies 32 layers, with each one having 4K neurons for the self-attention block and 10.5K for the MLP block. In our implementation, only 4-bit data is used to record the corresponding state of each neuron. Consequently, it only costs 232 KB for the neuron state table of LLaMA-7B.} We integrate the proposed predictor into the host CPU and store the table values in the last level cache for fast prediction.


\subsubsection{Online Adjustment guided by Predictor}
Given their ample memory capacity, instead of mapping only cold neurons, we store all the weight parameters on DIMMs. Thus, we only need to reload the actual hot neurons onto GPU memory to achieve online adjustment. The neuron state in our proposed predictor effectively represents the activity of each neuron. Specifically, as shown in the \fig \ref{fig:mapper}a, once the neuron state exceeds a certain threshold $T_h$, it can be viewed as the hot neuron. In this paper, we set the threshold $T_h = 10$. Accordingly, neurons 3, 6, and 9 are identified as hot neurons. We then use the neuron mapper to locate the corresponding hot neuron. As the hot neuron 6 is originally located on the DIMMs, an instruction is issued to copy the corresponding hot neuron to the GPU memory during the projection computation. Meanwhile, the neuron with the lowest state value (neuron 5) stored in GPU memory will be swapped out. Note that, since all neurons are stored in DIMMs, we only need to overwrite the location of the neuron to be swapped out in the GPU memory to achieve neuron swapping. In general, online neuron adjustment between GPU and NDP-DIMMs significantly improves the inference efficiency without inducing additional data transfer overhead.


\subsection{Online Remapping for Cold Neurons}\label{sec:cold-neuron-mapping}
Due to our implementation of a center buffer-based NDP-DIMM architecture, the total computation delay correlates with the count of activated neurons in each DIMM module. As shown in Equation \ref{eq2}, the total execution duration is constrained by the slowest-performing NDP-DIMM module. Hence, determining the optimal cold neuron assignment to ensure a balanced load across multiple NDP-DIMMs is crucial.
Despite using DIMM-link for inter-DIMM communication, the limited bandwidth (25GB/s) cannot afford over-frequent data exchanges between DIMMs. Therefore, we need to achieve a load balance across multiple NDP-DIMMs while minimizing the remapping of cold neurons.

\begin{algorithm}[t]
\scriptsize
    \caption{Window-based online scheduling}\label{alg:balance}

\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}

\Input{neuron mapping $C_{j,i}$; Activity for neuron $i$ within a window $A_{i}$; Number of NDP-DIMM modules $J$;}

\emph{{\textcolor{magenta}{// Compute the number of activated neurons for NDP-DIMM $i$.}}}
$Z_{j} = \sum_{i} C_{j,i} \cdot A_{i}$ \\ 
Sort $Z$ with the descending order \\

\For{{\textcolor{blue}{int}} id = 0; $id$ $<$ J/2; id++}{
  \While{$Z_{id} \le Z_{J-id}$}{
    Find the most activated neurons $h$ in NDP-DIMM $id$\\ 
    \emph{{\textcolor{magenta}{// Remapping the most activated neurons from $id$ to $J-id$}}}
    $C_{id, h} = 0$; $C_{J-id, h} = 1$
  }
}
\end{algorithm}

The similarity between tokens inspires us to develop a novel window-based online scheduling method for remapping cold neurons. In particular, we group every five consecutive tokens into a window. Based on our observations, due to the token-wise similarity, once the optimal mapping for cold neurons is identified, the runtime variance among different NDP-DIMMs within a window is under 5\%, indicating a balanced assignment. Nevertheless, when surpassing the window size, the performance disparity among different NDP-DIMMs varies from 1.2$\times$ to 2.5$\times$. Consequently, we can leverage the neuron activity within a window to guide the remapping of cold neurons. As shown in Algorithm \ref{alg:balance}, we initially gather the activated times for each neuron $i$ within a window and calculate the total activated neurons in NDP-DIMM $j$ based on the current neuron mapping $C_{j,i}$. $C_{j,i}$ is a binary matrix that denotes if neuron $i$ is mapped on NDP-DIMM $j$. We then sort the total activated neurons for NDP-DIMMs within the window and adjust neuron mappings between DIMM pairs accordingly. Specifically, the NDP-DIMM with the largest number of activated neurons is paired with the one that has the fewest activated neurons. Finally, the most activated neurons in the NDP-DIMM pair are remapped to achieve balance. As depicted in \fig \ref{fig:mapper}b, we record the activated neurons within a window into the neuron activity table, and calculate the activity for each NDP-DIMM based on the mapping results. As the count of activated neurons in DIMM-1 exceeds that of DIMM-2, neuron 5 from DIMM-1 is remapped to DIMM-2 for load balance between the two NDP-DIMMs. This strategy offers two advantages: first, the fixed inter-DIMM communication traffic is directed to different bridges to prevent congestion; second, the greedy remapping approach can quickly achieve balance with minimal data transfer.
