\section{Background}

\subsection{LLM Inference \& Architecture}\label{sec:llm-procedure}

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{Fig/llm-procedure.pdf}
    \vspace{-0.3cm}
    \caption{The LLM inference procedure and architecture.}
    \label{fig:llm-execution}
\vspace{-0.3cm}
\end{figure}

As shown in \fig \ref{fig:llm-execution}, the inference procedure of transformer-based LLMs comprises two stages: prompting and token generation. During the prompting stage, an input sequence is used to produce keys and values (KV cache) for each transformer layer in the LLM, and this is done just once per inference. In the token generation stage, previously generated tokens are used to update the KV cache and generate new tokens incrementally. This stage is executed multiple times, depending on the length of the output sequence. \update{Since token generation accounts for more than 90\% of the total runtime~\cite{li2024specpim}, this paper primarily focuses on optimizing inference efficiency in token generation.}

% Specifically, during the prompting stage, the input tensor shape is \([B, L, H]\), where \(B\) represents the inference batch size, \(L\) represents the input sequence length, and \(H\) represents the channel size of the model. During the token generation stage, as each step processes only one token, the corresponding input tensor shape changes to \([B, 1, H]\).

An LLM has multiple transformer layers, each containing a self-attention and an MLP block. In the self-attention block, input \(x\) is projected linearly to produce Q, K and V, processed by the attention operator to yield the attention result, and then computed by the projection layer for the MLP input. The MLP block includes fully connected (FC) layers and non-linear functions. For example, the OPT model uses two FC layers which are connected by one ReLU activation function. 


\subsection{Activation Sparsity in LLMs}\label{sec:activation-sparsity}

The activation function such as ReLU in the MLP block introduces the intrinsic activation sparsity to LLMs~\cite{liu2023deja, mirzadeh2023relu, song2024prosparse}. As shown in \fig \ref{fig:pruning-network}a, the ReLU function in the MLP block, can turn many activation values to zero, eliminating the need to load and compute these inactive neurons. As the red dashed box shows, a neuron in this paper represents a specific row or column within a weight matrix. For example, due to the ReLU function zeros out the 1st, 4th and 5th input values of the FC2 layer, the corresponding columns and rows in FC1 and FC2 weight matrix will not be activated. 

%bing:0724 其实上一段并没有把稀疏归因为ReLu, 这里讲替换为ReLu的写法很奇怪。逻辑上可以先说，现在模型上有替换ReLu的趋势，不影响性能，但能提高稀疏性。对模型设计者，这样能给他们同模型压缩的机会。那对于我们做优化的来说，也提供了潜在的提高性能的机会。
To further achieve activation sparsity on self-attention blocks, programmers insert ReLU functions before QKV generation~\cite{mirzadeh2023relu}, as illustrated in \fig \ref{fig:pruning-network}b. For LLMs that do not use ReLU as their activation function, such as LLaMA (SiLU) and Falcon (GELU)~\cite{touvron2023llama2, almazrouei2023falcon}, recent work has demonstrated that they can also be replaced by ReLU functions~\cite{mirzadeh2023relu, song2024prosparse}, as demonstrated in \fig \ref{fig:pruning-network}c. Previous studies~\cite{song2024turbo, zheng2024learn, song2024prosparse} also demonstrated that the activation sparsity within LLMs provides significant sparsity (ranging from 70\% to 90\%) with negligible accuracy degradation (less than 1\%).

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{Fig/sparsity-illustration.pdf}
    \vspace{-0.3cm}
    \caption{The inherent activation sparsity within certain LLMs is further enhanced to achieve higher sparsity across various LLMs.}
    \label{fig:pruning-network}
\vspace{-0.3cm}
\end{figure}

\subsection{Offloading-based LLM Inference Systems}\label{sec:background-offloading}
Most existing LLM inference systems~\cite{tensorrt-llm, rasley2020deepspeed, yu2022orca, kwon2023efficient} require the use of expensive server-grade GPUs, which provide high-capacity HBM to store the large-scale LLM parameters. This limits their deployment to easily accessible and affordable hardware. Offloading is a viable technique to enable LLM inference on such commodity hardware~\cite{jain2022hugging}. For instance, a single consumer-grade GPU can leverage the host memory resources to perform inference of LLaMA2-70B~\cite{rasley2020deepspeed, huggingface-accelerate}. 

Existing offloading-based inference systems utilize host memory to extend the storage capacity of the GPU to accommodate LLMs. As long as there is sufficient host memory, this strategy can be used to perform inference on LLMs of various sizes. 
HuggingFace Accelerate~\cite{jain2022hugging} integrates offloading techniques from training systems by automatically mapping and partitioning weights into GPU and host memory respectively, only transferring the necessary parameters during inference. However, the characteristics of LLM inference are quite different from training~\cite{cho2024llmservingsim}, making it inefficient. To address this issue, FlexGen~\cite{sheng2023flexgen} provides a novel zig-zag offloading strategy to maximize the inference throughput within a limited PCIe bandwidth. This zig-zag scheduling strategy integrates multiple tokens into a block and overlaps the weight-loading cost during token processing within one block. For instance, it computes all the tokens in one block (e.g., more than 100 tokens) with the weights in layer $i$, while prefetching the weights in layer $i+1$ simultaneously. The burdensome block computation in one layer effectively overlaps the weight prefetching cost for the next layer, especially for the prefill phase which occupies multiple tokens even with a single batch. However, this method is unsuitable for local deployment scenarios, which only occupy limited batch sizes~\cite{hong2024flashdecoding++} during token generation.
% FlexGen~\cite{sheng2023flexgen} proposes a zig-zag scheduling strategy to effectively leverage the limited bandwidth of PCIe, achieving high-throughput inference. However, this method is not suitable for scenarios sensitive to per-token latency, which is common in local deployments.
Deja Vu~\cite{liu2023deja} further exploits \textit{activation sparsity} to perform LLM inference by predicting and loading only the activated neurons, thereby reducing data access and computation overhead. 
However, since the activated neurons are dynamic and cannot be pre-loaded into the limited consumer-grade GPU memory, data still need to be loaded from host memory, resulting in inference efficiency being bounded by PCIe bandwidth. 
Overall, while existing offloading solutions can effectively extend the storage capacity of inference systems to support larger LLMs, the low bandwidth data transfer of PCIe results in poor inference performance. 
 
% For example, using dual GPUs instead of an offloading-based inference can achieve over a 10.2x speedup on the HuggingFace system.


