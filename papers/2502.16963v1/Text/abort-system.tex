\section{\name~System}
\subsection{System \& Architecture Overview}

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{Fig/overview.pdf}
    \vspace{-0.3cm}
    \caption{Overview of our proposed Hermes System. (a) Hermes augments GPU memory with NDP-DIMMs, and utilizes scheduler to control the inference workflow. (b) Multiple NDP-DIMMs are connected to support LLM inference and inter-DIMM communication. }
    \label{fig:system-overview}
\vspace{-0.3cm}
\end{figure*}

\subsubsection{Architecture}

\fig\ref{fig:system-overview}a illustrates the overview of \name. \name~augments consumer-grade GPU with NDP-DIMMs to achieve low-cost, high-performance inference system for LLMs. We elaborate the architecture detail of each component as follows.

\textbf{Consumer-grade GPU:}
We choose the easy to access and budget-friendly consumer-grade GPU to handle burdensome computation in LLM inference. Though with limited on-chip memory, consumer-grade GPUs possess abundant computing units, such as tensor core, to achieve high-performance parallel processing. Furthermore, consumer-grade GPUs usually feature high-speed GDDR memory, which offers superior bandwidth than external memory devices. For example, the NVIDIA RTX 4090 with 24GB GDDR6 memory provides 82.6 parallel processing TFLOPS, 1321 AI TOPS, and 936 GB/s of internal bandwidth, making it well-suited for the intensive computational demands of LLM inference. Hermes leverages the powerful processing capability of consumer-grade GPUs to achieve efficient execution on hot neurons. 

%[726] 不确定下面这句还要不要 : liu 暂时觉得可以不要，后面会主要说workflow，
% Given the abundant computational power of the GPU and the high-speed bandwidth but limited storage space of GDDR memory, during inference, we load only the hot neurons into GDDR and utilize the GPU's tensor cores to efficiently compute the hot neurons in parallel.

\textbf{NDP-DIMM:}
% Buffered DIMM is one of the commonly used DIMM types. 
% PUs (Processing Units) can be located near the bank/distributed buffer or in a center buffer within a single DIMM. Under dynamic sparsity, weight data access is random. In a near bank or distributed buffer strategy, PUs can only access part of the data stored in the DIMM. This leads to situations where some PUs have active weights performing computation while others are idle. Balancing the computational load through bank-to-bank or DRAM chip-to-chip data exchanges incurs additional read/write overhead. Using a center-buffer-based PIM allows PUs to access all data in the DIMM. This configuration matches the random computation pattern.
% To match the input-specific computation pattern of activation sparsity, we choose to put the process engines in the DIMM's center buffer, as presented in \fig \ref{fig:system-overview}b.
% Rather than locating near the bank or distributed buffer that can only access partial data within DIMM, using a center-buffer-based NDP allows near-data processors to access all data in the DIMM.
% Meanwhile, centralized buffer-based NDP allows us to achieve higher bandwidth for near-memory computation at a lower cost. 
% To support the typical computations in LLMs and potential inter-DIMM communication needs, we mainly add three components in the central buffer: GEMV unit, activation unit, and DIMM-link.
As the cold neuron exhibits a random computation pattern, all data stored on DIMM can potentially be accessed. %TODO here需要再考虑下
Therefore, as presented in \fig \ref{fig:system-overview}b, we choose the center-buffer based near data processing (NDP) DIMM, in which the process engine can access all the data stored in the DIMM.
% Additionally, center-buffer based NDP-DIMM is much cheaper than near-bank-based. 
Here we provide the microarchitectural details of our NDP-DIMM design.
To support the typical computations in LLMs and potential inter-DIMM communication needs, we mainly add GEMV units, activation units, and DIMM-link in the central buffer.

\textit{GEMV Unit}: 
The GEMV units read data from the DRAM cell and center buffer, performing the GEMV computation of cold neurons.
Each GEMV unit contains XXX vector MACs, an accumulation and XXX KB buffer. 
During computation, the vector MAC is responsible for the XXX bit multiplication and accumulation between vectors, and the accumulator is responsible for the addition of partial sums with data dependencies. The buffer stores the intermediate result.
% For example, for a neuron computation in qkv generation, the GEMV unit loads weights and inputs from DRAM cells and buffers respectively, and uses XXX vector Macs perform XXXbit computation each. After that, the accumulator completes the accumulation of XXX XXXbit results to finally obtain the result.
% During LLM inference, operands are first loaded into the center buffer with internal bandwidth and then processed using the GEMV unit. 
% Specifically, we keep the activation values in the central buffer as it only contains a few KBs. For each computation, weights need to be loaded to the GEMV unit.
% For instance, the activation values occupy minimal storage that can be accommodated in the central buffer, with weights being loaded into the buffer for each computation.

\textit{Activation Unit}: The activation unit is designed to support the necessary non-linear functions, such as softmax and ReLU for LLM inference. 
Within this unit are XXX FP32 exponentiation units, XXX FP32 addition units, XXX FP32 multiplication units, along with a comparator tree, adder tree, and a divider. The configuration of activation units aligns with the throughput requirements of GEMV units. 
Meanwhile, the activation unit reuses the comparator tree to enable the hardware support for both softmax and activation functions.
% By incorporating an activation unit in the central buffer, we can reduce unnecessary data transfers. For example, softmax requires the `score' output, and having the activation unit and score computation unit both in DIMM allows direct data access from the same center buffer, avoiding additional data transfer to the GPU.

\textit{DIMM-link}: 
Due to the input-specific nature of activation sparsity, it is necessary to adjust the neuron mappings across NDP-DIMMs to further improve the inference efficiency. Therefore, we adopt DIMM-link~\cite{zhou2023dimm} to achieve high-speed inter-DIMM communication. DIMM-link employs bidirectional external data links between DIMMs, facilitating efficient data transfers through point-to-point and inter-DIMM broadcasts. The DIMM-link controller and bridge in the central buffer enable high-speed communication between DIMMs.
% For example, DIMM0 and DIMM1 store weights for N channels, but if no computations are needed for DIMM0 and x channels need computation in DIMM1, we can exchange x/2 channels' weights between DIMM0 and DIMM1 to balance the load, thereby reducing computation latency.

\textbf{Scheduler}:
%[zsxQ726]这里是否需要说明有一个scheduler的理由：使用scheduler来进行简单快捷的GPU/NDP-DIMM上的neuron计算任务调度
During LLM inference, the scheduler in the host CPU allocates neuron computation tasks to the GPU and NDP-DIMM, which collaboratively complete the neuron computation. The scheduler mainly includes two components: a lightweight predictor and a neuron mapper. Additionally, scheduler contains a monitor to collect the runtime information to guide the predictor, and a instruction queue to invoke instructions for GPU and NDP-DIMMs. The lightweight predictor utilizes the token-wise similarity and layer-wise correlation patterns to achieve accurate prediction on neuron activity. The neuron mapper determines the hot/cold neurons partition based on the prediction record and online adjusts the location of each neuron to achieve efficient inference for both GPU and NDP-DIMMs. The detailed design of these two components will be elaborated in the following sections.
% The details of predictor design will be illustrated in Section \ref{sec:partition-design}.

% 流水线，两段，上面总结，下面详细说MLP（参考ATTACC
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{example-image-b}
    \caption{The workflow of \name~system.}
    \label{fig:workflow}
\end{figure}

\subsubsection{Workflow}
The workflow of LLMs inference on the \name~system is shown in the \fig \ref{fig:workflow}a. Due to the intensive computation loads, the prompting phase is entirely executed on the GPU, following a conventional offloading-based strategy. During prompting, the KV matrices are transferred to the NDP-DIMMs once they are generated. Meanwhile, the host scheduler records neuron activity information for future prediction and scheduling optimization. Once completing the prompting stage, only the predetermined hot neurons are reloaded into the GPU memory for further token generation. We will elaborate the offline hot/cold neurons partition in Section \ref{sec:offline}.

In the token generation stage, for each transformer layer, the QKV generation is collaboratively completed by GPU and NDP-DIMM. The output of QKV generation will be collected in the NDP-DIMMs for attention computation. 
Attention computation is memory bandwidth-bound, making it well-suited for execution on NDP-DIMMs that can enhance computational bandwidth. Moreover, offloading attention computation to NDP-DIMMs can conserve GPU memory, which is typically scarce, by reducing the requirement for KV cache storage.
After getting the attention result, the projection demands significant computational power as it lacks activation sparsity. To avoid additional synchronization overhead from simultaneous execution on both GPU and NDP-DIMM, the projection phase will be exclusively handled by the GPU. 
During this process, we will dynamically adjust hot/cold partitions and neuron distribution across DIMMs based on predicted outcomes. Then, similar to the QKV generation, MLP is offloaded to both GPU and NDP-DIMM, we will detail it later. 
Finally, the output of a decoder block is reduced in the NDP-DIMM. 
This generation process is repeated several times until all tokens are generated.

For the QKV generation and MLP, as shown in \fig \ref{fig:workflow}b, taking MLP as an example, it includes three steps.
After completing the corresponding predictions, the host CPU checks whether the activated neurons are in GPU memory and determines the computation load for both the GPU and NDP-DIMM based on the location of the activated neurons for the current token.
Once the computation load and location are determined for each part, the host CPU invokes APIs for both the GPU and NDP-DIMM to load data and perform computation.
For example, the host CPU uses `cudaLaunchKernel' to launch GPU operators for GEMM and GEMV computations. To ensure correctness, the host CPU inserts barriers for the GPU and NDP-DIMM to synchronize their computations. 
After each part completes its computation, a merge kernel is called on the NDP-DIMM side to collect the results from both ends. 
This approach is beneficial for two reasons. Since the GPU typically finishes computations faster due to its higher computational power, the latency of transferring data from the GPU to DIMM can be overlapped by DIMM computation and does not increase the system's overall runtime.
Meanwhile, since the attention computation is performed on the NDP-DIMM, merging the QKV generation results on the NDP-DIMM side reduces the extra data transfer overhead.

% As shown in \fig \ref{fig:workflow}
% The generation phase mainly includes the computation of :
% (i) 
% (ii) Prediction of Activation: The scheduler predicts the activation of different neurons based on the historical activation records in the prediction table. 
% (iii) Task Allocation: According to the prediction results, the scheduler determines whether the weights of the activated neurons reside in the GPU or PIM. For channels whose weights are in the NDP-DIMM, the scheduler preemptively balances the load across DIMMs by redistributing the storage of the respective channel weights. Since NDP-DIMM computes based on the data's storage location, it uses DIMM-to-DIMM weight exchanges to achieve load balance. During computation, active channels whose weights are in the GPU are processed by the GPU, while channels that need activation but whose weights are not in the GPU are processed by the NDP-DIMM.
% (iv) Merging Outputs and Updating Predictions: After the computations are completed by both the GPU and PIM, the outputs are merged to form the final output result. The scheduler then updates the activation prediction table based on the actual activation patterns observed during this computation.

% \subsubsection{Example}

% Figure X illustrates the computation process of the FFN operator with dynamic sparsity during LLM inference. The example below demonstrates how inputs are allocated between the GPU and NDP-DIMM based on their regions (hot and cold), and how the scheduler predicts activations and manages load balancing.

% 1. Hot and Cold Region Allocation: Upon receiving the input, it is divided based on hot (blue regions in the figure, e.g., 3, 5, 9) and cold regions (green regions, e.g., 0, 1, 2, 4, 6, 7, 8, a, b). The scheduler determines which parts of the input will be processed by the GPU or NDP-DIMM.

% 2. Prediction of Activation: The scheduler uses its prediction mechanism to forecast which channels are likely to be activated during this computation. For example, in this prediction, channels 3, 5, 6, 9, and b are more likely to be activated (step 1). Accordingly, channels 3, 5, and 9 are processed by the GPU, while channels 6 and b are sent to the DIMM-based PIM for processing (step 2).

% 3. Load Balancing: In the NDP-DIMM, channels 6 and b need to be processed, but their weights are both stored in DIMM1. To balance the load, the scheduler orchestrates an exchange: it swaps the storage contents of channel 2 in DIMM0 with channel 6 in DIMM1, thereby distributing the computation load across both DIMMs (step 3).

% 4. Computation and Merging Outputs: Finally, the GPU and NDP-DIMM load the data into their respective computation units and process the predicted activated regions. Upon completion, the results are merged to form the final output (step 4).

% 逻辑应该是： the neuron mapping is important，但是仅靠 online 映射是不可行的，开销巨大 （搜索空间大），因此本节首先找到一种基于离线信息的optimal neuron mapping，并以此为基础进行后续的online dajustment 

\subsection{Offline Neuron Mapping} \label{sec:offline}
Since both NDP-DIMMs and GPU are collaborated to process the same operator, the predetermined mapping for each neuron's location greatly influences the inference efficiency. However, due to the huge mapping space (e.g., more than $2^{1000}$ for LLaMA-7B), solely relying on online solutions is inevitably insufficient. To this end, we utilizes the offline profiled information to provide the optimal offline neuron mapping, alleviating the adjustment cost of subsequent online partition and scheduling.

% To fully unleash the computational capabilities of both NDP-DIMMs and GPU, we need to effectively partition the hot and cold neurons. Therefore, we first propose an offline neuron partition scheme to optimize data layout. 

% \subsubsection{Offline Parameter Placement}
\begin{table}
\caption{Terminology for our ILP formulation.}
\label{tab:terminology}
\scriptsize{
\centering
\begin{tabular}{clp{5cm}}
\hline
\textbf{Symbol} & \textbf{Type} & \textbf{Description} \\[1ex]
\hline
$\mathbb{L}$ &  Par & All layers \\
$\mathbb{N}$ & Par & All neurons \\
$\mathbb{D}$ & Par & All NDP-DIMMs \\ 
% $s_{i}$ &  Par & Sparsity ratio for layer i \\
$f_{i}$ &  Par & Activation frequency of neuron j \\
$N_{i}$ &  Par & Neuron in layer i \\
$M_{i}$ & Par & The memory size for neuron i \\
$T_{sync}$ &  Par & The time required for one synchronization \\
$x_{nl}^{j}$ &  Var & Binary variable to denote neuron n in layer l is placed on processing unit j \\
$T_{l}^{j}$ & Var & The time for computing one neuron in layer $l$ on processing j \\
$S_{j}$ & Par & The storage size for processing unit j \\
\hline
\end{tabular}
}
\end{table}

To acquire the optimal location of each neuron, we formalize the mapping problem as a linear programming problem (ILP). Specifically, we consider a series of factors, including each neuron's activated frequency, computation overhead, memory size, and synchronization overhead, to model the inference efficiency of Herems system. To accurately collect the corresponding factors, we run the model on widely used datasets, such as C4~\cite{raffel2020exploring} and Pile~\cite{gao2020pile}, and further use the execution monitor on the host CPU to record during inference. The notation we used for sloving the optimal offline neuron mapping problem is summarized in Table \ref{tab:terminology}.

\textbf{Objective function.}
The goal of acquired neuron mapping is to minimize the whole inference latency, as shown in Equation \ref{eq1}. Since each layer's execution involves both GPU and NDP-DIMM, the overall execution time is the maximum of the GPU and NDP-DIMM execution times. The NDP-DIMM's single-layer execution time is the maximum execution time among all DIMM modules, as illustrated in Equation \ref{eq2}. The GPU's single-layer execution time includes computation time and additional synchronization overhead due to retrieving input activation data from the DIMM and sending the compute results back to the DIMM to invoke the merge kernel. Therefore, as shown in Equation \ref{eq3}, the GPU's overall execution time includes twice the synchronization overhead. 

% 整体的求解目标 for layer l 
\begin{equation}
    \min T = \min \textstyle \sum_{l} max (T_{GPU-l}, T_{NDP-DIMM-l}), \quad \forall l \in \mathbb{L} \label{eq1}
\end{equation}
\begin{equation}
     T_{NDP-DIMM} = max (T_{dimm-kl}), \quad \forall k \in \mathbb{D} \label{eq2}
\end{equation}
\begin{equation}
    T_{GPU-l} = T_{compute-GPU-l} + 2\cdot T_{sync} \label{eq3}
\end{equation}
% \begin{align}
%     \min T = \min \textstyle \sum_{l} max (T_{GPU-l}, T_{NDP-DIMM-l}), \quad \forall l \in \mathbb{L} \label{eq1} \\
%     T_{NDP-DIMM} = max (T_{dimm-kl}), \quad \forall k \in \mathbb{D} \label{eq2}\\ 
%     T_{GPU-l} = T_{compute-GPU-l} + 2\cdot T_{sync} \label{eq3}
% \end{align}

The single-layer computation times for the GPU and NDP-DIMM are determined by the number of neurons stored in each and their activated frequencies. Suppose the time to compute a single neuron on the GPU is $T_{l}^{GPU}$. Then, the GPU's computation time for single-layer is the product of the activation number of neurons stored in the GPU memory and the computation time per neuron, as depicted in \eq \ref{eq4}. Similarly, the NDP-DIMM's single-layer computation time is shown in \eq \ref{eq5}.
% Similarly, the NDP-DIMM's single-layer computation time is the product of the activation counts of all neurons stored in a single DIMM module and the computation time per neuron, as shown in \eq \ref{eq5}.


% GPU & NDP-DIMM 计算时间
\begin{align}
    T_{compute-GPU-l} = T_{l}^{GPU} \cdot \sum_{n} f_{n}\cdot x_{nl}^{GPU}, \quad \forall n \in \mathbb{N} \label{eq4}\\ 
    T_{dimm-kl} = T_{l}^{DIMM} \cdot \sum_{n} f_{n} \cdot x_{nl}^{dimm-k}, \quad \forall n \in \mathbb{N} \label{eq5}
\end{align}

\textbf{Constraints.}
Meanwhile, the offline optimal parameter placement must satisfy the constraints in \eq \ref{eq6} and \ref{eq7}, which restricts the storage budget of neurons in each end that can not exceed the corresponding memory capacity. 

% GPU & DIMM 存储约束
\begin{align}
   \sum_{l} M_{n} \cdot x_{nl}^{GPU} \le S_{GPU}, \quad \forall l \in \mathbb{L} \label{eq6}\\ 
   \sum_{l} M_{n} \cdot x_{nl}^{dimm-k} \le S_{dimm-k}, \quad \forall l \in \mathbb{L} \label{eq7}
\end{align}

Consequently, we employ the open sourced optimization solver, PulP~\cite{pulp-solver}, to determine the optimal offline neuron mapping.
% Given that ILP problems are inherently NP-complete, directly solving them for an LLM with hundreds of billions of parameters presents a considerable computational challenge. 
According to our evaluation, solving for the optimal neuron mapping once requires approximately 110 seconds, which is suitable for a one-time offline compilation. 
During LLM inference, we first follow the mapping results to store the corresponding hot neurons onto GPU memory, and further online adjusts the mapping strategy to enhance efficiency.
% However, the activation sparsity causes the neuron activation distribution to be non-static, 
% preventing us from determining an optimal placement strategy in advance. Thus, load balance between processing units must be achieved through online scheduling. 
% The solver-based solution cannot achieve real-time scheduling, necessitating a novel online scheduling approach.



\subsection{Online Adjustment for Hot/Cold Neuron Partition}\label{sec:partition-design}
Since the neuron activation distribution changes dynamically during token generation, 
% an offline optimal layout is not enough for the dynamic nature of online computation. Therefore, 
we further propose an online scheduling strategy tailored for the GPU and NDP-DIMM systems to achieve real-time optimal scheduling adjustment.
In this section, we delve into the activation sparsity aware online adjustment based on the neuron predictor. 
We will first introduce the design of neuron predictors for activation sparsity and then explain the hot/cold neuron execution process and memory management after employing the predictor.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{example-image-c}
    \caption{predictor}
    \label{fig:predictor}
\end{figure}
\subsubsection{Predictor Design}\label{sec:predictor-design}

%[ZSX727Q]这里结合图可能更容易理解为啥预测和FC1的计算相关
Accurately forecasting activated neurons is crucial for reducing data loading and computation in the FC1 layer, as well as for minimizing kernel startup time and preloading necessary data. 
Previous MLP-based predictors, introduce significant additional storage and computational overhead, which is unacceptable for local deployment. 
% For instance, the predictor for LLaMA-13B models requires an extra 4GB of memory to store the weights, which is unacceptable for local deployment. 
Thus, leveraging the token-wise similarity and layer-wise correlation described in section \ref{sec:similar}, we propose a lightweight predictor, which includes a history-based and correlation-based bundle prediction strategy.

\textbf{Token-wise Prediction} The token-wise similarity suggests that the distribution of activated neurons is similar among adjacent tokens. Given that tokens are generated one-by-one during the token generation stage of LLM inference, token-wise similarity can be considered a temporal locality of activated neuron distribution. Inspired by well-known branch prediction strategies that also benefit from temporal locality, we propose a history-based prediction strategy.
As shown in \fig \ref{fig:predictor} a, we establish a prediction table where each neuron has a 4-bit state, ranging from 0 to 15, used to predict whether the neuron will be activated. During the prefill stage, we initialize each neuron's state based on the activation ratio in the whole prefill stage. Specifically, we divide the distribution of the activation ratio into 16 stages and initialize each state accordingly. For example, if a neuron's activation ratio exceeds 90\%, its state is set to `1111' (15), whereas if the ratio is below 2\%, the state is `0000' (0).

During the token generation stage, if a neuron's state exceeds a threshold \( T1 \), we predict it will be activated; otherwise, it will not. 
We update each neuron's state based on actual activations using a finite state machine. 
% After getting which neurons are actually activated, the prediction table states are updated: 
if a neuron is not activated, its state decreases by 1; if it is activated, its state increases by \( s \), which is typically set to 4, 5, or 6 depending on the model. A larger \( s \) ensures that all activated neurons are predicted successfully.
% [zsx727Q]下面这一句和后面说使用layer wise的原因有点重复了，在考虑是否要给下面换个说法，还是去掉这里
% and mitigates prediction inaccuracies due to local fluctuations in neuron activation.

\textbf{Layer-wise Prediction. } 
Token-wise similarity alone may not address fluctuations in activation between tokens. Thus, we further leverage layer-wise correlation to enhance prediction accuracy. Observations in section \ref{sec:similar} indicate that if highly correlated neurons in the previous layer are activated, the current neuron is also likely to be activated. Based on this, we propose a correlation-based bundle prediction strategy.
Taking neuron X as an example, We first offline sample the top 10 neurons in the previous layer that are most correlated with it and record their relationships in a correlation table. During token generation, to determine whether it needs activation, we record the activation number of its top 10 correlated neurons. If the number exceeds a threshold \( T2 \), the neuron is predicted to be activated; otherwise, it is not.

We finally combine token-wise similarity and layer-wise correlation. Specifically, if a neuron has a state \( s1 \) from the token-wise prediction table and \( s2 \) from the correlation table, its activation prediction follows: \( s1 + \lambda s2 > T \). During context switches, token similarity may vanish, but layer-wise correlation still enables effective prediction. Conversely, even if correlated neurons are not activated, observing neighboring tokens' activation states still yields accurate predictions.

Our evaluation shows that the proposed predictor achieves a recall rate exceeding 98\% and a precision rate above 80\% using less than 1MB of memory, effectively providing low-cost yet accurate predictions.
% Compared to MLP-based predictors that require storing large amounts of additional weights, the prediction and correlation tables in our predictor design use less than 1MB of memory. 
Therefore, we integrate the entire predictor design into the host CPU and store the table values in the CPU cache for fast access and prediction.

% After completing the corresponding predictions, the host CPU checks whether the activated neurons are in GPU memory and determines the computation size for both the GPU and NDP-DIMM based on the location of the activated neurons for the current token. Once the computation size and location are determined for each part, the host CPU invokes APIs for both the GPU and NDP-DIMM to load data and perform calculations.

% For example, the host CPU uses `cudaLaunchKernel` to launch GPU operators for GEMM and GEMV computations. To ensure correctness, the host CPU inserts barriers for the GPU and NDP-DIMM to synchronize their computations. After each part completes its computation, a merge kernel is called to combine the results from both ends. The merge kernel is executed on the NDP-DIMM side. 

% This approach is beneficial for two reasons:
% 1. Higher GPU Compute Power: Since the GPU typically finishes computations faster due to its higher computational power, transferring additional data handling overhead to the GPU does not increase the system's overall runtime.
% 2. Reduced Data Transfer Overhead: Since additional attention calculations are performed on the NDP-DIMM, merging the QKV generation results on the NDP-DIMM side reduces the data transfer overhead.


\subsubsection{Load Balance between GPU and NDP-DIMM}
The goal of online scheduling in GPU memory is to achieve load balance between GPU and NDP-DIMM at runtime, further improving the utilization of computational resources and minimizing the execution overhead of LLM.
Based on offline hot/cold neuron partition, we store copies of the hot neurons in the GPU, and all the neurons in DIMM. Thus, we only need to adjust the hot neuron data in the GPU to achieve the online adjustment.
% Considering that DIMM has sufficient memory, all neurons can be stored in DIMM. Thus, we only need to store an additional copy of the hot neurons in the GPU and adjust the hot neuron data in the GPU according to the actual computation process. 
Given the token-wise similarity, the hot/cold distribution of tokens generated over a period is generally consistent. Therefore, we can effectively use the data in the prediction table for online scheduling.
% Our online scheduling design is based on token-wise similarity and optimal offline placement. 
% Specifically, we first solve the optimal neuron placement in the sampling data scenario using an offline solver, and use this as a baseline for subsequent online scheduling. 
% Given that the state in the prediction table indicates the activation degree of neurons over recent times, we can use this value for online scheduling. 

% Compared to NDP-DIMM, the GPU provides ample computational power but is limited by storage size. Therefore, maximizing GPU utilization and reducing overall execution time can be achieved by storing frequently activated neurons in GPU memory. Based on this, we adjust the hot neurons stored in the GPU according to the state of each neuron in the prediction table. 
Specifically, as shown in the \fig \ref{fig:predictor}, once the neuron state exceeds a certain threshold $T_h$ and the neuron is still in DIMM, that neuron needs to be copied to GPU memory.
Considering that GPU memory is usually fully utilized, we need to replace the neuron with the lowest activation frequency stored in GPU memory. Thus, the neuron with the lowest state value stored in GPU memory will be swapped out.

Fortunately, since all neurons are stored in DIMM, we only need to overwrite the position of the neuron to be swapped out in the GPU memory to achieve the neuron swap operation, reducing extra data transfer overhead. Through transferring data from DIMM to GPU during the attention computation, the entire neuron swap operation is inserted during idle periods of DIMM bandwidth to maximize DIMM bandwidth utilization. 
Overall, the online neuron adjustment between GPU and NDP-DIMM significantly improves the inference efficiency without inducing extra data transfer overhead.
% The data transfer process is effectively masked during the attention computation. 
% Overall, the neuron swap between GPU and NDP-DIMM almost does not increase the additional inference time.

% In summary, the execution flow with our predictor is as follows:
% 1. The predictor, integrated into the host CPU, uses token-wise and layer-wise predictions to forecast the activation of neurons.
% 2. The host CPU checks the GPU memory for activated neurons and determines the computation sizes for the GPU and NDP-DIMM.
% 3. The host CPU invokes necessary APIs to load data and perform computations on both the GPU and NDP-DIMM.
% 4. Barriers are inserted to synchronize computations between the GPU and NDP-DIMM.
% 5. A merge kernel on the NDP-DIMM side combines the results from both ends, minimizing data transfer overhead and optimizing the overall system performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{example-image-a}
    \caption{balance}
    \label{fig:balance}
\end{figure}

\begin{algorithm}
    \caption{load balance}\label{alg:balance}
\end{algorithm}

\subsection{Balance Execution aware Scheduler}
% In addition to considering load balance between GPU and NDP-DIMM, load imbalance between different DRAM channels can also be an issue. 
Since we use a central-buffer NDP-DIMM design, the overall computation latency is proportional to the number of activated neurons in the current DIMM channel. Therefore, achieving load balance between multiple DIMM channels is necessary, as the overall execution time will be limited by the slowest NDP-DIMM channel. Meanwhile, the load imbalance also leads to decreased utilization of the idle NDP-DIMM channel.

On one hand, we support DIMM-link to enable fast and low-overhead data transfer between DIMMs. On the other hand, although DIMM-link supports data transfer between DIMMs without going through the host CPU, thereby improving transfer efficiency, the data transfer bandwidth between DIMMs is less than 10\% compared to the bandwidth NDP owns. Therefore, the DIMM-to-DIMM transfer overhead is much higher than that of neuron computation in NDP. Consequently, a naive task stealing strategy cannot achieve load balance between NDP-DIMMs.
To address this issue, we propose a window-based greedy strategy to adjust neuron placement within different DIMMs and ultimately achieve load balance between NDP-DIMMs. 

Given the token-wise similarity, in the short time window, a balanced neuron placement strategy for NDP-DIMM will not become excessively unbalanced. 
% Therefore, additional data transfer is not required. 
However, once a specific time window is exceeded, the execution time between DIMMs becomes increasingly imbalanced. At this point, additional data exchanges are necessary to alleviate the imbalance, optimize overall utilization, and reduce execution time.
According to our evaluation, within 10 tokens, the imbalance degree of NDP-DIMMs is less than 10\%, allowing different NDP-DIMMs to maintain high utilization. However, once the window exceeds 15 tokens, the imbalance degree exceeds 20\%. Therefore, we set the adjustment window to 10 tokens. After completing 10 token generations, we will collect the neuron activation frequency during these 10 generations and rearrange the neurons in NDP-DIMMs accordingly. 

During this process, we also aim to minimize the overhead of data transfer to reduce scheduling time as much as possible.
To achieve this, we propose an intuitive greedy strategy, where we sort the execution times of NDP-DIMMs within the window and perform data exchanges between pairs of DIMMs accordingly. Specifically, as shown in Algorithm 1, we pair the NDP-DIMM with the highest activation frequency with the NDP-DIMM with the lowest activation frequency for data exchange, and so on, pairing the second highest with the second lowest. The benefit of this strategy is twofold: first, the mutual data exchange can utilize different data paths to avoid congestion; second, the high-low matching strategy can quickly achieve overall balance. According to our evaluation, the average imbalance degree after scheduling is only 3\%.