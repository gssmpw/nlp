\section{Related Work}
\label{sec:related_work}
\vspace{-1.5ex}
\subsection{Video Scene Graph Generation (VidSGG)} 
\vspace{-1ex}
Video Scene Graph Generation (VidSGG) aims to learn the spatio-temporal dependencies of a video to predict the dynamic relationships between all object pairs. Existing VidSGG models are categorized into two settings based on the granularity of the generated video scene graph: video-level VidSGG **Li, "Temporal Knowledge Distillation for Video Scene Graph Generation"** and frame-level VidSGG ____ . 
Video-level VidSGG models generate a global scene graph for a video clip, while frame-level VidSGG models generate a scene graph for each frame in a video clip. Note that our work follows the frame-level VidSGG setting.
Following the release of the Action Genome dataset ____ , frame-level VidSGG models have been actively researched. Specifically, STTran ____ employs two separate transformers to capture spatio-temporal dependencies of objects and frames, and TRACE ____ proposes a hierarchical relation tree method to enhance spatial-temporal reasoning. DSG-DETR ____ mitigates the complexity issue arising from the fully-connected graph between frames, enabling the capture of long-term temporal context. {Recently, TEMPURA ____ and FloCoDe ____ aim to alleviate the long-tailed predicate issue through memory-guided learning and label correlation loss, respectively.} However, as these methods heavily rely on costly annotations on all frames, we propose a weakly supervised approach for VidSGG, which utilizes the readily available video captions, i.e., language supervision, for the first time.





% \begin{table}[t]
% \centering
% \caption{Performance over various video length. We use backbone as STTran.}
% % \vspace{-1ex}
% \resizebox{0.85\linewidth}{!}{
% \centering
% \begin{tabular}{c|l|c|cccc|c}
% \toprule
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training Dataset\\ (Caption)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Avg. Video Length}} & \multicolumn{2}{c}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{4-7}
%                                            & \multicolumn{1}{c|}{}                                 &                                             & \textbf{R@20}         & \textbf{R@50}        & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule
% \multirow{2}{*}{Action Genome}             & WS-ImgSGG                                             & \multirow{2}{*}{29.9 seconds}                   & 10.01                 & 12.83                & 9.02                 & 14.05                & 11.48                          \\
%                                            & \proposed                                                 &                                             & \textbf{15.61}        & \textbf{19.60}       & \textbf{15.92}       & \textbf{22.56}       & \textbf{18.42}                 \\ \midrule
% \multirow{2}{*}{MSVD}                      & WS-ImgSGG                                             & \multirow{2}{*}{9.5 seconds}                    & 6.22                  & 8.03                 & 7.69                 & 12.31                & 8.56                           \\
%                                            & \proposed                                              &                                             & \textbf{9.05}         & \textbf{11.31}       & \textbf{10.22}       & \textbf{16.60}       & \textbf{11.80}                 \\ \midrule
% \multirow{2}{*}{ActivityNet}               & WS-ImgSGG                                             & \multirow{2}{*}{117.3 seconds}                  & 10.86                 & 14.47                & 10.07                & 15.80                & 12.80                          \\
%                                            & \proposed                                                 &                                             & \textbf{13.46}        & \textbf{17.58}       & \textbf{13.94}       & \textbf{21.41}       & \textbf{16.60}                 \\ \bottomrule
% \end{tabular}
% \label{app_tab:longer_video}
% }
% \end{table}


\vspace{-1.5ex}
\subsection{Weakly Supervised Scene Graph Generation}
\vspace{-0.5ex}
\noindent\textbf{Weakly Supervised ImgSGG (WS-ImgSGG). }
To relax a heavy reliance on the costly annotation of a fully supervised approach, ImgSGG utilizes two types of weak supervision. \textbf{1) Unlocalized scene graph}: It uses ground-truth scene graphs represented in text format, which have not yet been grounded to bounding boxes. In this regard, related studies **Zhang et al., "Learning Scene Graphs for Image Captioning"**__ and ____ have focused on aligning these text-based scene graphs with suitable bounding boxes. LSWS ____ utilizes the linguistic structure within triplets for grounding, while another study ____ proposes a graph-matching module based on contrastive learning to further improve the grounding performance. \textbf{2) Language supervision:} In order to further relax the annotation costs associated with unlocalized scene graphs, the natural language description (i.e., image caption) is used for training the model ____ . SGNLS ____ is the first to enable model training with image captions. LLM4SGG ____ addresses semantic over-simplification and low-density scene graph issues arising from the triplet generation process. 

\smallskip
\noindent\textbf{Weakly Supervised VidSGG (WS-VidSGG). }
As the first WS-VidSGG approach, PLA ____ utilizes an unlocalized scene graph of the middle frame in a video clip as weak supervision to train a VidSGG model. 
However, the assumption of a ground-truth scene graph existing in the middle frame among other frames is not only unrealistic but also still requires manual human annotation. On the other hand, our proposed~\proposed{} framework only relies on the readily available video captions to train a VidSGG model, which further reduces annotation costs.

% On the other hand, we propose, for the first time in VidSGG, a simple yet effective approach that corresponds to language supervision, which further reduces annotation costs.


% \subsection{Large Language Model}
% Large Language Models ____ (LLMs) have shown exceptional generalizability in the NLP domain, performing various downstream tasks without fine-tuning. A myriad of works adopt LLMs for various purposes ____ benefiting from the generalizability. Specifically, Chameleon ____ and VisProg ____ employ LLMs as tool planners to effectively utilize different tools in suitable combinations. LLM4SGG ____ harnesses LLMs to construct triplets from image captions for the ImgSGG task, while other studies ____ aim to comprehend time-series forecasting datasets using LLMs through prompts. In the context of the WS-VidSGG task, we leverage the LLMs to understand the temporality within video captions.

\vspace{-2ex}