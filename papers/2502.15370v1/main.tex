
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{bbding}
% \usepackage{ulem}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[titletoc, title]{appendix}
\usepackage{titletoc}
\usepackage{lipsum}
\usepackage{makecell}
\linespread{0.961}
\definecolor{gainsboro}{RGB}{233,233,233}

\title{Weakly Supervised Video Scene Graph\\ Generation via Natural Language Supervision}


% \author{Kibum Kim \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\iclrfinalcopy
\author{Kibum Kim\textsuperscript{1}  \quad
Kanghoon Yoon\textsuperscript{1} \quad
Yeonjun In\textsuperscript{1} \quad 
Jaehyeong Jeon\textsuperscript{1} \quad
Jinyoung Moon\textsuperscript{2} \quad \\
\textbf{Donghyun Kim}\textsuperscript{3}
\textbf{Chanyoung Park}\textsuperscript{1}\thanks{Corresponding Author}\\
\textsuperscript{1}KAIST \quad \textsuperscript{2}ETRI \quad \textsuperscript{3}Korea University \\
\texttt{\{kb.kim,ykhoon08,yeonjun.in,wogud405,cy.park\}@kaist.ac.kr} \\
\texttt{jymoon@etri.re.kr}\quad \texttt{d\_kim@korea.ac.kr}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\proposed}{\textsf{NL-VSGG}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are
two key reasons that hinder such a  naive adoption:
1) \textit{Temporality within video captions}, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time-related details, and 2) \textit{Variability in action duration}, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration.
To address these issues, we propose a \textbf{N}atural \textbf{L}anguage-based \textbf{V}ideo \textbf{S}cene \textbf{G}raph \textbf{G}eneration (\proposed{}) framework that only utilizes the readily available video captions for training a VidSGG model.
\proposed{} consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. 
Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. 
As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by \proposed{} is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality. 
Our code is available at \href{https://github.com/rlqja1107/NL-VSGG}{https://github.com/rlqja1107/NL-VSGG}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\vspace{-2.0ex}
Scene graph is a visually-grounded structured graph in which objects are represented as nodes and the relationships between them as directed edges. A scene graph bridges computer vision and natural language with high-level information, facilitating its usage on various downstream tasks, such as question answering \citep{vqa_1}, captioning \citep{image_caption_1}, and retrieval \citep{image_retrieval_1}.

In general, studies for scene graph generation (SGG) \citep{kim2024adaptive,yoon2023unbiased,jeon2024semantic,yoon2024ra} have been conducted in the realm of images, referred to as \textbf{ImgSGG}. These studies primarily excel at predicting static relationships (e.g., \textsf{standing on}) within a single image, while struggling to predict dynamic relationships (e.g., \textsf{running}) that may exist over consecutive images, since ImgSGG models are unable to capture dynamic visual relations \citep{chen2023video}. In this regard, Video scene graph generation \citep{cong2021spatial,feng2023exploiting,teng2021target}, dubbed as \textbf{VidSGG}, has emerged to capture temporal context across video frames and predict dynamic relationships, extending its scope beyond merely predicting static relationships within a single image.

\begin{figure*}[t]
\centering
    \centering
    \includegraphics[width=.85\columnwidth]{Figure/Intro.pdf}
    \vspace{-1ex}
    \caption{(a) The fully supervised VidSGG requires costly localized scene graphs across all frames. (b) The pipeline of WS-ImgSGG. (c) The pipeline of WS-VidSGG needs to consider the \textit{temporality within the caption} addressed by temporal segmentation and the \textit{variability of action duration} addressed by temporal alignment.}
\label{fig:intro}
\vspace{-3ex}
\end{figure*}

Existing VidSGG studies \citep{cong2021spatial,nag2023unbiased,xu2022meta,wang2022dynamic} follow a fully supervised approach, indicating a heavy reliance on costly annotation involving class information (i.e., entity and relation) alongside bounding boxes for entities \textit{across all frames} in a video (See Fig.~\ref{fig:intro}(a)). This indicates that VidSGG requires greater annotation costs compared to ImgSGG, which only requires annotations for a single image. 
Despite recent weakly supervised ImgSGG (i.e., \textbf{WS-ImgSGG}) approaches \citep{zhong2021learning,zhang2023learning,ye2021linguistic,kim2024llm4sgg,li2022integrating} that address the annotation cost in ImgSGG, weakly supervised approach for VidSGG (i.e., \textbf{WS-VidSGG}), where annotation costs are more demanding, remains unexplored.
Although PLA \citep{chen2023video}, the first WS-VidSGG study, proposes a framework for training an VidSGG model based on a ground-truth unlocalized scene graph of the middle frame, we argue that the assumption of a ground-truth scene graph existing in the middle frame, among other frames, is not only unrealistic but also still requires manual human annotation, thus continuing to impose substantial annotation costs.
 

In this work, we are interested in training an VidSGG model without any human-annotated scene graphs, and there can be two different strategies.
The first strategy would be to apply a pre-trained ImgSGG model, preferably the one with a strong zero-shot predictive ability for relations (e.g., RLIP \citep{yuan2022rlip} and RLIPv2 \citep{yuan2023rlipv2}), to every frame of the video to obtain pseudo-labeled scene graphs in each frame. 
However, since such a model is trained based on static visual relationships in images, they are not capable of predicting dynamic visual relationships\footnote{In Table~\ref{tab:main}, we show that the performance of RLIP and RLIPv2 is subpar in the Action Genome \citep{ji2020action} dataset under the zero-shot setting.}.
The second strategy would be to leverage language supervision from video captions and follow the conventional WS-ImgSGG pipeline \citep{zhong2021learning,zhang2023learning,ye2021linguistic,kim2024llm4sgg,li2022integrating} as depicted in Fig.~\ref{fig:intro}(b). Specifically, we could first parse a video caption to extract triplets, align them with consecutive frames, and ground the aligned triplets within each frame. 
However, such a simple approach of adopting the WS-ImgSGG pipeline to VidSGG is limited\footnote{In Table~\ref{tab:main}, we show that the simple approach performs poorly on the Action Genome dataset.} due to the following two key reasons:
\begin{wrapfigure}[10]{r}{.21\columnwidth}
\vspace{-2.2ex}
\begin{center}
    \includegraphics[width=.21\columnwidth]{Figure/temporal_marker.pdf}
\end{center}
    % \rule{3cm}{7cm}
    \vspace{-4.2ex}
    \caption{Ratio of temporal markers.}
    \label{fig:temporal_marker}
\end{wrapfigure}




% \noindent 
\begin{itemize}[leftmargin=3.mm]
    \vspace{-3.0ex}
    \item \textbf{Temporality within Video Captions.} Contrary to image captions shown in Fig.~\ref{fig:intro}(b), video captions often contain temporal markers (e.g., \textsf{before}, \textsf{while}, \textsf{then}, \textsf{after}) representing time-related details. Without considering them, applying the above simple approach may erroneously supervise the model. For example, in Fig.~\ref{fig:intro}(c), if we overlook the temporal marker \textsf{before} in the caption, and use $<$\textsf{person}, \textsf{sitting on}, \textsf{sofa}$>$ to annotate earlier frames rather than 
    later frames, the model would be mistakenly supervised, resulting in the performance degradation.
    As temporal markers account for around 65\% of the Action Genome (AG) dataset used for VidSGG, while they account for only 2\% in the Visual Genome (VG) \citep{krishna2017visual} dataset used for ImgSGG (See Fig.~\ref{fig:temporal_marker}), considering temporality is especially crucial in video captions. 
\end{itemize}

\begin{itemize}[leftmargin=3.mm]
    \item \textbf{Variability in Action Duration.} Human actions unfold over varying duration in a video. For example, in Fig.~\ref{fig:intro}(c), \textit{Sentence 1} occurs within two frames, while \textit{Sentence 2} extends across a longer span of four frames. 
    However, if we overlook such variability in the action duration and naively align the first segmented sentence with the first four consecutive frames and the second sentence with the latter four consecutive frames (i.e., $8\text{ frames}\div 2\text{ sentences}$), the 3rd and 4th frames would end up being annotated with $<$\textsf{person}, \textsf{carrying}, \textsf{cup}$>$, while the last two frames would end up being annotated with $<$\textsf{person}, \textsf{sitting on}, \textsf{sofa}$>$, both of which are undesired.
\end{itemize}

To this end, we propose a simple yet effective weakly-supervised VidSGG framework, called \textbf{N}atural \textbf{L}anguage-based \textbf{V}ideo \textbf{S}cene \textbf{G}raph \textbf{G}eneration (\proposed{}), that only utilizes the readily available video captions for training a VidSGG model. 
Our proposed framework consists of two key modules: Temporality-aware Caption Segmentation (TCS) module, and Action Duration Variability-aware Caption-Frame Alignment (ADV) module.
% caption segmentation module and caption-frame alignment module.
More precisely, TCS segments the video caption into multiple sentences while also considering the temporality within the video caption based on a Large Language Model (LLM).
% we employ a Large Language Model (LLM) to split the video caption into multiple sentences considering the chronological order. 
Then, ADV aligns the temporally segmented sentences with corresponding consecutive frames in the video considering the variability in action duration. 
The main idea is to perform $K$-means clustering on the frames, and assign each segmented sentence to the frames within a cluster based on its similarity with the cluster centroids.
%%%%%%%%%
% Furthermore, to also benefit from the frames that are not aligned with any segmented sentences (e.g., the last two frames in Fig.~\ref{fig:intro}(c)), we propose a novel pseudo-labeling strategy that leverages negative action classes (e.g., \textsf{not looking at}), which are generally challenging to obtain due to the rarity of negation in the video captions.



Through our extensive experiments, we demonstrate the superiority of \proposed{} over the simple adoption of 1) a pre-trained ImgSGG model, and 2) the WS-ImgSGG pipeline to VidSGG. 
% Our proposed approach shows superior performance on the Action Genome datasets compared to the straightforward approach. 
It is worth noting that for the first time, we show the capability of training a VidSGG by only utilizing the readily available video captions, i.e., language supervision.
As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by \proposed{} is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.
% , leading to utilizing the video-text pair datasets for model training. 
% Furthermore, we explore a variety of action classes included in the video captions and show the model's capability to predict a broader range of action classes. 
Our contributions are summarized as follows:

\vspace{-1ex}
\begin{itemize}[leftmargin=3.mm]
    \item We identify two key reasons for why a simple adoption of the WS-ImgSGG pipeline to VidSGG fails, i.e., temporality within video captions and variability of action duration.
    \item We propose a simple yet effective weakly supervised VidSGG framework, called~\proposed{}, that only utilizes the readily available video captions for training. To the best of our knowledge, we are the first to enable the training of VidSGG model with language supervision without manual annotation. 
    \item Our extensive experiments on the Action Genome dataset demonstrate the superiority of~\proposed. 
    Our proposed method is practical in that utilizing the video captions as weak supervision allows the VidSGG model to be able to predict action classes that are not included in the training data.
\end{itemize}

\vspace{-2.5ex}
\section{WS-VidSGG with Natural Language Supervision}
\label{sec:proposed_approach}
\vspace{-1.5ex}
In this section, we describe the pipeline of \proposed{} for training a VidSGG model based on natural language supervision of video captions. We start by outlining the problem formulation of WS-VidSGG (Section~\ref{sec:formulation}). Subsequently, we describe Temporality-aware Caption Segmentation (TCS) module that segments a video caption in a temporal order via an LLM (Section~\ref{sec:temporal_segment}), followed by Action Duration Variability-aware Caption-Frame Alignment (ADV) module that aligns each of the segmented sentence with appropriate frames (Section~\ref{sec:alignment}). Then, we parse the aligned segmented sentences to extract triplets, ground them within each frame, and train the VidSGG model with pseudo-localized scene graphs (Section~\ref{sec:localization}). Finally, we describe a novel pseudo-labeling strategy that leverages negative action classes using motion cues within unaligned frames (Section~\ref{sec:negative_category}). The overall framework is shown in Fig.~\ref{fig:main}.

\begin{figure*}[t]
\centering
    \centering
    \includegraphics[width=.79\columnwidth]{Figure/main_figure.pdf}
    \vspace{-1ex}
    \caption{The overall framework of \proposed{}. With an input video and its caption, (a) we employ the TCS module to segment the input video caption into sentences based on temporality. (b) In the ADV module, each segmented sentence is aligned with appropriate frames considering the variability in action duration. (c) The segmented sentences are then parsed and grounded to generate pseudo-localized scene graphs. (d) Furthermore, we assign negative classes based on the motion cues within unaligned frames. (e) Utilizing the pseudo-localized scene graphs and pseudo-labeled negative classes, we then train a VidSGG model.}
\label{fig:main}
\vspace{-3.5ex}
\end{figure*}

\vspace{-2ex}
\subsection{Problem Formulation}
\label{sec:formulation}
\vspace{-1ex}
In this work, given a video $V=\{I^1,I^2,...,I^T\}$ and its paired video caption $S$, where $I^t$ is the $t$-th frame of the video $V$ and $T$ is the number of video frames in the video $V$, our goal is to generate a scene graph $G^t=\{s_j^t,p_j^t,o_j^t\}_{j=1}^{N^t}$ for each frame $I^t$,  where $N^t$ is the number of triplets in $I^t$. Moreover, $s_{j}^{t}$$\mathlarger{\slash}$$o_{j}^t$ denote the subject$\mathlarger{\slash}$object, whose entity classes are $s_{j,c}^t$$\mathlarger{\slash}$$o_{j,c}^t\in\mathcal{C}^e$, and their bounding boxes are given by $s_{j,b}^t$$\mathlarger{\slash}$$o_{j,b}^t$. $p_j^t$ denotes the action of $s_j^t$\footnote{In the Action Genome dataset, the subject is always a person.} interacting with $o_j^t$, and its class is denoted by $p_{j,c}^t\in \mathcal{C}^a$, where $\mathcal{C}^e$ and $\mathcal{C}^a$ are predefined entity classes and action classes, respectively. 
Given a scene graph $G^t$ across all frames along the time axis (i.e., $t\in \{1,2,...T\}$), the scene graph for video $V$ is represented by $\mathcal{G}=\{G^1,G^2,...,G^T\}$.

% \smallskip
\noindent \textbf{Difference with existing fully/weakly supervised approaches.} Note that a fully supervised approach \citep{cong2021spatial,nag2023unbiased} relies on a localized video scene graph $\mathcal{G}$ for training the model. 
% which requires costly human labor. 
On the other hand, a recently proposed weakly supervised approach (i.e., PLA \citep{chen2023video}) relies on a ground-truth unlocalized scene graph of the middle frame, i.e., $G^{t'}=\{s_j^{t'},p_j^{t'},o_j^{t'}\}_{j=1}^{N^{t'}}$, where $t'$ is $T/2$ and the bounding boxes $s_{j,b}^{t'}/o_{j,b}^{t'}$ are not provided. Both fully/weakly supervised approaches require structured scene graphs that demand costly human labor, while \proposed{} only necessitates a readily available video caption $S$.

% while \proposed{} only necessitates a readily available video caption $S$.

% \noindent \textbf{Difference with an existing weakly supervised approach.} 
\vspace{-1ex}
\subsection{Temporality-aware Caption Segmentation (TCS)}
\label{sec:temporal_segment}
\vspace{-1ex}
In this module, we segment the video caption $S$ considering the temporal order of events to clearly understand the sequence of actions. To this end, we employ an LLM based on a prompt that is designed to be aware of temporality within video captions.
Our newly designed prompt is described as follows. First, the initial instruction to inform the LLM about the task at hand while enforcing it to consider the temporality of events occuring in the video caption is designed as:
\textit{Your job is to split the given video caption into multiple compositional sentences and arrange them in chronological order.}
% \textit{Your job is to split multiple compositional sentences from the given video caption and list them in chronological order.}
Moreover, video captions often require coreference resolution \citep{peng2019solving} caused by a pronoun referring to the same object over time, which may hinder an accurate extraction of triplets. To this end, we provide an additional prompt: \textit{Note that you should specify the objects for the pronouns used in each of these sentences}. 
Following the above prompts, we provide a few examples related to the task at hand (i.e., in-context few-shot learning \citep{brown2020language}) to further engage the LLM. Finally, we instruct the LLM to segment the video caption $S$ in a temporal order, which in turn yields segmented sentences $\{\hat{S}^1,\hat{S}^2,..,\hat{S}^{m}, ..., \hat{S}^{M}\}$, where $M$ is the number of segmented sentences and $M<T$. For example, in Fig.~\ref{fig:main}, \textit{Sentence 1} and \textit{Sentence 2} correspond to $\hat{S}^1$ and $\hat{S}^2$, respectively, with $M$ being 2. In summary, we design prompts tailored to the WS-VidSGG task particularly focusing on capturing the temporality within the video caption and addressing the coreference issue. Please refer to Appendix~\ref{app_sec:detail_prompt} regarding the details of the prompt, and Appendix~\ref{app_sec:exp_coreference} regarding the impact of the prompt addressing the coreference resolution.


% However, we observe that this module sometimes fails due to the ambiguity of captions, e.g., 'and' is used to connect actions. For example, the video caption 'A person is eating a sandwich and reading a book' is ambiguous; it's unclear whether the person is eating the sandwich while reading the book or after eating the sandwich. We leave this issue for future work.

%To address this ambiguity, we may use a Multimodal Large Language Model to analyze visual features alongside segmenting the captions, which could be discussed for the future work.

\vspace{-1.5ex}
\subsection{Action Duration Variability-aware Caption-Frame Alignment (ADV)}
\label{sec:alignment}
\vspace{-1ex}
Having obtained the segmented sentences $\{\hat{S}^1,\hat{S}^2,..,\hat{S}^{m}, ..., \hat{S}^{M}\}$ from the video caption $S$, we need to align each segmented caption $\hat{S}^m$ with frames that visually correspond to the scene being described therein. However, this alignment process requires careful attention to ensure effective supervision of the model, taking into account the variability in action duration, as discussed in Section~\ref{sec:intro}. In doing so, it is crucial to estimate how the visual semantic of each frame $I^1,I^2,...,I^T$ is relevant to the textual semantic of each segmented sentence $\hat{S}^m$. Hence, we employ a vision-language model\footnote{For the vision-language model, we used DAC~\citep{doveh2024dense}, a variant of CLIP \cite{radford2021learning}, due to its compositional reasoning ability. Moreover, the vision-language model can be replaced with a video-language model (Refer to Appendix~\ref{app_sec:different_moodel} for experiments that replaces DAC with InternVideo).
} that captures the joint space of visual and textual semantics. More precisely, we feed $I^1,I^2,...,I^T$ into a visual encoder $f_{vis}$ to extract visual features (i.e., $\tilde{v}_t=f_{vis}(I^t)$) and $\hat{S}^m$ into a text encoder $f_{text}$ to extract textual features (i.e., $\tilde{t}_m=f_{text}(\hat{S}^m)$), followed by $K$-Means clustering algorithm on $\{\tilde{v}_1,\tilde{v}_2,...,\tilde{v}_{T}\}$ to generate $K$ proposals with which the textual features can be aligned. 
The most straightforward approach to align $\tilde{t}_m$ with corresponding video frames would be to compute the similarity scores between $\tilde{t}_m$ and the centroids $\{c_1,c_2,...,c_{K}\}$, and select the frames near the most similar centroid. 
However, this approach cannot effectively capture the variability of action duration, especially in long action duration cases, e.g., \textit{Sentence 2} in Fig.~\ref{fig:main}. 
To this end, we propose a simple yet effective method of leveraging the variation in similarity scores. 

\vspace{-1.0ex}

\smallskip
\noindent\textbf{Clustering-based Caption-Frame Alignment. }
For a sentence with long action duration, its corresponding frames would span across multiple clusters, exhibiting relatively high similarity scores with multiple cluster centroids. On the other hand, a sentence with short action duration would represent a confidently sharpened similarity score mainly focusing on a single cluster. Hence, for the representation $\hat{t}^m$ of each segmented sentence $\hat{S}^m$, we arrange its similarity scores with $K$ centroids in descending order, and determine the point where the scores show the steepest decline.
Note that this steepest decline point will determine the varying durations of actions by isolating the relevant multiple clusters, regardless of how long the action lasts.
Then, we choose all the clusters preceding this point, and align the frames in those clusters with $\hat{t}^m$. For example, for \textit{Sentence 2} in Fig.~\ref{fig:main}, we sort the cluster based on the similarity scores (i.e., $c_4$, $c_3$, $c_1$, $c_2$,...) and determine the point where the score shows a steepest decline (i.e., $c_1 \to c_2$). Then, we choose frames in clusters $c_4, c_3$, and $c_1$ to align with \textit{Sentence 2}. In this manner, we can adaptively choose clusters according to the variability of action duration. Regarding the $K$, our aim is to adapt the number of clusters depending on the length of the video. Thus, we set $K$ as $\frac{|V|}{\beta}$, where $\beta$ is a hyperparameter. 

\vspace{-1.0ex}

\smallskip
\noindent\textbf{Removing Unrealistic Caption-Frame Alignments. }
It is worth noting that we remove unrealistic alignments that violates the temporal order. 
% For example, if $\hat{S}^1$ is aligned with $I^2$ while $\hat{S}^2$ being aligned with $\{I^1,I^3,I^4,I^5,I^6\}$, we remove $I^1$ from $\hat{S}^2$ as $I^2$ is already aligned with $\hat{S}^1$ 
For example, if $\hat{S}^1$ is aligned with $I^2$, and $\hat{S}^2$ is aligned with ${I^1, I^3, I^4, I^5, I^6}$, we remove $I^1$ from $\hat{S}^2$ since $I^2$ is already aligned with $\hat{S}^1$, which precedes $\hat{S}^2$
(See Fig.~\ref{fig:main}).
Finally, each segmented sentence $\hat{S}^m$ is aligned with a sequence of consecutive frames within $V=\{I^1,I^2,...,I^T\}$, i.e., $[I]^{\hat{S}^m}$.

\vspace{-1.0ex}
\subsection{Generation of Pseudo-Localized Scene Graphs}
\label{sec:localization}
\vspace{-1ex}
Based on the $\{ (\hat{S}^m,[I]^{\hat{S}^m})\}_{m=1}^{M}$ obtained from ADV, we  construct pseudo-localized scene graphs for training the model, which can be described as follows:
% \vspace{-1.5ex}

% \smallskip
\noindent\textbf{Scene graph parsing.} We begin by converting each segmented sentence $\hat{S}^m$ into a triplet $<$$s_m,p_m,o_m$$>$\footnote{Although each segmented caption can be converted into more than one triplet, we assume here that it is converted to only one triplet for simplicity of explanation.} using either a rule-based parser \citep{schuster2015generating} or an LLM-based parser \citep{kim2024llm4sgg}. Note that the bounding boxes $s_{m,b}$, $o_{m,b}$ are not defined, and the classes $s_{m,c},o_{m,c}$, $p_{m,c}$ are not necessarily included in $\mathcal{C}^e$ and $\mathcal{C}^a$. To ensure $s_{m,c}$, $o_{m,c}\in\mathcal{C}^e$ and $p_{m,c}\in\mathcal{C}^a$, we map them to the classes of our interest (i.e., entity$\mathlarger{\slash}$action classes in the Action Genome dataset) by either using synsetsâ€™ lemmas and hypernyms in WordNet \citep{miller1995wordnet} or LLM-based alignment \citep{kim2024llm4sgg}. In this process, we obtain pseudo-unlocalized scene graphs. Note that we can also omit the class mapping process to let the model be able to predict a broader range of action classes that are not included in the entity$\mathlarger{\slash}$action classes of Action Genome, as will be shown in Section~\ref{sec:expansion_action}.
\vspace{-1.0ex}

\smallskip
\noindent\textbf{Scene graph grounding.} To define the bounding boxes $s_{m,b}$ and $o_{m,b}$, we follow a prior study \citep{chen2023video} that leverages the information of a pretrained object detector. Specifically, we ground $s_{m}$ to a detected bounding box whose entity class corresponds to $s_{m,c}$, while $o_m$ is grounded in a similar manner. After grounding $s_m$ and $o_m$, we assign $p_m$ between $s_m$ and $o_m$.

\smallskip
\noindent The above processes of scene graph parsing and scene graph grounding are adopted to each pair in $\{ (\hat{S}^m,[I]^{\hat{S}^m})\}_{m=1}^{M}$, where the scene graph grounding process is applied across aligned frames $[I]^{\hat{S}^m}$, after which pseudo-localized video scene graphs $\mathcal{G}$ can be obtained.

\vspace{-1.5ex}
\subsection{Optional: Dealing with Negative Action Classes in Action Genome}
\label{sec:negative_category}
\vspace{-1ex}
\begin{wrapfigure}[8]{r}{.397\columnwidth}
\vspace{-3.0ex}
\begin{center}
    \includegraphics[width=.397\columnwidth]{Figure/motion_figure.pdf}
\end{center}
    \vspace{-4.0ex}
    \caption{Example of motion cue.}
    \label{fig:motion}
    \vspace{-3.0ex}
\end{wrapfigure}

It is important to note that while the Action Genome dataset contains negative action classes (e.g., \textsf{not looking at} and \textsf{not contacting}), video captions usually do not contain negations\footnote{
Only 0.09\% (11$\mathlarger{\slash}$11,593) among all captions in Action Genome contain negation.}. For this reason, a VidSGG model trained in a weakly supervised manner based on video captions as in our study would fail to generate a scene graph containing negative action classes in the Action Genome dataset. To address this issue, we propose a novel Pseudo-Labeling strategy based on Motion cues (PLM) that assigns negative action classes between subject-object pairs. 
The core idea is to make use of the fact that a person usually does not look at (or contact) an object when moving apart from it.
Specifically, we adopt the generalized IoU \citep{rezatofighi2019generalized,gao2023compositional} (GIoU) as a metric to measure how close two objects are from each other.
% to determine whether two objects are moving apart from each other.
% To this end, we utilize common knowledge that the person usually does not look at the object when the person is moving apart from that object. 
% We transfer this knowledge into motion cues based on the spatio-temporal motion pattern. 
% Specifically, inspired by \citep{gao2023compositional}, we adopt the generalized IoU \citep{rezatofighi2019generalized} (GIoU) as the metric to discern the motion pattern. 
Note that for a given subject and an object, a small GIoU in the end frame (i.e., $\mathbf{G}_e$) and a large GIoU in the start frame (i.e., $\mathbf{G}_s$) indicates that they are getting farther from each other over time. 
For example, as a person is moving farther from a sofa from the 3rd frame to the last frame, the GIoU gets smaller (See Figure~\ref{fig:motion}).
% as shown in Figure~\ref{fig:motion}, the person is moving farther from the sofa, showing a small GIoU (i.e., $-0.5$) in the last frame and a relatively large GIoU (i.e., $0.3$) in the third frame. 
In other words, as $\mathbf{G}_e-\mathbf{G}_s$ gets smaller, the subject and the object are getting farther from each other over time.
% the farther the subject and object are moving apart over time. 
With this motion cue, we assign negative action classes, i.e., $\textsf{not looking at}$ and $\textsf{not contacting}$.

\looseness=-1
However, as considering all the frames in a video for pseudo-labeling of negative action classes is costly, we utilize frames that are not aligned with any of the segmented sentences for pseudo-labeling negative action classes. 
More precisely, for all the unaligned frames across the entire videos given in the dataset, 
we compute $\mathbf{G}_e-\mathbf{G}_s$ between the same subject-object pair. Then, we sort the computed GIoUs in the ascending order, and assign pseudo-labels (i.e., $\textsf{not looking at}$ and $\textsf{not contacting}$) to top-$\alpha\%$ aiming at assigning labels to more confident subject-object pairs. 
More precisely, we restrict the assignment of \textsf{not looking at} to the start and end frames within unaligned frames, and \textsf{not contacting} only to the end frames. Please refer to Appendix~\ref{app_sec:selection} regarding the different selection strategies for pseudo-label assignment. 
Since considering all subject-object pairs included in the training data is not only time-consuming but also prone to noise, we only select objects appearing in the pseudo-localized video scene graph $\mathcal{G}$ obtained from the aligned frames in each video.

It is important to note that the proposed PLM is an optional module that can be only applied when the dataset contains negative classes as in the Action Genome dataset. While we show its effectiveness on the original Action Genome dataset with negative classes, we also provide results for datasets without negative classes, namely the AG dataset without negative classes See Table~\ref{app_tab:ablation_without_negative} in Appendix~\ref{app_sec:ablation_study_wo_PLM}) and VidHOI dataset~\citep{chiou2021st} (See Table~\ref{app_tab:different_dataset} in Appendix~\ref{app_sec:vidhoi}).

\noindent\textbf{Model Training.} Lastly, having obtained the pseudo-localized video scene graph $\mathcal{G}$ and pseudo-labeled negative classes within unaligned frames, we then follow the training protocol of existing VidSGG models, i.e., STTran \citep{cong2021spatial} and DSG-DETR \citep{feng2023exploiting}.

\vspace{-2.3ex}
\section{Experiment}
\label{sec:experiment}
\vspace{-1.2ex}
\subsection{Experimental Setting}
\vspace{-1.3ex}
\noindent \textbf{Datasets. } 
To \textbf{train} a VidSGG model without ground-truth localized video scene graphs\footnote{The ground-truth localized video scene graphs are utilized for model training in a fully-supervised approach while we do not use them for weakly supervised approach.}, we use three video caption datasets: the Action Genome \citep{ji2020action} (AG) caption, the MSVD \citep{chen2011collecting} caption, and the ActivityNet caption \citep{krishna2017dense} datasets. For the AG caption dataset, we use 7,454 videos consisting of 166,785 frames, following previous studies \citep{chen2023video,cong2021spatial,ji2020action}. The duration of each video is on average 29.9 seconds. To show a more practical setting, which is to use external caption datasets, other than a benchmark VidSGG dataset containing video captions (i.e., AG dataset), as weak supervision, we use the MSVD and ActivityNet caption datasets, each of which contains 1,970 videos consisting of 40,863 frames with 2 FPS and 4,640 videos consisting of 569,836 frames with 1 FPS, respectively. The duration of each video for the MSVD and ActivityNet datasets is on average 9.5 and 117.3 seconds, respectively. Note that we mainly use the AG caption dataset for analysis throughout this paper while the MSVD and ActivityNet caption datasets are only used in Section~\ref{sec:video_text_pair_exp} and~\ref{sec:longer_video}. To \textbf{evaluate} our proposed \proposed{} framework, we use the AG dataset containing ground-truth localized video scene graphs with 36 object classes (i.e., $\mathcal{C}^e$) and 25 action classes (i.e., $\mathcal{C}^a$), whose categories are divided into three types, i.e., 3 attention classes, 6 spatial classes, and 16 contacting classes. Following previous studies \citep{chen2023video,cong2021spatial}, we use 1,747 videos with 54,429 frames. Furthermore, to validate the generalization on other datasets, we train and evaluate our proposed framework using the VidHOI \citep{chiou2021st} dataset, which is detailed in Appendix~\ref{app_sec:vidhoi}.

% To evaluate our proposed~\proposed{} framework, we use the Action Genome (AG) dataset, which is a benchmark dataset for the VidSGG task. 
% % we use 1,747 videos with 54,429 frames for evaluation among 9,201 videos. 
% Following previous studies \citep{chen2023video,cong2021spatial,ji2020action}, we used 7,454 videos consisting of 166,785 frames for training, and 1,747 videos with 54,429 frames for model evaluation.
% Each video contains ground-truth localized video scene graphs with 36 object classes (i.e., $\mathcal{C}^e$) and 25 action classes (i.e., $\mathcal{C}^a$), whose categories are divided into three types, i.e., 3 attention classes, 6 spatial classes, and 16 contacting classes. 
% To train a VidSGG model, we use two video caption datasets: the AG caption and the MSVD \citep{chen2011collecting} caption dataset. 
% When training the model on the AG dataset, it is worth noting that we only utilize its video captions, while fully supervised approaches use ground-truth localized video scene graphs. The duration of each video within the AG dataset is on average 29.9 seconds. 
% To evaluate the performance of our framework when using external caption datasets, other than a benchmark VidSGG dataset containing video captions (i.e., AG dataset), as weak supervision, we use the MSVD caption dataset, which is another video-text paired dataset containing 1,970 videos consisting of 40,863 frames with 2 FPS. Note that in this case, the model is trained on the MSVD dataset, and evaluated on the AG dataset.
% The duration of each video is on average 9.5 seconds. 
% {Furthermore, we train and evaluate our proposed framework using the VidHOI \citep{chiou2021st} dataset, which is detailed in Appendix~\ref{app_sec:vidhoi}.}


\smallskip
\noindent \textbf{Evaluation Metrics.}
\label{sec:evaluation_protocol}
We use the widely adopted Recall@K (R@K) with {``With Constraint''} and {``No Constraint''} strategies, following previous studies \citep{cong2021spatial,chen2023video,feng2023exploiting}. {``With Constraint''} allows only one action prediction with the highest score between subject and object, while {``No Constraint''} reflects the multi-label prediction capability in evaluation metric as the AG dataset follows a multi-label task. For example, if multiple high action scores are associated with a single subject-object pair, all of them are allowed in the top-K. Regarding the evaluation task, we adopt the Scene Graph Generation (SGDet) task, which is commonly used for WS-ImgSGG \citep{kim2024llm4sgg,zhong2021learning,zhang2023learning,ye2021linguistic}. In this task, the ground-truth (GT) bounding boxes and entity class information are not provided during evaluation, and the predicted triplet is considered valid only if the predicted bounding box overlaps with the GT bounding box with an IoU$>$0.5.

\vspace{-0.5ex}

\smallskip
\noindent \textbf{Baselines. }
\label{sec:baseline}
To evaluate our proposed framework, we compare it with models each of which corresponds to one of the four types of supervision:
% \textbf{Zero-shot supervision}
% \textbf{Full supervision: }
% \textbf{Weak supervision of Unlocalized scene graph (SG): }
% \textbf{Natural language supervision}, i.e., utilization of video caption.
\textbf{Zero-shot} supervision\footnote{We use the term \textsf{zero-shot setting} interchangeably with \textsf{zero-shot supervision.}} (No supervision), \textbf{Full} supervision, \textbf{Weak} supervision of \textbf{GT Unlocalized scene graph} (SG), and \textbf{Weaker} supervision of \textbf{Natural Language}, i.e., utilization of readily available video captions. Specifically, zero-shot supervision indicates that without any training process, each frame is considered as a static image and inferred with ImgSGG models equipped with strong zero-shot predictive ability for relations. For this purpose, we employ RLIP \citep{yuan2022rlip} and RLIPv2 \citep{yuan2023rlipv2}. Full supervision, which we consider as the upper bound of our model, involves training the model with ground-truth localized video scene graphs, and we leverage STTran \citep{cong2021spatial} and DSG-DETR \citep{feng2023exploiting} for VidSGG models. Weak supervision of GT unlocalized SG is to utilize a GT unlocalized scene graph in the middle frame, which is proposed by PLA \citep{chen2023video}. Moreover, we also compare with $\text{PLA}_{simp.}$, a simplified version of PLA trained using only unlocalized SG from the middle frame without any component of PLA. This baseline is included to demonstrate the heavy reliance of PLA on the annotated GT unlocalized scene graph, which is costly. Finally, the natural language supervision is based on the readily available video captions.  It includes two approaches: straightforward approach of the WS-ImgSGG method discussed in Section~\ref{sec:intro} (i.e., + WS-ImgSGG) and our proposed framework (i.e., + \proposed{}).
{In addition to those approaches, we add a new baseline adapted from PLA (i.e, + $\text{PLA}_{cap}$), where the GT unlocalized scene graph of the middle frame are replaced with pseudo-unlocalized scene graphs obtained from video captions.}
It is important to note that video captions provides a \textit{weaker} supervision signal to the VidSGG model compared with the GT unlocalized SG (hence we name it ``Weaker'').

% Note that the weak supervision of GT unlocalized SG provides a stronger supervision signal to a VidSGG model compared with the \textit{weaker} supervision of natural language. 


\vspace{-0.5ex}
\smallskip
\noindent \textbf{Implementation Details. }
For the pretrained object detector, we follow PLA \citep{chen2023video}, which employs VinVL \citep{zhang2021vinvl} with backbone ResNeXt-152 C4. This object detector only leaves bounding boxes for objects with a confidence score of 0.2 or higher. In the TCS module, we use \textit{gpt-3.5-turbo} in ChatGPT \citep{open2022chatgpt} for an LLM. In the ADV module, DAC \citep{doveh2024dense} is adopted for a vision-language model. Please refer to Appendix~ \ref{app_sec:different_moodel} regarding the experiment with an open-source smaller language model \citep{jiang2023mistral} and another vision-language model \citep{wang2022internvideo}. 
Additionally, $\beta$ used to determine the number of clusters $K$ is set to 4, and $\alpha$ used in the pseudo-labeling strategy is set to 15\%. Please refer to Appendix~\ref{app_sec:hyperparameter} regarding the sensitivity of hyperparameter $\beta$ and $\alpha$. Regarding the triplet extraction in Section~\ref{sec:localization}, we adopt an LLM-based approach \citep{kim2024llm4sgg}. Please refer to Appendix~\ref{app_sec:tripet_extraction} regarding the result of different triplet extraction processes. For the experimental environment, we implemented \proposed{} on both an NVIDIA GeForce A6000 48GB and an Intel Gaudi-v2.



\vspace{-1.6ex}
\subsection{Quantitative Results}
\vspace{-1.2ex}
Table~\ref{tab:main} shows the performance comparisons across four types of supervision, utilizing STTran \citep{cong2021spatial} and DSG-DETR \citep{feng2023exploiting} as backbones for full and weak/weaker supervision. We have the following observations: \textbf{1)} Models with zero-shot predictive ability for relation, such as RLIP \citep{yuan2022rlip} and RLIPv2 \citep{yuan2023rlipv2}, show inferior performance, especially in With Constraint setting. 
This suggests that they struggle to predict key relationships between subject-object pairs, highlighting the limitation of simply applying ImgSGG models to the VidSGG task, as these models fail to account for dynamic relationships.
% This indicates that they struggle to predict the important relationships between subject-object pairs, demonstrating the limitation of simply applying ImgSGG models to the VidSGG task due to their inability to predict dynamic relationships. 
% \begin{table}[t]
Please refer to Appendix~\ref{app_sec:rlip} for experiments regarding the incorporation of dynamic relationships into the RLIP. 
\begin{wraptable}{r}{.7\columnwidth}
\centering
\vspace{-3ex}
\caption{Results of four types of supervision on the AG dataset.
% : Zero-shot, Full supervision, Weak supervision of GT Unlocalized scene graph (SG), and Natural Language.
}
\resizebox{.7\textwidth}{!}{
\begin{tabular}{c|l|c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Backbone}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Supervision}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c}{\textbf{  No Constraint 
 }} \\ \cline{4-7} 
                                   & \multicolumn{1}{c|}{}                                 &                                       & \hspace{1.5ex}\textbf{R@20}         & \textbf{R@50}         & \hspace{1.5ex}\textbf{R@20}        & \textbf{R@50}       \\ \midrule \midrule
RLIP                               & \multicolumn{1}{c|}{\multirow{2}{*}{ImgSGG}}          & \multirow{2}{*}{Zero-shot}            & \hspace{1.5ex}7.93                  & 9.16                  & \hspace{1.5ex}9.70                 & 13.80               \\
RLIPv2                             & \multicolumn{1}{c|}{}                                 &                                       & \hspace{1.5ex}8.37                  & 10.05                 & \hspace{1.5ex}14.60                & 21.42               \\ \midrule
\multirow{7}{*}{STTran}            & \multicolumn{1}{c|}{Vanilla}                          & Full                                  & \hspace{1.5ex}33.98                 & 36.93                 & \hspace{1.5ex}36.20                & 48.88               \\ \cmidrule{2-7} 
                                   & \;\textbf{+}PLA                                                & \multirow{2}{*}{Weak (GT Unlocalized SG)}   & \hspace{1.5ex}20.94                 & 25.79                 & \hspace{1.5ex}22.34                & 31.69               \\
                                   & \;\textbf{+}$\text{PLA}_{simp.}$                               &                                       & \hspace{1.5ex}20.42                 & 25.43                 & \hspace{1.5ex}21.72                & 30.87               \\ \cmidrule{2-7} 
                                   & \;\textbf{+}WS-ImgSGG                                          &     & \hspace{1.5ex}10.01                 & 12.83                 & \hspace{1.5ex}9.02                 & 14.05               \\ 
                                                                   & \;\textbf{+}$\text{PLA}_{cap}$                                        &     & \hspace{1.5ex}10.40                 & 13.26                 & \hspace{1.5ex}10.64                 & 15.13               \\ 
                                \rowcolor{gainsboro} \cellcolor{white}
                                   & \;\textbf{+}\proposed                                          &    \multirow{-3}{*}{Weaker (Natural Language)} \cellcolor{white}                                   & \hspace{1.5ex}\textbf{15.61}                 & \textbf{19.60}                 & \hspace{1.5ex}\textbf{15.92}                & \textbf{22.56}               \\ \midrule
\multirow{7}{*}{DSG-DETR}          & \multicolumn{1}{c|}{Vanilla}                          & Full                                  & \hspace{1.5ex}34.80                 & 36.10                 & \hspace{1.5ex}40.90                & 48.30               \\ \cmidrule{2-7} 
                                   & \;\textbf{+}PLA                                                & \multirow{2}{*}{Weak (GT Unlocalized SG)}   & \hspace{1.5ex}21.30                 & 25.90                 & \hspace{1.5ex}22.70                & 31.90               \\
                                   & \;\textbf{+}$\text{PLA}_{simp.}$                               &                                      &  \hspace{1.5ex}20.78                     &  25.79                      &  \hspace{1.5ex}22.31                    & 31.69                    \\ \cmidrule{2-7} 
                                   & \;\textbf{+}WS-ImgSGG                                          &     &  \hspace{1.5ex}10.05                      &      12.96                 &               \hspace{1.5ex}10.29       & 14.77                    \\ 
                                                                      & \;\textbf{+}$\text{PLA}_{cap}$                                          &     & \hspace{1.5ex}10.36                 & 13.53                 & \hspace{1.5ex}10.57                 & 15.41               \\ 
                                   
                                   \rowcolor{gainsboro} \cellcolor{white}
                                   & \;\textbf{+}\proposed                                          &     \multirow{-3}{*}{Weaker (Natural Language)}       \cellcolor{white}                           &  \hspace{1.5ex} \textbf{15.75}                 & \textbf{20.40}                 & \hspace{1.5ex}\textbf{16.11}                & \textbf{23.21}               \\ \bottomrule
\end{tabular}
}
\vspace{-2ex}
\label{tab:main}
\end{wraptable}
\textbf{2)} $\text{PLA}_{simp.}$, which solely relies on a GT unlocalized SG of the middle frame without any components proposed in PLA, shows competitive performance with the vanilla PLA \citep{chen2023video}, while significantly surpassing the performance of RLIP/RLIPv2, which are trained on a large number of images with complex models.
This suggests that incorporating even a few ground-truth unlocalized video scene graphs is essential for strong performance in the VidSGG task, while also revealing PLA's heavy reliance on annotated GT unlocalized scene graphs, which are expensive to obtain.
{This is further verified by the significant performance drop when the GT unlocalized scene graph of the middle frame is replaced with a pseudo-unlocalized scene graph obtained from the video caption (PLA vs. $\text{PLA}_{cap}$).}
% \textcolor{blue}{Despite the relatively high performance of PLA and $\text{PLA}_{simp.}$,(X)} 
{Moreover,} we argue the assumption of a GT scene graph existing in the middle frame among other frames is not only unrealistic but also still requires manual human annotation, which makes these methods impractical in reality.
\textbf{3) } 
Given the natural language supervision from video captions, which further relieves the annotation cost of PLA, \proposed{} exhibits notably superior performance compared with a simple adoption of the WS-ImgSGG pipeline to VidSGG {and $\text{PLA}_{cap}$, both of which disregard the two key factors discussed in Section~\ref{sec:intro}, i.e., temporality within video captions and variability in action duration}.


\begin{wraptable}{r}{.6\columnwidth}
\vspace{-6ex}
\caption{Ablation studies on each module of~\proposed{}.}
\centering
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{c|ccc|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Row}} & \multirow{2}{*}{\textbf{TCS}} & \multirow{2}{*}{\textbf{ADV}} & \multirow{2}{*}{\textbf{PLM}} & \multicolumn{2}{c|}{\textbf{  With Constraint  }} & \multicolumn{2}{c|}{\textbf{  No Constraint   }} & \multicolumn{2}{c}{\textbf{ 
 Mean  }} \\
                              &                               &                               &                                              & 
\hspace{1.5ex} \textbf{R@20}         & \textbf{R@50}         & \hspace{1.5ex}\textbf{R@20}        & \textbf{R@50}        & \hspace{1.0ex}\textbf{R@20}     & \textbf{R@50}    \\ \midrule
(a)                           &                               &                               &                                              & \hspace{1.5ex}10.01                 & 12.83                 & \hspace{1.5ex}9.02                 & 14.05                & \hspace{1.0ex}9.52              & 13.44            \\
(b)                           & \checkmark                    &                               &                                              & \hspace{1.5ex}11.09                 & 14.66                 & \hspace{1.5ex}11.34                & 16.70                & \hspace{1.0ex}11.22             & 15.68            \\
(c)                           & \checkmark                    & \checkmark                    &                                              & \hspace{1.5ex}11.98                 & 15.58                 & \hspace{1.5ex}11.93                & 17.36                & \hspace{1.0ex}11.96             & 16.47            \\
(d)                           & \checkmark                    & \checkmark                    & \checkmark                                   & \hspace{1.5ex}\textbf{15.61}        & \textbf{19.60}        & \hspace{1.5ex}\textbf{15.92}       & \textbf{22.56}       & \hspace{1.0ex}\textbf{15.77}    & \textbf{21.08}   \\ \bottomrule
\end{tabular}
}
\label{tab:ablation}
% \vspace{-1ex}
\end{wraptable}

\vspace{-1.5ex}
\subsection{Ablation Studies}
\label{sec:ablation_study}
\vspace{-1.5ex}
\looseness=-1
In Table~\ref{tab:ablation}, we conduct ablation studies on the AG dataset to verify the effectiveness of each module in~\proposed{}. For the ablation studies, we use STTran \citep{cong2021spatial} as the backbone. The variant of not using any module (row (a)) corresponds to a simple approach of adopting the WS-ImgSGG pipeline to the VidSGG task. We have the following observations: 
\textbf{1. Effect of TCS:} We observe that the adoption of the TCS module to capture the temporality within the video captions enhances the performance compared with a simple approach (row (a) vs. (b)). Note that we apply the TCS module to the simple approach by considering each triplet as a segmented sentence.
The performance enhancement demonstrates the effectiveness of the TCS module in capturing the temporality, leading to accurate supervision for the model.
\textbf{2. Effect of ADV:} We observe that the incorporation of the ADV module responsible for capturing the variability in action duration further enhances the performance (row (b) vs. row (c)), which demonstrates the importance of considering the variability of human actions. {Regarding a qualitative analysis of the ADV module, please refer to Appendix~\ref{app_sec:adv}.} \textbf{3. Effect of PLM:} Our proposed pseudo-labeling strategy with motion cues significantly improves the performance (row (c) vs. (d)), demonstrating the effectiveness of pseudo-labeling negative action classes. 
We recognize that the PLM module contributes the most to the the final performance, since the negative action classes, i.e., \textsf{not looking at} and \textsf{not contacting}, belong to head predicate classes, accounting for {16.5\% and 8.7\%} of the predicates in the test set, respectively. 
Recall that PLM is an optional module that can be only applied when the dataset contains negative classes. Thus, to more precisely validate the effectiveness of the TCS and ADV modules, we intentionally removed negative classes from both training and test sets in the AG dataset, and show the results in Appendix~\ref{app_sec:ablation_study_wo_PLM}.
% \textcolor{red}{Please refer to Appendix~\ref{app_sec:ablation_study_wo_PLM} for the ablation studies where the PLM structure is not required.
% Despite its signficant contribution, we observe that the ADV and TCS modules still show their effectiveness evn when the PLM module is not required, which is shown in Appendix~. 

\vspace{-1.0ex}
\subsection{Exploring the Potential of utilizing external Video-Text Dataset}
\label{sec:video_text_pair_exp}
\vspace{-1ex}
\looseness=-1

\begin{wraptable}[6]{r}{.402\columnwidth}
    \vspace{-5ex}
    \centering
    \caption{Performance when an external video-text dataset is utilized for training.}
    \resizebox{.402\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\makecell{\textbf{Training Dataset}\\(Tested on the AG)}}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c}{\textbf{  No Constraint  }} \\ \cline{2-5} 
\multicolumn{1}{c|}{}                                  & \hspace{1.5ex}\textbf{R@20}         & \textbf{R@50}         & \hspace{1.5ex}\textbf{R@20}        & \textbf{R@50}       \\ \midrule
AG                                             & \hspace{1.5ex}15.61                 & 19.60                 & \hspace{1.5ex}15.92                & 22.56               \\
MSVD                                           & \hspace{1.5ex}9.05                  & 11.31                 & \hspace{1.5ex}10.22                & 16.60               \\
AG$\textbf{+}$MSVD                                        & \hspace{1.5ex}\textbf{15.71}        & \textbf{20.00}        & \hspace{1.5ex}\textbf{16.07}       & \textbf{23.21}      \\ \bottomrule
\end{tabular}
    }
    \vspace{-5ex}
    \label{tab:video_text_exp}  
\end{wraptable}

Note that our proposed framework is not only limited to a benchmark dataset, i.e., Action Genome \citep{ji2020action} (AG). In this section, we explore the potential of utilizing an external video-text paired dataset, i.e., MSVD \citep{chen2011collecting}, for training. 
For this experiment, we use STTran as the backbone. 
In Table~\ref{tab:video_text_exp}, we observe that the performance based on sole utilization of the MSVD caption dataset for training shows inferior performance compared to that of AG caption.




This is expected since the video domains of these datasets are very different (i.e., indoor scenes in AG and outdoor scenes in MSVD). However, we observe that the combination of AG and MSVD caption datasets performs the best, implying that collecting a large pool of video-text pair regardless of the domains is helpful. In this regard, we believe this study paves the way of enhancing the inherent limited VidSGG datasets.


\begin{wraptable}[8]{r}{.582\columnwidth}
    \vspace{-5ex}
    \centering
    \caption{Performance over various video length. We use backbone as STTran.}
    \resizebox{.582\textwidth}{!}{
\centering
\begin{tabular}{c|l|c|cccc|c}
\toprule
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training Dataset\\ (Caption)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Avg. Video Length}} & \multicolumn{2}{c}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{4-7}
                                           & \multicolumn{1}{c|}{}                                 &                                             & \textbf{R@20}         & \textbf{R@50}        & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule
\multirow{2}{*}{Action Genome}             & WS-ImgSGG                                             & \multirow{2}{*}{29.9 seconds}                   & 10.01                 & 12.83                & 9.02                 & 14.05                & 11.48                          \\
                                           & \proposed                                                 &                                             & \textbf{15.61}        & \textbf{19.60}       & \textbf{15.92}       & \textbf{22.56}       & \textbf{18.42}                 \\ \midrule
\multirow{2}{*}{MSVD}                      & WS-ImgSGG                                             & \multirow{2}{*}{9.5 seconds}                    & 6.22                  & 8.03                 & 7.69                 & 12.31                & 8.56                           \\
                                           & \proposed                                              &                                             & \textbf{9.05}         & \textbf{11.31}       & \textbf{10.22}       & \textbf{16.60}       & \textbf{11.80}                 \\ \midrule
\multirow{2}{*}{ActivityNet}               & WS-ImgSGG                                             & \multirow{2}{*}{117.3 seconds}                  & 10.86                 & 14.47                & 10.07                & 15.80                & 12.80                          \\
                                           & \proposed                                                 &                                             & \textbf{13.46}        & \textbf{17.58}       & \textbf{13.94}       & \textbf{21.41}       & \textbf{16.60}                 \\ \bottomrule
\end{tabular}
    }
\label{app_tab:longer_video}
\end{wraptable}

\vspace{-2ex}
\subsection{Analysis of performance on longer video dataset}
\label{sec:longer_video}
\vspace{-1.5ex}
To explore the impact of video length, we conducted experiments using the ActivityNet \citep{krishna2017dense} caption dataset (average length: 117.3 seconds), which is approximately 4 times longer than the Action Genome caption dataset (average length: 29.9 seconds) and 12 times longer than the MSVD dataset (average length: 9.5 seconds). As shown in Table~\ref{app_tab:longer_video}, we made the following two observations: 1) Regardless of video length, our proposed method consistently outperformed the naive approach (i.e., WS-ImgSGG). This indicates that our proposed method remains effective across videos of various lengths. 2) When comparing the performance between the MSVD and ActivityNet datasets, aside from the benchmark dataset (i.e., Action Genome), we observed that both WS-ImgSGG and \proposed{} achieved better performance on the longer ActivityNet dataset compared to the shorter MSVD dataset. We attribute it to the fact that longer videos allow the model to learn more diverse video content, thereby improving generalization, and provide more supervision as the duration of actions increases. In this context, despite the shorter video length of the Action Genome dataset compared to the ActivityNet dataset, our proposed method performs better on the Action Genome dataset. This is because the video distribution in the Action Genome dataset is more closely aligned with the test set, which is derived from Action Genome.




\vspace{-2ex}
\subsection{Qualitative Results for Expansion of Action Classes}
\label{sec:expansion_action}
\vspace{-1.5ex}
\begin{wrapfigure}{r}{.7\columnwidth}
% \begin{figure*}[t]
\centering
    \vspace{-2.5ex}
    \centering
    \includegraphics[width=.7\columnwidth]{Figure/open_set.pdf}
    \vspace{-4.5ex}
    \caption{Qualitative results of \proposed{} for broader range of action classes. 
    % The red-colored texts indicate predicates that are not included in the AG dataset.
    The red-colored texts indicate predicates with novel meanings that are not present in the AG dataset.
    }
\label{fig:open_set}
\vspace{-2.5ex}
\end{wrapfigure}
Recall that we can allow a VidSGG model trained by \proposed{} to be able to predict a broader range of action classes that are not included in the training data (i.e., AG dataset), as our framework utilizes the video captions as weak supervision. To validate this, we perform qualitative analysis with STTran backbone on the AG test set. 
Fig.~\ref{fig:open_set} shows qualitative results for scene graphs generated under the following two conditions: an VidSGG model trained on 1) constrained action classes\footnote{
% We utilize a fully supervised STTran model trained on 36 action classes.
Among the three types of action classes (i.e., attention, spatial and contacting), we generate scene graphs with the contacting class type, which conveys more descriptive information than the other two types.} that are obtained through the class mapping process described in Section~\ref{sec:localization}, and 2) expanded action classes (i.e., 500 classes in total\footnote{To filter out noisy classes, we select the top 500 most frequent action classes from 2,325 action classes. Refer to Appendix~\ref{app_sec:dist_of_large_voca} regarding for more information on 500 action classes. }) that are obtained without the class mapping process.
In Fig.~\ref{fig:open_set}(a), we observe that the expansion of action classes enhances the temporal coherence of video scene graphs in that in the 3rd frame, the prediction of \textsf{walking into}/\textsf{open} under expanded action classes carries a stronger temporal implication compared to the prediction of \textsf{standing on}/\textsf{holding} under constrained classes. Furthermore, we observe that in Fig.~\ref{fig:open_set}(b), the expansion of action classes allows for predicting \textsf{close}, which presents the opposite meaning to \textsf{open} in Fig.~\ref{fig:open_set}(a), while the VidSGG model trained on the constrained classes still generate \textsf{holding}. This indicates that the expansion of action classes with temporal implication significantly benefits VidSGG models, which are capable of capturing temporal context,  thereby generating video scene graphs with enhanced temporal coherence. In this vein, we believe that our proposed framework facilitates the training of VidSGG models with temporal coherence. 




% \begin{wraptable}{r}{.36\columnwidth}
%     \vspace{-4.8ex}
%     \centering
% \caption{Temporal Consistency (\%)}
% \resizebox{.36\textwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% \multicolumn{1}{c|}{\textbf{Method}} & \textbf{Recall} & \textbf{Precision} & \textbf{F1 Score} \\ \midrule \midrule
% \multicolumn{4}{c}{\textbf{Unlimited Configuration}}                                              \\ \midrule
% \multicolumn{1}{l|}{WS-ImgSGG}       &  3.96            & 36.53              & 7.18             \\
% \multicolumn{1}{l|}{VSNLS}           & \textbf{5.12}  & \textbf{50.66}     & \textbf{9.30}    \\ \midrule
% \multicolumn{4}{c}{\textbf{Limited Configuration}}                                            \\ \midrule
% \multicolumn{1}{l|}{WS-ImgSGG}       & 14.97           & 36.53              & 21.24             \\
% \multicolumn{1}{l|}{VSNLS}           & \textbf{20.42}  & \textbf{50.66}     & \textbf{29.10}    \\ \bottomrule
% \end{tabular}
%     }
% \vspace{-2ex}
% \label{tab:temporal_consistency}
% \end{wraptable}

% % \vspace{-1.7ex}
% % \section{Evaluation for temporal consistency of action labels}
% % \vspace{-1.7ex}
% % \textcolor{red}{
% % To measure how action labels inherent in video captions are accurately aligned with corresponding frames, we evaluate the temporal consistency by comparing the action labels extracted from video captions with the ground-truth action labels. Specifically, we regard the ground-truth action labels in each frame as actual values and the action labels extracted from the video captions and assigned to the frames as predictive values. We then compute Recall, Precision, and F1 score. Here, we consider two configurations depending on whether to restrict ground-truth action labels to only those found in each video caption (\textbf{Limited Configuration}) or to use all available ground-truth action labels (\textbf{Unlimited Configuration}). In other words, the Limited Configuration is aimed at directly evaluating temporal consistency, while the Unlimited Configuration offers a broader evaluation approach. As shown in Table~\ref{tab:temporal_consistency}, we find that our proposed method significantly increases the temporal consistency in both configurations compared to the naive approach (i.e., WS-ImgSGG), demonstrating the effectiveness of our proposed method.}


\vspace{-2.2ex}
\section{Related Work}
\label{sec:related_work}
\vspace{-1.5ex}
\subsection{Video Scene Graph Generation (VidSGG)} 
\vspace{-1ex}
Video Scene Graph Generation (VidSGG) aims to learn the spatio-temporal dependencies of a video to predict the dynamic relationships between all object pairs. Existing VidSGG models are categorized into two settings based on the granularity of the generated video scene graph: video-level VidSGG \citep{shang2017video,tsai2019video,shang2021video,wei2024defense} and frame-level VidSGG \citep{cong2021spatial,feng2023exploiting,teng2021target,li2022dynamic}. 
Video-level VidSGG models generate a global scene graph for a video clip, while frame-level VidSGG models generate a scene graph for each frame in a video clip. Note that our work follows the frame-level VidSGG setting.
Following the release of the Action Genome dataset \citep{ji2020action}, frame-level VidSGG models have been actively researched. Specifically, STTran \citep{cong2021spatial} employs two separate transformers to capture spatio-temporal dependencies of objects and frames, and TRACE \citep{teng2021target} proposes a hierarchical relation tree method to enhance spatial-temporal reasoning. DSG-DETR \citep{feng2023exploiting} mitigates the complexity issue arising from the fully-connected graph between frames, enabling the capture of long-term temporal context. {Recently, TEMPURA \citep{nag2023unbiased} and FloCoDe \citep{khandelwal2024flocode} aim to alleviate the long-tailed predicate issue through memory-guided learning and label correlation loss, respectively.} However, as these methods heavily rely on costly annotations on all frames, we propose a weakly supervised approach for VidSGG, which utilizes the readily available video captions, i.e., language supervision, for the first time.





% \begin{table}[t]
% \centering
% \caption{Performance over various video length. We use backbone as STTran.}
% % \vspace{-1ex}
% \resizebox{0.85\linewidth}{!}{
% \centering
% \begin{tabular}{c|l|c|cccc|c}
% \toprule
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training Dataset\\ (Caption)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Avg. Video Length}} & \multicolumn{2}{c}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{4-7}
%                                            & \multicolumn{1}{c|}{}                                 &                                             & \textbf{R@20}         & \textbf{R@50}        & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule
% \multirow{2}{*}{Action Genome}             & WS-ImgSGG                                             & \multirow{2}{*}{29.9 seconds}                   & 10.01                 & 12.83                & 9.02                 & 14.05                & 11.48                          \\
%                                            & \proposed                                                 &                                             & \textbf{15.61}        & \textbf{19.60}       & \textbf{15.92}       & \textbf{22.56}       & \textbf{18.42}                 \\ \midrule
% \multirow{2}{*}{MSVD}                      & WS-ImgSGG                                             & \multirow{2}{*}{9.5 seconds}                    & 6.22                  & 8.03                 & 7.69                 & 12.31                & 8.56                           \\
%                                            & \proposed                                              &                                             & \textbf{9.05}         & \textbf{11.31}       & \textbf{10.22}       & \textbf{16.60}       & \textbf{11.80}                 \\ \midrule
% \multirow{2}{*}{ActivityNet}               & WS-ImgSGG                                             & \multirow{2}{*}{117.3 seconds}                  & 10.86                 & 14.47                & 10.07                & 15.80                & 12.80                          \\
%                                            & \proposed                                                 &                                             & \textbf{13.46}        & \textbf{17.58}       & \textbf{13.94}       & \textbf{21.41}       & \textbf{16.60}                 \\ \bottomrule
% \end{tabular}
% \label{app_tab:longer_video}
% }
% \end{table}


\vspace{-1.5ex}
\subsection{Weakly Supervised Scene Graph Generation}
\vspace{-0.5ex}
\noindent\textbf{Weakly Supervised ImgSGG (WS-ImgSGG). }
To relax a heavy reliance on the costly annotation of a fully supervised approach, ImgSGG utilizes two types of weak supervision. \textbf{1) Unlocalized scene graph}: It uses ground-truth scene graphs represented in text format, which have not yet been grounded to bounding boxes. In this regard, related studies \citep{ye2021linguistic,shi2021simple,zareian2020weakly,li2022integrating} have focused on aligning these text-based scene graphs with suitable bounding boxes. LSWS \citep{ye2021linguistic} utilizes the linguistic structure within triplets for grounding, while another study \citep{shi2021simple} proposes a graph-matching module based on contrastive learning to further improve the grounding performance. \textbf{2) Language supervision:} In order to further relax the annotation costs associated with unlocalized scene graphs, the natural language description (i.e., image caption) is used for training the model \citep{zhong2021learning,kim2024llm4sgg,zhang2023learning}. SGNLS \citep{zhong2021learning} is the first to enable model training with image captions. LLM4SGG \citep{kim2024llm4sgg} addresses semantic over-simplification and low-density scene graph issues arising from the triplet generation process. 

\smallskip
\noindent\textbf{Weakly Supervised VidSGG (WS-VidSGG). }
As the first WS-VidSGG approach, PLA \citep{chen2023video} utilizes an unlocalized scene graph of the middle frame in a video clip as weak supervision to train a VidSGG model. 
However, the assumption of a ground-truth scene graph existing in the middle frame among other frames is not only unrealistic but also still requires manual human annotation. On the other hand, our proposed~\proposed{} framework only relies on the readily available video captions to train a VidSGG model, which further reduces annotation costs.

% On the other hand, we propose, for the first time in VidSGG, a simple yet effective approach that corresponds to language supervision, which further reduces annotation costs.


% \subsection{Large Language Model}
% Large Language Models \citep{chowdhery2022palm,upstage2023llama,open2022chatgpt} (LLMs) have shown exceptional generalizability in the NLP domain, performing various downstream tasks without fine-tuning. A myriad of works adopt LLMs for various purposes \citep{lu2024chameleon,gupta2023visual,jin2024time,kim2024llm4sgg,gruver2024large} benefiting from the generalizability. Specifically, Chameleon \citep{lu2024chameleon} and VisProg \citep{gupta2023visual} employ LLMs as tool planners to effectively utilize different tools in suitable combinations. LLM4SGG \citep{kim2024llm4sgg} harnesses LLMs to construct triplets from image captions for the ImgSGG task, while other studies \citep{jin2024time,gruver2024large} aim to comprehend time-series forecasting datasets using LLMs through prompts. In the context of the WS-VidSGG task, we leverage the LLMs to understand the temporality within video captions.

\vspace{-2ex}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-1ex}
In this work, we propose a weakly supervised \textbf{V}id\textbf{S}GG with \textbf{N}atural \textbf{L}anguage \textbf{S}upervision (\proposed{}) that relieve annotation costs for VidSGG, which is the first time to enable training a VidSGG model with readily available video captions.
We identify two key reasons for why a simple adoption of the WS-ImgSGG pipeline to VidSGG fails, i.e., temporality within video captions and variability of action duration.
Our Temporality-aware Caption Segmentation (TCS) module captures the temporality within the video captions, while Action Duration Variability-aware Caption-Frame Alignment (ADV) module addresses the variability in the action duration.
Furthermore, we propose a novel pseudo-labeling strategy based on motion cues (PLM) to deal with negative action classes in the Action Genome dataset.
% PLM strategy to assign negative classes. 
Our extensive experiments on the Action Genome dataset demonstrate the superiority of \proposed{} over the simple adoption of WS-ImgSGG pipeline to VidSGG. 
As a further appeal of \proposed{}, it allows the VidSGG models to predict a broader range of action classes that are not included in the training data, which makes our proposed framework practical in reality.

However, \proposed{} has a limitation regarding untrimmed, long-duration videos. For detailed information related to this and the future work addressing it, please refer to the Appendix.~\ref{app_sec:future_work}.

\textsc{\large Acknowledgements}  

\vspace{.5ex}

% This research was supported in part by the NAVER-Intel Co-Lab. The work was conducted by KAIST and reviewed by both NAVER and Intel.

% This work was supported by the Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2020-0-00004).

This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2022-II220077, Reasoning, and Inference from Heterogeneous Data), as well as another IITP grant funded by the Korea government(MSIT) (No.2020-0-00004). Furthermore, this research was supported in part by the NAVER-Intel Co-Lab. The work was conducted by KAIST and reviewed by both NAVER and Intel.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}



\clearpage

\startcontents[appendix]
\newpage
\begin{center}
    \huge{\emph{Supplementary Material}}
\end{center}

\begin{center}
    \large{\emph{- Weakly Supervised Video Scene Graph Generation via \\ Natural Language Supervision -}}
\end{center}

\vspace*{3mm}
\rule[0pt]{\columnwidth}{1pt}
% \vspace*{-.5in}
\appendix


\printcontents[appendix]{Appendix}{1.0}

\vspace*{.5in}

\clearpage

% \appendix
% \section{Appendix}

\begin{figure*}[h]
\centering
    \centering
    \includegraphics[width=.99\columnwidth]{Figure/prompt.pdf}
    \vspace{-2ex}
    \caption{Prompt used in the Temporality-aware Caption Segmentation (TCS) module.
    }
\label{app_fig:prompt}
\vspace{-2ex}
\end{figure*}

\section{Details of Prompt}
\label{app_sec:detail_prompt}
In Fig.~\ref{app_fig:prompt}, we provide the complete prompt used in the TCS module, which is discussed in Section 2.2 of the main paper.



\section{Ablation Study of Prompt Designed for Coreference Resolution}
\label{app_sec:exp_coreference}

\begin{wraptable}{r}{.43\columnwidth}
    \vspace{-4.4ex}
    \centering
    \caption{Impact of the prompt designed for coreference resolution.}
\resizebox{.43\textwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c}{\textbf{No Constraint}} \\
\multicolumn{1}{c|}{}                                 & \hspace{1.5ex}R@20                  & R@50                  & \hspace{1.5ex}R@20                 & R@50                \\ \midrule
Complete Prompt                                       & \hspace{1.5ex}15.61                 & 19.60                 & \hspace{1.5ex}15.92                & 22.56               \\
\;\textbf{+}w/o Coreference Resolution       & \hspace{1.5ex}15.48                 & 19.09                 & \hspace{1.5ex}15.80                & 21.76               \\ \bottomrule
\end{tabular}
    }
    \vspace{-2ex}
    \label{app_tab:coreference}  
\end{wraptable}

In this section, we conduct an experiment to validate the impact of the prompt introduced to address coreference resolution, 
% coreference resolution-guided prompt in the TCS module, 
which aids in specifying the pronouns and extracting accurate triplets. 
That is, we remove the following sentence in \textit{task description} of Fig.~\ref{app_fig:prompt}: \textit{Note that you should specify the objects for the pronouns used in each of these sentences.}, and evaluate~\proposed~without it.
% we inform an LLM with the coreference resolution-guided prompt: \textit{Note that you should specify the objects for the pronouns used in each of these sentences.} To ablate it, we only remove this prompt within the complete prompt and ask the LLM with its prompt to generate the segmented sentences. 
As shown in Table~\ref{app_tab:coreference}, the performance with the prompt designed for coreference resolution (i.e., Complete Prompt) is superior compared to the performance without that prompt (i.e., w/o Coreference Resolution). It is attributed to the increase in the number of triplets from 58K to 82K (i.e., 40\% increase) when using the Complete Prompt, resulting in the alleviation of lack of supervision \cite{kim2024llm4sgg}. This demonstrates the effectiveness of our prompt design for coreference resolution.



\section{Experiment on the VidHOI dataset}
\label{app_sec:vidhoi}
\noindent\textbf{Dataset.} The VidHOI \citep{chiou2021st} dataset consists of real-life videos capturing daily human activities without a scripted narrative.
This dataset includes manually annotated scene graphs on based on keyframes sampled at 1 FPS. Following the processing step of prior studies \citep{ni2023human,chiou2021st}, we obtain the training and test sets contain 6,366 and 756 videos along with 193,911 and 22,976 frames, respectively. There are 78 object classes and 50 predicate classes, which are divided into 8 spatial classes and 42 action classes. Note that since the VidHOI dataset does not include video captions, we employ a video captioning model (i.e., VideoChat \citep{li2023videochat}) to generate video captions.

\begin{wraptable}{r}{.46\columnwidth}
    \centering
    \vspace{-6ex}
\caption{Results on VidHOI dataset.}
\resizebox{.46\textwidth}{!}{
\begin{tabular}{c|l|c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Backbone}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Supervision}}             & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c}{\textbf{No Constraint}} \\ \cmidrule{4-7}
                                   & \multicolumn{1}{c|}{}                                 &                                                   & \textbf{R@20}         & \textbf{R@50}        & \textbf{R@20}        & \textbf{R@50}       \\ \midrule \midrule
\multirow{3}{*}{STTran}            & Vanilla                                               & \textbf{Full}                                     & 19.48                 & 20.30                & 25.63                & 28.05               \\ \cmidrule{2-7} 
                                   & \;+WS-ImgSGG                                          & \multirow{2}{*}{\textbf{Weaker}} & 7.56                       & 7.95                     &  21.16                    &   26.55                  \\
                                   & \;+\proposed{}                                              &                                                   &  10.41                     &  10.96                    &   21.44                   &   27.16                 \\ \bottomrule
\end{tabular}
    }
    \vspace{-2ex}
    \label{app_tab:different_dataset} 
\end{wraptable}


\noindent\textbf{Results.} Table~\ref{app_tab:different_dataset} shows performance comparisons under natural language supervision between a simple adoption of WS-ImgSGG pipeline (i.e., WS-ImgSGG) and our proposed framework (i.e., \proposed{}) on the VidHOI dataset. Beyond the comparisons made within the Action Genome dataset in the main paper, we also observe that in the VidHOI dataset, \proposed{} continues to outperform the WS-ImgSGG method, further validating the effectiveness of our proposed framework.



\section{Replacing ChatGPT and DAC with other models}
\label{app_sec:different_moodel}

\vspace{-1.0ex}

\begin{wraptable}{r}{.36\columnwidth}
    \vspace{-4.8ex}
    \centering
    \caption{Performance with other models.}
\resizebox{.36\textwidth}{!}{
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Row}} & \multicolumn{2}{c|}{\textbf{Module}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c}{\textbf{No Constraint}} \\ \cline{2-7} 
                              & TCS               & ADV              & \hspace{1.5ex}R@20                  & R@50                  & \hspace{1.5ex}R@20                 & R@50                \\ \midrule \midrule
                              % & Smaller LLM       & Video-Lang       & 15.27                 & 19.09                 & 15.53                & 21.81               \\
\rowcolor{gainsboro} (a)                           & \multicolumn{2}{c|}{WS-ImgSGG}                     & \hspace{1.5ex}10.01                 & 12.83                 & \hspace{1.5ex}9.02                & 14.05        \\ 

\rowcolor{gainsboro} (b)                           & ChatGPT               & DAC      & \hspace{1.5ex}15.61                 & 19.60                 & \hspace{1.5ex}15.92                & 22.56        \\       
(c)                           & Mistral-7B       & DAC      & \hspace{1.5ex}14.58                 & 18.74                 & \hspace{1.5ex}14.93                & 21.95               \\
(d)                           & ChatGPT               & InternVideo       & \hspace{1.5ex}15.58                 & 19.66                 & \hspace{1.5ex}15.92                & 22.38               
\\ \bottomrule
\end{tabular}
    }
    % \vspace{-5.0ex}
    \label{app_tab:foundation}  
\end{wraptable}
In this section, we replace the LLM used in the TCS module (i.e., ChatGPT) and the vision-language model used in ADV (i.e., DAC) with alternative models, and use the same prompts shown in Fig.~\ref{app_fig:prompt} to evaluate the performance. Table~\ref{app_tab:foundation} shows the results, where Row (a) is the performance of the baseline WS-ImgSGG model, and Row (b) is the performance of the original~\proposed~reported in the main paper.

% To explore the efficacy of each base model (i.e., ChatGPT used as the LLM and DAC used as the vision-language model) in TCS and ADV modules, we conduct experiments by replacing them with different base models.

\smallskip
\noindent \textbf{Replacing an LLM (i.e., ChatGPT) with a smaller LM (i.e., Mistral-7B) in TCS module. } Row (c) shows the performance of replacing ChatGPT-175B~\cite{open2022chatgpt} with a smaller Mistral-7B \cite{jiang2023mistral}. 
% For the TCS module, we aim to investigate the efficacy of a smaller LM rather than an LLM, i.e., ChatGPT \cite{open2022chatgpt}. To this end, we conduct experiments by replacing ChatGPT with Mistral-7B \cite{jiang2023mistral} based on the same prompt shown in Fig.~\ref{app_fig:prompt}. 
% As shown in Table~\ref{app_tab:foundation}, 
We observe that although the performance based on the smaller LM is inferior to that of ChatGPT as expected, it still outperforms WS-ImgSGG baseline shown in Row (a). This demonstrates the effectiveness of our proposed~\proposed~framework.
% reduced by 5\% on average compared to the performance based on ChatGPT (row (a) vs. (b)), 
% This reduction in performance mainly stems from the disparity of generalizability between the smaller LM (i.e., Mistral-7B) and the larger LLM (i.e., ChatGPT-175B), which is influenced by the model size \cite{jin2023time}. 

\smallskip
\noindent \textbf{Replacing a vision-language model (i.e., DAC) with a video-language model (i.e., InternVideo) in ADV module.} Within the ADV module, we raise the question: is it sufficient to use a vision-language model, which is trained on image-text pair datasets, for aligning segmented sentences with frames while capturing the temporal context between frames? To explore it, we conduct experiments by replacing the image-based vision-language model (i.e., DAC \cite{doveh2024dense}) with a video-language model (i.e., InternVideo \cite{wang2022internvideo}). Specifically, to explicitly reflect temporal context using InternVideo, we group two consecutive frames without overlap and encode each grouped frame to obtain a visual representation, followed by performing Clustering-based Caption-Frame Alignment, as discussed in Section 2.3 of the main paper.

As shown in Table~\ref{app_tab:foundation}, we observe that the performance based on the InternVideo (Row (d)) is competitive with that based on the DAC (Row (b)). It demonstrates that it is sufficient to use a vision-language model for aligning segmented sentences with frames while capturing the temporal context by simply averaging the frames, i.e., centroid in a cluster. This result is consistent with a prior research, i.e., CLIP4CLIP \cite{luo2022clip4clip}, which demonstrates that simply averaging frame visual representations yields competitive results compared to an advanced model reflecting temporal context between frames.
% we aim to explore the efficacy of being able to capture the temporal context of frames and align segmented sentences with them via a vision-language model trained on image-text pair datasets.
% To this end, we conduct experiments where we replace the vision-language model (i.e., DAC \cite{doveh2024dense}) with a video-language model (i.e, InternVideo \cite{wang2022internvideo}). Specifically, to capture temporal context using InternVideo, we group two consecutive frames without overlap and encode each grouped frame to obtain a visual representation that incorporates temporal information. Then, we utilize these representations to perform Clustering-based Caption-Frame Alignment, as discussed in Section 2.3 of the main paper, to align the segmented sentences with frames. As shown in Table~\ref{app_tab:foundation}, we observe that despite reflecting temporal context through InternVideo for ADV, it shows competitive performance compared to the performance utilizing the vision-language model (row (a) vs (c)). It indicates that simply averaging the representations (i.e., centroid in a cluster) encoded by the vision-language model for each frame is enough to capture some temporal context, followed by aligning the averaged representation with the segmented sentences. This observation is consistent with prior research, such as CLIP4CLIP \cite{luo2022clip4clip}, which demonstrates that simply averaging frame representations yields competitive results compared to incorporating temporal context between frames via Transformers \cite{vaswani2017attention}.


\vspace{-1.0ex}

\section{Experiment for Selection Strategy of Pseudo-Labeling}
\label{app_sec:selection}
\vspace{-1.0ex}

\begin{wrapfigure}[10]{r}{.28\columnwidth}
\vspace{-3.8ex}
\begin{center}
    \includegraphics[width=.28\columnwidth]{Figure/selection_strategy.pdf}
\end{center}
    % \rule{3cm}{7cm}
    \vspace{-4.2ex}
    \caption{Performance over diverse selection strategies.}
    \label{app_fig:selection}
    % \vspace{2ex}
\end{wrapfigure}

Here, we conduct experiments over various selection strategies for assigning pseudo-labels of negative classes (i.e., \textsf{not looking at} and \textsf{not contacting}) within unaligned frames, which is mainly addressed in Section 2.2 of the main paper. Specifically, these strategies involve assigning negative classes to the start (S), end (E), or both start and end (SE) frames for each negative class. The experiment results are shown in Fig.~\ref{app_fig:selection} reported with R@50 in With Constraint setting. For \textsf{not looking at}, we observe that the performance of assigning it on the end frame is inferior compared to the performance of the start frame ($\text{2}^{\text{rd}}$ vs. $\text{3}^{\text{rd}}$ row), while the performance of start+end frames is best ($\text{1}^{\text{rd}}$ row). It indicates that the start frame for assigning \textsf{not looking at} provides more confident supervision than assignment on the end frame, and assignment on both frames is most beneficial. 

For \textsf{not contacting}, in contrast to \textsf{not looking at}, we observe that the performance of assigning it on the start frame is inferior compared to the performance of the end frame ($\text{1}^{\text{rd}}$ vs $\text{2}^{\text{rd}}$ column), while the performance of start+end frames is best ($\text{3}^{\text{rd}}$ column), except when \textsf{not looking at} is exclusively labeled on the end frame ($\text{2}^{\text{rd}}$ row with $\text{3}^{\text{rd}}$ column). It indicates that the end frame rather provides more confident supervision than the start frame, while assignment on both frames is generally confident. We attribute the exceptional case to the noisy supervision, where \textsf{not looking at} is only assigned on the end frame.

% \vspace{-2.0ex}
\section{Experiment for Different Scene Graph Parsing Approach}
\label{app_sec:tripet_extraction}
% \vspace{-1.0ex}

\begin{wraptable}{r}{.40\columnwidth}
    \vspace{-4.8ex}
    \centering
    \caption{Performance over diverse scene graph parsing approaches.}
\resizebox{.40\textwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Scene graph parsing}}} & \multicolumn{2}{c|}{\textbf{ With Constraint }} & \multicolumn{2}{c}{\textbf{ No Constraint }} \\  
\cline{2-5}                                                & \hspace{1.5ex}R@20                  & R@50                  & \hspace{1.5ex}R@20                 & R@50                \\ \midrule
LLM-based approach                                             & \hspace{1.5ex}15.61                 & 19.60                 & \hspace{1.5ex}15.92                & 22.56               \\
SG Parser + KB approach                                                        & \hspace{1.5ex}11.08                 & 14.34                 & \hspace{1.5ex}11.26                & 16.51               \\ \bottomrule
\end{tabular}
    }
    % \vspace{-5ex}
    \label{app_tab:scene_graph_parsing}  
\end{wraptable}

In this section, we conduct experiments regarding the different approaches for scene graph parsing discussed in Section 2.4 of the main paper. Specifically, there are two approaches to scene graph parsing: one involves extracting triplets and aligning classes with those of our interest based on an LLM \cite{kim2024llm4sgg} (LLM-based approach\footnote{In the main paper, we use an LLM-based approach for scene graph parsing.}), while the other relies on a rule-based scene graph parser \cite{schuster2015generating} with knowledge base-based \cite{miller1995wordnet} alignment (SG parser+KB approach). Note that for the scene graph parsing, these two approaches are widely adopted in the realm of WS-ImgSGG \cite{kim2024llm4sgg,zhong2021learning,zhang2023learning,ye2021linguistic}. In Table~\ref{app_tab:scene_graph_parsing}, we observe that the performance of the SG Parser+KB approach is inferior compared to the LLM-based approach. It indicates that the heuristic rule-based parser for triplet extraction from video captions is ineffective, and knowledge base-based alignment struggles to accurately map the classes with complicated action classes (e.g., \textsf{drinking from, in front of}) in VidSGG. In this regard, we demonstrate that the LLM-based approach is particularly effective at WS-VidSGG due to the complex structure of video captions and complicated action classes. To further facilitate research in the WS-VidSGG, we make the triplets extracted based on the LLM publicly available.

% \vspace{-2.0ex}
\section{Experiment for Integration of Dynamic Relationships into RLIP}
\label{app_sec:rlip}
% \vspace{-1.0ex}
\begin{wraptable}{r}{.46\columnwidth}
    \vspace{-4.8ex}
    \centering
    \caption{Performance of RLIP \cite{yuan2022rlip} trained with dynamic relationships.}
\resizebox{.46\textwidth}{!}{
\begin{tabular}{l|c|cc|cc}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Model}}} & \multirow{2}{*}{\textbf{Supervision}} & \multicolumn{2}{c|}{\textbf{ With Constraint }} & \multicolumn{2}{c}{\textbf{ No Constraint }} \\ \cline{3-6} 
\multicolumn{1}{c|}{}                                &                                       & \hspace{1.5ex}R@20                  & R@50                  & \hspace{1.5ex}R@20                 & R@50                \\ \midrule \midrule
STTran                                               & \multirow{1}{*}{Full}                 & \hspace{1.5ex}33.98                 & 36.93                 & \hspace{1.5ex}36.20                & 48.88               \\ \midrule
RLIP                                                 & \multirow{2}{*}{Full (Fine-tune)}                                      & \hspace{1.5ex}31.89                 & 36.26                 & \hspace{1.5ex}34.54                & 41.00               \\
RLIP+STTran                                          &                                       & \hspace{1.5ex}34.02                 & 40.04                 & \hspace{1.5ex}35.10                & 42.72               \\ \midrule
RLIP                                                 & Zero-shot                             & \hspace{1.5ex}7.93                  & 9.16                  & \hspace{1.5ex}9.70                 & 13.80               \\ \bottomrule
\end{tabular}
    }
    \label{app_tab:rlip}  
\end{wraptable}

In Table 1 of the main paper, we observe that RLIP \cite{yuan2022rlip} exhibits subpar performance under the zero-shot setting due to its inability to predict dynamic relationships. To explore potential performance enhancement achievable by integrating dynamic relationships into RLIP, we conduct experiments by fine-tuning a pre-trained RLIP on the Action Genome dataset. Furthermore, we append the STTran \cite{cong2021spatial} module to RLIP in order to facilitate capturing temporal context. As shown in Table~\ref{app_tab:rlip}, we observe that RLIP trained with dynamic relationship substantially boosts performance. Moreover, RLIP with the STTran module further enhances performance by capturing temporal context. It indicates that the incorporation of dynamic relationships is crucial in VidSGG.
% , while it cannot be taken in ImgSGG. 
In summary, we demonstrate the importance of incorporating dynamic relationships and reflecting temporal context in VidSGG.


\begin{figure*}[t]
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=.99\linewidth]{Figure/k_hyper.pdf}
        \vspace{-5ex}
        \caption{Hyperparameter sensitivity of $\beta$. WC and NC stand for With Constraint and No Constraint setting, respectively.}
        \label{app_fig:k_hyper}
    \end{minipage}\hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=.99\linewidth]{Figure/alpha_hyper.pdf}
        \vspace{-5.2ex}
        \caption{Hyperparameter sensitivity of $\alpha$. WC and NC stand for With Constraint and No Constraint setting, respectively.}
    \label{app_fig:alpha}
    \end{minipage}
\end{figure*}

\section{Qualitative analysis of the ADV module}
\label{app_sec:adv}

\begin{figure*}[t]
\centering
    \centering
    \includegraphics[width=.79\columnwidth]{Figure/adv_qualitative.pdf}
    \vspace{-2ex}
    \caption{Qualitative analysis of the ADV module.
    }
\label{app_fig:adv_quali}
\end{figure*}
{
To qualitatively evaluate the effectiveness of the ADV module, we visualize the results of clustering video frames in the visual space using T-SNE, and show the distribution of their visual-language (VL) scores, i.e., the similarity scores between each segmented sentence and the cluster centroids.  
% We then examine the functionality of the ADV module. 
In Figure~\ref{app_fig:adv_quali}, we observe that in the case where a cluster is clearly separated (See A), the VL score distribution is concentrated within that cluster, resulting in the selection of a single cluster. On the other hand, in the case where visual features are concentrated but distributed across multiple clusters (See B), the ADV module adaptively selects multiple clusters. This indicates that the ADV module reflects the variability in action duration, allowing it to supervise the model accurately and thereby improve performance.
}


\section{Ablation Studies on the AG dataset without negative classes}
\label{app_sec:ablation_study_wo_PLM}


\begin{wraptable}[7]{r}{.46\columnwidth}
    \vspace{-4.8ex}
    \centering
\caption{Ablation studies where negative classes are excluded from the AG dataset.}
\resizebox{.46\textwidth}{!}{
\begin{tabular}{cc|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{TCS}} & \multirow{2}{*}{\textbf{ADV}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multicolumn{2}{c}{\textbf{Mean}} \\ \cmidrule{3-8} 
                              &                               & \textbf{R@20}         & \textbf{R@50}         & \textbf{R@20}        & \textbf{R@50}        & \textbf{R@20}   & \textbf{R@50}   \\ \midrule \midrule
                              &                               & 12.98                 & 17.10                 & 13.28                & 19.38                & 13.13           & 18.24           \\
\checkmark                    &                               & 15.34                 & 20.24                 & 15.47                & 21.72                & 15.41           & 20.98           \\
\checkmark                    & \checkmark                    & \textbf{15.95}        & \textbf{20.74}        & \textbf{16.19}       & \textbf{23.04}       & \textbf{16.07}  & \textbf{21.89}  \\ \bottomrule
\end{tabular}
    }
\label{app_tab:ablation_without_negative}
\end{wraptable}


\textcolor{black}{In Section~\ref{sec:ablation_study} of the main paper, we observed a relatively significant performance improvement in the PLM module due to the dominance of the negative class in the dataset. Therefore, to further clarify the effectiveness of the TCS and ADV modules, we perform ablation studies in which negative classes are excluded from both training and test sets in the Action Genome (AG) dataset. As shown in Table~\ref{app_tab:ablation_without_negative}, we observe that even in a dataset where the PLM module cannot be applied, TCS and ADV modules still show superiority, proving their effectiveness.}


% In this section, we perform ablation studies in which negative classes are excluded from both training and test sets in the Action Genome (AG) dataset, aiming to further clarify the effectiveness of TCS and ADV modules. As shown in Table~\ref{app_tab:ablation}, we observe that even in a dataset where PLM structure is not required, TCS and ADV modules still show superiority, proving their effectiveness similar to Section~\ref{sec:ablation_study}.


\section{Hyperparameter Sensitivity}
\label{app_sec:hyperparameter}
In \proposed{}, two hyperparameters are used: $\beta$ for defining $K$ (i.e., $\frac{|V|}{\beta}$) in ADV module (Section 2.3), and $\alpha$ for assigning negative classes within unaligned frames in PLM module (Section 2.5). We analyze the sensitivity of these hyperparameters.

\smallskip
\noindent \textbf{Hyperparameter $\beta$ for ADV module. } In Fig.~\ref{app_fig:k_hyper}, we conduct experiments over various $\beta$s. We observe that the performance decreases with large $\beta$ which reduces the cluster number $K$. It indicates that large $\beta$ (i.e., small $K$) cannot capture the fine-grained variability of action duration, resulting in deteriorating performance. On the other hand, small $\beta$ (i.e., large $K$) helps to capture fine-grained variability of action duration, leading to increasing performance. However, rather small $\beta$ (i.e., 3) decreases performance in No Constraint setting. We attribute it to the fact that rather small $\beta$ divides the frames into overly fine-grained clusters, making it difficult for the vision-language model to distinguish highly discriminative clusters in terms of similarity scores. In this regard, it is beneficial to appropriately set $\beta$ as 4 to capture the variability of action duration. 

\smallskip
\noindent \textbf{Hyperparameter $\alpha$ for PLM module. }
In Fig.~\ref{app_fig:alpha}, we conduct experiment over various $\alpha$s. We observe that the performance consistently increases up to 5\%, followed by fluctuation beyond 5\%. It is worthwhile noting that the pseudo-labels with smaller $\alpha$ would provide more confident supervision to models since their associated subject and object are distinctly getting farther over time. It suggests that up to 5\%, clear supervision is provided for negative classes within unaligned frames, but beyond that ratio, noisy supervision is included. Hence, setting $\alpha$ at around 5\% is preferable. However, we opt for a value of $\alpha$ at 15\% as it marginally enhances performance under the With Constraint setting.





% \begin{figure*}[h]
% \centering
%     \centering
%     \includegraphics[width=.93\columnwidth]{Figure/broaden_action_class.pdf}
%     \vspace{-2ex}
%     \captionof{figure}{Broaden action classes}
% \label{app_fig:broaden_action}
% \end{figure*}



\section{Details of Expanded Action Classes}
\label{app_sec:dist_of_large_voca}

In Table.~\ref{app_tab:detail_expanded_cls}, we enumerate all the expanded action classes sorted by frequency in descending order.



\section{Cost for Utilization of a Large Language Model}
\label{app_sec:cost_llm}

\begin{table}[t]
\centering
\caption{Summarization of cost for an LLM.}
% \vspace{-1ex}
\resizebox{0.7\linewidth}{!}{
\centering
\begin{tabular}{c|c|c}
\toprule
\multicolumn{1}{c|}{\textbf{Module}}                 & \textbf{Num. Output/Input tokens per video} & \textbf{Cost per video} \\ \midrule
TCS    & 0.045k / 0.68k                   & \$0.00041               \\
\bottomrule
\end{tabular}
\label{app_tab:cost_llm}
}
\end{table}

For the TCS module, the cost of using ChatGPT \cite{open2022chatgpt} is summarized in Table~\ref{app_tab:cost_llm}. Given that the cost of input tokens and output tokens is \$0.5 and \$1.5 per 1M tokens, respectively, the cost per video is computed by (680/1M) $\times$ 0.5 + (45/1M) $\times$ 1.5.
% For TCS module, we use \textit{gpt-3.5-turbo} in ChatGPT , which incurs costs. Table~\ref{app_tab:cost_llm} is summarized 



% \begin{wraptable}{r}{.40\columnwidth}
%     \vspace{-4.8ex}
%     \centering
% \caption{Temporal Consistency (\%)}
% \resizebox{.40\textwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% \multicolumn{1}{c|}{\textbf{Method}} & \textbf{Recall} & \textbf{Precision} & \textbf{F1 Score} \\ \midrule \midrule
% \multicolumn{4}{c}{\textbf{Unlimited Configuration}}                                              \\ \midrule
% \multicolumn{1}{l|}{WS-ImgSGG}       &  3.96            & 36.53              & 7.18             \\
% \multicolumn{1}{l|}{VSNLS}           & \textbf{5.12}  & \textbf{50.66}     & \textbf{9.30}    \\ \midrule
% \multicolumn{4}{c}{\textbf{Limited Configuration}}                                            \\ \midrule
% \multicolumn{1}{l|}{WS-ImgSGG}       & 14.97           & 36.53              & 21.24             \\
% \multicolumn{1}{l|}{VSNLS}           & \textbf{26.77}  & \textbf{50.66}     & \textbf{29.10}    \\ \bottomrule
% \end{tabular}
%     }
% \vspace{-2ex}
% \label{app_tab:temporal_consistency}
% \end{wraptable}

% \section{Evaluation of temporal consistency of action labels}
% \label{app_sec:temporal_consistency}
% \textcolor{red}{
% To measure how action labels inherent in video captions are accurately aligned with corresponding frames, we evaluate the temporal consistency by comparing the action labels extracted from video captions with the ground-truth action labels. Specifically, we regard the ground-truth action labels in each frame as actual values and the action labels extracted from the video captions and assigned to the frames as predictive values. We then compute Recall, Precision, and F1 score. Here, we consider two configurations depending on whether to restrict ground-truth action labels to only those found in each video caption (\textbf{Limited Configuration}) or to use all available ground-truth action labels (\textbf{Unlimited Configuration}). In other words, the Limited Configuration is aimed at directly evaluating temporal consistency, while the Unlimited Configuration offers a broader evaluation approach. As shown in Table~\ref{app_tab:temporal_consistency}, we find that our proposed method significantly increases the temporal consistency in both configurations compared to the naive approach (i.e., WS-ImgSGG) discussed in Section 1, demonstrating the effectiveness of our proposed method.}

% \textcolor{red}{
% In the future, we expect that these temporal consistency metrics will be employed as a key criterion for improvement in weakly supervised video scene graph generation.
% }

\begin{wraptable}{r}{.60\columnwidth}
    \vspace{-4.8ex}
    \centering
\caption{Performance over various clustering strategies.}
\resizebox{.60\textwidth}{!}{
\begin{tabular}{l|cc|cc|c}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Clustering Algorithm}}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{2-5}
\multicolumn{1}{c|}{}                                               & \textbf{R@20}         & \textbf{R@50}         & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule \midrule
K-Means                                                             & 15.61                 & 19.60                 & 15.92                & 22.56                & 18.24                          \\
Agglormerative                                                      & \textbf{15.78}                 & 19.69                 & \textbf{16.12}                & 23.01                & 18.65                          \\
GMM                                                                 & 15.31                 & \textbf{19.80}                 & 15.85                & \textbf{23.93}                & \textbf{18.72}                          \\ \bottomrule
\end{tabular}
    }
\vspace{-2ex}
\label{app_tab:different_clustering}
\end{wraptable}

\section{Experiment for Different Clustering in the ADV module}
To investigate the impact of different clustering strategies within the ADV module, we conducted experiments where we replaced the K-means clustering strategy with Agglomerative clustering and Gaussian Mixture Model (GMM) clustering strategies. As shown in Table~\ref{app_tab:different_clustering}, we observed that other clustering strategies exhibit competitive performance compared to that of the K-means clustering strategy, indicating that our proposed framework is robust to other clustering strategies. Another observation is that the performance with the GMM clustering strategy is relatively better on average. This result aligns with previous studies \citep{huang2022weakly,zheng2022weakly} that assume proposal distributions as Gaussian in the temporal grounding task in that GMM also assumes the same thing, resulting in effective clustering within the ADV module.





% \section{Analysis of performance on longer video dataset}
% \label{app_sec:longer_video}

% \textcolor{red}{
% To analyze the impact of video length, we conducted experiments using the ActivityNet \citep{krishna2017dense} caption dataset (average length: 117.3 seconds), which is approximately 4 times longer than the Action Genome caption dataset (average length: 29.9 seconds) and 12 times longer than the MSVD dataset (average length: 9.5 seconds). As shown in Table~\ref{app_tab:longer_video}, we made the following two observations: 1) Regardless of video length, our proposed method consistently outperformed the naive approach (i.e., WS-ImgSGG). This indicates that our proposed method remains effective across videos of various lengths. 2) When comparing the performance between the MSVD and ActivityNet datasets, aside from the benchmark dataset (i.e., Action Genome), we observed that both WS-ImgSGG and VSNLS achieved better performance on the longer ActivityNet dataset compared to the shorter MSVD dataset. We attribute it to the fact that longer videos allow the model to learn more diverse video content, thereby improving generalization, and provide more supervision as the duration of actions increases. In this context, despite the shorter video length of the Action Genome dataset compared to the ActivityNet dataset, our proposed method performs better on the Action Genome dataset. This is because the video distribution in the Action Genome dataset is more closely aligned with the test set, which is derived from Action Genome.
% }



% \begin{table}[t]
% \centering
% \caption{Performance over various video length. We use backbone as STTran.}
% % \vspace{-1ex}
% \resizebox{0.85\linewidth}{!}{
% \centering
% \begin{tabular}{c|l|c|cccc|c}
% \toprule
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Training Dataset\\ (Caption)\end{tabular}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multirow{2}{*}{\textbf{Avg. Video Length}} & \multicolumn{2}{c}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{4-7}
%                                            & \multicolumn{1}{c|}{}                                 &                                             & \textbf{R@20}         & \textbf{R@50}        & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule
% \multirow{2}{*}{Action Genome}             & WS-ImgSGG                                             & \multirow{2}{*}{29.9 seconds}                   & 10.01                 & 12.83                & 9.02                 & 14.05                & 11.48                          \\
%                                            & \proposed                                                 &                                             & \textbf{15.61}        & \textbf{19.60}       & \textbf{15.92}       & \textbf{22.56}       & \textbf{18.42}                 \\ \midrule
% \multirow{2}{*}{MSVD}                      & WS-ImgSGG                                             & \multirow{2}{*}{9.5 seconds}                    & 6.22                  & 8.03                 & 7.69                 & 12.31                & 8.56                           \\
%                                            & \proposed                                              &                                             & \textbf{9.05}         & \textbf{11.31}       & \textbf{10.22}       & \textbf{16.60}       & \textbf{11.80}                 \\ \midrule
% \multirow{2}{*}{ActivityNet}               & WS-ImgSGG                                             & \multirow{2}{*}{117.3 seconds}                  & 10.86                 & 14.47                & 10.07                & 15.80                & 12.80                          \\
%                                            & \proposed                                                 &                                             & \textbf{13.46}        & \textbf{17.58}       & \textbf{13.94}       & \textbf{21.41}       & \textbf{16.60}                 \\ \bottomrule
% \end{tabular}
% \label{app_tab:longer_video}
% }
% \end{table}




\begin{wraptable}{r}{.48\columnwidth}
    \vspace{-4.8ex}
    \centering
\caption{Performance for a combination of weakly supervised and fully supervised data.}
\resizebox{.48\textwidth}{!}{
\begin{tabular}{l|cc|cc|c}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Training dataset}}} & \multicolumn{2}{c|}{\textbf{With Constraint}} & \multicolumn{2}{c|}{\textbf{No Constraint}} & \multirow{2}{*}{\textbf{Mean}} \\ \cmidrule{2-5}
\multicolumn{1}{c|}{}                                           & \textbf{R@20}         & \textbf{R@50}         & \textbf{R@20}        & \textbf{R@50}        &                                \\ \midrule \midrule
AG (Full)                                                       & 33.98                 & 36.93                 & 36.20                & 48.88                & 39.00                          \\
AG+MSVD (Weak) â†’ AG (Full)                                      & \textbf{34.84}        & \textbf{37.64}        & \textbf{38.40}       & \textbf{49.55}       & \textbf{40.11}                 \\ \bottomrule
\end{tabular}
    }
\vspace{-2ex}
\label{app_tab:finetune_model}
\end{wraptable}



\section{Experiment Combining weakly supervised and fully supervised datasets}
In this section, we investigate a scenario where our proposed approach meets with the fully supervised approach. To this end, we attempted to fine-tune a model, which was initially trained on a weakly supervised dataset (i.e., pseudo-localized scene graphs in Section~\ref{sec:negative_category}), to the fully supervised dataset, i.e., ground-truth localized scene graphs. In this process, we assumed that a model trained on a larger dataset would provide more effective weight initialization, leading us to leverage the model trained on both the AG caption and MSVD caption datasets, as detailed in Section~\ref{sec:expansion_action} of the main paper. Interestingly, as shown in Table~\ref{app_tab:finetune_model}, we observed that the performance of the model fine-tuned to the AG dataset significantly outperformed that of the model initially trained on the AG dataset, implying that our proposed framework can synergize with the fully supervised approach.



\section{Discussion of Improvement for Parsing and Grounding}

For future work, we discuss the techniques developed for scene graph parsing and grounding, detailed in Section~\ref{sec:localization} of the main paper.


\noindent \textbf{Improvement for Scene Graph Parsing.} We may employ the ensemble approach \citep{wang2022self} to extract more high-quality scene graphs. Specifically, for an LLM-based parser, we can utilize temperature sampling or top-k sampling to extract diverse triplets for each segmented sentence, followed by taking a majority vote over diverse triplets to extract the most consistent triplet. With this technique, we can further develop the scene graph parsing over the state-of-the-art method.


\noindent \textbf{Improvement for Scene Graph Grounding.} 
We may improve the grounding accuracy. Basically, a triplet is grounded to a bounding box when the bounding boxâ€™s class matches the object class of the triplet parsed from the caption. However, inherent challenges in videos such as motion blur, fast movement, and occlusion often hinder accurate object class classification within the bounding box, resulting in grounding failures. To address it, we can utilize adjacent frames successfully grounded. Specifically, in the case where the object class of the bounding box in the target frame is ambiguous so that grounding fails, we can ensure grounding by selecting a bounding box with high IoU and visual similarity to a bounding box of an object that is grounded in an adjacent frame. This technique could compensate for the failures in the target frame, ensuring more reliable grounding.



\begin{figure*}[t]
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=.99\linewidth]{Figure/steepest_score_gap.pdf}
        \vspace{-5ex}
        \caption{The gap in the similarity score at the point of the steepest decline over the number of total clusters (i.e., $K$), which is proportional to the frames.}
        \label{app_fig:steepest}
    \end{minipage}\hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=.99\linewidth]{Figure/threshold_performance.pdf}
        \vspace{-5.2ex}
        \caption{Performance across various fixed thresholds in the ADV module, conducted under the With Constraint setting.}
    \label{app_fig:threshold}
    \end{minipage}
\end{figure*}


\section{Discussion of the Steepest Decline in the ADV module}

In the ADV module, detailed in the Section~\ref{sec:alignment} of the main paper, we originally determined the relevant clusters depending on the steepest decline in the vision-language similarity score.
Here, we explore the sensitivity of the steepest decline criterion to noise in the similarity score. To this end, we analyze the gap in the similarity score at the point of steepest decline\footnote{For example, in Figure~\ref{app_fig:adv_quali} of the Appendix, the gap in the similarity score at the point of the steepest decline is C1-C2 for (\textit{A}), and C4-C2 for (\textit{B}).} over the total number of clusters (i.e., $K$), which is proportional to the video frames. As shown in Figure~\ref{app_fig:steepest}, we observe that the gap in the steepest decline decreases as the number of total clusters increases (from the left to the right on the x-axis). This is because the similarity score is distributed across a greater number of clusters, thereby reducing the gap. However, it is important to note that from eight clusters onward, the gap converges to 0.2, demonstrating that even with noise in the similarity score, the steepest decline approach still reliably selects relevant clusters using this relatively large 0.2 gap.


In this regard, we raise a question: Why not determine the relevant clusters through a point where the gap in the decline exceeds 0.2 instead of using the steepest decline? Specifically, we could sort the similarity scores in descending order, find the first point where the gap exceeds 0.2, starting from the highest score, and select all the clusters preceding this point. Therefore, we conducted additional experiments where we set the threshold as a hyperparameter, ranging from 0.2 to 0.8, to determine the relevant clusters through a point where the gap in the decline exceeds this threshold. As shown in Figure~\ref{app_fig:threshold}, we had two observations: 1) Setting the threshold at 0.2 shows competitive performance with the steepest decline, aligning with an aforementioned discussion that a 0.2 gap is robust to determine the relevant clusters. 2) As the threshold increases, we observe a decrease in performance. This is because more irrelevant clusters are assigned as the threshold increases, thereby introducing noise.



However, we argue that \textit{this thresholding approach requires meticulous analysis of the gap in the steepest decline for each dataset to set the threshold, whereas the steepest decline approach has the advantage of adaptively capturing relevant clusters without needing to analysis each dataset}.


\section{Additional Related Work}
\subsection{Large Language Model-based Scene Graph Generation}
\vspace{-1ex}
As Large Language Models (LLMs) got a surge of interest from various domains, LLMs have also been applied to the SGG task to leverage the rich semantic knowledge. RECODE \citep{li2024zero} utilizes LLMs to generate an informative description for a predicate, enabling it to capture the fine-grained visual cues. For zero-shot scene graph generation task, ELEGANT \citep{zhao2023less} extracts the candidate relationships generated by the LLMs, which have profound reasoning and commonsense knowledge. For weakly supervised SGG, LLM4SGG \citep{kim2024llm4sgg} alleviates the long-tailed predicate issue and scarcity of datasets via the LLMs. In this work, we leverage the LLMs to understand the temporality within video captions for the WS-VidSGG task.


\section{Future Work}
\label{app_sec:future_work}
\vspace{-1ex}
A potential limitation of our work is that \proposed{} is mainly designed for handling relatively short-length videos ($\sim$120 seconds). As future work, we plan to generalize \proposed{} to longer untrimmed videos. One possible direction would be to perform untrimmed video temporal localization \citep{wang2023protege,yan2023unloc,li2023winner} tasks in computer vision, wherein short-length clips are extracted from longer videos, and apply \proposed{} to each clip for training a VidSGG model. This approach would enable the training of the VidSGG model with untrimmed longer videos.


Another direction for future work is to explore the possibility of using Multimodal Large Language Models (MLLMs) to extend open-set or multi-label prediction in the VidSGG task. Specifically, for open-set prediction, we can query the temporal relationship between a subject and an object to the MLLMs by inserting the union box and previous frames' caption for temporal context. For multi-label prediction, temperature \citep{ficler2017controlling} or top-\textit{k} sampling \citep{radford2019language}, commonly used in the NLP for generating diverse answers, can be used to generate various temporal relationships.

\begin{table}[h]
    \centering
    \caption{Enumeration of all expanded action classes.}
    \resizebox{!}{0.75\textwidth}{
    \begin{tabular}{l}
    \hline
put, pick up, open, take, hold, sit on, eat, grab, close, look at, throw, take off, drink, \\walk into, sit in, place, on, sit at, watch, with, put on, pour, walk to, read, set, \\drink from, in, clean, carry, walk over to, stand in, walk through, turn on, fold, \\work on, take out, look out, sit, remove, play with, use, get, look in, lie on, \\sweep, talk on, from, wash, fix, sit down on, tidy up, enter, walk out of, leave, into, \\move, turn off, lay on, walk in, go to, out of, look into, walk down, at, vacuum, play on, \\sit down, stand in front of, cook, turn, look through, off, stand on, wipe, run into, \\have, snuggle with, drop, walk up, make, lay down on, stand at, in front of, tidy, \\walk around, opening, check, do, pull out, toss, adjust, of, put down, pull, stand by, \\walk out, sit down at, shut, touch, open up, get up from, sit down in, pick, lie in, \\walk, hug, snuggle, lay in, onto, type on, look, walk across, drink out of, set down, \\sleep on, to, come into, flip through, write in, laugh at, walk up to, go through, \\stand, lay, lie down on, sneeze into, put away, talk to, wrap in, stir, sneeze, dry, wear, \\start, write, wrap, down on, reach for, stand next to, next to, hang, play, go into, \\stare at, retrieve, get out of, straighten, eat from, through, run through, run, grasps, \\walk towards, off of, smile at, under, look inside, answer, cook on, clean up, go over to, \\exit, fill, run out of, laugh, wipe off, stand near, look out of, reach into, walk over, \\smile, lean against, keep, cover with, tie, sweep with, rearrange, write on, stand hold, \\back on, sit in front of, mess with, wipe down, hold onto, hang up, organize, grasp, \\examine, prepare, run down, go out, wave, go back to, find, shake, walk in with, \\cover, bring, inspect, come in, undress, arrange, down, inside, awaken on, run to, \\push, dust, cover in, empty, lean on, sip from, by, stack, texte on, straighten up, \\walk with, interact with, walk away with, on the floor, bend down, lay down, flip, \\kick off, cuddle with, stand up, stand up from, clean off, get dress, on top of, \\leave through, walk away from, lock, fiddle with, reach, take out of, hold up, dump, \\go up, hand, fold up, point at, run around, change, cuddle, run in, kick, fluff, \\walk away, stand behind, knock, pour out, over, come through, swing, rub, \\enter through, get up out of, walk in through, unfold, unlock, sneeze on, scrub, \\take picture of, bend over, button up, walk past, type, consume, look for, continue, \\take a bite of, lie down, see, button, getting dress, near, smell, hit, get up off, \\lay down with, sit down to, take picture with, move to, get out, zips up, lift, turn out, \\dress in, walk toward, talk, give, rinse, look up at, read from, sit with, run up, get into, \\stop at, wrap up in, get off, sleep in, show, cover up with, twirl, wrap around, gather, \\lie down in, proceed to, be, around, check out, view, tidy up with, spray, \\undress out of, walk in hold, come in through, eat out of, hold on to, approach, \\appear to be in, swallow, fall on, sit next to, kneel on, dusting, stand nearby, walk by, \\search for, snuggle in, get on, stop, leave with, get up, for, search through, sniff, \\sit down in front of, on the ground, sweep up, undress from, back in, sit down with, \\asleep on, walk from, lean, look around, fidget with, look over, reach over, run out, \\cook at, rummage through, back into, polish, dress, come out of, dig through, return to, \\wake up from, swinge, comb, brush, sort, return, talk with, do work on, ball up, \\move towards, climb up, smile into, sit up, walk in front of, awaken, pace in, seize, \\undress in front of, climb into, roll up, finish, wet, pull up, place on, do something on, \\stand up with, spread, admire, out, wrap themselves in, untie, seat at, spill, wake up on, \\stand with, tap on, go in, go, juggle, behind, awaken from, work at, turn to, \\dance around, shine, turn away from, up to, pat, flick, zips, clean with, pack, reach over to, \\turn back to, walk back to, knock over, exit through, return with, walk back out of, \\vacuum around, run across, walk back down, full of, walk around with, follow, go out of, \\plug in, stand watch, away from, bite, switch, dump out, rock in, sprinkle, stick, \\sip out of, sit back down on, prop, underneath, seat on, cook with, do something with, \\recline on, flip on, lie, seat in, wake up, replace, continue up, reach in, kneel down, \\unbutton, text on, clothe, come to, appear to be tidy up, tie up, fill up, climb, \\reach up on, climb on, lay down in, down onto, on the back of, face, move from, unpack, \\on their lap, walk out with, go and sit on, walk back into, get to, look down at, drape, \\sleep at, straighten out, rifle through, move around, appear, shake out, proceed to eat, sit by, \\take from, pace back and forth, walk back, film, shuffle, come back in, up on, back onto, \\enjoy, their, exercise, eat in, tap, sit down to watch, on the shelf, dust off, walk back across
          \\ \hline
    \end{tabular}
    }
    \label{app_tab:detail_expanded_cls}
\end{table}


\end{document}
