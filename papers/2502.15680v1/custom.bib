@inproceedings{decop,
author = {Duarte, Andr\'{e} V. and Zhao, Xuandong and Oliveira, Arlindo L. and Li, Lei},
title = {DE-COP: detecting copyrighted content in language models training data},
year = {2024},
publisher = {JMLR.org},
abstract = {How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6\% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72\% for detecting suspect books on fully black-box models where prior methods give approximately 4\% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {475},
numpages = {17},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{compression,
 author = {Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {56244--56267},
 publisher = {Curran Associates, Inc.},
 title = {Rethinking LLM Memorization through the Lens of Adversarial Compression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/66453d578afae006252d2ea090e151c9-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{pythia,
author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
title = {Pythia: a suite for analyzing large language models across training and scaling},
year = {2023},
publisher = {JMLR.org},
abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {102},
numpages = {34},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{highfidelityextraction,
author = {Jagielski, Matthew and Carlini, Nicholas and Berthelot, David and Kurakin, Alex and Papernot, Nicolas},
title = {High accuracy and high fidelity extraction of neural networks},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: accuracy, i.e., performing well on the underlying learning task, and fidelity, i.e., matching the predictions of the remote victim classifier on any input.To extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model--i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights.We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {76},
numpages = {18},
series = {SEC'20}
}



@misc{hayes2024measuringmemorizationprobabilisticdiscoverable,
      title={Measuring memorization through probabilistic discoverable extraction}, 
      author={Jamie Hayes and Marika Swanberg and Harsh Chaudhari and Itay Yona and Ilia Shumailov},
      year={2024},
      eprint={2410.19482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19482}, 
}

@misc{GDPR2016,
  author       = {{Official Journal of the European Union}},
  title        = {{Regulation ({EU}) 2016/679 (General Data Protection Regulation)}},
  year         = {2016},
  month        = {April},
  url          = {https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679} 
}


@article{Fabbrini_Celeste_2020, title={The Right to Be Forgotten in the Digital Age: The Challenges of Data Protection Beyond Borders}, volume={21}, DOI={10.1017/glj.2020.14}, number={S1}, journal={German Law Journal}, author={Fabbrini, Federico and Celeste, Edoardo}, year={2020}, pages={55–65}} <div></div>


@InProceedings{approx_deletion,
  title = 	 { Approximate Data Deletion from Machine Learning Models },
  author =       {Izzo, Zachary and Anne Smart, Mary and Chaudhuri, Kamalika and Zou, James},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2008--2016},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/izzo21a/izzo21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/izzo21a.html},
  abstract = 	 { Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU’s General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and independent of the number of training data n. This is a significant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models. }
}

@inproceedings {unlearning_auditing,
author = {Anvith Thudi and Hengrui Jia and Ilia Shumailov and Nicolas Papernot},
title = {On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning},
booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
year = {2022},
isbn = {978-1-939133-31-1},
address = {Boston, MA},
pages = {4007--4022},
url = {https://www.usenix.org/conference/usenixsecurity22/presentation/thudi},
publisher = {USENIX Association},
month = aug
}


@inproceedings{carlini2022membership,
  title={Membership inference attacks from first principles},
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1897--1914},
  year={2022},
  organization={IEEE}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@inproceedings{yeom2018privacy,
  title={Privacy risk in machine learning: Analyzing the connection to overfitting},
  author={Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  booktitle={2018 IEEE 31st computer security foundations symposium (CSF)},
  pages={268--282},
  year={2018},
  organization={IEEE}
}

@article{salem2018ml,
  title={Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models},
  author={Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael},
  journal={arXiv preprint arXiv:1806.01246},
  year={2018}
}

@article{duan2024membership,
  title={Do membership inference attacks work on large language models?},
  author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2402.07841},
  year={2024}
}

@inproceedings{sablayrolles2019white,
  title={White-box vs black-box: Bayes optimal strategies for membership inference},
  author={Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Ollivier, Yann and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={5558--5567},
  year={2019},
  organization={PMLR}
}

@article{jagielski2024students,
  title={Students parrot their teachers: Membership inference on model distillation},
  author={Jagielski, Matthew and Nasr, Milad and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas and Tramer, Florian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{choquette2021label,
  title={Label-only membership inference attacks},
  author={Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  booktitle={International conference on machine learning},
  pages={1964--1974},
  year={2021},
  organization={PMLR}
}


@misc{duan2024uncoveringlatentmemoriesassessing,
      title={Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Frontier AI Models}, 
      author={Sunny Duan and Mikail Khona and Abhiram Iyer and Rylan Schaeffer and Ila R Fiete},
      year={2024},
      eprint={2406.14549},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.14549}, 
}


@misc{zlib,
  author = "Jean loup Gailly and Mark Adler",
  title = "zlib compression library",
}


@inproceedings{
nguyen2024understanding,
title={Understanding Transformers via N-Gram Statistics},
author={Timothy Nguyen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=WCc440cUhX}
}


@article{samarati1998protecting,
  title={Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression},
  author={Samarati, Pierangela and Sweeney, Latanya},
  year={1998},
  publisher={technical report, SRI International}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{cao2015towards,
  title={Towards making systems forget with machine unlearning},
  author={Cao, Yinzhi and Yang, Junfeng},
  booktitle={2015 IEEE symposium on security and privacy},
  pages={463--480},
  year={2015},
  organization={IEEE}
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@article{eldan2023s,
  title={Who's Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={{Gemini Team} and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{kudugunta2024madlad,
  title={Madlad-400: A multilingual and document-level large audited dataset},
  author={Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


% memorization literature 


@inproceedings {carlini-extraction,
author = {Nicholas Carlini and Florian Tram{\`e}r and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and {\'U}lfar Erlingsson and Alina Oprea and Colin Raffel},
title = {Extracting Training Data from Large Language Models},
booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
year = {2021},
isbn = {978-1-939133-24-3},
pages = {2633--2650},
url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
publisher = {USENIX Association},
month = aug,
}




@inproceedings{secretsharer,
author = {Carlini, Nicholas and Liu, Chang and Erlingsson, \'{U}lfar and Kos, Jernej and Song, Dawn},
title = {The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks},
year = {2019},
isbn = {9781939133069},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models--a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.},
booktitle = {Proceedings of the 28th USENIX Conference on Security Symposium},
pages = {267–284},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {SEC'19}
}

@misc{quantifying,
  doi = {10.48550/ARXIV.2202.07646},
  
  url = {https://arxiv.org/abs/2202.07646},
  
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Quantifying Memorization Across Neural Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{onioneffect,
 author = {Carlini, Nicholas and Jagielski, Matthew and Zhang, Chiyuan and Papernot, Nicolas and Terzis, Andreas and Tramer, Florian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {13263--13276},
 publisher = {Curran Associates, Inc.},
 title = {The Privacy Onion Effect: Memorization is Relative},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/564b5f8289ba846ebc498417e834c253-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}




@misc{panda2024teach,
      title={Teach LLMs to Phish: Stealing Private Information from Language Models}, 
      author={Ashwinee Panda and Christopher A. Choquette-Choo and Zhengming Zhang and Yaoqing Yang and Prateek Mittal},
      year={2024},
      eprint={2403.00871},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{jeopardize,
author = {Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang},
title = {When Machine Unlearning Jeopardizes Privacy},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484756},
doi = {10.1145/3460120.3484756},
abstract = {The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. footnoteOur code is available at urlhttps://github.com/MinChen00/UnlearningLeaks.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {896–911},
numpages = {16},
keywords = {machine learning security and privacy, machine unlearning, membership inference},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@misc{biderman2023emergent,
      title={Emergent and Predictable Memorization in Large Language Models}, 
      author={Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff},
      year={2023},
      eprint={2304.11158},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{counterfactual,
 author = {Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tramer, Florian and Carlini, Nicholas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {39321--39362},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Memorization in Neural Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7bc4f74e35bcfe8cfe43b0a860786d6a-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{eu,
title = {The EU Proposal for a General Data Protection Regulation and the roots of the ‘right to be forgotten’},
journal = {Computer Law \& Security Review},
volume = {29},
number = {3},
pages = {229-235},
year = {2013},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2013.03.010},
author = {Alessandro Mantelero},
keywords = {Right to be forgotten, General Data Protection Regulation, Directive 95/46/EC},
abstract = {The EU Proposal for a General Data Protection Regulation has caused a wide debate between lawyers and legal scholars and many opinions have been voiced on the issue of the right to be forgotten. In order to analyse the relevance of the new rule provided by Article 17 of the Proposal, this paper considers the original idea of the right to be forgotten, pre-existing in both European and U.S. legal frameworks. This article focuses on the new provisions of Article 17 of the EU Proposal for a General Data Protection Regulation and evaluates its effects on court decisions. The author assumes that the new provisions do not seem to represent a revolutionary change to the existing rules with regard to the right granted to the individual, but instead have an impact on the extension of the protection of the information disseminated on-line.}
}

@misc{cali,
  title = {Bill Text},
  howpublished = {\url{https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375}}
}

@misc{canada,
  title = {Privacy Commissioner seeks Federal Court determination on key issue for Canadians’ online reputation},
  year = {October 2018}, 
  howpublished = {\url{https://www.priv.gc.ca/en/opc-news/news-and-announcements/2018/an_181010/}}
}

@misc{right-forgotten,
  title = {Lex Access to European Union law},
  howpublished = {\url{https://eur-lex.europa.eu/eli/reg/2016/679/2016-05-04}}
}


@misc{wikitext,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{team2024codegemma,
  title={Codegemma: Open code models based on gemma},
  author={{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

@INPROCEEDINGS {PII-leakage,
author = {N. Lukas and A. Salem and R. Sim and S. Tople and L. Wutschitz and S. Zanella-Beguelin},
booktitle = {2023 IEEE Symposium on Security and Privacy (SP)},
title = {Analyzing Leakage of Personally Identifiable Information in Language Models},
year = {2023},
volume = {},
issn = {},
pages = {346-363},
abstract = {Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10× more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.},
keywords = {training;data privacy;differential privacy;privacy;analytical models;pipelines;training data},
doi = {10.1109/SP46215.2023.10179300},
url = {https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179300},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}



@article{jagielski2022measuring,
  title={Measuring forgetting of memorized training examples},
  author={Jagielski, Matthew and Thakkar, Om and Tramer, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and others},
  journal={arXiv preprint arXiv:2207.00099},
  year={2022}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}





@inproceedings{preserve-privacy,
author = {Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram\`{e}r, Florian},
title = {What Does it Mean for a Language Model to Preserve Privacy?},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534642},
doi = {10.1145/3531146.3534642},
abstract = {Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2280–2292},
numpages = {13},
keywords = {Data Sanitization, Differential Privacy, Natural Language Processing, Privacy},
location = {<conf-loc>, <city>Seoul</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {FAccT '22}
}

@INPROCEEDINGS{LiRA,
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramèr, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Membership Inference Attacks From First Principles}, 
  year={2022},
  volume={},
  number={},
  pages={1897-1914},
  keywords={Measurement;Training;Privacy;Differential privacy;Computational modeling;Training data;Machine learning},
  doi={10.1109/SP46214.2022.9833649}}

@misc{nasr2023scalable,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{sisa,
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)}, 
  title={Machine Unlearning}, 
  year={2021},
  volume={},
  number={},
  pages={141-159},
  keywords={Training;Data privacy;Privacy;Limiting;Transfer learning;Training data;Stochastic processes},
  doi={10.1109/SP40001.2021.00019}}

@INPROCEEDINGS{OG-unlearning,
  author={Cao, Yinzhi and Yang, Junfeng},
  booktitle={2015 IEEE Symposium on Security and Privacy}, 
  title={Towards Making Systems Forget with Machine Unlearning}, 
  year={2015},
  volume={},
  number={},
  pages={463-480},
  keywords={Training data;Data models;Machine learning algorithms;Data privacy;Learning systems;Computational modeling;Feature extraction;Machine Unlearning;Forgetting System;Adversarial Machine Learning},
  doi={10.1109/SP.2015.35}}

@misc{recite,
      title={Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon}, 
      author={USVSN Sai Prashanth and Alvin Deng and Kyle O'Brien and Jyothir S V au2 and Mohammad Aflah Khan and Jaydeep Borkar and Christopher A. Choquette-Choo and Jacob Ray Fuehne and Stella Biderman and Tracy Ke and Katherine Lee and Naomi Saphra},
      year={2024},
      eprint={2406.17746},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17746}, 
}

@inproceedings{top-p,
  author       = {Ari Holtzman and
                  Jan Buys and
                  Li Du and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {The Curious Case of Neural Text Degeneration},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=rygGQyrFvH},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HoltzmanBDFC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{liu2024rethinkingmachineunlearninglarge,
      title={Rethinking Machine Unlearning for Large Language Models}, 
      author={Sijia Liu and Yuanshun Yao and Jinghan Jia and Stephen Casper and Nathalie Baracaldo and Peter Hase and Yuguang Yao and Chris Yuhao Liu and Xiaojun Xu and Hang Li and Kush R. Varshney and Mohit Bansal and Sanmi Koyejo and Yang Liu},
      year={2024},
      eprint={2402.08787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.08787}, 
}

@misc{shi2024musemachineunlearningsixway,
      title={MUSE: Machine Unlearning Six-Way Evaluation for Language Models}, 
      author={Weijia Shi and Jaechan Lee and Yangsibo Huang and Sadhika Malladi and Jieyu Zhao and Ari Holtzman and Daogao Liu and Luke Zettlemoyer and Noah A. Smith and Chiyuan Zhang},
      year={2024},
      eprint={2407.06460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.06460}, 
}

@inproceedings{Differential_Privacy,
author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
title = {Deep Learning with Differential Privacy},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978318},
doi = {10.1145/2976749.2978318},
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {308–318},
numpages = {11},
keywords = {deep learning, differential privacy},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{Dwork_privacy,
author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
title = {Calibrating noise to sensitivity in private data analysis},
year = {2006},
isbn = {3540327312},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11681878_14},
doi = {10.1007/11681878_14},
abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.Previous work focused on the case of noisy sums, in which f = ∑ig(xi), where xi denotes the ith row of the database and g maps database rows to [0,1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case.The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.},
booktitle = {Proceedings of the Third Conference on Theory of Cryptography},
pages = {265–284},
numpages = {20},
location = {New York, NY},
series = {TCC'06}
}

@inproceedings{lample-etal-2016-neural,
    title = "Neural Architectures for Named Entity Recognition",
    author = "Lample, Guillaume  and
      Ballesteros, Miguel  and
      Subramanian, Sandeep  and
      Kawakami, Kazuya  and
      Dyer, Chris",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1030",
    doi = "10.18653/v1/N16-1030",
    pages = "260--270",
}

  

@inproceedings{
razdaibiedina2023progressive,
title={Progressive Prompts: Continual Learning for Language Models},
author={Anastasia Razdaibiedina and Yuning Mao and Rui Hou and Madian Khabsa and Mike Lewis and Amjad Almahairi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=UJTgQBc91_}
}


@inproceedings{
ke2023continual,
title={Continual Pre-training of Language Models},
author={Zixuan Ke and Yijia Shao and Haowei Lin and Tatsuya Konishi and Gyuhak Kim and Bing Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=m_GDIItaI3o}
}

@inproceedings{
jang2022towards,
title={Towards Continual Knowledge Learning of Language Models},
author={Joel Jang and Seonghyeon Ye and Sohee Yang and Joongbo Shin and Janghoon Han and Gyeonghun KIM and Stanley Jungkyu Choi and Minjoon Seo},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vfsRB5MImo9}
}


@misc{hayes2024inexactunlearningneedscareful,
      title={Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy}, 
      author={Jamie Hayes and Ilia Shumailov and Eleni Triantafillou and Amr Khalifa and Nicolas Papernot},
      year={2024},
      eprint={2403.01218},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.01218}, 
}


@inproceedings {234843,
author = {Supreeth Shastri and Melissa Wasserman and Vijay Chidambaram},
title = {The Seven Sins of {Personal-Data} Processing Systems under {GDPR}},
booktitle = {11th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 19)},
year = {2019},
address = {Renton, WA},
url = {https://www.usenix.org/conference/hotcloud19/presentation/shastri},
publisher = {USENIX Association},
month = jul
}

@inproceedings{
tirumala2022memorization,
title={Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models},
author={Kushal Tirumala and Aram H. Markosyan and Luke Zettlemoyer and Armen Aghajanyan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=u3vEuRr08MT}
}


@inproceedings{
henderson2022pile,
title={Pile of Law: Learning Responsible Data Filtering from the Law and a 256{GB} Open-Source Legal Dataset},
author={Peter Henderson and Mark Simon Krass and Lucia Zheng and Neel Guha and Christopher D Manning and Dan Jurafsky and Daniel E. Ho},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=3HCT3xfNm9r}
}


@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{cooper2024machineunlearningdoesntthink,
      title={Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice}, 
      author={A. Feder Cooper and Christopher A. Choquette-Choo and Miranda Bogen and Matthew Jagielski and Katja Filippova and Ken Ziyu Liu and Alexandra Chouldechova and Jamie Hayes and Yangsibo Huang and Niloofar Mireshghallah and Ilia Shumailov and Eleni Triantafillou and Peter Kairouz and Nicole Mitchell and Percy Liang and Daniel E. Ho and Yejin Choi and Sanmi Koyejo and Fernando Delgado and James Grimmelmann and Vitaly Shmatikov and Christopher De Sa and Solon Barocas and Amy Cyphert and Mark Lemley and danah boyd and Jennifer Wortman Vaughan and Miles Brundage and David Bau and Seth Neel and Abigail Z. Jacobs and Andreas Terzis and Hanna Wallach and Nicolas Papernot and Katherine Lee},
      year={2024},
      eprint={2412.06966},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.06966}, 
}

@misc{gdpr,
  author       = {{Proton AG}},
  title        = {General Data Protection Regulation ({GDPR})},
  url          = {https://gdpr.eu/},
  year         = {2016},
  note         = {[Online; accessed 14-Feb-2025]}
}

@misc{righttobeforgotten,
  title        = {Right to be Forgotten under {GDPR}},
  url          = {https://gdpr.eu/right-to-be-forgotten/},
  year         = {2018},
  note         = {[Online; accessed 14-Feb-2025]},
author={{GDPR}}
}

@misc{linkedinoptout,
  title        = {LinkedIn's Data Opt-Out Information},
  url          = {https://www.linkedin.com/help/linkedin/answer/a5538339},
  year         = {2023},
  note         = {[Online; accessed 14-Feb-2025]},
author={{L}inked{In}}
}


@article{hayes2024inexact,
  title={Inexact unlearning needs more careful evaluations to avoid a false sense of privacy},
  author={Hayes, Jamie and Shumailov, Ilia and Triantafillou, Eleni and Khalifa, Amr and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2403.01218},
  year={2024}
}

@inproceedings{Huang2024DemystifyingVM,
  title={Demystifying Verbatim Memorization in Large Language Models},
  author={Jing Huang and Diyi Yang and Christopher Potts},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
}