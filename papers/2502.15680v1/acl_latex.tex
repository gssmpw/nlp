\pdfoutput=1
\PassOptionsToPackage{svgnames}{xcolor}

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{placeins}
\usepackage{enumitem}


\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}

\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{wrapfig} 
\usepackage{float}
\usepackage{titlesec}


\input{macros}
\input{notation}


\usepackage[textsize=tiny]{todonotes}

\title{Privacy Ripple Effects from Adding or Removing \\ Personal Information in Language Model Training}


\author{
  \textbf{Jaydeep Borkar\textsuperscript{1}},
  \textbf{Matthew Jagielski\textsuperscript{2}},
  \textbf{Katherine Lee\textsuperscript{2}},
  \textbf{Niloofar Mireshghallah\textsuperscript{3}}, \\
  \textbf{David A. Smith\thanks{Equal senior authorship.}\textsuperscript{1}},
  \textbf{Christopher A. Choquette-Choo\footnotemark[1]\textsuperscript{2}} \\
\\
  \textsuperscript{1}Northeastern University,
  \textsuperscript{2}Google DeepMind,
  \textsuperscript{3}University of Washington
\\
 \small{
   \textbf{Correspondence:} \href{mailto:borkar.j@northeastern.edu}{borkar.j@northeastern.edu} and \href{mailto:cchoquette@google.com}{cchoquette@google.com}
 }
}


\begin{document}
\maketitle
\begin{abstract}
\input{sections/abstract}
\end{abstract}




\section{Introduction}

One of the most common methods to adapt large language models like ChatGPT~\citep{achiam2023gpt} and Gemini~\citep{team2023gemini} for specific applications is to fine-tune them on domain-specific datasets.\footnote{See \url{https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning} or \url{https://ai.google.dev/gemini-api/docs/model-tuning}} When these datasets contain private or personal data, models may be at risk of memorizing\footnote{We adopt the definition of “memorization” as used at \url{www.genlaw.org/glossary.html}} and regurgitating~\citep{quantifying} this information.
Though it is common to filter out sensitive information\footnote{We focus on PII as a more concrete privacy risk, though note that our results likely also extend to broader types of sensitive information. We thus use these terms interchangeably.} such as PII~\citep{team2024gemma2}, some sensitive information may still remain~\citep{vakili-etal-2022-downstream}.
Moreover, some downstream tasks, such as healthcare, may require PII, making eliminating PII completely from model training datasets challenging.


\begin{figure}[t]
  \includegraphics[width=1.0\columnwidth]{Figures/fig1.pdf}
  \vspace{-1.5em}
  \caption{
  We explore a phenomenon we call \emph{assisted memorization}, where unique PII that appeared earlier in the training at step $i-1$ and was not extracted at that step becomes extractable at a later step $i$, after fine-tuning on \emph{other} PII.}
  \vspace{-1em}
  \label{fig:taxonomy_assist}
\end{figure}



Modern-day language models deployed in real-world settings are also increasingly dynamic: it is common practice to continually update or retrain them with new and/or additional data~\citep{razdaibiedina2023progressive, ke2023continual, jang2022towards, jin-etal-2022-lifelong-pretraining}, e.g., if new users opt to share their data. 
There may also be data removal requests from existing users under the \emph{right to be forgotten}~\citep{234843}. Here, machine unlearning~\citep{cao2015towards,bourtoule2021machine} is often the proposed solution by enabling post-hoc removal of data (e.g., PII) from neural models after training. 

LLMs are known to memorize and regurgitate personal information and PII~\citep{carlini-extraction,nasr2023scalable}, which is a concrete privacy harm we study. In this literature, little focus has been given to how this may arise dynamically as a part of a machine learning system.
In this work, we study how various actions (continually training on more data, re-training with new data, or re-training after removing data) may influence PII memorization and extraction. We systematically study these operations to determine which improve or worsen the memorization of PII. In particular, we have four \textbf{main contributions}\footnote{Code available at \url{https://github.com/jaydeepborkar/Assisted-Memorization}}:
\begin{enumerate}[itemsep=0.5mm]
    \item We observe the phenomenon of \emph{assisted memorization}: PII may not be memorized immediately after it is seen, but may be memorized later in training (\S\ref{section:assisted_memorization} and Figure~\ref{fig:taxonomy_assist}). We find this is largely influence by \ngram statistics. 

    \item We propose a taxonomy of types of PII memorization that arise while training an LLM and show how they manifest (\S~\ref{section:continuous_training} and Figure~\ref{fig:taxonomy}). 
    
    \item We observe that introducing new PII into training data may worsen extraction of PII (\S\ref{section:opt-in}).
    
    \item We observe that reducing the PII memorization risks for one individual can worsen these risks for another individual (\S\ref{section:onion}).
    
 
   \end{enumerate}

  

\input{related_work}

\input{prelims}

\input{results} 
 

\section{Conclusion}
We study how the actions of continually training on more data, re-training with new data, or re-training after removing data can have ripple effects for privacy. In particular, we propose the phenomenon of \Assisted Memorization where examples that aren't extracted at existing checkpoints can get extracted later. This could create a false impression of privacy for examples that don't get extracted at a particular checkpoint, as training further on similar-appearing examples could lead to their extraction. We also find that including more PII in the training data can degrade privacy of existing PII by putting them at a higher risk of extraction. Furthermore, removing particular PII examples from training data could cause other examples to be extracted. This underscores the need for more holistic audits for memorization, where examples that aren't extracted at a particular timepoint are also evaluated for any potential risks.  


\section*{Limitations}
In this study, we use emails as an example of PII because they are a common form of personal information and can be readily studied using publicly available datasets, e.g., the Enron corpus. We do not examine other forms of PII, such as credit card numbers or mailing addresses, partly because they are not publicly available. However, analyzing these types of PII is important to determine whether certain categories are more vulnerable to the memorization risks identified here. We believe that our methods will generalize to other forms of PII with minor adjustments. We also observe a phenomenon akin to \emph{onion memorization}~\citep{onioneffect}, where removing particular emails from the dataset and retraining the model (\emph{exact unlearning}~\cite{sisa}) can cause new emails to be extracted. A promising direction is to investigate whether this effect persists under \emph{approximate} unlearning techniques (e.g.,~\citep{hayes2024inexact}), where the model is not fully retrained from scratch. Furthermore, our focus here is solely on extraction risks for training-data emails, but other generated or partially memorized emails could also pose privacy concerns---particularly if they can serve as keys to uncover additional information about specific individuals.

\section*{Ethics Statement}


We rely on the publicly available Enron Corpus to create our fine-tuning datasets, acknowledging that some of its contents may include sensitive or personally identifiable information. To mitigate privacy risks, we follow standard diligence practices for data handling. While no additional raw text or private details are disclosed beyond those already publicly released, we analyze memorization specifically to highlight risks inherent in large language models, rather than to reveal more personal data. Our experiments use established public models and datasets (GPT-2 family, Gemma 2B, Llama 3 8B, Wikitext, and Pile of Law) to facilitate reproducibility while maintaining responsible data practices. We align our work with accepted norms for ethical use of legacy datasets like Enron and emphasize the importance of privacy-preserving training and unlearning techniques for future systems.  

\bibliography{anthology, custom}

\clearpage
\appendix
\input{appendix}

\end{document}
