
\section{Hyperparameters that Influence PII Extraction}
\subsection{Greedy vs. Top-\textit{k} Sampling}
\label{section:greedyvstopk}

Model owners can employ either deterministic decoding such as greedy or stochastic sampling methods (such as top-\textit{k} \cite{fan-etal-2018-hierarchical} or top-\textit{p} \cite{top-p}) to improve the quality of the generated text. Several commercial APIs providing text-generation access to models such as ChatGPT\footnote{\url{https://platform.openai.com/docs/guides/text-generation}}, Gemini\footnote{\url{https://ai.google.dev/gemini-api/docs/text-generation?lang=python}}, and Claude\footnote{\url{https://docs.anthropic.com/en/api/complete}} use a combination of \texttt{top-\textit{k}} and \texttt{top-p} parameters to generate text and extraction rates vary across sampling schemes~\citep{hayes2024measuringmemorizationprobabilisticdiscoverable}. This makes it essential to study how PII extraction varies across different sampling methods. We find that we can extract significantly more PII using top-\textit{k} sampling than greedy decoding.  

We draw the following comparisons: (1) The ratio of total emails extracted using top-\emph{k} sampling compared to greedy decoding; (2) Total emails extracted using a fixed set of 25,000 prompts for both sampling methods; and (3) Total emails generated by both sampling methods when conditioned on same 25,000 prompts. 


It can be seen in Figure \ref{fig:A_topkvsgreedy} that top-\textit{k} can extract emails over 800 times higher than greedy decoding. Top-\textit{k} also consistently generates more unique emails than greedy. Model owners might employ top-\textit{k} sampling as it produces more diverse and higher-quality text compared to greedy. However, this approach may pose privacy risks, such as increased memorization and leakage of personal information. 


\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/subplots_e1_A.pdf}  
  \caption {(Left) We can extract significantly more emails with top-\emph{k} than with greedy decoding using the same set of prompts. (Middle) We can extract up to 800 times more emails using top-\emph{k}. (Right) top-\emph{k} generates more emails than greedy for the same amount of emails seen during training. The x-axis denotes a separate model obtained after adding an additional 10\% of total emails in the training data.}
  \label{fig:A_topkvsgreedy}
\end{figure*}






\section{More Results on PII Memorization in Continuous Training.}
\label{sec:appendix_moreresults_continued}
\textbf{More results from \S~\ref{section:continuous_training}}: We fine-tune various models on two datasets—Wikitext and the Pile of Law—and show that our findings are generalizable. We only use greedy decoding for sampling from these models. 

\paragraph{GPT-2 XL trained on the Pile of Law dataset:} Figure \ref{fig:B_pol} shows that our results are generalizable also on the Pile of Law dataset \citep{henderson2022pile}. We extract the \texttt{congressional\_hearings} instance from the dataset and insert enron emails in it according to our setup in \S~\ref{sec:prelim} while keeping the total number of tokens in the dataset the same as our original Wikitext dataset.  

 
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_pol_greedy_area.pdf}  
  \caption {Different memorization categories during continuous training for GPT-2 XL trained on the Pile of Law + Enron emails.}
  \label{fig:B_pol}
\end{figure*}


\paragraph{Llama3 8B and Gemma 2B models trained on our original dataset (WikiText + Enron emails):} Our results generalize to the current state-of-the-art models, including Llama3 with 8B parameters (Figure \ref{fig:B_llama3}) and Gemma 2B base model \citep{team2024gemma} (Figure \ref{fig:B_gemma}). 

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_Llama3_area.pdf}  
  \caption {Different memorization categories during continuous training for Llama3 8B trained on WikiText + Enron emails.}
  \label{fig:B_llama3}
\end{figure*}

 
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_Gemma_area.pdf}  
  \caption {Different memorization categories during continuous training for Gemma 2B trained on WikiText + Enron emails.}
  \label{fig:B_gemma}
\end{figure*}


\paragraph{GPT-2 Large 774M, Medium 355M, and Small 124M models trained on our original dataset (WikiText + Enron emails):} We also train the remaining members from the GPT-2 model family: Large (Figure \ref{fig:B_large}), Medium (Figure \ref{fig:B_medium}), and Small (Figure \ref{fig:B_small}). We observe that assisted memorization becomes less prominent in smaller models. 

\paragraph{Rate of \novel vs. \assisted memorization:} We find that the rate of \assisted memorization is higher than that of \novel memorization and the difference increases as training progresses. Figure~\ref{fig:imm_vs_assi} \& Figure~\ref{fig:auc} show this trend for different models. 

\paragraph{Forgetting of \novel vs. \assisted memorized examples:} We do not observe any significant difference between the forgetting rates of both. Figure~\ref{fig:forgetting_rate_gpt2} \& Figure~\ref{fig:forgetting_rate_pol} show this for GPT-2 XL, Figure~\ref{fig:forgetting_rate_gemma} shows this for Gemma 2B, and Figure~\ref{fig:forgetting_rate_llama3} shows this for Llama3 8B. 

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_L_area.pdf}  
  \caption {Different memorization categories during continuous training for GPT-2 Large trained on WikiText + Enron emails.}
  \label{fig:B_large}
\end{figure*}

 
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_M_area.pdf}  
  \caption{Different memorization categories during continuous training for GPT-2 Medium trained on WikiText + Enron emails.}
  \label{fig:B_medium}
\end{figure*}

 
\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/B_S_area.pdf}  
  \caption {Different memorization categories during continuous training for GPT-2 Small trained on WikiText + Enron emails.}
  \label{fig:B_small}
\end{figure*}


\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/memorization_ratio_plot.pdf}  
  \caption {Ratio of the rate of \assisted to \novel memorization. We observe that a large fraction of memorization in multiple models is \assisted. Note that some models at specific checkpoints had no immediate memorization.}
  \label{fig:imm_vs_assi}
\end{figure*}

 
\begin{figure*}[t]
 \centering
  \includegraphics[width=0.6\textwidth]{Figures/auc_plot.pdf}  
  \caption {Extraction rates for \assisted and \novel memorization denoting the area under curve when continuously trained for three epochs. GPT-2 XL W and P denote the models trained on WikiText and Pile of Law respectively. On average, we observe models have equal or more assisted memorization.}
  \label{fig:imm_vs_assi_auc}
\end{figure*}



\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/forgetting_rate.pdf}  
  \caption {Forgetting rates for GPT-2 XL trained on WikiText + Enron emails. We do not observe any notable difference in the forgetting rates, with \assisted (15.54\%) being marginally higher than \novel (12.08\%).}
  \label{fig:forgetting_rate_gpt2}
\end{figure*}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/forgetting_rate_pol.pdf}  
  \caption {Forgetting rates for GPT-2 XL trained on the Pile of Law + Enron emails. We do not observe any notable difference in the forgetting rates, with \assisted (21.06\%) being marginally higher than \novel (16.05\%).}
  \label{fig:forgetting_rate_pol}
\end{figure*}


\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/forgetting_rate_gemma.pdf}  
  \caption {Forgetting rates for Gemma 2B trained on WikiText + Enron emails. We do not observe any notable difference in the forgetting rates, with \assisted (20.69\%) being marginally higher than \novel (18.05\%).}
  \label{fig:forgetting_rate_gemma}
\end{figure*}


\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/forgetting_rate_llama3.pdf}  
  \caption {Forgetting rates for Llama3 8B trained on WikiText + Enron emails. We do not observe any notable difference in the forgetting rates, with \assisted (16.7\%) being marginally higher than \novel (14.52\%).}
  \label{fig:forgetting_rate_llama3}
\end{figure*}





\FloatBarrier
\section{More Details on Assisted Memorization}
\label{sec:additional-assisted}
We consider the following set of features for our logistic regression model.
\begin{enumerate}
    \item 2-, 3-, and 4-grams that overlap between tokens in an email and tokens in the data observed up to checkpoint $i-1$ (denoted as 2-gram$_{prev}$, 3-gram$_{prev}$, 4-gram$_{prev}$). Additionally, we compute the overlap between tokens in an email and tokens in the data seen between checkpoints $i-1$ and $i$ (denoted as 2-gram$_{ft}$, 3-gram$_{ft}$, 4-gram$_{ft}$). 
    \item Counts of \texttt{lastname} in the data seen up to checkpoint $i-1$\ (denoted as \texttt{lastname}$_{prev}$) as well as in the batches seen between checkpoints $i-1$ and $i$ (denoted as \texttt{lastname}$_{ft}$). 
    \item For each email, the number of times its domain (e.g., \texttt{enron.com}) occurs in the data up to checkpoint $i$ (denoted as domain$_{count}$).  
\end{enumerate}

\textbf{Dataset Creation for Logistic Regression Model.} We create a dataset by collecting each assisted-memorized email as a positive example and non-memorized emails that share the same \texttt{firstname} as negative examples. We normalize features by the maximum value. We obtain 192 assisted memorized emails and 886 non-memorized emails in total.
We train a logistic regresion model on this dataset after downsampling the non-memorized emails to achieve a 1:3 ratio between positive and negative samples. On each trial, we re-downsample the negative emails. We run 10 trials following 5-way cross-validation approach. Table~\ref{tab:weights} shows the weights of our classifier. 

\begin{table*}[t]
  \centering
  \begin{tabular}{lcp{12cm}}
    \hline
    \textbf{Feature} & \textbf{Weight} & \textbf{Description} \\ 
    \hline
    2-gram$_{ft}$ & 7.029   &  2-grams that overlap between tokens in an email and tokens in the data seen between checkpoints $i-1$ and $i$. \\ 
    3-gram$_{ft}$ & 0.887  & 3-grams that overlap between tokens in an email and tokens in the data seen between checkpoints $i-1$ and $i$. \\ 
    4-gram$_{ft}$ & 0.682  & 4-grams that overlap between tokens in an email and tokens in the data seen between checkpoints $i-1$ and $i$. \\ 
    2-gram$_{prev}$ & -0.599  & 2-grams that overlap between tokens in an email and tokens in the data observed up to checkpoint $i-1$. \\ 
    3-gram$_{prev}$ & -0.651  & 3-grams that overlap between tokens in an email and tokens in the data observed up to checkpoint $i-1$. \\  
    4-gram$_{prev}$ & -2.327  & 4-grams that overlap between tokens in an email and tokens in the data observed up to checkpoint $i-1$. \\ 
    \texttt{lastname}$_{prev}$ & 1.235 & Counts of \texttt{lastname} in the data seen up to checkpoint $i-1$. \\ 
    \texttt{lastname}$_{ft}$ & 0.900 & Counts of \texttt{lastname} in the data seen between checkpoints $i-1$ and $i$. \\ 
    domain$_{count}$ & 1.683 & The number of times its domain (e.g., \texttt{enron.com} occurs in the data up to checkpoint $i$.  \\ \hline
  \end{tabular}
\caption{Weights of features used to train our logistic regression model to predict \assisted memorization in \S\ref{section:sub_assisted}.}
  \label{tab:weights}
\end{table*} 
\FloatBarrier

\section{Additional Results on Adding More PII Increases Extraction Risks.}
\label{section:addition_moreresults_retrained}
\textbf{More results from \S~\ref{section:opt-in}}: We show that adding more PII can lead to an increased extraction for different models and datasets. We report our results for GPT-2 XL trained on WikiText + Enron emails (Figure~\ref{fig:add_xl}), GPT-2 XL trained on the Pile of Law + Enron emails (Figure~\ref{fig:add_pol}), and Gemma 2B trained on WikiText + Enron emails (Figure~\ref{fig:add_gemma}).  


\begin{figure*}[t!]
  \includegraphics[width=0.48\linewidth]{Figures/add_topk.pdf} \hfill
  \includegraphics[width=0.48\linewidth]{Figures/add_greedy.pdf}
  \caption {Adding more PII leads to more extraction in GPT-2 XL trained on WikiText + Enron emails for both top-\emph{k} sampling (left) and greedy decoding (right).}
  \label{fig:add_xl}
\end{figure*}

\begin{figure*}[t!]
  \includegraphics[width=0.48\linewidth]{Figures/add_topk_pol.pdf} \hfill
  \includegraphics[width=0.48\linewidth]{Figures/add_greedy_pol.pdf}
  \caption {Adding more PII leads to more extraction in GPT-2 XL trained on the Pile of Law + Enron emails for both top-\emph{k} sampling (left) and greedy decoding (right).}
  \label{fig:add_pol}
\end{figure*}

\begin{figure*}[t!]
  \includegraphics[width=0.48\linewidth]{Figures/add_greedy_gemma.pdf} \hfill
  \caption {Adding more PII leads to more extraction in Gemma 2B trained on WikiText + Enron emails. The results are for greedy decoding.}
  \label{fig:add_gemma}
\end{figure*}

\FloatBarrier


\section{Memorized Samples}
Figure \ref{fig:topk} shows some examples of memorized emails that are extracted from the GPT-2 XL model using top-\emph{k} sampling. Figure~\ref{fig:greedy} shows this for greedy decoding.  


\FloatBarrier








\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/topk.pdf}  
  \caption {Emails extracted using top-\emph{k} sampling from the GPT-2 XL model. \textbf{Generation}: a subset of tokens that fall in the vicinity of memorized emails are selected from 256 tokens for demonstration purposes. Emails in red are extracted from training data. Emails in green indicate they don't belong to our training data. \textbf{Org} denotes the company/organization that memorized email addresses belong to.}
  \label{fig:topk}
\end{figure*}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{Figures/greedy.pdf}  
  \caption {Emails extracted using greedy decoding for the GPT-2 XL model. \textbf{Generation}: a subset of tokens that fall in the vicinity of memorized emails are selected from 256 tokens for demonstration purposes. Emails in red are extracted from training data. Emails in green indicate they don't belong to our training data. \textbf{Org} denotes the company/organization that memorized email addresses belong to.}
  \label{fig:greedy}
\end{figure*}

