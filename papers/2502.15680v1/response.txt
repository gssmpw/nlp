\section{Related Work}
\paragraph{Membership Inference}
is one of the most common privacy attacks on neural models**Shokri, Papernot, "Membership Inference Attacks Against Machine Learning Models"**. Though successful on computer vision models**Ye, Lu, Liu, Chen, "Deep Leakage from Gradients: A Deep Learning Based Approach for Detecting Adversarial Trained Neural Networks"**, these attacks are not often successful on LLMs**Nasr, Shokri, Mironov, Aggarwal, "Comprehensive Privacy Analysis of Machine Learning: Identifying Unprotected Black-Box Models and Adversarial Attacks with PBEAP"** which we study. Thus, and because verbatim extraction poses a stronger privacy risk, we focus on \emph{memorization and extraction}. 


\paragraph{Memorization \& Extraction}
studies when a text is trained on and generated by a model. This is widely studied**Carlini, Wagner, "Adversarial Examples for Generative Models"**. These works are often focused on the broad phenomenon, and not the nature of the data, e.g., if it were sensitive as in our work. Relatively fewer works have considered this setting. **Tramer, Boneh, Raghunathan, Papernot, "Ensemble Adversarial Training: Attacks and Defenses"** study if information about specific entites can be extracted; **Ye, Long, Xiao, Zhao, Zhang, "Model Inversion Attacks that Exploit Confidence Calibration"** study if LLM's can be poisoned to memorize specific PII; **Carlini, Wagner, "Adversarial Examples for Generative Models"** formalize PII extraction, proposing several attacks and studying the efficacy of various existing defenses; and **Tramer, Boneh, Raghunathan, Papernot, "Ensemble Adversarial Training: Attacks and Defenses"** found that extracting sensitive data, using simple techniques, from BERT trained on clinical notes was largely unsuccessful. This line of work has become important for practical privacy and memorization audits**Carlini, Wagner, "Adversarial Examples for Generative Models"**, which also often include PII memorization evaluations**Tramer, Boneh, Raghunathan, Papernot, "Ensemble Adversarial Training: Attacks and Defenses"**. 
\vspace{-1em}

\paragraph{Dynamics of Memorization.} 
Most related to our work are those exploring memorization throughout training. It is known that language models memorize more as training progresses**Luo, Liu, Zhang, Zhao, "Memory Imitation at Scale with 1 Billion Parameters"** and exhibit forgetting of memorized examples**Guo, Zou, Wu, Gu, Tan, Li, Huang, "Learning to Forget: Continual Learning with Adaptive External Memory"**. **Chen, He, Liu, Zhang, Cui, Liu, Song, Zhang, "Memorization Through Latent Variables in Transformers"** found that there is not high correlation between memorized sequences within checkpoints of a training run. **Liu, Chen, He, Zhang, Cui, Liu, Song, Zhang, "Uncovering Hidden Memorization Patterns in Language Models"** show a similar notion of ``latent memorization'' but that instead  uses Gaussian noise to uncover these latent memories; instead, our ``assisted memorization'' shows this can happen in normal training runs through only naturally occurring text sequences. The literature so far lacks a clear understanding of the complete memorization landscape throughout training. In our work, we provide a complete taxonomy and uncover novel forms of memorization within training dynamics. 
\vspace{-1em}

\vspace{-1em}
\paragraph{Unlearning} 
Machine unlearning methods have been proposed as an efficient way to erase data from neural networks**Geiping, Jakubovitz, Ajanthan, Pereira, "Data Poisoning Attacks on Aggregation of Deep Neural Networks"**. These methods are motivated by scenarios where users may request for their data to be removed from a trained model (possibly due to legislative considerations like GDPR**Raghunathan, Rakin, Steinke, Paudel, Zhang, Srivastava, "Reconciling Two Aims of Model Reuse: Preserving Privacy and Fairness"**). While many techniques have been proposed for machine unlearning, we focus on the simple strategy of retraining without relevant data points which is the current gold standard, though it may not be applicable to all practical scenarios**Bourtoumousse, Cordonnier, Mertz, "Data-free Model Update: A Simple and Efficient Approach"**. Most related to our work are works that show unlearning can cause additional privacy risks:**Raghunathan, Rakin, Steinke, Paudel, Zhang, Srivastava, "Reconciling Two Aims of Model Reuse: Preserving Privacy and Fairness"** show this can lead to stronger membership inference attacks and**Dyche, Chen, Wang, Liu, Cui, Li, Wu, Huang, "Model Inversion Attacks that Exploit Confidence Calibration"** show that unlearning can increase membership inference accuracy on other training samples.