\section{A Dynamic Lens on PII Memorization}
\label{section:continuous_training}


Production language models today consist of many training stages (pre-training, post-training, product-specific fine-tuning, etc.) and may be continually updated or refreshed with new data, e.g., to incorporate new human data using RLHF~\citep{stiennon2020learning}.
These stages may incorporate varying degrees of personal information.
This raises the question: \emph{how does memorization of sensitive data like PII evolve in this dynamical system?}




\paragraph{Continuous Training Setup.}
To study this question, we use the simplest setup that generally captures all of the above scenarios: we study memorization throughout supervised fine-tuning. We train a model by keeping the rate of emails seen constant and save checkpoints at regular intervals (for efficiency, only every 10\% of training).
Details on the dataset construction are in \S\ref{sec:prelim}. 


\subsection{Categorizing Memorization Phenomena}
\label{sec:taxonomy}


Memorization analysis is typically based on \emph{only} the final model, in both academia~\citep{quantifying} and industry~\citep{team2024gemini,dubey2024llama3herdmodels,team2024gemma2}.
We now present our taxonomy for dynamic memorization analysis and use it to analyze how memorization manifests throughout continual training.

We begin by looking at the first step of training. There are but two options for any PII seen in this step: for the model to memorize it, or not. We call this type of memorization \novel, since by construction our dataset contains this email exactly once. Now, say this model were trained for another step. This new model may observe new (\novel) memorization. Beyond this, we would expect that the rest of the memorization overlaps with the prior model, which we call \retained memorization, similar to analysis in~\citet{biderman2023emergent}. Finally,~\citet{jagielski2022measuring} would tell us that we may also expect some sequences to be \forgotten.
\emph{However, we observe an additional phenomenon: \assisted memorization.} This occurs when PII not memorized at the immediate checkpoint becomes extractable later in training. We discuss this in more detail in \S~\ref{section:assisted_memorization}. Figure~\ref{fig:taxonomy} shows our complete memorization taxonomy. 


\subsection{Experimental Results}
Using this taxonomy of \novel, \retained, and \forgotten memorization (and \assisted memorization), we characterize all the extracted emails we observe throughout training (using the setup described above). 
Our results are shown in Figure~\ref{fig:B_greedy_normal}. We observe that there is a trend that more \novel memorization occurs near the beginning of training, whereas there is a lower rate of \novel memorization later in training.
This trend is particularly true for larger models, likely because these models memorize faster.

We also find that models are constantly \forgetting. Throughout the entirety of training (including the beginning and end), many models (see Appendix~\ref{sec:appendix_moreresults_continued} for more results on other models and datasets) exhibit a cycle of \forgetting and \novel memorization. This result sheds new light into the dynamic view of memorization: which samples are memorized by a model may be more a function of stochasticity than previously thought. The choice of which model to release may play a larger role in determining which samples are memorized, due to which samples were \forgotten or re-memorized than previously thought due to the stochasticity in data sampling.

\paragraph{Not all memorization occurs immediately.} When using our taxonomy to analyze memorizing, we observe that a significant fraction of memorization samples are not classified by these three categories. This leads to an another interesting finding: a lot of memorization is \emph{not} \textcolor{orange}{immediately} memorized.
In other words, at a given step, other text that \emph{was not trained on at this step} is now extractable by the model. %

\paragraph{Forgetting and Re-Extraction of PII.}
Our results in Figure~\ref{fig:B_greedy_normal} show that LLMs do forget some of the previously memorized PII as training progresses. 
Prior work has shown that some examples memorized early in training may be forgotten after additional training~\citep{jagielski2022measuring}. 
Further, we also observe that some \emph{forgotten} emails get \emph{re-extracted} when there is \ngram overlap between tokens from the email and tokens in the data during further training. This phenomenon is illustrated in Figure~\ref{fig:rememorization}, which shows how previously extracted samples that the model later \emph{forgets} can reappear at subsequent checkpoints. Each cell indicates the percentage of emails extracted both by the corresponding checkpoint and the reference checkpoint (diagonal cell). Since each diagonal cell serves as its own reference, its value is always 1. 

\begin{figure*}[t]
%\vspace{-1em}
  \includegraphics[width=1.0\textwidth]{Figures/B_greedy_area.pdf}  
  %\vspace{-2em}
  \caption{\textbf{Tracking memorization throughout training with our taxonomy.} The stacked bars show how many newly memorized emails are \novel, \retained, and \assisted, while red denotes forgotten emails since the last checkpoint. We see large amounts of \assisted memorization occurring later in training, underscoring that PII is not always memorized immediately. \textit{Takeaway:} memorization is more dynamic and stochastic than often assumed, with ongoing cycles of \textcolor{red}{forgotten} and newly \assisted emails.}
  %\vspace{-1.5em}
  \label{fig:B_greedy_normal}
\end{figure*}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\columnwidth]{Figures/rememorization.pdf}
  %\vspace{-1em}
  \caption{\textbf{Forgotten PII is re-extracted later.} The diagonal values \(d_{ii}\) represent the total extraction at each checkpoint; off-diagonal cells show which fraction of emails remain memorized at both checkpoints. 
  \textit{Takeaway:} memorized PII can sometimes slip out of memory, only to reappear once certain overlapping tokens occur in future training steps.}
  \label{fig:rememorization}
  %\vspace{-1em}
\end{figure}




\section{Assisted Memorization: Training on One's PII Can Reveal Another's}
\label{section:assisted_memorization}

In Figure~\ref{fig:B_greedy_normal}, we see that a large fraction of memorization is \assisted. This is especially true later in training, where we observe that more memorization is \assisted than \novel, specifically a mean rate of $0.03$ for \assisted compared to $0.01$ for \novel. This finding is not model- or data-specific, as our results in Appendix~\ref{sec:appendix_moreresults_continued} (e.g., Figures~\ref{fig:imm_vs_assi} and \ref{fig:imm_vs_assi_auc}) show similar trends.

The existence of \assisted memorization brings to light a deeper privacy concern. One may expect that data seen earlier is less vulnerable to privacy risks through a form of ``recency bias'' (implied by \forgetting effects). 
Our findings of \assisted memorization, however, show that this may not always be the case; the existence of this effect with sensitive data like PII is of particular concern because it shows that downstream training stages must be careful how they may elicit the extraction of earlier training data. The most common practical scenario for this is in the pre-training/fine-tuning setup that current LLMs undergo. Our results show that fine-tuning even on natural (non-adversarially) constructed training datasets can uncover the extraction of PII in pre-training data. Prior work~\citep{nasr2023scalable} only showed this may be possible with adversarial constructions. Pragmatically, our results also show that privacy and memorization audits, especially when PII is of concern, should encompass all data in the training history, and not just data from the most recent training stage.



\subsection{Assisted Memorization Is Not Simply Delayed}
\label{sec:not-delayed}
Above, we found that extraction can be elicited at training steps later than where a piece of sensitive text was seen during training, in what we call assisted memorization. Here, we explore to what degree this assisted memorization is assisted by particular text in the training data, or if it was inevitable and simply delayed.

We find emails that were identified as assisted memorization at various points in training. Our aim is to re-perform training between when they were first seen and when they were later extractable by selecting entirely fresh data from the remainder of the (unseen) training dataset. Then, we can observe if only this unique set of data elicited the memorization or if any batch could.

We know when data samples were first seen from data sampling. Then, we must identify exactly when each email became extractable, as any training beyond this may lead to \forgetting. Given that we only checkpoint our models every 10\% of training, for efficiency, we do not have this a priori. 
To determine this, we use a binary search performing an extraction test on each iteration of the search. This significantly reduces the overhead as the extraction test is expensive (recall we prompt the model thousands of times as described in \S\ref{sec:prelim}). 

Overall, we run this procedure on four unique emails and with seven trials each. We find that emails became extractable in only $35.7\%\pm15.9$ of them on average. While this refutes the idea that there may be a single unique set of data that leads to assisted memorization, this shows that most sets of data do not lead to it. Next, we explore what characteristics the successful trials share.


\subsection{Assisted Memorization Is Triggered by Training on Specific \ngrams}
\label{section:sub_assisted}
Our analysis here is inspired by~\citet{lee-etal-2022-deduplicating}, who show that data repetitions (duplication) heavily influence memorization of text. While our data setup in \S\ref{sec:prelim} has no exact duplicates of these emails, there can still be overlaps of important \ngrams.

\paragraph{Causally Removing \ngrams.}
To study this, we perform a causal intervention whereby we remove all training sequences that have high \ngram overlap with emails identified as assisted memorization. We use a similar setup to the previous \S\ref{sec:not-delayed} except that we notably remove any text that overlaps with the assisted memorized emails. For each trial of this experiment, we select a different checkpoint $\lm_i$ throughout our continuous fine-tuning setup; let $\dataset_i$ be the set of training sequences used to train $\lm_i$ from $\lm_{i-1}$. We take all emails identified as assisted memorization on $\lm_i$; for each, we construct a simple regex-based filter that checks for names in the email address based on common email formatting patterns (e.g., name@gmail.com or firstname.lastname@gmail.com). We use these regex filters to remove any text in $\dataset_i$ and then retrain $\lm_i$ from $\lm_{i-1}$ on this new dataset. 

Across all 30 checkpoints and 5 seeds, we find a total of 177 emails that were assisted memorized. After intervening to remove overlapping \ngrams from batch $D_{i}$, all but 10 of these assisted memorized emails were no longer memorized. 


\paragraph{Features Associated with Memorization} Next, we ask: when multiple emails share a \texttt{firstname}, why might a particular email with a different \texttt{lastname} get assisted memorized over another? For example, why might \texttt{john.mccarthy@gmail.com} be memorized over \texttt{john.williams@gmail.com}. We train a simple logistic regression model on features capturing \ngrams overlaps, last-name counts, and domain counts for all assisted memorized emails (positives) and those not memorized (negatives). More details are in Appendix~\ref{sec:additional-assisted}. 



Our logistic regression model is trained to predict assisted memorized emails from a dataset consisting of these emails labeled as positive, and other emails sharing the same \texttt{firstname} but a different \texttt{lastname} as negatives. We use a standard 5-way cross validation setup with 10 trials. Full details are in Appendix~\ref{sec:additional-assisted}. The model achieves a precision of 0.937 and recall of 0.874 indicating high success. 

    





In Figure \ref{fig:assisted_scatter}, we visualized the logistic regression model's score against the email likelihood from $\lm$, computed against the successful prompt that led to extraction. This shows that \assisted memorization emails tend to be well classified from these simple features.
We observe that \ngram statistics were the most important feature, further supporting our conclusions above (see Table~\ref{tab:weights} of Appendix~\ref{sec:additional-assisted} where we report the feature weights).







\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{Figures/assisted_scatter.pdf}
  %\vspace{-0.9em}
  \caption{\textbf{Overlap features predict which emails are assisted memorized.} We plot a logistic-regression score (x-axis) vs.\ conditional likelihood (y-axis). Emails that become assisted memorized (red) exhibit higher \ngrams overlap (i.e.\ higher model score), whereas those not memorized (grey) have lower overlap. 
  \textit{Takeaway:} overlapping \ngrams in future training data strongly drive which PII is triggered to appear in the model’s output.}
  \vspace{-1em}
  \label{fig:assisted_scatter}
\end{figure}






\section{Do PII Opt-ins/Opt-outs Impact Extraction?}
\label{section:in_out}
\subsection{Contributing More Data via Opt-ins}
\label{section:opt-in}
If many new users opt-in to contribute data to a model, then the model owner may want to incorporate new information (and sometimes, new PII) into the finetuning pipeline. 
One of the simplest ways to do this is by adding the new PII to existing training data and re-finetuning the model from scratch. From our results in \S\ref{section:assisted_memorization}, we know that continuing to train a model on additional PII could lead to increased extractability of previously unextracted PII. 
In this section, we study how retraining with additional PII changes the extractability of prior data.\looseness=-1

\paragraph{Setup} 
To mimic the above scenario, we design a \textbf{Retraining Experiment} where we add more emails to the existing dataset and re-finetune the model on the updated dataset. %
We write \(D_{x\%}\) as the finetuning dataset containing $x\%$ of the emails from the global set of emails $X$. We construct 10 different finetuning datasets containing increasing amounts of emails: $D_{10\%}, D_{20\%}, \cdots, D_{100\%}$. In $D_{x\%}$, we include $x\%$ of the global pool of emails $X$, such that, if $a<b$, all emails that are found in $D_{a\%}$ are also found in $D_{b\%}$. Before constructing these datasets, we randomly shuffle the emails in $X$ to ensure a uniform distribution of emails in each dataset.\looseness=-1

Next, we train ten distinct models \(M_{1}\) to \(M_{10}\), where $M_i$ is trained on $D_{10i\%}$ for three epochs, following the same training setup described in Section \ref{sec:prelim}. We highlight that the only change between these models is the additional emails. Otherwise, we use the same training process and the same prompts for all models when decoding.

\paragraph{Adding More PII Increases Extraction of Existing PII.}
We report the results of our experiment in Figure~\ref{fig:add_topk_greedy_e3}, for models finetuned for three epochs (more results in Appendix~\ref{section:addition_moreresults_retrained}). We highlight two major findings.

First, we find that the number of extracted emails increases substantially with the amount of PII contained in the model's finetuning set. This can be seen on the diagonals of Figure~\ref{fig:add_topk_greedy_e3}, which show the total amount of PII extracted from the relevant model. For top-$k$ sampling, we see that 283 emails are extracted from $M_{10}$, compared to only 57 at $M_{5}$, which was trained on half as many emails---the increase in extraction from top-$k$ sampling is superlinear in the fraction of emails included in the model's finetuning set. The increase is still substantial, but not superlinear, for greedy sampling.


Our second and main finding is that the inclusion of more PII leads to \emph{existing} PII being at higher risk of extraction from top-$k$ sampling. This can be seen from the general positive trend in extracted emails for each dataset $D_{x\%}$ along the $x$ axis. To validate this result, we run a binomial hypothesis test, for whether top-$k$ sampling extracts more emails from $D_{i\%}$ when run on $M_{j}$ ($j > i$) than when run on $M_{i}$. With 45 such comparisons, 41 show more extraction for models which see more emails ($p < 10^{-8}$, and $p < 10^{-4}$ for 1 and 2 epochs).



\begin{figure}[t]
  \centering
  \includegraphics[width=1\columnwidth]{Figures/add_e3_topk_greedy.pdf}
  \vspace{-1em}
  \caption{\textbf{Adding more PII leads to more extraction.} Each row corresponds to a dataset $D_{x\%}$, and each column corresponds to the model $M_j$ trained with $j\times 10\%$ of the emails. The values show how many emails in $D_{x\%}$ are extracted by $M_j$. 
  \textit{Takeaway:} introducing new PII during re-finetuning (moving along the x-axis) also increases extraction \emph{of old PII} that was already present in the training set. This effect can increase extraction by a factor of over $7\times$ in our settings, as seen in the extraction of emails in $D_{10\%}$ from $M_{10}$.}
  \label{fig:add_topk_greedy_e3}
\end{figure}






 


\begin{figure}[t]
  \centering
  %\vspace{-1em}
  \includegraphics[width=0.7\columnwidth]{Figures/removal_greedy.pdf}
  %\vspace{-1em}
  \caption{Removing extracted PII from the training data and retraining can lead to new memorized PII. 
  After four removal-and-retrain cycles (\textcolor{red}{Update~1}--\textcolor{red}{4}), 
  no additional PII is extracted under the same 25k prompts and greedy decoding. 
  \textcolor{blue}{START} denotes the original model.}
  \vspace{-1em}
  \label{fig:onion}
\end{figure}


\subsection{Protecting PII via Opt Outs}
\label{section:onion}

As data opt-outs are becoming increasingly common on the web~\citep{linkedinoptout}, %
we first study how removing a user's PII from the training data can inadvertently trigger extraction of additional PII.
We then investigate factors that correlate to PII becoming extractable once similar PII is removed.\looseness=-1 %


\paragraph{Setup}
\label{subsec:ppi_setup}
We study the simplest unlearning technique, often referred to as \emph{exact machine unlearning}~\citep{bourtoule2021machine}: removing all relevant PII from the dataset and retraining, or as here re-fine-tuning, the model.
This may be triggered if users submit an opt-out request.
Since retraining after each request is expensive, %
model owners may collect and process these requests in batches.\looseness=-1

Following a protocol similar to~\citet{onioneffect}, our experimental procedure is:
     \textbf{(1) Extraction:} Prompt the current model \(\mathcal{M}\) with 25,000 fixed prompts and sample using greedy decoding to identify memorized emails. Let \(E\) be the set of extracted emails.
     \textbf{(2) Removal:} Remove \(E\) from \(D\) and re-finetune the base model on \(\,D \setminus E\), producing a new model \(\hat{\mathcal{M}}\).
     \textbf{(3) Repeat:} Prompt \(\hat{\mathcal{M}}\) again with the same prompts, discovering any newly memorized emails \(\hat{E}\).
We iterate until no more emails are extracted using this fixed set of prompts and decoding strategy.









\begin{figure}[t] 
\centering
\includegraphics[width=0.8\columnwidth]{Figures/entropy_ppl.pdf} 
%\vspace{-1em}
\caption{{Perplexity and zlib entropy of memorized emails.} Emails extracted in the initial model (blue) and emails extracted in later re-finetuned models %
(green) have lower perplexities than emails that were never extracted by any model (grey). This clustering suggests that the newly-extracted (green) emails were near the threshold of memorization from the outset.\
} \label{fig:entropyvsppl}
%\vspace{-1em}
\end{figure}



\paragraph{Protecting One Person's PII May Leak Another's}



As mentioned above, in each iteration, we (1) prompt the current model \(\mathcal{M}\) (trained on dataset \(D\)) with 25,000 fixed prompts, (2) remove any newly discovered memorized emails \(E\) from \(D\), and (3) re-finetune the base model on \(D \setminus E\). 
Figure~\ref{fig:onion} illustrates four such rounds (\textcolor{blue}{START} through \textcolor{red}{Update~4}). 
While the first update successfully removes the previously identified emails from the set of extracted PII, it simultaneously extracts a \emph{new} set of emails. 
By \textcolor{red}{Update~4}, no additional emails are discovered under these prompts and greedy decoding, although changing prompts or sampling strategies could still reveal further memorization. 
 Our results confirm that this \emph{layered memorization}---called the Onion Effect by prior work on image classifiers~\cite{onioneffect}---extends to language models: removing one layer of memorized PII exposes a second layer, and so forth.

\paragraph{Removing Random Emails.}
We next conduct a similar experiment but remove a random subset of emails instead of the ones that are discovered through extraction. Specifically, we sample 10\% of the total emails in \(D\) uniformly at random and call this set \(E\). We then fine-tune a new model \(\hat{\mathcal{M}}\) on \(D \setminus E\). Prompting \(\hat{\mathcal{M}}\) with the same 25,000 prompts and sampling with greedy decoding yields a new set of extracted emails \(\hat{E}\). 
Thus, \emph{randomly removing} data can similarly expose new PII, underscoring how unlearning updates can inadvertently introduce new privacy risks.


\paragraph{Controlling for Randomness During Training.} 
A natural question is whether any newly extracted emails simply result from any randomness when retraining a new model. 
For instance, models trained with the same data order, same parameter initialization, and same hyperparameters could
still differ during inference as GPU operations are non-deterministic~\citep{highfidelityextraction}. We want to ensure that new extractions are solely the result of removing particular emails. To this end, we train five such new models and extract emails by feeding the exact same prompts that we give to our original model (\(\mathcal{M}\)) and the models trained after removing extracted and randomly sampled emails (\(\hat{\mathcal{M}}\)). 
We sample all three sets of models with greedy decoding and compare which emails were extracted. %
Across all five trials and for both types of removals (removing extracted emails and removing them randomly), the models re-finetuned-after-removal reveal strictly more \emph{unique} PII than these fresh counterparts. Hence, the effect is not merely a product of random training fluctuations but rather an outcome of selectively removing data from \(D\).\looseness=-1





\paragraph{PII on the Verge of Memorization Surfaces After Others Are Removed}
\label{subsec:almost_extracted_pii}
Because we use a fixed set of prompts and greedy decoding, we hypothesize that newly extracted emails in each unlearning round were already \emph{close} to being memorized under the original model. In other words, these emails were initially ``hidden'' behind a first layer of memorized PII. Once the first layer of emails is removed, these nearly extractable emails become more vulnerable. %

To investigate this, we compare the perplexity of the initial model on three categories of emails: 
\textit{(i)} those extracted in the initial model, 
\textit{(ii)} those that are extracted in subsequent rounds of removal and refinetuning %
and 
\textit{(iii)} those never extracted by any model. 
We also measure their zlib entropy, a compression-based proxy for memorization~\citep{carlini-extraction, recite, zlib}. 
As shown in Figure~\ref{fig:entropyvsppl}, %
newly-extracted emails (green) cluster with those initially extracted (blue), indicating that both groups have lower perplexity compared to never-extracted emails (grey). This supports our hypothesis: once one layer of extracted PII is removed from the training set, the next-likeliest set of emails crosses the threshold into extraction. 
Iterating this process eventually exhausts these ``hidden layers,'' although more sophisticated prompts or sampling strategies could still uncover additional memorization.


