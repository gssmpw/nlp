[
  {
    "index": 0,
    "papers": [
      {
        "key": "shokri2017membership",
        "author": "Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly",
        "title": "Membership inference attacks against machine learning models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yeom2018privacy",
        "author": "Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh",
        "title": "Privacy risk in machine learning: Analyzing the connection to overfitting"
      },
      {
        "key": "salem2018ml",
        "author": "Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael",
        "title": "Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models"
      },
      {
        "key": "sablayrolles2019white",
        "author": "Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Ollivier, Yann and J{\\'e}gou, Herv{\\'e}",
        "title": "White-box vs black-box: Bayes optimal strategies for membership inference"
      },
      {
        "key": "choquette2021label",
        "author": "Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas",
        "title": "Label-only membership inference attacks"
      },
      {
        "key": "carlini2022membership",
        "author": "Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian",
        "title": "Membership inference attacks from first principles"
      },
      {
        "key": "jagielski2024students",
        "author": "Jagielski, Matthew and Nasr, Milad and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas and Tramer, Florian",
        "title": "Students parrot their teachers: Membership inference on model distillation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "duan2024membership",
        "author": "Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh",
        "title": "Do membership inference attacks work on large language models?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "secretsharer",
        "author": "Carlini, Nicholas and Liu, Chang and Erlingsson, \\'{U}lfar and Kos, Jernej and Song, Dawn",
        "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"
      },
      {
        "key": "carlini-extraction",
        "author": "Nicholas Carlini and Florian Tram{\\`e}r and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and {\\'U}lfar Erlingsson and Alina Oprea and Colin Raffel",
        "title": "Extracting Training Data from Large Language Models"
      },
      {
        "key": "quantifying",
        "author": "Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan",
        "title": "Quantifying Memorization Across Neural Language Models"
      },
      {
        "key": "lee-etal-2022-deduplicating",
        "author": "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas",
        "title": "Deduplicating Training Data Makes Language Models Better"
      },
      {
        "key": "counterfactual",
        "author": "Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tramer, Florian and Carlini, Nicholas",
        "title": "Counterfactual Memorization in Neural Language Models"
      },
      {
        "key": "ippolito-etal-2023-preventing",
        "author": "Ippolito, Daphne and Tramer, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and Choquette Choo, Christopher and Carlini, Nicholas",
        "title": "Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy"
      },
      {
        "key": "biderman2023emergent",
        "author": "Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff",
        "title": "Emergent and Predictable Memorization in Large Language Models"
      },
      {
        "key": "pythia",
        "author": "Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar",
        "title": "Pythia: a suite for analyzing large language models across training and scaling"
      },
      {
        "key": "kudugunta2024madlad",
        "author": "Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan",
        "title": "Madlad-400: A multilingual and document-level large audited dataset"
      },
      {
        "key": "nasr2023scalable",
        "author": "Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tram\u00e8r and Katherine Lee",
        "title": "Scalable Extraction of Training Data from (Production) Language Models"
      },
      {
        "key": "chang-etal-2023-speak",
        "author": "Chang, Kent and Cramer, Mackenzie and Soni, Sandeep and Bamman, David",
        "title": "Speak, Memory: An Archaeology of Books Known to {C"
      },
      {
        "key": "ozdayi-etal-2023-controlling",
        "author": "Ozdayi, Mustafa and Peris, Charith and FitzGerald, Jack and Dupuy, Christophe and Majmudar, Jimit and Khan, Haidar and Parikh, Rahil and Gupta, Rahul",
        "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning"
      },
      {
        "key": "compression",
        "author": "Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary and Kolter, J. Zico",
        "title": "Rethinking LLM Memorization through the Lens of Adversarial Compression"
      },
      {
        "key": "decop",
        "author": "Duarte, Andr\\'{e} V. and Zhao, Xuandong and Oliveira, Arlindo L. and Li, Lei",
        "title": "DE-COP: detecting copyrighted content in language models training data"
      },
      {
        "key": "wang-etal-2024-unlocking",
        "author": "Wang, Zhepeng and Bao, Runxue and Wu, Yawen and Taylor, Jackson and Xiao, Cao and Zheng, Feng and Jiang, Weiwen and Gao, Shangqian and Zhang, Yanfu",
        "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "huang-etal-2022-large",
        "author": "Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan",
        "title": "Are Large Pre-Trained Language Models Leaking Your Personal Information?"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "panda2024teach",
        "author": "Ashwinee Panda and Christopher A. Choquette-Choo and Zhengming Zhang and Yaoqing Yang and Prateek Mittal",
        "title": "Teach LLMs to Phish: Stealing Private Information from Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "PII-leakage",
        "author": "N. Lukas and A. Salem and R. Sim and S. Tople and L. Wutschitz and S. Zanella-Beguelin",
        "title": "Analyzing Leakage of Personally Identifiable Information in Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lehman-etal-2021-bert",
        "author": "Lehman, Eric and Jain, Sarthak and Pichotta, Karl and Goldberg, Yoav and Wallace, Byron",
        "title": "Does {BERT"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "anil2023palm",
        "author": "Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others",
        "title": "Palm 2 technical report"
      },
      {
        "key": "team2023gemini",
        "author": "{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "key": "dubey-2024-evaluating",
        "author": "Dubey, Kush",
        "title": "Evaluating the fairness of task-adaptive pretraining on unlabeled test data before few-shot text classification"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "key": "team2024gemini",
        "author": "{Gemini Team} and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "key": "team2024gemma",
        "author": "{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others",
        "title": "Gemma: Open models based on gemini research and technology"
      },
      {
        "key": "team2024gemma2",
        "author": "{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\\'e}, Alexandre and others",
        "title": "Gemma 2: Improving open language models at a practical size"
      },
      {
        "key": "team2024codegemma",
        "author": "{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others",
        "title": "Codegemma: Open code models based on gemma"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tirumala2022memorization",
        "author": "Kushal Tirumala and Aram H. Markosyan and Luke Zettlemoyer and Armen Aghajanyan",
        "title": "Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models"
      },
      {
        "key": "recite",
        "author": "USVSN Sai Prashanth and Alvin Deng and Kyle O'Brien and Jyothir S V au2 and Mohammad Aflah Khan and Jaydeep Borkar and Christopher A. Choquette-Choo and Jacob Ray Fuehne and Stella Biderman and Tracy Ke and Katherine Lee and Naomi Saphra",
        "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon"
      },
      {
        "key": "huang-etal-2024-demystifying",
        "author": "Huang, Jing and Yang, Diyi and Potts, Christopher",
        "title": "Demystifying Verbatim Memorization in Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jagielski2022measuring",
        "author": "Jagielski, Matthew and Thakkar, Om and Tramer, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and others",
        "title": "Measuring forgetting of memorized training examples"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "biderman2023emergent",
        "author": "Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff",
        "title": "Emergent and Predictable Memorization in Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "duan2024uncoveringlatentmemoriesassessing",
        "author": "Sunny Duan and Mikail Khona and Abhiram Iyer and Rylan Schaeffer and Ila R Fiete",
        "title": "Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Frontier AI Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "sisa",
        "author": "Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas",
        "title": "Machine Unlearning"
      },
      {
        "key": "approx_deletion",
        "author": "Izzo, Zachary and Anne Smart, Mary and Chaudhuri, Kamalika and Zou, James",
        "title": " Approximate Data Deletion from Machine Learning Models "
      },
      {
        "key": "unlearning_auditing",
        "author": "Anvith Thudi and Hengrui Jia and Ilia Shumailov and Nicolas Papernot",
        "title": "On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "Fabbrini_Celeste_2020",
        "author": "Fabbrini, Federico and Celeste, Edoardo",
        "title": "The Right to Be Forgotten in the Digital Age: The Challenges of Data Protection Beyond Borders"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "cooper2024machineunlearningdoesntthink",
        "author": "A. Feder Cooper and Christopher A. Choquette-Choo and Miranda Bogen and Matthew Jagielski and Katja Filippova and Ken Ziyu Liu and Alexandra Chouldechova and Jamie Hayes and Yangsibo Huang and Niloofar Mireshghallah and Ilia Shumailov and Eleni Triantafillou and Peter Kairouz and Nicole Mitchell and Percy Liang and Daniel E. Ho and Yejin Choi and Sanmi Koyejo and Fernando Delgado and James Grimmelmann and Vitaly Shmatikov and Christopher De Sa and Solon Barocas and Amy Cyphert and Mark Lemley and danah boyd and Jennifer Wortman Vaughan and Miles Brundage and David Bau and Seth Neel and Abigail Z. Jacobs and Andreas Terzis and Hanna Wallach and Nicolas Papernot and Katherine Lee",
        "title": "Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "jeopardize",
        "author": "Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang",
        "title": "When Machine Unlearning Jeopardizes Privacy"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "onioneffect",
        "author": "Carlini, Nicholas and Jagielski, Matthew and Zhang, Chiyuan and Papernot, Nicolas and Terzis, Andreas and Tramer, Florian",
        "title": "The Privacy Onion Effect: Memorization is Relative"
      },
      {
        "key": "hayes2024inexact",
        "author": "Hayes, Jamie and Shumailov, Ilia and Triantafillou, Eleni and Khalifa, Amr and Papernot, Nicolas",
        "title": "Inexact unlearning needs more careful evaluations to avoid a false sense of privacy"
      }
    ]
  }
]