@article{Fabbrini_Celeste_2020, title={The Right to Be Forgotten in the Digital Age: The Challenges of Data Protection Beyond Borders}, volume={21}, DOI={10.1017/glj.2020.14}, number={S1}, journal={German Law Journal}, author={Fabbrini, Federico and Celeste, Edoardo}, year={2020}, pages={55–65}}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@InProceedings{approx_deletion,
  title = 	 { Approximate Data Deletion from Machine Learning Models },
  author =       {Izzo, Zachary and Anne Smart, Mary and Chaudhuri, Kamalika and Zou, James},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2008--2016},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/izzo21a/izzo21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/izzo21a.html},
  abstract = 	 { Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU’s General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. In this work, we propose a new approximate deletion method for linear and logistic models whose computational cost is linear in the the feature dimension d and independent of the number of training data n. This is a significant gain over all existing methods, which all have superlinear time dependence on the dimension. We also develop a new feature-injection test to evaluate the thoroughness of data deletion from ML models. }
}

@misc{biderman2023emergent,
      title={Emergent and Predictable Memorization in Large Language Models}, 
      author={Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff},
      year={2023},
      eprint={2304.11158},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{carlini2022membership,
  title={Membership inference attacks from first principles},
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1897--1914},
  year={2022},
  organization={IEEE}
}

@inproceedings{choquette2021label,
  title={Label-only membership inference attacks},
  author={Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  booktitle={International conference on machine learning},
  pages={1964--1974},
  year={2021},
  organization={PMLR}
}

@inproceedings{compression,
 author = {Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {56244--56267},
 publisher = {Curran Associates, Inc.},
 title = {Rethinking LLM Memorization through the Lens of Adversarial Compression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/66453d578afae006252d2ea090e151c9-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@misc{cooper2024machineunlearningdoesntthink,
      title={Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice}, 
      author={A. Feder Cooper and Christopher A. Choquette-Choo and Miranda Bogen and Matthew Jagielski and Katja Filippova and Ken Ziyu Liu and Alexandra Chouldechova and Jamie Hayes and Yangsibo Huang and Niloofar Mireshghallah and Ilia Shumailov and Eleni Triantafillou and Peter Kairouz and Nicole Mitchell and Percy Liang and Daniel E. Ho and Yejin Choi and Sanmi Koyejo and Fernando Delgado and James Grimmelmann and Vitaly Shmatikov and Christopher De Sa and Solon Barocas and Amy Cyphert and Mark Lemley and danah boyd and Jennifer Wortman Vaughan and Miles Brundage and David Bau and Seth Neel and Abigail Z. Jacobs and Andreas Terzis and Hanna Wallach and Nicolas Papernot and Katherine Lee},
      year={2024},
      eprint={2412.06966},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.06966}, 
}

@inproceedings{counterfactual,
 author = {Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tramer, Florian and Carlini, Nicholas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {39321--39362},
 publisher = {Curran Associates, Inc.},
 title = {Counterfactual Memorization in Neural Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/7bc4f74e35bcfe8cfe43b0a860786d6a-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{decop,
author = {Duarte, Andr\'{e} V. and Zhao, Xuandong and Oliveira, Arlindo L. and Li, Lei},
title = {DE-COP: detecting copyrighted content in language models training data},
year = {2024},
publisher = {JMLR.org},
abstract = {How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6\% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72\% for detecting suspect books on fully black-box models where prior methods give approximately 4\% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {475},
numpages = {17},
location = {Vienna, Austria},
series = {ICML'24}
}

@article{duan2024membership,
  title={Do membership inference attacks work on large language models?},
  author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2402.07841},
  year={2024}
}

@misc{duan2024uncoveringlatentmemoriesassessing,
      title={Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Frontier AI Models}, 
      author={Sunny Duan and Mikail Khona and Abhiram Iyer and Rylan Schaeffer and Ila R Fiete},
      year={2024},
      eprint={2406.14549},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.14549}, 
}

@article{hayes2024inexact,
  title={Inexact unlearning needs more careful evaluations to avoid a false sense of privacy},
  author={Hayes, Jamie and Shumailov, Ilia and Triantafillou, Eleni and Khalifa, Amr and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2403.01218},
  year={2024}
}

@article{jagielski2022measuring,
  title={Measuring forgetting of memorized training examples},
  author={Jagielski, Matthew and Thakkar, Om and Tramer, Florian and Ippolito, Daphne and Lee, Katherine and Carlini, Nicholas and Wallace, Eric and Song, Shuang and Thakurta, Abhradeep and Papernot, Nicolas and others},
  journal={arXiv preprint arXiv:2207.00099},
  year={2022}
}

@article{jagielski2024students,
  title={Students parrot their teachers: Membership inference on model distillation},
  author={Jagielski, Matthew and Nasr, Milad and Lee, Katherine and Choquette-Choo, Christopher A and Carlini, Nicholas and Tramer, Florian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{jeopardize,
author = {Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang},
title = {When Machine Unlearning Jeopardizes Privacy},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484756},
doi = {10.1145/3460120.3484756},
abstract = {The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. footnoteOur code is available at urlhttps://github.com/MinChen00/UnlearningLeaks.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {896–911},
numpages = {16},
keywords = {machine learning security and privacy, machine unlearning, membership inference},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@article{kudugunta2024madlad,
  title={Madlad-400: A multilingual and document-level large audited dataset},
  author={Kudugunta, Sneha and Caswell, Isaac and Zhang, Biao and Garcia, Xavier and Xin, Derrick and Kusupati, Aditya and Stella, Romi and Bapna, Ankur and Firat, Orhan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{nasr2023scalable,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{onioneffect,
 author = {Carlini, Nicholas and Jagielski, Matthew and Zhang, Chiyuan and Papernot, Nicolas and Terzis, Andreas and Tramer, Florian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {13263--13276},
 publisher = {Curran Associates, Inc.},
 title = {The Privacy Onion Effect: Memorization is Relative},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/564b5f8289ba846ebc498417e834c253-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{panda2024teach,
      title={Teach LLMs to Phish: Stealing Private Information from Language Models}, 
      author={Ashwinee Panda and Christopher A. Choquette-Choo and Zhengming Zhang and Yaoqing Yang and Prateek Mittal},
      year={2024},
      eprint={2403.00871},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{pythia,
author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
title = {Pythia: a suite for analyzing large language models across training and scaling},
year = {2023},
publisher = {JMLR.org},
abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {102},
numpages = {34},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@misc{quantifying,
  doi = {10.48550/ARXIV.2202.07646},
  
  url = {https://arxiv.org/abs/2202.07646},
  
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Quantifying Memorization Across Neural Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{recite,
      title={Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon}, 
      author={USVSN Sai Prashanth and Alvin Deng and Kyle O'Brien and Jyothir S V au2 and Mohammad Aflah Khan and Jaydeep Borkar and Christopher A. Choquette-Choo and Jacob Ray Fuehne and Stella Biderman and Tracy Ke and Katherine Lee and Naomi Saphra},
      year={2024},
      eprint={2406.17746},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17746}, 
}

@inproceedings{sablayrolles2019white,
  title={White-box vs black-box: Bayes optimal strategies for membership inference},
  author={Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Ollivier, Yann and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={5558--5567},
  year={2019},
  organization={PMLR}
}

@article{salem2018ml,
  title={Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models},
  author={Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael},
  journal={arXiv preprint arXiv:1806.01246},
  year={2018}
}

@inproceedings{secretsharer,
author = {Carlini, Nicholas and Liu, Chang and Erlingsson, \'{U}lfar and Kos, Jernej and Song, Dawn},
title = {The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks},
year = {2019},
isbn = {9781939133069},
publisher = {USENIX Association},
address = {USA},
abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models--a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.},
booktitle = {Proceedings of the 28th USENIX Conference on Security Symposium},
pages = {267–284},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {SEC'19}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{sisa,
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)}, 
  title={Machine Unlearning}, 
  year={2021},
  volume={},
  number={},
  pages={141-159},
  keywords={Training;Data privacy;Privacy;Limiting;Transfer learning;Training data;Stochastic processes},
  doi={10.1109/SP40001.2021.00019}}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{team2024codegemma,
  title={Codegemma: Open code models based on gemma},
  author={{CodeGemma Team} and Zhao, Heri and Hui, Jeffrey and Howland, Joshua and Nguyen, Nam and Zuo, Siqi and Hu, Andrea and Choquette-Choo, Christopher A and Shen, Jingyue and Kelley, Joe and others},
  journal={arXiv preprint arXiv:2406.11409},
  year={2024}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={{Gemini Team} and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@inproceedings{yeom2018privacy,
  title={Privacy risk in machine learning: Analyzing the connection to overfitting},
  author={Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  booktitle={2018 IEEE 31st computer security foundations symposium (CSF)},
  pages={268--282},
  year={2018},
  organization={IEEE}
}

