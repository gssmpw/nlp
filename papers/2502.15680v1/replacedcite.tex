\section{Related Work}
\paragraph{Membership Inference}
is one of the most common privacy attacks on neural models____. Though successful on computer vision models____, these attacks are not often successful on LLMs____ which we study. Thus, and because verbatim extraction poses a stronger privacy risk, we focus on \emph{memorization and extraction}. 


\paragraph{Memorization \& Extraction}
studies when a text is trained on and generated by a model. This is widely studied____. These works are often focused on the broad phenomenon, and not the nature of the data, e.g., if it were sensitive as in our work. Relatively fewer works have considered this setting. ____ study if information about specific entites can be extracted; ____ study if LLM's can be poisoned to memorize specific PII; ____ formalize PII extraction, proposing several attacks and studying the efficacy of various existing defenses; and ____ found that extracting sensitive data, using simple techniques, from BERT trained on clinical notes was largely unsuccessful. This line of work has become important for practical privacy and memorization audits____, which also often include PII memorization evaluations____. 
\vspace{-1em}

\paragraph{Dynamics of Memorization.} 
Most related to our work are those exploring memorization throughout training. It is known that language models memorize more as training progresses____ and exhibit forgetting of memorized examples____. ____ found that there is not high correlation between memorized sequences within checkpoints of a training run. ____ show a similar notion of ``latent memorization'' but that instead  uses Gaussian noise to uncover these latent memories; instead, our ``assisted memorization'' shows this can happen in normal training runs through only naturally occurring text sequences. The literature so far lacks a clear understanding of the complete memorization landscape throughout training. In our work, we provide a complete taxonomy and uncover novel forms of memorization within training dynamics. 
\vspace{-1em}

\vspace{-1em}
\paragraph{Unlearning} 
Machine unlearning methods have been proposed as an efficient way to erase data from neural networks____. These methods are motivated by scenarios where users may request for their data to be removed from a trained model (possibly due to legislative considerations like GDPR____). While many techniques have been proposed for machine unlearning, we focus on the simple strategy of retraining without relevant data points which is the current gold standard, though it may not be applicable to all practical scenarios____. Most related to our work are works that show unlearning can cause additional privacy risks:____ show this can lead to stronger membership inference attacks and____ show that unlearning can increase membership inference accuracy on other training samples.