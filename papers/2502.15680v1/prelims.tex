
\section{Experimental Setup}
\label{sec:prelim}
Our goal is to study how memorization of PII manifests during training.\footnote{We do not state or imply [here] that a model ``contains'' its training data in the sense that there is a copy of that data in the model.  Rather, a model memorizes attributes of its training data such that in certain cases it is statistically able to generate such training data when following rules and using information about features of its training data that it does contain.} This includes continual training or fine-tuning setups in \S\ref{section:continuous_training} and re-training or unlearning setups in \S\ref{section:in_out}. First, we describe our general experimental setup.


\begin{figure}[t]
   \centering
    \begin{subfigure}[t]{0.5\textwidth}
    \vskip 0pt  %
    \includegraphics[clip,width=\textwidth]{Figures/tab1.pdf}
    \end{subfigure}
  \caption{\textbf{Taxonomy of memorization for a continuous training setup.} We define \novel, \retained, \forgotten, and \assisted (described in Section~\ref{sec:taxonomy}). Note that text classified as \assisted memorization may also be forgotten or retained for steps $i+1$ onwards.}
  \vspace{-0.5em}
  \label{fig:taxonomy}
\end{figure}

 
\paragraph{Training Setup}
We use the GPT-2-XL model~\cite{radford2019language}, which has 1.5B parameters for our primary experiments, and also experiment with Llama 3 8B~\citep{dubey2024llama3herdmodels}\footnote{Llama experiments in this paper were conducted only by parties outside of Google.} and Gemma 2B~\citep{team2024gemma}. We fine-tune these models with a linear schedule: initial and end learning rate of zero, $500$ step warmup, cooldown, and peak learning rate of $2 \times10^{-5}$. We use $1 \times10^{-2}$ weight decay and a batch size of $8$. We run experiments 5 times, sampling fresh randomness (model weights, data order, etc.) each time. 

We fine-tune these models on two datasets. First, we use a modified version of the WikiText-2 dataset~\cite{merity2016pointer} to include unique emails from the Enron dataset\footnote{\url{https://www.cs.cmu.edu/ enron/}}. We take the entire WikiText-2 dataset and insert %
\(E\) unique email addresses (herein, emails) into each passage. We concatenate all passages during training and divide them into blocks of 128 tokens.
Second, we use the Pile of Law dataset~\citep{henderson2022pile}.
We ensure no emails were already memorized by querying the base models with the same prompts.
\citet{lee-etal-2022-deduplicating} found data duplication strongly increases memorization. 
In our study, all emails occur in the training corpus exactly once. %


\paragraph{Sampling}
We closely follow the methodology of~\citet{carlini-extraction,nasr2023scalable}. We focus on ``extractable memorization'' and use ten-token sequences sampled uniformly at random from Common Crawl. We randomly sample a unique set of $25,000$ different prompts for each experiment. We obtain a $256$ token output from the model for each prompt and evaluate it for successful extraction. Our method may lead to false negatives; however, this would only underestimate the PII regurgitation, and, we further believe our diverse and large prompt dataset reasonably captures the regurgitation rates. To further minimize false-negatives, where denoted we also evaluate ``discoverable'' memorization, where we prompt with the exact prefix the model trained on.
We use greedy decoding, or top-$k=40$ sampling when specified.


\paragraph{Defining Memorization and Extraction}
We primarily use the definition of \emph{extractable memorization} (and, where denoted, \emph{discovered memorization}) from 
~\citet{nasr2023scalable}. Herein, we will refer to a success as an extraction, which is whenever an \emph{email} is contained both in the training dataset and a language model's generation.
Formally, let $\dataset$ be the training dataset for a language model $\lm$. Let $f$ be a chosen sampling scheme that takes an input text prompt $p$ and returns the conditional generation $s=f_\lm(p)$. An email $e^i$ is said to be extracted if $e^i \in \dataset$ and $\exists p : e^i \in f_\lm(p)$.

\paragraph{Checking for Memorized PII}
We use a regular expression to identify any emails within the generations that belong to the model's training data. Unlike previous approaches that create a pool of generations by filtering based on factors like perplexity and entropy \citep{carlini-extraction}, we evaluate all 25,000 generations for memorization.




















