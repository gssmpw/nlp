\section{Related Work}


\paragraph{Membership Inference}
is one of the most common privacy attacks on neural models~\citep{shokri2017membership}. Though successful on computer vision models~\citep{yeom2018privacy,salem2018ml,sablayrolles2019white,choquette2021label,carlini2022membership,jagielski2024students}, these attacks are not often successful on LLMs~\citep{duan2024membership} which we study. Thus, and because verbatim extraction poses a stronger privacy risk, we focus on \emph{memorization and extraction}. 


\paragraph{Memorization \& Extraction}
studies when a text is trained on and generated by a model. This is widely studied~\citep{secretsharer,carlini-extraction, quantifying, lee-etal-2022-deduplicating, counterfactual, ippolito-etal-2023-preventing,biderman2023emergent, pythia, kudugunta2024madlad,nasr2023scalable, chang-etal-2023-speak, ozdayi-etal-2023-controlling, compression, decop, wang-etal-2024-unlocking}. These works are often focused on the broad phenomenon, and not the nature of the data, e.g., if it were sensitive as in our work. Relatively fewer works have considered this setting. \citet{huang-etal-2022-large} study if information about specific entites can be extracted; \citet{panda2024teach} study if LLM's can be poisoned to memorize specific PII; \citet{PII-leakage} formalize PII extraction, proposing several attacks and studying the efficacy of various existing defenses; and \citet{lehman-etal-2021-bert} found that extracting sensitive data, using simple techniques, from BERT trained on clinical notes was largely unsuccessful. This line of work has become important for practical privacy and memorization audits~\citep{anil2023palm,team2023gemini,dubey-2024-evaluating}, which also often include PII memorization evaluations~\citep{team2023gemini,team2024gemini,team2024gemma,team2024gemma2,team2024codegemma}. 
\vspace{-1em}

\paragraph{Dynamics of Memorization.} 
Most related to our work are those exploring memorization throughout training. It is known that language models memorize more as training progresses~\citep{tirumala2022memorization, recite,huang-etal-2024-demystifying} and exhibit forgetting of memorized examples~\citep{jagielski2022measuring}. \citet{biderman2023emergent} found that there is not high correlation between memorized sequences within checkpoints of a training run. \citet{duan2024uncoveringlatentmemoriesassessing} show a similar notion of ``latent memorization'' but that instead  uses Gaussian noise to uncover these latent memories; instead, our ``assisted memorization'' shows this can happen in normal training runs through only naturally occurring text sequences. The literature so far lacks a clear understanding of the complete memorization landscape throughout training. In our work, we provide a complete taxonomy and uncover novel forms of memorization within training dynamics. 
\vspace{-1em}

\vspace{-1em}
\paragraph{Unlearning} 
Machine unlearning methods have been proposed as an efficient way to erase data from neural networks~\citep{sisa, approx_deletion, unlearning_auditing}. These methods are motivated by scenarios where users may request for their data to be removed from a trained model (possibly due to legislative considerations like GDPR~\citep{Fabbrini_Celeste_2020}). While many techniques have been proposed for machine unlearning, we focus on the simple strategy of retraining without relevant data points which is the current gold standard, though it may not be applicable to all practical scenarios~\citep{cooper2024machineunlearningdoesntthink}. Most related to our work are works that show unlearning can cause additional privacy risks:~\citet{jeopardize} show this can lead to stronger membership inference attacks and~\citet{onioneffect, hayes2024inexact} show that unlearning can increase membership inference accuracy on other training samples. 
 

 
