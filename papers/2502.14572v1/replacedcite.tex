\section{Related Work}
\label{section-Related Work}
%\subsection{Concept-level interpretable neural network}
%引文放在句子后面！！！
%不要recent years!!!
\paragraph{Comprehensible Explanation under Perturbation.}
%\subsection{Comprehensible Explanation Under Perturbation}
%可理解的
Studies of comprehensible explanations under perturbations can be divided into two categories: attacks on comprehensibility and defenses of comprehensibility.
Studies of attacks on comprehensibility aim to design perturbations that misguide the model to generate incomprehensible explanations.
%By introducing imperceptible perturbations to the inputs, interpretable neural networks can be misled into generating incomprehensible explanations without changing the predicted results____.
Some methods modify salient mappings with perturbations that make the explanation incomprehensible to users____. 
% Yeh等人进一步展示扰动给解释造成的不可理解的，
Furthermore, there are several efforts that propose additional types of perturbations____.
%有几个研究提出了另外的扰动形式。它们展示不同类型的扰动都可能破坏解释的可理解性。
They demonstrate that many types of perturbations can undermine the comprehensibility of explanations. 
In contrast, studies on defenses of comprehensibility aim to design defensive strategies to suppress the effects of perturbations on interpretations.
These studies focus on adversarial training of interpretable neural networks so that the model generates comprehensible explanations despite perturbations. 
%这些方法为对抗样本标注解释标签，之后约束模型生成的解释与标签相似。
These studies are implemented in two ways. 
In the first approach, some methods annotate the adversarial samples with explanatory labels, and constrain the model to generate explanations similar to the labels____.
While promising, excessive adversarial training can easily lead to overfitting.
%另一部分研究在对抗训练的基础上，利用正则化项来缓解过拟合,允许解释存在合理的局部偏移。
In the second approach, some efforts further utilize different regularization terms based on adversarial training to mitigate overfitting, and allowing reasonable local shifts in explanations____.
%另外，基于概念的可解释方法在最近受到了关注，它们通过生成一组高级的语义概念来解释模型的决策，以促进人类对模型的直观理解。然而，已经有研究证明基于概念的解释在扰动下也会出错并丧失可理解性，并验证重训练在提升基于概念的解释的可理解性方面是有效的。
In addition, concept-based interpretable methods, which explain model decisions by generating a set of high-level semantic concepts, have gained great attention recently____.
Moreover, it has been demonstrated that concept-based explanations can be erroneous and lose comprehensibility under perturbations____.
Meanwhile, they verify that retraining is effective in enhancing the comprehensibility of concept-based explanations.
However, all the above methods assume that the perturbation is known to the model. Thus, how to improve the comprehensibility of the explanation under unknown perturbations remains open.

\paragraph{Knowledge Integration with Factor Graph.} 
%对因子图知识集成进行了深入的研究，集成因子图编码的外源知识有助于提高ML模型的预测精度。
There have been extensive studies on knowledge integration with factor graphs____.
%这些研究的普遍方案是用因子图去联合推理多个ML模型的预测结果。
These studies typically utilize factor graph reasoning to assemble predictions from multiple ML models.
%当一个模型预测错误时，因子图可以根据其它模型的预测，结合外源知识去纠正错误。
When one model predicts incorrectly, the factor graph can combine the exogenous knowledge to correct the error based on the predictions of other models.
%在本文中，我们没有用因子图集成的外源知识来提升预测精度，而是探索了用这些外源知识来引导可解释神经网络生成可理解的解释的可能性。
Empirical evidence suggests that integration of exogenous knowledge in factor graphs contributes to the predictive accuracy of ML models.
In this paper, instead of improving predictive accuracy, we explore the possibility of using exogenous knowledge to guide interpretable neural networks for generating comprehensible explanations.