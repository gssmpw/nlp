
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{algorithmic}
% \usepackage{algorithm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{array}
\usepackage{wrapfig} 
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{amsthm}
\usepackage{booktabs} 
\usepackage{color, xcolor}
\usepackage{soul}

\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output} % 重新定义Output为output
\SetKwInOut{Return}{return} 
\title{Factor Graph-based Interpretable Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
% \author{Yicong Li \\
% School of Software\\
% Dalian University of Technology\\
% Dalian, Liaoning 116000 \\
% \texttt{\{17640284112\}@mail.dlut.edu.cn} \\
% \And
% Shuo Yu \\
%    School of Computer Science and Technology \\
%    Dalian University of Technology\\
%    Dalian, Liaoning 116000 \\
%    \texttt{\{shuo.yu\}@ieee.org} \\
%    \And
%    Qiang Zhang \\
%    School of Computer Science and Technology \\
%    Dalian University of Technology\\
%    Dalian, Liaoning 116000 \\
%    \texttt{\{zhangq\}@dlut.edu.cn} \\
%     \And
%    Kuanjiu Zhou \\
%    School of Software \\
%    Dalian University of Technology\\
%    Dalian, Liaoning 116000 \\
%    \texttt{\{zhoukj\}@dlut.edu.cn} \\
%      \And
%    Renqiang Luo \\
%    School of Software \\
%    Dalian University of Technology\\
%    Dalian, Liaoning 116000 \\
%    \texttt{\{lrenqiang\}@outlook.com} \\
%      \And
%    Xiaodong Li \\
%    School of Computing Technologies \\
%    RMIT University\\
%    Melbourne, VIC 3000 \\
%    \texttt{xiaodong.li@rmit.edu.au} \\
%      \And     
%    Feng Xia \\
%    School of Computing Technologies \\
%    RMIT University\\
%    Melbourne, VIC 3000 \\
%    \texttt{f.xia@ieee.org} \\
% }
\author{
Yicong Li$^{1}$,
Kuanjiu Zhou$^{1}$,
Shuo Yu$^{1}$\thanks{Corresponding authors: \texttt{shuo.yu@ieee.org, zhangq@dlut.edu.cn}},
Qiang Zhang$^{1}$\footnotemark[1],
Renqiang Luo$^{2}$,
Xiaodong Li$^{3}$,
Feng Xia$^{3}$\\
$^{1}$ Dalian University of Technology, $^{2}$Jilin University, $^{3}$RMIT University
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. 
Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations.
To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations.
Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference.
Specifically, we construct the factor graph to express logical rules between explanations and categories.
By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic.
Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations.
Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. 
Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines\footnote{Source codes are available at~\url{https://github.com/yushuowiki/AGAIN}.}.
\end{abstract}

\section{Introduction}
Comprehensibility of neural network explanations depends on their consistency with human insights and real-world logic.
Comprehensible explanations promote better understanding of decisions and establish trust in the deployment of neural networks in high-stake scenarios, such as healthcare and finance~\citep{virgolin2023robustness,fokkema2023attribution,luo2024fugnn,luo2024fairgt}.
However, as shown in Figure~\ref{fig1}, interpretable neural networks are vulnerable to malicious perturbations which are infused into inputs, misguiding the model to generate incomprehensible explanations~\citep{tan2023robust,baniecki2024adversarial}.
Such explanations may cause users to make wrong judgments, resulting in security concerns in high-stake domains.
For example, a doctor prescribing medication based on a medically illogical explanation (i.e. incomprehensible explanation) of the pathological prediction may lead to misdiagnosis.
Therefore, it is crucial to ensure that interpretable neural networks generate comprehensible explanations under perturbations.

Several existing efforts have been devoted to investigating comprehensible explanations~\citep{kamath2024rethinking,Sarkar20212532,chen2019robust}. 
Many of them craft adversarial samples by adding perturbations to the dataset beforehand and retrain the model with extra regularization terms. 
Regularization terms constrain model to generate explanations that are similar to the explanation labels of the adversarial samples.
Empirical results show that retrained models are able to learn the adversarial sample data distribution and reduce the probability of being misled by the predetermined perturbation.
%Empirical shows that adversarial training contributes to the generating of comprehensible explanations under perturbations.

However, the above solutions assume perturbations are known to the model, which leads to their failure to generate comprehensible explanations under unknown perturbations~\citep{novakovsky2023obtaining}.
%However, the above solutions must assume that perturbations are known to the model~\citep{novakovsky2023obtaining}. 
%They fail to generate comprehensible explanations under unknown perturbations.
%The designer needs to craft adversarial samples based on known perturbations in advance. 
%While from the perspective of models, most of the perturbations are unknown.
The reasons are as follows:~1) it is impossible to craft adversarial samples for all unknown perturbation types~\citep{gurel2021knowledge};~2) even if the adversarial samples are available, retraining is effective for only a limited number of perturbation types simultaneously~\citep{tan2023robust}.
%because it is impossible for the designer to anticipate all perturbations in advance~\citep{}.
%这导致，重训练的模型在推理时一旦遭遇未知扰动，解释仍然会变得不可理解。
%As a result, the explanation remains incomprehensible once the retrained model suffers unknown perturbations during inference.
%因此，尽管在可理解的解释方面取得快速的进展，但在未知扰动下提供可理解的解释仍然具有挑战性。
Thus, despite recent progress on comprehensible interpretability, it is still challenging to provide comprehensible explanations under unknown perturbations.
%考虑此，我们试图解决这个问题从不同的角度——我们没有优化训练策略，而是优化推理过程。
Considering this, we seek to solve this problem from a different perspective - instead of optimizing the training strategy, we innovate the inference process.
%Our motivation is to design an interpretable neural network capable of identifying unknown perturbations during inference and eliminating their impact on explanations.
Our goal is to design an interpretable neural network capable of rectifying incomprehensible explanations under unknown perturbations during inference.
%We would want interpretable neural networks to be able to detect unknown perturbations during inference and eliminate their effect on explanations without assuming that the perturbations are known to the model.
%我们从因子图知识集成中获得了灵感。
We draw inspiration from knowledge integration with factor graphs.
%我们认为扰动往往会导致解释的语义违背因子图中的外源知识。
Unknown perturbations cause the explanatory semantics to violate the exogenous knowledge in the factor graph~\citep{tu2023deep,xia2021graph}.
%因此，因子图的逻辑推理有助于模型识别扰动，甚至纠正解释的逻辑偏差。
Factor graph reasoning enables us to identify and rectify explanatory logical errors without learning perturbations.
\begin{wrapfigure}{r}[0cm]{0pt}
  \includegraphics[width=0.6\linewidth]{img/CB.jpg}
  \vspace{-1em}
  \caption{Interpretable neural networks suffer from perturbations that generate incomprehensible explanations. For instance, the model predicts the input as ``Dog" but explains it with ``Wings" and ``Plume".}
  \label{fig1}
  \vspace{-1em}
\end{wrapfigure}

We propose AGAIN~(fActor GrAph-based Interpretable neural Network), which generates comprehensible concept-level explanations based on the factor graph under unknown perturbations~\citep{tiddi2022knowledge}. 
%Our method utilizes factor graphs to encode prior logical rules that provide logical guidance for the explanation. Then the neural network utilizes the improved explanation of the factor graph to predict the output, maintaining comprehensibility under unknown perturbations.
AGAIN consists of three modules, including factor graph construction, explanatory logic errors identification, and explanation rectification. In the first module, semantic concepts, label catergories, and logical rules between them are encoded as two kinds of nodes (i.e., variable and factor) in the factor graph, while their correlations are encoded as the edges. Based on the constructed factor graph, the logic relations among concepts and categories are explicitly represented.
In the second module, AGAIN generates the concept-level explanations and predictive categories and then imports them into the factor graph to identify erroneous concept activations through logical reasoning.
In the third module, we propose an interactive intervention switch strategy for concept activations to correct logical errors in explanations. 
The explanations that are further regenerated align with external knowledge.
The regenerated explanations are used to predict categories.
%This updated concept activation serves as a comprehensible explanation and predicts the final category.
%Extensive experiments on both synthetic and real datasets show that the proposed AGAIN can generate logically complete concept-level explanations under unknown perturbations.
Extensive experiments are conducted on three datasets including CUB, MIMIC-III EWS, and Synthetic-MNIST. 
Experimental results demonstrate concept-level explanations generated by the proposed AGAIN under unknown perturbations have better comprehensibility compared to baselines such as ICBM, PCBM, free CBM, and ProbCBM.

Our contributions can be summarized as follows:~1)~\textbf{against unknown perturbations:}~we present an innovative interpretable neural network based on factor graph. It integrates real-world logical knowledge to generate comprehensible explanations under unknown perturbations;~2)~\textbf{forward feedback:}~we design logic error identification and rectification methods based on the factor graph. Our method is able to rectify logic violating explanations during inference without learning perturbations, unlike previous methods;~3)~\textbf{theoretical foundation of factor graph:}~we prove that the comprehensibility of explanations is positively correlated with the involvement of factor graph;~4)~\textbf{superior performance:}~we conduct extensive experiments on three datasets to demonstrate that AGAIN can generate more comprehensible explanations than existing methods under unknown perturbations.

\section{Related Work}\label{section-Related Work}
%\subsection{Concept-level interpretable neural network}
%引文放在句子后面！！！
%不要recent years!!!
\paragraph{Comprehensible Explanation under Perturbation.}
%\subsection{Comprehensible Explanation Under Perturbation}
%可理解的
Studies of comprehensible explanations under perturbations can be divided into two categories: attacks on comprehensibility and defenses of comprehensibility.
Studies of attacks on comprehensibility aim to design perturbations that misguide the model to generate incomprehensible explanations.
%By introducing imperceptible perturbations to the inputs, interpretable neural networks can be misled into generating incomprehensible explanations without changing the predicted results~\cite{tan2023robust}.
Some methods modify salient mappings with perturbations that make the explanation incomprehensible to users~\citep{Ghorbani332019,dombrowski2022towards}. 
% Yeh等人进一步展示扰动给解释造成的不可理解的，
Furthermore, there are several efforts that propose additional types of perturbations~\citep{zhan2022towards,10.1609/aaai.v37i6.25847,Huai69352022}.
%有几个研究提出了另外的扰动形式。它们展示不同类型的扰动都可能破坏解释的可理解性。
They demonstrate that many types of perturbations can undermine the comprehensibility of explanations. 
In contrast, studies on defenses of comprehensibility aim to design defensive strategies to suppress the effects of perturbations on interpretations.
These studies focus on adversarial training of interpretable neural networks so that the model generates comprehensible explanations despite perturbations. 
%这些方法为对抗样本标注解释标签，之后约束模型生成的解释与标签相似。
These studies are implemented in two ways. 
In the first approach, some methods annotate the adversarial samples with explanatory labels, and constrain the model to generate explanations similar to the labels~\citep{boopathy2020proper,lakkaraju2020robust,pmlr-v119-chalasani20a}.
While promising, excessive adversarial training can easily lead to overfitting.
%另一部分研究在对抗训练的基础上，利用正则化项来缓解过拟合,允许解释存在合理的局部偏移。
In the second approach, some efforts further utilize different regularization terms based on adversarial training to mitigate overfitting, and allowing reasonable local shifts in explanations~\citep{kamath2024rethinking,Sarkar20212532,chen2019robust}.
%另外，基于概念的可解释方法在最近受到了关注，它们通过生成一组高级的语义概念来解释模型的决策，以促进人类对模型的直观理解。然而，已经有研究证明基于概念的解释在扰动下也会出错并丧失可理解性，并验证重训练在提升基于概念的解释的可理解性方面是有效的。
In addition, concept-based interpretable methods, which explain model decisions by generating a set of high-level semantic concepts, have gained great attention recently~\cite{koh2020concept,havasi2022addressing,Wang2022CVPR10254}.
Moreover, it has been demonstrated that concept-based explanations can be erroneous and lose comprehensibility under perturbations~\cite{sinha2023understanding}.
Meanwhile, they verify that retraining is effective in enhancing the comprehensibility of concept-based explanations.
However, all the above methods assume that the perturbation is known to the model. Thus, how to improve the comprehensibility of the explanation under unknown perturbations remains open.

\paragraph{Knowledge Integration with Factor Graph.} 
%对因子图知识集成进行了深入的研究，集成因子图编码的外源知识有助于提高ML模型的预测精度。
There have been extensive studies on knowledge integration with factor graphs~\citep{10430101,gurel2021knowledge,yang2022improving}.
%这些研究的普遍方案是用因子图去联合推理多个ML模型的预测结果。
These studies typically utilize factor graph reasoning to assemble predictions from multiple ML models.
%当一个模型预测错误时，因子图可以根据其它模型的预测，结合外源知识去纠正错误。
When one model predicts incorrectly, the factor graph can combine the exogenous knowledge to correct the error based on the predictions of other models.
%在本文中，我们没有用因子图集成的外源知识来提升预测精度，而是探索了用这些外源知识来引导可解释神经网络生成可理解的解释的可能性。
Empirical evidence suggests that integration of exogenous knowledge in factor graphs contributes to the predictive accuracy of ML models.
In this paper, instead of improving predictive accuracy, we explore the possibility of using exogenous knowledge to guide interpretable neural networks for generating comprehensible explanations.

\section{Notations and Preliminaries}\label{section-PRELIMINARY}
%我们首先介绍因子图，它为可解释学习提供了先验的逻辑知识。然后给出了可解释神经网络的定义和我们的任务制定。表 I 列出了本文中使用的主要符号。
\begin{wrapfigure}{r}[0cm]{0pt}
\includegraphics[width=0.3\linewidth]{img/FactorGraph.jpg}
\vspace{-1em}
\caption{An example of the factor graph. It consists of 4 factors and 8 variables.}
\label{fig:FactorGraph}
\vspace{-1em}
\end{wrapfigure}

\paragraph{Interpretable Neural Network.}
%可解释神经网络被定义为可以在推理中自动为决策生成解释的神经网络，其也被称为自解释神经网络。
%定义1：可解释神经网络
%假设给定一个输入x，可解释神经网络根据x同时生成一个解释E和一个预测类别Y,其中解释被用来E描述X-y映射的归因。
%根据解释的表示形式，可解释神经网络可以被分为特征级的和概念级的。
%本文专注于设计一种概念级可解释神经网络。概念级可解释神经网络将解释E形式化为一组布尔值向量A，向量中的每个元素对应一个事先定义的概念，布尔的元素值为1表示这个概念的语义被神经网络捕获，并作为将X预测为Y的重要特征信息。在本文中，我们利用概念瓶颈结构来实现上述直觉。我们给出了概念瓶颈结构形式化定义：
%定义2：概念瓶颈结构
%概念瓶颈结构包含一个概念预测器C和一个类别预测器Y。首先，概念预测器由B参数化，去学习概率分布P1。然后，类别预测器由B参数化，学习概率分布P2。概念瓶颈结构的目标是最大化互信息I,让概念预测器的输出尽可能保留Y的信息。最后，F的输出被二值转化为一个布尔值向量，去解释神经网络的决策。

Interpretable neural networks are defined as neural networks that automatically generate explanations for decisions~\citep{esterhuizen2022interpretable,rieger2020interpretations,DBLP:journals/tnn/PengTYBLA24}. 
%在本文中，我们的模型生成概念级的解释，因为概念级的解释比特征级是更可理解。
For more comprehensible explanations, we utilize a concept bottleneck model to generate concept-level explanations, which utilize various semantic concepts to explain the predictions~\cite{koh2020concept,Huang_Song_Hu_Zhang_Wang_Song_2024}.
%具体地，让x代表输入样本，概念级的可解释神经网络预测类别y，并输出一个有k个概念的布尔值向量c。
Specifically, let $x$ denote an input sample, the concept bottleneck model predicts the category $y$ and outputs a boolean vector $\mathbf{c}\in\left\{ {0,1} \right\}^{M}$ of $M$ concepts.
%c是模型预测的解释。令c代表一个概念。c=1表示概念c存在于x中并影响模型决策,反之亦然。
Let $c\in\mathbf{c}$ denote a concept.
Let $c=1$ indicate that concept $c$ is present in $x$ and influences the model decision.
$\mathbf{c}$ is the concept-level explanation of the model prediction.

\paragraph{Factor Graph.}
%因子图是一种概率图模型，用于描述事件之间的关联。为此，它的节点类型包含描述事件的变量节点，和描述事件之间关联的因子节点两种。因子图的边只存在于变量节点和因子节点之间，而相同类型的节点间没有边。在本文中，我们利用因子图去编码语义概念和预测类别间的逻辑规则，来为解释提供人类的先验知识，提升解释的人类可读性。为此，我们让每个变量节点表示一个概念，让因子节点表示概念之间的逻辑规则。
Factor graph serves as a probabilistic graphical model to depict relationships among events~\citep{10.1145/3539597.3573024,910572}. 
As shown in Figure~\ref{fig:FactorGraph}, within a factor graph, two node types exist:~1)~variables, which delineate events;~2)~factors, which articulate the relationships between events. 
Formally, a factor graph ${\cal G} = \left( {\cal V,\cal F} \right)$ contains the set of variables $\cal V$ and the set of factors $\cal F$. 
We denote the set of edges as $\cal E$.
For any $v_i\in \cal V$ and $f_i\in \cal F$, we let $\left(v_i, f_i\right)\in \cal E$ denote an edge of $\cal G$.
Let $ {\cal N}\left(f_i\right)= \left\{ {v_i \in {\cal V}\left| \left(v_i, f_i\right)\in \cal E \right.} \right\}$ denote the set of neighbors of factor $f_i$ in $\cal G$.
%在本文中，我们让变量对应概念和类别标签，让因子对应逻辑规则。这样，因子图就可以编码概念和类别之间的逻辑关联。
We let variables correspond to concept and category labels. We let factors correspond to logical rules. 
This enables ${\cal G}$ to encode logical rules between concepts and categories.
%In this paper, we employ the factor graph to encode logical rules between semantic concepts and prediction categories, thus leveraging human prior knowledge~\cite{xia2021graph}.
%一个因子图G包含因子集合和一个变量集合。
\paragraph{Known and Unknown Perturbation.}
%形式化地，让A表示一个扰动。
Formally, let $\delta$ denote perturbations uniformly. 
%模型的设计者针对A设计对抗样本，得到重训练的模型F，使得B最小化。其中Q是模型参数
The designer of the model crafts adversarial samples against one perturbation $\delta_k$ to obtain a retrained model $h$ that minimizes $\left\| {h\left( {x;\theta } \right) - h\left( {x + \delta_k ;\theta } \right)} \right\|$. $\theta$ is the model parameter.
%对于模型f, R是已知扰动，而A是未知扰动。
For model $h$, $\delta_k$ denotes one known perturbation, and any ${\delta _u} \in \left\{ {\delta \left| {\delta  \ne {\delta _k}} \right.} \right\}$ denotes one unknown perturbation.

\paragraph{Adversarial Attacks against Concept-level Explanations.}
%不同于标准的对抗性攻击，对解释的对抗性攻击不会破坏模型预测。对于概念级可解释模型，这种攻击被分为三种方式：A, B和C。
Unlike standard adversarial attacks, adversarial attacks against explanations do not compromise task predictions. 
For concept-level interpretable models, such attacks can be categorized into three types: erasure attacks, introduction attacks, and confounding attacks~\citep{sinha2023understanding}.
1) Erasure attacks:
the goal of the erasure attacks is to subtly remove concepts from a concept-level explanation; 
2) introduction attacks:
the goal of introduction attacks is to allow the existence of irrelevant concepts;
3) confounding attacks:
the goal of confounding attacks is to simultaneously remove relevant concepts and introduce irrelevant concepts.
These attacks are technically simpler to implement (see the Appendix~\ref{ap:ImplementationDetailsofAdversarialAttacks} for implementation details of these attacks).
\section{The Design of AGAIN}\label{section-The design of AGAIN}
%图 2 展示了AGAIN的整体架构，它被分为三个阶段：
%AGAIN被设计为三个阶段
AGAIN consists of three modules:~1)~first, we encode logic rules of the real-world as a factor graph (Section~\ref{method3.1}); 2)~then, we generate the initial concept-level explanation through the concept bottleneck. 
The factor graph reasoning is utilized to identify whether the explanation of concept bottleneck violates the external logic, and thus to detect whether the perturbation exists (Section~\ref{method3.2}); 
3)~finally, an interactive intervention strategy is designed to rectify the explanation and input it to the category predictor (Section~\ref{method3.3}).
The overall architecture of AGAIN is shown in Figure~\ref{fig:AGAIN}.
\begin{figure}
\includegraphics[width=1\linewidth]{img/AGAIN-bc.jpg}
\vspace{-1em}
\caption{Overall structure of AGAIN.}
\label{fig:AGAIN}
\vspace{-1em}
\end{figure}
\subsection{Factor Graph Construction}\label{method3.1}
To construct a factor graph, we first define the logic rule set ${\cal R}={\left\{ {r_i} \right\}_{i=1}^N}$, which contains two types: 1) concept-concept rule: all predicates consist of concepts. 
Such rules are used to constrain potential relationships among various concepts. 
For instance, there is a rule of coexistence or exclusion between concepts ${c_i}$ and ${c_j}$, which can be formalized in logical notation as: ${c_i} \Leftrightarrow {c_j}$ or ${c_i} \oplus {c_j}$. 
\begin{wrapfigure}{r}[0cm]{0pt}
\includegraphics[width=0.37\linewidth]{img/AGAIN-a.jpg}
\vspace{-1em}
\caption{Factor graph construction.}
\label{fig:AGAIN-a}
\vspace{-1em}
\end{wrapfigure}
2) Category-concept rule: all predicates are defined by concepts and categories. 
Such rules are used to constrain potential correlations between concepts and categories. 
For instance, the coexistence or exclusion rule that exists between concept ${c_i}$ and category label ${y_j}$ can be formalized as: ${c_i} \Leftrightarrow {y_j}$ or ${c_i} \oplus {y_j}$.


Then, we encode the above logic rules into a factor graph $\mathcal{G}$. 
As shown in Figure~\ref{fig:AGAIN-a}, we illustrate the construction of the factor graph.
Specifically, there are two types of variables ${\cal V}={{\cal V}^c \cup {\cal V}^y}$,
where ${\cal V}^c$ and ${\cal V}^y$ denote concept and category variable set, respectively, and are linked by $\cal F$.
In this way, each factor $f_i \in {\cal F}$ corresponds to the $i$-th logic rule $r_i$.
%另外，我们为每个因子fi设置一个权重和一个势函数fi.
%另外，我们为每个因子fi设置一个势函数fi.
Each factor is defined as a potential function that performs logical operations based on different rules, which can be categorized into coexistence and exclusion operations.
Moreover, we define a potential function $\psi _i$ for each factor $f_i$, which outputs a boolean value for each ${\cal N}\left(f_i\right)$.
If ${\cal N}\left(f_i\right)$ makes $r_i$ true, $\psi _i\left( {\cal N}\left(f_i\right) \right) = 1$, otherwise $\psi _i\left( {\cal N}\left(f_i\right) \right) = 0$.

For convenience, we denote $\psi _i\left( {\cal N}\left(f_i\right) \right)$ as $\psi _i$.
%我们为每个fi设置了一个权重wi，用来表示fi的置信度。
We define the weight $w_i \in \left[0,1\right]$ to represent the confidence level of $f_i$ in two methods, i.e., prior setting and likelihood estimation~(The details can be referred to the Appendix~\ref{ap:Estimation of Weights}).
% We set weight $w_i \in \left[0,1\right]$ to indicate the confidence level of the $f_i$.
% We provide two methods, prior setting and likelihood estimation, for setting up each $w_i$. 
% For details, see Appendix~\ref{ap:Estimation of Weights}.
%越高的wi说明fi的逻辑规则对推理越重要，反之亦然。\mathbb{R}_{\geq 0}
Higher $w_i$ indicates that the logic rules of $f_i$ are more important for reasoning, and vice versa.
\subsection{Explanatory Logic Errors Identification}\label{method3.2}
%在构建g之后，AGAIN生成一个初始的概念级解释，之后推理g来识别解释的逻辑偏差。
AGAIN generates an initial concept-level explanation and identifies logical errors in the initial explanation.
Specifically, we employ a concept bottleneck structure, a popular concept-level interpretable module, to capture the semantic information from instances, which can learn the mapping between semantic information and concepts~\citep{Koh53382020}. 
%概念瓶颈包含一个概念预测器A和一个类别预测器B。
The concept bottleneck contains a concept predictor ${h_c}:{\mathbb{R}^D} \to {\mathbb{R}^M}$ and a category predictor ${h_y}:{\mathbb{R}^M} \to {\mathbb{R}}$.
The instance $x$ is first mapped to the concept space by ${h_c}$ and obtains the corresponding concept activation vector, i.e., $\hat{\mathbf{c}} = h_c(x)$, where $\hat{\mathbf{c}}\in {\left[ {0,1} \right]^M}$, and then the conceptual activation vector is fed into ${h_y}$ to yield the final predicted category, i.e., $\hat{y} = h_y(\hat{\mathbf{c}})$, where $\hat{y}\in \left\{ {0,1} \right\}$.
$\hat{\mathbf{c}}$ is defined as the initial explanation.

%A和B经过推理后，因子图将c和y作为输入，为变量D和E赋值。
Next, $\cal G$ takes $\hat{\mathbf{c}}$ and $\hat{y}$ as inputs to assign ${\cal V}^c$ and ${\cal V}^y$, respectively.
%对于概念变量，我们设置如果C>0.5,则Vi=1，否则vi=0.
If concept ${\hat c}>0.5, {\hat c} \in \hat{\mathbf{c}}$, we set variable ${v_{\hat c}} = 1, {v_{\hat c}} \in {\cal V}^c$, otherwise ${v_{\hat c}} = 0$.
%对于类别变量，我们设置v=1, 并b=0.
For the category variables, we set ${v_{\hat{y}}} = 1, {v_{\hat{y}}} \in \mathcal{V}^y$, and $\mathcal{V}^y \setminus {v_{\hat{y}}} = \left\{ 0 \right\}^{K - 1}$.

Subsequently, we evaluate the likelihood of the variable assignment under rule constraints through logical reasoning. 
Firstly, after each variable (concept and category) in $\mathcal{G}$ is assigned a value, boolean values are output from potential functions of all factors. These boolean values indicate whether the assignments of concepts and categories satisfy the logical rules represented by potential functions. Therefore, the weighted sum of all potential functions quantifies the extent to which concept assignments satisfy the logic rules in $\mathcal{G}$.

Then, we seek to obtain the likelihood of the current concept assignments occurring, conditional on the known categories and logic rules. We quantify this likelihood by computing a conditional probability using the weighted sum of potential functions. We consider all possible concept assignments and compute the expectation of current concept assignments. This expected value is considered as the conditional probability, which is then used to detect whether concept activations are perturbed.
For illustrative purposes, we provide an example. Suppose there are concepts $A$ and $B$. The current concept assignment is $\{1,0\}$ denoting $A=1$ (active) and $B=0$ (inactive). We iterate through all four possible assignments: $\{1,0\}, \{0,1\}, \{1,1\}, \{0,0\}$. We compute the weighted sum of the potential functions for each of the four cases and compute the expectation of the potential function for $\{1,0\}$. This expectation is the conditional probability that concept assignment $\{1,0\}$ occurs conditionally on the known categories and logic rules.
Formally, we denote this conditional probability as $\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$:
\begin{equation}
\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right) = {\exp \left( {\sum\limits_{i \in N} {{w_i}} {\psi _i}} \right) \mathord{\left/
 {\vphantom {{\prod\limits_{i \in N} {{w_i}} {\psi _i}} {\sum\limits_{\tilde{{\cal V}^c} \in \Phi } {\left( \exp \left( {\sum\limits_{i \in N} {{w_i}} {\psi _i}} \right) \right)} }}} \right.
 \kern-\nulldelimiterspace} {\sum\limits_{\tilde{\cal V}^c \in \Phi } {\left( \exp \left( {\sum\limits_{i \in N} {{w_i}} {\psi _i}} \right) \right)} }}
\label{eq:jointprobability}
,
\end{equation}
where $\Phi$ represents all cases of concept assignments, and $\tilde{\cal V}^c$ represents a case in $\Phi$. 
%这意味着，公式2的分母为归一化常数项，我们用一个等价的符号Z来表示。
This implies that the denominator of Eq.~(\ref{eq:jointprobability}) is the normalized constant term.
We use $\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$ to evaluate the comprehensibility of explanation $\hat{\mathbf{c}}$.
Higher $\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$ indicates that $\hat{\mathbf{c}}$ is more comprehensible, and vice versa. In theory, we consider that a comprehensible $\hat{\mathbf{c}}$ should satisfy each $r_i \in \cal R$, ensuring that $\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$ attains an upper bound denoted as ${}_ \vee \mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$:
\begin{equation}
{}_\vee \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right) = \frac{1}{a}\max\left( \exp \left( {\sum\limits_{i \in N} {{w_i}} {\psi _i}} \right) \right),
\end{equation}
where $a$ denotes the denominator of Eq.~(\ref{eq:jointprobability}).
However, in practice, even concept explanations generated in a benign environment (without perturbations) rarely satisfy all the rules. 
Overly strict logical constraints may instead cause $\mathcal{G}$ to lose its ability to recognize perturbations. 
Therefore, we allow a comprehensible explanation to violate some low-weight rules. 
Naturally, we establish a relaxed identification condition for distinguishing explanations corrupted by perturbations from comprehensible explanations:
 \begin{equation}
\mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right) > \partial \cdot {}_\vee \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right)
,
\label{eq:identificationcondition}
\end{equation}
where $\partial  \in \left[ {0,1} \right]$ is a hyperparameter controlling the relaxation.
$\partial$ approximate 1 implies a stricter constraint imposed by $\mathcal{G}$ on the explanation.
%P>PW,则MLN判定解释为可理解的解释，否则视为错误解释。W是控制松弛度的在[0,1]的一个超参数，w越大，MLN对解释的约束越严格。换句话说，高的W意味着解释更容易被判定为是错误的。
If ${\cal V}^c$, ${\cal V}^y$ satisfies Eq.~(\ref{eq:identificationcondition}), then $\hat{\mathbf{c}}$ is comprehensible; otherwise, it is recognized as having logical error under perturbation. Further, we demonstrate theoretically that $\cal G$ contributes to comprehensible explanations. For a detailed theoretical analysis, please refer to Appendix~\ref{appendix:TheoreticalAnalysis}.

\subsection{Explanation Rectification}\label{method3.3}
%一旦识别出具有逻辑偏差的解释
Once explanations with logical errors are identified, AGAIN rectifies the explanation and put it as an input to the category predictor for the final prediction. 
For this objective, we propose an interactive intervention switch strategy aimed at enhancing the conditional probability of the ${\cal G}$. The proposed strategy intervenes on the values of ${\cal V}^c$ and interactively observing the potential function difference. 
In this paper, we assume that $\hat{y}$ are unaffected under perturbations, thus we do not intervene in ${\cal V}^y$.
The pseudocode of the interactive intervention switch is listed in Appendix~\ref{appendix:Algorithms}.

Our intervention strategy can be divided into three steps.
First, we traverse all factors with ${\psi_i} = 1$. 
For factor $f_i \in \cal F$, we modify the boolean value of its concept variables, considering the modification as a single intervention operation. 
Given that $f_i$ may be connected to multiple concept variables, there exist numerous intervention cases. 
For instance, consider $f_i$ containing concept variables $v_i$ and $ v_j$. There are three possible intervention cases: intervene only $v_i$, intervene only $v_j$, and intervene both $v_i$ and $v_j$. 
We define the full set of possible intervention cases for $f_i$ as ${\cal T}_i$. 
For each case $t_{i} \in {\cal T}_i$, we compute the potential function difference $s_{i}$, which represents the change in the potential function after executing $t_{i}$.
%注意，ti不仅仅会改变y,也会影响Vi的一跳因子的势函数.
Note that $t_{i}$ does not only change $f_i$, but also changes the 1-hop neighbor factors of ${\cal N}\left(f_i\right)$.
%因此，我们定义si定义为如下：
Thus, we define $s_{i}$ as follows:
\begin{equation}
s_{i} = \sum_{j \in \left| {\cal F}_i \right|}{w_j} \left( \psi_j^{t_i} - \psi_j \right)
,
\label{potential_function_gain}
\end{equation}
where ${\cal F}_i = {\left\{ {{f_j}\left| {{\cal N}\left( {{f_i}} \right) \in {\cal N}\left( {{f_j}} \right)} \right.} \right\}} \cup \left\{ {{f_i}} \right\}$.
%A表示ti干预后的b值。
$\psi_j^{t_i}$ denotes the $\psi_j$ value after $t_i$ intervention.
Subsequently, after traversing through all possible interventions in $\mathcal{T}_i$, we identify the intervention with the highest $s_i$ as a candidate intervention. We generate the candidate intervention for each factor with ${\psi_i} = 1$. We aggregate all the candidate interventions into a final intervention, denoted as $t_*$. We execute $t_*$ on ${\cal V}^c$. From the set of intervened ${\cal V}^c$ and $t_*$, we generate a binary concept intervention vector $\mathbf{z} \in {\left\{ {-1,1} \right\}^M}$ and a binary mask vector $\mathbf{m}_{t_*}\in\{0, 1\}^M$. $\mathbf{z}$ denotes the concept activation status, where -1 indicates activated, and 1 indicates inactivated. $\mathbf{m}_{t_*}$ denotes whether the concept is intervened or not, where 1 indicates intervened, and 0 indicates not intervened. 

Finally, we employ $\mathbf{z}$ and $\mathbf{m}_{t_*}$ to rectify the initial explanation $\hat{\mathbf{c}}$. %We utilize a sigmoid function to map binary discrete values in $\mathbf{z}$ to a continuous space, resulting in a normalized vector $\hat{\mathbf{z}}$:
%\begin{equation}
%\hat{\mathbf{z}} = \frac{1}{{1 + \exp(-4\mathbf{w_z} %\cdot \mathbf{z})}}
%,
%\label{normalized_vector_z}
%\end{equation}
%where $\mathbf{w_z}$ denotes the highest weight of factors that are connected to each intervened concept variable. We assume that if $z\in \mathbf{z}$ is obtained through an intervention constrained by a rule with higher weight, then $\hat{z} \in \hat{\mathbf{z}}$ should also be closer to 1.
We utilize $\mathbf{m}_{t_*}$ to aggregate $\hat{\mathbf{c}}$ and $\mathbf{z}$ for a rectified concept activation vector $\hat{\mathbf{c}}_{re}$:
\begin{equation}
\hat{\mathbf{c}}_{re} = \mathbf{z} \odot \mathbf{m}_{t_*} + \hat{\mathbf{c}} \odot \mathbf{m}'_{t_*}
,
\label{aggregate_rectified_vector}
\end{equation}
where $\mathbf{m}'_{t_*}$ is obtained by flipping the bits of $\mathbf{m}_{t_*}$. 
$\odot$ denotes dot product operation.
The purpose of employing the intervention mask is to facilitate $\hat{\mathbf{c}}_{re}$ to retain activations in $\hat{\mathbf{c}}$ that are not intervened.

\section{Experiments}\label{section-Experiments} 
\subsection{Experimental Settings}
\paragraph{Datasets and Baselines.}
%我们在2个真实数据集A,b,和一个合成数据集C上评估了AGAIN。
We evaluated AGAIN on two real-world datasets, CUB, MIMIC-III EWS, and one synthetic dataset, Synthetic-MNIST.
%我们考虑了6个概念级可解释神经网络基线，包括 A [10]、B [28]、C [22]。
%We selected 14 baselines from two categories of methods: concept-level interpretable methods and adversarial perturbation defense methods based on knowledge integration. See Appendix A for details of the baselines.
We choose two categories of methods.
(1) Concept-level methods: CBM~\citep{Koh53382020}, Hard AR~\citep{havasi2022addressing}, ICBM~\citep{Chauhan59482023}, PCBM~\citep{yuksekgonul2023posthoc},  ProbCBM~\citep{kim2023probabilistic}, Label-free CBM~\citep{oikarinen2023labelfree}, ProtoCBM~\citep{Huang_Song_Hu_Zhang_Wang_Song_2024}, and ECBMs~\citep{xu2024energybased}.
(2) Knowledge integration methods: DeepProblog~\cite{manhaeve2018deepproblog}, MBM~\cite{patel2022modeling}, C-HMCNN~\cite{giunchiglia2020coherent}, LEN~\cite{ciravegna2023logic}, DKAA~\cite{melacci2021domain}, and MORDAA~\cite{yin2021exploiting}.
% We consider 6 concept-level interpretable neural network baselines, including CBM~\citep{Koh53382020}, Hard AR~\citep{havasi2022addressing}, ICBM~\citep{Chauhan59482023}, PCBM~\citep{yuksekgonul2023posthoc},  \textcolor{red}{ProbCBM}~\citep{kim2023probabilistic}, Label-free CBM~\citep{oikarinen2023labelfree}, ProtoCBM~\citep{Huang_Song_Hu_Zhang_Wang_Song_2024}, and ECBMs~\citep{xu2024energybased}.
%此外，我们对比了我们的重训练版本。
In addition, we compare AGAIN with the retrained versions of these baselines that employ state-of-the-art adversarial training strategy.
%有关数据集和基线的更多详情分别见附录 A.5 和 A.6。
More details on the datasets and baselines are provided in Appendix~\ref{ap:Datasets and Data Preprocessing} and \ref{ap:Baselines}. The experimental results on the synthetic dataset are presented in Appendix~\ref{ex5.3}.

\paragraph{Evaluation Metrics and Implementation Details.} To evaluate the performance of AGAIN, we use five metrics: predictive accuracy~(P-ACC), explanatory accuracy~(E-ACC), logical satisfaction metric~(LSM), identification rate~(IR), and success rate~(SR). 
Higher scores indicate better performance for all metrics.
Detailed descriptions of each metric are given in Appendix~\ref{ap:EvaluationMetric}. Additionally, the implementation details of AGAIN are provided in Appendix~\ref{ap:ImplementationDetails}.
%我们同时在Synthetic-MNIST和CUB数据集上运行我们的实验。所有的数据处理和实验在一台具有两块Xeon-E5处理器，两块RTX4000GPU和64G内存的服务器上被执行。我们分别在两个数据集上构建了逻辑规则集和对应的因子图.
%在两个数据集上的数据处理的细节信息被总结为如下：
\subsection{Experimental Results on Real-world Datasets}\label{ex5.2}
%我们在CUB和EW上实现AGAIN，并评估AGAIN的精度，有效性以及。

\paragraph{Identifying Perturbations.}
% \begin{wraptable}{r}{0.46\textwidth}\scriptsize
% \centering
%   \caption{IR and SR on two real datasets.}
%   \renewcommand\arraystretch{0.9}
%       \begin{tabular}{c|c|cccc}
%       %{\linewidth}{>{\centering\arraybackslash}m{0.4cm}|>{\centering\arraybackslash}m{0.4cm}|>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}}
%     \toprule
%     \multicolumn{2}{c|}{\multirow{2}[0]{*}{Perturbation Type}} & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{MIMIC-III EWS} \\
%     \cmidrule{3-6}
%     %\multicolumn{2}{c}{} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{SR} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{SR} \\
%     \multicolumn{2}{c|}{} & IR & SR & IR & SR \\
%     \midrule
%     \multicolumn{2}{c|}{Clear} & / & 98.78 & / & 100.00  \\
%     \midrule
%     \multirow{4}[0]{*}{$\delta_k$} &$\epsilon$=4& 97.34 & 98.01 & 97.41 & 98.91  \\
%           &$\epsilon$=8& 98.93 & 98.71 & 98.34 & 99.32  \\
%           & $\epsilon$=16     &98.91 & 97.73 & 99.76 & 98.76  \\
%           &$\epsilon$=32&99.31 & 97.29 & 99.82 & 98.21  \\
%     \midrule
%     \multirow{4}[0]{*}{$\delta_u$} &$\epsilon$=4&97.34 & 97.48 & 98.35 & 99.30 \\
%           &$\epsilon$=8&97.27 & 97.20 & 99.57 & 97.12  \\
%           &$\epsilon$=16&97.56 & 96.82 & 100.00 & 98.63 \\
%           &$\epsilon$=32&98.33 & 97.75 & 100.00 & 99.41\\
%     \bottomrule
%     \end{tabular}%
%   \label{tab:exIdentifying attributional perturbations}%
% \end{wraptable}%
We apply adversarial perturbations acquired during black-box training to randomly perturb multiple instances in the test set. Known and unknown perturbations are denoted by $\delta_k$ and $\delta_u$, respectively, with $\epsilon$ representing the perturbation magnitude. We evaluate the ability of AGAIN to recognize logical errors of explanations by reporting SR and IR values under different perturbation magnitudes in Table \ref{tab:exIdentifying attributional perturbations}. The results demonstrate that AGAIN achieves remarkable IR and SR values under both $\delta_k$ and $\delta_u$. Specifically, AGAIN attains nearly 100\% IR across all perturbation magnitudes. With SR results averaging up to 98\%, we also validate that factor graph ${\cal G}$ can effectively identify explanations from benign instances and permit them to directly predict categories without logical reasoning. Furthermore, it is also worth noting that as the perturbation magnitude increases, the IR value also gets larger. This observation is attributed to the larger perturbation magnitude causing a more pronounced logical violation in the generated explanations. The ${\cal G}$ more readily identifies these violations.
\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{IR and SR on two real-world datasets.}
  \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {>{\centering\arraybackslash}m{1.63cm}|
    >{\centering\arraybackslash}m{0.65cm}|
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}|
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}
    >{\centering\arraybackslash}m{0.78cm}}
    \toprule
    \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Metrics} & \multicolumn{1}{c}{\multirow{2}[2]{*}{Clear}} & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
\cmidrule{4-11}          &       &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
    \midrule
    \multirow{2}[1]{*}{CUB} & IR & -  & 97.3(1.3) & 98.9(0.4) & 98.9(0.8) & 99.3(1.0) & 97.3(0.5) & 97.2(0.8) & 97.5(1.1) & 98.3(0.9) \\
          & SR & 98.7(0.4) & 98.0(1.2)  & 98.7(0.4) & 97.7(0.6) & 97.2(0.5) & 97.4(0.7) & 97.2(1.1)  & 96.8(0.9) & 97.7(1.1) \\
    \midrule
    \multirow{2}[1]{*}{MIMIC-III EWS} & IR & - & 97.4(0.2) & 98.3(0.3) & 99.7(0.2)  & 99.8(0.1) & 98.3(0.4) & 99.5(0.1) & 100.0(0.0) & 100.0(0.0) \\
          & SR & 100.0(0.0) & 98.91(0.4) & 99.3(0.1) & 98.7(0.4) & 98.2(1.1) & 99.3(0.3) & 97.1(0.3) & 98.6(0.4) & 99.4(0.1) \\
    \bottomrule
    \end{tabularx}%
  \label{tab:exIdentifying attributional perturbations}%
  \vspace{-1em}
\end{table*}%
\paragraph{Comprehensibility of Explanations.} To investigate the comprehensibility of the explanations generated by AGAIN, we perform extensive experiments on both datasets for evaluating the LSM of the explanations, and the comparison are reported in Table \ref{tab:Comprehensibility of explanations}.
The baselines subjected to the retraining are identified by the "-AT" suffix.
The results reveal that the comprehensibility of the explanations generated by AGAIN outperforms all concept-level methods, including the "-AT" versions of these baselines, under different perturbation magnitudes.
% The results reveal that the comprehensibility of the explanations generated by AGAIN outperforms all baseline methods, including the "-AT" versions of these baselines, under different perturbation magnitudes.
Particularly, previous interpretable models fail to generate logically complete explanations with LSMs lower than 48 under unknown perturbations of magnitude 32, but explanations from AGAIN can reach as high as 92.30. Moreover, we demonstrate that AGAIN is hardly affected by the perturbation magnitude compared to the baseline methods. This effect is attributed to the corrective capability provided by ${\cal G}$ for any level of logic violation.
For kowledge integration methods, since DeepProblog, MBM, and C-HMCNN are unable to generate concepts, we splice their knowledge integration modules onto the CBM.
%This ensures that all spliced variants are capable of generating a set of concepts.
The results show that the LSM of AGAIN is optimal.
In contrast, deepProblog can only constrain category predictions, not concept predictions, which results in low LSM under perturbation.
The knowledge introduced by methods MBM and C-HMCNN can constrain concepts, but they only use logical rules between concepts and concepts, making their performance inferior to AGAIN.
Meanwhile, since DKAA and MORDAA have Multi-label predictors, we directly use Multi-label predictors to predict concepts.
%The results indicate that AGAIN achieves the highest LSM. 
%In contrast, 
LEN can only constrain category predictions. 
DKAA and MORDAA detect adversarial perturbations in the samples using external knowledge, but they cannot correct the wrong concepts triggered by these perturbations. 
%Therefore, factor graph-based knowledge integration is more effective than the above methods in enhancing the comprehensibility of explanations under unknown perturbations.
\begin{table*}[h]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of LSM for AGAIN with other concept-level interpretable baselines.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {>{\centering\arraybackslash}m{0.9cm}|
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}|
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}
    >{\centering\arraybackslash}m{0.70cm}}
    \toprule
    \multirow{2}[2]{*}{Dataset} & \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{4-11}
          &       & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule
    \multirow{13}[2]{*}{CUB} & CBM & 96.3(2.2) & 89.2(3.5) & 77.4(2.8) & 53.7(1.3) & 39.4(5.4) & 89.3(3.1) & 77.4(5.5) & 53.1(3.8) & 39.4(5.3) \\
          & Hard AR & 85.6(0.9) & 77.3(1.2) & 66.4(3.2) & 50.8(1.8) & 47.6(0.8) & 77.4(0.5) & 66.2(2.6) & 50.4(1.4) & 47.2(1.8) \\
          & ICBM & 95.4(1.3) & 86.4(0.8) & 77.3(0.4) & 56.4(1.6) & 39.5(1.5) & 86.7(0.7) & 77.6(2.4) & 56.9(1.9) & 39.8(3.2) \\
          & PCBM & 95.7(1.6) & 85.6(1.0) & 76.3(1.5) & 58.7(2.1) & 40.3(1.2) & 87.5(1.5) & 78.1(1.4) & 57.6(1.8) & 41.6(2.6) \\
          & ProbCBM & \underline{\textbf{96.8(0.4)}} & 85.5(0.2) & 77.6(1.1) & 59.7(1.5) & 39.7(1.3) & 87.7(1.2) & 77.4(1.0) & 57.4(1.4) & 40.3(1.2) \\
          & Label-free CBM & 96.4(1.3) & 84.3(1.7) & 76.9(1.4) & 58.5(2.1) & 40.8(0.3) & 87.3(1.6) & 77.8(1.2) & 57.3(1.4) & 40.0(1.8) \\
          & ProtoCBM & 97.3(0.2) & 92.3(1.3) & 84.5(1.6) & 64.5(3.2) & 54.3(1.3) & 92.4(1.1) & 84.4(1.9) & 64.5(2.4) & 54.1(3.6) \\
          & ECBMs & 96.4(1.3) & 92.1(1.5) & 87.5(3.7) & 70.4(2.8) & 64.7(2.1) & 92.2(0.9) & 86.6(3.7) & 70.4(2.9) & 64.4(6.1) \\
           \cmidrule{2-11} 
          & CBM-AT & 93.4(1.4) & \underline{\textbf{92.9(1.3)}} & 85.6(0.7) & 75.6(1.7) & 59.6(1.6) & 87.6(0.6) & 77.5(1.0) & 53.3(1.9) & 39.7(1.5) \\
          & Hard AR-AT & 82.5(0.3) & 78.6(0.5) & 76.6(1.6) & 69.9(1.3) & 60.5(1.7) & 76.6(0.6) & 65.2(1.4) & 51.9(1.1) & 47.5(2.1) \\
          & ICBM-AT & 91.5(1.3) & 91.6(2.4) & 86.3(1.9) & 79.3(1.6) & 70.6(1.5) & 80.4(1.2) & 77.6(2.1) & 56.4(1.8) & 39.4(1.6) \\
          & PCBM-AT & 93.6(0.6) & 92.5(1.6) & 84.4(0.9) & 76.5(1.3) & 70.9(1.9) & 80.4(1.5) & 75.3(1.2) & 55.6(1.6) & 41.6(2.1) \\
          & ProbCBM-AT & 93.5(0.9) & 90.3(1.4) & 83.3(1.3) & 78.2(1.6) & 70.3(1.8) & 87.4(0.9) & 77.6(1.2) & 57.5(1.6) & 40.6(1.4) \\
          & Label-free CBM-AT & 93.4(1.3) &91.4(0.8) & 86.2(1.4) & 80.9(1.6) & 78.5(1.5) & 87.8(1.4) & 77.4(1.8) & 57.7(1.7) & 41.8(2.9) \\
          & ProtoCBM-AT & 94.4(0.7) & 92.7(1.1) & 87.2(1.9) & 70.3(4.2) & 68.7(2.7) & 91.7(1.2) & 82.5(1.5) & 60.2(2.4) & 52.1(6.1) \\
          & ECBMs-AT & 93.6(1.0) & 91.9(2.5) & 88.1(2.1) & 83.1(3.8) & 78.4(3.4) & 90.7(2.4) & 83.7(2.5) & 68.7(2.7) & 66.7(3.7) \\
          \cmidrule{2-11}
          & LEN & 96.4(0.8) &  89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) & 89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) \\
           & DKAA & 96.2(1.1) & 91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) & 91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) \\
        & MORDAA & 96.5(0.1) &  91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) & 91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) \\
     & DeepProblog & 96.4(0.2) &  89.2(3.5) & 77.4(2.8) & 53.7(1.3) & 39.4(5.4) & 89.2(3.5) & 77.4(5.5) & 53.1(3.8) & 39.4(5.4) \\
        & MBM & 96.2(0.2) &  93.5(3.1)  & 90.3(2.4) & 88.7(3.2) & 85.7(6.7) & 93.5(3.2)  & 90.3(2.7) & 88.7(3.2) & 85.7(6.7) \\
        & C-HMCNN & 96.5(0.4) &  93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) & 93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) \\
          \cmidrule{2-11}
          & AGAIN & 96.3(0.5) & 92.4(1.2) & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} & \underline{\textbf{94.5(1.6)}} & \underline{\textbf{93.3(1.7)}} & \underline{\textbf{93.8(1.4)}} & \underline{\textbf{92.1(2.1)}} \\
 \midrule    \multirow{13}[2]{*}{\makecell{MIMIC-III\\EWS}} & CBM & 95.7(0.2) & 90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) & 90.7(0.9) & 75.7(1.3) & 50.9(1.4) & 30.7(1.5) \\
          & Hard AR & \underline{\textbf{96.7(0.3)}} & 78.8(1.5) & 69.6(1.3) & 53.8(1.7) & 45.3(1.6) & 77.4(1.8) & 65.3(1.3) & 53.8(3.2) & 47.9(2.8) \\
          & ICBM & 95.6(0.4) & 86.5(1.3) & 75.0(1.6) & 56.8(2.1) & 39.3(3.2) & 86.5(1.7) & 77.6(1.4) & 56.7(2.1) & 30.7(1.9) \\
          & PCBM & 96.1(0.2) & 86.5(1.4) & 73.0(1.2) & 53.8(2.5) & 44.2(2.6) & 88.4(1.2) & 78.7(1.4) & 57.8(2.2) & 32.6(2.5) \\
          & ProbCBM & 96.1(0.1) & 84.6(1.4) & 76.6(1.6) & 56.9(1.3) & 39.7(3.1) & 86.8(1.4) & 76.9(3.1) & 57.4(3.5) & 40.3(4.0) \\
          & Label-free CBM & 96.1(0.1) & 86.5(1.2) & 76.5(1.6) & 65.5(2.1) & 40.3(2.3) & 86.9(0.9) & 77.6(4.1) & 56.8(6.3) & 42.3(7.4) \\
          & ProtoCBM & 96.7(0.6) & 87.6(1.4) & 81.4(1.0) & 76.4(1.4) & 70.8(2.4) & 87.4(1.2) & 81.7(1.1) & 76.3(2.1) & 69.4(2.4) \\
          & ECBMs & 97.9(0.2) & 88.4(1.2) & 79.4(1.3) & 70.8(2.6) & 65.6(3.2) & 88.6(2.7) & 79.4(2.1) & 70.4(5.4) & 66.1(4.8) \\
           \cmidrule{2-11} 
          & CBM-AT & 94.2(0.4) & 90.3(1.2) & 85.4(1.3) & 78.8(1.9) & 60.9(1.9) & 85.7(2.5) & 77.5(2.3) & 50.8(3.1) & 40.8(3.7) \\
          & Hard AR-AT & 94.2(0.7) & 88.4(1.4) & 76.9(1.6) & 70.2(2.6) & 65.3(3.1) & 77.5(1.1) & 62.1(1.1) & 50.7(1.1) & 44.2(1.1) \\
          & ICBM-AT & 92.3(0.4) & 90.3(2.1) & 86.3(1.9) & 86.5(2.7) & 71.1(3.5) & 86.5(2.6) & 77.6(4.7) & 58.7(6.8) & 39.4(9.3) \\
          & PCBM-AT & 94.2(0.6) & 90.3(1.7) & 84.4(3.2) & 76.9(3.4) & 69.2(4.7) & 81.7(3.7) & 75.3(3.3) & 54.6(4.1) & 42.3(4.9) \\
          & ProbCBM-AT & 93.0(0.4) & 91.8(1.4) & 88.4(1.5) & 78.8(3.6) & 73.0(3.8) & 86.5(1.4) & 76.9(4.8) & 56.3(7.4) & 40.6(12.8) \\
          & Label-free CBM-AT & 94.2(0.6) &89.5(1.7) & 86.7(1.8) & 84.3(3.7) & 77.3(3.8) & 84.2(1.4) & 78.9(2.5) & 56.7(4.6)& 44.2(7.9) \\
          & ProtoCBM-AT & 94.5(1.2) & 89.7(1.1) & 81.4(1.0) & 76.4(1.4) & 70.8(2.4) & 80.4(1.2) & 76.7(4.1) & 66.3(2.1) & 61.3(5.4) \\
          & ECBMs-AT & 93.9(0.6) & 89.8(4.2) & 82.3(2.1) & 75.4(2.4) & 70.6(2.8) & 79.6(2.7) & 74.2(6.2) & 69.2(6.7) & 65.9(6.5) \\
          \cmidrule{2-11}
          & LEN & 96.4(0.6) & 90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) & 90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) \\
          & DKAA & 96.5(1.5) & 96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) & 96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) \\
          & MORDAA & 95.2(1.4) & 95.9(2.4) & 93.7(2.3) & 86.3(1.8) & 79.4(4.6) & 95.9(2.4) & 93.7(2.3) & 86.3(1.8) & 79.4(4.6) \\
          & DeepProblog & 95.5(1.3) & 90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) & 90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) \\
          & MBM & 95.9(0.5) & 92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) & 92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) \\
          & C-HMCNN & 96.6(1.2) & 94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) & 94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) \\
          \cmidrule{2-11}
          & AGAIN & 96.1(0.3) & \underline{\textbf{96.1(0.7)}} & \underline{\textbf{94.2(1.4)}} & \underline{\textbf{96.1(1.2)}} & \underline{\textbf{94.2(1.2)}} & \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}}\\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:Comprehensibility of explanations}%
  \vspace{-1em}
\end{table*}%


% \begin{table*}[htbp]\scriptsize
%   \centering
%   \vspace{-1em}
%   \caption{Comparisons of LSM for AGAIN with other concept-level interpretable baselines.}
%    \renewcommand\arraystretch{0.8}
%     \begin{tabularx}{\linewidth}
%     {>{\centering\arraybackslash}m{1.0cm}|
%     >{\centering\arraybackslash}m{2.50cm}|
%     >{\centering\arraybackslash}m{0.55cm}|
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}|
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}
%     >{\centering\arraybackslash}m{0.65cm}}
%     \toprule
%     \multirow{2}[2]{*}{Dataset} & \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c|}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
%     \cmidrule{4-11}
%           &       & \multicolumn{1}{c|}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
%     \midrule
%     \multirow{13}[2]{*}{CUB} & CBM & 96.33 & 89.23 & 77.45 & 53.76 & 39.45 & 89.33 & 77.45 & 53.19 & 39.44 \\
%           & Hard AR & 85.68 & 77.34 & 66.45 & 50.81 & 47.65 & 77.47 & 66.23 & 50.45 & 47.23 \\
%           & ICBM & 95.45 & 86.45 & 77.34 & 56.45 & 39.57 & 86.75 & 77.64 & 56.92 & 39.82 \\
%           & PCBM & 95.76 & 85.65 & 76.37 & 58.76 & 40.39 & 87.56 & 78.19 & 57.69 & 41.63 \\
%           & ProbCBM & \underline{\textbf{96.84}} & 85.54 & 77.68 & 59.76 & 39.75 & 87.73 & 77.42 & 57.47 & 40.34 \\
%           & Label-free CBM & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           & ProtoCBM & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           & ECBMs & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%            \cmidrule{2-11} 
%           & CBM-AT & 93.45 & 92.94 & 85.67 & 75.68 & 59.69 & 87.65 & 77.56 & 53.37 & 39.74 \\
%           & Hard AR-AT & 82.56 & 78.65 & 76.65 & 69.95 & 60.54 & 76.63 & 65.25 & 51.93 & 47.53 \\
%           & ICBM-AT & 91.56 & 91.64 & 86.31 & 79.34 & 70.65 & 80.45 & 77.65 & 56.45 & 39.42 \\
%           & PCBM-AT & 93.65 & \underline{\textbf{92.45}} & 84.45 & 76.56 & 70.93 & 80.45 & 75.38 & 55.64 & 41.64 \\
%           & ProbCBM-AT & 93.54 & 90.36 & 83.35 & 78.23 & 70.36 & 87.45 & 77.64 & 57.58 & 40.64 \\
%           & Label-free CBM-AT & 93.46 &91.46 & 86.25 & 80.98 & 78.54 & 87.85 & 77.48 & 57.75 & 41.84 \\
%           & ProtoCBM-AT & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           & ECBMs-AT & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           \cmidrule{2-11}
%           & AGAIN (Ours) & 96.33 & 92.44 & \underline{\textbf{93.17}} & \underline{\textbf{93.84}} & \underline{\textbf{91.50}} & \underline{\textbf{94.50}} & \underline{\textbf{93.33}} & \underline{\textbf{93.85}} & \underline{\textbf{92.10}} \\
%  \midrule    \multirow{13}[2]{*}{\makecell{MIMIC-III\\EWS}} & CBM & 95.75 & 90.46 & 75.76 & 50.47 & 39.86 & 90.76 & 75.76 & 50.97 & 30.76 \\
%           & Hard AR & \underline{\textbf{96.75}} & 78.84 & 69.65 & 53.84 & 45.39 & 77.45 & 65.38 & 53.84 & 47.98 \\
%           & ICBM & 95.67 & 86.53 & 75.00 & 56.87 & 39.34 & 86.53 & 77.61 & 56.73 & 30.76 \\
%           & PCBM & 96.15 & 86.53 & 73.07 & 53.84 & 44.23 & 88.46 & 78.76 & 57.84 & 32.69 \\
%           & ProbCBM & 96.14 & 84.61 & 76.65 & 56.92 & 39.75 & 86.81 & 76.92 & 57.47 & 40.34 \\
%           & Label-free CBM & 96.12 & 86.53 & 76.59 & 65.54 & 40.34 & 86.93 & 77.65 & 56.83 & 42.30 \\
%           & ProtoCBM & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           & ECBMs & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%            \cmidrule{2-11} 
%           & CBM-AT & 94.23 & 90.38 & 85.46 & 78.84 & 60.91 & 85.71 & 77.56 & 50.87 & 40.82 \\
%           & Hard AR-AT & 94.23 & 88.46 & 76.92 & 70.22 & 65.38 & 77.56 & 62.19 & 50.76 & 44.23 \\
%           & ICBM-AT & 92.30 & 90.38 & 86.31 & 86.53 & 71.11 & 86.53 & 77.65 & 58.73 & 39.42 \\
%           & PCBM-AT & 94.23 & 90.38 & 84.45 & 76.92 & 69.23 & 81.72 & 75.38 & 54.64 & 42.30 \\
%           & ProbCBM-AT & 93.07 & 91.87 & 88.46 & 78.84 & 73.07 & 86.53 & 76.92 & 56.34 & 40.64 \\
%           & Label-free CBM-AT & 94.23 &89.58 & 86.73 & 84.34 & 77.35 & 84.23 & 78.93 & 56.73 & 44.23 \\
%           & ProtoCBM-AT & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           & ECBMs-AT & 96.45 & 84.34 & 76.92 & 58.54 & 40.82 & 87.32 & 77.82 & 57.36 & 40.08 \\
%           \cmidrule{2-11}
%           & AGAIN (Ours) & 96.15 & \underline{\textbf{96.15}} & \underline{\textbf{94.23}} & \underline{\textbf{96.15}} & \underline{\textbf{94.23}} & \underline{\textbf{94.07}} & \underline{\textbf{94.15}} & \underline{\textbf{94.23}} & \underline{\textbf{92.30}}\\
%     \bottomrule    
%     \end{tabularx}%  
%   \label{tab:Comprehensibility of explanations}%
%   \vspace{-1em}
% \end{table*}% 
\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{img/shiyan3.jpg}
\vspace{-1em}
\caption{The impact of the factor graph size on P-ACC and E-ACC across 4 perturbation magnitudes on two real-world datasets.}
\vspace{-1em}
\label{ex:Validity of the explanation}
\end{figure*}
\paragraph{Validity of the Factor Graph.}
% \begin{wraptable}{r}{0.50\textwidth}\scriptsize
%   \centering
%   \caption{P-ACC and E-ACC on two real datasets.}
%   \renewcommand\arraystretch{0.9}
%     %\begin{tabular}%{@{\hspace{10pt}}c@{\hspace{10pt}}|@{\hspace{10pt}}c@{\hspace{10pt}}|@{\hspace{8pt}}c@{\hspace{8pt}}@{\hspace{8pt}}c@{\hspace{8pt}}@{\hspace{8pt}}c@{\hspace{8pt}}@{\hspace{8pt}}c@{\hspace{8pt}}}
%     \begin{tabular}{c|c|cccc}
%     \toprule
%     \multicolumn{2}{c|}{\multirow{2}[0]{*}{Perturbation Type}} & \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{MIMIC-III EWS} \\
%     \cmidrule{3-6}
%     %\multicolumn{2}{c}{} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{SR} & \multicolumn{1}{c}{IR} & \multicolumn{1}{c}{SR} \\
%     \multicolumn{2}{c|}{} & P-ACC & E-ACC & P-ACC & E-ACC \\
%     \midrule
%     \multicolumn{2}{c|}{Clear} & 62.50 & 96.41 & 55.12 & 97.46  \\
%     \midrule
%     \multirow{4}[0]{*}{$\delta_k$} &$\epsilon$=4& 62.25 & 94.10 &49.76& 93.01  \\
%           &$\epsilon$=8& 61.25 & 94.18 &49.12& 93.26  \\
%           & $\epsilon$=16& 61.66 & 93.84 &49.40& 93.09  \\
%           &$\epsilon$=32&59.62& 93.69 &49.38& 93.01  \\
%     \midrule
%     \multirow{4}[0]{*}{$\delta_u$} &$\epsilon$=4&61.66&94.94&48.02& 93.09 \\
%           &$\epsilon$=8&60.33&94.90 &48.35& 93.63  \\
%           &$\epsilon$=16&59.47&93.56 &49.45& 93.35 \\
%           &$\epsilon$=32&59.62&93.92 &52.54& 93.23\\
%     \bottomrule
%     \end{tabular}%
%   \label{ex:tabValidity of the explanation}%
% \end{wraptable}%
As the theoretical analysis in Appendix~\ref{appendix:TheoreticalAnalysis} demonstrates, ${\cal G}$ improves the comprehensibility of explanations. 
We experimentally validate this claim and further demonstrate that increasing the number of factors in $\cal G$ enhances the predictive accuracy of concepts. 
Specifically, we employ subgraph $\cal {G'}$ extracted from the original $\cal {G}$ for reasoning and analyze the impact on prediction accuracy by increasing the ratio of $\cal {G'}$ to $\cal {G}$. 
In Figure~\ref{ex:Validity of the explanation}, we depict the changes in P-ACC and E-ACC across four perturbation magnitudes on both datasets. 
It is evident that both P-ACC and E-ACC exhibit substantial improvement as the number of factors in $\cal {G'}$ increases.
This observation indicates that $\cal G$ contributes in generating explanations with similarity to the ground truth explanations for improving the predictive accuracy.
Moreover, as the number of factors in $\cal {G'}$ exceeds that of $\cal {G}$ (ratio $>$ 1.0), E-ACC begin to converge. This also validates the setting for the number of factors in the original $\cal {G}$ is reasonable.
In addition, we report the P-ACC and E-ACC comparison results of AGAIN on the CUB dataset (see Table \ref{tab:EACC-CUB} and Table \ref{tab:PACC-CUB}). The comparison results of P-ACC and E-ACC on other datasets are provided in Appendix~\ref{E-ACC and P-ACC for AGAIN}.
The results indicate that AGAIN is optimal for E-ACC on all three datasets.
%, approaching prediction performances similar to those observed under benign conditions (clear).
Furthermore, since perturbations do not impact the final predictions, the P-ACC remains consistent across different levels of perturbation. The P-ACC of AGAIN are comparable to the other baselines because the factor graph does not improve the task predictive accuracy.
We present a comparison of E-ACC and P-ACC between the CBM and AGAIN on the two real-world datasets in Figure~\ref{ex:Validity of the explanation-bar}. The results show that AGAIN achieves higher E-ACC values and comparable P-ACC compared to CBM under $\epsilon=32$ perturbation. In the benign environment, E-ACC and P-ACC of AGAIN also remain comparable. The results suggest that factor graph logical reasoning does not affect prediction accuracy in the absence of perturbation. 
% Furthermore, we illustrate the accuracy comparison under perturbations with $\epsilon=32$. The results indicate the E-ACC of AGAIN significantly outperforms the CBM, implying that factor graph logical reasoning enhances concept predictive accuracy in the presence of perturbations. Furthermore, it is evident that P-ACC exhibits minimal fluctuation before and after the application of perturbation, underscoring the weaker impact of perturbations on category predictions.

\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of E-ACC between AGAIN and baselines on CUB.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 97.6(1.2) & 91.4(1.3) & 90.1(1.2) & 86.9(1.4) & 85.4(2.1) & 90.6(0.6) & 89.5(1.2) & 87.7(2.1) & 80.4(1.4) \\
          Hard AR-AT & 97.5(1.4) & 93.1(0.4) & 90.4(1.6) & 86.3(1.7) & 84.7(2.5) & 91.2(0.2) & 86.7(1.5) & 84.1(2.3) & 79.6(2.1) \\
          ICBM-AT & 96.8(1.1) & 94.0(2.1) & 89.7(2.4) & 85.8(2.3) & 84.8(3.2) & 89.3(2.7) & 85.1(2.1) & 80.5(3.1) & 78.9(1.2) \\
          PCBM-AT & 97.8(1.1) & 92.4(2.7) & 89.4(1.2) & 86.4(1.9) & 83.5(2.4) & 92.4(1.6) & 89.8(3.1) & 85.3(2.6) & 80.4(1.5) \\
          ProbCBM-AT & 96.9(1.0) & 92.5(1.0) & 90.2(1.4) & 84.6(2.3) & 83.9(2.1) & 90.6(3.2) & 88.6(1.6) & 82.5(1.7) & 79.4(1.3) \\
          Label-free CBM-AT & 97.9(0.9) &93.1(1.1) & 92.1(2.3) & 86.7(1.6) & 84.7(3.2) & 90.4(2.1) & 86.7(2.1) & 85.6(1.2) & 80.5(2.8) \\
          ProtoCBM-AT & 98.1(0.7) & 93.1(1.3) & 90.4(1.5) & 85.4(3.1) & 83.9(2.4) & 91.6(1.8) & 86.8(3.2) & 83.1(1.3) & 81.5(3.1) \\
          ECBMs-AT & \underline{\textbf{98.2(0.5)}} & 92.8(2.4) & 89.3(3.4) & 86.6(2.2) & 83.7(2.5) & 91.3(1.3) & 89.4(1.3) & 84.8(2.5) & 80.7(1.9) \\
          \midrule 
          LEN & 97.7(0.2) & 93.1(1.5) & 89.4(1.4) & 86.1(1.4) & 79.9(3.7) & 93.1(1.5) & 89.4(1.4) & 86.1(1.4) & 79.9(3.7) \\
          DKAA & 98.2(1.1) & 92.0(1.4) & 90.6(1.3) & 86.4(3.2) & 80.3(2.6) & 92.0(1.4) & 90.6(1.3) & 86.4(3.2) & 80.3(2.6) \\
          MORDAA & 98.1(1.4) & 93.1(0.7) & 91.2(1.4) & 86.3(1.8) & 79.4(4.6) & 93.1(0.7) & 91.2(1.4) & 86.3(1.8) & 79.4(4.6) \\
          DeepProblog & 96.9(0.8) & 91.0(2.3) & 89.8(1.1) & 85.2(1.3) & 80.2(1.4) & 91.0(2.3) & 89.8(1.1) & 85.2(1.3) & 80.2(1.4) \\
          MBM & 97.9(1.0) & 91.6(1.2) & 90.3(1.4) & 86.3(1.3) & 82.7(3.2) & 91.6(1.2) & 90.3(1.4) & 86.3(1.3) & 82.7(3.2) \\
          C-HMCNN & 98.1(0.2) & 91.3(1.4)  & 91.6(2.1) & 85.5(5.7) & 84.5(2.3) & 94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) \\
          \midrule 
          AGAIN & 97.5(0.1) & \underline{\textbf{94.1(0.2)}} & \underline{\textbf{94.1(0.2)}} & \underline{\textbf{93.8(0.4)}} & \underline{\textbf{93.6(0.7)}} & \underline{\textbf{94.9(0.3)}} & \underline{\textbf{94.9(0.7)}} & \underline{\textbf{93.5(1.1)}} & \underline{\textbf{93.9(0.7)}} \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:EACC-CUB}%
  \vspace{-1em}
\end{table*}%
\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of P-ACC between AGAIN and baselines on CUB.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 62.5(1.2) & 62.4(1.2) & 62.4(1.1) & 62.2(1.4) & 60.5(1.1) & 62.4(1.0) & 62.6(1.2) & 59.4(0.8) & 59.6(1.2) \\
          Hard AR-AT & 62.4(1.3) & 62.2(0.6) & 61.2(0.6) & 61.6(0.4) & 59.6(0.8) & 61.6(1.2) & 60.3(0.3) & 59.4(0.6) & 59.7(1.2) \\
          ICBM-AT & 62.4(1.7) & 62.2(0.6) & 61.3(0.6) & 61.8(1.0) & 59.3(1.1) & 62.4(1.1) & 60.2(0.2) & 59.5(0.4) & 59.8(1.2) \\
          PCBM-AT & 62.5(0.4) & 62.1(0.6) & 61.0(0.6) & 61.6(0.3) & 59.6(0.8) & 62.6(1.2) & 61.0(0.1) & 59.4(0.7) & 59.5(1.1) \\
          ProbCBM-AT & 62.4(0.5) & 62.2(0.4) & 61.4(1.1) & 61.6(0.4) & 59.5(0.8) & 61.6(1.2) & 60.6(0.3) & 59.3(1.1) & 59.6(0.8) \\
          Label-free CBM-AT & 62.7(1.3) & 62.7(0.6) & 61.1(0.4) & 61.3(0.2) & 60.7(1.1) & 61.3(1.2) & 60.6(0.3) & 59.3(0.6) & 59.5(0.8) \\
          ProtoCBM-AT & 62.4(1.3) & 62.1(0.7) & 61.3(0.8) & 61.6(0.3) & 59.2(0.7) & 61.4(1.0) & 60.1(1.2) & 59.4(0.9) & 59.5(1.2) \\
          ECBMs-AT & 62.6(1.2) & 62.4(1.6) & 62.4(1.6) & 61.7(0.4) & 59.6(0.8) & 61.5(1.1) & 60.3(1.2) & 59.3(1.1) & 59.6(1.2) \\
          \midrule 
          LEN & 62.4(0.4) & 62.2(1.3) & 62.3(0.5) & 59.4(1.6) & 59.4(1.4) & 62.2(1.3) & 62.3(0.5) & 59.4(1.6) & 59.4(1.4) \\
          DKAA & 61.4(1.5) & 62.6(1.2) & 62.4(0.5) & 59.4(1.2) & 59.2(1.7) & 62.6(1.2) & 62.4(0.5) & 59.4(1.2) & 59.2(1.7) \\
          MORDAA & 62.2(0.1) & 62.5(0.8) & 61.8(1.3) & 59.5(1.1) & 59.3(0.9) & 62.5(0.8) & 61.8(1.3) & 59.5(1.1) & 59.3(0.9) \\
          DeepProblog & 59.2(0.3) & 59.3(1.4) & 59.7(0.2) & 59.3(1.4) & 59.5(1.7) & 59.3(1.4) & 59.7(0.2) & 59.3(1.4) & 59.5(1.7) \\
          MBM & 62.5(1.2) & 62.5(1.2) & 61.6(1.1) & 59.7(1.4) & 59.4(1.1) & 62.5(1.2) & 61.6(1.1) & 59.7(1.4) & 59.4(1.1) \\
          C-HMCNN & 62.5(1.2) & 62.6(1.2) & 62.4(1.1) & 59.6(1.4) & 59.4(1.1) & 62.6(1.2) & 62.4(1.1) & 59.6(1.4) & 59.4(1.1) \\
          \midrule 
          AGAIN & 62.5(0.4) & 62.2(0.6) & 61.2(0.6) & 61.6(0.4) & 59.6(0.8) & 61.6(1.2) & 60.3(0.3) & 59.4(0.6) & 59.6(0.8) \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:PACC-CUB}%
  \vspace{-1em}
\end{table*}%

\begin{wrapfigure}{r}[0cm]{0pt}
\centering
\includegraphics[width=0.46\linewidth]{img/bar.jpg}
\vspace{-1em}
\caption{The comparison of P-ACC and E-ACC on the two real-world datasets.}
\vspace{-1em}
\label{ex:Validity of the explanation-bar}
\end{wrapfigure}
\paragraph{Rectification of Interactive Intervention Switch.}
In Figure~\ref{ex:Rectification of interactive interventions}~(a) and (b), we illustrate several instances of two real-world datasets along with rectified explanation segments with a dimension of 5 and show the utilized rules.
The interactive intervention switch effectively rectifies the logical error of the explanation based on the predefined rules, thereby enhancing the overall logical coherence of the explanation.
\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{img/shiyan6.jpg}
\vspace{-1em}
\caption{Rectified explanation results on three datasets. The bar represents the normalized activation values of the concepts. Blue bars indicate activated concepts and red bars indicate inactivated concepts. The orange area shows the logical rules followed.}
\vspace{-1em}
\label{ex:Rectification of interactive interventions}
\end{figure*}

\paragraph{Ablation Study.}
We conducted ablation studies to examine the effectiveness for each rule type in $\cal G$. 
The larger number of rules in CUB compared to MIMIC-III EWS contributes to a more significant ablation effect, so we executed ablation studies on the CUB data.
We investigated the performance of all factor graph variants by reporting the LSM results in Table \ref{tab:exAblation study}. 
${{\cal F}^y}$ and ${{\cal F}^c}$ denote the set of factors encoding category-concept rules and the set of factors encoding concept-concept rules, respectively.

According to Table \ref{tab:exAblation study}, we can draw the following conclusions. First, the variant without $\cal G$ (i.e., ${\cal F}=\emptyset$) yielded the lowest LSM values, affirming the essential role of $\cal G$ in the model. Second, $\cal G$ encoding both concept-concept and category-concept rules (i.e., ${\cal F}={{\cal F}^y}\cup{{\cal F}^c}$) achieved the best performance.
This factor graph (constructed in this paper) demonstrates an average improvement of 6.86 over other variants of $\cal G$ that encode only the concept-concept or category-concept rules.
\begin{wraptable}{r}{0.53\textwidth}\scriptsize
  \centering
  \vspace{-1em}
  \caption{Ablation study on the CUB dataset: impact of rule types.}
  \renewcommand\arraystretch{0.8}
    \begin{tabular}{c|cccc}
    \toprule
          Factor Set& $\epsilon=4$&$\epsilon=8$&$\epsilon=16$&$\epsilon=32$\\
          \midrule
    ${\cal F}=\emptyset$& 89.2(1.1) & 77.4(2.1) & 53.7(2.4) & 39.4(3.1) \\
    ${\cal F}={{\cal F}^y}$& 92.5(0.3)     & 89.5(2.1)    & 85.5(2.2)     & 84.0(2.5) \\
    ${\cal F}={{\cal F}^c}$& 91.4 (0.4)    & 86.8(2.4)    & 82.6(2.3)     & 80.1(2.3) \\
    ${\cal F}={{\cal F}^y}\cup{{\cal F}^c}$&\underline{\textbf{94.5(0.3)}} &\underline{\textbf{93.3(2.9)}}&\underline{\textbf{93.8(2.2)}}&\underline{\textbf{92.1(6.1)}}  \\
    \bottomrule
    \end{tabular}%3.04 6.53 11.2 12 8.1925 
  \label{tab:exAblation study}%
  \vspace{-1em}
\end{wraptable}%
This indicates that both rules are essential for comprehensibility of explanations. Finally, the variant containing only ${{\cal F}^c}$ performs lower than the variant containing only ${{\cal F}^y}$. This suggests that the category-concept rules contain more direct logical knowledge about category prediction.

\section{Conclusion and Discussion}\label{section-Conclusion}
In this paper, we explore the comprehensibility of explanations under unknown perturbations and propose AGAIN, an factor graph-based interpretable neural network. 
Inspired by the knowledge integration of factor graphs, AGAIN obtains comprehensible explanations by encoding prior logical rules as the factor graph and utilizing factor graph reasoning to identify and rectify logical error in explanations. 
It addresses the inherent limitations of current adversarial training-based interpretable models by guiding explanation generation during inference. 
Furthermore, we provide a theoretical analysis to demonstrate that factor graphs significantly contribute to obtaining comprehensible explanations. 
We present an initial attempt to generate comprehensible explanations under unknown perturbations from the inference perspective. 
AGAIN provides an effective solution for the defense of interpretable neural networks against various perturbations and meanwhile saves the high cost of retraining.
It takes a significant step towards resolving the crisis of trust between humans and interpretable models.
% In this paper, we propose AGAIN, a factor graph-based interpretable neural network. 
% AGAIN exhibits the capability to generate concept-level explanations with high comprehensibility under unknown perturbation. Initially, we design a factor graph to encode the a priori logical rules between semantic concepts and predicted categories.
% Subsequently, AGAIN leverages the factor graph to perform reasoning on the joint probability distribution of concepts, aiming to identify logical errors in explanations that are incomprehensible to humans.
% Finally, AGAIN employs an interactive intervention switch strategy to intervene in concept activations, with the aim of enhancing potential function difference in the factor graph to rectify logical biases within explanations. 
% Further, we provide a theoretical analysis that demonstrates that the comprehensible interpretations generated by AGAIN benefit from factor graphs.
% Extensive evaluations of explanation quality and validity on three datasets demonstrated AGAIN's capability to enhance explanation comprehensibility.  
%In future research, we plan to further reduce the construction cost of factor graphs by learning logical rules for attribute-relation extraction and automatically integrating domain knowledge into factor graphs. 
%We expect that this solution enables factor graph-based interpretable neural networks to work as end-to-end models.
In addition, we note two limitations of AGAIN:
1) the validity of AGAIN relies on the correct prediction categories. Wrong categories imported into the factor graph cause explanations to be wrongly rectified; 
%Notably, perturbations that disrupt the comprehensibility of explanations tend not to influence predictions; 
2) when domain knowledge changes, the factor graph needs to be reconstructed, which implies AGAIN lacks generalizability. 
We leave these for future work.

\subsubsection*{Acknowledgments}
This paper is supported by the National Key R$\&$D Program of China (Grant No. 2024YFA1012700). Thanks to Shan Jin and Ciyuan Peng for their help during the rebuttal process.
\newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
%\section{Appendix}
\section{Algorithms}\label{appendix:Algorithms}
The overall algorithm for the interactive intervention switch is summarized in Algorithm~1.
\begin{algorithm}[H]
\label{alg:alg11}
  %\SetAlgoLined
        \caption{Interactive intervention switch}
        \Input{The factor graph $\mathcal{G}$, original concept activation vector $\mathbf{a}$}
        \Output{Rectified concept activation vector $\mathbf{a}_{re}$} 
        \For{$f_i$ \bf{in} ${\cal F}$}
        {
            \If{${\psi_i} = 0$}
            {
                {Obtain the set ${\cal T}_i$ of all possible intervention cases for $f_i$; \\}
                \For{$t_i$ \bf{in} ${\cal T}_i$}
                {
                    {Compute $s_i$ through a single intervention $t_i$ according to Eq.~(\ref{potential_function_gain});}
                }
                {Select $t_i$ with the highest $s_i$ as a candidate intervention;}
            }
    }
{Aggregate all candidate interventions into final intervention $t_*$;} 

{Obtain intervention vector $\mathbf{z}$ and mask vector $\mathbf{m}_{t_*}$;}

%{Compute normalized vector $\hat{\mathbf{z}}$ according to Eq.~(\ref{normalized_vector_z});}

{Aggregate $\mathbf{z}$ and $\mathbf{a}$ based on Eq.~(\ref{aggregate_rectified_vector}) yields the resulting $\hat{\mathbf{c}}_{re}$;}

\textbf{return}~$\hat{\mathbf{c}}_{re}$;

\end{algorithm}
\section{Theoretical Analysis of AGAIN}\label{appendix:TheoreticalAnalysis}
%在本节中，我们将提出理论证明，以保证 AGAIN中的因子图的确有助于在未知扰动条件下生成可理解的解释。
In this section, we present theoretical proofs to ensure that the $\cal G$ in AGAIN contributes to generating comprehensible explanations under unknown perturbations.
%然而，在统计理论中，解释的可理解性很难被直接量化。
%但好在，以往关于可解释模型有效性的理论分析强调，可理解的解释往往与解释性标签具有高相似度。
Previous theoretical analyses on the validity of interpretable models have emphasized that comprehensible explanations tend to have a high similarity to explanatory labels~\citep{li2020survey,karpatne2017theory,von2021informed}.
%因此，本节希望通过证明因子图在未知扰动下有助于解释与解释标签近似，来间接地保证AGAIN可以生成可理解的解释。
Therefore, we hope to guarantee that AGAIN can generate comprehensible explanations by proving that $\cal G$ contributes to explanations under unknown perturbations in approximation to the explanatory labels.
Specifically, our theoretical proof is divided into two parts. In the first part, we establish correlations between the conditional probabilities of the factors and the lower bounds on the predictive accuracy of concepts. In the second part, we show that this lower bound increases according to the larger size of $\cal G$.

\subsection{The Concept Predictive Accuracy Lower Bound for AGAIN}\label{proofB.1}
%我们可以观察到，AGAIN的预测精度取决于生成解释和解释标签的相似度，我们的目标是通过因子图的势函数来表征AGAIN的预测精度的下界。
%在扰动环境下，给定一个归因扰动A,解释生成器q和解释标签E, 解释的准确度可以被表示为W:
Under the adversarial distribution, given an perturbation $\delta$ and the concept explanation label $\mathbf{c}$, the accuracy of the explanation is denoted as ${\cal A}^h$:
\begin{equation}
{\cal A}^h : = \prod_{c \in \mathbf{c}}\mathbb{P}_\delta\left( \hat{c} = c \right).
\end{equation}
%为了简化书写，我们用表示BAGAIN中纠正和识别的过程。
To simplify the writing, we use $h_{\cal G}\left( \hat{\mathbf{c}}, \hat{y} \right)$ to denote the process of identifying (Section~\ref{method3.2}) and rectifying (Section~\ref{method3.3}) in AGAIN.
%我们推广这个定义，得到AGAIN生成的解释的准确度${\cal A}^{\text{AGAIN}}$:
We extend this definition to assess the accuracy of explanations generated by AGAIN:
\begin{equation}
{\cal A}^{\text{AGAIN}} : = \prod_{c \in \mathbf{c}} \mathbb{P}_\delta\left( h_{\cal G}^{\left( m \right)}\left( \hat{\mathbf{c}}, \hat{y} \right) = c \right)
\label{eq:proof9}
,
\end{equation}
%这里$h_{\cal G}^{\left( \cdot  \right)}\left(\right)$表示关于第m个概念的因子图推理输出。
where $h_{\cal G}^{\left( m \right)}\left(\cdot\right)$ denotes the output of the factor graph reasoning with respect to the $m$-th concept.
%我们给出了关于证明和分析。我们证明了如果AGAIN的解释在归因扰动下的准确度的下界大于原始的概念瓶颈模型的准确度，则AGAIN没有为了提升解释的可读性而损失精度。
%我们改写公式(9)中的P为包含因子图势函数的形式:
% 为了求解下界，我们提取关于势函数的关键变化量：
%We express (9) in the form that incorporates factor graph potential functions:
%根据因子图的性质，P也等价为如下:
%根据（1）（2）和（3），我们进一步展开10中的概率不等式为包含因子图势函数的形式：
%\eqref!!
\paragraph{Lemma 1.} Given a factor graph $\cal G$, the following equation is valid:
\begin{equation}
\begin{aligned}
{A^{{\rm{AGAIN}}}} = \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {{\Delta _N}\left( {{v_{\hat c}}} \right) > 0\left| c \right.} \right),
\end{aligned}
\label{eq:proof12}
\end{equation}
where
\begin{equation}
{\Delta _{\cal N}}\left( {{v_{\hat c}}} \right) = \sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {\left( {2{w_i}{\psi _i} - {w_i}} \right)} .
\label{eq:proofDelta}
\end{equation}
\paragraph{Proof.} 
First, ${\cal A}^{\rm{AGAIN}} : = \prod_{c \in \mathbf{c}} \mathbb{P}_\delta\left( h_{\cal G}^{\left( m \right)}\left( \hat{\mathbf{c}}, \hat{y} \right) = c \right)$ is known. Then, based on the properties of $\cal G$, ${\mathbb{P}_\delta }\left( {h_{\cal G}^{\left( m \right)}\left( {\hat{\mathbf{c}}, \hat{y}} \right) = {c}} \right)$ can be expressed equivalently as $\mathbb{P}_\delta\left( \mathbb{P}\left(\mathcal{F}_{v_{\hat{c}}} \middle| v_{\hat{c}} = c \right) > \mathbb{P}\left( \mathcal{F}_{v_{\hat{c}}} \middle| v_{\hat{c}} \neq c \right) {\left| c \right.} \right)$.
According to~Eq.~(\ref{eq:jointprobability}), we extend $\mathbb{P}\left( {{\cal N}\left( {{v_{\hat c}}} \right){\rm{ }}|{v_{\hat c}} = c} \right) > \left( {{\cal N}\left( {{v_{\hat c}}} \right){\rm{ }}|{v_{\hat c}} \ne c} \right)$ to a formulation that incorporates potential functions within $\cal G$:
%where ${v_i} \in {\cal V}\left( {n_m^c} \right)$.
\[
\begin{aligned}
&\mathbb{P}\left( {{\cal N}\left( {{v_{\hat c}}} \right){\rm{ }}|{v_{\hat c}} = c} \right) > \left( {{\cal N}\left( {{v_{\hat c}}} \right){\rm{ }}|{v_{\hat c}} \ne c} \right)\\
& \Rightarrow \sum\limits_{i \in \left| {{\cal N}({v_{\hat c}})} \right|} {{w_i}{\psi _i}}  - (\sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {{w_i}(1 - {\psi _i})} ) > 0\\
&\Rightarrow \sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {{w_i}{\psi _i}}  - \sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {{w_i}}  + \sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {{w_i}{\psi _i}}  > 0\\
&\Rightarrow \sum\limits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {2{w_i}{\psi _i} - {w_i}}  > 0,
\end{aligned}
\label{eq:proof11}
\]
then, we obtain:
% \[
% \begin{aligned}
% {\cal A}^{\text{AGAIN}}&=\prod_{c \in \mathbf{c}} \mathbb{P}_\delta\left( h_{\cal G}^{\left( m \right)}\left( \hat{\mathbf{c}}, \hat{y} \right) = c \right)=\prod_{c \in \mathbf{c}}\mathbb{P}_\delta\left( \mathbb{P}\left(\mathcal{F}_{v_{\hat{c}}} \middle| v_{\hat{c}} = c \right) > \mathbb{P}\left( \mathcal{F}_{v_{\hat{c}}} \middle| v_{\hat{c}} \neq c \right) {\left| c \right.} \right)\\
% &=\prod_{c \in \mathbf{c}}\mathbb{P}_\delta\left(\sum\nolimits_{{i \in \left| {\cal N}\left({{v_\hat {c}}}\right) \right|}} {2w_i \psi_i - w_i} > 0 {\left| c \right.} \right).
% = \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {\sum\nolimits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {2{w_i}{\psi _i} - {w_i}}  > 0\left| c \right.} \right)
% \end{aligned}
% \]
% \qed
\[\begin{aligned}
{\cal A}^{{\rm{AGAIN}}} &= \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {h_G^{\left( m \right)}\left( {\widehat {\bf{c}},\hat y} \right) = c} \right)
 = \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {\left( {{{\cal F}_{{v_{\hat c}}}}\left| {{v_{\hat c}}} \right. = c} \right) > \left( {{{\cal F}_{{v_{\hat c}}}}\left| {{v_{\hat c}}} \right. \ne c} \right)\left| c \right.} \right)\\
 &= \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {\sum\nolimits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {2{w_i}{\psi _i} - {w_i}}  > 0\left| c \right.} \right)
 = \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {\sum\nolimits_{i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|} {2{w_i}{\psi _i} - {w_i}}  > 0\left| c \right.} \right)
\end{aligned}
\]
\qed

%我们观察到B决定了公式11的下界。
We observe that ${\Delta _{\cal N}}\left( {{v_{\hat c}}} \right)$ determines the lower bound of Eq.~(\ref{eq:proof12}).
%我们表征因子图G对概念的约束来约束P，使其左尾概率小于0。为此，对于g中一个概念变量节点c，我们利用其邻居因子节点来表征G在推理中对c的四种约束情况：
Further, we characterize the constraints on the concepts within $\cal G$ to impose bounds on the ${\Delta _{\cal N}}\left( {{v_{\hat c}}} \right)$, thereby restricting its left-tailed probability below 0. To this end, considering a concept variable ${v_{\hat c}}$, we characterize the four cases of constraints imposed by ${\cal G}$ on ${v_{\hat c}}$ during reasoning with its neighbor factors ${f_i} \in {\cal N}\left( {{v_{\hat c}}} \right)$:
\begin{equation}
L \le {\mathbb{P}_\delta }\left( {{\psi _i}\left| c \right.} \right) \le U
\label{eq:proof15}
,
\end{equation}
\begin{equation}
\begin{aligned}
&L^T_P \le T^{P} = \mathbb{P}_\delta\left( \psi_i = 1 \middle| v_{\hat{c}} = c \right) \le U^T_P \\
&L^T_N \le T^N = \mathbb{P}_\delta\left( \psi_i = 0 \middle| v_{\hat{c}} = 1 - c \right) \le U^T_N\\
&L^F_N \le F^N = \mathbb{P}_\delta\left( \psi_i = 0 \middle| v_{\hat{c}} = c \right) \le U^F_N \\
&L^F_P \le F^P = \mathbb{P}_\delta\left( \psi_i = 1 \middle| v_{\hat{c}} = 1 - c \right) \le U^F_P.
\end{aligned}
\label{eq:proof14}
\end{equation}


Moreover, we employ factor graph characterizations to represent the lower bound of ${\Delta _{\cal N}}\left( {{v_{\hat c}}} \right)$. To this end, a lemma is introduced below~\citep{gurel2021knowledge}. This lemma illustrates an inequality property between ${\Delta _{\cal N}}\left( {{v_{\hat c}}} \right)$ and factor graph characterizations. 
\paragraph{Lemma 2.}
Suppose each factor has the optimal weight. Then there exists
\begin{equation}
\mathbb{E}\left( {{\Delta _{\cal N}}\left( {{v_{\hat c}}} \right)\left| c \right.} \right) \ge {Z_1} + {Z_2} - \log \frac{{{c^{\left( {1 - 2c} \right)}}}}{{1 - c}}
\label{eq:proof16}
,
\end{equation}
where
\begin{equation}
\begin{aligned}
Z_1 &= c\left( T^P\log \frac{L^T_P}{1 - U^F_P} + \left( 1 - T^P \right)\log \frac{1 - U^T_P}{1 - L^F_P} -  F^N\log \frac{U^T_N}{L^F_N} + \left( 1 - F^N \right)\log \frac{1 - L^T_N}{1 - U^F_N} \right),\\
Z_2 &= \left( 1 - c \right)\left( T^N\log \frac{L^T_N}{U^F_N} + \left( 1 - T^N \right)\log \frac{1 - U^T_N}{1 - L^F_N} - F^P\log \frac{U^T_P}{L^F_P} + \left( 1 - F^P \right)\log \frac{1 - L^T_P}{1 - U^F_P} \right).
\end{aligned}
\label{eq:proof17}
\end{equation}

To facilitate the writing, we have opted for a simplified symbolic representation:
\begin{equation}
{L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}} = {Z_1} + {Z_2} - \log \frac{{{c^{\left( {1 - 2c} \right)}}}}{{1 - c}}.
\end{equation}

To further solve the bound of ${\cal A}^{\text{AGAIN}}$, we introduce another lemma~\citep{chen2024generalized}. This lemma outlines the definition and properties of Hoeffding's inequality.

\paragraph{Lemma 3.} Suppose the given $ {{x_1},...,{x_t}} $ are independent random variables, and ${x_t}-{x_{t-1}} \in \left[ {a_t,b_t} \right]$. The empirical mean of these variables denoted as $\bar x = ({{{x_1} + ... + {x_t}}) \mathord{\left/{\vphantom {{{x_1} + ... + {x_t}} t}} \right.\kern-\nulldelimiterspace} t}$. Then for any $\beta > 0$ there exists the Hoeffding's inequality as shown below:
\begin{equation}
\quad \mathbb{P}\left( \left| \bar{x} - \mathbb{E}\left( \bar{x} \right) \right| \ge \beta \right) \le 2\exp\left( -\frac{2\beta^2}{\sum_{i \in t} \left( {\tau _i} \right)^2} \right),
\end{equation}
where $ (b_i - a_i)\le {\tau _i}$ and $\mathbb{E}( \cdot )$ denotes the expectation.

Herein, we provide the lower bound with respect to ${\cal A}^{\text{AGAIN}}$. This lower bound imposes a minimum accuracy constraint on AGAIN.

\paragraph{Theorem 1.}\label{ProofTheorem1} For the AGAIN with $\cal G$, under the assumption that factors satisfying $Z_1>0$ and $Z_2>0$ are considered, the following inequality is established:
\begin{equation}
{\cal A}^{\text{AGAIN}} \ge \prod_{c \in  \mathbf{c}} \left( 1 - \mathbb{E}\left( \exp\left(\partial\right) \right) \right)
,
\end{equation}
where
\begin{equation}
\partial  =  - 2\frac{{{{\left( {{L^{{\Delta _{N\left( {{v_{\hat c}}} \right),c}}}}} \right)}^2}}}{{\sum\nolimits_{i \in {\cal N}} {{{\left( {\log \frac{{{U^T}(1 - {L^F})}}{{{L^T}(1 - {U^F})}}} \right)}^2}} }}.
\end{equation}
%证明：根据因子图的对称性，我们可以获得下述等式关系

\paragraph{Proof.} 
According to the symmetry of $\cal G$, we can derive the following equation:
\[
\begin{aligned}
{{\cal A}^{{\rm{AGAIN}}}}{\rm{ }} = \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {h_{\cal G}^{\left( c \right)}\left( {\widehat {\bf{c}},\hat y} \right) = c} \right) = \prod\limits_{c \in {\bf{c}}} {\left( {1 - {\mathbb{P}_\delta }\left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} < 0\left| c \right.} \right)} \right)}.
\end{aligned}
\]

According to Eq.~(\ref{eq:proof16}), we can get
\[
\begin{aligned}
{\mathbb{P}_\delta }\left( {{\Delta _{\cal N}}\left( {{v_{\hat c}}} \right) < 0\left| c \right.} \right) &= {\mathbb{P}_\delta }\left( {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} - {\mathbb{E}}{\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} \right]} + {\mathbb{E}}{\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} \right]} < 0\left| c \right.} \right)\\
 &= {\mathbb{P}_\delta }\left( {\left( {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} - {\mathbb{E}}{\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} \right]}} \right) <  - {\mathbb{E}}{\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} \right]}\left| c \right.} \right)\\
 &= {\mathbb{P}_\delta }\left( {\left( {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} - {\mathbb{E}}{\left[ {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}}\left| c \right.} \right]}} \right) <  - {\mathbb{E}}{\left[ {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}}\left| c \right.} \right]}\left| c \right.} \right)\\
 &\le {\mathbb{P}_\delta }\left( {\left( {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}} - {\mathbb{E}}{\left[ {{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}}\left| c \right.} \right]}} \right) <  - {L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}\left| c \right.} \right).
\end{aligned}
\label{eq:proof26}
\]
Substituting ${\mathbb{P}_\delta }\left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} < 0\left| c \right.} \right) \le {\mathbb{P}_\delta }\left( {\left( {{\Delta _{N\left( {{v_{\hat c}}} \right)}} - \mathbb{E}\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}\left| c \right.} \right]} \right) <  - {L^{{\Delta _{\cal N}}\left( {{v_{\hat c}}} \right),c}}\left| c \right.} \right)$ into Hoeffding's inequality and letting ${\beta  = {L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}}$ in Lemma 3, we evidently derive the following inequality:
\[
\begin{aligned}
{{\mathbb{P}}_\delta }\left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} < 0\left| c \right.} \right) \le {{\mathbb{P}}_\delta }\left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} - {\mathbb{E}}{\left[ {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}\left| c \right.} \right]} <  - {L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}\left| c \right.} \right) \le \exp \left( { - \frac{{2{{\left( {{L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}} \right)}^2}}}{{\sum\nolimits_{i \in {\cal N}} {{{\left( {{\tau _i}} \right)}^2}} }}} \right).
\end{aligned}
\label{eq:proof24}
\]

According to Lemma 3, we can use Eq.~(\ref{eq:proofDelta}) and Eq.~(\ref{eq:proof16}) to establish Hoeffding's inequality with respect to ${{\cal A}^{{\rm{AGAIN}}}}$. If we consider ${\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}}$ as the independent random variable $x_t$ in Lemma 3, we assume
\[
\begin{aligned}
x_{i + 1}^{(\hat c)} - x_i^{(\hat c)} = {w_i}\exp \left( {{\psi _i}} \right)\quad {\rm{s}}{\rm{.t}}{\rm{.}}\quad i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|,
\end{aligned}
\]
%进一步地，我们获得
%根据（17），我们可以获得
%根据（14）（15）和（16），我们可以分两种情况推导出W的上下界。如果A，则存在
%其次，如果B，则存在
according to Eq.~(\ref{eq:proof15}), Eq.~(\ref{eq:proof14}), and Eq.~(\ref{eq:proof16}), we can infer $a_i$ and $b_i$ for $\psi_i$ in two cases. If $\psi_i = 1$, then there exists
\[
\log \frac{L^T}{U^F} \le w_i\exp \left(\psi_i\right) = w_i e \le \log \frac{U^T}{L^F}
,
\]
if $\psi_i = 0$, then there exists
\[
\log \frac{1 - U^T}{1 - L^F} \le w_i\exp \left(\psi_i\right) = w_i \le \log \frac{1 - L^T}{1 - U^F}
.
\]

Since $w_i>0$, it is evident that $w_i e>w_i$. Therefore, for both cases above, there is a uniform range interval that can be inferred
\[
\log \frac{{1 - {U^T}}}{{1 - {L^F}}} \le w_i\exp \left(\psi_i\right) \le \log \frac{{{U^T}}}{{{L^F}}}
,
\]
further, it can be inferred that ${w_i}\exp \left( {{\psi _i}} \right) \le {\tau _i}$. We know $(b_i - a_i)\le {\tau _i}$ and $a_i \le w_i\exp \left(\psi_i\right) \le b_i$so ${x_t}-{x_{t-1}} \in \left[ {a_t,b_t} \right]$ holds.
\[
\begin{aligned}
\log \frac{{{U^T}}}{{{L^F}}} - \log \frac{{1 - {U^T}}}{{1 - {L^F}}} = \log \frac{{{U^T}(1 - {L^F})}}{{{L^F}(1 - {U^T})}} \le {\tau _i}\quad {\rm{s}}{\rm{.t}}{\rm{.}}\quad i \in \left| {{\cal N}\left( {{v_{\hat c}}} \right)} \right|
,
\end{aligned}
\]

according to \[
\begin{aligned}
{\mathbb{P}_\delta }\left( {{\Delta _N}\left( {{v_{\hat c}}} \right) < 0\left| c \right.} \right) \le \exp \left( { - \frac{{2{{\left( {{L^{{\Delta _N}\left( {{v_{\hat c}}} \right),c}}} \right)}^2}}}{{\sum\nolimits_{i \in N} {{{\left( {{\tau _i}} \right)}^2}} }}} \right),
\end{aligned}
\label{eq:proof24}
\]

we can be inferred that
\[
\begin{aligned}
\prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {h_{\cal G}^{\left( c \right)}\left( {\widehat {\bf{c}},\hat y} \right) = c} \right) &= \prod\limits_{c \in {\bf{c}}} {{\mathbb{P}_\delta }} \left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} > 0\left| c \right.} \right) = \prod\limits_{c \in {\bf{c}}} {\left( {1 - {\mathbb{P}_\delta }\left( {{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right)}} < 0\left| c \right.} \right)} \right)} \\
 &\ge \prod\limits_{c \in {\bf{c}}} {\left( {1 - \mathbb{E}\left[ {\exp \left( { - \frac{{2{{\left( {{L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}} \right)}^2}}}{{\sum\limits_{i \in N} {{{\left( {{\tau _i}} \right)}^2}} }}} \right)} \right]} \right)} \\
 &= \prod\limits_{c \in {\bf{c}}} {\left( {1 -  \mathbb{E}\left[ {\exp \left( { - \frac{{2{{\left( {{L^{{\Delta _{{\cal N}\left( {{v_{\hat c}}} \right),c}}}}} \right)}^2}}}{{\sum\limits_{i \in {\cal N}} {{{\left( {\log \frac{{{U^T}(1 - {L^F})}}{{{L^F}(1 - {U^T})}}} \right)}^2}} }}} \right)} \right]} \right)} 
\end{aligned}
\]
\qed
\subsection{Lower Bound of Accuracy Versus Number of Factors in the Factor Graph}\label{proofB.2}

After analyzing the lower bound of concept accuracy, we aim to demonstrate a positive correlation between this lower bound and the number of factors in the factor graph. In simpler terms, an increase in the number of factors is directly proportional to an enhancement in predictive accuracy. This implies that $\cal G$ contribute to the predictive performance of the model. To facilitate the analysis, we introduce a lemma below~\citep{gurel2021knowledge}. This lemma illustrates an unequal relationship of the accuracy lower bound under specific factor graph characterizations.

% 假设所有因子图特征具有相同的上下界，则存在
\paragraph{Lemma 4.} Suppose that the upper and lower bounds of each factor graph characteristic are identical, then there exists
\begin{equation}
\prod\limits_{c \in {\bf{c}}} {\left( {1 - \mathbb{E}\left[ {\exp \left( { - \frac{{2{{\left( {{L^{{\Delta _{\cal N}}\left( {{v_{\hat c}}} \right),c}}} \right)}^2}}}{{\sum\limits_{i \in N} {{{\left( {\log \frac{{{U^T}(1 - {L^F})}}{{{L^F}(1 - {U^T})}}} \right)}^2}} }}} \right)} \right]} \right)}  \ge \prod\limits_{c \in {\bf{c}}} {\left( {1 - \exp \left( { - 2N\left( {{\Theta ^T} - {\Theta ^F}} \right)} \right)} \right)} ,
\label{eq:proof31}
\end{equation}

where
\begin{equation}
\Theta^T: = {U^T} = {L^T} \quad \Theta^F: = {U^F} = {L^F}.
\end{equation}
% 进一步地，我们给出一个定理，它说明了概念精度的下界和因子图中的因子节点数量之间的一个重要关系。
%对于给定的AGAIN中的因子图G，如果它满足引理3的假设并且G中的每个因子节点满足T>F, 则由G纠正的解释的精度会随着因子节点个数的增加而以指数速度递增为1。

Furthermore, we present a theorem that highlights a significant correlation between the lower bound of concept accuracy and the number of factors in $\cal G$.

\paragraph{Theorem 2.} For the given $\cal G$ in AGAIN, if it satisfies the assumptions of Lemma 4 and each factor in $\cal G$ satisfies $\Theta^T > \Theta^F$, the lower bound on the concept predictive accuracy increases strictly monotonically as the number of factors $N$ of the factor graph increases. Moreover, the lower bound of concept predictive accuracy with factor graph is strictly larger than the lower bound without factor graph.

%根据（31）,如果T>F,则我们可以获得
\paragraph{Proof.} 

According to Eq.~(\ref{eq:proof31}), if $\Theta^T > \Theta^F$, we can achieve
\[
{\cal A}^{\text{AGAIN}} =\prod_{c \in  \mathbf{c}} \mathbb{P}_\delta \left( h_{\cal G}^{(c)}\left(\hat{\mathbf{c}}, \hat{y}\right) = c \right)\ge \prod_{c \in  \mathbf{c}} \left( 1 - \exp \left( - 2N{{\left( {\Theta^T - \Theta^F} \right)}} \right) \right).
\]
Let $\Theta^T - \Theta^F=\alpha>0$ and $f(N)=1 - \exp \left( - 2N{{\left( {\Theta^T - \Theta^F} \right)}} \right)$. Then there exists the first order derivative $f'(N)=2\alpha e^{-2\alpha N}>0$ of $f(N)$.
Therefore, $f(N)$ is monotonically increasing, i.e., ${\cal A}^{\text{AGAIN}}$ is a strictly monotonically increasing function with respect to $N$.

Notably, for the case without the factor graph (i.e., $N = 0$), we have $f(N)=1 - \exp \left( - 2N{{\left( {\Theta^T - \Theta^F} \right)}} \right)=0$. And for a factor graph $\cal G$ with an arbitrary number of nodes, $f(N) > f(0) = 0$. Therefore, the concept predictive accuracy with the introduction of the factor graph is constantly greater than the case without the factor graph.
% \[
% \begin{aligned}
%     \lim_{{N \to \infty }}{{\cal A}^{\text{AGAIN}}} &= \lim_{{N \to \infty }} \prod_{c \in  \mathbf{c}} \mathbb{P}_\delta \left( h_{\cal G}^{(c)}\left(\hat{\mathbf{c}}, \hat{y}\right) = c \right)  \\
%     &\ge \lim_{{N \to \infty }} \prod_{c \in  \mathbf{c}} \left(1 - \exp\left(-2N{\left(T - F\right)}\right)\right) = \lim_{{N \to \infty }} \prod_{c \in  \mathbf{c}} (1 - 0) = 1
%     ,
% \end{aligned}
% \]

% %显然，当I趋近于正无穷时，A的下界趋近于1，且它在I上是单调递增的。进一步地，我们证明A的下界可以指数收敛。考虑A的泰勒级数展开式
% it is evident that the lower bound of ${\cal A}^{\text{AGAIN}}$ converges to 1 as $N \to \infty$, and it is monotonic with respect to $N$. Moreover, we prove that the lower bound of ${\cal A}^{\text{AGAIN}}$ exhibits exponential convergence. We consider the Taylor series expansion of $\log \left( {1 - x} \right)$ around $x=0$:
% \begin{equation}
% \log \left( {1 - x} \right) \approx - x - \frac{{{x^2}}}{2} - \frac{{{x^3}}}{3} - \ldots - \frac{{{x^n}}}{n}.
% \label{eq:proof34}
% \end{equation}
% %根据33，令A=B.则B的泰勒级数展开可以背:

% According to Eq.~(\ref{eq:proof34}), let $f\left( N \right) = 1 - \exp\left(-2N{\left(T - F\right)}^2\right)$. We apply the Taylor series expansion to $\log \left( {f\left( N \right)} \right)$:
% \begin{equation}
% \begin{aligned}
%     \log\left(f(N)\right) &\approx -\exp\left(-2N(T - F)^2\right)- \ldots - \frac{\exp\left(-2nN(T - F)^2\right)}{n}.
% \end{aligned}
% \label{eq:proof38}
% \end{equation}
% %在此，我们讨论（35）右侧的无穷级数。由于其每一项都是负的，我们可以推断出

% Herein, we discuss the infinite series on the right-hand side of Eq.~(\ref{eq:proof38}). Given that each of its terms is negative, we get:
% \begin{equation}
% \left| {\log \left( {f\left( N \right)} \right)} \right| \le \sum\nolimits_{n = 1}^\infty  {\frac{{\exp \left( { - 2nN{{(T - F)}^2}} \right)}}{n}}
% ,
% \end{equation}
% %显然，A是一个发散的调和级数，而无穷级数的每一项都小于等于对应的调和级数的相应项。因此，假设我们给定一个S和A，则我们有：
% it is evident that $\sum\nolimits_{n = 1}^\infty  {\frac{1}{n}}$ is a divergent harmonic series, and each term of the infinite series is less than the term of the corresponding harmonic series. Hence, suppose that there exists given $d=1$ and $r<1$, we get:
% %因此，A是指数收敛的。由于A和B在收敛速度上是等价的，所以B是指数收敛的
% \begin{equation}
% \left| {\log \left( {f\left( N \right)} \right)} \right| \le d \cdot {r^N}
% ,
% \end{equation}
% therefore, ${\log \left( {f\left( N \right)} \right)}$ is exponentially convergent. Given that ${\log \left( {f\left( N \right)} \right)}$ and ${f\left( N \right)}$ exhibit equivalent rates of convergence, it can be asserted that ${f\left( N \right)}$ demonstrates exponential convergence. 
\qed
\subsection{Discussion}
%我们的理论证明证实了，只要因子图编码的规则对概念激活的约束是符合先验逻辑的，即满足定理2中限定充分条件，T>F,则因子图可以提升预测精度。直观地说，I越大，则B的下界就会指数升高，因此B就会显著提升。 我们看到这个结论是建立在引理2设定的条件，即U=L和U=L上的。实际上，当因子图编码的规则符合先验逻辑，则U=L和U=L是自然被满足的。更具体的说，当因子节点满足T>F且变量节点取值与标签相同时，则势函数一定为e，即势函数的条件概率是定值。因此，我们得到
Our theoretical proof validates that $\cal G$ has the potential to enhance prediction accuracy, provided that the rules embedded in the factor graphs restrict concept activation in alignment with prior logic, satisfying the qualifying sufficient condition outlined in Theorem 2 (i.e., $T > F$). Intuitively, as the size of $N$ increases, the lower bound on ${\cal A}^{\text{AGAIN}}$ grows exponentially, leading to a substantial improvement in ${\cal A}^{\text{AGAIN}}$. We observe that Theorem~2 holds depending on the condition set by Lemma 4~(i.e., ${U^T}={L^T}$ and ${U^F}={L^F}$). Indeed, ${U^T}={L^T}$ and ${U^F}={L^F}$ are inherently met when the rules governing $\cal G$ adhere to prior logic. More specifically, if a factor satisfies $\Theta^T > \Theta^F$ and its neighboring variables adopt the same value as ground-truth labels, then its potential function value must be $e$. In this case, the conditional probability of the potential function is fixed. 



%\section{Notations}
%In this paper, lower case letters (e.g.,~$x_i$), bold lower case letters (e.g.,~$\mathbf{x}$), and bold calligraphic letters (e.g.,~$\cal X$) denote variables, vectors, and sets, respectively. The primary notations used in this paper are listed in the Table~\ref{notation}.

\section{Experimental Details}\label{ap:Experimental Details}
\subsection{Datasets and Data Preprocessing}\label{ap:Datasets and Data Preprocessing}
All data processing and experiments are executed on a server with two Xeon-E5 processors, two RTX4000 GPUs and 64G memory. We construct logical rule sets on the three datasets respectively. The detailed information of data processing on both datasets is summarized as follows:
%Synthetic-MNIST是基于原有MNIST的拼接数据集。每个Synthetic-MNIST实例中包含四个手写数字，
\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{img/corr.jpg}
\vspace{-1em}
\caption{Correlation of association scores between concepts on the CUB dataset. The horizontal and vertical axes indicate the correlation scores of the two concepts. The top row shows the association scores for the 6 groups of concept pairs with PCCs $>$ 0.8 (red scatter). The bottom row shows the association scores for the 6 sets of concept pairs with PCCs $<$ -0.5 (blue scatter).}
\label{fig:ConceptCorr}
\vspace{-1em}
\end{figure*}

\paragraph{Synthetic-MNIST.} The Synthetic-MNIST dataset is a composite dataset derived from the original MNIST dataset. Each category of the MNIST handwritten digits is treated as a concept, and four digits from different categories are concatenated to form a Synthetic-MNIST sample. Consequently, each Synthetic-MNIST sample contains 4 concept labels and a synthetic category label. The Synthetic-MNIST comprised 79,261 samples from 12 synthetic categories. The mapping of each synthetic category to concepts is shown in Table~\ref{fig:mnistconcept-class}. According to Table~\ref{fig:mnistconcept-class}, we construct the first-order logical rule set for Synthetic-MNIST. The samples, whose category label is 0, are taken as examples. We construct category-concept rules: ${c_0} \Leftrightarrow {y_0}$, ${c_2} \Leftrightarrow {y_0}$, ${c_4} \Leftrightarrow {y_0}$, ${c_6} \Leftrightarrow {y_0}$; concept-concept rules: ${c_0} \oplus {c_1}$, ${c_2} \oplus {c_3}$, ${c_4} \oplus {c_5}$, ${c_4} \oplus {c_6}$, ${c_7} \oplus {c_8}$, ${c_9} \oplus {c_5}$. Note that the logical rules used on the synthetic dataset can encompass the entire knowledge required for the downstream tasks. Therefore, we randomly omit a subset of these rules to simulate the incompleteness of explicit knowledge~\footnote{\url{http://yann.lecun.com/exdb/mnist/}}.

\paragraph{CUB.} The CUB~(Caltech-UCSD Birds-200-2011) dataset comprises 11,788 images of birds distributed across 200 categories. Each image is annotated with 312 high-level semantic labels, such as wing color and beak shape, in addition to a single category label. We have retained 112 crucial semantic labels as concepts following a denoising process. Furthermore, potential associations between concepts and categories are manually labeled by the bird experts and quantified as an association score in the interval $[0,100]$. Association scores approaching 100 or 0 signify a significant degree of coexistence or exclusion, respectively, between the concept and the category. Convergence towards 50 indicates the absence of any discernible association. We formulate the logical rule set based on the association scores. Association scores near 100 are regarded as category-concept rules representing coexistence constraints, while association scores approaching 0 are identified as category-concept rules indicating exclusion constraints. To construct concept-concept rules, we create a score vector that includes association scores corresponding to a concept under each category. We calculate the Pearson correlation coefficient~(PCC) between each score vector, an elevated PCC between score vectors suggests a greater similarity between the two concepts. 
%我们分别可视化了5组互斥概念和5组共存概念在200个类别中的关联得分，如图所示。
As illustrated in Figure~\ref{fig:ConceptCorr}, we visualized the association scores of 6 sets of exclusive concepts and 6 sets of coexisting concepts across 200 categories, respectively. 
%我们观察到皮尔森系数>0.8的概念组,它们的关联得分呈正线性相关。皮尔森系数<0.5的概念组,它们的关联得分呈负线性相关。
We observe that for concept groups with PCCs $>$0.8, their association scores are positively linearly correlated. While concept groups with the PCC $<$-0.5 showed negative linear correlation.
%因此，我们对皮尔森相关性系数>0.8的概念组构建共存规则，为皮尔森相关性系数<-0.5的概念组构建互斥规则。
Therefore, we construct coexistence rules for groups of concepts with PCCs $>$ 0.8 and exclusion rules for groups of concepts with PCCs $<$ -0.5~\footnote{\url{http://www.vision.caltech.edu/visipedia/CUB-200.html}}.

\paragraph{MIMIC-III EWS.} The MIMIC-III EWS dataset is a medical dataset for an early warning score~(EWS) prediction task, comprising electronic health records from 17,289 patients. The EWS is the patient's vital sign score, ranging from 0 to 15. Deviation from normal vital signs results in an increase in EWS. We utilize 15 input attributes of patients to predict 16 labeled categories (each integer value of EWS as a category). Additionally, we predefine 22 concepts related to vital signs (such as body temperature, mean blood pressure, etc.) based on the Hard AR~\citep{havasi2022addressing} recommendations for generating explanations. We directly establish category-concept rules based on the existing probability analysis results from Hard AR and formulate concept-concept rules using cosine similarity~\footnote{\url{https://physionet.org/content/mimiciii/1.4/}}.

%A是一个用于BSW预测任务的医学数据集，包含来自17289名患者的电子健康记录。EWS是对患者生命体征评分，其范围从0到15。患者偏离正常的生命体征会导致EWS的上升。我们利用患者的15个输入属性去预测16个标签类别（EWS的每个整数值是一个类别）。另外，我们根据Hard AR的推荐预定义22个与生命体征（血压、壁炉率等）相关的概念，用于生成解释。我们使用已有的概率分析结果直接构建YC规则，并用余弦相似度构建cc规则。
%在上述数据集上构建的规则和因子图信息的如表二所示，它描述了每种逻辑规则的数量和因子图每种节点的数量。
The statistics of the rules and factor graphs constructed on the aforementioned dataset are presented in Table \ref{tab:Statistics}, illustrating the number of each type of logical rules and each type of nodes in $\cal G$.

\begin{table}\scriptsize
\centering
\vspace{-1em}
  \caption{Statistics of rule and factor graphs constructed on three datasets.}
    \begin{tabular}{c|c|ccc}
    \toprule
    \multicolumn{2}{c|}{Dataset} & \makecell{Synthetic\\-MNIST} & CUB   & \makecell{MIMIC\\-III EWS} \\
    \midrule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{\makecell{Logical \\ Rule}}} & Category-concept & 33     & 11,300     & 144 \\
          & Concept-concept & 13     & 3763     & 35 \\
    \midrule
    \multicolumn{1}{c|}{\multirow{3}[2]{*}{\makecell{Factor \\ Graph}}} & Concept Variable     & 10     & 112     & 22 \\
          & Category Variable     & 12     & 200     & 16 \\
          & Logical Factor     &   46    &   15,063    & 179 \\
    \bottomrule
    \end{tabular}%
  \label{tab:Statistics}%
  \vspace{-1em}
\end{table}%
%\textit{Synthetic-MNIST.} 
%每个类别标签对应的概念标签如表2所示
%改成图片形式！！

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{img/mnistconcept-class.jpg}
\vspace{-1em}
\caption{Category labels and concept labels for Synthetic-MNIST.}
\label{fig:mnistconcept-class}
\vspace{-1em}
\end{figure}

%我们根据表2构建Synthetic-MNIST的逻辑规则集。我们以类别标签为[0,1,2,3]的样本为例，我们构建B规则，A规则。注意合成数据集上的逻辑规则可以覆盖下游任务的全部知识，因此我们必须随机舍弃一部分规则，来模拟显式知识的不完备性。
%Overall, 15,063 rules are constructed on the CUB dataset, consisting of 1,000 A rules and 1,000 B rules.
%我们根据得分来构建逻辑规则集。我们将得分接近1的关联作为共存约束的A规则，将得分接近0的关联作为互斥约束的A规则。对于B规则，我们将一个概念在每个类别下的得分组成一个得分向量。我们计算每个概念的得分向量之间的余弦相似度，得分向量之间的余弦相似度越高，说明两个概念在不同类别间都具有相似的关联得分。受空间限制，我们仅可视化了随机选择的30个概念的余弦相似度，如图B所示。根据图B, 我们在余弦相似度>0.8的两个概念之间，构建一条共存约束的B规则，在余弦相似度<0.2的两个概念之间构建一条互斥约束的B规则。
%总的来说，15063条规则被构建在CUB数据集上。
%热图调整概念序号！！！
\subsection{Baselines}\label{ap:Baselines}
To evaluate the comprehensibility of the explanations, we compared AGAIN with 8 popular concept-level interpretable model baselines. The baseline models are listed as follows:
\begin{itemize}
\item{\textbf{CBM}~\citep{Koh53382020} is a classical interpretable neural network implemented with the concept bottleneck structure.
}
\item{\textbf{Hard AR}~\citep{havasi2022addressing} expands the concept set with side-channels, and predicts categories with binary concept vectors.
}
\item{\textbf{ICBM}~\citep{Chauhan59482023} introduces interactive policy learning with cooperative prediction to filter important concepts.
}
\item{\textbf{PCBM}~\citep{yuksekgonul2023posthoc} introduces a concept-level self-interpretation module that automatically captures concepts through multimodal models.
}
\item{\textbf{ProbCBM}~\citep{kim2023probabilistic} uses probabilistic concept embedding to model uncertainty in concept predictions and explains predictions in terms of the likelihood that the concept exists.
}
\item{\textbf{Label-free CBM}~\citep{oikarinen2023labelfree} introduces a multimodal model to generate a predefined concept set for the inputs and learns the mapping between input features and the concept set.
}
\item{\textbf{ProtoCBM}~\citep{Huang_Song_Hu_Zhang_Wang_Song_2024} utilizes cross-layer alignment and cross-image alignment to learn the mapping of different parts of the feature map to concept predictions, thereby promoting the model to capture trustworthy concept prototypes.
}
\item{\textbf{ECBMs}~\citep{xu2024energybased} utilizes conditional probabilities to quantify predictions, concept corrections, and conditional dependencies to capture higher-order nonlinear interactions between concepts for improving the reliability of concept activations.
}
\end{itemize}
\subsection{Evaluation Metric}\label{ap:EvaluationMetric}

\begin{itemize}
\item{\textbf{P-ACC}. 
P-ACC measures the validity of concept-level explanations by reapplying rectified explanations to the category predictor and examining the outcomes of category predictions.
}
\item{\textbf{E-ACC}. E-ACC measures the similarity between a concept-level explanation and the ground-truth explanatory concept set, which is computed as follows:
\begin{equation}
E\text{-}ACC = \mathbb{E}\left[\frac{\sum_{m \in M} \mathbb{I}\left[\hat{c}_{re,m} = c_m\right]}{M}\right]
,
\end{equation}
%其中，P表示一个样本的第m个概念激活的二值转换结果，cm表示第m个概念的真实标签。Q值越高，说明解释A越接近解释标签。
 where $\hat{c}_{re,m}$ represents the binary transformation result for the $m$-th concept activation, and $c_m$ denotes the ground-truth label of the $m$-th concept.
}
\item{\textbf{LSM}. Since the comprehensibility of explanations is based on prior human logics, the extent to which an explanation adheres to logical rules can be utilized as a criterion for evaluating its comprehensibility. In this experiment, we formulated LSM to quantify the extent to which an explanation adheres to the prior human logics rules. This metric is employed to evaluate the comprehensibility of the explanation. A higher LSM indicates a higher comprehensibility of the explanation. In particular, we calculate the LSM by considering the weighted sum of potential functions within the factor graph. For an explanation $\mathbf{a}$, the definition of LSM is as follows:
\begin{equation}
LSM = \mathbb{E}\left[\frac{\exp\left(\sum_{i \in N} {w_i \psi_i}\right)}{\prod_{i \in N} {w_i e}}\right].
\end{equation}
%逻辑满足度的最大值为1，表示解释A满足了被编码在因子图中的全部逻辑规则。

The maximum value for LSM is 1, signifying that the explanation adheres to all the logical rules encoded in $\cal G$.
}
\item{\textbf{IR and SR}. In this experiment, IR and SR are devised to evaluate the capacity of the factor graph for recognizing perturbations. IR quantifies the rate at which perturbed instances are identified by the factor graph, computed as $\mathbb{E}\left[ \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right) > \partial \cdot {}_\vee \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right) | x+\delta \right]$, while SR measures the rate at which benign instances are allowed to pass through the factor graph, computed as $ \mathbb{E}\left[ \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right) > \partial \cdot {}_\vee \mathbb{P}\left( {{\cal V}^c\left|{\cal V}^y \right.} \right)| x \right]$.
}
\end{itemize}
%\textit{Logical Satisfaction Metric.} %逻辑满足度。因为解释的可理解性是基于人类的先验逻辑的，因此可以采用解释对逻辑规则的满足程度去评估解释的可理解性。在本实验中，我们设计了一个逻辑满足度来度量解释对先验逻辑规则的满足程度，进而评估解释的可理解性。逻辑满足度越高说明解释的可理解性越强。具体地，我们根据因子图中的势函数加权和来计算逻辑满足度。对于一个解释A，我们给出了逻辑满足度的定义：
%在本实验中，为了评估因子图对的扰动的识别能力，A和B被设计。A度量受扰动的实例被因子图识别到的比率，它被计算为D，B度量良性的实例被因子图通过的比率，它被计算为F
%解释精度度量概念级解释与真实解释的相似程度，它被计算为如下形式。

Remarkably, only CBM, ProtoCBM, ECBMs, and ProbCBM baselines are suitable to Synthetic-MNIST dataset, as the concepts in this dataset is synthetic and cannot be automatically captured by other baselines.
\subsection{Implementation Details}\label{ap:ImplementationDetails}
AGAIN is implemented in PyTorch 1.1.0 based on Python 3.7.13. 
We construct ${\cal G}$ by instantiating the ${\cal G}$ as a Markov logic network in Pracmln 1.2.4. 
Within the AGAIN, a fully connected layer is utilized as the category predictor. InceptionV3~\citep{Koh53382020}, a popular convolutional neural network structure, is employed as the concept predictor trained on real-world datasets. 
We use a convolutional neural network with two convolutional layers and normalization operations as the concept predictor trained on Synthetic-MNIST.
We train the concept predictor~(real-world datasets) for 500 epochs, the concept predictor~(Synthetic-MNIST) for 30 epochs, and the category predictor for 15 epochs.
We leverage the sgd optimizer with a learning rate of 0.01 to optimize the model.  
We to mitigate the overfitting, weight decay of 0.00004 was configured. 
In the experiment, $\partial$ is set to 0.9.
All experiments were repeated 4 times and the average of the results is reported.
\subsection{Estimation of Weights}\label{ap:Estimation of Weights}
Due to the differences in the datasets, we use two methods to set rule weights in the factor graphs: prior setting and likelihood estimation. 
For the CUB dataset, we use the prior-based weighting method, as it provides predefined confidence for the mapping relations between concepts and categories. 
Therefore, we treat the weights as hyperparameters and directly convert the confidence into corresponding weights.

For the MIMIC-III EWS and Synthetic-MNIST datasets, we use likelihood estimation to learn the weights, as these datasets do not contain confidence information.
Specifically, we directly apply standard maximum likelihood estimation~\cite{yang2022improving} to learn the weights of each factor. 
We assume that the samples in the training set should satisfy all logical rules when there is no perturbation. 
Therefore, after assigning the concept activations of the samples to the factor graph, the weights of the factors should maximize the conditional probability $\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y \right.}\right)$.
In summary, we minimize the negative log-likelihood function:
\begin{equation}
\mathbf{w} = \mathop {\arg \min }\limits_\mathbf{w} \left\{ { - \sum\limits_N {\log \left( {\mathbb{P}\left({{\cal V}^c\left|{\cal V}^y,\mathbf{w} \right.}\right)} \right)} } \right\},
\end{equation}
where $N$ denotes the number of samples in the training set, and $\mathbf{w}$ denotes the weight vector formed by concatenating the weights of all factors in the factor graph.
%{\subsection{Details of Factor Graph Reasoning}\label{ap:DetailsofFactorGraphReasoning}
%\textcolor{red}{Specifically, after each variable (concept and category) in $\mathcal{G}$ is assigned a value, the potential function $\psi _i$ for each factor $f_i$ computes a boolean value. This value indicates whether the assigned values of the concepts and categories satisfy the logical rule $r_i$. The degree to which the concept activation vector $\hat{\mathbf{c}}$ satisfies the logical rules in $\mathcal{G}$ can then be quantified as a weighted sum of the potential functions of all factors. 
%Further, we transform the weighted sum of potential functions into a probability, representing the likelihood of the current concept activations occurring under the logical constraints of $\mathcal{G}$. This requires considering all possible variable assignments and computing the expectation of the current variable assignments. This expectation is treated as a conditional probability, which is then used to detect whether the concept activation has been perturbed.
%For example, suppose there are two concepts, $A$ and $B$. A binarized concept activation vector $\hat{\mathbf{c}}$=[1,0] indicates that $A=1$ (active) while $B=0$ (inactive). To compute the expectation for binarized $\hat{\mathbf{c}}$=[1,0], we calculate the weighted sum of the potential functions for all four possible variable assignment cases: $\{[1,0], [0,1], [1,1], [0,0]\}$.}

\section{More Experimental Results}\label{ap:More Experimental Results}
% \textcolor{red}{\subsection{Comparison Analysis}
% We choose six typical knowledge integration methods as baselines: DeepProblog~\cite{manhaeve2018deepproblog}, MBM~\cite{patel2022modeling}, C-HMCNN~\cite{giunchiglia2020coherent}, LEN~\cite{ciravegna2023logic}, DKAA~\cite{melacci2021domain}, and MORDAA~\cite{yin2021exploiting}. The goal is to compare factor graph-based knowledge integration methods with other existing approaches in terms of enhancing the comprehensibility of explanations. 
% Specifically, since DeepProblog, MBM, and C-HMCNN are unable to generate concepts, we splice their knowledge integration modules onto the CBM. This ensures that all spliced variants are capable of generating a set of concepts. We then evaluate their LSMs on each of the three datasets. The experimental results are shown in Table~\ref{ex:knowledge integration Ablation.}. The results show that the LSM of AGAIN is optimal.
% In contrast, deepProblog can only constrain category predictions, not concept predictions, which results in low LSM under perturbation.
% The knowledge introduced by methods MBM and C-HMCNN can constrain concepts, but they only use logical rules between concepts and concepts, making their performance inferior to our model.}

% \textcolor{red}{Furthermore, we compare AGAIN with DKAA, MORDAA, and LEN.
% Since DKAA and MORDAA have Multi-label predictors, we directly use Multi-label predictors to predict concepts.
% The experimental results are shown in Table~\ref{ex:Compare the LSM of AGAIN with knowledge integration based methods.}. 
% The results indicate that AGAIN achieves the highest LSM. In contrast, LEN can only constrain category predictions. 
% DKAA and MORDAA detect adversarial perturbations in the samples using external knowledge, but they cannot correct the wrong concepts triggered by these perturbations. 
% Therefore, factor graph-based knowledge integration is more effective than the above methods in enhancing the comprehensibility of explanations under unknown perturbations.}
% \begin{table*}[htbp]\scriptsize
%   \centering
%   \vspace{-1em}
%   \caption{\textcolor{red}{Comparison of LSM between AGAIN, DeepProblog, MBM, and C-HMCNN.}}
%   \renewcommand\arraystretch{0.8}
%     \begin{tabularx}{\linewidth}
%     {>{\centering\arraybackslash}m{1.00cm}|
%     >{\centering\arraybackslash}m{2.0cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}}
%     \toprule
%     \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Methods}  & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
% \cmidrule{3-10}          &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
%     \midrule
%     \multirow{4}[1]{*}{CUB} & DeepProblog+CBM &  89.2(3.5) & 77.4(2.8) & 53.7(1.3) & 39.4(5.4) & 89.2(3.5) & 77.4(5.5) & 53.1(3.8) & 39.4(5.4) \\
%         & MBM+CBM &  93.5(3.1)  & 90.3(2.4) & 88.7(3.2) & 85.7(6.7) & 93.5(3.2)  & 90.3(2.7) & 88.7(3.2) & 85.7(6.7) \\
%         & C-HMCNN+CBM &  93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) & 93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) \\
%         & AGAIN(Ours) &  \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} & \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} \\
%     \midrule
%     \multirow{4}[1]{*}{\makecell{MIMIC-III\\EWS}} & DeepProblog+CBM &  90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) & 90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) \\
%         & MBM+CBM &  92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) & 92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) \\
%         & C-HMCNN+CBM &  94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) & 94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) \\
%         & AGAIN(Ours) &  \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} & \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} \\
%     \midrule
%     \multirow{4}[1]{*}{\makecell{Synthetic-\\MNIST}} & DeepProblog+CBM & 97.6(3.5) & 95.6(3.5) & 92.6(3.5) & 86.8(4.6) & 98.6(3.5) & 95.6(3.5) & 92.6(3.5) & 86.8(4.6) \\
%         & MBM+CBM &  97.7(0.7)  & 95.1(1.2) & 92.7(3.4) & 90.7(0.9) &  97.7(0.7)  & 95.1(1.2) & 92.7(3.4) & 90.7(0.9) \\
%         & C-HMCNN+CBM &  96.8(1.2)  & 94.5(1.6) & 90.7(0.9) & 90.5(1.1) & 96.8(1.2)  & 94.5(1.6) & 90.7(0.9) & 90.5(1.1) \\
%         & AGAIN(Ours) &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}} &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}}\\
%     \bottomrule
%     \end{tabularx}%
%   \label{ex:knowledge integration Ablation.}%
%   \vspace{-1em}
% \end{table*}%
% \begin{table*}[htbp]\scriptsize
%   \centering
%   \vspace{-1em}
%   \caption{\textcolor{red}{Comparison of LSM between AGAIN, DeepProblog, MBM, and C-HMCNN.}}
%   \renewcommand\arraystretch{0.8}
%     \begin{tabularx}{\linewidth}
%     {>{\centering\arraybackslash}m{1.00cm}|
%     >{\centering\arraybackslash}m{2.0cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}}
%     \toprule
%     \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Methods}  & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
% \cmidrule{3-10}          &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
%     \midrule
%     \multirow{7}[1]{*}{CUB} & LEN &  89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) & 89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) \\
%     & DKAA &  91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) & 91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) \\
%     & MORDAA &  91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) & 91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) \\
%      & DeepProblog+CBM &  89.2(3.5) & 77.4(2.8) & 53.7(1.3) & 39.4(5.4) & 89.2(3.5) & 77.4(5.5) & 53.1(3.8) & 39.4(5.4) \\
%         & MBM+CBM &  93.5(3.1)  & 90.3(2.4) & 88.7(3.2) & 85.7(6.7) & 93.5(3.2)  & 90.3(2.7) & 88.7(3.2) & 85.7(6.7) \\
%         & C-HMCNN+CBM &  93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) & 93.6(7.2)  & 89.7(1.2) & 87.6(2.5) & 85.0(3.2) \\
%         & AGAIN(Ours) &  \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} & \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} \\
%     \midrule
%     \multirow{7}[1]{*}{\makecell{MIMIC-III\\EWS}} & LEN &  90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) & 90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) \\
%         & DKAA &  96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) & 96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) \\
%         & MORDAA &  95.9(2.4) & 94.7(2.3) & 86.3(1.8) & 79.4(4.6) & 95.9(2.4) & 94.7(2.3) & 86.3(1.8) & 79.4(4.6) \\
%     & DeepProblog+CBM &  90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) & 90.4(1.7) & 75.7(1.3) & 50.4(1.5) & 39.8(1.4) \\
%         & MBM+CBM &  92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) & 92.7(4.2) & 88.7(2.4) & 86.3(1.3) & 84.4(5.7) \\
%         & C-HMCNN+CBM &  94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) & 94.0(2.6)  & 92.5(1.6) & 85.5(5.7) & 83.5(5.2) \\
%         & AGAIN(Ours) &  \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} & \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} \\
%     \midrule
%     \multirow{7}[1]{*}{\makecell{Synthetic-\\MNIST}} & LEN &  97.6(1.4) & 95.5(2.9) & 92.3(2.1) & 87.0(1.1) & 98.6(1.4) & 95.5(2.9) & 92.3(2.1) & 87.0(1.1) \\
%         & DKAA &  98.1(0.8) & 96.7(1.8) & 96.0(1.7) & 92.1(2.1) & 98.1(0.8) & 96.7(1.8) & 96.0(1.7) & 92.1(2.1) \\
%         & MORDAA &  98.5(0.9) & 95.8(1.6) & 92.1(1.3) & 89.3(3.1) & 98.5(0.9) & 95.8(1.6) & 92.1(1.3) & 89.3(3.1) \\
%     & DeepProblog+CBM & 97.6(3.5) & 95.6(3.5) & 92.6(3.5) & 86.8(4.6) & 98.6(3.5) & 95.6(3.5) & 92.6(3.5) & 86.8(4.6) \\
%         & MBM+CBM &  97.7(0.7)  & 95.1(1.2) & 92.7(3.4) & 90.7(0.9) &  97.7(0.7)  & 95.1(1.2) & 92.7(3.4) & 90.7(0.9) \\
%         & C-HMCNN+CBM &  96.8(1.2)  & 94.5(1.6) & 90.7(0.9) & 90.5(1.1) & 96.8(1.2)  & 94.5(1.6) & 90.7(0.9) & 90.5(1.1) \\
%         & AGAIN(Ours) &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}} &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}}\\
%     \bottomrule
%     \end{tabularx}%
%   \label{ex:knowledge integration Ablation.}%
%   \vspace{-1em}
% \end{table*}%

% \begin{table*}[htbp]\scriptsize
%   \centering
%   \vspace{-1em}
%   \caption{\textcolor{red}{Comparison of LSM between AGAIN, LEN, DKAA, and MORDAA.}}
%   \renewcommand\arraystretch{0.8}
%     \begin{tabularx}{\linewidth}
%     {>{\centering\arraybackslash}m{1.62cm}|
%     >{\centering\arraybackslash}m{1.4cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}|
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}
%     >{\centering\arraybackslash}m{0.84cm}}
%     \toprule
%     \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Methods}  & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
% \cmidrule{3-10}          &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
%     \midrule
%     \multirow{4}[1]{*}{CUB} 
%         & LEN &  89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) & 89.1(3.4) & 77.8(1.6) & 56.7(1.2) & 40.4(1.3) \\
%         & DKAA &  91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) & 91.2(1.5) & 85.6(1.6) & 76.9(1.3) & 73.7(5.3) \\
%         & MORDAA &  91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) & 91.7(1.5) & 86.1(1.8) & 80.6(2.1) & 76.8(3.1) \\
%         & AGAIN(Ours) &  \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} & \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} \\
%     \midrule
%     \multirow{4}[1]{*}{MIMIC-III EWS} 
%         & LEN &  90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) & 90.3(2.6) & 75.7(2.8) & 50.3(3.8) & 40.2(7.1) \\
%         & DKAA &  96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) & 96.1(1.6) & 87.3(2.7) & 79.8(2.1) & 75.8(4.7) \\
%         & MORDAA &  95.9(2.4) & 94.7(2.3) & 86.3(1.8) & 79.4(4.6) & 95.9(2.4) & 94.7(2.3) & 86.3(1.8) & 79.4(4.6) \\
%         & AGAIN(Ours) &  \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} & \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} \\
%     \midrule
%     \multirow{4}[1]{*}{Synthetic-MNIST} 
%         & LEN &  97.6(1.4) & 95.5(2.9) & 92.3(2.1) & 87.0(1.1) & 98.6(1.4) & 95.5(2.9) & 92.3(2.1) & 87.0(1.1) \\
%         & DKAA &  98.1(0.8) & 96.7(1.8) & 96.0(1.7) & 92.1(2.1) & 98.1(0.8) & 96.7(1.8) & 96.0(1.7) & 92.1(2.1) \\
%         & MORDAA &  98.5(0.9) & 95.8(1.6) & 92.1(1.3) & 89.3(3.1) & 98.5(0.9) & 95.8(1.6) & 92.1(1.3) & 89.3(3.1) \\
%         & AGAIN(Ours) &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}} &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}}\\
%     \bottomrule
%     \end{tabularx}%
%   \label{ex:Compare the LSM of AGAIN with knowledge integration based methods.}%
%   \vspace{-1em}
% \end{table*}%

\subsection{Ablation Analysis of Factor Graphs}
%为了验证使用因子图的必要性，我们进行了一项消融研究，它在三个数据集上对比使用因子图和不使用因子图对LSM的影响。我们用”w/o Factor Graph“来表示只利用预先定义的逻辑规则集，用”w/ Factor Graph“表示用因子图来编码逻辑规则。实验结果如表2所示，我们发现，相比只利用逻辑规则集，使用因子图在三个数据集上均能获得更优的LSM。这得益于因子图所具有的不确定性推理的能力。因子图可以估计不同规则的置信度，并为规则分配合适的权重。值得注意的是在Synthetic-MNIST数据集上，用逻辑规则集能够获得与因子图相近的LSM，这是因为Synthetic-MNIST数据集中的大多数逻辑规则是合成的，它们具有确定性。但确定性的逻辑规则很难适用于真实场景。
To validate the necessity of using factor graphs, we conducted a set of ablation studies comparing the impact on LSM with and without factor graphs across three datasets. 
The altered model without factor graphs is denoted as “w/o Factor Graph,” which only uses a predefined set of logic rules, and the model with factor graphs is denoted as “w/ Factor Graph,” which encodes the logic rules using a factor graph. The experimental results, shown in Table~\ref{ex:Comparison of LSM with and without the factor graph.}, indicate that the model using factor graphs consistently yields better LSMs across all three datasets compared to the model that uses only the logic rule set. 
This improvement is attributed to the uncertainty reasoning capability of factor graphs, which can estimate the confidence levels of different rules and assign appropriate weights accordingly. 
Notably, on the Synthetic-MNIST dataset, the LSMs achieved with just the logic rule set are comparable to those with factor graphs. 
This is because most logic rules in the Synthetic-MNIST dataset are synthesized and deterministic by nature. 
Purely deterministic logic rules are often less applicable in real-world scenarios. Therefore, factor graphs offer an irreplaceable advantage in detecting erroneous explanations in real-world scenarios. 
\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparison of LSM with and without the factor graph.}
  \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {>{\centering\arraybackslash}m{1.22cm}|
    >{\centering\arraybackslash}m{1.8cm}|
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}|
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}
    >{\centering\arraybackslash}m{0.84cm}}
    \toprule
    \multirow{2}[2]{*}{Dataset} & \multirow{2}[2]{*}{Methods}  & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
\cmidrule{3-10}          &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
    \midrule
    \multirow{2}[1]{*}{CUB} & w/o Factor Graph&  89.0(5.2) & 87.6(6.7) & 87.1(7.1) & 86.1(6.2) & 89.0(5.2) & 87.6(6.7) & 87.1(7.1) & 86.1(6.2) \\
        & w/ Factor Graph &  \underline{\textbf{92.4(1.2)}} & \underline{\textbf{93.1(2.3)}} & \underline{\textbf{93.8(1.9)}} & \underline{\textbf{91.5(1.7)}} & \underline{\textbf{94.5(1.6)}} & \underline{\textbf{93.3(1.7)}} & \underline{\textbf{93.8(1.4)}} & \underline{\textbf{92.1(2.1)}} \\
    \midrule
    \multirow{2}[1]{*}{\makecell{MIMIC\\-III EWS}} & w/o Factor Graph &  89.9(4.1) & 89.8(7.2) & 88.9(7.6) & 87.4(6.9) & 89.9(4.1) & 89.8(7.2) & 88.9(7.6) & 87.4(6.9) \\
        & w/ Factor Graph &  \underline{\textbf{96.1(0.7)}} & \underline{\textbf{94.2(1.4)}} & \underline{\textbf{96.1(1.2)}} & \underline{\textbf{94.2(1.2)}} & \underline{\textbf{94.0(2.7)}} & \underline{\textbf{94.1(6.3)}} & \underline{\textbf{94.2(2.4)}} & \underline{\textbf{92.3(4.7)}} \\
    \midrule
    \multirow{2}[1]{*}{\makecell{Synthetic\\-MNIST}} &  w/o Factor Graph & 98.1(0.8) & 97.3(1.5) & 96.9(4.9) & 94.9(3.8) & 98.1(0.8) & 97.3(1.5) & 96.9(4.9) & 94.9(3.8) \\
        & w/ Factor Graph &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.4)}} &  \underline{\textbf{95.6(1.2)}} &  \underline{\textbf{98.2(1.4)}} &\underline{\textbf{97.9(1.3)}} &\underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}}\\
    \bottomrule
    \end{tabularx}%
  \label{ex:Comparison of LSM with and without the factor graph.}%
  \vspace{-1em}
\end{table*}%

\subsection{Computational Efficiency Analysis}
We assess the potential impact of increasing factor graph size on computational efficiency. 
Specifically, we evaluate four factor graph sizes, containing 30, 60, 90, and 112 concepts, on the CUB dataset, measuring their running time and LSM performance, respectively.
To the best of our knowledge, CUB is currently the dataset with the highest number of concepts (with 112 concept labels).  
In addition, it is not necessary to discuss other datasets that have more concepts than CUB. 
Too many concepts can lead to lengthy explanations, reducing comprehensibility. The evaluation is performed under the unknown perturbation of $\epsilon$=32.

We report the running time required to detect and correct erroneous explanations and the LSM of the corrected explanations for all 4 scales of factor graphs~(see Figure~\ref{fig:time}).
According to the results, the running time of the factor graph is inevitably higher than that of the other baselines due to the extra steps of detection and correction required for the factor graph. However, this extra overhead can be contained to the millisecond level. Furthermore, the running time is indeed proportional to the size of the factor graph, but there is no exponential explosion, suggesting that AGAIN has good scalability.

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{img/time.jpg}
\vspace{-1em}
\caption{The computational efficiency analysis of factor graphs with different sizes.}
\label{fig:time}
\vspace{-1em}
\end{figure}

\subsection{Analysis of Intervention Strategies}
%我们对比了不同干预策略在纠正概念激活时所产生的计算开销。具体地，我们选择了贪婪策略和启发式策略作为对比基线。在理论上，启发式策略和本文的概念干预策略类似。我们提供错误的概念索引给启发式策略，并提供势函数为0的因子索引给概念干预策略。我们以A=32的扰动为干扰环境，在CUB数据集上评估了三种策略的计算开销。
We compare the computational overhead associated with different intervention strategies for correcting concept activations. Specifically, we use the greedy strategy and the heuristic strategy as baselines for comparison. For the heuristic strategy, we provide the indexes of incorrect concepts, while for the concept intervention strategy, we supply the indexes of factors with a potential function of 0. We evaluate the computational overhead of these three strategies on the CUB dataset under a perturbation of $\epsilon$=32.
5 samples are selected and randomly modified between 1 and 10 concepts in each sample. Figure~\ref{fig:AnalysisInterventionStrategie} illustrates the average number of interventions performed by different strategies across these samples. The experimental results indicate that the number of interventions increases for all three strategies as the number of incorrect concepts rises. When the number of incorrect concepts is fewer than 10, the differences in the number of interventions among the three strategies are minimal. Notably, the number of perturbed wrong concepts is typically small. For instance, in the CUB dataset, the number of wrong concepts is typically only 1 to 10.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{img/AnalysisInterventionStrategie.jpg}
\vspace{-1em}
\caption{Comparison of intervention numbers for different intervention strategies.}
\label{fig:AnalysisInterventionStrategie}
\vspace{-1em}
\end{figure}

In addition, we theoretically analyze the computational complexity of the intervention strategy of AGAIN.
$\{\vee,\wedge,\neg\}$ is proved to be sufficient to express all logical relations. Based on this, if two concepts are randomly selected from a set of $M$ concepts, there are at most $\frac{{M\left( {M - 1} \right)}}{2}$ possible combinations. The maximum number of distinct logical rules between two concepts is 8, specifically: $A \wedge B;{}^\neg A \wedge B;A \wedge {}^\neg B;{}^\neg A \wedge {}^\neg B; A \vee B;{}^\neg A \vee B;A \vee {}^\neg B;{}^\neg A \vee {}^\neg B$
Thus, the maximum number of factors is $4M(M - 1)$. With a maximum of 3 interventions per rule (intervene A, intervene B, and both), the total number of interventions is $12M(M - 1)$, resulting in a complexity of $O(M^2)$.

\subsection{Experimental Results on Synthetic-MNIST}\label{ex5.3}
\paragraph{Comprehensibility of Explanations.}
Table~\ref{ex:Comprehensibility of explanations-MNIST} illustrates the LSM results of our proposed AGAIN in comparison with 10 baselines on the Synthetic-MNIST dataset. The results indicate that the explanations produced by AGAIN for the synthetic dataset are equally comprehensible, implying that the performance of AGAIN exhibits generalizability.
Specifically, when the unknown perturbation magnitude is 8, the LSM of AGAIN exhibits an average increase of 41.83\% compared to the baseline with attributional training. Furthermore, in benign environments, the LSM of AGAIN outperforms other baselines due to its ability to maintain optimal model parameters without adjusting them to accommodate the effects of perturbations.

\paragraph{Rectification of Interactive Intervention Switch.}
%图6可视化了AGAIN在MNIST上纠正后的完整解释。从图中，解释中被激活的手写数字概念可以和合成图像完全对应，这说明纠正后的解释是逻辑完备且富有语义的。这个结果证明interactive concept interventions在合成数据集上依然是有效的。
Figure~\ref{ex:Rectification of interactive interventions}~(c) illustrates the visual representations of the rectified explanations generated by AGAIN. The activated handwritten digit concepts in the explanations align seamlessly with the semantics of synthetic images, confirming the logical completeness and semantic richness of the rectified explanations. This outcome validates the continued effectiveness of interactive intervention switch strategy on the synthetic dataset.

\begin{table}\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of LSM for our AGAIN on the Synthetic-MNIST dataset.}
  \renewcommand\arraystretch{0.8}
    \begin{tabular}{c|ccc|cc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{2}{c|}{$\delta_k$}        & \multicolumn{2}{c}{$\delta_u$} \\
    \cmidrule{3-6}
          & \multicolumn{1}{r}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=4&$\epsilon$=8\\
    \midrule
    CBM & 97.8(0.6) & 92.6(3.5) & 86.8(4.6) & 92.6(4.7) & 86.3(5.4) \\
    ProbCBM & 98.6(1.1) & 92.9(4.2) & 87.3(3.4) & 92.4(3.2) & 86.7(3.1) \\
    ProtoCBM & 97.3(0.2) & 94.5(0.8) & 90.7(1.2) & 94.7(2.7) & 88.9(4.3) \\
    ECBMs & 97.5(0.7) & 92.6(3.5) & 88.3(2.8) & 92.6(3.4) & 88.7(2.7) \\
    \midrule
    CBM-AT & 93.5(0.8) & 93.0(2.4) & 90.4(3.1) & 92.6(2.4) & 86.3(1.3) \\
    ProbCBM-AT & 93.5(0.7) & 92.9(2.7) & 90.4(2.6) & 92.4(3.4) & 86.7(2.1) \\
    ProtoCBM-AT & 96.1(1.7) & 92.1(1.9) & 90.7(1.0) & 87.5(1.4) & 84.3(3.1) \\
    ECBMs-AT & 95.9(1.4) & 90.4(4.2) & 89.5(1.7) & 88.0(1.2) & 84.7(2.5) \\
    \midrule
    LEN & 98.6(0.3) &  97.6(1.4) & 95.5(2.9) & 98.6(1.4) & 95.5(2.9)\\
    DKAA & 98.4(1.0) & 98.1(0.8) & 96.7(1.8) & 98.1(0.8) & 96.7(1.8) \\
    MORDAA & 97.9(0.8) & 98.5(0.9) & 95.8(1.6) & 98.5(0.9) & 95.8(1.6) \\
    DeepProblog & 97.5(1.4) & 97.6(3.5) & 95.6(3.5) & 98.6(3.5) & 95.6(3.5)\\
    MBM & 98.7(0.1) &  97.7(0.7)  & 95.1(1.2) &  97.7(0.7)  & 95.1(1.2)\\
    C-HMCNN & 98.1(0.4) & 96.8(1.2)  & 94.5(1.6) & 96.8(1.2)  & 94.5(1.6)\\
    \midrule
    AGAIN (Ours) &  \underline{\textbf{98.9(1.3)}} &  \underline{\textbf{97.8(1.4)}} &  \underline{\textbf{95.6(1.2)}} &  \underline{\textbf{97.8(1.6)}} &  \underline{\textbf{97.8(2.0)}}\\
    \bottomrule
    \end{tabular}%
  \label{ex:Comprehensibility of explanations-MNIST}%
  \vspace{-1em}
\end{table}%

\subsection{E-ACC and P-ACC on EACC-MIMIC-IIIEWS and Synthetic-MNIST}\label{E-ACC and P-ACC for AGAIN}
We present the E-ACC of AGAIN on EACC-MIMIC-IIIEWS and Synthetic-MNIST in comparison to all baseline methods, as shown in Tables~\ref{tab:EACC-MIMIC-IIIEWS}, and~\ref{tab:EACC-Synthetic-MNIST}, respectively. The experimental results show that AGAIN is optimal for E-ACC on two datasets.

Secondly, we report the P-ACC of AGAIN on the two datasets, as shown in Tables~\ref{tab:PACC-MIMIC-IIIEWS}, and~\ref{tab:PACC-Synthetic-MNIST}, respectively. Notably, since perturbations do not impact the final predictions, the P-ACC remains consistent across different levels of perturbation. Furthermore, the factor graph does not improve the predictive accuracy of the categories, making the P-ACC of AGAIN comparable to that of the other baselines. Notably, since the methods based on knowledge integration are not retrained. Therefore, there is no difference between the effects of known and unknown perturbations on these methods. Therefore, their results under known and unknown perturbations are the same.



\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of E-ACC between AGAIN and baselines on MIMIC-III EWS.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 97.1(1.2) & 92.8(1.1) & 90.3(1.3) & 87.2(3.1) & 85.4(1.7) & 90.5(1.4) & 87.7(1.1) & 84.1(2.6) & 76.8(2.1) \\
          Hard AR-AT & 97.7(0.4) & 92.9(2.1) & 89.9(1.2) & 87.4(2.4) & 84.3(1.4) & 90.1(2.4) & 84.1(0.6) & 80.3(3.1) & 78.8(6.7) \\
          ICBM-AT & 97.8(1.4) & 91.9(1.4) & 86.8(1.5) & 84.5(2.7) & 83.2(3.2) & 89.6(1.6) & 85.9(2.3) & 82.7(2.1) & 79.1(4.2) \\
          PCBM-AT & 96.9(1.7) & 92.6(1.4) & 86.3(2.1) & 83.2(0.8) & 83.0(2.1) & 90.5(1.2) & 84.6(2.7) & 81.5(3.2) & 77.4(3.9) \\
          ProbCBM-AT & 98.1(0.3) & 92.1(1.2) & 90.3(1.1) & 87.4(1.3) & 84.4(2.4) & 90.0(2.2) & 85.5(1.2) & 84.2(1.0) & 81.6(3.2) \\
          Label-free CBM-AT & 97.1(1.6) &93.1(2.1) & 90.3(1.5) & 87.5(3.2) & 84.6(2.3) & 91.0(2.5) & 86.7(1.7) & 83.7(2.2) & 80.6(4.1) \\
          ProtoCBM-AT & 97.8(0.8) & 93.4(1.3) & 92.1(1.0) & 86.7(1.4) & 85.6(2.1) & 92.1(0.6) & 89.5(1.2) & 85.8(0.9) & 81.2(2.4) \\
          ECBMs-AT & \underline{\textbf{98.0(1.2)}} & \underline{\textbf{93.6(1.7)}} & 92.7(2.6) & 88.3(1.7) & 88.1(2.2) & 90.4(1.8) & 86.4(2.2) & 83.4(1.8) & 80.2(3.1) \\
          \midrule 
          LEN & 97.7(0.2) & 92.2(2.4) & 91.6(1.2) & 86.7(2.6) & 83.2(0.9) & 92.2(2.4) & 91.6(1.2) & 86.7(2.6) & 83.2(0.9) \\
          DKAA & 96.5(1.3) & 91.7(1.3) & 92.9(0.8) & 87.5(1.4) & 85.1(3.2) & 91.7(1.3) & 92.9(0.8) & 87.5(1.4) & 85.1(3.2) \\
          MORDAA & 97.1(1.4) & 91.5(3.7) & 90.5(3.1) & 84.3(1.4) & 81.5(2.1) & 91.5(3.7) & 90.5(3.1) & 84.3(1.4) & 81.5(2.1) \\
          DeepProblog & 96.3(1.2) & 90.7(1.0) & 89.7(3.1) & 85.2(1.3) & 81.7(1.3) & 90.7(1.0) & 89.7(3.1) & 85.2(1.3) & 81.7(1.3) \\
          MBM & 97.9(1.0) & 91.9(2.2) & 90.1(0.6) & 86.7(0.9) & 82.6(1.3) & 91.9(2.2) & 90.1(0.6) & 86.7(0.9) & 82.6(1.3) \\
          C-HMCNN & 97.6(0.7) & 93.4(1.1)  & 91.0(3.1) & 87.4(2.4) & 84.9(2.8) & 93.4(1.1)  & 91.0(3.1) & 87.4(2.4) & 84.9(2.8) \\
          AGAIN & 97.5(0.1) & 93.0(1.3) & \underline{\textbf{93.2(1.3)}} & \underline{\textbf{93.0(1.4)}} & \underline{\textbf{93.0(1.2)}} & \underline{\textbf{93.0(1.7)}} & \underline{\textbf{93.6(1.6)}} & \underline{\textbf{93.3(1.9)}} & \underline{\textbf{93.2(1.2)}} \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:EACC-MIMIC-IIIEWS}%
  \vspace{-1em}
\end{table*}%

\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of E-ACC between AGAIN and baselines on Synthetic-MNIST.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 98.4(0.1) & 98.1(1.2) & 96.1(1.4) & 95.2(1.5) & 90.3(1.6) & 97.1(1.2) & 96.5(2.1) & 92.1(1.5) & 89.4(2.1) \\
          Hard AR-AT & 97.9(0.1) & 97.6(0.5) & 96.2(0.2) & 93.7(1.2) & 91.4(1.1) & 95.6(0.5) & 93.1(1.2) & 91.7(1.0) & 87.9(2.1) \\
          ICBM-AT & 98.2(0.3) & 98.2(0.2) & 95.1(0.6) & 92.1(1.5) & 90.5(1.4) & 95.1(1.1) & 94.7(1.1) & 92.1(1.5) & 88.2(1.4) \\
          PCBM-AT & 97.9(0.2) & 97.4(1.1) & 96.1(0.4) & 95.5(1.0) & 92.3(1.3) & 95.2(1.3) & 94.1(1.2) & 91.0(1.4) & 90.1(1.2) \\
          ProbCBM-AT & 98.1(0.2) & 97.9(0.2) & 95.1(1.0) & 93.1(1.4) & 90.5(1.3) & 94.3(1.1) & 93.1(1.2) & 91.0(0.8) & 88.2(1.1) \\
          Label-free CBM-AT & 97.5(0.7) &97.7(0.6) & 95.9(1.2) & 92.5(1.3) & 91.6(1.3) & 95.1(1.8) & 92.9(3.1) & 90.2(4.2) & 87.2(2.3) \\
          ProtoCBM-AT & \underline{\textbf{98.8(0.3)}} & \underline{\textbf{98.5(1.2)}} & 97.1(1.2) & 95.6(2.1) & 93.9(2.1) & 94.1(1.0) & 92.5(2.1) & 90.2(3.2) & 89.3(2.4) \\
          ECBMs-AT & 98.5(0.6) & 98.1(0.6) & 97.1(1.2) & 95.3(1.3) & 92.1(2.5) & 95.2(1.1) & 92.5(1.7) & 90.1(1.3) & 88.6(1.6) \\
          \midrule 
          LEN & 98.4(1.1) & 98.4(1.2) & 96.9(1.4) & 95.4(0.9) & 92.8(1.2) & 98.4(1.2) & 96.9(1.4) & 95.4(0.9) & 92.8(1.2) \\
          DKAA & 98.6(0.7) & 98.4(0.9) & 97.2(1.4) & 94.1(1.2) & 92.5(2.1) & 98.4(0.9) & 97.2(1.4) & 94.1(1.2) & 92.5(2.1) \\
          MORDAA & 98.3(0.7) & 97.9(1.2) & 95.2(1.2) & 92.1(1.4) & 90.6(2.6) & 97.9(1.2) & 95.2(1.2) & 92.1(1.4) & 90.6(2.6) \\
          DeepProblog & 97.5(1.1) & 97.3(2.1) & 95.3(2.4) & 92.7(2.1) & 89.4(3.2) & 97.3(2.1) & 95.3(2.4) & 92.7(2.1) & 89.4(3.2) \\
          MBM & 98.7(1.3) & 97.4(1.5) & 96.4(1.3) & 91.6(1.3) & 89.4(2.2) & 97.4(1.5) & 96.4(1.3) & 91.6(1.3) & 89.4(2.2) \\
          C-HMCNN & 98.8(0.9) & 96.7(2.1)  & 95.1(2.4) & 93.2(2.5) & 90.5(2.5) & 96.7(2.1)  & 95.1(2.4) & 93.2(2.5) & 90.5(2.5) \\
          \midrule 
          AGAIN & 98.1(0.5) & 98.1(0.5) & \underline{\textbf{97.3(1.3)}} & \underline{\textbf{97.3(2.9)}} & \underline{\textbf{97.2(2.1)}} & \underline{\textbf{98.2(0.9)}} & \underline{\textbf{97.4(1.0)}} & \underline{\textbf{97.1(1.2)}} & \underline{\textbf{96.8(1.4)}} \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:EACC-Synthetic-MNIST}%
  \vspace{-1em}
\end{table*}%

\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of P-ACC between AGAIN and baselines on MIMIC-III EWS.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 49.7(1.2) & 49.6(1.2) & 49.1(1.1) & 49.2(1.4) & 49.6(1.3) & 49.7(1.2) & 49.3(2.4) & 49.8(1.1) & 49.5(1.4) \\
          Hard AR-AT & 48.5(2.1) & 49.1(1.5) & 49.1(1.2) & 49.5(1.1) & 49.6(1.2) & 49.1(1.5) & 49.1(2.3) & 48.9(1.1) & 49.0(1.6) \\
          ICBM-AT & 49.1(1.6) & 49.6(1.4) & 49.1(1.4) & 49.5(1.2) & 49.4(1.1) & 48.9(1.1) & 49.4(1.4) & 49.5(1.7) & 49.9(1.4) \\
          PCBM-AT & 49.6(1.4) & 49.9(1.3) & 49.8(2.1) & 50.1(1.3) & 49.8(1.2) & 49.7(1.2) & 49.1(1.3) & 49.3(1.4) & 50.1(1.1) \\
          ProbCBM-AT & 49.8(0.8) & 49.8(1.0) & 51.4(2.1) & 50.2(1.3) & 49.6(1.5) & 50.2(1.1) & 49.4(1.3) & 49.6(1.1) & 49.7(0.9) \\
          Label-free CBM-AT & 49.7(1.3) & 49.7(1.2) & 49.7(1.1) & 49.6(1.4) & 49.5(1.2) & 49.6(1.1) & 49.5(1.5) & 49.6(1.2) & 49.7(1.1) \\
          ProtoCBM-AT & 49.6(1.2) & 49.5(1.1) & 48.9(2.0) & 49.1(1.4) & 49.3(1.3) & 49.3(1.2) & 48.9(2.1) & 49.0(1.3) & 49.3(1.2) \\
          ECBMs-AT & 50.9(1.5) & 50.2(1.3) & 49.6(2.0) & 49.8(1.3) & 49.8(1.6) & 49.5(0.7) & 49.6(2.5) & 49.3(1.2) & 49.8(1.2) \\
          \midrule 
          LEN & 49.6(1.6) & 49.7(1.3) & 49.4(1.4) & 49.8(0.4) & 49.5(1.6) & 49.7(1.3) & 49.4(1.4) & 49.8(0.4) & 49.5(1.6) \\
          DKAA & 49.4(1.1) & 49.8(0.1) & 50.5(0.2) & 49.7(1.2) & 49.8(1.2) & 49.8(0.1) & 50.5(0.2) & 49.7(1.2) & 49.8(1.2) \\
          MORDAA & 49.5(0.1) & 49.8(1.2) & 49.7(1.3) & 50.1(1.4) & 49.4(1.6) & 49.8(1.2) & 49.7(1.3) & 50.1(1.4) & 49.4(1.6) \\
          DeepProblog & 48.4(1.2) & 48.7(1.3) & 48.4(1.2) & 48.5(1.1) & 48.2(1.1) & 48.7(1.3) & 48.4(1.2) & 48.5(1.1) & 48.2(1.1) \\
          MBM & 49.4(1.3) & 50.1(0.6) & 49.3(1.7) & 49.4(0.5) & 49.3(1.7) & 50.1(0.6) & 49.3(1.7) & 49.4(0.5) & 49.3(1.7) \\
          C-HMCNN & 49.4(0.8) & 49.5(1.1) & 49.4(1.1) & 49.2(1.5) & 49.2(1.2) &  49.5(1.1) & 49.4(1.1) & 49.2(1.5) & 49.2(1.2) \\
          \midrule
          AGAIN & 55.1(2.9) & 49.7(1.1) & 49.1(2.2) & 49.4(0.1)  & 49.3(2.7) & 48.0(1.5) & 48.3(1.2) & 49.4(1.9) & 52.5(0.9) \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:PACC-MIMIC-IIIEWS}%
  \vspace{-1em}
\end{table*}%

\begin{table*}[htbp]\scriptsize
  \centering
  \vspace{-1em}
  \caption{Comparisons of P-ACC between AGAIN and baselines on Synthetic-MNIST.}
   \renewcommand\arraystretch{0.8}
    \begin{tabularx}{\linewidth}
    {
    >{\centering\arraybackslash}m{2.04cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}|
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}
    >{\centering\arraybackslash}m{0.85cm}}
    \toprule
     \multicolumn{1}{c|}{\multirow{2}[2]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{clear}} & \multicolumn{4}{c|}{$\delta_k$}        & \multicolumn{4}{c}{$\delta_u$} \\
    \cmidrule{3-10}
           & \multicolumn{1}{c}{} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16& \multicolumn{1}{c|}{$\epsilon$=32} &$\epsilon$=4&$\epsilon$=8&$\epsilon$=16&$\epsilon$=32\\
    \midrule 
          CBM-AT & 98.2(0.1) & 98.2(0.1) & 98.1(0.4) & 98.5(0.2) & 98.1(0.5) & 98.5(1.2) & 98.5(0.6) & 97.6(0.3) & 98.2(0.5) \\
          Hard AR-AT & 98.4(0.3) & 98.3(1.1) & 98.4(0.2) & 98.4(0.2) & 98.2(0.1) & 98.1(0.3) & 98.8(0.5) & 98.1(0.6) & 98.2(0.5) \\
          ICBM-AT & 97.5(1.4) & 98.3(0.1) & 97.4(0.2) & 98.4(0.8) & 97.3(0.5) & 98.2(1.1) & 98.5(0.3) & 97.1(0.1) & 98.6(0.3) \\
          PCBM-AT & 98.2(0.7) & 98.3(0.4) & 97.4(2.1) & 98.4(1.3) & 98.4(0.1) & 98.4(0.1) & 97.5(0.2) & 98.5(0.3) & 97.4(0.8) \\
          ProbCBM-AT & 98.1(0.2) & 98.4(0.2) & 98.3(0.7) & 98.2(0.1) & 98.2(0.7) & 98.4(0.8) & 98.2(0.5) & 98.1(0.5) & 98.2(0.6) \\
          Label-free CBM-AT & 98.2(0.5) & 98.1(0.3) & 98.4(0.2) & 98.4(0.6) & 97.5(1.1) & 97.5(0.2) & 98.5(0.2) & 98.8(0.6) & 98.3(0.5) \\
          ProtoCBM-AT & 98.2(0.2) & 97.9(0.8) & 98.2(0.4) & 98.6(0.7) & 98.4(0.7) & 98.4(0.4) & 98.3(0.2) & 98.4(0.1) & 98.6(0.7) \\
          ECBMs-AT & 99.1(0.2) & 98.5(0.4) & 98.6(0.5) & 98.3(0.7) & 97.2(0.9) & 98.3(0.7) & 98.2(0.7) & 98.5(0.8) & 98.3(1.6) \\
          \midrule 
         LEN & 98.2(0.4) & 98.4(0.3) & 98.2(0.1) & 98.4(0.5) & 98.3(0.2) & 98.4(0.3) & 98.2(0.1) & 98.4(0.5) & 98.3(0.2) \\
          DKAA & 98.4(0.1) & 98.6(0.2) & 97.9(0.1) & 98.2(0.2) & 98.3(0.1) &  98.6(0.2) & 97.9(0.1) & 98.2(0.2) & 98.3(0.1) \\
          MORDAA & 98.6(0.1) & 98.3(0.2) & 97.9(0.2) & 98.5(0.7) & 98.1(0.3) & 98.3(0.2) & 97.9(0.2) & 98.5(0.7) & 98.1(0.3) \\
          DeepProblog & 97.2(0.1) & 97.2(0.4) & 97.1(0.3) & 97.5(0.1) & 97.1(0.4) & 97.2(0.4) & 97.1(0.3) & 97.5(0.1) & 97.1(0.4) \\
          MBM & 98.2(0.1) & 98.2(0.3) & 98.1(0.2) & 98.5(0.1) & 98.1(0.4) & 98.2(0.3) & 98.1(0.2) & 98.5(0.1) & 98.1(0.4) \\
          C-HMCNN & 98.2(0.1) & 98.2(0.2) & 98.1(0.3) & 98.5(0.6) & 98.1(0.2) & 98.2(0.2) & 98.1(0.3) & 98.5(0.6) & 98.1(0.2) \\
          \midrule
          AGAIN & 98.3(0.5) & 98.2(0.6) & 98.1(0.7) & 98.2(0.1) & 98.2(0.6) & 98.5(0.3) & 98.7(0.4) & 98.2(0.7) & 98.3(1.1) \\
    \bottomrule    
    \end{tabularx}%  
  \label{tab:PACC-Synthetic-MNIST}%
  \vspace{-1em}
\end{table*}%

\section{Implementation Details of Adversarial Attacks}\label{ap:ImplementationDetailsofAdversarialAttacks}
In this section, we provide implementation details of adversarial attacks against concept-level explanations.
Specifically, adversarial attacks against concept-level explanations can be categorized into three types: erasure attacks, introduction attacks, and confounding attacks.
\paragraph{Erasure Attacks.}
Erasure attacks attempt to subtly remove a specific concept without changing category prediction results.
Gaps in perception and missing concepts are confusing for an analyst and very difficult to detect.
For CBMs, we typically have a pre-set threshold $\gamma$ for determining whether a concept is an activated concept. Specifically, for a sample $x$, We use $h_c^{(m)}(x)$ to denote the activation value of the $m$-th concept output by the concept predictor $h_c(\cdot)$. 
If $h_c^{(m)}(x)-\gamma > 0$, then the $m$-th concept is an activated concept. 
Attackers learn adversarial perturbations for executing erasure attacks by the following objective function:
\begin{equation}
\begin{array}{l}
\text{MAX}\sum\limits_{m \in M_E} {\left( {(\mathbb{I}[\gamma - {h_c}^{(m)}(x + \delta )] - \mathbb{I}[\gamma - {h_c}^{(m)}(x)]} \right)} \\
\text{s.t.}\quad\text{argmax}\enspace{h_y}\left( {{h_c}\left( {x + \delta } \right)} \right) = \text{argmax}\enspace{h_y}\left( {{h_c}\left( x \right)} \right)
\end{array}
,
\end{equation}
where $\mathbb{I}(\cdot)$ denotes the indicator function, $h_c(\cdot)$ is the concept predictor, and $h_y(\cdot)$ is the category predictor (task predictor). $\delta$ is the learnable perturbation. $M_E$ denotes the concept set that the attacker wishes to delete away.
\paragraph{Introduction Attacks.}
The purpose of introduction attacks is to manipulate the presence of irrelevant concepts without modifying the classification results. 
Such attacks hinder accurate analysis of model explanations.
Attackers attempt to introduce new irrelevant concepts that do not previously exist in the concept set of the original sample.
Unlike erasure attacks, introduction attacks require to raise the activation value of irrelevant concepts above the threshold $\gamma$. 
Therefore, the objective function of introduction attacks is represented as follows:
\begin{equation}
\begin{array}{l}
\text{MAX}\sum\limits_{m \in M_I} {\left( {(\mathbb{I}[{h_c}^{(m)}(x + \delta )-\gamma] - \mathbb{I}[{h_c}^{(m)}(x)-\gamma]} \right)} \\
\text{s.t.}\quad\text{argmax}\enspace{h_y}\left( {{h_c}\left( {x + \delta } \right)} \right) = \text{argmax}\enspace{h_y}\left( {{h_c}\left( x \right)} \right)
,
\end{array}
\end{equation}
where $M_I$ denotes the set of concepts that the attacker wishes to introduce.

\paragraph{Confounding Attacks.}
%Confounding Attacks是一种相对更。
Confounding attacks build on introduction attacks and erasure attacks.The confounding attack simultaneously removes relevant concepts and introduces irrelevant concepts.
The confounding attack is a more powerful attack than the erasure attack and the introduction attack as it allows arbitrary tampering with the concept set of the original sample.
The objective function of confounding attacks is as follows:
\begin{equation}
\begin{array}{l}
\text{MAX}\sum\limits_{m \in M_E} {\left( {(\mathbb{I}[\gamma - {h_c}^{(m)}(x + \delta )] - \mathbb{I}[\gamma - {h_c}^{(m)}(x)]} \right)}\\
+\sum\limits_{m \in M_I} {\left( {(\mathbb{I}[{h_c}^{(m)}(x + \delta )-\gamma] - \mathbb{I}[{h_c}^{(m)}(x)-\gamma]} \right)} \\
\text{s.t.}\quad\text{argmax}\enspace{h_y}\left( {{h_c}\left( {x + \delta } \right)} \right) = \text{argmax}\enspace{h_y}\left( {{h_c}\left( x \right)} \right)
.
\end{array}
\end{equation}

In this paper, confounding attacks are utilized to disrupt AGAIN, which contributes to a more complete evaluation on the comprehensibility of the explanations generated by AGAIN when concepts are missing and confused.
%与Erasure Attacks不同,Introduction Attacks需要将不相关概念的激活值提升到阈值以上。因此，Introduction Attacks的目标函数如下：


%Let $M$ denote a set of concepts that the attacker wishes to remove. To remove the existence of these concepts, the attacker's goal is as follows:
%
%
% \subsection{Comparing search-based and learning-based methods}
% \begin{table}[h]\scriptsize
%   \centering
%   \vspace{-1em}
%   \caption{Comparing search-based and learning-based methods.}
%   \renewcommand\arraystretch{1.0}
%     \begin{tabularx}{\linewidth}
%     {>{\centering\arraybackslash}m{0.80cm}
%     >{\centering\arraybackslash}m{0.50cm}
%     >{\centering\arraybackslash}m{0.80cm}
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}|
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}
%     >{\centering\arraybackslash}m{0.90cm}}
%     \toprule
%     \multirow{2}[2]{*}{Method} & \multirow{2}[2]{*}{Metrics} & \multicolumn{1}{c}{\multirow{2}[2]{*}{Clear}} & \multicolumn{4}{c|}{$\delta_k$}         & \multicolumn{4}{c}{$\delta_u$} \\
% \cmidrule{4-11}          &       &       & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32    & $\epsilon$=4     & $\epsilon$=8     & $\epsilon$=16    & $\epsilon$=32 \\
%     \midrule
%     \multirow{2}[1]{*}{\makecell{Learning\\-based}
% } & LSM & 96.3(2.4)  & 90.3(7.4) & 86.6(12.5) & 79.3(14.5) & 74.3(17.5) & 90.3(7.5) & 85.7(9.5) & 82.3(14.5) & 75.3(17.1) \\
%           & Time & 43(0.6) & 44(0.8)  & 43(0.7) & 42(1.2) & 45(1.4) & 43(0.9) & 45(1.4)  & 44(1.3) & 43(1.2) \\
%     \midrule
%     \multirow{2}[1]{*}{\makecell{Search\\-based}} & LSM & 96.3(2.4) & 92.4(1.2) & 93.1(1.2) & 93.8(0.9)  & 91.5(2.3) & 94.5(1.3) & 93.3(1.0) & 93.8(1.4) & 92.1(1.1) \\
%           & Time & 67(4.5) & 125(13.7) & 261(27.4) & 434(21.1) & 606(13.4) & 126(11.8) & 264(24.1) & 429(26.5) & 613(22.3) \\
%     \bottomrule
%     \end{tabularx}%
%   \label{33Q7}%
%   \vspace{-1em}
% \end{table}%

\end{document}
