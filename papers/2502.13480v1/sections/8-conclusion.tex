\section{Conclusion}
\sysname proposes an automated parallelization strategy search framework that can optimize both training time and financial costs for the training of large-scale language models. In the experiment, \sysname was tested on the Llama series models (7B, 13B, 70B) and a variety of GPU numbers (32 to 4096). The results showed that its automatically generated parallel strategies were comparable to or even better than the strategies manually tuned by experts in terms of throughput. Especially under large-scale GPU clusters, \sysname's hybrid parallel strategy showed better scalability and efficiency. In addition, in the financial cost optimization experiment, \sysname was able to balance computing performance and cost through a variety of GPU configurations (such as H100, A800, etc.), providing targeted solutions for users with limited budgets. Overall, \sysname greatly reduces the complexity of parallel strategy tuning and provides strong support for efficient training of large-scale models in the future.

%In this paper, we presented \sysname, an innovative framework designed to automate the parallelization of large-scale Transformer models. The increasing complexity and scale of these models necessitate efficient distributed training strategies that are beyond the reach of manual optimization. \sysname addresses this need by systematically exploring a vast space of parallelization strategies, optimizing training time and financial cost.

%Our approach integrates data, tensor, and pipeline parallelism into a cohesive framework that can adapt to various model architectures and hardware configurations. By leveraging advanced algorithms and heuristics, \sysname provides a scalable solution that reduces the reliance on expert knowledge and manual tuning. This makes optimizing parallelization strategies more accessible to a broader range of users and applications.

%The comprehensive evaluation of \sysname demonstrated its effectiveness in optimizing training throughput and cost-efficiency across different models and GPU configurations. Compared to existing frameworks like Alpa and Galvatron, \sysname showed superior performance in finding optimal parallel strategies, reducing both the computational burden and financial costs of training large models.

%\sysname's ability to adapt to dynamic and heterogeneous GPU clusters further enhances its practicality for real-world deployments. By providing automated recommendations and detailed performance metrics, the framework aids users in making informed decisions about their training configurations.