\section{Related Work}\label{sec:moti}

In this section, we discuss the related works of \sysname.
First, we first discuss current large-scale distributed deep learning model training in \S\ref{sec:moti:large}.
Then, we introduce different parallel methods and the large search space of hybrid parallel in \S\ref{sec:moti:parallel}, which drives the need to build auto parallel for large language model training.
%Next, we discuss the applications of auto-parallelism in real-world situations and reveal how existing auto-parallel methods fail to cope with them in \S\ref{sec:moti:scene}.
%Finally, we propose our design goals for \sysname\ in \S\ref{sec:moti:goal} based on these motivations.

\subsection{Large-scale Distributed DL Training}\label{sec:moti:large}

\par In recent years, large-scale Transformer models have demonstrated exceptional performance across various application domains, pushing the frontiers of deep learning research. For instance, models like BERT \cite{devlin2018bert}, GPT-3 \cite{achiam2023gpt}, and T5 \cite{ni2021sentence} have achieved state-of-the-art results in natural language processing tasks such as machine translation, text classification, and question answering. These advancements underscore the capabilities of large-scale models in capturing complex patterns and generating high-quality outputs that surpass those of smaller models.

\par However, the sheer size of these models poses significant computational challenges. GPT-3, for example, consists of 175 billion parameters \cite{achiam2023gpt}, making it impractical to train on a single GPU due to memory and processing constraints \cite{hoffmann2022empirical}. Single-GPU setups cannot handle the extensive computations and data storage required by such massive models \cite{hoffmann2022empirical}. As model sizes increase, the demand for computational power and memory scales exponentially, further exacerbating the limitations of single-GPU environments \cite{geiping2023cramming}.

\par To address these challenges, the training of large-scale Transformer models necessitates the use of distributed training across multiple GPUs \cite{narayanan2021efficient, li2020pytorch, pipedream}. This approach leverages several GPUs' combined computational power and memory capacity, enabling the efficient handling of large models. 

%\begin{CJK}{UTF8}{gbsn}
%\begin{itemize}
%    \item Q：为什么需要大规模性并行？
%    \item A1：大规模性transformer模型的能力比较好
%    \item A2：大规模性transformer没办法用一张卡跑
%    \item A3：大规模性transformer需要大量GPU并行
%\end{itemize}
%\end{CJK}

\subsection{Different Parallel Methods}\label{sec:moti:parallel}

\par Existing parallel methods for training large-scale models include tensor parallelism \cite{narayanan2021efficient}, data parallelism \cite{hillis1986data, li2020pytorch, agarwal2012reoptimizing}, and pipeline parallelism \cite{li2021terapipe, kim2023bpipe, yang2021pipemare, narayanan2021memory, zhao2021v}, each offering distinct advantages and catering to different aspects of the training process (see Fig. \ref{fig:parallel}). 

\sssec{Tensor parallelism} involves partitioning the model's tensors (such as weight matrices) across multiple GPUs, allowing parallel computation of operations like matrix multiplications. 
This approach effectively reduces the memory footprint on each GPU by distributing the model parameters and computations, thereby accommodating larger models that would otherwise exceed the memory capacity of a single GPU \cite{narayanan2021efficient}.

\sssec{Data parallelism}, on the other hand, splits the training data across multiple GPUs \cite{hillis1986data, li2020pytorch, agarwal2012reoptimizing}. Each GPU maintains a complete copy of the model and processes a subset of the data. The gradients calculated by each GPU are then averaged and synchronized across all GPUs to ensure consistent model updates. This method is particularly effective in leveraging the computational power of multiple GPUs to handle large datasets and improve training speed. 

\sssec{Pipeline parallelism} divides the model into sequential stages, each assigned to a different GPU \cite{li2021terapipe, kim2023bpipe, yang2021pipemare, narayanan2021memory, zhao2021v}. As data flows through the pipeline, each GPU processes its assigned stage sequentially. This approach enables efficient utilization of GPU resources by maintaining continuous data flow and minimizing idle times, especially for intense models with numerous layers.

\sssec{Manual hybrid parallelism}. In practice, experts often combine these parallelism techniques to create hybrid parallelism strategies tailored to the specific requirements of their models and hardware configurations \cite{song2019hypar}. 
This expert-crafted hybrid parallelism involves significant manual effort and domain knowledge to balance the trade-offs between computation, memory usage, and communication overheads. 
For instance, systems like DeepSpeed \cite{rasley2020deepspeed} and Megatron-LM \cite{narayanan2021efficient} employ a mix of data parallelism, tensor parallelism, and pipeline parallelism to optimize the training of massive models like GPT-3 \cite{achiam2023gpt}.

\sssec{Automatic parallelism}. However, manual hybrid parallelism has its limitations. The complexity and diversity of modern deep learning models and hardware environments make it increasingly difficult for experts to design optimal parallelism strategies. 
This is where automatic parallelism comes into play. Automatic parallelism systems, such as Alpa \cite{zheng2022alpa}, Metis \cite{um2024metis}, and Galvatron \cite{miao2022galvatron}, automate finding efficient parallelism plans by exploring a vast search space of potential strategies using advanced algorithms and heuristics.

%\par Automatic parallelism outperforms expert-designed strategies in several ways. First, it can systematically explore \textbf{a broader range of parallelism combinations and configurations}, potentially discovering novel and more efficient strategies that human experts might overlook. Second, it \textbf{reduces the need for extensive manual tuning and domain-specific expertise}, making it accessible to a broader range of users and applications. Lastly, automatic parallelism can \textbf{adapt to modern GPU clusters' dynamic and heterogeneous nature}, optimizing resource utilization and real-time performance.


%\begin{CJK}{UTF8}{gbsn}
%\begin{itemize}
%    \item Q：为什么需要自动并行？
%    \item A1：现有的并行方法介绍:张量并行等
%    \item A2:手工并行介绍（介绍常用专家并行方案）
%    \item A3:手工专家并行为什么不行？
%    \item A4:自动并行是什么？为什么比专家并行好
%\end{itemize}
%\end{CJK}

%\begin{figure}[htb!]
%\centering
%\includegraphics[width=0.48\textwidth]{figs/AutoParallel-Architecture.pdf}
%\caption{
%In real world scene, 
%}
%\label{fig:motivation}
%\end{figure}

%\subsection{Background: Real-world scene}\label{sec:moti:scene}

%\par Previous auto-parallel systems like Alpa, while pioneering in automating parallelism, face several limitations that make them less practical in real-world scenarios. 
%Alpa operates at the operator level for parallelism, significantly increasing the search time for optimal strategies and making it computationally expensive and time-consuming in practical settings \cite{miao2022galvatron}. 
%Moreover, operator-level parallelism often \textbf{fails to support the latest optimization features} during deployment due to its granular nature. 
%Moreover, considering the large cost of supporting the operator-level parallelism without robust community support, current cloud GPU providers often choose traditional but more robust PipeDream, MegatronLM frameworks, etc. instead. 
%Additionally, the performance gains achieved by optimizing at the operator level are often marginal compared to layer-level optimizations, resulting in limited overall improvements \cite{miao2022galvatron}.

%\par Furthermore, previous methods ignore the need to search for the optimal GPU configurations for users. Previous automatic parallelism works have focused on finding the optimal strategy for a given network and hardware configuration (i.e., specific number and type of GPUs). However, in real-world scenarios, especially when using cloud service providers, users often have a variety of GPU models and quantities to choose from. The hardware configuration is not fixed, and users need a system that can adapt to these configurations to provide the best possible performance.

%\par Beyond computational efficiency, financial cost is a significant concern for users, particularly startups and other resource constrained entities. In such cases, the financial cost of the training plan is more critical than the time cost. Users may prioritize strategies that minimize expenditure over those that merely reduce training time. Therefore, an effective parallelism system should consider not only the computational efficiency but also the financial implications of different parallelism strategies, providing cost-effective solutions that cater to a diverse range of users and use cases.

%\subsection{Design Goals}\label{sec:moti:goal}

%\sssec{Time-optimized search}. Implement an efficient and comprehensive search algorithm to identify the optimal parallelization strategy that minimizes the total training time. This includes exploring various combinations of data parallelism, tensor parallelism, and pipeline parallelism. The search algorithm should systematically explore the vast search space of potential parallel strategies to find the most time-efficient configuration, leveraging advanced algorithms and heuristics.

%\sssec{Scalable deployment}. Ensure the framework is robust and capable of handling large-scale training in real-world scenarios without facing deployment issues related to operator parallelism. This involves addressing the complexity of manual hybrid parallelism and automating the generation of parallelization strategies to reduce the burden on experts. The system should be adaptable to various model architectures and hardware configurations, making it feasible to deploy across different environments without encountering infeasible operator parallelism scenarios.

%\sssec{Support for GPU configuration and financial cost search}.
%Enable the framework to support the search for optimal GPU configurations, including the type and number of GPUs required for efficient training. This involves integrating a cost model that accounts for different GPU types and their associated costs.
%Allow users to set financial constraints as part of the optimization process. Users can specify a budget limit, and the framework will search for parallelization strategies that meet this constraint while optimizing for other objectives.
%Provide the capability to use financial cost as an optimization target. The system should be able to identify the most cost-effective parallelization strategy that minimizes financial expenditure while maintaining acceptable training performance.

%\sssec{Comprehensive performance metrics}.
%Implement robust evaluation metrics to assess the performance of different parallelization strategies. This includes measuring training throughput, computational efficiency, memory usage, and financial cost.
%Ensure the framework provides detailed feedback and performance metrics to help users understand the trade-offs and benefits of different parallelization strategies, aiding in informed decision-making.

%\par By addressing these design goals, \sysname aims to provide a comprehensive and automated solution for optimizing the parallelization of large-scale Transformer models, balancing the trade-offs between time, cost, and resource utilization.