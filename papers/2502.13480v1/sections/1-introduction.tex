\section{Introduction}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.48\textwidth]{figs/Fig-parallel_methods.pdf}
\caption{
Different parallel methods have been proposed: b) tensor parallelism, c) data parallelism, d) pipeline parallelism, etc.
Furthermore, a new parallelism paradigm, hybrid parallelism, has been primarily applied in real-world applications that combines current parallelism methods.
}
\label{fig:parallel}
\end{figure}

The large language models like GPT-4 \cite{achiam2023gpt}, Llama \cite{touvron2023llama}, Llama2 \cite{touvron2023llama2}, Mistral \cite{jiang2023mistral}, etc., are consisted of billions of parameters and has achieved remarkable capabilities in natural language processing and other domains.
Training such large models requires the parallel computing of numerous GPUs, and numerous GPU parallel training methods have emerged, such as data parallel, tensor parallel, pipeline parallel, etc.
These methods have different advantages and disadvantages, for instance, tensor parallel saves memory consumption but raises the training latency.
There is not a single parallel method that is better than other parallel methods at each scene.


Thus, in real-world practice, LLM developers mainly combine those parallel methods into a hybrid parallel strategy to satisfy their needs.
Many industry developer teams rely on human experts to manually design hybrid parallel strategies based on their experience.
Other than relying on manual expert-designed hybrid parallel strategies, previous researchers have also proposed \textbf{automatical parallelization}, such as Alpa\cite{zheng2022alpa}, Aceso \cite{liu2024aceso} and Galvatron\cite{miao2022galvatron}, which automatically searches throughput-optimal hybrid parallel strategies based on cost models and optimization algorithms. 

However, previous methods still leave several realistic and important questions to answer.
Now, assume you are a developer of a GPU training cloud service:

\sssec{RQ 1-Large search space}: The GPU training cloud platform will have many customers with different needs.
Different customers have different requirements for model type, model scale, GPU scale (tens of cards to tens of thousands of cards), and GPU type.
Most customers of the GPU training cloud platform are novice users (otherwise they would build their private cloud).
When designing training strategies, they need a fast, accurate, and low-cost tool to obtain the best GPU configuration (GPU number) and the corresponding splitting parameters to guide training.
However, previous methods only focus on the scenes of fixed GPU numbers and GPU types.

\sssec{RQ 2-Heterogeneous GPUs}:
In pursuit of extreme cost, many customers of cloud platform providers purchase different types of GPUs for heterogeneous training.
The current method focuses on the scenario of a single GPU type and cannot solve the parallel strategy generation in this heterogeneous GPU heterogeneous training scenario.

\sssec{RQ 3-Money saving}:
In addition to focusing on training efficiency, cloud providers' customers may also be sensitive to the cost of model training.
For example, when a startup wants to train a large model for business, it often wants a more cost-effective training strategy due to initial funding constraints, rather than a training strategy with the best training efficiency.
The current solution only supports searching for the strategy with the best training efficiency.


To solve the above problems, we designed and implemented \sysname. \sysname supports three search modes.
First, homogeneous mode, specify a single GPU type and GPU number, \sysname can search for the optimal strategy.
Second, heterogeneous mode, in the heterogeneous training scenario of mixing different types of GPUs, we are the first to completely solve the simulation of a heterogeneous-GPU training scene, and build mathematical modeling and theoretical problems in heterogeneous-GPU scenarios, which can also search for the optimal strategy of heterogeneous GPUs.
Third, cost mode, with specifying the maximum number of GPUs and maximum money limit, \sysname also supports searching for the most efficient one.
Finally, the performance simulation efficiency of \sysname is very fast, which is 98.7\% lower than that of industry competitors. The performance simulation accuracy of \sysname is more than 95\%, which is better than expert strategies in most scenarios. \sysname has a low computational cost, it can run locally without high computing power need, and can search for the best strategy in 1.27 seconds for a single-GPU setting and less than 1.35 minutes for a heterogeneous-GPU setting on average with an accuracy of over 95\%.


% 1. Scale of deep learning models increase...
%Deep learning has witnessed unprecedented growth in recent years, particularly with the advent of large language models (LLMs). 
%Notable examples include models like GPT-4 \cite{achiam2023gpt}, Llama \cite{touvron2023llama}, Llama2 \cite{touvron2023llama2}, Mistral \cite{jiang2023mistral}, etc., which boast hundreds of billions of parameters and are trained on large datasets. 
%This increase in scale has enabled these models to achieve remarkable capabilities, pushing the boundaries of what is possible in natural language processing and other domains.

%2. Challenges brought to the large scale model training. Require experts to find the optimal hybrid parallel plans on their own

%Training such large-scale models poses substantial challenges. 
%Due to the large scale of the model's parameters, a single GPU can not support its training alone.
%Thus, numerous GPU parallel training methods have emerged, such as data parallel, operator parallel, pipeline parallel, etc.
%In real-world practice, LLM developers mainly combine those parallel methods into a hybrid parallel policy, which satisfies their needs.
%Experts are required to meticulously design and implement hybrid parallel plans that balance data, operator, and pipeline parallelism. 
%This intricate task demands a deep understanding of the model architecture and the underlying hardware, making it a complex and time-consuming endeavor.

%3. Tradeoffs of experts-based tuning

%However, relying on expert-based tuning presents several trade-offs. 
%While experienced engineers can develop highly optimized parallelization strategies, this approach is not scalable. 
%It often involves a lengthy trial-and-error process, where each new model or hardware configuration necessitates fresh optimizations. 
%Furthermore, the manual nature of this process limits its accessibility, as only a small number of individuals possess the requisite expertise to perform these optimizations effectively.
%According to our study, when it comes to a new model and related training requirements, an expert may need nearly 5-7 days to find an optimal parallel method that shows optimal training throughput.
%\begin{CJK}{UTF8}{gbsn}这里加一组数字，一个专家得出最优搜索方案大概要多久\end{CJK}

%4. How complex the parallel plans are

%The complexity of creating efficient parallel plans for large-scale models cannot be overstated. Practical training requires a nuanced combination of various parallelism strategies. Data parallelism, tensor, and pipeline parallelism have distinct memory consumption, communication overheads, and execution efficiency. Balancing these aspects to achieve optimal performance involves navigating an enormous search space, which becomes exponentially more challenging with increasing model and hardware scales.
%For instance, given a Llama-70B model with a set of 64 GPUs, we can have $9\times 10^3$ plans to search.
%\begin{CJK}{UTF8}{gbsn}这里加一组数字，比如搜索的plan数量能有多少\end{CJK}

%5. The need to auto parallel

%Given the inherent challenges and limitations of manual parallelization, there is a pressing need for automated solutions. Automated parallelization can significantly reduce the burden on experts, enabling them to focus on higher-level tasks such as model design and interpretation. By automating the generation of parallelization strategies, we can advance deep-learning techniques, accelerate innovation, and reduce the time and cost associated with training large-scale models.

%6. Previous auto parallel methods and its drawbacks

%Previous efforts to automate parallelization, such as Alpa and Galvatron, have made significant strides but also come with limitations. Alpa introduced a hierarchical approach to parallelism, combining intra-operator and inter-operator parallelism. While effective, this method can be computationally expensive and struggles to adapt to the latest optimization techniques. On the other hand, Galvatron focuses on hybrid parallelism, utilizing decision trees and dynamic programming to optimize parallel strategies. Despite its advancements, Galvatron’s approach can become overly complex with larger models and more GPUs, and its reliance on precise cost estimations can sometimes lead to suboptimal performance. Meanwhile, previous methods only focused on finding the optimal parallel plan for given models and devices, ignoring that real-world clients have a vast space for device choices. Meanwhile, they only consider the time cost of the parallel plans, ignoring some people or organizations' potential need to lower the financial cost of the training.

%7. The proposed AutoParallel

%To address the real-world scenario where users only provide the architecture of the deep learning (DL) network to be trained, we propose AutoParallel. AutoParallel allows users to specify only the DL network architecture for training, which fully automates the search for the optimal parallelization strategy tailored to the user’s needs (either time-optimized or cost-optimized). This parallelization strategy includes selecting the type and number of GPUs and the method for distributed training across these GPUs. AutoParallel builds upon the MegatronLM framework to perform this search.