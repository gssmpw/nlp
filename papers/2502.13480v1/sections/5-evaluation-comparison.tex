\subsection{Comparison with Other Schemes}\label{sec:exp:comparison}

\begin{table}[h!]
\centering
\caption{GPT-3 Model Specification}
\label{tab:gpt-3}
\begin{tabular}{ccccc}
\hline
\#params & Hidden size & \#layers & \#heads & \#gpus \\ \hline\hline
350M & 1024 & 24 & 16 & 1 \\ 
1.3B & 2048 & 24 & 32 & 4 \\ 
2.6B & 2560 & 32 & 32 & 8 \\ 
6.7B & 4096 & 32 & 32 & 16 \\ 
15B & 5120 & 48 & 32 & 32 \\ 
39B & 8192 & 48 & 64 & 64 \\ \hline\hline
\end{tabular}
\end{table}


\begin{table}[h!]
\centering
\caption{LLaMA Model Specification}
\label{tab:llama}
\begin{tabular}{ccccc}
\hline
\#params & Hidden size & \#layers & \#heads & \#gpus \\ \hline\hline
7B & 4096 & 32 & 32 & 8 \\
13B & 5120 & 40 & 40 & 16 \\
33B & 6656 & 60 & 52 & 32 \\
70B & 8192 & 80 & 64 & 64 \\ \hline\hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{GShard MoE Model Specification}
\label{tab:moe}
\begin{tabular}{cccccc}
\hline
\#params & Hidden size & \#layers & \#heads & \#experts & \#gpus \\ \hline\hline
380M & 768 & 8 & 16 & 8 & 1 \\
1.3B & 768 & 16 & 16 & 16 & 4 \\
2.4B & 1024 & 16 & 16 & 16 & 8 \\
10B & 1536 & 16 & 16 & 32 & 16 \\
27B & 2048 & 16 & 32 & 48 & 32 \\
70B & 2048 & 32 & 32 & 64 & 64 \\ \hline\hline
\end{tabular}
\end{table}

\sssec{Models and training workflows}.
For our experiments, we target three types of models: GPT-3, LLaMA, and a Mixture of Experts (MoE) model. These models represent a range of architectures, from homogeneous to heterogeneous, providing a comprehensive evaluation of our parallelism strategies. 

\par \textbf{GPT-3} (see Table \ref{tab:gpt-3}) is a homogeneous Transformer-based language model comprising many stacked layers. Its model parallelization plan has been extensively studied and optimized in various research efforts. \textbf{LLaMA} (see Table \ref{tab:llama}) is another advanced Transformer-based model designed for language modeling, with a focus on efficiency and performance in both pre-training and fine-tuning phases. \textbf{MoE} models (see Table \ref{tab:moe}), such as GShard, combine dense and sparse architectures by incorporating a mixture of expert layers. These layers replace the feed-forward layers in every few Transformer layers, making them highly adaptable to different computational environments.

\par To study the scalability and efficiency of training large models, we follow standard machine learning practices by scaling the model size proportionally with the number of GPUs, as reported in Table 4. For GPT-3, we increase the hidden size and the number of layers concurrently with the number of GPUs, following the methodology used in previous studies. For the MoE model, we primarily increase the number of experts, which is crucial for leveraging the model's sparse architecture and optimizing performance across multiple GPUs. For LLaMA, we adjust the model's depth (number of layers) and width (hidden size) to ensure it scales effectively with the available GPU resources.

\par In each experiment, we adopt the recommended global batch size per established ML practices to maintain consistent statistical behavior across different model configurations. We then fine-tune the micro-batch size for each model and system configuration to maximize overall system performance, with gradient accumulation applied across micro-batches.

\sssec{Baselines}. For each model, we compare our system, \sysname, against strong baselines, including Alpa and Galvatron, and manually designed strategies using Megatron-LM.

\par \textbf{Alpa} is chosen as one of the baselines due to its automated parallelization capabilities, particularly for large-scale models. Alpa utilizes a combination of intra-operator and inter-operator parallelism to optimize the training process. We configure Alpa to its best settings by following the guidelines provided in their documentation and research papers. Alpa is known for its comprehensive strategy space, which includes various parallelism paradigms such as data parallelism, tensor parallelism, and pipeline parallelism.

\par \textbf{Galvatron} is another baseline we employ, noted for its efficient transformer training over multiple GPUs using automatic parallelism. Galvatron incorporates multiple popular parallelism dimensions and automatically discovers the most efficient hybrid parallelism strategy through a decision tree decomposition and a dynamic programming search algorithm. We perform a grid search to determine the optimal configurations for Galvatron, ensuring that we fully leverage its capabilities.

\par \textbf{Megatron-LM} serves as the manually designed baseline, specifically for GPT-like models. Megatron-LM v2 is a state-of-the-art system that combines data parallelism, pipeline parallelism, and manually designed operator parallelism (denoted as TMP). This combination is controlled by three integer parameters that specify the degrees of parallelism assigned to each technique. Following the guidance from their research, we conduct a thorough grid search of these parameters and report the best configuration results. While Megatron-LM is highly specialized for GPT-like models, it does not support other models in our evaluation due to its lack of flexibility in handling different architectures.

Our comparison does not include open-source systems like \textbf{FlexFlow} and \textbf{Tofu} due to their limitations. FlexFlow lacks support for essential operators such as layer normalization and mixed-precision operators, and Tofu only supports single-node execution and is not open-sourced. Given these theoretical and practical constraints, we do not expect FlexFlow or Tofu to outperform the state-of-the-art manual baselines in our evaluation.

In summary, our evaluation includes \sysname, Alpa for its automated strategy space, Galvatron for its efficient hybrid parallelism discovery, and manually tuned Megatron-LM for its specialization in GPT-like models. This comprehensive approach thoroughly compares different parallelism strategies and model architectures.

\sssec{Evaluation metrics}. We measure training throughput in our evaluation. We evaluate the system's weak scaling when increasing the model size and the number of GPUs. Following \cite{narayanan2021efficient}, we use the aggregated peta floating-point operations per second (PFLOPS) of the whole cluster as the metric. After proper warmup, we measure it by running a few batches with dummy data. All our results (including those in later sections) have a standard deviation within 0.5\%, so we skip the error bars in our figures.

\sssec{GPT-3 results}.
\textcolor{red}{To be done}

\sssec{Llama results}.
\textcolor{red}{To be done}

\sssec{MoE results}.
\textcolor{red}{To be done}