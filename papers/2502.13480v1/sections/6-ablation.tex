\section{Ablation Analysis}\label{sec:ablation}

In this section, we study the impact of different parallel techniques on performance.

\subsection{Experiment Setting}

We conducted experiments using the Llama-2 model in three sizes: 7B, 13B, and 70B, to evaluate the impact of various parallelism strategies and memory management techniques. The experiments were executed on GPU clusters composed of NVIDIA A800 GPUs, with configurations ranging from 64 to 4096 GPUs. We employed a hybrid parallelism strategy (data parallelism + tensor parallelism) unless otherwise specified. Four key techniques were analyzed: parallelism strategies, system scale, memory offloading, recomputation, and overlap strategies.

We tested the following setups for each technique:
\begin{itemize}[noitemsep,topsep=1pt, leftmargin=*]
    \item \textbf{Parallelism Strategy}: We tested data parallelism (DP), tensor parallelism (TP), pipeline parallelism (PP), and hybrid parallelism strategies across different GPU counts and model sizes.
    \item \textbf{System Scale}: We varied the number of GPUs (64, 128, 256, 1024, and 4096) while keeping the model architecture fixed to isolate the impact of system scale on training efficiency.
    \item \textbf{Memory Offloading}: We tested configurations with no offloading, enable offloading, and memory bandwidth variations (DDR4 vs. DDR5).
    %\item \textbf{Recomputation}: We tested no recomputation, selective recomputation (recomputing specific layers), full recomputation, and hybrid recomputation.
    \item \textbf{Overlap Strategy}: We tested no overlap, gradient communication overlap, parameter gather overlap, and combined overlap strategies.
\end{itemize}

\begin{figure*}[htbp]
  \centering
    \subfloat[Llama-2-7B]{\includegraphics[width=0.33\textwidth]{figs/fig-dp-Llama-2-7B.pdf}}
    %\\
    \subfloat[Llama-2-13B]{\includegraphics[width=0.33\textwidth]{figs/fig-dp-Llama-2-13B.pdf}}
    %\\
    \subfloat[Llama-3-8B]{\includegraphics[width=0.33\textwidth]{figs/fig-dp-Llama-3-8B.pdf}}
  \caption{
  We compare \sysname's performance with all parallelism methods allowed and only data parallelism allowed.
  }
  \label{fig:ablation:dp}
\end{figure*}

\begin{figure*}[htbp]
  \centering
    \subfloat[Llama-2]{\includegraphics[width=0.33\textwidth]{figs/fig-scale-Llama-2.pdf}}
    %\\
    \subfloat[Llama-3]{\includegraphics[width=0.33\textwidth]{figs/fig-scale-Llama-3.pdf}}
    %\\
    \subfloat[GLM]{\includegraphics[width=0.33\textwidth]{figs/fig-scale-GLM.pdf}}
  \caption{
  We understand the system scale impact on training efficiency by training \sysname\ on different GPU numbers.
  }
  \label{fig:ablation:scale}
\end{figure*}

\sssec{Metrics}
We utilized three key metrics:
\begin{itemize}[noitemsep,topsep=1pt, leftmargin=*]
    \item \textbf{Training Throughput (samples/second)}: The primary metric for evaluating training efficiency, measuring the number of samples processed per second.
    \item \textbf{Scaling Efficiency (\%)}: This metric compares the achieved throughput at each scale to the ideal linear scaling expected as the number of GPUs increases.
    \item \textbf{Communication Overhead}: We measured the data transferred between GPUs to quantify the additional cost incurred by increasing the system size, focusing on inter-GPU and inter-node communication costs.
\end{itemize}

\sssec{Statistical Validation}
All experiments were run for a fixed number of iterations to ensure that the strategies had sufficient time to reach stable performance metrics. Each experiment was repeated three times to account for variability, and the results are reported as averages with standard deviations.

\subsection{Parallelism Strategy In-Depth Analysis}

\sssec{Method}. We analyzed the impact of different parallelism strategies (DP, TP, PP, and hybrid) on training performance across different GPU counts and model sizes. We monitored trade-offs between communication overhead and computation efficiency and identified cases where hybrid parallelism yielded improvements over single-strategy configurations.

\sssec{Results}.
Figure \ref{fig:ablation:dp} compares \sysnameâ€™s performance with all parallelism methods enabled against data parallelism (DP) alone across different system scales for Llama-2-7B, Llama-2-13B, and Llama-3-8B models. In all cases, \sysname consistently outperforms DP as the GPU count increases. The results show that \sysname maintains higher throughput by combining multiple parallelism strategies, effectively mitigating the communication overhead that causes DP's performance to degrade at larger scales. This demonstrates \sysname's superior scalability and adaptability, highlighting the benefits of hybrid parallelism over relying solely on data parallelism.

\subsection{System Scale Impact on Training Efficiency}

\sssec{Method}.
We explored the relationship between system scale (i.e., the number of GPUs) and training efficiency by analyzing changes in training throughput and communication overhead as the number of GPUs increased while keeping the model architecture fixed.

\sssec{Results}.
The results presented in Figure \ref{fig:ablation:scale} demonstrate a clear trend of diminishing per-GPU throughput as GPUs increase for each model (Llama-2, Llama-3, and GLM). For smaller models such as Llama-2 7B and 13B, the decrease in throughput per GPU is gradual, indicating that these models scale more efficiently with the number of GPUs. However, for larger models such as Llama-2 70B and GLM 130B, the throughput drops significantly as the GPU count exceeds 512, suggesting that these models experience higher communication overhead and resource contention at larger scales. 
This decline in efficiency highlights the limitations of scaling large models across many GPUs. As the system grows, the communication cost between GPUs begins to outweigh the computational benefits, leading to suboptimal utilization of the hardware resources. The sharp decrease in throughput for models like Llama-2 70B and GLM 130B at 1024 GPUs underscores the need for careful consideration of parallelization strategies to mitigate the impact of inter-GPU communication on training performance. These findings are consistent with previous observations in distributed training, where scaling efficiency degrades as the system size increases due to increased synchronization and data transfer costs.

\begin{figure}[htbp]
  \centering
    \subfloat{\includegraphics[width=0.4\textwidth]{figs/fig-offload-legend.pdf}}\\
    \addtocounter{subfigure}{-1}
    
    \subfloat[Llama-2-7B]{\includegraphics[width=0.16\textwidth]{figs/fig-offload-Llama-2-7B.pdf}}
    \subfloat[Llama-2-13B]{\includegraphics[width=0.16\textwidth]{figs/fig-offload-Llama-2-13B.pdf}}
    \subfloat[Llama-2-70B]{\includegraphics[width=0.16\textwidth]{figs/fig-offload-Llama-2-70B.pdf}}
    \\
    \subfloat[Llama-3-8B]{\includegraphics[width=0.24\textwidth]{figs/fig-offload-Llama-3-8B.pdf}}
    \subfloat[Llama-3-70B]{\includegraphics[width=0.24\textwidth]{figs/fig-offload-Llama-3-70B.pdf}}
    \\
    \subfloat[GLM-67B]{\includegraphics[width=0.24\textwidth]{figs/fig-offload-Llama-GLM-67B.pdf}}
    \subfloat[GLM-130B]{\includegraphics[width=0.24\textwidth]{figs/fig-offload-Llama-GLM-130B.pdf}}
  \caption{
  We compare \sysname's performance with offload allowed with unallowed
  }
  \label{fig:ablation:offload}
\end{figure}

\subsection{Memory Offloading Technique Analysis}

\sssec{Method}. We evaluated the effectiveness of memory offloading techniques, comparing disable/enable offloading, and the impact of different memory bandwidths on training performance, particularly in memory-constrained environments.

\sssec{Results}.
Figure \ref{fig:ablation:offload} compares \sysname's performance with and without memory offloading across different models and system scales. The results show that memory offloading becomes increasingly important as model size grow. For smaller models like Llama-2-7B and Llama-2-13B, the performance impact of offloading is minimal, but as models scale up (e.g., Llama-70B and GLM-130B), enabling offloading significantly improves throughput by alleviating memory bottlenecks. Without offloading, larger models experience sharp performance declines as GPU count increases. These findings highlight the critical role of memory offloading in maintaining efficient scaling for larger models across large GPU configurations.

%\subsection{Recomputation Technique Analysis}

%We analyzed the effects of recomputation techniques on training efficiency, focusing on trade-offs between memory savings and computational overhead across different recomputation configurations: no recomputation, selective recomputation, full recomputation, and hybrid recomputation.

%\sssec{Results}
%\textcolor{red}{@Haibin, please fill in.}

\begin{figure}[htbp]
  \centering
    \subfloat{\includegraphics[width=0.4\textwidth]{figs/fig-overlap-legend.pdf}}\\
    \addtocounter{subfigure}{-1}
    
    \subfloat[Llama-2-7B]{\includegraphics[width=0.16\textwidth]{figs/fig-overlap-Llama-2-7B.pdf}}
    \subfloat[Llama-2-13B]{\includegraphics[width=0.16\textwidth]{figs/fig-overlap-Llama-2-13B.pdf}}
    \subfloat[Llama-2-70B]{\includegraphics[width=0.16\textwidth]{figs/fig-overlap-Llama-2-70B.pdf}}
    \\
    \subfloat[Llama-3-8B]{\includegraphics[width=0.24\textwidth]{figs/fig-overlap-Llama-3-8B.pdf}}
    \subfloat[Llama-3-70B]{\includegraphics[width=0.24\textwidth]{figs/fig-overlap-Llama-3-70B.pdf}}
    \\
    \subfloat[GLM-67B]{\includegraphics[width=0.24\textwidth]{figs/fig-overlap-Llama-GLM-67B.pdf}}
    \subfloat[GLM-130B]{\includegraphics[width=0.24\textwidth]{figs/fig-overlap-Llama-GLM-130B.pdf}}
  \caption{
  We compare \sysname's performance with communication overlap allowed with unallowed
  }
  \label{fig:ablation:overlap}
\end{figure}

\subsection{Overlap Strategy Technique Analysis}

\sssec{Method}. We examined the effectiveness of overlapping communication with computation, comparing no overlap, gradient communication overlap, parameter gather overlap, and combined overlap strategies.

\sssec{Results}.
Figure \ref{fig:ablation:overlap} compares \sysnameâ€™s performance with and without communication overlap across different models and GPU scales. The results show that enabling communication overlap improves throughput, especially for larger models and higher GPU counts. For smaller models like Llama-2-7B and Llama-2-13B, the benefits are modest but noticeable, while for larger models such as Llama-2-70B and GLM-130B, overlap significantly reduces communication delays, resulting in better throughput. This highlights the importance of overlap strategies in optimizing performance and scalability, particularly for large-scale models where communication overhead becomes a bottleneck.