\subsection{Mode-1: Comparison with Expert Plans}\label{sec:exp:expert}

\sssec{Method}.
To prove the \sysname's ability to search the optimal strategy on MegatronLM, we compared \sysname\ with an expert.
We first selected three models with different parameter sizes (7 model settings in total): Llama-2 (7B, 13B, and 70B), Llama-3 (8B, 70B), and GLM (67B, 130B).
Then, we offer 4 GPU number settings: 32, 128, 256, and 1024.
Next, we asked six experts to craft a parallel strategy for each setting (different models and different GPU settings, overall $7\times 4=28$ settings) based on their expert experience.
Each participant has over six years of industry machine learning service or training experience.
Then, we ran each of the six participants' parallel strategies for each setting on MegatronLM and picked the optimal one (one with the largest throughput) among the six expert-crated strategies as the expert-optimal strategy.
At last, we run \sysname\ to search the optimal parallel strategy automatically and compare the \sysname's parallel strategy's throughput with the expert-optimal parallel strategy's throughput.

\sssec{Results}.
As shown in Fig. \ref{fig:expert:throughput}, \sysname demonstrates its ability to automatically generate parallel strategies that match or exceed expert-tuned plans across various model configurations. This highlights \sysname's capability to generalize and optimize without manual intervention.

\par A key finding is that \sysname consistently matches or outperforms manually designed strategies, showing that its automated search can achieve results on par with domain experts. This adaptability extends across diverse hardware and model types, while specific setups often constrain expert-tuned plans. \sysname dynamically adjusts to different configurations, optimizing parallel strategies based on the specific training environment.

\par Another important observation is \sysname’s flexibility in combining different parallelism techniques—data, tensor, and pipeline. While expert strategies often focus on one type of parallelism, \sysname optimally balances multiple forms, leading to superior performance, especially for large-scale models. This hybrid approach is likely the key to future parallelism strategies, where flexibility and adaptation are critical.