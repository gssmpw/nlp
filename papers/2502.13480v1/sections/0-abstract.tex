\begin{abstract}
In this paper, we introduce an efficient and money-saving automatic parallel strategies search framework on both homogeneous and heterogeneous GPUs: \sysname.
First, \sysname searches for the efficiency-optimal parallel strategy in both GPU configurations search space and parallel parameters search space.
Then, \sysname also provides the solution on heterogeneous GPUs by mathematically modeling the time consumption of heterogeneous training.
At last, \sysname is the first to propose the automatic parallel strategy search on money-saving.
The experiment results demonstrate that \sysname can achieve better throughput than expert-designed strategies.
The search time cost for \sysname can also be limited to 1.27 seconds in a single-GPU setting and less than 1.35 minutes in a heterogeneous-GPU setting on average with an accuracy of over 95\%.
%In recent years, the growth of deep learning has been marked by the development of large language models (LLMs) like GPT-4, Llama, and Mistral, pushing the boundaries of natural language processing and other domains. However, training these large-scale models presents significant computational challenges, necessitating multiple GPUs and sophisticated parallelization strategies. Existing approaches often require expert knowledge and manual tuning to design hybrid parallel plans, making the process time-consuming and less accessible. \sysname addresses these challenges by automating the search for optimal parallelization strategies. This paper presents the design and implementation of \sysname, a framework that automates the generation of efficient parallelization plans for large Transformer models. By leveraging a comprehensive search algorithm, \sysname systematically explores a vast space of potential parallel strategies, optimizing training time and financial cost. The framework supports dynamic and heterogeneous GPU clusters, providing adaptable and scalable solutions for real-world deployments. Our evaluations demonstrate that \sysname not only reduces the time and cost associated with training large-scale models but also outperforms existing automated parallelization frameworks in terms of efficiency and adaptability.

% In recent years, the growth of deep learning has been marked by the development of large language models (LLMs) like GPT-4, Llama, and Mistral, pushing the boundaries of natural language processing and other domains. However, training these large-scale models presents significant computational challenges, necessitating multiple GPUs and sophisticated parallelization strategies. Existing approaches often require expert knowledge and manual tuning to design hybrid parallel plans, making the process time-consuming and less accessible. AutoParallel addresses these challenges by automating the search for optimal parallelization strategies. This paper presents the design and implementation of AutoParallel, a framework that automates the generation of efficient parallelization plans for large Transformer models. By leveraging a comprehensive search algorithm, AutoParallel systematically explores a vast space of potential parallel strategies, optimizing training time and financial cost. The framework supports dynamic and heterogeneous GPU clusters, providing adaptable and scalable solutions for real-world deployments. Our evaluations demonstrate that AutoParallel not only reduces the time and cost associated with training large-scale models but also outperforms existing automated parallelization frameworks in terms of efficiency and adaptability.
\end{abstract}