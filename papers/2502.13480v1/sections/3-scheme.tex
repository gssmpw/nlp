\section{Proposed Scheme}\label{sec:scheme}

%In this section, we introduce our proposed scheme.
%We first provide an overview of \sysname's whole workflow in \S\ref{sec:scheme:overview}.
%Then, we introduce the MegatronLM runtime we used and analyze why we chose MegatronLM as the runtime basis.

\begin{figure*}[htb!]
\centering
\includegraphics[width=\textwidth]{figs/Fig-AutoParallel-Scheme.pdf}
\caption{
\sysname works as follows:
1) \underline{Input Preprocess}: \sysname extracts MegatronLM's parameter set as its parameter search space, parses the model architecture, and generates diverse GPU configurations based on GPU type, model, and quantity.
2) \underline{Parallel Strategy Search}: Using GPU configurations, parameter set, and model architecture, \sysname creates parallel strategies. User-defined rules and memory constraints filter these strategies.
Next, the memory-based filter computes per-stage's allocated GPU's memory. The strategy is filtered if the memory is out of the upper boundary.
3) \underline{Cost Simulation}: The simulator calculates communication and computation costs using an XGBoost model to estimate each operator's time, determining the total time for each strategy.
4) \underline{Money Calculation}: It computes the monetary cost of each strategy based on time and GPU configurations.
}
\label{fig:scheme}
\vspace{-15pt}
\end{figure*}

%\subsection{Overview}\label{sec:scheme:overview}

\sysname uses MegatronLM as the runtime backend due to various advantages of it (\S\ref{sec:scheme:runtime}).
\sysname works as follows:

1) \underline{Input Preprocess} (\S\ref{sec:scheme:input}): \sysname first extracts the parameter set of MegatronLM as the parameter search space of \sysname. Then, the training model architecture is parsed. Next, the GPU selector of \sysname iterates the GPU pool to generate a diverse set of GPU configurations, including the GPU type, model, number, etc.

2) \underline{Parallel Strategy Search} (\S\ref{sec:scheme:search}): Based on the three input information (GPU configurations, parameter set, model architecture), \sysname runs its search space generator to generate a diverse set of parallel strategies.
Then, the rule-based filter reads users' crafted rules to filter out the strategies not permitted by user-crafted rules.
Next, the memory-based filter computes the memory allocated to each stage's GPU. If the memory is outside the upper boundary, the strategy is filtered.

3) \underline{Heterogeneous GPU Search} (\S\ref{sec:scheme:heter}):
We re-modeled the time cost of heterogeneous training so that \sysname can search for heterogeneous training strategies. We also performed a computational complexity analysis on the search overhead of heterogeneous training.

3) \underline{Cost Simulation} (\S\ref{sec:scheme:cost}): Then, given the filtered parallel strategies, the performance simulator computes the communication cost and computation cost to get the overall time cost for each parallel strategy.
Each operator's time cost is predicted by a trained XGBoost model.

4) \underline{Money Calculation} (\S\ref{sec:scheme:money}): At last, \sysname calculates each parallel strategy's money cost for the training models.
The money computation calculator calculates the money cost for each parallel strategy due to the overall time cost and GPU configurations.


\subsection{MegatronLM-based Runtime}\label{sec:scheme:runtime}

%\sysname leverages MegatronLM as the runtime backend for several compelling reasons. It is an ideal choice for executing our search space and implementing the automatically derived parallelism strategies. 
\sysname leverages MegatronLM as the runtime backend for the parallel strategy execution.
Below, we describe MegatronLM's key features and benefits, substantiating its selection as our system's core runtime.

\sssec{Overview of MegatronLM}. MegatronLM is a highly optimized, scalable framework for training large-scale Transformer models. Developed by NVIDIA, it has been widely adopted in research and industry for its robust performance and flexibility in handling diverse parallelism techniques. 
We utilize MegatronLM as the \sysname's backend for the reasons below:

\begin{enumerate}[label={[\arabic*]}, itemsep=0pt, leftmargin=*,topsep=0pt]
    \item \textbf{Support for diverse parallelism methods}. MegatronLM is designed to handle a variety of parallelism strategies, including data parallelism, tensor parallelism, and pipeline parallelism. This versatility in parallelism methods allows MegatronLM to optimize training for different model architectures and hardware configurations, ensuring maximum efficiency.
    \item \textbf{Industry-validated reliability}. MegatronLM's reliability has been extensively validated in industrial applications. It is widely used in the industry for training some of the largest language models, such as GPT-3, demonstrating its robustness and stability. Furthermore, as an open-source framework, continuous contributions from both the research community and industry practitioners ensure that MegatronLM stays up-to-date with the latest advancements and best practices in deep learning.
\end{enumerate}


%\sssec{Industry-Validated Reliability}. MegatronLM's reliability has been extensively validated in industrial applications. It is widely used in the industry for training some of the largest language models, such as GPT-3, demonstrating its robustness and stability. The framework's proven track record in handling large-scale, production-level training workloads makes it a trustworthy choice for our runtime backend.

%\sssec{Scalability and Flexibility}. MegatronLM is built to scale seamlessly across multiple GPUs and nodes, making it suitable for training models that require significant computational resources. Its flexibility allows it to adapt to various hardware configurations, from single-node multi-GPU setups to large, distributed clusters. This scalability is essential for \sysname, enabling the system to explore and implement optimal parallelism strategies across different scales and hardware environments.

%\sssec{Community and Ecosystem Support}. As an open-source framework, MegatronLM benefits from a vibrant community and strong ecosystem support. Continuous contributions from both the research community and industry practitioners ensure that MegatronLM stays up-to-date with the latest advancements and best practices in deep learning. This ongoing development and support enhance the framework's capabilities and provide users with a reliable, cutting-edge tool for large-scale model training.

\subsection{Input Preprocess}\label{sec:scheme:input}

The input preprocess stage in \sysname\ involves several key steps to prepare and organize the data necessary for the subsequent parallel strategy search. This stage ensures the system can effectively explore various parallelization strategies and configurations to optimize model training. Hereâ€™s a detailed breakdown of the process:

\sssec{GPU pool}. 
\sysname supports three search modes, and generates the corresponding GPU pool:

\underline{Mode 1: homogeneous}: Specify a GPU type and the number of cluster GPUs, such as specifying A800 and 32768 GPUs, to find the best strategy:
\begin{equation}
    \mathcal{C}_{gpu}=(A800, 32768)
\end{equation}
\underline{Mode 2: heterogeneous}: Specify multiple GPU types and the number of cluster GPUs, such as specifying the number of cluster GPUs to be 8192, and the cluster uses two types of GPUs, A800 and H100 (the maximum number of these two GPUs can be specified at the same time), to find the best heterogeneous training strategy:
\begin{equation}
    \mathcal{C}_{gpu}=8192, (A800:2048),(H100:7168)
\end{equation}
where maximum number of A800 is 2048, maximum number of H100 is 7168.

\underline{Mode 3: cost}: Specify a GPU type(for example, H100) the maximum number of cluster GPUs(for example, 4096) and the maximum money limit to find the best strategy:
\begin{align}
\mathcal{C}_{gpu}=\{
[(H100, 2)], [(H100, 4)], ... [(H100, 4096)]\}
\end{align}
%For each GPU $g_i$, the parameters include the GPU type, the number of GPUs $N_{g_i}$, the per-GPU memory $M_{g_i}$, and the topology type (e.g., Mesh). 
%The GPU selector iterates over $\mathcal{G}$ to generate diverse GPU configurations $\mathcal{C}_{gpu}$:
%\begin{equation}
%    \mathcal{C}_{gpu} = \left\{ (g_1, N_{g_1}), (g_2, N_{g_2}), \dots, (g_k, N_{g_k}) \right\}, \quad g_i \in \mathcal{G}
%\end{equation}
Each configurations defines a runnable GPU collections for \sysname.

\sssec{Parameter set extraction}. \sysname\ extracts a detailed parameter set $\mathcal{P}$ from the MegatronLM framework, including micro-batch size, recompute activations, pipeline model parallel size, sequence parallel, tensor model parallel size, data model parallel size, off-load compute, recompute layers, etc. These parameters define the search space for possible parallelization parameter set:
\begin{equation}
    \mathcal{P} = \left\{ p_1, p_2, \dots, p_n \right\},
\end{equation}

\sssec{Model architecture parsing}. The system parses the architecture of the training model, denoted as $\mathcal{M}$. This includes details such as:
\begin{align}
    \mathcal{M} = \{ \text{model type}, \text{number of layers},
    \text{hidden size},
    \\\text{attention heads},
    \text{intermediate size},
    \text{vocabulary size}\}
\end{align}

\sssec{Integration of input information}. The selected GPU configurations $\mathcal{C}_{gpu}$, parameter set $\mathcal{P}$, and parsed model architecture $\mathcal{M}$ are integrated to form the foundational input for the parallel strategy search:
\begin{equation}
    \mathcal{I} = (\mathcal{C}_{gpu}, \mathcal{P}, \mathcal{M})
\end{equation}

\subsection{Parallel Strategies Search}\label{sec:scheme:search}

The parallel strategy search stage generates, filters, and identifies the most efficient parallelization strategies for training large-scale Transformer models. This stage consists of three components: 
(1) The \textbf{search space generator} generates diverse parallel strategies, 
(2) the \textbf{rule-based filter} filters illegal parallel strategies according to user-defined rules, and 
(3) the \textbf{memory-based filter} filters out parallel strategies that are out of memory bounds.

\sssec{Search space generator}. 
The search space generator iterates all GPU configurations $\mathcal{C}_{gpu}$ according to the user input,
all potential parallel parameters value denoted as $f(\mathcal{P})$ with the fixed model architecture $\mathcal{M}$ to get all potential $s_i \in \mathcal{S}$.
Each strategy is defined as
\begin{equation}
    s_i=\{ c_{gpu}, P', \mathcal{M} \}, c_{gpu} \in \mathcal{C}_{gpu}, P' \in f(\mathcal{P})
\end{equation}

The total number of strategies $\mathcal{S}$ is determined by the product of all parameter options and the number of all GPU configurations:
\begin{equation}
    |\mathcal{S}| = \prod_{P' \in f(\mathcal{P})} |P'| \times |\mathcal{C}_{gpu}|
\end{equation}

\sssec{Rule-based filter}. The Rule-Based Filter applies user-defined rules to filter out strategies that meet specific rules. Let $r_1, r_2, \dots, r_k$ represent the rules. A strategy $s_i \in \mathcal{S}$ is valid if:
\begin{equation}
    r_j(s_i) = \text{False} \quad \forall j \in \{1, 2, \dots, k\}
\end{equation}
which suggests that if the strategy meets any of the rules, it is dropped.

Each rule is a logical expression involving the strategy parameters, such as:
\begin{align}
    r_j = (\$use\_flash\_attn \neq \text{None}) \land 
    \\
    (\$recompute\_granularity = \text{selective})
\end{align}
If any rule is met, the strategy is filtered out.

To illustrate the design of our rule-based filters, we provide three examples of rules:

1. Flash attention rule:
\begin{align}
    \$use\_flash\_attn \neq \text{None} \quad \&\& 
    \\
    \quad \$recompute\_granularity = \text{selective}
\end{align}
This rule ensures that the flash attention is being used, 
and the recompute granularity can not be selective.

2. Layer recomputation rule:
\begin{align}
    \$recompute\_num\_layers > 
    \\
    \$pipeline\_model\_parallel\_size
\end{align}
This rule filters out the strategies where the number of layers to be recomputed exceeds the number of pipeline parallel stages.

3. GPU division rule:
\begin{align}
    \$num\_gpus \% (\$pipeline\_model\_parallel\_size \times 
    \\
    \$tensor\_model\_parallel\_size) \neq 0
\end{align}
This rule ensures that the total number of GPUs is divisible by the product of the pipeline model parallel size and tensor model parallel size.

These rules are written in a format:
\begin{equation}
    \text{expression} \ \&\&/\| \ \text{expression} \ \&\&/\| \ \text{expression} \dots
\end{equation}
where $\&\&$ has a higher precedence than $\|$, and the expressions are evaluated from left to right.

\sssec{Memory-based filter}. The Memory-Based Filter calculates the memory usage for each stage of a strategy. Let $M_i(s_j)$ denote the memory required for stage $i$ of strategy $s_j$. The strategy is filtered out if:
\begin{equation}
    M_i(s_j) > M_{gpu}, \quad \forall i, s_j
\end{equation}

Only strategies that satisfy the memory constraints for all stages are retained for further evaluation:
\begin{equation}
    \mathcal{S}_{valid} = \left\{ s_j \in \mathcal{S} \mid M_i(s_j) \leq M_{gpu}, \forall i \right\}
\end{equation}

Specifically, by conducting extensive offline experiments, we collect statistics on the memory consumption of a single layer under various configurations (e.g., enabling/disabling flash attention, enabling/disabling selective recomputation, enabling/disabling sequence parallelism, and simultaneously enabling both sequence parallelism and flash attention). Through these experiments, we derive an empirical formula for single-layer memory usage as a function of parameters such as the number of microbatches $b$, sequence length, hidden size, FFN size, tensor parallelism (TP), pipeline parallelism (PP), and the number of attention heads. This empirical formula allows us to estimate the total memory consumption for any given strategy based on the configuration settings.

\begin{figure*}[htb!]
\centering
\includegraphics[width=\textwidth]{figs/fig-HeterGPU.pdf}
\caption{
    The time cost of each pipeline stage is different, and the bubble time is also different, so the total duration cannot be converted by the duration of a pipeline stage and the bubble time.
}
\label{fig:heter}
\end{figure*}

\subsection{Heterogeneous GPUs Strategies Search}\label{sec:scheme:heter}

Compared with homogeneous GPU strategies search, heterogeneous GPU strategies search has several problems:
\begin{enumerate}[label={[\arabic*]}, itemsep=0pt, leftmargin=*,topsep=0pt]
    \item In a heterogeneous GPU scene, the time cost of each pipeline stage varies, and each pipeline's corresponding bubble time also differs. The classic total duration formula: 
    $
    T_{total} = T_{comp} + T_{comm} + T_{bubble}
    $
    with
    $
    T_{bubble}=\frac{pp - 1}{m} \times (T_{comp}+T_{comm})
    $
    is no longer applicable.% A new end-to-end duration formula needs to be derived.
    \item Each pipeline stage can be a different GPU type. The pipeline stages of different GPUs may have different numbers of model layers. The number of such combinations increases exponentially with the GPU type and the model number layers. %How to mathematically model the parameter space and minimize the number of combinations?
\end{enumerate}

\sssec{Definition}. Assume there are $M$ types of GPUs, with the computational capability of the $i$-th type denoted as $c_i$, and a maximum quantity of $l_i$ for each type. Set the following parameters: pipeline parallel size = $P$, data parallel = $D$, tensor parallel = $T$, number of micro-batches = $K$, model layers = $N$.

Consider deploying $P$ pipeline stages across $M$ types of GPUs. The partition of $P$ stages across $M$ types of GPUs is denoted by $\{p_1, p_2, \dots, p_P\}$, where each $p_i$ is a value representing the GPU type it is deployed on, ranging from $1$ to $M$. Thus, there are $O(M^P)$ possible configurations. An example of a simple pipeline parallelization plan (assuming four pipeline stages deployed on four types of GPUs, each stage containing a number of model layers $n_1, n_2, n_3, n_4$) is shown in Fig. \ref{fig:heter}. Each pipeline stage has a different computation latency, and the bubble time varies across stages, so the total latency cannot simply be the sum of the latencies and bubble times across all pipeline stages. 

\sssec{Heterogeneous GPU cost model}. Consider any given partition where the $P$ pipeline stages are divided across $M$ types of GPUs. Let $t_{p_i}$ represent the computation latency of the $i$-th pipeline stage, and $h_{p_i}$ represent the peer-to-peer communication latency. The forward latency for this partition (the backward latency is similar) can be derived as follows:

\begin{equation}
\label{eq:heter:def}
    \sum_{1 \leq i \leq P} (t_{p_i} + h_{p_i}) + (K - 1) \times \max(t_{p_i} + h_{p_i})
\end{equation}

Here, $t_{p_i}$ depends only on the number of model layers in the $i$-th pipeline stage and the GPU it is deployed on, while $h_{p_i}$ depends solely on the tensor shape during communication. Therefore, for any configuration, the order of $p_i$ can be rearranged to place identical values of $p_i$ in consecutive positions (i.e., each segment of $P$ pipeline stages is deployed on the same GPU type), reducing the total number of partitions from $O(M^P)$ to $C_{P-1}^{M-1} \times (M-1)! \approx O(P^{M-1})$.

In summary, when the model is deployed in a cluster with $M$ types of GPUs, the mathematical model of its strategy search space is equivalent to solving the following equation for $m_i$, $n_i$, and $n_i$ (assuming the number of model layers in the pipeline stage on GPU type $i$ is $n_i$):

\begin{equation}
\label{eq:heter:comp}
\left\{
\begin{array}{l}
m_i, n_i \left| \sum_{1 \leq i \leq M} m_i = P, \quad m_i \leq \frac{l_i}{D \cdot T}, \right. \\
\left. \quad \sum_{1 \leq i \leq M} m_i \cdot n_i = N \right.
\end{array}
\right\}
\end{equation}


\sssec{Search complexity analysis}. Dividing $P$ pipeline stages into $M$ sequential segments results in $\binom{P-1}{M-1} \times (M-1)! \approx O(P^{M-1})$ possible configurations. Solving Equation (\ref{eq:heter:comp}) for all combinations of $m_i$ has a time complexity of $O(P^{M-1})$, while for any fixed combination of $m_i$, solving Equation (\ref{eq:heter:comp}) for all $n_i$ has a time complexity of $O\left(\frac{N}{m_1} \times \frac{N}{m_2} \times \dots \times \frac{N}{m_{M-1}}\right) < O(N^{M-1})$.

Thus, the total time complexity for all combinations of $m_i$ and $n_i$ for Equation (\ref{eq:heter:comp}) is $O(N^{M-1}) \times O(P^{M-1})$.
%When $M=2$, the time complexity and output space grow by three orders of magnitude. When $M > 3$ (i.e., when there are more than three types of GPU clusters), further pruning and optimization are required.


\subsection{Cost Simulation}\label{sec:scheme:cost}

The Cost Simulation stage is essential for evaluating the performance of the filtered parallel strategies from the Parallel Strategy Search stage. This stage estimates the overall training time by considering communication and computation costs.

\sssec{Input: parallel strategy set}. The parallel strategies that pass through the Rule-Based Filter and Memory-Based Filter, denoted as $\mathcal{S}_{valid}$, are considered for cost simulation. These strategies represent feasible configurations that satisfy all the constraints and are now evaluated for their computational and communication efficiency. Let $\mathcal{S}_{cost}$ represent the set of strategies selected for cost evaluation:
\begin{equation}
    \mathcal{S}_{cost} = \{ s_1, s_2, \dots, s_m \}, \quad s_i \in \mathcal{S}_{valid}
\end{equation}

\begin{figure}[htb!]
\centering
\includegraphics[width=0.48\textwidth]{figs/fig-xgboost.pdf}
\caption{
\sysname' cost model is based on the XGBoost model.
Each computation and communication operator's latency cost is calculated based on XGBoost's prediction on efficiency and theoretical computing power and communication bandwidth.
}
\label{fig:xgboost}
\end{figure}

\sssec{XGBoost-based cost model}. The computation time for each operator is estimated as follows (see Fig. \ref{fig:xgboost}):

\begin{equation}
T_{comp}(s_i) = \frac{\theta_{comp}}{ \phi_{comp} \times \eta_{comp}},
\end{equation}
where $\theta_{comp}$ is the theoretical computing overhead, $\phi_{comp}$ is the theoretical computing power, $\eta_{comp}$ represents the GPU efficiency, a parameter estimated via XGBoost, with values constrained within the range $(0, 1]$.

Similarly, the communication time for each operation is calculated as:

\begin{equation}
T_{comm}(s_i) = \frac{\theta_{comm}}{ \phi_{comm} \times \eta_{comm}},
\end{equation}
where $\theta_{comm}$ is the theoretical communication overhead, $\phi_{comm}$ is the theoretical bandwidth, $\eta_{comp}$ represents the bandwidth efficiency, a parameter estimated via XGBoost, with values constrained within the range $(0, 1]$.


%\textcolor{red}{add more description to enhance}
One of the distinguishing features of the \sysname framework compared to other partitioning tools lies in its unique approach to estimating operator latency. 
Unlike conventional methods that rely on a database of pre-measured values, \sysname calculates the operation latency analytically, which enables automatic adaptation to new models and operators. 
This capability to dynamically adjust to diverse computational requirements is a core reason behind \sysnameâ€™s adaptability to novel architectures and computational operators.


%The time cost for each operator within a stage is predicted by a pre-trained XGBoost model, which uses features such as the size of data, the number of GPUs, and the type of operation (e.g., matrix multiplication, gradient synchronization). Let $f_{op}$ represent the feature set, and $T_{op}$ be the predicted time cost for a given operator. The model prediction is given by:
%\begin{equation}
%    T_{op} = \text{XGBoost}(f_{op})
%\end{equation}

%For each strategy $s_i$, the total time cost $T_{total}(s_i)$ is then calculated by summing the predicted times for all operators in all stages. This ensures accurate estimation of the overall time required to execute the strategy.

%\sssec{Overall Time Cost Calculation}. After calculating the computation and communication costs, the overall time cost $T_{total}(s_i)$ is used to rank the strategies based on their efficiency. Strategies with lower total time costs are considered more efficient:
%\begin{equation}
 %   T_{total}(s_i) = \sum_{j=1}^{n} \left( C_j(s_i) + Comm_j(s_i) \right)
%\end{equation}

%This ensures that the strategies are ranked according to their computational and communication efficiency, enabling the selection of the most time-efficient strategies for the given GPU configuration and model architecture.

%\sssec{Money Cost Integration}. In the subsequent Money Calculation stage, the overall time cost $T_{total}(s_i)$ is directly linked to the monetary cost calculation for each strategy. The final selected strategy is the one that minimizes both time and financial cost while meeting user-defined constraints.

\sssec{Performance simulator}. 
The Performance Simulator first computes the time cost for each pipeline stage $p_i \in \mathcal{S}_{cost}$ by evaluating two key components: 
computation cost $T_{comp}(p_i)$, communication cost $T_{comm}(p_i)$
\begin{align}
    T_{total}(p_i) &= \sum^{op_{comp}\in p_i}_{op_{comp}}T_{comp}(p_i, op_{comp}) 
    \\
    &+ \sum^{op_{comm}\in p_i}_{op_{comm}}T_{comm}(p_i, op_{comm})
\end{align}
And then, performance simulator utilizes the equation (\ref{eq:heter:def}) to compute the overall performance in both heterogeneous and homogeneous setting.


\subsection{Money-limit Search}\label{sec:scheme:money}

In this section, we present the process for calculating the monetary cost of parallel strategies in the \sysname framework. The method involves traversing all possible strategies, selecting the optimal strategies based on throughput and cost, and finally computing the financial cost for each strategy. This approach builds upon the principles outlined in \textit{Cost-based \sysname Framework}, where the highest throughput strategy is first identified without considering cost limitations.

\sssec{Optimal strategy pool}. Once all possible valid strategies have been generated, we aim to create an optimal pool of strategies. Let $P_i$ represent the throughput of strategy $i$ and $C_i$ its corresponding cost. We define the optimal pool such that for any strategy $(P_i, C_i)$ in the pool, no other strategy $(P_j, C_j)$ exists such that:
\begin{equation}
    P_j > P_i \quad \text{and} \quad C_j < C_i
\end{equation}

The goal is to find the set of strategies $\mathcal{S}_{opt}$ where:
\begin{align}
    \mathcal{S}_{opt} = \{ (P_i, C_i) \mid \nexists (P_j, C_j) 
    \\
    \text{ such that } P_j > P_i \land C_j < C_i \}
\end{align}

\sssec{Money cost calculation}. The monetary cost for each strategy in the optimal pool is calculated based on the GPU usage and the total training time. Let $T_i$ be the total time required for strategy $i$, and let $g_i$ denote the cost per hour for using GPU $g$. The total cost $M_i$ for strategy $i$ is given by:
\begin{equation}
    M_i = T_i \times \mathcal{N}_{g_i} \times \mathcal{F}_{g_i}
\end{equation}
where $\mathcal{F}_{g_i}$ is the fee for GPU $g_i$ per second, $\mathcal{N}_{g_i}$  is the number of GPUs used by strategy.

Using this formula, we can compute the monetary cost for all strategies in the optimal pool. The strategy with the highest throughput  that meets the user's money constraints is selected as the final strategy.

\sssec{Sorting strategies by cost and throughput}. To sort the strategies, we first rank them by throughput in descending order. If two strategies have the same throughput, we then compare them by cost in ascending order. Formally, the sorting function $S$ can be expressed as:
\begin{equation}
    S(P, C) = \begin{cases} 
    P_i > P_j, & \text{if } P_i \neq P_j \\
    C_i < C_j, & \text{if } P_i = P_j 
    \end{cases}
\end{equation}

This ensures that the strategies are sorted optimally for both performance and cost.