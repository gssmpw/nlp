\section{Related Work}\label{related}


\sssec{General DNN distributed training frameworks}.
\cite{lu2017flexflow} propose Flexflow, a unified framework that can leverage the complementary effects among feature map, neuron, and synapse parallelism to mitigate the mismatch.
Megatron-LM~\cite{shoeybi2019megatron} is the first to present tensor parallel, a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters.
PipeDream~\cite{narayanan2019pipedream} adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. 
Huang et al. propose GPipe~\cite{huang2019gpipe}, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers.
PTD-P~\cite{megatron} extends Megatron-LM~\cite{shoeybi2019megatron}, which explores model parallel for transformers to support data, model and pipeline parallelism, and study the empirical rules of combining them.
Yuan et al. ~\cite{yuan2021oneflow} present OneFlow, a novel distributed training framework based on an SBP (split, broadcast, and partial-value) abstraction and the actor model.
Xu et al. ~\cite{xu2021gspmd} present GSPMD, an automatic, compiler-based parallelization system for common machine learning computations.

\sssec{Domain-specific distributed training framework}.
GShard~\cite{lepikhin2020gshard} is designed for scaling the giant model with MoE~\cite{riquelme2021scaling}. It proposes lightweight annotation APIs for easily specifying tensor partitions, which is then automatically lowered to XLA.
PipeTransformer~\cite{he2021pipetransformer} is specifically designed for transformers with gradually freezing layers.
Varuna~\cite{athlur2021varuna} adapts distributed training on commodity networking which has lower bandwidth and higher latency/jitter.

\sssec{DNN model Partitioning}.
Piper~\cite{tarnawski2021piper}.
Automap~\cite{schaarschmidt2021automap} deals with SPMD-style parallelism to automatically partitioning and mapping DNN models to devices, with smooth integration with existing DNN framework and compiler.
Roc~\cite{jia2020improving} works on graph neural networks.
FlexFlow~\cite{jia2019beyond}.
Alpa~\cite{zheng2022alpa} is the first work to automatically search for the best execution plan combing all potential parallelism dimension.

%\para{Cost Models.}
