\input{latex/tables/long}
\vspace{-0.5em}
\section{Experiments}
In this section, we first present the experimental setup of this paper (in \cref{sec:setup}). Then we demonstrate the logical reasoning and factual retrieval ability of our \name in long-context inference through two widely-used benchmark (in \cref{sec:results}). Finally, we conduct the ablation study (in \cref{sec:ablation}) and reveal the influence of our method (in \cref{sec:analysis}).


\subsection{Experimental Setup}
\label{sec:setup}

\paragraph{Datasets and Implementation Details.}
We utilize 21 tasks from two widely used long document benchmarks: Long-Bench~\cite{longbench} and $\infty$-Bench~\cite{infinitebench} for evaluation. Specifically, 
Long-Bench has a 95\% sequence length of 32K, while $\infty$-Bench averages about 122K in sequence length.  We utilize \basellama~\cite{llama3} and  Qwen2.5-7B-Instruct~\cite{qwen2.5} as our base models with maximum input lengths of 8K and 32K, respectively.
In each inference step, we reuse only 2K KV pairs and store the remaining pairs in the Cache Management system, following the settings of InfLLM. This approach consumes approximately 19 GB of VRAM in our experiments. Inspired by previous works, we retain 64 attention sinks and 512 KV pairs from current context, and adapt the task description into \pq. Consequently, the budget for retrieved KV $k$ is 1,472. These KV pairs are organized into 46 chunks, with each chunk containing 32 pairs. The sliding window size is set to 256. {More details about the datasets and experimental setup is available in ~\cref{sec:alg}.}


\paragraph{Baseline Methods} The objective of \name is to effectively retrieve key-value pairs for long-context inference in LLMs. To achieve this, we evaluate two prominent baseline methods: \textbf{(a)} static KV selection and \textbf{(b)} KV retrieval. \textbf{(a):} Infinite~\cite{infinite-llm} employs global and local attention masks to broaden the attention scope, while Stream~\cite{stream-llm} ensures efficient inference by retaining attention sinks and KV pairs from recent tokens. \textbf{(b):} InfLLM~\cite{stream-llm} searches for KV pairs associated with the currently processed tokens, enabling the capture of long-distance dependency relationships. QLLM~\cite{qllm} focuses on KV memory relevant to the task description to process long sequences.  TokenSelect (TSLLM)~\cite{tokenselect} incorporates the token-level weight of  KV cache per-head for KV retrieval. EMLLM~\cite{emllm} integrates key aspects of human episodic memory and event cognition into KV cache. Notably, all the methods described above are \textbf{training-free}.


\subsection{Main Experiment Results}
\label{sec:results}

We first utilize Long-Bench to evaluate the long-context reasoning capabilities of \name, and then test the fact retrieval ability using $\infty$-Bench.
{We report the results based on Llama-3-8B-Instruct, and the others can be found in \cref{sec:experiments}.}


\paragraph{Long-Bench.} We present the results in \cref{tab:llama3}. (1) \name achieves an average score of 49.40, surpassing the full context setting (31K tokens) by 4.67 points while utilizing only 2K tokens. This highlights the \textbf{efficiency} of its key-value retrieval method in handling long-context inference with a significantly smaller KV budget. (2) Compared to the static KV selection methods Infinite and Stream, \name excels in capturing critical information required for reasoning tasks. (3) In comparison to SOTA KV retrieval methods such as TSLLM and EMLLM, our activation-aware retrieval approach achieves the best results, with improvements of +5.8\% and +4.6\%, respectively. Notably, for tasks like 2WikiMQA and Musique, \name shows substantial gains, demonstrating the effectiveness of activation-aware retrieval in capturing long-term dependencies by recalling fewer KV pairs (e.g., only with 80\% and 50\% budget).

 \input{latex/tables/infty}
 \vspace{-0.5em}
\paragraph{$\infty$-Bench.} Each sample in this benchmark has almost infinite length (avg. 122K), where the key lies in whether factual evidence can be found from the context. As shown in \cref{tab:llama3-infinitybench}, our \name obtains the best result 58.43 and outperforms the SOTA KV retrieval methods even with a smaller KV budget. Especially compared to the token-level retrieval method TSLLM, our approach sets the minimum retrieval unit as a chunk. Although larger chunks may seem less granular, our \pq effectively compensates for this, enhancing 3.5\% performance while simultaneously reducing both time and space complexity from $O(N)$ to $O(m)$. This demonstrates that our method can efficiently recall relevant KV pairs even with coarser granularity.

\input{latex/tables/ablation_study}

\begin{figure*}[!ht] 
\vspace{-0.5em}
    \centering
        \includegraphics[width=1.0\textwidth]{latex/pics/boxplot_with_ppl_line.pdf}
    \caption{Analysis of the top-$k$  (avg. k=1,472) most relevant KV pairs for each inference step across layers. We randomly select 50 samples from Long-Bench and filter out those with a length less than 8K. In each layer, we calculate 35,180 similarity scores generated by our \name and InfLLM respectively. Each score is calculated based on a \pq and a chunk containing 32 KV pairs. The average perplexity is calculated based on the perplexity within the scores of each sample.
    } 
    \label{fig:analysis}
    %\vspace{-0.5em}
\end{figure*}

\subsection{Ablation Studies}
\label{sec:ablation}
In this subsection, we present ablation studies shown in \cref{tab:ablation} to evaluate two key components of our method: the Activation-aware Probe-Query $Q_\text{probe}^t$ (APQ, see \cref{sec:pq}) and the Dynamic Cut-off Mechanism (DCM, see \cref{sec:co}). 

When using APQ for key-value (KV) pair matching, our method attains a comparable score of 48.8, especially getting the best result 98.0 in retrieval tasks. These results demonstrate that the APQ component effectively captures the semantic context of the window for KV matching, outperforming conventional mean pooling approaches. Moreover, the incorporation of DCM, which dynamically determines the number of KV pairs to recall at each layer, further enhances the model’s ability of irrelevant information filtering. Overall, our approach employs a two-stage KV retrieval process following the traditional information retrieval paradigms: first, an initial retrieval stage identifies potentially relevant KV pairs; subsequently, a refined recall stage optimizes the selection process, achieving a peak performance of 49.4.

\subsection{Analysis of Retrieved KV Pairs}
\label{sec:analysis}
%补一个实验，消融实验分析，A. 纵轴密度，横轴层数 B. 纵轴相似度和，横轴层数 C. 纵轴挑选数目，横轴层数 or D.纵轴保留比例（选取的相似度/总体的相似度）
%window 大小, chunk大小 性能，ppl
%三个模型 mistral/qwen/llama

In this subsection, we compare the retrieved KV pairs from our \name and InfLLM methods to evaluate the specific impact of our proposed approach. To facilitate this comparison, we present the distribution of cosine similarity scores and average perplexity in \cref{fig:analysis} and analyze the following:

\paragraph{Cosine Similarity.} The box of cosine similarity clearly shows that ActQKV consistently achieves higher similarity scores across most layers compared to InfLLM. This outcome can be attributed to the activation-aware query (\pq) we introduced, which more effectively captures the underlying semantic information of the window context for each inference step. Furthermore, the enlargement of the box plots indicates that the distribution of similarities becomes more dispersed. This suggests that our \pq covers a broader semantic space, thereby resulting in a more robust KV retrieval process. The greater spread in the similarity values also reflects the model’s ability to account for a wider range of relevant KV pairs, ultimately enhancing the precision and adaptability of the retrieval process across different contexts.

\paragraph{Average Perplexity.} With respect to average perplexity, \name consistently shows lower perplexity scores compared to InfLLM which maintains a value of around 46.0. This indicates that \name yields more coherent and predictable results across the all layers. Notably, in layers 0 and 13, we notice significant differences, with \name showing more variation than InfLLM. This suggests that our retrieval method can flexibly adapt to the characteristics of different layers. By reducing perplexity, \name improves the ability to discriminate relevant KV pairs from irrelevant ones, resulting in more coherent and less uncertain historical information for long-context inferences in LLMs.
