
\section{Background}
In this section, we first introduce the two stages of inference for long-context LLMs using sliding window attention with KV cache (in \cref{sec:window}), and then define the problem of KV Retrieval (in \cref{sec:problem}).

\subsection{Sliding Window Attention with KV Cache}
\label{sec:window}
Given an input sequence $\mathbf{X}$, the generation of the output sequence $\mathbf{Y}$ during LLMs inference can be divided into two stages: pre-filling the input $\mathbf{X}$ and decoding the output $\mathbf{Y}$.

To handle long sequences input of tasks, exiting works~\cite{stream-llm, infllm, qllm} use sliding window attention to process the text iteratively. In this mechanism, the lengthy input sequence $\mathbf{X}$ is partitioned into $T$ windows, denoted as $\mathbf{W} = \{ \mathbf{w}^{1}, \dots, \mathbf{w}^{T}\}, \mathbf{W}\in \mathbb{R}^{T \times m}$ and $m$ indicates the window size (see~\cref{fig:framework}(a)). To reduce computational costs, the model processes each window sequentially  and stores the historical key-value pairs in a cache (i.e., $\mathbf{K}_{\text{cache}}$ and $\mathbf{V}_{\text{cache}}$) for future reuse (see~\cref{fig:framework}(b)).

\textbf{During $t$-th pre-filling step  ($t \leq T$)}, the model utilizes the KV cache $\mathbf{K}^{t-1}_{\text{cache}}$ and $\mathbf{V}^{t-1}_{\text{cache}}$ from the historical sequence $\mathbf{W}[:t-1]$ to compute the attention output $\mathbf{O}^{t} \in \mathbb{R}^{m \times d}$ for the current $m$ window tokens $\mathbf{w}^{t} \in \mathbb{R}^{m}$ as follows:
\begin{equation}
\mathbf{O}^{t} = \text{Attention}\left( \mathbf{Q}^{t}, \left[ \mathbf{K}^{t}, \mathbf{K}^{t-1}_{\text{cache}} \right], \left[ \mathbf{V}^{t}, \mathbf{V}^{t-1}_{\text{cache}} \right] \right),
\label{eq:prefilling}
\end{equation}
where the triplet $\mathbf{Q}^{t}=\{\mathbf{q}^{t}_i\}^m_{i=1}$, $\mathbf{K}^{t}=\{\mathbf{k}^{t}_i\}^m_{i=1}$, $\mathbf{V}^{t}=\{\mathbf{v}^{t}_i\}^m_{i=1} \in \mathbb{R}^{m \times d}$ represents the generated attention vectors, each corresponds to $m$ tokens with $d$ hidden dimensions. To further save GPU memory, current methods select partial KV cache $\mathbf{K}^{*}$ and $\mathbf{V}^{*}$ for inference, denoted as:
\begin{equation}
\mathbf{O}^{t} = \text{Attention}\left( \mathbf{Q}^{t}, \left[ \mathbf{K}^{t}, \mathbf{K}^{*} \right], \left[ \mathbf{V}^{t}, \mathbf{V}^{*} \right] \right),
\label{eq:prefilling}
\end{equation}
where $\mathbf{K}^{*} \subseteq \mathbf{K}^{t-1}_{\text{cache}}$ and $\mathbf{V}^{*} \subseteq \mathbf{V}^{t-1}_{\text{cache}}$. 

\textbf{During $t$-th decoding step  ($t > T$)}, the model generates the output sequence $\mathbf{Y}$ token-by-token. Unlike pre-filling, the model uses only one single query vector $\mathbf{q}^{t} \in \mathbb{R}^{1 \times d}$ along with corresponding key and value vectors $\mathbf{k}^{t}, \mathbf{v}^{t} \in \mathbb{R}^{1 \times d}$ to predict one next token $y^{t} \in \mathbf{Y}$ in each step. Its corresponding attention output $\mathbf{o}^{t} \in \mathbb{R}^{1 \times d}$ can be computed as:
\begin{equation}
\vspace{-3pt}
\begin{aligned}
\mathbf{o}^{t} = \text{Attention}\left( \mathbf{q}^{t}, \left[ \mathbf{k}^{t}, \mathbf{K}^{*} \right], \left[ \mathbf{v}^{t}, \mathbf{V}^{*} \right] \right).
\end{aligned}
\end{equation}

\begin{figure*}[!ht] 
    \centering
        \includegraphics[width=1\textwidth]{latex/pics/framework_final.pdf}
    \caption{\text{Illustration of our \name}. Sliding window attention stores historical KV pairs in a cache and reuses them for subsequent window inference. Based on this, \name first identifies the anchor tokens within the window and then constructs the activation-aware \pq. This \pq is subsequently used to retrieve the top-k relevant KV pairs from the cache during the pre-filling stage. During the decoding stage, the cut-off mechanism dynamically adjusts the number of recalled KV pairs based on the distribution of key-values at each layer, ensuring the inclusion of relevant pairs while minimizing the influence of irrelevant ones. The cache can be stored in the CPU and transferred to the GPU when needed. All our contributions are highlighted in {red}. }
    \label{fig:framework}
\vspace{-0.5em}
\end{figure*}

\textbf{After the $t$-th step}, the newly generated key-value pairs will be stored in the cache (see~\cref{fig:framework}(e)), updating it as demonstrated below:
\begin{equation}
\begin{aligned}
\mathbf{K}^{t}_{\text{cache}},\mathbf{V}^{t}_{\text{cache}} = \mathbf{K}^{t-1}_{\text{cache}}\cup \mathbf{K}^{t}, \mathbf{V}^{t-1}_{\text{cache}}\cup \mathbf{V}^{t},
\end{aligned}
\end{equation}
where $\cup$ denotes the concatenation operation and the tensors of cache can be saved in either CPU or GPU memory. In general, saving in the CPU can significantly reduce the memory usage of the GPU. Note that $\mathbf{K}^t=\mathbf{k}^t$ and $\mathbf{V}^t=\mathbf{v}^t$ are $1\times d$ dimensions during decoding.


\subsection{Problem Setting}
\label{sec:problem}
During long-context inference in LLMs, the historical key-value pairs are essential for maintaining long-range dependencies and overcoming window size limitations. Given a cache comprising $\mathbf{K}^{t-1}_{\text{cache}}$ and $\mathbf{V}^{t-1}_{\text{cache}}$, the objective of KV retrieval is to identify the top-$k$ relevant subset $\mathbf{K}^{*}$ and $\mathbf{V}^{*}$ using the \pq $\mathbf{Q}^{t}_{\text{probe}}$ for the $t$-th inference step~\cite{infllm,emllm,tokenselect}, as described below:
\begin{equation}
\begin{aligned}
&\mathbf{K}^{*},\mathbf{V}^{*} =\mathbf{K}^{t-1}_{\text{cache}}[I^*],\mathbf{V}^{t-1}_{\text{cache}}[I^*], \\
I^* = \arg&\max_{I \subset [m],\atop |I| = k} \sum_{i \in I} \left( \frac{\mathbf{Q}^{t}_{\text{probe}} \cdot {\mathbf{K}^{t-1}_{\text{cache}}}[i]^\top}{\|\mathbf{Q}^{t}_{\text{probe}}\| \times \| \mathbf{K}^{t-1}_{\text{cache}}[i] \|} \right),\\ 
&\quad\quad\quad\quad\quad\quad\quad[m] = \{1,2,\ldots, m\},&
\end{aligned}
\end{equation}
where $\mathbf{Q}^{t}_{\text{probe}} \in \mathbb{R}^{1 \times d}$ denotes the overall representation of window context $\mathbf{w}^{t}$ and $k$ is the number of selected KV. These two factors significantly impact the factual relevance of the retrieved KV index $I^*$ for each transformer layer inference.