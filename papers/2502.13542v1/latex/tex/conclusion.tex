\section{Conclusion}


In this paper,  we present \name, a training-free method to KV retrieval efficiency for long-context LLMs inference. The primary challenge in KV retrieval stems from the inherent vagueness  of existing \pq, which inadequately filter irrelevant KV pairs. To address this limitation, we develop an activation-aware \pq construction strategy and a layer-wise KV cut-off mechanism to effectively match and recall  the relevant KV pairs. We hope this work can inspire the broader research for  LLMs representation methods, leading to improved long-context information filtering capabilities akin to specialized embedding models.
