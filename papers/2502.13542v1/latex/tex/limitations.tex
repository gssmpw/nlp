\section*{Limitations}
Our method achieves promising performance to enhance the relevant KV pairs retrieval for long-context LLMs inference. And we believe that the interpretability of the retrieved KV pairs requires further exploration in future works. Unlike non-autoregressive architectures in embedding models, the auto-regressive architecture of LLMs results in the semantics of current tokens being influenced by historical KV pairs. When processing a long context all at once, this interaction makes it difficult to separate the semantics from various events because the retrieved key-value pairs mostly show historical information. This introduces challenges in interpreting the retrieval results.