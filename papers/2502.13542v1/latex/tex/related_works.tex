\section{Related Works}


\textbf{KV cache retrieval}~\cite{adnan2024keyformer,zhang2023h2o,xiao2025duoattention} has become a critical optimization strategy aimed at reducing memory usage, minimizing inference latency and improving overall throughput in long-context LLMs inference. 

Recent studies employ a sliding window mechanism to address challenges in long-text inference, where tokens outside the window are stored in the cache and only used when needed for the current window. 
To accelerate the retrieval of essential KV, several approaches have proposed index-based methods that organize and access the KV cache at the block or cluster level, enabling efficient querying and extraction. InfLLM~\cite{infllm} maintains the full KV cache in blocks and uses a hierarchical storage strategy to facilitate long-sequence processing. This framework employs CPU-GPU memory orchestration, keeping essential KV and computational units in GPU memory while offloading less frequently accessed units to CPU memory. Q-LLM~\cite{qllm} enhances long-sequence processing by prioritizing memory related to task descriptions. This approach mimics human reading behavior: first reading the question, then searching for the answer in the context. 

In contrast to methods which use uniform KV block sizes, TokenSelect\cite{tokenselect} is based on the observation of sparsity in non-continuous attention patterns. It uses the Query-Key dot product to assess the importance of each KV cache stored at the token level. For each query, they dynamically calculates the importance of past KV caches per head at the token level and selects the most important tokens through a soft voting mechanism across heads. EM-LLM~\cite{emllm} dynamically segments incoming tokens into episodic events, employing a hybrid retrieval mechanism that combines semantic similarity matching with temporal context to efficiently access relevant KV cache segments. Additionally, some researchers focus on KV cache budget allocation across layers~\cite{cai2024pyramidkv, yang-etal-2024-pyramidinfer} and heads~\cite{feng2024ada, fu2025not} due to the hierarchical architecture of LLMs.

Most methods overlook the importance of probes for retrieval, especially given the fact that LLMs are not optimized for retrieval tasks. Therefore, this realization inspires our further exploration of \pq construction in this paper.



