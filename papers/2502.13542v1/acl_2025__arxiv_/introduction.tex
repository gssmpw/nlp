

\section{Introduction}

\begin{figure}[!ht] 
    \centering
        \includegraphics[width=0.45\textwidth]{./pics/similarity_matrix_comparision-final.pdf}
        \includegraphics[width=0.45\textwidth]{./pics/mean_cosine_similarity_per_query_with_full_question_enhanced.pdf}
    \vspace{-0.2em}
    \caption{Visualization of query vector status within \pq compared between \name and InfLLM: "Who is Sobe (Sister of Saint Anne)’s Grandchild?". We simply display the states of 15 tokens from a window of size 256 in the last transformer layer. The \pq generated by our \name aligns more closely with the SOTA embedding model BGE-M3~\cite{chen-etal-2024-m3}. In contrast, InfLLM generates evenly distributed similarities across the context, neglecting the prioritization of anchor tokens compared to our approach.
    } 
    \vspace{-1em}
    \label{fig:motivation}
\end{figure}

With the emergence of large language models (LLMs) capable of handling extended context lengths~\cite{ wang2024beyond,achiam2023gpt, dubey2024llama}, researchers are leveraging their advanced information understanding and filtering abilities to tackle various downstream tasks, including web-based search chatbot~\cite{Semnani2023WikiChat} and document-level question answering (QA)~\cite{lewis2020retrieval}. 
Inevitably, the context length has increased significantly, even surpassing the models’ context limitations. 
However, the computational complexity of attention mechanism~\cite{vaswani2017attention} grows quadratically $O(N^2)$ with the context length $N$ during inference. Specifically, each token from context will be embedded into Query (Q) and interactive with Key (K) and Value (V) embedded from all the $N$ tokens using attention weights, making the whole time and memory complexity $O(N^2)$ for the process.
Even worse, during inference, new tokens are generated one by one while each generation triggers a $O(N^2)$ computation, leading to an $O(N^2+MN^2)$ to generate an output of length $M$.
Therefore, efficiency is a critical challenge in the deployment of long-context LLMs~\cite{li2024survey}.


To handle this issue, the sliding window mechanism has been proposed to segment the input sequence into content blocks and incrementally convert them into a  key-value (KV) cache for reuse~\cite{beltagy2020longformer}. 
During inference, the model computes the KV vectors only for the current window and integrates them with the existing KV cache, thereby reducing redundant KV computations, leading to an $O(N^2+MN)$ complexity.
Building on this mechanism, recent works~\cite{infllm,liu2024retrievalattention} focus on retrieving top-$k$ relevant KV pairs in conjunction with current tokens for preserving long-term contextual dependencies, where further reduces the complexity to $O(kN+kM)$.
In this process, the queries from current window are typically compressed as a \textbf{\pq} for relevant KV retrieval. However, this \pq setting often fails to highlight those anchor tokens with critical activation signals, which are rare and essential to represent long context within the sliding window.

To address this challenge, we first investigate the similarity relationship between the composition of the \pq and KV cache.
Under sparse attention patterns (see upper of Fig.~\ref{fig:motivation}), the query vectors generated by InfLLM (the left) are disordered. In this scenario, each query vector influences the semantics of \pq, which makes the combined representation nondescript. To clearly demonstrate this nondescript (see bottom of Fig.~\ref{fig:motivation}), the {blue line}  employs a widely used mean pooling technology along KV dimension to represent the \pq. It is evident that the \pq fails to capture the distinctions because attention is distracted by all tokens instead of focusing on the anchors.
Therefore, such a nondescript \pq is hard to represent semantic of question and unsuitable for effective KV retrieval.


Motivated by these observations, we argue that only a subset of anchor tokens within the context window plays a dominant role in representing \pq for retrieval. In this paper, we propose \name, a training-free method that incorporates sliding window attention, which mainly involves two stages: matching and recall of relevant KV pairs. \textbf{In KV matching stage}, we construct the \pq for each context window to retrieve the relevant KV pairs in a streaming manner. To effectively estimate the anchor tokens during inference, we employ a window-level activation-aware strategy to monitor the fluctuation of query values for each token. Recognizing that the scarce outlier features is a critical factor affecting model performance~\cite{wang2024learning,wu2024retrieval}, we designate activated query vectors with prominent activation bias to dominate the representation of \pq for accurate retrieval, as shown in {red line} of Figure~\ref{fig:motivation}. 
\textbf{In KV recall stage}, due to the irregular distribution of KV pairs across layers, a fixed threshold often fails to yield optimal retrieval results. In particular, the decoding stage, which is highly sensitive to factual correctness, can be adversely affected by irrelevant KV pairs, potentially leading to hallucinations and degrading the overall quality of the generated text. Therefore, we introduce a KV cut-off mechanism that dynamically adjusts the number of selected pairs based on information density of each layer. Under a constrained KV budget, this mechanism enhances the recall of relevant KV pairs while reduces the introduction of irrelevant ones.


Our contributions are summarized as follows:
\vspace{-0.4em}
\begin{itemize}
  \item Motivated by attention distraction phenomenon, we introduce an activation-aware \pq that efficiently emphasizes anchor tokens essential for accurately matching KV pairs. It is the first exploration to extract long-context representations for KV retrieval without training.
  \vspace{-0.4em}
  \item To further eliminate irrelevant KV pairs and recall the relevant, we design a dynamic KV cut-off mechanism guided by information density across layers during the decoding stage. This method effectively enhances the model's factual filtering ability for reasoning QA.
  \vspace{-0.4em}
  \item Our \name outperforms existing SOTA KV retrieval-based methods with just 2K KV budget on two benchmarks, achieving up to a 16× KV reduction and 10.4\% accuracy improvement compared to using the full cache setting with a 2K budget on LongBench.
\end{itemize}

