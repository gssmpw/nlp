% token位置角标用i，数量用n—>m
% 元素位置角标用j,数量用m
% layer位置角标用\ell,数量用l
%位置用上标，数量用下标
%window size m下标
%inference step t上标
%大写字母表示集合，小写表示sample



\section{Methods}

In this section, we first present the overall framework of our \name, as illustrated in \cref{fig:framework}. We then demonstrate our two-stage approach: the Activation-aware Probe-Query Construction for KV matching (in \cref{sec:pq})and the Dynamic KV Cut-off Mechanism for KV recall (in \cref{sec:co}).



\subsection{Activation-Aware Probe-Query}
\label{sec:pq}
To identify the relevant KV pairs, we leverage the query vectors of each window to construct the attention-aware \pq for retrieval.
The primary distinction between our activation-aware \pq and other representation methods lies in the emphasis on identifying anchor tokens that effectively represent the entire context of the window for KV matching. The main challenge is to accurately distinguish and activate these tokens.

Formally, given a subset of context $\mathbf{w}^t = \{x^{t}_{1}, \dots, x^{t}_{m}\}$ extracted from a long sequence $\mathbf{W}$, we obtain the hidden states $\{z^{t}_{i}\}_{i=1}^m=\{f(x^{t}_{i})\}_{i=1}^m$ at each transformer layer, where $m$ denotes the window size and $f$ denotes the function mapping tokens to corresponding states. Intuitively, hidden states that deviate significantly from their statistical mean (i.e., $\mathbf{\bar{z}}^t$) can be considered that they are from anchor tokens compared to others. Specifically, token ${x}^{t}_{1}$ is deemed more essential than ${x}^{t}_{2}$ for the quality of generation, as indicated by previous works~\cite{wang2024learning, sun2024massive,pang-etal-2024-anchor}, if:
\begin{equation}
\begin{aligned}
\| \mathbf{\bar{z}}^t - f(x^{t}_{1}) \| > \| \mathbf{\bar{z}}^t - f(x^{t}_{2}) \|,
\label{eq:activation}
\end{aligned}
\end{equation}
where $||\cdot||$ is distance metrics.

Building on the aforementioned paradigm Eq.~\ref{eq:activation}, we propose an \textbf{Activation Bias} to distinguish the importance of each query vector within a window context. For the query vectors of the $t$-th pre-filling window $\mathbf{Q}^{t} = \{\mathbf{q}^{t}_{1}, \dots, \mathbf{q}^{t}_{m}\}$ in each layer, we first compute the token-level bias $\mathbf{\Phi}^t = \{\mathbf{\phi}^{t}_{1}, \dots, \mathbf{\phi}^{t}_{m}\}$, with $\mathbf{\Phi}^t \in \mathbb{R}^{m \times d}$, to estimate the energetic degree within $\mathbf{Q}^{t}$ as follows:

\begin{equation}
\begin{aligned}
\mathbf{\phi}^t_j = \frac{(\mathbf{q}^{t}_{j} - {\mathbf{\bar{z}}^t})^2}{\mathbf{\sigma}^2}&,
\end{aligned}
\end{equation}
% \begin{equation}
% \begin{aligned}
% \mathbf{\phi}^t_j = \frac{(\mathbf{q}^{t}_{j} - {\mathbf{\bar{z}}^t})^2}{\mathbf{\sigma}^2}&,\\
% \mathbf{\sigma}^2 = \frac{\sum_{i=1}^{t} \sum_{j=1}^{m} \left( \mathbf{q}^i_j - \mathbf{\bar{z}}^t \right)^2}{mt-1},\mathbf{\bar{z}}^t =&\frac{\sum_{i=1}^{t}\sum_{j=1}^{m} \mathbf{q}^i_j}{mt},
% \end{aligned}
% \end{equation}
where $\mathbf{\sigma}^2$ and $\mathbf{\bar{z}}^t \in \mathbb{R}^{1 \times d}$ represent the variance and mean of the query vectors respectively, computed as follows:
\begin{equation}
\begin{aligned}
\mathbf{\sigma}^2 = \frac{\sum_{i=1}^{t} \sum_{j=1}^{m} \left( \mathbf{q}^i_j - \mathbf{\bar{z}}^t \right)^2}{mt-1},\mathbf{\bar{z}}^t =&\frac{\sum_{i=1}^{t}\sum_{j=1}^{m} \mathbf{q}^i_j}{mt}.
\end{aligned}
\end{equation}

Based on the above estimated degree, we can construct the \pq $\mathbf{Q}^{t}_{\text{probe}}$ for KV matching by reassigning the activated weights of each query vector according to the activation bias $\mathbf{\Phi}^t$:
\begin{equation}
\begin{aligned}
\mathbf{Q}^{t}_{\text{probe}} = \sum_{j=1}^{m}\frac{\Vert \mathbf{\phi}^t_j \Vert_1}{\Vert \mathbf{\Phi}^t \Vert_1}\mathbf{q}^{t}_{j}.
\end{aligned}
\end{equation}
Our object is to enhance the weight of query vectors for those anchor tokens. With this activated \pq, we can match more precise KV pairs $\mathbf{K}^{*}$ and $\mathbf{V}^{*}$ that contain semantically relevant information for pre-filling stage Eq.~\ref{eq:prefilling}.

\subsection{Dynamic KV Cut-off Mechanism}
\label{sec:co}
During the decoding stage, the quality of the predicted answer greatly depends on the top-$k$ relevant pairs $\mathbf{K}^{*}$ and $\mathbf{V}^{*}$. However, due to the sparse and irregular attention pattern across each layer, the selection of $k$ KV pairs is highly sensitive to the \pq $\mathbf{Q}^{t}_{\text{probe}}=\mathbf{q}^{t}$. Therefore, we propose a KV cut-off mechanism to dynamically determine $k$ based on information density assessment for $L$ transformer layers. Compared to the preset threshold, this mechanism dynamically removes redundant KV pairs and improves the recall of relevant ones within a limited KV budget.

In the $t$-th decoding step, we first calculate the similarity scores $\mathbf{S}^\ell = \{s_{1}^\ell, \dots, s_{n}^\ell \}$ between the \pq $\mathbf{Q}^{t}_{\text{probe}}$ and the cache of key vectors $\mathbf{K}^{t-1}_{\text{cache}}$ for the $\ell$-th transformer layer, where $n = |\mathbf{K}^{t-1}_{\text{cache}}|$. The similarity scores are computed using cosine similarity as follows:
\begin{equation}
s_i^\ell = \frac{\mathbf{Q}^{t}_{\text{probe}} \cdot \mathbf{K}^{t-1}_{\text{cache}}[i]}{\|\mathbf{Q}^{t}_{\text{probe}}\| \times \|\mathbf{K}^{t-1}_{\text{cache}}[i]\|}.
\label{eq:cosine_similarity}
\end{equation}
Then, we apply the softmax function to normalize them and convert them into probabilities.


Based on the similarity distribution $\mathbf{S}^\ell$, we define the information density $\Theta^\ell$ for the $\ell$-th layer using the entropy function as follows:
\begin{equation}
\begin{aligned}
\Theta^\ell= - \sum_{i=1}^{n} & \frac{e^{s_i^\ell}}{\sum_{j=1}^{n} e^{s_j^\ell}} \log \left( \frac{e^{s_i^\ell}}{\sum_{j=1}^{n} e^{s_j^\ell}} \right),
\label{eq:entropy}
\end{aligned}
\end{equation}
where a uniform distribution results in a higher information density $\Theta^\ell$ compared to more concentrated distributions.

% To reduce the I/O burden of KV cache transmission from the CPU to the GPU, previous studies typically set a fixed budget. 


Now with the information density, we focus on dynamically assigning the budget instead of  a fixed value  $k$  for each layer. Given a total budget $\text{B}_{kv}$, we process from shallow to deep layers in the order of transformer computation to avoid decoding delays. Consequently, for the $\ell$-th layer in the  $t$-th decoding step, the budget $\text{B}^\ell$ can be estimated as follows:
\begin{equation}
\begin{aligned}
\text{B}^\ell = \frac{{\Theta}^\ell}{{{\Theta}^\ell}+{\sum_{j=\ell+1 }^{L}{\bar{\Theta}^j}}}\times \text{B}_{kv},
\label{eq:budget}
\end{aligned}
\end{equation}
where $\text{B}_{kv}$ is initialized as $L \times k$ and updated by $\text{B}_{kv} \leftarrow \text{B}_{kv}- \text{B}^\ell$ after  processing 
 the $\ell$-th layer, and $\bar{\Theta}^j$ denotes the mean $\Theta^\ell$ for the remaining unprocessed layers. In this part, we aim to assign a larger budget to layers with higher information density, where many KV pairs are potentially relevant to the \pq $\mathbf{Q}^{t}_{\text{probe}}$ for the $t$-th decoding step. Conversely, for layers with lower density, the relevant KV pairs with higher similarity are more prominent, making 
 the irrelevant pairs  more likely to be discarded. Based on the above Eq.~\ref{eq:budget}, the denominator, which adds  $\Theta^\ell$  to the cumulative average density  $\sum_{j=\ell+1}^{L} \bar{\Theta}^j$  of the remaining layers, quantifies the overall contribution of both the current and subsequent layers. A higher ratio indicates that the current layer holds a more significant portion of the relevant KV pairs, justifying a larger allocation. Compared to using a fixed threshold for retrieval, this dynamic KV cut-off mechanism eliminates redundant KV pairs and improves the recall of relevant ones within the limited KV budget.
 

In summary, we present our two-stage method separately, where the activation-aware \pq module guarantees the quality of historical KV pairs and the cut-off mechanism effectively utilizes them. The entire process is depicted in Algorithm~\ref{alg} as shown below:

\RestyleAlgo{ruled}
\begin{algorithm}[!h]
\small
\SetAlgoLined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{Effective KV Retrieval for Long-context LLMs Inference}\label{alg}
\Input{
    \( L \): Total number of transformer layers;
    \( \mathbf{Q}^{t}_{\text{probe}} \): Probe-Query for the \( t \)-th step;\quad 
    \( \mathbf{K}^{t-1}_{\text{cache}} \): Cache of key vectors for the $t-1$-th step;
    \( \text{B}_{kv} \): Initial KV  budget (\( L \times k \))
}
\Output{
    \( \mathbf{K}^{*} \) and \( \mathbf{V}^{*} \): Selected KV pairs for inference
}

\LinesNumbered

\For{\( \ell \gets 1 \) \KwTo \( L \)}{
    \For{\( i \gets 1 \) \KwTo \( n \)}{
        \( s_i^\ell \gets \frac{\mathbf{Q}^{t}_{\text{probe}} \cdot \mathbf{K}^{t-1}_{\text{cache}}[i]}{\|\mathbf{Q}^{t}_{\text{probe}}\| \times \|\mathbf{K}^{t-1}_{\text{cache}}[i]\|} \);
    }
    \( \mathbf{P}^\ell \gets \text{Softmax}(\mathbf{S}^\ell) \);
    \( \Theta^\ell \gets -\sum_{i=1}^{n} P(s_i^\ell) \log P(s_i^\ell) \);
}

\For{\( \ell \gets 1 \) \KwTo \( L \)}{
    \( \text{B}^\ell \gets \frac{\Theta^\ell}{\Theta^\ell + \sum_{j=\ell+1}^{L} \bar{\Theta}^j} \times \text{B}_{kv} \);
    \( \text{B}_{kv} \gets \text{B}_{kv} - \text{B}^\ell \);
}

\For{\( \ell \gets 1 \) \KwTo \( L \)}{
    Sort \( \mathbf{K}^{t-1}_{\text{cache}} \) based on \( \mathbf{P}^\ell \) in descending order;
    Select top \( \text{E}^\ell \) KV pairs;
    Add selected KV pairs to \( \mathbf{K}^{*} \) and \( \mathbf{V}^{*} \);
}

\Return \( \mathbf{K}^{*} \), \( \mathbf{V}^{*} \);

\end{algorithm}
