


\begin{abstract}

Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step.
% \sout{To improve inference efficiency for long-context reasoning with restricted GPU memory, previous work has proposed a sliding-window approach. This method accumulates a set of historical \textbf{key-value} (KV) pairs and selectively retains its subsets at each inference step.}
However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as \pq in each sliding window to effectively represent the entire context, which is an approach overlooked by existing methods.
Thus, we propose \textbf{\name}, a training-free, \textbf{Act}ivation-aware approach that dynamically determines probe-\textbf{Q}uery and leverages it to retrieve the relevant \textbf{KV} pairs for inference.
Specifically, \name monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of \pq for retrieval at pre-filling stage. 
To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency. 
%Our source code is available at \url{https://anonymous.4open.science/r/ActQKV-DDE1}.
% \footnote{The source code is available at \url{https://anonymous.4open.science/r/ActQKV-DDE1}.}
\end{abstract}