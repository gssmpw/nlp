\appendix
\label{sec:appendix}
\input{latex/tables/complexity}
\begin{table*}[!t]
\centering  
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{lclrccc}
\toprule
Dataset & ID & Source & Avg len & Metric & Language & \#data \\
\midrule
\emph{Single-Document QA} \\
NarrativeQA & 1-1 & Literature, Film & 18,409 & F1 & English & 200 \\
Qasper & 1-2 & Science & 3,619 & F1 & English & 200 \\
MultiFieldQA-en & 1-3 & Multi-field & 4,559 & F1 & English & 150 \\

\midrule
\emph{Multi-Document QA} \\
HotpotQA & 2-1 & Wikipedia & 9,151 & F1 & English & 200 \\
2WikiMultihopQA & 2-2 & Wikipedia & 4,887 & F1 & English & 200 \\
MuSiQue & 2-3 & Wikipedia & 11,214 & F1 & English & 200 \\

\midrule
\emph{Summarization} \\
GovReport & 3-1 & Government report & 8,734 & Rouge-L & English & 200 \\
QMSum & 3-2 & Meeting & 10,614 & Rouge-L & English & 200 \\
MultiNews & 3-3 & News & 2,113 & Rouge-L & English & 200 \\

\midrule
\emph{Few-shot Learning} \\
TREC & 4-1 & Web question & 5,177 & Accuracy (CLS) & English & 200 \\
TriviaQA & 4-2 & Wikipedia, Web & 8,209 & F1 & English & 200 \\
SAMSum & 4-3 & Dialogue & 6,258 & Rouge-L & English & 200 \\

\midrule
\emph{Retrieval} \\

PassageRetrieval-en & 5-1 & Wikipedia & 9,289 & Accuracy (EM) & English & 200 \\

\midrule
\emph{Code Completion} \\
LCC & 6-1 & Github & 1,235 & Edit Sim & Python/C\#/Java & 500 \\
RepoBench-P & 6-2 & Github repository & 4,206 & Edit Sim & Python/Java & 500 \\
\bottomrule
\end{tabular}
}
\caption{An overview of the dataset statistics in LongBench~\cite{longbench}. `Avg len' (average length) is computed using the number of words for the English (code) datasets and the number of characters for the Chinese datasets. `Accuracy (CLS)' refers to classification accuracy, while `Accuracy (EM)' refers to exact match accuracy.}
\label{tab:dataset-long}
\vspace{-5mm}
\end{table*}

\newpage

\section{The Complexity of LLMs Inference}

In this section, we focus on the attention computation and analyze the complexity of exiting methods shown in \cref{tab:complexity} as follows:


\textbf{Standard Attention Mechanism.}
Under the standard attention mechanism, during the pre-filling stage, each token in the input sequence undergoes attention calculations with all other tokens, resulting in a time complexity of $O(N^2)$. In the decoding stage, as the context grows, the complexity of generating each new token increases accordingly. When generating the $t$-th token, the length of the context to be processed is $N+t$, so the total time complexity of the decoding stage is $O(\sum^{M}_{t=1}M(N+t)^2)$, which is approximately $O(N^2M+M^3)$. Since the decoding length $M$ is usually much smaller than the input sequence length $N$, the overall complexity can be simplified to $O(N^2+MN^2)$.

\textbf{Sliding Window Mechanism with KV Cache.}
The sliding window mechanism divides the input sequence into several windows of fixed size, each with a size of $m$. During the pre-filling stage, the processing complexity of the tokens within each window is $O(m^2)$, and the interaction complexity between the KV caches of the windows is approximately $O(N)$, so the overall time complexity is $O(\frac{N}{m} \times m^2) = O(mN)$, which is equivalent to $O(N^2)$ when the window size m is constant and linearly dependent on N. In the decoding stage, the decoding of each new token only needs to interact with the m tokens in the current window and some tokens in the adjacent windows, resulting in a total time complexity of $O(MN)$. Overall, the time complexity can be simplified to $O(N^2+MN)$.


\begin{figure*}[!ht] 
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./pics/radar_chart_long.pdf}
        \subcaption{Long-Bench~\cite{longbench}.}
        \label{fig:radar_long}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./pics/radar_chart_infty.pdf}
        \subcaption{$\infty$-Bench~\cite{infinitebench}.}
        \label{fig:radar_infty}
    \end{minipage}
    \caption{Performance comparison based on different models: LLaMA3-8B~\cite{llama3} v.s. Qwen2.5-7B~\cite{qwen2.5}.}
    \label{fig:models}
\end{figure*}
\textbf{KV Retrieval for Long-Context Inference.}
When using the method of Top-k retrieval combined with the sliding window, the pre-filling stage divides the input sequence into windows of fixed size. During the processing of the tokens in each window, only the interaction with the top k most relevant key-value pairs is performed, so the complexity of the pre-filling stage is $O(\frac{N}{m}\times(m+k))$, which can be approximated as $O(kN)$ if the window size m and the retrieval range k meet certain conditions. In the decoding stage, the prediction of each new token only needs to interact with the top-$k$ most relevant key-value pairs, with a time complexity of $O(kM)$. Overall, the time complexity is simplified to $O(kN+kM)$.



\begin{table}[!ht]
    \footnotesize
    \centering
    \begin{tabular}{l|cccc}
        \toprule
        \textbf{Task}  & \textbf{Annotation}  & \textbf{\# Ex.} & \textbf{Avg Len} \\
        \midrule
        Ret.PassKey    & Auto & 590 & 122.4K/2\\
        Ret.Number     & Auto & 590 & 122.4K/4\\
        Ret.KV         & Auto & 500 & 121.1K/22.7\\
   
        En.MC       & Human & 229 & 184.4K/5.3\\

        Code.Debug  & Human & 394 & 114.7K/4.8\\

        Math.Find   & Auto & 350 & 87.9K/1.3 \\
        \bottomrule
    \end{tabular}
    \caption{Data statistics of $\infty$-Bench~\cite{infinitebench}. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.}
    \label{tab:dataset-infty}
\end{table}

\begin{figure*}[!ht] 
    \centering
        \includegraphics[width=1.0\textwidth]{./pics/dynamic-topk-analyse.pdf}
    \caption{Average number of relevant KV pairs recalled for each  layer in decoding stage based on \basellama~\cite{llama3}. We
randomly select 50 samples from Long-Bench and filter out those with a length less than 8K. } 
    \label{fig:recall}
\end{figure*}
\section{Details in Long-Bench and $\infty$-Bench}
\label{sec:alg}
Long-Bench (95\% sequence length is 32K) focuses on tasks that involve reasoning, such as question answering, summarization, few-shot learning, retrieval, and coding.  The groups of datasets are categorized as follows: \textbf{Single-doc QA}: NarrativeQA, Qasper, MultiFieldQA; \textbf{Multi-doc QA}: HotpotQA, 2WikiMQA, Musique; \textbf{Summarization}: GovReport, QMSum, MultiNews; \textbf{Few-shot Learning}: TREC, TriviaQA, SAMSum; \textbf{Retrieval}: PassageRetrieval; \textbf{Code}: RepoBench-P. And $\infty$-Bench (avg. length of 200K) emphasizes factual retrieval, covering domains such as code, mathematics, multiple-choice questions, and general retrieval tasks. The statistics and evaluation metrics of datasets are detailed in ~\cref{tab:dataset-long} and ~\cref{tab:dataset-infty}.


\section{Experimental Results}
\label{sec:experiments}
All experiments were implemented using PyTorch and performed on two NVIDIA A800 80GB GPUs. In all experiments in this paper, we use standard greedy decoding to ensure reliable results.

\subsection{Model Comparison}
We conduct experiments on Long-Bench~\cite{longbench} and $\infty$-Bench~\cite{infinitebench} using LLaMA3-8B~\cite{llama3} and Qwen2.5-7B~\cite{qwen2.5}, as illustrated in ~\cref{fig:models}. 

For LLaMA3-8B~\cite{llama3}, the model achieves state-of-the-art (SOTA) performance across tasks in both Long-Bench and $\infty$-Bench, demonstrating its versatility, particularly in factual retrieval and code-related tasks.
In contrast, although Qwen2.5-7B~\cite{qwen2.5} does not match the performance of LLaMA3-8B across all categories, it exhibits substantial improvements over the baseline. The most significant performance drop is observed in the Retrieval tasks, where Qwen2.5-7B underperforms relative to LLaMA3-8B. This highlights a challenge in handling retrieval-related aspects of the benchmarks. Nevertheless, Qwen2.5-7B consistently outperforms the baseline in these tasks, underscoring the effectiveness of our approach, even though it does not yet match the top-performing model in retrieval. However, Qwen2.5-7B excels in code-related tasks, even surpassing LLaMA3-8B in this domain. This demonstrates the modelâ€™s proficiency in handling complex, domain-specific tasks, such as those encountered in RepoBench-P. While Qwen2.5-7B shows some weaknesses in retrieval, its performance in other specialized areas is either competitive or superior.

In total, although Qwen2.5-7B experiences a decline in retrieval task performance compared to LLaMA3-8B, it still outperforms the baseline, validating the effectiveness of our method, \name.

\subsection{Dynamic KV Pairs Recall}

Our approach employs a layer-wise key-value cut-off mechanism and an activation-aware \pq construction strategy to more effectively match and recall relevant KV pairs. As shown in ~\cref{fig:recall}, we report the average number of relevant KV pairs recalled for each layer.

The results in ~\cref{fig:analysis} demonstrate that our method, \name, adapts to the varying distributions across layers, ensuring a robust and efficient retrieval process. Notably, in layer 13, which exhibits the lowest perplexity of similarity scores and receives the smallest KV budget, our method fully aligns with the objectives outlined in ~\cref{eq:budget}. This consistency allows LLMs to effectively process long-context information for long-context inference.





