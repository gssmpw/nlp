\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}
\usepackage{subfigure}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{placeins}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
% more packages
\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta,chains,positioning,shapes.geometric}
\usepackage{enumerate}
\usepackage{empheq}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{soul} % pour barrer du texte, pour les remarques
%\usepackage{floatrow}
\usepackage{caption}
\usepackage{stmaryrd}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{import}

%---------------------%
% Attempt to make hyperref and algorithmic work together better:

%\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\newcommand{\R}{\mathbb{R}}
\def\cC{{\mathcal C}}
\def\cN{{\mathcal N}}
\def\fS{{\mathfrak S}}
\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\P{{\mathbb P}}
\def\K{{\mathbb K}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bn}{\boldsymbol{n}}

\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\balpha}{\alpha}
\newcommand{\bId}{\boldsymbol{Id}}


%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{O-MMGP: Optimal Mesh Morphing Gaussian Process Regression}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}




%% Title
\title{O-MMGP: Optimal Mesh Morphing Gaussian Process Regression for Solving PDEs with non-Parametric Geometric Variations
%%%% Cite as
%%%% Update your official citation here when published 
%%%%\thanks{\textit{\underline{Citation}}: 
%%%%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Abbas Kabalan$^{1,2}$, Fabien Casenave$^{1}$, Felipe Bordeu$^{1}$ and Virginie Ehrlacher$^{2,3}$ \\
     $^1$Safran Tech, Digital Sciences Technologies - 
  78114 Magny-Les-Hameaux, France\\
  $^2$ CERMICS, ENPC, Institut Polytechnique de Paris, Marne-la-Vall√©e, France,\\
  $^3$ Inria Paris, CS 61534, 75647 Paris cedex, FRANCE. \\
  \texttt{\{abbas.kabalan, fabien.casenave, felipe.bordeu\}@safrangroup.com}\\
    \texttt{\{viginie.ehralcher\}@enpc.fr}
    }

\begin{document}

\maketitle

\begin{abstract}
We address the computational challenges of solving parametric PDEs with non parametrized geometric variations and non-reducible problems, such as those involving shocks and discontinuities of variable positions. Traditional dimensionality reduction methods like POD struggle with these scenarios due to slowly decaying Kolmogorov widths. To overcome this, we propose a novel non-linear dimensionality reduction technique to reduce the required modes for representation. The non-linear reduction is obtained through a POD after applying a transformation on the fields, which we call \textit{optimal mappings}, and is a solution to an optimization problem in infinite dimension. The proposed learning framework combines morphing techniques, non-linear dimensionality reduction, and Gaussian Process Regression (GPR). The problem is reformulated on a reference geometry before applying the dimensionality reduction. Our method learns both the optimal mapping, and the solution fields, using a series of GPR models, enabling efficient and accurate modeling of complex parametric PDEs with geometrical variability. The results obtained concur with current state-of-the-art models. We mainly compare our method with the winning solution of the ML4CFD NeurIPS 2024 competition.  \end{abstract}

\section{Introduction}\label{section:introduction}
\subsection{Background}
Many scientific and engineering challenges involve solving complex boundary value problems, often formulated as parametric partial differential equations (PDEs). These problems require exploring the influence of varying parameters such as material properties, boundary and initial conditions, or geometric configurations. Traditional numerical methods like the finite element method and finite difference methods, while accurate, are computationally expensive, particularly when repeated evaluations are necessary across a large and high-dimensional parameter set. To address this computational burden, techniques in model-order reduction and machine learning have been developed, offering efficient approximations without compromising accuracy.

Model-order reduction techniques, such as the reduced-basis method \cite{quarteroni2015reduced,hesthaven2016certified}, construct low-dimensional approximation vector spaces to represent the set of solutions of the parametric PDE, enabling fast computation for new parameter values. These approaches typically involve an offline phase, where high-fidelity models are used to generate a reduced basis, and an online phase, where this basis is employed for efficient computations. However, when the physical domain varies with the parameters, standard methods like Proper Orthogonal Decomposition (POD) face challenges due to the need to reconcile solutions defined on different domains. This often requires morphing techniques to map variable domains to a common one, transforming the problem into a form suitable for reduced-order modeling \cite{manzoni2017efficient,salmoiraghi2018free}. Linear reduced-order modeling techniques also face huge difficulties when the solution set of interest has a slowly decaying Kolmogorov width. Such problems, which we call herefafter non-reducible problems, are common for PDEs that involve shocks of variable position, discontinuities, boundary layers an so on. 
%Linear dimensionality reduction techniques such as the POD (proper orthogonal decomposition) are usually not very well suited in this case. The slowly decaying Kolmogorov width implies that a large number of modes need to be retained in order to capture the solution accurately.

On the other hand, machine learning methods, particularly those relying on deep learning, have shown promise in solving PDEs, learning solutions, and accelerating computations \cite{guo2016convolutional,lu2019deeponet}. Approaches relying on graph neural networks (GNNs) \cite{scarselli2008graph} have been particularly effective in handling unstructured meshes and varying geometric configurations. Despite their flexibility, these methods require substantial computational resources and large training datasets, and they often lack robust predictive uncertainty estimates.

\subsection{Contribution}
In this work, we propose a novel approach that integrates principles of morphing, non-linear dimensionality reduction, and classical machine learning to address the challenges posed by parametric PDEs with geometric variations for non-reducible problems. The main contribution is a novel algorithm used to minimize the number of modes needed to approximate non-reducible problems in moderate dimension. Such approaches are commonly called registration in the literature. Unlike most exsiting algorithms that aim at projecting the samples on one single mode, our approach is based on the resolution of an optimization problem in infinite dimension that may involve an arbitrary number of modes $r\in \mathbb{N}^*$, allowing topology changes in the fields of interest.



%More precisely, we start by recasting the problem with variable geometries to a fixed reference domain using mesh morphing techniques.  Once on the reference domain, we perform an optimization problem, commonly known as registration problem, to minimize the number of modes needed to approximate the problem in small dimension. Finally
%Our method achieves computational efficiency, predictive uncertainty estimation, and adaptability to varying domain geometries, making it a versatile tool for industrial and engineering applications where repeated PDE evaluations are required.

\subsection{Related works}
The challenges mentioned in this work have been extensively studied in the literature. Morphing techniques have been used for various applications in reduced order modeling to recast the problem on a reference domain \cite{porziani2021automatic,axelsson2015continuation,ye2024data}. Numerous approaches were also proposed to deal with non-reducible problems by using non-linear dimensionality reduction such as registration \cite{taddei2020registration,taddei2023compositional}, feature tracking \cite{mirhoseini2023model}, optimal transport \cite{cucchiara2024model}, neural networks \cite{lee2020model,fresca2022pod,barnett2023neural,berman2024colora}, and so on. Machine learning approaches leveraging neural networks have also demonstrated remarkable success in solving numerical simulations, particularly in capturing complex patterns and dynamics. Methods such as the Fourier Neural Operator (FNO) \cite{li2020fourier} and its extension, Geo-FNO \cite{li2023fourier}, efficiently learn mappings between function spaces by leveraging Fourier transforms for high-dimensional problems. Physics-Informed Neural Networks (PINNs) \cite{raissi2019physics} embed physical laws directly into the loss function, enabling solutions that adhere to governing equations. Deep Operator Networks (DeepONets) \cite{lu2021learning} excel in learning operators with small data requirements, offering flexibility in various applications. Mesh Graph Networks (MGNs) \cite{pfaff2020learning} use graph-based representations to model simulations on irregular domains, preserving geometric and topological properties. 

\section{Preliminaries and notations}
Let $n\in \N ^*$ and $\Omega_1 , \ldots , \Omega_n  \subset \R^d$ to be a family of $n$ distinct domains that share a common topology, with $d=2,3$. We suppose the parametrization of the domains is unknown and that for all $1\leq i \leq n$, each domain $\Omega_i$ is equipped with a (non-geometrical) parameter $\mu_i \in \mathcal{P}$  where $\mathcal{P} \subset \R^p$ is a set of parameter values (think of $\mu_i$ as being some material parameter value for instance). In addition, let $u_i: \Omega_i \to \mathbb{R}$ be the solution of a parametrized partial differential equation for the parameter value $\mu_i$ defined on $\Omega_i$ (think about a temperature field for instance). We assume in the following that for all $1\leq i \leq n$, $u_i \in L^2(\Omega_i)$. We assume that for all $1\leq i \leq n$ a finite element mesh $\mathcal{M}_i$ is chosen so that $\partial \mathcal M_i$ can be considered as an accurate enough approximation of the boundary of the domain $\partial \Omega_i$. We also assume the field $u_i$ can be accurately approximated by its finite element interpolation associated to the mesh $\mathcal M_i$. We fix a reference domain, denoted by $\Omega_0$, equipped with a mesh denoted by $\mathcal{M}_0$, that shares the same topology as the other domains. For all $0\leq i \leq n$, $\mathbf{M}_i$ be the space of bijective $W^{1, \infty}$ mappings from $\Omega_0$ onto $\Omega_i$, so that for all $\bpsi \in \mathbf{M}_i, \bpsi(\Omega_0)=\Omega_i$. We also introduce $\mathbf{M}:=\mathbf{M}_1 \times \mathbf{M}_2 \times \cdots \times \mathbf{M}_n$. 

For any family of functions $\mathcal{H}= (h_i)_{1\leq i\leq n} \in L^2(\Omega_0)^n$, we define the correlation matrix application $C_{\mathcal H}$ of $\mathcal{H}$ as the map
\begin{align}
    C_{\mathcal H} : \hspace{1em} &\mathbf{M}_0^n \to \mathcal S_n^+ \nonumber\\
        & \Phi \to C_{\mathcal H}[\Phi] := \left( C_{\mathcal H, ij}[\Phi]\right)_{1\leq i,j \leq n},   \label{correlation matrix}
\end{align}
 where $\mathcal S_n^+$ is the set of symmetric non-negative semi-definite matrices of dimension $n \times n$, and for all $\Phi:=(\bphi_i)_{1\leq i\leq n}\in \mathbf{M}_0^n$ and for all $1\leq i,j\leq n$, 
 $$
 \displaystyle C_{\mathcal H, ij}[\Phi]:= \langle h_i \circ \bphi_i, h_j \circ \bphi_j \rangle_{L^2(\Omega_0)}= \int_{\Omega_0}h_i \circ \bphi_i(x)  h_j \circ \bphi_j(x) dx.
 $$
 We denote by $\displaystyle \lambda_1^{\mathcal H}[\Phi] \geq \lambda_2^{\mathcal H}[\Phi] \geq \cdots \geq \lambda_n^{\mathcal H}[\Phi]$ the eigenvalues of $C_{\mathcal H}[\Phi]$ counting multiplicity and ranged in non-increasing order. We also denote by $(\displaystyle \zeta_1^{\mathcal H}[\Phi] , \zeta_2^{\mathcal H}[\Phi] , \cdots , \zeta_n^{\mathcal H}[\Phi]) \subset \mathbb{R}^n$ an orthonormal family of corresponding eigenvectors. Finally, given a positive integer $r\in \mathbb{N}\setminus\{0\}$, we define the functional
\begin{align}
    J_{\mathcal H,r} : \hspace{1em} & \mathbf{M}_0^n \to \R \nonumber \\
       & \Phi \mapsto J_{\mathcal H,r} [\Phi] :=  \frac{ \displaystyle \sum_{j=1}^r \lambda_j^{\mathcal H}[\Phi] }{Tr(C_{\mathcal H}[\Phi])}. \label{J}
\end{align}
%The functional $J_{\mathcal H,r}$ represents the (relative) energy contained in the first $r$ principal components of the PCA of the family of functions. 

We now introduce the terminology of three different configurations that will be used throughout the paper.
\begin{enumerate}
    \item First, we refer to the \textbf{physical configuration} as the collection of pairs $\{(\Omega_i,u_i)\}_{1\leq i\leq n}$.
    \item Given a reference domain $\Omega_0$ and $\Phi^{\rm geo}:=(\bphi^{\rm geo}_i)_{1\leq i\leq n} \in \mathbf{M}$, we refer to the \textbf{reference configuration} as the collection of pairs $\{(\Omega_0,u_i \circ \bphi^{\rm geo}_i)\}_{1\leq i\leq n}$. In this configuration, all the fields $u_i^{\rm ref} := u_i \circ \bphi^{\rm geo}_i$ belong to $L^2(\Omega_0)$, and classical dimensionality reduction techniques such as PCA can be applied on the family $\mathcal U:= (u_i^{\rm ref})_{1\leq i\leq n}\in L^2(\Omega_0)^n$. 
    \item Given the reference configuration and some $r\in \mathbb{N}\setminus \{0\}$, we refer to the \textbf{r-optimal configuration}  as the collection of pairs $\{(\Omega_0,u_i^{\rm ref}\circ\bphi^{\rm opt}_{i,r})\}_{1\leq i\leq n}$, where $\Phi^{\rm opt}_r:=(\bphi^{\rm opt}_{i,r})_{1\leq i\leq n} \in \mathbf{M}_0^n$ is a solution to the following maximization problem:
\begin{align}\label{optimization statement}
    \text{find } \Phi^{\rm opt}_r \in \arg \max_{\Phi \in \mathbf{M}_0^n} J_{\mathcal U, r}[\Phi].
\end{align} 

The maximization problem (\ref{optimization statement}) is considered so that the family $(u_i^{\rm opt})_{1\leq i\leq n}:= (u_i^{\rm ref}\circ\bphi^{\rm opt}_{r,i})$ can be accurately approximated by elements of a $r$-dimensional vector space.


\end{enumerate}
\section{Methodology}
 We present the methodology proposed in this paper for the training and inference phases.
\subsection{Training phase}
In the training phase, we suppose that we have access to the dataset of triplets $\{(\Omega_i,\mu_i,u_i)\}_{1\leq i\leq n}$. The domain $\Omega_i$ (or its mesh) and the parameter $\mu_i$ are the inputs to the physical solver, and the field $u_i$ is its output. We chose a reference domain (which can be one from the dataset) that shares the same topology.
\subsubsection{Pretreatment}
We perform these pretreatment steps in the training phase. 
\begin{enumerate}
    \item We pass from the physical configuration to the reference configuration (\ref{sec:geometrical mapping}) by computing for all $1\leq i\leq n$, a mapping $\bphi^{\rm geo}_i \in \mathrm{M}_i$. We apply POD on the family $(\bphi^{\rm geo}_i  -\bId)_{1\leq i\leq n}$ to obtain the POD modes $\{\bzeta^{\rm geo}_i\}_{1\leq i \leq n}$  and the generalized coordinates $\{\alpha^i\}_{1\leq i \leq n}$ where $\alpha^i = \left( \alpha^i_j \right)_{1\leq j \leq s}\in \mathbb{R}^{s}$, such that $$
\forall 1\leq j \leq s, \; \alpha_j^i= \langle \bphi^{\rm geo}_i -\bId , \bzeta^{\rm geo}_j \rangle_{\boldsymbol{L}^2(\Omega_0)}, 
$$ and $s$ is the number of retained modes for the geometrical mappings. Each domain $\Omega_i$ is defined now by the vector $\alpha^i$. 
\item We pass from the reference configuration to the optimal configuration (\ref{sec: optimal mapping}) by solving problem \ref{optimization statement}. Once we obtain the functions $(\bphi^{\rm opt}_i -\bId)_{1\leq i\leq n}$, we apply POD to obtain the POD modes $\{\bzeta^{\rm opt}_i\}_{1\leq i \leq n}$  and the generalized coordinates $\{\beta^i\}_{1\leq i \leq n}$ where $\beta^i = \left( \beta^i_j \right)_{1\leq j \leq t}\in \mathbb{R}^{t}$, such that $$
\forall 1\leq j \leq t, \; \beta^i_j= \langle \bphi^{\rm opt}_i  -\bId , \bzeta^{\rm opt}_j \rangle_{\boldsymbol{L}^2(\Omega_0)}, 
$$ and $t$ is the number of retained modes for the optimal mappings. 
\item Finally, after evaluating $\{u_i^{\rm opt}\}_{1\leq i\leq n}$, we apply POD to obtain the POD modes $\{\psi_i\}_{1\leq i \leq n}$  and the generalized coordinates $\{\gamma^i\}_{1\leq i \leq n}$ where $\gamma^i = \left( \gamma^i_j \right)_{1\leq j \leq r}\in \mathbb{R}^{r}$, such that $$
\forall 1\leq j \leq r, \; \gamma^i_j= \langle \psi_i, u_j^{\rm opt} \rangle_{\boldsymbol{L}^2(\Omega_0)}, 
$$
\end{enumerate}
After applying these dimensionality reductions, we obtain the following three approximations:
$\displaystyle
 \bphi^{\rm ref}_i \approx \bId + \sum_{j=1}^s \alpha^i_j  \bzeta^{\rm ref}_j, \bphi^{\rm opt}_i \approx \bId + \sum_{j=1}^t \beta^i_j  \bzeta^{\rm opt}_j,$ and $ \displaystyle u_i^{\rm opt} \approx  \sum_{j=1}^r \gamma^i_j  \psi_j.
$

%\begin{align}
%    &d_{\Omega_i} \approx  \sum_{j=1}^q d^i_j  \theta_j \\
%    &    \bphi^{\rm ref}_i \approx  \sum_{j=1}^s \alpha^i_j  \bzeta^{\rm ref}_j \\
%    &    \bphi^{\rm opt}_i \approx  \sum_{j=1}^t \beta^i_j  \bzeta^{\rm opt}_j\\
%    &u_i^{\rm opt} \approx  \sum_{j=1}^r \gamma^i_j  \psi_j
%\end{align}
Notice that the passing from the physical configuration to the reference configuration is decoupled and purely geometrical, that is it does not depends on the fields $\{u_i\}_{1\leq i\leq n}$. For example, we show in Figure \ref{fig: phi_geo} two forms of airfoils. $\bphi^{\rm geo}$ will transform one airfoil onto the other. On the other hand, the transition to the optimal configuration is coupled between all the samples in the training dataset. 


\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.13]{phi_geo.png}
\caption{ Two airfoils superimposed.}
\label{fig: phi_geo}
\end{figure}


\subsubsection{Training}

After performing the dimensionality reduction step, we train two Gaussian processes regression models \cite{rasmussen2006gaussian} as follows. 

\begin{enumerate}
    \item The first model is to learn the optimal mapping that transforms the reference configuration to the optimal configuration. This model takes as input the physical parameter $\mu_i$ and the geometrical mapping POD coefficient $\alpha_i$, and as output the optimal mapping POD coefficient $\beta_i$. We denote this model by $\mathcal R: \mathbb{R}^p \times \mathbb{R}^q \to \mathbb{R}^t$.
    \item The second model is to learn the field in the optimal configuration. This model takes as input the physical parameter $\mu_i$ and the geometrical mapping POD coefficient $\alpha_i$, and as output the field in the optimal configuration POD coefficient $\gamma_i$. We denote this model by $\mathcal O: \mathbb{R}^p \times \mathbb{R}^q \to \mathbb{R}^r$.
\end{enumerate}

\subsection{Inference phase}
In the inference phase, we are given a new unseen geometry $\widetilde{\Omega}$ in the inference phase, with a physical parameter $\widetilde{\mu}$. The goal is to predict the field of interest $\widetilde{u}$, solution to the physical simulation. We proceed in the following manner.
\begin{enumerate}
    \item First, we compute the geometrical mapping $\widetilde{\bphi}^{\rm geo}$ that maps $\Omega_0$ onto $\widetilde{\Omega}$, then we project $\widetilde{\bphi}^{\rm geo}-\bId$ on the POD basis $\{\bzeta^{\rm geo}_i\}_{1\leq i \leq n}$ to obtain the coefficient $\widetilde{\alpha}$.
    \item We use the pair $(\widetilde{\alpha},\widetilde{\mu})$ to infer $\widetilde{\beta}= \mathcal R (\widetilde{\alpha},\widetilde{\mu})$ and $\widetilde{\gamma}= \mathcal O (\widetilde{\alpha},\widetilde{\mu})$. We  obtain $\displaystyle \widetilde{\bphi}^{\rm opt} :=  \bId +\sum_{j=1}^t \widetilde{\beta}_j  \bzeta^{\rm opt}_j$ and $\displaystyle \widetilde{u}^{\rm opt} :=  \sum_{j=1}^r \widetilde{\gamma}_j  \psi_j$.    
    \item Finally, the quantity of interest $\widetilde{u}$ is obtained as $$\displaystyle \widetilde{u}:= \widetilde{u}^{\rm opt} \circ \left(\widetilde{\bphi}^{\rm opt}\right)^{-1} \circ \left(\widetilde{\bphi}^{\rm geo}\right)^{-1}.$$
\end{enumerate}

\section{Geometrical mapping}\label{sec:geometrical mapping}

Numerous techniques exists in the literature to construct a mapping between domains \cite{baker2002mesh,beg2005computing,taddei2022optimization}. In this work, we use RBF morphing \cite{de2007mesh} to construct the mapping from the reference domain $\Omega_0$ to each target domain $\Omega_i$. When using this technique, we suppose that the deformation on a subset of points in $\Omega_0$, called the control points, is known. These points are usually on the boundary of $\Omega_0$. Then, we can leverage the knowledge of $\bphi_i(\partial \Omega_0) = \partial \Omega_i$ to compute the deformation in the bulk of the domain $\bphi_i(\Omega_0) $. In this work, we suppose that the geometries are not parametrized and $\bphi_i(\partial \Omega_0)$ is not given. Thus, we start by computing $\bphi_i(\partial \Omega_0)$, for all $1 \leq i\leq n$, which is case dependent in this work. When computing $\bphi_i(\partial \Omega_0)$ is not available, methods as proposed in \cite{de2016optimization,kabalan2025elasticity} provides solutions to automatically finds a mapping from the reference onto the target domain. We refer to  appendix \ref{appendix: rbf} for a brief discussion on the RBF morphing method.

\section{Optimal mapping} \label{sec: optimal mapping}
The second major building block we introduce is the optimal mapping algorithm to compute $\Phi^{\rm opt}=(\bphi^{\rm opt}_i)_{1\leq i\leq n}$ that maximize the energy in the first $r$ modes. 

In order to pass from the reference configuration to the optimal configuration, we solve \eqref{optimization statement}: 
\begin{align*}
    \text{find } \Phi^* \in \arg \max_{\Phi \in \mathbf{M}_0^n} J_{\mathcal U,r}[\Phi].
\end{align*} 
The optimization problem presented here is non-convex with possibly infinitely many solutions. We present here briefly the optimization strategy employed to solve this problem. Refer to the appendixes for a detailed discussion. 
\subsection{Gradient algorithm in infinite dimension}
We can show that the differential of $J_{\mathcal U,r}$ with respect to $\Phi$ at a point $\Psi$ has the following expression (see appendix \ref{appendix: diff J}):
$$
D J_{\mathcal U,r}[\Phi][\Psi] = \sum_{i=1}^n \int_{\Omega_0} \f_i[\Phi] \cdot \bpsi_i dx.
$$ 
with 
\begin{align}\label{f_i expression}
\f_i [\Phi] :=\sum_{j=1}^n \theta_{ij} u_j^{\rm ref} \circ \bphi_j \Vec{\nabla} u_i^{\rm ref}\circ\bphi_i,
\end{align}
and $\displaystyle \theta_{ij} := \sum_{k=1}^r \left( \frac{ 2\zeta^{\mathcal U}_{k,i}[\Phi]\zeta^{\mathcal U}_{k,j}[\Phi] }{Tr (C_{\mathcal U}[\Phi]) } -\frac{ \displaystyle 2 \lambda_k^{\mathcal U}[\Phi]}{Tr (C_{\mathcal U}[\Phi])^2 }\delta_{ij}\right)$.\\
A standard gradient algorithm in infinite dimension consists of (i) choosing an appropriate inner product, denoted here by $a$, (ii) computing the Riesz representation of each $\f_i [\Phi]$ with respect to $a$, which will be denoted by $\bu_i[\Phi]$ and finally (iii) updating, at each iteration $m$, $\bphi_i^{(m)}$ by
\begin{align}
    \bphi_i^{(m+1)}= \bphi_i^{(m)} + \epsilon \bu_i[\Phi^{(m)}]
\end{align}
with $\epsilon > 0$ in the gradient step. The simplest example of $a$ is the $L_2(\Omega_0)$ inner product which gives the following iterative scheme: 
\begin{align}
    \bphi_i^{(m+1)}= \bphi_i^{(m)} + \epsilon \f_i[\Phi^{(m)}].
\end{align}
However, this may suffer from one of the following problems:
\begin{itemize}
    \item $\bphi_i^{(m+1)}$ might not be bijective.
    \item $\bphi_i^{(m+1)}$ might not map $\Omega_0$ onto itself.
    \item If the function $\Phi^{(m)}$ is far from a maximum, the algorithm may fail to converge properly.
\end{itemize}

We address these issues in the following subsections.
\subsection{Bijectivity}
To address bijectivity, we consider the following optimization statement: 
\begin{align} \label{eq: I definition}
    \text{find } \Phi^* \in \arg \max_{\Phi \in \mathbf{M}^n_0} I_{\mathcal U,r}[\Phi]:= J_{\mathcal U,r}[\Phi] - c_1 \sum_{i=1}^n E[\bphi_i],
\end{align}
where $E$ is an energy term to enforce bijectivity and $c_1>0$ is a penalization parameter. Ideally, the term $E$ should diverge to $+\infty$ if the mapping $\bphi_i$ becomes non-bijective. In this case, the objective function $I$ diverges to $-\infty$. One example for the energy term is the elastic energy for non-linear Neohookean model. In this work, we use linear elastic energy for the term $E$, (we note that it does not diverges to $+\infty$ for non-bijective mappings, however, it gives acceptable results). Finally, we solve the problem $\ref{eq: I definition}$ using the continuation method, which consists of iteratively decrease the parameter $c_1$ in order to obtain a solution to problem $\ref{optimization statement}$. We give more details about the continuation method and the term $E$ in appendix \ref{Continuation method}.
\subsection{Mapping condition}
To guarantee that $\bphi_i$ always maps $\Omega_0$ onto itself, we partition the boundary of $\Omega_0$ to $\partial \Omega_0 := \partial \Omega_0^p \bigcup \partial \Omega_0^c $ with $\partial \Omega_0^p$ is the union of all the faces (edges in $2d$) of $\partial \Omega_0$ and $\partial \Omega_0^c$ is the curved part of $\partial \Omega_0$. We define the space $H_{\bn}^1(\Omega_0):=\{\bu \in H^1(\Omega_0) : \bu \cdot \bn=0 \text{ on } \partial \Omega_0^p, \bu=0 \text{ on } \partial \Omega_0^c \}$
 
and the inner product $a$ on $H_{\bn}^1(\Omega_0)$ as 
\begin{align} \label{inner product}
     (\bu,\bv) \mapsto a(\bu,\bv) := \int_{ \Omega_0} \sigma ( \bu):\varepsilon (\bv) d\mathbf{x},
\end{align}
with $\sigma$ and $\varepsilon$ are respectively the elasticity stress and strain tensors. We compute, for all $1\leq i \leq n$, and for each iteration $m$, the Riesz representation $\bu_i^{(m)}$ of $\f_i$ with respect to this inner product, which is the unique solution to
\begin{align}\label{Riesz representation f}
        \forall \bv \in H_{\bn}^1({\Omega_0;\R^d}), \quad a(\bu_i^{(m)},\bv)= \int_{ \Omega_0}  \f_i[\Phi^{(m)}] \bv d\mathbf{x},
\end{align}
to obtain the iterative scheme: 
\begin{align}
    \bphi_i^{(m+1)}= \bphi_i^{(m)} + \epsilon \bu_i[\Phi^{(m)}].
\end{align}
We can also obtain a similar expression when solving problem $\eqref{eq: I definition}$ and using linear elastic energy (see more in appendix \ref{Continuation method}):
\begin{align}\label{gradient update I elasticity}
\bphi^{(m+1)}_i=\bphi^{(m)}_i + \epsilon \left(\bu_i[\Phi^{(m)}]- c_1 \left(\bphi_i^{(m)}-Id\right)\right). 
\end{align}
By using the above procedure, we guarantee that, if $\bphi_i^{(m)} + \epsilon \bu_i[\Phi^{(m)}]$ is bijective, then necessary we have that the points on $\partial \Omega_0^p$ stay on $\partial \Omega_0^p$, as they are only allowed to move tangentially, while points on $\partial \Omega_0^c$ stay fixed. Thus, $\bphi_i^{(m)} + \epsilon \bu_i[\Phi^{(m)}]$ preserves the boundary of $\Omega_0$. We note however that this is suboptimal as we do not allow $\partial \Omega_0^c$ to deform in this case. A full treatment of curved boundaries is the subject of future work. One possible solution would be to always choose the reference domain $\Omega_0$ to be a polytope (thus $\partial \Omega_0=\partial \Omega_0^p$) .
\subsection{Uncorrelated samples} \label{section: convlution}
One major reason for the poor convergence of the gradient algorithm is when the samples are heavily not correlated. This happens when the non-zero values of $u_i^{\rm ref}$ are compactly supported in $\Omega_0$. More precisely, if, for some $1 \leq j \leq n$, $\rm{supp}$$(u_j^{\rm ref} \circ \bphi_j)\bigcap \rm{supp}$$(\Vec{\nabla} u_i^{\rm ref}\circ\bphi_i) = \emptyset$, then the contribution of $u_j^{\rm ref} \circ \bphi_j \Vec{\nabla} u_i^{\rm ref}\circ\bphi_i$ in $\f_i$ is null. To this end, we transform the fields $\{u_i\}_{1\leq i\leq n}$ to the family of fields $\widehat{\mathcal{U}}(c_2):=\{\widehat{u}_i\}_{1\leq i\leq n}$, where each is defined as the solution to
\begin{align}\label{reaction diffusion}
    - &\Delta \widehat{u}_i + c_2 \widehat{u}_i = c_2 u_i,\\
    &\partial_n \widehat{u}_i =0 \nonumber
\end{align}
where $\Delta$ is the Laplacian operator and $c_2>0$. This equation has a unique solution. The transformed fields will diffuse the value of $u_i$ throughout the domain $\Omega_0$ for small values of $c_2$ (see Figures \ref{fig: nut_example}-\ref{fig: diffusion ex}). On the other hand, $\widehat{u}_i$ converges to $u_i$ as $c_2$ goes to $+\infty$. We use the fields $ \widehat{\mathcal{U}}(c_2)$ in order to solve \eqref{optimization statement}-\eqref{eq: I definition}. The value of $c_2$ is also changed throughout the iterations using the continuation method.
\section{Numerical examples}
In this section, we illustrate the method on two examples of non-reducible problems. 
\subsection{Example 1: advection-reaction equation }
This example is taken from \cite{taddei2020registration}, and it illustrates a non-reducible problem. Here, we show the effect of the optimal mapping algorithm on a fixed geometry. Let the following advection-reaction equation:
\[
\begin{cases}
\nabla \cdot (c_\mu u_\mu) + \sigma_\mu u_\mu = f_\mu & \text{in } \Omega = (0, 1)^2, \\
u_\mu = u_{D,\mu} & \text{on } \Gamma_{\text{in},\mu} := \{ x \in \partial \Omega : c_\mu \cdot \bn < 0 \},
\end{cases}
\]
where \(\bn\) denotes the outward normal to \(\partial \Omega\), and 
\[
c_\mu =
\begin{bmatrix}
\cos(\mu_1) \\
\sin(\mu_1)
\end{bmatrix}, \quad
\sigma_\mu = 1 + \mu_2 e^{x_1 + x_2}, \quad
f_\mu = 1 + x_1 x_2,
\]
\[
u_{D,\mu} = 4 \arctan \left( \mu_3 \left( x_2 - \frac{1}{2} \right) \right) (x_2 - x_2^2),
\]
\[
\mu = [\mu_1, \mu_2, \mu_3] \in \mathcal{P} := \left[ -\frac{\pi}{10}, \frac{\pi}{10} \right] \times [0.3, 0.7] \times [60, 100].
\]

We consider $n=250$ samples. In Figure \ref{advection_reaction}, we plot $u_{\mu}$ for three different values of the parameter before and after computing the optimal mappings that maximize $J_{\mathcal{U},r}$, for $r=1$. We can clearly see how the optimal mappings will align automatically the discontinuities in the samples. 

\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.38]{advection_1_before.png}
     \includegraphics[scale=0.38]{advection_2_before.png}
     \includegraphics[scale=0.38]{advection_3_before.png}

\vspace{1em}

     \includegraphics[scale=0.38]{advection_1_after.png}
     \includegraphics[scale=0.38]{advection_2_after.png}
     \includegraphics[scale=0.38]{advection_3_after.png}

\caption{ Top: three samples before the optimization. Bottom: three samples after the optimization.}
\label{advection_reaction}
\end{figure}
The optimal mappings algorithm can be seen as a multi-modal generalization to the registration methods, where the case $r=1$ gives similar results to aligning all the samples on one mode. However, the advantage of our method is that we can go beyond a single mode. 
\subsection{Example 2: ML4CFD NeurIPS 2024 competition}
In the second example, we apply the method to the airfoil design case considered in the ML4CFD NeurIPS 2024 competition \cite{yagoubi2024neurips}, and we compare it with the winning solution. The dataset adopted for the competition is the AirfRANS dataset from \cite{bonnet2022airfrans}. Each sample have two scaler inputs, the inlet velocity and the angle of attack, and three output fields, the velocity, pressure and the turbulent viscosity. The dataset is composed of three splits.
\begin{enumerate}
    \item Training set: composed of 103 samples.
    \item Testing set: composed of 200 samples.
    \item OOD testing set: composed of 496 samples, where the Reynold number considered for samples in this split is taken out of distribution.
\end{enumerate}
In this work, we focus on the turbulent viscosity field as it represents a non-reducible field. We illustrate this field in Figure \ref{fig: nut_example}.
\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.13]{nut_example.png}
     \vspace{2em}
     \includegraphics[scale=0.1285]{nut_example_2.png}
\caption{ The turbulent viscosity field illustrated for two of the samples.}
\label{fig: nut_example}
\end{figure}

\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.13]{diffusion.png}
\caption{ The diffused field $\widehat{u}$ for $c_2=1$ .}
\label{fig: diffusion ex}
\end{figure}


\subsubsection{MMGP: mesh morphing Gaussian process  }
The method we present in this paper is founded on the  MMGP method \cite{casenave2024mmgp}. The main differences being:
\begin{enumerate}
    \item First, in MMGP, the mapping $\phi_i^{\rm geo}$ is computed from each geometry onto the reference domain $\Omega_0$. In this case, the input to the GP model is the POD coefficients of the inverse mapping $(\phi_i^{\rm geo})^{-1}$. Thus, evaluating $(\phi_i^{\rm geo})^{-1}$ would introduce extra computations to the procedure. In addition, when using RBF morphing, such as in this work, computing $\phi_i^{\rm geo}$ from the same geometry proves to be much efficient numerically, as the RBF interpolation matrix $\mathbf{K}$ can be assembled and factorized once and for all (see appendix \ref{appendix: rbf} for the definition of $\mathbf{K}$).
    \item Secondly, the MMGP mappings are not field-optimized. This is equivalent to omitting the transition to the optimal configuration. So, when it comes to non-reducible problems, even if a large number of modes are retained, the error introduced by POD truncation and the discretization error would remain large to obtain reliable predictions for new samples.
\end{enumerate}
\subsubsection{NeurIPS solution: MMGP + wake line prediction}
The winning solution to the ML4CFD competition \cite{ML4CFD_Neurisp2024} relies on the application of MMGP, with the addition of aligning the wake line behind the airfoil for all the samples at same position. The latter step being necessary to obtain accurate prediction of the turbulent viscosity. While this correction step gives accurate results, it remains case-dependent and needs to be done manually. Moreover, it suffers from the same problem as any other registration technique, which is the restriction to one mode. 
\subsubsection{O-MMGP: optimal mesh morphing Gaussian process}
We run the optimal mapping algorithm in order to maximize the compression of the turbulent viscosity field. We choose the reference geometry $\Omega_0$ to be one of the samples in the training set. We then proceed to compute $\phi_i^{\rm geo}$ using RBF morphing. Once computed, we solve problem \ref{eq: I definition} for the family $\widehat{\mathcal U}(c_2)$. We choose $r:=1$, $c_1^{(0)}:=0.1$ and $c_2^{(0)}:=1 $. The last two parameters are changed throughout the iterations as mentioned in appendix \ref{Continuation method}. In Figure \ref{fig: phi opt decay}, we show that the eigenvalues of the correlation matrix of $(\bphi_i^{\rm opt})_{1\leq i \leq n}$ decay rapidly, proving numerically that this family is reducible, and justifying the regression on the optimal mapping POD coordinates.

\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.45]{eigenvalues_morphings_opt.png}
\caption{ Decay of the eigenvalues of the correlation matrix of the family $(\bphi_i^{\rm opt})_{1\leq i \leq n}$ .}
\label{fig: phi opt decay}
\end{figure}

After computing the optimal mappings, we train two Gaussian process regression models, one to learn the optimal mapping $\widetilde{\bphi}$, and the other to learn the field $\widetilde{u}$, which is the turbulent viscosity in this case. 

In order to show the efficiency of the method, we compare the results of the following three tests:
\begin{enumerate}
    \item First, we apply the method without solving the optimal mappings problem, similar to the original MMGP method. Thus, we predict the turbulent viscosity field in the reference configuration.
    \item Second, we run the winning solution of the NeurIPS 2024 challenge, aligning the wake line behind the airfoil manually.
    \item Finally, we apply the full O-MMGP procedure described in this paper.
\end{enumerate}
In table \ref{tab: MSE nut}, we report the mean square error (MSE) of the three tests on the turbulent viscosity field, for the testing and OOD splits. While the original MMGP performs poorly for this field, the O-MMGP method produces very accurate predictions, slightly surpassing the NeurIPS 2024 competition solution. The major advantage of the method is that the alignment of the snapshot was done automatically, without using the specificity of the case.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|} 
\hline
 & MMGP & NeurIPS solution & O-MMGP\\
\hline
 Test & 0.143 & 0.025 &  0.024 \\
\hline
 OOD & 0.171   & 0.048 &   0.046   \\
\hline
\end{tabular}
\caption{MSE errors for the different tests.}
\label{tab: MSE nut}
\end{table}
In Figure \ref{fig: nu t decays}, we report the decay of the eigenvalues of the correlation matrix for the three tests. For the MMGP solution, no aligning of the solution was performed, which explains the slow decay of the eigenvalues for this case. As expected, the eigenvalues decays the most rapidly when using the optimal mappings algorithm. 

\begin{figure}[ht] 
     \centering
     \includegraphics[scale=0.45]{eigenvalues_morphings_opt_noopt.png}
\caption{ Decay of the eigenvalues of the correlation matrix for the turbulent viscosity fields in the three tests.}
\label{fig: nu t decays}
\end{figure}



\section{Conclusion}
In this article, we have presented a new algorithm that aims to maximize the energy in the first $r$ principal modes of a family of functions. The novelty of the algorithm lies in the fact that it automatically aligns the functions on an arbitrary number of modes, unlike other methods in the literature which may require feature tracking of the solution or assume that the family of functions can be compressed onto a single mode. We have shown how the proposed method can be integrated into the MMGP workflow to learn and predict the solutions of PDEs with geometric variabilities. The quality of the prediction was found to be on a par with state-of-the-art methods. Current work aims to solve the problem in the general case of curved boundaries and provide a more thorough mathematical analysis of the method, as well as applying the method in the presence of multiple shocks.

\clearpage

\bibliography{main}
\bibliographystyle{plain}

\clearpage

\appendix

\section*{Appendix}

\section{Differential of $J_{\mathcal H,r}$} \label{appendix: diff J}
Let $\Phi = (\bphi_i)_{1\leq i\leq n}\in \mathbf{M}$, $\Psi =(\bpsi_i)_{1\leq i\leq n} $ a "small" variation around $\Phi$ and $\epsilon \in \R, \epsilon<<1$. To evaluate the differential of $J_{\mathcal H,r}$ at a point $\Phi$, we evaluate $J_{\mathcal H,r}$ at $\Bar{\Phi} := \Phi + \epsilon \Psi$ and calculate:
\begin{align}\label{differetial of J def}
    D J_{\mathcal H,r}[\Phi][\Psi] = \lim_{\epsilon\to 0} \displaystyle \frac{J_{\mathcal H,r}(\Phi + \epsilon \Psi) - J_{\mathcal H,r}(\Phi)}{\epsilon}
\end{align}
We have:
\begin{align*}
    \forall i, \hspace{0.5em} h_i \circ (\bphi_i + \epsilon \bpsi_i)(x) &= h_i(\bphi_i(x) + \epsilon \bpsi_i(x))\\
    &\simeq h_i(\bphi_i(x)) + \epsilon \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)
\end{align*}
where we neglect higher order terms. Next we evaluate :
\begin{flalign}\label{Cij differential}
\displaystyle  C_{\mathcal H,ij}[\Phi + \epsilon \Psi] & = \langle h_i \circ (\bphi_i + \epsilon \bpsi_i), h_j \circ (\bphi_j + \epsilon \bpsi_j) \rangle_{L^2(\Omega_0)} &&\nonumber\\
 &= \int_{\Omega_0}h_i \circ (\bphi_i + \epsilon \bpsi_i)(x)  h_j \circ (\bphi_j + \epsilon \bpsi_j)(x) dx &&\nonumber\\
 &=\int_{\Omega_0}h_i \circ \bphi_i(x)  h_j \circ \bphi_j(x) dx + \epsilon\int_{\Omega_0}h_i \circ \bphi_i(x)  \Vec{\nabla} h_j(\bphi_j(x))\cdot \bpsi_j(x)dx &\\
 &+ \epsilon\int_{\Omega_0}  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)  h_j \circ \bphi_j(x) dx + \epsilon^2 \int_{\Omega_0}  \Vec{\nabla} h_i(\bphi_i(x))\bpsi_i(x) \Vec{\nabla} h_j(\bphi_j(x))\bpsi_j(x)dx \nonumber&& \\
 &:= C_{\mathcal H,ij}[\Phi] + \epsilon D C_{\mathcal H,ij}[\Phi][\Psi] + \epsilon^2 D^2 C_{\mathcal H,ij}[\Phi][\Psi] \nonumber
\end{flalign}
and
\begin{flalign*}
Tr(C_{\mathcal H}[\Phi + \epsilon \Psi]) &= \sum C_{ii}[\Phi + \epsilon \Psi] && \\
&=Tr(C_{\mathcal H}[\Phi]) + 2\epsilon \sum_{i=1}^n \int_{\Omega_0} h_i \circ \bphi_i(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)  dx + \epsilon^2  \sum_{i=1}^n \int_{\Omega_0} [\Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)]^2  dx &&\\
&= Tr(C_{\mathcal H}[\Phi])+ \epsilon  Tr(D C_{\mathcal H}[\Phi][\Psi]) + \epsilon^2 D^2 C_{\mathcal H}[\Phi][\Psi] 
\end{flalign*}
with $D C_{\mathcal H,ij}[\Phi][\Psi] := \displaystyle \int_{\Omega_0}h_i \circ \bphi_i(x)  \Vec{\nabla} h_j(\bphi_j(x))\cdot \bpsi_j(x)dx +\int_{\Omega_0} \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)  h_j \circ \bphi_j(x) dx$ and $D C_{\mathcal H}[\Phi][\Psi]=(D C_{\mathcal H,ij}[\Phi][\Psi])_{1\leq i,j\leq n}$.\\\\
Next we evaluate $D  \lambda_i^{\mathcal H}[\Phi][\Psi]$. We have $\forall i$ : 
\begin{align*}
    ||\zeta_i^{\mathcal H}[\Phi] ||^2=1
\end{align*}
Now taking the differential on both sides, we get 
\begin{align*}
    \langle D \zeta_i^{\mathcal H}[\Phi][\Psi] , \zeta_i^{\mathcal H}[\Phi] \rangle = 0
\end{align*}
Using the fact that $C_{\mathcal H}[\Phi]\zeta_i^{\mathcal H}[\Phi]=  \lambda_i^{\mathcal H}[\Phi] \zeta_i^{\mathcal H}[\Phi] $, we get again by taking the differential on both sides 
\begin{align*}
    D C_{\mathcal H}[\Phi][\Psi] \zeta_i^{\mathcal H}[\Phi] + C_{\mathcal H}[\Phi]D \zeta_i^{\mathcal H}[\Phi][\Psi] = D  \lambda_i^{\mathcal H}[\Phi][\Psi]  \zeta_i^{\mathcal H}[\Phi] +  \lambda_i^{\mathcal H}[\Phi] D \zeta_i^{\mathcal H}[\Phi][\Psi]
\end{align*}
We multiply the last equation by $\zeta_i^{\mathcal H}[\Phi]$ to get 
\begin{align*}
   (\zeta_i^{\mathcal H}[\Phi])^TD C_{\mathcal H}[\Phi][\Psi] \zeta_i^{\mathcal H}[\Phi] + 0 = D  \lambda_i^{\mathcal H}[\Phi][\Psi] +0  
\end{align*}
Thus we have finally 
\begin{align} \label{eigen value exprssion}
    \lambda_i^{\mathcal H}[\Phi + \epsilon \Psi]=  \lambda_i^{\mathcal H}[\Phi]+ (\zeta_i^{\mathcal H}[\Phi])^TD C_{\mathcal H}[\Phi][\Psi] \zeta_i^{\mathcal H}[\Phi] \hspace{1em} ,\forall 1\leq i \leq n
\end{align}
Taking the sum over the first $r$ eigenvalues, we obtain: 
\begin{align}
    \displaystyle \sum_{j=1}^r \lambda_j^{\mathcal H}[\Phi + \epsilon \Psi] = \displaystyle \sum_{j=1}^r  \lambda_i^{\mathcal H}[\Phi][\Psi] + \epsilon Tr( (\mathrm{Z}^{\Phi}_r)^T D C_{\mathcal H}[\Phi][\Psi] \mathrm{Z}^{\Phi}_r )
\end{align}
with $\mathrm{Z}^{\Phi}_r= (\zeta_1^{\Phi}, \zeta_2^{\Phi}, \cdots, \zeta_r^{\Phi} )^T$. Now we can evaluate \eqref{differetial of J def} to obtain: 
\begin{align}\label{differential J general expression}
     D J_{\mathcal H,r}[\Phi][\Psi] =  \frac{Tr( (\mathrm{Z}^{\Phi}_r)^T D C_{\mathcal H}[\Phi][\Psi] \mathrm{Z}^{\Phi}_r )}{Tr (C_{\mathcal H}[\Phi]) } - \frac{ \displaystyle \sum_{k=1}^r  \lambda_k^{\mathcal H}[\Phi]}{Tr (C_{\mathcal H}[\Phi])^2 } \times Tr (D C_{\mathcal H}[\Phi][\Psi]) 
\end{align}
which can be written explicitly as
\begin{flalign*}
    D J_{\mathcal H,r}[\Phi][\Psi] &= \frac{2}{Tr(C_{\mathcal H}[\Phi])}\sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^r  \zeta^{\mathcal H}_{k,i}[\Phi] \zeta^{\mathcal H}_{k,j}[\Phi] \displaystyle \int_{\Omega_0}h_j \circ \bphi_j(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)dx &&\\
    &- \frac{ \displaystyle 2\sum_{k=1}^r  \lambda_k^{\mathcal H}[\Phi]}{Tr (C_{\mathcal H}[\Phi])^2 } \sum_{i=1}^n \int_{\Omega_0}h_j \circ \bphi_j(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)dx &&\\
    &=\sum_{i=1}^n D J_i[\Phi][\bpsi_i]
\end{flalign*} 
with 
\begin{align}
    D J_i[\Phi][\bpsi_i] &:= \sum_{k=1}^r \Big(\frac{2{\zeta^{\mathcal H}_{k,i}[\Phi]}^2}{Tr(C_{\mathcal H}[\Phi])} - \frac{ \displaystyle 2 \lambda_k^{\mathcal H}[\Phi]}{Tr (C_{\mathcal H}[\Phi])^2 } \Big) \int_{\Omega_0}h_i \circ \bphi_i(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)dx\nonumber \\
    &+ \sum_{j=1 \atop j\neq i}^n \sum_{k=1}^r \frac{ 2\zeta^{\mathcal H}_{k,i}[\Phi]\zeta^{\mathcal H}_{k,j}[\Phi] }{Tr (C_{\mathcal H}[\Phi]) } \int_{\Omega_0}h_j \circ \bphi_j(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)dx\nonumber \\
    &=\sum_{j=1}^n \sum_{k=1}^r \left( \frac{ 2\zeta^{\mathcal H}_{k,i}[\Phi]\zeta^{\mathcal H}_{k,j}[\Phi] }{Tr (C_{\mathcal H}[\Phi]) } -\frac{ \displaystyle 2 \lambda_k^{\mathcal H}[\Phi]}{Tr (C_{\mathcal H}[\Phi])^2 }\delta_{ij}\right)\int_{\Omega_0}h_j \circ \bphi_j(x)  \Vec{\nabla} h_i(\bphi_i(x))\cdot \bpsi_i(x)dx,
\end{align}
and $\delta_{ij}$ is the Kronecker delta. For simplicity, we note the term 
\begin{align}
\f_i [\Phi] :=\sum_{j=1}^n \sum_{k=1}^r \left( \frac{ 2\zeta^{\mathcal H}_{k,i}[\Phi]\zeta^{\mathcal H}_{k,j}[\Phi] }{Tr (C_{\mathcal H}[\Phi]) } -\frac{ \displaystyle 2 \lambda_k^{\mathcal H}[\Phi]}{Tr (C_{\mathcal H}[\Phi])^2 }\delta_{ij}\right) h_j \circ \bphi_j(x)  \Vec{\nabla} h_i(\bphi_i(x)),    
\end{align}
$\F[\Phi]:=(\f_1[\Phi], \cdots , \f_n[\Phi])$, and we write 
$$
D J_{\mathcal H,r}[\Phi][\Psi] = \sum_{i=1}^n \int_{\Omega_0} \f_i[\Phi] \cdot \bpsi_i dx.
$$ 
\section{Differential of $I_{\mathcal H,r}$}\label{appendix: J diff}
We define the objective function $I_{\mathcal H,r}$ on $\mathrm{M}_0^n$ as $$
 I_{\mathcal H,r}[\Phi]:= J_{\mathcal H,r}[\Phi] - c_1 \sum_{i=1}^n E[\bphi_i].
 $$

As mentioned in section \ref{sec: optimal mapping}, the energy term used in this work in the linear elastic energy defined as $$
E[\bphi_i] := \frac{1}{2} a(\bphi_i - \bId,\bphi_i - \bId),
$$ 
where we recall the definition of $a$ in \eqref{inner product}. We can easily show that $$DE[\bphi_i][\bpsi]=a(\bphi_i - \bId,\bpsi).$$

Using the linearity of the differential, we obtain the differential of $I_{\mathcal H,r}$ as
\begin{align*}
    DI_{\mathcal H,r}[\Phi][\Psi]&=DJ[\Phi][\Psi]-c_1\sum_{i=1}^n DE[\bphi_i][\bpsi_i] \\
    &=\sum_{i=1}^n \int_{\Omega_0} \f_i[\Phi] \cdot \bpsi_i dx-c_1 \sum_{i=1}^n DE[\bphi_i][\bpsi_i]\\
    &=\sum_{i=1}^n \left(\int_{\Omega_0} \f_i[\Phi] \cdot \bpsi_i dx-c_1 DE[\bphi_i][\bpsi_i] \right)\\
    &:=\sum_{i=1}^n DI_i[\Phi][\bpsi_i].
\end{align*}
In order to obtain equation \eqref{gradient update I elasticity}, we compute the Riesz representation of $DI_i[\Phi]$, denoted as $\Bar{\bu}$, with respect to the inner product $a$, the unique solution to
$$
a(\Bar{\bu}_i,\bpsi) = \int_{\Omega_0} \f_i[\Phi] \cdot \bpsi_i dx- c_1 a(\bphi_i - \bId,\bpsi),
$$
for all test function $\bpsi$. Since we also have $\bu_i$
as the Riesz representation of $DJ_i[\Phi]$ $\left(a(\bu_i,\bpsi)=DJ_i[\Phi][\bpsi]\right)$, we obtain
$$
a(\Bar{\bu}_i,\bpsi) = a(\bu_i,\bpsi)- c_1 a(\bphi_i - \bId,\bpsi).
$$ which is true for all test function $\bpsi$. Thus, by the linearity of $a$, we obtain 
$\Bar{\bu}_i=\bu_i-c_1(\bphi_i- \bId)$, and hence equation \eqref{gradient update I elasticity}.

\section{Continuation method} \label{Continuation method}

We give here a quick description on the continuation methods. Readers can refer to \cite{axelsson2015continuation,rheinboldt2000numerical} and references within for more details.\\\\
Continuation methods are a variety of methods used to solve equations of the type $$
G(x)=0
$$
where $G$ is a non-linear function, and $x$ is the unknown. Continuation methods rely on solving a succession of problems of the form$$
H(x,\lambda)=0
$$
where $H$ is another non-linear function and $H(x,\lambda^*)=G(x)$ for some $\lambda^*$. $\lambda$ is called the continuation parameter. In the simplest form of a continuation method, we start by choosing a set a values $\{\lambda^0,\lambda^1 \cdots \lambda^*\}$ for $\lambda$. We then solve $H(x^i,\lambda^i)=0$, 
and initialize $H(x^{i+1},\lambda^{i+1})=0$ with $x^i$.
\subsection{Continuation for $c_1$}
In order to solve equation \eqref{eq: I definition}, we proceed by using the continuation method as follows:
\begin{enumerate}
    \item We start by solving equation the optimization problem, using the gradient algorithm, for $c_1=c_1^0$, where $c_1^0$ should be sufficiently large in order to converge properly. This will produce the function $\Phi^0$.
    \item At each iteration $k$, we set $\displaystyle c_1^k:=\frac{c_1^{k-1}}{2}$, and we resolve the problem for $\Phi^k$ by initializing $\Phi^{(0),k}=\Phi^{k-1}$ .
    \item We iterate over $k$ as long as there are no elements of the morphed meshes are inverted.
\end{enumerate}
We give a few comments about the above procedure. First, we note that there are two loops for the algorithm: the outer loop over $k$ that updates $c_1$, and the inner loop that solves the non-linear equation using the gradient algorithm. Second, ideally we want to solve \eqref{eq: I definition} for $c_1=0$. If, while iterating over the outer loop, $c_1^k$ gets sufficiently small, we can set $c_1^{k+1}=0$. Finally, we note that full convergence for intermediate results of $\Phi^k$ is generally not necessary.
\subsection{Continuation for $c_2$}
Applying continuation with two or more parameters proves to be more complicated for the case of one parameter. To this end, we propose in this paper to alternate changing the values of the two parameters $c_1$ and $c_2$. We proceed in the same manner as above, expect that:
\begin{enumerate}
    \item At iteration $2k$, we set $c_1^{2k}:=\frac{c_1^{2k-1}}{2}$.
    \item We start with a small value for $c_2^{0}$. At iteration $2k+1$, we set $c_2^{2k+1}:=10 \times c_2^{2k}$.  
\end{enumerate}

\section{RBF morphing} \label{appendix: rbf}
In radial basis function (RBF) interpolation, we aim to approximate a scalar-valued function \( g(x) \) using a linear combination of radial basis functions as follows:  
\begin{equation}
    g(x) \approx \sum_{i=1}^{n_c} a_i \, \xi(\|x - x_i\|),
    \label{eq:rbf_interpolation}
\end{equation}
where $a_i \in \R$ and  \( \xi \) is a radial basis function that depends only on the Euclidean distance between \( x \) and a control point \( x_i \). The control points \( \{x_i \mid i = 1, \ldots, n_c\} \) are specific locations where the exact values of \( g(x) \), denoted \( g_i \), are known. To determine the coefficients \( a_i \), we enforce the interpolation condition at all control points, yielding the system:  
\begin{equation}
    \sum_{i=1}^{n_c} a_i \, \xi(\|x_j - x_i\|) = g_j, \quad j = 1, \ldots, n_c.
\end{equation}
This can be expressed in matrix form as:  
\begin{equation}
    \mathbf{K} \boldsymbol{a} = \boldsymbol{g},
\end{equation}
where \( \mathbf{K} \) is an \( n_c \times n_c \) interpolation matrix with entries \( K_{ij} = \xi(\|x_j - x_i\|) \), \( \boldsymbol{a} = [a_1, \ldots, a_{n_c}]^T \) is the vector of coefficients, and \( \boldsymbol{g} = [g_1, \ldots, g_{n_c}]^T \) is the vector of known values. Solving this linear system yields the coefficients \( a_i \), which can then be used to compute \( g(x) \) at any point \( x \).

In the context of radial basis function-based mesh morphing, RBF interpolation is applied separately to each component of the morphing function \( \boldsymbol{\phi}(x) = (\phi_1(x), \phi_2(x), \phi_3(x)) \) in 3D (or similarly in 2D). The control points are usually chosen as boundary points. The interpolated morphing function is then used to smoothly deform the interior of the mesh according to the control point displacements.
\end{document}