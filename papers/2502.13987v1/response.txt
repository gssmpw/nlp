\section{Related Work}
\label{sec:related}

\paragraph{Age transformation.}
Many studies have explored controlling various facial attributes, including age, by manipulating latent variables in GANs**Goodfellow et al., "Generative Adversarial Networks"**.
For example, **Shen et al., "Face Aging with Conditional Generative Adversarial Networks"**, demonstrated that facial age could be edited by shifting latent variables along the normal directions of hyperplanes that separate attributes.
**Huang et al., "Aging Face Editing with GANs: A Study on Latent Variable Manipulation"**, improved upon this by moving latent variables in multiple directions for a single attribute, enabling more natural age editing.
However, these methods allow for increasing or decreasing age but do not support specifying a precise target age.
Conversely, target age editing has been achieved within GAN-based image-to-image translation frameworks**Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**.
Nevertheless, these GAN-based methods often struggle to preserve identity during age transformations.

Recently, diffusion models have gained significant attention, leading to the development of attribute editing methods**Ho et al., "Diffusion Models for Attribute Editing in Images"**.
FADING**Ho et al., "Fine-Grained Image-to-Image Translation via Diffusion-Based Generative Models"**, is an age editing method based on a pretrained LDM**Niemeyer et al., "Denoising Diffusion Implicit Models"**.
Specifically, it fine-tunes LDM on an age-labeled dataset to specialize the model for age editing.
During inference, the input image is embedded into the model's latent space using Null-text Inversion**Ho et al., "Implicit Neural Representations for Image-to-Image Translation"**, and Prompt-to-Prompt**Ho et al., "Prompt-to-Prompt: A Diffusion-Based Framework for Image Editing"**, is applied to modify only age-related regions.
However, these existing methods do not sufficiently consider individual variations in age progression and regression.

\paragraph{Personalized image synthesis.}

The task of adapting an image generative model to a specific concept is known as personalization.
Personalized image synthesis has been explored with
both GAN-based methods**Miyato et al., "Image-to-Image Translation with Generative Adversarial Networks"** and diffusion-based methods**Ho et al., "Diffusion Models for Image Generation"**.
Many of these approaches fine-tune pretrained models so that generated images become close to a small set of reference images.
IDP**Chen et al., "Identity-Preserving Age Transformation via Diffusion-Based Generative Models"**, is a personalized age transformation method fine-tuned using self-reference images and diverse facial images**Lee et al., "Diverse Facial Images for Personalized Image Synthesis"**. 
However, because IDP is for generating new images but not for editing them, the composition and facial expression of the generated face image differ from those of the target person. Moreover, IDP restricts age input to predefined coarse categories (i.e., age groups). In contrast, our method can edit an existing image of the target person with integer target ages.

Concurrently, **Qi et al., "Personalized Facial Aging via Conditional Adversarial Networks"**, proposed a personalized facial aging method. Their method is GAN-based, whereas our method is diffusion-based. Their work is a preprint at the time of our submission, and the source code is not publicly available. Direct comparison is left as future work.