@inproceedings{Agarwal2024PromptLE, 
title={Prompt Leakage effect and defense strategies for multi-turn LLM interactions}, 
author={Divyansh Agarwal and A. R. Fabbri and Philippe Laban and Shafiq R. Joty and Caiming Xiong and Chien-Sheng Wu}, 
year={2024}, 
url={https://api.semanticscholar.org/CorpusID:269362119} }

@article{Carlini2023AreAN,
  title={Are aligned neural networks adversarially aligned?},
  author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Anas Awadalla and Pang Wei Koh and Daphne Ippolito and Katherine Lee and Florian Tram{\`e}r and Ludwig Schmidt},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.15447},
  url={https://api.semanticscholar.org/CorpusID:259262181}
}

@article{Huang2023CatastrophicJO,
  title={Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation},
  author={Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.06987},
  url={https://api.semanticscholar.org/CorpusID:263835408}
}

@article{Li2023DeepInceptionHL, 
title={DeepInception: Hypnotize Large Language Model to Be Jailbreaker}, 
author={Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han}, 
journal={ArXiv}, 
year={2023}, 
volume={abs/2311.03191}, 
url={https://api.semanticscholar.org/CorpusID:265033222}
}

@article{Li2023MultistepJP,
  title={Multi-step Jailbreaking Privacy Attacks on ChatGPT},
  author={Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Yangqiu Song},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.05197},
  url={https://api.semanticscholar.org/CorpusID:258060250}
}

@article{Liu2023JailbreakingCV,
  title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
  author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13860},
  url={https://api.semanticscholar.org/CorpusID:258841501}
}

@article{Liu2024ExploringVA,
  title={Exploring Vulnerabilities and Protections in Large Language Models: A Survey},
  author={Frank Weizhen Liu and Chenhui Hu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.00240},
  url={https://api.semanticscholar.org/CorpusID:270214323}
}

@article{Russinovich2024GreatNW, 
title={Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack}, 
author={Mark Russinovich and Ahmed Salem and Ronen Eldan}, 
journal={ArXiv}, 
year={2024}, 
volume={abs/2404.01833}, 
url={https://api.semanticscholar.org/CorpusID:268856920} 
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@article{cheng2024leveraging,
  title={Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks},
  author={Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G},
  journal={arXiv preprint arXiv:2402.09177},
  year={2024}
}

@inproceedings{ding2024wolf,
  title={A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={2136--2153},
  year={2024}
}

@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{grattafiori2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Grattafiori, Wenhan Xiong and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{li2025revisiting,
  title={Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective},
  author={Li, Tianlong and Wang, Zhenghua and Liu, Wenhao and Wu, Muling and Dou, Shihan and Lv, Changze and Wang, Xiaohua and Zheng, Xiaoqing and Huang, Xuan-Jing},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={3158--3178},
  year={2025}
}

@article{lin2024towards,
  title={Towards trustworthy LLMs: a review on debiasing and dehallucinating in large language models},
  author={Lin, Zichao and Guan, Shuyan and Zhang, Wending and Zhang, Huiyan and Li, Yugang and Zhang, Huaping},
  journal={Artificial Intelligence Review},
  volume={57},
  number={9},
  pages={1--50},
  year={2024},
  publisher={Springer}
}

@article{mehrotra2023tree,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@article{navigli2023biases,
  title={Biases in large language models: origins, inventory, and discussion},
  author={Navigli, Roberto and Conia, Simone and Ross, Bj{\"o}rn},
  journal={ACM Journal of Data and Information Quality},
  volume={15},
  number={2},
  pages={1--21},
  year={2023},
  publisher={ACM New York, NY}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
  journal={Nature medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{yang2024chain,
  title={Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM},
  author={Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong},
  journal={arXiv preprint arXiv:2405.05610},
  year={2024}
}

@article{yi2024jailbreak,
  title={Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}

@article{zhang2023defending,
  title={Defending large language models against jailbreaking attacks through goal prioritization},
  author={Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.09096},
  year={2023}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

