@article{zeng2024johnny,
  title={How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms},
  author={Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan},
  journal={arXiv preprint arXiv:2401.06373},
  year={2024}
}


@article{grattafiori2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Grattafiori, Wenhan Xiong and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
  journal={Nature medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  publisher={Nature Publishing Group US New York}
}


@article{cheng2024leveraging,
  title={Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks},
  author={Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G},
  journal={arXiv preprint arXiv:2402.09177},
  year={2024}
}


@article{lin2024towards,
  title={Towards trustworthy LLMs: a review on debiasing and dehallucinating in large language models},
  author={Lin, Zichao and Guan, Shuyan and Zhang, Wending and Zhang, Huiyan and Li, Yugang and Zhang, Huaping},
  journal={Artificial Intelligence Review},
  volume={57},
  number={9},
  pages={1--50},
  year={2024},
  publisher={Springer}
}


@article{navigli2023biases,
  title={Biases in large language models: origins, inventory, and discussion},
  author={Navigli, Roberto and Conia, Simone and Ross, Bj{\"o}rn},
  journal={ACM Journal of Data and Information Quality},
  volume={15},
  number={2},
  pages={1--21},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{ding2024wolf,
  title={A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={2136--2153},
  year={2024}
}

@article{yi2024jailbreak,
  title={Jailbreak Attacks and Defenses Against Large Language Models: A Survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{Liu2024ExploringVA,
  title={Exploring Vulnerabilities and Protections in Large Language Models: A Survey},
  author={Frank Weizhen Liu and Chenhui Hu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.00240},
  url={https://api.semanticscholar.org/CorpusID:270214323}
}

@article{Huang2024LevelsOA,
  title={Levels of AI Agents: from Rules to Large Language Models},
  author={Yu Huang},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.06643},
  url={https://api.semanticscholar.org/CorpusID:269757441}
}

@article{Liu2023JailbreakingCV,
  title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
  author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13860},
  url={https://api.semanticscholar.org/CorpusID:258841501}
}

@article{Li2023DeepInceptionHL, 
title={DeepInception: Hypnotize Large Language Model to Be Jailbreaker}, 
author={Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han}, 
journal={ArXiv}, 
year={2023}, 
volume={abs/2311.03191}, 
url={https://api.semanticscholar.org/CorpusID:265033222}
} 

@article{Li2023MultistepJP,
  title={Multi-step Jailbreaking Privacy Attacks on ChatGPT},
  author={Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Yangqiu Song},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.05197},
  url={https://api.semanticscholar.org/CorpusID:258060250}
}

@article{Huang2023CatastrophicJO,
  title={Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation},
  author={Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.06987},
  url={https://api.semanticscholar.org/CorpusID:263835408}
}

@article{Carlini2023AreAN,
  title={Are aligned neural networks adversarially aligned?},
  author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Anas Awadalla and Pang Wei Koh and Daphne Ippolito and Katherine Lee and Florian Tram{\`e}r and Ludwig Schmidt},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.15447},
  url={https://api.semanticscholar.org/CorpusID:259262181}
}

@article{Tong2024SecuringMC,
  title={Securing Multi-turn Conversational Language Models Against Distributed Backdoor Triggers},
  author={Terry Tong and Jiashu Xu and Qin Liu and Muhao Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.04151},
  url={https://api.semanticscholar.org/CorpusID:271039740}
}

@article{Zhou2024Speak, 
title={Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue}, 
author={Zhenhong Zhou and Jiuyang Xiang and Haopeng Chen and Quan Liu and Zherui Li and Sen Su}, 
journal={ArXiv}, 
year={2024}, 
volume={abs/2402.17262}, 
url={https://api.semanticscholar.org/CorpusID:268031931} 
}

@article{Russinovich2024GreatNW, 
title={Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack}, 
author={Mark Russinovich and Ahmed Salem and Ronen Eldan}, 
journal={ArXiv}, 
year={2024}, 
volume={abs/2404.01833}, 
url={https://api.semanticscholar.org/CorpusID:268856920} 
} 

@inproceedings{Agarwal2024PromptLE, 
title={Prompt Leakage effect and defense strategies for multi-turn LLM interactions}, 
author={Divyansh Agarwal and A. R. Fabbri and Philippe Laban and Shafiq R. Joty and Caiming Xiong and Chien-Sheng Wu}, 
year={2024}, 
url={https://api.semanticscholar.org/CorpusID:269362119} } 


@article{wang2024grammar,
  title={Grammar prompting for domain-specific language generation with large language models},
  author={Wang, Bailin and Wang, Zi and Wang, Xuezhi and Cao, Yuan and A Saurous, Rif and Kim, Yoon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2023defending,
  title={Defending large language models against jailbreaking attacks through goal prioritization},
  author={Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.09096},
  year={2023}
}

@misc{glm2024chatglm,
    title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
    author={GLM Team et al.},
    year={2024},
    eprint={2406.12793},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{qwen2,
  title={Qwen2 Technical Report},
  year={2024}
}

@article{qwen,
    title={Qwen Technical Report},
    author={Jinze Bai et al.},
    journal={arXiv preprint arXiv:2309.16609},
    year={2023}
}


@misc{bge-m3,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2024foot,
  title={Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology},
  author={Wang, Zhenhua and Xie, Wei and Wang, Baosheng and Wang, Enze and Gui, Zhiwen and Ma, Shuoyoucheng and Chen, Kai},
  journal={arXiv preprint arXiv:2402.15690},
  year={2024}
}

@article{deng2024pandora,
  title={Pandora: Jailbreak gpts by retrieval augmented generation poisoning},
  author={Deng, Gelei and Liu, Yi and Wang, Kailong and Li, Yuekang and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2402.08416},
  year={2024}
}



@article{yang2024chain,
  title={Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM},
  author={Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong},
  journal={arXiv preprint arXiv:2405.05610},
  year={2024}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}


@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}


@article{lu2024autojailbreak,
  title={AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens},
  author={Lu, Lin and Yan, Hai and Yuan, Zenghui and Shi, Jiawen and Wei, Wenqi and Chen, Pin-Yu and Zhou, Pan},
  journal={arXiv preprint arXiv:2406.03805},
  year={2024}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}


@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{yuan2023gpt,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}

@article{charan2023text,
  title={From text to mitre techniques: Exploring the malicious use of large language models for generating cyber attack payloads},
  author={Charan, PV and Chunduri, Hrushikesh and Anand, P Mohan and Shukla, Sandeep K},
  journal={arXiv preprint arXiv:2305.15336},
  year={2023}
}


@article{tian2024toward,
  title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2404.12253},
  year={2024}
}



@inproceedings{li2025revisiting,
  title={Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective},
  author={Li, Tianlong and Wang, Zhenghua and Liu, Wenhao and Wu, Muling and Dou, Shihan and Lv, Changze and Wang, Xiaohua and Zheng, Xiaoqing and Huang, Xuan-Jing},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={3158--3178},
  year={2025}
}

@article{liao2024imagination,
  title={Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models},
  author={Liao, Huanxuan and He, Shizhu and Xu, Yao and Zhang, Yuanzhe and Liu, Kang and Liu, Shengping and Zhao, Jun},
  journal={arXiv preprint arXiv:2403.15268},
  year={2024}
}


@inproceedings{mazeikaharmbench,
  title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
  author={Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}

}


@article{yu2023gptfuzzer,
  title={Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts},
  author={Yu, Jiahao and Lin, Xingwei and Xing, Xinyu},
  journal={arXiv preprint arXiv:2309.10253},
  year={2023}
}


@inproceedings{liu2024making,
  title={Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction},
  author={Liu, Tong and Zhao, Zhe and Dong, Yinpeng and Meng, Guozhu and Chen, Kai},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={4711--4728},
  year={2024}
}

@inproceedings{hartvigsen2022toxigen,
  title={ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3309--3326},
  year={2022}
}

@article{shen2023anything,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@article{mehrotra2023tree,
  title={Tree of attacks: Jailbreaking black-box llms automatically},
  author={Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin},
  journal={arXiv preprint arXiv:2312.02119},
  year={2023}
}

@inproceedings{zhu2023autodan,
  title={AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  booktitle={First Conference on Language Modeling},
  year={2023}
}

@article{zhang2024cs,
  title={CS-Eval—A Concise Benchmark for Evaluating the Security Risks of Large Language Models},
  author={Zhang, Yu and Gao, Yongbing and Yang, Lidong},
  year={2024},
  publisher={Preprints},
}

@article{kumar2024certifying,
  title={Certifying LLM Safety against Adversarial Prompting. arXiv 2024},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, AJ and Feizi, S and Lakkaraju, H},
  journal={arXiv preprint arXiv:2309.02705},
  year={2024}
}

@article{lv2024codechameleon,
  title={Codechameleon: Personalized encryption framework for jailbreaking large language models},
  author={Lv, Huijie and Wang, Xiao and Zhang, Yuansen and Huang, Caishuang and Dou, Shihan and Ye, Junjie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2402.16717},
  year={2024}
}

@article{chang2024play,
  title={Play guessing game with llm: Indirect jailbreak attack with implicit clues},
  author={Chang, Zhiyuan and Li, Mingyang and Liu, Yi and Wang, Junjie and Wang, Qing and Liu, Yang},
  journal={arXiv preprint arXiv:2402.09091},
  year={2024}
}

@article{yong2023low,
  title={Low-resource languages jailbreak gpt-4},
  author={Yong, Zheng-Xin and Menghini, Cristina and Bach, Stephen H},
  journal={arXiv preprint arXiv:2310.02446},
  year={2023}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@inproceedings{
yuan2024gpt,
title={{GPT}-4 Is Too Smart To Be Safe: Stealthy Chat with {LLM}s via Cipher},
author={Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@article{jiang2024artprompt,
  title={Artprompt: Ascii art-based jailbreak attacks against aligned llms},
  author={Jiang, Fengqing and Xu, Zhangchen and Niu, Luyao and Xiang, Zhen and Ramasubramanian, Bhaskar and Li, Bo and Poovendran, Radha},
  journal={arXiv preprint arXiv:2402.11753},
  year={2024}
}

@misc{li2024drattackpromptdecompositionreconstruction,
      title={DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers}, 
      author={Xirui Li and Ruochen Wang and Minhao Cheng and Tianyi Zhou and Cho-Jui Hsieh},
      year={2024},
      eprint={2402.16914},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.16914}, 
}

@inproceedings{yao2024fuzzllm,
  title={Fuzzllm: A novel and universal fuzzing framework for proactively discovering jailbreak vulnerabilities in large language models},
  author={Yao, Dongyu and Zhang, Jianshu and Harris, Ian G and Carlsson, Marcel},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={4485--4489},
  year={2024},
  organization={IEEE}
}


@article{handa2024jailbreaking,
  title={Jailbreaking proprietary large language models using word substitution cipher},
  author={Handa, Divij and Chirmule, Advait and Gajera, Bimal and Baral, Chitta},
  journal={arXiv preprint arXiv:2402.10601},
  year={2024}
}

@book{schwartz2022towards,
  title={Towards a standard for identifying and managing bias in artificial intelligence},
  author={Schwartz, Reva and Schwartz, Reva and Vassilev, Apostol and Greene, Kristen and Perine, Lori and Burt, Andrew and Hall, Patrick},
  volume={3},
  year={2022},
  publisher={US Department of Commerce, National Institute of Standards and Technology}
}


@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{ai4science2023impact,
  title={The impact of large language models on scientific discovery: a preliminary study using gpt-4},
  author={AI4Science, Microsoft Research and Quantum, Microsoft Azure},
  journal={arXiv preprint arXiv:2311.07361},
  year={2023}
}

@article{duan2025related,
  title={Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject},
  author={Duan, Zenghao and Duan, Wenbin and Yin, Zhiyi and Shen, Yinghan and Jing, Shaoling and Zhang, Jie and Shen, Huawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2502.06868},
  year={2025}
}

@inproceedings{zhou-etal-2024-alignment,
    title = "How Alignment and Jailbreak Work: Explain {LLM} Safety through Intermediate Hidden States",
    author = "Zhou, Zhenhong  and
      Yu, Haiyang  and
      Zhang, Xinghua  and
      Xu, Rongwu  and
      Huang, Fei  and
      Li, Yongbin",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.139/",
    doi = "10.18653/v1/2024.findings-emnlp.139",
    pages = "2461--2488",
    abstract = "Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns."
}

@article{lee2024mechanistic,
  title={A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity},
  author={Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K and Mihalcea, Rada},
  journal={arXiv preprint arXiv:2401.01967},
  year={2024}
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}