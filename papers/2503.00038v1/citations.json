[
  {
    "index": 0,
    "papers": [
      {
        "key": "yi2024jailbreak",
        "author": "Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi",
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "grattafiori2023code",
        "author": "Grattafiori, Wenhan Xiong and D{\\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel",
        "title": "Code Llama: Open Foundation Models for Code"
      },
      {
        "key": "thirunavukarasu2023large",
        "author": "Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei",
        "title": "Large language models in medicine"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zeng2024johnny",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan",
        "title": "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
      },
      {
        "key": "ding2024wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      },
      {
        "key": "zhang2023defending",
        "author": "Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Huang, Minlie",
        "title": "Defending large language models against jailbreaking attacks through goal prioritization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zeng2024johnny",
        "author": "Zeng, Yi and Lin, Hongpeng and Zhang, Jingwen and Yang, Diyi and Jia, Ruoxi and Shi, Weiyan",
        "title": "How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms"
      },
      {
        "key": "ding2024wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "navigli2023biases",
        "author": "Navigli, Roberto and Conia, Simone and Ross, Bj{\\\"o}rn",
        "title": "Biases in large language models: origins, inventory, and discussion"
      },
      {
        "key": "gallegos2024bias",
        "author": "Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K",
        "title": "Bias and fairness in large language models: A survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2023siren",
        "author": "Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others",
        "title": "Siren's song in the AI ocean: a survey on hallucination in large language models"
      },
      {
        "key": "lin2024towards",
        "author": "Lin, Zichao and Guan, Shuyan and Zhang, Wending and Zhang, Huiyan and Li, Yugang and Zhang, Huaping",
        "title": "Towards trustworthy LLMs: a review on debiasing and dehallucinating in large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Carlini2023AreAN",
        "author": "Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Anas Awadalla and Pang Wei Koh and Daphne Ippolito and Katherine Lee and Florian Tram{\\`e}r and Ludwig Schmidt",
        "title": "Are aligned neural networks adversarially aligned?"
      },
      {
        "key": "Liu2023JailbreakingCV",
        "author": "Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu",
        "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study"
      },
      {
        "key": "yi2024jailbreak",
        "author": "Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi",
        "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
      },
      {
        "key": "li2025revisiting",
        "author": "Li, Tianlong and Wang, Zhenghua and Liu, Wenhao and Wu, Muling and Dou, Shihan and Lv, Changze and Wang, Xiaohua and Zheng, Xiaoqing and Huang, Xuan-Jing",
        "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Liu2024ExploringVA",
        "author": "Frank Weizhen Liu and Chenhui Hu",
        "title": "Exploring Vulnerabilities and Protections in Large Language Models: A Survey"
      },
      {
        "key": "Huang2023CatastrophicJO",
        "author": "Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen",
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"
      },
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      },
      {
        "key": "mehrotra2023tree",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      },
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "Li2023DeepInceptionHL",
        "author": "Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han",
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ding2024wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "Li2023MultistepJP",
        "author": "Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Yangqiu Song",
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Russinovich2024GreatNW",
        "author": "Mark Russinovich and Ahmed Salem and Ronen Eldan",
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"
      },
      {
        "key": "zhou2024speak",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      },
      {
        "key": "cheng2024leveraging",
        "author": "Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G",
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "Russinovich2024GreatNW",
        "author": "Mark Russinovich and Ahmed Salem and Ronen Eldan",
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhou2024speak",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      },
      {
        "key": "cheng2024leveraging",
        "author": "Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G",
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "Russinovich2024GreatNW",
        "author": "Mark Russinovich and Ahmed Salem and Ronen Eldan",
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "Agarwal2024PromptLE",
        "author": "Divyansh Agarwal and A. R. Fabbri and Philippe Laban and Shafiq R. Joty and Caiming Xiong and Chien-Sheng Wu",
        "title": "Prompt Leakage effect and defense strategies for multi-turn LLM interactions"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "Li2023DeepInceptionHL",
        "author": "Xuan Li and Zhanke Zhou and Jianing Zhu and Jiangchao Yao and Tongliang Liu and Bo Han",
        "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker"
      },
      {
        "key": "ding2024wolf",
        "author": "Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      },
      {
        "key": "Li2023MultistepJP",
        "author": "Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Yangqiu Song",
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhou2024speak",
        "author": "Unknown",
        "title": "Unknown"
      },
      {
        "key": "yang2024chain",
        "author": "Yang, Xikang and Tang, Xuehai and Hu, Songlin and Han, Jizhong",
        "title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM"
      },
      {
        "key": "cheng2024leveraging",
        "author": "Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G",
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"
      },
      {
        "key": "Russinovich2024GreatNW",
        "author": "Mark Russinovich and Ahmed Salem and Ronen Eldan",
        "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack"
      }
    ]
  }
]