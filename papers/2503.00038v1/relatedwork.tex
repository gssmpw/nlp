\section{Related Work}
\paragraph{\textbf{\textit{Human Value Alignment for LLMs.}}}
% The rapid development of LLMs has revolutionized various domains \cite{yi2024jailbreak, achiam2023gpt, grattafiori2023code, thirunavukarasu2023large}. While LLMs demonstrate remarkable capabilities in handling complex tasks, aligning LLMs with human values remains a challenge due to biases in training data and trade-offs between usefulness and safety \cite{zeng2024johnny, ding2024wolf,zhang2023defending}.
Aligning LLMs with human values remains a challenge due to biases in training data and trade-offs between usefulness and safety \cite{zeng2024johnny, ding2024wolf}. 
Approaches such as Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training,bai2022training} have been proposed to improve fairness \cite{navigli2023biases,gallegos2024bias}, safety \cite{zou2023universal} and eliminate hallucinations \cite{zhang2023siren,lin2024towards}. 
% To further promote the alignment of LLM and human values, we explore the vulnerability in LLMs' imagination in our study.


\paragraph{\textbf{\textit{Jailbreak Attacks on LLMs.}}}
Jailbreak attacks threaten the safety alignment mechanisms of LLMs, potentially leading to the generation of harmful content \cite{Carlini2023AreAN,Liu2023JailbreakingCV,yi2024jailbreak,li2025revisiting}. 
% These attacks can be broadly categorized into white-box and black-box approaches. White-box attacks leverage detailed knowledge of model architectures and parameters to bypass safety controls \cite{Liu2024ExploringVA,Huang2023CatastrophicJO,zou2023universal}, while black-box attacks rely on crafted inputs to exploit vulnerabilities in alignment mechanisms \cite{chao2023jailbreaking,mehrotra2023tree,yang2024chain}. 
Our study is inspired by two key methods in black-box attacks: prompt nesting and multi-turn dialogue attacks.
1) {{{Prompt Nesting Attack.}}}
Prompt nesting bypasses security features by nesting malicious intents in normal prompts, altering LLMs’ context. 
% Techniques like 
DeepInception \cite{Li2023DeepInceptionHL} exploit nested scenarios, while ReNeLLM \cite{ding2024wolf} rewrites prompt to jailbreak based on code completion, text continuation, or form-filling tasks. MJP \cite{Li2023MultistepJP} uses multi-step approaches with contextual contamination to reduce moral constraints, prompting malicious responses.
2) {{Multi-turn Dialogue Attack.}}
LLMs that are safe in isolated, single-round interactions can be gradually manipulated into generating harmful outputs through multiple rounds of interaction \cite{Russinovich2024GreatNW,zhou2024speak,yang2024chain,cheng2024leveraging}. Multi-turn dialogue attack leverages the multi-turn nature of conversational interactions to gradually erode an LLM's content restrictions. 
% Crescendo \cite{Russinovich2024GreatNW} exploits seemingly benign exchanges to prompt malicious tasks. Chain-of-Attack \cite{yang2024chain} uses iterative prompting to gradually increase the relevance of responses to the harmful objective while avoiding explicit safety triggers.


%  {{Multi-turn Dialogue Attack.}}
% LLMs that are safe in isolated, single-round interactions can be gradually manipulated into generating harmful outputs through multiple rounds of interaction \cite{zhou2024speak,yang2024chain,cheng2024leveraging}. Multi-turn dialogue attack leverages the multi-turn nature of conversational interactions to gradually erode an LLM's content restrictions. Crescendo \cite{Russinovich2024GreatNW} exploits seemingly benign exchanges to prompt malicious tasks. Chain-of-Attack \cite{yang2024chain} uses iterative prompting to gradually increase the relevance of responses to the harmful objective while avoiding explicit safety triggers.
% % % To defend against these attacks, \cite{Agarwal2024PromptLE} evaluates methods like XML tagging, structured outputs, and query-rewriting.



% {{Prompt Nesting Attack.}}
% Prompt nesting conceals malicious intents within seemingly benign prompts, altering the LLM’s context to bypass security features \cite{Li2023DeepInceptionHL,ding2024wolf,Li2023MultistepJP}.

% {{Multi-turn Dialogue Attack.}}
% Multi-turn dialogue attacks exploit the iterative nature of conversations to gradually erode content restrictions across multiple rounds of interaction \cite{zhou2024speak,yang2024chain,cheng2024leveraging,Russinovich2024GreatNW}.