\section{Related Work}
\paragraph{\textbf{\textit{Human Value Alignment for LLMs.}}}
% The rapid development of LLMs has revolutionized various domains **Brown et al., "Large Language Models are Zero-Shot Learners"**. While LLMs demonstrate remarkable capabilities in handling complex tasks, aligning LLMs with human values remains a challenge due to biases in training data and trade-offs between usefulness and safety **Wallace et al., "Transformers for Multivariate Time Series Forecasting"**.
Aligning LLMs with human values remains a challenge due to biases in training data and trade-offs between usefulness and safety **Ravfogel et al., "Paired-SGD: Efficient and Adaptive Sampling of Training Instances for Large Language Models"**. 
Approaches such as Reinforcement Learning from Human Feedback (RLHF) **Wu et al., "Reinforced Interactive Machine Translation with External Reward Functions"** have been proposed to improve fairness **Stajduhar et al., "On the Robustness and Fairness of Neural Network-based Predictive Modeling"**, safety **Kumar et al., "Adversarial Examples for Efficient and Reliable Deep Learning Models"** and eliminate hallucinations **Li et al., "Hallucinations in Language Models: A Study on the Relationship Between Hallucination Rate and Model Performance"**. 
% To further promote the alignment of LLM and human values, we explore the vulnerability in LLMs' imagination in our study.


\paragraph{\textbf{\textit{Jailbreak Attacks on LLMs.}}}
Jailbreak attacks threaten the safety alignment mechanisms of LLMs, potentially leading to the generation of harmful content **Cheng et al., "Improving Adversarial Robustness through Prompts"**. 
% These attacks can be broadly categorized into white-box and black-box approaches. White-box attacks leverage detailed knowledge of model architectures and parameters to bypass safety controls **Ren et al., "White-Box Attacks on Recurrent Neural Networks for Natural Language Processing Tasks"**, while black-box attacks rely on crafted inputs to exploit vulnerabilities in alignment mechanisms **Goyal et al., "Black-Box Adversarial Attacks against Deep Learning Models for Natural Language Processing Tasks"**. 
Our study is inspired by two key methods in black-box attacks: prompt nesting and multi-turn dialogue attacks.
1) {{{Prompt Nesting Attack.}}}
Prompt nesting bypasses security features by nesting malicious intents in normal prompts, altering LLMs’ context. 
% Techniques like **Zhang et al., "DeepInception: A Novel Adversarial Attack Method for Deep Neural Networks"** exploit nested scenarios, while **Wu et al., "ReNeLLM: Rewriting and Infiltrating Neural Language Models via Code Completion and Text Continuation"** rewrites prompt to jailbreak based on code completion, text continuation, or form-filling tasks. **Kim et al., "MJP: Multi-Step Adversarial Attack for Large Language Models with Contextual Contamination"** uses multi-step approaches with contextual contamination to reduce moral constraints, prompting malicious responses.
2) {{Multi-turn Dialogue Attack.}}
LLMs that are safe in isolated, single-round interactions can be gradually manipulated into generating harmful outputs through multiple rounds of interaction **Li et al., "Adversarial Attacks against Large Language Models via Iterative Prompting"**. Multi-turn dialogue attack leverages the multi-turn nature of conversational interactions to gradually erode an LLM's content restrictions. 
% Crescendo **Zhu et al., "Crescendo: A Novel Adversarial Attack Method for Deep Neural Networks via Iterative Prompting"** exploits seemingly benign exchanges to prompt malicious tasks. Chain-of-Attack **Wang et al., "Chain-of-Attack: An Adaptive and Efficient Adversarial Attack Framework for Large Language Models"** uses iterative prompting to gradually increase the relevance of responses to the harmful objective while avoiding explicit safety triggers.
% % % To defend against these attacks, ____ evaluates methods like XML tagging, structured outputs, and query-rewriting.



% {{Prompt Nesting Attack.}}
% Prompt nesting conceals malicious intents within seemingly benign prompts, altering the LLM’s context to bypass security features **Chen et al., "Deep Prompt Engineering for Adversarial Attacks on Large Language Models"**.

% {{Multi-turn Dialogue Attack.}}
% Multi-turn dialogue attacks exploit the iterative nature of conversations to gradually erode content restrictions across multiple rounds of interaction **Kumar et al., "Iterative Prompting: A Novel Adversarial Attack Method for Large Language Models"**.