\subsection{Can \texttt{DINN}'s attention mechanism enhance portfolio efficiency across varying market conditions?}

A central question in applying deep learning models to portfolio management is whether these models can systematically identify and emphasize assets that represent the market well while delivering favorable risk-adjusted returns under varying conditions. To explore this, we analyze the performance of \texttt{DINN} under four distinct macroeconomic regimes: the COVID-19 pandemic (March to June 2020), periods of elevated weekly initial jobless claims (ICSA), surges in new home sales (HSN1), and extremely low consumer sentiment (UMCS). During each regime, we evaluate four asset selection strategies: (1) \texttt{DINN}, using stocks deemed “important” by the prob-sparse attention mechanism, (2) Other, consisting of stocks not selected by the attention module, (3) Random, constructed with intentionally corrupted embeddings, and (4) Uniform, representing an equal-weighted portfolio.

\begin{table}[!htbp]
\centering
\input{Tables/2_attention_result}
\captionsetup{font=footnotesize}
\caption{Performance comparison of portfolios constructed using different stock selection approaches across various market regimes from 2020 to 2023. \texttt{DINN} represents portfolios consisting of stocks selected by the prob-sparse attention mechanism, Other comprises stocks not selected by the attention mechanism, Random uses intentionally corrupted embedding information, and Uniform represents equal-weighted portfolios. Market regimes include: COVID-19 pandemic (March-June 2020), elevated initial jobless claims (ICSA $\ge$ 100,000, March-August 2020), housing market expansion (HSN1 $\ge$ 500, June-November 2022), and low consumer sentiment (UMCS $\le$ 60, May-December 2022). Performance metrics include Maximum Drawdown (MDD), Value at Risk (VaR), Sharpe Ratio (SR), and terminal wealth (Wealth). Arrows indicate whether lower ($\downarrow$) or higher ($\uparrow$) values are preferred. Bold values represent the best performance for each metric within each regime. Panel A reports results for the S\&P 100 dataset, while Panel B shows results for the DOW 30 dataset.}
\label{tab2:attns_result}
\vspace{-0.5cm}
\end{table}


\Cref{tab2:attns_result} presents the results across the S\&P 100 (Panel A) and DOW 30 (Panel B) datasets. The Sharpe ratios (SR) provide an intriguing perspective on \texttt{DINN}’s performance, suggesting that the attention mechanism may prioritize assets that efficiently balance risk and return. For instance, during the COVID-19 period, \texttt{DINN}-selected portfolios achieve an SR of 1.14 for the S\&P 100 and 0.87 for the DOW 30, exceeding the SR of portfolios formed from non-selected stocks (0.59 and 0.58, respectively). From an investment opportunity perspective\cite{kim2014cost}, these higher SR values could indicate that the attention mechanism identifies assets that approximate the efficient frontier more closely, potentially allowing for a more effective replication of market dynamics with fewer assets. In contrast, portfolios based on Random embeddings perform comparably to Uniform portfolios, with Sharpe ratios clustering close to those of simple equal-weighted strategies. This result appears to suggest that when embeddings are corrupted, the attention mechanism may lose its ability to prioritize meaningful assets effectively, leading to portfolios that do not achieve the same level of risk-adjusted returns observed with \texttt{DINN}.  

Other metrics, such as maximum drawdown (MDD) and terminal wealth, provide further observations. For example, in the ICSA regime, \texttt{DINN}-selected portfolios demonstrate lower MDD and higher terminal wealth compared to other strategies. These patterns suggest that \texttt{DINN}’s attention mechanism may adaptively select assets to mitigate downside risks while maintaining portfolio growth across diverse conditions. So, the results may indicate that \texttt{DINN}’s attention mechanism could contribute to constructing portfolios that more closely approximate the efficient frontier by selecting assets that represent the market effectively.

\subsection{How do we interpret $\frac{\partial \hat{w}_{t+h}}{\partial \hat{\mu}_{t+h}}$ and $\frac{\partial \hat{w}_{t+h}}{\partial \hat{\L}_{t+h}}$ within \texttt{DINN}'s portfolio decisions?}

Having established in Section 4.4 that \texttt{DINN}’s attention mechanism successfully isolates stocks of high importance, we now investigate whether such “importance” translates into an unintuitive performance gain in predictive accuracy. Specifically, we focus on two gradient-based sensitivities: (i) $\partial \hat{w}_{t+h}/\partial \hat{\mu}_{t+h}$ from Theorem 1, measuring how small changes in predicted returns $\hat{\mu}_{t+h}$ affect the model’s optimal weights $\hat{w}_{t+h}$, and (ii) $\partial \hat{w}/\partial \hat{L}$ from Theorem 2, measuring how the Cholesky factor $\hat{L}_{t+h}$ (and thus the predicted covariance $\hat{\Sigma}_{t+h}$) impacts $\hat{w}_{t+h}$. In principle, one might expect that assets whose weights are highly sensitive to errors in $\hat{\mu}_{t+h}$ or $\hat{\Sigma}_{t+h}$ (i.e., large gradients) would be more challenging to forecast, thereby yielding higher mean squared error (MSE). However, our findings contradict this conventional wisdom. When using only prediction loss, large gradients typically indicate a need for more model updates or suggest difficult-to-predict behavior. However, when incorporating decision-focused loss, we observe that these high-sensitivity assets actually show lower MSE. This occurs because DINN allocates greater learning capacity to stocks where prediction errors would result in higher decision-related costs. As a result, this improves predictive accuracy rather than increasing errors.


In \Cref{tab3:dwdmu_result}, we report the difference in MSE and MAE between “bottom” groups and “top” groups of assets, based on the absolute gradient magnitude ($\lvert\partial \hat{w}_{t+h}/\partial \hat{\mu}_{t+h}\rvert$). Specifically, the “10\%” column corresponds to $\bigr(\text{Bottom }10\% - \text{Top }10\%\bigr)$, meaning we first identify the 10\% of assets with the smallest gradients and the 10\% with the largest gradients, compute MSE or MAE for each group, and then subtract the top from the bottom. The same logic applies to the 20\% and 30\% columns. Panel A shows these differences for the S\&P 100 dataset, and Panel B for the DOW 30, spanning four macroeconomic regimes—COVID-19, ICSA, HSN1, UMCS—and the aggregated “ALL” period. For example, In Panel A, the S\&P 100 COVID-19 row have MSE of 2.4071 in the 10\% column means that the bottom-10\%-gradient group’s MSE is 2.4071 \emph{higher} than that of the top-10\%-gradient group. A similar pattern is evident across all regimes (ICSA, HSN1, UMCS) and is mirrored in MAE as well, consistently resulting in positive bottom-minus-top differences. Moving to the DOW 30 in Panel B, we observe the same phenomenon—for instance, the HSN1 regime shows a difference of 1.3214 in the 10\% column, implying the bottom group’s MSE exceeds that of the top group by 1.3214. 

We surmise that this arises because the decision-focused nature of \texttt{DINN} (and its training procedure) allocates additional modeling capacity to precisely those assets where misestimation would incur the highest decision-related costs. Consequently, \texttt{DINN} learns these “high-impact” assets more thoroughly, leading to lower MSE compared to stocks for which gradient-based sensitivities remain modest. Results for $\lvert \partial \hat{w}_{t+h}/\partial \hat{L}_{t+h} \rvert$ follow the same pattern (see Appendix A.4. for details).


\begin{table}[!htbp]
\centering
\input{Tables/3_gradient_dwdmu}
\captionsetup{font=footnotesize}
\caption{Differences in prediction error (\textit{Bottom}~\(x\%\)~\(-\)~\textit{Top}~\(x\%\)) for assets grouped by the absolute gradient $\bigl|\partial \hat{w}/\partial \hat{\mu}\bigr|$ under four macroeconomic regimes (COVID-19, ICSA, HSN1, UMCS) plus an aggregated “ALL” period. Panel A presents results for the S\&P 100 and Panel B for the DOW 30. The columns labeled 10\%, 20\%, and 30\% indicate the difference in mean squared error (MSE) or mean absolute error (MAE) between the bottom-$x\%$ group (smallest gradients) and the top-$x\%$ group (largest gradients). Each cell shows the average $\pm$ standard deviation computed across multiple training runs (random seeds). Positive values imply that high-gradient assets yield lower errors, suggesting that \texttt{DINN} prioritizes forecasting accuracy where decision-related costs are greatest.}
\label{tab3:dwdmu_result}
\vspace{-0.5cm}
\end{table}


