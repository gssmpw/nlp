\section{Related work}
In this section, we review two key research streams relevant to our work: parameter estimation in portfolio optimization and deep learning applications in financial forecasting. The first stream examines traditional and robust estimation techniques, while the second explores how modern machine learning approaches have transformed financial prediction. 

\subsection{Parameter estimation in portfolio optimization}
The foundation of modern portfolio theory rests upon accurate parameter estimation, particularly for expected returns and covariance matrices. Since \cite{Markowitz1952} seminal work establishing the mean-variance optimization framework, researchers have grappled with the challenge of reliably estimating these crucial parameters from historical data \cite{tan2020estimation, firoozye2023canonical}. This challenge has become central to the field of quantitative finance, as the performance of optimal portfolios heavily depends on the quality of these estimates.

The extensive reliance on historical financial data for parameter estimation has proven instrumental in advancing modern financial theory and practice. Historical data provides the empirical foundation for estimating critical parameters including expected returns, volatility, and covariance matricesâ€”essential inputs that drive portfolio optimization, risk management, and asset pricing models. This approach has led to breakthrough developments in financial modeling, most notably the Capital Asset Pricing Model (CAPM) \citep{sharpe1964capital, lintner1975valuation} and the Fama-French factor models \citep{fama1993common,fama2015five}. This mean-variance optimization framework, however, revealed significant challenges in parameter estimation. The sensitivity of portfolio optimization to parameter estimates was first systematically documented by \cite{michaud1989markowitz}, who characterized mean-variance optimization as "error maximization." This insight was further developed by \cite{best1991sensitivity}, who demonstrated the hypersensitivity of optimal portfolio weights to changes in mean estimates. \citep{chopra1993effect, chung2022effects} provided crucial quantitative evidence, showing that errors in mean estimates have approximately ten times the impact of errors in variance estimates on portfolio performance.

In response to these challenges, researchers developed increasingly sophisticated estimation techniques. Early efforts focused primarily on improving covariance matrix estimation. \citep{chan1999portfolio, loffler2003effects} proposed utilizing high-frequency data for enhanced volatility forecasts, while \citep{jagannathan2003risk} made the crucial observation that imposing portfolio constraints could effectively shrink extreme covariance estimates. The recognition of parameter uncertainty led to more sophisticated approaches, such as the shrinkage method \citep{ledoit2003improved, ledoit2004well,kourtis2012parameter}, which combines sample estimates with structured estimators to reduce estimation error. As understanding of estimation challenges deepened, robust estimators gained prominence, including the minimum covariance determinant (MCD) estimator \citep{rousseeuw1999fast} and the minimum volume ellipsoid (MVE) estimator \citep{van2009minimum}. A significant advancement came with \cite{gerber2022gerber} introduction of the Gerber statistic, a robust co-movement measure that extends Kendall's Tau by capturing meaningful co-movements while remaining insensitive to extreme values and noise.

The emergence of machine learning has transformed the landscape of parameter estimation \citep{kim2021mean, kim2024overview}. While traditional machine learning approaches typically treated prediction and optimization as separate steps, recent research has explored more integrated approaches. Notable contributions include the works of \cite{ban2018machine} and \cite{feng2020taming}, who demonstrated significant improvements over traditional approaches. However, these studies relied on the Sharpe ratio, which is an indirect performance measure calculated from returns and volatility after portfolio construction, rather than directly obtaining optimal portfolio weights through the optimization process itself. This indirect approach may not fully capture the actual decision-making process inherent in portfolio optimization, where the primary goal is to determine optimal portfolio weights that satisfy specific investment objectives and constraints. Fortunately, technological advances particularly in differentiable optimization have opened new frontiers. The introduction of cvxpylayers \citep{cvxpylayers2019} and the work of \cite{amos2017optnet} on differentiable optimization layers have enabled end-to-end training of machine learning models that incorporate the portfolio optimization step directly into the parameter estimation process. While these approaches represent significant progress in bridging the gap between prediction and optimization, current implementations often rely on simplistic linear models that may not fully capture the complex, non-linear dynamics of financial markets \citep{costa2023distributionally, anis2025end}. Moreover, these models typically focus on a limited set of financial variables, potentially overlooking important external factors such as macroeconomic condition, stock and sector relationship that can significantly impact portfolio performance. See \cite{lee2024overview} for more detailed review of the evolution from traditional two-stage approaches to modern end-to-end learning frameworks, including decision-focused learning (DFL) methodologies.

\subsection{Time-series forecasting with deep learning}

The application of deep learning to financial time-series forecasting represents a significant advancement in addressing the parameter estimation challenges. While traditional approaches to parameter estimation often struggle with the complex, non-linear relationships inherent in financial markets, deep learning models have demonstrated remarkable capability in capturing these dynamics. However, as discussed in our examination of parameter estimation challenges, improved predictive accuracy does not necessarily translate to better portfolio decisions.

Recent advances in deep learning architectures, particularly those based on attention mechanisms, have revolutionized time-series forecasting across various domains. The Transformer architecture and its variants have achieved state-of-the-art performance in various domains through innovations in handling long sequences \citep{informer_2021}, capturing interactions between different time scales \citep{zhou2022fedformer}, and modeling temporal patterns \citep{wu2023timesnet}. Unlike traditional autoregressive predictors such as LSTM, transformer-based models employ \textit{generative-style decoders} as non-autoregressive predictors, facilitating more efficient time series prediction. Notable advances include the Crossformer architecture \citep{zhang2023crossformer}, which explicitly models cross-dimensional dependencies, and PatchTST \citep{Yuqietal_PatchTST}, which adapts vision Transformer techniques to time-series data. The iTransformer \citep{liu2023itransformer} further enhances this approach by treating the temporal dimension as channels, enabling more efficient processing of long sequences. Also, The emergence of Large Language Models (LLMs) has introduced new possibilities for forecasting. Recent works such as Chronos \citep{ansari2024chronos} and GPT4TS \citep{zhou2023one} demonstrate that LLMs can effectively capture complex temporal patterns while incorporating broader market context. The PAttn framework \citep{tan2024language} specifically addresses various forecasting problems by combining linguistic and numerical features in a unified architecture.

However, these advances in predictive modeling, while impressive, often fall short in addressing the fundamental challenges of portfolio optimization. The primary limitation lies in their focus on minimizing prediction error rather than optimizing investment decisions. Even when these models incorporate financial performance metrics like the Sharpe ratio into their loss functions, they typically do so in a manner that fails to capture the full complexity of the portfolio optimization problem. This disconnect becomes particularly apparent when considering the challenges identified in our parameter estimation analysis \citep{hwang2024temporal}. While deep learning models may achieve superior accuracy in forecasting individual asset returns or volatilities, they often fail to account for the complex interplay between estimation errors and portfolio weights that makes the parameter estimation problem so challenging. The sensitivity of optimal portfolio weights to small changes in input parameters, as demonstrated by \citep{chopra1993effect}, suggests that even highly accurate predictions may lead to suboptimal portfolio decisions if the prediction-optimization interface is not properly considered. 

The application of general-purpose time-series models to financial markets presents additional challenges beyond prediction accuracy. While models like TimesNet \citep{wu2023timesnet} and Fedformer \citep{zhou2022fedformer} excel in capturing temporal dependencies, they often struggle to incorporate broader macroeconomic factors and market conditions that significantly influence asset prices. These models typically focus on historical price patterns while failing to account for important external factors such as monetary policy changes, real estate prices, or shifts in market sentiment. This limitation extends beyond individual models, as most existing research has focused primarily on pattern recognition within historical data. A more promising direction may be integrating deep learning models with methods that can effectively incorporate broader market context and macroeconomic indicators.

The evolution of deep learning approaches in financial time-series forecasting thus mirrors the broader challenges in portfolio optimization: while technical capabilities continue to advance, the fundamental challenge lies not in improving predictive accuracy, but in developing frameworks that directly optimize for investment decisions. This observation reinforces our motivation for developing more integrated approaches that combine the representational power of modern deep learning architectures with explicit consideration of the portfolio optimization objective.
