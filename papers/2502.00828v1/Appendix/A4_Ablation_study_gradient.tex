\section*{Appendix A.4: Additional Gradient-Based Analysis for $\boldsymbol{\hat{L}_{t+h}}$} \label{Appendix:A5}

Appendix K provides a more extensive examination of how $\bigl|\partial \hat{w}_{t+h} / \partial \hat{L}_{t+h}\bigr|$ influences forecast accuracy under a decision-focused neural network framework. In the main text, Section 4.5 focuses on gradient-based sensitivities for the predicted mean, $\hat{\mu}_{t+h}$. Here, we address corresponding sensitivities for $\hat{L}_{t+h}$, the Cholesky factor of the predicted covariance $\hat{\Sigma}_{t+h}$. Conceptually, assets whose allocations are highly sensitive to $\hat{L}_{t+h}$ (that is, those exhibiting large magnitudes for $\partial \hat{w}_{t+h} / \partial \hat{L}_{t+h}$) should receive more modeling “effort,” since inaccurate estimation of their covariance structure could prove costly for downstream portfolio decisions. If a model is truly decision-focused, it will tend to reduce errors specifically for high-sensitivity assets, thereby securing improved overall portfolio performance.

To explore this phenomenon, we replicate the same grouping strategy used in Section 4.5. Results in \Cref{tab:abs_dwdl} consistently exhibit positive values for both MSE and MAE differences across most market regimes and for both datasets. This indicates that assets with higher gradient magnitudes, which the portfolio optimization layer deems more influential for risk management, experience smaller forecasting errors than do lower-sensitivity assets. For instance, focusing on the S\&P 100 COVID row in Panel A, the difference of 2.3696 for MSE in the 10\% column means that the bottom group’s mean squared error is 2.3696 points larger than that of the top group. The same type of result characterizes other regimes, such as ICSA, HSN1, and UMCS, as well as the ALL category that aggregates the entire test period. A parallel pattern arises in the DOW 30 data, reinforcing the same conclusion: the bottom group (in terms of $\bigl|\partial \hat{w}_{t+h} / \partial \hat{L}_{t+h}\bigr|$) is forecast less accurately than the top group. Observing larger differences supports the notion that the decision-focused approach dedicates more learnable parameters or training “focus” to stocks whose covariance-factor misestimation would most detrimentally affect the risk-return trade-off.


\setcounter{table}{2}
\renewcommand{\thetable}{A.\arabic{table}}
\begin{table}[!htbp]
\centering
\input{Tables/3_gradient_dwdl}
\captionsetup{font=footnotesize}
\caption{Differences in prediction error (\textit{Bottom}~$x\%$~$-$~\textit{Top}~$x\%$) for assets grouped by the absolute gradient $\bigl\lvert \partial \hat{w}_{t+h}/\partial \hat{\mu}_{t+h}\bigr\rvert$ under four macroeconomic regimes (COVID-19, ICSA, HSN1, UMCS) plus an aggregated “ALL” period. Panel A presents results for the S\&P 100 and Panel B for the DOW 30. The columns labeled 10\%, 20\%, and 30\% indicate the difference in mean squared error (MSE) or mean absolute error (MAE) between the bottom-$x\%$ group (smallest gradients) and the top-$x\%$ group (largest gradients). Each cell shows the average $\pm$ standard deviation computed across multiple training runs (random seeds). Positive values imply that high-gradient assets yield lower errors, suggesting that \texttt{DINN} prioritizes forecasting accuracy where decision-related costs are greatest.}
\label{tab:abs_dwdl}
\vspace{-0.5cm}
\end{table}


