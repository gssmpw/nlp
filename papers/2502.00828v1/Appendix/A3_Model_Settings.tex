\section*{Appendix A.3: Hyper-Parameter Configuration Details} \label{Appendix:A3}

All experiments in this paper were conducted on a workstation with four NVIDIA RTX 3090 GPUs. Unless otherwise noted, the training proceeded with a mini-batch size of 16. Below, we detail the key hyper-parameter ranges and selection criteria employed for model training and evaluation. The codes are available at \href{https://anonymous.4open.science/r/Decision-informed-Neural-Networks-with-Large-Language-Model-Integration-for-Portfolio-Optimization-A441/README.md}{Anonymous Github}.



\subsection*{Model Hyper-Parameters and training strategy's}
\begin{itemize}
\item \textbf{Attention Heads:} We examined configurations with either 2 or 4 attention heads. Preliminary experiments indicated that increasing attention heads can help capture more nuanced inter-asset relationships; however, larger numbers of heads also slightly increase computational cost.
\item \textbf{Encoder Depths:} We explored encoder depths of 1, 2, and 4 layers. Deeper encoders generally improved representational capacity, albeit at the risk of overfitting if not adequately regularized.
\item \textbf{LLM Hidden Dimensions:} To reduce model size while preserving performance, we tested hidden dimensions of 12, 24, 36, and 72 for the Large Language Model (LLM) backbone. These smaller dimensions (compared to standard large LLM deployments) were sufficient for the financial time-series tasks in this paper and allowed us to balance model expressiveness with training efficiency.
\item \textbf{Training Epochs and Early Stopping:} All models were trained up to a maximum of 50 epochs. We employed early stopping on a validation set to prevent overfitting, monitoring the composite loss (prediction + decision-focused components) for convergence.
\item \textbf{Optimizer and Learning Rate:} We used the Adam optimizer with a base learning rate of $1 \times 10^{-4}$. Additionally, we adopted the following dynamic learning rate adjustment strategy \citep{jin2023time}.

\end{itemize}

\subsection*{Portfolio Optimization Settings}
For the decision-focused optimization component, we selected the risk-aversion parameter $\lambda$ from a candidate set $\{0.0145,\ 0.2656,\ 0.9545,\ 2.4305,\ 3.4623\}$. These five values respectively correspond to portfolios that may be characterized as highly aggressive, aggressive, balanced, conservative and highly conservative.
The final $\lambda$ used throughout the main text was $0.9545$, which corresponds to the “balanced” risk profile. For completeness, \Cref{ap1:abl_risk} compares the out-of-sample portfolio performance under these various risk-aversion levels.


\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\begin{table}[!htbp]
\centering
\input{Appendix_Tables/1_abl_risk_aversion}
\captionsetup{font=footnotesize}
\caption{Comparative performance metrics for various time series models applied to the S\&P100 and DOW 30 dataset. Each entry represents the mean metric value along with the standard deviation. Metrics include Annualised Return (Ret), Annualised Standard Deviation (Std), Sharpe Ratio (SR), Sortino Ratio (SOR), Maximum Drawdown (MDD), Monthly 95\% Value-at-Risk (VaR), Return Over VaR (RoV), and accumulated terminal wealth (Wealth). Higher values are desirable for Ret, SR, SOR, RoV, and Wealth; while lower values are preferred for Std, MDD, and VaR. All values are presented as mean $\pm$ standard deviation across experimental trials. \textbf{Bold} values indicate the best performance for each metric, with upward ($\uparrow$) and downward ($\downarrow$) arrows indicating the desired direction of each measure.}
\label{ap1:abl_risk}
\vspace{-0.5cm}
\end{table}

\clearpage