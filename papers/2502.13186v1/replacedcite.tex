\section{Related Work}
\label{section_relatedwork}

Hold-out estimators are commonly applied in cognitive modeling for learning data ____ for arbitrating between models. Theoretical analysis of hold-out procedures in the literature generally assumes the data stationary and independent ____. There exist some limited results for time-dependent data ____, but these involve very different approaches from ours. A key challenge in our context is that the training set is not independent from the validation set, which prevents the use of %more sophisticated 
techniques such as $V$-fold cross-validation.

Section~\ref{section_penalizedmle} is similar in design to the framework of  ____ for selecting the best histogram for density estimation or more generally to non asymptotic model selection ____. The main difference is that we are in a non stationary and non independent framework. %Therefore, to prove the oracle inequality of Section~\ref{section_penalizedmle}, we use instead a recent result for penalized log-likelihood estimators which is valid in this framework ____. 

In Section~\ref{section_penalizedmle}, unlike standard approaches in reinforcement learning (RL) ____, our goal is not to improve regret or to develop algorithms optimizing rewards as in ____; instead, we aim to understand how an individual learns. We select the contextual bandit model that best fits an individual's learning curve from their learning data, without assuming the individual understands the context-action relationship. Thus, we seek the most realistic model rather than an optimal one. This theoretical statistical problem was first studied in ____.%, but 
Our method goes further and accommodates misspecified models.


At first sight, this problem may seem similar to the perspectives of Imitation Learning (IL) and Inverse Reinforcement Learning (IRL).
Our work consists in trying to reproduce the learning curve of an expert (the individual under observation).
However, IL methods use data from an expert who has already mastered the task ____ and learn to perform it by copying them, while IRL ____ aims to infer an underlying reward function based on the expert's observed behavior across multiple trajectories. In contrast, in the field of cognition, experimenters control the reward function and seek to infer an individual's behavior based on a single learning trajectory.


%From an Imitation Learning (IL) or Inverse Reinforcement Learning (IRL) point of view, this problem could be seen as a learner trying to reproduce the learning curve of an expert. Usually in IL ____, we observe an expert who has already mastered the task, so the input data of a classic IL algorithm are not learning data. In IRL ____, %MLE might be used on data ____ but the IRL learner's goal is to infer the underlying reward function that best explains the expert's observed behavior thanks to multiple trajectories and then use this inferred reward function to guide its own decision-making. In our setting, it is the experimenter that controls the reward function and his goal is to infer the individual's actions, thanks to a single learning trajectory.


Our work is close to ____ who estimate how a learner's behavior evolves with Bayesian models, however they focus on reward estimation. Therefore we cannot compare their method (whose output is a reward function) with our method (whose output is a policy).






\begin{comment}