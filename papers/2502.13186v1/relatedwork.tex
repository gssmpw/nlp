\section{Related Work}
\label{section_relatedwork}

Hold-out estimators are commonly applied in cognitive modeling for learning data \citep{mezzadri2022hold,james2023strategy} for arbitrating between models. Theoretical analysis of hold-out procedures in the literature generally assumes the data stationary and independent \citep{massart2007concentration,arlot_lerasle,arlot2010survey}. There exist some limited results for time-dependent data \citep{Opsomer}, but these involve very different approaches from ours. A key challenge in our context is that the training set is not independent from the validation set, which prevents the use of %more sophisticated 
techniques such as $V$-fold cross-validation.

Section~\ref{section_penalizedmle} is similar in design to the framework of  \citet{castellan} for selecting the best histogram for density estimation or more generally to non asymptotic model selection \citep{massart2007concentration}. The main difference is that we are in a non stationary and non independent framework. %Therefore, to prove the oracle inequality of Section~\ref{section_penalizedmle}, we use instead a recent result for penalized log-likelihood estimators which is valid in this framework \citep{aubert:generalpmle}. 

In Section~\ref{section_penalizedmle}, unlike standard approaches in reinforcement learning (RL) \citep{BubeckBianchi}, our goal is not to improve regret or to develop algorithms optimizing rewards as in \citep{dimakopoulou2017estimation,foster2019model,NEURIPS2020_751d5152}; instead, we aim to understand how an individual learns. We select the contextual bandit model that best fits an individual's learning curve from their learning data, without assuming the individual understands the context-action relationship. Thus, we seek the most realistic model rather than an optimal one. This theoretical statistical problem was first studied in \citep{aubert23}.%, but 
Our method goes further and accommodates misspecified models.


At first sight, this problem may seem similar to the perspectives of Imitation Learning (IL) and Inverse Reinforcement Learning (IRL).
Our work consists in trying to reproduce the learning curve of an expert (the individual under observation).
However, IL methods use data from an expert who has already mastered the task \citep{hussein2017imitation} and learn to perform it by copying them, while IRL \citep{arora2021survey} aims to infer an underlying reward function based on the expert's observed behavior across multiple trajectories. In contrast, in the field of cognition, experimenters control the reward function and seek to infer an individual's behavior based on a single learning trajectory.


%From an Imitation Learning (IL) or Inverse Reinforcement Learning (IRL) point of view, this problem could be seen as a learner trying to reproduce the learning curve of an expert. Usually in IL \citep{hussein2017imitation}, we observe an expert who has already mastered the task, so the input data of a classic IL algorithm are not learning data. In IRL \citep{arora2021survey}, %MLE might be used on data \citep{ramponi2020inverse} but the IRL learner's goal is to infer the underlying reward function that best explains the expert's observed behavior thanks to multiple trajectories and then use this inferred reward function to guide its own decision-making. In our setting, it is the experimenter that controls the reward function and his goal is to infer the individual's actions, thanks to a single learning trajectory.


Our work is close to \citet{huyuk2022inverse,schulz2015learning,schulz2018putting} who estimate how a learner's behavior evolves with Bayesian models, however they focus on reward estimation. Therefore we cannot compare their method (whose output is a reward function) with our method (whose output is a policy).






\begin{comment}