\documentclass[acmsmall]{acmart}

\DeclareUnicodeCharacter{0301}{\'{e}}

% \usepackage{url}
% \usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     filecolor=magenta,      
%     urlcolor=blue}
\usepackage{xcolor}
\usepackage{float}
\usepackage{amsmath}

%\usepackage{amssymb}
%\usepackage{mathabx}
\usepackage{amsthm}
% \usepackage{algpseudocode}
% \usepackage{algorithm}
\usepackage{graphicx}
% \usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{positioning, calc, arrows.meta,automata,positioning}
\usepackage{cleveref}
\usepackage{subcaption}

\input{./ags.tex}

\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{0.0}

\acmJournal{TOMS}
\acmVolume{0}
\acmNumber{0}
\acmArticle{0}
\acmMonth{0}

\acmSubmissionID{0-0-0}

\citestyle{acmauthoryear}

\usepackage{ulem}
\newcommand{\FJHNote}[1]{\textcolor{blue}{#1}}

\begin{document}

\title{A Unified Implementation of Quasi-Monte Carlo Generators, Randomization Routines, and Fast Kernel Methods}

\author{Aleksei G Sorokin}
%\author{Fred J Hickernell}
%\authornote{Both authors contributed equally to this research.}
\email{asorokin@hawk.iit.edu}
\orcid{0000-0003-1004-4632}
%\authornotemark[1]
\affiliation{%
  \institution{Illinois Institute of Technology, Department of Applied Mathematics}
  \city{Chicago}
  \state{Illinois}
  \country{USA}
}

% \affiliation{%
%   \institution{Illinois Institute of Technology, Department of Applied Mathematics}
%   \city{Chicago}
%   \state{Illinois}
%   \country{USA}
% \email{hickernell@iit.edu}
% }

%\renewcommand{\shortauthors}{Trovato et al.}

% http://dl.acm.org/ccs.cfm
\begin{abstract}
    Quasi-random sequences, also called low-discrepancy sequences, have been extensively used as efficient experimental designs across many scientific disciplines. This article provides a unified description and software API for methods pertaining to low-discrepancy point sets. These methods include low discrepancy point set generators, randomization techniques, and fast kernel methods. Specifically, we provide generators for lattices, digital nets, and Halton point sets. Supported randomization techniques include random permutations / shifts, linear matrix scrambling, and nested uniform scrambling. Routines for working with higher-order digital nets and scramblings are also detailed. For kernel methods, we provide implementations of special shift-invariant and digitally-shift invariant kernels along with fast Gram matrix operations facilitated by the bit-reversed FFT, the bit-reversed IFFT, and the FWHT. A new digitally-shift-invariant kernel of higher-order smoothness is also derived. We also describe methods to quickly update the matrix-vector product or linear system solution after doubling the number of points in a lattice or digital net in natural order.
    
    % Quasi-random point sets were originally developed to replace the independent and identically distributed (IID) sequences used in classic Monte Carlo (MC) methods for high dimensional integral approximation. IID randomizations of low-discrepancy sequences gave way to randomized Quasi-Monte Carlo (RQMC) methods which enabled practical error analysis and yielded improved convergence rates. Specifically, IID-MC has a root mean squared error (RMSE) of order $\calO(n^{-1/2})$ in the number of samples $n$ while, for smooth integrands, RQMC methods have an RMSE of order $\calO(n^{-3/2+\delta})$ with $\delta>0$ arbitrarily small. Higher-order low-discrepancy point sets were also developed, and, for integrands with additional smoothness, a nested uniform scrambling (NUS) randomization of higher-order digital nets was shown to have an RMSE of order $\calO(n^{-\alpha-1/2+\delta})$.
    
    % Low-discrepancy point sets have also been used in kernel methods to quickly compute discrepancies and quickly fit kernel interpolants. While the standard costs of these tasks are $\calO(n^2)$ and $\calO(n^3)$ respectively, pairing lattice points in linear order or digital nets in natural order with certain kernels has been shown to yield nicely structured Gram matrices which enable both costs to be reduced to $\calO(n \log n)$. We derive a new digitally-shift-invariant kernel of higher-order smoothness to pair with digital nets. The resulting fast algorithms utilize the Fast Fourier Transform (FFT), inverse FFT (IFFT), and the Fast Walsh-Hadamard Transform (FWHT). We show that by using lattices in natural order we can skip the initial and final bit-reversal steps in the FFT and IFFT respectively for decimation-in-time algorithms. We also describe methods to quickly update the matrix-vector product or linear system solution after doubling the number of points in a lattice or digital net in natural order.
\end{abstract}

% https://dl.acm.org/ccs#
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002950.10003705.10003708</concept_id>
       <concept_desc>Mathematics of computing~Statistical software</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003705.10011686</concept_id>
       <concept_desc>Mathematics of computing~Mathematical software performance</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003705.10003707</concept_id>
       <concept_desc>Mathematics of computing~Solvers</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003648.10003671</concept_id>
       <concept_desc>Mathematics of computing~Probabilistic algorithms</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Statistical software}
\ccsdesc[300]{Mathematics of computing~Mathematical software performance}
\ccsdesc[300]{Mathematics of computing~Solvers}
\ccsdesc[100]{Mathematics of computing~Probabilistic algorithms}

\keywords{Low-discrepancy sequences, low-discrepancy point set randomizations, higher-order digital net scrambling, randomized Quasi-Monte Carlo, fast kernel methods, shift-invariant kernels, digitally-shift-invariant kernels}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\section{Introduction}

This article describes and implements a number of algorithms related to low-discrepancy point sets. These include implementations of
\begin{itemize}
    \item low-discrepancy point set generators and randomizations,
    \item higher-order digital net generators and randomizations, 
    \item shift-invariant and digitally-shift-invariant kernels of higher order smoothness,
    \item and fast transforms which enable fast Gram matrix operations when special kernels are paired with certain low-discrepancy point sets. 
\end{itemize}
While these methods have been developed and often extensively studied throughout the literature, we found implementations to be scattered or even missing in some popular programming languages. Our main contribution is to provide a unified and accessible software for computations using low-discrepancy sequences in the \texttt{QMCPy}\footnote{\url{https://qmcpy.readthedocs.io}} Python package \citep{choi.QMC_software,choi.challenges_great_qmc_software}.  We also integrate with the new \texttt{LDData}\footnote{\url{https://github.com/QMCSoftware/LDData}} repository which provides generating vectors for lattices and generating matrices for digital nets in standardized formats. This collection is, so far, mainly an aggregation of results from the \texttt{Magic Point Shop}\footnote{\url{https://people.cs.kuleuven.be/~dirk.nuyens/qmc-generators/}} \citep{MPS} and the websites of Frances Kuo on lattices\footnote{\url{https://web.maths.unsw.edu.au/~fkuo/lattice/index.html}} \citep{cools2006constructing,nuyens2006fast} and Sobol' points\footnote{\url{https://web.maths.unsw.edu.au/~fkuo/sobol/index.html}} \citep{joe2003remark,joe2008constructing}.

Below we summarize the novel components of our \texttt{QMCPy} software which we detail in this paper. Comparisons to existing implementations are also discussed. A more comprehensive review of Quasi-Monte Carlo software is given in \citep{choi.QMC_software}, along with more details on the \texttt{QMCPy} framework. 
\begin{description}
    \item[Lattice point sets] We support generating points in linear, natural, or Gray code order and randomizing points with shifts modulo one. Other implementations of shifted lattices include those in MATLAB's \texttt{GAIL}\footnote{\url{http://gailgithub.github.io/GAIL_Dev/}} (Guaranteed Automatic Integration Library) \citep{GAIL.software,hickernell2018monte}, the C++ \texttt{LatNet Builder}\footnote{\url{https://github.com/umontreal-simul/latnetbuilder}} software \citep{LatNetBuilder.software}, and the multi-language \texttt{MPS}\footnote{\url{https://people.cs.kuleuven.be/~dirk.nuyens/qmc-generators/}} (Magic Point Shop) \citep{MagicPointShop.software}. \texttt{LatNet Builder} and \texttt{MPS} also provide search routines for finding good generating vectors which define lattice point sets.
    \item[Digital nets in base $2$] We support generating digital nets in natural or Gray code order and randomizing with linear matrix scrambling (LMS) \citep{owen.variance_alternative_scrambles_digital_net}, digital shifts, nested uniform scrambling (NUS) \citep{owen1995randomly}, and permutation scrambling. To the best of our knowledge, we are also the only Python implementation to support digital interlacing for the creating of higher order digital nets and higher order scrambling with either LMS or NUS. 
    
    Early implementations of unrandomized digital sequences, including the Faure, Sobol' and Niederreiter-Xing constructions, can be found in \citep{fox1986algorithm,bratley1992implementation,bratley2003implementing,pirsic2002software}. Considerations for implementing scrambles for digital sequences was discussed in \citep{hong2003algorithm}. Recent digital net implementations supporting LMS and digital shifts  include those in \texttt{MPS}, PyTorch \citep{paszke2019pytorch}, SciPy \citep{virtanen2020scipy}, and MATLAB \citep{MATLAB}. \texttt{LatNet Builder} provides robust support for all randomization routines and digital interlacing for higher order nets and scramblings. \texttt{LatNet Builder} and \texttt{MPS} also provide search routines for finding good generating matrices which define digital nets. 
    \item[Halton point sets] As with digital nets, we support randomizing Halton point sets with LMS, digital shifts, NUS, and permutation scrambles. The implementation of Halton point sets and randomizations have been treated in \citep{owen_halton,wang2000randomized}. The \texttt{QRNG}\footnote{https://cran.r-project.org/web/packages/qrng/qrng.pdf} (Quasi-Random Number Generators) R package \citep{qrng.software} implements generalized digital nets which use optimized digital permutation scrambles; these are also supported in \texttt{QMCPy}. \texttt{LatNet Builder} also provides support for Halton points and their randomizations. 
    \item[Kernel Methods and Fast Transforms] We provide routines to evaluate special shift-invariant and digitally-shift-invariant kernels. This includes a new digitally-shift-invariant kernel of higher-order smoothness derived in this paper. These are not readily available elsewhere. 
    
    We also provide routines for fast matrix-vector multiplication and inversion of the nicely-structured Gram matrices produced when pairing these special kernels with certain low-discrepancy sequences. We also describe novel routines to efficiently update the matrix-vector products and linear system solves in the Gram matrix after doubling the number of lattice or digital net points. These fast operations are backed by implementations of the Fast Walsh Hadamard Transform (FWHT), the bit-reversed fast Fourier Transform (FFT), and the bit-reversed inverse FFT (IFFT). We are among the first to provide efficient and accessible Python interfaces to such fast kernel methods and the underlying fast transforms. \texttt{GAIL} has implemented fast Bayesian cubature routines utilizing this machinery. 
\end{description}
These above routines are most commonly used in Quasi-Monte Carlo methods and for fast kernel methods, which we describe in the following subsections. 

\subsection{Quasi-Monte Carlo methods}

Monte Carlo (MC) methods approximate a high dimensional integral over the unit cube by the sample average of function evaluations at certain sampling locations:
\begin{equation}
    \mu := \int_{[0,1]^d} f(\bx) \D \bx \approx \frac{1}{n} \sum_{i=0}^{n-1} f(\bx_i) =: \hmu.
    \label{eq:mc_approx}
\end{equation}
Here $f: [0,1]^d \to \bbR$ is a given integrand and $\{\bx_i\}_{i=0}^{n-1} \in [0,1]^{n \times d}$ is a point set. The integral on the left hand side may be viewed as taking the expectation $\bbE[f(\bX)]$ where $\bX$ is a standard uniform $\bX \sim \calU[0,1]^d$. Classic MC methods choose the sampling locations to be independent and identically distributed (IID) $d$-dimensional standard uniforms $\bx_0,\dots,\bx_{n-1} \simiid \calU[0,1]^d$. IID-MC methods for \eqref{eq:mc_approx} have a root-mean-squared-error (RMSE) of $\calO(n^{-1/2})$.

\begin{figure}%[H]
    \centering
    \includegraphics[width=1\textwidth]{./figs/pointsets.eps}
    \caption{An independent identically distributed (IID) point set and low-discrepancy (quasi-random) point sets of size $n=2^{13}=8192$. Notice the quasi-random points more evenly fill the space than IID points, which leads to faster convergence of Quasi-Monte Carlo methods compared to IID Monte Carlo methods. Randomized lattices, digital nets in base $b=2$ (DN${}_{2}$), and Halton points are shown. Randomizations include shifts, linear matrix scramblings (LMS), digital shifts (DS), and nested uniform scramblings (NUS). Digital interlacing of order $\alpha$ is used to create higher-order randomized digital nets (HO${}_\alpha$).}
    \Description[]{}
    \label{fig:point sets}
\end{figure}

Quasi-Monte Carlo (QMC) methods \citep{niederreiter.qmc_book,dick.digital_nets_sequences_book,kroese.handbook_mc_methods,dick2022lattice,lemieux2009monte,sloan1994lattice} replace IID point sets with low-discrepancy (LD) point sets which more evenly cover the unit cube $[0,1]^d$. For integrands with bounded variation, plugging LD point sets into $\hmu$ \eqref{eq:mc_approx} yields a worst-case error rate of $\calO(n^{-1+\delta})$ with $\delta>0$ arbitrarily small. Some popular LD point sets are plotted in \cref{fig:point sets} including lattices, digital nets (including higher-order versions), and Halton point sets.

Randomized Quasi-Monte Carlo (RQMC) uses randomized LD point sets to give improved convergence rates and enable practical error estimation. Specifically, if we again assume the integrand has bounded variation, then certain RQMC methods can achieve a RMSE of $\calO(n^{-3/2+\delta})$. Moreover, let $\{\bx_i^1\}_{i=0}^{n-1}, \dots, \{\bx_i^R\}_{i=0}^{n-1}$ denote $R$ IID randomizations of a low-discrepancy point set where typically $R$ is small e.g. $R=15$. Then the RQMC estimate 
\begin{equation}
    \hmu_R = \frac{1}{R} \sum_{r=1}^R \hat{\mu}^r \qquad \text{where} \quad \hat{\mu}^r = \frac{1}{n} \sum_{i=0}^{n-1} f(\bx_i^r)
    \label{eq:RQMC}
\end{equation}
may be used to construct a practical (approximate) $100(1-\tau)\%$ confidence interval $\hmu_R \pm t_{R-1,\tau/2} \hsigma_R/\sqrt{R}$ for $\mu$ where $\hsigma_R^2 = R^{-1}(R-1)^{-1} \sum_{r=1}^R (\hmu^r - \hmu_R)^2$ and $t_{R-1,\tau/2}$ is the $\tau/2$ quantile of a Student distribution with $R-1$ degrees of freedom. RQMC estimates are detailed in \citep[Chapter 17]{owen.mc_book}, and a recent comparison of RQMC confidence interval methods can be found in \citep{l2023confidence}.  QMC error estimation is treated more broadly in \citep{owen2024error}, while \citep{clancy2014cost,adaptive_qmc,clancy2014cost} detail additional considerations for adaptive QMC algorithms. For lattices, randomization is typically done using random shifts modulo $1$. For digital nets, randomization is typically done using nested uniform scrambling (NUS) or the cheaper combination of linear matrix scrambling (LMS) with digital shifts / permutations \citep{owen.variance_alternative_scrambles_digital_net,owen_halton,owen.gain_coefficients_scrambled_halton}.

Higher-order LD point sets were designed to yield faster convergence for integrands with additional smoothness. For integrands with square integrable mixed partial derivatives up to order $\alpha>1$, plugging higher-order digital nets into $\hmu$ \eqref{eq:mc_approx} yields a worst-case error rate of $\calO(n^{-\alpha+\delta})$ \citep{dick.walsh_spaces_HO_nets,dick.qmc_HO_convergence_MCQMC2008,dick.decay_walsh_coefficients_smooth_functions}. RQMC using scrambled digital nets (via either NUS or LMS with digital permutations) combined with digital interlacing has been shown to achieve an RMSE of order $\calO(n^{-\alpha-1/2+\delta})$ \citep{dick.higher_order_scrambled_digital_nets}. %We show empirically that RQMC digital nets with LMS, digital shifts, and digital interlacing are significantly cheaper to generate than NUS point sets while still achieving the the optimal $\calO(n^{-\alpha-1/2+\delta})$ convergence rate. Higher-order digital nets with NUS and with LMS plus digital shifts are also shown in \cref{fig:point sets}.  

% \textcolor{red}{High quality pseudo-random number generators (PRNGs) have been extensively developed \citep{lecuyer.random_number_generation_book}, and are readily available in nearly all popular programming languages. Generators and randomization routines for LD point sets have been generally less readily available than their PRNG counterparts. 
% Notable exceptions include \texttt{LatNet Builder}\footnote{\url{https://github.com/umontreal-simul/latnetbuilder}} \citep{LatNetBuilder.software} and \texttt{QMCPy}\footnote{\url{https://qmcpy.readthedocs.io}} \citep{choi.QMC_software,choi.challenges_great_qmc_software}.  These softwares aggregate LD sequence generation routines and randomization algorithms from across the QMC landscape into unified frameworks, see the references therein. To the best of our knowledge, there are also the only softwares with readily available implementations of NUS and LMS randomizations for higher-order digital nets.}

% \textcolor{red}{\texttt{LatNet Builder} stands out for providing a large number of search algorithms to find good generating vectors for lattices and generating matrices for digital nets. \texttt{LatNet Builder} also contains routines to generate and randomize LD point sets from the generating vectors and matrices. \texttt{QMCPy} stands out for providing a number of stopping criterion which determine the number of samples $n$ required for an RQMC approximation to satisfy a user-specified error tolerance. \texttt{QMCPy} also contains routines to generate and randomize LD sequences, but does not contain search algorithms for finding good generating vectors and matrices.}

% \textcolor{red}{\texttt{LDData}\footnote{\url{https://github.com/QMCSoftware/LDData}} is a repository of generating vectors and generating matrices in standardized formats. This collection is, so far, mainly an aggregation of results from the \texttt{Magic Point Shop}\footnote{\url{https://people.cs.kuleuven.be/~dirk.nuyens/qmc-generators/}} \citep{MPS} and the websites of Frances Kuo on lattices\footnote{\url{https://web.maths.unsw.edu.au/~fkuo/lattice/index.html}} \citep{cools2006constructing,nuyens2006fast} and Sobol' points\footnote{\url{https://web.maths.unsw.edu.au/~fkuo/sobol/index.html}} \citep{joe2003remark,joe2008constructing}. Support for \texttt{LatNet Builder} formats is in active development. We have implemented the presented algorithms into \texttt{QMCPy} with support for \texttt{LDData} formats.}

\subsection{Kernel computations} 

Low-discrepancy (LD) point sets can also been used to accelerated many kernel computations. Let $\{\bx_i\}_{i=0}^{n-1}$ be a point set, $K: [0,1]^d \times [0,1]^d \to \bbR$ be a symmetric positive definite (SPD) kernel, and $\mK = \left(K(\bx_i,\bx_k)\right)_{i,k=0}^{n-1}$ be the $n \times n$ SPD Gram matrix. Two motivating kernel computations are described below

\begin{description}
    \item[Discrepancy Computation] The error of the Monte Carlo approximation \eqref{eq:mc_approx} can be bounded by the product two terms: a measure of variation of the function $f$ and a measure of the discrepancy of the point set $\{\bx_i\}_{i=1}^{n-1}$.  The most well known bound of this type is the Koksma-Hlawka inequality \citep{hickernell.generalized_discrepancy_quadrature_error_bound,dick.high_dim_integration_qmc_way,hickernell1999goodness,niederreiter.qmc_book}. The discrepancy is frequently computed when designing and evaluating low-discrepancy point sets, see \citep{rusch2024message} for a newer idea in this area where good point sets are generated using a neural networks trained with a discrepancy-based loss function.  
    
    Let us consider the more general setting of approximating the mean $\mu$ in \eqref{eq:mc_approx} by a weighted cubature rule $\sum_{i=0}^{n-1} \omega_i f(\bx_i)$. If $f$ is assumed to lie in the RKHS $H$, the discrepancy in the error bound of such a cubature rule takes the form
    $$\int \int K(\bu,\bv) \D \bu \D \bv - 2 \sum_{i=0}^{n-1} \omega_i \int K(\bu,\bx_i) \D \bu + \sum_{i,k=0}^{n-1} \omega_i \omega_k K(\bx_i,\bx_k)$$
    following \citep{hickernell.generalized_discrepancy_quadrature_error_bound}. Here the integrals are understood to be over the unit cube $[0,1]^d$. Evaluating this discrepancy above requires computing $\mK \bomega$ where $\bomega = \{\omega_i\}_{i=0}^{n-1}$. Moreover, the discrepancy is minimized for a given point set by setting the weights to $\bomega^\star = \mK^{-1} \bkappa$ where $\kappa_i = \int K(\bu,\bx_i) \D \bu$ for $0 \leq i < n$.
    \item[Kernel Interpolation] Suppose we would like to approximate $f: [0,1)^d \to \bbR$ given observations $\by = \{y_i\}_{i=0}^{n-1}$ of $f$ at $\{\bx_i\}_{i=0}^{n-1}$ satisfying $y_i = f(x_i)$. Then a kernel interpolant approximates $f$ by $\hf \in H$ where 
    $$\hf(\bx) = \sum_{i=0}^{n-1} \omega_i K(\bx,\bx_i)$$
    and $\bomega = \mK^{-1} \by$. The above kernel interpolant may be reinterpreted as the posterior mean in Gaussian process regression, see \citep{rasmussen.gp4ml}. Fitting a Gaussian process regression model often includes optimizing a kernel's hyperparameters, which also may be done by computing  $\mK \bomega$ and $\mK^{-1} \by$.  %Kernel interpolation, or Gaussian process regression \citep{rasmussen.gp4ml}, has recently been paired with LD sequences to accelerate Bayesian cubature \citep{rathinavel.bayesian_QMC_lattice,rathinavel.bayesian_QMC_sobol,rathinavel.bayesian_QMC_thesis} and solving PDEs with random coefficients \citep{kaarnioja.kernel_interpolants_lattice_rkhs,kaarnioja.kernel_interpolants_lattice_rkhs_serendipitous}. 
\end{description}

Underlying these problems, and many others, is the requirement to compute the matrix-vector product $\mK \by$ or solve the linear system $\mK \boldsymbol{\omega} =\by$ for some length $n$ vector $\by$.
%\FJHNote{And sometimes finding the eigenvector eigenvalue decompostion of the Gram matrix for hyperparameter optimization.} 
The standard cost of these computations are $\calO(n^2)$ and $\calO(n^3)$ respectively. One method to reduce these high costs is to induce structure into $\mK$. Structure-inducing methods pairing lattices and digital nets with special kernels were proposed in \citep{zeng.spline_lattice_digital_net} and shown to reduce the cost of both computations to $\calO(n \log n)$. Specifically, when a lattice point set $\{\bx_i\}_{i=0}^{n-1}$ in linear order is paired with a shift-invariant kernel, $\mK$ becomes circulant and computations can be performed performed using the fast Fourier transforms (FFT) and inverse FFT (IFFT). Similarly, when a base $2$ digital net $\{\bx_i\}_{i=0}^{n-1}$ is paired with a digitally-shift-invariant kernel, $\mK$ becomes nested-block-Toeplitz and these computations can be performed using the Fast Walsh-Hadamard Transforms (FWHT) \citep{fino.fwht}. Such methods have recently been used to accelerate Bayesian cubature \citep{rathinavel.bayesian_QMC_lattice,rathinavel.bayesian_QMC_sobol,rathinavel.bayesian_QMC_thesis} and solving PDEs with random coefficients \citep{kaarnioja.kernel_interpolants_lattice_rkhs,kaarnioja.kernel_interpolants_lattice_rkhs_serendipitous}.

We provide novel methods which exploit past solutions to $\mK\by$ or $\mK^{-1}\by$ when doubling the sample size from the LD sequence. This requires lattices are generated in natural order so we do not need to reorder outputs after doubling the sample size. We show that the same speedups can be achieved for lattices in natural order using bit-reversed FFT and bit-reversed IFFT algorithms. These bit-reversed algorithms simply skip the initial and final bit-reversal steps in the decimation-in-time FFT and IFFT respectively. We provide implementations of the bit-reversed FFT, bit-reversed IFFT, and FWHT. 

Shift-invariant kernels of arbitrary smoothness are well known and can be computed based on the Bernoulli polynomials \citep{kaarnioja.kernel_interpolants_lattice_rkhs,kaarnioja.kernel_interpolants_lattice_rkhs_serendipitous,cools2021fast,cools2020lattice,sloan2001tractability,kuo2004lattice}. Until now, digitally-shift-invariant kernels were only known for order $1$ smoothness \citep{dick.multivariate_integraion_sobolev_spaces_digital_nets}. We present higher-order digitally-shift-invariant kernels whose corresponding RKHSs contain functions of arbitrary smoothness. We derive closed forms of low order kernels (up to order $4$ smoothness) in base $b=2$, with the order $4$ kernel not appearing elsewhere in the literature. These kernels are also of interest for computing the worst-case error of higher-order polynomial-lattice rules \cite{baldeaux.polylat_efficient_comp_worse_case_error_cbc}, although we do not explore this connection here. 

\subsection{Outline} 

%\tableofcontents

The remainder of this article is organized as follows. First, we describe our implementation in \cref{sec:implementation}. \Cref{sec:notation} discusses notation. \Cref{sec:LD_seq_and_randomizations} details rank $1$ lattices, digital sequences, and Halton sequences along with their randomization routines. \Cref{sec:fast_transforms} describes accelerated kernel computation with LD sequences, matching kernels, and fast transform algorithms. \Cref{sec:numerical_experiments} provides numerical experiments. Finally, \cref{sec:conclusions_future_work} gives a brief conclusion and outlines directions for future work. 

\section{Implementation} \label{sec:implementation}

We have implemented this work into \texttt{QMCPy}\footnote{\url{https://qmcpy.readthedocs.io}} version 1.6, installable with \texttt{pip install -U qmcpy}. The architecture and philosophy underlying \texttt{QMCPy} are described in \citep{choi.QMC_software,choi.challenges_great_qmc_software}. To follow along with code snippets presented in this article, please import \texttt{QMCPy} and \texttt{NumPy} \citep{NumPy.software} via 
\lstinputlisting[style=Python]{snippets/imports.txt}
These code snippets and code producing the plots in this article are available in notebook form\footnote{\url{https://github.com/QMCSoftware/QMCSoftware/blob/master/demos/ld_randomizations_and_higher_order_nets.ipynb}}.  

Most of the \texttt{QMCPy} objects and methods presented here are wrappers around the lower level \texttt{QMCToolsCL}\footnote{\url{https://qmcsoftware.github.io/QMCToolsCL/}} package. The Python functions in \texttt{QMCToolsCL} often call lower level C implementations for improved performance. \texttt{QMCToolsCL} interfaces for other languages which support C extensions is a direction of future work.

\section{Notation} \label{sec:notation}

Bold symbols denote vectors which are assumed to be column vectors e.g. $\bx \in [0,1]^d$. Capital letters in serif font denote matrices e.g. $\mK \in \bbR^{n \times n}$. Lower case letters in serif font denote digits in a base $b$ expansion e.g. $i = \sum_{t=0}^{m-1} \mi_t b^t$ for $0 \leq i < 2^m$. This may be combined with bold notation when denoting the vector of base $b$ digits e.g. $\bmi = (\mi_0,\mi_1,\dots,\mi_{m-1})^\intercal$. Modulo is always taken component-wise e.g. $\bx \mod 1 = (x_1 \mod 1,\dots,x_d \mod 1)^\intercal$. Permutations may be specified by vectors e.g. $\pi: \{0,1,2\} \to \{0,1,2\}$ with $\pi(0) = 2$, $\pi(1) = 0$, and $\pi(2)=1$ may be denoted by $\pi = (2,0,1)$. 

\section{Low-discrepancy sequences and randomization routines} \label{sec:LD_seq_and_randomizations}

For a fixed base $b \in \bbN$, write $i \in \bbN_0$ as $i = \sum_{t=0}^\infty \mi_t b^t$ so $\mi_t$ is the $t^\mathrm{th}$ digit in the base $b$ expansion of $i$. We denote the vector of the first $m$ base $b$ digits of $i$ by
$$\bD_m(i) = (\mi_0,\mi_1,\dots,\mi_{m-1})^\intercal.$$
For $\bmi = \bD_m(i)$ we use the notation 
$$F_m(\bmi) = \sum_{t=1}^m \mi_{t-1} b^{-t}$$
to flip the digits of $i$ about the decimal and convert back to a floating point number in $[0,1)$. Finally, let
\begin{equation}
    v(i) = \sum_{t=1}^\infty \mi_{t-1} b^{-t}
    \label{eq:v}
\end{equation}
so that $v(i) = F_m(\bD_m(i))$ when $i < b^m$, which is always the case in this article. The Van der Corput sequence in base $b$ is $(v(i))_{i \geq 0}$.

\begin{figure}%[H]
    \centering
    \newcommand{\scale}{5}
    \newcommand{\axsep}{1.5}
    \newcommand{\thisDelta}{0.227}
    \begin{subfigure}[t]{.2\textwidth}
    \begin{tikzpicture}
        \draw (0,\scale+1) node{$x$};
        \draw (\axsep,\scale+1) node{$z$};
        \draw[-,color=lightgray] (0,0) -- (\axsep,0);
        \draw[-,color=lightgray] (0,\scale) -- (\axsep,\scale);
        \draw[-] (0,0) -- (0,\scale);
        \draw[-] (\axsep,0) -- (\axsep,\scale); 
        \draw[-] (0,0) -- (.4,0);
        \draw[-] (0,\scale) -- (.4,\scale);
        \draw[-] (\axsep-.4,0) -- (\axsep,0);
        \draw[-] (\axsep-.4,\scale) -- (\axsep,\scale);
        \draw (\axsep/2,\scale+2) node{shifted lattice};
        \draw (1,\scale-\scale*\thisDelta) node{$1-\Delta$};
        \draw (\axsep-1,\scale*\thisDelta) node{$\Delta$};
        \draw[-] (0,\scale-\scale*\thisDelta) -- (.4,\scale-\scale*\thisDelta);
        \draw[-] (\axsep-.4,\scale*\thisDelta) -- (\axsep,\scale*\thisDelta);
        \draw[->] (0,\scale/2-\scale*\thisDelta/2) -- (\axsep,\scale-\scale/2+\scale*\thisDelta/2);
        \draw[-] (0,\scale-\scale*\thisDelta/2)-- (\axsep/2,\scale);
        \draw[->] (\axsep/2,0)-- (\axsep,\scale*\thisDelta/2);
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.25\textwidth}
    \begin{tikzpicture}
        \draw (0,\scale+1) node{$x$};
        \draw (2*\axsep,\scale+1) node{$z$};
        \draw[-] (0,0) -- (0,\scale);
        \draw[-] (\axsep,0) -- (\axsep,\scale); 
        \draw[-] (2*\axsep,0) -- (2*\axsep,\scale); 
        \draw[-,color=lightgray] (0,0) -- (\axsep,0);
        \draw[-,color=lightgray] (0,\scale) -- (\axsep,\scale);
        \foreach \x in {0,...,3}{
            \draw[-,color=lightgray] (\axsep,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
            \draw[-] (0,\scale*\x/3) -- (.4,\scale*\x/3);
            \draw[-] (\axsep-.4,\scale*\x/3) -- (\axsep+.4,\scale*\x/3);
            \draw[-] (2*\axsep-.4,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
        }
        \foreach \x in {0,...,9}{
            \draw[-] (\axsep,\scale*\x/9) -- (\axsep+.2,\scale*\x/9);
            \draw[-] (2*\axsep-.2,\scale*\x/9) -- (2*\axsep,\scale*\x/9);
        }
        \draw (\axsep,\scale+2) node{shifted digital net};
        \draw[->] (0,\scale*1.5/9)-- (\axsep,\scale*7.5/9);
        \draw[-] (0,\scale*4.5/9)-- (\axsep*3/4,\scale);
        \draw[->] (\axsep*3/4,0)-- (\axsep,\scale*1.5/9);
        \draw[-] (0,\scale*7.5/9)-- (\axsep/4,\scale);
        \draw[->] (\axsep/4,0)-- (\axsep,\scale*4.5/9);
        \draw[->] (\axsep,\scale/18) -- (2*\axsep,\scale*3/18);
        \draw[->] (\axsep,\scale*3/18) -- (2*\axsep,\scale*5/18);
        \draw[-] (\axsep,\scale*5/18) -- (\axsep+\axsep/2,\scale/3);
        \draw[->] (\axsep+\axsep/2,0) -- (2*\axsep,\scale/18);
        \draw[->] (\axsep,\scale/3+\scale/18) -- (2*\axsep,\scale/3+\scale*3/18);
        \draw[->] (\axsep,\scale/3+\scale*3/18) -- (2*\axsep,\scale/3+\scale*5/18);
        \draw[-,densely dotted] (\axsep,\scale/3+\scale*5/18) -- (\axsep+\axsep/2,\scale/3+\scale/3);
        \draw[->,densely dotted] (\axsep+\axsep/2,\scale/3) -- (2*\axsep,\scale/3+\scale/18);
        \draw[->] (\axsep,\scale*2/3+\scale/18) -- (2*\axsep,\scale*2/3+\scale*3/18);
        \draw[->] (\axsep,\scale*2/3+\scale*3/18) -- (2*\axsep,\scale*2/3+\scale*5/18);
        \draw[-] (\axsep,\scale*2/3+\scale*5/18) -- (\axsep+\axsep/2,\scale*2/3+\scale/3);
        \draw[->] (\axsep+\axsep/2,\scale*2/3) -- (2*\axsep,\scale*2/3+\scale/18);
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.25\textwidth}
    \begin{tikzpicture}
        \draw (0,\scale+1) node{$x$};
        \draw (2*\axsep,\scale+1) node{$z$};
        \draw[-] (0,0) -- (0,\scale);
        \draw[-] (\axsep,0) -- (\axsep,\scale); 
        \draw[-] (2*\axsep,0) -- (2*\axsep,\scale);
        \draw[-,color=lightgray] (0,0) -- (\axsep,0);
        \draw[-,color=lightgray] (0,\scale) -- (\axsep,\scale); 
        \foreach \x in {0,...,3}{
            \draw[-,color=lightgray] (\axsep,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
            \draw[-] (0,\scale*\x/3) -- (.4,\scale*\x/3);
            \draw[-] (\axsep-.4,\scale*\x/3) -- (\axsep+.4,\scale*\x/3);
            \draw[-] (2*\axsep-.4,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
        }
        \foreach \x in {0,...,9}{
            \draw[-] (\axsep,\scale*\x/9) -- (\axsep+.2,\scale*\x/9);
            \draw[-] (2*\axsep-.2,\scale*\x/9) -- (2*\axsep,\scale*\x/9);
        }
        \draw (\axsep,\scale+2) node{permuted digital net};
        \draw[->] (0,\scale*1/6) -- (\axsep,\scale*5/6);
        \draw[->] (0,\scale*3/6) -- (\axsep,\scale*1/6);
        \draw[->] (0,\scale*5/6) -- (\axsep,\scale*3/6);
        \draw[->] (\axsep,\scale*1/18) -- (2*\axsep,\scale*5/18);
        \draw[->] (\axsep,\scale*3/18) -- (2*\axsep,\scale*3/18);
        \draw[->] (\axsep,\scale*5/18) -- (2*\axsep,\scale*1/18);
        \draw[->] (\axsep,\scale*7/18) -- (2*\axsep,\scale*11/18);
        \draw[->] (\axsep,\scale*9/18) -- (2*\axsep,\scale*9/18);
        \draw[->] (\axsep,\scale*11/18) -- (2*\axsep,\scale*7/18);
        \draw[->] (\axsep,\scale*13/18) -- (2*\axsep,\scale*17/18);
        \draw[->] (\axsep,\scale*15/18) -- (2*\axsep,\scale*15/18);
        \draw[->] (\axsep,\scale*17/18) -- (2*\axsep,\scale*13/18);
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[b]{.25\textwidth}
    \begin{tikzpicture}
        \draw (0,\scale+1) node{$x$};
        \draw (2*\axsep,\scale+1) node{$z$};
        \draw[-] (0,0) -- (0,\scale);
        \draw[-] (\axsep,0) -- (\axsep,\scale); 
        \draw[-] (2*\axsep,0) -- (2*\axsep,\scale); 
        \draw[-,color=lightgray] (0,0) -- (\axsep,0);
        \draw[-,color=lightgray] (0,0+\scale) -- (\axsep,0+\scale);
        \foreach \x in {0,...,3}{
            \draw[-,color=lightgray] (\axsep,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
            \draw[-] (0,\scale*\x/3) -- (.4,\scale*\x/3);
            \draw[-] (\axsep-.4,\scale*\x/3) -- (\axsep+.4,\scale*\x/3);
            \draw[-] (2*\axsep-.4,\scale*\x/3) -- (2*\axsep,\scale*\x/3);
        }
        \foreach \x in {0,...,9}{
            \draw[-] (\axsep,\scale*\x/9) -- (\axsep+.2,\scale*\x/9);
            \draw[-] (2*\axsep-.2,\scale*\x/9) -- (2*\axsep,\scale*\x/9);
        }
        \draw (\axsep,\scale+2) node{NUS digital net};
        \draw[->] (0,\scale*1/6) -- (\axsep,\scale*5/6);
        \draw[->] (0,\scale*3/6) -- (\axsep,\scale*1/6);
        \draw[->] (0,\scale*5/6) -- (\axsep,\scale*3/6);
        \draw[->] (\axsep,\scale*1/18) -- (2*\axsep,\scale*1/18);
        \draw[->] (\axsep,\scale*3/18) -- (2*\axsep,\scale*3/18);
        \draw[->] (\axsep,\scale*5/18) -- (2*\axsep,\scale*5/18);
        \draw[->] (\axsep,\scale*7/18) -- (2*\axsep,\scale*9/18);
        \draw[->] (\axsep,\scale*9/18) -- (2*\axsep,\scale*11/18);
        \draw[->] (\axsep,\scale*11/18) -- (2*\axsep,\scale*7/18);
        \draw[->] (\axsep,\scale*13/18) -- (2*\axsep,\scale*17/18);
        \draw[->] (\axsep,\scale*15/18) -- (2*\axsep,\scale*15/18);
        \draw[->] (\axsep,\scale*17/18) -- (2*\axsep,\scale*13/18);
    \end{tikzpicture}
    \end{subfigure}
    \caption{Low-discrepancy randomization routines in dimension $d=1$. Each vertical line is a unit interval with $0$ at the bottom and $1$ at the top. A given interval is partitioned at the horizontal ticks extending to the right, and then rearranged following the arrows to create the partition of the right interval as shown by the ticks extending to the left. For the shifted lattice and digitally shifted digital net, when the arrow hits a horizontal gray bar it is wrapped around to the next gray bar below. See for example the dotted line in the shifted digital net. The lattice shift is $\Delta = \thisDelta$. All digital nets use base $b=3$ and $t_\mathrm{max}=2$ digits of precision. The digital shift is $\bDelta = (2,1)^\intercal$. Dropping the $j$ subscript for dimension, the digital permutations in the third panel are $\pi_1 = (2,0,1)$ and $\pi_2 = (2,1,0)$. Notice $\pi_1$ is equivalent to a digital shift by $2$, but $\pi_2$ cannot be written as a digital shift. The nested uniform scramble (NUS) has digital permutations $\pi = (2,0,1)$, $\pi_0 = (2,1,0)$, $\pi_1 = (0,1,2)$, and $\pi_2 = (1,2,0)$. Notice the permuted digital net in third panel has permutations depending only the current digit in the base $b$ expansion. In contrast, the full NUS scrambling in the fourth panel has permutations which depend on all previous digits in the base $b$ expansion.}
    \label{fig:ld_randomizations}
\end{figure}  

\subsection{Rank 1 lattices}

Consider a fixed \emph{generating vector} $\bg \in \bbN^d$ and fixed prime base $b$. Then we define the \emph{lattice sequence} 
$$\bz_i = v(i) \bg \mod 1, \qquad i \geq 0.$$
If $n=b^m$ for some $m \in \bbN_0$, then the lattice point set $\{\bz_i\}_{i=0}^{n-1}\in [0,1)^{n \times d}$ is equivalent to $\{i/n \bg \mod 1\}_{i=0}^{n-1}$ where we say the former is in \emph{natural order} while the latter is in \emph{linear order}. 

\begin{description}
    \item[Shifted lattice] For a shift $\bDelta \in [0,1)^d$, we define the shifted point set 
    $$\bx_i= (\bz_i + \bDelta) \mod 1$$ 
\end{description}
where $\bz_i$ are lattice point set. The shift operation mod 1 is visualized in \cref{fig:ld_randomizations}. Randomized lattices use $\bDelta \sim \calU[0,1]^d$. In the following code snippet we generate $R$ independently shifted lattices with shifts $\bDelta_1,\dots,\bDelta_R \simiid \calU[0,1]^d$. 
\lstinputlisting[style=Python]{snippets/lattice.txt}
Here we have used a generating vector from \citep{cools2006constructing} which is stored in a standardized format in the \texttt{LDData} repository. Other generating vectors from \texttt{LDData} may be used by passing in a file name from \url{https://github.com/QMCSoftware/LDData/tree/main/lattice/} or by passing an explicit array. 


\subsection{Digital nets} 

Consider a fixed prime base $b$ and \emph{generating matrices} $\mC_1,\dots\mC_d \in \{0,...,b-1\}^{t_\mathrm{max} \times m}$ where $m \in \bbN$ is fixed and $b$ is a given prime base. One may relax the assumption that $b$ is prime, but we do not consider that here or in the implementation. The first $n=b^m$ points of a digital sequence form a \emph{digital net} $\{\bz_i\}_{i=1}^{b^m-1} \in [0,1)^{b^m \times d}$ in \emph{natural order}. For $1 \leq j \leq d$ and $0 \leq i < b^m$ with digits $\bmi = \bD_m(i)$,
$$\bmz_{ij} = \mC_j \bmi \mod b \qquad\text{and}\qquad z_{ij} = F_{t_\mathrm{max}}(\bmz_{ij})$$
where $\bmz_{ij}$ is the base $b$ expansion vector of $z_{ij}$. 

\begin{description}
    \item[Digitally shifted digital net] Similar to lattices, one may apply a shift $\mDelta \in [0,1)^{t_\mathrm{max} \times d}$ to the digital net to get a digitally-shifted digital net $\{\bx_i\}_{i=0}^{b^m-1}$ where 
    $$x_{ij} = F_{t_\mathrm{max}}((\bmz_{ij} + \bDelta_j ) \mod b)$$
    and $\bDelta_j$ is the $j^\mathrm{th}$ column of $\mDelta$. Randomly shifted digital nets use $\mDelta \simiid \calU\{0,\dots,b-1\}^{t_\mathrm{max} \times d}$ i.e. is each digit is chosen uniformly from $\{0,\dots,b-1\}$. 
    \item[Digitally permuted digital net] In what follows will denote permutations of $\{0,\dots,b-1\}$ by $\pi$. Suppose we are given a set of permutations
    $$\mPi = \{\pi_{j,t}: \quad 1 \leq j \leq d, \quad 0 \leq t < t_\mathrm{max}\}.$$
    Then we may construct the digitally permuted digital net $\{\bx_i\}_{i=1}^{b^m-1}$ where 
    $$x_{ij} = F_{t_\mathrm{max}}((\pi_{j,0}(\mz_{ij0}),\pi_{j,1}(\mz_{ij1}),\dots,\pi_{j,t_\mathrm{max}-1}(\mz_{ij(t_\mathrm{max}-1)}))^\intercal).$$
    Randomly permuted digital nets use independent permutations chosen uniformly over all permutations of $\{0,\dots,b-1\}$. 
    \item[Nested Uniform Scrambling (NUS)] NUS is often called Owen scrambling for its conception in \citep{owen1995randomly}. As before, $\pi$ values denote permutations of $\{0,\dots,b-1\}$. Now suppose we are given a set of permutations 
    $$\calP = \{\pi_{j,\mv_1\cdots\mv_t}: \quad 1 \leq j \leq d, \quad 0 \leq t < t_\mathrm{max}, \quad \mv_k \in \{0,\dots,b-1\} \text{ for } 0 \leq k \leq t \}.$$
    Then a \emph{nested uniform scrambling} of a digital net is $\{\bx_i\}_{i=1}^{b^m-1}$ where
    $$x_{ij} = F_{t_\mathrm{max}}((\pi_{j,}(\mz_{ij0}),\pi_{j,\mz_{ij0}}(\mz_{ij1}),\pi_{j,\mz_{ij0}\mz_{ij1}}(\mz_{ij2}),\dots,\pi_{j,\mz_{ij0}\mz_{ij1}\cdots\mz_{ij(t_\mathrm{max}-2)}}(\mz_{ij(t_\mathrm{max}-1)}))^\intercal).$$ 
    As the number of elements in $\calP$ is  
    $$\lvert \calP \rvert = d(1+b+b^2+\dots+b^{t_\mathrm{max}-1}) = d \frac{b^{t_\mathrm{max}}-1}{b-1},$$ 
    our implementation cannot afford to generated all permutations apriori. Instead, permutations are generated and stored only as needed. As with digitally-permuted digital nets, NUS uses independent uniform random permutations.  
\end{description}
The randomization routines described above are visualized in \cref{fig:ld_randomizations}. 

\emph{Linear matrix scrambling} (LMS) is a computationally cheaper but less complete version of NUS which has proven sufficient for many practical problems. LMS uses scrambling matrices $\mS_1,\dots,\mS_d \in \{0,\dots,b\}^{t_\mathrm{max} \times t_\mathrm{max}}$ and sets the LMS generating matrices $\tmC_1,\dots,\tmC_d \in \{0,\dots,b\}^{t_\mathrm{max} \times m}$ so that 
$$\tmC_j = \mS_j \mC_j \mod b$$
for $j=1,\dots,d$. Following \citep{owen.variance_alternative_scrambles_digital_net}, let us denote elements in $\{1,\dots,b\}$ by $h$ and elements in $\{0,\dots,b\}$  by $g$. Then common structures for $\mS_j$ include 
$$\begin{pmatrix} h_{11} & 0 & 0 & 0 & \dots \\ g_{21} & h_{22} & 0 & 0 & \dots \\ g_{31} & g_{32} & h_{33} & 0 & \dots \\ g_{41} & g_{42} & g_{43} & h_{44} & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix}, \begin{pmatrix} h_1 & 0 & 0 & 0 & \dots \\ g_2 & h_1 & 0 & 0 & \dots \\ g_3 & g_2 & h_1 & 0 & \dots \\ g_4 & g_3 & g_2 & h_1 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix}, \quad\text{and}\quad \begin{pmatrix} h_1 & 0 & 0 & 0 & \dots \\ h_1 & h_2 & 0 & 0 & \dots \\ h_1 & h_2 & h_3 & 0 & \dots \\ h_1 & h_2 & h_3 & h_4 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix}$$
which corresponds to Matousek’s linear scrambling \citep{MATOUSEK1998527}, Tezuka's $i$-binomial scrambling \citep{tezuka2002randomization}, and Owen's striped LMS \citep{owen.variance_alternative_scrambles_digital_net} (not to be confused with NUS which is often called Owen scrambling). Random LMS chooses $g$ and $h$ values all independently and uniformly from $\{1,\dots,b\}$ and $\{0,\dots,b-1\}$ respectively. These scrambling are described in more detail and connected to NUS in \citep{owen.variance_alternative_scrambles_digital_net}.

\emph{Digital interlacing} enables the construction of higher-order digital nets. Denote the digitally interlaced version of $\mC_1,\mC_2,\dots \in \{0,\dots,b\}^{t_\mathrm{max} \times m}$ with interlacing factor $\alpha \in \bbN$ by $\tmC_1,\tmC_2,\dots \in \{0,\dots,b\}^{\alpha t_\mathrm{max} \times m}$. Then we have $\tmC_{jtk} = C_{\tj,\tt,k}$ where $\tj = \alpha (j-1) + (t \mod \alpha) + 1$ and $\tt = \lfloor t / \alpha \rfloor$ for $j \geq 1$ and $0 \leq t < \alpha t_\mathrm{max}$ and $1 \leq k \leq m$. For example, with $m=2$, $t_\mathrm{max}=2$, and $\alpha=2$ we have the following 
$$\mC_1 = \begin{pmatrix} c_{101} & c_{102} \\ c_{111} & c_{112} \end{pmatrix} \qquad \mC_2 = \begin{pmatrix} c_{201} & c_{202} \\ c_{211} & c_{212} \end{pmatrix} \qquad \mC_3 = \begin{pmatrix} c_{301} & c_{302} \\ c_{311} & c_{312} \end{pmatrix} \qquad \mC_4 = \begin{pmatrix} c_{401} & c_{402} \\ c_{411} & c_{412} \end{pmatrix} \qquad \dots$$
$$\tmC_1 = \begin{pmatrix} c_{101} & c_{102} \\ c_{201} & c_{202} \\ c_{111} & c_{112} \\ c_{211} & c_{212} \end{pmatrix} \qquad \tmC_2 = \begin{pmatrix} c_{301} & c_{302} \\ c_{401} & c_{402} \\ c_{311} & c_{312} \\ c_{411} & c_{412}\end{pmatrix} \qquad \dots$$
Higher-order NUS requires NUS be applied to the digital net point sets generated by $\mC_1,\dots,\mC_{\alpha d}$ and then interlacing be performed to the digits $\{\bmz_{i1}\}_{i=0}^{b^m-1},\dots,\{\bmz_{i,\alpha d}\}_{i=0}^{b^m-1}$ \citep{dick.higher_order_scrambled_digital_nets}. For higher-order LMS, we apply LMS to the generating matrices $\mC_1,\dots,\mC_{\alpha d}$, then interlace them, then generate the digital net. As we shown in the numerical experiments in \cref{sec:numerical_experiments}, LMS is significantly faster than NUS (especially for higher-order nets) while still achieving higher-order rates of RMSE convergence. 

A subtle difference between the above presentation and practical implementation is that $t_\mathrm{max}$ may change with randomization in practice. For example, suppose we are given generating matrices $\mC_1,\dots,\mC_d \in \{0,\dots,b-1\}^{32 \times32}$ but would like the shifted digital net to have $64$ digits of precision. Then we should generate $\bDelta \in \{0,\dots,b-1\}^{t_\mathrm{max} \times d}$ with $t_\mathrm{max}=64$ and treat $\mC_j$ as $t_\mathrm{max} \times t_\mathrm{max}$ matrices with appropriate rows and columns zeroed out. 

Gray code ordering of digital nets enables computing the next point $\bx_{i+1}$ from $\bx_i$ by only adding a single column from each generating matrix rather. Specifically, the $q^\mathrm{th}$ column of each generating matrix gets digitally added to the previous point where $q-1$ is the index of the only digit to be changed in Gray code ordering. Gray code orderings for $b=2$ and $b=3$ are shown in \cref{tab:Gray code}. 

\begin{table}[H]
    \centering
    \begin{tabular}{r | r l | r l }
        $i$ & $i_2$ & Gray code  $i_2$ & $i_3$ & Gray code $i_3$ \\
        \hline 
        $0$ & $0000_2$ & $0000_2$ & $00_3$ & $00_3$ \\
        $1$ & $0001_2$ & $0001_2$ & $01_3$ & $01_3$ \\ 
        $2$ & $0010_2$ & $0011_2$ & $02_3$ & $02_3$ \\ 
        $3$ & $0011_2$ & $0010_2$ & $10_3$ & $12_3$ \\ 
        $4$ & $0100_2$ & $0110_2$ & $11_3$ & $11_3$ \\ 
        $5$ & $0101_2$ & $0111_2$ & $12_3$ & $10_3$ \\ 
        $6$ & $0110_2$ & $0101_2$ & $20_3$ & $20_3$ \\ 
        $7$ & $0111_2$ & $0100_2$ & $21_3$ & $21_3$ \\ 
        $8$ & $1000_2$ & $1000_2$ & $22_3$ & $22_3$
    \end{tabular}
    \caption{Gray code order for bases $b=2$ and $b=3$. In Gray code order only one digit is incremented or decremented by $1$ (modulo $b$) when $i$ is incremented by $1$.}
    \label{tab:Gray code} 
\end{table}

The following code generates a base $2$ digital net with $R$ independent LMS, $R$ independent digital shifts, and $\alpha=2$ interlacing. 
\lstinputlisting[style=Python]{snippets/dnb2.txt}
Here we have used a set of Sobol' generating matrices from Joe and Kuo\footnote{the ``new-joe-kuo-6.21201'' direction numbers from \url{https://web.maths.unsw.edu.au/~fkuo/sobol/index.html}} \citep{joe2008constructing} which are stored in a standardized format in the \texttt{LDData} repository. Other generating matrices from \texttt{LDData} may be used by passing in a file name from \url{https://github.com/QMCSoftware/LDData/blob/main/dnet/} or by passing an explicit array. 

\subsection{Halton sequences}

The digital sequences described in the previous section used a fix prime base $b$. One may allow each dimension $j \in \{1,\dots,d\}$ to have its own prime base $b_j$. The most popular of such constructions is the Halton sequence which sets $b_j$ to the $j^\mathrm{th}$ prime, sets $\mC_j$ to the identity matrix, and sets $t_\mathrm{max} = m$. This enables the simplified construction of Halton points $\{\bx_i\}_{i=0}^{n-1}$ via
$$\bx_i = (v_{b_1}(i),\dots,v_{b_d}(i))^\intercal$$
where we have added a subscript to $v$ in \eqref{eq:v} to denote the base dependence. 

Almost all the methods described for digital sequences are immediately applicable to Halton sequences after accounting for the differing bases across dimensions. However, digital interlacing is not generally applicable when the bases differ. Halton with random starting points has also been explored in \citep{wang2000randomized}, although we do not consider this here.
%Our implementation is separated into base $b=2$ digital nets and generalized digital nets. This is done because base $b=2$ digital nets can store columns of generating matrices as integers and then quickly perform digital addition using exclusive or (XOR) instructions. \texttt{QMCPy} currently only supports Halton point sets, although this is a topic for future work. 
The following code generates a Halton point set with $R$ independent LMS and $R$ independent digital permutations.
\lstinputlisting[style=Python]{snippets/halton.txt}
The \texttt{QRNG} randomization follows the QRNG software package \citep{qrng.software} in generating a generalized Halton point set \citep{faure2009generalized} using a deterministic set of  permutation scrambles and random digital shifts. 

\section{Fast transforms and kernel computations} \label{sec:fast_transforms}

Recall from the introduction that we would like to compute the matrix-vector product $\mK \by$ and solve the linear system $\mK^{-1} \by$ where $\by$ is some known length $n$ vector and $\mK = \{K(\bx_i,\bx_k)\}_{i,k=0}^{n-1}$ is a $n \times n$ symmetric positive definite (SPD) Gram matrix based on an SPD kernel $K$ and point set $\{\bx_i\}_{i=0}^{n-1}$. We may reduce the standard $\calO(n^2)$ and $\calO(n^3)$ costs of solving these respective problems to both be $\calO(n \log n)$ by inducing structure into $\mK$ using special point set-kernel pairings. Specifically, pairing lattice points with a shift invariance kernel yields a circulant Gram matrix $\mK$ for which computations can be done quickly using the bit-reversed fast Fourier transform (FFT) and bit-reversed inverse FFT (IFFT). Similarly, pairing a digital net with digitally-shift-invariant kernel yields a nested block Toeplitz matrix for which computations can be done quickly using the fast Walsh Hadamard Transform (FWHT). Shift-invariant and digitally-shift-invariant kernels are shown in \cref{fig:SI_DSI_kernels}. The reduced costs achieved by inducing these structures are summarized in \cref{tab:com_kernel_costs}.

\begin{figure}%[H]
    \centering
    \includegraphics[width=1\textwidth]{./figs/kernels.eps}
    \caption{Shift-invariant kernels (top row) and digitally-shift-invariant kernels (bottom row). Here $\alpha$ denotes the smoothness of the kernel. The $\alpha=3$ and $\alpha=4$ digitally-shift-invariant kernels are nearly indistinguishable. Shift-invariant kernels are described in \cref{subsec:SI_kernels} while digitally-shift-invariant kernels are described in \cref{subsec:DSI_kernels}.}
    \Description[]{}
    \label{fig:SI_DSI_kernels}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{ccccccc} 
        $\{\bx_i\}_{i=0}^{n-1}$ & $K$ structure & $\mK$ storage & $\mK$ decomposition & $\mK \by$ cost & $\mK^{-1} \by$ cost & methods \\ 
        \hline 
        general & SPD & $\calO(n^2)$ & $\calO(n^3)$ & $\calO(n^2)$ & $\calO(n^2)$ & standard  \\
        lattice & SPD SI & $\calO(n)$ & $\calO(n \log n)$ & $\calO(n \log n)$ & $\calO(n \log n)$ & FFT-based \\ 
        digital net & SPD DSI & $\calO(n \log n)$ & $\calO(n)$ & $\calO(n \log n)$ & $\calO(n \log n)$ &FWHT-based
    \end{tabular}
    \caption{Comparison of storage and cost requirements for kernel methods. Decomposition of the symmetric positive definite (SPD) Gram matrix $\mK$ is the cost of computing the eigendecomposition or Cholesky factorization. The costs of matrix-vector multiplication and solving a linear system are the costs after performing the decomposition. Both storage and kernel computation costs are greatly reduced by pairing certain low-discrepancy point sets with special shift-invariant (SI) or digitally-shift-invariant (DSI) kernels.}
    \label{tab:com_kernel_costs}
\end{table}

The following code can be used to construct a Gram matrix with a lattice point set and shift-invariant kernel. The Gram matrix is then used perform matrix-vector multiplication and solve a linear system. Analogous code using \texttt{qp.DigitalNetB2}, \texttt{qp.KernelDigShiftInvar}, and \texttt{qp.FastGramMatrixDigitalNetB2} is also supported. 

\lstinputlisting[style=Python]{snippets/kernel_methods.txt}

Schematics of the bit-reversed FFT and FWHT underlying these fast computations are shown in \cref{fig:fast_transforms}. The one-dimensional bit-reversed FFT, one-dimensional bit-reversed IFFT, and one-dimensional FWHT are implemented into the \texttt{QMCPy} functions \texttt{qp.fftbr}, \texttt{qp.ifftbr}, and \texttt{qp.fwht} respectively. These fast transforms act along the last dimension which is required to to have length $2^m$ for some $m \geq 0$.  The following subsections detail the matching point set-kernel pairings and provide methods to quickly update the Gram matrix product or linear system solution after doubling the point size.

\begin{figure}%[H]
    \centering
    \newcommand{\h}{1}
    \newcommand{\w}{2}
    \newcommand{\y}{4}
    \newcommand{\z}{6}
    \newcommand{\p}{5}
    \begin{subfigure}[t]{.49\textwidth}
    \begin{tikzpicture}
        % top left axis
        \draw (\z/2,\p+8*\h) node{bit-reversed FFT};
        \draw ( 0,\p+7*\h) node[draw,circle](l00){$y_0$}; %y_0
        \draw ( 0,\p+6*\h) node[draw,circle](l10){$y_1$}; %y_4
        \draw ( 0,\p+5*\h) node[draw,circle](l20){$y_2$}; % y_2
        \draw ( 0,\p+4*\h) node[draw,circle](l30){$y_3$}; % y_6
        \draw ( 0,\p+3*\h) node[draw,circle](l40){$y_4$}; % y_1
        \draw ( 0,\p+2*\h) node[draw,circle](l50){$y_5$}; % y_5
        \draw ( 0,\p+1*\h) node[draw,circle](l60){$y_6$}; % y_6
        \draw ( 0,\p+0*\h) node[draw,circle](l70){$y_7$}; % y_7
        \draw (\w,\p+7*\h) node[draw,circle](l01){};
        \draw (\w,\p+6*\h) node[draw,circle](l11){};
        \draw (\w,\p+5*\h) node[draw,circle](l21){};
        \draw (\w,\p+4*\h) node[draw,circle](l31){};
        \draw (\w,\p+3*\h) node[draw,circle](l41){};
        \draw (\w,\p+2*\h) node[draw,circle](l51){};
        \draw (\w,\p+1*\h) node[draw,circle](l61){};
        \draw (\w,\p+0*\h) node[draw,circle](l71){};
        \draw (\y,\p+7*\h) node[draw,circle](l02){};
        \draw (\y,\p+6*\h) node[draw,circle](l12){};
        \draw (\y,\p+5*\h) node[draw,circle](l22){};
        \draw (\y,\p+4*\h) node[draw,circle](l32){};
        \draw (\y,\p+3*\h) node[draw,circle](l42){};
        \draw (\y,\p+2*\h) node[draw,circle](l52){};
        \draw (\y,\p+1*\h) node[draw,circle](l62){};
        \draw (\y,\p+0*\h) node[draw,circle](l72){};
        \draw (\z,\p+7*\h) node[draw,circle](l03){$\ty_0$};
        \draw (\z,\p+6*\h) node[draw,circle](l13){$\ty_1$};
        \draw (\z,\p+5*\h) node[draw,circle](l23){$\ty_2$};
        \draw (\z,\p+4*\h) node[draw,circle](l33){$\ty_3$};
        \draw (\z,\p+3*\h) node[draw,circle](l43){$\ty_4$};
        \draw (\z,\p+2*\h) node[draw,circle](l53){$\ty_5$};
        \draw (\z,\p+1*\h) node[draw,circle](l63){$\ty_6$};
        \draw (\z,\p+0*\h) node[draw,circle](l73){$\ty_7$};
        % first layer 
        \draw[->] (l00) -- (l11); 
        \draw[->] (l10) -- (l01); 
        \draw[->] (l20) -- (l31); 
        \draw[->] (l30) -- (l21); 
        \draw[->] (l40) -- (l51); 
        \draw[->] (l50) -- (l41); 
        \draw[->] (l60) -- (l71); 
        \draw[->] (l70) -- (l61);
        \draw (\w/2,\p+0.5*\h) node[draw,circle,fill=black,text=white]{$0$};
        \draw (\w/2,\p+2.5*\h) node[draw,circle,fill=black,text=white]{$0$};
        \draw (\w/2,\p+4.5*\h) node[draw,circle,fill=black,text=white]{$0$};
        \draw (\w/2,\p+6.5*\h) node[draw,circle,fill=black,text=white]{$0$};
        % second layer 
        \draw[->] (l01) -- (l22); 
        \draw[->] (l11) -- (l32); 
        \draw[->] (l21) -- (l02); 
        \draw[->] (l31) -- (l12); 
        \draw[->] (l41) -- (l62); 
        \draw[->] (l51) -- (l72); 
        \draw[->] (l61) -- (l42); 
        \draw[->] (l71) -- (l52);
        \draw (\w/2+\y/2,\p+1*\h) node[draw,circle,fill=black,text=white]{$2$};
        \draw (\w/2+\y/2,\p+2*\h) node[draw,circle,fill=black,text=white]{$0$};
        \draw (\w/2+\y/2,\p+5*\h) node[draw,circle,fill=black,text=white]{$2$};
        \draw (\w/2+\y/2,\p+6*\h) node[draw,circle,fill=black,text=white]{$0$};
        % third layer 
        \draw[->] (l02) -- (l43); 
        \draw[->] (l12) -- (l53); 
        \draw[->] (l22) -- (l63); 
        \draw[->] (l32) -- (l73); 
        \draw[->] (l42) -- (l03); 
        \draw[->] (l52) -- (l13); 
        \draw[->] (l62) -- (l23); 
        \draw[->] (l72) -- (l33);
        \draw (\y/2+\z/2,\p+2*\h) node[draw,circle,fill=black,text=white]{$3$};
        \draw (\y/2+\z/2,\p+3*\h) node[draw,circle,fill=black,text=white]{$2$};
        \draw (\y/2+\z/2,\p+4*\h) node[draw,circle,fill=black,text=white]{$1$};
        \draw (\y/2+\z/2,\p+5*\h) node[draw,circle,fill=black,text=white]{$0$};
        % combination diagram 
        \draw (0,3*\h) node[draw,circle](wit){$a$};
        \draw (0,0) node[draw,circle](wib){$b$};
        \draw (\y/3+2*\z/3,3*\h) node[draw,circle,minimum size=2cm](wot){$\frac{a+W_n^rb}{\sqrt{2}}$};
        \draw (\y/3+2*\z/3,0) node[draw,circle,minimum size=2cm](wob){$\frac{a-W_n^rb}{\sqrt{2}}$};
        \draw[->] (wit) -- (wob); 
        \draw[->] (wib) -- (wot); 
        \draw (\z/2,1.5*\h) node[draw,circle,fill=black,text=white]{$r$};
        \draw (\z/2,3*\h) node{$W_n = e^{-2 \pi \sqrt{-1}/n}$};
    \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}[t]{.49\textwidth}
    \begin{tikzpicture}
        % top left axis
        \draw (\z/2,\p+8*\h) node{FWHT};
        \draw ( 0,\p+7*\h) node[draw,circle](l00){$y_0$};
        \draw ( 0,\p+6*\h) node[draw,circle](l10){$y_1$};
        \draw ( 0,\p+5*\h) node[draw,circle](l20){$y_2$};
        \draw ( 0,\p+4*\h) node[draw,circle](l30){$y_3$};
        \draw ( 0,\p+3*\h) node[draw,circle](l40){$y_4$};
        \draw ( 0,\p+2*\h) node[draw,circle](l50){$y_5$};
        \draw ( 0,\p+1*\h) node[draw,circle](l60){$y_6$};
        \draw ( 0,\p+0*\h) node[draw,circle](l70){$y_7$};
        \draw (\w,\p+7*\h) node[draw,circle](l01){};
        \draw (\w,\p+6*\h) node[draw,circle](l11){};
        \draw (\w,\p+5*\h) node[draw,circle](l21){};
        \draw (\w,\p+4*\h) node[draw,circle](l31){};
        \draw (\w,\p+3*\h) node[draw,circle](l41){};
        \draw (\w,\p+2*\h) node[draw,circle](l51){};
        \draw (\w,\p+1*\h) node[draw,circle](l61){};
        \draw (\w,\p+0*\h) node[draw,circle](l71){};
        \draw (\y,\p+7*\h) node[draw,circle](l02){};
        \draw (\y,\p+6*\h) node[draw,circle](l12){};
        \draw (\y,\p+5*\h) node[draw,circle](l22){};
        \draw (\y,\p+4*\h) node[draw,circle](l32){};
        \draw (\y,\p+3*\h) node[draw,circle](l42){};
        \draw (\y,\p+2*\h) node[draw,circle](l52){};
        \draw (\y,\p+1*\h) node[draw,circle](l62){};
        \draw (\y,\p+0*\h) node[draw,circle](l72){};
        \draw (\z,\p+7*\h) node[draw,circle](l03){$\ty_0$};
        \draw (\z,\p+6*\h) node[draw,circle](l13){$\ty_1$};
        \draw (\z,\p+5*\h) node[draw,circle](l23){$\ty_2$};
        \draw (\z,\p+4*\h) node[draw,circle](l33){$\ty_3$};
        \draw (\z,\p+3*\h) node[draw,circle](l43){$\ty_4$};
        \draw (\z,\p+2*\h) node[draw,circle](l53){$\ty_5$};
        \draw (\z,\p+1*\h) node[draw,circle](l63){$\ty_6$};
        \draw (\z,\p+0*\h) node[draw,circle](l73){$\ty_7$};
        % first layer 
        \draw[->] (l00) -- (l41); 
        \draw[->] (l10) -- (l51); 
        \draw[->] (l20) -- (l61); 
        \draw[->] (l30) -- (l71); 
        \draw[->] (l40) -- (l01); 
        \draw[->] (l50) -- (l11); 
        \draw[->] (l60) -- (l21); 
        \draw[->] (l70) -- (l31);
        \draw (\w/2,\p+2*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2,\p+3*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2,\p+4*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2,\p+5*\h) node[draw,circle,fill=black,text=white]{};
        % second layer 
        \draw[->] (l01) -- (l22); 
        \draw[->] (l11) -- (l32); 
        \draw[->] (l21) -- (l02); 
        \draw[->] (l31) -- (l12); 
        \draw[->] (l41) -- (l62); 
        \draw[->] (l51) -- (l72); 
        \draw[->] (l61) -- (l42); 
        \draw[->] (l71) -- (l52);
        \draw (\w/2+\y/2,\p+1*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2+\y/2,\p+2*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2+\y/2,\p+5*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\w/2+\y/2,\p+6*\h) node[draw,circle,fill=black,text=white]{};
        % third layer 
        \draw[->] (l02) -- (l13); 
        \draw[->] (l12) -- (l03); 
        \draw[->] (l22) -- (l33); 
        \draw[->] (l32) -- (l23); 
        \draw[->] (l42) -- (l53); 
        \draw[->] (l52) -- (l43); 
        \draw[->] (l62) -- (l73); 
        \draw[->] (l72) -- (l63);
        \draw (\y/2+\z/2,\p+0.5*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\y/2+\z/2,\p+2.5*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\y/2+\z/2,\p+4.5*\h) node[draw,circle,fill=black,text=white]{};
        \draw (\y/2+\z/2,\p+6.5*\h) node[draw,circle,fill=black,text=white]{};
        % combination diagram 
        \draw (0,3*\h) node[draw,circle](wit){$a$};
        \draw (0,0) node[draw,circle](wib){$b$};
        \draw (\y/3+2*\z/3,3*\h) node[draw,circle,minimum size=2cm](wot){$\frac{a+b}{\sqrt{2}}$};
        \draw (\y/3+2*\z/3,0) node[draw,circle,minimum size=2cm](wob){$\frac{a-b}{\sqrt{2}}$};
        \draw[->] (wit) -- (wob); 
        \draw[->] (wib) -- (wot); 
        \draw (\z/2,1.5*\h) node[draw,circle,fill=black,text=white]{};
    \end{tikzpicture}
    \end{subfigure}
    \caption{Bit-reversed FFT and FWHT schemes. The bit-reversed FFT is performed via a decimation in time algorithm without the initial reordering of inputs. The bit-reversed IFFT may be performed by propagating $\{\ty_i\}_{i=0}^{2^m-1}$ to $\{y_i\}_{i=0}^{2^m-1}$ in the other direction through the bit-reversed FFT algorithm.}
    \label{fig:fast_transforms}
    \Description[]{}
\end{figure}  

\subsection{Shift invariant kernels, lattices,  and the bit-reversed FFT/IFFT} \label{subsec:SI_kernels}

A kernel is said to be \emph{shift-invariant} when $K(\bu,\bv) = \tK((\bu-\bv) \mod 1)$ for some $\tK$ i.e. the kernel is only a function of the component-wise difference between inputs modulo $1$. One set of shift-invariant kernels take the form 
\begin{equation}
    K(\bu,\bv) = \sum_{\fu \subseteq \calD} \gamma_\fu \prod_{j \in \fu} \eta_{\alpha_j}((u_j - v_j) \mod 1)
    \label{eq:K_SI_full}
\end{equation}
where 
$$\eta_\alpha(\delta) = \frac{(2\pi)^{2\alpha}}{(-1)^{\alpha+1}(2\alpha)!} B_{2\alpha}(\delta)$$
with $\balpha \in \bbN^d$, $\calD = \{1,\dots,d\}$, weights $\{\gamma_\fu\}_{\fu \subseteq \calD}$, and $B_\ell$ denoting the Bernoulli polynomial of degree $\ell$. The corresponding reproducing kernel Hilbert space (RKHS) $H$ is a weighted periodic unanchored Sobolev space of smoothness $\balpha \in \bbN^d$ with norm 
$$\lVert f \rVert_H^2 := \sum_{\fu \subseteq \calD} \frac{1}{(2 \pi)^{2 \lvert \fu \rvert}\gamma_\fu} \int_{[0,1]^{\lvert \fu \rvert}} \llvert \int_{[0,1]^{s - \lvert \fu \rvert}} \left(\prod_{j \in \fu} \frac{\partial^{\alpha_j}}{\partial y_j^{\alpha_j}}\right) f(\by) \D \by_{- \fu} \rrvert^2 \D \by_\fu, $$
where $f:[0,1]^d \to \bbR$, $\by_\fu = (\by_j)_{j \in \fu}$, $\by_{-\fu} := (y_j)_{j \in \calD \setminus \fu}$, and $\lvert \fu \rvert$ is the cardinality of $\fu$. The space $H$ is a special case of the weighted Korobov space which has real smoothness parameter $\alpha$ characterizing the rate of decay of Fourier coefficients, see e.g. \citep{kaarnioja.kernel_interpolants_lattice_rkhs,kaarnioja.kernel_interpolants_lattice_rkhs_serendipitous,cools2021fast,cools2020lattice,sloan2001tractability,kuo2004lattice}.

The kernel \eqref{eq:K_SI_full} is the sum over $2^d$ terms and thus becomes impractical to compute for large $d$. A simplified space assumes $\gamma_\fu$ are \emph{product weights} which take the form $\gamma_\fu = \prod_{j \in \fu} \gamma_j$ for some $\{\gamma_j\}_{j=1}^d$. The kernel with product weights takes the simplified form 
\begin{equation}
    K(\bu,\bv) = \prod_{j=1}^d \left(1+\gamma_j \eta_{\alpha_j}((u_j-v_j) \mod 1)\right).
    \label{eq:K_SI_prod}
\end{equation}
\Cref{fig:SI_DSI_kernels} shows one dimensional shift-invariant kernels with unit weight and varying smoothness. See \citep{kaarnioja.kernel_interpolants_lattice_rkhs_serendipitous} for a review of lattice-based kernel interpolation in such weighted spaces along with the development of serendipitous weights, which are a special form of high-performance product weights. 

Suppose $n=2^m$ for some $m \in \bbN_0$ and let $\{\bx_i\}_{i=0}^{b^m-1}$ be a shifted lattice with shift $\bDelta \in [0,1)^d$ generated in \emph{linear order}. For $0 \leq i,k < n$
$$K(\bx_i,\bx_k) = K\left(\frac{(i-k) \bg}{n},\bzero\right),$$ 
so the Gram matrix $\mK$ is circulant. Let $R_m(i)$ flip the first $m$ bits of $0 \leq i < 2^m$ in base $b=2$ so that if $i=\sum_{t=0}^{m-1} \mi_t 2^t$ then $R(i) = \sum_{t=0}^{m-1} \mi_{m-t-1} 2^t$. The first step in the FFT with decimation-in-time is to reorder the inputs $\{y_i\}_{i=0}^{2^m-1}$ into bit reversed order $\{y_{R(i)}\}_{i=0}^{2^m-1}$. The last step in computing the IFFT is to reorder the outputs $\{y_{R(i)}\}_{i=0}^{2^m-1}$ into bit reversed order $\{y_i\}_{i=0}^{2^m-1}$. Therefore, one should generate lattice points in natural order and skip the first step of the FFT and last step of the inverse FFT. 

The Fourier matrix is $\overline{\mF^{(m)}} = \{W_m^{ij}\}_{i,j=0}^{2^m-1}$ where $W_m = \exp(-2 \pi \sqrt{-1}/2^m)$. Let $\overline{\mT^{(m)}} = \{W_m^{iR_m(j)}\}_{i,j=0}^{2^m-1}$ so that  
$$\overline{\mF^{(m)} \{y_i\}_{j=0}^{2^m-1}} = \overline{\mT^{(m)} \{y_{R(j)}\}_{j=0}^{2^m-1}}.$$
For $\{\bx_i\}_{i=0}^{2^m-1}$ a lattice in natural order, we have 
$$\mK = \frac{1}{n} \mT^{(m)} \mLambda^{(m)} \overline{\mT^{(m)}}.$$

Notice that $\overline{\mT^{(m+1)}} = \left\{W_{m+1}^{i R_{m+1}(j)}\right\}_{i,j=0}^{2^{m+1}-1}$. For $0 \leq j < 2^m$ we have $R_{m+1}(j) = 2 R_m(j)$ so $W^{i R_{m+1}(j)}_{m+1}= W^{i R_m(j)}_m$. For $2^m \leq j < 2^{m+1}$ we have $R_{m+1}(j) = 2R_m(j-2^m)+1$ so $W^{i R_{m+1}(j)}_{m+1} = W_m^{i R(j-2^m)} W_{m+1}^i$. Moreover, for $0 \leq i < 2^m$ we have $W_{m+1}^{2^m+i} = -W_{m+1}^i$. Define $\tbw^{(m)} = \{W_{m+1}^i\}_{i=0}^{2^m-1}$. Then 
$$\overline{\mT^{(m+1)}} = \begin{pmatrix} \overline{\mT^{(m)}} & \diag(\tbw^{(m)}) \overline{\mT^{(m)}} \\ \overline{\mT^{(m)}} & - \diag(\tbw^{(m)}) \overline{\mT^{(m)}} \end{pmatrix}$$
and $\tbw^{(m+1)}$ is the $\alpha=2$ interlacing of $\tbw^{(m)}$ and $W_{m+1} \tbw^{(m)}$. 

\subsection{Digital nets, digitally-shift-invariant kernels, and the FWHT} \label{subsec:DSI_kernels}

For $x,y \in [0,1)$ with $x = \sum_{t=1}^{m} \mx_{t-1} b^{-t}$ and $y = \sum_{t=1}^m \my_{t-1} b^{-t}$ let 
$$x \oplus y = \sum_{t=1}^m ((\mx_{t-1} + \my_{t-1}) \mod b) b^{-t} \qquad\text{and}\qquad x \ominus y = \sum_{t=1}^m ((\mx_{t-1} - \my_{t-1}) \mod b) b^{-t}$$
denote digital addition and subtraction respectively. Note that in base $b=2$ these operations are both equivalent to the exclusive or (XOR) between bits.  For vectors $\bx,\by \in [0,1)^d$, digital operations $\bx \oplus \by$ and  $\bx \ominus \by$ act component-wise. 
A kernel is said to be \emph{digitally-shift-invariant} when 
$$K(\bu, \bv) = \tK(\bu \ominus \bv)$$
for some $\tK$. 

Suppose $n =b^m$ for some $m \in \bbN_0$ and let $\{\bx_i\}_{i=0}^{b^m-1}$ be a digitally shifted digital net in natural order (possibly of higher order and/or with LMS). Following \cite[Theorem 5.3.1,Theorem 5.3.2]{rathinavel.bayesian_QMC_sobol}, for $b=2$, the Gram matrix $\mK$ is \emph{nested block Toeplitz} which implies the eigendecomposition
$$\mK = \frac{1}{n} \mH^{(m)} \mLambda^{(m)} \mH^{(m)}$$ where $\mH^{(m)}$ is the $2^m \times 2^m$ Hadamard matrix defined by $\mH^{(0)} = (1)$ and the relationship 
$$\mH^{(m+1)} = \begin{pmatrix} \mH^{(m)} & \mH^{(m)} \\ \mH^{(m)} & - \mH^{(m)} \end{pmatrix}.$$
Multiplying by $\mH^{(m)}$ can be done at at $\calO(n \log n)$ cost using the FWHT. 
% notice that for $q \in \bbN_0, p \in \{0,\dots,b-1\}$ and any $0 \leq i,k < b^q$ we have  
% $$K(\bx_{i+pb^q},\bx_{k+pb^q}) = \tK((p\bc_{q+1} \oplus \bx_i) \ominus (p \bc_{q+1} \oplus \bx_k))= \tK(\bx_i \ominus \bz_x) = K(\bx_i,\bx_k).$$
% Note that the above does not generally hold for permuted or NUS digital nets. To see this, consider $b=5$ and $d=1$ with 
% $$\mC_1 = \begin{pmatrix} 4 & 1 \\ 0 & 4 \end{pmatrix}$$
% so that $z_0 = .00_5$ and $z_1 = .40_5$. Then for $p=1$ and $q=1$ we have $z_5 = .14_5$ and $z_6 = .04_5$. Taking permutations $\pi_1 = (0,2,1,3,4)$ and $\pi_2 = (0,1,2,3,4)$ we get $x_0 = .00_5$ and $x_1 = .40_4$ while $x_5 = .24_5$ and $x_6 = .04_5$. Thus $x_6 \ominus x_5 = .30_5$ but $x_1 \ominus x_0 = .40_5$. This implies it does not hold for NUS generally either. Permutations for $b=2$ nets is the same as digital shifts. 
Note that $\mK$ is not necessarily nested block Toeplitz under NUS. %To see this, take $d=1$ and 
%$$\mC_1 = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}.$$
%Then for $q=2$ and $p=1$ we have $z_2 = .110_2$ and $z_3 = .010_2$ while $z_6 = .001_2$ and $z_7 = .101_2$. A valid NUS may set $x_2 = .110_2$ and $x_3 = .010_2$ while $x_6 = .000_2$ and $x_7 = .101_2$. Then $x_3 \ominus x_2 = .100_2$ while $x_7 \ominus x_6 = 101_2$.

We now describe one dimensional digitally-shift-invariant kernels $K(u,v) = \tK(u \ominus v)$ of higher-order smoothness $\alpha$. Weighted sums over products these one dimensional kernels may be used to construct higher-dimensional kernels e.g. those with product weights 
$$K(\bu,\bv) = \prod_{j=1}^d \left(1+\gamma_j  \left(K(u_j,v_j)-1\right)\right).$$
To begin, let us write $k \in \bbN_0$ as $k = \sum_{\ell=1}^{\#k} \mk_{a_\ell} b^{a_\ell}$ where $a_1 > \cdots > a_{\#k} \geq 0$ are the $\#k$ indices of non-zero digits $\mk_{a_\ell} \in \{1,\dots,b-1\}$ in the base $b$ expansion of $k$. Then the $k^\mathrm{th}$ Walsh coefficient is 
$$\wal_k(x) = e^{2 \pi \sqrt{-1}/b \sum_{\ell=1}^{\#k} \mk_{a_\ell} \mx_{a_\ell+1}}.$$ 
For $\alpha \in \bbN$ we also define the weight function 
$$\mu_\alpha(k) = \sum_{\ell=1}^{\min\{\alpha,\#k\}} (a_\ell+1).$$
For $\alpha>1$, let the digitally-shift-invariant kernel 
\begin{equation}
    K(x,z) = \sum_{k \in \bbN_0} \frac{\wal_k(x \ominus z)}{b^{\mu_\alpha(k)}} = \tK_\alpha(x \ominus z)
    \label{eq:DSI_kernel}
\end{equation}
have corresponding RKHS $\tH_\alpha$. The following theorem shows $\tH_\alpha$ contains smooth functions. The case when $\alpha=1$ is treated separately in \citep{dick.multivariate_integraion_sobolev_spaces_digital_nets}.   
\begin{theorem} \label{thm:RKHS_DSI_contain}
    $\tH_\alpha \supset H_\alpha$ where $H_\alpha$ is an RKHS with inner product 
    $$\langle f,g \rangle_{H_\alpha} =\sum_{l=1}^{\alpha-1} \int_0^1 f^{(l)}(x) \D x \int_0^1 g^{(l)}(x) \D x + \int_0^1 f^{(\alpha)}(x) g^{(\alpha)}(x) \D x.$$
\end{theorem}
\begin{proof} 
    See \cref{appendix:proofs}
\end{proof}

The following theorem gives explicit forms of a few higher-order digitally-shift-invariant kernels in base $b=2$ which are plotted in \cref{fig:SI_DSI_kernels}. These kernels are also useful in the computation of the worst-case error of QMC with higher order polynomial-lattice rules as in \citep{baldeaux.polylat_efficient_comp_worse_case_error_cbc}. In fact, their paper details how to compute $\tK_\alpha$ at $\calO(\alpha \#x)$ cost where $\#x$ is the number of non-zero digits in the base $b$ expansion of $x$
\begin{theorem} \label{thm:explicit_DSI_low_order_forms}
    Fix the base $b=2$. Let $\beta(x) = - \lfloor \log_2(x) \rfloor$ and $t_\nu(x) = 2^{-\nu \beta(x)}$ where for $x=0$ we set $\beta(x) = t_\nu(x) = 0$. Then 
    \begin{align*}
        \tK_2(x) &= -\beta(x) x + \frac{5}{2}\left[1-t_1(x)\right], \\
        \tK_3(x) &= \beta(x)x^2-5\left[1-t_1(x)\right]x+\frac{43}{18}\left[1-t_2(x)\right], \\
        \tK_4(x) &= - \frac{2}{3}\beta(x)x^3+5\left[1-t_1(x)\right]x^2 - \frac{43}{9}\left[1-t_2(x)\right]x \\
        &+\frac{701}{294}\left[1-t_3(x)\right]+\beta(x)\left[\frac{1}{48}\sum_{a=0}^\infty \frac{\wal_{2^a}(x)}{2^{3a}} - \frac{1}{42}\right].
    \end{align*}
\end{theorem}
\begin{proof}
    The $\alpha \in \{2,3\}$ forms are due to \citep{baldeaux.polylat_efficient_comp_worse_case_error_cbc}. The $\alpha=4$ form is derived in \cref{appendix:proofs}. 
\end{proof}

%For $\alpha \leq 4$ and $\beta,\kappa \in \{0,1\}$  the right derivatives satisfy 
%$$\tK^{(\alpha,\beta)}(x,z|\alpha) = -2+(-2)^{\beta+\kappa} \tK(x,z|\alpha-\beta-\kappa).$$

\subsection{Fast kernel computations and efficient updates} 

Let us make the general assumptions satisfied by the formulations in the previous two subsections. 
\begin{enumerate}
    \item For $n=2^m$, the Gram matrix $\mK^{(m)} \in \bbR^{n \times n}$ can be written as
    $$\mK^{m} = \mV^{(m)} \mLambda^{(m)} \overline{\mV^{(m)}}$$
    where $\mV^{(m)} \overline{\mV^{(m)}} = \mI$ and the first column of $\mV^{(m)}$, denoted $\bv_1^{(m)}$, is the constant $1/\sqrt{n}$
    \item $V^{(m)} \in \bbC^{n \times n}$ satisfies $\mV^{(0)} = (1)$ and 
    $$\mV^{(m+1)} = \begin{pmatrix} \mV^{(m)} & \mV^{(m)} \\  \mV^{(m)} \overline{\diag(\tbw^{(m)})} & - \mV^{(m)} \overline{\diag(\tbw^{(m)})} \end{pmatrix}/\sqrt{2}.$$
    \item $\mV^{(m)} \by$ and $\overline{\mV^{(m)}} \by$ are each computable at cost $\calO(2^mm)$.
\end{enumerate}
For the case of lattice points with shift-invariant kernels and resulting circulant Gram matrices, $\mV^{(m)} = \mT^{(m)}/\sqrt{2^m}$ is the scaled and permuted Fourier matrix and $\tbw^{(m)} = \{\exp(-\pi \sqrt{-1}/2^m i)\}_{i=0}^{2^m-1}$, so $\mV^{(m)} \by$ and $\overline{\mV^{(m)}} \by$ can be computed using a bit-reversed FFT and bit-reversed IFFT respectively. For the case of digital nets with 
digitally-shift-invariant kernels  and resulting nested block Toeplitz Gram matrices, $\mV^{(m)} = \mH^{(m)}/\sqrt{n}$ is the scaled Hadamard matrix and $\tbw^{(m)} = \{1\}_{i=0}^{2^m-1}$, so $\mV^{(m)} \by$ and $\overline{\mV^{(m)}} \by$ can be computed using a FWHT and inverse FWHT respectively. 

For $\mLambda = \diag(\blambda)$, we have 
$$\blambda = \sqrt{n} \mLambda^{(m)}\overline{\bv_1^{(m)}} = \sqrt{n} \overline{\mV^{(m)}} \left(\mV^{(m)} \mLambda^{(m)} \overline{\bv_1^{(m)}}\right) = \sqrt{n} \overline{\mV^{(m)}} \bk_1$$
which can be computed at $\calO(n \log n)$ cost. Moreover, 
\begin{equation*}
    \mK \by = \mV^{(m)}(\tby \odot \blambda) \qquad \text{and}\qquad \mK^{-1} \by = \mV^{(m)} (\tby ./ \blambda)
\end{equation*}
may each be evaluated at cost $\calO(n \log n)$ where $\odot$ denotes the Hadamard (component-wise) product and $./$ denotes component-wise division.

To update the solutions of the motivating problems, it is easily shown that  
\begin{align*}
    \mK^{(m+1)} \by^{(m+1)} 
    % &= \sqrt{2} \mV^{(m+1)} \diag \begin{pmatrix} \blambda^{(1)} \\ \blambda^{(2)}\end{pmatrix} \overline{\mV^{(m+1)}} \begin{pmatrix} \by^{(1)} \\ \by^{(2)} \end{pmatrix} \\
    % &= \mV^{(m+1)} \left( \begin{pmatrix} \tby^{(1)}+\tbw^{(m)} \odot \tby^{(2)} \\ \tby^{(1)}-\tbw^{(m)} \odot \tby^{(2)}\end{pmatrix} \odot \begin{pmatrix} \blambda^{(1)}+\tbw^{(m)} \odot \blambda^{(2)} \\ \blambda^{(1)}-\tbw^{(m)} \odot \blambda^{(2)}\end{pmatrix} \right) \\
    &= \sqrt{2} \begin{pmatrix} \mV^{(m)} \left(\tby^{(1)} \odot \blambda^{(1)} + \tbw^{(m)} \odot \tbw^{(m)} \odot \tby^{(2)} \odot \blambda^{(2)} \right) \\ \mV^{(m)} \left(\tby^{(2)} \odot \blambda^{(1)} + \tby^{(1)} \odot \blambda^{(2)} \right)\end{pmatrix} \\
    \mK^{-(m+1)} \by^{(m+1)} 
    &= \frac{1}{\sqrt{2}} \begin{pmatrix} \mV^{(m)} \left(\left(\tby^{(1)} \odot \blambda^{(1)} - \tbw^{(m)} \odot \tbw^{(m)} \odot \tby^{(2)} \odot \blambda^{(2)} \right) ./ \tbgamma\right) \\ \mV^{(m)}\left( \left( \tby^{(2)} \odot \blambda^{(1)} - \tby^{(1)} \odot \blambda^{(2)} \right) ./ \tbgamma \right)\end{pmatrix}
\end{align*}
where $\tbgamma = \blambda^{(1)} \odot \blambda^{(1)} - \tbw^{(m)} \odot \tbw^{(m)} \odot \blambda^{(2)} \odot \blambda^{(2)}$. 


\section{Numerical experiments} \label{sec:numerical_experiments}

All numerical experiments were carried out on a 2019 MacBook Pro with an Intel Core i7 processor. \Cref{fig:timing} compares the wall-clock time required to generate point sets and perform fast transforms. Since scaling in the number dimensions and number of randomizations is linear, point sets are only generated in $d=1$ with a single randomization. IID points, lattices with shifts, and digital nets with LMS and digital shifts (including higher-order versions) are the fastest sequences to generate. Digital nets in base $b=2$ exploit Graycode order, integer storage of bit-vectors, and exclusive or (XOR) operations to perform digital addition. Generators of Halton point sets are slower to generate as they cannot exploit these advantages. NUS, especially higher order versions, are significantly slower to generate then LMS randomizations. Even so, higher-order LMS scrambling with digital shifts are sufficient to achieve higher-order RMSE convergence as we shown in the next experiment.  

\begin{figure}%[H]
    \centering
    \includegraphics[width=1\textwidth]{./figs/timing.eps}
    \caption{Comparison of time required to generate IID and low-discrepancy point sets and perform fast transforms. All point sets are generated in a single dimension and with a single randomizations. BR indicates the bit-reversed FFT and IFFT.}
    \Description[]{}
    \label{fig:timing}
\end{figure}

\Cref{fig:convergence} shows higher-order LMS with digital shifts for base $b=2$ digital nets achieve higher-order RMSE convergence for RQMC methods. Specifically, we are able to achieve an RMSE of $\calO(n^{-\min\{\alpha,\talpha\}-1/2-\delta})$ where $\alpha$ is the higher-order digital interlacing of the net, $\talpha$ is the smoothness of the integrand, and $\delta>0$ is arbitrarily small.  The first two integrands reproduce the experiments of \citep{dick.higher_order_scrambled_digital_nets}. The next two problems are test functions from Derek Bingham's Virtual Library of Simulation Experiments (VLSE)\footnote{\label{fn:VLSE}\url{https://www.sfu.ca/~ssurjano/uq.html}}. The last two Genz functions are used for forward uncertainty quantification in the Dakota software package \citep{adams2020dakota} among other places. For each problem, the RMSE of the (Q)MC estimator  \eqref{eq:mc_approx} with $\bX \sim \calU[0,1]^d$ is approximated using $300$ independent randomizations of a LD (or IID) point sets. For lattices, we periodize the integrand using a Baker-transform i.e. we use $\tilde{f}(\bx) = f(1-2\lvert \bx-1/2\rvert)$ in place of $f(\bx)$ in \eqref{eq:mc_approx}. The Baker-transform does not change the expectation. A description of the integrands follows 
\begin{description}
    \item[Simple function, $d=1$] has $f(X) = X e^X-1$. This was used in \citep{dick.higher_order_scrambled_digital_nets} where higher-order digital net scrambling was first proposed.
    \item[Simple function, $d=2$] has $f(\bX) = X_2 e^{X_1 X_2}/(e-2)-1$. This was also used in \citep{dick.higher_order_scrambled_digital_nets}.
    \item[Oakley \& O'Hagan, $d=2$] has $f(\bX) = g((\bX-1/2)/50)$ for $g(\bT) = 5+T_1+T_2+2\cos(T_1)+2\cos(T_2)$, see \citep{oakley2002bayesian} or the VLSE\footref{fn:VLSE}.
    \item[G-Function, $d=3$] has $f(\bX) = \prod_{j=1}^d \frac{\lvert 4X_j-2\rvert-a_j}{1+a_j}$ with $a_j = (j-2)/2$ for $1 \leq j \leq d$, see  \citep{crestaux2007polynomial,marrel2009calculations} or the VLSE\footref{fn:VLSE}. 
    \item[Oscillatory Genz, $d=3$] has $f(\bX) = \cos\left(-\sum_{j=1}^d c_j X_j \right)$ with coefficients of the third kind $c_j = \exp\left(k \log\left(10^{-8}\right)/d\right)$. This is a common test function for uncertainty quantification which is is available in the Dakota software \citep{adams2020dakota} among other.  
    \item[Corner-peak Genz, $d=3$] has $f(\bx) = \left(1+\sum_{j=1}^d c_j X_j\right)^{-(d+1)}$ with coefficients of the second kind $c_j = 1/j^2$. This is also used in the Dakota software \citep{adams2020dakota} among others. 
\end{description}

\begin{figure}%[H]
    \centering
    \includegraphics[width=1\textwidth]{./figs/convergence.eps}
    \caption{The RMSE of the RQMC estimate for a few different integrands. Higher-order digital nets with linear matrix scrambling and digital shifts achieve higher-order convergence.}
    \Description[]{}
    \label{fig:convergence}
\end{figure}

\section{Conclusion and future work} \label{sec:conclusions_future_work}

This work has reviewed routines for generating low-discrepancy point sets, randomizing them, and applying them to fast kernel methods. We have reviewed lattices, digital nets, and Halton point sets with randomizations spanning shifts, digital permutations, digital shifts, linear matrix scrambling, and nested uniform scrambling. Higher-order scramblings for digital nets were also considered. For kernel methods, we presented classes of kernels which pair with low-discrepancy sequences to accelerate kernel computations. This includes new higher-order digitally shift invariant kernels. The accelerated methods utilize modified fast Fourier transforms and the fast Walsh-Hadamard transform. The presented material has been implemented into the accessible open-source Python library \texttt{QMCPy}.  

\begin{acks}
    Thank you to Fred J Hickernell, Sou-Cheng T Choi, and Aadit Jain for helpful comments and prototyping. The author would like to acknowledge the support of NSF Grant DMS-2316011.
\end{acks}

%\bibliography{../../meta/ags,main}

\bibliographystyle{ACM-Reference-Format}
\bibliography{./ags,./main}

\appendix 

\section{Proofs of theorems} \label{appendix:proofs}

For $k \in \bbN_0$ write
$$\hf(k) = \int_0^1 f(x) \overline{\wal_k(x)} \D x.$$

\begin{proof}[Proof of \cref{thm:RKHS_DSI_contain}] 
    Suppose $f \in H_\alpha$. Using the fact that \eqref{eq:DSI_kernel} is a Mercer kernel and \citep[Theorem 14, Remark 19]{dick.decay_walsh_coefficients_smooth_functions}, we have 
    $$\llVert f \rrVert_{\tH_\alpha}^2 = \sum_{k \in \bbN_0} \llvert \hf(k) \rrvert^2 b^{\mu_\alpha(k)} \leq C_\alpha \sum_{k \in \bbN} b^{-\mu_\alpha(k)} < \infty$$ 
    for some $C_\alpha < \infty$, so $f \in \tH_K$.
\end{proof}

\begin{lemma}[Walsh coefficients of low order monomials]\label{lemma:walsh_low_order_monomials}
    Fix $b=2$. Let $f_p(x) := x^p$. When $k \in \bbN$ write 
    $$k = 2^{a_1}+\dots+2^{a_{\#k}}$$
    where $a_1 > a_2 > \dots > a_{\#k} \geq 0$. Then we have 
    \begin{align*}
        \hf_1(k) &= \begin{cases} 1/2, & k = 0 \\ -2^{-a_1-2}, & k=2^{a_1} \\ 0, & \mathrm{otherwise} \end{cases}, \\
        \hf_2(k) &= \begin{cases} 1/3, & k = 0 \\ -2^{-a_1-2}, & k=2^{a_1} \\ 2^{-a_1-a_2-3}, & k = 2^{a_1}+2^{a_2} \\ 0, & \mathrm{otherwise} \end{cases}, \\
        \hf_3(k) &= \begin{cases} 1/4, & k=0 \\ -2^{-a_1-2} + 2^{-3a_1-5}, & k = 2^{a_1} \\ 3 \cdot 2^{-a_1-a_2-4}, & k=2^{a_1}+2^{a_2} \\ -3 \cdot 2^{-a_1-a_2-a_3-5}, & k=2^{a_1}+2^{a_2}+2^{a_3} \\ 0, & \mathrm{otherwise} \end{cases}.
    \end{align*}
\end{lemma}
\begin{proof}
    The forms for $\hf_1$ and $\hf_2$ follow from \citep[Example 14.2, Example 14.3]{dick.digital_nets_sequences_book}. 
    For $k=0$ and any $x \in [0,1)$ we have $\wal_0(x) = 1$, so  
    $$\hf_3(x) = \int_0^1 x^3 \D x = 1/4.$$ 
    Assume $k \in \bbN$ going forward. For $k=2^{a_1}+k'$ with $0 \leq k' < 2^{a_1}$, \citep[Equation 3.6]{fine.walsh_functions} implies
    $$J_k(x) := \int_0^x \wal_k(t) \D t = 2^{-a_1-2} \left[\wal_{k'}(x) - \sum_{r=1}^\infty 2^{-r} \wal_{2^{a_1+r}+k}(x)\right].$$
    Using integration by parts and the fact that $J_k(0) = J_k(1) = 0$
    \begin{align*}
        \hf_3(k) &= \int_0^1 x^3 \wal_k(x) \D x 
        = \left[x^3 J_k(x) \right]_{x=0}^{x=1} - 3 \int_0^1 x^2 J_k(x) \D x \\
        &= -3*2^{-a_1-2} \left[\hf_2(k') - \sum_{r=1}^\infty 2^{-r} \hf_2(2^{a_1+r}+k)\right].
    \end{align*}
    \begin{itemize}
        \item If $\#k=1$, i.e. $k=2^{a_1}$ then 
        \begin{align*}
            \hf_3(k) &= -3*2^{-a_1-2} \left[\hf_2(0) - \sum_{r=1}^\infty 2^{-r} \hf_2(2^{a_1+r}+2^{a_1})\right] \\
            &= -3*2^{-a_1-2} \left[\frac{1}{3} - \sum_{r=1}^\infty 2^{-r} 2^{-(a_1+r)-a_1-3}\right] \\
            &= 2^{-3a_1-5} - 2^{-a_1-2}.
        \end{align*}
        \item If $\#k=2$ then 
        $$\hf_3(k) = -3*2^{-a_1-2} \hf_2(2^{a_2}) = 3 \cdot 2^{-a_1-a_2-4}.$$
        \item If $\#k=3$ then 
        $$\hf_3(k) = -3*2^{-a_1-2} \hf_2(2^{a_2}+2^{a_3}) = -3 \cdot 2^{-a_1-a_2-a_3-5}.$$
        \item If $\#k>3$ then $\hf_3(k)=0$.
    \end{itemize}
\end{proof}

\begin{proof}[Proof of \cref{thm:explicit_DSI_low_order_forms}]
    Write  
    $$\tK_\alpha(x) = 1 + \sum_{1 \leq \nu < \alpha} s_\nu(x) + \ts_\alpha(x)$$ 
    where $s_\nu$ sums over all $k$ with $\#k = \nu$ and $\ts_\alpha$ sums over all $k$ with $\#k \geq \alpha$. In \citep[Corollary 1]{baldeaux.polylat_efficient_comp_worse_case_error_cbc} it was shown that 
    \begin{align*}
        s_1(x) &= -2x+1, \\
        s_2(x) &= 2x^2 - 2x + \frac{1}{3}, \\
        \ts_2(x) &=  \left[2-\beta(x)\right]x + \frac{1}{2}\left[1-5t_1(x)\right], \\
        \ts_3(x) &= -\left[2-\beta(x)\right] x^2 - \left[1-5t_1(x)\right]x + \frac{1}{18}\left[1-43t_2(x)\right]
    \end{align*}
    from which $\tK_2$ and $\tK_3$ follow. We now find expressions for $s_3$ and $\ts_4$ from which $\tK_4$ follows. 

    Assume sums over $a_i$ are over $\bbN_0$ unless otherwise restricted. Recall from \Cref{lemma:walsh_low_order_monomials} that
    \begin{align*}
        x &= \frac{1}{2} - \sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+2}}, \\
        x^2 &= \frac{1}{3} - \sum_{a_1}\frac{\wal_{2^{a_1}}(x)}{2^{a_1+2}} + \sum_{a_1>a_2} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+3}}, \\
        x^3 &= \frac{1}{4} - \sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+2}} + \frac{3}{2}\sum_{a_1>a_2} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+3}} - \frac{3}{2} \sum_{a_1>a_2>a_3} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+4}} + \sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{3a_1+5}}
    \end{align*}
    so
    $$x^3-\frac{3}{2}x^2+\frac{1}{2}x = \frac{1}{32} \sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{3a_1}} - \frac{3}{4} \sum_{a_1>a_2>a_3} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+3}}$$
    and 
    $$s^{[3}(x) = \sum_{a_1>a_2>a_3} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+3}} = -\frac{4}{3} x^3+ 2x^2 -\frac{2}{3}x + \frac{1}{24} \sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{3a_1}}.$$
    Now,
    \begin{align*}
        \ts_4(x) &= \sum_{\substack{a_1>a_2>a_3>a_4 \\ 0 \leq k < 2^{a_4}}} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}+2^{a_4}+k}(x)}{2^{a_1+a_2+a_3+a_4+4}} \\
        &= \sum_{a_1>a_2>a_3>a_4} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}+2^{a_4}}(x)}{2^{a_1+a_2+a_3+a_4+4}} \sum_{0 \leq k < 2^{a_4}} \wal_k(x) \\
    \end{align*}
    If $x=0$ then 
    $$\ts_4(0) = \sum_{a_1 > a_2 > a_3 > a_4} \frac{1}{2^{a_1+a_2+a_3+4}} = \frac{1}{294}.$$ 
    Going forward, assume $x \in (0,1)$ so $\beta(x) = - \lfloor \log_2(x) \rfloor$ is finite. Recall that 
    $$\sum_{0 \leq k < 2^{a_4}} \wal_k(x) = \begin{cases} 2^{a_4}, & a_4 \leq \beta(x)-1 \\ 0, & a_4 > \beta(x)-1 \end{cases}.$$
    Moreover, since $\beta(x)$ is the index of the first $1$ in the base $2$ expansion of $x$, when $a_4 < \beta(x)-1$ we have $\wal_{2^{a_4}}(x) = (-1)^{\mx_{a_4+1}} = 1$ and when $a_4 = \beta(x)-1$ we have $\wal_{2^{a_4}}(x) = -1$. This implies 
    \begin{align*}
        \ts_4(x) &= \sum_{\substack{a_1>a_2>a_3>a_4 \\ \beta(x)-1 \geq a_4}} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}+2^{a_4}}(x)}{2^{a_1+a_2+a_3+4}} \\
        &= \sum_{\substack{a_1>a_2>a_3>a_4 \\ \beta(x)-1 > a_4}} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+4}} - \sum_{a_1>a_2 > a_3 > \beta(x)-1} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+4}} \\
        &=: T_1 - T_2.
    \end{align*}
    
    The first term is 
    \begin{align*}
        T_1 &= \sum_{\beta(x)-1 > a_4} \bigg(\sum_{a_1>a_2>a_3} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+4}} -  \sum_{a_4  \geq a_1>a_2>a_3} \frac{1}{2^{a_1+a_2+a_3+4}} \\ & \qquad\qquad  - \sum_{a_1 > a_4 \geq a_2 > a_3} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+a_2+a_3+4}} - \sum_{a_1 > a_2 > a_4 \geq a_3} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+a_3+4}} 
        \bigg) \\
        &=: \sum_{\beta(x)-1 > a_4} \left[V_1(a_4)-V_2(a_4) - V_3(a_4) - V_4(a_4) \right].
    \end{align*}
    Clearly $V_1(a_4) = s_3(x)/2$ and $V_2$ is easily computed. Now
    \begin{align*}
        V_3(a_4) &= \left(\sum_{a_4 \geq a_2 > a_3} \frac{1}{2^{a_2+a_3+3}}\right)\left(\sum_{a_1>a_4} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+1}}\right) \\
        &= \left(\sum_{a_4 \geq a_2 > a_3} \frac{1}{2^{a_2+a_3+3}}\right)\left(s_1(x) - \sum_{a_4 \geq a_1} \frac{1}{2^{a_1+1}} \right)
    \end{align*}
    and 
    \begin{align*}
        V_4(a_4) & = \left(\sum_{a_4 \geq a_3} \frac{1}{2^{a_3+2}}\right)\left(\sum_{a_1>a_2 > a_4} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+2}}\right) \\
        &= \left(\sum_{a_4 \geq a_3} \frac{1}{2^{a_3+2}}\right)\left(\sum_{a_1>a_2} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+2}} - \sum_{a_4 \geq a_1 > a_2} \frac{1}{2^{a_1+a_2+2}} - \sum_{a_1 > a_4 \geq a_2} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+a_2+2}}\right) \\
        &= \left(\sum_{a_4 \geq a_3} \frac{1}{2^{a_3+2}}\right)\left(s_2(x) - \sum_{a_4 \geq a_1 > a_2} \frac{1}{2^{a_1+a_2+2}} - \left(s_1(x) - \sum_{a_4 \geq a_1} \frac{1}{2^{a_1+1}} \right)\left(\sum_{a_4 \geq a_2} \frac{1}{2^{a_2+1}}\right)\right).
    \end{align*}
    
    The second term is 
    \begin{align*}
        T_2 &= \sum_{a_1>a_2>a_3} \frac{\wal_{2^{a_1}+2^{a_2}+2^{a_3}}(x)}{2^{a_1+a_2+a_3+4}} - \sum_{\beta(x)-1 > a_1 > a_2 > a_3} \frac{1}{2^{a_1+a_2+a_3+4}} + \sum_{\beta(x)-1 > a_2 > a_3} \frac{1}{2^{\beta(x)+a_2+a_3+3}} \\ & - \sum_{a_1 > \beta(x)-1 > a_2 > a_3} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+a_2+a_3+4}} + \sum_{a_1 > \beta(x)-1 > a_3}  \frac{\wal_{2^{a_1}}(x)}{2^{a_1+\beta(x)+a_3+3}} \\ &- \sum_{a_1 > a_2 > \beta(x)-1 > a_3}\frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+a_3+4}} + \sum_{a_1 > a_2 > \beta(x)-1} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+\beta(x)+3}} \\
        &=: W_1-W_2+W_3-W_4+W_5-W_6+W_7.
    \end{align*}
    Clearly $W_1 = s_3(x)/2$ and both $W_2$ and $W_3$ are easily computed. Similarity in the next two sums gives
    \begin{align*}
        W_5 - W_4 &= \left(\sum_{\beta(x)-1 > a_3}\frac{1}{2^{\beta(x)+a_3+2}}-\sum_{\beta(x)-1>a_2>a_3} \frac{1}{2^{a_2+a_3+3}}\right) \left(\sum_{a_1 > \beta(x)-1} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+1}} \right) \\
        &= \left(\sum_{\beta(x)-1 > a_3}\frac{1}{2^{\beta(x)+a_3+2}}-\sum_{\beta(x)-1>a_2>a_3} \frac{1}{2^{a_2+a_3+3}}\right)\left(s_1(x) - \sum_{\beta(x)-1 > a_1} \frac{1}{2^{a_1+1}} + \frac{1}{2^{\beta(x)}}\right)
    \end{align*}
    Similarity in the final two sums gives  
    \begin{align*}
        W_7 - W_6 &= \left(\frac{1}{2^{\beta(x)+1}}-\sum_{\beta(x)-1 > a_3} \frac{1}{2^{a_3+2}}\right) \left(\sum_{a_1>a_2 > \beta(x)-1} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+2}}\right).
    \end{align*}
    where 
    \begin{align*}
        \sum_{a_1>a_2 > \beta(x)-1} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+2}} &= \sum_{a_1>a_2} \frac{\wal_{2^{a_1}+2^{a_2}}(x)}{2^{a_1+a_2+2}} - \sum_{\beta(x)-1 > a_1 > a_2} \frac{1}{2^{a_1+a_2+2}} + \sum_{\beta(x)-1 > a_2} \frac{1}{2^{\beta(x)+a_2+1}} \\ &- \sum_{a_1 > \beta(x)-1 > a_2} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+a_2+2}} + \sum_{a_1 > \beta(x)-1} \frac{\wal_{2^{a_1}}(x)}{2^{a_1+\beta(x)+1}} \\
        &= s_2(x) - \sum_{\beta(x)-1 > a_1 > a_2} \frac{1}{2^{a_1+a_2+2}} + \sum_{\beta(x)-1 > a_2} \frac{1}{2^{\beta(x)+a_2+1}}\\
        &+ \left(\frac{1}{2^{\beta(x)}} - \sum_{\beta(x)-1 > a_2} \frac{1}{2^{a_2+1}}\right)\left(s_1(x) - \sum_{\beta(x)-1 > a_1} \frac{1}{2^{a_1+1}} + \frac{1}{2^{\beta(x)}}\right).
    \end{align*}
    This implies 
    \begin{align*}
        \ts_4(x) &=  \frac{2}{3}\left(2-\beta(x)\right)x^3 + \left(1-5\ t_1(x)\right)x^2 - \frac{1}{9} \left(1 - 43 t_2(x)\right)x \\
        &-\frac{1}{48} \left(2-\beta(x)\right)\sum_{a_1} \frac{\wal_{2^{a_1}}(x)}{2^{3a_1}} -\frac{1}{294} \left(7\beta(x)+701 t_3(x)\right) +\frac{5}{98}
    \end{align*}
    from which the result follows.
\end{proof}

% \section{FFT} 

% The discrete Fourier transform of $\{y_i\}_{i=0}^{n-1}$ is $\{\ty_i\}_{i=0}^{n-1}$ where for $0 \leq k < n$ 
% $$\ty_k = \sum_{i=0}^{n-1} y_i W_n^{ik}$$
% where $W_n = e^{-2 \pi \sqrt{-1}/n}$. When $n=2^m$ for some $m \in \bbN$, we may use the  Danielson-Lanczos lemma to break up the sum
% $$\ty_k = \sum_{i=0}^{n/2-1} y_{2i} W_{n/2}^{ik} + W_n^{k} \sum_{i=0}^{n/2-1} y_{2i+1} W_{n/2}^{ik}.$$
% If $m \geq 2$ then we may again apply the Danielson-Lanczos lemma to say 
% $$\ty_k = \sum_{i=0}^{n/4-1} y_{2(2i)} W_{n/4}^{ik} + W_{n/2}^k \sum_{i=0}^{n/4-1} y_{2(2i+1)} W_{n/4}^{ik} + W_n^k \left(\sum_{i=0}^{n/4-1} y_{2(2i)+1} W_{n/4}^{ik} + W_{n/2}^k \sum_{i=0}^{n/4-1} y_{2(2i+1)+1} W_{n/4}^{ik}\right).$$
% For $m=2$ this simplifies to 
% $$\ty_k = y_0 + W_n^{2k} y_2 + W_n^k (y_1 + W_n^{2k} y_3)$$
% Recursively applying the Danielson-Lanczos lemma for $m>2$ leads to the decimation in time fast Fourier transform (FFT), AKA the Cooley-Tukey algorithm, with computational cost $\calO(n \log n)$. This FFT schematic for $m=3$ is shown in \cref{fig:fast_transforms}. 

\end{document}