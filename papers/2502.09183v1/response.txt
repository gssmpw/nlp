\section{Related Work}
\textbf{LLMs for Code Generation} LLMs have shown exceptional code understanding abilities due to extensive pre-training on code-related corpora. Numerous models like **Brown, "GPT-4"**__**Guo et al., "Gemini"**__**Laferniere et al., "Claude-3.5"**__**Chen et al., "Qwen2.5-Coder"**__**Wang et al., "DeepSeek-Coder"** exhibit strong performance on code generation benchmarks. Recently, the release of **Brown, "OpenAI o1"**__**Li et al., "DeepSeek-R1"** has spurred a surge in research on deep reasoning LLMs, achieving expert-level performance on competitive programming problems (e.g., CodeForces) and further advancing LLMs in the code domain.




\begin{figure*}[t]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/model.pdf}
    \caption{\textbf{Overview of ACR in the $t$-th iteration.} \textbf{(1) Sampling}: The model $M_t$ samples $n$ code responses $\{y^{i}\}_{i=1}^n$ with high temperature for a programming instruction $x$ in the dataset $D_t$. \textbf{(2) Ranking}: A composite scoring system scores the $n+1$ code responses (including $y^0$ from origin dataset) and ranks them, identifying a winner $y^w$ and $n$ losers $\{y^{l_i}\}_{i=1}^n$. \textbf{(3) Refining}: Depending on the identity of the $y^w$, the selective critique strategy constructs a new single-round data $(x,y^w)$ or two-round critique data $(x, y^{l_1}, c, y^w)$, where $y^{l_1}$ is the highest-scoring loser, $c$ is critique. The original data $(x, y)$ is replaced with new data and all the new data form a new dataset $D_{t+1}$. \textbf{(4) Training}: The new model $M_{t+1}$ is fine-tuned using $D_{t+1}$.}
	\label{method:model}
\end{figure*.




\textbf{Distillation-based Code Fine-Tuning} 
Unlike proprietary models, many studies focus on fine-tuning open-source code pre-trained models, which has greatly contributed to the rapid development of the code generation field. A key technique for achieving this is distilling data from teacher models. **Raffel et al., "Code Alpaca"** introduces **Lample and Conneau, "Self-Instruct"** to distill **Brown, "GPT-3"**. Following this, **Chen et al., "WizardCoder"** evolves more complex code instruction data using **Guo et al., "Evol-Instruct"**. **Li et al., "MagiCoder"** proposes **OSS-Instruct**, where the model generates instructions and code responses sequentially based on open-source code snippets, thus creating more practical code data. In contrast to OSS-Instruct, **Wang et al., "InverseCoder"** reverses the order of instruction and code response generation. **Zhu et al., "WaveCoder"** constructs a multi-task code dataset, enhancing the model's versatility. Besides, **Kumar et al., "OpenCodeInterpreter"** builds multi-turn code data, enabling the model to learn from execution feedback and human feedback. However, these methods aim to enable the model to learn by imitating the teacher, overlooking the potential for refinement through self-generated code.
%Additionally, **EpiCoder extracts features from code snippets using Abstract Syntax Trees (AST), further enhancing code diversity**.




\textbf{Iterative Self-Refinement of LLMs} 
Iterative self-refinement refers to LLMs enhancing themselves iteratively by utilizing self-generated responses with the help of external signals. 
One line of research **Guo et al., "Focus on enabling self-correction during the inference stage"** focuses on enabling self-correction during the inference stage by iteratively calling LLMs and incorporating external signals. In the code domain, **Chen et al., "CodeT"**, **Zhu et al., "Self-Debugging"**, and **Wang et al., "LDB"** follow this approach. However, these prompt engineering methods with external feedback cannot improve the intrinsic capabilities. Another line of research focuses on iteratively training the model using self-generated outputs to enhance its intrinsic capabilities **Li et al., "DPO"**. **Kumar et al., "CodeLutra"** has successfully applied this approach to the code domain, but it heavily depends on golden labels, which limits its applicability. In contrast to the works above, we propose the ACR method, which achieves iterative self-refinement by only using a simple Supervised Fine-Tuning (SFT) loss. This approach is orthogonal to prompt engineering methods like **Zhu et al., "Self-Debugging"** and is more efficient and generalizable than **Kumar et al., "CodeLutra".