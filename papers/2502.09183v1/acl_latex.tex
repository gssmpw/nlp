% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}






% zhoucz
\usepackage{amsmath}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{rotating}
\usepackage{color}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{float}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{arydshln}
\usepackage{url}
\usepackage{subcaption}
\usepackage{graphicx}




% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation}
% \title{RefineCoder: Iterative Refinement of Code Generation Models with LLM-as-a-Judge and LLM-as-a-Critic}



%RefineCoder: Iterative Refinement of Code Generation Models Using LLM-as-a-Judge and LLM-as-a-Critic}

% RefineCoder: Iterative Enhancing of Code Generation Models based Pair-wise Judge and Critic via Dynamic Data Refinement
% RefineCoder: A Iterative Refinement Framework infusing LLM-as-a-Judge and LLM-as-a-Critic for Code Generation

% RefineCoder: Iterative Enhancement of Code Generation Models via Dynamic Data Refinement

% boosting code llm: LLM boosting for improved/refined code generation 
% caoyixin's google scholar


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Changzhi Zhou$^{1}$\thanks{~ Work done during internship at Meituan. Equal contribution.}, \  Xinyu Zhang$^{1}$\footnotemark[1], \  Dandan Song$^{1}$\thanks{~ Corresponding Author.}, \ Xiancai Chen$^{2}$, Wanli Gu$^{3}$, \\ {\bf Huipeng Ma$^{1}$, Yuhang Tian$^{1}$, Mengdi Zhang$^{3}$, Linmei Hu$^{1}$\footnotemark[2]} \\
  $^{1}$School of Computer Science and Technology, \\Beijing Institute of Technology, Beijing, China \\ $^{2}$Peking University  \ \ $^{3}$Meituan \\
  \texttt{zhouchangzhi97@gmail.com}  \ \ \texttt{\{sdd,hulinmei\}@bit.edu.cn}}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

% Iterative Code Refinement  ICR.

\begin{document}
\maketitle
\begin{abstract}
Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.
\end{abstract}

\section{Introduction}



Code generation, also called program synthesis, is a key application area of Large Language Models (LLMs) and has attracted significant attention from the research community~\cite{gulwani2017program,chen2021evaluating}. Numerous studies focus on developing code-specific pre-trained models, such as CodeLlama~\cite{roziere2023code}, DeepSeek-Coder~\cite{guo2024deepseek}, and Qwen-Coder~\cite{hui2024qwen2}. These LLMs demonstrate strong capabilities in code understanding owing to their pre-training on large-scale code corpus. However, they must undergo post-training to become user-friendly and effective for code-related tasks.



\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/example.pdf}
    \caption{Comparison of two fine-tuning paradigms. DFT uses code distilled by teacher model, whereas ACR adaptively constructs distinct data formats infusing self-generated code and external critique.}
    \label{intro:example}
\end{figure}



% SSR: Selective Self-Refinement

% utilizes two formats to reinforce self-generated high-quality code and critique self-generated low-quality code.
%Instruction tuning~\cite{wei2021finetuned} is commonly post-training technology used to help models follow human instructions and improve performance on downstream tasks. 
As a popular post-training technology, Distillation-based Fine-Tuning (DFT) leverages code instruction data synthesized by powerful teacher models (often proprietary LLMs like GPT-4) to fine-tune code LLMs, as shown in Figure~\ref{intro:example}.
%code instruction tuning aims to perform Supervised Fine-Tuning using code-related instruction data, as shown in Figure~\ref{intro:example}. 
%These data are usually synthesized by powerful teacher models (relying heavily on proprietary LLMs like GPT-4) due to that manual annotation is cost-expensive and impractical, leading to Distillation-based Fine-Tuning (DFT), as shown in Figure~\ref{intro:example}. 
Many works~\cite{zhang2024unifyingpe} based on this paradigm have been proposed recently. For example, Code Alpaca~\cite{codealpaca} initially establishes this synthesis paradigm using Self-Instruct~\cite{wang-etal-2023-self-instruct}. Subsequently, Code-related Evol-Instruct~\cite{luo2024wizardcoder} prompts LLMs to evolve more intricate code instructions, while OSS-Instruct~\cite{wei2024magicoder} generates more realistic code instructions based on open-source code snippets from The Stack~\cite{kocetkov2022stack}. 
In contrast, OpenCodeInterpreter~\cite{zheng-etal-2024-opencodeinterpreter} enables code LLMs to learn from compiler diagnostics and human feedback by constructing multi-turn feedback data. These diverse synthesis strategies have significantly advanced the development of code LLMs. However, this paradigm of \textbf{\textit{teacher model distillation}} inevitably suffers from a critical limitation: the performance of the student model largely relies on the teacher model, ignoring the potential of iterative refinement by self-generated code.

Apart from the research on code instruction fine-tuning, some studies~\cite{chen2023codet, huang2023agentcoder,chen2024teaching,chen2025revisitselfdebuggingselfgeneratedtests} have explored the ability of LLMs to refine self-generated code during inference. For example, Self-Debugging~\cite{chen2024teaching} iteratively debugs self-generated code using feedback from the code executor. However, these methods that freeze parameters are essentially a form of \textbf{\textit{prompt engineering with external feedback}}. They cannot improve the intrinsic one-off code generation ability of LLMs. Besides,  multiple calls to LLMs increase the inference latency.


% However, such \textbf{\textit{prompt engineering with external feedback}} methods can only be used in the inference stage, and thus cannot improve the inherent code generation ability of LLMs. Besides,  multiple calls of the LLMs increase the inference latency.

%these methods with frozen parameters are essentially a form of \textbf{\textit{prompt engineering using external feedback}}, and thus cannot improve the inherent code generation ability of LLMs. Besides,  multiple calls of the LLMs increase the inference latency.


In this paper, we propose Adaptive Critique Refinement (ACR), a novel fine-tuning paradigm to enhance the intrinsic one-shot code generation ability of LLMs, and on the basis of which we develop a series of code LLMs named RefineCoder.  ACR goes beyond traditional teacher distillation by using both self-generated code and external critique to refine the model. Specifically, 
%ACR \textcolor{red}{这句话应从全局介绍ACR，ACR的作用肯定不止是产生多个代码回复数据并打分，而应当是自适应的生成新数据，应该把这个讲出来，也符合acr的名字。我的修改标黄，你酌情参考generates multiple code responses and scores them with a new composite scoring system. }
ACR tailors the distinct new fine-tuning dataset by first scoring and critiquing the self-generated multiple code responses, and then adaptively forms new samples according to the scoring and critiquing results. The two types of samples are shown in Figure~\ref{intro:example}. 
Based on the new dataset, we fine-tune RefineCoder models. Finally, we iteratively applying the above ACR process to achieve continuous improvement in code generation capabilities of RefineCoder. Notably, in ACR, we design a composite scoring system and a selective critiquing strategy to better score code responses and selectively critique low-quality code. The former combines LLM-as-a-Judge, an Elo rating mechanism~\cite{elo1978rating}, and a code executor for accurate and comprehensive evaluation. The latter adaptively constructs  two types of data by comparing the scores of self-generated code with those of original code. When self-generated code performs worse than the original code, an LLM-as-a-Critic is introduced to provide specific critiques.



%Based on ACR, we develop a series of code LLMs (RefineCoder) that are fine-tuned on the constructed new dataset. We can iteratively perform the above process to achieve continuous improvement in the code generation capability.
%Then it constructs distinct data that includes self-generated code and critiques to tailors a new fine-tuning dataset, as shown in Figure~\ref{intro:example}. 
%Concretely, for each code instruction in the dataset, ACR firstly generates multiple code responses by self-sampling and scores them with a new composite scoring system. Then it adaptively updates the existing dataset by infusing self-generated code and critiques in Figure 3 and fine-tunes model on the new dataset, as shown in Figure~\ref{intro:example}. Finally, we iteratively perform the above ACR process to achieve continuous improvement in the code generation capability.
%this dataset comprises two types of examples: those reinforcing high-quality self-generated code and those critiquing low-quality self-generated code.
%This strategy resembles the process of a student first solving problems and then critiquing their answers based on provided solutions, rather than directly imitating the solutions~\cite{wang2025critique}. 
%Iteratively applying ACR can continuously improve code generation ability.




%In this paper, we propose a novel fine-tuning paradigm, \textbf{Adaptive Code Refinement (ACR)}, to refine the inherent one-off code generation ability using self-generated code and external critique, which goes beyond the limitations of teacher distillation. As shown in Figure~\ref{intro:example}, ACR adaptively constructs two types of instruction data to strengthen self-generated high-quality code and critique self-generated low-quality code. By iterative applying ACR, code LLMs can achieve continuous improvement in code generation. Concretely, ACR self-samples multiple code responses for each code instruction from the existing dataset using existing code LLMs and scores all code responses. Based on these scores, it constructs a new dataset comprising two types of data, and uses it to fine-tune. We design a composite scoring system and a selective critiquing strategy to make the pipeline work. The former provides a thorough and accurate scoring by infusing pairwise LLM-as-a-Judge, Elo mechanism~\cite{elo1978rating} and a code executor. The latter acts as the data construction engine for ACR, adaptively constructing two types of data based on the quality of both self-generated and original code. If self-generated code is worse than the original code in the dataset, an LLM-as-a-Critic is introduced to critique the self-generated code. 
%This strategy resembles the process of a student first solving problems and then refining his answers based on the provided solution rather than directly imitating the solution~\cite{wang2025critique}.








%Concretely, ASR includes two core components: composite scoring system and selective critiquing strategy. The former provides a thorough and accurate ranking of all code responses. It contains pairwise LLM-as-a-Judge, Elo mechanism~\cite{elo1978rating} and a code executor to ensure comprehensive and accurate scoring. The latter functions as the operational engine of ASR. If self-generated code is worse than the original code in the dataset, an LLM-as-a-Critic is introduced to critique the self-generated code using the original code as a reference. This strategy resembles the process of a student first solving problems and then refining his answers based on the provided solution rather than directly imitating the solution~\cite{wang2025critique}.


% oncretely, we introduce a composite scoring system and a selective critiquing strategy to make ASR effective. The former provides a thorough and accurate ranking of all code responses and serves as the prerequisite for ASR. Thanks to the distinguishing and judgment abilities of LLMs~\cite{dong2024self,li2024llmasajudge}, we employ pairwise LLM-as-a-Judge as the backbone of the scoring system, enhanced by the Elo mechanism~\cite{elo1978rating} and a code executor. The latter functions as the operational engine of ASR and is responsible for the adaptive construction of data in Figure~\ref{intro:example}. If self-generated code candidates are worse than the original code in the dataset, an LLM-as-a-Critic is introduced to critique the self-generated code using the original code as a reference. This strategy resembles the process of a student first solving problems and then refining his answers based on the provided solution rather than directly imitating the solution~\cite{wang2025critique}.





%Recent works~\cite{wu2024self,kim2024aligning,dong2024self,wang2024self,anonymous2024longpo} in non-code domains have explored \textbf{\textit{iterative refinement through self-generated responses}} as a path toward AGI. These methods sample the model multiple times and use the self-generated responses to construct preference pairs. They then design complicated reinforcement learning (RL) algorithms to implement iterative preference training. CodeLutra~\cite{tao2024codelutra} migrates this pipeline to the code domain with strong performance. However, these methods typically rely on gold labels, which limits their applicability in code instruction fine-tuning, where most instruction data is LLM-synthesized and lacks gold labels. Besides, the complexity of RL algorithms makes iterative training challenging.



% In this work, we propose an Iterative Code Refinement (ICR) framework that refines code LLMs using simple supervised fine-tuning (SFT) loss without relying on gold labels and RL. We fine-tune a pre-trained code model on a code instruction dataset to obtain initial model. In subsequent iterations, the model refines the instruction data with self-generated code responses and improves itself through SFT. Concretely, we introduce a composite scoring system and a binary refining strategy to make ICR effective. Thanks to LLMs' ability to distinguish and judge~\cite{dong2024self,li2024llmasajudge}, the score system integrates pairwise Judge, Elo algorithm, and Code Executor for an accurate and comprehensive evaluation. The binary refining strategy involves two steps: if self-generated code outperforms the original code in the dataset, it replaces the original; otherwise, a Critic evaluates the self-generated code, constructing a second-round feedback data. This process resembles a student first solving problems and then refining his answers based on the provided solution, rather than directly imitating the solution.


%Concretely, we introduce a composite scoring system and a binary refining strategy to make ICR effective. Thanks to LLMs' ability to distinguish and judge~\cite{dong2024self,li2024llmasajudge}, the score system integrates pairwise Judge, Elo algorithm, and Code Executor for an accurate and comprehensive evaluation. The binary refining strategy involves two steps: if self-generated code outperforms the original code in the dataset, it replaces the original; otherwise, a Critic evaluates the self-generated code, constructing a second-round feedback data. This process resembles a student first solving problems and then refining his answers based on the provided solution, rather than directly imitating the solution.


We conduct iterative ACR based on DS-Coder-6.7B-Base~\cite{guo2024deepseek} and Qwen2.5-Coder-7B-Base~\cite{hui2024qwen2}, resulting in the RefineCoder series. After three iterations, RefineCoder-DS-6.7B and RefineCoder-QW-7B achieve average pass@1 improvements of 2.4 and 3.0, respectively, on the HumanEval (+), MBPP (+), LiveCodeBench, and the BigCodeBench-hard benchmark~\cite{chen2021evaluating,austin2021programsynthesislargelanguage,liu2023is,jain2024livecodebench,zhuo2024bigcodebench}. The key contributions of this paper are as follows:

1) We propose Adaptive Critique Refinement (ACR), a novel fine-tuning paradigm that refines code LLMs with self-generated code and external critique, on the basis of which we develop a series of strong code LLMs (RefineCoder).

2) To ensure the efficacy of ACR, we design a composite scoring system with LLM-as-a-Judge and a selective critique strategy with LLM-as-a-Critic to score and critique self-generated code.

3) Experimental results from the RefineCoder series show that iterative ACR continuously enhances code generation performance. After three iterations, the RefineCoder series achieves comparable or even superior performance than baselines of the same size while requiring less data.




\section{Related Work}

\textbf{LLMs for Code Generation} LLMs have shown exceptional code understanding abilities due to extensive pre-training on code-related corpora. Numerous models like GPT-4o~\cite{gpt4o-blog}, Gemini~\cite{gemini-blog}, Claude-3.5~\cite{claude3p5-blog}, Qwen2.5-Coder~\cite{hui2024qwen2}, and DeepSeek-Coder~\cite{guo2024deepseek} exhibit strong performance on code generation benchmarks. Recently, the release of OpenAI o1~\cite{o1-blog} and DeepSeek-R1~\cite{2025deepseekr1} has spurred a surge in research on deep reasoning LLMs, achieving expert-level performance on competitive programming problems (e.g., CodeForces) and further advancing LLMs in the code domain.




\begin{figure*}[t]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/model.pdf}
    \caption{\textbf{Overview of ACR in the $t$-th iteration.} \textbf{(1) Sampling}: The model $M_t$ samples $n$ code responses $\{y^{i}\}_{i=1}^n$ with high temperature for a programming instruction $x$ in the dataset $D_t$. \textbf{(2) Ranking}: A composite scoring system scores the $n+1$ code responses (including $y^0$ from origin dataset) and ranks them, identifying a winner $y^w$ and $n$ losers $\{y^{l_i}\}_{i=1}^n$. \textbf{(3) Refining}: Depending on the identity of the $y^w$, the selective critique strategy constructs a new single-round data $(x,y^w)$ or two-round critique data $(x, y^{l_1}, c, y^w)$, where $y^{l_1}$ is the highest-scoring loser, $c$ is critique. The original data $(x, y)$ is replaced with new data and all the new data form a new dataset $D_{t+1}$. \textbf{(4) Training}: The new model $M_{t+1}$ is fine-tuned using $D_{t+1}$.}
	\label{method:model}
\end{figure*}




\textbf{Distillation-based Code Fine-Tuning} 
Unlike proprietary models, many studies focus on fine-tuning open-source code pre-trained models, which has greatly contributed to the rapid development of the code generation field. A key technique for achieving this is distilling data from teacher models. Code Alpaca~\cite{codealpaca} introduces Self-Instruct~\cite{wang-etal-2023-self-instruct} to distill GPT-3. Following this, WizardCoder~\cite{luo2024wizardcoder} evolves more complex code instruction data using Evol-Instruct~\cite{xu2024wizardlm}. MagiCoder~\cite{wei2024magicoder} proposes OSS-Instruct, where the model generates instructions and code responses sequentially based on open-source code snippets, thus creating more practical code data. In contrast to OSS-Instruct, InverseCoder~\cite{wu2024inversecoders} reverses the order of instruction and code response generation. WaveCoder~\cite{yu-etal-2024-wavecoder} constructs a multi-task code dataset, enhancing the model's versatility. Besides, OpenCodeInterpreter~\cite{zheng-etal-2024-opencodeinterpreter} builds multi-turn code data, enabling the model to learn from execution feedback and human feedback. However, these methods aim to enable the model to learn by imitating the teacher, overlooking the potential for refinement through self-generated code.
%Additionally, EpiCoder extracts features from code snippets using Abstract Syntax Trees (AST), further enhancing code diversity.




\textbf{Iterative Self-Refinement of LLMs} 
Iterative self-refinement refers to LLMs enhancing themselves iteratively by utilizing self-generated responses with the help of external signals. 
One line of research~\cite{huang-etal-2023-large,madaan2023selfrefine,hu-etal-2024-teaching} focuses on enabling self-correction during the inference stage by iteratively calling LLMs and incorporating external signals. In the code domain, CodeT~\cite{chen2023codet}, Self-Debugging~\cite{chen2024teaching}, and LDB~\cite{zhong-etal-2024-debug} follow this approach. However, these prompt engineering methods with external feedback cannot improve the intrinsic capabilities. Another line of research focuses on iteratively training the model using self-generated outputs to enhance its intrinsic capabilities~\cite{dong2024self,yuan2024selfrewarding,kim2025spread}. These methods typically rely on preference learning, such as DPO~\cite{rafailov2023direct}. CodeLutra~\cite{tao2024codelutra} has successfully applied this approach to the code domain, but it heavily depends on golden labels, which limits its applicability. In contrast to the works above, we propose the ACR method, which achieves iterative self-refinement by only using a simple Supervised Fine-Tuning (SFT) loss. This approach is orthogonal to prompt engineering methods like Self-Debugging and is more efficient and generalizable than CodeLutra.





\section{Methodology}

\subsection{Overview}




Adaptive Critique Refinement (ACR) aims to improve code LLMs by refining existing dataset with self-generated code and external critiques. The iterative application of ACR facilitates continuous improvement in code generation performance. As illustrated in Figure~\ref{method:model}, ACR starts with an existing dataset and a code model fine-tuned on it. The method updates the existing dataset through a process of sampling, ranking, and refining. The updated dataset is used for the next round of fine-tuning, iteratively improving the model's code generation capabilities. 

In the following sections, we will introduce two key components of the aforementioned pipeline: the composite scoring system for ranking and the selective critiquing strategy for data refining. Finally, we will introduce the iterative training and the resulting RefineCoder model.


%prompts the code model to sample multiple code responses with high temperature for each data in the dataset. A composite scoring system then scores all code responses and ranks them. Based on the identity of the highest-scoring code response (``winner''), ACR employs selective refining strategy to construct single-round data or second-round critique data. This new data is subsequently used to update the current dataset. Finally, the code base model is fine-tuned on the updated dataset, achieving iterative refinement from self-generated code and external critique.



%Taking the programming instruction $(x_i^*, y_i^*)$ from the current dataset $D_t$ as an example, the current model $M_t$ first samples multiple code responses, in which a high temperature is used to guarantee the diversity of codes. These diverse code responses, together with the original response from dataset, are fed into a sophisticated ranking system. Then, two distinct refining strategies generates updated two-turn data or single-turn data based on identifying the source of the top-ranked code response. The updated data aims to reinforce the model's strengths while enabling it to refine from its own weaknesses. Finally, the current model is fine-tuned with the new data to achieve iterative improvement. Next, we illustrate two core components in ICR: Ranking System and Refining Strategy.







\subsection{Composite Scoring System with LLM-as-a-Judge}

A comprehensive and accurate evaluation of code response quality is the foundation for effective ACR. Previous works~\cite{chen2023codet,zhang2024codedpo} rely on generating test cases to evaluate code quality. However, this method has issues with inaccurate test case generation~\cite{jain2024livecodebench}, inability to handle \textit{special judge}~\cite{quan2025codeelo}, and failure to evaluate non-code content in response such as code comments and explanations.
%However, the following three drawbacks restrict its effectiveness: 1) The accuracy of test cases generated by LLMs is limited~\cite{jain2024livecodebench}; 2) It is not well-suited for \textit{special judge}, where no unique correct outputs exist~\cite{quan2025codeelo}; 3) It is unable to evaluate non-code content in code response, such as code comments and explanations. 

Drawing on the judgment and evaluation abilities of LLMs~\cite{dong2024self,li2024llmasajudge}, we employ an LLM-as-a-Judge as the backbone of the scoring system, supplemented by the Elo mechanism~\cite{elo1978rating} and a code executor to ensure effective evaluation. Specifically, given a programming question $x$ from the existing dataset $D_t$ and the corresponding code response list $\{y^i\}_{i=0}^{n}$, the Judge assigns pairwise relative scores for all code response pairs by identifying their differences and evaluating these discrepancies:

\begin{equation}
\textrm{score}_{judge}^{ij}, \ \textrm{score}_{judge}^{ji} = \textrm{Judge}(x, y^i, y^j),
\end{equation}

where $i, j \in [0, n]$, $i \neq j$, and $\text{score}_{judge}^{ij} \in [1, 10]$ denotes the relative score of the $i$-th response when compared to the $j$-th response. The prompt for LLM-as-a-Judge is shown in Figure~\ref{prompt:pairjudge}. Next, we use Elo mechanism, a ranking algorithm that adjusts scores based on pairwise comparisons, to compute the pointwise score of each code response:
\begin{equation}
\small
\textrm{score}_{judge}^{i} = \textrm{Elo} \left( \left\{ \textrm{score}_{judge}^{ij} \mid j \in [0,n], j \neq i \right\} \right),
\end{equation}

where $i \in [0,n]$ and $\textrm{score}_{judge}^i \in [0,1]$. For more details on Elo in the appendix~\ref{appendix:elo}. In addition to the model scores provided by Judge and Elo, we also use a code executor to evaluate the executability of the code:

\begin{equation}
\textrm{score}_{exec}^i = \textrm{Executor}(y^i),
\end{equation}


where $i \in [0,n]$ and $\text{score}_{exec}^i \in \{0, 1\}$. Finally, we rank all code responses based on the composite score of them:

\begin{equation}
\begin{split}
    &\textrm{score}^i = \textrm{score}^i_{judge} + \textrm{score}^i_{exec}, \\
\end{split}
\end{equation}
\begin{equation}
\begin{split}
    &y^w, y^{l1}, \dots, y^{l_n} = \textrm{Sort}(\{y^i\}_{i=0}^{n} \mid \{\textrm{score}^i\}_{i=0}^{n}),
\end{split}
\end{equation}
where $\textrm{score}^i \in [0, 2]$, $y^w$ and $y^{l_1}$ denote the code responses with the highest and second-highest scores, respectively.



\subsection{Selective Critiquing Strategy with LLM-as-a-Critic}

After obtaining the sorted code responses, we design the selective critiquing strategy as data construction engine of ACR. It effectively utilizes high-quality and low-quality self-generated code in distinct ways, while further providing external critiques for the low-quality data. This strategy resembles the process of a student first solving problems and then critiquing their answers based on provided solutions, rather than directly imitating the solutions~\cite{wang2025critique}. 
% 这里的引用加不加呢？



Concretely, when all self-generated code responses are of lower quality than the original code in the existing dataset, namely $y^w$ is $y^0$ and $y^{l_1}$ is the self-generated code, LLM-as-a-Critic is used to critique $y^{l_1}$, using $y^0$ as a reference (The prompt is shown in Figure~\ref{prompt:paircritic}):

\begin{equation}
c = \textrm{Critic}(x, y^w, y^{l_1}).
\end{equation}

Otherwise, if $y^w$ is the self-generated code response, we directly construct new single-round data. The data update rules are as follows:

\begin{equation}
(x, y) \longrightarrow \begin{cases}
(x, y^{l_1}, c, y^w), & \text{if } y^w \text{ is } y^0, \\
(x, y^w), & \text{otherwise}.
\end{cases}
\end{equation}

If the data to be updated comes from a two-round critique rather than a single-round one, only the instruction and the final code response will be considered. Selective refining strategy updates all data to comprise a new instruction dataset $D_{t+1}$. 



\subsection{Iterative Training}

%Our iterative ACR procedure trains a RefineCoder series $M_0, ..., M_T$ where each model $M_t$ uses training data $D_t$ refined by the $M_{t-1}$. We define the RefineCoder series as follows:
The previous sections describes the process of a single ACR in Figure~\ref{method:model}. Next, we describe the iterative ACR process and define RefineCoder:

$\circ$ \textbf{RefineCoder} \textit{Iter0} ($M_0$): The code model trained using the initial SFT dataset $D_0$.

$\circ$ \textbf{RefineCoder}  \textit{Iter1} ($M_1$): The code model trained using dataset $D_1$, where $D_1$ is obtained by refining $D_0$ using $M_0$.

$\circ$ \textbf{RefineCoder}  \textit{Itert} ($M_t$): The code model trained using dataset $D_t$, where $D_t$ is obtained by refining $D_{t-1}$ using $M_{t-1}$.


We fine-tune the original base model from scratch in each iteration to prevent overfitting, consistent with previous iterative training works~\cite{zelikman2022star,wang2024selftaughtevaluators,dong2024self}.







\section{Experiments}
\subsection{Benchmarks}

We use \textbf{HumanEval (+)}~\cite{chen2021evaluating} and \textbf{MBPP(+)}~\cite{austin2021programsynthesislargelanguage, liu2023is} to assess the \textbf{fundamental coding ability} of code model. Meanwhile, we use \textbf{LiveCodeBench}~\cite{jain2024livecodebench} and \textbf{BigCodeBench-hard}~\cite{zhuo2024bigcodebench} to measure the \textbf{advanced coding ability} of code model. We restrict data usage to the range from 240701 to 250201 of the LiveCodeBench to maintain its contamination-free nature.
%\begin{itemize}
%\item \textbf{HumanEval} \& \textbf{MBPP}~\cite{chen2021evaluating,austin2021programsynthesislargelanguage}: They are two popular code generation benchmarks. We use both their base and plus versions to assess the \textbf{fundamental coding ability} of code model within the EvalPlus~\cite{liu2023is} framework.

%\item \textbf{LiveCodeBench}~\cite{jain2024livecodebench}: It is a contamination-free competitive programming benchmark designed to measure the \textbf{advanced coding ability} of code models. To prevent data leakage, we restrict data usage to the range from 240701 to 250201 (Version 3 to Version 5).

%\item \textbf{BigCodeBench}~\cite{zhuo2024bigcodebench}: It is a practical and challenging programming benchmark, and we use its hard version to measure the \textbf{advanced coding ability} of code model.



% \noindent $\bullet$ 








\setlength{\tabcolsep}{4.5pt}
\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccccccccl}
    \toprule[1.5pt]
    \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Models}}}  & \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Dataset}}} & & \multicolumn{2}{c}{\textbf{HumanEval}} & & \multicolumn{2}{c}{\textbf{MBPP}} & \multicolumn{1}{c}{\textbf{LCB}} & \multicolumn{2}{c}{\textbf{BCB-hard}} & & \multirow{2}[2]{*}{\textbf{Average}} \\
\cline{4-6} \cline{7-8} \cline{9-9} \cline{10-11} & & & \textit{Base} & \textit{Plus} & & \textit{Base} & \textit{Plus}  & \multicolumn{1}{c}{\textit{2407-2502}} & \multicolumn{1}{c}{\textit{Inst}} & \multicolumn{1}{c}{\textit{Comp}} & \\    
    \midrule[1.5pt]
    \multicolumn{13}{c}{\textbf{Proprietary Models}} \\
    \hdashline
    \textcolor{gray}{Gemini-1.5-Pro-002} & / & \textcolor{gray}{} & \textcolor{gray}{89.0} & \textcolor{gray}{79.3} & \textcolor{gray}{} & \textcolor{gray}{89.7} & \textcolor{gray}{74.6} &  \textcolor{gray}{30.9} & \textcolor{gray}{20.9} & \textcolor{gray}{32.4} & \textcolor{gray}{} & \textcolor{gray}{59.5} \\
    \textcolor{gray}{Claude-3.5-Sonnet-20240620} & / & \textcolor{gray}{} & \textcolor{gray}{87.2} & \textcolor{gray}{81.7} & \textcolor{gray}{} & \textcolor{gray}{89.4} & \textcolor{gray}{74.3} & \textcolor{gray}{32.0} & \textcolor{gray}{25.7} & \textcolor{gray}{33.1} & \textcolor{gray}{} & \textcolor{gray}{60.5} \\
    \textcolor{gray}{GPT-4o-20240806} & / & \textcolor{gray}{} & \textcolor{gray}{92.7} & \textcolor{gray}{87.2} & \textcolor{gray}{} & \textcolor{gray}{87.6} & \textcolor{gray}{72.2} & \textcolor{gray}{30.5} & \textcolor{gray}{25.0} & \textcolor{gray}{36.5} & \textcolor{gray}{} & \textcolor{gray}{61.7} \\
    \midrule[1pt]
    \multicolumn{13}{c}{\textbf{Open-source Models 7B+ Scale}} \\
    \hdashline
    Qwen2.5-Coder-32B-Instruct  & / & & 92.1          & 87.2       &    & 90.5        & 77.0   & 29.5   & 27.7              & 33.8          &     & 62.5        \\
    DeepSeek-V3  & / & & 91.5          & 86.6       &    & 87.6        & 73.0  & 36.3   & 27.7              & 39.9          &     & 63.2        \\
    Llama3.3-70B-Instruct & / & & 84.1 & 80.5 && 87.6   & 73.5   &  29.1  & 28.4   & 28.4  &     & 58.8   \\
    \midrule[1pt]
    \multicolumn{13}{c}{\textbf{Open-source Models $\sim$7B Scale}} \\
    \hdashline
\raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/deepseek.png}} DSCoder-6.7B-Base & /   &  & 47.6          & 39.6        &   & 72.0        & 58.7   & /                      & /    & 13.5                                    & & \ \ \ / \\
% \hdashline 
DSCoder-6.7B-Instruct & 2BT  & & 74.4          & 71.3       &    & 74.9        & 65.6  &  12.8    & 10.1              & 15.5          &     & 46.4        \\
MagiCoder-S-DS-6.7B  & 185K    &     & 76.8          & 71.3  &         & 79.4        & \textbf{69.0}    &13.4 & 13.5        &       12.8             & & 48.0   \\
OpenCodeInterpreter-DS-6.7B & 178K & &  \textbf{77.4}          & \textbf{72.0}         &  & 76.5        & 66.4     & 7.3     & 13.5              & \textbf{16.9}           &    & 47.1     \\
WaveCoder-Ultra-6.7B    & 130K   &   & 75.0          & 69.5        &   & 74.9        & 63.5 & 12.9 &  12.8              & \textbf{16.9}   &            & 46.5 \\
\rowcolor{gray!15} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter0}       & 80K     &    & 73.8          & 67.7      &     & 77.5        & 65.1      &  13.6      & 18.2            & 10.1   & & 46.6   \\
\rowcolor{gray!30}  \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter1}   & 80K         &   & 74.4          & 68.9      &    & 77.0        & 66.4     & 14.1              & 18.9              & 14.2      &        & 47.7\textcolor[rgb]{0,0.5,0}{$_{+1.1}$}                        \\
\rowcolor{gray!45} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter2}    & 80K         &   & 74.4          & 70.3        &  & 79.6        & 66.9      &  \textbf{14.3}                     & 19.6              & 14.2 &             & 48.5\textcolor[rgb]{0,0.5,0}{$_{+1.9}$} \\
\rowcolor{gray!60} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter3}   & 80K        &     & 75.0          & 70.7       &   & \textbf{80.2}        & 67.2     & 14.2     & \textbf{20.3}              & 15.5 & & \textbf{49.0}\textcolor[rgb]{0,0.5,0}{$_{+2.4}$}  \\
\raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/qwen.png}}Qwen2.5-Coder-7B-Base  & /   &  & 61.6         & 53.0        &   & 76.9        & 62.9   & /                       & /    & 16.2                                    & & \ \ \ / \\
% \hdashline
Qwen2.5-Coder-7B-Instruct & Millions && \textbf{88.4} & \textbf{84.1} && 83.5 & 71.7 &18.1 & 20.3 & 20.3 && 55.2  \\
\rowcolor{gray!15} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter0}       & 20K     &    & 85.4          & 78.0      &     & 79.4        & 65.1      &  18.2                         & 18.2            & \textbf{20.9}   & & 52.2   \\
\rowcolor{gray!30} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter1}    & 20K        &   & 86.0          & 79.3      &    & 84.7        & 71.4     &   17.4                        & 16.2              & 18.9      &        & 53.4\textcolor[rgb]{0,0.5,0}{$_{+1.2}$} \\
\rowcolor{gray!45} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter2}   & 20K         &   & 86.6          & 80.5      &    & 84.7        & 72.0     &   \textbf{19.2}                    & 18.2              & 19.6      &        & 54.4\textcolor[rgb]{0,0.5,0}{$_{+2.2}$}    \\
\rowcolor{gray!60} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter3}    & 20K        &   & 87.2    & 81.1      &    & \textbf{85.2}       & \textbf{73.0}    &  19.1    & \textbf{19.6}    & \textbf{20.9}      &        & \textbf{55.2}\textcolor[rgb]{0,0.5,0}{$_{+3.0}$}    \\
\bottomrule[1.5pt]
\end{tabular}
\caption{Main Results. LCB/BCB denotes LiveCodeBench/BigCodeBench. Inst/Comp denote Instruct/Complete. \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/deepseek.png}} and \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/qwen.png}} denotes the two base models, and the rest are the instruction models. 2BT denotes 2B tokens.}
\label{exp:main}
\end{table*}














\subsection{Baselines}


\noindent $\circ$ \textbf{Proprietary Models}: Gemini-1.5-Pro-002~\cite{gemini-blog}, Claude-3.5-Sonnet-20240620~\cite{claude3p5-blog}, and GPT-4o-20240806~\cite{gpt4o-blog}.

\noindent $\circ$ \textbf{Open-source Models 7B+ Scale}: Qwen2.5-Coder-32B-Instruct~\cite{hui2024qwen2}, DeepSeek-V3~\cite{deepseekai2024deepseekv3technicalreport}, and Llama3.3-70B-Instruct~\cite{grattafiori2024llama3herdmodels}.
    
\noindent $\circ$ \textbf{Open-source Models $\sim$7B Scale}: DSCoder-6.7B Series~\cite{guo2024deepseek}, MagiCoder-S-DS-6.7B~\cite{wei2024magicoder}, OpenCodeInterpreter-DS-6.7B~\cite{zheng-etal-2024-opencodeinterpreter}, WaveCoder-Ultra-6.7B~\cite{yu-etal-2024-wavecoder}, and Qwen2.5-Coder-7B Series~\cite{hui2024qwen2}.


\subsection{Initial SFT Dataset $D_0$}

The existing datasets vary in quality and carry the risk of data leakage~\cite{wang-etal-2024-code, yu-etal-2024-wavecoder}. To ensure the accuracy and reliability of the experimental results, we constructed a high-quality SFT dataset by combining the Evol-Instruct and SelfCodeAlign~\cite{wei2024selfcodealign}. Concretely, we first prompt GPT-4o to generate code-related concepts and corresponding programming instructions from Python code snippets. These code snippets have been preprocessed to ensure high quality and contamination-free. Then, we prompt GPT-4o again to iteratively evolve these instructions using Evol-Instruct strategy. Finally, we prompt GPT-4o to generate code responses for the instructions. Through this pipeline, we obtained an 80K high-quality and diverse python instruction dataset. See Appendix~\ref{appendix:sftdataset} for detailed data construction prompts.




\setlength{\tabcolsep}{5pt}
% Table generated by Excel2LaTeX from sheet 'asqp'
\begin{table*}[t]
  \centering
  \small
    \begin{tabular}{lrrccccccc}
    \toprule[1.2pt]

    \textbf{Dataset} && \textbf{Size} && \textbf{HumanEval (+)} & \textbf{MBPP (+)} && \textbf{LCB} && \textbf{BCB-hard} \\
    \midrule
    Magic-OSS-Instruct~\cite{wei2024magicoder}~\footnote{\url{https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K}} && 75K & & 4.45  &	9.40  &&	2.63 &&	4.84  \\
    Magic-Evol-Instruct~\cite{wei2024magicoder}~\footnote{\url{https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K}} && 110K & & \textcolor{red}{43.20} &	\textcolor{red}{19.40} &&	2.91 &&	4.46 \\
    Evol-CodeAlpaca-v1~\cite{luo2024wizardcoder}~\footnote{\url{https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1}} && 110K & & \textcolor{red}{47.04} & \textcolor{red}{19.46}  &&	2.91 &&	4.46 \\
    Code-FeedBack~\cite{zheng-etal-2024-opencodeinterpreter}~\footnote{\url{https://huggingface.co/datasets/m-a-p/Code-Feedback}} && 68K & & \textcolor{red}{30.50} &	\textcolor{red}{17.67} &&	3.16  && 4.78  \\
    \midrule
    Ours && 80K & & 4.97 	& 7.00 &&	1.54 &&	5.43  \\
    \bottomrule[1.2pt]
    \end{tabular}%
    \caption{The Test Leakage Indicator (TLI) quantifies data leakage by measuring the average maximum n-gram overlap between dataset samples and benchmark samples. The larger the TLI value, the more severe the data leakage. Values in red indicate severe data leakage.}
  \label{exp:dataleakage}%
\end{table*}%





\subsection{Implement Details}
\textbf{Iterative Training} We employ DSCoder-6.7B-Base and Qwen2.5-Coder-7B-Base as the base pre-trained models. To obtain an initial code model $M_0$, we fine-tune DSCoder-6.7B-Base on the 80K initial SFT dataset, resulting in RefineCoder-DS-6.7B (\textit{Iter0}). Furthermore, since Qwen2.5-Coder-Base exhibits strong code understanding and generation capabilities, we accelerate our experimental iterations by randomly sampling 20K examples from the same dataset for fine-tune it, yielding RefineCoder-QW-7B (\textit{Iter0}). The two RefineCoder models undergo three iterative training using their respective datasets under our ACR framework. In the ACR pipeline, the model self-samples 5 code responses with a temperature of 0.7. We use Qwen2.5-32B-Instruct~\cite{hui2024qwen2} as the Judge and Critic. We set the number of training epochs to 2, the global batch size to 64, and the learning rate to 5e-6, employing the AdamW optimizer with a cosine learning rate decay strategy. All our training is performed using 16 A100-80G GPUs, utilizing the LLaMA-Factory~\cite{zheng2024llamafactory}.

\textbf{Evaluation} We use the Pass@1 metric to evaluate the performance of model on the benchmark. For baseline results, we prioritize those from the benchmark leaderboard~\footnote{\url{https://evalplus.github.io/leaderboard.html}}~\footnote{\url{https://livecodebench.github.io/leaderboard.html}}~\footnote{\url{https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard}} or the original papers. If unavailable, we evaluate them locally using the same settings as the RefineCoder series.

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}
%         \centering
%         \includegraphics[width=0.8\linewidth]{figs/ablation_ds.pdf}
%     \end{subfigure}
%     \begin{subfigure}
%         \centering
%         \includegraphics[width=0.8\linewidth]{figs/ablation_qw.pdf}
%     \end{subfigure}
%     \caption{Ablation Study.}
%     \label{exp:ablation}
% \end{figure}

















% \setlength{\tabcolsep}{4pt}
% \begin{table*}[]
% %\small
% \begin{tabular}{lccccccccccccl}
%     \toprule[1.5pt]
%     \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Models}}}  & & \multicolumn{2}{c}{\textbf{HumanEval}} & & \multicolumn{2}{c}{\textbf{MBPP}} & & \multicolumn{1}{c}{\textbf{LCB}} & & \multicolumn{2}{c}{\textbf{BCB-hard}} & & \multirow{2}[2]{*}{\textbf{AVG}} \\
% \cline{3-4} \cline{6-7} \cline{9-9} \cline{11-12}  & & \textit{Base} & \textit{Plus} & & \textit{Base} & \textit{Plus}  & & \multicolumn{1}{c}{\textit{V3-V5}} & & \multicolumn{1}{c}{\textit{Inst}} & \multicolumn{1}{c}{\textit{Comp}} & \\    
%     \midrule[1.5pt]
%     \multicolumn{14}{c}{\textit{Proprietary Models}} \\
%     \hdashline
%     \textcolor{gray}{Gemini-1.5-Pro-002} & \textcolor{gray}{} & \textcolor{gray}{89.0} & \textcolor{gray}{79.3} & \textcolor{gray}{} & \textcolor{gray}{89.7} & \textcolor{gray}{74.6} & \textcolor{gray}{} & \textcolor{gray}{30.9} & \textcolor{gray}{} & \textcolor{gray}{20.9} & \textcolor{gray}{32.4} & \textcolor{gray}{} & \textcolor{gray}{59.5} \\
%     \textcolor{gray}{Claude-3.5-Sonnet-20240620} & \textcolor{gray}{} & \textcolor{gray}{87.2} & \textcolor{gray}{81.7} & \textcolor{gray}{} & \textcolor{gray}{89.4} & \textcolor{gray}{74.3} & \textcolor{gray}{} & \textcolor{gray}{32.0} & \textcolor{gray}{} & \textcolor{gray}{25.7} & \textcolor{gray}{33.1} & \textcolor{gray}{} & \textcolor{gray}{60.5} \\
%     \textcolor{gray}{GPT-4o-20240806} & \textcolor{gray}{} & \textcolor{gray}{92.7} & \textcolor{gray}{87.2} & \textcolor{gray}{} & \textcolor{gray}{87.6} & \textcolor{gray}{72.2} & \textcolor{gray}{} & \textcolor{gray}{30.5} & \textcolor{gray}{} & \textcolor{gray}{25.0} & \textcolor{gray}{36.5} & \textcolor{gray}{} & \textcolor{gray}{61.7} \\
%     \midrule[1pt]
%     \multicolumn{14}{c}{\textit{Open-source Models 7B+ Scale}} \\
%     \hdashline
%     Qwen2.5-Coder-32B-Instruct  & & 92.1          & 87.2       &    & 90.5        & 77.0  &      & 29.5      &   & 27.7              & 33.8          &     & 62.5        \\
%     DeepSeek-V3  & & 91.5          & 86.6       &    & 87.6        & 73.0  &      & 36.3      &   & 27.7              & 39.9          &     & 63.2        \\
%     Llama3.3-70B-Instruct & & 84.1 & 80.5 && 87.6   & 73.5   &      & 29.1     &   & 28.4   & 28.4  &     & 58.8   \\
%     \midrule[1pt]
%     \multicolumn{14}{c}{\textit{Open-source Models $\sim$7B Scale}} \\
%     \hdashline
% \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/deepseek.png}} DSCoder-6.7B-Base   &  & 47.6          & 39.6        &   & 72.0        & 58.7      &  & / &                        & /    & 13.5                                    & & \ \ \ / \\
% % \hdashline 
% DSCoder-6.7B-Instruct  & & 74.4          & 71.3       &    & 74.9        & 65.6  &      & 12.8      &                   & 10.1              & 15.5          &     & 46.4        \\
% MagiCoder-S-DS-6.7B     &     & 76.8          & 71.3  &         & 79.4        & \textbf{69.0}    &    & 13.4 && 13.5        &       12.8             & & 48.0   \\
% OpenCodeInterpreter-DS-6.7B & &  \textbf{77.4}          & \textbf{72.0}         &  & 76.5        & 66.4     &   & 7.3                    &     & 13.5              & \textbf{16.9}           &    & 47.1     \\
% WaveCoder-Ultra-6.7B      &   & 75.0          & 69.5        &   & 74.9        & 63.5 & 
%  & 12.9 & &  12.8              & \textbf{16.9}   &            & 46.5 \\
% \rowcolor{gray!15} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter0}           &    & 73.8          & 67.7      &     & 77.5        & 65.1      &  & 13.6         &                & 18.2            & 10.1   & & 46.6   \\
% \rowcolor{gray!30}  \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter1}           &   & 74.4          & 68.9      &    & 77.0        & 66.4     &  & 14.1          &              & 18.9              & 14.2      &        & 47.7\textcolor[rgb]{0,0.5,0}{$_{+1.1}$}                        \\
% \rowcolor{gray!45} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter2}            &   & 74.4          & 70.3        &  & 79.6        & 66.9      & & \textbf{14.3}   &                     & 19.6              & 14.2 &             & 48.5\textcolor[rgb]{0,0.5,0}{$_{+1.9}$} \\
% \rowcolor{gray!60} \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter3}          &     & 75.0          & 70.7       &   & \textbf{80.2}        & 67.2     &  & 14.2               &         & \textbf{20.3}              & 15.5 & & \textbf{49.0}\textcolor[rgb]{0,0.5,0}{$_{+2.4}$}  \\
% \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/qwen.png}}Qwen2.5-Coder-7B-Base    &  & 61.6         & 53.0        &   & 76.9        & 62.9      &  & / &                        & /    & 16.2                                    & & \ \ \ / \\
% % \hdashline
% Qwen2.5-Coder-7B-Instruct && \textbf{88.4} & \textbf{84.1} && 83.5 & 71.7 && 18.1 && 20.3 & 20.3 && 55.2  \\
% \rowcolor{gray!15} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter0}           &    & 85.4          & 78.0      &     & 79.4        & 65.1      &  & 18.2         &                & 18.2            & \textbf{20.9}   & & 52.2   \\
% \rowcolor{gray!30} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter1}           &   & 86.0          & 79.3      &    & 84.7        & 71.4     &  & 17.4          &              & 16.2              & 18.9      &        & 53.4\textcolor[rgb]{0,0.5,0}{$_{+1.2}$} \\
% \rowcolor{gray!45} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter2}           &   & 86.6          & 80.5      &    & 84.7        & 72.0     &  & \textbf{19.2}         &              & 18.2              & 19.6      &        & 54.4\textcolor[rgb]{0,0.5,0}{$_{+2.2}$}    \\
% \rowcolor{gray!60} \textbf{RefineCoder-QW-7B} \ \ \textit{Iter3}           &   & 87.2    & 81.1      &    & \textbf{85.2}       & \textbf{73.0}    &  & 19.1  &    & \textbf{19.6}    & \textbf{20.9}      &        & \textbf{55.2}\textcolor[rgb]{0,0.5,0}{$_{+3.0}$}    \\

% \bottomrule[1.5pt]

% \end{tabular}
% \caption{Main Results (Pass@1). LCB/BCB/AVG is the abbreviation of LiveCodeBench/BigCodeBench/Average. V5/Inst/Comp is the abbreviation of Version5/Instruct/Complete. \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/deepseek.png}} and \raisebox{-0.1cm}{\includegraphics[width=0.5cm]{figs/qwen.png}} denotes the two base models, and the rest are the instruction models.}
% \end{table*}



















% \section{Experimental Results}

% 先是主实验，数据泄露分析、然后是分布外实验（多语言实验）。然后是消融实验；推理时反馈实验；数据泄露分析，强化实验
\subsection{Experimental Results}
As shown in Table~\ref{exp:main}, the RefineCoder series achieves impressive performance gains on various code generation benchmarks. While the initial RefineCoder-DS-6.7B and RefineCoder-QW-7B exhibit moderate performance, the two models achieve an average Pass@1 improvement of 2.4 and 3.0, respectively, on all benchmarks after three iterations of refinement. Furthermore, it is worth noting that the RefineCoder series is fine-tuned with a very limited amount of data (80K or 20K). Despite this, the RefineCoder models (\textit{Iter3}) still outperform or match baselines of the same size, which are fine-tuned with more extensive data.


Concretely, data refinement improves the fundamental programming abilities of the RefineCoder series, as demonstrated by continuous performance gains over iterations on HumanEval, HumanEval+, and MBPP+. Notably, RefineCoder-QW-7B shows a remarkable 7.9 point increase (\textit{Iter0} -> \textit{Iter3}) in Pass@1 on MBPP+, highlighting the efficacy of this approach. In the more challenging benchmarks, LiveCodeBench and BigCodeBench-hard, the performance of the RefineCoder also improves after iterations. Compared to the baseline OpenCodeInterpreter, RefineCoder-DS-6.7B (\textit{Iter2}) outperforms it by 7 points on LiveCodeBench, illustrating the strong competitive programming abilities of the RefineCoder series. These results demonstrate that the model can iteratively enhance code generation capabilities by self-generated code and critique with the help of our ACR method.







\section{Further Study}

\subsection{Data Leakage Analysis}
In Table~\ref{exp:main}, we observe that while the RefineCoder (\textit{Iter3}) surpasses or matches the baselines on average pass@1, it still lags behind the state-of-the-art baseline of the same size on HumanEval (+) and MBPP+. To better explore this performance gap, we investigate potential data leakage in the datasets used by the baselines.

Concretely, we collect the open-source datasets used by MagiCoder-S-DS-6.7B, OpenCodeInterpreter-DS-6.7B, and WaveCoder-Ultra-6.7B (the datasets for the other baselines are not public) and evaluate data leakage using the TLI metric proposed by~\citet{wang-etal-2024-code}. The results are shown in Table~\ref{exp:dataleakage}, Magic-OSS-Instruct and our dataset are entirely free from data leakage. In contrast, Magic-Evol-Instruct, Evol-CodeAlpaca-v1, and Code-FeedBack demonstrate severe data leakage with respect to HumanEval (+) and MBPP (+). These datasets are used by the three aforementioned baselines, making it challenging for RefineCoder to surpass these baselines. We also present a intuitive scatter plot of similarity scores in the appendix~\ref{appendix:data_leakage} for a more detailed analysis.




% Table generated by Excel2LaTeX from sheet '强化'
\setlength{\tabcolsep}{2pt}
\begin{table}[t]
  \centering
  \small
    \begin{tabular}{lllcllc}
    \toprule
    \textbf{HumanEval+ (\%)} &&& \textbf{Best} && \textbf{Worst} & \textbf{Error Rate} \\
    \midrule
    RefineCoder-DS-6.7B \textit{Iter3} &&& 76.2  && 60.4\textcolor[rgb]{1,0,0}{$_{-15.8}$}  & 4.3   \\
    RefineCoder-QW-7B \textit{Iter3} &&& 81.7  && 75.0\textcolor[rgb]{1,0,0}{$_{-6.7}$}  & 3.0    \\
    \midrule
    \midrule
    \textbf{MBPP+ (\%)} &&& \textbf{Best} && \textbf{Worst} & \textbf{Error Rate} \\
    \midrule
    RefineCoder-DS-6.7B \textit{Iter3} &&& 65.3  && 60.1\textcolor[rgb]{1,0,0}{$_{-5.2}$}  & 6.3   \\
    RefineCoder-QW-7B \textit{Iter3} &&& 70.4  && 69.6\textcolor[rgb]{1,0,0}{$_{-0.8}$}  & 5.8    \\
    \bottomrule
    \end{tabular}%
    \caption{Best and Worst denote the performance of code with the highest and lowest scores. Error rate denotes the proportion of cases where the best code fails the test cases, but the worst code passes.}
  \label{exp:score}%
\end{table}%




\subsection{The effectiveness of Composite Scoring System}
The performance improvement through iterative training preliminarily validates the effectiveness of the scoring system. In this section, we further verify it by enabling the model sampling 10 code responses for each programming question in benchmarks and scoring them. As shown in Table~\ref{exp:score}, the performance of the highest-scoring code far exceeds that of the lowest-scoring code, while the scoring system maintains a low error rate. This clearly validates its effectiveness.



\subsection{The effectiveness of Selective Critique Strategy}

To validate the effectiveness of the selective critique strategy in ACR, we conduct ablation experiments. As shown in Figure~\ref{exp:ablation}, for each iteration, we construct two variants of the RefineCoder without the 3.1 module in Figure~\ref{method:model}, and another without the 3.2 module. During the iteration process, ablating either module leads to a performance drop in both RefineCoder-QW-7B and RefineCoder-DS-6.7B, indicating the effectiveness of our strategy. Furthermore, removing the second-round critique data has a greater impact on performance. We believe this action prevents the model from reflecting on self-generated low-quality codes and external critiques, thereby hindering performance improvement.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/ablation.pdf}
    \caption{Ablation Study. The y-axis denotes the average pass@1 value on all benchmarks.}
    \label{exp:ablation}
\end{figure}



\subsection{Multilingual Code Generation} 



\begin{figure*}[t]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/eval_feedback.pdf}
    \caption{Evaluation with feedback using RefineCoder-QW-7B.}
    \label{exp:feedback}
\end{figure*}




\setlength{\tabcolsep}{4.5pt}
\begin{table*}[t]
\small
  \centering
    \begin{tabular}{lccccccccccc}
    \toprule
          \textbf{Models} & & \textbf{C++} && \textbf{C\#} & &\textbf{Java} && \textbf{Bash} & \textbf{TypeScript} & \textbf{JavaScript} & \textbf{AVG} \\
    \midrule
    \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter0} & & 56.5  && 58.9  && \textbf{58.9} && \textbf{40.5} & 62.3  & 64.6  & 57.2  \\
    \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter1}  & & 62.1  && \textbf{61.4} && 55.1  && 35.4  & \textbf{65.4} & \textbf{66.5} & \textbf{58.2}  \\
    \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter2}  & & \textbf{63.4} && 56.2  && 56.2  && 38.6  & \textbf{65.4} & 65.8  & 57.5  \\
    \textbf{RefineCoder-DS-6.7B} \ \ \textit{Iter3}  & & \textbf{63.4} && 58.2  && 57.0  && 39.2  & 62.9  & 65.8  & 57.8  \\
    \midrule
    \textbf{RefineCoder-QW-7B} \ \ \textit{Iter0} & & 60.2  && 73.4 & & 68.4 & & 49.4  & 78.0  & \textbf{78.3} & 68.7  \\
    \textbf{RefineCoder-QW-7B} \ \ \textit{Iter1} & & 62.7  && 74.1 & & 65.2  && 51.3  & 76.1  & 77.0  & 68.6  \\
    \textbf{RefineCoder-QW-7B} \ \ \textit{Iter2} & & \textbf{63.8} && 74.1 & & 69.6 & & 48.7  & 78.0    & 77.6  & 69.4  \\
    \textbf{RefineCoder-QW-7B} \ \ \textit{Iter3} & & 60.2  && \textbf{75.3} && \textbf{73.4} && \textbf{53.2} & \textbf{79.2} & 77.6  & \textbf{70.6}  \\
    \bottomrule
    \end{tabular}%
     \caption{Performance of RefineCoder on the MultiPL-E.}
  \label{exp:multiple}%
\end{table*}%




As shown in Table~\ref{exp:multiple}, we also evaluate the out-of-distribution (OOD) code generation performance on multilingual benchmark MultiPL-E~\cite{cassano2022multipl}, despite fine-tuning on a Python-only dataset. After one and three iterations, RefineCoder-DS-6.7B and RefineCoder-QW-7B achieve optimal average performance, surpassing the initial model by 1.0 and 1.9 points, respectively. In particular, RefineCoder-QW-7B (\textit{Iter3}) achieves best results among half of the programming languages. This demonstrates that the effectiveness of our ACR method generalizes well to the OOD code generation domain.





\subsection{Evaluation with External Feedback}



Iterative ACR not only improves the one-off code generation performance of RefineCoder but also endows it with the ability to correct errors based on feedback. Following ~\citet{zheng-etal-2024-opencodeinterpreter}, we design two types of external feedback to evaluate this ability of RefineCoder: 1) \textbf{Execution Feedback}: Model refines its self-generated erroneous code based on execution results from the executor. 2) \textbf{Human Feedback}: GPT-4o first analyzes the programming question, initial error code, and execution results to generate improvement suggestions that mimic human thinking. Model then refines the code based on these suggestions. The results of RefineCoder-QW-7B on HumanEval (+) and MBPP (+) are shown in Figure~\ref{exp:feedback}. It can be observed that external feedback improves performance on all benchmarks, with human feedback yielding a more significant enhancement. Surprisingly, with the help of human-like improvement suggestions, RefineCoder-QW-7B (\textit{Iter1} and \textit{Iter3}) outperforms GPT-4o on three benchmarks, demonstrating its ability to understand feedback and refine code.




% Table generated by Excel2LaTeX from sheet '强化'
\setlength{\tabcolsep}{4.5pt}
\begin{table}[t]
  \centering
  \small
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Models}}} & \multicolumn{2}{c}{\textbf{HumanEval}} & \multicolumn{2}{c}{\textbf{MBPP}} \\
          & \textit{Base} & \textit{Plus} & \textit{Base} & \textit{Plus} \\
    \midrule
    \textbf{Qwen2.5-Coder-7B-Base} & 61.6  & 53.0  & 76.9  & 62.9  \\
        \ \ \ +DPO & 59.8  & 52.4  & 44.2  & 32.6  \\
        \ \ \ +ORPO & \textbf{85.4} & \textbf{79.9} & \textbf{84.4} & \textbf{70.4} \\
    \midrule
    \textbf{RefineCoder-QW-7B} \textit{Iter0} & 85.4  & 78.0  & 79.4  & 65.1  \\
        \ \ \ +DPO & 61.0  & 50.0  & 60.6  & 42.3  \\
        \ \ \ +ORPO & \textbf{86.6} & \textbf{79.3} & \textbf{81.7} & \textbf{69.3} \\
    \bottomrule
    \end{tabular}%
    \caption{Performance comparison of different preference optimization strategies in reinforcement learning.}
  \label{exp:rl}%
\end{table}%




\subsection{Preference Optimization}
During the iterative process, a substantial amount of preference data is generated. For each programming instruction, we select the top-1 and bottom-1 ranked responses to form preference pairs, thereby exploring the potential of preference learning. As shown in Table~\ref{exp:rl}, we use DPO~\cite{rafailov2023direct} and ORPO~\cite{hong-etal-2024-orpo} to optimize the base and SFT models. We observe the opposite results due to the differences in preference strategies. DPO leads to a significant performance decline due to the intrinsic risk of reducing the likelihood of correct code during training~\cite{feng2024analyzingunderstandinglimitationsdpo,tao2024codelutra,pal2024smaug}. In contrast, ORPO builds upon the SFT loss to maximize the likelihood of correct code, resulting in improved performance. The results are consistent with those presented in the CodeLutra~\cite{tao2024codelutra}, reflecting the indispensability of SFT loss and the critical impact of preference strategy design on performance.





\section{Conclusions}

In this paper, we propose the Adaptive Critique Refinement (ACR) method to iteratively refine code LLMs using self-generated code and external critique, which breaks away from the traditional teacher distillation paradigm and improves the intrinsic one-off code generation ability. We design a composite scoring system and a selective critiquing strategy. These two components are centred around LLM-as-a-Judge and LLM-as-a-Critic to evaluate and critique self-generated code. This simulates the process where a student solves a problem independently and then refines it by comparing it with the reference answer. We develop the RefineCoder-DS-6.7B and RefineCoder-QW-7B models and demonstrate the effectiveness of iterative ACR on HumanEval (+), MBPP (+), LiveCodeBench, and BigCodeBench-hard. Further studies reveal the impact of the components in ACR and demonstrate its OOD code generation ability.


\section{Limitations}

Although Adaptive Critique Refinement (ACR) and the associated RefineCoder demonstrate significant improvements in the code generation task, several limitations remain. First, ACR is primarily designed to refine code responses based on programming instructions, necessitating an initial set of high-quality and diverse instructions. Therefore, a specialized code instruction generator is still required to make ACR more automated. Furthermore, while ACR can apply to other reasoning-intensive tasks, such as mathematics, this paper has not fully explored these domains.

%\section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\renewcommand{\ULthickness}{0pt}
\renewcommand{\uline}{\relax}

\bibliography{custom}

\renewcommand{\ULthickness}{0.8pt}
\renewcommand{\uline}{\ULine}





\appendix



\newpage

\section{Elo Algorithm}
\label{appendix:elo}


Firstly, we initialize Elo score $E^i=1000$ for each code response $y^i$. Then, we iteratively update the Elo scores by using the relative scores between any two responses. Taking the response pair $y^i$ and $y^j$ as an example, we obtain their match results $S^i$ and $S^j$:

\begin{align}
    S^i = \frac{\textrm{score}_{judge}^{ij}}{\textrm{score}_{judge}^{ij} + \textrm{score}_{judge}^{ji}}, \\
    S^j = \frac{\textrm{score}_{judge}^{ji}}{\textrm{score}_{judge}^{ij} + \textrm{score}_{judge}^{ji}}.
\end{align}

Then we obtain the expected probabilities of winning $P^{ij}$ and $P^{ji}$ for $y^i$ and $y^j$:

\begin{align}
    & P^{ij} = \frac{1}{1+10^{\frac{E^j-E^i}{400}}}, \\
    & P^{ji} = 1 - P^{ij}.
\end{align}

Finally, we update the Elo score:
\begin{align}
E^i_{new} = E^i + K \times (S^i - P^{ij}), \\
E^j_{new} = E^j + K \times (S^j - P^{ji}).
\end{align}
where $K=32$ is a adjustment factor. After the iteration, we obtain the Elo score for each code response and then normalize it to derive the final judge score:

\begin{align}
\textrm{score}^i_{judge} = \frac{E_i - \min(E)}{\max(E) - \min(E)},
\end{align}
where $i \in [0, n]$ and $\textrm{score}^i_{judge} \in [0, 1]$. 



\section{Prompts for Judge and Critic}
\label{appendix:judge_critic}

The prompt for pairwise LLM-as-a-Judge and LLM-as-a-Critic are shown in Figure~\ref{prompt:pairjudge} and ~\ref{prompt:paircritic}.


\section{Prompts for constructing SFT Dataset}
\label{appendix:sftdataset}

We called GPT-4o three times to create the SFT dataset, with the following prompts used:

$\circ$ Prompt for generating code-related concepts and programming instructions, as shown in Figure~\ref{prompt:gen_ins}.

$\circ$ Prompts for evolving existing instructions, as shown in Figures~\ref{prompt:evol_addition}, ~\ref{prompt:evol_breath}, ~\ref{prompt:evol_comp},~\ref{prompt:evol_concre},~\ref{prompt:evol_deep},~\ref{prompt:evol_diversion},~\ref{prompt:evol_increase}, ~\ref{prompt:evol_misd}, ~\ref{prompt:evol_reason}.

$\circ$ For generating code responses, We directly feed the instructions to the model.



\section{Prompts for Evaluation}
\label{appendix:eval}

For Humaneval (+) and MBPP (+), we use the prompts designed by OpenCodeInterpreter~\cite{zheng-etal-2024-opencodeinterpreter}; for Livecodebench and Bigcodebench-hard, we use the official prompts. When conducting evaluations with execution feedback, we have designed a prompt as illustrated in Figure~\ref{prompt:eval_exec}. When executing human feedback, following OpenCodeInterpreter, we generate two prompts: one for generating improvement suggestions and the other for refine code according to human feedback, as shown in Figures~\ref{prompt:eval_gen_sugg} and ~\ref{prompt:eval_human}.



\section{Data Leakage Analysis}
\label{appendix:data_leakage}

We use 4-grams and 5-grams to compute the similarity between the instructions in the training set and the questions in the benchmark. And then we draw scatter plots by selecting the top-200 data points with the highest similarity from each dataset, as shown in Figure~\ref{fig:dataleakage}. The scatter plot visually illustrates the data leakage phenomenon in the dataset, primarily concentrated in the HumanEval (+) and the latter half of the MBPP (+). This makes it difficult for our model to surpass baselines on these two benchmarks. In contrast, on the leakage-free LiveCodeBench and BigCodeBench-hard benchmarks, our model can outperform all baselines.


\section{Scaling Law of Iterative ACR}

We study the performance trends of RefineCoder-DS-6.7B obtained using ACR and the standard SFT method as the dataset size increases. For the SFT method, we directly trained the model using four datasets of different sizes. For RefineCoder series, we employ an iterative training strategy. For example, after obtaining RefineCoder-DS-10k by fing-tuning on the 10k dataset, we apply ACR to update this 10k dataset, and then randomly sample another 10k from the data pool to form a 20k dataset. The model is fine-tuned again to obtain RefineCoder-DS-20k. The experimental results are shown in Figure~\ref{exp:scaling}, as the dataset size increases, our model achieves greater performance improvements compared to the SFT model on HumanEval and MBPP. Moreover, the performance gap between the two models does not show any signs of narrowing, underscoring the impressive scaling capabilities of the iterative ACR method.




\begin{figure*}[t]
    \centering
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/dl_HumanEvalPlus.pdf}
    \end{minipage}%
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/dl_MbppPlus.pdf}
    \end{minipage}%
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/dl_livecodebench_v3-v5.pdf}
    \end{minipage}%
    \begin{minipage}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/dl_bigcodebench-hard.pdf}
    \end{minipage}
    \caption{Scatter plot of similarity scores between the datasets and four benchmarks, with average similarity scores for different datasets indicated by horizontal dashed lines. We selected the top 200 data points with the highest similarity from each dataset. Different y-axis ranges are set for better visualization.}
    \label{fig:dataleakage}
\end{figure*}

\begin{figure}[]
    \centering
    \includegraphics[width=1.0\columnwidth]{figs/scaling.pdf}
    \caption{Performance curves of RefineCoder obtained using iterative ACR and the standard SFT model as the dataset size increases.}
		\label{exp:scaling}
\end{figure}









\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/judge.pdf}
    \caption{The prompt template for pairwise LLM-as-a-Judge.}
		\label{prompt:pairjudge}
\end{figure*}






\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/critic.pdf}
    \caption{The prompt template for pairwise LLM-as-a-Critic.}
		\label{prompt:paircritic}
\end{figure*}







\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/gen_instruction.pdf}
    \caption{The prompt template for generating code-related concepts and instructions.}
    \label{prompt:gen_ins}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_addition.pdf}
    \caption{The prompt template for Addition evolution.}
    \label{prompt:evol_addition}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_breath.pdf}
    \caption{The prompt template for Breath evolution.}
    \label{prompt:evol_breath}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_complexity.pdf}
    \caption{The prompt template for Complexity evolution.}
    \label{prompt:evol_comp}
\end{figure*}
\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_concretizing.pdf}
    \caption{The prompt template for Concretizing evolution.}
    \label{prompt:evol_concre}
\end{figure*}
\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_deepening.pdf}
    \caption{The prompt template for Deepening evolution.}
    \label{prompt:evol_deep}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_diversion.pdf}
    \caption{The prompt template for Diversion evolution.}
    \label{prompt:evol_diversion}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_increase.pdf}
    \caption{The prompt template for Increase evolution.}
    \label{prompt:evol_increase}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_misdirection.pdf}
    \caption{The prompt template for Misdirection evolution.}
    \label{prompt:evol_misd}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/evol_reasoning.pdf}
    \caption{The prompt template for Reason evolution.}
    \label{prompt:evol_reason}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/eval_execution.pdf}
    \caption{The prompt template for Execution Feedback.}
    \label{prompt:eval_exec}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/eval_human.pdf}
    \caption{The prompt template for Human Feedback.}
    \label{prompt:eval_human}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/prompts/eval_gen_suggestion.pdf}
    \caption{The prompt template for generating improvement suggestions.}
    \label{prompt:eval_gen_sugg}
\end{figure*}









\end{document}
