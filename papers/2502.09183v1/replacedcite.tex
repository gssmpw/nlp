\section{Related Work}
\textbf{LLMs for Code Generation} LLMs have shown exceptional code understanding abilities due to extensive pre-training on code-related corpora. Numerous models like GPT-4o____, Gemini____, Claude-3.5____, Qwen2.5-Coder____, and DeepSeek-Coder____ exhibit strong performance on code generation benchmarks. Recently, the release of OpenAI o1____ and DeepSeek-R1____ has spurred a surge in research on deep reasoning LLMs, achieving expert-level performance on competitive programming problems (e.g., CodeForces) and further advancing LLMs in the code domain.




\begin{figure*}[t]
    \centering
    \includegraphics[width=2.0\columnwidth]{figs/model.pdf}
    \caption{\textbf{Overview of ACR in the $t$-th iteration.} \textbf{(1) Sampling}: The model $M_t$ samples $n$ code responses $\{y^{i}\}_{i=1}^n$ with high temperature for a programming instruction $x$ in the dataset $D_t$. \textbf{(2) Ranking}: A composite scoring system scores the $n+1$ code responses (including $y^0$ from origin dataset) and ranks them, identifying a winner $y^w$ and $n$ losers $\{y^{l_i}\}_{i=1}^n$. \textbf{(3) Refining}: Depending on the identity of the $y^w$, the selective critique strategy constructs a new single-round data $(x,y^w)$ or two-round critique data $(x, y^{l_1}, c, y^w)$, where $y^{l_1}$ is the highest-scoring loser, $c$ is critique. The original data $(x, y)$ is replaced with new data and all the new data form a new dataset $D_{t+1}$. \textbf{(4) Training}: The new model $M_{t+1}$ is fine-tuned using $D_{t+1}$.}
	\label{method:model}
\end{figure*}




\textbf{Distillation-based Code Fine-Tuning} 
Unlike proprietary models, many studies focus on fine-tuning open-source code pre-trained models, which has greatly contributed to the rapid development of the code generation field. A key technique for achieving this is distilling data from teacher models. Code Alpaca____ introduces Self-Instruct____ to distill GPT-3. Following this, WizardCoder____ evolves more complex code instruction data using Evol-Instruct____. MagiCoder____ proposes OSS-Instruct, where the model generates instructions and code responses sequentially based on open-source code snippets, thus creating more practical code data. In contrast to OSS-Instruct, InverseCoder____ reverses the order of instruction and code response generation. WaveCoder____ constructs a multi-task code dataset, enhancing the model's versatility. Besides, OpenCodeInterpreter____ builds multi-turn code data, enabling the model to learn from execution feedback and human feedback. However, these methods aim to enable the model to learn by imitating the teacher, overlooking the potential for refinement through self-generated code.
%Additionally, EpiCoder extracts features from code snippets using Abstract Syntax Trees (AST), further enhancing code diversity.




\textbf{Iterative Self-Refinement of LLMs} 
Iterative self-refinement refers to LLMs enhancing themselves iteratively by utilizing self-generated responses with the help of external signals. 
One line of research____ focuses on enabling self-correction during the inference stage by iteratively calling LLMs and incorporating external signals. In the code domain, CodeT____, Self-Debugging____, and LDB____ follow this approach. However, these prompt engineering methods with external feedback cannot improve the intrinsic capabilities. Another line of research focuses on iteratively training the model using self-generated outputs to enhance its intrinsic capabilities____. These methods typically rely on preference learning, such as DPO____. CodeLutra____ has successfully applied this approach to the code domain, but it heavily depends on golden labels, which limits its applicability. In contrast to the works above, we propose the ACR method, which achieves iterative self-refinement by only using a simple Supervised Fine-Tuning (SFT) loss. This approach is orthogonal to prompt engineering methods like Self-Debugging and is more efficient and generalizable than CodeLutra.