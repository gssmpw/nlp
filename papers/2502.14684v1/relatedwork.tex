\section{Related Work}
\label{Related Work}
\begin{sloppypar}
In this section, we first introduce the development of \gls{nvs} techniques from traditional geometric approaches to recent neural rendering methods. 
We then discuss the integration of depth information in \gls{3dgs} and various depth-based optimization strategies. 
Finally, we describe the fundamental formulation of \gls{3dgs} that serves as the basis for our method. 

%---------------------------------------------------------------------
\textbf{Novel View Synthesis.} 
\gls{nvs} aims to generate novel views of a scene from unseen perspectives. It commonly relies on 3D reconstruction techniques to capture spatial structures and preserve visual details. 
A fundamental step in \gls{nvs} involves estimating the camera’s intrinsic and extrinsic parameters \citep{mueller2019image}, a task typically performed using Structure-from-Motion (SfM) methods \citep{ullman1979interpretation}. 
These methods leverage feature matching across multiple views and epipolar geometry to recover both camera poses and sparse 3D scene structure. 
Building upon \gls{sfm} results, \gls{mvs} techniques \citep{tomasi1992shape} can further enhance the reconstruction by creating denser 3D models. 

Beyond traditional methods such as \gls{sfm} and \gls{mvs}, deep learning has brought new advances to \gls{nvs}, particularly with \gls{nerf}. 
\gls{nerf} represents scenes as continuous volumetric radiance fields encoded by a neural network \citep{mildenhall2021nerf}. It achieves photorealistic rendering by modeling the color and density of each 3D point based on the viewing direction. 
Despite its success in generating high-quality novel views, NeRF’s computational demands often limit its efficiency in real-time applications. 
To meet the demand for faster rendering, \gls{3dgs} has emerged as a promising alternative. It offers an efficient approach by using \(\alpha\)-blending rasterization rather than the computationally intensive volume rendering. 
By optimizing the spatial distribution, scales, rotations, and opacities of the Gaussian primitives, \gls{3dgs} achieves both real-time rendering and high-quality reconstruction \citep{chung2024depth}. 

%---------------------------------------------------------------------
\textbf{Gaussian Splatting with Depth Information.} 
Recent works have explored integrating monocular depth estimation into \gls{3dgs} optimization. 
Various depth estimation models have been employed, such as DPT \citep{ranftl2021vision}, which has been applied in \citet{chung2024depth} and \citet{turkulainen2024dn}, and ZoeDepth \citep{bhat2023zoedepth}, which has been utilized in \citet{li2024dngaussian} and \citet{zhu2023fsgs}. 
Recent advances in monocular depth estimation have led to new models such as Depth Anything V2 \citep{yang2024depth}. 
In this work, we investigate the integration of this recent depth estimation model into \gls{3dgs} optimization.

Many existing methods adopt the depth-based regularization framework introduced by \citet{chung2024depth}, which extends the original \gls{3dgs} rasterization pipeline to produce rendered depth maps for geometric supervision. 
Expanding on this framework, researchers have proposed strategies to enhance \gls{3dgs} performance, such as achieving comparable \gls{nvs} quality with fewer training images \citep{zhu2023fsgs} and improving rendering details \citep{li2024dngaussian}. 
In terms of depth regularization, different approaches have been explored, including fixed-weight balancing between depth and image losses \citep{liu2024endogaussian}, segmented depth regularization with varying emphases \citep{li2024dngaussian}, and early stopping strategies based on depth loss variations \citep{chung2024depth}.
However, these approaches rely on the estimated depth values without explicitly considering their reliability, which may lead to inconsistent optimization behavior across different scenes. 
Furthermore, while existing methods have demonstrated improvements in 2D image synthesis quality, there has been limited systematic analysis of how depth information affects the 3D geometric accuracy.  
To tackle these limitations, our method proposes a confidence-aware depth regularization strategy that selectively incorporates depth supervision based on multi-cue reliability assessment, aiming to achieve more stable optimization in both novel view synthesis and geometric reconstruction tasks.
\end{sloppypar}

%---------------------------------------------------------------------
\textbf{3D Gaussian Splatting Formulation.}
\gls{3dgs} represents 3D scenes using a collection of 3D Gaussian primitives. Each primitive is parameterized by its center \( \mu \in \mathbb{R}^3 \), scale \( s \in \mathbb{R}^3 \), rotation (quaternion) \( q \in \mathbb{R}^4 \), opacity \( \alpha \in \mathbb{R} \), and color features \( f \in \mathbb{R}^K \). The complete parameter set for the \( i \)-th Gaussian is denoted as \( \theta_i = \{ \mu_i, s_i, q_i, \alpha_i, f_i \} \) \citep{kerbl20233d}. The Gaussian function is expressed as:
\begin{equation}
G_i(x) = \exp \left(-\frac{1}{2} (x - \mu_i)^T \Sigma_i^{-1} (x - \mu_i)\right),
\end{equation}
where the covariance matrix \( \Sigma \) is determined by scale \( s \) and rotation \( q \).

For rendering, \gls{3dgs} employs an efficient rasterization pipeline that performs \(\alpha\)-blending of projected Gaussians. The color \( C \) of each pixel is computed by blending contributions from \( N \) overlapping Gaussians:
\begin{equation}
C = \sum_{j \in N} c_j \alpha_j T_j,
\end{equation}
where \( T_j = \prod_{k=1}^{j-1} (1 - \alpha_k) \) represents the accumulated transparency. Here, \( c_j \) and \( \alpha_j \) denote the color and opacity of the \( j \)-th Gaussian, respectively. This formulation naturally handles occlusion by giving priority to Gaussians closer to the camera.

The depth value \( D \) for each pixel is computed through normalized \(\alpha\)-weighted averaging:
\begin{equation}
D = \frac{\sum_{j \in N} d_j \alpha_j T_j}{\sum_{j \in N} \alpha_j T_j},
\end{equation}
where \( d_j = (R_i p_j + T_i)_z \) represents the depth of the \( j \)-th Gaussian relative to the \( i \)-th camera. This normalization ensures robust depth estimation even in regions with sparse Gaussian coverage \citep{chung2024depth}.
%---------------------------------------------------------------------
%---------------------------------------------------------------------