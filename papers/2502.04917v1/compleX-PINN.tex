\documentclass{article}

\newtheorem{theorem}{Theorem}
\usepackage{arxiv}
%\usepackage{amsthm} % This package helps define theorem-like environments
% Define a new "Remark" environment
%\newtheorem{remark}{Remark}[section]
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url} 
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{multirow}
\usepackage{bm}
% simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\graphicspath{ {./images/} }
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathbf{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\my}[1]{{\color{red} #1}}

\title{Complex Physics-Informed Neural Network}


\author{
  Chenhao Si \\
  School of Data Science\\
  The Chinese University of Hong Kong, Shenzhen\\
  Shenzhen, China  \\
  \texttt{222042011@link.cuhk.edu.cn} \\
  %% examples of more authors
  \And
  Ming Yan \\
  School of Data Science\\
  The Chinese University of Hong Kong, Shenzhen \\
  Shenzhen, China  \\
  \texttt{yanming@cuhk.edu.cn} \\
  \And
  Xin Li\\
  Department of Computer Science\\
  Northwestern University\\
  IL, USA \\
  \texttt{xinli2023@u.northwestern.edu} \\
  \And
  Zhihong Xia* \\School of Science, Great Bay University \\ Guangdong, China \\
  \& Department of Mathematics\\
  Northwestern University\\
  IL, USA \\
  \texttt{xia@math.northwestern.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
We propose compleX-PINN, a novel physics-informed neural network (PINN) architecture that incorporates a learnable activation function inspired by Cauchy’s integral theorem. By learning the parameters of the activation function, compleX-PINN achieves high accuracy with just a single hidden layer. Empirical results show that compleX-PINN effectively solves problems where traditional PINNs struggle and consistently delivers significantly higher precision- often by an order of magnitude. %Furthermore, we provide detailed theoretical steps on how the Cauchy activation function is derived based on the Cauchy integral theorem.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
Physics-Informed Neural Networks (PINNs) have emerged as a powerful method for solving both forward and inverse problems involving Partial Differential Equations (PDEs)~\cite{ref1, ref2, ref3, ref4}. 
PINNs leverage the expressive power of neural networks to minimize a loss function that enforces the governing PDEs and boundary/initial conditions. 
%PINNs integrate the fundamental equation directly into the neural network's structure, enhancing the loss function with an additional term derived from the equation's residual. 
This approach has been widely applied across various domains, including heat transfer~\cite{ref5, ref6, ref7}, solid mechanics~\cite{ref8, ref9, ref10}, incompressible flows~\cite{ref11, ref12, ref13}, stochastic differential equations~\cite{ref14, ref15}, and uncertainty quantification~\cite{ref16, ref17}. 

Despite their success, PINNs face significant challenges and often struggle to solve certain classes of problems~\cite{ref18, ref19}. One major difficulty arises in scenarios where the solution exhibits rapid changes, such as in ‘stiff’ PDEs~\cite{ref20}, leading to issues with convergence and accuracy. To address these limitations, researchers have proposed various techniques to improve training efficiency and precision.

Over the years, numerous strategies have been developed to enhance the performance of PINNs, including adaptive weighting of loss functions and selective sampling of training points. For example, Wang et al.\cite{ref19} leveraged the Neural Tangent Kernel (NTK) to analyze gradient evolution in PINN training, adjusting the weights of each loss component accordingly. Other studies\cite{ref21, ref22} have explored methods for dynamically learning these weights during training. Additionally, adaptive sampling techniques have been introduced to tackle stiff problems by focusing on regions with high residuals. Lu et al.\cite{ref23} proposed a threshold-based approach for selecting new training points, while Wu et al.\cite{ref24} introduced a probability density function derived from residuals to improve sampling efficiency. Further extensions include high-dimensional adaptive methods~\cite{ref25} and re-sampling techniques targeting failure regions~\cite{ref26}.

Beyond these approaches, causality has been recognized as an influential factor in PINN training~\cite{ref27, RRef3}. Wang et al.~\cite{ref27} introduced a Causality-PINN, which assigns time-dependent weights to the loss function for time-dependent PDEs, while Daw et al.\cite{RRef3} further integrated causality with importance sampling techniques. 

Additional advancements include domain decomposition methods~\cite{ref28, ref29, ref30, ref31}, improved network initialization schemes~\cite{ref31, ref32, ref33}, novel loss functions~\cite{RRef5}, and alternative network architectures~\cite{RRef2, PIRBN, ref34, ref35}.
%Beyond these approaches, researchers have explored alternative enhancements such as domain decomposition \cite{ref28, ref29, ref30, ref31}, improved network initialization methods \cite{ref31, ref32, ref33}, novel loss functions \cite{RRef5}, and innovative network architectures \cite{RRef2, PIRBN, ref34, ref35}. 
Collectively, these improvements have expanded the capabilities of PINNs for solving complex real-world problems.
%Collectively, these strategies contribute to the ongoing improvement and versatility of PINNs, enabling them to effectively solve a wide range of complex, real-world problems.


However, existing methods primarily rely on additional training techniques or significantly larger models, such as transformers~\cite{RRef2} and convolutional neural networks~\cite{ref35}.
%However, the models are all either adding additional training techniques on PINNs or using a larger network such as transformers~\cite{RRef2} or convolutional neural network~\cite{ref35}. 
While these approaches can enhance performance, they also introduce substantial computational overhead. Auxiliary networks and gradient-based modifications~\cite{ref36, ref37} increase training costs, whereas larger models suffer from slower convergence due to the large number of parameters~\cite{ref18, RRef4}.
%The former approach increases training costs by incorporating auxiliary networks \cite{ref36, ref37} and processing gradients \cite{ref18, RRef4}, while the latter slows down the training process due to the larger model size and increased number of parameters in a large magnitude.

To overcome these limitations, developing a network that enhances both convergence and predictive accuracy without relying on additional training algorithms or significantly increasing model complexity is crucial. 
%Therefore, it is essential to have a network that can independently enhance convergence and prediction accuracy without introducing new training algorithms or altering its structure into a more complex and larger model. 
Motivated by this, we introduce compleX-PINN, which incorporates the Cauchy integral formula~\cite{XNet1, XNet2} into a novel activation function, offering a more efficient and effective alternative to traditional PINNs.
%Inspired by this, we introduce compleX-PINN, which incorporates the Cauchy integral formula~\cite{XNet1,XNet2} into a new activation function for the network.

The main contributions of this paper are summarized as follows: 
\begin{itemize}
    \item To the best of our knowledge, this is the first comprehensive study on using Cauchy-based activation functions in PINNs.
    %Up to the authors' best acknowledgment, this is the first comprehensive work on using Cauchy activation functions in PINN. 
    \item We provide a detailed derivation and motivation for incorporating Cauchy activation functions into PINN architectures.
    %We show a detailed motivation for how we derive the Cauchy activation functions for PINN.
    \item Empirical results demonstrate that compleX-PINN outperforms several PINN-based models.
    %Empirical results demonstrate that compleX-PINN alone outperforms several PINN-based models from recent years.
    \item We show that compleX-PINN is compatible with existing PINN training techniques, further enhancing its performance when integrated with these methods.
    %We enhance our network by integrating it with the training techniques originally designed for PINNs, further improving performance. This shows that our network is compatible with PINN-based training techniques and highlights the significant potential of complX-PINN compared to vanilla PINN.
    %\item We conducted further research on convection and reaction PDEs with varying coefficients. For example, previous studies have developed algorithms capable of solving convection PDEs with a convection coefficient $\beta$ of up to 50 \cite{RRef1, RRef2, RRef3}, and a recent study extended this to 100 \cite{ref38}. However, we demonstrate that compleX-PINN, without auxiliary algorithms, can accurately predict $\beta = 150$.
\end{itemize}

The organization of this paper is as follows. Section~\ref{Sec2.1} provides a brief introduction to PINNs. Our proposed model, compleX-PINN, is introduced in Section~\ref{Cauchy activation function}, where we first present the Cauchy activation function using Cauchy's 1D integral formula in Section~\ref{Sec 3.1}, extend it to high-dimensional cases in Section~\ref{Sec 3.2}, and apply it to neural networks in Section~\ref{Extend to high dimension}. 
%In detail, we start to introduce the Cauchy activation function by Cauchy's 1D integral formula in Section~\ref{Sec 3.1} and extend it into the high-dimensional cases in Section~\ref{Sec 3.2} and apply to neural networks in Section~\ref{Extend to high dimension}. 
Finally, numerical results are presented in Section~\ref{Sec4}, followed by the conclusion.
%The numerical results will be concluded and discussed in Section~\ref{Sec4}.


\section{Physics-Informed Neural Network}
\label{Sec2.1}
Denote the spatial domain as $\Omega \subset \mathbb{R}^n$ with boundary $\partial \Omega$, and let $T$ represent the time domain. The spatial-temporal variable is given by $(\mathbf{x}, t) \in \Omega \times T$. A time-dependent partial differential equation (PDE) over this domain is defined as follows:
\begin{align}
    \mathcal{F}[u](\mathbf{x}, t) &= 0, \label{(1)} \quad (\mathbf{x}, t) \in \Omega \times T, \\
    \mathcal{B}[u](\mathbf{x}, t) &= 0, \label{(2)} \quad (\mathbf{x}, t) \in \partial \Omega \times T, \quad \text{(boundary condition)} \\
    \mathcal{I}[u](\mathbf{x}, 0) &= 0, \quad \mathbf{x} \in \Omega, \hspace{51pt} \text{(initial condition)}
\end{align}
where $\mathcal{F}$, $\mathcal{B}$, and $\mathcal{I}$ are differential operators, and $u(\mathbf{x}, t)$ is the solution to the PDE, subject to boundary and initial conditions.

A PINN parameterized by $\theta$ approximates the solution $u(\mathbf{x}, t)$. The input to the neural network is $(\mathbf{x}, t)$, and the approximation is denoted by $\hat{u}(\theta)(\mathbf{x}, t)$. The PINN minimizes the following objective function:
\begin{align}
    \mathcal{L}(\theta) = \lambda_F \mathcal{L}_F(\theta) + \lambda_B \mathcal{L}_B(\theta) + \lambda_I \mathcal{L}_I(\theta), \label{(3)}
\end{align}
where
\begin{align}
    \mathcal{L}_F(\theta) &= \frac{1}{N_f} \sum_{(\mathbf{x}, t) \in \Omega_F} \big| \mathcal{F}[\hat{u}(\theta)](\mathbf{x}, t) \big|^2, \label{(4)} \\
    \mathcal{L}_B(\theta) &= \frac{1}{N_b} \sum_{(\mathbf{x}, t) \in \Omega_B} \big| \mathcal{B}[\hat{u}(\theta)](\mathbf{x}, t) \big|^2, \label{(5)} \\
    \mathcal{L}_I(\theta) &= \frac{1}{N_0} \sum_{(\mathbf{x}, 0) \in \Omega_I} \big| \mathcal{I}[\hat{u}(\theta)](\mathbf{x}, 0) \big|^2. \label{(6)}
\end{align}
Here, $\Omega_F$, $\Omega_B$, and $\Omega_I$ are the training sets for the PDE residual, boundary condition, and initial condition, respectively, with cardinalities $N_f$, $N_b$, and $N_0$. The weights $\lambda_F$, $\lambda_B$, and $\lambda_I$ are hyper-parameters tuning the contributions of each loss component. Notably, $\Omega_F$ may include points on the boundary or at the initial time, allowing $\Omega_F \cap \Omega_B$ and $\Omega_F \cap \Omega_I$ to be non-empty.

The choice of activation function $\sigma(\cdot)$ is crucial in PINNs, as it introduces the nonlinearity necessary to approximate complex solutions to PDEs. The hyperbolic tangent (tanh) is commonly used in PINNs for its smoothness and training stability~\cite{ref1, ref2}. Wavelet-based activations have also been explored to capture multi-scale features~\cite{ref40, RRef2, ref34}. Recent work by Li et al. \cite{XNet1} proposed the Cauchy activation function, which has shown strong performance in computer vision and time-series forecasting tasks \cite{XNet2}. 

In this study, we extend the application of the Cauchy activation function to address PDEs that are challenging for standard PINNs. The Cauchy activation function and the novel compleX-PINN model are introduced in the following section.

\section{Complex Physics-Informed Neural Network}
\label{Cauchy activation function}

The Cauchy activation function, introduced in \cite{XNet1}, is defined as:
\begin{align}
    \Phi(x; \mu_1, \mu_2, d) = \frac{\mu_1 x}{x^2 + d^2} + \frac{\mu_2}{x^2 + d^2}, \label{(12)}
\end{align}
where $\mu_1$, $\mu_2$, and $d$ are trainable parameters. This activation function is inspired by Cauchy's integral formula, as we further elaborate in Section~\ref{Sec 3.1}. We refer to a PINN model employing the Cauchy activation function as compleX-PINN.

We would like to note at the outset that our network is initially constructed with a single hidden layer, where each neuron has a unique set of parameters $\{\mu_1,\mu_2,d\}$. Consequently, the total number of trainable parameters for the Cauchy activation function is $3\times N_{\text{neuron}}$, where $N_{\text{neuron}}$ represents the width of the layer.


\subsection{1D Cauchy's integral formula and the Cauchy activation function}
\label{Sec 3.1}

This section introduces Cauchy's integral formula and derives the Cauchy activation function from it.

\begin{theorem}[Cauchy's Integral Formula]
\label{theorem 1}
Let $f$ be a complex-valued function on the complex plane. If $f$ is holomorphic inside and on a simple closed curve \( C \), and $z$ is a point inside \( C \), then:
\[
f(z) = \frac{1}{2\pi i} \oint_C \frac{f(\zeta)}{\zeta - z} \, d\zeta.
\]
\end{theorem}

Cauchy's integral formula expresses the value of a function at any point $z$ as a function of known values along a closed curve \( C \) that encloses \( z \). Remarkably, this principle is akin to machine learning, where the values at new points are inferred from the known values.

In practice, we approximate the integral using a Riemann sum over a finite number of points on the curve \( C \). Let \( \zeta^1, \zeta^2, \ldots, \zeta^m \) be a sequence of \( m \) points on \( C \). Then,
\begin{align} 
    f(z) \approx \frac{1}{2\pi i} \sum_{k=1}^{m} \frac{f(\zeta^{k})}{\zeta^{k} - z} \, (\zeta^{k+1} - \zeta^{k}) := \sum_{k=1}^m \frac{\lambda_k}{\zeta^k - z}, \label{equ9}
\end{align}
where, for convenience, we set \( \zeta^{m+1} = \zeta^1 \) and define \( \lambda_k = \frac{f(\zeta^k) \, (\zeta^{k+1} - \zeta^k)}{2\pi i} \).

If our target function \( f \) is real and one-dimensional, we obtain:
\begin{align}
    f(x) \approx \text{Re} \left( \sum_{k=1}^m \frac{\lambda_k}{\zeta^k - x} \right) = \sum_{k=1}^m \frac{\text{Re}(\lambda_k) \, \text{Re}(\zeta^k) + \text{Im}(\lambda_k) \, \text{Im}(\zeta^k) - \text{Re}(\lambda_k) x}{(x - \text{Re}(\zeta^k))^2 + (\text{Im}(\zeta^k))^2}. \label{(11)}
\end{align}

With the Cauchy activation function defined in~\eqref{(12)}, we have
\begin{align}
    f(x) \approx \sum_{k=1}^m \Phi \left( x - \text{Re}(\zeta^k); -\text{Re}(\lambda_k), \, \text{Re}(\lambda_k) \, \text{Re}(\zeta^k) + \text{Im}(\lambda_k) \, \text{Im}(\zeta^k), \, (\text{Im}(\zeta^k))^2 \right).
\end{align}
This shows that a one-layer neural network with the Cauchy activation function~\eqref{(12)} can approximate the real function \( f(x) \). 

At this point, we would like to highlight the power of the Cauchy approximation. For traditional activation functions such as ReLU or Sigmoid, the best one can hope for is a first-order approximation. However, with the Cauchy approximation, the error term is explicitly produced by approximating the contour integral with discrete values of the function. While we could have used higher-order Newton–Cotes formulas (such as Simpson’s rules or Boole’s rules) for numerical integration, they would yield {\em exactly the same} Cauchy approximation formula (\ref{equ9}). Naturally, the values for $\lambda_k$ would vary slightly with different integration schemes, but since $\lambda_k$ are training variables meant to best fit the input data, this variation is inconsequential. Ultimately, the order of approximation can be arbitrarily high. In fact, for any integer $p$, the error term is estimated to be of the order $o(n^{-p})$.

%The Cauchy activation function has many advantages over the traditional functions that PINN often uses as we will show. 

\subsection{Multi-dimensional Cauchy's integral formula}
\label{Sec 3.2}

This section extends Cauchy's integral formula to the multi-dimensional case. 

\begin{theorem}[Multi-Dimensional Cauchy's Integral Formula]
\label{theorem 2}
Let \( f(z) \) be holomorphic in a compact domain \( U \subset \mathbb{C}^N \) within \( N \)-dimensional complex space. For simplicity, assume that \( U \) has a product structure: \( U = U_1 \times U_2 \times \ldots \times U_N \), where each \( U_i \), \( i = 1, 2, \dots, N \), is a compact domain in the complex plane. Let \( P \) denote the surface defined by
\[
P = \partial U_1 \times \partial U_2 \times \ldots \times \partial U_N,
\]
then a multi-dimensional extension of Cauchy's integral formula for \( (z_1, z_2, \ldots, z_N) \in U \) is given by:
\[
f(z_1, z_2, \ldots, z_N) = \left(\frac{1}{2\pi i}\right)^N \int\cdots\int_P \frac{f(\zeta_1, \zeta_2, \ldots, \zeta_N)}{(\zeta_1 - z_1)(\zeta_2 - z_2) \cdots (\zeta_N - z_N)} \, d\zeta_1 \cdots d\zeta_N.
\]
\end{theorem}

Similarly, we approximate the integral by a Riemann sum over a finite number of points. More precisely, for any integer \( l = 1, \ldots, N \), let
\( \zeta_l^1, \zeta_l^2, \ldots, \zeta_l^{m_l} \) be a sequence of \( m_l \) points on \( \partial U_l \). Then,
\begin{align}
    f(z_1, z_2, \ldots, z_N) \approx \left(\frac{1}{2\pi i}\right)^N
    \sum_{k_1=1}^{m_1} \cdots \sum_{k_N=1}^{m_N}
    \frac{f(\zeta_1^{k_1}, \zeta_2^{k_2}, \ldots, \zeta_N^{k_N})}{(\zeta_1^{k_1} - z_1)(\zeta_2^{k_2} - z_2) \cdots (\zeta_N^{k_N} - z_N)}
    (\zeta_1^{k_1+1} - \zeta_1^{k_1}) \cdots (\zeta_N^{k_N+1} - \zeta_N^{k_N}),
\end{align}
where, for convenience, we set \( \zeta_l^{m_l+1} = \zeta_l^1 \) for \( l = 1, 2, \ldots, N \).

Collecting all terms that are independent of \( z_1, \ldots, z_N \), we define
\[
\lambda_{k_1, \ldots, k_N} = \left(\frac{1}{2\pi i}\right)^N f(\zeta_1^{k_1}, \zeta_2^{k_2}, \ldots, \zeta_N^{k_N}) (\zeta_1^{k_1+1} - \zeta_1^{k_1}) \cdots (\zeta_N^{k_N+1} - \zeta_N^{k_N}),
\]
so that we can rewrite the approximation as
\begin{align}
    f(z_1, z_2, \ldots, z_N) \approx \sum_{k_1=1}^{m_1} \cdots \sum_{k_N=1}^{m_N} \frac{\lambda_{k_1, \ldots, k_N}}{(\zeta_1^{k_1} - z_1)(\zeta_2^{k_2} - z_2) \cdots (\zeta_N^{k_N} - z_N)}.
\end{align}

Since the order of the sample points no longer matters, we can rewrite the sample points as a single sequence \( (\zeta_1^k, \ldots, \zeta_N^k) \) for \( k = 1, 2, \ldots, m \), where \( m = m_1 m_2 \cdots m_N \). Thus, we finally obtain
\begin{align}
    f(z_1, z_2, \ldots, z_N) \approx \sum_{k=1}^m \frac{\lambda_k}{(\zeta_1^k - z_1)(\zeta_2^k - z_2) \cdots (\zeta_N^k - z_N)}, \label{(8)}
\end{align}
where \( \lambda_1, \lambda_2, \ldots, \lambda_m \) are parameters that depend on the sample points \( (\zeta_1^k, \zeta_2^k, \ldots, \zeta_N^k) \) and the values \( f(\zeta_1^k, \zeta_2^k, \ldots, \zeta_N^k) \) for \( k = 1, 2, \ldots, m \).

%Cauchy's integral formula guarantees the accuracy of the above approximation if enough points on $P$ are taken. However, there is no reason that the sample points must be taken on any specific $P$ to achieve the best approximation. Indeed, our neural network will learn, from training data, where the best sample points are.
\subsection{Extend the Cauchy activation function to high dimensional space}
\label{Extend to high dimension}
The Cauchy approximation formula derived above can be computationally inefficient when the dimension N is high, due to the large number of multiplicative terms in the denominator. Therefore, for large N, we opt for a simplified representation of the function by applying the Cauchy activation function to linear combinations of the variables. Generally, this corresponds to a dual representation of the function, which is especially efficient for feature-finding in high-dimensional problems.  Specifically, we approximate the target function \( f(x_1, x_2, \dots, x_N) \) by
\begin{align}
    f(x_1, x_2, \dots, x_N) \approx \sum_{k=1}^m \Phi(W_{k1} x_1 + W_{k2} x_2 + \dots + W_{kN} x_N + b_k; \mu_{k1}, \mu_{k2}, d_k), \label{eq:multi_cauchy_activation}
\end{align}
where each \( \Phi \) is a Cauchy activation function as defined in Equation~\eqref{(12)}. Here, the parameters $W_{k1}, W_{k2}, \ldots, W_{kN}$, $b_k, \mu_{k1}, \mu_{k2}$, and \( d_k \) are trainable, allowing the network to capture the complex relationships among the input variables.

While the theoretical foundation of the approximation involves a single hidden layer, in practice, we can enhance the model’s capacity and expressiveness by constructing a neural network with multiple layers. This multilayer approach allows the model to capture hierarchical features and complex dependencies among the input variables, which are especially useful in high-dimensional cases.

Each hidden layer can be viewed as applying a set of nonlinear transformations to the input space, creating intermediate representations that capture interactions across multiple input variables. For a network with \( L \) hidden layers, we recursively define each layer's output as:
\begin{align}
    h^{(l)} = \Phi\left(W^{(l)} h^{(l-1)} + b^{(l)}; \mu^{(l)}, d^{(l)}\right),
\end{align}
where
\( h^{(l)} \) is the output of the \( l \)-th hidden layer,
\( W^{(l)} \) is the weight matrix of layer \( l \),
\( b^{(l)} \) is the bias vector,
\( \mu^{(l)} \) and \( d^{(l)} \) are the parameters of the Cauchy activation function for the \( l \)-th layer.

This recursive formulation allows the network to approximate increasingly complex functions as the number of layers grows, providing both depth and flexibility.

In this way, the network gains both practical advantages from multilayer structures and theoretical grounding from Cauchy's integral approach, providing a novel architecture that can effectively approximate high-dimensional functions.


%\begin{remark}
%The parameters $\{\mu_1,\mu_2,d\}$ are supposed to differ during each neuron's training.
%\end{remark}

%\begin{remark}
%Our network is initially constructed with a single hidden layer. The next section will show the efficiency and accuracy compared with the MLP. Moreover, we can add more hidden layers, with either Cauchy activation functions or traditional activation functions such as Tanh.
%\end{remark}



%\section{Experiments}
\section{Numerical Experiments}
\label{Sec4}

In this section, we compare compleX-PINN with traditional PINN~\cite{ref1}, Residual-Based Attention (RBA) PINN~\cite{RRef4}, and gradient-enhanced PINN (gPINN)~\cite{RRef5} across various partial differential equations (PDEs), including the wave equation (Section~\ref{Sec Wave}), diffusion-reaction system (Section~\ref{Sec Diffusion}), Helmholtz equation (Section~\ref{Sec Helm}), reaction equation (Section~\ref{Sec reaction}), and convection equation (Section~\ref{Sec Con-exp}).

CompleX-PINN is uniquely characterized by a single hidden layer, referred to as the \textbf{Cauchy layer}. Unlike other methods, each neuron in the Cauchy layer is parameterized by trainable parameters $\{\mu_1, \mu_2, d\}$, which provide greater flexibility in capturing complex solution patterns. Although each neuron introduces three additional parameters, the overall number of trainable parameters in compleX-PINN remains significantly smaller than multi-layer architectures used by competing methods, as a single hidden layer is sufficient for high performance. By default, the parameters $\{\mu_1, \mu_2, d\}$ are initialized to $0.1$, unless otherwise specified in particular experiments. 

For testing, a uniform grid of points is generated for each dimension. For example, a $300 \times 300$ grid is used for the 2D case, a $300 \times 300 \times 300$ grid for the 3D case, and this pattern is extended for higher-dimensional cases. Training points are then randomly sampled from these grids and are kept fixed across methods to ensure consistency. The number of training points is denoted as follows: $N_f$ for the residual, $N_b$ for the boundary conditions, and $N_0$ for the initial conditions. 

To guarantee a fair comparison, all experiments are conducted with identical settings, including the learning rate, optimizer, and number of training epochs. Furthermore, the same random seed is used across all methods to enhance reproducibility. However, experimental setups, such as the number of training points or specific network architectures, may vary between PDEs to accommodate their unique characteristics. 

Performance is evaluated using the relative $L^2$ error and the $L^{\infty}$ norm, which are defined as follows:
\begin{align}
    \text{(Relative) } L^2 \text{ error} & = \frac{\sqrt{\sum_{k=1}^N|\hat{u}(\mathbf{x}_k, t_k) - u(\mathbf{x}_k, t_k)|^2}}{\sqrt{\sum_{k=1}^N |u(\mathbf{x}_k, t_k)|^2}},
\label{L2error}\\
    L^{\infty} \text{ norm} & = \max_{1 \leq k \leq N} \left|\hat{u}(\mathbf{x}_k, t_k) - u(\mathbf{x}_k, t_k)\right|,\label{Linferror}
\end{align}
where $u$ is the true solution of the PDE, $\hat{u}$ is the output from the tested method, and $N$ is the number of testing points.


%The empirical results presented in this section demonstrate that compleX-PINN consistently outperforms traditional PINN~\cite{ref1}, Residual-Based Attention (RBA) PINN~\cite{RRef4}, and gradient-enhanced PINN (gPINN)~\cite{RRef5}. Specifically, compleX-PINN achieves superior predictions with fewer iterations across a diverse range of equations, including the wave equation (Section~\ref{Sec Wave}), diffusion-reaction system (Section~\ref{Sec Diffusion}), convection equations (Sections~\ref{Sec Con-exp} and \ref{Sec Con-sine}), and reaction equation (Section~\ref{Sec reaction}). Visualizations of the predictions, training histories, and error metrics are provided for each case to highlight the comparative performance. 

% Note that compleX-PINN is initially defined to have one single hidden layer (which we will sometimes call the Cauchy layer), where each neuron has the different trainable parameters $\{\mu_1,\mu_2, d\}$. By default, the parameters ${\mu_1, \mu_2, d}$ are initialized to $0.1$, unless otherwise specified in specific experiments.

% For each dimension, we uniformly generate 300 points, resulting in a $300 \times 300$ grid for 2D case, a $300 \times 300 \times 300$ grid for 3D case, and so on. Training points are then randomly sampled from the generated grid. We denote the number of training points for the residual as $N_f$, for the boundary conditions as $N_b$, and for the initial conditions as $N_0$. 

% To ensure a fair comparison across methods, each experiment uses the same random seed and identical setups, including the network architecture, learning rates, optimizer, training points, and number of training epochs. Note that the setups may vary between different experiments. We evaluate the performance using the relative $L^2$ error (referred to simply as $L^2$ error) and $L^{\infty}$ norm. These two metrics are defined as follows:
% \begin{align}
%     \text{$L^2$ error} & = \frac{\sqrt{\sum_{k=1}^N|\hat{u}(\mathbf{x}_k, t_k) - u(\mathbf{x}_k, t_k)|^2}}{\sqrt{\sum_{k=1}^N |u(\mathbf{x}_k, t_k)|^2}},
% \label{L2error}\\
%      L^{\infty}\text{ norm} &= \max_{1 \leq k \leq N} \left|\hat{u}(\mathbf{x}_k, t_k) - u(\mathbf{x}_k, t_k)\right|,\label{Linferror}
% \end{align}
% where $u$ is the true solution to the PDE and $\hat u$ is the output of different methods.

%{\color{blue}Before moving on the the specific PDEs, we would like to give a remark that, by the definition of the Cauchy activation function in Section~\ref{Cauchy activation function}, we only have one set of trainable parameters $\{\mu_1,\mu_2, d\}$ in each hidden layer. So the total number of the trainable parameters of compleX-PINN is not increased too much, compared with the traditional PINN. }

%\subsection{One hidden layer compleX-PINN is efficient and accurate}
%\label{single layer}

% The empirical results in this section show that compleX-PINN will receive a better prediction in fewer iterations compared with traditional PINN~\cite{ref1}, residual-based attention (RBA) PINN~\cite{RRef4}, and gradient-enhanced PINN (gPINN)~\cite{RRef5}. These comparisons are conducted on multiple types of equations: wave equation (Section~\ref{Sec Wave}), diffusion-reaction system (Section~\ref{Sec Diffusion}), the convection equation (Section~\ref{Sec Con-exp} and \ref{Sec Con-sine}) and reaction equation (Section~\ref{Sec reaction}).

\subsection{Wave equation}
\label{Sec Wave}
We consider the following 1D wave equation:
\begin{align}
    &u_{tt} = 4 u_{xx}, \quad &x \in [0,1],~t \in [0,1], \\
    &u(0,x) = \sin(\pi x) + \frac{1}{2}\sin(4\pi x), \quad &x \in [0,1],\\
    &u_t(0,x) = 0, \quad &x \in [0,1],\\
    &u(t,-1) = u(t,1) = 0, \quad &t \in [0,1],
\end{align}
whose exact solution is given by:
\begin{align}
    u(t,x) = \sin(\pi x)\cos(2\pi t) + \frac{1}{2}\sin(4\pi x)\cos(8\pi t).
\end{align}

The wave equation is known to be challenging for PINNs due to its stiffness and the resulting difficulty in capturing high-frequency components of the solution~\cite{ref19, ref21}. Traditional PINNs have slow convergence and suboptimal predictions. 
To address this challenge, the Residual-Based Attention (RBA)~\cite{RRef4} has been proposed to improve PINNs by dynamically adjusting the weights of training points based on their residuals. RBA adaptively prioritizes regions of the solution domain where the model struggles the most, making it particularly effective for stiff PDEs. The weight $\lambda_{F_i}$ in Eq.~\eqref{(3)} is updated iteratively using the following rule:
\begin{align}
\lambda_{F_i}^{k+1} = \gamma \lambda_{F_i}^{k} + \eta \frac{|r_i|}{\|r\|_{\infty}},
\end{align}
where $\lambda_{F_i}$ is the weight associated with the training point $(\mathbf{x}_i, t_i)$, $r_i$ is the residual $r_i = \mathcal{F}[u](\mathbf{x}_i, t_i)$, and $r$ is the residual vector. The hyperparameters $\gamma$ and $\eta$ control the contribution of the residual term and its smoothing, and are set as in the original RBA implementation ($\eta = 0.001$, $\gamma = 0.999$). For additional details, refer to~\cite{RRef4}.



In this experiment, we set \( N_f = 6000 \) and the boundary conditions and initial conditions are enforced by the hard constraint formulated as $\hat{u} = 20x(1-x)t^2\hat{u}_{NN} + \sin{(\pi x)} + 0.5\sin{(4\pi x)}$~\cite{ref24, lu2021physics, sukumar2022exact}, where $\hat{u}_{NN}$ represents the network output and $\hat{u}$ denotes the final prediction. For the standard PINN and RBA, we use a network with 5 hidden layers, each containing 300 neurons. For the compleX-PINN, we adopt a single hidden layer architecture with 1000 neurons. The number of parameters for PINN and RBA are $3\times 300+ 301\times 300\times 4+301\times 1=362,401$, while that for compleX-PINN is only $6\times 1000+1001\times 1=7,001$. Both models are trained using the Adam optimizer with a learning rate of \( 4 \times 10^{-5} \) for 100k iterations. %The results demonstrate that compleX-PINN outperforms traditional PINN even without RBA and achieves further improvements when enhanced with the RBA technique.


% The 1D wave equation above is known to be challenging for PINNs due to its stiffness~\cite{ref19, ref21}. 
% In this section, we demonstrate that compleX-PINN can accurately predict the solution without requiring such additional techniques, and its performance can be enhanced when combined with those additional techniques. 

% %Various strategies, such as adaptive learning rates~\cite{ref18, ref19, ref21, ref22, ref27, RRef5}, re-sampling of training residual points~\cite{ref23, ref24, ref25, ref26, RRef3}, and evolutionary training strategies~\cite{ref33,RRef3,ref38}, have been developed to address this issue. In this section, we demonstrate that compleX-PINN can accurately predict the solution without requiring such additional techniques, and its performance can be enhanced when combined with those additional techniques.

% We set \( N_f = 6000 \) and \( N_0 = N_b = 300 \). For the standard PINN, we employ a network with 5 hidden layers, each containing 300 neurons. For the compleX-PINN, we use a network with a single hidden layer of 1000 neurons. Both models are trained using the Adam optimizer with a learning rate of \( 4 \times 10^{-5} \) for 100k iterations. Additionally, we include the Residual-Based Attention (RBA)~\cite{RRef4}, a method that adaptively adjusts the weights of training points to improve the performance of PINN for stiff PDEs. When applied to compleX-PINN, this enhanced method is referred to as compleX-RBA.% Note the standard PINN has $3\times 500+501\times 500\times 4+501=1,004,001$ parameters while compleX-PINN has only $6\times 1000+1001=7,001$ parameters. 

% The RBA algorithm modifies the weight $\lambda_F$ in Eq.~\eqref{(3)} dynamically using:
% \begin{align}
% \lambda_{F_i}^{k+1} = \gamma \lambda_{F_i}^{k} + \eta \frac{|r_i|}{\|r\|_{\infty}},
% \end{align}
% where $\lambda_{F_i}$ is the weight for the training point $(\mathbf{x}_i, t_i)$, $r_i$ is the residual $r_i = \mathcal{F}[u](\mathbf{x}_i, t_i)$, and $r$ is the residual vector. The parameters $\gamma$ and $\eta$ are hyperparameters initialized as in the original RBA implementation ($\eta = 0.001$, $\gamma = 0.999$). For further details, refer to~\cite{RRef4}.

The results of these experiments are presented in Fig.~\ref{Wave1} and Fig.~\ref{wavehist}, with numerical results summarized in Table~\ref{Wave table}. As illustrated in Fig.~\ref{wavehist}, the standard PINN consistently exhibits a high relative $L^2$ error (more than 0.4) throughout training, highlighting its difficulty in solving stiff PDEs, as also reported in~\cite{ref19, ref21}. When the Residual-Based Attention (RBA) technique is applied to the standard PINN, the relative $L^2$ error decreases rapidly, reaching 0.02 at 20k iterations and increasing to 0.08 at 100k iterations, showcasing the effectiveness of RBA in addressing these challenges.

In comparison, compleX-PINN alone achieves significantly more accurate predictions, reducing the relative $L^2$ error rapidly to below $0.01$ within just 20k iterations. When further enhanced with RBA (referred to as compleX-RBA), the relative $L^2$ error reduces from $3.01\times 10^{-3}$ to $1.14\times 10^{-3}$ after 100k iterations, demonstrating the synergistic effectiveness of combining compleX-PINN with RBA for solving stiff wave equations.

% The results of these experiments are presented in Fig.~\ref{Wave1} and Fig.~\ref{wavehist}, with numerical results summarized in Table~\ref{Wave table}. 
% %where $\lambda_{F_i}$ represents the training point-specific weight for $(\mathbf{x}_i, t_i)$, $r_i$ is the residual $r_i = \mathcal{F}(\mathbf{x}_i; u(\mathbf{x}_i, t_i))$, and $r$ is the residual vector. Here, $\gamma$ and $\eta$ are hyperparameters defined initially. For a detailed description of RBA, refer to~\cite{RRef4}. In our experiment, we use the same hyperparameters as in the RBA authors’ code ($\eta = 0.01$ and $\gamma = 0.999$). The empirical results are presented in Fig.~\ref{Wave1} and Fig.~\ref{wavehist}, while the numerical results are summarized in Table~\ref{Wave table}.
% As shown in Fig.~\ref{Wave1} and Fig.~\ref{wavehist}, the standard PINN consistently exhibits relatively high $L^2$ error during training, confirming its difficulty in solving stiff PDEs, as reported in~\cite{ref19, ref21}. When RBA is applied to the standard PINN, the $L^2$ error decreases by approximately two orders of magnitude. 

\begin{figure}[!htb]
    %{\color{red}[Comment out the figure for fast process.]}
    \centering
    %\iffigure
    % First image on its line
    % \begin{minipage}{0.4\textwidth}
    % \centering
    % \includegraphics[width=0.98\linewidth, height=0.55\textwidth]{WaveExact.png}    
    % \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WavePINNPred.png}    
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WavePINNErr.png}
    \end{minipage}
    
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveRBAPred.png}    
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveRBAErr.png}
    \end{minipage}
    
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveXNet-Pred.png}    
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveXNet-Err.png}
    \end{minipage}
    
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveXNet-RBAPred.png}    
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{WaveXNet-RBAErr.png}
    \end{minipage}
    
    \vspace{1em} % Add some vertical space between the images

    \caption{Prediction and pointwise error for the 1D wave equation using four methods: standard PINN (first row), RBA-PINN (second row), compleX-PINN (third row), and compleX-RBA (last row). CompleX-PINN achieves an \( L^\infty \) error of \( 6.33 \times 10^{-3} \), which is further reduced to \( 2.03 \times 10^{-3} \) when the RBA technique is applied to compleX-PINN.}\label{Wave1}
\end{figure}

%In comparison, compleX-PINN alone achieves significantly more accurate predictions, reducing the $L^2$ error rapidly to around $1 \times 10^{-3}$ within 20k iterations. When further enhanced with RBA, referred to as compleX-RBA, it reduces the $L^2$ error by an additional one. orders of magnitude, demonstrating the combined effectiveness of compleX-PINN and RBA for solving stiff wave equations.
\begin{table}[!h]
    \caption{Final performance of different models for the 1D wave equation after 100k iterations.}
    \label{Wave table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model & Relative $L^2$ error & $L^{\infty}$ norm  \\ \hline
        PINN\cite{ref1} & $6.26\times 10^{-1}$ &  $7.56\times 10^{-1}$\\ \hline    
        RBA\cite{RRef4} & $8.57\times 10^{-2}$ &  $2.30\times 10^{-1}$  \\ \hline
        compleX-PINN & $3.01\times 10^{-3}$ &  $6.33\times 10^{-3}$  \\ \hline
        compleX-RBA & \bm{$1.14\times 10^{-3}$} &  \bm{$2.03\times 10^{-3}$}\\ \hline
    \end{tabular}

\end{table}

\begin{figure}[!htb]
 \centering
      \includegraphics[width=0.65\textwidth]{Wave-HIST-l2.png}    
      \vspace{1em}
     \caption{Relative $L^2$ error history of PINN, RBA, compleX-PINN, and compleX-RBA for the 1D Wave equation. compleX-PINN outperforms both the standard PINN and RBA, and its performance is further improved when combined with the RBA technique.}\label{wavehist}
\end{figure}

\subsection{Diffusion-reaction Equation}
\label{Sec Diffusion}
The diffusion-reaction equation, a parabolic PDE, models the macroscopic behavior of particles undergoing Brownian motion combined with chemical reactions. It finds applications in various fields, including information theory, materials science, and biophysics. In this section, we consider the following system, identical to the one presented in Ref.~\cite{RRef5}:
\begin{align}
&u_t = u_{xx} + R(x,t), \quad x \in [-\pi, \pi], \, t \in [0,1], \\
&u(\pi,t) = u(-\pi,t) = 0, \\
&u(x,0) = \sum_{n=1}^4 \frac{\sin(nx)}{n} + \frac{\sin(8x)}{8},
\end{align}
where \( R(x,t) \) represents the reaction term:
\begin{align}
R(x,t) = e^{-t} \left[\frac{3}{2} \sin(2x) + \frac{8}{3} \sin(3x) + \frac{15}{4} \sin(4x) + \frac{63}{8} \sin(8x)\right].
\end{align}
The analytical solution to this system is given by:
\begin{align}
u(x,t) = e^{-t} \left(\sum_{n=1}^4 \frac{\sin(nx)}{n} + \frac{\sin(8x)}{8}\right).
\end{align}

For this experiment, we set \( N_f = 500 \) and \( N_0 = N_b = 50 \). We compare compleX-PINN with RBA and Gradient-Enhanced PINN (gPINN)~\cite{RRef5}. The compleX-PINN uses a hidden layer with 1500 neurons, while RBA and gPINN employ a 4-layer neural network with 100 neurons per hidden layer. All models are trained using the Adam optimizer with a learning rate of \( 1 \times 10^{-5} \) for 15k iterations. 

The predictions and pointwise errors are illustrated in Fig.~\ref{Diffusion1}, while the relative \( L^2 \) error histories are shown in Fig.~\ref{diffusion HIST}. The results demonstrate that compleX-PINN converges much faster to a lower relative \( L^2 \) error compared to the other methods.

\begin{figure}[!htb]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diffusion-PINN-Error.png}
        \vspace{0.5em}
        \small PINN: $2.05\times 10^{-1} (L^\infty)$
    \end{minipage}
   % \hfill
    % Second row: 3 pictures
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diffusion-gPINN-Error.png}
        \vspace{0.5em}
        \small gPINN: $9.30\times 10^{-2} (L^\infty)$
    \end{minipage}\\
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diffusion-RBA-Error.png}
        \vspace{0.5em}
        \small RBA: $3.89\times 10^{-2} (L^\infty)$
    \end{minipage}%
  %  \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{diffusion-XNet-Error.png}
        \vspace{0.5em}
        \small compleX-PINN: ${\mathbf{6.01\times 10^{-3}}} (L^\infty)$
    \end{minipage}

     \caption{Pointwise error for the diffusion-reaction system using four methods: PINN, gPINN, RBA, and compleX-PINN. Among them, compleX-PINN exhibits the smallest $L^\infty$ error, while the errors of the other methods are more than six times larger.}
     \label{Diffusion1}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.55\textwidth]{diffusion-HIST-L2.png}    
    \caption{Relative \( L^2 \) error history of different models for the diffusion-reaction equation. CompleX-PINN achieves a significantly lower relative \( L^2 \) error compared to all other models, and does so in fewer iterations. Specifically, compleX-PINN reduces the error below \( 1 \times 10^{-2} \) within 5k iterations, whereas other models reach their lowest error of approximately \( 2 \times 10^{-2} \) only after 15k iterations.}\label{diffusion HIST}
\end{figure}

\subsection{2D Helmholtz equation}
\label{Sec Helm}
\input{Helmholtz}

\subsection{Reaction equation}
\label{Sec reaction}
The reaction problem is a hyperbolic PDE used to model chemical reactions, formulated with periodic boundary conditions as follows:
\begin{align}
    &u_t - \rho u (1-u) = 0,\quad x \in [0, 2\pi],\ t \in [0, 1], \label{reaction_eq} \\
    &u(0, t) = u(2\pi, t), \quad \ t \in [0, 1],\label{reaction_bc} \\
    &u(x, 0) = u_0(x), \quad x \in [0, 2\pi].\label{reaction_ic}
\end{align}
The system has an analytical solution in the form:
\begin{align}
    u(x,t) = \frac{u_0(x) e^{\rho t}}{u_0(x) e^{\rho t} + 1 - u_0(x)}. \label{reaction_exact}
\end{align}
We define the initial condition as:
\begin{align}
    u_0(x) = \exp\left(-\frac{(x-\pi)^2}{2(\pi/4)^2}\right). \label{reaction_initial}
\end{align}

We evaluate the performance of PINN, RBA, compleX-PINN, and compleX-RBA for $\rho \in \{5, 10, 15, 20\}$. We set $N_f = 6000$ and $N_0 = N_b = 150$. PINN is set to be a 5-layer network with 80 neurons in each hidden layer and compleX-PINN has 1200 neurons for the Cauchy layer. Training is conducted using the Adam optimizer with a learning rate of $5 \times 10^{-4}$ for 10k iterations. The relative $L^2$ error for all models are shown in Fig.~\ref{Reaction rho HIST} and Table~\ref{reaction rho table}. This figure illustrates that solving the PDE becomes increasingly challenging as the parameter $\rho$ grows larger. Notably, compleX-PINN consistently outperforms PINN and RBA, demonstrating its ability to handle higher stiffness in the reaction equation. Besides, compleX-RBA can reduce the relative $L^2$ errors to a lower value. This further demonstrates the potential of our model, suggesting that its performance can be enhanced by incorporating techniques originally proposed for PINNs, consistent with the conclusion in Section~\ref{Sec Wave}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.6\textwidth]{reaction-XNet-HIST-l2.png}    
    \vspace{1em}
    \caption{Relative $L^2$ error history of PINN, RBA, compleX-PINN, and compleX-RBA for the reaction equation with $\rho\in\{5,10,15,20\}$. In detail, PINN fails to train for large $\rho\in\{10,15,20\}$. Although RBA can make the relative $L^2$ errors to be below $1\times 10^{-1}$, compleX-PINN consistently outperforms them. The highest relative $L^2$ of compleX-PINN at 10k iteration for different $\rho$ values is about $1.52\times 10^{-2}$ and the lowest can archieve $7.04\times 10^{-3}$. Also, when we use the RBA technique to compleX-PINN, compleX-RBA can reduce the errors to lower values.}\label{Reaction rho HIST}
\end{figure}

\begin{table}[h!]
  \begin{center}
    \caption{Relative $L^2$ errors of the reaction equation for different models with $\rho \in \{5, 10, 15, 20\}$ after 10k iterations.}
    \label{reaction rho table}
    \begin{tabular}{|c|c|c|c|c|}\hline
      $\rho$ value & PINN & RBA & compleX-PINN & compleX-RBA\\ % <-- added & and content for each column
      \hline
      5 & $6.90\times10^{-2}$ & $3.31\times 10^{-2}$ & $7.04\times 10^{-3}$& \bm{$4.23\times 10^{-3}$}\\\hline  % <--
      10 & $9.64\times 10^{-1}$ & $3.40\times 10^{-2}$ & $1.38\times 10^{-2}$& \bm{$4.75\times 10^{-3}$}\\\hline  % <--
      15 & $9.65\times 10^{-1}$ & $4.15\times 10^{-2}$ & $1.52\times 10^{-2}$& \bm{$5.07\times 10^{-3}$}\\\hline 
      20 & $9.91\times 10^{-1}$ & $6.42\times 10^{-2}$ & $1.18\times 10^{-2}$ & \bm{$6.23\times 10^{-3}$}\\\hline% <--
    \end{tabular}
  \end{center}
\end{table}


\subsection{Convection equation}
\label{Sec Con-exp}

The convection equation~\cite{RRef1, RRef2, RRef3, ref38} describes the transport of a conserved scalar field by a velocity field over time. This equation is widely applied in fluid dynamics and other related fields. We consider the following equation:
\begin{align}
    &u_t + \beta u_x = 0, \quad (x,t) \in [0,2\pi] \times [0,1], \label{convection_eq} \\
    &u(0,t) = u(2\pi, t), \quad t \in [0,1],\\
    &u(x,0) = \sin(x), \quad x \in [0,2\pi], \label{convection_bc} 
\end{align}
with the corresponding exact solution:
\begin{align}
    u(x,t) = \sin(x - \beta t), \label{convection_sine_exact}
\end{align}
which oscillates periodically between \( -1 \) and \( 1 \) and \( \beta \) denotes the constant velocity parameter.

Previous studies have successfully trained PINNs for \( \beta = 50 \), including approaches such as importance sampling~\cite{RRef3}, transformer-based PINN~\cite{RRef2}, and sequence-to-sequence learning~\cite{RRef1}. A more recent work~\cite{ref38} extended this to \( \beta = 100 \), which improves the accuracy of PINNs for time-dependent problems by exactly enforcing temporal continuity between sequential time segments through a solution ansatz. The lowest relative $L^2$ error recorded in Ref.~\cite{ref38} for \( \beta = 50 \) and \( 100 \) is \( 3.75 \times 10^{-3} \) and \( 6.28 \times 10^{-3} \), respectively.

However, as \( \beta \) increases further, the complexity of the problem increases significantly, making it increasingly difficult for traditional PINNs to maintain accurate predictions. The aforementioned works do not extend to higher \( \beta > 100 \). To address this, we extend the research by testing compleX-PINN for higher values of \( \beta \), specifically for \( \beta \in \{50, 80, 90, 100, 120, 150\} \).

We use 2500 neurons for the Cauchy layer and set \( N_f = 50 \times \beta \) and \( N_0 = N_b = 100 \). The compleX-PINN is trained using the Adam optimizer with a learning rate of \( 5 \times 10^{-4} \) over 100k iterations. 

The relative \( L^2 \) error history is shown in Fig.~\ref{convection-sine-HIST}, and the final performance after training is summarized in Table~\ref{convection-sine-table}. The performance for $\beta = 50$ and $\beta = 100$ exceeds that reported in Ref.~\cite{ref38}. Furthermore, we extend the analysis to $\beta = 120$ and $\beta = 150$, both of which achieve relative $L^2$ errors below $1 \times 10^{-2}$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\textwidth]{convection-sine-one-layer-XNet-l2.png}    
    \vspace{1em}
    \caption{Relative \( L^2 \) error history of compleX-PINN for the convection equation with \( \beta \in \{50, 80, 90, 100, 120, 150\} \).}\label{convection-sine-HIST}
\end{figure}

\begin{table}[!h]
    \caption{Final relative \( L^2 \) error of compleX-PINN for the convection equation after 100k iterations with different $\beta$.}
    \label{convection-sine-table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
         & $\beta = 50$ & $\beta = 80$ & $\beta = 90$ & $\beta = 100$ & $\beta = 120$ & $\beta = 150$ \\ \hline
        Relative $L^2$ error&  $1.69\times 10^{-3}$ &  $3.69\times 10^{-3}$ & $3.59\times 10^{-3}$ & $4.82\times 10^{-3}$ & $7.54\times 10^{-3}$ & $7.66\times 10^{-3}$ \\ \hline
        
    \end{tabular}
    
\end{table}
%We consider two different types of initial conditions in the following. 

%{\color{red} Put the following into the corresponding subsections, and briefly introduce only. }
%Ref.~\cite{ref35} defines the initial condition as an exponential function, which we extend to a finite sum of exponential functions:
%\begin{align}
%    u_0(x) = \sum_{k=0}^K e^{-500 (x - p + k\delta)^2}, \label{convection_exp}
%\end{align}
%where \( K \), \( p \), and \( \delta \) are coefficients that govern the form of the initial condition.

%On the other hand, Refs.~\cite{RRef1, RRef2, RRef3, ref38} focus on the case where the initial condition is given by:
%\begin{align}
%    u_0(x) = \sin(x), \label{convection_sine}
%\end{align}
%and the corresponding exact solution is:
%\begin{align}
%    u(x,t) = \sin(x - \beta t),\label{convection_sine_exact}
%\end{align}
%which oscillates periodically between $-1$ and $1$. 

%This section evaluates the performance of compleX-PINN under two different scenarios. The case defined by Eq.~\eqref{convection_exp} is referred to as the "convection-exp equation."

%We will evaluates compleX-PINN under both scenarios to conduct a more comprehensive study in a separate Section~\ref{Sec Con-exp} and Section~\ref{Sec Con-sine}. The case in Eq.~\eqref{convection_exp} is referred to as the “convection-exp equation” in our paper, while the one in Eq.~\eqref{convection_sine} will be referred as “convection-sine equation”.

%\subsubsection{Convection-exp equation}
%\label{Sec Con-exp2}
%Ref.~\cite{ref35} defines the initial condition as a single exponential function for $[a, b] = [0,1]$, which we extend to a finite sum of exponential functions:
%\begin{align}
%    u_0(x) = \sum_{k=0}^K e^{-500 (x - p + k\delta)^2}, \label{convection_exp}
%\end{align}
%where \( K \), \( p \), and \( \delta \) are coefficients that determine the form of the initial condition. This case is referred to as the convection-exp equation.

%We consider two parameter sets: \( (p, \delta, K, \beta) = (0.75, 0.25, 5, 0.6) \) and \( (p, \delta, K, \beta) = (0.15, 0.6, 12, 1) \). The exact solutions corresponding to these configurations are shown in Fig.~\ref{convection-exp exact}. For both parameter sets, we use \( N_f = 3000 \) and \( N_0 = N_b = 100 \). A 5-layer PINN with 60 neurons in each hidden layer is employed, while for compleX-PINN, we use 800 neurons in the hidden layer. Both networks are trained using the Adam optimizer with a learning rate of \( 5 \times 10^{-4} \) over 100k iterations.

%A numerical summary for different metrics is provided in Table~\ref{convection table 1}, and performance visualizations are illustrated in Fig.~\ref{convection1}. PINN fails to provide solid predictions in both cases, even when the oscillations are lower ($K = 5$). Additionally, its predictions are only rough for certain parts of the domains. In contrast, X-PINN achieves significantly more accurate predictions across the entire domain, with maximum absolute errors of approximately $5.06 \times 10^{-3}$ and $4.39 \times 10^{-3}$, respectively.


% \my{Can you display the trained $\mu$ and $d$? I want to see what they are.}
%{\color{red}You need to mention the result at least use 1-2 sentences. In addition, it seems that PINN performs better for the case $K=12$ if you look at the lower right part. Any explanation for it? }


%\begin{figure}[!htb]
    %{\color{red}[Comment out the figure for fast process.]}
%    \centering
    % % Second and third images on the same line
%    \begin{minipage}{0.45\textwidth}
%    \centering
%    \includegraphics[width=0.98\linewidth, height=0.65\textwidth]{convectionExact.png}    
%    \end{minipage}
%    \begin{minipage}{0.45\textwidth}
 %   \centering
%    \includegraphics[width=0.98\linewidth, height=0.65\textwidth]{convectionExact-2.png}
%    \end{minipage}
    %\vspace{1em} % Add some vertical space between the images
%   \caption{The exact solutions for the convection-exp equation with two different parameter settings: \( (p, \delta, K) = (0.75, 0.25, 5) \) (left) and \( (p, \delta, K) = (0.15, 0.6, 12) \) (right).}\label{convection-exp exact}
%\end{figure}

%\begin{table}[h]
%   \caption{Summary of the relative \( L^2 \) error and the \( L^\infty \) norm for PINN and compleX-PINN applied to the convection-exp equation with two different parameter settings.}
%    \label{convection table 1}
%    \centering
%    \begin{tabular}{|c|c|c|c|}
%        \hline
%        Convection-exp equation & Model & Relative $L^2$ error& $L^{\infty}$ norm \\ \hline
%        \multirow{2}{*}{$(p, \delta, K) = (0.75, 0.25, 5)$} & PINN & $5.52\times 10^{-1}$  & $7.37\times 10^{-1}$ \\ \cline{2-4}
%                                & compleX-PINN & $2.12\times 10^{-3}$ & $5.06\times 10^{-3}$\\ \hline
%        \multirow{2}{*}{$(p, \delta, K) = (0.15, 0.6, 12)$} & PINN & $4.10\times10^{-1}$ &  $6.06\times 10^{-1}$ \\ \cline{2-4}
 %                               & compleX-PINN & $2.10\times 10^{-3}$ &  $4.39\times 10^{-3}$ \\ \hline
%    \end{tabular}
%\end{table}


%\begin{figure}[!htb]
    %{\color{red}[Comment out the figure for fast process.]}
%    \centering
    %\iffigure
    % % Second and third images on the same line
%    \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{convectionPINNPred.png}    
%    \end{minipage}
%    \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{convectionPINNErr.png}
 %   \end{minipage}
 %   \vspace{1em} % Add some vertical space between the images
    
    % Fourth and fifth images on the same line
 %   \begin{minipage}{0.4\textwidth}
%    \centering
%   \includegraphics[width=\linewidth]{convectionXNetPred.png}  
%    \end{minipage}
%    \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{ConvectionXNetErr.png}    
 %   \end{minipage}
    
%    \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{convectionPINNPred-2.png}    
 %   \end{minipage}
 %   \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{convectionPINNErr-2.png}
%    \end{minipage}
%    \vspace{1em} % Add some vertical space between the images
    
    % Fourth and fifth images on the same line
%    \begin{minipage}{0.4\textwidth}
%    \centering
%    \includegraphics[width=\linewidth]{convectionXNetPred-2.png}  
 %   \end{minipage}
 %   \begin{minipage}{0.4\textwidth}
 %   \centering
%    \includegraphics[width=\linewidth]{convectionXNetErr-2.png}    
%    \end{minipage}

    %\else
    %\fi
%     \caption{Predictions and point-wise errors for the convection-exp equation at different parameter settings. The first and second rows display the predictions and point-wise errors for $K = 5$ of PINN (first row) and compleX-PINN (second row), respectively. The third and fourth rows show the predictions and point-wise errors for $K = 12$ of PINN (third row) and compleX-PINN (fourth row), respectively. PINN struggles to make accurate predictions when the oscillations are relatively low, as shown in the case $K = 5$. In contrast, compleX-PINN can reliably predict the convection equation even when $K = 12$, where the solutions change more frequently.}\label{convection1}
%\end{figure}

%\subsection{Multi-layer compleX-PINN could be better}
%\label{more layers}
%Even the theory for compleX-PINN is built on a single-layer network, as Section~\ref{Extend to high dimension} announces, we can improve the model's capacity and expressiveness by using a multilayer neural network, enabling it to capture hierarchical features and complex dependencies among input variables.

%In the following subsections, we are testing the deep compleX-PINN, which contains more hidden layers. In addition, we aim to incorporate all the added hidden layers with tanh activation functions (in Section~\ref{Sec Con-sine}) as well as with Cauchy activation functions (in Section~\ref{Sec reaction}), which we will denote as Cauchy layer in this section.

%\subsubsection{Convection-sine equation}
%\label{Sec Con-sine}

%\begin{table}[!h]
%    \centering
%    \begin{tabular}{|c|c|c|c|c|c|}
%        \hline
%        Model &  $\beta = 80$ & $\beta = 90$ & $\beta = 100$ & $\beta = 120$ & $\beta = 150$ \\ \hline
%        Single-layer compleX-PINN &  $9.91\times 10^{-5}$ & $8.94\times 10^{-5}$ & $4.49\times 10^{-4}$ & $4.29\times 10^{-4}$ & $6.36\times 10^{-4}$ \\ \hline
%        Multi-layer compleX-PINN (Tanh) & $5.61\times 10^{-5}$ & $5.29\times 10^{-5}$ & \bm{$4.98\times 10^{-5}$} & $2.58\times 10^{-4}$ & $2.27\times 10^{-4}$ \\ \hline
 %       Multi-layer compleX-PINN (Cauchy) & \bm{$5.51\times 10^{-5}$} & \bm{$3.17\times 10^{-5}$} & $9.20\times 10^{-5}$ & \bm{$9.12\times 10^{-5}$} & \bm{$1.40\times 10^{-4}$} \\ \hline
%    \end{tabular}
%\caption{MSE for $\beta \in \{80, 90, 100, 120, 150\}$ chosen for the convection-sine equation solved by compleX-PINN.}
%\label{convection-sine-table}
%\end{table}

%Furthermore, we fix $\beta = 150$ and vary the number of hidden layers we add. We tested the cases for adding $2$, $3$, and $4$ layers into compleX-PINN. The history of MSE can be seen in Fig.~\ref{convection-sine-layer}. Increasing the network depth improves the compleX-PINN's performance after 100k training iterations. These results show that our model is not restricted to a single-layer structure and can benefit from additional hidden layers.

%\begin{figure}[!htb]
% \centering
%      \includegraphics[width=0.45\textwidth]{convection-sineHIST-layer.png}    
%      \vspace{1em}
 %    \caption{MSE history of PINN, RBA, compleX-PINN, and the compleX-RBA for the convection-sine equation for $\beta = 150$. We change the network structure into 3-layer, 4-layer, and 5-layer compleX-PINN. The Tanh layer stands for the hidden layers with Tanh activation functions, while 
 %The Cauchy layer stands for the Cauchy activation function.}\label{convection-sine-layer}
%\end{figure}

\section{Conclusion}
In this paper, we introduce compleX-PINN, which is a single-layer network that utilizes the Cauchy integral formula. Complementing our theoretical advances, the empirical results show that compleX-PINN can achieve a much lower relative $L^2$ error with fewer iterations. Also, the number of parameters in compleX-PINN is much smaller than in the traditional PINN. This enhanced efficiency and accuracy validate the efficacy of our theoretical contributions and highlight the practical advantages of compleX-PINN.

\section*{Acknowledgments}
This work was partially supported by the National Natural Science Foundation of China (72495131, 82441027), Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001), Shenzhen Stability Science Program, and the Shenzhen Science and Technology Program under grant no. ZDSYS20211021111415025.


\bibliographystyle{unsrt}  
\bibliography{ref}
\end{document}