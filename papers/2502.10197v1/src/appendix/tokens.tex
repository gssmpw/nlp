\subsection{Analysis of Output Tokens} \label{app:tokens}

\begin{figure}[!hbt]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tokens_pct_overall.pdf}
        \vspace{-6mm}
        \caption{Distribution of the number of output tokens.}
        \label{fig:tokens_overall}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tokens_pct_correct.pdf}
        \vspace{-6mm}
        \caption{Correctness per number of output tokens.}
        \label{fig:tokens_avg}
    \end{minipage}
    \label{fig:tokens}
\end{figure}

Reasoning models typically produce large amount of output tokens to arrive at the final solution.
In this analysis, we take the three best performing models from our experiments (\textsc{o1}, \geminithink, and \textsc{o1-mini}), and compare the number of output tokens that they produce when evaluating on our benchmark, as well as their accuracy for each amount of tokens.

First, in~\cref{fig:tokens_overall}, we show the distribution of the number of output tokens for all models.
We can observe that \geminithink{} produces the largest amount of output tokens, even going over $100$k tokens for some problems.
Overall, \oone{} generally produces less tokens and has higher accuracy.

When looking at the correctness for each number of output tokens, as shown in~\cref{fig:tokens_avg}, we can observe that for \geminithink{} and \textsc{o1-mini} the accuracy steadily decreases with larger number of output tokens. However, \geminithink{} still produces some percentage of correct solutions even for $50$k tokens. 
Interestingly, we can observe that \oone{} has 40\% accuracy with around $40$k output tokens. Recall that models receive feedback from the parser when their solution is not formatted correctly, and they have a chance to fix it.
This bump in accuracy comes from the samples where \oone{} was able to correct its solution after the initial round of feedback.
