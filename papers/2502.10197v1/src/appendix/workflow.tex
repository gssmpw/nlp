\section{Dataset Development Workflow}\label{app:workflow}

Here we describe our workflow for developing the \mc benchmark, elaborating on the details presented in the main text (\cref{sec:benchmark}). 

\paragraph{Problem selection}
The first part of our workflow described in~\cref{sec:benchmark:selection} is the selection of suitable problems from existing competitions.
We assigned particular competitions (possibly splitting by year) to different members of our team, and each member selected problems from the assigned competitions according to the criteria described in~\cref{sec:benchmark:selection}.
Overall we examined around $3500$ problems from $20$ different competitions. However, many of these problems were immediately discarded as they do not contain a constructive component (\eg almost all geometry problems). Overall, this phase yielded at least $158$ problems.
Note that sometimes this step involved rephrasing the problem statement to make it more suitable for our benchmark.

\paragraph{Problem encoding}
The second part of our workflow relates to formalizing the problems and encoding them into a format that can be used for evaluation, as discussed in~\cref{sec:benchmark:encoding}.
Each problem is encoded in a single Python file, into an object of type \texttt{Problem}.
The first part of the object is a configuration that contains metadata about the problem such as the problem statement, formatting instructions for presenting the solution, the parameters, the source of the problem, parameters of the original version of the problem, and the original solution.
Additionally, the configuration contains a list of tags that describe the type of the problem and its relationship to the original version (is it simplified, generalized, etc.).
In~\cref{fig:problem:config} we show an example of this configuration for the problem from the IMC 2012 competition mentioned earlier in the paper in~\cref{sec:benchmark:encoding}.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
config = ProblemConfig(
    name=Problem.get_name(__file__),
    statement=PROBLEM_TEMPLATE,
    formatting_instructions=get_matrix_template(),
    parameters=["n"],
    source="IMC 2012 P2",
    original_parameters={"n": 7},
    original_solution=get_solution(7),
    problem_url="imc-math.org.uk/imc2012/IMC2012-day1-questions.pdf",
    solution_url="imc-math.org.uk/imc2012/IMC2012-day1-solutions.pdf",
    tags=[Tag.ALGEBRA, Tag.FIND_MAX_MIN, Tag.IS_SIMPLIFIED],
)
\end{lstlisting}
\caption{Problem configuration for the problem from the IMC 2012 competition}
\label{fig:problem:config}
\end{figure}

The second important component of the problem is the encoding of the verification function $V_\theta$ as a Python function that receives a proposed solution and checks whether it satisfies the constraints of the problem. In \cref{fig:problem:checker}, we show an example of this function from the same problem. The function returns a tuple of: a boolean value indicating whether the solution is correct, a feedback string explaining why the solution is incorrect, and a tag indicating the type of the error.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
def check(self, x: list[list[float]]):
    if len(x) != self.n:
        return False, f"The number of rows is not n ({self.n}).", 
               CheckerTag.INCORRECT_LENGTH
    if any(len(row) != self.n for row in x):
        return False, f"The number of columns is not n ({self.n}).", 
               CheckerTag.INCORRECT_LENGTH
    if any(x[i][i] != 0 for i in range(self.n)):
        return False, f"Some diagonal elements are not zero.",
               CheckerTag.INCORRECT_SOLUTION
    for i in range(self.n):
        for j in range(self.n):
            if x[i][j] <= 0: 
                return False, 
                    f"Some off-diagonal elements are not positive.",
                    CheckerTag.INCORRECT_SOLUTION
    rank = np.linalg.matrix_rank(x)
    if rank > 3:
        return False, f"The rank is {rank}, which is greater than 3.",
               CheckerTag.INCORRECT_SOLUTION
    return True, "OK", CheckerTag.CORRECT
\end{lstlisting}
\vspace{-3mm}
\caption{Implementation of the verification function for the problem from the IMC 2012 competition}
\label{fig:problem:checker}
\end{figure}

Finally, the problem contains a generation function that generates a set of variations of the problem by plugging in different values for the parameters $\theta$. We show an implementation of this function for the same problem below in~\cref{fig:problem:generation}. For this problem, the generation function simply returns an integer sampled uniformly at random from a given interval.

\begin{figure}[h]
\begin{lstlisting}[language=Python]
def generate() -> "Problem_IMC_2012_2":
    n = random.randint(6, 20)
    return Problem_IMC_2012_2(n)
\end{lstlisting}
\vspace{-3mm}

\caption{Implementation of the variation generator for the problem from the IMC 2012 competition}
\vspace{-3mm}
\label{fig:problem:generation}
\end{figure}

Once the problem is fully encoded, we also include a variety of unit tests.
These unit tests typically ensure that our official solution to the problem is correct (namely, that it satisfies the verification function).
Ideally, the unit tests also test that wrong solutions are rejected by the verification function, but it is generally hard to cover all possible wrong solutions.

\paragraph{Problem review}

The final step of the workflow is reviewing the problems, as described in~\cref{sec:benchmark:review}.
The key part of this workflow is our application for data analysis, which is shown in~\cref{fig:data:analysis}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/app_screenshot.png}
    \caption{Screenshot of our web application for data analysis}
    \label{fig:data:analysis}
\end{figure}

The application shows for each problem and for each variation the following information: problem statement, formatting instructions, our ground truth solution, and interaction with the model. The interaction with the model shows responses from the model, feedback from our parser (checking whether solution can be extracted from the model response, see \cref{app:parser}), and the final result of the verification function (correct or incorrect).
We developed the application using FastHTML~\cite{fasthtml}.

The application generally allows us to quickly review the problems and to identify problems that are not suitable for our benchmark.
For example, by investigating responses from the model, we identified issues with our problem statement or formatting instructions.
Additionally, an important part of the review was identifying issues with our parsing of the LLM solutions in cases where it was correct, but provided in a format that our parser was not able to understand.
