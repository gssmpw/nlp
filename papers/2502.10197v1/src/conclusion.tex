\section{Conclusion} \label{sec:conclusion}

In this work, we introduced \mc, a novel benchmark designed to evaluate the mathematical reasoning capabilities of LLMs through constructive proofs.
Unlike existing benchmarks, \mc uniquely combines the intrinsic challenge of constructing valid mathematical objects with the ease of their correctness verification, creating a challenging set of tasks for LLM evaluation.
Starting from $126$ carefully curated problems, we generated $475$ problems instances by systematically varying key parameters in the original problem statements.
Our extensive evaluation of $14$ leading LLMs on these tasks revealed that even the most advanced models struggle significantly with these tasks.
Overall, we believe that by focusing on construction problems, \mc\ pushes the boundaries of mathematical reasoning benchmarks, offering a valuable resource for driving future advancements in the field.
