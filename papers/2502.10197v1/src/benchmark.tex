\section{\mc} \label{sec:benchmark}

Constructive proofs are a powerful tool for mathematicians, but turning these into a benchmark for evaluating the reasoning abilities of LLMs requires significant effort. 
In this section, we describe our approach for the creation of a reliable benchmark of difficult constructive proof problems, detailing the key steps: problem selection (\cref{sec:benchmark:selection}), problem encoding (\cref{sec:benchmark:encoding}), and post-hoc problem review (\cref{sec:benchmark:review}). Further details on the development process are given in~\cref{app:workflow}.

\input{figures/problem_encoding}


\subsection{Problem Selection} \label{sec:benchmark:selection}
As a first step in the creation of \mc, our team consisting of students with significant experience with math competitions preselected a preliminary set of constructive proof problems.
To ensure quality, the problems were exclusively sourced from reputable high-level mathematics competitions, including high-school olympiads, undergraduate contests, and national high-school competitions.
In total, our team read around $3500$ problems from archives of $20$ different competitions, selecting $158$ possible problems that met our relevance and quality criteria.

\paragraph{Problem selection criteria} Our criteria were as follows:
\vspace{-3mm}
\begin{itemize}\setlength\itemsep{0.02em}
    \item \emph{Difficult construction}: 
    For \mc{} to be future-proof and pose a challenge to current and future models, obtaining the required construction should involve non-trivial reasoning, and constitute the majority of the official solution to the competition problem.
    \item \emph{Complex objects}: To further ensure difficulty and reduce the probability of lucky guessing the answer, the required object should be non-trivial, \ie the space of possible constructions should be large. In particular, we generally avoid problems where the result of the construction is a single integer.
    \item \emph{Tractable verification}: On the other hand, the verification function must be straightforward to implement relative to the problem difficulty, and feasible to run. 
    \item \emph{Variations}: To distinguish genuine reasoning from memorization or lucky guessing, we prioritize problems that allow for multiple variations by altering parameters. In~\cref{sec:eval} we introduce \emph{robust accuracy}, requiring models to solve multiple variations to be considered successful on a given problem. Since a human who knows the general solution can solve each variation trivially, this criterion ensures that the model has a similar level of understanding. 
\end{itemize}

\paragraph{Adapting different problem types}
The most common problems in \mc{} are \emph{Any}-problems, which ask for any object satisfying the given constraints.
However, several other problem types can be adapted to fit our criteria.
For example, \emph{Inf}-problems originally ask for a proof that there are infinitely many objects that satisfy the given constraints---these often require coming up with an infinite solution class, whose knowledge can be demonstrated by producing $k$ fitting constructions for $k \to \infty$. 
Similar reasoning holds for \emph{All}-problems, which ask for all objects satisfying some constraints.
Finally, \emph{Max}-problems originally ask for the maximum (or minimum) value of a function of an object. Often, these are solved by first proving a lower (resp. upper) bound and then constructing an object that achieves this bound.
If the second part is sufficiently difficult, these problems are good candidates for \mc{}. 
Importantly, when adapting problems to become constructive proofs, we confirm that the quality and difficulty criteria above hold, \emph{even after the necessary problem modifications}.

\subsection{Math Construction Problems} \label{sec:benchmark:encoding}

We now discuss the formalization of construction problems and their encoding into a format that can be used for programmatic evaluation.

\begin{definition}[Construction Problem]
    A construction problem is a tuple $(P, \theta, V_\theta)$ where $P$ is a symbolic problem statement in natural language, $\theta$ are concrete parameters that replace symbolic variables in the problem statement, and $V_\theta\colon \mathcal{O} \to \{0, 1\}$ is a verification function that takes an object and checks whether it satisfies the constraints of the problem.
    A valid solution or a \emph{constructive proof} for this problem is any object $o \in \mathcal{O}$ such that $V_\theta(o) = 1$.
\end{definition}

Each problem is encoded as a single Python file, containing all of its components $P, \theta,$ and $V_\theta$. Additionally, the file includes a variation generator function which generates a set of variations of the problem by plugging in different values for the parameters $\theta$, and formatting instructions that instruct the model to output the solution in a specific format.

An example of an encoded \mc{} problem is given in~\cref{fig:problem_encoding}.
Here, the problem statement is $P=$\emph{``Find an $n \times n$ matrix\ldots''}, the parameters are $\theta = \{n\colon 10\}$, and the verification function is given by
\begin{equation*}
V_\theta(M) = \text{rank}(M) \leq 3 \land M_{i,i} = 0 \land (i = j \lor M_{i,j} > 0).
\end{equation*}
When solving the problem, the model is given a concretized problem statement obtained by replacing the parameters in the problem statement $P$ with concrete parameter values $\theta$. 
Similar to human contestants, the model does not have access to the verification function $V_\theta$.

We now further discuss each of the problem components.

\paragraph{Symbolic problem statement}

While many problems are originally stated with concrete values, they can often be generalized to obtain a \emph{symbolic problem statement} $P$. We do this for every problem where it is possible, replacing concrete values with symbolic parameters $\theta$, allowing us to plug in different values for the parameter. For example, any positive integer can replace parameter $n$ in the problem statement in~\cref{fig:problem_encoding}.

\paragraph{Verification}
In our implementation, the verification function $V_\theta$ takes a construction and returns a tuple $(c, f)$ where $c$ is a boolean indicating whether the construction satisfies the constraints of the problem, and $f$ is a detailed feedback string that when $c=\mathrm{false}$ explains why the construction is incorrect.
For example, in the problem in~\cref{fig:problem_encoding}, the verification function checks that the matrix has the correct dimensions, the diagonal is zero, the off-diagonal elements are positive, and the rank is at most 3.
We use the feedback strings during our review process for manual quality checks (see \cref{sec:benchmark:review}). 
Following the criteria from~\cref{sec:benchmark:selection}, we see that despite the significant difficulty of the problem in~\cref{fig:problem_encoding}, the verification function is simple to implement and run, and the feedback is straightforward to understand.

\paragraph{Variations}

Given a problem, humans can typically find a general solution that works for all parameter values.
For example, to solve the problem in~\cref{fig:problem_encoding} (and the majority of our problems), one can find a general procedure that works for any $n$.
Most popular benchmarks~\cite{hendrycks2021math, omnimath, olympiadbench} do not have symbolic variations, meaning it is difficult to evaluate whether the model can generalize to similar problems or simply guessed the answer.
Instead, we create a number of variations for each problem, allowing us to study the robustness of the model to changes in the problem parameters.

\begin{definition}[Problem Variations]
    Problem variations are a set of construction problems that share the same symbolic problem statement $P$ and verification function $V_\theta$, but have different problem parameters $\theta_1, \theta_2, \ldots, \theta_k$.
\end{definition}


\paragraph{Formatting and parsing}
Since the mathematical objects involved in constructive proofs have a wide variety of types, we also provide 
formatting instructions for each problem which can be used to prompt the model to output the solution in a specific format.
Furthermore, we create a specialized parser, detailed in~\cref{app:parser}, that can parse arbitrary combinations of lists, matrices, and \LaTeX{} objects from natural language solutions. 
By default, models evaluated on our benchmark receive detailed feedback from our parser, allowing them to correct their solutions over multiple rounds, which reduces the risk of syntax errors.

\subsection{Problem Review} \label{sec:benchmark:review}    
We performed a review of \mc{} problems, in each stage discarding problems that did not meet our quality criteria and revising those that could be improved.

\paragraph{Manual quality checks}
First, each problem author was tasked with implementing a generic solution function that computes valid constructions for the problem. Additionally, extensive unit tests were implemented to check each problem's verification and solution functions, ensuring correct implementation.
Next, each author was asked to ensure that their problem is solvable by a human using pen and paper, to ensure that we are testing reasoning, and not merely calculation skills.
We remark that some of our problems may include more calculation than typical in human competitions (\eg writing the complete $n \times n$ matrix instead of simply describing it)---however, this step is trivial once the correct insight for the problem is found.
Finally, in a peer review process, each problem was reviewed by at least one other team member, checking that the problem statement is sound and clear, that the verification function is feasible and returns informative feedback in case of errors, and that the problem is sufficiently challenging and of high quality.

\paragraph{Automated quality checks}
Complementing manual review, we also implemented automated checks.
First, we verified that all problems are solvable by an LLM when explicitly given the solution.
This verifies that the formatting instructions are unambiguous and that our parser and verifier are error-free. 
Second, we flagged for additional review all problems where our solution is longer than $4000$ characters, as LLMs should not be significantly hampered by the difficulty of outputting large amounts of text.
Finally, we implemented a code agent that flags problems that are solvable using a brute-force approach, as we want to ensure that each problem is only solvable via genuine reasoning.

\paragraph{Final set of problems}
Most issues above were resolved by revising the problem statement or the set of variations. Discarding the $32$ problems that had unresolvable issues, we arrived at a final set of $126$ problems in \mc{}---a detailed overview of our sources and the number of problems per source is given in~\cref{app:benchmark_details}.

\input{tables/bench_cattype_summary}
In~\cref{tab:bench_cattype_summary} we summarize \mc{} across different problem categories (\eg Combinatorics) and types, as introduced above (\eg \emph{Any}-problems). 
We see that the problems are well-distributed across categories, and that around half of the problems are \emph{Any}-problems, the type that most closely illustrates constructive proofs.
The other half was obtained by adapting other problem types to fit our criteria.
