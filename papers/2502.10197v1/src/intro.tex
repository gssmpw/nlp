\vspace{-4mm}
\section{Introduction} \label{sec:intro}
Evaluating the mathematical reasoning abilities of Large Language Models (LLMs) requires high-quality public benchmarks that accurately measure progress. 
As shown in~\cref{fig:lead_figure}, existing benchmarks, such as MATH~\citep{hendrycks2021math}, are becoming increasingly saturated as state-of-the-art models improve, highlighting the need for more challenging evaluation tasks. 
Many complex mathematical problems involve \textit{proofs}, which are a fundamental component of advanced reasoning. However, current benchmarks primarily focus on problems where LLM outputs can be directly compared to ground truth answers, making them unsuitable for evaluating proofs. A promising alternative, formalized proof generation, requires LLMs to generate proofs that can be verified by automated theorem provers such as Lean~\citep{lean}. Unfortunately, even models explicitly fine-tuned for this task struggle to perform well~\citep{xin2024deepseekprover}. Furthermore, this approach does not fully leverage the strong natural language reasoning capabilities of LLMs. This raises an important question: Is there a class of proof-based problems that are both challenging for LLMs and easy to verify for correctness?

\input{figures/lead_figure}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/intro_figure.pdf}
    \caption{Two sample problems from \mc, each consisting of a  natural language statement and a verifier function that returns a boolean value indicating the validity of a proposed construction. The ability to easily generate problem variations (\emph{values colored in blue and brown}), the complexity of the required constructions, and the difficulty of the problems make \mc well-suited for evaluating LLMs' reasoning abilities.}
    \label{fig:intro}
\end{figure*}


\paragraph{Constructive proofs}
One important class of proofs, commonly appearing in real-world applications and advanced mathematical competitions, involves \textit{constructive proofs}. These proofs establish a mathematical result by explicitly constructing an object---such as a set, matrix, or graph---that satisfies specific constraints.

For instance, one of the most challenging problems from the 2022 International Mathematical Olympiad (IMO), shown in~\cref{fig:intro} (left), requires constructing an $n \times n$ matrix that maximizes a particular quantity. More generally, disproving a conjecture often involves constructing a counterexample, as seen in Cantor's diagonal argument~\citep{cantor1890ueber}. Similarly, proving a bound frequently requires constructing an object that achieves that bound, as in the proof of the Four Color Theorem~\citep{appel1989fourcolor}.

Constructive proofs are particularly well-suited for LLM benchmarking as coming up with valid constructions is often difficult for humans, and thus likely to also challenge the models.
Yet, verifying if a proposed construction satisfies the problem constraints is usually straightforward, enabling the use of automated verifiers to judge model responses.

\paragraph{Our benchmark: \mc}
Leveraging this, we introduce \mc, a new benchmark designed to evaluate LLMs' reasoning capabilities through constructive proofs. \mc consists of $126$ unique problems sourced from olympiad-level mathematics competitions. Each problem is encoded as a natural language statement and a corresponding verifier function that determines the correctness of a proposed construction (see~\cref{sec:benchmark} for details). 
In \cref{fig:intro}, we illustrate two sample problems from \mc.

Beyond its challenging nature and ease of verification, \mc has several features that make it particularly valuable for LLM evaluation. 
Most importantly, all problem statements are phrased symbolically, enabling systematic generation of \emph{variations} that test models' robustness to small changes in problem parameters. 
Second, the required constructions often involve complex mathematical objects (\eg matrices, colorings) rather than simple numerical answers, making guesswork and memorization less effective. 
Finally, our rigorous hand-curation process (detailed in~\cref{sec:benchmark}) ensures high problem quality, robustness against brute-force solutions, and broad coverage across mathematical domains.

\paragraph{Evaluation} 
In~\cref{sec:eval}, we evaluate state-of-the-art LLMs on \mc, including \gptfo{} and \othree{}~\citep{openai2024o1}, the \gemini{}  family~\citep{gemini-1.5}, and the \claude{} family~\citep{anthropic2024claude}. 
By generating variations of each problem, we evaluate the models on $475$ distinct problem instances.
Even with access to code execution, these models struggle with \mc. The best model achieves only $54\%$ accuracy, as shown in \cref{fig:lead_figure}; complete results are presented in~\cref{tab:main_results}. We further provide a thorough analysis of LLMs' failure modes, and study the impact of different variations, problem brute-forceability, and data contamination on performance.

\paragraph{Contributions}

Our key contributions are:
\vspace{-2mm}
\begin{itemize}[leftmargin=15pt]\setlength\itemsep{0.01em}
    \item We propose \mc, a benchmark of $126$ challenging constructive problems (\cref{sec:benchmark}).
    \item We conduct a rigorous evaluation of $14$ state-of-the-art LLMs on \mc, demonstrating its difficulty and significance for LLM evaluation (\cref{sec:main_results}).
    \item We provide a detailed analysis of LLM performance on \mc, including the impact of various factors on model performance (\cref{sec:alternative_eval,sec:error_analysis,sec:contamination,sec:problem_variations}).
\end{itemize}
\vspace{-2mm}
