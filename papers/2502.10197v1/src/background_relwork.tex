\section{Related Work} \label{sec:background}

This section reviews related work on mathematical benchmarking and constructive proofs in machine learning.

\paragraph{Easier math benchmarks}
Most math benchmarks for LLMs focus on problems where the final answer is a numerical value or algebraic expression that can be compared with a fixed ground truth. Among these, early benchmarks such as GSM8K~\citep{gsm8k} and MATH~\citep{hendrycks2021math} have been saturated by recent models~\citep{openai2024o1, deepseek2025r1}. More advanced problem sets, such as AIME 2024\footnote{See \href{https://artofproblemsolving.com/wiki/index.php/2024\_AIME\_I}{American Invitational Mathematics Examination}}, are more difficult, yet state-of-the-art models still solve ${\approx}87\%$.

\paragraph{Olympiad-based benchmarks} To introduce more complexity, newer benchmarks such as OlympiadBench~\citep{olympiadbench}, Omni-MATH~\citep{omnimath}, and HARP~\citep{harp} incorporate olympiad-level problems, including image-based and multilingual problems. However, these benchmarks still rely on fixed-answer verification, often using an LLM as a judge for comparison. In contrast, \mc focuses on constructive proofs rather than verifying predefined answers, providing a more challenging evaluation of reasoning abilities. This difference in evaluation methodology is highlighted in a problem statement that appears both in Omni-Math and \mc. Specifically, in the problem on the left of~\cref{fig:intro}, Omni-MATH considers a lower bound, $2n(n-1)+1$, as the correct answer, whereas \mc requires models to construct an object that achieves the lower bound, which is significantly more challenging in this case. 

\paragraph{Private benchmarks} Several private benchmarks, such as FrontierMath~\citep{frontiermath} and \mbox{LastExam}~\citep{lastexam}, have recently been introduced. Although FrontierMath includes problems that require more complex verification methods, the private nature of these benchmarks prevents large-scale use and makes it difficult to evaluate the progress of the field as a whole.

\paragraph{Benchmarks with variations} None of the above benchmarks incorporate problem variations, even though their value has been acknowledged in prior work. In particular, recent efforts in related domains, such as GSM-Symbolic~\citep{gsmsymbolic}, Putnam-Axiom~\citep{gulati2024putnamaxiom}, and UTMath~\citep{utmath} use symbolic problem reformulation. \mc aims to extend this line of work by applying variations to problems that require the construction of complex mathematical objects.

\paragraph{Formal math benchmarks}
Another category of benchmarks evaluates formal theorem proving, requiring solutions in languages such as Lean~\citep{lean}. Examples include miniF2F~\citep{minif2f}, FIMO~\citep{fimo}, and PutnamBench~\citep{putnambench}. Despite specialized training~\citep{xin2024deepseekprover}, LLMs struggle to solve more than a few problems in this environment, indicating significant room for improvement.

\paragraph{Logical reasoning benchmarks} Orthogonally, a large body of work evaluates logical reasoning in LLMs on logical puzzles and satisfiability problems~\citep{gui2024logicgame, puzzlebench, korbench}, many of which are NP-Hard~\citep{nphardeval}. Examples include puzzles such as the knapsack problem and Sudoku. These tasks primarily test algorithmic problem-solving rather than mathematical reasoning. Furthermore, the problems are often well-known, making them susceptible to memorization. While most problems in \mc are also NP-Hard, they ensure tractability for human solvers using pen and paper, as they originate from real mathematics competitions. This makes \mc a more direct evaluation of reasoning ability in mathematical contexts.


\paragraph{Constructions with machine learning}
Machine learning has also been previously applied to mathematical object construction and pattern discovery. \citet{wagner2021constructions} use reinforcement learning to find counterexamples that disprove conjectures in graph theory. Other works leverage neural networks for combinatorial optimization~\citep{bello2017neural, gasse2019exact}. More broadly, machine learning has been used to identify relationships between mathematical objects, as seen in \citet{davies2021advancing} and \citet{davila2024txgraffiti}.
