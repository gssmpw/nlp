\subsection{Effect of Problem Variations} \label{sec:problem_variations}

Finally, to investigate the impact of problem variations on model performance, we test the robustness of \geminithink{} against a range of extreme variations. Specifically, we select $10$ problems where the model performs reasonably well and evaluate its accuracy on $24$ variations of each. These variations purposefully include both trivially small cases and impractically large ones---scenarios intentionally excluded from the default version of \mc{}. For reference, we also include \claudesonnet{} (\textsc{Brute}), the brute-forcing agent introduced in~\cref{sec:alternative_eval}. 
As a proxy for variant difficulty, we define \emph{variant size} as the string length of our (\textsc{Gold}) solution. 

In~\cref{fig:length} we show the accuracy of each model on these variations, grouped by their size. As expected, the \textsc{Gold} solution always achieves $100\%$ accuracy. This reflects the performance of a human contestant who has solved the problem in its general form and can apply that solution to any variation. The brute-force agent is only successful on very small variations, where the problem often degenerates into a trivial form.
For example, setting $n=2$ in the problem in~\cref{fig:problem_encoding} requires a $2 \times 2$ matrix of rank $\leq 3$, which holds for any $2 \times 2$ matrix.
This illustrates the importance of our problem review process (\cref{sec:benchmark:review}), where variants with this behavior were generally excluded from \mc{}.

The \geminithink{} results follow a similar pattern: it achieves nearly $100\%$ on trivial variations but struggles with larger ones.
As variant size increases, accuracy declines due to the need to generate long responses and perform operations on large numbers, both of which increase the risk of errors. As noted in~\cref{sec:benchmark:review}, our problem review process included a step where we manually reviewed variant size and aimed to keep it within a reasonable range (see \cref{app:benchmark_details} for a histogram of original problem variant sizes and~\cref{app:tokens} for the distribution of model output tokens).
Within this range, \geminithink{} is fairly consistent, implying well-balanced variant difficulty in \mc{}.

