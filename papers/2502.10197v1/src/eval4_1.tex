\vspace{-1mm}
\subsection{Main Results} \label{sec:main_results}

We evaluate 14 state-of-the-art models on our benchmark and summarize the results in \cref{tab:main_results}, which expands on \cref{fig:lead_figure}. Each model is tasked with solving the problems in \mc while adhering to specific formatting guidelines for their responses (see \cref{app:prompts} for details). To ensure correct parsing, models receive two rounds of feedback from the parser, allowing them to refine their answers.

We report two key metrics: \emph{average accuracy}, which first computes accuracy over all variations of a problem and then averages these values across all problems, and \emph{robust accuracy}, which considers a problem solved only if all its variations are answered correctly. The latter metric reflects a stricter evaluation, analogous to how a human who solves the general form of a problem can solve all instantiations. Additionally, we provide the total cost of running each model on the benchmark, measured in USD.


\paragraph{Results} Among all models, \othree{} performs best, achieving $53.8\%$ accuracy and $34.9\%$ robust accuracy, outperforming the second-best model, \oone{}, by a $12\%$ margin. Among non-reasoning models, \flash{} leads with $11.3\%$ accuracy and $3.2\%$ robust accuracy, significantly ahead of other non-reasoning models.

Despite \oone{}'s impressive performance, it incurs a high cost, requiring USD $443.3$ to complete the benchmarkâ€” three times the combined cost of all other models. In contrast, \gemini{} models are currently free (at a limited rate), making them a more cost-effective alternative.

Notably, all models struggle with solving every variation of a problem, as reflected in the robust accuracy scores, which are approximately half of the average accuracy values. This highlights the difference between human and model performance, as humans can often solve all variations of a problem once they have solved the problem.

\input{tables/main_results}

\begin{figure}[t]
  \centering
    \includegraphics[width=0.85\linewidth]{figures/cost_vs_accuracy.pdf}
    \vspace{-3mm}
    \caption{Cost and accuracy of models with or without code execution. Future cost of \flash{} is estimated (now free).}
    \label{fig:accuracy_vs_cost}
    \vskip -2em
\end{figure}

\begin{figure*}[!t]
  \centering
    \includegraphics[width=0.85\linewidth]{figures/error_types.pdf}
    \vspace{-4mm}
    \caption{Error types of models before and after parser feedback.}
    \label{fig:error_types}
    \vskip -1em
\end{figure*}

