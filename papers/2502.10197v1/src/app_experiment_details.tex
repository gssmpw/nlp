\section{Experimental Details} \label{app:exp_details}
In this section, we describe in further detail how we performed all our experiments and evaluation.

\subsection{Experimental Setup}
\label{app:exp:exp_setup}

\paragraph{Model Names} For readability, we adopt shortened names for some models throughout this section. Specifically, we refer to \textsc{Gemini-2.0-Flash-Exp} as \flash{} and \textsc{Gemini-2.0-Flash-Thinking-Exp} as \geminithink{}. Similarly, \textsc{Claude-3.5-Sonnet} and \textsc{Claude-3.5-Haiku} are denoted as \claudesonnet{} and \claudehaiku{}, respectively.

\paragraph{Inference} Inference of the models was done through API calls to the appropriate model. For the \textsc{Llama} and \textsc{Qwen} models we used the Together API. For all other models, we used the API of the corresponding model provider. The total cost of these experiments are reported in \cref{tab:main_results}. Each model was queried with a temperature of $1$ and nucleus sampling with parameter $\text{top\_p}=0.9$. We use a maximum output length of 16000 tokens, except for the \oone{} model family and for \geminithink{}, where we increased this to respectively 32000 and unlimited number of tokens.

\paragraph{Code execution} To safely execute untrusted LLM code, we conducted all experiments within a lightweight Docker container. Any generated code was executed on a single core of an Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz with 1GB of RAM. The coding agents operated in an isolated environment without network access, restricted to using only the standard Python libraries along with \texttt{numpy}, \texttt{scipy}, and \texttt{sympy} libraries.

\paragraph{Problem setup}

For our experiments, we selected 126 problems, each paired with an original variation, which was kept identical to the original when possible. In cases where this was not feasible, we chose a variation that was sufficiently challenging and met the criteria outlined in \cref{sec:benchmark:selection} and \cref{sec:benchmark:review}. This resulted in a total of 475 variations, all of which were used to conduct our evaluation.

\paragraph{Run setup}
For each experiment, we used the prompts outlined in \cref{app:prompts} and provided the respective formatting instructions, with examples shown in \cref{app:exp:formatting}. We distinguish between the Chain-of-Thought (CoT) solver and multiple coding agents in our execution process.

In the CoT experiments, after receiving the model's response, we parse the expression inside the boxed environment, if present. If our parser detects a formatting issue or no response is found, the model is reprompted up to two times to provide a valid construction with detailed feedback. Specifically, the parser provides the model with the error in the following format: 

\begin{verbatim}
The solution parser encountered the following error:
{error}
Please format your reply accurately.
{Repetition of the formatting instructions}
\end{verbatim}

For the coding models, we execute any code block appearing in the model output with a time limit of 1 minute for the general coding model and 2 minutes for the brute-force agents. If the code runs successfully, we report the output to the model. If not, we provide the model with an error trace. Both appear to the model in the following format:

\begin{verbatim}
Code Output:
```{output}```
\end{verbatim}

We do not allow the brute-force agent to correct mistakes or adapt their strategy after the output is returned. This is to prevent the model from coming up with clever, non brute-force solutions. For the coding and brute-force inference agents, we allow them to correct mistakes and adapt their strategy after the output is returned. They are then reprompted to continue reasoning. We provide parser feedback up to two times per solution, allowing no more than four code attempts. Finally, we ask for a final response containing the boxed solution, and parse it as described earlier.


\subsection{Formatting Instructions}
\label{app:exp:formatting}
We differentiate between 4 main types of formats that we require from a model for verification of the solutions. This includes:
\vspace{-3mm}
\begin{itemize}\setlength\itemsep{0.01em}
    \item A single primitive object, \ie an integer.
    \item A list/set of primitives, \ie a tuple or a sequence.
    \item A matrix containing primitives, \ie a board or a table.
    \item A symbolic template for any \LaTeX{} expression, most generally fractions.
\end{itemize}
For each of these, we present a generic formatting template that we tweak for problems that do not fit the template.

Primitive template: 
\vspace{-3mm}
    \begin{verbatim}
    Output the answer as an <integer/string> inside of $\boxed{...}$. 
    For example, $\boxed{123}$.
    \end{verbatim}

List template:
    \vspace{-3mm}
    \begin{verbatim}
    Output the answer as a comma separated list 
    inside of $\boxed{...}$. For example, $\boxed{1, 2, 3}$. 
\end{verbatim}
Matrix template: 
\vspace{-3mm}
    \begin{verbatim}
    Output the answer between \verb|\begin{array}{...}| and 
    \verb|\end{array}| inside of $\boxed{...}$. For example, 
    $\boxed{\begin{array}{ccc}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9
    \end{array}}$.
    \end{verbatim}

Symbolic template (for fractions): 
\vspace{-3mm}
    \begin{verbatim}
    Output the answer as a fraction inside of $\boxed{...}$. 
    For example $\boxed{\frac{1}{2}}$.
    \end{verbatim}
