
\subsection{Error Analysis} \label{sec:error_analysis}

By leveraging the detailed feedback given by our parser and verification methods (described in~\cref{sec:benchmark:encoding}), we conducted a detailed error analysis of the models. Specifically, we categorized the errors into the following types: \textit{unparseable}, where the model output could not be parsed, \textit{no solution}, where the model does not provide a solution, \textit{reject}, where the model rejects the question's premise and states there is no solution, \textit{format}, where the output did not correctly follow the formatting instructions, and \textit{incorrect}, where the solution does not satisfy the problem constraints. 

\cref{fig:error_types} illustrates the distribution of error types for both \othree{} and \qwq{} before and after parser feedback. A key observation is that \othree{} frequently produces unparseable answers but significantly benefits from parser feedback. When reprompted, \othree{} successfully incorporates the feedback, leading to a notable accuracy improvement of $+15\%$. This suggests that as mathematical benchmarks increase in complexity, models should be systematically provided with parser feedback to ensure their performance is accurately evaluated. Otherwise, we risk underestimating their capabilities. 

Both models frequently fail to provide a solution in their initial attempts, either by omitting the required \texttt{\textbackslash boxed} environment or getting stuck. However, in many cases, parser feedback enables the models to correct these mistakes, encouraging them to try again or make an educated guess.
Interestingly, \qwq{} exhibits a distinct failure pattern: in $10\%$ of its errors, it explicitly rejects the premise of the question, asserting that no solution exists in its final answer. Moreover, unlike \othree{}, \qwq{} does not improve its parseability after receiving feedback, highlighting its inability to understand and follow instructions.
In \cref{app:case_study}, we additionally perform a case study comparing two models, \oone{} and \geminithink{}, on several problems where \oone{} demonstrates stronger pattern recognition capabilities, while \geminithink{} does not recognize patterns and resorts to exhaustive search.
