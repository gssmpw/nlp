\section{Prompts} \label{app:prompts}

In this section, we outline the various prompting methods used for the different evaluations we conducted, along with example solutions for each evaluation

\subsection{Chain-of-Thought Solver}
For our main results, presented in~\cref{tab:main_results}, we use a direct approach where each model is asked to solve the problem. We further give instructions for the precise way in which the answer should be formatted. 

\input{prompts/example}

\newpage

\subsection{Coding Agent}
The coding agents are allowed to run any code they deem necessary. We give instructions on how this can be done, as below:

\input{prompts/example_code.tex}

\subsection{Bruteforce Solver}
For the brute-force approach, the models are asked to generate a single block of code that naively creates a solution satisfies the problem's constraints. Only one attempt to produce the code is permitted.
\input{prompts/example_brute.tex}

\subsection{Brute+Infer Solver}
Unlike the pure brute-force solver, the brute-force inference agent is allowed to explore solutions with smaller parameters as to generalize the pattern. It is allowed up to two feedback rounds to explore settings and test different strategies. The interaction below demonstrates how a model, in this case GPT-4o, can adapt its naive brute-force approach into a more effective algorithm by combining logical reasoning with a programmatic solution.
\input{prompts/example_brute_infer.tex}   
