
\subsection{Alternative Evaluation Settings} \label{sec:alternative_eval}

We explore alternative evaluation settings on our benchmark and analyze their effects.  

\paragraph{Code agents} We first evaluate the performance of models with access to a Python interpreter. Specifically, each model can execute Python code up to four times per problem to generate solutions, verify reasoning steps, or perform calculations. The output of each execution is fed back to the model, allowing it to iteratively refine its reasoning. \cref{fig:accuracy_vs_cost} shows how accuracy and cost change when models are allowed to execute code. Compared to their base performance from \cref{tab:main_results}, both accuracy and cost increase significantly. Most models roughly double their accuracy, at the expense of a fivefold increase in cost. Notably, \claudesonnet{} improves from $5\%$ to $15.3\%$ accuracy. \flash{} still achieves the highest accuracy in this setting, reaching $17.5\%$.


\paragraph{Brute-force solutions} Some problems in our benchmark are susceptible to brute-force methods, as identified during our review process (\cref{sec:benchmark:review}). To evaluate this, we tested brute-force agents and adjusted problems that allowed trivial brute-force solutions. We considered two brute-force approaches: \emph{pure brute-force}, where the model is explicitly instructed to generate a brute-force solution, and \emph{brute-force inference}, where the model is encouraged to solve smaller instances in a brute-force manner and then generalize its findings to solve the full problem. To facilitate the latter strategy, we allow the model to refine its solution up to $3$ times, using feedback from our parser to adapt its strategy.

\cref{tab:brute_force} presents results for these brute-force agents on the final version of our benchmark. The pure brute-force method (\textsc{Brute}) achieves less than $8\%$ accuracy. A manual review indicates that most correct solutions either involve non-trivial reasoning steps or arise from luck. In contrast, the brute-force inference agent (\textsc{Brute+Infer}) performs significantly better, reaching up to $16.7\%$ accuracyâ€”surpassing the code agent from \cref{sec:alternative_eval}. This agent frequently discovers patterns by generalizing from smaller instances, effectively solving problems in a more structured manner. As a result, we did not remove problems that this method solved, as they demonstrate meaningful reasoning rather than brute-force execution.

\input{tables/brute-force}
  
