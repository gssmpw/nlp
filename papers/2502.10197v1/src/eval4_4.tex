
\begin{figure*}[t]
  \centering
  \begin{minipage}[t]{0.46\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/contamination.pdf}
    \caption{Contamination analysis of models on the benchmark and its rephrased equivalent.}
    \label{fig:contamination}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.46\textwidth}
      \centering
      \includegraphics[width=\linewidth]{figures/lengthstudy.pdf}
      \caption{The effect of problem variations on accuracy.}
      \label{fig:length}
  \end{minipage}
\end{figure*}


\subsection{Contamination Analysis} \label{sec:contamination}
We further investigate the impact of data contamination on performance, which is particularly important since olympiad problems are commonly included in training datasets. Assessing contamination is crucial for verifying both the reliability of benchmark results. Given the modifications and variations we introduced to the problems, we expect minimal contamination. To confirm this, we follow \citet{constat} and compare model performance on the original benchmark against a rephrased version, where problem statements have been rewritten by \gptfo{}. 

In \cref{fig:contamination}, we present the results for all models except \oone{} and \othree{}, which was excluded due to cost constraints. We find that model performance on the rephrased benchmark closely matches performance on the original, suggesting minimal contamination. However, \textsc{o1-mini} exhibits a small but noticeable deviation. Using bootstrapping, we estimate the $2\sigma$ confidence interval of this deviation to be $4.7 \pm 3.2\%$, indicating that the deviation is slight, but statistically significant. This suggests that \textsc{o1-mini} may slightly underperform in real-world scenarios compared to its benchmark results. Nevertheless, the effect is small and does not alter its ranking among the evaluated models.

