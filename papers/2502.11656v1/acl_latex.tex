% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{booktabs}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%%%%%%%% the package that i include
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{colortbl}
%\usepackage[table]{xcolor} % 推荐，提供更多颜色支持
\usepackage{xcolor}

%\usepackage[x11names]{xcolor}

\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{amsfonts} % mathbb

%\usepackage{graphicx}

\usepackage{amsmath}
%\usepackage{listings}
\usepackage{subcaption}
%\usepackage{graphicx}

\usepackage[utf8]{inputenc}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{courier}

% \title{Unlocking the Potential of Direct Preference Optimization in Text-to-SQL Through Chain-of-Thought Reasoning}
% \title{Chain-of-Thought Reasoning as a Catalyst for Preference Learning: Lessons from Text-to-SQL}
\title{Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
    \textbf{Hanbing Liu\textsuperscript{1}\thanks{Equal contribution.}},
    \textbf{Haoyang Li\textsuperscript{2,3}\footnotemark[1]},
    \textbf{Xiaokang Zhang\textsuperscript{2,3}},
    \textbf{Ruotong Chen\textsuperscript{2}}, \\ 
    \textbf{Haiyong Xu\textsuperscript{5}},
    \textbf{Tian Tian\textsuperscript{5}},
    \textbf{Qi Qi\textsuperscript{1}},
    \textbf{Jing Zhang\textsuperscript{2,4}}\thanks{Corresponding author.}
    \\
    \textsuperscript{1}Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, \\
    \textsuperscript{2}School of Information, Renmin University of China, Beijing, China, \\
    \textsuperscript{3}Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, China, \\
    \textsuperscript{4}Engineering Research Center of Database and Business Intelligence, Beijing, China, \\
    \textsuperscript{5}China Mobile Information Technology Center
    \\
    %\texttt{\{liuhanbing, lihaoyang.cs, zhang2718, chen2022, qi.qi, zhang-jing\}@ruc.edu.cn}
    %\\
    {\fontfamily{zi4}\selectfont\{liuhanbing, lihaoyang.cs, zhang-jing\}@ruc.edu.cn}
    %\\
    %\texttt{\{xuhaiyong, tiantianit\}@chinamobile.com}
    %\\
    %\small{
    %    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
    %}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\usepackage{CJKutf8}
\begin{document}
\maketitle
\begin{abstract}
% Direct Preference Optimization (DPO) has shown significant success in complex reasoning tasks, such as math word problems and code generation. However, when applied to Text-to-SQL datasets, we find that DPO often fails to enhance model performance and can even degrade it. Through extensive investigation, we identify the root cause of this limitation: unlike math and code tasks, which inherently leverage Chain-of-Thought (CoT) reasoning alongside DPO, Text-to-SQL datasets typically provide only final answers (i.e., golden SQL queries) without detailed CoT solutions. By augmenting existing Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, stable and significant performance improvements using DPO for Text-to-SQL tasks.
% %To address this, we introduce a CoT-synthesizer that augments Text-to-SQL datasets with step-by-step reasoning. With this simple yet effective augmentation technique, we achieve, for the first time, stable and significant performance improvements using DPO on Text-to-SQL tasks. 
% To understand why CoT reasoning is essential for unlocking DPO’s potential, we conduct a comprehensive analysis. Our findings reveal that incorporating CoT mitigates reward hacking during DPO training, strengthens the model’s discriminative ability, and enhances scalability. We believe these insights could provide valuable guidance for the Text-to-SQL community in building more robust and effective models. To support further research, we publicly release the code and CoT-enhanced datasets~\footnote{\url{https://anonymous.4open.science/r/ARR_Submission}}.
Direct Preference Optimization (DPO) has proven effective in complex reasoning tasks like math word problems and code generation. However, when applied to Text-to-SQL datasets, it often fails to improve performance and can even degrade it. Our investigation reveals the root cause: unlike math and code tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO, Text-to-SQL datasets typically include only final answers (gold SQL queries) without detailed CoT solutions. By augmenting Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, consistent and significant performance improvements using DPO.

Our analysis shows that CoT reasoning is crucial for unlocking DPO’s potential, as it mitigates reward hacking, strengthens discriminative capabilities, and improves scalability. These findings offer valuable insights for building more robust Text-to-SQL models. To support further research, we publicly release the code and CoT-enhanced datasets
%~\footnote{\url{https://anonymous.4open.science/r/ARR_Submission}}.
~\footnote{\url{https://github.com/RUCKBReasoning/DPO_Text2SQL}}.

% , by training large language models (LLMs) to distinguish between correct and incorrect responses


% Existing fine-tuning-based text-to-SQL methods typically train on benchmarks to directly generate the desired SQL queries. This approach overlooks the powerful reasoning capabilities obtained during the pre-training phase and fails to fully leverage the potential of preference learning. To address this, we propose a new Text-to-SQL pipeline that utilizes synthesized chain-of-thought (CoT) reasoning paths to enhance direct preference optimization (DPO). We find that incorporating CoT and DPO significantly improves performance on the Spider, Bird, and four robustness Text-to-SQL benchmarks. Furthermore, we observe that when SQL queries are generated directly without CoT (the vanilla setting), DPO often leads to performance degradation. To explain this phenomenon, we provide a comprehensive analysis, revealing that DPO may exhibit severe reward hacking in the vanilla setting. In contrast, the introduction of CoT could mitigate this issue, resulting in improved performance.

% DPO is a powerful technique xxx


% This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
% The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
% These instructions should be used both for papers submitted for review and for final versions of accepted papers.
\end{abstract}

\section{Introduction}
% Recently, Text-to-SQL has attracted significant interest from both the natural language processing and database communities~\citep{li2024codes, wang2020rat-sql, DBLP:journals/pvldb/FuLWLTS23catsql, pourreza2024din-sql}. This technology plays a crucial role in translating natural language questions into SQL queries, enabling non-experts to easily access and query data. As a result, it serves as a valuable tool for business intelligence, data exploration, and a wide range of data-centric applications.
Text-to-SQL has recently gained significant attention in natural language processing and database research~\citep{li2024codes, wang2020rat-sql, DBLP:journals/pvldb/FuLWLTS23catsql, pourreza2024din-sql}. It translates natural language questions into SQL queries, allowing non-experts to easily access data, making it a valuable tool for business intelligence, data exploration, and other data-centric applications.


% The wide application of such model in real-world scenarios attracted interest from researchers in the fields of both database systems and natural language processing.

% With the advent of large language models (LLMs), the Text-to-SQL paradigm is shifting towards using prompting-based multi-agent frameworks~\citep{talaei2024chess, pourreza2024din-sql, pourreza2024chase-sql}. However, these approaches heavily depend on closed-source LLMs, such as GPT-4~\citep{openai2024@gpt4-turbo} and Gemini~\citep{DBLP:journals/corr/abs-2312-11805gemini}, which pose significant challenges for real-world Text-to-SQL applications, including high usage costs, data privacy concerns, and slow inference speeds. To address these issues, recent studies have explored training open-source LLMs using supervised fine-tuning objectives~\citep{pourreza2024dts-sql, li2023resdsql, li2024codes}.

% With the advent of large language models (LLMs), two primary approaches have emerged for solving Text-to-SQL: prompting-based methods~\citep{talaei2024chess, pourreza2024din-sql, pourreza2024chase-sql} and supervised fine-tuning-based (SFT-based) methods~\citep{pourreza2024dts-sql, li2023resdsql, li2024codes}. In real-world applications, prompting-based methods are often impractical due to their reliance on powerful closed-source LLMs, leading to high usage costs, data privacy concerns, and slower inference speeds. Conversely, SFT approaches train open-source, deploy-friendly LLMs using existing benchmark datasets like Spider~\cite{} and Bird~\cite{}. However, the performance of SFT models is frequently constrained by the limited availability of high-quality training data, which is labor-intensive and time-consuming to acquire.

With large language models (LLMs), two main approaches have emerged for solving Text-to-SQL: prompting-based methods~\citep{talaei2024chess, pourreza2024din-sql, pourreza2024chase-sql}  and supervised fine-tuning (SFT) methods~\citep{pourreza2024dts-sql, li2023resdsql, li2024codes}. Prompting-based methods often rely on powerful closed-source LLMs, making them costly and slow, and raising data privacy concerns. In contrast, SFT trains open-source, deployable LLMs using benchmark datasets like Spider and Bird. However, SFT performance is often limited by the scarcity of high-quality training data, which is expensive and time-consuming to create.

\begin{figure}[t!]
  \vspace{\baselineskip}
  \centering
  \includegraphics[width=0.98\linewidth]{figures/peformance_changes_after_dpo.pdf}
  \caption{Model performance gains (greedy decoding) achieved by DPO over SFT (Improved Execution Accuracy, \%). Chain-of-thought reasoning is crucial for unlocking DPO's potential, ensuring its effectiveness and stability. }
  \label{fig:dpo_changes}
\end{figure}

Recent studies in complex reasoning tasks, such as math word problems~\cite{DBLP:conf/emnlp/XuLLHLZWZDZ0D24chatglm} and code generation~\cite{DBLP:journals/corr/abs-2406-06887plum}, have demonstrated that preference optimization algorithms (\emph{e.g.}, DPO~\cite{rafailov2024dpo}, KTO~\cite{DBLP:journals/corr/abs-2402-01306kto}, SimPO~\cite{DBLP:journals/corr/abs-2405-14734simpo}) can significantly enhance SFT models. These algorithms leverage preference data pairs to enable models to distinguish between correct and incorrect responses, addressing the limitations of simple SFT. Despite the proven success of preference optimization techniques, recent works in Text-to-SQL have rarely adopted these methods to improve the Text-to-SQL capabilities of LLMs. This raises a critical question: \textit{How much improvement can preference optimization bring to the Text-to-SQL task?}

\textbf{Preliminary Experiments.} To answer this question, we conduct initial experiments on Bird~\cite{li2024bird}, a challenging cross-domain Text-to-SQL benchmark. Each data sample consists of a <question, database, SQL query> triplet. Text-to-SQL models receive the question and database information (\emph{e.g.}, table names, column names, data types, etc.) and generate the target SQL query. To ensure the universality of our findings, we evaluate 10 open-source LLMs, ranging from 6.7B to 15B parameters. For preference optimization, we employ DPO, a widely adopted technique used in cutting-edge LLMs like LLaMA3~\cite{dubey2024llama3}, Qwen2.5~\cite{qwenreport}, and Mixtral~\cite{mixtral}.

Specifically, we follow the standard DPO training pipeline, which consists of three key steps:
(1) \textbf{SFT}: The base LLM is first fine-tuned on Bird's training set.
(2) \textbf{Preference Pair Construction}: Using the SFT model, multiple SQL queries are sampled for each training sample. Correct and incorrect queries are identified through database execution to create preference pairs.
(3) \textbf{DPO Training}: Finally, the SFT model is further trained on these preference pairs using the DPO loss, resulting in the final DPO model.

\textbf{Observations.} The ``Original Bird'' area in Figure~\ref{fig:dpo_changes} illustrates the performance gains introduced by DPO, measured as the improvement in execution accuracy between the DPO model and the SFT model with greedy search inference. Surprisingly, the results reveal that DPO does not consistently improve performance; in fact, it leads to performance degradation for 6 out of the 10 evaluated LLMs. To make preference optimization effective for Text-to-SQL, we additionally explore several strategies, including hyperparameter tuning~\cite{rafailov2024dpo}, integrating SFT loss~\cite{ouyang2022rlhf}, replacing DPO with KTO~\cite{DBLP:journals/corr/abs-2402-01306kto}, and using a small model to construct preference data~\cite{yang2024sense}. However, as shown in Appendix~\ref{apx:dpotricks}, these attempts still result in limited performance improvements ($<$1.5\%). 

\textbf{Hypothesis.} After extensive but unsuccessful algorithmic exploration, we hypothesize that the suboptimal performance of DPO in the Text-to-SQL task is primarily due to a critical yet often-overlooked factor: the quality of the data. By analyzing datasets for complex reasoning tasks, such as MATH~\cite{DBLP:conf/nips/HendrycksBKABTS21MathBenchmark}, GSM8K~\cite{DBLP:journals/corr/abs-2110-14168gsm8k}, CodeUltraFeedback~\cite{codeultrafeedback}, Orca-Math~\cite{orcamath}, and DART-Math~\cite{dartmath}, we observe that these datasets provide not only final answers but also chain-of-thought (CoT)-styled solutions with detailed reasoning steps. These CoT solutions bridge the gap between input questions and final answers, enabling LLMs to achieve better generalization and interpretability during SFT and DPO training. In contrast, Text-to-SQL datasets like Bird~\cite{li2024bird}, Spider~\cite{yu2018spider}, WikiSQL~\cite{wikisql}, and ScienceBenchmark~\cite{sciencebenchmark} only provide final answers (\emph{i.e.}, gold SQL queries), forcing SFT and DPO to rely solely on SQL queries as training labels. This discrepancy leads us to propose a hypothesis: \textit{The effectiveness of DPO is likely attributed to the use of CoT, a crucial factor that is often overlooked.}

\textbf{Verification.} 
% To validate this hypothesis, we introduce a pipeline to investigate how CoT impacts DPO's performance in the Text-to-SQL task. First, to efficiently and accurately generate CoT solutions for Text-to-SQL datasets with minimal human intervention, we adopt an LLM-based CoT synthesizer. The synthesizer takes as input the database information, the question, and the golden SQL query, and produces step-by-step CoT solutions. Next, using the same settings from preliminary experiments, we apply SFT and DPO to the CoT-enhanced Bird dataset. As illustrated in Figure~\ref{fig:dpo_changes }, as we expected, the introduction of CoT leads to stable and significant performance improvements in DPO across all 10 evaluated LLMs. Furthermore, in the experiment section, we extend our evaluations beyond Bird to a diverse set of Text-to-SQL benchmarks, including Spider~\cite{}, Spider-DK~\cite{}, Spider-Syn~\cite{}, Spider-Realistic~\cite{}, and Dr.Spider~\cite{}. Consistent trends are observed across these benchmarks.
To test this hypothesis, we introduce a pipeline to study how CoT affects DPO's performance in the Text-to-SQL task. We use an LLM-based CoT synthesizer to efficiently generate step-by-step CoT solutions for Text-to-SQL datasets with minimal human effort. The synthesizer takes the database information, question, and gold SQL query as input. Then, using the same settings as earlier experiments, we apply SFT and DPO to the CoT-enhanced Bird dataset. As shown in Figure~\ref{fig:dpo_changes}, adding CoT significantly improves DPO's performance across all 10 evaluated LLMs. Additionally, we extend our evaluations to other Text-to-SQL benchmarks, including Spider, Spider-DK~\cite{gan2021spiderdk}, Spider-Syn~\cite{gan2021spidersyn}, Spider-Realistic~\cite{deng2021spiderrealitic}, and Dr.Spider~\cite{DBLP:conf/iclr/Changdrspider}. Consistent trends are observed across these benchmarks.

% However, existing fine-tuning-based Text-to-SQL methods have two limitations, which may compromise their performance: 

% \textbf{(1) Lack of Leveraging Language Reasoning Ability}: Many studies have shown that encouraging large language models (LLMs) to generate step-by-step Chain-of-Thought (CoT) responses, rather than directly producing answers, significantly enhances performance on complex reasoning tasks~\cite{DBLP:conf/nips/Wei0SBIXCLZ22chain, DBLP:conf/iclr/ZhouSHWS0SCBLC23least, DBLP:conf/iclr/0002WSLCNCZ23selfconsis}. CoT helps unlock the language reasoning abilities acquired during pre-training by breaking down complex tasks into simpler, logical steps. However, current Text-to-SQL benchmarks, such as Spider~\citep{yu2018spider} and Bird~\cite{li2024bird}, typically do not provide CoT paths from questions to the desired SQL queries. As a result, most fine-tuning methods train models to generate SQL queries directly, without incorporating reasoning steps or explanations. This omission prevents models from fully leveraging their pre-trained knowledge, limiting their performance on existing benchmarks and their ability to generalize to unseen or more challenging scenarios.


% \textbf{(2) Under-exploration of Preference Learning}: After initial supervised fine-tuning (SFT), direct preference optimization (DPO)~\citep{rafailov2024dpo} being the most widely adopted post-training strategy, which can further enhance a model's performance. By training on preference pair data, DPO enlarges the probability margin between chosen and rejected responses, showing potential in complex tasks like math word problems~\citep{DBLP:conf/emnlp/XuLLHLZWZDZ0D24chatglm, DBLP:journals/corr/abs-2406-18629stepdpo} and code synthesis~\citep{DBLP:journals/corr/abs-2406-06887plum}. However, its impact on improving Text-to-SQL models beyond the fine-tuning stage remains uncertain although the database can provide accurate feedback to help us construct pair-wise training data for DPO. Therefore, further research is needed to evaluate the potential benefits and limitations of DPO in this context, as well as to explore alternative strategies that might better leverage preference learning to improve model performance in generating accurate SQL queries.

% To address these limitations, this paper introduces a framework that explores how CoT and DPO can affect models' performance for the Text-to-SQL task. Our pipeline consists of three steps. First, for each training sample in benchmarks, we use a powerful LLM (GPT-4o-mini~\citep{openai2024@gpt4-mini} in this work) to generate multiple CoT reasoning paths, outlining the step-by-step conversion from the question to the desired SQL query. Next, we perform supervised fine-tuning using this CoT-enhanced dataset. Finally, we sample multiple responses from the SFT model and use database feedback to construct a preference dataset, upon which we perform DPO training.

% To achieve comprehensive and convincing experimental results, we conduct extensive experiments on 10 open-source base models, ranging from 6.7B to 15B parameters. We use the widely adopted Spider benchmark~\citep{yu2018spider}, the challenging Bird benchmark~\citep{li2024bird}, and four robustness Text-to-SQL benchmarks: Spider-DK~\citep{gan2021spiderdk}, Spider-Syn~\citep{gan2021spidersyn}, Spider-Realistic~\citep{deng2021spiderrealitic}, and Dr.Spider~\citep{DBLP:conf/iclr/Changdrspider}. 

% Our results reveal that incorporating CoT consistently enhances the DPO stages. Interestingly, when models are trained in a vanilla setting (\emph{i.e.}, generating SQL queries directly without CoT), DPO often leads to a performance drop compared to the original SFT model. 

\textbf{Analysis.} To understand why CoT reasoning is essential for unlocking DPO's potential, we conduct a comprehensive analysis and make three key observations. First, introducing CoT significantly reduces reward hacking during DPO training, ensuring stable and effective performance. Second, CoT enhances DPO's effectiveness as an implicit reward model, improving its ability to discriminate between correct and incorrect responses. Finally, CoT increases DPO's scalability, both in terms of the number of preference data and inference-time sampling budgets.

Our contributions are summarized as follows:
\begin{itemize}[leftmargin=1.0em, itemsep=0.1em, parsep=0em, topsep=0em]
    % \item We conduct extensive preliminary experiments on Text-to-SQL datasets to evaluate the effectiveness of DPO using the standard training pipeline. Contrary to observations in prior studies, we find that DPO does not consistently enhance model performance and can even degrade it.
    % \item We identify that this limitation stems from the absence of Chain-of-Thought (CoT) solutions in existing Text-to-SQL datasets. By augmenting these datasets with synthetic CoT solutions, we achieve, for the first time, stable and significant performance improvements using DPO on the Text-to-SQL task. {\color{red} We believe these findings can provide practitioners with valuable insights on how to effectively incorporate DPO into their Text-to-SQL pipelines.}
    \item  We conduct extensive experiments on Text-to-SQL datasets to evaluate DPO within the standard training pipeline. Contrary to prior studies, we find that DPO does not consistently improve performance and can sometimes degrade it. However, by augmenting these datasets with synthetic CoT solutions, we achieve stable and significant performance improvements with DPO for the first time. As existing works overlook the critical data issue in Text-to-SQL, our findings provide important insights for effectively integrating DPO into Text-to-SQL pipelines.
    \item We also provide a comprehensive analysis to understand why CoT reasoning is essential for DPO. Our findings reveal that incorporating CoT mitigates reward hacking, strengthens discriminative ability, and enhances scalability.
    % \item We believe these insights will inspire researchers and practitioners to design better open-source Text-to-SQL models and advance preference optimization techniques in the Text-to-SQL domain.
    % \item We propose a new Text-to-SQL framework that leverages LLMs to extend existing benchmarks through the synthesis of CoT reasoning paths. Using this enhanced dataset, we perform SFT and utilize feedback from the database to guide subsequent DPO training.
    % \item Extensive experiments across multiple base models and various Text-to-SQL benchmarks demonstrate that our framework significantly and consistently improves performance by incorporating both CoT and DPO. Interestingly, we find that applying DPO in the vanilla setting often results in decreased Text-to-SQL performance, a phenomenon not observed in previous studies.
    % \item We offer a comprehensive analysis explaining why DPO is effective with CoT but not with the vanilla approach. We believe our conclusions can benefit the Text-to-SQL community and broader tasks involving complex reasoning. % Our findings indicate that incorporating CoT alleviates reward hacking during DPO training, leading to better performance.
    % \item We have made the CoT-enhanced datasets and source code publicly available to support further advancements in the text-to-SQL community\footnote{}.
\end{itemize}


% ($e.g.$ code synthesis), Text-to-SQL propose two fundamental challenges. The first is schema linking, that is to map the flexible language description to the exact entity stored in the database. The second is skeleton parsing, which requires model to generate possibly complex query structures that precisely return the records that user intended. 

% Recent progress of Text-to-SQL includes refinement of database prompt construction and post-training techniques such as self-correction, execution-based self-consistency, and multi-agent frameworks. However, the training pipeline of sql-generating model remain fixed, that is directly put into production after supervised fine-tuning.

% Two trending?

% % \begin{figure}[t]
% %   \includegraphics[width=\columnwidth]{figures/Intro.pdf}
% %   \caption{With the help of synthesized Chain-of-Thought reasoning solutions, model learns to correct its own mistakes, and to generate right SQL after DPO.}
% %   \label{fig:Intro}
% % \end{figure}

% Preference learning is a training phase applied after supervised fine-tuning, initially designed to align model for better instruction-following ability and more human-favorable content generation, and becomes a standard procedure to train a general purpose foundation model.

% Preference learning equips model with the capability to tell good answers from bad ones, which is beneficial to its reasoning ability. Recently, several works has already shown that it can boost model's performance in complex reasoning tasks as code completion and solving mathematical problems. 

% On the other hand, there is hardly any successful application of preference learning in the Text-to-SQL domain, although comparison of execution result with ground truth sql provides high quality feedback signal, and the reason is remained unclear. 

% \input{Tables/IntroDPO}

% Another trend is that chain-of-thought has strong positive effect on complex reasoning tasks. Chain-of-thought reasoning requires model to output its rationales and intermediate computation step before the direct answer, which significantly improved model zero-shot performance and stability in code and math tasks. 

% Despite the promising evidence from similar fields requires logical reasoning, in Text-to-SQL, chain-of-thought is not prevailing in train-based methods. The reasons are two-folds. First, the chain-of-thought solution is hard to gather or annotate in the real world, since the logic to tackle a specific Text-to-SQL question is not clear as solving procedures in math problems. Second, to fine-tune local models with chain-of-thought Text-to-SQL solutions distilled from aligned large model suffers a sub-optimal performance under generic inference strategies.

% \input{Tables/IntroCoT}

% Surprisingly, by combining them together, we find that the alignment phase will produce a consistent improvement, also result a large margin enhancement in end-to-end performance compared to generic SFT-only non-CoT Text-to-SQL models, in spite of the unsatisfactory performance of preference learning and chain-of-thought reasoning standalone in Text-to-SQL. This result hold true across model family, size, and model specialty, and is robust on well-received Text-to-SQL benchmarks including Bird, Spider and Dr. Spider.

% Furthermore, we investigate the mechanism behind the preference learning in Text-to-SQL, and find that its credit assignment ability is benefited by chain-of-thought reasoning, and its propensity to correct mistake of information redundancy preferences by extensive quantitative and qualitative analysis. 

% Additionally, we show that preference learning enhanced by CoT can make a consistent performance gain for all state-of-the-art Text-to-SQL methods, confirming the potential of its application in real-world Text-to-SQL systems.

% Our contribution is summarized as follows:

% \begin{enumerate}
%     \item We identify chain-of-thought reasoning as the key to the success of direct preference learning in Text-to-SQL task. Also, through careful experiment and analysis, we reveal the reason behind its success, and provide useful best practice guidelines. 
%     \item To deal with scarcity of chain-of-thought solution to Text-to-SQL in the wild, We propose a simple yet powerful pipeline do chain-of-thought version direct preference learning for Text-to-SQL task, which steadily improves end-to-end performance of models, who differ in model family, size and specialty, across various benchmarks and inference-time strategies.
%     \item We also prove that our method is compatible with most state-of-the-art Text-to-SQL pipeline, showcasing that the combination of chain-of-thought reasoning and preference learning has a great potential in the field of Text-to-SQL.
% \end{enumerate}

% The reminders of this paper is organized as follows. 

\section{Related Work}

\input{RelatedWork}

% \section{Direct Preference Optimization}
% As preliminaries, in this section we briefly review direct preference optimization (DPO).
% \input{DPO}

\section{Pipeline}
\input{sections/Pipeline}

\section{Experiment Setup}
\subsection{Datasets}

\textbf{Common Benchmark:} Spider~\citep{yu2018spider} is a widely used Text-to-SQL dataset comprising a training set of 7,000 samples and a development set of 1,034 samples. This dataset covers 200 databases across 138 diverse domains.

\textbf{Challenging Benchmark:} Bird~\citep{li2024bird} presents a more challenging benchmark, featuring a training set of 9,428 samples and a development set of 1,534 samples. It includes 95 large databases across 37 professional domains. In contrast to Spider, Bird offers a more realistic scenario that aligns with real-world applications. 
% Specifically, Bird emphasizes real, large-scale, and noisy database contents.  requires reasoning using external knowledge.

% Additionally, BIRD incorporates external knowledge (EK) for specific samples to support the generation of accurate SQL queries. By default, EK is utilized in our experiments.

% The majority of our experiments are done on the Bird benchmark, since its complexity best fits real-world scenarios. We also test our pipeline on the following prevailing Text-to-SQL test sets, to show the robustness of proposed method.

\textbf{Robustness Benchmarks:} Spider-DK~\citep{gan2021spiderdk}, Spider-Syn~\citep{gan2021spidersyn}, and Spider-Realistic~\citep{deng2021spiderrealitic} are three widely adopted robustness evaluation sets that modify the development set of Spider to simulate real-world scenarios. Another significant derivative, Dr.Spider~\citep{DBLP:conf/iclr/Changdrspider}, creates 17 distinct robustness evaluation sets by comprehensively perturbing the Spider development set across 3 aspects: questions, databases, and SQL queries. 
% In practice, we train the Text-to-SQL models using Spider and evaluate them to these robustness benchmarks.

% Due to the limit of pages, in the main context, we mainly show the evaluation result on the Bird benchmark and defer evaluation on other datasets to Appendix~\ref{}. 

\subsection{Evaluation Metrics}
For all benchmarks, we use the execution accuracy (EX) metric~\citep{yu2018spider} to evaluate the accuracy of the model's predictions. EX measures whether the predicted and gold SQL queries produce identical execution results on the given database. For Spider's development set, we additionally employ a more robust metric, test-suite accuracy (TS)~\citep{DBLP:conf/emnlp/ZhongYK20testsuite}, which extends EX by evaluating whether the predicted SQL query consistently passes the EX evaluation across multiple test-suite database instances.

\subsection{Inference Strategy}
Given a Text-to-SQL model, we explore three inference strategies: \textbf{(a) Greedy}: Use greedy decoding with a temperature of 0 to generate a response. \textbf{(b) Pass@1}: Sample a response with a temperature of 1.0. To ensure stability, we repeat this process 16 times and report the average scores. \textbf{(c) Maj@K}: Sample $K$ responses with a temperature of 1.0 and conduct majority voting based on the execution results of the predicted SQL queries. The final prediction is selected from the most-voted group.

% sampling one response with a temperature of 1.0, and (c) sampling multiple responses with a temperature of 1.0.


% \textbf{Pass@1} is to sample $K$ times for each input. Then we calculate average pass rate over all roll-outs as the score of this test case.

% \textbf{Major Voting} is to sample $K$ times for each input, then group roll-outs with same execution result together. Return a random SQL in the largest group as final output. Ties are broken arbitrarily. 

% Since the EX performance is related to decoding and selection strategies in inference time, in our experiments, we refer to the EX metric under each strategy as \textbf{greedy}, \textbf{pass@1}, \textbf{maj@K}, respectively. 

\subsection{Implementation Details}
We select 10 base models from various model families, including Deepseek~\citep{bi2024deepseekllm, guo2024deepseekcoder}, Qwen~\citep{yang2024qwen, hui2024qwencoder}, Llama~\cite{dubey2024llama3, roziere2023codellama}, and CodeS~\citep{li2024codes}. These models cover different specialties (general-purpose, code- or SQL-specific) and range from 6.7B to 15B parameters. For each LLM, we conduct SFT and DPO using either the original training dataset (Vanilla) or the CoT-enhanced dataset (Syn CoT). More implementation details are listed in Appendix~\ref{apx:imp_details}. Details about training data can be found in Appendix~\ref{apx:datadetails}.

% We report the best checkpoint according to the given metric on Dev. 

\section{Experimental Results}

\input{sections/MainResult}

\section{Why Does DPO Benefit From CoT?}

\input{sections/CoTHelpDPO}

% In our previous experiments, database prompt construction followed the approach proposed by \citet{li2024codes} in CodeS. Through error analysis in Section~\ref{sec:errorAnalysis}, we demonstrated that there is a complementary relationship between DPO and existing Text-to-SQL methods. As a proof-of-concept attempt, we selected two representative methods and pluged our module into their pipelines. The results are summarized in the Table~\ref{tab:OtherPipeline}. 
\section{Prospects}
Our trained DPO model can be integrated into existing multi-agent Text-to-SQL frameworks. As a proof of concept, we adopt Qwen2.5-7B-Instruct as the base model and integrate it into two Text-to-SQL frameworks: DTS-SQL~\citep{pourreza2024dts-sql} and C3-SQL~\citep{dong2023c3}. Implementation details are provided in Appendix~\ref{apx:application}. Results in Table~\ref{tab:OtherPipeline} demonstrate that our model could improve performance over existing frameworks.

% Therefore, we can utilize our trained DPO model to replace the Text-to-SQL component in existing multi-agent Text-to-SQL frameworks. As a proof of concept, we adopt Qwen2.5-7B-Instruct as the base model and integrate our trained model into two representative frameworks, DTS-SQL~\citep{pourreza2024dts-sql} and C3-SQL~\citep{dong2023c3}. Implementation details are in Appendix~\ref{apx:application}. Results in Table~\ref{tab:OtherPipeline} show that our model significantly improves performance over existing frameworks. This highlights a new opportunity to enhance existing Text-to-SQL systems.


% \textbf{Existing Text-to-SQL Methods Get Improved After Integrating Our Module.} Specifically, we replace SQL generation models in these methods with trainable substitutes and apply our pipeline. This led to an enhancement in performance.

% Our findings highlight a new opportunity to improve the performance of open-source models, paving the way for the development of safer and more interpretable Text-to-SQL systems.

\input{Tables/Transferability}
\section{Conclusion}
In this work, we demonstrate, for the first time, consistent and effective performance improvements using DPO on the Text-to-SQL task, enabled by synthetic CoT solutions. Through comprehensive experiments and detailed analyses, we show that CoT reasoning is essential for unlocking DPO's potential, as it mitigates reward hacking, enhances the model's discriminative capabilities, and improves scalability during DPO training. We believe these findings will inspire researchers and practitioners to develop more robust and effective open-source Text-to-SQL models.

% Chain-of-Thought (CoT) synthesis into the training process of Text-to-SQL models. CoT enables the model to achieve consistent improvements through Direct Preference Optimization (DPO). Through comprehensive experiments and detailed analyses, we further find that the CoT provides DPO with a more accurate implicit reward model, a more stable training process less prone to reward hacking, and good scalability with respect to sample budgets. Finally, we apply our trained model to two existing Text-to-SQL frameworks, achieving consistent performance enhancements, and thereby demonstrating the practical potential of our pipeline.

% We advocate for further research on customizing the construction of CoT reasoning, as well as the alignment algorithms for this task, to build more powerful, interpretable, and privacy-preserving open-source Text-to-SQL systems.

\section{Limitations}
The databases provided by the Bird dataset are typically large, leading to significant time consumption when executing SQL queries. This slows down the collection of preference data, as all sampled SQL queries must be executed on the databases to obtain feedback. To address this, we modify Bird's evaluation script to enable parallel execution of SQL queries using multi-processing, significantly accelerating the preference data construction process. However, resource contention among multiple processes can lead to SQL execution timeouts, causing correct predicted SQL queries to be incorrectly classified as incorrect, thereby introducing false negatives. This inaccuracy in feedback signals could potentially impact DPO training.

In contrast, the Spider dataset presents a different challenge due to the simplicity of its database values. Relying on execution results to distinguish between correct and incorrect SQL queries can lead to false positives~\cite{DBLP:conf/emnlp/ZhongYK20testsuite}, resulting in potentially unreliable feedback signals for DPO training.

% \section{Introduction}

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.31\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.31\linewidth]{example-image-b} \hfill
%   \includegraphics[width=0.31\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliography{custom}
\bibliography{alignment, custom, text-to-sql}

\appendix

\section{Preliminary Experiments on DPO Tricks}\label{apx:dpotricks}
Before exploring chain-of-thought reasoning, we extensively experiment with applying various DPO tricks to the original Bird dataset. This included hyperparameter tuning, using different loss variants, and exploring alternative preference data construction strategies. Specifically, for hyperparameter tuning, we evaluate different values of the $\beta$ parameter in DPO, testing 0.05 and 0.2 (the default value is 0.1). For loss variants, we experiment with augmenting the DPO loss by incorporating the SFT loss for correct responses (i.e., the correct sampled SQL queries). Additionally, we explore replacing DPO with its variant, KTO~\citep{DBLP:journals/corr/abs-2402-01306kto}. For preference data construction strategies, we follow SENSE~\cite{yang2024sense} to fine-tune a small-scale language model, Deepseek-coder-1.3b-instruct, on Bird's training set to collect preference data. The results, summarized in Table~\ref{tab:dpotrick}, show that none of these approaches yield significant performance improvements for DPO.


% Specifically, for the hyperparameter tuning, we try different $\beta$ parameters in DPO with 0.05 and 0.2 (the default value is 0.1). For the loss variants, we have tried to add the SFT loss of the correct responses (i.e., the correct sampled SQL queries) to the DPO loss. In addition, we have tried to replace DPO with its variant algorithm KTO~\citep{DBLP:journals/corr/abs-2402-01306kto}. For data construction strategies, we follow SENSE~\cite{} to fine-tune a small-scale language model, Deepseek-coder-1.3b, to construct preference pairs. The results are summarized in Table~\ref{tab:dpotrick}. We can observe that none of these approaches yielded significant performance improvements using DPO.

\definecolor{darkgreen}{RGB}{0,150,0}
\begin{table}[ht]
    \centering
\begin{adjustbox}{max width=\columnwidth}
    \begin{tabular}{ >{\centering\arraybackslash}p{0.4\columnwidth} | >{\centering\arraybackslash}p{0.4\columnwidth} }
        \toprule
         \multicolumn{2}{c}{\textbf{DPO Tricks (SFT$=58.8\%$)}} \\
         \midrule
         \multicolumn{2}{c}{\texttt{Hyper-parameter Tuning}} \\ \midrule
        $\beta=0.05$ & $\beta=0.2$ \\
        57.6\% (\textbf{\textcolor{darkgreen}{-1.2\%}}) & 57.8\% (\textbf{\textcolor{darkgreen}{-1.0\%}}) \\
        \midrule
         \multicolumn{2}{c}{\texttt{Loss Variants}} \\ \midrule 
         +SFT Loss & KTO \\
         60.0\% (\textbf{\textcolor{red}{+1.2\%}}) & 59.8\% (\textbf{\textcolor{red}{+1.0\%}}) \\
         \midrule
         \multicolumn{2}{c}{\texttt{Data Construction Strategies}} \\ \midrule
        \multicolumn{2}{c}{SENSE \citep{yang2024sense}} \\ 
        %+Gold SQL \\
        \multicolumn{2}{c}{58.4\% (\textbf{\textcolor{darkgreen}{-0.4\%}})} \\
        %& 59.2\% (\textbf{\textcolor{red}{+0.4\%}}) \\
        \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{Even with tricks applied, DPO still struggles to improve model performance on the Bird dataset (greedy decoding). The base model is Qwen2.5-7B-Instruct.}
    \label{tab:dpotrick}
\end{table}

\section{Chain-of-Thought Solutions} \label{apx:cotSynthesis}
In this section, we present the prompts used for Chain-of-Thought (CoT) reasoning synthesis and provide qualitative examples of the model's step-by-step Text-to-SQL responses.

\subsection{Prompt for Synthesis}
We carefully design the prompts used for CoT synthesis, as shown in Table~\ref{tab:prompt}. In our template, the gold SQL from the dataset is provided as a reference answer. This design enables the model to generate diverse reasoning paths during sampling while striving to maintain the correctness of the synthesized outputs.

\subsection{Synthesized Chain-of-Thought}
A synthesized Chain-of-Thought solution, generated by the \verb|gpt-4o-mini-2024-07-18| on an instance from the Bird Train dataset, is illustrated in Table~\ref{tab:synthesisCoT}. The response begins with an analysis of the input question, followed by a step-by-step breakdown of the SQL generation process. After generating the SQL, the model further provides explanations for each component of the SQL query.

\subsection{Samples From Syn CoT Models}
We select a sample question from the Bird development set and compare the responses generated during the SFT stage and the DPO stage. The SFT-generated response is shown in Table~\ref{tab:cotResponseSFT}, while the DPO-generated response is presented in the Table~\ref{tab:cotResponseDPO}. Notably, DPO corrected an entity mismatch error present in the SFT response.

\section{Direct Preference Optimization}\label{apx:dpo}
We provide a brief overview of direct preference optimization (DPO) and demonstrate how to utilize the trained DPO model to calculate the implicit reward of a response, as well as the credit assignment for each token within that response.
\subsection{Learning Objective}
For Reinforcement Learning with Human Feedback (RLHF) incorporating KL penalty, the learning objective is defined as \citep{ouyang2022rlhf}:
%\begin{adjustbox}{max width=\textwidth}
\begin{align*}
\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y|x)} & \left[r_{\phi}(x,y)\right] \\
 -\beta\mathbb{D}_{\mathrm{KL}} & \left[\pi_{\theta}(y\mid x)\mid\mid\pi_{\mathrm{ref}}(y\mid x)\right]
\end{align*}
%\end{adjustbox}

Here, $\pi_{\mathrm{ref}}$ and $\pi_{\theta}$ represent the initial model distribution and the optimized policy, respectively, while $r_{\phi}$ denotes a parameterized reward model.

Direct Preference Optimization (DPO) reformulates the objective by replacing the reward function with a differentiable form, reflecting the relationship between the optimal policy and the reward function. This leads to a new objective:
%$$\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=$$
%\resizebox{\linewidth}{!}{}
%\begin{adjustbox}{max width=\textwidth}
%\begin{align*}
%$$\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log \sigma\left(\beta\log\frac{\pi_{\theta}(y_{w}\mid x)}{\pi_{\mathrm{ref}}(y_{w}\mid x)}
%-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\mathrm{ref}}(y_{l}\mid x)}\right )\right ]$$
%\end{align*}
%\end{adjustbox}
\begin{align*}
\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log \sigma\left(\beta \log\frac{\pi_{\theta}(y_{w}\mid x)}{\pi_{\mathrm{ref}}(y_{w}\mid x)}\right.\right. \\
\left.\left.  -\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\mathrm{ref}}(y_{l}\mid x)} \right)\right]
\end{align*}


In this formulation, $y_w$ denotes the chosen output and $y_l$ denotes the rejected output. The parameter $\beta$ controls the penalty strength imposed by the KL divergence. By collecting pairwise preference data, the model can be optimized using supervised fine-tuning, achieving a performance comparable to RLHF \citep{rafailov2024dpo}.

\subsection{Implicit Reward}\label{apx:implicit_reward}
%\textbf{Implicit Reward}

DPO implicitly encodes a reward model within the generative model. The reward of a given input-output pair $(x, y)$ can be derived as:

$$r_{\theta}(x, y) = \beta \log \frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$$

As the DPO training progresses, the optimized model simultaneously becomes a better generative model and a more refined reward model. After training, the implicit reward model, which is derived via conditional likelihood, can be independently used as a reward function \citep{lambert2024rewardbench, chen2024bootstrappinglanguagemodelsdpo, DBLP:journals/corr/abs-2402-06457vstar}.

%\textbf{Credit Assignment}
\subsection{Token-level Credit Assignment}\label{apx:token_reward}

The implicit reward scores the entire output as a whole. By decomposing the conditional probability that featuring autoregressive generation process, the reward can be re-expressed as:
$$r_{\theta}(x,y) = \sum_{t=1}^{T}\beta \log \frac{\pi_\theta(y_t\mid x,y_{1:t-1})}{\pi_{\mathrm{ref}}(y_t\mid x,y_{1:t-1})}$$

This decomposition allows for the calculation of token-level rewards, as the model score for each token can be identified separately. Although DPO training uses supervision at the full-sequence level, evidence has shown that the model can generalize compositionality to some extent, allowing it to distribute the reward signal to key tokens \citep{rafailov2024dpo2}. This facilitates credit assignment across the output sequence. The resulting dense reward can be utilized for further training and optimization \citep{zhong2024dpo-ppo}.

\section{Implementation Details}\label{apx:imp_details}

The computational environment used in our experiments is equipped with a 64-core Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz and 8 NVIDIA A800 GPUs with 80GB of memory each, running CUDA version 12.1.1.

Training is conducted using Llama Factory \citep{zheng2024llamafactory}, with FlashAttention 2.0 \citep{dao2023flashattention} and DeepSpeed ZeRO-3 \citep{rasley2020deepspeed} enabled. For models larger than 10B parameters, the DPO stage utilizes CPU offloading for both model parameters and optimizer states. During inferenece, the trained models are hosted with vLLM \citep{kwon2023vllm}.

For training, we employ full-parameter fine-tuning with bf16 mixed-precision \citep{micikevicius2018bf16}. The optimizer used was AdamW \citep{DBLP:conf/iclr/LoshchilovH19adamw} with default parameters (\(\beta_1 = 0.9\), \(\beta_2 = 0.99\)). A cosine decay learning rate schedule and a linear warmup over the first 5\% of training steps are also applied. The context window of models is set to 4096 tokens.

Across all training phases, we adopt a consistent batch size of 64. The learning rates for the SFT and DPO phases are set to \(1 \times 10^{-5}\) and \(1 \times 10^{-6}\), respectively, for models smaller than 10B, and \(7 \times 10^{-6}\) and \(7 \times 10^{-7}\), respectively, for models larger than 10B. The \(\beta\) parameter for DPO is set to 0.1. 

All models are trained for 4 epochs during the SFT phase, and the best checkpoint is selected to serve as the reference model in DPO, based on the maj@K metric on the development set. Training is then continued for 8 epochs during the DPO phase.

Unless otherwise specified, during inference time across all stages (including chain-of-thought synthesis), the sampling budget is set to 16, with the $temperature$ and $topK$ parameters set to 1.0 and 32, respectively.

\section{Training Data Details}\label{apx:datadetails}
\noindent\textbf{Quantity.}
In Bird dataset, the Vanilla SFT data consists of $9,428$ instances, while the CoT SFT data consists of $9,428 \times K$ instances (for Table~\ref{tab:model-comparison}, $K=16$, as we generate 16 CoT solutions for each training sample in the original dataset). 

The size of the DPO training data is smaller than that of the SFT training data and is model-dependent. This is because, during the construction of the preference data, we exclude data samples for which all SFT model-generated CoT solutions are either entirely correct or entirely incorrect. As a result, both Vanilla and Syn CoT preference datasets contain approximately 1.5k-2.5k preference pairs (e.g., Qwen-7b-Instruct Syn CoT has 1,546 pairs). The relationship between sample budgets and the quantity of DPO training data is illustrated in Figure~\ref{fig:scalePrefFull} and Figure~\ref{fig:scalePrefLog}.

\noindent\textbf{Construction of Input-Output Sequences and Their Average Length.}
Input prompt has an average length of $965$ tokens, which is the same for Vanilla and Syn CoT settings. The input prompt includes not only the question but also database information, such as table and column names, primary and foreign key relationships, and potentially useful database values. Following CodeS~\citep{li2024codes}, we first use a schema item classifier to identify the most relevant tables and columns based on the question. To improve recall accuracy, we replace the backbone model of the classifier from RoBERTa-Large (355M)~\citep{liu2019roberta} to XLM-RoBERTa-XL (3.5B)~\citep{goyal2021xlroberta}. We then use the ``coarse-to-fine'' database value-matching approach to retrieve question-related values from the database. The retrieved tables, columns, values, and remaining primary and foreign keys form the database prompt.

The output label of Vanilla (gold SQL in the Bird dataset) has an average length of $44$ tokens, while the synthesized chain-of-thought solutions have an average length of $404$ tokens. Reported token numbers is measured by the tokenizer of Qwen-7b-Instruct.

\section{Data Quality Ablation}\label{apx:qualityablation}

From Table~\ref{tab:model-comparison}, we can see that there are many cases where Syn CoT SFT has already surpassed Vanilla SFT, thus, it is natural to doubt that the benefit could be brought about by the potent proprietary model. In this section, we first analyze the ability of GPT-4o-mini on Text-to-SQL, then, we replace GPT-4o-mini with smaller open-sourced models to synthesize CoT reasoning paths, therefore further confirm that it is the chain-of-thought style solution path itself that enhance the effect of DPO.

\noindent \textbf{Ability of the Synthesizers.} We evaluate GPT-4o-mini's capability on Bird development set, as well as other open-sourced models that we will use as synthesizers. The result is shown in Table~\ref{tab:cotquality}.

\definecolor{darkgreen}{RGB}{0,150,0}
\begin{table}[ht]
    \centering
\begin{adjustbox}{max width=\columnwidth}
    \begin{tabular}{c | c | c | c }
        \toprule
         \multirow{2}{*}{\textbf{Model}} &  \multicolumn{3}{c}{\textbf{Bird Dev}} \\ \cline{2-4}
         & ZS-CoT & FS-SQL & FS-CoT \\ \midrule
        GPT-4o-mini & 50.1 & 55.3 & 53.0 \\
        Qwen(1.5B) & 15.6 & 23.1 & 18.9 \\
        Qwen(7B) & 41.8 & 47.6 & 43.1 \\
        Qwen(32B) & 52.9 & 55.3 & 53.0 \\
        \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{Performance of models used for chain-of-thought solution synthesis under different prompting strategies (Greddy). \textbf{Qwen (XB)}: Qwen2.5-XB-Instruct; \textbf{ZS-CoT}: Zero-shot CoT; \textbf{FS-SQL}: 3-Shot examples randomly chosen from Bird Train; \textbf{FS-CoT}: 3-Shot examples randomly chosen from its own synthesized CoT solutions on Bird Train. }
    \label{tab:cotquality}
\end{table}
As a comparison, Qwen2.5-7B-Instruct, after fine-tuning on original Bird Train data, reaches 58.8, which is higher than the EX of any synthesizer, as shown in Table~\ref{tab:model-comparison}.

It is evident that directly prompting closed-source instruction-tuned models does not inherently offer an advantage in the Text-to-SQL task, consistent with findings from previous work \citep{li2024codes}. Closed-source models only perform well when incorporating with complex multi-agent designs \citep{pourreza2024chase-sql}. Therefore, in this paper, we use GPT-4o-mini merely as a substitute for manual effort, allowing us to quickly and cost-effectively obtain CoT solutions of Text-to-SQL data, rather than distilling capabilities from a powerful model.

\noindent \textbf{DPO with Different CoT Quailities.} As shown in Table~\ref{tab:cotquality}, CoT quality varies for different size of synthesizers. We then use each of them to synthesize CoT solutions, and train the corresponding Syn CoT model separately (Base model is Qwen2.5-7B-Instruct). The results are presented in Table~\ref{tab:opencotpdo}.

\definecolor{darkgreen}{RGB}{0,150,0}
\begin{table}[ht]
    \centering
\begin{adjustbox}{max width=\columnwidth}
    \begin{tabular}{c | c | c }
        \toprule
         \multirow{2}{*}{\textbf{Synthesizer}} &  \multicolumn{2}{c}{\textbf{Bird Dev}} \\ \cline{2-3}
         & SFT & DPO \\ \midrule
        GPT-4o-mini & 57.4 & 61.9 {\color{red}(+4.5)} \\
        Qwen(1.5B) & 40.9 & 59.1 {\color{red}(+18.2)} \\
        Qwen(7B) & 54.3 & 60.6 {\color{red}(+6.3)} \\
        Qwen(32B) & 59.3 & 62.5 {\color{red}(+3.2)} \\
        \bottomrule
    \end{tabular}
\end{adjustbox}
    \caption{Performance of models trained from CoT-enhanced data generated by different synthesizers (Greddy). \textbf{Qwen (XB)}: Qwen2.5-XB-Instruct. }
    \label{tab:opencotpdo}
\end{table}

Its evident that all these models achieve significant improvement in the DPO phase. Interestingly, the impact of CoT quality on model performance is significantly weakened after DPO.

\section{Results on Spider and Its Robustness Variants} \label{apx:spider}

In addition to the Bird benchmark, we also train and evaluate three representative base models on the Spider benchmark. We further assess the models trained on Spider with robustness test sets. 

However, during our experiments, we identified several issues with using CoT and DPO techniques on the Spider dataset:

\begin{enumerate}
    \item \textbf{Simplicity of SQL in Spider: Most SQL queries in Spider are quite simple.} A typical example is: \texttt{Q: How many concerts are there in year 2014 or 2015? A: SELECT count(*) FROM concert WHERE year = 2014 OR year = 2015}. smaller pre-trained language model methods have already achieved good results on these queries \citep{li2023resdsql}. Additionally, the best models on the Spider benchmark have reached human-level performance (91.2\%\footnote{from the Spider Leaderboard: \url{ttps://yale-lily.github.io/spider}}). It is widely acknowledged that CoT often does not perform better on simple questions, possibly due to overthinking issues \citep{sprague2024cotcotchainofthoughthelps}.
    \item \textbf{Inaccurate Feedback on Execution Results in Spider:} When constructing preference data, we rely on execution results on the database to judge the correctness of sampled SQL. The Bird dataset's databases are specially designed with massive rows, whereas Spider's databases have fewer rows (\#Row/DB: Bird ($549K$) vs. Spider ($2K$), $<0.5\%$ \citep{li2024bird}), leading to potential false positives during evaluation \citep{DBLP:conf/emnlp/ZhongYK20testsuite}. Although subsequent work has attempted to mitigate this by constructing multiple test suite (TS) databases, the Spider benchmark only provides TS databases for the development set, not for the training set, leading to inaccurate preference pair construction.
    \item \textbf{Small Scale of Constructible Preference Data on Spider:} Due to the first issue, most models on the Spider dataset achieve very high accuracy on the training set (Pass@16 $\geq 99\%$, compared to Bird's $\leq80\%$), resulting in a very small preference dataset ($0.1-0.3k$, with the SFT phase dataset being $7k$. As for comparison, DPO data on Bird is approximately $1.5-2.5k$).
\end{enumerate}

Based on these issues, we choose to focus primarily on the model's performance on the Bird dataset, one of the most challenging Text-to-SQL benchmarks that closely reflects real-world scenarios. Therefore, results related to Spider are included as a reference in the appendix due to constraints of limited space, as shown below.

\subsection{Spider}

%During the training on the Spider dataset, we utilize the standard training set (7,000 samples). Since Spider only provides test suites on the development set, we have to rely on the EX metric as the criterion for distinguishing positive and negative responses during preference data collection. This may introduce false positives in the DPO dataset, as many SQL queries in Spider yield empty execution results.

The model performance on Spider's development set is summarized in Table~\ref{tab:spiderDev}. Despite the aforementioned challenges, the Syn-CoT model consistently achieved improvements during the DPO stage. 

\subsection{Spider Variants}

We select the best checkpoint according to the Spider development set and directly evaluate it on these robustness test sets. The results for Spider-Syn, Spider-Realistic, and Spider-DK are presented in Table~\ref{tab:spiderVariants}. The CoT model continues to demonstrate consistent performance improvements during the DPO stage, further confirming its strong generalization capabilities after DPO training.

Dr.~Spider is a more comprehensive and sophisticated robustness test set, which categorizes all perturbations into three major types: database (DB), natural language question (NLQ), and SQL query. It then further subdivides them into 17 subcategories. For each type of perturbation, dedicated test sets are constructed~\citep{DBLP:conf/iclr/Changdrspider}.

For DB perturbations, the results of Syn CoT models are shown in Table~\ref{tab:drSpiderDBCoT}, alongside the results of the vanilla model in Table~\ref{tab:drSpiderDBVanilla}. For NLQ perturbations, the results of Syn CoT models are shown in Table~\ref{tab:drSpiderNLQCoT}, alongside the results of the vanilla model in Table~\ref{tab:drSpiderNLQVanilla}. For SQL perturbations, the results of Syn CoT models are shown in Table~\ref{tab:drSpiderSQLCoT}, alongside the results of the vanilla model in Table~\ref{tab:drSpiderSQLVanilla}.

Except for a few specific cases, Syn CoT model still demonstrates consistent improvements during the DPO stage.

\section{Error Classifications} \label{apx:class}
\subsection{Description}
In this paper, we classify errors made by predicted SQL into 6 major categories with 17 variant types. Descriptions of each category or type are shown in Table~\ref{table:ErrorCategoryDescription}.

\subsection{Error Samples}
We provide samples for each error type in our classification criteria, for external knowledge, see Table~\ref{table:ErrorSampleEK}, for schema linking, see Table~\ref{table:ErrorSampleSchema}, for value retrieval, see Table~\ref{table:ErrorSampleValue}, for operation, see Table~\ref{table:ErrorSampleOperation}, for information, see Table~\ref{table:ErrorSampleInfo}, for syntax error, see Table~\ref{table:ErrorSampleSyntax}. These are selected from model predictions on the Bird development set.


\subsection{Classification Result}
The total number of errors and the proportion of each error type are presented in pie charts. Results of Syn CoT models is shown in Figure~\ref{tab:ErrorStatCoT}, and results of vanilla models is shown in Figure~\ref{tab:ErrorStatVanilla}.

A SQL query may commit multiple types of errors simultaneously. In our analysis, however, we only attribute each erroneous SQL to the most prominent type of mistake it made logically, for the convenience of analysis.

\section{More Analysis of DPO} \label{apx:analysis}

\subsection{Overall Effect}
The changes in the correctness of model-generated outputs before and after the DPO stage are illustrated in Figure~\ref{fig:overallDPO}. The CoT model corrects a greater number of errors during the DPO stage, while the proportion of newly introduced errors remained comparable to that of the vanilla model.

\subsection{Effect on Difficulty Classes}
We analyze the impact of DPO on questions of varying difficulty levels, as shown in Figure~\ref{fig:difficultiesDPO}. The CoT model exhibits significant improvements in performance on medium- and high-difficulty questions during the DPO stage, while the improvements on simpler questions are relatively limited.

\subsection{Fix Rate Difference}
To facilitate a detailed comparison of how the CoT model enhances DPO's error correction capabilities for different error types, we rank the error types by the increase in fix rates introduced by DPO. The results are presented in Table~\ref{tab:fixRateRank}.

\subsection{Emerging Errors}
The number of newly introduced errors during the DPO process is summarized in Table~\ref{tab:newErrorStat}. Overall, the distribution of emerging errors in the Syn CoT model is similar to that of the vanilla model. However, while CoT improves DPO's ability to address hallucination errors, it also leads to an increase in newly introduced hallucinations during the DPO stage.

\subsection{Transition Matrices}
The transitions between error categories before and after DPO are depicted in Figure~\ref{fig:matrixTight}. For a more detailed view of these transitions, the vanilla model's error type transitions are shown in Figure~\ref{fig:matrixFullVanilla}, while those of the Syn CoT model are presented in Figure~\ref{fig:matrixFullCoT}.

\subsection{Weakness}
\textbf{Existing Text-to-SQL Methods Can Complement DPO's Weaknesses.} Despite improvements brought by CoT, DPO remains less effective at fixing certain error types. However, these align with core challenges that existing Text-to-SQL methods aim to address. For example, the model frequently fails to recall relevant \underline{Table} (Fix~15.9\%) and \underline{Column} (Fix~16.1\%), a key challenge of schema linking. Notable recent works include CHESS \citep{talaei2024chess} and E-SQL \citep{DBLP:journals/corr/abs-2409-16751esql}. 
%Many Text-to-SQL approaches specifically focus on improving performance in this area. 
\underline{Syntax Error} (Fix~13.3\%) is another tricky problem. \citet{dac-sql, magic-sql} have proposed post-generation execution and repair strategies to ensure executable returned SQL.

\section{Experiment Design} \label{apx:design}
\subsection{Evaluation Preference Dataset}

To ensure a fair comparison of the discriminative capabilities between the Vanilla and CoT models, we construct the mentioned evaluation preference dataset as follows. 

First, both SFT models are used to sample from the development set. For any data point where both models could generate paired data ($i.e.$, both could simultaneously sample a positive and a negative example), we use the sampling outcomes from the CoT model to randomly construct a preference pair. Subsequently, we extract the SQL portion of the pair to serve as the preference pair for the vanilla model, incorporating it into their respective evaluation sets. Through this construction process, we ensure that the databases, questions, and SQLs in the dataset for both models are identical.

\section{Prominent Reward Hacking Patterns}\label{sec:RHPatterns}
In this section, we provide other prominent reward hacking patterns of the DPO training process in the Vanilla setting from our observations, as illustrated in Table~\ref{tab:egRH1},~\ref{tab:egRH2},~\ref{tab:egRH3}.

\section{More Scaling Results} \label{apx:scaling}
Scaling behavior of performance on the CoT synthesis budget and sample budget of preference data collection under all inference strategies are complemented in Figure~\ref{fig:scaleSynFull} and Figure~\ref{fig:scalePrefFull}, respectively.

It is noteworthy that performance saturation regarding sample budget in the preference data collection stage is mainly caused by the diminishing return of new preference pairs, as can be clearly seen from the log-scale plot Figure~\ref{fig:scalePrefLog}.

\section{Application Details}\label{apx:application}
\subsection{DTS-SQL}
DTS-SQL divides the Text-to-SQL task into two stages: Schema-Linking and SQL-Generation \citep{pourreza2024dts-sql}. Based on the code available in its repository, we construct the training and testing datasets for the SQL-Generation stage. (Since the original code is developed for the Spider dataset, we refer to the submitted \verb|DTS_SQL_BIRD_submission.py| file to account for incorporating Hints when constructing Prompts for the Bird dataset.) At this stage, we obtain a dataset without CoT. Subsequently, we utilize \texttt{gpt-4o-mini-2024-07-18} to generate 4 CoT paths, thereby creating two distinct datasets for training our two types of models.
\subsection{C3}
C3 stands for Clear Prompting, Calibration of Model Bias, and Consistency Output \citep{dong2023c3}. For the Clear Prompting and Calibration of Model Bias components, we use the C3 prompt templates, removing parts related to CoT. We run the Schema-Linking code to generate prompt inputs, which are paired with the correct SQL to form input-output pairs. Additionally, we use \texttt{gpt-4o-mini-2024-07-18} to generate 4 CoT paths, thereby creating two datasets for training our two models. After training the models, we test on Spider Dev using a Consistency Output subset of size 16, identical to Maj@16.


% C Chain-of-Thought Solutions
\input{Tables/ExamplePrompt}
\input{Tables/ExampleSynCoT}
\input{Tables/ExampleCoTSFT}
\input{Tables/ExampleCoTDPO}

\clearpage

% D Result on Other Benchmarks
\input{Tables/MainExp_Spider}
\input{Tables/MainExp_SpiderOther}
\input{Tables/MainExp_DrDB}
\input{Tables/MainExp_DrNLQ}
\input{Tables/MainExp_DrSQL}

\clearpage

% E Error Classifications
\input{Tables/ErrorCategories}
\input{Tables/ErrorSamples}
\begin{figure*}[t]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PieCoTSFT.pdf}
    \subcaption{SFT}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PieCoTDPO.pdf}
    \subcaption{DPO}
  \end{subfigure}
  \caption{Error statistics of Syn CoT model. The percentage of each category and total error count are on the chart.}
  \label{tab:ErrorStatCoT}
\end{figure*}
\begin{figure*}[t]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PieVanillaSFT.pdf}
    \subcaption{SFT}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PieVanillaDPO.pdf}
    \subcaption{DPO}
  \end{subfigure}
  \caption{Error statistics of Vanilla model. The percentage of each category and total error count are on the chart.}
  \label{tab:ErrorStatVanilla}
\end{figure*}

\clearpage

% F More Analysis of DPO
\begin{figure*}[t]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/VanillaOverall.pdf}
    \subcaption{Vanilla}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/CoTOverall.pdf}
    \subcaption{Syn CoT}
  \end{subfigure}
  \caption{Overall effect of DPO. T/F indicates an item either pass or fail, the first one is the status of SFT, while the second one refers to the status of DPO. For example, 'FT' represents instances that are corrected after DPO training. \textbf{Syn CoT outperforms Vanilla in DPO mainly by fixing more errors.}}
  \label{fig:overallDPO}
\end{figure*}

\begin{figure*}[t]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/VanillaDifficulty.pdf}
    \subcaption{Vanilla}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/CoTDifficulty.pdf}
    \subcaption{Syn CoT}
  \end{subfigure}
  \caption{Effect of DPO on different difficulty sets. Vanilla DPO struggles in every difficult set. \textbf{Syn CoT DPO mainly enhances model performance on moderate questions, then harder questions.} Surprisingly, even with Syn CoT, model performance on simple questions does not improve much.}
  \label{fig:difficultiesDPO}
\end{figure*}
\input{Tables/ErrorAnalysisRanking}
\input{Tables/ErrorAnalysisNewError}
\begin{figure*}[t]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/MatrixTightVanilla.pdf}
    \subcaption{Vanilla}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/MatrixTightCoT.pdf}
    \subcaption{Syn CoT}
  \end{subfigure}
  \caption{Comparison of transition matrix among error categories. G indicates correct instances.}
  \label{fig:matrixTight}
\end{figure*}

\begin{figure*}[t]
  \centering
  % 子图 (a)
    \includegraphics[width=\linewidth]{figures/MatrixFullVanilla.pdf}
  \caption{Vanilla full transition matrix of error types. G1 indicates correct instances.}
  \label{fig:matrixFullVanilla}
\end{figure*}

\begin{figure*}[t]
  \centering
  % 子图 (a)
    \includegraphics[width=\linewidth]{figures/MatrixFullCoT.pdf}
  \caption{Syn CoT full transition matrix of error types. G1 indicates correct instances.}
  \label{fig:matrixFullCoT}
\end{figure*}

\clearpage

% Hacking Patterns
\include{Tables/ExampleHackingPatterns}

\clearpage

% G More Scaling Results
\begin{figure*}[ht]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Synthesis1.pdf}
    \subcaption{Greedy}
  \end{subfigure}
  \hfill
  % 子图 (b)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Synthesis2.pdf}
    \subcaption{Pass@1}
  \end{subfigure}
  \hfill
  % 子图 (c)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Synthesis3.pdf}
    \subcaption{Maj@K}
  \end{subfigure}
  \caption{Model performance with different sample budget $K$ in Chain-of-Thought reasoning synthesis tested under different inference strategies. The base model is Qwen2.5-7B-Instruct. }
  \label{fig:scaleSynFull}
\end{figure*}

\begin{figure*}[ht]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceData1.pdf}
    \subcaption{Greedy}
  \end{subfigure}
  \hfill
  % 子图 (b)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceData2.pdf}
    \subcaption{Pass@1}
  \end{subfigure}
  \hfill
  % 子图 (c)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceData3.pdf}
    \subcaption{Maj@16}
  \end{subfigure}
  \caption{Model performance with different sample budgets in preference data collection tested under different inference strategies. The base model is Qwen2.5-7B-Instruct.}
  \label{fig:scalePrefFull}
\end{figure*}

\begin{figure*}[ht]
  \centering
  % 子图 (a)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceDataLog2.pdf}
    \subcaption{Greedy}
  \end{subfigure}
  \hfill
  % 子图 (b)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceDataLog2.pdf}
    \subcaption{Pass@1}
  \end{subfigure}
  \hfill
  % 子图 (c)
  \begin{subfigure}[t]{0.31\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceDataLog3.pdf}
    \subcaption{Maj@16}
  \end{subfigure}
  \caption{Model performance with different preference data sizes in DPO training tested under different inference strategies. The base model is Qwen2.5-7B-Instruct.}
  \label{fig:scalePrefLog}
\end{figure*}

\end{document}
