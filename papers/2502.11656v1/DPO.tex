%\textbf{Learning Objective}

\subsection{Learning Objective}

For Reinforcement Learning with Human Feedback (RLHF) incorporating KL penalty, the learning objective is defined as \citep{ouyang2022rlhf}:
%\begin{adjustbox}{max width=\textwidth}
\begin{align*}
\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y|x)} & \left[r_{\phi}(x,y)\right] \\
 -\beta\mathbb{D}_{\mathrm{KL}} & \left[\pi_{\theta}(y\mid x)\mid\mid\pi_{\mathrm{ref}}(y\mid x)\right]
\end{align*}
%\end{adjustbox}

Here, $\pi_{\mathrm{ref}}$ and $\pi_{\theta}$ represent the initial model distribution and the optimized policy, respectively, while $r_{\phi}$ denotes a parameterized reward model.

Direct Preference Optimization (DPO) reformulates the objective by replacing the reward function with a differentiable form, reflecting the relationship between the optimal policy and the reward function. This leads to a new objective:
%$$\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=$$
%\resizebox{\linewidth}{!}{}
%\begin{adjustbox}{max width=\textwidth}
%\begin{align*}
%$$\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log \sigma\left(\beta\log\frac{\pi_{\theta}(y_{w}\mid x)}{\pi_{\mathrm{ref}}(y_{w}\mid x)}
%-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\mathrm{ref}}(y_{l}\mid x)}\right )\right ]$$
%\end{align*}
%\end{adjustbox}
\begin{align*}
\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log \sigma\left(\beta \log\frac{\pi_{\theta}(y_{w}\mid x)}{\pi_{\mathrm{ref}}(y_{w}\mid x)}\right.\right. \\
\left.\left.  -\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\mathrm{ref}}(y_{l}\mid x)} \right)\right]
\end{align*}


In this formulation, $y_w$ denotes the chosen output and $y_l$ denotes the rejected output. The parameter $\beta$ controls the penalty strength imposed by the KL divergence. By collecting pairwise preference data, the model can be optimized using supervised fine-tuning, achieving a performance comparable to RLHF \citep{rafailov2024dpo}.

\subsection{Implicit Reward}
%\textbf{Implicit Reward}

DPO implicitly encodes a reward model within the generative model. The reward of a given input-output pair $(x, y)$ can be derived as:

$$r_{\theta}(x, y) = \beta \log \frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}$$

As the DPO training progresses, the optimized model simultaneously becomes a better generative model and a more refined reward model. After training, the implicit reward model, which is derived via conditional likelihood, can be independently used as a reward function \citep{lambert2024rewardbench, chen2024bootstrappinglanguagemodelsdpo}.

%\textbf{Credit Assignment}
\subsection{Credit Assignment}

The implicit reward scores the entire output as a whole. By decomposing the conditional probability that featuring autoregressive generation process, the reward can be re-expressed as:
$$r_{\theta}(x,y) = \sum_{t=1}^{T}\beta \log \frac{\pi_\theta(y_t\mid x,y_{1:t-1})}{\pi_{\mathrm{ref}}(y_t\mid x,y_{1:t-1})}$$

This decomposition allows for the calculation of token-level rewards, as the model score for each token can be identified separately. Although DPO training uses supervision at the full-sequence level, evidence has shown that the model can generalize compositionality to some extent, allowing it to distribute the reward signal to key tokens \citep{rafailov2024dpo2}. This facilitates credit assignment across the output sequence. The resulting dense reward can be utilized for further training and optimization \citep{zhong2024dpo-ppo}.

