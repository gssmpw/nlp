To further investigate why CoT reasoning is essential for DPO, we conduct a series of analyses using Qwen2.5-7B-Instruct on the Bird benchmark. 

\subsection{Better Discriminative Ability}

\begin{figure*}[ht]
    \centering
    % 左边的单图
    \begin{minipage}{0.31\linewidth} % 调整宽度
        \centering
    \includegraphics[width=\columnwidth]{figures/Discriminator.pdf}
  \caption{Comparison of model's discriminative ability during DPO (measured by classification accuracy on curated evaluation set). }
  \label{fig:discriminator}
    \end{minipage}
    \hfill % 让两部分之间留出间隙
    % 右边的双子图
    \begin{minipage}{0.62\linewidth} % 调整宽度
        \centering
          % 子图 (a)
          \begin{subfigure}[ht]{0.49\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/SelfRewardVanilla.pdf}
            \subcaption{Vanilla}
            \label{fig:selfreward-a1}
          \end{subfigure}
          \hfill
          % 子图 (b)
          \begin{subfigure}[ht]{0.49\columnwidth}
            \centering
            \includegraphics[width=\columnwidth]{figures/SelfRewardCoT.pdf}
            \subcaption{Syn CoT}
            \label{fig:selfreward-b}
          \end{subfigure}
          \caption{Comparison of model's self-assessed performance (average implicit reward policy model given to its own roll-outs) and real performance (EX) on Bird development set (Pass@1) during DPO training.}
          \label{fig:selfreward}
    \end{minipage}
\end{figure*}


\begin{figure}[t]
  \centering
  % 子图 (a)
%\begin{adjustbox}{max width=0.9\columnwidth}
  % \begin{subfigure}[t]{0.31\linewidth}
  %   \centering
  %   \includegraphics[width=\linewidth]{figures/Synthesis.pdf}
  %   \subcaption{Chain-of-Thought Synthesis}
  %   \label{fig:scaleMaj-a}
  % \end{subfigure}
  % \hfill
  % 子图 (b)
  \begin{subfigure}[t]{0.493\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/PreferenceData.pdf}
    \subcaption{Preference Data}
    \label{fig:scaleMaj-b}
  \end{subfigure}
  \hfill
  % 子图 (c)
  \begin{subfigure}[t]{0.493\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/Inference.pdf}
    \subcaption{Inference-time}
    \label{fig:scaleMaj-c}
  \end{subfigure}
  \caption{Model performance with different sample budget $K$ in each stage (Maj@K). Qwen2.5-7B-Instruct is used as the base model.}
  \label{fig:scaleMaj}
  %  Base model is Qwen2.5-7B-Instruct. Synthesized Chain-of-Thought makes model more scalable in the DPO Stage.
%\end{adjustbox}
\end{figure}

%To ensure a fair comparison of the discriminative capabilities between the Vanilla and CoT models, we designed the following experiment.

We design an evaluation preference dataset to compare the discriminative capabilities of Vanilla and Syn CoT models after DPO. Construction details are in Appendix~\ref{apx:design}. During DPO training, we assess the model's ability to select the correct response by classification accuracy based on implicit reward. The results in Figure~\ref{fig:discriminator} show that \textbf{CoT reasoning enables DPO models to achieve more stable and superior discriminative ability}.

%First, we constructed a test set. Specifically, both SFT models were used to sample from the Dev set. For any data point where both models could generate paired data ($i.e.$, both could simultaneously sample a positive and a negative example), we used the sampling outcomes from the CoT model to randomly construct a preference pair. Subsequently, we extracted the SQL portion of the pair to serve as the preference pair for the Vanilla model, incorporating it into their respective test sets. Through this construction process, we ensured that the databases, questions, and SQLs in the test sets for both models were identical.

%Next, during the DPO training of the models, we do evaluation every 5 steps. For each preference pair in the test set, we computed the implicit reward for both the chosen and rejected responses. If the model assigned a higher score to chosen, we considered the classification to be correct. We then calculated the average classification accuracy across the entire test set. The results, as shown in the Figure~\ref{fig:discriminator}, indicate that the \textbf{CoT model achieves a higher convergent classification accuracy}.

\input{Tables/TrainRobGreedy}

\subsection{Better Training Stability}
From a different perspective, we examine the discrepancy between the models' self-evaluation rewards for their generated outputs and the actual execution accuracy on the development set. Specifically, at the end of each epoch during DPO training, we sample responses using the current checkpoint and calculate the average implicit reward, as defined in Appendix~\ref{apx:implicit_reward}. Figure~\ref{fig:selfreward} illustrates the results for both the Syn CoT and Vanilla models.

\textbf{Vanilla Model is Susceptible to Reward Hacking.} As shown in Figure~\ref{fig:selfreward} (a), during DPO training, the performance of the Vanilla model initially peaks but then drops. Despite this drop, its self-reward scores continue to rise, indicating that DPO's underlying reward model mistakenly believes its outputs are improving, thereby demonstrating the phenomenon of reward hacking.

% During DPO training, the performance of the Vanilla model quickly reaches its peak but then declines. Despite this decline in actual performance, the model's self-reward scores it assigns to its own outputs continue to increase throughout the training process. This indicates that the Vanilla model mistakenly believes it is producing increasingly better results, even though its true performance deteriorates.

\textbf{In Contrast, The Self-reward of Syn CoT Model Reflects Its Actual Performance.} As shown in Figure~\ref{fig:selfreward} (b), In the early stages of DPO training, as the model's capabilities improve, its self-reward scores also increase. In the later stages, when the model's performance saturates, the self-reward scores stabilize rather than exhibiting the reward-hacking observed in the Vanilla model. This suggests that the Syn CoT model can recognize when its outputs are no longer improving and avoids overestimating its performance.

% the later stages of 
\textbf{Output Statistics.} We compare the statistical changes in the outputs of the two models (Vanilla vs. Syn CoT) after DPO in Table~\ref{tab:RewardHackingStat}. For the Vanilla model, the average output length increased significantly (+26.6\%) after DPO, accompanied by a substantial rise in the proportion of invalid SQL queries (+17.6\%).
In contrast, the Syn CoT model exhibited minimal changes in SQL length (+2\%), and a reduced non-executable rate (-2.5\%).

\input{Tables/RewardHackingStat}

\textbf{A Case of Reward Hacking in Vanilla + DPO.} Furthermore, we show a classical non-executable output generated by the Vanilla DPO model in Figure~\ref{fig:RewardHackingCase}. In this example, the model assigns an exceptionally high reward to an incorrect token, even though this token does not appear in the training dataset. This indicates a clear occurrence of reward hacking. Other prominent reward hacking patterns are presented in Appendix~\ref{sec:RHPatterns}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\columnwidth]{figures/RewardHacking.pdf}
  \caption{A case of reward hacking from Bird development set. The background color indicates token rewards.}
  \label{fig:RewardHackingCase}
\end{figure}

\textbf{Syn CoT Enhances DPO Robustness to Hyperparameter $\beta$.} As shown in Table~\ref{table:RobBeta}, varying the hyperparameter $\beta$ reveals that the DPO training of the vanilla model is highly sensitive to changes in $\beta$. In contrast, the Syn CoT model demonstrates notable robustness, consistently delivering strong performance despite these perturbations.

\subsection{Better Scalability}

% In our proposed pipeline, the sample budget at each stage can be adjusted. 
In this section, we discuss the scalability of our proposed pipeline to provide guidance for best practices. We report the Maj@K results in Figure~\ref{fig:scaleMaj}, and results for Greedy and Pass@16 are provided in Appendix~\ref{apx:scaling}.
% \textbf{CoT Synthesis (Figure~\ref{fig:scaleMaj-a}).} As the number of synthesized CoTs increases, model performance initially improves and then declines. Notably, synthesizing too few (\emph{e.g.}, 1) or too many CoTs (\emph{e.g.}, 64) negatively impacts performance for SFT. Furthermore, regardless of the sample budget, DPO performance consistently exceeds that of SFT.
% Increasing the number of synthesized reasoning paths allows the DPO to achieve further gains when SFT starts to saturate, ultimately reaching a higher peak performance.
\textbf{Preference Data Collection (Figure~\ref{fig:scaleMaj-b}).} For vanilla models, collecting additional preference data does not enhance performance during the DPO stage. In contrast, increasing the sample budget for Syn CoT models consistently results in continuous performance improvements with DPO. % However, performance gains tend to saturate eventually, primarily due to the diminishing growth rate of preference data.
\textbf{Inference-time Computation (Figure~\ref{fig:scaleMaj-c}).} 
%For both synthesized CoT and Vanilla SFT models, performance is very similar when the sample budget is small. 
%However, as the budget increases, the synthesized CoT model gradually outperforms the Vanilla model by a margin of approximately 2\%. 
By increasing the inference budget, both vanilla and Syn CoT models show improved performance, although the gains eventually stabilize. When the budget is limited, DPO offers a significant advantage for Syn CoT models over Vanilla models. For instance, ``CoT + DPO'' with 2 samples achieves the same performance as the ``Vanilla + DPO'' with 16 samples.

% \subsection{Case Study}

% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=\textwidth]{figures/Case.pdf}
%   \caption{A case study of Syn CoT DPO, the question is selected from Bird Dev set. At beginning, SFT model fails to link entity correctly. During DPO, model learns to assign credit to critical wrong tokens, and replace them with right ones. Finally, DPO model is able to generate correct SQL with sound logic.}
%   \label{fig:CaseStudy}
% \end{figure*}

% We wrap up this section by a case study, where we demonstrate how Syn CoT effectively corrects entity linking errors during DPO training.

% As shown in figure \ref{fig:CaseStudy}, The SFT model failed to utilize the data sample provided in the database prompt (underlined in the figure), instead copying conditions directly from the question, resulting in a mismatch with the stored database information.

% During the DPO process, the model assigned low rewards to the erroneous key tokens, indicating its ability to identify specific error locations. Additionally, the model learned to select the correct tokens in such scenarios. Ultimately, after training, the DPO model is capable of generating a coherent rationale and returning the correct SQL query.