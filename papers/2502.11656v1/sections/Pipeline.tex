
% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=\textwidth]{figures/Pipeline.pdf}
%   \caption{Overview of the proposed pipeline, which consists of three sequential stages: (a) CoT Synthesis: Use GPT-4o-mini to generate $K$ chain-of-thought reasoning paths for each Text-to-SQL training sample. (b) SFT: Fine-tune a language model to translate questions into corresponding CoT paths using database-related information. (c) DPO: For each training sample, sample $N$ CoT responses from the SFT model and use database execution feedback to create a preference dataset, then perform DPO on top of the SFT model.}
%   \label{fig:overview}
% \end{figure*}

% , which consists of three sequential stages: (a) CoT Synthesis: Use GPT-4o-mini to generate $K$ chain-of-thought reasoning paths for each Text-to-SQL training sample. (b) SFT: Fine-tune a language model to translate questions into corresponding CoT paths using database-related information. (c) DPO: For each training sample, sample $N$ CoT responses from the SFT model and use database execution feedback to create a preference dataset, then perform DPO on top of the SFT model.

\subsection{Overview}
Figure~\ref{fig:overview} illustrates our pipeline, which consists of three steps: (1) CoT synthesis, (2) supervised fine-tuning (SFT), and (3) direct preference optimization (DPO). Initially, we use an LLM as the CoT synthesizer to generate CoT solutions for a given Text-to-SQL dataset. With this CoT-enhanced dataset, we perform SFT and utilize feedback from databases to construct preference data pairs, followed by DPO. Further details are provided below.

% fine-tune a language model to develop the policy model. Finally, for each training sample, we generate multiple candidate CoT reasoning paths from the policy model and identify accepted and rejected CoTs based on the execution results of the predicted SQL query compared to the labeled gold SQL query. 

\subsection{Chain-of-Thought Synthesis}
% Unlike most math word problem benchmarks (\emph{e.g.}, MATH~\citep{DBLP:conf/nips/HendrycksBKABTS21MathBenchmark}, GSM8K~\citep{DBLP:journals/corr/abs-2110-14168gsm8k}), which provide step-by-step solutions to connect questions to final answers, Text-to-SQL benchmarks like WikiSQL~\cite{}, Spider~\citep{yu2018spider} and Bird~\citep{li2024bird} typically lack of human-labeled CoT solutions. 
To minimize human annotation costs, we utilize an LLM as the CoT synthesizer to generate CoT solutions for existing Text-to-SQL datasets. Specifically, for each data sample <question, database, SQL qeury>, we employ GPT-4o-mini~\citep{openai2024@gpt4-mini} to generate $K$ diverse CoT solutions that demonstrate the step-by-step conversion of the question into the gold SQL query. Since the final answers (\emph{i.e.}, gold SQL queries) are provided to the CoT synthesizer, the generated CoT solutions are both reliable and accurate, making them suitable for subsequent SFT and DPO training. The prompts used for CoT synthesis and qualitative examples are detailed in Appendix~\ref{apx:cotSynthesis}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/overview-simple-version.pdf}
  \caption{Overview of the proposed pipeline.}
  \label{fig:overview}
\end{figure}

\subsection{Supervised Fine-tuning}
Then, we perform supervised fine-tuning (SFT) using the CoT-enhanced dataset. The input prompt includes not only the question but also the database prompt, including information such as table names, column names, column data types, primary and foreign key relationships, etc. Details about the prompt construction can be found in Appendix~\ref{apx:datadetails}. The output sequences for SFT are LLM-synthesized CoT solutions. Formally, we denote the question as $q$, the database prompt as $d$, and the output CoT solution as $c$. The objective of SFT is guided by a conditional next-token prediction loss:
\begin{equation*}
\mathcal{L_{SFT}} = - \mathbb{E}_{(q,d,c) \sim D_{S}} [\log \pi_{base}(c \mid q, d)],
\end{equation*}
where $D_{S}$ denotes the CoT-enhanced training set, and $\pi_{base}$ refers the base model. After fine-tuning, we obtain model $\pi_{SFT}$, serving as the reference model for the subsequent DPO.

% We first construct database prompt for train data following CodeS \citep{li2024codes}, procedures mainly includes schema linking via fine-tuned transformer encoder, and value matching from databases.

% Next, we pair the database prompt with its corresponding SQL label, and then leverage strong proprietary model ($i.e.$ gpt-4o-mini in our experiments) to complete missing chain-of-thought reasoning for the pair in a zero-shot manner. 

% We sample $K$ times to acquire diversified reasoning path, result in $K$ answers per question.

% We then train model on this LLM-annotated train set with generic next-token prediction SFT loss.


\subsection{Direct Preference Optimization}
Then, we apply DPO \citep{rafailov2024dpo} to further improve the Text-to-SQL capabilities of the SFT model. A brief overview of the DPO algorithm, including its learning objective, implicit reward mechanism, and token-level credit assignment, is provided in Appendix~\ref{apx:dpo}.

% \textbf{Execution Feedback}
% The main-stream benchmarks of Text-to-SQL are accompanied with ground truth SQL and delicately curated database instance, which guarantees a minimal false-positive rate for execution result comparison.

% Therefore, for each chain-of-thought output on a train set question, we first extract SQL with regular expressions, and then mark it as either pass or fail based on execution result comparison with ground truth SQLs. 

\textbf{Preference Dataset Construction.}
DPO requires a preference dataset to teach the model to distinguish between correct and incorrect responses. In this study, each preference data sample consists of a quadruple: <question, database, correct CoT solution, incorrect CoT solution>. To construct this dataset, we sample CoT solutions from the supervised fine-tuned (SFT) model and use database feedback to determine the correctness of each sampled CoT solution. Specifically, for each data sample in the training set, we generate $N$ distinct CoT solutions from the reference model $\pi_{SFT}$. We then extract predicted SQL queries from these solutions using regular expressions and execute both the predicted and gold SQL queries on the corresponding databases. A sampled CoT solution is labeled as correct only if the execution results (\emph{e.g.}, records that satisfy certain filter conditions) of the predicted SQL query completely match those of the gold SQL query; otherwise, it is labeled as incorrect. For data samples with multiple correct and incorrect CoT solutions, we randomly select one correct and one incorrect solution to form a preference pair for DPO training.


% DPO requires a preference dataset to teach the model discriminate correct and incorrect responses. In this paper, each preference data sample consists of a <question, database, correct CoT solution, incorrect CoT solution> quad. To obtain such a preference dataset, we sample CoT solutions from the SFT model and use database feedback to identify the correctness of each sampled CoT solution. Specifically, each data sample in the training set, we sample $N$ different CoT solutions from the reference model $\pi_{SFT}$. Then, we extract predicted SQL queries using regular expressions from these sampled solutions. We execute these predicted SQL queries and gold SQL queries on the databases and compare their execution results. A sampled CoT solution is considered correct only if the database execution results of the predicted SQL query match those of the gold SQL query. Otherwise, it is considered incorrect. For a data sample having multiple correct and incorrect sampled CoT solutions, we randomly select a correct and an incorrect solution to form a preference pair for DPO training. 

% {\color{red} [Refine the expression]}

% If a question has both passed and failed roll-outs, we randomly select a pair, then label them as chosen and rejected respectively to get triplet $(x,y_\mathbf{b},y_\mathbf{f})$ and therefore form the preference dataset $\mathcal{D}_{EX}$. Instances either uniformly passed or failed are discarded. 

\textbf{DPO Learning Objective.}
The goal of DPO is to maximize the margin between the log-likelihood of the correct and incorrect responses while ensuring the model remains close with the reference policy. Formally, for a question $q$ and its corresponding database prompt $d$, let $c^{+}$ and $c^{-}$ represent the correct and incorrect CoT solutions, respectively. The DPO loss is defined as: 
\begin{equation*}
\scalebox{0.8}{$
\begin{aligned}
&\mathcal{L_{DPO}} = - \mathbb{E}_{(q,d,c^{+},c^{-}) \sim D_{P}} \\
&\log \sigma\left(\beta \log\frac{\pi_{DPO}(c^{+}\mid q,d)}{\pi_{SFT}(c^{+}\mid q,d)} -\beta\log\frac{\pi_{DPO}(c^{-} \mid q,d)}{\pi_{SFT}(c^{-}\mid q,d)}\right),
\end{aligned}$
}
\end{equation*}
where $\mathcal{D}_{P}$ is the preference dataset, $\sigma(\cdot)$ is the sigmoid function, $\beta$ is a hyperparameter that controls the penalty strength imposed by the KL divergence, $\pi_{DPO}$ represents the DPO model, which is initialized from the SFT model at the start of training.

%\begin{adjustbox}{max width=\columnwidth}
%$-\sum_{(x,y_{\mathbf{p}},y_{\mathbf{f}})\in \mathcal{D}_{EX}}\log \sigma\left(\beta \log\frac{\pi_{\theta}(y_{\mathbf{p}}\mid x)}{\pi_{\mathrm{SFT}}(y_{\mathbf{p}}\mid x)}-\beta\log\frac{\pi_{\theta}(y_{\mathbf{f}}\mid x)}{\pi_{\mathrm{SFT}}(y_{\mathbf{f}}\mid x)} \right)$
%\end{adjustbox}

% \subsection{Inference}
% In inference time, for an input, we sample $\pi_{DPO}$ for $K$ times, and return majority voting results in accordance to execution outcome of extracted SQLs. Ties are broke arbitrarily. 
