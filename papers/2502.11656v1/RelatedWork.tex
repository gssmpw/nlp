\textbf{Text-to-SQL.}
The Text-to-SQL task aims to convert natural language questions into SQL queries for a given database.
%Early efforts concentrate on developing various representation learning techniques to enhance the model's encoding capabilities, including graph neural networks~\citep{wang2020rat-sql, DBLP:conf/acl/CaoC0ZZ020lgesql, DBLP:conf/nips/CaiYXH21SADGA} and pre-trained language model encoders~\citep{DBLP:conf/iclr/0009WLWTYRSX21grapppa, DBLP:conf/acl/YinNYR20tabert, DBLP:conf/naacl/DengAMPSR21STRUG}. Subsequently, an increasing number of studies approach the Text-to-SQL task as a sequence-to-sequence generation challenge, utilizing T5~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20T5} to model the distribution mapping~\cite{DBLP:conf/emnlp/ScholakSB21picard, li2023resdsql, DBLP:conf/acl/ShawCPT20, DBLP:conf/aaai/LiHCQ0HHDSL23graphix}. More recently, 
With the emergence of large language models (LLMs) like GPT-4~\citep{openai2024@gpt4-turbo} and Gemini~\citep{DBLP:journals/corr/abs-2312-11805gemini}, the field has rapidly shifted towards leveraging LLMs for unified processing. These methods typically involve fine-tuning open-source language models~\cite{li2024codes, pourreza2024dts-sql, yang2024sense} or prompting closed-source LLMs through a multi-agent framework~\cite{pourreza2024din-sql, gao2024dali-sql, talaei2024chess, pourreza2024chase-sql, wang2024mac-sql}. Our work focuses on utilizing open-source LLMs and investigates the joint effectiveness of CoT reasoning and preference learning.


% Initial efforts focused on zero-shot in-context learning \citep{rajkumar2022first-llm}, which was later followed by a range of more advanced techniques, including Chain-of-Thought (CoT) reasoning, self-consistency, query decomposition, and execute-fix frameworks.

% Recent research trends have explored finer-grained schema linking and multi-agent frameworks to achieve improved performance. Another line of work investigates the use of API calls to fine-tune private models for better Text-to-SQL translation.

% A second research direction emphasizes privacy-preserving Text-to-SQL systems, which leverage fine-tuned open-source language models to achieve comparable or superior performance to closed-source alternatives. Seminal work in this domain includes CodeS, which created a dedicated open-source Text-to-SQL model through continued pretraining. Other works, such as DTS-SQL, leverage fine-tuned models for generative schema linking, while SENSE employs diverse model samples to augment training datasets and introduces Direct Preference Optimization (DPO) in the training pipeline.

% This paper follows the second line of research. Unlike previous methods where fine-tuning is directly used to generate SQL queries, we introduce synthetic Chain-of-Thought (CoT) reasoning into Text-to-SQL model training for the first time. Additionally, we propose an alignment phase following supervised fine-tuning (SFT) to further enhance the modelâ€™s performance.

\textbf{Preference Optimization.} Preference optimization aims to align the model with the preferences of responses, which typically requires a compare-based training set.  
%Unlike SFT, preference optimization typically requires a compare-based training set, where each input prompt is associated with a chosen response and a rejected response. 
%The goal is to maximize the probability margin between the chosen and rejected responses.
To this end, various methods have been proposed, from DPO~\citep{rafailov2024dpo} to its variants SimPO~\citep{DBLP:journals/corr/abs-2405-14734simpo}, KTO~\citep{DBLP:journals/corr/abs-2402-01306kto}, and IPO~\citep{DBLP:conf/aistats/AzarGPMRVC24ipo}. In this study, we employ DPO as our preference optimization algorithm. We note that only one study, SENSE~\citep{yang2024sense}, also employs DPO for optimizing Text-to-SQL models. However, their approach differs significantly from ours. Specifically, we follow the standard DPO pipeline, collecting preference data directly from the SFT model, whereas SENSE uses a small-scale model (1B parameters) to construct preference data for larger models, which may pose challenges in terms of transferability. Additionally, SENSE continues to use SQL queries as training labels, while this paper leverages CoT-style solutions.

\textbf{Learning from Execution Feedback.} Prior to the advent of LLMs, learning through execution feedback had already been widely applied to code-related tasks. CodeRL \citep{coderl} uses unit test results as rewards to optimize of pre-trained models through reinforcement learning. Other notable works include RLTF \citep{rltf}, StepCoder \citep{stepcoder}, and PseudoFeedback \citep{psudofeedback}. Nowadays, similar approaches are employed in the post-processing of general-purpose models to enhance their logical and coding capabilities \citep{qwenreport, dscoderv2}. However, existing general code models have not shown advantages in Text-to-SQL \citep{li2024codes}, and the substantial memory and CPU consumption of SQL execution makes online methods infeasible. We are the first to achieve stable and significant gains in Text-to-SQL by leveraging execution feedback and preference optimization.

% , and the substantial memory and CPU consumption of SQL execution makes online methods infeasible

%  but find that its direct application to the Text-to-SQL task results in suboptimal performance
% To address this issue, we incorporate CoT into the SFT and DPO training process, which helps mitigate the reward hacking issue.

% This approach can be somewhat challenging when transferring to various base models.

% alignment stage of open-source language models, such as LLaMA3~\citep{llama3}, Mixtral~\citep{Mixtral_of_Experts}.

% was initially proposed as a simpler alternative to Reinforcement Learning with Human Feedback (RLHF) for aligning large language models (LLMs). DPO achieves similar improvements in instruction following and value alignment using a straightforward supervised fine-tuning (SFT) loss, thereby avoiding the computational overhead of reinforcement learning. DPO has seen widespread adoption in the training of open-source general-purpose language models.

% However, DPO faces several challenges, including limited generalization capacity, sensitivity to the initial distribution, susceptibility to reward hacking, and a substantial increase in output length. To address these issues, a range of improvements has been proposed, such as SimPO, KTO, and IPO.

% A key advantage of DPO is its ability to leverage negative samples, a feature that is unavailable during the conventional SFT phase. As a result, DPO and its variants have been applied in complex reasoning tasks where negative feedback can be obtained from explicit result comparisons. For instance, in mathematics problems, answers can be directly compared to ground-truth solutions, while in code generation, unit tests can be employed to verify the correctness of generated code.

% For the Text-to-SQL task, high-quality feedback can be obtained by comparing the execution results of SQL queries on the database. Despite this potential, no existing work has demonstrated a convincing application of DPO in Text-to-SQL tasks. Our study reveals that a direct application of DPO to this task yields suboptimal results. To overcome this, we introduce synthetic Chain-of-Thought (CoT) reasoning into the DPO process, enabling DPO to be effectively applied to Text-to-SQL, thereby significantly improving task performance.