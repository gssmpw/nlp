@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@article{christ2023undetectable,
  title={Undetectable Watermarks for Language Models},
  author={Christ, Miranda and Gunn, Sam and Zamir, Or},
  journal={arXiv preprint arXiv:2306.09194},
  year={2023}
}

@article{kuditipudi2023robust,
  title={Robust Distortion-free Watermarks for Language Models},
  author={Kuditipudi, Rohith and Thickstun, John and Hashimoto, Tatsunori and Liang, Percy},
  journal={arXiv preprint arXiv:2307.15593},
  year={2023}
}

@inproceedings{kaptchuk2021meteor,
  title={Meteor: Cryptographically secure steganography for realistic distributions},
  author={Kaptchuk, Gabriel and Jois, Tushar M and Green, Matthew and Rubin, Aviel D},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1529--1548},
  year={2021}
}

@article{ziegler2019neural,
  title={Neural linguistic steganography},
  author={Ziegler, Zachary M and Deng, Yuntian and Rush, Alexander M},
  journal={arXiv preprint arXiv:1909.01496},
  year={2019}
}

@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{jawahar2020automatic,
  title={Automatic detection of machine generated text: A critical survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2011.01314},
  year={2020}
}

@article{tay2020reverse,
  title={Reverse engineering configurations of neural text generation models},
  author={Tay, Yi and Bahri, Dara and Zheng, Che and Brunk, Clifford and Metzler, Donald and Tomkins, Andrew},
  journal={arXiv preprint arXiv:2004.06201},
  year={2020}
}

@inproceedings{gambini2022pushing,
  title={On pushing {DeepFake Tweet} Detection capabilities to the limits},
  author={Gambini, Margherita and Fagni, Tiziano and Falchi, Fabrizio and Tesconi, Maurizio},
  booktitle={Proceedings of the 14th ACM Web Science Conference 2022},
  pages={154--163},
  year={2022}
}

@article{wolff2020attacking,
  title={Attacking neural text detectors},
  author={Wolff, Max and Wolff, Stuart},
  journal={arXiv preprint arXiv:2002.11768},
  year={2020}
}

@article{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{tian2023gptzero,
  title={{GPTzero} update v1},
  author={Edward Tian},
  journal={ https://gptzero.substack.com/p/
gptzero-update-v1},
  year={2023}
}

@article{kirchner2023new,
  title={New {AI} classifier for indicating {AI}-written text},
  author={Kirchner, Jan Hendrik and Ahmad, Lama and Aaronson, Scott and Leike, Jan},
  journal={OpenAI},
  year={2023}
}

@article{krishna2023paraphrasing,
  title={Paraphrasing evades detectors of {AI}-generated text, but retrieval is an effective defense},
  author={Krishna, Kalpesh and Song, Yixiao and Karpinska, Marzena and Wieting, John and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2303.13408},
  year={2023}
}

@article{chakraborty2023possibilities,
  title={On the possibilities of {AI}-generated text detection},
  author={Chakraborty, Souradip and Bedi, Amrit Singh and Zhu, Sicheng and An, Bang and Manocha, Dinesh and Huang, Furong},
  journal={arXiv preprint arXiv:2304.04736},
  year={2023}
}

@inproceedings{hopper2002provably,
  title={Provably secure steganography},
  author={Hopper, Nicholas J and Langford, John and Von Ahn, Luis},
  booktitle={Advances in Cryptology—CRYPTO 2002: 22nd Annual International Cryptology Conference Santa Barbara, California, USA, August 18--22, 2002 Proceedings 22},
  pages={77--92},
  year={2002},
  organization={Springer}
}

@article{dedic2009upper,
  title={Upper and lower bounds on black-box steganography},
  author={Dedi{\'c}, Nenad and Itkis, Gene and Reyzin, Leonid and Russell, Scott},
  journal={Journal of Cryptology},
  volume={22},
  pages={365--394},
  year={2009},
  publisher={Springer}
}

@inproceedings{abdelnabi2021adversarial,
  title={Adversarial watermarking transformer: Towards tracing text provenance with data hiding},
  author={Abdelnabi, Sahar and Fritz, Mario},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={121--140},
  year={2021},
  organization={IEEE}
}

@inproceedings{Aaronson2022,
  title={My {AI} Safety Lecture for {UT} Effective Altruism,},
  author={Aaronson, Scott},
  url={https://scottaaronson.blog/?p=6823},
  year={2022}
}

@article{qiang2023natural,
  title={Natural language watermarking via paraphraser-based lexical substitution},
  author={Qiang, Jipeng and Zhu, Shiyu and Li, Yun and Zhu, Yi and Yuan, Yunhao and Wu, Xindong},
  journal={Artificial Intelligence},
  volume={317},
  pages={103859},
  year={2023},
  publisher={Elsevier}
}

@article{yoo2023robust,
  title={Robust natural language watermarking through invariant features},
  author={Yoo, KiYoon and Ahn, Wonhyuk and Jang, Jiho and Kwak, Nojun},
  journal={arXiv preprint arXiv:2305.01904},
  year={2023}
}

@article{munyer2023deeptextmark,
  title={Deeptextmark: Deep learning based text watermarking for detection of large language model generated text},
  author={Munyer, Travis and Zhong, Xin},
  journal={arXiv preprint arXiv:2305.05773},
  year={2023}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{hu2023unbiased,
  title={Unbiased watermark for large language models},
  author={Hu, Zhengmian and Chen, Lichang and Wu, Xidong and Wu, Yihan and Zhang, Hongyang and Huang, Heng},
  journal={arXiv preprint arXiv:2310.10669},
  year={2023}
}

@article{liu2023unforgeable,
  title={An unforgeable publicly verifiable watermark for large language models},
  author={Liu, Aiwei and Pan, Leyi and Hu, Xuming and Li, Shu’ang and Wen, Lijie and King, Irwin and Yu, Philip S},
  journal={arXiv preprint arXiv:2307.16230},
  year={2023}
}

@article{liu2023semantic,
  title={A semantic invariant robust watermark for large language models},
  author={Liu, Aiwei and Pan, Leyi and Hu, Xuming and Meng, Shiao and Wen, Lijie},
  journal={arXiv preprint arXiv:2310.06356},
  year={2023}
}

@article{zhao2023provable,
  title={Provable robust watermarking for ai-generated text},
  author={Zhao, Xuandong and Ananth, Prabhanjan and Li, Lei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2306.17439},
  year={2023}
}
@article{yang2020prioritizing,
  title={Prioritizing genetic variants in GWAS with lasso using permutation-assisted tuning},
  author={Yang, Songshan and Wen, Jiawei and Eckert, Scott T and Wang, Yaqun and Liu, Dajiang J and Wu, Rongling and Li, Runze and Zhan, Xiang},
  journal={Bioinformatics},
  volume={36},
  number={12},
  pages={3811--3817},
  year={2020},
  publisher={Oxford University Press}
}

@inproceedings{yang2019lasso,
  title={ET-lasso: a new efficient tuning of lasso-type regularization for high-dimensional data},
  author={Yang, Songshan and Wen, Jiawei and Zhan, Xiang and Kifer, Daniel},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={607--616},
  year={2019}
}
@article{wen2023feature,
  title={Feature-splitting algorithms for ultrahigh dimensional quantile regression},
  author={Wen, Jiawei and Yang, Songshan and Wang, Christina Dan and Jiang, Yifan and Li, Runze},
  journal={Journal of Econometrics},
  pages={105426},
  year={2023},
  publisher={Elsevier}
}
@misc{chakraborty2022using,
  title={Using disentangled learning to train an interpretable deep learning model},
  author={Chakraborty, Supriyo and Calo, Seraphin Bernard and Wen, Jiawei},
  year={2022},
  month=jun # "~23",
  publisher={Google Patents},
  note={US Patent App. 17/133,437}
}

@article{cai2022asset,
  title={Asset splitting algorithm for ultrahigh dimensional portfolio selection and its theoretical property},
  author={Cai, Zhanrui and Li, Changcheng and Wen, Jiawei and Yang, Songshan},
  journal={Journal of Econometrics},
  pages={105291},
  year={2022},
  publisher={Elsevier}
}

@article{wu2023dipmark,
  title={Dipmark: A stealthy, efficient and resilient watermark for large language models},
  author={Wu, Yihan and Hu, Zhengmian and Zhang, Hongyang and Huang, Heng},
  journal={arXiv preprint arXiv:2310.07710},
  year={2023}
}

@article{chen2024your,
  title={Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection},
  author={Chen, Ruibo and Wu, Yihan and Chen, Lichang and Liu, Guodong and He, Qi and Xiong, Tianyi and Liu, Chenxi and Guo, Junfeng and Huang, Heng},
  journal={arXiv preprint arXiv:2402.12501},
  year={2024}
}

@article{xu2017low,
  title={Low-entropy cloud computing systems},
  author={Xu, Zhiwei and Li, Chundian},
  journal={Scientia Sinica Informationis},
  volume={47},
  number={9},
  pages={1149--1163},
  year={2017}
}

@article{feng2018indexing,
  title={Indexing techniques of distributed ordered tables: A survey and analysis},
  author={Feng, Chen and Li, Chun-Dian and Li, Rui},
  journal={Journal of Computer Science and Technology},
  volume={33},
  pages={169--189},
  year={2018},
  publisher={Springer}
}

@inproceedings{wu2023adversarial,
  title={Adversarial weight perturbation improves generalization in graph neural networks},
  author={Wu, Yihan and Bojchevski, Aleksandar and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10417--10425},
  year={2023}
}

@inproceedings{wu2022retrievalguard,
  title={Retrievalguard: Provably robust 1-nearest neighbor image retrieval},
  author={Wu, Yihan and Zhang, Hongyang and Huang, Heng},
  booktitle={International Conference on Machine Learning},
  pages={24266--24279},
  year={2022},
  organization={PMLR}
}

@inproceedings{wu2023law,
  title={A law of robustness beyond isoperimetry},
  author={Wu, Yihan and Huang, Heng and Zhang, Hongyang},
  booktitle={International Conference on Machine Learning},
  pages={37439--37455},
  year={2023},
  organization={PMLR}
}

@inproceedings{
hong2024improving,
title={Improving Non-Transferable Representation Learning by Harnessing Content and Style},
author={Ziming Hong and Zhenyi Wang and Li Shen and Yu Yao and Zhuo Huang and Shiming Chen and Chuanwu Yang and Mingming Gong and Tongliang Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}



@inproceedings{pmlr-v202-hu23g,
  title =   {Learning to Learn from {API}s: Black-Box Data-Free Meta-Learning},
  author =       {Hu, Zixuan and Shen, Li and Wang, Zhenyi and Wu, Baoyuan and Yuan, Chun and Tao, Dacheng},
  booktitle =   {Proceedings of the 40th International Conference on Machine Learning},
  pages =   {13610--13627},
  year =   {2023},
  publisher =    {PMLR},
}



@inproceedings{
wang2023defending,
title={Defending against Data-Free Model Extraction by  Distributionally Robust Defensive Training},
author={Zhenyi Wang and Li Shen and Tongliang Liu and Tiehang Duan and Yanjun Zhu and Donglin Zhan and David Doermann and Mingchen Gao},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}




@article{wang2023distributionally,
  title={Distributionally robust memory evolution with generalized divergence for continual learning},
  author={Wang, Zhenyi and Shen, Li and Duan, Tiehang and Suo, Qiuling and Fang, Le and Liu, Wei and Gao, Mingchen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@inproceedings{zhang2024remark,
  title={$\{$REMARK-LLM$\}$: A Robust and Efficient Watermarking Framework for Generative Large Language Models},
  author={Zhang, Ruisi and Hussain, Shehzeen Samarah and Neekhara, Paarth and Koushanfar, Farinaz},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={1813--1830},
  year={2024}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{huo2024token,
  title={Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models},
  author={Huo, Mingjia and Somayajula, Sai Ashish and Liang, Youwei and Zhang, Ruisi and Koushanfar, Farinaz and Xie, Pengtao},
  journal={arXiv preprint arXiv:2402.18059},
  year={2024}
}

@article{dauparas2022robust,
  title={Robust deep learning--based protein sequence design using ProteinMPNN},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{ghazvininejad2019mask,
  title={Mask-predict: Parallel decoding of conditional masked language models},
  author={Ghazvininejad, Marjan and Levy, Omer and Liu, Yinhan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1904.09324},
  year={2019}
}

@inproceedings{kasai2020non,
  title={Non-autoregressive machine translation with disentangled context transformer},
  author={Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and Gu, Jiatao},
  booktitle={International conference on machine learning},
  pages={5144--5155},
  year={2020},
  organization={PMLR}
}

@inproceedings{huang2022improving,
  title={Improving non-autoregressive translation models without distillation},
  author={Huang, Xiao Shi and Perez, Felipe and Volkovs, Maksims},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{alamdari2023protein,
  title={Protein generation with evolutionary diffusion: sequence is all you need},
  author={Alamdari, Sarah and Thakkar, Nitya and van den Berg, Rianne and Lu, Alex Xijie and Fusi, Nicolo and Amini, Ava Pardis and Yang, Kevin K},
  journal={bioRxiv},
  pages={2023--09},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
@inproceedings{du2021order,
  title={Order-agnostic cross entropy for non-autoregressive machine translation},
  author={Du, Cunxiao and Tu, Zhaopeng and Jiang, Jing},
  booktitle={International conference on machine learning},
  pages={2849--2859},
  year={2021},
  organization={PMLR}
}

@article{shih2022training,
  title={Training and inference on any-order autoregressive models the right way},
  author={Shih, Andy and Sadigh, Dorsa and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2762--2775},
  year={2022}
}

@article{gu2017non,
  title={Non-autoregressive neural machine translation},
  author={Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor OK and Socher, Richard},
  journal={arXiv preprint arXiv:1711.02281},
  year={2017}
}

@article{wu2024distortion,
  title={Distortion-free Watermarks are not Truly Distortion-free under Watermark Key Collisions},
  author={Wu, Yihan and Chen, Ruibo and Hu, Zhengmian and Chen, Yanshuo and Guo, Junfeng and Zhang, Hongyang and Huang, Heng},
  journal={arXiv preprint arXiv:2406.02603},
  year={2024}
}
@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Zidek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{ProteinWatermark,
  title={Enhancing Biosecurity with Watermarked Protein Design},
  author={Chen, Yanshuo and Hu, Zhengmian and Wu, Yihan and Chen, Ruibo and Jin, Yongrui and Chen, Wei and Huang, Heng},
  journal={bioRxiv},
  pages={2024--05},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}
@article{liu2024few,
  title={Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt},
  author={Liu, Chenxi and Wang, Zhenyi and Xiong, Tianyi and Chen, Ruibo and Wu, Yihan and Guo, Junfeng and Huang, Heng},
  journal={arXiv preprint arXiv:2403.09857},
  year={2024}
}

@article{chen2020non,
  title={Non-autoregressive transformer for speech recognition},
  author={Chen, Nanxin and Watanabe, Shinji and Villalba, Jes{\'u}s and {\.Z}elasko, Piotr and Dehak, Najim},
  journal={IEEE Signal Processing Letters},
  volume={28},
  pages={121--125},
  year={2020},
  publisher={IEEE}
}
@inproceedings{peng2020non,
  title={Non-autoregressive neural text-to-speech},
  author={Peng, Kainan and Ping, Wei and Song, Zhao and Zhao, Kexin},
  booktitle={International conference on machine learning},
  pages={7586--7598},
  year={2020},
  organization={PMLR}
}

@article{chen2021nast,
  title={NAST: Non-autoregressive spatial-temporal transformer for time series forecasting},
  author={Chen, Kai and Chen, Guang and Xu, Dan and Zhang, Lijun and Huang, Yuyao and Knoll, Alois},
  journal={arXiv preprint arXiv:2102.05624},
  year={2021}
}

@article{tang2021probabilistic,
  title={Probabilistic transformer for time series analysis},
  author={Tang, Binh and Matteson, David S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23592--23608},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{kirchenbauer2023reliability,
  title={On the reliability of watermarks for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Shu, Manli and Saifullah, Khalid and Kong, Kezhi and Fernando, Kasun and Saha, Aniruddha and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2306.04634},
  year={2023}
}

@article{wen2024nonconvex,
  title={Nonconvex Dantzig selector and its parallel computing algorithm},
  author={Wen, Jiawei and Yang, Songshan and Zhao, Delin},
  journal={Statistics and Computing},
  volume={34},
  number={6},
  pages={180},
  year={2024},
  publisher={Springer}
}

@article{mao2024watermark,
  title={A Watermark for Low-entropy and Unbiased Generation in Large Language Models},
  author={Mao, Minjia and Wei, Dongjun and Chen, Zeyu and Fang, Xiao and Chau, Michael},
  journal={arXiv preprint arXiv:2405.14604},
  year={2024}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Y},
  journal={arXiv preprint arXiv:2001.08210},
  year={2020}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{piet2023mark,
  title={Mark my words: Analyzing and evaluating language model watermarks},
  author={Piet, Julien and Sitawarin, Chawin and Fang, Vivian and Mu, Norman and Wagner, David},
  journal={arXiv preprint arXiv:2312.00273},
  year={2023}
}

@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}

@article{tu2023waterbench,
  title={Waterbench: Towards holistic evaluation of watermarks for large language models},
  author={Tu, Shangqing and Sun, Yuliang and Bai, Yushi and Yu, Jifan and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2311.07138},
  year={2023}
}

@inproceedings{see-etal-2017-get,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail  and
      Liu, Peter J.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}


@InProceedings{bojar-EtAl:2016:WMT1,
  author    = {Bojar, Ond
{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
  title     = {Findings of the 2016 Conference on Machine Translation},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {131--198},
  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
}

@article{chen2024watermark,
  title={A Watermark for Order-Agnostic Language Models},
  author={Chen, Ruibo and Wu, Yihan and Chen, Yanshuo and Liu, Chenxi and Guo, Junfeng and Huang, Heng},
  journal={arXiv preprint arXiv:2410.13805},
  year={2024}
}