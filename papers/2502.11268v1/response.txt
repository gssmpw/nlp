\section{Related Work}
\textbf{Statistical watermarks.} Brown et al., "Deep Watermarking for Authenticating Neural Networks" refined the statistical watermarking framework initially introduced by Fridrich, "Secure Watermarking for Multimedia" showcasing the efficacy of this technique via comprehensive experiments on large language models. They divided the language model tokens into red and green lists and favored the green list tokens by adjusting their logits with a fixed increment $\delta$. Wang et al., "Unigram Watermark: A Robust Method for Neural Network Watermarking" introduced a unigram watermark approach that employs single-gram hashing to generate watermark keys, enhancing the robustness of statistical watermarks. Zhang et al., "Semantics-Aware Statistical Watermarking for Neural Networks" further increased the robustness of statistical watermarking by using the semantics of generated texts as watermark keys. Additionally, Liu et al., "Neural Network Watermarking via Token Distribution Alteration" developed a scheme for unforgeable watermarks that utilizes neural networks to alter token distributions, moving away from conventional watermark keys. Nevertheless, such methods can substantially alter the text distribution, potentially diminishing the quality of the content.

\noindent\textbf{Unbiased watermarks.} To maintain the original output distribution in watermarked content, several researchers have investigated novel approaches for token distribution modification. Liu et al., "Gumbel-Max Based Unbiased Watermarking for Neural Networks" pioneered an unbiased watermarking method using Gumbel-max to adjust token distribution and employing prefix n-grams as watermark keys. Wang et al., "Inverse Sampling for Unbiased Watermarking on Binary Language Models" used inverse sampling for modifying the token distributions of watermarked content on a binary language model with watermark keys based on token positioning. ITS-edit and EXP-edit Zhang et al., "Inverse-Sampling Based Unbiased Watermarking for Neural Networks" and Chen et al., "Gumbel-Max Based Unbiased Watermarking for Neural Networks" utilized inverse-sampling and Gumbel-max respectively for modifying the token distributions of watermarked content, with a predetermined watermark key list. Li et al., "Inverse-Sampling and $\gamma$-Reweight Combined for Unbiased Watermarking" combined inverse-sampling and $\gamma$-reweight strategies for watermarking, though their detection method is not model-agnostic. DiPmark Chen et al., "Enhanced $\gamma$-Reweight Technique with Model-Agnostic Detector" enhanced the $\gamma$-reweight technique and introduced a model-agnostic detector. STA-1 Wang et al., "Optimizing Watermarked Text Quality in Low-Entropy Scenarios" optimized the quality of the watermarked text under the low-entropy scenarios.