\section{Related Work}
\label{sec:related_work}
Our work explores structure transformations for adversarial prompts, a relatively underexplored domain while building on extensive research in content-based adversarial prompting. We categorize related work into three primary areas, namely, generalization gaps, prompt optimization, and prompt mining.

\heading{Generalization Gaps.} Research in this category focuses on exploiting mismatches between language model pre-training and safety-aligned fine-tuning data. Attackers transform harmful queries into formats seen during pre-training but less frequently during safety tuning, thus bypassing safety filters. Simple techniques include translation into low-resource languages~\cite{deng2024multilingual}, while more advanced methods employ transformations such as ASCII art~\cite{jiang2024artprompt} and novel encoding schemes~\cite{Handa2024WhenCompetencyinRO}. Some attacks also leverage LLMs' code-processing abilities by exploiting function-call interfaces~\cite{wu2024dark}, injecting code~\cite{kang2024exploiting}, or rephrasing prompts as code-completion tasks~\cite{lv2024codechameleon}. These approaches illustrate the broad vulnerability of language models to shifts in data representation.

\heading{Prompt Optimization.} Prompt optimization focuses on the systematic refinement of adversarial prompts to enhance their effectiveness. Basic approaches use LLMs to iteratively generate and improve prompts~\cite{chaojailbreaking}.
More advanced techniques use structured search and optimization algorithms to discover effective attack patterns, including Greedy Coordinate Gradient (GCG)~\cite{zou2023universal} and AutoDAN~\cite{liu2023autodan}, which enhance gradient-based optimization with interpretability improvements.
However, token-level optimization often assumes white-box access to the models and hundreds of iterative calls to the LLM \cite{liao2024amplegcg,jones2023automatically}.
%AmpleGCG~\cite{liao2024amplegcg} further develops GCG by generating hundreds of adversarial prompt variants, improving efficiency across different models. Token-level optimizations, such as those in GCG and ARCA~\cite{jones2023automatically}, have also proven highly effective, with precise token modifications leading to a substantial impact on model behavior. These advancements demonstrate the adaptability of adversarial prompt optimization, particularly for white-box models.

\heading{Prompt Mining.} Mining for adversarial prompts leverages large-scale data and systematic exploration to uncover novel attack patterns. WildTeaming~\cite{jiang2024wildteaming} uses human-LLM conversations to identify naturally occurring adversarial prompts, providing real-world insights into emergent attack strategies. Rainbow Teaming~\cite{samvelyan2024rainbow} expands on this by systematically exploring attack dimensions like risk category and style to identify vulnerabilities through grid search across the feature space. Methods like FuzzLLM~\cite{yao2024fuzzllm} and GPTFUZZER~\cite{yu2023gptfuzzer} further enhance this approach by applying genetic algorithms and mutation-based techniques to refine the prompts, demonstrating their utility in black-box settings where model gradients are inaccessible.

Our work is similar to studies in generalization gaps but diverges from existing research by focusing on structure transformations rather than traditional content transformations. While methods like CodeAttack~\cite{ren2024codeattack} explored specific transformations, our systematic framework is the first to analyze structural vulnerabilities across diverse syntax spaces, achieving perfect attack success with minimal attempts. Additionally, our evaluations reveal that SOTA defenses, such as deliberative alignment struggle against these structure-based attacks, underscoring significant gaps in current defenses for mitigating this new attack vector.