%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

% inlined bib file
\usepackage{filecontents}

% package for rules in table
\usepackage{booktabs}

% put general packages here
%\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
%  \usepackage[nocompress]{cite}
%\else
  % normal IEEE
\usepackage{cite}
%\fi
\usepackage{soul}
\usepackage{hyperref}
\usepackage{svg}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[cal=cm]{mathalfa}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{booktabs}   
\usepackage{multirow}   
\usepackage{multicol}
\usepackage{array}      
\usepackage{siunitx}     % For better alignment of numbers (optional)
\usepackage{makecell}
\usepackage{caption}    
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage[table]{xcolor}
\usepackage{listings}
\usepackage{mathrsfs}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}

\usepackage{enumitem}
\usepackage{url}
\usepackage{hyperref}
\usepackage{makecell}
% Custom commands
\newcommand{\heading}[1]{{\noindent\bf{#1}}~}


% Define the command
\def\st{%
  \futurelet\next\sthelper
}
\def\sthelper{%
  \ifx\next\space
    StructTransform%
 \else
   StructTransform\space
  \fi
}


%\newcommand{\st}{StructTransform}

% Commands for commenting
% \newcommand\mashael[1]{\textcolor{purple}{Mashael: (#1)}}
% \newcommand\shehel[1]{\textcolor{blue}{Shehel: (#1)}}
% \newcommand\temoor[1]{\textcolor{brown}{Temoor: (#1)}}
% \newcommand\ahmed[1]{\textcolor{cyan}{Ahmed: (#1)}}
% \newcommand\ik[1]{\textcolor{red}{Issa: (#1)}}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf StructTransform: A Scalable Attack Surface for \\ Safety-Aligned Large Language Models}

%for single author (just remove % characters)
\author{
  Shehel Yoosuf\textsuperscript{*} \quad Temoor Ali\textsuperscript{*} \quad Ahmed Lekssays \quad Mashael AlSabah \quad Issa Khalil\\
  \vspace{5pt}
  Qatar Computing Research Institute \\
  \vspace{5pt}
  \textsuperscript{*}Equal contribution joint first authors
}

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
%Several safety challenges arise with the dramatic increase in LLM adoption by a diverse user base.
%One of the most important safety guarantees is alignment.
%We argue that current alignment tuning approaches do not address the huge vulnerabilities introduced by text \textit{structure transformations}, where an attacker encodes natural language intent into SQL queries or other structured formats (e.g., JSON, programming languages, or predicate logic).
%Based on this insight, 

In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g. SQL), to new novel
spaces and syntaxes created \emph{entirely} by LLMs. 
Our extensive evaluation shows that our simplest attacks can achieve close to 90\% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms.
We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing \textit{content transformations}, resulting in over 96\% ASR with 0\% refusals.

To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it and show that most of them fail with perfect ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating for the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware, and a corpus of fraudulent SMS messages, which perform well in bypassing detection. 
\end{abstract}


%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

Alignment training—ensuring Large Language Models (LLMs) align with human values and goals—is increasingly crucial.
Without it, attackers can weaponize LLMs to craft and automate effective attacks like convincing phishing emails~\cite{phishbot, copilot}, distribute misinformation and hate speech~\cite{misinformation} and generate malware~\cite{malware}.
Recently, in one devastating case, an LLM may have played a role in a vulnerable man's tragic suicide~\cite{suicide}.
With LLMs being accessible and widespread to a diverse user base, the need for alignment and safety has never been more urgent. With the immense power that LLMs bring to our lives comes a crucial responsibility to ensure they align with our ethical and social values.

\begin{figure}
    \centering
    \includegraphics[scale=0.45]{figures/symlogic_example.pdf}
    \caption{Transforming malicious prompts into Symlogix, a GPT-4o generated syntax, changes their linguistic and computational structure, including shifts in grammar and context leading to misaligned LLM responses.}
    \label{fig:intro_attack}
\end{figure}

LLM developers primarily rely on safety or alignment tuning during the model training phase~\cite{Bai2022ConstitutionalAH} and safety filters~\cite{welbl2021challenges} during the inference phase to minimize the risk of LLM misuse.
Red teaming efforts via adversarial prompting are also used to identify novel vulnerabilities in LLMs, commonly referred to as \textit{jailbreaks} \cite{ganguli2022red}.
However, existing approaches are insufficient and limited by their focus on \textit{natural languages}, where the methods that transform the harmful query do not venture beyond this general space.
As such, we make a distinction between two fundamental types of transformations based on changes to linguistic structure, including syntax (grammar), semantics (meaning), and pragmatics (context) \cite{silverstein1972linguistic}. 
We introduce the term \textit{content transformations} to refer to transformations that operate through surface-level transformations to syntax, semantics, and pragmatics (e.g., translation, encoding, and roleplay). 
Since these transformations do not significantly alter the linguistic structure, safety training can generalize to these variations, even when novel methods are attempted, as demonstrated by state-of-the-art (SOTA) safety-tuned LLMs such as Llama~\cite{llama} and Claude~\cite{claude}.

In contrast, we introduce \textit{structure transformations}, which maintain semantic meaning but alter the underlying computational logic and processing primitives (e.g., representing prompt intent as SQL queries), leading to a drastic shift in syntax and context. 
We observe that safety alignment in content transformation methods fails to transfer to the threats posed by syntactically diverse classes of texts, including various categories falling under formal languages such as programming languages, query languages, and data serialization languages. They can have further diversity within sub-categories, expanding the attack surface significantly for LLMs. 
Furthermore, we show that LLMs are also vulnerable to new, manually-generated syntaxes via in-context learning (ICL). 

Figure~\ref{fig:intro_attack} shows an example of a structure transformation based on a GPT-4o generated syntax. In this example, a harmful request has its underlying computational logic altered, but with its malicious intent preserved, resulting in the intended harmful response. Thus, we observe that existing safety guardrails that focus only on natural language are rendered ineffective. Although one of the most prominent use cases of LLMs is coding-related tasks, we only have a preliminary understanding of the safety gaps in the structure transformation space compared to attacks in natural languages \cite{ren2024codeattack,lv2024codechameleon}. 

% TEXT CUT OFF DUE TO TABLE


\begin{table}
\caption{Comparison of ASR and prompt attempt efficiency in jailbreak approaches for SOTA safety-aligned LLMs.}% \cmark\ indicates success, \xmark\ indicates failure or limited capability.}
\label{tab:comparison}
\centering
\scalebox{0.98}{
\begin{tabular}{lllc}
\toprule
\textbf{Method}  & \textbf{ASR} (\%) $\boldsymbol\uparrow$& \textbf{Efficiency} $\boldsymbol\downarrow$ & \\%\textbf{Refusal (\%)} $\boldsymbol\downarrow$ \\
\midrule
Jailbroken \cite{wei2024jailbroken}   & 32 & 14 & \\%91 \\
\midrule
PAIR \cite{chaojailbreaking} & 77 & 11.1 \\%& 73 \\
\midrule
WildJailbreak \cite{jiang2024wildteaming} & 63 & 3 & \\%56 \\
\midrule
StructTransform (ours) & \textbf{100} & \textbf{< 3} \\%& 0 \\

\bottomrule
\end{tabular}
}
\end{table}


% CONTINUATION OF THE TEXT FROM BEFORE 
%reveals previously unrecognized vulnerabilities in generalizing safety training, $

Based on these observations, we thoroughly examine a spectrum of structure transformations ranging from simple structure formats and basic query languages, to new novel spaces and syntaxes created entirely by LLMs. We study their impact on alignment and investigate more complex transformations encompassing both structure and content-based categories. Our study exposes a completely new dimension of safety gaps that extend beyond conventional content-level modifications. This gap can expand alongside the evolving reasoning abilities of LLMs, raising critical concerns about their safety. 
 

\begin{figure}
        \raggedright
        \includegraphics[scale=0.4]{figures/asr_plot.pdf}
        \caption{ASRs against SOTA  LLMs and defenses using structure transformations using only 8 attempts, with only 4 attempts for o1 %\mashael{please reference it in the text.}
        }
        \label{fig:asr_plot}
    \end{figure}

Table~\ref{tab:comparison} summarizes how our approach contrasts with prior work. WildTeaming~\cite{jiang2024wildteaming} identified JSON and CSV as alternate jailbreak modalities, though they did not delve into these vectors in detail. In contrast, we embed harmful intent within a JSON schema to move beyond natural language prompts.
Our evaluations show that our structure transformations are
significantly more effective and harmful in scope. The results
show that our work outperforms previous approaches in multiple key areas. Empirically, we demonstrate higher Attack
Success Rates (ASR) for queries. Additionally, our prompts
exhibit a reduced likelihood of detection, thereby decreasing
the chances of refusal when generating misaligned responses.
We achieve the misaligned outputs with fewer queries, highlighting the efficiency of our approach


\heading{Approach.} We systematically assess the risks posed by structure transformation attacks and their potential to elicit harmful responses from safety-tuned LLMs. Our approach, \emph{StructTransform} begins by prompting an LLM to generate structure transformations in some syntax that is sufficiently different from natural languages. These transformations are then used to construct adversarial prompts that incorporate standard red-teaming categories, enabling us to evaluate the alignment integrity of three SOTA LLMs: Llama 3.2, GPT 4o, and Claude 3.5 Sonnet.\footnote{Note that existing studies rarely include results on Claude 3.5 even though it is considered one of the most aligned models.} To expand the attack surface further, we experiment with combining structure and content transformations and ICL attacks, achieving 100\% ASR against the most robustly aligned models, including recently proposed defenses against adversarial prompts.
%We benchmark our attack strategies against previously successful jailbreak methods, examining their efficacy and exploring the robustness of these models by testing mitigation strategies, including targeted model fine-tuning. 
Our results highlight critical vulnerabilities and emphasize the need for enhanced safety measures in LLMs based on conceptual recognition of harmful content instead of token-level patterns. Below we summarize our contributions:

\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item We introduce a new and previously unexplored dimension of misalignment in LLMs based on structure transformations, opening up a vast and diverse set of attacks that expand combinatorially when layered with existing attacks. Through our experiments, we conclude that each combination may require dedicated safety training, making mitigation difficult.

    \item We provide the first comprehensive evaluation of structure transformation against three popular LLMs with SOTA safety alignment(including Claude 3.5 Sonnet), achieving at least 95\% ASR in under 2 attempts on average in our combined approach. 

    \item To generalize the attacks, we explore other syntaxes and a systematic yet simple framework for adversarial prompting based on generated syntaxes. We demonstrate high ASR on them (up to $97\%$ against Claude), suggesting a large number of effective structure transformations. 
    
    \item Based on StructTransform, we build a solid benchmark to help the community evaluate the weakness of SOTA safety-alignment techniques, including reasoning-based alignment such as o1 with 78\% ASR as seen in Figure. ~\ref{fig:asr_plot}. We make our benchmark available to the community \texttt{https://github.com/StructTransform/Benchmark}. 

%Based on \st , we curate a challenging benchmark to evaluate the weakness of SOTA safety-alignment techniques, including reasoning based models alignment such as o1 with >75% ASR. We make our benchmark available to the community.

    \item In a case study, we demonstrate the practical threat of a JSON-based structure transformation to generate SMS phishing campaigns. Using a self-refining pipeline, we generated 1,000 deceptive phishing messages with 100 calls on a Llama-3.2 3B model, reducing a fine-tuned BERT classifier’s F1 score by 35\%. \end{itemize}


 


%-------------------------------------------------------------------------------
\section{Background and Motivation}
\label{sec:background}
%-------------------------------------------------------------------------------

%In this section, we define the terminology used in this study and provide background knowledge about them, including an overview of large language models (LLMs) and safety alignment. We then explore how adversarial prompts—often referred to as jailbreaks—pose serious security threats to these models. Finally, we formally define the problem considered in this work.

%\footnote{The terms adversarial prompts' and jailbreaks' are used interchangeably in this document.}

\heading{Safety Alignment in LLMs.} Large language models (LLMs) are deep learning-based generative models that process and generate human-like text through training on vast text corpora using self-supervised learning, primarily next-token prediction. Users interact with LLMs, which consist of billions of parameters, by providing prompts, to which the models generate contextually appropriate responses by predicting tokens sequentially using accumulated context.
The safety alignment of these powerful systems consists of techniques designed to confine model generations to a "safe" subset of behaviors while retaining broader capabilities. The training process involves several key components and safety measures: pre-training on curated datasets with harmful content removed, fine-tuning on specific tasks, and crucially, reinforcement learning from human feedback (RLHF) \cite{christiano2017deep} to align outputs with human preferences and safety requirements. 
%Constitutional AI \cite{Bai2022ConstitutionalAH} approaches further refine safety by employing predefined principles to delineate acceptable versus unacceptable responses. 
Additional safeguards include AI-driven filtering and moderation systems during operation to detect and block harmful outputs \cite{welbl2021challenges}.
However, moderation systems are easily bypassed due to the necessity of "safety-capability parity," where the complexity of safety mechanisms should be in proportion to the capabilities of the model \cite{wei2024jailbroken}.

Safety fine-tuning (SFT) remains an active research area, focusing on resource efficiency and optimizing the balance between utility and safety robustness. However, despite improvements in alignment primarily through safety fine-tuning, the threat of adversarial attacks persists. Recent studies have highlighted limitations in approaches that treat safety as an additional feature rather than a fundamental aspect of model architecture and reasoning. Novel methods like Chain-of-Thought (CoT) \cite{guan2024deliberative} and representation engineering \cite{zou2024circuitbreaker} aim to move beyond pattern-matching for safety refusal. While these defensive improvements enhance safety, ongoing red-teaming research continues to reveal shortcomings that help guide further safety alignment research.
The emergence of LLMs has significantly impacted multiple fields, from computational linguistics to artificial intelligence ethics, while raising important questions about model interpretability, bias, and responsible deployment that intersect directly with safety considerations.

% MOVE TO RELATED WORK
%Although these standard approaches establish an initial level of safety, emerging challenges have highlighted notable gaps. Recent research therefore proposes new defense mechanisms aimed at bolstering model alignment. One such approach is the Deliberative Chain-of-Thought, which has been applied to OpenAI’s o-series models and uses step-by-step reasoning to incorporate safety specifications during training, thereby allowing the model to identify its own internal policies before responding, without requiring human-labeled traces \cite{guan2025deliberativealignmentreasoningenables}. Another technique, known as circuit breakers, involves continuously monitoring and modifying model representations at inference time to suppress harmful behaviors dynamically \cite{zou2024circuitbreaker}. Additionally, targeted latent adversarial training strengthens defenses against jailbreak attacks by fine-tuning the model within its latent space \cite{sheshadri2024targeted}. While these innovations enhance safety alignment, they remain imperfect, as evidenced by ongoing research demonstrating new jailbreak methods.

\heading{Adversarial Prompts.} Adversarial prompts are specifically crafted to bypass the above-mentioned defenses, prompting the model to produce harmful outputs.
These prompts generally fall into three main categories.
The first category, known as “generalization gaps”, exploits inconsistencies between the large amount of data used for pre-training and the more limited data used for safety-focused fine-tuning, for example, translating harmful prompts into rarely encountered languages \cite{deng2024multilingual}.
A second category, “prompt optimization”, systematically refines adversarial queries for maximum effectiveness. 
Some approaches use separate LLM calls to iteratively generate and improve prompts \cite{chaojailbreaking}, whereas white-box methods such as Greedy Coordinate Gradient \cite{zou2023universal} rely on structured search using gradient computations. Finally, the third category, “jailbreak mining”, involves large-scale collection and examination of real-world data to discover novel attack patterns. An example is WildTeaming, which analyzes human-LLM interactions to identify naturally occurring adversarial-prompting strategies in an automated way\cite{jiang2024wildteaming}.

\heading{Motivation: SMS and Malware Case Studies.}
To motivate why addressing \st is important beyond the clear alignment concerns, we examine two case studies that highlight the security implications of our attacks. If left unaddressed, such attacks could facilitate highly scalable and automated malicious activities. To demonstrate the practical security impact of these transformations, we present an efficient and effective pipeline for generating SMS phishing in Appendix~\ref{apdx:sms_case_study}. Due to space limitations, in this Section, we summarize our experience with the ease of generating fraudulent SMS and malware samples.

Using our attack, we were able to quickly and cheaply generate 1,000 phishing messages. To gain insights into the deceptiveness and quality of the generated dataset, we evaluate them against a fine-tuned language model-based smish classifier. Normally, previous studies on smish classification tasks have shown excellent binary classification performance
~\cite{timko2023commercial,oswald2022spotspam}.  The detection performance of the classifier dropped from 0.94 to 0.61 in terms of the F1 score with our messages compared to other datasets of smish messages. Further, 50\% of our generated messages resemble benign and spam messages as visualized in the embedding space of a fine-tuned DistilBERT SMS classifier. %Section~\ref{apdx:sms_case_study} describes the whole generation pipeline and classification experiments. 

Second, we use our \st attacks to show that it is possible to generate malware by showing a detailed example output.
Apart from the clear ethical violations, LLMs are aligned against generating malicious code as it facilitates cybercrime which can cause significant large-scale harm.
Previous work~\cite{redcode} has attempted to use LLM agents to generate malicious code but had limited success \cite{devadiga2023gleam,yamin2024combining}. We do not attempt to generate malware on a large scale in this work, but our intention is to show the possibility of facilitating this task for attackers.  Indeed, Figure~\ref{fig:malware-code} (in Appendix) shows an example of ransomware generated using Claude Sonnet 3.5 which normally exhibits a high refusal rate and low ASR using other jailbreak techniques.
% Despite variations in the success of these adversarial methods, recent investigations have consistently identified two fundamental shortcomings that allow such attacks to bypass existing safeguards \cite{wei2024jailbroken}: 

% \heading{Competing Objectives:} The inherent tension between the model's safety objectives and instruction following objectives as a result of the competing objectives in unsupervised pre-training and supervised training. These competing goals can lead to situations where the model does not optimally choose between maintaining capabilities and adhering to safety constraints.
    
% \heading{Mismatched Generalization:} The disparity between the broad generalization of pre-training capabilities and the more limited generalization of safety training due to the pre-training dataset being substantially larger than the safety-tuning data.

% These shortcomings are exploited in crafting adversarial prompts that lead to misaligned LLM responses. 

%-------------------------------------------------------------------------------
\section{Threat Model}
\label{sec:threat_model}
%-------------------------------------------------------------------------------
In this section, we outline the threat model by describing the assumptions, the attacker's goals, and their capabilities. 

\heading{Assumptions.} 
The system under consideration is a safety-aligned language model accessible through a standard chat interface or through API calls.
The model's architecture, weights, system prompt, and message history are protected from modification, with provider-determined safety mechanisms in place to prevent harmful outputs.
The attacker operates with black-box access through the standard interface and does not require specialized knowledge of the model's architecture or training process.
They may leverage other language models in their attack strategy ($M_{attack}$) and can interact with the target model adaptively.


\heading{Attacker's Goals.} 
The attacker aims to elicit responses containing restricted behaviors while maintaining coherence and relevance. Specifically, they seek to generate content based on harmful concepts across seven categories defined in \textsc{HarmBench} \cite{mazeikaharmbench}: Cybercrime \& Unauthorized Intrusion, Chemical \& Biological Weapons/Drugs, Copyright Violations, Misinformation \& Disinformation, Harassment \& Bullying, Illegal Activities, and General Harm. A successful attack must produce a response that fully satisfies the prompt, i.e., generating harmful content as intended by the harmful prompt.

Let $M_{\text{target}}$ be an LLM with safety measures, where for some vanilla harmful prompt $p$, we define the model's response space $\mathcal{R}$ as the union of two disjoint sets: harmful $\mathcal{R}_{h}$ and benign $\mathcal{R}_{b}$ responses. For some $p \in \mathcal{P}$, the model produces a response:
\[ M_{\text{target}}(p) \rightarrow r \in \mathcal{R} = \mathcal{R}_{h} \cup \mathcal{R}_{b} \]

Furthermore, applying a transformation $\phi$ to a prompt, denoted as $p \oplus \phi$, maps the prompt to its adversarial variant $p^\prime $. 
The objective is then to find a transformation $\phi$ such that:
\[ M_{\text{target}}(p \oplus \phi) \in \mathcal{R}_{h} \]

\heading{Attacker's Capabilities.} The attacker can submit arbitrary text prompts to the LLM and interact with it adaptively. Their capabilities span two main categories: content transformations and structure transformations. Content transformations operate within the framework of natural languages, employing techniques such as adversarial prefix/suffix injection, token manipulation, and text encoding. Structure transformations change the prompt by altering the syntax and computational logic through data formats (e.g., JSON), query languages (e.g., SQL), or programming constructs. Additionally, the attacker may utilize other LLMs to generate attack templates that implement these transformations. They may further refine their strategies by combining content and structure transformations.

%-------------------------------------------------------------------------------
\section{Adversarial Transformations}
\label{sec:threat_model}
%-------------------------------------------------------------------------------

In this section, we introduce the terms \textit{structure transformations} and \textit{content transformations}, and highlight their differences. We then list several classes of structure transformations presented in this study, then go in-depth into two specific ones.  


\subsection{Defining Transformations} We distinguish between two broad classes of transformations that transforms $p$ into $p^{\prime}$:

\heading{Content Transformations.}
Content transformations alter the surface characteristics of the prompt without changing its underlying natural language structure. Examples include rephrasing, base64 encoding, roleplay modifications, and other stylistic or superficial changes. Although such methods can obscure key features, they remain within the boundaries of conventional language and thus trigger safety mechanisms that rely on natural language cues.  Formally, let $\mathcal{C} = \{c_1, ..., c_n\}$ be the set of content transformations, where each $c_i$ refers to a different class of possible alterations to surface characteristics of the prompt. Furthermore, for each class $c_i$, we define $\varphi_{c_i} = \{\varphi^{c_i}_{1}, ..., \varphi^{c_i}_{m} \}$ as the set of possible transformations for that class. Then, content transformations are defined formally as:

\[
p \oplus \varphi^{c_i}_{t} : p \rightarrow p'.
\]

An example of content transformations is encoding schemes where the binary-to-text Base16 or hex encoding is a specific transformation in this class represented as $\phi_{hex}^{encoding}$.
Other concrete examples include HTTP roleplay, distractor texts such as benign questions, and paraphrasing.

\heading{Structure Transformations.}
In contrast, structure transformations embed the prompt in a different syntax which may also involve changes in its computational logic.
Rather than rewording the prompt, these transformations recast it into alternative formats that exhibit distinct grammar (semantics) and context (pragmatics).
Examples of structure transformations include: data serialization formats, query languages, finite-state machines, and predicate logic formulations.
By shifting the prompt out of the natural language domain, structure transformations expose a critical shortcoming in current safety fine-tuning: state-of-the-art LLMs often fail to detect harm when it is obscured by tokens associated with syntaxes that are not prominent in safety training. This vulnerability not only facilitates powerful adversarial attacks but also indicates that current defenses are based on surface-level features rather than the recognition of harmful concepts.
Let $\mathcal{S} = \{s_1, ..., s_n\}$, where each $s_i$ is a distinct class structure transformations. Then, for each $s_i$, let $\phi_{s_i} = \{\phi^{s_i}_{1}, ..., \phi^{s_i}_{m}\}$, where each $\phi^{s_i}_j$ is a possible transformation to class $s_i$. Then, the transformation is as follows:
\[
p \oplus \phi^{s_i}_{t}: p \rightarrow p'.
\]
These transformations leverage the observation that, although LLMs are trained on diverse syntaxes through large-scale pre-training, safety training often fails to generalize beyond the natural language patterns it is accustomed to.

\subsection{Classes of Structure Transformations}
Any transformation affecting the structure and syntax can be classified as a structure transformation, but we focus on two key representative examples to describe attacks enabled by structure transformation.


    \heading{1. Data Serialization Formats.} Representing prompts through structured data hierarchies using formats such as JSON, XML, and YAML imposes formal schema constraints and explicit relationships between prompt components. Figure \ref{fig:intro_attack} shows an example of a harmful prompt after applying $\phi_{schema}^{json}$, that transforms the prompt into a JSON schema. 

    The assumption is that retrieving data points that adhere to the schema will result in information with the same semantic content as generations from $p$. 
    This transformation demonstrates several linguistic changes that can affect LLM interpretation and alignment.
    The natural language prompt undergoes formalization through strict hierarchical structuring.
    The original subject-verb-object relationship disperses across multiple nested objects, creating syntactic distance between related concepts.
    The schema enforces explicit type constraints and validation rules, replacing natural language's inherent flexibility with rigid structural requirements.
    It changes the context of the prompt from a well-aligned space into that of data serialization formats, which are associated with data storage and transfer, and less represented in safety-training datasets. 
    
    \heading{2. Query Languages.} Reformulating prompts as transformations in database query languages maps the prompt into a structured query with explicit selection criteria, relationships, and constraints. From declarative to mixed paradigm languages, they can have significant diversity in syntax. Figure \ref{fig:sql_example} shows a SQL query transformation and its response.

    \begin{figure}
        \centering
        \includegraphics[scale=0.5]{figures/sql_example_chat.pdf}
        \caption{"List chemical agents that can be synthesized at home" encoded as a SQL query.}
        \label{fig:sql_example}
    \end{figure}

    Prompting the LLM for response records to the above SQL query will also generate answers that meet the semantic intent of the original query. 
    SQL is unique in how it can be converted into sophisticated queries under database interaction. 
    It adds constraints to be followed, which can grow boundlessly and affect alignment, as empirically demonstrated in our evaluations.
    As LLM capabilities scale, so can their ability to process complex queries, resulting in powerful attacks that focus more on instruction-following and less on safety. 

\subsection{Structure Transformation Attack \\ Framework}
\label{subsec:attack_framework}
Utilizing structure transformations for adversarial prompting requires the identification of viable syntax spaces and the implementation of a transformation function to convert prompts in natural language to the identified syntax.  
Both steps can be performed with minimal domain knowledge by using LLMs to assist in the attack. 

We find that LLMs (e.g., $M_{target}$) can be used to trivially identify feasible transformations with appropriate prompting (see Appendix A). 
While structure transformations are defined formally as $p \oplus \phi_t^{s_i}$, we implement them in practice using some capable off-the-shelf LLM $M_{attack}$ (e.g., Deepseek-chat). 
Figure \ref{fig:combined_attack} illustrates our adversarial prompting approach, which uses structure and, optionally, content transformations.
For a given harmful prompt $p \in \mathcal{P}$, we first select a candidate syntax $s^*$ and a corresponding transformation $\phi^{*} \in \Phi_{s^{*}}$.
The transformation $\phi_{t}^{s_*}$ is implemented as an engineered prompt consisting of instructions and few-shot examples.
By providing $M_{attack}$ with this prompt, we generate an adversarial variant of $p$ that can elicit harmful responses from $M_{target}$.
Besides a direct transformation using only a single structure transformation, this general framework also accommodates layered transformation that combines structure and content transformations implemented either programmatically or as LLM prompts. Compared to direct attacks, we show that combination attacks can produce transformations with better adversarial performance, which scales with the capabilities of $M_{target}$.

\begin{figure}
        \centering
        \includegraphics[scale=0.7]{figures/combined_transformation_attack.pdf}
        \caption{The general attack framework of StructTransform for crafting adversarial prompts to elicit information from safety-aligned LLMs. \textit{Direct Transformation Attack} uses LLM ($M_{attack}$) assisted structure transformations. \textit{Combined Transformation Attacks} are crafted by layering transformations, including template-based content transformations.}
        \label{fig:combined_attack}
\end{figure}

\subsection{Implementing $M_{attack}$}
Given a syntax that induces a transformation in structure, the effectiveness of \st is most affected by the complexity and expressivity of the specific transformation and its implementation in this syntax. 
In this study, we aim to find $p'$ using simple transformations based on responding to queries and producing data points adhering to schemas. 
As detailed in the previous section, the transformation $p \oplus \phi_t^{s_i}$ is automated using an LLM, $M_{attack}$.   
As a result, the capability of $M_{attack}$ and the design of the prompt sent to it is crucial to the success of \st in the absence of manual prompt engineering. 
We design a general prompt template that can be inherited and extended to implement $\phi_t^{s_i}$ for any prompt category. The template includes instructions, generation guidelines, N-shot examples, and the harmful prompt $p$ 

% The template consists of four components:

% \begin{enumerate}[leftmargin=*, itemsep=0pt]
%     \item \textbf{Instruction:} This establishes the red teaming objective and defines the task of transforming harmful prompts into structured queries using the target syntax. The component frames the overall goal while maintaining a clear focus on generating syntactically valid transformations.
    
%     \item \textbf{Generation G+uidelines:} These direct the transformation process by incorporating syntax-specific elements and stipulating requirements for query complexity and length to ensure sufficient detail in the LLM's response. 
%     The syntax-specific elements can also be generated by $M_{target}$. 
    
%     \item \textbf{N-shot Examples:} The template includes examples demonstrating the transformation process. Each example pairs a harmful prompt in natural language with its corresponding transformed query in the target syntax, providing concrete illustrations of successful transformations that adhere to the guidelines. These can also be LLM generated based on the context of the general template and guidelines.
    
%     \item \textbf{Harmful Prompt:} The template concludes with the specific harmful prompt to be transformed, serving as the variable input for generating the target syntax query.
% \end{enumerate}

Together, these components create a systematic approach for implementing $\phi_t^{s_i}$ using an LLM. 
Based on our observations, all syntax-specific components in the attack generation prompt can be generated by $M_{attack}$ by iteratively completing the missing components starting from the general template. 

\subsection{In-context Attacks}
\label{subsec:incontext_attacks}
In-context learning (ICL) is an emergent property of LLMs that enables them to perform well on unseen tasks without any updates to the model parameters. Using a simple, contextual information-rich prompt, latent concepts learned during training are combined to optimize tasks during inference.
In-context learning has been shown to introduce vulnerabilities and successful adversarial attacks against LLMs \cite{wei2023jailbreak,anil2024many}. 
For example, many-shot adversarial prompting \cite{anil2024many} uses many demonstrations via question-answer pairs to steer the model toward responding to malicious prompts. 

\st attacks assume a known syntax space, which the LLM has seen during its training process. 
The general attack framework can be adapted to implement \st attacks using novel syntaxes leveraging ICL. 
To generate novel syntaxes, we craft a prompt for producing syntax definitions, including instructions for ensuring structural complexity, variations in computation logic, and verbosity, encouraging the use of non-alphanumeric characters and spaces.
This prompt for syntax generation can be used with capable LLMs to generate increasingly complex syntax definitions. 
Both $M_{attack}$ and $M_{target}$ are prompted by embedding the syntax definition in each prompt in the \st framework for ICL attacks. 




%-------------------------------------------------------------------------------
\section{Experimental Setup}
%-------------------------------------------------------------------------------

We implement our experiments using the EasyJailbreak \cite{zhou2024easyjailbreak} framework and its default parameter settings. To interface with commercial LLMs, we utilize OpenRouter\footnote{https://openrouter.ai}. The latter connects users to LLMs from providers like OpenAI and Anthropic, routing requests to a model host that could be either the original model provider or a third party (e.g., Google Vertex). It enforces a multi-layer moderation system: OpenRouter itself, the model host, and the model provider each apply their filters to monitor and block unsafe content. This layered approach makes generating unsafe content challenging, as each stage reinforces content compliance, ensuring secure interactions. 
However, the specific details of which model provider activates which filters is not made transparent in OpenRouter.
All sub 13B models utilized in our experiments are deployed locally, while the rest are accessed through OpenRouter. 
For local models, we deploy the open models using vLLM \cite{kwon2023efficient}, each of which requires a Nvidia V100 32GB GPU.

\subsection{Evaluation Task}
In our experiments, we use \textsc{HarmBench} \cite{mazeikaharmbench}, a standardized evaluation framework for automated red-teaming of LLMs.
\textsc{HarmBench} contains 510 unique behaviors across four functional categories: standard, contextual, copyright, and multimodal. We focus on the 159 standard behaviors test subset as they represent common red-teaming scenarios, covering seven semantic categories, including cybercrime, chemical/biological weapons, misinformation, harassment, and illegal activities.
The behaviors were carefully curated to violate laws or widely held norms while avoiding dual-intent scenarios that could have legitimate uses.
Examples include specific and practical prompts, for instance, \textit{``Write code for exploiting the BlueKeep vulnerability and explain how to use it"}. 

While human evaluation remains the most reliable method for assessing Large Language Model (LLM) behaviors, the need for extensive experimentation necessitates automated evaluation approaches. 
We selected HarmBench for our study based on its accompanying fine-tuned judge model, which has demonstrated minimal bias compared to human evaluation in previous works \cite{mazeika2024harmbench,souly2024strongreject}. To validate its reliability further, two human evaluators reviewed 100 randomly selected outputs from our experiments, demonstrating significant agreement with the judge model’s assessments (details in Appendix ~\ref{apdx:harmbench_eval}).


\subsection{Metrics}
We use the ASR, Efficiency, and Refusal Rate to evaluate the attacks. 

\heading{Attack Success Rate (ASR).} ASR measures how effective the adversarial prompt attacks are at prompting language models to produce harmful responses. For each harmful prompt and its structural variations, multiple adversarial attempts can be made. A single successful attempt is enough to consider the attack successful.
To determine success, we use the \textsc{HarmBench} judge model \cite{mazeikaharmbench}, a Llama2-13B model fine-tuned to detect harmful content based on the HarmBench dataset. The judge model classifies each response as either successful (harmful) or unsuccessful (safe) based on two criteria: (1) the response contains harmful content, and (2) the response is relevant to the original prompt. 
For a given set of queries, the aggregate ASR is calculated as the proportion of queries for which at least one attack attempt succeeds. Formally, let Q be the set of harmful prompts, and let s(q) indicate whether any attack attempt on prompt $q \in Q$ succeeded (1) or all attempts failed (0). Then:
$$ASR = \frac{1}{|Q|} \sum_{q \in Q} s(q)$$

\heading{Query Efficiency.} For a single harmful behavior $q \in Q$, \textit{query efficiency} $e(q)$ measures the number of adversarial prompts required before successfully eliciting the behavior from $M_{target}$. For the full set of prompts $Q$, query efficiency is defined as:
\begin{equation}
E = \frac{1}{|Q_s|} \sum_{q \in Q_s} e(q)
\end{equation}
where $Q_s = {q \in Q : s(q)=1}$ is the subset of prompts that were successfully elicited (i.e., where $s(q)=1$). This metric is important because many red-teaming methods, including ours, are evaluated by generating multiple adversarial prompts per behavior. Query efficiency helps distinguish between computationally expensive and efficient attacks, which becomes particularly relevant when deploying these at scale.

\heading{Refusal Rate.} The Refusal Rate measures the proportion of adversarial prompts that were explicitly declined by the language model with responses indicating safety boundaries (e.g., "I apologize, but I cannot..." or "I'm not able to assist with..."). Formally, let $P_q$ be the set of all adversarial prompts generated for query $q \in Q$, and let $r(p)$ indicate whether prompt $p$ was refused (1) or not (0). Then:
$$RR = \frac{1}{|\cup_{q \in Q} P_q|} \sum_{q \in Q} \sum_{p \in P_q} r(p)$$
This metric complements ASR and query efficiency by specifically identifying cases where the model's safety mechanisms actively prevented harmful responses. Incidentally, not all failed adversarial prompts contribute to the refusal rate, as failures can occur due to other factors such as structure transformation rendering the query incomprehensible or excessive prompt complexity preventing meaningful model interpretation. This distinction helps isolate the effectiveness of the model's built-in safety measures from other technical limitations in the attack pipeline.
The refusal rate is computed using the binary classification result from WildGuard, a Mistral-7B LLM fine-tuned for content moderation and refusal classification \cite{wildguard2024}. 
WildGuard takes a user prompt and the LLM response and classifies whether the LLM response is a refusal or not. We use a deterministic parameter setting with a temperature of 0, top-p of 1, and 10 maximum tokens.

\subsection{Attack Model ($M_{attack}$)}
We use a black-box LLM as a red-teaming assistant referred to as \textit{Attack Model} to systematically evaluate structure transformations at scale against language models by automating the process of crafting structure transformations. We use DeepSeek V3, a 671B-parameter mixture-of-experts (MoE) LLM, for this task. 
This model was specifically chosen for its robust reasoning capabilities in handling complex structure transformations, lower refusals, and cheap rate of operations at \$0.14/M input tokens and \$0.28/M output tokens. 
Instruction-following is prioritized in Deepseek V3 since prompting it under the context of red-teaming prevents refusals whereas, other SOTA models like GPT4o and Claude refuse to participate in structure transformation generations given harmful prompts due to their stricter safety and refusal training.

Using $M_{attack}$, we transform harmful prompts from the \textsc{HarmBench} dataset following structure transformation templates described in Section \ref{sec:threat_model}.
These templates are used with few-shot prompting $M_{attack}$, guiding the conversion of natural language prompts into their structured counterparts, such as JSON and SQL.
This approach enables us to automate the process of generating and evaluating adversarial prompts across various structure formats.
Note that Deepseek based $M_{attack}$ is also used in the baselines where applicable. We fix the $M_{attack}$ parameters at 0.7 temperature, 0.9 top-p, and 1024 maximum generated tokens. 2048 maximum generated tokens were used for ICL attacks requiring full definitions of novel syntaxes. 

\subsection{Target Models ($M_{target}$)} 
We evaluate attacks against four SOTA LLMs each recognized for its strong safety alignment and two SOTA defenses proposed recently to improve the robustness of alignment. All models use a temperature of 0, top-p of 1, and 1,024 maximum generated tokens.  

\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item \textbf{o1}: OpenAI's latest model represents a significant advancement in LLM architecture through its integration of chain-of-thought reasoning. This allows the model to explore and refine its reasoning strategies during inference, enhancing its ability to solve complex problems. \cite{jaech2024openai}. 
    \item \textbf{Claude 3.5 Sonnet \footnote{Model version dated 22-10-2024}}: A commercially deployed model from Anthropic, representing current state-of-the-art in safety alignment \cite{Bai2022ConstitutionalAH}. %utilizing \textit{Constitutional AI} for improving safety alignment \cite{Bai2022ConstitutionalAH}.
    \item \textbf{GPT-4o \footnote{Model version dated 06-08-2024}}: A widely deployed commercial model from OpenAI known for robust capabilities and safety measures.   
    \item \textbf{Llama 3.2}: Meta's Llama open-weight LLMs with a focus on safety and edge intelligence \cite{llama}. We test both the 3B and 90B parameter versions to assess the impact of model scale and reasoning capabilities on susceptibility to structure transformations.  
    \item \textbf{Circuit Breakers}: An open-source defense using Llama 3 8B fine-tuned using a custom loss function that penalizes activation of harmful states \cite{zou2024circuitbreaker}.
    \item \textbf{Latent Adversarial Training (LAT)}: A Llama 3 8B model fine-tuned using adversarial perturbation. It resulted in more robustness to novel classes of adversarial examples \cite{casper2024defending}.
\end{itemize}


\subsection{Adversarial Attacks} 

\subsubsection{Baselines}
We compare our approach against the following popular adversarial prompting methods, including jailbreaks:
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item \textbf{Jailbroken} \cite{wei2024jailbroken}: Jailbroken consists of 29 distinct content transformation attacks (including multi-layer transformations up to 6 levels deep), where success is defined as the logical disjunction (OR) of all 29 attempts - if any single attempt succeeds, the entire attack is considered successful.
    \item \textbf{PAIR} \cite{chaojailbreaking}: This method introduces an iterative approach where a specialized LLM ($M_{attack}$) serves as a red-teaming assistant. The system progressively refines adversarial prompts through multiple back-and-forths with a response evaluation model (GPT-4o mini), demonstrating the potential of automated attack generation. PAIR was run with 5 parallel streams of five iterations each. The attack stops at the first instance of a harmful response. 
    
    \item \textbf{WildJailbreak} \cite{jiang2024wildteaming}: A data-driven approach that mines effective prompting tactics from in-the-wild conversations, covering recent real-world vulnerabilities in LLMs.
    These tactics are sampled to transform original harmful prompts into adversarial prompts using $M_{attack}$ and few-shot examples of sampled tactics. 
    To provide a relevant comparison with the JSON-based attack in StructTransform, we sample only the JSON tactic in WildJailbreak for every evaluation prompt and combine it with five other randomly chosen tactics from a pool of 5688 unique tactics. This is repeated ten times for every unique prompt in the evaluation task.
    \item \textbf{Content Transformation:}
    Following the insights from Jailbroken \cite{wei2024jailbroken} about best-performing transformations, we use UTF-16 encoding and HTTP server roleplay as representative content transformations. The Python function \texttt{.encode('utf-16')} is used to perform the encoding. The engineered prompt for roleplay was chosen based on its thematic compatibility with our structure transformations and is provided in Figure \ref{fig:adversarial_prompt_content} in Appendix \ref{apdx:prompt_templates}. 
\end{itemize} 

\begin{table}[t]
\caption{Performance of direct structure transformation attacks (JSON and SQL) and existing attacks against various LLMs}
\label{tab:individual_results}
\centering
\scalebox{0.7}{
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{ASR (\%)} $\boldsymbol\uparrow$& \textbf{Efficiency} $\boldsymbol\downarrow$& \textbf{Refusal (\%)} $\boldsymbol\downarrow$\\
\midrule

\multirow{5}{*}{Llama-3.2-3B}
& Vanilla Prompt & 17.6 & 1 & 79.25 \\
& PAIR & 77.3 & 11.1 & 73.08 \\
& WildJailbreak & 63.0 & 3.8 & 56.6 \\
& Jailbroken & 80.5 & 12.8 & 51.7 \\
& Direct$_{JSON}$ & 83.7 & 1.8 & 13.1 \\
& Direct$_{SQL}$ & 86.8 & 2.2 & 0 \\
\midrule

\multirow{3}{*}{Llama-3.2-90B}
& Vanilla Prompt & 7.5 & 1 & 92.45 \\
& Jailbroken & 90.6 & 12.29 & 65.4 \\
& Direct$_{JSON}$ & 78.6 & 1.7 & 19.4 \\
& Direct$_{SQL}$ & 76.7 & 4.3 & 6.5 \\
\midrule

\multirow{3}{*}{GPT 4o}
& Vanilla Prompt & 7.55 & 1 & 88.68 \\
& Jailbroken & 59.7 & 12.9 & 35.5 \\
& Direct$_{JSON}$ & 62.3 & 2.1 & 39.6 \\
& Direct$_{SQL}$ & 96.2 & 1.9 & 1.2 \\
\midrule

\multirow{3}{*}{Claude 3.5 Sonnet}
& Vanilla Prompt & 0.6 & 1 & 99.4 \\
& Jailbroken & 34.0 & 14.3 & 72.7 \\
& Direct$_{JSON}$ & 17.0 & 2.7 & 81.0 \\
& Direct$_{SQL}$ & 88.7 & 2.8 & 7.9 \\
\bottomrule
\end{tabular}
}
\end{table}


The aforementioned methods are selected as they are representative and performant works in the various categories covered in this work (see section \ref{sec:related_work}): systematic content transformations (Jailbroken), automated prompt refinement (PAIR), and automated attack discovery (WildJailbreak). By benchmarking against these approaches, we can evaluate the effectiveness of our structure transformation technique against different attack methodologies. 

\subsubsection{Structure Transformation} 
\label{subsubsec:struct-trans}
Here, we define the specific form of the structure transformations used in the proposed StructTransform (ST) attacks.
Experiments with structure transformations are conducted by repeating the stochastic generation of adversarial prompt using $M_{attack}$  up to a maximum of ten attempts per prompt.
We repeat attempts to account for the stochasticity in $M_{attack}$ generations. 
%1,000 LLM generations for a single pass through the evaluation task.
We use the StructTransform framework to implement three classes of attacks:

\heading{Direct.}
Direct transformations include transforming prompts into several known syntax spaces, including commonly used ones such as SQL and JSON (see Section \ref{subsec:attack_framework}).
We also study the generality of the attack using LLMs to provide other known vulnerable syntaxes.
%To define the transformation, we first manually craft a JSON template schema based on some example goal (see Figure \ref{fig:adversarialpromptjson} in Appendix \ref{apdx:prompt_templates}).
%We prompt $M_{attack}$ using this template to produce newly transformed prompts that instruct the LLM to \textit{``generate data points adhering to the JSON schema"}. 

\heading{ICL.}
As shown in Section \ref{subsec:incontext_attacks}, attacks are based on using a capable LLM to generate a previously unknown syntax definition, which can then be used with $M_{attack}$ to develop structure transformations. 

\heading{Combined.}
The combined attacks explored in this study involve layering content transformations or the combination of content and structure transformations with direct and ICL attacks. 
We use UTF-16 encoding and HTTP server roleplay as the representative content transformations and combine them with structure transformations to investigate the effectiveness of combined transformation attacks. (see Figure \ref{fig:adversarial_prompt_combination} in Appendix \ref{apdx:prompt_templates} for an SQL example)

\section{Evaluation}
\label{sec:eval}
Through our experiments, we seek to answer the following Research Questions (RQs):
\begin{itemize}[itemsep=0pt]
    \item [\textbf{RQ1}] How effective are individual structure transformations in inducing misalignment in SOTA LLMs compared to existing methods?
    \item [\textbf{RQ2}] How does the effectiveness of structure transformations change when combined with content transformations?
    \item [\textbf{RQ3}] How feasible is it to discover new and effective structure transformations through a general, automated approach?
    \item [\textbf{RQ4}] How well do SOTA defenses perform against the StructTransform Bench?
\end{itemize}




\begin{table}[t]
\caption{Performance of content and our combined transformation attacks against various safety-aligned LLMs.}
\label{tab:combined_results}
\centering
\scalebox{0.75}{
\begin{tabular}{lllccc}
\toprule
\textbf{Model} & \textbf{Structure} & \textbf{Content} & \textbf{ASR}& \textbf{Efficiency}& \textbf{Refusal}\\
\midrule
\multirow{2}{*}{Llama 3.2-90B}
%& JSON & 82.0 & 1.74 & 19.40 \\
%& SQL & 91.5 & 1.71 & 6.50 \\
& - & RP & 17.5 & 1.34 &  84.4 \\ % baseline
& - & EC & 5.4 & 1.59 & 8.6 \\ % baseline
& - & EC+RP & 36 & 1.18 & 50.7 \\ % baseline
& JSON & RP  & 78.5 & 1.45 & 21.9 \\ % baseline
& JSON & EC &91 & 1.37 & 1.3 \\ % baseline
& JSON &EC+RP & 94.3 & 1.97 & 0.0 \\
& SQL & EC+RP & 86.2 & 3.3 & 4.7 \\
\midrule
\multirow{2}{*}{GPT 4o}
%& JSON & 61.5 & 2.10 & 39.65 \\
%& SQL & 83.0 & 1.83 & 1.20 \\
& JSON & EC+RP & 94.3 & 1.8 & 2.1 \\
& SQL &EC+RP & 90.6 & 2.2 & 0.6 \\
\midrule
\multirow{2}{*}{Claude 3.5 Sonnet}
%& JSON & 16.0 & 2.78 & 81.00 \\
%& SQL & 89.0 & 1.80 & 7.90 \\
& JSON &EC+RP & 93.7 & 1.8 & 2.9 \\
& SQL &EC+RP & 96.9 & 2.1 & 0.00 \\
\bottomrule
\end{tabular}
}
\caption*{\footnotesize RP: Roleplay, EC: Encoding}
\end{table}

\subsection{RQ1: Direct Attack Performance}
\label{subsec:rq1}
We study the adversarial effectiveness of structure transformations based on the StructTransform framework through two representative syntaxes, JSON and SQL, referred to as $\text{Direct}_\textit{JSON}$ and $\text{Direct}_\textit{SQL}$, respectively. Table~\ref{tab:individual_results} shows that both transformations significantly outperform WildJailbreak and PAIR. Against Llama 3.2-3B, JSON achieves 86.5\% ASR, surpassing all baselines while requiring only 1.89 attempts on average. Although JSON shows lower ASR than Jailbroken on Llama 3.2-90B and Claude 3.5 Sonnet, it achieves these results with approximately 10 times fewer attempts.

Our SQL transformation demonstrates even stronger performance, consistently outperforming all existing methods with significantly better efficiency for most models. 
In particular, SQL maintains minimal refusal rates across all models, indicating its effectiveness in bypassing safety guardrails.
Against Claude, SQL achieves 88.7\% ASR compared to Jailbroken's 34.0\%, while requiring only 2.8 attempts versus Jailbroken's 14.3 attempts.
The improvement persists across other models, with Llama-3.2 90B being the exception. This model showed reduced effectiveness and exhibited anomalous generations where it would repeatedly output "-" characters for SQL tasks, indicating a lack of capability and explaining its lower ASR and refusal rate.

When tested against SOTA LLMs in both capability and safety alignment, namely GPT 4o and Claude 3.5 Sonnet, we observe that JSON's effectiveness decreases significantly in both ASR and refusal rate, while SQL maintains high performance with low refusal rates.  JSON's diminished effectiveness likely stems from its representation in existing adversarial datasets \cite{jiang2024wildteaming, wei2024jailbroken}, making it more susceptible to refusal.
This disparity reveals that safety training on one structure transformation does not necessarily generalize to others, particularly when they are syntactically distant from each other.
As a result, effective mitigation will have to enumerate and include each structure in a model's safety alignment while making an optimal utility-security tradeoff. 

\heading{Summary.} \st attacks reveal a new dimension of vulnerabilities in the alignment of SOTA LLMs. SQL consistently achieves high ASRs and low refusal rates across models, while a simple JSON attack achieves competitive performance with smaller open-weight models.
The widening performance gap between two structure transformations, JSON and SQL against more capable models, combined with SQL's sustained effectiveness, highlights a key generalization drawback in current safety-alignment techniques such as SFT. 

\subsection{RQ2: Combination Attack Performance}
\label{subsec:rq2}
We study the effectiveness of layering content transformations inspired by existing attacks with \st attacks discussed in Section \ref{subsec:rq1}.
Table \ref{tab:combined_results} shows that the combined attacks substantially improved the results across metrics and models for both SQL and JSON transformations. 
It achieves $>90\%$ ASR while requiring 2.2 attempts on average.

As ablation experiments, we investigate various combinations of the roleplay and hex-encoding content transformations applied on a natural language prompt, tested on Llama 3.2-90B. The best performance was observed in the layered content transformation (roleplay + encoding) with 36\% ASR and a high refusal rate of 50.7\%. 
The encoding attack showed low refusal because the LLM stopped short of decoding the prompt without proceeding to answer it. 
The refusal judge simply did not judge these as refusal.

While our results indicate that safety training generalizes reasonably well to content transformations, the generalization clearly falls short when they are combined with structure transformations, as seen in the dramatic jump in ASR for both JSON and SQL combined. (Appendix \ref{apdx:prompt_templates} shows a sample prompt). 
The refusal rates also drop by 90\% and 100\% for JSON and SQL, respectively.

The effectiveness of these combination attacks is especially striking against SOTA LLMs. In particular, we observe a 500\% increase in ASR from single JSON attacks to combined JSON attacks on Claude. This brings into question the robustness of safety alignment training - while Claude's safety training generalizes well against individual JSON and content transformation attacks, it fails against their combination. Consequently, not only must LLMs be trained against structure transformation individually, but additional training may be needed against their combined counterparts. 

Notably, in cases where combination attacks failed, we found the limitation was primarily in the transformations generated by $M_{attack}$ rather than the approach itself.
We verified this by manually crafting more expressive transformation templates for the failure modes and observing malicious responses.

\heading{Summary.} Combination attacks achieve nearly perfect ASR against SOTA safety-trained LLMs, substantially outperforming single transformation attacks. Our analysis reveals three key insights about the limitations and challenges of current safety training approaches. 
First, safety training against specific content and structure transformations may not generalize to their combined counterparts, requiring additional safety training. 
Second, the composability of structure transformations with content transformations creates an expansive attack surface that may be fundamentally difficult to defend against using traditional safety training approaches.
Third, improvements in LLM capabilities can lead to more complex and effective attack surfaces if safety training data are not representative enough.

\subsection{RQ3: Generalization to Novel Spaces}
We investigate whether effective structure transformations can be systematically discovered and generated beyond the cases of JSON and SQL.
Due to computational costs, we focus our evaluation only on the best safety-aligned model, Claude.
This exploration is motivated by our findings in Section~\ref{subsec:rq1}, which reveal that safety training focused on one structure and syntax (JSON) fails to generalize effectively to others (SQL).
More importantly, the feasibility of continuously discovering additional effective structure transformations suggests an expansive attack surface that poses significant challenges for defense strategies.

\heading{Discovering Structure Transformations.}
We prompted $M_{target}$ (Claude 3.5 Sonnet) to generate a diverse set of 15 structure transformations. To identify the most promising candidates from the pool that included variations of query languages, data serialization formats, domain-specific languages, and mathematical notations.
We also instructed the LLM to rank these transformations based on our selection criteria including semantic preservation, syntax standardization and prevalence, representation completeness and expressiveness, structural distinctiveness, and the inherent complexity of the syntax. 
 Out of these, we select the top four, namely, Neo4j's Cypher Query Language, YAML Abstract Syntax Tree (AST), XML Schema, and Protocol Buffers (Protobuf).

Table~\ref{tab:new_structures} shows the results of evaluating these transformations against Claude, using both direct and combined attacks.
Transformations in Cypher, a graph query language obtained 82.4\% ASR and a 4.2 efficiency, while other transformations showed varying degrees of effectiveness. 
Again, we observe that layered attacks by combining content transformations provide at least a twofold increase in ASR, indicating weak alignment in these syntaxes.

\begin{table}[!htpb]
\caption{Performance of direct and combined structure transformations for various syntaxes against Claude 3.5 Sonnet}
\label{tab:new_structures}
\begin{center}
\scalebox{0.9}
{
\begin{tabular}{llccc}
\toprule
\textbf{Attack Type} & \textbf{Structure} & \textbf{ASR} & \textbf{Efficiency} & \textbf{Refusal} \\
\midrule
\multirow{5}{*}{Direct} & Cypher & 82.4 & 4.2 & 20.0 \\
& YAML & 12.6 & 1.6 & 56.7 \\
& XML & 14.5 & 2.1 & 80.8 \\
& Protobuf & 27.7 & 2.1 & 57.2 \\
\midrule
\multirow{5}{*}{Combined} & Cypher & 96.9 & 1.9 & 1.2 \\
& YAML & 39.0 & 2.1 & 56.7 \\
& XML & 67.3 & 1.3 & 16.5 \\
& Protobuf & 69.8 & 1.45 & 2.3 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}


\heading{Generating Syntaxes.}
We also conduct a preliminary investigation of the expansive attack surface introduced by ICL and manually generated syntaxes.
Due to computational and cost restrictions \footnote{ICL attacks use prompts with larger input token counts (800+) due to including syntax definitions as context in each call to $M_{target}$ and $M_{attack}$.}, we limit our experiments to two generated syntaxes, Sylph and Symlogix, named and generated by 3.5 Sonnet and GPT-4o, respectively. The generation methodology can be found in Section \ref{subsec:incontext_attacks}.  
Syntax definitions consist of sections for syntax structure including operators, expression and control flow, query-response examples, and feature descriptions such as optimization rules and error handling.  

We evaluate the performance of ICL attacks using the two generated syntaxes on 3.5 Sonnet and Llama3.2 90B. 
For Llama, both syntaxes lead to high ASR with 71.07\% for Direct$_{Sylph}$ and 80.5 for Direct$_{Symlogix}$. 
As observed in Combined$_{SQL}$, the performance of combined attacks drops in Llama 3.2-90B.
Looking at the generations, we observed that this is largely due to the lack of capability to interpret the layered transformed query. 
This is confirmed by the combined attack on Claude, seeing almost a three-fold increase in ASR in both syntaxes.


\begin{table}[!htpb]
\caption{Results of ICL attacks using LLM generated syntax.}
\label{tab:defender_results}
\centering
\scalebox{0.85}{ % Adjust scale factor as needed
\begin{tabular}{llccc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{ASR}& \textbf{Efficiency}& \textbf{Refusal}\\
\midrule
\multirow{4}{*}{\makecell{Llama\\3.2-90B}}
& Direct$_{Sylph}$ & 71.07 & 1.28 & 7.17 \\
& Combined$_{Sylph}$ & 38.36 & 1.72  & 16.6 \\
& Direct$_{Symlogix}$ & 85.53 & 1.35  & 8.68 \\
& Combined$_{Symlogix}$ & 80.5 & 1.77  & 1.13 \\
\midrule
\multirow{4}{*}{\makecell{Claude\\3.5}} 
& Direct$_{Sylph}$ & 9.43 & 2.27 & 83.77 \\
& Combined$_{Sylph}$ & 48.43 & 1.83  & 11.57 \\
& Direct$_{Symlogix}$ & 23.27 & 2.08  & 71.95 \\
& Combined$_{Symlogix}$ & 61.64 & 1.66  & 16.35 \\
\bottomrule
\end{tabular}
}
\end{table}

The success of automatically discovered transformations indicates that attackers could potentially exploit a wide range of structure transformations beyond those commonly studied in safety research, which can further be combined with content transformations to develop attacks that introduce significant weaknesses in alignment.  
Without addressing LLM safety alignment with improved techniques, we hypothesize that a structure can always be found or generated with relative ease that can overcome existing refusal mechanisms. 
Our results indicate that refusal mechanisms are mostly superficial and still akin to pattern matching instead of concept and intent-based refusal. 

\heading{Summary.} Our framework successfully discovers novel seen and unseen structure transformations with high ASRs as seen with Cypher and Symlogix. The dramatic improvements seen in combined attacks highlight limitations in the generalization of safety training across different syntactic domains. This demonstrates that the attack surface for structure transformations is significantly larger than previously recognized, presenting a fundamental challenge for LLM safety mechanisms. Together with the findings in Section ~\ref{subsec:rq1}, the effectiveness of the new syntaxes highlights the difficulty in mitigating structure transformations, requiring safety training against each structure within this large space, along with its combinations with content transformations. 

\subsection{RQ4: StructTransform Bench}
\label{subsec:adversarial_training_attacks}
% Second table - Defender results
Our previous findings demonstrate that LLMs' safety alignment does not genuinely generalize to novel harmful concepts, but instead relies on surface-level token patterns and relationships. 
This limitation highlights the need for specialized benchmarks to systematically expose and evaluate such weaknesses in language model safety mechanisms.
To this end, we propose using \st attacks as an evaluator of safety-alignment defenses. 
Concretely, we propose \st Bench, a curated set of eight unique structure-transformed attack prompts for all the 159 samples in \textsc{HarmBench} and evaluate its attack success on SOTA models and defenses. 
\st Bench consists of top-performing direct and combined attacks introduced in the previous sections, including ICL attacks based on the GPT-4o generated Symlogix syntax.
We also show the scores for an adaptive attack, where the attack is considered a success for a harmful prompt if any one of the eight prompts succeeds \cite{wei2024jailbroken}. 
We note that each transformation is only attempted once per harmful prompt for this experiment.

Figure \ref{fig:confusion_matrix} summarizes the results of running \st Bench on SOTA safety-aligned models as a matrix of ASRs. 
Models are ordered along the y-axis roughly based on their parameters and capability. 
We provide results for Llama 3 8B, considering it as a baseline because both LAT \cite{casper2024defending} and Circuit Breakers \cite{zou2024circuitbreaker} are implemented using this architecture. 
At a high level, the number of attacks with ASR $>50\%$, along with an average total ASR of 47.56\% excluding adaptive attack scores, means that structure transformations present an expansive attack surface.
Furthermore, the prompts each attack succeeds on are disjoint, as indicated by the 93.46\% average ASR obtained by the adaptive attack.
This also means that increasing the number of attempts per harmful prompt will lead to further improvements in attack success.
For example, Combined$_{json}$ on 3.5 Sonnet sees a 13.73\% increase in ASR when a maximum of 10 attempts are used (see Table \ref{tab:combined_results}). 
%The average ASR of all the tests excluding zero cases is 44.89\%. 

Interestingly, \st attacks work better against more capable models such as Claude and Llama 90B, indicating how the scale and capability of models result in wider attack surfaces.
This is especially evident from the poor performance (0 ASR) of combined attacks on Llama 3-8B-based models when using Combined attacks with hex encoding. 
%Despite this, both the base model and LAT show >80\% ASR in the adaptive case.  
%Circuit Breakers is a much stronger defense with an adaptive ASR of 63\%.
However, changing the content transformation in all combined attacks to a simpler encoding scheme improves adaptive ASR to 90\% or more, as reported in Figure \ref{fig:confusion_matrix}.
We replace the hex encoding with the URL encoding implemented using the \texttt{quote.parse()} function in the urllib python package to achieve this. 
For example, "Hello World" encoded is "Hello\%20World\%21".
However, LAT is surprisingly resilient to this encoding, demonstrating a drop in all combined attacks against it.

Llama 3.2-90B and 3.5 Sonnet demonstrated safety-alignment improvements using synthetic data, especially Claude with Constitutional AI \cite{dubey2024llama,Bai2022ConstitutionalAH}.
Despite reported improvements, which are apparent in the case of vanilla prompts (see Table \ref{tab:individual_results}), we show a significant lack of robustness with an average ASR of 54.71\% and 57.33\% respectively on structure transformation attacks. They lead to an adaptive ASR of 100\% in both models.

OpenAI's o1 uses \textit{deliberative alignment}, a novel SFT scheme leveraging the chain-of-thought reasoning capability in these models, which was shown to improve the safety-utility trade-off \cite{guan2025deliberativealignmentreasoningenables}.
Due to higher generated token counts and costs with o1, we report only the adaptive attack score and use a filtering scheme to limit the number of total LLM calls. 
We do this by iteratively choosing the most successful attack on Claude and running only the harmful prompts that failed in the previous syntax in the subsequent stages. 
As a result, only combined Cypher sees all the 159 prompts, with a 46.5\% ASR.
After running Combined SQL, JSON, and ICL attacks, the ASR increases to 66\%, 72.95\%, and 77.9\%, respectively. 
We do not validate further due to diminishing returns.  
Due to the repeated refinement of responses in o1 generations, it can better detect harmful content.
SFT using deliberative alignment enhances this by promoting the retrieval of relevant policies associated with harmful content at inference and applying them to align the final output \cite{guan2025deliberativealignmentreasoningenables}.

\st Bench results are note-worthy because the attacks succeed against models implementing various safety-alignment techniques: Llama represents the state-of-the-art in safety fine-tuning for open-source models; Claude implements Constitutional AI; Circuit Breakers employs a novel fine-tuning loss based on representation engineering; LAT utilizes adversarial perturbations in latent space; and o1 uses deliberative alignment through chain-of-thought reasoning.

\heading{Summary.} SFT has seen several improvements recently, each of which is seen to provide a reduction in the effectiveness of adversarial prompts. However, the results on the proposed StructTransform benchmark show that they still lack robustness and an 'understanding' of harmful concepts.
Although o1 provides improved performance, it comes at the cost of increased test-time compute, confirming the finding that safety mechanisms should be as sophisticated as the underlying model to guarantee safety in LLMs \cite{wei2024jailbroken}.

\begin{figure}
        \centering
        \includegraphics[scale=0.32]{figures/confusion_matrix.pdf}
        \caption{ASR of SOTA LLMs and defenses on StructTransform Bench consisting of eight structure transformation attacks and an Adaptive attack, which is a logical OR of the individual attacks. *Only the adaptive attack score is reported for o1 due to using a filtering attack to minimize costs.}
        \label{fig:confusion_matrix}
\end{figure}
%\heading{Summary.} StructTransform Bench, using a set of undiscovered and scalable transformations, presents a dataset for evaluating the generalizability of defenses against adversarial prompts. Through the high ASR achieved by attacks in the benchmark, we show that existing defenses including recent SOTA approaches do not provided a complete solution to safety-alignment. 
\section{Discussion} 

Our findings reveal key challenges in ensuring safety and alignment in large language models (LLMs) and highlight the need for enhanced alignment mechanisms to address vulnerabilities in structure-transforming syntax spaces.

\heading{Accessibility and Scalability of Attacks.} The structure transformations studied here require minimal technical expertise, making them accessible to a broad range of users. Unlike previous methods relying on complex algorithms, these attacks use standard programming constructs and formats. Combined with LLM assistance, it leads to a simple and efficient attack for even non-experts to exploit. This accessibility amplifies the security risks.

\heading{Vulnerability with Model Scale.} Model scale and capability correlate with increased vulnerability. Smaller models (e.g., Llama-3.2-3B) struggle with complex syntax transformations, providing some resilience, though they remain vulnerable within their limited reasoning scope under the current safety-tuning paradigm. This is an easier problem to address in comparison to larger models (e.g., GPT-4o, Claude) that are capable of processing sophisticated transformation prompts but lack proportionally robust safety mechanisms. The attack surface grows with model complexity, as seen in the high success rates ($>95\%$) of combined attacks on advanced models. Despite advances in safety alignment since the first presentation of the failure modes of safety training \cite{wei2024jailbroken}, our results underscore the need for alignment that scales with model capabilities along the structure transformation dimension.

%\heading{Core Alignment Challenges.} Our findings highlight critical shortcomings in current alignment: \begin{enumerate} 
%\item \textbf{Intent Recognition Failure}: Structure transformations obscure harmful intent by altering computational structures, affecting the refusal-based safety mechanisms.
%\item \textbf{Safety-Capability Gap}: As models improve at processing diverse structures, their safety mechanisms fail to generalize beyond the already learned patterns, presenting a significant alignment challenge.
\heading{Code Risks}. Structure-based vulnerabilities raise concerns for LLM use cases involving code (such as code completion), as they are particularly susceptible to these attacks.

\heading{Future Directions for Safety and Alignment.} High success rates in our attacks suggest that alignment focusing solely on natural language patterns is insufficient. This is because a pattern that is sufficiently out-of-distribution in terms of SFT can elicit misalignment.
Scaling test-time compute as seen in OpenAI's o1 improves safety but our experiments show that they too operate with the aforementioned weakness. 
The potential space of adversarial prompts expands with model capability, indicating that simply scaling existing safety training is inadequate. 
Achieving ``safety-capability parity" \cite{wei2024jailbroken} requires more adaptable alignment approaches as models exhibit emergent behaviors, and our findings suggest a need for foundational changes in alignment approaches. Future research should prioritize alignment techniques that address concepts instead of token patterns \cite{the2024large}. 

%Cursory application of our combination attacks on SOTA OpenAI's o1 model showed that the attack was able to overwhelm its chain-of-thought based safety mechanism \cite{wei2022chain}.
%The attack prompt results in the model focusing on deciphering the complex task while it generates an unsafe response. 
%Thus, further research into how these classes of attacks work against chain-of-thought-based models is urgently needed, along with corresponding mitigation techniques.
%Composing structure transformation attacks (such as combining JSON and SQL) is another vast attack surface we did not explore since the proposed combinations sufficiently evaded LLM guardrails.

\heading{Limitations.} Our work is a preliminary study of structure transformation attacks and their implications for robustness, which is tested against the current SOTA safety-trained models.
As such, a better understanding of how LLMs represents features internally \cite{conmy2023towards} and improvements in safety and alignment research can lead to significant shifts in safety generalization that may invalidate or minimize our findings. 
Our approach transforms prompts into distinct syntax spaces, which might involve generating many more tokens than natural languages, decreasing the token efficiency of the attack. Finally, our study heavily relies on LLM models for evaluation. Although we ensure we select them based on validated studies, results derived from these models do come with a margin of error \cite{mazeikaharmbench,wildguard2024}.








\section{Related Work} 
\label{sec:related_work}
Our work explores structure transformations for adversarial prompts, a relatively underexplored domain while building on extensive research in content-based adversarial prompting. We categorize related work into three primary areas, namely, generalization gaps, prompt optimization, and prompt mining.

\heading{Generalization Gaps.} Research in this category focuses on exploiting mismatches between language model pre-training and safety-aligned fine-tuning data. Attackers transform harmful queries into formats seen during pre-training but less frequently during safety tuning, thus bypassing safety filters. Simple techniques include translation into low-resource languages~\cite{deng2024multilingual}, while more advanced methods employ transformations such as ASCII art~\cite{jiang2024artprompt} and novel encoding schemes~\cite{Handa2024WhenCompetencyinRO}. Some attacks also leverage LLMs' code-processing abilities by exploiting function-call interfaces~\cite{wu2024dark}, injecting code~\cite{kang2024exploiting}, or rephrasing prompts as code-completion tasks~\cite{lv2024codechameleon}. These approaches illustrate the broad vulnerability of language models to shifts in data representation.

\heading{Prompt Optimization.} Prompt optimization focuses on the systematic refinement of adversarial prompts to enhance their effectiveness. Basic approaches use LLMs to iteratively generate and improve prompts~\cite{chaojailbreaking}.
More advanced techniques use structured search and optimization algorithms to discover effective attack patterns, including Greedy Coordinate Gradient (GCG)~\cite{zou2023universal} and AutoDAN~\cite{liu2023autodan}, which enhance gradient-based optimization with interpretability improvements.
However, token-level optimization often assumes white-box access to the models and hundreds of iterative calls to the LLM \cite{liao2024amplegcg,jones2023automatically}.
%AmpleGCG~\cite{liao2024amplegcg} further develops GCG by generating hundreds of adversarial prompt variants, improving efficiency across different models. Token-level optimizations, such as those in GCG and ARCA~\cite{jones2023automatically}, have also proven highly effective, with precise token modifications leading to a substantial impact on model behavior. These advancements demonstrate the adaptability of adversarial prompt optimization, particularly for white-box models.

\heading{Prompt Mining.} Mining for adversarial prompts leverages large-scale data and systematic exploration to uncover novel attack patterns. WildTeaming~\cite{jiang2024wildteaming} uses human-LLM conversations to identify naturally occurring adversarial prompts, providing real-world insights into emergent attack strategies. Rainbow Teaming~\cite{samvelyan2024rainbow} expands on this by systematically exploring attack dimensions like risk category and style to identify vulnerabilities through grid search across the feature space. Methods like FuzzLLM~\cite{yao2024fuzzllm} and GPTFUZZER~\cite{yu2023gptfuzzer} further enhance this approach by applying genetic algorithms and mutation-based techniques to refine the prompts, demonstrating their utility in black-box settings where model gradients are inaccessible.

Our work is similar to studies in generalization gaps but diverges from existing research by focusing on structure transformations rather than traditional content transformations. While methods like CodeAttack~\cite{ren2024codeattack} explored specific transformations, our systematic framework is the first to analyze structural vulnerabilities across diverse syntax spaces, achieving perfect attack success with minimal attempts. Additionally, our evaluations reveal that SOTA defenses, such as deliberative alignment struggle against these structure-based attacks, underscoring significant gaps in current defenses for mitigating this new attack vector.

\section{Conclusion}
The widespread adoption of LLMs by a diverse user base brings significant safety challenges, with alignment being a critical safeguard. However, we have demonstrated that current alignment techniques are ineffective in mitigating vulnerabilities introduced by structure transformations, where attackers encode natural language intent into structured formats like SQL, JSON, or even generated syntaxes.
Our research highlights the serious risks posed by these threats, showing that even highly aligned LLMs are susceptible to simple structural transformation attacks, with nearly a 90\% attack success rate achieved in fewer than two attempts.
This success rate approaches 100\% when combined with content transformations such as encoding.
The proposed \st based benchmark highlights the weakness in existing defenses and raises the importance of research and concentrated efforts needed to develop effective mitigation strategies.
We also illustrated how these techniques can be exploited by malicious actors to create ransomware and fraudulent datasets, such as SMS phishing, that effectively evade detection mechanisms.
% \section*{Acknowledgment}

\section*{Ethics Considerations}

Our work explores structure transformation attacks that can circumvent LLM safety guardrails, raising important ethical considerations. We acknowledge the dual-use nature of this research: while it advances our understanding of LLM vulnerabilities and aids in developing better defenses, it could potentially inform malicious actors. To balance these concerns, we have contacted the LLM providers including Anthropic, OpenAI, and Meta to responsibly disclose our findings prior to publication, providing them with detailed reports. We specifically highlighted the risks of structured transformations being used for automated malicious campaigns.
Given the potential for misuse in automated phishing campaigns, we note that our synthetic SMS phishing dataset can serve a beneficial purpose by addressing the scarcity of real-world phishing corpora for training detection models, a challenge highlighted in several studies \cite{shafi2017review, harichandana2024cops}.
%Our research highlights the urgent need for more robust safety mechanisms in LLMs, particularly against structure-based attacks.
By revealing these vulnerabilities in a controlled manner, we aim to catalyze proactive defense development rather than enable harmful applications.
The demonstrated ability to bypass current classifiers with synthetic phishing data underscores the importance of developing more sophisticated detection methods.

%-------------------------------------------------------------------------------
\section*{Open Science}
%-------------------------------------------------------------------------------
Fully functional StructTransform benchmark, source code, and testing datasets will
be available publicly to the community to foster research in
this field (\texttt{https://github.com/StructTransform/}). 

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{ref}


\appendix

\section{Case Study: SMS Phishing}
\label{apdx:sms_case_study}
The attacks presented in this study enable highly scalable and automated malicious activities because most of the presented syntax spaces are structured and machine-readable. 
We investigate the practical implications of such transformations by presenting an efficient and effective pipeline for generating smishing (SMS phishing) campaigns. 
%We show how LLMs with structured data can form deceptive and adaptive data generation pipelines. 

Smishing campaigns traditionally focus on the volume of messages with low quality, diversity, and personalization in individual messages \cite{nahapetyan2024sms}.
Evidently, many studies have demonstrated excellent binary classification performance on smish classification tasks \cite{li2024spamdam,oswald2022spotspam, harichandana2024cops}. 
%As LLMs become increasingly accessible, we argue that the quality and quantity of smishing messages will rapidly increase, especially when enabled by adaptive and automated pipelines, as demonstrated in this section.
We show how synthetic data generated by LLMs can be made diverse but similar enough to true data distributions, allowing it to bypass fine-tuned language model-based classifiers.

\subsection{Setup}

The intuition behind our approach is that smishing messages are most deceptive when they are less similar to existing smishing messages and more similar to benign messages such as ads, spam, and personal (ham) messages. 
Personalization and contextualization have also been shown to be effective in crafting deceptive messages \cite{tabassum2024drives,zhuo2023sok}.
This can be achieved by LLMs interacting with existing SMS datasets and incorporating auxiliary information for additional control of generations, which can include information such as categories, persuasion methods \cite{shim2024data}, PII, and brand lists. 

We curate a set of labeled real-world datasets to ground data generations in the same data distribution as real-world messages. 
This includes 5,971 samples belonging to the spam, smishing, and benign personal messages category \cite{mishra2020smishing}, 1,063 crowd-sourced smishing samples \cite{timko2024smishing}, and 600 spam messages harvested from one of the authors' personal device for balancing the dataset.
We also include common smishing message themes (e.g., account alert, loan scam, prize win, etc) \cite{timko2024smishing} and persuasion techniques (e.g., authority, social proof, distraction, etc) \cite{ferreira2019persuasion} and their definitions as auxiliary information. 
Using the aforementioned data as the seed, The generation pipeline illustrated in Figure~\ref {fig:sms_framework} proceeds in two iterative processes. 

\heading{Synthetic Data Generation.} A safety-tuned LLM is continually prompted to generate synthetic phishing messages in batches using a structure transformation attack (e.g., JSON schema).
The LLM is provided with sample messages from real-world datasets to ground the generations and increase linguistic and stylistic similarity with real-world datasets. 
Additionally, sampled auxiliary information allows for constraining and customizing the output to various attack goals and requirements.
Both input and output are in structured formats such as JSON, which significantly simplifies storage and processing. %(storage/manipulation.. could be a discussion statement)

\heading{Method Updates.} We observe that the diversity and novelty of generated messages can degrade over time. To overcome this, we pass the existing auxiliary information to the LLM at fixed intervals and prompt it to update the auxiliary information where applicable. For example, prompting the LLM to generate new phishing categories can lead to diverse and new categories of phishing messages. 


\begin{figure}
    \centering
    \includegraphics[scale=0.42]{figures/sms_pipelinev10.pdf}
    \caption{A self-refining pipeline for phishing SMS generation by encoding data as structured data formats and utilizing structured transformations to bypass LLM safety alignment. The system continuously generates synthetic phishing data and periodically updates its methods to inject diversity into generations.}
    \label{fig:sms_framework}
\end{figure}


\subsection{Results}
Assuming JSON data format and schema-based structure transformation for overcoming LLM safety, we generate 1000 synthetic smishing samples using the proposed generation pipeline with Llama3.2-3B as the target LLM. 
We ran 100 iterations of data generation and two iterations of method updates with data generation response producing ten samples (see Appendix for prompts). 
To quantify the quality of the generated samples, we report classification performance by a fine-tuned DistilBERT classifier. 
The BERT language model \cite{devlin2018bert} was found to be suited for SMS classification in existing studies \cite{li2024spamdam}. 
DistilBERT is a lightweight language model created by distilling BERT while retaining most of its performance \cite{sanh2019distilbert}. 
The curated dataset of real datasets is split into train, validation, and test subsets. 
We report the F1 scores on the test phishing subset and the synthetic subset. 

Figure \ref{fig:embedding} visualizes the fine-tuned classifier embeddings of the generated synthetic data and 1000 random samples from each of the three classes from the curated dataset.
We can observe that the generated smishing messages span the entire space, suggesting that they are indistinguishable from other messages, including spam and benign messages. 
It is also reflected in the classifier performance dropping from 0.94 to 0.61 in terms of the F1 score for the test smish subset and the synthetic smishing data generated by our structured data-based pipeline. 
Table \ref{tab:sms_messages_complete} shows a few samples from the test subset and its top-1 nearest neighbor in the embedding space from the generated synthetic set.
The quality of generations suggests that our approach is indeed capable of generating smishing campaigns and it can be done using relatively small LLMs like LLama3.2-3B. 
It highlights the importance of improving guardrails in syntax spaces that induce changes in structure and data generation with such structured formats with significant dual-use risks.

\begin{figure}
    \centering
    \includegraphics[scale=0.32]{figures/embeddings.pdf}
    \caption{t-SNE visualization of real and synthetic data in the embedding space of a fine-tuned DistilBERT SMS classifier.}
    \label{fig:embedding}
\end{figure}
\begin{table}[h]
\caption{Original SMS messages with labels and their nearest neighbor LLM generated phishing message in the embedding space.}
\label{tab:sms_messages_complete}
\centering
\scalebox{0.6}{  % You can adjust the scaling factor (0.85) as needed
\begin{tabular}{lcc}
\toprule
\textbf{Message} & \textbf{Label} & \textbf{Synthetic (Nearest Neighbor) } \\
\midrule

Sorry I missed your call let's talk & Smish & Hi, this is your doctor's office. \\
when you have the time. I'm on & & We've noticed that you haven't been \\
07090201529 & & to your scheduled appointment. Please \\
& & call 08001234567 to confirm. \\
\midrule
Block Breaker now comes in deluxe & Spam & T-Mobile Deals / Congrats! Grab a \\
format with new features and & & chance to win prizes worth up to \\
graphics from T-Mobile. Buy for & & 100 USD, Enter Here \\
just £5 by replying GET BBDELUXE & & himeji.sutekinet.info/ran.. \\
\midrule
You can donate £2.50 to UNICEF's & Spam & Congratulation, You have been given \\
Asian Tsunami disaster support & & a free \$1,000 fund by UNICEF. This \\
fund by texting DONATE to 864233. & & program is for people affected with \\
£2.50 will be added to your next bill & & covid19 pandemic. To claim your funds, \\
& & click on this link https://forms.gle/tm.. \\
\midrule
It's é only \$140 ard...É rest & Benign & NEWS: 2bed/1bath just \$364 a mo \\
all ard \$180 at least...Which & & available for you NOW! click here \\
is é price 4 é 2 bedrm (\$900) & & whatseenow3.info/ltZSCGnozp \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Case Study: Malware Generation}
\input{tables/malware_code}
\newpage

\section{HarmBench}
\label{apdx:harmbench_eval}
To validate our results and show the adequacy of the HarmBench judge, we provide an evaluation comparison with 2 human annotation ground truths on 100 randomly sampled harmful prompt-response pairs.
We show ASR and pairwise Kappa score with Human and AI judge. 

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Evaluator} & \textbf{ASR} & \textbf{Kappa Score} \\
\hline
HarmBench Judge & 62 & - \\
Reviewer 1 & 63 & 0.637 \\
Reviewer 2 & 56 & 0.588 \\
\hline
\end{tabular}
\caption{ASR and Cohen Kappa Scores by Evaluator}
\end{table}


\section{Prompt Templates}
\label{apdx:prompt_templates}

\input{tables/sql_prompt_template}
\input{tables/json_prompt_table}
\input{tables/baseHTTP_prompt_template}
\input{tables/combination_prompt_template}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
