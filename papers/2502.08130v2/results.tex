\paragraph{In-Domain Performance}
\label{subsec:same_task_same_domain}
\input{in_domain_recall_acc}

Table \ref{tab:in_domain_recall_acc} reports the accuracy of Mistral-Instruct-v2 (7B) fine-tuned over GSM8K, MBPP and NQ datasets. Here, we evaluate the base, SFT, SDFT and S3FT models over the test set corresponding to the training dataset.

The base model shows suboptimal performance on all the datasets.
Fine-tuning using our method significantly improves the model's performance. S3FT achieves a 2.1\% gain on GSM8K, 3.6\% gain on MBPP and comparable performance to the state-of-the-art method SDFT on reading comprehension. We note that S3FT significantly outperforms SFT on all three datasets. This improvement stems from a simple yet effective technique of using the base model's responses for training when the base model is correct instead of the gold response. This verifies that S3FT learns the fine-tuning task well outperforming all other baselines (RQ1). 
\vspace{0.2ex}
\paragraph{Generalization}
\input{other_tasks_mistral}
A major issue with SFT is its tendency to diminish the model's previously learned skills.
To demonstrate that S3FT alleviates this issue, we 
compare the baselines and our method against the base model on a diverse set of publicly available benchmarks. Specifically, we evaluate them on MMLU \cite{mmlu}, Truthful-QA \cite{truthfulqa}, Hellaswag \cite{hellaswag} and, Winogrande \cite{sakaguchi2019winogrande}. Table \ref{tab:other_tasks_mistral} reports our findings. 

We observe that irrespective of the task used for fine-tuning, there is a drop in the performance of SFT models across all benchmarks. 
On average, SFT on Mistral-7B results in an average drop of $4.4$, $2.7$ and $5.8$ when trained using GSM8K, MBPP and NQ respectively. 
On the other hand, S3FT results in an average drop of only $2.5$ when trained on GSM8K and MBPP and a drop of $1.0$ when trained on MBPP.
This clearly demonstrates that our proposed technique for fine-tuning preserves the base model's capabilities (RQ2) without relying upon any kind of replay buffer which might not even be available in many cases. On the other hand, standard SFT results in overfitting to the training dataset, resulting in catastrophic forgetting of the skills acquired by the base model during pre-training and instruction tuning. 
\paragraph{Effect of Gold response's paraphrasing}
Our findings from Tables \ref{tab:in_domain_recall_acc} and \ref{tab:other_tasks_mistral} indicate that using the model's response as the target rather than the gold response significantly enhances fine-tuning performance without compromising the model's overall capabilities. Training on gold responses can cause a shift from the original distribution, negatively impacting the model's generalization. Figure \ref{fig:log_prob_dist} illustrates this gap between the distributions of gold responses, gold paraphrases, and the base model's responses. The distribution is plotted using $84$ random samples for which the model's prediction and the paraphrases of the gold responses are acceptable. The plot shows that in several cases model responses can be valid and closer to model's own distribution and therefore curating the training data in this way leads to minimal changes in the model parameters while fine-tuning. Thus, the closer the gold response's paraphrases to the model distribution, the better it is (RQ3). 

\paragraph{Training Data Proportions}
Table \ref{tab:human_judge_study_proportions} presents the proportions of examples in which S3FT utilizes base model responses, gold paraphrases, and gold responses across the three datasets. Notably, the base model's responses were deemed acceptable for at least 30\% of the training samples. When incorporating gold paraphrases, more than 50\% of the training data originates from the model's own responses. This suggests that the base model's outputs play a crucial role in S3FT's success by acting as an effective regularizer, helping to mitigate overfitting in the fine-tuning process.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{distribution_of_probability_ref.pdf}
    \caption{Histogram of the log probability assigned by \mistral\ to the gold responses, paraphrase of gold responses and model's own predictions. The distribution is based on 84 examples from the MBPP training data.}
    \label{fig:log_prob_dist}
    \vspace{-1em}
\end{figure}
\input{human_judge_study}