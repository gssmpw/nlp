In this paper, we present S3FT, a fine-tuning approach that enhances both task-specific performance and generalization across tasks, as shown on benchmarks like MMLU and Truthful QA. S3FT leverages the idea that multiple correct outputs may exist and avoids unnecessary changes by fine-tuning on gold response (or its paraphrase) only when the model's response is incorrect. In future work, we plan to investigate techniques like few-shot prompting for sampling correct outputs that are closer to the model's own distribution to reduce the changes from the base model's weights.