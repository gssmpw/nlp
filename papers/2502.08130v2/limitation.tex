
S3FT requires running model inference on the entire training dataset, identifying correct and incorrect responses, then performing gold rephrasing on incorrect responses and evaluating their correctness. These additional steps introduce a few extra requirements not present in standard SFT.
First, S3FT's improved results come at the cost of increased computational demands. Second, it requires a qualified judge. As we show, reliable heuristics can be used to assess response equivalence in mathematical reasoning and Python programming tasks. However, for open-ended tasks such as summarization and translation, no simple heuristics exist.
Here, we demonstrate that for reading comprehension, an LLM judge can serve as a reliable evaluator. As LLM judges improve, the usability and applicability of our method can extend to a broader range of tasks.


