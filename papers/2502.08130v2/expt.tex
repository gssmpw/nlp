\noindent We evaluate and compare the proposed S3FT method with the vanilla SFT and other methods that try to perform fine-tuning while retaining generalization capabilities. We aim to answer the following research questions: \textbf{RQ1. In-Domain Performance:} How well does S3FT learn the fine-tuning task compared to baselines when fine-tuned and evaluated on the same dataset? \textbf{RQ2. Generalization:} How well does S3FT retain the inherent capabilities of the base model post fine-tuning? \textbf{RQ3. Effect of gold response's paraphrasing:} How beneficial is it to use gold paraphrases that are closer to the base model's distribution? 

\noindent \textbf{Datasets:} 
We focus on three tasks, improving the mathematical reasoning abilities, basic python programming and reading comprehension skills. For enhancing the mathematical skills we experiment with GSM8K~\cite{cobbe2021training} dataset. This dataset consists of grade school level math word problems and solutions. To boost the Python programming skill, we experiment with MBPP~\cite{austin2021program} dataset which consists of a task description, three test cases, and a code solution for each example. We experiment with a variant of NQ~\cite{kwiatkowski2019natural} dataset as introduced in \cite{slobodkin2023curious} to enhance the reading-comprehension skills. The NQ dataset from \cite{slobodkin2023curious}  contains 3800 (context, question, answer) pairs. The questions are categorized into two types: (i) answerable -- where the provided context is relevant and contains sufficient information to derive an answer, and ii) unanswerable -- where the context lacks the necessary information to answer the question. A detailed description of the datasets, along with the training, validation and testing splits, is provided in the Appendix \ref{app:data_detail}.
\vspace{0.2ex}

\noindent \textbf{Evaluation Metrics:} For GSM8K, we assess the correctness of a generated response by checking if its predicted answer matches the final answer in the gold solution. For MBPP, we evaluate the generated code by executing it against the provided test cases. If the code passes all the test cases, it is considered correct. For NQ dataset, we employ Mistral-instruct-v2 (7B) as a judge. Following \cite{badshah2408reference}, we use a reference-guided prompt to judge the correctness of the generated responses. More details about the LLM judge can be found in appendix \ref{app:llm-judge}. Finally, we report the accuracy of each method for the three datasets.

\noindent \textbf{Human Study on Judgesâ€™ Accuracy:} Since responses in GSM8K and NQ are more open-ended, we conduct a human study to assess the reliability of the judges used for evaluating correctness. For GSM8K, we randomly sampled 50 examples and found that the judge is approximately 96\% accurate. Similarly, for NQ, testing on a set of 200 random samples yielded an accuracy of 86\%. Additionally, we highlight that improving the quality of the judges could further enhance the evaluation process and improve the overall generalizability of the method.

\vspace{0.2ex}
\noindent \textbf{Base model and Baselines:} 
We experiment with Mistral-instruct-v2 (7B) \cite{mistral} as our base model. We use three baselines: (1) prompting the base model (see Appendix \ref{app:task-prompts} for the exact prompt), (2) Supervised Fine-Tuning (SFT), and (3) Self-distill Fine-tuning (SDFT) \cite{yang2024self}. SDFT is a contemporary work that adopts gold answer rephrasing to preserve the generalization of the fine-tuned model.

\noindent \textbf{Implementation Details:} For all fine-tuning, we use Low-Rank Adaptation (LoRA) \cite{hu2022lora} with a rank of 8, a scaling factor of 16 and a dropout of 0.1. Please see Appendix \ref{app:train_detail} for more details. 

