\subsection{Prompt for Generating the Response}
\label{app:task-prompts}
In this section, we list the prompts used with mistral-instruct-v2 to generate the base model responses, gold paraphrases, and training. For the sake of consistency and fair comparison, the same prompts are used for fine-tuning using SFT, SDFT and S3FT techniques. Figure \ref{fig:mbpp_predict} and Figure  \ref{fig:mbpp_gold_para} present the prompts used for generating the base model prediction and gold paraphrasing for MBPP dataset. To simplify the process of extracting the code in the generated output, we always ask the model to generate only the Python code starting with the string "[[BEGIN]]" and ending in the string "[[DONE]]". For the MBPP dataset, the training prompt is the same as the base model's prediction prompt. Figure \ref{fig:gsm8k_base_res_prompt} and Figure \ref{fig:gsm8k_gold_para} present the prompt used for generating the base model response and paraphrasing the gold response. The details of the training prompt are provided in Figure \ref{fig:gsm8k_train_prompt}. Similarly, Figures \ref{fig:nq_base_prompt} and \ref{fig:nq_gold_para_prompt} show the prompts used for generating the base model output and gold paraphrasing, respectively.
\subsection{Datasets Details}
\label{app:data_detail}
\paragraph{GSM8K:} GSM8K is a math word problem dataset comprising of 7473 training examples and 1319 test samples. Each question takes approximately 2-8 steps to solve. Solving these problem require knowledge of basic algebra only. Solutions are human written containing steps in natural language as well. We note that GSM8K does not have a validation set, so we took randomly sampled 150 samples from the dataset as validation.

\paragraph{MBPP:} MBPP stands for Mostly basic python programming dataset. As the name suggests, each example of this dataset contains a task description along with three test cases that the code solving the given task should pass. The gold response contains the python code. There are 372 training examples, 90 validation examples and 500 examples in the testing dataset.

\paragraph{NQ:} NQ is a content-grounded QA dataset. To increase the complexity of the task, \cite{slobodkin2023curious} augment the NQ dataset with unanswerable queries. Here, the grounding content consists of a single paragraph and the gold answers are short phrases.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gsm8k_train_prompt.pdf}
    \caption{Prompt used for training the model on GSM8K dataset.}
    \vspace{-0.5em}
    \label{fig:gsm8k_train_prompt}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{gsm8k-base-response-gen-prompt.pdf}
    \caption{Prompt used for predicting the base model's output on GSM8K dataset.}
    \vspace{-0.5em}
    \label{fig:gsm8k_base_res_prompt}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{nq_base_prompt.pdf}
    \caption{Prompt used for predicting the base model's output on NQ dataset.}
    \vspace{-0.5em}
    \label{fig:nq_base_prompt}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{mbpp_paraphrase_prompt.pdf}
    \caption{Prompt used for paraphrasing the gold response of training partition of the MBPP dataset.}
    \label{fig:mbpp_gold_para}
\end{figure*} 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{gsm8k-gold-paraphrase-prompt.pdf}
    \caption{Prompt used for paraphrasing the gold response of training partition of the GSM8K dataset.}
    \label{fig:gsm8k_gold_para}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{llm-judge-prompt.pdf}
    \caption{The prompt used for judging the correctness of the responses generated by the model for NQ dataset. Here, {{question}} refer to the question that the model is answering, {{predicted response}} is the response generated by the fine-tuned model and gold response is the gold answer for the question.}
    \label{fig:llm-judge-prompt}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{nq_gold_para_prompt.pdf}
    \caption{Prompt used for paraphrasing the gold response of training partition of the NQ dataset.}
    \label{fig:nq_gold_para_prompt}
\end{figure}

\subsection{Training Details}
\label{app:train_detail}
Training for all the experiments was carried out on a single A100 (80 GB) GPU. None of the experiments took more than 1.5 hours to train. To generate the base model's responses, we deployed \mistral using vLLM on a single A100 (80 GB) GPU. It took 1 hour to generate the base model's predictions for GSM8K, 30 minutes to generate the base model's responses for NQ dataset and 15 minutes on MBPP dataset. The entire life cycle, including training data generation, fine-tuning and evaluation, did not take more than 5 hours.

We use a learning rate of $1 \times e^{-4}$. For GSM8K and NQ, we train all the models for 5000 steps, validate after every 500 steps, and select the best checkpoint. For MBPP, we train the models for 1000 steps, validate after every 100 steps, and select the base checkpoint based on the accuracy over the validation set.

For evaluating the GSM8K dataset, we matched the last number of the gold response with the last number extracted from the predicted response. If these answers matched, we considered the generated response to be correct. For evaluating the Python codes, we use bigcode-evaluation-harness \cite{bigcode-evaluation-harness}. LLM-judge's output was parsed to check the correctness of the answers for the NQ dataset. If the judge responded "TRUE", the answer was considered correct; else, if the judge predicted "FALSE", the answer was deemed incorrect. We always used greedy decoding when generating the model responses. Thus, we do a single run of the evaluation and report the numbers. To evaluate the trained models on various benchmarks, the widely popular lm-evaluation-harness \cite{eval-harness} repository was used.

\subsection{S3FT Algorithm}
\label{app:algo}
Here we show the algorithm for S3FT.

\begin{algorithm}
\caption{Training Method}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Base Model $\model_{\theta_0}$ \\
Data $D = \{(\inp_i, \outp_i)\}_{i=1}^{n}$
\STATE $D' \gets \{\}$
\FOR{each example $(\inp_i, \outp_i) \in D$}
    \STATE $\pred_i \gets \model_{\theta_0}(\inp_i)$
    \IF{$\pred_i = \outp_i$}
        \STATE $D' \gets D' \cup \{(\inp_i, \pred_i)\}$
    \ELSE
        % \STATE $\tilde{\pred_i}  \gets \model_{\theta_0}(\text{``Rephrase: ''} \concat \outp_i)$
        \STATE $\tilde{y_i}  \gets \model_{\theta_0}(\outp_i)$
        \IF{$\tilde{y_i}= \outp_i$}
            \STATE $D' \gets D' \cup \{(\inp_i, \tilde{y_i})\}$
        \ELSE
            \STATE $D' \gets D' \cup \{(\inp_i, \outp_i)\}$
        \ENDIF
    \ENDIF
\ENDFOR
\STATE Train $\model_{\theta_0}$ on $D'$ to obtain updated parameters $\theta$
\STATE \textbf{Output:} Updated model parameters $\theta$
\end{algorithmic}
\end{algorithm}

\subsection{Judges for evaluating the model performance}
\label{app:llm-judge}
For tasks like reading comprehension, conventional metrics like BLEU and ROUGE are helpful but inadequate to capture the semantics of the generated responses. Thus, we employ a Language Model (LLM) as an evaluator. Specifically, inspired by \cite{badshah2408reference}, we utilize Mistral-instruct-v2 (7B) model as a judge guided by the prompt illustrated in Figure \ref{fig:llm-judge-prompt}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{mbpp_train_prompt.pdf}
    \caption{Prompt used for predicting the base model response on MBPP training dataset.}
    \label{fig:mbpp_predict}
\end{figure}
