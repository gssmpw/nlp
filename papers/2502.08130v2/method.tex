\newcommand{\model}{\mathcal{M}}
\newcommand{\basemodel}{\mathcal{M}_{\theta_0}}

\newcommand{\task}{\mathcal{T}}
\newcommand{\traindata}{\mathcal{D}}
\newcommand{\inp}{x}
\newcommand{\outp}{y}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\sftloss}{\mathcal{L}_{SFT}}
\newcommand{\prob}{Pr}
\newcommand{\pred}{\hat{y}}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\goodsubset}{\mathcal{R}}
\newcommand{\goldsubset}{\mathcal{G}}

\newcommand{\ssrloss}{\mathcal{L}_{SSR}}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{s3ft_arch.pdf}
    \caption{\textbf{An overview of our proposed approach}: Given the input $\inp_i$ and its corresponding gold response $\outp_i$, we employ the base model $\mathcal{M}_{\theta}$ to transform $\outp_i$ such that it is correct but at the same time closer to model's distribution. First, the model predicts the output $\hat{y}$. The judge decides whether the $\outp_i$ is correct. If true, it becomes part of the training dataset; otherwise, we paraphrase $([x_i;y_i])$ to obtain $\tilde{y_i}$. The judge evaluates the correctness of $\tilde{y_i}$. If true, we use $\tilde{y_i}$; otherwise, we use $\outp_i$ as the target response. The resulting dataset $\mathcal{D'}$ is used to train the model.}
    \label{fig:overview}
    \vspace{-0.5em}
\end{figure*}

Let $\model_\theta$ parameterized by $\theta$ be a given large language model. 
Let $\theta = \theta_0$ be the given model weights obtained after pre-training and instruction tuning the model.
We refer to $\model_{\theta_0}$ as the base model.
Further, let $\task$ be the new task that we wish to teach the model $\model_\theta$, and let $\traindata = \{(\inp_i, \outp_i)\}_{i = 1}^{N}$ be the corresponding training dataset for $\task$.
In standard SFT, we backpropagate through the standard Cross Entropy loss over the training dataset, $\traindata$.
However, SFT can cause a degradation of $\model_{\theta_0}$ general capabilities by forcing the model to predict a gold response which is further away from the $\mathcal{M}_{\theta_0}$ responses' distribution.

To solve this, our method relies on two key observations.
First, for many NLP tasks e.g. machine translation, summarization, reading comprehension \etc, the same input $\inp$ can have multiple
valid responses. \citet{nandwani2020neural} define this setup as 1oML (one of many learning) and propose various strategies to handle it, albeit for combinatorial problems.
Second, teaching the model using its own words can help preserve its own distribution. This can regularizes the model training, helping it to tackle catastrophic forgetting and maintaining its general capabilities.
We note that the standard practice of regularization via replay buffer \cite{hayes2020remind},
which involves mixing a subset of instruction-tuning dataset with the given task-specific data $\traindata$ is not always feasible as the instruction-tuning dataset may not be available.
These two observations are the basis for S3FT.
For each example in the data, we start by generating a prediction $\pred_i = \model_{\theta_0}(\inp_i)$ with the base model where $\inp_i$ is the input. 
If $\pred_i$ is equivalent to $\outp_i$, we use $(\inp_i$, $\pred_i)$ for model training. If $\pred_i$ is not equivalent, we use $\model_{\theta_0}$ to rephrase the gold answer $\outp_i$ in its own language, $\tilde{y_i} = \model_{\theta_0}([\inp_i; \outp_i])$. If $\tilde{y_i}$ is not equivalent to $\outp_i$ we use the gold answer, $\outp_i$. Figure \ref{fig:overview} and the algorithm in Appendix \ref{app:algo} gives an overview of our approach.

An important component of S3FT is the ability to identify the equivalence of model's prediction or gold paraphrasing to the gold answer.
Towards that end, we can use judges that aim to assess the equivalence with $\pred_i$, either by heuristics such as checking the bottom-line agreement of the predicted and gold response or employing a stronger LLM as a judge to measure more semantic equivalence.
