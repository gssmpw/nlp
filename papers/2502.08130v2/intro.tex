\newcommand{\mistral}{Mistral-7B-Instruct-v0.2}

\begin{table}[t]
\small
\begin{tabular}{p{0.2\columnwidth}@{}|p{0.75\columnwidth}@{}}
\toprule
\textit{Task} & Write a function to add two lists using map and lambda function. \\ \cmidrule(l){1-2}
\textit{Gold} & def add\_list(nums1,nums2): \\
  & \ \ result = map(lambda x, y:x+y, nums1, nums2) \\
  & \ \ return list(result) \\
& \sethlcolor{cyan!30} \hl{log probability = -84.32} \\ \cmidrule(l){1-2}
\textit{Model} & def add\_list(list1, list2):\\
\textit{Prediction} & \ \ return list(map(lambda x, y:x+y, list1, list2)) \\
& \sethlcolor{lime} \hl{log probability = -36.64} \\ \bottomrule
\end{tabular}
%}
\caption{An example from the MBPP (Python Programming dataset) \cite{austin2021program}, along with \mistral's prediction.}
\label{tab:intro-example}
\vspace{-0.5em}
\end{table}

Large Language Models (LLMs) have made remarkable progress in recent years, demonstrating impressive capabilities across a wide range of tasks, including question-answering \cite{rajpurkar2016squad}, summarization \cite{nallapati2016abstractive}, and more \cite{brown2020language}. Supervised fine-tuning (SFT) of LLMs on task-specific data is a widely used approach to enhance their performance in specialized applications. While fine-tuning improves task accuracy, it can cause the model to overfit to the domain or style present in the training data, potentially limiting its broader applicability. In this work, we focus on exploring how to fine-tune an LLM for a specific task while preserving its general-purpose capabilities.

SFT relies on gold responses for training. We make two key observations when performing SFT on LLMs: (1) in many datasets, model-generated responses—though differing from gold responses—are still valid and acceptable, and (2) the distribution of gold responses often diverges significantly from the model’s own response distribution. For instance, consider an example from the MBPP dataset in Table \ref{tab:intro-example}. The base model, \mistral, assigns a log probability of $-84.32$ to the gold answer. When prompted with the same question, the model generates a response containing the same information as the gold output but with a much higher log probability of $-36.64$. This phenomenon is common in generation tasks, where semantically equivalent responses can have widely varying log-likelihood scores. This observation suggests that model-generated responses can align more closely with the model's native distribution, whereas gold responses may lie further apart. As a result, training exclusively on gold responses risks introducing distributional drift, potentially reducing the model’s ability to generalize effectively.

To address this issue, we propose Selective Self-to-Supervised Fine-Tuning (S3FT), a simple yet powerful technique that utilizes model-generated answers for a subset of the training dataset to adapt the model to desirable behaviours while maintaining generalization. S3FT fine-tunes the model on its own generated output for cases where it behaves desirably and on gold output (or its paraphrase) for the remaining data. This approach allows the model to learn from its successes while benefiting from human-labeled data when needed. 


In our experiments, we show that S3FT outperform existing approaches, including SFT, on diverse tasks, namely code generation~\citet{austin2021program}, math problem solving~\cite{cobbe2021training} and reading comprehension~\cite{kwiatkowski2019natural}. To show that S3FT generalizes better and retains the base model's capabilities, we evaluate on multiple datasets such as MMLU \cite{mmlu}, TruthfulQA \cite{truthfulqa}, and Hellaswag \cite{hellaswag}. We observe that the drop in performance for S3FT on these benchmarks is smaller than that of existing approaches, demonstrating better generalization capabilities.
