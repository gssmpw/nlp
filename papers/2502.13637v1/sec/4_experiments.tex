\section{Experiments}
\label{sec:experiments}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/comparison_8x5.png}
  \caption{Qualitative comparison of the proposed method with existing human affordance generation techniques by Wang \emph{et al.} \cite{wang2017binge}, Zhang \emph{et al.} \cite{zhang2022inpaint}, and Yao \emph{et al.} \cite{yao2023scene}.}
  \label{fig:comparison}
  \vspace{-1.0em}
\end{figure*}

% --------------------------------------------------------------------------------
\subsection{Dataset}
\label{sec:experiments_dataset}
The number of large-scale annotated public datasets for complex 2D affordance generation is significantly limited in the literature. The main challenge is to obtain a specific frame in two different states -- \emph{with} and \emph{without} populated with random persons. Researchers \cite{zhang2022inpaint, zhu2023topnet} have attempted to resolve the issue by randomly removing and inpainting existing person instances from the scene. However, these datasets \cite{zhang2022inpaint, zhu2023topnet} are not publicly available, and our attempt to remove persons from a complex scene leaves significant visual artifacts even with state-of-the-art inpainting techniques \cite{suvorov2022resolution}, making such data generation methods \cite{zhang2022inpaint, zhu2023topnet} insufficient for our purpose.

Following the recent works \cite{wang2017binge, yao2023scene}, we train and evaluate the proposed method on an openly available large-scale sitcom dataset \cite{wang2017binge} for fair comparisons. The dataset comprises 28837 human interaction samples over 11499 video frames extracted from seven sitcoms. The training set consists of 24967 poses over 10009 frames from six sitcoms, while the evaluation set contains 3870 poses over 1490 frames from one sitcom. Each human pose is represented with 16 keypoints following the standard MPII format \cite{andriluka142d}.

% --------------------------------------------------------------------------------
\subsection{Visual results and evaluation metrics}
\label{sec:experiments_results}
\noindent
\textbf{Qualitative analysis:} To demonstrate the efficacy of the proposed method, we compare the visual results against major existing works \cite{wang2017binge, yao2023scene, zhang2022inpaint} on 2D human affordance generation. In \cite{wang2017binge, yao2023scene}, the networks directly learn from a single modality of image features. In \cite{zhang2022inpaint}, the authors introduce an adversarial learning mechanism with two additional modalities of segmentation and depth maps alongside image features. We notice that adopting a transformer-based architecture \cite{yao2023scene} or combining multiple modalities in an adversarial learning method \cite{zhang2022inpaint} provide only marginal improvements over the baseline approach \cite{wang2017binge}. In contrast, the proposed method focuses on imposing a better semantic constraint in the learning strategy by introducing a novel cross-attention mechanism. Fig. \ref{fig:comparison} illustrates a qualitative comparison of the proposed method with existing human affordance generation techniques \cite{wang2017binge, yao2023scene, zhang2022inpaint}. The visual results show that our approach produces more realistic human interactions in complex scenes than previous methods.

\noindent
\textbf{Quantitative evaluation:}
We evaluate the alignment of the generated pose with the ground truth for analytically comparing the proposed method against existing human affordance generation techniques \cite{wang2017binge, yao2023scene, zhang2022inpaint}. In particular, we use the percentage of correct keypoints (PCK/PCKh) \cite{yang2012articulated}, average keypoint distance (AKD), mean absolute error (MAE), mean squared error (MSE), and cosine similarity (SIM) as the evaluation metrics for pose alignment. To estimate the correctness of the estimated scale, we measure the intersection over union (IOU) between the target and predicted pose bounding boxes. Following \cite{yao2023scene}, our comparative study also includes evaluation results from additional baseline methods (heatmap and regression), pose estimation techniques \cite{artacho2020unipose, li2021pose}, and object placement algorithms \cite{zhang2020learning, zhou2022learning}. Additionally, we train and evaluate another variation of the proposed architecture by replacing \emph{\textbf{semantic segmentation maps}} with \emph{\textbf{depth maps}}. \textbf{PCK} and \textbf{PCKh} measure the similarity between two poses by computing the fraction of correctly aligned keypoints, where a valid alignment denotes that the distance between two respective keypoints is within a predetermined tolerance. PCK uses $\alpha * torso~width$, while PCKh uses $\beta * head~size$ as the tolerance threshold, where $0 < \alpha, \beta \leqslant 1$. \textbf{AKD} is the average Euclidean distance between respective pairs of target and predicted keypoints. \textbf{MAE} and \textbf{MSE} measure the average absolute and squared linear deviations, respectively, along both coordinate axes between all respective pairs of actual and inferred keypoints. \textbf{SIM} evaluates the average cosine similarity between the positional vectors corresponding to every target and predicted keypoint pair. \textbf{IOU} enumerates the intersection over union ratio between the bounding rectangles of predicted and target poses. We summarize the quantitative evaluation scores for each competing method in Table \ref{tab:comparison}. The proposed method exhibits significantly better evaluation scores, reflecting the apparent visual superiority of the qualitative analysis.

\noindent
\textbf{Subjective evaluation (User study):}
While the evaluation metrics are analogous to visual rationality for most cases in our experiments, the quantitative scores alone may not be sufficient to claim the superiority of a generation scheme. The reason is the potential uncertainty of a generated human pose. Within the given scene context, it is possible to obtain multiple valid human interactions with the environment. Therefore, the evaluation scores against a specific ground truth pose may not always provide a rational judgment of superiority. We have conducted an opinion-based user study with 42 volunteers to select the most visually realistic sample from a pool of images generated by the competing methods, including ground truth. The mean opinion score \textbf{(MOS)} is estimated as the average fraction of times a method is preferred over the others. As shown in Table \ref{tab:comparison}, the proposed approach achieves a significantly higher user preference over other existing methods, indicating the best semantic integrity in the generated samples similar to the ground truth.

\begin{table*}[t]
\centering
\caption{Quantitative comparison of the proposed method with existing pose estimation \cite{artacho2020unipose, li2021pose}, object placement \cite{zhang2020learning, zhou2022learning}, and affordance generation \cite{wang2017binge, yao2023scene, zhang2022inpaint} techniques. The best scores are in \textbf{bold}, and the second-best scores are \underline{underlined}.}
\label{tab:comparison}
% \resizebox{0.98\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\hline
\rowcolor[HTML]{ECECEC}
  \textbf{Method} &
  \textbf{PCK $\uparrow$} &
  \textbf{PCKh $\uparrow$} &
  \textbf{AKD $\downarrow$} &
  \textbf{MAE $\downarrow$} &
  \textbf{MSE $\downarrow$} &
  \textbf{SIM $\uparrow$} &
  \textbf{IOU $\uparrow$} &
  \textbf{\textcolor{blue}{User Score $\uparrow$}} \\ \hline
Heatmap                                     & 0.363 & 0.422 & 11.298 & ~7.112 & ~53.45 & 0.9864 & 0.402 & 0.000 \\
Regression                                  & 0.386 & 0.451 & 10.929 & ~6.840 & ~51.29 & 0.9899 & 0.426 & 0.000 \\
UniPose \cite{artacho2020unipose}           & 0.387 & 0.447 & ~9.966 & ~6.241 & ~46.78 & 0.9918 & 0.471 & 0.012 \\
PRTR \cite{li2021pose}                      & 0.408 & 0.474 & ~9.724 & ~6.088 & ~45.72 & 0.9934 & 0.489 & 0.025 \\
PlaceNet \cite{zhang2020learning}           & 0.060 & 0.072 & 79.978 & 50.325 & 377.78 & 0.9476 & 0.118 & 0.000 \\
GracoNet \cite{zhou2022learning}            & 0.380 & 0.441 & 10.576 & ~6.614 & ~49.60 & 0.9912 & 0.427 & 0.000 \\ \hline
Wang \emph{et al.} \cite{wang2017binge}     & 0.401 & 0.462 & ~9.940 & ~6.208 & ~46.65 & 0.9928 & 0.482 & 0.022 \\
Zhang \emph{et al.} \cite{zhang2022inpaint} & 0.372 & 0.428 & 10.252 & ~6.409 & ~48.14 & 0.9906 & 0.405 & 0.005 \\
Yao \emph{et al.} \cite{yao2023scene}       &
\underline{0.414} &
\underline{0.479} &
~9.514 &
~5.918 &
~44.86 &
0.9954 &
0.494 &
0.104 \\ \hline
\rowcolor[HTML]{FFFFCC}
\textbf{Ours \emph{(Depth)}} &
0.407 &
0.472 &
\underline{~6.680} &
\underline{~4.163} &
\underline{~32.78} &
\underline{0.9966} &
\underline{0.566} &
\underline{0.205} \\
\rowcolor[HTML]{FFFFCC}
\textbf{Ours \emph{(Semantic)}} &
\textbf{0.433} &
\textbf{0.503} &
\textbf{~6.352} &
\textbf{~3.972} &
\textbf{~29.81} &
\textbf{0.9969} &
\textbf{0.566} &
\textbf{0.299} \\ \hline \hline
Ground Truth                                & 1.000 & 1.000 & ~0.000 & ~0.000 & ~~0.00 & 1.0000 & 1.000 & 0.328 \\ \hline
\end{tabular}%
% }
\end{table*}

\begin{table*}[t]
\centering
\caption{Quantitative ablation analysis of different variations of the proposed network architecture.}
\label{tab:ablation}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccccccc}
\hline
\rowcolor[HTML]{ECECEC}
  \textbf{Model} &
  \textbf{Context} &
  \textbf{PCK $\uparrow$} &
  \textbf{PCKh $\uparrow$} &
  \textbf{AKD $\downarrow$} &
  \textbf{MAE $\downarrow$} &
  \textbf{MSE $\downarrow$} &
  \textbf{SIM $\uparrow$} &
  \textbf{IOU $\uparrow$} \\ \hline
\texttt{\textbf{Baseline}} & None &
0.274 \textcolor{gray}{$\pm$0.006} & 0.322 \textcolor{gray}{$\pm$0.008} & 9.998 \textcolor{gray}{$\pm$0.042} & 6.248 \textcolor{gray}{$\pm$0.034} & 48.920 \textcolor{gray}{$\pm$0.408} & 0.9845 \textcolor{gray}{$\pm$0.0005} & 0.398 \textcolor{gray}{$\pm$0.004} \\ \hline
\texttt{\textbf{Self-I}} & Image &
0.346 \textcolor{gray}{$\pm$0.004} & 0.399 \textcolor{gray}{$\pm$0.005} & 7.899 \textcolor{gray}{$\pm$0.035} & 4.925 \textcolor{gray}{$\pm$0.025} & 38.714 \textcolor{gray}{$\pm$0.406} & 0.9947 \textcolor{gray}{$\pm$0.0004} & 0.458 \textcolor{gray}{$\pm$0.012} \\ \hline
\texttt{\textbf{Self-D}} & Depth &
0.317 \textcolor{gray}{$\pm$0.004} & 0.372 \textcolor{gray}{$\pm$0.006} & 8.582 \textcolor{gray}{$\pm$0.025} & 5.625 \textcolor{gray}{$\pm$0.028} & 42.104 \textcolor{gray}{$\pm$0.344} & 0.9888 \textcolor{gray}{$\pm$0.0002} & 0.406 \textcolor{gray}{$\pm$0.002} \\
\texttt{\textbf{Cross-D/I}} & Image &
0.341 \textcolor{gray}{$\pm$0.004} & 0.387 \textcolor{gray}{$\pm$0.008} & 8.146 \textcolor{gray}{$\pm$0.032} & 5.077 \textcolor{gray}{$\pm$0.028} & 39.972 \textcolor{gray}{$\pm$0.228} & 0.9927 \textcolor{gray}{$\pm$0.0005} & 0.447 \textcolor{gray}{$\pm$0.006} \\
\texttt{\textbf{Cross-I/D}} & Depth &
0.374 \textcolor{gray}{$\pm$0.005} & 0.429 \textcolor{gray}{$\pm$0.005} & 7.260 \textcolor{gray}{$\pm$0.047} & 4.424 \textcolor{gray}{$\pm$0.025} & 35.649 \textcolor{gray}{$\pm$0.301} & 0.9952 \textcolor{gray}{$\pm$0.0002} & 0.498 \textcolor{gray}{$\pm$0.002} \\
\rowcolor[HTML]{FFCCCC}
\texttt{\textbf{Mutual-I+D}} & Image + Depth &
0.407 \textcolor{gray}{$\pm$0.003} & 0.472 \textcolor{gray}{$\pm$0.002} & 6.680 \textcolor{gray}{$\pm$0.031} & 4.163 \textcolor{gray}{$\pm$0.020} & 32.782 \textcolor{gray}{$\pm$0.294} & 0.9966 \textcolor{gray}{$\pm$0.0001} & 0.566 \textcolor{gray}{$\pm$0.004} \\ \hline
\texttt{\textbf{Self-S}} & Semantic &
0.336 \textcolor{gray}{$\pm$0.006} & 0.391 \textcolor{gray}{$\pm$0.009} & 8.174 \textcolor{gray}{$\pm$0.038} & 5.112 \textcolor{gray}{$\pm$0.024} & 38.452 \textcolor{gray}{$\pm$0.404} & 0.9895 \textcolor{gray}{$\pm$0.0001} & 0.418 \textcolor{gray}{$\pm$0.014} \\
\texttt{\textbf{Cross-S/I}} & Image &
0.355 \textcolor{gray}{$\pm$0.005} & 0.407 \textcolor{gray}{$\pm$0.008} & 7.746 \textcolor{gray}{$\pm$0.039} & 4.904 \textcolor{gray}{$\pm$0.018} & 36.354 \textcolor{gray}{$\pm$0.477} & 0.9950 \textcolor{gray}{$\pm$0.0002} & 0.478 \textcolor{gray}{$\pm$0.008} \\
\texttt{\textbf{Cross-I/S}} & Semantic &
0.398 \textcolor{gray}{$\pm$0.006} & 0.458 \textcolor{gray}{$\pm$0.004} & 6.904 \textcolor{gray}{$\pm$0.045} & 4.325 \textcolor{gray}{$\pm$0.029} & 32.402 \textcolor{gray}{$\pm$0.344} & 0.9958 \textcolor{gray}{$\pm$0.0002} & 0.515 \textcolor{gray}{$\pm$0.005} \\
\rowcolor[HTML]{CCFFCC}
\texttt{\textbf{Mutual-I+S}} & Image + Semantic &
\textbf{0.433} \textcolor{gray}{$\pm$0.004} & \textbf{0.503} \textcolor{gray}{$\pm$0.004} & \textbf{6.352} \textcolor{gray}{$\pm$0.036} & \textbf{3.972} \textcolor{gray}{$\pm$0.022} & \textbf{29.810} \textcolor{gray}{$\pm$0.326} & \textbf{0.9969} \textcolor{gray}{$\pm$0.0001} & \textbf{0.566} \textcolor{gray}{$\pm$0.002} \\ \hline %\hline
% \texttt{\textbf{Ground Truth}} & &
% 1.000 \textcolor{gray}{$\pm$0.000} & 1.000 \textcolor{gray}{$\pm$0.000} & 0.000 \textcolor{gray}{$\pm$0.000} & 0.000 \textcolor{gray}{$\pm$0.000} & 00.000 \textcolor{gray}{$\pm$0.000} & 1.0000 \textcolor{gray}{$\pm$0.0000} & 1.000 \textcolor{gray}{$\pm$0.000} \\ \hline
\end{tabular}%
}
\end{table*}

\noindent
\textcolor{black}{\textbf{Visualization of the learned distribution:}
Due to many feasible outcomes, there are substantial ambiguities when inferring a possible pose at a random location within the scene, depending on the surrounding context. The ground truth is only one of the many possibilities. For example, a selected position in front of a chair can lead to either \emph{standing} or \emph{sitting} postures. Therefore, the association between an estimated pose and scene objects is an ideal way to visualize the sampled pose distribution. We discover two broad pose categories, \emph{standing} and \emph{sitting}, in the dataset by manually inspecting the given templates. To visualize the learned distribution, we randomly sample 10000 poses for a scene and assign a pose category (\emph{standing} or \emph{sitting}) to each sampled location based on the predicted pose template at that position. Then, we visualize the bivariate distribution of the sampled coordinates for each pose category. Fig. \ref{fig:distribution} shows a few such visualizations, exhibiting ideal associations between a pose type and the scene objects.}

\begin{figure}[t]
\centering
\captionsetup[subfloat]{labelfont=bf}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/1a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/1b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/1c.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/2a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/2b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/2c.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/3a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/3b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/distribution/3c.png}}
\caption{\textcolor{black}{Visualization of the learned distribution. \textbf{(Left)} Input scene. \textbf{(Middle)} Distribution of \emph{standing} poses. \textbf{(Right)} Distribution of \emph{sitting} poses.}}
\label{fig:distribution}
\vspace{-1.0em}
\end{figure}

% --------------------------------------------------------------------------------
\subsection{Ablation study}
\label{sec:experiments_ablation}

\begin{figure*}[t]
\centering
\captionsetup[subfloat]{labelfont=bf}
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_d/2.png}}\hfil
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_d/4.png}}\hfil
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_d/6.png}}\hfil
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_s/2.png}}\hfil
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_s/4.png}}\hfil
\subfloat[]{\includegraphics[width=0.16\linewidth]{fig/ablation/1441_s/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_d/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_d/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_d/6.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_s/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_s/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/1461_s/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_d/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_d/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_d/6.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_s/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_s/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/2512_s/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_d/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_d/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_d/6.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_s/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_s/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3093_s/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_d/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_d/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_d/6.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_s/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_s/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3104_s/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_d/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_d/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_d/6.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_s/2.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_s/4.png}}\hfil
\subfloat{\includegraphics[width=0.16\linewidth]{fig/ablation/3776_s/6.png}}
\caption{Qualitative ablation analysis of the proposed network architecture with different input modalities. \textbf{(a)} Depth map. \textbf{(b)} Estimated bounding region from depth context. \textbf{(c)} Estimated pose from depth context. \textbf{(d)} Semantic map. \textbf{(e)} Estimated bounding region from semantic context. \textbf{(f)} Estimated pose from semantic context.}
\label{fig:ablation}
% \vspace{-0.5em}
\end{figure*}

In the ablation analysis, we evaluate ten different network configurations by varying attention mechanisms in the proposed MCMA block and altering contextual modalities (\emph{depth map} or \emph{semantic map}) to determine an optimal network architecture. The \textbf{\texttt{Baseline}} architecture entirely excludes contextual supervision by removing all MCMA blocks from the proposed architecture. Configurations \textbf{\texttt{Self-I}}, \textbf{\texttt{Self-D}}, and \textbf{\texttt{Self-S}} also drop all MCMA blocks but introduce \emph{self-attention} on a single modality. In particular, \textbf{\texttt{Self-I}} uses self-attention on the image $I$ itself, while \textbf{\texttt{Self-D}} and \textbf{\texttt{Self-S}} use self-attention on the \emph{depth map} $D$ and \emph{semantic segmentation map} $S$, respectively. The next four architectures \textbf{\texttt{Cross-D/I}}, \textbf{\texttt{Cross-I/D}}, \textbf{\texttt{Cross-S/I}}, and \textbf{\texttt{Cross-I/S}} introduce \emph{cross-attention} from an additional input stream. Specifically, for configurations \textbf{\texttt{Cross-D/I}} and \textbf{\texttt{Cross-S/I}}, we compute query ($Q$) from $D$ and $S$, respectively, while retrieving key ($K$) and value ($V$) from $I$. Likewise, for configurations \textbf{\texttt{Cross-I/D}} and \textbf{\texttt{Cross-I/S}}, we compute $Q$ from $I$, while estimating $K$ and $V$ from the contextual input $D$ or $S$, respectively. The final two network variants \textbf{\texttt{Mutual-D}} and \textbf{\texttt{Mutual-S}} use the proposed \emph{mutual cross-modal attention} (MCMA) mechanism utilizing $D$ or $S$ as the auxiliary contextual supervision, respectively, alongside $I$. In all our experiments, we estimate the \emph{depth maps} using a recent monocular depth estimation technique \emph{Depth Anything} \cite{yang2024depth} and the \emph{semantic segmentation maps} using a recent transformer-based universal image segmentation method \emph{OneFormer} \cite{jain2023oneformer}.

To measure the spatial alignment of the estimated pose against the ground truth, we evaluate PCK, PCKh, AKD, MAE, MSE, and SIM. Additionally, we compute the IOU between the predicted and target pose bounding boxes to measure the correctness of inferred scales. Table \ref{tab:ablation} summarizes the evaluation scores for every network variant in our ablation study. The analysis shows that additional supervision from other modalities generally results in better performance. Introducing the MCMA block into the architecture further improves this performance gain by a significant margin, reflecting the efficacy of the proposed approach for robust scene context representation. Interestingly, we observe that \emph{semantic segmentation maps} generally perform better than \emph{depth maps} as auxiliary contextual input. We hypothesize that the object labels in a segmentation map provide additional semantic context to the model. Fig. \ref{fig:ablation} shows a visual comparison of the predicted pose from \emph{depth}-context against \emph{semantic}-context.

\textcolor{black}{To investigate the impact of semantic label granularity, pose templates, and dedicated VAEs for scale and deformation on the proposed method, we analyze six additional configurations \texttt{\textbf{A - F}} of the architecture. The first four models \texttt{\textbf{A - D}} use different numbers of semantic labels in the segmentation maps. Specifically, configuration \texttt{\textbf{A}} uses 2 labels for \emph{foreground} (all objects merged) and \emph{background}. Configuration \texttt{\textbf{B}} uses 3 labels by dividing the foreground objects into \emph{non-human objects} and \emph{humans}. Configuration \texttt{\textbf{C}} uses 4 labels by further splitting the non-human object labels into \emph{fixed} (wall, floor, stairs) and \emph{movable} (table, chair, bed) object categories. Configuration \texttt{\textbf{D}} retains all 150 initially estimated semantic labels unaltered. To verify the requirements of multiple pose templates, Configuration \texttt{\textbf{E}} drops the classifier from the architecture and uses the first template as a fixed predefined pose. To validate the necessity of 2 dedicated VAEs for separately estimating scale and deformation parameters, Configuration \texttt{\textbf{F}} uses a single unified VAE to predict both scale and deformation parameters as a single vector. Finally, Configuration \texttt{\textbf{G}} denotes the proposed architecture that uses 8 semantic labels, 30 pose templates with a template classifier, and 2 dedicated VAEs for estimating scale and deformation parameters separately.}

\begin{table}[t]
\centering
\caption{\textcolor{black}{Quantitative ablation analysis of different variations of the proposed network architecture.}}
\label{tab:ablation_2}
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccccc}
\hline
\rowcolor[HTML]{ECECEC}
  \textcolor{black}{\textbf{Model}}            &
  \textcolor{black}{\textbf{PCK $\uparrow$}}   &
  \textcolor{black}{\textbf{PCKh $\uparrow$}}  &
  \textcolor{black}{\textbf{AKD $\downarrow$}} &
  \textcolor{black}{\textbf{MAE $\downarrow$}} &
  \textcolor{black}{\textbf{MSE $\downarrow$}} &
  \textcolor{black}{\textbf{SIM $\uparrow$}}   &
  \textcolor{black}{\textbf{IOU $\uparrow$}}   \\ \hline
\textcolor{black}{\texttt{\textbf{A}}} &
  \textcolor{black}{0.217}  &
  \textcolor{black}{0.267}  &
  \textcolor{black}{11.941} &
  \textcolor{black}{7.223}  &
  \textcolor{black}{52.88}  &
  \textcolor{black}{0.9821} &
  \textcolor{black}{0.409}  \\
\textcolor{black}{\texttt{\textbf{B}}} &
  \textcolor{black}{0.292}  &
  \textcolor{black}{0.351}  &
  \textcolor{black}{~9.112} &
  \textcolor{black}{5.917}  &
  \textcolor{black}{45.11}  &
  \textcolor{black}{0.9901} &
  \textcolor{black}{0.477}  \\
\textcolor{black}{\texttt{\textbf{C}}} &
  \textcolor{black}{0.377}  &
  \textcolor{black}{0.441}  &
  \textcolor{black}{~7.019} &
  \textcolor{black}{4.508}  &
  \textcolor{black}{37.79}  &
  \textcolor{black}{0.9964} &
  \textcolor{black}{0.564}  \\
\textcolor{black}{\texttt{\textbf{D}}} &
  \textcolor{black}{0.376}  &
  \textcolor{black}{0.436}  &
  \textcolor{black}{~7.313} &
  \textcolor{black}{4.573}  &
  \textcolor{black}{40.12}  &
  \textcolor{black}{0.9959} &
  \textcolor{black}{0.557}  \\
\textcolor{black}{\texttt{\textbf{E}}} &
  \textcolor{black}{0.352}  &
  \textcolor{black}{0.400}  &
  \textcolor{black}{~7.915} &
  \textcolor{black}{4.841}  &
  \textcolor{black}{47.69}  &
  \textcolor{black}{0.9951} &
  \textcolor{black}{0.541}  \\
\textcolor{black}{\texttt{\textbf{F}}} &
  \textcolor{black}{0.369}  &
  \textcolor{black}{0.423}  &
  \textcolor{black}{~7.206} &
  \textcolor{black}{4.523}  &
  \textcolor{black}{38.02}  &
  \textcolor{black}{0.9961} &
  \textcolor{black}{0.563}  \\
\rowcolor[HTML]{CCFFCC}
\textcolor{black}{\texttt{\textbf{G}}} &
  \textcolor{black}{\textbf{0.433}}  &
  \textcolor{black}{\textbf{0.503}}  &
  \textcolor{black}{\textbf{~6.352}} &
  \textcolor{black}{\textbf{3.972}}  &
  \textcolor{black}{\textbf{29.81}}  &
  \textcolor{black}{\textbf{0.9969}} &
  \textcolor{black}{\textbf{0.566}}  \\ \hline
\end{tabular}%
}
\end{table}

\textcolor{black}{Table \ref{tab:ablation_2} summarizes evaluation scores for all the network configurations. The results show that using \emph{too few} or \emph{too many} semantic labels does not contribute towards performance improvements. Also, with a fixed template, the linear deviations between the target and template keypoints can vary more randomly. For example, the deviation of a \emph{standing} pose template from a \emph{standing} target pose is often much smaller than from a \emph{sitting} target pose. However, with multiple pose templates, the model estimates a probable pose (template) first and then samples linear deformation parameters from a more predictable range to translate the template into the target pose. Likewise, the possible ranges of scaling and deformation parameters are widely different. The scaling parameters are the height and width of the minimal bounding box around a human pose. These values are much larger than the deformation parameters comprising small linear deviations between two sets of pose keypoints. So, forcing the architecture to infer the scaling and deformation parameters with a single unified VAE causes noticeable instability due to poor normalization. These observations further justify the proposed network design.}

% --------------------------------------------------------------------------------
\subsection{Limitations}
\label{sec:experiments_limitations}

\begin{figure}[t]
\centering
\captionsetup[subfloat]{labelfont=bf}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/44/2.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/44/4.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/44/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/2868/2.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/2868/4.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/2868/6.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/1114/2.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/1114/4.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/limitations/1114/6.png}}
\caption{Visual examples of limiting cases inferred from the proposed method. \textbf{(Left)} Auxiliary semantic context. \textbf{(Middle)} Estimated bounding region. \textbf{(Right)} Estimated human pose.}
\label{fig:limitations}
\end{figure}

Estimating a valid pose for a non-existent person in complex scenes is a fundamentally challenging problem with multiple acceptable solutions other than the ground truth. For example, for a scene containing a bed, the probable pose can be standing, sitting or lying down, with many feasible variations for each case. While on most occasions, the proposed method generally infers realistic poses with affordance-aware human interactions, there are a few instances when the technique fails to sample an acceptable posture. Alongside the advantages of the modularity and flexibility in the disentangled multi-stage approach, a potential error propagation problem also exists. More precisely, inferential error in an earlier stage may propagate through later stages, negatively impacting the overall predictive performance of the pipeline. We show a few examples of such limiting cases in Fig. \ref{fig:limitations}, illustrating the misinterpretation in sampled location \textbf{(top row)}, scale \textbf{(middle row)} or deformation \textbf{(bottom row)} by the proposed method.

% --------------------------------------------------------------------------------
\subsection{Downstream applications}
\label{sec:experiments_applications}

\begin{figure}[t]
\centering
\captionsetup[subfloat]{labelfont=bf}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/1a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/1b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/1c.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/2a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/2b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/2c.png}}
\vspace{-0.75em}
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/3a.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/3b.png}}\hfil
\subfloat{\includegraphics[width=0.32\linewidth]{fig/rendering/3c.png}}
\caption{Visual examples of downstream rendering of human instances. \textbf{(Left)} Input scene. \textbf{(Middle)} Estimated pose by the proposed method. \textbf{(Right)} Rendered person using PIDM \cite{bhunia2023person}.}
\label{fig:rendering}
\end{figure}

The ability to sample semantically-valid scene-aware complex human poses directly facilitates downstream tasks such as novel person instance generation using off-the-shelf pose transfer or pose rendering techniques. Such downstream rendering to inject novel person instances into complex scenes is critical in various application domains, including augmented / virtual reality, digital media, and synthetic data generation. While the state-of-the-art keypoint-based person generation techniques provide high-quality photorealistic rendering, the algorithms also demand precise supervision of the target pose. Therefore, the proposed method must sample a valid and accurate human pose for successful downstream rendering. Fig. \ref{fig:rendering} shows a few examples of rendered persons using an off-the-shelf pose transfer technique \emph{PIDM} \cite{bhunia2023person} on sampled poses from our method, demonstrating the geometric correctness of the predicted keypoints.
