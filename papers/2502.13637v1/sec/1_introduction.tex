\section{Introduction}
\label{sec:introduction}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/teaser.png}
  \caption{Overview of the proposed method. \textbf{Left:} Predicted locations for a new person in the scene. \textbf{Middle:} Estimated scale at each predicted location. \textbf{Right:} Final human pose estimated after scaling and deformation at each predicted location.}
  \label{fig:introduction}
\end{figure*}

The original conception \cite{gibson1979ecological} of relating perception with action describes affordance as the opportunities for interaction with the environment. In computer vision, human affordance prediction involves probabilistic modeling of novel human actions such that the estimated pose interprets a semantically meaningful interaction with the environment. The task is fundamental to many vision problems, such as machine perception, robot navigation, scene understanding, contextually sound novel human pose generation, and content creation. However, predicting a contextually relevant valid pose for a non-existent person is extremely challenging because, unlike the generic pose estimation task, we do not have an actual human body for supervision. So, in this case, the generator has to rely exclusively on the environmental semantics, requiring an intricate focus on the scene context representation.

Earlier works on affordance-aware human pose generation have explored knowledge base representation \cite{zhu2014reasoning}, social reasoning constraints \cite{chuang2018learning}, variational autoencoder \cite{wang2017binge}, adversarial learning \cite{wang2021scene, zhang2022inpaint} and transformer \cite{yao2023scene}. However, the majority of existing methods investigate different network design philosophies while putting less emphasis on the contextual representation of the scene. Unlike 3D, the lack of rich information about the surrounding environment in a 2D scene makes the problem particularly challenging and requires a robust representation of the scene context.
In this paper, we propose an affordance-aware human pose generation method in complex 2D indoor scenes by introducing a novel context representation technique leveraging cross-attention between two spatial modalities. The key idea is to mutually attend the convolution feature spaces from the scene image and the corresponding semantic segmentation map to get a modulated encoding of the scene context. Initially, we estimate a probable location where a person can be centered using a VAE conditioned on the global context encoding of the entire scene. After determining a potential center, local context embeddings are computed from square patches centered around that location at multiple scales. Next, we use a classifier on the multi-scale context vectors to predict the most likely pose as the template class from a set of existing human pose candidates. Finally, we use two VAEs conditioned on the context embeddings and the template class to sample the scale and linear deformations separately. The estimated scale and deformations are applied to the predicted pose template to get the target pose.

\vspace{1.20em}

\noindent
\textbf{Contributions:} The main contributions of the proposed work are summarized as follows.

\begin{itemize}
  \item We propose an affordance-aware human pose generation method in complex 2D indoor scenes by introducing a novel scene context representation technique that leverages mutual cross-attention between two spatial modalities for robust semantic encoding.
  \item Unlike the existing methods where the target position is user-defined, the proposed method utilizes global scene context to sample probable locations for the target, which provides additional flexibility for constructing a fully automated human affordance generation pipeline.
\end{itemize}

The remainder of the paper is organized as follows. We discuss the relevant literature in Sec. \ref{sec:related_work}. The proposed approach is discussed in Sec. \ref{sec:method}. Sec. \ref{sec:experiments} describes the dataset, experimental protocols, evaluation metrics, qualitative results, quantitative analysis, and a detailed ablation study. Finally, we conclude the paper in Sec. \ref{sec:conclusions} with a summary of key findings, potential use cases, and future scopes.

