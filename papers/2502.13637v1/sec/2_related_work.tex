\section{Related Work}
\label{sec:related_work}

The original investigation \cite{gibson1979ecological} on the relationship between visual perception and human action defines \emph{affordance} as the opportunities for interaction with the surrounding environment. Behavioral studies on regular and cognitively impaired persons have shown evidence that perception results in both visual and motor signals in the human brain. An extended study \cite{anderson2002attentional} shows that visual attention to the spatial characteristics of the perceived objects initiates automatic motor signals for different actions. In computer vision, human affordance learning involves novel pose prediction such that the estimated pose represents a valid human action within the scene context. The task is fundamental to many problems requiring robust semantic reasoning about the environment, such as human motion synthesis \cite{wang2021scene} and scene-aware human pose generation \cite{wang2017binge, roy2016multi, zhang2022inpaint, yao2023scene}.

Earlier methods of affordance learning have explored knowledge mining \cite{zhu2014reasoning} and multimodal feature cues \cite{roy2016multi} to address the problem. In \cite{zhu2014reasoning}, the authors use a Markov Logic Network for constructing a knowledge base by extracting several object attributes from different image and metadata sources, which can perform various downstream visual inference tasks without any additional classifier, including zero-shot affordance prediction. In \cite{roy2016multi}, the authors use depth map, surface normals, and segmentation map as multimodal cues to train a multi-scale convolutional neural network (CNN) for scene-level semantic label assignment associated with specific human actions. In \cite{do2018affordancenet}, the authors design a multi-branch end-to-end CNN with two separate pathways for object detection and affordance label assignment to achieve high real-time inference throughput. Researchers \cite{chuang2018learning} have also explored socially imposed constraints for affordance learning. In \cite{chuang2018learning}, the authors propose a graph neural network (GNN) to propagate contextual scene information from egocentric views for action-object affordance reasoning.

Probabilistic modeling of scene-aware human motion generation also involves semantic reasoning of human interaction with the environment. Initial works on human motion synthesis have taken different architectural approaches, such as sequence-to-sequence models \cite{barsoum2018hp}, generative adversarial networks (GAN) \cite{barsoum2018hp, cai2018deep, yang2018pose}, graph convolutional networks (GCN) \cite{yan2019convolutional}, and variational autoencoders (VAE) \cite{guo2020action2motion}. However, these methods have mostly ignored the role of environmental semantics. Due to potential uncertainty in human motion, in a recent approach \cite{wang2021scene}, the authors address such motion synthesis with a GAN conditioned on scene attributes and motion trajectory to predict probable body pose dynamics.

One key challenge of human affordance generation in 2D scenes is the lack of large-scale datasets with rich pose annotations. In \cite{wang2017binge}, the authors compile the only public dataset of annotated human body poses in complex 2D indoor scenes by extracting frames from sitcom videos. Aiming to generate a contextually valid human affordance at a user-defined location, the authors propose sampling the scale and deformation parameters for an existing human pose template using a VAE conditioned on the localized image patches as scene context. In \cite{zhang2022inpaint}, the authors introduce a two-stage GAN architecture for achieving a similar goal by estimating the affine bounding box parameters to localize a probable human in the scene and then generating a potential body pose at that location. The method uses the input scene, corresponding depth, and segmentation maps as semantic guidance. In \cite{yao2023scene}, the authors propose a transformer-based approach with knowledge distillation for generating human affordances in 2D indoor scenes.

