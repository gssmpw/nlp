\section{Introduction}
\label{sec:introduction}
In robotics, traditional map representations such as point-cloud and voxel maps constitute a critical component of the robotics stack, enabling downstream behavior prediction, planning, and control across many problem domains, e.g., navigation and manipulation. However, these map representations often lack the expressiveness required to capture high-fidelity visual details and semantics \cite{kerbl20233d}, limiting their applications in fine-grained robotics tasks, e.g., in dexterous open-vocabulary manipulation \cite{shen2023distilled}. To address these fundamental limitations, recent robotics research has adopted radiance fields as flexible, high-fidelity $3$D scene representations, e.g., in robot navigation \cite{chen2024splat, qiu2024learning} and manipulation \cite{rashid2023language, shorinwa2024splat}. Radiance fields, e.g., neural radiance fields (NeRFs) \cite{mildenhall2021nerf} and Gaussian Splatting (GSplat) \cite{kerbl20233d}, are trained entirely from monocular images, typically collected by a single robot on a single deployment.

However, practical real-world robot mapping requires multiple deployments and multiple robot platforms, especially when mapping large-scale areas. For example, mobile robots have a limited battery life, while fixed-base robotic manipulators have a limited workspace, making map registration a necessity for covering large-scale areas.
Fusing map information across multiple robot platforms and deployments remains a key challenge, particularly with radiance field maps.
Prior work has explored fusing multiple radiance field maps \cite{chang2025gaussreg, yuan2024photoreg}; however, these methods either require a good initialization of inter-map correspondences or access to the camera poses and images, which is often unavailable in many practical situations.
Moreover, these methods often fail in unstructured real-world environments, an important operational domain for robots.
To address these challenges, we introduce \emph{\algname}, a semantic, initialization-free registration algorithm for multi-robot Gaussian Splatting maps.

Although often unexploited, many real-world scenes contain rich semantic information, e.g., associated with objects such as vehicles, people, utensils, and vegetation. Leveraging this key insight, \algname trains a semantic GSplat to directly embed semantic features in GSplat maps and subsequently uses the inherent semantics in the local maps to identify feature-rich regions of the local maps, providing a more reliable set of Gaussians for the identification of candidate correspondences. This critical design choice underpins the superior performance of \algname. Specifically, the core challenge in map registration can be largely attributed to the difficulty in identifying accurate correspondences between points \cite{tam2012registration}. In fact, given accurate correspondences, the map registration problem can be solved efficiently in closed-form. By centering the registration problem on feature-rich regions of the local maps to derive a reliable set of correspondences, \algname addresses this core challenge. Subsequently, \algname harnesses the robustness of semantic features to formulate a geometric optimization problem for a coarse non-rigid relative transformation aligning Gaussian primitives across the local maps. We solve the geometric optimization problem efficiently in closed-form. Although the coarsely aligned map may be satisfactory in certain scenarios, the fused map often lacks the photorealism afforded by GSplat  maps. To address this weakness, \algname leverages novel-view synthesis of GSplat maps to render images across the local maps, used as a supervision signal for computing a high-accuracy relative transformation. To guard against the impacts of inaccurate renderings from the local GSplats, we utilize a semantics-based image filter to identify reliable candidate images, which we use for supervision.
Consequently, \algname generates photorealistic fused GSplat maps from the local multi-robot maps, illustrated in \Cref{fig:banner}, where we show the key components of \algname.

We demonstrate the superior effectiveness of \algname compared to both existing GSplat registration methods and classical point-cloud registration methods across different real-world datasets, including standard benchmarks for radiance fields and data collected across three different robot hardware platforms: a quadruped, drone, and fixed-base manipulator. In almost all settings, \algname achieves lower rotation, translation, and scale errors compared to all baselines, especially in the quadruped mapping task, where \algname achieves about $90$x lower rotation error, $300$x lower translation error, and $44$x lower scale errors. We summarize our contributions:
\begin{itemize}
    \item We introduce a \emph{semantics-grounded} feature extraction and matching method for GSplat map registration, centering the registration problem on feature-rich regions, addressing a key challenge of registration algorithms.
    \item We derive a Gaussian-to-Gaussian registration procedure for coarse alignment, utilizing \emph{semantic correspondence} to identify and mitigate the effects of outliers in the registration process.
    \item We present a photometric registration procedure, leveraging novel-view synthesis of GSplats and a \emph{semantics-based} image filter to compute high-accuracy relative transformations, generating photorealistic fused maps.
    \item Together, these components constitute \algname, enabling the registration of multi-robot GSplat maps, with \emph{zero} access to source images or poses and no inter-map relative pose initialization.
\end{itemize}
