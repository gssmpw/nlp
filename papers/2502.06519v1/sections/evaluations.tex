\section{Experiments}
\label{sec:evaluation}
We examine the performance of \algname in comparison to existing registration methods for Gaussian Splatting and point clouds. Specifically, we compare two variants of \algname---i.e., \algname-NR, which solves the optimization problem \eqref{eq:coarse_registration} in closed-form without RANSAC, and \algname-R, which utilizes RANSAC for coarse registration---to the GSplat registration methods GaussReg \cite{chang2025gaussreg} and PhotoReg \cite{yuan2024photoreg}, in addition to RANSAC-based global registration (RANSAC-GR) \cite{fischler1981random, holz2015registration}, Fast Global Registration (FGR) \cite{zhou2016fast}, and variants of the Iterative Closest Point (ICP) \cite{rusinkiewicz2001efficient, park2017colored}. We evaluate each method not only on standard benchmark datasets for radiance fields, but also on real-world data collected by heterogeneous robot platforms, including a quadruped, drone, and manipulator (in the case of SIREN). In all our experiments, we only require the trained GSplat models as input; however, some of the baselines require access to the set of camera poses, which we provide when evaluating these methods.
Further, we ablate the different components of \algname, to quantify the relative improvements in performance provided by each component, and examine the gains in visual fidelity afforded by finetuning the fused model. We provide these results in Appendix~\ref{sec:appendix_experiments}, as well as additional discussion of the results presented in this section. 
Lastly, we demonstrate \algname in collaborative multi-robot mapping, where the mapping task cannot be accomplished by a single robot, necessitating mapping with multiple robots for task success. 

\phantomsection
\label{ssec:experiment_metrics}
\smallskip
\noindent\textbf{Experimental Setup and Metrics.}
For the real-world robot data, we utilize the Unitree Go1 Quadruped and a Modal AI drone with an onboard camera and the Franka Panda manipulator with a wrist camera to collect RGB images. In addition, we evaluate all methods on the real-world scenes in the Mip-NeRF360 dataset \cite{barron2022mip}, a state-of-the-art benchmark dataset for neural rendering. We train the GSplat models using the original implementation provided by the authors of \cite{kerbl20233d} for baselines which require this pipeline and utilize Nerfstudio \cite{tancik2023nerfstudio} for \algname. We execute SIREN on a desktop computer with a 24GB NVIDIA GeForce RTX 3090 GPU and the baselines on an H20 GPU after training the GSplat maps for $30000$ iterations.
We note that in robotics, the geometric fidelity of robot's map is of significant importance for effective localization and collision avoidance. Hence, we compare all methods in terms of the rotation error (RE) [deg.], translation error (TE), and scale error (SE) [in non-metric units] attained by each method, in addition to the computation time (CT) [sec.]. Moreover, we examine the photometric quality of the fused maps generated by each method, computing the peak signal-to-noise ratio (PSNR), the structural similarity index measure (SSIM), and the learned perceptual image patch similarity (LPIPS), standard metrics in the computer vision community for assessing visual fidelity. 
We provide color-coded results for each metric with the red shade denoting the top-performing statistic, the yellow shade denoting the second-best, and the green shade denoting the third-best. In all the registration methods, we do not pre-process the individual submaps to remove floaters (i.e., non-existing geometry). Consequently, floaters present in these submaps are retained in the fused map.

\subsection{Mip-NeRF360 Dataset}
We utilize the \emph{Playroom}, \emph{Truck}, and \emph{Room} scenes in the Mip-NeRF360 Dataset. These real-world scenes were all collected in realistic settings with natural lighting effects, both indoors and outdoors. While the \emph{Playroom} and \emph{Room} scenes were captured indoors, the \emph{Truck} scene was captured outdoors. We split the datasets into two subsets with varying overlap. Specifically, the first subset of the \emph{Truck} scene captures the left side of the truck, while the second subset captures the right side of the truck. The only overlap between both subsets occurs at the front and rear of the truck. We split the \emph{Room} scene into two subsets following the same procedure. In the \emph{Playroom} scene, we allow for greater overlap, with the density of images per subregion of the scene varying between both subsets. We train independent GSplat maps for each scene-subset pair.  

\smallskip
\noindent\textbf{Geometric Evaluation.}
In \Cref{tab:baseline_geometric_performance_metrics}, we report the geometric errors of each registration method across the three scenes. \algname-R, our method, achieves the lowest rotation and translation errors in two of the three scenes (\emph{Playroom} and \emph{Truck}): with about $1.14$x to $8.89$x lower rotation errors and about $6$x to $46$x lower translation errors compared to the baseline methods. Meanwhile, in the \emph{Room} scene, \algname-NR achieves the lowest translation and scale error, with \algname-R achieving the second-best performance on these metrics. In summary, \algname achieves the lowest geometric errors (i.e., rotation, translation, and scale errors) across all scenes, except the rotation error in the \emph{Room} scene.

\begin{table*}[th]
	\centering
	\caption{Geometric performance of the registration algorithms on the Mip-NeRF360 dataset (see Section~\ref{ssec:experiment_metrics} for a description of the metrics).}
	\label{tab:baseline_geometric_performance_metrics}
	\begin{adjustbox}{width=\linewidth}
		{\begin{tabular}{l | c c c c | c c c c | c c c c}
				\toprule
                    & \multicolumn{4}{c |}{\emph{Playroom}} & \multicolumn{4}{c |}{\emph{Truck}} & \multicolumn{4}{c}{\emph{Room}} \\
				Methods & RE $\downarrow$ & TE  $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ & RE $\downarrow$ & TE $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ & RE $\downarrow$ & TE $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ \\
				\midrule
                    PhotoReg \cite{yuan2024photoreg} & 6.036 & 18806 & 841.3 & 2177 & 177.3 & 2856 & 444.0 & 1814 & \cellcolor{WildStrawberry!40}0.161 & 4983 & 452.7 & 1409 \ \\
                    GaussReg \cite{chang2025gaussreg} & 0.766  & 55.50 & \cellcolor{GreenYellow!40}0.364 & 15.06 & 21.10 & \cellcolor{GreenYellow!40}316.3 & 16.76 & 5.174 & 7.464  & 628.3 & \cellcolor{GreenYellow!40}91.97 & 6.932 \\
                    RANSAC-GR \cite{fischler1981random, holz2015registration} & 4.835 & 56.22 & 17.85 & 0.996 & 46.72 & 2642 & \cellcolor{GreenYellow!40}13.64 & \cellcolor{WildStrawberry!40}{2.569} & 8.139 & \cellcolor{GreenYellow!40}194.7 & 152.5 & 0.517 \\
                    FGR \cite{zhou2016fast} & 2.988 & 18.83 & 14.37 & \cellcolor{WildStrawberry!40}{0.887} & 3.778 & 2231 & 79.45 & 3.480 & 4.869 & 265.6 & 219.6 & \cellcolor{WildStrawberry!40}{0.511} \\
                    ICP \cite{rusinkiewicz2001efficient} & 2.362 & 19.11 & 14.37 & 2.127 & \cellcolor{GreenYellow!40}3.672 & 2232 & 79.45 & 3.805 & 5.154 & 266.1 & 219.6 & 1.579 \\
                    Colored-ICP \cite{park2017colored} & \cellcolor{Goldenrod!40}{0.194} & \cellcolor{GreenYellow!40}{12.28} & \textcolor{black}{14.37} & 3.951 & 4.043 & 2250 & 79.45 & 6.392 & 2.256 & 232.7 & 219.6 & 3.815 \\
                    \algname-NR [{Ours}] & \cellcolor{GreenYellow!40}0.348 & \cellcolor{Goldenrod!40}{4.860} & \cellcolor{Goldenrod!40}{0.282} & 41.16 & \cellcolor{Goldenrod!40}{0.511} & \cellcolor{Goldenrod!40}{8.07} & \cellcolor{Goldenrod!40}9.581 & 53.42 & \cellcolor{GreenYellow!40}{0.381} & \cellcolor{WildStrawberry!40}{2.648} & \cellcolor{WildStrawberry!40}{1.016} & 40.24 \\
                    \algname-R [{Ours}] & \cellcolor{WildStrawberry!40}{0.170} & \cellcolor{WildStrawberry!40}{1.933} & \cellcolor{WildStrawberry!40}{0.170} & 39.73 & \cellcolor{WildStrawberry!40}{0.413} & \cellcolor{WildStrawberry!40}{6.845} & \cellcolor{WildStrawberry!40}{2.548} & 52.47 & \cellcolor{Goldenrod!40}{0.237} & \cellcolor{Goldenrod!40}{3.289} & \cellcolor{Goldenrod!40}{2.673} & 39.71 \\
				\bottomrule
		\end{tabular}}
	\end{adjustbox}
\end{table*}

\smallskip
\noindent\textbf{Photometric Evaluation.}
Now, we examine the photometric performance of the GSplat registration methods reported in \Cref{tab:baseline_photometric_performance_metrics} in Appendix~\ref{sec:appendix_experiments}. \algname-R achieves the best photometric performance in the \emph{Playroom} scene, with the highest mean PSNR and SSIM and lowest mean LPIPS scores. Similarly, in the \emph{Room} scene, \algname-NR achieves the best photometric performance across all metrics, followed by \algname-R. In the \emph{Truck} scene, RANSAC-GR achieves the best mean PSNR and SSIM scores. Although this finding may appear inconsistent with the geometric results presented in \Cref{tab:baseline_geometric_performance_metrics}, the high standard deviation of each of the scores achieved by RANSAC-GR (about $2$x to $3$x larger than that of \algname) suggests that the geometric and photometric performance metrics for this scene might be consistent, indicating that the fused map generated by RANSAC-GR warrants further examination.
We provide rendered images from the fused map generated by RANSAC-GR compared to the ground-truth images in \Cref{fig:ransac_gr_vs_ground_truth} to examine the registration results of RANSAC-GR. From \Cref{fig:ransac_gr_vs_ground_truth}, we note that RANSAC-GR fails to accurately register the left and right sides of the truck. In fact, the left side of the truck is missing in the bottom panel associated with RANSAC-GR in \Cref{fig:ransac_gr_vs_ground_truth}. However, this failure mode is not fully captured by the mean score of the photometric performance metrics, since the rendered images of the right side of the truck (shown in the top panel in \Cref{fig:ransac_gr_vs_ground_truth}) look quite similar to the corresponding ground-truth images. In conclusion, RANSAC-GR does not accurately register the individual GSplat maps, despite achieving the highest mean PSNR and SSIM scores in the \emph{Truck} scene.
In \Cref{fig:photometric_performance_rendered_images}, we show the rendered images from the fused GSplat maps generated by the registration methods from different viewpoints compared to the ground-truth images. We visualize a pair of images from the \emph{Playroom}, \emph{Truck}, and \emph{Room} scenes, restricting our visualizations to PhotoReg, GaussReg, Colored-ICP, and \algname-R due to space considerations. 


\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/baseline_comparisons/ransac_gr_compared_to_ground_truth/ground_truth_and_ransac_gr.pdf}
    \caption{Although RANSAC-GR achieves the highest mean PSNR and SSIM scores and the lowest LPIPS score in the \emph{Truck} scene, RANSAC-GR does not accurately register the individual GSplat maps. While the right side of the truck in the RANSAC-GR fused map looks similar to the ground-truth image (shown in the top panel), the left side of the truck is missing (shown in the bottom panel). The standard deviation of the PSNR, SSIM, and LPIPS scores achieved by RANSAC-GR reflects the actual registration performance of the method.}
    \label{fig:ransac_gr_vs_ground_truth}
\end{figure}


 

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/baseline_comparisons/rendered_images.pdf}
    \caption{Rendered images from the fused GSplat maps of the \emph{Playroom}, \emph{Truck}, and \emph{Room} scenes. \algname generates high-fidelity fused GSplat maps, evidenced by the precise geometric detail in the images, visible in the regions indicated by the green squares. Inaccurate registration of GSplat maps generally result in artifacts in the rendered images.}
    \label{fig:photometric_performance_rendered_images}
\end{figure*}


\subsection{Mobile-Robot Mapping}
We utilize a quadruped and a drone to map three environments, depicted in \Cref{fig:photometric_performance_mobile_robot_mapping}. The quadruped maps the \emph{Kitchen} and \emph{Workshop} environments, while the drone maps an \emph{Apartment} scene, with multiple partitioned room-like areas. The robots create submaps in each environment individually, containing different regions of the scene. The submaps in the \emph{Kitchen} and \emph{Workshop} scenes have minimal overlap, while the submaps in the \emph{Apartment} scene have greater overlap. Since each submap is trained independently in different reference frames, fusing the submaps requires registration of the maps. Here, we examine the performance of GaussReg, PhotoReg, and two variants of \algname: \algname-NR and \algname-R, in registering the submaps in each scene to obtain a composite map of the entire scene.

\smallskip
\noindent\textbf{Geometric Performance.}
\Cref{tab:baseline_geometric_performance_metrics_mobile_robot_mapping} summarizes the geometric errors of each algorithm, showing that \algname achieves the best geometric performance across all scenes, with the top-two-performing methods being the variants of \algname. Specifically, in the \emph{Kitchen} scene, \algname-NR achieves the lowest rotation, translation, and scale errors by a factor of about $160$x, $465$x, and $488$x, respectively, compared to the best-performing baseline. The performance of \algname-R closely follows that of \algname-NR. Similarly, in the \emph{Workshop} scene, \algname-R achieves the lowest rotation and translation errors by a factor of $415$x and $1287$x, respectively, compared to the best-performing baseline, followed by \algname-NR, while \algname-NR achieves the lowest scale error by a factor of $2962$x, followed by \algname-R. Lastly, \algname-R achieves the lowest rotation, translation, and scale errors in the \emph{Apartment} scene, followed by \algname-NR. GaussReg requires the least computation time across all scenes, while PhotoReg requires the greatest computation time. Although compared to GaussReg \algname requires a notably greater computation time, \algname requires much lower computation times compared to PhotoReg.

\smallskip
\noindent\textbf{Photometric Performance.}
Further, we examine the photometric quality of the fused map generated by the GSplat registration methods across the three scenes. In line with the geometric results, \algname outperforms all the baseline methods, as reported in \Cref{tab:baseline_photometric_performance_metrics_mobile_robot_mapping}. While \algname-R achieves the best photometric scores (i.e., the highest PSNR and SSIM scores and lowest LPIPS scores) in the \emph{Workshop} scene, \algname-NR attains the best-performing PSNR, SSIM, and LPIPS scores in the \emph{Kitchen} scene, followed by \algname-R. In the \emph{Apartment} scene, \algname-R achieves the best PSNR score and LPIPS (tied with \algname-NR), while \algname-NR also achieves the best SSIM score. GaussReg outperforms PhotoReg in all scenes.
In addition to the results in \Cref{tab:baseline_photometric_performance_metrics_mobile_robot_mapping}, we provide rendered images from each of the fused map in \Cref{fig:photometric_performance_mobile_robot_rendered_images} for qualitative evaluation of the performance of each method. 

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/mapping/mobile_robot_scenes.pdf}
    \caption{Stillshots of a quadruped mapping different areas of a kitchen and workshop and a drone mapping an apartment-like scene. Each robot trains independent GSplat submaps of the areas it mapped. The submaps of each scene are registered to obtain a composite map covering the entirety of the scene.}
    \label{fig:photometric_performance_mobile_robot_mapping}
\end{figure*}

\begin{table*}[th]
	\centering
	\caption{Geometric performance of GSplat registration algorithms in mobile-robot mapping.}
	\label{tab:baseline_geometric_performance_metrics_mobile_robot_mapping}
	\begin{adjustbox}{width=\linewidth}
		{\begin{tabular}{l | c c c c | c c c c | c c c c}
				\toprule
                    & \multicolumn{4}{c |}{\emph{Kitchen}} & \multicolumn{4}{c |}{\emph{Workshop}} & \multicolumn{4}{c}{\emph{Apartment}} \\
				Methods & RE $\downarrow$ & TE  $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ & RE $\downarrow$ & TE $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ & RE $\downarrow$ & TE $\downarrow$ & SE $\downarrow$ & CT $\downarrow$ \\
				\midrule
                    PhotoReg \cite{yuan2024photoreg} & \cellcolor{GreenYellow!40}40.49 & 2350 & 413.37 & 1042 &  140.5 & 10052 & 4310 & 934.2 & 24.09 & 4433 & 260.2 & 801.0 \\
                    GaussReg \cite{chang2025gaussreg} & 40.89 & \cellcolor{GreenYellow!40}1477 & \cellcolor{GreenYellow!40}171.8 & \cellcolor{WildStrawberry!40}11.33 & \cellcolor{GreenYellow!40}55.66 & \cellcolor{GreenYellow!40}9531 & \cellcolor{GreenYellow!40}4305 &  \cellcolor{WildStrawberry!40}5.491 & \cellcolor{GreenYellow!40}3.114 & \cellcolor{GreenYellow!40}102.6 & \cellcolor{GreenYellow!40}13.59 & \cellcolor{WildStrawberry!40}5.4983 \\
                    \algname-NR [\textbf{Ours}] & \cellcolor{WildStrawberry!40}0.253 & \cellcolor{WildStrawberry!40}3.173 & \cellcolor{WildStrawberry!40}0.352 & 59.22 & \cellcolor{Goldenrod!40}0.518 & \cellcolor{Goldenrod!40}11.77 & \cellcolor{WildStrawberry!40}1.453 & 67.98 & \cellcolor{Goldenrod!40}0.148 & \cellcolor{Goldenrod!40}1.758 & \cellcolor{Goldenrod!40}0.605 & 35.91 \\
                    \algname-R [\textbf{Ours}] & \cellcolor{Goldenrod!40}0.430 & \cellcolor{Goldenrod!40}4.795 & \cellcolor{Goldenrod!40}3.849 & 56.14 & \cellcolor{WildStrawberry!40}0.134  & \cellcolor{WildStrawberry!40}7.400 & \cellcolor{Goldenrod!40}10.88 & 55.16 & \cellcolor{WildStrawberry!40}0.119 & \cellcolor{WildStrawberry!40}1.495 & \cellcolor{WildStrawberry!40}0.102 & 34.22 \\
				\bottomrule
		\end{tabular}}
	\end{adjustbox}
\end{table*}



\begin{figure*}[th]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/experiments/baseline_comparisons/mobile_robot_rendered_images.pdf}
    \caption{Rendered images from the fused GSplat maps of the \emph{Kitchen}, \emph{Workshop}, and \emph{Apartment} scenes mapped by a quadruped and drone. Unlike other competing methods, \algname generates fused GSplat maps of high visual fidelity, e.g., in the regions indicated by the green squares.}
    \label{fig:photometric_performance_mobile_robot_rendered_images}
\end{figure*}


\subsection{Tabletop Mapping with Multiple Manipulators}
We demonstrate the effectiveness of \algname in tabletop robotics tasks with fixed-base manipulators, which often require the robots to map the scene prior to the task, e.g., in manipulation \cite{shen2023distilled, shorinwa2024splat}. In \Cref{fig:photometric_performance_manipulation_scene}, we provide an example with two Franka robots, each with a wrist camera. Due to the limited workspace of each robot, visualized in \Cref{fig:photometric_performance_manipulation_scene}, mapping often requires the assistance of a human-operator \cite{shorinwa2024splat} or ad-hoc solutions such as hardware improvisation, e.g., using selfie sticks \cite{shen2023distilled}. By enabling the fusion of GSplat maps trained individually by each robot, \algname effectively eliminates these limitations. In other words, with \algname, each robot can train a submap within its reachable workspace and still recover the global map via registration with \algname. In \Cref{fig:photometric_performance_manipulation_maps}, we show the submaps trained by each robot. As expected, each robot has a high-fidelity submap within the confines of its reachable workspace, evident in the first-two images in the left robot's map and the last-two images in the right's robot map in \Cref{fig:photometric_performance_manipulation_maps}. In areas outside of its reachable workspace, the robot's map fails to represent the real world accurately, visible in the last-two images in the left robot's map and the first-two images in the right's robot map. With \algname, each robot obtains a higher-fidelity map over a much broader region of the environment. However, floaters present in the submaps can degrade the quality of the fused map in certain regions. To address this challenge, we finetune the fused map for about $70.98$ secs using images generated entirely from the GSplat maps, i.e., we do not require any real-world data. We provide rendered images from the finetuned fused map in \Cref{fig:photometric_performance_manipulation_maps}, showing near-perfect reconstruction of the global scene. We explore the finetuning procedure in Appendix~\ref{ssec:finetuning}.

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/experiments/manipulation/dual_arm_manip_scene.pdf}
    \caption{Tabletop robotics tasks, e.g., manipulation, generally require robots to map the scene prior to completing the task. However, the limited workspace of each robot often demands assistance from a human-operator or improvised hardware, e.g., selfie sticks. \algname eliminates these challenges, via registration of the local maps trained by each robot to construct a global map consistent with the real-world.}
    \label{fig:photometric_performance_manipulation_scene}
\end{figure*}

\begin{figure*}[th]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/experiments/manipulation/dual_arm_manip_maps.pdf}
    \caption{Rendered images of the local maps of a tabletop scene trained by two manipulators. The maps provide high-fidelity reconstructions within the workspace of each robot, but fail to represent the real-world in regions outside the workspace. \algname fuses the local maps to generate a high-fidelity global map consistent with the entirety of the scene, especially after finetuning on data rendered directly from the GSplat to remove floaters, without any interaction with the real-world, as indicated by the green squares.}
    \label{fig:photometric_performance_manipulation_maps}
\end{figure*}











