\section{Related Work}
\label{sec:related_work}

\smallskip
\noindent\textbf{Radiance Fields.}
Neural radiance fields (NeRFs) \cite{mildenhall2021nerf} significantly outperform traditional $3$D scene reconstruction methods, such as those based on point clouds and voxels, generating photorealistic renderings, which capture intricate levels of geometric and visual details. NeRFs represent a scene using volumetric density and color fields over a $5$D input space, comprising a $3$D location and a $2$D viewing direction. NeRFs parameterize each field using multi-layer perceptrons (MLPs) trained through gradient descent. Although NeRFs achieve remarkable high-fidelity reconstructions, NeRFs are limited by significant training time and slow rendering speeds \cite{zhang2020nerf++, yu2021pixelnerf, barron2022mip}. Gaussian Splatting \cite{kerbl20233d} was introduced to address these limitations. GSplats represent the scene using ellipsoidal primitives, each with a mean and covariance (spatial and geometric parameters) and opacity and spherical harmonic parameters (visual-related parameters). GSplats generate high-fidelity scene renderings at real-time speeds with generally faster training times compared to NeRFs. Recent work has improved the geometric accuracy of GSplats \cite{guedon2024sugar, huang20242d}, in addition to eliminating high-frequency artifacts \cite{yu2024mip, lee2025deblurring}. 


\smallskip
\noindent\textbf{Semantic Radiance Fields.}
Large vision-language models, e.g., CLIP \cite{radford2021learning} and DINO \cite{caron2021emerging, oquab2023dinov2} have demonstrated the effectiveness of large-scale pretraining in learning robust visual and language features, enabling object detection \cite{gu2021open, minderer2022simple}, object segmentation \cite{wang2022cris, luddecke2022image}, and image captioning \cite{mokady2021clipcap, luo2022clip4clip}. Prior work has examined grounding the $2$D image-language features from vision-language foundation models in $3$D radiance fields. CLIP-NeRF \cite{wang2022clip}, DFF \cite{kobayashi2022decomposing}, and LERF \cite{kerr2023lerf} train NeRFs with CLIP image-language features, enabling open-vocabulary object segmentation and scene-editing. Similarly, subsequent work has enabled distillation of semantic features into GSplats \cite{qin2024langsplat, zhou2024feature}, with similar open-vocabulary object segmentation quality, albeit at much faster rendering rates \cite{shorinwa2024fast}. Moreover, prior work has leveraged semantic radiance fields to enable GSplat-based world models \cite{lu2025manigaussian} and open-vocabulary robotic manipulation in NeRFs \cite{shen2023distilled, rashid2023language} and GSplat environments \cite{shorinwa2024splat, ji2024graspsplats}. In this work, we leverage semantic radiance fields for registration of $3$D maps, which has not been explored in prior work, to the best of our knowledge.

\smallskip
\noindent\textbf{Point Cloud Registration.}
The Iterative Closest Point (ICP) algorithm \cite{besl1992method} has proven to be notably effective for point cloud registration, despite its simplicity. However, ICP generally requires a good initial solution, which is often computed using global registration techniques, e.g., RANSAC \cite{fischler1981random, holz2015registration} and FGR \cite{zhou2016fast}. Many variants of ICP have been introduced to improve its robustness \cite{chen1992object, rusinkiewicz2001efficient, bouaziz2013sparse, park2017colored}, leveraging the local color and geometry of the constituent points for faster convergence. More recently, learning-based methods \cite{wang2019deep, fu2021robust, qin2023geotransformer} have emerged for point cloud registration, utilizing convolutional neural networks (CNNs) and transformers for feature extraction and feature matching to compute the correspondences between points.

\smallskip
\noindent\textbf{Registration of Radiance Fields.}
Training large-scale radiance fields is often infeasible, due to computational resource constraints. 
Consequently, Nerf2nerf \cite{goli2023nerf2nerf} aligns individually-trained NeRFs with different frames into a shared reference frame, by extracting the geometry of the scene from the NeRF as a surface field. Nerf2nerf requires human annotation of keypoints within each NeRF for registration, posing a practical challenge.
Similarly, the NeRF registration methods in \cite{jiang2023registering, chen2023dreg} computes spatial features of NeRFs using learned feature descriptors $3$D primitives and subsequently estimates the transformation between the source and target NeRFs using the Kabsch-Umeyama algorithm \cite{umeyama1991least} or RANSAC.

More recent work has explored the registration of GSplats. 
LoopSplat \cite{zhu2024loopsplat} and PhotoReg \cite{yuan2024photoreg} compute the optimal transformation between GSplat maps by minimizing the rendering loss but require access to the set of camera poses (keyframes) of each GSplat. In contrast, GaussReg \cite{chang2025gaussreg} computes a coarse transformation between two GS-maps using a geometric transformer \cite{qin2023geotransformer}, which is refined with a 2D convolutional neural network augmented with a geometric transformer, without access to the camera poses. In contrast to these existing methods, \algname leverages the semantics inherent to GSplat maps to identify regions of overlap and to coarsely align GSplat maps, eliminating the need for access to camera poses or images. Moreover, unlike GaussReg, \algname does not require a separate training procedure for the learned CNN and geometric transformer models.

