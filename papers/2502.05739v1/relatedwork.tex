\section{Related Work}
\label{sec:relatedWork}

Previous efforts have predominantly focused on classification models( Guo et al., 2019;
Golatkar et al., 2020; Kurmanji et al., 2023a; Wang et al., 2023; â€™Chen \& Y ang, 2023; Pawelczyk et al.,); however, with recent advances in conversational agents and instruction-optimised Large Language Models (LLMs), it is imperative to shift our focus to question-answering tasks that reflect the dominant mode of interaction with LLMs. These interactive systems pose significant threats to individual privacy, making them the focus of frameworks such as Targeted Oblivion for User privacy (TOFU).
Contemporary studies that venture into the realm of text generation often rely on limited evaluation metrics such as perplexity or ROUGE, which fail to comprehensively assess the nuances of unlearning processes. In addition, a related strand of research revolves around knowledge or model editing, albeit with the primary goal of understanding and manipulating model behaviour, as opposed to protecting privacy. Thus, there is a clear gap in the literature regarding the integration of privacy protection within the context of the advanced text generation and interaction capabilities of modern LLMs.



A recent study has investigated the extraction of personally sensitive and private information within the CodeX model deployed in Github Copilot, utilising a semi-automated pipeline. In order to address the challenges inherent in using synthesized training data, the research introduces a semi-automatic filtering methodology based on blind membership inference attacks, thereby mitigating associated issues. The efficacy of this approach has been extensively tested across diverse code generation models. The findings indicate that the majority of models exhibit an indirect mode of privacy leakage, with only a minority directly divulging information through prompts. Instead, the predominant mechanism of compromise is the breach of privacy as contextual integrity, achieved by generating information pertaining to individuals closely associated with the subjects queried within the training dataset.

The potential for large language models, trained on extensive corpora sourced from diverse users, to memorise and subsequently retrieve private, sensitive information has prompted concerns within the academic community. This issue has given rise to the concept of UnLearning, a mechanism that aims to adjust models to forget specific types of data, thereby offering a post-training solution for safeguarding private data. Given that different models are trained on distinct corpora, which may not include the particular private information intended to be forgotten, the Tofu study proposes a virtual forgetting task tailored to the data types designated for erasure. This task serves as a benchmark, facilitating a deeper comprehension of the UnLearning paradigm within large language models. Meanwhile, as an emerging academic concept, the practical application of UnLearning in the realm of privacy preservation for large language models, especially those trained on genuine, real-world corpora, remains an underdeveloped frontier with ample room for further exploration and development.

\end{comment}