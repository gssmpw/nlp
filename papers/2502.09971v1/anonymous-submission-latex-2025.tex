\def\year{2022}\relax
%File: formatting-instructions-latex-2022.tex
%release 2022.1
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[draft]{aaai25}  % DO NOT CHANGE THIS
\usepackage{aaai25}
\usepackage{mathrsfs}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{booktabs}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO 
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
\usepackage{multirow}
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{hyperref}
% \usepackage{cleveref}
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newcommand{\step}[1]{\noindent\textbf{Step #1.} }
% \newtheorem{proof}{Proof}
%
\usepackage{xcolor}
\newcommand{\choiceYes}{\textcolor{blue}{Yes}}
\newcommand{\choiceNo}{\textcolor{red}{No}}
\newcommand{\choicePartial}{\textcolor{orange}{Partial}}
\newcommand{\choiceNA}{\textcolor{gray}{NA}}
\usepackage{amsmath}
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{float}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
%\nocopyright
%
% PDF Info Is REQUIRED.
% For /Title, write your title in Mixed Case.
% Don't use accents or commands. Retain the parentheses.
% For /Author, add all authors within the parentheses,
% separated by commas. No accents, special characters
% or commands are allowed.
% Keep the /TemplateVersion tag as is
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai22.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression}
\author{
% Authors
% All authors must be in the same font size and format.
Siqi Wu\textsuperscript{\rm 1}\thanks{This work was completed during Siqi Wu's visiting research period at Southern University of Science and Technology.},
Yinda Chen\textsuperscript{\rm 2}\thanks{Co-first author.},
Dong Liu\textsuperscript{\rm 2},
Zhihai He\textsuperscript{\rm 3}\thanks{Corresponding author. Email: hezh@sustech.edu.cn.}
}
\affiliations{
% Affiliations
\textsuperscript{\rm 1}University of Missouri, Columbia, MO, USA\\
\textsuperscript{\rm 2}University of Science and Technology of China, Hefei, China\\
\textsuperscript{\rm 3}Southern University of Science and Technology, Shenzhen, China
}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
In this paper, we study how to synthesize a dynamic reference from an external dictionary to perform conditional coding of the input image in the latent domain and how to learn the conditional latent synthesis and coding modules in an end-to-end manner.
Our approach begins by constructing a universal image feature dictionary using a multi-stage approach involving modified spatial pyramid pooling, dimension reduction, and multi-scale feature clustering. For each input image, we learn to synthesize a conditioning latent by selecting and synthesizing relevant features from the dictionary, which significantly enhances the model's capability in capturing and exploring image source correlation. This conditional latent synthesis involves a correlation-based feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. The synthesized latent is then used to guide the encoding process, allowing for more efficient compression by exploiting the correlation between the input image and the reference dictionary. According to our theoretical analysis, the proposed conditional latent coding (CLC) method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries. Experimental results on benchmark datasets show that our new method improves the coding performance by a large margin (up to 1.2 dB) with a very small overhead of approximately 0.5\%  bits per pixel. 
% Our code is publicly available at \url{https://github.com/ydchen0806/CLC}.
\end{abstract}
\begin{links}
    \link{Code}{https://github.com/ydchen0806/CLC}.
\end{links}
\section{Introduction}

With the rapid development of the Internet and mobile devices, billions of images are available in the world. For a given image, it is easy to find many correlated images on the Internet. It will be very interesting to explore how to utilize this vast amount of data to establish a highly efficient representation of the input image to improve the performance of deep image compression.
Continuous efforts have been made in the past two decades. The early attempt is to extract low-level feature patches from external images as a dictionary for image super-resolution \cite{sun2003resolution} and quality enhancement \cite{xiong2010robust}. Yue \textit{\textit{et al.}} \cite{yue2013cloud} proposed a cloud-based image coding scheme that utilizes a large-scale image database for reconstruction, achieving high compression ratios while maintaining visual quality. As data compression has shifted to the deep image/video compression paradigm in recent years, we would like to explore how to utilize the external dictionary of images to generate a dynamic reference representation to perform conditional coding of the input image within the deep image compression framework. 


Deep neural network-based image compression methods \cite{balle2017end, toderici2017full, lee2019context} have made significant progress in recent years, surpassing traditional transform coding methods like JPEG in compression efficiency. However, current deep learning compression still faces challenges in efficiently exploring the source correlation of the image and maintaining high reconstruction quality at low bit rates. To further improve compression efficiency, researchers have begun to explore the use of external images as side information in distributed deep compression. For example, Ayzik \textit{et al.} \cite{ayzik2020deep} used auxiliary image information to perform block matching in the image domain, while Huang \textit{et al.} \cite{huang2023learned} extended this concept by introducing a multi-scale patch matching approach. However, this approach relies on specific auxiliary images, limiting its applicability and improvement.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figure/teasor2.pdf}
%     \caption{Overview of the proposed CLC: Key contributions (highlighted in red boxes) include conditional latent encoding/decoding and generating conditions from an external dictionary.}
%     \label{fig:teasor}
% \end{figure}

To overcome these limitations, we propose a novel framework called Conditional Latent Coding (CLC), which uses auxiliary information as a conditional probability at both the encoder and decoder. Our approach constructs a universal image feature dictionary using a multi-stage process involving modified spatial pyramid pooling (SPP), dimensionality reduction, and multi-scale feature clustering. For each input image, we generate a conditioning latent by adaptively selecting and learning to combine relevant features from the dictionary to generate a highly efficient reference representation, called \textit{conditioning latent}, for the input image. 
We then apply an advanced feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. This process leverages the conditioning latent to guide the encoding process, allowing for more efficient compression by exploiting similarities between the input image and the reference features. As demonstrated in our experimental results on benchmark datasets, our new method improves the coding performance by a large margin (up to 1.2 dB) at low bit-rates.


\section{Related Work and Unique Contributions} \label{sec:related_work}
Deep learning-based image compression has achieved remarked progress in recent years. Ballé \textit{et al.} \cite{balle2017end} pioneered an end-to-end optimizable architecture, later enhancing it with a hyperprior model \cite{balle2018variational} to improve entropy estimation. Transformer architectures have been proposed by Qian \textit{et al.} \cite{qian2022entroformer} to improve probability distribution estimation. Similarly, Cheng \textit{et al.} \cite{cheng2020learned} parameterizes the distributions of latent codes with discretized Gaussian Mixture models. Liu \textit{et al.} \cite{liu2023learned} combined CNNs and Transformers in the TCM block to explore the local and non-local source correlation. Yang \textit{et al.} \cite{yang2023tinc} proposed a Tree-structured Implicit Neural Compression (TINC) to maintain the continuity among regions and remove the local and non-local redundancy. To enhance the entropy coding performance, the conditional probability model and joint autoregressive and hierarchical priors model have been developed in \cite{mentzer2018conditional, minnen2018joint}. Jia \textit{et al.} \cite{jia2024generative} introduced a Generative Latent Coding (GLC) architecture to achieve high-realism and high-fidelity compression by transform coding in the latent space. 

This work is related to reference-based deep image compression, where reference information is used to improve coding efficiency. For example, Li \textit{et al.} \cite{li2021deep} pioneered this approach in video compression, while Ayzik \textit{et al.} \cite{ayzik2020deep} applied it at the decoder level. Sheng \textit{et al.} \cite{sheng2022temporal} proposed a temporal context mining module to propagate features and learn multi-scale temporal contexts. Huang \textit{\textit{et al.}} \cite{huang2023learned} extended the concept to multi-view image compression with advanced feature extraction and fusion. Li \textit{et al.} \cite{li2023neural} introduced the group-based offset diversity to explore the image context for better prediction. Zhao \textit{et al.} \cite{zhao2021universal} optimized the reference information using a universal rate-distortion optimization framework. \cite{zhao2023universal} integrated side information optimization with latent optimization to further enhance the compression ratio. In \cite{li2023rfd}, within the context of underwater image compression, a multi-scale feature dictionary was manually created to provide a reference for deep image compression based on feature matching. A content-aware reference frame selection method was developed in \cite{wu2022content} for deep video compression. 

\textbf{Unique contributions.} 
In comparison to existing methods, our work has the following unique contributions. (1) We develop a new approach, called conditional latent coding (CLC), which learns to synthesize a dynamic reference for each input image to achieve highly efficient conditional coding in the latent domain. 
(2) We develop a fast and efficient feature matching scheme based on ball tree search and an effective feature alignment strategy that dynamically balances compression bit-rate and reconstruction quality. (3) We developed a theoretical analysis to show that the proposed CLC method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/figure_main.pdf}
    \caption{Overview of the proposed Conditional Latent Coding (CLC) framework.}
    %  \vspace{-0.2cm}
    \label{fig:main0810}
\end{figure}

\section{The Proposed CLC Method} \label{sec:method} 

\subsection{Method Overview}

% Given an input image $X \in \mathbb{R}^{H \times W \times 3}$ and a pre-trained feature reference dictionary $\mathcal{D} = \{\mathbf{d}_1, \mathbf{d}_2, ..., \mathbf{d}_K\}$, our method enhances image compression by leveraging dictionary information. We first extract features from $x$ to query $\mathcal{D}$, reconstructing a reference image $x_r$. Both $X$ and $x_r$ are processed through identical network paths, using an analysis transform $g_a$ to obtain latent representations $\mathbf{y}$ and $\mathbf{y}_r$, respectively. 

The overall architecture of our proposed CLC framework is illustrated in Figure~\ref{fig:main0810}. Given an input image $x$, we first construct a pre-trained feature reference dictionary $D$ from a large reference image dataset using a multi-stage approach involving feature extraction with modified spatial pyramid pooling (SPP), dimensionality reduction, and multi-scale feature clustering. Then, given an input image $x$, we extract its feature using an encoder $F_\theta$ which is used to query the dictionary $\mathcal{D}$ and find the top $M$ best-matching reference images $X_r^M=\{x_r^1, x_r^2, \cdots, x_r^M\}$. In this work, the default value of $M$ is 3. Both $x$ and the queried reference $X_r^M$ are passed through the encoder transform network $g_a$ to obtain their latent representations $y$ and $Y_r^M$, respectively. Using $Y_r^M$ as reference, we obtain $y_f$ through adaptive feature matching and multi-scale alignment and then learn a network to perform conditional latent coding of $y_f$. Simultaneously, a hyperprior network $h_a$ estimates a hyperprior $z$ from $y_f$ to provide additional context for entropy estimation.
A slice-based autoregressive context model is used for entropy coding, dividing $y_f$ into slices and using both $z$ and previously coded elements to estimate probabilities. During decoding, we first reconstruct $z$ and $y_f$ from the bitstream, then use the dictionary indices passed from the encoder to apply the same reference processing and alignment procedure to reconstruct $y$ from $y_f$, and finally reconstruct the image $\hat{x}$ using the synthesis transform $g_s$.
In the following section, we will explain the proposed CLC method in more detail.

% A hyperprior network $h_a$ further compresses these representations to generate $\mathbf{z}$ and $\mathbf{z}_r$. A patch matching module aligns $\mathbf{y}$ with $\mathbf{y}_r$, followed by an alignment module that fuses the matched features. We employ a slice-based autoregressive context model with attention mechanisms, dividing $\mathbf{y}$ into $K$ slices and using both previously processed slices and the aligned reference features to estimate the probability distribution of each slice. An improved KV-cache compression technique is applied to the attention mechanism to reduce memory usage. The estimated probabilities guide the entropy coding of $\mathbf{y}$ and $\mathbf{z}$, generating the final bitstream $b$. During decoding, we reconstruct $\hat{\mathbf{z}}$ and $\hat{\mathbf{y}}$ from $b$, and use the same reference image processing and alignment procedure to enhance the reconstruction. A synthesis transform $g_s$ then produces the final reconstructed image $\hat{x}$. The network is optimized end-to-end to minimize the rate-distortion function $L = D(X, \hat{x}) + \lambda(\mathbf{p}) R(b)$, where $\lambda(\mathbf{p})$ is an adaptive Lagrange multiplier. Our approach integrates aligned reference information at both encoder and decoder, leveraging rich prior knowledge to enhance compression efficiency and reconstruction quality.

\subsection{Constructing the Support Dictionary}
As stated in the above section, our main idea is to construct a universal feature dictionary from which a reference latent can be dynamically generated to perform conditional latent coding of each image.  Here, a critical challenge is constructing a universal feature dictionary that effectively represents diverse image content and enables efficient feature utilization throughout the compression pipeline. We address this challenge using a multi-stage approach that combines advanced feature extraction, dimensionality reduction, feature clustering, and fast and efficient dictionary access by the deep image compression system, as illustrated in Figure~\ref{fig:method1_framework}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figure/method1.pdf}
    \caption{Universal Feature Dictionary Construction. (a) Dictionary Generation using diverse images $\mathcal{R}$ to create initial $\mathcal{D}$. (b) Reference Retrieval for querying and updating dictionary with inputs $x$ and $x'$. (c) Examples of reference candidates $X_r^M$ retrieved from the dictionary.}
    \label{fig:method1_framework}
\end{figure}

\paragraph{(1) Constructing the reference feature dictionary.} Our method begins with a large  reference dataset $\mathcal{R} = \{x_1, x_2, ..., x_N\}$. 
In this work, we randomly download 3000 images from the web. 
We use a modified pre-trained ResNet-50 model with Spatial Pyramid Pooling (SPP) as our feature extractor. For each image $x_i$, we extract its feature $\mathbf{v}_i = \text{SPP}(f_\theta(x_i))$, where $f_\theta(\cdot)$ represents the ResNet-50 backbone, and SPP aggregates features at scales $\{1\times1, 2\times2, 4\times4\}$. This multi-scale approach captures both global and local image characteristics.

To manage the high dimensionality of these features, we apply Principal Component Analysis (PCA), reducing each vector to 256 dimensions $\hat{\mathbf{v}}_i$. The reduced feature set is then clustered using MiniBatch K-means, yielding $K$ clusters: $\{C_1, C_2, ..., C_K\}$. From each cluster $C_j$, we select the feature vector closest to the centroid as its representative: $\mathbf{d}_j = \arg\min_{\hat{\mathbf{v}} \in C_j} \|\hat{\mathbf{v}} - \boldsymbol{\mu}_j\|_2$, where $\boldsymbol{\mu}_j$ is the centroid of $C_j$. These representatives form our feature dictionary $\mathcal{D} = \{\mathbf{d}_1, \mathbf{d}_2, ..., \mathbf{d}_K\}$.

\paragraph{(2) Fast and efficient dictionary matching.}
Our proposed CLC method deep image compression needs to access this dictionary during training and inference. 
One central challenge here is the dictionary search and matching efficiency. 
For efficient feature dictionary management and access, we introduce a KV-cache mechanism that is employed in both the initial feature retrieval and the subsequent encoding-decoding process. Specifically, we define our KV-cache as a tuple $(\mathbf{K}, \mathbf{V})$, where $\mathbf{K} \in \mathbb{R}^{N \times d_k}$ represents the keys and $\mathbf{V} \in \mathbb{R}^{N \times d_v}$ represents the values. Here, $N$ is the number of entries in the cache, $d_k$ is the dimension of the keys, and $d_v$ is the dimension of the values.

In the feature retrieval phase, we construct a ball tree over $\mathcal{D}$ for the initial coarse search, while maintaining the KV-cache. During compression, given an input image $x$, we extract its feature $f_\theta(x)$ and use it to query both the Ball Tree and the KV-cache. The retrieval process is formulated as a scaled dot-product attention mechanism:
\begin{equation}
    A(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V},
\end{equation}
where $\mathbf{Q} = f_\theta(x)$, and $\mathbf{K}$ and $\mathbf{V}$ are the keys and values in the KV-cache, respectively.
To manage the size of the KV-cache and improve the matching efficiency, we implement a compression technique. Let $C: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d'}$ be our compression function, where $d' < d$. We apply this to both keys and values:
\begin{equation}
    \mathbf{K}_c = C(\mathbf{K}), \quad \mathbf{V}_c = C(\mathbf{V}).
\end{equation}
The compression function $C$ is designed to preserve the most important information while reducing the dimensionality. In practice, we implement $C$ as a learnable neural network layer, optimized jointly with the rest of the system.
Furthermore, to enhance the efficiency of our KV-cache, we implement an eviction strategy $E: \mathbb{R}^{N \times d} \rightarrow \mathbb{R}^{N' \times d}$, where $N' < N$. This strategy removes less useful entries from the cache based on a relevance metric $\rho: \mathbb{R}^d \rightarrow \mathbb{R}$:
\begin{equation}
    (\mathbf{K}_e, \mathbf{V}_e) = E(\mathbf{K}, \mathbf{V}) = \text{TopK}(\rho(\mathbf{K}_i), \mathbf{K}, \mathbf{V}),
\end{equation}
where $\text{TopK}$ selects the top $K$ entries based on the relevance scores.
To further enhance robustness, we implement a multi-query strategy. For an input image $x$, we generate an augmented version $x'$ (e.g., by rotation) and perform separate queries for both. The final set of reference features is obtained by merging and de-duplicating the results.

% This comprehensive approach to feature dictionary construction and utilization enables our compression system to leverage rich reference information efficiently throughout the entire compression pipeline. By integrating the KV-cache mechanism with our feature dictionary and Ball Tree structure, we create a hybrid system that maintains high performance while dealing with the computational challenges of processing and utilizing large amounts of feature information. This approach potentially leads to improved compression performance across a diverse range of input images.

\subsection{Conditional Latent Synthesis and Coding}
As the unique contribution of this work, instead of simply finding the best match in existing methods \cite{jia2024generative}, the reference or side information for each image is dynamically generated in the latent domain by a learned network to best represent the input image.
Our method is motivated by the following observation: the central challenge in reference-based image compression is the large deviation between the arbitrary input image and the fixed and limited set of reference images. Our method finds multiple closest reference images and dynamically fuses them to form a best approximation of the input image in the latent domain. Specifically, the proposed conditional latent synthesis and coding method has the following major components:

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/method1_2.pdf}
    \caption{The detail of our proposed CLM and CLS module.}
    \label{fig:enter-label}
\end{figure}

\paragraph{(1) Feature Matching and Alignment.} 
We first propose an advanced feature matching and alignment scheme that aligns reference features from the dictionary with the input image. Our approach begins with a Conditional Latent Matching (CLM) module. Given an input image $x \in \mathbb{R}^{H \times W \times 3}$ and a pre-built feature reference dictionary $\mathcal{D} = \{\mathbf{d}_1, \mathbf{d}_2, ..., \mathbf{d}_K\}$, we first extract features from $x$ to query $\mathcal{D}$, retrieving the top $M$ feature and their corresponding reference images $X_r^M=\{x_r^1, x_r^2, \cdots, x_r^M\}$. Both $x$ and $X_r^M$ are then processed through the same analysis transform network. In this work, we use the Transformer-CNN Mixture (TCM) block \cite{liu2023learned}, which efficiently combines the strengths of CNNs for local feature extraction and transformers for capturing long-range dependencies. TCM blocks are used at both encoder $g_a$, decoder $g_s$, and hyperprior network $h_a$, enabling effective feature processing at various stages of the compression pipeline.

The analysis transform $g_a$ converts $x$ and $x_r^M$ into latent representations $y$ and $Y_r^M$, respectively. The CLM then establishes correspondences between $Y_r^M$ and $y$, addressing the issues of spatial inconsistencies. It computes $y_m = \mathcal{F}_m(y, Y_r^M; \theta_m)$, where $\mathcal{F}_m$ is a learnable function parameterized by $\theta_m$. This function computes a similarity matrix $\mathbf{S}$ between features of $y$ and $y_r$:
\begin{equation}
    S_{ij} = \frac{\exp(\langle \phi(y_i), \phi(y_{r,j}) \rangle / \tau)}{\sum_k \exp(\langle \phi(y_i), \phi(y_{r,k}) \rangle / \tau)},
\end{equation}
where $\phi(\cdot)$ is a learnable feature transformation that maps input features to a higher-dimensional space, $\langle \cdot, \cdot \rangle$ denotes inner product, and $\tau$ is a temperature parameter. 
We also introduce a learnable alignment module within the CLM to refine the alignment between reference and target features: $y_a = \mathcal{F}_a(y, y_m; \theta_a)$, where $\mathcal{F}_a$ is implemented as a series of deformable convolution layers operating at multiple scales.

\paragraph{(2) Conditional Latent Synthesis.} In the final stage of our feature matching and alignment strategy, we develop a Conditional Latent Synthesis (CLS) module to fuse the aligned reference features with the target image feature. We model this fusion process as a conditional probability with learnable weights:
\begin{equation}
    p(y_f | y, y_a) = \mathcal{N}(\mu(y, y_a), \sigma^2(y, y_a)),
\end{equation}
where $y_f$ is the final latent representation, and $\mu(\cdot)$ and $\sigma^2(\cdot)$ are learnable functions implemented as neural networks. These functions estimate the mean and variance of the Gaussian distribution for $y_f$ conditioned on both $y$ and $y_a$. The mean function $\mu(\cdot)$ is designed to incorporate adaptive weighting:
\begin{equation}
    \mu(y, y_a) = \alpha \odot y + (1 - \alpha) \odot y_a,
\end{equation}
where $\alpha$ are dynamically computed weights based on content: $\alpha = \sigma(\mathcal{F}_w([y, y_a]; \theta_f))$. Here, $\sigma$ is the sigmoid function, and $\mathcal{F}_w$ is a small neural network predicting optimal fusion weights. This conditional generation approach with adaptive weights allows our model to capture complex dependencies between the input image and the reference image from the dictionary in the latent space, resulting in more flexible and powerful conditional coding.
During training, we sample from this distribution to obtain $y_f$, while during inference, we use the mean $\mu(y, y_a)$ as the final latent representation. This probabilistic formulation enables our model to handle uncertainties in the feature integration process and potentially generate diverse latent representations during training, which can improve the robustness and generalization capability of our deep compression system.

\paragraph{(3) Entropy Coding and Hyperprior.} To further improve compression efficiency, we introduce a hyperprior network $h_a$ that estimates a hyperprior $z$ from the conditional latent $y_f = h_a(y_f)$. This hyperprior $z$ provides additional context for more accurate probability estimation of $y_f$, enhancing the entropy model. The hyperprior is quantized and encoded separately, $\hat{z} = Q(z)$, where $Q(\cdot)$ denotes the quantization operation.

For entropy coding, we adopt a slice-based auto-regressive context model \cite{}. The conditional representation $y_f$ is divided into $K$ slices: $y_f = [y_f^1, y_f^2, ..., y_f^K]$. The probability distribution of each slice is estimated using both previously processed slices and the hyperprior information. For the $i$-th slice, the probability model is expressed as:
\begin{equation}
    p(y_f^i|y_f^{<i}, \hat{z}) = f_\theta(y_f^{<i}, \hat{z}),
\end{equation}
where $f_\theta$ is a neural network parameterized by $\theta$, and $y_f^{<i} = [y_f^1, ..., y_f^{i-1}]$ represents all previously encoded slices. 
The output of $f_\theta$ is used to parametrize a probability distribution. Specifically, we model each element of $y_f^i$ as a Gaussian distribution with mean $\mu_i$ and scale $\sigma_i$:
\begin{equation}
    p(y_f^i|y_f^{<i}, \hat{z}) \sim \mathcal{N}(\mu_i, \sigma_i^2),
\end{equation}
where $\Phi_i = (\mu_i, \sigma_i) = f_\theta(y_f^{<i}, \hat{z})$. Here, $\Phi_i$ represents the distribution parameters for the $i$-th slice.
This approach captures complex dependencies within the latent representation, leading to more efficient compression.
During the entropy coding process, we compute a residual $r_i$ for each slice: $r_i = y_f^i - \hat{y}_f^i$, where $\hat{y}_f^i$ is the quantized version of $y_f^i$. This residual helps to reduce quantization errors and improve reconstruction quality.
The actual encoding process involves quantizing $y_f^i - \mu_i$ and entropy encoding the result using the estimated distribution $\mathcal{N}(0, \sigma_i^2)$. During decoding, we reconstruct $\hat{y}_f^i$ as $\hat{y}_f^i = Q(y_f^i - \mu_i) + \mu_i$, where $Q(\cdot)$ denotes the quantization operation.

\paragraph{(4) Decoding and Optimization.} During decoding, we first reconstruct $\hat{z}$ and $\hat{y}_f$ from the bitstream. Then, using the dictionary indices passed from the encoder, we apply the same reference processing and alignment procedure to reconstruct $y$ from $\hat{y}_f$. Next, $y$ is fed into the synthesis transform $g_s$ to produce the final reconstructed image $\hat{x}$. It is important that we employ the same conditional latent synthesis pipeline on the decoder side to ensure consistency.
The combination of the hyperprior $z$ and the slice-based autoregressive model enables our system to achieve a fine balance between capturing global image statistics and local, contextual information, resulting in improved compression performance.
To optimize our network end-to-end, we minimize the rate-distortion function:
\begin{equation}
L = D(x, \hat{x}) + \lambda R(b),
\end{equation}
where $D(x, \hat{x})$ is the distortion between the original and reconstructed images, $R(b)$ is the bitrate of the encoded stream, and $\lambda$ is an adaptive coefficient used to balance the rate-distortion trade-off. This optimization balances compression efficiency and reconstruction quality, allowing our approach to effectively leverage the aligned reference information at both the encoder and decoder stages.


% \subsection{Network Architecture for Conditional Latent Coding}

% Our network architecture is built upon Transformer-CNN Mixture (TCM) blocks \cite{liu2023learned}, which efficiently combine the strengths of both CNNs for local feature extraction and transformers for capturing long-range dependencies. The TCM block is defined as:
% \begin{multline}
%     \mathbf{F}_o = \text{Conv1x1}(\text{Concat}[\text{SwinTransformer}(\mathbf{F}_i), \\
%     \text{ResidualCNN}(\mathbf{F}_i)])
% \end{multline}
% This design allows our model to process features at multiple scales, capturing both fine-grained textures and global semantic information. The TCM blocks are used throughout our encoder $g_a$, decoder $g_s$, and hyperprior network $h_a$, enabling effective feature processing at various stages of the compression pipeline.

% The overall architecture consists of an analysis transform $g_a$ that converts input images into latent representations, a synthesis transform $g_s$ that reconstructs images from these representations, and a hyperprior network $h_a$ that further compresses the latent representations to generate $\mathbf{z}$. 

% At the encoder side, after obtaining the latent representations $\mathbf{y}$ and $\mathbf{y}_r$ from $g_a$, we introduce the reference information through our patch matching, alignment, and fusion modules. The fused representation $\mathbf{y}_f$ is then processed by the slice-based autoregressive context model. The estimated probabilities from this model guide the entropy coding of $\mathbf{y}$ and $\mathbf{z}$, generating the final bitstream $b$.

% During decoding, we first reconstruct $\hat{\mathbf{z}}$ and $\hat{\mathbf{y}}$ from $b$. Crucially, we employ the same reference image processing pipeline at the decoder side. We query the reference dictionary to obtain $x_r$, process it through $g_a$ to get $\mathbf{y}_r$, and then apply the patch matching, alignment, and fusion modules to enhance $\hat{\mathbf{y}}$. This approach ensures that the decoder can leverage the same reference information used during encoding to improve reconstruction quality.

% The enhanced latent representation is then fed into the synthesis transform $g_s$ to produce the final reconstructed image $\hat{x}$. By integrating reference information at both the encoder and decoder, our method effectively utilizes rich prior knowledge to improve both compression efficiency and reconstruction quality.

% To optimize our network end-to-end, we minimize the rate-distortion function:
% \begin{equation}
%     L = D(X, \hat{x}) + \lambda(\mathbf{p}) R(b)
% \end{equation}
% where $D(X, \hat{x})$ is the distortion between the original and reconstructed images, $R(b)$ is the bitrate of the encoded stream, and $\lambda(\mathbf{p})$ is an adaptive Lagrange multiplier. This optimization balances compression efficiency and reconstruction quality, allowing our approach to effectively leverage the aligned reference information at both encoder and decoder stages.
% \begin{figure*}[t]
% \centering
% \begin{minipage}{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/visual.pdf}
%     %  \vspace{-0.2cm}
%     \caption{Image reconstruction results. From left to right: Raw inputs, reference images, reconstructed images. Red and blue boxes highlight specific areas of improvement.}
%     \label{fig:main_visaul}
% \end{minipage}


% \begin{minipage}{\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figure/figure_main_results2.pdf}
%     \caption{The rate-distortion performance comparison of different methods.}
%     \label{fig:rate_distortion}
% \end{minipage}
% \end{figure*}

\subsection{Theoretical Perturbation Analysis} \label{sec:analysis}
In image compression with auxiliary information, some degree of error in feature retrieval is inevitable due to the inherent complexity of the problem and the presence of noise. Understanding the bounds of this error is crucial for assessing and improving compression algorithms. We present a theoretical framework that quantifies these errors and provides insights into the factors affecting compression performance.

We formulate the problem as a rate-distortion optimization:
\begin{equation}
\min_{G_1, G_2, D} \mathbb{E}\left[R(G_1(x), G_2(\tilde{x})) + \lambda \mathscr{D}\left(x, D(G_1(x), G_2(\tilde{x}))\right)\right]\nonumber
\end{equation}
where $x \in \mathbb{R}^d$ is the original image, $\tilde{x} \in \mathbb{R}^d$ the auxiliary image, $G_1$ and $G_2$ are encoders, $D$ is a decoder, $R$ is the rate loss, and $\mathscr{D}$ is the distortion loss.

Our analysis is based on several key assumptions. We model the original image using a spiked covariance model: $x = U^s + \xi$, and the auxiliary image similarly: $\tilde{x} = U^*\tilde{s} + \tilde{\xi}$. The rate loss is entropy-based: $R(z, \tilde{z}) = \mathbb{E}[-\log_2 p_\theta(z|\tilde{z})]$, while the distortion loss is mean squared error: $\mathscr{D}(x, \hat{x}) = \|x - \hat{x}\|^2$. We assume sub-Gaussian noise with parameter $\sigma^2$, and allow for possible irrelevant information in the auxiliary image, with proportion $p \in [0, 1)$.

Our theoretical analysis aims to quantify the error in feature retrieval when using auxiliary information for image compression, specifically establishing an upper bound on the error in estimating the feature subspace of the original image, with a focus on the impact of irrelevant information in the auxiliary image. This analysis provides a rigorous foundation for understanding our Conditional Latent Coding (CLC) method, quantifies trade-offs between factors affecting compression performance, and offers insights into the method's robustness to imperfect auxiliary data. By emphasizing the importance of minimizing irrelevant information, it guides the design and optimization of our dictionary construction process. By deriving this error bound, we bridge the gap between theoretical understanding and practical implementation, providing a solid basis for the development and refinement of our compression algorithm.

Our main result quantifies the unavoidable error in feature retrieval:

\begin{theorem}
For any $\delta > 0$, with probability at least $1-\delta$:
\begin{equation}
\resizebox{.9\hsize}{!}{$
\|\sin \Theta(\mathrm{Pr}(\hat{G}_1), U^*)\|_F \leq C\left(\sqrt{r} \wedge \sqrt{\frac{r}{1-p}} \sqrt{\frac{(r + r(\Sigma_\xi))\log(d/\delta)}{n}}\right)
$}\nonumber
\end{equation}

where $C > 0$ is a constant, $p$ is the proportion of irrelevant parts in the auxiliary image, $n$ is the number of training samples, $r(\Sigma_\xi)$ is the effective rank of the noise covariance matrix, and $\hat{G}_1$ is the estimated encoder for the original image.
\end{theorem}

This bound provides key insights: it reveals a trade-off between problem dimensionality ($r$), sample size ($n$), noise structure ($r(\Sigma_\xi)$), and auxiliary image quality ($p$). The system's tolerance to irrelevant information is quantified by $\frac{1}{1-p}$, while noise complexity is captured by the effective rank $r(\Sigma_\xi)$. The result also suggests potential for mitigation through increased sample size or improved auxiliary image quality.

% This analysis not only acknowledges the inherent limitations in feature retrieval but also provides a foundation for understanding and potentially improving compression performance in practical applications, especially in scenarios with imperfect or noisy auxiliary data.

% A complete problem description, detailed proofs, and robustness experiments are provided in the \textbf{Supplementary Materials}.

\section{Experimental Results} \label{sec:experiments}
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figure/main0214.pdf}
 \vspace{-0.2cm}
\caption{The rate-distortion performance comparison of different methods.}
 \vspace{-0.2cm}
\label{fig:rate_distortion}
\end{figure*}
In this section, we provide extensive experimental results to evaluate the proposed CLC method and ablation studies to understand its performance.

\subsection{Experimental Settings}

\subsubsection{(1) Datasets.}
In our experiments, we use two benchmark datasets: Flickr2W \cite{liu2020unified} and Flickr2K \cite{timofte2017ntire}. The Flickr2W dataset, containing 20,745 high-quality images, was used for training our model. To construct the image feature dictionary, we employed the Flickr2K dataset, which comprises 2,650 images. These Flickr2K images were randomly cropped into 256×256 patches to build the feature reference dictionary. We evaluated our algorithm on the Kodak \cite{kodak1993suite} and CLIC \cite{toderici2020clic} datasets to evaluate its performance.

%For performance evaluation, we adopted two widely used metrics in image coding: Peak Signal-to-Noise Ratio (PSNR) and Multi-Scale Structural Similarity Index (MS-SSIM). PSNR provides a quantitative measure of the reconstructed image quality, while MS-SSIM offers a perceptual quality assessment that correlates well with human visual perception. These metrics were chosen for their complementary nature in assessing both objective and perceptual image quality.

\subsubsection{(2) Implementation Details.}
Our model was implemented using PyTorch and trained on 8 NVIDIA RTX 3090 GPUs. We trained the network for 30 epochs using the Adam optimizer with an initial learning rate of $1\times10^{-4}$, which was reduced by a factor of 0.5 every 10 epochs. The batch size was set to 16 for each GPU.
For the patch matching module, we used a patch size of 16$\times$16 pixels. The initial value of the adaptive fusion weight $\alpha$ was set to 0.5. The number of slices K in the slice-based autoregressive context model was set to 8.
In the KV-cache, we set the dimension of keys $d_k$ and values $d_v$ to 256. The cache size $N$ was initially set to 300 and dynamically adjusted based on the GPU memory availability. The number of clusters $K$ in Mini-Batch K-means was set to 3,000.

%We compared our method with standard codecs including VVC (VTM 12.0), HEVC (HM 16.20), and JPEG (libjpeg 9d), using their default configurations. For learned compression methods, we used the official implementations with their default parameters.

\subsection{Performance Results}

% We report the rate-distortion results in Figure~\ref{fig:rate_distortion}, which shows that our proposed CLC method outperforms existing methods across different bit-rates. The compared methods include traditional codecs such as BPG~\cite{bellard2014bpg}, VTM (Versatile Video Coding Test Model)~\cite{bross2021overview}, HM (HEVC Test Model)~\cite{sullivan2012overview}, and JPEG~\cite{wallace1992jpeg}. We also compare with recent learning-based methods, including the hyperprior model~\cite{balle2018variational}, Cheng et al.'s approach~\cite{cheng2021learned}, ELIC~\cite{zou2022elic}, and TCM~\cite{chen2023transformer}. Additionally, we include results from the AV1 codec~\cite{chen2018overview} to provide a comprehensive comparison against both traditional and modern compression techniques. The improvement in compression efficiency is significant. For example, on the Kodak dataset, when the MS-SSIM is 0.95, the bit rate of our CLC method is about 0.1 bpp, while the bit rates for the TCM, VTM, BPG, and JPEG methods are 0.15, 0.18, 0.22, and 0.38 bpp, respectively. Compared with TCM, VTM, BPG, and JPEG, the CLC method has increased the compression ratio by 1.5 times, 1.8 times, 2.2 times, and 3.8 times, respectively. Figure \ref{fig:enter-label} shows samples of visual comparison results. 
% On the CLIC dataset, the performance improvement is even more significant. When the PSNR is 34 dB, the bit-rate of CLC is about 0.2 bpp, while the bit rates for the TCM, VTM, Hyperprior, and JPEG methods are 0.25, 0.28, 0.35, and 0.45 bpp, respectively. This indicates that our CLC methods achieve an even larger compression efficiency gain on this dataset compared to other methods. 
% We have visualized our experimental results in Figure \ref{fig:main_visaul}. Our analysis reveals that our method demonstrates significant performance improvements in preserving detailed texture structures, particularly at low bit rates. This enhancement is especially noticeable in horizontal and vertical textures, as exemplified by the railing sections depicted in the figure.
We report the rate-distortion results in Figure~\ref{fig:rate_distortion}, showing our proposed CLC method outperforms existing methods across different bit-rates. The compared methods include traditional codecs like BPG~\cite{bellard2014bpg}, VTM~\cite{bross2021overview}, HM~\cite{sullivan2012overview}, and JPEG~\cite{wallace1992jpeg}, as well as recent learning-based methods: the hyperprior model~\cite{balle2018variational}, Cheng et al.'s approach~\cite{cheng2021learned}, ELIC~\cite{zou2022elic}, and TCM~\cite{chen2023transformer}. We also include results from AV1~\cite{chen2018overview} for comparison. The improvement in compression efficiency is significant. On Kodak at MS-SSIM 0.95, CLC achieves 0.1 bpp, while TCM, VTM, BPG, and JPEG require 0.15, 0.18, 0.22, and 0.38 bpp, respectively, representing a 1.5 to 3.8 times increase in compression ratio. On CLIC at 34 dB PSNR, CLC achieves 0.2 bpp, compared to 0.25, 0.28, 0.35, and 0.45 bpp for TCM, VTM, Hyperprior, and JPEG, indicating larger efficiency gains. Figure~\ref{fig:main_visaul} demonstrates our method's superior performance in preserving detailed textures, particularly horizontal and vertical structures at low bit rates, as seen in railings and architectural features.

\subsection{Ablation Studies}

We conducted ablation studies to evaluate components of our CLC method, focusing on reference images, dictionary cluster size, and component contributions. We report results on both Kodak and CLIC datasets to demonstrate the performance across different image types.



\paragraph{(1) Ablation Studies on the Number of Reference Images.}
% We changed the number of reference images from 1 to 5 to examine its impact on the compression performance. Table \ref{tab:ref_images} shows the BD-rate savings compared to the VTM method with different numbers of reference images. Here, BD-Rate$_\text{P}$ represents the BD-rate savings in terms of PSNR, while BD-Rate$_\text{M}$ represents the BD-rate savings in terms of MS-SSIM.
% We can see that using three reference images achieves the best performance on both datasets. Compared to the VTM methods, it saves the BD-rate by 14.5\% and 13.9\% on the Kodak and CLIC datasets, respectively. When the number of reference images becomes larger than 3, the performance degrades. This is because too much redundancy has been introduced into the conditional coding process.

We changed the number of reference images from 1 to 5 to examine the impact on compression performance. Table \ref{tab:ref_images} shows BD-rate savings compared to the VTM method with different numbers of reference images. BD-Rate$_\text{P}$ represents savings in PSNR, while BD-Rate$_\text{M}$ represents savings in MS-SSIM. Using three reference images achieves the best performance on both datasets, saving 14.5\% and 13.9\% BD-rate on Kodak and CLIC, respectively. More than three images introduce redundancy, degrading performance.

\begin{table}[t]
%  \vspace{-0.2cm}

\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.85} % 调整行间距
\begin{tabular}{@{}lcccc@{}}
\toprule[1.5pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Num of\\Ref. Images\end{tabular}} & \multicolumn{2}{c}{Kodak} & \multicolumn{2}{c}{CLIC} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& BD-Rate$_\text{P}$ & BD-Rate$_\text{M}$ & BD-Rate$_\text{P}$ & BD-Rate$_\text{M}$ \\
\midrule
1 & -10.2 & -11.5 & -9.8 & -10.9 \\
2 & -12.8 & -13.7 & -12.1 & -13.2 \\
3 & \textbf{-14.5} & \textbf{-15.2} & \textbf{-13.9} & \textbf{-14.7} \\
4 & -14.3 & -15.0 & -13.7 & -14.5 \\
5 & -14.2 & -14.9 & -13.6 & -14.4 \\
\bottomrule[1.5pt]
\end{tabular}
\caption{BD-rate savings (\%) vs. VTM for different numbers of reference images.}
\label{tab:ref_images}
%  \vspace{-0.2cm}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/visual.pdf}
     \vspace{-0.2cm}
    \caption{Image reconstruction results at around 0.1 bpp. From left to right: Raw inputs, reference images, reconstructed images. Red and blue boxes highlight specific areas of improvement.}
    \label{fig:main_visaul}
\end{figure*}
\paragraph{(2) Ablation Studies on the Dictionary Cluster Size.}
We conducted experiments with different dictionary cluster sizes to find the balance between compression efficiency and computational complexity. Table \ref{tab:dict_size} shows the BD-rate savings and encoding time for different cluster sizes. A cluster size of 3000 provides the best trade-off between performance and complexity for both datasets, achieving significant BD-rate savings with reasonable encoding times. The sharp increase in the encoding time for cluster sizes beyond 3000 highlights the importance of carefully selecting this parameter to balance compression efficiency and computational cost.

\begin{table}[t]

%  \vspace{-0.2cm}

\centering
\small
\setlength{\tabcolsep}{2pt} % 调整列间距
\renewcommand{\arraystretch}{0.85} % 调整行间距
\begin{tabular}{@{}lccccc@{}}
\toprule[1.5pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Cluster\\Size\end{tabular}} & \multicolumn{2}{c}{Kodak} & \multicolumn{2}{c}{CLIC} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Encoding\\Time (s)\end{tabular}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& BD-Rate$_\text{P}$ & BD-Rate$_\text{M}$ & BD-Rate$_\text{P}$ & BD-Rate$_\text{M}$ & \\
\midrule
1000 & -11.8 & -12.5 & -11.2 & -11.9 & 0.52 \\
2000 & -13.7 & -14.3 & -13.1 & -13.8 & 0.78 \\
3000 & \textbf{-14.5} & \textbf{-15.2} & \textbf{-13.9} & \textbf{-14.7} & 1.05 \\
4000 & -14.6 & -15.3 & -14.0 & -14.8 & 2.31 \\
5000 & -14.7 & -15.4 & -14.1 & -14.9 & 5.67 \\
\bottomrule[1.5pt]
\end{tabular}
%  \vspace{-0.2cm}
\caption{BD-rate savings (\%) vs. VTM and encoding time for different dictionary cluster sizes}
\label{tab:dict_size}
\end{table}

\paragraph{(3) Ablation Studies on Major Algorithm Components.}
We conducted ablation experiments to evaluate the contribution of major components. Table \ref{tab:components} shows each component's impact on the Kodak dataset performance. All components contribute significantly, with CLS having the most substantial impact (4.7\% BD-rate savings), highlighting the importance of adaptive feature modulation. The KV-cache, while minimally impacting compression performance, significantly reduces encoding time (from 1.87s to 1.05s). Multi-sample query in dictionary construction improves BD-rate savings by 0.7\% (BD-Rate$_\text{P}$) and 0.6\% (BD-Rate$_\text{M}$), enhancing overall compression capability through more diverse representations.

\begin{table}[t]

%  \vspace{-0.2cm}

\centering
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.85} % 调整行间距
\begin{tabular}{@{}lccc@{}}
\toprule[1.5pt]
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Model\\Configuration\end{tabular}} & \multicolumn{2}{c}{BD-Rate Savings} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Encoding\\Time (s)\end{tabular}} \\
\cmidrule(lr){2-3}
& BD-Rate$_\text{P}$ & BD-Rate$_\text{M}$ & \\
\midrule
Full Model & \textbf{-14.5} & \textbf{-15.2} & 1.05 \\
w/o CLM & -12.3 & -13.1 & 0.98 \\
w/o CLS & -9.8 & -10.5 & 0.92 \\
w/o KV-cache & -14.4 & -15.1 & 1.87 \\
w/o Multi-example Query & -13.8 & -14.6 & 0.97 \\
\bottomrule[1.5pt]
\end{tabular}
%  \vspace{-0.2cm}
\caption{BD-rate savings (\%) vs. VTM for different model configurations on Kodak dataset}
\label{tab:components}
\end{table}

 


\section{Conclusion} \label{sec:conclusion}
This study proposes Conditional Latent Coding (CLC), a novel deep learning-based image compression method that dynamically generates latent reference representations through a universal image feature dictionary. We develop innovative techniques for dictionary construction, efficient search/matching, alignment, and fusion, with theoretical analysis of robustness to dictionary and latent perturbations. While focused on compression, CLC's adaptive feature utilization principles may inspire broader vision tasks. Future work includes balancing compression efficiency and visual information utilization to address growing data transmission demands.

\clearpage
\section*{Acknowledgements}
This research was supported by the National Natural Science Foundation of China (No. 62331014) and Grant 2021JC02X103.
% \bibliographystyle{aaai25.bst}
% \bibliography{aaai25}
\input{anonymous-submission-latex-2025.bbl}

\appendix
\input{figure/appendix_after}

\end{document}
