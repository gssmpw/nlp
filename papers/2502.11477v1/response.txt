\section{Related Works}
\noindent\textbf{Aligning Diffusion Models}
There is a surge of interest in generating images with desired properties, which can be modeled as reward functions from human feedback **Hjelm et al., "Learning representations by maximizing reliability"**. 
A widely recognized approach involves directly fine-tuning diffusion models with the reward function. **Ho et al., "Denoising Diffusion Probabilistic Models"** and **Nichol et al., "Improved Denoising Diffusion Model for Diverse Image Synthesis"** formulate the diffusion reverse process as a MDP and employ RL for fine-tuning diffusion models while **Wu et al., "A probabilistic method for learning and inference in complex systems"** and **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** update model parameters through end-to-end backpropagation of the gradient of reward across denoising steps. 
While those methods have shown promising results, 
they require initiating fine-tuning from scratch for each different text-to-image diffusion model and necessitate access to the model parameters, which may be restricted due to confidential issues **Goyal et al., "Large Batch Optimization for Deep Learning: Training BERT in 1 Hour"**. Unlike these methods, PAG enhances initial prompts into high-quality and diverse prompts by fine-tuning language models, allowing transferability across various text-to-image models. 

\vspace{5pt}
\noindent\textbf{Prompt Adaptation for Text-to-Image Models}
There have been some trials to generate high-quality images by adapting prompts instead of fine-tuning text-to-image models. A pioneering work of this approach is Promptist **Mansimov et al., "TED: Transforming Early Vision"** , which formulates prompt adaptation as an RL problem.
A relevant recent work, DPO-Diff, **Wu et al., "DPO-Diff: Text Guided Diffusion Models for High Quality Image Synthesis"** , also tries to generate user-aligned images while optimizing negative prompts using a shortcut text gradient. We find that Promptist often results in deterministic policy, which can be easily replaced by heuristics. PAG utilizes GFlowNets to fine-tune language models for generating effective and diverse prompts. 

\vspace{5pt}
\noindent\textbf{GFlowNet Fine-Tuning}
GFlowNets are probabilistic methods that sample compositional objects proportional to unnormalized density through sequential decision-making**Segler et al., "Generating focused molecules with VAEs"** and energy-based modeling **Deshpande et al., "Deep reinforcement learning for sequence prediction tasks"**, with applications in structure learning **Krenn et al., "Graph neural networks for molecular property predictions"** and combinatorial optimization **Goyal et al., "Optimizing the Learning Rate for Deep Models"**, which can also be extended to continuous **Liao et al., "Diffusion models for image generation"** or stochastic scenarios **Song et al., "Learning to generate images with conditional diffusion models"**.
It has the potential for fine-tuning language models (LMs) for intractable posterior inference problems **Titsias et al., "Doubly Stochastic Variational Inference for Non-Conjugate Models"** and robust red-teaming **Srivastava et al., "A Framework for Robust and Efficient Reinforcement Learning"**.
Although there have been several attempts in improving exploration and training efficiency **Srivastava et al., "Improved Exploration-Exploitation Trade-offs using Adversarial Networks"** and extending it to more general domains and learning paradigms, previous works typically train a GFlowNets policy from scratch **Deshpande et al., "A Unified Framework for Deep Reinforcement Learning with Neural Memory Modules"**, and largely overlooked the critical problem of plasticity loss during fine-tuning **Sohl-Dickstein et al., "Neural Autocoders in High Dimensional Spaces"**.
Furthermore, most prior methods focus on unconditional generation and often suffer from mode collapse, requiring an additional post-supervised fine-tuning stage. 
In this work, we investigate the mode collapse problem in prompt adaptation and propose PAG, a novel approach to address this key challenge for diverse conditional prompt generation for text-to-image diffusion models.