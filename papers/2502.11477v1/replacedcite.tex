\section{Related Works}
\noindent\textbf{Aligning Diffusion Models}
There is a surge of interest in generating images with desired properties, which can be modeled as reward functions from human feedback ____. 
A widely recognized approach involves directly fine-tuning diffusion models with the reward function. ____ and ____ formulate the diffusion reverse process as a MDP and employ RL for fine-tuning diffusion models while ____ and ____ update model parameters through end-to-end backpropagation of the gradient of reward across denoising steps. 
While those methods have shown promising results, 
they require initiating fine-tuning from scratch for each different text-to-image diffusion model and necessitate access to the model parameters, which may be restricted due to confidential issues ____. Unlike these methods, PAG enhances initial prompts into high-quality and diverse prompts by fine-tuning language models, allowing transferability across various text-to-image models. 

\vspace{5pt}
\noindent\textbf{Prompt Adaptation for Text-to-Image Models}
There have been some trials to generate high-quality images by adapting prompts instead of fine-tuning text-to-image models. A pioneering work of this approach is Promptist ____, which formulates prompt adaptation as an RL problem.
A relevant recent work, DPO-Diff, ____, also tries to generate user-aligned images while optimizing negative prompts using a shortcut text gradient. We find that Promptist often results in deterministic policy, which can be easily replaced by heuristics. PAG utilizes GFlowNets to fine-tune language models for generating effective and diverse prompts. 

\vspace{5pt}
\noindent\textbf{GFlowNet Fine-Tuning}
GFlowNets are probabilistic methods that sample compositional objects proportional to unnormalized density through sequential decision-making____ and energy-based modeling____, with applications in structure learning____ and combinatorial optimization____, which can also be extended to continuous____ or stochastic scenarios____.
It has the potential for fine-tuning language models (LMs) for intractable posterior inference problems____ and robust red-teaming____.
Although there have been several attempts in improving exploration and training efficiency____ and extending it to more general domains and learning paradigms, previous works typically train a GFlowNets policy from scratch____, and largely overlooked the critical problem of plasticity loss during fine-tuning____.
Furthermore, most prior methods focus on unconditional generation and often suffer from mode collapse, requiring an additional post-supervised fine-tuning stage. 
In this work, we investigate the mode collapse problem in prompt adaptation and propose PAG, a novel approach to address this key challenge for diverse conditional prompt generation for text-to-image diffusion models.