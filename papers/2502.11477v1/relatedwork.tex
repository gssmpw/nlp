\section{Related Works}
\noindent\textbf{Aligning Diffusion Models}
There is a surge of interest in generating images with desired properties, which can be modeled as reward functions from human feedback \citep{ouyang2022training}. 
A widely recognized approach involves directly fine-tuning diffusion models with the reward function. \citet{blacktraining} and \citet{fan2024reinforcement} formulate the diffusion reverse process as a MDP and employ RL for fine-tuning diffusion models while \citet{clarkdirectly} and \citet{prabhudesai2023aligning} update model parameters through end-to-end backpropagation of the gradient of reward across denoising steps. 
While those methods have shown promising results, 
they require initiating fine-tuning from scratch for each different text-to-image diffusion model and necessitate access to the model parameters, which may be restricted due to confidential issues \citep{ramesh2022hierarchical, saharia2022photorealistic}. Unlike these methods, PAG enhances initial prompts into high-quality and diverse prompts by fine-tuning language models, allowing transferability across various text-to-image models. 

\vspace{5pt}
\noindent\textbf{Prompt Adaptation for Text-to-Image Models}
There have been some trials to generate high-quality images by adapting prompts instead of fine-tuning text-to-image models. A pioneering work of this approach is Promptist \citep{hao2024optimizing}, which formulates prompt adaptation as an RL problem.
A relevant recent work, DPO-Diff, \citep{wang2024discrete}, also tries to generate user-aligned images while optimizing negative prompts using a shortcut text gradient. We find that Promptist often results in deterministic policy, which can be easily replaced by heuristics. PAG utilizes GFlowNets to fine-tune language models for generating effective and diverse prompts. 

\vspace{5pt}
\noindent\textbf{GFlowNet Fine-Tuning}
GFlowNets are probabilistic methods that sample compositional objects proportional to unnormalized density through sequential decision-making~\citep{bengio2021flow, bengio2023gflownet} and energy-based modeling~\citep{zhang2022generative}, with applications in structure learning~\citep{deleu2022bayesian} and combinatorial optimization~\citep{zhangrobust,Zhang2023LetTF}, which can also be extended to continuous~\citep{lahlou2023cgfn} or stochastic scenarios~\citep{pan2023stochastic,zhang2023distributional}.
It has the potential for fine-tuning language models (LMs) for intractable posterior inference problems~\citep{huamortizing} and robust red-teaming~\citep{lee2024learning}.
Although there have been several attempts in improving exploration and training efficiency~\citep{pan2023generative, kimlocal, lau2024qgfn} and extending it to more general domains and learning paradigms, previous works typically train a GFlowNets policy from scratch~\citep{pan2024pre,he2024looking}, and largely overlooked the critical problem of plasticity loss during fine-tuning~\citep{zhang2024improving,liu2024efficientdiversitypreservingdiffusionalignment}.
Furthermore, most prior methods focus on unconditional generation and often suffer from mode collapse, requiring an additional post-supervised fine-tuning stage. 
In this work, we investigate the mode collapse problem in prompt adaptation and propose PAG, a novel approach to address this key challenge for diverse conditional prompt generation for text-to-image diffusion models.