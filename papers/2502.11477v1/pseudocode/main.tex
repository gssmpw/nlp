\begin{algorithm}[t]
    \caption{Prompt Adaptation with GFlowNets (PAG)}
    \algrenewcommand\algorithmicrequire{\textbf{Input:}}
    \algrenewcommand\algorithmicensure{\textbf{Output:}}
    \begin{algorithmic}[1]
    \Require{
        Initial prompt dataset $\mathcal{D}$, pretrained LM $p_{\text{ref}}$, number of training rounds $N$, reset period $M$, batch size $b$.
    }
    \Ensure{Fine-tuned policy $P_{F}(\cdot\vert\cdot;\theta)$}
    \State{Initialize $P_{F}(\cdot\vert\cdot;\theta)\leftarrow p_{\text{ref}}$, $\mathcal{B}\leftarrow\emptyset$}, and $F_{\theta}$
    \For {$n=1,\cdots,N$}
        \State{Initialize $\ell\leftarrow0$}
        \For {$i=1,\cdots,b$}
            \State{Uniformly sample $s$ from $\{0, 1\}$}
            \If{$s==0$} 
                \State{Sample $\mathbf{x}\sim\mathcal{D}$ and $\mathbf{y}^{i}\sim P_{F}(\cdot\vert\mathbf{x})$}
                \State{Compute $R(\mathbf{x},\mathbf{y}^{i})$ with \cref{eq:reward}}
                \State{$\mathcal{B}\leftarrow\mathcal{B}\cup\{(\mathbf{x}, \mathbf{y}^{i}, R(\mathbf{x}, \mathbf{y}^{i}))\}$}
            \Else
            % \State{\textcolor{Maroon}{\textbf{// Reward-Prioritized Sampling}}}
                \State{Sample $(\mathbf{x}, \mathbf{y}^{i}, R(\mathbf{x},\mathbf{y}^{i}))\sim\mathcal{B}$ with \cref{eq:prt}}
            \EndIf
            \State{Compute $\ell\leftarrow\ell+\mathcal{L}(\mathbf{x},\mathbf{y}^{i};\theta) / b$} with \cref{eq:fl_db}
        \EndFor
        % \State{\textcolor{NavyBlue}{\textbf{// Reward Decomposition}}}
        \State{Update $P_{F}(\cdot\vert\cdot;\theta)$ and $F(\cdot;\theta)$} with computed loss $\ell$
        % \State{\textcolor{orange}{\textbf{// Flow Reactivation}}}
        \If{$n\text{ mod }M == 0$}
            \State{Reset the last layer of $F(\cdot;\theta)$}
        \EndIf
    \EndFor
    \end{algorithmic}
    \label{alg:main}
\end{algorithm}