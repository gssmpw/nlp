% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt,dvipsnames]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{tcolorbox}
\newcommand{\hquad}{\hspace{0.5em}} 
\newcommand{\fixedwidth}{{2.0cm}} 
\newcommand{\llmmarker}[1]{\tcbox[colback=SkyBlue!70, colframe=SkyBlue!70, 
    arc=5pt, boxrule=0pt, left=2pt, right=2pt, top=0pt, bottom=0pt, 
    on line, boxsep=1pt]{\makebox[\fixedwidth][c]{#1}}}
\newcommand{\humanmarker}[1]{\tcbox[colback=Salmon!70, colframe=Salmon!70, 
    arc=5pt, boxrule=0pt, left=2pt, right=2pt, top=0pt, bottom=0pt, 
    on line, boxsep=1pt]{\makebox[\fixedwidth][c]{#1}}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ExaGPT: Example-Based Machine-Generated Text Detection\\for Human Interpretability}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ryuto Koike$^1$ \hquad Masahiro Kaneko$^{2,1}$ \hquad Ayana Niwa$^{2}$ \hquad Preslav Nakov$^{2}$ \hquad Naoaki Okazaki$^{1,3,4}$ \\
  $^1$Institute of Science Tokyo \quad $^2$MBZUAI \quad $^3$AIST \quad $^4$NII LLMC\\
  \texttt{\{ryuto.koike@nlp., okazaki@\}comp.istc.ac.jp}\\
  \texttt{\{masahiro.kaneko, ayana.niwa, preslav.nakov\}@mbzuai.ac.ae}
  \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Detecting texts generated by Large Language Models (LLMs) could cause grave mistakes due to incorrect decisions, such as undermining student's academic dignity.
LLM text detection thus needs to ensure the interpretability of the decision, which can help users judge how reliably correct its prediction is. 
When humans verify whether a text is human-written or LLM-generated, they intuitively investigate with which of them it shares more similar spans.
However, existing interpretable detectors are not aligned with the human decision-making process and fail to offer evidence that users easily understand. 
To bridge this gap, we introduce \textbf{ExaGPT}, an interpretable detection approach grounded in the human decision-making process for verifying the origin of a text.
ExaGPT identifies a text by checking whether it shares more similar spans with human-written vs. with LLM-generated texts from a datastore.
This approach can provide similar span examples that contribute to the decision for each span in the text as evidence.
Our human evaluation demonstrates that providing similar span examples contributes more effectively to judging the correctness of the decision than existing interpretable methods.
Moreover, extensive experiments in four domains and three generators show that ExaGPT massively outperforms prior powerful detectors by up to +40.9 points of accuracy at a false positive rate of 1\%.
% We will release our code. 
% \footnote{\url{https://www.github.com/ryuryukke/ExaGPT}}
\end{abstract}

\section{Introduction}
LLMs can yield human-like texts in response to various textual instructions \citep{chatgpt,touvron2023llama}.
Ironically, the powerful generative capability has resulted in various misuses of LLMs, such as cheating in student homework assignments and mass-producing fake news \citep{tang2023science,wu2023survey}. Such abuse of LLMs has sparked the demand for discerning LLM-generated texts from human-written ones. 

Recent studies have developed LLM-generated text detectors with promising performance \citep{mitchell2023detectgpt,su2023detectllm,Koike:OUTFOX:2024,hans2024spotting,verma2024ghostbuster}.

While LLM text detection can help prevent potential misuse of LLMs, misclassifications could lead to severe consequences.
% The potential impact of LLM text detection could be quite severe.
For instance, web content writers have recently been at risk of losing their careers because of false-positive classification \cite{gizmodo}. In school education, incorrect detection results might ruin students' academic dignity \citep{educatorconsiderations4chatgpt,bloomberg}. 
At the same time, it is extremely difficult, if not impossible, to develop a perfect detector with 100\% accuracy in such real-world scenarios, and there remain edge cases where human-written texts can be misidentified as LLM-generated and vice versa.
Thus, it is crucial to create a detector that provides interpretable evidence, allowing users to judge how reliably correct the detection results are \citep{tang2023science,ji2024detectingmachinegeneratedtextsjust}.

\begin{figure}[t]
 \begin{center}
 \small
  \centering\includegraphics[width=\columnwidth]{figures/exagpt_image.png}
  \caption{Identifying the author of a text (human vs. LLM) by examining if it shares more similar spans, including verbatim overlaps and semantically similar spans, with human-written vs. LLM-generated texts.}
  \label{human_intuitive_identification}
 \end{center}
\end{figure}

\begin{figure*}[t]
 \begin{center}
 \small
  \centering\includegraphics[width=0.95\textwidth]{figures/exagpt.png}
  \caption{Overview of ExaGPT. It detects the author of a text by examining whether the text shares more similar spans with human-written texts vs. with LLM-generated texts from a datastore.}
  \label{exagpt}
 \end{center}
\end{figure*}

Most detectors lack the interpretability of their decisions, outputting only binary labels of who authored the text.
There are few studies on the interpretability of the detection.
\citet{gehrmann2019gltr} color-highlighted the tokens with high probability under the predicted distribution of LMs. 
\citet{mitrović2023chatgpt,wang-etal-2024-m4} showed which part of a text contributed to a decision based on prediction shifts via perturbations to the text.
\citet{yang2023dnagptdivergentngramanalysis} provided the $n$-gram overlaps between the original text and re-prompted ones generated by LLMs.
Here, humans intuitively judge whether a text is human-written or LLM-generated by assessing with which source it shares more \textit{similar spans}, including verbatim overlaps and semantically similar spans \cite{maurer06,barron-cedeno-etal-2013-plagiarism}.
However, current detectors are not aligned with the human decision-making process (Figure \ref{human_intuitive_identification}) and fail to yield sufficiently interpretable evidence for users.

% However, humans assess text originality by examining verbatim overlaps or semantically similar spans between the text and existing source texts \cite{MaurerKZ06,barron-cedeno-etal-2013-plagiarism}; prior detectors are not aligned with this decision-making process and still lack interpretability.

Motivated by this gap, we present \textbf{ExaGPT}, an interpretable detection method based on the human decision-making process of verifying the origin of a text. In particular, ExaGPT makes a prediction by examining whether the text shares more similar spans with human-written vs. with LLM-generated texts from a datastore.
This approach can provide similar span examples that contribute to the decision for each span in the text as interpretable evidence.
To present interpretable span-segmented text as a final result, we apply a dynamic programming algorithm and determine the optimal span break. It balances the long span length and its high frequency with the datastore (i.e.,~many similar phrases to the span exist in the datastore).
The similarity of the retrieved spans to each span in the target text can help users judge the reliability of the detection result.

To evaluate the interpretability of LLM detection, we conducted a human evaluation of how well people can infer the correctness of the detection from the detector's evidence, and we found that
% This evaluation setting aligns with the real scenario where users judge how reliably correct the detection results are.
providing similar span examples contributes more effectively to judging the correctness of the detection than existing interpretable methods.
Moreover, extensive experiments in four domains and three generators showed that ExaGPT massively outperforms prior interpretable and powerful detectors by up to +40.9 points accuracy, even at a constant false positive rate of 1\%. From these results, we observe that ExaGPT achieves high interpretability in its detection result and also high detection performance.

\section{Methodology}
ExaGPT classifies a text based on whether it shares more similar spans with human-written or with LLM-generated texts from a datastore.
As a final result, ExaGPT offers the span-segmented text where each span is accompanied by similar span examples that contribute to the decision.
Figure~\ref{exagpt} illustrates the workflow of ExaGPT, which has two phases: \textbf{Span Scoring} and \textbf{Span Selection}.
In the first phase, we mainly investigate whether each span in the target text shares more similar spans with human-written or LLM-generated texts from a datastore. Meanwhile, we calculate scores for each span, which we use in the second phase (\S\ref{span_scoring}).
% to determine the optimal span segmentation as a final result.
In the second phase, we primarily decide the optimal span segmentation to aid users' understanding of the final result. 
Specifically, we apply a dynamic programming (DP) algorithm with the scores from the first phase to find the span boundaries, balancing span length and its frequency within the datastore (\S\ref{span_selection}). Finally, we detect the target text based on the selected spans and we provide similar span examples for each target span as evidence (\S\ref{detection_score}).
We will go into further details below.

\subsection{Span Scoring with \textit{k}-NN Search} 
\label{span_scoring}
Given a target text $x$ to be classified, we define an $n$-gram span in the text $x$ as $x_{i:i+n}$, which is any continuous sequence of $n$ tokens starting in the $i$-th token. For each $n$-gram target span $x_{i:i+n}$, we retrieve the top-$k$ most similar\footnote{We encode the target span, and all spans in the datastore into the same embedding space. We then perform $k$-nearest neighbor ($k$-NN) search based on the cosine similarity of each two span embeddings. See more details in \S\ref{setting_exagpt}.} $n$-gram spans $s_{j}\;(j \in \{1, \dots, k\})$ from the datastore, with each original label and similarity $\{(s_{j}, l_{j}, c_{j})\}_{j=1}^{k}$. Here, $l_{j}$ is \texttt{Human} when the span $s_j$ is part of a human-written text, or \texttt{LLM} when the span $s_j$ is a part of a LLM-generated text. $c_{j}$ is the similarity between the target span $x_{i:i+n}$ and each retrieved span $s_{j}$.

Consequently, we calculate the following metrics for each target span $x_{i:i+n}$: \emph{length score} $L$, \emph{reliability score} $R$, and \emph{prediction score} $P$. 
The length score $L$ is the number of tokens in the target span:

\begin{equation}
L(x_{i:i+n}) = n
\end{equation}
The reliability score $R$ is the mean similarity $c_{j}$ between the target span and each retrieved span: 

\begin{equation}
R(x_{i:i+n}) = \frac{\sum_{j=1}^{k}c_j}{k}
\end{equation}
The reliability score $R$ indicates how many similar spans exist in the datastore for the target span. The prediction score $P$ is a ratio of \texttt{LLM} label in the original labels $l_{j}$ of the retrieved spans:

\begin{equation}
P(x_{i:i+n}) = \frac{\sum_{j=1}^{k}\mathbbm{1}(l_j = \text{\texttt{LLM}})}{k} .
\end{equation}
The prediction score $P$ indicates whether the target span shares more similar spans with human-written vs. with LLM-generated texts in the datastore.

\subsection{Span Selection with a DP Algorithm}
\label{span_selection}
In this phase, we select spans $T = [t_1, \dots, t_H]$ in the target text $x$, so that the text is segmented without overlaps as a final result:

\begin{algorithm}[t]
    \caption{Span Segmentation Optimization}
    \label{dp_algo}
\begin{algorithmic}
    \STATE {\bfseries Input:} Target text $x$; Length of target text $m$; Length score $L$; Reliability score $R$; Maximum length of \textit{n}-gram span $N$; Hyper-parameter $\alpha$
    \STATE {\bfseries Output:} List of selected \textit{n}-grams $T$
    \STATE ${\rm dp}[0, \ldots, m-1] \leftarrow [([0], {\rm None})] * m$ 
    \FOR{$i=1$ {\bfseries to} $m$}
        \FOR{$j={\rm min}(i - N, 0)$ {\bfseries to} $i$}
        \STATE $l, r \leftarrow  L^{\rm std}(x_{j:i}), R^{\rm std}(x_{j:i})$
        % \STATE Compute 
        % \STATE $l_{\rm std}, r_{\rm std} \leftarrow  {\rm standardize}(l), {\rm standardize}(r)$
        \STATE $scores \leftarrow {\rm dp}[j][0] + [\alpha l + (1-\alpha) r]$
        \STATE $s_{\rm cand} \leftarrow {\rm average}(scores)$
        \IF{${\rm average}({\rm dp}[i][0]) < s_{\rm cand}$}
            \STATE ${\rm dp}[i] \leftarrow (scores, j)$
    \ENDIF
        \ENDFOR
    \ENDFOR
    \STATE Traverse ${\rm dp}$ backward and collect span breaks
    \STATE \textbf{return} List of selected \textit{n}-grams $T$
    % \STATE $next, breaks \leftarrow dp[-1][1], [m]$
    % \WHILE{$next \neq \rm None$}
    %     \STATE $breaks, next \leftarrow breaks + [next], dp[next][1]$
    %     % \STATE $next \leftarrow dp[next][1]$
    % \ENDWHILE
    % \STATE $start, T \leftarrow 0, [\varnothing]$
    % \FOR {$end$ in ${\rm reverse}(breaks)$}
    %     \STATE $T, start \leftarrow T + [x_{start:end}], end$
    %     % \STATE $start \leftarrow end$
    % \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{equation}
\begin{aligned}
& x = t_1 \oplus t_2 \oplus \dots \oplus t_H, \\
& t_i \cap t_j = \varnothing \quad (i, j \in \{1,\ldots,H\}, i \neq j) 
\end{aligned}
\end{equation}
% Here, when humans measure text originality, they focus on whether there are long and similar phrases between the text and other texts.
To facilitate users' understanding of the final result, we optimize the span segmentation that includes longer and more similar spans with ones from the datastore.
Algorithm \ref{dp_algo} describes our dynamic programming strategy to find the best span break. Formally, we select spans $T$ to maximize the score $S$ across the spans in the target text:
\begin{equation}
\label{alpha_formulation}
S(T) = \frac{\sum_{h=1}^{H} \{ \alpha L^{\rm std}(t_h) + (1-\alpha)R^{\rm std}(t_h)\}}{H} .
\end{equation}
Here, $L^{\rm std}(t_h)$ and $R^{\rm std}(t_h)$ are the normalized\footnote{To align the scales of the length score and the reliability score, each score is normalized using the mean and the variance in the validation split of our dataset.} versions of the length score $L$ and the reliability score $R$ of the span $t_h$, respectively. $\alpha$ is an interpolation coefficient ranging from 0.0 to 1.0. $\alpha$ determines the relative contribution of the length score and the reliability score to the span segmentation.

\subsection{Overall Detection with Evidence}
\label{detection_score}
% $[(t_1, P(t_1)), \dots, (t_H, P(t_H))]$
Given a sequence of the selected spans $T$ each with a prediction score for the target text $x$, ExaGPT identifies a text based on the mean prediction score:

\begin{equation}
P_{\rm overall} = \frac{\sum_{h=1}^{H}P(t_h)}{H}.
\end{equation}
ExaGPT classifies a text as \texttt{LLM} if $P_{\rm overall}$ exceeds a detection threshold $\epsilon$, and otherwise as \texttt{Human}.
As evidence of the decision, ExaGPT provides retrieved top-\textit{k} similar spans for each span in the text: 

\begin{equation}
E = [(t_{h}, [s_{h}^{1}, \ldots, s_{h}^{k}])]_{h=1}^{H} .
\end{equation}
The similarity of the retrieved spans to each span in the target text can help users judge how reliably correct the detection result is.

\section{Experiments and Results}
\label{experiments}
% In this section, we explore the interpretability and detection performance of the ExaGPT.

\subsection{Overall Setup}
\label{setup}

\paragraph{Evaluation Measures.}
To assess the detection performance, we use the Area Under Receiver Operating Characteristic curve (AUROC) measure, which is widely used in studies on LLM detection. However, it is only useful to observe the overall behavior of a detector through all possible thresholds. In practical scenarios, it is quite important to minimize the false positive classification, i.e.,~wrongly identifying human-written texts as LLM-generated. We thus report the detection accuracy with a threshold by fixing the false-positive rate (FPR) at 1\%, which is an evaluation stream among recent robustness studies \cite{krishna2023paraphrasing,hans2024spotting,dugan-etal-2024-raid}.


\paragraph{Datasets.}
We use the M4 dataset \cite{wang-etal-2024-m4}, which is a large-scale LLM detection benchmark consisting of pairs of human-written and LLM-generated texts across multiple languages, domains, and generators. In our experiments, we use the English subset, including 3,000 pairs of human-written and LLM-generated texts from each combination of \textbf{four domains}: Wikipedia, Reddit, WikiHow, and arXiv, as well as \textbf{three generators}: ChatGPT, GPT-4 as closed-source LLMs, and Dolly-v2 \cite{dolly} as open-source LLMs. For each combination, we split the dataset into three parts: train/validation/test with 2,000/500/500 pairs, respectively.

\paragraph{Baselines.}
In our experiments, we compare ExaGPT to \textbf{three strong and interpretable detectors} (as detailed in \S\ref{interpretable_detectors}): RoBERTa with SHAP \cite{mitrović2023chatgpt}, LR-GLTR \cite{wang-etal-2024-m4}, and DNA-GPT \cite{yang2023dnagptdivergentngramanalysis}. 
The first one is a supervised classifier based on RoBERTa\footnote{\url{https://huggingface.co/FacebookAI/roberta-base}} \cite{liu2019roberta}, which we fine-tune for LLM detection on our train split. 
Similarly, we train the LR-GLTR detector on our train split with selected and hand-crafted GLTR features \cite{gehrmann2019gltr}, following \cite{wang-etal-2024-m4}. 
The hyper-parameter settings for training both RoBERTa and LR-GLTR are aligned with \cite{wang-etal-2024-m4}.
For the parameter configuration of DNA-GPT, we set the truncation ratio $\gamma$ to 0.7 and 0.5, and the number of re-generations $K$ to 10 and 5 for closed-source and open-source LLMs, respectively.
We also ensured that the \texttt{temperature} is the same as the one used to generate a target text and that the generation prompt is known.
These configurations were found to ensure the favorable performance of DNA-GPT in \cite{yang2023dnagptdivergentngramanalysis}. 
We set all other hyper-parameters to their default values.
Further configuration details of the baseline detectors are given in Appendix~\ref{configurations}.


\paragraph{Settings of ExaGPT.}
\label{setting_exagpt}
In the span scoring phase, ExaGPT leverages our train split as the datastore for each combination of domains and generators. 
We consider the size of $n$-gram to be from 1 to 20 throughout the entire dataset.
We embed the target span and all spans in the datastore into the same vector space using BERT-large\footnote{\url{https://huggingface.co/google-bert/bert-large-uncased}}. For a span embedding, we feed a text into the BERT-large and take the mean second-layer\footnote{We select the layer where the \textit{k}-NN spans are similar to the target span well-balanced lexically and semantically, enhancing its interpretability in our pilot study.} hidden outputs of tokens included in the span.
We retrieve the top-10 most similar spans from the datastore for each target span via $k$-NN search using the FAISS library \cite{faiss_paper}.

In the span selection phase, we select the optimal $\alpha$ from values between 0.0 and 1.0 at 0.125 intervals, where ExaGPT exhibits the best detection performance in our validation split. The $\alpha$ is constant through our evaluation of the interpretability and the detection performance of ExaGPT.

% \setcounter{footnote}{3}

\begin{figure}[t]
 \begin{center}
  \centering\includegraphics[width=\columnwidth]{figures/exagpt_demo.png}
  \caption{User interface of ExaGPT. Hovering over a text span displays the tooltip about the retrieved similar spans each with the similarity to the span and the original label distribution.}
  \label{exagpt_demo}
 \end{center}
\end{figure}


\paragraph{Human Evaluation in Terms of Interpretability.}
We assess the interpretability of the detectors via human evaluation, as it is vital to for a good detector to offer interpretable evidence, allowing users to judge how reliably correct the detection result is.
Accordingly, we design human evaluation where participants are provided with detection evidence and judge whether the detection is correct.
Therefore, the evaluation metric for interpretability is the accuracy of the human judgments on the detection correctness based on the evidence.
For each detector, we evaluate 96 samples\footnote{The 96 samples for each detector consist of two samples (one correct and one incorrect) across four domains and three generators, distributed among four participants.} from our test split in all combinations of domains and generators so that the ratio of correct and incorrect detections\footnote{We focus on the setting of the 1\% FPR threshold based on practical scenarios.} is even.
In our human evaluation, four annotators, including one MSc student, one PhD student, and two researchers working in natural language processing, were provided with different samples. 
% To ensure the consistency of evaluations by the five annotators, we additionally measure Fleiss' Kappa as the inter-annotator agreement based on their evaluations of 20 common samples for each detector.

Figure \ref{exagpt_demo} shows the user interface of ExaGPT\footnote{We implemented a demo app of ExaGPT with the streamlit framework: \url{github.com/streamlit/streamlit}.} in our human evaluation. 
The spans are highlighted\footnote{ExaGPT performs the overall detection rather than detecting each span individually. However, for better readability, each span is color-highlighted on its prediction score.} in red, green, and blue for which prediction score $P$ is lower than 0.5 (human-written), equal to 0.5 (neither), and higher than 0.5 (LLM-generated), respectively.
The participants identify the correctness of the detection by mainly investigating similar span examples for each span in the text.
We elaborate on the detection evidence of each baseline detector in Appendix~\ref{evidence_baselines}.


\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{tabular}{lc}
\toprule
\multicolumn{1}{c}{\textbf{Detector}} & \textbf{ACC. of Human Judgements (\%) $\uparrow$} \\
\midrule
RoBERTa & 47.9 \\
LR-GLTR & 57.3 \\
DNA-GPT & 53.1 \\
ExaGPT & \textbf{61.5} \\
\bottomrule
\end{tabular}
\caption{Comparison of the accuracy (ACC.) of human judgments on the correctness of detections based on evidence across baseline detectors and ExaGPT. Higher accuracy implies that the detector provides more interpretable evidence to users.}
\label{detection_interpretability_result}
\end{table}


\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{5pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{tabular}{clcccccccccc}
\toprule
 \multirow{2.5}{*}{\textbf{Generator}} & \multicolumn{1}{c}{\multirow{2.5}{*}{\textbf{Detector}}} & \multicolumn{2}{c}{\textbf{Wikipedia}} & \multicolumn{2}{c}{\textbf{Reddit}} & \multicolumn{2}{c}{\textbf{WikiHow}} & \multicolumn{2}{c}{\textbf{arXiv}} & \multicolumn{2}{c}{\textbf{Avg.}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} 
 & & AUROC & ACC. & AUROC & ACC. & AUROC & ACC. & AUROC & ACC. & AUROC & ACC. \\
\midrule
\multirow{4}{*}{ChatGPT} & RoBERTa & \textbf{100.0} & 50.0 & \textbf{100.0} & 50.0 & \textbf{100.0} & 75.3 & \textbf{100.0} & 60.9 & \textbf{100.0} & 59.1 \\
& LR-GLTR & 99.6 & \textbf{96.5} & 99.4 & 93.1 & 97.0 & 75.6 & 99.6 & 96.5 & 98.9 & 90.4\\
& DNA-GPT & 84.8 & 49.4 & 92.3 & 62.9 & 99.4 & 93.5 & 89.0 & 59.9 & 91.4 & 66.4\\
& ExaGPT & 98.8 & 95.0 & 99.0 & \textbf{95.0} & 99.5 & \textbf{96.8} & 99.6 & \textbf{98.2} & 99.2 & \textbf{96.2} \\
\midrule
\multirow{4}{*}{GPT-4} & RoBERTa & \textbf{100.0} & 77.7 & \textbf{100.0} & 76.9 & \textbf{100.0} & 57.9 & \textbf{100.0} & 65.7 & \textbf{100.0} & 69.6 \\
& LR-GLTR & 99.6 & 94.7 & 99.4 & 93.2 & 95.7 & 65.3 & \textbf{100.0} & 97.1 & 98.7 & 87.6\\
& DNA-GPT & 40.3 & 48.1 & 71.9 & 68.6 & 44.6 & 49.9 & 72.2 & 54.4 & 57.3 & 55.3\\
& ExaGPT & 98.8 & \textbf{94.9} & 99.3 & \textbf{96.1} & 98.8 & \textbf{94.9} & 99.8 & \textbf{99.0} & 99.2 & \textbf{96.2}\\
\midrule
\multirow{4}{*}{Dolly-v2} & RoBERTa & \textbf{100.0} & \textbf{86.9} & \textbf{100.0} & 50.0 & \textbf{100.0} & 57.4 & \textbf{100.0} & 50.0 & \textbf{100.0} & 61.1 \\
& LR-GLTR & 90.5 & 70.5 & 94.5 & 69.1 & 89.8 & 64.5 & 90.4 & 66.5 & 91.3 & 67.7 \\
& DNA-GPT & 68.0 & 61.5 & 67.5 & 66.1 & 87.7 & 82.3 & 64.9 & 57.7 & 72.0 & 66.9 \\
& ExaGPT & 85.8 & 78.4 & 96.2 & \textbf{90.8} & 94.4 & \textbf{87.0} & 85.2 & \textbf{76.9} & 90.4 & \textbf{83.3} \\
\bottomrule
\end{tabular}
\caption{Comparison of detection performances of ExaGPT and baseline detectors on texts from various domains and generators. \emph{ACC.} indicates the detection accuracy at 1\% FPR. \emph{Avg.} indicates the average performance within each row across domains. \textbf{Bold} indicates the best performance within each column for each combination of domains and generators.}
\label{detection_performance_result}
\end{table*}

\subsection{Results}
\label{results}
\paragraph{Detection Interpretability.}
Table \ref{detection_interpretability_result} presents the difference in the accuracy of human judgments on the detection correctness based on evidence across baseline detectors and ExaGPT. The accuracy of human judgments on ExaGPT is relatively higher compared to baseline detectors by up to +13.6 points. This indicates that ExaGPT offers more interpretable evidence than other baselines, helping humans judge the correctness of detections more effectively.
Here, DNA-GPT also offers $n$-gram span overlaps between the target text and the re-generated LLM texts from the truncated part as evidence.
The comparison of the human evaluation score between DNA-GPT and ExaGPT suggests that providing not only simple overlaps but also semantically similar spans contributes to better interpretability.
We further investigate how the similarity between the target span and retrieved spans correlates with the correctness of the detection of ExaGPT in \S\ref{what_makes_exagpt_interpretable}.

\paragraph{Detection Performance.}
Table \ref{detection_performance_result} shows the difference in the detection performance of baseline detectors and ExaGPT across four domains and three generators. The detection performance includes AUROC and the accuracy at 1\% FPR. 
Overall, ExaGPT consistently demonstrates detection performance on par with or better than baseline detectors, including supervised classifiers. Specifically, on accuracy at 1\% FPR, ExaGPT massively outperforms baseline detectors by a large margin of up to +40.9 points. This suggests that ExaGPT is the most effective detector in practical scenarios, where there is a need to minimize the number of false positives.


In summary, ExaGPT achieved both superior interpretability of the detection and exceptional detection performance compared to previous interpretable detectors.

\begin{figure}[t]
 \begin{center}
  \centering\includegraphics[width=\columnwidth]{figures/reliable_score_variation.png}
  \caption{Reliability score distributions of long spans (\textit{n} $\geq$ 10) in correct and incorrect samples of ExaGPT, respectively.}
  \label{reliable_score_variation}
 \end{center}
\end{figure}

\section{Analysis}
% In this section, we explore the reason behind the high interpretability of ExaGPT. We specifically focus on the span length and the semantic similarity between each target span and retrieved spans, which are prioritized in the span selection.
% Furthermore, we investigate the robustness of the detection performance of ExaGPT with respect to the interpolation coefficient $\alpha$ and the size of the datastore, respectively. 
% we explore the reason behind the high interpretability of ExaGPT, specifically in terms of the proximity between the target span and retrieved \textit{k}-NN spans.

\subsection{What Makes ExaGPT Interpretable}
\label{what_makes_exagpt_interpretable}
Our human evaluations demonstrate that ExaGPT provides highly interpretable evidence for its detection compared to prior detectors. To explore the reason for this, we investigated the difference in the characteristics of the selected spans as a final output between correct and incorrect predictions by ExaGPT.
Specifically, we focused on span length and mean similarity between each target span and the retrieved spans (reliability score $R$), which are prioritized in the span selection. We randomly selected 1,000 correct and 1,000 incorrect ExaGPT predictions on our test splits across all combinations of domains and generators. 

Figure \ref{reliable_score_variation} presents the reliability score distributions of long spans (\textit{n} $\geq$ 10) in the correct and in the incorrect samples. 
A rightward shift indicates that correct samples of ExaGPT include more long spans with higher reliability scores than incorrect ones. 
From the shift, we empirically observe that offering long spans with high reliability scores helps users judge the correctness of the detections.
Table~\ref{knn_spans} presents examples of long spans (\textit{n} $=$ 19) with high reliability scores for a target span retrieved by ExaGPT.
We can see that the retrieved spans are well-balanced, and are lexically and semantically similar to the target span.

% \begin{table*}[t]
% \caption{Examples of retrieved spans for a target span retrieved by ExaGPT.}
% \centering
% \small
% \setlength{\tabcolsep}{2pt} % Default value: 6pt
% \renewcommand{\arraystretch}{0.7} % Default value: 1
% \begin{tabular}{llcc}
% \toprule
% \textbf{Target Span} & \textbf{\textit{k}-NN Spans} & \textbf{Similarity} & \textbf{Original Label} \\
% \midrule
% \multirow{12.5}{*}{\texttt{\shortstack{published in 1993. The novel tells\\the story of a young Jewish slave, Hadassah, }}} & \shortstack{and was first published in 1936. The book tells the story of three orphaned sisters,} & 0.92 & \texttt{LLM}\\
% \cmidrule(lr){2-4}
%  & \shortstack{published in 2012. The novel revolves around the story of a young woman} & \multirow{2}{*}{0.92} & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{and published in 2010. The novel tells the story of Michael Beard, a} & 0.9 & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{ling of the biblical book, Song of Solomon, and is considered one of the} & 0.9 & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{man and published in 1963. The book was later adapted into a Disney film of the} & 0.9 & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{. The film tells the story of a young} & 0.9 & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{the Xanth series. It is the second book of a trilogy beginning with Vale of the} & 0.89 & Human\\
%  \cmidrule(lr){2-4}
%  & \shortstack{published in 1959. The novel is set in the Arctic region and follows the story of Dr.} & 0.89 & LLM\\
%  \cmidrule(lr){2-4}
%  & \shortstack{. It is the third novel in the Dahak trilogy, after the de} & 0.89 & Human\\
%  \cmidrule(lr){2-4}
%  & \shortstack{for his semi-autobiographical novel, "The Watch that Ends the Night". Born in} & 0.89 & LLM\\
% \bottomrule
% \end{tabular}
% \label{knn_spans}
% \end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.3} % Default value: 1
\begin{tabular}{c|c|l}
\toprule
\textbf{Target Span} & \llmmarker{\texttt{LLM}} & published in 1993. The novel tells the story of a young Jewish slave, Hadassah, \\
\midrule
\multirow{10}{*}{\textbf{\textit{k}-NN Spans}} & \llmmarker{\texttt{LLM (0.92)}} & and was first published in 1936. The book tells the story of three orphaned sisters, \\
 & \llmmarker{\texttt{LLM (0.92)}} & published in 2012. The novel revolves around the story of a young woman \\
 & \llmmarker{\texttt{LLM (0.90)}} & and published in 2010. The novel tells the story of Michael Beard, a\\
 & \llmmarker{\texttt{LLM (0.90)}} & ling of the biblical book, Song of Solomon, and is considered one of the\\
 & \llmmarker{\texttt{LLM (0.90)}} & man and published in 1963. The book was later adapted into a Disney film of the\\
 & \llmmarker{\texttt{LLM (0.90)}} & . The film tells the story of a young\\
 & \humanmarker{\texttt{Human (0.89)}} & the Xanth series. It is the second book of a trilogy beginning with Vale of the\\
 & \llmmarker{\texttt{LLM (0.89)}} & published in 1959. The novel is set in the Arctic region and follows the story of Dr. \\
 & \humanmarker{\texttt{Human (0.89)}} & . It is the third novel in the Dahak trilogy, after the de \\
 & \llmmarker{\texttt{LLM (0.89)}} & for his semi-autobiographical novel, ``The Watch that Ends the Night''. Born in \\
\bottomrule
\end{tabular}
\caption{Examples of $k$-NN spans for a target span retrieved by ExaGPT. The colored part represents the original label for each span (\texttt{LLM} in \textcolor{blue}{blue} and \texttt{Human} in \textcolor{red}{red}, respectively). In the part of $k$-NN spans, the similarity between the target span and each $k$-NN span is added.}
\label{knn_spans}
\end{table*}



\subsection{Impact of \texorpdfstring{$\alpha$}{Lg}}
\label{alpha}
In our experiments, we determined the optimal interpolation coefficient $\alpha$ of ExaGPT (as used in Equation \ref{alpha_formulation}), where it exhibits the best detection performance on our validation split. To investigate the robustness of ExaGPT against the choice of $\alpha$, we examine the detection performance variation according to the multiple choices of $\alpha$.

Figure \ref{alpha_effect} depicts the relationship between $\alpha$ and the detection performance of ExaGPT across four domains and three generators: $\alpha$ ranges between 0.0 and 1.0 with 0.125 intervals, and we observe that the higher the $\alpha$, the lower the detection performance. This implies that taking the reliability score more into account (i.e.,~selecting target spans that are more similar to spans in the datastore) can improve detection performance.
On the other hand, across four domains, the lowest performance of AUROC and accuracy at 1\% FPR are 98.5\% and 93.4\%, respectively. This suggests that the variation of $\alpha$ in ExaGPT does not lead to its substantial performance drop that could greatly affect the performance ranking of detectors. We find similar overall trends of the impact of $\alpha$ for other LLMs, including GPT-4 and Dolly-v2 as generators. The impact of $\alpha$ on detection performance of ExaGPT in all generators can be found in Appendix \ref{analysis_details}.

\begin{figure}[t]
 \begin{center}
  \centering\includegraphics[width=\columnwidth]{figures/alpha_chatgpt.png}
  \caption{Impact of $\alpha$ on the detection performance of ExaGPT, including the AUROC and the accuracy at 1\% FPR, across four domains using ChatGPT as a generator.}
  \label{alpha_effect}
 \end{center}
\end{figure}


\subsection{Impact of the Datastore Size}
\label{datastore}
In our evaluation, ExaGPT leverages our train split as the datastore from which it retrieves top-\textit{k} similar spans for each span in a target text. To explore the robustness of ExaGPT against the size of the datastore, we examine the detection performance variation according to various sizes of the datastore. Specifically, our train split contains 2,000 pairs of human-written and LLM-generated texts. We randomly sample \{500, 1,000, 1,500, 2,000\} pairs from our train split as datastores of different sizes.

Figure \ref{datastore_size_effect_chatgpt} presents the relationship between the datastore size and the detection performance of ExaGPT across four domains using ChatGPT as a generator. Overall, we find that the larger the size of the datastore, the higher the detection performance. Interestingly, we also observe that even when the datastore size is limited to 500 pairs, the detection performance remains quite strong. Particularly, the detection accuracy at 1\% FPR is at least 94.5\% across the four domains, outperforming all other baselines in Table~\ref{detection_performance_result}.
See Appendix \ref{analysis_details} for consistent trends in all generators, including GPT-4 and Dolly-v2.


\begin{figure}[t]
 \begin{center}
  \centering\includegraphics[width=\columnwidth]{figures/datastore_chatgpt.png}
  \caption{Effect of the datastore size on the detection performance of ExaGPT, including the AUROC and the accuracy at 1\% FPR, across four domains using ChatGPT as a generator.}
  \label{datastore_size_effect_chatgpt}
 \end{center}
\end{figure}

\section{Related Work}
% In this section, we briefly walk through the landscape of LLM-generated text detection algorithms and demonstrate the current situation of interpretability in its decision.
\paragraph{LLM-Generated Text Detection.} 
Prior studies have presented various types of detection algorithms for LLM-generated texts.
They primarily fall into three categories: \emph{text watermarking}, \emph{metrics-based}, and \emph{supervised classifiers}.
Text watermarking is a detection approach by calculating the ratio of secret tokens in a target text.
Such tokens are randomly selected by a hash function, and their probabilities are intentionally increased at each time step during the LLM decoding process \cite{kirchenbauer2023watermark}.
% Our work only targets non-watermarked LLMs that are mainly for our daily use.
The metrics-based methods mainly catch the probabilistic discrepancy of a text with the predicted distribution of LLMs.
These metrics include token log probabilities \cite{gehrmann2019gltr}, token ranks \cite{solaiman2019release,su2023detectllmleveraginglogrank}, entropy \cite{ent08}, perplexity \cite{bere16,hans2024spotting}, and negative curvature of perturbed text probabilities \cite{mitchell2023detectgpt,bao2024fastdetectgptefficientzeroshotdetection}.
The supervised classifiers are basically models specifically fine-tuned to discern human-written and LLM-generated texts with labeled datasets. The classifiers vary from probabilistic \cite{ippolito-etal-2020-automatic,crothers2023machinegeneratedtextcomprehensive} to neural methods \cite{uchendu-etal-2020-authorship,rodriguez-etal-2022-cross,guo2023close}. 


\paragraph{Interpretability of the Detection Results.}
\label{interpretable_detectors}
To minimize the undesired consequences of LLM detection (e.g., undermining student's academic dignity), there is need to develop an LLM detector that provides interpretable evidence for the decision. 
While most detectors output only binary predicted labels, there have been a few studies aiming to provide interpretable evidence.
\citet{gehrmann2019gltr} built a detection tool (called GLTR) that color-highlights tokens in a text with high likelihood under the predicted distribution of LMs. 
% While this approach provides users some insight into detecting the text, such probabilistic distribution is unfamiliar to non-expert users.
\citet{mitrović2023chatgpt,wang-etal-2024-m4} used explainable machine learning methods, such as LIME \cite{lime16} and SHAP \cite{shap17}, to supervised classifiers. Both explanation approaches basically apply random perturbations to a text and estimate the contribution of each feature to the decision based on the prediction shift. 
\citet{yang2023dnagptdivergentngramanalysis} presented DNA-GPT, a detection method by examining the average ratio of overlapped $n$-gram spans between a truncated target text and multiple LLM-generated continuations.
This approach can provide actual LLM-generated texts, including $n$-gram overlaps with the target text as evidence of the detection.
% LLM文とのハードな一致しか見ていない（ただこれって解釈性という観点での指摘になってるか？？）

Unlike prior interpretable detectors, our ExaGPT is grounded by the human decision-making process \cite{maurer06,barron-cedeno-etal-2013-plagiarism} of verifying the origin of a text and can provide more interpretable evidence, as explained in the previous sections.
% Specifically, ExaGPT compares a text with both human-written and LLM-generated texts from a datastore and investigates which class the text shares more similar spans with.
% As evidence of the detection, it provides similar span examples for each span in the text.
% The proximity of the span examples to each span in the text can help users judge how reliably correct the detection result is.

% When humans verify the origin of a text, they intuitively compare the text to other human-written and LLM-generated texts and investigate with which source it shares overlaps or rephrased similar spans \cite{maurer06,barron-cedeno-etal-2013-plagiarism}.
% Here, current detectors are not aligned with the human decision-making process and fail to present sufficiently interpretable evidence.
% Our work bridges this gap by proposing ExaGPT, an interpretable detection approach based on the human process of verifying the origin of a text.
% It compares the text with both human-written and LLM-generated texts from a datastore and investigates which class the text shares more similar spans with. Moreover, it can provide similar span examples for each span in the text as evidence. 
% The proximity of the retrieved spans to each span in the target text can help users judge how reliably correct the detection result is.

\paragraph{Example Retrieval for Interpretability.}
Beyond the field of LLM text detection, presenting retrieved similar examples has contributed to improving the interpretability of models in various natural language processing tasks.
These tasks range from text generation, e.g.,~machine translation \cite{khandelwal2020nearest}, to sequential text classification, e.g.,~part-of-speech tagging \cite{wiseman-stratos-2019-label}, named entity recognition \cite{jurafsky2020proceedings}, and grammatical error correction \cite{kaneko-etal-2022-interpretability}.
At each time step, these methods predict a token or a label from the output distribution of a base model interpolated with the distribution derived from retrieved nearest neighbor examples.

Our work has a similar direction of using retrieved similar examples for better interpretability with prior studies in other NLP tasks. In LLM text detection, it is particularly crucial to segment the target text into $n$-gram spans for better interpretability, with labels assigned individually \cite{cheng2025beyond}. Thus, ExaGPT offers a unique mechanism that retrieves similar span examples for each $n$-gram span in the target text and optimizes the final span segmentation based on the examples using dynamic programming.

\section{Conclusion and Future Work}
% The rapid proliferation of texts generated by machines has sparked the various misuses of LLMs, including misinformation, spam, and plagiarism. Although various powerful LLM-generated-text detectors have been developed to prevent such harm, there are often edge cases where detectors misclassify human-written texts as LLM-generated and vice versa in practice. Such misidentification could lead to undesired consequences, such as content writers losing their jobs or students compromising their academic dignity. We thus build interpretable detectors where users can judge how reliably correct the detection results are based on the evidence provided.
% Here, when humans verify whether a text is human-written or LLM-generated, they intuitively investigate which it shares more verbatim overlaps or semantically similar spans.

We introduced ExaGPT, an interpretable human vs. machine detection approach grounded in the human decision-making process of verifying the origin of a text. In particular, ExaGPT classifies a text by examining whether it shares more verbatim and semantically similar spans with human-written vs. with LLM-generated texts from an available datastore. As evidence of the detection, ExaGPT offers similar span examples for each span in the text.
The human evaluation and further analysis show that providing similar span examples allows users to judge the correctness of the detection more effectively than prior interpretable detectors.
Moreover, extensive experiments in various domains and generators revealed that ExaGPT has shown notably superior detection performance compared to previous strong detectors, even at a false positive rate of 1\%. 
These results indicate that ExaGPT is a detector with both high interpretability in its decision and high detection performance.

Our work focused on the in-domain setting where the domain and the generator are the same between the target text and the datastore.
In future work, we plan to investigate the behavior of ExaGPT in cross-domain and cross-generator settings. This will lead to identifying common distinctive features of LLMs across different domains and different generators.

\section{Limitations}
% In this section, we will discuss the recognized limitations of our study.

\paragraph{Inference Cost.}
ExaGPT includes a mechanism for retrieving similar spans with each target span from a datastore.
In our experiments, the datastore consists of $n$-gram spans ($1 \leq n \leq 20$) from a pair of 2,000 human-written and 2,000 LLM-generated texts.
We used four NVIDIA A6000 GPUs to perform the detection within a reasonable time by searching through a vast number of the span instances, which is relatively costly.
We could reduce this cost a bit by decreasing the size of the datastore without sacrificing the detection performance (as explained in \S\ref{datastore}).

\paragraph{Bias in the Human Judgments.}
Human judgments always carry the risk of subjectivity. Moreover, our evaluation of detector interpretability involves four participants, all of whom are familiar with natural language processing, but in reality, most detector users would not have such expertise. This should be taken into account when interpreting our evaluation results on interpretability.

\section{Ethics and Broader Impact}

\paragraph{Human Subject Considerations.}
In our study, human subjects are engaged in identifying the correctness of the detection based on evidence. All annotators provided informed consent, were fully aware of the study's objectives, and had the right to withdraw at any time. 

\paragraph{Transparency and Reproducibility.}
To promote open research, we release our code and data to the public, including all human annotations.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
% \onecolumn
\section{Detailed Configurations of Baselines}
\label{configurations}
\paragraph{LR-GLTR.}
Following the setting of \cite{wang-etal-2024-m4}, we leverage the two categories of GLTR features: (1) the number of tokens in the top-\{10, 100, 1,000, 1,000+\} ranks in the predicted probability distribution of LLMs (four features), and (2) the probability distribution of the word divided by the maximum probability of any word at the same position over 10 bins between 0.0 and 1.0 (ten features).

\section{Detection Evidence of Baselines}
\label{evidence_baselines}
\paragraph{RoBERTa with SHAP.} 
Figure \ref{shap} depicts an example of evidence by RoBERTa with SHAP.
We visualize the evidence using the SHAP library\footnote{\url{https://shap.readthedocs.io/}}.
Overall, the red parts are spans that contribute to predicting LLM-generated. The blue parts are spans that contribute to predicting human-written. In the evidence, if the prediction value, $f({\rm inputs})$ moves further to the right compared to the base value (the expected value across all data samples), it is more likely to be LLM-generated.
When we hover over a colored part, we can also see a score of how much the part contributes to the detection result. The more a span contributes to the decision, the darker its color.

\paragraph{LR-GLTR.}
Figure \ref{gltr} displays an example of evidence by LR-GLTR.
We leverage a demo app\footnote{\url{http://demo.gltr.io/client/index.html}} of GLTR, provided by \citet{gehrmann2019gltr}. 
It highlights tokens in different colors based on their rank of top-\{10, 100, 1,000, 1,000+\} in the predicted token distribution from an LLM. The higher the rank of the token, the more likely an LLM is to generate the token.
The green parts are spans that an most likely LLM-generated. The degree decreases in the order of green, yellow, red, and purple.
When we hover a cursor on a colored part, we can also see the predicted token distribution of an LLM.

\paragraph{DNA-GPT.}
Figure \ref{dnagpt} shows an example of evidence by DNA-GPT.
We implemented a demo app of DNA-GPT with the streamlit framework\footnote{\url{https://github.com/streamlit/streamlit}}.
It shows overlapped $n$-gram spans between a truncated target text and multiple LLM-generated continuations.
The more blue spans, the more likely the text is LLM-generated. For span matching, we follow the original implementation of DNA-GPT\footnote{\url{https://github.com/Xianjun-Yang/DNA-GPT}} where it was achieved by token-level matching based on preprocessing of the lower casing and stemming. We also set $n$ to 8 in order to show a large number of overlapped spans enough to interpret as evidence.

\section{Analysis Details}
\label{analysis_details}
\paragraph{Impact of \texorpdfstring{$\alpha$}{Lg}.}
Figure \ref{alpha_effect_all} showcases the impact of $\alpha$ on the detection performance of ExaGPT across four domains and three generators.
We found similar overall trends of the impact of $\alpha$ in other LLMs, including GPT-4 and Dolly-v2, with the impact in ChatGPT, as explained in \S\ref{alpha}.

\paragraph{Impact of the Datastore Size.}
Figure \ref{datastore_size_effect_all} showcases the impact of the datastore size on the detection performance of ExaGPT across four domains and three generators.
We can observe similar overall trends of the impact of datastore size in other LLMs, including GPT-4 and Dolly-v2, with the impact in ChatGPT as explained in \S\ref{datastore}.

\section{Computational Budget}
We run all the experiments with two AMD EPYC 7453 CPUs and four NVIDIA A6000 GPUs. The total processing time is approximately 25 hours.


\begin{figure*}[t]
 \begin{center}
  \centering\includegraphics[width=0.95\textwidth]{figures/roberta_shap.png}
  \caption{Example of evidence by RoBERTa with SHAP.}
  \label{shap}
 \end{center}
\end{figure*}

\begin{figure*}[t]
 \begin{center}
  \centering\includegraphics[width=0.95\textwidth]{figures/gltr.png}
  \caption{Example of evidence by LR-GLTR.}
  \label{gltr}
 \end{center}
\end{figure*}

\begin{figure*}[t]
 \begin{center}
  \centering\includegraphics[width=0.95\textwidth]{figures/dna_gpt.png}
  \caption{Example of evidence by DNA-GPT.}
  \label{dnagpt}
 \end{center}
\end{figure*}


\begin{figure*}[t]
 \begin{center}
  \centering\includegraphics[width=0.95\textwidth]{figures/alpha_all_models.png}
  \caption{Impoact of $\alpha$ on the detection performance of ExaGPT, including the AUROC and the accuracy at 1\% FPR, across four domains and three generators.}
  \label{alpha_effect_all}
 \end{center}
\end{figure*}


\begin{figure*}[t]
 \begin{center}
  \centering\includegraphics[width=0.95\textwidth]{figures/datastore_all_models.png}
  \caption{Impact of the datastore size on the detection performance of ExaGPT, including the AUROC and the accuracy at 1\% FPR, across four domains and three generators.}
  \label{datastore_size_effect_all}
 \end{center}
\end{figure*}

\end{document}
