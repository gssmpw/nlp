\section{Related Work}
% In this section, we briefly walk through the landscape of LLM-generated text detection algorithms and demonstrate the current situation of interpretability in its decision.
\paragraph{LLM-Generated Text Detection.} 
Prior studies have presented various types of detection algorithms for LLM-generated texts.
They primarily fall into three categories: \emph{text watermarking}, \emph{metrics-based}, and \emph{supervised classifiers}.
Text watermarking is a detection approach by calculating the ratio of secret tokens in a target text.
Such tokens are randomly selected by a hash function, and their probabilities are intentionally increased at each time step during the LLM decoding process ____.
% Our work only targets non-watermarked LLMs that are mainly for our daily use.
The metrics-based methods mainly catch the probabilistic discrepancy of a text with the predicted distribution of LLMs.
These metrics include token log probabilities ____, token ranks ____, entropy ____, perplexity ____, and negative curvature of perturbed text probabilities ____.
The supervised classifiers are basically models specifically fine-tuned to discern human-written and LLM-generated texts with labeled datasets. The classifiers vary from probabilistic ____ to neural methods ____. 


\paragraph{Interpretability of the Detection Results.}
\label{interpretable_detectors}
To minimize the undesired consequences of LLM detection (e.g., undermining student's academic dignity), there is need to develop an LLM detector that provides interpretable evidence for the decision. 
While most detectors output only binary predicted labels, there have been a few studies aiming to provide interpretable evidence.
____ built a detection tool (called GLTR) that color-highlights tokens in a text with high likelihood under the predicted distribution of LMs. 
% While this approach provides users some insight into detecting the text, such probabilistic distribution is unfamiliar to non-expert users.
____ used explainable machine learning methods, such as LIME ____ and SHAP ____, to supervised classifiers. Both explanation approaches basically apply random perturbations to a text and estimate the contribution of each feature to the decision based on the prediction shift. 
____ presented DNA-GPT, a detection method by examining the average ratio of overlapped $n$-gram spans between a truncated target text and multiple LLM-generated continuations.
This approach can provide actual LLM-generated texts, including $n$-gram overlaps with the target text as evidence of the detection.
% LLM文とのハードな一致しか見ていない（ただこれって解釈性という観点での指摘になってるか？？）

Unlike prior interpretable detectors, our ExaGPT is grounded by the human decision-making process ____ of verifying the origin of a text and can provide more interpretable evidence, as explained in the previous sections.
% Specifically, ExaGPT compares a text with both human-written and LLM-generated texts from a datastore and investigates which class the text shares more similar spans with.
% As evidence of the detection, it provides similar span examples for each span in the text.
% The proximity of the span examples to each span in the text can help users judge how reliably correct the detection result is.

% When humans verify the origin of a text, they intuitively compare the text to other human-written and LLM-generated texts and investigate with which source it shares overlaps or rephrased similar spans ____.
% Here, current detectors are not aligned with the human decision-making process and fail to present sufficiently interpretable evidence.
% Our work bridges this gap by proposing ExaGPT, an interpretable detection approach based on the human process of verifying the origin of a text.
% It compares the text with both human-written and LLM-generated texts from a datastore and investigates which class the text shares more similar spans with. Moreover, it can provide similar span examples for each span in the text as evidence. 
% The proximity of the retrieved spans to each span in the target text can help users judge how reliably correct the detection result is.

\paragraph{Example Retrieval for Interpretability.}
Beyond the field of LLM text detection, presenting retrieved similar examples has contributed to improving the interpretability of models in various natural language processing tasks.
These tasks range from text generation, e.g.,~machine translation ____, to sequential text classification, e.g.,~part-of-speech tagging ____, named entity recognition ____, and grammatical error correction ____.
At each time step, these methods predict a token or a label from the output distribution of a base model interpolated with the distribution derived from retrieved nearest neighbor examples.

Our work has a similar direction of using retrieved similar examples for better interpretability with prior studies in other NLP tasks. In LLM text detection, it is particularly crucial to segment the target text into $n$-gram spans for better interpretability, with labels assigned individually ____. Thus, ExaGPT offers a unique mechanism that retrieves similar span examples for each $n$-gram span in the target text and optimizes the final span segmentation based on the examples using dynamic programming.