\section{Related Works}
\label{Related Works}
\subsection{Classical Methods}
Classical vision-based navigation systems typically employ a sequential pipeline that partitions perception, mapping, planning, and control into separate modules~\cite{tordesillas2021faster, zhou2021raptor}. The workflow begins by converting depth images from onboard cameras into 3D point clouds, which are then aggregated to form volumetric representations such as occupancy grid maps or euclidean signed distance fields (ESDFs)~\cite{zhou2021raptor}. Next, collision-free trajectories are generated using trajectory-optimization methods, and finally, these trajectories are executed via closed-loop control~\cite{lee2010geometric, Falanga2018}. 

While this modular architecture is straightforward and interpretable, it introduces several significant drawbacks. Discretization artifacts arise due to the finite resolution of grid-based maps, leading to reduced map fidelity. These issues are further exacerbated during high-speed maneuvers, where increased pose-estimation errors can degrade accuracy. Furthermore, the sequential nature of the pipeline imposes cumulative latency, limiting its responsiveness in dynamic and time-critical scenarios. These challenges highlight the need for alternative approaches to improve navigation performance under such conditions.

\subsection{Imitation Learning}
Learning-based methods have emerged as a promising alternative to address the limitations of classical vision-based navigation systems. Unlike module-based methods, learning-based methods focused on generating trajectories or control commands directly from raw image inputs without explicit perception, mapping, and planning modules~\cite{loquercio2021learning, gandhi2017learning, loquercio2018dronet, nguyen2024uncertainty}. One of the most widely-used imitation learning approaches is behavior cloning (BC). BC is popular due to its straightforward implementation and high sample efficiency. However, BC training requires high quality datasets. Studies such as~\cite{gandhi2017learning, loquercio2018dronet} collected datasets in real-world environments, while others, including~\cite{loquercio2021learning, nguyen2024uncertainty}, utilized synthesized data from simulation environments for training.

While BC policies can perform well when high-quality datasets are available, they often suffer from compounding errors and distributional shifts due to overfitting to specific scenarios. To address this, ~\cite{loquercio2021learning} applied the DAgger method, which collects additional expert data in unseen states during training. However, this method incurs high costs and is challenging to implement in real-time scenarios where an oracle expert is unavailable.

Another approach extends imitation learning by leveraging privileged information during training to directly optimize the cost associated with generated trajectories, thus training a path generation model~\cite{lu2023lpnet, lu2024you, yang2023iplanner}. For instance, studies such as~\cite{lu2023lpnet, lu2024you} calculate Q-functions based on map data to update policies without explicit labeling. On the other hand,~\cite{yang2023iplanner} employs a differentiable cost map to optimize trajectory quality directly, without relying on Q-function learning or reinforcement signals. This method focuses on efficient optimization of path generation under given constraints and can be effective as it operates without explicit labeled data. However, it still faces challenges such as reliance on the quality of the cost map and computational overhead, which may limit its scalability.

\subsection{Reinforcement Learning}
Reinforcement learning (RL) has demonstrated remarkable results across various domains and has shown promise even in challenging fields such as drone visual navigation. Recent studies have explored end-to-end learning approaches that utilize visual data to directly generate low-level control commands~\cite{yu2024mavrl, bhattacharya2024vision, xing2024bootstrapping,  song2023learning, kulkarni2024reinforcement}.

However, RL methods that rely on raw visual information often suffer from slow convergence and require a large amount of data to train. Moreover, the design of effective reward functions poses a significant challenge, as it requires careful consideration to ensure alignment with the desired behaviors and to avoid unintended consequences. These limitations necessitate powerful parallel simulation environments capable of providing diverse state information to train robust policies for various environments~\cite{makoviychuk2isaac, song2021flightmare, kulkarni2023aerial}. Despite these advancements, training vision-based RL policies remains a challenging task, prompting researchers to propose alternative methods to address these difficulties.

For instance, Xing et al.~\cite{xing2024bootstrapping} employed a DAgger-based policy as a foundation and refined it using RL for effective state embedding. Song et al.~\cite{song2023learning} introduced a framework where a state-based RL policy is first trained, followed by knowledge distillation to transfer the knowledge into a vision-based RL policy. Similarly, Bhattacharya et al.~\cite{bhattacharya2024vision} developed a neural network combining vision transformers (ViT) with LSTM to achieve efficient state embeddings.

In drone racing, policies based on raw pixel data have also been investigated~\cite{song2023reaching, geles2024demonstrating}. While these methods demonstrate promising results in constrained racing track environments, their applicability to more diverse and unstructured scenarios, such as natural or urban environments, has not been fully established. Consequently, developing RL methods that effectively utilize raw visual inputs while ensuring robust generalization and fast convergence remains an open and significant research challenge.

\subsection{Inverse Reinforcement Learning}
Inverse reinforcement learning (IRL) aims to find a proper reward from expert samples. IRL is particularly beneficial in applications where reward design is difficult, yet its adaptation to vision-based tasks remains a significant challenge~\cite{ho2016generative}. While some studies have successfully applied IRL to autonomous driving~\cite{lee2021approximate, liu2022improved}, its application to drones is still unexplored. Compared with autonomous driving, autonomous drone navigation is more demanding as it involves 3D spatial awareness and attitude control-including pitch, roll, and yaw-thus making policy learning considerably more complex. Furthermore, drone navigation necessitates highly accurate action generation as the drone is at significant risk of crashing if the action is not generated with sufficient precision. This challenge becomes even greater in high-speed drone flight~\cite{arora2021survey}, where raw sensory visual data exacerbate the difficulty of reward shaping and policy learning. Moreover, drones are highly sensitive to external factors such as wind disturbances, sensor noise, and limited onboard computational resources, adding further complexity to the effective use of IRL. Consequently, direct application of IRL to vision-based high-speed drone flight remains as significant challenge.