\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Training-free Task-oriented Grasp Generation}
\author{\IEEEauthorblockN{Wang Jiaming}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{National University of Singapore}\\
jiaming@comp.nus.edu.sg}
\and
\IEEEauthorblockN{Chen Jizhuo}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{National University of Singapore}\\
e0774920@u.nus.edu}
\and
\IEEEauthorblockN{Liu Diwen}
\IEEEauthorblockA{\textit{School of Computing} \\
\textit{National University of Singapore}\\
e0905370@u.nus.edu}
}


\maketitle
\begin{abstract}
This paper presents a training-free pipeline for task-oriented grasp generation that combines pre-trained grasp generation models with vision-language models (VLMs). Unlike traditional approaches that focus solely on stable grasps, our method incorporates task-specific requirements by leveraging the semantic reasoning capabilities of VLMs. We evaluate five querying strategies, each utilizing different visual representations of candidate grasps, and demonstrate significant improvements over a baseline method in both grasp success and task compliance rates, with absolute gains of up to 36.9\% in overall success rate. Our results underline the potential of VLMs to enhance task-oriented manipulation, providing insights for future research in robotic grasping and human-robot interaction.
\end{abstract}

% \begin{IEEEkeywords}
% Task-oriented Grasp generation, Vision Language Models, Robotics, AI
% \end{IEEEkeywords}

\section{Introduction}

Several grasp generation methods have been introduced in recent research \cite{fang2023anygrasp, mousavian20196, mahler2017dex, sundermeyer2021contact}, with most focusing on achieving stable, collision-free grasps. While stability and collision avoidance are critical, human grasping behavior is inherently task-driven. For humans, grasping is not just about securing an object safely but about fulfilling a specific goal. For instance, when picking up a cup to drink, a person typically grasps it by the handle, even though other stable, collision-free grasps — like holding the cup’s body — are possible. This task-oriented approach highlights that grasp selection is based on the intended use of the object—an idea not fully embraced by traditional robotic grasping techniques.

To enable service robots to effectively perform a wide variety of tasks in real-world environments, it is crucial that they learn to predict task-oriented grasps for a diverse range of objects. For example, a robot may need to grasp a tool differently depending on whether it is going to use it for cutting, hammering, or passing it to a person. Such task-oriented grasp prediction is essential for improving a robot's functionality in practical applications \cite{murali2021same}.

Training a task-oriented grasp generation policy directly would require an extensive dataset to account for the vast diversity of objects, which presents significant challenges in terms of data collection and cost. Additionally, there are concerns regarding whether such a trained policy would generalize effectively to unseen objects and tasks. Achieving this level of generalization would demand a semantic understanding of both the task and the properties of unfamiliar objects, which is difficult to achieve through training on datasets that consist primarily of grasp annotations.

In contrast, vision-language models (VLMs) \cite{yang2023dawn, li2023m}, which are trained on massive, internet-sourced datasets, have demonstrated remarkable generalization capabilities in understanding both natural language and images. This raises an intriguing question: can VLMs be leveraged to aid in task-oriented grasp generation? Several studies have explored this potential \cite{barad2023graspldm, taunyazov2023grace}. However, these approaches often require additional training steps to generate task-oriented grasps (see Section \ref{review}) and tend to be tightly integrated with a particular grasp generation model. This rigidity limits flexibility and adaptability, making it challenging to incorporate more advanced grasp generation models as they emerge.

In this project, we propose a training-free pipeline for task-oriented grasp generation by combining a pre-trained grasp generation model with a vision-language model (VLM). The key insight is that the pre-trained grasp generation model can be augmented to produce task-oriented grasps by leveraging a VLM to filter or select candidate grasps based on task-specific requirements.

We extensively evaluated and compared different approaches for integrating grasp generation models with VLMs in a simulation environment \cite{gu2023maniskill2}. Our experiments demonstrate that this simple yet effective strategy achieves a 36.9\% absolute improvement in the overall success rate. This study provides valuable insights into how pre-trained models and VLMs can be seamlessly integrated to enhance task-oriented grasping.

The main contributions of this work are as follows:
\begin{itemize}
    \item We propose a simple, flexible, and training-free pipeline for task-oriented grasp generation, along with an automated evaluation system.
    \item We conduct extensive evaluations of different integration approaches, analyze failure modes in detail, and provide insights to guide future research.
\end{itemize}

This report is structured as follows: \begin{itemize} \item Section \ref{review} reviews related work on grasp generation and foundation models in robotics. \item Section \ref{task-oriented-grasp-generation} defines the problem and presents our proposed solution. \item Section \ref{experiment} describes the experimental setup, evaluation methods, and results. \item Section \ref{conclusion} concludes the report and outlines directions for future research. \end{itemize}

\begin{figure*}[tb]  % Use only 'ht' or 'tb'
  \centering
  \includegraphics[width=\textwidth]{overview.eps}
  \caption{Overview of the system. The depth image is unprojected into a point cloud, which is processed by a grasp generation model (e.g., Contact GraspNet) to produce a set of unconditional grasps. Simultaneously, an open-vocabulary object detector identifies the target object from the RGB image, generating a segmentation mask that filters the grasps. The top K grasps are selected based on confidence and further refined by a motion planner to ensure trajectory feasibility. These filtered grasps are then evaluated using a vision-language model (VLM) to select the best grasp for the task.}
  \label{fig:overview}
\end{figure*}

\begin{figure*}[tb]  % Use only 'ht' or 'tb'
  \centering
  \includegraphics[width=\textwidth]{5in1.eps}
  \caption{Illustration of the five query methods used for grasp evaluation. The images sent to the VLM vary based on the query method, differing in how the grasps are visualized: represented either by a fork-shaped mesh or a colored dot, displayed as individual images (one grasp per image) or combined into a single image (all grasps in one image). For the CPG method, only the raw image is sent, and the VLM is asked to indicate the grasping point.}
  \label{fig:visualization_methods}
\end{figure*}

\section{Literature Review} \label{review} \subsection{Non-task-oriented Grasp Generation} 
There exists a substantial body of research on general grasp generation methods. For instance, \cite{mousavian20196} trained a variational autoencoder (VAE) to sample grasps in cluttered scenes, while \cite{urain2023se} utilized diffusion models to capture the multimodal distribution of grasps for a given object. \cite{sundermeyer2021contact} proposed a method that samples dense grasp poses for each possible contact point on an object. Additionally, \cite{fang2023anygrasp} developed a robust grasp generation model trained on noisy real-world data. These approaches successfully generate grasps that avoid collisions and generalize well across a wide variety of objects. However, they do not account for the specific tasks associated with manipulating the object.

\subsection{Task-oriented Grasp Generation} 
In contrast, task-oriented grasp generation focuses on predicting grasps that are optimized for a specific task. Researchers in \cite{murali2021same, jang2017end} introduced data-driven approaches, where large datasets were collected to train neural networks capable of predicting task-oriented grasps. More recently, several studies have explored the integration of large language models (LLMs) and vision language models (VLMs) to enhance task-oriented grasp generation. For example, \cite{tang2023task, li2024semgrasp} leveraged language information alone, while \cite{chang2024text2grasp} required the training of additional grasp generation models. \cite{taunyazov2023grace} employed differentiable classifiers to guide the grasp sampling process; however, while this approach is training-free, it is not directly applicable to more general scenarios where gradient information is not available.

\subsection{Foundation Models in Robotics} 
Building on the success of foundation models, there is growing interest in the use of LLMs and VLMs in various robotic applications. With their large capacities, these models can be trained or fine-tuned to directly predict a sequence of actions given a task description in natural language and RGB observations \cite{liu2024moka, team2024octo, brohan2023rt}. However, training such large models for open-world scenarios typically requires extensive data and supervision.

An alternative approach involves leveraging pre-trained VLMs in combination with lower-level skills to perform open-vocabulary manipulation tasks \cite{weerakoon2024behav, nasiriany2024pivot} or navigation tasks \cite{liu2024moka, wu2024helpful}. Nevertheless, recent studies \cite{ramakrishnan2024does} suggest that foundation models still struggle with spatial reasoning in general. In this work, we aim to investigate whether VLMs can effectively comprehend and reason about the spatial and semantic information related to grasping tasks. Unlike MOKA \cite{liu2024moka}, which assumes a fixed tabletop setup, our approach focuses on task-oriented grasp generation using VLMs. This approach is not constrained to a specific environment, offering a flexible framework that integrates various grasp generation models with VLMs to accommodate a wider range of scenarios. This adaptability enables a more versatile approach to task-oriented manipulations.


\section{Task-oriented Grasp Generation}
\label{task-oriented-grasp-generation}

\subsection{Task Definition}
We address the problem of generating task-oriented grasps for robotic manipulation, given the point cloud of an object and task-oriented constraints expressed in natural language. Our objective is to estimate the grasp distribution \( P(G^* \mid X, T) \), where \( X \) represents the RGB and depth image inputs, \( T \) denotes the task-oriented constraints, and \( G^* \) refers to the set of successful grasps that satisfy both stability and task-oriented requirements. As in prior work \cite{sundermeyer2021contact, fang2023anygrasp}, each grasp \( g \in G^* \) is defined as a grasp pose \( (R, t) \in SE(3) \) for a parallel-jaw gripper.

For the task-oriented constraints \( T \), we consider natural language task descriptions that provide guidance on how the object should be grasped. For instance, given a pair of scissors and the task description ``passing them to someone,'' the robot should grasp the blade area; whereas for the task ``use it,'' the robot should grasp the handle.


\subsection{System Overview}
\label{overview}
Our proposed approach decomposes the estimation of \( P(G^* \mid X, T) \) into two stages: (1) task-agnostic grasp sampling \( P(G^* \mid X) \), and (2) task-oriented grasp evaluation \( P(S \mid X, T, g) \). This factorization allows us to leverage existing work focused on stable grasp generation while incorporating task-oriented constraints to ensure the grasp is suitable for the task at hand. An overview of our proposed system is illustrated in Fig.~\ref{fig:overview}.

Given an RGB and depth image as input, our pipeline begins by processing the depth image to generate a point cloud through unprojection. This point cloud is then input into a pre-trained grasp generation model to produce a set of candidate grasps \( \{G_i\}_{i=1}^N \). For our experiments, we utilized Contact GraspNet \cite{sundermeyer2021contact} due to its robustness. However, our pipeline is flexible and can accommodate other, potentially more advanced, grasp generation models.

Simultaneously, we employ an open-vocabulary object detector \cite{minderer2022simple} to identify the target object from the RGB image, generating a segmentation mask for the detected object. This mask is used to filter the candidate grasps, retaining only those associated with the target object.

Generated grasps are passed to a motion planner, which filters out any grasps that are not feasible due to reachability or collision constraints. This step is performed prior to querying the vision-language model (VLM) to reduce computational overhead.

\begin{figure*}[tb]  % Use only 'ht' or 'tb'
  \centering
  \includegraphics[width=\textwidth]{kmeans_diagram.eps}
  \caption{Illustration of using K-means clustering and scores to select diverse grasps while taking into account for the quality of each grasp.}
  \label{fig:kmeans}
\end{figure*}

\subsection{Grasp Clustering}
\label{clustering}

One limitation of the proposed pipeline is that the pre-filtered top-\(k\) grasp candidates (based on the confidence scores from the grasp generation model) often lack diversity. These candidates typically belong to the same cluster of grasps. For instance, in the case of a screwdriver, most top-rated grasps focus on the midpoint of the handle (Fig. \ref{fig:not_diverse}). While these grasps yield a high grasp success rate, they are less useful for task-oriented grasp generation, which requires a diverse set of grasps that can then be ranked by the Vision-Language Model (VLM).

To address this issue, we generate \(N\) valid grasps (pre-filtered by the motion planner as described earlier in section \ref{overview}) and perform \(k\)-means clustering on the \textbf{contact points} (i.e., the center of the gripper) in Euclidean space. For our experiments, we set \(K=3\). From each cluster, we select the grasp with the highest confidence score as predicted by the grasp generation model. This strategy ensures that the final set of candidate grasps \( \{\bar{G}_1, \bar{G}_2, \bar{G}_3\} \) is both diverse and of high quality (Fig. \ref{fig:kmeans}).

It is worth noting that more sophisticated clustering methods could also be employed. For example, \(k\)-means clustering could be applied to features extracted from a vision foundation model such as DINO \cite{zhang2022dino}. In this case, each cluster would represent grasps that target a similar part of the object, while different clusters would correspond to grasps targeting distinct parts of the object. However, in this work, we use the simpler strategy outlined above, as it empirically yields satisfactory results.


\subsection{Task-Oriented Grasp Filtering by VLM}
\label{vlm-filter}

After generating \(K = 3\) grasp candidates, we query the Vision-Language Model (VLM) to select the grasp most suitable for the specified task. Queries are constructed by combining the grasp candidates with the task description and submitting them to the VLM. The VLM's response is then used to select the best grasp for the task.

We employed five distinct strategies to query the VLM, each differing in how the candidate grasps are visualized and presented:

\begin{enumerate}
    \item \textbf{Grasps in a Single Image (GSI):} All candidate grasps are visualized in a single image, with each grasp represented by the gripper shape at its corresponding pose on the object's point cloud. This composite visualization enables the VLM to compare all grasps directly within one scene.

    \item \textbf{Contact Points in a Single Image (CPSI):} All candidate grasps are visualized as their respective contact points (colored dots) in a single image. The VLM selects the best grasp based on the task description, facilitating direct comparison of contact points in one image.

    \item \textbf{Grasps in Multiple Images (GMI):} Each candidate grasp is presented in a separate image, with the grasp pose depicted by a simplified gripper shape positioned on the object's point cloud. The gripper mesh, generated using the \texttt{trimesh} \cite{trimesh} library, is designed for clarity. The VLM selects the image that contains the grasp best suited for the task.

    \item \textbf{Contact Points in Multiple Images (CPMI):} Each candidate grasp is visualized in a separate image, represented by a colored dot indicating the center of the grasp on an RGB image. This simplified visualization aims to reduce visual complexity for the VLM.

    \item \textbf{Contact Point Generation (CPG):} Instead of providing predefined grasp candidates, we directly query the VLM to suggest a grasping point on the image that best satisfies the task description by specifying pixel coordinates \((u, v)\). To ensure the selected point can be unprojected into 3D space, we explicitly request that the VLM identify a point on the target object. We then choose the 3D point \(P\) whose projection onto the image plane is closest to the \((u, v)\) coordinate. Subsequently, we select the grasp whose contact point is nearest to the 3D point \(P\).
\end{enumerate}

An illustration of these query methods is provided in Fig.~\ref{fig:visualization_methods}.


\begin{table*}[t!]
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method (VLM)} & \textbf{Grasp Success Rate} & \textbf{Task Compliance Rate} & \textbf{Combined Success Rate} \\ \hline
\multicolumn{4}{|l|}{\textbf{Qwen-7B}} \\ \hline
GSI & 0.754 & 0.561 & 0.439 \\ \hline
CPSI & 0.702 & 0.491 & 0.386 \\ \hline
GMI & 0.754 & 0.526 & 0.386 \\ \hline
CPMI & 0.702 & 0.649 & 0.351 \\ \hline
CPG & 0.737 & 0.614 & 0.544 \\ \hline
\multicolumn{4}{|l|}{\textbf{Molmo}} \\ \hline
GSI & 0.649 & 0.544 & 0.421 \\ \hline
CPSI & 0.825 & 0.614 & 0.526 \\ \hline
GMI & 0.737 & 0.579 & 0.351 \\ \hline
CPMI & 0.702 & 0.544 & 0.474 \\ \hline
CPG & 0.754 & 0.649 & 0.456 \\ \hline
\textbf{Baseline} & 0.754 & 0.298 & 0.175 \\ \hline
\end{tabular}

\vspace{0.3cm}
\caption{Experiment results of the proposed pipeline using different VLM query strategies for each VLM.}
\label{table:performance}
\end{table*}

\section{Experiments}
\label{experiment}

We conducted experiments to evaluate our training-free, task-oriented grasp generation pipeline using various vision-language models (VLMs) and querying strategies. Our goal was to assess the effectiveness of integrating pre-trained grasp generation models with VLMs to select grasps that satisfy specific task requirements.

\subsection{Experimental Setup}

Our experiments were performed in the ManiSkill simulator \cite{gu2023maniskill2}, using the Franka Emika Panda robotic arm equipped with a parallel-jaw gripper. We selected 29 objects from the YCB \cite{calli2015ycb} and Google Scanned Objects (GSO) \cite{borja2014googlescan} datasets, ensuring a diverse range of shapes, sizes, and functionalities that align with our predefined tasks. These objects were chosen to fit the specific task scenarios we designed.

For each object, we defined a set of tasks with specific grasping requirements. We formulated five broad task categories, such as ``gripping a specific part of a given tool'' and ``gripping a specific part of a given toy animal.'' Each category was further divided into 2 or 3 sub-tasks, resulting in a total of 14 unique task descriptions. Examples include ``gripping the tail of the toy animal'' and ``gripping the head of the toy animal'' which form a pair of sub-tasks, and ``gripping the handle of the tool to ensure a secure hold'' and ``gripping the tool by a part other than the handle for handing over'', forming another pair of sub-tasks under one of the 5 tasks. (For more details on our definition of the tasks, please refer to Appendix A~\ref{AA}.)

\subsection{Grasp Generation and Selection}

We utilized Contact GraspNet (CGN) \cite{sundermeyer2021contact} to generate a set of candidate grasps for each object. CGN provides grasps along with associated quality scores. A motion planner (\texttt{mplib}) \cite{mplib} was applied to filter out infeasible grasps due to reachability issues or potential collisions. We then followed the steps described in Sections \ref{overview} and \ref{clustering} to generate a final set of diverse, high-quality grasp candidates.

We employed five strategies for querying the VLM, as detailed in Section \ref{vlm-filter}, which differ in how the candidate grasps are visualized and presented. Five corresponding prompts were carefully crafted (fig~\ref{fig:visualization_methods}) to ensure the VLM could effectively interpret the task, reason through the grasp selection process, and provide responses in a consistent and easily parsable format. (For details on the prompts, please refer to Appendix B.)

We experimented with two VLMs:

\begin{itemize}
    \item \textbf{Qwen-7B} \cite{Qwen2VL}: A state-of-the-art vision-language model known for its strong language understanding and visual reasoning capabilities.
    \item \textbf{Molmo} \cite{deitke2024molmo}: A vision-language model designed specifically for robotic applications, including point-based interactions, making it particularly suitable for the PSI strategy.
\end{itemize}

For each VLM, we applied all five query strategies, resulting in a total of ten configurations. While a more powerful VLM (e.g., Qwen-72B) could potentially enhance performance, we used the 7B versions for both models due to computational constraints.

We compared the performance of these ten configurations against a baseline system, which selects the grasp generated by the grasp generation model with the highest confidence score. This baseline does not involve task-specific reasoning or diversity in grasp selection, serving as a straightforward benchmark for evaluating the advantages of the proposed pipeline.


\subsection{Evaluation Metrics}

The performance of our approach was evaluated using the following metrics:

\begin{itemize}
    \item \textbf{Grasp Success Rate:} The percentage of trials in which the robot successfully grasps the object and lifts it above a specified height without dropping it.
    \item \textbf{Task Compliance Rate:} The percentage of trials in which the VLM selects a grasp that aligns with the specific task requirements.
    \item \textbf{Combined Success Rate:} The percentage of trials in which the executed grasp both lifts the object successfully and meets the task-oriented requirements.
\end{itemize}



\subsection{Automated Evaluation}

Manual evaluation of task compliance by humans can be time-consuming. To automate this process, we used a more powerful VLM, Qwen-72B \cite{Qwen2VL},  as an evaluator. This model received the task description, the selected grasp, and the three candidate grasps, then provided an explanation followed by a final judgment of "Yes" or "No" indicating task compliance. The explanations allowed for improved accuracy and traceability of the judgments. (Details on the prompts used for querying Qwen-72B for automated evaluation are provided in Appendix C.)


To validate the reliability of this automated evaluation, we manually reviewed a subset of the results and compared them against Qwen-72B's judgments. The precision of the automated evaluation (i.e., the agreement between Qwen-72B's predictions and human evaluations) is \textbf{0.717}.

The main limitation observed is that while humans rely on videos to evaluate the results, the Qwen-72B model bases its predictions solely on static images highlighting the grasp point. For certain objects, such as water bottles, a top-down view of the grasp point often fails to convey whether the grasp is targeting the handle or another part of the object, leading to ambiguity. To address this, we propose either utilizing a more powerful model or providing more informative images, such as capturing the object from multiple viewpoints.

We observe that the Qwen-72B model achieves a high task compliance prediction accuracy (\(71.69\%\)), though it is not entirely free from errors. Most prediction inaccuracies stem from the aforementioned ambiguities. Consequently, we chose to rely on human evaluations for the final results. We hypothesize that adopting more advanced models (e.g., GPT-4o) or incorporating richer visual information could significantly enhance task compliance prediction, potentially achieving reliability suitable for fully automated evaluation.


\subsection{Experiment Results}

Table~\ref{table:performance} summarizes our experimental results. Notably, our proposed system consistently outperforms the baseline system, achieving an absolute improvement in combined success rate ranging from 17.5\% to 54.4\%, depending on the query strategy. Among these strategies, the Contact Point Generation (CPG) method demonstrates the best performance, surpassing the baseline by 36.9\% in combined success rate. While the CPG method excels in reasoning tasks, gripper shape methods (GSI, GMI) show better grasp success rates, and contact point methods (CPSI, CPMI) perform better in task compliance, reflecting their alignment with task-specific requirements. This demonstrates the importance of tailoring visual and textual strategies to different model strengths for optimal performance.

We observe that the grasp success rate for methods utilizing the VLM is comparable to that of the baseline method, indicating that incorporating the VLM does not compromise grasp quality. Furthermore, the task compliance rate for methods leveraging the VLM is significantly higher than the baseline, demonstrating the effectiveness of our proposed system in aligning grasps with task-specific requirements.

\textbf{Single image vs multiple images}
The results highlight the distinction between single-image and multi-image strategies in grasp evaluation. Single-image approaches, such as GSI and CPSI, generally perform better in task compliance, possibly due to the simplicity of presenting all grasps in one visual context. In contrast, multi-image strategies, like GMI and CPMI, spread grasps across multiple images, which may complicate the VLM's ability to effectively compare them. For Qwen and Molmo, single-image methods achieved higher combined success rates, suggesting they better leverage the VLM’s ability to process integrated visual and textual information.

\textbf{Gripper shape or contact point}
These results clearly reflect the difference in performance with respect to gripper shape versus contact point methods for the evaluation of grasps. Gripper shape methods (GSI and GMI) consistently achieve higher grasp success rates but lag in task compliance rates compared to contact point methods (CPSI and CPMI). This suggests that gripper shape representations provide better spatial understanding for stable grasps, while contact point methods, which simplify visual complexity, are more effective at aligning with task-specific requirements. Notably, CPSI achieves the highest task compliance rate with Molmo, while CPMI performs best in combined success rate, highlighting the importance of tailored visual representation for task-oriented grasps.



\textbf{Comparison of VLM}
In the comparison of the two Vision-Language Models (VLMs), Qwen-7B demonstrates better performance in the Contact Point Generation (CPG) method, where no candidate grasps are explicitly provided. This suggests that Qwen excels at reasoning directly about task-oriented grasping points based solely on spatial and semantic cues without relying on visualized candidates. In contrast, Molmo achieves higher combined success rates in methods that provide candidate visualizations, such as CPSI and CPMI, indicating its strength in interpreting explicit visual grasp candidates. These observations emphasize that while Qwen-7B performs exceptionally well in reasoning tasks, Molmo benefits more from structured and simplified candidate representations for task-oriented grasp evaluation.


\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{diverse_eg.eps}
  \caption{Example where clustering fails to produce diverse grasp candidates, resulting in grasps concentrated in similar regions of the object (at the handle of the screwdriver).}
  \label{fig:not_diverse}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\columnwidth]{amb1.eps}
  \caption{Example of an ambiguous case involving tasking the VLM to select a grasp that grips the lid/body of the bottle while the image given is in top-down view, making it hard for VLM to interpret different grasps.}
  \label{fig:amb1}
\end{figure}

\subsection{Failure Modes}

Although our proposed system demonstrates promising results in task-oriented grasp generation in a training-free manner, several limitations were observed during the experiments:

\textbf{Clustering Method Does Not Provide Sufficiently Diverse Clusters:} The clustering method used in our approach sometimes fails to generate clusters that are diverse enough. As shown in Fig.~\ref{fig:not_diverse}, many grasps from different clusters are still concentrated in similar regions of the object, limiting the effectiveness of the diversity-based selection process.

\textbf{Ambiguous Query Images:} When the query image is captured from certain viewpoints, some objects or parts of objects in the image may appear ambiguous. For example, as illustrated in Fig.~\ref{fig:amb1}, when tasking VLM to select a grasp that grips the lid of the given bottle, it is hard to tell from the image whether the candidate grasps are gripping the lid or the body. This ambiguity can lead to suboptimal grasp suggestions by the VLM.

\textbf{Limitations of the VLM:} The VLM itself has inherent limitations, such as difficulty in understanding complex spatial relationships in 3D scenes or accurately reasoning about physical constraints. Additionally, its responses can sometimes be inconsistent or overly reliant on the specific wording of the task description, which may impact the quality of the selected grasp.

Addressing these failure modes will require improvements in clustering techniques, better handling of viewpoint selection for query images, and leveraging more advanced VLMs with enhanced spatial reasoning and contextual understanding capabilities.


\section{Conclusion}
\label{conclusion}

In this paper, we introduced a training-free pipeline for task-oriented grasp generation that integrates pre-trained grasp generation models with vision-language models (VLMs) to select grasps aligned with specific task requirements. Our approach demonstrated significant improvements over a baseline system, achieving up to a 36.9\% absolute gain in overall success rate. Among the evaluated strategies, the Contact Point Generation (CPG) method showed the best task compliance, while other methods exhibited complementary strengths in grasp success and task-specific alignment. Despite its effectiveness, we identified limitations such as insufficient grasp diversity, query image ambiguities, and VLM constraints, providing directions for future improvements. This study highlights the potential of combining pre-trained grasp models and VLMs for flexible, efficient task-oriented manipulation without additional training, offering a promising framework for advancing robotic grasping capabilities.


\newpage
% \bibliographystyle{plain}
\input{main.bbl}  % Replace "main.bbl" with your actual filename

% \bibliography{ref}

\newpage
\section{Appendix A: Task Descriptions} \label{AA}


\begin{itemize}
    \item \textbf{Task 1: Grip Toys}
    \begin{enumerate}
        \item Grip the toy by the front (head or front paws).
        \item Grip the toy by the tail to keep the head visible.
    \end{enumerate}
    
    \item \textbf{Task 2: Retrieve Objects with Handles}
    \begin{enumerate}
        \item Retrieve the object by gripping the handle to ensure a secure hold.
        \item Retrieve the object by gripping a part other than the handle to make it easier for me to handle.
    \end{enumerate}
    
    \item \textbf{Task 3: Retrieve Tools}
    \begin{enumerate}
        \item Retrieve the tools by gripping the handle to ensure a secure hold.
        \item Retrieve the tools by gripping a part other than the handle to make it easier for me to handle.
    \end{enumerate}
    
    \item \textbf{Task 4: Retrieve Bottles}
    \begin{enumerate}
        \item Retrieve the bottle by gripping the body to allow easy twisting.
        \item Retrieve the bottle by gripping the head part to make it easier for me to handle.
    \end{enumerate}
    
    \item \textbf{Task 5a: Identify and Retrieve Bottles}
    \begin{enumerate}
        \item Among all the bottles here, identify and retrieve the ketchup bottle.
        \item Among all the bottles here, identify and retrieve the milk bottle.
        \item Among all the bottles here, identify and retrieve the honey bottle.
    \end{enumerate}
    
\end{itemize}


\section{Appendix B: Query Strategies and Prompts}
\label{BB}
\subsection*{Gripper One Visualization}
\noindent
\texttt{You are an intelligent system that can specify the best grasping point for a robot to pick up an object from multiple images.}\\
You are given multiple images, each showing the object with a different grasp candidate highlighted in red, and a task description. Your task is to analyze the grasp candidates in each image and choose the index of the grasp that would best accomplish the task. Note that there may be multiple valid grasping points, but you should select the one that is most suitable for the given task. Consider factors like stability, object orientation, and the requirements of the task in your decision. \\
\textbf{Task:} \texttt{\{task\}}\\
Explain your reasoning clearly, then state the chosen grasp index \texttt{\{index\_list[num\_images-1]\}} at the end of your response.

\subsection*{Gripper All Visualization}
\noindent
\texttt{You are an intelligent system that can specify the best grasping point for a robot to pick up an object from a single image.}\\
You are given an image showing the object with different grasp candidates highlighted in red, green, and blue. Your task is to analyze these grasp candidates based on the task description and choose the best one. Note that there may be multiple valid grasping points, but you should select the one that is most suitable for the given task. Consider factors like stability, object orientation, and the requirements of the task in your decision. \\
\textbf{Task:} \texttt{\{task\}}\\
Explain your reasoning clearly, then repeat the chosen color \texttt{\{rgb\_list[num\_images-1]\}} at the end of your response.

\subsection*{Contact One Visualization}
\noindent
\texttt{You are an intelligent system that can specify the best grasping point for a robot to pick up an object from a set of images.}\\
You are given multiple images, each showing the object with a different grasp candidate with a red dot, and a task description. Your task is to analyze the grasp candidates in each image and choose the index of the grasp that would best accomplish the task. Note that there may be multiple valid grasping points, but you should select the one that is most suitable for the given task. Consider factors like stability, object orientation, and the requirements of the task in your decision. \\
\textbf{Task:} \texttt{\{task\}}\\
Explain your reasoning clearly, then state the chosen grasp index \texttt{\{index\_list[num\_images-1]\}} at the end of your response.

\subsection*{Contact All Visualization}
\noindent
\texttt{You are an intelligent system that can specify the best grasping point for a robot to pick up an object from a single image.}\\
You are given an image showing the object with different grasp candidates highlighted in red, green, and blue dots. Your task is to analyze these grasp candidates based on the task description and choose the best one. Note that there may be multiple valid grasping points, but you should select the one that is most suitable for the given task. Consider factors like stability, object orientation, and the requirements of the task in your decision. \\
\textbf{Task:} \texttt{\{task\}}\\
Explain your reasoning clearly, then repeat the chosen color \texttt{\{rgb\_list[num\_images-1]\}} at the end of your response.

\section*{Distance Visualization}
\noindent
\texttt{You are an intelligent system tasked with predicting the best grasp for a robot from one image.}\\
Given an image of the scene and a task description, you should provide a point on the image where the robot should grasp the object to best accomplish the task. Note that there may be multiple valid grasping points, but you should select the one that is most suitable for the given task. Consider factors like stability, object orientation, and the requirements of the task in your decision. \\
\textbf{Task:} \texttt{\{task\}}\\
Explain your reasoning clearly, then repeat the answer with a point coordinate in percentage (e.g., (x, y)) on the image at the end of your response.


\section{Appendix C: Automated Evaluation Prompts}
\label{CC}
\subsubsection*{Contact All Evaluation}
\noindent
\texttt{The robot is considering multiple grasp candidates to achieve the task.}\\
The task description is: \texttt{'{task\_description}'}. Each of the colored dots (highlighted in \texttt{{parsed\_original\_res}}) shows a different grasp candidate. To accomplish the given task effectively, the robot has chosen to grasp the object at a point highlighted in \texttt{{chosen}}.\\
Please evaluate if this chosen grasp is the most appropriate one among the candidates for effectively accomplishing the task.\\
\textbf{Explain your reasoning clearly, then conclude with 'Yes' or 'No' at the end of your response.}

\subsubsection*{Gripper All Evaluation}
\noindent
\texttt{The robot is considering multiple grasp candidates to achieve the task.}\\
The task description is: \texttt{'{task\_description}'}. Each of the colored grip poses (highlighted in \texttt{{parsed\_original\_res}}) shows a different grasp candidate. To accomplish the given task effectively, the robot has chosen to grasp the object at a grip pose highlighted in \texttt{{chosen}}.\\
Please evaluate if this chosen grip pose is the most appropriate one among the candidates for effectively accomplishing the task.\\
\textbf{Explain your reasoning clearly, then conclude with 'Yes' or 'No' at the end of your response.}

\subsubsection*{Contact One Evaluation}
\noindent
\texttt{The robot is considering multiple grasp candidates to achieve the task.}\\
The task description is: \texttt{'{task\_description}'}. Each of the first \texttt{{num\_image}} images shows a different grasp candidate highlighted in a red dot, and the last image shows the chosen grasp (also highlighted in red).\\
Please evaluate if this chosen grasp is the most appropriate one among the candidates for effectively accomplishing the task.\\
\textbf{Explain your reasoning clearly, then conclude with 'Yes' or 'No' at the end of your response.}

\subsubsection*{Gripper One Evaluation}
\noindent
\texttt{The robot is considering multiple grasp candidates to achieve the task.}\\
The task description is: \texttt{'{task\_description}'}. Each of the first \texttt{{num\_image}} images shows a different grasp candidate's pose highlighted in red, and the last image shows the chosen grasp (also highlighted in red).\\
Please evaluate if this chosen grasp is the most appropriate one among the candidates for effectively accomplishing the task.\\
\textbf{Explain your reasoning clearly, then conclude with 'Yes' or 'No' at the end of your response.}

\subsubsection*{Distance Evaluation}
\noindent
\texttt{The robot has autonomously chosen a grasp point on the object in the image to accomplish the task.}\\
The task description is: \texttt{'{task\_description}'}.\\
Please evaluate if the robot's chosen grasp point is appropriate and fulfills the task requirements.\\
\textbf{Explain your reasoning clearly, then conclude with 'Yes' or 'No' at the end of your response.}


\end{document}