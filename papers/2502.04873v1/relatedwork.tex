\section{Literature Review}
\label{review} \subsection{Non-task-oriented Grasp Generation} 
There exists a substantial body of research on general grasp generation methods. For instance, \cite{mousavian20196} trained a variational autoencoder (VAE) to sample grasps in cluttered scenes, while \cite{urain2023se} utilized diffusion models to capture the multimodal distribution of grasps for a given object. \cite{sundermeyer2021contact} proposed a method that samples dense grasp poses for each possible contact point on an object. Additionally, \cite{fang2023anygrasp} developed a robust grasp generation model trained on noisy real-world data. These approaches successfully generate grasps that avoid collisions and generalize well across a wide variety of objects. However, they do not account for the specific tasks associated with manipulating the object.

\subsection{Task-oriented Grasp Generation} 
In contrast, task-oriented grasp generation focuses on predicting grasps that are optimized for a specific task. Researchers in \cite{murali2021same, jang2017end} introduced data-driven approaches, where large datasets were collected to train neural networks capable of predicting task-oriented grasps. More recently, several studies have explored the integration of large language models (LLMs) and vision language models (VLMs) to enhance task-oriented grasp generation. For example, \cite{tang2023task, li2024semgrasp} leveraged language information alone, while \cite{chang2024text2grasp} required the training of additional grasp generation models. \cite{taunyazov2023grace} employed differentiable classifiers to guide the grasp sampling process; however, while this approach is training-free, it is not directly applicable to more general scenarios where gradient information is not available.

\subsection{Foundation Models in Robotics} 
Building on the success of foundation models, there is growing interest in the use of LLMs and VLMs in various robotic applications. With their large capacities, these models can be trained or fine-tuned to directly predict a sequence of actions given a task description in natural language and RGB observations \cite{liu2024moka, team2024octo, brohan2023rt}. However, training such large models for open-world scenarios typically requires extensive data and supervision.

An alternative approach involves leveraging pre-trained VLMs in combination with lower-level skills to perform open-vocabulary manipulation tasks \cite{weerakoon2024behav, nasiriany2024pivot} or navigation tasks \cite{liu2024moka, wu2024helpful}. Nevertheless, recent studies \cite{ramakrishnan2024does} suggest that foundation models still struggle with spatial reasoning in general. In this work, we aim to investigate whether VLMs can effectively comprehend and reason about the spatial and semantic information related to grasping tasks. Unlike MOKA \cite{liu2024moka}, which assumes a fixed tabletop setup, our approach focuses on task-oriented grasp generation using VLMs. This approach is not constrained to a specific environment, offering a flexible framework that integrates various grasp generation models with VLMs to accommodate a wider range of scenarios. This adaptability enables a more versatile approach to task-oriented manipulations.