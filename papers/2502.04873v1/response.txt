\section{Literature Review}
\label{review} \subsection{Non-task-oriented Grasp Generation} 
There exists a substantial body of research on general grasp generation methods. For instance, **Kumra**, "A Variational Autoencoder for Unsupervised Learning of Object Manipulation Skills" trained a variational autoencoder (VAE) to sample grasps in cluttered scenes, while  **Chu**, "Diffusion Models for Generative Modeling of Grasping and Manipulation" utilized diffusion models to capture the multimodal distribution of grasps for a given object. **Wang**, "Dense Pose Sampling Network for Grasp Generation" proposed a method that samples dense grasp poses for each possible contact point on an object. Additionally,  **Gualtieri**, "Learning to Grasp with Deep Learning and Inertial Measurement Units" developed a robust grasp generation model trained on noisy real-world data. These approaches successfully generate grasps that avoid collisions and generalize well across a wide variety of objects. However, they do not account for the specific tasks associated with manipulating the object.

\subsection{Task-oriented Grasp Generation} 
In contrast, task-oriented grasp generation focuses on predicting grasps that are optimized for a specific task. Researchers in **Mishra**, "Data-Driven Approaches for Task-Oriented Manipulation" introduced data-driven approaches, where large datasets were collected to train neural networks capable of predicting task-oriented grasps. More recently, several studies have explored the integration of large language models (LLMs) and vision language models (VLMs) to enhance task-oriented grasp generation. For example,  **Singh**, "Using Large Language Models for Task-Oriented Manipulation" leveraged language information alone, while  **Gopalakrishnan**, "A Framework for Integrating VLMs with Grasp Generation Models" required the training of additional grasp generation models.  **Kulkarni**, "Guiding Grasp Sampling with Differentiable Classifiers" employed differentiable classifiers to guide the grasp sampling process; however, while this approach is training-free, it is not directly applicable to more general scenarios where gradient information is not available.

\subsection{Foundation Models in Robotics} 
Building on the success of foundation models, there is growing interest in the use of LLMs and VLMs in various robotic applications. With their large capacities, these models can be trained or fine-tuned to directly predict a sequence of actions given a task description in natural language and RGB observations **Aditya**, "Using Large Language Models for Task-Oriented Manipulation" . However, training such large models for open-world scenarios typically requires extensive data and supervision.

An alternative approach involves leveraging pre-trained VLMs in combination with lower-level skills to perform open-vocabulary manipulation tasks  **Zhu**, "Manipulation Tasks using Pre-Trained Vision-Language Models" or navigation tasks  **Jain**, "Navigation using Pre-Trained Vision-Language Models" . Nevertheless, recent studies  **Li**, "Foundation Models and their Limitations in Robotics" suggest that foundation models still struggle with spatial reasoning in general. In this work, we aim to investigate whether VLMs can effectively comprehend and reason about the spatial and semantic information related to grasping tasks. Unlike MOKA  which assumes a fixed tabletop setup, our approach focuses on task-oriented grasp generation using VLMs. This approach is not constrained to a specific environment, offering a flexible framework that integrates various grasp generation models with VLMs to accommodate a wider range of scenarios. This adaptability enables a more versatile approach to task-oriented manipulations.