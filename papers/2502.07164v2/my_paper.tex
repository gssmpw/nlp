% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{comment}

\title{Does Training on Synthetic Data Make Models Less Robust?}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Lingze Zhang \and Ellie Pavlick\\
  Brown University \\
  \texttt{\{lingze\_zhang, ellie\_pavlick\}@brown.edu} \\}

    


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain ``blindspots'' by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our ``blindspot'' task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn't necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized. \footnote{Our code is available at \url{https://github.com/untakenJ/synthetic-data-blindspot}.}
\end{abstract}

\section{Introduction and Related Work}
Constructing a dataset for a specific task in natural language processing can be costly in terms of time and labor. An increasingly common approach to solve this problem is to take advantage of large language models (LLMs) to generate training data. Itâ€™s simple to fine-tune an LLM or just use in-context learning to generate huge amounts of training data with a relatively small number of demonstrations. However, how effective the model-written datasets are for different tasks is still an open question.

%[method of using synthetic data for model training, and their contributions]
Model-generated training data is widely used in different domains like image classification~\cite{9053146,gowal2021improving}, visual language concepts understanding~\cite{cascante2023going} and medical image understanding~\cite{fernandez2022can}. In many NLP tasks, such as commonsense reasoning~\cite{yang-etal-2020-generative}, question-answering~\cite{bartolo-etal-2021-improving,paranjape-etal-2022-retrieval}, sycophancy reduction~\cite{wei2024simplesyntheticdatareduces}, cultural debiasing~\cite{li2024culturellmincorporatingculturaldifferences,li2024cultureparkboostingcrossculturalunderstanding} and general instruction alignment~\cite{wang-etal-2023-self-instruct}, synthetic data created with generative models are utilized in model training. In cases where there are limited sources for model training, synthetic data would greatly benefit the performance of finetuned model. High-quality model-written datasets may also be used for evaluations. \citet{perez-etal-2023-discovering} created 154 evaluation datasets and discovered inverse scaling of language models in some scenarios.

%[sythetic data has limitations and risks: model degradation][more examples about potential harm asn bias]
However, synthetic data may also be harmful. \citet{shumailov2024ai} found that language models may collapse if recursively finetuned with generated text. Such degradation has also been discovered in image generation tasks~\cite{alemohammad2024selfconsuming}. The use of synthetic data is also criticized from the perspective of ethics and social impact~\cite{Susser2024-SUSCPF}. There's a series of research about what bias is manifested in synthetic data and how the performance in specific tasks is affected. For example, gender stereotype is a common kind of bias amplified in data generated by language models~\cite{kirk2021bias,kotek2023gender}. \citet{li-etal-2023-synthetic} investigated the text classification task and showed that subjectivity is a matter affecting the performance of models trained with synthetic data.~\citet{bisbee2023synthetic} found less variation in ChatGPT responses than in the real ANES survey. Similarly, a study from~\citet{chen-etal-2024-unveiling-flaws} indicates that the uniform format of synthetic data can lead to pattern overfitting and thus harm the instruction-following capabilities of the model trained with it.~\citet{seddik2024how} reveals that the recursive training loop makes the tails of the original distribution disappear and makes the model forget the real distribution from a statistical perspective.

%[sometimes risk can be identified by evalutation, while sometimes not. There are blind spots: more difficult subset of tasks or false correlation]
% We are aware that the distribution of synthetic data is shifted to be more centralized than the real-world population on different potential attributes. Some dimensions, like gender bias, are relatively easier to detect so that the issue can be mitigated with further finetuning or manually filtering the dataset. However, some aspects of biases can be very subtle and hard to discover. The synthetic data is often not as diverse as the population~\cite{whitney2024real}, and creating a more diversified synthetic dataset helps improve the performance of the trained model~\cite{chen2024diversitysyntheticdataimpact,yu2024large}. A crucial issue here is that the \textit{attributes} for the consideration of diversity is usually a predefined set, and it cannot capture everything. If some aspects of the biases are left unattended when training a model with synthetic data, it's possible that those biases would be reinforced and thus make the performance on some specific subsets of tasks worse without being noticed. We define such a subset as a \textit{blindspot}.

% [some knowledge learned by the model may not be generalizable (it's one kind of failure that is hard to detect) -> model failure in some scenarios -> define it as blindspot.]
One particular way in which synthetic data might be harmful is if it reinforces ungeneralizable heuristics. It is well know that LLMs often rely on features that perform well on the training set but do not necessarily generalize as we would like, for example, relying on gender bias~\cite{10.1145/3582269.3615599}, word-overlap bias in NLI~\cite{rajaee-etal-2022-looking}, or exhibiting a preference toward longer responses in text generation~\cite{singhal2024a}. We refer to these types of heuristics as \textit{blindspots}.  % Though creating a more diversified synthetic dataset helps improve the performance of the trained model~\cite{chen2024diversitysyntheticdataimpact,yu2024large}, the \textit{attributes} for the consideration of diversity is usually a predefined set that cannot capture everything possible.

%[two folds effect: (1) LLMs may have knowledge about the bilidspots, which may mitigate the issue; (2) On the other hand, the unawareness of the bias may result in reinforcing the bias because of the absense of generated example addressing the bias] --> probably in discussion part
%[our hypothesis: sythetic data may make things worse on blindspots]
%[We experimented on the issue.]
In this work, we hypothesize that, because synthetic data less diverse than the original training data \cite{whitney2024real}, it is more likely to have blindspots and thus that fine-tuning on model-generated data will exacerbate these blindspots in the tuned model. In particular, we hypothesize that the synthetic data will encode the heuristic to a larger extent than naturally occurring data would, and thus that fine-tuning on synthetic data will lead the model to more strongly favor the heuristic. This weakness would be revealed in data that specifically is designed to test whether models are using the heuristic, as models trained on synthetic data might still show improved performance on generic test sets on which the heuristic performs well. 

As a case study, we focus on the natural language inference (NLI) task evaluated with the MultiNLI dataset~\cite{williams-etal-2018-broad}. The MultiNLI dataset covers general examples collected from various sources, but models trained on MultiNLI may tend to make judgments based on superficial syntactic properties and perform badly on HANS, an adversarial dataset created with syntactic heuristics~\cite{mccoy-etal-2019-right}. The HANS task can be regarded as a measure of the model's ``blindspot''.

%[Findings: dispersion may happen in some cases, but not always]
%[When training with synthetic data improves greatily on the general task, the bias may not be reinfroced, though not as good as using original data]
%[When the bias in reinforced, the general performance on the original task is not improving, maybe because of overfitting. In this case the trained model may not be depolyed in real life]
%[contribution: evlauated the effect of using sythetic data to finetune LMs settings simulating different scenarios on a NLI task as an example. We find that the dispersion is not a consistent pattern]
Our expected result is that finetuning an NLI model with synthetic MultiNLI-like data will reduce its performance on HANS while improving its performance on the MultiNLI test set. However, we observed that this is not a consistent pattern under various settings of starting point model and size of synthetic dataset, though some biases do exist in the synthetic dataset. Our hypothesis is thus not fully supported by the experimental results. We have nonetheless discovered different patterns of performance change on both test sets in different scenarios. We hope the discovered insights will foster novel research ideas in understanding model degradation with synthetic training data and advancing fairness and robustness of language models.


\section{Methods}
\subsection{Overview}
% We are starting the study with a simple setup: a \textbf{t}ask model \(T\) and a \textbf{g}enerator model \(G\).~\(T\) can be a model for any kind of NLP tasks, and \(G\) is a language model used to generate training samples for \(T\). Let $\mathcal{X}_T$ denote the set of all possible input of model $T$. For each $x\in\mathcal{X}_T$, there's a corresponding gold label $y_x$\footnote{The label can also be a set of all possible acceptable output. Without loss of generality, we just consider the single-label case, which is consistent with the NLI task we are studying in this paper.}. We define that a \textit{blindspot} of $\mathcal{X}_T$ is a subset $\widetilde{\mathcal{X}_T}\subseteq\mathcal{X}_T$ for on which the model \(T\) performs worse. Assuming that we have a dataset $\mathcal{D}^G$ consisting of the generated input $\{x^{G}_{1}, x^{G}_{2}, x^{G}_{3}, ..., x^{G}_{N}\}$ and corresponding labels $\{y_{x^{G}_{1}}, y_{x^{G}_{2}},y_{x^{G}_{3}}, ..., y_{x^{G}_{N}}\}$ generated by $G$ that is not tuned with explicit knowledge about the blindspot $\widetilde{\mathcal{X}_T}$, and that model $T^{\mathcal{D}^G}$ is an altered version of $T$ that is influenced by the information from $\mathcal{D}^G$ (for example, fine-tuned with  $\mathcal{D}^G$), our main research question is to figure out whether tuning the model $T$ with $\mathcal{D}^G$ mitigates or reinforces the blindspot issue, or the relationship between $P(T(\widetilde{X}) = y_{\widetilde{X}})$ and $P(T^{\mathcal{D}^G}(\widetilde{X}) = y_{\widetilde{X}})$. We are studying this question empirically with experiments described in later subsections.

We assume a \textbf{t}ask model \(T\) and a \textbf{g}enerator model \(G\).~\(T\) can be a model for any kind of NLP tasks, and \(G\) is a language model used to generate training examples for \(T\). Let $\mathcal{X}_T$ denote the set of all possible input of model $T$. The existence of a blindspot means that there's a non-random subset $\widetilde{\mathcal{X}_T}\subseteq\mathcal{X}_T$  on which the model \(T\) performs worse than on $\mathcal{X}_T$ in general. %, or $P(T(\widetilde{X}) = y_{\widetilde{X}}) < P(T({X}) = y_{{X}})$ where $X$ and $\widetilde{X}$ are samples unifromly sampled from $\mathcal{X}_T$ and $\widetilde{\mathcal{X}_T}$, respectively (with labels $y_{X}$ and $y_{\widetilde{X}}$).
Let $\mathcal{D}^G$ denote the synthetic dataset generated by \(G\), and $T^{\mathcal{D}^G}$ denote the model fine-tuned on $\mathcal{D}^G$.
Our hypothesis is that $T^{\mathcal{D}^G}$ will perform worse than $T$ on $\widetilde{\mathcal{X}_T}$, but better than $T$ on $\mathcal{X}_T$.
%What we care about is the relationship between $P(T(\widetilde{X}) = y_{\widetilde{X}})$ and $P(T^{\mathcal{D}^G}(\widetilde{X}) = y_{\widetilde{X}})$, which indicates whether fine-tuning the model $T$ with $\mathcal{D}^G$ mitigates or reinforces the blindspot issue.

\subsection{Tasks, Models and Datasets} \label{tmd}
\subsubsection{Tasks}
In this study, we focus on the natural language inference (NLI) task. An input example of this task contains a premise sentence, a hypothesis sentence, and a label indicating the relationship between the two sentences. The label can be one of $\{entailment, neutral, contradictory\}$.



\subsubsection{Models and Input}
Our experiments are based on the Llama-2-7B-hf model~\cite{touvron2023llama}. We fine-tuned a Llama-2 model with a classification head on top with MultiNLI as the task model $T$. The input sequence is constructed with the template 

\begin{quote}
    \textit{Please indicate the relationship between the premise and the hypothesis with entailment, neutral or contradiction. Premise: <premise> Hypothesis: <hypothesis> The relationship between premise and hypothesis is}
\end{quote} and the classification is based on the embedding of the last token in the input sequence. Our generator $G$ is also a Llama-2-7B-hf model fine-tuned with MultiNLI. It's tuned to generate examples in the form of 
\begin{quote}
\textit{This is an example where the relationship between the premise and the hypothesis is <label>. Premise: <premise> Hypothesis: <hypothesis> -- This is the end of the example.} 
\end{quote}
The label is put before the premise and the hypothesis for more flexible control of generated labels.




\subsubsection{Datasets}
We use MultiNLI \cite{williams-etal-2018-broad} as a measure of the models performance on NLI in general. The original task model $T$ and generator $G$ are both Llama-2-7B-hf models finetuned on MultiNLI. To measure the presence of the ``blindspot'', we use HANS~\cite{mccoy-etal-2019-right}. HANS is an NLI dataset created adversarially with three heuristics:  the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. Poor performance on HANS indicates that the model is likely using these heuristics to solve the NLI task. 

When training the task model $T$, we used the training set of MultiNLI as the training data, with 750 examples (250 for each label) excluded as the dev set. HANS is not used in training at all, but the results on its test set are reported. The maximum training set size for $T$ is 391,722.

Note that there are only two labels in HANS (because of how the dataset is constructed): \textit{entailment} and \textit{non-entailment}. In our experiments, the base task model and generator are fine-tuned with three labels of MultiNLI. When testing on HANS, predicted labels \textit{neutral} and \textit{contradictory} are both regarded as \textit{non-entailment}.


\subsection{Experiments}
\subsubsection{Basic Setting}
In our experiment pipeline, we first fine-tuned a classifier model $T$ with the MultiNLI training set from the pretrained Llama-2-7B-hf model with a classification head. Then we fine-tuned another Llama-2-7B-hf model as the generator $G$, also with the MultiNLI training set. After training $G$, we generated a dataset $\mathcal{D}^G$ with it and used $\mathcal{D}^G$ to further fine-tune $T$ to obtain the further tuned model $T^{\mathcal{D}^G}$. We varied $T$ (by changing the number of MultiNLI examples used for the initial fine-tuning) and $\mathcal{D}^G$ for different settings.

\subsubsection{Starting Models}
The initial task model $T$ is fine-tuned with data from the original MultiNLI dataset. In order to simulate task models in different stages, we trained 6 starting models with training set sizes of 0 (meaning the official pretrained model with a random classification head), 5000, 10000, 20000, 100000, and 391722.


\subsubsection{Synthetic Datasets}
The synthetic data examples are all generated by a Llama-2-7B-hf model $G$ fine-tuned with the MultiNLI training set for 1 epoch. The generator model is fine-tuned to generate text in the specific format aforementioned with the following prompt:

\begin{quote}
    \textit{This is an example where the relationship between the premise and the hypothesis is <label>}
\end{quote} We kept the generated examples in which the premise and the hypothesis can be extracted with a regular expression without further filtering. %This is a simulation of real-world scenarios where there's not enough labor force to refine the generated dataset.

We generated 1,819,813 examples, which is more than necessary for the training. We sampled two kinds of synthetic datasets: uniformly random sampled datasets and showcasing datasets with a stronger bias. We took the lexical overlap (LO) heuristic addressed in the HANS dataset as an example. Lexical overlap means all the words in the hypothesis appear in the premise. %, and we regard the wrong correlation between lexical overlap and entailment label as a possible blindspot bias.

Based on the availability of synthetic data, we constructed synthetic training sets of three sizes: 73080, 36040, and 18020. In each synthetic set, there are equal numbers of examples with each label. The random synthetic dataset (marked as \textit{Synthetic}) is uniformly sampled for each label, and the more strongly biased dataset (\textit{Biased Synthetic}) is sampled to make sure all entailment examples follow the lexical overlap heuristic and all other examples do not. We also included baseline datasets sampled from the original MultiNLI training set of the same sizes, marked as \textit{Original}. The datasets used in the experiment can be represented as $\{73080, 36040, 18020\} \times \{Original, Synthetic, Biased\ Synthetic\}$.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/output_new_large.png} 
  \caption {Augmented model performance under different settings.}
  \label{fig:results}
\end{figure*}

\subsubsection{Test Sets}
We report our results on three test sets: the MultiNLI Matched test set, the HANS test set, and the subset of the HANS test set with lexical overlap and a non-entailment label, which reflects the model's performance specifically on the blind spot. In addition to the augmented model with different training sets, we also report the classification performance of each starting model.

\section{Results}
Our main results are reported in Figure \ref{fig:results}. Each subplot corresponds to a different starting model $T$. 
When starting with undertrained task models, further fine-tuning with synthetic data will improve the performance on the MultiNLI Matched test set. The amount of improvement is on par with the model fine-tuned with original MultiNLI training data if the training set size is large enough. For relatively well-established starting models, neither fine-tuning with original nor synthetic data would significantly improve the performance of MultiNLI.

The performance on the HANS test set is trickier. The hypothesized trend , in which the performance of HANS goes down while the performance of MultiNLI goes up, only happens in for the 20K starting point. We also see a fairly sizable drop in HANS performance for the 392K starting point, but the curve is not monotonic and thus it is inconclusive. Overall, under most settings, further fine-tuning with original MultiNLI data would always benefit more or harm less on HANS performance than synthetic data. The gap does exist, but may not be as serious as expected.

As a sanity check, we also trained the model with the biased synthetic dataset in which all examples with lexical overlap are labeled entailment, and no example with neural or contradiction label satisfies the lexical overlap heuristic. As expected, such models perform worst in almost all tests, with the accuracy on the HANS subset of lexical overlap heuristic and non-entailment label dropping significantly towards zero over training. This indicates that a very biased synthetic dataset could exacerbate blindspots as expected, and thus implies that true synthetic data does not overrepresent the heuristic as much as hypothesized.




%[dispersions happens in some cases but not always]
%[when training with synthetic data boosts the performance of MNLI, typically in case of undertrained starting point, the performace on HANS is not likely to drop significantly.]
%[when the model is adquately trained with original data, furthier finetuning will not improve its performance on MNLI]

\section{Conclusion}
From the simulated experiments, we observed that while training the task model with synthetic data contributes to the performance on the general tasks almost equally compared with training with the original data, the contribution gap on the ``more difficult'' blindspot task does exist. Under some settings, there's a dispersion where the accuracy on the blindspot task goes down while the general task accuracy goes up, but this is not a consistent tendency. Reinforcement of bias while training may happen, but this would probably not cause significant issues if we just use the unfiltered synthetic data for training. 

\newpage

\section*{Limitations}

\begin{table}[h]
\small
\centering
\begin{tabular}{rrrrrr}
\hline
\textbf{Dataset} & \textbf{Label} & \textbf{Count} & \textbf{\# LO} & \textbf{\% LO} & \textbf{\% S:O} \\ \hline
                 & Ent.           & 631992         & 24360          & 3.854          & 2.352           \\
Syn.             & Neu.           & 562822         & 690            & 0.123         & 2.320           \\
                 & Con.           & 624999         & 1504           & 0.241        & 1.827           \\ \hline
                 & Ent.           & 130541         & 2139           & 1.639         & -               \\
Orig.            & Neu.           & 130573         & 69             & 0.053        & -               \\
                 & Con.           & 130608         & 172            & 0.132         & -               \\ \hline
\end{tabular}
\caption{\label{tab:dstat}
Statistics of examples with lexical overlap in original and synthetic data. There are generally more cases in synthetic data, and the correlation between lexical overlap and the entailment label is reinforced.
  }
\end{table}

We need to note that the study with MultiNLI and HANS is a case study addressing the issue of bias reinforcement when training models with synthetic data. It's still an open question whether the results about the biases we are studying are generalizable to other cases. According to Table \ref{tab:dstat}, lexical overlap is more common in the synthetic dataset than in MulitNLI for all labels, which may indicate that synthetic data is less diverse. However, the correlation between lexical overlap and entailment label is just slightly stronger. Different kinds of bias can emerge in very different ways in synthetic data, and this makes it challenging to evaluate the effect of training models with synthetic data holistically.

\begin{comment}

\begin{table*}[]
\centering
\begin{tabular}{rrrrrr}
\hline
\textbf{Dataset} & \textbf{Label} & \textbf{Count} & \textbf{\# LO} & \textbf{\% LO} & \textbf{\% S:O} \\ \hline
                 & Ent.           & 631992         & 24360          & 3.854          & 2.352           \\
Syn.             & Neu.           & 562822         & 690            & 0.1226         & 2.320           \\
                 & Con.           & 624999         & 1504           & 0.2407        & 1.827           \\ \hline
                 & Ent.           & 130541         & 2139           & 1.6386         & -               \\
Orig.            & Neu.           & 130573         & 69             & 0.05284        & -               \\
                 & Con.           & 130608         & 172            & 0.1317         & -               \\ \hline
\end{tabular}
\caption{\label{tab:dstat}
Statistics of examples with lexical overlap in original and synthetic data. There are generally more cases in synthetic data, and the correlation between lexical overlap and the entailment label is reinforced.
  }
\end{table*}
\end{comment}

Our design choices about the experiments may also be arbitrary. The task model we choose is the Llama-2-7B-hf model with a classification head. The pretrained Llama model is a relatively strong model, while the initialization of the classification head is random. Whether jointly training these parts is a reasonable choice is still arguable. Moreover, the two-step approach of model training is also not the only choice. It's also common to mix the original and synthetic data in different ratios and train the model with the mixed dataset in one run. Varying the experiment design is also necessary for further validations about the findings in this study.

Another notable point is that pretrained large language models, such as Llama, encode a wealth of world knowledge. Many potential biases may have been mitigated during training, particularly for models deployed in real-world applications, which are typically much larger and more powerful than the fine-tuned Llama-2-7B-hf generator used in our study. On the other hand, human-created or audited data are not inherently free from implicit biases. The more concentrated distribution and reduced diversity of synthetic data might reinforce biases in certain blindspot scenarios. However, the rich world knowledge embedded in the generator model can also address some biases, potentially outperforming humans in certain cases and contributing positively to bias mitigation. A critical direction for future research is to disentangle these two effects and assess the significance of each, thereby enhancing our understanding of the impact of training models with synthetic data.



\bibliography{anthology,custom}

\end{document}
