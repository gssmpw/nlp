\section{Adaptive Task Allocation}
\label{section:ata}

Here, we first show how to reduce the problem in \eqref{eq:ata_obj} to a \emph{non-linear} stochastic Multi-Armed Bandit (MAB) problem. Then, we propose an efficient algorithm for this formulation.

\subsection{Reduction to Multi-Armed Bandit and Proxy Loss}
\label{sec:reduction}
The stochastic MAB problem is a fundamental framework in sequential decision-making under uncertainty.
It involves a scenario where an agent must choose among a set of arms, each associated with an unknown reward distribution.
The agent aims to maximize cumulative reward (or equivalently minimize the cumulative loss) over time by balancing exploration (gathering information about the reward distributions) and exploitation (leveraging the best-known arm).
The challenge lies in the trade-off between exploring suboptimal arms to refine reward estimates and exploiting the arm with the highest observed reward, given the stochastic nature of the outcomes.
Using the terminology from bandit literature, here we will refer to each worker as an ``arm.''

However, differently from the standard MAB problem, we have a harder problem because $\E{C(\bm{a}_k)}$ depends on the joint distribution of all the arms in the support of $\bm{a}_k$, rather than on their expectations only.
This dependency potentially renders the task of relying on estimates of $\E{C(\bm{a})}$ for $\bm{a} \in \mathcal{A}$ computationally challenging due to the combinatorial nature of the set $\mathcal{A}$.

To solve this issue, our first idea is to introduce a \emph{proxy loss} $\ell : \mathcal{A}\times \mathbb{R}_{\ge 0}^n \to \mathbb{R}_{\ge 0}$, defined as
\begin{equation}\label{eq:def_l}
	\ell(\bm{a},\bm{\mu}) \eqdef  \max_{i \in [n]} \  a_{i} \mu_i~.
\end{equation}
Due to the convexity of $C(\cdot)$, the introduced proxy-loss underestimates the expected computation time.
However, in \Cref{sec:proof_2} we prove that this quantity also upper bounds the expected computation time up to a constant that depends on the distribution of the arms. In particular, for any $\bm{a} \in \mathcal{A}$, we show that
\begin{equation}\label{eq:enc}
	\ell(\bm{a}, \bm{\mu}) \le \E{C(\bm{a})} \le (1+\eta\sqrt{\ln B}) \ell(\bm{a}, \bm{\mu}),
\end{equation}
where $\eta$ is defined as
\begin{equation}\label{def:eta}
	\eta := \max_{i \in [n]} \frac{\sigma_i}{\mu_i}~.
\end{equation}
In words, $\eta$ provides an upper bound on the ratio between the standard deviation and the mean of the arms.
Note that in the literature, it is common to consider exponential, Erlang, or Gamma distributions, where the ratio $\eta$ is typically\footnote{For $\mathrm{Gamma}(\alpha, \lambda)$, $\sigma / \mu = 1 / \sqrt{\alpha}$, so the claim holds for $\alpha \geq 1$.} bounded by $1$.

The bound above will allow us to derive guarantees on the total computation time of an allocation strategy based on its guarantees for the proxy loss $ \ell(\cdot)$, up to a factor of the order $1 + \eta\sqrt{\ln B}$.
We remark that in the special case where the arms' distributions are deterministic ($\eta = 0$) or the query budget is unitary ($B = 1$), the two targets $\E{C(\bm{a})}$ and $\ell$ exactly coincide.


%\marginpar{{\color{red}I recommend\\changing\\the name\\Harmonia\\into\\`Oracle': it\\is more\\inline with\\literature\\+more\\informative}}

% \textbf{Comparison with the combinatorial bandits setting.}
\subsection{Comparison with the combinatorial bandits setting}
Our setting is closely related to the combinatorial multi-armed bandits (CMAB) framework \citep{cesa2012combinatorial}, particularly due to the combinatorial nature of the action space and the semi-bandit feedback, where the learner observes outcomes from all chosen arms.
However, our formulation differs in two significant ways.
First, while CMAB typically involves selecting a subset of $n$ arms, resulting in an action space with a maximum size of $2^n$, our action space $\mathcal{A}$ has a cardinality of $\binom{n + B - 1}{B}$.
The ratio between these two can be extremely large, potentially growing exponentially with $n$.
%A smaller action space translates in a smaller growth of the regret with time.
Second, although most works in this domain assume a linear loss function in the arms' means, some notable exceptions address non-linear reward functions \citep{chen2013combinatorial, lin2015stochastic, chen2016combinatorial, wang2018thompson}.
However, these approaches generally rely on assumptions such as smoothness, Lipschitz continuity, or higher-order differentiability of the reward function.
In contrast, our loss function $\ell(\cdot, \bm{\mu})$ is not continuous with respect to the arms' means.
Finally, motivated by the practical requirements of our setting, we place a strong emphasis on computational efficiency that rules out most of the approaches based on CMAB.

\subsection{Adaptive Task Allocation Algorithm}
Now, we introduce our Adaptive Task Allocation algorithm (\algname{ATA}).
\algname{ATA} operates without requiring prior knowledge of the horizon $K$ and only assumes an upper bound on the Orlicz norms of the arm distributions, $\alpha \ge \max_{i \in [n]}{\norm{X_i}_{\psi_1}}$.
The core idea of the procedure is to allocate the workers based on \emph{lower confidence bound estimates} on the arm means $(\mu_i)_{i \in [n]}$, in order to balance exploration and exploitation.

For each arm $i \in [n]$ and round $k \in [K]$, let $K_{i,k}$ represent the number of samples collected from the distribution of arm $i$ up to round $k$. At each round $k$, we compute an empirical mean, denoted by $\hat{\mu}_{i,k}$, using the $K_{i,k}$ samples obtained so far. Based on these empirical means, we define the lower confidence bounds $s_{i,k}$ as
\begin{equation}
\label{eq:s}
	s_{i,k} = \left(\hat{\mu}_{i,k} - \text{conf}(i,k) \right)_{+},
\end{equation}
where $(x)_{+} = \max\{x, 0\}$ and $\text{conf}(\cdot, \cdot)$ is defined as
\begin{equation*}
	\conf(i, k) =
	\begin{cases} 
		4e\alpha\(\sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+\frac{\ln(2k^2)}{K_{i,k}}\), & K_{i,k} \geq 1, \\
		+\infty, &  K_{i,k} = 0~.
	\end{cases}
\end{equation*}

The term $\text{conf}(\cdot, \cdot)$ is derived from a known concentration inequality for sub-exponential variables with an Orlicz norm bounded by $\alpha$ (\Cref{prop:concentration} in the Appendix).

Given the confidence bounds $\bm{s}_k := (s_{1,k}, \dots, s_{n,k})$, the learner selects the action $\bm{a}_k \in \mathcal{A}$ at round $k$ that minimizes the loss $\ell(\cdot, \bm{s}_k)$, defined in \eqref{eq:def_l}. While nonconvex, we show in \Cref{sec:RAS} that this optimization problem can be solved using a recursive routine, whose computational efficiency is 
$$
	\mathcal{O}(n \ln(\min\{B, n\}) + \min\{B, n\}^2).
$$

\begin{algorithm}[t]
	\caption{\algname{ATA} ({\red A}daptive {\red T}ask {\red A}llocation)}
    \label{alg:ata}
	\begin{algorithmic}[1]
        \STATE \textbf{Input}: allocation budget $B$, $\alpha>0$
        \STATE \textbf{Initialize}: empirical means $\hmu_{i,1} = 0$, usage counts $K_{i,1} = 0$, and usage times $T_{i,1} = 0$, for all $i \in [n]$
		\FOR{$k = 1,\ldots, K$}
        \STATE Compute LCBs $(s_{i,k})$ for all $i \in [n]$ using \eqref{eq:s}
        \STATE Find allocation:
        $
        \bm{a}_k \in  \argmin_{\ba \in \mathcal{A}} \ell(\ba, \bm{s}_k)
        $
		\STATE Allocate $a_{i,k}$ tasks to each worker $i \in [n]$
        \STATE {\color{orange} Update optimization parameters}
        \FOR{$i$ such that $a_{i,k} \neq 0$}
        \STATE $K_{i,k+1} = K_{i,k} + a_{i,k}$
        \STATE $T_{i,k+1} = T_{i,k} + \sum_{j=1}^{a_{i,k}} X_{i,k}^{(j)}$
        \STATE $\hmu_{i,k+1} = T_{i,k+1} / K_{i,k+1}$
        \ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\begin{remark}
	Line 7 of the algorithm acts as a placeholder for the optimization method, where the optimization parameters are updated using the quantities computed by the workers (e.g., gradients in the case of \algname{SGD}). In this view, the allocation algorithm is independent of the specifics of the chosen optimization algorithm. Refer to \Cref{section:other_methods} for further details.
\end{remark}


As last step, the feedback obtained after applying the allocation $\bm{a}_k$ is used to update the lower confidence bounds. The complete pseudocode for \algname{ATA} is provided in \Cref{alg:ata}.


\subsection{Upper-bound on the total computation time}
We provide guarantees for \algname{ATA} in the form of an upper bound on the expected total computation time required to perform $K$ iterations of the optimization procedure. Recall that the proxy loss $\ell(\cdot, \bm{\mu})$ and the expected computation time are related through \eqref{eq:enc}. This relationship and \Cref{thm:main} allow us to derive guarantees on the expected total computation time, denoted by
$$
	\mathcal{C}_K := \sum_{k=1}^{K} \E{C(\bm{a}_k)}~.
$$

We define the optimal allocation for minimizing the computation time as 
$$
\bm{a}^* \in \argmin_{\bm{a} \in \mathcal{A}} \ \E{C(\bm{a})}~.
$$
Consequently, the optimal expected total computation time in this framework is given by 
$$
\mathcal{C}_K^{*} := K \E{C(\bm{a}^*)}~.
$$
\begin{theorem}[Proof in \Cref{sec:proof_2}]
    \label{cor:main}
	Suppose \Cref{a:sube} holds and let $\eta := \max_{i \in [n]} \frac{\sigma_i}{\mu_i}$.
	Then, the total expected computation time after $K$ rounds, using the allocation prescribed by \algname{ATA} with inputs $(B, \alpha)$ satisfies
	$$
		\mathcal{C}_K \le \left(1+\eta\sqrt{\ln B}\right)\mathcal{C}_K^* + \mathcal{O}(\ln K)~.
	$$
\end{theorem}
\begin{remark}
	The $\cO(\cdot)$ term hides an instance dependent factor. We will give its full specifics in the regret upper bound of \Cref{thm:main}.
\end{remark}

The bound in Theorem~\ref{cor:main} shows that the total expected computation time of \algname{ATA} remains within a multiplicative factor of $1 + \eta \sqrt{\ln B}$ of the optimal computation time $\mathcal{C}_K^*$, with an additional remainder term that scales logarithmically with $K$. Since $\mathcal{C}_K = \Omega(K)$, this additive term is negligible compared to $\mathcal{C}^*_K$. In practical scenarios, where computation time follows common distributions such as exponential or Gamma, the factor $\eta$ is typically of order $1$, and $\sqrt{\ln B}$ remains relatively small for the batch sizes commonly used in optimization algorithms like \algname{SGD}.

The reader might wonder if the more ambitious goal of deriving bounds with a multiplicative factor of exactly $1$ is achievable. However, achieving this goal would require significantly more precise estimates of the expected computation time $\mathbb{E}[C(\bm{a})]$ for all $\bm{a} \in \mathcal{A}$. Since $\mathbb{E}[C(\bm{a})]$ depends on the joint distribution of all workers in the support $\bm{a}$, obtaining such precise estimates would come at the cost of computational efficiency in the allocation strategy.


We note that it is unsurprising that $\eta$ appears in the upper bound of \Cref{cor:main}, since having a heavier-tailed distribution increases the gap between $\ell(\bm{a}, \bm{\mu})$ and $\E{C(\bm{a})}$ through the convexity of $C(\cdot)$.
Instead, the factor $\sqrt{\ln B}$ arises because $C(\cdot)$ is expressed as the maximum of up to $B$ random variables.
Moreover, in the edge cases where $\eta = 0$ (deterministic case) or $B=1$ (linear cost function), we guarantee that the expected computation time is at most an \emph{additive} factor away from the optimal one.



\section{Empirical Adaptive Task Allocation}
\label{sec:ata-em}

%\textbf{TO DO: first, tell me what are the shortcomings of ATA. Then, tell me the proposed solution and its advantages both from a theoretical point of view (bound) and from a practical point of view (less exploration on useless arms?). Finally, tell me the trade-off in terms of what I need to know.}

The \algname{ATA} procedure is based on a lower confidence bound approach that relies on concentration inequalities. These bounds play a key role in performance, as sharper concentration bounds lead to more accurate estimates and reduce exploration of suboptimal options. Since workers' computation times follow sub-exponential distributions, their concentration behavior is determined by the Orlicz norm of the corresponding variables.
In \algname{ATA}, the only prior knowledge available is an upper bound on the largest Orlicz norm among all arms. When the Orlicz norms of the arms' distributions vary significantly, this uniform bound may result in loose confidence intervals and inefficient exploration. 

To address this issue, we introduce \algname{ATA-Empirical}, which better adapts to the distribution of each arm, particularly its Orlicz norm. This adaptation is achieved through a novel data-dependent concentration inequality for sub-exponential variables.
%, presented in Lemma~\ref{lem:0}.
Unlike \algname{ATA}, which depends on the maximum Orlicz norm, \algname{ATA-Empirical} accounts for the individual Orlicz norms of all arms, denoted by $(\alpha_i)_{i\in [n]}$. This improvement is reflected in the upper bounds on regret presented in \cref{section:theory}. In practice, this leads to improved performance at least some settings, as shown in our simulations in \cref{section:experiments}. However, this increased adaptivity comes with a trade-off since \algname{ATA-Empirical} requires an upper bound on the quantity $\eta = \max_i {\sigma_i/\mu_i}$, rather than a bound on the largest Orlicz norm. That said, for many distributions of interest, the ratios $\sigma_i/\mu_i$ across different arms tend to be of the same order, whereas their Orlicz norms can vary significantly.


%, and the obtained lower confidence bounds used in \algname{ATA-Empirical} are defined as follows: for arm $i \in [n]$ at round $k \in [K]$, if $K_{i,k} = 0$ let $\hat{s}_{i,k} = \infty$, otherwise if $K_{i,k}\ge 1$
%\begin{equation*}
%	\textstyle
%	\hat{s}_{i,k} =
%		\hat{\mu}_{i,k} \left[ 1-4e\,\xi\left(\sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+ \frac{\ln(2k^2)}{K_{i,k}} \right) \right]_{+}.
%\end{equation*}
%By comparing this new confidence bound with the standard approach, we observe that the requirement for prior knowledge of the Orlicz norm is replaced by a need for prior knowledge of the ratios $(\sigma_i / \mu_i)$. As discussed in \Cref{sec:setup}, such knowledge is realistic in cases where the learner is aware of the class of distributions governing the arms (e.g., exponential, Gamma, etc.).

%\textbf{TO DO: maybe better here to state the result, or at least to be more explicit on what are the differences with the result for ATA.}
The \algname{ATA-Empirical} procedure differs from \algname{ATA} only in the lower confidence bounds it uses. These bounds are derived from the novel concentration inequality in Lemma~\ref{lem:0} and are defined for arm $i \in [n]$ at round $k \in [K]$ as
\begin{equation}\label{def:s}
	\hat{s}_{i,k} =
	\hat{\mu}_{i,k} \left[ 1-4e\,\xi\left(\sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+ \frac{\ln(2k^2)}{K_{i,k}} \right) \right]_{+},
\end{equation}
where $\xi = (1+\sqrt{4\eta^2+5})/2$ and $\eta \ge \max_{i \in [n]} \sigma_i/\mu_i$.

The expected total computation time $\mathcal{C}_K$ of \algname{ATA-Empirical} satisfies the same guarantee presented in Theorem~\ref{thm:main}, but we obtain an improved multiplicative factor of the additive logarithmic term. The precise expressions of these factors are provided in the next section, and they show that the guarantees of \algname{ATA-Empirical} adapt to the Orlicz norms $\norm{X_i}_{\psi_1}$ of each arm, while the guarantees of \algname{ATA} depend on the maximum Orlicz norm $\max_i\norm{X_i}_{\psi_1}$.

