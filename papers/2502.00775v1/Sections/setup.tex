\section{Problem Setup}
\label{sec:setup}

In this section, we formally describe the problem setup.


% \textbf{Task allocation protocol.}
\subsection{Task allocation protocol}
We consider a system of $n$ workers, each responsible for computing gradients.
In each round, the allocation algorithm has a budget of $B$ units that must be allocated among the $n$ workers.
Each unit allocation will result in one gradient computation.
We denote by $K$ the total number of rounds, which is assumed to be unknown to the learner.
We denote by $X_{i,k}^{(u)}$ the computation time of the worker $i \in [n]:=\{1, 2, \dots, n\}$ for round $k \in [K]$ on its $u$-th gradient.
Consequently, the computation time required for worker $i$ to perform its task of computing $a_{i,k}$ gradients in round $k$ is given by $\sum_{u=1}^{a_{i,k}} X_{i,k}^{(u)}$ if $a_{i,k} \geq 1$, and $0$ otherwise.

In each round $k$, the allocation algorithm must choose an allocation vector $\bm{a}_k \in \mathbb{N}^n$ such that $\norm{\bm{a}}_1 = B$, based on the information available prior to round $k$.
The feedback consists of $a_{i,k}$ observed times for all the chosen workers.
We will denote the action set by 
$$
\mathcal{A} := \{ \bm{a} \in \mathbb{N}^n : \norm{\bm{a}}_1 = B \}~,
$$
where $\mathbb{N}$ is the set of natural numbers, including the $0$.

The objective of the allocation strategy in each round $k$ is to minimize the total computation time.
Hence, the objective is to minimize $C : \mathcal{A} \to \mathbb{R}_+$, the computation time that the optimizer waits to receive $B$ gradients using an allocation vector $\bm{a} \in \mathcal{A}$, defined as
\begin{equation}\label{eq:def_c}
	C(\bm{a}_k)
	:= \max_{i\in \text{supp}(\bm{a}_k)} \  \sum_{u=1}^{a_{i,k}} X_{i,k}^{(u)}~.
\end{equation}

%The allocation protocol is summarized in Algorithm~\ref{alg:protocol}.

% \begin{algorithm}[t]
% 	\caption{\algname{Task Allocation Protocol}}
% 	\label{alg:protocol}
% 	\begin{algorithmic}[1]
% 		\STATE \textbf{Input:} Budget $B$
% 		\FOR{$k=1,\dots, K$}
% 		\STATE Choose an action $\bm{a}_k \in \mathcal{A}:= \{ \bm{a} \in \mathbb{N}^n : \norm{\bm{a}}_1 = B \}$
% 		\STATE For  $i \in \text{supp}(\bm{a}_k)$, observe $\left(X_{i,k}^{(u)}\right)_{u \in [a_{i,k}]}$
% 		%\STATE Incurr the proxy-loss $\ell(\bm{a}_k) = \max_{i \in [n]} \{a_{i,k}\mu_i\}$.
% 		\ENDFOR
% 	\end{algorithmic}
% \end{algorithm}


% \textbf{Modeling assumptions.}
\subsection{Modeling assumptions}
We assume that the computation time of each worker $i \in [n]$ are i.i.d. drawn from a random variable $X_i$ following a probability distribution $\nu_i$.
We denote by $\bm{\mu} = (\mu_1, \dots, \mu_n)$ the vector of unknown means and by $\bm{\sigma} = (\sigma_1, \dots, \sigma_n)$ the vector of standard deviations.
Hence, the random variables $(X_{i,k}^{(u)})$ with $u \in \{1, \dots, a_{i,k}\}$ are $a_{i,k}$ i.i.d. samples drawn from $\nu_i$.

We assume that the distribution $\nu_i$ of the computation times to be sub-exponential random variables. To quantify this assumption, we recall the definition of the sub-exponential norm, also known as the Orlicz norm, for a real-valued random variable $X$:
\begin{equation}\label{eq:def_se}
	\norm{X}_{\psi_1} := \inf\{C > 0: \E{\exp(\abs{X}/C)} \le 2\}~.
\end{equation}
Hence, formally we make the following assumption.
\begin{assumption}\label{a:sube}
	Let $\alpha \geq 0$. For all $i \in [n]$, $X_{i}$ is a positive random variable and $\norm{X_{i}}_{\psi_1} \le \alpha$.
\end{assumption}
%\textbf{TODO: the random variables were defined as $X_{i,k}$ and here as $X^{(u)}_{i,k}$: pick one}
%We assume that the only prior knowledge available to the learner about the distributions $\nu_i$ is an upper bound on their maximal sub-exponential norms, denoted by $\alpha$.

The considered class encompasses several other well-known classes of distributions in the literature, such as support-bounded and sub-Gaussian distributions.
Moreover, it includes exponential distributions, which are frequently used in the literature to model waiting or computation times in queueing theory and resource allocation in large distributed systems \cite{gelenbe2010analysis, gross2011fundamentals,hadjis2016omnivore, mitliagkas2016asynchrony, dutta2018slow, nguyen2022federated}.

 
%Given the stochastic and online nature of the problem, it makes sense to minimize the \emph{expected cumulative regret} over $K \ge 1$ rounds, defined as
%\begin{equation}\label{eq:ata_obj}
%	 \mathcal{R}_K := \sum_{k=1}^{K} \E{C(\bm{a}_k)} - K \min_{\bm{a} \in \mathcal{A}}\ \E{C(\bm{a})}~.
%\end{equation}

% \textbf{Objective of the allocation algorithm.}
\subsection{Objective of the allocation algorithm}
The main objective of this work is to develop an online allocation strategy with small expected total computation time, defined as 
$$
	\mathcal{C}_K := \sum_{k=1}^{K} \E{C(\bm{a}_k)}~.
$$
If the distributions of the arms were known in advance, the optimal allocation $\bm{a}^* \in \mathcal{A}$ would be selected to minimize the expected computation time per round, $\mathbb{E}[C(\cdot)]$, and this allocation would be used consistently over $K$ rounds, leading to the optimal total computation time 
$$
	\mathcal{C}^*_K = K \E{C(\bm{a}^*)}~.
$$

Our goal is to design a strategy that ensures the computation time $\mathcal{C}_K$ remains within a small multiplicative factor of the optimal time $\mathcal{C}^*_K$, plus an additional negligible term. Specifically, we aim to satisfy
\begin{equation}\label{eq:ata_obj}
	\mathcal{C}_K \leq \gamma \cdot \mathcal{C}^*_K + \mathcal{E}_K,	
\end{equation}
where $\gamma \geq 1$ is a constant close to $1$, and $\mathcal{E}_K$ is a negligible term compared to $\mathcal{C}^*_K$ when $K\to \infty$. This would assure us that in the limit we are a constant multiplicative factor away from the performance of the optimal allocation strategy that has full knowledge of the distributions of the computational times of the workers.


%\textbf{TO DO: how the regret is connected to the actual objectives in federated learning? If not connected, minimizing regret would be unjustified. Maybe the regret is actually not what we want, instead we should directly say we want a result like the one in \Cref{cor:main}}

%\textbf{TO DO: Add something here explaining why this problem is hard: non-convexity, partial feedback, online, etc.}
%Finding a strategy minimizing the objective \eqref{eq:ata_obj} gives raise to several technical difficulties: first the set of possible actions $\mathcal{A}$ over which we optimize is discrete, the non-linearilty of the computation time functions $C(\cdot)$ prevents us from reducing the minimization of the expected regret to a convex problem. Second, the number of possible allocations $\mathcal{A}$ is of combinatorial nature, as its cardinality is of order $\binom{n+B-1}{B}$. This raises the need to identify and exploit the structure of the problem in order to develop efficient strategies. Third, the computation times of the workers are stochastic, which raises the need for strategies that can handle uncertainty. Finally, and most importantly, due to the online nature of the problem, the learner must balance exploration and exploitation in its decision making. This is due to the limited allocation budget it has at hand consisting of $B$ units, and the partial nature of the feedback, as the learner only observes the computation times of workers that were allocated some units. The last point suggests that a first step for solving this problem is to frame it in a multi-armed bandit problem.
  
%In the next section, we show how we reduce this non-convex online optimization problem to a multi-armed bandit problem.

Finding a strategy solving the objective in \eqref{eq:ata_obj} presents several technical challenges. First, the action space $\mathcal{A}$ is discrete, and the nonlinearity of the computation time functions $C(\cdot)$ prevents reducing our objective to a convex problem. Second, the size of $\mathcal{A}$ is combinatorial, growing on the order of $\binom{n + B - 1}{B}$, which necessitates exploiting the inherent problem structure to develop efficient strategies. Third, because the workers' computation times are stochastic, any solution must account for uncertainty. Finally, the online setting forces the learner to balance exploration and exploitation under a limited allocation budget of $B$ units per round and partial feedback---only the computation times of workers who receive allocations are observed. This last point naturally suggests adopting a MAB approach.

In the next section, we show how to reduce this problem to a MAB problem and how to efficiently solve it.

