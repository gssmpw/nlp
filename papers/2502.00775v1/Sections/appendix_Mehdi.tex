\section{Proofs of \Cref{thm:main}, \Cref{thm:main2}, and \Cref{cor:main}}
\label{sec:proof_1}

We start by recalling the notation. For $i\in [n]$ and $k \in [K]$, $(X^{(u)}_{i,k})_{u \in [B]}$ denote $B$ independent samples at round $k$ from distribution $\nu_i$. When using an allocation vector $\bm{a}_k \in \mathcal{A}$, the total computation time of worker $i$ at round $k$ is $\sum_{u=1}^{a_{i,k}} X^{(u)}_{i,k}$, when $a_{i,k} >0$. $\bm{\mu} = (\mu_1, \dots, \mu_K)$ is the vector of means. For each $k \in [K]$, when using the allocation vector $\bm{a}_k$, we recall the definition of the proxy loss $\ell: \mathcal{A}\times \mathbb{R}_{\ge0}^n \to \mathbb{R}_{\ge0}$ by
$$
\ell(\bm{a}_k, \bm{\lambda}) = \max_{i\in [n]} \ a_{i,k} \lambda_i,
$$
where $\bm{\lambda} = (\lambda_1, \dots, \lambda_n)$ is a vector of non-negative components. When $\bm{\lambda} = \bm{\mu}$, we drop the dependence on the second input of $\ell$.
For each $\bm{\lambda}$, let $\bar{\bm{a}}_{\bm{\lambda}}\in \mathcal{A}$, be the action minimizing this loss
$$
\bar{\bm{a}}_{\bm{\lambda}} \in \argmin_{\bm{a} \in \mathcal{A}} \ \ell(\bm{a}, \bm{\lambda})~.
$$
We drop the dependency on $\bm{\mu}$ from $\bar{\bm{a}}_{\bm{\mu}}$ to ease notation. The actual (random) computation time at round $k$ is denoted by $C: \mathcal{A} \to \mathbb{R}_+$:
\begin{equation}\label{eq:def_C}
	C(\bm{a}_k) := \max_{i\in [n]} \ \sum_{u=1}^{a_{i,k}} X_{i,k}^{(u)}~.
\end{equation}
Let $\bm{a}^*$ be the action minimizing the expected time
$$
\bm{a}^* \in \argmin_{\bm{a} \in \mathcal{A}} \ \E{C(\bm{a})}~.
$$
The expected regret after $K$ rounds is defined as follows
$$
\mathcal{R}_K := \sum_{t=1}^{K} \E{\ell(\bm{a}_{k})-\ell(\bar{\bm{a}})}~.
$$

\noindent For the remainder of this analysis we consider $\bar{\bm{a}} \in \argmin_{a \in \mathcal{A}} \ \ell(\bm{a})$ found using the \algname{RAS} procedure.
For each $i\in [n]$, recall that $k_i$ is the smallest integer such that
\begin{equation}\label{eq:def_n}
	(\bar{a}_i+k_i)\mu_i > \ell(\bar{\bm{a}})~.
\end{equation}

Below we present a technical lemma used in the proofs of Theorems~\ref{thm:main} and~\ref{thm:main2}.
\begin{lemma}
	\label{lem:1}
	Let $\bm{x}=(x_1, \dots, x_n) \in \mathbb{R}_{\ge 0}^n$. Let $\bm{a}$ be the output of $\algname{RAS}(\bm{x}; B)$. For each $i, j \in [n]$, we have
	$$ 
	a_{ j} x_j \le \left(a_{ i}+1 \right) x_i~.
	$$
\end{lemma}
%
\begin{proof}
	Fix $\bm{x} \in \mathbb{R}_+^n$, and let $\bm{a} = \algname{RAS}(\bm{x};B)$. The result is straightforward when $\min\limits_{i\in [n]}{x_i} = 0$.
	
	\noindent Suppose that $x_i >0$ for all $i \in [n]$.
	Let $s\ge 1$ denote the cardinality
	$$
	s:= \abs{\argmax_{i \in [n]} \  a_{i} x_i }~.
	$$
	Fix $i,j \in [n]$, let $k \in \argmax_{i \in [n]} \  a_{i} x_i $. We need to show that
	$$
	a_{k} x_k \le (a_{i}+1)x_i~.
	$$
	We use a proof by contradiction.
	Suppose that we have $ a_{k} x_k > (a_{i}+1)x_i$ consider the allocation vector $\bm{a}'\in \mathcal{A}$ given by $a'_k = a_{k}-1$, $a'_i = a_i+1$ and $a'_u = \bar{a}_u$ when $u \notin \{i,k\}$. Let $R := \max_{u \neq i,k} \{a_u x_u \}$. We have
	\begin{align*}
		\ell(\bm{a}',\bm{x})
		= \max_{u \in [n]} \ a'_u x_u
		= \max \{(a_i+1) x_i, (a_k-1) x_k, R \}~.
	\end{align*}
	We consider two cases:
	\begin{itemize}
		\item Suppose that $s=1$ (i.e., the only element in $[n]$ such that $a_ux_u=\ell(\bm{a}, \bm{x})$ is $k$), then we have necessarily $R< a_k x_k $. Moreover, by the contradiction hypothesis, $(a_i+1)x_i < a_k x_k$.
		Therefore,
		\begin{align*}
			\ell(\bm{a}', \bm{x}) = \max\{ (a_i+1)x_i, (a_k-1)x_k, R\}
			< 	a_k x_k = \ell(\bm{a}, \bm{x}),
		\end{align*}
		which contradicts the definition of $\bm{a}$.
		\item Suppose that $s\ge 2$, since by hypothesis $ a_k x_k > (a_i+1)x_i$, we clearly have $a_ix_i < \ell(\bm{a}, \bm{x})$ therefore among the set $[n]\setminus \{k,i\}$ there are exactly $s-1$ elements such that $a_u x_u = \ell(\bm{a}, \bm{x})$. In particular, this gives
		\begin{align*}
			\ell(\bm{a}',\bm{x})
			= \max_{u \in [n]} \ \{(a_i+1) x_i, (a_k-1) x_k, R \}
			= R = \ell(\bm{a}, \bm{x})~.
		\end{align*}
		Therefore, $\bm{a}' \in \argmin_{\bm{a} \in \mathcal{A}} \ \ell(\bm{a}, \bm{x})$ and the number of elements such that $a'_i x_i = \ell(\bm{a}', \bm{x})=\ell(\bm{a}, \bm{x})$ is at most $s-1$, which contradicts the fact that $s$ is minimal given the \algname{RAS} choice and Lemma~\ref{thm:minimal_cardinality}.
	\end{itemize}
	As a conclusion we have $a_k x_k\le (a_i+1)x_i$.
\end{proof}
\begin{remark}
	Recall that Lemma~\ref{lem:1} guarantees that $k_i$ defined in \eqref{eq:def_n} satisfy: $k_i \in \{1, 2\}$ for each $i \in [n]$.
\end{remark}


\subsection{Proof of \Cref{thm:main}}
\label{proof:thm:main}

Below we restate the theorem.

\begin{restate-theorem}{\ref{thm:main}}
	Suppose that Assumption~\ref{a:sube} holds. Let $\bar{\bm{a}} \in \argmin_{\bm{a} \in \mathcal{A}} \ell(\bm{a})$, in case of multiple optimal actions, we consider the one output by \algname{RAS} when fed with $\bm{\mu}$.
	Then, the expected regret of \algname{ATA} with inputs $(B, \alpha)$  satisfies
	$$
	\mathcal{R}_K
	\le 2n\max_{i \in [n]} \{B\mu_i -\ell(\bar{\bm{a}})\}+c \cdot\sum_{i=1}^{n} \frac{\alpha^2(\bar{a}_i+k_i)(B \mu_i - \ell(\bar{\bm{a}})) }{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}\cdot \ln K,
	$$
	where $\alpha = \max_{i \in [n]} \norm{X_i}_{\psi_1}$, and $c$ is a constant.
\end{restate-theorem}
%
\begin{proof}
	Let $K_{i,k}$ be the number of rounds where arm $i$ was queried prior to round $k$ (we take $K_{i,1}=0$). Recall that we chose the following confidence bound: if $K_{i,k} \ge 1$, then
	$$
	\text{conf}(i,k) = 4e\alpha\sqrt{\frac{ \ln(2k^2)}{K_{i,k}}}+4e\alpha\frac{ \ln(2k^2)}{K_{i,k}},
	$$
	and $\text{conf}(i,k) = \infty$ otherwise. Recall that $\hat{\mu}_{i,k}$ denotes the empirical mean of samples from $\nu_i$ observed prior to $k$ if $K_{i,k}\ge 0$ and $\hat{\mu}_{i,k}=0$ if $K_{i,k}=0$. Let $s_{i,k}$ denote the lower confidence bound used in the algorithm:
	$$
	s_{i,k} = \left(\hat{\mu}_{i,k} -\text{conf}(i,k)\right)_{+}~.
	$$
	
	\noindent We introduce the events $\mathcal{E}_{i,k}$ for $i \in [n]$ and $k \in [K]$ defined by
	$$
	\mathcal{E}_{i,k} := \left\lbrace \abs{\hat{\mu}_{i,k}-\mu_i} > \text{conf}(i,k)\right\rbrace.
	$$
	Let 
	$$
	\mathcal{E}_k = \cup_{i \in [n]} \mathcal{E}_{i,k}.
	$$
	Let us prove that for each $k \in [K]$ and $i \in [n]$: $\mathbb{P}\left(\mathcal{E}_{i,k}\right) \le \frac{1}{k^2}$, which gives using a union bound $\mathbb{P}(\mathcal{E}_k) \le \frac{n}{k^2}$. 
	Let $i \in [n]$, using \Cref{prop:concentration} and taking $\delta = 1/k^2$, we have
	\begin{align*}
		\mathbb{P}(\mathcal{E}_{i,k})
		= \mathbb{P}\{\abs{\hat{\mu}_{i,k}-\mu} > \text{conf}(i,k)\}
		\le \frac{1}{k^2}~.
	\end{align*}

	\noindent We call a ``bad round", a round $k$ where we have $\ell(\bm{a}_{k}) > \ell(\bar{\bm{a}})$. Let us upper bound the number of bad rounds. 
	
	\noindent Observe that in a bad round there is necessarily an arm $i \in [K]$ such that $a_{i,k} \mu_i > \ell(\bar{\bm{a}})$. For each $i\in [n]$, let $N_i(k)$ denote the number of rounds $q\in \{1,\dots, k\}$ where $a_{i,q} \mu_i > \ell(\bar{\bm{a}})$ and $i \in \argmax_{j \in [n]} \ a_{j,q} \mu_j$ (this corresponds to a bad round triggered by worker $q$)
	$$
	N_i(k) := \abs{\left\lbrace q \in \{1, \dots, k\}: a_{i,q}\mu_i > \ell(\bar{\bm{a}}) \text{ and } a_{i,q}\mu_i = \ell(\bm{a}_q) \right\rbrace}~.
	$$
	%This implies in particular that $a_{t,i} \ge a_i^*+n_i$ (using the definition of $n_i$). We conclude that there exists an arm $j\neq i$ such that $a_{t,j} \le a_j^*-n_i$. 
	We show that in the case of $\ell(\bm{a}_k) > \ell(\bar{\bm{a}})$, the following event will hold: there exists $i \in [n]$ such that 
	$$
	E_{i,k} := \mathcal{E}_k \text{ or }\left\lbrace N_i(k-1) \le  \frac{256e^2\alpha^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}  \right\rbrace~.
	$$
	To prove this, suppose that for each $i \in [n]$, $\neg E_{i,k}$ holds. This gives in particular
	\begin{equation}\label{eq:ni}
		N_i(k-1) > \frac{256e^2\alpha^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~.
	\end{equation}
	Observe that in each round where $N_i(\cdot)$ is incremented, the number of samples received from the distribution $\nu_i$ increases by at least $\bar{a}_{i}+k_i$. 
	Therefore, we have \eqref{eq:ni} implies
	\begin{align*}
		K_{i,k}
		> \frac{256e^2\alpha^2(\bar{a}_i+k_i)^2 \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}
		= \frac{256e^2\alpha^2 \ln(2K^2)}{\left(\mu_i - \frac{\ell(\bar{\bm{a}})}{\bar{a}_i+k_i}\right)^2}~.
	\end{align*}
	
	
	\noindent Then we have, using the expression of $\text{conf}(\cdot)$
	\begin{align*}
		2\text{conf}(i,k) &=  8e\alpha\sqrt{\frac{ \ln(2k^2)}{K_{i,k}}}+8e\alpha\frac{ \ln(2k^2)}{K_{i,k}}\\
		&\le \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) \left[ 8e\alpha \sqrt{\frac{\ln(2k^2)}{256e^2\alpha^2 \ln(2K^2)}}+ 8e\alpha \frac{\ln(2k^2)}{256e^2\ln(2K^2)\alpha^2}\left(\mu_i-\frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right)\right]\\
		&\le \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) \left[\frac{1}{2} + \frac{1}{32e\alpha} \left(\mu_i-\frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) \right]~.
	\end{align*}
	Recall that using Lemma~\ref{lem:tech1}, we have $\mu_i-\frac{\ell(\bar{\ba})}{\bar{a}_i+k_i} \le \mu_i \le \alpha$. Therefore, we have
	\begin{equation}\label{eq:conf}
		2\text{conf}(i,k) < \mu_i - \frac{\ell(\bar{\bm{a}})}{\bar{a}_i+k_i}~.
	\end{equation}
	Suppose for a contradiction argument that we have $\neg E_{i,k}$ and $\{ a_{i,k}\mu_i > \ell(\bar{\bm{a}}) \text{ and } a_{i,k} = \ell(\bm{a}_k)\}$.
	Using the definition of $k_i$ and the fact that $a_{i,k} \mu_i > \ell(\bar{\bm{a}})$, we have that $a_{i,k} \ge \bar{a}_i + k_i$. Therefore, \eqref{eq:conf} gives
	\begin{equation}\label{eq:conf2}
		2\text{conf}(i,k) < \mu_i - \frac{\ell(\bar{\bm{a}})}{a_{i,k}}~.
	\end{equation}
	Observe that in each round $\norm{\bm{a}_k}_0 = B$, therefore if we have $a_{i,k} \ge \bar{a}_i+k_i > \bar{a}_i$ for some $i$, we necessarily have that there exists $j \in [n]\setminus \{i\}$ such that $a_{j,k} \le \bar{a}_j-1$. Using the fact that $\ell(\bar{\bm{a}}) \ge \bar{a}_j \mu_j$ with \eqref{eq:conf2}, we get
	\begin{equation}\label{eq:e1}
		a_{i,k}(\mu_i-2\text{conf}(i,k)) > \bar{a}_j \mu_j~.
	\end{equation}
	Since both $\neg \mathcal{E}_{i,k}$ and $\neg \mathcal{E}_{j,k}$ hold (because $\neg E_{i,k}$ implies $\neg \mathcal{E}_k$), we have that
	\begin{align}
		\mu_i - 2\text{conf}(i,k) &\le \hat{\mu}_{i, k} - \text{conf}(i,k)
		\le s_{i,k},\label{eq:mu2}
	\end{align} 
	and $\mu_j \ge \hat{\mu}_{j,k} - \text{conf}(j,k)$.
	Recall that $\mu_j \ge 0$, therefore
	\begin{align}
		\mu_j %&\ge \hat{\mu}_{j,k} - \text{conf}(j,k)\\
		\ge \left(\hat{\mu}_{j,k} - \text{conf}(j,k)\right)_{+}
		= s_{j,k}~.\label{eq:mu3}
	\end{align}
	Using the bounds \eqref{eq:mu2} and \eqref{eq:mu3} in \eqref{eq:e1}, we have
	$$
	a_{i,k} s_{i,k} > \bar{a}_j s_{j,k} \ge (a_{j,k}+1) s_{j,k},
	$$
	where we used the definition of $j$ in the second inequality.
	This contradicts the statement of Lemma~\ref{lem:1}, which concludes the contradiction argument. Therefore, the event that $k$ is a bad round implies that $E_{i,k}$ holds for at least one $i\in [n]$.
	We say that a bad round was triggered by arm $i$, a round where $N_i(\cdot)$ was incremented. 
	Observe that if $k \in [K]$ is not a bad round then $\E{\ell(\bm{a}_k)}-\ell(\bar{\bm{a}})=0$, otherwise if $k$ is a bad round triggered by $i \in [n]$ then $\E{\ell(\bm{a}_{k})}-\ell(\bar{\bm{a}}) \le B\mu_i-\ell(\bar{\bm{a}})$.
	To ease notation we introduce for $i\in [n]$
	$$
	H_i := \frac{256e^2\alpha^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~.
	$$
	The expected regret satisfies
	\begin{align*}
		\mathcal{R}_K &= \sum_{i=1}^{K} \mathbb{E}\left[\ell(\bm{a}_k)-\ell(\bar{\bm{a}})\right]\\
		&\le \sum_{i=1}^{n} (B\mu_i-\ell(\bar{\bm{a}}))\mathbb{E}[N_i(K)]\\ 
		&= \sum_{i=1}^{n}\sum_{k=1}^{K} (B\mu_i-\ell(\bar{\bm{a}}))\mathbb{E}\left[\mathds{1}(k \text{ is a bad round triggered by }i)\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{t=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n}(B\mu_i-\ell(\bar{\bm{a}}))\sum_{k=1}^{K} \mathbb{E}\left[\mathds{1}(k \text{ is a bad round triggered by }i) \mid \neg \mathcal{E}_k\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{t=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n}(B\mu_i-\ell(\bar{\bm{a}}))\sum_{k=1}^{K} \mathbb{E}\left[\mathds{1}(N_i(k)=1+N_i(k-1) \text{ and } N_i \le H_i ) \mid \neg \mathcal{E}_k\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{k=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n} (B\mu_i-\ell(\bar{\bm{a}}))H_i\\
		&\le 2n \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}+  \sum_{i=1}^{n} \frac{256e^2\alpha^2 (\bar{a}_i+k_i)(B\mu_i-\ell(\bar{\bm{a}})) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~. \qedhere
	\end{align*}
\end{proof}


\subsection{Proof of \Cref{thm:main2}}
\label{proof:thm:main2}

\begin{restate-theorem}{\ref{thm:main2}}
	Suppose that Assumption~\ref{a:sube} holds. Let $\bar{\bm{a}} \in \argmin_{\bm{a} \in \mathcal{A}} \ell(\bm{a})$, in case of multiple optimal actions, we consider the one output by \algname{RAS} when fed with $\bm{\mu}$.
Then, the expected regret of \algname{ATA-Empirical} with the empirical confidence bounds using the inputs $(B, \eta)$  satisfies
\begin{align*}
	\mathcal{R}_K &\le 2n\max_{i \in [n]} \{B\mu_i -\ell(\bar{\bm{a}})\}
	 +c \cdot\sum_{i=1}^{n} \frac{(1+\eta^2) \alpha_i^2(\bar{a}_i+k_i)(B \mu_i - \ell(\bar{\bm{a}})) }{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}\cdot \ln K,
\end{align*}
where $\alpha_i =  \norm{X_i}_{\psi_1}$, and $c$ is a constant.
\end{restate-theorem}
%
\begin{proof}
	We build on the techniques used in the proof of \Cref{thm:main}. Recall the expression of $\xi$:
	$$
	\xi = \frac{1+\sqrt{4\eta^2+5}}{2}~.
	$$
	Define the quantities $C_{i,k}$ by
	$$
	C_{i,k} = 4e \sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+4e \frac{\ln(2k^2)}{K_{i,k}}~.
	$$
	Recall that the lower confidence bounds used here are defined as
	$$
	\hat{s}_{i,k} = \hat{\mu}_{i,k} \left(1-\xi C_{i,k}\right)_{+}~.
	$$
	We additionally define the following quantities
	\begin{equation*}
		\hat{u}_{i,k} := \hat{\mu}_{i,k} \left(1+\frac{4}{3}\xi C_{i,k}\right)~.
	\end{equation*}
	\noindent We introduce the events $\mathcal{E}_{i,k}$ for $i \in [n]$ and $k \in [K]$ defined by
	$$
	\mathcal{E}_{i,k} := \left\lbrace \abs{\mu_i - \hat{\mu}_{i,k}} \le \alpha_i C_{i,k}\right\rbrace~.
	$$
	Let 
	$$
	\mathcal{E}_k = \cup_{i \in [n]} \mathcal{E}_{i,k}~.
	$$
	We have using Proposition~\ref{prop:concentration} for each $k \in [K]$ and $i \in [n]$: $\mathbb{P}\left(\mathcal{E}_{i,k}\right) \le \frac{1}{k^2}$, which gives using a union bound $\mathbb{P}(\mathcal{E}_k) \le \frac{n}{k^2}$. 
	Moreover, following Lemma~\ref{lem:conc2}, for each $i \in [n]$ and $k \in [K]$, we have that $\mathcal{E}_{i,k}$ implies
	\begin{equation}\label{eq:lcb}
		\mu_i \ge \hat{s}_{i,k}~.
	\end{equation}
	
	\noindent Following similar steps as in the proof of Theorem~\ref{thm:main}, we call a ``bad round", a round $k$ where we have $\ell(\bm{a}_{k}) > \ell(\bar{\bm{a}})$. Let us upper bound the number of bad rounds. 
	
	\noindent Observe that in a bad round there is necessarily an arm $i \in [K]$ such that $a_{i,k} \mu_i > \ell(\bar{\bm{a}})$. For each $i\in [n]$, let $N_i(k)$ denote the number of rounds $q\in \{1,\dots, k\}$ where $a_{i,q} \mu_i > \ell(\bar{\bm{a}})$ and $i \in \argmax_{j \in [n]} \{ a_{j,q} \mu_j\}$ (this corresponds to a bad round triggered by worker $q$):
	$$
	N_i(k) := \abs{\left\lbrace q \in \{1, \dots, k\}: a_{i,q}\mu_i > \ell(\bar{\bm{a}}) \text{ and } a_{i,q}\mu_i = \ell(\bm{a}_q) \right\rbrace}~.
	$$
	%This implies in particular that $a_{t,i} \ge a_i^*+n_i$ (using the definition of $n_i$). We conclude that there exists an arm $j\neq i$ such that $a_{t,j} \le a_j^*-n_i$. 
	We show that in the case of $\ell(\bm{a}_k) > \ell(\bar{\bm{a}})$, the following event will hold: there exists $i \in [n]$ such that 
	$$
	E_{i,k} := \mathcal{E}_k \text{ or }\left\lbrace N_i(k-1) \le  \frac{1024e^2 \xi^2\alpha_i^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}  \right\rbrace~.
	$$
	To prove this, suppose for a contradiction argument that we have for each $i \in [n]$ $\neg E_{i,k}$. This gives in particular
	\begin{equation}\label{eq:ni2}
		N_i(k-1) >  \frac{1024e^2 \xi^2\alpha_i^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~.
	\end{equation}
	Observe that in each round where $N_i(\cdot)$ is incremented, the number of samples received from the distribution $\nu_i$ increases by at least $\bar{a}_{i}+k_i$. 
	Therefore, we have \eqref{eq:ni2} implies
	\begin{align*}
		K_{i,k}
		>  \frac{1024e^2 \xi^2\alpha_i^2 (\bar{a}_i+k_i)^2 \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}
		=  \frac{1024e^2 \xi^2\alpha_i^2  \ln(2K^2)}{\left(\mu_i - \frac{\ell(\bar{\bm{a}})}{\bar{a}_i+k_i}\right)^2}~.
	\end{align*}
	Therefore, we have
	\begin{align*}
		C_{i,k} &= 4e \sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+4e \frac{\ln(2k^2)}{K_{i,k}}\\
		&\le \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) \left[4e \sqrt{\frac{\ln(2k^2)}{1024e^2 \xi^2 \alpha_i^2 \ln(2K^2)}}+ \frac{4e \ln(2k^2)\left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right)}{1024e^2\xi^2\alpha_i^2 \ln(2K^2)} \right]  \\
		&\le \frac{1}{4\xi \alpha_i} \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) \left[ \frac{1}{2}+ \frac{\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}}{256\xi \alpha_i}\right]~.
	\end{align*}
	Using Lemma~\ref{lem:tech1}, we have $\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i} \le \mu_i \le \alpha_i$. Moreover, by definition of $\xi$, we have $\xi \ge 1$. We conclude using the bound above that
	\begin{equation}\label{eq:Ci}
		C_{i,k} \le \frac{3}{20\xi \alpha_i}\left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right)~.
	\end{equation}
	\noindent Recall that since $\neg \mathcal{E}_{k}$ holds, in particular $\neg \mathcal{E}_{i,k}$ holds, which gives
	\begin{align*}
		2\hat{\mu}_{i,k} -2\hat{s}_{i,k} &= 2\hat{\mu}_{i,k} \left(1-\left(1-\xi C_{i,k}\right)_{+}\right)\\
		&\le 2\hat{\mu}_{i,k} \left(1-\left(1-\xi C_{i,k}\right)\right)\\
		&= 2\xi C_{i,k}\hat{\mu}_{i,k}\\
		&\le 2\xi C_{i,k} (\mu_i+\alpha_i C_{i,k})\\
		&\le 2\xi \alpha_i C_{i,k} (1+ C_{i,k}),
		%&\le 2\mu_i -2\hat{\mu}_{i,k}\nonumber\\
		%&\le 16e\alpha_i \sqrt{\frac{\ln(2k^2)}{K_{i,k}}}+16e\alpha_i \frac{\ln(2k^2)}{K_{i,k}} \nonumber\\
		%&\le \mu_i - \frac{\ell(\bar{\bm{a}})}{\bar{a}_i+k_i}. \label{eq:Ki2}
	\end{align*}
	where we used the event $\neg \mathcal{E}_{i,k}$ in the penultimate inequality, and $\mu_i \le \alpha_i$ as showed in Lemma~\ref{lem:tech1} in the last inequality.
	Using bound \eqref{eq:Ci} in the previous display gives
	\begin{align}
		2\hat{\mu}_{i,k} -2\hat{s}_{i,k} &\le \frac{3}{10} \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right) (1+C_{i,k})
		\le \frac{3}{8} \left(\mu_i - \frac{\ell(\bar{\ba})}{\bar{a}_i+k_i}\right), \label{eq:conf22}
	\end{align} 
	where we used in the last line the fact that following \eqref{eq:Ci}: $C_i \le \frac{3}{20\xi \alpha_i}(\mu_i - \ell(\bar{\ba})/(\bar{a}_i+k_i)) \le 3/20$, since $\xi \ge 1$ by definition and $\alpha_i \ge \mu_i \ge \mu_i - \ell(\bar{\ba})/(\bar{a}_i+k_i)$ following Lemma~\ref{lem:tech1}.
	
	\noindent Recall that \eqref{eq:Ci} implies in particular that $C_{i,k} \le \frac{3\mu_i}{20\xi \alpha_i} \le 3/(20\xi)$. Since $\neg \mathcal{E}_{i,k}$ is true, we have  $\abs{\hat{\mu}_{i,k} - \mu_i} \le \alpha_i C_{i,k}$. Therefore, using \Cref{lem:conc2}, we have
	\begin{align}
		\mu_i &\le \hat{\mu}_{i,k} \left(1 + \frac{4}{3} \xi C_{i,k}\right)\label{eq:ucb0}\\
		&\le \frac{21}{20}~\hat{\mu}_{i,k}~. \label{eq:ucb}
	\end{align}
	
	Observe that in each round $\norm{\bm{a}_k}_0 = B$, therefore if we have $a_{i,k} \ge \bar{a}_i+k_i > \bar{a}_i$ for some $i$, we necessarily have that there exists $j \in [n]\setminus \{i\}$ such that $a_{j,k} \le \bar{a}_j-1$. Using the fact that $\ell(\bar{\bm{a}}) \ge \bar{a}_j \mu_j$ with \eqref{eq:conf22}, we get
	$$
	5\hat{\mu}_{i,k} -5~\hat{s}_{i,k} < \mu_i - \frac{\ell(\bar{\ba})}{a_{i,k}}~.
	$$
	Therefore, we obtain
	\begin{equation}\label{eq:e12}
		a_{i,k}(\mu_i+ 5\hat{s}_{i,k}-5\hat{\mu}_{i,k}) > \ell(\bar{\ba}) \ge \bar{a}_j \mu_j~.
	\end{equation}
	Since both $\neg \mathcal{E}_{i,k}$ and $\neg \mathcal{E}_{j,k}$ hold (because $\neg E_{i,k}$ implies $\neg \mathcal{E}_k$), we have that
	\begin{align*}
		\mu_i +5\hat{s}_{i,k}-5\hat{\mu}_{i,k} &=  \hat{s}_{i,k} + \mu_i - \hat{\mu}_{i,k} + 4 \left(\hat{s}_{i,k} - \hat{\mu}_{i,k} \right)\\
		&= \hat{s}_{i,k} + \mu_i - \hat{\mu}_{i,k} + 4\hat{\mu}_{i,k} \left((1-\xi C_{i,k})_{+}-1 \right)\\
		&\le \hat{s}_{i,k} + \mu_i - \hat{\mu}_{i,k} - 4\hat{\mu}_{i,k} \xi C_{i,k}\\
		&\le \hat{s}_{i,k} + \mu_i - \hat{u}_{i,k}\\
		&\le \hat{s}_{i,k},
	\end{align*} 
	where we used in the last line the bound \eqref{eq:ucb0}. Since $\neg \mathcal{E}_{j,k}$ holds, we also have
	\begin{equation*}
		\mu_j \ge \hat{s}_{j,k}~.
	\end{equation*}
	Using the two last bounds in \eqref{eq:e12}, we have
	$$
	a_{i,k} \hat{s}_{i,k} > \bar{a}_j \hat{s}_{j,k} \ge (a_{j,k}+1) \hat{s}_{j,k},
	$$
	where we used the definition of $j$, as an arm satisfying $\bar{a}_j \ge 1+a_{j,k}$, in the second inequality.
	This contradicts the statement of Lemma~\ref{lem:1}, which concludes the contradiction argument. Therefore, the event that $k$ is a bad round implies that $E_{i,k}$ holds for at least one $i\in [n]$.
	We say that a bad round was triggered by arm $i$, a round where $N_i(\cdot)$ was incremented. 
	Observe that if $k \in [K]$ is not a bad round then $\E{\ell(\bm{a}_k)}-\ell(\bar{\bm{a}})=0$, otherwise if $k$ is a bad round triggered by $i \in [n]$ then $\E{\ell(\bm{a}_{k})}-\ell(\bar{\bm{a}}) \le B\mu_i-\ell(\bar{\bm{a}})$.
	To ease notation we introduce for $i\in [n]$
	$$
	H_i := \frac{1024e^2 \xi^2\alpha_i^2 (\bar{a}_i+k_i) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~.
	$$
	The expected regret satisfies
	\begin{align*}
		\mathcal{R}_K &= \sum_{i=1}^{K} \mathbb{E}\left[\ell(\bm{a}_k)-\ell(\bar{\bm{a}})\right]\\
		&\le \sum_{i=1}^{n} (B\mu_i-\ell(\bar{\bm{a}}))\mathbb{E}[N_i(K)]\\ 
		&= \sum_{i=1}^{n}\sum_{k=1}^{K} (B\mu_i-\ell(\bar{\bm{a}}))\mathbb{E}\left[\mathds{1}(k \text{ is a bad round triggered by }i)\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{t=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n}(B\mu_i-\ell(\bar{\bm{a}}))\sum_{k=1}^{K} \mathbb{E}\left[\mathds{1}(k \text{ is a bad round triggered by }i) \mid \neg \mathcal{E}_k\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{t=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n}(B\mu_i-\ell(\bar{\bm{a}}))\sum_{k=1}^{K} \mathbb{E}\left[\mathds{1}(N_i(k)=1+N_i(k-1) \text{ and } N_i \le H_i ) \mid \neg \mathcal{E}_k\right]\\
		&\le \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}\cdot\sum_{k=1}^{K} \mathbb{P}(\mathcal{E}_k)+ \sum_{i=1}^{n} (B\mu_i-\ell(\bar{\bm{a}}))H_i\\
		&\le 2n \max_{i\in [n]}\{(B\mu_i-\ell(\bar{\bm{a}}))\}+  \sum_{i=1}^{n} \frac{1024e^2 \xi^2\alpha_i^2 (\bar{a}_i+k_i)(B \mu_i - \ell(\bar{\ba})) \ln(2K^2)}{\left((\bar{a}_i+k_i)\mu_i - \ell(\bar{\bm{a}})\right)^2}~. \qedhere
	\end{align*}
\end{proof}













\subsection{Proof of \Cref{cor:main}}
\label{sec:proof_2}

Let us first restate the theorem.
\begin{restate-theorem}{\ref{cor:main}}
	Suppose \Cref{a:sube} holds and let $\eta := \max_{i \in [n]} \frac{\sigma_i}{\mu_i}$.
	Then, the total expected computation time after $K$ rounds, using the allocation prescribed by \algname{ATA} with inputs $(B, \alpha)$ satisfies
	$$
	\mathcal{C}_K \le \left(1+\eta\sqrt{\ln(B)}\right)\mathcal{C}_K^* + \mathcal{O}(\ln K)~.
	$$
\end{restate-theorem}
%
\begin{proof}
Let $\mathbb{E}_k$ be the expectation with respect to the variables observed up to and including $k$ and $\mathcal{F}_k$ the corresponding filtration. Using the tower rule, we have
$$
\sum_{k=1}^{K}\mathbb{E}\left[C(\bm{a}_k)\right] = \mathbb{E}\left[ \sum_{k=1}^{K} \mathbb{E}_{k-1}[C(\bm{a}_k)]\right].
$$
Consider round $k \in [K]$, let us upper bound $\mathbb{E}_{k-1}[C(\bm{a}_t)]$ using $\mathbb{E}_{k-1}[\ell(\bm{a}_k)]$. We have (recall that $\bm{a}_k \in \mathcal{F}_{k-1}$)
\begin{align*}
	\mathbb{E}_{k-1}\left[C(\bm{a}_k) \right] &= \mathbb{E}_{k-1}\left[ \max_{i \in \text{supp}(\bm{a}_k)}\left\lbrace \sum_{u=1}^{a_{i,k}} X^{(u)}_{i,k}  \right\rbrace\right]\\
	&\le \max_{i \in \text{supp}(\bm{a}_k)}\left\lbrace a_{i,k} \mu_i\right\rbrace + \max_{i \in \text{supp}(\bm{a}_k)} \{ a_{i,k} \sigma_i\} \cdot \sqrt{\ln B}\\
	&\le \max_{i \in \text{supp}(\bm{a}_k)}\left\lbrace a_{i,k} \mu_i\right\rbrace + \max_{i \in \text{supp}(\bm{a}_k)} \{ a_{i,k}\, \eta\mu_i\} \cdot \sqrt{\ln B}\\
	&= \left(1+\eta \sqrt{\ln(B)}\right)\max\left\lbrace a_{i,k} \mu_i\right\rbrace.
\end{align*}
Moreover, using Jensen's inequality, we have
\begin{align*}
	\max_{i \in [n]} \{a^*_i \mu_i\}
	\le \mathbb{E}\left[\max_{i \in [n]} \left\lbrace \sum_{u=1}^{a_{k,i}} X^{(u)}_{i,k} \right\rbrace \right]
	= \mathbb{E}[C(\bm{a}^*)]~.
\end{align*}

Using the last two bounds with the result of \Cref{thm:main}, we get the result.

\end{proof}



\section{Technical Results}
\label{sec:technical}

We consider the following concentration inequality for sub-exponential variables by \citet{maurer2021concentration}.

\begin{proposition}[Proposition 7 \citep{maurer2021concentration}
	]\label{prop:concentration}
	Suppose $X_1, \dots, X_n$ are positive i.i.d variables such that $\norm{X_1}_{\psi_1} < \infty$ and $\mu = \mathbb{E}[X_1]$. Let $\delta >0$, with probability at least $1-\delta$
	$$
	\abs{\frac{1}{n}\sum_{i=1}^{n}X_i - \mu} \le 4e\norm{X_1}_{\psi_1} \sqrt{\frac{\ln(2/\delta)}{n}}+4e\norm{X_1}_{\psi_1} \frac{\ln(2/\delta)}{n}~.
	$$ 
\end{proposition}


\begin{lemma}
	Let $X_1, \dots, X_n$ be a sequence of nonnegative  random variables. Such that $\mathbb{E}[X_i]=\mu_i$ and $\text{Var}(X_i) = \sigma_i^2$ for each $i \in [n]$. Then we have
	$$
	\mathbb{E}[\max\{X_1, \dots, X_n\}] \le \max\{\mu_1, \dots, \mu_n\}+\max_{i\in [n]} \{\sigma_i \}\cdot \sqrt{\ln n}~.
	$$  
\end{lemma}


\begin{lemma}\label{lem:tech1}
	Let $X$ be a positive random variable with mean $\mu := \mathbb{E}[X]>0$ and variance $\sigma^2 = \text{Var}(X)$. Then the sub-exponential norm of $X$ satisfies
	$$
	\norm{X}_{\psi_1} \le \frac{1+\sqrt{4\eta^2+5}}{2}\cdot \mu,
	$$
	where $\eta := \frac{\sigma}{\mu}$.
	Moreover, we have
	$$
	\mu \le \norm{X}_{\psi_1}~.
	$$
\end{lemma}
%
\begin{proof}
	Let $\alpha = \norm{X}_{\psi_1}$, $\sigma := \sqrt{\text{Var}(X)}$, $\mu := \mathbb{E}[X]$, and $\eta := \frac{\sigma}{\mu}$. We aim to prove that
	$$
	\alpha \le \frac{1+\sqrt{4\eta^2+5}}{2}\cdot \mu~.
	$$
	For $\epsilon \in (0, \alpha/2)$, we have by definition of $\alpha$
	$$
	\mathbb{E}[\exp(X/(\alpha-\epsilon))] \ge 2~.
	$$
	Recall that we have for any $x\ge 0: \exp(x) \le 1+x+\frac{x^2}{2}e^{x}$, therefore
	$$
	\mathbb{E}\left[\exp(X/(\alpha-\epsilon)) \right] \le 1+ \frac{\mu}{\alpha-\epsilon}+ \frac{\mathbb{E}[X^2]}{2(\alpha-\epsilon)^2} \mathbb{E}[\exp(X/(\alpha-\epsilon))]~.
	$$
	Therefore,
	$$
	1+ \frac{\mu}{\alpha-\epsilon}+ \frac{\mathbb{E}[X^2]}{2(\alpha-\epsilon)^2} \mathbb{E}[\exp(X/(\alpha-\epsilon))] \ge 2~.
	$$
	Taking $\epsilon \to 0$, by continuity we have
	$$
	1+ \frac{\mu}{\alpha}+ \frac{\mathbb{E}[X^2]}{2\alpha^2} \mathbb{E}[\exp(X/\alpha)] \ge 2~.
	$$
	Therefore,
	$$
	1+ \frac{\mu}{\alpha}+ \frac{\mathbb{E}[X^2]}{\alpha^2} \ge 2~.
	$$
	Solving the last inequality gives
	$$
	\alpha \le \frac{\mu + \sqrt{\mu^2+4\mathbb{E}[X^2]}}{2},
	$$
	and using $\mathbb{E}[X^2] = \sigma^2+\mu^2 = (1+\eta^2)\mu^2$, we get
	$$
	\alpha \le \frac{1+\sqrt{4\eta^2+5}}{2}\mu~.
	$$
	The second bound is a direct consequence of Jensen's inequality and the definition of $\norm{X}_{\psi_1}$.
\end{proof}

\begin{lemma}\label{lem:conc2}
	Consider the notation in Lemma~\ref{prop:concentration} and Lemma~\ref{lem:tech1}. Define $C_{\cdot, \cdot}$, $F(\cdot, \cdot)$ and $G(\cdot, \cdot)$ by:
	\begin{align*}
		C_{n,\delta} &:= 4e  \sqrt{\frac{\ln(2/\delta)}{n}}+4e\cdot \frac{\ln(2/\delta)}{n}\\
		F(n, \delta) &:= \hat{X}_n \left(1 - \xi C_{n, \delta}\right)_+\\	
		G(n, \delta) &:=  \hat{X}_n \left(1 + \frac{4}{3}\xi C_{n,\delta}\right)~,
	\end{align*}
	where we use the notation $(a)_+ = \max\{0,a\}$.
	Then, if
	$$
	\abs{\hat{X}_n - \mu} \le \alpha C_{n,\delta},
	$$
	we have
	$$
	\mu \ge F(n, \delta)~.
	$$
	Moreover, if we have additionally $ C_{n, \delta} \le \frac{1}{4\xi}$, then
	$$
	\mu \le G(n,\delta)~.
	$$
\end{lemma}
%
\begin{proof}
	Fix $n, \delta$. 
	Suppose that
	\begin{equation}\label{eq:conc}
		\abs{\hat{X}_n - \mu} \le \alpha C_{n, \delta}~.
	\end{equation}
	\textbf{Proof of $\mu \ge F(n, \delta)$:}
	we have that if $\xi C_n \ge 1$ then $F(n, \delta) = 0$ and the result is straightforward. Suppose that $\xi C_n < 1$, if $\hat{X}_n \le \mu$, we have that $F(n, \delta) = \hat{X}_n (1-\xi C_n) \le \mu$, if $\hat{X}_n \ge \mu$, we have using \eqref{eq:conc} with the bound of \Cref{lem:tech1}
	\begin{align}
		\label{eq:conc2}
		\abs{\hat{X}_n - \mu} &\le \alpha C_{n, \delta}
		\le	\mu\xi\cdot C_{n, \delta}.
	\end{align}
	Therefore, when $\hat{X}_n \ge \mu$, we have
	\begin{align*}
		F(n, \delta) &= \hat{X}_n \left(1-\xi C_{n, \delta}\right)
		= \mu+ \abs{\hat{X}_n - \mu} - \hat{X}_n \xi C_{n, \delta}
		\le  \mu+ \abs{\hat{X}_n - \mu} - \mu \xi C_{n, \delta}
		\le \mu~.
	\end{align*}
	\textbf{Proof of $\mu \le G(n,\delta)$:} Suppose that $C_{n,\delta} \le \frac{1}{4\xi}$. 
	Therefore, \eqref{eq:conc2} gives that $\hat{X}_n \ge \frac{3}{4}\mu$. Using \eqref{eq:conc2} again gives
	\begin{align*}
		\mu &\le \hat{X}_n+ \mu \xi \cdot C_{n,\delta}
		\le \hat{X}_n+  \frac{4}{3}\xi\hat{X}_n\cdot C_{n,\delta}
		= G(n,\delta)~. \qedhere
	\end{align*}
\end{proof}


%\begin{lemma}\label{lem!KL}
%	Let $X$ and $Y$ be two sub-exponential distributions with parameters $x$ and $y=x+\epsilon$ respectively, for $\epsilon \ge 0$. Then we have:
%	$$
%	\text{KL}\left(Y; X\right) \le \frac{\epsilon^2}{2x^2},
%	$$
%	where $\text{KL(.,.)}$ denotes the Kullback-Leibler divergence between $Y$ and $X$.
%\end{lemma}
%\begin{proof}
%	Using the definition of KL divergence between $X$ and $Y$ given the expression of the exponential distribution densities, we have:
%	\begin{align*}
%		\text{KL}\left(Y; X\right) &\le \ln\frac{x}{x+\epsilon}+\frac{x+\epsilon}{x}-1\\
%		&\le \frac{\epsilon^2}{2x^2}.
%	\end{align*}
%	
%\end{proof}
