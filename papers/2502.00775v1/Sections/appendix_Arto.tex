\section{Concrete optimization methods}
\label{section:other_methods}

In this section, we provide concrete examples of optimization algorithms using the \algname{ATA} and \algname{GTA} allocation strategies.

For optimization problems, we focus on \algname{SGD} and \algname{Asynchronous SGD}.
Other methods, such as stochastic proximal point methods and higher-order methods, can be developed in a similar fashion.

\subsection{Stochastic Gradient Descent}

For \algname{SGD}, it is important to distinguish homogeneous and heterogeneous cases.
Let us start from the homogeneous case.

\subsubsection{Homogeneous regime}

Consider the problem of finding an approximate stationary point of the optimization problem
\begin{equation}
    \label{eq:homo_problem}
    \min_{\bm{x} \in \R^d} \ \left\{f(\bm{x}) \eqdef \ExpSub{\bm{\xi} \sim {\cal D}}{f(\bm{x};\bm{\xi})}\right\}.
\end{equation}
We assume that each worker is able to compute stochastic gradient $f(\bm{x};\bm{\xi})$ satisfying $\mathbb{E}_{\bm{\xi} \sim {\cal D}}\left[ \|f(\bm{x};\bm{\xi}) - \nabla f(\bm{x})\|^2\right] \leq \sigma^2$ for all $\bm{x}\in \R^d$.

In this case, \algname{SGD} with allocation budget $B$ becomes \algname{Minibatch SGD} with batch size $B$. The next step is determining how the batch is collected. For \algname{ATA}, we refer to this method as \algname{SGD-ATA}, as described in \Cref{alg:sgd-ata}.

\begin{algorithm}[H]
	\caption{\algname{SGD-ATA} (Homogeneous)}
    \label{alg:sgd-ata}
	\begin{algorithmic}[1]
		\STATE \textbf{Optimization inputs}: initial point $\bm{x}_0 \in \R^d$, stepsize $\gamma > 0$
        \STATE \textbf{Allocation inputs}: allocation budget  $B$
        \STATE \textbf{Initialize}: empirical means $\hmu_{i,1} = 0$, usage counts $K_{i,1} = 0$, and usage times $T_{i,1} = 0$, for all $i \in [n]$
		\FOR{$k = 1,\ldots, K$}
        \STATE Compute LCBs $(s_{i,k})$ for all $i \in [n]$
        \STATE Find allocation:
        $
            \bm{a}_k \in  \argmin_{\ba \in \mathcal{A}} \ell(\ba, \bm{s}_k)~.
        $
		\STATE Allocate $a_{i,k}$ tasks to each worker $i \in [n]$
        \STATE Update $\bm{x}$:
        $$
            \bm{x}_{k+1} = \bm{x}_k - \frac{\gamma}{B} \sum_{i=1}^n \sum_{j=1}^{a_{i,k}} \nabla f\(\bm{x}_k;\bm{\xi}_i^j\)
        $$
        \FOR{$i$ such that $a_{i,k} \neq 0$}
        \STATE $K_{i,k+1} = K_{i,k} + a_{i,k}$
        \STATE $T_{i,k+1} = T_{i,k} + \sum_{j=1}^{a_{i,k}} X_{i,k}^{(j)}$
        \STATE $\hmu_{i,k+1} = \frac{T_{i,k+1}}{K_{i,k+1}}$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

In this case, each task consists in calculating the gradient using the device's local data, which is assumed to have the same distribution as the data on all other devices. Because of this, it does not matter which device performs the task. The method then averages these gradients to obtain an unbiased gradient estimator and performs a gradient descent step.


Now, let us give the version of \algname{Minibatch SGD} using greedy allocation \Cref{alg:sgd-gta}.

\begin{algorithm}[H]
	\caption{\algname{SGD-GTA} (Homogeneous)}
    \label{alg:sgd-gta}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: initial point $\bm{x}_0 \in \R^d$, stepsize $\gamma > 0$, allocation budget $B$
		\FOR{$k = 1,\ldots, K$}
        \STATE $b=0$
        \STATE Query single gradient from each worker $i \in [n]$ 
        \WHILE{$b<B$}
        \STATE Gradient $\nabla f(\bm{x}_k; \bm{\xi}_{k_b})$ arrives from worker $i_{k_b}$
        \STATE $\bm{g}_k = \bm{g}_k + \nabla f(\bm{x}_k; \bm{\xi}_{k_b})$; $\; b= b+1$
        \STATE Query gradient at $\bm{x}_k$ from worker $i_{k_b}$ \\ 
        \ENDWHILE
        \STATE Update the point: $\bm{x}_{k+1} = \bm{x}_k - \gamma \frac{\bm{g}^k}{B}$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\Cref{alg:sgd-gta} is exactly \algname{Rennala SGD} method proposed by \citet{tyurin2024optimal}, which has optimal time complexity when the objective function is non-convex and smooth.

If the computation times are deterministic, then \algname{GTA} makes the same allocation in each iteration. In that case, \algname{SGD-ATA} will converge to this fixed allocation. If the times are random, the allocation found by \algname{GTA} may vary in each iteration. In this case, \algname{SGD-ATA} will approach the best allocation for the expected times.

\subsubsection{Heterogeneous regime}
Now let us consider the following heterogeneous problem
$$
    \min_{x \in \mathbb{R}^d} \ \left\{ f(\bm{x}) := \frac{1}{n} \sum_{i=1}^{n} \ExpSub{\bm{\xi}_i \sim \mathcal{D}_i}{f_i(\bm{x}; \bm{\xi}_i)} \right\}~.
$$
Here each worker $i$ has its own data distribution $\cD_i$.

We start with the greedy allocation.
The algorithm is presented in \Cref{alg:sgd-gta-hetero}.

\begin{algorithm}[H]
	\caption{\algname{SGD-GTA} (Heterogeneous)}
    \label{alg:sgd-gta-hetero}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: initial point $\bm{x}_0 \in \R^d$, stepsize $\gamma > 0$, parameter $S$
		\FOR{$k = 1,\ldots, K$}
        \STATE $s_i=0$ and $\bm{g}_{i,k} = \bm{0}$ for all $i \in [n]$
        \STATE Query single gradient from each worker $i \in [n]$ 
        \WHILE{$\(\frac{1}{n} \sum_{i=1}^n \frac{1}{s_i}\)^{-1}<\frac{S}{n}$}
        \STATE Gradient $\nabla f_{j}(\bm{x}_{k}; \bm{\xi}_{k})$ arrives from worker $j$
        \STATE $\bm{g}_{j,k} = \bm{g}_{j,k} + \nabla f_{j}(\bm{x}_{k}; \bm{\xi}_{k})$; $\; s_j = s_j+1$
        \STATE Query gradient at $\bm{x}_{k}$ from worker $j$ \\ 
        \ENDWHILE
        \STATE Update the point: $\bm{x}_{k+1} = \bm{x}_k - \gamma \frac{1}{n} \sum_{i=1}^n \frac{1}{s_i} \bm{g}_{i,k}$
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\Cref{alg:sgd-ata-hetero} presents the \algname{Malenia SGD} algorithm, proposed by \citet{tyurin2024optimal}, which is also optimal for non-convex smooth functions.

In each iteration, \Cref{alg:sgd-gta-hetero} receives at least one gradient from each worker.
Building on this idea, we design a method incorporating \algname{ATA}, given in \Cref{alg:sgd-ata-hetero}.

\begin{algorithm}[H]
	\caption{\algname{SGD-ATA} (Heterogeneous)}
    \label{alg:sgd-ata-hetero}
	\begin{algorithmic}[1]
		\STATE \textbf{Optimization inputs}: initial point $\bm{x}_0 \in \R^d$, stepsize $\gamma > 0$
        \STATE \textbf{Allocation inputs}: allocation budget  $B$
        \STATE \textbf{Initialize}: empirical means $\hmu_{i,1} = 0$, usage counts $K_{i,1} = 0$, and usage times $T_{i,1} = 0$, for all $i \in [n]$
		\FOR{$k = 1,\ldots, K$}
        \STATE Compute LCBs $(s_{i,k})$ for all $i \in [n]$
        \STATE Find allocation:
        $
        \bm{a}_k = \algname{RAS} (\bm{s}_k; B)
        $
		\STATE Allocate $a_{i,k} + 1$ tasks to each worker $i \in [n]$
        \STATE Update $\bm{x}$:
        $$
            \bm{x}_{k+1} = \bm{x}_k - \frac{\gamma}{n} \sum_{i=1}^n \frac{1}{a_{i,k}+1}\sum_{j=1}^{a_{i,k}+1} \nabla f_i\(\bm{x}_k;\bm{\xi}_i^j\)
        $$
        \STATE For all $i\in[n]$, update:
        \begin{align*}
            K_{i,k+1} &= K_{i,k} + a_{i,k} \\
            T_{i,k+1} &= T_{i,k} + \sum_{j=1}^{a_{i,k}} X_{i,k}^{(j)} \\
            \hmu_{i,k+1} &= \frac{T_{i,k+1}}{K_{i,k+1}} \\
        \end{align*}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


\subsection{Asynchronous SGD}

Here, we focus on the homogeneous problem given in \Cref{eq:homo_problem}. The greedy variant, \algname{Ringmaster ASGD}, was proposed by \citet{maranjyan2025ringmasterasgdasynchronoussgd} and, like \algname{Rennala SGD}, achieves the best runtime.

We now present its version with \algname{ATA}, given in \Cref{alg:asgd-ata}.

\begin{figure*}[h]
    \begin{minipage}[t]{0.48\textwidth}

\begin{algorithm}[H]
	\caption{\algname{ASGD-ATA}}
    \label{alg:asgd-ata}
	\begin{algorithmic}[1]
		\STATE \textbf{Optimization inputs}: initial point $\bm{x}_0 \in \R^d$, stepsize $\gamma > 0$
        \STATE \textbf{Allocation inputs}: allocation budget  $B$
        \STATE \textbf{Initialize}: empirical means $\hmu_{i,1} = 0$, usage counts $K_{i,1} = 0$, and usage times $T_{i,1} = 0$, for all $i \in [n]$
		\FOR{$k = 1,\ldots, K$}
        \STATE Compute LCBs $(s_{i,k})$ for all $i \in [n]$
        \STATE Find allocation:
        $
        \bm{a}_k = \algname{RAS} (\bm{s}_k; B)
        $
        \STATE Update $\bm{x}_k$ using \Cref{alg:asgd} with allocation $\bm{a}_k$
        \STATE For all $i$ such that $a_{i,k} \neq 0$, update:
        \begin{align*}
            K_{i,k+1} &= K_{i,k} + a_{i,k} \\
            T_{i,k+1} &= T_{i,k} + \sum_{j=1}^{a_{i,k}} X_{i,k}^{(j)} \\
            \hmu_{i,k+1} &= \frac{T_{i,k+1}}{K_{i,k+1}} \\
        \end{align*}
        \vspace{-1cm}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

    \begin{algorithm}[H]
        \caption{\algname{ASGD}}
        \label{alg:asgd}
        \begin{algorithmic}[1]
            \STATE \textbf{Input:} Initial point $\bm{x}_0 \in \mathbb{R}^d$, stepsize $\gamma > 0$, allocation vector $\bm{a}$ with $\|\bm{a}\|_1 = B$
            \STATE Workers with $a_i > 0$ start computing stochastic gradients at $\bm{x}_0$
            \FOR{$s = 0, 1, \ldots, B-1$}
                \STATE Receive gradient $\nabla f(\bm{x}_{s+\delta_s}; \bm{\xi}_{s+\delta_s}^{i})$ from worker $i$
                \STATE Update: $\bm{x}_{s+1} = \bm{x}_{s} - \gamma \nabla f(\bm{x}_{s+\delta_s}; \bm{\xi}_{s+\delta_s}^{i})$
                \IF{$a_i > 0$}
                    \STATE Worker $i$ begins computing $\nabla f(\bm{x}_{s+1}; \bm{\xi}_{s+1}^{i})$
                    \STATE Decrease remaining allocation for worker $i$ by one: $a_i = a_i - 1$
                \ENDIF
            \ENDFOR
            \STATE \textbf{return:} $\bm{x}_{B}$
        \end{algorithmic}
        \vspace{0.2cm}
        The sequence $\{\delta_s\}$ represents delays, where $\delta_s \geq 0$ is the difference between the iteration when worker $i$ started computing the gradient and iteration $s$, when it was applied.
    \end{algorithm}

\end{minipage}
\end{figure*}


Here, the task remains gradient computation, but each worker's subsequent tasks use different points for computing the gradient. These points depend on the actual computation times and the asynchronous nature of the method, hence the name \algname{Asynchronous SGD}.


\section{Recursive Allocation Selection Algorithm}
\label{sec:RAS}

In this section, we introduce an efficient method for finding the best allocation.
Given LCBs $\bm{s}_k$ and allocation budget $B$, each iteration of \algname{ATA} (\Cref{alg:ata}) determines the allocation by solving
$$
    \bm{a}_k \in \argmin_{\ba \in \mathcal{A}} \ \ell(\bm{a}, \bm{s}_k),
$$
where 
$$
    \ell(\bm{a},\bm{\mu}) \eqdef \max_{i \in [n]} \  a_{i} \mu_i  = \infnorm{\bm{a}\odot \bm{\mu}},
$$
with $\odot$ denoting the element-wise product.
When clear from context, we write $\ell(\bm{a})$ instead of $\ell(\bm{a}, \bm{\mu})$.

In the early iterations, when some $s_i$ values are $0$, \algname{ATA} allocates uniformly across these arms until all $s_i$ values become positive.
After that, the allocation is determined using the recursive routine in \Cref{alg:RAS}.

\begin{algorithm}[H]
    \caption{Recursive Allocation Selection (\algname{RAS})}
    \label{alg:RAS}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Scores $s_1, \dots, s_n$, allocation budget $B$
        \STATE Assume without loss of generality that $s_1 \leq s_2 \leq \dots \leq s_n$ (i.e., sort the scores)
        \IF{$B = 1$}
            \STATE \textbf{return:} $(1, 0, \dots, 0)$
        \ENDIF
        \STATE Find the previous best allocation:
        $$
            \bm{a} = (a_1, \dots, a_n) = \algname{RAS}\(s_1, \dots, s_n; B-1\)
        $$
        \STATE Determine the first zero allocation:
        \begin{equation}
            \label{eq:r}
            r = 
            \begin{cases}
                \min\{i \mid a_i = 0\}, & \text{if }\ a_n = 0 \\
                n, & \text{otherwise}
            \end{cases}
        \end{equation}
        \STATE Find the best next query allocation set: \label{alg_line:min}
        $$
        M = \argmin_{i \in [r]} \  \infnorm{(\bm{a} + \bm{e}_i) \odot \bm{s}},
        $$
        where $\bm{e}_i$ is the unit vector in direction $i$.
        \STATE Select $j \in M$ such that the cardinality of 
        $$
        \argmax_{i \in [r]} \ (a_i + e_{j,i}) s_i
        $$ 
        is minimized
        \STATE \textbf{return:} $\bm{a} + \bm{e}_j$
    \end{algorithmic}
\end{algorithm}

\begin{remark}
    The iteration complexity of \algname{RAS} is $\mathcal{O}(n \ln(\min\{B, n\}) + \min\{B, n\}^2)$. In fact, the first $n \ln(\min\{B, n\})$ term arises from identifying the smallest $B$ scores. For the second term, note that in \eqref{eq:r}, we have $r \leq \min\{B, n\}$.
\end{remark}

\subsection{Optimality}
We now prove that \algname{RAS} finds the optimal allocation, as stated in the following lemma.
\begin{lemma}
    \label{thm:RAS_optimality}
    For positive scores $0<s_1 \le s_2 \le \ldots \le s_n$, \algname{RAS} (\Cref{alg:RAS}) finds an optimal allocation $\bh \in \cA$, satisfying
    $$
    \bh \in \argmin_{\ba \in \cA} \  \infnorm{\ba \odot \bs} ~.
    $$
\end{lemma}
%
\begin{proof}
    We prove the claim by induction on the allocation budget $B$.
    
    \textbf{Base Case ($B = 1$):}  
    When $B = 1$, \algname{RAS} (\Cref{alg:RAS}) allocates the task to worker with the smallest score (line 9).
    Thus, the base case holds.

    \textbf{Inductive Step:}  
    Assume that \algname{RAS} finds an optimal allocation for budget $B - 1$, denoted by
    $$
        \bar{\bh} = \algname{RAS}(s_1, \ldots, s_n; B-1)~.
    $$
    We need to prove that the solution returned for budget $B$, denoted by $\bh = \bar{\bh} + \be_r$, is also optimal.

    Assume, for contradiction, that there exists $\ba \in \cA$ such that $\ba \neq \bh$ and $\ell(\ba) < \ell(\bh)$. 
    Write $\ba = \bar{\ba} + \be_q$ for some $q \in [n]$. Observe that $\|\bar{\ba}\|_1=B-1$ because $\ba \in \mathcal{A}$.

    We consider two cases based on the value of $\ell\(\bar{\bh} + \be_r\)$:

    \begin{itemize}
        \item $\ell\(\bar{\bh} + \be_r\) = h_k s_k$ for some $k \neq r$.  
        In this case, adding one unit to index $r$ does not change the maximum value, i.e., $\ell\(\bar{\bh}\) = \ell\(\bar{\bh} + \be_r\)$. 
        By the inductive hypothesis, $\bar{\bh}$ minimizes $\ell(\bx)$ for budget $B - 1$. 
        Therefore, we have
        $$
        \ell(\ba) \geq \ell\(\bar{\ba}\) \geq \ell\(\bar{\bh}\) = \ell\(\bar{\bh} + \be_r\) =\ell(\bh),
        $$
        which contradicts the assumption that $\ell(\ba) < \ell(\bh)$.

        \item $\ell\(\bar{\bh} + \be_r\) = \(\bar{h}_r + 1\)s_r$.  
        By the algorithm's logic, $\(\bar{h}_r + 1\)s_r \leq \(\bar{h}_i + 1\)s_i$ for all $i \neq r$.
        Since $\ell(\bar{\bh}+\be_r)\leq \ell(\bar{\bh}+\be_q)$ and we assumed $\ell(\bar{\ba}+\be_q)=\ell(\ba)<\ell(\bh)=\ell(\bar{\bh}+\be_r)$, then $\bar{\ba} \neq \bar{\bh}$ otherwise $\ell(\bar{\ba}+\be_q)<\ell(\bar{\ba}+\be_r)$.
        Given that $\|\bar{\bh}\|_1=\|\bar{\ba}\|_1$, this implies that there exists some $u \in [n]$ such that $0\le\bar{a}_u \leq \bar{h}_u - 1$ and another index $v \in [n]$ where $\bar{a}_v \geq \bar{h}_v + 1$.

        In addition, note that $r$ is chosen such that $\ell\(\bar{\bh} + \be_r\)$ is minimum. Using the fact that $\ell\(\bar{\bh} + \be_r\) = \(\bar{h}_r + 1\)s_r$, we have that for any index $q$, we also necessarily have $\ell\(\bar{\bh} + \be_q\) = \(\bar{h}_q + 1\)s_q$.
        Using this, we deduce
        $$
        \ell(\bh)
        =\ell\(\bar{\bh} + \be_r\)
        \leq \ell\(\bar{\bh} + \be_v\)
        = \(\bar{h}_v + 1\)s_v
        \leq \max_i \  \bar{a}_i s_i
        = \ell\(\bar{\ba}\) \leq \ell(\ba),
        $$
        where in the second inequality we used the fact that $\bar{a}_v \geq \bar{h}_v + 1$ and in the last inequality we used the fact that the loss is not decreasing for we add one element to the vector.
        This chain of inequalities again contradicts the assumption that $\ell(\ba) < \ell(\bh)$.
    \end{itemize}

    Since both cases lead to contradictions, we conclude that no $\ba \in \cA$ exists with $\ell(\ba) < \ell(\bh)$. 
    Thus, \algname{RAS} produces an optimal allocation for budget $B$.
\end{proof}

\subsection{Minimal Cardinality}

Among all possible allocations \algname{RAS} choose one that always minimizes the cardinality of the set:
$$
    \argmax_{i \in [n]} \  a_i s_i~.
$$

The reason for this choice is just technical as it allows the \Cref{lem:1} to be true. 

\begin{lemma}
    \label{thm:minimal_cardinality}
    The output of $\algname{RAS}$ ensures the smallest cardinality of the set:
    $$
        \argmax_{i \in [n]} \ a_i s_i
    $$
    among all the optimal allocations $\ba$.
\end{lemma}
%
\begin{proof}
    This proof uses similar reasoning as the one before.

    Let $\bh = \algname{RAS}(\bs;B)$, and denote the cardinality of the set $\argmax_{i \in [n]} \ a_i s_i $ for allocation $\ba$ by
    $$
    C_B(\ba) = \left| \argmax_{i \in [n]} \ a_i s_i  \right| \geq 1~.
    $$  
    We prove the claim by induction on $B$.

    \textbf{Base Case ($B=1$):}  
    For $B=1$, there is a single coordinate allocation, thus $C_1(\bh) = 1$, which is the smallest possible cardinality.

    \textbf{Inductive Step:}  
    Assume that $\algname{RAS}$ finds an optimal allocation for budget $B-1$ with the smallest cardinality, denote its output by
    $$
    \bar{\bh} = \algname{RAS}(s_1, \ldots, s_n; B-1)~.
    $$  
    We need to prove that $\bh = \bar{\bh} + \be_r$ minimizes $C_B(\ba)$ among all optimal allocations for budget $B$.

    Assume, for contradiction, that there exists $\ba \in \cA$ such that $\ba \neq \bh$, $\ell(\ba) = \ell(\bh)$, and $C_B(\ba) < C_B(\bh)$. 
    Write $\ba = \bar{\ba} + \be_q$ for some $q \in [n]$.
    We consider three cases:

    \begin{itemize}
        \item 
        $C_B(\bh) = 1$.  
        %This occurs when $\ell(\bh) > \ell\(\bar{\bh}\)$.
        Since the minimum cardinality is exactly 1, we must have $C_B(\ba) \ge 1 = C_B(\bh)$, that contradicts our assumption.

        \item 
        $C_B(\bh) = C_{B-1}\(\bar{\bh}\)>1$.
        This occurs when $\ell(\bh) = \ell\(\bar{\bh}\) \ne \(\bar{h}_r + 1\)s_r$. 
        By the optimality of $\bh$, we have $\ell\(\bar{\bh}\) \le \ell\(\bar{\ba}\) \le \ell(\ba) = \ell(\bh)=\ell(\bar{\bh})$, which implies $\ell\(\bar{\ba}\) = \ell(\ba)$.
        Therefore, $C_{B-1}\(\bar{\ba}\) \le C_B(\ba)$. 
        Since the induction hypothesis holds for $B-1$, we have $C_{B-1}\(\bar{\bh}\) \le C_{B-1}\(\bar{\ba}\)$. 
        Thus,
        $$
        C_B(\bh) = C_{B-1}\(\bar{\bh}\) \le C_{B-1}\(\bar{\ba}\) \le C_B(\ba),
        $$
        which leads to a contradiction.

        \item 
        $C_B(\bh) = C_{B-1}\(\bar{\bh}\) + 1$.  
        This occurs when $\ell(\bh) = \ell\(\bar{\bh}\) = \(\bar{h}_r + 1\)s_r$. 
        Proceeding as in the previous case, we have $\ell\(\bar{\ba}\) = \ell(\ba)$, and hence $C_{B-1}\(\bar{\ba}\) \le C_B(\ba)$.
        Since the induction hypothesis holds for $B-1$, we know $C_{B-1}\(\bar{\bh}\) \le C_{B-1}\(\bar{\ba}\)$.

        We now have additional cases:
        \begin{itemize}
        \item If $C_{B-1}\(\bar{\ba}\) = C_{B-1}\(\bar{\bh}\) + 1$, then
        $$
        C_B(\bh) = C_{B-1}\(\bar{\bh}\) + 1 = C_{B-1}\(\bar{\ba}\) \le C_B(\ba),
        $$
        which leads to a contradiction.

        \item Now assume $C_{B-1}\(\bar{\ba}\) = C_{B-1}\(\bar{\bh}\)$.
        We will show that in this case, $C_B(\ba) = C_{B-1}\(\bar{\ba}\) + 1$. 
        By contradiction, suppose $C_B(\ba) = C_{B-1}\(\bar{\ba}\)$, which implies $(\bar{a}_q + 1)s_q < \ell(\ba)$. Let $k$ be an index such that $\bar{a}_k s_k = \ell(\ba)$.
        Construct a new allocation $\ba' = \bar{\ba} + \be_q - \be_k$. 
        Then,
        $$
        C_{B-1}\(\ba'\) = C_{B-1}\(\bar{\ba}\) - 1 < C_{B-1}\(\bar{\bh}\),
        $$
        which contradicts the induction hypothesis. 
        Thus, $C_B(\ba) = C_{B-1}\(\bar{\ba}\) + 1$.
        Using this, we have
        $$
        C_B(\bh) = C_{B-1}\(\bar{\bh}\) + 1 = C_{B-1}\(\bar{\ba}\) + 1 = C_B(\ba),
        $$
        which again contradicts $C_B(\ba) < C_B(\bh)$.
        \end{itemize}
    \end{itemize}

    This concludes the proof.
\end{proof}
