\section{Related Work}
\label{section:related}

Most of the literature on asynchronous methods focuses on demonstrating advantages over their synchronous counterparts. For the simplest method, \algname{SGD}, this was only recently established by \citet{tyurin2024optimal}. With this result in place, the community can now shift its focus to reducing the overhead of asynchrony. Our work may be the first step in this direction.

In federated learning (FL) \citep{konevcny2016federated,mcmahan2016federated,kairouz2021advances}, several works account for system heterogeneity. The most well-known FL method, \algname{FedAvg} \citep{mcmahan2017communication}, operates by performing multiple local steps on workers, where each step can be viewed as a task. Some works adjust the number of local steps based on worker computation times \citep{li2020federated, maranjyan2022gradskip}, effectively adapting task assignments to worker speed. However, these methods rely on prior knowledge of these times rather than learning them adaptively, as we do.

We reformulate our problem as an online bandit problem. The literature on bandit algorithms is vast, and we refer the reader to \citet{LattimoreS18} for an introduction to this subject.
Our algorithm is based on the approach of using Lower Confidence Bounds (LCBs) on the true means of the arms. This idea, originally proposed by \citet{auer2002finite} for the classical Multi-Armed Bandit (MAB) setting, has since been widely adopted in the stochastic combinatorial bandits literature \citep{gai2012combinatorial, chen2013combinatorial, combes2015combinatorial, kveton2015tight}. Using LCBs instead of the empirical estimates of the means allows to trade-off optimally exploration and exploitation.

The ``greedy'' approach we employ, which involves selecting the action that minimizes the loss function based on lower confidence bounds instead of the unknown means, is a standard technique in the literature \citep{chen2013combinatorial, lin2015stochastic}.
However, note that our larger action space and the discontinuity of our loss function necessitates a more tailored analysis.
To the best of our knowledge, this is the first work addressing a non-continuous loss function in a stochastic combinatorial MAB-like framework.
To overcome this challenge, we exploit the specific structures of our loss function and action space to control the number of rounds where suboptimal actions are chosen.
Additionally, our procedure is computationally efficient.
