\section{Experiments}
\label{section:experiments}

\begin{figure*}[t]
    \centering
    \begin{tabular}{cccc}
        \includegraphics[width=0.234\textwidth]{plots/linear/n=17/plot1_runtime_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=17/plot2_total_worker_time_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=17/plot3_iterations_vs_average_iteration_time.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=17/plot4_iterations_vs_proxy_avg_regret.pdf} \\
        \includegraphics[width=0.234\textwidth]{plots/linear/n=51/plot1_runtime_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=51/plot2_total_worker_time_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=51/plot3_iterations_vs_average_iteration_time.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=51/plot4_iterations_vs_proxy_avg_regret.pdf} \\
        \includegraphics[width=0.234\textwidth]{plots/linear/n=153/plot1_runtime_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=153/plot2_total_worker_time_vs_grad.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=153/plot3_iterations_vs_average_iteration_time.pdf} &
        \includegraphics[width=0.234\textwidth]{plots/linear/n=153/plot4_iterations_vs_proxy_avg_regret.pdf}
    \end{tabular}
    \caption{
        We use the same setup as in \Cref{fig:sqrt}, with each row tripling the number of workers, starting from $n=17$.
    }
    \label{fig:linear}
\end{figure*}


In this section, we validate our algorithms by simulating a scenario with $n$ workers, where we solve a simple problem using \algname{SGD}.
In each iteration, we collect $B=23$ gradients from the workers and perform a gradient descent step.

The objective function $f \,:\, \R^d \to \R$ is a convex quadratic defined as
$$
    f(x) = \frac{1}{2} \bm{x}^\top \mA \bm{x} - \bm{b}^\top \bm{x},
$$
where
\begin{align*}
    \mA &= \frac{1}{4}
    \begin{bmatrix}
    2 & -1 &  & 0 \\
    -1 & \ddots & \ddots &  \\
    & \ddots & \ddots & -1 \\
    0 & & -1 & 2 \\
    \end{bmatrix}
    \in \mathbb{R}^{d \times d} ~, \\
    \bm{b} &= \frac{1}{4}
    \begin{bmatrix}
    -1 \\
    0 \\
    \vdots \\
    0 \\
    \end{bmatrix}
    \in \mathbb{R}^d~.
\end{align*}

We denote $f^*$ as the minimum value of the function $f$.
Each of the $n$ workers is able to calculate unbiased stochastic gradients $\bm{g}(\bm{x})$ that satisfy
$
    \mathbb{E}[\sqnorm{\bm{g}(\bm{x}) - \nabla f(\bm{x})}] \le 0.01^2~.
$
This is achieved by adding Gaussian noise to the gradients of $f$.

The computation time for worker $i$ is modeled by the distribution
$
    \nu_i = 29\sqrt{i} + \mathrm{Exp}(29\sqrt{i}),
$
for all $i \in [n]$, where $\mathrm{Exp}(\beta)$ denotes the exponential distribution with scale parameter $\beta$. The expected value of this distribution is $\mu_i = 2 \cdot 29 \sqrt{i}$.
Furthermore, the Orlicz norm satisfies the bound $\alpha_i \le 2\mu_i$.

We consider three benchmark algorithms.
\algname{GTA-SGD}, originally introduced as \algname{Rennala SGD} by \citet{tyurin2024optimal}.
Additionally, we include \algname{OFTA} ({\red O}ptimal {\red F}ixed {\red T}ask {\red A}llocation), which assumes the oracle knowledge of the mean computation times and uses the optimal allocation $\bar{\bm{a}}$ in \eqref{eq:proxy_loss} in each iteration, and \algname{UTA} ({\red U}niform {\red T}ask {\red A}llocation), which distributes $B$ tasks uniformly among the $n$ workers. If $n>B$, then in \algname{UTA} we select $B$ workers at random, each one tasked to calculate one stochastic gradient.
Our algorithms aim to achieve a performance close to the one of \algname{OFTA}, without any prior knowledge of the true means.

The experiments were implemented in Python.
The distributed environment was emulated on machines with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz.

For \algname{ATA} we set $\alpha = \alpha_n = 4 \cdot 29 \sqrt{n}$, while for \algname{ATA-Empirical} we use $\eta = 1$.
The results of our experiments are shown in \Cref{fig:sqrt}.

As expected, \algname{GTA} is the fastest in terms of runtime (first column), but it performs poorly in terms of total worker time (second column).
This is because it uses all devices, most of which perform useless computations that are never used, leading to worse performance as the number of workers increases.
In fact, its performance can become arbitrarily worse. On the other hand, \algname{OFTA} performs best in terms of total worker time.
Although it is slower in terms of runtime, the difference is by a constant factor that does not increase as $n$ grows.
This is because additional workers are less efficient and do not provide significant benefits for \algname{GTA}.

Turning our attention to our algorithms, both \algname{ATA} and \algname{ATA-Empirical} initially behave like \algname{UTA}, as it is expected by the need to perform an initial exploration phase with uniform allocations.
However, after this phase, they begin to converge to the performance of \algname{OFTA}.

%With a better starting point, it can converge to the optimal allocation sooner and behave similarly.

The last two columns contain plots that confirm our theoretical derivations.
The third plot validates \Cref{cor:main}, showing that \algname{ATA} and \algname{ATA-Empirical} converge to \algname{OFTA} up to a constant.
The final column shows the averaged cumulative regret, vanishing over time as predicted by Theorems~\ref{thm:main} and \ref{thm:main2}.

% Let $R_\textnormal{A}$ and $R_\textnormal{G}$ represent the runtimes of \algname{SGD-ATA} and \algname{SGD-GTA}, respectively, when both achieve $f(x) - f^{\inf} < 10^{-5}$.
% Similarly, let $T_\textnormal{A}$ and $T_\textnormal{G}$ denote the total worker times for \algname{SGD-ATA} and \algname{SGD-GTA}, respectively, under the same condition.

% The total worker time is defined as the sum of the computation times of all workers involved during the training process.
% In the case of \algname{SGD-GTA}, all workers are active throughout the entire training process.
% As a result, \algname{SGD-GTA} exhibits arbitrarily worse total worker time as $n$ increases.


\begin{table}[h!]
	\caption{
        Ratios of total worker times and runtimes required to achieve $f(\bm{x}) - f^{*} < 10^{-5}$.
        For total worker time, we divide the total worker time of \algname{GTA} by the corresponding total worker times of the other algorithms listed.
        For runtime, we do the opposite, dividing the runtime of the other algorithms by the runtime of \algname{GTA}, since \algname{GTA} is the fastest.
        To simplify the naming, we refer to \algname{ATA-Empirical} as \algname{ATA-E}.
        }
	\label{table:sqrt}

	\begin{center}
	\begin{small}
	\begin{sc}
	\begin{tabular}{l|ccc|ccc}
	\toprule
	\multirow{2}{*}{$n$} & \multicolumn{3}{c|}{Tot. worker time ratio} & \multicolumn{3}{c}{Runtime ratio} \\
	\cmidrule(lr){2-4} \cmidrule(lr){5-7}
	& \algname{ATA} & \algname{ATA-E} & \algname{OFTA} & \algname{ATA} & \algname{ATA-E} & \algname{OFTA} \\
	\midrule
	$17$ & $1.3$ & $1.26$ & $1.26$ & $1.73$ & $1.75$ & $1.74$ \\
	$51$ & $2.91$ & $2.69$ & $3.03$ & $2.43$ & $2.45$ & $2.17$ \\
	$153$ & $7.22$ & $7.02$ & $9.1$ & $3.44$ & $3.14$ & $2.17$ \\
	$459$ & $12.45$ & $14.1$ & $27.3$ & $6.36$ & $5.51$ & $2.17$ \\
	\bottomrule
	\end{tabular}
	\end{sc}
	\end{small}
	\end{center}
\end{table}


In \Cref{table:sqrt}, we compare the results numerically.
Both the total worker time ratio and runtime ratio increase as $n$ grows.
The total worker time ratio increases because \algname{GTA} becomes less efficient, using more resources than necessary.
The runtime ratio grows for \algname{ATA} and \algname{ATA-Empirical} since a larger number of workers requires more exploration.
However, for \algname{OFTA} this ratio remains unchanged, as discussed earlier.

We remark that in these experiments we started all runs for \algname{ATA} and \algname{ATA-Empirical} without prior knowledge of the computation time distribution.
However, in real systems, where these algorithms are used multiple times, prior estimates of computation times from previous runs could be available.
With this information, \algname{ATA} and \algname{ATA-Empirical} would be much faster, as they would spend less time on exploration, approaching the performance of \algname{OFTA} in a faster way.

In \Cref{sec:linear_noise}, we conducted similar experiments using a different time distribution, where the means exhibit a linear dependence across the arms.
Additionally, in \Cref{sec:regret}, we present the performance of the regret, confirming its logarithmic behavior for both Theorems~\ref{thm:main} and \ref{thm:main2}.


\subsection{Linear noise}
\label{sec:linear_noise}

In this section we model the computation time for worker $i$ by the distribution
$$
    \nu_i = 29i + \mathrm{Exp}(29i), \quad \text{for all} \quad i \in [n]~.
$$
The expected value of this distribution is $\mu_i = 2 \cdot 29 i$~.
Furthermore, the Orlicz norm satisfies the bound $\alpha_i \le 2\mu_i$.

We again set $B = 23$ and run simulations similar to those in \Cref{section:experiments}.
The results are shown in \Cref{fig:linear}.
%

The important difference to the previous \Cref{fig:sqrt} is that here \algname{ATA-Empirical} performs better than \algname{ATA}.
This is because the Orlicz norm $\alpha = 4\cdot29n$ is much larger.

Similarly, we provide a numerical comparison in \Cref{table:linear}.
\begin{table}[H]
	\caption{
        This table presents ratios similar to those in \Cref{table:sqrt}.
        }
	\label{table:linear}

	\begin{center}
	\begin{small}
	\begin{sc}
	\begin{tabular}{l|ccc|ccc}
	\toprule
	\multirow{2}{*}{$n$} & \multicolumn{3}{c|}{Tot. worker time ratio} & \multicolumn{3}{c}{Runtime ratio} \\
	\cmidrule(lr){2-4} \cmidrule(lr){5-7}
	& \algname{ATA} & \algname{ATA-E} & \algname{OFTA} & \algname{ATA} & \algname{ATA-E} & \algname{OFTA} \\
	\midrule
	$17$ & $2.32$ & $1.91$ & $2.1$ & $1.71$ & $1.71$ & $1.58$ \\
	$51$ & $6.71$ & $5.02$ & $6.29$ & $3.27$ & $2.12$ & $1.58$ \\
	$153$ & $3.41$ & $8.68$ & $18.87$ & $7.96$ & $4.5$ & $1.58$ \\
	% $459$ & $1.67$ & $5.87$ & $56.59$ & $40.28$ & $20.06$ & $1.58$ \\
	\bottomrule
	\end{tabular}
	\end{sc}
	\end{small}
	\end{center}
\end{table}


\subsection{Regret}
\label{sec:regret}

In this section, we verify Theorems~\ref{thm:main} and \ref{thm:main2} on regret through simulations. We set $n = 20$ and $B = 5$, with the computation time for worker $i$ following the distribution
$$
\nu_i = \mathrm{Exp}(2i), \quad \text{for all} \quad i \in [n]~.
$$
We ran the simulation five times, and the plots include standard deviation bars, although they are not visible. The results are presented in \Cref{fig:regret}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{plots/regret.pdf}
\caption{Regret growth over iterations.}
\label{fig:regret}
\end{figure}

As expected, the regret grows logarithmically.
