\documentclass[a4paper]{article} % 一般的なスタイルの書き方
\usepackage[margin=1in]{geometry}
\renewcommand{\baselinestretch}{1.3}


\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{here}
% \usepackage{authblk}

% \usepackage{CJKutf8}
% \usepackage[whole]{bxcjkjatype}

% Recommended, but optional, packages for figures and better typesetting:

\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{algorithm} % for algorithm
\usepackage{algpseudocode} % for algorithm
\usepackage{url}
\usepackage{multirow}
% \usepackage{hyperref} 

% \usepackage{subcaption}
% \usepackage{booktabs} % for professional tables

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{bbm}
% \usepackage[whole]{bxcjkjatype}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}

\newcommand{\mybecause}[1]{&& \bigl(\because #1 \bigr)}
\newcommand{\indep}{\mathop{\perp\!\!\!\!\perp}}
\newcommand{\disteq}{\overset{\mathrm{d}}{=}}

\input{macros}

% show equation number only if referenced
\mathtoolsset{showonlyrefs}
% \renewcommand{\arraystretch}{1.2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{authblk}

\title{Distributionally Robust Active Learning \\for Gaussian Process Regression}
\date{}

\author[1,2]{Shion Takeno}

\author[1]{Yoshito Okura}
\author[3]{Yu Inatsu}
\author[1]{Aoyama Tatsuya}
\author[1]{Tomonari Tanaka}
\author[1]{Akahane Satoshi}
\author[2]{Hiroyuki Hanada}
\author[2]{Noriaki Hashimoto}
\author[4]{Taro Murayama}
\author[4]{Hanju Lee}
\author[4]{Shinya Kojima}
\author[1,2]{Ichiro Takeuchi}

\affil[1]{Nagoya University}
\affil[2]{RIKEN AIP}
\affil[3]{Nagoya Institute of Technology}
\affil[4]{DENSO CORPORATION}

\affil[ ]{\texttt{{takeno.s.mllab.nit@gmail.com}}}
\affil[ ]{\texttt{{takeuchi.ichiro.n6@f.mail.nagoya-u.ac.jp}}}

\begin{document}
\maketitle


\begin{abstract}
     Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction.
     %
     Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem.
     %
     However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution.
     %
     Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult.
     %
     Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates.
     %
     We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions.
     %
     Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.
\end{abstract}

% Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.


% Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing approaches do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two active learning methods that effectively reduce the worst-case target prediction error for GPR, where the worst-case is taken with the target distribution candidates. We show an information-theoretic upper bound of the worst-case expected squared error, which suggests that under mild conditions, the error will be arbitrarily small within a finite number of data labels. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.

% The first version
% Active learning is a framework for actively collecting data labels to achieve an accurate prediction with fewer data labels under some statistical model. However, many existing approaches do not theoretically guarantee prediction accuracy for target distribution. In addition, as discussed in the literature on distributionally robust learning, specifying the target distribution is often difficult. This paper proposes two active learning methods that effectively reduce the worst-case target prediction error when we use the Gaussian process regression, a widely used kernel regression model. We show an information-theoretic upper bound for the worst-case expected squared error, where the worst-case is taken with the target distribution candidates. This upper bound shows that the worst-case prediction error will be arbitrarily small with a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{manuscripts/1_intro}
\input{manuscripts/2_preliminary}
\input{manuscripts/3_problem}
\input{manuscripts/4_proposed}
\input{manuscripts/5_related}
\input{manuscripts/6_experiments}
\input{manuscripts/7_conclusion}

\section*{Acknowkedgements}
This work was partially supported by 
JST ACT-X Grant Number (JPMJAX23CD and JPMJAX24C3), 
JST PRESTO Grant Number JPMJPR24J6, 
JST CREST Grant Numbers (JPMJCR21D3 including AIP challenge program and JPMJCR22N2),
JST Moonshot R\&D Grant Number JPMJMS2033-05, 
%%%%%%%
JSPS KAKENHI Grant Number (JP20H00601, JP23K16943, JP23K19967, JP24K15080, and JP24K20847), 
%%%%%%%
NEDO (JPNP20006),
and RIKEN Center for Advanced Intelligence Project.


% MEXT Program: Data Creation and Utilization-Type Material Research and Development Project Grant Number JPMXP1122712807,
% JSPS KAKENHI Grant Numbers JP20H00601,JP21H03498,JP22H00300,JP23K16943,JP23K19967,
% JST CREST Grant Numbers JPMJCR21D3,JPMJCR22N2, 

% JST ACT-X Grant Number JPMJAX23CD,
% NEDO Grant Numbers JPNP18002,JPNP20006, 
% and RIKEN Center for Advanced Intelligence Project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \clearpage

% \section*{Impact Statements}

% This paper focuses on the theoretical and algorithmic aspects of the machine learning method.
% %
% We consider that some potential societal consequences of this paper need not necessarily be highlighted here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \subsubsection*{References}
\bibliography{ref}
\bibliographystyle{plainnat}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{manuscripts/9_appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
