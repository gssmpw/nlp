\section{Conclusion}
\label{sec:conclusion}

This paper investigated the DRAL problem for the GPR, in which we aim to reduce the worst-case error $E_T$.
%
We first showed several properties of this problem for the GPR, which implies that minimizing the variance guarantees a decrease in $E_T$. 
%
Therefore, we designed two algorithms that reduce the target variance and incorporate information about target distribution candidates for practical effectiveness.
%
Then, we proved the theoretical error convergence of the proposed methods, whose practical effectiveness is demonstrated via synthetic and real-world datasets.



\paragraph{Limitation and Future Work:}
We can consider several future research directions.
%
First, since we do not show the optimality of the convergence rate, developing a (near) optimal algorithm for $E_T$ is vital.
%
For this goal, the approximate submodularity~\citep{bian2017guarantees} may be relevant from the empirical superiority of DR variance reduction.
%
Second, since the expectation over $p(\*x^*)$ may be intractable, an analysis incorporating the approximation error or developing an algorithm that is efficient without expectation computation may be crucial (DR random does not require the expectation but is often inefficient).
%
Third, although our analyses only require the existence of the maximum over $\cP$, our experiments are limited to the discrete distribution set defined by $L_{\infty}$ ball.
%
Thus, more general experiments regarding, e.g., the continuous probability distributions and the ambiguity sets defined by Kullback-Leibler divergence~\citep{hu2013kullback} and Wasserstein distance~\citep{frogner2021incorporating}, are interesting from the practical perspective.

% The following are important future work: (i) developing (near) optimal algorithm for $E_T$, in which approximate submodularity~\citep{bian2017guarantees} may be relevant from the empirical superiority of the approximate greedy algorithm, (ii) more general experiments regarding the ambiguity set $\cP$, such as the continuous probability distributions and the ball defined by Kullback-Leibler divergence~\citep{hu2013kullback} and Wasserstein distance~\citep{frogner2021incorporating}.

% Although we show the upper bounds of the error, we do not show the optimality of our analysis.
% %
% Developing (near) optimal algorithms is an important future direction.
% %
% In this direction, although our error is not submodular in general, the analysis for approximate submodular functions~\citep{bian2017guarantees} may be interesting, particularly from the empirical superiority of the approximate greedy algorithm in the experiments.
% %
% In addition, although our assumption on the ambiguity set is only that the maximum exists and can be computed, our empirical evaluation is limited to the ambiguity sets defined by $L_{\infty}$ norm.
% %
% Experiments with other ambiguity sets, e.g., using Kullback-Leibler divergence~\citep{hu2013kullback}, are of interest, although this paper mainly concentrates on the theoretical perspective.