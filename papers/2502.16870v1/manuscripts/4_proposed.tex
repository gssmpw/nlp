\section{Proposed Methods and Analysis}
\label{sec:proposed}

We aim to design algorithms that enjoy both a similar convergence guarantee as the US and RS and practical effectiveness incorporating the information of $\cP$.
%
In particular, we consider two algorithms inspired by the greedy algorithm and the RS and show theoretical guarantees.
%
Algorithm~\ref{alg:proposed} shows the pseudo-code of proposed algorithms.


\begin{algorithm}[!t]
    \caption{Proposed DRAL methods}\label{alg:proposed}
    \begin{algorithmic}[1]
        \Require Domain $\cX$, GP prior $\mu$ and $k$, ambiguity set $\cP$
        \State $\cD_{0} \gets \emptyset$
        \For{$t = 1, \dots, T$}
            \State Update $\sigma_{t-1}^2 (\cdot)$ according to Eq.~\eqref{eq:GP}
            \State Compute $\*x_t$ according to Eq.~\eqref{eq:RS} or Eq.~\eqref{eq:greedy}
        \EndFor
        \State Observe $y_1, \dots, y_T$ 
        \State Update $\mu_{T} (\cdot)$ and $\sigma_{T}^2 (\cdot)$ according to Eq.~\eqref{eq:GP}
        \State \Return $\mu_{T} (\cdot)$ and $\sigma_{T}^2 (\cdot)$
    \end{algorithmic}
\end{algorithm}



\subsection{Algorithms}

First, we consider the RS-based algorithm.
%
The algorithm is straightforward as follows:
\begin{align}
    \*x_t \sim p_t(\*x),
    \label{eq:RS}
\end{align}
where $p_t(\*x) = \argmax_{p \in \cP} \EE_{p(\*x^{*})}[\sigma_{t-1}^2 (\*x^{*})]$ and we assume that we can generate the sample from $p_t$.
%
By using the worst-case distribution $p_t$ for each iteration, this algorithm incorporates the information of $\cP$.


% \begin{itemize}
%     \item 貪欲法は一般に強いためそれに基づく方法を考える.
%     \item しかし, $\max_{p \in \cP}$を次ステップの全候補に対し計算するのに多大な計算量が必要なため, 貪欲法すら計算不可能
%     \item そこで, $p_t(\*x) = \argmax_{p \in \cP} \EE_{p(\*x)}[\sigma_t^2 (\*x)]$を固定した貪欲法を考える.
%     \item しかし, もはや貪欲法ですらないこの方法の近似保証は我々には難しかった.
%     \item そこで, 保守的なUSに理論保証があることから, 少し保守的になる (uncertainな候補を選択する) ように候補を制限することを考えた.
%     \item 最終的なアルゴリズムをXXXに示す.
% \end{itemize}

Second, we consider the greedy algorithm since its practical efficiency has often been reported~\citep[e.g., ][]{bian2017guarantees}.
%
However, in our setup, the greedy algorithm should be
\begin{align*}
    \argmin_{\*x \in \cX} \max_{p \in \cP} \EE_{p(\*x^{*})}[\sigma_{t}^2 (\*x^{*} \mid \*x)],
\end{align*}
which requires huge computational time in general due to min-max optimization.
%
Thus, we consider an approximately greedy algorithm as follows:
\begin{align*}
    \argmin_{\*x \in \cX} \EE_{p_t(\*x^{*})}[\sigma_{t}^2 (\*x^{*} \mid \*x)],
\end{align*}
where $p_t(\*x) = \argmax_{p \in \cP} \EE_{p(\*x^{*})}[\sigma_{t-1}^2 (\*x^{*})]$ is the worst-case distribution defined by $(\*x_i)_{i \in [t-1]}$.
%
On the other hand, the theoretical guarantee for this algorithm is challenging for us.
%
Hence, inspired by the fact that the US has a theoretical guarantee, we set the constraint so that the chosen input is uncertain than $\EE_{p_t(\*x^{*})}[\sigma_{t-1}^2 (\*x^{*})]$:
\begin{align}
    \*x_t = \argmin_{\*x \in \cX_t} \EE_{p_t(\*x^{*})}[\sigma_t^2 (\*x^{*} \mid \*x)],
    \label{eq:greedy}
\end{align}
where $\cX_t \coloneqq \{ \*x \in \cX \mid \sigma^2_{t-1}(\*x) \geq \EE_{p_t(\*x^{*})}[\sigma_{t-1}^2 (\*x^{*})] \}$.
%
Note that $|\cX_t| \geq 1$ holds due to the definition.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis}
\label{sec:analysis}

Here, we show the error convergence by Eqs.~\eqref{eq:RS} and \eqref{eq:greedy}:
\begin{theorem}
    Fix $\delta \in (0, 1)$.
    %
    Assume that $\cX \subset \RR^d$ is a compact subset.
    %
    If we run Algorithm~\ref{alg:proposed} with Eq.~\eqref{eq:RS}, the following holds with probability at least $1 - \delta$:
    \begin{align*}
        \max_{p \in \cP} \EE_{p(\*x^{*})}[\sigma_{T}^2 (\*x^{*})] 
        &\leq \frac{2 C_1 \gamma_T}{T} + \cO \left( \frac{\log (1 / \delta)}{T} \right),
    \end{align*}
    where $C_1 = 1 / \log(1 + \sigma^{-2})$.
    \label{theo:error_convergence_RS}
\end{theorem}
%
\begin{theorem}
    Assume that $\cX \subset \RR^d$ is a compact subset.
    %
    If we run Algorithm~\ref{alg:proposed} with Eq.~\eqref{eq:greedy}, the following holds:
    \begin{align*}
        \max_{p \in \cP} \EE_{p(\*x^{*})}[\sigma_{T}^2 (\*x^{*})] 
        &\leq \frac{C_1 \gamma_T}{T},
    \end{align*}
    where $C_1 = 1 / \log(1 + \sigma^{-2})$.
    \label{theo:error_convergence_greedy}
\end{theorem}
See Appendix~\ref{sec:proposed_proof} for the proof, in which Lemma~3 in \citet{kirschner2018-information} is used to show Theorem~\ref{theo:error_convergence_RS}.



Consequently, our proposed methods achieve almost the same convergence as those of the US and RS shown in Proposition~\ref{prop:us_rs}.
%
Furthermore, by combining Lemmas~\ref{lem:UB_error_discrete}, \ref{lem:UB_error_frequentist_continuous}, and \ref{lem:UB_error_bayesian_continuous}, we can see that the upper bound of $E_T$:
\begin{corollary}
    Fix $\delta \in (0, 1)$ and $T \in \NN$.
    %
    Then, if we run Algorithm~\ref{alg:proposed}, the following hold with probability at least $1 - \delta$:
    \begin{enumerate}
        \item When Assumption~\ref{assump:Bayesian} or Assumptions~\ref{assump:frequentist} holds, 
        \begin{align*}
            E_T = \cO\left( \frac{\log (|\cX| / \delta) \gamma_T}{T} \right);
        \end{align*}
        \item When Assumptions~\ref{assump:Bayesian} and \ref{assump:Bayesian_continuous} or Assumptions~\ref{assump:frequentist} and \ref{assump:frequentist_continuous} hold, 
        \begin{align*}
            E_T = \cO\left( \frac{\log (T / \delta) \gamma_T}{T} \right),
        \end{align*}
    \end{enumerate}
\end{corollary}
\begin{proof}
    We can obtain the result by combining Lemmas~\ref{lem:UB_error_discrete}, \ref{lem:UB_error_frequentist_continuous}, and \ref{lem:UB_error_bayesian_continuous}, Theorems~\ref{theo:error_convergence_RS} and \ref{theo:error_convergence_greedy}, and the union bound.
    %
    Note that we assume $|\cX| > T$.
\end{proof}
%
Thus, the error incurred by the proposed algorithms converges to $0$ with high probability for discrete and continuous input domains, at least with linear, SE, and Mat\'ern kernels.

