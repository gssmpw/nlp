


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}


% AL algorithms
As discussed in Section~\ref{sec:intro}, many AL algorithms have been developed~\citep{settles2009-active}.
%
The AL algorithms for classification problems are heavily discussed compared with the regression problem~\citep[for example, ][]{houlsby2011bayesian,zhao2021active,bickford2023-prediction}.
%
In particular, theoretical properties for binary classification problems are well-investigated~\citep{hanneke2014theory}.
%
On the other hand, the theoretical analysis of AL for the regression problem is relatively limited.




% Experimental design methods
AL is often referred to as optimal experimental design (OED)~\citep{lindley1956-on,cohn1993neural,Chaloner1995bayesian,cohn1996active,ryan2007modern}.
%
The OED frameworks aim to reduce the uncertainty of target parameters or statistical models.
%
For this purpose, many measures for the optimality have been proposed, such as A-optimality (average), D-optimality (determinant), and V-optimality (variance)~\citep{pukelsheim2006optimal,zhu2017near}.
%
The OED methods for various models, such as the linear model~\citep[e.g., ][]{zhu2017near}, neural network~\citep[e.g., ][]{cohn1993neural}, and GPs~\citep[e.g., ][]{yu2006active}, have been proposed.
%
Our analysis concentrates on the V-optimality of the GPR~\citep{Seo2000gaussian,yu2006active,Shoham2023experimental} and its DR variant, for which, to our knowledge, a theoretical guarantee has not been shown.



% Several examples
% For example, \citet{krause2008-near,Guestrin2005-near} proposed that the greedy algorithm for the mutual information, which corresponds to the D-optimality, is the near-optimal algorithm for the GP model.
% %
% Other many studies have concentrated on the information-based criteria for Bayesian models~\citep{mackay1992information,houlsby2011bayesian,kirsch2021test,kirsch2022unifying,bickford2023-prediction}.
% %
% However, the relationship between the prediction error and the mutual information is not obvious.
%
% On the other hand, to our knowledge, theoretical analysis of classical OED methods mainly concentrated on the linear models~\citep{pukelsheim2006optimal}.



% subset selection
In OED or AL, subset selection algorithms~\citep{das2008algorithms} are often leveraged.
%
The subset selection is a general problem whose goal is finding the subset that maximizes some set function.
%
Therefore, the AL can be seen as the subset selection of $\*x_1, \dots, \*x_t \in \cX$.
%
In this literature, the submodular property of the set function, for which the greedy algorithm can be optimal, is commonly investigated~\citep{das2008algorithms,krause2008-near,Guestrin2005-near,bian2017guarantees}.
%
The criteria for the AL, such as the D-optimality of the GPR~\citep{krause2008-near,Guestrin2005-near}, sometimes satisfy the submodular property.
%
% On the other hand, the V-optimality of the GPR does not satisfy the submodularity in general.
%
Furthermore, \citet{das2008algorithms} have shown sufficient conditions for that the greedy algorithms are optimal in Theorem~3.4 (an assumption can be rephrased as $k(\*x, \*x^\prime) \leq \frac{1}{4T}$ in our problem) and Section~8.
%
However, even if we consider minimizing $\EE_{p(\*x^*)}\left[ \sigma_T^2 (\*x^*) \right]$ with $|\cP| = 1$, these conditions and the submodularity do not hold in general.
%
Therefore, the DR maximization of submodular function~\citep[e.g., ][]{krause2008robust,staib2019distributionally} also cannot be applied directly.



% information gain-based approach
% Most AL methods based on the Bayesian models are based on mutual information, also called information gain~\citep{mackay1992information,houlsby2011bayesian}.
% %
% The information-based approach can apply to the GPR model due to its versatility.
% %
% When the mutual information between the target function we want to learn about and the label we can obtain is considered, the US can be seen as a greedy algorithm for maximizing mutual information.
% %
% Then, the optimality of the information gain-based approach is theoretically shown using the sub-modularity of mutual information~\citep{Guestrin2005-near,krause2008-near}.
% %
% Furthermore, transductive information-based AL algorithms have been proposed~\citep{kirsch2021test,kirsch2022unifying,bickford2023-prediction}.
% %
% However, there are no theoretical guarantees since transductive variants do not satisfy the sub-modularity.



% target distribution-aware AL
Several studies have discussed the target distribution-aware AL.
%
At least from \citet{sugiyama2005active}, the effectiveness of AL incorporating the information of target distribution for misspecified models has been discussed.
%
Transductive AL~\citep{Seo2000gaussian,yu2006active,Shoham2023experimental} can be interpreted as the expected error minimization when the uniformly random target distribution $p(\*x) = 1 / |\cX|$ is specified.
%
\citet{kirsch2021test,kirsch2022unifying,bickford2023-prediction} extended this setting so that an arbitrary target distribution can be considered.
%
For the non-Bayesian model, \citet{frogner2021incorporating} further extended to DRAL using the AF called expected model change~\citep{settles2009-active}.
%
However, most existing methods are heuristic greedy algorithms and are not theoretically guaranteed for the prediction error.
% Relation to transductive learning, target distribution-aware AL, V-optimal design
% On the other hand, since $|\cP| = 1$ with any target distribution in our problem setup, our study includes the target distribution-aware AL discussed above, where only one test function is specified.


The DRAL is inspired by the DR learning (DRL)~\citep{chen2018robust,chen2020distributionally}.
%
DRL considers learning the robust statistical model by optimizing the model parameter so that the worst-case expected loss is minimized, where the worst-case is taken regarding the target distribution candidates called an \emph{ambiguity set}.
%
Therefore, DRAL is an intuitive extension of DRL to AL.


% core-set selection
Another related literature is core-set selection~\citep{sener2018active}, which selects the subset of the training dataset to maintain prediction accuracy while reducing the computational cost.
%
Our proposed methods can be applied to the core-set selection for the GPR.
%
However, its effectiveness may be limited since the information on training labels is not leveraged.





% BO, kernel bandit
Other highly relevant literature is kernelized bandits, also called Bayesian optimization (BO)~\citep{Kushner1964-new,Srinivas2010-Gaussian,Shahriari2016-Taking}.
%
BO aims for efficient black-box optimization using the GPR model.
%
For this purpose, several properties of GPs, such as the confidence intervals and the MIG, have been analyzed~\citep{Srinivas2010-Gaussian,vakili2021-optimal,vakili2021-information}.
%
Our analyses heavily depended on the existing results in this field.



%LSE
In addition, level set estimation (LSE)~\citep{gotovos2013active,bogunovic2016truncated,inatsu2024active} is an AL framework using the GPR model, which aims to classify the test input set by whether or not a black-box function value proceeds a given threshold.
%
In particular, \citet{inatsu2021active} considers the variant of LSE, which aims to classify by whether or not the DR measure defined by the black-box function proceeds a given threshold.
%
Thus, our problem setup differs from the problem of \citet{inatsu2021active}.



