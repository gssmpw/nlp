%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs for Section~\ref{sec:problem}}
\label{sec:problem_proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:lipschitz_posterior_mean}}
\label{sec:proof_lipschitz_posterior_mean}

% Our derivation is almost from Lemma~F.1 of \citet{vakili2022improved}.
% %
% From the definition of $\mu_T$, we obtain
% \begin{align*}
%     \| \mu_t (\cdot) \|_{\cH_k} 
%     &\leq \| \*v_t^\top (\cdot) \*y_t \|_{\cH_k} \\
%     &\leq \| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} + \| \*v_t^\top (\cdot) \*\epsilon_t \|_{\cH_k},
% \end{align*}
% where $\*v_t (\*x) = \left(\*k_t^\top(\*x) (\*K_t + \sigma^2 \*I_t)^{-1}\right)^\top$, $\*f_t = \bigl(f(\*x_1), \dots, f(\*x_t) \bigr)^\top$, and $\*\epsilon_t = (\epsilon_1, \dots, \epsilon_t)^\top$.
% %
% Then, we consider the upper bound of the first and second terms.


% For the first term $\| \*v_t^\top (\cdot) \*f_t \|_{\cH_k}$, recall the RKHS-based definition of kernel ridge estimator:
% \begin{align*}
%     \mu_t = \argmin_{\mu \in \cH_k} \sum_{i=1}^t \bigl( y_{\*x_i} - \mu(\*x_i) \bigr)^2 + \sigma^2 \| \mu \|_{\cH_k}.
% \end{align*}
% %
% Therefore, we can derive
% \begin{align*}
%     \min_{\mu \in \cH_k} \sum_{i=1}^t \bigl( f(\*x_i) - \mu(\*x_i) \bigr)^2 + \sigma^2 \| \mu \|_{\cH_k} 
%     &= \sum_{i=1}^t \bigl( f(\*x_i) - \*v_t^\top (\*x_i) \*f_t \bigr)^2 + \sigma^2 \| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \\
%     &\leq \sum_{i=1}^t \bigl( f(\*x_i) - f (\*x_i) \bigr)^2 + \sigma^2 \| f \|_{\cH_k} && \left(\because f \in \cH_k \right) \\
%     &= \sigma^2 \| f \|_{\cH_k}.
% \end{align*}
% %
% Hence, we obtain $\| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \leq \| f \|_{\cH_k} \leq B$.
% %
% % Therefore, considering the noiseless observations $\*f_t$, we obtain
% % \begin{align*}
% %     \*k_t^\top(\cdot) \*K_t^{-1} \*f_t &= \argmin_{\mu \in \cH_k} \sum_{i=1}^t \bigl( f(\*x_i) - \mu(\*x_i) \bigr)^2 \\
% %     \*v_t^\top (\cdot) \*f_t &= \argmin_{\mu \in \cH_k} \sum_{i=1}^t \bigl( f(\*x_i) - \mu(\*x_i) \bigr)^2 + \sigma^2 \| \mu \|_{\cH_k}.
% % \end{align*}
% % %
% % Therefore, we can see that $\| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \leq \| \*k_t^\top(\cdot) \*K_t^{-1} \*f_t \|_{\cH_k}$.
% % %
% % Furthermore, from Theorem~3.5 in \citet{kanagawa2018gaussian}, $\| \*k_t^\top(\cdot) \*K_t^{-1} \*f_t \|_{\cH_k} \leq B$.
% %
% % Consequently, $\| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \leq B$.


% For the second term $\| \*v_t^\top (\cdot) \*\epsilon_t \|_{\cH_k}$, from Eq.~(37) in \citet{vakili2022improved}, we obtain
% \begin{align*}
%     \| \*v_t^\top (\cdot) \*\epsilon_t \|_{\cH_k} \leq \sqrt{\frac{\|\*\epsilon_t\|_2^2}{\sigma^2}}.
% \end{align*}
% %
% Since $(\epsilon_i)_{i \in [t]}$ is sub-gaussian random variables, by applying Chernoff-Hoeffding inequality for each $i \in [t]$, the following inequality holds with the probability at least $1 - \delta$:
% \begin{align*}
%     \epsilon_i^2 \leq 2 R^2 \log(2 / \delta).
% \end{align*}
% %
% Then, applying the union bound for all $i \in [t]$, we obtain
% \begin{align*}
%     \frac{\|\*\epsilon_t\|_2^2}{\sigma^2} \leq \frac{ 2t R^2 \log(2t / \delta) }{\sigma^2}
% \end{align*}
% with probability at least $1 - \delta$.
% %
% Therefore, we obtain
% \begin{align*}
%     \| \*v_t^\top (\cdot) \*\epsilon_t \|_{\cH_k} \leq \frac{R}{\sigma} \sqrt{ 2t \log \left( \frac{2t}{\delta} \right)}.
% \end{align*}
% %
% This concludes the proof by combining Lemma~\ref{lem:RKHS_lipschitz}.


% \begin{lemma}
%     Fix $\delta \in (0, 1)$ and $t \in [T]$.
%     %
%     Suppose that Assumptions~\ref{assump:frequentist} and ~\ref{assump:frequentist_continuous} hold.
%     %
%     Then, $\mu_t(\cdot)$ is Lipschitz continuous with the Lipschitz constant,
%     \begin{align*}
%         L_k \left( B + \frac{R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( d / \delta \right)} \right)
%     \end{align*}
%     with probability at least $1 - \delta$.
%     \label{lem:lipschitz_posterior_mean}
% \end{lemma}

From the definition of $\mu_t$, we obtain
\begin{align*}
    \mu_t (\cdot)
    &\leq \*v_t^\top (\cdot) \*y_t  \\
    &\leq \*v_t^\top (\cdot) \*f_t  + \*v_t^\top (\cdot) \*\epsilon_t,
\end{align*}
where $\*v_t (\*x) = \left(\*k_t^\top(\*x) (\*K_t + \sigma^2 \*I_t)^{-1}\right)^\top$, $\*f_t = \bigl(f(\*x_1), \dots, f(\*x_t) \bigr)^\top$, and $\*\epsilon_t = (\epsilon_1, \dots, \epsilon_t)^\top$.
%
Therefore, the Lipschitz constant of $\mu_t$ is bounded from above by the Lipscthiz constants of $\*v_t^\top (\cdot) \*f_t$ and $\*v_t^\top (\cdot) \*\epsilon_t$.


For the first term $\*v_t^\top (\cdot) \*f_t$, we follow the proof of Lemma~4 of \citet{vakili2021-optimal}.
%
Recall the RKHS-based definition of kernel ridge estimator:
\begin{align*}
    \mu_t = \argmin_{\mu \in \cH_k} \sum_{i=1}^t \bigl( y_{\*x_i} - \mu(\*x_i) \bigr)^2 + \sigma^2 \| \mu \|_{\cH_k}.
\end{align*}
%
Therefore, we can derive
\begin{align*}
    \min_{\mu \in \cH_k} \sum_{i=1}^t \bigl( f(\*x_i) - \mu(\*x_i) \bigr)^2 + \sigma^2 \| \mu \|_{\cH_k} 
    &= \sum_{i=1}^t \bigl( f(\*x_i) - \*v_t^\top (\*x_i) \*f_t \bigr)^2 + \sigma^2 \| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \\
    &\leq \sum_{i=1}^t \bigl( f(\*x_i) - f (\*x_i) \bigr)^2 + \sigma^2 \| f \|_{\cH_k} && \left(\because f \in \cH_k \right) \\
    &= \sigma^2 \| f \|_{\cH_k}.
\end{align*}
%
Hence, we obtain $\| \*v_t^\top (\cdot) \*f_t \|_{\cH_k} \leq \| f \|_{\cH_k} \leq B$.
%
By combining Lemma~\ref{lem:RKHS_lipschitz}, $\*v_t^\top (\cdot) \*f_t$ is $B L_k$ Lipschitz continuous.


For the second term $\*v_t^\top (\cdot) \*\epsilon_t$, we leverage the confidence bounds of kernel ridge estimator~\citep[Theorem~3.11 in][]{abbasi2013online}.
%
Let $g: \cX \times \{0, 1\} \rightarrow \RR$ as $g(\*x, 0) = g(\*x, 1) = 0$ and fix $j \in [d]$.
%
Then, the zero function $g$ belongs to the RKHS with any kernel function $\overline{k}$.
%
Thus, we design the following kernel function $\overline{k}$:
\begin{align*}
    \overline{k}\left( (\*x, 0), (\*z, 0)  \right) &= k(\*x, \*z), \\
    \overline{k}\left( (\*x, 1), (\*z, 1)  \right) &= \frac{\partial^2 k(\*x, \*z)}{\partial x_j \partial z_j}, \\
    \overline{k}\left( (\*x, 0), (\*z, 1)  \right) &= \frac{\partial k(\*x, \*z)}{ \partial z_j}, 
\end{align*}
for all $\*x, \*z \in \cX$.
%
Note that since the kernel function, $k$ has partial derivatives due to Assumption~\ref{assump:frequentist_continuous}, the derivative of the kernel and the kernel itself are the kernels again as discussed in, e.g., Sec.~9.4 in \citet{Rasmussen2005-Gaussian} and Sec.~2.2 in \citet{adler1981geometry}.
%
Thus, we can interpret $\overline{\*v}_t^\top (\cdot) \*\epsilon_t$ as the kernel ridge estimator for $g(\*x, 1)$, where $\overline{\*v}_t (\*x) = \left( \frac{\partial \*k_t^\top(\*x)}{\partial x_j} (\*K_t + \sigma^2 \*I_t)^{-1}\right)^\top$.
%
In addition, $\| g \|_{\cH_{\overline{k}}} = 0$.
%
Therefore, from Theorem~3.11 in \citet{abbasi2013online} and $|g(\*x, 1) - \overline{\*v}_t^\top (\*x) \*\epsilon_t| = |\overline{\*v}_t^\top (\*x) \*\epsilon_t|$, we obtain
\begin{align*}
    \Pr \left( |\overline{\*v}_t^\top (\*x) \*\epsilon_t| \leq \overline{\sigma}_t(\*x) \frac{R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( 1 / \delta \right)}, \forall \*x \in \cX \right) \geq 1 - \delta,
\end{align*}
where $\delta \in (0, 1)$ and $\overline{\sigma}_t(\*x) = \overline{k}\left( (\*x, 1), (\*x, 1) \right) - \frac{\partial \*k_t^\top(\*x)}{\partial x_j} (\*K_t + \sigma^2 \*I_t)^{-1} \frac{\partial \*k_t(\*x)}{\partial x_j}$ is the posterior variance that corresponds to this kernel ridge estimation.
%
Note that since the kernel matrix $\*K_t$ is defined by $ k(\*x, \*z)$, the MIG is the usual one defined by $\cX$ and $t$.
%
In addition, due to the monotonic decreasing property of the posterior variance, $\overline{\sigma}_t(\*x) \leq L_k$, we obtain
\begin{align*}
    \Pr \left( |\overline{\*v}_t^\top (\*x) \*\epsilon_t| \leq \frac{L_k R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( 1 / \delta \right)}, \forall \*x \in \cX \right) \geq 1 - \delta,
\end{align*}
and thus,
\begin{align*}
    \Pr \left( \sup_{\*x \in \cX} |\overline{\*v}_t^\top (\*x) \*\epsilon_t| \leq \frac{L_k R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( 1 / \delta \right)} \right) \geq 1 - \delta.
\end{align*}
%
Consequently, by using the union bound for all $j \in [d]$, we derive
\begin{align*}
    \Pr \left( \sup_{j \in [d]} \sup_{\*x \in \cX} |\overline{\*v}_t^\top (\*x) \*\epsilon_t| \leq \frac{L_k R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( d / \delta \right)} \right) \geq 1 - \delta,
\end{align*}
which shows that $\*v_t^\top (\*x) \*\epsilon_t$ is $\frac{L_k R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( d / \delta \right)}$ Lipschitz continuous.


Combining the Lipschitz constants of $\*v_t^\top (\cdot) \*f_t$ and $\*v_t^\top (\cdot) \*\epsilon_t$, we can obtain the result.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:bayesian_lipschitz_posterior_mean}}
\label{sec:proof_bayesian_lipschitz_posterior_mean}

First, we fix $(\*x_i)_{i \in [t]}$ without loss of generality since
\begin{align*}
    \Pr \left( \sup_{\*x \in \cX} \left| \frac{\partial \mu_t(\*u)}{\partial u_j} \Big|_{\*u = \*x} \right| > L \right)
    &= \EE_{(\*x_i)_{i \in [t]}} \left[ \Pr \left( \sup_{\*x \in \cX} \left| \frac{\partial \mu_t(\*u)}{\partial u_j} \Big|_{\*u = \*x} \right| > L \bigg| (\*x_i)_{i \in [t]} \right) \right].
\end{align*}
%
That is, the upper bound of the conditional probability given any $(\*x_i)_{i \in [t]}$ directly suggests the upper bound of target probability in the left-hand side.
%
Note that from the assumption $(\*x_i)_{i \in [t]}$ is independent of $(\epsilon_i)_{i \in [t]}$ and $f$, the observations $\*y_t$ follows Gaussian distribution even if $(\*x_i)_{i \in [t]}$ is fixed.


We leverage Slepian's inequality shown as Proposition~A.2.6 in \citet{van1996weak}:
\begin{lemma}[Slepian, Fernique, Marcus, and Shepp]
    Let $X$ and $Y$ be separable, mean-zero Gaussian processes indexed by a common index set $T$ such that
    \begin{align*}
        \EE[(X_s - X_t)^2] \leq \EE[(Y_s - Y_t)^2],
    \end{align*}
    for all $s, t \in T$. Then,
    \begin{align*}
        \Pr\left( \sup_{t \in T} X_t \geq \lambda \right) \leq \Pr\left( \sup_{t \in T} Y_t \geq \lambda \right),
    \end{align*}
    for all $\lambda > 0$.
    \label{lem:slepian}
\end{lemma}
%
The separability~\citep[Definition~5.22 in][]{Handel-HDP} holds commonly.
%
As discussed in  Remark~5.23 in \citet{Handel-HDP}, for example, if the sample path is almost surely continuous, then the separability holds.
%
Furthermore, the sample path defined by the commonly used kernel functions, such as linear, SE, and Mat\'ern-$\nu$ kernels with $\nu \geq 1$, is continuous almost surely~\citep{dacosta2024samplep}.
%
In addition, if the kernel function is continuous, the posterior mean function is also continuous, almost surely.


First, we provide the proof of the result regarding $\mu_t(\*x)$.
%
Since $\*y_t \mid (\*x_i)_{i \in [t]} \sim \cN(\*0, \*K_t + \sigma^2\*I_t)$, we can see that $\mu_t \mid (\*x_i)_{i \in [t]} \sim \cG \cP \bigl(0, k_{\mu_t}(\*x, \*z) \bigr)$, where $k_{\mu_t}(\*x, \*z) = \*k_{t}(\*x) ^\top \bigl(\*K + \sigma^2 \*I_{t} \bigr)^{-1} \*k_{t}(\*z)$.
%
Furthermore, it is known that if the kernel has mixed partial derivative $\frac{\partial^2 k(\*x, \*z)}{ \partial x_j \partial z_j}$, $f$ and its derivative $\partial f(\*x) / \partial x_j$ jointly follow GPs~\citep{Rasmussen2005-Gaussian,adler1981geometry}.
%
Specifically, the derivative is distributed as
\begin{align*}
    f^{(j)} \coloneqq \frac{\partial f(\*x)}{\partial x_j} &\sim \cG \cP\left(0, \tilde{k}(\*x, \*z) \coloneqq \frac{\partial^2 k(\*u, \*v)}{\partial u_j \partial v_j } \bigg|_{\*u=\*x, \*v=\*z} \right),
\end{align*}
for all $j \in [d]$.
%
Note that since the prior mean of $f$ is zero, the prior mean of $f^{(j)}$ is also zero.
%
As with $f$, the derivative of $\mu_t$ is distributed as 
\begin{align*}
    \mu_t^{(j)} \coloneqq \frac{\partial \mu_t(\*x)}{\partial x_j} \mid (\*x_i)_{i \in [t]} &\sim \cG \cP\left(0, \tilde{k}_{\mu_t}(\*x, \*z) \coloneqq \frac{\partial^2 k_{\mu_t}(\*u, \*v)}{\partial u_j \partial v_j } \bigg|_{\*u=\*x, \*v=\*z} \right),
\end{align*}
for all $j \in [d]$.
%
In addition, the covariance is given as
\begin{align*}
    {\rm Cov}\left( f(\*x), f^{(j)}(\*z) \right)
    &= \frac{\partial k(\*x, \*u)}{\partial u_j } \bigg|_{\*u=\*z}.
\end{align*}


Then, we see that the posterior variance of the derivative can be obtained in the same way as the usual GP calculation as follows:
\begin{align*}
    {\rm Var}\left( f^{(j)}(\*x) \mid \cD_{t} \right) &= \tilde{k}(\*x, \*x) - \tilde{k}_{\mu_t}(\*x, \*x), \\
    {\rm Cov}\left( f^{(j)}(\*x), f^{(j)}(\*z) \mid \cD_{t} \right) &= \tilde{k}(\*x, \*z) - \tilde{k}_{\mu_t}(\*x, \*z).
\end{align*}
%
On the other hand, we can obtain that
\begin{align*}
    \EE\left[ \bigl( f^{(j)}(\*x) - f^{(j)}(\*z) \bigr)^2 \right] &= \tilde{k}(\*x, \*x) + \tilde{k}(\*z, \*z) - 2\tilde{k}(\*x, \*z), \\
    \EE\left[ \bigl( \mu_t^{(j)}(\*x) - \mu_t^{(j)}(\*z) \bigr)^2 \mid (\*x_i)_{i \in [t]} \right] &= \tilde{k}_{\mu_t}(\*x, \*x) + \tilde{k}_{\mu_t}(\*z, \*z) - 2\tilde{k}_{\mu_t}(\*x, \*z),
\end{align*}
for all $\*x, \*z \in \cX$.
%
Then, we obtain
\begin{align*}
    &\EE\left[ \bigl( f^{(j)}(\*x) - f^{(j)}(\*z) \bigr)^2 \right] - \EE\left[ \bigl( \mu_t^{(j)}(\*x) - \mu_t^{(j)}(\*z) \bigr)^2 \mid (\*x_i)_{i \in [t]}\right] \\
    &= \tilde{k}(\*x, \*x) + \tilde{k}(\*z, \*z) - 2\tilde{k}(\*x, \*z) - \left( \tilde{k}_{\mu_t}(\*x, \*x) + \tilde{k}_{\mu_t}(\*z, \*z) - 2\tilde{k}_{\mu_t}(\*x, \*z) \right) \\
    &= {\rm Var}\left( f^{(j)}(\*x) \mid \cD_{t} \right) + {\rm Var}\left( f^{(j)}(\*z) \mid \cD_{t} \right) - 2 {\rm Cov}\left( f^{(j)}(\*x), f^{(j)}(\*z) \mid \cD_t \right)
    \geq 0.
\end{align*}
%
Consequently, by applying Lemma~\ref{lem:slepian}, we obtain
\begin{align*}
    \Pr\left( \sup_{\*x \in \cX} \mu^{(j)}_t(\*x) \geq \lambda \biggm\vert (\*x_i)_{i \in [t]} \right) \leq \Pr\left( \sup_{\*x \in \cX} f^{(j)}_t(\*x) \geq \lambda \right).
\end{align*}
%
Since $\mu^{(j)}_t(\*x)$ and $f^{(j)}_t(\*x)$ follow centered GPs, we obtain
\begin{align*}
    \Pr\left( \sup_{\*x \in \cX} \left| \mu^{(j)}_t(\*x) \right| \geq \lambda \biggm\vert (\*x_i)_{i \in [t]} \right) 
    \leq 2 \Pr\left( \sup_{\*x \in \cX} \mu^{(j)}_t(\*x) \geq \lambda \biggm\vert (\*x_i)_{i \in [t]} \right)
    \leq 2 \Pr\left( \sup_{\*x \in \cX} \left| f^{(j)}_t(\*x) \right| \geq \lambda \right).
\end{align*}
%
Hence, from Lemma~\ref{assump:Bayesian_continuous}, we obtain the desired result.



We can obtain the result regarding $f^{(j)}(\*x) - \mu_t^{(j)}(\*x)$ in almost the same proof.
%
We can see that
\begin{align*}
    f^{(j)}(\*x) - \mu_t^{(j)}(\*x) \sim \cG \cP(0, \tilde{k}(\*x, \*z) - \tilde{k}_{\mu_t}(\*x, \*z)).
\end{align*}
%
Then, 
\begin{align*}
    &\EE\left[ \bigl( f^{(j)}(\*x) - f^{(j)}(\*z) \bigr)^2 \right] - \EE\left[ \left( f^{(j)}(\*x) - \mu_t^{(j)}(\*x) - \bigl( f^{(j)}(\*x) - \mu_t^{(j)}(\*x) \bigr) \right)^2 \biggm\vert (\*x_i)_{i \in [t]} \right] \\
    &= \tilde{k}_{\mu_t}(\*x, \*x) + \tilde{k}_{\mu_t}(\*z, \*z) - 2\tilde{k}_{\mu_t}(\*x, \*z)
    \geq 0.
\end{align*}
%
Remained proof is the same as the case of $\mu_t^{(j)}$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:UB_error_frequentist_continuous}}
\label{sec:proof_UB_error_frequentist_continuous}

As with the existing studies~\citep[e.g., ][]{Srinivas2010-Gaussian}, we consider the discretization of input space.
%
Let $\overline{\cX} \subset \cX$ be a finite set with each dimension equally divided into $\lceil \tau d r \rceil$, where $\tau > 0$.
%
Therefore, $|\overline{\cX}| = \lceil \tau d r \rceil^d$ and $\sup_{\*x \in \cX} \| \*x - [\*x] \|_1 \leq \frac{1}{\tau}$, where $[\*x]$ is the nearest input in $\overline{\cX}$, that is, $[\*x] = \argmin_{\tilde{\*x} \in \overline{\cX}} \|\tilde{\*x} - \*x \|_1$.
%
Note that we leverage $\overline{\cX}$ purely sake for the analysis, and $\overline{\cX}$ is not related to the algorithm.


From Assumption~\ref{assump:frequentist_continuous} and Lemma~\ref{lem:RKHS_lipschitz}, we see that $f$ is $B L_k$ Lipschitz continuous.
%
Furthermore, from Lemma~\ref{lem:lipschitz_posterior_mean}, $\mu_t$ is $L_k \left( B + \frac{R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( \frac{d}{\delta} \right)} \right)$ Lipschitz continuous with probability at least $1 - \delta$.
%
Combining the above, we see that $f - \mu_t$ is $L_k \left( 2B + \frac{R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( \frac{d}{\delta} \right)} \right)$ Lipschitz continuous with probability at least $1 - \delta$.

% From Lemma~\ref{lem:lipschitz_posterior_mean} and Assumprion~\ref{assump:frequentist}, we can see that
% \begin{align*}
%     \| f(\*x^*) - \mu_T(\*x^*) \|_{\cH_k} 
%     \leq \| f(\*x^*) \|_{\cH_k} + \| \mu_T(\*x^*) \|_{\cH_k}
%     \leq 2B + \frac{R}{\sigma} \sqrt{ 2T \log \left( \frac{2T}{\delta} \right)}.
% \end{align*}
% %
% Therefore, from Assumption~\ref{assump:frequentist_continuous} and Lemma~\ref{lem:RKHS_lipschitz}, $f(\*x^*) - \mu_T(\*x^*)$ is Lipschitz continuous with high probability.


From the above arguments, by combining Lemma~\ref{lem:bound_vakili} and the union bound, the following events hold simultaneously with probability at least $1 - \delta$:
\begin{enumerate}
    \item $f(\*x) - \mu_T(\*x)$ is $L_{\rm res}(T)$ Lipschitz continuous, where $L_{\rm res}(T) = L_k \left( 2B + \frac{R}{\sigma} \sqrt{ 2\gamma_T + 2 \log \left( \frac{2d}{\delta} \right)} \right)$.
    \item The confidence bounds on $\overline{\cX}$ hold; that is, 
    \begin{align*}
        \forall \*x \in \overline{\cX}, |f(\*x) - \mu_T(\*x)| \leq \beta_{\delta, \tau}^{1/2} \sigma_T (\*x),
    \end{align*}
    where $\beta_{\delta, \tau} = \left( B + \frac{R}{\sigma} \sqrt{ 2 d \log \left( \tau d r + 1 \right) + 2 \log \left( \frac{4}{\delta} \right)} \right)^2$.
\end{enumerate}



Then, we can obtain the upper bound as follows:
\begin{align*}
    E_T &= \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f(\*x^*) - \mu_T(\*x^*) \right)^2 \right] \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f([\*x^*]) - \mu_T([\*x^*]) + L_{\rm res}(T) \| \*x^* - [\*x^*] \|_1 \right)^2  \right] 
    && \left(\because \text{The above event~1} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f([\*x^*]) - \mu_T([\*x^*]) + \frac{L_{\rm res}(T)}{\tau} \right)^2  \right] 
    && \left(\because \text{The definition of $\overline{\cX}$} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( \beta_{\delta, \tau}^{1/2} \sigma_T([\*x^*]) + \frac{L_{\rm res}(T)}{\tau} \right)^2  \right] 
    && \left(\because \text{The above event~2} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( \beta_{\delta, \tau}^{1/2} \sigma_T(\*x^*) + \frac{\beta_{\delta, \tau}^{1/2} L_{\sigma} + L_{\rm res}(T)}{\tau} \right)^2  \right] 
    && \left(\because \text{Lemma~\ref{lem:Lipschitz_posterior_var}} \right) \\
    &\leq 2 \beta_{\delta, \tau} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] + 2 \left( \frac{\beta_{\delta, \tau}^{1/2} L_{\sigma} + L_{\rm res}(T)}{\tau} \right)^2.
    && \left(\because (a + b)^2 / 2 \leq a^2 + b^2 \right)
\end{align*}
%
If we set $\tau = T$, noting that $L_{\rm res}(T) = \cO(\sqrt{\gamma_T})$ and $\beta_{\delta, \tau}^{1/2} = \cO(\log(T / \delta))$, we obtain the following:
\begin{align*}
    E_T 
    &\leq 2 \beta_{\delta, T} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] 
    + \cO \left( \frac{\max\{\gamma_T, \log(T / \delta)\}}{T^2} \right).
\end{align*}
%
Although by setting $\tau = \Omega(T)$, we can make the second term small arbitrarily, $\beta_{\delta, T} = \Theta\left( \log( T / \delta) \right)$ and $\EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] = \Omega(1 / T)$~\citep[Lemma~4.2 in][]{takeno2024-posterior}.
%
Therefore, since the first term is $\Omega \left( \frac{\log( T / \delta)}{T} \right)$ and $\frac{\max\{\gamma_T, \log(T / \delta)\}}{T^2} = \cO\left( \frac{\log( 1 / \delta)}{T} \right)$ if $\gamma_T$ is sublinear, we do not set $\tau$ more large value for simplicity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:UB_error_bayesian_continuous}}
\label{sec:proof_UB_error_bayesian_continuous}


As with the existing studies~\citep[e.g., ][]{Srinivas2010-Gaussian}, we consider the discretization of input space.
%
Let $\overline{\cX} \subset \cX$ be a finite set with each dimension equally divided into $\lceil \tau d r \rceil$, where $\tau > 0$.
%
Therefore, $|\overline{\cX}| = \lceil \tau d r \rceil^d$ and $\sup_{\*x \in \cX} \| \*x - [\*x] \|_1 \leq \frac{1}{\tau}$, where $[\*x]$ is the nearest input in $\overline{\cX}$, that is, $[\*x] = \argmin_{\tilde{\*x} \in \overline{\cX}} \|\tilde{\*x} - \*x \|_1$.
%
Note that we leverage $\overline{\cX}$ purely sake for the analysis, and $\overline{\cX}$ is not related to the algorithm.


In addition, from Lemma~\ref{lem:bayesian_lipschitz_posterior_mean}, the following inequality holds with probability at least $1 - \delta$:
\begin{align*}
    \sup_{j \in d} \sup_{\*x \in \cX} 
    \left| \frac{\partial r_t(\*u)}{\partial u_j} \Big|_{\*u = \*x} \right|
    \leq b \sqrt{\log (2ad / \delta)},
\end{align*}
which implies that $L_{\rm res}$, the Lipschitz constant of $r_t(\*x) = f(\*x) - \mu_T(\*x)$, can be bounded from above.



Then, by combining the above argument, Lemma~\ref{lem:bound_srinivas}, and the union bound, the following events hold simultaneously with probability at least $1 - \delta$:
\begin{enumerate}
    \item $f(\*x) - \mu_T(\*x)$ is $L_{\rm res}$ Lipschitz continuous, where $L_{\rm res} =b \sqrt{\log (4ad / \delta)}$.
    \item The confidence bounds on $\overline{\cX}$ hold; that is, 
    \begin{align*}
        \forall \*x \in \overline{\cX}, |f(\*x) - \mu_T(\*x)| \leq \beta_{\delta, \tau}^{1/2} \sigma_T (\*x),
    \end{align*}
    where $\beta_{\delta, \tau} = 2d \log (\tau d r + 1) + 2 \log (2 / \delta)$.
\end{enumerate}



Hence, we can obtain the upper bound as follows:
\begin{align*}
    E_T &= \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f(\*x^*) - \mu_T(\*x^*) \right)^2 \right] \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f([\*x^*]) - \mu_T([\*x^*]) + L_{\rm res} \| \*x^* - [\*x^*] \|_1 \right)^2  \right] 
    && \left(\because \text{The above event~1} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( f([\*x^*]) - \mu_T([\*x^*]) + \frac{L_{\rm res}}{\tau} \right)^2  \right] 
    && \left(\because \text{The definition of $\overline{\cX}$} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( \beta_{\delta, \tau}^{1/2} \sigma_T([\*x^*]) + \frac{L_{\rm res}}{\tau} \right)^2  \right] 
    && \left(\because \text{The above event~2} \right) \\
    &\leq \max_{p \in \cP} \EE_{p(\*x^*)} \left[ \left( \beta_{\delta, \tau}^{1/2} \sigma_T(\*x^*) + \frac{\beta_{\delta, \tau}^{1/2} L_{\sigma} + L_{\rm res}}{\tau} \right)^2  \right] 
    && \left(\because \text{Lemma~\ref{lem:Lipschitz_posterior_var}} \right) \\
    &\leq 2 \beta_{\delta, \tau} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] + 2 \left( \frac{\beta_{\delta, \tau}^{1/2} L_{\sigma} + L_{\rm res}}{\tau} \right)^2.
    && \left(\because (a + b)^2 / 2 \leq a^2 + b^2 \right)
\end{align*}
%
Then, by setting $\tau = T$, we can see that
\begin{align*}
    E_T 
    &\leq 2 \beta_{\delta, T} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] 
    + \cO\left( \frac{\log(T / \delta)}{T^2} \right).
\end{align*}


Although by setting $\tau = \Omega(T)$, we can make the second term small arbitrarily, we do not do so since the first term is dominant compared with $\cO\left( \frac{\log(T / \delta)}{T^2} \right)$ term.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:UB_absolute_error}}
\label{sec:UB_absolute_error_proof}


Since the maximum $\tilde{p}_T(\*x^{*}) = \argmax_{p \in \cP} \EE_{p(\*x^{*})} \left[ |f(\*x^{*}) - \mu_T(\*x^{*})| \right] $ exists, we obtain
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ |f(\*x^{*}) - \mu_T(\*x^{*})| \right] 
    &= \EE_{\tilde{p}_T(\*x^{*})} \left[ |f(\*x^{*}) - \mu_T(\*x^{*})| \right], \\
    &\leq \sqrt{ \EE_{\tilde{p}_T(\*x^{*})} \left[ (f(\*x^{*}) - \mu_T(\*x^{*}))^2 \right]}, \\
    &\leq \sqrt{ \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ (f(\*x^{*}) - \mu_T(\*x^{*}))^2 \right]},
    % &\coloneqq \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ ( f(\*x^{*}) - \mu_T(\*x^{*}) )^2 \right]
\end{align*}
where we used Jensen's inequality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:UB_entropy}}
\label{sec:UB_entropy_proof}

Since the maximum $\overline{p}_T(\*x^{*}) = \argmax_{p \in \cP} \EE_{p(\*x^{*})} \left[ H[f(\*x) \mid \cD_t] \right] $ exists, we obtain
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x)}[ H[f(\*x) \mid \cD_t] ]
    &= \frac{1}{2} \EE_{\overline{p}_T(\*x^{*})}[ \log(2\pi e \sigma_{T}^2(\*x)) ] \\
    &\leq \frac{1}{2} \log(2\pi e \EE_{\overline{p}_T(\*x^{*})}[  \sigma_{T}^2(\*x) ]) && (\because \text{Jensen's inequality}) \\
    &\leq \frac{1}{2} \log \left( 2\pi e \max_{p \in \cP} \EE_{p (\*x)}[ \sigma_{T}^2(\*x) ] \right).
    % &\leq \frac{1}{2} \log\left( \frac{2\pi e}{T} \left( 2 C_1 \gamma_T + 4\log\frac{1}{\delta} + 8 \log(4) + 1 \right) \right) && \left(\because  \text{Theorem~\ref{theo:error_convergence_RS} and \ref{theo:error_convergence_greedy}}\right).
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs for Section~\ref{sec:proposed}}
\label{sec:proposed_proof}

\subsection{Proof of Theorem~\ref{theo:error_convergence_RS}}

From the definition, $\sigma_t^2(\*x)$ is monotonically decreasing along with $\cD_{t-1} \subset \cD_t$.
%
Therefore, for all $t \leq T$ and $\*x_1, \dots, \*x_T$, 
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{T}^2(\*x^*) \mid \*x_1, \dots, \*x_T]
    \leq 
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{t}^2(\*x^*) \mid \*x_1, \dots, \*x_t].
\end{align*}
%
Note that $\*x_1, \dots, \*x_T$ are random variables due to the randomness of the algorithm.
%
Hence, we obtain
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{T}^2(\*x^*) \mid \*x_1, \dots, \*x_T]
    &\leq 
    \frac{1}{T} \sum_{t=1}^T
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{t-1}^2(\*x^*) \mid \*x_1, \dots, \*x_{t-1}] \\
    &\leq
    \frac{1}{T} \sum_{t=1}^T
    \EE_{p_t(\*x_t)}[\sigma_{t-1}^2(\*x_t) \mid \*x_1, \dots, \*x_{t-1}]. && (\because \text{Definition of $p_t$})
\end{align*}
%
Then, we apply the following lemma \citep[Lemma 3 in][]{kirschner2018-information}:
\begin{lemma}
    Let $Y_t$ be any non-negative stochastic process adapted to a filtration $\{ \cF_t \}$, and define $m_t = \EE[Y_t \mid \cF_{t-1}]$.
    %
    Further assume that $Y_t \leq b_t$ for a fixed, non-decreasing sequence $(b_t)_{t \geq 1}$.
    %
    Then, if $b_T \geq 1$, with probability at least $1 - \delta$ for any $T \geq 1$, it holds that,
    \begin{align*}
        \sum_{t=1}^T m_t \leq 2 \sum_{t=1}^T Y_t + 4 b_T \log \frac{1}{\delta} + 8b_T \log(4 b_T) + 1.
    \end{align*}
\end{lemma}
%
The random variable $\EE_{p_t(\*x)}[\sigma_{t}^2(\*x) \mid \*x_1, \dots, \*x_t]$ satisfies the condition of this lemma by setting $b_t = 1$ for all $t \in [T]$.
%
Therefore, with probability at least $1 - \delta$,
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{T}^2(\*x^*) \mid \*x_1, \dots, \*x_T]
    &\leq 
    \frac{1}{T} \left( 
    2 \sum_{t=1}^T \sigma_{t-1}^2(\*x_t) + 4\log\frac{1}{\delta} + 8 \log(4) + 1
    \right) \\
    &\leq 
    \frac{1}{T} \left( 
    2 C_1 \gamma_T + 4\log\frac{1}{\delta} + 8 \log(4) + 1
    \right).
\end{align*}
%
Here, we use $\sum_{t=1}^T \sigma_{t-1}^2(\*x_t) \leq C_1 \gamma_T$ \citep[Lemma~5.2 in][]{Srinivas2010-Gaussian}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{theo:error_convergence_greedy}}

From the definition, $\sigma_t^2(\*x)$ is monotonically decreasing along with $\cD_{t-1} \subset \cD_t$.
%
Therefore, for all $t \leq T$ and $\*x_1, \dots, \*x_T$, 
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{T}^2(\*x^*) \mid \*x_1, \dots, \*x_T]
    \leq 
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{t}^2(\*x^*) \mid \*x_1, \dots, \*x_t].
\end{align*}
%
Hence, we obtain
\begin{align*}
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{T}^2(\*x^*) \mid \*x_1, \dots, \*x_T]
    &\leq 
    \frac{1}{T} \sum_{t=1}^T
    \max_{p \in \cP} \EE_{p(\*x^*)}[\sigma_{t-1}^2(\*x^*) \mid \*x_1, \dots, \*x_{t-1}] \\
    &\leq
    \frac{1}{T} \sum_{t=1}^T \sigma_{t-1}^2(\*x_t) && (\because \text{Definition of $\cX_t$}) \\
    &\leq \frac{C_1 \gamma_T}{T}. && (\because \text{Lemma~5.2 in \citet{Srinivas2010-Gaussian}})
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other Experimental Settings and Results}
\label{sec:exp_settings}



\subsection{Results for Variance}
\label{sec:exp_variance}

Figure~\ref{fig:syn_loss_results} shows the result of $\max_{p \in \cP} \EE_{p(\*x^*)} \left[ \sigma_t^2 (\*x^*) \right]$, which suggests that the proposed methods effectively minimize $\max_{p \in \cP} \EE_{p(\*x^*)} \left[ \sigma_t^2 (\*x^*) \right]$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/experiments/legend.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_loss_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_loss_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_loss_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_loss_0.1.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_loss_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_loss_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_loss_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_loss_0.1.pdf}
    \caption{
        Result of $\max_{p \in \cP} \EE_{p(\*x^*)} \left[ \sigma_t^2 (\*x^*) \right]$ in the synthetic data experiments with $\eta=0, 0.001, 0.01, 0.1$.
        %
        The horizontal and vertical axes show the number of iterations and $\max_{p \in \cP} \EE_{p(\*x^*)} \left[ \sigma_t^2 (\*x^*) \right]$, respectively.
        %
        The error bar shows mean and standard errors for 20 random trials regarding the random initial point (and the algorithm's randomness).
        %
        The top and bottom columns represent the results of the GPR model with SE and Mat\'ern kernels, respectively.
        %
        % 最悪期待損失を用いて評価を行った場合の人工データ実験結果.
        % %
        % 横軸は反復数, 縦軸は最悪期待損失を示す.
        % %
        % ランダムに初期点を変更したことによる20回試行の実験結果の平均と標準誤差を示す.
        % %
        % 上段はRBFカーネル, 下段はMaternカーネルをGP回帰のカーネルとして用いた場合の結果を示す.
        % %
        % 参照分布は平均が${\* 0}$, 共分散行列が$0.2*{\*I}_3$の三次元正規分布とする.
        % %
        % ここで, ${\*I}_3$は三次元の単位行列である.
        % %
        % また, 参照分布からの最大距離$\epsilon=0, 0.001, 0.01, 0.1$とする.
        %
    }
    \label{fig:syn_loss_results}
\end{figure*}


\subsection{Details on Implementation of EPIG}
\label{sec:EPIG}

EPIG is defined as follows~\citep{bickford2023-prediction}:
\begin{align*}
    \*x_t = \argmax_{\*x \in \cX} \EE_{p(\*x^*)}\left[ H[ y_{\*x^*} \mid \cD_{t-1}, \*x] - H[ y_{\*x^*} \mid \cD_{t-1}] \right].
\end{align*}
%
Although \citet{bickford2023-prediction} have discussed the efficient computation for EPIG, in the regression problem, the EPIG can be computed analytically except for the expectation over $p(\*x^*)$ as follows:
\begin{align*}
    \*x_t = \argmax_{\*x \in \cX} \EE_{p(\*x^*)}\left[ \log \left( \frac{\sigma_{t-1}^2(\*x, \*x^*) }{ \sqrt{\sigma_{t-1}^2(\*x) + \sigma^2} \sqrt{\sigma_{t-1}^2(\*x^*) + \sigma^2} } \right) \right],
\end{align*}
where $\sigma_{t-1}^2(\*x, \*x^*)$ is the posterior covariance between $\*x$ and $\*x^*$.
%
Since we focus on the discrete input domain in the experiments, the expectation over $p(\*x^*)$ can also be computed analytically.
%
We used the above equation for the implementation.


\subsection{Details on Real-World Datasets}
\label{sec:datasets_detail}

The King County house sales dataset is a dataset used to predict house prices in King County by 7-dimensional features, such as the area and the number of rooms. 
%
This dataset has been used for testing the regression~\citep{park2020additive}, and a similar dataset has also been used for the AL studies~\citep{park2020robust}.
%
Although this dataset includes 20000 data, we used randomly chosen 1000 data for simplicity.


% King County House Sales Datasetは回帰問題の対象として使用されてきたデータセット\citep{park2020additive}であり, 既存の能動学習の研究においても類似したデータセットが用いられている\citep{park2020robust}.
%
% データセットには20000個のデータが含まれているが, 本実験では扱いやすさのため1000個をランダムに選択して使用する.
%
% データの入力次元は7次元である.


Red wine quality dataset~\citep{wine_quality_186} is a dataset used to predict the quality of wines from the wine ingredients expressed by 11-dimensional features.
%
This dataset includes 1600 data and has been used for the regression problem~\citep{cortez2009using}.


% Red Wine Quality Dataset\citep{wine_quality_186}はワインの成分からワインの品質を予測することを目的とするデータセットである.
% %
% Red Wine Quality Datasetは回帰問題の対象として使用されてきたデータセットである\citep{cortez2009using}. 
% %
% データセットには1600個のデータが含まれており, データの入力次元は11次元である.


Auto MPG dataset~\citep{auto_mpg_9} is a dataset used to predict automobile fuel efficiency from 6-dimensional features, such as the weight of the automobile and engine horsepower.
%
Auto MPG dataset has been used for the AL reserch~\citep{park2020robust}.
%
This dataset includes 399 data.


% Auto MPG Dataset\citep{auto_mpg_9}は自動車の重量やエンジンの馬力から自動車の燃費を予測することを目的としたデータセットである.
% %
% Auto MPG Datasetは既存の能動学習の研究で使用されているデータセットである\citep{park2020robust}.
% %
% データセットには399個のデータセットが含まれており, データの入力次元は6次元である.
