\section{Problem Statements and Its Property}
\label{sec:problem}

This section provides our problem setup and its properties.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Statement}

We aim to minimize the worst-case expected errors regarding the GP prediction $\mu_T (\*x)$ after $T$-th function evaluations:
\begin{align}
    % {\rm DRAE}_T &\coloneqq \max_{p \in \cP} \EE_{p(\*x)} \left[ | f(\*x) - \mu_T(\*x) | \right] \\
    E_T &\coloneqq \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ ( f(\*x^{*}) - \mu_T(\*x^{*}) )^2 \right],
    \label{eq:target_error}
\end{align}
where $\cP$ is a set of target distributions over the input space $\cX$ called ambiguity set~\citep{chen2020distributionally}.
%
We assume that $\max_{p \in \cP} \EE_{p(\*x^*)} \left[ g(\*x^*) \right]$ exists for any continuous function $g: \cX \rightarrow \RR$.
%
This paper concentrates on the setting where the training input space from which we can obtain labels includes the test input space.



Our problem setup can be seen as the generalization of the target distribution-aware AL and the AL for the worst-case error $\max_{\*x \in \cX} ( f(\*x) - \mu_T(\*x) )^2$.
%
This is because our problem is equivalent to the target distribution-aware AL if we set $|\cP| = 1$ and to the worst-case error minimization if $\cP$ includes $\{p \in \cP_{\cX} \mid \exists \*x \in \cX, p(\*x) = 1 \}$, where $\cP_{\rm \cX}$ is the set of the distributions over $\cX$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High Probability Bound of Error}

% First, we provide the upper bound by the posterior variance.
%
If the input space $\cX$ is finite, we can obtain the upper bound of Eq.~\eqref{eq:target_error} as the direct consequence of Lemmas~\ref{lem:bound_srinivas} and \ref{lem:bound_vakili}:
\begin{lemma}
    Fix $\delta \in (0, 1)$ and $T \in \NN$.
    %
    Suppose that Assumption~\ref{assump:Bayesian} holds and $\beta_\delta$ is set as in Lemma~\ref{lem:bound_srinivas}, or Assumption~\ref{assump:frequentist} holds and $\beta_\delta$ is set as in Lemma~\ref{lem:bound_vakili}.
    %
    Then, the following holds with probability at least $1 - \delta$:
    \begin{align*}
        E_T &\leq \beta_{\delta} \max_{p \in \cP} \EE_{p(\*x^{*})}\left[ \sigma^2_{T}(\*x^{*}) \right].
    \end{align*}
    \label{lem:UB_error_discrete}
\end{lemma}


% Next, let us consider the case that $\cX = [0, r]^d$.
%
For continuous $\cX$, the confidence parameter $\beta_\delta \propto \log |\cX|$ diverges if we apply Lemmas~\ref{lem:bound_srinivas} and \ref{lem:bound_vakili} directly.
%
Therefore, in this case, the Lipschitz property is often leveraged~\citep{Chowdhury2017-on,vakili2021-optimal}.
%
The Lipschitz constant of $f$ can be directly derived from the Assumption~\ref{assump:Bayesian_continuous}, or Assumption~\ref{assump:frequentist_continuous} and Lemma~\ref{lem:RKHS_lipschitz}~\citep{Srinivas2010-Gaussian,freitas2012exponential}.


Furthermore, we need the Lipschitz constant of $\mu_T$.
%
In the frequentist setting, the Lipschitz constant for $\mu_T$ can be derived as $\cO(L_k \sqrt{t \log t})$ by Lemma~4 in \citet{vakili2021-optimal} and Lemma~\ref{lem:RKHS_lipschitz}.
%
To obtain a slightly tighter upper bound, we show the following lemma:
% \begin{lemma}[Modified from Lemma~F.1 of \citet{vakili2022improved}]
%     Fix $\delta \in (0, 1)$ and $t \in [T]$.
%     %
%     Suppose that Assumptions~\ref{assump:frequentist} and ~\ref{assump:frequentist_continuous} hold.
%     %
%     Then, the RKHS norm of $\mu_t(\cdot)$ satisfies the following with probability at least $1 - \delta$:
%     \begin{align*}
%         \| \mu_t \|_{\cH_k} \leq B + \frac{R}{\sigma} \sqrt{ 2t \log \left( \frac{2t}{\delta} \right)}.
%     \end{align*}
%     %
%     Thus, $\mu_T$ is $L_k \bigl( B + \frac{R}{\sigma} \sqrt{ 2t \log \left( 2t / \delta \right)} \bigr)$ Lipschitz continuous.
%     \label{lem:RKHS_norm_posterior_mean}
% \end{lemma}
\begin{lemma}
    Fix $\delta \in (0, 1)$ and $t \in [T]$.
    %
    Suppose that Assumptions~\ref{assump:frequentist} and ~\ref{assump:frequentist_continuous} hold.
    %
    Then, $\mu_t(\cdot)$ is Lipschitz continuous with the Lipschitz constant,
    \begin{align*}
        L_k \left( B + \frac{R}{\sigma} \sqrt{ 2 \gamma_t + 2 \log \left( \frac{d}{\delta} \right)} \right)
    \end{align*}
    with probability at least $1 - \delta$.
    \label{lem:lipschitz_posterior_mean}
\end{lemma}
We show the proof in Appendix~\ref{sec:proof_lipschitz_posterior_mean}.
%
Since the MIG $\gamma_T$ is sublinear for the kernels on which we mainly focus, the upper bound $\cO(L_k \sqrt{\gamma_t})$ is tighter than $\cO(L_k \sqrt{t \log t})$.



In the Bayesian setting, the upper bound of the Lipschitz constant for $\mu_T$ has not been shown to our knowledge.
%
Therefore, we show the following lemma:
\begin{lemma}
    Fix $\delta \in (0, 1)$ and $t \in [T]$.
    %
    Suppose that Assumptions~\ref{assump:Bayesian} and \ref{assump:Bayesian_continuous} hold and the kernel has mixed partial derivative $\frac{\partial^2 k(\*x, \*z)}{ \partial x_j \partial z_j}$ for all $j \in [d]$.
    %
    Set $a$ and $b$ as in Lemma~\ref{assump:Bayesian_continuous}.
    %
    Assume that $(\*x_i)_{i \in [t]}$ is independent of $(\epsilon_i)_{i \in [t]}$ and $f$.
    %
    Then, $\mu_t$ and $r_t(\*x) \coloneqq f(\*x) - \mu_t(\*x)$ satisfies the following:
    \begin{align*}
        \Pr \left( \sup_{\*x \in \cX} \left| \frac{\partial \mu_t(\*u)}{\partial u_j} \Big|_{\*u = \*x} \right| > L \right) \leq 2a \exp \left( - \frac{L^2}{b^2} \right), \\
        \Pr \left( \sup_{\*x \in \cX} \left| \frac{\partial r_t(\*u)}{\partial u_j} \Big|_{\*u = \*x} \right| > L \right) \leq 2a \exp \left( - \frac{L^2}{b^2} \right), 
    \end{align*}
    for all $j \in [d]$.
    \label{lem:bayesian_lipschitz_posterior_mean}
\end{lemma}
See Appendix~\ref{sec:proof_bayesian_lipschitz_posterior_mean} for the proof, in which we leverage Slepian's inequality~\citep[Proposition~A.2.6 in][]{van1996weak} and the fact that the derivative of the sample path follows GP jointly when the kernel is differentiable.



By leveraging the above results, even if $\cX$ is continuous, we can obtain the following upper bound of Eq.~\eqref{eq:target_error}:
\begin{lemma}
    Suppose that Assumptions~\ref{assump:frequentist} and ~\ref{assump:frequentist_continuous} hold.
    %
    Fix $\delta \in (0, 1)$ and $T \in \NN$.
    %
    Then, the following holds with probability at least $1 - \delta$:
    \begin{align*}
        E_T 
        &\leq 2 \beta_{\delta, T} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] 
        + \cO \left( \frac{\max\{\gamma_T, \log(\frac{T}{\delta})\}}{T^2} \right).
    \end{align*}
    where $\beta_{\delta, T} = \left( B + \frac{R}{\sigma} \sqrt{ 2 d \log \left( T d r + 1 \right) + 2 \log \left( \frac{4}{\delta} \right)} \right)^2$.
    \label{lem:UB_error_frequentist_continuous}
\end{lemma}
\begin{lemma}
    Suppose that Assumptions~\ref{assump:Bayesian} and \ref{assump:Bayesian_continuous} hold.
    %
    Fix $\delta \in (0, 1)$ and $T \in \NN$.
    %
    Then, the following holds with probability at least $1 - \delta$:
    \begin{align*}
        E_T 
        &\leq 2 \beta_{\delta, T} \max_{p \in \cP} \EE_{p(\*x^*)} \left[  \sigma_T^2(\*x^*) \right] 
        + \cO\left( \frac{\log(\frac{T}{\delta})}{T^2} \right),
    \end{align*}
    where $\beta_{\delta, T} = 2d \log (T d r + 1) + 2 \log (2 / \delta)$.
    \label{lem:UB_error_bayesian_continuous}
\end{lemma}
See Appendices~\ref{sec:proof_UB_error_frequentist_continuous} and ~\ref{sec:proof_UB_error_bayesian_continuous} for the proof.


Consequently, we can  minimize Eq.~\eqref{eq:target_error} by minimizing $\max_{p \in \cP} \EE_{p(\*x^{*})}\left[ \sigma^2_{T}(\*x^{*}) \right]$.
%
In this perspective, the US and RS are theoretically guaranteed because of $\max_{p \in \cP} \EE_{p(\*x^{*})}\left[ \sigma^2_{T}(\*x^{*}) \right] \leq \max_{\*x \in \cX} \sigma^2_T (\*x)$ and Proposition~\ref{prop:us_rs}.
%
However, the US and RS do not incorporate the information of $\cP$.
%
Therefore, the practical effectiveness of the US and RS is limited.
%
% Hence, we design algorithms that enjoy both a similar convergence guarantee and practical effectiveness incorporating the information of $\cP$.


\subsection{Other Performance Mesuares}

Although we mainly discuss the squared error, other measures can also be bounded from above:
\begin{lemma}
    The worst-case expected absolute error for any $T \in \NN$ is bounded from above as follows:
    \begin{align*}
        \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ |f(\*x^{*}) - \mu_T(\*x^{*})| \right]
        \leq \sqrt{E_T},
        % &\coloneqq \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ ( f(\*x^{*}) - \mu_T(\*x^{*}) )^2 \right]
    \end{align*}
    where $E_T$ is defined as in Eq.~\eqref{eq:target_error}.
    \label{lem:UB_absolute_error}
\end{lemma}
%
\begin{lemma}
    The worst-case expectation of entropy for any $T \in \NN$ is bounded from above as follows:
    \begin{align*}
        \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ H\left[ f(\*x^*) \mid \cD_T \right] \right]
        % &= \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ \frac{1}{2} \log \left(2 \pi e \sigma_T^2(\*x^*) \right) \right] \\
        &\leq \frac{1}{2} \log \left(2 \pi e \tilde{E}_T \right),
        % &\leq \frac{1}{2} \log \left(2 \pi e \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ \sigma_T^2(\*x^*) \right] \right),
        % &= \cO\left( \log \left( \max_{p \in \cP} \EE_{p(\*x^{*})} \left[ \sigma_T^2(\*x^*) \right] \right)\right)
    \end{align*}
    where $\tilde{E}_T = \max_{p \in \cP} \EE_{p(\*x^{*})}\left[ \sigma^2_{T}(\*x^{*}) \right]$ and $H[f(\*x) \mid \cD_T] = \log \left(\sqrt{2 \pi e} \sigma_T(\*x) \right)$ is Shannon entropy.
    \label{lem:UB_entropy}
\end{lemma}
%
See Appendices~\ref{sec:UB_absolute_error_proof} and \ref{sec:UB_entropy_proof} for the proof.
%
Therefore, minimizing $\max_{p \in \cP} \EE_{p(\*x^{*})}\left[ \sigma^2_{T}(\*x^{*}) \right]$ also provides the convergence of the absolute error and the entropy\footnote{For the absolute error, we can design algorithms that directly reduce $\sigma_t$, not $\sigma_t^2$, and achieves the similar theoretical guarantee.}.


% \subsection{Discussion}

% Our problem setup can be seen as the generalization of the target distribution-aware AL and the AL for the worst-case error, i.e., $\max_{\*x \in \cX} ( f(\*x) - \mu_T(\*x) )^2$.
% %
% This is because our problem is equivalent to the target distribution-aware AL if we set $|\cP| = 1$ and to the worst-case error minimization if $\cP$ includes $\{p \in \cP_{\cX} \mid \exist \*x \in \cX, p(\*x) = 1 \}$, where $\cP_{\rm \cX}$ is the set of the distributions over $\cX$.
% %
% Clearly, for the worst-case analysis for $\max_{\*x \in \cX} ( f(\*x) - \mu_T(\*x) )^2$, we must use the method that reduce the largest variance $\max_{\*x \in \cX} \sigma_t^2(\*x)$.
% %
% This is satisfied by the US and RS, as shown in Proposition~\ref{prop:us_rs}.