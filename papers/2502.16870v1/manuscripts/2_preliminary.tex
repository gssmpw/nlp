\section{Background}
\label{sec:background}

This section provides the known properties of the GPR.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GPR model}

The GPR model \citep{Rasmussen2005-Gaussian} is a kernel-based regression model.
%
Let us consider that we have already obtained the training dataset of input-output pair $\cD_t = \{ (\*x_i, y_{\*x_i}) \}_{i=1}^t$, where $\forall i, \*x_i \in \cX \subset \RR^d$, $y_{\*x_i} \in \RR$, and $d$ is an input dimension.
%
The GPR model assumes that, without loss of generality, $f$ follows zero-mean GP, that is, $f \sim \cG \cP (0, k)$, where $k: \cX \times \cX \mapsto \RR$ is a predefined positive semidefinite kernel function.
%
In addition, the $i$-th observation $y_{\*x_i}$ is assumed to be contaminated by i.i.d. Gaussian noise $\epsilon_i \sim \cN(0, \sigma^2)$ as $y_{\*x_i} = f(\*x_i) + \epsilon_i$.
%
Then, the posterior distribution of $f$ becomes again a GP, whose mean and variance are analytically derived as follows:
\begin{equation}
    \begin{split}
        \mu_{t}(\*x) &= \*k_{t}(\*x)^\top \bigl(\*K + \sigma^2 \*I_{t} \bigr)^{-1} \*y_{t}, \\
        \sigma_{t}^2 (\*x) &= k(\*x, \*x) - \*k_{t}(\*x) ^\top \bigl(\*K + \sigma^2 \*I_{t} \bigr)^{-1} \*k_{t}(\*x),
    \end{split}
    \label{eq:GP}
\end{equation}
where $\*k_{t}(\*x) \coloneqq \bigl( k(\*x, \*x_1), \dots, k(\*x, \*x_{t}) \bigr)^\top \in \RR^{t}$, $\*K \in \RR^{t \times t}$ is the kernel matrix whose $(i, j)$-element is $k(\*x_i, \*x_j)$, $\*I_{t} \in \RR^{t \times t}$ is the identity matrix, and $\*y_{t} \coloneqq (y_{\*x_1}, \dots, y_{\*x_t})^\top \in \RR^{t}$.
%
Finally, for later use, let the posterior variance at $\*x^*$ when $\*x_t = \*x$ be $\sigma_{t}^2 (\*x^* \mid \*x) \coloneqq \sigma_{t}^2 (\*x^* \mid \*x_t = \*x)$.
%
Note that the posterior variance calculation does not require $\*y_t$.
%
Furthermore, it is known that $\mu_t(\*x)$ is equivalent to the kernel ridge regression estimator with regularization parameter $\lambda = \sigma^2$~\citep{kanagawa2018gaussian}.



\paragraph{Maximum Information Gain (MIG):}
%
Further, we define MIG \citep{Srinivas2010-Gaussian,vakili2021-information}:
\begin{definition}[Maximum information gain]
    Let $f \sim \cG \cP (0, k)$ over $\cX \subset [0, r]^d$.
    %
    Let $A = \{ \*a_i \}_{i=1}^T \subset \cX$.
    %
    Let $\*f_A = \bigl(f(\*a_i) \bigr)_{i=1}^T$, $\*\epsilon_A = \bigl(\epsilon_i \bigr)_{i=1}^T$, where $\forall i, \epsilon_i \sim \cN(0, \sigma^2)$, and $\*y_A = \*f_A + \*\epsilon_A \in \RR^T$.
    %
    Then, MIG $\gamma_T$ is defined as follows:
    \begin{align*}
        \gamma_T \coloneqq \max_{A \subset \cX; |A| = T} I(\*y_A ; \*f_A),
    \end{align*}
    where $I$ is the Shannon mutual information.
    \label{def:MIG}
\end{definition}
\noindent
%
It is known that MIG is sublinear for commonly used kernel functions, for example, $\gamma_T = \cO\bigl( d \log T \bigr)$ for linear kernels, $\gamma_T = \cO\bigl( (\log T)^{d+1} \bigr)$ for squared exponential (SE) kernels $k_{\rm SE} (\*x, \*x^\prime) = \exp\left( - \| \*x - \*x^\prime \|_2^2 / (2 \ell^2) \right)$, and $\gamma_T = \cO\bigl( T^{\frac{d}{2\nu + d}} (\log T)^{\frac{2\nu}{2\nu + d}} \bigr)$ for Mat\'{e}rn-$\nu$ kernels $k_{\rm Mat} = \frac{2^{1 - \nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu} \| \*x - \*x^\prime \|_2 }{\ell} \right)^{\nu} J_{\nu} \left( \frac{\sqrt{2\nu} \| \*x - \*x^\prime \|_2 }{\ell} \right) $, where $\ell, \nu > 0$ are the lengthscale and smoothness parameter, respectively, and $\Gamma(\cdot)$ and $J_{\nu}$ are Gamma and modified Bessel functions, respectively \citep{Srinivas2010-Gaussian,vakili2021-information}.



\paragraph{Lipschitz Consatant of $\sigma_t(\*x)$:}
We will use the following useful result from Theorem~E.4 in~\citet{Kusakawa2022-bayesian}:
\begin{lemma}[Lipschitz constant for posterior standard deviation]
    Let $k(\*x, \*x^\prime): \RR^d \times \RR^d \to \RR$ be linear, SE, or Mat\'{e}rn-$\nu$ kernel and $k(\*x, \*x) \leq 1$. 
    %
    Moreover, assume that a noise variance $\sigma^2$ is positive.
    %
    Then, for any $t \geq 1$ and $\cD_{t}$, the posterior standard deviation $\sigma_{t} (\*x )$ satisfies that 
    \begin{align*}
        \forall \*x,\*x^\prime \in \RR^d, \ | \sigma_{t} (\*x ) - \sigma_{t} (\*x^\prime ) | \leq L_{\sigma} \| \*x - \*x^\prime \| _1,
    % \label{eq:ineq_sigma_Lip}
    \end{align*}
    where $L_{\sigma}$ is a positive constant given by 
    \begin{align*}
        L_{\sigma} = \left\{
        \begin{array}{ll}
            1 & \text{if $k(\*x,\*x^\prime)$ is the linear kernel}, \\
            \frac{\sqrt{2}}{\ell} & \text{if $k(\*x,\*x^\prime)$ is the SE kernel}, \\
            \frac{\sqrt{2}}{\ell}
            \sqrt{\frac{\nu}{\nu-1} } & \text{if $k(\*x,\*x^\prime)$ is the Mat\'{e}rn kernel},
        \end{array}\right.
    \end{align*}
    where $\nu > 1$.
    % where $\ell$ is length scale parameters in Gaussian and Mat\'{e}rn kernels, and $\nu > 1$ is a degree of freedom with $\nu > 1$.
    \label{lem:Lipschitz_posterior_var}
\end{lemma}
% \begin{proof}
%     We can see that
%     \begin{align*}
%         \frac{\partial \sigma^2_t(\*u)}{\partial u_j} \bigg|_{\*u = \*x} =  2 \sigma_t(\*x) \frac{\partial \sigma_t(\*u)}{\partial u_j} \bigg|_{\*u = \*x} \leq 2 \frac{\partial \sigma_t(\*u)}{\partial u_j} \bigg|_{\*u = \*x}.
%     \end{align*}
%     %
%     From Theorem~E.4 in~\citet{Kusakawa2022-bayesian}, we see that $2\frac{\partial \sigma_t(\*u)}{\partial u_j} \bigg|_{\*u = \*x} \leq L_{\sigma_t^2}$ for all $\*x \in \cX$ and $j \in [d]$.
% \end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncerrainty Sampling and Random Sampling}
% The US and RS are common AL methods.
%
For the GPR model, the US selects the most uncertain input as $t$-th input:
\begin{align*}
    \*x_t = \argmax_{\*x \in \cX} \sigma^2_{t-1}(\*x).
\end{align*}
%
The RS randomly selects a $t$-th input by a fixed probability distribution $p(\*x)$ over $\cX$:
\begin{align*}
    \*x_t \sim p(\*x).
\end{align*}
%
For both algorithms, the upper bound of the maximum variance is known:
\begin{proposition}
    Assume $\cX$ is a compact subset of $\RR^d$.
    %
    If we run the US, the following inequality holds:
    \begin{align*}
        \max_{\*x \in \cX} \sigma^2_T (\*x) \leq \frac{C_1 \gamma_T}{T},
    \end{align*}
    where $C_1 = 1 / \log(1 + \sigma^{-2})$.
    %
    Furthermore, if we run the RS, the following inequality holds with probability at least $1 - \delta$, where $\delta \in (0, 1)$, under several conditions:
    \begin{align*}
        \max_{\*x \in \cX} \sigma^2_T (\*x) = \cO \left(\frac{\sigma^2\gamma_T}{T}\right).
    \end{align*}
    \label{prop:us_rs}
\end{proposition}
\begin{proof}
    For the US, see, e.g., Eq.~(16) in \citet{vakili2021-optimal} and Lemma~5.4 in \citet{Srinivas2010-Gaussian}.
    %
    For the RS, see Theorem~3.1 in \citet{salgia2024random}.
\end{proof}
When $\gamma_T$ is sublinear, the above upper bounds suggest that the maximum variance will be arbitrarily small within the finite time horizons.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regularity Assumptions and Predictive Guarantees}

% We consider two conditions called Bayesian and frequentist assumptions.
%
Here, we provide the details of Bayesian and frequentist assumptions and predictive guarantees for both assumptions.


% \paragraph{Bayesian Assumption and Credible Interval}
\subsubsection{Bayesian Assumption}

We consider the following assumption:
\begin{assumption}
    The function $f$ is a sample path $f \sim \cG \cP (0, k)$ and the $i$-th observation $y_{\*x_i}$ is contaminated by i.i.d. Gaussian noise $\epsilon_i \sim \cN(0, \sigma^2)$ as $y_{\*x_i} = f(\*x_i) + \epsilon_i$.
    %
    In addition, the kernel function is normalized as $k(\*x, \*x^\prime) \leq 1$ for all $\*x, \*x^\prime \in \cX$.
    \label{assump:Bayesian}
\end{assumption}
%
Furthermore, for continuous $\cX$, we assume the following smoothness condition:
\begin{assumption}
    Let $\cX \subset [0, r]^d$ be a compact set, where $r > 0$.
    %
    Assume that the kernel $k$ satisfies the following condition on the derivatives of a sample path $f$.
    %
    There exist the constants $a, b > 0$ such that,
    \begin{align*}
        \Pr \left( \sup_{\*x \in \cX} \left| \frac{\partial f(\*u)}{\partial u_j} \Big|_{\*u=\*x} \right| > L \right) \leq a \exp \left( - \frac{L^2}{b^2} \right),
    \end{align*}
    for all $j \in [d]$.
    \label{assump:Bayesian_continuous}
\end{assumption}
%
This assumption holds for stationary and four times differentiable kernels \citep[Theorem~5 of][]{Ghosal2006-posterior}, such as SE kernel and Mat\'{e}rn-$\nu$ kernels with $\nu > 2$~\citep[Section~4 of][]{Srinivas2010-Gaussian}.
%
These assumptions are commonly used \citep{Srinivas2010-Gaussian,Kandasamy2018-Parallelised,paria2020-flexible,Takeno2023-randomized,takeno2024-posterior}.



As with Lemma~5.1 in \citet{Srinivas2010-Gaussian}, the credible interval can be obtained as follows:
\begin{lemma}
    Suppose that $\cX$ is finite and Assumption~\ref{assump:Bayesian} holds.
    %
    Pick $\delta \in (0, 1)$ and $t \in \NN$.
    %
    Then, for any given $\cD_{t}$,
    \begin{align*}
        \Pr \left( |f(\*x) - \mu_{t}(\*x) | \leq \beta^{1/2}_{\delta} \sigma_{t}(\*x), \forall \*x \in \cX \mid \cD_{t} \right)
        \geq 1 - \delta,
    \end{align*}
    where $\beta_{\delta} = 2 \log (|\cX| / \delta)$.
    \label{lem:bound_srinivas}
\end{lemma}



% \paragraph{Frequentist Assumption and Confidence Interval}
\subsubsection{Frequentist Assumption}

We assume that $f$ is an element of the reproducing kernel Hilbert space (RKHS) specified by the kernel $k$ as with \citet{Srinivas2010-Gaussian,Chowdhury2017-on,vakili2021-optimal,vakili2022improved,li2022gaussian}:
\begin{assumption}
    Let $f$ be an element of RKHS $\cH_k$ specified by the kernel $k$ used in the GPR model.
    %
    Furthermore, the RKHS norm of $f$ is bounded as $\| f \|_{\cH_k} \leq B < \infty$ for some $B > 0$, where $\| \cdot \|_{\cH_k}$ denotes the RKHS norm of $\cH_k$.
    %
    In addition, the $i$-th observation $y_{\*x_i}$ is contaminated by independent sub-Gaussian noises $\{ \epsilon_i \}_{i \in \NN}$ as $y_{\*x_i} = f(\*x_i) + \epsilon_i$.
    %
    That is, for all $i \in \NN$, for all $\eta \in \RR$, and for some $R > 0$, the moment generating function of $\epsilon_i$ satisfies $\EE [\exp (\eta \epsilon_i)] \leq \exp\bigl( \frac{\eta^2 R^2}{2} \bigr) $.
    %
    Finally, the kernel function is normalized as $k(\*x, \*x^\prime) \leq 1$ for all $\*x, \*x^\prime \in \cX$.
    \label{assump:frequentist}
\end{assumption}


Furthermore, for continuous $\cX$, we assume the following smoothness condition as with \citet{Chowdhury2017-on,vakili2021-optimal,vakili2022improved}:
\begin{assumption}
    The kernel function $k$ satisfies the following condition on the derivatives.
    %
    There exists a constant $L_k$ such that,
    \begin{align*}
        \sup_{\*x \in \cX} \sup_{j \in [d]} \left| \frac{\partial^2 k(\*u, \*v)}{\partial u_j \partial v_j}\bigg|_{\*u=\*v=\*x} \right|^{1/2} \leq L_k.
    \end{align*}
    \label{assump:frequentist_continuous}
\end{assumption}
%
This assumption provides the Lipschitz constant of $f$:
\begin{lemma}[Lemma~5.1 in \citet{freitas2012exponential}]
    Suppose that Assumption~\ref{assump:frequentist_continuous} holds.
    %
    Then, any $g \in \cH_k$ is Lipschitz continuous with respect to $\|g\|_{\cH_k} L_k$.
    \label{lem:RKHS_lipschitz}
\end{lemma}


We rely on the confidence bounds for non-adaptive sampling methods, which is a direct consequence of Theorem~1 in \citet{vakili2021-optimal} and the union bound:
\begin{lemma}
    Suppose that $\cX$ is finite and Assumption~\ref{assump:frequentist} holds.
    %
    Pick $\delta \in (0, 1)$ and $t \in \NN$.
    %
    Assume that $(\*x_i)_{i \in [t]}$ is independent of $(\epsilon_i)_{i \in [t]}$.
    %
    Then, the following holds:
    \begin{align*}
        \Pr \left( |f(\*x) - \mu_{t}(\*x) | \leq \beta^{1/2}_{\delta} \sigma_{t}(\*x), \forall \*x \in \cX \right)
        \geq 1 - \delta,
    \end{align*}
    where $\beta_{\delta} = \left( B + \frac{R}{\sigma} \sqrt{2 \log ( \frac{2 |\cX|}{\delta})} \right)^2$.
    \label{lem:bound_vakili}
\end{lemma}


