\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/experiments/legend.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/RBF/results_0.1.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/Matern/results_0.1.pdf}
    \caption{
        Result of the error $E_T$ in the synthetic data experiments with $\eta=0, 0.001, 0.01, 0.1$.
        %
        The horizontal and vertical axes show the number of iterations and $E_T$, respectively.
        %
        The error bar shows mean and standard errors for 20 random trials regarding the random initial point (and the algorithm's randomness).
        %
        The top and bottom columns represent the results of the SE and Mat\'ern kernels, respectively.
        %
        % 最悪期待二乗誤差を用いて評価を行った場合の人工データ実験結果.
        % %
        % 横軸は反復数, 縦軸は最悪期待二乗誤差を示す.
        % %
        % ランダムに初期点を変更したことによる20回試行の実験結果の平均と標準誤差を示す.
        % %
        % 上段はRBFカーネル, 下段はMaternカーネルをGP回帰のカーネルとして用いた場合の結果を示す.
        % %
        % 参照分布は平均が${\* 0}$, 共分散行列が$0.2*{\*I}_3$の三次元正規分布とする. 
        % %
        % ここで, ${\*I}_3$は三次元の単位行列である.
        % %
        % また, 参照分布からの最大距離$\eta=0, 0.001, 0.01, 0.1$とする.
    }
    \label{fig:syn_results}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/experiments/legend.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/house/results_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/house/results_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/house/results_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/house/results_0.1.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/wine/results_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/wine/results_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/wine/results_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/wine/results_0.1.pdf}\\
    \includegraphics[width=0.24\linewidth]{fig/experiments/car/results_0.0.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/car/results_0.001.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/car/results_0.01.pdf}
    \includegraphics[width=0.24\linewidth]{fig/experiments/car/results_0.1.pdf}    
    \caption{
        Result of the error $E_T$ in the real-world data experiments with $\eta=0, 0.001, 0.01, 0.1$.
        %
        The horizontal and vertical axes show the number of iterations and $E_T$, respectively.
        %
        The error bar shows mean and standard errors for 20 random trials regarding the random initial point (and the algorithm's randomness).
        %
        The top, middle, and bottom columns represent the results of the King County house sales dataset, red wine quality dataset, and auto MPG dataset, respectively.
        %
        % 実データ実験結果.
        % %
        % 横軸は反復数, 縦軸は最悪期待二乗誤差を示す.
        % %
        % ランダムに初期点を変更したことによる20回試行の実験結果の平均と標準誤差を示す.
        % %
        % 各段はそれぞれKing County House Sales Dataset, Red Wine Quality Dataset, Auto MPG Datasetを用いた実験結果を示す.
        % %
        % 参照分布は平均が${\* 0}$, 共分散行列が$0.3*{\*I}_d$の$d$次元正規分布とする. 
        % %
        % ここで, $d$は各データセットの入力次元を表し, ${\*I}_d$は$d$次元の単位行列である.
        % %
        % また, 参照分布からの最大距離$\eta=0, 0.001, 0.01, 0.1$とする. 
    }
    \label{fig:real_results}
\end{figure*}


\section{Experiments}
\label{sec:experiment}

In this section, we demonstrate the effectiveness of the proposed methods via synthetic and real-world datasets.
%
We employ RS, US, variance reduction~\citep{yu2006active}, and expected predictive information gain (EPIG)~\citep{bickford2023-prediction} as the baseline.
%
We show the implementation details of EPIG in Appendix~\ref{sec:EPIG}.
%
Furthermore, as the ablation study, we performed the method, referred to as DR variance reduction, that greedily minimizes $E_{p_t(\*x^*)}\left[ \sigma^2_{t}(\*x^* \mid \*x) \right]$, that is, the unconstrained version of Eq.~\eqref{eq:greedy}.
%
We referred to the proposed methods as DR random and constrained DR variance reduction (CDR variance reduction).
%
We evaluate the performance by the error $E_T$ defined in Eq.~\eqref{eq:target_error}.
%
Furthermore, for the synthetic dataset, we show the result of $\max_{p \in \cP} \EE_{p(\*x)} \left[ \sigma_t^2 (\*x) \mid \cD_t \right]$ in Appendix~\ref{sec:exp_variance}.


In these experiments, by some $\eta \geq 0$, we define the ambiguity set as follows:
\begin{align*}
  \cP = \{p \in \cP_{\cX} \mid \|p_{\rm ref} - p \|_{\infty} \leq \eta \},
\end{align*}
where $\cP_{\cX}$ and $p_{\rm ref}$ are sets of all distributions over $\cX$ and some reference distribution, respectively, and $\|\cdot\|_{\infty}$ denotes $L_{\infty}$ norm.
%
Note that $\eta = 0$ matches the case that the unique target distribution $p_{\rm ref}$ is specified.
%
Since we consider the case of discrete $\cX$, maximization over $\cP$ can be written as linear programming for which we used CVXPY~\citep{diamond2016cvxpy,agrawal2018rewriting}.



% 本章では, 人工データと実データを用いた実験を通じて提案手法の有効性を示す.
% %
% どちらの実験においても比較手法としては, 既存の能動学習手法であるRS, US, variance reduction, EPIGと提案法における推薦候補に対する制限をなくした手法であるvariance reductionを用いる.
% %
% 評価指標として, 人工データ実験では最悪期待損失と最悪期待二乗誤差, 実データ実験では最悪期待二乗誤差のみを用いる.
% %
% ここで, 最悪期待損失は, 以下に定義される$t$反復目における最悪ケースの分布に対する予測分散の期待値とする.
% %
% \begin{align*}
%     {\max_{p^* \in \cP^*}} \EE_{p^*(\*x)} \left[ \sigma_t (\*x) \mid \cD_t \right]
% \end{align*}
% また，最悪期待二乗誤差は, 以下に定義される$t$反復目における最悪ケースの分布に対する二乗誤差の期待値とする．
% %
% \begin{align*}
%     {\max_{p^* \in \cP^*}} \EE_{p^*(\*x)} \left[ (\mu_t(\*x) - f(\*x))^2 \mid \cD_t \right]
% \end{align*}
% %
% このとき, 最悪ケースの分布が含まれる候補分布族$\cP^*$は任意の参照分布から一定の距離$\eta$以内に存在する分布とする. 
% %
% $\cP^*$の構成は次の式で表される. 
% \begin{align*}
%   \cP^* = \{p \in \cP_{\cX} \mid \|p_{\rm ref} - p \|_{\infty} \leq \eta \}
% \end{align*}
% ここで, $\cP_{\cX}$は$\cX$上の任意の分布, $p_{\rm ref}$は参照分布, $\|\cdot\|_{\infty}$は$L^{\infty}$距離を表す. 




\subsection{Synthetic Data Experiemnts}

We set $\cX=\{-1, -0.8, \ldots,  1\}^3$, where $|\cX| = 11^3 = 1331$.
%
The target function $f$ is the sample path from GPs, where we use SE and Mat\'ern-$\nu$ kernels with $\nu = 5/2$.
%
We use the fixed hyperparameters of the kernel function in the GPR model, which is used to generate $f$, and fix $\sigma^2 = 10^{-4}$.
%
The first input $\*x_1$ is selected by uniformly random, and $T$ is set as $400$.
%
Furthermore, we set $p_{\rm ref} = \cN(\*0, 0.2 \*I_3)$.

% 人工データ実験における候補集合は$\cX=\{-1, -0.8, \ldots,  1\}$とし, $|\cX| = 11^3 = 1331$である.
% %
% 出力にはGP回帰からのサンプルパスを用いる.
% %
% GP回帰のカーネルとして, RBF (Radial Basis Function) カーネルとMat\`ernカーネルを用いる.
% %
% RBFカーネルは$k({\*x}, {\* x}')=\exp(-r)$, Mat\`ernカーネルは$k({\*x}, {\* x}')=(1 + \sqrt{5}r + 5/3r^2)\exp(-\sqrt{5}r)$とし, $r=\| {\*x} - {\*x}' \|^2_2/(2 \times 0.5^2)$とする.
% %
% ノイズ分散は$\sigma^2_n = (10^{-4})^2$とし, カーネルのハイパーパラメータとノイズ分散は固定する.
% %
% 初期点はランダムに1点とり, 実験は400反復まで行う. 


Figures \ref{fig:syn_results} shows the result.
%
We can see that DR and CDR variance reductions show superior performance consistently for all the kernel functions and $\eta$, although the DR random is often inferior to those due to the randomness.
%
This result suggests that the DR and CDR variance reductions effectively incorporate the information of $\cP$.
%
Furthermore, although the constraint by $\cX_t$ is required for the theoretical analysis in CDR variance reduction, we can confirm that it does not sacrifice the practical effectiveness.
%
On the other hand, the usual AL methods, such as US and variance reduction, deteriorate when $\eta$ is small since they do not incorporate the information of $\cP$.
%
When $\eta$ is large since our problem approaches the worst-case error minimization, the US and variance reduction result in relatively good results.
%
On the other hand, the EPIG designed for the case $\eta = 0$ is inferior for all $\eta$ since the EPIG is based on the entropy $\cO\left(\log (\sigma_t(\*x))\right)$, not the squared error.
%
That is, since $\frac{\partial \log a}{\partial a} = \frac{1}{a}$ is large when $a \rightarrow 0$, the EPIG tends to make a small $\sigma_t(\*x)$ more small than making a large $\sigma_t(\*x)$ small.


% 人工データ実験の結果を図\ref{fig:syn_results}と図\ref{fig:syn_loss_results}に示す.
% %
% 図\ref{fig:syn_results}と図\ref{fig:syn_loss_results}では, 縦軸にそれぞれ最悪期待二乗誤差と最悪期待損失, 横軸に反復数を示す.
% %
% 実験結果はランダムに初期点を変更したことによる20回試行の平均と標準誤差を示す.
% %
% 各図の上段はRBFカーネル, 下段はMaternカーネルをGP回帰のカーネルとして用いた場合の結果を示す.
% %
% 参照分布は平均が${\* 0}$, 共分散行列が$0.2*{\*I}_3$の三次元正規分布とする.
% %
% ここで, ${\*I}$は三次元の単位行列である.
% %
% また, 参照分布からの最大距離$\eta=0, 0.001, 0.01, 0.1$とする.



% 図\ref{fig:syn_results}より, カーネルの種類や$\eta$の値に関わらず, 提案法とvariance reductionが最も少ない反復数で最悪期待二乗誤差を小さくしていることが確認できる.
% %
% 提案法とvariance reductionはほぼ同等の性能を示すが, 提案法には理論保証が示されている.
% %
% EPIGは$\eta$が小さい時は最悪期待二乗誤差を小さくすることができているが, $\eta$が大きくなると性能が悪化する.
% %
% これは, EPIGは一意なテスト分布をのみを考慮するためである.
% %
% また, USは$\eta$が小さい場合は提案法と比較して精度が悪いが, $\eta$が大きくなると提案法の性能に近づく.
% %
% これは, $\eta$が大きくなると, 最悪ケースの分布として不確実性の大きいデータに出現確率が偏った分布が計算されるため, 提案法はUSと同様に不確実性の大きなデータを観測しやすくなるからである.
% %


% 図\ref{fig:syn_loss_results}より, カーネルの種類や$\eta$の値に関わらず, 提案法とvariance reductionが最も少ない反復数で最悪期待損失を小さくしていることが確認できる.
% %
% 他の手法の性能に関しても最悪期待二乗誤差を用いて評価を行った場合と同様の傾向が確認できた.


\subsection{Real-World Dataset Experiments}
We use the King County house sales\footnote{\url{https://www.kaggle.com/datasets/harlfoxem/housesalesprediction}}, the red wine quality~\citep{wine_quality_186}, and the auto MPG datasets~\citep{auto_mpg_9} (See Appendix~\ref{sec:datasets_detail} for details).
%
For all experiments, we used SE kernels, where the hyperparameters $\ell$, and $\sigma^2$ are adaptively determined by the marginal likelihood maximization~\citep{Rasmussen2005-Gaussian} per 10 iterations.
%
The first input is selected uniformly random.
%
Furthermore, we normalize the inputs and outputs of all datasets before the experiments and set $p_{\rm ref} = \cN(\*0, 0.3 \*I_d)$.

% 実データ実験ではKing County House Sales Dataset, Red Wine Quality Dataset, Auto MPG Datasetの3つのデータセットを用いて実験を行った.
% %
% 全ての実験において, GP回帰のカーネルとしてRBFカーネル$k({\*x}, {\* x}')=\sigma^2\exp(-\| {\*x} - {\*x}' \|^2_2/(2 \times l^2)$を用いる.
% %
% カーネルのハイパーパラメータ$\sigma^2$, $l$とノイズ分散$\sigma^2_n$は周辺尤度最大化を用いたハイパーパラメータ最適化によって決定する.
% %
% このハイパーパラメータ最適化は10反復に1回行う.
% %
% 初期点はランダムに一点とり, King County House Sales DatasetとRed Wine Quality Datasetは700反復まで, Auto MPG Datasetでは399反復まで実験を行う.
% %
% また, 扱いやすさのためデータの入力と出力は事前に全データを用いて平均0, 分散1に標準化する. 


Figure~\ref{fig:real_results} shows the result.
%
We can confirm the same tendency that DR and CDR variance reductions show superior performance consistently as the synthetic data experiments shown in Figure~\ref{fig:syn_results}.
%
Note that the fluctuations come from the hyperparameter estimation.
%



% 実データ実験の結果を図\ref{fig:real_results}に示す.
% %
% 縦軸は最悪期待二乗誤差, 横軸は反復数を示す.
% %
% 実験結果はランダムに初期点を変更したことによる20回試行の平均と標準誤差を示す.
% %
% 図\ref{fig:real_results}の各段は上からKing County House Sales Dataset, Red Wine Quality Dataset, Auto MPG Datasetを用いて実験を行った場合の結果を示す.
% %
% 参照分布は平均が${\* 0}$, 共分散行列が$0.3*{\*I}_d$の$d$次元正規分布とする. 
% %
% ここで, $d$は各データセットの入力次元を表し, ${\*I}_d$は$d$次元の単位行列である.
% %
% また, 参照分布からの最大距離$\eta=0, 0.001, 0.01, 0.1$とする. 


% 図\ref{fig:real_results}より, データセットの種類や$\eta$の値に関わらず, 提案法とvariance reductionが最も少ない反復数で最悪期待二乗誤差を小さくしていることが確認できる.
% %
% 提案法とvariance reductionはほぼ同等の性能を示すが, 提案法には理論保証が示されている.
% %
% EPIGは$\eta$が小さい場合は提案法と同等の性能を示すが, $\eta$が大きくなると性能が悪化する.
% %
% また, 人工データと同様に, USは$\eta$が小さい場合は提案法と比較して精度が悪いが, $\eta$が大きくなると提案法の性能に近づく.




% \subsection{Discussion}

% \begin{itemize}
%     \item TODO: Discussion about unknown hyperparameter setting
%     \item DRAL is the middle framework between target distribution aware AL and worst case prediction $\max {\rm error}$.
%     \item TODO: EPIG aims to reduce $\log \sigma_t$. So, EPIG often concentrates on reducing the posterior variance of a certain point rather than reducing the posterior variance over the whole input space. This is just a difference in the goal of the algorithm. Ours is like V-optimal, but EPIG is like D-optimality.
% \end{itemize}