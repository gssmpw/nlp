\section{New Algorithm for solving COCO}
In this section, we present a simple algorithm (Algorithm \ref{coco_alg_1}) for solving COCO.
\begin{algorithm}[tb]
   \caption{Online Algorithm for COCO}
   \label{coco_alg_1}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Sequence of convex cost functions $\{f_t\}_{t=1}^T$ and constraint functions $\{g_t\}_{t=1}^T,$ $G=$ a common Lipschitz constant,  $d$ dimension  of the admissible set $\mathcal{X},$ step size $\eta_t = \frac{D}{G \sqrt{t}}$. 
   %an upper bound $G$ to the Euclidean norm of their (sub)gradients, 
    $D=$ Euclidean diameter of the admissible set $\mathcal{X},$ $\mathcal{P}_\mathcal{X}(\cdot)=$ Euclidean projection operator on the set $\mathcal{X}$,      \State {\bfseries Initialization:} Set $ x_1 \in \mathcal{X}$ arbitrarily, $\text{CCV}(0)=0$.
   \State {\bf For} \ {$t=1:T$}
   \State \quad Play $x_t,$ observe $f_t, g_t,$ incur a cost of $f_t(x_t)$ and constraint violation of $(g_t(x_t))^+$
   %\State Update constraint violation $\text{CCV}(t)=\text{CCV}(t-1)+\tilde{g}_t(x_t).$
   \State \quad Set $S_t$ as defined in \eqref{defn:S}
    \State \quad $y_{t} =  \mathcal{P}_{S_{t-1}}\left(x_t - \eta_t \nabla f_t(x_t)\right)$
   \State \quad $x_{t+1} =  \mathcal{P}_{S_t}\left(y_t\right)$
   \State {\bf EndFor}
\end{algorithmic}
\end{algorithm}
Algorithm \ref{coco_alg_1} is essentially an online projected gradient algorithm (OGD), 
which first takes an OGD step from the previous action $x_{t-1}$ with respect to the most recently revealed loss function $f_{t-1}$ with appropriate step-size which is then projected onto $S_{t-2}$ to reach $y_{t-1}$, and then projects $y_{t-1}$ onto  the most recently revealed set $S_{t-1}$ to get $x_t$,  the action to be played at time $t$.
\eqref{defn:S}. 

\begin{rem} Step 6 of Algorithm \ref{coco_alg_1} might appear unnecessary, however, its useful for proving Theorem \ref{thm:tvmonotone}.
\end{rem}

Since Algorithm \ref{coco_alg_1} is essentially an online projected gradient algorithm, similar to classical result on OGD, next, we show that the regret of Algorithm \ref{coco_alg_1} is $O(\sqrt{T})$.
\begin{lemma}\label{lem:regretbound}
The $\textrm{Regret}_{[1:T]}$ for Algorithm \ref{coco_alg_1} is $O(\sqrt{T})$.
\end{lemma}
Extension of Lemma \ref{lem:regretbound} when $f_t$'s are strongly convex which results in $\textrm{Regret}_{[1:T]}=O(\log{T})$ for Algorithm \ref{coco_alg_1} follows standard arguments \cite{Hazan} and is omitted.




The real challenge is to bound the total $\text{CCV}$ for Algorithm \ref{coco_alg_1}. 
Let $x_t$ be the action played by Algorithm \ref{coco_alg_1}. Then by definition, $x_t \in S_{t-1}$. Moreover, from \eqref{eq:distviolationrelation}, the constraint violation at time $t$, $\text{CCV}(t) \le G \text{dist}(x_{t}, S_t)$.
The next action $x_{t+1}$ chosen by Algorithm \ref{coco_alg_1} belongs to $S_t$, however, it is obtained by first taking an OGD step from $x_t$ to reach $y_t$ and then projects $y_t$ onto $S_t$. Since $f_t$'s are arbitrary, the OGD step could be towards any direction, and thus, there is no direct relationship between $x_{t+1}$ and $x_t$. Informally, $(x_1, x_2, \dots, x_T)$ is not a connected curve with any useful property. Thus, we take recourse in upper bounding the CCV via upper bounding the total movement cost $M$ (defined below) between nested convex sets using projections.

  The total constraint violation for Algorithm \ref{coco_alg_1} is
\begin{align}\nn
\text{CCV}_{[1:t]} & \le G\sum_{\tau=1}^t \text{dist}(x_{\tau}, S_{\tau}), \\ \label{defn:genconvxmovement}
&\stackrel{(a)} \le G  \sum_{\tau=1}^t ||x_{\tau}-  b_\tau||, \\
&\stackrel{(b)} = G M_t,
\end{align}
where in $(a)$ $b_t$ is the projection of $x_t$ onto $S_{t}$, i.e., $b_t=\cP_{S_{t}}(x_t)$ and in $(b)$
\begin{equation} \label{defn:totalmovementcost1}
M_t= \sum_{\tau=1}^t ||x_{\tau}-  b_\tau||
\end{equation} is defined to be the  total movement cost  on the instance $S_1, \dots, S_t$. 
%The upper bound in \eqref{defn:genconvxmovement} corresponds to the maximum length between any point on $S_{t-1}$ and its projection onto $S_t$. 
The object of interest is $M_T$.

%In the next section, we will upper bound $M_T$. Instead of bounding 
%Note that in \eqref{defn:genconvxmovement}, if we fix $a_t=x_t$ which in fact will give the correct CCV, then we will get upper bound on $M_T$ that will be algorithm dependent and not just instance dependent which can potentially be lower than that we are going to derive next that will be only instance dependent.
 %\begin{algorithm}[tb]
%   \caption{Policy $\mathrm{Switch}$ for COCO}
%   \label{coco_alg}
%\begin{algorithmic}[1]
%   \State {\bfseries Input:} Sequence of convex cost functions $\{f_t\}_{t=1}^T$ and constraint functions $\{g_t\}_{t=1}^T,$ $G=$ a common Lipschitz constant,  $d$ dimension  of the admissible set $\mathcal{X},$
%   %an upper bound $G$ to the Euclidean norm of their (sub)gradients, 
%    $D=$ Euclidean diameter of the admissible set $\mathcal{X},$ $\mathcal{P}_\mathcal{X}(\cdot)=$ Euclidean projection operator on the set $\mathcal{X}$, $z(d) = (D d \log d)$
%     %\State {\bfseries Parameter settings:} 
%     %\begin{enumerate}
%     	%\item \textbf{Convex cost functions:} $\beta = (2GD)^{-1}, V=1, \lambda = \frac{1}{2\sqrt{T}}, \Phi(x)= \exp(\lambda x)-1.$
%     
%    %\item \textbf{$\alpha$-strongly convex cost functions:} $\beta =1, V=\frac{8G^2 \ln(Te)}{\alpha}, \Phi(x)= x^2.$
%    %\end{enumerate}
%     %$ \alpha=\frac{1}{2GD}, n=\max(2, \lceil \ln T \rceil), V=(n-1)^{n-1}T^{\frac{n-1}{2}}, \Phi(x)=x^n.$ 
%%   \REPEAT
%  \State {\bfseries Initialization:} Set $ x_1 \in \mathcal{X}$ arbitrarily, $\text{CCV}(0)=0$.
%   \ForEach{$t=1:T$}
%   \State Play $x_t,$ observe $f_t, g_t,$ incur a cost of $f_t(x_t)$ and constraint violation of $\tilde{g}_t(x_t)=(g_t(x_t))^+$
%   \State Update constraint violation $\text{CCV}(t)=\text{CCV}(t-1)+\tilde{g}_t(x_t).$
%   \State Set $S_t$ as defined in \eqref{defn:S}
%   \If{$\text{CCV}(t) < z(d)$}
%   \State $\eta_t= \frac{1}{\sqrt{t}}$
%    \State $x_{t+1} =  \mathcal{P}_{\mathcal{X}}\left(x_t - \eta_t \frac{\nabla f_t(x_t)}{||\nabla f_t(x_t)||}\right)$
%    \Else
%    \If{$x_t\in S_t$}
%    \State $\eta_t= \frac{1}{\sqrt{t}}$
%    \State $x_{t+1} = \mathcal{P}_{S_t}\left(x_t - \eta_t \frac{\nabla f_t(x_t)}{||\nabla f_t(x_t)||}\right)$
%    \Else
%    \State  $x_{t+1} = \mathsf{Centroid}(S_t)$
%    \EndIf
%    \EndIf
%  
%   	
%%   \IF{$x_i > x_{i+1}$}
%%   \STATE Swap $x_i$ and $x_{i+1}$
%%   \STATE $noChange = false$
%%   \ENDIF
%   \EndForEach
%%   \UNTIL{$noChange$ is $true$}
%\end{algorithmic}
%\end{algorithm}


%using the $\mathsf{Centroid}$ algorithm described in Section \ref{sec:NCBC}. The pseudo code 
%of the algorithm is given in Algorithm \ref{coco_alg}, where the basic idea is as follows. Our target for $\text{CCV}_{[1:T]}$ is $z(d) = (Dd \log d)D$ that is independent of $T$. 
%Thus, Algorithm \ref{coco_alg} in phase 1 tries to optimize just the regret (with respect to $f_t$'s) by employing online gradient descent (OGD) algorithm while disregarding the constraint violation as long as the CCV is at most $z(d)$. If CCV never exceeds $z(d)$, we are done, since OGD achieves the optimal $O(\sqrt{T})$ regret following \cite{Hazan}. 
%
%Therefore, the real case of interest is that at some time $t< T$ (defined as $t_{\min}$), $\text{CCV}_{[1:t_{\min]}}$ is greater than $z(d)$. 
%From time $t_{\min}$ onwards, whenever, the action $x_t \notin S_t$,  the $\mathsf{Centroid}$ algorithm is used to select the next action. It is important to note that in the pseudo code of Algorithm \ref{coco_alg} $\mathsf{Centroid}(S_t)$ means the output of the $\mathsf{Centroid}$ algorithm which is not necessarily the centroid of the `full' set $S_t$.
%
%In the other case when $x_t \in S_t$, Algorithm \ref{coco_alg} tries to optimize just the regret (with respect to $f_t$'s) by employing OGD algorithm without considering constraint violation, similar to phase 1.
%
%
%As we show next, the regret of Algorithm \ref{coco_alg} is $O(\sqrt{T})$ while the CCV is $O(z(d))$.
%
%\begin{theorem}\label{thm:main}
%For algorithm \ref{coco_alg}, the regret \eqref{intro-regret-def} 
%	$$\textrm{Regret}_{[1:T]} = O(\sqrt{T}),$$
%	while the CCV \eqref{intro-gen-oco-goal}
% 	$$\textrm{CCV}_{[1:T]} = O(D d \log d).$$
%\end{theorem}
%To show this result, we essentially bound the regret \eqref{intro-regret-def}  by the sum of the total movement cost incurred by the $\mathsf{Centroid}$ algorithm and an $O(\sqrt{T})$ term, and then show that the total movement cost incurred by the $\mathsf{Centroid}$ algorithm is $O(D d \log d)$. Moreover, since the $\textrm{CCV}_{[1:T]}$ is also upper bounded by the total movement cost incurred by the $\mathsf{Centroid}$ algorithm, we get the result.
%%\begin{algorithm}[tb]
%   \caption{Online Policy for COCO}
%   \label{coco_alg}
%\begin{algorithmic}[1]
%   \State {\bfseries Input:} Sequence of convex cost functions $\{f_t\}_{t=1}^T$ and constraint functions $\{g_t\}_{t=1}^T,$ $G=$ a common Lipschitz constant,  $d$ dimension  of the admissible set $\mathcal{X},$
%   %an upper bound $G$ to the Euclidean norm of their (sub)gradients, 
%    $D=$ Euclidean diameter of the admissible set $\mathcal{X},$ $\mathcal{P}_\mathcal{X}(\cdot)=$ Euclidean projection operator on the set $\mathcal{X}$, $z(d) = (D d \log d)$
%     %\State {\bfseries Parameter settings:} 
%     %\begin{enumerate}
%     	%\item \textbf{Convex cost functions:} $\beta = (2GD)^{-1}, V=1, \lambda = \frac{1}{2\sqrt{T}}, \Phi(x)= \exp(\lambda x)-1.$
%     
%    %\item \textbf{$\alpha$-strongly convex cost functions:} $\beta =1, V=\frac{8G^2 \ln(Te)}{\alpha}, \Phi(x)= x^2.$
%    %\end{enumerate}
%     %$ \alpha=\frac{1}{2GD}, n=\max(2, \lceil \ln T \rceil), V=(n-1)^{n-1}T^{\frac{n-1}{2}}, \Phi(x)=x^n.$ 
%%   \REPEAT
%  \State {\bfseries Initialization:} Set $ x_1 \in \mathcal{X}$ arbitrarily, $\text{CCV}(0)=0$.
%   \ForEach{$t=1:T$}
%   \State Play $x_t,$ observe $f_t, g_t,$ incur a cost of $f_t(x_t)$ and constraint violation of $\tilde{g}_t(x_t)=(g_t(x_t))^+$
%   \State Update constraint violation $\text{CCV}(t)=\text{CCV}(t-1)+\tilde{g}_t(x_t).$
%   \State Set $S_t$ as defined in \eqref{defn:S}
%   \If{$\text{CCV}(t) < z(d)$}
%   \State $\eta_t= \frac{1}{\sqrt{t}}$
%    \State $x_{t+1} =  \mathcal{P}_{\mathcal{X}}\left(x_t - \eta_t \frac{\nabla f_t(x_t)}{||\nabla f_t(x_t)||}\right)$
%    \Else
%    \If{$x_t\in S_t$}
%    \State $\eta_t= \frac{1}{\sqrt{t}}$
%    \State $x_{t+1} = \mathcal{P}_{S_t}\left(x_t - \eta_t \frac{\nabla f_t(x_t)}{||\nabla f_t(x_t)||}\right)$
%    \Else
%    \State  $x_{t+1} = \mathsf{Centroid}(S_t)$
%    \EndIf
%    \EndIf
%  
%   	
%%   \IF{$x_i > x_{i+1}$}
%%   \STATE Swap $x_i$ and $x_{i+1}$
%%   \STATE $noChange = false$
%%   \ENDIF
%   \EndForEach
%%   \UNTIL{$noChange$ is $true$}
%\end{algorithmic}
%\end{algorithm}
