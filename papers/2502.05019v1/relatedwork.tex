\section{Prior Work}
{\bf Constrained OCO (COCO): (A) Time-invariant constraints:} COCO with time-invariant constraints, \emph{i.e.,} $g_{t} = g, \forall \ t$ \citep{yuan2018online, jenatton2016adaptive, mahdavi2012trading, yi2021regret} has been considered extensively, where functions $g$ are assumed to be known to the algorithm \emph{a priori}. The algorithm is allowed to take actions that are infeasible at any time to avoid the costly projection step of the vanilla projected OGD algorithm and the main objective was to design an \emph{efficient} algorithm  with a small regret and CCV while avoiding  the explicit projection step. 

{\bf (B) Time-varying constraints:} The more difficult question is solving COCO problem when the constraint functions, \emph{i.e.}, $g_{t}$'s, change arbitrarily with time $t$.  
In this setting, all prior work on COCO made the feasibility assumption.
One popular algorithm for solving COCO considered a Lagrangian function optimization that is updated using the primal and dual variables \citep{yu2017online, pmlr-v70-sun17a, yi2023distributed}. Alternatively, \citet{neely2017online} and \cite{georgios-cautious} used the drift-plus-penalty (DPP) framework  \citet{neely2010stochastic} to solve the COCO, but which needed additional assumption, e.g. the Slater's condition in \citet{neely2017online} and with weaker form of the feasibility assumption \cite{neely2017online}'s. 
%constrained problem under various assumptions. In particular,
%  \citet{neely2017online} proposed a DPP-based policy for COCO  upon assuming the Slater's condition, \emph{i.e.,} $g_{t,i}(x^\star) < -\eta$, for some $\eta>0$ $\forall i,t$. Clearly, this condition precludes the important case of non-negative constraint functions (\emph{e.g.,} constraint functions of the form $\max(0, g_t(x))$). Furthermore, the bounds obtained upon assuming Slater's condition depend inversely with the Slater's constant $\eta$ (usually hidden under the big-Oh notation). Since $\eta$ could be arbitrarily small, these bounds could be arbitrarily loose. 
%  %Furthermore, a sublinear violation bound obtained upon assuming Slater's condition is loose by a quantity that increases \emph{linearly} with the horizon-length $T$ compared to a sublinear violation bound obtained without this assumption.    
%(which arises, e.g., upon a $\max(0,\cdot)$ operation with a given constraint). 
%Moreover, the regret bound presented in \cite{neely2017online} diverges to infinity as $\eta \searrow 0.$ 
% \cite{georgios-cautious} extended \cite{neely2017online}'s result by considering a weaker form of the feasibility assumption without assuming Slater's condition. 
% they show that a DPP-based policy achieves a regret $\mathcal{R}_T = O(ST/V + \sqrt{T})$ and CCV ${\mathbb V}(T) = O(\sqrt{VT})$. Here, $V$ is an adjustable parameter that can take any value in $[S, T).$ Hence, \emph{a priori}, their algorithm needs to know the value of the parameter $S,$ which, unfortunately, depends on the online constraints.
%It can be seen that the violation penalty bound achieved by their policy is at least $O(T^{3/4}),$ which is suboptimal. 
%Furthermore, although these DPP-based results are interesting, they have not been able to provide improved regret or CVV bounds 
%when the cost functions $f_t$'s are strongly convex because of the linearization step inherent in this approach. 


More recently paper, \citet{guo2022online} obtained the bounds similar to \citet{neely2017online} but without assuming Slater's condition. However, the algorithm \citet{guo2022online} was quite computationally intensive since it requires solving a convex optimization problem on each round. 
Finally, very recently, the state of the art guarantees on simultaneous bounds on regret $O (\sqrt{T})$ and CCV $O (\sqrt{T}\log T)$ for COCO were derived in \cite{Sinha2024} with a very simple algorithm that combines the loss function at time $t$ and the CCV accrued till time $t$ in a single loss function, and then executes the online gradient descent (OGD) algorithm on the single loss function with an adaptive step-size.
 %Compared to this, all policies proposed in this paper take only a single gradient-descent step and perform only one Euclidean projection on each round. 
%Moreover, it is unclear how to extend \citet{guo2022online}'s policy to the more general $S$-feasible benchmark, where it is necessary to compensate for constraint violations at some rounds with strictly satisfying constraints on some other rounds. 
Please refer to Table \ref{gen-oco-review-table} for a brief summary of the prior results.


%inefficient as, instead of performing a single gradient-descent step per round (as in our and most of the previous algorithms), their algorithm needs to solve a general convex optimization problem at every round. Moreover, their algorithm needs access to the full description of the constraint function $g_t(\cdot)$ for the optimization step, whereas ours and most of the previous algorithms need to know only the gradient and the value of the constraint function for the current action $x_t$. 
%In a recent paper, \cite{yi2023distributed} consider the same problem in a distributed setup and derive tighter bounds upon assuming Slater's condition.

The COCO problem has been considered in the {\it dynamic} setting as well  \citep{chen2018bandit, cao2018online, vazecocowiopt2022, liu2022simultaneously} where the benchmark $x^\star$ in \eqref{intro-regret-def} is replaced by $x_t^\star$ ($x_t^\star = \arg \min_x f_t(x)$) that is also allowed to change its actions over time. However, in this paper, we focus our entire attention on the static version.
%\paragraph{The Online Constraint Satisfaction (\textsc{OCS}) Problem:} %In addition to studying the above problem, we also introduce a new but related problem, 
A special case of COCO is the 
online constraint satisfaction (OCS) problem that does not involve any cost function, \emph{i.e.,} $f_t=0, \ \forall t,$ and the only object of interest is minimizing the CCV. The algorithm with state of the art guarantee for COCO \cite{Sinha2024} was shown to have a CCV of $O(\sqrt{T}\log T)$ for the OCS.