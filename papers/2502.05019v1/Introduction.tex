\section{Introduction}
In this paper, we consider the constrained version of the standard online convex optimization (OCO) framework, called constrained OCO or COCO. In COCO, on every round $t,$ the online algorithm first chooses an admissible action $x_t \in \mathcal{X} \subset \bbR^d,$ 
 and then the adversary chooses a convex loss/cost function $f_t: \mathcal{X} \to \mathbb{R}$ and a constraint function of the form $g_{t}(x) \leq 0,$ where $g_{t}: \mathcal{X} \to \mathbb{R}$ is a convex function. Since $g_{t}$'s are revealed after the action $x_t$ is chosen, an online algorithm need not necessarily take feasible actions on each round, and in addition to the static regret 
 \begin{eqnarray} \label{intro-regret-def}
	\textrm{Regret}_{[1:T]} \equiv \sup_{\{f_t\}_{t=1}^T} \sup_{x^\star \in \mathcal{X}} \textrm{Regret}_T(x^\star), ~\textrm{where~}\textrm{Regret}_T(x^\star) \equiv \sum_{t=1}^T f_t(x_t) - \sum_{t=1}^T f_t(x^\star),
\end{eqnarray}
an additional metric of interest  is the total cumulative constraint violation (CCV)  defined as 
 %\vspace{-0.1in}
  \begin{eqnarray} \label{intro-gen-oco-goal}
 	\textrm{CCV}_{[1:T]}  \equiv \sum_{t=1}^T \max(g_{t}(x_t),0). 
	\end{eqnarray}
Let $\mathcal{X}^\star$ be the feasible set consisting of all admissible actions that satisfy all constraints $g_{t}(x) \leq 0, t\in [T]$. Under the standard assumption that $\mathcal{X}^\star$ is not empty (called the {\it feasibility assumption}), the goal is to design an online algorithm to simultaneously achieve a small regret \eqref{intro-regret-def} with respect to any admissible benchmark $x^\star \in \mathcal{X}^\star$ and a small CCV \eqref{intro-gen-oco-goal}. 
%We refer to this problem as the constrained OCO (COCO). The assumption $\mathcal{X}^\star \neq \emptyset $ will be relaxed in Section \ref{simul_constr} for the  Online Constraint Satisfaction (OCS) problem where the cost functions are set to zero, and the objective is to minimize just the CCV.
%

With constraint sets 
${\cal G}_t = \{x\in \cX : g_t(x)\le 0\}$ being convex for all $t$, and the assumption $\mathcal{X}^\star = \cap_t G_t  \neq \varnothing $  implies that sets
$S_t = \cap_{\tau=1}^t {\cal G}_\tau$ are convex and are nested, i.e. $S_t\subseteq S_{t-1}$ and $\mathcal{X}^\star \in S_t$ for all $t$. Essentially, set $S_t$'s are sufficient to quantify the CCV.

\subsection{Prior Work}
 {\bf Constrained OCO (COCO): (A) Time-invariant constraints:} COCO with time-invariant constraints, \emph{i.e.,} $g_{t} = g, \forall \ t$ \citep{yuan2018online, jenatton2016adaptive, mahdavi2012trading, yi2021regret} has been considered extensively, where functions $g$ are assumed to be known to the algorithm \emph{a priori}. The algorithm is allowed to take actions that are infeasible at any time to avoid the costly projection step of the vanilla projected OGD algorithm and the main objective was to design an \emph{efficient} algorithm  with a small regret and CCV while avoiding  the explicit projection step. 

{\bf (B) Time-varying constraints:} The more difficult question is solving COCO problem when the constraint functions, \emph{i.e.}, $g_{t}$'s, change arbitrarily with time $t$.  
In this setting, all prior work on COCO made the feasibility assumption.
One popular algorithm for solving COCO considered a Lagrangian function optimization that is updated using the primal and dual variables \citep{yu2017online, pmlr-v70-sun17a, yi2023distributed}. Alternatively, \citet{neely2017online} and \cite{georgios-cautious} used the drift-plus-penalty (DPP) framework  \citet{neely2010stochastic} to solve the COCO, but which needed additional assumption, e.g. the Slater's condition in \citet{neely2017online} and with weaker form of the feasibility assumption \cite{neely2017online}'s. 
%constrained problem under various assumptions. In particular,
%  \citet{neely2017online} proposed a DPP-based policy for COCO  upon assuming the Slater's condition, \emph{i.e.,} $g_{t,i}(x^\star) < -\eta$, for some $\eta>0$ $\forall i,t$. Clearly, this condition precludes the important case of non-negative constraint functions (\emph{e.g.,} constraint functions of the form $\max(0, g_t(x))$). Furthermore, the bounds obtained upon assuming Slater's condition depend inversely with the Slater's constant $\eta$ (usually hidden under the big-Oh notation). Since $\eta$ could be arbitrarily small, these bounds could be arbitrarily loose. 
%  %Furthermore, a sublinear violation bound obtained upon assuming Slater's condition is loose by a quantity that increases \emph{linearly} with the horizon-length $T$ compared to a sublinear violation bound obtained without this assumption.    
%(which arises, e.g., upon a $\max(0,\cdot)$ operation with a given constraint). 
%Moreover, the regret bound presented in \cite{neely2017online} diverges to infinity as $\eta \searrow 0.$ 
% \cite{georgios-cautious} extended \cite{neely2017online}'s result by considering a weaker form of the feasibility assumption without assuming Slater's condition. 
% they show that a DPP-based policy achieves a regret $\mathcal{R}_T = O(ST/V + \sqrt{T})$ and CCV ${\mathbb V}(T) = O(\sqrt{VT})$. Here, $V$ is an adjustable parameter that can take any value in $[S, T).$ Hence, \emph{a priori}, their algorithm needs to know the value of the parameter $S,$ which, unfortunately, depends on the online constraints.
%It can be seen that the violation penalty bound achieved by their policy is at least $O(T^{3/4}),$ which is suboptimal. 
%Furthermore, although these DPP-based results are interesting, they have not been able to provide improved regret or CVV bounds 
%when the cost functions $f_t$'s are strongly convex because of the linearization step inherent in this approach. 


More recently paper, \citet{guo2022online} obtained the bounds similar to \citet{neely2017online} but without assuming Slater's condition. However, the algorithm \citet{guo2022online} was quite computationally intensive since it requires solving a convex optimization problem on each round. 
Finally, very recently, the state of the art guarantees on simultaneous bounds on regret $O (\sqrt{T})$ and CCV $O (\sqrt{T}\log T)$ for COCO were derived in \cite{Sinha2024} with a very simple algorithm that combines the loss function at time $t$ and the CCV accrued till time $t$ in a single loss function, and then executes the online gradient descent (OGD) algorithm on the single loss function with an adaptive step-size.
 %Compared to this, all policies proposed in this paper take only a single gradient-descent step and perform only one Euclidean projection on each round. 
%Moreover, it is unclear how to extend \citet{guo2022online}'s policy to the more general $S$-feasible benchmark, where it is necessary to compensate for constraint violations at some rounds with strictly satisfying constraints on some other rounds. 
Please refer to Table \ref{gen-oco-review-table} for a brief summary of the prior results.


%inefficient as, instead of performing a single gradient-descent step per round (as in our and most of the previous algorithms), their algorithm needs to solve a general convex optimization problem at every round. Moreover, their algorithm needs access to the full description of the constraint function $g_t(\cdot)$ for the optimization step, whereas ours and most of the previous algorithms need to know only the gradient and the value of the constraint function for the current action $x_t$. 
%In a recent paper, \cite{yi2023distributed} consider the same problem in a distributed setup and derive tighter bounds upon assuming Slater's condition.

The COCO problem has been considered in the {\it dynamic} setting as well  \citep{chen2018bandit, cao2018online, vazecocowiopt2022, liu2022simultaneously} where the benchmark $x^\star$ in \eqref{intro-regret-def} is replaced by $x_t^\star$ ($x_t^\star = \arg \min_x f_t(x)$) that is also allowed to change its actions over time. However, in this paper, we focus our entire attention on the static version.
%\paragraph{The Online Constraint Satisfaction (\textsc{OCS}) Problem:} %In addition to studying the above problem, we also introduce a new but related problem, 
A special case of COCO is the 
online constraint satisfaction (OCS) problem that does not involve any cost function, \emph{i.e.,} $f_t=0, \ \forall t,$ and the only object of interest is minimizing the CCV. The algorithm with state of the art guarantee for COCO \cite{Sinha2024} was shown to have a CCV of $O(\sqrt{T}\log T)$ for the OCS. 

\subsection{Convex Body Chasing Problem}  \label{cbc}
A well-studied problem related to the COCO is the
{\it nested convex body chasing (NCBC)} problem \citep{bansa2018nested,argue2019nearly,bubeck2020chasing}, 
where at each round $t$, a convex set $\chi_t \subseteq \chi$ is revealed such that 
$\chi_t\subseteq \chi_{t-1}$, and  $\chi_0=\chi \subseteq {\mathbb R}^d$ is a convex, compact, and bounded set. 
The objective is to choose action  $x_t \in \chi_t$ so as to minimize the total movement cost 
$C =   \sum_{t=1}^T  ||x_t - x_{t-1}||,$
where $x_0 \in \chi$ is some fixed action. Best known-algorithms for NCBC \citep{bansa2018nested,argue2019nearly,bubeck2020chasing} choose $x_t$ to be the 
centroid or Stiener point of $\chi_t$, essentially well inside the newly revealed convex set in order to reduce the future movement cost. 
With COCO, such an approach does not provide any useful bounds because of the presence of cost functions $f_t$'s whose minima could be towards the boundary of relevant convex sets $S_t$'s.




%In NCBC, action $x_t$ is chosen \emph{after} the set $\chi_t$ is revealed. This is in contrast to the \textsc{OCS} problem, where $x_t$ must be chosen \emph{before} the constraints $g_{t,i}$'s are revealed at round $t$. Moreover, note that the nested condition $\chi_t \subseteq \chi_{t-1}$ is stricter than Assumption \ref{feas-constr}, which is applicable to the \textsc{OCS} problem.
%However, as we show next, a feasible algorithm for NCBC also provides an upper bound on the CCV of the \textsc{OCS} problem under Assumption \ref{feas-constr}.
%
%In this reduction, we define $\chi_t $ as the intersection of the first $kt$ convex constraints $g_{\tau,i} \leq 0, 1\leq \tau \leq t, i\in [k],$ revealed up to round $t$ for the \textsc{OCS} problem. It is easy to see that $\chi_t$ is convex and $\chi_t \subseteq \chi_{t-1}, \forall t.$
%Let $x_t$ be the action chosen by an algorithm $\cal A$ for the NCBC problem after the set $\chi_t$ is revealed. Note that $\chi_t \neq \emptyset,$ thanks to Assumption \ref{feas-constr}. We now choose $y_{t} := x_{t-1}$ as the action for the \textsc{OCS} problem on round $t$, ensuring that action $y_t$ is chosen before the set $ \chi_t$ is revealed.
%The resulting $i^{th}$ constraint violation for the \textsc{OCS} problem at round $t$ is given by 
%\[
%	g_{t,i}(y_{t}) \stackrel{(a)}\le g_{t,i}(y_{t}) - g_{t,i}(y_{t+1}) \le G ||y_{t} - y_{t+1}||,
%\]
%where $(a)$ follows from the feasibility of $\cal A$ for NCBC, $y_{t+1}= x_{t} \in \chi_{t}$ and hence $g_{t,i}(y_{t+1}) \leq 0$. Summing across rounds $t=1, \dots, T$, and taking the $\max$ over  all the $k$ constraints, we get that the CCV using $\cal A$ for the \textsc{OCS} is upper bounded by $ \sum_{t=2}^T G ||y_{t} - y_{t+1}|| \le \sum_{t=2}^T G ||x_{t-1} - x_{t}|| \le G \cdot C_{\cal A},$
%where $C_{\cal A}$ is the movement cost of $\cal A$ for the NCBC problem.

%From prior work \cite{bansa2018nested,argue2019nearly,bubeck2020chasing}, it is known that for NCBC, a Steiner point-based algorithm that chooses $x_t$ as the Steiner point of $\chi_t$ can achieve
%$C_{\cal A} = O(\sqrt{d \log d})$, where $\chi \subset {\mathbb R}^d$. Thus, the Steiner point-based algorithm (even though computationally intensive) provides an $O(\sqrt{d \log d})$ constraint violation for the 
%\textsc{OCS} as well. However, this result is effective for problems where  $\sqrt{d \log d} = o(T).$ Our result efficiently overcomes this hurdle and provides a bound under weaker feasibility assumptions even beyond $\sqrt{d \log d} = o(T)$ -- a setting that is better motivated in practice for modern deep learning applications which are characteristically high-dimensional.

\subsection{Limitations of Prior Work}
We explicitly show in Lemma \ref{lem:algwc} that the best known algorithm \cite{Sinha2024} for solving COCO suffers a CCV of $\Omega(\sqrt{T}\log T)$ for `simple' problem instances where $f_t=f$ and $g_t=g$ for all $t$ and $d=1$ dimension, for which ideally the CCV should be $O(1)$. The same is true for most other prior algorithms, where the main reason for their large CCV for simple instances is that all these algorithms treat minimizing the CCV as a regret minimization problem for functions $g_t$. What they fail to exploit is the geometry of the 
underlying nested convex sets $S_t$'s  that control the CCV.
\subsection{Main open question}

In comparison to the above discussed upper bounds, the best known simultaneous lower bound \cite{Sinha2024}  for COCO is $\cR_{[1:T]} = \Omega(\sqrt{d})$ and $\text{CCV}_{[1:T]} = \Omega(\sqrt{d})$, where $d$ is the dimension of the action space $\cX$. Without constraints, i.e., $g_t\equiv0$ for all $t$, the lower bound on $\cR_{[1:T]} = \Omega(\sqrt{T})$ \cite{Hazan}.
Thus, there is a fundamental gap between the lower and upper bound for the CCV, and the main open question for COCO is : 

{\it  Is it possible to simultaneously achieve $\cR_{[1:T]} =O(\sqrt{T})$ and $\text{CCV}_{[1:T]} = o(\sqrt{T})$ or $\text{CCV}_{[1:T]} = O(1)$} for COCO?

Even though we do not fully resolve this question, in this paper, we make some meaningful progress by proposing an algorithm that exploits the geometry of the nested sets $S_t$'s and show that it is possible to simultaneously achieve  $\cR_{[1:T]} =O(\sqrt{T})$ and $\text{CCV}_{[1:T]} = O(1)$ in certain cases, and for general case, give a bound on the CCV that depends on the shape of the convex sets $S_t$'s while achieving  $\cR_{[1:T]} =O(\sqrt{T})$. In particular, the contributions of this paper are as follows.

\subsection{Our Contributions}
In this paper, we propose an algorithm (Algorithm \ref{coco_alg_1}) that tries to exploit the geometry of the nested convex sets $S_t$'s. In particular, Algorithm \ref{coco_alg_1} at time $t$, first takes an OGD step from the previous action $x_{t-1}$ with respect to the most recently revealed loss function $f_{t-1}$ with appropriate step-size to reach $y_{t-1}$, and then projects $y_{t-1}$ onto  the most recently revealed set $S_{t-1}$ to get $x_t$,  the action to be played at time $t$. 
Let $F_t$ be the ``projection" hyperplane passing through $x_t$ that is perpendicular to $x_t-y_{t-1}$. For Algorithm \ref{coco_alg_1}, we derive the following guarantees.
\begin{itemize}
\item The regret of the Algorithm \ref{coco_alg_1} is $O(\sqrt{T})$.
\item The CCV for the Algorithm \ref{coco_alg_1} takes the following form 
\begin{itemize}
\item When sets $S_t$'s are `nice', e.g. are spheres, or axis parallel cuboids, CCV is $O(1)$.
\item For general $S_t$'s, the CCV is upper bounded by a quantity $\cV$ that is a function of the distance between the consecutive sets $S_t$ and $S_{t+1}$ for all $t$, the shape of $S_t$'s, dimension $d$ and the diameter $D$.  Since $\cV$ depends on the shape of $S_t$'s, there is no universal bound on $\cV$, and the derived bound is instance dependent.
\item 
For the special case of $d=2$, when projection hyperplanes $F_t$'s progressively make increasing angles with respect to the first projection hyperplane $F_1$, the CCV is $O(1)$.
\end{itemize}
\item As pointed out above, for general $S_t$'s, there is no universal bound on the CCV of Algorithm \ref{coco_alg_1}. 
Thus, we propose an algorithm $\mathrm{Switch}$  that combines Algorithm \ref{coco_alg_1} and the algorithm from \cite{Sinha2024} to provide a regret bound of $O(\sqrt{T})$ and a CCV that is minimum of $\cV$ and $O(\sqrt{T} \log T)$. Thus, $\mathrm{Switch}$ provides a best of two worlds  CCV guarantee, which is small if the sets $S_t$'s are nice, while in the worst case it is at most $O(\sqrt{T} \log T)$.
\item For the OCS problem, we show that the CCV of Algorithm \ref{coco_alg_1} is $O(1)$ compared to the CCV of $O(\sqrt{T} \log T)$ \cite{Sinha2024}.
\end{itemize}

 \begin{table*}[t]
%\hspace{-30pt}
 % \title{Summary of the results for the constrained OCO problem}
  %\centering
  %\hspace{-40pt}
  \begin{tabular}{llllll}
    \toprule
    %\multicolumn{2}{c}{Part}                   \\
   % \cmidrule(r){1-2}
   \small { Reference}  & \small {Regret} & \small {CCV} & \small {Complexity per round}\\
    \midrule
  %  a & b& c& d & e  \\
%      \small {\citet{mahdavi2012trading}}  & \small {$O(\sqrt{T})$} & \small {$O(T^{\nicefrac{3}{4}})$} & \small {Projection} & \small{Time-invariant constraints} \\
   % \small {\citet{jenatton2016adaptive}}  & \small {$O(T^{\max(\beta, 1-\beta)})$} & \small {$O(T^{1-\beta/2})$} & \small {Projection} & \small{Time-invariant constraints} \\
  %  \small {\citet{pmlr-v70-sun17a}}  & \small {$O(\sqrt{T})$} & \small {$O(T^{\nicefrac{3}{4}})$}& \small {Bregman Projection} & \small -  \\
    \small {\citet{neely2017online}}  & \small {$O(\sqrt{T})$} & \small {$O(\sqrt{T})$} & \small {Conv-OPT, \small {Slater's condition}} \\
    %    \small {\citet{yu2017online}}  & \small {$O(\sqrt{T})$} & \small {$O(\sqrt{T})$} & \small {Conv-OPT} & \small {Slater condition} \\
 % \small {\citet{yuan2018online}} & \small {$O(T^{\max(\beta, 1-\beta)})$} & \small {$O(T^{1-\beta/2})$ }  & \small {Projection} & \small{Time-invariant constraints} \\
%      \small {\citet{yu2020low}}  & \small {$O(\sqrt{T})$} & \small {$O(1)$} & \small {Conv-OPT} & \small {Slater \& Time-invariant constraints} \\
 % \citet{yu2017online} & Stochastic & $O(\sqrt{T})$ & $O(\sqrt{T})$& OGD+drift+penalty & Slater condition \\
 % \small {\citet{yi2021regret}} & \small {$O(T^{\max(\beta, 1-\beta)})$} & \small {$O(T^{(1-\beta)/2})$} & \small {Conv-OPT} & \small{Time-invariant constraints} \\ 
 % \small {\citet{yi2022regret}}  & \small {$O(T^{\beta})$} & \small {$O(T^{1-\beta/2})$} & \small {Projection} & \small {Strongly convex cost} \\
    \small {\citet{guo2022online}}  & \small {$O(\sqrt{T})$} & \small {$O(T^{\frac{3}{4}})$} & \small {Conv-OPT} \\
       % \small {\citet{guo2022online}}  & \small {$O(\log T)$} & \small {$O(\sqrt{T \log T})$} & \small {Conv-OPT} & \small {Strongly convex cost} \\ 
  \small {\citet{yi2023distributed}}  & \small {$O(T^{\max(\beta, 1-\beta)})$} & \small {$O(T^{1-\beta/2})$} & \small {Conv-OPT}  \\
   % \small {\citet{yi2023distributed}} & \small {$O(\log(T))$} & \small {$O(\sqrt{T \log T})$} & \small {Conv-OPT} & \small {Strongly convex cost} \\
    %\citet{georgios-cautious} & Adversarial, convex & $S$ & $O(\sqrt{ST})$ & OGD & Known $S$ \\
               %\citet{guo2022online} & Adversarial, strongly-convex & $1$ & $O(\sqrt{T \log T})$ & Convex opt. each round & -do-, Known $\alpha$ \\
   %      \small {\citet{georgios-cautious}}  & &\ \textcolor{red}{($S$,$O(\sqrt{ST})$)}& \small {OGD} & \small {Known $S$} \\
  %\small {\textbf{This paper}}  & \small {$O(\sqrt{T})$} & \small {$O(T^{3/4})$} & \small {Ad-OCO} & - \\
     \small {\citet{Sinha2024}} & \small {$O(\sqrt{T})$} & \small {$O(\sqrt{T}\log T)$} & \small {Projection} \\
 \small {\textbf{This paper}} &   \small {$O(\sqrt{T})$} & \small {$O(\min\{\cV,\sqrt{T}\log T\})$} &\small {Projection}  \\
 %\small {\textbf{This paper}} & \small {$O(\log T)$} & \small {$O(\frac{\log T}{\alpha})$} &\small {Projection} & \small {Strongly convex cost, $\textrm{Regret}_T \geq 0,$} \\
       \bottomrule
  \end{tabular}
  \vspace{5pt}
  \caption{\small{Summary of the results on COCO for arbitrary time-varying convex constraints and convex cost functions. In the above table, $0\leq \beta \leq 1$ is an adjustable parameter. Conv-OPT refers to solving a constrained convex optimization problem on each round. Projection refers to the Euclidean projection operation on the convex set $\mathcal{X}$. The CCV bound for this paper is stated in terms of $\cV$ which can be $O(1)$ or depends on the shape of convex sets $S_t$'s. }}
    \label{gen-oco-review-table}
\end{table*}
