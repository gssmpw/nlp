\section{Algorithm from \cite{Sinha2024}}
The best known algorithm (Algorithm \ref{coco_sinha}) to solve COCO \cite{Sinha2024} was shown to have the following guarantee. 
\begin{theorem}\label{thm:sinha2024}[\cite{Sinha2024}]
Algorithm \ref{coco_sinha}'s $\textrm{Regret}_{[1:T]} = O(\sqrt{T})$ and  $\textrm{CCV}_{[1:T]} = O(\sqrt{T}\log T)$ when $f_t,g_t$ are convex.
\end{theorem}
We next show that in fact the analysis of \cite{Sinha2024} is tight for the CCV even when $d=1$ and $f_t(x)=f(x)$ and $g_t(x) =g(x)$ for all $t$. 
With finite diameter $D$ and the fact that any $x^\star \in \cX^\star$ belongs to all nested convex bodies $S_t$'s, when $d=1$, one expects that the CCV for any algorithm in this case will be $O(D)$. However, we as we show next,  Algorithm \ref{coco_sinha} does not effectively make use of geometric constraints imposed by nested convex bodies $S_t$'s.

\begin{algorithm}[tb]
   \caption{Online Algorithm from  \cite{Sinha2024}}
   \label{coco_sinha}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Sequence of convex cost functions $\{f_t\}_{t=1}^T$ and constraint functions $\{g_t\}_{t=1}^T,$ $G=$ a common Lipschitz constant, $T=$ Horizon length,
   %an upper bound $G$ to the Euclidean norm of their (sub)-gradients, 
    $D=$ Euclidean diameter of the admissible set $\mathcal{X},$ $\mathcal{P}_\mathcal{X}(\cdot)=$ Euclidean projection oracle on the set $\mathcal{X}$ 
     \State {\bfseries Parameter settings:} 
     \begin{enumerate}
     	\item \textbf{Convex cost functions:} $\beta = (2GD)^{-1}, V=1, \lambda = \frac{1}{2\sqrt{T}}, \Phi(x)= \exp(\lambda x)-1.$
     
    \item \textbf{$\alpha$-Strongly convex cost functions:} $\beta =1, V=\frac{8G^2 \ln(Te)}{\alpha}, \Phi(x)= x^2.$
    \end{enumerate}
     %$ \alpha=\frac{1}{2GD}, n=\max(2, \lceil \ln T \rceil), V=(n-1)^{n-1}T^{\frac{n-1}{2}}, \Phi(x)=x^n.$ 
%   \REPEAT
  \State {\bfseries Initialization:} Set $ x_1={\bf 0}, \text{CCV}(0)=0$.
   \State {\bf For} $t=1:T$
   %\For{$t=1:T$}
   \State \quad Play $x_t,$ observe $f_t, g_t,$ incur a cost of $f_t(x_t)$ and constraint violation of $(g_t(x_t))^+$
   \State \quad $\tilde{f}_t \gets \beta f_t, \tilde{g}_t \gets \beta \max(0,g_t).$
   \State \quad $\text{CCV}(t)=\text{CCV}(t-1)+\tilde{g}_t(x_t).$
   \State \quad Compute $\nabla_t = \nabla \hat{f}_t(x_t),$ where $\hat{f}_t(x):= V\tilde{f}_t(x)+ \Phi'(\text{CCV}(t)) \tilde{g}_t(x), ~~ t \geq 1$.
   \State \quad $x_{t+1} = \mathcal{P}_\mathcal{X}(x_t - \eta_t \nabla_t)$, where 
   \quad \begin{eqnarray*}
   \eta_t =\begin{cases}
   	\frac{\sqrt{2}D}{2\sqrt{\sum_{\tau=1}^{t} ||\nabla_\tau||_2^2}}, ~&~\textrm{for convex costs} \\
   	\frac{1}{\sum_{s=1}^t H_s}, ~ &~ \textrm{for strongly convex costs } (H_t \textrm{ is the strong convexity parameter of } f_t). 
   	\end{cases}
   	\end{eqnarray*}
   	
%   \IF{$x_i > x_{i+1}$}
%   \STATE Swap $x_i$ and $x_{i+1}$
%   \STATE $noChange = false$
%   \ENDIF
   %\EndFor
   \State {\bf EndFor}
%   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}




\begin{lemma}\label{lem:algwc}
Even when $d=1$ and $f_t(x)=f(x)$ and $g_t(x) =g(x)$ for all $t$, for Algorithm \ref{coco_sinha}, its $\textrm{CCV}_{[1:T]}  = \Omega(\sqrt{T} \log T)$.
\end{lemma}
\begin{proof}
{\bf Input:} Consider $d=1$, and let $\cX=[1, a], a>2$. Moreover, let $f_t(x)=f(x)$ and $g_t(x) =g(x)$ for all $t$. Let $f(x) = c x^2$ for some (large) $c>0$ and $g(x)$ be such that $G=\{x: g(x)\le 0\} \subseteq [a/2, a]$ and let $|\nabla g(x)|\le1$ for all $x$.

Let $1< x_1 < a/2$. Note that $\text{CCV}(t)$ (defined in Algorithm \ref{coco_sinha}) is a non-decreasing function, and let $t^\star$ be the earliest time $t$ such that $\Phi'(\text{CCV}(t)) \nabla g(x) <- c $. 
For $f(x) = c x^2$, $\nabla f(x) \ge c$ for all $x>1$. 
Thus, using Algorithm \ref{coco_sinha}'s definition, it follows that for all $t\le t^\star$, $x_t < a/2$, since the derivative of $f$ dominates the derivative of $\Phi'(\text{CCV}(t))  g(x)$ until then.


Since  $\Phi(x)= \exp(\lambda x)-1$ with $\lambda = \frac{1}{2\sqrt{T}}$, and by definition $|\nabla g(x)| \le 1$ for all $x$, thus, we have that by time $t^\star$, 
$\textrm{CCV}_{[1:t^\star]} = \Omega(\sqrt{T} \log T)$. Therefore, $\textrm{CCV}_{[1:T]} =\Omega(\sqrt{T} \log T)$.

 %Moreover, Algorithm \ref{coco_sinha}'s actions $x_t$'s keeps oscillating around $a/2$ after  time $t^\star$ leading to $\Omega(\sqrt{T})$ regret.
 \end{proof}
 
Essentially, Algorithm \ref{coco_sinha} is treating minimizing the $\text{CCV}$ problem as regret minimization for function $g$ similar to function $f$ and this leads to its CCV of $\Omega(\sqrt{T}\log T)$.
For any given input instance with $d=1$, an alternate algorithm that  chooses its actions following online gradient descent (OGD) projected on to the most recently revealed feasible set $S_t$ achieves $O(\sqrt{T})$ regret (irrespective of the starting action $x_1$) and $O(D)$ $\text{CCV}$ (since any $x^\star \in S_t$ for all $t$).  We extend this intuition in the next section, and present an algorithm that tries to exploit the geometry of the nested convex sets $S_t$ for general $d$.





