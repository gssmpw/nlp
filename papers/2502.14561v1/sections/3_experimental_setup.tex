\section{Methodology}\label{sec:experimental_setup}
% \section{\ser{Experimental Setup and}{Evaluation} Methodology}\label{sec:experimental_setup}

% \emph{\color{red} \textbf{Paris:} Should write something introductory here - probably merged froom Section 3.1 below.}

% \subsection{Overview and Objectives}\label{sec:overview_objectives}
% \emph{\color{red} \textbf{Paris:} Might merge this and 3.1 with above.}

In this section, we elaborate on the evaluation methodology followed in our experimental study. We first provide an overview of the used models (Section~\ref{sec:models}), datasets (Section~\ref{sec:datasets}), and configuration parameters (Section~\ref{sec:configuration-params}). Finally, in Section~\ref{sec:technical_specifications}, we present the technical specifications of the system we used for the experimentation.  







% \begin{table}
%     \centering
%     \footnotesize
%     \begin{tabular}{lcc}
%       \hline
%       \textbf{Model}                      & \textbf{\shortstack{Context Length}}  \\
%       \hline
%       Llama 3.2 Instruct~--~1B            & 128K                       \\
%       Llama 3.2 Instruct~--~3B            & 128K                       \\
%       Llama 3.1 Instruct~--~8B            & 128K                       \\
%       Llama 3 Instruct~--~8B              & 8192                       \\
%       Mistral Nemo Instruct~--~12B        & 128K                       \\
%       Phi3 Medium Instruct~--~14B         & 128K                       \\
%       Phi3.5 Mini Instruct~--~3.8B        & 128K                       \\
%       Gemma 2 Instruct~--~2B              & 8192                       \\
%       Gemma 2 Instruct~--~9B              & 8192                       \\
%       Gemma 2 Instruct~--~27B             & 8192                       \\
%       Qwen 2 Instruct~--~7B               & 32K                        \\
%       Qwen 2.5 Instruct~--~14B            & 32K                        \\
%       \hline
%     \end{tabular}
%     \caption{Details of selected models for evaluation.}\label{tab:models}
% \end{table}



% \begin{table*}
%     \centering
%     \footnotesize
%     \begin{tabular}{llll}
%       \hline
%       \textbf{Model}                      & \textbf{Context Length}    & \textbf{Parameters}  & \textbf{HF Links} \\
%       \hline
%       Llama 3.2~--~1B Instruct            & 128K                       & 1B                   & \href{http://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF}{hugging-quants/Llama-3.2-1B-Instruct-Q8\_0-GGUF} \\
%       Llama 3.2~--~3B Instruct            & 128K                       & 3B                   & \href{https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q8_0-GGUF}{hugging-quants/Llama-3.2-3B-Instruct-Q8\_0-GGUF} \\
%       Llama 3.1~--~8B Instruct            & 128K                       & 8B                   & \href{https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF}{lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF} \\
%       Llama 3~--~8B Instruct              & 8192                       & 8B                   & \href{https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF}{lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF} \\
%       Mistral Nemo~--~12B Instruct        & 128K                       & 12B                  & \href{https://huggingface.co/lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF}{lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF} \\
%       Phi3 Medium 128K~--~14B Instruct    & 128K                       & 14B                  & \href{https://huggingface.co/bartowski/Phi-3-medium-128k-instruct-GGUF}{bartowski/Phi-3-medium-128k-instruct-GGUF} \\
%       Phi3.5 Mini~--~Instruct             & 128K                       & 3.8B                 & \href{https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF}{bartowski/Phi-3.5-mini-instruct-GGUF} \\
%       Gemma 2~--~2B Instruct              & 8192                       & 2B                   & \href{https://huggingface.co/lmstudio-community/gemma-2-2b-it-GGUF}{lmstudio-community/gemma-2-2b-it-GGUF} \\
%       Gemma 2~--~9B Instruct              & 8192                       & 9B                   & \href{https://huggingface.co/bartowski/gemma-2-9b-it-GGUF}{bartowski/gemma-2-9b-it-GGUF} \\
%       Gemma 2~--~27B Instruct             & 8192                       & 27B                  & \href{https://huggingface.co/bartowski/gemma-2-27b-it-GGUF}{bartowski/gemma-2-27b-it-GGUF} \\
%       Qwen 2~--~7B Instruct               & 32K                        & 7B                   & \href{https://huggingface.co/Qwen/Qwen2-7B-Instruct}{Qwen/Qwen2-7B-Instruct} \\
%       Qwen 2.5~--~14B Instruct            & 32K                        & 14B                  & \href{https://huggingface.co/lmstudio-community/Qwen2.5-14B-Instruct-GGUF}{lmstudio-community/Qwen2.5-14B-Instruct-GGUF} \\
%       \hline
%     \end{tabular}
%     \caption{Models with Context Length, Number of Parameters, and HuggingFace Links. \emph{\color{red} \textbf{Paris}: Might remove links and make smaller -- or just mention them by name and put the detailed table in Appendix.}} \label{tab:models}
% \end{table*}


\subsection{Models}\label{sec:models}

For our experiments, we selected the Instruction-tuned versions of five prominent open-weight model families: \textit{LLaMA}~\citep{llama3}, \textit{Mistral}~\citep{mistral}, \textit{Phi}~\citep{phi3}, \textit{Gemma}~\citep{gemma2}, and \textit{Qwen}~\citep{qwen2, qwen2_5}. 
% For several models, we test different variations of training parameters, resulting to a total of $12$ different models.
% The experimental design was shaped by practical constraints and research goals. 
A key challenge was performing the experiments on commodity hardware, both due to computational limitations and to demonstrate that citation intent classification can be executed efficiently with limited resources. This influenced the selection of models, which range in size from small ($1$B parameters) to medium-sized ($32$B parameters).
% \ser{The design also sought to maximize the range of configurations explored to identify optimal performance scenarios.}{}

% \ser{The selection criteria prioritized models with parameter sizes ranging from 1B to 32B to ensure compatibility with commodity single-GPU machines.}{}
Since in-context learning inherently increases the number of tokens to each prompt (particularly in the many-shot scenario), we opted for a lower cutoff of $8,192$ tokens in the context length. 
This ensures that all selected models could process the longest prompts in our experiments without truncation.

To reduce the memory footprint and computational requirements of our evaluation, we utilize the 8-bit (Q8) quantized versions of the models. This approach significantly reduces memory usage without compromising performance. \textit{Quantization} involves converting model parameters from 16-bit floating-point precision to 8-bit integers, enabling more efficient computation while preserving expressive power and accuracy~\citep{jm3}.

% Table~\ref{tab:models} summarizes the selected models. A detailed view along with the HuggingFace links for each model is available at Appendix~\ref{sec:appendix-c-models}.
% Table~\ref{tab:models} \ser{provides an overview of}{summarizes} the selected models, \ser{including their}{listing} context length, \ser{}{and} number of parameters\ser{, and HuggingFace links for easy access}{}.

The model variations used for our evaluation were the following (a more detailed view is available in Appendix~\ref{sec:appendix-c-models}):

\begin{itemize}[nosep,topsep=1pt]
    \item Llama 3 \& 3.1 (8B), Llama 3.2 (1B, 3B)
    \item Mistral Nemo (12B)
    \item Phi 3 Medium (14B), Phi 3.5 Mini (3.8B)
    \item Gemma 2 (2B, 9B, 27B)
    \item Qwen 2 (7B), Qwen 2.5 (14B)
\end{itemize}

\subsection{Datasets}\label{sec:datasets}
For our experiments, we used two datasets:

\begin{itemize}[nosep,topsep=1pt]
    \item \textit{SciCite}~\citep{CAZ2019} consists of approximately $11,000$ citation strings annotated with three classes: \texttt{Background Information}, \texttt{Method}, and \texttt{Results Comparison}. 
    % \ser{It is widely used in the literature as a benchmark for citation intent tasks.}{}
    \item \textit{ACL-ARC}~\citep{JKH2018} contains 2,000 citation strings classified into six categories: \texttt{Background}, \texttt{Motivation}, \texttt{Uses}, \texttt{Extends}, \texttt{Compares or Contrasts}, and \texttt{Future}. 
    % \ser{It provides a more granular classification of citation intents.}{}
\end{itemize}

These two datasets are widely used in citation intent classification research. 
The SciCite dataset is larger and more diverse, while the ACL-ARC dataset offers a more granular classification scheme. 

% These characteristics allow us to evaluate model performance across a range of citation intent classification tasks.

% \emph{\color{red} \textbf{Paris:} Will add more details about the datasets from their respective papers.}

\subsection{Configuration Parameters}
\label{sec:configuration-params}
In this section, we describe the configuration parameters of the examined models, including prompting methods, system prompts, query templates, example methods, and temperature settings. 
% Details on the range of values for these parameters can be found in Table~\ref{tab:experimental-params}.

% \begin{table}[t]
%     \centering
%     \footnotesize
%     \begin{tabular}{cc}
%       \hline
%       \textbf{Parameter} & \textbf{Range of values} \\
%       \midrule
%       \multirow{2}{*}{\textbf{Prompting methods}} & Zero-shot, One-shot, \\
%                                                   & Few-shot, Many-shot \\
%       \midrule
%       \textbf{System prompts} & SP1, SP2, SP3 \\
%       \midrule
%         \textbf{Query templates} & Simple, Multiple-choice \\
%         \textbf{Example methods} & Inline, Roles \\
%         \textbf{Temperature} & $0.0$, $0.2$, $0.5$, $1.0$ \\
%       \hline
%     \end{tabular}
%     \caption{Configuration parameters. \emph{\color{red} (\textbf{Paris:} Might remove)}}\label{tab:experimental-params}
% \end{table}



\subsubsection{In-context Learning Methods}\label{sec:in_context_learning}
To evaluate model performance, we applied four prompting methods (PM) by following the In-Context Learning paradigm~\citep{BMR2020}:
% \begin{itemize} [nosep,topsep=1pt]
%     \item \textbf{Zero-shot prompting:} No examples are provided. The model infers the task solely from the system prompt.
%     \item \textbf{One-shot prompting:} One example per class is provided to guide the model.
%     \item \textbf{Few-shot prompting:} Five examples per class are included to offer additional context.
%     \item \textbf{Many-shot prompting:} Ten examples per class are provided, representing a more extensive context.
% \end{itemize}
\textit{Zero-shot} (no examples),
\textit{One-shot} (a single example per class),
\textit{Few-shot} ($5$ examples per class), and
\textit{Many-shot} ($10$ examples per class).

The aforementioned prompting methods were selected to evaluate whether model performance improves as the number of examples increases, and to identify any saturation point where additional examples yield diminishing or negative returns. 
This aligns with prior literature on in-context learning methods, where these specific configurations have been extensively studied~\citep{icl-survey}.
% \ser{In-context learning is particularly suited to citation intent classification, as it allows the models to leverage their pre-trained contextual understanding without requiring task-specific fine-tuning.}{}
Incorporating examples directly into the prompts allows models to better understand the task and the expected output format, which is especially important for citation intent classification, where the citation context is key to determining the correct label.




\subsubsection{System Prompts}\label{sec:system_prompts}
System prompts (SP) play a critical role in our experimental design, as they set the context and expectations for the model's behavior. 
They guide the model's attention on citation intent classification, providing task-specific context, 
class definitions, and guidelines to ensure outputs are aligned with the evaluation requirements.

We explored three distinct variations of system prompts. The first (SP1) was a simple, intuitive instruction, serving as a natural starting point for introducing the task.
The second prompt (SP2) adopted a structured approach inspired by the CO-STAR framework~\citep{costar}, which organizes prompts into six key components. In particular, \textit{Context} provides background information to help the model understand the scenario, while \textit{Objective} clearly defines the task to direct its focus. \textit{Style}, \textit{Tone}, and \textit{Audience} shape the model's writing style, sentiment, and intended readership, respectively. 
Finally, \textit{Response} defines the expected format to ensure clarity and downstream processing.

% \begin{itemize}[nosep,topsep=1pt]
%     \item \textbf{Context (C):} Providing background information helps the model understand the specific scenario.
%     \item \textbf{Objective (O):} Clearly defining the task directs the model’s focus.
%     \item \textbf{Style (S):} Specifying the desired writing style aligns the model’s response.
%     \item \textbf{Tone (T):} Setting the tone ensures the response resonates with the required sentiment.
%     \item \textbf{Audience (A):} Identifying the intended audience tailors the model’s output.
%     \item \textbf{Response (R):} Defining the expected output format ensures clarity and facilitates downstream processing.
% \end{itemize}
Since our task requires a single citation label as output, 
we excluded stylistic components (i.e.,~Style, Tone, and Audience),
and incorporated detailed class definitions in the Response section.
The third variation (SP3) refined the structured prompt by explicitly reiterating the expected labels in the Response section; this aims to further clarify the task, and enhance the consistency of the model's outputs. 
% \ser{Initial testing suggested that explicitly reinforcing the available classes improved performance.}{}
All the prompts can be found in Appendix~\ref{sec:appendix-a-system-prompts}.

% \emph{\color{red} \textbf{Paris:} Will add the prompts in the Appendix.}

\subsubsection{Query Templates}\label{sec:query_templates}
\emph{Query templates} (QT) define the structure in which citation sentences are presented to the model, both as examples during prompting and as queries during evaluation. 
Initially, we adopted a \textit{Simple Query} template: 
the citation sentence was followed by ``Class:'', 
either pre-filled with the correct class (in examples) or left blank (in queries). However, we observed that some models struggled to align with the expectation of responding solely with the class labels, leading to inconsistent performance and increased variance.
To address this, we introduced a \textit{Multiple-choice Query} template, where, following the citation sentence, the model is explicitly asked to identify the most likely citation intent, with the possible class labels presented as multiple-choice options. 
While this approach increases the token count per query, it demonstrates significant performance gains, as discussed in Section~\ref{sec:parameter_performance}.

% \ser{In both templates, minimal post-processing was necessary to extract the predicted class label from the model’s output and ensure alignment with the evaluation criteria.}{}

\begin{table*}[t]
    \centering
    \footnotesize
    \begin{tabular}{cclcccccc}
      \hline
      \textbf{Dataset}            &  \textbf{Rank} &   \textbf{Model}          & \textbf{Overall}    & \textbf{Zero-Shot}  & \textbf{One-Shot} & \textbf{Few-Shot}  & \textbf{Many-Shot} \\
      \hline
      \multirow{4}{*}{SciCite}  & 1             &   Qwen 2.5~--~14B         & 112                               & 20                  &  37               & 27                 & 28                 \\
                                & 2             &   Mistral Nemo~--~12B     & 22                                & 0                   &  0                & 12                 & 10                 \\
                                & 3             &   Gemma 2~--~9B           & 4                                 & 0                   &  3                & 1                  & 0                  \\
                                & 4             &   Gemma 2~--~27B          & 2                                 & 0                   &  0                & 0                  & 2                  \\
      \hline
      \multirow{2}{*}{ACL-ARC}  & 1             &   Qwen 2.5~--~14B         & 24                          & 4                   &  4                & 8                  & 8                  \\
                                & 2             &   Gemma 2~--~27B          & 4                           & 0                   &  4                & 0                  & 0                  \\
      \hline
    \end{tabular}
    \caption{Model ranking based on Best-Performing Count (Overall \& By Prompting Method).}\label{tab:best_performer_evaluation}
\end{table*}

\begin{table*}[t]
    \centering
    \footnotesize
    \begin{tabular}{cccccccccc}
        \hline
        \textbf{Dataset}            & \textbf{Rank}     & \textbf{Model}       & \textbf{PM}    & \textbf{SP}   & \textbf{QT}       & \textbf{EM}   & \textbf{T}    & \textbf{F1-Score} \\
        \hline
        \multirow{5}{*}{SciCite}    & 1                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 0.5           & 78.27 \\
                                    & 2                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 1.0           & 78.39 \\
                                    & 3                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 0.2           & 78.34 \\
                                    & 4                 & Qwen 2.5~--~14B       & Many-shot     & SP3           & Multiple-Choice   & Roles         & 0.0           & 78.00 \\
                                    & 5                 & Qwen 2.5~--~14B       & Many-shot     & SP3           & Multiple-Choice   & Roles         & 0.5           & 77.93 \\
        \hline
        \multirow{5}{*}{ACL-ARC}    & 1                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 1.0           & 60.88 \\
                                    & 2                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 0.0           & 59.75 \\
                                    & 3                 & Qwen 2.5~--~14B       & Few-shot      & SP3           & Multiple-Choice   & Roles         & 0.2           & 59.66 \\
                                    & 4                 & Qwen 2.5~--~14B       & One-shot      & SP3           & Multiple-Choice   & Roles         & 0.5           & 64.87 \\
                                    & 5                 & Qwen 2.5~--~14B       & One-shot      & SP3           & Multiple-Choice   & Roles         & 1.0           & 64.32 \\
        \hline
    \end{tabular}
    \caption{Top-5 Configurations on SciCite and ACL-ARC (Q8).}
    \label{tab:top_5_configurations}
\end{table*}

\subsubsection{Example Method Variations}\label{sec:example_method_variations}

In our experiments, we explored two methods for presenting examples to the models (EM): \emph{Inline} and \emph{Roles}.

In the Inline approach, example citation sentences and their corresponding classes (as per the defined query templates) are provided directly within the system prompt. After this setup, the prompt then transitions to the evaluation phase, where the ``user'' provides a citation sentence without a corresponding class and the model, acting as the ``assistant'', predicts the correct class.

The Roles method, on the other hand, simulates a conversational exchange between the ``user'' and ``assistant''.
Each example follows a back-and-forth interaction, with the ``user'' providing a citation sentence and the ``assistant'' responding with the correct class. 
After several such examples, the interaction moves to the evaluation phase, where the ``user'' presents a new citation sentence, and the model predicts its class.

% \ser{We did not identify any immediate theoretical advantage to either method, so both were tested to evaluate their potential impact on model performance. Further discussion on the outcomes and insights derived from these methods can be found in Section~\ref{sec:parameter_performance}.}{}

\subsubsection{Temperature}\label{sec:temperature}

Temperature (T) is a hyperparameter that controls the randomness or creativity of a language model's outputs by adjusting the probability distribution of possible next tokens~\citep{jm3}. Lower temperatures (i.e., close to $0$) correspond to greedy decoding, where the model deterministically selects the most probable token at each step, while higher temperatures (closer to $1$) introduce greater variability by allowing the model to sample from a broader range of options.

For classification tasks, we aim for the model to output the most probable class label. 
Therefore, we use a temperature of $0$ as the baseline, ensuring fully deterministic predictions. 
To explore how controlled randomness affects the model's behavior and classification performance, we also evaluate higher temperatures, such as 0.2, 0.5, and 1.0.
A temperature of 0.2 introduces a small degree of randomness but still heavily favors the most probable answer, while 0.5 strikes a balance between randomness and determinism. At 1.0, randomness is maximized, allowing us to assess whether excessive stochasticity degrades performance.

% {\color{red}
% The temperature settings tested were as follows:
% \begin{itemize}[nosep,topsep=1pt]
%     \item \textbf{Temperature 0.0 (Greedy Decoding):} The model is fully-deterministic and is forced to choose the most probable answer without any randomness. We use this as our baseline to compare how flexible temperatures improve or worsen accuracy.
%     \item \textbf{Temperature 0.2:} Introduces a small degree of randomness but still heavily favors the most probable answer.
%     \item \textbf{Temperature 0.5:} Strikes a balance between randomness and determinism.
%     \item \textbf{Temperature 1.0:} Maximizes randomness. We investigate this to test if too much randomness degrades performance significantly.
% \end{itemize}
% }

% \begin{table*}[t]
%     \centering
%     \footnotesize
%     \begin{tabular}{clcccccc}
%       \hline
%       \textbf{Rank} &   \textbf{Model}          & \textbf{Best-Performing}    & \textbf{Zero-Shot}  & \textbf{One-Shot} & \textbf{Few-Shot}  & \textbf{Many-Shot} \\
%       \hline
%       1             &   Qwen 2.5~--~14B         & 112                               & 20                  &  37               & 27                 & 28                 \\
%       2             &   Mistral Nemo~--~12B     & 22                                & 0                   &  0                & 12                 & 10                 \\
%       3             &   Gemma 2~--~9B           & 4                                 & 0                   &  3                & 1                  & 0                  \\
%       4             &   Gemma 2~--~27B          & 2                                 & 0                   &  0                & 0                  & 2                  \\
%       \hline
%     \end{tabular}
%     \caption{Model ranking based on Best-Performing Count (Overall \& By Prompting Method) on SciCite.}\label{tab:best_performer_evaluation_scicite}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \footnotesize
%     \begin{tabular}{clcccccc}
%       \hline
%       \textbf{Rank} &   \textbf{Model}          & \textbf{Best-Performing}    & \textbf{Zero-Shot}  & \textbf{One-Shot} & \textbf{Few-Shot}  & \textbf{Many-Shot} \\
%       \hline
%       1             &   Qwen 2.5~--~14B         & 24                          & 4                   &  4                & 8                  & 8                  \\
%       2             &   Gemma 2~--~27B          & 4                           & 0                   &  4                & 0                  & 0                  \\
%       \hline
%     \end{tabular}
%     \caption{Model ranking based on Best-Performing Count (Overall \& By Prompting Method) on ACL-ARC.{\color{red}SER: merge these 2 tables}}\label{tab:best_performer_evaluation_aclarc}
% \end{table*}

\subsection{Technical Specifications}\label{sec:technical_specifications}

We conducted our experiments on an M1 Max Mac Studio with $64$GB of memory, chosen to demonstrate the feasibility of running inference for Citation Intent Classification on commodity hardware.

For model hosting, 
we used LM Studio\footnote{\url{https://lmstudio.ai/}} which offers an intuitive interface for testing and interacting with models hosted on HuggingFace or locally. 
It also supports a local server mode compatible with the OpenAI API, allowing interaction through an API accessible in multiple programming languages. 
A command-line interface (CLI) tool\footnote{\url{https://github.com/lmstudio-ai/lms}} is also available for managing the server without using the UI.

% For hosting the models, we used LM Studio. LM Studio provides:
% \begin{enumerate}
%     \item User Interface (UI) for testing and interacting with models hosted on HuggingFace or locally.
%     \item Local server mode that provides API compatibility with the OpenAI API and supports multiple programming languages.
%     \item Command-line interface (CLI) tool (lms) for managing the server without relying on the UI.
% \end{enumerate}

% While alternatives like Ollama or HuggingFace's transformers library exist, 
% we chose LM Studio for its ease of use and compatibility with the OpenAI API, which aligned with our workflow.

% \ser{Since we are open-sourcing our evaluation platform, \ser{and the models and experimental setups are}{with} configurable \ser{}{models and setups}, 
% we \ser{are considering adding}{plan to add} support for alternative hosting tools in the future to accommodate different user needs and computational environments.}{}

% \emph{ \color{red} \textbf{Paris:} I want to say more about the evaluation framework and technical stuff, so I might make this section smaller and add more details to an appendix called ``Evaluation Platform''.}
