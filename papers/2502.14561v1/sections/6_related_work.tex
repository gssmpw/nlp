\section{Related Work}\label{sec:related_work}

% Outline:

% - First, mention the theoretical beginnings~\citep{Gar1965, MM1975, Spi1977}.

% - Then, go to first attempts to classify citations by their function in a fully automatic manner~\citep{GM2000, TST2006a}

% - Then, mention~\citep{JKH2018} that introduced the ACL-ARC dataset and \citep{PK2020} that uses a similar classification scheme with an additional layer for the COMPARE\_CONTRAST category (similarities, differences, disagreement).

% - Then go to DL methods and transformers-based models like BERT and XLNet and ensembles of them, \citep{CAZ2019, BLC2019, MRR2021, HHD2022, PVD2024}.

% - Finally, go to LLM-related architectures \citep{LSM2023, SKK2024}.

% - Close with the prompting strategies paper \citep{KPK2023} - mention the closed OpenAI models - contrast our work with this.

The theoretical beginnings of citation analysis can be traced back to foundational works such as \citeposs{Gar1965} identification of reasons for citation \citeposs{MM1975} studies on citation function. Early annotation schemes, such as those by~\citet{Spi1977}, were later adapted by \citet{TST2006a} for supervised machine learning approaches to citation classification.

\citet{JKH2018} introduced the ACL-ARC dataset, which contains nearly $2,000$ citations from papers in the NLP field, annotated for their function with a classification scheme of six classes. \citet{PK2020} extended this classification scheme by refining the comparison class to capture similarities, differences, and disagreement. Around the same time, \citet{CAZ2019} proposed a multitask model incorporating structural information from scientific papers. They also introduced the SciCite dataset, which is significantly larger and spans multiple scientific domains, with three intent classes. 

\citet{BLC2019} introduced SciBERT, a BERT-based~\citep{delvin-bert} pre-trained language model (PLM) for scientific text, which has since become the backbone of many citation intent classification methods. SciBERT has been widely adopted due to its ability to generalize across scientific domains. For example, \citet{MRR2021} introduced ImpactCite, an XLNet-based method for citation impact analysis, which was later used by \citet{PVD2024} to achieve state-of-the-art results on the SciCite dataset. Paolini et al. demonstrated the effectiveness of ensemble classifiers combining fine-tuned SciBERT and XLNet models. On the ACL-ARC dataset, \citet{HHD2022} achieved state-of-the-art results by studying domain adaptation of language models using a variational masked autoencoder.


\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method}                                     & \textbf{SciCite}  & \textbf{ACL-ARC}  \\
        \midrule
        Feature-rich RF~\citep{JKH2018}                     & --                & 53.00 \\
        Structural Scaffolds~\citep{CAZ2019}                & 84.00             & 67.90              \\
        SciBERT~\citep{BLC2019}                             & 85.22             & --                \\
        ImpactCite~\citep{MRR2021}                          & 88.93             & --                \\
        VarMAE~\cite{HHD2022}                               & 86.32             & 76.50             \\
        CitePrompt~\citep{LSM2023}                          & 86.33             & 68.39             \\
        EnsIntWS~\citep{PVD2024}                            & 89.46             & --                \\
        EnsIntWoS~\citep{PVD2024}                           & 88.48             & --                \\
        MTL Finetuning (Search)~\citep{SKK2024}             & 85.25             & 64.56             \\
        MTL Finetuning (TRL)~\citep{SKK2024}                & 85.35             & 75.57             \\
        \bottomrule
        \end{tabular}
        \caption{Reported F1-Scores of notable works from the literature, sorted chronologically.}\label{tab:literature-f1}
\end{table}

Recent research has continued to explore PLM-based methods for citation intent classification. \citet{LSM2023} used a prompt-based learning approach on SciBERT to identify citation intent, while \citet{SKK2024} propose a multi-task learning framework that jointly fine-tunes SciBERT on a dataset of primary interest together with multiple auxiliary datasets to take advantage of additional supervision signals. \citet{KPK2023} explored various prompting and tuning strategies on SciBERT, including fixed and dynamic context prompts, and found that parameter updating with prompts improved performance. They also briefly experiment on LLMs by evaluating the zero-shot performance of GPT-3.5, which performed well on their recently introduced ACT2 dataset but poorly on the ACL-ARC dataset. However, GPT-3.5 was not evaluated on SciCite. In contrast, our work is the first to focus entirely on evaluating and fine-tuning numerous open-weight LLMs without using models pre-trained specifically for the scientific domain. 

Table~\ref{tab:literature-f1} provides a summary of the reported F1-scores of notable works in the literature, highlighting the progression of methods and datasets. SciBERT-based methods dominate the field, while our work is the first to examine LLMs on this task without any reliance to SciBERT.

% \begin{itemize}[nosep,topsep=1pt]
%     \item Feature-rich Random Forest~\cite{JKH2018}
%     \item Scaffolds~\citep{CAZ2019}
%     \item ImpactCite~\citep{MRR2021}
%     \item CitePrompt~\citep{LSM2023}
%     \item SciBERT~\citep{BLC2019}
%     \item EnsInt~\citep{PVD2024}
%     \item Multi-task Learning~\citep{SKK2024}
%     \item VarMAE~\cite{hu2022varmae}
% \end{itemize}
