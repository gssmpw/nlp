\section{Introduction}
\label{sec:intro}

Citations are references of research articles to external sources of information included to support claims, provide context, criticize, or acknowledge prior work. Although their primary function is to inform and redirect the reader, citations are also frequently used for other purposes, such as serving as proxies of scientific impact in various types of analysis~\citep{KanellosVSDV21}. In such use cases, understanding the exact intent of a citation is crucial, as not all types of citations should be considered. For instance, while measuring the impact of an article, citations that criticize the work should generally not be considered as contributing to its impact.   

Predicting citation intent based on its context (i.e., the sentences in the manuscript that accompany the citation) and other related information, has become an important classification problem to support the aforementioned use cases. Existing approaches have traditionally relied on linguistic features~\citep{JKH2018}, machine learning methods \citep{TST2006a}, and, in recent years, on pre-trained language models (PLMs) such as SciBERT~\citep{BLC2019} that require large scientific datasets such as~\citep{s2orc} and task-specific architectures. In contrast, this work is the first to explore the potential of open, general-purpose Large Language Models (LLMs) to accurately identify citation intent without using any scientific-trained PLMs, evaluating their effectiveness through in-context learning and fine-tuning on minimal task-specific data. 

% Large Language Models (LLMs) are artificial intelligence (AI) approaches trained on vast amounts of text data to understand and generate human-like language. They use deep learning, particularly transformer architectures, to process and generate text, making them highly effective for various tasks such as translation and summarization. Over time, LLMs have evolved into accessible tools for tackling machine learning problems without requiring extensive expertise in AI. Their key strengths include ease of use, as they can be accessed via APIs or chat interfaces and versatility, since they can handle a wide range of tasks without task-specific training. Clearly, exploring their potential in citation intent classification presents an intriguing and valuable research opportunity. 

Large Language Models (LLMs) are advanced natural language processing systems trained on extensive text corpora to perform a wide range of language tasks. Unlike traditional pre-trained language models (PLMs), LLMs are general-purpose models capable of adapting to new tasks with minimal additional training. Their ability to process and generate coherent text across diverse contexts makes them particularly suitable for tasks like citation intent classification, where understanding nuanced language patterns is essential.

In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration~\citep{BMR2020, icl-survey}. In-context learning is particularly suited to citation intent classification, as it allows the models to leverage their contextual understanding of language to make accurate predictions.

In our study, we conduct an extensive experimental analysis of $12$ general-purpose instruction-tuned LLMs from $5$ model families,  on two widely used datasets for this task. In this context, we analyze the impact of multiple parameters on model performance and identify the optimal configurations and best-performing models for our problem. These experiments address the following research questions: 

\begin{itemize}[nosep,topsep=1pt]
    \item \textit{RQ1:} How well can pre-trained LLMs perform on citation intent classification without task-specific training? 
    \item \textit{RQ2:} What are the differences in performance between open LLMs of varying parameter counts?
    \item \textit{RQ3:} How do different prompting-related parameters affect model performance? 
\end{itemize}

%\begin{table}[b]
%    \centering
%    \footnotesize
%    \begin{tabular}{lp{0.8\linewidth}}
%      \hline
%                     & \textbf{Question}  \\
%      \hline
%      RQ1 & How well can pre-trained LLMs perform on citation intent classification without task-specific training compared to the state-of-the-art? \\
%      \hline
%      RQ2 & What are the differences in performance between small and mid-sized state-of-the-art open LLMs?\\
%      \hline
%      RQ3 & How do different prompting-related parameters affect model performance? Are there definitive optimal choices for some parameters?\\
%      \hline
%      RQ4 & How much does supervised fine-tuning with task-specific training increase the performance of the instruction-tuned models?\\
%      \hline
%      \end{tabular}
%          \caption{Research questions.}\label{tab:rqs}
%      \end{table}

Furthermore, we take the analysis a step further by selecting the top-performing cases from the aforementioned experiments for Supervised Fine-Tuning (SFT) on our datasets, allowing us to evaluate their optimal performance. This essentially addresses an additional research question:

\begin{itemize}[nosep,topsep=1pt]
    \item \textit{RQ4:} How much does supervised fine-tuning with task-specific training affect the performance of the instruction-tuned models?
\end{itemize}

% \subsection{Experimental Setup}\label{sec:experimental_design}
% \emph{\color{red} \textbf{Paris:} The bullets will be removed, but I'm not sure how/if to put the into the text.}
% \ser{}{[SER: merge with the intro in 3? most of these are explained in dedicated subsections]}
% In this section, we outline the experimental design for our evaluation. Our methodology includes the following steps:
% \begin{itemize} [nosep,topsep=1pt]
%     \item We select 12 models across five model families (LLaMA, Mistral, Phi, Gemma, Qwen).
%     \item We create three system prompts: one simple, one with a structured format and well-defined rules, and a variation of the latter.
%     \item We test models using four prompting methods:
%     \begin{enumerate}[nosep,topsep=1pt]
%         \item Zero-shot (no examples)
%         \item One-shot (1 example per class)
%         \item Few-shot (5 examples per class)
%         \item Many-shot (10 examples per class)
%     \end{enumerate}
%     \item Examples are presented to the models in two ways: inline within the system prompt or using the ``Roles'' mechanism of the LMStudio API.
%     \item Two query templates are used to structure the examples:
%     \begin{enumerate}
%         \item Simple: Presenting the sentence and the label directly
%         \item Multiple-choice: Presenting the sentence and asking the model to select the correct label from a list of options
%     \end{enumerate}
%     \item Four temperature settings (0.0, 0.2, 0.5, 1.0) are evaluated to assess variability in response generation.
% \end{itemize}
% \ser{To benchmark against the state-of-the-art}{Specifically}, we compare \ser{model performance}{$12$ general- LLM models from $5$ discrete categories (Section~\ref{sec:models}), against the state-of-the-art,} on \ser{theSciCite and ACL-ARC which serve as the primary datasets for our experiments.}{two widely used datasets for citation intent (Section~\ref{sec:datasets})}

Our experiments  aim to contribute to the broader research community by guiding prompt engineering and model selection strategies for similar tasks. To further assist researchers, we openly\footnote{\url{https://github.com/athenarc/CitationIntentOpenLLM}} provide our complete testing suite and evaluation results, allowing seamless integration of Hugging Face models and easy configuration adjustments. We also publish the weights and checkpoints of the fine-tuned models\footnote{\url{https://huggingface.co/collections/sknow-lab/citationintentllm-67b72f1d5ca6113f960dba04}} presented in Section~\ref{sec:fine_tuning}.

