\section{Experimental Results and Analysis}\label{sec:results}
This section presents the results of our experiments, focusing on the performance of models across various configurations and parameter settings. We analyze the best-performing models, evaluate their rankings, and explore how specific parameters influence performance.

% \emph{\color{red} \textbf{Paris:} \textbf{1}~--~My issue with this section is the is that we don't really show the actual results anywhere (apart from Table \ref{tab:optimal_configurations} which has some cherry-picked results - might get removed for different reasons, see section 4.3 notes). I plan to put the Top-20 or Top-50 in the appendix, but maybe we should put a Top-5 here and reference it in the 4.1 introductory section (it would look like Table \ref{tab:top_5_configurations})? \textbf{2}~--~I also had a section for the statistical tests as a second form of parameter performance analysis (section 4.2.2), but I removed it because it took too much space and didn't really provide any new insights (it was mostly a confirmation of the quantile-based analysis). It is still commented-out in the tex file. We could put it in the appendix, but I'm not sure if it's worth it~--~it also feels that we might be abusing the unlimited appendix at this point.}


\subsection{Model Performance over Configurations}\label{sec:best_models}

Our first evaluation focuses on identifying the top-performing models across the configurations described in Section~\ref{sec:experimental_setup}. We employed two complementary approaches: (i) the Best-Performer Evaluation identifies the single best model, and (ii) the Ranked Evaluation considers the relative performance of all models across configurations.

% \begin{table*}[t]
%     \centering
%     \footnotesize
%     \begin{tabular}{ccccccccccccc}
%         \hline
%         \textbf{Dataset}            & \textbf{Rank}     & \textbf{Model}       & \textbf{Precision}    & \textbf{Recall}   & \textbf{F1-Score}     & \textbf{Accuracy}     & \textbf{PM} & \textbf{SP} & \textbf{QT} & \textbf{EM} & \textbf{T} \\
%         \hline
%         \multirow{5}{*}{SciCite}    & 1                 & QWEN 2.5~--~14B      & 0.783 & 0.789 & 0.783 & 0.807 & Few-shot & 3 & 2 & 2 & 0.5 \\
%                                     & 2                 & QWEN 2.5~--~14B      & 0.783 & 0.789 & 0.783 & 0.807 & Few-shot & 3 & 2 & 2 & 1.0 \\
%                                     & 3                 & QWEN 2.5~--~14B      & 0.783 & 0.786 & 0.783 & 0.807 & Few-shot & 3 & 2 & 2 & 0.2 \\
%                                     & 4                 & QWEN 2.5~--~14B      & 0.780 & 0.794 & 0.782 & 0.808 & Many-shot &3 & 2 & 2 & 0.0 \\
%                                     & 5                 & QWEN 2.5~--~14B      & 0.781 & 0.786 & 0.781 & 0.805 & Few-shot & 3 & 2 & 2 & 0.0 \\
%         \hline
%         \multirow{5}{*}{ACL-ARC}    & 1                 & QWEN 2.5~--~14B      & 0.609 & 0.654 & 0.610 & 0.669 & Few-shot & 3 & 2 & 2 & 1.0 \\
%                                     & 2                 & QWEN 2.5~--~14B      & 0.597 & 0.645 & 0.602 & 0.655 & Few-shot & 3 & 2 & 2 & 0.0 \\
%                                     & 3                 & QWEN 2.5~--~14B      & 0.597 & 0.645 & 0.601 & 0.655 & Few-shot & 3 & 2 & 2 & 0.2 \\
%                                     & 4                 & QWEN 2.5~--~14B      & 0.649 & 0.578 & 0.600 & 0.662 & One-shot & 3 & 2 & 2 & 0.5 \\
%                                     & 5                 & QWEN 2.5~--~14B      & 0.643 & 0.584 & 0.598 & 0.669 & One-shot & 3 & 2 & 2 & 1.0 \\
%         \hline
%     \end{tabular}
%     \caption{Top 5 Configurations on SciCite and ACL-ARC.}
%     \label{tab:top_5_configurations}
% \end{table*}


\subsubsection{Best-Performer Evaluation}\label{sec:best_performer_evaluation}

To identify the most effective models across configurations, we initially conducted a Best-Performer Evaluation. In particular, for each configuration, we examined a metrics table containing precision, recall, F1-score, and accuracy for all models. 
The model with the highest F1-score in each configuration was selected as the best-performing model~--~in case of a tie, we used Accuracy as the deciding factor. 
% We then aggregated the results across all configurations and grouped them by model to determine how often each model achieved the best performance accross our configurations. \ser{This approach allowed us to quantify the dominance of each model across the experimental space.}{}

The aggregated results of our evaluation are presented in Table~\ref{tab:best_performer_evaluation};
additionally, Table~\ref{tab:top_5_configurations} lists the top-$5$ best-performing configurations on each dataset.
It is evident that Qwen 2.5 14B was the most dominant model on both datasets, significantly outperforming others across all prompting methods.


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{clc}
      \hline
      \textbf{Rank} &   \textbf{Model}          &   \textbf{Score} \\
      \hline
      1	            &   Qwen 2.5~--~14B         &   124.164        \\
      2	            &   Mistral Nemo~--~12B     &   59.442         \\
      3	            &   Gemma 2~--~27B          &   48.421         \\
      4	            &   Gemma 2~--~9B           &   47.904         \\
      5	            &   Qwen 2~--~7B            &   31.927         \\
      6	            &   LLaMA 3.1~--~8B         &   23.591         \\
      7	            &   LLaMA 3~--~8B           &   21.535         \\
      8	            &   Phi 3.5 Mini~--~3.8B    &   18.869         \\
      9	            &   Phi 3 Medium~--~14B     &   16.877         \\
      10	        &   Gemma 2~-~2B            &   15.467         \\
      11            &   LLaMA 3.2~-~3B          &   14.603         \\
      12            &   LLaMA 3.2~-~1B          &   11.620         \\
      \hline
    \end{tabular}
    \caption{Model ranking based on RRF on SciCite.}\label{tab:rrf_evaluation_scicite}
\end{table}

\subsubsection{Ranked Evaluation}\label{sec:ranked_evaluation}

While the Best-Performer Evaluation provided insights into the most dominant models across configurations, it lacked the ability to capture nuances among high-performing models. For instance, a model that consistently ranked second in multiple configurations would not be reflected in that approach.
To address this limitation, we adopted a ranking methodology inspired by Reciprocal Rank Fusion (RRF)~\citep{cormack2009reciprocal}. 
This approach allowed us to evaluate all $12$ models considering their relative performance across different configurations.

For each experimental configuration \(c \in C\),  models \(M =\{ m_1, m_2, \dots, m_{12} \}\) were ranked based on their F1-scores~--~in case of a tie, Accuracy was used to determine the rank.
Each model was then assigned a score based on its rank, where the score \(S(m_{k}, c)\) for the \(k\)-th ranked model in configuration \(c\) was defined as \(S(m_{k}, c) = \frac{1}{k}\). 
This score is inversely proportional to the rank, meaning that higher-ranked models (lower-\(k\)) receive higher scores, while lower-ranked models (higher-\(k\)) receive lower scores. 
To calculate the overall ranking score for each model \(m\), we aggregated its scores across all configurations \(c \in C\):
\begin{equation}
    Rank(m) = \sum_{c \in C} S(m_{k}, c)
\end{equation}
% The results of this ranked evaluation are presented in 
Tables~\ref{tab:rrf_evaluation_scicite} and~\ref{tab:rrf_evaluation_aclarc} present the results of this ranked evaluation. The ranked evaluation aligns with the Best-Performer Evaluation in identifying Qwen 2.5 14B as the most dominant model. However, it also offers insights into the relative performance of other models, 
highlighting distinctions among high performers. 
% \ser{This additional layer of analysis offers a more nuanced understanding of model performance across the experimental space.}{}

\subsection{Parameter Performance Analysis}\label{sec:parameter_performance}


% With approximately 2K experiments conducted across 165 configurations and two datasets, identifying the most impactful parameters influencing model performance was a challenging task. To address this, we employed two complementary methods: \emph{Quantile-Based Analysis} and \emph{Analysis with Statistical Tests}.

% The Quantile-Based Analysis focused on identifying trends by examining the distribution of parameter settings in the top 5\% of configurations based on F1-scores. Meanwhile, for the Statistical Tests we used the Chi-Square Test of Independence and the Kruskal-Wallis Test to rigorously evaluate the relationship between parameter settings and F1-scores. Together, these methods allowed us to identify parameter settings that consistently led to better performance.

% The insights gained from this analysis informed the identification of optimal configurations, discussed in Section~\ref{sec:optimal_configurations}, and helped narrow the experimental space for fine-tuning models, as explored in Section~\ref{sec:fine_tuning}.

In this section, we examine how different parameter configurations drive model performance.
% \ser{With approximately 2,000 experiments conducted across 165 configurations and two datasets, identifying the most impactful parameters influencing model performance was a challenging task.}
Identifying the most impactful parameters was not a straightforward task due to the complexity of the search space and the variety of configurations tested.
To address this, we conducted a Quantile-Based Analysis, focusing on the top $5$\% of configurations based on F1-scores. 
Examining the distribution of parameter settings within this high-performing subset, allows us to 
% \ser{isolate the configurations that achieved the best results. By doing so, we aimed to}{}
uncover trends and patterns that could guide the selection of optimal configurations. 

For this analysis, we focused exclusively on the SciCite dataset, and the insights gained from it were then used to limit the range of experiments conducted on the ACL-ARC dataset.
% \ser{The}{A detailed} analysis of the parameter performance on the ACL-ARC dataset is available in \ser{the supplementary material}{\color{red}Appendix XXX}. 
The findings from this analysis can be seen in Table~\ref{tab:parameter_performance} and are summarized below.


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{clc}
      \hline
      \textbf{Rank} &   \textbf{Model}          &   \textbf{Score} \\
      \hline
      1	            &   Qwen 2.5~--~14B         &   26.000         \\
      2	            &   Gemma 2~--~27B          &   9.747          \\
      3	            &   Gemma 2~--~9B           &   8.498          \\
      4	            &   Qwen 2~--~7B            &   7.554          \\
      5	            &   LLaMA 3.1~--~8B         &   6.437          \\
      6	            &   Mistral Nemo~--~12B     &   6.000          \\
      7	            &   Phi 3.5 Mini~--~3.8B    &   5.268          \\
      8	            &   Phi 3 Medium~-~14B      &   3.399          \\
      9             &   LLaMA 3.2~-~3B          &   3.281          \\
      10            &   LLaMA 3.2~-~1B          &   2.718          \\
      11	        &   Gemma 2~-~2B            &   2.506          \\
      12            &   LLaMA 3~-~8B            &   2.396          \\
      \hline
    \end{tabular}
    \caption{Model ranking based on RRF on ACL-ARC.}\label{tab:rrf_evaluation_aclarc}
\end{table}

We discern that few-shot prompting was the most common prompting method among the top-performing configurations, accounting for nearly 45\% of the results. 
Many-shot prompting followed closely with 34.83\%, while one-shot and zero-shot prompting were significantly less represented, with only 16.85\% and 3.37\%, respectively. 
This suggests that providing several examples during prompting generally leads to better performance, with the optimal achieved at five examples (few-shot), followed by a slight decrease at larger numbers of examples.

In contrast, the temperature parameter exhibited a relatively even distribution among the top-performing configurations, with $0.0$ and $0.2$ being the most frequent settings (30.34\% each). Higher temperatures, such as $1.0$ and $0.5$, 
were less common, suggesting that lower temperatures contribute to more consistent and deterministic outputs, 
which align better with the task requirements.

Similarly, the system prompt parameter was fairly evenly distributed among the top configurations, with SP3 being slightly more frequent (35.96\%), followed by SP2 (32.58\%), and SP1 (31.46\%). 
While all system prompts contributed to high performance, 
these results suggest that SP3 has offered a slight advantage in effectiveness.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{@{}lccc@{}}
      \hline
      \textbf{Parameter}                    &  \textbf{Setting}     & \textbf{Count} & \textbf{Percent}     \\
      \hline
      \multirow{4}{*}{Prompting Method}     & Few-shot              & 40             & 44.94\%              \\
                                            & Many-shot             & 31             & 34.83\%              \\
                                            & One-shot              & 15             & 16.85\%              \\
                                            & Zero-shot             & 3              & 3.37\%               \\ \midrule
      \multirow{4}{*}{Temperature}          & 0.0                   & 27             & 30.34\%              \\
                                            & 0.2                   & 27             & 30.34\%              \\
                                            & 1.0                   & 18             & 20.22\%              \\
                                            & 0.5                   & 17             & 19.10\%              \\ \midrule
      \multirow{3}{*}{System Prompt}        & SP3                     & 32             & 35.96\%              \\
                                            & SP2                     & 29             & 32.58\%              \\
                                            & SP1                     & 28             & 31.46\%              \\ \midrule
      \multirow{2}{*}{Query Template}       & Multiple-Choice       & 83             & 93.26\%              \\
                                            & Simple                & 6              & 6.74\%               \\ \midrule
      \multirow{2}{*}{Examples Method}      & Roles                 & 44             & 51.16\%              \\
                                            & Inline                & 42             & 48.84\%              \\ 
      \hline
    \end{tabular}
    \caption{Parameter Performance Analysis of Top 5\% Configurations on SciCite (sorted by percentage).}\label{tab:parameter_performance}
\end{table}


% \begin{table*}[t]
%     \centering
%     \footnotesize
%     \begin{tabular}{ccccccc}
%         \hline
%         \textbf{Dataset}            & \textbf{Precision}    & \textbf{Recall}   & \textbf{F1-Score}     & \textbf{Accuracy}     & \textbf{EM}   & \textbf{T} \\
%         \hline
%         \multirow{4}{*}{SciCite}    & 0.783                 & 0.786             & 0.783                 & 0.807                 & 2             & 0.2 \\
%                                     & 0.781                 & 0.786             & 0.781                 & 0.805                 & 2             & 0.0 \\
%                                     & 0.751                 & 0.801             & 0.768                 & 0.787                 & 1             & 0.2 \\
%                                     & 0.748                 & 0.798             & 0.765                 & 0.785                 & 1             & 0.0 \\
%         \hline
%         \multirow{4}{*}{ACL-ARC}    & 0.597                 & 0.645             & 0.602                 & 0.655                 & 2             & 0.0 \\
%                                     & 0.597                 & 0.645             & 0.601                 & 0.655                 & 2             & 0.2 \\
%                                     & 0.630                 & 0.616             & 0.588                 & 0.647                 & 1             & 0.0 \\
%                                     & 0.623                 & 0.610             & 0.580                 & 0.640                 & 1             & 0.2 \\
%         \hline
%     \end{tabular}
%     \caption{Qwen 2.5 14B Performance on Optimal Configurations for SciCite and ACL-ARC Datasets. Prompting Method: \textbf{few-shot}, Query Template: \textbf{2}, Prompt Version: \textbf{3} 
%     \emph{\color{red} (\textbf{Paris:} Might remove)}}\label{tab:optimal_configurations}
% \end{table*}

The query template parameter showed a strong preference for multiple-choice templates, which appeared in 93.26\% of the top-performing configurations. 
This overwhelming dominance suggests that multiple-choice queries are significantly more effective for the evaluated task, likely due to their structured.

The examples method parameter showed a near-equal split between roles (51.16\%) and inline (48.84\%), indicating that both approaches are effective, with no clear advantage for either method among the top-performing configurations.

% The key findings from the Quantile-Based Analysis are summarized below:
% \begin{itemize}
%     \item Few-shot prompting emerged as the most effective prompting method, followed by many-shot prompting, while one-shot and zero-shot prompting were less effective.
%     \item Lower temperature settings (0.0 and 0.2) were more common among high-performing configurations, suggesting that deterministic outputs are preferable for the evaluated tasks.
%     \item While all system prompts contributed to high performance, Prompt 3 was slightly more frequent in the top 5\%.
%     \item Multiple-choice query templates were overwhelmingly dominant, indicating their strong alignment with the task requirements.
%     \item Both examples methods (roles and inline) were nearly equally effective, with no significant preference observed.
% \end{itemize}

These findings offer valuable insights into the parameter settings that consistently enhance performance. To verify this analysis, we also performed a Chi-Square statistical test, outlined in Appendix~\ref{sec:appendix-b-statistical-test}.

% Paris: Could we put this in an appendix?
% \subsubsection{Statistical Tests}\label{sec:statistical_tests}
% To complement the Quantile-Based Analysis, we also conducted an analysis with Statistical Tests to evaluate the relationship between parameter settings and F1-scores. This approach allowed us to determine whether specific parameter settings were significantly associated with performance outcomes. Two statistical tests were employed: the \emph{Chi-Square Test of Independence} and the \emph{Kruskal-Wallis Test}.

% The Chi-Square Test of Independence was used to assess whether the distribution of parameter settings was independent of performance outcomes. A significant result (p < 0.05) indicates that certain parameter settings are disproportionately represented among high-performing configurations.

% The Kruskal-Wallis Test was used to evaluate whether differences in parameter settings led to statistically significant variations in F1-scores.

% The Statistical Tests revealed several significant relationships between parameter settings and model performance. Both the Chi-Square and Kruskal-Wallis tests indicated that the prompting method had a significant impact on performance (p < 0.01), confirming that certain methods, such as few-shot or many-shot prompting, were more effective. For temperature, the Chi-Square test did not find significant differences (p = 0.23), but the Kruskal-Wallis test suggested a significant effect (p = 0.012), indicating that specific temperature settings, like 0.0 or 0.2, may still influence performance outcomes. The system prompt parameter was strongly associated with performance, with both tests showing high significance (p < 0.001). Similarly, the query template parameter showed highly significant results (p < 0.001) in both tests, confirming that multiple-choice templates were strongly linked to better performance. Interestingly, while the Chi-Square test found no significant relationship for the examples method (p = 0.91), the Kruskal-Wallis test revealed a significant effect (p < 0.001), suggesting that specific approaches, such as roles, may be more effective for certain tasks. Overall, these results emphasize the importance of prompting method, system prompt, and query template, while highlighting nuanced effects for temperature and examples method that may warrant further exploration.

% \subsection{Outcomes and Optimal Configurations}\label{sec:optimal_configurations}

% \emph{\color{red} \textbf{Paris:} All this could be removed if we lack space - It's a summary with nothing new to add.}

% \emph{\color{red} \textbf{Paris:} Might remove this. I wanted to have this as a summary of the two analyses with an emphasis on the table, but the table feels like cherry picking (statistically optimal parameters don't mean that they are THE most optimal configurations - also, we are going to show the top-50 configs in the appendix). Without the table, the entire section feels redundant.}

% From our Model Performance Analysis (see Section~\ref{sec:best_models}), 
% we identified Qwen 2.5 14B as the best-performing model across the all parameter configurations. 
% This model consistently outperformed others in terms of F1-scores,
% showing significant potential 
% for the task of citation intent classification.

% The Parameter Performance Analysis (Section~\ref{sec:parameter_performance}) 
% further revealed the optimal configurations that contributed to this performance. 
% Few-shot prompting emerged as the most effective prompting method, while lower temperature settings (0.0 and 0.2) were more commonly associated with high-performing configurations. Multiple-choice query templates dominated the top-performing setups, and both roles and inline examples were nearly equally effective.

% Table~\ref{tab:optimal_configurations} summarizes the evaluation metrics for the best-performing configurations of Qwen 2.5 14B on the SciCite dataset.

% These results provide a strong foundation for further experimentation. On the next section, we outline our process for fine-tuning Qwen 2.5 14B on our task.

% scicite
% Precision	Recall	F1-Score	Accuracy	Method	Prompt_Version	Query_Template	Examples_Method	Temperature
% 0.783	0.786	0.783	0.807	few-shot	2.5	2	2	0.2
% 0.781	0.786	0.781	0.805	few-shot	2.5	2	2	0.0
% 0.751	0.801	0.768	0.787	few-shot	2.5	2	1	0.2
% 0.748	0.798	0.765	0.785	few-shot	2.5	2	1	0.0

% acl-arc
% Precision	Recall	F1-Score	Accuracy	Method	Prompt_Version	Query_Template	Examples_Method	Temperature
% 0.597	0.645	0.602	0.655	few-shot	2.5	2	2	0.0
% 0.597	0.645	0.601	0.655	few-shot	2.5	2	2	0.2
% 0.630	0.616	0.588	0.647	few-shot	2.5	2	1	0.0
% 0.623	0.610	0.580	0.640	few-shot	2.5	2	1	0.2


