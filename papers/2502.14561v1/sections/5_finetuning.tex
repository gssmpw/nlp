% \newpage
\section{Fine-tuning}\label{sec:fine_tuning}

In this section, we investigate the impact of fine-tuning in the the performance of the instruction-tuned Qwen 2.5 14B model on the task of citation intent classification. 
% \ser{While achieving state-of-the-art results was a secondary goal, the}{Our} primary focus \ser{of this study was}{is} to demonstrate the viability of large language models for this task and explore their potential to generalize to domain-specific challenges.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{lc}
        \hline
        \textbf{Parameter}           & \textbf{Value} \\
        \hline
        Learning Rate                & 5e-5           \\
        Epochs                       & 10             \\
        Batch Size                   & 16             \\
        Cutoff Length                & 512            \\
        Optimizer                    & AdamW          \\
        Compute Type                 & fp16           \\
        Warmup Steps                 & 500            \\
        DeepSpeed Offload            & Enabled        \\
        DeepSpeed Stage              & 3              \\
        LoRA Rank                    & 8              \\
        LoRA Alpha                   & 16             \\
        LoRA Dropout                 & 0.1            \\
        \hline
    \end{tabular}
    \caption{Training Parameters for Fine-Tuning Qwen 2.5 14B.}\label{tab:training_parameters}
\end{table}

\subsection{Training Configuration}

For this experiment, we used the SciCite and ACL-ARC datasets, which were converted into the Alpaca format~\citep{alpaca}. This format includes a system prompt, an instruction, the citing sentence, and the true label. 
% An example of our training data in the Alpaca format is included in the appendix, with the full training dataset available in the supplementary material. 
The use of Supervised Fine-Tuning (SFT) was motivated by its ability to adapt pre-trained models to specific tasks using minimal labeled data, making it an effective approach for citation intent classification.

To fine-tune the Qwen 2.5 14B Instruct model, we used LLaMA-Factory~\citep{llamafactory} on an AWS g6e.12xlarge EC2 instance equipped with 4 NVIDIA L40S GPUs, providing a combined GPU memory of 192GB. The training process was conducted using fp16 mixed precision to optimize memory usage and speed. To prevent memory issues during fine-tuning, we employed DeepSpeed ZeRO Stage 3 Offload~\citep{deepspeed}, which enabled efficient memory management by offloading optimizer states and gradients to the CPU. The training parameters included a learning rate of 5e-5, a batch size of 16, and 10 epochs. The model was optimized using AdamW, with a cutoff length of 512 tokens for input sequences. 

To optimize the fine-tuning process, we employed Low-Rank Adaptation (LoRA), a widely used parameter-efficient fine-tuning (PEFT) method~\citep{lora, peft-survey}. LoRA enables efficient adaptation of large language models by freezing the pre-trained model weights and introducing trainable low-rank matrices into the Transformer layers, significantly reducing the number of trainable parameters while maintaining performance. For this experiment, we configured LoRA with a rank of 8, alpha of 16, and a dropout of 0.1, which allowed us to fine-tune the model effectively without exceeding memory constraints. The outlined training parameters are also summarized in Table~\ref{tab:training_parameters}.


\subsection{Results}
% {\color{red} \textbf{Paris:} Discuss the increase of performance in both datasets. Maybe add a plot of the changes per prompting method, since there are large differences in performance.}

After fine-tuning, we evaluated the new model using the same prompting methods as in Section~\ref{sec:in_context_learning} (i.e.,~zero-shot, one-shot, few-shot, and many-shot) to draw comparisons with the baseline Qwen 2.5 14B Instruct model. Although in the previous evaluation we only used 8-bit quantized models (Q8) due to the extensiveness of our experiment space, in this experiment, we also evaluate the 16-bit floating-point (FP16) version of the model to examine the impact of bit precision on performance.

\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model}      & \textbf{SciCite}  & \textbf{ACL-ARC}  \\
        \midrule
        Instruct Q8         & 78.33             & 61.04             \\
        Instruct FP16       & 80.41             & 61.73             \\
        Fine-tuned Q8        & 86.47             & \textbf{68.48}    \\
        Fine-tuned FP16      & \textbf{86.84}    & 67.73             \\
        \bottomrule
        \end{tabular}
        \caption{F1-Score Performance of Qwen 2.5~--~14B Instruct and Fine-tuned variants on the SciCite and ACL-ARC datasets.}\label{tab:results-simple}
\end{table}

%% SHORT VERSION %%
The results in Table~\ref{tab:results-simple} show that fine-tuning improved performance on both SciCite and ACL-ARC. On SciCite, the fine-tuned FP16 model achieved an F1-score of 86.84, an 8\% improvement over the instruction-tuned baseline, while the fine-tuned Q8 model achieved a comparable score of 86.47. On ACL-ARC, the fine-tuned Q8 model achieved the highest F1-score of 68.48, outperforming the instruction-tuned baseline by nearly 11\%.

Precision settings showed minor differences, with FP16 models generally outperforming Q8 on SciCite, while Q8 slightly outperformed FP16 on ACL-ARC. This suggests that while FP16 precision is generally advantageous, Q8 remains competitive and may offer robustness for tasks with more granular classification schemes, such as ACL-ARC's 6-class setup.

Table~\ref{tab:results-pm} highlights the impact of fine-tuning across prompting methods. On SciCite, fine-tuning led to consistent improvements, particularly in zero-shot and one-shot settings, where the fine-tuned Q8 model improved by nearly 10\% over the instruction-tuned baseline. In few-shot and many-shot setups, performance approached saturation, with the FP16 model achieving the highest F1-scores of 86.84 and 85.79, respectively. On ACL-ARC, fine-tuned models showed similar trends, with the Q8 model achieving the highest F1-scores in one-shot (68.48) and few-shot (64.35) setups, outperforming FP16 in most cases.
These results demonstrate that fine-tuning enhances generalization in low-context scenarios while effectively leveraging additional context during inference.
%% END SHORT VERSION %%

%% LONG VERSION %%
% The results presented in Table~\ref{tab:results-simple} demonstrate that fine-tuning significantly improved performance across both SciCite and ACL-ARC. 
% On the SciCite dataset, the fine-tuned FP16 model achieved an F1-score of 86.84, compared to 80.41 for the instruction-tuned FP16 baseline, representing an improvement of approximately 8\%. Similarly, the fine-tuned Q8 model achieved a comparable score of 86.47, highlighting the effectiveness of fine-tuning in adapting the model to our task. On the more challenging ACL-ARC dataset, the fine-tuned Q8 model achieved the highest F1-score of 68.48, outperforming the instruction-tuned FP16 model's score of 61.73 by nearly 11\%.

% In terms of precision settings, FP16 models generally outperformed their Q8 counterparts, albeit with relatively small differences. For instance, on SciCite, the fine-tuned FP16 model slightly outperformed the Q8 variant, achieving an F1-score of 86.84 versus 86.47. However, on ACL-ARC, the fine-tuned Q8 model slightly outperformed the FP16 model, achieving an F1-score of 68.48 versus 67.73. These results suggest that while FP16 precision generally provides an advantage, Q8 precision remains competitive and, in some cases, performs equally well.

% Table~\ref{tab:results-pm} provides a breakdown of the model's performance across the different prompting methods, further illustrating the impact of fine-tuning. On SciCite, fine-tuning led to consistent improvements across all prompting setups, with the greatest gains observed in the zero-shot and one-shot settings. Specifically, in the zero-shot setting, the fine-tuned Q8 model achieved an F1-score of 84.84, compared to 75.26 for the instruction-tuned Q8 model, representing a relative improvement of nearly 10\%. 
% Similar trends were observed for one-shot prompting, where the fine-tuned models also significantly outperformed the instruction-tuned baselines. In the few-shot and many-shot setups, the fine-tuned models achieved near-saturation in performance, with the FP16 model achieving the highest F1-scores of 86.84 (few-shot) and 85.79 (many-shot). These results indicate that fine-tuning not only enhances the model's ability to generalize in scenarios with minimal context but also allows it to effectively leverage additional context provided during inference.

% On ACL-ARC, for the zero-shot setting, the fine-tuned FP16 model achieved an F1-score of 60.00, compared to 50.17 for the instruction-tuned FP16 model, an improvement of nearly 10\%. In the one-shot and few-shot setups, the fine-tuned Q8 model performed exceptionally well, achieving the highest F1-scores of 68.48 and 64.35, respectively. Interestingly, the fine-tuned Q8 model consistently outperformed its FP16 counterpart across most prompting setups on this dataset, suggesting that Q8 precision may provide additional robustness for classification tasks with highly granular classification schemes, such as the 6-class setup of ACL-ARC compared to the 3-class scheme of SciCite.

%% END LONG VERSION %%

% Fine-tuning proved to be highly effective across both datasets and all prompting methods, with particularly strong improvements in the zero-shot and one-shot setups. The results show that fine-tuning enhances the model's ability to generalize to unseen examples with minimal context, while also enabling it to make better use of additional context in few-shot and many-shot scenarios. Additionally, the minimal performance trade-offs observed with Q8 precision suggest that it remains a viable option for environments where computational efficiency is a priority. Overall, the fine-tuned Qwen 2.5 14B model demonstrated substantial improvements over the instruction-tuned baseline, highlighting the value of task-specific adaptation for citation intent classification.

% \emph{\color {red} \textbf{Paris:} \textbf{1}~--~We should discuss the framing of our results.}
%  \textbf{2}~--~I have results for the Q8 instruction-tuned Qwen 2.5, and for the Q8 and BF16 finetuned. I don't have results for the BF16 instruction-tuned. Maybe we should calculate these as well, as we observe a slight impovement from Q8 to BF16 in the finetuned model}

\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{clllll}
      \hline
      \multirow{2}{*}{\textbf{Dataset}}         & \multirow{2}{*}{\textbf{Model}}    & \textbf{Zero-}   & \textbf{One-}   & \textbf{Few-}   & \textbf{Many-}  \\
                                &                   & \textbf{Shot}   & \textbf{Shot}        & \textbf{Shot}             & \textbf{Shot}  \\

      \hline
      \multirow{4}{*}{SciCite}  & Instruct Q8       & 75.26             & 76.29             & 78.33             & 78.32             \\
                                & Instruct FP16     & 75.38             & 77.22             & 80.41             & 78.94             \\
                                & Fine-tuned Q8      & \textbf{84.84}    & \textbf{85.46}    & 86.47             & 85.62             \\
                                & Fine-tuned FP16    & 84.49             & 85.38             & \textbf{86.84}    & \textbf{85.79}    \\
      \hline
      \multirow{4}{*}{ACL-ARC}  & Instruct Q8       & 50.02             & 60.02             & 61.04             & 59.75             \\
                                & Instruct FP16     & 50.17             & 61.73             & 59.04             & 54.46             \\
                                & Fine-tuned Q8      & 59.40             & \textbf{68.48}    & \textbf{64.35}    & \textbf{65.43}    \\
                                & Fine-tuned FP16    & \textbf{60.00}    & 67.73             & 64.34             & 64.62             \\
      \hline
    \end{tabular}
    \caption{F1-Score Performance of Qwen 2.5 â€“ 14B Instruct and Fine-tuned variants on the SciCite and ACL-ARC datasets divided by prompting method.}\label{tab:results-pm}
\end{table}

\subsection{Discussion}

The primary goal of this work was to investigate the capability of large language models to perform citation intent classification, rather than to compete directly with state-of-the-art methods. Nonetheless, it is worth noting that the fine-tuned model achieved performance on SciCite within less than a 3\% margin of the best-reported results in the literature, surpassing most of the existing approaches (see Table~\ref{tab:literature-f1} - Section~\ref{sec:related_work}). This demonstrates that LLMs can approach the performance of specialized models, even without task-specific architectures or optimization.

We consider a key advantage of LLMs to be their ease of use and deployment. Tools such as LM Studio and Ollama\footnote{\url{https://ollama.com/}} allow models like those outlined in this paper to be deployed locally with zero technical expertise, making them accessible to users without a computer science background. 
In addition to their accessibility, LLMs offer significant adaptability. Unlike traditional methods, which often require complex pretraining or domain-specific tuning, LLMs can be fine-tuned for a wide range of scientometric tasks, such as citation recommendation, paper summarization, or trend analysis, without the need for bespoke architectures. Furthermore, LLMs can scale effectively to new scientific domains or datasets, requiring only small amounts of task-specific data to adapt to underexplored scientific fields. For instance, our fine-tuned models were trained only on the several thousand citing sentences provided by our task-specific datasets, whereas SciBERT has been trained on millions of scientific papers.

% We consider a key advantage of LLMs to be their ease of use and deployment. Unlike traditional methods, which are often narrowly optimized for specific tasks, LLMs offer a flexible solution that can be fine-tuned for a variety of scientometric tasks with minimal effort. For example, the same fine-tuned LLM used for citation intent classification can also be adapted for tasks such as citation recommendation, paper summarization, or trend analysis in research topics, eliminating the need for bespoke architectures for each task. Furthermore, tools such as LM Studio and Ollama allow models like those outlined in this paper to be deployed locally with zero technical expertise, making them accessible to users with limited technical knowledge.

% We also find that fine-tuned LLMs are particularly well-suited for scalability to new domains or datasets. Traditional methods often require significant retraining or re-engineering to adapt to new datasets or emerging fields of research. In contrast, LLMs can be fine-tuned on a small subset of relevant data, making them particularly effective for analyzing citation intent in underexplored scientific domains This scalability, combined with their versatility, positions LLMs as a practical and efficient choice for researchers working across a wide range of challenges in scientometrics.

The promising results observed in this study suggest that the performance of LLMs in citation intent classification can be further improved with techniques such as chain-of-thought prompting and reasoning-focused models. These methods could enhance the models' ability to better distinguish subtle differences in citation intent, refining their predictions and improving overall classification accuracy.

% {\color{red} \textbf{Paris:} Discuss once again WHY we did this (the investigation) and that the LLM reaches a near-identical level of performance in scicite compared to reported literature. Mention ease of use and deployment due to the ubiquity of local LLM options as a large advantage. Think about other advantages. Mention that there is a lot of room for improvement.}
