\section{Conclusions}

This study explored the capabilities of open LLMs for citation intent classification, focusing on their performance with and without task-specific training. We found that while instruction-tuned LLMs perform reasonably well through in-context learning, supervised fine-tuning with minimal data leads to significant improvements. Additionally, our experiments on prompting-related parameters offer practical insights into optimizing model performance. To support further research, we provide our evaluation platform and models to the research community.