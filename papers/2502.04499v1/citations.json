[
  {
    "index": 0,
    "papers": [
      {
        "key": "hintonkd",
        "author": "Geoffrey Hinton and Oriol Vinyals and Jeff Dean",
        "title": "Distilling the Knowledge in a Neural Network"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "engine",
        "author": "Tu, Lifu  and\nPang, Richard Yuanzhe  and\nWiseman, Sam  and\nGimpel, Kevin",
        "title": "{ENGINE}: Energy-Based Inference Networks for Non-Autoregressive Machine Translation"
      },
      {
        "key": "minillm",
        "author": "Yuxian Gu and Li Dong and Furu Wei and Minlie Huang",
        "title": "Mini{LLM}: Knowledge Distillation of Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mode-seeking",
        "author": "Bishop, Christopher M.",
        "title": "Pattern recognition and machine learning (information science and statistics)"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fdistill",
        "author": "Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili",
        "title": "$f$-{{divergence minimization}} for {{sequence-level knowledge distillation}}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hao2022teacher",
        "author": "Hao, Yongchang and Liu, Yuxin and Mou, Lili",
        "title": "Teacher Forcing Recovers Reward Functions for Text Generation"
      },
      {
        "key": "llmr",
        "author": "Li, Dongheng and Hao, Yongchang and Mou, Lili",
        "title": "{{LLMR}}: {{Knowledge distillation}} with a {{large language model-induced reward}}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "exp-bias",
        "author": "Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam",
        "title": "Scheduled sampling for sequence prediction with recurrent Neural networks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "patientkd",
        "author": "Sun, Siqi  and\nCheng, Yu  and\nGan, Zhe  and\nLiu, Jingjing",
        "title": "Patient Knowledge Distillation for {BERT} Model Compression"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "patientkd",
        "author": "Sun, Siqi  and\nCheng, Yu  and\nGan, Zhe  and\nLiu, Jingjing",
        "title": "Patient Knowledge Distillation for {BERT} Model Compression"
      },
      {
        "key": "distilBERT",
        "author": "Sanh, V",
        "title": "{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tinybert",
        "author": "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
        "title": "{{TinyBERT}}: {{Distilling BERT}} for {{natural language understanding}}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "minilmv2",
        "author": "Wang, Wenhui and Bao, Hangbo and Huang, Shaohan and Dong, Li and Wei, Furu",
        "title": "{MiniLMv2}: {M}ulti-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rkd",
        "author": "Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu",
        "title": "Relational Knowledge Distillation"
      },
      {
        "key": "fcd",
        "author": "Kun Huang and Xin Guo and Meng Wang",
        "title": "Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "patientkd",
        "author": "Sun, Siqi  and\nCheng, Yu  and\nGan, Zhe  and\nLiu, Jingjing",
        "title": "Patient Knowledge Distillation for {BERT} Model Compression"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "tinybert",
        "author": "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
        "title": "{{TinyBERT}}: {{Distilling BERT}} for {{natural language understanding}}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "alpkd",
        "author": "Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun",
        "title": "{ALP-KD}: Attention-Based Layer Projection for Knowledge Distillation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "railkd",
        "author": "Haidar, Md Akmal and Anchuri, Nithin and Rezagholizadeh, Mehdi and Ghaddar, Abbas and Langlais, Philippe and Poupart, Pascal",
        "title": "{{RAIL-KD}}: {{Random intermediate layer mapping}} for {{knowledge distillation}}"
      }
    ]
  }
]