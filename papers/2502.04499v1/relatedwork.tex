\section{Background and Related Work}
Knowledge Distillation (KD) is a method of transferring rich knowledge contained in a teacher model to a student model. To inform the student of the task, it is essential to match the student's and teacher's predictions.  For the teacher distribution $p$ and student distribution $q_{\bm\theta_s}$, \citet{hintonkd} suggest minimizing the Kullback--Leibler (KL) divergence between them: 
\begin{equation}
    \mathcal{L}_{\text{KL}}(\bm\theta_s) =  
    \mathbb{E}_{\mathrm y\sim p(\mathrm y| \mathrm x)}\big[
        \log{
            \tfrac{p(\mathrm y | \mathrm x)}
            {q_{\bm\theta_s}(\mathrm y | \mathrm x)}
        }
        \big]
\end{equation}
where $\mathrm x$ represents the input, and the output $\mathrm y$ (conditioned on $\mathrm x$) is sampled from $p$. The student's parameters $\bm\theta_s$ are optimized, whereas the teacher's parameters are frozen. 

Other than minimizing KL, different prediction matching approaches have been proposed.  When the teacher distribution is diverse, for example, the reverse KL divergence~\cite{engine,minillm} is used due to its mode-seeking behavior, i.e., the student only focuses on one of the high-probability regions in the teacher distribution~\cite{mode-seeking}. \citet{fdistill} propose an $f$-divergence KD framework, where symemtric divergences (such as Jensen--Shannon and total variation distance) provide a balance between mode averaging and mode seeking. Reinforcement learning can also be applied to KD~\cite{hao2022teacher,llmr}, which makes the student aware of its prefix and addresses the exposure bias problem~\cite{exp-bias}.

Regarding intermediate-layer matching, it distills the teacher's hidden states, thus providing additional supervisory signals to the student~\cite{patientkd}. Let $\mathcal M=\{(\varsigma_i, \tau_i)\}_{i}$ be the mapping between student and teacher layers, i.e., the $\varsigma_i$th layer of the student is mapped to the $\tau_i$th layer of the teacher. Intermediate-layer matching typically penalizes the distance between the matched layers, given by
\begin{equation}
    \mathcal L_{\text{hid}}( \bm\theta_s, \{\boldsymbol A_i\}_i) = \sum\nolimits_{i} \operatorname{dist}(\boldsymbol A_i \bm h_{\varsigma_i}^{(s)}, \boldsymbol h_{\tau_i}^{(t)}) 
\end{equation}
where $\operatorname{dist}$ is a distance metric (such as mean squared error). The trainable linear operator $\bm A_i$ transformers the student's hidden state $\bm h_{\varsigma_i}^{(s)}$ to the space of the teacher's hidden state $\boldsymbol h_{\tau_i}^{(t)}$, when their dimensions do not match. Otherwise, $\bm A_i$ may be an identity matrix.

Intermediate-layer matching can be applied to different types of representations.  Traditionally, this is achieved by matching the student's and teacher's activations~\cite{patientkd, distilBERT}. Other studies match attention logits~\cite{tinybert}, attention's query--key--value relations~\cite{minilmv2}, and cross-sample relations~\cite{rkd, fcd}. In our work, we focus on matching activations because it is the most fundamental approach in intermediate-layer matching. 

Various layer-selection strategies have been proposed for matching a shallow student to a deep teacher. \citet{patientkd} and \citet{tinybert} suggest mapping evenly spaced teacher layers to the student. \citet{alpkd} match each student layer to
 a weighted combination of all teacher layers to retain more knowledge. \citet{railkd} randomly reselect a sequence of teacher layers to match with the student after each epoch, so that the student is exposed to different teacher layers.

Overall, different layer-selection strategies perform unexpectedly similarly (as mentioned in \S\ref{sec:intro}), which inspires our work. We observe an intriguing phenomenon that the layer-selection strategy does not matter (much), even with unusual mappings such as reverse order; we also provide an interpretation for this phenomenon. 

\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|ll|l|r|cccc|cc|}
\hline
\multicolumn{2}{|l|}{} &  & \multicolumn{1}{c|}{} & \multicolumn{4}{c|}{Classification Tasks} & \multicolumn{2}{c|}{Generation Tasks} \\ \cline{5-10} 
\multicolumn{2}{|l|}{\multirow{-2}{*}{Model}} & \multirow{-2}{*}{\begin{tabular}[c]{@{}l@{}}Layer \\ Matching\end{tabular}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\#}} & \begin{tabular}[c]{@{}c@{}}MNLI-m/mm\\ Acc\end{tabular} & \begin{tabular}[c]{@{}c@{}}QQP\\ Acc/F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}QNLI\\ Acc\end{tabular} & \begin{tabular}[c]{@{}c@{}}SST-2\\ Acc\end{tabular} & \begin{tabular}[c]{@{}c@{}}DART\\ BLEU\end{tabular} & \begin{tabular}[c]{@{}c@{}}WMT16 En--Ro\\ BLEU\end{tabular} \\ \hline
\multicolumn{1}{|l|}{} & Previous work & -- & 1 & 84.6/83.4 & \ \ \ --  \    /71.2 & 90.5 & 93.5 & 48.56 & 25.82 \\
\multicolumn{1}{|l|}{\multirow{-2}{*}{Teacher}} & Our replication & -- & 2 & 84.5/84.1 & 89.0/71.4 & 90.8 & 93.1 & 48.80 & 25.90 \\ \hline
\multicolumn{1}{|l|}{} &  & None & 3 & 63.2/63.6 & 81.5/56.4 & 61.2 & 81.1 & {\color[HTML]{000000} 38.76} & 8.02 \\
\multicolumn{1}{|l|}{} &  & Forward & 4 & 72.5/72.0 & 83.9/61.3 & 64.7 & 85.1 & {\color[HTML]{000000} 32.64} & 18.13 \\
\multicolumn{1}{|l|}{} &  & Reverse & 5 & 69.3/68.9 & 84.3/61.8 & 65.2 & 83.3 & {\color[HTML]{000000} 33.12} & 17.15 \\
\multicolumn{1}{|l|}{} &  & All-to-one & 6 & 74.0/73.8 & 83.4/60.2 & 65.0 & 85.4 & {\color[HTML]{000000} 33.86} & 17.16 \\
\multicolumn{1}{|l|}{} & \multirow{-5}{*}{\begin{tabular}[c]{@{}l@{}}Randomly \\ initialized\end{tabular}} & Random & 7 & 71.2/71.2 & 82.4/58.8 & 64.4 & 82.9 & {\color[HTML]{000000} 32.67} & 16.70 \\ \cline{2-10} 
\multicolumn{1}{|l|}{} &  & None & 8 & 77.4/76.5 & 87.6/67.1 & 81.2 & 88.7 & 46.32 & 22.36 \\
\multicolumn{1}{|l|}{} &  & Forward & 9 & 79.7/78.8 & 88.2/69.1 & 83.8 & 92.3 & 47.94 & 22.65 \\
\multicolumn{1}{|l|}{} &  & Reverse & 10 & 79.2/78.2 & 88.1/68.3 & 83.2 & 89.6 & 48.45 & 21.57 \\
\multicolumn{1}{|l|}{} &  & All-to-one & 11 & 79.4/78.7 & 87.6/68.6 & 82.8 & 91.4 & 47.10 & 21.89 \\
\multicolumn{1}{|l|}{\multirow{-10}{*}{Student}} & \multirow{-5}{*}{Weights copied} & Random & 12 & 79.3/78.3 & 87.5/67.2 & 82.6 & 90.7 & 48.18 & 22.04 \\ \hline
\end{tabular}%
}\vspace{-.2cm}
\caption{Main results on various layer-selection strategies.}\vspace{-.2cm}
\label{tab:tab1}
\end{table*}