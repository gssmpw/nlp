@inproceedings{alpkd,
    title={{ALP-KD}: Attention-Based Layer Projection for Knowledge Distillation},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/17610},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Passban, Peyman and Wu, Yimeng and Rezagholizadeh, Mehdi and Liu, Qun},
    year={2021}, 
    pages={13657-13665}
}

@article{distilBERT,
  title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, V},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019},
  url={https://arxiv.org/abs/1910.01108},
}

@inproceedings{engine,
    title = "{ENGINE}: Energy-Based Inference Networks for Non-Autoregressive Machine Translation",
    author = "Tu, Lifu  and
      Pang, Richard Yuanzhe  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.251",
    pages = "2819--2826",
}

@inproceedings{exp-bias, 
    author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam}, 
    title = {Scheduled sampling for sequence prediction with recurrent Neural networks}, 
    year = {2015}, 
    booktitle = {Advances in Neural Information Processing Systems}, 
    pages = {1171â€“1179},
    url="https://proceedings.neurips.cc/paper/2015/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html"
}

@inproceedings{fcd,
  title={Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation},
  author={Kun Huang and Xin Guo and Meng Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:268030766}
}

@inproceedings{fdistill,
  title = {$f$-{{divergence minimization}} for {{sequence-level knowledge distillation}}},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
  year = {2023},
  pages = {10817--10834},
  doi = {10.18653/v1/2023.acl-long.605},
}

@inproceedings{hao2022teacher,
    author = {Hao, Yongchang and Liu, Yuxin and Mou, Lili},
     booktitle = {Advances in Neural Information Processing Systems},
     pages = {12594--12607},
     title = {Teacher Forcing Recovers Reward Functions for Text Generation},
     url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/51ae7d9db3423ae96cd6afeb01529819-Paper-Conference.pdf},
     volume = {35},
     year = {2022}
}

@article{hintonkd,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      url={https://arxiv.org/abs/1503.02531}, 
      journal={arXiv preprint arXiv:1503.02531}
}

@inproceedings{llmr,
  title = {{{LLMR}}: {{Knowledge distillation}} with a {{large language model-induced reward}}},
  shorttitle = {{{LLMR}}},
  booktitle = {Proceedings of the {{Joint International Conference}} on {{Computational Linguistics}}, {{Language Resources}} and {{Evaluation}}},
  author = {Li, Dongheng and Hao, Yongchang and Mou, Lili},
  year = {2024},
  pages = {10657--10664},
  url={url = "https://aclanthology.org/2024.lrec-main.932",}
}

@inproceedings{minillm,
    title={Mini{LLM}: Knowledge Distillation of Large Language Models},
    author={Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
    booktitle={International Conference on Learning Representations},
    url={https://openreview.net/forum?id=5h0qf7IBZZ},
    year={2024},
}

@inproceedings{minilmv2,
  title = {{MiniLMv2}: {M}ulti-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}}},
  author = {Wang, Wenhui and Bao, Hangbo and Huang, Shaohan and Dong, Li and Wei, Furu},
  year = {2021},
  pages = {2140--2151},
  doi = {10.18653/v1/2021.findings-acl.188},
}

@book{mode-seeking, 
    author = {Bishop, Christopher M.}, 
    title = {Pattern recognition and machine learning (information science and statistics)}, 
    year = {2006}, 
    isbn = {0387310738}, 
    publisher = {Springer-Verlag}, 
    url = {https://link.springer.com/book/9780387310732}
}

@inproceedings{patientkd,
    title = "Patient Knowledge Distillation for {BERT} Model Compression",
    author = "Sun, Siqi  and
      Cheng, Yu  and
      Gan, Zhe  and
      Liu, Jingjing",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing",
    year = "2019",
    doi = "10.18653/v1/D19-1441",
    pages = "4323--4332",
}

@inproceedings{railkd,
  title = {{{RAIL-KD}}: {{Random intermediate layer mapping}} for {{knowledge distillation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}}},
  author = {Haidar, Md Akmal and Anchuri, Nithin and Rezagholizadeh, Mehdi and Ghaddar, Abbas and Langlais, Philippe and Poupart, Pascal},
  year = {2022},
  pages = {1389--1400},
  doi = {10.18653/v1/2022.findings-naacl.103},
}

@inproceedings{rkd,
    title = {Relational Knowledge Distillation},
    author = {Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year = {2019},
    URL = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.html},
    pages={3967-3976}
} 
}

@inproceedings{tinybert,
  title = {{{TinyBERT}}: {{Distilling BERT}} for {{natural language understanding}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}}},
  author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  year = {2020},
  pages = {4163--4174},
  doi = {10.18653/v1/2020.findings-emnlp.372},
}

