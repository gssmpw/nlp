\section{Related Works}
\subsection{SAM-based Medical Image Segmentation}
SAM represents a significant breakthrough in transforming image segmentation from specialized applications to a general-purpose tool~\cite{MAZUROWSKI2023102918}. 
After training on large-scale datasets, SAM builds a broad knowledge base and relies on manually provided explicit prompts with precise locations (e.g., points and bounding boxes) to trigger segmentation responses~\cite{ZHANG2024108238}. However, due to the substantial domain gap between natural and medical images, SAM exhibits limited generalizability in medical imaging. To bridge this gap, studies such as MedSAM and SAM Med2D integrate extensive medical imaging data with specialized fine-tuning strategies, effectively improving performance in medical scenarios~\cite{ma2024medsam, le2024medficientsam, cheng2023sam2d}. However, the accuracy of SAMâ€™s segmentation is sensitive to the positional bias of the prompts (as validated in Section~\ref{sec:manual}). Consequently, the segmentation process often requires extensive manual intervention or professional detectors, turning it into a multi-stage workflow. In certain clinical settings(e.g., surgeries~\cite{yue2024surgicalsam}), providing explicit prompts for every frame is impractical. To address this limitation, methods like MaskSAM and UV-SAM employ task-specific expert models to generate coarse positional prompts, reducing reliance on explicit user input~\cite{xie2024masksam, zhang2024uv}. While these approaches mitigate the need for explicit prompts, they lack fine-grained class-specific prompting capabilities and face scalability challenges~\cite{paranjape2024adaptivesam, yue2023part}. In contrast, our method introduces a class-based prompting approach, leveraging a diffusion process to generate prompt embeddings from image features. This simplified prompting mechanism streamlines the segmentation pipeline, and enhances fine-grained class differentiation.

% Its extensive pretraining on large-scale data has endowed it with remarkable zero-shot generalization capabilities~\cite{ZHANG2024108238}. Research on SAM-based medical image segmentation can be broadly divided into two categories. The first category focuses on fine-tuning while retaining the manual prompt structure to enhance performance. MedSAM was trained with a large volume of medical data to create a medical version of SAM~\cite{ma2024medsam}. MedficientSAM employs the EfficientViT model to replace the image encoder in SAM and improves the model's performance after fine-tuning it on multimodal data~\cite{le2024medficientsam}.
% SAMed applied a low-rank adaptation strategy to SAM's image encoder and fine-tuned it along with the prompt encoder and mask decoder on a multi-organ abdominal segmentation dataset~\cite{zhang2023SAMed}. 
% However, manual prompts are difficult to provide accurately in clinical settings~\cite{yue2024surgicalsam}, leading to a second category of research focusing on replacing manual prompts to make SAM a fully automated process, as seen in models like MaskSAM and UV-SAM~\cite{xie2024masksam, zhang2024uv}. These approaches introduce additional models to generate rough masks, but the added structures are often task-specific, making the SAM model more cumbersome. Adaptive SAM and SP-SAM utilize CLIP to encode textual prompts into prompt embeddings~\cite{paranjape2024adaptivesam, yue2023part}. 
% Although these methods achieve a fully automated segmentation process, they still rely on prompts, leaving the issues outlined in Section~\ref{sec:introduction} unresolved. Our proposed AutoMedSAM not only eliminates the need for manual prompts but also fully taps into SAM's inherent potential. Moreover, the class prompt encoder can be extended to all SAM-based foundational models.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{fig_overview_new.pdf}
    \caption{\small An overview of the AutoMedSAM. AutoMedSAM generates dense and sparse prompt embeddings through a diffusion-based class prompt encoder, eliminating the need for explicit prompts. During training, we employ an uncertainty-aware joint optimization strategy with multiple loss functions for supervision, while transferring MedSAM's pre-trained knowledge to AutoMedSAM. This approach improves training efficiency and generalization. With end-to-end inference, AutoMedSAM overcomes SAM's limitations, enhancing usability and expanding its application scope and user base.}
    \label{fig:overview}
\end{figure*}

\subsection{Diffusion Models for Medical Domain}
% Diffusion modeling is a promising approach for medical imaging, capable of generating high-quality medical images and aiding various tasks like segmentation, classification, and anomaly detection
Diffusion models show strong potential in medical imaging, achieving notable success in tasks such as image generation, segmentation, and classification~\cite{YAN2024112350, zhu2024diffusion, fontanella2024diffusion}. The initial applications of diffusion models in the medical field primarily focused on generating medical data, which has proven useful for medical data augmentation~\cite{kazerouni2023diffusion}. In addition, several scholars have investigated the potential of medical images generated by diffusion models as a substitute for real data in training deep networks. M. Usman Akbar et al. and D. Stojanovski et al. demonstrated that these synthetic data are effective for downstream tasks~\cite{usman2024brain, stojanovski2024efficient}. Recent studies have used diffusion models for cross-modality synthesis~\cite{luo2024target, pan2024synthetic}. For example, DCE-diff addresses data heterogeneity by leveraging multimodal non-contrast images to extract anatomical details from structural MRI sequences and perfusion information from ADC images, enabling the synthesis of early-phase and late-phase DCE-MRI~\cite{ramanarayanan2024dce}. D. Stojanovski et al. demonstrated that visual realism during model training does not necessarily correlate with model performance. Consequently, these models can generate organ or lesion features optimized for deep learning, thereby enhancing the accuracy of downstream tasks. Furthermore, utilizing more efficient models can significantly reduce computational costs~\cite{stojanovski2024efficient}. By progressively refining representations through noise-based generation and denoising, diffusion models inherently capture fine-grained structural details and semantic consistency, which are particularly beneficial for complex medical image tasks.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{fig_overview_new.pdf}
%     \caption{\small An overview of the AutoMedSAM. AutoMedSAM generates dense and sparse prompt embeddings through a diffusion-based class prompt encoder, eliminating the need for explicit prompts. During training, we employ an uncertainty-aware joint optimization strategy with multiple loss functions for supervision, while transferring MedSAM's pre-trained knowledge to AutoMedSAM. This approach improves training efficiency and generalization. With end-to-end inference, AutoMedSAM overcomes SAM's limitations, enhancing usability and expanding its application scope and user base.}
%     \label{fig:overview}
% \end{figure*}