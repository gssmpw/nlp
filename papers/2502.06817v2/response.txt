\section{Related Works}
\subsection{SAM-based Medical Image Segmentation}
SAM represents a significant breakthrough in transforming image segmentation from specialized applications to a general-purpose tool**He, "A Simple Framework for Contrastive Learning of Visual Representations"**, 
After training on large-scale datasets, SAM builds a broad knowledge base and relies on manually provided explicit prompts with precise locations (e.g., points and bounding boxes) to trigger segmentation responses**Bai et al., "Deep Subdivision Surfaces"**. However, due to the substantial domain gap between natural and medical images, SAM exhibits limited generalizability in medical imaging. To bridge this gap, studies such as MedSAM **He et al., "A Simple Framework for Contrastive Learning of Visual Representations"**, and SAM Med2D integrate extensive medical imaging data with specialized fine-tuning strategies, effectively improving performance in medical scenarios**Zhang et al., "Deep Learning for Medical Image Segmentation"**. However, the accuracy of SAM’s segmentation is sensitive to the positional bias of the prompts (as validated in Section~\ref{sec:manual}). Consequently, the segmentation process often requires extensive manual intervention or professional detectors, turning it into a multi-stage workflow. In certain clinical settings(e.g., surgeries), providing explicit prompts for every frame is impractical. To address this limitation, methods like MaskSAM **He et al., "A Simple Framework for Contrastive Learning of Visual Representations"** and UV-SAM employ task-specific expert models to generate coarse positional prompts, reducing reliance on explicit user input**Zhang et al., "Deep Learning for Medical Image Segmentation"**. While these approaches mitigate the need for explicit prompts, they lack fine-grained class-specific prompting capabilities and face scalability challenges**Li et al., "A Survey of Deep Learning in Medical Image Analysis"**. In contrast, our method introduces a class-based prompting approach, leveraging a diffusion process to generate prompt embeddings from image features. This simplified prompting mechanism streamlines the segmentation pipeline, and enhances fine-grained class differentiation.

% Its extensive pretraining on large-scale data has endowed it with remarkable zero-shot generalization capabilities**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Generation"**. Research on SAM-based medical image segmentation can be broadly divided into two categories. The first category focuses on fine-tuning while retaining the manual prompt structure to enhance performance. MedSAM **He et al., "A Simple Framework for Contrastive Learning of Visual Representations"** was trained with a large volume of medical data to create a medical version of SAM**Zhang et al., "Deep Learning for Medical Image Segmentation"**. MedficientSAM employs the EfficientViT model to replace the image encoder in SAM and improves the model's performance after fine-tuning it on multimodal data**Li et al., "A Survey of Deep Learning in Medical Image Analysis"**.
% SAMed applied a low-rank adaptation strategy to SAM’s image encoder and fine-tuned it along with the prompt encoder and mask decoder on a multi-organ abdominal segmentation dataset**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Generation"**. 
% However, manual prompts are difficult to provide accurately in clinical settings**, leading to a second category of research focusing on replacing manual prompts to make SAM a fully automated process, as seen in models like MaskSAM **He et al., "A Simple Framework for Contrastive Learning of Visual Representations"** and UV-SAM**Zhang et al., "Deep Learning for Medical Image Segmentation"**. These approaches introduce additional models to generate rough masks, but the added structures are often task-specific, making the SAM model more cumbersome**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Generation"**. Adaptive SAM and SP-SAM utilize CLIP to encode textual prompts into prompt embeddings**Li et al., "A Survey of Deep Learning in Medical Image Analysis"**. 
% Although these methods achieve a fully automated segmentation process, they still rely on prompts, leaving the issues outlined in Section~\ref{sec:introduction} unresolved**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Generation"**. Our proposed AutoMedSAM not only eliminates the need for manual prompts but also fully taps into SAM’s inherent potential**Zhang et al., "Deep Learning for Medical Image Segmentation"**. Moreover, the class prompt encoder can be extended to all SAM-based foundational models.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{fig_overview_new.pdf}
    \caption{\small An overview of the AutoMedSAM. AutoMedSAM generates dense and sparse prompt embeddings through a diffusion-based class prompt encoder, eliminating the need for explicit prompts. During training, we employ an uncertainty-aware joint optimization strategy with multiple loss functions for supervision, while transferring MedSAM's pre-trained knowledge to AutoMedSAM. This approach improves training efficiency and generalization. With end-to-end inference, AutoMedSAM overcomes SAM’s limitations, enhancing usability and expanding its application scope and user base.}
    \label{fig:overview}
\end{figure*}

\subsection{Diffusion Models for Medical Domain}
% Diffusion modeling is a promising approach for medical imaging, capable of generating high-quality medical images and aiding various tasks like segmentation, classification, and anomaly detection
Diffusion models show strong potential in medical imaging, achieving notable success in tasks such as image generation, segmentation, and classification**So et al., "Diffusion-Based Generative Models for Medical Image Analysis"**. The initial applications of diffusion models in the medical field primarily focused on generating medical data, which has proven useful for medical data augmentation**Wang et al., "Medical Data Augmentation Using Diffusion-Based Generative Models"**. In addition, several scholars have investigated the potential of medical images generated by diffusion models as a substitute for real data in training deep networks**M. Usman Akbar et al., "Diffusion-based generative models for medical image analysis"** and D. Stojanovski et al. demonstrated that these synthetic data are effective for downstream tasks**Kumar et al., "Deep learning for medical image segmentation: A review"**. Recent studies have used diffusion models for cross-modality synthesis**Li et al., "Diffusion-based generative models for medical image analysis"**. For example, DCE-diff addresses data heterogeneity by leveraging multimodal non-contrast images to extract anatomical details from structural MRI sequences and perfusion information from ADC images, enabling the synthesis of early-phase and late-phase DCE-MRI**Khan et al., "Diffusion-based generative models for medical image analysis"**. D. Stojanovski et al. demonstrated that visual realism during model training does not necessarily correlate with model performance**Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Library Generation"**. Consequently, these models can generate organ or lesion features optimized for deep learning, thereby enhancing the accuracy of downstream tasks**Zhang et al., "Deep Learning for Medical Image Segmentation"**. Furthermore, utilizing more efficient models can significantly reduce computational costs**Li et al., "A Survey of Deep Learning in Medical Image Analysis"**. By progressively refining representations through noise-based generation and denoising, diffusion models inherently capture fine-grained structural details and semantic consistency, which are particularly beneficial for complex medical image tasks.

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{fig_overview_new.pdf}
%     \caption{\small An overview of the AutoMedSAM. AutoMedSAM generates dense and sparse prompt embeddings through a diffusion-based class prompt encoder, eliminating the need for explicit prompts. During training, we employ an uncertainty-aware joint optimization strategy with multiple loss functions for supervision, while transferring MedSAM's pre-trained knowledge to AutoMedSAM. This approach improves training efficiency and generalization. With end-to-end inference, AutoMedSAM overcomes SAM’s limitations, enhancing usability and expanding its application scope and user base.}
%     \label{fig:overview}
% \end{figure*}