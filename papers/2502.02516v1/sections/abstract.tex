\begin{abstract}
We study the policy evaluation problem in an online multi-reward multi-policy discounted setting, where multiple reward functions  must be evaluated simultaneously for different policies. We adopt an $(\epsilon,\delta)$-PAC perspective to achieve $\epsilon$-accurate estimates with high confidence across finite or convex sets of rewards, a setting that has not been investigated in the literature. Building on prior work on Multi-Reward Best Policy Identification, we adapt the {\tt MR-NaS} exploration scheme \cite{russomulti} to jointly minimize sample complexity for evaluating different policies across different reward sets. Our approach leverages an instance-specific lower bound revealing how the sample complexity scales with a measure of value deviation, guiding the design of an efficient exploration policy. Although computing this bound entails a hard non-convex optimization, we propose an efficient convex approximation that holds for both finite and convex reward sets. Experiments in tabular domains demonstrate the effectiveness of  this adaptive exploration scheme.


% standard policy evaluation strategies, our approach leverages instance-specific guarantees to reduce sample complexity for each reward of interest.



% Our approach leverages behavior policy search, incorporating an online procedure to refine the behavior policy for improved estimation accuracy. By unifying instance-specific lower-bound insights with a principled search for behavior policies, we establish a theoretically grounded framework that achieves low-variance, unbiased return estimates under multiple reward functions. Empirical results in both tabular and deep RL domains demonstrate that this adaptive exploration technique yields more efficient and reliable multi-reward policy evaluations.

%  that tightly aligns exploration with reward-specific difficulty. Unlike prior methods that primarily rely on standard policy evaluation strategies, our approach leverages instance-specific guarantees to reduce sample complexity for each reward of interest. Experimental results demonstrate that this adaptive strategy consistently achieves lower variance and higher confidence across multiple reward functions in both tabular and deep RL settings.

% Rewards are a critical aspect of formulating Reinforcement Learning (RL) problems; often, one may be interested in testing multiple reward functions, or the problem may naturally involve multiple rewards. In this study, we investigate the Multi-Reward Best Policy Identification (MR-BPI) problem, where the goal is to determine the best policy for all rewards in a given set with minimal sample complexity and a prescribed confidence level. We derive a fundamental instance-specific lower bound on the sample complexity required by any Probably Correct (PC) algorithm in this setting. This bound guides the design of an optimal exploration policy attaining minimal sample complexity. However, this lower bound involves solving a hard non-convex optimization problem. We address this challenge by devising a convex approximation, enabling the design of sample-efficient algorithms. We propose MR-NaS, a PC algorithm with competitive performance on hard-exploration tabular environments. Extending this approach to Deep RL (DRL), we also introduce DBMR-BPI, an efficient algorithm for model-free exploration in multi-reward settings.
\end{abstract}