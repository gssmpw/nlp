\section{Conclusions}\label{sec:conclusions}   
In this work, we studied  the problem of devising an exploration strategy for online multi-reward multi-policy evaluation, accommodating reward sets that are either finite or convex, potentially encompassing all possible rewards.
%To devise \mrnas{}, we leveraged tools information theoretical tools from Best Policy Identification, and optimized the behavior policy that is used to collect data in order to maximize the information collected from the MDP for the policy evaluation problem.
Leveraging tools from Best Policy Identification, we derived an instance-dependent sample complexity lower bound for the $(\epsilon,\delta)$-PAC setting. Based on this bound, we extended \mrnas{} \cite{russomulti} to this policy-evaluation setting, and showed its asymptotic efficiency.
Lastly, we compared \mrnas{} against other adaptive exploration  across various domains, demonstrating the efficiency of \mrnas{}.
%Finally, we compared our strategy to other adaptive exploration methods for multi-task policy evaluation on different domains, demonstrating the effectiveness of \mrnas{}.
