\section{Introduction}\label{sec:introduction}
This paper investigates methods for efficiently evaluating one or more policies across various reward functions in an online discounted setting, a key challenge in Reinforcement Learning (RL) \cite{sutton2018reinforcement}, where the aim is to compute the value of each policy. Accurate value estimation serves many purposes, from verifying a policy’s effectiveness to providing insights for policy improvement.


There are several applications where one has multiple policies to evaluate, e.g.,  multiple policies arising from using different hyperparameters \cite{dann2023reinforcement,chen2024multiple,poddarpersonalizing}. Similarly, multiple reward functions often arise in real-world decision-making problems, making it crucial to evaluate how a policy performs across diverse objectives. As an example, large language models \cite{brown2020language} are fine-tuned on human feedback that spans a wide range of user preferences and goals \cite{ziegler2019fine,rafailov2024direct,poddarpersonalizing}, effectively producing multiple distinct reward signals. 

Other applications, similarly, involve multiple rewards, such as: user-preference modeling, robotics tasks aiming to reach different goals, or intent-based radio network optimization \cite{nahum2023intent, de2023towards, russomulti,poddarpersonalizing}.



In general, it is challenging to efficiently and accurately
evaluate a policy over multiple objectives, potentially for
multiple policies aimed at solving different task \cite{sutton2011horde,mcleod2021continual,jain2024adaptive}. Indeed, when multiple policies and distinct reward sets are involved, it is not obvious how best to gather data in a way that balances efficiency and accuracy.


Prior research approached this issue in different ways. One direction aims to minimize the mean squared error (MSE) of the value estimator (or the variance of the importance sampling estimator) to guide adaptive exploration. In \cite{hanna2017data}, this is done for single-reward policy evaluation, while \cite{jain2024adaptive} proposes a variance-driven exploration scheme for a finite collection of policy–reward pairs. However, these methods may not always guarantee sample-efficient exploration or provide $(\epsilon,\delta)$-PAC guarantees. By contrast, other works \cite{dann2023reinforcement,chen2024multiple} address multi-policy evaluation for a \emph{single reward} under the $(\epsilon,\delta)$-PAC criteria in the episodic setting. However, their sample complexity guarantees are instance-dependent in the transition dynamics (e.g., dependent on state-action visitation structure) but not in the reward. Consequently, these bounds do not characterize how the interaction between rewards and transitions—such as sparse rewards under specific dynamics—affects evaluation complexity. 


To investigate the problem of devising an exploration strategy for this setting, we study the sample complexity of multi-reward multi-policy evaluation, and adopt an $(\epsilon,\delta)$-PAC viewpoint to achieve $\epsilon$-accurate estimates with confidence $1-\delta$ over finite or convex sets of rewards.  Furthermore, while prior works focus on estimating the expected value of a policy under the initial state distribution, our work evaluates policies across all states (a value vector). This broader scope is critical for applications requiring reliable verification of policy behavior and enabling explainable RL \cite{puiutta2020explainable,ruggeri2025explainable}.

 Our approach optimizes the behavior policy to maximize the evidence  gathered from the environment at each-time step, and builds on techniques from the Best Policy Identification (BPI) literature \cite{al2021navigating}, which itself draws inspiration from Best Arm Identification techniques in bandit problems \cite{garivier2016optimal,kaufmann2016complexity}. These methods cast the problem of finding the optimal policy as a hypothesis testing problem. The core insight is to establish an instance-specific sample complexity lower bound—formulated as an optimization problem—where its solution directly yields the optimal exploration distribution

\setlength{\intextsep}{0pt}%
\setlength{\columnsep}{3pt}%
\begin{wrapfigure}{l}{0.3\linewidth}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/multirew.png}
  \end{center}
  %\caption{Birds}
\end{wrapfigure} 
Building on insights from Multi-Reward BPI \cite{russomulti}, which extends the BPI framework to multiple rewards, we develop an instance-specific sample complexity lower bound for multi-reward multi-policy evaluation, dependent on the dynamics and the rewards. To our knowledge, this constitutes the first such bound even for the single-policy single-reward case.
Furtheremore, for convex reward sets, we present an alternative sample complexity characterization compared to \cite{russomulti}, which yields a closed-form solution when the set of rewards includes all possible rewards. 

Our result exhibits a sample complexity that scales with the worst-case policy-reward pair—rather than the sum over individual policy-reward pairs—capturing the inherent difficulty of efficiently evaluating all combinations simultaneously. Particularly, the complexity scales according to a measure of value deviation $\rho_r^\pi(s,s')\coloneqq V_r^\pi(s') - \mathbb{E}_{\hat s \sim P(s,\pi(s))}[V_r^\pi(\hat s)]$, similar to the variance of the value:
\[
O\left(\sup_{\pi\in \Pi,r\in {\cal R}_\pi} \max_{s,s'\in\statespace} \frac{ \gamma^2|\rho_r^\pi(s,s')|^2}{\epsilon^2 (1-\gamma)^2 \omega^\star(s,\pi(s))}\right).
\]
Finally, we adapt {\tt MR-NaS} \cite{russomulti}, an extension of {\tt NaS} \cite{al2021navigating}, to our setting, and prove its asymptotic optimality for policy evaluation up to a constant factor. We further illustrate its practical efficiency through experiments in various  tabular environments.





