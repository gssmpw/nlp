\section{Algorithms for tabular MDPs and Deep Reinforcement Learning}\label{sec:algorithms}
In this section show how to adapt the \mrnas{} (Multi-Reward Navigate and Stop) algorithm \cite{russomulti}  for multi-reward multi-policy evaluation based on the results from the previous section.

\subsection{MR-NaS}
{\tt MR-NaS} (\cref{algo:mr-nas}), and similar algorithms, are designed with 2 key components: (1) a sampling rule and (2) a stopping rule. We now discuss each of these.

\paragraph{Sampling Rule.} The key idea is to sample according to the policy induced by   \[\omega^\star=\argmin_{\omega\in \Omega(M)} U_{\epsilon/2}(\omega;M).\] Indeed, sampling actions according to $\pi^\star(a|s) = \omega^\star(s,a)/\sum_b \omega^\star(s,b)$ guarantees   optimality with respect to $U_{\epsilon/2}^\star$, as the solution $\omega^\star$  matches the rate in \cref{thm:sample_complexity_lb}. The factor $\epsilon/2$ arises from  the lower bound analysis that requires $2\epsilon$-separation. By tightening the  accuracy to  $\epsilon/2$, we ensure the $(\epsilon,\delta)$-PAC guarantee,  which  would otherwise be hard to prove. This results in an additional constant factor $4$ in the sample complexity.

However, $\omega^\star$ cannot be computed without knowledge of the MDP $M$. As in previous works \cite{garivier2016optimal,al2021navigating}, we employ the certainty equivalence principle (CEP): plug-in the current estimate at time $t$ of the transition function   and compute  $\omega_t^\star$. The allocation $\omega_t^\star$ rapidly eliminates models confusing for $M_t$, efficiently determining whether the true model $M$ is non-confusing--motivating our use of the CEP.



\begin{algorithm}[t]
	\caption{\mrnas{} \cite{russomulti}}
	\label{algo:mr-nas}
    % \small
	\begin{algorithmic}[1]
    \WHILE{$t < U_{\epsilon/2}( N_t/t; M_t)\beta(N_t,\delta)$} 
            \STATE Compute $\omega_t^\star= \inf_{\omega \in\Omega(M_t)} U_{\epsilon/2}(\omega; M_t)$.
            \STATE Set $\pi_t(a|s_t) = (1-\varepsilon_t)\pi_t^\star(a|s_t) + \varepsilon_t \pi_{f,t}(a|s_t)$,  where $\pi_t^\star(a|s) = \omega_t^\star(s,a)/\sum_{a'}\omega_t^\star(s,a')$. \label{lst:line:final_policy}
            \STATE Play $a_t\sim \pi_t(\cdot|s_t)$ and observe $s_{t+1}\sim P(\cdot|s_t,a_t)$.
            \STATE Update MDP estimate $M_t$ and set $t\gets t+1$.
            \ENDWHILE{}
	\end{algorithmic}
\end{algorithm}
\begin{figure*}[!b]
    \centering    \includegraphics[width=\linewidth]{figures/1000000_multi_policy_multi_reward_abs_error_large.pdf}
    \caption{Multi-policy evaluation over finite set of rewards on different  environments. Shaded curves represent 95\% confidence intervals.}
    \label{fig:multi_policy_finite_rewards}
\end{figure*}
In summary, the algorithm proceeds as follows: at each time-step $t$ the agent computes the optimal visitation distribution $\omega_t^\star=\argmin_{\omega\in \Omega(M_t)} U_{\epsilon/2}(\omega;M_t)$ with respect to $M_t$, the estimate of the MDP (which is, practically speaking, the estimate of the transition function).  
The  policy $\pi_t^\star(a|s) $ induced by $\omega_t^\star$ is mixed with a \emph{forcing} policy $\pi_{f,t}(\cdot|s)$ (e.g., a uniform distribution over actions; see also \cite{russomulti}) that guarantees all actions are sampled infinitely often.  The mixing factor $\varepsilon_t$ can be chosen as $\varepsilon_t=1/N_t(s_t)$, where $N_t(s)$ is the number of visits of state $s$ up to time $t$. 
The resulting exploration policy, $\pi_t$, is used to sample an action $a_t$. Upon observing the next state, the transition function is updated using the empirical average.
\begin{remark}\label{remark:actions}\it 
   At first glance, the optimization problem $\min_{\omega\in \Omega(M_t)}U_{\epsilon/2}(\omega;M)$ might appear to overlook actions $a\neq\pi(s)$. However, these actions are accounted for through the forward constraint $\omega\in \Omega(M_t)$. %Note that from a practical standpoint, it is important to properly initialize the empirical transition function, e.g., by using additive smoothing \cite{chen1999empirical}.
\end{remark}
\paragraph{Stopping rule.} Lastly, the method stops whenever sufficient  evidence has been gathered to obtain  $(\epsilon,\delta)$-PAC guarantees. These requires approximately $U_{\epsilon/2}^\star(M)\log(1/\delta)$ samples (by inspecting \cref{thm:sample_complexity_lb,thm:relaxed_characteristic_time}).

This rule is defined by two quantities: (1) a threshold \[\beta(N_t,\delta) = \log(1/\delta) + (S-1) \sum_{s,a} \log\left(e\left[1 + \frac{N_t(s,a)}{S-1}\right]\right);\] (2) the empirical characteristic time  $U_{\epsilon/2}(N_t/t;M_t)$. In both, $N_t(s,a)$ is the number of times action $a$ has been selected in state $s$ up to time $t$, and $N_t=(N_t(s,a))_{s,a}$. 
In conclusion, we stop as soon as $t \geq U_{\epsilon/2}(N_t/t;M_t)\beta(N_t;\delta)$. Hence, we have the following guarantees  (see proof in \cref{app:thm:mrnas_guarantees}).
\begin{tcolorbox}
\begin{theorem}\label{thm:mrnas_guarantees}
    \mrnas{} 
   guarantees $\mathbb{P}_M[\forall \pi\in \Pi,r\in \mathcal{R}_\pi: \|V_r^\pi - \hat V_r^\pi\|_\infty \leq \epsilon]\geq 1-\delta$; $\mathbb{P}_M[\tau<\infty]=1$ and $\limsup_{\delta \to 0} \frac{\mathbb{E}_M[\tau]}{\log(1/\delta)} \leq 4U_\epsilon^\star(M).$
\end{theorem}
\end{tcolorbox}


