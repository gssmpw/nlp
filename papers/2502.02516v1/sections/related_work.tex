\section{Related Work}\label{sec:related_work}

 Reinforcement Learning (RL) exploration techniques have tipically focused only on the problem on learning the optimal policy for a single objective \cite{sutton2018reinforcement}. This domain has generated a vast body of work, often inspired by the multi-armed bandit literature \cite{lattimore2020bandit}, with approaches ranging from $\epsilon$-greedy and Boltzmann exploration \cite{watkins1989learning,sutton2018reinforcement} to more sophisticated methods based on Upper-Confidence Bounds (UCB) \cite{auer2002using}, Bayesian procedures \cite{osband2013more,russo2018tutorial} or Best Policy Identification techniques \cite{al2021navigating,wagenmaker2022beyond, taupin2023best}.

Despite these advances, the challenge of designing exploration strategies for  online policy evaluation has received comparatively little attention. Early work in this direction examined multi-armed bandits problems  \cite{antos2008active} and function evaluation \cite{carpentier2012adaptive}, showing that efficient exploration requires allocating more samples where variance is higher. More recently, \cite{linke2020adapting} considers a bandit setting with a finite number of tasks, focusing on minimizing the mean squared error.

 
In \cite{hanna2017data}, the authors introduced the idea of optimizing the behavior policy (i.e., the exploration strategy) by directly minimizing the variance of importance sampling—one of the earliest efforts to design an exploration strategy specifically for policy evaluation in Markov Decision Processes. Separately, \cite{mcleod2021continual} proposed \texttt{SF-NR}, which uses the Successor Representation framework \cite{dayan1993improving} to guide exploration for value estimation over a finite set of tasks. More closely related to our work is \cite{jain2024adaptive}, where the authors tackle the evaluation of a finite set of policy–reward pairs and propose \texttt{GVFExplorer}, an adaptive exploration method that optimizes the behavior policy with the goal of minimizing the estimation MSE. However, these works do not provide PAC guarantees, nor directly  provide a strategy that directly minimizes the sample complexity.

In \cite{dann2023reinforcement} they investigate  the multi-policy single-reward evaluation problem in a PAC framework. They propose an on-policy method that leverages the fact that the policies may overlap in a significant way. In \cite{chen2024multiple} the authors design a behavior policy that optimizes the coverage of the target policy set. However, these works do not provide instance-dependent   bounds in terms of the value of the target policies, nor study the multi-reward policy evaluation problem.