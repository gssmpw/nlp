\section{Problem Setting}\label{sec:problem_setting}
In this section, we describe the MDP model considered, and the policy evaluation setting.

\subsection{Markov Decision Processes (MDPs)}
Markov Decision Processes (MDPs) are widely utilized to model sequential decision-making tasks \citep{puterman2014markov}. In these tasks, at each time-step $t\in \mathbb{N}$ an agent  observes the current state of the MDP $s_t$ and selects an action $a_t$ to achieve a  desired objective. This objective is encapsulated in terms of a  reward $r_t\in [0,1]$, observed after selecting the action. In  RL, the primary goal of the agent is to determine a sequence of actions that maximizes the total  reward collected from an unknown MDP.

\paragraph{Discounted MDPs.} We consider a discounted MDP \citep{puterman2014markov} of the type $M=(\statespace,\actionspace,P,\gamma)$, where: $\statespace$ is a finite state space of size $S=|\statespace|$; $\actionspace$ is a finite action space of size $A=|\actionspace|$; $P:\statespace\times \actionspace \to \Delta(\statespace)$ is the transition function, which maps state-action pairs to distributions over states; lastly, $\gamma\in(0,1)$ is the discount factor.
In the following we consider also reward functions $r: \statespace\times \actionspace \to [0,1]$ that are bounded and deterministic functions of state-action pairs, and write $M_r=(M,r)$ to denote the MDP $M$ with reward $r$. 
In classical RL an agent is interested in maximizing the total discounted reward collected from $M_r$:  $r_1+\gamma r_2+\gamma^2 r_3+\dots$. The problem of computing an optimal sequence of actions (i.e., that maximizes the total discounted reward) can be reduced to that of finding a Markovian policy $\pi:S \to \Delta(\actionspace)$ that maps states to distributions over actions \citep{puterman2014markov}.

 
 For a Markovian policy $\pi$ we  define the discounted value of $\pi$ in $M_r$ at state $s$ as  $V_r^\pi(s)\coloneqq\mathbb{E}^\pi[\sum_{t\geq1}\gamma^{t-1} r(s_t,a_t)|s_1=s]$, where $s_{t+1}\sim P(\cdot|s_t,a_t)$ and $a_t\sim \pi(\cdot|s_t)$ (if $\pi$ is deterministic, we write $a_t=\pi(s_t)$).  We also write $V_r^\pi$, and omit the dependency on $M$ when it is clear from the context. We also define $V_r^\star(s) \coloneqq \max_{\pi} V_r^\pi(s)$ to be the optimal value in $s$ over all Markovian policies.  We further define the action-value function in $M_r$ as $Q_r^\pi(s,a)\coloneqq r(s,a)+\gamma \mathbb{E}_{s'\sim P(\cdot|s,a)}[V_r^\pi(s')]$. 
 
 Lastly,  for two MDPs $M,M'$ with transition functions, respectively, $P,P'$, we  write ${\rm KL}_{P|P'}(s,a)$ to indicate the KL-divergence between the two transition functions in $(s,a)\in \statespace\times \actionspace$. We also write ${\rm kl}(x,y)=x\log(x/y)+(1-x)\log((1-x)/(1-y))$ to denote the Bernoulli KL-divergence between two parameters $x,y\in[0,1]$.





 
 \paragraph{Assumptions.} We consider the problem of evaluating a \emph{finite set of deterministic target policies} $\Pi=\{\pi_1,\dots,\pi_N\}$ over  finite, or possibly convex, reward sets (we discuss more on this in the next paragraph). On the MDP $M$ we impose the following assumption.  \begin{assumption}\label{assumption:mdp_learner}\it
    The MDP $M$ is communicating, and the learner has no prior knowledge of $P$.
\end{assumption}

As in \cite{al2021navigating, russomulti}, we assume the MDP to be communicating, which avoids the awkward case where the algorithm could enter a subclass of states from which there is no possible comeback, and thus it becomes impossible to identify the value of a policy $\pi$ to the desired accuracy.  While the results can be extended to weakly-communicating MDPs, for ease of exposition we leave the extension for future work.



\paragraph{Set of Rewards.} In the following we consider sets of reward functions  $\rewardspace$ that are either  finite, or convex, and assume that these sets are  known to the agent beforehand. 

For tabular MDPs (i.e., with finite state-action spaces), assuming a natural labeling of the states $\statespace=\{s_1,\dots, s_{S}\}$,
we represent a reward $r$,  for a policy $\pi$, as a vector in $[0 ,1]^{S}$ (for simplicity, we omit the dependency on $\pi$).  The $i$-th element of $r$  corresponds to the reward $r(s_i,\pi(s_i))$.
For $\pi$, we indicate the canonical reward set by ${\cal R}_{\rm canon}^\pi=\{e_1,\dots,e_{S}\}$ in $\mathbb{R}^{S}$, with $e_i$ being the $i$-th vector of the canonical basis, defined as $(e_i)_j = \mathbf{1}_{(i=j)},\; i,j\in\{1\dots,S\}$. Lastly, we denote the ${\cal R}=[0,1]^S$ case as reward-free since  it encompasses all possible reward for $\pi$.






\subsection{Online Multi-Reward Multi-Policy Evaluation}
In an online single-reward Policy Evaluation (PE) setting, the objective is to learn the value vector of a single policy–reward pair $(\pi,r)$. In the multi-reward, multi-policy case, we aim instead to learn the value vectors for $\Theta\coloneqq\{(\pi, {\cal R}_\pi): \pi\in \Pi\}$, meaning each policy $\pi\in \Pi$ is evaluated on every reward in its own reward set  ${\cal R}_\pi$, which can differ across policies. 


This setting is closely related to off-policy policy evaluation (OPE) \citep{thomas2016data,precup2000eligibility}: in fact, to learn the value of a policy $\pi$, it might be a good idea to collect data from a different policy than $\pi$! However, OPE deals with the problem of learning the value of a policy $\pi$ using some other policy $\pi_\beta$ (and, in most cases, using a finite set of data). 
In our case, we instead study the problem of devising an optimal data-collection policy that, by online interactions with the MDP $M$, permits the agent to learn the value of $\Theta$ as quickly as possible up to the desired accuracy.

\paragraph{Online Multi-Reward Multi-Policy Evaluation.} We formalize our objective using the $(\epsilon,\delta)$-PAC (Probably Approximately Correct) framework. In such framework, an algorithm {\rm Alg} interacts with $M$ until sufficient data has been gathered to output the value of $\pi$ up to $\epsilon$ accuracy for any reward $r\in \rewardspace_\pi$, for  all $\pi\in \Pi$, with confidence $1-\delta$. 

Formally, an online PE algorithm \texttt{Alg} consists of:
\begin{itemize}[topsep=-5px,partopsep=0px,nosep]
    \item  a \textit{sampling rule} $(a_t)_{t\ge 1}$: upon observing  a state $s_t$, {\tt Alg} selects an action $a_t$, and then observes the next state $s_{t+1} \sim P(\cdot|s_t,a_t)$.
    \item  a \textit{stopping rule} $\tau$ that dictates when to stop the data collection process. $\tau$ is a stopping rule w.r.t. the filtration $({\cal F}_t)_{t\geq 1}$, where ${\cal F}_t=\sigma(\{s_1,a_1,\dots,a_{t-1},s_t\})$  is the $\sigma$-algebra generated by the random observations made under {\tt Alg} up to time $t$. 
    \item  an  \textit{estimated value} $\hat V_r$: at the stopping time $\tau$, {\tt Alg} returns the estimated value $\hat V_r^\pi$ of the policy $\pi\in \Pi$ in $M$ for any chosen reward $r\in \rewardspace_\pi$.
\end{itemize}
Denoting by $\mathbb{P}_M$ (resp. $\mathbb{E}_M$) the probability law (resp. expectation) of the data observed under {\tt Alg} in $M$, we define an algorithm to be $(\epsilon,\delta)$-PAC as follows.

\begin{definition}[$(\epsilon,\delta)$-PAC algorithm]
    An algorithm {\tt Alg} is said to be multi-reward multi-policy $(\epsilon,\delta)$-PAC if, for any MDP $M$,  policies-rewards set $\Theta$, $\delta\in (0,1/2), \epsilon \in (0, \frac{1}{2(1-\gamma)})$, we have $\mathbb{P}_M[\tau<\infty]=1$ (the algorithm stops almost surely) and \begin{equation}
        \mathbb{P}_M\left[\exists\pi \in \Pi,\exists r\in {\cal R}_\pi: \|V_r^\pi - \hat V_r^\pi\|_\infty > \epsilon\right]\leq \delta.\end{equation}
\end{definition}
In other words, with probability at least $1-\delta$, the algorithm’s estimate $\hat{V}_r^\pi$ is within $\epsilon$ of $V_r^\pi$ for every  $r \in \mathcal{R}_\pi, \pi\in \Pi$.

In the following section, we investigate the sample complexity of this problem and determine the minimal number of samples required to achieve the $(\epsilon,\delta)$-PAC guarantees. Our analysis reveals that the problem  can be more challenging under certain rewards. In fact, the sample complexity is governed by the most difficult reward-policy pair in $\Theta$, rather than a sum of the individual complexities across pairs.