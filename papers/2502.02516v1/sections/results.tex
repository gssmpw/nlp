\section{Numerical Results}\label{sec:num_results}
In this section, we present the numerical results  of \mrnas{}, and other algorithms, on various environments with different reward sets (results consider $30$ seeds).

\paragraph{Settings.} We study \mrnas{} in 4 different environments: {\tt Riverswim} \cite{strehl2004empirical}, {\tt Forked Riverswim} \cite{russo2023model}, {\tt DoubleChain} \cite{Kaufmann21a} and {\tt NArms} \cite{strehl2004empirical} (an adaptation of {\tt SixArms}). For each environment we evaluated 3 scenarios: (1) multiple policy evaluation with finite reward sets for each policy; (2) multiple reward-free policy evaluation; (3) single reward-free policy evaluation (results for this one can be found in \cref{app:additional_results}). 

While our framework supports various settings, we focus on what we consider to be the most important and novel scenarios. For multiple-policy evaluation, we sampled three random policies for each seed. These policies were sampled uniformly from the set of policies optimal for one-hot rewards, where each reward equals 1 at a single state-action pair and 0 elsewhere. In the case of finite reward sets, each policy was evaluated using the corresponding rewards from these sets. In the reward-free scenario, evaluations were conducted across the canonical basis ${\cal R}_{\rm canon}^\pi$ for each $\pi$. 

\begin{figure}[t]
    \centering    \includegraphics[width=\linewidth]{figures/1000000_multi_policy_rewfree_abs_error_large.pdf}
    \caption{Reward-Free multi-policy evaluation. Here we depict the average error over the canonical basis  ${\cal R}_{\rm canon}^\pi$ for each policy. Shaded curves represent 95\% confidence intervals.}\label{fig:multi_policy_rewardfree}
\end{figure}

\paragraph{Algorithms.} While our work is one of the first to study the reward-free evaluation problem, there are some prior works that study the multi-task policy evaluation. We consider (1) {\tt SF-NR} \cite{mcleod2021continual}, an algorithm for multi-task policy evaluation based on the Successor Representation, and we adapted it to also consider the reward-free setting (see \cref{app:sec:numerical_results} for more details). Next, we consider (2) {\tt GVFExplorer} \cite{jain2024adaptive}, a variance-based exploration strategy for learning general value functions \cite{sutton2011horde} based on minimizing the MSE. However, such exploration strategy is not applicable to the reward-free setting. We also evaluated
(3) {\tt Noisy Policy - Uniform}, a mixture of the target policies $\pi_{\rm mix}(a|s) = \frac{|\{\pi\in \Pi: \pi(s)=a\}|}{|\Pi|}$,  mixed with a uniform policy $\pi_u$ with a constant mixing factor $\varepsilon_t=0.3$. The resulting behavior policy is $\pi_b =(1-\varepsilon_t)\pi_{\rm mix} +\varepsilon_t \pi_u$. Lastly, (4) {\tt Noisy Policy - Visitation}, computes the same behavior policy as in (3)  with a non-constant mixing factor $\epsilon_t=1/N_t(s_t)$, which is based on the number of visits.

\paragraph{Discussion.} The results for the first two settings are shown in \cref{fig:multi_policy_finite_rewards,fig:multi_policy_rewardfree} (policy evaluation was performed using the MDP estimate $M_t$ at each time-step).  \mrnas{} achieves good accuracy on all environments. On the other hand,  {\tt SF-NR} and {\tt GVFExplorer}  have mixed performance. While {\tt SF-NR} is not designed to optimize a behavior policy, we note that the exploration strategy used by {\tt GVFExplorer} is similar to solving a problem akin to \cref{eq:T_epsilon_omega}, but neglects the forward equations when optimizing the behavior policy (see also \cref{app:algorithms} for details).  
As a result, {\tt GVFExplorer} tends to perform worse in environments where certain rewards are hard to obtain under a uniform policy. Lastly, we refer the reader to \cref{app:additional_results} for more details, and to the {\tt README.me} file in the supplementary material to reproduce the results.
