\section{Sample Complexity Results}\label{app:sec:lower_bound}

\subsection{Alternative Models and Value Deviation}\label{app:subsec:confusing_models}
In this section we prove some of the results in \cref{sec:lower_bound}, and provide additional properties on the set of alternative models.

We begin by proving \cref{prop:suffnecc_cond_confusing_models} and an additionel necessary (minimax) condition for a model $M'$ to be an alternative mode.

\subsubsection{Proof of \cref{prop:suffnecc_cond_confusing_models} and an additional minimax necessary condition}
\label{app:prop:suffnecc_cond_confusing_models}
\begin{proof}[Proof of \cref{prop:suffnecc_cond_confusing_models}]

We first prove the sufficient condition, and then prove the necessary condition.

    \underline{\bf Sufficient condition}. Let $s_0,s_1\in \statespace$ and fix a reward $r\in \rewardspace$. In the following we omit the subscript $r$ for simplicity.
    Define a transition function $P^{'}$ satisfying $P^{'}(s'|s,a) =P(s'|
s,a)$ for all $s\neq  s_0,a,s'$,  and $P^{'}(s_1|s_0, \pi(s_0))=\delta + P(s_1|s_0,\pi(s_0))(1-\delta),  P^{'}(s'|s_0,\pi(s_0))=(1-\delta)P(s'|s_0,\pi(s_0))$ for $s'\neq s_1$, with $\delta\in (0,1)$. Therefore $P(s,a)\ll P^{'}(s,a)$ for all $(s,a)$.

Let $V_P^\pi$ be the value of $\pi$ in $P$, and similarly define $V_{P'}^\pi$.
For $s\neq s_0$ the difference in value $\Delta V^\pi = V_P^\pi - V_{P'}^\pi$ satisfies
\[
\Delta V^\pi(s) =
        \gamma P^\pi(s)^\top\Delta V^\pi,
\]
where $P^\pi(s)=\begin{bmatrix}
    P(s_1|s,\pi(s))&\dots& P(s_{S}|s,\pi(s))
\end{bmatrix}^\top$ is the vector of transitions over the next state starting from $(s,\pi(s))$.

To analyse the case $s=s_0$ we indicate by $P^{'\pi}(s)$ the vector of transitions in $s$ for $P^{'}$. We obtain the following sequence of equalities
\begin{align*}
    \Delta V^\pi(s_0)&=\gamma[P^\pi(s_0)^\top (V_P^\pi - (1-\delta)V_{P'}^\pi) - \delta V_{P'}^\pi(s_1)]\\
&=\gamma[P^\pi(s_0)^\top (V_P^\pi - (1-\delta)V_{P'}^\pi) - \delta V_{P'}^\pi(s_1) \pm \delta P^\pi(s_0)^\top V_P^\pi \pm \delta V_P^\pi(s_1)]\\&=\gamma[P^\pi(s_0)^\top (1-\delta)\Delta V^\pi + \delta\Delta V^\pi(s_1) + \delta (P^\pi(s_0)^\top V_P^\pi- V_P^\pi(s_1))]\\
    &=\gamma P^{'\pi}(s_0)^\top \Delta V^\pi+\underbrace{\gamma\delta(P^\pi(s_0)^\top V_P^\pi - V_P^\pi(s_1))}_{\eqqcolon b_{s_0}}.
\end{align*}
Hence, we can rewrite the expression of $\Delta V^\pi$ in matrix form as
\[
\Delta V^\pi = b + \gamma P^{'\pi} \Delta V^\pi
\]
where $b\in \mathbb{R}^S$, equal to $b_s=0$ for $s\neq s_0$ and $b_{s_0}=\gamma\delta\left(\mathbb{E}_{s'\sim P(s_0,\pi(s_0))}[V_P^\pi(s')] - V_P^\pi(s_1)\right)=-\gamma\delta \rho_r^\pi(s_0,s_1)$.

Rewriting $ (I-\gamma M)\Delta V^\pi = b$, and applying the infinity norm $\|(I-\gamma M)\Delta V^\pi \|_\infty = \|b\|_\infty$, we find that $\|b\|_\infty = \gamma \delta |\rho_r^\pi(s_0,s_1)|$, thus
\[
\gamma \delta |\rho_r^\pi(s_0,s_1)|\leq \|\Delta V^\pi\|_\infty (1-\gamma).
\]
Therefore, if $\delta$ is chosen such that $\delta=2\delta', \delta'\in (0,1/2)$, then for
\[
\frac{\epsilon(1-\gamma)}{\gamma|\rho_r^\pi(s_0,s_1)|} < \delta',
\]
the model is confusing (as long as the l.h.s. is strictly smaller than $1$). This proves the first part of the statement.


\underline{\bf Necessary condition}. As before, let $V_M^\pi$ be the value of $\pi$ in $M$ with a reward $r$ (we omit the subscript $r$ for simplicity), and similarly define $V_{M'}^\pi$.  If $M'$ is confusing for some reward $r\in \rewardspace$, then we have that there exists a state $s$ such that $2\epsilon < |\Delta V^\pi(s)|$, with $\Delta V^\pi(s)=V_P^\pi(s) - V_{M'}^\pi(s)$. Note that the following inequalities hold for any $s$
 \begin{align*}
     |\Delta V^\pi(s)| &\leq \gamma \left|P(s,\pi(s))^\top V_{M}^\pi- P_{r}'(s,\pi(s))^\top V_{M'}^\pi\right|,\\
     &\leq \gamma \left|\Delta P(s,\pi(s))^\top V_{M}^\pi+P'(s,\pi(s))^\top \Delta V^\pi\right| ,\\
     &\leq \gamma  \left|\Delta P(s,\pi(s))^\top V_{M}^\pi\right|  + \gamma \|\Delta V^\pi\|_\infty.
 \end{align*}
Since the inequality holds for all $s$, it implies that $|\Delta V^\pi(s)| \leq \frac{\gamma}{1-\gamma}  \left|\Delta P(s,\pi(s))^\top V_{M}^\pi\right| $. Letting ${\bf e}$ be the vector of ones, and noting that: (1) $\mathbb{E}_{s'\sim P(s,\pi(s))}[V_M^\pi(s')]$ is a constant and (2) $V_{M}^\pi-  \mathbb{E}_{s'\sim P(s,\pi(s))}[V_M^\pi(s')]{\bf e}=\rho_r^\pi(s)$, then
\begin{align*}
    |\Delta V^\pi(s)|& \leq   \frac{\gamma}{1-\gamma}  \left|\Delta P(s,\pi(s))^\top \rho_r^\pi(s)\right|,\\
    &\leq \frac{2\gamma}{1-\gamma}  \|\Delta P(s,\pi(s))\|_{TV} \|\rho_r^\pi(s)\|_\infty.
\end{align*}
Since $\|\Delta P(s,\pi(s))\|_{TV}\leq 1$, we find that  a necessary condition for $M'$ being a confusing model  (for $r$) is that there exists a state $s$ satisfying
\[
\frac{\epsilon(1-\gamma)}{\gamma \|\rho_r^\pi(s)\|_\infty} < \|\Delta P(s,\pi(s))\|_{TV}\leq 1.
\]
\end{proof}
In the following proof we provide an alternative minimax necessary condition for the existance of an alternative model for $(\pi,r)$. To derive the result, we use the following lemma.
\begin{tcolorbox}
\begin{lemma}[Lemma 3 \cite{achiam2017constrained}]
    \label{le:achiam}
        The divergence between discounted future state visitation distributions, $||d^{\pi'}-d^\pi||_1$, is bounded by an average divergence of the policies $\pi'$ and $\pi$:
        \begin{align*}
            ||d^{\pi'}-d^\pi||_1\le \frac{2\gamma}{1-\gamma}\mathbb{E}_{s\sim d^\pi}[\|
        \bar \pi(s)-\bar\pi'(s)\|_{TV}]
        \end{align*}
    \end{lemma}
    \end{tcolorbox}
Then, we have the following result.
\begin{tcolorbox}
\begin{lemma}[A necessary minimax condition for $M'$ to be a confusing model]
\label{le:necessary_condition_confusing_model}
Consider a reward $r$ and a target policy $\pi$. If $M'$ is a confusing model, i.e. $||V^\pi_{M'} - V^\pi_M||_\infty > 2\epsilon$, then there  $\exists s$ such that
    \begin{align*}
       \mathbb{E}_{a\sim\pi(\cdot|s)}[{\rm KL}_{P|P'}(s,a)]>\frac{2(1-\gamma)^2}{\gamma^2}\epsilon^2.
    \end{align*}
\end{lemma}
\end{tcolorbox}
\begin{proof}
The proof relies on constructing an alternative MDP and a policy $\bar \pi$ such that we can transform  the problem of evaluating two different transition function into a problem of comparing two different policies.

For simplicity, we also assume a deterministic policy $\pi$, and explain at the end how to extend the argument to a general stochastic policy.

\paragraph{Imaginary MDP $\bar M$.}
 We begin by constructing an imaginary deterministic MDP $\overline M$ where the action space is $\bar \actionspace=\statespace$.
The transition function of this MDP is $\bar P(s'|s,u)=\mathbf{1}_{\{u=s'\}}$, i.e., taking the action $u$ leads the agent to state $s'$ with probability $1$.
The reward function instead is $\bar r(s,u)=r(s,\pi(s))$.

Define now the policy $\overline{\pi}(u|s) = P(u|s,\pi(s))$. 
    With $\overline M_{\bar r}$, we can convert the value of the policy under the original MDP and confusing models to the value of different policies under $\overline M_{\bar r}$. Specifically, the value of the policy $\pi$ (deterministic) under the MDP $M_r$ with transitions $P$ is 
    \begin{align*}
        V^\pi_{M_r}(s) = V^{\bar \pi}_{\bar M_{\bar r}}(s).
    \end{align*} This follows from the fact that
    \[
     V^{\bar \pi}_{\bar M_{\bar r}}(s) = \sum_{u,s'} \bar \pi(u|s) \bar P(s'|s,u) [\bar r(s,u) + \gamma V_{\bar M{\bar r}}^{\bar \pi}(s')] = \sum_{u} P(u|s,\pi(s))  [r(s,\pi(s)) + \gamma V_{\bar M_{\bar r}}^{\bar \pi}(u)].
    \]
    Then, by an appropriate application of  the Bellman operator one can see that $V^{\bar \pi}_{\bar M_{\bar r}}(s) =V^{ \pi}_{M_r}(s) $
    
\paragraph{Bounding using the difference lemma.}
    Consider now a confusing model $M_r'\in {\rm Alt}_{\pi,r}^\epsilon(M)$ with transition $P'$, and  define a policy $\bar \pi'(u|s)=P'(s|s,\pi(s))$.
    Hence, we have
    \begin{align}
    \label{eq:mdp_to_policy}
        |V^\pi_{M_r}(s)-V^\pi_{M_r'}(s)|=|V^{\bar \pi}_{\bar M_{\bar r}}(s)- V^{\bar \pi'}_{\bar M_{\bar r}}(s)|.
    \end{align}

    Based on \cref{le:achiam} and \cref{eq:mdp_to_policy}, we have $\forall s$ 
    \begin{align}
        |V^\pi_{M_r}(s)-V^\pi_{M_r'}(s)|&=|V_{\bar M_{\bar r}}^{\bar\pi}(s)-V_{\bar M_{\bar r}}^{\bar \pi'}(s)|, \\
        &\le ||d_{\bar M_{\bar r}}^{\bar \pi} - d_{\bar M_{\bar r}}^{\bar \pi'}||_1, \\
        &\le \frac{2\gamma}{1-\gamma}\mathbb{E}_{s'\sim d_{\bar M_{\bar r}}^\pi}[\|
        \bar \pi(s)-\bar\pi'(s)\|_{TV}],
    \end{align}
    where $d_{\bar M_{\bar r}}^{\pi}$ denotes the visitation distribution induced by $\pi$ in $\bar M_{\bar r}$.
    
    If $\forall s$, we have $\|\bar\pi(s)-\bar\pi'(s)\|_{TV}\le \frac{1-\gamma}{\gamma}\epsilon$, then $|V^\pi_{M_r}(s)-V^\pi_{M_r'}(s)|\le 2\epsilon,\forall s$, i.e. $M_r'$ is not a confusing model.  Hence we have
    \[
    \frac{1-\gamma}{\gamma}\epsilon \leq\|\bar\pi(s)-\bar\pi'(s)\|_{TV}=\|P(s,\pi(s))-P'(s,\pi(s))\|_{TV} \leq \sqrt{\frac{1}{2} {\rm KL}_{P|P'}(s,\pi(s))}. 
    \]

\paragraph{Extension to a stochastic policy.} The extension to a stochastic policy involves a few more steps, and it is omitted for simplicity. It follows from defining an imaginary MDP $\bar M$ with $\bar P(s'|s,u)=\mathbf{1}_{\{u=s'\}}$, $\bar r(s,u)=\sum_{a'}r(s,a')\pi(a'|s)$ and $\bar \pi(u|s)=\sum_a P(u|s,a)\pi(a|s)$. The argument concludes by noting that if $M$ is confusing then 
\[
 \frac{(1-\gamma)^2}{\gamma^2}\epsilon\leq \| \bar \pi(s)-\bar\pi'(s)\|_{TV}^2 \leq  \frac{1}{2}{\rm KL}(\bar \pi(s), \bar \pi'(s)) \leq \frac{1}{2}\mathbb{E}_{a\sim \pi(\cdot|s)}\left[{\rm KL}_{P|P'}(s,a)\right].
\]
by the data-processing inequality.
\end{proof}

\subsubsection{Additional Results on the Value Deviation $\rho$}
\label{app:value_deviation}

\paragraph{Value deviation.} To analyze these confusing sets, and their implications for sample complexity, we define the following instance-dependent quantity, that we refer to as the \emph{one-step value deviation}:
\begin{align*}\rho_r^\pi(s,a,s')&\coloneqq V_r^\pi(s') - \mathbb{E}_{\hat s \sim P(s,a)}[V_r^\pi(\hat s)] \quad\forall s,s'\in\statespace,\\
\rho_r^\pi(s,s')&\coloneqq \rho_r^\pi(s,\pi(s),s').
\end{align*}
This quantity measures how much the value at $s'$ differs from the expected value of the next state when starting at $s$.  

We also define these quantities in vector form
$
\rho_r^\pi(s) \coloneqq \begin{bmatrix}
    \rho_r^\pi(s,s_1) &\dots &\rho_r^\pi(s,s_S)
\end{bmatrix}^\top$,
so that $\|\rho_r^\pi(s)\|_\infty= \max_{s'} |\rho_r^\pi(s,s')|$ is the maximum
one-step deviation at $s$ (similarly, one defines $\rho_r^\pi(s,a)$).
The deviation $\rho_r^\pi$ is closely related to the \emph{span} of the value function \begin{equation}
    {\rm sp}(V_r^\pi) \coloneqq \max_{s'}V_r^\pi(s')-\min_{s'} V_r^\pi(s),
\end{equation}
but, is in general smaller. In the following lemma we also show that it is unlikely that $\max_{s,s'}\rho_r^\pi(s,s')$ is achieved for $s'=s$, but rather for $s'\neq s$. 
Depending on the characteristics of the MDP, it is more plausible that $\rho_r^\pi(s,s)\approx 0$. 
\begin{tcolorbox}
\begin{lemma}\label{lemma:bound_rho}
        For any reward vector $r \in [0,1]^S$, states $s,s'\in \statespace$, we have:
     (I) $|\rho_r^\pi(s,s)| \leq 1$; (II) 
     $|\rho_r^\pi(s,s')|\leq {\rm sp}(V_r^\pi)$ and (III) \[|\rho_r^\pi(s,s')-\rho_r^\pi(s',s)|\leq  |\Delta_r^\pi(s,s')|+ \Gamma_{s,s'}^\pi{\rm sp}(V_r^\pi),\] where $\Delta_r^\pi(s,s')\coloneqq r(s,\pi(s))-r(s',\pi(s'))$ and $\Gamma_{s,s'}^\pi \coloneqq \frac{1+\gamma}{2}\max_{s,s'}\|P(s,\pi(s))- P(s',\pi(s'))\|_1$.
\end{lemma}
\end{tcolorbox}
\begin{proof}[Proof of \cref{lemma:bound_rho}]
    {\bf First part.} For the first part of the lemma, note that
\[
\rho_r^\pi(s,s) = V_r^\pi(s) -  P(s,\pi(s))^\top V_r^\pi = r(s,\pi(s)) + (\gamma-1)P(s,\pi(s))^\top V_r^\pi.
\]
Since $P(s,\pi(s))^\top V_r^\pi\geq 0$, and  $\gamma \in (0,1)$, then $(\gamma-1)P(s,\pi(s))^\top V_r^\pi\leq 0$. Using that the reward is bounded in $[0,1]$, we obtain $\rho_r^\pi(s,s)\leq 1$.

Then, using that $0\leq V_r^\pi(s)\leq 1/(1-\gamma) \Rightarrow  -1 \leq (\gamma-1)V_r^\pi(s) $  we also have 
 $\rho_r^\pi(s,s) \geq r(s,\pi(s)) -1 \geq -1$. Thus $|\rho_r^\pi(s,s)|\leq 1$.

{\bf Second part.} The second part is rather straightforward, and follows from the definition of span ${\rm sp}(V_r^\pi)=\max_s V_r^\pi(s) - \min_s V_r\pi(s)$. Indeed we have
\[
\rho_r^\pi(s,s')=V_r^\pi(s')-P(s,\pi(s))^\top V_r^\pi \leq \max_s V_r^\pi(s) -\min_s V_r^\pi(s)={\rm sp}(V_r^\pi)
\]
and
\[
\rho_r^\pi(s,s')\geq \min_{s}V_r^\pi(s)-\max_{s} P(s,\pi(s))^\top V_r^\pi\geq -{\rm sp}(V_r^\pi).
\]

{\bf Third part.} For the third and last part we first note the rewriting 
\begin{align*}
\rho_r^\pi(s,s') -\rho_r^\pi(s',s)  &=V_r^\pi(s')-V_r^\pi(s)+ \left[P(s',\pi(s')) -P(s,\pi(s)) \right]^\top V_r^\pi,\\
&=r(s',\pi(s'))-r(s,\pi(s)) + (\gamma+1)\left[P(s',\pi(s')) -P(s,\pi(s)) \right]^\top V_r^\pi,\\
\end{align*}
Then, define ${\cal Z}(s,s') \coloneqq \{z\in\statespace: P(z|s,\pi(s)) \geq P(z|s',\pi(s'))\}$ and
\[
P_+^\pi(s,s') \coloneqq \sum_{z \in {\cal Z}(s,s')} P(z|s',\pi(s')) - P(z|s,\pi(s)),\quad P_-^\pi(s,s') \coloneqq \sum_{z \in \statespace\setminus {\cal Z}(s,s')} P(z|s',\pi(s')) - P(z|s,\pi(s)).
\]
Also observe $P_+^\pi(s,s')=-P_-^\pi(s,s')$, and $\|P(s,\pi(s)) - P(s',\pi(s'))\|_1= P_+^\pi(s,s')- P_-^\pi(s,s')$, from which follows that $P_+^\pi(s,s')=\frac{1}{2}\|P(s,\pi(s)) - P(s',\pi(s'))\|_1$.
Then, we have that
\begin{align*}
    \left[P(s',\pi(s')) -P(s,\pi(s)) \right]^\top V_r^\pi &= \sum_{z} \left[P(z|s',\pi(s')) - P(z|s,\pi(s))\right]V_r^\pi(z),\\
    &=\sum_{z\in {\cal Z}(s,s')} \left[P(z|s',\pi(s')) - P(z|s,\pi(s))\right]V_r^\pi(z) \\&\qquad\qquad+ \sum_{z\in \statespace\setminus{\cal Z}(s,s')} \left[P(z|s',\pi(s')) - P(z|s,\pi(s))\right]V_r^\pi(z),\\
    &\leq P_+^\pi(s,s') \max_s V_r^\pi(s) + \sum_{z\in \statespace\setminus{\cal Z}(s,s')} \underbrace{\left[P(z|s',\pi(s')) - P(z|s,\pi(s))\right]}_{<0}V_r^\pi(z),\\
    &\leq P_+^\pi(s,s') \max_s V_r^\pi(s) + \sum_{z\in \statespace\setminus{\cal Z}(s,s')} \left[P(z|s',\pi(s')) - P(z|s,\pi(s))\right]\min_s V_r^\pi(s),\\
    &\leq P_+^\pi(s,s') \max_s V_r^\pi(s) + \underbrace{P_-^\pi(s,s')}_{=-P_+^\pi(s,s')}\min_s V_r^\pi(s),\\
    &\leq P_+^\pi(s,s') {\rm sp}(V_r^\pi),\\
    &\leq \frac{1}{2}\|P(s,\pi(s)) - P(s',\pi(s'))\|_1{\rm sp}(V_r^\pi).
\end{align*}
By taking the absolute value one obtains the result.
\end{proof}


Another useful metric is the dispersion factor $\lambda_r^\pi$, which is defined as 
\[
\lambda_r^\pi \coloneqq \frac{\min_{s} V_r^\pi(s)}{\max_s V_r^\pi(s)},
\]
from which we see its relationship with the span, i.e., ${\rm sp}(V_r^\pi)/\max_s V_r^\pi(s) = 1- \lambda_r^\pi$.

The parameter $\lambda_r^\pi$ measures the spread of the value function across different states. Specifically, a smaller $\lambda_r^\pi$ indicates a greater dispersion of values among states.



\begin{tcolorbox}
\begin{lemma}
    The dispersion factor $\lambda_r^\pi$ satisfies:
    \begin{enumerate}
        \item $V_r^\pi(s')\geq \lambda_r^\pi V_r^\pi(s)$ for any pair $s,s'\in \statespace$.
        \item $\max_s V_r^\pi(s)- V_r^\pi(s')\leq (1-\lambda_r^\pi) \max_s V_r^\pi(s)$ for any state $s'$.
    \end{enumerate}
\end{lemma}
\end{tcolorbox}
\begin{proof}
First, clearly $\lambda_r^\pi \in [0,1]$. Then, the first property is derived from the following inequalities that hold for any pair $(s,s')$:
    \[
    \lambda_r^\pi V_r^\pi(s)\leq \lambda_r^\pi \max_s V_r^\pi(s)= \min_{s} V_r^\pi(s) \leq  V_r^\pi(s').
    \]

    The second statement stems from the simple fact that  $-V_r^\pi(s') \leq -\min_s V_r^\pi(s)$ for any $s'$. Hence:
    \[
     \max_s V_r^\pi(s)-V_r^\pi(s') \leq \max_s V_r^\pi(s)- \min_{s} V_r^\pi(s)  = \max_s V_r^\pi(s)- \lambda_r^\pi\max_{s} V_r^\pi(s).
    \]
\end{proof}
Using this metric, we are able to provide the following result, providing a bound on $\rho_r^\pi(s,s')$ (note that one could  rewrite the following result in terms of the span).
\begin{tcolorbox}
    
\begin{lemma}
    For any reward vector $r \in [0,1]^S$, we have:$\rho_r^\pi(s,s') \in \left[-\frac{1-\lambda_r^\pi \gamma}{1-\gamma}, \max\left(1, \frac{1 -\lambda_r^\pi}{1-\gamma} \right)\right]$ for all $s,s' \in \statespace$.
    Furthemore, for $\lambda_r^\pi=\gamma$ we have that $|\rho_r^\pi(s,s')|\leq 2$ for all $s,s'$.
\end{lemma}
\end{tcolorbox}
\begin{proof}
{\bf First part.} For the first part of the lemma, note that
\[
\rho_r^\pi(s,s) = V_r^\pi(s) -  P(s,\pi(s))^\top V_r^\pi = r(s,\pi(s)) + (\gamma-1)P(s,\pi(s))^\top V_r^\pi.
\]
Since $P(s,\pi(s))^\top V_r^\pi\geq 0$, and  $\gamma \in (0,1)$, then $(\gamma-1)P(s,\pi(s))^\top V_r^\pi\leq 0$. Using that the reward is bounded in $[0,1]$, we obtain $\rho_r^\pi(s,s)\leq 1$.

Then, using that $0\leq V_r^\pi(s)\leq 1/(1-\gamma) \Rightarrow  -1 \leq (\gamma-1)V_r^\pi(s) $  we also have 
 $\rho_r^\pi(s,s) \geq r(s,\pi(s)) -1 \geq -1$. Thus $|\rho_r^\pi(s,s)|\leq 1$.

 {\bf Second part.} For the second part of the lemma let $P_s^\pi=P(s,\pi(s))$ be the vector of transition probabilities in $s$ under action $\pi(s)$. We first prove that $V_r^\pi(s)\geq \lambda_r^\pi V_r^\pi(s') \Rightarrow (P_s^\pi)^\top V_r^\pi \geq \lambda_r^\pi (P_{s'}^\pi)^\top V_r^\pi$ . The proof is simple, and follows from the following inequalities that hold for any pair $(s,\hat s)\in \statespace^2$:
 \[(P_s^\pi)^\top V_r^\pi\geq \min_{s'} V_r^\pi(s')\geq \lambda_r^\pi \max_{s'} V_r^\pi(s')\geq \lambda_r^\pi (P_{\hat s}^\pi)^\top V_r^\pi.\]

We can use this fact to prove the result. From the proof of \cref{prop:rho_zero_ness_succ} we know that
 \[
\rho_r^\pi = {\bf 1}(V_r^\pi)^\top - (P^\pi V_r^\pi){\bf 1}^\top = {\bf 1}(r+\gamma P_\pi V_r^\pi)^\top - (P^\pi V_r^\pi){\bf 1}^\top,
\]
where $r$ is the vector of rewards.  Let $W= P_\pi V_r^\pi$ and note the two facts (1) $W_s  =(P_s^\pi)^\top V_r^\pi$  and (2) $0\leq W_s\leq 1/(1-\gamma)$. Then, one can see that $(\rho_r^\pi)_{s,s'}$ is
\[
(\rho_r^\pi)_{s,s'}=\rho_r^\pi(s,s')=r(s') + \gamma W_{s'} - W_s.
\]
We prove the statement in two steps:
\begin{enumerate}
    \item We first prove $\rho_r^\pi(s,s')\leq \max\left(1, \frac{1 -\lambda_r^\pi}{1-\gamma} \right)$. Using that for any pair $(s,s')$ we have $\lambda_r^\pi W_{s'} \leq W_s$ it follows that   
\begin{align*}r(s') + \gamma W_{s'} - W_s &=r(s') + \gamma W_{s'} - W_s \pm \lambda_r^\pi W_{s'},\\
&=r(s') + (\gamma-\lambda_r^\pi) W_{s'} + \underbrace{\lambda_r^\pi W_{s'}- W_s}_{\leq 0},\\
&\leq r(s') + (\gamma-\lambda_r^\pi) W_{s'},\\
&\leq 1+ \max\left(0,\frac{\gamma -\lambda_r^\pi}{1-\gamma}\right),\\
&\leq  \max\left(1, \frac{1 -\lambda_r^\pi}{1-\gamma} \right).
\end{align*}
\item We now  prove a lower bound on $\rho_r^\pi(s,s')$. The idea is to seek  a value of $\eta \geq 0$ such that $\gamma W_{s'} -W_s \geq (\gamma-1)W_s \eta$, which implies $\gamma W_{s'}-W_s\geq -\eta$ (using that $-W_s\geq -1/(1-\gamma)$). 
Then, we find 
$
W_{s'}  \geq \frac{(\eta \gamma-\eta +1)}{\gamma}W_s$. Since $W_{s'}\geq \lambda_r^\pi W_s$, we can set
\[
\frac{(\eta \gamma-\eta +1)}{\gamma} = \lambda_r^\pi \Rightarrow \eta=\frac{\lambda_r^\pi \gamma-1}{\gamma-1},
\]
(notably, if $\lambda_r^\pi=\gamma$, then $\eta = 1+\gamma \leq 2$).
Lastly, we obtain $ \rho_r^\pi(s,s') \geq r(s')+\gamma W_{s'}-W_s\geq -\eta=-\frac{1-\lambda_r^\pi \gamma}{1-\gamma}$, which concludes the proof.
\end{enumerate}


\end{proof}
Hence, if the value is similar across states, we can expect a smaller sample complexity.


\subsubsection{Proof of \cref{prop:rho_zero_ness_succ} and related results}
\label{app:prop:rho_zero_ness_succ}
In the following proof, we prove \cref{prop:rho_zero_ness_succ}, which characterizes the set of rewards for which $\max_s\|\rho_r^\pi(s)\|_\infty = 0$. In the proof we define the following deviation matrix
\begin{equation}
\rho_r^\pi = \begin{bmatrix} \rho_r^\pi(s_1)& \dots & \rho_r^\pi(s_S)\end{bmatrix}^\top,
\end{equation}
that is used to prove the proposition.

\begin{proof}[Proof of \cref{prop:rho_zero_ness_succ}]
First, note that
    \begin{align*}
    \rho_r^\pi(s,s')&= V_{M_r}^\pi(s')- \mathbb{E}_{\hat s\sim P(s,\pi(s))}[V_{r}^\pi(\hat s)],\\
    &=  (e_{s'} - P_s )^\top V_{r}^\pi ,
\end{align*}
where $e_s$ is the $s$-th element of the canonical basis, $P_s=P(s,\pi(s))$. Hence, we can write $\rho_r^\pi(s)$ as
\[
\rho_r^\pi(s) = V_{r}^\pi - P_s^\top V_r^\pi {\bf 1}
\]
where ${\bf 1}$ is the vector of ones. Therefore
\[
\rho_r^\pi = {\bf 1}(V_r^\pi)^\top - (P^\pi V_r^\pi){\bf 1}^\top,
\]
where $P^\pi$ is a $S\times S$ matrix satisfying $(P^\pi)_{s,s'}=P(s'|s,\pi(s))$.
 We consider the condition $\rho_r^\pi=0$ being identically $0$, that is
\[
{\bf 1}(V_r^\pi)^\top = (P^\pi V_r^\pi){\bf 1}^\top \iff V_r^\pi = (P_s^\pi V_r^\pi) {\bf 1} \quad \forall s\in \statespace.
\]
Since $(P_s^\pi V_r^\pi) {\bf 1} = {\bf 1}(P_s^\pi)^\top V_r^\pi$, letting $M_s^\pi= {\bf 1}(P_s^\pi)^\top $, then $\rho_r^\pi = 0 \iff V_r^\pi = M_s^\pi V_r^\pi$ for every $s\in\statespace$, that is, $V_r^\pi$ is a right eigenvector of $M_s^\pi$ with eigenvalue $1$ for all $s\in\statespace$. Since $M_s^\pi$ has rank $1$ for every $s$, then $V_r^\pi$ is the only right eigenvector for eigenvalue $1$ for all states.

Now, one can easily see that $M_s^\pi {\bf 1}= {\bf 1}(P_s^\pi)^\top {\bf 1}={\bf 1}$, thus the right eigenvector of $M_s^\pi$ associated to the eigenvalue $1$ is the ones vector ${\bf 1}$ itself, for all states.


To translate this condition onto a condition on the rewards, we use the fact that  $V_r^\pi=G^\pi r$, with $G^\pi=(I-\gamma P^\pi)^{-1}$ (which is invertible). Therefore, we require $r$ to satisfy
\[
{\bf 1}=  G^\pi r \iff (I-\gamma P^\pi) {\bf 1}=r.
\]





\paragraph{Conclusion.}
Using that $P^\pi{\bf 1}={\bf 1}$, we conclude  that
\[
\rho_r^\pi = 0 \iff r \propto  {\bf 1}.
\]
The set of all vectors $r$ for which $\rho_r^\pi=0$ is precisely the one-dimensional subspace $\{\alpha {\bf 1} : \alpha \in \mathbb{R}\}$. Thus, $r$ must be a scalar multiple of ${\bf 1}$.
\end{proof}



We conclude this subsection by characterizing in what cases, $\rho_r^\pi(s,s)$ is identically zero across  states. First, note the following result.
\begin{tcolorbox}
\begin{lemma}\label{lemma:rho_r_pi}
    Let $ {\rm diag}(\rho_r^\pi) = \begin{bmatrix} \rho_r^\pi(s_1,s_1) & \dots \rho_R^\pi(s_S,s_S)\end{bmatrix}^\top$. We have that
    \begin{equation}
        {\rm diag}(\rho_r^\pi) = (I-P^\pi)(I-\gamma P^\pi)^{-1}r.
    \end{equation}
\end{lemma}
\end{tcolorbox}
\begin{proof}
First, note that
    \begin{align*}
    \rho_r^\pi(s,s')&= V_{M_r}^\pi(s')- \mathbb{E}_{\hat s\sim P(s,\pi(s))}[V_{M_r}^\pi(\hat s)],\\
    &=  (e_{s'} - P_s )^\top V_{M_r}^\pi ,\\
    &= (e_{s'} - P_s )^\top G^\pi r ,
\end{align*}
where $e_s$ is the $s$-th element of the canonical basis, $P_s=P(s,\pi(s))$ and in the last step we used the fact that  $V_{M_r}^\pi=G^\pi r$ with $G^\pi=(I-\gamma P^\pi)^{-1}$.

Since for any $s$ we can write $\rho_r^\pi(s,s)=(e_s-P_s)^\top G^\pi r$, it follows that $\rho_r^\pi(s,s)=[(I-P^\pi)G^\pi r]_s$, and thus ${\rm diag}(\rho_r^\pi)=(I-P^\pi)G^\pi r$.
\end{proof}
Then, we have the following characterization.

\begin{tcolorbox}
    
\begin{proposition}
Consider a partition  of the set of recurrent states of $P^\pi$ into $m$ disjoint closed irreducible sets $C_1,\dots, C_m$. Define the vector $v_i\in \mathbb{R}^S$  as $(v_i)_s \coloneqq  1_{(s\in C_i)}$. Let $\mathbf{e} \in \mathbb{R}^S $ denote the all-ones (unit) vector. Then, for any non-zero reward vector $ r \in \mathbb{R}^S $, we have that:
    \[
        \max_s |\rho_r^\pi(s,s)| = 0 \iff r \in {\rm span}\left\{ \mathbf{e}, \, v_{i_1}, v_{i_2}, \dots, v_{i_{m-1}} \right\},
    \]
    for some distinct indices $ i_1, i_2, \dots, i_{m-1} \in \{1, 2, \dots, m\} $. In other words, the reward  $ r $ lies in the span of the unit vector $\mathbf{e}$ and any $m-1$ vectors selected from $ \{v_1, v_2, \dots, v_m\}$.
\end{proposition}
\end{tcolorbox}
In other words, the one-step value deviation at state $s$ is zero provided that the reward function is constant on each irreducible closed set of $P^\pi$
\begin{proof}
From \cref{lemma:rho_r_pi}, we know that 
\[        {\rm diag}(\rho_r^\pi) = (I-P^\pi)(I-\gamma P^\pi)^{-1}r.\]
Then, let $x=(I-\gamma P^\pi)^{-1} r$ and consider the solutions to $(I-P^\pi) x= 0$. In other words, we are interested in the real-valued right eigenvectors of $P^\pi$ associated to the eigenvalue $1$, that is, $P^\pi x= x$. Then, for any such right eigenvector we obtain
\[
(I-\gamma P^\pi)x =r \Rightarrow (1-\gamma)x=r.
\]
Hence, since $P^\pi x=x$, it follows that  $r$ must satisfy $P^\pi r=r$, and thus be a right eigenvector of $P^\pi$ associated to its eigenvalue $1$.

Therefore, finding the reward vectors satisfying $\|{\rm diag}(\rho_r^\pi)\|_\infty=0$ amounts to finding the real-valued right eigenvectors of $P^\pi$ associated to its eigenvalue $1$.

In general, for an irreducible stochastic transition matrix $P^\pi$ it is well-known \cite{seneta2006non} that the only real-valued eigenvector associated to the eigenvalue $1$ is the unitary vector, and thus $r_\lambda$ is a solution for any $\lambda\in \mathbb{R}$.

For a generic $P^\pi$, the above argument can be extended by partitioning the set of recurrent states into $m$ disjoint closed irreducible sets $C_1,\dots, C_m$. Therefore, as shown in \cite{puterman2014markov}, the state space can be rewritten as $\statespace = C_1\cup C_2\cup \dots C_m \cup T$, where $T$ is the set of transient states. After relabeling the states, we can express the transition matrix $P^\pi$ as
\[
P^\pi = \begin{bmatrix}
    P_1 & 0 &\dots & 0 & 0\\
    0 & P_2 &\dots & 0 & 0\\
    \vdots & &\ddots & &  \vdots\\
    0 & 0 &  \dots & P_m & 0 \\
    Q_1 & Q_2 & \dots & Q_m & Q_{m+1}
\end{bmatrix}
\]
where $P_i$ corresponds to the transition function in $C_i$, $Q_i$ to transitions from states in $T$ to states in  $C_i$, and $Q_{m+1}$ to transition between states in $T$ \cite{puterman2014markov}. Moreover, we also know from \citep[Theorem A.5]{puterman2014markov} that the eigenvalue $1$ has multiplicity $m$. Since each $P_i$ is an irreducible stochastic matrix, there is only one right eigenvector associated to $1$, and it is the vector $v_i$, defined as $(v_i)_s = \mathbf{1}_{(s\in C_i)}$. Since also the unitary vector $\mathbf{e}=\begin{bmatrix} 1 & 1 &\dots & 1\end{bmatrix}^\top$ satisfies $P^\pi \mathbf{e}=\mathbf{e}$, we have that
 $\{v_1,\dots, v_{m-1}, \mathbf{e}\}$ are  $m$ linearly independent eigenvectors of $P^\pi$ associated to the eigenvalue $1$. Thus, any  $r\in {\rm span}\{v_1,\dots, v_{m-1},\mathbf{e}\}$ yields $\|{\rm diag}(\rho_r^\pi)\|_\infty=0$.
\end{proof}

\subsection{Sample Complexity Results}\label{app:subsec:sample_complexity}
Now we provide proofs for the sample complexity lower bound, and related results.
\subsubsection{Proof of \cref{thm:sample_complexity_lb}}\label{app:thm:sample_complexity_lb}
\begin{proof}[Proof of \cref{thm:sample_complexity_lb}]
The initial part of the proof follows the same technique as in \cite{al2021navigating,russo2023model}, and leverages change of measure arguments \cite{lai1985asymptotically, garivier2016optimal}.

Consider a policy-reward pair $\pi\in \Pi, r\in {\cal R}_\pi$.
 For a confusing model $M_{\pi,r}'\in {\rm Alt}_{\pi,r}^\epsilon(M)$ consider the log-likelihood ratio up to time $t$ of a sequence of observations $(z_0,z_1,\dots)$, with $z_i = (s_i,a_i)$,  under the original MDP $M_r$ and an alternative model $M_{\pi,r}'\in {\rm Alt}_{\pi,r}(M)$:
 \[
    L_{t,\pi,r}= \sum_{n=1}^t \log\frac{P(s_n|z_{n-1})}{P_{\pi,r}'(s_n|z_{n-1})}.
    \]
Then, as in \cite{russo2023model}, one can show that for all $t\in \mathbb{N}$ the following equality holds
\begin{align*}
\mathbb{E}_{M}[L_{t,\pi,r}] &= \sum_{s,a}\sum_{t=0}^\infty \mathbb{P}_{M}[N_t(s,a)\geq t]{\rm KL}_{P|P_{\pi,r}'}(s,a),\\
&=  \sum_{s,a}\mathbb{E}_{M}[N_t(s,a)]{\rm KL}_{P|P_{\pi,r}'}(s,a).
\end{align*}
Now we proceed to lower bound the expected log-likelihood ratio at the stopping time $\tau$.  We indicate by $\mathbb{P}_{M_{\pi,r}'}$ the measure induced by $M_{\pi,r}'$. Applying the information processing inequality in \cite{kaufmann2016complexity},   we  lower bound the expected log-likelihood at the stopping time $\tau$ as
\[
\mathbb{E}_{M}[L_{\tau,\pi,r}] \geq {\rm kl}(\mathbb{P}_{M}({\cal E}), \mathbb{P}_{M_{\pi,r}'}({\cal E}))
\]
for some event ${\cal E}$ that is ${\cal F}_\tau$-measurable.
Then, define the event
 ${\cal E}=\{\exists \pi\in \Pi,r\in {\cal R}_\pi: \|V_{M_r}^\pi - \hat V_r\|_\infty > \epsilon\}$. By definition of $(\epsilon,\delta)$-PAC algorithm we have $\mathbb{P}_M({\cal E})\leq \delta$. If we can show that $\mathbb{P}_{M_{\pi,r}'}({\cal E})\geq 1-\delta$, then we can lower bound ${\rm kl}(\mathbb{P}_{M}({\cal E}), \mathbb{P}_{M_{\pi,r}'}({\cal E}))\geq {\rm kl}(\delta,1-\delta)$ by the monotonicity properties of the KL-divergence.

 To show $\mathbb{P}_{M_{\pi,r}'}({\cal E})\geq 1-\delta$, few steps are needed:
 \begin{align*}
         \mathbb{P}_{M_{\pi,r}'}({\cal E}) & \stackrel{(a)}{\geq} \max_{\bar\pi\in \Pi,\bar r\in {\cal R}_{\bar\pi}} \mathbb{P}_{M_{\pi,r}'}(\|V_{M_{\bar r}}^{\bar\pi} - \hat V_{\bar r}\|_\infty > \epsilon),\\
         &\stackrel{(b)}{\geq} \max_{\bar\pi\in \Pi,\bar r\in {\cal R}_{\bar\pi}} \mathbb{P}_{M_{\pi,r}'}(\|V_{M_{\bar r}}^{\bar\pi} - V_{M_{\bar r}'}^{\bar\pi}\| -\|V_{M_{\bar r}'}^{\bar\pi}- \hat V_{\bar r}\|_\infty > \epsilon),\\
         &\geq 
         \mathbb{P}_{M_{\pi,r}'}(\|V_{M_{ r}}^\pi - V_{M_{ r}'}^\pi\| -\|V_{M_{ r}'}^\pi- \hat V_{ r}\|_\infty > \epsilon),\\
         &\stackrel{(c)}{\geq} \mathbb{P}_{M_{\pi,r}'}(\|V_{M_{ r}'}^\pi- \hat V_{ r}\|_\infty < \epsilon),\\
         &\geq \min_{\pi\in \Pi, r\in {\cal R}_\pi} \mathbb{P}_{M_{\pi,r}'}(\|V_{M_{ r}'}^\pi- \hat V_{ r}\|_\infty < \epsilon),\\
         &\stackrel{(d)}{\geq}  \mathbb{P}_{M_{\pi,r}'}(\forall \pi\in \Pi,r\in {\cal R}_\pi:\|V_{M_{ r}'}^\pi- \hat V_{ r}\|_\infty < \epsilon)\\&=1-  \mathbb{P}_{M_{\pi,r}'}(\exists \pi\in \Pi, r\in{\cal R}_\pi:\|V_{M_{ r}'}^\pi- \hat V_{ r}\|_\infty \geq  \epsilon)\geq 1-\delta.
     \end{align*}
 where (a) and (d) follow from the FrÃ©chet inequalities; (b) is an application of  the triangle inequality; (c) follows from the fact that $M_{\pi,r}'$ is confusing for  $(\pi,r)$ and the last inequality from the definition of $(\epsilon,\delta)$-PAC algorithm.
 Henceforth,  for a  $(\epsilon,\delta)$-PC algorithm one can conclude that $\mathbb{E}_{M}[L_{\tau,\pi,r}] \geq  {\rm kl}(\delta,1-\delta)$.


As the inequality above holds for all $\pi\in \Pi, r\in {\cal R}_\pi$, we have
\begin{align*} \inf_{\pi\in \Pi, r\in \rewardspace_\pi,M_r'\in{\rm Alt}_{\pi,r}^\epsilon(M)}&\sum_{s,a}\mathbb{E}_{M}[N_\tau(s,a)]{\rm KL}_{M|M_r'}(s,a)\\
 &\quad\geq {\rm kl}(\delta,1-\delta).
\end{align*}

Lastly, let $\omega(s,a) = \mathbb{E}_M[N_\tau(s,a)]/\mathbb{E}_M[\tau]$.
By optimizing the l.h.s. of the previous inequality  over all possible allocations $\omega \in \Omega$, we have
$
    \mathbb{E}_M[\tau]T_\epsilon^\star(M)^{-1} \geq {\rm kl}(\delta,1-\delta)$.
%where ultimately we also optimized over the sampling distribution $\omega(s,a)$, yielding the desired result. 
Finally, taking the limit as $\delta\to 0$ yields the result as in \cite{al2021navigating}.
\end{proof}




\subsubsection{Optimality of  behavior policies and mixture policies}
\label{app:optimality_behavior_policies}
An interesting question is whether there are some instances where a mixture of the target policies in $\Pi$ achieves the optimal behavior policy  $\pi_{\rm opt}(s,a)\coloneqq \omega_{\rm opt}(s,a)/\sum_b \omega_{\rm opt}(s,b)$.

For instance, consider the MDP in \cref{fig:example_non_convex_mdp} and the  single target policy $\pi(\cdot|s)=a_2\;\forall s$. For that MDP,  it is \emph{in general} sub-optimal to sample according to such policy, since under $\pi$ state $s_1$ becomes transient. In the following lemma, proved in \cref{app:subsec:sample_complexity}, we formalize this intuition, and provide optimality condition for the following policy mixture
  $
    \pi_\alpha(a | s) 
    \coloneqq
    \sum_{i=1}^N \alpha_{i,s}\mathbf{1}_{\{\pi_i(s)=a\}}
    $
    where $\alpha_{i,s} \ge 0$ and $
    \sum_{i=1}^N \alpha_{i,s} = 1$ for all $s\in \statespace$. 
    \begin{tcolorbox}
\begin{lemma}\label{lemma:optimality_pi_1}
 If the policy $\pi_\alpha$ does not induce a Markov chain with a closed class over $\statespace$, then the state-action visitation distribution induced by $\pi_\alpha$ is not optimal as $\delta\to 0$.
\end{lemma}
\end{tcolorbox}
\begin{proof}[Proof of \cref{lemma:optimality_pi_1}]
    Let $C^\pi(s)$ be the recurrent class (i.e., closed) induced by $\pi$ when starting in state $s$, and assume that $C^\pi(s)$ is a strict subset of $\statespace$. First, note that there cannot exists $s'$ such that $C^\pi(s')=\statespace$, since that would also imply that $C^\pi(s)=\statespace$ by the fact that any state is reachable under $\pi$. 

    Therefore, for any starting state $s_0$ there exists some transient state $s'(s_0)$. Since $s'(s_0)$ is a transient state, one can show that the expected  number of returns of the Markov chain to state $s'(s_0)$ is bounded, i.e., $\sum_a\mathbb{E}_M^\pi[N_t(s'(s_0),a)] <\infty$ for all $t\geq 1$.
\end{proof}

Clearly, one can also immediately see that there exists an optimal mixture $\alpha^\star$ of the target policies, so that $\pi_{\alpha^\star}=\pi_{\rm opt}$, as long as, $\forall s\in \statespace$, the distribution $\omega_{\rm opt}(\cdot|s)$ has support only on the actions in 
    $\{\pi(s): \pi \in \Pi\}$.

    We conclude this section with a sufficient condition on the optimality of a general behavior policy $\pi_\beta$, when used to evaluate a single policy $\pi$ on a single reward. What the proposition points out is that $\pi_\beta$ needs to induce a sampling distribution $d^{\pi_\beta}$ that with large sampling rates where $\|\rho_r^\pi(s)\|_\infty$ is sufficiently large. The extension to multi-reward follows naturally.
\begin{tcolorbox}
    \begin{proposition}
Consider a behavior policy $\pi_\beta$ and a target policy $\pi$.
Let $s_0=\argmin_s d^{\pi_\beta}(s)$, where $d^{\pi_\beta}$ is the stationary distribution induced by ${\pi_\beta}$.
    Assume that: (1) $\|\rho_r^{\pi}(s_0)\|_\infty >3\epsilon(1-\gamma)/\gamma$; (2) $\epsilon>\frac{\gamma}{(1-\gamma)\sqrt{\kappa_0}}$ with $\kappa_0=\min_{s,a} \frac{\pi_\beta(a| s)}{\pi(a| s)}$; (3) there exists $\varepsilon >0$ such that  $d^{\pi_\beta}(s_0) \geq \omega(s_0)-\varepsilon$ for all $\omega\in \Omega(M)$ satisfying the forward equations. Then the policy $\pi$ is $2\epsilon$-optimal in the following sense
    \[
    \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}d^{\pi_\beta}(s,a){\rm KL}_{P|P'}(s,a) \geq \sup_{\omega\in \Omega(M)}\inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}\omega(s,a){\rm KL}_{P|P'}(s,a)-2\varepsilon.
    \]
\end{proposition}
\end{tcolorbox}
\begin{proof}
First, note that  assumption (1) implies that
\[
\frac{2\epsilon (1-\gamma)} {\gamma \|\rho_r^{\pi}(s_0)\|_\infty} <  \frac{2}{3}. 
\]
Hence the set of alternative models ${\rm Alt}_{r,\pi}^\epsilon$ is not empty.

Consider an alternative model $P'$  similar to the one used in the proof of \cref{prop:rho_zero_ness_succ}:  $P'(s'|s,a) =P(s'|
s,a)$ for all $s\neq  s_0,a,s'$,  and $P'(s_0|s_0, a)=\delta + P(s_0|s_0,a)(1-\delta),  P^{'}(s'|s_0,a)=(1-\delta)P(s'|s_0,a)$ for $s'\neq s_0$, with $\delta\in (0,1)$.  From the same proposition we also know that if 
\[
\frac{2\epsilon(1-\gamma)}{\gamma \|\rho_r^{\pi}(s_0)\|_\infty} < \delta < 1,
\]
the model $P'$ is confusing. 


Since $\frac{2\epsilon(1-\gamma)}{\gamma \|\rho_r^{\pi}(s_0)\|_\infty}<2/3$, we can take $\delta =  2/3$.
We now proceed with an upper bound of the KL divergence between $P$ and $P'$ in $s_0$:
\begin{align*}
        {\rm KL}_{P|P'}(s_0,a)&=\sum_{s'}P(s'|s_0,a)\log\left(\frac{P(s'|s_0,a)}{P'(s'|s_0,a)}\right), \\
        &\leq\log(\frac{1}{1-\delta}), \\
        &\leq \frac{\delta}{1-\delta},\\
        &\le 2.
    \end{align*}
 
    The first inequality holds by plugging in the definition of $P'$ and noticing $\log(\frac{P(s_0|s_0,a)}{\delta+(1-\delta)P(s_0|s_0,a)})\le \log(\frac{P(s_0|s_0,a)}{(1-\delta)P(s_0|s_0,a)})$; the second inequality holds due to $\log(x)\le x-1$; the last inequality uses that $\delta \leq 2/3$ implies $\delta/(1-\delta)\leq 2$.

    Therefore, we can show, $\forall \omega\in\Omega$,
    \begin{align*}
        \inf_{ M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}\omega(s,a){\rm KL}_{P|P'}(s,a)&\le \sum_{s,a}\omega(s,a){\rm KL}_{P|P'}(s,a) \\
        &= \sum_a \omega(s_0,a){\rm KL}_{P|P'}(s_0,a)\\
        &\le 2\omega(s_0) \\
        &\le 2d^{\pi_\beta}(s_0) +2\varepsilon
    \end{align*}
    The last inequality holds due to the second condition $d^{\pi_\beta}(s_0)\ge \omega(s_0) - \varepsilon,\forall \omega\in\Omega$.

    On the other hand, we have shown above that for a model to be a confusing model, the necessary condition is $\exists \hat s$ such that $\sum_a \pi(a|\hat s) KL_{P|P'}(\hat s,a)\ge \frac{2(1-\gamma)^2}{\gamma^2}\epsilon^2$. Hence, we have,
    \begin{align*}
        \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}d^{\pi_\beta}(s,a){\rm KL}_{P|P'}(s,a)&\geq \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_a d^{\pi_\beta}(\hat s,a){\rm KL}_{P|P'}(\hat s,a),\\
        &\geq \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}d^{\pi_\beta}(\hat s)\sum_a \pi_\beta(a|\hat s)\frac{\pi(a|\hat s)}{\pi(a|\hat s)}{\rm KL}_{P|P'}(\hat s,a),\\
        &\geq d^{\pi_\beta}(\hat s)\frac{2(1-\gamma)^2}{\gamma^2}\epsilon^2 \kappa_0,\\
        &\ge d^{\pi_\beta}(s_0)\frac{2(1-\gamma)^2}{\gamma^2}\epsilon^2 \kappa_0\\
        &\ge 2d^{\pi_\beta}(s_0).
    \end{align*}
    The last inequality holds due to the first condition $\frac{\gamma}{(1-\gamma)\sqrt{\kappa_0}}\le \epsilon$.

    Conclusively, we have shown that 
    \begin{align*}
       2\varepsilon+ \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}d^{\pi_\beta}(s,a){\rm KL}_{P|P'}(s,a)\ge \inf_{M'\in {\rm Alt}_{\pi,r}^\epsilon(M)}\sum_{s,a}\omega(s,a){\rm KL}_{P|P'}(s,a)\qquad  \forall \omega\in\Omega(M),
    \end{align*}
    which means $\pi_\beta$ is $2\varepsilon$-optimal.
\end{proof}


The previous results, with the following one, are useful when studying how mixing a target policy with a uniform policy affects sample complexity.
In the following lemma we study the rate of visits at stationarity when the target policy is mixed with a uniform distribution. Let $P_u$ be the transition function induced by the uniform policy, i.e., $P_u(s'|s) = \sum_a P(s'|s,a)/A$, and let $P_u^k$ be the $k$-th step transition matrix induced by the uniform policy. From the ergodicity of $P_u$ there exists an integer $k_0$ such that $P_u^k(s'|s)>0$ for all $k\geq k_0$ and all $(s',s)\in \statespace^2$ (the existence of $k_0$ is guaranteed by \citet[Proposition 1.7]{levin2017markov}).

Hence, to that aim let us define
\[\eta_{k} = \min_{s,s'} P_u^{k}(s'|s),\] be the minimal probability of reaching $s'$ from $s$ in $k$ steps, and let $k_0= \min\{k \in \mathbb{N}: \eta_k >0\}$. We then have the following result. 
\begin{tcolorbox}
\begin{lemma}
\label{lemma:lower_bound_visits}
Let $\pi=(1-\epsilon)\pi_{tgt} + \epsilon\pi_u$ be a mixture policy defined as the mixture between a target policy $\pi_{tgt}$ and a uniform policy $\pi_u$, with mixing coefficient $\epsilon \in (0,1]$. 
Let $d^\pi(s)$ denote the average number of visits to state $s$ under policy $\pi$ at stationarity. Then, for all states we have
\begin{equation}
d^\pi(s) \geq \epsilon^{k_0} \eta_{k_0}.
\end{equation}
\end{lemma}
\end{tcolorbox}
\begin{proof}
In vector form, we can write the stationary equation $d= d P_\pi$. Then, we also know that $d = d P_\pi^2$, and therefore $d\geq \epsilon^2 d P_u^2$ holds element-wise.
\begin{align*}
d(s) &= \sum_{s',s''} d(s') P_\pi(s'|s'')P_\pi(s''|s),\\
&\geq \sum_{s',s''}d(s')\epsilon^2 P_u(s'|s'') P_u(s''|s),\\
&= \epsilon^2  \sum_{s'}d(s')P_u^2(s'|s) \geq \epsilon^2 \eta_2.
\end{align*}
One can also easily show that this property holds for any $k$-step, thus proving that $d(s)\geq \epsilon^k \eta_k$.
\end{proof}
The previous result is important: for environments where $k_0$ is small, and the uniform policy is enough to guarantee high visitation rates (i.e., $\eta_{k_0}$ is not small), then all states are visited regularly.



\subsubsection{Proof of \cref{thm:relaxed_characteristic_time} (Relaxed Characteristic Rate)}
\label{app:thm:relaxed_characteristic_time}
\begin{proof}[Proof of \cref{thm:relaxed_characteristic_time}]
We prove the theorem for single-policy, since the extension to multi-policy follows immediately.

 Start by noting that for all $r\in \rewardspace_\epsilon$ we have $\{M_r':  \|V_{M_r}^\pi - V_{M_r'}^\pi\|_\infty > 2\epsilon \}\subseteq \cup_s \{M_r':  |V_{M_r}^\pi(s) - V_{M_r'}^\pi(s)| > 2\epsilon \} $. Using this decomposition, we can show  that
\begin{align*}
  T_\epsilon(\omega;M)^{-1}
  \geq \inf_{r\in \rewardspace_\epsilon} &\min_s \inf_{M_r': |V_{M_r}^\pi(s) - V_{M_r'}^\pi(s)| > 2\epsilon}\mathbb{E}_{(s',a)\sim \omega}[{\rm KL}_{P|P_r'}(s',a)]\\
  &=   \inf_{r\in \rewardspace_\epsilon} \min_s \inf_{M_r': |V_{M_r}^\pi(s) - V_{M_r'}^\pi(s)| > 2\epsilon} \sum_{s'} \omega(s',\pi(s')) {\rm KL}_{P|P_r'}(s',\pi(s')),\\
  &=   \inf_{r\in \rewardspace_\epsilon} \min_s \inf_{M_r': |V_{M_r}^\pi(s) - V_{M_r'}^\pi(s)| > 2\epsilon}\omega(s,\pi(s)) {\rm KL}_{P|P_r'}(s,\pi(s)),
\end{align*}
where we used the fact that the problem is unconstrained for state-action pairs $(s,a)\neq (s,\pi(s))$. In fact, letting $\Delta V_r^\pi(s)=V_{M_r}^\pi(s) - V_{M_r'}^\pi(s)$ we have $\Delta V_r^\pi(s)=\Delta P(s,\pi(s))^\top V_{M_r}^\pi+P_{r}'(s,\pi(s))^\top \Delta V_{r}^\pi$, which involves only  the transition probability in $(s,\pi(s))$. This means that to design a confusing MDP it is sufficient to change the transition in $(s,\pi(s))$ only.


Then, observe that we also have the following set of inequalities:
 \begin{align*}
     |\Delta V_r^\pi(s)| &\leq \gamma  \left|P(s,\pi(s))^\top V_{M_r}^\pi- P_{r}'(s,\pi(s))^\top V_{M_r'}^\pi\right| ,\\
     &\leq \gamma  \left|\Delta P(s,\pi(s))^\top V_{M_r}^\pi+P_{r}'(s,\pi(s))^\top \Delta V_{r}^\pi\right| ,\\
     &\leq \gamma \left|\Delta P(s,\pi(s))^\top V_{M_r}^\pi\right| + \gamma \|\Delta V_r^\pi\|_\infty.
 \end{align*}
Since these inequalities holds for all $s$, we derive  $|\Delta V_r^\pi(s)| \leq \frac{\gamma}{1-\gamma}   \left|\Delta P(s,\pi(s))^\top V_{M_r}^\pi\right| $, and thus
\[
4\epsilon^2<|\Delta V_r^\pi(s)|^2 \leq \frac{\gamma^2}{(1-\gamma)^2}  \left|\Delta P(s,\pi(s))^\top V_{M_r}^\pi\right|^2 .
\]
Now, note the following equality
\[
\left|\Delta P(s,\pi(s))^\top V_{M_r}^\pi\right|=\left|\Delta P(s,\pi(s))^\top \rho_r^\pi(s)]\right|,
\]
Therefore, using Holder's inequality and Pinsker's inequality we obtain
\begin{align*}
4\epsilon^2 &\leq \frac{2\gamma^2}{(1-\gamma)^2}  \|\Delta P(s,\pi(s))\|_{TV}^2  \|\rho_r^\pi(s)\|_\infty^2,\\
&\leq \frac{\gamma^2}{1-\gamma^2} {\rm KL}_{P|P_r'}(s,\pi(s)) \|\rho_r^\pi(s)\|_\infty^2.
\end{align*}

Using this inequality in the initial lower bound of $T_\epsilon(\omega;M)^{-1}$ we find
\begin{align*}
T_\epsilon(\omega;M)^{-1} &\geq  \inf_{r\in \rewardspace_\epsilon} \min_s 4\epsilon^2 \frac{(1-\gamma)^2\omega(s,\pi(s))}{\gamma^2 \|\rho_r^\pi(s)\|_\infty^2},\\
&\geq  \inf_{r\in \rewardspace} \min_s 4\epsilon^2 \frac{(1-\gamma)^2\omega(s,\pi(s))}{\gamma^2 \|\rho_r^\pi(s)\|_\infty^2},
\end{align*}
which concludes the proof.
\end{proof}



\subsubsection{Proof of \cref{cor:relaxed_characteristic_time_convex_set}}\label{app:cor:relaxed_characteristic_time_convex_set}
\begin{proof}[Proof of \cref{cor:relaxed_characteristic_time_convex_set}]
We begin by rewriting the following optimization problem of $U_\epsilon(\omega;M)$. Note that 
\begin{align*} \sup_{r\in {\cal R}} \max_{s}  \|\rho_r^\pi(s)\|_\infty^2 &= \max_{s}  \sup_{r\in {\cal R}}  \max_{s'}|\rho_r^\pi(s,s')|^2,\\
&= \max_{s}    \max_{s'} \left(\sup_{r\in {\cal R}}|\rho_r^\pi(s,s')|\right)^2,\\
&= \max_{s}    \max_{s'} \max\left(\sup_{r\in {\cal R}}\rho_r^\pi(s,s'), \sup_{r\in {\cal R}}-\rho_r^\pi(s,s')\right)^2.
\end{align*}


From the proof of \cref{lemma:rho_r_pi} we can derive the following expression 
 $\rho_r^\pi(s,s')= e_{s'}\Gamma^\pi(s) r$, where \[
\Gamma^\pi(s)\coloneqq K^\pi(s)G^\pi,\quad K^\pi(s)\coloneqq(I-\mathbf{1}P(s,\pi(s))^\top),\quad G^\pi\coloneqq(I-\gamma P^\pi)^{-1},\]
and $e_{s'}$ is the $s'$-th element of the canonical basis in $\mathbb{R}^S$. Hence, the optimization problem $\sup_{r\in \rewardspace} \rho_r^\pi(s,s')$ is a linear program.


In the last part of the proof we consider the case where $\rewardspace=[0,1]^S$. We exploit the following fact that holds for any vector $y \in \mathbb{R}^n$:$\max_{x\in [0,1]^n} y^\top x = \sum_{i: y_i>0} y_i$.  
Then, let $\Gamma_{ij}^\pi(s)\coloneqq \left(K^\pi(s)G^\pi\right)_{ij}$  and  define 
\[
\Gamma_+^\pi(s,s') \coloneqq \sum_{j: \Gamma_{s',j}^\pi(s)>0} \Gamma_{s',j}^\pi(s),
\]
and, similarly,  $\Gamma_-^\pi(s,s') \coloneqq -\sum_{j: \Gamma_{s',j}^\pi(s)<0} \Gamma_{s',j}^\pi(s)$.
 Hence, we have
 \[
 \max_{r\in [0,1]^S}|\rho_r^\pi(s,s')| = \max\left( \Gamma_+^\pi(s,s'), \Gamma_-^\pi(s,s') \right).
 \]
\end{proof}


Hence, for a general polytope of rewards, defined by $(A,b)$, one can write the following program.
\begin{equation}\label{convex_program_polytope}
\begin{aligned}
    \max_{r, t,s,s'} \quad & t^2 \\
    \text{subject to} \quad & e_{s'}\Gamma^\pi(s) \, r \leq t \cdot \mathbf{1}, \\
    & -e_{s'}\Gamma^\pi(s) \, r \leq t \cdot \mathbf{1}, \\
    & A \, r \leq b, \\
    & t \geq 0,
\end{aligned}
\end{equation}
This formulation is a linear program since the objective and all constraints are linear in the decision variables. The polytope can represent various convex sets, including hypercubes, simplices, or other polyhedral shapes, depending on the specific application.


\paragraph{Scaling of Reward-free Sample Complexity - An Example}
    As an example, in \cref{fig:example-riverswim-complexity} we show the policy evaluation sample complexity in the Riverswim environment \cite{strehl2004empirical} for a single policy $\pi$. In this environment the agent swims towards the river's end, while the river's current opposes the movement of the agent. The policy tries to move the agent toward the river's end, and, as $p$ (the probability of moving toward the river's end) decreases, the reward-free sample complexity increases.

 \begin{figure}[t]
    \centering
\includegraphics[width=.7\linewidth]{figures/riverswim_complexity.pdf}
    \caption{Complexity of Riverswim  for varying reward sets and state space sizes for a single policy $\pi(\cdot)=a_2$ that moves towards the river's end. Left: complexity for different $p$ values (prob. of moving toward the river's end). Right: complexity for $p=0.3$ with varying state space sizes. Solid curves evaluate on ${\cal R}=[0,1]^S$, dashed curves on ${\cal R}=\{r\in [0,1]^S:\|r\|_2\leq 1\}$. }
    \label{fig:example-riverswim-complexity}
\end{figure}