% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[justification=justified,singlelinecheck=false]{caption}
\usepackage{ragged2e}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{fontawesome5}
\usepackage{subcaption}
\usepackage[draft]{minted}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}
\usepackage{tikz}
\usepackage{tikz-dependency}
\usepackage{adjustbox}
\usepackage{todonotes}
\usepackage[multiple]{footmisc}
\usepackage{pgfplots}
\usepackage{rotating}
\usetikzlibrary{positioning, fadings}
\usepackage{transparent}
\usepackage{cclicenses}
\usepackage[ragged]{sidecap}
\usepackage{bbm}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage{mdframed}
\usepackage{listings}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\usepackage{lscape, float} 
\usepackage[most]{tcolorbox}
\usepackage{subcaption}
\renewcommand{\floatpagefraction}{.99} % Favor text beneath big pictures
\usepackage{rotating}
\usepackage{tabularx}

\tcbset{
    promptstyle/.style={
        enhanced,
        width=\linewidth,
        colback=white,
        colframe=black,
        colbacktitle=gray!20,
        coltitle=black,
        rounded corners,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        attach boxed title to top left={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=gray!20
        }
    },
    replystyleg/.style={
        enhanced,
        width=\linewidth,
        colback=green!15,
        colframe=black,
        colbacktitle=green!30,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=green!40
        }
    },
    replystyler/.style={
        enhanced,
        width=\linewidth,
        colback=red!15,
        colframe=black,
        colbacktitle=red!40,
        coltitle=black,
        boxrule=0.5pt,
        drop shadow=black!50!white,
        rounded corners,
        sharp corners=north,
        attach boxed title to top right={
            xshift=-2mm,
            yshift=-2mm
        },
        boxed title style={
            rounded corners,
            size=small,
            colback=red!40
        }
        }
    }

\newtcolorbox{promptbox}[1][]{
    promptstyle,
    title=Prompt,
    #1
}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=.6pt] (char) {#1};}}

\newcommand{\red}[1]{\textcolor{red}{\textbf{#1}}}

\newcommand{\sefl}{\textsc{SEFL}}
\newcommand{\llamabig}{\texttt{Llama-3.1-70B}}
\newcommand{\qwenbig}{\texttt{Qwen2.5-72B}}

\newcommand{\qwenmini}{\texttt{Qwen2.5-0.5B}}
\newcommand{\llamamini}{\texttt{Llama-3.2-1B}}
\newcommand{\llamasmall}{\texttt{Llama-3.2-3B}}
\newcommand{\llamamed}{\texttt{Llama-3.1-8B}}
\newcommand{\qwenmed}{\texttt{Qwen2.5-14B}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\sefl{}: Harnessing Large Language Model Agents to \\ Improve Educational Feedback Systems}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Mike Zhang \hspace{1.5em} Amalie Pernille Dilling \hspace{1.5em} Léon Gondelman \\ {\bf Niels Erik Ruan Lyngdorf} \hspace{1.5em} {\bf Euan D. Lindsay} \hspace{1.5em} {\bf Johannes Bjerva} \\
       Aalborg University, Denmark \\
       \texttt{jjz@cs.aau.dk}}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\begin{document}
\maketitle
\begin{abstract} 
\looseness=-1
Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce \textbf{S}ynthetic \textbf{E}ducational \textbf{F}eedback \textbf{L}oops (\sefl{}), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In \sefl{}, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, \sefl{} specifically focuses on replicating the teacher$\rightarrow$student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that \sefl{}-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal \sefl{}’s potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.
\end{abstract}

\begin{SCfigure*}
    \centering
    \includegraphics[width=12cm]{figures/fig1-v2.pdf}
    \caption{\justifying\textbf{\sefl{} Setup.} We use a two-agent framework~\cite{wu2023autogen} with LLMs acting as a Student and Teacher. The Teacher creates assignments from Fineweb-Edu~\cite{lozhkov2024fineweb-edu}, the Student responds with errors, and finally the Teacher addresses each mistake. This synthetic interaction data is then used to fine-tune multiple LLMs, whose performance is measured via human ratings and an LLM-as-judge approach.
    }
    \label{fig:fig1}
\end{SCfigure*}

\section{Introduction}
Constructive feedback is a cornerstone of higher education, promoting critical thinking and fostering deeper understanding~\cite{hattie2008visible, costello2013technologies}. In many higher education settings, however, providing consistent, high-quality feedback remains a labor-intensive task, further complicated by privacy, consent, and transparency considerations in data collection~\cite{fischer2020mining, suresh-etal-2022-talkmoves, demszky-hill-2023-ncte, wang-demszky-2024-edu, wang2024tutor,lindsay2024responsibledevelopmentautomatedstudent}. Advances in NLP offer promising opportunities to simulate and augment feedback processes, addressing these limitations.

With respect to language technology, prior research has explored areas such as peer learning~\cite{bauer2023using}, aligning mathematical questions~\cite{botelho2023leveraging}, enhancing critical thinking~\cite{guerraoui-etal-2023-teach}, and using large language models (LLMs) for research feedback alignment~\cite{liang2024can, sonkar2024pedagogical}. Tools for monitoring student progress~\cite{schwarz2018orchestrating, aslan2019investigating, alrajhi2021urgency} have also been investigated. However, to the best of our knowledge, this work is the first to leverage LLMs for generating abundant and scalable feedback for student work.
Researchers have identified key characteristics of ``good feedback'', including goal-orientation, actionability, timeliness, user-friendliness, and consistency, as well as fostering student autonomy through self-evaluation~\cite{carless2011developing, wiggins2012seven}. Overly elaborate commentary can undermine clarity, highlighting the value of brevity. Moreover, immediate, formative feedback is crucial for continuous improvement~\cite{wiggins2012seven}, a requirement that LLM-based systems are well suited to fulfill.

LLMs have shown remarkable capabilities in education~\cite{wang2024large}, including automated grading~\cite{ke2019automated, ramesh2022automated, stahl-etal-2024-exploring} and personalized tutoring~\cite{yun2024enhancing, liu2024personality, rooein2024conversations, ross-andreas-2024-toward, kwon2024biped, zhang2024leveraginglargelanguagemodels}. Yet, simulating dynamic teacher--student feedback interactions in agentic, dialogic settings~\cite{xi2023rise, guo2024large, zhang2024simulating} remains largely unexplored, despite its potential to generate scalable synthetic datasets and alleviate real-world data scarcity. We seek to answer: \emph{How can synthetic teacher--student interactions generated by LLMs be leveraged to enable scalable and effective educational feedback systems?}

To this end, we introduce \textbf{S}ynthetic \textbf{E}ducational \textbf{F}eedback \textbf{L}oops (\sefl{}), a framework that generates synthetic teacher-student interactions using LLMs. In this framework, two LLMs---one acting as the teacher and the other as the student---simulate \emph{formative}\footnote{Formative feedback is used early in the learning process, allowing students to refine their work and deepen their understanding~\cite{conole2006contemporary, nicol2007assessment}.} feedback workflows, addressing the limitations of using a single LLM for multiple tasks. This synthetic data is then used to fine-tune smaller autoregressive models, enabling the development of scalable educational feedback systems that can operate efficiently on modest computational infrastructure, such as that available in higher education institutions.

\paragraph{Contributions.} To answer the research question, we contribute the following: \circled{1} A novel framework for simulating teacher-student feedback loops using agentic LLMs. \circled{2} A pipeline for generating synthetic educational data to fine-tune smaller models. \circled{3} An LLM-as-a-judge framework for rating feedback using GPT-4o, Claude-3.5, Command-R+, and DeepseekV3. \circled{4} An open-source release of all the models, data, and code.\footnote{Code and resources available at \url{https://github.com/jjzha/sefl} and on \href{https://hf.co/collections/jjzha/sefl-synthetic-educational-feedback-loops-67b48768dab5123a2e3e9d69}{\texttt{HuggingFace}}.}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{l|rr}
    \toprule
                    & Valid (/ 5,000) & BERTScore     \\
                    \midrule
    \llamabig{}     & 2,513             & 0.877         \\
    \qwenbig{}      & 454               & 0.919         \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Generation Capabilities.} First, We show the number of valid examples, measured by correct \texttt{JSON} format and whether each feedback refers to an error. \llamabig{} generates more valid examples. Second, we measure BERTScore as a proxy for relatedness between error--feedback pairs of the valid generations.}
    \label{tab:valid}
\end{table}
\section{Synthetic Educational Feedback Interactions}

\subsection{Synthetic Data Generation}
We use a two-agent framework~\cite{wu2023autogen}. Both the teacher and student roles are simulated by two separate \llamabig{} models for a two-turn conversation.\footnote{Note that if we mention a model, it is always the \textit{post-trained} version (i.e., \texttt{-Instruct}).} The models are tasked to generate assignment$\rightarrow$answer$\rightarrow$feedback tuples. First, the student-agent asks for an assignment using Fineweb-Edu~\cite{lozhkov2024fineweb-edu} texts (\circled{1}). Second, the teacher-agent creates an assignment that can be of any domain, e.g., math, humanities, role-playing (\circled{2};~\cref{fig:fig1}). Then, the student-agent (\circled{3}) submits assignments containing a number of explicit errors, and the teacher-agent (\circled{4}) provides targeted feedback addressing each error. We investigated both \qwenbig{} and \llamabig{} for interactions. We initially generated 5,000 interaction tuples with each model, where we validated the output as a sanity check.

We show in~\cref{tab:valid} the results of this experiment. Out of 5,000 examples, \llamabig{} generates the most valid examples (i.e., valid \texttt{JSON} format and each feedback refers to an error). For a further check, we use BERTScore~\cite{Zhang2020BERTScore} as a proxy to investigate whether each error--feedback pair of the valid generations relate to each other.\footnote{We only calculate it of the samples where both error and feedback have the same number of generations.} We show regardless of \llamabig{} generating more valid examples, the BERTScore stays in a similar range as \qwenbig{}. Consequently, we use \llamabig{}-generated data as the basis for all subsequent model fine-tuning. For the full prompt, see~\cref{fig:prompt} (\cref{sec:prompts}).


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{l|r}
    \toprule
    \textbf{Feature}                                     & \textbf{Value}                              \\
                                                    \midrule
    Instances                                       & 19,841                             \\
    Assignment Length                            & 78.6                               \\
    Length (Student)                           & 168.1                              \\
    \hspace{1.5em} \# Errors Points                 & 2.5                                \\
    \hspace{1.5em} Length \# Errors              & 20.7                               \\
    Length (Teacher)                           & 120.5                              \\
    \hspace{1.5em} \# Feedback Points                & 2.5                                \\
    \hspace{1.5em} Length \# Feedback            & 34.6                               \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Generation Statistics.} We show the dataset statistics in \emph{averages}, where length is measured in whitespace-separated tokens.}
    \label{tab:stats}
\end{table}

\paragraph{Statistics.} 
\cref{tab:stats} presents the final dataset. 
The generation lengths for each agent are intentionally kept concise ($<$170 tokens), based on the hypothesis that overly lengthy feedback may be counterproductive.
This is in line with observations from \citet{ferguson2011student}, who observes that students tend to favor brief comments, finding a general overview of an assignment more useful. 
Balancing supportive and critical feedback is crucial as, by default, LLMs often produce excessively verbose responses, which can influence the preferences of both humans and language models~\cite{saito2023verbosity}.

\subsection{Fine-Tuning}
The total amount of data synthesized by \llamabig{} amounts to 19.8K conversations, which we use to fine-tune five smaller open-weight LLMs: \qwenmini{}, \llamamini{}, \llamasmall{}, \llamamed{}, \qwenmed{}. 
Each model is further instruction-tuned using a standard language modeling objective (see \cref{sec:hyperparmeters} for more details).

\subsection{Evaluation}
\paragraph{Human Evaluation.} To test the performance of \sefl{}, we have a human evaluation pipeline. 
We randomly sample 150 samples from the validation set. 
Then, we have both the original instruction-tuned model (A) and the model that was further fine-tuned with \sefl{} (B). 
We have three human raters judge whether A$>$B or A$<$B. Additionally, we also ask the coders to indicate whether the assignment$\rightarrow$student answer$\rightarrow$feedback tuple are related to each other or whether the model seems to be generating unrelated content.
Our human raters are in the age range of 20--40 and from Europe, two have a background in Computer Science and one in Engineering Education, they all work in higher education with near-native English proficiency. For more details, the annotation guidelines can be found in~\cref{tab:annotation_guidelines} (\cref{app:humaneval}). 

\paragraph{LLM-as-a-Judge.} 
We also evaluate the fine-tuned models' output using a LLM-as-a-judge framework, a method gaining traction as a method for evaluating text output~\cite{liu-etal-2023-g,zheng2024judging,chen-etal-2023-exploring-use,verga2024replacing,tornberg2023chatgpt,naismith-etal-2023-automated,gilardi2023chatgpt,kocmi-federmann-2023-large,huang-etal-2024-chatgpt, gu2024surveyllmasajudge,falkandchen2025how}. 
The same 150 random instances are rated by four LLMs, namely \texttt{GPT-4o}~\cite{hurst2024gpt}, \texttt{Claude3.5-Sonnet}, \texttt{Command-R+}, and \texttt{DeepSeek-V3}~\cite{liu2024deepseek}. 
We picked these models based on their recency and performance on RewardBench~\cite{lambert2024rewardbench}, JudgeBench~\cite{tan2024judgebench}, and JudgeArena.\footnote{\url{https://huggingface.co/spaces/AtlaAI/judge-arena}.} For the full prompt, see~\cref{fig:prompt-llm} (\cref{sec:prompts}).

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l|rrrrrrr}
    \toprule
    \textbf{Models} & \textbf{H1} & \textbf{H2} & \textbf{H3} & \textbf{J1} & \textbf{J2} & \textbf{J3} & \textbf{J4}\\
    \midrule
     \qwenmini{}            & 94 & 85           & 85 & 97       & 91 & 62       & 91 \\
     \llamamini{}           & 97 & 85           & 81 & 79       & 91 & \red{27} & 79 \\
     \llamasmall{}          & 90 & 61           & 65 & 71       & 74 & \red{26} & 77 \\
     \llamamed{}            & 90 & \red{45}     & 94 & \red{39} & 71 & \red{16} & 65 \\
     \qwenmed{}             & 94 & 77           & 81 & 55       & 65 & \red{10} & \red{19} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Results in Win Rate.} We show the win rate of our \emph{SEFL-tuned models}. A win rate $>$50\% indicates that \sefl-tuned models are better in giving feedback than their vanilla-counterpart; in red everything $<$50\%. We show results of 3 human annotators (H\#) and 4 LLM judges: \texttt{gpt-4o} (J1), \texttt{claude-3.5-sonnet} (J2), \texttt{command-r-plus} (J3), and \texttt{deepseek-v3} (J4).}
    \label{tab:results}
\end{table}


\section{Results}
Our results are in \cref{tab:results}. We show the \emph{win rates} of models fine-tuned with \sefl{} vs. their original, non-tuned versions, as evaluated by both human raters and an LLM-based judges. A value above 50\% indicates that the \sefl-tuned models are preferred over their original versions.

\paragraph{Human Assessment.} 
Overall, human rater evaluations in~\cref{tab:results} show that the \sefl-tuned models often attain high win rates, surpassing 85\% in several cases. Annotators differed in their views on the 8B model's output quality; however, they generally converged on the observation that the fine-tuned 14B model produces superior feedback compared to its non-tuned version. By contrast, models not fine-tuned with \sefl{} had lower win rates, suggesting that the synthetic feedback loops provide an edge in generating more coherent and context-relevant feedback. In addition, we asked annotators whether the synthetic assignment$\rightarrow$answer$\rightarrow$feedback sequences were consistent. In over 75\% of cases, they affirmed the alignment between assignment, student response, and the feedback given, showing the pipeline's effectiveness in keeping contextual relevance.

\looseness=-1
\paragraph{LLM-as-a-Judge Results.} For the LLM-as-a-judge evaluations, we observe notable differences in win rates depending on the model and scale. The results largely mirror the human assessment trend up to the 3B scale. The results from the four LLM judges (J1: \texttt{gpt-4o-2024-08-06}, J2: \texttt{claude-3-5-sonnet-20241022}, J3: \texttt{command-r-plus-08-2024}, J4: \texttt{deepseek-v3}) reveal that \sefl-tuned models demonstrate varying levels of performance relative to their non-tuned counterparts. For instance, \qwenmini{} achieved the highest win rates across all four judges (62\% on J3), indicating a consistent preference for the fine-tuned version. In contrast, larger models such as \llamamed{} and \qwenmed{} exhibit lower win rates, particularly on J3 (16\% and 10\%, respectively), suggesting that fine-tuning with \sefl{} may yield diminishing returns or challenges at larger scales. 

\paragraph{Agreement.} We calculate the pairwise agreement between the judges and human raters. The results show a Cohen's $k$ values between 0.48--0.63, see~\cref{app:agreement}. Though this is considered a \emph{moderate} to \emph{substantial agreement}~\cite{landis1977measurement}, it indicates the subjectivity of feedback.

\section{Discussion}
\paragraph{Human Qualitative Insights.} In addition to the quantitative win rates summarized in~\cref{tab:results}, our human annotators provided rich qualitative feedback on the generated responses. Generally, the annotators notice that if the student answer is too short or incomplete, neither model could generate appropriate feedback on that the assignment is incomplete. More specifically, feedback from \qwenmini{} was frequently noted for being clear and concise, while \llamasmall{} sometimes reiterated assignment details without offering actionable suggestions. Annotators commented that \llamamini{} generally provided more specific and actionable feedback, yet occasionally its tone was perceived as too harsh, whereas \llamamed{} often missed key aspects of the answer. Meanwhile, \qwenmed{} was critiqued for being overly verbose and less aligned with the assignment context. Overall, although \qwenmini{} achieved high human win rates (94, 85, and 85 across three annotators), the qualitative insights suggest that even the best-performing models could improve in error detection, tone refinement, and contextual sensitivity. For all the comments, we refer to~\cref{tab:comments-collection} (\cref{app:qualitative}).

\paragraph{LLM-as-a-Judge.} We used LLM judges to rate the feedback generated by \sefl-tuned models against their vanilla counterparts. 
This provides a rapid, scalable way to measure feedback quality, reducing the need for extensive human annotation. Three out of four LLM judges consistently favored \sefl-tuned \qwenmini{}, \llamamini{}, \llamasmall{}, and \qwenbig{}. 
With Command-R, we notice that it performs worse than GPT-4o and Claude3.5-Sonnet on JudgeArena, indicating that the performance might have to do with instruction following. Nonetheless, we see it as a practical first step for large-scale feedback comparisons in educational contexts. We recommend supplementing LLM-based assessments with targeted human evaluations for more granular insights, possibly aligning more with authentic instructional objectives.

\section{Conclusion}
We introduced \sefl{}, a framework that simulates teacher$\rightarrow$student interactions via two-agent LLMs to generate synthetic data for fine-tuning smaller models. This yields concise, context-sensitive feedback that often surpasses the performance of original instruction-tuned models. While LLM judges provide a scalable way to assess feedback quality, human insights remain crucial for capturing nuances like clarity and tone. As higher education digitalizes, \sefl{} offers a promising avenue for immediate, personalized feedback at scale.

\section*{Limitations}
\sefl{} relies on synthetically generated assignments and errors, and are not real student submissions, which could have implications. Although this approach helps create large datasets, it risks producing feedback unaligned with authentic classroom contexts. Our evaluation also uses LLM-based judges, introducing potential biases related to each judge's training data and objectives. Lastly, while we focused on short-answer tasks, longer or more domain-specific assignments may require specialized or more diverse synthetic data.

\section*{Ethical Considerations}

The use of synthetic data provides an opportunity to train automated feedback systems without the constraints of privacy and consent that come from repurposing actual student assignments as training data. However, it also raises questions about transparency and potential misuse~\cite{lindsay2024responsibledevelopmentautomatedstudent}. For instance, malicious actors could manipulate synthetic data to disseminate misleading or biased feedback, undermining trust in educational tools. Users may also mistake synthetic feedback for real, expert guidance. Moreover, automated feedback systems risk reinforcing biases if the underlying models carry skewed training data. We believe educators and institutions should remain aware of these risks and incorporate human oversight to ensure that such systems \textit{complement}, rather than replace, genuine pedagogical engagement.

\section*{Acknowledgments}
MZ, EDL, and JB are supported by the research grant (VIL57392) from VILLUM FONDEN. We further thank the AAU-NLP group for useful discussions and feedback on an earlier version of this work.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom, anthology}
\clearpage
\appendix

\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{c}{\textit{Data Split}} \\
\midrule
Training data & 17,856 \\
Validation data & 1,985 \\
\midrule
\multicolumn{2}{c}{\textit{Training Configuration}} \\
\midrule
Vocabulary size & 151K (Qwen2.5) \\
                & 128K (Llama3.1/3.2) \\
Context length & 131K (Qwen2.5)\\
               & 128K (Llama3.1/3.2)\\
Number of epochs & 3 \\
Batch size & 4 \\
Global batch size & 16 \\
Seed & 42 \\
\midrule
\multicolumn{2}{c}{\textit{Optimizer Parameters (AdamW)}} \\
\midrule
$\beta_1$; $\beta_2$ & 0.9; 0.999 \\
$\epsilon$ & $10^{-8}$ \\
Learning rate & $2 \times 10^{-5}$ \\
Scheduler type & Linear \\
% Minimum learning rate & $5 \times 10^{-8}$ \\
Weight decay & 0.1 \\
Gradient clipping & 1.0 \\
\bottomrule
\end{tabular}
\caption{\textbf{Fine-tuning Hyperparameters and Configuration Details.}}
\label{tab:training-params}
\end{table}

\section{Fine-tuning Hyperparameters \& Compute}\label{sec:hyperparmeters}
We show our fine-tuning parameters in~\cref{tab:training-params}. We train our model using standard supervised fine-tuning with a language modeling objective. The compute we train the models on are AMD Radeon Instinct MI250X GPUs and it took a total of 467 GPU hours. For the closed-source models' LLM-as-a-judge experiments, we use their respective APIs and the total costs were approximately 10 USD.

\section{Prompts}
\label{sec:prompts}
In~\cref{fig:prompt}, we show the prompts that we give to the agent models. Additionally, in~\cref{fig:prompt-llm}, we show the LLM-as-a-judge that we give to the judge models.

\begin{figure*}[t]
\centering
\begin{tcolorbox}[title=Prompts for Agent-based Educational Feedback Loop, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\footnotesize,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
\begin{lstlisting}[linewidth=\linewidth]
##########################
### Student System Prompt ###
##########################

You are a diligent student who solves all assignments efficiently. Your key traits are: 
1. Direct and Concise Answers: Answer questions directly and concisely; use appropriate academic language. 
2. Show Your Work: Demonstrate your problem-solving process; provide step-by-step solutions when necessary. 
3. Encourage Learning: Focus on assisting with academic tasks; promote understanding through your answers. 
4. Intentional Mistakes: Make some obvious mistakes that the teacher can give feedback on; ensure mistakes are explicit and noticeable. 
5. Response Format: When responding to the teacher's assignment, give your answer and make explicit errors in your answer in valid JSON Lines (JSONL) format without any additional text, using the structure: {'answer': 'Your answer here', 'error_1': 'Description of the first mistake', 'error_2': 'Description of the second mistake'}. Do not write anything else.


##########################
### Teacher System Prompt ###
##########################

You are a skilled teacher specializing in creating concise, effective assignments and providing constructive, targeted feedback. Your key responsibilities are: 
1. Assignment Creation: Create short, clear assignments across various subjects; provide brief, focused instructions. 
2. Feedback Provision: Offer constructive feedback on completed work; explain concepts succinctly when needed; do not give grades, only feedback for each mistake. 
3. Encouragement and Adaptation: Encourage critical thinking and creativity; adapt to different learning styles and levels. 
4. Response Format: When creating an assignment, give your answer in valid JSON format using {'assignment': 'Your assignment text here', 'task': 'Specific task instructions here'}; when providing feedback on a student's reply, respond in valid JSONL format with {'answer': 'Your global feedback here', 'feedback_1': 'Feedback on the first mistake', 'feedback_2': 'Feedback on the second mistake'}. Do not write anything else. Your goal is to facilitate learning through well-designed tasks and helpful guidance.


######################
### Initial User Prompt ###
######################

{Fineweb-Edu Text Example}
\n\n
Create a short and concise one-question higher education level assignment given the text, be creative. Give your answer in valid jsonl format: {assignment: <text>, task_1: <text>, task_2: <text>, ...}. Do not write anything else.

\end{lstlisting}
\end{tcolorbox}
    \caption{\textbf{Prompt for Generating Synthetic Teacher$\rightarrow$Student Feedback Loops.} We show the prompt we use for the agentic setting.}
    \label{fig:prompt}
\end{figure*}


\begin{figure*}[t]
\centering
\begin{tcolorbox}[title=Prompt LLM-as-a-judge, promptstyle]
\lstset{
    basicstyle=\normalfont\sffamily\footnotesize,
    breaklines=true,
    frame=none,
    columns=fullflexible,
}
\begin{lstlisting}[linewidth=\linewidth]
##################
### Judge Prompt ###
##################

You are tasked with evaluating assignment feedback provided by two different models (Model A and Model B). As an objective evaluator, follow these steps: 
1. Analysis Criteria: 
- Accuracy: Does the feedback directly address specific strengths and weaknesses without unnecessary elaboration? 
- Actionability: Are suggestions clear, specific, and implementable without being overly prescriptive? 
- Conciseness: Is the feedback brief and focused while remaining meaningful? 
- Tone: Does the feedback maintain efficiency while being constructive? 
2. Evaluation Process: 
- First, review the original assignment task carefully 
- Then examine both Model A's and Model B's feedback responses 
- Compare them against the above criteria 
- Prioritize focused, efficient feedback over exhaustive detail 
3. Scoring Rules: 
- Responses should not include numerical grades 
- Feedback must be concise and directly related to the student's work 
- Each point should be essential and identify specific aspects of the response 
- Avoid unnecessary categorization and theoretical benefits 
4. Output Format: 
- Respond with a single character: 'A' or 'B' 
- Choose the model that provides more targeted, efficient feedback 
- Do not provide any additional explanation or commentary 
- Your response must contain exactly one character.

Assignment Prompt:
{prompt}

Model A feedback:
{model_a_feedback}

Model B feedback:
{model_b_feedback}

Which is better? Please respond with a single character: A or B."

\end{lstlisting}
\end{tcolorbox}
    \caption{\textbf{Prompt for LLM-as-a-Judge.} We show the prompt that we use for each LLM-as-a-Judge.}
    \label{fig:prompt-llm}
\end{figure*}

\section{Human Evaluation Guidelines}
\label{app:humaneval}
In~\cref{tab:annotation_guidelines}, we show the annotation guidelines for the human raters to rate the model feedback. The annotators were also instructed that the data will be made publicly available.

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.5}
\begin{tabularx}{\textwidth}{|p{3cm}|X|}
\hline
\rowcolor[HTML]{EFEFEF} \textbf{Section} & \textbf{Details} \\ \hline

\textbf{Overview} & 
Your task is to evaluate pairs of feedback responses (Model A and Model B) given to student assignments. You will select which model provides better feedback according to specific criteria.
\newline
\textbf{Key Principles:}
\begin{itemize}
    \item Focus on efficiency and specificity.
    \item Value concise, meaningful feedback over lengthy explanations.
    \item Prioritize direct, actionable suggestions.
    \item Consider both content and delivery.
\end{itemize}
Remember to take breaks; I suggest spending a maximum of 10 minutes per row.
\\ \hline

\textbf{Sheet Information} & 
In the table, pick the one you got assigned. You will see 7 columns and need to fill in columns C and F:
\begin{itemize}
    \item \textbf{Appendix\_assignment:} What the large language model saw when generating an assignment with a possible answer.
    \item \textbf{Assignment:} What the model generated as an assignment and answered.
    \item \textbf{Model A:} Feedback generated by Model A.
    \item \textbf{Model B:} Feedback generated by Model B.
    \item \textbf{Which is better?} The most important part is to evaluate both feedback responses and determine which one is better, based on the assignment and answer.
    \item \textbf{Comments:} Leave comments if needed.
\end{itemize}
\\ \hline

\textbf{Evaluation Criteria} & 
\textbf{Accuracy:} Does the feedback address specific strengths and weaknesses? Are comments relevant to the student work? Is the critique substantive rather than superficial? 
\newline
\textbf{Actionability:} Are suggestions clear and specific? Can students easily understand what to improve? Are recommendations implementable? 
\newline
\textbf{Conciseness:} Is the feedback brief while remaining meaningful? Does it avoid unnecessary elaboration? Is there minimal redundancy? 
\newline
\textbf{Tone:} Is the feedback constructive while being efficient? Does it balance recognition with criticism? Is the language professional? 
\\ \hline

\textbf{Format} & 
\textbf{Preferred Feedback Style:}
\begin{itemize}
    \item Shows good understanding of the concept.
    \item Uses specific examples from the text to support arguments.
    \item Addresses the main question directly.
\end{itemize}
\textbf{Less Preferred Feedback Style:}
\begin{itemize}
    \item Generalized or vague feedback.
    \item Overly verbose or structured responses.
    \item Focuses on theoretical completeness rather than practical advice.
\end{itemize}
\\ \hline

\textbf{Scoring and Pitfalls} & 
\textbf{Scoring:}
\begin{enumerate}
    \item Read the original assignment carefully.
    \item Review both feedback responses.
    \item Evaluate against the criteria.
    \item Select the model that better aligns with the criteria as ``A'' or ``B.''
\end{enumerate}
\textbf{Pitfalls:}
\begin{itemize}
    \item Avoid preferring longer feedback just because it’s lengthy.
    \item Do not choose feedback that only lists general principles.
    \item Avoid letting formatting alone affect your choice.
\end{itemize}
\\ \hline
\end{tabularx}
\caption{Human Annotation Guidelines for Evaluating Assignment Feedback.}
\label{tab:annotation_guidelines}
\end{table*}

\section{Qualitative Feedback}\label{app:qualitative}
In~\cref{tab:comments-collection}, we show the qualitative feedback that the three annotators gave to the feedback of each model.


\begin{table*}
\centering
\tiny
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{p{3cm} p{8cm} X X}
\toprule
\textbf{Model} & \textbf{H1 Comments} & \textbf{H2 Comments} & \textbf{H3 Comments} \\
\midrule

\texttt{Qwen2.5-0.5B-Instruct}
& The answer and feedback from both models doesnt make sense. | The answer does make sense, but states deliberate errors. | The answer doesn't fit the assignment, but is understadable. | Feedback from model B fails to address key aspects of the answer, such as suddenly changing the name of the main character. | Answer is just repeating the assignment | Model A feedback mentions ``unnecessary dialogue'', but the answer doesn't metion incorporating any dialogue. This part of the feedback seems redundant. | The feedback from model A mentions improvements in a lot of the areas that the answer already covers, e.g. the headlines. | The feedback from model A is prefered, but is in this case useless. The answer doesn't answer the assignment in any way. | Model A prefered, but completly wrong/false feedback. The answer perfectly follows the assignment. | The assignment makes sense, but the answer should be a visual. The feedback from model A is prefered, but completely made up as there is nothing to provide feedback on. | The feedback from model A just reiterates what the answer already states, but presents it as areas to improve | Neither model is good, does not live up to any of the evaluation criteria. The answer is also very bad. | The tone of the feedback from model A could sound a bit harsh. | Same Assignment + answer as from row 2 | Same assignment + answer as from row 16
& B is cleary better | both are actually good | not an answer but A properly identified it! | B does not make sense | A's review is too vague | A is concise, B is too lengthy and not a feedback realy | B too detailed | B is not really a feedback | B is too vague | Both feedback are non-sense | A is more concise and clear
& Feedback is not based on the answer | Many assignments consist of several parts, e.g. describe, explain, and discuss. Many answers are short and only do 1 of the three. The feedback does not reflect this. \\

\midrule
\texttt{Qwen2.5-0.5B-Instruct-SEFI}
& Model A feedback mentions ``unnecessary dialogue'', but the answer doesn't metion incorporating any dialogue. This part of the feedback seems redundant. | The feedback from model A mentions improvements in a lot of the areas that the answer already covers, e.g. the headlines. | The feedback from model A is prefered, but is in this case useless. The answer doesn't answer the assignment in any way. | Model A prefered, but completly wrong/false feedback. The answer perfectly follows the assignment. | The assignment makes sense, but the answer should be a visual. The feedback from model A is prefered, but completely made up as there is nothing to provide feedback on. | The feedback from model A just reiterates what the answer already states, but presents it as areas to improve | Neither model is good, does not live up to any of the evaluation criteria. The answer is also very bad. | The tone of the feedback from model A could sound a bit harsh. | Same Assignment + answer as from row 2 | Same assignment + answer as from row 16 | The answer and feedback from both models doesnt make sense. | The answer does make sense, but states deliberate errors. | The answer doesn't fit the assignment, but is understadable. | Feedback from model B fails to address key aspects of the answer, such as suddenly changing the name of the main character. | Answer is just repeating the assignment
& B does not make sense | A's review is too vague | A is concise, B is too lengthy and not a feedback realy | B too detailed | B is not really a feedback | B is too vague | Both feedback are non-sense | A is more concise and clear | B is cleary better | both are actually good | not an answer but A properly identified it!
& Feedback is not based on the answer | Many assignments consist of several parts, e.g. describe, explain, and discuss. Many answers are short and only do 1 of the three. The feedback does not reflect this. \\

\midrule\midrule
\texttt{Llama-3.2-1B-Instruct}
& Model A feedback mentions ``unnecessary dialogue'', but the answer doesn't metion incorporating any dialogue. This part of the feedback seems redundant. | Feedback from model A is prefered, but is not accurate/relevant | Same Assignment + answer as from row 2 | The feedback from model A just reiterates what the answer already states, but presents it as areas to improve | Same assignment + answer as from row 16 | Model A is more concise, byt the feedback in model B is good too. | Is it possible to make the model aware that it does not have enough information to provide feedback? Or motivate to put more effort in, instead of making up feedback? | Same assignment + answer as from row 33 | Feedback from model B is prefered, but is not accurate | Model B, Tone: could benefit from addressing the student directly . | Model B: really nice and encouraging | Model B: referencing the article/appendix incorrectly | Model B: Repetition in feedback.
& B does not make sense | Both are bad | B is more precise | A does not make sense | a bit repetitive though
& In many cases, answers are shorter than the assignment requires. This is not reflected in the feedback. \\

\midrule
\texttt{Llama-3.2-1B-Instruct-SEFI}
& Feedback from model B is prefered, but is not accurate | Model B, Tone: could benefit from addressing the student directly . | Model B: really nice and encouraging | Model B: referencing the article/appendix incorrectly | Model B: Repetition in feedback. | Model A feedback mentions ``unnecessary dialogue'', but the answer doesn't metion incorporating any dialogue. This part of the feedback seems redundant. | Feedback from model A is prefered, but is not accurate/relevant | Same Assignment + answer as from row 2 | The feedback from model A just reiterates what the answer already states, but presents it as areas to improve | Same assignment + answer as from row 16 | Model A is more concise, byt the feedback in model B is good too. | Is it possible to make the model aware that it does not have enough information to provide feedback? Or motivate to put more effort in, instead of making up feedback? | Same assignment + answer as from row 33
& B is more precise | A does not make sense | a bit repetitive though | B does not make sense | Both are bad
& In many cases, answers are shorter than the assignment requires. This is not reflected in the feedback. \\

\midrule\midrule

\texttt{Llama-3.2-3B-Instruct}
& Both models are good, but model A is nicer in tone and actionability | Model B: The tone of the feedback seems restictive (``should''). | Model B: Harsh tone | Neither model is good. They don't seem accurate to the answer provided. | This is not a language I understand, so the assignment and answer might still make sense. I chose model A, as model B had some weird repetitions. | Model B: Good structure, bad wording. What errors is it refering to? | The assignment makes sense, but the answer should be a visual. The feedback from model A is prefered, but completely made up as there is nothing to provide feedback on. | Model A: Repetition in feedback. | Model A: feedback way to elaborate considering the answer.
& but both are good here | B feedback is wrong | but both are good | clearly b is good | not in english! | both are good | A seems more natural | A has repetitions
& Language? | Feedback is not based on the answer \\

\midrule
\texttt{Llama-3.2-3B-Instruct-SEFI}
& The assignment makes sense, but the answer should be a visual. The feedback from model A is prefered, but completely made up as there is nothing to provide feedback on. | Model A: Repetition in feedback. | Model A: feedback way to elaborate considering the answer. | Both models are good, but model A is nicer in tone and actionability | Model B: The tone of the feedback seems restictive (``should''). | Model B: Harsh tone | Neither model is good. They don't seem accurate to the answer provided. | This is not a language I understand, so the assignment and answer might still make sense. I chose model A, as model B had some weird repetitions. | Model B: Good structure, bad wording. What errors is it refering to?
& but both are good here | A has repetitions | B feedback is wrong | but both are good | clearly b is good | not in english! | both are good | A seems more natural
& Feedback is not based on the answer | Language? \\


\midrule\midrule
\multicolumn{4}{r}{\textit{Continued on next page}}\\

\bottomrule
\end{tabularx}
% \caption{Overview of candidate models and collected human comments (H1, H2, H3). The bar (\texttt{|}) separators in the comment fields indicate multiple examples of feedback for each row.}
\label{tab:comments-collection}
\end{table*}



\begin{table*}
\centering
\tiny
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\textwidth}{p{3cm} p{8cm} X X}
\toprule
\multicolumn{4}{c}{\cref{tab:comments-collection} \ -- \textit{continued from previous page}}\\
\toprule
\textbf{Model} & \textbf{H1 Comments} & \textbf{H2 Comments} & \textbf{H3 Comments} \\
\midrule

\texttt{Llama-3.1-8B-Instruct}
& Model B: This is great feedback!! | Model B: consider tone | Model B: not accurate? | Niether of the models are good. | Model B: there is nothing to give feedback. on. not accurate. | The stucture of feedback in model B is prefered, but in this case I think the feedback from model A is more helpful. | Answer starts to repeat. | The feedback form model A is best, but also provides partial solutions | Model B is better on actionability and accuracy, but model A is formatted nicer | Model A: Good structure, bad wording. What errors is it refering to? | Model A is more actionable, but not very concise | Model A: provides answers as well as feedback | Answer repeating the assignment back | Model A: provides the answers, not very actionable | Same assignment + answer as from row 33 | Model A: best feedback, but answers the assignment
& Both are good, but A is better | B is more clear and concise | B repeats the paragraph | B is bogus | neither is good | A aims better that the answer is too short | Finally, B founds that the answer is incomplete | B is good!
& \,\\

\midrule

\texttt{Llama-3.1-8B-Instruct-SEFI}
& Answer starts to repeat. | The feedback form model A is best, but also provides partial solutions | Model B is better on actionability and accuracy, but model A is formatted nicer | Model A: Good structure, bad wording. What errors is it refering to? | Model A is more actionable, but not very concise | Model A: provides answers as well as feedback | Answer repeating the assignment back | Model A: provides the answers, not very actionable | Same assignment + answer as from row 33 | Model A: best feedback, but answers the assignment | Model B: This is great feedback!! | Model B: consider tone | Model B: not accurate? | Niether of the models are good. | Model B: there is nothing to give feedback. on. not accurate. | The stucture of feedback in model B is prefered, but in this case I think the feedback from model A is more helpful.
& Finally, B founds that the answer is incomplete | B is good! | Both are good, but A is better | B is more clear and concise | B repeats the paragraph | B is bogus | neither is good | A aims better that the answer is too short
& \,\\

\midrule\midrule

\texttt{Qwen2.5-14B-Instruct}
& Model B is best, but is way to elborate | Model B: Really good feedback on all parameters | Neither model is good, both provides a new answer. But the last part of feedback from model A is better in tone. | This doesn't make sense | Model B: Isn't accurate and provides answer | The answer and feedback from both models doesnt make sense. | Model A also provides partial solution | Answer is just repeating the assignment | Model A: I havent checked for accuracy of the calculation, but otherwise the ebst. | Tone of model A could be better
& neither is good | not an answer but A properly identified it! | both are bad
& Feedback is not based on the answer \\

\midrule
\texttt{Qwen2.5-14B-Instruct-SEFI}
& The answer and feedback from both models doesnt make sense. | Model A also provides partial solution | Answer is just repeating the assignment | Model A: I havent checked for accuracy of the calculation, but otherwise the ebst. | Tone of model A could be better | Model B is best, but is way to elborate | Model B: Really good feedback on all parameters | Neither model is good, both provides a new answer. But the last part of feedback from model A is better in tone. | This doesn't make sense | Model B: Isn't accurate and provides answer
& neither is good | not an answer but A properly identified it! | both are bad
& Feedback is not based on the answer \\

\bottomrule
\end{tabularx}
\caption{Overview of candidate models and collected human comments (H1, H2, H3). The bar (\texttt{|}) separators in the comment fields indicate multiple examples of feedback for a row.}

\label{tab:comments-collection}
\end{table*}

\section{Annotator Agreement}
\label{app:agreement}
In~\cref{fig:kappa}, we show the pairwise Cohen's $k$ values computed between the LLM-as-a-Judge and our human raters. To further assess evaluation consistency, we computed inter-annotator agreement using Cohen's $k$~\cite{cohen1960coefficient}. Notably, the agreement between H1 and H3 was 0.6348, between H1 and H2 0.4791, and between H2 and H3 0.4759. These values fall within the moderate range, with the highest agreement observed between H1 and H3 indicating substantial consensus, while the slightly lower values between H1 and H2 and between H2 and H3 still reflect acceptable consistency given the subjective nature of feedback evaluation.

\begin{figure*}
    \centering
    \includegraphics[width=.8\linewidth]{figures/cohens_kappa_heatmap.pdf}
    \caption{\textbf{Pairwise Cohen's $k$.} In the figure, we show the pairwise Cohen's $k$ between each LLM-as-a-judge and annotator.}
    \label{fig:kappa}
\end{figure*}

\end{document}
