\section{Related Work}
%
% \noindent\textbf{Image-based Virtual Try-on}.
% % Image-based virtual try-on has been a subject of extensive research over the years, evolving into a promising and challenging field. 
% Numerous studies have been conducted using GANs Brock et al., "Large Scale GAN Training for High Fidelity Natural Image Synthesis"__ to achieve more natural generation. 
% However, GAN-based methods often encounter difficulties in producing high-fidelity and realistic outfitted images. 
% In light of the significant advancements in Text-to-Image diffusion models Parmar et al., "A Large-Scale Study on Unconditional Text-to-Image Generation"__ in recent years, there has been a growing interest Park et al., "DALL-E: An Example-Based Generative Model for Image Synthesis"__ in incorporating pre-trained diffusion models as generative priors within the virtual try-on domain. 
% Subsequent research has formulated virtual try-on as an exemplar-based image inpainting problem, with a focus on fine-tuning inpainting diffusion models using virtual try-on datasets to generate high-quality try-on images. 
% IDM-VTON__ introduces attention modules to encode high-level semantics and low-level features to preserve fine-grained details. FitDiT__ is a customized Diffusion Transformers assigning more parameters and attention to high-resolution features for high-fidelity virtual try-on. 
% While these methods have garnered attention for the realism and quality of synthesized images, they still face challenges in cross-category virtual try-on. This is primarily due to the absence of priors necessary for reasoning about size mismatches, a common issue in real-world  scenarios.

\noindent\textbf{Image-based Virtual Try-on}.
GAN-based methods Liu et al., "Few-Shot Adversarial Learning for Human-Object Interaction"__ have been extensively explored for natural image generation but often struggle to produce high-fidelity outfitted images. With the rapid progress of Text-to-Image diffusion models Parmar et al., "A Large-Scale Study on Unconditional Text-to-Image Generation"__, recent studies Park et al., "DALL-E: An Example-Based Generative Model for Image Synthesis"__ have adopted pre-trained diffusion models as generative priors for virtual try-on.
%
Approaches like IDM-VTON__ and FitDiT__ refine inpainting diffusion models to preserve details and enhance image realism. However, despite their success in generating high-quality results, these methods face significant challenges in cross-category virtual try-on due to the lack of priors necessary for reasoning about size mismatches, a key challenge in real-world applications. Motivated by this, we present a novel CrossVTON method here.


% This is primarily due to the significant effort required for the explicit warping process, which can ignore the importance of realistic garment textures. 
% Additionally, GAN-based approaches tend to lack robust generalization capabilities__, resulting in notable performance deterioration when applied to diverse person images, particularly those beyond the training distribution. 

% For instance, TryOnDiffusion__ employs parallel U-Nets Diffusion to enhance the details of garments and warp them for virtual try-on. 

% For example, LADI-VTON__ and DCI-VTON__ have been proposed, treating clothing as pseudo-words or utilizing warping networks to seamlessly integrate garments into pre-trained diffusion models. 
% To generate harmonized upper and lower styles, Anyfit__ employs a diffusion model and introduces a Hydra Block for attire combinations.

% Although these methods have received attention for the realism and quality of synthesized images, they still face challenges in cross-category virtual try-on stemming from the lack of priors to reasoning the size mismatching issues, which are common in real-world virtual try-on scenarios. 

% virtual try-on scenarios.

% and rethink the different functionalities of different zones. 
% maintaining rich textures and ensuring accurate size fitting, which are common issues in real-world virtual try-on scenarios.
%%%ditfit hand-vton 2篇
% Although these methods have received attention for the realism and quality of synthesized images, they still face challenges in maintaining rich textures and ensuring accurate size fitting, which are common issues in real-world virtual try-on scenarios.




% \noindent\textbf{Cross-category Virtual Try-on}.
% Although cross-category virtual try-on poses a significant challenge and is prevalent in various scenarios, few studies have addressed it as a critical issue, hindering the broader application and progress towards a more generalized virtual try-on solution. 
% AVTON__ introduces a Limbs Prediction Module to predict human body parts, addressing simpler cases of cross-category try-on. AnyFit__ employs an Adaptive Mask Boost to adjust mask lengths during the inference stage for cross-category virtual try-on. 
% However, the aforementioned algorithms only handle simpler cross-category cases (\textit{e.g.,} long sleeves $\leftrightarrow$ short sleeves) and fail in more complex scenarios (\textit{e.g.,} long skirts $\leftrightarrow$ upper jackets). To address this challenge, we propose a tri-zone prior approach to mimic logical reasoning for inputs (cross-category garments and model images), thereby distinguishing the different functionalities of various zones.

\noindent\textbf{Cross-category Virtual Try-on}.
Cross-category virtual try-on remains a challenging yet under-explored problem, limiting the development of generalized solutions. AVTON__ introduces a limbs prediction module for basic cross-category cases, while AnyFit__ uses an adaptive mask boost to refine masks during inference. However, these methods only handle simple scenarios (\textit{e.g.,} long sleeves $\leftrightarrow$ short sleeves) and struggle with complex cases such as long skirts $\leftrightarrow$ upper jackets. To address this, we propose a tri-zone prior framework to emulate logical reasoning, enabling differentiation of functional zones in cross-category try-on.


% To prevent the entire inpainting area from being filled due to a strict mask strategy, FitDiT__ proposes a dilated-relaxed mask strategy to preserve fine-grained details.

% \noindent\textbf{Iterative Data Constructor and Progressive Training}.
% Chain of Thought Prompting Tri-zone Priors for Cross-category Virtual Try-on

%%
% 1. iterative data constuctor
%%2. propose a tri-zone priors to adatively disentangle the zone into the three functional zones
%%3. multi-stage and progrssively training pipelines 
% belongs to the try-on, reconstruction, or imagination area.
% Anyfit 


% The challenge 


%%gan->diffusion（没有脑补区域）->（需要呼应一下logic reasoning）问题->解决问题->创新点



% The {\it IJCAI--25 Proceedings} will be printed from electronic
% manuscripts submitted by the authors. These must be PDF ({\em Portable
%         Document Format}) files formatted for 8-1/2$''$ $\times$ 11$''$ paper.

% \subsection{Length of Papers}


% All paper {\em submissions} to the main track must have a maximum of seven pages, plus at most two for references / acknowledgements / contribution statement / ethics statement.

% The length rules may change for final camera-ready versions of accepted papers and
% differ between tracks. Some tracks may disallow any contents other than the references in the last two pages, whereas others allow for any content in all pages. Similarly, some tracks allow you to buy a few extra pages should you want to, whereas others don't.

% If your paper is accepted, please carefully read the notifications you receive, and check the proceedings submission information website\footnote{\url{https://proceedings.ijcai.org/info}} to know how many pages you can use for your final version. That website holds the most up-to-date information regarding paper length limits at all times.


% \subsection{Word Processing Software}

% As detailed below, IJCAI has prepared and made available a set of
% \LaTeX{} macros and a Microsoft Word template for use in formatting
% your paper. If you are using some other word processing software, please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{Figure/main.pdf}
    % \vspace{-1mm}
    \caption{An overview of the whole pipeline and the structure of CrossVTON which consists of Tri-zone  and Try-on Net. The pipeline illustrates two rounds iterative cross-category data construction by synthesizing the Intra-category, Any-to-dress, and  Dress-to-any data. At each round, the CrossVTON is trained progressively to generate tri-zone priors and endow the ability of cross-category virtual try-on.}
    \label{fig:main_framework}
    % \vspace{-1mm}
\end{figure*}