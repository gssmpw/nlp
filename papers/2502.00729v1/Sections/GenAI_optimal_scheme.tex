\section{The Impact of Selective Response on GenAI's Revenue} \label{sec: genai effect}
In this section, we analyze the revenue-maximization problem faced by GenAI. Subsection~\ref{subsec:impact} examines the impact of using selective responses on both user proportions and generated data. We show that
any $\tau$-selective modification of any strategy and any $\tau$ generates more future data and attracts more users to GenAI from round $\tau+1$ onward. Subsequently, we develop two approaches for maximizing GenAI's welfare. In Subsection~\ref{sec: GenAI revenue maximization}, we develop an approximately optimal algorithm for maximizing GenAI's optimal revenue. In Subsection~\ref{sec: welfare constrained revenue maximization}, we focus on undiscounted settings, i.e., $\gamma=1$, and consider a welfare-constraint revenue maximization: Maximizing revenue under a minimal social welfare level constraint. We emphasize the trade-off between our approaches: The first approach cannot handle welfare constraints, while the second is restricted to undiscounted revenue. %In Subsection~\ref{sec:Beyond Discrete set}, we relax the assumption of discrete actions $A$ and extend our algorithmic results to actions in the $[0,1]$ interval.

\subsection{Selective Response Implies Increased User Proportions}\label{subsec:impact}
Next, we analyze the impact of using a $\tau$-selective modification of any base strategy on the proportions and data generation. At first glance, using selective responses harms immediate revenue, as it encourages users to turn to Forum. However, as suggested by Observation~\ref{obs: withholding impact}, lower response levels can ultimately prove beneficial. But why is this the case?

The answer lies in the dynamics of data generation. By employing a more selective response, GenAI incentivizes users to engage with Forum, which results in the creation of more data. This additional data becomes crucial in future rounds, enabling GenAI to attract a more significant user proportion in subsequent interactions. While this reasoning is intuitive, its application over time presents a technical challenge: As the proportion of users choosing GenAI increases, the marginal data generated per round may decrease, potentially leading to less data than a strategy where GenAI answers every query. However, the theorem below demonstrates the compounding effect of selective response, guaranteeing consistently higher user proportions in future rounds.
\begin{theorem} \label{thm: not answering increase proportions}
Fix any strategy $\bft{x}$. For every $t > \tau$ it holds $\mathcal{D}_t(\bft{x}^\tau) > \mathcal{D}_t(\bft{x})$ and $p_t(\bft x^\tau) \geq p_t(\bft x)$ where $p_t(\bft x^\tau) = p_t(\bft x)$ if and only if $x_t = x_t^\tau= 0$.
\end{theorem}

\begin{sketch}{thm: not answering increase proportions}
To prove this theorem, we first show that $ \mathcal{D}_t(\bft{x}^\tau) - \mathcal{D}_t(\bft{x}) > 0 $ for every $ t > \tau $. To do so, we introduce some additional notations. First, we define $ \qp(\mathcal{D}, x) = x \frac{e^{\beta a(\mathcal{D}) x}}{e^{\beta a(\mathcal{D}) x} + e^{\beta w^s}} $, which represents the resulting proportion when using a selective response $ x $ with data $ \mathcal{D} $. Next, we define $ f(\mathcal{D}, x) = D - \qp(\mathcal{D}, x) $ as the total data generated when choosing $ x $ with initial data $ D $. Note that for every $ t \in [T] $, we have $ f(\mathcal{D}_t(\bft{x}), x_t) = \mathcal{D}_{t+1}(\bft{x}) $ and $ \qp(\mathcal{D}_t(\bft{x}), x_t) = p_t(\bft{x}) $. Following, we prove that $ f(\mathcal{D}, x) $ is monotonically increasing with respect to $ \mathcal{D} $.

\begin{proposition} \label{lemma: monotone data} For every $x \in [0, 1]$ and $\mathcal{D} \in \mathbb{R}_{\geq 0}$ it holds that $\frac{df(\mathcal{D}, x)}{d\mathcal{D}} > 0$.
\end{proposition}

Proposition~\ref{lemma: monotone data}, combined with Assumption~\ref{assumption: data lip}, imply that for every $ t > \tau $, if $ \mathcal{D}_t(\bft{x}^\tau) > \mathcal{D}_t(\bft{x}) $, then it follows that $ \mathcal{D}_{t+1}(\bft{x}^\tau) > \mathcal{D}_{t+1}(\bft{x}) $. Iterating Proposition~\ref{lemma: monotone data} leads to $ \mathcal{D}_t(\bft{x}^\tau) > \mathcal{D}_t(\bft{x}) $ for every $ t > \tau $. Finally, since $ \qp(\mathcal{D}, x) $ is monotonically increasing with respect to $ \mathcal{D} $, we conclude that $ p_t(\bft{x}^\tau) \geq p_t(\bft{x}) $ for every $ t > \tau $, thus completing the proof of \Cref{thm: not answering increase proportions}.
\end{sketch}




\subsection{Revenue Maximization} \label{sec: GenAI revenue maximization}
In this subsection, we develop an approximately optimal algorithm for maximizing GenAI's revenue. We begin by noting the challenges of the problem, emphasizing why identifying the optimal strategy is nontrivial.

Recall that Theorem~\ref{thm: not answering increase proportions} demonstrates that employing a selective response increases future proportions. This argument can be applied iteratively by employing selective responses in different rounds, further enhancing the future proportions. This intuition hints that a step function-based strategy could be optimal: GenAI should answer no queries in early rounds and then answer all queries. In such a case, the effective space of optimal strategy reduces to all $T$ step function-based strategies. Unfortunately, this intuition is misleading.
\begin{observation} \label{obs: non-binary optimal}
There exist instances where the optimal strategy $\bft{x}^\star \notin \{0, 1\}^T$.
\end{observation}
Due to Observation~\ref{obs: non-binary optimal}, the search for optimal strategies spans the continuous domain $[0, 1]^T$. This observation motivates us to adopt an approximation-based approach to identify near-optimal strategies efficiently. To that end, we devise the $\algname$ algorithm, which stands for \textbf{A}pproximately optimal \textbf{S}elective \textbf{R}esponse. $\algname$ follows a standard dynamic programming structure, but its approximation analysis is nontrivial, as we elaborate below. Therefore, we introduce it in \appnx{Appendix~\ref{appn: revenue maximization}} and provide an informal description here, along with key insights from its analysis.

%We propose an algorithm based on dynamic programming using a back-propagation approach. Given a , our goal is to find the approximately optimal strategy with respect to $A$. Specifically, we maximize over the strategies in $A^T$. We elaborate on the optimization over $A$ after presenting the algorithm guarantees and its proof sketch.

%Notice that the possible data values induced by strategies in $A^T$ are finite. A naive approach would be to use back-propagation, where for each data point, we calculate the optimal selective response based on the current proportion and the expected future revenue. However, this approach is not tractable, as in any round $t$, the number of possible data points is $\left|A\right|^{t-1}$.\footnote{The set of data points is obtained by traversing a $|A|$-ary tree, where at each data point, GenAI can choose from $|A|$ different actions, each leading to a distinct data point at the next level of the tree.} 

\paragraph{Overview of the $\algname$ algorithm}
Fix any finite set $A$, $A \subset [0, 1]$. Naively, if we wish to find  $ \argmax_{\bft{x} \in A^T}U(\bft x)$, we could exhaustively search along all $A^T$ strategies via inefficient dynamic programming. However, we show how to design a small-size state representation and execute dynamic programming effectively. The challenge is ensuring that any strategy's revenue within the small state representation approximates the actual revenue of that strategy. To achieve this, we discretize the amount of data $\mathcal D$. Recall that in each round, the amount of generated data is at most $1$, meaning that for any strategy $\bft{x}$, the total data up to round $t$ is $\mathcal{D}_t(\bft{x}) \in [0, t - 1]$. Consequently, we define states by the round $t$ and the discretized data value within $[0, t-1]$. At the heart of our dynamic programming approach is the calculation of the expected revenue for each state and action $y \in A$, based on the induced proportions, generated data, and the anticipated next state. The next theorem provides the guarantees of $\algname$. 
\begin{theorem} \label{thm: alg guarantees fixed actions}
Fix any instance and let $\varepsilon > 0$. The $\algname$ outputs a strategy $\bft{x}$ such that
\begin{equation}\label{eq:thm unconst}
U(\bft x) > \max_{\bft{x'} \in A^T}U(\bft x') - \varepsilon L_r T^2,    
\end{equation}
and its run time is $O\left(\frac{T^2 \left| A \right|}{\varepsilon}\right)$.
\end{theorem}

\begin{sketch}{thm: alg guarantees fixed actions}
%We use the notations $f,q$ from the proof of \Cref{thm: not answering increase proportions}. 
To prove the theorem, there are two key elements we need to establish. First, for any two similar data quantities under any selective response strategy, the resulting revenues are similar as well. Imagine that $\mathcal D^1$ is the actual data quantity generated by some strategy up to some arbitrary round, and $\mathcal D^2$ is the discretized data quantity of the same strategy in our succinct representation. If GenAI plays $x\in A$ in the next round, how different do we expect the data quantity to be in the next round? In other words, we need to bound the difference $\left| f(\mathcal{D}^1, x) - f(\mathcal{D}^2, x) \right|$, where $f$ follows the definition from the proof of \Cref{thm: not answering increase proportions}. To that end, we prove the following lemma. 
\begin{lemma} \label{lemma: sketch bounded data}
For any $\mathcal{D}^1, \mathcal{D}^2 \in \mathbb{R}_{\geq 0}$ and $x\in A$, it holds that $\left| f(\mathcal{D}^1, x) - f(\mathcal{D}^2, x) \right| \leq \left| \mathcal{D}^1 - \mathcal{D}^2 \right|$.
\end{lemma}
We further leverage this lemma in proving the second key element: The discrepancy of the induced proportions is bounded by the discrepancy in the data quantities, i.e., $\left|\qp(\mathcal{D}^1, x) - \qp(\mathcal{D}^2, x)\right| < \left|\mathcal{D}^1 - \mathcal{D}^2 \right|$. 
% Lemma~\ref{lemma: sketch bounded data} relies on an insight from Theorem~\ref{thm: not answering increase proportions}: If $\mathcal{D}^1 > \mathcal{D}^2$, then $q(\mathcal{D}^1, x) > q(\mathcal{D}^2, x)$ \omer{delete from here till end?why do we care about $1-q$?}, and therefore the data generated in the next round, $1-q(\mathcal D,x)$, satisfies $1 - q(\mathcal{D}^1, x) < 1 - q(\mathcal{D}^2, x)$. The second key element is proving that the discrepancy of the induced proportions is bounded by the discrepancy in the data quantities. Formally, \omer{Boaz - doesn't this follow immediately from the lemma above?}
% \begin{lemma} \label{lemma: sketch bounded proportions}
% It holds that $\left|q(\mathcal{D}^1, x) - q(\mathcal{D}^2, x)\right| < \left|\mathcal{D}^1 - \mathcal{D}^2 \right|$.  
% \end{lemma}
Equipped with Lemma~\ref{lemma: sketch bounded data} and the former inequality, we bound the discrepancy the dynamic programming process propagates throughout its execution.
\end{sketch}
Observe that Theorem~\ref{thm: alg guarantees fixed actions} guarantees approximation with respect to the best strategy that chooses actions from $A$ only. Indeed, the right-hand-side of Inequality~\eqref{eq:thm unconst} includes $\max_{\bft{x'} \in A^T}U(\bft x')$. In fact, by taking $A$ to be the $\delta$-uniform discretization of the $[0,1]$ interval for a small enough $\delta>0$, we can extend our approximation guarantees to the best continuous strategy at the expense of a slightly larger approximation factor.
\begin{theorem} \label{thm: ASR alg optimal bound}
Let $\delta \in (0, \frac{1}{\beta}]$ and let $A_\delta = \{ 0, \delta, 2\delta \ldots 1 \}$. Let $\bft{x}$ be the solution of $\algname$ with parameters $\varepsilon > 0$ and $A_\delta$. Then,
\begin{align*}
U(\bft{x}) \geq \max_{\bft{x}'} U(\bft{x}') - \frac{7\beta + 1}{4\left(1-\gamma \right)^2}  L_r \delta - \varepsilon L_r T^2,
\end{align*}
and the run time of $\algname$ is $O\left(\frac{T^2}{\varepsilon \delta}\right)$.
\end{theorem}
% \begin{align*}
% U(\bft{x})  \geq  \max_{\bft{x}' \in [0,1]^T} U(\bft{x}') -\frac{7\beta + 1}{4\left(1-\gamma \right)^2}  L_r \delta - \varepsilon L_r T^2,
% \end{align*}
% with a run time of $O\left(\frac{T^2}{\varepsilon\delta}\right)$.
\subsection{Welfare-Constrained Revenue Maximization} \label{sec: welfare constrained revenue maximization}
While the $\algname$ algorithm we developed in the previous subsection guarantees approximately optimal revenue, it might harm user welfare. Indeed, Observation~\ref{obs: withholding impact} implies that selective response can improve revenue but decrease welfare. This motivates the need for a welfare-constrained revenue maximization framework, where the objective is to maximize GenAI's revenue while ensuring that the social welfare remains above a predefined threshold $W$. Formally, 
% \begin{align}\label{prob: max U welfare} 
% & \max_{\bft{x} \in A^T} U(\bft{x}) \quad \textnormal{s.t. }  W(\bft{x}) \geq W. \tag{P1} 
% \end{align}
\begin{equation}\label{prob: max U welfare}
\begin{aligned}
& \max_{\bft{x} \in A^T} U(\bft{x}) \\
& \textnormal{s.t. }  W(\bft{x}) \geq W.
\end{aligned} \tag{P1}
\end{equation}
Noticeably, if the constant $W$ is too large, that is, $W > \max_{\bft{x}} W(\bft{x})$, Problem~\eqref{prob: max U welfare} has no feasible solutions; hence, we assume $W \leq \max_{\bft{x}} W(\bft{x})$.
%Recall from Observation~\ref{obs: withholding impact} that maximizing GenAI's revenue can come at the cost of reduced social welfare. As a result, the solution from Subsection~\ref{sec: GenAI revenue maximization} is not directly applicable, prompting us to develop a new approach for solving Problem~\eqref{prob: max U welfare}. 
Our approach for this constrained optimization problem is inspired by the PARS-MDP problem \cite{ben2024principal}. We reduce it to a graph search problem, where we iteratively discover the Pareto frontier of feasible revenue and welfare pairs, propagating optimal solutions of sub-problems. Due to space constraints, we defer its description to \appnx{Appendix~\ref{sec:appendix of constrain}} and present here its formal guarantees.
\begin{theorem} \label{thm: alg welfare contraint}
Fix an instance such that $\gamma = 1$. Let $\varepsilon > 0$ and let $\bft{x}^\star$ be the optimal solution for Problem~\eqref{prob: max U welfare}.  There exists an algorithm with output $\bft{x}$ that guarantees
\begin{enumerate}%[topsep=2pt, itemsep=2pt, parsep=0pt]%Here, topsep=2pt adjusts the space before and after the entire list, itemsep=2pt sets the spacing between items, and parsep=0pt removes extra space after each itemâ€™s text. Feel free to tweak these values to get just the right amount of padding.
    \item $U(\bft{x}) > U(\bft{x}^\star) - \varepsilon T^2 \max \{1, L_r\}$, 
    \item $W(\bft{x}) > W - 2\varepsilon T^2 (L_a+1)$,
\end{enumerate}
and its running time is $O\left(\frac{T^2 |A|}{\varepsilon^2} \log(\frac{T |A|}{\varepsilon})\right)$.
\end{theorem}
Unfortunately, the technique we employed in the previous subsection for extending the approximation from the optimal discrete strategy to the optimal continuous strategy is ineffective in the constrained variant; see Section~\ref{sec:discussion}.
