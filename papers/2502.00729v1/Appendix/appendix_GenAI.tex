\section{Proofs Omitted from Section~\ref{sec: genai effect}}
\subsection{Proofs Omitted from Subsection~\ref{subsec:impact}}\label{sec:selective increased}

\begin{proofof}{lemma: monotone data}
We take the derivative of $f(\mathcal{D}, x)$:

\begin{align*}
\frac{df(\mathcal{D}, x)}{d\mathcal{D}} &= 1 - x\frac{dq(\mathcal{D}, x)}{d\mathcal{D}} \\
&= 1 - \beta x^2 q(\mathcal{D}, x)\left(1-q(\mathcal{D},x)\right) \frac{da(\mathcal{D})}{d\mathcal{D}}.
\end{align*}


Notice that $q(d, x) \in [0, 1]$ for every $d \in \mathbb{R}_{\geq 0}$ and $x \in [0, 1]$. Furthermore, the expression $q(1-q)$ has one maximum point at $q = 0.5$, therefore

\begin{align*}
\frac{df(\mathcal{D}, x)}{d\mathcal{D}} &= 1 - \beta x^2 q(\mathcal{D}, x)\left(1-q(\mathcal{D},x)\right) \frac{da(\mathcal{D})}{d\mathcal{D}} \\
&\geq 1- \frac{\beta x^2}{4}\frac{da(\mathcal{D})}{d\mathcal{D}} \geq 1- \frac{\beta}{4}\frac{da(\mathcal{D})}{d\mathcal{D}} > 0.
\end{align*}

This completes the proof of Proposition~\ref{lemma: monotone data}.
\end{proofof}



\begin{proofof}{thm: not answering increase proportions}
We first show that if $y < x_\tau$ then $\mathcal{D}_{\tau + 1}(\bft x^\tau) >\mathcal{D}_{\tau + 1}(\bft x)$. By definition of $\bft x^\tau$ and $\bft x$ it holds that $\mathcal{D}_t(\bft x^\tau) = \mathcal{D}_t(\bft x)$ for every $t \leq \tau$.
Next, notice that if $y < x_\tau$ then
\begin{align*}
p_\tau(\bft x^\tau) &= x_\tau^\tau \frac{e^{\beta a(d_\tau(\bft x^\tau)) x^\tau_\tau}}{e^{\beta a(d_\tau(\bft x^\tau)) x^\tau_\tau} + e^{\beta w^s}} \\
&= y \frac{e^{\beta a(d_\tau(\bft x^\tau)) y}}{e^{\beta a(d_\tau(\bft x^\tau)) y} + e^{\beta w^s}} < x_\tau \frac{e^{\beta a(d_\tau(\bft x)) x_\tau}}{e^{\beta a(d_\tau(\bft x)) x_\tau} + e^{\beta w^s}} = p_\tau(\bft x);
\end{align*}
therefore, it holds that
\begin{align*}
\mathcal{D}_{\tau + 1}(\bft x^\tau) &= \mathcal{D}_{\tau}(\bft x^\tau) + (1 - p_\tau(\bft x^\tau)) \\
& > \mathcal{D}_{\tau}(\bft x) + (1 - p_\tau(\bft x)) = \mathcal{D}_{\tau + 1}(\bft x).
\end{align*}

Next, we use the following proposition to show that $\mathcal{D}_{\tau + 1}(\bft x^\tau) >\mathcal{D}_{\tau + 1}(\bft x)$.

\begin{proposition} \label{prop: monotonicity}
Let $\tau \in [T]$ and $\bft{x}$, $\tilde{\bft{x}}$ be two selective response strategies such that $x_t = \tilde{x}_t$ for every $t \geq \tau$. If $\mathcal{D}_t(\bft{x}) > \mathcal{D}_t(\tilde{\bft{x}})$ and $\frac{d a(\mathcal{D})}{d\mathcal{D}} < \frac{4}{\beta}$ Then for every $t \geq \tau$ it holds that $\mathcal{D}_t(\bft{x})_t > \mathcal{D}_t(\tilde{\bft{x}})_t$ and $p_t(\bft{x}) \geq p_t(\tilde{\bft{x}})$ where inequality holds only if $x_t = 0$.
\end{proposition}
Thus, by Proposition~\ref{prop: monotonicity} it holds that $p_t(\bft x^\tau) \geq p_t(\bft x)$. This completes the proof of Theorem~\ref{thm: not answering increase proportions}.
\end{proofof}



\begin{proofof}{prop: monotonicity}
We prove our claim by proving a slightly stronger version using induction over the rounds. In addition to the original claim, we also prove that $\mathcal{D}_t(\bft{x}) > \mathcal{D}_t(\tilde{\bft{x}})$ for every $t \geq \tau$. We start with the base case $t = \tau$. Notice that $p_\tau(\bft x) = q(\mathcal{D}_\tau(\bft{x}), x_\tau)$ and $p_\tau(\tilde{\bft{x}}) = q(\mathcal{D}_\tau(\tilde{\bft{x}}), \tilde{x}_\tau)$. 

We now use the following lemma:
\begin{lemma} \label{lemma: property data monotone}
For every $x \in [0, 1]$ and $\mathcal{D} \in \mathbb{R}_{\geq 0}$, it holds that $q(\mathcal{D}, x)$ satisfies $\frac{dq(\mathcal{D}, x)}{d\mathcal{D}} \geq 0$.
\end{lemma}
Since $\mathcal{D}_\tau (\bft{x}) > \mathcal{D}_\tau (\tilde{\bft{x}})$ then from Lemma~\ref{lemma: property data monotone} it holds that $p_\tau(\bft x) \geq p_\tau (\tilde{\bft{x}})$. Next, we show that $\mathcal{D}_{\tau+1} (\bft{x}) > \mathcal{D}_{\tau+1} (\tilde{\bft{x}})$. Notice that $\mathcal{D}_{\tau+1} (\tilde{\bft{x}}) = \mathcal{D}_{\tau} (\tilde{\bft{x}}) + (1-p_\tau(\tilde{\bft{x}})) = f(\mathcal{D}_{\tau} (\tilde{\bft{x}}), \tilde{x}_{\tau})$, similarly $\mathcal{D}_{\tau+1} (\bft{x}) = f(\mathcal{D}_{\tau} (\bft{x}), x_\tau)$. By Proposition~\ref{lemma: monotone data} it holds that $f(\mathcal{D}, x_\tau)$ is monotonic increasing in $\mathcal{D}$. Therefore, $\mathcal{D}_\tau (\bft{x}) > \mathcal{D}_\tau(\tilde{\bft{x}})$ leads to $f(\mathcal{D}_{\tau} (\bft{x}) > f(\mathcal{D}_{\tau} (\tilde{\bft{x}}), \tilde{x}_{\tau})$ and thus $\mathcal{D}_{\tau+1} (\bft{x}) > \mathcal{D}_{\tau+1} (\tilde{\bft{x}})$. 

Assume the claim holds for $t-1 > \tau$, and we prove it holds for $t$. Since it holds for $t-1$, then $\mathcal{D}_t(\bft x) > \mathcal{D}_t(\tilde{\bft x})$. Therefore, by Lemma~\ref{lemma: property data monotone} it holds that $p_t(\bft x) > p_t(\tilde{\bft x})$. Lastly, by Proposition~\ref{lemma: monotone data} it holds that
\[
\mathcal{D}_{t+1}(\bft x) = f(\mathcal{D}_{t+1}(\bft x), x_t) > f(\mathcal{D}_{t+1}(\tilde{\bft x}), \tilde{x}_t) = \mathcal{D}_{t+1}(\tilde{\bft x}).
\]

This completes the proof of Proposition~\ref{prop: monotonicity}.
\end{proofof}

\begin{proofof}{lemma: property data monotone}
We take the derivative of $q(\mathcal{D}, x)$:

\begin{align*}
\frac{dq(\mathcal{D}, x)}{d\mathcal{D}} &= \beta x^2 q(\mathcal{D}, x)(1-q(\mathcal{D}, x)) \frac{da(\mathcal{D})}{d\mathcal{D}}.
\end{align*}
As we assume in the model, $\frac{da(\mathcal{D})}{d\mathcal{D}} \geq 0$. Furthermore, $q(\mathcal{D}, x) \in [0, 1]$ for every $x \in [0, 1]$, and therefore $\frac{dq(\mathcal{D}, x)}{d\mathcal{D}} \geq 0$. This completes the proof of Lemma~\ref{lemma: property data monotone}.
\end{proofof}





















\subsection{Proofs Omitted from Subection~\ref{sec: GenAI revenue maximization}} \label{appn: revenue maximization}

\begin{proofof}{obs: non-binary optimal}
Consider the instance $a(\mathcal{D}) = 0.7(1-e^{0.4 \mathcal{D}}) + 0.3$, $\gamma = 1$, $r(p) = p$, $\beta = 41$ and $w^s = 0.66$. Let $T = 3$ and observe the revenue for the following schemes:
\begin{enumerate}
    \item $\Bar{\bft{x}} = (1,1,1)$.
    \item $\bft{x}^1 = (0, 1, 1)$.
    \item $\bft{x}^2 = (0, 0, 1)$.
    \item $\bft{x}^3 = (1, 0, 1)$.
    \item $\bft{x} = (0.04, 0.97, 1)$.
\end{enumerate}
Notice that we do not consider schemes where $x_3 = 0$ since for any such scheme, the scheme which is identical at round $t = 1$, $t = 2$ and plays $x_3 = 1$ induces higher revenue; therefore, the revenue difference between $\bft{x}$ and the other schemes is as follows:
\begin{itemize}
    \item $U(\bft{x}) - U(\Bar{\bft{x}}) > 9.71 \cdot 10^{-6}$.
    \item $U(\bft{x}) - U(\bft{x}^1) > 9.71 \cdot 10^{-6}$.
    \item $U(\bft{x}) - U(\bft{x}^2) > 7.9 \cdot 10^{-6}$.
    \item $U(\bft{x}) - U(\bft{x}^3) > 7.89 \cdot 10^{-6}$.
\end{itemize}

This completes the proof of Observation~\ref{obs: non-binary optimal}.
\end{proofof}


\begin{algorithm}[t]
\textbf{Input:} $T, A, \varepsilon$ \\
\textbf{Output:} $\bft x$
\begin{algorithmic}[1]
\small % add this command to decrease the font size
\caption{Approximately optimal Selective Response ($\algname$)} \label{alg: ARMS}
\STATE for every $t \in [T+1]$ and $d \in \{0, \varepsilon ,\ldots, T\}$ set $V(t, d) \leftarrow 0$, $\pi(t, d) \leftarrow 0$

\FOR{$t = T \ldots 1$}
    \FOR{$d \in \{0, \varepsilon, \ldots, t-1 \}$}
        \STATE for every $y \in A$, set $U(y) \gets 0$ 
        \FOR{$y \in A$}
            \STATE $p \gets y \frac{e^{\beta a(d) y}}{e^{\beta a(d) y} + e^{\beta w^s}}$
            \STATE $d' \gets \epsfloor{d + (1-p)}$ \label{arms: def next data}
            \STATE $v_d(y) \gets r(p) + \gamma V(t+1, d')$
        \ENDFOR

        \STATE $V(t, d) \gets \max_y v_d(y)$
        \STATE $\pi(t, d) \gets \argmax_y v_d(y)$
    \ENDFOR
\ENDFOR

\STATE extract $\bft x$ from $\pi$ starting at $t = 1, d = 0$
\RETURN{$\bft x$} \;
\end{algorithmic}
\end{algorithm}



\begin{proofof}{thm: alg guarantees fixed actions}
We denote $\Delta_t = (t-1)\varepsilon$ and $U_t(\bft{x})$ the accumulated revenue from round $t$ until $T$ following scheme $\bft{x}$, formally $U_t(\bft{x}) = \sum_{i = t}^T \gamma^{i-t} p_i(\bft{x})$. We use the following lemma to show the relationship between $V(t, \epsfloor{d})$ and $U_t(\bft x^\star)$.

\begin{lemma}\label{lemma: alg fixed actions data gap}
Fix round $t \in [T]$. For every $d \in \{ 0, \varepsilon, \ldots, T \}$ such that $\left| d - \mathcal{D}_t(\bft{x}^\star) \right| \leq \Delta_t$ it holds that
\[
V(t,d) > U_t(\bft{x}^\star) - L_r \sum_{i = t}^T \Delta_i \gamma^{i-t}.
\]
\end{lemma}

Notice that $\mathcal{D}_1 = 0$ by definition, and thus $U_1(\bft{x}^\star) = U(\bft{x}^\star)$. Therefore, Lemma~\ref{lemma: alg fixed actions data gap} suggests that
\begin{align*}
V(1,0) > U(\bft x^\star) - L_r \sum_{i = 1}^T \Delta_i \gamma^{i-1}.
\end{align*}

We use the following lemma to evaluate the differences between $U(\bft{x})$ and $V(1,0)$.

\begin{lemma} \label{lemma: alg fixed action lower data}
Let $(d_t)_{t = 1}^T$ be the sequence defined by $d_t = 0$ and $d_{t+1} = 
\epsfloor{d_t + (1-x_t q(d_t, x_t))}$. Then, for every $t \in [T]$ it holds that $d_t < \mathcal{D}_{t}(\bft{x})$.
\end{lemma}

Therefore, by Lemma~\ref{lemma: property data monotone} it holds that
\begin{align*}
V(1,0) = \sum_{t = 1}^T r(x_t q(d_t, x_t)) \leq \sum_{t = 1}^T r(x_t q(\mathcal{D}_t(\bft{x}), x_t)) = \sum_{t = 1}^T r(p_t(\bft{x})) = U(\bft{x}).
\end{align*}
Thus, we can write:
\begin{align*}
U(\bft x) \geq V(1,0) > U(\bft x^\star) - L_r \sum_{i = 1}^T \Delta_i \gamma^{i-1}.
\end{align*}

To complete the proof of Theorem~\ref{thm: alg guarantees fixed actions}, we prove the following lemma.
\begin{lemma} \label{lemma: alg fixed actions approximation}
It holds that $\sum_{i = 1}^T \Delta_i \gamma^{i-1} < \varepsilon T^2$.
\end{lemma}
This completes the proof of Theorem~\ref{thm: alg guarantees fixed actions}.
\end{proofof}




\begin{proofof}{lemma: alg fixed actions data gap}

We prove this lemma using backward induction, starting with the base case from round $T$. For that, we start by bounding the difference in proportions using the following lemma.

\begin{lemma} \label{lemma: limited diff q same action}
Let $d^1, d^2 \in \mathbb{R}_{\geq 0}$ and $y \in [0, 1]$. If $d^1 < d^2$ then $0 \leq y(q(d^2, y) - q(d^1, y)) < d^2 - d^1$.   
\end{lemma}


In round $T$ it holds that $\left| d - \mathcal{D}_T(\bft{x}^\star)\right| < \Delta_T$. Therefore, for every $y \in A$ it holds that
\begin{align*}
y\left| q(d, y) - q(\mathcal{D}_T(\bft{x}^\star), y) \right| < \left| d - \mathcal{D}_T(\bft{x}^\star) \right| < \Delta_T.
\end{align*}
Let $d'(y) = \epsfloor{d + (1-yq(d, y))}$. Consequently,
\begin{align*}
\left| V(T,d) - U_T(\bft x^\star) \right| &= \left| \max_{y \in A} \{ r(y q(d, y)) + \gamma V(T+1,d'(y))\} - U_T(\bft x^\star) \right| \\
&= \left| \max_{y \in A} r(y q(d, y)) - U_T(\bft x^\star) \right| \\
&= \left| \max_{y \in A} r(y q(d, y)) - r(x^\star_T q(\mathcal{D}_T(\bft{x}^\star), \bft x^\star_T)) \right| \\
&= \left| r(\max_{y \in A} y q(d, y)) - r(x^\star_T q(\mathcal{D}_T(\bft{x}^\star), \bft x^\star_T)) \right| \\
&= L_r \left| \max_{y \in A} y q(d, y) - x^\star_T q(\mathcal{D}_T(\bft{x}^\star), \bft x^\star_T) \right| \\
&< L_r \Delta_T.
\end{align*}

We finished with the base case and move on to the induction step. Assume the lemma is true for $t + 1$ and we show it holds for round $t$.

according to the assumptions in the lemma, it holds that $\left| d - \mathcal{D}_t(\bft{x}^\star)\right| < \Delta_t$, therefore according to Lemma~\ref{lemma: limited diff q same action}, for every $y \in A$ it holds that
\[
 \left| r(yq(d, y)) - r(yq(\mathcal{D}_t(\bft{x}^\star), y)) \right| \leq L_r \left| yq(d, y) - yq(\mathcal{D}_t(\bft{x}^\star), y) \right|  < L_r \left| d - \mathcal{D}_t(\bft{x}^\star) \right| < L_r \Delta_t.
\]

We use the next lemma to bound the difference in data at step $t+1$.

\begin{lemma} \label{lemma: alg fixed actions diff data}
it holds that $\left| f(\mathcal{D}_t(\bft{x}^\star), y) - \epsfloor{f(d, y)} \right| < \Delta_{t+1}$.
\end{lemma}

Lemma~\ref{lemma: alg fixed actions diff data} suggests that the condition for the induction step holds, and therefore according to our induction step:
\begin{align*}
V(t+1,\epsfloor{f(d, x^\star_t)}) > U_{t+1}(\bft{x}^\star) - L_r\sum_{i = t+1}^T \Delta_i \gamma^{i - (t+1)};
\end{align*}
therefore,
\begin{align*}
v_d(x^\star_t) &= x^\star_t q(d, x^\star_t) + \gamma V(t+1,\epsfloor{f(d, x^\star_t)}) \\
&> x^\star_t q(\mathcal{D}_t(\bft{x}^\star), x^\star_t) - L_r \Delta_t + \gamma V(t+1,\epsfloor{f(d, x^\star_t)}) \\
&> x^\star_t q(\mathcal{D}_t(\bft{x}^\star), x^\star_t) - L_r \Delta_t + \gamma \left( U_{t+1}(\bft x^\star) - L_r \sum_{i = t+1}^T \Delta_i \gamma^{i - (t+1)} \right) \\
&= x^\star_t q(\mathcal{D}_t(\bft{x}^\star), x^\star_t) + \gamma U_{t+1}(\bft x^\star) - L_r \Delta_t - \gamma L_r \sum_{i = t+1}^T \Delta_i \gamma^{i - (t+1)} \\
&= U_t(\bft x) - L_r \sum_{i = t}^T \Delta_i \gamma^{i - t}.
\end{align*}

Finally, it holds that
\begin{align*}
V(t,d) &= \max_{y \in A} v_d(y) \geq v_d(x_t^\star) > U_t(\mathcal{D}, \bft x) - L_r \sum_{i = t}^T \Delta_i \gamma^{i-t}.
\end{align*}

This completes the proof of Lemma~\ref{lemma: alg fixed actions data gap}.
\end{proofof}


\begin{proofof}{lemma: limited diff q same action}
Since $d^2 > d^1$ then according to Proposition~\ref{lemma: monotone data} it holds that
\begin{align*}
f(d^2, y) - f(d^1, y) > 0;
\end{align*}
hence,
\begin{align*}
f(d^2, y) - f(d^1, y) &= d^2 + (1-yq(d^2, y)) - d^1 - (1-yq(d^1, y)) \\
&= d^2 - yq(d^2, y) - d^1 + yq(d^1, y) > 0.
\end{align*}
Rearranging the above inequality, we get that
\begin{align*}
y(q(d^2, a) - q(d^1, a)) < d^2 - d^1.
\end{align*}

Furthermore, from Lemma~\ref{lemma: property data monotone} it holds that $q(d^2, a) \geq q(d^1, a)$ and therefore we can summarize
\begin{align*}
0 \leq y(q(d^2, a) - q(d^1, a)) < d^2 - d^1.
\end{align*}
This completes the proof of \Cref{lemma: limited diff q same action}.
\end{proofof}






\begin{proofof}{lemma: alg fixed actions diff data}
We prove that for any $\mathcal{D} \in \mathbb{R}_{\geq 0}$ it holds that
it holds that $\left| f(D, y) - \epsfloor{f(d, y)} \right| < \Delta_{t+1}$.

First, we use the following lemma.
\begin{lemma}\label{lemma: non-expanding data}
Let $d^1, d^2 \in [0, T]$ then it holds that
\[
\left| f(d^1, y) - f(d^2, y) \right| < \left| d^1 - d^2 \right|.
\]
\end{lemma}

Therefore, using Lemma~\ref{lemma: non-expanding data} we get
\begin{align*}
\left| f(\mathcal{D}, y) - \epsfloor{f(d, y)} \right| &\leq \left| f(\mathcal{D}, y) - f(d, y) \right| + \varepsilon \\
&< \left| \mathcal{D} - d \right| + \varepsilon \\
&\leq \Delta_t + \varepsilon \\
&= (t-1)\varepsilon + \varepsilon \\
&= t\varepsilon = \Delta_{t+1}.
\end{align*}

This completes the proof of Lemma~\ref{lemma: alg fixed actions diff data}.
\end{proofof}

\begin{proofof}{lemma: non-expanding data}
Assume without loss of generality that $d^1 < d^2$. Therefore, according to Proposition~\ref{lemma: monotone data}, for every $y \in A$ it holds that $f(d^1, y) < f(d^2, y)$. Furthermore, from Lemma~\ref{lemma: property data monotone} it holds that $q(d^1, y) \leq q(d^2, a)$. Thus, we can write:

\begin{align*}
\left| f(d^2, y) - f(d^1, y) \right| &= f(d^2, y) - f(d^1, y) \\
&= d^2 + (1-yq(d^2, y)) - d^1 - (1-yq(d^1, y)) \\
&= d^2 - d^1 - yq(d^2, y) + yq(d^1, y) \\
&\leq d^2 - d^1 - yq(d^1, y) + yq(d^1, y) \\
&= d^2 - d^1 \\
&\leq \left| d^2 - d^1 \right|.
\end{align*}

This completes the proof of Lemma~\ref{lemma: non-expanding data}.
\end{proofof}


\begin{proofof}{lemma: alg fixed action lower data}
By definition and by Proposition~\ref{lemma: monotone data} it holds that:
\begin{align*}
d_{t+1} = \epsfloor{d_t + (1-x_t q(d_t, x_t))} = \epsfloor{f(d_t, x_t)} \leq f(d_t, x_t) \leq f(\mathcal{D}_t(\bft{x})) = \mathcal{D}_{t+1}(\bft{x}).
\end{align*}   

This completes the proof of Lemma~\ref{lemma: alg fixed action lower data}.
\end{proofof}


\begin{proofof}{lemma: alg fixed actions approximation}
Since $\gamma \leq 1$ it holds that $\sum_{i = 1}^T \Delta_i \gamma^{i-1} \leq \sum_{i = 1}^T \Delta_i$. Notice that we now have a sum of an arithmetic series and therefore

\begin{align*}
\sum_{i = 1}^T \Delta_i &= \varepsilon\sum_{i = 1}^T (i-1) = \varepsilon \sum_{i = 0}^{T-1} i = \varepsilon \frac{T(T-1)}{2} < \varepsilon T^2.
\end{align*}

This completes the proof of Lemma~\ref{lemma: alg fixed actions approximation}.
\end{proofof}




\begin{proofof}{thm: ASR alg optimal bound}
Denote $\bft{x}^\star = \max_{\bft{x}'} U(\bft{x}')$ and we define the following $T$ different strategies $\{ \bft{x}(i) \}_{i = 1}^{T+1}$ such that
\begin{align*}
    x(i)_t = \begin{cases}
        \deltfloor{x^\star_t} & \mbox{$t \geq i$} \\
        x^\star_t & \mbox{Otherwise}
    \end{cases}.
\end{align*}

Notice that by definition $\bft{x}(T+1) = \bft{x}^\star$. Furthermore, observe that the strategies $\bft{x}(i)$ and $\bft{x}(i+1)$ for every $i \in [T]$ differ only in round $i$. The following lemma bound the difference between strategy $\bft{x}(i)$ and strategy $\bft{x}(i+1)$.

\begin{lemma} \label{lemma: similar schemes bounds}
For every $i \in [T]$ it holds that
\begin{align*}
\left| U(\bft{x}(i)) - \bft{x}(i+1) \right| \leq \frac{\gamma^{i - 1}}{1-\gamma} \left(\frac{7}{4}\beta + 1\right) L_r \delta.
\end{align*}
\end{lemma}

Observe that 
\begin{align*}
\left| U(\bft{x}(1)) - U(\bft{x}(T+1)) \right| \leq \sum_{i = 1}^T \left| U(\bft{x}(i)) - U(\bft{x}(i+1)) \right|.
\end{align*}

Therefore, by lemma~\ref{lemma: similar schemes bounds} we get that
\begin{align*}
\left| U(\bft{x}(1)) - U(\bft{x}(T+1)) \right| \leq \sum_{i = 1}^T \frac{\gamma^{i - 1}}{1-\gamma} \left(\frac{7}{4}\beta + 1\right) L_r \delta \leq \frac{7\beta + 1}{4\left(1-\gamma \right)^2}  L_r \delta.
\end{align*}

Lastly, notice that $U(\bft{x}^\star) \geq \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') \geq U(\bft{x}(1))$. Therefore, we can write:

\begin{align*}
\left| U(\bft{x}^\star) - U(\bft{x}) \right| &= \left| U(\bft{x}^\star) - \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') + \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') - U(\bft{x}) \right| \\
&\leq \left| U(\bft{x}^\star) - \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') \right| + \left| \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') - U(\bft{x}) \right| \\
&\leq \left| U(\bft{x}^\star) - U(\bft{x}(1)) + U(\bft{x}(1)) - \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') \right| + \left| \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') - U(\bft{x}) \right| \\
&\leq \left| U(\bft{x}^\star) - U(\bft{x}(1)) \right| + \left| \max_{\bft{x}' \in A_\delta^T} U(\bft{x}') - U(\bft{x}) \right| \\
&\leq \frac{7\beta + 1}{4\left(1-\gamma \right)^2}  L_r \delta + \varepsilon L_r T^2.
\end{align*}

This completes the proof of Theorem~\ref{thm: ASR alg optimal bound}.
\end{proofof}

\begin{proofof}{lemma: similar schemes bounds}
By definition, $x(i)_t = x(i+1)_t$ for every $t < i$ and therefore $\mathcal{D}_t(\bft{x}(i)) = \mathcal{D}_t(\bft{x}(i+1))$.

Next, we use the following lemma:
\begin{lemma} \label{lemma: q equality ref}
For every $\mathcal{D}^1, \mathcal{D}^2 \in [0, T]$ and $x, x' \in [0, 1]$ it holds that
\begin{align*}
\left| q(\mathcal{D}, x) - q(\mathcal{D}, x') \right| = q(\mathcal{D}, x) \left(1-q(\mathcal{D}, x')\right) \left| 1 - e^{\beta \left(x' a(\mathcal{D}) - x a(\mathcal{D}) \right)} \right|.
\end{align*}
\end{lemma}

Notice that in our case, for every $\left| x(i)_i - x(i+1)_i \right| < \delta$. Therefore,

\begin{align*}
& \left| q(\mathcal{D}_i(\bft{x}(i)), x(i)_i) - q(\mathcal{D}_i(\bft{x}(i+1)), x(i+1)_i) \right| \\
&= q(\mathcal{D}_i(\bft{x}(i)), x(i)_i) \left(1-q(\mathcal{D}_i(\bft{x}(i+1)), x(i+1)_i)\right) \left| 1 - e^{\beta a(\mathcal{D}_i(\bft{x}(i))) \left(x(i+1)_i - x(i)_i \right)} \right| \\
&\leq \left| 1 - e^{\beta a(\mathcal{D}_i(\bft{x}(i))) \left(x(i+1)_i - x(i)_i \right)} \right| \\
&\leq \frac{7}{4} \beta \delta.
\end{align*}
Where the last inequality follows from $\left|1 - e^x \right| \leq \frac{7x}{4}$ for every $\left| x \right| < 1$. Next, notice that for every $\mathcal{D} \in \mathbb{R}_{\geq 0}$ and $x, x' \in [0, 1]$ such that $\left|x - x'\right| \leq \delta$ it holds that

\begin{align*}
\left| xq(\mathcal{D}, x) - x'q(\mathcal{D}, x') \right| &= \left| xq(\mathcal{D}, x) - \left(x' -x + x \right)q(\mathcal{D}, x') \right| \\
&= \left| xq(\mathcal{D}, x) - xq(\mathcal{D}, x') - \left(x' -x\right)q(\mathcal{D}, x') \right| \\
&\leq \left| xq(\mathcal{D}, x) - xq(\mathcal{D}, x') \right| + \left|x' -x\right| q(\mathcal{D}, x') \\
&\leq \left| xq(\mathcal{D}, x) - xq(\mathcal{D}, x') \right| + \left|x' -x\right| \\
&\leq \left(\frac{7}{4}\beta + 1\right) \delta.
\end{align*}

Therefore, by Corollary~\ref{cor: loose bound} it holds that
\begin{align*}
\left| U(\bft{x}(i)) - U(\bft{x}(i+1)) \right| &\leq \gamma^{i - 1} \left| r(p_i(\bft{x}(i))) - r(p_i(\bft{x}(i+1))) \right| + L_r \gamma^{i} \frac{\left| p_i(\bft{x}(i)) - p_i(\bft{x}(i+1)) \right|}{1 - \gamma} \\
&\leq \gamma^{i - 1} L_r \left| p_i(\bft{x}(i)) - p_i(\bft{x}(i+1)) \right| + L_r \gamma^{i} \frac{\left| p_i(\bft{x}(i)) - p_i(\bft{x}(i+1)) \right|}{1 - \gamma} \\
&= \gamma^{i - 1} L_r \left| p_i(\bft{x}(i)) - p_i(\bft{x}(i+1)) \right| \frac{1}{1-\gamma} \\
&\leq \frac{\gamma^{i - 1}}{1-\gamma} \left(\frac{7}{4}\beta + 1\right) L_r \delta.
\end{align*}

This completes the proof of Lemma~\ref{lemma: similar schemes bounds}.
\end{proofof}

\begin{proofof}{lemma: q equality ref}
This lemma is a special case of Lemma~\ref{lemma: q equality} and is hence omitted. 
\end{proofof}






\subsection{Proofs Omitted from Subsection~\ref{sec: welfare constrained revenue maximization}}\label{sec:appendix of constrain}


\begin{proofof}{thm: alg welfare contraint}
The proof is constructed in 5 parts. First, we simplify and write our problem explicitly. Then, we define an approximation to our problem and build an MDP to describe it. The third step is to show that our approximation problem can be viewed as an instance of the problem in \cite{ben2024principal} and thus has an optimal solution. In the last step, we calculate the gap between the optimal solution of the approximated problem and the optimal solution of our original problem.

\paragraph{Step 1.}
We start by rewriting Problem~\ref{prob: max U welfare}. Notice that the welfare at each round can be written as
\begin{align*}
w_t(\bft{x}) = p_t(\bft{x}) w^g_t(\bft{x}) + (1-p_t(\bft{x})) w^s = p_t(\bft{x}) \left(w^g_t(\bft{x}) - w^s \right) + w^s.
\end{align*}
Therefore, the social welfare can be expressed as $W(\bft{x}) = T w^s + \sum_{t = 1}^T p_t(\bft{x}) \left(w^g_t(\bft{x}) - w^s \right)$. By denoting $W^1 = W - Tw^s$ we can rewrite our problem as 
\begin{align*}
    & \max_{\bft{x}} \sum_{t = 1}^T r(p_t(\bft{x}))  \\
    & s.t \quad \sum_{t = 1}^T p_t(\bft{x}) \left(w^g_t(\bft{x}) - w^s \right) \geq W^1.
\end{align*}

\paragraph{Step 2.} We now build a graph to represent an approximation of our problem. Notice that the maximum amount of data that can be generated in each round is $1$ and therefore $\mathcal{D}_t(\bft{x}) < T$ for every $t \in [T]$ and scheme $\bft{x}$. Therefore, given $\varepsilon > 0$, we discretize all the available data values by increments of $\frac{T}{\varepsilon}$. We know describe the components of our graph. Our graph is a deterministic MDP with an underlying layered graph as follows: let $S = \{S_1, \ldots S_{T+1} \}$ the set of all states where $S_t = \{ s_t^0, s_t^\varepsilon, \ldots, s_t^{\varepsilon T} \}$ denote the state in the $t$'th layer where $s_t^d$ represents the state where GenAI is in round $t$ with $d$ data. The set of actions is $A$, and there are 2 reward functions defined for each state-action pair. First is defined by $\mathcal{R}(s_t^{d}, y) = \epsfloor{r(yq(d, y))}$ while the second is $\mathcal{W}(s_t^{d}, y) = \epsfloor{yq(d, y) \left(a(d)y - w^s\right)}$. Next, we let $\mathcal{T}(s, y, s')$ denote the transition function, which denotes the probability of reaching state $s'$ by playing $y$ in state $s$. The transition function in our MDP is deterministic and defined by
\begin{align*}
\mathcal{T}(s_t^{d}, y, s_{t'}^{d'}) = \begin{cases}
    1 & \mbox{$t' = t+1$ and $d' = \epsfloor{d + 1- yq(d, y)}$} \\
    0 & \mbox{Otherwise}
\end{cases}.
\end{align*}
In terms of graphs, the states are analogous to vertices, and $T(s, y, s') = 1$ specifies an edge from state $s$ to state $s'$. An illustration of this graph for $\varepsilon = 0.5$ is presented in Figure~\ref{graph example}.
\input{Figures/graph_wcssp.tex}


By the construction of the layered graph, the horizon is $T+1$, and GenAI starts at state $s_1^0$. We define policy $\pi : S \rightarrow A$ to be the mapping between each state and the action GenAI should take in that state. For a deterministic MDP, a policy is equivalent to a path $\tau$, which in our case is a sequence of $T$ edges starting from state $s_1^0$ and leading to a state in $S_{T+1}$. Notice that each edge represents a state and an action from that state, and therefore, path $\tau$ can also be defined as a sequence of state-action pairs.

The problem we aim to solve using the graph is the following problem:
\begin{align}
& \max_{\tau} \sum_{(s, y) \in \tau} \mathcal{R}(s, y) \tag{P3} \label{eq: approx max U welfare} \\
& \nonumber s.t \sum_{(s, y) \in \tau} \mathcal{W}(s, y) \geq W^1.
\end{align}





\paragraph{Step 3.}
Recall that a selective response strategy is a vector that specifies the portion of queries GenAI should answers. 
We denote $\pi^{\bft{x}}$ that follows scheme $\bft{x}$, that is $\pi^{\bft{x}}$ assigns the same action to all states at round $t$ as $x_t$, formally $\pi^{\bft{x}}(s_t^d) = x_t$ for every $t \in [T]$ and $d \in \{0, \varepsilon, \ldots T \}$.

We now introduce some notations that we use in this step. First, we denote $U_t(\bft{x})$ and $W_t(\bft{x})$ the accumulative revenue and welfare from round $t$ until $T$, following scheme $\bft{x}$. Formally 
\begin{itemize}
\item $U_t(\bft{x}) = r(x_t q(\mathcal{D}_t(\bft{x}), x_t)) + U_{t+1}(\bft{x})$,
\item $W_t(\bft{x}) =  x_t q(\mathcal{D}_t(\bft{x}), x_t)\left( a(\mathcal{D}_t(\bft{x}))x_t - w^s \right) + W_{t+1}(\bft{x}).$
\end{itemize}

We now define the analog of $U_t$ and $W_t$ in our MDP. Let $V^{G}(\pi, s)$ denote the sum of rewards with respect to reward function $\mathcal{R}$, following policy $\pi$ and starting at state $s$ in our MDP. Similarly, denote $V^{W}(\pi, s)$ the sum of rewards with respect to reward function $\mathcal{W}$. Formally
\begin{itemize}
\item $V^G(\pi, s_t^d) = \mathcal{R}(s_t^d, \pi(s)) + V^G(\pi, s_{t+1}^{d'})$,
\item $V^W(\pi, s_t^d) = \mathcal{W}(s_t^d, \pi(s)) + V^W(\pi, s_{t+1}^{d'})$.
\end{itemize}

We are now ready to compare the values of the revenue and social welfare following a given selective response strategy to those from the MDP. Let $\bft{x}$ be an arbitrary selective response strategy and $M = \max \{1, L_r \}$. We use the following lemma.

\begin{lemma} \label{lemma: max u welfare bounds}
Fix round $t \in [T]$, then for every $d \in \{0, \varepsilon, \ldots T\}$ such that $\left|d - \mathcal{D}_t(\bft{x}) \right| < (t-1)\varepsilon$ it holds that

\begin{itemize}
\item $\left| V^G(\pi^{\bft{x}}, s_t^d) - U_t(\bft{x}) \right| \leq \varepsilon M \sum_{i = t}^T i$,
\item $\left| V^W(\pi^{\bft{x}}, s_t^d) - W_t(\bft{x}) \right| \leq \varepsilon (L_a+1) \sum_{i = t}^T i$.
\end{itemize}
\end{lemma}

Notice that $\sum_{i = 1}^T i < T^2$ and therefore we can simplify the summations in Lemma~\ref{lemma: max u welfare bounds}.

Given the optimal selective response strategy $\bft{x}^\star$, Lemma~\ref{lemma: max u welfare bounds} suggests that

\begin{itemize}
\item $V^G(\pi^{\bft{x}^\star}, s_1^0) \geq U_t(\bft{x}^\star) -  \varepsilon M T^2$,
\item $V^W(\pi^{\bft{x}^\star}, s_1^0) \geq W_t(\bft{x}^\star) - (L_a+1) \varepsilon T^2$.
\end{itemize}

We finished Step 2 and now move on to develop the machinery to find the selective response strategy that gives us the guarantees of our theorem.






\paragraph{Step 4.} We define the Weight-Constrained Shortest Path (WCSSP) \cite{guide_theory_np_completeness}. Given a weighted graph $G=(V, E)$ with weights $\{ w_e\}_{e \in E}$, costs $\{ c_e \}_{e \in E}$ and a maximum weight $W \in \mathbb{R}$, the problem is to find the path with the least cost while keeping the total weights below $W$. Let $\tau$ denote a path, and therefore, the WCSSP problem is defined as
\begin{align}
&\min_{\tau} \sum_{e \in \tau} c_e \tag{P4} \label{eq: wcssp} \\
& \nonumber \sum_{e \in \tau} w_e \leq \tilde{W}.
\end{align}

Problem~\ref{eq: approx max U welfare} can be seen as an instance of Problem~\eqref{eq: wcssp} for by setting:
\begin{itemize}
\item $c(s, y) = -\mathcal{R}(s, y)$,
\item $w(s, y) = -\mathcal{W}(s, y)$,
\item $\tilde{W} = -W^1$.
\end{itemize}

To account for the approximation error in the welfare due to calculating it using the MDP, we choose $\tilde{W} = - \left(W^1 - \varepsilon T^2 (L+1)\right)$.

Problem~\eqref{eq: wcssp} is a known NP-Hard problem with a reduction to the PARS-MDP problem \cite{ben2024principal} with a deterministic transition function. The PARSE-MDP problem is defined over an MDP with two reward functions $R^A, R^P : S \times A \rightarrow \mathbb{R}_{\geq 0}$ and a budget $B \in \mathbb{R}_{\geq 0}$. The goal is to construct a new reward function $R^B : S \times A \rightarrow \mathbb{R}_{\geq 0}$ such that the total rewards over the whole MDP is less than $B$, and the induced policy that maximizes $R^A + R^B$ also maximizes $R^P$ under the constraint. Formally, the PARS-MDP is defined as follows:

\begin{align}
& \nonumber \max_{R^B}{V(\pi, R^P)} \\
&  \sum_{s\in S, y\in A}{R^B(s, y)} \leq B \label{eq: principles problem}  \tag{P5}\\ 
& \nonumber  {R^B(s, y) \geq 0} \textnormal{ for every } s\in S, y\in A(s) \\
& \nonumber  \pi \in \mathcal{A}(R^A+R^B)
\end{align}

where $V(\pi, R^P)$ is the total sum of rewards from $R^P$ following policy $\pi$. Therefore, we make the following definitions to represent Problem~\eqref{eq: approx max U welfare} as an instance of Problem~\eqref{eq: principles problem}: First, denote $\tau^A$ the path that maximizes $R^A$, i.e $\tau^A \in \argmax_{\tau} \sum_{s, y \in \tau} R^A(s, y)$. Notice that $\tau^A$ can be computed using standard methods which run in polynomial time with respect to the problem's parameters. Thus, we refer to $\tau^A$ as a known parameter and define the parameters of PARS-MDP as follows:

\begin{itemize}
\item $R^P(s, y) = \mathcal{R}(s, y)$,
\item $R^A(s, y) = \mathcal{W}(s, y)$,
\item $B = \sum_{s, y \in \tau^A} R^A(s, y) - \left(W^1 - \varepsilon T^2 (L_a + 1)\right)$.
\end{itemize}

Notice that $\mathcal{W}, \mathcal{R}$ are in increments of $\varepsilon$ by the construction of our MDP. Therefore, we can use Theorem~$(5)$ from \cite{ben2024principal} to show that the optimal path of Problem~\eqref{eq: approx max U welfare} can be found in polynomial time with respect to the problem's parameters.

\begin{theorem} \label{thm: pars-mdp alg}
There is a known algorithm to compute the path $\tilde{\tau}$ which induces
\begin{itemize}
    \item $\sum_{s, y \in \tilde{\tau}} R^P(s, y) = \max_{\tau} \sum_{s, y \in \tau} R^P(s, y)$,
    \item $\sum_{s, y \in \tilde{\tau}} R^A(s, y) \geq \sum_{s, y \in \tau^A} R^A(s, y) - B$.
\end{itemize}
in time $O(\frac{|S||A|T}{\varepsilon} \log(\frac{|A|T}{\varepsilon}))$.
\end{theorem}


Using the terms from our MDP, the solution from the algorithm in Theorem~\ref{thm: pars-mdp alg} guarantees

\begin{itemize}
\item $\sum_{s, y \in \tilde{\tau}} \mathcal{R}(s, y) = \max_{\tau} \sum_{s, y \in \tau} \mathcal{R}(s, y)$,
\item $\sum_{s, y \in \tilde{\tau}} \mathcal{W}(s, y) \geq W^1 -  \varepsilon T^2 (L_a+1)$.
\end{itemize}

Let $\tau^\star$ be the path corresponding to $\bft{x}^\star$. Notice that $\tau^\star$ guarantees
\[
\sum_{s, y \in \tau^\star} \mathcal{W}(s, y) \geq W(x^\star) - \varepsilon T^2 (L_a+1) \geq W^1 - \varepsilon T^2 (L_a+1).
\]

The path $\tau^\star$ is a possible solution of the PARS-MDP and therefore, by Theorem~\ref{thm: pars-mdp alg}, the path $\tilde{\tau}$ guarantees

\begin{itemize}
\item $\sum_{s, y \in \tilde{\tau}} \mathcal{R}(s, y) \geq \sum_{s, y \in \tau^\star} \mathcal{R}(s, y) \geq U(\bft{x}^\star) - \varepsilon M T^2$,
\item $\sum_{s, y \in \tilde{\tau}} \mathcal{W}(s, y) \geq W^1 - \varepsilon T^2 (L_a+1)$.
\end{itemize}


\paragraph{Step 5.} 
Let $\tilde{\bft{x}}$ be the selective response strategy corresponding to path $\tilde{\tau}$ and we compare the revenue and welfare when playing $\tilde{\bft{x}}$.

We begin with the following lemma.

\begin{lemma} \label{lemma: alg lower data}
Fix scheme $\bft{x}$ Let $(d_t)_{t = 1}^T$ be the sequence defined by $d_t = 0$ and $d_{t+1} = f^\varepsilon(d_t, x_t)$ then for every $t \in [T]$ it holds that $d_t < \mathcal{D}_{t}(\bft{x})$.
\end{lemma}

Let $(d_t)_{t = 1}^T$ be the sequence defined by $d_t = 0$ and $d_{t+1} = f^\varepsilon(d_t, \tilde{x}_t)$, then by Lemma~\ref{lemma: property data monotone} we get that

\begin{align*}
V^G(\pi^{\tilde{\bft{x}}}, s_1^0) &= \sum_{t = 1}^T \epsfloor{r(yq(d_t, \tilde{x}_t))} \leq \sum_{t = 1}^T r(yq(d_t, \tilde{x}_t)) \\
& \leq \sum_{t = 1}^T r(yq(\mathcal{D}_t(\tilde{\bft{x}}), \tilde{x}_t)) = \sum_{t = 1}^T r(p_t(\tilde{\bft{x}})) = U(\tilde{\bft{x}}).
\end{align*}

Therefore, it holds that
\[
U(\tilde{\bft{x}}) > V^G(\pi^{\tilde{\bft{x}}}, s_1^0) > U(\bft{x}^\star) - \varepsilon M T^2.
\]

We move on to evaluate the welfare. Notice that the welfare is not monotonic in $p_t$. Instead of using our previous technique, we use Lemma~\ref{lemma: max u welfare bounds} and get that

\begin{align*}
W(\tilde{\bft{x}}) &\geq V^W(\tilde{\bft{x}}, s_1^0) - \varepsilon T^2 (L_a+1) \\
& \geq W^1 - \varepsilon T^2 (L_a+1) - \varepsilon T^2 (L_a+1) \\
& = W^1 - 2\varepsilon T^2 (L_a+1).
\end{align*}

This completes the proof of Theorem~\ref{thm: alg welfare contraint}.
\end{proofof}







\begin{proofof}{lemma: max u welfare bounds}
We begin by showing that there cannot be a large gap between the data accumulated in the original problem and the data according to our MDP following the same scheme.
Let $\Delta_t = (t-1)\varepsilon$ and let $f^\varepsilon(d, y) = \epsfloor{f(d, y)} = \epsfloor{d + 1 - yq(d, y)}$ the data in the next round given that in the current round, GenAI started with $d$ data and played $y$. We use the following lemma to show that the accumulated data in the MDP cannot be too far from the accumulated in our original problem.

\begin{lemma} \label{lemma: max u welfare data bound}
Let $d \in \{0, \varepsilon, \ldots T\}$ and fix round $t \in [T]$. If $\left| d - \mathcal{D}_t(\bft{x}) \right| < \Delta_t$ then $\left| f^\varepsilon(d, x_t) - \mathcal{D}_{t+1}(\bft{x}) \right| < \Delta_{t+1}$.
\end{lemma}

We now use backward induction to prove our lemma, starting at round $T$. Let $d \in \{0, \varepsilon, \ldots, T \}$ such that $\left| d - \mathcal{D}_T(\bft{x})\right| < \Delta_T$ and we use the following lemma:

\begin{lemma} \label{lemma: same action approx bounded q}
Let $d \in \{0, \varepsilon, \ldots T\}$ and fix round $t \in [T]$. If $\left| d - \mathcal{D}_t(\bft{x}) \right| < \Delta_t$ then it holds that
\[
\left| \epsfloor{r(x_t q(d, x_t))} - r(x_t q(\mathcal{D}_t(\bft{x}), x_t)) \right| \leq M \Delta_{t+1}.
\]
\end{lemma}


Therefore, by Lemma~\ref{lemma: same action approx bounded q} we get that

\begin{align*}
\left| V^G(\pi^{\bft{x}}, s_T^d) - U_T(\bft{x}) \right| = \left| \epsfloor{r(x_T q(d, x_T))} - r(x_T q(\mathcal{D}_T(\bft{x}), x_T)) \right| \leq M \Delta_{T+1} .
\end{align*}

Similarly for $V^W$ and $W_T$. We use the following lemma.

\begin{lemma} \label{lemma: same action bounded welfare}
Let $d^1, d^2 \in \mathbb{R}_{\geq 0}$ and any $y \in [0, 1]$ then it holds that
\[
\left| yq(d^1, y)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \leq \left| d^1 - d^2 \right| (L_a + 1).
\]
\end{lemma}

Therefore, it holds that:

\begin{align*}
&\left| V^W(\pi^{\bft{x}}, s_T^d) - W_T(\bft{x}) \right| \\
&= \left| \epsfloor{x_T q(d, x_T) \left(a(d) x_T - w^s \right)} - x_T q(\mathcal{D}_T(\bft{x}), x_T) \left( a(\mathcal{D}_T(\bft{x})) x_T - w^s \right) \right| \\
&\leq \left| x_T q(d, x_T) \left(a(d) x_T - w^s \right) - x_T q(\mathcal{D}_T(\bft{x}), x_T) \left( a(\mathcal{D}_T(\bft{x})) x_T - w^s \right) \right| + \varepsilon \\
&\leq \left| d - \mathcal{D}_T(\bft{x}) \right| (L+1) + \varepsilon \\
&\leq \Delta_T (L_a+1) + \varepsilon \\
&< \Delta_{T+1} (L_a+1).
\end{align*}

We are done with the base case and can continue towards the induction step. Assume the lemma holds for round $t+1$, and we prove it for round $t$.

We start with the revenue at round $t$. Let $d \in \{ 0, \varepsilon, \ldots, T\}$ and denote $d' = f^\varepsilon (d, x_t)$. Then, for every $d$ such that $\left| d - \mathcal{D}_t(\bft{x}) \right| < \Delta_t$ it holds that

\begin{align*}
\left| V^G(\pi^{\bft{x}}, s_t^d) - U_t(\bft{x}) \right| &= \left| \epsfloor{r(x_t q(d, x_t))} + V^G(\pi^{\bft{x}}, s_{t+1}^{d'}) - r(x_t q(\mathcal{D}_t(\bft{x}), x_t)) - U_{t+1}(\bft{x}) \right| \\
&\leq \left| \epsfloor{r(x_t q(d, x_t))} - r(x_t q(\mathcal{D}_t(\bft{x}), x_t)) \right| + \left| V^G(\pi^{\bft{x}}, s_{t+1}^{d'}) - U_{t+1}(\bft{x}) \right|.
\end{align*}

We use Lemma~\ref{lemma: same action approx bounded q} to bound the first expression. Furthermore, notice that according to Lemma~\ref{lemma: max u welfare data bound} it holds that $V^G(\pi^{\bft{x}}, s_{t+1}^{d'})$ satisfies the conditions of our induction step. Therefore,

\begin{align*}
\left| V^G(\pi^{\bft{x}}, s_t^d) - U_t(\bft{x}) \right| &\leq M\Delta_{t+1} + \sum_{i = t+2}^{T+1} M\Delta_i = M\sum_{i = t}^{T} \Delta_{i+1} = \varepsilon M\sum_{i = t}^T i.
\end{align*}

We perform a similar calculation for the welfare:

\begin{align*}
&\left| V^W(\pi^{\bft{x}}, s_t^d) - W_t(\bft{x}) \right| \\
&= \big| \epsfloor{x_t q(d, x_t) \left(a(d) x_t - w^s \right)} + V^W(\pi^{\bft{x}}, s_{t+1}^{d'}) \\
&\quad - x_t q(\mathcal{D}_t(\bft{x}), x_t) \left( a(\mathcal{D}_t(\bft{x})) x_t - w^s \right) - W_{t+1}(\bft{x}) \big| \\
&\leq \big| \epsfloor{x_t q(d, x_t) \left(a(d) x_t - w^s \right)} - x_t q(\mathcal{D}_t(\bft{x}), x_t) \big| \\
&\quad + \big| V^W(\pi^{\bft{x}}, s_{t+1}^{d'}) - W_{t+1}(\bft{x}) \big| \\
&\leq \Delta_{t+1} (L_a+1) + (L_a+1)\sum_{i = t+2}^{T+1} \Delta_i \\
&= (L_a+1) \sum_{i = t}^T \Delta_{i+1} = (L_a+1) \varepsilon \sum_{i = t}^T i.
\end{align*}
This completes the proof of Lemma~\ref{lemma: max u welfare bounds}.
\end{proofof}





\begin{proofof}{lemma: max u welfare data bound}
This is a special case of Lemma~\ref{lemma: alg fixed actions diff data} and is hence omitted.
\end{proofof}


\begin{proofof}{lemma: same action approx bounded q}
By Lemma~\ref{lemma: limited diff q same action}, it holds that
\begin{align*}
\left| \epsfloor{r(x_t q(d, x_t)}) - r(x_t q(\mathcal{D}_T(\bft{x}), x_t)) \right| &\leq \left| r(x_t q(d, x_t)) - r(x_t q(\mathcal{D}_T(\bft{x}), x_t)) \right| + \varepsilon \\ 
& \leq L_r \left| x_t q(d, x_t) - x_t q(\mathcal{D}_T(\bft{x}), x_t) \right| + \varepsilon \\
&\leq L_r \left| d - \mathcal{D}_T(\bft{x}) \right| + \varepsilon \\
&\leq L_r \Delta_t + \varepsilon \\
&\leq \Delta_{T+1} \max \{ L_r, 1 \}.
\end{align*}

This completes the proof of Lemma~\ref{lemma: same action approx bounded q}.
\end{proofof}


\begin{proofof}{lemma: same action bounded welfare}
We prove starting from the definition.

\begin{align*}
& \left| yq(d^1, y)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \\
&= \left| y\left( q(d^1, y) - q(d^2, y) + q(d^2, y) \right)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \\
&\leq \left| yq(d^2, y)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \\
&+ \left| y\left( q(d^1, y) - q(d^2, y)\right)\left(a(d^1)y - w^s \right) \right| \\
&\leq \left| yq(d^2, y)\left(a(d^1)y - w^s - a(d^2)y + w^s\right) \right| + \left| y\left( q(d^1, y) - q(d^2, y)\right)\left(a(d^1)y - w^s \right) \right| \\
&= \left| yq(d^2, y)\left(a(d^1)y - a(d^2)y\right) \right| + \left| y\left( q(d^1, y) - q(d^2, y)\right)\left(a(d^1)y - w^s \right) \right|.
\end{align*}

We use Lemma~\ref{lemma: limited diff q same action} and therefore
\begin{align*}
& \left| yq(d^1, y)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \\
&\leq \left| yq(d^2, y)\left(a(d^1)y - a(d^2)y\right) \right| + \left| d^1 - d^2\right| \left|a(d^1)y - w^s \right| \\
&= y^2 q(d^2, y) \left| a(d^1) - a(d^2) \right| + \left| d^1 - d^2\right| \left|a(d^1)y - w^s \right| \\
&\leq y^2 q(d^2, y) L \left| d^1 - d^2 \right| + \left| d^1 - d^2\right| \left|a(d^1)y - w^s \right|.
\end{align*}

Notice that $y, q(d^2, y), a(d^1), w^s \leq 1$ and thus we get that
\begin{align*}
& \left| yq(d^1, y)\left(a(d^1)y - w^s \right) - yq(d^2, y)\left(a(d^2)y - w^s \right) \right| \\
&\leq L_a\left| d^1 - d^2 \right| + \left| d^1 - d^2 \right| = (L_a+1) \left| d^1 - d^2 \right|.
\end{align*}
This completes the proof of Lemma~\ref{lemma: same action bounded welfare}.
\end{proofof}


\begin{proofof}{lemma: alg lower data}
This is a special case of Lemma~\ref{lemma: alg fixed action lower data} and is hence omitted.
\end{proofof}