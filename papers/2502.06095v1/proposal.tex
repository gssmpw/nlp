
\section{Proposed Solution: Rateless and Lossy Source-Channel Codes}
\label{sec:proposal}

With the limitation in information exchange described in \remref{rem:exchange}, we propose a joint venture between the network and the application where the following  set of ground rules are established initially between the two and followed during the course of semantic communication service. 
\begin{itemize}
    \item The network and the application exchange over a binary  interface. Thus, the end-to-end channel from the application's perspective is a vector binary channel. The network stabilizes the end-to-end  link by maintaining a bit flipping ratio (or \gls{ber}) across frames. 
    \item The source-channel code operated by the application is \emph{rateless} in that it is designed and optimized for a %(continuous or discrete) 
    range of rate values $[R_{lo}, R_{hi}]$. The source signal is  encoded at rate $R_{hi} = K/N$ and the network decides on the actual rate $(K-L)/N$ within that range, by puncturing $L$ bits out of a codeword it receives from the application. Without loss of generality, we assume that the $L$ bits are selected from the end of the codeword.
\end{itemize}

The puncturing length $L$ is modeled as a non-negative integer discrete random variable with distribution $\mathsf{P}_L$, where $L \in \{0, \ldots, \bar{L}\}$. At each time $t$, it is determined by the \emph{scheduler} at the network (denoted by function $r$ in \figref{fig:system-model}) based on the state of the network, $(\tilde{\gamma_t},P_t,W_t)$.

\begin{remark}
    The joint venture of the network and the application in this problem can be seen as a cooperative interaction in game theory. The players, network and application, exchange some ground rules---known as service level agreement or link configuration in telecommunication terminology---and cooperate towards achieving the goal. This also implies network's cooperative effort to maximize $P_t$ and $W_t$ within its practical limits.
\end{remark}

% \begin{remark}
%     A straightforward implementation of the application's rateless code is to generate several encoded copies of the source signal at different rates, covering the desired range of code rates. The copies are delivered to the network and the network chooses the copy with the right rate match to the achievable rate on the end-to-end link. Such approach can become very inefficient as the number of copies grow large, more so, if the end-to-end link between source and destination spans more than one radio hop---for instance, when both the source and destination are connected wirelessly to the internet.
% \end{remark}

\subsection{Network's Operation: Rate-Adaptive and Stable Link for  Semantic Communications}
\label{sec:ratelessradiolink}

Let us focus on the single-user setting, where total transmit power $\bar{P}$ and total channel use $\bar{W}$ is allocated to the user. The block fading channel provides a \gls{snr} of $\bar{P}\gamma / (\sigma_n \bar{W})$, where $\sigma_n$ is the \gls{awgn} noise spectral density. The network uses modulated communication, e.g., from \gls{qam} constellation, to transmit the bits over  faded and noisy channel. Using the knowledge of  $\tilde{\gamma}$, the conditional cummulative distribution of $\gamma$ is denoted by $\mathsf{F}_{\gamma | \tilde{\gamma}}$.
% and the corresponding estimated \gls{snr} of $\tilde{\mathsf{snr}} = \bar{P}\tilde{\gamma} / (\sigma_n \bar{W})$ 
The network can choose a  modulation constellation to guarantee a target bit-flipping ratio between $\mathbf{x}$ and $\mathbf{y}$. 

Let us assume the modulation  constellation order $M$ is chosen from a finite ordered set $ \{M_1 < M_2 < \ldots < M_c\}$. Furthermore, let us use $\mathsf{Q}(\mathsf{snr},M) = \mathsf{ber}$ to denote the deterministic mapping between \gls{snr}, $M$ and the expected value of \gls{ber} denoted by $\mathsf{ber}$. The inverse of this mapping for given $M$ is  available and provides  $\mathsf{Q}^{-1}(q_0,M) = \mathsf{snr}_{M,\text{th}}$, where $\mathsf{snr}_{M,\text{th}}$ is the threshold of \gls{snr} for $q_o$ \gls{ber} cross-over for modulation order $M$. Similarly, we have $\mathsf{\gamma}_{M,\text{th}} = \frac{\sigma_n \bar{W} } { \bar{P}} \mathsf{snr}_{M,\text{th}}$.

Both $\mathsf{Q}(.)$ and $\mathsf{Q}^{-1}(.)$ can be empirically derived through sufficient Monte Carlo sampling of the underlying channel distribution.  The network chooses the $M$  that yields the highest spectral efficiency, i.e.,  $\log_2 M$, while stabilizing a target \gls{ber} around $q_o$ for every block, by
\begin{align} \label{eq:modorder}
M = & \arg \max_{m \in \{M_1, \ldots,  M_c\}}  m
\\ \nonumber
&  s.t.  \quad \mathsf{F}_{\gamma | \tilde{\gamma}}\left( \mathsf{\gamma}_{M,\text{th}} \right) \leq \epsilon.
\end{align}
The constraint in \eqref{eq:modorder} aims to curb the chance of \gls{ber} above $q_o$, thus stabilizes the link \gls{ber}.  Note that \eqref{eq:modorder} can readily be extended to a frequency selective channel with multitude of coherence blocks for each frame of communication.
The combination of $M$ and ${W}_t$ then determine the number of bits  that can be supported by the network during block $t$. The parameter $\epsilon \in (0, 1) $ measures the \emph{instability} of the end-to-end binary link  and can be seen as a semantic communication \gls{qos} metric (see discussion in \secref{sec:blueprint}).

\begin{definition}
    A stabilized vector binary symmetric channel $\text{SVBSC}(q_o,\epsilon)$ is a set of parallel \glspl{bsc}  of $\text{BSC}(q)$ where $q$ is stabilized ``around'' $q_o$, that is, $\mathsf{Pr}[ q > q_o] \leq \epsilon$. Similarly, $\text{SVBSC}(q_o,\epsilon, K, \bar{L})$ is a stabilized vector binary symmetric channel of length $K-L$, where the discrete random variable $L \in \{0,\ldots, \bar{L}\}$.
\end{definition}

The network therefore adopts a rate-control process through  mapping $r$, where $L$ ($L \leq \bar{L} < K$) bits are dropped before transmission.%, causing $(K - L)/K \leq 1$. 
The rate-control function $r$ thus intelligently  \emph{punctures}  $L$ bits out of the $K$ bits received from the source, and provides a $\text{SVBSC}(q_o,\epsilon)$ of length $K - L$ for the remainder of the bits. 

\begin{remark}
    It's worth stressing that such rate-control mechanism is a new paradigm in communication where a portion of the data bits can be intentionally removed by the network. In the conventional communication systems, the network doesn't discard  information bits intentionally---it may deliver them correctly to the receiver as a packet, or drop them altogether, due to packet transmission failure.
\end{remark}

At the receiver side of the network the punctured bits are padded with null values $\varnothing$ (or, 0.5, which contains null information in binary), where the operation is denoted here by $r^{*}$, effectively notifying the decoder $g$ of puncturing of those bits\footnote{In practice, due to the binary interface between network and application on both ends, instead of null bits an accompanying control signal from the network to the application notifies the latter about $L$.}. The maximum number of punctured bits $\bar{L}$ is guaranteed by the network and can be seen as another semantic communication \gls{qos} metric, e.g., corresponding to the highest distortion tolerable by the application. Additionally, a cap on the long-term average of puncturing length over frames can be agreed between the application and the network as another \gls{qos} metric, to guarantee network's best effort in minimizing puncturing (see discussion in \secref{sec:blueprint}).


\begin{remark}
    The uncoded QAM version of $\text{SVBSC}(q_o,\epsilon, K, \bar{L})$, presented above, is only appropriate for the simple case where a channel coherence block and the communication frame align. More commonly, a communication frame may tolerate a latency that spans over multiple coherence blocks in time. Then, use of retransmission techniques, such as hybrid automatic repeat request (HARQ) together with error correction coding can complement the modulation order selection proposed above and improve throughput. Nevertheless, the definition of $\text{SVBSC}(q_o,\epsilon, K, \bar{L})$ remains the same.  For simplicity, here we focused on the uncoded modulation case.
\end{remark}


%---------------------------
\begin{figure*}[ht]
\begin{center}
\psfrag{ss}[c][c][1]{$\mathbf{s}$}
\psfrag{ll0}[c][c][1]{$\hat{\mathbf{s}}$}
\psfrag{ll1}[c][c][1]{\scalebox{\textsizescaleM}{$f_{\theta^*_1}$}}
\psfrag{ll2}[c][c][1]{\scalebox{\textsizescaleM}{$f_{\theta^*_2}$}}
\psfrag{ll3}[c][c][1]{\scalebox{\textsizescaleM}{$f_{\theta_i}$}}
\psfrag{ll4}[c][c][1]{\scalebox{.35}{concatenate}}
\psfrag{ll5}[c][c][1]{\scalebox{.35}{concatenate}}
\psfrag{g1}[c][c][1]{\scalebox{\textsizescaleM}{$g_{\phi^*_1}$}}
\psfrag{g2}[c][c][1]{\scalebox{\textsizescaleM}{$g_{\phi^*_2}$}}
\psfrag{g3}[c][c][1]{\scalebox{\textsizescaleM}{$g_{\phi_i}$}}
\psfrag{svbsc1}[c][c][1]{\scalebox{\textsizescaleM}{$\text{SVBSC}(q_o,\epsilon, C_1, 0)$}}
\psfrag{svbsc2}[c][c][1]{\scalebox{\textsizescaleM}{$\text{SVBSC}(q_o,\epsilon, C_2, 0)$}}
\psfrag{svbsc3}[c][c][1]{\scalebox{\textsizescaleM}{$\text{SVBSC}(q_o,\epsilon, C_i, 0)$}}
\psfrag{losssssh3}[c][c][1]{\scalebox{\textsizescaleM}{$\mathcal{L}({\mathbf{s}},\hat{\mathbf{s}})$}}
    \psfrag{delta2}[c][c][1]{\scalebox{\textsizescaleM}{$\Delta{\theta_i}$}}
\psfrag{delta1}[c][c][1]{\scalebox{\textsizescaleM}{$\Delta{\phi_i}$}}
\includegraphics[width=.7\textwidth,keepaspectratio]{images/trainfig.eps}
\caption{Training iteration $i$ for RLACS code, where the trained encoder and decoder modules from previous iterations are frozen (blue color). Dashed arrow lines point to optimization of $f_i$ and $g_i$ based on the loss calculation.}
\label{fig:trainfig}
\end{center}
\end{figure*}
%----------------

\subsection{Application's Operation: Rateless JSCC}
\label{sec:ratelessjscc}


The  code is designed to minimize the distortion over a binary channel where \gls{ber} is bounded ``around'' $q_o$, and for all the rates in the range $[R_{lo}, R_{hi}]$, where $R_{lo}$ and $R_{hi}$ correspond to $L = \bar{L}$ and $L = 0$ puncturing lengths, respectively. 


\begin{definition} \label{def:def1}
A \((K,N, \bar{L}, \mathbf{d})\) binary rateless and lossy joint source-channel code is a tuple of mappings, consisting of an encoder \( f \), a rate control bit-puncturing function $r$, and a decoder \( g \), 
\begin{align}%\nonumber
      f: \mathbb{R}^N\rightarrow \{0,1\}^K, \quad r: \{0,1\}^{K} \rightarrow \{0,1\}^{K-L}  , \quad   g: \{0,1, \varnothing \}^K \rightarrow \mathbb{R}^N 
\label{eq:prb-def}
\end{align}
that  satisfies
\begin{align} \label{Eq:vectorloss}
\forall L \in \{0, \ldots, \bar{L}\} \quad \mathbb{E} \Big( \mathsf{d}({\mathbf{s}}, \hat{\mathbf{s}} ) | L \Big) \leq \mathbf{d}[L],
\end{align}
where the expectation is over the source distribution and channel randomness, 
$\mathsf{d}:  \mathbb{R}^N \times  \mathbb{R}^N \rightarrow [0, \infty)$ is the  distortion function,
$\mathbf{d}$ is a vector of distortion values,
\( \mathbf{u} = f(\mathbf{s}) \), \( \mathbf{x} = r(\mathbf{u}) = \mathbf{u}_{1:K-L} \), \( \mathbf{y} \sim P_{Y|X}(\mathbf{x}) \) is a vector binary channel, 
$\hat{\mathbf{u}} =  \mathbf{y} \Vert \bm{\varnothing}_L$, 
and \( \hat{\mathbf{s}} = g(\hat{\mathbf{u}}) \) (see \figref{fig:system-model}).
\end{definition}

In \defref{def:def1}, $\bm{\varnothing}_L$ denotes a vector of $L$ null values and $\Vert$ denotes concatenation of two vectors. The channel \( \mathbf{y} \sim P_{Y|X}(\mathbf{x}) \) is assumed to be a vector binary channel, as also illustrated in \figref{fig:system-model}. Thus, the channel may in practice include a channel mapping function that maps $\mathbf{x}$ to $M$ complex symbols, passes them through an underlying complex valued channel, and then de-maps the received $M$ complex symbols to binary vector $\mathbf{y}$. Thus, the channel denoted by $P(\mathbf{y}|\mathbf{x})$ comprises the underlying channel and the signal processing functions of the network. 
The SVBSC link from \secref{sec:ratelessradiolink} is an example of such vector binary channel. For ease of notation we omit denoting those constituents of the vector binary channel.




Rateless coding adds another dimension to the performance of source-channel coding which is the \emph{importance} of the bits in reconstruction loss---higher importance is implicitly enforced in bits that are less likely to be punctured, and vice versa, less important information is encoded in bits that are more likely to be punctured. Such unequal importance can be exploited towards semantic and effectiveness encoding. of  As was noted in \secref{sec:priorwork}, other works have already acknowledged this effect in developing \gls{jscc} techniques  under noisy channel conditions \cite{1095155,748707,tung2024multilevelreliabilityinterfacesemantic,bursalioglu2008lossy,bursalioglu2013joint}. Here, we utilize such unequal importance towards rateless-ness of the code, which then enables the network to seamlessly adapt the coding rate based on network state, which differentiates our work.


\begin{definition} \label{def:acheivability}
A rate-distortion mapping vector of $[(\frac{K-l}{N},\mathbf{d}[l])]$ is said to be achievable if there exists a \((K,N, \bar{L}, q_o)\) rateless joint source-channel code as defined in \defref{def:def1}.
\end{definition}
% %%%%%%


% \textbf{Encoding}: The encoder \( f: \{0,1\}^k \rightarrow \{0,1\}^n \) maps a \( k \)-bit message \( x \in \{0,1\}^k \) to an \( n \)-bit codeword \( c = f(x) \in \{0,1\}^n \).

% \textbf{Puncturing}: The puncturing function \( r: \{0,1\}^k \rightarrow \{0,1\}^{k-l} \) maps the original message \( x \) to a punctured message \( x' = r(x) \in \{0,1\}^{k-l} \), where \( l \) is the number of punctured bits satisfying \( 0 \leq l \leq l_m \).

% \textbf{Channel}: The punctured message \( x' \) is transmitted over a binary symmetric channel (BSC) with bit flip probability \( q \), introducing random errors in the received message.

% \textbf{Decoding}: The decoder \( g: \{0,1\}^n \rightarrow \{0,1\}^k \) maps the received noisy codeword \( y \in \{0,1\}^n \) to an estimated message \( \hat{x} = g(y) \in \{0,1\}^k \).

% \textbf{Distortion}: For any puncturing level \( l \) with \( 0 \leq l \leq l_m \), the expected distortion \( D(x, \hat{x}) \) between the original message \( x \) and the estimated message \( \hat{x} \) is minimized, such that:
% \[
% \mathbb{E}[D(x, g(y))] \leq D_{\text{min}},
% \]
% where the expectation is taken over the channel noise and the random choice of \( l \).
% \CM{I should probably revert to Kostina's distortion probability in the definition, and also change the remark I have above in this regard}
% This $(k, n, l_m, q)$ code framework is designed to address scenarios in which a portion of the message bits can be intentionally removed or lost. The encoder \( f \), puncturing function \( r \), and decoder \( g \) are structured to ensure reliable communication and accurate reconstruction of the original message, even under conditions of channel noise and message puncturing. The key performance metric, \( D_{\text{min}} \), quantifies the minimum expected distortion, representing the average discrepancy between the original and reconstructed messages.


%%%%%%



The encoder $f$ operates at the coding rate of $\hat{R} = K/N$, while the mapping $r$ reduces the rate by $(K-L)/K$.
The effective coding rate of $f$ and $r$ mappings is thus $R = (K-L)/M$,   representing the inverse of the compression rate of the lossy and rateless \gls{jscc}. 

% The network has a binary interface to the application on the both ends. At the receiver, a decoder $g$ maps the binary vector from the channel to an estimate of the source information. 


% Note that in general, the message $\hat{\mathbf{u}}$ doesn't need to be hard binary bits. In fact, the receiver end of a communication link in theory can pass the soft values out of the communication link function, or a quantized version of it,  to the \gls{jscc} decoder function $g$. Such 

The adopted model relaxes the need for a probabilistic model of an adaptive quantizer function for the encoder output, commonly used in variable-rate compression techniques. In fact, the goal of the proposed \emph{rateless} encoding is to commission the network with the rate-control function---a common realization of such function is rate adaptation according to \gls{csi} that is commonly used in wireless communication technologies. 

\begin{remark}
    Effectively, the null bits at the receiver can be seen as bit erasures. From the point of view of decoder $g$, one difference between the flipped bits (ratio $q$ out of $K-L$) and the $L$ null bits is that that the position of the former is unknown while the latter is in a known position (e.g., end of the vector). 
    % Thus, the latter can be ignored by $g$ while the former cannot. 
\end{remark}


To the best of author's knowledge, the  rateless JSCC framework and the rate-adaptive and stable link operation presented above, are novel with respect to the literature. In the following section an autoencoder based realization of the rateless JSCC is proposed and its preformance is evaluated over the proposed rate-adaptive and stable link. Investigating the possiblity of using classical compression and channel codes to realize rateless JSCC is for future work.




\section{Rateless and Lossy Autoencoder Channel-Source Codes (RLACS)}
\label{sec:relax}

The design and implementation of a rateless JSCC described in \secref{sec:ratelessjscc} is the subject of this section. We propose an autoencoder based solution with binary latent space for this type of code. The code is designed to operate over the  rate control mechanism  and $\text{SVBSC}(q_o,\epsilon, K, \bar{L})$ channel described in \secref{sec:ratelessradiolink}. We focus on the case of image transmission over a noisy and faded channel, however the proposed solution can be easily extended to semantic communication of other media formats too.


The encoder and decoder mappings  in \eqref{eq:prb-def} are designed as sets of $F$ encoder and decoder mappings $f = \{f_{\theta_1},\ldots, f_{\theta_F} \}$ and $g = \{g_{\phi_1},\ldots, g_{\phi_F} \}$, where $\theta_i$ and $\phi_i$ are  the \gls{dnn} parameter sets that parametrize $f_{\theta_i}$ and $g_{\phi_i}$, respectively. The mappings are designed so that $f_i: \mathbb{R}^N\rightarrow \{0,1\}^{C_i - C_{i-1}}$ and $g_i: \{0,1,\varnothing\}^{C_i} \rightarrow \mathbb{R}^N$, where  $C_0=0$,   $C_1 = K - \bar{L}$ and $C_F = K$, while $F$ and other $C_i$s are design parameters that determine the complexity-performance tradeoff of the training process. Notably, the $i$th decoder takes input that is originated from all the encoders $1,\ldots,i$. Ideally, the $C_i$s are picked to cover all possible puncturing length $L$, i.e., $\mathsf{P}_L(L) \neq 0$.

% \begin{remark}
%     A scenario where the application is aware of the puncturing distribution $P_L$ can be envisioned too. For instance, assume a two-user network where the total bandwidth is shared between a semantic user and another user with random traffic.
% \end{remark}




\subsection{Training}

We first sequentially train an autoencoder ladder structure with $F$ iterations of  training. In the $i$th iteration, the previously trained $i-1$ encoders and decoders parameters  are ``frozen'', that is, they are only used for evaluation and not for training. The $i$th encoder and decoder are trained, while decoder $i$ effectively takes input from encoder $i$ and all the frozen encoders before $i$. Therefore, the training iteration $i$ optimizes the \gls{dnn} parameters as follows. 
\begin{align}\label{eq:opt}
(\theta^*_i, \phi^*_i) = \argmin_{\theta_i, \phi_i} \mathbb{E} \left( \mathcal{L}({\mathbf{s}},\hat{\mathbf{s}}) | \theta^*_{i-1}, \ldots, \theta^*_1 \right),
\end{align}
where $\mathcal{L}({\mathbf{s}},\hat{\mathbf{s}})$ is the  loss measured between the input image $\mathbf{s}$ and its reconstruction $\hat{\mathbf{s}}$, and the expectation is over the training data set and the randomness of the training channel. Note that training iteration $i$ effectively trains the code to minimize the loss  for  puncturing of $L = K - C_i$ in \eqref{Eq:vectorloss}, corresponding to effective coding rate  of $C_i / N$. During the $i$th training iteration, a $\text{SVBSC}(q_o,\epsilon, C_i, 0)$ connects the encoders and decoder $i$. For smooth convergance, during training we assume perfect channel estimate is available to the $\text{SVBSC}$, thus we can set $\epsilon = 0$. The illustration in \figref{fig:trainfig} demonstrates the training phase for the $i$th iteration, where $\Delta{\theta_i}$ and $\Delta{\phi_i}$ represent the flow of loss gradients for parameter optimization.






\subsection{Distortion Minimization}


The loss function utilized for the training process is \gls{mse} reconstruction loss, where
\begin{align}
\mathcal{L}({\mathbf{s}},\hat{\mathbf{s}}) = \frac{1}{N} \sum_{i=1}^{N} \left( s_i - \hat{s}_i \right)^2,
\end{align}
and $N$ is the image size representing the total number of pixels in all  color channels of the image. The inference loss is demonstrated in the following  using the more popular  \gls{psnr}, computed as
\[
\text{PSNR} = 10 \times \log_{10}\left(\frac{\text{1}^2}{\mathcal{L}({\mathbf{s}},\hat{\mathbf{s}})}\right).
\]



%---------------------------
\begin{figure}[t]
\begin{center}
\psfrag{xxx0}[c][c][1]{\scalebox{.7}{input CIFAR10 image}}
\psfrag{xxx1}[c][c][1]{\scalebox{.7}{$3 \times 32 \times 32$ pixels}}
\psfrag{ll0}[c][c][1]{\scalebox{\textsizescale}{residual block}}
\psfrag{lll0}[c][c][1]{\scalebox{\textsizescale}{256 output channels}}
\psfrag{ll1}[c][c][1]{\scalebox{\textsizescale}{residual block, $x$-sample}}
\psfrag{lll1}[c][c][1]{\scalebox{\textsizescale}{$y$ output channels}}
\psfrag{k2}[c][c][1]{\scalebox{\textsizescale}{residual block}}
\psfrag{h2}[c][c][1]{\scalebox{\textsizescale}{$y$ output channels}}
\psfrag{ll3}[c][c][1]{\scalebox{\textsizescale}{attention module}}
\psfrag{ll4}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{lll4}[c][c][1]{\scalebox{\textsizescale}{$x=$ down, $y=256$}}
\psfrag{ll5}[c][c][1]{\scalebox{\textsizescale}{residual block}}
\psfrag{lll5}[c][c][1]{\scalebox{\textsizescale}{$\frac{C_i - C_{i-1}}{2}$ output channels}}
\psfrag{ll6}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{lll6}[c][c][1]{\scalebox{\textsizescale}{$x=$ down, $y=256$}}
\psfrag{k1}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{k4}[c][c][1]{\scalebox{\textsizescale}{$x=$ down, $y=\frac{C_i - C_{i-1}}{2}$}}
\psfrag{ll8}[c][c][1]{\scalebox{\textsizescale}{flatten layer}}
\psfrag{ll9}[c][c][1]{\scalebox{\textsizescale}{fully connected}}
\psfrag{lll9}[c][c][1]{\scalebox{\textsizescale}{vector output size $C_i - C_{i-1}$}}
\psfrag{llll}[c][c][1]{\scalebox{\textsizescale}{Sigmoid}}
\psfrag{xxx2}[c][c][1]{\scalebox{.7}{$C_i - C_{i-1}$ coded bits}}
\psfrag{k3}[c][c][1]{\scalebox{.7}{Big Block $x$, $y$}}
\psfrag{yyy0}[c][c][1]{\scalebox{.7}{(channeled) coded bits}}
\psfrag{yyy1}[c][c][1]{\scalebox{.7}{vector size $C_i$}}
\psfrag{g0}[c][c][1]{\scalebox{\textsizescale}{fully connected, 3D output}}
\psfrag{uu0}[c][c][1]{\scalebox{\textsizescale}{ size of $( C_i \times z) \times 8 \times 8$}}
\psfrag{g1}[c][c][1]{\scalebox{\textsizescale}{spatial reshape}}
\psfrag{uu1}[c][c][1]{\scalebox{\textsizescale}{leaky ReLU}}
\psfrag{g3}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{uu3}[c][c][1]{\scalebox{\textsizescale}{ $y= C_i \times z$}}
\psfrag{g4}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{uu4}[c][c][1]{\scalebox{\textsizescale}{$y= 2 \times C_i \times z$}}
\psfrag{h0}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{ss}[c][c][1]{\scalebox{\textsizescale}{$x = $ up, $y= 2 \times C_i \times z$}}
\psfrag{g6}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{uu6}[c][c][1]{\scalebox{\textsizescale}{$x = $ up, $y=  C_i \times z$}}
\psfrag{g7}[c][c][1]{\scalebox{\textsizescale}{attention module}}
\psfrag{g8}[c][c][1]{\scalebox{\textsizescale}{Big Block}}
\psfrag{uu8}[c][c][1]{\scalebox{\textsizescale}{$y= \frac{1}{2} \times C_i \times z$}}
\psfrag{n4}[c][c][1]{\scalebox{\textsizescale}{residual block}}
\psfrag{m4}[c][c][1]{\scalebox{\textsizescale}{$3$ output channels}}
\psfrag{nn}[c][c][1]{\scalebox{\textsizescale}{Sigmoid}}
\psfrag{yyy2}[c][c][1]{\scalebox{.7}{reconstructed image}}
\psfrag{yyy3}[c][c][1]{\scalebox{.7}{$3 \times 32 \times 32$ pixels}}
\includegraphics[width=\archfigscale\columnwidth,keepaspectratio]{images/encdec.eps} 
\caption{Architecture of the \gls{dnn} used for $f_{\theta_i}$  and  $g_{\phi_i}$ are shown on the right and the left sides, respectively. The dimensions of the encoder and decoder layers adapts to $C_i - C_{i-1}$ and $C_i$, to ensure a sufficient capacity in the network architecture. Additionally, in the decoder structure,  layer dimensions first expand through the middle of the network and then contract, using a factor $z = \min (1, 256/C_i)$. To simplify the illustration, a ``Big Block'' is introduced in the bottom right corner, which takes a variable $x$ to choose between up-sampling and down-sampling (stride 2), if either is used. In the decoder, skip connections retain intermediate outputs from earlier layers and integrate them into later layers. The retained outputs are resized to match the current layer's resolution and combined with its features. An attention module then selects the most important information from the combined features.
%During training, the encoder output is mapped to the decoder input through VIMCO sampling and the training channel. During evaluation, the encoder output is quantized to binary by simple thresholding of the Sigmoid layer output around $0.5$.
}
\label{fig:encdec}
\end{center}
\end{figure}
%----------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]%
    \centering
    %
    % First minipage for Algorithm 1
    \begin{minipage}[t]{0.45\linewidth}
        \begin{algorithm}[H]
        \caption{RLACS encoding}
        \label{alg:enc}
        \begin{algorithmic}[1]
        \REQUIRE Sample image $\mathbf{s}$, $\Theta^* = \{\theta^*_1, \ldots, \theta^*_F\}$
        \STATE $\mathbf{u} = [ \ ]$
        \FOR{$i = 1$ \TO $F$}
            \STATE $\mathbf{u}=\text{queue\_push}(\mathbf{u},f_{\theta^*_i}(\mathbf{s}))$
        \ENDFOR
        \RETURN $\mathbf{u}$
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \hfill
    %
    % Second minipage for Algorithm 2
    \begin{minipage}[t]{0.51\linewidth}
        \begin{algorithm}[H]
        \caption{RLACS decoding}
        \label{alg:dec}
        \begin{algorithmic}[1]
        \REQUIRE $K$, $L$, $\hat{\mathbf{u}}$, $\Phi^* = \{\phi^*_1, \ldots, \phi^*_F\}$,  $\{C_1, \ldots, C_F\}$
        \STATE $\mathbf{r}=\hat{\mathbf{u}}_{1:K-L}$  
        \STATE $i \gets \argmin_{j} \{ C_j \mid C_j \geq K-L \} $
        \IF{$C_i > K-L $}
            \STATE $\mathbf{r}=\text{queue\_push}(\mathbf{r},\mathbf{0.5}_{C_i - K + L})$
        \ENDIF
        \RETURN $\hat{\mathbf{s}} = g_{\phi^*_i}({\mathbf{r}})$
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    %
    % \caption{RLACS encoding and decoding algorithms.}
    % \label{fig:side_by_side_algs}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Evaluation and Inference}
During evaluation, the encoder output is quantized to binary by simple thresholding of the Sigmoid layer output around $0.5$.
The trained \gls{relax} code is tested over $\text{SVBSC}(q_o,\epsilon, K, \bar{L})$. The application source  encodes using $f = \{f_1,\ldots, f_F \}$  at maximum rate of $K/N$ (see \Algref{alg:enc}), while the application destination, being aware of the puncturing length, picks the corresponding decoder from the set $\{g_1,\ldots, g_F \}$ and performs decoding over the received bits from the network (see \Algref{alg:dec}).

\begin{remark}\label{rem:vectorepsilon}
    In practice, the proposed solution can be designed for a set of $q_i$ and $\epsilon_i$ values related to $i \in \{1,\ldots,F\}$. Such setting will introduce more degrees of flexibility for optimizing the joint venture between the network and the application. In the following we foucs on $q_i = q_o$ and $\epsilon_i = \epsilon$ for all $i$.
\end{remark}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{RLACS encoding}
% \label{alg:enc}
% \begin{algorithmic}[1]
% \REQUIRE Sample image $\mathbf{s}$, $\Theta = \{\theta_1, \ldots, \theta_F\}$
% % \ENSURE Encoded image, binary vector of length $K$ 
% \STATE $\mathbf{u} = []$
% \FOR{$i = 1$ \TO $F$}
%     \STATE $\mathbf{u}=\text{queue\_push}(\mathbf{u},f_{\theta_i}(\mathsf{s}))$
% \ENDFOR
% \RETURN $\mathbf{u}$
% \end{algorithmic}
% \end{algorithm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{algorithm}
% \caption{RLACS decoding}
% \label{alg:example}
% \begin{algorithmic}[1]
% \REQUIRE $\hat{\mathbf{u}}$, $\Phi = \{\phi_1, \ldots, \phi_F\}$,  $L$, $\{C_1, \ldots, C_F\}$
% % \ENSURE Output results
% \STATE $\mathbf{r}=\hat{\mathbf{u}}_{1:K-L}$
% \STATE \( i \gets \argmin_{j} \{ C_j \mid C_j \geq K-L \} \)
% \IF{\( C_i > K-L \)}
%     \STATE $\mathbf{r}=\text{queue\_push}(\mathbf{r},\mathbf{0}_{C_i - K + L})$
% \ENDIF
% \STATE $\hat{\mathbf{s}} = g_{\phi_i}({\mathbf{r}})$
% \RETURN $\hat{\mathbf{s}}$
% \end{algorithmic}
% \end{algorithm}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\subsection{An Example RLACS Code}
\label{sec:examplerlacs}

In the following we present one example of a \gls{relax} code. We use \glspl{dnn}, more specifically  residual attention network architecture \cite{wang2017residual} with binary latent space to embody the encoders and decoders. To convert the real-valued  output of the  encoder \glspl{dnn} into a binary latent space during trainig,  we use the  variational inference for Monte Carlo objectives (VIMCO) estimator  proposed in \cite{mnih16_variat_monte_carlo}   to estimate the gradient of the loss function with respect to the \glspl{dnn} weights. Notably, 50 Monte Carlo samples are used for each training step. The architectures  used for $f_{\theta_i}$  and  $g_{\phi_i}$ are illustrated in \figref{fig:encdec}.%, and are available as PyTorch class modules in the repository in \cite{Saeed_Reza_Khosravirad_RLACS}. 

The models  are trained over  $\text{SVBSC}(q_o,\epsilon, C_i, 0)$ training channel with $q_o = 0.05$ and $\epsilon = 0$. The choice of $q_o$ can arguably affect the performance. For example, a lower $q_o$ could improve  performance of the autoencoder module and increase convergence rate, while in turn it reduces the spectral efficiency of the channel. Therefore, there exists  a clear empirical tradeoff for $q_o$ optimization---in this work, we do not exercise that optimization and refer it to future research.

All models are trained with the similar settings: a base learning rate of $10^{-3}$ to $10^{-5}$, an  Adam optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, decay $= 0$, a batch size of 320  and training epochs from 500 to 2000 (depends on model size). A learning rate scheduler reduces learning rate by factor of $0.5$ with 10 epochs patience, and stops the training after 50 epochs patience, where the patience for both is based on invariability of validation loss that is based on  a  subset of the data disjoint from the training and testing set. The GradScaler is used to facilitate mixed-precision training \cite{paszke2019pytorch} by dynamically scaling gradients with an initial scale of  $2^{16}$, adjusting it using a growth factor of $2$ and a backoff factor of $0.5$ every $2000$ steps.

We utilize the CIFAR10 image dataset \cite{krizhevsky09_learn_multip_layer_featur_tiny_images} where, out of the 60K images, 40K and 10K disjoint subsets are  selected for training and evaluation, respectively. Each image has three color channels amounting to total of $3 \times 32 \times 32$ real valued pixels, resulting in $N = 3072$. The network is assumed to utilize $W = 128$ channel uses, resulting in an effective pixel real-value to channel real-value compression ratio of $1/24$. 

\subsection{Evaluation Results}


Two testing channels are explored to test the example \gls{relax} code proposed in \secref{sec:examplerlacs}. In both cases, we assume a block fading channel model that follows Rician distribution  with line-of-sight ratio of $20$ dB.


\paragraph*{Perfect CSI}  The test channel is block fading with perfect \gls{csi} at the transmitter and the receiver ends of the network. In effect, this resembles an \gls{awgn} channel with varying but known SNR.

\paragraph*{Imperfect CSI} To study the effect of imperfect CSI (and $\epsilon$) on the performance, we assume a  block fading channel with imperfect  \gls{csi}, where the Gaussian channel estimation noise variance $\sigma_e$ is proportional to \gls{snr}, according to \cite{1193803,9206086}, as  $\sigma_e = \frac{1}{1+n_p \cdot \mathsf{snr}}$, with $n_p$ denoting the number of pilot symbols used per coherence block for channel estimation and $\mathsf{snr}$ is the wireless channel \gls{snr}. We set $n_p = 10$.

For these two test channels, \figref{fig:bervssnr} shows the average  \gls{ber} against channel \gls{snr}. The case of perfect CSI follows the \gls{ber} of the uncoded QAM modulation. For the imperfect CSI case, the average \gls{ber} naturally depends on the configured $\epsilon$. Notably, a tighter constraint on $\epsilon$ results on  a lower average \gls{ber}. However, as shown in \figref{fig:etavssnr}, this in turn results in lowering the espectral efficiency of the link (i.e., increased chance of using a lower modulation order). This demonstrates the tradeoff between stability and efficiency in a $\text{SVBSC}$ link. We test our RLACS code over this tradeoff to see the effect of instability parameter $\epsilon$ on image reconstruction quality too.





Three main JSCC codes are tested over the test channels, where all three are based on the  \gls{dnn} architectures in \figref{fig:encdec}. 
\begin{itemize}
    \item \textbf{Code 1},  $F = 1$ and $C_1 = 1280$: corresponds to a benchmark where the code is trained for maximum spectral efficiency of 10 bpcu, but the code is not rateless, i.e., it is not specifically optimized with puncturing effect from the network in mind. 
    \item \textbf{Code 2}, $F = 2$, and $C_i-C_{i-1} = 640$ for all $i$: is the proposed \gls{relax} code, thus, is rateless, and is optimized  for code lengths of $i \times 640$ for $i \in \{1, 2\}$. 
    \item \textbf{Code 3}, $F = 10$, and $C_i-C_{i-1} = 128$ for all $i$: is also the proposed \gls{relax} code, but with $C_i$s adjusted such that the puncturing lengths (resulted from choice of modulation order) aligns perfectly as $C_i \leftrightarrow \bar{W} \cdot \log_2(M_i)$, thus providing finer granularity of rateless-ness compared to the above two codes. 
\end{itemize}
Notably, the codes are sized to the maximum spectral efficiency of the test channel with $M_c = 1024$ QAM, which is $\bar{W} \cdot \log_2(M_c) = 1280$ bits---that is, $C_F = 1280$ for all three codes. It's worth noting that a RLACS code  can be easily extended to cover higher modulation orders (and higher spectral efficiency) by adding more encoder and decoder pairs to the ensemble, without the need to retrain the previous encoder and decoder pairs.

%---------------------------
\begin{figure}[t!]
\begin{center}
\psfrag{ber}[c][c][1]{\scalebox{.8}{average bit error rate}}
\psfrag{snr}[c][c][1]{\scalebox{.8}{channel signal to noise ratio [dB]}}
\psfrag{Xxxxxxxx1Xxxxxxxx1}[l][l][1]{\scalebox{\textsizescale}{$\mathsf{Q}(\mathsf{snr},M)$, uncoded QAM}}
\psfrag{data1}[l][l][1]{\scalebox{\textsizescale}{$q_o = 0.05$}}
\psfrag{Xxxxxxxx3Xxxxxxxx1}[l][l][1]{\scalebox{\textsizescale}{perfect \gls{csi}}}
\psfrag{X0}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.01$}}
\psfrag{X1}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.05$}}
\psfrag{X2}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.1$}}
\includegraphics[width=\imagescalesize\columnwidth,keepaspectratio]{images/bervssnr.eps}
\caption{Average bit error rate experienced over the  testing channels $\text{SVBSC}(q_o=0.05,\epsilon, 1280, 1152)$ (top legend) against the underlying wireless channel \gls{snr} with $\bar{W}=128$. The uncoded QAM curves cover $M = {2, 4, 8, 16, 32, 64, 128, 256, 512, 1024}$ modulation orders (ordered from left to right). In the case of perfect \gls{csi}, \gls{ber} can be perfectly kept below $q_o$. In case of imperfect \gls{csi}, $\epsilon$ determines the average \gls{ber}. A tight $\epsilon$ of $0.01$ for instance, results in average \gls{ber} almost one order of magnitude below $q_o$. Average \gls{ber} is not in itself  an important measure---more critical metric is the average reconstruction loss over the randomness of \gls{ber}, when \gls{csi} is imperfect.}
\label{fig:bervssnr}
\end{center}
\end{figure}
%----------------

Code 1 is optimized for the maximum rate (10 bpcu) with no constraints related to lower rates and thus its performance should be seen as a the upper bound of the performance for the other two codes at 10 bpcu rate, i.e., in high \gls{snr}. Nevertheless, the experimental results in \figref{fig:psnrcodes}  show that the gap between the three curves in the high rate/SNR region is in fact negligible, thus empirically demonstrating  optimality of the example \gls{relax} code with large $F$. 





\subsection{Discussion of the Results and Take-Away Points}
\label{sec:discussionofresults}

Let us now delve into the details of the presented results and discuss the important points.




\paragraph*{Ralteless coding significantly improves performance, especially in low SNR} As \gls{snr} decreases, and the modulation order that satisfies \eqref{eq:modorder} decreases, more of the bits has to be punctured off of the  packet. Code 1 is optimized for 10 bpcu rate and is not rateless.  Thus, while this code  perform optimally well at high \gls{snr} it is sub-optimal for low \gls{snr}. The gap between the performance of code 1 and code 3 illustrates the effect of rateless JSCC coding in maintaining efficient performance against all SNR values, without the need to exchange CSI knowledge with the application. Notably,  code 3 outperforms code 1 in \figref{fig:psnrcodes} by more than $5$ dB in reconstruction quality, at low channel SNR, emphasizing the importance of rateless JSCC design. The rateless characteristics of a code in effect enforces higher importance in bits that are less likely to be punctured, and vice versa. 

%---------------------------
\begin{figure}[t!]
\begin{center}
\psfrag{eta}[c][c][1]{\scalebox{.8}{average spectral efficiency [bpcu]}}
\psfrag{channel SNR [dB]}[c][c][1]{\scalebox{.8}{channel signal to noise ratio [dB]}}
\psfrag{Xxxxxxxx1Xxxxxxxx1}[l][l][1]{\scalebox{\textsizescale}{perfect CSI}}
\psfrag{X0}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.01$}}
\psfrag{X1}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.05$}}
\psfrag{X2}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.1$} }
\includegraphics[width=\imagescalesize\columnwidth,keepaspectratio]{images/etavssnr.eps}
\caption{Average spectral efficincy of the curves in \figref{fig:bervssnr}. With $\epsilon = 0.05$ the average spectral efficiency, and the average \gls{ber} closely follow the case of perfect \gls{csi}.}
\label{fig:etavssnr}
\end{center}
\end{figure}
%----------------

\paragraph*{Rateless coding performs  in  harmony with the rate-adaptive and stable link for semantic communication} The impact of stability parameter $\epsilon$ on the rateless coding performance is illustrate in \figref{fig:psnr}, for code 3. The closest performance to the case of perfect CSI, with fraction of a dB gap, is achieved  with $\epsilon = 0.05$. Incidentally, $\epsilon = 0.05$ also provides the closest spectral efficiency to the case of  perfect CSI in \figref{fig:etavssnr}. Note that we have only explored a few choices of $\epsilon$ values here (mainly selected based on the average BER performance those offer in \figref{fig:bervssnr}), and a more careful optimization of $\epsilon$ may further improve the performance of our proposed solutions.







\paragraph*{Granularity of RLACS code design is critical in low SNR} Granularity of RLACS code is controlled by $C_i$ parameters. Ideally, those design parameters must match the possible puncturing lengths, so that at each puncturing length distortion minimization is guaranteed---code 3 is designed so. When the granularity is lowered, e.g., in case of code 2, the performance is affected, especially in low SNR. Notably, code 2 and code 3 have comparable performance around the point of 5 bpcu, i.e., around 14 dB channel SNR. That is the point where code 2 experiences zero puncturing thanks to its first decoder. As channel SNR grows beyond 14 dB, code 2 again falls below code 3 in performance until the two curves meet again at 10 bpcu region, i.e., right side of \figref{fig:psnrcodes}. This further stresses the importance of granularity in RLACS code design. In practical system design, the possible range of puncturing length can be pre-agreed on between the network and the application, e.g., through standard specified look-up tables. The application can then make sure to match those values while optimizing the code.



%---------------------------
\begin{figure}[t!]
\begin{center}
\psfrag{ber}[c][c][1]{\scalebox{.8}{image reconstruction PSNR [dB]}}
\psfrag{snr}[c][c][1]{\scalebox{.8}{channel signal to noise ratio [dB]}}
\psfrag{Xxxxxxxx2Xxxxxxxx1Xxxxxxxx0}[l][l][1]{\scalebox{\textsizescale}{Code 1, not rateless}}
\psfrag{X1}[l][l][1]{\scalebox{\textsizescale}{{Code 2}, rateless with low granularity}}
\psfrag{X2}[l][l][1]{\scalebox{\textsizescale}{{Code 3}, rateless with ideal granularity}}
\psfrag{X3}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.1$} }
\includegraphics[width=\imagescalesize\columnwidth,keepaspectratio]{images/psnrvssnrcodes.eps}
\caption{Image reconstruction  in PSNR for the three  codes, tested with perfect CSI testing channel.}
\label{fig:psnrcodes}
\end{center}
\end{figure}
%----------------

\paragraph*{RLACS performs strongly against channel SNR variation}   \gls{relax} code can be  compared with the solutions in \cite{bourtsoulatze19_deep_joint_sourc_chann_codin} and \cite{tung2024multilevelreliabilityinterfacesemantic}. We refer to  \cite[Figure~10]{tung2024multilevelreliabilityinterfacesemantic}, where those two solutions are tested for a similar compression ratio of $1/24$ over \gls{awgn} channel (comparable to our testing channel with perfect CSI). Most notably, code 3 performs on par with Split-JSCC of \cite{tung2024multilevelreliabilityinterfacesemantic}, while in contrast, RLACS relaxes the need for code optimization over several training SNRs, and, only requires JSCC operation in the application, significantly reducing the optimization complexity. There is however a gap in performance of RLACS compared to DeepJSCC \cite{bourtsoulatze19_deep_joint_sourc_chann_codin} which is rooted in the assumption of a binary interface between the application and the network in our proposal. Performance of RLACS, thanks to the rateless operation, gracefully degrades as \gls{snr} decreases, and avoids the \emph{cliff effect} and \emph{leveling-off effect} associated to separate source and channel coding  reported in \cite{gunduz2024joint}, while maintaining separation of design and optimization of the network and application, intact. In fact, looking at code 1 performance in \figref{fig:psnrcodes}, one can postulate that those effects are mainly the result of packet-ized and error-free approach to communication of bits in conventional separate source-channel coding (see further discussion in \secref{sec:blueprint}).

%---------------------------
\begin{figure}[t!]
\begin{center}
\psfrag{ber}[c][c][1]{\scalebox{.8}{image reconstruction PSNR [dB]}}
\psfrag{snr}[c][c][1]{\scalebox{.8}{channel signal to noise ratio [dB]}}
\psfrag{Xxxxxxxx1Xxxxxxxx0}[l][l][1]{\scalebox{\textsizescale}{perfect CSI}}
\psfrag{X1}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.01$}}
\psfrag{X2}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.05$}}
\psfrag{X3}[l][l][1]{\scalebox{\textsizescale}{imperfect CSI, $\epsilon = 0.1$} }
\includegraphics[width=\imagescalesize\columnwidth,keepaspectratio]{images/psnrvssnr.eps}
\caption{Image reconstruction of the testing process in PSNR for code 3. The case of perfect CSI (also present in \figref{fig:psnrcodes}) provides a reference point to demonstrate the performance loss due to channel estimation error. Notably, the gap caused by imperfect CSI is a fraction of a dB when $\epsilon = 0.05$.}
\label{fig:psnr}
\end{center}
\end{figure}
%----------------


\paragraph*{Modulation order granularity is critical for smooth rate-adaptation in SVBSC link} As shown in \figref{fig:bervssnr}, the available QAM modulation orders in \gls{5g} system (i.e., 2--1024) provide a limited range of \gls{snr} for a given target BER $q_o$. At SNR values lower and higher than that range, the experience BER will be higher and lower than the target, respectively. Extending that range is easily possible by adding higher modulation orders to cover beyond $ M = 1024$, and by adding channel codes with $<1$ code rate to $M=2$ modulation to cover below $M=2$. The RLACS code granularity (i.e., $C_i$ values) can then  be accordingly adjusted. Alternatively, as noted earlier in \remref{rem:vectorepsilon}, different  $q$ and $\epsilon$ values can be used for the first and the last encoder and decoder pair, compared to the rest, to match the actual BER in \figref{fig:bervssnr}.


\paragraph*{JSCC can be fully performed by the application, without the knowledge of the network's state} Exchanging network's state with the application can become a hurdle in practice, preventing the code to perform at its optimal rate. With rateless JSCC this issue is efficiently resolved by commissioning the network with the rate-control responsibility according to network's state, while ensuring optimal code design for potential puncturing of the codeword bits by the network. Unlike the works in \cite{bourtsoulatze19_deep_joint_sourc_chann_codin} and \cite{tung2024multilevelreliabilityinterfacesemantic}, where training (optimizing) a code for different SNR values is exercised to match the training and testing channels, our proposed rateless JSCC approach relaxes the need for the application to worry about the \emph{channel}.

%----------------
\begin{figure}[t]
\begin{center}
% Labels for 11 columns, centered above the image
\includegraphics[width=\columnwidth,keepaspectratio]{images/comparison.eps}
\setlength{\unitlength}{\columnwidth} % Normalize width
\begin{picture}(1,0)  % Width = 1 (normalized to column width), height = 0 (text only)
    \put(0.045,0){\makebox(0,0){\scalebox{\LabelFontSize}{Original}}}
    \put(0.135,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 2$}}}
    \put(0.225,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 4$}}}
    \put(0.315,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 8$}}}
    \put(0.405,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 16$}}}
    \put(0.495,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 32$}}}
    \put(0.585,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 64$}}}
    \put(0.675,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 128$}}}
    \put(0.765,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 256$}}}
    \put(0.855,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 512$}}}
    \put(0.945,0){\makebox(0,0){\scalebox{\LabelFontSize}{$M = 1024$}}}
\end{picture}
\caption{Progressive coding effect of the proposed rateless JSCC is evident in the examples illustrated here. Original image is compared against the reconstructions using Code 3, where the modulation order used for the communication channel is noted below each column. As modulateion order increases more redundancy becomes available to the decoder, i.e., puncturing length $L$ becomes smaller, hence, the reconstruction quality progressively improves.}
\label{fig:comparison}
\end{center}
\end{figure}
%----------------



\paragraph*{Progressive performance improvement as rate increases} As shown in \figref{fig:comparison}, the proposed rateless JSCC framework, and the DNN-based realization of it in RLACS, can be seen as \emph{progressive} JSCC coding solutions. Conventional progressive coding is used for compression purposes without dealing with errors, or in combination  with channel coding to recover from errors too \cite{sherwood1997progressive}. Rateless JSCC is thus, to our knowledge, the first true progressive JSCC solution, and is able to  deal with channel errors while being  unaware of the channel state.


\paragraph*{A systematic and practical framework for code design and semantic communication}
Importantly, \gls{relax} code is built on a systematic approach with practical considerations taken into account. \gls{relax} code doesn't require to be designed separately for each \gls{snr} operation point and can be optimized agnostic to the undelying channel. This is a major advantage of general separate source-channel coding solutions which was missing in joint source-channel codings. With rateless JSCC and rate-adaptive stable link operation, the performance benefit of JSCC coding is offered while maintaining the modular design and optimization benefits of conventional communication networks, allowing the two to evolve and be optimized independently.



\paragraph*{Additional points to consider towards future work}
\begin{itemize}
    \item While autoencoders show good potential for error correction coding, optimization  and training convergance for them proves complex, especially, when the error rates are too high. That suggests targetting a low $q_o$ in designing \gls{relax} codes. A lower BER requires a  higher \gls{snr}, thus,  there is a clear tradeoff to be explored in designing \gls{dnn}-based rateless JSCCs. For example, it may be better to do additional error correction using well-optimized channel codes such as \gls{ldpc} or Polar codes, either at the network or at the application.  The role of such channel code is not to correct all the errors, but to reduce and stabilize the \gls{ber}. This way, the power of classic channel codes in error correction will be combined with the versatility of \glspl{dnn} in semantic \gls{jscc}. As the reader may be picturing it at this point, through these ideas we are exploring the \emph{spectrum} of design options between \emph{fully-separate} and \emph{fully-joint} source-channel coding. 
    \item In practical systems, the network and the application have limited exchange of information. While the interface between the two can improve, and the information exchanged between them can become richer (e.g., see discussion in \secref{sec:blueprint}), it will remain impractical to share real-time information about the channel state with the application. Thus, the joint venture between the network and application must consider this pragmatic separation of responsibilities: network stabilizes the end-to-end bit pipe in one dimension (e.g., \gls{ber}) while remain stochastic in the other dimension (size of the pipe, i.e., a random puncturing rate), and the application tries to maximize its utilization of the stabilized end-to-end channel, knowing that behavior. 
    % \item In absense of \gls{csi} at the application, the existing solutions to adapt the application \gls{jscc} to the channel variation has been so far limited to training autoencoders for a set of specific \gls{snr} ranges, as for example studied  in \cite{bourtsoulatze19_deep_joint_sourc_chann_codin} and \cite{tung2024multilevelreliabilityinterfacesemantic}. Such approaches will require the application to interface more than one version of the encoded message with the network, and thus,  not  practical. In contrast, the present work proposes a rateless approach to the code design where the application anticipates a wide range of channel states when optimizing the code, and the network adapts the encoded message size through puncturing to the actual real-time state of the channel.
\end{itemize}

