\section{Theoretical Analysis and Insights}
\label{sec:theory}

In this section, we first formalize the optimization dynamics of multi-domain data aggregation for enhancing LLMs' comprehensive capabilities. Building on the empirical observations from Sec.~\ref{sec:observation}, we then establish theoretical connections between distributional diversity and emergent model behaviors. Finally, we demonstrate the fundamental limitation of explicit diversity optimization in undetermined-domain scenarios.

\subsection{Ideal Optimization Formulation}

During the training process of LLMs, various sources of labeled data are often collected to enhance models' capabilities across multiple dimensions. Let \(\mathcal{K} = \{1, \ldots, K\}\) represent the set of labeled domains, where \(K\) is determined by real-world task requirements (e.g., \(K=4\) in our experiments). For each domain \(k \in \mathcal{K}\), data samples \(\mathbf{x}_i^{(k)} \in \mathcal{D}_k\) are generated according to a distribution \(\mathcal{D}_k\). The data distributions are assumed mutually distinct across different domains, leading to an ideal \textbf{I.I.D. per-domain hypothesis}: each domain \(k\) corresponds to an independent model \(h_k \in \mathcal{H}\). The standard optimization goal thus can be:
\begin{equation}
\small
\label{eq:original-loss-formulation}
    \forall k \in \mathcal{K}, \quad \min_{h_k \in \mathcal{H}} \mathcal{L}_{\mathcal{D}_k}(h_k),
\end{equation}
where \(\mathcal{L}_{\mathcal{D}_k}(h_k) = \mathbb{E}_{(x,y) \sim \mathcal{D}_k} \left[ l(h_k(x), y) \right]\) is the empirical risk given a specific loss function \(l\).

\paragraph{Counterintuitive Findings}
If the distributions \(\{\mathcal{D}_k\}_{k=1}^K\) are strictly I.I.D., the risk \(\mathcal{L}_{\mathcal{D}_k}(h_k)\) should depend only on \(\mathcal{D}_k\). However, numerous previous studies~\cite{zhao2024beyond, tirumala2023d4} have demonstrated the presence of synergistic and antagonistic effects between different datasets. This is further demonstrated in Sec.~\ref{sec:observation} (Fig.~\ref{tab:main-diversity}) that certain domain mixtures lead to catastrophic performance degradation (e.g. \textit{Intra-D (0-20)} in MATH with Qwen2). This contradicts the above I.I.D. hypothesis, suggesting that the simplified and conventional formulation in Eq~\eqref{eq:original-loss-formulation} fails to capture cross-domain interference.

\subsection{Introducing Mixture of Underlying Distributions}

Inspired by the counterintuitive findings, we posit that each domain distribution \(\mathcal{D}_k\) arises from the mixture of \(M\) latent distributions \(\{\tilde{\mathcal{D}}_m\}_{m=1}^M\) representing foundational LLM capabilities (\(M \ll K\) in practice). This leads to our core statistical assumption:

\begin{assumption}[Latent Capability Structure]
\label{assump:latent}
For each observed domain \(k \in \mathcal{K}\), its data distribution decomposes into \(M\) latent capability distributions:
\begin{equation}
\small
    \mathcal{D}_k = \sum_{m=1}^M \pi_{km} \tilde{\mathcal{D}}_m, \quad \sum_{m=1}^M \pi_{km} = 1,
\end{equation}
where \(\tilde{\mathcal{D}}_m\) indicates the \(m\)-th foundational capability. The weights \(\pi_{km}\) reflect domain-specific capaility composition.
\end{assumption}

To analyze how data optimization interacts with latent capability distributions, we further posit that each foundational capability admits an optimal configuration:

\begin{assumption}[Capability Optimality]
\label{assump:opt}
Each foundational capability admits a unique optimal predictor:
\begin{equation}
\small
    \exists h_{\theta_m^*} \in \mathcal{H} \text{ s.t. } \theta_m^* = \mathop{\mathrm{argmin}}_{\theta} \mathbb{E}_{(x,y) \sim \tilde{\mathcal{D}}_m}[l(h_\theta(x), y)].
\end{equation}
The loss \(l\) is strongly convex, holding for cross-entropy or MSE, where \(h_{\theta_m}\), termed the component model, represents a model's manifestation of specific foundational capabilities.
\end{assumption}

This leads to our key result formalizing how real-world training data activates latent capabilities via component models:

\begin{proposition}
\label{prop:latent-opt}
Under Assumption~\ref{assump:latent}-\ref{assump:opt}, the minimizer of the \textit{cross-domain average risk} 
\begin{equation}
\small
\label{eq:cross-domain-risk}
    \min_{\{\theta_m\}} \mathbb{E}_{k \sim \mathcal{K}} \left[ \mathcal{L}_{\mathcal{D}_k}\left( \textstyle\sum_{m=1}^M \pi_{km} h_{\theta_m} \right) \right]
\end{equation}
admits a solution \(\{ \theta_m^* \}_{m=1}^M\) where each \(h_{\theta_m^*}\) optimizes the corresponding foundational capability \(\tilde{\mathcal{D}}_m\). 
\end{proposition}

\textbf{Remark} Proposition~\ref{prop:latent-opt} reveals an underlying fact: optimal or suboptimal solutions can be achieved through data selection to enhance the overall model performance. This is further validated by the role of the coefficients \(\pi_{km}\), which act as \textit{capability selectors} to configure domain-specific behavior, demonstrating the impact of data composition.



\subsection{Diversity Influence on the Optimization}
Building upon Proposition~\ref{prop:latent-opt}, we analyze the intrinsic relationship between diversity and capability composition. The mixture coefficients \(\pi_{km}\) inherently govern both inter- and intra-domain diversity formulated in Sec.~\ref{sec:obser:contrastive-dist}, differing in their geometric properties in the latent capability space.

\textbf{Centroids with Coefficients} Let \(\mathcal{C}_m \in \mathbb{R}^d\) denote the centroid vector of latent capability \(\tilde{\mathcal{D}}_m\) in the embedding space. The domain centroid \(\mathcal{C}_k\) can be expressed as:
\begin{equation}
\small
\label{eq:centroid-coe}
    \mathcal{C}_k = \sum_{m=1}^M \pi_{km} \mathcal{C}_m.
\end{equation}
This linear combination implies that the inter- and intra-diversity are determined by \(\pi_{km}\) configurations:
\begin{proposition}[Diversity Decomposition]
\label{prop:diversity-decomp}
The inter-diversity metric decomposes into:
\begin{equation}
\small
    \Phi_{\text{inter}} = \sum_{m=1}^M \sum_{n=1}^M \lambda_m \lambda_n \|\mathcal{C}_m - \mathcal{C}_n\|_2,
\end{equation}
where \(\lambda_m = \mathbb{E}_k[\pi_{km}]\). And the intra-diversity satisfies:
\begin{equation}
\small
    \Phi_{\text{intra}}^{(k)} \leq \sum_{m=1}^M \pi_{km} \|\mathcal{C}_m - \mathcal{C}_k\|_2^2 + \mathbb{E}_{m\sim\pi_k}[\text{Var}(\tilde{\mathcal{D}}_m)].
\end{equation}
\end{proposition}
The inter-diversity result follows from substituting \(\mathcal{C}_k = \sum \pi_{km}\mathcal{C}_m\) into \(\mathbb{E}_{k\neq l}\|\mathcal{C}_k - \mathcal{C}_l\|\) and applying Jensen's inequality. The intra-diversity bound combines the variance decomposition within each latent capability. Full proof is provided in Appendix~\ref{sec:appendix-proof-prop4.4}.

\subsection{Theoretical Insights}
\label{sec:theory-insights}

Collectively, results in this section provide a unified framework to explain the varied patterns and existence of peak performance observed in Sec.~\ref{sec:exps-observation}, providing theoretical insights and guidance for designing our following algorithm dealing with undetermined data.

\textbf{Diversity's Influence on LLMs' Capability:} Based on Proposition~\ref{prop:diversity-decomp}, we conjecture that the Inter-/Intra-Diversity, which can be explicitly modeled by $\pi$, also governs $\pi$-variation control for component model mixing and thus the final overall performance. Proposition~\ref{prop:latent-opt} further suggests that improper diversity levels may induce model suboptimality by constraining overall capability, consistent with the distribution-dependent empirical results in Sec.~\ref{sec:observation}.

\textbf{Robust Diversity Selection Strategies:} Based on Proposition~\ref{prop:diversity-decomp}, it can be observed that averaging the \(\pi_{km}\) helps to mitigate the missing of underlying optimal predictor \(\theta_m^*\). From this view, the random baseline can gain relatively stable improvements in expectation. A more robust yet simple strategy can be derived: ensemble across candidate models trained on multiple data pools with distribution-varied inter- and intra-diversity, e.g., via techniques like voting or model weights averaging.
This can be corroborated by results in Table~\ref{tab:main-diversity}, such as the case for Llama3.1-8B.

\textbf{Challenges in Undetermined-Domain Scenarios:} 
However, all the selection strategies discussed so far face intractable solutions in undetermined domains with explicit domain labels, as Proposition~\ref{prop:diversity-decomp} technically revealed: (1) ambiguous domain boundaries, e.g., blended instruction types; (2) partial overlaps in latent capabilities; and (3) absence of centroid priors $\{\mathcal{C}_m\}$. 
These intrinsic constraints motivate our \textit{model-aware diversity reward} in the following Sec.~\ref{sec:daar}, which harnesses domain-diversity awareness into reward modeling without requiring explicit parameterization for such structural assumptions.

% Proposition~\ref{prop:diversity-decomp} reveals that diversity-driven optimization requires explicit domain labels, yet faces intractable solutions in undetermined domains: 1) ambiguous domain boundaries (e.g., blended instruction types), 2) partial overlaps in latent capabilities, and 3) absence of centroid priors $\{\mathcal{C}_m\}$. These intrinsic constraints motivate our \textit{model-aware diversity reward} in Sec.~\ref{sec:daar}, which synergizes domain-diversity awareness into reward modeling without explicit structural assumptions.
