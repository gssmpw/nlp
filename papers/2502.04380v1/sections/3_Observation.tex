\section{Motivational Observations}
\label{sec:observation}
In this section, we will explore how data with different domain-specific diversity affects model capabilities through experimental data pools, featuring contrastive data distributions in terms of inter-domain and intra-domain diversity.

\subsection{Seed Data Pools and Basic Setting}
\label{sec:sec3-data-pool}

\paragraph{Data Pools and Sources} 
To explore how selecting data samples from extensive and diverse data repositories affects the foundational capabilities of LLMs, we construct various data pools and consistently fine-tune LLMs on them. We then analyze the performance changes and attribute these changes to the different controlled data pools.


The seed data pool is sourced from following datasets: Dolly-15k~\cite{dolly-15k} for common sense, Cot-en~\cite{cot-en} for reasoning, Math-Instruct~\cite{math-instruct} for mathematics, and Code-Alpaca~\cite{code-alpaca} for coding. Each dataset was randomly tailored to 10,000 entries, resulting in a combined data pool of 40,000 entries. Following instruction tuning practices~\cite{zhou2024lima, liu2023deita}, we then uniformly sample 8,000 data entries as a referenced data pool for random baseline. In subsequent sections, we will introduce how to construct other comparative pools with the same size to random pool.

\paragraph{Benchmarks}
Aligned with representative capabilities of leading open-source LLMs, we select the following widely used evaluation sets: NQ~\cite{nq} and TriviaQA~\cite{triviaqa} for common sense, Hellaswag~\cite{hellaswag} for reasoning, GSM8K~\cite{gsm8k} and MATH~\cite{math} for mathematics, MBPP~\cite{mbpp} and HumanEval~\cite{humaneval} for coding. To evaluate the comprehensive performance of LLMs across domains, we employ the average metric (\textsc{Avg}) as the primary evaluation criterion.

\paragraph{Models \& Implementation}
To ensure the effectiveness and applicability of our empirical findings, we employ the Qwen2 series (Qwen2-7B \& Qwen2.5-7B)~\cite{qwen2series} and the Llama3.1-8B~\cite{llama3series} as representative SOTA base models to be fine-tuned.
%To rigorously assess the effectiveness of our empirical findings against state-of-the-art baselines, we employ the Qwen2 series (Qwen2-7B \& Qwen2.5-7B)~\cite{qwen2series} and the Llama3.1-8B~\cite{llama3series} as representative cutting-edge models. 
All experiments are conducted under identical training and evaluation protocols with two independent repetitions. Full platform, training and evaluation details are provided in Appendix \ref{sec:appendix-div-setting}.


\subsection{Data Pools with Contrastive  Distributions}
\label{sec:obser:contrastive-dist}

To systematically analyze the impact of domain-specific diversity patterns on model capabilities, we propose a contrastive construction with three phases: (A) Foundational Definitions, (B) Diversity Metric Formulation, and (C) Distribution Synthesis.

\paragraph{(A) Foundational Definitions} 
Let the composite dataset \( \mathcal{D} = \bigcup_{k=1}^K \mathcal{D}_k \) comprise \( K=4 \) distinct domains, where each domain subset \( \mathcal{D}_k \) contains \( N_k = |\mathcal{D}_k| \) samples. We represent each data instance through its semantic embedding \( \mathbf{x}_i^{(k)} \in \mathbb{R}^d \) extracted from the \textbf{\textit{Embedding}} layer of the pretrained LLM, capturing high-dimensional semantic features. The domain centroid \( \mathcal{C}_k \) serves as the semantic prototype:
\begin{equation}
\small
\mathcal{C}_k = \frac{1}{N_k} \sum_{i=1}^{N_k} \mathbf{x}_i^{(k)}.
\end{equation}

This centroid-based representation enables geometric interpretation of domain characteristics in the embedding space. We dissect data diversity into two complementary aspects:

\paragraph{(B.1) Inter-Diversity} 
It quantifies the diversity between distinct domains through centroid geometry. For sample \( \mathbf{x}_i^{(k)} \), its cross-domain similarity is measured by:
\begin{equation}
\small
\phi_{\text{inter}}(\mathbf{x}_i^{(k)}) = \sum_{\substack{j=1 \\ j \neq k}}^K \frac{\mathbf{x}_i^{(k)} \cdot \mathcal{C}_j}{\|\mathbf{x}_i^{(k)}\| \|\mathcal{C}_j\|}.
\end{equation}

The global inter-diversity metric \( \Phi_{\text{inter}} \) computes the expected pairwise centroid distance: 
\begin{equation}
\small
\label{eq:inter-diversity}
\Phi_{\text{inter}} = \mathbb{E}_{k \neq l} \left[ \|\mathcal{C}_k - \mathcal{C}_l\|_2 \right] = \frac{1}{\binom{K}{2}} \sum_{k=1}^{K-1} \sum_{l=k+1}^K \|\mathcal{C}_k - \mathcal{C}_l\|_2.
\end{equation}

This formulation reflects a key insight: maximizing \( \Phi_{\text{inter}} \) encourages domain separation, while minimization leads to overlapping representations. Fig.~\ref{fig:tsne} demonstrates this continuum through t-SNE projections â€“ high \( \Phi_{\text{inter}} \) manifests as distinct cluster separation with clear margins (Fig.~\ref{fig:tsne}.(c)), whereas low values produce entangled distributions (Fig.~\ref{fig:tsne}.(d)). Full analysis is detailed in Appendix~\ref{sec:appendix-div-tsne-all}.


\paragraph{(B.2) Intra-Diversity}

Focusing solely on the separation between different domains may hinder the model's ability to learn the knowledge specific to a given domain. Hence we measure variation within each domain. We calculate sample similarity to its domain center:
\begin{equation}
\small
\phi_{\text{intra}}(\mathbf{x}_i^{(k)}) = \frac{\mathbf{x}_i^{(k)} \cdot \mathcal{C}_k}{\|\mathbf{x}_i^{(k)}\| \|\mathcal{C}_k\|}.
\end{equation}
And the domain-level variance metric is defined as:
\begin{equation}
\small
\label{eq:intra-diversity}
\Phi_{\text{intra}}^{(k)} = \frac{1}{N_k} \sum_{i=1}^{N_k} \|\mathbf{x}_i^{(k)} - \mathcal{C}_k\|_2^2.
\end{equation}

Controlled manipulation of \( \Phi_{\text{intra}} \) reveals critical trade-offs: lower variance (tight clusters near \( \mathcal{C}_k \)) enhances domain-specific feature learning but risks over-specialization. Higher variance improves robustness at the cost of potential cross-domain interference. The visualization in Fig.~\ref{fig:tsne} (e-f) illustrates this scenario that concentrated distributions exhibit sharp marginal peaks, while dispersed variants show overlapping density regions.

\paragraph{(C) Distribution Synthesis} For each domain \( \mathcal{D}_k \), we compute sample-wise diversity scores \( \{\phi_{\text{inter}}(\mathbf{x}_i^{(k)})\}_{i=1}^{N_k} \) and \( \{\phi_{\text{intra}}(\mathbf{x}_i^{(k)})\}_{i=1}^{N_k} \). The construction proceeds via partition each \( \mathcal{D}_k \) into 20\% intervals based on the percentiles of \( \phi_{\text{inter}} \) for \textbf{inter-diversity control}, and partition the \( \phi_{\text{intra}} \) scores into 20\% quantile intervals for \textbf{intra-diversity control}.

The 20\% interval results in five choices of data selection per domain, parameterizing the trade-off between diversity preservation and domain specificity.
%The 20\% intervals create five adjustable thresholds per domain, parametrizing the trade-off between diversity preservation and domain specificity. 
As demonstrated in Appendix~\ref{sec:appendix-div-tsne} (Fig.~\ref{fig:inter-diversity-tsne} and Fig.~\ref{fig:intra-diversity-tsne}), this quantization process induces measurable distribution shifts.

\subsection{Experimental Observations}
\label{sec:exps-observation}

Table~\ref{tab:main-diversity} presents comprehensive evaluations across seven benchmarks, where the notation \textit{Inter-Diversity (X-Y)} indicates samples ranked in the top (100-Y)\% to (100-X)\% of cross-domain similarity scores. Due to space constraints, we present only the results for the top 20\%, middle 20\%, and bottom 20\%. More results can be found in Appendix~\ref{sec:appendix-total-diversity}.

\textbf{Diversity-Aware Performance:} Our diversity-controlled selections reveal two critical observations:
\begin{itemize}[leftmargin=*]
   \item \textbf{Varied Improvement Patterns}: 
   Both models demonstrate marked improvements over \textsc{Raw} distributions across all diversity conditions, but the effects of their improvements vary.
   For Llama3.1-8B, \textit{Inter-D (80-100)} achieves 38.98 average accuracy (+3.12 over \textsc{Raw}), outperforming the \textsc{Random} baseline by 1.71, while \textit{Inter-D (0-20)} is below \textsc{Random} of 0.09.

    \item \textbf{Model-dependent Performance Peak}: 
    Each model exhibits distinct optimal operating points along the diversity spectrum. Llama3.1-8B reaches peak performance at \textit{Inter-D (80-100)} and \textit{Intra-D (80-100)}, suggesting complementary benefits from both diversity types. 
    Qwen2-7B peaks in inter-type selection at low inter-diversity, while it peaks in intra-type selection at high intra-diversity.
\end{itemize}

These results show the promising potential of diversity-aware data selection, motivating us to further understand the performance variance more formally and propose principled solutions to adaptively achieve the performance peaks.

\paragraph{Practical Challenges}
Despite existing positive improvements on overall performance, the optimal distribution parameters exhibit model-dependent variability. This parameter sensitivity suggests the existence of \textit{multiple local optima} in the diversity-performance landscape. Two constraints merit consideration for real-world applications. \textbf{(1) Label Dependency}: The studied heuristic strategies \textit{Inter-Diversity} and \textit{Intra-Diversity} currently require domain-labeled data for centroid calculation. \textbf{(2) Distribution Transiency}: The optimal diversity parameters (e.g., 80-100 vs. 40-60) show sensitivity across tasks and models, necessitating automated and potentially costly configuration search.


\begin{table*}[t]
\centering
\caption{Performance of Llama3.1-8B and Qwen2-7B on various downstream task benchmarks under different constructed Inter-Diversity (Inter-D) and Intra-Diversity (Intra-D) distributions.}
\label{tab:main-diversity}
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{clcccccccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Distribution}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
~ & ~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\multirow{9}{*}{\textbf{Llama3.1-8B}} & \textbf{\textsc{Raw}} & 14.13 & 65.90 & 74.62 & 54.80 & 7.90 & 5.00 & 28.66 & 35.86 \\
& \textbf{\textsc{Random}} & 21.99 & 64.83 & 74.72 & 55.70 & 14.50 & 5.10 & 24.09 & 37.27 \\
& Inter-D (0-20) & 19.28 & 65.79 & 74.44 & 54.90 & 6.50 & 4.30 & 35.06 & 37.18 \\
& Inter-D (40-60) & 23.70 & 65.14 & 74.86 & 56.40 & 17.15 & 5.00 & 24.40 & 38.09 \\
& \textbf{Inter-D (80-100)} & 23.76 & 64.43 & 75.20 & 56.40 & 15.05 & 4.50 & 33.54 & \textbf{\underline{38.98}} \\
& Intra-D (0-20) & 22.08 & 65.08 & 75.00 & 54.70 & 16.20 & 4.40 & 33.54 & 38.71 \\
& Intra-D (40-60) & 22.12 & 64.74 & 74.87 & 54.00 & 16.00 & 6.00 & 27.44 & 37.88 \\
& \textbf{Intra-D (80-100)} & 19.78 & 64.77 & 74.51 & 56.50 & 13.00 & 5.20 & 37.50 & \textbf{\underline{38.75}} \\
\midrule
\multirow{8}{*}{\textbf{Qwen2-7B}} & \textbf{\textsc{Raw}} & 8.03 & 59.58 & 73.00 & 78.00 & 5.70 & 5.00 & 60.98 & 41.47 \\
& \textbf{\textsc{Random}} & 13.28 & 58.27 & 73.00 & 75.35 & 35.36 & 52.20 & 63.72 & 53.02 \\
& \textbf{Inter-D (0-20)} & 15.18 & 59.28 & 73.34 & 74.50 & 34.94 & 53.10 & 68.60 & \textbf{\underline{54.13}} \\
& Inter-D (40-60) & 14.62 & 58.58 & 73.35 & 72.50 & 34.50 & 52.50 & 61.28 & 52.47 \\
& Inter-D (80-100) & 9.30 & 57.72 & 73.14 & 74.60 & 28.00 & 51.30 & 63.42 & 51.07 \\
& Intra-D (0-20) & 12.64 & 58.54 & 73.35 & 75.10 & 8.75 & 51.10 & 61.59 & 48.72 \\
& Intra-D (40-60) & 15.24 & 58.57 & 73.12 & 74.70 & 32.50 & 51.80 & 64.02 & 52.85 \\
& \textbf{Intra-D (80-100)} & 11.91 & 57.88 & 73.29 & 75.00 & 36.05 & 52.50 & 66.16 & \textbf{\underline{53.25}} \\
\bottomrule
\end{tabular}}
\end{table*}
