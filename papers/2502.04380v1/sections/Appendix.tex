\appendix
\onecolumn

\section*{Appendix}
Our appendix is organized as follows:
\begin{itemize}
    \item Sec.~\ref{sec:app:notations}: The \textbf{symbols} used in this paper.
    \item Sec.~\ref{sec:appendix-div-setting}: The \textbf{details} about \textbf{experiments and \ours implementation}, including the construction of data pools (Sec.~\ref{sec:appendix-data-pool}), the benchmarks (Sec.~\ref{sec:appendix-benchmark}), the training \& evaluation implementation (Sec.~\ref{sec:appendix-implementation}), the implementation of baselines (Sec.~\ref{sec:appendix-baselines}), the visualization of baseline's selected data samples (Sec.~\ref{sec:appendix-baseline-tsne}), the implementation of the synthetic data generation (Sec.~\ref{sec:appendix-syn-data}), the implementation of seed generation \& clustering (Sec.~\ref{sec:appendix-domain-description}), and the prompts for the generation stage (Sec.~\ref{sec:appendix-prompt}).
    \item Sec~\ref{sec:appendix-proof}: The \textbf{full proof} for theoretical results are presented in Section~\ref{sec:theory}.
    \item Sec~\ref{sec:app-exp}: The \textbf{more experimental results}, including the visualization results of embeddings on Llama3.1-8B \& Qwen2.5-7B (Sec~\ref{sec:appendix-div-tsne-all}), the visualization results of inter- \& intra-diversity distributions (Sec~\ref{sec:appendix-div-tsne}), the similarity analysis in generated centroid quantities (Sec.~\ref{sec:appendix-centroids-gen}), the similarity analysis in generated centroid domains (Sec.~\ref{sec:appendix-centroids-domain}), the \textsc{DaaR} layer selection protocol (Sec.~\ref{sec:layer-selection}), the comprehensive training dynamics (Sec.~\ref{sec:appendix-dynamics}), the validation of inter-diversity and intra-diversity (Sec.~\ref{sec:appendix-total-diversity}) and the complete results of \ours with baselines on all LLMs (Sec.~\ref{sec:appendix-total-daar}).
\end{itemize}

\section{Notation}\label{sec:app:notations}

For ease of reading and reference, we present all mathematical symbols used in this paper in Table~\ref{tab:symbols-notation}.

\begin{table}[h]
\centering
\caption{Symbol Notation}
\label{tab:symbols-notation}
\begin{tabular}{c l | c l}
\toprule
\textbf{Symbol} & \textbf{Description} & \textbf{Symbol} & \textbf{Description} \\ 
\midrule
$\mathcal{D}$ & Composite dataset & $\mathcal{D}_k$ & Distinct domain dataset \\
$x_i^{(k)}$ & Raw $i$-th data sample in domain-$k$ & $\mathbf{x}_i^{(k)}$ & Embedded $i$-th data sample in domain-$k$ \\ 
$N_k$ & Number of $\mathcal{D}_k$ data samples & $\mathcal{C}_k$ & Domain centroid of $k$ \\ 
$\phi_{\text{inter}}$ & Sample-level cross-domain similarity & $\Phi_{\text{inter}}$ & Global inter-diversity \\ 
$\phi_{\text{intra}}$ & Sample similarity to its domain centroid & $\Phi_{\text{intra}}^{(k)}$ & Domain-level variance of domain-$k$ \\ 
$\mathcal{K}$ & Set of labeled domains & $h_k$ & Independent model of domain $k$ \\ 
$l$ & Loss function & $\mathcal{L}_{\mathcal{D}_k}$ & Empirical risk \\
$\tilde{\mathcal{D}}_m$ & Latent distribution of foundational capabilities & $\pi_{km}$ & Mixture weights \\
$h_{\theta_m}$ & Component model & $\theta_m^*$ & Optimal predictor \\
$\mathcal{S}_k^{(0)}$ & Generated centroids' seed sample & $\mathcal{S}_k^{(t)}$ & Generated centroid sample at $t$-th round \\
$\mathcal{M}_\text{ebd}$ & Embedding layer of LLM & $\mathcal{M}_3$ & Layer-3 of LLM \\
$\mathcal{D}_{\text{probe}}$ & \ours training set & $\tau$ & A similarity threshold \\
$\tilde{x}_i$ & Data sample in $\mathcal{D}_{\text{probe}}$ & $\tilde{y}_i$ & Pseudo-label of $\tilde{x}_i$ \\
$\psi_{\text{dom}}$ & A MLP-based domain classifier & $\psi_{\text{div}}$ & A MLP-based entropy predictor \\
$p_k(\tilde{x})$ & Domain predicted probability by $\psi_{\text{dom}}$ & $\hat{H}(\mathbf{x})$ & Predictive entropy by $\psi_{\text{dom}}$ \\
$\mathcal{L}_{\text{dom}}$ & Cross-entropy loss of $\psi_{\text{dom}}$ & $\mathcal{L}_{\text{div}}$ & MSE loss of $\psi_{\text{dom}}$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Experiments and Method Details}
\label{sec:appendix-div-setting}

\subsection{Construction of Data Pool}
\label{sec:appendix-data-pool}

To investigate data selection from large data pools and its impact on the mixture of downstream tasks, we construct a data pool with distinct properties to mimic practical settings. We select the following datasets to evaluate specific abilities:

\begin{itemize}
    \item \textbf{\textit{Common Sense}}: \textbf{Dolly-15K}~\cite{dolly-15k} with 15,011 samples, an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories.
    \item \textbf{\textit{Reasoning}}: \textbf{Cot-en}~\cite{cot-en} with 74,771 samples, is created by formatting and combining nine CoT datasets released by FLAN.
    \item \textbf{\textit{Mathematics}}: \textbf{Math-Instruct}~\cite{math-instruct} with 262,039 samples, is compiled from 13 math rationale datasets, six of which are newly curated by this work. It uniquely ensures extensive coverage of diverse mathematical fields.
    \item \textbf{\textit{Coding}}: \textbf{Code-Alpaca}~\cite{code-alpaca} with 20,016 samples, is constructed from real-world code examples, providing a rich set of tasks designed to guide models in generating accurate and functional code.
\end{itemize}

Each dataset was initially filtered and randomly reduced to 10,000 entries, resulting in a combined data pool of 40,000 entries. Specifically, for the Math-Instruct dataset, due to its inclusion of CoT and certain coding capabilities, we extract a highly mathematics-related subset and use regular expressions to filter out the coding-related content (including 'program', 'python', 'def', 'import', 'print', 'return'), ensuring it remains within the domain of mathematics.


\subsection{Benchmarks} \label{sec:appendix-benchmark}

To evaluate the models' true capabilities and performance across different domains, we follow the approach of major open-source LLMs (e.g., the Llama3 series~\cite{llama3series} and Qwen2 series~\cite{qwen2series}) and select the following widely used evaluation sets. All evaluations were conducted on the OpenCompass platform\footnote{https://opencompass.org.cn/}.

\begin{itemize}
    \item \textbf{\textit{Common Sense}}: \textbf{NQ}~\cite{nq} and \textbf{TriviaQA}~\cite{triviaqa}, which cover factual knowledge-based questions of varying difficulty.
    \item \textbf{\textit{Reasoning}}: \textbf{HellaSwag}~\cite{hellaswag}, which effectively evaluates the model's comprehensive reasoning ability.
    \item \textbf{\textit{Mathematics}}: \textbf{GSM8K}~\cite{gsm8k} and \textbf{MATH}~\cite{math} benchmarks, which encompass problems ranging from elementary to competition-level difficulty.
    \item \textbf{\textit{Coding}}: \textbf{MBPP}~\cite{mbpp} and \textbf{HumanEval}~\cite{humaneval}, which include evaluations of basic coding abilities in Python. We use the average of various metrics to demonstrate the models' overall performance across different domains.
\end{itemize}

Considering the numerous evaluation tasks, utilizing the complete evaluation set would result in significant time expenditure. To accelerate the evaluation process while maintaining fairness and accuracy, we randomly tailor the original evaluation sets into evaluation subsets, as detailed in Table \ref{tab:benchmark_samples}. \textbf{\textit{All experiments were conducted using this consistent setup}} to ensure the fairness of the experiments.
\begin{table}[h]
\centering
\caption{Number of samples in various evaluation benchmarks' datasets.}
\label{tab:benchmark_samples}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{\textbf{Number of Samples}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{Triviaqa} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} \\
\midrule
\textbf{Original} & 3,610 & 8,837 & 10,042 & 1,319 & 5,000 & 974 & 164 \\
\textbf{Utilized} & 3,610 & 5,000 & 10,042 & 500 & 1,000 & 500 & 164 \\
\bottomrule
\end{tabular}}
\end{table}


\subsection{Training and Evaluation Details}\label{sec:appendix-implementation}

\paragraph{Platform} We implement our approaches using PyTorch \cite{paszke2019pytorch} v2.4.1, coupled with PEFT v0.12.0 and the Transformers library \cite{wolf2020transformers} v4.45.2. Experiments are conducted on a computing platform equipped with four NVIDIA A100 GPUs (40GB), with pre-trained LLMs loaded as 16-bit floating-point numbers. 
The specific data-model development processes are completed in Data-Juicer Sandbox \cite{djsandbox}, via integration with the ms-swift~\cite{ms-swift} training repository, and the OpenCompass~\cite{opencompass} evaluation repository. 
%The trained rewarding models and the proposed selection algorithm are released and incorporated into Data-Juicer Sandbox \footnote{https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md}.

\paragraph{Training Details} 

In our experimental setup, we employ Low-Rank Adaptation (LoRA)~\cite{hu2021lora} adapters for the fine-tuning process, utilizing a LoRA-rank of 8 and a LoRA-alpha of 16. The learning rate was consistently maintained at $5 \times 10^{-5}$ across all experiments to ensure uniformity in training dynamics. We utilize a batch size of 4 and set the maximum sequence length to 2048 tokens to accommodate the model's capacity. To optimize the training process, a warmup ratio of 0.05 was applied and a validation ratio of 0.03 was used. The training was conducted over a single epoch, balancing computational efficiency with the need for effective model adaptation. Following some effective instruction-tuning work~\cite{zhou2024lima, lu2023instag}, we set the size of our subset to 8,000 entries, which constitutes 20\% of the data pool.

\paragraph{Evaluation Details} 

Following the guidelines provided by OpenCompass~\cite{opencompass}, we adhered to the default settings for our evaluation process. We select the hf-type as \textbf{\textit{base}} and utilized a batch size of \textbf{\textit{16}} to ensure efficient processing. For most tasks, we employ the \textbf{\textit{gen}} mode, while for the Hellaswag task, we opt for the \textbf{\textit{ppl}} mode to better assess perplexity. 

\subsection{Implementation of Baselines}
\label{sec:appendix-baselines}
We select the following representative methods and works related to data selection as our baselines to evaluate their performance in the context of a mixture of downstream tasks.

\paragraph{\textsc{Random Selection(Rand)}} A random selection of 8,000 data samples from the data pool was made, which to some extent reflects the distribution characteristics of the original data pool.

\paragraph{\textsc{Instruction Length (IL)}} The length of the instruction can be considered a measure of input complexity. It is widely believed~\cite{cao2023instruction, zhao2023preliminary} that more complex data is beneficial for enhancing model capabilities. Therefore, we select a subset of 8,000 entries with the maximum number of words (based on spaces) in the concatenation of Instruction and Input as part of the filtering process.

\paragraph{\textsc{AlpaGasus~\cite{chen2024alpagasus}}} Using prompts to directly score and annotate the quality of the data leverages the powerful cognitive abilities of LLMs for evaluation and selection. Based on the original work, we use the GPT-3.5-Turbo API to score the data with the following prompt:

\begin{promptbox}[Implementation of \textsc{AlpaGasus}]
    \vspace{-0.2cm}
    \textbf{System Prompt:}\newline
    We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following.\newline\newline
    Instruction: [Instruction]\newline
    Input: [Input]\newline
    Response: [Response]\newline
    
    \textbf{User Prompt:}\newline
    Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates a higher level of accuracy. The generated scores can be precise to decimal points, not just in increments of 0.5. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.
\end{promptbox}

The distribution of direct ratings for the 40,000 data pool is shown in Fig.~\ref{fig:Alpagasus}. Although the original paper's prompts were strictly followed and efforts were made to minimize potential bias, most scores still clustered around 5. Since we require a uniform selection of 8,000 data samples, we randomly select the subset with a rating of 5 to serve as the baseline data.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{pics/Alpagasus_Score.pdf}
\caption{The distribution of the score by \textsc{Alpagasus}.}
\label{fig:Alpagasus}
\end{figure}

\paragraph{\textsc{Instag~\cite{lu2023instag}}} first utilizes ChatGPT to tag the samples based on semantics and intentions, then trains a LLaMA-based tagger on the ChatGPT tags to tag data. They use the number of tags as a proxy for complexity. We directly use ChatGPT-3.5-Turbo as a tagger to achieve better performance. Following the original paper, the prompt is as follows.

\begin{promptbox}[Implementation of \textsc{Instag Complexity}]
    \vspace{-0.2cm}
    \textbf{System Prompt:}\newline
    You are a tagging system that provides useful tags for instruction intentions to distinguish instructions for a helpful AI assistant. Below is an instruction:

    [begin]\newline
    \{Instruction + Input\}\newline
    [end]\newline
    \textbf{User Prompt:}\newline
    Please provide coarse-grained tags, such as "Spelling and Grammar Check" and "Cosplay", to identify main intentions of above instruction. Your answer should be a list including titles of tags and a brief explanation of each tag. Your response have to strictly follow this JSON format: [{"tag": str, "explanation": str}]. Please response in English.
\end{promptbox}

A total of 19,585 tags were assigned across 40,000 data samples, with the distribution shown below. Subsequently, based on the procedures outlined in the original text, tags were deduplicated as follows: 1) filter out long-tail tags that appear fewer than $\alpha$ times in the entire annotated dataset, and 2) transform all tags to lowercase to mitigate the influence of capitalization.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\columnwidth]{pics/Instag_Score.pdf}
\caption{The distribution of the tags by \textsc{Instag}.}
\label{fig:Instag}
\end{figure}

\begin{itemize}
    \item \textbf{\textsc{Instag Complexity (Instag-C)}}: To reduce redundancy, we apply a threshold of $\alpha = 5$, resulting in a set of 1,948 valid tags. Following the definition of Complexity, we select the top 8,000 entries with the highest tag counts. Specifically, there are 8,211 entries with more than three tags, so we include all records with more than four tags and randomly supplement from those with exactly four tags until reaching a total of 8,000 entries.
    \item \textbf{\textsc{Instag Diversity (Instag-D)}}: For Diversity, we use $\alpha = 1$ to reduce redundancy. The algorithm employed involves sorting all data in descending order based on the number of tags, and prioritizing records with more tags. To ensure dataset diversity, a tag is only added to the final dataset if it increases the overall size of the tag set. This approach captures a broader range of unique tags, thereby enhancing the diversity and representativeness of the dataset.
\end{itemize}

\paragraph{\textsc{SuperFilter~\cite{li2024super}}} introduces a method that astonishingly utilizes a small GPT2 model to successfully filter out the high-quality subset from the existing GPT4-generated instruction tuning dataset. The core concept is the instruction-following difficulty (IFD) score. By meticulously following this methodology, we utilize GPT-2 to select 8,000 data samples.

\paragraph{\textsc{Deita~\cite{liu2023deita}}} is an open-sourced project designed to facilitate Automatic Data Selection for instruction tuning in LLMs. It delves into the relationships between data complexity, quality, and diversity, and develops a series of methodologies. Specifically, we utilize it as the following three baselines:

\begin{itemize}
    \item \textbf{\textsc{Deita Complexity (Deita-C)}}: Enhances instruction complexity by evolving examples with techniques like adding constraints. ChatGPT ranks these evolved samples for complexity, and the scores train a LLaMA-1-7B model to predict complexity in new instructions, refining complexity assessment. We utilize its LLaMA-1-7B-based complexity-scorer to evaluate the data pool and select the top 20\% of them to reach 8,000 entries.
    \item \textbf{\textsc{Deita Quality (Deita-Q)}}: Enhances response quality by prompting ChatGPT to iteratively improve helpfulness, relevance, depth, creativity, and detail. After five iterations, ChatGPT ranks and scores the responses, providing nuanced quality distinctions. These scores train an LLaMA-1 7B model to predict quality scores for new instruction-response pairs. We utilize its LLaMA-1-7B-based quality scorer to evaluate the data pool and select the top 20\% of them to reach 8,000 entries.
    \item \textbf{\textsc{Deita Deita (Deita-D)}}: Data-Efficient Instruction Tuning for Alignment, it selects data by combining complexity and quality into an evol score, prioritizing higher scores. It uses the REPR FILTER to iteratively add samples, ensuring diversity and avoiding redundancy, resulting in a balanced dataset of complexity, quality, and diversity. Following its methodology, we gain 8,000 entries as a baseline.
\end{itemize}

\subsection{Visualization of Baseline's Selected Data}
\label{sec:appendix-baseline-tsne}

We project all baseline-selected data samples through Llama3.1-8B's, Qwen2-7B's and Qwen2.5-7B's embedding layer and visualize their distributions via t-SNE dimensionality reduction in Fig.~\ref{fig:baseline-tsne-l3.1}, Fig.~\ref{fig:baseline-tsne-qw2} and Fig.~\ref{fig:baseline-tsne-qw2.5} respectively. This visualization provides intuitive insights into how different data selection strategies handle domain-mixed data. Three key observations emerge: 

\begin{itemize}
    \item[1] Certain methods (notably \textsc{Instruction Length}, \textsc{Deita-C}, and \textsc{Deita-Q} baselines) exhibit significant distributional bias against coding-domain samples, correlating with their poor HumanEval performance in Table~\ref{tab:qwen2-total-daar}.
    \item[2] The visualization reveals inherent challenges in maintaining original data distributions when handling domain-undetermined samples.
    \item[3]  Despite observable distribution shifts, not all methods show proportional performance degradation, aligning with our theoretical analysis in Section~\ref{sec:theory} about the link between latent distribution and LLM's fundamental capabilities.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\columnwidth]{pics/Baseline_tsne_l3.1.png}
\vspace{-0.4cm}
\caption{t-SNE visualization of data samples selected by different baselines using Llama3.1-8B embeddings. While original labels were removed during training, we preserve them in the legend for interpretability.}
\label{fig:baseline-tsne-l3.1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\columnwidth]{pics/Baseline_tsne_qw2.png}
\vspace{-0.4cm}
\caption{t-SNE visualization of data samples selected by different baselines using Qwen2-7B embeddings. While original labels were removed during training, we preserve them in the legend for interpretability.}
\label{fig:baseline-tsne-qw2}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{pics/Baseline_tsne_qw2.5.png}
\vspace{-0.3cm}
\caption{t-SNE visualization of data samples selected by different baselines using Qwen2.5-7B embeddings. While original labels were removed during training, we preserve them in the legend for interpretability.}
\label{fig:baseline-tsne-qw2.5}
\end{figure}



\subsection{Implementation of the Synthetic Data Generation}
\label{sec:appendix-syn-data}

For the generation of synthetic data, we utilize the \textbf{\textit{model.generate}} function from the PyTorch library. Key parameters included setting \textbf{\textit{max-new-tokens}} to \textbf{\textit{2048}} to control the length of the output, enabling \textbf{\textit{do-sample}} to \textbf{\textit{True}} to allow sampling, and configuring \textbf{\textit{top-p}} to \textbf{\textit{0.95}} and \textbf{\textit{temperature}} to \textbf{\textit{0.9}} to ensure diversity in the generated content. During the content extraction phase, we employ \textbf{\textit{regular expressions}} to efficiently extract and structure the desired information.

\subsection{Implementation of Seed Generation \& Clustering}
\label{sec:appendix-domain-description}

Due to the difficulty of pre-trained LLMs in providing accurate and coherent responses, we utilize their corresponding Instruct versions for centroid data generation. Specifically, we apply Llama3.1-8B-Instruct for Llama3.1-8B, Qwen2-7B-Instruct for Qwen2-7B, and Qwen2.5-7B-Instruct for Qwen2.5-7B. The domain description and the zero-shot prompt used in the Seed Generation phase are as follows. When clustering reward training data based on the seed, we select 5,000 entries that match the distribution of the fine-tuning data but do not overlap with it. This practice has proven that even a small amount of data can effectively train a reward probe module of \ours.

\begin{promptbox}[Domain's Description]
\vspace{-0.2cm}
\textbf{Common Sense}: Common sense generally includes a knowledge-based question and its corresponding answer, without reasoning.

\textbf{Reasoning}: Reasoning involves the ability to think logically about a situation or problem, to draw conclusions from available information, and to apply knowledge in new situations. 

\textbf{Mathematics}: Mathematical skills include the ability to perform calculations, understand mathematical concepts, solve hard and professional math problems, and apply mathematical reasoning. 

\textbf{Coding}: Design and generate specific code programs, or apply algorithms and data structures, with code generation in the Output.
\end{promptbox}

\begin{promptbox}[Seed Generation Zero-shot Prompt]
\vspace{-0.2cm}
You are an AI model with expertise in \{selected\_domain\}. Here's a brief description of this domain:
\{Prompt 1\}

Generate 5 different instruction pairs related to this field with various lengths. Maintain the index format: Instruction [1 ... 5].

The response should include three parts: \\
1. Instruction: A clear command or question that can be understood by the assistant.

2. Input: Any information provided to help it understand the instruction. If there is no need to generate, just keep it empty.

3. Output: The expected answer or action.

Keep the generated content focused on \{selected\_domain\}. And do not involve \{unselected\_domains\} related knowledge.
\end{promptbox}

\subsection{Prompts for the Generation Stage}\label{sec:appendix-prompt}
\vspace{0.2cm}

\begin{promptbox}[Instruction Pairs Generation]
\vspace{-0.2cm}
You are an AI model with expertise in \{selected\_domain\}. Here's a brief description of this domain:\{Prompt 1\} \newline
Generate only an instruction pair related to this field. The response should include three parts:

Instruction: A clear command or question that can be understood by the assistant.\newline
Input: Any information provided to help it understand the instruction. If there is no need to generate, just keep empty.\newline
Output: The expected answer or action.

Keep the generated content focused on \{selected\_domain\}. Do not involve \{unselected\_domain\} related knowledge.

Note that you should generate content strongly unrelated and different to these examples to ensure diversity in the generated output:\newline
Counterexample: \{\}

The format of the generated content should be: Instruction: [], Input: [], Output: [].
\end{promptbox}

\section{Proof of Section~\ref{sec:theory}}\label{sec:appendix-proof}

\subsection{Proof of Proposition~\ref{prop:latent-opt}}
\label{sec:appendix-proof-prop}

\begin{lemma}[Cross-Domain Risk Decomposition]
\label{lemma:risk-decomp}
Under Assumption~\ref{assump:latent}, the average cross-domain risk becomes:
\begin{equation}
    \mathbb{E}_{k \sim \mathcal{K}} \mathcal{L}_{\mathcal{D}_k}\left(\textstyle\sum_{m=1}^M \pi_{km}h_{\theta_m}\right) = \sum_{m=1}^M \lambda_m \mathbb{E}_{\tilde{\mathcal{D}}_m}\left[l\left(\textstyle\sum_{m'=1}^M \pi_{km'}h_{\theta_{m'}}(x), y\right)\right],
\end{equation}
where \(\lambda_m = \mathbb{E}_k[\pi_{km}]\) represents global activation of capability \(m\).
\end{lemma}

\begin{proof}
Under the finite variance condition \(\mathbb{E}_{\tilde{\mathcal{D}}_m}[\|l\|_2^2] < \infty\), Fubini's theorem holds, and results in:
\begin{align}
    \mathbb{E}_k \mathcal{L}_{\mathcal{D}_k} &= \mathbb{E}_k \mathbb{E}_{(x,y)\sim\mathcal{D}_k}\left[l\left(\textstyle\sum_m \pi_{km}h_{\theta_m}(x), y\right)\right] \notag \\ 
    &= \mathbb{E}_k \mathbb{E}_{m\sim\pi_k} \mathbb{E}_{\tilde{\mathcal{D}}_m}\left[l\left(\textstyle\sum_{m'} \pi_{km'}h_{\theta_{m'}}(x), y\right)\right] \notag \\ 
    &= \sum_{m=1}^M \lambda_m \mathbb{E}_{\tilde{\mathcal{D}}_m}\left[l\left(\textstyle\sum_{m'} \pi_{km'}h_{\theta_{m'}}(x), y\right)\right]
\end{align}

\end{proof}

\begin{theorem}[Global Optimality Condition]
\label{thm:global-opt}
Under Assumptions~\ref{assump:latent}-\ref{assump:opt}, the optimal predictors \(\{h_{\theta_m^*}\}\) minimize the cross-domain risk when:
\begin{equation}
    \forall m,\ \theta_m^* = \mathop{\mathrm{argmin}}_{\theta} \mathbb{E}_{\tilde{\mathcal{D}}_m}[l(h_\theta(x), y)].
\end{equation}
\end{theorem}

\begin{proof}
Taking derivatives of Lemma~\ref{lemma:risk-decomp}'s right-hand side:
\begin{align*}
    \frac{\partial}{\partial \theta_m} \sum_{m'=1}^M \lambda_{m'} \mathbb{E}_{\tilde{\mathcal{D}}_{m'}}[\cdot] = \lambda_m \mathbb{E}_{\tilde{\mathcal{D}}_m}\left[\frac{\partial l}{\partial h}\frac{\partial h_{\theta_m}}{\partial \theta_m}\right]  + \sum_{m'\neq m} \lambda_{m'} \mathbb{E}_{\tilde{\mathcal{D}}_{m'}}\left[\frac{\partial l}{\partial h}\frac{\partial h_{\theta_m}}{\partial \theta_m}\right].
\end{align*}
By strong convexity and optimality of \(\theta_m^*\), all terms vanish when \(\theta_m = \theta_m^*\).
\end{proof}

\subsection{Proof of Proposition~\ref{prop:diversity-decomp}}
\label{sec:appendix-proof-prop4.4}
\paragraph{Proof of Inter-Diversity}

To prove the inter-diversity decomposition as given in Proposition~\ref{prop:diversity-decomp}, we begin by considering the inter-diversity metric \(\Phi_{\text{inter}}\), defined as the expected pairwise distance between domain centroids from Eq~(\ref{eq:inter-diversity}):
\begin{equation}
\Phi_{\text{inter}} = \mathbb{E}_{k \neq l} \left[ \|\mathcal{C}_k - \mathcal{C}_l\|_2 \right].
\end{equation}

Substituting the expression for each domain centroid (Eq~(\ref{eq:centroid-coe})) \(\mathcal{C}_k = \sum_{m=1}^M \pi_{km} \mathcal{C}_m\), we have:
\begin{equation}
    \|\mathcal{C}_k - \mathcal{C}_l\|_2 = \left\| \sum_{m=1}^M \pi_{km} \mathcal{C}_m - \sum_{n=1}^M \pi_{ln} \mathcal{C}_n \right\|_2.
\end{equation}

Applying the triangle inequality and expanding, the distance becomes:
\begin{equation}
    = \left\| \sum_{m=1}^M \sum_{n=1}^M (\pi_{km} - \pi_{ln}) \mathcal{C}_m + \pi_{ln} \mathcal{C}_n \right\|_2.
\end{equation}

Taking the expectation over all domain pairs \(k \neq l\) and using linearity, we have:
\begin{equation}
    \Phi_{\text{inter}} = \mathbb{E}_{k \neq l} \left[ \sum_{m=1}^M \sum_{n=1}^M (\pi_{km} \pi_{ln}) \|\mathcal{C}_m - \mathcal{C}_n\|_2 \right].
\end{equation}

Recognizing that \(\lambda_m = \mathbb{E}_k[\pi_{km}]\) is the expected activation of capability \(m\), we can simplify further:
\begin{equation}
    = \sum_{m=1}^M \sum_{n=1}^M \lambda_m \lambda_n \|\mathcal{C}_m - \mathcal{C}_n\|_2.
\end{equation}

This completes the proof, showing that the inter-diversity metric is determined by the expected pairwise distances between latent capability centroids, weighted by their activation probabilities.

\paragraph{Proof of Intra-Diversity}

The intra-diversity metric for domain \(k\), denoted as \(\Phi_{\text{intra}}^{(k)}\), measures the variance of data samples within the domain relative to its centroid \(\mathcal{C}_k\). We aim to show that:
\begin{equation}
    \Phi_{\text{intra}}^{(k)} \leq \sum_{m=1}^M \pi_{km} \|\mathcal{C}_m - \mathcal{C}_k\|_2^2 + \mathbb{E}_{m\sim\pi_k}[\text{Var}(\tilde{\mathcal{D}}_m)].
\end{equation}

\begin{proof}
To prove this bound, we start by expressing the intra-diversity metric \(\Phi_{\text{intra}}^{(k)}\) as the expected squared distance from each sample \(\mathbf{x}_i^{(k)}\) to the domain centroid \(\mathcal{C}_k\) from Eq~(\ref{eq:intra-diversity}):
\begin{equation}
    \Phi_{\text{intra}}^{(k)} = \frac{1}{N_k} \sum_{i=1}^{N_k} \|\mathbf{x}_i^{(k)} - \mathcal{C}_k\|_2^2.
\end{equation}

Using the definition of the domain centroid (Eq (\ref{eq:centroid-coe})) \(\mathcal{C}_k = \sum_{m=1}^M \pi_{km} \mathcal{C}_m\) in , we can decompose the squared distance for each sample:
\begin{equation}
    \|\mathbf{x}_i^{(k)} - \mathcal{C}_k\|_2^2 = \|\mathbf{x}_i^{(k)} - \sum_{m=1}^M \pi_{km} \mathcal{C}_m\|_2^2.
\end{equation}

Applying the variance decomposition, we have:
\begin{equation}
    \|\mathbf{x}_i^{(k)} - \mathcal{C}_k\|_2^2 = \|\mathbf{x}_i^{(k)} - \sum_{m=1}^M \pi_{km} \mathcal{C}_m\|_2^2 = \sum_{m=1}^M \pi_{km} \|\mathbf{x}_i^{(k)} - \mathcal{C}_m\|_2^2 + \sum_{m=1}^M \pi_{km} \|\mathcal{C}_m - \mathcal{C}_k\|_2^2,
\end{equation}
where the first term represents the variance within each latent capability, and the second term accounts for the distance between latent capability centroids and the domain centroid.

By taking the expectation over the samples in domain \(k\), we get:
\begin{equation}
    \Phi_{\text{intra}}^{(k)} \leq \sum_{m=1}^M \pi_{km} \|\mathcal{C}_m - \mathcal{C}_k\|_2^2 + \mathbb{E}_{m\sim\pi_k}[\text{Var}(\tilde{\mathcal{D}}_m)],
\end{equation}
where \(\text{Var}(\tilde{\mathcal{D}}_m)\) represents the variance within the \(m\)-th latent capability. This completes the proof of the intra-diversity bound.
\end{proof}



\section{More Experimental Results}\label{sec:app-exp}
\subsection{Visualization of Embeddings on Llama3.1-8B \& Qwen2.5-7B}
\label{sec:appendix-div-tsne-all}

We process concatenated data samples with ``instruction''+``input''+``output'' pairs through each LLM's embedding layer, computing \textbf{mean token} embeddings for visualization. Beyond Qwen2-7B results in Fig.~\ref{fig:tsne}, we present the t-SNE visualization of data samples from Llama3.1-8 and Qwen2.5-7B with different distributions in Fig.~\ref{fig:tsne-l3.1} and Fig.~\ref{fig:tsne-qw2.5}. It is evident that, despite differences in architecture or model, the method of filtering data through Inter-Diversity and Intra-Diversity is effective. Notably, although the embedding distributions of different models are not identical, they exhibit similar behavior, indicating that the representations learned by the embedding layers are comparable.

\begin{figure*}[h]
    \centering
\includegraphics[width=\textwidth]{pics/tSNE_l3.1.png}
\vspace{-0.3cm}
    \caption{The t-SNE visualization of embeddings for data samples with different distributions on Llama3.1-8B. (a) The data pool of all 40,000 samples, (b) Randomly selected subset, (c) Distribution of data farthest from other domain centroids based on Inter-Diversity, (d) Distribution of data closest to other domain centroids based on Inter-Diversity, (e) Distribution of data closest to its own domain centroid based on Inter-Diversity, (f) Distribution of data farthest from its own domain centroid based on Inter-Diversity.}
    \label{fig:tsne-l3.1}
\end{figure*}

\begin{figure*}[h]
    \centering
\includegraphics[width=\textwidth]{pics/tSNE_qw2.5.png}
\vspace{-0.3cm}
    \caption{The t-SNE visualization of embeddings for data samples with different distributions on Qwen2.5-7B. (a) The data pool of all 40,000 samples, (b) Randomly selected subset, (c) Distribution of data farthest from other domain centroids based on Inter-Diversity, (d) Distribution of data closest to other domain centroids based on Inter-Diversity, (e) Distribution of data closest to its own domain centroid based on Inter-Diversity, (f) Distribution of data farthest from its own domain centroid based on Inter-Diversity.}
    \label{fig:tsne-qw2.5}
\end{figure*}




\subsection{The Visualization of Inter- \& Intra-Diversity Distribution Data}
\label{sec:appendix-div-tsne}

Using the Qwen2-7B model as an example, we construct data based on the Inter-Diversity and Intra-Diversity distribution methods, selecting a batch of data every 20\%. The visualization process is shown in Fig.~\ref{fig:inter-diversity-tsne} and Fig.~\ref{fig:intra-diversity-tsne}. As seen in the figures, the data gradually transitions from domain-aware diverse to domain-aware closed, indicating that our data construction method effectively controls the distribution of different data. However, it is important to note that t-SNE dimensionality reduction is used only for visualization purposes; during the reduction process, positional information is lost, and it cannot fully capture the true high-dimensional relationships between the data.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{pics/Inter_Diversity_qw2.png}
\vspace{-0.6cm}
\caption{Data visualization (t-SNE) based on different \textbf{Inter-Diversity} distributions on Qwen2-7B.}
\label{fig:inter-diversity-tsne}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{pics/Intra_Diversity_qw2.png}
\vspace{-0.6cm}
\caption{Data visualization (t-SNE) based on different \textbf{Intra-Diversity} distributions on Qwen2-7B.}
\label{fig:intra-diversity-tsne}
\end{figure}

\subsection{Similarity in Generated Centroid Quantities}
\label{sec:appendix-centroids-gen}

To assess the impact of the amount of generated data on the accuracy of Domain Semantic Centroids, we select quantities of 10 and 30 samples for generation. Subsequently, we calculate the semantic cosine similarity between these two sets, with results presented in Table~\ref{tab:similarity_10_30} (using Qwen2-7B and Llama3.1-8B as examples). The observed differences between varying data quantities were not significant, indicating that the generalization of the generated data is sufficient and further increasing the data quantity does not significantly enhance the accuracy of the centroids.
\begin{table}[h]
\centering
\caption{Semantic cosine similarity between generated samples of size 10 and 30 across different domains.}
\label{tab:similarity_10_30}
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\textbf{Similarity of 10 \& 30} & \textbf{Common Sense} & \textbf{Reasoning} & \textbf{Mathematics} & \textbf{Coding} \\
\midrule
\textbf{Qwen2-7B} & 0.9686 & 0.9866 & 0.9919 & 0.9881 \\
\textbf{Llama3.1-8B} & 0.9851 & 0.9795 & 0.9685 & 0.9895 \\
\bottomrule
\end{tabular}}
\label{tab:similarity}
\end{table}

\subsection{Similarity in Generated Centroid Domains}
\label{sec:appendix-centroids-domain}

To investigate whether there is a distinct separation between different domains generated, we compute the cosine similarity of semantic centroids across different domains. The experimental results are shown in Fig~\ref{fig:heatmap}. It is evident that, compared to variations within the same domain at different data quantities, there are significant differences between different domains. This validates that this method of generating synthetic data can produce domain-representative data with clear distinction.

Notably, despite variations in model architecture and parameter count, the generated content consistently exhibits the greatest divergence between common sense, reasoning, and coding domains. Specifically, the discrepancy between common sense and coding, as well as reasoning and coding, is markedly pronounced. Conversely, the semantic difference between common sense and reasoning is relatively smaller. This pattern suggests that the models, although differing in complexity and size, exhibit varying sensitivity to different data types, highlighting their nuanced capability to distinguish between certain domain-specific characteristics while maintaining subtle distinctions in others.

\begin{figure}[h]
\centering
\includegraphics[width=0.72\columnwidth]{pics/Heatmap.pdf}
\caption{Semantic cosine similarity across different domains for generated samples of size 10.}
\label{fig:heatmap}
\end{figure}

\subsection{\textsc{DaaR} Layer Selection Protocol}
\label{sec:layer-selection}

To determine an appropriate layer for embedding \ours into a large language model (LLM) that balances performance and computational cost, we first evaluate the capacity of different layers for domain awareness. Using Qwen2-7B as an example, we conduct t-SNE visualizations of Layer-5, Layer-10, Layer-15, Layer-20, Layer-25, and the last hidden layer, based on the average token vectors, as shown in Fig.~\ref{fig:layers}.

\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{pics/layers_qw2.png}
\vspace{-0.8cm}
\caption{t-SNE visualization of mean token embeddings across Layer-5, Layer-10, Layer-15, Layer-20, Layer-25, and the last hidden layer of Qwen2-7B.}
\label{fig:layers}
\end{figure}

Our observations indicate that even at the early Layer-5, the model begins to exhibit classification capabilities across different datasets. This prompted us to focus on the initial five layers to enhance learning outcomes. Considering that data clustering is inherently influenced by the embedding layer, a natural approach was to directly connect to the embedding layer for training. However, the training results showed persistently high training loss and a validation accuracy of only about 0.6, indicating suboptimal learning performance. Furthermore, related studies indicate that LLMs can learn semantic understanding and perception in the early layers~\cite{wei2022emergent}, consequently, we select Layer-3 for \ours.

\begin{figure}[h]
\centering
\includegraphics[width=0.35\columnwidth]{pics/DaaR_Training_Dynamic_ebd.png}
\vspace{-0.4cm}
\caption{Training loss and validation process of Embedding Layer on Qwen2-7B.}
\end{figure}

\subsection{Comprehensive Training Dynamics}
\label{sec:appendix-dynamics}

We present the training loss and validation results for two distinct models, Llama3.1-8B and Qwen2.5-7B, in Fig~\ref{fig:DaaR-dynamic-total}. It can be observed that across different models, the training process of \ours method consistently ensures gradual convergence, achieving high domain predictability and calibrated entropy fitting.

\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{pics/Training_Dynamic_l_q.pdf}
\vspace{-0.8cm}
\caption{Training loss and validation process of the two training stages of \ours on Llama3.1-8B and Qwen2.5-7B, showing the model gradually converging.}
\label{fig:DaaR-dynamic-total}
\end{figure}




\subsection{Complete Validation Results of Inter-Diversity and Intra-Diversity}
\label{sec:appendix-total-diversity}

We present the complete experimental results of Llama3.1-8B, Qwen2-7B, and Qwen2.5-7B in the validation experiments in Tables~\ref{tab:llama3.1-total-diversity}, Tables~\ref{tab:qwen2-total-diversity}, and Tables~\ref{tab:qwen2.5-total-diversity}, respectively. It can be observed that for any complete dataset, the conclusions from Section~\ref{sec:exps-observation} remain valid, specifically that the peak distribution of results is uneven, with significant differences among them.

\begin{table}[h]
\centering
\caption{Validation Results of Inter-Diversity and Intra-Diversity on Llama3.1-8B across various benchmarks.}
\label{tab:llama3.1-total-diversity}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Llama3.1-8B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 14.13 & 65.90 & 74.62 & 54.80 & 7.90 & 5.00 & 28.66 & 35.86 \\
\textbf{\textsc{Rand}} & 21.99 & 64.83 & 74.72 & 55.70 & 14.50 & 5.10 & 24.09 & 37.27 \\
\midrule
Inter-Diversity (0-20) & 19.28 & 65.79 & 74.44 & 54.90 & 6.50 & 4.30 & 35.06 & 37.18 \\
Inter-Diversity (20-40) & 21.91 & 65.48 & 74.54 & 52.50 & 16.65 & 5.70 & 26.53 & 37.62 \\
Inter-Diversity (40-60) & 23.70 & 65.14 & 74.86 & 56.40 & 17.15 & 5.00 & 24.40 & 38.09 \\
Inter-Diversity (60-80) & 22.42 & 64.52 & 74.76 & 55.10 & 14.60 & 7.40 & 31.10 & 38.56 \\
\textbf{Inter-Diversity (80-100)} & 23.76 & 64.43 & 75.20 & 56.40 & 15.05 & 4.50 & 33.54 & \textbf{\underline{38.98}} \\
\midrule
Intra-Diversity (0-20) & 22.08 & 65.08 & 75.00 & 54.70 & 16.20 & 4.40 & 33.54 & 38.71 \\
Intra-Diversity (20-40) & 22.41 & 64.44 & 74.66 & 52.60 & 15.30 & 4.20 & 27.44 & 37.29 \\
Intra-Diversity (40-60) & 22.12 & 64.74 & 74.87 & 54.00 & 16.00 & 6.00 & 27.44 & 37.88 \\
Intra-Diversity (60-80) & 20.83 & 64.30 & 74.36 & 52.20 & 14.90 & 4.50 & 35.98 & 38.15 \\
\textbf{Intra-Diversity (80-100)} & 19.78 & 64.77 & 74.51 & 56.50 & 13.00 & 5.20 & 37.50 & \textbf{\underline{38.75}} \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[h]
\centering
\caption{Validation Results of Inter-Diversity and Intra-Diversity on Qwen2-7B across various benchmarks.}
\label{tab:qwen2-total-diversity}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Qwen2-7B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 8.03 & 59.58 & 73.00 & 78.00 & 5.70 & 5.00 & 60.98 & 41.47 \\
\textbf{\textsc{Rand}} & 13.28 & 58.27 & 73.00 & 75.35 & 35.36 & 52.20 & 63.72 & \underline{53.02} \\
\midrule
\textbf{Inter-Diversity (0-20)} & 15.18 & 59.28 & 73.34 & 74.50 & 34.94 & 53.10 & 68.60 & \textbf{\underline{54.13}} \\
Inter-Diversity (20-40) & 13.77 & 58.42 & 73.18 & 73.60 & 32.55 & 53.00 & 64.33 & 52.69 \\
Inter-Diversity (40-60) & 14.62 & 58.58 & 73.35 & 72.50 & 34.50 & 52.50 & 61.28 & 52.47 \\
Inter-Diversity (60-80) & 14.31 & 58.60 & 73.33 & 74.80 & 33.90 & 51.40 & 60.68 & 52.43 \\
Inter-Diversity (80-100) & 9.30 & 57.72 & 73.14 & 74.60 & 28.00 & 51.30 & 63.42 & 51.07 \\
\midrule
Intra-Diversity (0-20) & 12.64 & 58.54 & 73.35 & 75.10 & 8.75 & 51.10 & 61.59 & 48.72 \\
Intra-Diversity (20-40) & 14.17 & 58.78 & 73.10 & 74.10 & 29.20 & 52.10 & 63.41 & 52.12 \\
Intra-Diversity (40-60) & 15.24 & 58.57 & 73.12 & 74.70 & 32.50 & 51.80 & 64.02 & 52.85 \\
Intra-Diversity (60-80) & 14.02 & 57.40 & 73.06 & 75.20 & 32.20 & 53.50 & 66.77 & 53.16 \\
\textbf{Intra-Diversity (80-100)} & 11.91 & 57.88 & 73.29 & 75.00 & 36.05 & 52.50 & 66.16 & \textbf{\underline{53.25}} \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[h]
\centering
\caption{Validation Results of Inter-Diversity and Intra-Diversity on Qwen2.5-7B across various benchmarks.}
\label{tab:qwen2.5-total-diversity}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Qwen2.5-7B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 8.84 & 58.14 & 72.75 & 78.20 & 9.10 & 7.40 & 78.05 & 44.64 \\
\textbf{\textsc{Rand}} & 11.46 & 57.85 & 73.08 & 78.90 & 13.15 & 62.50 & 71.65 & \underline{52.65} \\
\midrule
Inter-Diversity (0-20) & 13.23 & 58.15 & 73.27 & 78.70 & 11.45 & 62.30 & 69.21 & 52.33 \\
Inter-Diversity (20-40) & 10.81 & 58.11 & 73.02 & 77.90 & 16.95 & 62.30 & 68.29 & 52.48 \\
\textbf{Inter-Diversity (40-60)} & 10.75 & 57.89 & 72.90 & 73.30 & 26.70 & 62.80 & 69.51 & \textbf{\underline{53.41}} \\
Inter-Diversity (60-80) & 10.43 & 58.19 & 73.10 & 78.40 & 17.05 & 62.80 & 71.95 & 53.13 \\
Inter-Diversity (80-100) & 10.00 & 58.10 & 73.11 & 77.30 & 16.45 & 62.30 & 67.07 & 52.05 \\
\midrule
\textbf{Intra-Diversity (0-20)} & 10.68 & 58.52 & 73.18 & 80.10 & 25.80 & 62.50 & 68.90 & \textbf{\underline{54.24}} \\
Intra-Diversity (20-40) & 11.21 & 58.14 & 73.02 & 79.50 & 17.75 & 62.90 & 67.38 & 52.84 \\
Intra-Diversity (40-60) & 11.57 & 58.11 & 72.94 & 76.00 & 15.65 & 62.50 & 65.25 & 51.72 \\
Intra-Diversity (60-80) & 10.89 & 57.91 & 72.92 & 75.80 & 11.35 & 62.00 & 66.16 & 51.00 \\
Intra-Diversity (80-100) & 12.79 & 58.09 & 73.21 & 75.40 & 16.05 & 62.50 & 49.39 & 49.63 \\
\bottomrule
\end{tabular}}
\end{table}



\newpage
\subsection{Complete Results of \ours with Baselines}
\label{sec:appendix-total-daar}
Additionally, we include the complete results of \ours and the comparative baselines in the three tables: Table~\ref{tab:llama3.1-total-daar}, Table~\ref{tab:qwen2-total-daar} and Table~\ref{tab:qwen2.5-total-daar}. The results illustrate the challenges of the scenario, particularly for the Qwen2 series, where baseline methods struggle to outperform random selection. Furthermore, they demonstrate the robustness and effectiveness of our approach, consistently achieving the highest average scores across different models.

\begin{table}[h]
\centering
\caption{Performance of \ours with baselines on Llama3.1-8B across various benchmarks.}
\label{tab:llama3.1-total-daar}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Llama3.1-8B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 14.13 & 65.90 & 74.62 & 54.80 & 7.90 & 5.00 & 28.66 & 35.86 \\
\textbf{\textsc{Rand}} & 21.99 & 64.83 & 74.72 & 55.70 & 14.50 & 5.10 & 24.09 & 37.27 \\
\textbf{\textsc{Instruction Len}} & 15.34 & 63.60 & 73.73 & 54.00 & 15.40 & 3.60 & 30.80 & 36.64 \\
\textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 21.57 & 64.37 & 74.87 & 55.20 & 17.65 & 4.60 & 16.16 & 36.34 \\
\textbf{\textsc{Instag-C}~\cite{lu2023instag}} & 18.12 & 64.96 & 74.01 & 55.70 & 15.50 & 4.80 & 37.81 & 38.70 \\
\textbf{\textsc{Instag-D}~\cite{lu2023instag}} & 21.94 & 64.69 & 74.87 & 54.80 & 12.80 & 4.10 & 9.76 & 34.71 \\
\textbf{\textsc{SuperFilter}~\cite{li2024super}} & 22.95 & 64.99 & 76.39 & 57.60 & 6.05 & 2.60 & 40.55 & \underline{38.73} \\
\textbf{\textsc{Deita-C}~\cite{liu2023deita}} & 15.58 & 64.97 & 74.21 & 55.00 & 13.05 & 4.60 & 34.46 & 37.41 \\
\textbf{\textsc{Deita-Q}~\cite{liu2023deita}} & 19.57 & 64.22 & 75.15 & 54.00 & 7.20 & 4.20 & 28.35 & 36.10 \\
\textbf{\textsc{Deita-D}~\cite{liu2023deita}} & 20.97 & 63.32 & 75.10 & 54.90 & 7.00 & 4.00 & 31.71 & 36.71 \\
\midrule
\textbf{\ours (Ours)} & 20.08 & 64.55 & 74.88 & 54.8 & 15.30 & 4.70 & 37.50 & \textbf{\underline{38.83}} \\
\bottomrule
\end{tabular}}
\end{table}


\begin{table}[h]
\centering
\caption{Performance of \ours with baselines on Qwen2-7B across various benchmarks.}
\label{tab:qwen2-total-daar}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Qwen2-7B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 8.03 & 59.58 & 73.00 & 78.00 & 5.70 & 5.00 & 60.98 & 41.47 \\
\textbf{\textsc{Rand}} & 13.28 & 58.27 & 73.00 & 75.35 & 35.36 & 52.20 & 63.72 & \underline{53.02} \\
\textbf{\textsc{Instruction Len}} & 8.62 & 58.44 & 72.86 & 73.30 & 27.05 & 53.10 & 63.72 & 51.01 \\
\textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 13.67 & 57.94 & 73.04 & 73.90 & 32.30 & 51.40 & 63.41 & 52.24 \\
\textbf{\textsc{Instag-C}~\cite{lu2023instag}} & 9.51 & 58.50 & 73.06 & 74.70 & 35.35 & 51.90 & 64.70 & 52.53 \\
\textbf{\textsc{Instag-D}~\cite{lu2023instag}} & 12.87 & 57.48 & 72.80 & 74.40 & 33.75 & 51.80 & 64.02 & 52.45 \\
\textbf{\textsc{SuperFilter}~\cite{li2024super}} & 19.16 & 58.98 & 72.99 & 73.70 & 30.10 & 52.40 & 58.85 & 52.31 \\
\textbf{\textsc{Deita-C}~\cite{liu2023deita}} & 8.94 & 58.07 & 73.06 & 73.90 & 35.55 & 52.90 & 62.20 & 52.09 \\
\textbf{\textsc{Deita-Q}~\cite{liu2023deita}} & 14.06 & 59.07 & 73.16 & 75.80 & 35.50 & 23.00 & 58.24 & 48.40 \\
\textbf{\textsc{Deita-D}~\cite{liu2023deita}} & 16.41 & 57.80 & 72.70 & 76.10 & 29.05 & 52.40 & 64.63 & 52.73 \\
\midrule
\textbf{\ours (Ours)} & 16.88 & 57.58 & 73.03 & 75.40 & 38.1 & 52.00 & 64.94 & \textbf{\underline{53.99}} \\
\bottomrule
\end{tabular}}
\end{table}


\begin{table}[h]
\centering
\caption{Performance of \ours with baselines on Qwen2.5-7B across various benchmarks.}
\label{tab:qwen2.5-total-daar}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{\textbf{Qwen2.5-7B}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\textbf{\textsc{Raw}} & 8.84 & 58.14 & 72.75 & 78.20 & 9.10 & 7.40 & 78.05 & 44.64 \\
\textbf{\textsc{Rand}} & 11.46 & 57.85 & 73.08 & 78.90 & 13.15 & 62.50 & 71.65 & \underline{52.65} \\
\textbf{\textsc{Instruction Len}} & 11.34 & 58.01 & 72.79 & 78.00 & 15.80 & 62.30 & 68.12 & 52.34 \\
\textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 10.40 & 57.87 & 72.92 & 77.20 & 18.75 & 61.80 & 65.55 & 52.07 \\
\textbf{\textsc{Instag-C}~\cite{lu2023instag}} & 10.81 & 58.45 & 73.27 & 76.00 & 13.30 & 61.80 & 68.29 & 51.70 \\
\textbf{\textsc{Instag-D}~\cite{lu2023instag}} & 11.08 & 58.40 & 72.79 & 76.40 & 16.40 & 62.90 & 70.43 & 52.63 \\
\textbf{\textsc{SuperFilter}~\cite{li2024super}} & 13.54 & 58.51 & 72.89 & 79.30 & 11.35 & 39.50 & 65.25 & 48.62 \\
\textbf{\textsc{Deita-C}~\cite{liu2023deita}} & 10.50 & 58.17 & 73.14 & 74.60 & 16.60 & 62.00 & 72.26 & 52.47 \\
\textbf{\textsc{Deita-Q}~\cite{liu2023deita}} & 11.24 & 57.83 & 72.97 & 78.50 & 12.95 & 38.10 & 67.68 & 48.47 \\
\textbf{\textsc{Deita-D}~\cite{liu2023deita}} & 10.48 & 57.81 & 73.05 & 77.20 & 15.25 & 52.90 & 69.21 & 50.84 \\
\midrule
\textbf{\ours (Ours)} & 15.83 & 58.65 & 72.48 & 80.20 & 16.70 & 64.20 & 68.29 & \textbf{\underline{53.76}} \\
\bottomrule
\end{tabular}}
\end{table}







