\section{Introduction}
The advancement of large language models (LLMs), exemplified by open-source models such as the Llama3 series~\cite{llama3series}, Qwen2 series~\cite{qwen2series}, and the DeepSeek-V3 series~\cite{deepseekv3}, along with closed-source models such as GPT-4~\cite{gpt4}, has revolutionized artificial intelligence by enhancing capabilities in foundational abilities such as common sense, reasoning, mathematics, and coding. Fine-tuning these models further optimizes their usability by enhancing their performance and aligning with specific human instructions and preferences ~\cite{alpaca2023, dpo}.

To cultivate comprehensive capabilities in LLMs, 
fruitful studies have explored preferable trade-offs between quality, quantity, and diversity of their training data  ~\cite{li2024quantity,djv2,qin2024surveycodev,zhao2024beyond}.
For example, methods focusing on data selection~\cite{li2024super, xialess,wang2023far} and mixture~\cite{ge2024bimix} demonstrate promising capabilities to enhance model performance, particularly through semantic diversity ~\cite{lu2023instag, liu2023deita}.

However, real-world applications frequently encounter unlabeled data and difficulties in domain labeling \cite{ge2023openagi}, posing challenges for data mixture methodologies that take the domain tags as a prior, as well as the data selection approaches, which often prioritize quality over diversity especially with data sourced from quite different domains.

To leverage the best of both worlds, in this work, we propose a new fine-tuning method for LLMs named \ours, which encourages the given model to learn \textbf{d}iversity \textbf{a}s \textbf{a} \textbf{r}eward signal, and then to autonomously select domain-undetermined training data to achieve a theoretically informed trade-off to maximize diversity for overall model performance enhancement.

Our investigation begins by diving into the semantic diversity of LLM data, considering how to explicitly model it and how it can influence the overall performance of LLMs. We systematically construct contrastive data pools and conduct extensive empirical examinations of different distributional performances across various foundational LLM capabilities. Based on these observations, we provide theoretical discussions on a general assumption that a mixture of underlying component LLMs determines the data mixing effect, and the explanations regarding inter- and intra-diversity modeled from a data semantic perspective.


\begin{figure*}[t!]
    \centering
\includegraphics[width=\textwidth]{pics/Combined_tSNE_2.png}
    %\vspace{-0.2in}
    \caption{The t-SNE visualization of embeddings for data samples with different distributions on Qwen2-7B. (a) The data pool of all 40,000 samples, (b) Randomly selected subset, (c) Distribution of data farthest from other domain centroids based on Inter-Diversity, (d) Distribution of data closest to other domain centroids based on Inter-Diversity, (e) Distribution of data closest to its own domain centroid based on Inter-Diversity, (f) Distribution of data farthest from its own domain centroid based on Inter-Diversity.}
    \label{fig:tsne}
        %\vspace{-0.15in}
\end{figure*}

Our theoretical insights illuminate that optimal solutions for overall model performance enhancement cannot be explicitly applied to domain-undetermined data. To address this, we integrate an external multi-layer perceptron (MLP) structure into the LLM to be tuned, acting as a trainable probe module conditioned on the inherent knowledge and weights of the given LLM. 
The probe module takes semantic entropy as a proxy measure of diversity, and output reward scores, and select suitable data subsets aligned with the LLM's underlying mixture distributions for better diversity.
This is achieved by training the probe module on synthetically generated domain-aware data by the model itself, and using the data after selection to further self-fine-tuning.
Extensive experiments on various state-of-the-art (SOTA) LLMs like the Qwen and LLaMA series verify the effectiveness of \ours in enhancing overall model capabilities over many SOTA baseline methods.

Our contributions are threefold:
%\vspace{-0.1in}
\begin{itemize}[leftmargin=*]
    \item We empirically and theoretically demonstrate how data plays a crucial role to enhance LLMs in semantic diversity, providing generalized modeling based on mixture of component models.
    \item We propose a lightweight, self-rewarding method for selecting data in domain-undetermined scenarios, which utilizes synthesized model-aware data to maximize domain diversity and fine-tuning performance. 
    \item We verify that the proposed methods can effectively balance seven diverse benchmarks for improved overall performance when applied to representative LLMs, whereas other SOTA methods struggle in such challenging scenarios. Our code is open-sourced \footnote{\href{https://github.com/modelscope/data-juicer/tree/DaaR}{https://github.com/modelscope/data-juicer/tree/DaaR}} to foster more in-depth understanding and future progress.
\end{itemize}
