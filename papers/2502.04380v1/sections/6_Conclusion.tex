\section{Conclusion and Future Work}
In this paper, we propose a new approach to fine-tuning LLMs with data that lacks clear domain labels, using diversity as a guiding principle. Our method allows LLMs to automatically select and benefit from diverse datasets. 
By measuring semantic diversity with entropy, we employ a self-reward mechanism built upon the given LLM, identifying data that best fits the model's natural tendencies in terms of its underlying knowledge distribution. 
Our experiments with various SOTA LLMs show notable superiority of the method over SOTA methods, highlighting the potential of data diversity to enhance model overall performance.

This research demonstrates feasibility of reward modeling on LLM data diversity and deepens the understanding of its utilities, especially when domain labels are missing or uncertain. 
Future work could include developing methods to efficiently adjust diversity measures and learning algorithm, thus supporting LLMs in self-evolving towards artificial general intelligence in dynamic environments.





% %\vspace{-0.1in}
% \section{Conclusion}
% In this paper, we propose a new approach named \ours to fine-tuning LLMs with data that lacks clear domain labels, using diversity as a guiding principle. \ours allows LLMs to automatically select and benefit from diverse datasets. By measuring semantic diversity with entropy, we employ a self-reward mechanism built upon the given LLM, identifying data that best fits the model's natural tendencies in terms of its underlying knowledge distribution. Extensive experiments with various SOTA LLMs show notable superiority of \ours over SOTA methods, demonstrating the feasibility of reward modeling on LLM data diversity.
% %This research shows feasibility of reward modeling on LLM data diversity and deepens the understanding of its utilities, especially when domain labels are missing or uncertain. 
