\section{\ours: Diversity as a Reward}
\label{sec:daar}
%\subsection{A Data-Centric Method with Diversity Reward}
To address the challenges identified and leverage the insights gained in previous Sec.~\ref{sec:observation} and Sec.~\ref{sec:theory}, we establish a data selection method \ours guided by diversity-aware reward signals. 
It comprises three key components illustrated in subsequent sections: (1) model-aware centroid synthesis, (2) two-stage training with reward probe, and (3) diversity-driven data selection.

\subsection{Model-Aware Training Data}
\paragraph{Model-Aware Centroid Construction}
The proposed method initiates with centroid self-synthesis through a two-phase generation process to address two fundamental challenges: (1) eliminating dependency on human annotations through automated domain prototyping, and (2) capturing the base model's intrinsic feature space geometry for model-aware domain separation. 

\begin{itemize}[leftmargin=*]
\item \textbf{Phase 1 - Seed Generation}: For each domain $k$, generate 5 seed samples $\mathcal{S}_k^{(0)}$ via zero-shot prompting with domain-specific description templates, establishing initial semantic anchors. The prompt details and ablation of choices on sample number are provided in Appendix~\ref{sec:appendix-domain-description}.

\item \textbf{Phase 2 - Diversity Augmentation}: Iteratively expand \( S_k^{(t)} \) through context-aware generation, conditioned on a sliding window buffer with 3 random anchors sampled from the $(t-1)$ iteration \( S_k^{(t-1)} \). 
The generated sample \( \mathbf{x'} \) will be admitted during the iteration via geometric rejection sampling:
\begin{equation}
    \max_{\mathbf{x} \in S_k^{(t-1)}} \cos(\mathcal{M}_{\text{ebd}}(\mathbf{x'}), \mathcal{M}_{\text{ebd}}(\mathbf{x})) < \tau = 0.85,
\end{equation}
where $\mathcal{M}_\text{ebd}(\cdot) $ indicates the embedding layer outputs of the given LLM. This process terminates when \( |S_k| = 30 \).
More analysis and ablation on these choices are provided in the Appendix. 
In Appendix~\ref{sec:appendix-centroids-gen}, we found that the adopted number of samples sufficiently leads to a stable convergence and larger data quantity does not impact the final centroids.
In Appendix~\ref{sec:appendix-centroids-domain}, we found that this synthetic data has the ability to produce domain-representative data with clear distinction. Interestingly, the generated content consistently exhibits the greatest divergence between common sense, reasoning, and coding domains across model architectures and parameters.
\end{itemize}

The domain centroid is then computed from the final augmented set $\mathcal{S}_k$ using the model's knowledge prior:
\begin{equation}
\mathcal{C}_k = \frac{1}{|\mathcal{S}_k|} \sum_{\mathbf{x}_i \in \mathcal{S}_k} \mathcal{M}_\text{ebd}(\mathbf{x}_i).
\end{equation} 
This captures the LLM's intrinsic feature space geometry while eliminating dependency on human annotations.

\paragraph{Domain-Aware Clustering} 
We then automatically construct pseudo-labels for the given data samples based on the previously synthesized centroids $\{C_k\}_{k=1}^K$. Take them as fixed prototypes, we perform constrained k-means clustering in the embedding space:
\begin{equation}
    \arg\min_{\{S_k\}} \sum_{k=1}^K \sum_{\tilde{x} \in S_k} \|\mathcal{M}_\text{ebd}(\tilde{x}) - \mathcal{C}_k\|^2.
\end{equation}
This produces the seed dataset $\mathcal{D}_{\text{probe}}$ containing 
pseudo-labels $\{\tilde{y}_i\}_{i=1}$ where $\tilde{y}_i \in \{1, \ldots, K\}$, with model-induced and embedding-derived domain label assignments.

% We then construct the \ours's seed training set $\mathcal{D}_{\text{probe}}$ by sampling 5,000 samples $\{\tilde{x}_i\}_{i=1}^{5000}$ from the model's candidate dataset to be selected but without any data leaking. Using synthesized centroids $\{C_k\}_{k=1}^K$ as fixed prototypes, we perform constrained k-means clustering in the embedding space:
% \begin{equation}
%     \arg\min_{\{S_k\}} \sum_{k=1}^K \sum_{\tilde{x} \in S_k} \|\mathcal{M}_\text{ebd}(\tilde{x}) - \mathcal{C}_k\|^2.
% \end{equation}
% This produces pseudo-labels $\{\tilde{y}_i\}_{i=1}^{5000}$ where $\tilde{y}_i \in \{1, \ldots, K\}$, representing model-induced domain assignments. The final $\mathcal{D}_{\text{probe}}$ comprises pairs $\{(\tilde{x}_i, \tilde{y}_i)\}_{i=1}^{5000}$ with embedding-derived labels.

\subsection{Training for Self-Rewarding Abilities}

\paragraph{Stage 1: Domain Discrimination}
The proposed \ours then establishes model-aware domain discrimination abilities through a multi-layer perceptron (MLP) probe module, $\psi_{\text{dom}}$, attached to the layer-3 of the LLMs $\mathcal{M}_3(\tilde{x})$. 
The probe will be trained meanwhile all the parameters of the LLM are frozen.
This achieves a preferable balance between effectiveness and cost, with detailed analysis regarding the choice of Layer-3 presented in Appendix~\ref{sec:layer-selection}.
Specifically, with pseudo-label $\tilde{y}$, we can compute domain probabilities as:
\begin{equation}
p_k(\tilde{x}) = \text{softmax}\left(\psi_{\text{dom}}(\mathcal{M}_3(\tilde{x}))\right), \quad \psi_{\text{dom}}:\mathbb{R}^d\rightarrow\mathbb{R}^K,
\end{equation}
where $\psi_{\text{dom}}$ is optimized via cross-entropy loss function:
\begin{equation}
\mathcal{L}_{\text{dom}} = -\frac{1}{|\mathcal{D}_{\text{probe}}|}\sum_{(\tilde{x},\tilde{y})\in\mathcal{D}_{\text{probe}}} \sum_{k=1}^K \mathbb{I}_{[k=\tilde{y}]} \log p_k(\tilde{x}),
\end{equation}
where $\mathbb{I}_{[k=\tilde{y}]}$ denotes the indicator function. We employ single-sample batches with the AdamW optimizer to prevent gradient averaging across domains. Training consistently converges and achieves 92.7\% validation accuracy on domain classification, as shown in Fig.~\ref{fig:DaaR-dynamic} (a).

\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{pics/DaaR_Training_Dynamic_qw2.pdf}
%\vspace{-0.5cm}
\caption{Training loss and validation process of the two stages of \ours on Qwen2-7B, detailed results are in Appendix~\ref{sec:appendix-dynamics}.}
\label{fig:DaaR-dynamic}
\end{figure}

\paragraph{Stage 2: Diversity Rewarding}
Building on the stabilized domain probe module, we quantify sample-level diversity through predictive entropy:
\begin{equation}
H(\tilde{x}) = -\sum_{k=1}^K p_k(\tilde{x})\log p_k(\tilde{x}).
\end{equation}
To enable efficient reward computation during data selection, we then train another 5-layer MLP $\psi_{\text{div}}$ to directly estimate $H(\tilde{x})$ from $\mathcal{M}_3(\tilde{x})$:
\begin{equation}
\hat{H}(\tilde{x}) = \psi_{\text{div}}(\mathcal{M}_3(\tilde{x})), \quad \psi_{\text{div}}:\mathbb{R}^d\rightarrow\mathbb{R}^+.
\end{equation}
This diversity probe module $\psi_{\text{div}}$ shares $\psi_{\text{dom}}$'s architecture up to its final layer (replaced with single-output regression head), trained using entropy-scaled MSE:
\begin{equation}
\mathcal{L}_{\text{div}} = \frac{1}{|\mathcal{D}_{\text{probe}}|}\sum_{\tilde{x}\in\mathcal{D}_{\text{probe}}} \left(\hat{H}(\tilde{x}) - H(\tilde{x})\right)^2.
\end{equation}
This module is also well-converged as shown in Fig.~\ref{fig:DaaR-dynamic} (b).

\begin{table*}[t]
\centering
\caption{Evaluation results of Llama3.1-8B, Qwen2-7B, and Qwen2.5-7B across various downstream task benchmarks. \ours demonstrates superiority in average performance (\textsc{AVG}) compared to other baselines.}
\label{tab:main-daar}
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{clcccccccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Distribution}} & \multicolumn{2}{c}{\textbf{Common Sense}} & \multicolumn{1}{c}{\textbf{Reasoning}} & \multicolumn{2}{c}{\textbf{Mathematics}} & \multicolumn{2}{c}{\textbf{Coding}} & \multirow{2}{*}{\textbf{Avg}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
~ & ~ & \textbf{NQ} & \textbf{TriviaQA} & \textbf{Hellaswag} & \textbf{GSM8K} & \textbf{MATH} & \textbf{MBPP} & \textbf{HumanEval} & \\
\midrule
\multirow{9}{*}{\textbf{Llama3.1-8B}} & \textbf{\textsc{Raw}} & 14.13 & 65.90 & 74.62 & 54.80 & 7.90 & 5.00 & 28.66 & 35.86 \\
& \textbf{\textsc{Random}} & 21.99 & 64.83 & 74.72 & 55.70 & 14.50 & 5.10 & 24.09 & 37.27 \\
& \textbf{\textsc{Instruction Len}} & 15.34 & 63.60 & 73.73 & 54.00 & 15.40 & 3.60 & 30.80 & 36.64 \\
& \textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 21.57 & 64.37 & 74.87 & 55.20 & 17.65 & 4.60 & 16.16 & 36.34 \\
& \textbf{\textsc{Instag-Best}~\cite{lu2023instag}} & 18.12 & 64.96 & 74.01 & 55.70 & 15.50 & 4.80 & 37.81 & 38.70 \\
& \textbf{\textsc{SuperFilter}~\cite{li2024super}} & 22.95 & 64.99 & 76.39 & 57.60 & 6.05 & 2.60 & 40.55 & \underline{38.73} \\
& \textbf{\textsc{Deita-Best}~\cite{liu2023deita}} & 15.58 & 64.97 & 74.21 & 55.00 & 13.05 & 4.60 & 34.46 & 37.41 \\
\cmidrule(lr){2-10}
& \ours (Ours) & 20.08 & 64.55 & 74.88 & 54.8 & 15.30 & 4.70 & 37.50 & \textbf{\underline{38.83}} \\
\midrule
\multirow{9}{*}{\textbf{Qwen2-7B}} & \textbf{\textsc{Raw}} & 8.03 & 59.58 & 73.00 & 78.00 & 5.70 & 5.00 & 60.98 & 41.47 \\
& \textbf{\textsc{Random}} & 13.28 & 58.27 & 73.00 & 75.35 & 35.36 & 52.20 & 63.72 & \underline{53.02} \\
& \textbf{\textsc{Instruction Len}} & 8.62 & 58.44 & 72.86 & 73.30 & 27.05 & 53.10 & 63.72 & 51.01 \\
& \textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 13.67 & 57.94 & 73.04 & 73.90 & 32.30 & 51.40 & 63.41 & 52.24 \\
& \textbf{\textsc{Instag-Best}~\cite{lu2023instag}} & 9.51 & 58.50 & 73.06 & 74.70 & 35.35 & 51.90 & 64.70 & 52.53 \\
& \textbf{\textsc{SuperFilter}~\cite{li2024super}} & 19.16 & 58.98 & 72.99 & 73.70 & 30.10 & 52.40 & 58.85 & 52.31 \\
& \textbf{\textsc{Deita-Best}~\cite{liu2023deita}} & 16.41 & 57.80 & 72.70 & 76.10 & 29.05 & 52.40 & 64.63 & 52.73 \\
\cmidrule(lr){2-10}
& \ours (Ours) & 16.88 & 57.58 & 73.03 & 75.40 & 38.1 & 52.00 & 64.94 & \textbf{\underline{53.99}} \\
\midrule
\multirow{9}{*}{\textbf{Qwen2.5-7B}} & \textbf{\textsc{Raw}} & 8.84 & 58.14 & 72.75 & 78.20 & 9.10 & 7.40 & 78.05 & 44.64 \\
& \textbf{\textsc{Random}} & 11.46 & 57.85 & 73.08 & 78.90 & 13.15 & 62.50 & 71.65 & \underline{52.65} \\
& \textbf{\textsc{Instruction Len}} & 11.34 & 58.01 & 72.79 & 78.00 & 15.80 & 62.30 & 68.12 & 52.34 \\
& \textbf{\textsc{Alpagasus}~\cite{chen2024alpagasus}} & 10.40 & 57.87 & 72.92 & 77.20 & 18.75 & 61.80 & 65.55 & 52.07 \\
& \textbf{\textsc{Instag-Best}~\cite{lu2023instag}} & 11.08 & 58.40 & 72.79 & 76.40 & 16.40 & 62.90 & 70.43 & 52.63 \\
& \textbf{\textsc{SuperFilter}~\cite{li2024super}} & 13.54 & 58.51 & 72.89 & 79.30 & 11.35 & 39.50 & 65.25 & 48.62 \\
& \textbf{\textsc{Deita-Best}~\cite{liu2023deita}} & 10.50 & 58.17 & 73.14 & 74.60 & 16.60 & 62.00 & 72.26 & 52.47 \\
\cmidrule(lr){2-10}
& \ours (Ours) & 15.83 & 58.65 & 72.48 & 80.20 & 16.70 & 64.20 & 68.29 & \textbf{\underline{53.76}} \\
\bottomrule
\end{tabular}}
%\vspace{-0.1in}
\end{table*}



\textbf{Data Selection}: 
After training the module $\psi_{\text{div}}$, we can use its output to select data samples. Building on the theoretical insights in Sec.~\ref{sec:theory-insights}, data points that are closer to other centroids and more dispersed within their own centroid are more beneficial for enhancing the comprehensive capabilities of the model. Therefore, we use the predicted entropy-diversity score as a reward, selecting the top 20\% with the highest scores as the final data subset for fine-tuning.


\subsection{Empirical Validation and Main Results}

To validate the efficacy of \ours, we conduct experiments comparing more SOTA methods on the data pools in Sec.~\ref{sec:sec3-data-pool} with critical modifications: all domain-specific labels are deliberately stripped. This constraint mimics more challenging real-world scenarios and precludes direct comparison with data mixture methods requiring domain label prior. 

\paragraph{Baselines}
% We adopt the following data selection methods for comprehensive and competitive evaluation:
% (1) \textsc{Random Selection}: a conventional random sampling strategy; 
% (2) \textsc{Instruction Len}: quantifying instruction complexity through token count ~\cite{cao2023instruction, zhao2023preliminary}; 
% (3) \textsc{Alpagasus}~\cite{chen2024alpagasus}: leveraging ChatGPT for direct quality scoring of instruction pairs; 
% (4-5) \textsc{Instag}~\cite{lu2023instag}: semantic analysis based method, comprising \textsc{Instag-C} (complexity scoring via tag quantity) and \textsc{Instag-D} (diversity measurement through tag set expansion); 
% (6) \textsc{SuperFilter}~\cite{li2024super}: response-loss-based complexity estimation using compact models; 
% (7-9) \textsc{Deita}~\cite{liu2023deita}: model-driven evaluation method, comprising \textsc{Deita-C} (complexity scoring), \textsc{Deita-Q} (quality scoring), and \textsc{Deita-D} (diversity-aware selection).
% Detailed configurations of these baselines are documented in Appendix \ref{sec:appendix-baselines}.
We use the following data selection methods for comprehensive evaluation: 
(1) \textsc{Random Selection}: traditional random sampling; 
(2) \textsc{Instruction Len}: measuring instruction complexity by token count ~\cite{cao2023instruction}; 
(3) \textsc{Alpagasus}~\cite{chen2024alpagasus}: using ChatGPT for direct quality scoring of instruction pairs; 
(4-5) \textsc{Instag}~\cite{lu2023instag}: semantic analysis approach with \textsc{Instag-C} (complexity scoring via tag quantity) and \textsc{Instag-D} (diversity measurement through tag set expansion); 
(6) \textsc{SuperFilter}~\cite{li2024super}: response-loss-based complexity estimation using compact models; 
(7-9) \textsc{Deita}~\cite{liu2023deita}: model-driven evaluation with \textsc{Deita-C} (complexity scoring), \textsc{Deita-Q} (quality scoring), and \textsc{Deita-D} (diversity-aware selection). 
Detailed configurations for these baselines are in Appendix \ref{sec:appendix-baselines}.



\subsubsection{Overall Performance}

% Our experiments demonstrate the effectiveness of \ours across three major language models and seven challenging benchmarks. As shown in Table~\ref{tab:main-daar}. We adopt \textsc{Instag-Best} and \textsc{Deita-Best} to denote the optimal variants from their respective method families.
The experimental results are presented in Table~\ref{tab:main-daar}, where \textsc{Instag-Best} and \textsc{Deita-Best} represent the optimal variants from their respective method families. Our experiments clearly demonstrate the effectiveness of \ours across three major language models and seven challenging benchmarks. A detailed analysis is provided below.

\textbf{High-Difficulty Scenario}: The task of balanced capability enhancement proves particularly challenging for existing methods. While some baselines achieve strong performance on specific tasks (e.g., SuperFilter's 40.55 on HumanEval for Llama3.1), they suffer from catastrophic performance drops in other domains (e.g., SuperFilter's 6.05 on MATH). Only three baseline methods perform better than \textsc{Random} selection, with notably severe degradation applied in Qwen-series models, all of which fall below the \textsc{Random} performance. We hypothesize this stems from \textit{over-specialization} â€“ excessive focus on narrow capability peaks at the expense of broad competence (visualized in Appendix~\ref{sec:appendix-baseline-tsne}). Our method's robustness stems from preventing extreme distribution shifts through diversity constraints.

\textbf{Universal Superiority}: \ours establishes new SOTA averages across all models, surpassing the best baselines by \textbf{+0.14} (Llama3.1), \textbf{+0.97} (Qwen2), and \textbf{+1.11} (Qwen2.5). The proposed method uniquely achieves \textit{dual optimization} in critical capabilities: \textbf{Mathematical Reasoning}: Scores 38.1 MATH (Qwen2) and 16.70 MATH (Qwen2.5), with 7.4\% and 27.0\% higher than respective random baselines. 
\textbf{Coding Proficiency}: Maintains 64.94 HumanEval (Qwen2) and 64.20 MBPP (Qwen2.5) accuracy with \textless1\% degradation from peak performance. This demonstrates \ours's ability to enhance challenging STEM capabilities while preserving core competencies, a critical advancement over specialized but unstable baselines.

% \textbf{Computational and Storage Efficiency}:
\textbf{Cost-Efficiency and Flexibility}:
Compared to baseline methods requiring GPT-based evaluators (\textsc{Alpagasus}, \textsc{Instag}) or full LLaMA-7B inference (\textsc{Deita}), \ours achieves superior efficiency through data-model co-optimizations. Our method makes the LLM ability of self-rewarding from dedicated data synthesis, and operates on frozen embeddings from layer 3 (vs. full 32-layer inference in comparable methods), reducing computational overhead while maintaining capability integrity. 
The lightweight 5-layer MLP probe module requires only 9GB GPU memory during training (vs. 18$\sim$24GB for LLM-based evaluators) and adds merely 76M parameters. 
This plug-and-play feature enables seamless integration of \ours with existing LLMs without additional dependency management.
