\section{Conclusion}

This paper explicitly analyzes split conformal prediction through the lens of an MVS estimation problem and show that, in order to minimize the length of the prediction interval, the base predictor should minimize the $(1-\alpha)$-QAE. This motivates two new methods, \method~and \methodAD~, that are both empirically showed to be more robust than baselines over a significant spectrum of data-distributions. For \method, a detailed theoretical analysis highlights how the complexity of the prediction function classes impacts the prediction interval's length. It also reveals that the calibration step allows to provide an almost sure coverage guarantee, at the cost of slightly increasing the excess volume loss, with a term dominated by the statistical error due to the learning step.

In the future, it would be interesting to propose a computationally efficient algorithm for Problem~\eqref{eq:new-algo-adap} in Appendix~\ref{sec:adapt-bonus}. It would also be relevant to consider more complex classes of prediction sets, such as union of intervals, and to propose extensions of our framework to multivariate outputs and metric spaces. 

%as well as to further develop the optimization of the $(1-\alpha)$-QAE, by making links, for instance, with bilevel optimization. This could also be relevant in order to consider more complex classes of prediction sets or multivariate outputs. %\todo{On est ok avec ce future work un peu bidon?}

%\bat{Future work: Algo adap de l'appendice en pratique + Meilleur optim avec convergence clair+ Bilevel optim + non-uniform bound ? +CVAR + federated ? Other class of sets and MVS estimators ? Multivariate output}