% !TEX root = ../ICML.tex
\subsubsection{Optimization of the empirical $(1-\alpha)$-QAE}\label{sec:optim_emp_QAE}

We assume that $f\in\calF$ is parametrized by $\theta \in\Theta$, and for the sake of generality, we consider the problem: 
\begin{align} \label{eq:optim_quantile}
	%	&\hat{\theta} = \underset{\theta \in \Theta}{\argmin}  \; \widehat{Q}(1-\alpha;\{|Y_i-f_{\theta}(X_i)|\}_{i\in\calD^{lrn}}) \; . \\
	&\min_{\theta} \; \widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \; .
\end{align}
%
Here, $\ell:\Theta\times\calZ\rightarrow\IR$ is a loss function, taking as input a parameter $\theta$ and a data point $Z_i$. In the step 1 of \texttt{EffOrt}, $Z_i=(X_i,Y_i)$ and $\ell(\theta;Z_i)=|Y_i-f_\theta(X_i)|$.

To solve this problem, one natural idea is to use a gradient descent algorithm on $\widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}})$. However, this function is not differentiable in $\theta$. We therefore follow the strategy of \citet{pena2020solving} and consider a smooth approximation of it. More precisely, we first approximate the empirical cumulative distribution function (cdf) $\widehat{F}(t, \theta) := \sum_{i\in\calD^{lrn}} \1\{ \ell(\theta;Z_i) \leq t\}$ by another function $\widetilde{F}_{\varepsilon}$ where the indicator is replaced by a smooth version of it:
%
\begin{align*}
	\widetilde{F}_{\varepsilon}(t, \theta) = \sum_{i\in\calD^{lrn}} \Gamma_{\varepsilon}(\ell(\theta;Z_i) - t) \; ,
\end{align*}
%
where $\varepsilon > 0$ is a parameter of the approximation. One possible choice for $\Gamma_{\varepsilon}$ is given in \citet[Eq. (2.6)]{pena2020solving} and is detailed in Appendix \ref{sec:optim_append}. Then, we define the smooth empirical quantile function by:
%
\begin{equation}\label{eq:smooth_quant}
	\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}}) = \inf \{t \,:\, \widetilde{F}_{\varepsilon}(t, \theta) \geq q \} \; .
\end{equation}
%
For a given $q$ and $\varepsilon > 0$, under mild assumptions on the loss function $\ell(\cdot)$, one can show that the gradient of Eq.  \eqref{eq:smooth_quant} is well-defined and has a closed-form that can be used in a gradient descent algorithm. The full procedure is detailed in Appendix \ref{sec:optim_append}.

% We can therefore use a gradient descent algorithm to solve:
% %
% \begin{align} \label{eq:optim_smooth_quantile}
% 	&\min_{\theta} \; \widetilde{Q}_{\varepsilon}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \; .
% \end{align}
% %
% To this end, starting from an initial guess $\widetilde{\theta}_1$, we simply make the iterates:
% %
% \begin{align*}
% 	\widetilde{\theta}_{k+1} = \widetilde{\theta}_{k} - \gamma_k \nabla_{\theta} [\widetilde{Q}_{\varepsilon}(1-\alpha; (\ell(\widetilde{\theta}_k;Z_i))_{i\in\calD^{lrn}})] \; ,
% \end{align*}
% where $\gamma_k > 0$ is the stepsize and $\nabla_{\theta}$ denotes the gradient with respect to $\theta$. The full procedure is detailed in Appendix \ref{??}.