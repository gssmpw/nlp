\begin{figure}[t]
	\centering
	\includegraphics[width=.4\linewidth]{./img/size_normal_linear.pdf}
	\hspace{-.5em}\includegraphics[width=.4\linewidth]{./img/size_pareto_linear.pdf}
	
	\includegraphics[width=.4\linewidth]{./img/size_normal_adEffort.pdf}
	\hspace{-.5em}\includegraphics[width=.4\linewidth]{./img/size_pareto_adEffort.pdf}
	\vspace{-.4em}
	\caption{Boxplots of the $50$ empirical expected lengths obtained by evaluating \method~in Section \ref{sec:xpEffort} (top) and \methodAD~in Section \ref{sec:xpADEffort} (bottom). The white circle corresponds to the mean.}  
	\label{fig:illustr_synth}
\end{figure}

\section{Experiments}
\label{sec:xps}

In this section we compare our methods, \method~and \methodAD, to the standard and locally adaptive versions of split CP on synthetic data. Due to lack of space, additional results on real data are deferred to Appendix \ref{app:real-data}. Code to run all methods is in the Supplementary Material.

\subsection{Evaluation of \method}\label{sec:xpEffort}
%In this section, %we show empirically that \method~returns valid prediction sets of smaller size than those returned by the split CP method. 
%\pie{In particular, 
We first show the ability of~\method~to return valid prediction sets of smaller size than those returned by standard split CP methods. More precisely, we consider asymmetric and heavy-tailed distribution%(with potentially extreme values)
, illustrating the robustness of our method to a wide range of realistic situations. %\looseness = -1

%\paragraph{Data sets and metrics:} 
We consider a linear regression model $Y = X^T \theta + \calE$ where $\theta \sim \calU(0, 1)^{\otimes 3}$ is fixed, $X \sim \calN(0, I_3)$ with $I_3$ the identity matrix of size $3 \times 3$, and $\calE$ follows 4 different distributions: A standard normal, a mixture distribution $0.95 \cdot \calN(0, 1) + 0.05 \cdot \calN(2, 1)$, a Pareto distribution with shape and scale parameters equal to $2$ and $1$, and another mixture equals to $0.95 \cdot \text{Pareto}(2, 1) + 0.05 \cdot \calN(-20, 1)$. In the two mixtures, the additional normal distributions allow simulating extreme values. For each scenario, we generate $n_{lrn}=n_{cal}=1000$ pairs $(X_i, Y_i)$, as well as $n_{test}=1000$ test points to compute the empirical marginal coverage and the average size of the returned set. We repeat this procedure $50$ times.


%\paragraph{Learning and calibration steps:} 
During the learning step of \method, we solve the $(1-\alpha)$-QAE Problem \eqref{eq:QAE} using the gradient descent strategy of Section \ref{sec:optim_emp_QAE}. The smoothing parameter $\varepsilon$ is set to $0.1$, $n_{iter}=1000$, and the step-size sequence is $\{(1/t)^{0.6}\}^{n_{iter}}_{t=1}$. Furthermore, the space of research $\calF$ is restricted to the space of linear functions (see Appendix \ref{sec:add_xp} for additional results with Neural-Networks (NN)). For the split CP method, the regression function is either estimated using a linear regression or, in order to be fair in our comparisons, using a robust linear regression with Huber loss with parameter $\delta=1.35$. For all methods, the score function is the absolute value of the residuals, i.e., $s(x, y) = \lvert y - \fh(x) \rvert$ and we set $\alpha = 0.1$.

\textbf{Results:}
Figure \ref{fig:illustr_synth} (top) displays the boxplots of the $50$ test lengths obtained in the 4 scenarios (the coverage can be found in Appendix \ref{sec:add_xp} and is, as expected, near $0.9$). Overall, \method~produces more efficient marginally valid sets than those obtained with the split CP method, in all scenarios. Interestingly, when the noise follows a normal distribution (Figure \ref{fig:illustr_synth} - top left panel), \method~and the split CP method with a standard or a robust linear regressor return similar sets. This was expected because with this type of distribution, the least-square regressor is supposed to be as good as the minimizer of the QAE. This is as opposed to the mixture of Gaussians, where extreme points brings asymmetry and makes the linear least square regression not suitable anymore. When the noise follows a Pareto distribution (Figure \ref{fig:illustr_synth} top right panel), its heavy tail also makes the Split CP with robust regression output larger prediction sets. This could be explained by the fact that, in the learning step, the robust regression somehow gets rid of extreme points that should be kept, enforcing the calibration step to make a larger correction. In the last scenario, both baselines are outperformed by \method.

\subsection{Evaluation of \methodAD} \label{sec:xpADEffort}
We now compare \methodAD~to the Locally Weighted CP (LW-CP) and CQR methods (see Example \ref{exemple:base-predictor}).
%
%\paragraph{Data sets and metrics:} 
We consider a simple heteroscedastic linear regression model $Y = X + \calE(X)$ where $X \sim \calN(0, 1)$ and $\calE(x)$ follows the 4 distributions of the previous section and with variance multiply by $x^2$. %We generate $n_{lrn}=n_{cal}=1000$ pairs $(X_i, Y_i)$ as well as $n_{test}=1000$ test points to compute the empirical marginal coverage and the average size of the returned set. We repeat this procedure $50$ times for each of the 4 distributions.
During the learning step of \methodAD, we solve the $(1-\alpha)$-QAE Problem \eqref{eq:QAE} using the gradient descent strategy of Section \ref{sec:optim_emp_QAE}. The space of research $\calF$ is restricted to the space of linear functions.  We then learn $\hat{s}(\cdot)$ (second step of \methodAD) using a Random Forest (RF) quantile regressor. For the LW-CP method, the regression function is estimated using a linear regression and $\hat{\sigma}(\cdot)$ using a RF. Finally, for CQR, we also use a RF quantile regression. We set $\alpha = 0.1$. More details on the experimental setup are available in Appendix \ref{sec:add_xp_synth}.

\textbf{Results:} Figure \ref{fig:illustr_synth} (bottom) displays the boxplots of the length for the 3 methods. The coverage can be found in Appendix \ref{sec:add_xp} and are near $0.9$. Furthermore, an illustration of the returned sets is given in Figure \ref{fig:illustr_synth_adEffort_example} of Appendix \ref{sec:add_xp}. We see that \methodAD~returns valid marginal sets with length, on average, smaller or similar that the two other methods. Furthermore, the size of the boxplots are much smaller for our method than for the others. This means that \methodAD~returns sets with more consistent sizes. Finally, we would like to point out that, although CQR gives similar results to our method in some situations (e.g., with the Pareto distribution), it has the drawback to not assess the uncertainty of a particular prediction model $\fh$.% to make good predictions. % since. Indeed, the final set may not even contain $\fh(X_{new})$, which is clearly a problem.\todo{un peu venere ca non ?}

%\begin{figure*}[h!]
%	\centering
%	\includegraphics[width=.48\linewidth, height=.18\textheight]{./img/size_normal_adEffort.pdf}
%	\includegraphics[width=.48\linewidth, height=.18\textheight]{./img/size_pareto_adEffort.pdf}
%	%\includegraphics[width=.48\linewidth]{./img/coverage_normal_adEffort.pdf}
%	%\includegraphics[width=.48\linewidth]{./img/coverage_pareto_adEffort.pdf}
%	\caption{Synthetic data:} 
%	\label{fig:illustr_synth_adEffort}
%\end{figure*}