\section{Extension to intervals with adaptive size}
\label{sec:adaptive}

We now consider the case of prediction intervals whose size adapts to the value of $X$. Formally, we consider the class of prediction sets $\calC^{\text{adap}}_{\calF,\calS} = \{C_{f,s}(x) = [f(x)-s(x), f(x)+s(x)] : f \in \calF, s\in\calS\}$, where $\calS$ is a class of non-negative functions. Importantly, this class of prediction sets encapsulates the Locally-Weighted Conformal Inference and the CQR methods (see Examples~\ref{exemple:base-predictor}.2 and~\ref{exemple:base-predictor}.3).

\subsection{Oracle prediction set and conditioning over $X=x$}

\label{sec:adap-oracle}

Following a similar reasoning as in Section~\ref{sec:constant}, we could first consider $f$ fixed and derive a closed-form oracle expression for $s$ by solving Problem~\eqref{eq:formal-opt} with $\calC_{\text{Borel}}$ replaced by $\calC^{\text{adap}}_{\calF,\calS}$. %This would give the problem $\min_{s\in\calS}\{\EE[s(X)] \text{ s.t. } \IP(|Y-f(X)|\leq s(X)) \geq 1-\alpha\}$. 
Unfortunately, contrary to the previous section, the solution of this problem does not have a direct expression.

For this reason, we propose to modify the problem so that $s$ admits an oracle closed-form expression which can be naturally estimated empirically. More precisely,
% instead of solving Problem~\eqref{eq:formal-opt} over $\calC^{\text{adap}}_{\calF,\calS}$,
we condition the optimization problem over the event $X=x$, where $x\in\calX$. In that case, the problem becomes:
\begin{equation}
    \label{eq:obj-adap}
    \min_{s \in \calS}  s(x) \hspace{0.1cm} \text{s.t.} \hspace{0.1cm}  \IP(|Y-f(x)|\leq s(x)|X=x) \geq 1-\alpha 
\end{equation}
This problem is more difficult than \eqref{eq:QAE} as a \emph{conditional} coverage constraint is now required, which is known to be harder to obtain in practice \citep{vovk2012conditional, lei2014distribution}. If $\calS$ is sufficiently complex, Problem~\eqref{eq:obj-adap} has an oracle close-form solution, which is given by the $(1-\alpha)$-quantile of $|Y-f(X)|$ conditioned on $X=x$, denoted by $s^*(x) := Q(1-\alpha;|Y-f(X)|_{|X=x})$. Interestingly, the function $s^*(x)$ is the quantile regression function of $|Y-f(X)|$ given $X=x$, and corresponds to the solution of $\min_{s\in\calS}\EE[\rho_{1-\alpha}(|Y-f(X)| - s(X))]$, where $\rho_{1-\alpha}$ is the pinball loss. Hence, a natural solution is to use an empirical plug-in estimator of $s^*$, i.e. minimizing an empirical version of the pinball risk, as suggested in the next section.

\begin{remark}
    Another strategy could be to directly solve an empirical version of $\min_{f\in\calF,s\in\calS}\EE[s(X)]$ s.t.  $\IP(|Y-f(X)|\leq s(X)) \geq 1-\alpha$. This would allows deriving results similar to those of the previous section (see Appendix~\ref{sec:adapt-bonus}), but solving it in practice can be challenging, notably because of the empirical coverage constraint.
    Notice that, although their objective is different from ours, \citet{baiefficient} face a similar optimization problem, where they propose a smooth and differentiable relaxation to solve it. \looseness=-1%, using the Lagrangian function and a surrogate loss to approximate the constraint. %However, they do not provide theoretical guarantees regarding their approach.    
\end{remark}

%On the other side, the second solution, namely rewriting the problem in order to have a closed-form expression for $t$, can be more interesting in terms of interpretability. 


\subsection{\texttt{Ad-EffOrt}}

We now describe our second method,~\methodAD, which extends \method~to prediction intervals with adaptive size. Like in~\method, we consider the split CP framework, having access to a learning dataset $\calD^{lrn}$ used to learn the base predictors $f$ and $s$, and a calibration data set $\calD^{cal}$. %$\calD^{cal}$ used to conformalize the learned prediction set $C_{f,s}(x)$ by increasing (or decreasing) its size to satisfy the coverage guarantee.
\methodAD~consists in the following steps:%\todo{pas tellement specifique a Effort finalement. C'est un nouveau score l'idée ici. Ce que je voulais dire ici c'est que le step 2 de \methodAD peut aussi bien etre fait avec le splitCP standard (ou meme avec un $f$ donné).}
\begin{enumerate}[leftmargin=*]
    \item $\hat{f} \in \underset{f\in\calF}{\argmin}  \; \widehat{Q}(1-\alpha;\{|Y_i-f(X_i)|\}_{i\in\calD^{lrn}})$
    \item $\hat{s} \in \underset{s\in\calS}{\argmin}\frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\rho_{1-\alpha}(|Y_i-\hat{f}(X_i)| - s(X_i))$
    \item $\hat{t} = \widehat{Q}\Big((1-\alpha)\frac{n_c+1}{n_c};\{|Y_i-\hat{f}(X_i)| - \hat{s}(X_i)\}_{i\in\calD^{cal}}\Big)$
    \item For any test point $X\in\calX$, output $C_{\hat{f}, \hat{s},\hat{t}}^{1-\alpha}(X) = [\hat{f}(X)-\hat{s}(X)-\hat{t},\hat{f}(X)+\hat{s}(X) +\hat{t}] \; .$
\end{enumerate}
\vspace{-.4em}
%
In the first two steps of \methodAD, we learn the model $f$ as in \method~and then fit the residuals using a quantile regression or order $1-\alpha$. %As usual in machine learning, regularization terms can be added on top of the empirical error terms. 
Note that, in those two steps, the same data are used to learn both the prediction model $f$ and the quantile regressor $s$, but we also might split the learning set in two. %If we were to seek theoretical guarantees, this would help to provide some consistency results. 
Then, in the third step (calibration), we take the quantile of $\{|Y_i-\hat{f}(X_i)| - \hat{s}(X_i)\}_{i\in\calD^{cal}}$. %\todo{on a pas cité l'autre du coup.} %, which differs from \method. 
This comes from the fact that the final prediction interval is in the form $[f(x)-s(x)-t,f(x)+s(x)+t]$, and, given the base predictors $(f, s)$, the smallest $t$ such that we satisfy the coverage is $Q\big((1-\alpha);|Y-f(X)| - s(X)\big)$. This claim is easily proved by following the analysis of Section~\ref{sec:f-given}. 

The main limitation of \methodAD~is the difficulty of providing a theoretical guarantee similar to that of Theorem~\ref{thme:main-constant}. This is notably due to the fact that while $s$ in learned in order to obtain conditional guarantees, $f$ is learned as in \method, i.e.~in order to obtain marginal guarantees. When $f$ is fixed, one could actually derive guarantees on $\hat{s}$ and its ability to solve~\eqref{eq:obj-adap} by providing a setting under which the quantile regressor is consistent, making~\eqref{eq:obj-adap} asymptotically verified. Last but not least, it is worth mentioning that, thanks to the calibration step, the marginal coverage guarantee is verified.%\todo{J'ai l'impression que cette derniere phrase est quand meme importante mais je pas ou la mettre mieux}

%\pie{using a strategy similar to the one of \citep{han2022split}}\todo{A placer après sans dire qu'on a suivit leur "strategy". Jsp ou le mettre... Peut-être plus haut?}

%\bat{Ajouter ici, ou dans les xps, une comapraison avec CQR et locally weighted}
