\section{Restriction to intervals with constant size}
\label{sec:constant}

In this section, we restrict the space of research in Problem \eqref{eq:formal-opt} to the class of prediction sets $\calC^{\text{const}}_{\calF} = \{C_{f,t}(\cdot) = [f(\cdot)-t, f(\cdot)+t]; f \in \calF, t\geq0\}$. This class is already quite interesting as it encapsulates the standard split CP regressor (see Example~\ref{exemple:base-predictor}.1). Notice that for simplicity of exposition, and because it does not depend on $x$, in this section the expected size of $C_{f,t}\in\calC^{\text{const}}_{\calF}$ is simply denoted $\lambda(C_{f,t})=2t$. 

\subsection{Base predictor $f\in\calF$ is given: optimality of the conformal step}
\label{sec:f-given}

We first start in the setting where the base predictor $f$ is given, meaning that we do not consider the learning phase. Over $\calC^{\text{const}}_{\calF}$, the optimization problem~\eqref{eq:formal-opt} becomes:
%
% \begin{align}
%     \min_{t \geq 0} \quad &\; 2t  \\
%      \quad \text{s.t.} \quad & \IP(|Y-f(X)|\leq t) \geq 1-\alpha\;. \nonumber
% \end{align}
\begin{equation}
    \label{eq:obj-constant}
    \min_{t \geq 0} \hspace{0.2cm}  2t  \hspace{0.2cm}\text{s.t.}\hspace{0.2cm}  \IP(|Y-f(X)|\leq t) \geq 1-\alpha\hspace{0.05cm}.
\end{equation}
%
Denoting by $S=|Y-f(X)|$ the random variable of the absolute residual, the solution of the above optimization corresponds to the quantile of order $1-\alpha$ of the random variable $S$. More formally, if we denote by $Q(\hspace{.2em} \cdot \hspace{.2em}  ; S):[0,1]\rightarrow \IR$ the quantile function of $S$, then the optimal value solving~\eqref{eq:obj-constant} is exactly $t^*=Q(1-\alpha; S)$ and the associated optimal set is %$$C^{1-\alpha}_{f,t^*} = \{y \in \calY : |y-f(x)| \leq t^*\} \; .$$
$C^{1-\alpha}_{f,t^*}(x) = [f(x) - t^*, f(x) + t^*] \; .$

Importantly, notice that the conformal step of the original split CP in fact solves an empirical version of the previous problem, but with a slightly increased coverage: %Indeed, by replacing the probability constraint with its empirical counterpart \pie{(with an inflated coverage) - bof}, we obtain:
%
\begin{align}
    \min_{t \geq 0} \quad &\; 2t \label{eq:obj-constant-emp} \\
     \quad \text{s.t.} \quad & \frac{1}{n_c}\sum_{i=1}^{n_c}\1\{|Y_i-f(X_i)|\leq t\} \geq \frac{(1-\alpha)(n_c+1)}{n_c}\; , \nonumber
\end{align}
%
with solution $\hat{t} = S_{(\lceil (n_c+1)(1-\alpha) \rceil)}$ and associated set denoted %\todo{Attention car ici on a dans la notation $\alpha$ mais en fait c'est le "inflated"} %$$C^{1-\alpha}_{f,\hat{t}} = \{y \in \calY : |y-f(x)| \leq \widehat{t}\} \; .$$
$C^{1-\alpha}_{f,\hat{t}}(x) = [f(x) - \hat{t}, f(x) + \hat{t}] \; .$
%
As mentioned above, $\hat{t}$ is the quantity computed during the calibration step of the split CP method (see Section~\ref{sec:conform-background}). It corresponds to the empirical quantile function of $S$, defined by $\widehat{Q}(\hspace{.1em} q \hspace{.1em};\{S_i\}_{i=1}^{n_c}):= \inf\{t : \frac{1}{n_c}\sum_{i=1}^{n_c}\1\{S_i\leq t\}\geq q\}$, evaluated at $(1-\alpha)(n_c+1)/n_c$ instead of $1-\alpha$ to be slightly more conservative. In other words, this means that, when $f$ is given, the calibration step in split CP outputs a conservative empirical estimator of the oracle prediction interval solution of Problem \eqref{eq:obj-constant}.

%Let us denote by $C^{1-\alpha}_{f,t^*}$ (respectively $C^{1-\alpha}_{f,\hat{t}}$) the oracle interval (respectively the estimated interval). 
From the theory of CP, we already know that $\IP(Y\in C^{1-\alpha}_{f,\hat{t}}(X))\geq 1-\alpha$ (see e.g. \citet[Theorem 2.2]{lei2018distribution}). It remains to study the excess volume loss of $C^{1-\alpha}_{f,\hat{t}}$ which is measured by the difference in length between $C^{1-\alpha}_{f,t^*}$ and $C^{1-\alpha}_{f,\hat{t}}$. To this aim, it is sufficient to study the difference between the empirical quantile $\hat{t} = \widehat{Q}((1-\alpha)\frac{n_c+1}{n_c};\{S_i\}_{i=1}^{n_c})$ and the true quantile $t^* = Q(1-\alpha; S)$,  %The following result provides a distribution-free upper bound on the length of $C^{1-\alpha}_{f,\hat{t}}$.
as done in the following proposition (proof in Appendix~\ref{app:proof-prop}).\\

\begin{proposition}
    \label{prop:upper-f-given}
    Let $\hat{t} = \widehat{Q}((1-\alpha)\frac{n_c+1}{n_c};\{S_i\}_{i=1}^{n_c})$ and $C^{1-\alpha}_{f,\hat{t}}$ the corresponding set. If the points in $\calD^{cal}$ are i.i.d., and if $(n_c+1)(1-\alpha)$ is not an integer, then with probability greater than $1-\delta$ we have:
    \begin{equation}
        \label{eq:upper-f-given}
        \lambda\Big(C^{1-\alpha}_{f,\hat{t}}\Big) \leq 2Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\;.
    \end{equation}
\end{proposition}
% \begin{proof}
% 	The proof is given in Appendix \ref{proof_prop:upper-f-given}.
% \end{proof}

Interestingly, the right-hand side of~\eqref{eq:upper-f-given} also corresponds to the optimal length of a more conservative oracle, namely $\lambda\Big(C^{1-\alpha+\beta_{n_c}}_{f,t^*}\Big)$ with $\beta_{n_c}=\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}$. This means that, with high probability, the empirical interval obtained with the conformal step is smaller than the smallest oracle interval with increased coverage $1-\alpha+\beta_{n_c}$, and where $\beta_{n_c}$ is tending to $0$ as $n_c$ grows.

Although interesting, the previous result does not really tell us how different is the size of the predicted interval compared with the oracle one. 
To obtain a finite-sample upper bound on this difference, we must consider some regularity assumption on the distribution of $S$, and more particularly on its quantile function.
%
\begin{assumption}\emph{(Regularity condition).}
    \label{ass:regularity}
    Let $S=|Y-f(X)|$. $\forall f\in \calF$, $\forall \alpha \in (0,1), \exists r,\gamma \in (0,1]$ and $L>0$ such that $Q(\vspace{.2em}\cdot\vspace{.2em};S)$ is locally $(\gamma,L)$-Hölder continuous, i.e. $\forall q_1,q_2 \in [1-\alpha - r, 1-\alpha + r]$: $$|Q(q_1;S) - Q(q_2;S)|\leq L|q_1-q_2|^\gamma \; .$$
%     Either one of the two following conditions is true. \bat{Bat: Il faudra en choisir une et mentionner l'autre dans le texte. La première me semble plus general au sens où la deuxieme implique la première.}
%
% \begin{enumerate}
%     \item $\forall f\in \calF$, $\forall \alpha \in (0,1), \exists r,\gamma \in (0,1]$ and $L>0$ such that $Q(\cdot;S)$ is locally $(\gamma,L)$-Hölder continuous, i.e. $\forall q_1,q_2 \in [1-\alpha, 1-\alpha + r]$, $|Q(q_1;S) - Q(q_2;S)|\leq L|q_1-q_2|^\gamma$.
%     \item The random variable $S$ admits a density $p_S$ with respect to $\lambda$. Moreover, $\exists r'\in(0,1)$ and $L'>0$ such that $\forall q\in [1-\alpha, 1-\alpha + r']$, $p_S(Q(q;S))\geq L'$.
% \end{enumerate} 
\end{assumption}
%
This type of regularity condition can notably be found in \citet{lei2013distribution,yang2024selection}, where it is used to obtain finite-sample bounds on the volume of the returned set. Given this assumption, we can derive the following corollary.

\begin{corollary}
    \label{cor:f-given}
    Let the conditions of Proposition~\ref{prop:upper-f-given} and Assumption~\ref{ass:regularity} hold. If $n_c$ is large enough so that $\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}} \leq r$, then with probability greater than $1-\delta$:
    \begin{equation}
        \label{eq:upper-f-given-final}
        \lambda\Big(C^{1-\alpha}_{f,\hat{t}}\Big) \leq \lambda\Big(C^{1-\alpha}_{f,t^*}\Big) + 2 L\Big(\frac{1}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}\Big)^\gamma\;.
    \end{equation} 
    %Moreover, we have the coverage guarantee $\IP(Y\in C^{1-\alpha}_{f,\hat{t}})\geq 1-\alpha$. \pie{nescessaire ca ? on l'a deja dit.} %\bat{Under the second assumption, take $\gamma=1$, replace $r$ by $r'$ and $L$ by $1/L'$.}
\end{corollary}
\begin{proof}
    Direct application of Prop~\ref{prop:upper-f-given} with Assumption~\ref{ass:regularity} and using the fact that $1-\alpha\leq1$. %The coverage guarantee is directly obtained as a consequence of split CP (see e.g.  \citet[Theorem 2.2]{lei2018distribution}). %\bat{For the second assumption, we first apply the mean value theorem.}
\end{proof}

The previous corollary provides an excess volume upper-bound for $C^{1-\alpha}_{f,\hat{t}}$ compared to the oracle $C^{1-\alpha}_{f,t^*}$. This bound does not only confirm the asymptotic optimality of the conformal procedure when $f$ is given, but also provides a rate of convergence dominated by $\tilde{\calO}(n_c^{-\gamma})$ when we get rid of constants and log factors. Although simple to be obtained, to our knowledge this type of bound has never been shown.%is the first of its kind.

\begin{remark}
    \label{rmk:nested}
    When the base predictor is given, all the previous study can be easily extended to the general CP nested set view of \citet{gupta2022nested}. For simplicity of exposition, this analysis is deferred to Appendix~\ref{sec:nested}
\end{remark}


\subsection{Base predictor $f\in\calF$ is \emph{not} given: sub-optimality of the least-square regressor}

In the previous section we saw that, when $f$ is fixed, the calibration step of the split CP method corresponds to the minimization of the size of the interval, up to some statistical error. Now, we investigate how $f$ should be learned during the learning step to obtain a prediction interval of minimal size. Let us consider Problem~\eqref{eq:formal-opt} over $\calC^{\text{const}}_{\calF}$:
% \begin{align*}
%     \min_{f\in\calF, t \geq 0} &\; 2t \\
%      \quad s.t. \quad & \Big\{\IP(|Y-f(X)|\leq t) \geq 1-\alpha \Longleftrightarrow
% \end{align*}
%
%\noindent
%\begin{minipage}{0.45\textwidth}
%\[
%\begin{aligned}
%    \min_{f\in\calF, t \geq 0} &\; \quad 2t \\
%     \quad \text{s.t.} \quad & \IP(|Y-f(X)|\leq t) \geq 1-\alpha 
%\end{aligned}
%\] \vspace{.3em}
%\end{minipage}
%\hfill
%\(\iff\)
%\hfill
%\begin{minipage}{0.45\textwidth}
%\[
%\begin{aligned}
%    \min_{f\in\calF} &\; 2Q(1-\alpha ; |Y-f(X)|)\;, 
%\end{aligned}
%\]
%\end{minipage}
%
%where the bottom optimization problem is directly obtained by replacing $t$ by its optimal value as a function of $f$, i.e. $t^*=Q(1-\alpha ; |Y-f(X)|)$. In words, this optimization problem tells us that $f$ should minimize the $(1-\alpha)$ quantile of the scores $S = |Y-f(X)|$, referred to as the $(1-\alpha)$-QAE (Quantile Absolute Error) in the following. This is quite natural, since this quantile is the one selected to build the prediction interval, and the smaller it is, the smaller the interval will be.
%
\begin{equation}
\min_{f\in\calF, t \geq 0} \hspace{0.2cm}  2t  \hspace{0.2cm}\text{s.t.}\hspace{0.2cm}  \IP(|Y-f(X)|\leq t) \geq 1-\alpha\hspace{0.05cm}.
\end{equation}
By replacing $t$ with its optimal value as a function of $f$, i.e. $t^*=Q(1-\alpha ; |Y-f(X)|)$, we obtain what we call the $(1-\alpha)$-QAE problem (Quantile Absolute Error):
%
\begin{align}\label{eq:QAE}
\min_{f\in\calF} &\; Q(1-\alpha ; |Y-f(X)|)\;.
\end{align}
%
In words, this optimization problem tells us that $f$ should minimize the $(1-\alpha)$-quantile of the distribution of $S = |Y-f(X)|$. This is quite natural, since this quantile is the one selected to build the prediction interval, and the smaller it is, the smaller the interval will be.

What this optimization problem also tells us is that taking $f$ as the minimizer of the Mean Squared Error (MSE) $\EE[(Y-f(X))^2]$, denoted $\mu(x) = \EE[Y|X=x]$, like it is suggested in classical split CP, is not generally optimal in terms of volume minimization, and one should rather take the minimizer of the $(1-\alpha)$-QAE. Notice that, while in general the minimizer of the MSE does not match the one of the $(1-\alpha)$-QAE,
%A possible intuition behind that is that the mean is very sensitive to heavy-tailed distributions and extreme values,\todo{Finalement j'aime pas ce truc...} which could damage the base model, significantly increase the values of the scores, and thus the size of the prediction interval. However, %while in all generality the minimizer of the MSE is not optimal and one should rather take the minimizer of the $(1-\alpha)$-QAE, 
it does in some settings. For instance, in \citet[Section 3]{lei2018distribution}, the authors claim %\todo{je mettrais la ref plutot en fin et la phrase en affirmatif.} 
that if the residual distribution $Y-\mu(X)$ is independent of $X$ and admits a symmetric density with one mode at $0$, then taking $f=\mu$ is optimal, i.e. the minimizer of the MSE matches the minimizer of the $(1-\alpha)$-QAE. However, this kind of assumptions can be quite strong in practice, reason why it is preferable to keep the minimization of the $(1-\alpha)$-QAE as the main objective, since it is optimal on $\calC^{\text{const}}_{\calF}$ no matter the distribution of $(X,Y)$. \looseness = -1

% \begin{remark}
%     The minimizer of the $(1-\alpha)$-QAE should not be confused with the $(1-\alpha)$-quantile regressor of $Y|X=x$. One way to illustrate that is to recall that the first one can be the same as the MSE in some settings. \bat{supprimer si besoin}
% \end{remark}

\subsection{\texttt{EffOrt}: EFFiciency-ORienTed split conformal regression}
In this section, we propose a methodology to approach the oracle prediction set $C^{1-\alpha}_{f^*,t^*}(x) = [f^*(x) - t^*, f^*(x) + t^*]$, with $f^*$ the minimizer of the $(1-\alpha)$-QAE (Problem \eqref{eq:QAE}) and $t^* = Q(1-\alpha ; |Y-f^*(X)|)$. We place ourselves in the split conformal framework of Section~\ref{sec:conform-background}, having access to a learning data set $\calD^{lrn}$ used to learn $f$, and a calibration data set $\calD^{cal}$. With a slight abuse of notation we will write $i\in\calD^{lrn}$ or $\calD^{cal}$ to indicate $(X_i,Y_i)\in\calD^{lrn}$or $\calD^{cal}$.


The proposed methodology, referred to as \texttt{EffOrt}, consists in the following steps:
\begin{enumerate}
    \item Learn $\hat{f} \in \underset{f\in\calF}{\argmin}  \; \widehat{Q}(1-\alpha;\{|Y_i-f(X_i)|\}_{i\in\calD^{lrn}})$, i.e. minimize the empirical version of the $(1-\alpha)$-QAE
    \item Proceed to the calibration step, i.e. take $\hat{t} = \widehat{Q}\Big((1-\alpha)\frac{n_c+1}{n_c};\{|Y_i-\hat{f}(X_i)|\}_{i\in\calD^{cal}}\Big)$
    \item For any test point $X\in\calX$, output the prediction interval $C_{\hat{f},\hat{t}}^{1-\alpha}(X) = [\hat{f}(X)-\hat{t},\hat{f}(X)+\hat{t}]$
\end{enumerate}
%
In \texttt{EffOrt}, the main difficulty is in the first step, where the empirical $(1-\alpha)$-QAE must be minimized. Indeed, it does not have a closed-form solution, and if we want to use a gradient-based optimization algorithm, we must compute the gradient of the empirical $(1-\alpha)$-QAE which not trivial, or might even not be clearly defined. In the following, we present a gradient-based optimization procedure inspired by \citet{pena2020solving}.


%
\input{subfiles/optimization.tex}


\subsection{Theoretical analysis}

In this last subsection, we theoretically analyze the performance of the prediction set output by \texttt{EffOrt}. We are interested in two types of guarantees: (i) a coverage guarantee and (ii) an excess volume loss guarantee like the one in Eq.~\eqref{eq:upper-f-given-final}. To this aim, we require the following assumption.

\begin{assumption} \label{ass:complexity} There exists $\phi(\calF,\delta,n)<+\infty$ such that with probability at least $1-\delta$:
    %\begin{equation}
    %    \IP\left(\sup_{t\geq 0, f\in\calF}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}\Big|\leq \phi(\calF,\delta,n)\right)\geq 1-\delta
    %\end{equation}
   \begin{align*}
       &\sup_{\overset{\scriptstyle t\geq 0}{f\in\calF}}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}\Big| \leq \phi(\calF,\delta,n) \; .
       %
   \end{align*}
\end{assumption}
In this assumption, $\phiF$ bounds the worst-case estimation error of $\IP(|Y-f(X)|\leq t)$ using the empirical estimate $\frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}$ over the whole function class $\calF$ and for any value of $t$. Typically, $\phiF$ will decrease with an increasing number of data points $n$ and increase as the \emph{complexity} of $\calF$ gets larger. In the following proposition, we explicitly derive a closed-form expression for $\phiF$ when the function class $\calF$ is finite. 
%
\begin{proposition}\emph{(Finite class $\calF$).}
    \label{prop:phi-finite}
    If $|\calF|<\infty$, then Assumption~\ref{ass:complexity} is verified with $\phiF = \sqrt{\frac{\log(2|\calF|/\delta)}{2n}}$.
\end{proposition}
%
% \begin{proof}
% 	The proof is given in Appendix \ref{..}.
% \end{proof}
Similarly to the “classical” statistical learning framework, where it is possible to obtain generalization bounds for infinite hypothesis classes, it is possible to derive other closed-forms for $\phiF$ in the infinite case by involving complexity measures like VC dimensions or Rademacher complexities. This, along with the proof of Prop.~\ref{prop:phi-finite}, is discussed in Appendix \ref{sec:closed-form-phi}. We can now present our main theoretical result.

\begin{theorem}
    \label{thme:main-constant}
    Let $C_{\hat{f},\hat{t}}^{1-\alpha}(x)$ %= [\hat{f}(x)-\hat{t},\hat{f}(x)+\hat{t}]$ 
    be the prediction interval output by \method~. If Assumption~\ref{ass:regularity} and~\ref{ass:complexity} are satisfied, the distribution of $Y$ is atomless, $n_c$ and $n_\ell$ are large enough so that $\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}} \leq r$ and $\phiFl\leq r$, then:
    \vspace{-0.2cm}
    \begin{enumerate}[leftmargin=*]
        \item $\IP(Y\in C_{\hat{f},\hat{t}}^{1-\alpha}(X)|\calD^{lrn})\geq 1-\alpha$ a.s. %\todo{pas besoin de toutes les assumptions ici}
        \item With probability greater that $1-2\delta$:
        \begin{align}
            \label{eq:thme-const}
            &\lambda\left(C_{\hat{f},\hat{t}}^{1-\alpha}\right)\leq \lambda\left(C_{f^*,t^*}^{1-\alpha}\right) + 2L\Big(\frac{1}{n_{c}} +\sqrt{\frac{\log(2/\delta)}{2n_{c}}}\Big)^{\gamma} + 4L\phiFl^\gamma %\nonumber
        \end{align}
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof sketch - Details in Appendix~\ref{app:proof-main}] The first result is classical \citep{lei2018distribution}. Let us focus on the second one, proved with the following steps.
    

\underline{Step 1:} In the first step of \method, we are actually solving the empirical objective $\min_{f\in\calF, t\geq 0}~ \{t$ s.t. $n_l^{-1}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f(X_i)|\leq t\}\geq 1-\alpha\}$, with solutions denoted by $\hat{f}$ and $\hat{t}_{lrn}$. Using the theory of MVS estimation \citep{NIPS2005_d3d80b65}, we can compare this solution to the oracle one. Indeed, by adapting the proof of \citet[Theorem 1]{NIPS2005_d3d80b65}, we can show that with probability greater than $1-\delta$:
\begin{equation}
    \label{eq:lower-scott}
    \IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})\geq 1-\alpha - \phiFl
\end{equation}
and
\begin{equation}
    \label{eq:lower-scott-bis}
    \lambda\left(C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}\right) \leq \lambda\left(C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}\right) \;,
\end{equation}
where $\phi \equiv \phiFl$ and $C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}$ denotes the optimal oracle interval with coverage increased by $\phiFl$. This tells us that after the learning step we already have some guarantees: (i) a high probability coverage guarantee, with a looser coverage decreased by $\phiFl$, (ii) an excess volume guarantee, ensuring that the volume of the learned interval is smaller than the optimal one with coverage increased by $\phi$. Interestingly, this also means that the conformal step  allows to obtain an almost sure coverage guarantee, and to get rid of the statistical error due to $\phiFl$ in the coverage. %\todo{Mettre ce dernier commentaire en dehors du sketch pour plus d'impact?}

\underline{Step 2:}  %\todo{Il faudrait des "with high probability" un peu partout mais baleck ?}
% Since the distribution of $Y$ is atomless, with probability greater than $1-\delta$ we have  \citep{humbert2024marginal}:
% \begin{equation} 
%     \label{eq:upper-classic}
%     \IP( |Y-\hat{f}(X)| \leq \hat{t} \big|\calD)\leq 1-\alpha + \frac{1}{n_c+1}+\sqrt{\frac{\log(1/\delta)}{n_{c}+1}}\;.
% \end{equation}
From~\eqref{eq:lower-scott-bis} we have $\hat{t}_{lrn} \leq t_{1-\alpha+\phi}^*$ and therefore $\hat{t} \leq t^* + \hat{t} - \hat{t}_{lrn} + t_{1-\alpha+\phi}^* - t^*$. 

With~\eqref{eq:upper-f-given} in Prop.~\ref{prop:upper-f-given}, we have $\hat{t} \leq Q(1-\alpha + \frac{1-\alpha}{n_c}+\sqrt{\frac{\log(2/\delta)}{2n_{c}}}; |Y-\hat{f}(X)|_{|\calD^{lrn}})$. Moreover, from~\eqref{eq:lower-scott}, $\hat{t}_{lrn}\geq Q(1-\alpha - \phiFl; |Y-\hat{f}(X)|_{|\calD^{lrn}})$. Hence, thanks to Assumption~\ref{ass:regularity}, $\hat{t} - \hat{t}_{lrn} \leq L\Big(\frac{1}{n_{c}} +\sqrt{\frac{\log(2/\delta)}{2n_{c}}}\Big)^{\gamma} + L\phiFl^\gamma$.
It remains to bound $t_{1-\alpha+\phi}^* - t^*$. By definition, we have $t_{1-\alpha+\phi}^* = Q(1-\alpha + \phi; |Y-f_{1-\alpha+\phi}^*(X)|)$, and $t^* = Q(1-\alpha; |Y-f^*(X)|)$. Moreover, we notice that $t_{1-\alpha+\phi}^* \leq Q(1-\alpha + \phi; |Y-f^*(X)|)$ since by definition $f_{1-\alpha+\phi}^*$ minimizes $Q(1-\alpha + \phi; |Y-f(X)|)$ over all $f\in\calF$. Hence, $t_{1-\alpha+\phi}^* - t^* \leq L\phiFl^\gamma$, by Assumption~\ref{ass:regularity}. We conclude by combining everything. %We conclude using the fact that $\lambda(C_{\hat{f},\hat{t}}^{1-\alpha}) = 2\hat{t}$ and $\lambda(C_{f^*,t ^*}^{1-\alpha}) = 2t^*$.
\end{proof}


To the best of our knowledge, Theorem~\ref{thme:main-constant} is one of the first to provide such a finite-sample upper bound on the excess-volume loss. It explicitly reveals the impact of the two split conformal steps of \method. The two first error terms (involving $n_c$) match the bound of Corollary~\ref{cor:f-given}, and can be seen as the volume loss due to the calibration step. While the third term, with $\phiFl$, is the error due to the learning step. If we omit the dependence in $\delta$, $\phiFl$ will typically be in the form of $\sqrt{\frac{\text{Compl}(\calF)}{n_\ell}}$, where $\text{Compl}(\calF)$ measures the complexity of $\calF$ (see Prop.~\ref{prop:phi-finite} and Appendix \ref{sec:closed-form-phi}). In most settings, we have $\text{Compl}(\calF)\gg \log(1/\delta)$. Hence, the rate in Eq.~\eqref{eq:thme-const} supports the important intuition that the learning step remains more important than the conformal step, at least in the sense that more data-points are needed to reach convergence. It is thus preferable to assign more points to the learning than the calibration.
%, since in most settings we have $\text{Compl}(\calF)\gg \log(1/\delta)$.% the error due to the learning step dominates the error due to the conformal step%, and contrary to what is usually done, i.e.~taking $n_c=n_\ell=n/2$.%, our result suggests taking instead $n_c$ in the order of $\sqrt{n_\ell}$.

% \begin{remark}
%     The more complex is the class of regression function $\calF$, the more data-points are needed to reach convergence. However, it should be kept in mind that the complexity $\calF$ must also be high enough to hopefully contain the \emph{true} minimizer of the $(1-\alpha)$-QAE.  
% \end{remark}

% \todo{The more complex is the class of regression function $\calF$, the more data-points are needed to reach convergence. However, it should be kept in mind that the complexity $\calF$ must also be high enough to hopefully contain the \emph{true} minimizer of the $(1-\alpha)$-QAE.  }