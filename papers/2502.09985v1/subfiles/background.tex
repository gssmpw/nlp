% !TEX root = ../ICML.tex

\section{Background}

\subsection{Preliminaries on Split Conformal Prediction}

\label{sec:conform-background}

In this section, we give some important reminders on CP, focusing on the split approach at the core of this paper \citep{papadopoulos2002inductive}. 


Let us assume that we have access to a 
data set $\calD=\{(X_i,Y_i)\}_{1 \leq i \leq n}$, that we split into 
a learning set $\calD^{lrn}$ and a non-overlapping calibration set 
$\calD^{cal}$, 
containing respectively $n_\ell\geq 1$ and $n_c \geq 1$ data points such that $n_\ell+n_c=n$.% and the calibration data 
%$(X_i,Y_i)$, $i \in \intset{n_c} \egaldef \{ 1 , \ldots, n_c \}$, are i.i.d. 
%and follow the same distribution as the independent 
%test point $(X,Y)$.%\footnote{\bat{In the CP literature, calibration data are usually assumed to be only \emph{exchangeable}. Here, we assume them i.i.d. for simplicity and continuity with our contribution.}}

The first step of split CP, referred to as the \textit{learning step}, consists in finding a \textit{base} predictor $f\in\calF$ using the learning data set $\calD^{lrn}$. This predictor, denoted $\hf$, is then used to define a nonconformity score function 
$s = s_{\hf}: \calX \times \calY \rightarrow \IR$, 
such that for a pair $(x,y) \in \calX \times \calY$, 
$s_{\hf}(x,y)$ measures the level of non-conformity of the point $(x,y)$ with respect to the base predictor $\hf$. In other word, it measures how far is the true value~$y$ from the prediction $\hf(x)$. Whether we are in the regression or classification setting, many possible base predictors and score functions exist in the literature (see e.g.~\cite{angelopoulos2023conformal}). In Example~\ref{exemple:base-predictor}, we recall the most widely used base predictors and associated score functions for conformal regression.
% In regression, for instance, a common choice is the fitted absolute residual
% %
% $s_{\hf} : (x,y) \mapsto \lvert y - \hf(x) \rvert$. 
% %
% In the sequel, we often write $s$ instead of $s_{\hf}$ for simplicity. 
% Furthermore, note that split CP does not assume a particular choice of score function, so throughout the paper, we keep the function $s$ abstract. 

In the second step of split CP, referred to as the \textit{calibration step}, we construct the prediction set. To this end, we first calculate the values of $s_{\hf}$ taken on the calibration set $\calD^{cal}$, 
called the nonconformity scores 
$S_i := s_{\hf}(X_i,Y_i)$, $i \in \intset{n_c}$. Then, we compute the $\lceil (n_c+1)(1-\alpha) \rceil$-th smallest nonconformity score 
$\hat{q}_{1-\alpha}:=S_{(\lceil (n_c+1)(1-\alpha) \rceil)}$, % $:= \Qh_{(\lceil (n_c+1)(1-\alpha) \rceil)}(\calS^{cal}_{n_c})$, 
%where 
%$\calS^{cal}_{n_c} := ( S_{1},\ldots, S_{n_c} )$
%and $\Qh_{(\cdot)}$ is the sample quantile function defined by 
%
%\begin{equation}
%\label{def:Qk}
%\forall r , N \geq 1 \, , \, 
%\forall \calS' \in \IR^N \, ,
%\qquad 
%\Qh_{(r)}(\calS') := \begin{cases}
%\calS'_{(r)} & \text{if } r \leq N  \\
%+ \infty & \text{otherwise} 
%\, ,
%\end{cases}
%\end{equation}
%
where $S_{(1)} \leq \ldots \leq S_{(n_c)}$, and we return, for any $x\in \calX$, the set-valued function $\Chat:\calX \rightarrow 2^\calY$ such that $\forall x\in\calX$:
%
\begin{align}
\label{set_conf}
\Chat(x) 
:= \Bigl\{ y \in \calY \,:\, s_{\hf}(x, y) \leq \hat{q}_{1-\alpha} \Bigr\} 
\, . 
\end{align} 
%
In the case where $\lceil (n_c+1)(1-\alpha) \rceil>n_c$, we fix $\hat{q}_{1-\alpha} = +\infty$, meaning that we take the trivial prediction set $\Chat(x) = \calY$. %\todo{il faudrait faire l'autre cote du coup...si < 1 (plus tard)}
 Stated differently, $\hat{q}_{1-\alpha}$ corresponds to the $(1-\alpha)$-quantile of the data set $\{S_i\}_{i=1}^{n_c}\cup \{+\infty\}$. Quite remarkably, if we only assume that the scores $S_1,\ldots, S_{n_c}$ and $s(X, Y)$ are \textit{exchangeable}, the set \eqref{set_conf} satisfies condition~\eqref{eq:conform-obj} \citep{papadopoulos2002inductive}. Moreover, if the scores are continuous random variables, it can be shown that $\IP(Y\in \Chat(X)) \leq 1-\alpha + 1/(n_c+1)$. Note that this type of guarantees are referred as \textit{marginal} because the probabilities are taken with respect to the test point $(X,Y)$ and the calibration set $\calD^{cal}$. %In order to go further than these guarantees there has been also a huge interest in demonstrating \textit{conditional} ones \citep{vovk2012conditional,bian2023training}. This other types of guarantees typically condition the probability of coverage over $\calD$ or $X$, and provide lower and upper bound on the coverage, with high probability.\\ %Notice that since these are more difficult to obtain, only assuming exchangeable scores is not sufficient anymore, and one has to assume that the data are i.i.d.

\begin{exemple}\emph{(Conformal regressors).}
    \label{exemple:base-predictor}
    %\bat{Revoir notation $f$,$f_1$ etc.}
    \begin{enumerate}%[leftmargin=*]
    	%
        \item In the standard \emph{Split CP} \citep{papadopoulos2002inductive} the base predictor is a function $\mu$ in $\calF$, a class of regression function. Typically, $\hat{\mu} = \argmin_{\mu\in \calF}\sum_{i=1}^{n_\ell}(Y_i - \mu(X_i))^2$. Then, the score function is taken to be the absolute residual $s(x, y) = |y-\hat{\mu}(x)|$. This gives the interval $\Chat(x)=[\hat{\mu}(x)-\hat{q}_{1-\alpha}, \hat{\mu}(x)+\hat{q}_{1-\alpha}]$.
        %
        \item In \emph{Locally-Weighted Conformal Inference} \citep{papadopoulos2008normalized}, an additional base predictor is added in order to have interval sizes that adapt to the value of $X$. More precisely, we have $f=(\mu,\sigma)$, with $\mu \in \calF_1$, $\sigma \in \calF_2$. $\hat{\mu}$ is fitted as above, and $\hat{\sigma}$ fits the residuals given $X=x$, i.e. $\hat{\sigma} = \argmin_{\sigma \in \calF_2} \sum_{i=1}^{n_\ell}(R_i - \sigma(X_i))^2$ where $R_i=|Y_i-\hat{\mu}(X_i)|$. Taking the scoring function $s(x, y) = |y-\hat{\mu}(x)|/\hat{\sigma}(x)$, the resulting prediction interval is given by $\Chat(x)=[\hat{\mu}(x)-\hat{\sigma}(x)\hat{q}_{1-\alpha}, \hat{\mu}(x)+\hat{\sigma}(x)\hat{q}_{1-\alpha}]$.
        %
        \item In \emph{Conformalized Quantile Regression} (CQR) \citep{romano2019conformalized}, we have $f=(Q_{\alpha/2},Q_{1-\alpha/2})$ where $Q_{\alpha/2} \in \calF_1$ (respectively $Q_{1-\alpha/2}\in \calF_2$) is a quantile regressor of $Y$ given $X=x$, of order $\frac{\alpha}{2}$ (respectively $1-\frac{\alpha}{2}$). For instance, we take $\widehat{Q}_{\alpha/2} = \argmin_{Q\in\calF_1}\sum_{i=1}^{n_\ell}\rho_{\alpha/2}(Y_i-Q(X_i))$, where $\rho_{\alpha/2}$ is the ``pinball'' loss \citep{koenker2001quantile}. $\widehat{Q}_{1-\alpha/2}$ is defined analogously with $\rho_{1-\alpha/2}$. Then, we take $s(x,y) = \max\{\widehat{Q}_{\alpha/2}(x) - y, y- \widehat{Q}_{1-\alpha/2}(x)\}$, which gives $\Chat(x)=[\widehat{Q}_{\alpha/2}(x) - \hat{q}_{1-\alpha}, \widehat{Q}_{1-\alpha/2}(x) + \hat{q}_{1-\alpha}]$.
        %
    \end{enumerate}
\end{exemple}


% Note that, although the first CP methods were the \emph{split} 
% and the related \emph{full} methods not presented here \citep{papadopoulos2002inductive, vovk2005algorithmic}, 
% many extensions based upon them and with similar guarantees have been proposed in the literature. 
% Their principal novelty either lies in a trade-off between the split and the full conformal techniques, namely the cross-conformal approaches [REF], or in a clever choice of the non-conformity score function~$s$. See for instance \citep{kivaranovic2020adaptive, sesia2021conformal, gupta2022nested, ndiaye2022stable, han2022split, guan2023localized} for recent works. 
As our task here is not to be exhaustive on the CP literature, we refer to~\citet{vovk2005algorithmic},~\citet{angelopoulos2023conformal}, and~\citet{fontana2023conformal} for in-depth presentations of CP and 
to \citet{manokhin_2022_6467205} for a curated list of papers.

%\bat{Parler des nested set si besoin pour la suite}

\subsection{Problem statement}
\label{sec:problem-statement}

In this work, we focus on conformal regression problems with $\calY=\IR$. Precisely, we study when and how split CP outputs prediction sets approximating the solution of Problem~\eqref{eq:informal-opt}. Since we consider regression tasks, let us first re-write the latter optimization problem by replacing $\mu$ with the Lebesgue measure $\lambda:\calB(\IR)\rightarrow [0,+\infty]$, $\calB(\IR)$ being the Borel $\sigma$-algebra on $\IR$:
% \begin{align}
%     \min_{C_\alpha \in \calC_{\text{Borel}}} &\; \IE[\lambda(C_\alpha (X))] \label{eq:formal-opt} \\
%      \quad \text{s.t.} \quad & \IP(Y\in C_\alpha(X)) \geq 1-\alpha\;, \nonumber
% \end{align}
\begin{equation}
    \label{eq:formal-opt}
    \min_{C \in \calC_{\text{Borel}}} \IE[\lambda(C (X))]  \hspace{0.2cm}\text{s.t.}\hspace{0.2cm}  \IP(Y\in C(X)) \geq 1-\alpha\hspace{0.05cm},
\end{equation}
where $\calC_{\text{Borel}}:=\{\text{Measurable functions } C:\calX\rightarrow \calB(\IR)\}$. In the following, we refer to $C^*$ as one minimizer of~\eqref{eq:formal-opt}.
%
Note that optimizing over all possible measurable functions in $\calC_\text{Borel}$ can be difficult in practice but also sometimes useless. For instance, in the regression setting where $Y=f^*(X) + \calN(0, \sigma^2)$, the distribution of $Y$ given $X=x$ is symmetric and has only one mode. The optimal $C^*(x)$ will thus necessarily be an interval centered at $f^*(x)$ (see the discussion below on closed-form expressions for $C^*$). In this simple case, we see that looking at the full set $\calC_{\text{Borel}}$ is useless as one could only consider the set of functions $C(x)$ that outputs intervals.  

In this work, we will restrict the space of research $\calC_{\text{Borel}}$ in~\eqref{eq:formal-opt} to smaller sets of set-valued functions, namely those outputting intervals. Like in statistical learning theory, this restriction can be thought of as a source of \emph{approximation error}. In other words, we would like the restricted set to be sufficiently complex so that it includes (one of) the function solving~\eqref{eq:formal-opt}. If it is not the case, we face such an approximation error. Nevertheless, controlling this error is not the objective of this paper, as we are going to mostly focus on the \emph{estimation error}, which comes from the fact that only an empirical version of~\eqref{eq:formal-opt} is going to be solved.%, using a data set of finite sample size.

%\vspace{-0.2cm}

\paragraph{On closed-form expressions for~\eqref{eq:formal-opt}.} In some settings, we can derive oracle prediction sets solving~\eqref{eq:formal-opt}. For instance, when there is no covariate $X$, we recover the Minimum Volume Set (MVS) estimation problem of \citet{NIPS2005_d3d80b65}. In that case, if $Y$ admits a density $p_Y(y)$ with respect to $\lambda$, we can derive a closed-form expression for $C^*$ in terms of density level sets: $\exists t_\alpha\geq 0$ such that $C^*=\{y\in\IR : p_Y(y) \geq t_\alpha\}$ as soon as $\lambda(\{y\in\IR : p_Y(y) = t_\alpha\})=0$. Similarly, if we condition the expectation and the probability in~\eqref{eq:formal-opt} on $X=x$, and if $Y|X=x$ admits a conditional density $p_{Y|X}(y|x)$, we get $C^*(x)=\{y\in\IR : p_{Y|X}(y|x) \geq t'_\alpha(x)\}$ for some $t'_\alpha(x)\geq 0$ \citep{polonik2000conditional,lei2014distribution}. This has led to a whole literature based on plug-in (conditional) density estimators, which is not the approach considered in this paper but which is worth mentioning.



\subsection{Related work}


\paragraph{Minimum Volume Sets and Density Level Sets estimation.} As mentioned above, problem~\eqref{eq:formal-opt} is strongly linked with the MVS estimation Problem \citep{NIPS2005_d3d80b65}, which is itself linked with the problems of support estimation \citep{scholkopf2001estimating,munoz2006estimation} and density level sets estimation \citep{polonik2000conditional}. Despite the fact that these methods can all be used to construct prediction sets with a desired coverage level, their link with Conformal Prediction has received little attention in the past. Among the most well known works, we can mention those taking the idea of plug-in (conditional) density estimators mentioned above, on top of which they add a calibration step to obtain better coverage guarantees \citep{lei2013distribution,lei2014distribution,izbicki2022cd,chernozhukov2021distributional}. However, their theoretical results regarding the size of the returned set are mostly asymptotic. More importantly, modern supervised learning and CP techniques are rarely based on a non-parametric estimation of the (conditional) density, which can be difficult in practice. They are mostly based on the learning of a (parametrized) prediction function that belongs to a set of hypotheses (see Example~\ref{exemple:base-predictor}). Thus, the framework of Section~\ref{sec:problem-statement}, largely inspired by \citet{NIPS2005_d3d80b65}, where we restrict the class of prediction sets to a smaller subset, seems more appropriate for the design and analysis of CP methods.


\paragraph{Efficient Conformal Prediction.} 

%Although all CP methods seek to obtain efficient prediction sets, i.e.~sets that are as small as possible, most only demonstrate this empirically. Some works sometimes manage to obtain asymptotic guarantees, but often on the basis of important assumptions. More importantly, contrary to our work, the learning of the base predictor itself is rarely justified by the minimization of the size of the prediction set. \bat{On peut virer le premier paragraphe et passer direct Ã  la suite si besoin de place :} 

Recently, the question of controlling the size of the learned prediction set and explicitly see this as a minimization objective has attracted a lot of attention. For instance in \citet{yang2024selection} and \citet{liang2024conformal}, the authors focus on efficiency-oriented model selection. Closer to our work, we can mention \citet{stutz2021learning} and \citet{kiyani2024length}, which consider an optimization problem similar to that of~\eqref{eq:formal-opt}, with a focus on the optimization aspects and on relaxations of the problem. However, they do not provide statistical guarantees on the learned estimates. Finally, there is the work of \citet{baiefficient}, which proposes a generalization of the split calibration step, where instead of a single quantile, multiple learnable parameters are optimized to minimize the size of the final prediction set.

