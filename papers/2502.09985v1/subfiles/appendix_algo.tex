\section{Detailed implementation of the empirical $(1-\alpha)$-QAE minimization}\label{sec:optim_append}

As explain in Section \ref{sec:optim_emp_QAE}, to solve Problem \eqref{eq:optim_quantile} we use a gradient descent strategy. However, because the empirical quantile is not differentiable, we replace $\widehat{Q}$ in Problem \eqref{eq:optim_quantile} by the following smooth approximation:
%
\begin{equation*}
	\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}}) = \inf \{t \,:\, \widetilde{F}_{\varepsilon}(t, \theta) \geq q \} \; ,
\end{equation*}
%
where $\widetilde{F}_{\varepsilon}$ is an approximation of the empirical distribution of the loss-values $(\ell(\theta;Z_i))_{i\in\calD^{lrn}}$ defined for $\varepsilon > 0$ by
\begin{align*}
	\widetilde{F}_{\varepsilon}(t, \theta) = \sum_{i\in\calD^{lrn}} \Gamma_{\varepsilon}(\ell(\theta;Z_i) - t) \; ,
\end{align*}
%
with
%
\begin{align*}
	\Gamma_{\varepsilon}(z) = \left\{
	\begin{array}{ll}
		1 & z \leq - \varepsilon \\
		\gamma_{\varepsilon}(z) & -\varepsilon < z < \varepsilon \\
		0 & z \geq \varepsilon
	\end{array}
	\right. \; ,
\end{align*}
and $\gamma_{\varepsilon} : [-\varepsilon, \varepsilon] \longrightarrow [0, 1]$ a symmetric and strictly decreasing function such that it makes $\Gamma_{\varepsilon}$ differentiable. One possible choice for $\gamma_{\varepsilon}$ is given in \citep[Eq. (2.6)]{pena2020solving}:
%
\begin{equation}\label{eq:gamma_epsi}
	\gamma_{\varepsilon}(z) = \dfrac{15}{16}\left(-\dfrac{1}{5} \left(\dfrac{z}{\varepsilon}\right)^5 + \dfrac{2}{3}\left(\dfrac{z}{\varepsilon}\right)^3 - \dfrac{z}{\varepsilon} + \dfrac{8}{15} \right) \; .
\end{equation}
%

For a given $q$ and $\varepsilon > 0$, under some assumptions on the loss (see \citet{pena2020solving}), the implicit function theorem implies that:
%
\begin{align}\label{eq:grad_epsi}
	\nabla_{\theta} [\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})] &= \dfrac{\sum_{i\in\calD^{lrn}} \Gamma'_{\varepsilon}(\ell(\theta;Z_i) - \widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})) \cdot \nabla_{\theta} \ell(\theta;Z_i)}{\sum_{i\in\calD^{lrn}} \Gamma'_{\varepsilon}(\ell(\theta;Z_i) - \widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}}))} \; ,
\end{align}
%
where $\nabla_{\theta}$ denotes the gradient with respect to $\theta$ and $\Gamma'$ is the differential of $\Gamma$. We can therefore use a gradient descent algorithm to solve an approximation of the QAE Problem \eqref{eq:optim_quantile} given by:
%
\begin{align*}
	&\min_{\theta} \; \widetilde{Q}_{\varepsilon}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \; .
\end{align*}
%
To this end, starting from an initial guess $\widetilde{\theta}_1$, we simply make the iterates:
%
\begin{align*}
	\widetilde{\theta}_{k+1} = \widetilde{\theta}_{k} - \eta_k \nabla_{\theta} [\widetilde{Q}_{\varepsilon}(1-\alpha; (\ell(\widetilde{\theta}_k;Z_i))_{i\in\calD^{lrn}})] \; ,
\end{align*}
where $\eta_k > 0$ is the step-size. The full procedure is summary in Algorithm \ref{alg:min_quantile} when $\gamma_{\varepsilon}$ is an in Eq. \eqref{eq:gamma_epsi}.
\begin{algorithm}
	\caption{Gradient descent to solve the QAE problem (step 1 of \method~ and \methodAD)}
	\label{alg:min_quantile}
	\begin{algorithmic}[1]
				\State \textbf{Inputs:} $\varepsilon$, $\widetilde{\theta}_1$, $n_{iter}$, $(\eta_k)_{1 \leq k \leq n_{iter}}, \alpha$
				\For{$k = 1, \ldots, n_{iter}$}
				\State $A \gets \widetilde{Q}_{\varepsilon}(1-\alpha; (\ell(\widetilde{\theta}_k;Z_i))_{i\in\calD^{lrn}}))$
				\For{$i \in \calD^{lrn}$}
				\State $B_i \gets \Gamma'_{\varepsilon}(\ell(\widetilde{\theta}_{k};Z_i) - A) = -\dfrac{15}{16}\left(\Big(\varepsilon^2 - (\ell(\widetilde{\theta}_{k};Z_i) - A)^2\Big)^2/\varepsilon^5\right) \cdot \1\{-\varepsilon < (\ell(\widetilde{\theta}_{k};Z_i) - A) < \varepsilon\}$
				\State $C_i \gets \nabla_{\theta} \ell(\theta;Z_i)$
				\EndFor
				\State $\widetilde{\theta}_{k+1} \gets \widetilde{\theta}_{k} - \eta_k \cdot \sum_{i} (B_i C_i) / \sum_{i} B_i$
				\EndFor
				\State \textbf{Output:} $\widetilde{\theta}_{n_{iter}+1}$
			\end{algorithmic}
\end{algorithm}

\begin{remark}
	In our setting, $\ell$ is not differentiable because of the absolute value function. In practice, we therefore replace the gradient by a subdifferential (this is what we do in the experiments). Another possibility could be to replace the absolute value function with a smooth approximation, such as the Huber loss \citep{huber1964}. Furthermore, as also done in \citet{luo2022empirical}, in Eq \eqref{eq:grad_epsi} we replace $\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})$ by the empirical quantile for computation efficiency.
	%While in practice replacing the gradient by a subdifferential works well\todo{Dire quelque part que c'est ce qu'on a fait dans les xps?} (this is what we do in the experiments), the implicit function theorem and the following theoretical\todo{Ces resultats on disparus ? Si tu ne veux pas les mettre, mentionner que la convergence est obtenue direct car c'est une GD ?} results are only true for differentiable (and smooth) loss functions. In order to fit with the theory, a possibility is to follow a similar strategy as above, by replacing the absolute value function with a smooth approximation, such as the Huber loss \citep{huber1964}.
\end{remark}

\begin{remark}(Link with other formulations)
	Problem \eqref{eq:optim_quantile} is in fact similar to the \textit{single chance constraint problem} (see e.g. \citep{curtis2018sequential}). It can also be reformulated as the following  bi-level optimization problem: 
	%
	\begin{align*}
		\min_{\theta} &\; t(\theta)
		\quad \text{s.t.} \quad t(\theta) = \arg\min_{t} \sum_{i\in\calD^{lrn}} \rho_{1-\alpha}(\ell(\theta;Z_i) - t) \; .
	\end{align*}
	%
	where $\rho_{1-\alpha}$ is the pinball loss. Indeed, from \cite{koenker1978regression, biau2011sequential} we know that $t(\theta) = \widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}})$.
\end{remark}
%\todo{Je propose de le faire pour une version arxiv par exemple car on a pas fait d'xp avec. Ou alors on en parle pour "future work" ?}

%\section{Detailed implementation of step 1 of \method}
%
%Recall the setting presented in the main paper for step 1 of \method. We assumed that $f\in\calF$ was parametrized by $\theta \in\Theta$, and for the sake of generality, we considered the minimization of $\widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i=1}^n)$ with respect to $\theta$. Here, $\ell:\Theta\times\calZ\rightarrow\IR$ is a loss function, taking as input a parameter $\theta$ and a data point $Z_i$. In the setting of \texttt{EffOrt}, $Z_i=(X_i,Y_i)$, $\ell(\theta;Z_i)=|Y_i-f_\theta(X_i)|$, and its first step is then equivalent to:
%
%\begin{align} \label{eq_appen:optim_quantile}
%	%	&\hat{\theta} = \underset{\theta \in \Theta}{\argmin}  \; \widehat{Q}(1-\alpha;\{|Y_i-f_{\theta}(X_i)|\}_{i\in\calD^{lrn}}) \; . \\
%	&\min_{\theta} \; \widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \; .
%\end{align}
%%
%To solve this problem, one natural idea is to use a gradient descent algorithm on the empirical quantile function $\widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}})$. However, this function is not differentiable in $\theta$. We therefore follow the strategy of \citep{pena2020solving} and consider a smooth approximation of it. More precisely, given the loss-values $(\ell(\theta;Z_i))_{i\in\calD^{lrn}}$, we first approximate their empirical cdf $\widehat{F}(t, \theta) := \sum_{i\in\calD^{lrn}} \1\{ \ell(\theta;Z_i) \leq t\}$ by another function $\widetilde{F}_{\varepsilon}$ where the indicator is replaced by a smooth version of it, i.e.:
%%
%\begin{align*}
%	\widetilde{F}_{\varepsilon}(t, \theta) = \sum_{i\in\calD^{lrn}} \Gamma_{\varepsilon}(\ell(\theta;Z_i) - t) \; ,
%\end{align*}
%%
%where $\varepsilon > 0$ is a parameter of the approximation:
%%
%\begin{align*}
%	\Gamma_{\varepsilon}(y) = \left\{
%	\begin{array}{ll}
%		1 & y \leq - \varepsilon \\
%		\gamma_{\varepsilon}(y) & -\varepsilon < y < \varepsilon \\
%		0 & y \geq \varepsilon
%	\end{array}
%	\right. \; ,
%\end{align*}
%and $\gamma_{\varepsilon} : [-\varepsilon, \varepsilon] \longrightarrow [0, 1]$ is a symmetric and strictly decreasing function such that it makes $\Gamma_{\varepsilon}$ differentiable. One possible choice for $\gamma_{\varepsilon}$ is given in \citep[Eq. (2.6)]{pena2020solving}. Then, we define the smooth empirical quantile function by:
%%
%\begin{equation}\label{eq_appen:smooth_quant}
%	\psi_{\varepsilon}(\theta) := \widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}}) = \inf \{t \,:\, \widetilde{F}(t, \theta) \geq q \} \; .
%\end{equation}
%%
%For a given $q$ and $\varepsilon > 0$, if the loss function $\ell(\cdot)$ is differentiable, the implicit function theorem \pie{(il faut Lemma 2.1 de \citep{pena2020solving}) ??} implies that: %\citep{luo2022empirical}:
%%
%\begin{align*}
%	\nabla_{\theta} [\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})] &= \dfrac{\sum_{i\in\calD^{lrn}} \Gamma'_{\varepsilon}(\ell(\theta;Z_i) - \widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})) \cdot \nabla_{\theta} \ell(\theta;Z_i)}{\sum_{i\in\calD^{lrn}} \Gamma'_{\varepsilon}(\ell(\theta;Z_i) - \widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}}))} \; ,
%\end{align*}
%%
%where $\nabla_{\theta}$ denotes the gradient with respect to $\theta$ and $\Gamma'$ is the differential of $\Gamma$.
%
%\begin{remark}
%	In our setting, $\ell$ is not differentiable because of the absolute value function. While in practice replacing the gradient by a subdifferential works well (this is what we do in the experiments), the implicit function theorem and the following theoretical results are only true for differentiable (and smooth) loss functions. In order to fit with the theory, a possibility is to follow a similar strategy as above, by replacing the absolute value function with a smooth approximation, such as the Huber loss \citep{huber1964}.
%\end{remark}
%
%Now that we know how to compute the gradient of the smooth quantile estimator, we can use the gradient descent algorithm to solve:% the smooth version of Problem \ref{eq:optim_quantile}: 
%%
%\begin{align} \label{eq_appen:optim_smooth_quantile}
%	&\min_{\theta} \; \widetilde{Q}_{\varepsilon}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \; .
%\end{align}
%%
%To this end, starting from an initial guess $\widetilde{\theta}_1$, we simply make the iterates:
%%
%\begin{align*}
%	\widetilde{\theta}_{k+1} = \widetilde{\theta}_{k} - \gamma_k \nabla_{\theta} [\widetilde{Q}_{\varepsilon}(1-\alpha; (\ell(\widetilde{\theta}_k;Z_i))_{i\in\calD^{lrn}})] \; ,
%\end{align*}
%where $\gamma_k > 0$ is the stepsize.
%
%%The full procedure is summary in Algorithm \ref{alg:min_quantile}.
%%\begin{algorithm}
%%	\caption{...}
%%	\label{alg:min_quantile}
%%	\begin{algorithmic}[1]
%	%		\State \textbf{Inputs:} $\varepsilon$, $\widetilde{\theta}_1$, $n_{iter}$, $(\gamma_k)_{1 \leq k \leq n_{iter}}$
%	%		\For{$k = 1, \ldots, n_{iter}$}
%	%		\State $\widetilde{\theta}_{k+1} \gets \widetilde{\theta}_{k} - \gamma_k \nabla_{\widetilde{\theta}_k} [\widetilde{Q}(1-\alpha; (\ell(\widetilde{\theta}_k;Z_i))_{i\in\calD^{lrn}})]$
%	%		\EndFor
%	%		\State \textbf{Output:} $\widetilde{\theta}_{n_{iter}+1}$
%	%	\end{algorithmic}
%%\end{algorithm}
%
%\begin{lemma}[\cite{pena2020solving}]\label{lemma:smooth_grad}
%	If $\gamma_{\varepsilon}$ is defined as in \pie{???}, $\ell(\cdot)$ has a Lipschitz continuous gradient, and $(1 - \alpha)\cdot\lvert \calD^{lrn} \rvert \notin \mathbb{Z}$, then the function $\psi_{\varepsilon} : \Theta \rightarrow \IR$ defined in Eq.~\eqref{eq:smooth_quant} has also a (bounded) Lipschitz continuous gradient.
%\end{lemma}
%
%\begin{proposition} Let us suppose that the assumptions of Lemma \ref{lemma:smooth_grad} hold and denote by $L > 0$ the Lipschitz smoothness constant of $\psi_{\varepsilon}$. After $T$ iterations of the gradient descent algorithm starting at $\widetilde{\theta}_1$ and with constant stepsize equal to $1/L$, we have:
%	\begin{align*}
%		\lVert \nabla_{\theta}\psi_{\varepsilon}(\zeta_T) \rVert \leq \sqrt{\dfrac{2 L \cdot (\psi_{\varepsilon}(\widetilde{\theta}_1) - \psi_{\varepsilon}(\widetilde{\theta}_{\varepsilon}^*)) }{T}} \; ,
%	\end{align*}
%	where $\zeta_T = \widetilde{\theta}_{T_{\min}}$ with $T_{\min} = \arg\min_{0<k\leq T+1}\lVert \nabla_{\theta}\psi_{\varepsilon}(\widetilde{\theta}_k) \rVert$ and where $\widetilde{\theta}_{\varepsilon}^*$ is one minimizer of Problem \eqref{eq:optim_smooth_quantile} \pie{existance de $\widetilde{\theta}_{\varepsilon}^*$?}. In other words, $\psi_{\varepsilon}(\zeta_T)$ is a $\calO(1/\sqrt{T})$-approximate first-order critical point.
%\end{proposition}
%\begin{proof}
%	\url{https://www.ceremade.dauphine.fr/~waldspurger/tds/22_23_s1/non_convex.pdf} \pie{Preuve dans ce pdf mais j'imagine que c'est standard.}
%\end{proof}
%
%\begin{remark}%(What happen when $\varepsilon$ goes to $0$?)
%	%By construction, we know that $\Gamma_{\varepsilon}(y) \longrightarrow \1\{y \leq 0\}$ and $\widetilde{Q}_{\varepsilon}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}}) \longrightarrow \widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}})$ as $\varepsilon \longrightarrow 0$. Furthermore, we also have that, 
%	If $\varepsilon$ tends to $0$, then $\nabla_{\theta} [\widetilde{Q}_{\varepsilon}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})] \longrightarrow \nabla_{\theta}\ell(\theta;Z_{i^*})$ where $i^*$ is the index such that $\ell(\theta;Z_{i^*}) = \widehat{Q}(q; (\ell(\theta;Z_i))_{i\in\calD^{lrn}})$. This means that, for this particular case, at each step of the gradient descent, instead of computing the gradient over all the loss-values $(\ell(\theta;Z_i))_{i\in\calD^{lrn}}$, we simply compute it for $\ell(\theta;Z_{i^*})$ which is the $\lceil q \cdot \lvert\calD^{lrn} \rvert \rceil$-th smallest value in $(\ell(\theta;Z_i))_{i\in\calD^{lrn}}$. %Hence, the gradient descent step boils done to Algorithm 1 in \citep{lecue2020robust}.
%	\pie{a rendre rigoureux}
%\end{remark}
%
%\begin{remark}(Link with other formulations)
%	Problem \ref{eq:optim_quantile} is in fact similar to the \textit{single chance constraint problem} (see e.g. \citep{curtis2018sequential}). It can also be reformulated as the following  bi-level optimization problem: 
%	%
%	\begin{align*}
%		\min_{\theta} &\; t(\theta) \label{eq:obj-constant} \\
%		\quad s.t. \quad & t(\theta) = \arg\min_{t} \sum_{i\in\calD^{lrn}} \rho_{1-\alpha}(\ell(\theta;Z_i) - t) \; .
%	\end{align*}
%	%
%	where $\rho_{1-\alpha}$ is the pinball loss. Indeed, from \cite{koenker1978regression, biau2011sequential} we know that $t(\theta) = \widehat{Q}(1-\alpha;\{\ell(\theta;Z_i)\}_{i\in\calD^{lrn}})$.
%\end{remark}
%
%\bat{AJOUTER MON ALGO ??}