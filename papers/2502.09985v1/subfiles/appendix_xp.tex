\section{Additional results} \label{sec:add_xp}

\subsection{Synthetic data}\label{sec:add_xp_synth}
\paragraph{Experimental setup details for Section \ref{sec:xpADEffort}:}
During the learning step of \methodAD, we solve the $(1-\alpha)$-QAE Problem \eqref{eq:QAE} using the gradient descent strategy of Section \ref{sec:optim_emp_QAE}. The smoothing parameter $\varepsilon$ is set to $0.1$, $n_{iter}=1000$, and the step-size sequence is $\{(1/t)^{0.6}\}^{n_{iter}}_{t=1}$. The space of research $\calF$ is restricted to the space of linear functions. The function $\hat{s}(\cdot)$ (second step of \methodAD) and the two quantile regression functions of CQR are learned by using a Random Forest (RF) quantile regressor, implemented in the Python package sklearn-quantile\footnote{\href{https://sklearn-quantile.readthedocs.io}{https://sklearn-quantile.readthedocs.io}}. The function $\hat{\sigma}$ in LW-CP is learned using the RF regression implementation of scikit-learn \citep{scikit-learn}. Each time, the max-depth of the RF is set to $5$ and the other parameters are the default ones of the sklearn-quantile and scikit-learn packages.

\paragraph{Additional experiments:} We now present additional results on synthetic data:

\begin{itemize}
	
	\item In Figure \ref{fig:illustr_synth_coverage}, we display the coverage obtained on the scenarios of Section \ref{sec:xpEffort}. We see that, as expected, all methods return sets with average coverage of $1-\alpha=0.9$ (white circle) regardless of the distribution of the noise.
	%
	\item In figure \ref{fig:illustr_synth_NN}, we present additional results obtained when the base predictor is a Networks (NNs) and not a linear regressor as made in the main paper. We consider the model $Y = X^2 + \calE$ with $\calE$ following the same distributions as presented in Section \ref{sec:xpEffort}. In detail, we learn NNs with one hidden layer of size $10$ and with a ReLU activation function. In \method, the NN is learned using the gradient descent strategy of Section \ref{sec:optim_emp_QAE}. The smoothing parameter $\varepsilon$ is set to $0.1$, $n_{iter}=1000$ and the step-size sequence is $\{(1/t)^{0.6}\}^{n_{iter}}_{t=1}$. The gradient with respect to the NN weights involved in the gradient descent is calculated using automatic differentiation. For split CP, the NN is learned using an ADAM optimizer and the loss is either a Huber loss (robust NN) or a least squares loss. Again, in all scenarios, \method~returns marginally valid sets in general smaller than those of the split CP method. This confirms that learning a model via the $(1-\alpha)$-QAE problem is a better way of obtaining small prediction sets during the calibration step.
	
	\item In Figure \ref{fig:illustr_synth_adEffort_coverage}, we display the coverage obtained on the scenarios of Section \ref{sec:xpADEffort} when using \methodAD. We see again that, as expected, all methods return sets with average coverage of $1-\alpha=0.9$ (white circle) regardless of the distribution of the noise. Finally, Figure \ref{fig:illustr_synth_adEffort_example} shows examples of prediction sets returned by \methodAD, Locally weighted CP (LW-CP) and CQR when the noise is Gaussian.
\end{itemize}




\begin{figure*}[h!]
	\centering
	\includegraphics[width=.4\linewidth]{./img/coverage_normal_linear.pdf}
	\includegraphics[width=.4\linewidth]{./img/coverage_pareto_linear.pdf}
	\caption{Synthetic data: Boxplots of the $50$ empirical coverages obtained by evaluating \method~(see Section \ref{sec:xpEffort}). The white circle corresponds to the mean.} 
	\label{fig:illustr_synth_coverage}
\end{figure*}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.4\linewidth]{./img/size_normal_NN.pdf}
	\includegraphics[width=.4\linewidth]{./img/size_pareto_NN.pdf}
	\includegraphics[width=.4\linewidth]{./img/coverage_normal_NN.pdf}
	\includegraphics[width=.4\linewidth]{./img/coverage_pareto_NN.pdf}
	\caption{Synthetic data: %Length (top) and Coverage (bottom) obtained by evaluating \method (see Section \ref{sec:xpEffort}) when $\hat{f}$ is a neural-network. The white circle corresponds to the mean.
	Boxplots of the $50$ empirical expected lengths (top) and coverages (bottom) obtained by evaluating \method~(see Section \ref{sec:xpEffort}). The white circle corresponds to the mean.} 
	\label{fig:illustr_synth_NN}
\end{figure*}

\begin{figure*}[h!]
	\centering
	%\includegraphics[width=.48\linewidth]{./img/size_normal_adEffort.pdf}
	%\includegraphics[width=.48\linewidth]{./img/size_pareto_adEffort.pdf}
	\includegraphics[width=.4\linewidth]{./img/coverage_normal_adEffort.pdf}
	\includegraphics[width=.4\linewidth]{./img/coverage_pareto_adEffort.pdf}
	\caption{Synthetic data: Boxplots of the $50$ empirical coverages obtained by evaluating \methodAD~(see Section \ref{sec:xpADEffort}). The white circle corresponds to the mean.} 
	\label{fig:illustr_synth_adEffort_coverage}
\end{figure*}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.3\linewidth]{./img/res_adEffort.pdf}
	\includegraphics[width=.3\linewidth]{./img/res_LWCP.pdf}
	\includegraphics[width=.3\linewidth]{./img/res_CQR.pdf}
	\caption{Synthetic data: Example of sets returned by \methodAD~(left), LW-CP (middle), and CQR (right).} 
	\label{fig:illustr_synth_adEffort_example}
\end{figure*}

\subsection{Real data}
\label{app:real-data}
%
We finally compare \methodAD~with Locally Weighted CP (LW-CP) and CQR on the following public-domain real data sets also considered in e.g. \citep{romano2019conformalized}: abalone \citep{abalone_1}, boston housing (housing) \citep{harrison1978hedonic}\footnote{\href{https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html}{https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html}}, and concrete
compressive strength (concrete) \citep{yeh1998modeling}.\footnote{\href{http://archive.ics.uci.edu/dataset/165/concrete+compressive+strength}{http://archive.ics.uci.edu/dataset/165/concrete+compressive+strength}} We randomly split each data set $10$ times into a training set, a calibration set and a test set of respective "size" $40\%$, $40\%$, and $20\%$. The training and calibration sets are used to apply \methodAD, LW-CP, and CQR, and the test set to compute the coverage and length metrics. For \methodAD~and LW-CP the base prediction function $\hf$ is a Neural-Network (NN) with one hidden layer of size $10$ and a ReLU activation function. The function $\hs$ in the step 2 of \methodAD~ and the two quantile regression functions of CQR are learned with a Random Forest (RF) quantile regressor, implemented in the Python package sklearn-quantile. The function $\hat{\sigma}$ in LW-CP is learned using the RF regression implementation of scikit-learn \citep{scikit-learn}. Each time, the max-depth of the RF is set to $5$ and the other parameters are the default ones of the sklearn-quantile and scikit-learn packages. To illustrate the robustness of our approach, we finally add, in all the data sets, $5\%$ of outliers to the values to be predicted, using a Gaussian distribution whose mean is equal to 2 times the maximum value of the original data.

Figure \ref{fig:real_data} displays the length and the normalized length (i.e. the length divided by the maximal length obtained with the three methods in the $10$ splits) obtained on each data set. We can see that \methodAD~is competitive, as it generally returns marginally valid sets (see figure \ref{fig:real_data_cov} for coverage) of smaller or similar size to at least one of the other two methods. This is in line with the results obtained on synthetic data (Section \ref{sec:xps} and Appendix \ref{sec:add_xp_synth}). Note also that the variability of the coverage metric (represented by the length of the boxes in Figure \ref{fig:real_data_cov}) is much smaller for \methodAD~than LW-CP. Overall, these results show that \methodAD~is empirically competitive with the main existing CP methods, while enjoying a strong theoretical grounding. It is therefore a method of choice for all practical applications.

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.4\linewidth]{./img/length_realdata.pdf}
	\includegraphics[width=.4\linewidth]{./img/normlength_realdata.pdf}
	%\includegraphics[width=.33\linewidth]{./img/coverage_realdata.pdf}
	\caption{Real data: Boxplots of the lengths (left) and normalized lengths (right) obtained with \methodAD, LW-CP, and CQR on real data sets. The white circle corresponds to the mean.} 
	\label{fig:real_data}
\end{figure*}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.4\linewidth]{./img/coverage_realdata.pdf}
	\caption{Real data: Boxplots of the coverages obtained with \methodAD, LW-CP, and CQR on real data sets. The white circle corresponds to the mean.} 
	\label{fig:real_data_cov}
\end{figure*}

%\subsection{Real data}
%
%We finally evaluate \methodAD~on the abalone data set \citep{abalone_1}. The goal of this data set is to predict the age of abalone from physical measurements. The true age of an abalone is in general determined by cutting its shell and by counting the number of rings through a microscope. Because this is a time-consuming task, predicting the age of abalone using other more accessible measurements is of huge interest. The data set is composed of 9 variables: sex, length, diameter, height, whole weight, shucked weight, viscera weight, shell weight and rings. The variable to predict is the age which is equals to (number of rings + 1.5).
%
%During the learning step of \method, we solve the $(1-\alpha)$-QAE Problem \eqref{eq:QAE} using the gradient descent strategy of Section \ref{sec:optim_emp_QAE} and with $\calF$ restricted to the space of Neural-Network (NN) with one hidden-layer of size $10$. The smoothing parameter $\varepsilon$ is set to $1$, $n_{iter}=1000$, and the step-size sequence is $\{(1/i)^{0.6}\}^{n_{iter}/10}_{i=1}$.