
\begin{center}
    {\Large\textbf{Appendix}}
\end{center}

\section{Proofs of main results}

In this section we give the proofs of the main results of the paper, starting with a reminder of the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality used several times in the proofs.

\begin{lemma}\emph{(DKW inequality \citep{dvoretzky1956asymptotic,massart1990tight})} \label{lem:DKW} Let $X_1, X_2,\ldots, X_n$ be real-valued independent and identically distributed random variables with cumulative distribution function $F(\cdot)$. Let $\hat{F}_n$ denote the associated empirical distribution function defined by $\hat{F}_n(x) = \sum_{i=1}^n\1\{X_i\leq x\}$. For all $\varepsilon > 0$:
%
\begin{equation*}
    \IP\Big(\sup_{x \in\IR}  |F(x) - \hat{F}_n(x)| > \varepsilon \Big) \leq 2e^{-2n\varepsilon^2} \; .
\end{equation*}
  %
\end{lemma}

\subsection{Proof of Proposition~\ref{prop:upper-f-given}}
\label{app:proof-prop}



    We have that $\lambda\Big(C^{1-\alpha}_{f,\hat{t}}\Big) = 2\hat{t}$. Let the events $E_1:=\left\{\hat{t} > Q\left(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\right)\right\}$ and $E_{DKW}:=\left\{\sup_{t\geq 0}  |F_S(t) - \hat{F}_S(t)| >\sqrt{\frac{\log(2/\delta)}{2n_c}}\right\}$, 
    where $F_S(t) = \IP(S\leq t)$ and $\hat{F}_S(t) = \frac{1}{n_c}\sum_{i=1}^{n_c}\1\{S_i\leq t\}$. The main objective of the proof is to show that the event $E_1  \subset E_{DKW}$.
    
    We first recall that $\hat{t} = \widehat{Q}((1-\alpha)\frac{n_c+1}{n_c};\{S_i\}_{i=1}^{n_c})$, then $E_1$ is equivalent to:

    \begin{equation*}
        \widehat{Q}\Big(1-\alpha + \frac{1-\alpha}{n_c};\{S_i\}_{i=1}^{n_c}\Big) > Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\;,
    \end{equation*}
    which then implies that

    \begin{equation*}
        1-\alpha + \frac{1-\alpha}{n_c} > \hat{F}_S\Big(Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\Big)\;.
    \end{equation*}
    
    Where the last implication can be found for instance in the left-hand side of Eq. (34) in \citet{howard2022sequential}. It comes from the fact that $(1-\alpha)(n_c+1)$ is not an integer and, in that case, $\widehat{Q}(\cdot;\{S_i\}_{i=1}^{n_c})$ acts as an inverse of $\hat{F}_S$.
    
    % at 
    % Applying $\hat{F}_S$ on both sides, we get $\hat{F}_S\left(Q\left(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(1/\delta)}{2n_c}}; S\right)\right) < 1-\alpha + \frac{1-\alpha}{n_c}$ \bat{(See for instance Eq.~(34-35) in \citet{howard2022sequential})}. 
    
    Moreover, by definition of the quantile function and its relation with the cumulative distribution function (cdf), we have that 

    \begin{equation*}
        F_S\Big(Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\Big)\geq 1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}\;.
    \end{equation*}

    Hence, $\Big|F_S\Big(Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\Big) - \hat{F}_S\Big(Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; S\Big)\Big)\Big| > \sqrt{\frac{\log(2/\delta)}{2n_c}}$, which implies $E_{DKW}$.
    
    In the end, we have $\IP(E_1)\leq \IP(E_{DKW})$, and we conclude the proof by applying the DKW inequality from Lemma~\ref{lem:DKW} with $\varepsilon = \sqrt{\frac{\log(2/\delta)}{2n_c}}$.

\subsection{Proof of Theorem~\ref{thme:main-constant}}
\label{app:proof-main}

We now detail the proof of our main result, which follows the sketch provided in the main text. 

The first point of Theorem~\ref{thme:main-constant}, on the almost sure coverage guarantee, is a classical result of the conformal prediction literature, see e.g. \citet[Theorem 2.2]{lei2018distribution}. 
    Let us focus on the second result, which can be proved by following the two steps described hereafter. 
    
    \underline{\textbf{Step 1}:} We first notice that in the first step of \method, we are actually solving the empirical minimization objective:
    \begin{align*}
       	& \min_{f\in\calF, t\geq 0}~ t\\
        &\text{s.t.} \quad \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f(X_i)|\leq t\}\geq 1-\alpha \;, \nonumber
    \end{align*}
    with solutions denoted by $\hat{f}$ and $\hat{t}_{lrn}$.
    
    Using the theory of MVS estimation \citep{NIPS2005_d3d80b65}, we can compare this solution to the one of the oracle problem and show that with probability greater than $1-\delta$:
    
    \begin{equation}
        \label{eq:lower-scott-app}
        \IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})\geq 1-\alpha - \phiFl
    \end{equation}
    and
    \begin{equation}
        \label{eq:lower-scott-app2}
        \lambda\left(C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)\right) \leq \lambda\left(C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}(X)\right)\;,
    \end{equation}
    where $\phi \equiv \phiFl$ and $C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}(X)$ denotes the optimal oracle interval with increased coverage $1-\alpha + \phiFl$. In other word, $f_{1-\alpha+\phi}^*$ and $t_{1-\alpha+\phi}^*$ are the solutions of: 
    %
    \begin{align*}
    	&\min_{f\in\calF, t\geq 0}~ t \\
    	%
    	&\text{s.t.} \quad \IP(|Y-f(X)|\leq t)\geq 1-\alpha + \phiFl \; .
    \end{align*}

    \begin{proof}[Proof of~\eqref{eq:lower-scott-app} and~\eqref{eq:lower-scott-app2}]
        Let:
        \begin{itemize}
            \item $\Theta_\IP=\Big\{\IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})< 1-\alpha - \phiFl\Big\}$
            \item $\Theta_\lambda=\Big\{\lambda\left(C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)\right) > \lambda\left(C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}(X)\right)\Big\}$
            \item $\Theta_\phi=\Big\{\sup_{t\geq 0, f\in\calF}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-f(X_i)|\leq t\}\Big|> \phiFl\Big\}$
        \end{itemize}

    The objective is to show that $\Theta_\IP\cup\Theta_\lambda \subset \Theta_\phi$ since this would be mean that $\IP(\Theta^c_\IP\cap\Theta^c_\lambda)\geq \IP(\Theta^c_\phi)$, where $\Theta^c$ is the complementary of $\Theta$. Then, applying Assumption~\ref{ass:complexity} gives the desired result.

    \underline{$\Theta_\IP\subset  \Theta_\phi$:} Consider $\Theta_\IP$ is verified, i.e.

    \begin{align*}
        & \IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})< 1-\alpha - \phiFl \\
        \Longrightarrow \hspace{0.1cm}& \IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})- \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-\hat{f}(X_i)|\leq \hat{t}_{lrn}\}< 1-\alpha - \phiFl - \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-\hat{f}(X_i)|\leq \hat{t}_{lrn}\}\\
        \Longrightarrow\hspace{0.1cm} & \IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})- \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-\hat{f}(X_i)|\leq \hat{t}_{lrn}\}< - \phiFl\\
        \Longrightarrow \hspace{0.1cm}& \Big|\IP(Y\in C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)|\calD^{lrn})- \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-\hat{f}(X_i)|\leq \hat{t}_{lrn}\}\Big| > \phiFl \\
        \Longrightarrow \hspace{0.1cm}& \Theta_\phi
    \end{align*}
    Where the second implication is obtained using the fact that by construction $\frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-\hat{f}(X_i)|\leq \hat{t}_{lrn}\} \geq 1-\alpha$.
    
    \underline{$\Theta_\lambda\subset  \Theta_\phi$:}

    Let us first show that $\Theta_\lambda$ implies that:

    \begin{equation}
        \label{eq:proof-omega}
        \frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-f^*_{1-\alpha+\phi}(X_i)|\leq t^*_{1-\alpha+\phi}\} < 1-\alpha
    \end{equation}
    Indeed, if we had $\frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-f^*_{1-\alpha+\phi}(X_i)|\leq t^*_{1-\alpha+\phi}\} \geq 1-\alpha$, then we would necessarily have $\hat{t}_{lrn} \leq t^*_{1-\alpha+\phi}$, since $\hat{t}_{lrn}$ is minimal over the empirical coverage constraint, which would imply that $\lambda\left(C_{\hat{f},\hat{t}_{lrn}}^{1-\alpha}(X)\right) \leq \lambda\left(C_{f_{1-\alpha+\phi}^*,t_{1-\alpha+\phi}^*}^{1-\alpha + \phi}(X)\right)$, i.e. that $\Theta_\lambda$ is not verified.

    It remains to show that~\eqref{eq:proof-omega} implies $\Theta_\phi$. By~\eqref{eq:proof-omega}, and using the fact that $\IP\Big(|Y-f^*_{1-\alpha+\phi}(X)|\leq t^*_{1-\alpha+\phi}\Big)\geq 1- \alpha + \phiFl$: 

    \begin{align*}
        &\frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-f^*_{1-\alpha+\phi}(X_i)|\} - \IP\Big(|Y-f^*_{1-\alpha+\phi}(X)|\leq t^*_{1-\alpha+\phi}\Big) < - \phiFl \\
        \Longrightarrow \hspace{0.1cm}& \Big|\frac{1}{n_\ell}\sum_{i=1}^{n_\ell}\1\{|Y_i-f^*_{1-\alpha+\phi}(X_i)|\} - \IP\Big(|Y-f^*_{1-\alpha+\phi}(X)|\leq t^*_{1-\alpha+\phi}\Big)\Big| > \phiFl \\
        \Longrightarrow \hspace{0.1cm}& \Theta_\phi
    \end{align*}
    This concludes the proof that $\Theta_\IP\cup\Theta_\lambda \subset \Theta_\phi$ and therefore Eq.~\eqref{eq:lower-scott-app} and~\eqref{eq:lower-scott-app2}.
    \end{proof}
 
    
    \underline{\textbf{Step 2}:} From Eq.~\eqref{eq:upper-f-given} in Prop.~\ref{prop:upper-f-given} we have that with probability greater than $1-\delta$:
    \begin{equation}
        \label{eq:upper-classic-app}
        \hat{t}\leq Q\left(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; |Y-\hat{f}(X)|_{|\calD^{lrn}}\right)
    \end{equation}
  With an abuse of notation, we therefore have $\IP(\{\eqref{eq:upper-classic-app}\})\geq 1-\delta$ and $\IP(\{\eqref{eq:lower-scott-app}\}\cap\{\eqref{eq:lower-scott-app2}\})\geq 1-\delta$. Therefore, using the union bound over the complementary events, we get that $\IP(\{\eqref{eq:upper-classic-app}\}\cap\{\eqref{eq:lower-scott-app}\}\cap\{\eqref{eq:lower-scott-app2}\})\geq 1-2\delta$. In the following, we show that if \eqref{eq:upper-classic-app}, \eqref{eq:lower-scott-app} and \eqref{eq:lower-scott-app2} are true, we have our final upper-bound, which will conclude the proof.
    
    The size of the intervals being equal to $2$ times their radius $t$, the objective here is to provide a high probability upper-bound on $\hat{t}$. Thanks to \eqref{eq:lower-scott-app2}, we have that $\hat{t}_{lrn} \leq t_{1-\alpha+\phi}^*$ and therefore:
    \begin{align*}
        \hat{t} = \hat{t} - \hat{t}_{lrn} + \hat{t}_{lrn} \leq \hat{t} - \hat{t}_{lrn} + t_{1-\alpha+\phi}^* =  t^* + \hat{t} - \hat{t}_{lrn} + t_{1-\alpha+\phi}^* - t^*
    \end{align*}

    \underline{We first control $\hat{t} - \hat{t}_{lrn}$}.

    %Applying the quantile function $Q(\cdot; |Y-\hat{f}(X)|_{|\calD^{lrn}})$ on both sides of~\eqref{eq:upper-classic-app}, we get that $\hat{t} \leq Q(1-\alpha + \frac{1}{n_c+1}+\sqrt{\frac{\log(1/\delta)}{n_c+1}}; |Y-\hat{f}(X)|_{|\calD^{lrn}})$. 
    
    Applying the quantile function $Q(\cdot; |Y-\hat{f}(X)|_{|\calD^{lrn}})$ on~\eqref{eq:lower-scott-app} gives $\hat{t}_{lrn}\geq Q(1-\alpha - \phiFl; |Y-\hat{f}(X)|_{|\calD^{lrn}})$. Hence, thanks~\eqref{eq:upper-classic-app} and to Assumption~\ref{ass:regularity}, we have: %$\hat{t} - \hat{t}_{lrn} \leq L(\frac{1}{n_c+1} + \phiFl)^\gamma\leq L(\frac{1}{(n_c+1)^\gamma} + \phiFl^\gamma)$.

    \begin{align*}
        \hat{t} - \hat{t}_{lrn} &\leq Q\Big(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}}; |Y-\hat{f}(X)|_{|\calD^{lrn}}\Big) - Q\Big(1-\alpha - \phiFl; |Y-\hat{f}(X)|_{|\calD^{lrn}}\Big) \\
        & \leq L\Big(\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(2/\delta)}{2n_c}} + \phiFl\Big)^\gamma\\
        &\leq L\Big(\frac{1}{n_c}+\sqrt{\frac{\log(2/\delta)}{2n_c}}\Big)^{\gamma} + L\phiFl^\gamma
    \end{align*}
    
    \underline{It remains to bound $t_{1-\alpha+\phi}^* - t^*$}. By definition, we have $t_{1-\alpha+\phi}^* = Q(1-\alpha + \phi; |Y-f_{1-\alpha+\phi}^*(X)|)$, and $t^* = Q(1-\alpha; |Y-f^*(X)|)$. Moreover, we notice that $Q(1-\alpha + \phi; |Y-f_{1-\alpha+\phi}^*(X)|) \leq Q(1-\alpha + \phi; |Y-f^*(X)|)$ since by definition $f_{1-\alpha+\phi}^*$ minimizes $Q(1-\alpha + \phi; |Y-f(X)|)$ over all $f\in\calF$.
     In the end, by Assumption~\ref{ass:regularity} we have:
     
     \begin{equation*}
        t_{1-\alpha+\phi}^* - t^* \leq Q(1-\alpha + \phi; |Y-f^*(X)|) - Q(1-\alpha; |Y-f^*(X)|) \leq L\phiFl^\gamma\;.
     \end{equation*}

     %$t_{1-\alpha+\phi}^* - t^* \leq Q(1-\alpha + \phi; |Y-f^*(X)|) - Q(1-\alpha; |Y-f^*(X)|) \leq L\phiFl^\gamma$, by Assumption~\ref{ass:regularity}. 
     
     
     We conclude the proof using the fact that $\lambda(C_{\hat{f},\hat{t}}^{1-\alpha}(X)) = 2\hat{t}$ and $\lambda(C_{f^*,t ^*}^{1-\alpha}(X)) = 2t^*$. \qed
    

\section{Additional Results}

\subsection{The Nested Sets View}
\label{sec:nested}

The split CP method described in Section~\ref{sec:conform-background} can also be described through the notion of \emph{nested sets} \citep{gupta2022nested}, which encapsulates many types of prediction sets, base predictors and scoring functions considered in the literature. As claimed in Remark~\ref{rmk:nested}, this framework will allow us to generalize the results of Section~\ref{sec:f-given} to a wider class of prediction sets.

In the nested set view, we consider the class of prediction sets $\calC^{\text{nested}}_{\calF,\calT} = \{C_{f,t}(x) \text{ nested }; f \in \calF, t\in\calT\subset\IR\}$, where `nested' means that for any fixed $f\in\calF$ and $x\in\calX$, $C_{f,t}(x)\subset C_{f,t'}(x)$ as soon as $t\leq t'$. Here, we consider a fixed base predictor $f$, but as usual, $f$ is learned during the learning stage of the split method. In this setting, we can define the following general scoring function:
\begin{equation*}
    s_f(x,y) = \inf\{t\in\calT : y\in C_{f,t}(x)\}\;.
\end{equation*}
Then, the procedure is the same as in Section~\ref{sec:conform-background}: compute the nonconformity scores 
$S_i := s_{f}(X_i,Y_i)$, $i \in \intset{n_c}$ and find the $\lceil (n_c+1)(1-\alpha) \rceil$-th smallest one 
$\hat{q}_{1-\alpha}:=S_{(\lceil (n_c+1)(1-\alpha) \rceil)}$. Finally, for any $x\in \calX$, the prediction set is $C_{f,\hat{q}_{1-\alpha}}(x)$. As usual, the marginal guarantee is satisfied \citep[Prop. 1]{gupta2022nested}.

As mentioned above, an interesting aspect of this nested set framework is that it encapsulates many split CP approaches \citep[Table 1]{gupta2022nested} such as the ones of Example~\ref{exemple:base-predictor}, as shown in the following Example. 


\begin{exemple}\emph{(Nested sets view of Example~\ref{exemple:base-predictor}).}
    %\label{exemple:base-predictor}
    %\bat{Revoir notation $f$,$f_1$ etc.}
    \begin{enumerate}
        \item The original \emph{Split CP} \citep{papadopoulos2002inductive} is recovered in the nested set framework by taking $f=\{\mu\}$, $C_{\mu,t}(x) = [\mu(x)-t;\mu(x)+t]$ and $\calT=[0,\infty)$.
        %
        \item In \emph{Locally-Weighted Conformal Inference} \citep{papadopoulos2008normalized}, $f=\{\mu,\sigma\}$, $C_{f,t}(c)=[\mu(x)-\sigma(x)t;\mu(x)+\sigma(x)t]$ and $\calT=[0,\infty)$.
        %
        \item In \emph{Conformalized Quantile Regression} (CQR) \citep{romano2019conformalized}, we have $f=\{Q_\alpha,Q_{1-\alpha}\}$, $C_{f,t}(x)=[Q_{\alpha}(x) - t;Q_{1-\alpha}(x) + t]$ and $\calT=\IR$.
    \end{enumerate}
\end{exemple}

We can now extend our results from Section~\ref{sec:f-given} to the nested framework, aiming at showing that, when $f$ is given, the conformal step indeed minimizes the size of the prediction set, up to an error that vanishes as $n_c$ grows.

To this aim, we need the following additional assumption on the way the size of the nested set grows with $t$. %More precisely, we require it to evolve linearly with $t$.

\begin{assumption}\emph{(Linear growth of the size.)} \label{ass:linear-size} $\forall f\in\calF$, $\exists a,b>0$ such that $\EE[\lambda(C_{f,t}(X))] = at+b$.
\end{assumption}

If we take the three previous examples, we have in 1) $a=2$ and $b=0$, 2) $a=2\EE[\sigma(X)]$ and $b=0$, and 3) $a=2$ and $b=\EE[Q_{1-\alpha}(X)-Q_{\alpha}(X)]$.


Over $\calC^{\text{nested}}_{\calF,\calT}$ and under Assumption~\ref{ass:linear-size}, when $f$ is fixed the optimization problem~\eqref{eq:formal-opt} becomes:
\begin{align*}
    \min_{t \geq 0} &\; at+b \\
     \quad \text{s.t.} \quad & \IP(Y\in C_{f,t}(X)) \geq 1-\alpha\;, \nonumber
\end{align*}
which has the same solution as:
\begin{align*}
    \min_{t \geq 0} &\; t \label{eq:obj-nested} \\
     \quad \text{s.t.} \quad & \IP(s_f(X,Y)\leq t) \geq 1-\alpha\;, \nonumber
\end{align*}
with solution $t^*=Q(1-\alpha; s_f(X,Y))$. Similarly, the conformal step solves an empirical version of the previous problem:
\begin{align*}
    \min_{t \geq 0} &\; t  \\
     \quad \text{s.t.} \quad & \frac{1}{n_c}\sum_{i=1}^{n_c}\1\{s_f(X_i,Y_i)\leq t\} \geq (1-\alpha)(n_c+1)/n_c\; \nonumber
\end{align*}
with solution $\hat{t} = \widehat{Q}((1-\alpha)(n_c+1)/n_c;\{s_f(X_i,Y_i)\}_{i=1}^{n_c})$. As in Section~\ref{sec:f-given}, controlling the volume sub-optimality is equivalent to control the error of an empirical quantile estimate, and we can provide a very simple extension of Proposition~\ref{prop:upper-f-given} and Corollary~\ref{cor:f-given}.

\begin{proposition}
    %\label{prop:upper-f-given}
    Let $\hat{t} = \widehat{Q}((1-\alpha)\frac{n_c+1}{n_c};\{s_f(X_i,Y_i)\}_{i=1}^{n_c})$ and $C_{f,\hat{t}}(x)$ the corresponding (nested) prediction set. If Assumption \ref{ass:linear-size} holds, the points in $\calD^{cal}$ are i.i.d., and $(n_c+1)(1-\alpha)$ is not an integer, then with probability greater than $1-\delta$ we have:
    \begin{equation*}
        %\label{eq:upper-f-given}
        \EE\Big[\lambda\Big(C_{f,\hat{t}}(X)\Big) \Big| \calD^{lrn}\Big] \leq a\times Q\left(1-\alpha + \frac{1-\alpha}{n_c} + \sqrt{\frac{\log(1/\delta)}{2n_c}}; S\right) + b\;.
    \end{equation*}

    Moreover, if Assumption~\ref{ass:regularity} is true for $S=s_f(X,Y)$ and if $n_c$ is large enough so that $\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(1/\delta)}{2n_c}} \leq r$, then with probability greater than $1-\delta$ we have:
    \begin{equation*}
        %\label{eq:upper-f-given-final}
        \EE\Big[\lambda\Big(C_{f,\hat{t}}(X)\Big)\Big| \calD^{lrn}\Big] \leq \EE\Big[\lambda\Big(C_{f,t^*}(X)\Big)\Big] + a L\left(\frac{1-\alpha}{n_c} + \sqrt{\frac{\log(1/\delta)}{2n_c}}\right)^\gamma\;.
    \end{equation*} 
\end{proposition}

\begin{proof}
    The proof is essentially the same as the one of Proposition~\ref{prop:upper-f-given} and Corollary~\ref{cor:f-given}, and is therefore omitted.
\end{proof}


\subsection{Closed-form expressions for $\phiF$}
\label{sec:closed-form-phi}
In Proposition~\ref{prop:phi-finite} we give a closed form expression for $\phiF$ in the case of finite function class $\calF$. The proof is given hereafter.

%\subsubsection{Proof of Proposition~\ref{prop:phi-finite}}

\begin{proof}[Proof of Proposition~\ref{prop:phi-finite}]
    Let $\varepsilon>0$, 
    \begin{align*}
        & \IP\left(\sup_{t\geq 0, f\in\calF}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}\Big|> \varepsilon\right) \\
         & =\IP\left(\underset{f\in\calF}{\cup}\left\{\sup_{t\geq 0}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}\Big|> \varepsilon\right\}\right) \\
         & \leq \sum_{f\in\calF}\IP\left(\sup_{t\geq 0}\Big|\IP\left(|Y-f(X)|\leq t\right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|\leq t\}\Big|> \varepsilon\right) \\
         & \leq 2|\calF|e^{-2n\varepsilon^2}\;,
    \end{align*}
    where in the last inequality we use the DKW inequality, and the fact that $\calF$ is finite. Finally, taking $\varepsilon = \sqrt{\frac{\log(2|\calF|/\delta)}{2n}}$ concludes the proof.
\end{proof}

Other closed-form expressions can be obtained for infinite function classes using the classical notions of Rademacher complexity and VC dimension, as shown below.

Let $\widetilde{\calF} = \{(x,y)\mapsto \1\{|y-f(x)|\leq t\} : f\in\calF, t\geq 0 \}$. The Rademacher complexity of the function class $\widetilde{\calF}$ is the quantity 
\begin{equation*}
    \calR_n(\widetilde{\calF})=\EE_{\calD,\epsilon}\Big[\sup_{f\in\calF,t\geq 0}\frac{1}{n}\sum_{i=1}^n\epsilon_i\1\{|Y_i-f(X_i)|\leq t\}\Big]\;,
\end{equation*}
where $\epsilon_1,\ldots,\epsilon_n$ are Rademacher random variables. Then, a direct extension of the proof of Theorem 3.3 in \citet{mohri2018foundations} gives the closed-form $\phiF = 2\calR_n(\widetilde{\calF})+\sqrt{\frac{\log(1/\delta)}{2n}}$. %\bat{C'est vraiment la même preuve, si ce n'est une valeur absolue qui traine mais qui ne change rien. A voir si on ajoute la preuve "for completeness".} %The only difference being that the bilateral McDiarmid's inequality is applied, instead of the unilateral one.

It is also possible to bound the Rademacher complexity of $\widetilde{\calF}$, first in terms of its associated Growth function (Massart's Lemma), and then in terms of its VC dimension, denoted $\text{VC}(\widetilde{\calF})$ (Sauer's Lemma). Applying Corollary 3.8 and Corollary 3.18 in \citet{mohri2018foundations} gives the closed-form $\phiF = \sqrt{\frac{8\text{VC}(\widetilde{\calF})\log(en/\text{VC}(\widetilde{\calF}))}{n}}+\sqrt{\frac{\log(1/\delta)}{2n}}$.

It should be noted that more informative close-forms could be obtained by specifying the function class of $\calF$. For instance we could fix $\calF$ to be the set of linear functions. 




\subsection{Algorithm with excess volume loss in the adaptive size setting} %\bat{CHANGER TITRE ?}
\label{sec:adapt-bonus}


%\hspace{5cm}{\LARGE\textbf{\bat{V2: To Discuss with Pierre}}}


In Section~\ref{sec:adap-oracle}, if $\forall s\in \calS$ and $t\geq 0$ we have $s+t\in\calS$ (stability with addition of a scalar), then the oracle problem is equivalent to:
\begin{align*}
    \min_{f\in\calF,s\in\calS,t\geq 0} &\; \EE[s(X)] + t \\
     \quad \text{s.t.} \quad &\;  \IP(|Y-f(X)| - s(X)\leq t) \geq 1-\alpha
     \;. \nonumber
\end{align*}

In practice, we propose to use~\methodAD, however in order to obtain theoretical results similar to that of Theorem~\ref{thme:main-constant}, another possibility would be to solve, during the learning step, an empirical version of the previous oracle problem, which is complicated to apply in practice:
\begin{align}
    \min_{f\in\calF,s\in\calS,t\geq 0} &\; \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}s(X_i) + t \label{eq:new-algo-adap} \\
     \quad s.t. \quad &\;  \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f(X_i)|-s(X_i)\leq t\}\geq 1-\alpha - \phiFSl
     \;, \nonumber
\end{align}
where $\phiFSl$ is a penalty term relaxing the coverage constraint in order to obtain a smaller prediction set. This term corresponds to the statistical error of the empirical coverage, explicitly defined in the following assumption, which is necessary to derive a result similar to that of Theorem~\ref{thme:main-constant}.

\begin{assumption} \label{ass:complexity-adv2} There exists two quantities $\phiFS<+\infty$ and $\psiS<+\infty$ such that:
    \begin{equation*}
        \IP\left(\sup_{f\in\calF,s\in\calS,t\geq 0}\Big|\IP\left(|Y-f(X)|-s(X)\leq t \right) - \frac{1}{n}\sum_{i=1}^n\1\{|Y_i-f(X_i)|-s(X_i)\leq t\}\Big|\leq \phiFS\right)\geq 1-\delta
    \end{equation*}
    and
    \begin{equation*}
        \IP\left(\sup_{s\in\calS}\Big|\EE[s(X)] - \frac{1}{n}\sum_{i=1}^n s(X_i)\Big|\leq \psiS\right)\geq 1-\delta \; .
    \end{equation*}
\end{assumption}

In words, this assumption generalizes Assumption~\ref{ass:complexity} to the adaptive size setting, at least for the first equation. Since in this setting we also estimate the expectation of the size, we need the second equation to make sure that its worst-case estimation error is bounded w.h.p. Closed-form expressions for $\phiFS$ and $\psiS$ can be obtained similarly as in Appendix~\ref{sec:closed-form-phi}.

Denote by $(\hf,\hs,\hat{t})$ the solutions of the empirical problem, $(f^*,s^*,t^*)$ the solutions of the oracle one, and $\forall (f,s,t)$, denote $C_{f,s,t}(x)=[f(x)-s(x)-t,f(x)+s(x)+t]$. 
Under Assumption~\ref{ass:complexity-adv2}, we can derive the following lemma, which is an extension of the result obtained at the end of Step 1 in the proof of Theorem~\ref{thme:main-constant}.

\begin{lemma}
    \label{lem:scott-adapv2}
    Under Assumption~\ref{ass:complexity-adv2}, we have with probability greater than $1-2\delta$:    
    \begin{equation}
        \label{eq:lower-scott-app-ADv2}
        \IP(Y\in C_{\hf,\hs,\hat{t}}^{1-\alpha}(X)|\calD^{lrn})\geq 1-\alpha - 2\phiFSl
    \end{equation}
    and
    \begin{equation}
        \label{eq:lower-scott-app2-ADv2}
        \EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}}^{1-\alpha}(X)\right)\Big|\calD^{lrn}\right] \leq \EE\left[\lambda\left(C_{f^*,s^*,t^*}^{1-\alpha}(X)\right)\right] + 4\psiSl\;.
    \end{equation}
    % where $\phi \equiv \phiFSl$ and $C_{f_{1-\alpha+\phi}^*,s_{1-\alpha+\phi}^*}^{1-\alpha + \phi}(X)$ denotes the optimal oracle interval with increased coverage $1-\alpha + \phiFSl$. In other word, $f_{1-\alpha+\phi}^*$ and $s_{1-\alpha+\phi}^*$ are the solutions of $\min_{f\in\calF, S\in \calS}~ \EE[s(X)]$ s.t. $\IP(|Y-f(X)|\leq t)\geq 1-\alpha + \phiFl$.
\end{lemma}

\begin{proof}
    The proof closely follows the one of Step 1 in Appendix~\ref{app:proof-main}.
    Let:
    \begin{itemize}
        \item $\Theta_\IP=\Big\{\IP(Y\in C_{\hf,\hs,\hat{t}}^{1-\alpha}(X)|\calD^{lrn})< 1-\alpha - 2\phiFSl\Big\}$
        \item $\Theta_\lambda=\Big\{\EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}}^{1-\alpha}(X)\right)\Big|\calD^{lrn}\right] > \EE\left[\lambda\left(C_{f^*,s^*,t^*}^{1-\alpha + \phi}(X)\right)\right] + 4\psiSl\Big\}$
        \item $\Theta_\phi=\Big\{\underset{f\in\calF,s\in\calS,t\geq 0}{\sup}\Big|\IP\left(|Y-f(X)|-s(X)\leq t \right) - \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f(X_i)|-s(X_i)\leq t\}\Big|> \phiFSl\Big\}$
        \item $\Theta_\psi = \Big\{\sup_{s\in\calS}\Big|\EE[s(X)] - \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}} s(X_i)\Big|> \psiSl\Big\}$
    \end{itemize}

The objective is to show that $(\Theta_\IP\cup\Theta_\lambda) \subset (\Theta_\phi \cup \Theta_\psi)$. Indeed, using the union bound and Assumption~\ref{ass:complexity-adv2}, this would imply that $\IP(\Theta_\IP\cup\Theta_\lambda)\leq \IP(\Theta_\phi\cup \Theta_\psi)\leq 2\delta$, concluding the proof.

\underline{$\Theta_\IP\subset  (\Theta_\phi \cup \Theta_\psi)$:} Proved by showing that $\Theta_\IP\subset \Theta_\phi$ using the same arguments as in the proof of the main result.

\underline{$\Theta_\lambda\subset (\Theta_\phi \cup \Theta_\psi)$:}

Let the event $\Omega=\Big\{\frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f^*(X_i)| - s^*(X_i)\leq t^* \}<1-\alpha - \phiFSl\Big\}$. We first show that $\Theta_\lambda\subset (\Omega \cup \Theta_\psi)$, by proving that $(\Omega^c \cap \Theta^c_\psi)\subset \Theta_\lambda^c$. Indeed, under $(\Omega^c \cap \Theta^c_\psi)$ we have:

\begin{align*}
    &\frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f^*(X_i)| - s^*(X_i)\leq t^* \} \geq 1-\alpha -\phiFSl \\
    \Longrightarrow\hspace{0.1cm} & \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\hs(X_i) + \hat{t} \leq \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}s_{1-\alpha+\phi}^*(X_i) + t^* \\
    \Longrightarrow\hspace{0.1cm} & \EE[\hs(X)|\calD^{lrn}]+ \hat{t}+\frac{1}{n_\ell}  \sum_{i\in\calD^{lrn}}\hs(X_i) - \EE[\hs(X)|\calD^{lrn}] \leq \EE[s_{1-\alpha+\phi}^*(X)] + t^*+ \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}s_{1-\alpha+\phi}^*(X_i) -  \EE[s_{1-\alpha+\phi}^*(X)]\\
    \Longrightarrow\hspace{0.1cm} & \EE[\hs(X)|\calD^{lrn}] +\hat{t} \leq \EE[s_{1-\alpha+\phi}^*(X)]+ t^* + 2\sup_{s\in\calS}\Big|\EE[s(X)] - \frac{1}{n_\ell}\sum_{i\in\calD^{lrn}} s(X_i)\Big| \\
    \Longrightarrow\hspace{0.1cm} & \EE[\hs(X)|\calD^{lrn}]  +\hat{t}\leq \EE[s_{1-\alpha+\phi}^*(X)] + t^*+ 2\psiSl\\
    \Longrightarrow\hspace{0.1cm} & 2\EE[\hs(X)|\calD^{lrn}] +2\hat{t} \leq 2\EE[s_{1-\alpha+\phi}^*(X)] + 2t^*+ 4\psiSl\Longrightarrow \Theta_\lambda^c \, .
\end{align*}

It remains to prove that $\Omega \subset \Theta_\phi$. Under $\Omega$ and using the fact that $\IP\Big(|Y-f^*(X)|-s^*(X)\leq t ^*\Big)\geq 1- \alpha$: 

    \begin{align*}
        &\frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f^*(X_i)| - s^*(X_i)\leq t^* \} - \IP\Big(|Y-f^*(X)|-s^*(X)\leq t ^*\Big) < - \phiFSl \\
        \Longrightarrow \hspace{0.1cm}& \Big|\frac{1}{n_\ell}\sum_{i\in\calD^{lrn}}\1\{|Y_i-f^*(X_i)| - s^*(X_i)\leq t^* \} - \IP\Big(|Y-f^*(X)|-s^*(X)\leq t ^*\Big)\Big| > \phiFSl \\
        \Longrightarrow \hspace{0.1cm}& \Theta_\phi \; .
    \end{align*}
    Hence $\Omega \subset \Theta_\phi$, i.e. $\Theta_\lambda\subset (\Omega \cup \Theta_\psi)\subset (\Theta_\phi \cup \Theta_\psi)$, which concludes the proof.
\end{proof}

Like after step 1 of~\method, with Lemma~\ref{lem:scott-adapv2} we have probabilistic guarantees on the coverage and on the expected size of the returned set. Using conformal prediction, we can now obtain an almost sure guarantee on the coverage, at the cost of slightly increasing the size of the set by $\hat{t}_c = \widehat{Q}\Big((1-\alpha)\frac{n_c+1}{n_c};\{|Y_i-\hat{f}(X_i)|-\hat{s}(X_i)\}_{i\in\calD^{cal}}\Big)$.


\begin{theorem}
    \label{thme:main-adap}
    Consider that Assumption~\ref{ass:regularity} is satisfied for $S=|Y-f(X)|-s(X)$. Assume further that Assumption~\ref{ass:complexity-adv2} is verified, that the distribution of $Y$ is atomless, that $n_c$ and $n_\ell$ are large enough so that $\frac{1}{n_c+1} +\sqrt{\frac{\log(1/\delta)}{n_c+1}}\leq r $ and $\phiFSl\leq r$, then we have:
    \begin{enumerate}[leftmargin=*]
        \item $\IP(Y\in C_{\hat{f},\hs,\hat{t}_c}^{1-\alpha}(X)|\calD^{lrn})\geq 1-\alpha$ a.s. %\todo{pas besoin de toutes les assumptions ici}
        \item With probability greater that $1-3\delta$:
        \begin{equation}
            \label{eq:thme-adap}
            \hspace{-0.3cm}\EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}_c}^{1-\alpha}(X)\right)\Big|\calD^{lrn},\calD^{cal}\right] \leq \EE\left[\lambda\left(C_{f^*,s^*,t^*}^{1-\alpha}(X)\right)\right] + 4\psiSl + 2L\Big(\frac{1}{n_c+1}+\sqrt{\frac{\log(1/\delta)}{n_c+1}} + 2\phiFSl\Big)^\gamma.
        \end{equation}
    \end{enumerate}
\end{theorem}


\begin{proof}
    Like in Theorem~\ref{thme:main-constant}, the first point of Theorem~\ref{thme:main-adap}, on the almost sure coverage guarantee, is a classical result of the conformal prediction literature.
    
    

    We start the proof of the second point by recalling that since the distribution of $Y$ is assumed atomless, we have with probability greater than $1-\delta$:
    \begin{equation}
        \label{eq:upper-classic-appv2}
        \IP( |Y-\hat{f}(X)| -\hs(X) \leq \hat{t}_c \big|\calD^{lrn},\calD^{cal})\leq 1-\alpha + \frac{1}{n_c+1} +\sqrt{\frac{\log(1/\delta)}{n_c+1}}\;.
    \end{equation}
    See Section 2.1 and Proposition 24 in \citet{humbert2024marginal} for details on this result.
    Like in the proof of Theorem~\ref{thme:main-constant}, we have $\IP(\{\eqref{eq:upper-classic-appv2}\}\cap\{\eqref{eq:lower-scott-app-ADv2}\}\cap\{\eqref{eq:lower-scott-app2-ADv2}\})\geq 1-3\delta$, and it suffices to show that if \eqref{eq:upper-classic-appv2}, \eqref{eq:lower-scott-app-ADv2} and \eqref{eq:lower-scott-app2-ADv2} are true, we have our final upper-bound.

    We have $\EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}_c}^{1-\alpha}(X)\right)\Big|\calD^{lrn},\calD^{cal}\right] = \EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}}^{1-\alpha}(X)\right)\Big|\calD^{lrn}\right] - 2\hat{t} + 2\hat{t}_c$. With~\eqref{eq:lower-scott-app2-ADv2} we have an upper-bound on $\EE\left[\lambda\left(C_{\hat{f},\hs,\hat{t}}^{1-\alpha}(X)\right)\Big|\calD^{lrn}\right]$, and it remains to show that $\hat{t}_c - \hat{t} \leq L\left(\frac{1}{(n_{c}+1)^\gamma} + 2\phiFl^\gamma\right)$.



Applying the quantile function $Q(\cdot; |Y-\hat{f}(X)|-\hs(X)_{|\calD^{lrn}})$ on~\eqref{eq:upper-classic-appv2}, we get that $\hat{t}_c \leq Q(1-\alpha + \frac{1}{n_c+1}+\sqrt{\frac{\log(1/\delta)}{n_c+1}}; |Y-\hat{f}(X)|-\hs(X)_{|\calD^{lrn}})$. Similarly, applying it on~\eqref{eq:lower-scott-app-ADv2} gives $\hat{t}\geq Q(1-\alpha - 2\phiFSl; |Y-\hat{f}(X)| - \hs(X))_{|\calD^{lrn}}$. Hence, thanks to the regularity condition, we have: %$\hat{t} - \hat{t}_{lrn} \leq L(\frac{1}{n_c+1} + \phiFl)^\gamma\leq L(\frac{1}{(n_c+1)^\gamma} + \phiFl^\gamma)$.

    \begin{align*}
        \hat{t}_c - \hat{t} & \leq L\Big(\frac{1}{n_c+1}+\sqrt{\frac{\log(1/\delta)}{n_c+1}} + 2\phiFSl\Big)^\gamma \; .
    \end{align*}
    
    % \underline{It remains to bound $t_{1-\alpha+\phi}^* - t^*$}. By definition, we have $t_{1-\alpha+\phi}^* = Q(1-\alpha + \phi; |Y-f_{1-\alpha+\phi}^*(X)|)$, and $t^* = Q(1-\alpha; |Y-f^*(X)|)$. Moreover, we notice that $Q(1-\alpha + \phi; |Y-f_{1-\alpha+\phi}^*(X)|) \leq Q(1-\alpha + \phi; |Y-f^*(X)|)$ since by definition $f_{1-\alpha+\phi}^*$ minimizes $Q(1-\alpha + \phi; |Y-f(X)|)$ over all $f\in\calF$.
    %  In the end, by Assumption~\ref{ass:regularity} we have:
     
    %  \begin{equation*}
    %     t_{1-\alpha+\phi}^* - t^* \leq Q(1-\alpha + \phi; |Y-f^*(X)|) - Q(1-\alpha; |Y-f^*(X)|) \leq L\phiFl^\gamma\;.
    %  \end{equation*}

     %$t_{1-\alpha+\phi}^* - t^* \leq Q(1-\alpha + \phi; |Y-f^*(X)|) - Q(1-\alpha; |Y-f^*(X)|) \leq L\phiFl^\gamma$, by Assumption~\ref{ass:regularity}. 
     
     
     %We conclude the proof using the fact that $\lambda(C_{\hat{f},\hat{t}}^{1-\alpha}(X)) = 2\hat{t}$ and $\lambda(C_{f^*,t ^*}^{1-\alpha}(X)) = 2t^*$.
\end{proof}
