\section{Results Beyond Squared Loss} \label{subsec:recover_quantify}

In regression problems under some assumptions, \citet{charikar2024quantifying} proves that the strong model’s error is smaller than the weak model’s, with the gap at least the strong model’s error on the weak labels.
This observation naturally raises the following question:
\textit{Can their proof be extended from squared loss to output distribution divergence?}
In this section, we show how to theoretically bridge the gap between squared loss and KL divergence within the overall proof framework established in~\citet{charikar2024quantifying}.
% Interestingly, our analysis reveals that employing KL divergence as the loss function can potentially lead to a reduction in the reverse KL divergence, and vice versa.
To begin with, we restate an assumption used in previous study.
\begin{assumption}[Convexity Assumption~\citep{charikar2024quantifying}] \label{convex_set}
The strong model learns fine-tuning tasks from a function class $\cF_{s}$, which is a convex set. 
\end{assumption}
\vspace{-5pt}
It requires that, for any $f, g \in \cF_s$, and for any $\lambda \in [0,1]$, there exists $h \in \cF$ such that for all $z \in \R^{d_s}$, $h(z) = \lambda f(z) + (1-\lambda) g(z)$. 
% And we do not assume anything about either $f^\star$ or $f_w$, which need not belong to $\cF$. 
To satisfy the convex set assumption, $\cF_s$ can be the class of all linear functions.
In these cases, $\cF_s$ is a convex set. 
Note that it is validated by practice: a popular way to fine-tune a pre-trained model on task-specific data is by tuning the weights of only the last linear layer of the model~\citep{howard2018universal,kumar2022fine}.

\subsection{Upper Bound (Realizability)} \label{subsub:realize}

Firstly, we consider the case where $\exists f_s \in \cF_s$ such that $F_s = F^\star$ (also called ``Realizability''~\citep{charikar2024quantifying}).
It means we can find a $f_s$ such that $f_s \circ h_s = f^\star \circ h^\star$.
This assumption implicitly indicates the strong power of pre-training. 
It requires that the representation $h_s$ has learned extremely enough information during pre-training, which is reasonable in modern large language models pre-trained on very large corpus~\citep{touvron2023llama,achiam2023gpt}.
The scale and diversity of the corpus ensure that the model is exposed to a broad spectrum of lexical, syntactic, and semantic structures, enhancing its ability to generalize effectively across varied language tasks.

We state our result in the realizable setting, which corresponds to Theorem 1 in~\citet{charikar2024quantifying}.
\begin{theorem}[Proved in \cref{proof_theorem_1-main}]
\label{thm:realizable-main}

Given $F^\star$, $F_w$ and $F_{sw}$ defined above.
Consider $\cF_s$ that satisfies Assumption~\ref{convex_set}. 
Consider WTSG using reverse KL divergence loss:
\begin{align*}
    f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(f \circ h_s, f_w \circ h_w).
\end{align*}
Assume that $\exists f_s \in \cF_s$ such that $F_s = F^\star$.
Then
\begin{align} \label{eqn:realizable-main}
    \dist(F^\star, F_{sw}) \le \dist(F^\star, F_w) - \dist(F_{sw}, F_w).
\end{align}
\end{theorem}

\begin{remark}
    The corresponding theorem and proof in the case of forward KL divergence loss is provided in~\cref{thm:realizable} from~\cref{proof_theorem_1}, under an additional assumption.
\end{remark}

In contrast to the symmetric squared loss studied in prior work~\citep{charikar2024quantifying}, the emergence of the reverse KL divergence is inherently tied to the asymmetric properties of the KL divergence.
Although extending previous work to both forward and reverse KL divergences presents significant technical challenges, our results demonstrate the theoretical guarantees of WTSG in these settings.
In Inequality~\eqref{eqn:realizable-main}, the left-hand side represents the error of the weakly-supervised strong model on the true data. 
On the right-hand side, the first term denotes the true error of the weak model, while the second term captures the disagreement between the strong and weak models, which also serves as the minimization objective in WTSG. 
This inequality indicates that the weakly-supervised strong model improves upon the weak model by at least the magnitude of their disagreement, $\dist(F_{sw}, F_w)$.
To reduce the error of $F_{sw}$, \cref{thm:realizable-main} aligns with~\cref{lemma:upper_lower_inf}, highlighting the importance of selecting an effective weak model and the inherent limitations of the optimization objective in WTSG.


% Notice that the error of weak model and strong model in~\cref{thm:realizable-main} is the reverse version, which fundamentally stems from the asymmetric properties of KL divergence.
% Despite this subtle difference, our empirical results in the experiments demonstrate consistent trends between forward and reverse KL divergence.











\subsection{Upper Bound (Non-Realizability)}

Now we relax the ``realizability'' condition and draw $n$ i.i.d. samples to perform WTSG.
We provide the result in the ``unrealizable'' setting, where the condition $F_s = F^\star$ may not be satisfied for any $f_s \in \cF_s$.
It corresponds to Theorem 2 in~\citet{charikar2024quantifying}.

\begin{theorem}[Proved in~\cref{proof_non-realizable-main}] \label{thm:non-realizable-finite-samples-main}
Given $F^\star$, $F_w$ and $F_{sw}$ defined above.
Consider $\cF_s$ that satisfies~\cref{convex_set}.
Consider weak-to-strong generalization using reverse KL:
\begin{align*}
    & f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(f \circ h_s, f_w \circ h_w),
    \\ & \hat{f}_{sw} = \argmin_{f \in \cF_{s}}\; \hat{d}_{\cP}(f \circ h_s, f_w \circ h_w),
\end{align*}
Denote $\dist(F^\star, F_s) = \eps$. 
With probability at least $1-\delta$ over the draw of $n$ i.i.d. samples, there holds
\begin{multline} 
\dist(F^\star, \hat{F}_{sw}) \le \dist(F^\star, F_w) - \dist(\hat{F}_{sw}, F_w) + \\ \cO(\sqrt{\eps}) +  \cO\left(\sqrt{\frac{\cC_{\cF_s}}{n}}\right) + \cO\left(\sqrt{\frac{\log(1/\delta)}{n}}\right),
\end{multline}
where $\cC_{\cF_s}$ is a constant capturing the complexity of the function class $\cF_s$, and the asymptotic notation is with respect to $\eps \to 0, n \to \infty$.
\end{theorem}

\begin{remark}
    The extension to forward KL divergence loss is provided in~\cref{thm:non-realizable-finite-samples} from~\cref{proof_non-realizable}, under an additional assumption.
\end{remark}


Compared to Inequality~\eqref{eqn:realizable-main}, this bound introduces two another error terms: the first term of $\cO(\sqrt{\eps})$ arises due to the non-realizability assumption, and diminishes as the strong ceiling model $F_s$ becomes more expressive.
The remaining two error terms arise from the strong model $\hat{F}_{sw}$ being trained on a finite weakly-labeled sample. They also asymptotically approach zero as the sample size increases.







\begin{figure*}[t]
  \centering
  \subfigure[Realizable (pre-training).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/realizable-pretrain.pdf}
  }
  \label{fig3:a}
  \subfigure[Non-realizable (pre-training).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/unrealizable-pretrain.pdf}
  }
  \subfigure[Non-realizable (perturbation).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/perturb.pdf}
  }
  \subfigure[Realizable (pre-training).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/realizable-pretrain_forward.pdf}
  }
  \subfigure[Non-realizable (pre-training).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/unrealizable-pretrain_forward.pdf}
  }
  \subfigure[Non-realizable (perturbation).]{
    \includegraphics[width=0.31\textwidth]{images/exp_2/perturb_forward.pdf}
  }
  \vspace{-5pt}
  \caption{Experiments on synthetic data using reverse KL divergence loss (\textbf{a-c}) and forward KL divergence loss (\textbf{d-f}). 
  Each point corresponds to a task and the gray dotted line represents $y=x$. 
  $h^{\star}$ is a 16-layer MLP. 
  (\textbf{a,d}) Realizable (pre-training): $h_s=h^\star$, and $h_w$ is a 2-layer MLP obtained by pre-training. (\textbf{b,e}) Non-realizable (pre-training): $h_s$ is an 8-layer MLP, and $h_w$ is a 2-layer MLP. Both $h_s$ and $h_w$ are obtained by pre-training. (\textbf{c,f}) Non-realizable (perturbation): Both $h_s$ and $h_w$ are obtained by directly perturbing the weights in $h^{\star}$:  $h_s=h^{\star}+ \mathcal{N}\left(0,0.01\right)$, and $h_w=h^{\star}+\mathcal{N}\left(0,9\right)$.}
  \label{syn_result:reverse}
  \vspace{-10pt}
\end{figure*}




\subsection{Synthetic Experiments} \label{section:syn_exp}
In this section, we conduct experiments on synthetically generated data to validate the theoretical results in~\cref{subsec:recover_quantify}.
While drawing inspiration from the theoretical framework of~\citet{charikar2024quantifying}, we extend their synthetic experiments by replacing the squared loss used in their work with the output distribution divergence defined in~\cref{def:kl_dist_emp}.


\subsubsection{Experimental Setting}

In our setup, The data distribution $\mathcal{P}$ is chosen as $\mathcal{N}(0, \sigma^2 I)$, with $\sigma=500$ to ensure the data is well-dispersed.
The ground truth representation $h^\star:\R^8 \to \R^{16}$ is implemented as a randomly initialized 16-layer multi-layer perceptron (MLP) with ReLU activations.
Let the weak model and strong model representations $h_w, h_s: \R^8 \to \R^{16}$ be 2-layer and 8-layer MLP with ReLU activations, respectively.
Given $h_w$ and $h_s$ frozen, both the strong and weak models learn from the fine-tuning task class $\cF_s$, which consists of linear functions mapping $\R^{16}\to\R$.
This makes $\cF_s$ a convex set. 

For the ``realizable'' setting, we set $h_s = h^\star$.
For the ``unrealizable'' setting, we adopt the approach of~\citet{charikar2024quantifying} and investigate two methods for generating weak and strong representations: (1) \textbf{Pre-training}: 20 models $f_1^\star,\dots,f_{20}^\star: \R^8 \to \R^{16}$ are randomly sampled as fine-tuning tasks. 2000 data points are independently generated from $\cP$ for these tasks. Accordingly, $h_w$ and $h_s$ are obtained by minimizing the average output distribution divergence between ground truth label ($f_t^\star \circ h^\star$) and model prediction ($f_t^\star \circ h_w$ and $f_t^\star \circ h_s$) over the 20 tasks.
(2) \textbf{Perturbations}: As an alternative, we directly perturb the parameters of $h^\star$ to obtain the weak and strong representations. 
Specifically, we add independent Gaussian noises $\mathcal{N}(0, \sigma_s^2)$ and $\mathcal{N}(0, \sigma_w^2)$ to every parameter in $h^\star$ to generate $h_s$ and $h_w$, respectively.
% We add independent Gaussian noise $\mathcal{N}(0, \sigma_s^2)$ to every parameter in $h^\star$ to generate $h_s$. Similarly, we perturb $h^\star$ with $\mathcal{N}(0, \sigma_w^2)$ to generate $h_w$. 
To ensure the strong representation $h_s$ is closer to $h^\star$ than $h_w$~\citep{charikar2024quantifying}, we set $\sigma_s=0.1$ and $\sigma_w=3$.



\noindent \textbf{Weak Model Finetuning.} 
We freeze the weak model representation $h_w$ and train the weak models on new fine-tuning tasks.
We randomly sample 100 new fine-tuning tasks $f_{21}^\star,\dots,f_{120}^\star: \R^8 \to \R^{16}$, and independently generate another 2000 data points from $\mathcal{P}$. 
For each task $t \in \{ 21, \cdots, 120 \}$, the corresponding weak model is obtained by minimizing the output distribution divergence between ground truth label and weak model prediction.

\noindent \textbf{Weak-to-Strong Supervision.} 
Using the trained weak models, we generate weakly labeled data to supervise the strong model.
Specifically, we first independently generate another 2000 data points from $\mathcal{P}$.
Then for each task $t \in \{ 21, \cdots, 120 \}$, the strong model is obtained by minimizing the output distribution divergence between weak model supervision and strong model prediction.
At this stage, the weak-to-strong training procedure is complete. The detailed introduction of above is in~\cref{appendix:syn_train}.

\noindent \textbf{Evaluation.}
We independently draw an additional 2000 samples from $\cP$ to construct the test set.
They are used to estimate $\dist(F^\star, F_{sw})$, $\dist(F^\star, F_w)$ and $\dist(F_{sw}, F_w)$ for each task $t \in \{ 21, \cdots, 120 \}$.
We estimate these quantities using their empirical counterparts: $\disthat(F^\star, F_w)$, $\disthat(F^\star, F_{sw})$, and $\disthat(F_{sw}, F_w)$.
To validate~\cref{thm:realizable-main}-\ref{thm:non-realizable-finite-samples-main} and visualize the trend clearly, we plot $\disthat(F^\star, F_w)-\disthat(F^\star, F_{sw})$ on the $x$-axis versus $\disthat(F_{sw}, F_w)$ on the $y$-axis. The results are presented in~\cref{syn_result:reverse}(a)-(c).
We also examine forward KL divergence loss. 
To validate~\cref{thm:realizable}-\ref{thm:non-realizable-finite-samples}, which extend~\cref{thm:realizable-main}-\ref{thm:non-realizable-finite-samples-main} to the case of using forward KL divergence loss in WTSG,
we plot $\disthat(F_w, F^\star)-\disthat(F_{sw}, F^\star)$ on the $x$-axis versus $\disthat(F_w, F_{sw})$ on the $y$-axis. 
The results are presented in~\cref{syn_result:reverse}(d)-(f).


\subsubsection{Results and Analysis}

\noindent \textbf{Reverse KL divergence loss.}
Similar to previous results of squared loss~\citep{charikar2024quantifying}, the points in our experiments also cluster around the line $y=x$.
This suggests that 
$\disthat(F^\star, F_w)-\disthat(F^\star, F_{sw}) \approx \disthat(F_{sw}, F_w)$.
It is consistent with our theoretical framework, suggesting that the improvement over the weak teacher can be quantified by the disagreement between strong and weak models.

\noindent \textbf{Forward KL divergence loss.}
The observed trend closely mirrors that of reverse KL. 
The dots are generally around the line $y=x$.
It suggest that the relationship 
$\disthat(F_w, F^\star)-\disthat(F_{sw}, F^\star) \approx \disthat(F_w, F_{sw})$
may also hold, indicating a similar theoretical guarantee for forward KL in WTSG.









