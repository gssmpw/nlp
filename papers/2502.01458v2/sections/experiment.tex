\subsection{Experimental Validation in Language Models}



\begin{figure*}[t]
  \centering
  \subfigure[Accuracy (Pythia).]{
    \includegraphics[width=0.47\textwidth]{images/exp_1/pythia_kl.pdf}
    \label{fig:pythia_acc}
  }
  \subfigure[Accuracy (GPT-2).]{
    \includegraphics[width=0.47\textwidth]{images/exp_1/gpt_2.pdf}
    \label{fig:gpt_acc}
  }
  \subfigure[ECE (Pythia).]{
    \includegraphics[width=0.47\textwidth]{images/exp_1/pythia_calibration.pdf}
    \label{fig:pythia_calibration}
  }
  \subfigure[ECE (GPT-2).]{
    \includegraphics[width=0.47\textwidth]{images/exp_1/gpt_2_calibration.pdf}
    \label{fig:gpt_calibration}
  }
  \vspace{-5pt}
  \caption{Accuracy and calibration results for Pythia and GPT-2 series. (\textbf{a}) Test accuracies of Pythia series. Each curve demonstrates the variation in accuracy of WTSG as strong models are supervised by weak models of varying capabilities. ``Strong Ceiling'' corresponds to models fine-tuned using ground truth data. (\textbf{b}) Test accuracies of GPT-2 series. (\textbf{c}) Expected calibration errors of Pythia series. Each curve depicts the change in ECE as strong models are supervised by different weak teachers. (\textbf{d}) Expected calibration errors of GPT-2 series.}
  \label{exp_llm_main}
  \vspace{-7pt}
\end{figure*}







\begin{figure*}[t]
  \centering
  \subfigure[Accuracy (Pythia).]{
    \includegraphics[width=0.23\textwidth]{images/exp_1/pythia_epoch.pdf}
  }
  \subfigure[Accuracy (GPT-2).]{
    \includegraphics[width=0.23\textwidth]{images/exp_1/gpt_epoch.pdf}
  }
  \subfigure[ECE (Pythia).]{
    \includegraphics[width=0.23\textwidth]{images/exp_1/pythia_epoch_conf.pdf}
  }
  \subfigure[ECE (GPT-2).]{
    \includegraphics[width=0.23\textwidth]{images/exp_1/gpt_epoch_conf.pdf}
  }
  \vspace{-5pt}
  \caption{Ablation study for the Pythia and GPT-2 series. (\textbf{a})-(\textbf{b}) Test accuracies of Pythia and GPT-2. The accuracies of Pythia-70M and GPT-2-Base fine-tuned on ground truth data is 92.45\% and 90.95\%, respectively.
  (\textbf{c})-(\textbf{d}) ECE of Pythia and GPT-2. The ECE of Pythia-70M and GPT-2-Base fine-tuned on ground truth data is 0.049 and 0.042, respectively.}
  \label{exp_llm_ablation}
  \vspace{-7pt}
\end{figure*}







In this section, we use language models to verify our theoretical results in WTSG.

\subsubsection{Experimental Setting}

\noindent \textbf{Dataset.}
We define the alignment objective as enabling a weak model to guide a strong model in achieving harmlessness. To this end, we employ CAI-Harmless~\citep{bai2022constitutional}, which is a widely adopted single-turn harmless dataset for reward modeling task. 
Each sample is structured as $(x;y_c,y_r)$, where $x$ denotes the prompt, and $y_c$ and $y_r$ represent the human-preferred and human-rejected completions, respectively.
The dataset is randomly split into three 4K-sample subsets: one for fine-tuning both weak and strong base models, another for weak supervision via weak model predictions, and the last for testing and evaluation.

% The dataset is randomly divided into three distinct subsets:
% (1) 4K ground truth samples for fine-tuning both weak and strong base language models;
% (2) A held-out set of 4K samples, where labels are predicted by the weak model and used to provide weak supervision for training the strong model;
% (3) The remaining 4K samples, reserved for testing and evaluating the performance of all models.











\noindent \textbf{Model.}
To explore weak-to-strong generalization, we utilize GPT-2 series ~\citep{radford2019language} (including GPT-2-Base, GPT-2-Medium, GPT-2-Large, and GPT-2-XL) and Pythia series~\citep{biderman2023pythia} (including Pythia-70M, Pythia-160M, Pythia-410M and Pythia-1B).
For each model, we append a linear projection head to facilitate logit predictions for each completion pair $\Tilde{x}=(x;y_c,y_r)$. 
Consequently, the task can be framed as a binary classification problem, where the model $F$ predicts the soft label as 
$$F(\Tilde{x}) = \text{Sigmoid}(F(y_c)-F(y_r)).$$
\noindent \textbf{Training.}
% Following~\citep{burns2023weak,yang2024super} in weak-to-Strong generalization, we adopt binary cross-entropy loss between model predictions and ground truth labels across the training set. 
% Notice that minimizing this loss is equivalent to minimizing KL divergence, as their difference is a constant.
We adopt KL divergence loss to train these models. Refer to~\cref{exp_llm_training_detail} for the training details.



\noindent \textbf{Metric.}
To evaluate whether a model $F$ can effectively distinguish between chosen and rejected completions ($y_c$ and $y_r$) for a given prompt $x$, we aim for $F$ to assign a higher score to the chosen completion compared to the rejected one.
Specifically, this requires $F(y_c)-F(y_r)>0$ for each completion pair $\Tilde{x}=(x;y_c,y_r)$, which implies $F(\Tilde{x})>0.5$.
Accordingly, the test accuracy of a model $F$ is reported as the fraction of predictions that satisfy $F(\Tilde{x})>0.5$.





\subsubsection{Results and Analysis}
The main results of WTSG for Pythia and GPT-2 series are shown in~\cref{exp_llm_main}.
To further investigate the optimization result $\dist(F_w, F_{sw})$ in WTSG,
we increase the number of epochs to train a strong model that more closely imitates the weak model.
The corresponding results are in~\cref{exp_llm_ablation}.

\noindent \textbf{Main results.}
\cref{fig:pythia_acc} and~\cref{fig:gpt_acc} demonstrate that, for the same strong model, the generalization of WTSG increases when supervised by a weak model of greater capacity.
It is consistent with~\cref{lemma:upper_lower_inf}.
\cref{fig:pythia_calibration} and~\cref{fig:gpt_calibration} illustrate the results for calibration errors. 
Interestingly, we observe that stronger models with higher capacity tend to exhibit larger ECE. 
Furthermore, increasing the weak model's capacity results in a U-shaped trend in ECE.
% initially decreasing before rising again. 
This pattern suggests a potential trade-off between the weak model's calibration quality and the teacher-student disagreement, which is consistent with~\cref{theorem:calibration}.
% This aligns with previous findings on the negative correlation between model capacity and calibration~\citep{guo2017calibration}.
% Additionally, since we utilize weak models with small calibration errors, the resulting strong models also maintain relatively low calibration errors without significant degradation.





\noindent \textbf{Ablation study.}
To investigate WTSG over extended training epochs, we design a series of teacher-student pairs with increasing model capacities. 
Specifically, we employ Pythia-70M as the weak teacher to supervise larger student models, including Pythia-160M, 410M, and 1B. 
We also utilize GPT-2-Base as the weak teacher for supervising GPT-2-Medium, Large, and XL.
\cref{exp_llm_ablation} illustrates that as we increase the number of epochs to reduce $\dist(F_w, F_{sw})$, \textit{there is a simultaneous decline in both the accuracy and calibration error of other strong models}.
Taking the Pythia series as an example, \cref{fig:pythia_acc} and~\cref{fig:pythia_calibration} demonstrate that Pythia-70M achieves the lowest accuracy and best ECE performance among the Pythia models. While~\cref{theorem:calibration} indicates that reducing $\dist(F_w, F_{sw})$ causes the accuracy and calibration results of strong models to converge toward those of the weak model, 
our experiments show that increasing the number of epochs leads to reduced accuracy and ECE for Pythia-160M, 410M, and 1B.
In other words, the accuracy and ECE of strong models approach those of the weak model, consistent with~\cref{lemma:upper_lower_inf} and~\cref{theorem:calibration}. 
And this trend is also observed in the GPT-2 series.

\noindent \textbf{Potential overfitting.}
As the number of epochs increases, \textit{the accuracy of GPT-2-XL drops even below that of GPT-2-Base} (90.95\%). 
This is attributed to the strong expressive power of GPT-2-XL, which leads to overfitting to the weak supervision provided by GPT-2-Base.
Note that the upper bounds derived in~\cref{lemma:upper_lower_inf} and~\cref{theorem:calibration} do not guarantee that the strong model will outperform the weak model in terms of both generalization performance and calibration properties.
The underlying intuition is that if a strong model overfits to the weak supervision, it may closely mimic the weak model's generalization and calibration behavior. 
Consequently, the strong model could end up performing on par with or potentially even worse than the weak model.
















