\section{Preliminaries}


\subsection{Classification and Regression} \label{prelim_class_regress}

We examine two problem settings.
In the first case, we consider classification tasks using KL divergence as the loss function. Minimizing this loss is equivalent to minimizing cross-entropy loss, which is widely used in the WTSG literature~\citep{burns2023weak}. 
In the second case, we focus on regression tasks, employing the KL divergence between the predictions of two models as the loss function. 
The model outputs over the entire data domain are normalized to form probability distributions.
% and we analyze the KL divergence between these output distributions as the regression loss. 
This approach is an extension of previous result~\citep{charikar2024quantifying} on squared loss, and provides an intuitive framework for understanding WTSG.

Given the data distribution $\cP$, data domain $\cX$ and output domain $\cY$, let $\cF: \cX \to \cY$.
Consider the difference $\dist$ and empirical difference $\disthat$ between two models, where $\dist,\disthat: \cF \times \cF \to \R_0^+$. We define the below two settings:

\noindent \textbf{Setting 1: KL divergence loss.}
Firstly, we consider a $k$-classification problem.
Given the data domain $\cX \subseteq \R^d$ and output domain $\cY \subseteq \R^k$. Consider the model with the softmax module, i.e., $\forall y = (y_1, \cdots, y_k)^T \in \cY$, there holds $\sum_{i=1}^k y_i=1$ and $0 < y_i \le 1$.
Given two models $f,g \in \cF$,
define $\dist$ and $\disthat$:
\begin{align}
& \dist(f,g) \triangleq \bE_{x \sim \cP} \left[ \mathrm{D}_{\mathrm{KL}}(f(x) \| g(x)) \right], \\ \label{def:cross_entropy}
& \disthat(f,g) \triangleq \frac{1}{n} \sum_{j=1}^n \mathrm{D}_{\mathrm{KL}}(f(x_j) \| g(x_j)),  
\end{align}
where $\mathrm{D}_{\mathrm{KL}}(f(x) \| g(x)) = \sum_{i=1}^k [f(x)]_i \log \frac{[f(x)]_i}{[g(x)]_i}$ is the KL divergence between predictions, and $[f(x)]_i, [g(x)]_i$ are elements of $f(x), g(x)$. 
% The expectation inner product between them is defined as: 
% $$\left \langle f,g \right \rangle_E \triangleq \bE_{x \sim \cP} [f(x)^Tg(x)].$$



\noindent \textbf{Setting 2: Output distribution divergence.} 
Secondly, we consider a regression problem.
Let the data domain and output domain be $\cX \subseteq \R^d$ and $\cY = \{ y \in \R| 0 < y \le 1 \}$, respectively. 
% Given two probability spaces $(\cX, \mathcal{F}, \cP_f), (\cX, \mathcal{F}, \cP_g)$ with the density function $f,g: \cX \to \cY$.
In this setting, the outputs of the model for all input data are probability-normalized to ensure they form valid probability distributions. The difference between two models $f,g \in \cF$ is then measured as the KL divergence between their corresponding output distributions:
% Define the population risk $\dist$ as the KL divergence between output distributions over $\cX$ and its empirical counterpart:
% \mathrm{D}_{\mathrm{KL}}(\cP_f \| \cP_g) = 
\begin{align}
& \dist(f,g) \triangleq \int_{\cX} f(x) \log \frac{f(x)}{g(x)} d x, \label{def:kl_dist} \\
& \disthat(f,g) \triangleq \sum_{i=1}^n f(x_i) \log \frac{f(x_i)}{g(x_i)}. \label{def:kl_dist_emp}
\end{align}
% We also define the inner product of functions 
% $$\left \langle f,g \right \rangle \triangleq \int_{\cX} f(x)g(x) d x.$$





% \begin{align}
% & \sq(f, g) \triangleq \mathbb{E}_{x \sim \cP} (f(x)-g(x))^2, \label{def:square_dist} \\
% & \yw{\mathrm{D}_{\mathrm{KL}}(\cP_f \| \cP_g) = \kl(f \| g) \triangleq \int_{\cX} f(x) \log \frac{f(x)}{g(x)} d x.} \label{def:kl_dist}
% \end{align}
% Therefore, the population risk of a model $f: \cX \to \cY$ over $\cP$ can be defined as $\dist(f \| f^\star \circ h^\star)$, which is the distance between $f$ and the ground truth model.




\subsection{Weak-to-strong Generalization}

In the context of WTSG, we focus on the fine-tuning phase after pre-training.
% we assume the existence of \textit{pre-training and fine-tuning tasks}. 
% that enable strong models and weak models to learn data representations through functions $h_s:\cX \to \R^{d_s}$ and $h_w:\cX \to \R^{d_w}$, respectively.
Let $h^\star:\R^d \to \R^{d^\star}$ denotes the ground truth representation function, which maps data $x \in \cX$ to an ideal, fully enriched representation $h^\star(x)$. 
The target fine-tuning task, composed with the ground truth representation, is denoted as $f^\star \circ h^\star$, where $f^\star:\R^{d^\star}\to \cY$.
The \textit{weak model} learns a mapping $f_w \circ h_w$, where the pre-trained representation $h_w:\cX \to \R^{d_w}$ extracts features from the input data, and $f_w:\R^{d_w} \to \cY$ is fine-tuned using supervised data with ground truth labels.
The strong model, on the other hand, aims to learn a mapping $f_{sw} \circ h_s$, where $h_s: \cX \to \R^{d_s}$ is the representation, and $f_{sw} \in \cF_{s}$ is a task-specific function from a hypothesis class $\cF_{s}: \R^{d_s} \to \cY$.
The strong model leverages the representation $h_s$ to improve performance on the fine-tuning task.
In the convention setting of AI alignment~\citep{ouyang2022training}, 
% ~\citep{ziegler2019fine,ouyang2022training,bai2022training}, 
the model is learned through human-annotated ground truth data:
\begin{align}
    \label{eqn:alignment-population-minimizer}
    f_{s} = \argmin_{f \in \cF_{s}}\; \dist(f^\star \circ h^\star, f \circ h_s).
\end{align}
Nevertheless, the acquisition of human-generated data is both costly and time-consuming.
To address this challenge, the WTSG framework leverages weak supervision from the weak model's predictions, enabling the strong model to be trained through population risk minimization:
\begin{align}
    \label{eqn:fsw-population-minimizer}
    f_{sw} = \argmin_{f \in \cF_{s}}\; \dist(f_w \circ h_w, f \circ h_s).
\end{align}
In practice, we label $n$ i.i.d. samples using the weak model and minimize the empirical risk to conduct WTSG:
\begin{align} \label{eqn:erm}
    \hat{f}_{sw} = \argmin_{f \in \cF_s} \disthat(f_w \circ h_w, f \circ h_s).
\end{align}
Denote the labeling function $F^\star=f^\star \circ h^\star$, strong ceiling model $F_s=f_s \circ h_s$, weak model $F_w=f_w \circ h_w$, and strong models $F_{sw}=f_{sw} \circ h_s$, $\hat{F}_{sw}=\hat{f}_{sw} \circ h_s$, respectively.

% Therefore, the population risk of $f_{s}$ learned by conventional AI alignment (\cref{eqn:alignment-population-minimizer}) and $f_{sw}$ learned by weak-to-strong generalization (\cref{eqn:fsw-population-minimizer}) is $R(f_{s} \circ h_s)$ and $R_{w}(f_{sw} \circ h_s)$, respectively.
% Through super-alignment, we aim to let $R(f_{sw} \circ h_s)$ approaches $R(f_{s} \circ h_s)$ as close as possible.

