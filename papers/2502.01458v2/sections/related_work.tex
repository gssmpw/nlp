\section{Related Work}

In this section, we introduce AI alignment and WTSG.
Additional related work including teacher-student learning paradigm, weakly-supervised learning, calibration and information-theoretic analysis is provided in~\cref{appendix:related_work}.





\noindent \textbf{AI Alignment.}
AI alignment~\citep{ji2023ai,shen2023large} aims to ensure AI systems act in accordance with human values. 
A popular approach to achieve this goal is fine-tuning models on human-annotated data, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{ouyang2022training,bai2022training} and Direct Preference Optimization (DPO)~\citep{rafailov2024direct}. 
However, this alignment paradigm faces significant challenges: human oversight becomes insufficient as AI surpasses human capabilities~\citep{kim2024road}, and obtaining scalable, high-quality human feedback remains difficult~\citep{casper2023open}. 
These challenges highlight the critical need to align superhuman AI systems~\citep{openai_superalignment}.
In contrast to these approaches, our work explores WTSG, which does not rely on extensive human supervision and instead leverages weaker guidance to achieve the alignment goal.





% A promising approach is weak-to-strong generalization~\citep{burns2023weak}, which investigates how weaker  supervision can align stronger models.
% In this paper, we present an early theoretical exploration of WTSG, emphasizing its significance for advancing our understanding of its capabilities and limitations. Gaining deeper insights into this phenomenon is crucial for addressing the alignment challenges posed by advanced AI systems in the near future.






\noindent \textbf{Weak-to-Strong Generalization.}
To explore the effect of weak models to supervise strong models, \citet{burns2023weak} first find that strong models supervised by weak models can exhibit better performance on corresponding tasks than their weak supervisors, indicating the possibility of stimulating greater power from super models under weak supervisions.
There are also algorithms~\citep{zhu2024weak,agrawal2024ensemw2s,sang2024improving,guo2024improving} and empirical analysis~\citep{yang2024super,ye2024weak} for it.
However, to the best of our knowledge, only a limited number of theoretical studies have been conducted on this topic.
\citet{lang2024theoretical} analyzes it by introducing theoretical bounds that account for pseudolabel correction and coverage expansion. \citet{somerstep2024statistical} frame WTSG as a transfer learning problem, revealing limitations of fine-tuning on weak labels. \citet{wu2024provable} study linear models under a spiked covariance setting and derive asymptotic bounds. \citet{charikar2024quantifying} take a convex-theoretic approach in regression, quantifying performance improvements under squared loss via the misfit error between weak and strong models.

The work most closely related to ours is~\citet{charikar2024quantifying}, which primarily focuses on squared loss in regression. 
In contrast, we consider KL divergence-like losses, including KL divergence for classification and output distribution divergence for regression. 
Furthermore, while they focuses on establishing upper bounds, our study incorporates both upper and lower bounds as well as calibration analysis through experiments on language models.
% providing a more comprehensive understanding of the fundamental capabilities and limitations of WTSG.




