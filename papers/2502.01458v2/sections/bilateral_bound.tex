\section{Universal Results in WTSG} \label{section:universal_result}

In this section, we consider the classification problem, where $\dist$ is the KL divergence loss defined in~\cref{def:cross_entropy}.
We first establish lower and upper generalization error bounds of the strong model in WTSG in~\cref{section_lower_upper}.
Then the lower and upper calibration error bounds are shown in~\cref{subsec:suff_nece_condition}.



\subsection{Lower and Upper Bound} \label{section_lower_upper}



\begin{theorem}[Proved in \cref{proof_lemma_inf}] \label{lemma:upper_lower_inf}
% Let $\dist$ be the KL divergence loss.
Given the data domain $\cX$, output domain $\cY$ and models $F_{sw}, F_w, F^\star$ defined above. 
Then there holds
\begin{multline}
    \dist\left( F^\star, F_w \right) - C_1\sqrt{\dist(F_w, F_{sw})} \\ \le \dist(F^\star, F_{sw}) \le \\ \dist\left( F^\star, F_w \right) + C_1\sqrt{\dist(F_w, F_{sw})},
\end{multline}
% \begin{align}
%     \left| \dist(F^\star, F_{sw}) - \dist\left( F^\star, F_w \right) \right| \le \sqrt{C_\gamma \dist(F_{sw}, F_w)},
% \end{align}
where $C_1$ is a positive constant.
\end{theorem}

\begin{remark}
    The proof can be also extended to the regression setting, which is provided in~\cref{proof:lower_upper}.
\end{remark}

% \begin{remark}
%     The lower bound can be tighter by reducing $C_1$, which is proved in~\cref{constant:theorem}.
% \end{remark}

% \begin{remark}
%     Our proof can also cover reverse KL divergence~\citep{malinin2019reverse,he2024training,shi2024choosy}, with the corresponding proof in~\cref{proof_lemma_inf}.
% \end{remark}
% , which is also used in AI alignment literature~\citep{wang2023beyond}


\cref{lemma:upper_lower_inf} provides a quantitative framework for assessing the performance gap between weak model and strong model in WTSG.
Specifically, the value of $\dist(F^\star, F_{sw})$ is constrained by two terms: 
(1) $\dist(F^\star, F_{sw})$, which reflects the performance of the weak model, and 
(2) $\dist(F_w, F_{sw})$, which is decided by the optimization result in~\cref{eqn:fsw-population-minimizer} and measures how the strong model learns to imitate the weak supervisor.
This result is examined from two complementary perspectives: a lower bound and an upper bound.
They offer insights into the fundamental limitation and theoretical guarantee for WTSG. 

\noindent \textbf{Lower bound.}
The lower bound indicates the fundamental limitation: $\dist(F^\star, F_{sw})$ cannot be arbitrarily reduced.
Firstly, a minimal $\dist(F^\star, F_{sw})$ is intrinsically tied to the weak model performance $\dist\left( F^\star, F_w \right)$.
To improve the strong model, the weak model becomes critical---that is, $\dist\left( F^\star, F_w \right)$ should be as small as possible. It underscores the importance of \textbf{\textit{carefully selecting the weak model}}~\citep{burns2023weak,yang2024super}.
Secondly, the performance improvement of strong model over the weak model cannot exceed $\cO \left(\sqrt{\dist(F_w, F_{sw})} \right)$.
In WTSG, while the student-supervisor disagreement $\dist(F_w, F_{sw})$ is minimized in~\cref{eqn:fsw-population-minimizer}, we anticipate $\cO \left(\sqrt{\dist(F_w, F_{sw})} \right)$ to remain relatively small.
However, a paradox arises: achieving a smaller $\dist\left( F^\star, F_{sw} \right)$ necessitates a larger $\dist(F_w, F_{sw})$.
This implies that \textbf{\textit{the performance improvement of WTSG is probably constrained by its own optimization objective}}.





\noindent \textbf{Upper bound.}
The upper bound provides a theoretical guarantee for WTSG by ensuring that $\dist(F^\star, F_{sw})$ remains bounded and does not grow arbitrarily large.
Firstly, effective WTSG requires choosing a weak model that produces supervision signal closely aligned with the true score, i.e., achieving a small $\dist\left( F^\star, F_w\right)$. 
To this end, employing a stronger weak model is crucial to obtain a tighter upper bound of $\dist(F^\star, F_{sw})$.
Secondly, the worst-case performance of the strong model is constrained by the sum of $\dist\left( F^\star, F_w\right)$ and $\cO \left(\sqrt{\dist(F_w, F_{sw})} \right)$.
By appropriately selecting the weak model and determining the minimizer of~\cref{eqn:fsw-population-minimizer}, both $\dist\left( F^\star, F_w\right)$ and $\cO \left(\sqrt{\dist(F_w, F_{sw})} \right)$ can be kept small, ensuring the effectiveness and practicality of the strong model.
















\subsection{Calibration in Weak-to-strong Generalization} \label{subsec:suff_nece_condition}

% In the previous section, \cref{lemma:upper_lower_inf} does not ensure that the strong model will necessarily surpass the performance of its weak supervisor, i.e., $\dist(F^\star, F_{sw}) \le \dist\left( F^\star, F_w \right)$.
% Establishing such a bound would provide a stronger theoretical foundation for weak-to-strong generalization. 
% This raises the question of whether a tighter upper bound can be derived, such as $\dist(F^\star, F_{sw}) \le \dist\left( F^\star, F_w \right) - \dist(F_w, F_{sw})$.
% The previous section sheds light on upper and lower bounds of the strong model performance.
In this section, we further explore WTSG through the lens of calibration~\citep{kumar2019verified}, which requires that the prediction confidence should match the actual outcome.
We first state the definition of Marginal Calibration Error (MCE)~\citep{kumar2019verified}, which is an extended version of Expected Calibration Error (ECE)~\citep{guo2017calibration} designed for multi-class classification.
In particular, we use an $\ell_1$ version of it, with the weight constant $\frac{1}{k}$ omitted.
\begin{definition}[Marginal Calibration Error~\citep{kumar2019verified}] \label{def:cal:mce}
Let $x \in \cX$, ground truth $y=[y_1, \cdots, y_k]^T \in \{ 0,1 \}^k$ where $\sum_{i=1}^k y_i=1$, and a model $f: \cX \to \cY$.
Define the marginal calibration error of $f$ as:
\begin{align} \label{def:cal_err}
    \textit{MCE}(f) = \sum_{i=1}^k \expect_x \left| [f(x)]_i-\prob[y_i=1|[f(x)]_i] \right|.
\end{align}
\end{definition}

\vspace{-3pt}
It measures the difference between model confidence and actual outcome, and $\textit{MCE}(f) \in [0,2]$.
For binary classification, $\textit{MCE}$ is twice the $\textit{ECE}$.
We shed light on upper and lower bounds of calibration of the strong model.

\begin{theorem}[Proved in~\cref{proof:calibration}] \label{theorem:calibration}
% Consider $\dist$ as the KL divergence loss.
Let $\text{MCE}(\cdot)$ be the marginal calibration error in~\cref{def:cal:mce}.
Then there holds
\begin{multline}
    \textit{MCE}(F_w) - 2 \cdot \sqrt{1-\exp{\left(-\dist(F_w,F_{sw})\right)}} \\ \le \textit{MCE}(F_{sw}) \le \\ \textit{MCE}(F_w) + 2 \cdot \sqrt{1-\exp{\left(-\dist(F_w,F_{sw})\right)}}.
\end{multline}
\end{theorem}


\cref{theorem:calibration} demonstrates that the calibration error of $F_{sw}$ is influenced by two key factors: (1) the calibration error of $F_w$, and (2) the teacher-student disagreement, as characterized by the optimization result in~\cref{eqn:fsw-population-minimizer}.
% In our experiments on large language models
% the practical use of this bound depends on $\textit{MCE}(F_w)$ (usually 0.08-0.24), and $\dist(F_w,F_{sw})$ (which is usually 0.0004-0.04, making $2 \cdot \sqrt{1-\exp{\left(-\dist(F_w,F_{sw})\right)}}$ falls into 0.04-0.4).
This theoretical result yields two insights.
First, to achieve a strong model with acceptable calibration, the weak teacher should also exhibit acceptable calibration. Otherwise, the strong model will inherit a non-trivial calibration error from the weak teacher as $\dist(F_w,F_{sw})$ goes to zero.
Second, closely imitating the weak supervisor minimizes $\dist(F_w,F_{sw})$, causing the calibration errors of the strong and weak models to converge. 
Taking them together, to ensure WTSG with reasonable calibration and prevent a poorly-calibrated $F_{sw}$, it is crucial to \textit{\textbf{avoid using a poorly-calibrated weak model with an overfitted strong model}}.
Additionally, since models with larger capacity may exhibit higher calibration errors~\citep{guo2017calibration}, \textit{\textbf{a potential trade-off exists between the weak model's calibration error and the teacher-student disagreement}}.
In other words, $\textit{MCE}(F_w)$ and $\sqrt{1-\exp{\left(-\dist(F_w,F_{sw})\right)}}$ may not be minimized simultaneously, posing a challenge in selecting the weak model and designing an effective optimization strategy to achieve better calibration in the strong model.


% Note that the upper bounds derived in~\cref{lemma:upper_lower_inf} and~\cref{theorem:calibration} do not guarantee that the strong model will outperform the weak model in terms of both generalization performance and calibration properties in WTSG.
% The intuition behind this is that if the strong model overfits the weak supervision, it will closely mimic the weak model's generalization and calibration behavior, potentially performing equally poorly or even worse.
% It will be validated in our experiments below.




% First, employing a better-calibrated weak model in WTSG leads to a better-calibrated strong model. 
% Given that recent studies have shown that aligned large language models exhibit non-trivial calibration errors~\citep{zhu2023calibration, tian2023just}, we emphasize the importance of carefully considering the calibration properties of weak models used to align strong models. Both the accuracy and calibration of the weak model are critical in WTSG.


% We demonstrate that a better-calibrated $F_{sw}$ contributes to an improved $\dist(F_w, F_{sw})$, thereby mitigating the issue of overfitting to weak supervision~\citep{burns2023weak}, where the strong model overly imitates the weak supervisor.

% \cref{theorem:calibration} offers a potential strategy to ensure that $\dist(F_w, F_{sw})$ does not asymptotically approach zero.
% Specifically, if $F_{sw}$ achieves a lower calibration error than $F_w$, $\dist(F_w, F_{sw})$ will remain positive, thereby potentially mitigating overfitting to weak supervision.
% This observation highlights a promising direction for enhancing weak-to-strong generalization: improving the calibration of the strong model. 
% Such an approach opens new avenues for algorithmic advancements in this area. 
% Moreover, this perspective sheds light on the overfitting mitigation effect of the auxiliary loss employed in~\citep{burns2023weak}, which strengthens the strong modelâ€™s confidence in its predictions.




% To further investigate the benefits of calibration, we decompose $\dist(F^\star, F_{sw})$ in the following equation:

% \begin{proposition}[Proved in~\cref{proof:general_equation_true}] \label{general_equation_true}
% % Let $\dist$ be the KL divergence loss.
% Given the models $F_{sw}, F_w, F^\star$ defined above. Then there holds
% \begin{multline} \label{eq:general_equation_true}
%     \dist(F^\star, F_{sw}) = \dist(F^\star, F_w) - \underbrace{\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E}_{R}.
% \end{multline}
% % If $\dist$ is the output distribution divergence, the same result can be recovered by replacing $\left \langle \cdot, \cdot \right \rangle_E$ with $\left \langle \cdot, \cdot \right \rangle$.
% \end{proposition}


% Therefore, $\dist(F^\star, F_{sw}) \le \dist(F^\star, F_w)$ holds if and only if the remainder term $R \ge 0$.
% Furthermore, an increasing $R$ contributes to a smaller $\dist(F^\star, F_{sw})$, indicating an improved strong model.
% Intuitively, $R$ is more likely to increase if $\forall x \in \cX$ and $i \in \{ 1, \cdots, k \}$: 
% \begin{itemize}
%     \item when $[F^\star(x)]_i$ is small, $[F_{sw}(x)]_i \le [F_w(x)]_i$;
%     \item when $[F^\star(x)]_i$ is large, $[F_{sw}(x)]_i \ge [F_w(x)]_i$.
% \end{itemize}
% In other words, we expect the confidence of $F_{sw}$ aligns closely with the true outcome $F^\star$, suggesting a strong calibration property of $F_{sw}$.
% Additionally, if a promising calibration property is achieved and the remainder $R \ge 0$ is successfully satisfied, we prove that the performance improvement of the strong model relative to the weak model is bounded by $\cO \left( \sqrt{\dist(F_w, F_{sw})} \right)$:

% \begin{theorem}[Proved in~\cref{proof:theorem:residue}] \label{theorem:residue}
% % Let $\dist$ be the KL divergence loss.
% Given $F_{sw}, F_w, F^\star$ defined above. 
% Denote $R=\left \langle F^\star, \log{\frac{F_{sw}}{F_w}} \right \rangle_E$. 
% Then $R \le C'\sqrt{\dist(F_w, F_{sw})}$, where $C'$ is a positive constant.
% \end{theorem}

% % \begin{remark}
% %     We also prove that if $R \ge 0$ and $\dist(F_w, F_{sw}) \ge \sqrt{2}C'$, then $R \le \dist(F_w, F_{sw})$, which means that there holds $\dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - \dist(F_w, F_{sw})$.
% % \end{remark}

% \cref{theorem:residue} and~\cref{general_equation_true} collectively states that
% \begin{align*}
%     \dist(F^\star, F_{sw}) \ge \dist\left( F^\star, F_w \right) - C'\sqrt{\dist(F_w, F_{sw})}.
% \end{align*}

% It provides an alternative perspective to~\cref{lemma:upper_lower_inf} with different proof techniques, demonstrating that the maximum potential gain of the strong model over the weak model is related to $\sqrt{\dist(F_w, F_{sw})}$.
% It reinforces that the key bottleneck for performance improvement over $F_w$ arises from the optimization objective's inherent nature.


