\section{Introduction}

Recently, human supervision~\citep{ouyang2022training,bai2022training} plays a crucial role in building both effective and safe artificial intelligence systems~\citep{achiam2023gpt,touvron2023llama}.
However, as future superhuman models exhibit increasingly complex behaviors, reliable human oversight becomes increasingly challenging~\citep{openai_superalignment}.
% ~\citep{ziegler2019fine,ouyang2022training,bai2022training}
% ~\citep{achiam2023gpt,touvron2023llama,dubey2024llama}



To tackle this issue, the Weak-To-Strong Generalization (WTSG) paradigm~\citep{burns2023weak} is proposed.
It finds that, strong pre-trained language models, when fine-tuned using labels produced by weaker models, consistently achieve better performance than their weak supervisors.
This intriguing phenomenon has not only driven the development of diverse alignment algorithms~\citep{zhu2024weak,liu2024co}, but also inspired efforts~\citep{pawelczyk2024generalizing,yang-etal-2024-weak,guo2024vision} to extend the concept to other tasks.
However, despite its empirical success, the theoretical foundations of WTSG remain underdeveloped.
Although several elegant theoretical frameworks~\citep{lang2024theoretical,somerstep2024statistical,wu2024provable,charikar2024quantifying} are proposed, a universal framework is still lacking to address fundamental questions, such as: \textit{What is the optimal generalization performance a strong model can achieve after WTSG? Besides generalization, what other factors are influenced by WTSG?}



To answer these questions, we provide a comprehensive theoretical analysis of WTSG, shedding lights on its capabilities and limitations.
Firstly, in classification tasks, by assuming that the output of the softmax module lies in the interval $(0,1]$, our analysis of lower and upper generalization bounds under KL divergence loss reveals that the strong model's performance is fundamentally determined by two key factors:
(1) the disagreement between strong and weak models, which serves as the minimization objective in WTSG, and (2) the weak model's performance.
These findings suggest that (1) achieving the minimal optimization objective in WTSG limits the strong model’s ability to significantly outperform its weak supervisor, and (2) selecting a stronger weak model can enhance the performance of the strong model.
Secondly, we investigate how strong model's calibration~\citep{guo2017calibration,kumar2019verified}—the property that a model's predicted confidence aligns with its actual accuracy—is affected in the WTSG framework.
Our theoretical bounds reveal that the calibration of the strong model depends on both the calibration of the weak model and the disagreement between the two models.
They highlight the importance of avoiding a poorly-calibrated weak model and an overfitted strong model.
The above theory is validated using GPT-2 series~\citep{radford2019language} and Pythia series~\citep{biderman2023pythia}.


In addition to classification setting, we also consider the regression problem.
In particular, we build on the work of~\citet{charikar2024quantifying} by extending their analysis of squared loss to output distribution divergence, a measure of the difference between two models' output distributions. 
In this setting, the model outputs are normalized to form valid probability distributions over all input data, and the output distribution divergence between two models is defined as the KL divergence of their respective output distributions.
Since theoretically analyzing the asymmetric and nonlinear KL divergence introduces additional technical challenges, we introduce an assumption about model confidence.
With this assumption, we recover the findings from~\citet{charikar2024quantifying} and show that the strong model's generalization error is provably smaller than the weak model's, with the gap no less than the WTSG minimization objective—namely, the strong model's error on the weak labels.
We conduct synthetic experiments to support our theoretical insights.








