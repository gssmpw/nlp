\section{Related Work}
\label{gen_inst}
\vspace{-0.5em}
\paragraph{Knowledge Distillation}

Knowledge distillation (KD) ____ is a popular model compression technique where a small student model learns to mimic the output distributions, hidden layer outputs ____, inter-layer relationships ____, sample relationships ____, or attributions ____ of one or more larger teacher models ____. In the context of LLMs, KD typically focuses on minimizing the Kullback-Leibler Divergence (KLD) between the output distributions of the student and teacher models at each time step. Recent research has proposed various optimizations to improve the efficiency and effectiveness of this process. For instance, MiniLLM ____ employs sequence-level reverse KLD to encourage the student model to focus on the most significant modes of the teacher's output distributions. DistiLLM ____, on the other hand, %increases the efficiency of the distillation process by using Skew-KLD loss to combined with adaptive off-policy methods.
introduces a novel skew KLD objective, which interpolates between the teacher's and student's distributions to ensure stable gradients and reduce optimization errors.
Likewise, $f$-distill ____ minimizes a symmetric $f$-divergence to mitigate challenges such as mode collapse, while Adaptive Kullback-Leiber (AKL) ____ balances forward and reverse KLD to ensure the student model effectively learns across different parts of the distribution. Other approaches, including Vicuna ____ and MCC-KD ____, take advantage of outputs generated by the teacher model to train the student, thereby enhancing its ability to follow instructions or perform more complex reasoning tasks.
\vspace{-0.5em}
\paragraph{Preference Alignment}
Preference alignment aims to align the outputs of LLMs with human preferences and values. This objective is traditionally achieved by RLHF ____, which relies on a reward model (RM) trained on preference data to guide the optimization of the policy model through policy gradient optimization methods like Proximal Policy Optimization (PPO) ____. Recent research has increasingly focused on using contrastive learning methods to eliminate the need of RM and complex online reinforcement learning (RL). Notable approaches in this area include Direct Preference Optimization (DPO) ____ and SLiC-HF ____. Other studies explore fine-grained rewards to provide more detailed guidance to the policy model. For example, ____ defined sequence-level rewards as aggregations of token-wise rewards learned through sequence-level RM training on preference datasets. In addition, Token-Level Continuous Reward (TLCR)____ employs GPT-4 as a reviser to analyze preference pairs and modify dispreferred responses to generate token-level preference labels that are then used to train a discriminator for assigning token-level rewards.



Given the high cost of obtaining quality preference labels for training reward models, recent research has focused on leveraging larger and more powerful LLMs to provide feedback on the preferences of candidate responses. For example, RLAIF ____ uses an off-the-shelf LLM to offer feedback on candidate responses, which are then used to train a reward model for RL. Zephyr ____ and Starling ____ gather responses from multiple LLMs and rank them using GPT-4 to obtain preference data. While the former uses this data to train a policy with DPO,  the latter employs it to train a reward model for RL. Other approaches, such as DPKD ____ and PLaD ____, treat the teacher's outputs as preferred responses and the student's outputs as dispreferred and conduct preference learning. RLCD ____ designs positive and negative prompts to elicit corresponding responses and uses them to train a reward model for RL. Reward model distillation ____ aligns the distribution predicted by the policy with that of a trained reward model to enhance preference optimization robustness.

\vspace{-0.5em}