\section{Related Work}
\label{gen_inst}
\vspace{-0.5em}
\paragraph{Knowledge Distillation}

Knowledge distillation (KD) \citep{hinton2015distilling} is a popular model compression technique where a small student model learns to mimic the output distributions, hidden layer outputs \citep{chen2017learning}, inter-layer relationships \citep{yim2017gift}, sample relationships \citep{reddi2021rankdistil}, or attributions \citep{wu2023ad} of one or more larger teacher models \citep{liu2020adaptive}. In the context of LLMs, KD typically focuses on minimizing the Kullback-Leibler Divergence (KLD) between the output distributions of the student and teacher models at each time step. Recent research has proposed various optimizations to improve the efficiency and effectiveness of this process. For instance, MiniLLM \citep{gu2024minillm} employs sequence-level reverse KLD to encourage the student model to focus on the most significant modes of the teacher's output distributions. DistiLLM \citep{ko2024distillm}, on the other hand, %increases the efficiency of the distillation process by using Skew-KLD loss to combined with adaptive off-policy methods.
introduces a novel skew KLD objective, which interpolates between the teacher's and student's distributions to ensure stable gradients and reduce optimization errors.
Likewise, $f$-distill \citep{wen2023f} minimizes a symmetric $f$-divergence to mitigate challenges such as mode collapse, while Adaptive Kullback-Leiber (AKL) \citep{wu2024rethinking} balances forward and reverse KLD to ensure the student model effectively learns across different parts of the distribution. Other approaches, including Vicuna \citep{vicuna2023} and MCC-KD \citep{chen2023mcc}, take advantage of outputs generated by the teacher model to train the student, thereby enhancing its ability to follow instructions or perform more complex reasoning tasks.
\vspace{-0.5em}
\paragraph{Preference Alignment}
Preference alignment aims to align the outputs of LLMs with human preferences and values. This objective is traditionally achieved by RLHF \citep{ouyang2022training}, which relies on a reward model (RM) trained on preference data to guide the optimization of the policy model through policy gradient optimization methods like Proximal Policy Optimization (PPO) \citep{schulman2017proximal}. Recent research has increasingly focused on using contrastive learning methods to eliminate the need of RM and complex online reinforcement learning (RL). Notable approaches in this area include Direct Preference Optimization (DPO) \citep{rafailov2024direct} and SLiC-HF \citep{zhao2023slic}. Other studies explore fine-grained rewards to provide more detailed guidance to the policy model. For example, \citet{yang2024preference} defined sequence-level rewards as aggregations of token-wise rewards learned through sequence-level RM training on preference datasets. In addition, Token-Level Continuous Reward (TLCR)~\citep{yoon2024tlcr} employs GPT-4 as a reviser to analyze preference pairs and modify dispreferred responses to generate token-level preference labels that are then used to train a discriminator for assigning token-level rewards.



Given the high cost of obtaining quality preference labels for training reward models, recent research has focused on leveraging larger and more powerful LLMs to provide feedback on the preferences of candidate responses. For example, RLAIF \citep{lee2023rlaif} uses an off-the-shelf LLM to offer feedback on candidate responses, which are then used to train a reward model for RL. Zephyr \citep{tunstall2023zephyr} and Starling \citep{starling2023} gather responses from multiple LLMs and rank them using GPT-4 to obtain preference data. While the former uses this data to train a policy with DPO,  the latter employs it to train a reward model for RL. Other approaches, such as DPKD \citep{li2024direct} and PLaD \citep{zhang2024plad}, treat the teacher's outputs as preferred responses and the student's outputs as dispreferred and conduct preference learning. RLCD \citep{yang2023rlcd} designs positive and negative prompts to elicit corresponding responses and uses them to train a reward model for RL. Reward model distillation \citep{fisch2024robust} aligns the distribution predicted by the policy with that of a trained reward model to enhance preference optimization robustness.

\vspace{-0.5em}