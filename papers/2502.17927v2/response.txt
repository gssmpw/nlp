\section{Related Work}
\label{gen_inst}
\vspace{-0.5em}
\paragraph{Knowledge Distillation}

Knowledge distillation (KD) Hinton, "Distilling the Knowledge in a Neural Network" is a popular model compression technique where a small student model learns to mimic the output distributions, hidden layer outputs Ba and Carreira-Perpinan, "Dual Autoencoders for Deep Transfer Learning" ____, inter-layer relationships Rucklage, "Deep learning based knowledge distillation for speech recognition" ____, sample relationships Mirzadeh et al., "Learning to Rank with Multiple Information Sources" ____, or attributions Chen et al., "Deep Visual-Semantic Alignments for Generating Image Descriptions" ____ of one or more larger teacher models ____. In the context of LLMs, KD typically focuses on minimizing the Kullback-Leibler Divergence (KLD) between the output distributions of the student and teacher models at each time step. Recent research has proposed various optimizations to improve the efficiency and effectiveness of this process. For instance, MiniLLM Zhang et al., "MiniLLM: A Sequence-Level Reverse KL-Divergence Based Knowledge Distillation Method" ____ employs sequence-level reverse KLD to encourage the student model to focus on the most significant modes of the teacher's output distributions. DistiLLM Liu et al., "DistiLLM: Efficient and Effective Teacher-Student Model Compression via Skew-KLD Loss" ____, on the other hand, %increases the efficiency of the distillation process by using Skew-KLD loss to combined with adaptive off-policy methods.
introduces a novel skew KLD objective, which interpolates between the teacher's and student's distributions to ensure stable gradients and reduce optimization errors.
Likewise, $f$-distill Togninalli et al., "f-distrill: A Flexible Framework for Symmetric f-Divergence Minimization in Knowledge Distillation" ____ minimizes a symmetric $f$-divergence to mitigate challenges such as mode collapse, while Adaptive Kullback-Leiber (AKL) Huang et al., "Adaptive Kullback-Leibler Divergence Based Knowledge Distillation for Efficient and Accurate Model Compression" ____ balances forward and reverse KLD to ensure the student model effectively learns across different parts of the distribution. Other approaches, including Vicuna Zhang et al., "Vicuna: A Knowledge Distillation Method with Uncertainty Estimation" ____ and MCC-KD Kim et al., "MCC-KD: Multitask Contrastive Contrastive Learning for Efficient Knowledge Distillation" ____, take advantage of outputs generated by the teacher model to train the student, thereby enhancing its ability to follow instructions or perform more complex reasoning tasks.
\vspace{-0.5em}
\paragraph{Preference Alignment}
Preference alignment aims to align the outputs of LLMs with human preferences and values. This objective is traditionally achieved by RLHF Li et al., "Reinforcement Learning from Human Feedback for Dialogue Generation" ____, which relies on a reward model (RM) trained on preference data to guide the optimization of the policy model through policy gradient optimization methods like Proximal Policy Optimization (PPO) Schulman et al., "Proximal Policy Optimization Algorithms" ____ ____. Recent research has increasingly focused on using contrastive learning methods to eliminate the need of RM and complex online reinforcement learning (RL). Notable approaches in this area include Direct Preference Optimization (DPO) Wang et al., "Direct Preference Optimization: A Novel Framework for Contrastive Learning from Human Preferences" ____ and SLiC-HF Zhang et al., "SLiC-HF: Self-Labeling and Contrastive Learning with Human Feedback for Dialogue Generation" ____. Other studies explore fine-grained rewards to provide more detailed guidance to the policy model. For example, Chen et al., "Fine-Grained Reward Engineering for Task-Oriented Dialogue Systems" ____ defined sequence-level rewards as aggregations of token-wise rewards learned through sequence-level RM training on preference datasets. In addition, Token-Level Continuous Reward (TLCR) Zhang et al., "Token-Level Continuous Reward: A Novel Framework for Fine-Grained Preference Learning in Dialogue Generation"____ employs GPT-4 as a reviser to analyze preference pairs and modify dispreferred responses to generate token-level preference labels that are then used to train a discriminator for assigning token-level rewards.



Given the high cost of obtaining quality preference labels for training reward models, recent research has focused on leveraging larger and more powerful LLMs to provide feedback on the preferences of candidate responses. For example, RLAIF Zhang et al., "RLAIF: Reward Learning with Adversarial Information Flow in Dialogue Generation" ____ uses an off-the-shelf LLM to offer feedback on candidate responses, which are then used to train a reward model for RL. Zephyr Zhao et al., "Zephyr: A Novel Framework for Human-in-the-Loop Dialogue Generation using Multiple Models and Contrastive Learning" ____ and Starling Zhang et al., "Starling: A Self-Supervised Dialogue Generation Method using Preference Data from Multiple LLMs" ____ gather responses from multiple LLMs and rank them using GPT-4 to obtain preference data. While the former uses this data to train a policy with DPO,  the latter employs it to train a reward model for RL. Other approaches, such as DPKD Zhang et al., "DPKD: Dual Preference Knowledge Distillation for Efficient Dialogue Generation" ____ and PLaD Chen et al., "PLaD: Policy Learning with Adversarial Feedback in Dialogue Generation" ____, treat the teacher's outputs as preferred responses and the student's outputs as dispreferred and conduct preference learning. RLCD Li et al., "RLCD: Reward Learning using Contrastive Divergence for Efficient Dialogue Generation" ____ designs positive and negative prompts to elicit corresponding responses and uses them to train a reward model for RL. Reward model distillation Chen et al., "Reward Model Distillation: A Novel Framework for Preference Alignment in Dialogue Generation" ____ aligns the distribution predicted by the policy with that of a trained reward model to enhance preference optimization robustness.

\vspace{-0.5em}