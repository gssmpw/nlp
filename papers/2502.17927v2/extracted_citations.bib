@article{chen2017learning,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{chen2023mcc,
  title={MCC-KD: Multi-CoT Consistent Knowledge Distillation},
  author={Chen, Hongzhan and Wu, Siyue and Quan, Xiaojun and Wang, Rui and Yan, Ming and Zhang, Ji},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={6805--6820},
  year={2023}
}

@article{fisch2024robust,
  title={Robust preference optimization through reward model distillation},
  author={Fisch, Adam and Eisenstein, Jacob and Zayats, Vicky and Agarwal, Alekh and Beirami, Ahmad and Nagpal, Chirag and Shaw, Pete and Berant, Jonathan},
  journal={arXiv preprint arXiv:2405.19316},
  year={2024}
}

@inproceedings{gu2024minillm,
  title={MiniLLM: Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, G},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{ko2024distillm,
  title={DistiLLM: Towards Streamlined Distillation for Large Language Models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@article{li2024direct,
  title={Direct Preference Knowledge Distillation for Large Language Models},
  author={Li, Yixing and Gu, Yuxian and Dong, Li and Wang, Dequan and Cheng, Yu and Wei, Furu},
  journal={arXiv preprint arXiv:2406.19774},
  year={2024}
}

@article{liu2020adaptive,
  title={Adaptive multi-teacher multi-level knowledge distillation},
  author={Liu, Yuang and Zhang, Wei and Wang, Jun},
  journal={Neurocomputing},
  volume={415},
  pages={106--113},
  year={2020},
  publisher={Elsevier}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{reddi2021rankdistil,
  title={Rankdistil: Knowledge distillation for ranking},
  author={Reddi, Sashank and Pasumarthi, Rama Kumar and Menon, Aditya and Rawat, Ankit Singh and Yu, Felix and Kim, Seungyeon and Veit, Andreas and Kumar, Sanjiv},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2368--2376},
  year={2021},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{starling2023,
  title={Starling-7b: Improving helpfulness and harmlessness with rlaif},
  author={Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Ganesan, Karthik and Chiang, Wei-Lin and Zhang, Jian and Jiao, Jiantao},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{wen2023f,
  title={f-Divergence Minimization for Sequence-Level Knowledge Distillation},
  author={Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={10817--10834},
  year={2023}
}

@inproceedings{wu2023ad,
    title = "{AD}-{KD}: Attribution-Driven Knowledge Distillation for Language Model Compression",
    author = "Wu, Siyue  and
      Chen, Hongzhan  and
      Quan, Xiaojun  and
      Wang, Qifan  and
      Wang, Rui",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.471/",
    doi = "10.18653/v1/2023.acl-long.471",
    pages = "8449--8465",
    abstract = "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher`s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods."
}

@article{wu2024rethinking,
  title={Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models},
  author={Wu, Taiqiang and Tao, Chaofan and Wang, Jiahao and Zhao, Zhe and Wong, Ngai},
  journal={arXiv preprint arXiv:2404.02657},
  year={2024}
}

@inproceedings{yang2023rlcd,
  title={RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment},
  author={Yang, Kevin and Klein, Dan and Celikyilmaz, Asli and Peng, Nanyun and Tian, Yuandong},
  booktitle={The Twelfth International Conference on Learning Representations},
    year={2023}
}

@article{yang2024preference,
  title={Preference-grounded token-level guidance for language model fine-tuning},
  author={Yang, Shentao and Zhang, Shujian and Xia, Congying and Feng, Yihao and Xiong, Caiming and Zhou, Mingyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{yim2017gift,
  title={A gift from knowledge distillation: Fast optimization, network minimization and transfer learning},
  author={Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4133--4141},
  year={2017}
}

@inproceedings{yoon2024tlcr,
  title={TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback},
  author={Yoon, Eunseop and Yoon, Hee Suk and Eom, SooHwan and Han, Gunsoo and Nam, Daniel and Jo, Daejin and On, Kyoung-Woon and Hasegawa-Johnson, Mark and Kim, Sungwoong and Yoo, Chang},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={14969--14981},
  year={2024}
}

@inproceedings{zhang2024plad,
    title = "{PL}a{D}: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
    author = "Zhang, Rongzhi  and
      Shen, Jiaming  and
      Liu, Tianqi  and
      Wang, Haorui  and
      Qin, Zhen  and
      Han, Feng  and
      Liu, Jialu  and
      Baumgartner, Simon  and
      Bendersky, Michael  and
      Zhang, Chao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.923/",
    doi = "10.18653/v1/2024.findings-acl.923",
    pages = "15623--15636",
    abstract = "Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate the student`s estimation of sequence likelihood, which steers the student`s focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM`s internal states, tackles the student`s expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework."
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

