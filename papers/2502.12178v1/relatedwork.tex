\section{Related Works}
\noindent
{\bf Traffic Simulation.} There are two principal approaches to traffic simulation methods: rule-based and learning-based. Rule-based approaches~\cite{rulebase1, rulebase2} analyze vehicle movements and control them along fixed paths. While these methods are intuitively understandable to users, they often lack the expressiveness necessary to accurately replicate real-world driver behavior, resulting in movements that may significantly deviate from actual driving patterns. In contrast, learning-based approaches~\cite{learningbase1, learningbase2, learningbase3, trace} use real-world traffic data~\cite{nuscenes} to train deep generative models such as VAE~\cite{vae} and diffusion model~\cite{diffusion}. Notably, recent advancements in diffusion models, which have demonstrated high performance, show great promise in traffic simulation by enabling the generation of highly realistic scenarios~\cite{motiondiffuser, traffcdiffusion1,trafficdiffusion2, trafficdiffusion3}. However, a notable limitation of learning-based approaches is their lack of controllability. In response, recent research have shown that it is possible to achieve controlled trajectory generation in learning-based models. Diffuser~\cite{diffuser} demonstrates great progress in this area, suggesting that trajectories generated by learning-based models can indeed be controlled. Recent works have sought to merge the strengths of learning-based and rule-based approaches to enhance both realism and controllability. For example, DiffScene~\cite{trafficdiffusion4} combines a diffusion model with adversarial optimization, while KING~\cite{king} use imitation learning with vehicle dynamics to achieve desired trajectories. Another approach, DJINN \cite{navigate} ensures controllability in a learning-based model by using a task-mask to control the trajectories generated, BehaviorGPT~\cite{behaviorgpt} and  InteractTraj~\cite{xia2024language} aimed to generate realistic interactive traffic trajectories using LLM. While these efforts concentrate on increasing realism and controllability of traffic scenarios, our work additionally focuses on scenario diversity, addressing a critical aspect of traffic simulation.


\noindent
{\bf Fine-tuning Diffusion Model.} Large-scale generative models, including diffusion models, have the potential to produce a broad spectrum of outcomes. Customizing these models to align with particular datasets or desired objectives is pivotal in the field of generative model research. Recent works have customized diffusion models by fine-tuning various components, such as the weights~\cite{fine-tuning1}, the embedding layer~\cite{fine-tuning2} or adaptors~\cite{fine-tuning3}, to better serve specific datasets. Additionally, other works~\cite{fine-tuning4,fine-tuning5} have focused on fine-tuning diffusion models for few-shot adaptation. Fine-tuning diffusion models significantly expands their capabilities, enabling them to address a wider range of tasks and preferences. Recent research~\cite{multitaskdiffusionfinetuning} has demonstrated that by applying multi-task learning during fine-tuning, a diffusion model can be adapted to cover multiple tasks simultaneously. Furthermore, other studies~\cite{diffusionDPO, diffusionrlhf, uehara2024feedback} use a fine-tuning approach to incorporate human preferences into the model. With the advent of Reinforcement Learning with Human Feedback~(RLHF) and DPO, methods for fine-tuning generative models to align with human preference, enable the incorporation of diverse intentions into models in terms of {\it preference}.

Our work builds upon these advancements, particularly inspired by DPO-SDXL~\cite{diffusionDPO}. We apply DPO, which is posited to be more effective than RLHF in certain contexts, as it bypasses the need for learning a reward model and consequently avoids the pitfalls of reward hacking~\cite{dpo}. By leveraging DPO, we aim to generate traffic scenarios that are not only more realistic and diverse but also more controllable, addressing key challenges in traffic simulation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%