\section{Related Works}
\noindent
{\bf Traffic Simulation.} There are two principal approaches to traffic simulation methods: rule-based and learning-based. Rule-based approaches____ analyze vehicle movements and control them along fixed paths. While these methods are intuitively understandable to users, they often lack the expressiveness necessary to accurately replicate real-world driver behavior, resulting in movements that may significantly deviate from actual driving patterns. In contrast, learning-based approaches____ use real-world traffic data____ to train deep generative models such as VAE____ and diffusion model____. Notably, recent advancements in diffusion models, which have demonstrated high performance, show great promise in traffic simulation by enabling the generation of highly realistic scenarios____. However, a notable limitation of learning-based approaches is their lack of controllability. In response, recent research have shown that it is possible to achieve controlled trajectory generation in learning-based models. Diffuser____ demonstrates great progress in this area, suggesting that trajectories generated by learning-based models can indeed be controlled. Recent works have sought to merge the strengths of learning-based and rule-based approaches to enhance both realism and controllability. For example, DiffScene____ combines a diffusion model with adversarial optimization, while KING____ use imitation learning with vehicle dynamics to achieve desired trajectories. Another approach, DJINN ____ ensures controllability in a learning-based model by using a task-mask to control the trajectories generated, BehaviorGPT____ and  InteractTraj____ aimed to generate realistic interactive traffic trajectories using LLM. While these efforts concentrate on increasing realism and controllability of traffic scenarios, our work additionally focuses on scenario diversity, addressing a critical aspect of traffic simulation.


\noindent
{\bf Fine-tuning Diffusion Model.} Large-scale generative models, including diffusion models, have the potential to produce a broad spectrum of outcomes. Customizing these models to align with particular datasets or desired objectives is pivotal in the field of generative model research. Recent works have customized diffusion models by fine-tuning various components, such as the weights____, the embedding layer____ or adaptors____, to better serve specific datasets. Additionally, other works____ have focused on fine-tuning diffusion models for few-shot adaptation. Fine-tuning diffusion models significantly expands their capabilities, enabling them to address a wider range of tasks and preferences. Recent research____ has demonstrated that by applying multi-task learning during fine-tuning, a diffusion model can be adapted to cover multiple tasks simultaneously. Furthermore, other studies____ use a fine-tuning approach to incorporate human preferences into the model. With the advent of Reinforcement Learning with Human Feedback~(RLHF) and DPO, methods for fine-tuning generative models to align with human preference, enable the incorporation of diverse intentions into models in terms of {\it preference}.

Our work builds upon these advancements, particularly inspired by DPO-SDXL____. We apply DPO, which is posited to be more effective than RLHF in certain contexts, as it bypasses the need for learning a reward model and consequently avoids the pitfalls of reward hacking____. By leveraging DPO, we aim to generate traffic scenarios that are not only more realistic and diverse but also more controllable, addressing key challenges in traffic simulation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%