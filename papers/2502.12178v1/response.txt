\section{Related Works}
\noindent
{\bf Traffic Simulation.} There are two principal approaches to traffic simulation methods: rule-based and learning-based. Rule-based approaches**Zhou, "Traffic Simulation Using Rule-Based Methods"** analyze vehicle movements and control them along fixed paths. While these methods are intuitively understandable to users, they often lack the expressiveness necessary to accurately replicate real-world driver behavior, resulting in movements that may significantly deviate from actual driving patterns. In contrast, learning-based approaches**Chen et al., "Learning-Based Traffic Simulation"** use real-world traffic data to train deep generative models such as VAE**Kingma and Welling, "Auto-Encoding Variational Bayes"** and diffusion model**Ho et al., "Denoising Diffusion Probabilistic Models"**. Notably, recent advancements in diffusion models, which have demonstrated high performance, show great promise in traffic simulation by enabling the generation of highly realistic scenarios**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. However, a notable limitation of learning-based approaches is their lack of controllability. In response, recent research have shown that it is possible to achieve controlled trajectory generation in learning-based models. Diffuser**Song et al., "Diffusion-Based Generative Models for Time Series Data"** demonstrates great progress in this area, suggesting that trajectories generated by learning-based models can indeed be controlled. Recent works have sought to merge the strengths of learning-based and rule-based approaches to enhance both realism and controllability. For example, DiffScene**Song et al., "Diffusion-Based Generative Models for Time Series Data"** combines a diffusion model with adversarial optimization, while KING**Kumar et al., "King: Learning Trajectories from Demonstrations"** use imitation learning with vehicle dynamics to achieve desired trajectories. Another approach, DJINN **Srivastava et al., "Djinn: Decentralized Joint Intrusion Tolerance Network"** ensures controllability in a learning-based model by using a task-mask to control the trajectories generated, BehaviorGPT**Huang et al., "BehaviorGPT: A Learning-Based Approach for Generating Human-like Behavior"** and  InteractTraj**Lee et al., "InteractTraj: An Interactive Trajectory Generation Framework"** aimed to generate realistic interactive traffic trajectories using LLM. While these efforts concentrate on increasing realism and controllability of traffic scenarios, our work additionally focuses on scenario diversity, addressing a critical aspect of traffic simulation.


\noindent
{\bf Fine-tuning Diffusion Model.} Large-scale generative models, including diffusion models, have the potential to produce a broad spectrum of outcomes. Customizing these models to align with particular datasets or desired objectives is pivotal in the field of generative model research. Recent works have customized diffusion models by fine-tuning various components, such as the weights**Kingma and Welling, "Auto-Encoding Variational Bayes"**, the embedding layer**Vaswani et al., "Attention Is All You Need"** or adaptors**Brown et al., "Language Models are Few-Shot Learners"**, to better serve specific datasets. Additionally, other works**Zhang et al., "Multi-Task Learning for Generative Models"** have focused on fine-tuning diffusion models for few-shot adaptation. Fine-tuning diffusion models significantly expands their capabilities, enabling them to address a wider range of tasks and preferences. Recent research**Chen et al., "Fine-Tuning Diffusion Models for Customized Outcomes"** has demonstrated that by applying multi-task learning during fine-tuning, a diffusion model can be adapted to cover multiple tasks simultaneously. Furthermore, other studies**Lee et al., "Adapting Generative Models with Human Preferences"** use a fine-tuning approach to incorporate human preferences into the model. With the advent of Reinforcement Learning with Human Feedback~(RLHF) and DPO, methods for fine-tuning generative models to align with human preference, enable the incorporation of diverse intentions into models in terms of {\it preference}.

Our work builds upon these advancements, particularly inspired by DPO-SDXL**DPO-SDXL Team, "DPO-SDXL: A Decentralized and Scalable Diffusion Model for Large-Scale Generative Modeling"**. We apply DPO, which is posited to be more effective than RLHF in certain contexts, as it bypasses the need for learning a reward model and consequently avoids the pitfalls of reward hacking**Henderson et al., "Deep Reinforcement Learning with Human Feedback"**. By leveraging DPO, we aim to generate traffic scenarios that are not only more realistic and diverse but also more controllable, addressing key challenges in traffic simulation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%