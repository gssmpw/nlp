\documentclass[11pt]{article}
\everypar{\looseness=-1}
\usepackage{fullpage}


\usepackage{authblk}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %

\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}





\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx} %
\usepackage{subfig}
\usepackage{latexsym}
\usepackage{amscd}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsthm}
\usepackage{comment} 
\usepackage{cleveref}
\usepackage{nicefrac}
\usepackage{enumitem}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\usepackage[textsize=tiny]{todonotes}

\crefname{assumption}{assumption}{assumptions}
\usepackage{nicefrac}
\usepackage{xcolor}
\usepackage{url}
\usepackage{natbib}
\newcommand\cmnt[2]{\;
{\textcolor{red}{[{\em #1 --- #2}] \;}
}}
\newcommand\bcmnt[2]{\;
{\textcolor{blue}{[{\em #1 --- #2}] \;}
}}
\newcommand\angela[1]{\bcmnt{#1}{Angela}}

\usepackage{shortcuts}
\newcommand{\Enb}[1]{\E_n\bracks{{#1}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Batch-Adaptive Annotations for Causal Inference with Complex-Embedded Outcomes}
\author[1]{Ezinne Nwankwo}
\author[2]{Lauri Goldkind}
\author[3]{Angela Zhou}
\affil[1]{Department of Electrical Engineering and Computer Sciences, University of California, Berkeley}
\affil[2]{Department of Social Work, Fordham University}
\affil[3]{Department of Data Sciences and Operations, University of Southern California}

\begin{document}
\maketitle


\begin{abstract}
Estimating the causal effects of an intervention on outcomes is crucial. But often in domains such as healthcare and social services, this critical information about outcomes is documented by unstructured text, e.g. clinical notes in healthcare or case notes in social services. For example, street outreach to homeless populations is a common social services intervention, with ambiguous and hard-to-measure outcomes. Outreach workers compile case note records which are informative of outcomes. Although experts can succinctly extract relevant information from such unstructured case notes, it is costly or infeasible to do so for an entire corpus, which can span millions of notes. %
Recent advances in large language models (LLMs) enable scalable but potentially inaccurate annotation of unstructured text data. We leverage the decision of which datapoints should receive expert annotation vs. noisy imputation under budget constraints in a ``design-based" estimator combining limited expert and plentiful noisy imputation data via \textit{causal inference with missing outcomes}. We develop a two-stage adaptive algorithm that optimizes the expert annotation probabilities, estimating the ATE with optimal asymptotic variance.
We demonstrate how expert labels and LLM annotations can be combined strategically, 
efficiently and responsibly in a causal estimator. We run experiments on simulated data and two real-world datasets, including one on street outreach, to show the versatility of our proposed method.
\end{abstract}

\section{Introduction}

Evaluating causal effects of a treatment or policy intervention is a challenging problem in its own right, but an added layer of complexity comes when there is missing data. In this paper, we consider the case where there is missingness in the outcome variable, and it would take substantial cost and effort to obtain more expert coded labels for the outcome. Recent advances in machine learning and LLMs have shown that these models have extreme promise as tools for generating such labels. But in the social sciences, ultimately inference is the goal and these labels can be error-prone and biased. 

Motivated by a collaboration with a New York based nonprofit, we seek to evaluate the impact of street outreach on housing outcomes. In New York City alone, approximately $\$80,000,000$ per year is invested in homeless street outreach to an unclear effect. It is a time-consuming process, and it is unclear how the impacts of such intensive individualized outreach might compare to other proposed approaches, such as those focusing on placing entire networks of individuals together. While the nonprofit reports key metrics such as number of completed placements in housing services, these can be somewhat rare due to length of outreach, delays in waiting for housing, matching issues, etc; moreover, much of a successful placement is out of the control of outreach due to highly limited housing capacities. Measuring the impacts of street outreach on intermediate outcomes such as accessing benefits and services, completing required appointments and interviews, can better reflect the immediate impacts of street outreach. %
However, this additional outcome information is frequently only measured in unstructured text data or case notes which provide a noisy view on the ground-truth of what happened during an outreach contact. Was a client making progress towards completing an housing application or their goals, or were they facing other barriers? This structured information can be extracted from case notes via qualitative case notes --- in our experience, outreach workers can readily fill in the broader context of engagements and recognize important milestones when reading case notes from others --- but the volume of case notes is simply impossible for expert human annotators to expertly label. While modern natural language processing tools can facilitate annotation at scale, they are often inaccurate. \textit{
Given an annotation budget constraint, how can we strategically collect ground truth data, such as by assigning expert annotation, while leveraging additional data sources or weaker annotation to optimize causal effect estimation?} 

This problem is not unique to the social work domain and can generally apply to cases with measurement error with misaligned modalities (such as text or images), where it is possible to query the ground truth directly for some portion of the data and at a cost.
In some settings, we can query other data sources for ground-truth labels directly for some portion of the data and at a cost, while weaker imputation of the labels based on auxiliary information is feasible at scale but second-best due to inaccuracy. 
For example, when an outcome variable, wages, is only observed from self-reported working individuals, surveyors could conduct follow-up interviews with participants to obtain wage data, but this can be expensive. Nonetheless, noisy imputation can be possible with noisy measures from the same dataset (such as last year's wages) or transporting prediction models from national wage databases. %

Another example is when raw observations are available, such as raw text or images. We gemerally term this additional source of data as \textit{complex embedded outcomes}, $\tilde{Y}$, in our framework, where the embedded outcome encodes information about the ground truth, $Y$. %
We can either extract $Y$ from $\tilde{Y}$ at a cost, the best-possible oracle, or obtain model annotated outcomes (for example from a pretrained model) $f(\tilde{Y})$ and use it to supplement our analysis. %
Such general trade-offs between expert annotation and scalable, weaker imputation are pervasive in data-intensive machine learning, for example as in the recent ``LLM-as-a-judge" framework \citep{zheng2023judging}.%

In an ideal world, researchers would have expertly labeled data for the entire dataset but obtaining such labels can be very expensive. A common solution would be to only use expertly annotated data for causal estimation but we lose out on potential information and efficiency gain from $\tilde{Y}$. Because our complex embedded outcomes are high-dimensional and often on a different scale than our ground truth labels, we cannot directly replace the outcomes $Y$ with $\tilde{Y}$. Instead, we would like to take advantage of the combined strengths of ground truth expert labels and model annotated outcomes, $f(\tilde{Y})$, that we get from our complex embedded outcomes to obtain valid causal estimates. $f(\tilde{Y})$ can contain measurement error that leads to bias in downstream estimators and our ground truth labels may not be enough to get accurate estimates, so we would like to find a way to take advantage of the model labels when they are accurate and ground truth labels when they are available. To do this, we make use of the double robustness properties of the augmented inverse propensity weighted (AIPW) estimator for the average treatment effect (ATE) with missing outcomes. And with the methods in our work, we can optimally choose which data points to label in order to minimize the asymptotic variance (hence uncertainty, valid confidence interval width) of the final estimator. 





 When data collection resources are limited, we would like to strategically choose data points to get expert labels for data points where our model annotations are expected to do poorly on. %
 In our setting, some data influence the estimator more than others, so we aim to sample more in that region to mitigate potential impacts. For example, data observed in regions of low treatment propensity or noisy outcome information heavily influence causal inference; increasing the effective sample complexity by labeling improves final variance. %



This study makes the following contributions: Methodologically, we propose a two-stage batch-adaptive algorithm for efficient ATE estimation from complex embedded outcomes. We derive the expert labeling probability that minimizes the asymptotic variance of an orthogonal estimator \citep{bia2021dmlsampleselection}. We design a two-stage adaptive experiment and estimation procedure. The first stage estimates nuisance functions for the asymptotic variance on the fully observed data. We use the estimates and functions from the first stage to estimate the optimal labeling probabilities in the second stage. The final proposed estimator combines the model-annotated labels and the expert labels under each treatment assignment in a doubly robust estimator for the ATE. We show that this two-stage design is adaptive, achieving the optimal asymptotic variance. We present empirical evidence of the improvements of our approach on simulated data, where ground-truth is available. 




\section{Related work}

Our model is closest to a design-based perspective on a non-classical measurement error model with validation data, wherein we choose which datapoints to query for validation data. Typical distributional conditions for nonclassical measurement error \citep{schennach2016recent} are generally inapplicable to the relationship between ground-truth annotation and pretrained decoders, our motivating application. 

The most related recent work is that of \cite{naoki2023dsl,zrnic2024active}, which leverages the fact that sampling probabilities for data annotation are known to obtain doubly-robust estimation via causal inference. These works generally address non-causal estimands such as mean estimation and M-estimation (therefore without discussion of treatment effect estimation). 
Our batched adaptive allocation protocol is closest to \citep{hahn2011adaptive}, which studied a two-stage procedure for estimating the ATE, although the goals of estimation are very different. They consider a proportional asymptotic and a two-stage procedure, and show asymptotic equivalence of their batched adaptive estimator to the optimal asymptotic variance. However, \citep{hahn2011adaptive} and other papers focus on allocating treatments, while we allocate probability of revealing the outcome for a datapoint (i.e via expert annotation); the optimization problem is different. We additionally focus on a double-machine learning estimator. The recent work of \citep{li2024double} is most relevant, though our set up is different and we further characterize the closed-form of the optimal labelling probabilities for our setting, which can be of independent interest. 




Regarding the use of such auxiliary information in causal inference, many recent works have studied the use of surrogate or proxy information. Although we use ``pretrained decoders" in a similar sense as colloquial notions of surrogates or proxies, recent advances in surrogate and proxy variables in causal inference have coalesced around certain models that are different from our setting \citep{athey2019surrogate,kallus2024role,naoki2023dsl}. However, in much of the surrogate literature, surrogates measure an outcome that is impossible to measure at the time of analysis. The canonical example in \cite{athey2019surrogate} studies the long-term intervention effects of job training on lifetime earnings, by using only short-term outcomes (surrogates) such as yearly earnings. In this regime, the ground truth cannot be obtained at the time of analysis. In this paper, we focus a different regime where obtaining the ground truth from expert data annotators is feasible but budget-binding. 
We do leverage that we can design the sampling probabilities of outcome observation (ground-truth annotation) or missingness for doubly-robust estimation, like some methods in the surrogate literature or data combination \citep{yang2020combining,kallus2024role}. But we treat the underlying setting as a single unconfounded dataset with missingness. 
The different setting of proximal causal inference \citep{tchetgen2024introduction,cui2024semiparametric} seeks proxy outcomes/treatments that are informative of unobserved confounders; we assume unconfoundedness holds. Recently, \citep{chen2024proximal} study the ``design-based supervised learning" perspective of \citep{naoki2023dsl} specifically for proxies for unobserved confounding.



Many exciting recent works study adaptive experimentation under different desiderata, such as full adaptivity, in-sample decision regret or finite-sample, non-asymptotic guarantees \citep{gao2019batched,zhao2023adaptive,cook2024semiparametric}. Such designs are closely related to covariate-adaptive-randomization; the recent work of  \citep{shiusing} studies delayed outcomes. We outline how our approach is a good fit for our motivating data annotation setting. Full-adaptivity is less relevant in our setting with ground-truth annotation from human experts, due to distributed-computing-type issues with random times of annotation completion. But standard tools such as the martingale CLT can be applied to extend our theoretical results to full adaptivity. Additionally, many recent works primarily focus on the different problem of treatment allocation for ATE estimation. In-sample regret is less relevant for our setting of data annotation, which is a pure-exploration problem.

\section{Problem Setup}
We overview our problem formulation. We assume the ground-truth data-generating process comprises of $(X,Z,Y(Z)),$ where $Y(Z)$ are potential outcomes in the Neyman-Rubin potential outcome framework, where we only observe $Y(Z)$ for the realized treatment assignment under the usual stable unit value treatment assumption (SUTVA). 


If the ground-truth data were observed, we would have a standard causal inference task at hand. However, it is not; we assume that we have observed data $(X_i, Z_i, \tilde{Y}_i)_{i=1}^n,$ where $X \in \mathbb{R}^p$ are covariates, $Z\in\{0,1\}$ is (binary) treatment, and $\tilde{Y}$ is widely available ``complex encoded outcome" information.\footnote{Although we can define a potential outcome $\tilde{Y}(Z)$, we are generally uninterested in causal inference in the ambient high-dimensional space of $\tilde{Y}(Z)$ itself - corresponding to, in our examples, the effect of the presence of a tumor on the pixel image, the effect of street outreach on the linguistic characteristics of casenotes written for documentation, etc --- $\tilde{Y}(Z)$ is relevant to causal estimation insofar as it is informative of latent outcomes $Y(Z)$. This is consistent with viewing certain types of NLP tasks consistent with ``anti-causal learning" \citep{scholkopf2012causal}/, wherein outcomes cause measurements thereof, such as in supervised classification where a label of ``cat" or ``dog" causes the classification covariates (e.g. image) \citep{jin2021causal}.
} Though $\tilde{Y}$ is observable for every datapoint, it is generally unsuitable for use in direct estimation, such as complex multi-modal data: images, text, etc. However, it is assumed that causal effects operate through a latent true outcome $Y$, of which $\tilde{Y}$ is a complex observation thereof. We focus in particular on a setting where $Y$ can be recovered from $\tilde{Y}$ with some resource-intensive effort (such as crowd-labeling, expert annotation, or outreach interviews). We however assume that $Y$ is not a deterministic function of $\tilde{Y}$. 
\begin{assumption}[Complex embedded outcomes]
    $\tilde{Y} = g(Y) + \epsilon, \epsilon \neq 0 \;a.s.$
\end{assumption}

We let $R\in\{0,1\}$ denote the presence or absence of the expert annotations $Y$, where $R=1$ indicates the latent outcome $Y$ is observed. 

Therefore, our observational dataset for estimation is $(X,Z,\tilde{Y}, R,RY)$, i.e. with missingness in the expert outcomes due to budget constraints. 
\begin{assumption}[Consistency]\label{asn-consistency}
$Y = Z R Y_1 + (1-Z) R  Y_0.$
\end{assumption}
For the purposes of causal identification, we generally proceed under the following assumptions: 

\begin{assumption}[Treatment ignorability]\label{asn-tx-ignorability}
 $Y(Z) \perp Z \mid X.$
\end{assumption}

\begin{assumption}[$R$-ignorability]\label{asn-r-ignorability}
      $R \independent Y(Z) \mid Z,X$ 
\end{assumption}
\Cref{asn-tx-ignorability}, or unconfoundedness, posits that the observed covariates are fully informative of treatment. It is generally untestable but robust estimation is possible in its absence, e.g. via sensitivity analysis and partial identification \citep{zhao2019sensitivity,kallus2021minimax}. 

On the other hand, \Cref{asn-r-ignorability} is true by design: we choose what datapoints are annotated for ground-truth. \Cref{asn-r-ignorability} rules out instances of domain shift where documents from the target population are not available at annotation time, but captures the case where the full corpus of documents needed to be annotated is available from the outset. 


Although one approach is completely random sampling, we are particularly concerned with \textit{how can we select datapoints for expert annotation for optimal estimation}? We assume we have a limited budget for data annotation, but have control over the assignment of data to expert annotations. Then the probability of revealing the true outcome $Y$, $\pi(Z,X) = P(R = 1|Z,X)$, is in fact a decision variable of our choosing. 


Define
\begin{align*}
        e_z(X) &:=P(Z=z|X) \tag{propensity score}\\
        \pi(Z,X) &:= P(R=1|Z,X) \tag{annotation probability}
\end{align*}
\begin{assumption}[Treatment and annotation positivity]\label{asn-positivity}
    ${P}(\epsilon < \pi(z,X) < 1-\epsilon), z \in \{0,1\}$ and
    
    ${P}(1/\nu < e_1(X)<1-1/\nu), \nu > 0$ 
\end{assumption}

We also make an exclusion restriction assumption about the nature of the complex embedded outcomes; the direct effect of treatment passes through the latent outcomes only. 
\begin{assumption}[]
    $Z \perp \tilde{Y}(Z) \mid X,Y(Z)$
\end{assumption}
This can be restrictive and defines our regime of ``complex embedded" outcomes closest to that of measurement error. It asserts that treatment assignment does not affect aspects of $\tilde{Y}$ beyond the effects on latent outcomes $Y$. For example, in the medical setting, the assumption holds if a treatment affects underlying biological phenomena, e.g. makes a tumor smaller, and that it is these phenomena themselves which are recorded via clinical notes or imaging, e.g. raw pixel images. It assures that leveraging $\tilde{Y}$ to predict latent outcomes $Y$ does not introduce colider bias. It is potentially testable in a validation dataset, i.e. after the first batch of our procedure. 

In our setting, we allow the outcome model to depend on the complex embedded $\tilde{Y}$:   \begin{align*}\mu_z(1,X, \tilde{Y})&:= \Eb{Y|Z=z,R=1,X,\tilde{Y}} \end{align*}

The outcome model can take on many different forms and ultimately it is up to the practitioner to decide which outcome model to use. We denote a prediction based on $\tilde{Y}$ (with $X$ covariates and treatment information) as $f_z(X, \tilde{Y});$ for example it could arise from zero-shot prediction using an LLM or other pretrained model. There are a few options to incorporate model-annotated predictions in the outcome model: 
\begin{align*}
\mu_z(1, X, \tilde{Y})  &=  f_z(X, \tilde{Y}), \text{ or } \tag{zero-shot predictions} \\
&=\E[Y\mid Z=z,R=1,f_z(X, \tilde{Y})], \text{ or }
\tag{ground-truth-calibrated predictions}\\
&= \E[Y\mid Z=z,R=1,X, f_z(X, \tilde{Y})] \tag{prediction-augmented outcome modeling}
\end{align*}
This last approach is suggested in  \citet{naoki2023dsl}.
Later on, we find that in practice, choosing the outcome model that reduces the mean squared error leads to better numerical results. 




We consider a two-batch adaptive setup, where the first batch has $n_1 $ observations and the second batch has $n_2$ observations randomly drawn from the population, and $n= n_1 + n_2$. In the first batch, we randomly assign units an annotation probability $\pi_1$, which does not depend on covariates but is subject to a budget constraint on the expected number of collected expert labels, $n_b$. We consider a proportional asymptotic regime where the budget is a fixed proportion of the dataset size.
\begin{assumption}[Proportional asymptotic]
    \[\lim_{n \rightarrow\infty} \frac{n_1}{n} = \kappa\] %
\end{assumption}

 In the first batch, outcomes are realized and observed, and the nuisance models ($\hat{\mu}_z, \hat{e}_z(X), \hat{\sigma}^2_z(X) $) are trained on the observed data. In the second batch, we solve for $\hat{\pi}_2$, which is the probability that units in batch 2 are expertly annotated. We combine the results from both batches and use for ATE estimation, which we describe in the next section.           





\section{Method}


This section outlines our proposed methodology. We first recap the AIPW estimator for the missing outcomes case and provide the lower bound for the asymptotic variance in section 4.1. Then we consider a treatment budget and global budget optimization problem and solve for the optimal $\pi^*(z,X)$ in section 4.2. We also design a two-batch adaptive experiment. In section 4.3, we discuss feasible estimation of the ATE by the AIPW estimator (with missing outcomes) and prove a central limit theorem (CLT) for the setting where annotation probabilities are assigned adaptively and nuisance parameters must be estimated. 

\subsection{Recap: Optimal asymptotic variance for the ATE with missing outcomes}
Our target parameter of interest is the average treatment effect (ATE) of a binary treatment vector $Z$ on an outcome $Y$.
\[ \tau = \E[Y(1) - Y(0)].\]

Under the identifying assumptions in Section 3, it is implied that 
\[ \E[Y(z)|X] = \E[Y|Z=z,X] = \E[Y|Z=z,R=1,X], \] and the mean potential outcome is identified by \[\E[Y(z)] 
=\E[\E[Y|Z=z,R=1,X, \tilde{Y} ]]
=\E[\E[Y|Z=z,R=1,X]].\] %



Since $\tilde{Y}$ is fully observed on the single dataset, we state the following developments for the sufficient marginalization $\E[Y|Z=z,R=1,X],$ with the understanding that analogous estimators are easily obtained for $\E[Y|Z=z,R=1,X,\tilde{Y}],$ including various specifications of outcomes  From \citet{bia2021dmlsampleselection}, we obtain the following identification result, in the case of outcomes that are missing at random: 

\begin{align*}
    \E[Y(z)] &= \E[\psi_z], \mathrm{ where } \;\psi_z %
    = \frac{\mathbb{I}[Z=z] R  (Y-\mu_z(1,X))}{e_z(X)  \pi(z,X)} + \mu_z(1,X)\\
    \tau_{AIPW} &= \E[\psi_1 - \psi_0]   \nonumber
\end{align*}









\citet{hahn1998variancebound} provide a lower bound for the variance of the ATE. We derive the terms of the lower bound for $\tau_{AIPW}$ above. 
Define  
    \begin{align*}
        \sigma^2_z(X) &= \E[(Y - \mu_z(1,X))^2 \mid Z=z,X=x].
    \end{align*}
\begin{proposition}\label{prop-avar-ate}

    Any regular estimator for the ATE with missing outcomes %
    has the following lower bound on the asymptotic variance
    \begin{align*}
    V %
    \geq 
     \mathrm{Var}\bigg[\mu_1(1,X) - \mu_{0}(1,X)\bigg] +
    \sum_{z\in\{0,1\}} &\Eb{ \frac{\sigma^2_z(X)}{e_z(X) \cdot \pi(z,X)}  } 
\end{align*}

\end{proposition}

Note that by marginalization over $\tilde{Y}$, the estimators $\hat{\mu}_z(1,X,\tilde{Y})$ and $\hat{\sigma}^2_z(X) = \E[(Y - \hat{\mu}_z(1,X,\tilde{Y}))^2 \mid Z=z,X=x]$ can work for our estimator. 

\subsection{Two-batch adaptive design and optimal $\pi^*(z,X)$}

We first introduce the two-batch adaptive design using an oracle $\tau_{AIPW}$ and solving for the population optimal $\pi^*(z,X)$. For the oracle, we assume that we know the true values of the typically unknown nuisance functions. In the first batch, we sample data points according to probability $\pi_1$. We already know the true values for $\mu_z,\sigma^2_z(X),$ and $e_z(X)$. In the second batch, we plug in the values for the nuisance parameters and find the $\pi^*(z,X)$ that minimizes the asymptotic variance bound on $\tau_{AIPW}$ and satisfies the sampling budget constraint. The remainder of this sections shows the solution to the optimal annotation probability.  

To find the optimal sampling probabilities $\pi^*(z,X)$, we optimize this expression for the asymptotic variance over $\pi(z,X)$. This is interesting only when there is a sampling budget, which is nonetheless relevant in many cases. We first consider a setting with different a priori fixed budgets within each treatment group, where $$\text{sampling budget proportion } B_z \in [0,1]$$ is the max percentage of the treated group $Z=z$ that can be annotated.  
Given that we are trying to choose the $\pi$ that minimizes this variance bound, we only need to focus on the terms that depend on $\pi$ and can drop the rest. Supposing oracle knowledge of propensities and outcome models, the optimization problem, for each $z \in \{0,1\}$ is: 
\begin{align}
  \min_{0 < \pi(z,x) \leq 1, \forall z,x}  & \left\{ \Eb{ \frac{\sigma^2_z(X)}{e_z(X)  \pi(z,X)}  }  
  \colon 
\Eb{\pi(z,X) \mid Z=z} \leq B_z, z\in \{0,1\} \right\} \tag{z-budget}
\end{align}


\begin{theorem}[]\label{thm-z-budget-solution}
The solution to the within-$z$-budget problem is: $$
\pi^*(z,X) = \frac{\sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}}}{\Eb{ \sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}}\mid Z=z } } \cdot B_z 
$$. 
\end{theorem}


We also consider the case with a global budget proportion $B \in [0,1]$ over all annotation, without additional restrictions on how many are treated within each treatment subgroup. 
\begin{align}
  \min_{0 < \pi(z,x) \leq 1, \forall z,x} &\sum_{z\in\{0,1\}}
  \left\{ \Eb{ \frac{\sigma^2_z(X)}{e_z(X)  \pi(z,X)}  }  \colon \E[\pi(Z,X)]\leq B 
  \right\} 
  \tag{OPT (global budget)} \label{eqn-opt-global-budget}
\end{align}
Note that in the global budget constraint, $\E[\pi(Z,X)]=\E[\pi(1,X)\mathbb{I}[Z=1] +\pi(0,X)\mathbb{I}[Z=0] ]$. We can characterize the solution as follows. 


\begin{theorem}[]\label{thm-global-budget-solution}
The solution to the global budget problem is:     
\begin{align*}
&\pi^*(z,X) 
= 
 \sqrt{\frac{\sigma^2_z(X)}{  e_z^2(X)}}B
\left(
\Eb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\sigma^2_1(X)}{  e_1^2(X)}} 
+ 
\mathbb{I}[Z=0]\sqrt{\frac{\sigma^2_0(X)}{  e_0^2(X)}} }\right)^{-1} 
\end{align*} 
\end{theorem}

Note that sampling probabilities increase in the conditional variance/uncertainty of the model, $\sigma^2(X)$, and the inverse propensity score. The latter is specific to the ATE setting, though the solution recovers that of mean estimation, for example, if treatments were uniformly assigned. Characterizing the closed-form solution is useful for our analysis later on, in establishing convergence of estimation to the limiting optimal data annotation probabilities. 

\subsection{Feasible batch adaptive design and estimator}

Finding the optimal $\pi^*(z,X)$ as we have above assumes that we have knowledge of the mean squared error between the true outcome and our conditional mean outcome, that is $\sigma^2_z(X)$ and the propensity scores $e_z(X)$. %
We propose a feasible two-batch adaptive design leveraging the double machine learning (DML) framework \citep{chernozhukov2018double,bia2021dmlsampleselection}, with cross-fitting within each batch. We conduct uniform sampling in a first batch, whose data we use to estimate nuisance functions that are plugged-into the asymptotic variance expressions. Cross-fitting was recently proposed in \citep{li2024csbae}, convergent split batch adaptive experiment (CSBAE). 

In standard double machine learning with iid data, cross-fitting splits the data, estimating nuisance functions on one fold that are used to evaluate the estimator on another fold of data. (The number of folds can vary; $K=2$ to $K=5$ is typical). Cross-fitting guarantees that nuisance function estimates are independent of the observations in fold k which is a key argument in ensuring that the feasible estimator $\tau_{AIPW}$ is asymptotically equivalent to $\tau$. Let $D=(X,Z,R,Y)$ and $\mathcal{I}_k$ denote the index set of the $k$th fold. A typical DML estimator with cross-fitting  computes $\hat{\tau}_{AIPW}$ empirically as $\frac 1n \sum_{k=1}^K \sum_{i\in \mathcal{I}_k} \hat{\psi}_1(D_i;\hat{\mu}_1^{(-k)}(\cdot), \hat{e}_1^{(-k)}(\cdot) ,\hat{\pi}^{(-k)}(1,\cdot)) - \hat{\psi}_0(D_i;\hat{\mu}_0^{(-k)}(\cdot), \hat{e}_0^{(-k)}(\cdot),\hat{\pi}^{(-k)}(0,\cdot))$, where %
$\hat{\mu}_z^{(-k)}(\cdot), \hat{e}_z^{(-k)}(\cdot),\hat{\pi}^{(-k)}(z,\cdot)$ are out-of-fold nuisance estimates learned on data outside of fold $k$. %

\begin{algorithm}[t!]
   \caption{Batch Adaptive Causal Estimation With Complex Embedded Outcomes}
   \label{alg:example}
 \begin{algorithmic}
    \STATE {\bfseries Input:} Data $\mathcal{D} = \{(X_i,Z_i,Y_i,\tilde{Y}_i)\}_{i=1}^n$, sampling budget $B_z$ for $z\in \{0,1\}$ 
    \STATE {\bfseries Output:} ATE estimator $\hat{\tau}_{AIPW}$
    \STATE Partition $\mathcal{D}$ into 2 batches and K folds $\mathcal{D}^{(k)}_{1},\mathcal{D}^{(k)}_{2}$ for $k=1,\hdots,K$
    \STATE \textit{ Batch 1:} 
    \FOR{$k=1,\hdots,K$}
    \STATE
    On $\mathcal{D}^{(k)}_{1}$:  Sample $R_1 \sim \mathrm{Bern}(\pi_1(Z,X))$, where $\pi_1(z,x) = B_z$. 

    Estimate nuisance models:  Where $R=1$, estimate $\hat{\mu}^{(k)}_z$ by regressing $Y$ on $X,\tilde{Y}$, and $\hat{\sigma}^{2(k)}_z$ by regressing $(Y-\hat{\mu}_z)^2$ on $X$. Estimate $\hat{e}^{(k)}_z$ by regressing $Z$ on $X$.
    \ENDFOR
      \STATE \textit{ Batch 2:} 
        \FOR{$k=1,\hdots,K$} 
            \STATE  On $\mathcal{D}^{(k)}_{2}$:  Obtain $\pi^*$ by optimizing \cref{eqn-opt-global-budget}, plugging in  $\hat{\mu}^{(-k)}_z$, $\hat{\sigma}^{2(-k)}_z$, and $\hat{e}^{(-k)}_z$. 
            \STATE Solve for 
            $\hat{\pi}^{(k)}_{2}(X_{i}) = \frac{1}{1-\kappa}(\pi^*(X_i) - \kappa\pi_1)$
            \STATE Sample $R_2 \sim \mathrm{Bern}(\hat{\pi}^{(k)}_2(X_{i})$
        \ENDFOR
    \STATE Obtain $\mathcal{D}^{(k)}$ for $k=1,\hdots,K$ by pooling across batches $\mathcal{D}^{(k)}_1$ and $\mathcal{D}^{(k)}_2$
    \STATE On $\mathcal{D}^{(k)}$, re-estimate $\hat{\mu}^{(k)}_z$, $\hat{\sigma}^{2(k)}_z$,and $\hat{e}^{(k)}_z$  on observed outcomes $RY$ for $k=1,\hdots,K$
    \STATE On $\mathcal{D}^{(k)}$, run optimization procedure to get $\pi^{*(-k)}$ with out of fold nuisances $\hat{\mu}^{(-k)}_z$, $\hat{\sigma}^{2(-k)}_z$, and $\hat{e}^{(-k)}_z$. 
    \STATE On full data $\mathcal{D}$, estimate ATE by using AIPW estimator in \cref{eq:AIPW-estimator} and out of fold nuisances $\pi^{*(-k)}$, $\hat{\mu}^{(-k)}_z$, $\hat{\sigma}^{2(-k)}_z$, and $\hat{e}^{(-k)}_z$
 \end{algorithmic}
\end{algorithm}
Next, we define the crossfitting procedure CSBAE which offers a way to maintain such independence in a two-batch adaptive experiment. 
\begin{definition}[Cross-fitting for batch adaptive experiment \citep{li2024csbae}]\label{defn-csbae}
    First, we split the observations in each batch $t=1,2$ into $K$ folds (e.g. $K=5$). Let $\mathcal{I}_{k}$ denote the set of batch and observation indices $(t,i)$ assigned to fold $k$ and batch $t$. The number of observations in batch $t$ assigned to fold $k$ is $n_{t,k} = \mid \{(t,i) \in \mathcal{I}_k\mid i = 1,\hdots,n_t \}\mid$. Then within each fold, we estimate nuisance models on observations in batch 1 and we adaptively assign annotation probabilities in batch 2. This ensures independence, that is the nuisance models only depend on observations in the previous batch from the same fold.

\end{definition}

We use the CSBAE cross-fitting procedure to estimate $\tau_{AIPW}$ as follows: 

\begin{enumerate}
    \item We split the dataset $\mathcal{D} = \{X,Z, Y,\tilde{Y}\}$ into 2 batches, each with K folds. Therefore $\mathcal{D}_{tk}$, of size $n_{t,k}$ data points, denotes the batch-$t$ and fold-$k$ data,  $\mathcal{D}_{tk} =  \{X_i,Z_i, Y,\tilde{Y}_i\}_{(t,i) \in \mathcal{I}_k}$.
    \item For all the folds in batch 1, we sample expert annotations uniformly with  $R_{1}\sim Bern(\pi_1)$. Estimate the nuisance models $\hat\mu^{(k)}_z,\hat\sigma^{2(k)}_z,\hat{e}^{(k)}_z$ within each fold.
    \item Using the nuisance models from batch 1, evaluate $\mu^{(-k)}_z,
    \hat\sigma^{2(-k)}_z,
    \hat{e}^{(-k)}_z%
    $ on batch 2 data for each fold $k$. Obtain optimal $\pi^{*}_{2(k)}(z_{2},X_{2})$ for batch 2 data by minimizing an estimate of the asymptotic variance (or plugging in feasible out-of-fold nuisances to our closed-form expression). That is 
    \begin{align*}
    \min_{\pi^{(-k)}(z_2,x_2)}
    \sum_{z\in\{0,1\}} &
    \sum_{k=1}^K \sum_{(2,i)\in\mathcal{I}_k}
    { \frac{\hat{\sigma}^{2(-k)}_z(X_i)}{\hat{e}^{(-k)}_z(X_i)  \pi^{(-k)}(z,X_i)}  }
    \end{align*} This gives the optimal annotation probability, which on the entire dataset will be analyzed as a mixture across batches $\pi^{*(-k)}(z_2,X_2) = \kappa\pi_1 + (1-\kappa)\hat{\pi}_2$, where $\kappa = \frac{n_{1}}{n_{1} + n_{2}}$. We then solve for $\hat{\pi}^{(-k)}_{2} = \frac{1}{1-\kappa}(\pi^{*(-k)}(z_2,X_2) - \kappa\pi^{(-k)}_1)$
    
    \item On batch 2, for each fold, we assign $R_{2}=1$ with sampling probabilities $\hat{\pi}^{(-k)}_2(z_i,X_i)$. We collect data from both stages $(X,Z,R,Y)$ to construct the feasible estimator,  $\hat\tau_{AIPW}.$
    \begin{align}
    \hat{\tau}_{AIPW} &= \frac{1}{n}\sum_{t=1}^2 \sum_{k=1}^K\sum_{(t,i)\in\mathcal{I}_k} \hat{\psi}_{1,i} - \hat{\psi}_{0,i}, \nonumber \\
    &\text{ where } \hat{\psi}_{z,i}=\frac{\mathbb{I}[Z_i=z]R_i(Y_i-\hat{\mu}^{(-k)}_z(1,X_i,\tilde{Y}_i))}{\hat{e}^{(-k)}_z(X_i)  \hat{\pi}^{(-k)}(z,X_i)} 
    +\hat{\mu}^{(-k)}_z(1,X_i,\tilde{Y}_i).\label{eq:AIPW-estimator}
    \end{align}

\end{enumerate}



\begin{assumption}[Consistent estimation and boundedness]
Assume the following conditions on bounded outcomes, moments, and consistent estimation: 
\label{asn-consistent-bounded}
    \begin{enumerate}
    \item $(\E[Y(z)^2])^{1/2} \leq C_1,(\E[\mu_z(1,X)^2])^{1/2} \leq C_2$ 
    \item $\E[(Y-\mu_z(1,X))^2] \leq 4B_{\sigma^2}$ 
    \item  $\E[(\mu_z(1,X) - \hat\mu_z(1,X))^2]\leq K_\mu n^{-r_\mu}$ 
\end{enumerate}
for some constants $C_1,C_2,B_{\sigma^2}, K_\mu, r_\mu \geq 0$. 

\end{assumption}

\begin{assumption}[Product error rates  \citep{bia2021dmlsampleselection}]
    
Denote  $\norm{\cdot}_2 = (\E[(\cdot)^2])^{1/2}$.
\begin{align*}
    &\norm{\hat{\mu}_z(1,X) - \mu_z(1,X)}_2 \times \norm{\hat{\pi}(z,X) - \pi(z,X)}_2 \leq \delta_n n^{-1/2}\\
    &\norm{\hat{\mu}_z(1,X) - \mu_z(1,X)}_2 \times \norm{\hat{e}_z(X) - e_z(X)}_2 \leq \delta_n n^{-1/2}
\end{align*}

where $\delta_n$ is sufficiently large.
\label{asn-product-error-rates}
\end{assumption}

\begin{assumption}[VC dimension for nuisance estimation]
The nuisance estimation of $\mu$, $e$, and $\sigma^2$ occurs over function classes with finite VC-dimension. 
\end{assumption}

\begin{assumption}[Sufficiently weak dependence across batches]
    $$\sqrt{\frac{1}{n_{t, k}} \sum_{i:(t, i) \in \mathcal{I}_k}\left\|\mathbb{E}\left[\hat\psi_i(R;  \hat{e}^{(-k)},\hat{\pi}^{(-k)}, \hat\mu^{(-k)})-\psi_i(R;  {e}^{(-k)},{\pi}^{(-k)}, \mu^{(-k)}) \mid \mathcal{I}^{(-k)}, X_{i}\right]\right\|^2} =o_p(n^{-\frac 14})$$
\end{assumption}

\begin{theorem}[]\label{thm-asymptotic-convergence} Given \Cref{asn-consistency,asn-r-ignorability,asn-tx-ignorability,asn-positivity},
suppose that we construct the feasible estimator $\hat{\tau}_{AIPW}$ (\Cref{eq:AIPW-estimator}) using the CSBAE crossfitting procedure in \Cref{defn-csbae} with estimators satisfying \Cref{asn-consistent-bounded,asn-product-error-rates} (consistency and product error rates). 




We then have 
\[ \sqrt{N}(\hat{\tau}_{AIPW} - \tau)\Rightarrow \mathcal{N}(0,V_{AIPW}),\]

where $V_{AIPW}=\sum_{z\in{0,1}}\E\left[\frac{\sigma^2_z(X)}{e_z(X)\pi^*(z,X)}\right] + \mathrm{Var}\left[\mu_1(1,X,\tilde{Y})-\mu_{0}(1,X,\tilde{Y})\right].$ Here $\tau$ is the ATE.%
\end{theorem}

For the proof, see Appendix B. The main result from \Cref{thm-asymptotic-convergence} shows that the batch adaptive design and feasible estimator has an asymptotic variance equal to the variance of the true ATE under missing outcomes and the optimal $\pi^*$. This implies that our procedure successfully minimizes the asymptotic variance bound. With this, we can also quantify the uncertainty of our treatment effect estimates by producing level-$\alpha$ confidence intervals for $\tau$ that achieve coverage with $1-\alpha$ probability.  


\section{Experiments}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{mse_and_widths_simtrials100v3.pdf}
    \caption{\textbf{Left:} Mean squared error between estimated ATE and true ATE averaged over 100 trials across varying budgets. \textbf{Right:} Average confidence interval width averaged over 100 trials across varying budgets.}
    \label{fig:mse_simulation_lineplot}
\end{figure*} 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{estimates_widths_simtrials100.pdf}
    \caption{\textbf{Left:} Boxplots of ATE estimates compared to skyline $\hat{\tau}_{AIPW}$ when the labeling budget is the entire dataset in blue and the grey dotted line is $\tau$. \textbf{Right:} Average confidence interval width averaged over 100 trials across varying budgets.}
    \label{fig:estimates_simulation_boxplot}
\end{figure*}

We evaluate our batch adaptive allocation protocol on synthetic and real-world datasets and compare it to a baseline and a skyline, a method with additional information than ours that nonetheless informs of best (infeasible) performance. The baseline does not use our adaptively learned $\hat{\pi}(z,X)$, but instead uses uniform random sampling at different budget values. The skyline that we compare against is the standard AIPW estimator with fully observed outcomes, that is when the budget equals 1 or $R=1$ for all data points. We compute standard errors for each method. We report the distribution of ATE estimates and the average interval width (on the log scale) for varying budgets, averaged over 100 trials (see Appendix). We report the empirical MSEs for the ATE $\tau$; that is, $\frac{1}{100}\sum_{i=1}^{100} (\tau - \hat{\tau}_{AIPW}^{(i)})^2$, where $\hat{\tau}_{AIPW}^{(i)}$ denotes the estimator of the ATE in the $i$-th trial.  

\subsection{Synthetic Data}
\paragraph{Basic simulation setup.} We generate a dataset $\mathcal{D} = \{X
,Z,Y,Y(1),Y(0) \}$, of size $1000$ and where the true ATE $\tau = \E[Y(1)] - \E[Y(0)] = 3$. We sample each covariate $X\in\mathbb{R}^5$ from a standard normal distribution, 
$X \sim \mathcal{N}(0,I_5)$. 
Treatment $Z$ is drawn with logistic probability $\gamma_z(X) = (1 + e^{X_{2} + X_{3}+0.5})$. We define $\sigma_{z}^2(X)$ as follows: 
\begin{align*} \sigma_{1}^2(X)&:= 5 + 1.2\mathrm{sin}(2X_{1}) + (X_1 + X_1^2)/2 \\
 \sigma_{0}^2(X)&:= 2 + 0.8\mathrm{cos}(X_{1}/2) + X_1^2/5. 
 \end{align*}
Finally, the outcome models are defined as:
\begin{align*} Y(0) &= 5 + X_1 - 2X_2 + \epsilon_0\\
Y(1) &= Y(0) + \theta_0 + \epsilon_1,\end{align*}
where $\epsilon_0\sim\mathcal{N}(0,\sigma_0(X))$ and $\epsilon_1 \sim \mathcal{N}(0,\sigma_1(X))$. The observed outcomes are $Y = Z\cdot Y(1) + (1-Z)\cdot Y(0)$.

\paragraph{Results.}  We see the greatest advantage with our adaptive estimation for budgets between 0.2 and 0.5. While for larger budgets, even as the MSE for both estimators converge, the interval width for the adaptive estimator is still relatively small. Adaptive annotation with a larger budget introduces additional variation in inverse annotation probabilities, as compared to uniform sampling, which is equivalent to full-information estimation at a marginally smaller budget. This regime of improvement for small budgets is nonetheless practically relevant and consistent with other works.  








\clearpage


\bibliographystyle{plainnat}
\bibliography{for-aaai-aigov-workshop/aaai25,activeannotation}


\newpage
\appendix
\onecolumn

\section{Notation}
\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{ll}
        \hline
        \( Y_i \) & Ground truth outcomes, observed when label is provided by experts \\ 
        \( \tilde{Y}_i \) & Complex embedded outcomes, such as raw text \\ 
        
        \( X_i \) & Covariates included in estimation \\ 
        \( Z_i\) & Treatment assignment indicator \\ 
        \( R_i \) & Missingness indicator, indicates whether $i$ is expertly labeled\\ 
        \( e_z(X_i) \) & Propensity score, probability of being assigned treatment $Z=z$\\
        \( \pi(Z_i,X_i)\) & Annotation probability, probability of sampling unit $i$ for expert annotation \\
        \( f(\tilde{Y}_i) \) & Estimated function of complex embedded outcomes, e.g. zero-shot LLM prediction from raw text \\  
        \(  \mu_z(1,X_i,f(\tilde{Y}_i)) \) & Estimated model predicting $Y$ as function of $f(\tilde{Y})$ alone or $(X,f(\tilde{Y}))$\\ 
        \hline
    \end{tabular}}
    \caption{Summary of Notation}
    \label{tab:notation}
\end{table}


\section{Proofs}


\begin{proof}[Proof of \Cref{prop-avar-ate}]
We simplify the expression for the asymptotic variance of the ATE with missing outcomes to isolate the components affected by the data annotation probability. 

First the variance of the ATE defined in terms of the efficient influence function $\psi_z$ is
\begin{align*}
    \mathrm{Var}[\psi_z - \psi_{z'} ] &= \mathrm{Var}\bigg[\frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)} + \mu(z,1,X) - \frac{Z' \cdot R\cdot [Y-\mu(z',1,X)]}{e_{z'}(X) \cdot \pi(z',X)} + \mu(z',1,X) \bigg] \\
    &= \underbrace{\mathrm{Var}\bigg[\frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)}  + \mu(z,1,X) \bigg]}_{V_1} + \underbrace{\mathrm{Var}\bigg[\frac{Z' \cdot R\cdot [Y-\mu(z',1,X)]}{e_{z'}(X) \cdot \pi(z',X)} + \mu(z',1,X) \bigg]}_{V_2} \\ &-\underbrace{2\mathrm{Cov}\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)} + \mu(z,1,X),\frac{Z' \cdot R\cdot [Y-\mu(z',1,X)]}{e_{z'}(X) \cdot \pi(z',X)} + \mu(z',1,X) \bigg]}_{V_3} 
\end{align*}
\textbf{For $V_3$}: 
\begin{align*}
    &2\mathrm{Cov}\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)} + \mu(z,1,X),\frac{Z' \cdot R\cdot [Y-\mu(z',1,X)]}{e_{z'}(X) \cdot \pi(z',X)} + \mu(z',1,X) \bigg]\\
    = &2 \Bigg[ \Eb{\frac{Z \cdot R}{e_z(X) \cdot \pi(z,X)} [\underbrace{\Eb{Y|Z,R=1,X}-\mu(z,1,X)}_{=0}]}\Bigg] \\
    + &\Bigg[\Eb{\mu(z,1,X) \cdot \frac{Z' \cdot R}{e_{z'}(X) \cdot \pi(z',X)} [\underbrace{\Eb{Y|Z',R=1,X}-\mu(z',1,X)}_{=0}] + \mu(z',1,X)}\Bigg] \\ 
    &- \Eb{\frac{Z \cdot R}{e_z(X) \cdot \pi(z,X)} [\underbrace{\Eb{Y|Z,R=1,X}-\mu(z,1,X)}_{=0}] + \mu(z,1,X)} \\
    &\times \Eb{\frac{Z' \cdot R}{e_{z'}(X) \cdot \pi(z',X)} [\underbrace{\Eb{Y|Z',R=1,X}-\mu(z',1,X)}_{=0}] + \mu(z',1,X)} \Bigg] \\
    = &2\bigg[ \Eb{\mu(z,1,x)\cdot \mu(z',1,x)} - \Eb{\mu(z,1,x)\mu(z',1,x)}\bigg]  
\end{align*}

\textbf{For $V_1$:}
\begin{align*}
    &\mathrm{Var}\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)} + \mu(z,1,x)\bigg] \\
    &= \mathrm{Var}\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)}\bigg]  + \mathrm{Var}[\mu(z,1,x)] + 2\underbrace{\mathrm{Cov}\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)},\mu(z,1,x)\bigg]}_{=0} \\
    &= \Eb{\bigg[ \frac{Z \cdot R\cdot [Y-\mu(z,1,X)]}{e_z(X) \cdot \pi(z,X)}\bigg]^2} - \bigg[ \frac{Z \cdot R\cdot }{e_z(X) \cdot \pi(z,X)} [\underbrace{\Eb{Y|Z,R=1,X}-\mu(z,1,X)}_{=0}]\bigg]^2 \\
    &+ \Eb{\mu(z,1,X)^2} -\Eb{\mu(z,1,X)}^2\\ 
    &= \Eb{\bigg[ \frac{Z^2 \cdot R^2}{e^2_z(X) \cdot \pi^2(z,X)} \cdot [Y-\mu(z,1,X)]^2\bigg]} + \Eb{\mu(z,1,X)^2} -\Eb{\mu(z,1,X)}^2 \\
    &= \Eb{\frac{Z\cdot R}{e^2_z(X) \cdot \pi^2(z,X)} \cdot [Y-\mu(z,1,X)]^2} + \Eb{\mu(z,1,X)^2} -\Eb{\mu(z,1,X)}^2\\
    &=\Eb{ \frac{1}{e_z(X) \cdot \pi(z,X)} \cdot [Y-\mu(z,1,X)]^2} + \Eb{\mu(z,1,X)^2} -\Eb{\mu(z,1,X)}^2
\end{align*}

Lastly, $V_1 = V_2$. So the full variance term is
\begin{align*}
    \mathrm{Var}[\psi_z - \psi_{z'}] &= \Eb{ \frac{1}{e_z(X) \cdot \pi(z,X)} \cdot [Y-\mu(z,1,X)]^2} + \Eb{ \frac{1}{e_{z'}(X) \cdot \pi(z',X)} \cdot [Y-\mu(z',1,X)]^2}\\
    &+ \Eb{(\mu(z,1,X) - \mu(z',1,X))^2} - \Eb{\mu(z,1,X) -\mu(z',1,X)}^2 \\
    &= \Eb{ \frac{1}{e_z(X) \cdot \pi(z,X)} \cdot [Y-\mu(z,1,X)]^2} + \Eb{ \frac{1}{e_{z'}(X) \cdot \pi(z',X)} \cdot [Y-\mu(z',1,X)]^2} \\ 
    &+ \mathrm{Var}\bigg[\mu(z,1,X) - \mu(z',1,X)\bigg]
\end{align*} 

Rewriting the bound from Hahn (1998), we get 

\begin{align*}
    V &\geq \Eb{ \frac{1}{e_z(X) \cdot \pi(z,X)} \cdot [Y-\mu(z,1,X)]^2} + \Eb{ \frac{1}{e_{z'}(X) \cdot \pi(z',X)} \cdot [Y-\mu(z',1,X)]^2} \\
    &+ \mathrm{Var}\bigg[\mu(z,1,X) - \mu(z',1,X)\bigg] 
\end{align*}

\end{proof}

\begin{proof}[Proof of \Cref{thm-z-budget-solution}]

Finding the optimal $\pi$ can be separated into sub-problems for each treatment $z \in \{0,1 \}$, since the objective and dual variables are separable across $z$. We first look at a solution for $\pi(z,X)$ for a given $z$: 
\begin{align}
  \min_{\pi(z,x)}  &
  \Eb{ \frac{\sigma^2_z(X)}{e_z(X)  \pi(z,X)}  }  \tag{z-budget} \\
   \text{ s.t. } & \Eb{\pi(z,X) \mid Z=z} \leq B_z, \nonumber\\
    &0 < \pi(z,x) \leq 1, \; \forall x \nonumber
\end{align}
    
We define the Lagrangian of the optimization problem and introduce dual variables $\lambda$ for the budget constraint and $\eta$ and $\nu$ for the the constraint that $0 < \pi(z,X) \leq 1$:  
$$ 
\mathcal{L} = \Eb{ \frac{ (Y- \mu_z(X))^2 }{e_z(X)  \pi(z,X)} }  
+ \lambda_z(\Eb{\pi(z,X) \mid Z=z } - B_z) + 
\sum_{x\in\mathcal{X}} (\nu_x^z (\pi(z,x)-1) - \eta_x^z \pi(z,x))
$$

Define the conditional outcome variance $\sigma^2(X) = \Eb{(Y -\mu(z,1,X))^2|X}$. Note that by iterated expectations, 
$$ 
\mathcal{L} = \Eb{ \frac{\sigma^2_z(X)}{e_z(X)  \pi(z,X)} }  
+ \lambda_z(\Eb{\pi(z,X)  \mid Z=z } - B_z) + 
\sum_{x\in\mathcal{X}} (\nu_x^z (\pi(z,x)-1) - \eta_x^z \pi(z,x))
$$








We can find the optimal solution by setting the derivative equal to 0. Since $p(X=x\mid Z=z) = \frac{e_z(x) p(x)}{p(Z=z)}$
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \pi(z,X)} &= 
 -\frac{{\sigma^2(X)}}{e_z(X)(\pi^2(z,X))}p(x)  + \lambda_z \frac{e_z(x) p(x)}{p(Z=z)}
 + \nu_x - \eta_x = 0, \text{ where } p(x)>0
\\ &= -\frac{{\sigma^2(X)}}{e_z^2(X)\pi^2(z,X)} + \frac{\lambda_z}{p(Z=z)} + 
 \frac{ (\nu_x^z - \eta_x^z) }{p(x)e_z(x) }= 0
 \\
\end{align*}
Therefore
$$
\pi(z,x) = \sqrt{\frac{ \sigma^2(x) }{e_z^2(x)(
\frac{\lambda_z}{p(Z=z)} +  \frac{ (\nu_x^z - \eta_x^z) }{p(x)e_z(x) })}}$$
Next we give a choice of $\lambda$ that results in an interior solution with $0 \leq \pi(z,x)\leq 1$, so that $\nu^z_x, \eta_x^z$ can be set to $0$ without loss of generality to satisfy complementary slackness. 














We posit a closed form solution
$$
\pi^*(z,X) = \frac{\sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}}}{\Eb{ \sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}}
\mid Z=z
} } \cdot B_z 
$$. 

Note that this solution is self-normalized to satisfy the budget constraint such that 

$$
\Eb{\pi^*(z,X)  \mathbb{I}[Z=z ]} = \Eb{\frac{\sqrt{\nicefrac{\sigma^2(X)}{  e_z^2(X)}}}{\Eb{ \sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}} \mid Z=z }  }  B_z \mid Z=z } = B_z
$$

This solution corresponds to a choice of $\lambda^*_z = \nicefrac{p(Z=z)\Eb{ \sqrt{\mathbb{I}[Z=z ]\nicefrac{\sigma^2(X)}{  e_z^2(X)}}}^2 }{B_z^2}$ in the prior parametrized expression. 

\begin{align*}
    \pi_{\lambda}(z,X) &= \pi^*(z,X) \\
    \sqrt{\frac{{\sigma^2_z(X)}}{ e_z^2(X) \frac{\lambda}{p(Z=z)}}} &= \frac{\sqrt{\nicefrac{\sigma^2_z(X)}{e_z^2(X)}}}{\Eb{ \sqrt{\nicefrac{\sigma^2_z(X)}{  e_z^2(X)}} \mid Z=z }}\cdot B_z 
\end{align*} 

We can check that the KKT conditions are satisfied at  $\pi^*(z,X)$ and $\lambda^*$. We note that since $\pi^*(z,X)$ is an interior solution then w.l.o.g we can fix $\nu_x, \eta_x= 0$ to satisfy complementary slackness. 

It remains to check that $\frac{\partial \mathcal{L} }{\partial \pi^*(z,X)} = 0$, we have that:   

$$
\frac{\partial\mathcal{L}}{\partial \pi(z,X)} = 
-\frac{{\sigma^2_z(X)}}{e_z(X)} \cdot  
\frac{e_z^2(X)\Eb{ \sqrt{\nicefrac{\sigma^2_z(X)}{  e_z(X)}} \mid Z=z
}^2
}{ 
\sigma^2_z(X) \cdot B_z^2
}
+ \frac{\Eb{\sqrt{\nicefrac{\sigma^2(X)}{e_z(X)}} \mid Z=z 
}^2 \sigma^2_z(X) e_z(X)}{\sigma^2_z(X)\cdot B_z^2} + 0 = 0.
$$

Thus we have shown that $\pi^*(z,X)$ is optimal. 

\end{proof}

\begin{proof}[Proof of \Cref{thm-global-budget-solution}]
Proceed as in the proof of \Cref{thm-z-budget-solution}. 

The Lagrangian of the optimization problem (with a single global budget constraint) is: 

\begin{align*}
\mathcal{L} &= \sum_{z\in\{0,1\}} \resizebox{\textwidth}{!}{%
$
\left\{ \Eb{ \frac{ (Y- \mu_z(X))^2 }{e_z(X)  \pi(z,X)} } + 
\sum_{x\in\mathcal{X}} (\nu_x^z (\pi(z,x)-1) - \eta_x^z \pi(z,x))
+ \lambda(\Eb{\pi(1,X) \mathbb{I}[Z=1]+\pi(0,X) \mathbb{I}[Z=0] } - B)\right\} $}
\end{align*}

Again by iterated expectations, 
$$ 
\mathcal{L} = \Eb{ \frac{\sigma^2_z(X)}{e_z(X)  \pi(z,X)} }  
+ \lambda(\Eb{\pi(1,X) e_1(X) + \pi(0,X) e_0(X) } - B_z) + 
\sum_{x\in\mathcal{X}} (\nu_x^z (\pi(z,x)-1) - \eta_x^z \pi(z,x))
$$
We can find the optimal solution by setting the derivative equal to 0. 
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \pi(z,X)} &= 
 -\frac{{\sigma^2(X)}}{e_z(X)(\pi^2(z,X))}p(x)  + \lambda p(x) e_z(x)  + \nu_x^z - \eta_x^z = 0, \text{ where } p(x)>0
\\ &= -\frac{{\sigma^2(X)}}{e_z^2(X)\pi^2(z,X)} + \lambda + 
 \frac{ (\nu_x^z - \eta_x^z) }{p(x)e_z(x) }= 0
\end{align*}
Therefore we obtain a similar expression parametrized in $\lambda$, but this parameter is the same across both groups under a global budget. 
$$
\pi(z,x) = \sqrt{\frac{ \sigma^2(x) }{e_z^2(x)(\lambda +  \frac{ (\nu_x^z - \eta_x^z) }{p(x)e_z(x) })}}$$

We can similarly give a closed-form expression for a different choice of $\lambda$ yielding an interior solution, so that we can set $\nu_x^z,\eta_x^z=0$ without loss of generality. 
$$
\lambda =  \frac{\Eb{ 
\mathbb{I}[Z=1]\sqrt{\nicefrac{\sigma^2_1(X)}{  e_1^2(X)}} 
+ \mathbb{I}[Z=0]\sqrt{\nicefrac{\sigma^2_0(X)}{  e_0^2(X)}}
}^2 
}{B^2}
$$
Notice that this satisfies the normalization requirement that $\E[\pi^\lambda(1,X)\mathbb{I}[Z=1] +\pi^\lambda(0,X)\mathbb{I}[Z=0] ]\leq B,$ and similarly note that the partial derivatives with respect to $\pi(z,x)$ are $0$. 
\end{proof}

\begin{proof}[Proof of \Cref{thm-asymptotic-convergence}]

\textbf{Proof sketch.} 

The proof proceeds in two steps. The first establishes that the feasible AIPW estimator converges to the AIPW estimator with oracle nuisances. It follows from standard analysis with cross-fitting, in particular the variant used across batches. 

Note the AIPW estimator can be rewritten as a sum over estimators within batch-$t$, fold-$k$, $\hat{\tau}^{(t,k)}_{AIPW}$, as follows: 
$$  \hat{\tau}_{AIPW} = 
    \sum_{t =1}^2
\sum_{k=1}^K
\frac{ n_{t,k} }{n}\sum_{(t,i)\in\mathcal{I}_k} \frac{1}{n_{t,k} }
\{
\hat {\psi}_{1,i}(e, \pi, \mu) - \hat {\psi}_{0,i}(e, \pi, \mu) \} 
= \sum_{t =1}^2
\sum_{k=1}^K \frac{ n_{t,k} }{n}
  \hat{\tau}^{(t,k)}_{AIPW}
  $$
Define the AIPW estimator with oracle nuisances as 
    \begin{align*}
 \hat{\tau}^*_{AIPW} &= 
  \frac 1n 
\sum_{i=1}^n
\{
{\psi}_{1,i}(e, \pi, \mu) - {\psi}_{0,i}(e, \pi, \mu) \} 
= \sum_{t =1}^2
\sum_{k=1}^K
  \frac{ n_{t,k} }{n}\hat{\tau}^{*,(t,k)}_{AIPW},\\
&\text{ where } {\psi}_{z,i}(e,\pi,\mu)=\frac{\mathbb{I}[Z_i=z]R_i(Y_i-{\mu}_z(1,X_i,\tilde{Y}_i))}{{e}_z(X_i)  {\pi}(z,X_i)} 
    +{\mu}_z(1,X_i,\tilde{Y}_i).
    \end{align*}


We will show that the feasible estimator converges to the oracle estimator within a batch$-t$, fold$-k$ subset; the decomposition above gives that this also holds for the original estimators. 
$$ \sqrt{n} (\hat{\tau}^{(t,k)}_{AIPW}
- \hat{\tau}^{*,(t,k)}_{AIPW})
\to_p 0$$

The second step studies the limiting mixture distribution propensity arising from the two-batch process and shows that the use of the double-machine learning estimator (AIPW), under the weaker product error assumptions, gives that the oracle estimator is asymptotically equivalent to the oracle estimator where missingness follows the limiting mixture missingness probability. The latter of these is a sample average of iid terms and follows a standard central limit theorem. Defining a random variable $\tilde{R}_i = \mathbb{I}[U_i \geq \pi^*(X_i)],$ we wish to show that 
$$ \sum_z \E_n [ {\psi}_{z,i}(R, \hat e, \hat \pi, \hat \mu) ] - \E_n [ {\psi}_{z,i}(\tilde{R},  e,  \pi,  \mu) ] = o_p(n^{-\frac 12}).$$


\textbf{Step 1 (feasible estimator converges to oracle)}







If we look at one term for one treatment and datapoint in the above (the rest follows for the others), we obtain the following decomposition into error and product-error terms: 

\begin{align*}
&  \frac{Z_i R_i  (Y_i-\hat\mu_1(1,X_i))}{\hat e_1(X_i)  \hat\pi(1,X_i)} -  \frac{Z_i R_i  (Y_i-\mu_1(1,X_i))}{ e_1(X_i)  \pi(1,X_i)}
 +
 (\hat\mu_1(1,X_i)-\mu_1(1,X_i))  \\
&=   (\mu_1(1,X_i)-\hat\mu_1(1,X_i))
\left( \frac{Z_i R_i  }{ e_1(X_i)  \pi(1,X_i)} - 1 \right) 
    + Z_i R_i  (Y_i- \hat \mu_1(1,X_i))(\frac{1}{\hat e_1(X_i)  \hat\pi(1,X_i)} 
    - \frac{1}{ e_1(X_i)  \pi(1,X_i)})\tag{by  $\pm  \frac{Z_i R_i  (Y_i-\hat \mu_1(1,X_i))}{ e_1(X_i)  \pi(1,X_i)} $}
    \\
    &=    (\mu_1(1,X_i)-\hat\mu_1(1,X_i))
\left( \frac{Z_i R_i  }{ e_1(X_i)  \pi(1,X_i)} - 1 \right) 
    + Z_i R_i  (Y_i- \mu_1(1,X_i))(\frac{1}{\hat e_1(X_i)  \hat\pi(1,X_i)} 
    - \frac{1}{ e_1(X_i)  \pi(1,X_i)})\\
   & \qquad 
     + Z_i R_i  (\mu_1(1,X_i)-\hat\mu_1(1,X_i))(\frac{1}{\hat e_1(X_i)  \hat\pi(1,X_i)} 
    - \frac{1}{ e_1(X_i)  \pi(1,X_i)}) \tag{by $\pm Z_i R_i  \mu_1(1,X_i)(\frac{1}{\hat e_1(X_i)  \hat\pi(1,X_i)} 
    - \frac{1}{ e_1(X_i)  \pi(1,X_i)})$}\\
    &     =    (\mu_1(1,X_i)-\hat\mu_1(1,X_i))
\left( \frac{Z_i R_i  }{ e_1(X_i)  \pi(1,X_i)} - 1 \right) \\
& \qquad 
    + Z_i R_i  (Y_i- \mu_1(1,X_i))
    \left({\hat\pi(1,X_i)^{-1}}(
{\hat e_1(X_i)}^{-1} -   { e_1(X_i)^{-1}}) 
    + { e_1(X_i)}^{-1}({ \hat  \pi(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right) \\
   & \qquad 
     + Z_i R_i  (\mu_1(1,X_i)-\hat\mu_1(1,X_i))
  \left({\hat\pi(1,X_i)^{-1}}(
{\hat e_1(X_i)}^{-1} -   { e_1(X_i)^{-1}}) 
    + { e_1(X_i)}^{-1}({ \hat  \pi(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right)
    \tag{by $\pm \frac{1}{ e \hat\pi }$}
\end{align*}

We want to show that 
$$ \sqrt{n_{t,k}} (\hat{\tau}^{(t,k)}_{AIPW}
- \hat{\tau}^{*,(t,k)}_{AIPW})
\to_p 0$$


Now that we have written out this expansion for one datapoints, we can write out this expansion within a batch-$t$, fold-$k$ subset, and write out the cross-fitting terms for reference: 
\resizebox{\textwidth}{!}{%
$
\begin{aligned}
&\sqrt{n_{t,k}}\left(\hat{\tau}_{A I P W}^{(t, k)}-\hat{\tau}_{A I P W}^{*,(t, k)}\right) \\
&=\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k}   (\mu_1(1,X_i)-\hat\mu_1^{(-k)}(1,X_i))
\left( \frac{Z_i R_i  }{ e_1(X_i)  \pi(1,X_i)} - 1 \right) \\
&\quad +
\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k}  Z_i R_i  (Y_i- \mu_1(1,X_i))
    \left({\hat\pi^{(-k)}(1,X_i)^{-1}}(
{\hat e_1^{(-k)}(X_i)}^{-1} -   { e_1(X_i)^{-1}}) 
    + { e_1(X_i)}^{-1}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right) \\
 &\quad + 
 \frac{1}{\sqrt{n_{t,k}}}\sum_{i:(t,i)\in\mathcal{I}_k} Z_i R_i  (\mu_1(1,X_i)-\hat\mu_1^{(-k)}(1,X_i))
  \left({\hat\pi^{(-k)}(1,X_i)^{-1}}(
{\hat e_1^{(-k)}(X_i)}^{-1} -   { e_1(X_i)^{-1}}) 
    + { e_1(X_i)}^{-1}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right)
\end{aligned}
$
}


\textbf{Bound for third term}: 

\resizebox{\textwidth}{!}{%
$
\begin{aligned}
     &\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} Z_i R_i  (\mu_1(1,X_i)-\hat\mu_1^{(-k)}(1,X_i))({\hat\pi^{(-k)}(1,X_i)^{-1}}(
\hat{e}^{(-k)}_1(X_i)^{-1} - e_1(X_i)^{-1}) 
    + { e_1(X_i)}^{-1}(\hat{\pi}^{(-k)}(1,X_i)^{-1}-\pi(1,X_i)^{-1}) \\
 &= \frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} Z_i R_i \hat\pi^{(-k)}(1,X_i)^{-1}  (\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))(
{\hat e_1^{(-k)}(X_i)}^{-1} -   { e_1(X_i)^{-1}}) \\
    &+ Z_i R_i e_1(X_i)^{-1}(\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))( \hat{\pi}^{(-k)}(1,X_i)^{-1}-
 \pi(1,X_i)^{-1}) \\
 &\leq (\lambda_{\pi} + \nu_e)\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k}  (\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))(
{\hat e_1^{(-k)}(X_i)}^{-1} -   { e_1(X_i)^{-1}}) + (\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))( \hat{\pi}^{(-k)}(1,X_i)^{-1}-
 \pi(1,X_i)^{-1}) \\
 &\leq (\lambda_{\pi} + \nu_e)\delta_n n^{-1/2} 
\end{aligned}
$
}

where the last inequality makes use of product error rate assumptions 5-6 and nuisance function convergence rates from \Cref{lemma-convergence-nuissance}. %
Thus, we find that this term is $o_p(1/\sqrt{n})$

\textbf{Bound for the first term}: 

The key to bounding the first term is that cross-fitting allows us to treat this term as the average of independent mean-zero random variables. We will bound it with Chebyshev's inequality, which requires a bound on the second moment.

\begin{align*}
&\E\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} \bigg((\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i)) \bigg(\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)} -1\bigg)\bigg)^2\mid \mathcal{I}_{(-k)},\{X_i\}\bigg] \\
& \qquad = \mathrm{Var}\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} (\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i)) \bigg(\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)} -1\bigg)\mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \\
& \qquad = \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \E\bigg[(\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))^2 \bigg(\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)} -1\bigg)^2\mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \tag{expectation of $(\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)}-1)^2$} \\
& \qquad = \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \frac{1-e_1(X_i)\pi(z,X_i)}{e_1(X_i)\pi(1,X_i)}(\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))^2 \\
&\qquad \leq \frac{1-\nu_e\lambda_\pi}{\nu_e\lambda_\pi} \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k}  ((\mu_1(1,X_i)-\hat\mu^{(-k)}_1(1,X_i))^2 = o_p(\frac{1}{n^{1+2r_\mu}}) 
\end{align*}

where for the third equality, we use the fact that 

\resizebox{\textwidth}{!}{%
$
\begin{aligned}
\E[(\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)}-1)^2\mid \mathcal{I}_{(-k)},\{X_i\}] = \E[(\frac{Z^2_i R^2_i}{e^2_1(X_i)\pi^2(1,X_i)} - 2\frac{Z_i R_i}{e_1(X_i)\pi(1,X_i)} +1\mid \mathcal{I}_{(-k)},\{X_i\}] = \frac{1}{e_1(X_i)\pi(1,X_i)} - 1
\end{aligned}
$
}

Since $r_\mu \geq 0$, we can conclude by Chebyshev's inequality that the first term is $o_p(n^{-1/2})$. 


\textbf{Bound for the second term}: 
We bound the second term following a similar argument as above.

\begin{align*}
&\E\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} \bigg(Z_i R_i  (Y_i-\mu_1(1,X_i))
  \left({\hat\pi^{(-k)}(1,X_i)^{-1}}(
{\hat e_1^{(-k)}(X_i)}^{-1} - { e_1(X_i)^{-1}})\right)^2\mid \mathcal{I}_{(-k)},\{X_i\}\bigg] \\
    &+ \E\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} \bigg(Z_i R_i  (Y_i-\mu_1(1,X_i))\left({ e_1(X_i)}^{-1}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right)\bigg)^2\mid \mathcal{I}_{(-k)},\{X_i\}\bigg] \\
& \qquad = \mathrm{Var}\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} \bigg(Z_i R_i  (Y_i-\mu_1(1,X_i))
  \left({\hat\pi^{(-k)}(1,X_i)^{-1}}(
{\hat e_1^{(-k)}(X_i)}^{-1} - { e_1(X_i)^{-1}})\right)\mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \\
&+ \mathrm{Var}\bigg[\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in\mathcal{I}_k} \bigg(Z_i R_i  (Y_i-\mu_1(1,X_i))\left({ e_1(X_i)}^{-1}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right)\mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \\
& \qquad = \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \E\bigg[\left({\hat\pi^{(-k)}(1,X_i)^{-1}}(
{\hat e_1^{(-k)}(X_i)}^{-1} - { e_1(X_i)^{-1}})\right)^2 \frac{Z^2_i R^2_i}{(\hat{\pi}^{(-k)}(1,X_i))^2}  (Y_i-\mu_1(1,X_i))^2 \mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \\
&+ \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \E\bigg[\left({ e_1(X_i)}^{-1}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})
    \right)^2 \frac{Z^2_i R^2_i}{(\hat{\pi}^{(-k)}(1,X_i))^2}  (Y_i-\mu_1(1,X_i))^2 \mid \mathcal{I}_{(-k)},\{X_i\} \bigg] \\
& \qquad = \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \frac{e^2_1(X_i)\pi^2(z,X_i)}{(\hat{\pi}^{(-k)}(1,X_i))^2}\E[\sigma^2(X_i)\mid \mathcal{I}_{(-k)},\{X_i\}]
{\hat e_1^{(-k)}(X_i)}^{-1} - { e_1(X_i)^{-1}}))^2 \\
 &+\frac{e^2_1(X_i)(\pi^{(-k)}(z,X_i))^2}{e_1(X_i)}\E[\sigma^2(X_i)\mid \mathcal{I}_{(-k)},\{X_i\}]({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})^2  \\
&\qquad \leq \frac{1}{n_{t,k}} \sum_{i:(t,i)\in\mathcal{I}_k} \frac{\nu_e^2\lambda^2_{\pi}}{(\hat{\pi}^{(-k)}(1,X_i))^2}B_{\sigma^2}(
{\hat e_1^{(-k)}(X_i)}^{-1} - { e_1(X_i)^{-1}}))^2 
 +\frac{\nu_e^2\lambda_\pi^2}{\nu_e^2}B_{\sigma^2}({ \hat  \pi^{(-k)}(1,X_i)}^{-1}-
 {   \pi(1,X_i)}^{-1})^2 \\
 &\qquad = o_p(\frac{1}{n^{1+2r_e + 2r_\pi}}) 
\end{align*} 

where the last inequality is because $\sigma^2(X)$ is bounded above, $\sigma^2(X)\leq B_{\sigma^2}$, by \Cref{lemma-convergence-nuissance}. Thus, by similar argument to the first term, since this term is a sum of zero-mean random variables and since $r_\pi,r_e \geq 0$, we can apply Chebyshev's inequality and get that this term is also $o_p(1/\sqrt{n})$. This holds for both treatments. Therefore, 

$$ \sqrt{n_{t,k}} (\hat{\tau}^{(t,k)}_{AIPW}
- \hat{\tau}^{*,(t,k)}_{AIPW})
\to_p 0.$$



\textbf{Step 2 (oracle with $\hat{\pi}$ converges to oracle with $\pi^*$)}


Let $\tilde{R}_i = \mathbb{I}[U_i \geq \pi^*(X_i)].$

Restricting attention to a single treatment value $z\in \{0,1\}$, we want to show that: 
\begin{align*}
& \sum_{t=1}^2 \sum_{k=1}^K \frac{n_{t, k}}{n} \sum_{(t, i) \in \mathcal{I}_k} \frac{1}{n_{t, k}}\left\{\hat{\psi}_{1, i}(\tilde{R}, e, \pi, \mu)-\hat{\psi}_{1, i}(R, e, \pi, \mu)\right\} \\
   & 
  = \sum_{t=1}^2 \sum_{k=1}^K \frac{n_{t, k}}{n} \sum_{(t, i) \in \mathcal{I}_k} \frac{1}{n_{t, k}}
    \left\{\frac{\mathbb{I}[Z_i=z] \tilde{R}_i  (Y_i-\mu_z(1,X_i))}{ e_z(X_i)  \pi(z,X_i)}-\frac{\mathbb{I}[Z_i=z] R_i  (Y_i-\mu_z(1,X_i))}{e_z(X_i)  \pi(z,X_i)} \right\} = o_p(n^{-1/2}).
\end{align*}
Without loss of generality we further consider one summand on batch-$t$, fold-$k$ data, the same argument will apply to the other summands and the final estimator. 

We will show this by Chebyshev's inequality. At a high level, consistency of nuisance estimates implies the second moments are small and overall the error term concentrates quickly. 

First we bound the second moment. Note that by consistency of potential outcomes, for any data point we have that
$$
\frac{\mathbb{I}[Z_i=z] \tilde{R}_i  (Y_i-\mu_z(1,X_i))}{ e_z(X_i)  \pi(z,X_i)}-\frac{\mathbb{I}[Z_i=z] R_i  (Y_i-\mu_z(1,X_i))}{e_z(X_i)  \pi(z,X_i)} 
= \frac{\mathbb{I}[Z_i=z] (\tilde{R}_i-R_i)  (Y_i(z)-\mu_z(1,X_i))}{ e_z(X_i)  \pi(z,X_i)}
$$

For each batch $t=1,\hdots,T$ and fold $k=1,\hdots,K$, according to the CSBAE crossfitting procedure, we observe that conditional on $\mathcal{I}_{(-k)}$ for a given batch and the observed covariates, the summands are independent mean-zero. The final estimator will consist of the sum over batches and folds. We start by looking at the estimator over one batch $t$ and one fold $k$ and the rest follows for the other batches and folds.

\begin{align*}
 &   \E\left[  
  \left(\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in \mathcal{I}_k} \frac{\mathbb{I}[Z_{i}=z] (\tilde{R}_{i}-R_{i})  (Y_{i}(z)-\mu_z(1,X_{i}))}{ e_z(X_{i})  \pi(z,X_{i})} \right)^2\mid \mathcal{I}_{(-k)}, \{X_{i}\} \right] \\
  & = 
  \mathrm{Var} \left[  
  \left(\frac{1}{\sqrt{n_{t,k}}} \sum_{i:(t,i)\in \mathcal{I}_k} \frac{\mathbb{I}[Z_{i}=z] (\tilde{R}_{i}-R_{i})  (Y_{i}(z)-\mu_z(1,X_{i}))}{ e_z(X_{i})  \pi(z,X_{i})} \right)\mid \mathcal{I}_{(-k)}, \{X_{i}\} \right] \\
   & = 
\frac{1}{n_{t,k}}   
\sum_{i:(t,i)\in \mathcal{I}_k} 
 \E \left[  \left(\frac{\mathbb{I}[Z_{i}=z] (\tilde{R}_{i}-R_{i})^2  (Y_{i}(z)-\mu_z(1,X_{i}))^2}{ e_z(X_{i})  \pi(z,X_{i})} \right) 
 \mid \mathcal{I}_{(-k)}, \{X_{i}\} \right] 
 \tag{iid batch data conditional on $\mathcal{I}_{(-k)}$} \\
  & \leq \nu_e \gamma_{\sigma^2} \frac{1}{n_{t,k}}   
\sum_{i\in \mathcal{I}_1}^n 
  \E[\mathbb{I}[Z_{i}=z] ((\tilde{R}_{i}-R_{i})^2) \mid \mathcal{I}_{(-k)}, \{X_{i}\} ]
 \E \left[  \left({ (Y_{i}(z)-\mu_z(1,X_{i}))^2} \right) \right]
 \mid \mathcal{I}_{(-k)}, \{X_{i}\} ]
 \tag{By overlap, independence of $U_i$ from all else, $Y(Z) \perp R \mid X$ by design} \\
 \tag{and pull-out property of conditional expectation }\\
  & \leq \nu_e \gamma_{\sigma^2} B_{\sigma^2}\frac{1}{n_{t,k}}   
\sum_{i:(t,i)\in \mathcal{I}_k} 
  \E[\mathbb{I}[Z_{i}=z] ((\tilde{R}_{i}-R_{i})^2) \mid \mathcal{I}_{(-k)}, \{X_{i}\} ] \\
  & \leq \resizebox{\textwidth}{!}{$ \nu_e \gamma_{\sigma^2} B_{\sigma^2} 8 \frac{1}{n_{t,k}}   
\sum_{i:(t,i)\in \mathcal{I}_k}^n 
  \E[\mathbb{I}[Z_{i}=z] ((\tilde{R}_{i}-\pi^*(z,X_{i}))^2)+(\pi^*(z,X_{i})-\hat\pi(z,X_{i}))^2-(R_{i}-\hat\pi(z,X_{i}))^2) \mid \mathcal{I}_{(-k)}, \{X_{i}\} ]$
  } 
  \tag{ by the elementary bound $(a+b)^2 \leq 2(a^2+b^2)$}
\end{align*}

The first and third terms are the variance of a Bernoulli random variable. Therefore, the first and third terms are bounded by $\frac{\nu_e \gamma_{\sigma^2}B_{\sigma^2}}{n_{t,k}}$ and are hence $o_p(n^{-1}).$

The second term is 
\begin{align*}
   & \frac{1}{n_{t,k}}   
\sum_{i:(t,i)\in \mathcal{I}_k} 
  \E[\mathbb{I}[Z_{i}=z] (\pi^*(z,X_{i})-\hat\pi(z,X_{i}))^2 \mid \mathcal{I}_{(-k)}, \{X_{i}\} ]  = o_p(n^{-(1/2)}) \tag{by \Cref{lemma-convergence-nuissance}}
\end{align*}

The dominant term is $o_p(n^{-1})$.

Putting it all together, we see that the second moment is $o_p(n^{-1})$ as well. Finally, we apply Chebyshev's inequality to obtain that 
$$
\sum_{(t, i) \in \mathcal{I}_k} \frac{1}{n_{t, k}}\left\{\hat{\psi}_{1, i}(\tilde{R}, e, \pi, \mu)-\hat{\psi}_{1, i}(R, e, \pi, \mu)\right\} = o_p(n^{-1/2}).
    $$
    Putting these results from Step 1 and Step 2 together, along with the fact that $\frac{n_{t,k}}{n} \to \frac 1K$, gives the theorem. 



\end{proof}

\section{Additional Lemmas}
\subsection{Results appearing in other works, stated for completeness.}
\begin{lemma}[Conditional convergence implies unconditional convergence, from \citep{chernozhukov2018double}]
    Lemma 6.1. (Conditional Convergence implies unconditional) Let $\left\{X_m\right\}$ and $\left\{Y_m\right\}$ be sequences of random vectors. (a) If, for $\epsilon_m \rightarrow 0, \operatorname{Pr}\left(\left\|X_m\right\|>\epsilon_m \mid Y_m\right) \rightarrow_{\operatorname{Pr}} 0$, then $\operatorname{Pr}\left(\left\|X_m\right\|>\epsilon_m\right) \rightarrow 0$. In particular, this occurs if $E\left[\left\|X_m\right\|^q / \epsilon_m^q \mid Y_m\right] \rightarrow_{P r} 0$ for some $q \geq 1$, by Markov's inequality. (b) Let $\left\{A_m\right\}$ be a sequence of positive constants. If $\left\|X_m\right\|=O_P\left(A_m\right)$ conditional on $Y_m$, namely, that for any $\ell_m \rightarrow \infty$, $\operatorname{Pr}\left(\left\|X_m\right\|>\ell_m A_m \mid Y_m\right) \rightarrow_{P r} 0$, then $\left\|X_m\right\|=O_P\left(A_m\right)$ unconditionally, namely, that for any $\ell_m \rightarrow \infty$, $\operatorname{Pr}\left(\left\|X_m\right\|>\ell_m A_m\right) \rightarrow 0$.
\end{lemma}
\begin{lemma}[Chebyshev's inequality]
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. Then, for any $t>0$, we have

$$
{P}(|X-\mu| \geq t) \leq \frac{\sigma^2}{t^2}
$$

\end{lemma}

\begin{lemma}[Theorem 8.3.23 (Empirical processes via VC dimension), \citep{vershynin2018high}]\label{lemma-chaining-vc}
Let $\mathcal{F}$ be a class of Boolean functions on a probability space $(\Omega, \Sigma, \mu)$ with finite $V C$ dimension $\operatorname{vc}(\mathcal{F}) \geq 1$. Let $X, X_1, X_2, \ldots, X_n$ be independent random points in $\Omega$ distributed according to the law $\mu$. Then

$$
\mathbb{E} \sup _{f \in \mathcal{F}}\left|\frac{1}{n} \sum_{i=1}^n f\left(X_i\right)-\mathbb{E} f(X)\right| \leq C \sqrt{\frac{\operatorname{vc}(\mathcal{F})}{n}}
$$

\end{lemma}

\subsection{Lemmas}
\begin{lemma}[Convergence of $\hat \pi$]
\label{lemma-convergence-nuissance}
  Assume that with high probability, for some large constant $K$, 
$\norm{\hat e(X) - e(X)}_2 \leq K n^{-r_e},\norm{\hat \sigma^2(X)-\sigma^2(X)}_2 \leq K n^{-r_\sigma}$. Assume that $\sigma^2(X) >0$ so that its inverse is bounded $1/\sigma^2(X) \leq \gamma_{\sigma}.$
Recall that \Cref{thm-global-budget-solution} gives that \begin{align*}
&\pi^*(z,X) 
= 
 \sqrt{\frac{\sigma^2_z(X)}{  e_z^2(X)}}B
\left(
\Eb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\sigma^2_1(X)}{  e_1^2(X)}} 
+ 
\mathbb{I}[Z=0]\sqrt{\frac{\sigma^2_0(X)}{  e_0^2(X)}} }\right)^{-1} 
\end{align*} 
Define $\hat\pi^*(z,x)$ to be a plug-in version of the above (with $\hat\sigma^2,\hat e$, and $\Enb{\cdot}$).
Then $$\norm{\hat\pi^*(z,X)-\pi^*(z,X)}_2 = o_p(n^{-\min (r_e, r_\sigma, 1/2)}).$$
\end{lemma}



\begin{proof}

Let $a=\frac{\sigma^2_z(X)}{  e_z^2(X)}$, $b=\Eb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\sigma^2_1(X)}{  e_1^2(X)}} 
+ 
\mathbb{I}[Z=0]\sqrt{\frac{\sigma^2_0(X)}{  e_0^2(X)}} }$.

Let $c=\frac{\hat\sigma^2_z(X)}{  \hat e_z^2(X)}$, $d=\Enb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\hat \sigma^2_1(X)}{  \hat e_1^2(X)}} 
+ 
\mathbb{I}[Z=0]\sqrt{\frac{\hat \sigma^2_0(X)}{  \hat e_0^2(X)}} }$. 

Then $\norm{\pi^*(z,X)-\hat\pi^*(z,X)}_2 =\norm{a/b-c/d}_2.$ 

Positivity of $\sigma^2_z(X)$ gives the elementary equality that $\frac{a}{b} - \frac{c}{d} = \left(\frac{a - b}{b}\right) + \left(\frac{d - c}{d}\right)$. 

Therefore, by triangle inequality and boundedness, 
\begin{align}
&\norm{\pi^*(z,X)-\hat\pi^*(z,X)}_2 \leq 
\gamma_\sigma \norm{
\sqrt{ {\sigma^2 (X)}/{e^2(X)}}
-
\sqrt{{\hat \sigma^2 (X)}/{\hat e^2(X)}}
}_2 \nonumber
 \\
& 
+ 
\gamma_\sigma \abs{
\Enb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\hat \sigma^2_1(X)}{  \hat e_1^2(X)}}+\mathbb{I}[Z=0]\sqrt{\frac{\hat \sigma^2_0(X)}{  \hat e_0^2(X)}} }-\Eb{ \mathbb{I}[Z=1 ]\sqrt{\frac{\sigma^2_1(X)}{  e_1^2(X)}} 
+ 
\mathbb{I}[Z=0]\sqrt{\frac{\sigma^2_0(X)}{  e_0^2(X)}} }
}    
\label{eq-decomposition}
\end{align}

Next we show that for $z\in\{0,1\},$
\begin{equation}
    \norm{\sqrt{\hat \sigma^2_z (X)/\hat e^2_z(X)}
-\sqrt{ \sigma^2_z (X)/ e^2_z(X)}}_2 \leq \nu_e B_{\sigma^2} 
(\norm{\sqrt{\hat \sigma^2_z (X)}- \sqrt{ \sigma^2_z (X)}  }_2
+\norm{{e_z(X)} - {\hat e_z(X)} }_2) \label{eqn-num-bound}
\end{equation} 
In the below, we drop the $z$ argument. 

    By the triangle inequality, boundedness of $1/\hat e(X) \leq \nu_e $, and of $\sigma^2(X) \leq B_{\sigma^2}$:
\begin{align*}
&    \norm{\sqrt{\hat \sigma^2 (X)/\hat e^2(X)}
-\sqrt{ \sigma^2 (X)/ e^2(X)}}_2\\
& =     \norm{\sqrt{\hat \sigma^2 (X)/\hat e^2(X)}
\pm \sqrt{ \sigma^2 (X)/ \hat e^2(X)}
-\sqrt{ \sigma^2 (X)/ e^2(X)}}_2
\\
& \leq 
\nu_e \norm{\sqrt{\hat \sigma^2 (X)}- \sqrt{ \sigma^2 (X)}  }_2
+B_{\sigma^2} \norm{\frac{1}{e(X)} - \frac{1}{\hat e(X)} }_2
\end{align*}
For the second term:
\begin{align*}
    B_{\sigma^2} \norm{ \frac{1}{e(X)} - \frac{1}{\hat e(X)} }_2 \leq  B_{\sigma^2} \norm{\frac{1}{e(X)} - \frac{1}{\hat e(X)} }_2
    \leq B_{\sigma^2} \nu_e \norm{ e(X) - \hat e(X) }_2 
\end{align*}
since $1/e(X)$ is Lipschitz on the assumed bounded domain (overlap assumption).

For the first term: 
$$
\nu \norm{\sqrt{\hat \sigma^2 (X)}- \sqrt{ \sigma^2 (X)}  }_2 \leq \nu_e B_{\sigma^2} \norm{\hat{\sigma}^2(X) - \sigma^2(X)}_2$$
since $\sigma^2(X)$ is bounded away from 0, then $\sqrt{\sigma^2(X)}$ is Lipschitz. 

This proves \Cref{eqn-num-bound}, which bounds the first term of \Cref{eq-decomposition}. For the second term, denote for brevity
$$ \hat\beta (\sigma, e) = \Enb{ \mathbb{I}[Z=1 ]\sqrt{\frac{ \sigma^2_1(X)}{   e_1^2(X)}}+\mathbb{I}[Z=0]\sqrt{\frac{ \sigma^2_0(X)}{ e_0^2(X)}} },$$
and $\beta(\sigma,e)$ to be the above with $\Eb{\cdot}$ instead of $\Enb{\cdot}.$ Then the second term of \Cref{eq-decomposition} is $\hat\beta (\hat\sigma, \hat e) - \beta (\sigma, e),$ and decomposing further, that
\begin{align*}
    \hat\beta (\hat\sigma, \hat e) - \beta (\sigma, e) = \hat\beta (\hat\sigma, \hat e) - \hat\beta (\sigma,  e) 
    + \hat \beta (\sigma, e)- \beta (\sigma, e).
\end{align*}
Note that by Cauchy-Schwarz inequality,  and \Cref{lemma-chaining-vc} (chaining with VC-dimension), 
$$\hat\beta (\hat\sigma, \hat e) - \hat\beta (\sigma,  e)  \leq 2 \nu_e B_{\sigma^2}\left(\left\|\sqrt{\hat{\sigma}_z^2(X)}-\sqrt{\sigma_z^2(X)}\right\|_2+\left\|e_z(X)-\hat{e}_z(X)\right\|_2\right) + 2C \sqrt{\frac{\operatorname{vc}(\mathcal{F}_{\sqrt{\frac{\sigma^2}{e}}})}{n}} $$
And another application of \Cref{lemma-chaining-vc} gives that 
\begin{align*}
    \hat \beta (\sigma, e)- \beta (\sigma, e) & = \textstyle  (\mathbb{E}_n-\E)\left[\mathbb{I}[Z=1] \sqrt{\frac{\sigma_1^2(X)}{e_1^2(X)}}+\mathbb{I}[Z=0] \sqrt{\frac{\sigma_0^2(X)}{e_0^2(X)}}\right]
    \leq 2C \sqrt{\frac{\operatorname{vc}(\mathcal{F}_{\sqrt{\frac{\sigma^2}{e}}})}{n}}.
\end{align*}
Combining the above bounds with \Cref{eq-decomposition}, we conclude that $\left\|\pi^*(z, X)-\hat{\pi}^*(z, X)\right\|_2 = o_p(n^{-\min (r_e, r_\sigma, 1/2)}).$
\end{proof}


\end{document}
