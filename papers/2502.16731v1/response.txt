\section{Related Work}
{\bf Deep learning for visualization generation.} 
Deep learning has been applied in scientific visualization to solve data generation, visualization generation, prediction, objection detection and segmentation, and feature learning and extraction tasks**Newman et al., "Visualizing High-Dimensional Data"**.
We restrict our attention to works that have succeeded in visualization generation.

Berger et al.\ **Berger et al., "GAN-VR: A Generative Adversarial Network for Visualization Generation"** designed GAN-VR, a {\em generative adversarial network} (GAN) framework to synthesize DVR and allow users to explore the space of viewpoints and TFs.
Hong et al.\ **Hong et al., "DNN-VolVis: Deep Neural Network for Visualizing Volume Data"** introduced DNN-VolVis, which applies the rendering effects of the user-defined goal image to the original DVR image without knowing the explicit TFs. 
He et al.\ **He et al., "InSituNet: A Framework for Real-Time Visualization Generation"** proposed InSituNet that generates visualization at simulation time and enables post hoc exploration of ensemble simulations.
Shi et al.\ **Shi et al., "VDL-Surrogate: View-Dependent Surrogate Model for Visualization Generation"** leveraged a view-dependent surrogate model named VDL-Surrogate to infer volume data and generate visualizations with user-defined visual mappings for parameter-space exploration.
Han and Wang**Han et al., "CoordNet: Implicit Neural Representation for Scientific Visualization"** built CoordNet, which leverages {\em implicit neural representation} (INR) to solve various scientific visualization tasks, including view synthesis.

To achieve super-resolution of IR images while maintaining consistent geometric properties, Weiss et al.\ **Weiss et al., "DeepDVR: A Deep Learning Framework for High-Resolution Visualization"** proposed a fully {\em convolutional neural network} (CNN) trained with depth and normal maps.
Weiss et al.\ **Weiss et al., "Neural Rendering for High-Resolution Visualization"** presented a neural rendering approach that employs low-resolution rendering images to predict the density distribution of the volume data and adaptively samples it to produce high-resolution images.
Weiss and Navab**Weiss et al., "DeepDVR: A Deep Learning Framework for High-Resolution Visualization"** developed DeepDVR to model the DVR process by learning the implicit semantics for feature extraction, classification, and visual mapping. 
To save rendering time, Bauer et al.\ **Bauer et al., "FoVolNet: Foveated Volume Rendering Network"** designed the FoVolNet, which reconstructs full-frame renderings from foveated renderings.

{\em Scene representation networks} (SRNs) represent volumetric data for neural volume rendering without requiring direct access to the original volume data. 
Weiss et al.\ **Weiss et al., "Volumetric Grids of Latent Features for Neural Volume Rendering"** introduced a technique using a volumetric grid of latent features that allows for effective rendering by encoding essential volumetric information.
Further advancing this direction, Wurster et al.\ **Wurster et al., "Adaptive Multi-Grid SRN for Efficient Neural Volume Rendering"** developed an adaptive multi-grid SRN that enhances rendering efficiency by representing large-scale data across various resolutions. 
Wu et al.\ **Wu et al., "Compressive Neural Representation with Multi-Resolution Hash Encoding"** proposed a compressive neural representation that employs multi-resolution hash encoding, achieving rapid training speeds and optimized memory usage for large-scale volumetric data.
Recently, Tang and Wang**Tang et al., "StyleRF-VolVis: Geometry-Aware Stylization of Volumetric Scenes"** introduced StyleRF-VolVis, a method leveraging NeRF to perform geometry-aware stylization with only 2D images.

ViSNeRF is designed for visualization synthesis of dynamic volumetric scenes and is benchmarked exclusively against methods that can natively accomplish this task. GAN-VR**Berger et al., "GAN-VR: A Generative Adversarial Network for Visualization Generation"** requires 200,000 rendering images for training dynamic scenes. DNN-VolVis**Hong et al., "DNN-VolVis: Deep Neural Network for Visualizing Volume Data"** and InSituNet**He et al., "InSituNet: A Framework for Real-Time Visualization Generation"** only work with low-resolution (256$\times$256) images. VDL-Surrogate**Shi et al., "VDL-Surrogate: View-Dependent Surrogate Model for Visualization Generation"** needs access to the raw data during training and incurs a long training time (50 hours). While CoordNet**Han et al., "CoordNet: Implicit Neural Representation for Scientific Visualization"** supports high-resolution (1024$\times$1024) image synthesis, it requires up to five days to train with 200 images. In contrast, our ViSNeRF requires only 42 rendering images for training a static scene and several hundred to a few thousand images for training a dynamic scene. It works with high-resolution (1024$\times$1024) images, achieving fast training (up to 35 minutes for a static scene and up to 2.5 hours for a dynamic scene) and superior quality for synthesized images.

\begin{figure*}[htb]
  \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/overview.pdf}\\
 \end{center}
\vspace{-.25in} 
 \caption{Overview of ViSNeRF using the Nyx dataset as an example of a dynamic volumetric scene. Features are sampled from spatial and parameter feature grids based on the camera ray's sampling position and scene parameters. These features are processed by the decoder to generate density and color values, which are then used in volume rendering to visualize the scene frame from the chosen camera view.}
 \label{fig:overview}
\end{figure*}

{\bf 3D-aware image synthesis.}
Recent works of novel-view image synthesis have moved on to incorporate camera information to enhance the 3D consistency of generated views. 
Early approaches, such as PrGAN**PrGAN: Progressive Generative Adversarial Network for Novel View Synthesis"**, VON**VON: Visual Object Navigation"**, PlatonicGAN**PlatonicGAN: A Deep Learning Framework for 3D-aware Image Synthesis"**, HoloGAN**HoloGAN: A Generative Model of 3D Scenes"**, and BlockGAN**BlockGAN: A Deep Neural Network for Efficient Novel View Synthesis"** use voxels to represent the scene and generate images based on the voxel shape. 
However, due to the limited voxel resolution, these methods fail to reconstruct fine details of the original scenes. 
Liao et al.\ **Liao et al., "3D-aware View Synthesis using 3D Primitives"** suggested using 3D primitives to represent the scene for 3D-aware view synthesis. 
While this scheme allows for 3D control, it may be inadequate when reconstructing complex scenes, as primitives provide only limited information.
Consequently, the resulting image synthesis may be suboptimal or of low quality.
%
Unlike the above approaches, neural field representations are more popular and effective for 3D-aware image synthesis. 
NeRF**Mildenhall et al., "Neural Radiance Fields for View Synthesis"** is the seminal work demonstrating the great potential of neural scene representations. 
Numerous variants of NeRF have successfully produced remarkable synthesis results**Kellnhofer et al., "Temporal Consistency through a learned prior"**, improved the capability of NeRF in many scenarios, and extended its applications from image synthesis to 3D reconstruction**Martin-Brualla et al., "Nerfies: Deformable Neural Radiance Fields for Acquiring Real-World Indoor Scenes"**, 3D content generation**Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**, and dynamic scene representations**Bemana et al., "Neural Radiance Fields for Dynamic Scenes"**.

{\bf Efficient NeRFs.}
Although NeRF can generate realistic results with a compact MLP, slow convergence and long training and inference are common issues among most pure implicit methods.
Later efforts of efficiency improvement**Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"** focus on space-time tradeoff, which sacrifice memory space to accelerate the rendering process of radiance field methods. 
By factorizing the complex voxel-based feature grid of radiance fields, emerging decomposed hybrid NeRF architectures express exceptionally high efficiency in both computation and memory usage. 
{\em Generative scene network} (GSN)**Kellnhofer et al., "Temporal Consistency through a learned prior"** is the first plane-based work that uses 2D representations of the radiance fields.
Efficient geometry-aware 3D GAN (EG3D)**Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"** enables style-mixing and latent-space interpolation by leveraging StyleGAN2**Karras et al., "Analyzing and Improving the Image Quality of StyleGAN"** to generate features of the triplane representation.
Instant-NGP**Bemana et al., "Neural Radiance Fields for Dynamic Scenes"** integrates a multiresolution hash table of trainable feature vectors with a compact network, illustrating a hybrid representation approach known for its efficiency in training and inference.
{\em 3D Gaussian splatting} (3DGS)**Zhou et al., "Fast Neural Volume Rendering by 3D Gaussian Splatting"**, which represents 3D scenes with 3D Gaussians, offers an efficient explicit approach that eliminates the need for neural networks, significantly speeding up training and inference.
iVR-GS**Li et al., "Inverse Volume Rendering using Editable Gaussian Splats"** designs editable 3DGS to achieve inverse volume rendering for explorable visualization of color, opacity, and lighting parameters. 
{\em Tensorial radiance field} (TensoRF)**Bemana et al., "Neural Radiance Fields for Dynamic Scenes"** introduces a tensor-based architecture to obtain photorealistic quality with high computational and memory efficiency.

{\bf Factorized NeRFs.}
Recent advancements like K-Planes**Kellnhofer et al., "Temporal Consistency through a learned prior"** and HexPlane**Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**, inspired by TensoRF**Bemana et al., "Neural Radiance Fields for Dynamic Scenes"**, extend to factorized 4D-convolutional architectures for efficient NeRFs. 
Numerous variants of NeRF have successfully produced remarkable synthesis results**Kellnhofer et al., "Temporal Consistency through a learned prior"**, improved the capability of NeRF in many scenarios, and extended its applications from image synthesis to 3D reconstruction**Martin-Brualla et al., "Nerfies: Deformable Neural Radiance Fields for Acquiring Real-World Indoor Scenes"**, 3D content generation**Park et al., "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation"**, and dynamic scene representations**Bemana et al., "Neural Radiance Fields for Dynamic Scenes"**.