\section{Related Work}
{\bf Deep learning for visualization generation.} 
Deep learning has been applied in scientific visualization to solve data generation, visualization generation, prediction, objection detection and segmentation, and feature learning and extraction tasks~\cite{Wang-TVCG}.
We restrict our attention to works that have succeeded in visualization generation.

Berger et al.\ \cite{Berger-TFGAN-TVCG19} designed GAN-VR, a {\em generative adversarial network} (GAN) framework to synthesize DVR and allow users to explore the space of viewpoints and TFs.
Hong et al.\ \cite{Hong-DNN-VolVis-PVIS19} introduced DNN-VolVis, which applies the rendering effects of the user-defined goal image to the original DVR image without knowing the explicit TFs. 
He et al.\ \cite{He-InSituNet-TVCG20} proposed InSituNet that generates visualization at simulation time and enables post hoc exploration of ensemble simulations.
Shi et al.\ \cite{Shi-VDL-TVCG22} leveraged a view-dependent surrogate model named VDL-Surrogate to infer volume data and generate visualizations with user-defined visual mappings for parameter-space exploration.
Han and Wang~\cite{Han-CoordNet-TVCG} built CoordNet, which leverages {\em implicit neural representation} (INR) to solve various scientific visualization tasks, including view synthesis.

To achieve super-resolution of IR images while maintaining consistent geometric properties, Weiss et al.\ \cite{Weiss-SRNet-TVCG21} proposed a fully {\em convolutional neural network} (CNN) trained with depth and normal maps.
Weiss et al.\ \cite{Weiss-LearningAdaptive-TVCG22} presented a neural rendering approach that employs low-resolution rendering images to predict the density distribution of the volume data and adaptively samples it to produce high-resolution images.
Weiss and Navab~\cite{Weiss-DeepDVR-ArXiv21} developed DeepDVR to model the DVR process by learning the implicit semantics for feature extraction, classification, and visual mapping. 
To save rendering time, Bauer et al.\ \cite{Bauer-FoVolNet-TVCG23} designed the FoVolNet, which reconstructs full-frame renderings from foveated renderings.

{\em Scene representation networks} (SRNs) represent volumetric data for neural volume rendering without requiring direct access to the original volume data. 
Weiss et al.\ \cite{Weiss-CGF22} introduced a technique using a volumetric grid of latent features that allows for effective rendering by encoding essential volumetric information.
Further advancing this direction, Wurster et al.\ \cite{Wurster-TVCG24} developed an adaptive multi-grid SRN that enhances rendering efficiency by representing large-scale data across various resolutions. 
Wu et al.\ \cite{Wu-TVCG24} proposed a compressive neural representation that employs multi-resolution hash encoding, achieving rapid training speeds and optimized memory usage for large-scale volumetric data.
Recently, Tang and Wang~\cite{Tang-VIS24} introduced StyleRF-VolVis, a method leveraging NeRF to perform geometry-aware stylization with only 2D images.

ViSNeRF is designed for visualization synthesis of dynamic volumetric scenes and is benchmarked exclusively against methods that can natively accomplish this task. GAN-VR~\cite{Berger-TFGAN-TVCG19} requires 200,000 rendering images for training dynamic scenes. DNN-VolVis~\cite{Hong-DNN-VolVis-PVIS19} and InSituNet~\cite{He-InSituNet-TVCG20} only work with low-resolution (256$\times$256) images. VDL-Surrogate~\cite{Shi-VDL-TVCG22} needs access to the raw data during training and incurs a long training time (50 hours). While CoordNet~\cite{Han-CoordNet-TVCG} supports high-resolution (1024$\times$1024) image synthesis, it requires up to five days to train with 200 images. In contrast, our ViSNeRF requires only 42 rendering images for training a static scene and several hundred to a few thousand images for training a dynamic scene. It works with high-resolution (1024$\times$1024) images, achieving fast training (up to 35 minutes for a static scene and up to 2.5 hours for a dynamic scene) and superior quality for synthesized images.

\begin{figure*}[htb]
  \begin{center}
    \includegraphics[width=1.0\linewidth]{figures/overview.pdf}\\
 \end{center}
\vspace{-.25in} 
 \caption{Overview of ViSNeRF using the Nyx dataset as an example of a dynamic volumetric scene. Features are sampled from spatial and parameter feature grids based on the camera ray's sampling position and scene parameters. These features are processed by the decoder to generate density and color values, which are then used in volume rendering to visualize the scene frame from the chosen camera view.}
 \label{fig:overview}
\end{figure*}

{\bf 3D-aware image synthesis.}
Recent works of novel-view image synthesis have moved on to incorporate camera information to enhance the 3D consistency of generated views. 
Early approaches, such as PrGAN~\cite{PrGAN}, VON~\cite{VON}, PlatonicGAN~\cite{PlatonicGAN}, HoloGAN~\cite{HoloGAN}, and BlockGAN~\cite{BlockGAN}, use voxels to represent the scene and generate images based on the voxel shape. 
However, due to the limited voxel resolution, these methods fail to reconstruct fine details of the original scenes. 
Liao et al.\ \cite{ControllableGAN} suggested using 3D primitives to represent the scene for 3D-aware view synthesis. 
While this scheme allows for 3D control, it may be inadequate when reconstructing complex scenes, as primitives provide only limited information.
Consequently, the resulting image synthesis may be suboptimal or of low quality.
%
Unlike the above approaches, neural field representations are more popular and effective for 3D-aware image synthesis. 
NeRF~\cite{Mildenhall-NeRF-ECCV20} is the seminal work demonstrating the great potential of neural scene representations. 
Numerous variants of NeRF have successfully produced remarkable synthesis results~\cite{Schwarz-GRAF-NeurIPS20,Yu-pixelNeRF-CVPR21,Chan-PiGAN-CVPR21,Jain-DietNeRF-ICCV21,Mildenhall-RawNeRF-CVPR22,Gu-StyleNeRF-ICLR22}, improved the capability of NeRF in many scenarios, and extended its applications from image synthesis to 3D reconstruction~\cite{Oechsle-UNISURF-ICCV21,Wang-NeuS-NeurIPS21,Yariv-VolSDF-NeurIPS21}, 3D content generation~\cite{Niemeyer-GIRAFFE-CVPR21, Jain-DreamField-CVPR22, Wang-CLIP-NeRF-CVPR22, Chan-EG3D-CVPR22}, and dynamic scene representations~\cite{Park-Nerfies-ICCV21, Pumarola-D-NeRF-CVPR21, Singer-Text-To-4D-arXiv23, Cao-HexPlane-CVPR23}.

{\bf Efficient NeRFs.}
Although NeRF can generate realistic results with a compact MLP, slow convergence and long training and inference are common issues among most pure implicit methods.
Later efforts of efficiency improvement~\cite{Liu-NSVF-NeurIPS20, Yu-PlenOctrees-ICCV21, Fridovich-Keil-Plenoxels-CVPR22} focus on space-time tradeoff, which sacrifice memory space to accelerate the rendering process of radiance field methods. 
By factorizing the complex voxel-based feature grid of radiance fields, emerging decomposed hybrid NeRF architectures express exceptionally high efficiency in both computation and memory usage. 
{\em Generative scene network} (GSN)~\cite{Devries-GSN-ICCV21} is the first plane-based work that uses 2D representations of the radiance fields.
Efficient geometry-aware 3D GAN (EG3D)~\cite{Chan-EG3D-CVPR22} enables style-mixing and latent-space interpolation by leveraging StyleGAN2~\cite{Karras-StyleGAN2-CVPR20} to generate features of the triplane representation.
Instant-NGP~\cite{Thomas-InstantNGP} integrates a multiresolution hash table of trainable feature vectors with a compact network, illustrating a hybrid representation approach known for its efficiency in training and inference.
{\em 3D Gaussian splatting} (3DGS)~\cite{Kerbl-TOG23}, which represents 3D scenes with 3D Gaussians, offers an efficient explicit approach that eliminates the need for neural networks, significantly speeding up training and inference.
iVR-GS~\cite{Tang-PVIS25} designs editable 3DGS to achieve inverse volume rendering for explorable visualization of color, opacity, and lighting parameters. 
{\em Tensorial radiance field} (TensoRF)~\cite{Chen-TensoRF-ECCV22} introduces a tensor-based architecture to obtain photorealistic quality with high computational and memory efficiency.

{\bf Factorized NeRFs.}
Recent advancements like K-Planes~\cite{Fridovich-Keil-K-Planes-CVPR23} and HexPlane~\cite{Cao-HexPlane-CVPR23}, inspired by TensoRF, extend to factorized 4D NeRF representations to reconstruct dynamic scenes.
We also acknowledge works, such as Tensor4D~\cite{Shao-Tensor4D-CVPR23}, which factorize signed distance fields to represent 3D geometry. 
Yet, those works do not perform factorization on radiance fields, so they do not fit our scope.
ViSNeRF adopts a generalized factorization strategy for multidimensional NeRF representations to address the demands of visualization synthesis of dynamic volumetric scenes.

%\vspace{0.1in}