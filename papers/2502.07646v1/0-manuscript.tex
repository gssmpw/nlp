\documentclass[accepted]{uai2025arxiv} 
\usepackage[american]{babel}
% \usepackage[british]{babel}
\usepackage{amssymb}
%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz}
\usepackage{algorithm2e}
\usepackage{enumitem}
\setlistdepth{9}
\setlist[itemize,1]{label=$\bullet$}
\setlist[itemize,2]{label=$+$}
\setlist[itemize,3]{label=$--$}
\setlist[itemize,4]{label=$\star$}
\setlist[itemize,5]{label=$\circ$}
\setlist[itemize,6]{label=$\bullet$}
\setlist[itemize,7]{label=$\bullet$}
\setlist[itemize,8]{label=$\bullet$}
\setlist[itemize,9]{label=$\bullet$}
\renewlist{itemize}{itemize}{9}
\title{Causal Additive Models with Unobserved Causal Paths and Backdoor Paths}
\usepackage{times}
\usepackage{comment}

%\input{math_commands.tex}

\usepackage{url}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage[symbol]{footmisc}
\usepackage[inline]{enumitem}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nameref}
\usepackage{mathtools}
%\usepackage{subcaption}
\usepackage{bm}

\let\listofalgorithms\relax
\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algorithm2e}
\LinesNumbered 

%%%%%%%
\usepackage{color,xcolor}


%%%%%%%
\usepackage{hyperref}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}

\newcommand{\cind}{\mathrel{\perp\mspace{-9mu}\perp}}
\newcommand{\notcind}{\mathrel{\,\not\!\cind}}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}

\newtheorem{definition}{Definition}[section]
%\input{style}
%\input{comments}


\author[1,2]{\href{mailto:<thong-pham@biwako.shiga-u.ac.jp>}{Thong Pham\thanks{Corresponding author}}}
\author[3,2]{Takashi Nicholas Maeda}
\author[4,1,2]{Shohei Shimizu}
% Add affiliations after the authors
\affil[1]{%
    Faculty of Data Science, Shiga University\\
    Hikone, Japan
}
\affil[2]{%
    RIKEN AIP\\
    Tokyo, Japan
}
\affil[3]{%
    Computer Centre, Gakushuin University\\
    Tokyo, Japan
}
\affil[4]{%
    SANKEN, Osaka University\\
    Osaka, Japan
}
\begin{document}

\maketitle

\begin{abstract}%
Causal additive models have been employed as tractable yet expressive frameworks for causal discovery involving hidden variables. State-of-the-art methodologies suggest that determining the causal relationship between a pair of variables is infeasible in the presence of an unobserved backdoor or an unobserved causal path. Contrary to this assumption, we theoretically show that resolving the causal direction is feasible in certain scenarios by incorporating two novel components into the theory. The first component introduces a novel characterization of regression sets within independence between regression residuals. The second component leverages conditional independence among the observed variables. We also provide a search algorithm that integrates these innovations and demonstrate its competitive performance against existing methods. 
\end{abstract}

\section{Introduction}
 

Causal Additive Models (CAMs)~\citep{CAM} are nonlinear causal models in which the causal effects and the error terms are additive. Due to their tractability, they have been studied considerably and have many 
 practical applications in machine learning~\citep{pmlr-v162-budhathoki22a,yokoyama_2025}. 

When there are hidden variables, the problem of causal discovery in CAM, i.e., identification of the causal graph, remains underexplored, limiting its practical adoption where hidden variables are almost always present. Although causal discovery with hidden variables can be treated in full generality by the framework of Fast Causal Inference (FCI)~\citep{Spirtes2000}, it is natural to expect that we can do better than FCI in certain aspects by exploiting specific properties of CAMs.

\cite{maeda21a} identify cases in which the parent-child relationship in CAMs can be determined in the presence of unobserved variables by analyzing independencies and dependencies between certain regression residuals. This is significant and relies on specific properties of CAMs, as parent-child relationships are unidentifiable in the FCI framework.

Although~\cite{maeda21a} allow for hidden variables, their results require that no unobserved backdoor or causal paths exist. If such paths are present, they consider the parent-child relationship unidentifiable.

\cite{Schultheiss_2024} provide sufficient conditions for identifying the causal effect of an observed variable $X_i$ on another observed variable $X_j$ in nonlinear additive noise models with unobserved variables. While they do not discuss causal search as an application of their theory, it can formally be used to identify causal directions in CAMs: if the causal effect is nonzero, $X_i$ can be identified as an ancestor of $X_j$.  

Focusing on CAMs with unobserved variables, we show that a) the parent-child relationship can be identified in certain cases even in the presence of an unobserved backdoor or causal path, which is an improvement over~\cite{maeda21a}, and b) some causal directions beyond those identified by~\cite{Schultheiss_2024} can be determined. A high-level summary of our main contributions is as follows.
\begin{itemize}
\item By characterizing the regression sets used in determining independence, we show that the causal direction between a pair of variables can sometimes be identified using independence between the residuals, even when there are unobserved backdoor paths. We give an example, in Remark~\ref{remark:fig_2a},  of a causal direction that can be identified by this approach but cannot be identified by the theory of~\cite{Schultheiss_2024}.
\item We introduce a novel identification strategy that combines conditional independence between the original variables with independence between regression residuals to identify causal directions or parent-child relationships in certain pairs of observed variables, even in the presence of unobserved backdoor and causal paths. In Remark~\ref{remark:fig_2c}, we provide an example of a causal direction identifiable by this new approach but not by the theory of~\cite{Schultheiss_2024}.

\item We introduce CAM-UV-X, an extension of the CAM-UV algorithm of~\cite{maeda21a}. Our algorithm incorporates the above innovations to identify parent-child relationships and causal directions in the presence of unobserved backdoor and causal paths. Additionally, it addresses a previously overlooked limitation of the CAM-UV algorithm in identifying causal relationships when all backdoor and causal paths are observable.
\end{itemize}

The paper is organized as follows. We review the background in Section~\ref{sec:prelim}. New identifiability results are presented in Section~\ref{sec:theory}. Our proposed search method based on these theoretical results is described in Section~\ref{sec:algorithms}. Related works are discussed in Section~\ref{sec:related_works}. Numerical experiments are provided in Section~\ref{sec:experiment}. Conclusions are given in Section~\ref{sec:concluding}. 

\section{Preliminaries}\label{sec:prelim}
\subsection{The causal Model}
We assume the following causal additive model with unobserved variables as in~\cite{maeda21a}.
Let $X = \{x_i\}$ and $U = \{u_i\}$ be the sets of observable and unobservable variables, respectively. $G = (V,E)$ is the DAG with the vertex set $V = \{v_i\} = X \cup U$ and the edge set $E = \{(i,j) \mid v_i \in V, v_j \in V\}$. The data generation model is
\begin{equation}
v_i = \sum_{j \in P_i}f_{j}^{(i)}(x_j) + \sum_{k\in Q_i}f_{k}^{(i)}(u_k) + n_i,\label{eq:CAM_UV}
\end{equation}
where $P_i = \{j \mid (i,j) \in V \land x_j \in X\}$ is the set of observable direct causes of $v_i$, $Q_i = \{k \mid (i,k) \in V \land u_k \in U\}$ is the set of unobservable direct causes of $v_i$, $f_{j}^{(i)}$ is a non-linear function, and $n_i$ is the external noise at $v_i$. The functions and the external noises are assumed to satisfy Assumption~1 of~\cite{maeda21a} (see Appendix~\ref{appendix:assumption}). 

\subsection{Unobserved backdoor paths and unobserved causal paths}
Unobserved backdoor paths (UBPs) and unobserved causal paths (UCPs) play central roles in the theory of causal additive models with unobserved variables~\citep{maeda21a}.


\begin{definition}[Unobserved Causal Path]
A path in $G$ is called an unobserved causal path between $x_i$ and $x_j$ iff it is of the form $x_i \rightarrow \cdots \rightarrow u_k \rightarrow x_j$. 
\end{definition}

\begin{definition}[Unobserved Backdoor Path]
A path in $G$ is called an unobserved backdoor path between $x_i$ and $x_j$ iff it is of the form $x_i \leftarrow u_{k}\leftarrow \cdots  v_i \rightarrow \cdots \rightarrow u_l \rightarrow x_j$.
\end{definition}

Figs.~\ref{fig:UBP_UCP}a and b illustrate UCPs and UBPs, respectively.
\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{UBP_UCP.png}
\caption{Illustrations of unobserved backdoor paths and unobserved causal paths.\label{fig:UBP_UCP}}
\end{figure}

When a UBP or a UCP exists between $x_i$ and $x_j$, the confounder effect of the UBP or the causal effect of the UCP cannot be fully eliminated by regressions on any set of observed variables~\citep{maeda21a}. Consequently, the presence or absence of an edge between $x_i$ and
$x_j$ and the direction of causality are obscured. This intuition is formalized in the next section.

\subsection{Visible and invisible pairs}
The following concepts of visible parents, visible non-edge, and invisible pairs are fundamental to both the CAM-UV algorithm and our proposed CAM-UV-X. The lemmas are introduced and proved in~\cite{maeda21a}. In these lemmas, $\mathcal{G}$ is a subclass of the generalized additive models (GAMs)~\citep{hastie_GAM} that additionally satisfies Assumption~2 of~\cite{maeda21a} (see Appendix~\ref{appendix:assumption}).  For a function $G_i\in \mathcal{G}$, since $G_i$ belongs to GAMs,  $G_i(N) = \sum_{x_m\in N} g_{i,m}(x_m)$ where each $g_{i,m}(x_m)$ is a nonlinear function of $x_m$.

\begin{lemma}[visible parent]\label{lemma:visible_parent}
If and only if Eqs.~(\ref{eq:visible_parent_1}) and~(\ref{eq:visible_parent_2}) are satisfied, $x_j$ is a parent of $x_i$, and there is no UBP or UCP between $x_j$ and $x_i$. We call $x_j$ a visible parent of $x_i$.
\begin{align}
\forall G_1,G_2\in \mathcal{G},M \subseteq X \setminus \{x_i,x_j\}, N \subseteq X \setminus \{x_j\}:\nonumber \\ 
x_i - G_1(M) \notcind x_j - G_2(N), \label{eq:visible_parent_1}\\
\exists G_1,G_2\in \mathcal{G},M \subseteq X \setminus \{x_i\}, N \subseteq X \setminus \{x_i,x_j\}:\nonumber\\
x_i - G_1(M) \cind x_j - G_2(N). \label{eq:visible_parent_2}
\end{align}
\end{lemma}

\begin{lemma}[visible non-edge]\label{lemma:visible_non_edge}
If and only if Eq.~(\ref{eq:non_edge}) is satisfied, there is no direct edge between $x_j$ and $x_i$, and there is no UBP or UCP between $x_j$ and $x_i$. $(x_i,x_j)$ is called a visible non-edge.
\begin{align}
\exists G_1,G_2\in \mathcal{G},M \subseteq X \setminus \{x_i, x_j\}, N \subseteq X \setminus \{x_i,x_j\}:\nonumber\\
x_i - G_1(M) \cind x_j - G_2(N). \label{eq:non_edge}
\end{align}
\end{lemma}

\begin{lemma}[invisible pairs]\label{lemma:invisible}
If and only if Eq.~(\ref{eq:invisible}) is satisfied, there is a UBP/UCP between $x_j$ and $x_i$. $(x_i,x_j)$ is called an invisible pair.
\begin{align}
\forall M \subseteq X \setminus \{x_i\}, N \subseteq X \setminus \{x_j\},\forall G_{i},G_{j}\in \mathcal{G}:\nonumber \\
x_i - G_{i}(M) \notcind x_j - G_{j}(N). \label{eq:invisible}
\end{align}
\end{lemma}

Visible pairs are identifiable from the observed data, by checking Eqs.~(\ref{eq:visible_parent_1}) and~(\ref{eq:visible_parent_2}) for the case of a visible edge, and checking Eq.~(\ref{eq:non_edge}) for the case of a visible non-edge. Similarly, Eq.~(\ref{eq:invisible}) can certify that a pair is invisible from the observed data. However, Eq.~(\ref{eq:invisible}) cannot identify the causal directions in invisible pairs.

The CAM-UV algorithm~\citep{maeda21a} is designed to detect visible edges and non-edges while marking invisible pairs as such. However, it does not identify causal directions in invisible pairs.

\section{Identifiability in invisible pairs}\label{sec:theory}
We present new results that identify parent-child relationships and causal directions in invisible pairs. All omitted proofs can be found in Appendix~\ref{appendix:proof}.

\subsection{Identifiability by independence between regression residuals}\label{sub_sec:independent}

In this section, identifiability improvements come from characterizing the content of the regression sets $M$ and $N$ in Lemmas~\ref{lemma:visible_parent} and~\ref{lemma:visible_non_edge}. 

We provide Lemma~\ref{lemma:visible_edge_implication} to characterize the regression sets in Eq.~(\ref{eq:visible_parent_2}). For a visible edge $(x_i,x_j)$, the lemma tells us cases where one can infer that some variable $x_k$ in $M$ or $N$ must be a parent of $x_i$ or $x_j$. 
\begin{lemma}
\label{lemma:visible_edge_implication}
Consider distinct $x_i$, $x_j$, and $x_{k_1},\cdots,x_{k_n}$. Let $K = \{x_{k_1},\cdots,x_{k_n}\}$. If Eqs.~(\ref{eq:visible_edge_implication_1}),~(\ref{eq:visible_edge_implication_2}),  and~(\ref{eq:visible_edge_implication_3}) are satisfied, then 
\begin{enumerate}
\item $x_j$ is a visible parent of $x_i$, and 
\item Each $x_{k_q}$ is a parent of $x_j$ or a parent of $x_i$. Since $x_j$ is a parent of $x_i$, each $x_{k_q}$ is thus an ancestor of $x_i$.
\end{enumerate}
\begin{align}
\text{For } q = 1,\cdots,n: \forall M \subseteq X \setminus \{x_i,x_{k_q}\}, \nonumber \\
N \subseteq X \setminus \{x_j,x_{k_q}\},
\forall G_{i}^1,G_{j}^1\in \mathcal{G}:\nonumber \\
x_i - G_{i}^{1}(M) \notcind x_j - G_{j}^1(N),\label{eq:visible_edge_implication_1}
\\
\forall M \subseteq X \setminus \{x_i,x_j\}, N \subseteq X \setminus \{x_j\},\forall G_{i}^1,G_{j}^1\in \mathcal{G}:\nonumber\\
x_i - G_{i}^{1}(M) \notcind x_j - G_{j}^1(N),\label{eq:visible_edge_implication_2}\\
\exists Q_{1},Q_{2} \subseteq K: Q_{1}\cup Q_{2} = K, Q_{1}\cap Q_{2} = \varnothing:\nonumber\\ 
\exists G_{i}^{2},G_{j}^{2}\in \mathcal{G},M, N \subseteq X \setminus \{x_i,x_j\}\setminus K:\nonumber\\ 
 x_i - G_{i}^{2}\left(M \cup \{x_j\} \cup Q_1\right) \cind x_j - G_{j}^{2}(N\cup Q_2 )\label{eq:visible_edge_implication_3}
\end{align}
\end{lemma}

The intuition is that changes in independence status between regression residuals when a variable in $K$ is included/excluded from $M$ or $N$ allow inference of causal relationships between that variable and $x_i$, $x_j$, even when such relationships are invisible. 
 
\textbf{Example~1.} In Fig.~\ref{fig:illustrative_1}a, let $x_i = x_2$, $x_j = x_1$, $K =\{x_3\}$, $Q_1 = \{x_3\}$, and $Q_2 = \varnothing$. The variable $x_3$ is needed to block the backdoor path $x_1 \leftarrow U_1 \rightarrow x_3 \rightarrow x_2$ between $x_1$ and $x_2$. Therefore, $x_2 - G_2(M)$ and $x_1 - G_1(N)$ cannot be independent for any $M \subseteq X\setminus \{x_2,x_3\}$ and $N \subseteq X \setminus \{x_1,x_3\}$, i.e., Eq.~(\ref{eq:visible_edge_implication_1}) is satisfied. When $x_3$ is included to the regression set, all backdoor/causal paths are blocked, thus we have $\exists G_1,G_2: x_2 - G_2(x_1,x_3) \cind x_1$, i.e., Eq.~(\ref{eq:visible_edge_implication_3}) is satisfied. Furthermore, $x_2 - G_2(M)$ and $x_1 - G_1(N)$ cannot be independent for any $M \subseteq \{x_1,x_2\}$, $N \subseteq X\setminus \{x_1\}$, i.e., Eq.~(\ref{eq:visible_edge_implication_2}) is satisfied. By Lemma~\ref{lemma:visible_edge_implication}, the edge $x_1 \rightarrow x_2$ is identified and $x_3$ can be identified to be an ancestor of $x_2$, even when $(x_3,x_2)$ is invisible.   
 
\begin{figure}[!htb]
\centering
\includegraphics[width = \columnwidth]{graph_examples.png}
\caption{Examples of identifying causal relationships in the presence of UBPs/UCPs.\label{fig:illustrative_1}}
\end{figure}

\begin{remark}\label{remark:fig_2a} In Fig.~\ref{fig:illustrative_1}a, since no set of observed variables can d-separate $x_3$ and $U_2$, the condition (A1) of~\cite{Schultheiss_2024} cannot be satisfied. Therefore, their theory cannot identify the causal effect from $x_3$ to $x_2$, and thus cannot identify that $x_3$ is an ancestor of $x_2$.
\end{remark}

Similarly, we have the following lemma to characterize the regression sets in Eq.~(\ref{eq:non_edge}) in the case of a visible non-edge:
\begin{lemma}
\label{lemma:visible_non_edge_implication}
Consider distinct $x_i$, $x_j$, and $x_{k_1},\cdots,x_{k_n}$. Let $K = \{x_{k_1},\cdots,x_{k_n}\}$. If Eqs.~(\ref{eq:visible_edge_implication_1}) and~(\ref{eq:visible_non_edge_implication_1}) are satisfied, then
\begin{enumerate}
\item $(x_j,x_i)$ is a visible non-edge, and 
\item Each $x_{k_q}$ is either a parent of $x_i$ or a parent of $x_j$.
\end{enumerate}
\begin{align}
\exists Q_{1},Q_{2} \subseteq K: Q_{1}\cup Q_{2} = K, Q_{1}\cap Q_{2} = \varnothing:\nonumber\\ 
\exists G_{i}^{2},G_{j}^{2}\in \mathcal{G},M, N \subseteq X \setminus \{x_i,x_j\}\setminus K:\nonumber \\ 
 x_i - G_{i}^{2}\left(M \cup Q_1 \right) \cind x_j - G_{j}^{2}(N\cup Q_2). \label{eq:visible_non_edge_implication_1} 
\end{align}
\end{lemma}

\textbf{Example 2.}
In Fig.~\ref{fig:illustrative_1}b, let $x_i = x_1$, $x_j = x_2$, $K =\{x_3,x_4\}$, $Q_1 = \{x_3\}$, and $Q_2 = \{x_4\}$. The variable $x_4$ is needed to block the backdoor path $x_1 \leftarrow U_2 \rightarrow x_4 \rightarrow x_2$ between $x_1$ and $x_2$. Similarly, the variable $x_3$ is needed to block the backdoor path $x_1 \leftarrow x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, $x_2 - G_2(M)$ and $x_1 - G_1(N)$ cannot be independent for any $M, N$ that do not contain $x_3$ or $x_4$, i.e., Eq.~(\ref{eq:visible_edge_implication_1}) is satisfied. When $x_3$ and $x_4$ are included to the regression set, all backdoor/causal paths are blocked, thus we have $\exists G_1,G_2: x_2 - G_2(x_4) \cind x_1 - G_1(x_3)$, i.e., Eq.~(\ref{eq:visible_non_edge_implication_1}) is satisfied. By Lemma~\ref{lemma:visible_edge_implication}, the non-edge $(x_1,x_2)$ is identified, $x_3$ is identified as a parent of $x_1$ or $x_2$, and $x_4$ is also identified as a parent of $x_1$ or $x_2$.

\subsection{Identifiability by Conditional Independence}\label{sub_sec:conditional_independent}
In this section, identifiability improvements come from a novel approach of combining independence between regression residuals and conditional independence between the original variables. 

We introduce the following key lemma.
\begin{lemma}
\label{lemma:y_structure}
If $x_k$ is an ancestor of $x_i$ and $x_k \cind x_j \mid x_i$, then
\begin{enumerate}
    \item There is no backdoor path between $x_i$ and $x_j$, and 
    \item $x_j$ is not an ancestor of $x_i$.
\end{enumerate}
\end{lemma}
Although Lemma~\ref{lemma:y_structure} can be used alone to identify that $x_j$ is not an ancestor of $x_i$, combining it with independence conditions between regression residuals can identify the parent-child relation or the causal direction in certain invisible pairs as in the following corollaries.
 
\begin{corollary}\label{coro:ancestorship}
If Eq.~(\ref{eq:invisible}) is satisfied, and $x_k$ is an ancestor of $x_i$ and $x_k \cind x_j \mid x_i$, then $x_i$ is an ancestor of $x_j$.
\end{corollary}
\begin{proof}
Eq.~(\ref{eq:invisible}) implies that a UBP/UCP must exist between $x_i$ and $x_j$, due to Lemma~\ref{lemma:invisible}. Lemma~\ref{lemma:y_structure} rules out the possibilities of any UBP and the causal direction from $x_j$ to $x_i$. Therefore, the only possible scenario is that there is a UCP from $x_i$ to $x_j$, which means $x_i$ is an ancestor of $x_j$.
\end{proof}

\begin{corollary}\label{coro:parentship}
Suppose that $x_k$ is a parent of $x_i$ or a parent of $x_j$ (e.g., by Lemma~\ref{lemma:visible_edge_implication} or Lemma~\ref{lemma:visible_non_edge_implication}). Furthermore, for a fourth variable $x_u$, if $x_u$ is an ancestor of $x_i$ and $x_u \cind x_k \mid x_i$, then $x_k$ is a parent of $x_j$.   
\end{corollary}
\begin{proof}
Lemma~\ref{lemma:y_structure} implies that $x_k$ is not an ancestor, and thus not a parent, of $x_i$. Therefore, the only possibility is that $x_k$ is a parent of $x_j$.
\end{proof}

\textbf{Example 3.} In Fig.~\ref{fig:illustrative_1}c, consider the invisible pair $(x_4,x_5)$. Since this pair is not on any backdoor path or causal path of any visible pair, Lemmas~\ref{lemma:visible_edge_implication} and~\ref{lemma:visible_non_edge_implication} cannot identify the causal direction in this pair. From the visible pairs $(x_2,x_4)$ and $(x_3,x_4)$, $x_2$ and $x_3$ are identified to be parents of $x_4$ by Lemma~\ref{lemma:visible_parent}. From the visible non-edge $(x_2,x_3)$, $x_1$ is identified to be a parent of $x_2$ or a parent of $x_3$ by applying Lemma~\ref{lemma:visible_non_edge_implication} with $K=\{x_1\}$. Thus, $x_1$ is identified to be an ancestor of $x_4$. The ancestors of $x_4$ are $x_2$, $x_3$, and $x_1$. Since $x_2 \notcind x_5 \mid x_4$ and $x_3 \notcind x_5 \mid x_4$ due to the unobserved $U_4$ and $U_5$, Corollary~\ref{coro:ancestorship} cannot be applied with $x_k = x_2$ or $x_k = x_3$. Nevertheless, we have $x_1 \cind x_5 \mid x_4$. After checking Eq.~(\ref{eq:invisible}) for $x_i = x_4$ and $x_j = x_5$, we can identify $x_4$ as an ancestor of $x_5$ by applying Corollary~\ref{coro:ancestorship} with $x_k = x_1$.

\begin{remark}\label{remark:fig_2c} In Fig.~\ref{fig:illustrative_1}c, since no set of observed variables can d-separate $x_4$ and $U_3$, the condition (A1) of~\cite{Schultheiss_2024} cannot be satisfied. Thus, their theory cannot identify that $x_4$ is an ancestor of $x_5$.
\end{remark}
 
%An example is shown in Fig.~\ref{fig:illustrative_4}.

\textbf{Example 4.} Consider the invisible pair $(x_2,x_3)$ in Fig~\ref{fig:illustrative_1}d. Note that Corollary~\ref{coro:ancestorship} cannot identify the causal direction in this pair. Applying Lemma~\ref{lemma:visible_non_edge_implication} to the visible non-edge $(x_1,x_2)$ with $K = \{x_3\}$, one can identify that $x_3$ is a parent of $x_1$ or a parent of $x_2$. The edge $x_4\rightarrow x_1$ is visible and thus can be identified. One can check that $x_4 \cind x_3 \mid x_1$. Applying Corollary~\ref{coro:parentship} with $x_k = x_3$, $x_i = x_1$, $x_j = x_2$, and $x_u = x_4$ identifies $x_3$ as a parent of $x_2$. 
\begin{remark}
Identifying the causal directions between $(x_3,x_2)$ in Fig.~\ref{fig:illustrative_1}a, $(x_4,x_5)$ in Fig.~\ref{fig:illustrative_1}c, and $(x_3,x_2)$ in Fig.~\ref{fig:illustrative_1}d, as well as the non-edge $(x_1,x_2)$ in Fig.~\ref{fig:illustrative_1}b, relies on independence conditions between regression residuals specific to CAMs. General methods based solely on Markov equivalence classes of conditional independence, such as FCI, cannot make such identifications.
\end{remark}

\section{Search methods}\label{sec:algorithms}
In Section~\ref{subsection:cam_uv_limitation}, we highlight previously overlooked limitations of the CAM-UV algorithm, which makes it erroneously identify certain visible pairs as invisible. We provide our method in Section~\ref{subsection:our_method}.

\subsection{Limitations of the CAM-UV algorithm}\label{subsection:cam_uv_limitation}
The CAM-UV algorithm is not complete in identifying visible edges/non-edges, and is not sound in identifying invisible pairs. \emph{Soundness} and \emph{completeness} are two concepts often used in evaluations of a causal discovery algorithm~\citep{Spirtes2000}. The concepts can be adapted to our context as follows. Recall that $E$ is the true edge set and $G$ is the true causal graph. Let $A$ denote the adjacency matrix estimated by an algorithm. An algorithm is complete for identifying visible edges iff $(i,j) \in E$ and $(i,j)$ is visible in $G$ implies $A(i,j) = 1$ in the algorithm output. An algorithm is sound for identifying visible edges iff $A(i,j) = 1$ implies $(i,j) \in E$ and $(i,j)$ is visible in $G$. Soundness and completeness can be defined similarly for visible non-edges and invisible pairs.

\textbf{Fig.~\ref{fig:illustrative_1}a} can be used to demonstrate that CAM-UV is not complete in identifying visible edges and not sound in identifying invisible pairs. The edge $x_1\rightarrow x_2$ is visible, i.e., identifiable from observed data using Lemma~\ref{lemma:visible_parent}. However, $x_3$ is an invisible parent of $x_2$ due to the unobserved $U_2$. In such cases, the CAM-UV algorithm in principle will erroneously mark the visible edge as invisible. See Appendix~\ref{appendix:cam_uv_step_by_step} for a proof obtained by executing CAM-UV step by step in this example. 

\textbf{Fig.~\ref{fig:illustrative_1}b} can be used to demonstrate that CAM-UV is also not complete in identifying visible non-edges. The non-edge $(x_1,x_2)$ is visible, i.e, identifiable from observed data by Lemma~\ref{lemma:visible_non_edge}. However, to block all backdoor and causal paths between $x_1$ and $x_2$, one must add $x_3$ and $x_4$ to the regression sets in Eq.~(\ref{eq:non_edge}). Furthermore, the pairs $(x_1,x_3)$, $(x_1,x_4)$, $(x_2,x_3)$, and $(x_2,x_4)$ are all invisible. In this case, CAM-UV will erroneously mark the visible non-edge as invisible. See Appendix~\ref{appendix:cam_uv_step_by_step} for a step-by-step execution of CAM-UV in this example. 

\subsection{Proposed search method}\label{subsection:our_method}
Our proposed method, described in Algorithm~\ref{alg:CAM_UV_extended}, addresses the limitations of the CAM-UV algorithm by improving the identification of visible pairs. Additionally, it leverages Lemmas~\ref{lemma:visible_edge_implication} and~\ref{lemma:visible_non_edge_implication}, as well as Corollaries~\ref{coro:ancestorship} and~\ref{coro:parentship}, to infer parentships or causal directions in invisible pairs. The output of CAM-UV-X is $A$, $M_1,\ldots,M_p$, $H_{1},\ldots, H_{p}$, and $C_1,\ldots,C_p$. $A$ is the adjacency matrix over the observed variables. $A(i,j) = 1$ if $x_j$ is inferred to be a parent of $x_i$, $0$ if there is no directed edge from $x_j$ to $x_i$, and NaN (Not a Number) if $(x_i,x_j)$ is inferred to be invisible. $M_i$ is the set of ancestors of $x_i$ identified by Lemma~\ref{lemma:visible_edge_implication} and Corollary~\ref{coro:ancestorship}. $H_{i}$ is the set of nodes guaranteed to be not an ancestor of $x_i$, identified by, for example, Lemma~\ref{lemma:y_structure}. $C_k$ contains unordered pairs $[i,j]$ such that $x_k$ is a parent of $x_i$ or $x_k$ is a parent of $x_j$.  
\SetKw{Continue}{continue}
\SetKw{Break}{break}
\begin{algorithm}[!htb] 
\caption{CAM-UV-X}\label{alg:CAM_UV_extended}
\KwData{$n\times p$ data matrix $X$ for $p$ observed variables, maximum number of parents $d$, significant level $\alpha$}
\KwResult{$A$, $\{M_1,\ldots,M_p\}$, $\{H_1,\ldots,H_p\}$, $\{C_1,\ldots,C_p\}$}
 $A \gets$ CAM-UV($X,d,\alpha$)\;

Initialize $M_i \gets \emptyset$, $H_i \gets \emptyset$, $C_i \gets \emptyset$ for $i = 1,\ldots,p$

 $noChange \gets False$ \;
 
 \While{$noChange$ == False}{
  $noChange \gets True$ \;
  
  Find the set $S = \{ (i,j) \mid A(i,j) = A(j,i) = \text{NaN}\}$ 

    \For{each $(i,j) \in S$}{
    $noChange \gets \texttt{checkVisible}(i,j)$
 }
  Find the set $S = \{ (i,j) \mid A(i,j) = A(j,i) = \text{NaN}\}$ 
  
      \For{each $(i,j) \in S$}{
    $noChange \gets \texttt{checkCI}(i,j)$ 
 }
 
   $noChange \gets \texttt{checkParentInvi}()$   
 }
\end{algorithm}

In line 1, CAM-UV is executed to obtain an initial estimation of $A$. The Boolean variable $noChange$ indicates whether a modification has occurred in $A$ or in any $M_{1},\ldots, M_{p}$, $H_1,\ldots,H_p$,   $C_{1},\ldots,C_{p}$. CAM-UV-X executes the loop in line 4 until no further modification is detected.

The procedure~\texttt{checkVisible}, described in Algorithm~\ref{alg:checkVisible}, tests whether each NaN element in the current matrix $A$ can be converted to $1$, that is, a visible edge, by Lemma~\ref{lemma:visible_edge_implication} (lines 6-15), or to $0$, i.e., a visible non-edge, by Lemma~\ref{lemma:visible_non_edge_implication} (lines 17-28). 

\begin{algorithm}[!htb] 
\setcounter{AlgoLine}{0}
\caption{\texttt{checkVisible}}\label{alg:checkVisible}
\KwIn{indices $i$ and $j$}
\KwOut{Boolean variable $noChange$} 
   $P_i \gets \{v\mid A(i,v) = 1\}$; $P_j \gets \{v \mid A(j,v) = 1\}$
    
    $Q \gets \{k \mid A(j,k) = \text{NaN} \text{ and } A(i,k) = \text{NaN} \}$ 
    
    \For{size in $1,\ldots,|Q|$}{
    \For{each $K \subseteq Q , |K| == size$}{
    \For{each $(Q_1, Q_2)$ such that $Q_1\cup Q_2 = K$ and $Q_1 \cap Q_2 = \emptyset$}{
    
$e\gets\widehat{\text{p-HSIC}}\big(x_i - G_1(P_i \cup \{x_j\} \cup Q_1 \setminus \{x_i\}),$
    $x_j - G_2(P_j \cup Q_2 \setminus \{x_i\} \setminus \{x_j\})\big)$  
    
    \eIf{$e > \alpha \ \land$ \texttt{checkTrueEdge}(i,j)}
    {
    $noChange \gets False$
    
    $A(i,j) \gets 1$; $A(j,i) \gets 0$
    
    \For{$k \in K$}{
    \If{\texttt{checkOnPath}(i,j,k)}{
    $C_k \gets C_k \cup \{[i,j]\}$;
    $M_i \gets M_i \cup \{x_k\}$; $H_k \gets H_k \cup \{x_i\}$; $A(k,i) \gets 0$
    }
    {}
    \Break
    }
    }{
    $h \gets \widehat{\text{p-HSIC}}\big(x_i - G_1(P_i \cup Q_1 \setminus \{x_i\} \setminus \{x_j\}),$
 $x_j - G_2(P_j \cup Q_2  \setminus \{x_i\}\setminus\{x_j\})\big)$

   \If{$h > \alpha$}{
    $noChange \gets False$
    
    $A(i,j) \gets 0$; $A(j,i) \gets 0$
    
    \For{$k \in K$}{
    \If{\texttt{checkOnPath}(i,j,k)}{
    $C_k \gets C_k \cup \{[i,j]\}$
    }
    }
    \Break
    }
    }
    }
    \If{noChange == False}{\Break}
    }
    \If{noChange == False}{\Break}
    }
\end{algorithm}

On lines 6-15 of \texttt{checkVisible}, Lemma~\ref{lemma:visible_edge_implication} is checked. Eq.~(\ref{eq:visible_edge_implication_3}) is satisfied if the value $e$, calculated on line 6, is greater than $\alpha$. Here, $\widehat{\text{p-HSIC}}$ is the p-value of the gamma independence test based on Hilbert–Schmidt Independence Criteria~\citep{gretton_hsic}. The algorithm then invokes the procedure \texttt{checkTrueEdge}, described in Algorithm~\ref{alg:check_True_Edge}, to check Eq.~(\ref{eq:visible_edge_implication_2}). If the equation is satisfied, $x_j \rightarrow x_i$ is concluded to be an edge. Consequentially, $A(j,i)$ and $A(j,i)$ are modified at line 9. For each $k$ in $K$, the procedure \texttt{checkOnPath}, described in Algorithm~\ref{alg:check_OnPath}, is invoked to check Eq.~(\ref{eq:visible_edge_implication_1}). If the equation is satisfied, on line 12 $x_k$ is added to $M_i$, $x_i$ is added to $H_k$, and $[i,j]$ is added to $C_k$. Furthermore, $A(k,i)$ is set to $0$ due to acyclicity. 

\begin{algorithm}[!htb] 
\setcounter{AlgoLine}{0}
\caption{\texttt{checkTrueEdge}}\label{alg:check_True_Edge}
\KwIn{indices $i$ and $j$}
\KwOut{Boolean value $isEdge$}
$P_i \gets \{v\mid A(i,v) = 1\}$; $P_j \gets \{v \mid A(j,v) = 1\}$
    
    $Q \gets \{k \mid A(j,k) = \text{NaN} \text{ and } A(i,k) = \text{NaN} \}$ 
    
$isEdge \gets True$

\For{each set $M \subseteq Q$ and $N \subseteq Q$}{ 
$a\gets\widehat{\text{p-HSIC}}\big(x_i - G_1(P_i \cup M),x_j - G_2(P_j \cup N)\big)$

  \If{$a > \alpha$} {
  $isEdge \gets False$
  
  \Break
  }
}
\end{algorithm}

\begin{algorithm}[!htb] 
\setcounter{AlgoLine}{0}
\caption{\texttt{checkOnPath}}\label{alg:check_OnPath}
\KwIn{indices $i$, $j$, and $k$}
\KwOut{Boolean value $isOnPath$}

$isOnPath \gets True$

$P_i \gets \{v\mid A(i,v) = 1\}$; $P_j \gets \{v \mid A(j,v) = 1\}$

\For{each set $M \subseteq X \setminus \{x_i,x_k\}$ and $N \subseteq X \setminus \{x_j,x_k\}$}{ 
$a\gets\widehat{\text{p-HSIC}}\big(x_i - G_1(P_i \cup M),x_j - G_2(P_j \cup N)\big)$

  \If{$a > \alpha$} {
  $isOnPath \gets False$
  
  \Break
  }
}
\end{algorithm}


On lines 17-28 of \texttt{checkVisible}, Lemma~\ref{lemma:visible_non_edge_implication} is checked. Eq.~(\ref{eq:visible_non_edge_implication_1}) is satisfied if the value $h$, calculated on line 17, is greater than $\alpha$. If so, $(x_i,x_j)$ is concluded to be a non-edge. Consequentially, $A$ is modified on line 20. For each $k$ in $K$, the procedure \texttt{checkOnPath} is invoked to check Eq.~(\ref{eq:visible_edge_implication_1}). If the equation is satisfied, the pair $[i,j]$ is added to $C_k$ on line 23.


The procedure \texttt{checkCI}, described in Algorithm~\ref{alg:check_CI}, checks conditional independence of the form $x_k \cind x_i \mid x_j$ with $A(i,j) = $ NaN and $x_k$ being an ancestor of $x_i$. $\widehat{\text{p-CI}}$ is the p-value of some conditional independence test. Some examples are the conditional mutual information test based on nearest-neighbor estimator (CMIknn of~\cite{Runge_CMIknn}) and the conditional independence test based on Gaussian process regression and distance correlations(GPDC of~\citep{GPDC}). If conditional independence is satisfied (line 3), $x_j$ is not an ancestor of $x_i$ due to Lemma~\ref{lemma:y_structure}. Thus, $x_j$ is added to $H_i$ and $A(i,j)$ is set to $0$ in line 4. Furthermore, Eq.~(\ref{eq:invisible}) is checked, and if satisfied, $x_i$ is an ancestor of $x_j$ due to Corollary~\ref{coro:ancestorship}. Thus, $x_i$ is added to $M_j$ in line 6.
\begin{algorithm}[!htb] 
\setcounter{AlgoLine}{0}
\caption{\texttt{checkCI}}\label{alg:check_CI}
\KwIn{indices $i$ and $j$}
\KwOut{Boolean value $noChange$}

\For{each ancestor $x_k$ of $x_i$}{ 
$e \gets \widehat{\text{p-CI}}(x = x_k, y = x_j, z = x_i)$

  \If{$e > \alpha$} {
  $H_i \gets H_i\cup \{x_j\}$;  $A(i,j) \gets 0$
  
  \If{Eq.~(\ref{eq:invisible}) is satisfied}{ 
  $M_j \gets M_j \cup \{x_i\}$
  }
  $noChange \gets False$
  }
}
\end{algorithm}

The procedure \texttt{checkParentInvi}, described in Algorithm~\ref{alg:enforceConsistency}, checks Corollary~\ref{coro:parentship}. For each ordered pair $[i,j] \in C_k$, we check whether $x_k$ is not an ancestor of $x_i$ by checking whether $x_k$ is in $H_i$. If so, we conclude that $x_k$ is a parent of $x_j$.  
\begin{algorithm}[!htb] 
\setcounter{AlgoLine}{0}
\caption{\texttt{checkParentInvi}}\label{alg:enforceConsistency}
\KwOut{Boolean value $noChange$}

\For{$k = 1,\cdots, p$}{
    \For{each ordered pair $[i,j] \in C_k$}{
        \If{$x_k \in H_i$}{
            $A(j,k) \gets 1; A(k,j) \gets 0$

            $noChange \gets False$
            }
        }
}
\end{algorithm}

\section{Related works}\label{sec:related_works}
CAMs~\citep{Buhlmann14CAM} belong to a subclass of the Additive Noise Models (ANMs)~\citep{Hoyer09NIPS}, which are causal models that assume nonlinear causal functions with additive noise terms, but the causal effects can be non-additive. Another important causal model is the Linear Non-Gaussian Acyclic Model (LiNGAM)~\citep{Shimizu06JMLR}, which assumes linear causal relationships with non-Gaussian noise.

A key extension of these causal models involves cases with hidden common causes~\citep{Hoyer08IJAR,Zhang10UAI-GP,Tashiro14NECO,Salehkaleybar2020learning}. To address such scenarios, methods like the Repetitive Causal Discovery (RCD) algorithm~\citep{Maeda20AISTATS} and the CAM-UV (Causal Additive Models with Unobserved Variables) algorithm~\citep{maeda21a} have been developed.

Estimation approaches for causal discovery can be generally categorized into three groups: constraint-based methods~\citep{Spirtes91PC,Spirtes95FCI}, score-based methods~\citep{Chickering2002}, and continuous-optimization-based methods~\citep{Zheng18Neurips,Bhattacharya21ABC}. Additionally, a hybrid approach that combines the ideas of constraint-based and score-based methods has been proposed for non-parametric cases~\citep{Ogarrio16Hybrid}.

\section{Experiments}\label{sec:experiment}
\subsection{Illustrative examples}\label{sec:experiment_illustrative}
To assess whether CAM-UV-X addresses the limitations of CAM-UV discussed in Section~\ref{subsection:cam_uv_limitation}, we demonstrate the performance of CAM-UV-X on the graphs in Figs.~\ref{fig:illustrative_1}a and b. The data generating process in Eq.~(\ref{eq:CAM_UV}) is set as follows. The non-linear function $f_{j}^{(i)}(x_j)$ is set to $(x_j + a)^c + b$ with random coefficients $a$, $b$, and $c$. We use the same setting for $f_{k}^{(i)}$. Each external noise $n_i$ is randomly chosen from pre-determined non-Gaussian noise distributions. For each graph, $100$ datasets, each of $500$ samples, are generated. 

We ran CAM-UV and CAM-UV-X with the confidence level $\alpha = 0.1$. In CAM-UV-X, we use CMIknn of~\citep{Runge_CMIknn} as the conditional independence estimator.

We measured the success rate of identifying the visible edge $x_1\rightarrow x_2$ and identifying $x_3$ as an ancestor of $x_2$ in Fig.~\ref{fig:illustrative_1}a, and identifying the non-edge $(x_1,x_2)$ in Fig.~\ref{fig:illustrative_1}b. We also evaluated precision, recall, and $F_1$ in estimating the true adjacency matrix. The results are shown in Figs.~\ref{fig:illustrative_2} and~\ref{fig:illustrative_3}.

\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{illustrative_1.png}
\caption{Performance on the graph in Fig.~\ref{fig:illustrative_1}a.}\label{fig:illustrative_2}
\end{figure}

\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{illustrative_2.png}
\caption{Performance on the graph in Fig.~\ref{fig:illustrative_1}b.}\label{fig:illustrative_3}
\end{figure}
In Fig.~\ref{fig:illustrative_2}, CAM-UV-X is more successful at identifying the visible edge $x_1 \rightarrow x_2$ and at identifying $x_3$ as an ancestor of $x_2$, and achieves higher precision,  recall, and $F_1$. In Fig.~\ref{fig:illustrative_3}, while the precision of CAM-UV-X is lower, CAM-UV-X is more successful at identifying the non-edge and achieves higher recall.

\subsection{Random graph experiment}
We investigate CAM-UV-X in simulated causal graphs generated using the popular Barab{\'{a}}si–Albert (BA) model~\citep{ba_model}, which produces random graphs with heavy-tailed degree distributions commonly observed in real-world networks~\citep{barabasi2016network}. See Appendix~\ref{appendix:ER_experiment} for an experiment with causal graphs generated from the Erd\"{o}s-R\'{e}nyi (ER) model~\citep{erdos59a}, which produces random graphs with binomial degree distributions.

We generate $50$ BA graphs with $40$ nodes, where each node has five children. For each graph, we create data using the same process as in Section~\ref{sec:experiment_illustrative}. We then randomly select $10$ variables and create a final dataset of only these $10$ variables. Each dataset contains $500$ samples. We ran the algorithms with confidence levels $\alpha = 0.05$, $0.1$, and $0.2$.

\textbf{Identifying the adjacency matrix.} The results are shown in Fig.~\ref{fig:BA_exp_adjacency}. CAM-UV-X is better in all metrics. See Appendix~\ref{appendix:ER_experiment} for definitions of metrics used in the experiments.

\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{BA_adjacency.png}
\caption{Performance of identifying the adjacency matrix in BA random  graphs.}\label{fig:BA_exp_adjacency}
\end{figure}

\textbf{Identifying ancestor relationships.}
The results are shown in Fig.~\ref{fig:BA_exp_ancestor}. While precision is similar, CAM-UV-X achieves higher recall and F1.

\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{BA_ancestor.png}
\caption{Performance of identifying ancestors in BA random graphs.}\label{fig:BA_exp_ancestor}
\end{figure}

Taking into account the results in Appendix~\ref{appendix:ER_experiment}, CAM-UV-X achieves higher recall in both tasks in both graph types, higher F1 in both tasks in BA graphs, higher precision in identifying the adjacency matrix in both graph types, and comparable metric values for other cases.

\subsection{FMRI data}
We conducted experiments on simulated fMRI data from~\cite{SMITH2011875}, which is based on a well-established mathematical model of brain region interactions~\citep{FRISTON20031273}. 
Specifically, we used their ``sim2’’ dataset, which they describe as one of the most “typical” network scenarios. 
Using this dataset with ten variables, we randomly chose $k \in [2,3,4,5,6]$ variables, extracted randomly $1000$ samples from those variables, and executed the algorithms. This process was repeated $100$ times for each $k$. The results are shown in Fig.~\ref{fig:fMRI}.
\begin{figure}[!htb] 
\centering
\includegraphics[width = \columnwidth]{fMRI.png}
\caption{Performance of identifying ancestor relationships in the fMRI dataset.}\label{fig:fMRI}
\end{figure}

Overall, CAM-UV-X and CAM-UV performed the same in this dataset.

\section{Conclusions}\label{sec:concluding}
We introduced new identifiability results for parent-child relationships and causal directions in the presence of unobserved backdoor or causal paths in causal additive models. These results stem from a new characterization of the regression sets used in determining independence and a novel approach that combines independence between regression residuals with conditional independence between the original variables.

We introduced the CAM-UV-X algorithm, which integrates these theoretical insights and addresses previously overlooked limitations of the CAM-UV algorithm. Through experiments, we demonstrated that CAM-UV-X effectively resolves these limitations while comparing favorably with CAM-UV.

A systematic investigation to fully characterize what is identifiable in causal additive models with hidden variables remains future work. We believe that our approach of combining independence and conditional independence can yield new identifiability results for other causal models, such as linear non-Gaussian models with hidden variables. Exploring this potential is an avenue for future research.

\begin{acknowledgements}
This work was partially supported by the Japan Science and Technology Agency (JST) under CREST Grant Number JPMJCR22D2 and by the Japan Society for the Promotion of Science (JSPS) under KAKENHI Grant Number JP24K20741.
\end{acknowledgements}

\bibliography{ML_CI_library.bib,bib_sshimizu}

\newpage

\onecolumn
\appendix
\begin{center}
\Large \textbf{APPENDIX}
\end{center}



\renewcommand\thefigure{\thesection.\arabic{figure}}    
\setcounter{figure}{0}  
\renewcommand\thetable{\thesection.\arabic{table}}    
\setcounter{table}{0} 
\section{Assumptions of the CAM-UV model}\label{appendix:assumption}
The CAM-UV model of~\cite{maeda21a} makes the following assumptions.

\textbf{Assumption 1.}
If variables $v_i$ and $v_j$ have terms involving functions of the same external effect $n_k$, then $v_i$ and $v_j$ are mutually dependent, i.e., $(n_k \notcind v_i) \land (n_k \notcind v_j) \Rightarrow v_i \notcind v_j$.

\textbf{Assumption 2.}
When both $x_i- G_i(M)$ and $x_j-G_j(N)$ have terms involving functions of the same external effect $n_k$, then $x_i -G_i(M)$ and $x_j -G_j(N)$ are mutually dependent, i.e., $(n_k \notcind x_i - G_i(M))\land (n_k \notcind x_j - G_j(N)) \Rightarrow x_i -  G_i(M)\notcind x_j - G_j(N)$.

\section{Proofs}\label{appendix:proof}
\subsection{Proof of Lemma~\ref{lemma:visible_edge_implication}}
If Eq.~(\ref{eq:visible_edge_implication_1}) is satisfied, it can be shown that, for each $k_q$, there is at least a BP/CP not blocked by the set $X \setminus \{x_{k_q}\}$, by viewing $x_{k_q}$ as an unobserved variable and applying Lemma~\ref{lemma:invisible}. 

When Eqs.~(\ref{eq:visible_edge_implication_2}) and~(\ref{eq:visible_edge_implication_3}) are satisfied, $x_j$ is a parent of $x_i$ and there is no BP/CP not blocked by the set $X$, due to Lemma~\ref{lemma:visible_parent}. Thus, for each $k_q$, there is a UBP/UCP relative to the set $X\setminus \{x_{k_q}\}$ that is blocked by $x_{k_q}$. This means that $x_{k_q}$ is a parent of $x_j$ or a parent of $x_i$. 

\subsection{Proof of Lemma~\ref{lemma:visible_non_edge_implication}}
If Eq.~(\ref{eq:visible_edge_implication_1}) is satisfied, for each $k_q$, there is at least a BP/CP not blocked by the set $X \setminus \{x_{k_q}\}$, by viewing $x_{k_q}$ as an unobserved variable and applying Lemma~\ref{lemma:invisible}. 

When Eq.~(\ref{eq:visible_non_edge_implication_1}) is satisfied, $(x_i,x_j)$ is a non-edge and there is no BP/CP not blocked by the set $X$, due to Lemma~\ref{lemma:visible_non_edge}. Thus, for each $k_q$, there is a UBP/UCP relative to the set $X\setminus \{x_{k_q}\}$ that is blocked by $x_{k_q}$. This means that $x_{k_q}$ is a parent of $x_j$ or a parent of $x_i$.  

\subsection{Proof of Lemma~\ref{lemma:y_structure}}
We prove by contradiction.
\begin{enumerate}
\item Suppose there is a backdoor path $x_i \leftarrow \cdots \leftarrow v \rightarrow \cdots \rightarrow x_j$. Since $x_k$ is an ancestor of $x_i$, the path $x_k \rightarrow \cdots \rightarrow x_i \leftarrow \cdots \leftarrow v \rightarrow \cdots \rightarrow x_j$ exists. By conditioning on $x_i$, which is a collider on this path, the path is open and thus $x_k$ and $x_j$ cannot be independent. This contradicts the assumption $x_k \cind x_j \mid x_i$.
\item Suppose $x_j$ is an ancestor of $x_i$. Since $x_k$ is an ancestor of $x_i$, the path $x_k \rightarrow \cdots \rightarrow x_i \leftarrow \cdots \leftarrow x_j$ exists. By conditioning on $x_i$, which is a collider on this path, the path is open and thus $x_k$ and $x_j$ cannot be independent. This contradicts the assumption $x_k \cind x_j \mid x_i$. 
\end{enumerate}

\section{Step-by-step Execution of CAM-UV on Figs.~\ref{fig:illustrative_1}a and~b}\label{appendix:cam_uv_step_by_step}
We work out step-by-step the execution of the CAM-UV algorithm for the graphs in Figs.~\ref{fig:illustrative_1} a and~b. We assume that 1) all independence tests are correct, and 2) Line 10 of Algorithm~1 of CAM-UV can correctly find a sink of the set $K$.  
\subsection{Fig.~\ref{fig:illustrative_1}a}
\begin{itemize}
\item Algorithm~1:
\begin{itemize}
\item Phase 1:
\begin{itemize}
\item $t = 2$: The sets $\{x_1,x_2\}$, $\{x_1,x_3\}$, and $\{x_2,x_3\}$ are considered as $K$. The candidate sink $x_b$ of $K$ is searched. 
\begin{itemize}
\item $K = \{x_1,x_3\}$ or $K = \{x_2,x_3\}$: Since these pairs are invisible,  $x_b-G_1(M_b\cup K\setminus \{x_b\})$ and $x_j-G_2(M_j)$ for $j\in K\setminus \{x_b\}$ are not independent, regardless of $x_b$. Thus, line 15 will fail since $e \le \alpha$. There is no change in $M_i$.
\item $K = \{x_1,x_2\}$: CAM-UV correctly finds $x_b = x_2$. However, $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_2 - G_1(x_1)$ and $x_j-G_2(M_j) = x_1$ is not independent, due to the unblocked backdoor path $x_1 \leftarrow U_1 \rightarrow x_3 \rightarrow x_2$. Thus, line 15 will fail since $e \le \alpha$. There is no change in $M_i$.
\end{itemize}
$M_i$ remains empty for each $i$ and $t$ increases to $3$.
\item $t = 3$: $K = \{x_1,x_2,x_3\}$. The algorithm correctly chooses $x_b = x_2$. However, $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_2 - G(x_1,x_3)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_2 \rightarrow x_2$. Therefore, line 15 will fail again, since $e \le \alpha$.
\end{itemize}
$M_i$ remains empty for each $i$. Phase 1 of the Algorithm~1 ends.
\item Phase 2: Since $M_i$ is empty for each $i$, Phase~2 ends. 
\end{itemize}
Algorithm~1 ends with every $M_i$ being empty.
\item Algorithm~2: For each pair $(i,j)$, line 5 is satisfied. Therefore, the algorithm concludes every pair is invisible. CAM-UV ends.
\end{itemize}

The final output is an adjacency matrix where each off-diagonal element is NaN.

\subsection{Fig.~\ref{fig:illustrative_1}b}
\begin{itemize}
\item Algorithm~1:
\begin{itemize}
\item Phase 1:
\begin{itemize}
\item $t = 2$: The sets $\{x_1,x_2\}$, $\{x_1,x_3\}$, $\{x_1,x_4\}$, $\{x_2,x_3\}$, $\{x_2,x_4\}$, and $\{x_3,x_4\}$ are considered as $K$. The candidate sink $x_b$ of $K$ is searched. 
\begin{itemize}
\item $K = \{x_1,x_2\}$: Regardless of which $x_b$ is, $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_b-G_1(\{x_1,x_2\}\setminus \{x_b\})$ and $x_j-G_2(M_j) = x_j$ for $j\in K\setminus \{x_b\}$ are not independent, since there are unblocked BPs/CPs when $x_3$ and $x_4$ is not added to the regression. Thus, line 15 will fail since $e \le \alpha$. There is no change in $M_i$.
\item The remaining pairs are all invisible. Therefore, $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_b - G_1(K\setminus \{x_b\})$ and $x_j-G_2(M_j)$ is not independent. Thus, line 15 will fail since $e \le \alpha$. There is no change in $M_i$.
\end{itemize}
$M_i$ remains empty for each $i$ and $t$ increases to $3$.
\item $t = 3$: 
\begin{itemize}
\item $K = \{x_1,x_2,x_3\}$: 
\begin{itemize}
\item $x_b = x_2$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_2 - G(x_1,x_3)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$. 
\item $x_b = x_1$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_1 - G(x_2,x_3)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_1 \leftarrow U_1 \rightarrow x_3$. Therefore, line 15 will fail, since $e \le \alpha$. 
\item $x_b = x_3$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_3 - G(x_1,x_2)$ and $x_j-G_2(M_j) = x_2$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$. 
\end{itemize}

\item $K = \{x_2,x_3,x_4\}$: 
\begin{itemize}
\item $x_b = x_2$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_2 - G(x_3,x_4)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$. 
\item $x_b = x_3$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_3 - G(x_2,x_4)$ and $x_j-G_2(M_j) = x_2$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$. 
\item $x_b = x_4$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_4 - G(x_2,x_3)$ and $x_j-G_2(M_j) = x_2$ is not independent, due to the unobserved backdoor path $x_2 \leftarrow U_3 \rightarrow x_4$. Therefore, line 15 will fail, since $e \le \alpha$. 
\end{itemize}
\item $K = \{x_1,x_3,x_4\}$:
\begin{itemize}
\item $x_b = x_1$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_1 - G(x_3,x_4)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_1 \rightarrow x_1$. Therefore, line 15 will fail, since $e \le \alpha$. 

\item $x_b = x_3$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_3 - G(x_1,x_4)$ and $x_j-G_2(M_j) = x_1$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_1 \rightarrow x_1$. Therefore, line 15 will fail, since $e \le \alpha$. 
\item $x_b = x_4$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_4 - G(x_1,x_3)$ and $x_j-G_2(M_j) = x_1$ is not independent, due to the unobserved causal path $x_1 \rightarrow U_2 \rightarrow x_4$. Therefore, line 15 will fail, since $e \le \alpha$. 
\end{itemize}
\end{itemize}
$M_i$ remains empty for each $i$ and $t$ increases to $4$.

\item $t = 4$:
\begin{itemize}
\item $K = \{x_1,x_2,x_3,x_4\}$.

\begin{itemize}

\item $x_b = x_1$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_1 - G(x_2,x_3,x_4)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_1 \rightarrow x_1$. Therefore, line 15 will fail, since $e \le \alpha$. 

\item $x_b = x_2$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_2 - G(x_1,x_3,x_4)$ and $x_j-G_2(M_j) = x_3$ is not independent, due to the unobserved backdoor path $x_3 \leftarrow U_4 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$.

\item $x_b = x_3$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_3 - G(x_1,x_2,x_4)$ and $x_j-G_2(M_j) = x_1$ is not independent, due to the unobserved backdoor path $x_1 \leftarrow U_1 \rightarrow x_3$. Therefore, line 15 will fail, since $e \le \alpha$.

\item $x_b = x_4$: $x_b-G_1(M_b\cup K\setminus \{x_b\}) = x_4 - G(x_1,x_2,x_3)$ and $x_j-G_2(M_j) = x_2$ is not independent, due to the unobserved backdoor path $x_4 \leftarrow U_3 \rightarrow x_2$. Therefore, line 15 will fail, since $e \le \alpha$.

\end{itemize}
\end{itemize}
\end{itemize}

$M_i$ remains empty for each $i$. Phase 1 of the Algorithm~1 ends.
\item Phase 2: Since $M_i$ is empty for each $i$, Phase~2 ends. 
\end{itemize}
Algorithm~1 ends with every $M_i$ being empty.
\item Algorithm~2: For each pair $(i,j)$, line 5 is satisfied. Therefore, the algorithm concludes every pair is invisible. CAM-UV ends.
\end{itemize}

The final output is an adjacency matrix where each off-diagonal element is NaN.

\section{Experiments with ER graphs}\label{appendix:ER_experiment}
We generated random graphs from the ER model with 10 observed variables and edge probability $0.2$. We randomly selected 20 pairs of observed variables and introduced a hidden confounder between each pair, and another 20 pairs of observed variables and add a hidden intermediate variable between each pair. We generated $50$ random graphs in this way. For each random graph, we generated $10$ datasets with the same data generating process as in Section~\ref{sec:experiment_illustrative}. Each dataset contains $500$ samples. We ran the algorithms with confidence level $\alpha = 0.05$, $0.1$, and $0.2$.

\textbf{Metrics.}
We define the metrics used in the experiments. $TP$ is the number of true positives. $TN$ is the number of true negatives. $FN$ is the number of false negatives. $FP$ is the number of false positives. In identifying the adjacency matrix $A$, for the case when the estimated $A(i,j)$ is NaN, we add $0.5$ to \emph{both} $FN$ and $FP$. 

The $precision$, $recall$, and $F_1$ are calculated as follows. $Precision$ is $TP/(TP + FP)$. $Recall$ is $TP/(TP + FN)$. $F_1$ is $2 * (precision * recall) / (precision + recall)$.

\textbf{Identifying the adjacency matrix.}  
The results are shown in Fig.~\ref{fig:ER_exp_adjacency}. CAM-UV-X is better than CAM-UV-X in all metrics.

\begin{figure}[!h] 
\centering
\includegraphics[width = 0.5\columnwidth]{ER_adjacency.png}
\caption{Performance of identifying the adjacency matrix in ER random graphs.}\label{fig:ER_exp_adjacency}
\end{figure}

\textbf{Identifying ancestor relationships.}
The results are shown in Fig.~\ref{fig:ER_exp_ancestor}. While precision and $F_1$ are similar, CAM-UV-X achieved slightly higher recall.

\begin{figure}[!h] 
\centering
\includegraphics[width = 0.5\columnwidth]{ER_ancestor.png}
\caption{Performance of identifying ancestors in ER random graphs.}\label{fig:ER_exp_ancestor}
\end{figure}



\end{document}
