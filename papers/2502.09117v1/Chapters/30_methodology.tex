\section{Methodology}
\label{sec:methodology}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{Figures/methodology_NodeRED.pdf}
\caption{Methodology of the conducted study.}
\label{fig:methodology}
\end{figure}


Figure~\ref{fig:methodology} shows the methodology of the conducted work, structured into five steps.
They are presented in detail below.






\subsection{Crawling of the Node-RED library (step \textbf{{\small\Circled{1}}})}
\label{sub:crawling_methodology}

As first step in the performed study, the Node-RED library was crawled to retrieve the source code of all currently listed node packages listed.
A custom script was implemented that scrapes all node package identifiers from the library's official website~\footnote{\url{https://flows.nodered.org/search?type=node}}.
At the time of the analysis, September 10th 2024, 5051 node packages were listed.
The script also retrieves each node package's number of downloads within the last week, which is used to rank their popularity in later steps, the same measure as Node-RED uses.
The analysis script then retrieved the source code of 5032 of the listed node packages from the npm package manager.
The other 19 node packages (0.4\% of the library) had broken download links.
From the downloaded node packages, 133 (2.6\% of the library) were excluded because they did not contain any nodes.
Instead, they contain other content related to Node-RED, such as customizations to change the appearance of the development dashboard or tools that support users in the development.
A further 101 node packages (2.0\% of the library) were removed because they did not contain parsable specifications.
After these steps, the source code and specifications of 4798 valid node packages remained (95.0\% of the library).


The valid node packages contained 17603 individual nodes. 
Figure~\ref{fig:nodes_per_package_distribution} shows the distribution of the number of nodes per node package, excluding the 10 most extreme outliers because they skew the graph.
Four of these outliers are node packages containing more than 100 individual nodes (507, 195, 137, and 134 nodes), the other six range between 61 and 96 nodes.
Most of the valid node packages (2035) contain a single node.




\begin{figure}[h]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/node_numbers.pdf}
        \caption{Distribution of nodes per node package. Not showing the ten highest outliers because they skew the graph.}
        \label{fig:nodes_per_package_distribution}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/LOCs_per_node.pdf}
        \caption{Distribution of the size of nodes (in LOC). Not showing the 100th percentile (51 samples) because it skews the graph.}
        \label{fig:LOCs_per_node}
    \end{minipage}
\end{figure}



Figure~\ref{fig:LOCs_per_node} shows the distribution of the size of node packages in lines of code (LOC).
The calculation of LOC only considered the .js, .ts, and .html files, i.e., those files that can contain functional code.
The figure does not show the highest percentile of values, since they would skew the graph. 
The largest node package has a size of 148397 LOC. 
Another outlier has 114110 LOC, the remaining 49 values in the 100th percentile are below 100000 LOC.
As shown in the plot, the majority of node packages contains up to 2500 LOC. 

For later use in the conformance analysis, the node packages' specifications were parsed by the analysis script to retrieve the numbers of specified inputs and outputs of each node.
For this, the HTML files in each node package were searched for the parameters \textit{inputs} and \textit{outputs}, considering all different formats of how these parameters are defined throughout the library.






\subsection{Identification of nodes' endpoints (step \textbf{{\small\Circled{2}}})}
\label{sub:endpoint_identification_methodology}

As preliminary for the CodeQL analysis and subsequent conformance analysis, a CodeQL query was required that captures the sources and sinks that CodeQL should consider in its information flow analysis.
The query needed to be created manually, since the endpoints are specific to the Node-RED ecosystem and no such resource could be found in the related literature, Node-RED documentation, or other sources.

To this end, we selected a subset of all valid node packages and manually analyzed their source code to identify sources and sinks of information flows.
The sample size of nodes is chosen with a sample size calculator such that the margin of error is below 10\% with a confidence level of 95\%.
The comparatively high margin of error is tolerated due to the high required manual effort, as indicated by the size of individual node packages shown in Figure~\ref{fig:LOCs_per_node}.
To further strengthen the validity of this step, a saturation criterion was defined: after the initial set of node packages, further ones have to be analyzed until no new sources or sinks are encountered within the last 10 node packages.

Based on the above parameters and the size of the Node-RED library at the time of analysis, there were 97 node packages to be analyzed.
From our experience of the variability of the source code of nodes in the ecosystem, this number is sufficiently high to create a comprehensive CodeQL query.

The samples to be analyzed should cover both the major node packages that are most often used in Node-RED flows and which are often developed and maintained by a team of experienced developers over a long time, as well as less common nodes that are created by independent developers with possibly less rigor.
Therefore, half of the sample were selected from the most popular nodes based on the number of downloads within the last week (this metric is reported and used for ranking by the Node-RED library), and the other half as random node packages from the complete library of valid node packages.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/saturation_plot.pdf}
    \caption{Saturation of manually detected sources and sinks in the analyzed node packages. 100\% $\hat{=}$ 127 endpoints.}
    \label{fig:saturation_plot}
\end{figure}

For the analysis, all JavaScript, TypeScript, and HTML files in the node packages were manually examined.
We identified all methods, variables, and functions in the code that serve as a source or sink (see Section~\ref{sub:codeql}).
Figure~\ref{fig:saturation_plot} visualizes the saturation progression of this process.
It shows the cumulative number of manually detected sources and sinks against the number of analyzed node packages.
As can be seen, 90\% of the overall sources and sinks were identified in the first 60 node packages, and the formulated saturation criterion was met by analyzing the initial 97 node packages already.
Specifically, no new sources or sinks were encountered within the last 20 analyzed node packages.
Thus, no further samples were analyzed afterwards.
We deduce from the process and the saturation plot that the formulated saturation criterion seems applicable and that the manual analysis of 97 node packages looks to be sufficiently exhaustive to base the further conformance analysis on the resulting list of sources and sinks.

The process resulted in the identification of 59 sources and 68 sinks (127 endpoints in total).
They were then translated into a CodeQL query, following the documentation of the query language \textit{QL}~\footnote{\;\url{https://codeql.github.com/docs/}}.
To verify the correctness of the query, we ran it with information flow analysis turned off, that is, so that only the location of detected sources and sinks is reported.
For each of the 127 endpoints, we selected one of the manually identified occurrences of it in the source code and verified that CodeQL had detected this location with the created query.
This process was performed repeatedly during the creation of the query, and possible errors were fixed. 

Although this step required substantial manual effort, it does not have to be repeated if the presented conformance analysis should be repeated in the future, as long as it is reasonable to assume that the source code of node packages is comparable to the current state of the library.














\subsection{Conformance analysis (step \textbf{{\small\Circled{4}}})}
\label{sub:conformance_analysis_methodology}

The conformance analysis of all valid node packages in the Node-RED library of nodes forms the core of this article.
It is based on an information flow analysis performed with CodeQL, which takes as input the source code of the analyzed node packages (as retrieved during step {\small\Circled{1}}) and the CodeQL query file capturing the endpoints that it should detect (as created in step {\small\Circled{2}}).
The analysis is performed on the granularity of node packages instead of individual nodes (see Section~\ref{sub:node_red}).
Although an analysis of individual nodes would allow for a more detailed assessment, information flows detected by CodeQL cannot be reliably assigned to an individual node if it is contained in a node package with other nodes.
While there are clear cases where a single file belongs to each node and hence the association of a detected flow can be done via its location in the source code, there are also many node packages where such a separation is not possible.
For example, if nodes share components such as recurring functionality in custom libraries. 
If an information flow were to be detected in such a file, an additional analysis would have to identify all nodes that access this file and check if this access can trigger the information flow.
The added information gain would not merit the reduced reliability introduced by the additional complexity.
Since the nodes in a node package are created by the same developer or group of developers, we can assume that their level of conformance between implementation and specification should be similar for all nodes in the package.
Therefore, an analysis on the level of node packages is deemed the most precise option.

To perform the CodeQL analysis, each node package was transformed into a CodeQL database, and the created query was applied to each database.
These steps were realized with custom scripts that execute the commands as indicated in the official CodeQL documentation on each node package.
The outcome was an individual CodeQL result file per node package indicating all possible information flows within the node package.

The duplicate endpoints were then merged.
Since CodeQL considers all possible information flows between endpoints, it can detect multiple flows that have the same source and the same sink, for instance, if they differ in a hard-coded input parameter.
In addition, flows can have different sources but the same sink or vice versa. 
Since the argumentation in this article is on the level of individual endpoints, such duplicates had to be resolved to identify only distinct endpoints.
The merging was performed with a custom script that considers the file, line number, and method, function, or variable that form the endpoint to clearly identify the distinct endpoints.
After merging duplicate endpoints, the number of distinct sources and sinks per node package was counted.

Finally, the number of identified sources and sinks was compared against the number of specified inputs and outputs as parsed from the specifications in step {\small\Circled{1}}.
Based on this comparison, the conformance case (convergence, absence, or divergence, see Section~\ref{sub:conformance_analysis}) of each node package was determined and some statistics such as the exact number of additional or missing endpoints were calculated.




\subsection{Evaluation of CodeQL's correctness (step \textbf{{\small\Circled{3}}})}
\label{sub:evaluation_methodology}

To evaluate the accuracy of the analysis pipeline used, we manually checked the correctness of a subset of the information flows that CodeQL detected.
The information flow analysis with CodeQL is the only part of the conducted conformance analysis that is subject to major threats to its validity, due to the difficult nature of such an analysis and the large impact that the quality of the created CodeQL query has on it.
All other steps (retrieving the library of node packages, parsing the specifications, and comparing the specifications against the detected information flows) are simple and robust, and their validity was easily verified during their implementation.

To evaluate CodeQL's accuracy, we selected a subset of all valid node packages following the same process as for the creation of the CodeQL.
We ensured that no node package that had been used to create the query was used for this evaluation of CodeQL as well, since this could thwart the results.
All information flows detected by CodeQL in the thus selected 97 node packages were manually checked for correctness.
Specifically, the locations of the source and sink of each flow reported by CodeQL were examined, and it was checked whether a control flow is possible during the node's execution in which information from the source propagates to the sink.

CodeQL detected 964 information flows across the 97 analyzed node packages.
The manual checking yielded 951 true positives and 13 false positives, resulting in a precision of 0.99.
Considering the chosen margin of error, the precision is seen as sufficiently high to draw conclusions about the Node-RED ecosystem from our conformance analysis based on the evaluated CodeQL information flow analysis.
The false positives detected were caused mainly by ambiguous variable names, e.g., a variable called ``key'' which refers to a key in a dictionary instead of a secret.

Note that while checking for additional information flows not detected by CodeQL would be desirable, such an analysis is extremely labor intensive and error-prone and out of the scope of this work -- compare the sizes of node packages reported in Figure~\ref{fig:LOCs_per_node}.
We hope for the emergence of a dataset from future work that could serve as the ground truth for a study such as the reported one, thus allowing for an evaluation of CodeQL's recall.
Currently, only its precision can be evaluated by us.
Due to this, we cannot make sound statements about absence cases in the conformance analysis and therefore focus on divergence cases in the discussion of the results.








\subsection{Risk assessment (step \textbf{{\small\Circled{5}}})}
\label{sub:risk_assessment_methodology}

Since not all detected divergences necessarily pose a security risk, we assessed the impact of the identified divergent information flows on the security of users.
To this end, the risk associated to a subset of the information flows detected by CodeQL was evaluated.
While the conformance analysis was performed on the level of endpoints, the risk assessment needed to consider complete information flows, to take into account the sensitivity of the transferred data.

The same set of node packages as for the evaluation of CodeQL's correctness (step \Circled{4}) was used for this assessment because these information flows' correctness had been manually verified.
The node packages were classified as different conformance cases.
The selection was not restricted to divergent node packages because it is not possible to distinguish between the information flows within a node that had been considered by its specifications and those that are divergent.
Therefore, information flows from any node package are suitable for this process.

For the risk assessment, the CodeQL results of each sample were analyzed manually, and all detected information flows were classified by the action they perform and the type of data that they send.
In this context, an action can be displaying an error message, writing to a file, sending a message to another node, etc..
The grouping by type of data sent is done via its sensitivity.

The first author performed the classification following an inductive approach, where each assessed information flow was assigned to an existing group if both the performed action and the type of transferred data matched the group, and where a new group was created whenever no suitable one had yet been created.
After assigning each flow to a group, the list was revised by merging groups that could be generalized without loosing specificity about the induced risk, e.g., if the performed action of two groups were different but comparable from the perspective of a risk assessment.
Resulting from this analysis process was a list of groups and a distribution of the analyzed information flows across them.
A severity level was then assigned to each group in a discussion with three authors, indicating the risk introduced by each group of information flows.
The factors influencing the assessment of the severity were: (i) the sensitivity of the transferred data, (ii) the context to which data is exposed (local or outside), and (iii) the possible impact of exposed data on the control flow of the program.
We assumed an attacker model where only the flow development process with the Node-RED framework is confidential, and all other contexts could potentially be observed by an attacker.
We believe this to be reasonable, since the developed programs are deployed on IoT devices that usually run autonomously without supervision and can also be located so that physical access is not controlled.
The assessment followed a worst-case classification in cases of ambiguity where the risk depended on the deployment context.

The classification was performed for all information flows detected by CodeQL, before they are merged in the later stages of the conformance analysis.
We did so to not lose information due to the merging.
If, for example, CodeQL detects two flows for two different information objects from the same input to the same output, these are merged into one in the conformance analysis. 
However, the two information objects can be of different sensitivity and therefore have different security implications when exposed.
Performing the risk assessment before the merging thus provides more accurate results.






\subsection{Threats to validity}
\label{sec:threats}

The conclusions drawn from the results presented in this work are subject to some threats to validity.
We present possible limitations and how we mitigated them in the following.


\subsubsection{Internal}
Most of the results presented are based on the work of one individual researcher, which introduces the risk of researcher bias and errors during the analysis.
We addressed this threat to validity by discussing the critical parts and intermediate results of the analysis process with the other authors.
A second limitation is introduced by the exploratory nature of the conducted study, for which no existing ground truth could be used to validate the correctness of the non-conformance analysis.
As a mitigation, we manually checked a subset of CodeQL's results, which is both the most crucial and most complex part of the analysis.
The important CodeQL query also had to be created by us and was repeatedly checked for its correct capturing of all manually identified sources and sinkd in the analyzed node packages.
The results of the CodeQL evaluation and the iterative creation of the query indicate, that the analysis pipeline functions as intended, as far as we could check it in the given context.






\subsubsection{External}
Since we analyzed the complete population of the investigated domain (i.e., the complete library of Node-RED node packages), the generalization of the presented results is not an issue. 
However, the results should not be generalized to other domains such as other IoT development frameworks without verifying a proper comparability.


\subsubsection{Construct}
Conformance analysis is a suitable tool to investigate compliance between different system artifacts and is commonly used for this purpose.
Therefore, we consider the threat of misinterpreting the very definition of convergence, divergence, and absence.
The scope of our study is analyzing the compliance between the design-level model (HTML specification) and implementation-level model (data flow graph).
In this context (to the best of our knowledge) our interpretations are in-line with the existing literature.
Using the nodes' specifications as comparator for the information flows detected by CodeQL might not be suitable for other analyses because they are developer-created and therefore not reliable, however, this is exactly the mismatch we aimed to investigate. 
Consequently, it is seen as fitting in our case.

Using static analysis might not be suitable for cases where configurations influence how nodes behave at runtime.
Still, it is considered the best tool for the conducted investigation, as such cases should be rare.



