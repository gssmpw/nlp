

\section{Introduction}
\label{section:intro}

LLMs have recently emerged with extraordinary capabilities \cite{waswani2017attention,lewis2020retrieval, ahn2022can, hadi2023large} and have rapidly become integral to numerous fields, transforming everyday tasks such as text generation \cite{touvron2023llama,chiang2023vicuna,jiang2023mistral, achiam2023gpt}, image generation \cite{saharia2022photorealistic, nichol2021glide}, and complex decision-making tasks \cite{topsakal2023creating, wu2023autogen}. Despite their advantages, the widespread deployment of LLMs has unveiled critical security vulnerabilities \cite{perez2022ignore, wan2023poisoning}, making them susceptible to involuntary utilization in cyber-attacks and other malicious activities \cite{fang2024llm, yao2024survey}.

A common strategy to enhance the robustness of LLMs is alignment training, i.e., adjusting the model’s outputs to adhere to desired safety and ethical standards \cite{shen2023large, wu2024meta, wang2023aligning, lee2023rlaif}. The alignment process distinguishes between harmless prompts that the model should comply with and harmful prompts that it should refuse \cite{glaese2022improving,wang2020understanding}, effectively segmenting the input space into \emph{Compliance} and \emph{Refusal} subspaces \cite{yu2024enhancing}. However, this segmentation has inadvertently fueled adversarial jailbreak attacks that transform inputs to coerce models into providing compliance outputs \cite{marshall2024refusal,baumann2024universal,huang2024stronger,yu2024robust}. Jailbreaks then denote such successful manipulation of models, and attacks demonstrate significant $ASR$ against alignment training methods \cite{chao2023jailbreaking,deng2023jailbreaker}. 

Among these, gradient-based jailbreak attacks achieve high $ASR$ and undermine the security of LLMs. They utilize models' gradients to systematically transform textual prompts, coercing them to comply with harmful instructions \cite{zou2023universal,liu2023autodan,zheng2024jailbreaking}. Moreover, such attacks present significant $ASR$ when transferred across models, i.e., with only query-based access to attacked models. As such, the robustness of LLMs and corresponding alignment mechanisms are evaluated via similar methods \cite{mazeika2024harmbench,zhou2024defending}. Despite their effectiveness, these attacks are computationally expensive and require complex optimization of the latent output space over increasingly growing model sizes \cite{tao2024scaling,dubey2024llama}.

In addition, embedding-based jailbreak attacks have introduced new vulnerabilities by directly manipulating inputs' embedding to expedite the attack process \cite{yu2024enhancing,arditi2024refusal}. Such approaches leverage the alignment-induced distinction in the latent space and require significantly fewer computational resources. However, they are often inapplicable in realistic scenarios where attackers cannot access the model’s internal mechanisms. 



% Integrating insights from these alignment vulnerabilities into textual jailbreak attacks within initialization strategies represents a pivotal advance that could address the computational burden while transforming latent vulnerability insights into a practical, efficient seed for adversarial success in jailbreak methodology.



% Although gradient-based approaches require substantial computational resources, alignment inadvertently introduces a critical vulnerability \cite{marshall2024refusal,baumann2024universal,huang2024stronger,yu2024robust} that embedding-based approaches have begun to exploit \cite{arditi2024refusal,yu2024enhancing}. These approaches leverage the alignment-induced demarcation between \emph{compliance} and \emph{refusal} in the latent space. Integrating insights from these alignment vulnerabilities into textual jailbreak attacks within initialization strategies represents a pivotal advance that could address the computational burden while transforming latent vulnerability insights into a practical, efficient seed for adversarial success in jailbreak methodology.

% Nonetheless, the success of such methods poses the question of whether inputs' embedding can be leveraged to enhance optimization over the textual domain.

% This underscores a critical gap: the need for a scalable and attack-agnostic initialization framework that can operate effectively within the textual domain.

\input{vis/fig_demo}

% Our framework is attack-agnostic and leverages the full optimization 

This work introduces $CRI$, a novel initialization framework designed to leverage pre-trained jailbreak prompts to initialize attacks over unseen prompts. With this intention, we consider the embedding-space \emph{refusal direction} defined by \citet{arditi2024refusal} as the average difference between compliance and refusal prompt embeddings. In the context of gradient-based attacks, this direction corresponds to the prompt's transformation over the optimization process, which we denote as jailbreak transformations ($JT$). Therefore, utilizing precomputed $JT$ as initializations shortens the distance to jailbreak, enhancing $ASR$ while reducing computational overhead.


% enhancing attacks $ASR$ and reducing the computational overhead.


% and decreases the computational overhead.

% We, therefore, utilize successful transformation as a more informative initialization of jailbreak attacks. 


% Old if approved delete:
%===================================================
%This work introduces $CRI$, a novel initialization framework designed to leverage successful jailbreak prompts to initialize attacks over unseen prompts. With this intention, we consider the embedding-space \emph{refusal direction} defined by \citet{arditi2024refusal} as the average difference between compliance and refusal prompt embeddings. In the context of gradient-based attacks, this direction corresponds to the prompt's transformation over the optimization process\amit{Hard to understand the meaning}, which we denote as jailbreak transformations ($JT$). We, therefore, utilize successful transformation as a more informative initialization of jailbreak attacks. 

%===================================================


In \cref{fig:intro_fig}, we present a $2D$ visualization of harmful prompts, comparing $CRI$ and standard initialization and showcasing their effect on proceeding jailbreak attacks. $CRI$ presents a more informative initialization and reduces the number of steps to jailbreak. We continue to discuss this figure in \cref{appendix:intro_fig_explanation}. Additionally, in \cref{fig:framework}, we provide an example comparing our proposed $CRI$ with standard initialization and the resulting text generation by the model. We denote the initializations as the first step in each attack and the clean prompts as the zeroth step. In this example, $CRI$ achieves jailbreak in two optimization steps, while standard initialization requires $102$ steps. Our framework is attack-agnostic, pre-trained once for a given model and jailbreak attack, and can be integrated over any gradient-based jailbreak approach. Below, we outline our main contributions.  




% In \cref{fig:intro_fig}, we present a $2D$ visualization of $CRI$ and standard initialization over harmful prompts and their effect on proceeding jailbreak attacks.\avi{The plot is divided into the refusal region and the Compliance region, and as expected, most of the clean harmful prompts, are located at the refusal region. Standard initialization does not help move many prompts to the compliance region, so further attack steps are required to achieve that goal. Thus,} $CRI$ presents a more informative initialization and \avi{so it shifts a significant number of prompts to the compliance region and} reduces the number of steps to jailbreak \avi{for the rest}. We continue to discuss this figure in \cref{appendix:intro_fig_explanation}. Additionally, in \cref{fig:framework}, we provide an example comparing our proposed $CRI$ with standard initialization and the resulting text generation by the model. We denote the initializations as the first step in each attack and the clean prompts as the zeroth step. In this example, $CRI$ achieves jailbreak in a single optimization step, while standard initialization requires $101$ steps. Our framework is attack-agnostic, pre-trained once for a given model and jailbreak attack, and can be integrated over any gradient-based jailbreak approach. Below, we outline our main contributions.  








% We denote the initializations as the first attack step, 


% where $CRI$ achieves jailbreak in a single step. We then denote the initializations as the first step in the attack

% how initialization influences harmful prompts, shifting them from one output region to another while also reducing the number of steps required for jailbreak attacks (e.g., $GCG$), as illustrated in \cref{fig:intro_fig}. Additionally, we provide an example comparing our proposed $CRI$ with standard initialization in \cref{fig:framework}, demonstrating that $CRI$ enables jailbreak in a single gradient step, where step one corresponds to an empty attack in this work. Our framework is attack-agnostic, requiring only a one-time pre-training for a given model and jailbreak attack method, and can be seamlessly integrated into any gradient-based jailbreak approach.

% Below, we outline our main contributions.  
 

% Old if approved delete:
%===================================================
%In \cref{fig:intro_fig}, we present the $2D$ visualization of $CRI$ compared to standard initialization and show that it effectively shortens the distance to jailbreaks. In addition, \cref{fig:framework} presents an example of $CRI$ compared to standard initialization, where $CRI$ achieves jailbreak in a single step. Our framework is attack-agnostic, pre-trained once for a given model and jailbreak attack, and can be integrated over any gradient-based jailbreak approach. Below, we outline our main contributions.  
%===================================================

% Unlike existing methods, our framework is universal, pre-trained once for any model, and attack agnostic, enabling seamless integration with any greedy gradient-based jailbreak approach. By redefining the Refusal-Compliance relationship, CRI advances low attack loss criteria during the initialization stage  \cref{fig:intro_fig}, providing such initializations which transfer harmful prompts from the Refusal region to the Compliance region. This effectively shortens the attack path, reduces the required number of attack steps to success  \cref{fig:intro_fig}, significantly lowers the computational burden associated with attack initialization, and enhances both scalability and success rates. 

\begin{itemize}
    \item We propose $CRI$, an attack-agnostic initialization framework utilizing individual or universal attacks to pre-train jailbreak attacks' initializations.
    \item We integrate our method into greedy and genetic optimization schemes, as demonstrated by $GCG$ \cite{zou2023universal} and $AutoDAN$ \cite{liu2023autodan}.
    \item We evaluate our method on these aforementioned works across multiple models, showing that $CRI$ enhances $ASR$ while reducing computational overhead.
\end{itemize}  
    % \item We consider two approaches to $CRI$ corresponding to different interpretations of the embedding refusal direction in the textual domain. The first considers a set of $JT$ and directly initializes new attacks via these transformations. The second approach utilizes a multi-prompt universal attack to generate a single $JT$, enhancing its generalization to unseen prompts.
    % \item We propose an attack-agnostic initialization framework and adapt it to the $GCG$ \citet{zou2023universal} and $AutoDAN$ \citet{liu2023autodan} attacks, representing common gradient-based jailbreak approaches. Our method initializes the greedy algorithm in $GCG$ and reference prompts in $AutoDAN$ via $CRI$. We demonstrate that $CRI$ improves $ASR$ while reducing computational overhead compared to standard initialization.
    % \item We examine the generalization properties of $JT$s to unseen prompts and show that they can successfully produce jailbreaks without further optimization. Moreover, with only several optimization steps, attacks achieve nearly $100\%$ $ASR$ in multiple settings. This indicates the similarity of jailbreak attacks across different prompts and entails that current models are not sufficiently robust to such attacks.
    % Finally, we leverage our findings to contribute to broader efforts in uncovering alignment vulnerabilities in jailbreak defenses \rom{alignment defences?}, providing critical insights to improve the robustness of alignment training methods.  

The rest of the paper is organized as follows: \cref{section:bground_related} discusses the gradient-based jailbreak attack setting and related works, \cref{section:method} describes our proposed method, \cref{section:experiments} provides our experimental results, \cref{Discussion} concludes the paper, and \cref{section:Ethics} discusses the ethical concerns and broader impact of our work. 

%\input{vis/fix_hist}


