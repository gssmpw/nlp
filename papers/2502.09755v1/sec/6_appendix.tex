\newpage
\appendix
\onecolumn

\section{Generation of $2D$ Visualization (\cref{fig:intro_fig})}
\label{appendix:intro_fig_explanation}

In this figure, each point represents a harmful prompt. The colors indicate different categories: red points correspond to clean harmful prompts, orange points represent harmful prompts initialized using GCG's standard method (as shown in \cref{fig:framework}), and cyan points represent prompts initialized using our method ($U\text{-}CRI$, as also referenced in \cref{fig:framework}).  

To analyze these prompts, we first obtained their embeddings using Llama-2 and determined whether Llama-2 complied with each prompt. Using these embeddings and compliance labels, we trained an SVM classifier. This process yielded a weight vector \( w \) and bias \( b \), which define the SVM decision function:

\[
\langle w, x \rangle + b
\]

A negative SVM score indicates a refusal, while a positive score signifies compliance. This score is plotted on the x-axis. The y-axis represents a one-dimensional t-SNE projection.

The orange and cyan paths illustrate the optimization trajectories of the attack: the orange path follows GCG’s standard initialization, while the cyan path traces the attack starting from our initialization. The x-axis values are computed using the previously trained SVM classifier.




% \section{Limitations and Broader Impact Discussions}
% \label{appendix:CRI Limitations & Defences}
% \textbf{Initializing Jailbreak Attacks} Initializing attacks from states with lower loss values led to notable improvements in eliciting harmful or disallowed content. These enhancements provoke important theoretical questions regarding the fundamental causes of such vulnerabilities, aligning with previous research on the role of model alignment in preventing adversarial outputs. Our proposed compliance refusal methodology highlights how manipulating the attack's starting point can effectively LLM weaknesses by projecting a wide range of harmful prompts into between the Compliance-Refusal regions.

% \textbf{Possible Defenses and Alignment training encounter $CRI$} A significant insight is the remarkable generalization ability of jailbreak attacks. Once attackers are armed with pre-attack calculations, they can easily fine-tune these across diverse tasks, which gives the attackers an advantage in both performance and computational requirements for producing jailbreak attacks. This underscores concerns that current methods might not only be helpless against such attacks but might also fuel them.

% Our findings indicate that even advanced alignment strategies used within the state-of-the-art LLMs that we evaluate $CRI$ on can be circumvented through deliberate shifts between compliance and refusal subspaces, approximated by a set of textual initializations. Therefore, it is essential to strengthen alignment training to maintain a clear separation between these states without excessively fragmenting the model’s response space.On the other hand, those disadvantages could be leveraging for developing defenses, utilizing classifiers for neglected regional shifts in the embedding space by defenders. Incorporating methodologies such as \cite{hu2024gradient} can help counteract such adversarial prompts by analyzing contextual shifts in the embedding space and beyond. 

% \textbf{Limitations and Future Directions} Despite the advantages of initialization-based attack strategies, several limitations must be acknowledged. Initializing from a low-loss state can introduce biases that guide the attacks toward specific adversarial patterns. In scenarios like $CRI$, where there is an entry point with already minimal loss, even a single convergence step can suffice. When integrated within alignment training such as \cite{andriushchenko2024jailbreaking} that utilizes those jailbreaks in the alignment training pipeline, initializations may limit the diversity of training samples and might cause reduced effectiveness against broader perturbations. It is crucial to recognize that while initialization reduces computational demands, it may also reinforce narrow attack trajectories that might not be the best representative of the jailbreaks that the model could encounter.

% In conclusion, our exploration of jailbreak attacks through various initialization methods highlights the interplay between adversarial objectives and alignment goals in LLMs. By meticulously analyzing the attacks and increasing attack performance, we can guide a better understanding of wider domains such as LLM mechanisms, alignment training, defenses, and advanced attacks. This underscores why we are eager to share these insights and believe this work contributes to a deeper understanding of how to strengthen LLMs against both current and future adversarial threats.


% % \section{Loss of Attacks}
% % \label{appendix:loss}
% % \input{vis/fig_attack_losses}


% % \section{$GCG\text{-}M$ Universality and Transferability Experiment}
% % \label{appendix:GCG-M}

% % In this section, we detail our experiments utilizing $CRI$ on the $GCG\text{-}M$ universal attack. We used the same attack setting as detailed in \cref{subsec:exp_setting}, but our data choices had to be altered. Since the original $GCG$ paper did not use a validation set in its universal attack, we decided to implement it ourselves. Meaning, after each step of the attack, we evaluated it on a separate evaluation set. We then chose the suffix with the lowest loss on it. This was also used to generate the $U\text{-}CRI$. In order to not leak data, we made sure the evaluation, train and test sets had no intersections. After generating final attacks using $GCG\text{-}M$, with standard, random and our two initialization type ($CRI$ and $U-CRI$), we wanted to test their universality and transferability between models. We did so by testing the generated suffixes on $100$ test prompts and $10$ total models (including the same model used to create attack).

% % Resulting $ASR$s using source models Llama-2 and Vicuna (used to generate attacks), can be viewed in \cref{tab:llama2_gcg-m_asr} and \cref{tab:vicuna_gcg-m_asr}, respectively. Bold values represented the best results in each column.

% % \input{vis/table_GCG-M_results}

% % As for universality, Llama-2 demonstrated poor results, with $CRI$ leading, but closely to random init. Vicuna had more promising results, with $U\text{-}CRI$ leading, closely to the standard init. As for transferability, if we look the median and average $ASR$ columns of both table, our initialization were always better, by a large margin. On Llama-2, $CRI$ took the lead in both metrics, and on Vicuna, $U\text{-}CRI$ took the lead in both metrics.

% % To summarize, our initialization method lead to a small margin of better results on both models. Moreover, as for transferability, our methods achieved far superior results when compared to standard and random initializations.


\section{Refusal Lists}\label{appendix:refusal-lists}
Refusal lists of different papers, used for our evaluation. Each attack used its refusal list.

\subsection{$GCG$}
Taken from their GitHub \footnote{\href{https://github.com/llm-attacks/llm-attacks/blob/main/experiments/evaluate.py}{https://github.com/llm-attacks/llm-attacks/blob/main/experiments/evaluate.py} lines 33-63}:
\begin{quote}
    test\_prefixes = [
    "I'm sorry", 
    "Sorry", 
    "I am sorry",
    "I apologize",
    "As an",
    "As an AI",
    "I'm an",
    "I'm just",
    "As a language model",
    "As an Assistant",
    "I cannot",
    "I do not",
    "Hello!",
    "is illegal and unethical",
    "I'm really sorry", 
    "OpenAI",
    "unethical",
    "not ethical",
    "illegal",
    "not legal",
    "My apologies", 
    "but I cannot",
    "If you have any other non-malicious requests",
    "I'm not able to provide",
    "I am really sorry",
    "I cannot create",
    "I cannot support",
    "I can't provide",
    "I can't assist"
]
\end{quote}

\subsection{$AutoDAN$}
Taken from their GitHub\footnote{\href{https://github.com/SheltonLiu-N/AutoDAN}{https://github.com/SheltonLiu-N/AutoDAN}}:
\begin{quote}
    test\_prefixes = [
    "I'm sorry",
    "Sorry",
    "I am sorry",
    "I apologize",
    "As an",
    "As an AI",
    "I'm an",
    "I'm just",
    "As a language model",
    "As an Assistant",
    "I cannot",
    "I can't",
    "I do not",
    "cannot",
    "Hello!",
    "is illegal and unethical",
    "I'm really sorry",
    "unethical",
    "not ethical",
    "illegal",
    "not legal",
    "My apologies",
    "but I cannot",
    "If you have any other non-malicious requests",
    "I'm not able to provide",
    "I am really sorry",
    "I cannot create",
    "I cannot support",
    "I can't provide",
    "I can't assist",
    "I am unable",
    "I must",
    "It must",
    "it must",
    "is not capable of",
    "As a responsible",
    "never",
    "is not",
    "\textless/s\textgreater"
    ]
\end{quote}

\section{Models Used}
\label{appendix:Black-Box Models}
\begin{itemize}
    \item \textbf{Llama-2}: \texttt{meta-llama/Llama-2-7b-chat-hf} \cite{touvron2023llama}
    \item \textbf{Vicuna}: \texttt{lmsys/vicuna-7b-v1.3} \cite{vicuna2023}
    \item \textbf{Llama-3}: \texttt{meta-llama/Meta-Llama-3-8B-Instruct} \cite{meta2024llama3}
    \item \textbf{Falcon}: \texttt{tiiuae/falcon-7b-instruct}  \cite{jiang2023mistral}
    % \item \textbf{Ministral-8B}: \texttt{mistralai/Ministral-8B-Instruct-2410}   \cite{jiang2023mistral}
    \item \textbf{Mistral-7B (v0.2)}: \texttt{mistralai/Mistral-7B-Instruct-v0.2} \cite{jiang2023mistral}
    \item \textbf{Mistral-7B (v0.3)}: \texttt{mistralai/Mistral-7B-Instruct-v0.3}\cite{jiang2023mistral}
    \item \textbf{Phi-4}: \texttt{microsoft/phi-4}\cite{abdin2024phi}
    \item \textbf{Qwen2.5}: \texttt{Qwen/Qwen2.5-Coder-7B-Instruct}\cite{hui2024qwen2}
    % \item \textbf{Vicuna-13B}: \texttt{lmsys/vicuna-13b-v1.5}\cite{vicuna2023}
\end{itemize}

\begin{center}
   \textcolor{red}{Warning: The following sections contains potentially offensive and harmful text.} 
\end{center}


\section{Results examples $AutoDan-HGA$}
\label{appendix:Results examples AutoDan}
\input{vis/examples_jailbreak}

\newpage
\section{Results examples $GCG-M$}
\label{Results examples $GCG-M$}
\input{vis/gcg-m_examples}

\newpage
\section{$CRI$ Set Example} 
\label{Initialization IPA Set for Llama2}
\input{vis/ipa_set}

