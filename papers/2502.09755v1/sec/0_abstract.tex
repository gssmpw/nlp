\begin{abstract}
%\rom{change definition of jailbreak - or do not mention definition and only mention ours.}
% Jailbreak attacks on Large Language Models (LLMs) manipulate inputs to provoke models into generating targeted, often harmful outputs. These attacks coerce models to comply in their responses over malicious queries that should be refused instead. Such attacks enable the unwilling utilization of LLMs in cyber-attacks and present a vital security risk. However, while effective, optimizing jailbreak prompts requires extensive computational resources and often results in suboptimal solutions. This work introduces a novel, attack-agnostic jailbreak framework that initializes the attack closer to the 'compliance' textual output subspace. We demonstrate our approach over the mainstream GCG and AutoDAN jailbreak attacks, showing that it increases the attacks' effectiveness and significantly reduces their computational overhead.

%Jailbreak attacks on Large Language Models (LLMs) manipulate model inputs to provoke targeted, often harmful outputs. These attacks coerce models to comply with malicious queries that should otherwise be refused, posing a vital security risk. However, while effective, optimizing jailbreak prompts often requires extensive computational resources and may still converge to suboptimal optima. We propose a novel attack-agnosti initialization framework for jailbreak attacks. Our initialization aims to reduce the gap to the \emph{compliance} subspace over harmful prompts. We demonstate our approach on two prominent jailbreak paradigms—GCG and AutoDAN—demonstrating consistent gains in attack success rates and reductions in computational cost across multiple LLM architectures. We provide a reference implementation of our work in \href{google.com}{google.com}.
 
 
Jailbreak attacks aim to exploit large language models (LLMs) and pose a significant threat to their proper conduct; they seek to bypass models' safeguards and often provoke transgressive behaviors. However, existing automatic jailbreak attacks require extensive computational resources and are prone to converge on suboptimal solutions. In this work, we propose \textbf{C}ompliance \textbf{R}efusal \textbf{I}nitialization ($CRI$), a novel, attack-agnostic framework that efficiently initializes the optimization in the proximity of the compliance subspace of harmful prompts. By narrowing the initial gap to the adversarial objective, $CRI$ substantially improves adversarial success rates ($ASR$) and drastically reduces computational overhead—often requiring just a single optimization step. We evaluate $CRI$ on the widely-used $AdvBench$ dataset over the standard jailbreak attacks of $GCG$ and $AutoDAN$. Results show that $CRI$ boosts $ASR$ and decreases the median steps to success by up to \textbf{$\times{60}$}. The project page, along with the reference implementation, is publicly available at \href{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}{\includegraphics[height=1em]{fig/github-mark.png}}.%\yaniv{Always write numbers, abbreviations, and names in math, such as $CRI$, $AdvBench$, $GCG$, $AutoDAN$, $98.5\%$. Please change this in the entire paper, including tables}
% \amit{At the end we mentioned we enhance the run-time and the ASR twice}\yaniv{answer: the first is theoretical and the second is practical}


%\avi{I suggest replacing the abstract with "Jailbreaking involves making minor modifications to input prompts that persuade an LLM to generating harmful content. Traditionally, designing effective jailbreak prompts requires extensive computational resources; we propose a novel attack-agnostic initialization framework that aims to reduce the gap to the \emph{compliance} subspace over harmful prompts and so can achieve successful rate attacks while using significantly less resources. We demonstrate our approach in two prominent jailbreak paradigms, GCG and AutoDAN, showing a consistent gain in attack success rates and a reduction in computational cost across multiple LLM architectures. We provide a reference implementation of our work in \href{google.com}{google.com}."}
% Amit: Done
%\avi{pleasae add here the essence of the results we got}

%\yaniv{I slightly modified this; please go over it}
%\amit{Done, \\

\end{abstract}