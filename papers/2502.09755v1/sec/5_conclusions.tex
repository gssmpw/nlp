\section{Discussion}
\label{Discussion}
In this work, we introduce $CRI$, a novel attack-agnostic initialization framework designed to enhance the efficiency and effectiveness of jailbreak attacks by initializing them in the proximity of the \emph{compliance} subspace. Our approach involves pre-training a set of $JT$s and using them to initialize subsequent attacks. Experimental results demonstrate that $CRI$ consistently achieves the highest $ASR$ across all evaluated settings, significantly outperforming other initialization strategies. Moreover, our method substantially reduces the computational overhead required for successful jailbreaks. For instance, applying $CRI$ to $AutoDAN\text{-}HGA$ on Llama-2 achieves an $ASR$ of $92$ with an $MSS$ of $4.5$, corresponding to a $37\%$ improvement in $ASR$ and a $5\times$ reduction in $MSS$ compared to standard initialization.

The success of our approach provides empirical evidence that $JT$s generalize effectively to unseen prompts. In many cases, these pre-trained $JT$s can induce jailbreaks without requiring further optimization or with only a few additional optimization steps. This suggests that jailbreak attacks exhibit high similarity across different prompts, revealing a notable weakness in current models' robustness to adversarial manipulations.

Our proposed $CRI$ leverages this inherent similarity to systematically map a broad range of harmful prompts closer to the \emph{compliance} subspace. This observation aligns with the findings of \citet{arditi2024refusal}, which suggest that a single principal direction governs refusal behavior in large language models (LLMs). The consistency of jailbreak attacks across different prompts may be attributed to the optimization process predominantly operating along this \emph{refusal} direction. Since LLM alignment processes presumably induce this \emph{refusal} direction, our findings suggest that some vulnerabilities of LLMs may arise as a byproduct of the alignment process itself.

From a practical perspective, integrating pre-trained $JT$s into LLM alignment training could offer a more efficient alternative to conventional jailbreak data collection methods. Instead of generating computationally expensive jailbreak examples repeatedly, as done in \citet{mazeika2024harmbench}, our approach could help reduce the time complexity required to obtain robust adversarial examples, ultimately contributing to developing more resilient models.

\subsection{Limitations}
Despite the advantages of initialization-based attack strategies, these initializations may restrict the diversity of the produced $JT$s. Initialization that converges in several optimization steps can lead to narrow attack trajectories and may not be relevant to enhancing models' robustness to other jailbreak attacks. When utilized for alignment training \cite{mazeika2024harmbench}, such initializations can inadvertently harm the model's effectiveness against real-world jailbreak threats. 

\section{Ethics and Broader Impact}
\label{section:Ethics}

While this work can potentially facilitate the generation of harmful data on open-source LLMs or reveal vulnerabilities that expedite attacks, we believe it is vital to highlight these threats to promote AI security research. By identifying and studying these weaknesses, we can build stronger defenses and reduce risks in an environment where LLMs are rapidly developing. Recognizing the risks and limitations is essential for creating adequate safeguards, allowing researchers and developers to address emerging threats proactively.

% which may not fully represent real-world jailbreak threats. 


% initializing from a low-loss state could have limitations, when incorporated into dataset that used for training alignment (e.g., \cite{andriushchenko2024jailbreaking}), such initializations may restrict the diversity of adversarial examples, reducing the alignment or defense effectiveness against broader perturbations. Advanced initialization that could coverage in one optimization step can lead for narrow attack trajectories, which may not fully represent real-world jailbreak threats. 


    % \item We consider two approaches to $CRI$ corresponding to different interpretations of the embedding refusal direction in the textual domain. The first considers a set of $JT$ and directly initializes new attacks via these transformations. The second approach utilizes a multi-prompt universal attack to generate a single $JT$, enhancing its generalization to unseen prompts.
    % \item We propose an attack-agnostic initialization framework and adapt it to the $GCG$ \citet{zou2023universal} and $AutoDAN$ \citet{liu2023autodan} attacks, representing common gradient-based jailbreak approaches. Our method initializes the greedy algorithm in $GCG$ and reference prompts in $AutoDAN$ via $CRI$. We demonstrate that $CRI$ improves $ASR$ while reducing computational overhead compared to standard initialization.
    % \item We examine the generalization properties of $JT$s to unseen prompts and show that they can successfully produce jailbreaks without further optimization. Moreover, with only several optimization steps, attacks achieve nearly $100\%$ $ASR$ in multiple settings. This indicates the similarity of jailbreak attacks across different prompts and entails that current models are not sufficiently robust to such attacks.
    % Finally, we leverage our findings to contribute to broader efforts in uncovering alignment vulnerabilities in jailbreak defenses \rom{alignment defences?}, providing critical insights to improve the robustness of alignment training methods.  

% This paper proposes $CRI$, a novel attack-agnostic initialization framework designed to generate a starting point that positions harmful prompts in the proximity of the compliance region. We adapt and evaluate our initialization framework across four different attack scenarios, including greedy and genetic algorithms, as well as individual and universal attack settings, on three different LLMs. Our results demonstrate $CRI$’s profound positive impact on both the speed and success of jailbreak attacks, as well as universality and transferability.

% Beyond its success, $CRI$ fits into broader research contributions, relating to vulnerabilities induced by the gap between compliance and refusal, further discussed in \ref{appendix:CRI Limitations & Defences}. This highlights a critical issue in current alignment strategies caused the rigid dichotomy of prompts into "refuse" or "comply" categories—a practice that can inadvertently facilitate generalized attacks utilizing $JT$s, as demonstrated by $CRI$. Looking forward, the $CRI$ framework highlights this significant concern, emphasizing the urgent need to address these risks in future alignment and LLM defense studies.







% \paragraph{Limitations.} While $CRI$ enhances jailbreak performance, it also has inherent limitations. One inherent limitation is caused by the information contained within a finished $JT$, which is later used as a start for further optimization steps. Using this $JT$, we do not start the new attack from a clean slate, 

% Using an attack's minimal-loss seeds may bias the adversarial process toward specific trajectories, potentially overlooking broader jailbreak patterns. Training alignment or defense methods on such jailbreak prompts may lead to overfitting to these specific patterns, reducing the diversity of adversarial samples and limiting adaptability to varied perturbations.

%\rom{IPA has overfitting based on what it was optimized on - creates bias towards one target. Universal has higher loss, and is comparable to regular compliance prompts. On the other hand, IPA has a much lower loss than compliance and UPA - what can point toward overfitting. Meanwhile in our experiments, IPA has better performance than UPA, we expect that more robust models will trap IPA into local minima.}


