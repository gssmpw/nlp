\section{Background and Related Work}
\label{section:bground_related}

\input{vis/fig_framework}

%We first present the traditional jailbreak attack setting in \cref{subsec:background}, we then continue to discuss related work in \cref{subsec:related}.


\subsection{Background}
\label{subsec:background}

% \subsubsection{Jailbreak Attacks Setting}
% \label{subsubsec:setting}

% We formalize and update the objective of jailbreak attacks as suggested by \citet{zou2023universal}:\\
Let $V$ be some vocabulary that contains the empty token $\phi$, let $V^*\equiv \bigcup_{i=1}^{\infty}V^i$ be the set of all sequences over $V$, and let $M:V^*\to[0,1]^{|V|}$ be an LLM that maps a sequence of tokens to a probability distribution over the following token to be generated. Given input and target sequences $x_{1:n}\in V^n, t_{1:H}\in V^{H}$ we extend the definition of $M$ to consider the probability of generating $t_{1:H}$ over $x_{1:n}$. 

\newpage
Formally, denoting $\oplus$ as sequence concatenation, and $t_{0} \equiv\phi$:
\begin{align}
M(x_{1:n})&\equiv\{p_M(x_{n+1} | x_{1:n})\}_{x_{n+1}\in V}\\
p_M(t_{1:H} | x_{1:n}) &\equiv \prod_{i=1}^{H} p_M(t_{i} | x_{1:n}\oplus t_{0:i-1})
\end{align}
In addition, let $\ell_M(x', t)$ be the negative log probability of generating $t\in V^*$ by $M$ over some input $x'\in V^*$. Given input and target $(x,t)\in V^*\times V^*$ and a set of $JT$s $\mathcal{T}^k\subset V^n\to V^{n+k}$, a prompt jailbreak attack $A$ targets the minimization of $\ell_M$ over the transformed input. Similarly, given a set of input and target sequences $\{x_i,t_i\}_i\subset V^*\times V^*$, a universal prompt jailbreak attack $A^U$ targets the same minimization in expectation over the set while applying a single fixed transformation. Formally:
\begin{align}
\ell_M(x', t) &= -\log p_M(t | x') \label{eq:att_crit} \\
A(x,t) &=\arg\min_{T \in \mathcal{T}^k} \ell_M(T(x), t) \label{eq:indiv_att}\\
A^U(\{x_i,t_i\}_i) &= \arg\min_{T \in \mathcal{T}^k} \mathbb{E}_i \left[ \ell_M(T(x_i), t_i) \right]\label{eq:univ_att}
\end{align} 
Hereby, the set of $JT$, $\mathcal{T}^k$ bounds the scope of transformed inputs. Standard bounds limit these transformations to either add a suffix to the prompt or to add both a suffix and a prefix:
\begin{align}
\mathcal{T}^k_{s}(x_{1:n}) &= \{x_{1:n}\oplus s\}_{s\in V^k}\\
\mathcal{T}^k_{ps}(x_{1:n}) &= \{p\oplus x_{1:n}\oplus s\}_{p,s\in V^{\nicefrac{k}{2}}}
\end{align}

% Similarly, a \textbf{universal prompt jailbreak attack} $A^U$ aims to optimize a single $JT$ across a set of input sequences $\{x^i_{1:n}\}_i$ and corresponding targets $\{x^{i*}_{n+k+1:n+k+H}\}_i = \{t_i\}_i$, seeking to minimize the total criterion across all inputs and targets:
% \begin{equation}
% A^U(\{x^i_{1:n}\}_i, \{t_i\}_i) = \arg\min_{T \in \mathcal{T}^k} \sum_i \ell_M(T(x^i_{1:n}), t_i)
% \end{equation}



% Specifically, they utilize a set of harmful prompts \(\mathcal{D}_{\text{harmful}}\) and a set of harmless prompts \(\mathcal{D}_{\text{harmless}}\). For each layer \(l\) in the LLM and token index \(i\), they compute:
% \begin{equation}
%     r_i^{(l)} 
%     = \frac{1}{|\mathcal{D}_{\text{harmful}}|} 
%     \sum_{t \in \mathcal{D}_{\text{harmful}}} x_i^{(l)}(t)
%     \;-\;
%     \frac{1}{|\mathcal{D}_{\text{harmless}}|} 
%     \sum_{t \in \mathcal{D}_{\text{harmless}}} x_i^{(l)}(t),
% \end{equation}
% where \( x_i^{(l)}(t) \) denotes the embedding of the \(i\)-th token in layer \(l\) when processing a prompt \( t \). By applying \(r_i^{(l)}\) as an embedding transformation to unseen prompts, it becomes possible to \emph{induce} or \emph{remove} refusal behaviors in the model’s responses, as demonstrated in the original study.\amit{the connection between the objective into the refusal direction formula is good, however, why do we mentioning the harmless direction as well? we don’t use it and its out of our work scope, might could add noise to the reading stream}


% A crucial aspect of jailbreak attacks is understanding how LLMs balance compliance (i.e., granting harmful requests) versus refusal (i.e., adhering to policy and refusing harmful requests). \citet{arditi2024refusal} introduce a useful perspective on this by defining a \emph{refusal direction} in the model’s embedding space. Specifically, they construct two sets of prompts: a harmful set \(\mathcal{D}_{\text{harmful}}\) and a harmless set \(\mathcal{D}_{\text{harmless}}\). Then, they measure the embedding difference in each LLM layer \(l\) and token index $i$ between these two sets, defining:
% \begin{equation}
%     r_i^{(l)} 
%     = \frac{1}{|\mathcal{D}_{\text{harmful}}|} 
%     \sum_{t \in \mathcal{D}_{\text{harmful}}} x_i^{(l)}(t)
%     \;-\;
%     \frac{1}{|\mathcal{D}_{\text{harmless}}|} 
%     \sum_{t \in \mathcal{D}_{\text{harmless}}} x_i^{(l)}(t)
% \end{equation}
% where \( x_i^{(l)}(t) \) denotes the embedding of the \(i\)-th token in layer \(l\) for a prompt \( t \). By utilizing \(r_i^{(l)}\) as an embedding transformation, it is possible to \emph{induce} or \emph{remove} refusal behaviors in the model’s outputs, as demonstrated in the original work.

% However, embedding-based techniques do not readily translate into a purely textual form. This raises an open question: can the same embedding manipulations be replicated at the textual level? If so, one could rely on textual prompt modifications rather than internal embedding interventions to achieve the same \emph{refusal direction} effect, thereby enabling new strategies to either induce or prevent model refusals.



% NEWER
% \citet{arditi2024refusal} defined the \emph{refusal direction} in the embedding space of the LLM. More specifically, taking a set of harmful $\mathcal{D}_{\text{harmful}}$ and harmless prompts $\mathcal{D}_{\text{harmless}}$, for each layer $l\in[L]$ in an LLM and token position $i$, the refusal direction is defined as:
% \begin{equation}
%     r_i^{(l)} = \frac{1}{\mathcal{D}_{\text{harmful}}} \sum_{t\in\mathcal{D}_{\text{harmful}}}x_i^{(l)}(t) - \frac{1}{\mathcal{D}_{\text{harmless}}} \sum_{t\in\mathcal{D}_{\text{harmless}}}x_i^{(l)}(t)
% \end{equation}
% where $x_i^{(l)}(t)$ denotes the embedding of token $t_i$ in layer $l$ of the LLM.

% $r_i^{(l)}$ represents a transformation in the embedding space, that can induce or remove refusal altogether, as demonstrated in the paper. It is important to highlight that this transformation is only applicable in the embedding space, and cannot be easily integrated into a $JT$ as we define. An important question arises: can this embedding transformation be generalized into a textual domain, and let us gain the same benefits?




%\rom{1.Again but divided according this order for three paragraphs  Current jailbreak methods: GCG, AutoDAN.
%2. Current methods that utilize initializations (e.g. I-GCG).
%3. Compliance Refusal works.}

\subsection{Related Work}
\label{subsec:related}
% \paragraph{Gradient-Based Jailbreak Attacks.}  
Gradient-based jailbreak attacks have enabled systematic, automated optimization of jailbreak attacks and utilize greedy-algorithm-based \cite{jia2024improved,yu2024enhancing,mehrotra2023tree} or genetic-algorithm-based \cite{liu2023autodan,liu2024autodan,zhao2024accelerating} optimization schemes. \citet{zou2023universal} first suggested the greedy-based approach and the $GCG$ attack, which is initialized via repetitions of hand-picked tokens and optimizes prompts' suffixes through greedy gradient steps. \citet{liu2023autodan} then suggested the genetic-algorithm-based approach and the $AutoDAN$ attack, which is initialized via handcrafted reference jailbreak strings; this attack focuses on bypassing jailbreak classifiers and utilizes genetic algorithms to refine both prefixes and suffixes of prompts. 

In many jailbreak attacks \cite{liu2023autodan, zou2023universal, jia2024improved}, the target $t$ in the attack objective $\ell_M$ is selected to elicit an affirmative response, such as "Sure, here is...". The intention is then to achieve the model's compliance over harmful prompts. In contrast, \citet{arditi2024refusal} introduces a complementary perspective by defining a refusal direction within the model’s embedding space. By analyzing embeddings of harmful and harmless prompts across LLM layers, they construct a linear transformation capable of inducing or suppressing refusals. These compliance and refusal transformations underscore the distinct separation of these behaviors within LLMs' internal representations.

% \paragraph{Compliance-Refusal}



% Gradient-based methods have also been leveraged for developing defenses, alignment training, and model analysis techniques \cite{yi2024jailbreak}. 


% Notably, universal variants such as $GCG\text{-}M$ \cite{zou2023universal} extend this capability by generalizing jailbreak prompts across multiple inputs and settings. Gradient-based methods have also been leveraged for developing defenses, alignment training, and model analysis techniques \cite{yi2024jailbreak}. 

% Gradient-based jailbreak attacks have significantly advanced adversarial methods on LLMs by enabling systematic, automated optimization of input perturbations to generate specific, often harmful outputs across diverse task settings. These attacks exhibit high transferability, allowing adversaries to replicate jailbreak prompts across both open-source and closed-source models without adaptation, effectively enabling black-box attacks. Notably, universal variants such as $GCG\text{-}M$ \cite{zou2023universal} extend this capability by generalizing jailbreak prompts across multiple inputs and settings. Gradient-based methods have also been leveraged for developing defenses, alignment training, and model analysis techniques \cite{yi2024jailbreak}. 

% Broadly, these methods fall into two categories: Greedy Coordinate Gradient ($GCG$)-based and Genetic Algorithm ($GA$)-based approaches, with some techniques combining both. The $GCG$ family, introduced by the original $GCG$ method \cite{zou2023universal}, optimizes input suffixes through greedy gradient steps and serves as the foundation for many later techniques \cite{jia2024improved,yu2024enhancing,mehrotra2023tree}. In contrast, the $GA$ family, exemplified by $AutoDAN$ \cite{wu2023autogen}, refines control slices—the modifiable portions of the attack—using iterative optimization, handcrafted patterns, and reference jailbreak strings. This approach focuses primarily on bypassing jailbreak classifiers and has influenced multiple advanced attack strategies \cite{liu2024autodan,zhao2024accelerating}.

% \paragraph{Initialization for Jailbreak Attacks.}


% Early jailbreak methods relied on naive attack initialization techniques, such as using repetitions of hand-picked tokens \cite{zou2023universal}. These techniques were later refined by $AutoDAN$, which initialized attacks based on manually scripted prompt injection patterns, where the control slice—the portion of the input modified during optimization—was explicitly defined. Further advancements in the GCG family of jailbreak attacks, such as $I\text{-}GCG$ \cite{jia2024improved}, observed that different categories of prompts converge at varying rates. This method selects a fast-converging, successful attack (e.g., fraud-related prompts) as an initialization strategy. While both $AutoDAN$ and $I\text{-}GCG$ have significantly improved attack efficiency and success rates, they remain computationally expensive and struggle to generalize across diverse models and attack scenarios. farther more recent advanced attacks in other settings which is out of our frame work relay in strict handcrafted intilizations such \cite{li2024jailpo,zheng2024jailbreaking,chao2023jailbreaking} 


%\paragraph{Alignment-Training and Refusal in LLMs}
%Alignment-training methods has been pivotal in mitigating the misuse of LLMs using training techniques such as reinforcement learning\cite{wang2023aligning, lee2023rlaif} aim to align model outputs with safety and ethical standards by optimizing reward mechanisms that prioritize desirable responses while penalizing harmful or unsafe ones \cite{glaese2022improving}. This alignment effectively segregates prompts into \emph{compliance} and \emph{refusal} subspaces, enhancing the model’s ability to distinguish and appropriately respond to varying inputs \cite{wang2020understanding}.




% Jailbreak attacks exploit vulnerabilities in aligned models by manipulating inputs to coerce harmful outputs \rom{we already said this in the intro...} \cite{winograd2022loose, robey2024jailbreaking, mozes2023use, zhang2024llms, cohen2024unleashing}. Early approaches to jailbreak attacks relied on simple initialization strategies, such as repeating fixed tokens \cite{zou2023universal}, which were later advanced by methods like $AutoDAN$ \cite{liu2023autodan}. $AutoDAN$ introduced handcrafted initializations and utilized a subsequent reference file for candidate selection during attack iterations, modifying multiple tokens per step to enhance effectiveness. Subsequent research, namely ($I\text{-}GCG$) \cite{jia2024improved}, demonstrated the use of a previous jailbreak attack as a starting point for further attacks. In their work, they noticed different categories of prompts converge to a successful jailbreak attack at different rates, thus picking a successful attack that converged fast as a starting point (fraud related prompt). While $AutoDAN$ and $I\text{-}GCG$ have made significant strides in improving attack efficiency and success rates. However, they remain constrained by high computational costs and limited generalization ability across different models and attack scenarios. Additionally, embedding-based methods, while efficient, lack practicality in real-world attacks where internal model access is restricted.

%\paragraph{Embedding-Based Jailbreaks.}
%\rom{Do we really need this? we already talk about this in the intro and background enough...}
%Embedding-based Jailbreak methods operate in the model's latent space, where harmful prompts are projected into representations aligned with compliance subspaces \cite{yu2024enhancing, arditi2024refusal}. By manipulating and analyzing the embedding space, these methods achieve faster and more efficient adversarial transformations compared to textual gradient-based approaches. However embedding-based methods have several practical limitations. They require access to the model's internal embeddings and gradient information, which may not be available in real-world applications. Additionally, embedding-based attacks are less flexible in adapting to unseen tasks, commonly use as defenses or analysis approaches 
 %\cite{marshall2024refusal, hu2024gradient}. This underscores the need for robust, attack-agnostic initialization strategies that operate effectively within the textual domain, a gap that our CRI framework aims to fill.





% \label{subsec:related}  
% Adversarial attacks have illuminated critical vulnerabilities in various domains of machine learning, particularly in computer vision, where subtle input perturbations can induce model misclassification 
% \citet{szegedy2014intriguing,biggio2013evasion}. Foundational studies revealed how seemingly inconsequential modifications could compromise the integrity of the model, requiring extensive research on offensive strategies and defensive countermeasures \citet{carlini2017towards}. Within the natural language processing domain, these attacks are further complicated by the discrete nature of textual data. Commonly referred to as "jailbreaks," adversarial prompt engineering aims to circumvent the alignment of large language models (LLMs), thus eliciting unintended or unauthorized output \citet{wei2023jailbroken}. Despite advances such as Human Feedback Reinforcement Learning (RLHF), which integrates ethical constraints and user preferences into training pipelines \citet{bai2022training,ouyang2022training}, significant alignment gaps persist, leaving LLMs susceptible to adversarial exploitation.

% Drawing inspiration from methodologies in computer vision, gradient-based algorithms such as Greedy Coordinate Gradient (GCG) \citet{he2023gcg} have been adapted to automate adversarial prompt generation for LLMs. These algorithms have demonstrated substantial improvements in attack efficacy and transferability across diverse model architectures. Innovations like I-GCG \citet{anonymous2024improved} further refined these techniques, incorporating specialized initializations (e.g., "fraud prompts") designed to accelerate convergence during optimization. However, these approaches continue to face challenges including perplexity constraints \citet{liu2023autodan} and computational bottlenecks \citet{anonymous2024improved}.

% Recent studies have highlighted a fundamental dichotomy within the latent space of LLM prompts, distinguishing between regions associated with refusal and compliance. For example, \citet{Yu2024EnhancingJA} introduced the concept of an "Ethical Boundary," which implicitly categorizes prompts into ethical and nonethical inputs. This separation is attributed to post-alignment processes, wherein the model learns to reject certain prompts explicitly. Building on this framework, \citet{Arditi2024RefusalIL} proposed embedding-space jailbreak techniques that exploit a "Refusal Direction," effectively quantifying the latent space divergence between refusal and compliance zones. These contributions collectively validate the notion of distinct refusal and compliance spaces, offering a robust foundation for advancing methodologies to leverage this attribute for more sophisticated adversarial strategies.

% \amit{integrate this as well}

% \textbf{Alignment Training:}
% Recent studies indicate that alignment training can inadvertently introduce vulnerabilities by clearly segmenting output spaces into refusal and compliance subspaces, thereby simplifying adversarial manipulation \citet{universal_jailbreak_backdoors,chen2023reversing}. This segmentation potentially facilitates targeted adversarial actions, as attackers can exploit these defined boundaries to shift outputs from refusal to compliance.


