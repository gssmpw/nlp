\section{Method}
\label{section:method}
We now discuss our suggested $CRI$. We first define $CRI$'s objective and the corresponding initialization sets in \cref{subsec:objective}. We then present the integration of our framework over greedy-based (e.g., $GCG$) and genetic-algorithm-based (e.g., $AutoDAN$) works in \cref{subsec:method_integration}.
\input{vis/fig_gcg_asr_row}

\subsection{Objective}
\label{subsec:objective}
% \yaniv{ why do we need a small subset? Instead, we are limited by the size of the testing set. Perhaps it is similar to batch active learning in terms of expected benefit and variance over samples. we are working with a limited fine-tuning set and want to make the most benefit from it. To avoid bias, we limit ourselves to only using each sample once. we, therefore, wish to separate the tuning set into smaller subsets and average the $JT$ on each. The extremities are then to take independent transformations on each prompt or to average the $JT$ on all the prompts. As any other option requires a non-trivial choice of prompt grouping, we consider them out of the scope of the current work. We therefore take the before mentioned $2$ options and check each, denoted as CRI and UCRI. It's a shame we haven't checked both together. Can we claim it's always the maximum value when we use topk?}
$CRI$'s objective is to optimize an initial set of $JT$s $\mathcal{T}_0 \subseteq \mathcal{T}^k$ over a fine-tuning set $S_{FT}$, while targeting the proceeding jailbreak attack over an unknown prompt, sampled from the same distribution. As we aim to provide an informative initialization, our optimization targets the initial value of the attack criterion. We, therefore, optimize this initialization over $S_{FT}$ and consider its generalization to unknown prompts. In addition, applying a limited set of transformations on a given input only requires corresponding inference passes, the computational overhead of which is negligible compared to back-propagation over the complex textual domain. We, therefore, take the best-performing initialization when applying $CRI$ on a given input. Formally:
\begin{align}
    \mathcal{T}_0 = \arg\min_{\mathcal{T}_0 \subseteq \mathcal{T}^k} \underset{(x,t)\in S_{FT}}{\mathbb{E}} \left[ \min_{T\in \mathcal{T}_0} \ell_M(T(x),t) \right] \label{eq:objective}
\end{align}
We now approximate the inner minimization via some individual or universal attacks $A, A^U$, and correspondingly denote the resulting sets as $\mathcal{T}_{CRI}, \mathcal{T}_{U\text{-}CRI}$. $\mathcal{T}_{CRI}$ then corresponds to optimizing each $T\in \mathcal{T}_0$ independently over each $(x,t)\in S_{FT}$, and $\mathcal{T}_{U\text{-}CRI}$ corresponds to optimizing a single $\{T\}\equiv \mathcal{T}_0$ that optimizes the expectation over the entire fine-tuning set:
\begin{align}
    \mathcal{T}_{CRI} &= \{A(x,t)\}_{(x,t)\in S_{FT}}\\
    \mathcal{T}_{U\text{-}CRI} &= \{A^u(S_{FT})\}
\end{align}
The single $JT$ in $\mathcal{T}_{U\text{-}CRI}$ considers multiple prompts and aims for optimized generalization to unknown prompts. In contrast, $\mathcal{T}_{CRI}$ optimizes each $JT$ to achieve the minimum over a different prompt in \cref{eq:objective}. Doing so will achieve lower loss over the fine-tuning set and provide various initial $JT$s, which may be relevant to different prompts in deployment. We can consider taking a combination of the approaches in $\mathcal{T}_{CRI}, \mathcal{T}_{U\text{-}CRI}$ by grouping prompts for applying $A^U$. However, doing so would either reuse prompts, which may cause overfitting over $S_{FT}$, or require a non-trivial choice of prompts' grouping. We, therefore, consider such approaches out of the scope of the current work. Similarly, we limit this work to considering the same attack in fine-tuning and deploying $CRI$. Nonetheless, we consider both individual and universal attack variants when available.

% \paragraph{Interpretation and Trade-Offs.}

% Increasing \(K\) (the size of the transformation set) generally lowers the overall loss but also raises the complexity of managing or searching through more transformations. In practice, a relatively small \(K\) that achieves robust performance across a variety of harmful prompts is ideal, balancing computational overhead with a possible better starting point.



% Each transformation \(T \in \mathcal{T}^k\) modifies a textual input (e.g., by adding a prefix and/or suffix), striving to increase the likelihood that the model will comply with the harmful request (i.e. generate the target). Formally, we measure the success of a transformation \(T\) using the negative log-likelihood loss \(\ell_M\) for generating the desired harmful target \(t\) from an input \(x_{1:n}\). We let \(K \in \mathbb{N}\) denote the maximum number of allowed transformations in \(\mathcal{T}_0\).


 % Our goal is to find a subset of $JT$s, \(\mathcal{T}_0 \subseteq \mathcal{T}^k\), of bounded size \(\lvert \mathcal{T}_0\rvert \le K\), that minimizes the sum over for any input-target pair \((x_{1:n}, t)\) from the dataset, sum over the loss correspondence to the transformation \(T \in \mathcal{T}_0\) that yields for this pair a minimal jailbreak loss. Following \cref{subsec:background}, recall that \(\ell_M(\cdot, \cdot)\) is the negative log-likelihood criterion we aim to minimize. We let \(K \in \mathbb{N}\) denote the maximum number of allowed transformations in \(\mathcal{T}_0\).

% \subsubsection{Distribution-based Objective}

% Let \(\mathcal{D}\) be an unknown distribution over pairs of inputs and desired outputs \((x_{1:n}, t)\). Our ideal goal is to find a subset \(\mathcal{T}_0\subseteq \mathcal{T}^k\) of size at most \(K\) that minimizes the expected loss:

% \begin{equation}
% \label{eq:dist-obj}
% \mathcal{T}_0^*
% \;=\;
% \arg\min_{\substack{\mathcal{T}_0 \subseteq \mathcal{T}^k \\ \lvert \mathcal{T}_0\rvert \,\le\,K}}
% \;\;
% \mathbb{E}_{(x_{1:n},\,t)\,\sim\,\mathcal{D}}
% \bigl[\,
%   \min_{T\,\in\,\mathcal{T}_0}\,
%   \ell_M\bigl(T(x_{1:n}),\,t\bigr)
% \bigr].
% \end{equation}

% In words, for each input-target pair \((x_{1:n}, t)\) drawn from \(\mathcal{D}\), we pick the single transformation \(T \in \mathcal{T}_0\) that yields the smallest loss, and we want the set \(\mathcal{T}_0\) that performs best on expectation over \(\mathcal{D}\).



% We begin with the idealized objective under an unknown distribution \(\mathcal{D}\) of input-target pairs:
% \begin{equation}
% \label{eq:dist-obj}
% \mathcal{T}_0^*
% \;=\;
% \arg\min_{\substack{\mathcal{T}_0 \subseteq \mathcal{T}^k \\ |\mathcal{T}_0|\le K}}
% \;
% \mathbb{E}_{(x_{1:n},\,t)\,\sim\,\mathcal{D}}
% \Bigl[
%   \min_{T \in \mathcal{T}_0}
%   \ell_M\bigl(T(x_{1:n}),\,t\bigr)
% \Bigr]
% \end{equation}
% In words, for a draw \((x_{1:n}, t)\) from \(\mathcal{D}\), we select the single transformation \(T\) within the set \(\mathcal{T}_0\) that achieves the smallest loss \(\ell_M\). The outer \(\arg\min\) then searches for the best possible set \(\mathcal{T}_0\) (whose size is at most \(K\)), so as to minimize the expected loss under \(\mathcal{D}\).

% \subsubsection{Empirical Approximation}

% Since \(\mathcal{D}\) is typically unknown, we approximate it by a finite \emph{fine-tuning set} \(\mathcal{F} = \{\,(y_i, t_{y_i})\}_{i=1}^m\). This reduces the problem to a standard empirical risk minimization:

% \begin{equation}
% \label{eq:emp-obj}
% \widehat{\mathcal{T}_0^*}
% \;=\;
% \arg\min_{\substack{\mathcal{T}_0 \subseteq \mathcal{T}^k \\ \lvert \mathcal{T}_0\rvert \,\le\,K}}
% \;\;
% \sum_{(y_i,\,t_{y_i}) \,\in\,\mathcal{F}}
% \Bigl[
%   \min_{T \,\in\, \mathcal{T}_0}
%   \ell_M\bigl(T(y_i),\,t_{y_i}\bigr)
% \Bigr].
% \end{equation}
% \yaniv{I think we should approximate the inner summation via our jailbreak attack. We can explain here why we only take one or K as a possible size and then we produce the attacks correspondingly. This also explains why fine-tuning once for a given model and attack is necessary.}


% Here, the summation replaces the intractable expectation over \(\mathcal{D}\). At inference time, given a new input-target pair \((x_{1:n}, t)\), we choose the single \(T \in \widehat{\mathcal{T}_0^*}\) that minimizes \(\ell_M\bigl(T(x_{1:n}), t\bigr)\).



% Because \(\mathcal{D}\) is generally unknown, we approximate the expectation by a finite \emph{fine-tune set} \(\mathcal{F} = \{(y_i, t_{y_i})\}_{i=1}^m \sim \mathcal{D}\), consisting of representative input-target pairs. This leads to the standard empirical-risk-minimization problem:
% \begin{equation}
% \label{eq:emp-obj}
% \widehat{\mathcal{T}_0^*}
% \;=\;
% \arg\min_{\substack{\mathcal{T}_0 \subseteq \mathcal{T}^k \\ |\mathcal{T}_0|\le K}}
% \;
% \sum_{(y_i,\,t_{y_i}) \,\in\, \mathcal{F}}
% \Bigl[
%   \min_{T \in \mathcal{T}_0}
%   \ell_M\bigl(T(y_i),\,t_{y_i}\bigr)
% \Bigr]
% \end{equation}


% \paragraph{Interpretation and Further Insights.}
% In practice, we solve the empirical objective in \cref{eq:emp-obj} to obtain \(\widehat{\mathcal{T}_0^*}\).  This set \(\widehat{\mathcal{T}_0^*}\) is then used to determine the best transformation for a new input-target pair at inference time, ideally leading to low average loss under the true distribution \(\mathcal{D}\). 
% % \input{vis/histogram}

% There exists an inherent trade-off between the size of the set of initialization transformations and their feasibility. Intuitively, as \(K\) increases, the corresponding objective in \cref{eq:emp-obj} decreases. However, this also makes it more difficult to identify the optimal transformation for each input-target pair, since one must iterate through all possible transformations.


% \subsection{Relation to Compliance-Refusal}
% \label{subsec:obj_comp_ref}
% % Explain the theory on how it relates to stuff we mentioned before
% \rom{How our objective brings us closer to compliance. MAYBE REMOVE}

% As our target responses are of a \emph{compliant} nature, a lower value in the criterion (\cref{eq:att_crit}) indicates closer alignment with \emph{compliance}. Thus, our objective can be viewed as identifying a set \(\mathcal{T}_0\) that nudges new, unseen harmful prompts toward the \emph{compliance} subspace. In this sense, \(\mathcal{T}_0\) serves as an informative \emph{compliance direction} for harmful prompts. We investigate its effectiveness in moving from refusal to compliance by leveraging \(\mathcal{T}_0\) as a starting point for subsequent attacks.


% % As our interested targets correspond with \emph{compliant} responses, with low criterion (\cref{eq:att_crit}) representing closer proximity to \emph{compliance}, our objective can be thought as finding the set $\mathcal{T}_0$ that, on new unseen harmful prompts, brings us closer to the \emph{compliance} subspace. Essentially, $\mathcal{T}_0$ represents an informative \emph{compliance direction} for harmful prompts. We study this transformation set's ability to take us from refusal to compliance by utilizing it to find a starting point for further attacks.

\subsection{Integration}
\label{subsec:method_integration}
We now discuss integrating our method in greedy and genetic optimization schemes, such as those suggested by $GCG$ and $AutoDAN$. We first discuss greedy-based attacks, followed by genetic-based attacks.

\paragraph{Greedy algorithms}
Greedy-based jailbreak attacks utilize local optimization schemes to improve the attack criterion over the $JT$ iteratively. As such, we initialize such optimization schemes via our suggested $JT$ sets. The local optimization then continues seamlessly from this point forward.

\paragraph{Genetic algorithms}
Genetic-based jailbreak attacks use reference $JT$ to initialize the optimization, e.g., hand-crafted prompts. We, therefore, extend these references with our suggested $JT$ sets. The genetic-based optimizations then consider these additional references along with the pre-existing ones.

% In $GCG$, the interested $JT$s are those adding a suffix to the prompt, meaning:
% \begin{align}
%     \mathcal{T}^k_{GCG}(x_{1:n}) = \mathcal{T}^k_{s}(x_{1:n}) = \{x_{1:n}\oplus s\}_{s\in V^k}
% \end{align}

% Moreover, $GCG$ defines both an individual prompt jailbreak attack and universal prompt attack, with the objectives defined in \cref{subsec:background}.

% We intend to utilize both attacks to find optimal initialization $JT$ sets. First, we suggest an individual prompt attack $CRI$ strategy - which simply runs a jailbreak attack on each prompt-target pair in the \emph{fine-tune set}, finding $m$ different transformations:
% \begin{align}
%     \mathcal{T}^k_{CRI} &= \{T_{CRI}^i\}_{i=1}^m\subset \mathcal{T}^k_{GCG}
% \end{align}
% Notice how \(\mathcal{T}^k_{CRI}\) perfectly minimizes our objective in \cref{eq:objective} when \(K=m\).

% Second, we suggest a Universal $CRI$ strategy (termed $U\text{-}CRI$) which simply entails running a universal attack on the entire \emph{fine-tune set} and finding a single transformation:
% \begin{align}
%     \mathcal{T}^k_{U\text{-}CRI} &= \{T_{U\text{-}CRI}\} \subset \mathcal{T}^k_{GCG}
% \end{align}
% Notice how \(\mathcal{T}^k_{U\text{-}CRI}\) perfectly minimizes our objective in \cref{eq:objective} when \(K=1\).



% \subsubsection{$AutoDAN$}
% \label{subsubsec:autodan_imp}

% In $AutoDAN$, the interested $JT$s are those adding both a prefix and a suffix to the prompt, meaning:
% \begin{align}
%     \mathcal{T}^k_{AutoDAN}(x_{1:n}) = \mathcal{T}^k_{ps}(x_{1:n}) = \{p\oplus x_{1:n}\oplus s\}_{p,s\in V^{\nicefrac{k}{2}}}
% \end{align}
% Note that prefix and suffix sizes are not fixed or even the same size, this is manifested through the use of the empty token \(\phi\) in \(V\).

% In contrast to $GCG$, $AutoDAN$ has only an individual prompt jailbreak attack available. Following the same thought process, we suggest an individual prompt attack $CRI$ strategy - which again runs a jailbreak attack on each prompt and target pair in the \emph{fine-tune set}, finding \(m\) different transformations:
% \begin{align}
%     \mathcal{T}^k_{CRI} &= \{T_{CRI}^i\}_{i=1}^m\subset \mathcal{T}^k_{AutoDAN}
% \end{align}

