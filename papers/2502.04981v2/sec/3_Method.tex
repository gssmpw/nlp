\definecolor{nbarrier}{RGB}{255, 120, 50}
\definecolor{nbicycle}{RGB}{255, 192, 203}
\definecolor{nbus}{RGB}{255, 255, 0}
\definecolor{ncar}{RGB}{0, 150, 245}
\definecolor{nconstruct}{RGB}{0, 255, 255}
\definecolor{nmotor}{RGB}{200, 180, 0}
\definecolor{npedestrian}{RGB}{255, 0, 0}
\definecolor{ntraffic}{RGB}{255, 240, 150}
\definecolor{ntrailer}{RGB}{135, 60, 0}
\definecolor{ntruck}{RGB}{160, 32, 240}
\definecolor{ndriveable}{RGB}{255, 0, 255}
\definecolor{nother}{RGB}{139, 137, 137}
\definecolor{nsidewalk}{RGB}{75, 0, 75}
\definecolor{nterrain}{RGB}{150, 240, 80}
\definecolor{nmanmade}{RGB}{213, 213, 213}
\definecolor{nvegetation}{RGB}{0, 175, 0}
\definecolor{nfence}{RGB}{35, 135, 230}
\definecolor{ntrunk}{RGB}{195,85,85}
\definecolor{npole}{RGB}{213, 0, 139}



\section{Method}
\label{methodd}


%-------------------------------------------------------------------------
%\subsection{Overview}
As shown in Figure~\ref{fig:overview}, we provide an overview of our proposed auto-annotation pipeline. Given a multi-view image sequence as input, we employ a fixed text prompt to enumerate all possible objects within the scene. Concurrently, our method supports LiDAR input, serving as a robust geometric prior constraint.

\subsection{Vision-Language Guidance}
Human annotations are both costly and labor-intensive. In contrast, world prior knowledge acquired from Vision-Language Models (VLMs) offers a cost-effective and efficient alternative, supporting open-ended semantic category perception. Current VLMs and VFMs are limited to specific 2D single-image tasks, such as captioning and segmentation.
%
These methods often struggle with multimodal interactions and multi-view consistency, potentially leading to mismatches and 3D semantic ambiguities. Moreover, they lack a comprehensive understanding of the entire 3D space. To overcome these limitations, we propose a guidance framework centered around semantic attention maps and resolve ambiguities through scene reconstruction, thereby preserving 3D semantic and geometric coherence.


\vspace{-4mm}
\paragraph{Semantic Attention Map.} 
We employ semantic attention maps to integrate and guide the acquisition of desired prior knowledge from vision-language models at the semantic level. Given a multi-view image sequence, we prompt the VLM~\cite{chen2024internvl} to consistently generate all possible object categories within each image. Specifically, we use the attention map generation method~\cite{abnar2020quantifying, lin2024training} to compute and aggregate the attentions from transformer decoder, with $N$ output tokens $ S = \{s_1, \cdots, s_N \} $ and the attention tensor $ A \in \mathbb{R}^{H \times L \times N \times N} $, with $H$ attention heads, $L$ layers:
\begin{equation}
\begin{aligned}
    Attn(s_n^{l}) = \sum_{l=0}^{L} (\frac{1}{|H^{\prime}|} \sum_{h \in H^{\prime}}A_{h,l,k,j}) ,
\end{aligned}
\end{equation}
where $s_n^l \in S$ is the output of $n$-th semantic from the transformer layer $l \in L$, $A_{h,l,k,j}$ is the attention tensor between query $j$ and key $k$ in the head $h$ among subset of heads $H^{\prime}$.
%
We then rasterize the attention maps corresponding to these semantic categories into 2D feature maps, with each category represented by an aggregated attention map $M$. Notably, we establish a dynamically updated query list that incorporates the semantic information generated by VLMs.
We implement a semantic integration strategy that merges similar sub-vocabularies with excessive gradients into unified semantic categories, thereby enhancing efficiency and mitigating visual ambiguity. For instance, we consolidate ``tree'' and ``shrub'' under the general term ``vegetation''.

\vspace{-4mm}
\paragraph{Attention-guided Visual Prior.} 
Semantic attention maps unveil category-related visual cues, which we subsequently leverage to guide the generation of semantic-aligned masks and depth information. Concretely, we input semantic attention maps as prompt cues into the the off-the-shelf segmentation models~\cite{zhao2023fast, zhang2023faster}, which then generates multiple masks within the region of interest. These masks are merged into instance-level candidate masks to fully delineate the targeted semantic regions. The mask with the highest similarity score to the embeddings of the semantic attention query is then selected.

In parallel, we employ semantic attention maps to guide depth estimation~\cite{piccinelli2024unidepth, yang2024depth} at the semantic level, decoupling background and foreground objects while excluding sky regions to avoid interference from infinite distances. We then aggregate depth information from multi-view images using semantic attention cues, where pixels within each region of interest yield a set of pseudo 3D point clouds that represents an individual instance.


\begin{figure}[ht]
  \centering
   % \vspace{-10pt}
  \includegraphics[width=1.0\linewidth]{gaussian2.pdf}
  \vspace{-2mm}
  \caption{\textbf{Vision-Language Guided Gaussian Splatting (VL-GS)} efficiently reconstructs semantic instances using a scalable strategy guided by semantic attention maps from VLMs. Additionally, VL-GS models dynamic objects through dynamic Gaussians driven by self-estimated flow.}
  \label{fig:gaussians}
  \vspace{-2mm}
\end{figure}


\subsection{VL-GS}
Although vision-language guidance provides valuable priors, 3D occupancy annotation still encounters three major challenges: 1) Semantic conflicts across multi-views make na√Øve 2D-to-3D projection prone to misalignment and ambiguity; 2) Errors in depth estimation lead to geometric distortions in 3D space; 3) Dynamic objects disrupt both spatial and temporal consistency in semantics and geometry.

To overcome these challenges, we propose Vision-Language Guided Gaussian Splatting (VL-GS), which efficiently reconstructs the entire scene while maintaining both semantic and geometric 3D consistency by combining attention-based priors and differentiable rendering. The core of VL-GS is the semantic-aware scalable GS, guided by semantic attention maps from vision-language models. During reconstruction, VL-GS smooths out 2D semantic ambiguities at the instance level and optimizes the geometric details of objects. We also introduce a self-estimated flow module to capture and reconstruct dynamic objects using temporally-aware dynamic gaussians. 3D Semantic occupancy is then directly annotated through cumulative GS-Voxel splatting, which is both efficient and precise.




%------------------------------------------------


\begin{table*}[t]
\footnotesize
\setlength{\tabcolsep}{0.004\linewidth}
\centering
\caption{
\textbf{Semantic occupancy annotation on Occ3D-nuScenes~\cite{tian2024occ3d}.} C represents camera, and L denotes LiDAR. ``cons. veh.'' and ``drive. surf.'' stand for construction vehicles and driveable surfaces, respectively.
\ourmethod{}-V uses only images as input, while \ourmethod{}-M integrates both camera and LiDAR data. The intersection over union (IoU)
and mean IoU of semantic classes (mIoU) are calculated over all voxels. For fair comparisions, we replicate SurroundOcc*~\cite{wei2023surroundocc} and  OpenOcc*~\cite{Wang_2023_ICCV} by replacing the manually annotated results with the semantic point clouds projected from VLMs.
} 
\vspace{-3mm}
\begin{tabular}{l | c c c | c c c c c c c c c c c c c c c}

    \toprule
    Method 
    & \rotatebox{90}{Input}
    & \rotatebox{90}{IoU $\uparrow$}
    & \rotatebox{90}{mIoU $\uparrow$}
    & \rotatebox{90}{\textcolor{nbarrier}{$\blacksquare$} barrier} %
    & \rotatebox{90}{\textcolor{nbicycle}{$\blacksquare$} bicycle} %
    & \rotatebox{90}{\textcolor{nbus}{$\blacksquare$} bus} %
    & \rotatebox{90}{\textcolor{ncar}{$\blacksquare$} car} %
    & \rotatebox{90}{\textcolor{nconstruct}{$\blacksquare$} cons. veh.} %
    & \rotatebox{90}{\textcolor{nmotor}{$\blacksquare$} motorcycle} %
    & \rotatebox{90}{\textcolor{npedestrian}{$\blacksquare$} pedestrian} %
    & \rotatebox{90}{\textcolor{ntraffic}{$\blacksquare$} traffic cone} %
    & \rotatebox{90}{\textcolor{ntrailer}{$\blacksquare$} trailer} %
    & \rotatebox{90}{\textcolor{ntruck}{$\blacksquare$} truck} %
    & \rotatebox{90}{\textcolor{ndriveable}{$\blacksquare$} drive. surf.} %
    & \rotatebox{90}{\textcolor{nsidewalk}{$\blacksquare$} sidewalk} %
    & \rotatebox{90}{\textcolor{nterrain}{$\blacksquare$} terrain} %
    & \rotatebox{90}{\textcolor{nmanmade}{$\blacksquare$} manmade} %
    & \rotatebox{90}{\textcolor{nvegetation}{$\blacksquare$} vegetation} \\ %
    \midrule
    

\rowcolor{LGray} GaussianOcc~\cite{wan2024gaussianocc}  & C & 51.22 & 12.59 & 1.88 & 6.42 & 13.94 & 16.75 & 2.02 & 3.41 & 6.84 & 12.33 & 1.75 & 10.32 & 41.28 & 19.32 & 18.26 & 12.41 & 21.88 \\
LangOcc~\cite{boeder2024langocc}  & C & 46.55 & 12.04 & 2.73 & 7.21 & 5.78 & 13.92 & 0.51 & 10.80 & 6.42 & 8.67 & 3.24 & 11.02 & 42.10 & 12.44 & 27.17 & 14.13 & 14.55 \\
\rowcolor{LGray} VEON~\cite{zheng2025veon}  & C & 57.92 & 14.51 & 5.03 & 4.65 & 13.88 & 11.04 & 9.63 & 10.25 & 4.51 & 10.99 & 4.32 & 12.63 & 47.50 & 11.43 & 20.52 & 25.43 & 25.76 \\
SurroundOcc*~\cite{wei2023surroundocc}  & L & 68.87 & 18.59 & 18.68 & 17.23 & 18.19 & 18.31 & 10.27 & 18.29 & 17.34 & 14.95 & 21.19 & 19.88 & 21.33 & 20.74 & 18.11 & 23.26 & 21.02 \\
\rowcolor{LGray} OpenOcc*~\cite{Wang_2023_ICCV}  & C\&L & 70.59 & 17.76 & 23.73 & 8.06 & 26.10 & 22.95 & 11.72 & 11.59 & 10.36 & 9.72 & 5.60 & 19.13 & 39.51 & 22.15 & 20.87 & 13.19 & 21.81 \\
VLM-LiDAR  & C\&L & 73.28 & 16.32 & 13.34 & 10.37 & 17.04 & 20.65 & 7.26 & 15.20 & 14.61 & 5.88 & 19.40 & 21.47 & 15.13 & 13.32 & 15.74 & 28.17 & 27.24 \\
\rowcolor{LGray} OVIR-3D~\cite{lu2023ovir}  & C\&L & 54.30 & 18.47 & 18.54 & 10.69 & 15.30 & 23.82 & 9.42 & 13.13 & 11.57 & 8.32 & 10.19 & 20.49 & 36.85 & 24.22 & 21.84 & 16.30 & 36.33 \\
\rowcolor{violet!10} \textbf{\ourmethod{}-V}  & C & \underline{83.01} & \underline{20.92} & 12.70 & 10.45 & 7.81 & 20.42 & 5.79 & 17.58 & 18.50 & 24.25 & 4.23 & 12.88 & 55.54 & 24.23 & 27.14 & 35.62 & 36.61 \\
\rowcolor{violet!10} \textbf{\ourmethod{}-M}  & C\&L & \underline{88.62} & \underline{25.84} & 21.19 & 16.08 & 18.42 & 25.90 & 4.32 & 14.58 & 25.62 & 27.18 & 3.51 & 20.93 & 58.38 & 32.03 & 29.80 & 46.15 & 43.59 \\
    
\bottomrule
\end{tabular}
\vspace{-2mm}
\label{table:occ3d-nusc}
\end{table*}


%------------------------------------------------




\vspace{-4mm}
\paragraph{Semantic-aware Scalable Gaussian.} 
Obviously, different semantic objects occupy varying "weights" within a scene, which is intuitively reflected in their semantic occupancy across scales. Meanwhile, the ability to model at multiple granularities is expected to represent the diverse geometric complexities of instances. Based on this, we propose designing a semantic-aware scalable Gaussian that adaptively scales and reconstructs different semantic objects. Unlike dense voxels or point clouds, our method allows for representing regions of interest with sparse Gaussians, aided by scalability and semantic attention maps.

Given semantic attention cues from VLMs, we assign semantic attributes and corresponding scaling factors to each Gaussian. The blended semantic category of Gaussians can be obtained via $\alpha$-blending:
\begin{equation}
\begin{aligned}
    \Gamma_i = \sum_{i=1}^{N} \mathrm{softmax}(\gamma_i) \alpha_{i} \prod_{j=1}^{i-1}(1-\alpha_{j}) ,
\end{aligned}
\end{equation}
where $\Gamma_i$ is the rendered semantic for each pixel, weighted by the Gaussians' semantic attributes $\gamma$ and opacity $\alpha$. 
%
The scaling factor needs to be linearly related to the space occupied by each Gaussian, which cannot be simply calculated from the Gaussian centroid position $\{o_x,o_y,o_z\}$ due to the variations in anisotropic shape and spatial overlap. Thus, we estimate the occupied range of each Gaussian by considering the distance from the nearest tangent surface of the Gaussian ellipsoid to the voxel as:
\begin{equation}
\begin{aligned}
    d = o_z - \cfrac{\eta^{-1}\Sigma_{0,2}^{-1}(o_x - \kappa_x) + \eta^{-1}\Sigma_{1,2}^{-1}(o_y - \kappa_y)}{\Sigma_{2,2}^{-1}} ,
\end{aligned}
\end{equation}
where $d$ is the occupied depth from the voxel to the Gaussian ellipsoid, $\eta$ is the ray direction from the voxel center $k =(\kappa_x, \kappa_y, \kappa_z)$ to the Gaussian. $\Sigma$ is the covariance matrix, with $\Sigma_{i,j}$ denoting the corresponding matrix elements.
%
The Gaussian value $G(x)$ can be formulated as:
\begin{equation}
\begin{aligned}
    G(x) = e^{-\frac{1}{2}(\kappa-o)^{\top}\Sigma^{-1}(\kappa-o)} .
\end{aligned}
\end{equation}
The scaling factor is then adaptively adjusted based on the gradients of Gaussian values and the occupied range of the Gaussians. Notably, Gaussians of the same semantic category share similar scaling factor ranges, as objects with the same semantics exhibit comparable scales and geometries. As shown in Figure~\ref{fig:gaussians}, semantic-aware scalable gaussians enable the representation of large background areas (e.g., buildings) with sparse gaussians at a larger scale, while capturing finer geometries (e.g., cyclist) with denser gaussians at a smaller scale.






\vspace{-4mm}
\paragraph{Self-estimated Flow for Dynamic Objects.}
Dynamic objects could cause trailing effects due to temporal variations, thereby reducing the accuracy of occupancy annotation. Independently handling dynamic objects facilitates the enhancement of temporal and spatial consistency in semantics. Thus, we introduce a self-estimated 3D flow module, which is used to capture and aggregate dynamic objects. We also assign dynamic attributes to dynamic Gaussians to better model the motion of objects.

Specifically, we model the translation of each Gaussian kernel $p$ from time $t$ to time $t+ \Delta t$ as a flow vector $f$. Our goal is to minimize the point distances between object's source points $U_1$ and target points $U_2$ to estimate the flow by applying Chamfer distance (CD)~\cite{fan2017point}. Since the same dynamic object is often represented by spatially adjacent Gaussians with the same semantics, we search for correspondences between paired points among the nearest Gaussian neighbors that share the same semantic:
\begin{equation}
\begin{aligned}
    CD(p,p^{\prime}) = \sum_{p \in U_1}\mathop{\min}_{p^{\prime} \in U_2}||p-p^{\prime} ||_2^2 + \sum_{p^{\prime} \in U_2}\mathop{\min}_{p \in U_1}||p^{\prime} -p||_2^2 ,
\end{aligned}
\end{equation}
where $p$ and $p^{\prime}$ are the position of Gaussian kernels with the same semantic at time $t$ and $t+ \Delta t$, respectively.
%
we define a dynamic indicator function between paired Gaussians to determine whether an object is in motion:
\begin{equation}
\begin{aligned}
    \mathbbm{1}(D) = \rho - \frac{1}{m} \sum_{i=1}^{m} \|p_{t+\Delta t}^{i} - p_t^{j}\|_{2} ,
\end{aligned}
\end{equation}
where $D$ is the average distance between paired Gaussians with the same semantic, $\rho$ is the dynamic threshold, $m$ denotes the number of Gaussian ellipsoids. The centroid position at the $i$-th frame is denoted by $o_i$. Subsequently, we aggregate all temporally paired Gaussians based on semantic attention map and motion cues.






\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{nuscenespress.pdf}
  \vspace{-2mm}
  \caption{\textbf{Qualitative results of semantic occupancy annotation on Occ3D-nuScenes~\cite{tian2024occ3d}.} Our method achieves annotation accuracy and completeness comparable to human labeling, outperforming current multi-stage offline and self-supervised semantic occupancy ground truth generation pipelines. \ourmethod{} demonstrates good performance in capturing fine-grained geometry, ensuring semantic consistency, and handling temporal dynamics.}
  \label{figure:compare}
  \afterfig
 \end{figure*}



 


\vspace{-4mm}
\paragraph{Geometric constraints from LiDAR.}
LiDAR points are widely used by existing occupancy annotation methods due to their precise geometric priors. Our pipeline also supports the use of LiDAR to obtain geometric constraints and continuously optimize the distribution of Gaussians.

Similar to~\cite{zhao2020fusion, zhao2023lif}, a point $p_{i,t}$ in the LiDAR sweep $L_t$ is projected onto the frame $I_t$, and its initial semantic label can be obtained by $K^{-1}[R^\top \phi_{x,y,t} + T]$, where $(K, R, T)$ are the corresponding camera parameters and homogenous transformation matrix, and $\phi$ is the pixel-level semantic label.
We aggregate the multi-frame of LiDAR points over time and compute the anchor centers $p_c = (x_c^i, y_c^i, z_c^i)$. We then implement a geometry-aware loss to enforce the alignment of Gaussian ellipsoid distributions with the geometric priors of their corresponding semantic regions:
\begin{equation}
\begin{aligned}
    L_{geo} = -\sum_{c=1}^{C} \sum_{i=1}^{M} \frac{1}{\| o_c(i) - p_c(i) \|_{2}^{2}} ,
\end{aligned}
\end{equation}
where $C$ denotes the number of semantic categories, $M$ is the number of Gaussian ellipsoid centers within the anchor range, and $o_i$ is the coordinate of the $i$-th Gaussian center.
%


\vspace{-4mm}
\paragraph{Cumulative GS-Voxel Splatting.}
Finally, we cumulatively splat VL-GS onto the voxel grid at an arbitrary voxel size, with each voxel's semantic label determined by weighting the occupied range and opacity from Gaussians:
\begin{equation}
\begin{aligned}
    \digamma(o) = \sum_{i=1}^{N} d_i G(x_i) \alpha_{i} \mathrm{softmax}(\gamma_i) ,
\end{aligned}
\end{equation}
where $d_i$ is the occupied depth of the Gaussian-to-3D voxel, treated as the splatting weight coefficient. $\alpha_{i}$ is the opacity, and $\mathrm{softmax}(\gamma_i)$ computes the semantic probability. 


























