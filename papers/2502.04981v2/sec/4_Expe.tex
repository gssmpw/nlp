\definecolor{nbarrier}{RGB}{255, 120, 50}
\definecolor{nbicycle}{RGB}{255, 192, 203}
\definecolor{nbus}{RGB}{255, 255, 0}
\definecolor{ncar}{RGB}{0, 150, 245}
\definecolor{nconstruct}{RGB}{0, 255, 255}
\definecolor{nmotor}{RGB}{200, 180, 0}
\definecolor{npedestrian}{RGB}{255, 0, 0}
\definecolor{ntraffic}{RGB}{255, 240, 150}
\definecolor{ntrailer}{RGB}{135, 60, 0}
\definecolor{ntruck}{RGB}{160, 32, 240}
\definecolor{ndriveable}{RGB}{255, 0, 255}
\definecolor{nother}{RGB}{139, 137, 137}
\definecolor{nsidewalk}{RGB}{75, 0, 75}
\definecolor{nterrain}{RGB}{150, 240, 80}
\definecolor{nmanmade}{RGB}{213, 213, 213}
\definecolor{nvegetation}{RGB}{0, 175, 0}
\definecolor{nfence}{RGB}{35, 135, 230}
\definecolor{ntrunk}{RGB}{195,85,85}
\definecolor{npole}{RGB}{213, 0, 139}


\section{Experiments}
\label{sec:expe}





\begin{table*}[t]
\footnotesize
\setlength{\tabcolsep}{0.002\linewidth}
\centering
\caption{
\textbf{Zero-shot cross dataset performance on  SemanticKITTI~\cite{behley2019semantickitti}.} Other-veh. and moto-cyc. are short for Occ3D-nuScenes, other-vehicle, and motorcycles, respectively. Novel class refers to unseen semantics, while base class includes those seen during training. Metric mIoU-base denotes the mIoU computed solely on base classes.
} 
\vspace{-3mm}
\begin{tabular}{l | c c c | c c c c c c c | c c c c c c c c c c c | c}

    \toprule
    \multicolumn{4}{c}{\textbf{(a) Val: SemanticKITTI}} & \multicolumn{7}{c}{\textbf{(b) Novel Class}} & \multicolumn{12}{c}{\textbf{(c) Base Class}}  \\
    \hline
    Method 
    & \rotatebox{90}{Input}
    & \rotatebox{90}{IoU $\uparrow$}
    & \rotatebox{90}{mIoU $\uparrow$}
    & \rotatebox{90}{\textcolor{ntrailer}{$\blacksquare$} bicyclist} %
    & \rotatebox{90}{\textcolor{nbus}{$\blacksquare$} moto-cyc.} %
    & \rotatebox{90}{\textcolor{nbarrier}{$\blacksquare$} parking} %
    & \rotatebox{90}{\textcolor{nfence}{$\blacksquare$} fence} %
    & \rotatebox{90}{\textcolor{ntrunk}{$\blacksquare$} trunk} %
    & \rotatebox{90}{\textcolor{npole}{$\blacksquare$} pole} %
    & \rotatebox{90}{\textcolor{ntraffic}{$\blacksquare$} traffic-sign} %
    & \rotatebox{90}{\textcolor{nbicycle}{$\blacksquare$} bicycle} %
    & \rotatebox{90}{\textcolor{ncar}{$\blacksquare$} car} %
    & \rotatebox{90}{\textcolor{nconstruct}{$\blacksquare$} other-veh.} %
    & \rotatebox{90}{\textcolor{nmotor}{$\blacksquare$} motorcycle} %
    & \rotatebox{90}{\textcolor{npedestrian}{$\blacksquare$} pedestrian} %
    & \rotatebox{90}{\textcolor{ntruck}{$\blacksquare$} truck} %
    & \rotatebox{90}{\textcolor{ndriveable}{$\blacksquare$} road} %
    & \rotatebox{90}{\textcolor{nsidewalk}{$\blacksquare$} sidewalk} %
    & \rotatebox{90}{\textcolor{nterrain}{$\blacksquare$} terrain} %
    & \rotatebox{90}{\textcolor{nmanmade}{$\blacksquare$} building} %
    & \rotatebox{90}{\textcolor{nvegetation}{$\blacksquare$} vegetation}
    & \rotatebox{90}{mIoU-base $\uparrow$}

    \\ %
    \midrule
    
 %\rowcolor{LGray} 
\rowcolor{LGray} GaussianOcc~\cite{wan2024gaussianocc} & C & 22.42 & 4.18 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.33 & 7.10 & 2.81 & 3.06 & 2.91 & 3.42 & 15.80 & 10.43 & 3.78 & 2.55 & 22.11 & 6.84 \\
%
 %\rowcolor{LGray}
OVO~\cite{tan2023ovo} & C & 20.94 & 5.83 & 0.90 & 0.0 & 0.68 & 3.50 & 2.31 & 0.60 & 2.20 & 0.40 & 12.70 & 3.50 & 0.20 & 0.74 & 0.70 & 19.44 & 24.81 & 4.86 & 11.70 & 15.62 & 8.61 \\
%
\rowcolor{LGray} SurroundOcc~\cite{wei2023surroundocc} & L & 27.83 & 6.39 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 1.52 & 23.19 & 4.81 & 6.71 & 4.37 & 3.16 & 24.32 & 11.98 & 9.95 & 5.79 & 19.14 & 10.45 \\
%
VLM-LiDAR & C\&L & 28.12 & 5.32 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 2.04 & 19.17 & 3.31 & 2.13 & 2.64 & 5.89 & 19.02 & 16.58 & 6.31 & 3.59 & 14.98 & 8.69 \\
%
\rowcolor{violet!10} \textbf{\ourmethod{}-V}   & C & \underline{35.64} & \underline{9.36} & 1.38 & 3.60 & 0.59 & 4.34 & 5.36 & 14.32 & 6.62 & 4.71 & 22.29 & 3.89 & 10.35 & 7.54 & 8.78 & 26.14 & 15.66 & 9.84 & 4.14 & 18.87 & \underline{12.02} \\
%
\rowcolor{violet!10} \textbf{\ourmethod{}-M}   & C\&L & \underline{41.23} & \underline{12.76} & 1.27 & 5.23 & 0.33 & 5.71 & 5.97 & 15.17 & 8.72 & 7.83 & 24.60 & 4.92 & 9.30 & 11.18 & 8.39 & 44.74 & 24.43 & 5.85 & 17.01 & 29.12 & \underline{17.03} \\

    
\bottomrule
\end{tabular}
%\vspace{-5mm}
\label{table:semantic-kitti}
\end{table*}






%\begin{figure*}[ht]
%  \centering
%  \includegraphics[width=0.9\linewidth]{pic/compare-kitti3-press.pdf}
%  \caption{\textbf{Qualitative results of zero-shot validation on SemanticKITTI~\cite{behley2019semantickitti}.} \ourmethod{} achieves promising results in zero-shot cross dataset experiments, achieving high-quality semantic occupancy reconstruction of complex geometries and distant, fine objects.}
%  \label{figure:compare-kitti}
%  \afterfig
% \end{figure*}





\subsection{Implementation Details} 
We use two benchmarks for evaluation: Occ3D-nuScenes, which is used to compare the performance of our method with other occupancy annotation methods for specific categories, while SemanticKITTI is used to assess the zero-shot capability across datasets and unseen categories.
We set the resolutions of images as 900 × 1600 for Occ3D-nuScenes and 370 × 1226 for SemanticKITTI. During optimization, we scale the image size to 225 × 400 and double it every 300 steps until reaching the original resolution. Same as~\cite{lin2024training}, we chose CogVLM-17B~\cite{wang2024cogvlm} with EVA2-CLIP-E~\cite{sun2023eva} and Vicuna-7B-v1.5~\cite{chiang2023vicuna} as the vision-language model. We follow GenerateU~\cite{lin2024generative} to adopt CLIP~\cite{radford2021learning} text encoder and map the generated categories to predefined categories in datasets for evaluation. 
We use the AdamW optimizer for optimization with an initial learning rate of 0.005. The learning rate for the position parameters decays every 250 steps with a decay rate of 0.98.

\subsection{Performance Evluation and Analysis}
We evaluate our method against the state-of-the-art (SOTA) methods for automatic semantic occupancy annotation, including offline methods~\cite{wei2023surroundocc, lu2023ovir, Wang_2023_ICCV} and self-supervised online methods~\cite{wan2024gaussianocc, boeder2024langocc, zheng2025veon}.

\vspace{-4mm}
\paragraph{Compared with point-based voxelization pipelines.}
Point-based voxelization annotation pipelines directly use LiDAR with 3D annotations (semantic points and 3D bounding boxes) as input. SurroundOcc~\cite{wei2023surroundocc} performs mesh reconstruction and nearest neighbors algorithm to densify semantic points. OpenOcc~\cite{Wang_2023_ICCV} proposes the AAP pipeline to densify the voxel, followed by human post-processing to purify artifacts. For fair comparisons, we replicate these methods by replacing the manually annotated results with the semantic point clouds projected from VLMs. As shown in Table~\ref{table:occ3d-nusc}, our vision-centric method outperforms these pipelines that utilize LiDAR point clouds.

\vspace{-4mm}
\paragraph{Compared with 2D-to-3D projection methods.}
Projecting annotated or generated 2D labels back onto 3D representation is a natural idea, which is further refined by several methods~\cite{xu2023sampro3d, lu2023ovir}. However, these methods rely on pre-built 3D representations (e.g., point clouds or mesh) and employ multi-stage post-processing, including voting, filtering, and merging, to eliminate overlapping information. Undoubtedly, this strategy leads to the loss of crucial details and misalignment between semantics and representations. \ourmethod{} performs well against SAMPro3D~\cite{xu2023sampro3d} and OVIR-3D~\cite{lu2023ovir}, both of which project the outputs of SAM~\cite{ren2024grounded} onto 3D point clouds. 
%
We also design a baseline that directly projects the results of VLM and SAM onto LiDAR point clouds (VLM-LiDAR) and voxelizes them into semantic occupancy.
%
Table~\ref{table:occ3d-nusc} shows that still demonstrates better performance, based on the deep integration of VLM guidance and differentiable reconstruction.

\vspace{-4mm}
\paragraph{Compared with self-supervised methods.}
Self-supervised methods enable occupancy estimation from image features without relying on manual annotations. For a fair comparison, we extend existing self-supervised approaches by incorporating image sequences as historical frames and performing multi-frame feature aggregation. We further perform temporal fusion of the above outputs in the global coordinate system. As shown in Table~\ref{table:occ3d-nusc}, using pure visual input, our method outperforms GaussianOcc~\cite{wan2024gaussianocc}, which utilizes vanilla GS as an intermediate representation.
%
\ourmethod{} also performs well against LangOcc~\cite{boeder2024langocc} and VEON~\cite{zheng2025veon}, which are specifically designed for open-vocabulary occupancy estimation in surrounding-view scenes. 
%
While the aforementioned approaches do not require additional supervision, they struggle with efficiently modeling semantic geometry and neglect dynamic objects, leading to performance degradation.

\vspace{-4mm}
\paragraph{Qualitative results.} 
Figure~\ref{figure:compare} and~\ref{fig:day} shows that our method excels in semantic occupancy annotation, showcasing superior scene completeness, consistency, and dynamic object handling, even without the use of LiDAR. In extreme weather conditions (e.g., rain and nighttime), our method maintains robust performance, achieving annotation results comparable to or even surpassing manually labeled ground truth. For instance, in areas where ground truth is missing due to rain, \ourmethod{} successfully reconstructs both the geometry and semantics of the road surface.


\subsection{Zero-shot and Generalization Ability}
SemanticKITTI differs from Occ3D-nuScenes in terms of semantic categories, sensor parameters, camera distribution, and voxel size. We evaluate on SemanticKITTI to verify the zero-shot and cross-dataset generalization capability.
%

\begin{table*}[ht]
\footnotesize
    \renewcommand\arraystretch{1.2}
    \setlength{\tabcolsep}{0.015\linewidth}
    \centering
    \caption{\textbf{Comparisons of annotation efficiency.} Open-ended stands for the annotation capability for undefined classes. Label-free means training without any human-labeled annotations. $\dagger$ indicates the use of VLMs to obtain 2D semantics instead of human labeling.
    }
    \label{tab:methods_comparison}
    \vspace{-3mm}
    \begin{tabular}{lcccccccccc}
    \Xhline{0.75pt}
    Method & Anno. Time & Input Modality & Representation & Memory &  Number & Open-Ended & Label-Free\\
        \hline


\rowcolor{LGray}    Auto+Human~\cite{Wang_2023_ICCV} & 4000+ human hours & L & Point Cloud & - & 1.2 M & \textcolor{red}{\usym{2717}} & \textcolor{red}{\usym{2717}} \\
%
    GaussianOcc~\cite{wan2024gaussianocc} $\dagger$ & $\approx$60 GPU hours & C & Vanilla GS & 32 G & 0.8 M & \textcolor{red}{\usym{2717}} & \textcolor{ForestGreen}{\usym{2713}}\\
%
\rowcolor{LGray}    \rowcolor{LGray} SurroundOcc~\cite{wei2023surroundocc} $\dagger$ & 1000+ GPU hours & L & Mesh \& Voxel & 73 G & 3.0 M & \textcolor{red}{\usym{2717}} & \textcolor{red}{\usym{2717}} \\
%
    VLM-LiDAR $\dagger$ & $\approx$50 GPU hours & C\&L & Point Cloud & 34 G & 1.2 M & \textcolor{red}{\usym{2717}}  & \textcolor{red}{\usym{2717}} \\
    %
    % \arrayrulecolor{black}
    % \cdashline{1-8}[1.5pt/4pt]
%
    \rowcolor{violet!10} \textbf{Ours} $\dagger$ & $\approx$30 GPU hours & C or C\&L & VL-GS & 5.0 G & 0.3 M & \textcolor{ForestGreen}{\usym{2713}} & \textcolor{ForestGreen}{\usym{2713}} \\
        
    \Xhline{0.75pt}
    \end{tabular}
    \label{tab:efficiency}
    \vspace{-2mm}
\end{table*}


%-----------------------------------------------------
To evaluate the zero-shot and open-ended semantic annotation ability, we select novel classes from SemanticKITTI as the test set, which are not visible during the annotation process.
%
Table~\ref{table:semantic-kitti} shows that all self-supervised methods~\cite{ wan2024gaussianocc, tan2023ovo} suffer significant performance degradation, as they are tailored to specific camera parameters and occupancy distributions.
For novel classes unseen during learning, these methods fail to label undefined semantic occupancy.
%
Compared to offline annotation pipelines, including point-based voxelization and semantic projection, our method shows better robustness and enhanced capability for open-ended semantic annotation.

\begin{figure}[ht]
  \centering
   % \vspace{-10pt}
  \includegraphics[width=0.9\linewidth]{daypress.pdf}
  \caption{\textbf{Qualitative comparison} of our method with human annotations under complex lighting and extreme weather conditions.}
  \label{fig:day}
  \vspace{-4mm}
\end{figure}


%-----------------------------------------------------

%HERE
\subsection{Annotation Efficiency}
Table~\ref{tab:efficiency} presents evaluations on representation characteristics and model efficiency. 
Notably, \ourmethod{} demonstrates an advantage in computational cost, delivering better performance with reduced memory requirements. 
In contrast, scene representations based on dense voxels and Point Cloud incur redundant computational costs.
In addition, \ourmethod{} strikes a balance between efficiency and flexibility, enabling open-ended scene-aware occupancy reconstruction, supporting open-vocabulary semantic occupancy annotation, and requiring no human-labeled annotations.

\begin{figure}[ht]
  \centering
   % \vspace{-10pt}
  \includegraphics[width=0.9\linewidth]{dynamic3press.pdf}
  \caption{\textbf{Semantic occupancy of dynamics.} \ourmethod{} accurately annotate semantic occupancy of dynamic objects, maintains spatiotemporal consistency, and infers occluded parts.}
  \label{fig:dynamic}
  \vspace{-4mm}
\end{figure}


% 'Ablation_Study'
\begin{table}[!t]
  \centering
  \caption{
  {\textbf{Effect of each module in our method.} SFM is short for the self-estimated flow module and SSG denots the employment of the semantic-aware scalable gaussians.}}
    \footnotesize
  \centering
  \setlength{\tabcolsep}{8mm}{
  % \resizebox{0.46\textwidth}{!}
  {
  \vspace{-3mm}
    \begin{tabular}{ccc}
    % \hline
 \toprule
    \textbf{Model} & \textbf{IoU $\uparrow$} & \textbf{mIoU $\uparrow$} \\
    \hline % \hline
     w/o SFM             & 82.65     & 16.84  \\
     w/o $L_{geo}$      &  81.49     &  20.36 \\
     w/o SSG  &  80.27     &  17.67 \\
    % \rowcolor{cyan} 
    \ourmethod{}-V               & 83.01 & 20.92  \\
    % \rowcolor{cyan} 
     \ourmethod{}-M               &  88.62  & 25.84   \\
    \hline
    \end{tabular}%
    }
    }
  \aftertab
  \label{tab:Ablation}%
  \vspace{-2mm}
\end{table}%




\subsection{Ablation Studies}
We analyze the effect of self-estimated flow module for dynamic objects by disabling the clustering of dynamic objects and optimizing them together with static foregrounds. Figure~\ref{fig:dynamic} shows that the self-estimated flow module effectively mitigates the challenges of dynamic trailing and spatial occlusion in the annotation of occupancy.
We further ablate the effect of LiDAR geometric priors and semantic-aware scalable Gaussians by either removing the $L_{geo}$ loss or replacing our SSG with vanilla Gaussians in our framework.
The degraded results highlight the importance of these modules in constraining the shape and distribution of Gaussians, thereby enabling a more accurate reconstruction of the overall scene structure.  
%
More quantitative and qualitative results are available in the supplementary material.




