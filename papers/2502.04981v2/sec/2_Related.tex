\section{Related Work}
\label{sec:formatting}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=.9\linewidth]{overviewpress.pdf}
  \caption{\textbf{Overall pipeline of our method.} \ourmethod{} is a vision-centric automated pipeline for semantic occupancy annotation. Our method starts with multi-view image inputs (optionally with LiDAR), extracts semantic attention maps from VLMs, and refines a dynamic semantic query list. We then propose Vision-Language Guided Gaussian Splatting (VL-GS), incorporating semantic-aware scalable Gaussians and self-estimated flow for dynamic objects. The final occupancy annotation is generated through a forward-pass Cumulative GS-Voxel Splatting. \ourmethod{} demonstrates strong generalization and open-ended annotation capabilities without relying on manual priors or LiDAR.
  }
  \label{fig:overview}
  \afterfig
\end{figure*}

\noindent {\bf{Semantic Occupancy Annotation.}}
%
Semantic occupancy annotation aims to label semantic 0-1 occupancy from sensor data. However, current automated and semi-automated methods~\cite{wei2023surroundocc, Wang_2023_ICCV, tong2023scene} heavily rely on LiDAR point clouds and human pre-annotated 2D or 3D ground truth. Most of these methods also require time-consuming post-processing and expensive manual purification. In contrast, we design a vision-centric fully automated occupancy annotation pipeline that eliminates the reliance on LiDAR. Our method also integrates VLMs and VFMs, supporting open-ended semantic category annotation.

\noindent {\bf{3D Occupancy Estimation.}}
Semantic 3D occupancy estimation~\cite{tian2024occ3d, zhang2023occformer, Wang_2023_ICCV} aims to estimate the occupancy states and semantics of complex scenes, which is crucial for 3D perception and planning.
%
Existing learning-based occupancy models~\cite{li2022bevformer, shi2024occupancy, zhang2023occformer, huang2023tri, li2023fb, pan2023uniocc, yu2023flashocc} are heavily reliant on the extensive labeled training data generated by annotation pipelines.
%
Recent advances in self-supervised methods~\cite{huang2024selfocc, zhang2023occnerf, boeder2024occflownet, wan2024gaussianocc, boeder2024langocc} for estimating 3D occupancy have diminished reliance on costly annotations and can be regarded as online occupancy labeling techniques.
%
However, these approaches introduce ambiguity and illusions, resulting in misaligned geometry and temporal inconsistencies due to their limited awareness of intricate spatial structures and dynamic objects. They are also hampered by limited cross-dataset and scene-aware generalization capabilities.

To address these limitations, we propose a reconstruction-based occupancy annotation framework that requires no manual 2D or 3D annotations, achieving high-precision open-ended understanding, zero-shot learning, and cross-dataset generalization.

\vspace{1mm}
\noindent {\bf{Scene Representation and Reconstruction.}}
Efficient scene representation is the core to occupancy annotation.
%
Dense voxel-based methods~\cite{li2022unifying, chen2023voxelnext, li2022voxel, cao2022monoscene} assign each voxel a feature vector, inevitably suffering from high computational cost due to redundant grids. 
%
As a compressed representation, BEV~\cite{harley2023simple, man2023bev, chambon2024pointbev, li2024fast} encodes 3D information on the ground plane, but struggles to capture diverse 3D geometry using flattened vectors.
%
By implicitly modeling 3D space, ~\cite{zhang2023occnerf, huang2024selfocc, gan2024comprehensive, pan2024renderocc} create a NeRF-style 3D volume to estimate scene occupancy. 
However, the continuous implicit neural fields struggle with modeling complex dynamic scenes, and dense sampling leads to redundant, memory-intensive operations.
%
Most recently, 3D Gaussian splatting (3DGS)~\cite{kerbl20233d, yu2024mip, qin2024langsplat} has demonstrated its powerful capability in reconstruction, even for driving scenes~\cite{zhou2024drivinggaussian, tian2024drivingforward, fischer2024dynamic}. 
%
By treating each vertex as a Gaussian, ~\cite{wan2024gaussianocc} adopts a self-supervised approach for occupancy estimation but results in a dramatic increase in computational cost.

In contrast to prior art, we propose VL-GS, specifically designed to reconstruct semantic instances and dynamic objects, leveraging semantic attention clues from vision-language models. As a more efficient representation, VL-GS achieves high precision and versatile occupancy annotation with reduced cost.

\noindent {\bf Open-World Understanding.} 
\label{open}
Existing open-world understanding methods~\cite{wu2024towards} are confined to 2D images and can be broadly classified into two types: open-set~\cite{scheirer2012toward} and open-ended~\cite{lin2024generative}. Open-set methods~\cite{li2022grounded, liu2024grounding, cheng2024yolo} focus on text-image embedding matching using a predefined vocabulary bank. In contrast, open-ended methods~\cite{lin2024training, lin2024training} continuously update observed object categories via language models. The key difference lies in the reliance on predefined categories, which allows open-ended approaches to produce more precise and comprehensive semantic representations, ultimately enhancing semantic occupancy annotation in open-world scenarios.

\vspace{1mm}
\noindent {\bf VLMs and VFMs.}
Vision language models (VLM)~\cite{kuo2022f, xu2021vlm} and visual foundation models (VFMs)~\cite{ravi2024sam, scheirer2012toward} have shown promising results and generalization ability in various visual tasks.
However, their application to 3D occupancy annotation has received limited attention.
Unlike direct training with 3D annotations, existing foundational vision models~\cite{kirillov2023segment, ke2024segment, liu2023grounding, ren2024grounded} are primarily trained on 2D images, which may challenge the consistency of 3D occupancy across different cameras and frames.
%
In this work, we explore the potential of applying VLMs~\cite{kirillov2023segment, liu2023grounding, ren2024grounded} to occupancy annotation.