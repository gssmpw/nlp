\section{Introduction}
\label{sec:intro}
3D semantic occupancy has attracted a considerable amount of attention in autonomous driving~\cite{wang2021learning, tong2023scene, wang2024panoocc} and embodied intelligence~\cite{ramakrishnan2020occupancy, ramakrishnan2021exploration, chaplot2021seal}, demonstrating great potential to facilitate understanding of 3D scenes and perception of irregular objects. Despite its promising applications, automatic generation of precise and complete semantic occupancy annotations from raw sensor data remains a fundamental challenge, particularly in the pursuit of cost-effective solutions for real-world deployment.

Vision-centric automated 3D semantic occupancy annotation has long been undervalued, while existing occupancy annotation pipelines heavily rely on LiDAR point clouds (Table~\ref{tab:labeling}), requiring human pre-annotations and labor-intensive post-processing (over 4k+ human hours for nuScenes~\cite{tong2023scene}).
%
Current automated or semi-automated annotation pipelines primarily follow three paths. 
(1) Automated-assisted manual annotation, which is labor-intensive and costly. 
(2) Point cloud voxelization guided by manual annotation priors relies heavily on manual priors and multi-stage post-processing, making it time-consuming. 
(3) 2D-to-3D projection-based methods, which simply merge 2D segmentation results into 3D point clouds or meshes, struggle to ensure precise 3D consistency.
%
These annotation methods heavily rely on LiDAR point clouds while overlooking semantic and geometric cues from multi-view images. Given that LiDAR point clouds are inherently sparse and incomplete, they are insufficient for comprehensive scene modeling.
%
These approaches also employ voxel-based scene representations that require excessive parameters and incur redundant computational costs.
%
Recent self-supervised occupancy models~\cite{huang2024selfocc, gan2024comprehensive, zhang2023occnerf, boeder2024occflownet, wan2024gaussianocc} have eliminated the need for extensive labeled training data by leveraging 2D features from image inputs and semantic information from visual foundation models (VFMs), such as SAM~\cite{kirillov2023segment} and OpenSeed~\cite{zhang2023simple}. Nevertheless, these methods struggle to ensure complete, consistent scene occupancy, and exhibit limited generalization across diverse scenes. 

Additionally, these pipelines are all confined to closed-set or open-set occupancy classes that require predefined categories. However, real-world scenes often involve open-ended occupancyâ€”objects outside any predefined category, making it unwise to label all undefined semantics as ``others.'' For example, self-driving vehicles may encounter collapsed poles or plastic sheets on road surfaces that require distinct occupancy annotations for safe driving strategies.

%-------------------------------------------------------------------------



\begin{table*}[ht]
\footnotesize
    \renewcommand\arraystretch{1.2}
    \setlength{\tabcolsep}{0.015\linewidth}
    \centering
    \caption{\textbf{Comparisons between \ourmethod{} and existing semantic occupancy annotation pipelines.} The definitions of closed-set, open-set, and open-ended are introduced in Section~\ref{open}. Our method achieves high-quality occupancy annotation without additional manual labeling or post-processing while maintaining superior speed and generalization. C represents camera, and L denotes LiDAR.
    }
    \label{tab:labeling_comparison}
    \vspace{-3mm}
    \begin{tabular}{l|ccccccccc}
    \Xhline{0.75pt}
    Method & Categories
 & Modality
 & Manual-label
 & Post-processing
 & Speed
 % & Label-free
 & Zero-shot
 & Dynamic
\\
        \hline

    Point-based Voxelization~\cite{wei2023surroundocc, Wang_2023_ICCV,tong2023scene} & Close-set & L & 3D GT & Human & Slow & \textcolor{red}{\usym{2717}} & \textcolor{ForestGreen}{\usym{2713}} \\
    2D-to-3D Projection~\cite{lu2023ovir, zhang2023sam3d} & Close/Open-set & C\&L & 2D GT & Auto\&Human & Slow & \textcolor{red}{\usym{2717}} & \textcolor{red}{\usym{2717}} \\
    \rowcolor{violet!10} \textbf{Ours (AutoOcc)} & Open-ended & C or C\&L & N/A & N/A & Fast & \textcolor{ForestGreen}{\usym{2713}} & \textcolor{ForestGreen}{\usym{2713}} \\
        
    \Xhline{0.75pt}
    \end{tabular}
    \label{tab:labeling}
    \vspace{-2mm}
\end{table*}


%------------------------------------------------------------------------

To address these limitations, we present \ourmethod{}, a fully automated framework for open-ended semantic occupancy annotation that requires neither manual labeling nor predefined categories.
%
To achieve open-ended semantic occupancy labeling, we employ semantic attention maps generated by vision-language models (VLMs) to describe the scene, constructing a continuously evolving semantic query list. The generated attention maps are used simultaneously to prompt segmentation in SAM and guide instance-level depth estimation from UniDepth, thereby eliminating the need for manual annotations. We further introduce a self-estimated flow module to identify and manage dynamic objects in temporal rendering. We further propose Gaussian Splatting with open-ended semantic awareness (VL-GS) as an intermediate representation, offering a more comprehensive modeling, improved spatiotemporal consistency, and finer geometry with fewer primitives. Compared to densified point clouds and voxels, VL-GS achieves higher representation efficiency, greater accuracy, and reduced memory consumption. The semantic occupancy annotation is then automatically generated end-to-end through cumulative Gaussian-to-voxel splatting.
%
%
Extensive experiments demonstrate that \ourmethod{} outperforms existing automated occupancy annotation methods. Our method further exhibits excellent open-ended and zero-shot generalization capabilities, as evidenced by cross-dataset experiments.
%
Our main contributions include:
\begin{itemize}
\item We present \ourmethod{}, a vision-centric automatic annotation pipeline that supports open-ended semantic occupancy label generation, based on vision-language guided differentiable reconstruction.

\item We devise VL-GS, an efficient and comprehensive scene representation for occupancy annotation. VL-GS integrates vision-language attention with visual foundation models, effectively handles dynamic objects over time, and enhances both spatiotemporal consistency and geometric detail. 

\item \ourmethod{} gains notable improvements over the existing automatic occupancy annotation pipelines, even without relying on manual priors or LiDAR. Our method also demonstrates strong generalization and open-ended understanding capabilities.
\end{itemize}
