\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{setspace}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[numbers]{natbib}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bbm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\jl}[1]{{\color{red}#1}}
 \renewcommand{\Affilfont}{\footnotesize}

%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\bibliographystyle{plainnat}

%%%%%% Title %%%%%%
% Full titles can be a maximum of 200 characters, including spaces. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions
\title{Flow-based Conformal Prediction for Multi-dimensional \\ Time Series}

\author[1]{Junghwan Lee}
\author[1]{Chen Xu}
\author[1]{Yao Xie\footnote{Correspondence: yao.xie@isye.gatech.edu}}

%%%%%% Affiliations %%%%%%
\affil[1]{H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology}

%%%%%% Date %%%%%%
% Date is optional
\date{}

%%%%%% Spacing %%%%%%
% Use paragraph spacing of 1.5 or 2 (for double spacing, use command \doublespacing)
\onehalfspacing

\begin{document}

\maketitle

%%%%%% Abstract %%%%%%
\begin{abstract}

Conformal prediction for time series presents two key challenges: (1) leveraging sequential correlations in features and non-conformity scores and (2) handling multi-dimensional outcomes. We propose a novel conformal prediction method to address these two key challenges by integrating Transformer and Normalizing Flow. Specifically, the Transformer encodes the historical context of time series, and normalizing flow learns the transformation from the base distribution to the distribution of non-conformity scores conditioned on the encoded historical context. This enables the construction of prediction regions by transforming samples from the base distribution using the learned conditional flow. We ensure the marginal coverage by defining the prediction regions as sets in the transformed space that correspond to a predefined probability mass in the base distribution. The model is trained end-to-end by Flow Matching, avoiding the need for computationally intensive numerical solutions of ordinary differential equations. We demonstrate that our proposed method achieves smaller prediction regions compared to the baselines while satisfying the desired coverage through comprehensive experiments using simulated and real-world time series datasets.
\end{abstract}

%%%%%% Main Text %%%%%%
\section{Introduction}
Uncertainty quantification has become essential in many scientific domains where black-box machine learning models are often used~\cite{angelopoulos2021gentle}. Conformal prediction (CP) has gained popularity as a prominent technique for providing valid predictive inference in such models~\cite{shafer2008tutorial,vovk2005algorithmic}. CP relies on three key components to construct predictive intervals or regions: a prediction model $\hat{f}$, features $X$, and outcomes $Y$. By computing non-conformity scores that quantify how atypical potential outputs are, CP generates reliable predictive intervals or regions.

Time series prediction involves forecasting future values based on sequentially ordered observations~\cite{box2015time}. Recent advances in machine learning have led to the development of numerous models for various time series prediction tasks. While the increased use of machine learning methods for time series prediction tasks necessitates uncertainty quantification, CP for time series is challenging due to the violation of the exchangeability assumption inherent to standard CP~\cite{barber2023conformal}. Additionally, time series often show stochastic variations and strong correlations that could result in non-conformity scores also have stochastic variations and correlations. Various CP methods have been developed to overcome the exchangeability assumption for time series: for instance, \citet{xu2023sequential} introduced the Sequential Predictive Conformal Inference (SPCI) framework. Their approach accounts for correlations in prediction residuals to provide more robust prediction intervals by adopting a quantile regression estimator sequentially.

\begin{figure}[tp]
    \centering
    \includegraphics[width=.45\textwidth]{figures/figure1.pdf}
    \caption{Prediction regions constructed by FCP (our method) on the traffic dataset. x-axis indicates the time index. y-axis and z-axis represent the scalar value of features. Black dots are the true outcomes.}
    \label{fig:prediction_regions_showcase}
\end{figure}


Another challenge of CP for time series is that time series data in recent days often contain high-dimensional features and outcomes. While CP methods for univariate outcomes are well-established, CP methods for multidimensional outcomes remain relatively under-explored. Existing studies have proposed methods leveraging non-conformity scores based on various uncertainty sets, such as copulas~\cite{messoudi2021copula} and ellipsoids~\cite{messoudi2022ellipsoidal}. However, these methods rely on the exchangeability assumption, rendering them unsuitable for time series. A few studies seek to extend CP to time series with multidimensional outcomes. For instance, \citet{xu2024conformal} extended the SPCI framework to multi-dimensional time series using ellipsoids. \citet{sun2022copula} introduced a method to construct multi-step prediction regions for time series using copulas, but their approach also assumed exchangeability in the time series segments.

In this study, we propose a novel conformal prediction method designed for multi-dimensional time series. Our method aims to address the aforementioned two key challenges by integrating Transformer~\cite{vaswani2017attention} and Normalizing Flow~\cite{chen2018neural}. The transformer encodes the historical context of the time series, and normalizing flow learns the transformation from the base distribution to the conditional distribution of non-conformity scores conditioned on the encoded historical context. As a result, we can construct a prediction interval using the learned conditional flow. The model is trained end-to-end by Flow Matching~\cite{lipman2022flow}, avoiding the computationally expensive numerical solutions of ordinary differential equations. We demonstrate that our proposed method constructs smaller prediction regions satisfying the target coverage compared to the baselines, including the previous state-of-the-art methods through comprehensive experiments on simulated and real-world multi-dimensional time series datasets.

\begin{figure*}[tp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/wind_region.pdf}
        \caption{Wind dataset}
    \end{subfigure}
    \hfill % Add horizontal space between subfigures
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/traffic_region.pdf}
        \caption{Traffic dataset}
    \end{subfigure}
    \hfill % Add horizontal space between subfigures
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/solar_region.pdf}
        \caption{Solar dataset}
    \end{subfigure}
    \vspace{-2mm}
    \caption{Comparison of the prediction regions with a target coverage of 0.95, constructed by FCP (ours), MultiDimSPCI, and conformal prediction using copulas on (a) the wind dataset, (b) the traffic dataset, and (c) the solar dataset. The prediction regions were manually selected from the test set for clear comparison.}
    \label{fig:comprasion_prediction_regions}
\end{figure*}


\section{Related Works}
\label{sec:related_works}

\subsection{Conformal Prediction for Time Series}

Conformal prediction has gained widespread popularity for its effectiveness in uncertainty quantification for black-box models, requiring only the exchangeability assumption~\cite{vovk2005algorithmic}. However, applying CP methods to time series poses significant challenges, as time series inherently violate the exchangeability assumption due to their sequential and temporal dependencies. 

Numerous studies have extended conformal prediction (CP) beyond the exchangeability assumption. A significant line of research focuses on assigning unequal weights to past non-conformity scores or leveraging their historical context, allowing more informative scores to contribute more effectively. Such works include \citet{xu2023sequential}, \citet{xu2021conformal}, \citet{tibshirani2019conformal}, and \citet{lee2024kernel}. In particular, \citet{xu2023sequential} introduced the Sequential Predictive Conformal Inference (SPCI) framework, which incorporates correlations in non-conformity scores to construct more robust prediction intervals by sequentially adopting a quantile regression estimator. Based on this idea, several studies have employed neural networks to enhance CP for time series. For example, \citet{lee2024transformer} utilized the Transformer~\cite{vaswani2017attention} to capture the correlations in non-conformity scores. \citet{auer2024conformal} proposed HopCPT, which leverages Hopfield networks to achieve a similar objective.


\begin{figure*}[htb]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/region_transformation_covered_y0.pdf}
        \caption{Covered example ($p_0$)}
        \label{fig:covered_y0}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/region_transformation_covered_y1.pdf}
    \caption{Covered example ($q$)}
    \label{fig:covered_y1}
    \end{subfigure}
    
    \vspace{0.5cm}
    \begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/region_transformation_uncovered_y0.pdf}
        \caption{Uncovered example ($p_0$)}
        \label{fig:uncovered_y0}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/region_transformation_uncovered_y1.pdf}
        \caption{Uncovered example ($q$)}
        \label{fig:uncovered_y1}
    \end{subfigure}
    \caption{Transformation of an example between the base distribution ($p_0$) space and the target distribution ($q$) space using the trained conditional flow. The prediction region in the target distribution space (b,d) is visualized by transforming samples from the circumference of the ball with a probability measure of 0.95. The region in the base distribution space (a,c) is obtained by transforming the prediction region back to the base distribution space.}
    \label{fig:transformation}
\end{figure*}

\subsection{Conformal Prediction for Multi-dimensional Data}

Conformal prediction for multi-dimensional data has been actively studied, as modern data often contain multiple variables. One of the simplest approaches involves constructing coordinate-wise prediction intervals with Bonferroni correction. For instance, \citet{stankeviciute2021conformal} applied this idea to generate coordinate-wise prediction intervals for multi-step time series forecasting by adjusting the significance level using Bonferroni correction. A similar approach has been explored for multivariate functional data~\cite{diquigiovanni2022conformal} and multivariate functional time series data~\cite{diquigiovanni2021distribution}.

Recent studies have explored various uncertainty sets, such as copulas and ellipsoids, to construct prediction regions for multidimensional data. For example, \citet{messoudi2021copula} investigated the use of copulas for constructing prediction regions, while \citet{messoudi2022ellipsoidal} and \citet{johnstone2022exact} utilized ellipsoidal uncertainty sets. \citet{sun2022copula} extended the application of copulas to exchangeable time series. \citet{xu2024conformal} applied the SPCI framework to non-conformity scores defined as the radius of ellipsoidal uncertainty sets, leveraging sequential correlations of the non-conformity scores to handle multi-dimensional outcomes.

\citet{anonymous2025contra} used normalizing flow for CP with multi-dimensional outcomes of exchangeable data. They defined non-conformity scores as the distances from the origin and employed split conformal prediction to construct prediction regions. While their study shares some methodological similarities with ours, it differs in two significant aspects: they used discrete normalizing flows and focused exclusively on exchangeable data.


 
\subsection{Probabilistic Forecasting using Deep Learning}

Probabilistic forecasting is a method of prediction that estimates the distribution of outcomes. Unlike typical time series forecasting, which outputs a point prediction, probabilistic forecasting can be used for uncertainty quantification since it outputs the distribution of the outcomes. With recent advances in deep learning, numerous probabilistic forecasting methods have been developed. Among these, DeepAR~\cite{salinas2020deepar} and Temporal Fusion Transformer (TFT)~\cite{lim2021temporal} are widely used methods. DeepAR leverages RNNs and TFT utilizes attention mechanisms to capture the temporal dependencies for probabilistic forecasting. \citet{rasul2020multivariate} also applied conditional normalizing flows for probabilistic forecasting, similar to our approach, but they used a discrete set of normalizing flow layers~\cite{dinh2016density} instead of continuous transformation.



\section{Problem Setup}

We consider a sequence of observations $\{ (x_i, y_i) : i=1,2,\ldots\}$, where $x_t \in \mathbb{R}^{d_x}$ represents $d_x$-dimensional feature, and $y_i \in \mathbb{R}^{d_y}$ represents $d_y$-dimensional continuous scalar outcome.  We assume that we have a base predictor $\hat{f}$ that provides a point prediction $\hat{y}_i$ for $y_i$, given by $\hat{y}_i = \hat{f}(x_i)$. The base predictor $\hat{f}$ can be any black-box model and is not subject to any specific constraints.

Assuming that the first $T$ examples, $\{(x_i, y_i)\}_{i=1}^{T}$, are used for training and validation, our goal is to sequentially construct a prediction region $\hat{C}_{i-1} (z_i)$, beginning at time $T+1$. $z_t$ denotes the features to construct $\hat{C}_{i-1}$. In the most basic setup, $z_i$ only contains $x_i$, but it also can contain other useful features such as past features and outcomes.

The prediction regions are constructed using $z_i$, desirably satisfying the marginal coverage defined as:
\begin{equation}
    \mathbb{P} \left ( {y_i} \in \hat{C}_{i-1} (z_i) \right ) \geq 1 - \alpha, \quad \forall i,
\label{eq:marginal_coverage}
\end{equation}
where $\alpha \in [0,1]$ denotes a pre-defined significance level. Although infinitely large prediction regions trivially satisfy the marginal coverage, they do not provide useful information to quantify uncertainty. Therefore, the goal is to construct \textit{efficient} prediction regionsâ€”the prediction regions with smaller sizes that satisfy the marginal coverage~\cite{vovk2005algorithmic}.

To avoid confusion between the time index of time series and ordinary differential equations, we used the subscript $i$ to denote the time index of time series and subscript $t$ to denote the time index of ordinary differential equations throughout this paper. The time index of the time series is omitted for clarity when it is not necessary.




\section{Method}

\subsection{Preliminary: Conditional Normalizing Flow}

Normalizing Flow aims to learn a bijective mapping $\phi$, referred to as \textit{flow}, to transform a simple base distribution (e.g., Gaussian) $p_0$ into a more complex target distribution $q$ via the change of variables~\cite{rezende2015variational,papamakarios2021normalizing}. Continuous normalizing flow formulates $\phi$ as a time-dependent diffeomorphic
map, $\phi:[0,1] \times \mathbb{R}^{d_x} \to \mathbb{R}^{d_x}$, governed by ordinary differential equation (ODE):
\begin{equation}
    \frac{d}{dt} \phi_t(x) = v_t (\phi_t(x)),
\end{equation}
with the initial condition:
\begin{equation}
    \phi_0(x) = x_,
\end{equation}
where $x \in \mathbb{R}^{d_x}$ is sampled from the base distribution $p_0$, and $v_t: [0,1] \times \mathbb{R}^{d_x} \to \mathbb{R}^{d_x}$ represents the \textit{time-dependent vector field}, which represents the evolution of $\phi$ at a given time $t \in [0,1]$. Here, $\phi_t (x)$ is shorthand notation for $\phi(x,t)$ representing the transformed state of $x$ at time $t$. The vector field $v_t$ is a learnable function that enables the mapping of samples from $p_0$ to the target distribution $q$ by solving the ODE from $t=0$ to $t=1$. Previous studies used neural networks to model the vector field $v_t$, which results in a deep parametric model for flow $\phi$~\cite{chen2018neural,grathwohl2018ffjord}.

Building upon the framework of continuous normalizing flow, we can model a conditional probability distribution $q(x|h)$ via the ODE defined as:
\begin{equation}
    \frac{d}{dt} \phi_{t,h}(x) = v_{t,h} (\phi_{t,h}(x)),
\label{eq:conditional_vector_field}
\end{equation}
with the initial condition:
\begin{equation}
    \phi_{0,h}(x) = x,
\label{eq:conditional_vector_field_init}
\end{equation}
where $h \in \mathbb{R}^{d_h}$ is a conditional vector and $v_{t,h}: [0,1] \times \mathbb{R}^{d_h} \times \mathbb{R}^{d_x} \to \mathbb{R}^{d_x}$ denotes the \textit{time-dependent conditional vector field}, which represents the evolution of $\phi_{t,h}(x)$ at a given time $t$ conditioned on $h$. Here, $\phi_{t,h}(x)$ denotes the transformed state of $x$ at time $t$ conditioned on $h$.



\subsection{Conformal Prediction using Conditional Normalizing Flow}

Let the non-conformity score at time $i$ be defined as $s_i = g( \hat{f}, x_i, y_i)$, where $g$ is a function that outputs non-conformity scores and $s_i \in \mathbb{R}^{d_s}$. In this study, we defined the non-conformity score as prediction residual: $s_i = \hat{f}(x_i) - y_i$. 

%This choice also ensures that $s_i$ and $x \sim p_0$ are both in $\mathbb{R}^{d_y}$, a necessary condition for the transformation $\phi$ to be well-defined. 

To account for potential sequential correlations and dependencies in non-conformity scores, we model their conditional distribution $q(s_i|h_i)$, where $h_i$ is a vector encapsulating the historical context of the time series. This conditional distribution can be effectively learned using the conditional normalizing flow, as defined in equations (\ref{eq:conditional_vector_field}) and (\ref{eq:conditional_vector_field_init}).

An encoder can be used to encode the historical context of time series into $h_i$:
\begin{equation}
    h_i = \text{Encoder}([x_{i-w},\ldots,x_{i}]),
\end{equation}
where $w$ denotes the context window size. We used Transformer~\cite{vaswani2017attention} in our method, though other sequence model architectures, such as recurrent neural networks (RNNs), can also be used.

Using the conditional normalizing flow, we can transform $x \sim p_0$ to $s_i$ given $h_i$ by solving the conditional flow ODE. Similarly, $s_i$ can be transformed back to $x$ by solving the conditional flow ODE in reverse time. This enables the construction of prediction regions defined as:
\begin{equation}
    \hat{C}_{i-1}(z_i) = \left\{ y_i : s_i = \phi_{t=1,h_i}(x), x \in \mathcal{B} \right\},
\label{eq:prediction_region}
\end{equation}
where $\mathcal{B}$ is a set in the base distribution space, and $x$ is sampled from the base distribution. Here, $\phi_{t=1,h_i}(x)$ represents the transformation of $x$ at time $t=1$ conditioned on $h_i$. We ensure that the prediction region $\hat{C}_{i-1}$ preserves the same probability measure of $\mathcal{B}$ under the flow transformation based on Proposition~\ref{proposition:mass_preserving}.
%
\begin{proposition}[Probability measure preserving property of flow] 

Let $\mathcal{A}$ be a set with a probability measure $P(\mathcal{A}) \in [0,1]$, and $\phi$ be a diffeomorphic
map. The set $\mathcal{A}':= \phi(\mathcal{A})$ that is transformed by $\phi$ preserves the probability measure, which is $P(\mathcal{A}) = P(\mathcal{A}')$.

\label{proposition:mass_preserving}
\end{proposition}

To guarantee the marginal coverage of $\hat{C}_{i-1}$ as defined in equation (\ref{eq:prediction_region}), we used an isotropic Gaussian as the base distribution. By defining $\mathcal{B}$ as a ball with a probability measure of $1-\alpha$, we can ensure that the prediction region satisfies the desired coverage $1-\alpha$. Since Proposition~\ref{proposition:mass_preserving} holds, Proposition~\ref{proposition:marginal_coverage} follows.

\begin{proposition}[Marginal coverage] 
    Let $\alpha \in [0,1]$ be a pre-defined significance level. In equation~\ref{eq:prediction_region},  if the ball $\mathcal{B}$ has $1-\alpha$ probability measure, then the prediction region $\hat{C}_{i-1}(z_i)$ satisfies the marginal coverage of $1-\alpha$.
\label{proposition:marginal_coverage}
\end{proposition}
    
    

\subsection{Training via Flow Matching}

We employed multilayer perceptions (MLP) to model the \textit{time-dependent conditional vector field} $v_{t,h}$ and used Transformer as the time series encoder. Since the model architecture includes multiple layers of neural networks, training by maximizing the likelihood of training data imposes a significant computational burden due to the need for numerically solving ODEs during training.~\cite{chen2018neural,grathwohl2018ffjord}. 

Recently, Flow Matching was proposed to train flow models, avoiding the expensive computations~\cite{lipman2022flow}. Therefore, we trained our model using Flow Matching. Flow Matching builds a probability path interpolating between the base distribution and the target distribution and then trains the \textit{vector field} using the defined probability path.

We designed the probability path as follows:
\begin{equation}
    p_{t,h|s}(\cdot|s) := \mathcal{N}(t s, (1-t)^2I),
\label{eq:probability_path}
\end{equation}
where $s$ is the non-conformity score corresponding to a data example, and $t$ is the time index of the conditional flow ODE. This probability path can be interpreted as the probability path conditioning on $s$ and results in the \textit{target vector field}:
\begin{equation}
    u_{t,h}(x|s) = \frac{s - x}{1-t},
\end{equation}
where $x \sim p_0$. As a result, we can train the model end-to-end, via a simple regression objective:
\begin{equation}
\mathcal{L}_{\text{FM}} = \mathbb{E}_{t, p_0, q} \left[ \left\| v_{t,h}(x_t) - u_{t,h}(x_t|s) \right\|^2 \right],
\label{eq:flow_matching_objective}
\end{equation}
where $t \sim \text{Unif}[0,1]$ and $|| \cdot ||$ denotes the $L_2$ norm.


\subsection{Prediction Regions}
\label{sec:prediction_regions}

The prediction regions are determined by the trained flow transformation as in equation~(\ref{eq:prediction_region}); solving the conditional flow ODE is necessary to obtain the prediction regions.

\paragraph{Size of the prediction regions.} 

The size of the prediction regions can be computed as:
\begin{equation}
    \int_{\mathcal{B}} \left| \det(J_{\phi_{t=1,h}}(x)) \right| \, dx, 
\label{eq:prediction_region_size}
\end{equation}
where $J_{\phi_{t=1,h}}$ denotes the Jacobian of $\phi_{t=1,h}$, and $x \in \mathcal{B}$. This can be approximated using Monte Carlo estimation:
\begin{equation}
    \text{Size}(\mathcal{B}) \frac{1}{N}\sum_{j=1}^{N} \left| \det(J_{\phi_{t=1,h}}(x_j)) \right|,
\label{eq:prediction_region_size_estimation}
\end{equation}
where $N$ is the sampling size. However, computing the Jacobian $\phi_{t=1,h}(x)$ requires to solve the flow ODE to obtain $\phi_{t=1,h}(x)$. Consequently, the Jacobian ODE must be defined and solved alongside the conditional flow ODE, with the initial condition $J_{\phi_{t=0,h}}(x) = I$.

The evolution of the log-determinant of the Jacobian is defined as:
\begin{equation}
    \frac{d}{dt} \log |\det J_{\phi_{t,h}}(x)| = \nabla \cdot v_{t,h} (\phi_{t,h}(x)),
\end{equation}
where $\nabla \cdot$ denotes the divergence operator. This log-determinant of the Jacobian ODE can be solved concurrently with the conditional flow ODE. Detailed derivations are provided in Appendix~\ref{app:jacobian_ode}.

%\begin{equation}
   % \frac{d}{dt} J_{\phi_{t,h}}(x) = %\frac{\partial v_{t,h} (\phi_{t,h}(x))}%{\partial \phi_{t,h}(x)} J_{\phi_{t,h}}(x),
%\end{equation}
%with the initial condition:
%\begin{equation}
%    J_{\phi_{0,h}}(x) = 0.
%\end{equation}

\paragraph{Properties of the prediction regions.}
\label{paragraph:properties_prediction_regions}

The prediction regions constructed through flow transformations do not have a definite geometrical shape, such as spheres or ellipsoids. Nonetheless, useful topological properties of the prediction regions can be inferred. Specifically, the prediction regions are guaranteed to be closed and connected based on Proposition~\ref{proposition:closed_connected_set}. Figure~\ref{fig:comprasion_prediction_regions} shows the prediction regions in a 2-dimensional space constructed by our method compared to two other methods based on copula and ellipsoid. The figure visually confirms that the prediction regions constructed by our method are connected and have flexible shapes.


\begin{proposition}[Closed and connected sets under a continuous map, \cite{munkres2000topology}]
Let \( Z \) and \( Y \) be topological spaces, and let \( \psi: Z \to Y \) be a continuous map. If \( E \subset Z \) is closed and connected, then \( \psi(E) \subset Y \) is also closed and connected.
\label{proposition:closed_connected_set}
\end{proposition}

To determine whether a sample $(x_i,y_i)$ is included in the prediction region, the corresponding non-conformity score of the sample must be transformed back to the base distribution space. This is achieved by solving the conditional flow ODE in reverse time. Figure~\ref{fig:transformation} depicts the transformation, where Figures \ref{fig:covered_y0} and \ref{fig:covered_y1} show the transformation of the example covered by the prediction region, and \ref{fig:uncovered_y0} and \ref{fig:uncovered_y1} shows the transformation of the example uncovered by the prediction region.

\paragraph{Coverage guarantee.}
To obtain prediction regions that satisfy the target coverage $1-\alpha$, it is necessary to sample from a set in the base distribution space that has a probability measure of $1-\alpha$ when solving the conditional flow ODE, as described in Proposition~\ref{proposition:marginal_coverage}. 
Since we used an isotropic Gaussian with mean 0 and covariance matrix $\beta I$ as the base distribution $p_0$, where $\beta \in \mathbb{R}^+$ is a scaling factor, the appropriate sampling region is a sphere centered at the origin with radius $r = \lambda \chi_{d_y}^{-1}(1-\alpha)$, where $\chi_{d_y}^{-1}$ is the inverse cumulative distribution function of the chi distribution with $d_y$ degrees of freedom.



\subsection{Computational Complexity}

Let $L$ and $d_m$ denote the number of layers and the model dimension of the Transformer, respectively. Let $K$ be the number of layers in the MLP used to model the vector field $v_{t,h}$. We assume that both the Transformer and MLP share the same hidden dimension $d_m$, and $w \gg d_m$. The computational complexity of the model with this architecture is computed as:
\begin{equation}
    O(Ld_mw^2 + L{d_m}^2w + K{d_m}^2) \approx O(Ld_mw^2),
\end{equation}
where the term $O(Ld_mw^2)$ and $O(L{d_m}^2w)$ correspond to the self-attention and fully connected layers of the Transformer, respectively, while $O(K{d_m}^2)$ corresponds to the MLP used to model the vector field.


\begin{table*}[ht]
\centering
\caption{Average empirical coverage and size of the prediction regions constructed by FCP (ours) and all baselines on simulation with varying outcome dimensions $d_y$. The reported values represent the averages and standard deviations computed across five independent experiments. The target coverage was set to 0.9. Results where the average empirical coverage fell below the target coverage are grayed out, and the smallest prediction region sizes, excluding the grayed-out results, are highlighted in bold.}
\vspace*{2mm}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccc}
\toprule
\textbf{Data} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{2}{c}{$p=2$} & \multicolumn{2}{c}{$p=4$} & \multicolumn{2}{c}{$p=8$}  \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} 
 &  & \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} \\
\midrule
\multirow{8}{*}{\textbf{AR}} 

 & FCP (ours)  & $0.902_{\pm .028}$ & $\textbf{3.35}_{\pm .698}$  & $0.936_{\pm .031}$ & $\textbf{32.6}_{\pm 6.23}$ & $0.904_{\pm .041}$ &  $\textbf{850.4}_{\pm 265.3}$ \\
 
 & MultiDimSPCI  & $0.912_{\pm .035}$  & $7.40_{\pm 1.61}$ & \textcolor{gray}{$0.880_{\pm .020}$}  & \textcolor{gray}{$90.3_{\pm 15.4}$} & \textcolor{gray}{$0.862_{\pm .016}$} & \textcolor{gray}{${1.60 \times 10^4}_{\pm 1.02 \times 10^4}$}  \\

 & Local Ellipsoid & \textcolor{gray}{$0.868_{\pm .035}$} & \textcolor{gray}{$7.74_{\pm 2.28}$} & \textcolor{gray}{$0.870_{\pm .022}$} & \textcolor{gray}{$140.4_{\pm 38.9}$} & \textcolor{gray}{$0.842_{\pm .030}$} &  \textcolor{gray}{${5.70\times 10^4}_{\pm 5.57 \times 10^4}$} \\
 
 & Empirical Copula & \textcolor{gray}{$0.844_{\pm .034}$} & \textcolor{gray}{$7.11_{\pm 2.12}$} & \textcolor{gray}{$0.838_{\pm .038}$} & \textcolor{gray}{$80.5_{\pm 26.2}$} & \textcolor{gray}{$0.826_{\pm .035}$} & \textcolor{gray}{${2.29 \times10^4}_{\pm 1.38 \times10^4}$} \\

 & Gaussian Copula & \textcolor{gray}{$0.844_{\pm .034}$}  & \textcolor{gray}{$7.12_{\pm 2.11}$}  & \textcolor{gray}{$0.858_{\pm  .043}$} & \textcolor{gray}{$99.9_{\pm 24.3}$}  & \textcolor{gray}{$0.854_{\pm .037}$} & \textcolor{gray}{${2.72 \times 10^4}_{\pm 1.07 \times 10^4}$} \\

 & CopulaCPTS  & $0.996_{\pm .005}$ & $14.1_{\pm 6.13}$ & $0.956_{\pm .054}$ & $208.1_{\pm 61.8}$ & $0.984_{\pm .008}$ & ${3.08 \times 10^4}_{\pm 1.35 \times 10^4}$ \\ 
 
 & TFT  & $0.945_{\pm .023}$ & $5.09_{\pm 1.54}$ & \textcolor{gray}{$0.845_{\pm .068}$} & \textcolor{gray}{$21.9_{\pm 6.51}$} & \textcolor{gray}{$0.708_{\pm .051}$} & \textcolor{gray}{$762.2_{\pm 436.3}$} \\
 
 & DeepAR  & $0.929_{\pm .058}$ & $4.37_{\pm 1.27}$ & \textcolor{gray}{$0.806_{\pm .056}$} & \textcolor{gray}{$20.6_{\pm 7.01}$} & \textcolor{gray}{$0.261_{\pm .050}$} & \textcolor{gray}{$232.9_{\pm 139.7}$} \\
 
\midrule
\multirow{8}{*}{\textbf{VAR}} 

 & FCP (ours)  & $0.918_{\pm .043}$ & $\textbf{4.32}_{\pm .663}$  & $0.940_{\pm .015}$ &  
 $\textbf{24.5}_{\pm 8.05}$ & $0.920_{\pm 0.24}$ & $\textbf{265.7}_{\pm 133.0}$ \\
 
 & MultiDimSPCI  & \textcolor{gray}{$0.892_{\pm .023}$} & \textcolor{gray}{$6.61_{\pm 1.83}$}  & \textcolor{gray}{$0.886_{\pm .026}$} & \textcolor{gray}{$39.0_{\pm 35.6}$} & \textcolor{gray}{$0.878_{\pm .052}$} & \textcolor{gray}{$507.1_{\pm 550.3}$}  \\
 
 & Local Ellipsoid & \textcolor{gray}{$0.872_{\pm .034}$} & \textcolor{gray}{$7.92_{\pm 1.85}$} & \textcolor{gray}{$0.884_{\pm .038}$} & \textcolor{gray}{$605.9_{\pm 1025}$} & $0.900_{\pm .028}$ & ${2.24 \times 10^4}_{\pm 1.79 \times 10^4}$ \\
 
 & Empirical Copula  & \textcolor{gray}{$0.858_{\pm .032}$} & \textcolor{gray}{$7.26_{\pm 1.09}$} & \textcolor{gray}{$0.868_{\pm .041}$} & \textcolor{gray}{$73.2_{\pm 17.2}$} &  \textcolor{gray}{$0.842_{\pm .021}$} & \textcolor{gray}{${1.56 \times 10^4}_{\pm 6395}$} \\

 & Gaussian Copula & \textcolor{gray}{$0.866_{\pm .027}$}  & \textcolor{gray}{$7.55_{\pm 1.20}$}  & \textcolor{gray}{$0.884_{\pm .047}$} & \textcolor{gray}{$92.2_{\pm 17.3}$}  & \textcolor{gray}{$0.872_{\pm .037}$} & \textcolor{gray}{${2.25 \times 10^4}_{\pm 1.11 \times 10^4}$} \\

 & CopulaCPTS  & $0.972_{\pm .032}$ & $15.2_{\pm 6.43}$ & $0.988_{\pm .024}$ & $552.9_{\pm 491.1}$ & $0.976_{\pm .015}$ & ${2.29 \times 10^4}_{\pm 3524}$\\

 & TFT  & $0.939_{\pm .021}$ & $4.98_{\pm .064}$ & $0.914_{\pm .078}$  & $26.4_{\pm 10.6}$ & \textcolor{gray}{$0.737_{\pm .077}$} & \textcolor{gray}{$639.3_{\pm 306.4}$} \\
 
 & DeepAR  & $0.931_{\pm .014}$ & $4.69_{\pm 1.07}$ & \textcolor{gray}{$0.839_{\pm .095}$} & \textcolor{gray}{$19.4_{\pm 6.66}$} & \textcolor{gray}{$0.516_{\pm .053}$} & \textcolor{gray}{$261.6_{\pm 100.7}$} \\
\bottomrule
\label{table:simulation_results}
\end{tabular}
}
\end{table*}


\section{Experiments}

\subsection{Setup}


\paragraph{Baselines.}

We evaluated our method with several conformal prediction methods developed for multi-dimensional time series or multi-dimensional outcomes: MultiDimSPCI~\cite{xu2024conformal}, conformal prediction using local ellipsoid, CopulaCPTS~\cite{sun2022copula}, as well as conformal prediction using empirical copula~\cite{messoudi2021copula} and Gaussian copula.

We also included two probabilistic time series forecasting methods: Temporal Fusion Transformer (TFT)~\cite{lim2021temporal} and DeepAR~\cite{salinas2020deepar}. While TFT and DeepAR were originally developed for time series with univariate outcomes, we adopted those baselines to our setup by constructing an independent copula with the predicted interval of each output dimension. For notational convenience, we refer to our method as FCP, an abbreviation for \textbf{F}low-based \textbf{C}onformal \textbf{P}rediction. 


\paragraph{Evaluation metrics.}


Since \textit{efficient} prediction regions are smaller regions satisfying the marginal coverage, we used empirical coverage and the average size of prediction regions as evaluation metrics. The empirical coverage is defined as:
\begin{equation}
    \frac{1}{|\mathcal{D}_{\text{test}}|} \sum_{\{x_i, y_i\} \in \mathcal{D}_{\text{test}}} \mathbbm{1} \left(y_i \in \hat{C}_{i-1} (z_i) \right),
\label{eq:empirical_cov}
\end{equation}
where $\mathcal{D}_{\text{test}}$ denotes test split of the time series dataset. The empirical coverage of FCP can be computed by solving the conditional flow ODE in reverse time as detailed in~\ref{sec:prediction_regions}.


\paragraph{Base predictor.} 

Base predictor $\hat{f}$ is used to provide a point prediction $\hat{y}$ of true $y$. For simulations, we used multivariate linear regression as the base predictor. For experiments on real-world datasets, we used leave-one-out bootstrap linear regression with 15 models as the base predictor.

\paragraph{Implementation details.}

\texttt{dopri5}~\cite{dormand1980family} at absolute and relative tolerances of 1e-5 was used to solve the ODEs. A grid search was conducted to determine the optimal hyperparameters for FCP, including the number of MLP layers in the vector field, the hidden dimension of the MLP layers, the number of Transformer heads and layers, the model dimension of the Transformer, and the scaling factor $\beta$. A detailed description of the hyperparameter search and the implementation of the baseline methods is provided in Appendix~\ref{appendix:implementation_details}. The source code is available at \url{https://github.com/Jayaos/flow_cp}.


\begin{table*}[ht]
\centering
\caption{Average empirical coverage and size of the prediction regions constructed by FCP (ours) and all baselines on three real-world datasets with varying outcome dimensions $d_y$. The reported values represent the averages and standard deviations computed across five independent experiments. The target coverage was set to 0.95. Results where the average empirical coverage fell below the target coverage are grayed out, and the smallest prediction region sizes, excluding the grayed-out results, are highlighted in bold.}
\label{table:real_data_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccc}
\toprule
\textbf{Dataset} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{2}{c}{$d_y=2$} & \multicolumn{2}{c}{$d_y=4$} & \multicolumn{2}{c}{$d_y=8$} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
 &  & \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} \\
\midrule
\multirow{8}{*}{\textbf{Wind}} 
 & FCP (ours)  & $0.953_{\pm .033}$ & $\textbf{1.16}_{\pm .232}$ & $0.968_{\pm .018}$ & $\textbf{2.44}_{\pm .336}$ & $0.977_{\pm .010}$ & $\textbf{15.9}_{\pm 3.43}$ \\
 
 & MultiDimSPCI  & $0.956_{\pm .016}$ & $\textbf{1.16}_{\pm .175}$ & $0.958_{\pm .017}$ & $9.50_{\pm 9.26}$ & $0.956_{\pm .023}$ & $598.3_{\pm 711.7}$ \\

 
 & Local Ellipsoid & $0.964_{\pm .015}$ & $1.38_{\pm .419}$ & $0.971_{\pm .013}$ & $8.63_{\pm 5.90}$ & $0.974_{\pm .012}$ & $394.9_{\pm 522.4}$ \\
 
 & Empirical Copula  & $0.953_{\pm .024}$ & $1.26_{\pm .445}$ & \textcolor{gray}{$0.938_{\pm .015}$} & \textcolor{gray}{$4.34_{\pm 2.01}$} & \textcolor{gray}{$0.909_{\pm .010}$} & \textcolor{gray}{$210.4_{\pm 385.4}$} \\

 & Gaussian Copula & $0.953_{\pm .024}$ & $1.38_{\pm .483}$  & \textcolor{gray}{$0.937_{\pm .015}$} & \textcolor{gray}{$4.34_{\pm 2.01}$}  & $0.951_{\pm .013}$ & $342.3_{\pm 326.1}$ \\

 & CopulaCPTS  & $0.987_{\pm .016}$ & $7.08_{\pm 7.34}$ & $0.992_{\pm .010}$ & $229.5_{\pm 393.7}$ & $0.992_{\pm .013}$ & ${6.66 \times 10^4}_{\pm 1.30 \times 10^5}$ \\
 
 & TFT & \textcolor{gray}{$0.723_{\pm .172}$} & \textcolor{gray}{$1.34_{\pm .588}$} & \textcolor{gray}{$0.515_{\pm .174}$} & \textcolor{gray}{$4.26_{\pm 3.52}$} & \textcolor{gray}{$0.187_{\pm .126}$} & \textcolor{gray}{$6.75_{\pm 3.19}$} \\
 
 & DeepAR & \textcolor{gray}{$0.909_{\pm .036}$} & \textcolor{gray}{$1.32_{\pm .445}$} & \textcolor{gray}{$0.672_{\pm .130}$} & \textcolor{gray}{$4.84_{\pm 3.86}$} & \textcolor{gray}{$0.320_{\pm .160}$} & \textcolor{gray}{$52.8_{\pm 64.5}$} \\
 
\midrule
\multirow{8}{*}{\textbf{Traffic}} 

 & FCP (ours)  & $\textcolor{gray}{0.934_{\pm .012}}$ & $\textcolor{gray}{0.465_{\pm .078}}$ & $0.959_{\pm .007}$ & $\textbf{0.672}_{\pm .147}$ & $0.971_{\pm .005}$ & $\textbf{0.801}_{\pm .329}$ \\
 
 & MultiDimSPCI  & $0.959_{\pm .002}$  & $\textbf{0.987}_{\pm .346}$ & $0.963_{\pm .011}$ & $1.81_{\pm 1.01}$ & $0.966_{\pm .004}$ & $5.70_{\pm 5.28}$\\

 
 & Local Ellipsoid  & $0.970_{\pm .007}$ & $2.04_{\pm .505}$ & $0.975_{\pm .005}$ & $2.95_{\pm 1.06}$ & $0.980_{\pm .003}$ & $3.82_{\pm 2.47}$ \\
 
 & Empirical Copula  & $0.964_{\pm .009}$ & $2.25_{\pm .361}$ & $0.972_{\pm .008}$ & $5.03_{\pm 1.30}$ & $0.963_{\pm .009}$ & $28.4_{\pm 8.97}$ \\

 & Gaussian Copula & $0.965_{\pm .009}$ & $2.30_{\pm .432}$ & $0.972_{\pm .008}$ & $5.02_{\pm 1.29}$ & $0.963_{\pm .010}$ & $28.3_{\pm 8.97}$ \\

 & CopulaCPTS  & $0.997_{\pm .001}$ & $8.08_{\pm 2.92}$ & $1.00_{\pm .000}$ & $55.89_{\pm 30.46}$ & $1.00_{\pm .000}$ & $2195_{\pm 940}$ \\
 
 & TFT  & \textcolor{gray}{$0.407_{\pm .065}$} & \textcolor{gray}{$0.292_{\pm .089}$} & \textcolor{gray}{$0.189_{\pm .306}$} & \textcolor{gray}{$0.07_{\pm .031}$} & \textcolor{gray}{$0.09_{\pm .007}$} & \textcolor{gray}{$0.009_{\pm .007}$} \\
 
 & DeepAR  & \textcolor{gray}{$0.443_{\pm .095}$} & \textcolor{gray}{$0.308_{\pm .088}$} & \textcolor{gray}{$0.197_{\pm .054}$} & \textcolor{gray}{$0.07_{\pm .030}$} & \textcolor{gray}{$0.09_{\pm .028}$} & \textcolor{gray}{$0.004_{\pm .003}$} \\

\midrule
\multirow{8}{*}{\textbf{Solar}} 
 & FCP (ours)  & $0.973_{\pm .004}$ & $\textbf{0.878}_{\pm .157}$ & $0.973_{\pm .004}$ & $\textbf{1.95}_{\pm .811}$ & -  & -  \\
 
 & MultiDimSPCI  & $0.974_{\pm .007}$ & $1.61_{\pm .406}$ & $0.974_{\pm .003}$ & $9.22_{\pm 2.56}$ & - & - \\
 
 
 & Local Ellipsoid  & \textcolor{gray}{$0.947_{\pm .004}$} & \textcolor{gray}{$1.44_{\pm .188}$} & \textcolor{gray}{$0.949_{\pm .005}$} & \textcolor{gray}{$1.87_{\pm .540}$} & - & - \\
 
 & Empirical Copula  & $0.951_{\pm .005}$ & $2.01_{\pm .228}$ & \textcolor{gray}{$0.946_{\pm .003}$} & \textcolor{gray}{$5.89_{\pm .511}$} & - & - \\

 & Gaussian Copula & $0.953_{\pm .004}$ & $2.06_{\pm .213}$  & $0.957_{\pm .002}$ & $8.36_{\pm .864}$  & - &  -\\

  
 & CopulaCPTS  & $0.999_{\pm .001}$ & $21.8_{\pm 11.7}$ & $0.999_{\pm .001}$ & $960.8_{\pm 828.9}$ & - &  - \\
 
 & TFT  & \textcolor{gray}{$0.782_{\pm .026}$} & \textcolor{gray}{$0.779_{\pm .056}$} & \textcolor{gray}{$0.722_{\pm .028}$} & \textcolor{gray}{$3.18_{\pm .415}$} & - & - \\
 
 & DeepAR  & \textcolor{gray}{$0.802_{\pm .121}$} & \textcolor{gray}{$1.03_{\pm .114}$} & \textcolor{gray}{$0.713_{\pm .086}$} & \textcolor{gray}{$6.73_{\pm 1.09}$} & - & - \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsection{Simulation}

We formulated two simulated datasets to conduct controlled experiments evaluating our methods and baselines: independent autoregressive sequence and vector autoregressive sequence. Both sequences are generated as:
\begin{equation}
Y_t = \sum_{l=1}^m \alpha_l Y_{t-l} + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \Sigma),
\end{equation}
where $ Y_t = [Y_{t1}, \dots, Y_{td_y}] \in \mathbb{R}^{d_y} $ for $ d_y \geq 2$, and $m$ is a lag order; $\alpha_{l} \in \mathbb{R}^{d_y \times d_y}$ is a coefficient matrix that was designed to ensure the stationarity of the sequence.

\paragraph{Independent autoregressive sequence.}

In generating an independent autoregressive sequence (AR), we set $\Sigma = I_{d_y}$ and $m=3$.

\paragraph{Vector autoregressive sequence.}

In generating vector autoregressive sequence (VAR), we set $\Sigma = BB^T$ to be a positive definite matrix, where $B_{ij} \sim \text{Unif}[-1,1]$, and $m=3$.
%

We generated five AR and VAR sequences, each with a length of 1000, with outcome dimensions $d_y \in \{ 2, 4, 8\}$. Each feature and outcome was independently standardized across dimensions. To ensure robust results, we conducted five independent experiments for each method across all datasets and dimensions. The first 80\% of each sequence was used as the training set, while the remaining 20\% was split into validation and test sets. A validation set was used for methods requiring a calibration set. For methods that do not require a validation set, the validation set was incorporated into the test set.

\paragraph{Results.}

Table~\ref{table:simulation_results} presents the experimental results on simulated datasets. We observed that FCP consistently achieved the smallest prediction regions compared to all baselines while maintaining the target coverage. Since the average empirical coverage of TFT and DeepAR is close to the target coverage, having smaller prediction region sizes compared to FCP, we conducted additional experiments by manually increasing the target coverage of TFT and DeepAR. However, both TFT and DeepAR either failed to reach the target coverage or constructed prediction regions larger than FCP (see Appendix~\ref{app:additional_experiments}).

%Particularly, FCP successfully maintained the target coverage across all experiments, whereas other baseline methods frequently failed to achieve the target coverage.




\subsection{Experiments on Real-world Datasets}


We evaluated our method and baselines using three real-world time series datasets: wind, solar, and traffic datasets. 

\paragraph{Wind dataset.}

The wind dataset contains wind speed records measured at 30 different wind farms~\cite{zhu2021multi}. Each wind farm location provides 768 records with 5 features at each timestamp.


\paragraph{Traffic dataset.}

The traffic dataset contains traffic flow collected at 15 different traffic sensor locations~\cite{xu2021conformalanomaly}. Each sensor location provides 8778 observations with 5 features at each timestamp.


\paragraph{Solar dataset.}
The solar dataset considers solar radiation in Diffused Horizontal Irradiance (DHI) units at 9 different solar sensor locations~\cite{zhang2021solar}. Each location provides 8755 records with 5 features at each timestamp.

For the wind and traffic datasets, we randomly selected $d_y \in \{ 2, 4, 8\}$ locations to construct five sequences of $d_y$-dimensional time series. For the solar dataset, we randomly selected $d_y \in \{ 2, 4\}$ locations to similarly construct five sequences of $d_y$-dimensional time series. We did not construct sequences with $d_y = 8 $ for the solar dataset due to the limited number of unique locations, which could lead to overlapping sequences across different trials of experiments. We conducted five independent experiments for each method across all datasets and dimensions. The sequence split and standardization followed the same approach as in the simulation.

\paragraph{Results.}

Table~\ref{table:real_data_results} presents the experimental results on three real-world datasets. FCP consistently obtained smaller or comparable prediction regions compared to all baselines while maintaining the target coverage. Notably, FCP achieved particularly smaller prediction regions for higher $d_y$. Since the standard deviation of prediction region sizes tends to increase with $d_y$, we visualized the distribution of the prediction region sizes using paired box plots across five independent experiments in Figure~\ref{fig:paired_box_plots}, comparing FCP, MultiDimSPCI, and conformal prediction with local ellipsoids for a clear analysis. As shown in Figure~\ref{fig:paired_box_plots}, FCP obtained more stable prediction region sizes compared to the two baselines.


\begin{figure*}[tp]
    \centering
    \includegraphics[width=\textwidth]{figures/paired_box_combined.pdf}
        \vspace*{-8mm}
    \caption{Paired box plots of the sizes of the prediction regions constructed by FCP (ours), MultiDimSPCI, and conformal prediction using local ellipsoids on the wind (upper) and traffic (lower) datasets with $d_y = 8$. The sizes of the prediction regions for conformal prediction using local ellipsoids on the wind dataset exceed the displayed range in the last two experiments.}
    \label{fig:paired_box_plots}
\end{figure*}




\section{Conclusion}

In this study, we proposed a novel conformal prediction method for multi-dimensional time series by integrating Transformer and Normalizing Flow. Prediction regions are constructed by leveraging the learned distribution of non-conformity scores conditioned on the encoded historical context of time series, where Transformer was used as the encoder and conditional normalizing flow was used to model the conditional distribution. Through comprehensive empirical evaluations, we demonstrated that our method produces smaller prediction regions compared to baseline methods satisfying the target coverage. Future work includes exploring other normalizing flow methods and analyzing the properties of the prediction regions obtained by our method.

%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}

The authors would like to thank Hanyang Jiang for the insightful and productive discussions. This work is partially supported by NSF CAREER CCF-1650913, NSF DMS-2134037, CMMI-2015787, CMMI-2112533, DMS-1938106, DMS-1830210, and the Coca-Cola Foundation.


\bibliography{refs}

\appendix


\section{Proof}

\subsection{Proof for Proposition~\ref{proposition:mass_preserving}}
\begin{proof}

Let $x \sim p_0$ and define:
\begin{equation}
    x' = \phi(x)
\end{equation}

Since $\phi$ is a diffeomorphic map, the probability density function of $x'$ is given by:
\begin{equation} 
    q(x') = p_0(x) \left| \det J_{\phi}(x) \right|^{-1}, 
\label{eq:change_of_variables_density} 
\end{equation}
where $J_{\phi}(x) = \frac{\partial \phi(x)}{\partial x}$ is the Jacobian of the $\phi$.


The probability measure of the set $\mathcal{A}$ is defined as:
\begin{equation}
    P(\mathcal{A}) = \int_{\mathcal{A}} p_0(x) \, dx.
\end{equation}

After the flow transformation $\phi$, the probability measure of the transformed set $\mathcal{A}'$ is given by:
\begin{equation}
    P(\mathcal{A}') = \int_{\mathcal{A}'} q(x') \, dx'.
\label{eq:measure_A_transformation}
\end{equation}





Using the change of variables,
\begin{equation}
    dx'=|\det J_{\phi}(x)|dx.
\label{eq:change_of_variables_dx}
\end{equation}

By substituting equations (\ref{eq:change_of_variables_density}) and (\ref{eq:change_of_variables_dx}) into the equation (\ref{eq:measure_A_transformation}),

\begin{equation}
   P(\mathcal{A}') = \int_{\mathcal{A}'} p(x') \, dx' = \int_{\mathcal{A}} p_0(x) \left| \det J_{\phi}(x) \right|^{-1} \cdot \left| \det J_{\phi}(x) \right| dx. 
\end{equation}

Therefore,
\begin{equation}
    P(\mathcal{A}') = \int_{\mathcal{A}} p_0(x) \, dx = P(\mathcal{A}),
\end{equation}
which states that $P(\mathcal{A}) = P(\mathcal{A}')$.

\end{proof}


\section{The log-determinant of the Jacobian ODE}
\label{app:jacobian_ode}

The Jacobian ODE is defined as:
\begin{equation}
    \frac{d}{dt} J_{\phi_{t,h}}(x) = \frac{\partial v_{t,h}(\phi_{t,h} (x) )}{\partial \phi_{t,h} (x)} \frac{\partial \phi_{t,h} (x)}{\partial x} = \frac{\partial v_{t,h}(\phi_{t,h} (x) )}{\partial \phi_{t,h} (x)} J_{\phi_{t,h}}(x),
\label{eq:jacobian_ode}
\end{equation}
with the initial condition:
\begin{equation}
    J_{\phi_{t=0,h}}(x) = I.
\end{equation}

By using Jacobi's formula,
\begin{equation}
    \frac{d}{dt} \det J_{\phi_{t,h}}(x) = \det J_{\phi_{t,h}}(x) \cdot \text{tr}\left(J^{-1}_{\phi_{t,h}}(x) \frac{d}{dt} J_{\phi_{t,h}}(x) \right).
\label{eq:jacobian_det_ode}
\end{equation}

Substituting equation (\ref{eq:jacobian_ode}) into equation (\ref{eq:jacobian_det_ode}), we obtain:
\begin{equation}
    \frac{d}{dt} \det J_{\phi_{t,h}}(x) = \det J_{\phi_{t,h}}(x)\cdot \text{tr}\left(\frac{\partial v_{t,h}(\phi_{t,h} (x) )}{\partial \phi_{t,h} (x)}\right).
\label{eq:jacobian_det_ode2}
\end{equation}

Therefore, 
\begin{equation}
    \frac{d}{dt} \log |\det J_{\phi_{t,h}}(x)| = \text{tr}\left(\frac{\partial v_{t,h}(\phi_{t,h} (x) )}{\partial \phi_{t,h} (x)}\right).
\end{equation}

Since the trace of the Jacobian of a vector field corresponds to its divergence, we have:
\begin{equation}
    \text{tr}\left(\frac{\partial v_{t,h}(\phi_{t,h} (x) )}{\partial \phi_{t,h} (x)} \right) = \nabla \cdot v_{t,h}(\phi_{t,h} (x)  ),
\end{equation}
where $\nabla \cdot$ denotes the divergence operator.

Therefore, the log-determinant of the Jacobian ODE is defined as:
\begin{equation}
    \frac{d}{dt} \log |\det J_{\phi_{t,h}}(x)| = \nabla \cdot v_{t,h}(\phi_{t,h} (x) )
\end{equation}
with the initial condition:
\begin{equation}
     \log |\det J_{\phi_{t=0,h}}(x)| = 0.
\end{equation}

\section{Additional Implementation Details}
\label{appendix:implementation_details}


\begin{table}[ht]
\centering
\caption{Hyperparameter grid space for TFT.}
\vspace{2mm}
\label{tb:grid_search_tft}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Grid space}            \\ 
\midrule
model dimension  & \{ 16, 32 \}              \\
the number of attention heads & \{ 2, 4 \} \\
the number of layers & \{ 1, 2 \} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[tb]
\centering
\caption{Hyperparameter grid space for DeepAR.}
\vspace{2mm}
\label{tb:grid_search_ar}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter} & \textbf{Grid space}            \\ 
\midrule
model dimension  & \{ 16, 32 \}              \\
the number of layers & \{ 1, 2 \} \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{MultiDimSPCI}

We implemented MultiDimSPCI using the source code provided by the authors on GitHub. The context window size was set to $w=20$ for simulations and $w=50$ for experiments on real-world datasets.


\paragraph{Comformal prediction using copulas}

We implemented the method using the source code provided by the authors on GitHub.

\paragraph{Conformal prediction using local ellipsoids}

We implemented the method using the source code provided by the authors on GitHub.

\paragraph{CopulaCPTS}

We implemented the method using the source code provided by the authors on GitHub.

\paragraph{Temporal Fusion Transformer} 

We implemented Temporal Fusion Transformer (TFT) using \texttt{pytorch\_forecasting}. A hyperparameter grid search was conducted on each dataset with $d_y=2$ to determine the optimal configuration. Table~\ref{tb:grid_search_tft} shows the hyperparameter grid space. The performance was observed to saturate at a model dimension of 32, two attention heads, and two layers. Therefore, we used these hyperparameter settings across all experiments. The Adam optimizer was used with a learning rate of 0.001 and a maximum epoch of 50. The dropout rate was set to 0.1. Quantile loss with $q \in \{ 0.05,0.95 \}$ was used for the target quantile 0.9 and $q \in \{0.025,0.975 \}$ was used for the target quantile 0.95.



\begin{table}[ht]
\centering
\caption{Hyperparameter grid space for FCP.}
\vspace{2mm}
\label{tb:grid_search_fcp}
\begin{tabular}{ll}
\toprule
\textbf{Hyperparameter}        & \textbf{Grid space}            \\ \midrule
the number of MLP layers of the vector field  & \{2, 4\}                    \\
the number of heads of Transformer & \{ 2, 4 \} \\
the number of layers of Transformer & \{ 2, 4 \} \\
the hidden dimension of the MLP of the vector field and Transformer   & \{ 16, 32, 64 \}               \\
scaling factor $\beta$           & \{ 1, 2, 3, 4 \}                   \\
dropout & \{ 0, 0.1 \}  \\
\bottomrule
\end{tabular}
\label{tb:grid_search}
\end{table}



\paragraph{DeepAR}



We implemented DeepAR using \texttt{pytorch\_forecasting}. A hyperparameter grid search was conducted on each dataset with $d_y=2$ to determine the optimal configuration. Table~\ref{tb:grid_search_ar} shows the hyperparameter grid space. The performance was observed to saturate at a model dimension of 32 and two layers. Therefore, we used these hyperparameter settings across all experiments. The Adam optimizer was used with a learning rate of 0.001 and a maximum epoch of 50. The dropout rate was set to 0.1. Multivariate normal distribution loss with $q \in \{ 0.05,0.95 \}$ was used for the target quantile 0.9 and $q \in \{0.025,0.975 \}$ was used for the target quantile 0.95.


\begin{table*}[ht]
\centering
\caption{The optimized hyperparameter setting for FCP on simulations.}
\vspace{2mm}
\label{tb:hyperparameter_grid_results_simulation}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccc}
\toprule
\textbf{Dataset} & \textbf{Hyperparameter} & $d=2$ & $d=4$ & $d=8$ \\
\midrule
\multirow{8}{*}{\textbf{AR}} 
 & the number of MLP layers of the vector field & 2  & 2  & 2 \\
 & the number of heads of Transformer & 2 & 2 & 2 \\
 & the number of layers of Transformer & 2 & 2 & 2 \\
 & the hidden dimension of the MLP of the vector field and Transformer & 32 & 32 & 32 \\
 & scaling factor $\beta$ & 2 & 3 & 4 \\
 & dropout & 0.1 & 0.1 & 0.1 \\
 & batch size & 4 & 4 & 4 \\
\midrule
\multirow{8}{*}{\textbf{VAR}} 
 & the number of MLP layers of the vector field & 2  & 2  & 2 \\
 & the number of heads of Transformer & 2 & 2 & 2 \\
 & the number of layers of Transformer & 2 & 2 & 2 \\
 & the hidden dimension of the MLP of the vector field and Transformer & 32 & 32 & 32 \\
 & scaling factor $\beta$ & 2 & 3 & 4 \\
 & dropout & 0.1 & 0.1 & 0.1 \\
 & batch size & 4 & 4 & 4 \\
\bottomrule
\end{tabular}
}
\end{table*}





\paragraph{FCP}

We used multilayer perceptions (MLP) to model the time-dependent conditional vector field $v_{t,h} : [0,1] \times \mathbb{R}^{d_h} \times \mathbb{R}^{d_y} \to \times \mathbb{R}^{d_y}$. The input to the MLP was formed by concatenating $h$, $x \sim p_0$, and $t \in [0,1]$ to model $v_{t,h}(x)$. 

A hyperparameter grid search was conducted to determine the optimal configuration. The training set was used for the hyperparameter grid search. Table~\ref{tb:grid_search_fcp} shows the hyperparameter grid space and Table~\ref{tb:grid_search_fcp} shows the optimized hyperparameter settings. The context window size was set to $w=20$ for simulations and $w=50$ for experiments on real-world datasets. The Adam optimizer~\cite{kingma2014adam} was used with a learning rate of 0.0001 and a maximum epoch of 50 for all experiments. 

We set the sampling size $N$ based on the $d_y$ to estimate the size of the prediction regions obtained by FCP, we used $N=1000$ for experiments with $d_y=2$, $N=5000$ for experiments with $d_y=4$, and $N=10000$ for experiments with $d_y=8$. We observed that further increasing the sampling size did not significantly impact the estimation of the prediction region size.





\begin{table*}[ht]
\centering
\caption{The optimized hyperparameter settings for FCP on experiments using real-world datasets.}
\vspace{2mm}
\label{tb:hyperparameter_grid_results_real}
\resizebox{\textwidth}{!}{
\begin{tabular}{llccc}
\toprule
\textbf{Dataset} & \textbf{Hyperparameter} & $d=2$ & $d=4$ & $d=8$ \\
\midrule
\multirow{8}{*}{\textbf{Wind}} 
 & the number of MLP layers of the vector field & 4  & 4  & 4 \\
 & the number of heads of Transformer & 2 & 2 & 2 \\
 & the number of layers of Transformer & 4 & 4 & 4 \\
 & the hidden dimension of the MLP of the vector field and Transformer & 32 & 32 & 32 \\
 & scaling factor $\beta$ & 1 & 2 & 2 \\
 & dropout & 0.1 & 0.1 & 0.1 \\
 & batch size & 4 & 4 & 4 \\
\midrule
\multirow{8}{*}{\textbf{Traffic}} 
 & the number of MLP layers of the vector field & 4  & 4  & 4 \\
 & the number of heads of Transformer & 2 & 2 & 2 \\
 & the number of layers of Transformer & 4 & 4 & 4 \\
 & the hidden dimension of the MLP of the vector field and Transformer & 32 & 32 & 32 \\
 & scaling factor $\beta$ & 1 & 2 & 4 \\
 & dropout & 0.1 & 0.1 & 0.1 \\
 & batch size & 8 & 8 & 8 \\
 \midrule
\multirow{8}{*}{\textbf{Solar}} 
 & the number of MLP layers of the vector field & 4  & 4  & - \\
 & the number of heads of Transformer & 2 & 2 & - \\
 & the number of layers of Transformer & 4 & 4 & - \\
 & the hidden dimension of the MLP of the vector field and Transformer & 32 & 32 & - \\
 & scaling factor $\beta$ & 1 & 2 & - \\
 & dropout & 0.1 & 0.1 & - \\
 & batch size & 8 & 8 & - \\
\bottomrule
\end{tabular}
}
\end{table*}

\section{Additional Details of Datasets}

\paragraph{Simulation}

We designed $\alpha_l$ to ensure the stationarity of the AR and VAR sequences. First, a set of random values between 0 and 0.5 is sampled to be the roots of the characteristic polynomial. These roots are then used to construct a polynomial, which guarantees that the resulting AR process is stationary, as stationarity requires that all characteristic roots lie inside the unit circle. The polynomialâ€™s coefficients are then used to generate the sequences.


\begin{table*}[ht]
\centering
\caption{Average empirical coverage and size of the prediction regions constructed by TFT and DeepAR on simulations with varying outcome dimensions $d_y$. We manually adjusted the target coverage to 0.95. The reported values represent the averages and standard deviations computed across five independent experiments.}
\begin{tabular}{llcccc}
\toprule
\textbf{Dataset} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{2}{c}{$d_y=4$} & \multicolumn{2}{c}{$d_y=8$} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
 && \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} \\
\midrule
\multirow{2}{*}{\textbf{AR}} 
 
 & TFT  & $0.920_{\pm .033}$ & $58.9_{\pm 21.2}$  & $0.792_{\pm .065}$  & $2738_{\pm 1332}$ \\
 
 & DeepAR & $0.898_{\pm .049}$ & $37.71_{\pm 12.42}$  & $0.506_{\pm .070}$  & $933.5_{\pm 622.0}$  \\
 
\midrule
\multirow{2}{*}{\textbf{VAR}} 
 
 & TFT  & $0.945_{\pm 0.56}$ & $52.1_{17.9}$  & $0.820_{\pm .049}$ & $2513_{\pm 1184}$  \\
 
 & DeepAR  & $0.920_{\pm .068}$ & $40.7_{\pm 15.1}$ & $0.693_{\pm 0.089}$ & $948.0_{\pm 449.5}$ \\
\bottomrule
\end{tabular}
\label{table:additional_experiments_prob_forecsting_95}
\end{table*}



\begin{table*}[ht]
\centering
\caption{Average empirical coverage and size of the prediction regions constructed by TFT and DeepAR on simulations with varying outcome dimensions $d_y$. We manually adjusted the target coverage to 0.99. The reported values represent the averages and standard deviations computed across five independent experiments.}
\begin{tabular}{llcccc}
\toprule
\textbf{Dataset} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{2}{c}{$d_y=4$} & \multicolumn{2}{c}{$d_y=8$} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} 
 & &  \textbf{Coverage} & \textbf{Size} & \textbf{Coverage} & \textbf{Size} \\
\midrule
\multirow{2}{*}{\textbf{AR}} 
 
 & TFT  & $0.971_{\pm .012}$ & $163.6_{\pm 55.2}$   & $0.951_{\pm .027}$  &  ${3.05 \times 10^4}_{\pm 1.87 \times 10^4}$ \\
 
 & DeepAR  &  $0.949_{\pm .026}$ & $88.8_{\pm 30.0}$ & $0.680_{\pm .069}$ & $5506_{\pm 4160}$  \\
 
\midrule
\multirow{2}{*}{\textbf{VAR}} 

 & TFT & $0.976_{\pm .019}$ & $166.0_{\pm 65.7}$ & $0.939_{\pm .014}$ & ${3.50 \times 10^4}_{\pm 1.60 \times 10^4}$   \\
 
 & DeepAR & $0.959_{\pm .031}$  & $91.0_{\pm 28.9}$ & $0.802_{\pm .097}$ & $5369_{\pm 2210}$    \\
\midrule
\end{tabular}
\label{table:additional_experiments_prob_forecsting_99}
\end{table*}



\section{Additional Experiments}
\label{app:additional_experiments}

\subsection{Temporal Fusion Transformer and DeepAR}

Table~\ref{table:simulation_results} shows that the coverage of TFT and DeepAR closely aligns with the target coverage while producing smaller prediction region sizes compared to FCP. To further investigate their ability to achieve \textit{efficient} prediction regions, we conducted additional experiments with TFT and DeepAR by manually adjusting the target coverage. We adjusted the quantile loss of TFT to $q \in \{ 0.025, 0.975\}$ and $q \in \{ 0.005, 0.995\}$ setting the target coverage to 0.95 and 0.99, respectively. Similarly, for DeepAR, we adjusted the multivariate normal distribution loss to $q \in \{ 0.025, 0.975\}$ and $q \in \{ 0.005, 0.995\}$ also setting the target coverage to 0.95 and 0.99. 

Table~\ref{table:additional_experiments_prob_forecsting_95} shows the average empirical coverage and size of the prediction regions constructed by TFT and DeepAR on simulations, setting the target coverage to 0.95. We can observe that both TFT and DeepAR achieved the target coverage with $d_y=4$ but the average region size was larger than FCP. When $d_y=8$, both TFT and DeepAR failed to achieve the target coverage. Table~\ref{table:additional_experiments_prob_forecsting_99} shows the average empirical coverage and size of the prediction regions constructed by TFT and DeepAR on simulations, setting the target coverage to 0.99. While TFT achieved the target coverage on all simulations, the average empirical region size was significantly larger than FCP and MultiDimSPCI. DeepAR achieved the target coverage with $d_y=4$ but the average region size was larger than FCP. When $d_y=8$, DeepAR failed to achieve the target coverage.




\end{document}
