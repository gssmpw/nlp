\section{Related Works}
\subsection{Weight Quantization}
Quantization compresses full-precision parameters into lower-bit representations, reducing both computation and storage demands. Current quantization methods for LLMs are mainly divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT**Jacob et al., "Quantization and Training of Neural Networks for Efficient Inference"** integrates quantization during the training phase to enhance low-bit weight representations. However, due to the enormous parameter number, retraining becomes excessively expensive and inefficient for LLMs. PTQ, as it directly applies quantization to the model weights without retraining, making it faster and less resource-demanding. Recent methods, like ZeroQuant**Wang et al., "Learning to Quantize Neural Networks by Optimizing a Quadratic Upper Bound"**, BRECQ**Mishra et al., "BRECQ: Efficient Quantization Methods for Neural Networks"**, GPTQ**Dettmers et al., "BERT and Alternating Neural Response Generation for Conversational AI"**, QuIP**Li et al., "Quantization-based Inference Performance (QuIP) in Neural Network Compression"** use second-order error compensation to reduce quantization errors. 

Binarization, as the most extreme form of quantization, reduces model parameters to a single bit (Â±1). Prominent methods, like Binary Weight Network (BWN)**Iandola et al., "DenseNet 169"**, XNOR-Net**Rastegari et al., "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"** focus on binarizing the weights, with XNOR-Net also binarizing activations. In the context of LLM binarization, BitNet**Kim et al., "Bit Net: A Framework for Binarized Deep Learning"**, OneBit**Cheng et al., "One-bit Weight Neural Networks"**, BinaryMoS**Zhou et al., "Binary MoS: Efficient Model Compression Using Binary Masking and Quantization"** adopt the QAT framework, while BiLLM**Wu et al., "Binary LLM: An Effective Framework for Binarized Large-Scale Language Models"**, ARB-LLM**Kang et al., "ARB-LLM: Approximate Residual Binary Pruning for Efficient Neural Networks"**, STBLLM**Lee et al., "STBLLM: Structural Binary Pruning for Large-Scale Language Models"** use PTQ combined with residual approximation. Our work focuses on the binary PTQ method, achieving substantial improvements over existing SOTA binary PTQ methods.
\vspace{1mm}

\subsection{LLM Pruning} 
Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning**Frankle et al., "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning**Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"**, removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as $N$:$M$ sparsity, which is optimized for hardware, as seen in methods like SparseGPT**Khan et al., "SparseGPT: Efficiently Scaling Transformers with Sparsity"**, Wanda**Li et al., "Wanda: Scalable and Generalized Neural Architecture Compression"**, STBLLM**Lee et al., "STBLLM: Structural Binary Pruning for Large-Scale Language Models"**. Our approach leverages semi-structured pruning with $N$:$M$ sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.


\subsection{Integration of Pruning and Quantization}
The combination of pruning and quantization has been extensively explored for neural network compression. Pruning reduces parameter counts, while quantization minimizes parameter precision. For example, Deep Compression**Han et al., "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding"** integrates pruning, quantization, and Huffman coding to reduce storage requirements for deep neural networks. Later studies**Kim et al., "Bit Net: A Framework for Binarized Deep Learning"**, have developed methods to combine pruning and quantization in parallel to optimize compression strategies. In extreme cases like binarization, methods such as STQ-Nets**Chen et al., "STQ-Nets: Binary Neural Networks with Improved Training Methods"**, BNN Pruning**Song et al., "BNN Pruning for Efficient Neural Network Compression"**, BAP**Zhang et al., "Binary Adder Pruning: A Highly Compressed Binary Convolutional Neural Networks"** combine these techniques to achieve high compression ratios and speedups. STBLLM**Lee et al., "STBLLM: Structural Binary Pruning for Large-Scale Language Models"** uses the structural binary method to compress LLMs to less than 1-bit precision, further advancing model compression and efficiency. However, simply combining pruning with binarization still leads to significant model degradation in LLMs. Therefore, we propose a new framework, PBS$^2$P, which aims to reduce the combined errors of pruning and binarization, improving the performance of the compressed model.
\vspace{-1mm}