%ARB
@inproceedings{li2024arb,
  title={ARB-LLM: Alternating Refined Binarizations for Large Language Models},
  author={Li, Zhiteng and Yan, Xianglong and Zhang, Tianao and Qin, Haotong and Xie, Dong and Tian, Jiang and Kong, Linghe and Zhang, Yulun and Yang, Xiaokang and others},
  booktitle={ICLR},
  year={2025}
}
]

%STBLLM
@inproceedings{dong2024stbllm,
  title={STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs},
  author={Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others},
  booktitle={ICLR},
  year={2025}
}

% BiLLM
@inproceedings{huang2024billm,
  title={Billm: Pushing the limit of post-training quantization for llms},
  author={Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan},
  booktitle={ICML},
  year={2024}
}

% PB-LLM
@inproceedings{shang2023pb,
  title={Pb-llm: Partially binarized large language models},
  author={Shang, Yuzhang and Yuan, Zhihang and Wu, Qiang and Dong, Zhen},
  booktitle={ICLR},
  year={2024}
}

%BitNet
@article{wang2023bitnet,
  title={Bitnet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

% OneBit
@inproceedings{xu2024onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  booktitle={NeurIPS},
  year={2024}
}

% BinaryMoS 
@inproceedings{jo2024mixture,
  title={Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models},
  author={Jo, Dongwon and Kim, Taesu and Kim, Yulhwa and Kim, Jae-Joon},
  booktitle={NeurIPS},
  year={2024}
}

% EfficientQAT
@article{chen2024efficientqat,
  title={EfficientQAT: Efficient Quantization-Aware Training for Large Language Models},
  author={Chen, Mengzhao and Shao, Wenqi and Xu, Peng and Wang, Jiahao and Gao, Peng and Zhang, Kaipeng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2407.11062},
  year={2024}
}

% GPTQ
@inproceedings{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={ICLR},
  year={2023}
}

% Quip
@inproceedings{chee2024quip,
  title={Quip: 2-bit quantization of large language models with guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M},
  booktitle={NeurIPS},
  year={2024}
}


% Network Sketching: Exploiting Binary Structure in Deep CNNs
@inproceedings{guo2017network,
  title={Network sketching: Exploiting binary structure in deep cnns},
  author={Guo, Yiwen and Yao, Anbang and Zhao, Hao and Chen, Yurong},
  booktitle={CVPR},
  year={2017}
}

% Alternating Multi-bit Quantization for Recurrent Neural Networks
@inproceedings{xu2018alternating,
  title={Alternating multi-bit quantization for recurrent neural networks},
  author={Xu, Chen and Yao, Jianqiang and Lin, Zhouchen and Ou, Wenwu and Cao, Yuanbin and Wang, Zhirong and Zha, Hongbin},
  booktitle={ICLR},
  year={2018}
}

%OBC
@inproceedings{frantar2022optimal,
  title={Optimal brain compression: A framework for accurate post-training quantization and pruning},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={NeurIPS},
  year={2022}
}

%LLM-QAT
@inproceedings{liu2023llm,
  title={Llm-qat: Data-free quantization aware training for large language models},
  author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
  booktitle={ACL},
  year={2024}
}

%bitdistriller
@inproceedings{du2024bitdistiller,
  title={Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation},
  author={Du, Dayou and Zhang, Yijia and Cao, Shijie and Guo, Jiaqi and Cao, Ting and Chu, Xiaowen and Xu, Ningyi},
  booktitle={ACL},
  year={2024}
}

%Qlora
@inproceedings{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={NeurIPS},
  year={2024}
}

%Zeroquant
@inproceedings{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  booktitle={NeurIPS},
  year={2022}
}

%Brecq
@inproceedings{li2021brecq,
  title={Brecq: Pushing the limit of post-training quantization by block reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  booktitle={ICLR},
  year={2021}
}

%Spqr
@inproceedings{dettmers2023spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  booktitle={ICLR},
  year={2024}
}

%Smoothquant
@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={ICML},
  year={2023},
}

%AWQ
@inproceedings{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={MLSys},
  year={2024}
}

%Owq
@inproceedings{lee2024owq,
  title={Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models},
  author={Lee, Changhun and Jin, Jungyu and Kim, Taesu and Kim, Hyungjun and Park, Eunhyeok},
  booktitle={AAAI},
  year={2024}
}

%db-llm
@article{chen2024db,
  title={DB-LLM: Accurate dual-binarization for efficient LLMs},
  author={Chen, Hong and Lv, Chengtao and Ding, Liang and Qin, Haotong and Zhou, Xiabin and Ding, Yifu and Liu, Xuebo and Zhang, Min and Guo, Jinyang and Liu, Xianglong and others},
  journal={arXiv preprint arXiv:2402.11960},
  year={2024}
}

% transformer
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  booktitle={NeurIPS},
  year={2017}
}

% llama1
@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

% llama2
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

% llama3
@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% opt
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

% low-rank
@inproceedings{zhang2023loraprune,
  title={Loraprune: Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  booktitle={ACL},
  year={2024}
}

% low-rank
@article{yuan2023asvd,
  title={Asvd: Activation-aware singular value decomposition for compressing large language models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

% Wanda
@inproceedings{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  booktitle={ICLR},
  year={2024}
}

% Sparsegpt
@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023},
}


% knowledge distillation
@inproceedings{zhong2024revisiting,
  title={Revisiting knowledge distillation for autoregressive language models},
  author={Zhong, Qihuang and Ding, Liang and Shen, Li and Liu, Juhua and Du, Bo and Tao, Dacheng},
  booktitle={ACL},
  year={2024}
}

% knowledge distillation
@inproceedings{gu2023knowledge,
  title={Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  booktitle={ICLR},
  year={2024}
}

% c4 dataset
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}

% wikitest2 dataset
@inproceedings{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={ICLR},
  year={2017}
}

% ptb dataset
@inproceedings{marcus1994penn,
  title={The penn treebank: Annotating predicate argument structure},
  author={Marcus, Mitch and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  booktitle={HLT},
  year={1994}
}

% arc dataset
@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

% boolq dataset
@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

% hellaswag dataset
@inproceedings{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={ACL},
  year={2019}
}

% obqa dataset
@inproceedings{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={EMNLP},
  year={2018}
}

% piqa dataset
@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI},
  year={2020}
}

%rte dataset
@article{chakrabarty2021figurative,
  title={Figurative language in recognizing textual entailment},
  author={Chakrabarty, Tuhin and Ghosh, Debanjan and Poliak, Adam and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2106.01195},
  year={2021}
}

% winograd dataset
@inproceedings{sakaguchi2019adversarial,
  title={WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={AAAI},
  year={2020}
}

% pytorch
@inproceedings{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={NeurIPS},
  year={2019}
}

% huggingface
@inproceedings{paszke1912imperative,
  title={An imperative style, high-performance deep learning library},
  author={Paszke, A and Gross, S and Massa, F and Lerer, A and Bradbury, JP and Chanan, G and Killeen, T and Lin, Z and Gimelshein, N and Antiga, L and others},
  booktitle={NeurIPS},
  year={2019}
}

% vicuna
@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

% dabnn
@inproceedings{zhang2019dabnn,
  title={dabnn: A super fast inference framework for binary neural networks on arm devices},
  author={Zhang, Jianhao and Pan, Yingwei and Yao, Ting and Zhao, He and Mei, Tao},
  booktitle={ACM MM},
  year={2019}
}

% bnn
@inproceedings{helwegen2019latent,
  title={Latent weights do not exist: Rethinking binarized neural network optimization},
  author={Helwegen, Koen and Widdicombe, James and Geiger, Lukas and Liu, Zechun and Cheng, Kwang-Ting and Nusselder, Roeland},
  booktitle={NeurIPS},
  year={2019}
}

% bnn2
@inproceedings{qin2020forward,
  title={Forward and backward information retention for accurate binary neural networks},
  author={Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Shen, Mingzhu and Wei, Ziran and Yu, Fengwei and Song, Jingkuan},
  booktitle={CVPR},
  year={2020}
}

% ste
@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

% xnor
@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={ECCV},
  year={2016},
}

%LLM-shearing
@inproceedings{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  booktitle={ICLR},
  year={2024}
}

%FLAP
@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={AAAI},
  year={2024}
}

%Slicegpt
@inproceedings{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  booktitle={ICLR},
  year={2024}
}

%Llm-pruner
@inproceedings{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  booktitle={NeurIPS},
  year={2023}
}

%Pruner-Zero
@inproceedings{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  booktitle={ICML},
  year={2024}
}

%owl
@inproceedings{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Li, Gen and Jaiswal, Ajay and Pechenizkiy, Mykola and Liang, Yi and others},
  booktitle={ICML},
  year={2024}
}

%Plug-and-play
@inproceedings{zhang2024plug,
  title={Plug-and-play: An efficient post-training pruning method for large language models},
  author={Zhang, Yingtao and Bai, Haoli and Lin, Haokun and Zhao, Jialin and Hou, Lu and Cannistraci, Carlo Vittorio},
  booktitle={ICLR},
  year={2024}
}

%Deep compression
@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={ICLR},
  year={2016}
}

%Clip-q
@inproceedings{tung2018clip,
  title={Clip-q: Deep network compression learning by in-parallel pruning-quantization},
  author={Tung, Frederick and Mori, Greg},
  booktitle={CVPR},
  year={2018}
}

%Automatic compression
@inproceedings{yang2020automatic,
  title={Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach},
  author={Yang, Haichuan and Gui, Shupeng and Zhu, Yuhao and Liu, Ji},
  booktitle={CVPR},
  year={2020}
}

%opq
@inproceedings{hu2021opq,
  title={Opq: Compressing deep neural networks with one-shot pruning-quantization},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Aly, Mohamed M Sabry and Lin, Jie},
  booktitle={AAAI},
  year={2021}
}

%STQ-Nets
@inproceedings{munagala2020stq,
  title={STQ-Nets: Unifying Network Binarization and Structured Pruning.},
  author={Munagala, Sri Aurobindo and Prabhu, Ameya and Namboodiri, Anoop M},
  booktitle={BMVC},
  year={2020}
}

%Bnn pruning
@inproceedings{li2020bnn,
  title={Bnn pruning: Pruning binary neural network guided by weight flipping frequency},
  author={Li, Yixing and Ren, Fengbo},
  booktitle={ISQED},
  year={2020}
}

%Extremely sparse networks
@article{wang2021extremely,
  title={Extremely sparse networks via binary augmented pruning for fast image classification},
  author={Wang, Peisong and Li, Fanrong and Li, Gang and Cheng, Jian},
  journal={TNNLS},
  year={2021}
}

@article{dumitru2024change,
  title={Change is the only constant: Dynamic llm slicing based on layer redundancy},
  author={Dumitru, Razvan-Gabriel and Clotan, Paul-Ioan and Yadav, Vikas and Peteleaza, Darius and Surdeanu, Mihai},
  journal={arXiv preprint arXiv:2411.03513},
  year={2024}
}

%Optimal brain damage(prune)
@inproceedings{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  booktitle={NeurIPS},
  year={1989}
}

@misc{nvidia2020,
    title= {Nvidia a100 tensor core gpu architecture},
    author= {Nvidia},
    year= 2020,
}