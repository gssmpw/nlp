\section{Related Works}
\subsection{Weight Quantization}
Quantization compresses full-precision parameters into lower-bit representations, reducing both computation and storage demands. Current quantization methods for LLMs are mainly divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT____ integrates quantization during the training phase to enhance low-bit weight representations. However, due to the enormous parameter number, retraining becomes excessively expensive and inefficient for LLMs. PTQ, as it directly applies quantization to the model weights without retraining, making it faster and less resource-demanding. Recent methods, like ZeroQuant____ and BRECQ____, improve quantization accuracy by incorporating custom quantization blocks and group labels. While GPTQ____ and QuIP____ use second-order error compensation to reduce quantization errors. 

Binarization, as the most extreme form of quantization, reduces model parameters to a single bit (Â±1). Prominent methods, like Binary Weight Network (BWN)____ and XNOR-Net____, focus on binarizing the weights, with XNOR-Net____ also binarizing activations. In the context of LLM binarization, BitNet____, OneBit____, and BinaryMoS____ adopt the QAT framework, while BiLLM____, ARB-LLM____, and STBLLM____ use PTQ combined with residual approximation. Our work focuses on the binary PTQ method, achieving substantial improvements over existing SOTA binary PTQ methods.
\vspace{1mm}

\subsection{LLM Pruning} 
Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning____ eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning____, removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as $N$:$M$ sparsity, which is optimized for hardware, as seen in methods like SparseGPT____, Wanda____, and STBLLM____. Our approach leverages semi-structured pruning with $N$:$M$ sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.


\subsection{Integration of Pruning and Quantization}
The combination of pruning and quantization has been extensively explored for neural network compression. Pruning reduces parameter counts, while quantization minimizes parameter precision. For example, Deep Compression____ integrates pruning, quantization, and Huffman coding to reduce storage requirements for deep neural networks. Later studies____ have developed methods to combine pruning and quantization in parallel to optimize compression strategies. In extreme cases like binarization, methods such as STQ-Nets____, BNN Pruning ____, and BAP____ combine these techniques to achieve high compression ratios and speedups. STBLLM____ uses the structural binary method to compress LLMs to less than 1-bit precision, further advancing model compression and efficiency. However, simply combining pruning with binarization still leads to significant model degradation in LLMs. Therefore, we propose a new framework, PBS$^2$P, which aims to reduce the combined errors of pruning and binarization, improving the performance of the compressed model.
\vspace{-1mm}