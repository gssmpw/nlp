%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig} % Retained subfig
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For revision
\providecommand{\yulun}[1]{\textcolor{red}{[{\bf #1}]}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{arydshln} % For dashed lines in tables
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{colortbl}
\definecolor{colorTab}{rgb}{0.9,0.9,0.98}
\definecolor{color3}{gray}{0.95}
\usepackage{enumitem}
\usepackage{multicol}
% \usepackage{multirow}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\definecolor{css}{rgb}{0.7529, 0, 0}
\definecolor{fss}{rgb}{0, 0.7, 0.3}
\definecolor{pbp}{rgb}{0.2, 0.2, 0.6}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\mycomment}[1]{\hfill\small\texttt{$\triangleright$ #1}}




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Progressive Binarization with Semi-Structured Pruning for LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xianglong Yan}{equal,sjtu}
\icmlauthor{Tianao Zhang}{equal,sjtu}
\icmlauthor{Zhiteng Li}{sjtu}
\icmlauthor{Yulun Zhang\textsuperscript{\textdagger}}{sjtu}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sjtu}{Shanghai Jiao Tong University}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Yulun Zhang}{yulun100@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} 
% otherwise use the standard text.

\begin{abstract}
Large language models (LLMs) have achieved remarkable success in natural language processing tasks, but their high computational and memory demands pose challenges for deployment on resource-constrained devices. Binarization, as an efficient compression method that reduces model weights to just 1 bit, significantly lowers both computational and memory requirements. Despite this, the binarized LLM still contains redundancy, which can be further compressed. Semi-structured pruning provides a promising approach to achieve this, which offers a better trade-off between model performance and hardware efficiency. However, simply combining binarization with semi-structured pruning can lead to a significant performance drop. To address this issue, we propose a \textbf{P}rogressive \textbf{B}inarization with \textbf{S}emi-\textbf{S}tructured \textbf{P}runing (PBS$^2$P) method for LLM compression. We first propose a Stepwise semi-structured Pruning with Binarization Optimization (SPBO). Our optimization strategy significantly reduces the total error caused by pruning and binarization, even below that of the no-pruning scenario. Furthermore, we design a Coarse-to-Fine Search (CFS) method to select pruning elements more effectively. Extensive experiments demonstrate that PBS$^2$P achieves superior accuracy across various LLM families and evaluation metrics, noticeably outperforming state-of-the-art (SOTA) binary PTQ methods. The code and models will be available at \url{https://github.com/XIANGLONGYAN/PBS2P}. 
\end{abstract}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}

\section{Introduction}
Transformer-based large language model (LLM)~\cite{vaswani2017attention} have achieved outstanding results across various natural language processing (NLP) tasks. However, their exceptional performance is largely attributed to the massive scale, often consisting of billions of parameters. For example, the OPT (open pre-trained Transformer) series~\cite{zhang2022opt} include models with up to 66 billion parameters in its largest configuration. Likewise, the LLaMA family \cite{touvron2023llama1} features even larger models, such as the LLaMA3-70B \cite{dubey2024llama3}. These models push the boundaries of NLP capabilities. However, their substantial memory demands create significant hurdles for deployment on mobile devices and other systems with limited resources.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig1_v2.pdf}
\vspace{-7.5mm}
\caption{Perplexity of LLaMA-13B on WikiText2 under different bit-widths. Round-to-nearest (RTN) and GPTQ experience a sharp increase in perplexity at ultra-low bit-widths. While promising binarization methods, like BiLLM, ARB-LLM, and STBLLM, still exhibit a significant performance gap compared to full-precision models, our PBS$^2$P substantially narrows this gap.}
\label{fig:binary_results}
\vspace{-6mm}
\end{figure}

The compression of LLMs can be broadly classified into several approaches, including weight quantization~\cite{lin2024awq, frantar2022gptq}, low-rank factorization~\cite{zhang2023loraprune, yuan2023asvd}, network pruning~\cite{sun2023simple, frantar2023sparsegpt}, and knowledge distillation~\cite{zhong2024revisiting, gu2023knowledge}. Binarization, as an extreme form of quantization, can compress a model to 1 bit, significantly reducing its memory footprint. Currently, many binarization methods adopt Post-Training Quantization~\cite{huang2024billm,li2024arb,dong2024stbllm}, which simplifies computation by eliminating backpropagation, accelerating binarization and improving practicality. Specifically, BiLLM~\cite{huang2024billm} proposes a residual approximation strategy to improve 1-bit LLMs, while ARB-LLM~\cite{li2024arb} uses an alternating refined method to align the distribution between binarized and full-precision weights. STBLLM~\cite{dong2024stbllm} pushes the limits by compressing LLMs to less than 1-bit precision. However, binarized LLMs still exhibit redundancy, which can be further reduced for even greater compression.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig2_v4.pdf}
\vspace{-8.5mm}
\caption{Total error curves with varying pruning ratios. The total error reaches its minimum at an optimal pruning ratio.}
\label{fig:loss curve}
\vspace{-7mm}
\end{figure}

Pruning~\cite{lecun1989optimal} is a promising approach to further reduce redundancy in binarized models. However, conventional structured pruning~\cite{ma2023llm,ashkboos2024slicegpt,xia2023sheared,an2024fluctuation} often causes significant performance degradation in LLMs. Unstructured pruning~\cite{dong2024pruner} struggles with hardware acceleration and storage efficiency. Semi-structured pruning~\cite{frantar2023sparsegpt,sun2023simple,dong2024stbllm}, effectively reduces model redundancy, offering a balanced trade-off between performance and hardware efficiency. Yet, directly combining binarization and semi-structured pruning results in a substantial performance drop. Moreover, a significant challenge is how to effectively select pruning elements in LLMs to enhance the pruning efficiency and effectiveness, while preserving model performance.



To address those challenges, we propose a \textbf{P}rogressive \textbf{B}inarization with \textbf{S}emi-\textbf{S}tructured \textbf{P}runing (PBS$^2$P) for LLMs, which achieves significant model compression while maintaining strong performance (see Figure~\ref{fig:binary_results}). We first propose Stepwise semi-structured Pruning with Binarization Optimization (SPBO) method. SPBO prunes a subset of elements at each step and optimizes the binarized parameters simultaneously. This approach effectively minimizes the total error introduced by both pruning and binarization. Next, we design a Coarse-to-Fine Search (CFS) method to select pruning elements more effectively. In the coarse-stage, the pruning ratio for each layer is determined based on layer redundancy. In the fine-stage, we use a Hessian-based metric to identify the specific elements to prune, with the pruning ratio guiding the selection process. Surprisingly, we find that with our SPBO method, the total error at an appropriate pruning ratio can be even lower than that of binarization alone. Figure~\ref{fig:loss curve} shows the relationship between total error and pruning ratio, demonstrating the effectiveness of our method in reducing the total error.

Extensive experiments show that PBS$^2$P achieves SOTA performance across multiple LLM families, outperforming existing binary PTQ methods on various evaluation metrics. As illustrated in Figure~\ref{fig:binary_results}, on the WikiText-2~\cite{merity2016pointer} evaluation metric, PBS$^2$P achieves a perplexity of 6.20 on LLaMA-13B~\cite{touvron2023llama1} with an average bit-width of only 0.8 bits, compared to 5.47 for the full-precision model. PBS$^2$P significantly narrows the gap between binarized and full-precision models.

Our key contributions can be summarized as follows:
\vspace{-2.5mm}
\begin{itemize}[itemsep=4pt,topsep=6pt,parsep=0pt]
    \vspace{-1mm}
    \item We propose a novel framework, \textbf{PBS$^2$P}, which integrates binarization and semi-structured pruning seamlessly for effective LLM compression.
    \vspace{-1mm}
    \item We propose a Stepwise semi-structured Pruning with Binarization Optimization (SPBO) method, significantly reducing the total error from pruning and binarization, even below that of the no-pruning scenario.
    \vspace{-5mm}
    \item We design a Coarse-to-Fine Search (CFS) method to select pruning elements, which effectively reduces redundancy while maintaining performance.
    \vspace{-1mm}
    \item Extensive experiments demonstrate that \textbf{PBS$^2$P} outperforms SOTA binary PTQ methods, significantly narrowing the performance gap between binarized models and their full-precision counterparts.
\end{itemize}


\begin{figure*}[!ht]
\includegraphics[width=1\textwidth]{figs/overview_v5.pdf}
\vspace{-7mm}
\caption{Overview of our PBS$^2$P framework. \textcolor{css}{\textbf{Coarse-Stage Search}}: using the LR score to assign pruning ratios to each layer. \textcolor{fss}{\textbf{Fine-Stage Search}}: searching the elements to be pruned based on the Hessian-based score matrics. 
\textcolor{pbp}{\textbf{Stepwise semi-structured Pruning with Binarization Optimization }}: stepwise pruning with alternating optimization of binarized parameters.}
\label{main_pbpllm}
\vspace{-5mm}
\end{figure*}

\vspace{-3mm}
\section{Related Works}
\subsection{Weight Quantization}
Quantization compresses full-precision parameters into lower-bit representations, reducing both computation and storage demands. Current quantization methods for LLMs are mainly divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT~\cite{liu2023llm,chen2024db,du2024bitdistiller} integrates quantization during the training phase to enhance low-bit weight representations. However, due to the enormous parameter number, retraining becomes excessively expensive and inefficient for LLMs. PTQ, as it directly applies quantization to the model weights without retraining, making it faster and less resource-demanding. Recent methods, like ZeroQuant~\cite{yao2022zeroquant} and BRECQ~\cite{li2021brecq}, improve quantization accuracy by incorporating custom quantization blocks and group labels. While GPTQ~\cite{frantar2022gptq} and QuIP~\cite{chee2024quip} use second-order error compensation to reduce quantization errors. 

Binarization, as the most extreme form of quantization, reduces model parameters to a single bit (±1). Prominent methods, like Binary Weight Network (BWN)~\cite{rastegari2016xnor} and XNOR-Net~\cite{rastegari2016xnor}, focus on binarizing the weights, with XNOR-Net~\cite{rastegari2016xnor} also binarizing activations. In the context of LLM binarization, BitNet~\cite{wang2023bitnet}, OneBit~\citep{xu2024onebit}, and BinaryMoS~\cite{jo2024mixture} adopt the QAT framework, while BiLLM~\cite{huang2024billm}, ARB-LLM~\cite{li2024arb}, and STBLLM\cite{dong2024stbllm} use PTQ combined with residual approximation. Our work focuses on the binary PTQ method, achieving substantial improvements over existing SOTA binary PTQ methods.
\vspace{1mm}

\subsection{LLM Pruning} 
Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning~\cite{ma2023llm,ashkboos2024slicegpt,xia2023sheared,an2024fluctuation} eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning~\cite{dong2024pruner}, removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as $N$:$M$ sparsity, which is optimized for hardware, as seen in methods like SparseGPT~\cite{frantar2023sparsegpt}, Wanda~\cite{sun2023simple}, and STBLLM~\cite{dong2024stbllm}. Our approach leverages semi-structured pruning with $N$:$M$ sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.


\subsection{Integration of Pruning and Quantization}
The combination of pruning and quantization has been extensively explored for neural network compression. Pruning reduces parameter counts, while quantization minimizes parameter precision. For example, Deep Compression~\cite{han2015deep} integrates pruning, quantization, and Huffman coding to reduce storage requirements for deep neural networks. Later studies~\citep{tung2018clip, yang2020automatic, hu2021opq} have developed methods to combine pruning and quantization in parallel to optimize compression strategies. In extreme cases like binarization, methods such as STQ-Nets~\cite{munagala2020stq}, BNN Pruning ~\cite{li2020bnn}, and BAP~\cite{wang2021extremely} combine these techniques to achieve high compression ratios and speedups. STBLLM~\cite{dong2024stbllm} uses the structural binary method to compress LLMs to less than 1-bit precision, further advancing model compression and efficiency. However, simply combining pruning with binarization still leads to significant model degradation in LLMs. Therefore, we propose a new framework, PBS$^2$P, which aims to reduce the combined errors of pruning and binarization, improving the performance of the compressed model.
\vspace{-1mm}
\section{Method}
\vspace{-1mm}
In this section, We first introduce the fundamentals of binarization and semi-structured pruning in Section \ref{observation}. We then propose two novel methods: Stepwise semi-structured Pruning with Binarization Optimization (SPBO) and Coarse-to-Fine Search (CFS), which are elaborated in Sections \ref{PBP} and \ref{C2FS}, respectively. In Section \ref{Pipeline}, we provide the pipeline of PBS$^2$P (Fig.~\ref{main_pbpllm}) and its implementation details.

\subsection{Preliminary}\label{observation}
\textbf{Binarization.$\quad$} First, let’s briefly review the process of standard binarization~\cite{rastegari2016xnor}. Binarization begins by performing a row-wise redistribution on the full-precision matrix  $\mathbf{W}\in \mathbb{R}^{ n \times m}$ , resulting in a matrix  $\widetilde{\mathbf{W}}\in \mathbb{R}^{ n \times m}$  with a mean of 0:
\begin{equation} \label{eq:1}
    \widetilde{\mathbf{W}} = \mathbf{W} -  \mu, \quad \text{where} \, \mu= \frac{1}{m}\sum_{j=1}^m\mathbf{W}_{.j}\, .
\end{equation}
Then, the objective of binarization is defined as follows:
\begin{equation}
    \label{eq:2} \mathop{\arg\min}\limits_{\alpha,\mathbf{B}}||\widetilde{\mathbf{W}}- 
    \alpha \mathbf{B}||^2_F,
\end{equation}
where  $\alpha \in \mathbb{R}^n$  represents the scaling factor applied to each row, and  $\mathbf{B} \in \{+1, -1\}^{n \times m}$  is the binary matrix. Under the binarization objective (\Cref{eq:2}), the optimal solutions for \( \alpha \) and \( \mathbf{B} \) are given by \( \alpha = \frac{1}{m} \sum_{j=1}^m |\widetilde{\mathbf{W}}_{.j}| \) and \( \mathbf{B} = \operatorname{sign}(\widetilde{\mathbf{W}}) \), respectively~\cite{huang2024billm}. 


\textbf{N:M Sparsity.$\quad$}
$N$:$M$ sparsity is a form of semi-structured pruning that strikes a balance between model compression and hardware efficiency. Unlike unstructured pruning, which removes arbitrary weights and leads to irregular sparsity patterns that are difficult to accelerate on modern hardware, $N$:$M$ sparsity enforces a structured constraint by retaining exactly $N$ nonzero elements out of every $M$ consecutive weights~\cite{frantar2023sparsegpt,sun2023simple,dong2024stbllm}. A common setting, such as 2:4 sparsity, ensures that only two out of every four weights are retained in a layer, effectively reducing the number of parameters while maintaining computational efficiency. Due to this structured constraint, $N$:$M$ sparsity enables efficient deployment on NVIDIA Ampere architecture~\cite{nvidia2020}, leveraging hardware acceleration capabilities.



\subsection{Stepwise Semi-Structured Pruning with Binarization Optimization}\label{PBP}
We first define the total error $\mathcal{L}_2$ for binarization and semi-structured pruning as follows:
\begin{equation}
    \label{eq:layerwise-pruning}   
    \mathcal{L}_2=||\mathbf{W} \mathbf{X} - (\mathbf{M}_{\text{p}} \odot  \mathbf{\widehat{W}}) \mathbf{X}||_F^2,
\end{equation}
where $\mathbf{X}$ denotes the calibration data, $\mathbf{M}_\text{p}$ denotes the pruning mask. Optimizing both of the pruning mask $\mathbf{M}_{\text{p}}$ and binarized parameters $\mathbf{\widehat{W}}$  simultaneously is an NP-hard problem. Therefore, we focus on the optimization of binarized parameters with a fixed mask. The selection of the pruning mask will be elaborated in Section \ref{C2FS}. 


\textbf{N:M Sparsity Pruning Mask Group.$\quad$} 
We apply $N$:$M$ sparsity, a semi-structured pruning method to prune the model. We segment the overall pruning mask into $M-N$ sub-pruning masks, as illustrated in Figure 4. In the $N$:$M$ sparsity scheme, every $M$ consecutive elements will have $M-N$ elements pruned, retaining $N$ elements. The $M-N$ pruned elements are distributed across $M-N$ pruning masks. This process is given by the following formula:
\begin{equation}
   \mathbf{M}_{\text{p}} = \bigcup_{i=1}^{M-N} \mathbf{M}_{\text{p}}^i.
   \vspace{-1mm}
\end{equation}
This way, we can distribute the optimization of the binarized parameters across each pruning step. This ensures that not too many elements are pruned at once, thus preventing the binarization difficulty from increasing. A detailed analysis of binarization difficulty and the impact of pruning on binarization can be found in the supplementary material.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/fig6.pdf}
\vspace{-8.5mm}
\caption{Illustration of the $N$:$M$ sparsity pruning mask segmentation, where the pruning mask is divided into $M-N$ sub-pruning masks, each representing a subset of the pruned elements.}
\label{fig:prune_mask}
\vspace{-5.5mm}
\end{figure}

\textbf{Stepwise Pruning with Binarization Optimization.$\quad$} 
Our SPBO method optimizes the binarized parameters after each pruning step, which significantly reduces $\mathcal{L}_2$. Inspired by ARB-LLM~\cite{li2024arb}, we decouple $\mathbf{X}$ and  $\mathbf{W}$ to reduce computational complexity. We perform a total of $M-N$  pruning steps to achieve $N$:$M$ sparsity. Suppose we are currently performing the $k$-th pruning step, we first calculate the $k$-th pruning mask $(\mathbf{M}_\text{p})_k$:
\begin{equation}
   (\mathbf{M}_{\text{p}})_k = \bigcup_{i=1}^{k} \mathbf{M}_{\text{p}}^i.
\end{equation}
Then, we obtain the binarized matrix $\mathbf{B}_k$ and full-precision weight matrix $\mathbf{W}_k$ after the $k$-th pruning round:
\begin{equation}
   \mathbf{B}_k = \mathbf{B}\odot(\mathbf{M}_\text{p})_k, \, \,\,\mathbf{W}_k = \mathbf{W}\odot(\mathbf{M}_\text{p})_k.
\end{equation}
We begin optimizing the binarized parameters $\mu$ and $\alpha$. First, we set $\partial \mathcal{L}_2 / \partial \mu=0$ to adjust $\mu$ during the $k$-th pruning:
\begin{align}
\mu_k = \frac{\mathbf{1}^\top \mathbf{S} (\mathbf{W}_k - \alpha_{k-1} \mathbf{B}_k)^\top}{\mathbf{1}^\top \mathbf{S} \mathbf{1}},
\end{align}
where $\mathbf{S} = \sum\nolimits_b\mathbf{X}_b^T\mathbf{X}_b$ represents the decoupled calibration data $\mathbf{X}$. Then, we adjust  $\alpha$  by setting $\partial \mathcal{L}_2 / \partial \alpha=0$:
\begin{align}
\alpha_k= \frac{\operatorname{diag}(\mathbf{B}_k\mathbf{S}(\mathbf{W}_k-\mu_k)^\top)}{\operatorname{diag}(\mathbf{B}_k\mathbf{S}\mathbf{B}_k^\top)}.
\end{align}
During the $k$-th pruning round, we can alternately update $\mu_k$ and $\alpha_k$ to further reduce $\mathcal{L}_2$. We perform total $M-N$ steps of pruning until all pruning elements are removed. For the detailed procedure of the SPBO method, refer to Algorithm~\ref{mainalg}. The detailed derivation of optimizing $\mu$ and $\alpha$ can be found in the supplementary material.

\begin{algorithm*}[!h]
    \caption{SBPO algorithm: More details of each function are shown in supplementary material}
\vspace{-4.2mm}
\begin{multicols}{2}
\label{mainalg}
func $\operatorname{SPBO}$($\mathbf{W}$, $\mathbf{M}_\text{p}$, $\mathbf{X}$ ,$T$)\\ 
{\bf Input:} $\mathbf{W} \in \mathbb{R}^{n\times m}$ - full-precision weight \\
\hspace*{0.43in}$ \mathbf{M}_\text{p} \in \mathbb{R}^{n\times m}$ - pruning mask \\
\hspace*{0.43in}$ \mathbf{X} \in \mathbb{R}^{B\times L \times m}$ - calibration data \\
\hspace*{0.43in}$T$ - binarized parameter optimization steps\\
{\bf Output:} $ \mathbf{\widehat{W}} \in \mathbb{R}^{n\times m}$
\begin{algorithmic}[1]
\STATE $\mathbf{S} \coloneqq  \operatorname{X2S}(\mathbf{X})$ \mycomment{decouple $\mathbf{W}$ and $\mathbf{X}$}
\STATE $\widehat{\mathbf{W}},\alpha,\mathbf{B},\mu \coloneqq \operatorname{binary}(\mathbf{W})$
\STATE $\textbf{M}_\text{group} \coloneqq \operatorname{split\_mask}(\mathbf{M}_\text{p})$
\STATE $\mathbf{M}_\text{k} \coloneqq \mathbf{M}_\text{group}[0] $
\FOR{$\textbf{M} \text{ in } \mathbf{M}_\text{group}$}
    \STATE $\mathbf{B} \leftarrow \mathbf{B}\odot \mathbf{M}$\mycomment{prune binarized matrix}
    \STATE $\mathbf{W} \leftarrow \mathbf{W}\odot \mathbf{M}$
    \STATE $\mathbf{M}_k \leftarrow \mathbf{M}_k \cup \mathbf{M}$ \mycomment{current pruning mask}

    \FOR{$iter = 1, 2, \ldots, T$}
   
       % \STATE $\mathbf{P}\coloneqq \operatorname{einsum}(\mathbf{R})$
    % \STATE $\mathcal{L} \coloneqq \sum_k\sum_l (\mathbf{S}\odot  \mathbf{P})_{kl}$
        \STATE $\mu \leftarrow \operatorname{update\_\mu}(\mathbf{S},\mathbf{W},\mathbf{B},\alpha)$\mycomment{update $\mu$}
        \STATE $\alpha \leftarrow \operatorname{update\_\alpha}(\mathbf{S},\mathbf{W},\mu,\mathbf{B})$\mycomment{update $\alpha$}
        \STATE $\widehat{\mathbf{W}} \leftarrow (\alpha\cdot\mathbf{B}+\mu)\odot\mathbf{M}_k$
    \ENDFOR
\ENDFOR

\STATE {\bf return}  $\mathbf{\widehat{W}}$

\end{algorithmic}

func $\operatorname{update\_\alpha}$ $(\mathbf{S},\mathbf{W},\mathbf{\mu},\mathbf{B})$
\begin{algorithmic}[1]
\STATE $\widetilde{\mathbf{W}}\coloneqq \mathbf{W}-\mu$
\FOR{$i = 1, 2, \ldots, n$}
    \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
        \STATE $\mathbf{U}_{kl}\coloneqq \mathbf{B}_{ik}\widetilde{\mathbf{W}}_{il}$
        \STATE $\mathbf{V}_{kl}\coloneqq\mathbf{B}_{ik}\mathbf{B}_{il}$
    \ENDFOR
    \STATE $num \coloneqq \sum_k\sum_l(\mathbf{S}\odot \mathbf{U})_{kl}$
    \STATE $den \coloneqq \sum_k\sum_l(\mathbf{S}\odot\mathbf{V})_{kl}+\epsilon$
    \STATE $\alpha_i \coloneqq \frac{num}{den}$\mycomment{refined $\alpha$}
\ENDFOR
\STATE {\bf return}  $\alpha$
\end{algorithmic}

func $\operatorname{update\_\mu}$ $(\mathbf{S},\mathbf{W},\mathbf{B},\alpha)$
\begin{algorithmic}[1]
\FOR{$i = 1, 2, \ldots, n$}
    \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
        \STATE $\mathbf{P}_{kl}\coloneqq\mathbf{W}_{ik}-\alpha_i\mathbf{B}_{il}$
    \ENDFOR
    \STATE $num \coloneqq \sum_k\sum_l(\mathbf{S}\odot\mathbf{P})_{kl}$
    \STATE $den \coloneqq \sum_k\sum_l\mathbf{S}_{kl}+\epsilon$
    \STATE $\mu_i \coloneqq \frac{num}{den}$\mycomment{refined $\mu$}
\ENDFOR
\STATE {\bf return}  $\mu$
\end{algorithmic}

func $\operatorname{binary}$ $(\mathbf{W})$
\begin{algorithmic}[1]
\STATE $\mu \coloneqq \frac{1}{m}\sum_{j=1}^m\mathbf{W}_{.j}$
\STATE $\widetilde{\mathbf{W}}\coloneqq \mathbf{W}-\mu$\mycomment{row-wise redistribution}
% \STATE $\alpha \coloneqq \dfrac{||\widetilde{\mathbf{W}}||_{\ell1}}{n\times m}$
\STATE $\alpha \coloneqq \frac{1}{m}\sum_{j=1}^m|\widetilde{\mathbf{W}}_{.j}|$\mycomment{row-wise scaling factor}
\STATE $\mathbf{B} \coloneqq  \operatorname{sign}(\widetilde{\mathbf{W}})$
\STATE $\mathbf{\widehat{W}} \coloneqq \alpha\cdot \mathbf{B} + \mu $\mycomment{binarized output}
\STATE {\bf return}  $\mathbf{\widehat{W}},\alpha,\mathbf{B},\mu$
\end{algorithmic}


func $\operatorname{X2S}$ $(\mathbf{X})$
\begin{algorithmic}[1]
\FOR{$b = 1, 2, \ldots B$}
    \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
        \STATE  $\mathbf{S}_{kl} = \sum_b\sum_i(\mathbf{X}_b)_{ik}(\mathbf{X}_b)_{il}$
    \ENDFOR
\ENDFOR
\STATE {\bf return}  $\mathbf{S}$
\end{algorithmic}






\end{multicols}
\vspace{-3mm}
\end{algorithm*}
% \vspace{-4mm}



\subsection{Coarse-to-Fine Search}\label{C2FS}
\textbf{Coarse-Stage: Pruning Ratio Allocation.$\quad$} Inspired by \citet{dumitru2024change}, we use the Layer Redundancy (LR) score to measure the importance of different layers:
\begin{equation}
    \text{LR}(\textbf{L}_i) = \frac{\mathbf{I}_i \cdot \mathbf{O}_i}{\|\mathbf{I}_i\|_2 \|\mathbf{O}_i\|_2},
\label{LRScore}
\end{equation}

where \( \mathbf{I_i} \) denotes the input of the \(i\)-th layer $\mathbf{L}_i$, and \( \mathbf{O_i} \) denotes the output of the same layer. A higher LR score indicates greater redundancy within the layer, meaning that the importance of this layer is lower. We take the different importance of layers into account when selecting pruning elements. We first rank the layers of the model from low to high based on the LR score, with  $k_i$  denoting the rank of the i-th layer. The corresponding pruning parameter $N_i$  for the i-th layer is then assigned using the following formula:
\begin{equation}
    N_i = \left\lfloor N_{\text{high}} - \left( N_{\text{high}} - N_{\text{low}} \right) \cdot \frac{k_i-1}{L-1} + \frac{1}{2} \right\rfloor,
\end{equation}
where \( L \) represents the total number of layers in the model. \( N_{\text{high}} \) is the maximum pruning parameter, and \( N_{\text{low}} \) is the minimum pruning parameter. We define the target pruning parameter \( N_{\text{target}} \) as the average pruning parameter across the entire model. In our experiments, \( N_{\text{high}} \) is set to \( N_{\text{target}} + 1 \), while \( N_{\text{low}} \) is set to \( N_{\text{target}} - 1 \).


\begin{table*}[!t]
\vspace{-2mm}
\renewcommand{\arraystretch}{1.0}
\footnotesize
\centering
% \setlength{\tabcolsep}{1.90mm}
\caption{Perplexity comparison of RTN, GPTQ, BiLLM, ARB-LLM, STBLLM, and PBS$^2$P on the LLaMA and OPT families. The evaluation results demonstrate the perplexity performance on the Wikitext2 dataset across various model sizes. }
\label{tab:llama_series}
\vspace{2mm}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccrrrrrrcrrr}
    \toprule
    \rowcolor{color3}
    \multicolumn{3}{c}{\textbf{Settings}} & \multicolumn{4}{c}{\textbf{ LLaMA-1}}  & \multicolumn{2}{c}{\textbf{ LLaMA-2}} & \multicolumn{1}{c}{\textbf{ LLaMA-3}}  &
    \multicolumn{3}{c}{\textbf{ OPT}}\\
    \midrule
   Method & \#Block & \multicolumn{1}{p{4.19em}}{W-Bits} & 7B    & 13B   & 30B   & 65B   & 7B    & 13B   & 8B & 1.3B & 2.7B & 30B\\
    \midrule
    FP16 & -     & 16    & 5.68  & 5.09  & 4.1   & 3.53  & 5.47  & 4.88  & 6.14 & 14.62 & 12.47 & 9.56\\
    \cdashline{1-13}
    \addlinespace[0.2em]

    RTN    &   -  & 3  & 25.54 & 11.40 & 14.89 & 10.59 & 5.4e2 & 10.68 & 2.2e3 
    & 1.3e4 & 1.6e4 & 1.6e3\\
    GPTQ     & 128  & 3 & 8.63 & 5.67 & 4.87 & 4.17
    & 6.44   & 5.46  & 18.68  & 16.45 & 13.61 & 9.71\\ 
    RTN    &   -  & 2  & 1.1e5 & 5.7e4 & 2.7e4 & 2.0e4 & 1.8e4 & 5.1e4 & 1.3e6
    & 1.1e4 & 9.5e3 & 1.7e5\\
    GPTQ     & 128  & 2 & 1.3e2 & 20.46 & 15.29 & 8.66
    & 52.22   & 23.63  & 1.4e3  & 1.2e2 & 59.53 & 13.04\\ 
    \cdashline{1-13}

    \addlinespace[0.2em]
    RTN    &   -  & 1  & 1.7e5 & 1.4e6 & 1.5e4 & 6.5e4 & 1.6e5 & 4.8e4 & 1.4e6 
    & 1.7e4 & 3.7e4 & 6.5e3\\
    GPTQ     & 128  & 1 & 1.6e5 & 1.3e5 & 1.0e4 & 2.0e4
    & 6.0e4   & 2.3e4  & 1.1e6  & 8.7e3 & 1.2e4 & 1.4e4\\ 
    BiLLM & 128   & 1.11  & 49.79 & 14.58 & 9.90 & 8.37  & 32.31 & 21.35 & 55.80 
    & 69.05 & 48.61 &  13.86\\
    ARB-LLM & 128   & 1.11  & 14.03 & 10.18 & 7.75 & 6.56  & 16.44 & 11.85 & 27.42
    & 26.63 & 19.84 & 11.12\\
    \cdashline{1-13}
    \addlinespace[0.2em]
    STBLLM & 128   & 0.80  & 15.03 & 9.66  & 7.56  & 6.43  & 13.06 & 11.67 & 33.44  & 29.84 & 17.02 & 12.80\\
    STBLLM & 128   & 0.70   & 19.48 & 11.33 & 9.19  & 7.91  & 18.74 & 13.26 & 49.12 & 33.01 & 20.82 & 14.38\\
    STBLLM & 128   & 0.55 & 31.72 & 17.22 & 13.43 & 11.07 & 27.93 & 20.57 & 253.76 &45.11 & 30.34 & 18.80\\
    \cdashline{1-13}
    \addlinespace[0.2em]
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.80  & \textbf{7.36} & \textbf{6.20} & \textbf{5.21} & \textbf{4.60} & \textbf{7.17} & \textbf{6.27} & \textbf{10.75} & \textbf{23.72} & \textbf{18.32} & \textbf{10.50}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.70  & \textbf{8.09} & \textbf{6.85} & \textbf{5.78} & \textbf{5.09} & \textbf{8.00} & \textbf{6.89} & \textbf{12.29} & \textbf{27.10} & \textbf{20.17} & \textbf{10.82}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.55   & \textbf{10.78} & \textbf{9.24} & \textbf{7.19} & \textbf{6.39} & \textbf{10.64} & \textbf{8.68} & \textbf{17.45} & \textbf{44.53} & \textbf{27.42} & \textbf{11.87}\\
    \bottomrule
    \end{tabular}
}
\vspace{-5mm}
\end{table*}

\vspace{1mm}
\textbf{Fine-Stage: Selecting Pruning Elements.$\quad$} Once the pruning parameter $N$ is assigned in the coarse-stage, we proceed to search for the specific pruning elements within each layer. Inspired by BiLLM~\cite{huang2024billm}, we utilize the Hessian matrix to evaluate the importance of elements in the weight matrix. The Hessian-based score matrix is defined as $s_i = \frac{w_i^2}{\left[\mathbf{H}^{-1}\right]_{ii}^2}$, where \( \mathbf{H} \) denotes the Hessian matrix of the layer, and \( w_i \) represents the original value of each element. Based on the given $N$:$M$ ratio and the corresponding  $s_i$  values, we select the top $N$ elements with the largest  $s_i$ values from every $M$ consecutive elements to retain. The other $M-N$ elements are selected for pruning.


\subsection{PBS$^2$P Pipeline}\label{Pipeline}
\textbf{PBS$^2$P Workflow.$\quad$}
PBS$^2$P quantizes all the linear layers in each transformer block of the LLM. We first perform the coarse-stage search to determine the pruning ratio for different transformer blocks. Then we proceed with the fine-stage search for pruning elements layer by layer and perform stepwise semi-structured pruning with binarization optimization. Following BiLLM~\cite{huang2024billm}, ARB-LLM~\cite{li2024arb}, and STBLLM~\cite{dong2024stbllm}, we divide the weights into salient and non-salient parts. For the salient part, we employ residual binarization. For the non-salient part, similar to STBLLM, we use two split points  $p$  to divide the non-salient weights into three segments, each binarized separately. It ensuring that the total parameter count remains the same for a fair comparison. We adopt the block-wise compensation strategies from GPTQ~\cite{frantar2022gptq} and OBC~\cite{frantar2022optimal} to further mitigate quantization error.

\textbf{Average Bits.$\quad$}
Following BiLLM~\cite{huang2024billm} and STBLLM~\cite{dong2024stbllm}, we add extra bits while pruning the redundant or less important weights. The weight parameters and additional hardware overhead are as follows:
% \vspace{3mm}
\begin{equation}
    % N_{total} = \{2 * r_{salient} + 1 * (1-r_{salient})\} + \{ 1 + \frac{1}{b_{size}} \}
    % \begin{cases}
    % N_{param} = 2 * r_{salient} + 1 * (1-r_{salient})
    % b \quad x > 0
    % \end{cases}
    \left\{
         \begin{array}{lr}
         N_{\text{param}} = \left[2 \times r_{\text{salient}} +(1 - r_{\text{salient}}) \right] \times \frac{N}{M}, \\
         N_{\text{storing}} =  2 + \dfrac{1}{b_{\text{size}}}, \\
         \end{array}
    \right.
\end{equation}
where \( r_{\text{salient}} \) represents the proportion of salient weights, \( N \):\( M \) denotes the predefined pruning parameters for the entire model, and \( b_{\text{size}} \) indicates the block size used in OBC compensation, with 2 bits reserved to mark the division between salient and non-salient weights. Our parameter settings and $N$:$M$ configuration are identical to our main comparison method STBLLM. We have the same bit-width with STBLLM, ensuring a fair comparison.



\section{Experiments}
\subsection{Settings}
All experiments are performed using PyTorch~\cite{paszke2019pytorch} and Huggingface~\cite{paszke1912imperative} on a single NVIDIA A800-80GB GPU. Following the work of \citet{frantar2022gptq}, \citet{huang2024billm}, and \citet{li2024arb}, we use 128 samples from the C4~\citep{raffel2020exploring} dataset for calibration. Since PBS$^2$P is an efficient PTQ framework, it eliminates fine-tuning, enabling completion through a single process combining binarization and pruning.

\textbf{Models and Datasets.$\quad$}
We conduct extensive experiments on the LLaMA~\cite{touvron2023llama1}, LLaMA-2~\cite{touvron2023llama2}, and LLaMA-3~\cite{dubey2024llama3} families and the OPT family~\citep{zhang2022opt}. To evaluate the effectiveness of PBS$^2$P, we measure the perplexity of LLM's outputs on WikiText2~\citep{merity2016pointer}, PTB~\citep{marcus1994penn}, and C4~\citep{raffel2020exploring}. Moreover, we also evaluate the accuracy for 7 zero-shot QA datasets: ARC-c~\citep{clark2018think}, ARC-e~\citep{clark2018think}, BoolQ~\citep{clark2019boolq}, Hellaswag~\citep{zellers2019hellaswag}, OBQA~\citep{mihaylov2018can}, RTE~\citep{chakrabarty2021figurative}, and Winogrande~\citep{sakaguchi2019adversarial}.

\begin{table*}[!t]
  \centering
    \caption{
  Accuracies (\%) for 7 zero-shot tasks from semi-structured binarized LLaMA-1-13B, LLaMA-2-13B, and LLaMA-1-30B with STBLLM and PBS$^2$P. We compare the accuracies under the same $N$:$M$ settings. }
\vspace{1mm}
\resizebox{1.0\textwidth}{!}{
  \setlength{\tabcolsep}{5.5pt}
    \begin{tabular}{llccccccccc}
    \toprule
    \rowcolor{color3}
    \textbf{Models} & \textbf{Method} &\textbf{W-Bits}& \textbf{Winogrande} & \textbf{OBQA}  & \textbf{Hellaswag} & \textbf{Boolq} & \textbf{ARC-e}  & \textbf{ARC-c}  & \textbf{RTE}   & \textbf{Average} \\
    \midrule
    % \multirow
          & FP16 &16 & 72.69  & 33.20  & 59.91  & 77.89  & 77.40  & 46.42  & 70.40  & 63.80  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & STBLLM&0.80 & 65.98  & 36.20  & 63.67  & 65.38  & 68.86  & 34.04  & 56.68  & 55.83  \\
          \textbf{LLaMA-1-13B}& STBLLM&0.55 & 63.06  & 34.80  & 52.65  & 62.48  & 56.90  & 28.33  & 52.71  & 50.13  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80& \cellcolor{colorTab}\textbf{72.77} & \cellcolor{colorTab}\textbf{31.00} & \cellcolor{colorTab}\textbf{54.80} & \cellcolor{colorTab}\textbf{74.71} & \cellcolor{colorTab}\textbf{74.37} & \cellcolor{colorTab}\textbf{42.32} & \cellcolor{colorTab}\textbf{68.23} & \cellcolor{colorTab}\textbf{59.74}  \\
          & \cellcolor{colorTab}PBS$^2$P&\cellcolor{colorTab}0.55 &\cellcolor{colorTab}\textbf{69.30} & \cellcolor{colorTab}\textbf{26.80} & \cellcolor{colorTab}\textbf{46.83} & \cellcolor{colorTab}\textbf{71.56} & \cellcolor{colorTab}\textbf{65.70} & \cellcolor{colorTab}\textbf{32.68} & \cellcolor{colorTab}\textbf{55.96} & \cellcolor{colorTab}\textbf{52.69}  \\
    \midrule
    % \multirow 
          & full-precision&16 & 72.22  & 35.20  & 60.03  & 80.55  & 79.42  & 48.38  & 65.34  & 65.00  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & STBLLM&0.80 & 63.93  & 37.00  & 57.76  & 71.53  & 60.56  & 31.99  & 54.15  & 53.85  \\
          \textbf{LLaMA-2-13B}& STBLLM&0.55 & 55.88  & 29.40  & 44.03  & 64.31  & 48.86  & 26.54  & 52.71  & 45.96  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{72.45} & \cellcolor{colorTab}\textbf{31.00} & \cellcolor{colorTab}\textbf{54.43} & \cellcolor{colorTab}\textbf{80.61} & \cellcolor{colorTab}\textbf{74.28} & \cellcolor{colorTab}\textbf{42.75} & \cellcolor{colorTab}\textbf{59.21} & \cellcolor{colorTab}\textbf{59.24} \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{69.85} & \cellcolor{colorTab}\textbf{27.00} & \cellcolor{colorTab}\textbf{47.75} & \cellcolor{colorTab}\textbf{75.50} & \cellcolor{colorTab}\textbf{69.19} & \cellcolor{colorTab}\textbf{35.58} & \cellcolor{colorTab}\textbf{62.82} & \cellcolor{colorTab}\textbf{55.38} \\
    \midrule
    % \multirow
          & full-precision&16 & 75.77  & 36.00  & 63.37  & 82.69  & 80.30  & 52.90  & 67.15  & 67.40  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & STBLLM &0.80 & 71.59 & 41.00 & 69.85 & 77.37 & 71.55 & 41.3 & 48.01 & 60.10 \\
          \textbf{LLaMA-1-30B}& STBLLM &0.55 & 64.01 & 34.60 & 56.46 & 63.06 & 60.86 & 31.48 & 51.99 & 51.78 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{75.93} & \cellcolor{colorTab}\textbf{35.00} & \cellcolor{colorTab}\textbf{59.45} & \cellcolor{colorTab}\textbf{82.14} & \cellcolor{colorTab}\textbf{79.29} & \cellcolor{colorTab}\textbf{47.95} & \cellcolor{colorTab}\textbf{63.18} & \cellcolor{colorTab}\textbf{63.28}  \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{72.14} & \cellcolor{colorTab}\textbf{31.20} & \cellcolor{colorTab}\textbf{53.00} & \cellcolor{colorTab}\textbf{79.76} & \cellcolor{colorTab}\textbf{73.99} & \cellcolor{colorTab}\textbf{41.13} & \cellcolor{colorTab}\textbf{69.31} & \cellcolor{colorTab}\textbf{60.08} \\
    \bottomrule
    \end{tabular}%
}
\vspace{-3mm}
\label{tab:zero_shot_acc}
\end{table*}

\begin{figure*}[!t]
\centerline{\includegraphics[width=1.\textwidth]{figs/fig5_v2.pdf}}
\vspace{-0.15in}
\caption{BiLLM, ARB-LLM, and PBS$^2$P performed on the PTB and c4 datasets, mainly on LLaMA-7B, LLaMA-13B, and LLaMA-2-7B, and we found that PBS$^2$P at 0.8 bits outperforms other methods at 1.11 bits.}
\label{ptb_c4}
\vspace{-5mm}
\end{figure*}

\textbf{Baselines.$\quad$}
We mainly compare our PBS$^2$P with STBLLM~\cite{dong2024stbllm}, a structural binary PTQ framework designed for compressing LLMs to precisions lower than 1-bit. We implement PBS$^2$P with a block size set to 128. We compare the results of PBS$^2$P with STBLLM under the same $N$:$M$ settings (\textit{e.g.}, 4:8, 5:8, 6:8). Previous low-bit methods like ARB-LLM~\cite{li2024arb}, BiLLM~\cite{huang2024billm}, GTPQ~\citep{frantar2022gptq} and vanilla RTN are also selected for comparison. 


\vspace{-1mm}
\subsection{Main Results}
We perform a comprehensive comparison of different LLM families (like LLaMA and OPT) with various model sizes. To keep fairness, We follow STBLLM~\cite{dong2024stbllm} to report the average bit-width of all methods, where our methods have the same bit-width as STBLLM. 

\textbf{Results on LLaMA Family.$\quad$}As shown in Table~\ref{tab:llama_series}, the models using RTN and GPTQ methods find it hard to maintain model performance at 1-bit precision. BiLLM achieves a satisfactory perplexity of 1.11 bits but performs worse than ARB-LLM at the same bit-width. At sub-1-bit precision, PBS$^2$P surpasses STBLLM and significantly reduces perplexity at the same bit-width across model sizes from 7B to 65B. For instance, PBS$^2$P achieves a substantial improvement over STB-LLM on LLaMA-1-7B, with perplexity dropping from 31.72 to 10.78, a reduction of approximately 66.0\%, in the extreme case of 4:8 structured binarization, where half of the parameters are pruned. 

Furthermore, PBS$^2$P, with a precision of 0.8 bits, outperforms both RTN at 3 bits, GPTQ at 2 bits, BiLLM and ARB-LLM at 1.11 bits, and STBLLM at 0.8 bits in terms of perplexity across all model sizes. Those comparisons show that our PBS$^2$P achieves a better trade-off between bit precision and performance. It is worth noting that PBS$^2$P outperforms GPTQ at 3 bits on LLaMA 1-7B and LLaMA 3-8B. We extend perplexity evaluation to the PTB and C4 datasets. Figure~\ref{ptb_c4} shows the performance of the LLaMA-7B, LLaMA-13B, and LLaMA-2-7B models. PBS$^2$P continues to achieve a leading edge in performance while operating at a relatively lower bit-width compared to other methods.

\textbf{Results on OPT Family.$\quad$} We extend our experiments to the OPT family (1.3B to 30B) under sub-1-bit PTQ settings, similar to the setup for LLaMA family. As shown in Table~\ref{tab:llama_series}, PBS$^2$P continues to outperform STBLLM across most of the models and $N$:$M$ structured binarization configurations. More results are provided in the supplementary material. 



\subsection{Zero-Shot Results}
To provide a more thorough evaluation of binary LLMs, we extend our experiments to 7 zero-shot datasets and test on models from the LLaMA family: LLaMA-1-13B, LLaMA-2-13B, and LLaMA-1-30B. Each model is evaluated across various compression methods, including full-precision, STBLLM (6:8), STBLLM (4:8), PBS$^2$P (6:8), and PBS$^2$P (4:8). As shown in Table~\ref{tab:zero_shot_acc}, models compressed with PBS$^2$P significantly outperform those compressed with STBLLM in terms of average accuracy. Such comparisons demonstrate that PBS$^2$P provides a more effective solution for compressing LLMs to less than 1-bit.

\begin{table*}[t]
\vspace{-3mm}
\caption{Ablation study on LLaMA-7B, where all PBS$^2$P is applied an $N$:$M$ sparsity of 4:8. Results are measured by perplexity on the Wikitext2 and C4 datasets. Our results are highlighted in \textbf{bold}.}
\vspace{1mm}
\hspace{22mm}
\subfloat[\small Ablation for SPBO Strategy. \label{tab:Progressive_Strategy}]{\hspace{-2mm}\vspace{-2mm}
\scalebox{0.9}
{\begin{tabular}{c c c}
\toprule
\rowcolor{color3}
\textbf{SPBO Strategy} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
\ding{55} & 14.43 & 16.76 \\
\ding{51} & \textbf{10.78} & \textbf{13.06} \\
\bottomrule
\end{tabular}}
}\hspace{10mm}\vspace{-2mm}
\subfloat[\small Ablation for Group Size \label{tab:group_size}]
{\scalebox{0.9}
{\hspace{-1.5mm}
\begin{tabular}{c c c c c c}
\toprule
\rowcolor{color3}
\textbf{Group Size} & \textbf{64} & \textbf{128} & \textbf{256} &\textbf{512}\\
\midrule
\textbf{Wikitext2}$\downarrow$& 10.24  & \textbf{10.78} & 11.20 & 11.91  \\
\textbf{C4}$\downarrow$ & 12.50  & \textbf{13.06} & 13.68 & 14.51  \\
\bottomrule
\end{tabular}
}
}

\hspace{6.5mm}
\subfloat[\small Ablation for Metric in Coarse-Stage Search (CSS)\label{tab:Coarse-Stage_Search}]{\hspace{-0mm}\vspace{-2mm}
\scalebox{0.85}
{\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{Coarse-Stage Search} & \textbf{Metric} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
\ding{55} & — & 11.55 & 13.94 \\
\ding{51}  & RI & 14.43 & 16.76 \\
\ding{51} & LR & \textbf{10.78} & \textbf{13.06}\\
\bottomrule
\end{tabular}}
}\hspace{3mm}\vspace{-2mm}
\subfloat[\small Ablation for Pruning Type\label{tab:Prune_Type}]
{\scalebox{0.85}
{\hspace{-1.5mm}\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{Prune Type} & \textbf{Hardware-friendly} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
Structured & \ding{51} & 621.16 & 400.50 \\
Unstructured & \ding{55} & 8.54 & 10.95 \\
Semi-Structured & \ding{51} & \textbf{10.78} & \textbf{13.06} \\
\bottomrule
\end{tabular}}
}

\hspace{10mm}
\subfloat[\small Ablation for Metric in Fine-Stage Search\label{tab:Fine-Stage_Metric}]{\hspace{0mm}\vspace{-5mm}
%\resizebox{0.5\textwidth}{\height}
\scalebox{0.9}
{\begin{tabular}{c c c c c c}
\toprule
\rowcolor{color3}
\textbf{Metric} & \textbf{Random} & \textbf{Magnitude} & \textbf{Wanda} & \textbf{SI} & \textbf{Ours}\\
\midrule
\textbf{Wikitext2$\downarrow$} & 7,779.28 & 38.90 & 10.89 & 196.61 & \textbf{10.78} \\
\textbf{C4$\downarrow$} & 6,797.09 & 24.47 & 13.16 & 148.85 & \textbf{13.06} \\
\bottomrule
\end{tabular}}
}\hspace{5mm}\vspace{-0mm}
\subfloat[\small Ablation for Number of Split Points\label{tab:split_points}]
{\scalebox{0.9}
{\hspace{-1.5mm}\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{\#Split Points} &\textbf{1} & \textbf{2} & \textbf{3} \\
\midrule
\textbf{Wikitext2}$\downarrow$ & 13.41 & \textbf{10.78} & 10.55\\
\textbf{C4}$\downarrow$ & 15.97 & \textbf{13.06} & 12.83 \\
\bottomrule
\end{tabular}}
}
\hspace{3mm}

\label{tab:ablations}
\vspace{-8mm}
\end{table*}


\vspace{-0.5mm}
\subsection{Ablation Study}
\vspace{-0.5mm}
\textbf{Ablation for SPBO Strategy.$\quad$}
To validate the effectiveness of our SPBO strategy, we provide the performance of PBS$^2$P with and without its application. As shown in Table~\ref{tab:Progressive_Strategy}, the performance of SPBO surpasses that of the vanilla pruning followed by binarization approach. In the vanilla method, pruning is done in a single step, where all elements are pruned simultaneously, and binarization is applied to the remaining elements afterward. However, the vanilla approach makes it hard to effectively minimize the combined errors. These results demonstrate the performance improvement achieved through our SPBO strategy.

\textbf{Ablation for Group Size.$\quad$}
Table~\ref{tab:group_size} presents the results of our ablation study on the group size configuration. It indicates that a smaller group size, meaning finer-grained grouping, leads to better performance. However, this also comes with increased computational and storage costs. To strike a balance between performance and resource efficiency, we select a group size of 128.

\textbf{Ablation for Metric in Coarse-Stage Search.$\quad$}
Table~\ref{tab:Coarse-Stage_Search} shows the performance of the coarse-stage search under three conditions: without the coarse-stage search, using the relative importance (RI) metric from STBLLM, and applying our layer redundancy (LR) score. The results reveal that using the RI metric causes a performance drop compared to the baseline without coarse-stage search. In contrast, applying our LR score leads to a clear improvement, demonstrating not only the effectiveness of our coarse-stage search but also the superiority of the LR score metric.

\vspace{1mm}
\textbf{Ablation for Pruning Type.$\quad$}
In Table~\ref{tab:Prune_Type}, we present the impact of different pruning types on the results, comparing structured pruning, unstructured pruning, and semi-structured pruning under the same pruning ratio (50\%). For semi-structured pruning, we use an $N$:$M$ sparsity of 4:8, while for structured pruning, we apply column pruning. For unstructured pruning, we perform elementwise pruning based on weight importance. Semi-structured pruning outperforms structured pruning for LLM compression. It also maintains hardware-friendliness while achieving performance levels similar to unstructured pruning. This demonstrates that semi-structured pruning strikes a good trade-off between hardware efficiency and model performance.


\textbf{Ablation for Metric in Fine-Stage Search.$\quad$}
The Table~\ref{tab:Fine-Stage_Metric} presents the performance of different pruning metrics in the fine-stage search. We compare several metrics, including random selection, magnitude-based selection, Wanda, SI, and our Hessian-based metric. The experimental results demonstrate that our metric significantly outperforms both random selection and magnitude-based pruning, achieving superior performance compared to Wanda and SI, thereby highlighting the effectiveness of our approach. 


\textbf{Ablation for Number of Split Points.$\quad$}
We use a grouping strategy to quantize non-salient weights, where a split point  $p$  is used to group the non-salient weights. Table~\ref{tab:split_points} shows the impact of different numbers of split points on performance. From the table, we can see that as the number of split points increases, the model performance improves. However, an increase in the number of split points also leads to higher computational and storage demands. As a result, we choose the number of split points as 2, which strikes a balance between performance and resource requirements. This choice ensures a fair comparison across methods.


\section{Conclusions}
\vspace{1mm}
In this work, we propose a progressive binarization with semi-structured pruning (PBS$^2$P) method for LLM compression. We propose SPBO, a stepwise semi-structured pruning with binarization optimization, which prunes a subset of elements at each step while simultaneously optimizing the binarized parameters. SPBO effectively reduces total errors from pruning and binarization. Additionally, we propose the coarse-to-fine search (CFS) method to improve pruning element selection, further enhancing compression efficiency. Extensive experiments show that PBS$^2$P outperforms SOTA binary PTQ methods, achieving superior accuracy across various LLM families and metrics. Our work offers an effective solution for deploying LLMs on resource-constrained devices while maintaining strong performance and introduces a new perspective to combining compression techniques for extreme LLM compression.
% Acknowledgements should only appear in the accepted version.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
