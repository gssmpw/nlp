%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig} % Retained subfig
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For revision
\providecommand{\yulun}[1]{\textcolor{red}{[{\bf #1}]}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{arydshln} % For dashed lines in tables
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{colortbl}
\definecolor{colorTab}{rgb}{0.9,0.9,0.98}
\definecolor{color3}{gray}{0.95}
\usepackage{enumitem}
\usepackage{multicol}
% \usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}  % For resizing the algorithm


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\definecolor{css}{rgb}{0.7529, 0, 0}
\definecolor{fss}{rgb}{0, 0.7, 0.3}
\definecolor{pbp}{rgb}{0.2, 0.2, 0.6}
\definecolor{reasonable}{HTML}{92D050}
\newcommand{\mycomment}[1]{\hfill\small\texttt{$\triangleright$ #1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% \newcommand{\mycomment}[1]{\hfill\small\texttt{$\triangleright$ #1}}r.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}
\twocolumn[
\icmltitle{Progressive Binarization with Semi-Structured Pruning for LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}



% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
% \tableofcontents


\section{Analysis of Binarization Difficulty}
After the standard binarization process, we define the quantization error \( \mathcal{L}_1 \) after binarization as:
\begin{equation}
    \mathcal{L}_1 = \|\mathbf{W} - \widehat{\mathbf{W}}\|_F^2, \quad \text{where} \, \widehat{\mathbf{W}} = \alpha \mathbf{B} + \mu \,.
\end{equation}
$\mathcal{L}_1$ represents the difference between the binarized matrix and the full-precision matrix. It can be observed that during the binarization process, the row-wise scaling factor $\alpha$ is used to approximate the magnitude of elements in each row, and the quantization error $\mathcal{L}_1$ varies depending on the weight distribution. Intuitively, when using the row-wise scaling factor approach for binarization, weight matrices with more dispersed row-wise distributions tend to face greater binarization difficulty, which in turn results in larger quantization errors.

\textbf{Binarization Difficulty Score.$\quad$}We revisit the binarization process and propose a new metric, the Binarization Difficulty (BD) score, which quantifies the inherent difficulty of binarizing a given weight matrix. This score not only provides a measure of how challenging the binarization task is, but also serves as an indicator of the potential quantization error that may arise as a result of the binarization process. The definition of BD score is as follows:
\begin{equation}
    \textit{BD} = \frac{1}{n} \sum_{i=1}^{n} \text{Var}\left( |\mathbf{W}_{i.} - \mu_i| \right),
\end{equation}
where $\mathbf{W}$ represents the given weight matrix. We take into account the redistribution of the weight matrix caused by standard binarization in the calculation of the BD score, and apply absolute value processing before computing the variance. BD score quantifies the difficulty of binarization, where higher values of BD score indicate more dispersed weight distributions and thus greater binarization challenges. 

To validate the effectiveness of the BD score, we performed binarization experiments on different weight matrices and analyzed the relationship between the BD score and quantization error. The results are shown in Figure~\ref{fig:fig4} left. As observed, there is a positive correlation between the BD score and quantization error. We hope that the BD score can serve as a guiding metric in the process of binarization.
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig4_v2.pdf}
\vspace{-5mm}
\caption{\textbf{Left}: The left side shows the relationship between quantization error and BD score, where they exhibit a positive correlation. \textbf{Right}: The right side shows the BD score of the same weight matrix at different pruning ratios, where the BD score only decreases at an appropriate pruning ratio.}
\label{fig:fig4}
\vspace{-5mm}
\end{figure}



% \begin{equation}\label{eq1}
% %\vspace{-0.1in}
%     \widehat{\mathbf{W}} = \alpha \cdot \mathbf{B} + \mu
% \end{equation}

\section{Impact of Pruning on Binarization Difficulty}
\textbf{Pruning Enhances Binarization.$\quad$}
We then attempt to apply the $N:M$ sparsity, a semi-structured pruning method, to prune the model. After pruning, we calculate the BD score of the remaining weights in the weight matrix. Figure~\ref{fig:fig4} right shows the BD score of a weight matrix at different pruning ratios. We observe that an appropriate pruning ratio can reduce the difficulty of binarization, leading to lower quantization errors in the binarization phase. Of course, pruning introduces additional pruning errors. In our experiments, we recorded the errors from both pruning error and binarization error, as well as their total error. It is evident that when we use an appropriate pruning ratio, the total error is lower than the binarization error without pruning. This is a crucial finding, suggesting that by combining pruning and binarization techniques, we can effectively reduce errors during model compression, thereby preserving both model performance and efficiency.

\section {SPBO Implementation}\label{func}
Here, we provide the function for splitting the pruning mask. The SPBO algorithm is already largely implemented in the main text. For simplicity, a simplified version of splitting mask is used in the main text. In the detailed implementation, while searching for the pruning mask, we simultaneously perform segmentation on the mask, ultimately returning a pruning mask list. The specific function implementation is shown in Algorithm~\ref{split}.

% \begin{algorithm}[!h]
% \caption{Main Framework of BiLLM: Inner details of each function are shown in Algorithm \ref{alg2} }
% \label{alg1}
% \func{$\operatorname{SplitMask}$}$(\mathbf{W}, N, M, \mathbf{scores})$
% \begin{algorithmic}[1]
% \STATE \textbf{Input:} $\mathbf{W}$ (weight matrix), $N$, $M$, $\mathbf{scores}$ (score matrix)
% \STATE \textbf{Output:} Mask list

% \STATE $\text{rows}, \text{cols} \leftarrow \text{shape of } \mathbf{W}$
% \STATE $\text{num\_groups} \leftarrow \frac{\text{cols}}{M}$
% \STATE $\text{mask\_list} \leftarrow []$
% \STATE $\mathbf{scores\_grouped} \leftarrow \text{reshape}(\mathbf{scores}, \text{rows}, \text{num\_groups}, M)$

% \FOR{$i = 1, 2, \ldots, M - N$}
%     \STATE $\text{top\_indices} \leftarrow \text{torch.topk}(\mathbf{scores\_grouped}, M-i-1, \text{dim}=-1).\text{indices}$
%     \STATE $\text{batch\_indices} \leftarrow \text{torch.arange(rows)}.\text{view}(-1, 1, 1).\text{expand\_as}(\text{top\_indices})$
%     \STATE $\text{group\_indices} \leftarrow \text{torch.arange(num\_groups)}.\text{view}(1, -1, 1).\text{expand\_as}(\text{top\_indices})$
%     \STATE $\mathbf{mask\_grouped} \leftarrow \text{torch.zeros\_like}(\mathbf{scores\_grouped}, \text{dtype}=\text{torch.bool})$
%     \STATE $\mathbf{mask\_grouped}[\text{batch\_indices}, \text{group\_indices}, \text{top\_indices}] \leftarrow \text{True}$
%     \STATE $\mathbf{mask} \leftarrow \text{reshape}(\mathbf{mask\_grouped}, \text{rows}, \text{cols})$
%     \STATE $\text{mask\_list}.append(\mathbf{mask})$
% \ENDFOR
% \STATE \textbf{return} $\text{mask\_list}$
% \end{algorithmic}
% \end{algorithm}
\begin{algorithm*}[!h]
    \caption{Detailed algorithm for splitting pruning mask into group}
% \vspace{-4.2mm}
% \begin{multicols}{2}
\label{split}
\func{$\operatorname{SplitMask}$}$(\mathbf{W}, N, M, \mathbf{scores})$
\begin{algorithmic}[1]
\STATE \textbf{Input:} $\mathbf{W}$ (weight matrix), $N$, $M$, $\mathbf{scores}$ (score matrix)
\STATE \textbf{Output:} Mask list

\STATE $\text{rows}, \text{cols} \leftarrow \text{shape of } \mathbf{W}$
\STATE $\text{num\_groups} \leftarrow \frac{\text{cols}}{M}$
\STATE $\text{mask\_list} \leftarrow []$
\STATE $\mathbf{scores\_grouped} \leftarrow \text{reshape}(\mathbf{scores}, \text{rows}, \text{num\_groups}, M)$

\FOR{$i = 1, 2, \ldots, M - N$}
    \STATE $\text{top\_indices} \leftarrow \text{torch.topk}(\mathbf{scores\_grouped}, M-i-1, \text{dim}=-1).\text{indices}$
    \STATE $\text{batch\_indices} \leftarrow \text{torch.arange(rows)}.\text{view}(-1, 1, 1).\text{expand\_as}(\text{top\_indices})$
    \STATE $\text{group\_indices} \leftarrow \text{torch.arange(num\_groups)}.\text{view}(1, -1, 1).\text{expand\_as}(\text{top\_indices})$
    \STATE $\mathbf{mask\_grouped} \leftarrow \text{torch.zeros\_like}(\mathbf{scores\_grouped}, \text{dtype}=\text{torch.bool})$
    \STATE $\mathbf{mask\_grouped}[\text{batch\_indices}, \text{group\_indices}, \text{top\_indices}] \leftarrow \text{True}$
    \STATE $\mathbf{mask} \leftarrow \text{reshape}(\mathbf{mask\_grouped}, \text{rows}, \text{cols})$
    \STATE $\text{mask\_list}.append(\mathbf{mask})$
\ENDFOR
\STATE \textbf{return} $\text{mask\_list}$
\end{algorithmic}

% \end{multicols}
% \vspace{-3mm}
\end{algorithm*}


% \begin{algorithm}[!h]
% % \begin{multicols}{2}
% \caption{Main Framework of BiLLM: Inner details of each function are shown in Algorithm \ref{alg2} }
% \label{alg1}

% % \func{$\operatorname{Mask2Group}$}$(\mathbf{M})$
% % \begin{algorithmic}[1]
% % \FOR{$b = 1, 2, \ldots, B$}
% %     \FOR{$g = 1, 2, \ldots, G$}
% %         \STATE  $\mathbf{M}_\text{group}[b,g] = \mathbf{M}[b, g \cdot M : (g+1) \cdot M]$
% %     \ENDFOR
% % \ENDFOR
% % \STATE \textbf{return} $\mathbf{M}_\text{group}$

% \func {$\operatorname{Mask2Group}$}$(\mathbf{W}, N, M, \mathbf{Scores})$
% \begin{algorithmic}[1]
%     \STATE Let rows, cols = dimensions of W
%     \STATE Let num_groups = cols // M
%     \STATE Initialize empty mask_list

%     \STATE Reshape scores into groups: scores_grouped = reshape(scores, (rows, num_groups, M))

%     \For {i from 0 to (M - N - 1)}
%         \STATE Find the top M-i-1 indices in each group: top_indices = topk(scores_grouped)
%         \STATE Generate batch and group indices
%         \STATE Initialize mask_grouped with zeros (shape: scores_grouped)
%         \STATE Set mask_grouped elements corresponding to top_indices to True
%         \STATE Reshape mask_grouped back to the original shape (rows, cols)
%         \STATE Append the mask to mask_list
%     \ENDFOR
% \STATE \textbf{return} $\mathbf{M}_\text{group}$



% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm*}[!h]
%     \caption{aaa}
% % \vspace{-4.2mm}
% % \begin{multicols}{2}
% \label{mainalg}
% func $\operatorname{SPBO}$($\mathbf{W}$, $\mathbf{M}_\text{p}$, $\mathbf{X}$ ,$T$)\\ 
% {\bf Input:} $\mathbf{W} \in \mathbb{R}^{n\times m}$ - full-precision weight \\
% \hspace*{0.43in}$ \mathbf{M}_\text{p} \in \mathbb{R}^{n\times m}$ - pruning mask \\
% \hspace*{0.43in}$ \mathbf{X} \in \mathbb{R}^{B\times L \times m}$ - calibration data \\
% \hspace*{0.43in}$T$ - binarized parameter optimization steps\\
% {\bf Output:} $ \mathbf{\widehat{W}} \in \mathbb{R}^{n\times m}$
% \begin{algorithmic}[1]
% \STATE $\mathbf{S} \coloneqq  \operatorname{X2S}(\mathbf{X})$ \mycomment{decouple $\mathbf{W}$ and $\mathbf{X}$}
% \STATE $\widehat{\mathbf{W}},\alpha,\mathbf{B},\mu \coloneqq \operatorname{binary}(\mathbf{W})$
% \STATE $\textbf{M}_\text{group} \coloneqq \operatorname{split\_mask}(\mathbf{M}_\text{p})$\mycomment{pruning mask group}
% \STATE $\mathbf{M}_\text{k} \coloneqq \mathbf{M}_\text{group}[0] $
% \FOR{$\textbf{M} \text{ in } \mathbf{M}_\text{group}$}
%     \STATE $\mathbf{B} \leftarrow \mathbf{B}\odot \mathbf{M}$\mycomment{prune binarized matrix}
%     \STATE $\mathbf{W} \leftarrow \mathbf{W}\odot \mathbf{M}$
%     \STATE $\mathbf{M}_k \leftarrow \mathbf{M}_k \cup \mathbf{M}$ \mycomment{current pruning mask}

%     \FOR{$iter = 1, 2, \ldots, T$}
   
%        % \STATE $\mathbf{P}\coloneqq \operatorname{einsum}(\mathbf{R})$
%     % \STATE $\mathcal{L} \coloneqq \sum_k\sum_l (\mathbf{S}\odot  \mathbf{P})_{kl}$
%         \STATE $\mu \leftarrow \operatorname{update\_\mu}(\mathbf{S},\mathbf{W},\mathbf{B},\alpha)$\mycomment{update $\mu$}
%         \STATE $\alpha \leftarrow \operatorname{update\_\alpha}(\mathbf{S},\mathbf{W},\mu,\mathbf{B})$\mycomment{update $\alpha$}
%         \STATE $\widehat{\mathbf{W}} \leftarrow (\alpha\cdot\mathbf{B}+\mu)\odot\mathbf{M}_k$
%     \ENDFOR
% \ENDFOR

% \STATE {\bf return}  $\mathbf{\widehat{W}}$

% \end{algorithmic}

% func $\operatorname{update\_\alpha}$ $(\mathbf{S},\mathbf{W},\mathbf{\mu},\mathbf{B})$
% \begin{algorithmic}[1]
% \STATE $\widetilde{\mathbf{W}}\coloneqq \mathbf{W}-\mu$
% \FOR{$i = 1, 2, \ldots, n$}
%     \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
%         \STATE $\mathbf{U}_{kl}\coloneqq \mathbf{B}_{ik}\widetilde{\mathbf{W}}_{il}$
%         \STATE $\mathbf{V}_{kl}\coloneqq\mathbf{B}_{ik}\mathbf{B}_{il}$
%     \ENDFOR
%     \STATE $num \coloneqq \sum_k\sum_l(\mathbf{S}\odot \mathbf{U})_{kl}$
%     \STATE $den \coloneqq \sum_k\sum_l(\mathbf{S}\odot\mathbf{V})_{kl}+\epsilon$
%     \STATE $\alpha_i \coloneqq \frac{num}{den}$\mycomment{refined $\alpha$}
% \ENDFOR
% \STATE {\bf return}  $\alpha$
% \end{algorithmic}

% func $\operatorname{update\_\mu}$ $(\mathbf{S},\mathbf{W},\mathbf{B},\alpha)$
% \begin{algorithmic}[1]
% \FOR{$i = 1, 2, \ldots, n$}
%     \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
%         \STATE $\mathbf{P}_{kl}\coloneqq\mathbf{W}_{ik}-\alpha_i\mathbf{B}_{il}$
%     \ENDFOR
%     \STATE $num \coloneqq \sum_k\sum_l(\mathbf{S}\odot\mathbf{P})_{kl}$
%     \STATE $den \coloneqq \sum_k\sum_l\mathbf{S}_{kl}+\epsilon$
%     \STATE $\mu_i \coloneqq \frac{num}{den}$\mycomment{refined $\mu$}
% \ENDFOR
% \STATE {\bf return}  $\mu$
% \end{algorithmic}
% % \vfill  % 填充垂直空间
% % \columnbreak  % 强制换列

% func $\operatorname{binary}$ $(\mathbf{W})$
% \begin{algorithmic}[1]
% \STATE $\mu \coloneqq \frac{1}{m}\sum_{j=1}^m\mathbf{W}_{.j}$
% \STATE $\widetilde{\mathbf{W}}\coloneqq \mathbf{W}-\mu$\mycomment{row-wise redistribution}
% % \STATE $\alpha \coloneqq \dfrac{||\widetilde{\mathbf{W}}||_{\ell1}}{n\times m}$
% \STATE $\alpha \coloneqq \frac{1}{m}\sum_{j=1}^m|\widetilde{\mathbf{W}}_{.j}|$\mycomment{row-wise scaling factor}
% \STATE $\mathbf{B} \coloneqq  \operatorname{sign}(\widetilde{\mathbf{W}})$
% \STATE $\mathbf{\widehat{W}} \coloneqq \alpha\cdot \mathbf{B} + \mu $\mycomment{binarized output}
% \STATE {\bf return}  $\mathbf{\widehat{W}},\alpha,\mathbf{B},\mu$
% \end{algorithmic}


% func $\operatorname{X2S}$ $(\mathbf{X})$
% \begin{algorithmic}[1]
% \FOR{$b = 1, 2, \ldots B$}
%     \FOR{$k = 1, 2, \ldots, m;l = 1, 2, \ldots, m$}
%         \STATE  $\mathbf{S}_{kl} = \sum_b\sum_i(\mathbf{X}_b)_{ik}(\mathbf{X}_b)_{il}$
%     \ENDFOR
% \ENDFOR
% \STATE {\bf return}  $\mathbf{S}$
% \end{algorithmic}






% \end{multicols}
% \vspace{-3mm}
% \end{algorithm*}

\section{Derivation of the Optimization Formulas in SPBO}
Since simultaneously optimizing the pruning mask and binarization parameters is an NP-hard problem, we use a greedy algorithm to solve this issue. First, we derive the optimization formula for the binarization parameters without the pruning mask. The current definition of quantization error is:
\begin{equation}
    \mathcal{L} = ||\mathbf{W}\mathbf{X}-\widehat{\mathbf{W}}\mathbf{X}||^2_{F}.
\end{equation}

\textbf{Rewritten quantization error to decouple $\mathbf{W}$ and $\mathbf{X}$}\quad
We first rewrite the quantization error $\mathcal{L}$ to decouple $\mathbf{W}$ and $\mathbf{X}$, reducing the computational cost when calculating the quantization error. We define $\widetilde{\mathbf{W}}$ as $\widetilde{\mathbf{W}} = \mathbf{W}-\mu$. Then we rewrite the quantization error as:
\begin{align} \label{eq2}
    \mathcal{L} 
    &= ||\mathbf{W}\mathbf{X}-\widehat{\mathbf{W}}\mathbf{X}||_F^2\\
    &= ||\mathbf{X}(\widetilde{\mathbf{W}}-\alpha\mathbf{B})^\top||_F^2\\
    &= \sum_i\sum_j(\sum_b\sum_k(\mathbf{X}_b)_{ik}(\widetilde{\mathbf{W}}_{jk}-\alpha_j\mathbf{B}_{jk}))^2.
\end{align}

The residual matrix is defined as $\mathbf{R}=\mathbf{W}-\mu-\alpha\mathbf{B}$ and further simplify $\mathcal{L}$:
\begin{align} \label{eq2}
    \mathcal{L} 
    &= \sum_i\sum_j(\sum_b\sum_k(\mathbf{X}_b)_{ik}\mathbf{R}_{jk})^2\\
    &= \sum_i\sum_j(\sum_b\sum_k\sum_l(\mathbf{X}_b)_{ik}(\mathbf{X}_b)_{il}\mathbf{R}_{jk}\mathbf{R}_{jl})\\
    &= \sum_k\sum_l(\sum_b\sum_i(\mathbf{X}_b)_{ik}(\mathbf{X}_b)_{il})(\sum_j\mathbf{R}_{jk}\mathbf{R}_{jl}).
\end{align}

After that, we define the matrix $\mathbf{S}$ using the following formula:
\begin{equation}\label{eq10}
    \mathbf{S}_{kl} = \sum_b\sum_i(\mathbf{X}_b)_{ik}(\mathbf{X}_b)_{il},
\end{equation}
where $k=1,2,\dots,m$, $l=1,2,\dots,m$. Then we obtain the final simplified $\mathcal{L}$ as
\begin{equation}
    \mathcal{L} = \langle \mathbf{S}, \mathbf{R^\top R} \rangle_F = \text{Tr}(\mathbf{R}\mathbf{S}\mathbf{R}^\top).
\end{equation}

\textbf{Parameter Optimization Formula}\quad
We use the quantization error $\mathcal{L}$ to update $\mu$:
\begin{align} \label{eq2}
    \mathcal{L} 
    &=\sum_k\sum_l\mathbf{S}_{kl}\sum_j\mathbf{R}_{jk}\mathbf{R}_{jl}\\
    &= \sum_k\sum_l\mathbf{S}_{kl}\sum_j(\widetilde{\mathbf{W}}_{jk}\widetilde{\mathbf{W}}_{jl}\\
    &-\alpha_j(\mathbf{B}_{jk}\widetilde{\mathbf{W}}_{jl}+\mathbf{B}_{jl}\widetilde{\mathbf{W}}_{jk})+\alpha_j^2\mathbf{B}_{jk}\mathbf{B}_{jl})\\
    &=\sum_k\sum_l\mathbf{S}_{kl}\sum_j((\mathbf{W}_{jk}-\mu_j)(\mathbf{W}_{jl}-\mu_j)\\
    &-\alpha_j(\mathbf{B}_{jk}(\mathbf{W}_{jl}-\mu_j)\\
    &+\mathbf{B}_{jl}(\mathbf{W}_{jk}-\mu_j))+\alpha_j^2\mathbf{B}_{jk}\mathbf{B}_{jl}).
\end{align}


To obtain the optimal solution for $\mu$, we take the partial derivative of $\mathcal{L}$ with respect to $\mu_j$, where $j=1,2,\dots,n$:
\begin{equation} \label{eq2}
    \frac{\partial \mathcal{L}}{\partial \mu_j} = \sum_k\sum_l\mathbf{S}_{kl}(-\mathbf{W}_{jl}-\mathbf{W}_{jk}+2\mu_j+\alpha_j\mathbf{B}_{jk}+\alpha_j\mathbf{B}_{jl}).
\end{equation}

We set $\frac{\partial \mathcal{L}}{\partial \mu_j} = 0$ to get the optimal solution for $\mu_j$:
\begin{align} \label{eq2}
    \mu_j 
    &= \frac{\sum_k\sum_l\mathbf{S}_{kl}(\mathbf{W}_{jk}-\alpha_j\mathbf{B}_{jk}+\mathbf{W}_{jl}-\alpha_j\mathbf{B}_{jl})}{2\sum_k\sum_l\mathbf{S}_{kl}},\\
    &\text{where $j=1,2,\dots,n$}.
\end{align}

Then, we define the matrix $\mathbf{P}$ as:
\begin{align} \label{eq2}
   &\mathbf{P}_{kl} = \mathbf{W}_{jk}-\alpha_j\mathbf{B}_{jl},\\
   &\text{where $k=1,2,\dots,m$, $l=1,2,\dots,m$}.
\end{align}

After that, we can simplify $\mu_j$ as
\begin{align} \label{eq2}
    &\mu_j = \frac{\sum_k\sum_l(\mathbf{S}\odot(\mathbf{P}+\mathbf{P}^\top))_{kl}}{2\sum_k\sum_l\mathbf{S}_{kl}},\\
    &\text{where $j=1,2,\dots,n$}.
\end{align}

Since $\mathbf{S}$ is symmetric, we can further simplify the above equation as:
\begin{align} \label{eq2}
    \mu_j 
    &= \frac{\sum_k\sum_l(\mathbf{S}\odot\mathbf{P})_{kl}}{\sum_k\sum_l\mathbf{S}_{kl}},\quad \text{where $j=1,2,\dots,n$}.
\end{align}

We can also express $\mu$ in a more compact vector form:
\begin{align}
\mu = \frac{\mathbf{1}^\top \mathbf{S} (\mathbf{W} - \alpha \mathbf{B})^\top}{\mathbf{1}^\top \mathbf{S} \mathbf{1}}.
\end{align}

Similarly, we use the same quantization error to update $\alpha$:
\begin{align} \label{eq2}
    \mathcal{L} 
    &= \sum_k\sum_l\mathbf{S}_{kl}\sum_j\mathbf{R}_{jk}\mathbf{R}_{jl}\\
    &= \sum_k\sum_l\mathbf{S}_{kl}\sum_j(\widetilde{\mathbf{W}}_{jk}\widetilde{\mathbf{W}}_{jl}\\
    &-\alpha_j(\mathbf{B}_{jk}\widetilde{\mathbf{W}}_{jl}+\mathbf{B}_{jl}\widetilde{\mathbf{W}}_{jk})+\alpha_j^2\mathbf{B}_{jk}\mathbf{B}_{jl}).
\end{align}

To obtain the optimal solution for $\alpha$, we take the partial derivative of $\mathcal{L}$ with respect to $\alpha_j$, where $j=1,2,\dots,n$:
\begin{equation} \label{eq2}
    \frac{\partial \mathcal{L}}{\partial \alpha_j} = \sum_k\sum_l\mathbf{S}_{kl}(2\mathbf{B}_{jk}\mathbf{B}_{jl}\alpha_j-(\mathbf{B}_{jk}\widetilde{\mathbf{W}}_{jl}+\mathbf{B}_{jl}\widetilde{\mathbf{W}}_{jk})).
\end{equation}

We set $\frac{\partial \mathcal{L}}{\partial \alpha_j} = 0$ to get the optimal solution for $\alpha_j$:
\begin{align} \label{eq2}
   & \alpha_j 
    = \frac{\sum_k\sum_l\mathbf{S}_{kl}(\mathbf{B}_{jk}\widetilde{\mathbf{W}}_{jl}+\mathbf{B}_{jl}\widetilde{\mathbf{W}}_{jk})}{2\sum_k\sum_l\mathbf{S}_{kl}(\mathbf{B}_{jk}\mathbf{B}_{jl})},\\
    &\text{where $j=1,2,\dots,n$}.
\end{align}

Then, we define the matrix $\mathbf{U}$ and $\mathbf{V}$ as:
\begin{equation} \label{eq2}
   \mathbf{U}_{kl} = \mathbf{B}_{jk}\widetilde{\mathbf{W}}_{jl},\,\mathbf{V}_{kl}=\mathbf{B}_{jk}\mathbf{B}_{jl},
\end{equation}
where $k=1,2,\dots,m$, $l=1,2,\dots,m$. After that, we simplify $\alpha_j$ using $\mathbf{U}$ and $\mathbf{V}$:
\begin{equation} \label{eq2}
   \alpha_j= \frac{\sum_k\sum_l(\mathbf{S}\odot(\mathbf{U}+\mathbf{U}^\top))_{kl}}   {2\sum_k\sum_l(\mathbf{S} \odot \mathbf{V})_{kl}}.
\end{equation}

Since $\mathbf{S}$ is symmetric, we can further simplify the above equation as
\begin{equation} \label{eq2}
   \alpha_j= \frac{\sum_k\sum_l(\mathbf{S}\odot\mathbf{U})_{kl}}   {\sum_k\sum_l(\mathbf{S} \odot \mathbf{V})_{kl}}.
\end{equation}

We can also express $\alpha$ in a more compact vector form:
\begin{equation}
    \alpha= \frac{\operatorname{diag}(\mathbf{B}\mathbf{S}(\mathbf{W}-\mu)^\top)}{\operatorname{diag}(\mathbf{B}\mathbf{S}\mathbf{B}^\top)}.
\end{equation}
This is the parameter optimizing formula in the absence of a pruning mask. In the presence of a pruning mask, we directly prune the $\mathbf{W}$ and $\mathbf{B}$ matrices and then apply the optimizing formula. This is a simplified optimization approach, where only the retained weights undergo binary approximation.




\section{More Experimental Results}
\subsection{Perplexity results} 
We present more experimental results related to perplexity. In Table~\ref{tab:ptb_series}, we show the PTB test results of our method PBS$^2$P compared to other baseline methods. It is evident that our method outperforms the existing SOTA binarization methods. Table~\ref{tab:c4_series} displays the results on the C4 dataset, where our method continues to perform the best, and we also achieve a lower average bit count.
\subsection{Zero-shot results}
We also provide the comparison of 7 zero-shot QA datasets on the OPT family and LLaMA family, as shown in~\Cref{tab:zero_shot_acc_opt} and in ~\Cref{tab:zero_shot_acc_llama}.
\subsection{Ablation Study}
We also conducted our ablation experiments on the LLaMA-2-7B model, and observed the same trends and phenomena as in the ablation experiments on LLaMA-1-7B in the main text. This further strengthens the conclusions drawn from our ablation study and validates the effectiveness of our method. Table~\ref{tab:Progressive_Strategy} to Table~\ref{tab:split_points} present the results of our ablation experiments on LLaMA-2-7B.
% \subsection{Experimental Results for All Pruning Ratios}
\section{Dialog Examples} \label{ap_dialog}
In Figure~\ref{fig:dialog}, we present several dialogue results, including methods such as BiLLM, ARB-LLM, and PBSP. These methods are applied to the LLaMA-13B and LLaMA-2-13B models, and the corresponding dialogue results are shown.

\newpage
\begin{table*}[t]
\vspace{-2mm}
\renewcommand{\arraystretch}{1.0}
\footnotesize
\centering
% \setlength{\tabcolsep}{1.90mm}
\caption{Perplexity comparison of RTN, GPTQ, BiLLM, ARB-LLM, and PBS$^2$P on the LLaMA and OPT families. The evaluation results demonstrate the perplexity performance on the PTB dataset across various model sizes. }
\label{tab:ptb_series}
\vspace{2mm}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccrrrrrrcrrr}
    \toprule
    \rowcolor{color3}
    \multicolumn{3}{c}{\textbf{Settings}} & \multicolumn{4}{c}{\textbf{ LLaMA-1}}  & \multicolumn{2}{c}{\textbf{ LLaMA-2}} & \multicolumn{1}{c}{\textbf{ LLaMA-3}}  &
    \multicolumn{3}{c}{\textbf{ OPT}}\\
    \midrule
   Method & \#Block & \multicolumn{1}{p{4.19em}}{W-Bits} & 7B    & 13B   & 30B   & 65B   & 7B    & 13B   & 8B & 1.3B & 2.7B & 30B\\
    \midrule
    FP16 & -     & 16    & 41.15  & 28.09   &  23.51   & 25.06  & 37.91 &50.93    & 11.18  & 20.29&17.97  &14.03 \\
    \cdashline{1-13}
    \addlinespace[0.2em]

    RTN    &   -  & 3  & 3.2e2 & 64.52 & 80.45 & 81.56 & 1.6e3 &2.2e2 &1.8e3 & 8.9e3 & 9.0e3 & 1.0e3\\
    GPTQ     & 128  & 3 & 84.88 & 26.40 &20.21 & 19.54
    & 4.8e3  & 40.33 & 18.83 &17.54 & 15.15 &11.28 \\ 
    RTN    &   -  & 2  &1.2e5 &8.4e4 & 3.2e4& 2.1e4&2.4e4& 5.1e4 & 6.3e5& 8.0e3& 5.9e3 & 1.0e5\\
    GPTQ     & 128  & 2 &1.4e3 & 2.2e2 &69.46 &47.70 & 5.5e3   &4.1e2 & 717.23  &1.1e2 & 58.38&14.18\\ 
    \cdashline{1-13}

    \addlinespace[0.2em]
    RTN    &   -  & 1  & 1.5e5 & 1.9e6 & 1.4e4& 6.8e4 & 9.9e4 &3.8e4  &7.6e5 & 1.1e4 & 2.8e4 & 5.4e3\\
    GPTQ     & 128  & 1 &1.2e5 &1.0e5 &1.0e4&2.0e4& 6.6e4 &2.7e4  & 9.7e5 & 6.5e3 & 8.4e3 & 7.1e3\\ 
    BiLLM & 128   & 1.11  & 3.7e2 & 84.87  & 43.10  & 44.68   & 5.2e3  &3.0e2 & 87.25  &1.1e2  & 88.52  &21.41  \\
    ARB-LLM & 128   & 1.11  &1.9e2   &54.38   & 34.65  & 32.20   & 389.59  & 1.9e2  &45.49  & 43.34 &31.77 & 16.88 \\
    \cdashline{1-13}
    \addlinespace[0.2em]
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.80  & \textbf{80.27} & \textbf{34.01} & \textbf{26.68} & \textbf{27.54} & \textbf{67.74} & \textbf{68.90} & \textbf{16.15} & \textbf{39.40} & \textbf{29.40} & \textbf{13.04}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.70  & \textbf{87.69} & \textbf{38.11} & \textbf{28.38} & \textbf{29.12} & \textbf{75.22} & \textbf{79.59} & \textbf{18.65} & \textbf{46.98} & \textbf{31.71} & \textbf{17.14}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.55   & \textbf{134.63} & \textbf{54.06} & \textbf{32.47} & \textbf{33.36} & \textbf{119.30} & \textbf{114.40} & \textbf{25.86} & \textbf{76.55} & \textbf{42.85} & \textbf{18.73}\\
    \bottomrule
    \end{tabular}
}
\vspace{-5mm}
\end{table*}

\newpage
\begin{table*}[!t]
\vspace{-2mm}
\renewcommand{\arraystretch}{1.0}
\footnotesize
\centering
% \setlength{\tabcolsep}{1.90mm}
\caption{Perplexity comparison of RTN, GPTQ, BiLLM, ARB-LLM, and PBS$^2$P on the LLaMA and OPT families. The evaluation results demonstrate the perplexity performance on the C4 dataset across various model sizes. }
\label{tab:c4_series}
\vspace{2mm}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccrrrrrrcrrr}
    \toprule
    \rowcolor{color3}
    \multicolumn{3}{c}{\textbf{Settings}} & \multicolumn{4}{c}{\textbf{ LLaMA-1}}  & \multicolumn{2}{c}{\textbf{ LLaMA-2}} & \multicolumn{1}{c}{\textbf{ LLaMA-3}}  &
    \multicolumn{3}{c}{\textbf{ OPT}}\\
    \midrule
   Method & \#Block & \multicolumn{1}{p{4.19em}}{W-Bits} & 7B    & 13B   & 30B   & 65B   & 7B    & 13B   & 8B & 1.3B & 2.7B & 30B\\
    \midrule
    FP16 & -     & 16    & 7.34   & 6.80   &  6.13   & 5.81   & 7.26  &6.73    & 9.45   & 16.07 &14.34  &11.45 \\
    \cdashline{1-13}
    \addlinespace[0.2em]

    RTN    &   -  & 3  & 28.24 & 13.24 & 28.58 & 12.76 & 3.8e2 & 12.50 & 5.7e2& 5.0e3& 1.1e4 & 1.0e3\\
    GPTQ     & 128  & 3 & N/A & 7.15  &6.51 & 6.03
    & 7.94  & 7.05 & 17.68 & 16.11 & 14.16 & 10.91\\ 
    RTN    &   -  & 2  &1.1e5 & 5.8e4 &2.7e4& 2.2e4 & 3.0e4 & 5.1e4& 7.7e5& 7.4e3& 7.3e3 & 6.1e4\\
    GPTQ     & 128  & 2 &79.06 &18.97&14.86 &10.23&  35.26  &19.65  & 3.9e2 & 63.05 & 35.80&12.92 \\ 
    \cdashline{1-13}

    \addlinespace[0.2em]
    RTN    &   -  & 1  & 1.9e5 & 9.94 & 1.3e4 & 1.3e5 &1.1e5&4.6e4  &1.4e6 &1.0e4& 2.3e4 &5.0e3 \\
    GPTQ     & 128  & 1 &1.8e5&1.0e5 &9.5e3 &2.3e4 & 6.7e4  & 1.9e4 & 1.1e6 &6.3e3 & 6.7e3 &7.9e3 \\ 
    BiLLM & 128   & 1.11  &46.96  &16.83   & 12.11  &  11.09  &39.38 & 25.87  &61.04   &64.14   & 44.77  &16.17  \\
    ARB-LLM & 128   & 1.11  & 17.92  & 12.48  & 10.09  &  8.91  & 20.12  & 14.29  &35.70  &28.19  & 21.46  &13.34  \\
    \cdashline{1-13}
    \addlinespace[0.2em]
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.80  & \textbf{9.52} & \textbf{8.11} & \textbf{7.16} & \textbf{6.63} & \textbf{9.43} & \textbf{8.44} & \textbf{15.62} & \textbf{25.62} & \textbf{20.78} & \textbf{12.69}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.70  & \textbf{10.35} & \textbf{8.96} & \textbf{7.83} & \textbf{7.04} & \textbf{10.43} & \textbf{9.20} & \textbf{17.48} & \textbf{30.12} & \textbf{22.73} & \textbf{13.04}\\
    \rowcolor{colorTab}
    PBS$^2$P & 128   & 0.55   & \textbf{13.06} & \textbf{11.44} & \textbf{9.33} & \textbf{8.52} & \textbf{13.12} & \textbf{11.19} & \textbf{22.90} & \textbf{46.07} & \textbf{30.33} & \textbf{14.21}\\
    \bottomrule
    \end{tabular}
}
\end{table*}







\newpage
\begin{table*}[h]
  \centering
    \caption{
  Accuracy(\%) of 7 QA datasets on \textbf{OPT} family. We compare the results among GPTQ, PB-LLM, BiLLM, ARB-LLM, and PBSP$^2$P to validate the quantization effect.}
\vspace{1mm}
\resizebox{1.0\textwidth}{!}{
  \setlength{\tabcolsep}{5.5pt}
    \begin{tabular}{llccccccccc}
    \toprule
    \rowcolor{color3}
    \textbf{Models} & \textbf{Method} &\textbf{W-Bits}& \textbf{PIQA} $\uparrow$& \textbf{BoolQ} $\uparrow$& \textbf{OBQA} $\uparrow$& \textbf{Winogrande} $\uparrow$& \textbf{ARC-e} $\uparrow$& \textbf{ARC-c} $\uparrow$ & \textbf{Hellaswag} $\uparrow$ & \textbf{Average} $\uparrow$ \\
    \midrule
    \multirow
          & FP16 &16 &71.71 &57.74 &23.20 &59.75 &57.11 &23.38 &41.49 &47.77 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & GPTQ & 2.00  & 59.47 & 42.66 & 15.80 & 50.04 & 37.21 & 21.42 & 30.92 & 36.79\\
          & PB-LLM & 1.70  & 54.57 & 61.77 & 13.00 & 50.99 & 28.79 & 20.56 & 26.55 & 36.60\\
          \textbf{OPT-1.3B}& BiLLM & 1.11  & 59.52 & 61.74 & 14.80 & 52.17 & 36.53 & 17.83 & 29.64 & 38.89  \\
          & ARB-LLM& 1.11  & 65.45& 60.31 & 15.40 & 53.04 & 48.27 & 19.37 & 33.44 & 42.18 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80& \cellcolor{colorTab}\textbf{65.72} & \cellcolor{colorTab}\textbf{61.93} & \cellcolor{colorTab}\textbf{16.00} & \cellcolor{colorTab}\textbf{56.04} & \cellcolor{colorTab}\textbf{47.35} & \cellcolor{colorTab}\textbf{22.10} & \cellcolor{colorTab}\textbf{34.51} & \cellcolor{colorTab}\textbf{43.38}  \\
          & \cellcolor{colorTab}PBS$^2$P&\cellcolor{colorTab}0.55 &\cellcolor{colorTab}\textbf{62.62} & \cellcolor{colorTab}\textbf{62.02} & \cellcolor{colorTab}\textbf{13.80} & \cellcolor{colorTab}\textbf{53.04} & \cellcolor{colorTab}\textbf{39.60} & \cellcolor{colorTab}\textbf{19.11} & \cellcolor{colorTab}\textbf{31.13} & \cellcolor{colorTab}\textbf{40.19}  \\
    \midrule
    \multirow 
          & FP16 &16&73.78 &60.28 &25.00 &61.01 &60.77 &26.88 &45.86 & 50.51   \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & GPTQ & 2.00  & 61.81 & 54.43 & 15.40 & 52.33 & 40.15 & 20.56 & 32.55 & 39.60\\
          & PB-LLM & 1.70  & 56.42 & 62.23 & 12.80 & 50.12 & 31.61 & 18.60 & 27.61 & 37.06\\
          \textbf{OPT-2.7B}& BiLLM & 1.11  & 62.57 & 62.20 & 15,40 & 52.57 & 39.65 & 19.80 & 30.88 & 40.44  \\
          & ARB-LLM & 1.11  & 68.50 & 61.99 & 21.60 & 58.33 & 52.82 & 22.27 & 37.50 & 46.14 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{69.04} & \cellcolor{colorTab}\textbf{62.69} & \cellcolor{colorTab}\textbf{19.40} & \cellcolor{colorTab}\textbf{56.75} & \cellcolor{colorTab}\textbf{54.38} & \cellcolor{colorTab}\textbf{22.44} & \cellcolor{colorTab}\textbf{37.74} & \cellcolor{colorTab}\textbf{46.06} \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{66.65} & \cellcolor{colorTab}\textbf{62.45} & \cellcolor{colorTab}\textbf{14.40} & \cellcolor{colorTab}\textbf{54.85} & \cellcolor{colorTab}\textbf{48.02} & \cellcolor{colorTab}\textbf{20.39} & \cellcolor{colorTab}\textbf{33.90} & \cellcolor{colorTab}\textbf{42.95} \\
    \midrule
    \multirow
          & FP16 &16 & 76.28  & 65.99  & 27.60  & 65.39  & 65.66  & 30.63  & 50.51  & 54.57  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & GPTQ & 2.00  & 69.37 & 55.05 & 21.20 & 55.80 & 56.06 & 23.38 & 41.29 & 46.02\\
          & PB-LLM & 1.70  & 56.47 & 55.57 & 13.20 & 50.28 & 29.97 & 18.69 & 27.50 & 35.95\\
          \textbf{OPT-6.7B}& BiLLM &1.11 & 58.60 & 62.14 & 13.20 & 53.12 & 33.75 & 18.26 & 28.83 & 38.27 \\
          & ARB-LLM & 1.11 & 72.47 &62.87 & 22.20 & 60.62 & 59.09 & 26.79 & 42.08 & 49.45 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{73.39} & \cellcolor{colorTab}\textbf{62.48} & \cellcolor{colorTab}\textbf{24.40} & \cellcolor{colorTab}\textbf{61.88} & \cellcolor{colorTab}\textbf{62.04} & \cellcolor{colorTab}\textbf{26.11} & \cellcolor{colorTab}\textbf{44.07} & \cellcolor{colorTab}\textbf{50.62}  \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{70.02} & \cellcolor{colorTab}\textbf{62.14} & \cellcolor{colorTab}\textbf{20.40} & \cellcolor{colorTab}\textbf{58.64} & \cellcolor{colorTab}\textbf{55.81} & \cellcolor{colorTab}\textbf{23.98} & \cellcolor{colorTab}\textbf{39.53} & \cellcolor{colorTab}\textbf{47.22} \\
    \midrule
    \multirow
          & FP16 &16 & 77.64 &70.43 &30.20  &68.19 &70.12 &34.56 & 54.30  & 57.92  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & GPTQ & 2.00  & 73.88 & 63.94 & 24.20 & 62.19 & 60.77 & 28.24 & 47.88 & 51.59\\
          & PB-LLM & 1.70  & 66.76 & 62.29 & 17.40 & 51.07 & 49.33 & 22.53 & 36.53 & 43.70\\
          \textbf{OPT-30B}& BiLLM &1.11 & 72.74 & 62.35 & 21.00 & 60.14 & 60.69 & 27.56 & 42.81 & 49.61 \\
          & ARB-LLM & 1.11  & 75.08 & 65.78 & 26.40 & 65.43 & 64.81 & 29.69 & 48.59 & 53.68 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{76.44} & \cellcolor{colorTab}\textbf{64.07} & \cellcolor{colorTab}\textbf{26.40} & \cellcolor{colorTab}\textbf{66.30} & \cellcolor{colorTab}\textbf{67.38} & \cellcolor{colorTab}\textbf{32.34} & \cellcolor{colorTab}\textbf{49.68} & \cellcolor{colorTab}\textbf{54.66}  \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{75.95} & \cellcolor{colorTab}\textbf{62.57} & \cellcolor{colorTab}\textbf{23.00} & \cellcolor{colorTab}\textbf{64.33} & \cellcolor{colorTab}\textbf{64.90} & \cellcolor{colorTab}\textbf{29.69} & \cellcolor{colorTab}\textbf{47.14} & \cellcolor{colorTab}\textbf{52.51} \\

    \bottomrule
    \end{tabular}%
}
\vspace{-3mm}
\label{tab:zero_shot_acc_opt}
\end{table*}

\begin{table*}[ht]
  \centering
    \caption{
  Accuracies (\%) for 7 zero-shot tasks from semi-structured binarized LLaMA families with PSB$^2$P.}
\vspace{1mm}
\resizebox{1.0\textwidth}{!}{
  \setlength{\tabcolsep}{5.5pt}
    \begin{tabular}{llccccccccc}
    \toprule
    \rowcolor{color3}
    \textbf{Models} & \textbf{Method} &\textbf{W-Bits}& \textbf{PIQA} $\uparrow$& \textbf{BoolQ} $\uparrow$& \textbf{OBQA} $\uparrow$& \textbf{Winogrande} $\uparrow$& \textbf{ARC-e} $\uparrow$& \textbf{ARC-c} $\uparrow$ & \textbf{Hellaswag} $\uparrow$ & \textbf{Average} $\uparrow$\\
    \midrule
    \multirow
          & FP16 &16 & 78.67  & 75.05  & 34.20  & 70.01  & 75.34  & 41.89  & 56.93  & 61.70  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & BiLLM&1.11 & 61.53  & 60.12  & 13.60  & 56.83  & 38.47  & 21.42  & 31.68  & 40.50  \\
          \textbf{LLaMA-1-7B}& ARB-LLM &1.11 & 68.23  &69.17  & 21.60  & 62.43  & 53.66  & 25.68  & 38.96  & 48.50  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80& \cellcolor{colorTab}\textbf{75.95} & \cellcolor{colorTab}\textbf{67.49} & \cellcolor{colorTab}\textbf{27.60} & \cellcolor{colorTab}\textbf{68.11} & \cellcolor{colorTab}\textbf{68.01} & \cellcolor{colorTab}\textbf{34.90} & \cellcolor{colorTab}\textbf{50.51} & \cellcolor{colorTab}\textbf{56.08}  \\
          & \cellcolor{colorTab}PBS$^2$P&\cellcolor{colorTab}0.55 &\cellcolor{colorTab}\textbf{70.95} & \cellcolor{colorTab}\textbf{65.54} & \cellcolor{colorTab}\textbf{23.00} & \cellcolor{colorTab}\textbf{64.96} & \cellcolor{colorTab}\textbf{60.69} & \cellcolor{colorTab}\textbf{28.75} & \cellcolor{colorTab}\textbf{43.78} & \cellcolor{colorTab}\textbf{51.10}  \\
    \midrule
    \multirow
          & FP16&16 & 78.07 &77.68  &31.40  &68.98  &76.30  &43.34  &57.16 &61.80  \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & BiLLM &1.11 & 59.90 & 54.37 & 16.20 & 53.04 & 41.92 & 20.73 & 30.37 & 39.50 \\
          \textbf{LLaMA-2-7B}& ARB-LLM &1.11 & 66.16 & 66.82 & 20.80 & 59.27 & 51.56 & 24.32 & 37.61 & 46.60 \\
          \cdashline{2-11}
    \addlinespace[0.2em]
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.80 & \cellcolor{colorTab}\textbf{75.57} & \cellcolor{colorTab}\textbf{70.28} & \cellcolor{colorTab}\textbf{29.20} & \cellcolor{colorTab}\textbf{67.09} & \cellcolor{colorTab}\textbf{70.88} & \cellcolor{colorTab}\textbf{37.37} & \cellcolor{colorTab}\textbf{50.67} & \cellcolor{colorTab}\textbf{57.29}  \\
          & \cellcolor{colorTab}PBS$^2$P &\cellcolor{colorTab}0.55 & \cellcolor{colorTab}\textbf{70.78} & \cellcolor{colorTab}\textbf{63.73} & \cellcolor{colorTab}\textbf{23.80} & \cellcolor{colorTab}\textbf{65.59} & \cellcolor{colorTab}\textbf{62.37} & \cellcolor{colorTab}\textbf{31.48} & \cellcolor{colorTab}\textbf{43.90} & \cellcolor{colorTab}\textbf{51.66} \\
    \bottomrule
    \end{tabular}%
}
\vspace{-3mm}
\label{tab:zero_shot_acc_llama}
\end{table*}



\begin{table*}[t]
% \label{tab:ablation}
\vspace{-3mm}
\caption{Ablation study on LLaMA-2-7B, where all PBS$^2$P is applied an $N$:$M$ sparsity of 4:8. Results are measured by perplexity on the Wikitext2 and C4 datasets. Our results are highlighted in \textbf{bold}.}
\label{tab:ablations} 
\vspace{1mm}
\hspace{22mm}
% 子表格1
\subfloat[\small Ablation for SPBO Strategy. \label{tab:Progressive_Strategy}]{\hspace{-2mm}\vspace{-2mm}
\scalebox{0.9}
{\begin{tabular}{c c c}
\toprule
\rowcolor{color3}
\textbf{SPBO Strategy} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
\ding{55} & 13.14 & 15.60 \\
\ding{51} & \textbf{10.64} & \textbf{13.12} \\
\bottomrule
\end{tabular}}
}\hspace{10mm}\vspace{-2mm}
% 子表格2
\subfloat[\small Ablation for Group Size \label{tab:group_size}]
{\scalebox{0.9}
{\hspace{-1.5mm}
\begin{tabular}{c c c c c c}
\toprule
\rowcolor{color3}
\textbf{Group Size} & \textbf{64} & \textbf{128} & \textbf{256} &\textbf{512}\\
\midrule
\textbf{Wikitext2}$\downarrow$& 10.17  & \textbf{10.64} & 11.01 & 11.77  \\
\textbf{C4}$\downarrow$ & 12.29  & \textbf{13.12} & 14.11 & 14。76  \\
\bottomrule
\end{tabular}
}
}

\hspace{6.5mm}
% 子表格3
\subfloat[\small Ablation for Metric in Coarse-Stage Search (CSS)\label{tab:Coarse-Stage_Search}]{\hspace{-0mm}\vspace{-2mm}
\scalebox{0.85}
{\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{Coarse-Stage Search} & \textbf{Metric} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
\ding{55} & — & 11.53 & 14.58 \\
\ding{51}  & RI & 14.85 & 18.39 \\
\ding{51} & LR & \textbf{10.64} & \textbf{13.12}\\
\bottomrule
\end{tabular}}
}\hspace{3mm}\vspace{-2mm}
% 子表格4
\subfloat[\small Ablation for Pruning Type\label{tab:Prune_Type}]
{\scalebox{0.85}
{\hspace{-1.5mm}\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{Prune Type} & \textbf{Hardware-friendly} & \textbf{Wikitext2$\downarrow$} & \textbf{C4$\downarrow$} \\
\midrule
Structured & \ding{51} & 516.18 & 251.02 \\
Unstructured & \ding{55} & 8.72 & 11.29 \\
Semi-Structured & \ding{51} & \textbf{10.64} & \textbf{13.12} \\
\bottomrule
\end{tabular}}
}


\hspace{10mm}
\subfloat[\small Ablation for Metric in Fine-Stage Search\label{tab:Fine-Stage_Metric}]{\hspace{0mm}\vspace{-5mm}
%\resizebox{0.5\textwidth}{\height}
\scalebox{0.9}
{\begin{tabular}{c c c c c c}
\toprule
\rowcolor{color3}
\textbf{Metric} & \textbf{Random} & \textbf{Magnitude} & \textbf{Wanda} & \textbf{SI} & \textbf{Ours}\\
\midrule
\textbf{Wikitext2$\downarrow$} & 7,569.45 & 80.23 & 11.02 & 99.68 & \textbf{10.64} \\
\textbf{C4$\downarrow$} & 4,893.59 & 14.25 & 13.77 & 92.23 & \textbf{13.12} \\
\bottomrule
\end{tabular}}
}\hspace{5mm}\vspace{-0mm}
% 子表格6
\subfloat[\small Ablation for Number of Split Points\label{tab:split_points}]
{\scalebox{0.9}
{\hspace{-1.5mm}\begin{tabular}{c c c c}
\toprule
\rowcolor{color3}
\textbf{\#Split Points} &\textbf{1} & \textbf{2} & \textbf{3} \\
\midrule
\textbf{Wikitext2}$\downarrow$ & 13.23 & \textbf{10.64} & 10.18\\
\textbf{C4}$\downarrow$ & 15.91 & \textbf{13.12} & 12.77 \\
\bottomrule
\end{tabular}}
}
\hspace{3mm}

\label{tab:ablations}
\vspace{-8mm}
\end{table*}






\begin{figure*}[h]
% \label{fig:dialogue}
  \centering
  \vspace{-1mm}
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  % \fbox{\parbox[c][9cm]{\linewidth}{Abstract}}
   \includegraphics[width=1.0\textwidth]{figs/dialog.pdf}
   \vspace{-8mm}
   \caption{Conversation examples on LLaMA-13B (language supplementary) and LLaMA-2-13B(Q\&A). We compare our best method PBS$^2$P with BiLLM and ARB-LLM. \textcolor{red}{Inappropiate} and \textcolor{reasonable}{reasonable} responses are shown in corresponding colors.}
   \label{fig:dialog}
   \vspace{-5mm}
\end{figure*}
% \section{}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
