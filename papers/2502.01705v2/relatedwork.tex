\section{Related Works}
\subsection{Weight Quantization}
Quantization compresses full-precision parameters into lower-bit representations, reducing both computation and storage demands. Current quantization methods for LLMs are mainly divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT~\cite{liu2023llm,chen2024db,du2024bitdistiller} integrates quantization during the training phase to enhance low-bit weight representations. However, due to the enormous parameter number, retraining becomes excessively expensive and inefficient for LLMs. PTQ, as it directly applies quantization to the model weights without retraining, making it faster and less resource-demanding. Recent methods, like ZeroQuant~\cite{yao2022zeroquant} and BRECQ~\cite{li2021brecq}, improve quantization accuracy by incorporating custom quantization blocks and group labels. While GPTQ~\cite{frantar2022gptq} and QuIP~\cite{chee2024quip} use second-order error compensation to reduce quantization errors. 

Binarization, as the most extreme form of quantization, reduces model parameters to a single bit (Â±1). Prominent methods, like Binary Weight Network (BWN)~\cite{rastegari2016xnor} and XNOR-Net~\cite{rastegari2016xnor}, focus on binarizing the weights, with XNOR-Net~\cite{rastegari2016xnor} also binarizing activations. In the context of LLM binarization, BitNet~\cite{wang2023bitnet}, OneBit~\citep{xu2024onebit}, and BinaryMoS~\cite{jo2024mixture} adopt the QAT framework, while BiLLM~\cite{huang2024billm}, ARB-LLM~\cite{li2024arb}, and STBLLM\cite{dong2024stbllm} use PTQ combined with residual approximation. Our work focuses on the binary PTQ method, achieving substantial improvements over existing SOTA binary PTQ methods.
\vspace{1mm}

\subsection{LLM Pruning} 
Pruning is a widely used technique for compressing neural networks by removing less significant parameters, reducing the number of active weights. This results in sparse networks that are more efficient in memory, computation, and size. In LLMs, pruning methods are generally divided into structured, unstructured, and semi-structured approaches. Structured pruning~\cite{ma2023llm,ashkboos2024slicegpt,xia2023sheared,an2024fluctuation} eliminates entire structured model components to improve efficiency. However, this approach can lead to substantial performance degradation, often requiring retraining to restore lost functionality. Unstructured pruning~\cite{dong2024pruner}, removes weight elements individually based on their importance, maintaining high performance even at higher sparsity levels, but the resulting sparsity patterns are not hardware-efficient. Semi-structured pruning strikes an optimal balance by keeping regular sparsity patterns, such as $N$:$M$ sparsity, which is optimized for hardware, as seen in methods like SparseGPT~\cite{frantar2023sparsegpt}, Wanda~\cite{sun2023simple}, and STBLLM~\cite{dong2024stbllm}. Our approach leverages semi-structured pruning with $N$:$M$ sparsity, aiming to minimize performance degradation while maintaining hardware efficiency.


\subsection{Integration of Pruning and Quantization}
The combination of pruning and quantization has been extensively explored for neural network compression. Pruning reduces parameter counts, while quantization minimizes parameter precision. For example, Deep Compression~\cite{han2015deep} integrates pruning, quantization, and Huffman coding to reduce storage requirements for deep neural networks. Later studies~\citep{tung2018clip, yang2020automatic, hu2021opq} have developed methods to combine pruning and quantization in parallel to optimize compression strategies. In extreme cases like binarization, methods such as STQ-Nets~\cite{munagala2020stq}, BNN Pruning ~\cite{li2020bnn}, and BAP~\cite{wang2021extremely} combine these techniques to achieve high compression ratios and speedups. STBLLM~\cite{dong2024stbllm} uses the structural binary method to compress LLMs to less than 1-bit precision, further advancing model compression and efficiency. However, simply combining pruning with binarization still leads to significant model degradation in LLMs. Therefore, we propose a new framework, PBS$^2$P, which aims to reduce the combined errors of pruning and binarization, improving the performance of the compressed model.
\vspace{-1mm}