@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={AAAI},
  year={2024}
}

@inproceedings{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{chee2024quip,
  title={Quip: 2-bit quantization of large language models with guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M},
  booktitle={NeurIPS},
  year={2024}
}

@article{chen2024db,
  title={DB-LLM: Accurate dual-binarization for efficient LLMs},
  author={Chen, Hong and Lv, Chengtao and Ding, Liang and Qin, Haotong and Zhou, Xiabin and Ding, Yifu and Liu, Xuebo and Zhang, Min and Guo, Jinyang and Liu, Xianglong and others},
  journal={arXiv preprint arXiv:2402.11960},
  year={2024}
}

@inproceedings{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  booktitle={ICML},
  year={2024}
}

@inproceedings{dong2024stbllm,
  title={STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs},
  author={Dong, Peijie and Li, Lujun and Zhong, Yuedong and Du, Dayou and Fan, Ruibo and Chen, Yuhan and Tang, Zhenheng and Wang, Qiang and Xue, Wei and Guo, Yike and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{du2024bitdistiller,
  title={Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation},
  author={Du, Dayou and Zhang, Yijia and Cao, Shijie and Guo, Jiaqi and Cao, Ting and Chu, Xiaowen and Xu, Ningyi},
  booktitle={ACL},
  year={2024}
}

@inproceedings{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={ICML},
  year={2023},
}

@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{hu2021opq,
  title={Opq: Compressing deep neural networks with one-shot pruning-quantization},
  author={Hu, Peng and Peng, Xi and Zhu, Hongyuan and Aly, Mohamed M Sabry and Lin, Jie},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{huang2024billm,
  title={Billm: Pushing the limit of post-training quantization for llms},
  author={Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan},
  booktitle={ICML},
  year={2024}
}

@inproceedings{jo2024mixture,
  title={Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models},
  author={Jo, Dongwon and Kim, Taesu and Kim, Yulhwa and Kim, Jae-Joon},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{li2020bnn,
  title={Bnn pruning: Pruning binary neural network guided by weight flipping frequency},
  author={Li, Yixing and Ren, Fengbo},
  booktitle={ISQED},
  year={2020}
}

@inproceedings{li2021brecq,
  title={Brecq: Pushing the limit of post-training quantization by block reconstruction},
  author={Li, Yuhang and Gong, Ruihao and Tan, Xu and Yang, Yang and Hu, Peng and Zhang, Qi and Yu, Fengwei and Wang, Wei and Gu, Shi},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{li2024arb,
  title={ARB-LLM: Alternating Refined Binarizations for Large Language Models},
  author={Li, Zhiteng and Yan, Xianglong and Zhang, Tianao and Qin, Haotong and Xie, Dong and Tian, Jiang and Kong, Linghe and Zhang, Yulun and Yang, Xiaokang and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{liu2023llm,
  title={Llm-qat: Data-free quantization aware training for large language models},
  author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
  booktitle={ACL},
  year={2024}
}

@inproceedings{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{munagala2020stq,
  title={STQ-Nets: Unifying Network Binarization and Structured Pruning.},
  author={Munagala, Sri Aurobindo and Prabhu, Ameya and Namboodiri, Anoop M},
  booktitle={BMVC},
  year={2020}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{tung2018clip,
  title={Clip-q: Deep network compression learning by in-parallel pruning-quantization},
  author={Tung, Frederick and Mori, Greg},
  booktitle={CVPR},
  year={2018}
}

@article{wang2021extremely,
  title={Extremely sparse networks via binary augmented pruning for fast image classification},
  author={Wang, Peisong and Li, Fanrong and Li, Gang and Cheng, Jian},
  journal={TNNLS},
  year={2021}
}

@article{wang2023bitnet,
  title={Bitnet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@inproceedings{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{xu2024onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{yang2020automatic,
  title={Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach},
  author={Yang, Haichuan and Gui, Shupeng and Zhu, Yuhao and Liu, Ji},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  booktitle={NeurIPS},
  year={2022}
}

