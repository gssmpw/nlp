\section{Related Work}
\subsection{Anomaly detection on logs}
Based on the usage of log data, current detection methods for APTs can be broadly categorized into two types: 

\noindent \textbf{Rule-based methods} often rely on a large number of predefined expert rules, which depend on extensive threat intelligence resources and expert annotation. For example, Conan~\cite{xiong2020conan} detects APT attacks using a finite state automaton (FSA)-based state transition framework. Holmes~\cite{milajerdi2019holmes} defines a set of graph-matching rules based on TTPs associated with APT behaviors for threat detection. SLEUTH~\cite{hossain2017sleuth} and Morse~\cite{hossain2020combating} use label-based information flow techniques to reconstruct attack scenarios. CAPTAIN~\cite{wang2024incorporating} uses gradients to dynamically adjust detection rules, enhancing the flexibility of rule-based methods. However, all of these methods fail to identify unknown threats.

\noindent \textbf{Learning-based methods} have garnered more attention due to their higher accuracy in threat detection. Log2vec~\cite{log2vec} constructs a heterogeneous graph from the logs and applies graph embedding to detect abnormal activities. Deeplog~\cite{deeplog} treats the audit logs as sentences and utilizes LSTM models to detect abnormal events [11]. Streamspot~\cite{manzoor2016streamspot} extracts local graph features through breadth-first search and clusters snapshots to detect abnormalities. Unicorn~\cite{han2020unicorn} employs a graph similarity matching method to detect abnormal graphs. Threatrace~\cite{wang2022threatrace} and Flash~\cite{rehman2024flash} compare predicted node types with actual node types to perform node-level anomaly detection. Shadewatcher~\cite{zengy2022shadewatcher} constructs a knowledge graph from system logs and uses a GNN-based recommendation system to detect malicious interactions. Atlas~\cite{alsaheel2021atlas} applies lemmatization and word embeddings to generate sequences and uses LSTM networks to predict whether sequences are related to attacks.Prov-GEM~\cite{kapoor2021prov} uses multi-embedding and supervised learning to classify graphs based on aggregated node embeddings for anomaly detection.

However, learning-based methods have limitations. For instance, Streamspot, Shadewatcher, Flash and Threatrace use GNNs to aggregate multi-hop neighbor information, making it hard to capture long-range context. Prov-GEM and Atlas rely on supervised learning, requiring attack-containing data. Except for Atlas, they all fail to capture long-range context.

\subsection{Graph Transformer}

The Transformer with attention mechanisms has achieved significant success in various fields, including Natural Language Processing~\cite{vaswani2017attention} and Computer Vision. In Graph Transformers~\cite{dwivedi2021generalization}, global attention is typically computed, allowing each node to attend to all other nodes, regardless of edge connections. This enables Graph Transformers to effectively capture long-range dependencies.
% Sentient designs Graph Comprehension based on Graph Transformer.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/overview.pdf}
    \caption{Overview Of Sentient's architecture.}
    \label{fig:Overview}
    \vspace{-0.1in} 
\end{figure*}

\subsection{State Space Models}
General state space models (SSMs), such as hidden Markov models and RNNs, process sequences through recurrent updates of hidden states, generating outputs by combining hidden states with inputs. Structured state space models (S4) improve computational efficiency with reparameterization~\cite{gu2021efficiently}.
Building on S4, Mamba~\cite{gu2023mamba} and Mamba2~\cite{dao2024transformers} introduce a data-dependent selection mechanism to better capture long-range context as sequence length increases. These models achieve linear-time efficiency for long-sequence modeling.
% Sentient designs Multiple Scenario Comprehension based on Mamba2.