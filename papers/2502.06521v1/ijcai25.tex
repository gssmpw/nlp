%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass[anonymous]{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amssymb}
\usepackage{subcaption}

% add
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
% end
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Sentient: Multi-Scenario Behavioral Intent Analysis for Advanced Persistent Threat Detection}

% Single author syntax
\iffalse
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}
\fi

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)

\author{
Wenhao Yan$^{1,2}$
\and
Ning An$^{1,2}$\and
Wei Qiao$^{1,2}$\and
Weiheng Wu$^{1,2}$\and
Bo Jiang$^{1,2}$\and
Yuling Liu$^{1,2}$\and
Zhigang Lu$^{1,2}$\and
Junrong Liu$^{1,2}$\\
\affiliations
$^1$Institute of Information Engineering, Chinese Academy of Sciences\\
$^2$School of Cyber Security, University of Chinese Academy of Sciences\\
\emails
\{yanwenhao, anning, qiaowei, wuweiheng, jiangbo, liuyuling, luzhigang, liujunrong\}@iie.ac.cn
}

% \author{
% Wenhao Yan$^1$
% \and
% Bingsheng Bi$^2$\and
% Weiheng Wu$^{2,3}$\and
% Wei Qiao$^4$\and
% Yuling Liu$^4$\and
% Junrong Liu$^4$\and
% Zhigang Lu$^4$\and
% Bo Jiang$^4$\and
% Ning An$^4$\\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }

\begin{document}

\maketitle

\begin{abstract}

Advanced Persistent Threats (APTs) are challenging to detect due to their complexity and stealth. To mitigate such attacks, many approaches utilize provenance graphs to model entities and their dependencies, detecting the covert and persistent nature of APTs. However, existing methods face several challenges: 1) Environmental noise hinders precise detection; 2) Reliance on hard-to-obtain labeled data and prior knowledge of APTs limits their ability to detect unknown threats; 3) The difficulty in capturing long-range interaction dependencies, leading to the loss of critical context.
We propose Sentient, a threat detection system based on behavioral intent analysis that detects node-level threats from audit logs. Sentient constructs a provenance graph from the audit logs and uses this graph to build multiple scenarios. By combining graph comprehension with multiple scenario comprehension, Sentient learns normal interaction behaviors. Sentient detects anomalies by identifying interactions that deviate from the established behavior patterns.
We evaluated Sentient on three widely used datasets covering both real-world and simulated attacks. The results confirm that Sentient consistently delivers strong detection performance. Notably, Sentient achieves entity-level APT detection with a precision of 96\% and a recall of 99\%.

\end{abstract}

\section{Introduction}
Advanced Persistent Threats (APTs) are notorious for their stealth and complexity. Attackers meticulously plan their strategies to gain long-term control over the target system and remain dormant, causing significant damage to modern enterprises and institutions~\cite{alshamrani2019aptsurvey}. For example, the SolarWinds~\cite{solarwinds2020} attack in 2020, which compromised multiple U.S. government agencies and major private companies, exposed sensitive information and disrupted operations on a global scale. Thus, effective detection of APTs has been a focus of both industry and academia.

When attackers infiltrate systems, they inevitably leave traces. Utilizing audit logs for threat detection is effective, but traditional Intrusion Detection Systems (IDS) struggle to extract sufficient information, making them less effective against APTs~\cite{inam2023sok}. Recent studies propose using provenance graphs, where entities (e.g., processes, files) are nodes and interactions (e.g., write, open) are edges. By analyzing valuable information in provenance graphs, these methods effectively leverage the rich contextual data in audit logs, providing strong support for threat detection and attribution.
In academia, provenance graph-based detection methods are generally classified into two categories: 1) Rule-based methods~\cite{xiong2020conan,hassan2020tactical,milajerdi2019poirot,milajerdi2019holmes}, which rely on expert-defined rules to detect threats; and 2) learning-based methods~\cite{han2020unicorn,jia2024magic,yang2023prographer,rehman2024flash}, which use machine learning techniques to identify anomalous behaviors and attack patterns in provenance graphs. Although previous studies have demonstrated some effectiveness in detecting APTs attacks, three key practical challenges limit their real-world applicability:

\begin{itemize}
\item \textbf{Noisy detection environment}: Existing methods~\cite{rehman2024flash,wang2022threatrace} identify anomalies by analyzing the full provenance graph. However, compromised nodes often perform both malicious and normal tasks, causing attack behaviors to be interfered with by legitimate behaviors in noisy environments, leading to false negatives. Moreover, the unclear boundary between anomalous and benign behaviors can lead to false positives, as the same behaviors may indicate different intentions (attack or benign) depending on the context.

\item \textbf{Insufficient prior knowledge and labeled data}: Rule-based methods rely on expert-defined rules, but the scarcity of threat intelligence limits their effectiveness, particularly against unknown threats. Moreover, supervised learning methods~\cite{kapoor2021prov} depend on prior knowledge and labeled data, and insufficient labeled data complicates training and makes detecting novel APTs challenging.

\item \textbf{Context Loss in Long-Distance Interactions}: Many approaches~\cite{cheng2024kairos} use Graph Neural Networks (GNNs) to model relationships in provenance graphs by aggregating nearby nodes' information. However, GNNs struggle to capture dependencies between distant nodes, leading to the loss of crucial contextual information needed to detect sophisticated attack patterns.

\end{itemize}

This paper introduces Sentient, a self-supervised APT detection method based on multi-scenario learning, designed to address the challenges mentioned above. Sentient learns exclusively from legitimate interactions within benign data, enabling it to accurately identify suspicious behaviors in malicious scenarios and abstract audit event semantics into behavioral patterns, thereby alleviating the threat investigation burden on security analysts. Specifically, Sentient has the following features: (1) it generates a provenance graph from audit logs, segments multi-scenario environments using a random walk algorithm, and includes long event sequences in each scenario to address long-range context loss; (2) it designs two self-supervised tasks (Graph Comprehension and Multiple Scenario Comprehension) for intent learning, allowing Sentient to operate without prior knowledge or labeled data during training and detection. By combining overall scenario information with specific contextual details from multi-scenario environments, Sentient learns normal node interactions and mitigates noise-related challenges; (3) it predicts node interactions in specific scenarios and detects anomalous behaviors that deviate from normal interactions; (4) it clusters event intents (composed of node and event embeddings) and constructs intent sequences to reduce the manual audit workload.

We have implemented Sentient and conducted comprehensive evaluations on three distinct APT attack datasets: the DARPA Transparent Computing datasets~\cite{darpa_e3}, the StreamSpot dataset~\cite{streamspot}, and the Unicorn Wget dataset~\cite{Unicorn}. The DARPA dataset contains real-world attacks, while the StreamSpot and Unicorn Wget datasets are fully simulated in a controlled environment.

In summary, this paper makes the following contributions:
\begin{itemize}
\item We propose Sentient, an APT detection method that combines graph comprehension with multiple scenario comprehension. By integrating contextual information from different scenarios, Sentient identifies malicious behaviors based on their underlying intentions.
\item We design two self-supervised tasks for intent learning, enabling Sentient to detect anomalous activities by learning only from normal behaviors in benign logs, addressing the challenge of insufficient labeled data.
\item Sentient performs scenario segmentation based on the provenance graph, with each scenario containing long-range contextual information necessary for intent learning. This enables Sentient to perceive threats with a broader perspective.
\end{itemize}

\noindent \textbf{Availability.} Sentient is available online at https://
anonymous.4open.science/r/Sentient-17A4 (Anonymized).

\section{Related Work}
\subsection{Anomaly detection on logs}
Based on the usage of log data, current detection methods for APTs can be broadly categorized into two types: 

\noindent \textbf{Rule-based methods} often rely on a large number of predefined expert rules, which depend on extensive threat intelligence resources and expert annotation. For example, Conan~\cite{xiong2020conan} detects APT attacks using a finite state automaton (FSA)-based state transition framework. Holmes~\cite{milajerdi2019holmes} defines a set of graph-matching rules based on TTPs associated with APT behaviors for threat detection. SLEUTH~\cite{hossain2017sleuth} and Morse~\cite{hossain2020combating} use label-based information flow techniques to reconstruct attack scenarios. CAPTAIN~\cite{wang2024incorporating} uses gradients to dynamically adjust detection rules, enhancing the flexibility of rule-based methods. However, all of these methods fail to identify unknown threats.

\noindent \textbf{Learning-based methods} have garnered more attention due to their higher accuracy in threat detection. Log2vec~\cite{log2vec} constructs a heterogeneous graph from the logs and applies graph embedding to detect abnormal activities. Deeplog~\cite{deeplog} treats the audit logs as sentences and utilizes LSTM models to detect abnormal events [11]. Streamspot~\cite{manzoor2016streamspot} extracts local graph features through breadth-first search and clusters snapshots to detect abnormalities. Unicorn~\cite{han2020unicorn} employs a graph similarity matching method to detect abnormal graphs. Threatrace~\cite{wang2022threatrace} and Flash~\cite{rehman2024flash} compare predicted node types with actual node types to perform node-level anomaly detection. Shadewatcher~\cite{zengy2022shadewatcher} constructs a knowledge graph from system logs and uses a GNN-based recommendation system to detect malicious interactions. Atlas~\cite{alsaheel2021atlas} applies lemmatization and word embeddings to generate sequences and uses LSTM networks to predict whether sequences are related to attacks.Prov-GEM~\cite{kapoor2021prov} uses multi-embedding and supervised learning to classify graphs based on aggregated node embeddings for anomaly detection.

However, learning-based methods have limitations. For instance, Streamspot, Shadewatcher, Flash and Threatrace use GNNs to aggregate multi-hop neighbor information, making it hard to capture long-range context. Prov-GEM and Atlas rely on supervised learning, requiring attack-containing data. Except for Atlas, they all fail to capture long-range context.

\subsection{Graph Transformer}

The Transformer with attention mechanisms has achieved significant success in various fields, including Natural Language Processing~\cite{vaswani2017attention} and Computer Vision. In Graph Transformers~\cite{dwivedi2021generalization}, global attention is typically computed, allowing each node to attend to all other nodes, regardless of edge connections. This enables Graph Transformers to effectively capture long-range dependencies.
% Sentient designs Graph Comprehension based on Graph Transformer.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/overview.pdf}
    \caption{Overview Of Sentient's architecture.}
    \label{fig:Overview}
    \vspace{-0.1in} 
\end{figure*}

\subsection{State Space Models}
General state space models (SSMs), such as hidden Markov models and RNNs, process sequences through recurrent updates of hidden states, generating outputs by combining hidden states with inputs. Structured state space models (S4) improve computational efficiency with reparameterization~\cite{gu2021efficiently}.
Building on S4, Mamba~\cite{gu2023mamba} and Mamba2~\cite{dao2024transformers} introduce a data-dependent selection mechanism to better capture long-range context as sequence length increases. These models achieve linear-time efficiency for long-sequence modeling.
% Sentient designs Multiple Scenario Comprehension based on Mamba2.

\section{Motivation \& Threat Model}
\subsection{Motivating Example}\label{sec:3.1}

Figure~\ref{fig:Motivation} presents a real-world APT attack scenario, based on DARPA data. The attacker initially compromises a trusted website (nhra.com) and waits for a victim with a vulnerable Firefox browser to connect to a malicious server. When the victim visits the site, the attacker exploits the Drakon vulnerability to download and execute loaderDrakon in Firefox's memory. This allows the attacker to establish a connection with their server (IP 189.141.204.211) and issue commands. The attacker then uses a previously installed driver, BinFmt Elevate, to escalate Firefox's privileges, gaining root access. Finally, the attacker injects shellcode into the sshd process using the Inject2 Process technique, writing a file (sshdlog) to disk, and proceeds to connect to other malicious servers to exfiltrate data.

\begin{figure}[h]
    \vspace{-0.05in}
    \centering
    \includegraphics[width=\linewidth]{fig/motivation2.pdf}
    \caption{Provenance of the Drakon attack from DARPA. R = Read, W = Write, O = Open, E = Execute, S = Send, and Rc = Receive.
}
    \label{fig:Motivation}
    \vspace{-0.1in}
\end{figure}

In this instances, red and green markers represent malicious and benign nodes, respectively. However, many nodes exhibit both benign and malicious behaviors (marked in gray), complicating threat detection. This indicates that noisy environments hinder effective threat detection.

\subsection{Threat Model}
In this threat model, we assume the attacker operates stealthily, blending malicious activities with normal system operations to obscure traces, making detection difficult~\cite{goyal2023sometimes}. The attacker may exploit zero-day vulnerabilities, bypassing traditional detection mechanisms. Techniques like "defensive evasion" and "camouflage" further complicate detection by manipulating system processes to appear legitimate. However, malicious activities will inevitably leave identifiable anomalies in the system's audit logs.
Similar to prior works~\cite{cheng2024kairos,wang2020path_de,rehman2024flash,jia2024magic}, this model assumes the auditing system provides accurate, integrity-protected activity records. By ensuring log integrity with tamper-resistant storage, these logs serve as a reliable foundation for behavioral analysis and threat detection.

\section{Methodology}

In this section, we provide a detailed description of the overall design framework of Sentient and its key components: Graph Construction, Intent Learning, Threat Detection, and Attack Investigation. The overall framework is illustrated in Figure~\ref{fig:Overview}.

\subsection{Graph Construction and Representation}\label{sec:4.1}
\noindent \textbf{Graph Construction}: 
To correlate the interaction behaviors between system entities, Sentient constructs a system provenance graph (PG) from audit data collected by logging infrastructures such as Windows ETW, Linux Audit, and CamFlow~\cite{camflow}. In this graph, the nodes represent entities (e.g., files, processes) with attribute information (e.g., file path, process name), while the edges denote system interactions (e.g., write, open). As shown in Figure~\ref{fig:example}, to identify unique node interactions and reduce computational overhead, we adopt the method of~\cite{jia2024magic} by removing duplicate edges based on edge types and merging different edge types to minimize redundancy.

\noindent \textbf{Initial Embedding}: 
To better represent the interaction scenarios in the graph and allow the model to fully leverage the rich attributes between system logs and various system entities, encoding techniques can be used to project these attributes (e.g., process name, file path) into vector representations. Sentient employs Word2Vec \cite{mikolov2013efficient}, which incorporates contextual attributes to project these features into low-dimensional vector representations while preserving the deeper correlations between node attributes. As a result, the attributes of files with similar or related functionalities (e.g., \texttt{/bin/vim}, \texttt{/bin/nano}, \texttt{/bin/cat}, \texttt{/usr/share/vim}) are projected into closer vector spaces, while files with distinct functionalities (e.g., \texttt{/bin/vim} and \texttt{/bin/python3}) are projected into more discrete vector spaces.

Previous studies \cite{ying2021transformers,huang2024good} have shown that the Laplacian eigenvectors of a graph possess strong position-awareness capabilities. In Sentient, we apply a Laplacian spectral transformation to the adjacency matrix of the provenance graph to construct Laplacian eigenvectors, which are then used as positional representations for the nodes. The eigenvectors are derived from the graph Laplacian matrix factorization:
\begin{align}
L &= I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}, \\
L v_i &= \lambda_i v_i, \quad i = 1, 2, \dots, k+1.
\end{align}

where \( A \) is the adjacency matrix and \( D \) is the degree matrix. The \( k \) smallest non-trivial eigenvectors, denoted as \( v_i \) for each node \( i \), are used as positional encodings for the nodes, with \( \lambda_i \) representing the eigenvalue associated with \( v_i \).

\begin{figure}[h]
    \vspace{-0.05in}
    \centering
    \includegraphics[width=\linewidth]{fig/example.pdf}
    \caption{An example of Log Parsing (left) and Graph Construction(right).}
    \label{fig:example}
    \vspace{-0.1in}
\end{figure}

\noindent \textbf{Scenario Segmentation}:
As discussed in section \ref{sec:3.1}, noisy detection environments hinder effective detection, and in order to differentiate clean detection scenarios suitable for detection from noisy global scenarios, Sentient employs the random wandering algorithm \cite{nikolentzos2020random} to enrich the scenario in the graph.

\subsection{Intent Learning}

As shown in Figure~\ref{fig:Overview}B, Intent learning consists of two components: Graph Comprehension and Multiple Scenario Comprehension.
% In Graph Comprehension, Sentient learns the structural and attribute features of nodes in the provenance graph to uncover the interaction relationships among nodes. In Multiple Scenario Comprehension, Sentient focuses on learning the interaction patterns of nodes across diverse scenarios, revealing their behavioral patterns in specific contexts.

\noindent \textbf{Graph Comprehension}:
To comprehensively learn both the attribute and structural information in the provenance graph, Sentient combines node feature encoding \(a_0\) and Laplacian positional encoding \(b_0\) as inputs to the Graph Transformer (GT) \cite{dwivedi2021generalization}. These two encodings are linearly transformed and combined to form the unique initial representation \(h_0\) for each node.
% \begin{align}
% h_i^0 &= \hat{h}_i^0 + \lambda_i^0 \;.
% \end{align}

where \(\hat{h}_i^0\) represents the attribute information and \(\lambda_i^0\) represents the location information. \(h_i^0\) is the final initial representation, which will be used as an input to the GT. We formally describe the Graph transformer as follows:
\begin{align}
h'^{(l+1)} &= \text{MHA}(\text{LN}(h^{(l)})) + h^{(l)}, \\
h^{(l+1)} &= \text{FFN}(\text{LN}(h'^{(l+1)})) + h'^{(l+1)}.
\end{align}

The output \( h^{(l)} \) from the final layer is used as the node embedding. Unlike traditional GNNs that primarily focus on the local neighborhood of each node, these embeddings can capture the full topological structure of the graph even without multiple iterations.

During the Graph Comprehension phase, nodes learn the contextual information of the global scenario and generate node embeddings. Subsequently, Sentient utilizes these node embeddings and employs a Multi-Layer Perceptron (MLP) layer to generate the probability distribution of node types: \( P(h_i) = \text{MLP}(h_i) \).
where \( h_i \) represents the embedding of node \( i \), and \( P(h_i) \) denotes the node type (e.g., File, Network, Process) probability matrix for node \( i \). However, the distribution of node types in system logs often exhibits significant imbalance. To counteract this, we use a weighted cross-entropy loss function. 
% The equation for the weighted cross-entropy loss is:
% \begin{equation}
% \begin{split}
% L(y, f(x)) = &-\sum_{i=1}^n \big(w_i y_i \log(f(x)_i)\\
% & + (1 - y_i) \log(1 - f(x)_i) \big).
% \end{split}
% \end{equation}
% Where \(y\) represents the ground truth label vector, \(f(x)\) is the predicted label vector, \(n\) denotes the number of classes, \(w_i\) is the weight for the \(i\)-th class, and \(\log\) is the natural logarithm. 
By training our GT model with this loss function, we ensure that it effectively learns the structural relationships between different node types in the provenance graph.
It is worth noting that the node classification task here is a pseudo-task, aimed solely at obtaining an ideal node embedding.

\noindent \textbf{Multiple Scenario Comprehension}: 
As shown in Figure~\ref{fig:Mamba2}, each scenario generated in Section~\ref{sec:4.1} is essentially a long sequence of node IDs. In this section, Sentient replaces the node IDs in the sequence with node embeddings generated from the Graph Comprehension phase to create scenario representations.

The Mamba-2 block, a member of the State Space Models (SSMs) class, demonstrates significant potential in natural language processing (NLP) and computer vision (CV) tasks ~\cite{qu2024survey}. 
A standard Structured State Space Sequence Model (S4) transforms a 1-dimensional sequence \(x \in \mathbb{R}^T\) into another sequence \(y \in \mathbb{R}^T\) using a latent state \(h \in \mathbb{R}^{T \times N}\), as follows:
\begin{align}
h_t = A h_{t-1} + B x_t,  &\quad y_t = C h_t + D x_t.
\end{align}

where \( A \in \mathbb{R}^{N,N} \), \( B \in \mathbb{R}^{N,1} \), \( C \in \mathbb{R}^{1,N} \), \( D \in \mathbb{R} \). \(h_{t-1}\) represents the hidden state at the previous time step, and \(x_t\) is the input at the current time step, \(h_t\) denotes the hidden state derivative.

The original Mamba-2 block processes sequences in a single direction. To capture the rich evolution patterns in time series across different directions and the complex inter dependencies within sequences, we extend it to construct a bidirectional Mamba-2 (Bi-Mamba2) model, as illustrated in Figure~\ref{fig:Mamba2}(b). This design leverages both forward and backward contextual information to learn the behavioral intentions of events in scenario sequences. Specifically, the scenario sequence \(L \) serves as the input.
% where \(B\), \(W\), and \(D\) represent the batch size, sequence length, and feature dimension, respectively.
For the two directions, we denote the input embeddings at layer \(l\) as \(\mathbf{E}_{x,\text{dir}}^{(l)}\), where \(\text{dir} \in \{\text{forward}, \text{backward}\}\). The outputs of the forward and backward Mamba-2 blocks are \(\mathbf{E}_{y,\text{forward}}^{(l)}\) and \(\mathbf{E}_{y,\text{backward}}^{(l)}\), respectively. The Bi-Mamba2 encoder layer receives its input through the following computation:
\begin{align}
\mathbf{E}_{x}^{(l+1)} = \sum_{\text{dir} \in \{\text{forward}, \text{backward}\}} \mathbf{F}(\mathbf{E}_{y,\text{dir}}^{(l)}, \mathbf{E}_{x,\text{dir}}^{(l)}).
\end{align}

where \(\mathbf{F}\) combines the residual connection and feed forward layers neural network layers. This bidirectional design improves the model’s ability to capture interactions.

As shown in Figure~\ref{fig:Mamba2}(c), Sentient learns the contextual information of sequences through Bi-Mamba2 to identify the normal behavioral patterns between nodes under specific scenarios and generate event embeddings. To predict the benign interaction behaviors between nodes in a particular context, Sentient employs MLP to calculate the probability distribution of interaction types (e.g., write, execute) involved in events constituted by the source node \(v_{\text{src}}\) and the destination node \(v_{\text{dst}}\): \( P(\text{Event}_{\text{src,dst}}) = \text{MLP}(\text{Event}_{\text{src,dst}}) \).

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/mamba-2.pdf}
    \caption{The architecture of (a) Mamba-2, (b) Bi-Mamba2 and (c) Scenario Comprehension.}
    \label{fig:Mamba2}
    \vspace{-0.1in}
\end{figure}

During training, Sentient minimizes the reconstruction error (RE) between the predicted probability vector $P(\text{event}_{\text{src,dst}})$ and the actual edge type $L(\text{event}_{\text{src,dst}})$ observed in the benign origin graph:
\begin{align}\label{ali:re}
\text{RE} = \text{CrossEntropy}(P(\text{Event}_{\text{src,dst}}), L(\text{Event}_{\text{src,dst}})).
\end{align}

\( L(\text{event}) \) represents a one-hot vector, where the probability corresponding to the actual interaction type present in the event is 1 and the rest 0.

\subsection{Threat Detection}

During the detection phase, we utilize a detection model that has been trained on benign interaction behaviors. Sentient streams audit logs and converts them into provenance graphs, initializing the embedding values of the nodes and constructing multi-scenario contexts based on the provenance graph structure. Subsequently, Sentient inputs the provenance graph into the Graph Comprehension module to learn node embeddings and generates context representations based on these embeddings and multi-scenario contexts. The Multiple Scenario Comprehension module then predicts the expected interaction behaviors of events under different scenarios $\text{P(Event)}$ and calculates the reconstruction loss (RE) between the predicted interactions and the actual event interactions $\text{L(Event)}$ using Equation~\ref{ali:re}. If the reconstruction loss RE exceeds a predefined threshold $\text{thr}$, the event in the context is classified as malicious; otherwise, it is classified as benign.

Since we diversify the scenarios, the same event may appear in multiple scenarios. However, most malicious events exhibit malicious behavior only in specific scenarios. Therefore, if an event is identified as malicious in any scenario, the two nodes associated with the event are labeled as malicious.

\subsection{Attack Investigation}

Despite the detection capabilities of threat detection systems, false positives and false negatives remain inevitable. Consequently, security analysis continues to rely heavily on manual review~\cite{false_postivte}. The sheer volume of audit logs makes it challenging for analysts to process them efficiently, while the complexity of raw logs further exacerbates their cognitive burden. In this section, Sentient alleviates this burden by providing high-level behavioral representations through event intent clustering, which is composed of node embeddings and event embeddings related to the events.

As shown in Figure~\ref{fig:Overview}D, Sentient categorizes similar event intents into the same class, and subsequently constructs intent chains based on event sequences and intent classifications.  This process provides security analysts with a concise representation of event relationships, thereby reducing the complexity of log analysis for security analysts.

\section{Evaluation}
\subsection{Datasets and Settings}

To comprehensively evaluate our approach, we selected three representative datasets for evaluation:

\noindent \textbf{Steamspot}~\cite{streamspot}: This dataset includes five benign scenarios (CNN, Download, Gmail, Vgame, YouTube) and one malicious scenario, each executed 100 times.  Using the Linux System Tap logging system, 100 provenance graphs are generated for each scenario.

\noindent \textbf{Unicorn Wget Dataset}~\cite{Unicorn}: This dataset consists of simulated attacks designed by UNICORN and includes 150 batches of logs collected by CamFlow, with 125 batches being benign and 25 batches malicious.

\noindent \textbf{DARPA-E3 Dataset}~\cite{darpa_e3}: This dataset was collected from an enterprise network during a defense exercise, where the Red Team conducted Advanced Persistent Threat (APT) attacks to exploit vulnerabilities and steal sensitive data. We selected the Cadets, Theia, and Trace datasets to evaluate Sentient. Since no official labels are available, we adopt the ground truth labels used by ThreaTrace and Flash for evaluation.

We adopted the evaluation metrics used by Threatrace~\cite{wang2022threatrace} to compare performance across different granularities, ensuring a fair comparison with other detection methods. The performance metrics include Precision, Recall, F1-score, Accuracy, and False Positive Rate (FPR).

To comprehensively evaluate efficacy, Sentient is compared with both coarse-grained and fine-grained state-of-the-art methods. Unfortunately, some methods were excluded from our evaluation due to their closed-source nature and reliance on private datasets, which made it infeasible to reproduce their systems based on their paper descriptions. For instance, Shadewatcher~\cite{zengy2022shadewatcher} is not fully open-source, and the authors acknowledged that one of the key components in their system is proprietary.

\noindent \textbf{Implementation}: We implemented Sentient in Python 3.10, using the torch-based DGL framework for graph learning and the Gensim library for Word2Vec, with PyTorch as the core development framework. All experiments were conducted on a system running Ubuntu 22.04, equipped with an Intel(R) Core(TM) i5-12490F CPU, an NVIDIA GTX 4060Ti GPU (16GB), and 64GB of RAM.

\subsection{Detection Performance Comparsion}

The Sentient model is trained using a portion of the benign data from the dataset, while the remaining benign data mixed with malicious data is used for testing.
\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}clllll@{}}
        \toprule
        Datasets & Systems & Precision & Recall & F-score & FRP \\ \midrule
        \multirow{5}{*}{E3-Cadets} & DeepLog & 23\% & 74\% & 35\% & 4.4\% \\
         & Log2vec & 49\% & 85\% & 62\% & 1.6\% \\
         & Threatrace & 84\% & 99\% & 91\% & 0.7\% \\
         & Flash & 94\% & 99\% & 96\% & 0.2\% \\
         & \textbf{Sentient} & \textbf{96\%} & \textbf{99\%} & \textbf{97\%} & \textbf{0.1\%} \\ \midrule
        \multirow{5}{*}{E3-Theia} & DeepLog & 16\% & 14\% & 15\% & 0.5\% \\
         & Log2vec & 62\% & 66\% & 64\% & 0.3\% \\
         & Threatrace & 79\% & 99\% & 88\% & 2.1\% \\
         & Flash & 91\% & 99\% & 95\% & 0.7\% \\
         & \textbf{Sentient} & \textbf{95\%} & \textbf{99\%} & \textbf{97\%} & \textbf{0.5\%} \\ \midrule
        \multirow{5}{*}{E3-Trace} & DeepLog & 41\% & 68\% & 51\% & 2.7\% \\
         & Log2vec & 54\% & 78\% & 64\% & 1.8\% \\
         & Threatrace & 82\% & 99\% & 90\% & 1.3\% \\
         & Flash & 95\% & 99\% & 97\% & 0.3\% \\
         & \textbf{Sentient} & \textbf{97\%} & \textbf{99\%} & \textbf{98\%} & \textbf{0.2\%} \\ \bottomrule
        \end{tabular}
    }
    \caption{Performance Comparison of Sentient with State-of-the-Art Methods at the Node Level.}
    \label{compare1}
    \vspace{-0.1in}
\end{table}

As shown in Table~\ref{compare1}, we evaluated Sentient's performance at the node level using the DARPA E3 dataset and compared it with DeepLog~\cite{deeplog}, Log2vec~\cite{log2vec}, Threatrace, and Flash~\cite{rehman2024flash}. Sentient outperforms Threatrace and Flash in terms of Precision, F-score, and False Positive Rate (FPR), demonstrating its ability to generate fewer false positives during detection.

\begin{table}[h]
    \vspace{-0.05in}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}llllll@{}}
            \toprule
            Datasets & Systems & Precision & Recall & F-score & FPR \\ \midrule
            \multirow{5}{*}{Streamspot} & Streamspot & 73\% & 91\% & 81\% & 6.6\% \\
             & Unicorn & 95\% & 97\% & 96\% & 1.1\% \\
             & Threatrace & 95\% & 93\% & 96\% & 0.4\% \\
             & Flash & 99\% & 99\% & 99\% & 0.2\% \\
             & \textbf{Sentient} & \textbf{99\%} & \textbf{100\%} &  \textbf{99\%} & \textbf{0.2\%} \\ \midrule
            \multirow{5}{*}{Unicorn Wget} & Unicorn & 86\% & 95\% & 90\% & 15.5\% \\
             & Prov-Gem & \textbf{100\%} & 80\% & 89\% & \textbf{0\%} \\
             & Threatrace & 93\% & 98\% & 95\% & 7.4\% \\
             & Flash & 92\% & 96\% & 94\% & 8.0\% \\
             & \textbf{Sentient} & 96\% & \textbf{99\%} & \textbf{97\%} & 4.0\% \\ \bottomrule
        \end{tabular}
    }
    \caption{Performance Comparison of Sentient with State-of-the-Art Methods at the Graph Level.}\label{compare2}
    \vspace{-0.1in}
\end{table}

As shown in Table~\ref{compare2}, we evaluated Sentient's performance at the graph level using the Streamspot and Unicorn Wget datasets. For the Streamspot dataset, we compared Sentient with Streamspot~\cite{manzoor2016streamspot}, Unicorn~\cite{han2020unicorn}, Threatrace, and Flash. For the Unicorn Wget dataset, we compared Sentient with Unicorn, Prov-Gem~\cite{kapoor2021prov}, Threatrace, and Flash. On the Streamspot, which involves relatively simple attack scenarios, Sentient easily detected anomalies and achieved near-perfect performance. On the Unicorn Wget, Sentient outperformed all other methods in terms of Recall, while its Precision and FPR were slightly lower than those of Prov-Gem. This is because Prov-Gem uses a supervised learning approach to train its model. However, Sentient demonstrated superior awareness of malicious samples, resulting in higher Recall compared to Prov-Gem.

We attribute Sentient's superior efficacy to its ability to capture long-range context and a cleaner detection environment. In contrast, Threatrace and Flash rely on local context and attempt fine-grained detection in noisy environments.

\subsection{Efficacy in Attack Investigation}
We used the Streamspot dataset, which includes five scenarios (YouTube, Gmail, Vgame, Download, CNN), to validate the similarity in high-level representations of behaviors with similar intentions. As shown in Figure~\ref{fig:tsne} , the t-SNE visualization of event intention embeddings illustrates this similarity. Figure~\ref{fig:tsne}(b) presents a zoomed-in view of Figure~\ref{fig:tsne}(a), highlighting different types of operations in the Vgame scenario. These operations exhibit clear boundaries compared to other scenarios. In Figure~\ref{fig:tsne}(b), the circled areas represent the unlink and close operations performed by processes on files in the Vgame scenario. These behaviors partially overlap in the t-SNE representation due to their similar intentions.

\begin{figure}[h]
    \vspace{-0.1in}
    \centering
    \includegraphics[width=\linewidth]{fig/Streamspot_tsne.pdf}
    \caption{t-SNE visualization: (a) shows the embedding space of StreamSpot, where each point represents a behavioral intent; (b) is an enlarged view of the circled area in (a).
}
    \label{fig:tsne}
    \vspace{-0.1in}
\end{figure}

\subsection{Performance Overhead}

% \begin{figure}[h]
%     \vspace{-0.05in}
%     \centering
%     \includegraphics[width=\linewidth]{fig/overhead.pdf}
%     \caption{Performance overhead of Sentient on the E3-Cadets.
% }
%     \label{fig:overhead}
%     \vspace{-0.1in}
% \end{figure}
\begin{figure}[htbp]
    \vspace{-0.05in}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/overhead1.pdf}
        \caption{Time Overhead}
        \label{fig:overhead1}
    \end{subfigure}
    % \hfill
    % \hspace{0.02\textwidth}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/overhead2.pdf}
        \caption{Memory Usage}
        \label{fig:overhead2}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Performance overhead of Sentient on the E3-Cadets.}
    \label{fig:overhead}
    \vspace{-0.1in}
\end{figure}

We evaluated Sentient’s time and memory overhead using the CADETS dataset from DARPA E3. The results presented here are based on the average values from multiple experiments. Figure~\ref{fig:overhead1} shows the time overhead in both the preprocessing phase (converting raw logs into provenance graphs) and the detection phase. The x-axis represents the batch, with the average time overhead for data preprocessing and detection being 1.82 seconds and 1.23 seconds, respectively. Figure~\ref{fig:overhead2} illustrates Sentient’s memory usage during the detection phase, with a peak memory overhead of 2061.6 MiB. The E3-Cadets dataset was collected over two weeks, generating approximately 2.6 GB of audit logs per day. For our experiments, we selected two days' worth of log data, and the average time required to complete the detection (including preprocessing) for one day's logs was only 63.6 seconds. This indicates that Sentient can meet the daily detection needs of small organizations or enterprises with an acceptable performance overhead.

\subsection{Ablation Study}

% \begin{figure}[h]
%     \vspace{-0.05in}
%     \centering
%     \includegraphics[width=\linewidth]{fig/Ablation.pdf}
%     \caption{Ablation study.}
%     \label{fig:Ablation}
%     \vspace{-0.1in}
% \end{figure}
\begin{figure}[htbp]
    \vspace{-0.05in}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/ablation1.pdf}
        \caption{Detection Performance}
        \label{fig:Ablation1}
    \end{subfigure}
    % \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/ablation2.pdf}
        \caption{Performance Overhead}
        \label{fig:Ablation2}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Ablation study.}
    \label{fig:Ablation}
    \vspace{-0.1in}
\end{figure}


We conducted an ablation study of Sentient using the Cadets dataset to evaluate the importance of different components, as shown in Figure~\ref{fig:Ablation}.

\noindent \textbf{Graph Comprehension (GC)}: Removing Sentient's GC component, which combines node attribute and position encoding for scenario learning input, caused precision to drop by 41.9\%.

\noindent \textbf{Multiple Scenario Comprehension (MSC)}: Removing the Bi-Mamba2 model from MSC, which directly feeds scenario representation to the MLP for event reconstruction, led to a 46.5\% decrease in precision.

Despite the removal of either component, recall remained largely unaffected, indicating that both Global Learning and Scenario Learning are strong at anomaly detection. However, their combination effectively reduces false positives.


\subsection{Hyperparameter Impact on Performance}
In previous sections, we evaluated Sentient with a set of fixed hyperparameters. Here, we conduct experiments on the Cadets dataset, varying key parameters to assess their impact on Sentient's performance, as shown in Figure~\ref{fig:parameter}.

\noindent \textbf{Node Embedding Dimension}: Sentient maps structural and attribute information of nodes into embeddings during the global learning phase. The embedding dimension affects how well the embeddings represent node information. We explored different embedding dimensions, as shown in Figure~\ref{fig:parameter1}. Lower dimensions reduce expressive power, negatively impacting detection, while higher dimensions lead to sparse features. We found that 64 dimensions yielded optimal performance.

\noindent \textbf{Sequence Length}: Sequence length determines the receptive field of events. As shown in Figure~\ref{fig:parameter2}, longer sequences offer a broader receptive field but also increase performance overhead. Performance gains plateau beyond a length of 30. Considering both benefits and costs, we found 30 to be the optimal length. In practice, event sequence lengths rarely exceed 30.
% \begin{figure}[h]
%     \vspace{-0.05in}
%     \centering
%     \includegraphics[width=\linewidth]{fig/parameter.pdf}
%     \caption{Hyperparameter Impact on Performance.}
%     \label{fig:parameter}
%     \vspace{-0.1in}
% \end{figure}
\begin{figure}[htbp]
    \vspace{-0.05in}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/parameter1.pdf}
        \caption{Embedding Dimension}
        \label{fig:parameter1}
    \end{subfigure}
    % \hspace{0.01\textwidth}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\linewidth]{fig/parameter2.pdf}
        \caption{Sequence Length}
        \label{fig:parameter2}
    \end{subfigure}
    \vspace{-0.1in}
    \caption{Hyperparameter Impact on Performance.}
    \label{fig:parameter}
    \vspace{-0.1in}
\end{figure}

\subsection{Robustness against Mimicry Attacks}

To assess the resilience of Sentient against adversarial attacks, we used the method proposed by ~\cite{goyal2023sometimes} for Mimicry Attacks. This method involves adding more benign structures to malicious nodes in an attempt to conceal malicious behaviors with benign actions.

We evaluated the adversarial attack resilience of Sentient on the Cadets dataset by conducting multiple experiments with varying numbers of added benign structures. The experimental results are presented in Table~\ref{Adversarial}. The experiments demonstrate that mimicry attacks have almost no negative impact on Sentient's performance. In fact, these attacks slightly improve Sentient's detection rate (Recall). This is because adding too many benign structures leads to denser interactions between nodes, which raises suspicion. Furthermore, since Sentient combines both global and local contexts for detecting malicious intent, although mimicry attacks may improve the overall impression of a malicious node in most systems, they cannot hide the specific attack scenarios. As a result, Sentient is able to detect anomalous behaviors across diverse scenarios.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
    \vspace{-0.05in}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
     & Precision & Recall & F-score & FRP \\ \midrule
    None Attack & 95.88\% & 99.23\% & 97.53\% & 0.1\% \\
    Mimicry Attacks (10 Events) & 95.74\% & 99.27\% & 97.47\% & 0.1\% \\
    Mimicry Attacks (20 Events) & 95.43\% & 99.09\% & 97.23\% & 0.1\% \\
    Mimicry Attacks (30 Events) & 95.07\% & 99.68\% & 97.32\% & 0.1\% \\ \bottomrule
    \end{tabular}
    }
    \caption{Adversarial mimicry attack against our system with varying numbers of added Events.}\label{Adversarial}
    \vspace{-0.2in}
\end{table}

\section{Conclusion}
In this paper, we propose Sentient, an APT detection method that integrates global context learning with scenario-specific context learning. Sentient uses two self-supervised tasks for benign behavior learning and threat detection. It employs Segmentation provenance graphs to create multiple scenarios, reducing noise in detection contexts. Additionally, Sentient's Multiple Scenario Comprehension module captures long-range dependencies to preserve contextual information. We evaluate Sentient's performance on three widely used datasets, demonstrating superior detection performance.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

