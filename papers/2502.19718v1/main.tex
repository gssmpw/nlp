
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\input{my_instructions.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}


\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\newtheorem{innercustomdef}{Definition}
\newenvironment{customdef}[1]
  {\renewcommand\theinnercustomdef{#1}\innercustomdef}
  {\endinnercustomdef}

\newtheorem{innercustomassum}{Assumption}
\newenvironment{customassum}[1]
  {\renewcommand\theinnercustomassum{#1}\innercustomassum}
  {\endinnercustomassum}

\newtheorem{innercustomCorollary}{Corollary}
\newenvironment{customCorollary}[1]
  {\renewcommand\theinnercustomCorollary{#1}\innercustomCorollary}
  {\endinnercustomCorollary}


\title{Learning Mask Invariant Mutual Information for Masked Image Modeling}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Tao Huang$^1$\thanks{Equal contributions. $^\dagger$Corresponding author.}\quad Yanxiang Ma$^{1*}$ \quad Shan You$^2$ \quad Chang Xu$^{1\dagger}$\\
$^{1}$School of Computer Science, Faculty of Engineering, The University of Sydney\\
$^2$SenseTime Research\\
% \texttt{\{t.huang,xxxx,c.xu\}@sydney.edu.au\quad youshan@sensetime.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Masked autoencoders (MAEs) represent a prominent self-supervised learning paradigm in computer vision. Despite their empirical success, the underlying mechanisms of MAEs remain insufficiently understood. Recent studies have attempted to elucidate the functioning of MAEs through contrastive learning and feature representation analysis, yet these approaches often provide only implicit insights. In this paper, we propose a new perspective for understanding MAEs by leveraging the information bottleneck principle in information theory. Our theoretical analyses reveal that optimizing the latent features to balance relevant and irrelevant information is key to improving MAE performance. Building upon our proofs, we introduce MI-MAE, a novel method that optimizes MAEs through mutual information maximization and minimization. By enhancing latent features to retain maximal relevant information between them and the output, and minimizing irrelevant information between them and the input, our approach achieves better performance. Extensive experiments on standard benchmarks show that MI-MAE significantly outperforms MAE models in tasks such as image classification, object detection, and semantic segmentation. Our findings validate the theoretical framework and highlight the practical advantages of applying the information bottleneck principle to MAEs, offering deeper insights for developing more powerful self-supervised learning models.
\end{abstract}

\section{Introduction}

Masked autoencoders (MAEs) \citep{he2022masked,xie2022simmim,bao2022beit} have emerged as a powerful self-supervised learning paradigm, particularly in the realm of computer vision. Inspired by the success of masked language models like BERT~\citep{devlin2019bert} in natural language processing, MAEs leverage a similar masking and reconstruction strategy to learn meaningful visual representations. The fundamental concept involves masking a portion of the input image and training a model to predict the missing parts, thereby enabling the model to capture the underlying structure and semantics of the visual data. This approach has proven effective in numerous applications \citep{li2022exploring,kirillov2023segment,tong2022videomae,fang2023eva}, showcasing the potential of MAEs to learn robust and generalizable features from unlabeled data.

Despite their success, the understanding of how MAEs function and why they perform well remains an open question. Recent research has sought to demystify the inner workings of MAEs, providing valuable insights into their operation. Several studies have approached this task from various perspectives, including contrastive learning \citep{zhang2022mask,kong2023understanding,huang2023contrastive} and feature representation analysis \citep{xie2023revealing,pan2023towards}. Specifically, \citep{kong2023understanding} proposed that MAEs inherently learn occlusion-invariant features by treating masked patches as a form of data augmentation. This approach, also suggested by \citep{zhang2022mask,yue2023understanding}, aligns MAEs with contrastive learning frameworks, where the models learn to align features between different masked views of the same image. Other studies such as \citep{xie2023revealing,pan2023towards} analysed the latent feature representations learned by MAEs to understand how these models capture and organize visual information.  However, these efforts often provide only implicit insights and do not fully address the need for a comprehensive and systematic understanding of the learning objectives and framework of MAEs.

In this paper, we propose a new perspective for understanding MAEs by leveraging information theory, specifically the information bottleneck (IB) principle \citep{IB_NN}. This perspective provides a systematic and comprehensive framework for optimizing MAEs, offering theoretical insights that can guide the development of more effective models. The IB principle posits that any deep neural network can be understood as a system that balances the trade-off between retaining relevant information and compressing irrelevant information. By applying this principle to MAEs, we aim to provide a more robust understanding of their mechanisms and to identify key areas for improvement.

Based on our findings, we introduce a novel masked image modeling method, dubbed MI-MAE, to learn Mask Invariant Mutual Information for MAEs through the lens of the information bottleneck theory. Our method systematically optimizes the latent features produced by the encoder, ensuring that they contain maximal relevant information and minimal irrelevant information on the information bottleneck of MAE. Concretely, we introduce two aspects of mutual information based losses on the latent feature: (1) Mutual information maximization. To optimize the autoencoder in the latent space, we derive a loss to maximize the mutual information between the latent features of multiple orthogonal masks\footnote{Here, ``orthogonal'' means the inner productions between each mask are 0, which means we are completely dividing the image into visible parts of multiple masks.}. (2) Mutual information minimization. We optimize an upper bound of mutual information between the input and latent space to minimize the irrelevant information in the latent features and thus maximize the capacity of relevant information. This comprehensive optimization strategy helps in achieving better feature representations and improved performance.

We conduct a series of evaluations on standard benchmarks, showing that our method performs significant improvements over MAE in various tasks, including image classification, object detection, and semantic segmentation. For example, our 400-epoch model achieves $83.9\%$ accuracy on ImageNet-1K, surpassing the $1600$-epoch MAE by 0.5\%. The experimental results validate our theoretical findings and highlight the practical benefits of applying the information bottleneck principle to masked autoencoders. Additionally, by providing a new perspective and a rigorous analytical framework, our work paves the way for future research in this area, offering insights that can drive the development of even more powerful self-supervised learning models.

\section{Related Works}

\textbf{Contrastive learning.} Contrastive learning \citep{chen2020simple,he2020momentum,chen2021exploring,grill2020bootstrap,caron2021emerging} stands out as the leading self-supervised representation learning approach in computer vision, achieving invariance by comparing different augmentations of the same image. A notable example is SimCLR \citep{chen2020simple}, which enhances semantic representations by increasing the similarity between various views of the same image in the latent space. MoCo v3 \citep{chen2021empirical} applies contrastive learning techniques to pre-train vision transformers. DINO \citep{caron2021emerging} delves into novel properties of self-supervised vision transformers.

\textbf{Masked image modeling.} Masked image modeling (MIM) has gained significant traction in the field of computer vision as an effective self-supervised learning paradigm. Recently, with the widespread use of vision transformers (ViTs) \citep{dosovitskiy2021an,liu2021swin}, a series of notable methods such as BEiT~\citep{bao2022beit}, MAE~\citep{he2022masked}, and SimMIM~\citep{xie2022simmim} have been proposed to pre-train ViTs following the BERT-style masked modeling paradigm used in natural language processing (NLP) \citep{devlin2019bert,liu2019roberta}. Many follow-up works extend masked pre-training by exploring data augmentations \citep{chen2023mixed,fang2023corrupted}, mask strategies \citep{li2022semmae,wang2023hard,wang2024droppos}, and hierarchical structures \citep{xie2022simmim,huang2022green,woo2023convnext}. Additionally, there is growing interest in understanding MAE and its connection with contrastive learning \citep{zhang2022mask,xie2023revealing,huang2023contrastive,kong2023understanding,pan2023towards}. In this paper, we further investigate MAE from an information bottleneck perspective.

\textbf{Information bottleneck.} Under information theory, any closed system can be quantified by the mutual information between bottleneck and output variables \citep{IB_BASE}. A DNN with a given input can be considered as a closed system that introduces no other information. During the forward propagation, the complexity of the intermediate variables usually decreases in a general prediction model, as does the amount of information they contain. It is possible to measure the goodness of each layer and even the whole prediction network by the mutual information that can be used between the intermediate variables or the outputs and the network's prediction target \citep{IB_NN}.

\section{Preliminaries}
\label{sec:pre}

Masked Autoencoders (MAEs) \citep{he2022masked} are a type of self-supervised model designed to reconstruct masked patches in images. These autoencoders consist of two main components: an encoder, which encodes the image into latent features, and a decoder, which predicts the masked patches. During training, each input image is first embedded into a feature representation $X$, which is divided into multiple patches. A random mask $m$ is then generated to select a subset of these patches as visible patches. The visible patches, represented as $X\cdot(1-m)$, are concatenated with a learnable class token and fed into a Vision Transformer (ViT) encoder to obtain the latent feature $z$.

Subsequently, the latent feature $z$ is concatenated with a set of learnable mask tokens representing the masked patches and passed into a decoder to predict the original unmasked patches $\hat{X}$. Finally, a linear projector is used to generate the reconstructed image $\phi(\hat{X})$. The loss function for training MAEs is based on the reconstruction errors between the original masked pixels and the predicted masked pixels:
\begin{equation}
    \mathcal{L}_\mathrm{rec} = ||o(X\cdot m) - \phi(\hat{X}\cdot m)||_2^2,
    \label{eq:LMAE}
\end{equation}
where $o(X\cdot m)$ denotes the original pixel values of the image associated with the masked patches $X\cdot m$.

\textbf{Approaches in understanding MAEs.} Recent studies have provided several insights into the functioning of Masked Autoencoders (MAEs), with works mainly understanding MAEs from the perspective of contrastive learning. For instance, \citep{kong2023understanding} proposed that MAEs inherently learn occlusion-invariant features by treating masked patches as a form of data augmentation. This perspective aligns MAEs with contrastive learning frameworks, suggesting that MAEs implicitly align features between different masked views of the same image. Similarly, U-MAE~\citep{zhang2022mask} established a theoretical connection between MAEs and contrastive learning, showing that the reconstruction loss of MAEs aligns well with the alignment of mask-induced positive pairs, thereby enhancing feature uniformity and diversity. However, these papers only provide implicit analyses of MAEs by introducing additional views of the image to justify that the MAEs implicitly align with contrastive learning, and the introduced methods only perform on par with the original MAE. For a comprehensive understanding of MAEs, further analyses of the learning objective and autoencoder framework are needed.

Taking the above approaches into account, we conclude that optimizing the latent features produced by the encoder is crucial for improving MAEs. This motivates us to perform an in-depth analysis of MAEs and the latent features using information theory. In this paper, we provide a more comprehensive and theoretically sound analysis following the information bottleneck principle and show that the key to improving MAEs is maximizing relevant information while compressing irrelevant information in the latent space. Through our analysis with information theory, we find that the contrastive learning on latent space can help minimize the IB distortion

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/pipeline_all.pdf}
  \vspace{-4mm}
  \caption{Pipeline of MI-MAE for each mask $m_k$. We introduces two losses $l_{i,k}^{(\mathrm{max\_mi})}$ and $l_{i}^{(\mathrm{min\_mi})}$ on the latency to maximize the relevant information and minimize the irrelevant information respectively, and $\Ls_\mathrm{rec}$ denotes the original MAE loss. The top sequence in the figure denotes forward propagation and the bottom denotes back propagation. $m$ denotes mask and $X$ denotes the original map. $\gamma$ is the inverse function of a decoder, $\zeta$ is the output of the reduced target map of the MAE on $\gamma$, and $Z$ is defined as a latent feature on a small neighbourhood of $\zeta$, and their bias $\varepsilon_z$ is decided by $\epsilon_y$. $\nabla$ in backpropagation represents gradients, while $\nabla_h$ is the gradient in layer $h$ of the encoder.}
  \label{fig:gamma}
\end{figure}

\section{Method}
\label{sec:Method}

In Section~\ref{sec:Method_MAE_IB}, we introduce the information theorem to explain the workings of MAEs. The information bottleneck principle, as introduced by \citep{IB_NN}, is employed and customized for MAEs to elucidate the overall training objective via the information bottleneck framework. In Section~\ref{sec:Obj_w_Info}, we break down the overall objective and propose two new loss functions for MAEs, based on the assumptions considering the information bottleneck within MAEs.

\subsection{MAE with Information Bottleneck Principle}
\label{sec:Method_MAE_IB}

In information theory, deep neural networks suffer from information distortion as the information complexity of intermediate variables decreases \citep{IB_BASE, IB_NN}. 
\begin{customdef}{1}
    \label{definition:ID}
    Based on the definition of notation in Section~\ref{sec:pre}, the information distortion in MAEs is defined as
    \begin{equation}
        D_{I}=I(X \cdot (1-m);X \cdot m|\widehat{X \cdot m}),
    \end{equation}
    where $\widehat{X \cdot m}$ is the prediction and $I(\cdot;\cdot)$ denotes the mutual information between two variables.
\end{customdef}

The information distortion describes the portion of the mutual information between the masked image and the unmasked image that is not captured by the recovered image. For a given preset, training the neural network reduces the information distortion. The internal variable that captures all the mutual information between the masked image and the recovered image is called the effective description, with the one having the least information complexity referred to as the simplest effective description~\citep{IB_NN}.
For any given MAE and training data, the information distortion is determined by the simplest effective distortion, denoted by $\widetilde{X \cdot (1-m)}$. The simplest effective distortion is considered as the information bottleneck (IB) in MAEs. Thus, information distortion is limited by the IB as $D_{IB} = I(X \cdot (1-m);X \cdot m|\widetilde{X \cdot (1-m)})$. According to \citep{IB_BASE,IB_NN}, the goal of the MAE can be re-interpreted as minimizing a Lagrangian term that includes $D_{IB}$, formulated as:
\begin{equation}
    L[p(\hat{x}|x)] = I(X \cdot (1-m);\widetilde{X \cdot (1-m)})+\beta D_{IB}.
\label{eq:lag}
\end{equation}

In this Lagrangian term, the first sub-term represents the complexity of the simplest effective description of the samples, and the second sub-term represents the information distortion of the given network. It is challenging to precisely find $\widetilde{X \cdot (1-m)}$ by training MAE on a given data distribution \citep{IB_NN}. The MAE can only find a sub-optimal effective description in the neighborhood of $\widetilde{X \cdot (1-m)}$.

\begin{customthm}{2}
\label{thm:1}
Denote $\widetilde{X \cdot (1-m)} + r$ as a biased simplest effective description found through training, where $r$ is the bias. Let the predicted latent feature for the MAE be $\hat{z}$. The latent feature is the information bottleneck for the MAE, and thus $\hat{z} = \widetilde{X \cdot (1-m)} + r$. The mutual information $I(\widetilde{X \cdot (1-m)}; X \cdot m)$ can be upper bounded by a generalization bound as:
\begin{equation}
    I(\widetilde{X \cdot (1-m)}; X \cdot m) \leqslant \hat{I}(\hat{z}; X \cdot m) + O(\frac{K_x|Y|}{\sqrt{n_x}}) - I(\hat{z};X \cdot m|r),
    \label{eq:thm2}
\end{equation}
where $K_x = |\widetilde{X}|$ denotes the complexity of $\widetilde{X \cdot (1-m)}$, $n_x$ is the size of $X \cdot (1-m)$, and $\hat{I}$ is the empirical estimate of the mutual information from the given training set. 
\end{customthm}

The proof of Theorem~\ref{thm:1} is in Appendix~\ref{appendix:1}. From the upper bound, it can be seen that mitigating the bias on the information bottleneck helps in achieving better MAE performance. Unfortunately, optimizing the latent feature encounters the problem of difficulty in finding the optimal latent feature. In the following, we will analyse how to learn using a sub-optimal latent feature.

\subsection{Multiple Objectives with Information Theorem}
\label{sec:Obj_w_Info}

Define an optimal simplest effective description as $\zeta = \widetilde{X \cdot (1-m)}$. Maximizing the mutual information between the latent feature and $\zeta$ will help reduce $I(\hat{z}; X \cdot m|r)$. Unfortunately, it is challenging to find a precise $\zeta$ in the latent space for all samples. One approach is to use a sub-optimal latent feature.

\begin{customassum}{3}
\label{assumption:Lmae}
The MAE loss has been minimized on the given training set, i.e. $\Ls_\mathrm{rec} \leqslant \epsilon_l$. Denote $Y$ as the ground truth of the MAE, and $\hat{Y}$ as the prediction. An upper bound of the information distortion at the output layer can be found as:
\begin{equation}
H(Y|\hat{Y}) \leqslant \epsilon_Y,
\end{equation}
where $H(Y)$ is the information in $Y$, $\epsilon_l$ is a small constant, and $\epsilon_Y$ is determined by $\epsilon_l$.
\end{customassum}

Proof of the validity of Assumption \ref{assumption:Lmae} is shown in the Appendix. Under Assumption \ref{assumption:Lmae}, we can find a set of sub-optimal latent features for $X \cdot (1-m)$, defined as $Z$, as the target for the samples whose latent feature is similar to $Z$. Furthermore, $|Z - \zeta| \leqslant \epsilon_z$. This idea is similar to contrastive learning. For a set of samples with similar relevant information in the ground truth, the relevant information in the information bottleneck is also similar in the same MAE. To make such a set, we consider generating multiple masks for a certain image before training and keeping the masks invariant. Consider the following case: for a given image $X$, generate $N$ mutually orthogonal masks as a pre-selected set $M = \{m_1, m_2, ... , m_N\}$. For each generated mask $m_i$, the input is denoted $X_i=X \cdot (1-m_i)$. In particular, represent $X_0$ as the part of $X$ that is not included in any input, \ie, $X_0 = X-\sum\limits_{i=1}^n{X_i}$. Then the ground truth can be expressed as $ X \cdot m_i = \sum\limits^{j=0}_{n;j \neq i} X_j$. Let each mask $m_i$ correspond to an optimal latent feature $z_i = \zeta$, whose prediction is $\hat{z}_i$. Note that the prediction of the latent feature is the biased simplest effective description of $X_i$, and the optimal latent feature is the simplest effective description of $X_i$.

\begin{customCorollary}{4}
\label{thm:max_IB}
With Assumption \ref{assumption:Lmae} standing, there exists a certain mask $m_i$, whose predicted latent feature $\hat{z}_i = Z$. The mutual information between the prediction and the optimal latent feature for the other masks is
\begin{equation}
    I(\hat{z}_k;z_k) \leqslant l_i + I(\hat{z}_k;\hat{z}_i) - I(\hat{z}_i;X_0|z_k) - \sum\limits^{j=1}_{n;j \notin \{i,k\}} [I(\hat{z_i};X_j|z_k) - I(\hat{z_k};X_j|z_i)],
\end{equation}
where $l_i = I(\hat{z}_i;X_0) + \sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{z}_i;X_j)$. For a given $z_i$, $l_i$ is a fixed value. $I(\hat{z}_k;z_k)$ can be \textbf{maximized} only when the three following conditions are satisfied: (1). $I(\hat{z}_k;\hat{z}_i)$ is \textbf{maximized}; (2). $I(\hat{z}_i;X_0|z_k)$ is \textbf{minimized}; (3). for any $j$ that satisfies $j \in \mathbb{Z} \cap [1, N]$ and $j \notin \{i, k\}$, $\sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{z}_k;X_j|z_i)$ is \textbf{maximized}, where $\mathbb{Z}$ denotes the set of integers. 
\end{customCorollary}

From the first condition of the mutual information maximization in Corollary \ref{thm:max_IB}, we can adopt InfoNCE, a widely-used loss to maximize the lower bound of mutual information \citep{oord2018representation}, \ie,
\begin{equation}
    l^{(\mathrm{max\_mi})}_{i,k} = -\log\frac{\exp(\mathrm{sim}(\hat{z}_i, \hat{z}_k) / \tau)}{\sum_{c=1}^{NB}\mathbbm{1}_{[c\ne i]}\exp(\mathrm{sim}(\hat{z}_i, \hat{z}_c) / \tau)},
    \label{eq:L_contr}
\end{equation}
where $\mathrm{sim}(u, v) = u^\top v / \lVert u \rVert \lVert v \rVert$ denotes the cosine similarity between two feature vectors, $\mathbbm{1}{[c\ne i]}$ is an indicator function that evaluates to $1$ if and only if $c \ne i$, $B$ denotes the batch size, $NB$ is the total number of masked images with $N$ masks per image, and $\tau$ is a temperature factor. We set $\tau=0.07$ in all experiments. Therefore, the final MI maximization loss among all the image pairs is formulated as
\begin{equation}
    \mathcal{L}_\mathrm{max\_mi} = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{k=1}^{N}\mathbbm{1}_{[i\ne k]}l^{(\mathrm{max\_mi})}_{i,k}.
\end{equation}

Considering the first term in Eq. \ref{eq:lag}, we also need to minimize the mutual information between the latent feature and the masked image. Unlike $l^{(\mathrm{max\_mi})}_{i,k}$, the mutual information about $\hat{z}_j$ and $X_j$ cannot be represented by the cosine similarity, since they are not in the same feature space. Therefore, for the minimization of MI, we use the Mutual Information Neural Estimator (MINE) \citep{belghazi2018mutual} to represent the mutual information in KL divergence. This $I(\hat{z_k}; X_j)$ representation requires prior probability $p(\hat{z}_j | X_j)$. Since the prior probability is intractable, we follow \citep{kingma2013auto,cheng2020club} and use an approximation neural network to estimate the variational distribution of $p(\hat{z}_j | X_j)$, where the loss function for minimizing it is the negative log-likelihood between $z_i$ and $X_i$, \ie,
\begin{equation}
\Ls_\mathrm{approx} = \frac1N\sum_{j=1}^{N}-\log q_\theta(z_j | X_j),
\label{eq:L_approx}
\end{equation}
where $\theta$ is the parameters in the approximation network.
With the estimated posterior probability, we use the upper bound of MI presented in CLUB~\citep{cheng2020club} to minimize $I(\hat{z_k}; X_j)$, for all $j$, we aim to minimize:
\begin{equation}
\label{eq:l_approx_1}
    \Ls_\mathrm{min\_mi} = \frac1N\sum_{j=1}^N l_j^\mathrm{(min\_mi)}
    \quad \text{with\ } l_j^\mathrm{(min\_mi)} = \log q_\theta(\hat{z}_j | X_j) - \frac1N\sum_{k=1}^N\log q_\theta(\hat{z}_k | X_j).
\end{equation}
Detailed derivations for the upper bound can be found in Appendix \ref{sec:appendix_ub_mi}. Additionally, we find that by optimizing Eq. \ref{eq:l_approx_1}, the third condition in Corollary \ref{thm:max_IB} is also satisfied.

\begin{algorithm}[t]
\caption{Self-supervised pre-training with MI-MAE. \graytext{Our changes to MAE are marked with *.}}
\label{alg:mi_mae}
\begin{algorithmic}[1]
    \REQUIRE Encoder $\mathbb{E}$, decoder $\mathbb{D}$, variational distribution approximation network $\mathbb{V}$ with parameters $\theta$, training dataset $\mathcal{D}_{tr}$, number of masks per image $N$.
    \FOR{iteration in total\_iterations}
        \STATE $X \leftarrow \mathcal{D}_{tr}$; \hfill\graytext{\textit{\# Sample a batch of images from training set}}
        \STATE * Generate $N$ orthogonal masks $M = \{m_1, m_2, ..., m_N\}$ for each image;
        \STATE Encode the masked images, $\hat{z}_i \leftarrow \mathbb{E}(X \cdot (1 - m_i)), \forall 1\le i \le N$;
        \STATE Decode the latents, $\hat{Y} \leftarrow \mathbb{D}(\hat{z}_i)$;
        \STATE * Predict variational distribution $q_\theta(\hat{z}|X) \leftarrow \mathcal{N}(\hat{z}; \mu(X; \theta), \sigma(X; \theta));$
        \STATE Compute $\Ls_\mathrm{rec}$, $\Ls_\mathrm{max\_mi}$, and $\Ls_\mathrm{min\_mi}$ with $X, M, \hat{z}, q_\theta(\hat{z}|X)$;
        \STATE * Optimize encoder $\mathbb{E}$ with $\nabla(\lambda_1\Ls_\mathrm{rec} + \lambda_2\Ls_\mathrm{max\_mi} + \lambda_3\Ls_\mathrm{min\_mi})$ (Eq. \ref{eq:final_loss});
        \STATE Optimize decoder $\mathbb{D}$ with $\nabla\lambda_1\Ls_\mathrm{rec}$;
        \STATE * Optimize approximation network $\mathbb{V}$ with $\nabla\mathcal{L}_\mathrm{approx}$;
    \ENDFOR
    \ENSURE Trained encoder $\mathbb{E}$ and decoder $\mathbb{D}$.
\end{algorithmic} 
\end{algorithm}


In addition, Assumption \ref{assumption:Lmae} needs the MAE loss to be limited to a small value. Thus, we should also add the original MAE loss as a part of the training loss. Considering Assumption \ref{assumption:Lmae}, with both parts of the Lagrangian term minimized, our final loss becomes
\begin{equation}
\Ls_\text{MI-MAE} = \lambda_1\Ls_\mathrm{rec} + \lambda_2\Ls_\mathrm{max\_mi} + \lambda_3\Ls_\mathrm{min\_mi},
\label{eq:final_loss}
\end{equation}
where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are hyper-parameters for balancing the loss terms. Considering all the losses, the pipeline of our MI-MAE is shown in Fig. \ref{fig:gamma}. To be more specific, for the major MAE, we add $\Ls_\mathrm{min\_mi}$ and $\Ls_\mathrm{max\_mi}$ to the latent features after the forward propagation of the encoder. Before $\Ls_\mathrm{min\_mi}$ is calculated, the approximation network is deployed to get the posterior probabilities $q_\theta(\hat{z}_j | X_j)$ and $q_\theta(\hat{z}_k | X_j)$. Note that the gradient of $\Ls_\mathrm{min\_mi}$ and $\Ls_\mathrm{max\_mi}$ will only influence the encoder in back propagation. After that, the decoder will recover the latent feature to the image space. $\Ls_\mathrm{rec}$ will then be used to influence the whole MAE in back propagation. The training process of our model is illustrated in Algorithm~\ref{alg:mi_mae}. Specifically, the size of $X_0$ in our method is set to $0$ to satisfy the second condition in Corollary \ref{thm:max_IB}.

\input{tables/main_exp.tex}

\section{Experiments}

\subsection{Experiment Setup}

To sufficiently validate the efficacy of our method, we conducted a series of experiments on image classification, object detection, and semantic segmentation tasks.

\textbf{Image classification.} Our method is developed based on the official code of MAE~\citep{he2022masked}. We strictly adhere to the original pre-training and fine-tuning settings on ImageNet-1K~\citep{russakovsky2015imagenet}.

\begin{itemize}
    \item \textbf{Pre-training.} Since our method samples four masks for every image, for fair comparisons, we reduce our training epochs to one-quarter of the epochs used in the compared models. Specifically, we pre-train the models using an AdamW optimizer~\citep{loshchilov2018decoupled} with $\beta_1=0.9$, $\beta_2=0.95$, and a weight decay of $0.05$. The total batch size is $1024$ (equivalent to $4096$ as we augment each image with 4 masks inside the model). We use a cosine decay learning rate schedule with a 10-epoch warmup and a base learning rate of $1.5\times10^{-4}$. For the hyper-parameters introduced by our MI-MAE, we set $\lambda_1=\lambda_2=1$ and $\lambda_3=10$. For Assumption~\ref{assumption:Lmae}, $\epsilon_l$ is set to $0.5$. This means that we only use $\Ls_\mathrm{rec}$ before $\Ls_\mathrm{rec}$ is less than $0.5$, and the entire loss is used after $\Ls_\mathrm{rec}$ meets the assumption. We set $N=4$, which means four orthogonal masks are generated in each iteration for each image with a masking ratio of $0.75$. We also conduct experiments on SimMIM~\citep{xie2022simmim} architecture with a masking ratio of $0.5$, and the $N$ is set to $2$ accordingly.
    \item \textbf{Fine-tuning.} The base learning rate for fine-tuning is set to $1\times10^{-3}$. We warm up the learning rate for five epochs and train the models for a total of 100 epochs with an overall batch size of $1024$. Stronger augmentations and regularization such as RandAugment~\citep{cubuk2020randaugment}, label smoothing~\citep{szegedy2016rethinking}, and mixup~\citep{zhang2018mixup} are adopted.
    \item \textbf{Linear probing.} For linear probing, we use the pre-trained and fixed feature of the class token to learn a linear predictor. We use the LARS optimizer~\citep{you2017large} with a base learning rate of $0.1$ and a batch size of $16384$. The weight decay is set to $0$. We train the linear probing for 90 epochs with a 10-epoch warmup. All our experiments use NVIDIA V100 GPUs.
\end{itemize}

\textbf{Object detection.} We transfer the pre-trained ViT models to COCO~\citep{lin2014microsoft} dataset. We adopt Mask R-CNN framework~\citep{he2017mask}, which predicts detections and instance segmentations simultaneously. We follow the model setup and training strategy used in ViTDet~\citep{li2022exploring}.

\textbf{Semantic segmentation.} We conduct semantic segmentation experiments on the ADE20K~\citep{zhou2017scene} dataset, using the same settings as in MAE~\citep{he2022masked}. Specifically, we fine-tune UperNet~\citep{xiao2018unified} for 160k iterations with a batch size of $16$.

\subsection{Major Results}

\textbf{Image classification.} We conduct pre-training on the ImageNet dataset and report the fine-tuning, linear probing, and low-shot (1\% samples) fine-tuning accuracies in Tab. \ref{tab:exp_main_imgnet}. After pre-training ViT-B for 200 epochs (equivalent to 800 epochs of MAE), our method achieves significant improvements of 0.6, 2.3, and 2.8 percentage points on fine-tuning, linear probing, and low-shot fine-tuning, respectively. Compared to the optimal 1600-epoch MAE, our method still surpasses it by 0.5, 1.3, and 1.2 percentage points, obtaining an outstanding final fine-tuning accuracy of 84.1\%. We also implement our method on SimMIM~\citep{xie2022simmim} framework, another typical masked image modeling method with hierarchical Swin model \citep{liu2021swin}. The results show that, our MI-SimMIM achieves 84.1\% accuracy, outperforming previous methods such as PixMIM and SimMIM.

To evaluate the transfer learning performance of our method, we apply our pre-trained 400-epoch model to downstream tasks on the COCO and ADE20K datasets.

\textbf{Object detection and instance segmentation.} Tab.~\ref{tab:exp_downstream} reports the bounding box AP and mask AP performance on COCO detection. Compared to MAE, our method achieves significant improvements of 0.8 and 0.6 in AP$^{\text{box}}$ and AP$^\text{mask}$, respectively. This validates our method's superiority in dense prediction tasks.

\textbf{Semantic segmentation.} We also report the performance on the ADE20K segmentation task in Tab.~\ref{tab:exp_downstream}. Our method achieves a notable improvement of 1.2 mIoU compared to MAE, demonstrating its superiority in discriminating semantic pixels.

\subsection{Ablation Study}

\input{tables/ab_exp.tex}

To investigate the contributions of each innovation in our method and aid in determining the design choices, we conduct ablation experiments on our method. All experiments are performed with ViT-B on ImageNet-1K. We pre-train the models for $50$ epochs, while for our MAE baseline, we train the model for $200$ epochs to match the number of mask samples in our methods. We compare the $100$-epoch fine-tuning accuracies of the models.

\textbf{Ablation on the proposed losses.} As reported in Tab.~\ref{tab:exp_ab} (a), we pre-train the models with different combinations of losses proposed in our method and obtain the following findings:\\
(1) Compared to the original MAE (a), maximizing the mutual information between the latent features of an image in (b) results in a $0.3$ percentage point improvement. \\
(2) Using $l_{i,k}^\mathrm{(max\_mi)}$ only in (c) results in a collapse of training, \ie, the autoencoder cannot reconstruct the image and has low linear probing accuracy, as the effective maximization of $I(z_i|X_i\cdot(1-m))$ requires a small reconstruction loss as per Assumption \ref{assumption:Lmae}. \\
(3) Further adding $l_i^\mathrm{(min\_mi)}$ in (d) reduces the accuracy of (b) by $0.1$ percentage points, since the approximation network needs to be trained by $\Ls_\mathrm{approx}$ to predict the correct conditional relation of $p(z_i|X_i\cdot(1-m))$. \\
(4) Optimizing the complete losses of mutual information minimization boosts the accuracy by $0.2$ percentage points in (e) and by $0.3$ percentage points in our complete method (f).

\textbf{Loss weights.} We conduct experiments to tune the loss weights $\lambda_2$ and $\lambda_3$ for $l_{i,k}^\mathrm{(max\_mi)}$ and $l_i^\mathrm{(min\_mi)}$, respectively. For the loss weight $\lambda_1$ of the original reconstruction loss, we keep it at $1$. As summarized in Tab.~\ref{tab:exp_ab} (b), the optimal result occurs when $\lambda_2=1$ and $\lambda_3=10$.

\textbf{Orthogonal masks \textit{vs.} independent masks.} In our method, we generate four orthogonal masks for an image, with each randomly covering $75\%$ of the pixels, and the remaining $25\%$ of the visible pixels from each mask do not overlap (\ie, when combined, these masked images reveal the entire original image). We compare the results of the orthogonal masks and the independently generated masks. As summarized in Tab.~\ref{tab:exp_ab} (c), the pre-training with independently generated masks drops the accuracy by $0.2\%$, suggesting that complete mutual information learning of all the image patches helps MI-MAE to learn better representations. This phenomenon makes sense since in the independent case, there is no guarantee that $X_0$ is $0$. Thus, the second condition in Corollary~\ref{thm:max_IB} worsens compared to the orthogonal case.

\begin{wrapfigure}{r}{0.5\linewidth}
    \vskip -0.18in
    \includegraphics[width=\linewidth]{fig/ab_mask_ratio.pdf}
    \vspace{-8mm}
    \caption{Ablation of masking ratios.}
    \label{fig:ab_mask_ratio}
    \vspace{-6mm}
\end{wrapfigure}
\textbf{Masking ratios.} We design experiments to explore the influences of masking ratios. Unlike the original MAE, which generates one mask for each image, our MI-MAE generates $4$ masks for each image with a masking ratio of $0.75$. For other masking ratios, we design two masking strategies: (1) Complete masking: the number of orthogonal masks is determined by $\min(1/(1-\mathrm{ratio}), 2)$. For the number $N$ lower than $2$, we set it to $2$ to utilize our losses. This ensures every iteration processes all the patches of each image. (2) Fixed $4$ masks: all ratios use the same $4$ masks. The training epochs are adjusted to match the same number of mask samples.

As summarized in Fig.~\ref{fig:ab_mask_ratio}, we observe optimal performance with a masking ratio of $0.75$. For other ratios, complete masking shows superiority at a ratio of $0.9$, while for smaller ratios, the fixed $4$ masks strategy gains advancements. The difference between these two strategies is that complete masking generates $10$ masks at a $0.9$ ratio, while only $2$ masks at ratios $0.1\sim0.5$. This indicates that more positive samples in MI maximization loss $l_{i,k}^\mathrm{(max_mi)}$ leads to better performance. We also investigate the selected $0.75$ ratio with $6$ and $8$ masks, finding minor differences in the results (82.9\%, 82.7\%). This is reasonable as we find the $l_j^\mathrm{(min\_mi)}$ of all the numbers are close to $0$ and the third condition of Corollary~\ref{thm:max_IB} is well satisfied for each $N$. This means that the third condition of Corollary~\ref{thm:max_IB} only relies on $I(\hat{z}_k;X_j|z_i)$, which is  already considered in $l_j^\mathrm{(min\_mi)}$. For simplicity, we keep our complete masking strategy. 

We also find that the reconstruction loss does not change much, while $l_j^\mathrm{(min\_mi)}$ remains at a low level. This means $\epsilon_l$ in Assumption~\ref{assumption:Lmae} can always be satisfied as observed in Appendix~\ref{appendix:2}.

\begin{wraptable}{r}{0.5\linewidth}
    \vskip -0.3in
    \centering
    \caption{ImageNet results on different ViTs.}
    \label{tab:ab_vit_s}
    \footnotesize
    \renewcommand\arraystretch{1}
    \renewcommand{\tabcolsep}{2mm}
    \begin{tabular}{cccc}
        Model & Method & FT & LIN \\
        \midrule
        \multirow{2}*{ViT-B} & MAE & 83.3 & 65.6 \\
        ~ & MI-MAE & 83.9 & 67.9\\
        \midrule
        \multirow{2}*{ViT-S} & MAE & 78.4 & 50.5 \\
        ~ & MI-MAE & 79.8 & 53.1\\
    \end{tabular}
    \vspace{-4mm}
\end{wraptable}
\textbf{Pre-train with ViT-S.} To validate our efficacy on other model size, we perform pre-training on a smaller ViT-S model. As summarized in Tab.~\ref{tab:ab_vit_s}, with ViT-S, our MI-MAE obtains 79.8\% accuracy on ImageNet, which still obviously outperforms MAE by 1.4\%. This further demonstrate the effectiveness of our method.

\subsection{Architecture of Approximation Network}

Following CLUB~\citep{cheng2020club}, we design an approximation network to estimate the posterior distribution of $p(\hat{z}_j|X_j)$. Similar to VAE works \citep{kingma2013auto,pu2016variational}, the approximation network has two branches to predict the mean $\mu(X_j)$ and variance $\sigma(X_j)$ of the Gaussian distribution, respectively. Then, the approximation is determined by $q_\theta(\hat{z}_j|X_j) = \mathcal{N}(\hat{z}_j; \mu(X_j), \sigma(X_j))$.

We use simple architectures in these two branches to predict the mean and variance. For the $\mu(X_j)$ branch, taking $X_j$ as input, we adopt a multi-layer perceptron (MLP) consisting of two fully-connected layers and an intermediate GELU activation function to encode the feature, then predict $\mu(X_j)$ by another fully-connected layer and a LeakyReLU activation. Similarly, the $\sigma(X_j)$ branch has a similar architecture but with the last activation LeakyReLU replaced by ReLU. We train the approximation network simultaneously with the autoencoder as in Algorithm~\ref{alg:mi_mae}.

\textbf{Note on the computation cost.} The approximation network is light-weight and is optimized by reusing the input $X$ and latent feature $\hat{z}$ obtained in autoencoder training. Hence, we did not observe noticeable increment in the training time.

\section{Conclusion}

In this paper, we introduced a new perspective for understanding and improving masked autoencoders (MAEs) by leveraging the information bottleneck (IB) theory. Building on these insights, we proposed MI-MAE, a novel method that enhances MAEs through mutual information maximization and minimization losses on the latent features. We conducted extensive theoretical and empirical analyses of our method, and experiments on tasks such as image classification, object detection, and semantic segmentation demonstrated its effectiveness. Our findings validate the theoretical framework and highlight the practical advantages of applying the information bottleneck principle to MAEs, providing deeper insights for developing more powerful self-supervised learning models. Future research could build on our findings to further explore and enhance the capabilities of MAEs, potentially leading to new advancements in self-supervised learning and computer vision.

\section*{Acknowledgements}
This work was supported in part by the Australian Research Council under Projects DP240101848 and FT230100549.

\bibliography{main}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}

\subsection{Proof of Theorem \ref{thm:1}}
\label{appendix:1}

\subsubsection{Proof of How Equation 4 Influence the Lagrangian Term}

From Eq.~\ref{eq:lag}, to minimize the Lagrangian term, $I(X\cdot(1-m);\widetilde{X\cdot(1-m)})$ should be minimized. Since $I(\widetilde{X \cdot (1-m)};X\cdot(1-m))=I(X\cdot m;X\cdot(1-m))-(H(X\cdot m)-I(\widetilde{X\cdot(1-m)};X\cdot m))$. As $I(X\cdot m;X\cdot(1-m))$ is determined only by the data, we minimize $I(\widetilde{X\cdot(1-m)};X\cdot m))$ which is the left-hand side of Eq.~\ref{eq:thm2}.

\subsubsection{Proof of Bias in the Estimation of Mutual Information}

According to the IB principle \citep{IB_NN}, where $I(\hat{X}; Y) \le \hat{I}(\hat{X}; Y) + O(\frac{K|y|}{\sqrt{n}})$, we derive the mutual information bound for the decoder of MAE. In this case, the decoder takes the latent feature $\widetilde{X \cdot (1-m)}$ as the input and $X \cdot m$ as the output, so a generalization bound for the mutual information between the simplest effective description and the output as,
\begin{equation}
    I(\widetilde{X \cdot (1-m)}; X \cdot m) \leqslant \hat{I}(\widetilde{X \cdot (1-m)}; X \cdot m) + O(\frac{K_x|Y|}{\sqrt{n_x}}).
\end{equation}

However, it is hard to find the precise simplest effective description $\widetilde{X \cdot (1-m)}$ due to the following reasons: (1) The computation of $\widetilde{X \cdot (1-m)}$ is based on empirical data, which is influenced by sample size and distribution. This introduces biases and approximations into the optimization process. (2) The prediction of $\widetilde{X \cdot (1-m)}$ is constrained by the model capacity of the encoder-decoder structure, which limits its ability to fully capture the optimal representation. As a result, in limited data distribution and model capacity, we can only find empirical estimation of biased simplest effective description as $\hat{I}(\hat{z}, ; X \cdot m)$, where $\hat{z} = \widetilde{X \cdot (1-m)} + r$ and $r$ is the bias term.

We now provide formal proof to the existence of bias $r$. The mutual information between the optimal effective description $\widetilde{X\cdot (1-m)}$ and the observed unmasked data $X\cdot m$ is defined as:
\begin{equation}
    I(\widetilde{X\cdot (1-m)}; X \cdot m) = H(\widetilde{X\cdot (1-m)}) - H(\widetilde{X\cdot (1-m)}|X\cdot m),
\end{equation}
where $H$ is the entropy function. For the predicted latent feature $\hat{z}$, the empirical estimation of the mutual information is 
\begin{equation}
\hat{I}(\hat{z}; X \cdot m)=\hat{H}(\hat{z})-\hat{H}(\hat{z}|X\cdot m), 
\end{equation}
where $\hat{H}$ and $\hat{H}(\cdot|\cdot)$ are empirical estimates of entropy and conditional entropy, respectively.

We compare the true and empirical mutual information as
\begin{align}
    \begin{split}
        \Delta_\mathrm{MI} &:= \mathbb{E}[\hat{I}(\hat{z};X\cdot m)]-I(\widetilde{X\cdot (1-m)};X\cdot m)\\
        &= [\mathbb{E}[\hat{H}(\hat{z})]-H(\widetilde{X\cdot (1-m)})]-[\mathbb{E}[\hat{H}(\hat{z}|X\cdot m)]-H(\widetilde{X\cdot (1-m)}|X\cdot m)].
    \end{split}
\end{align}
As a result, the bias $r$ arises from two components:
\begin{enumerate}
    \item Bias in the entropy term: $\mathbb{E}[\hat{H}(\hat{z})]-H(\widetilde{X\cdot (1-m)})$.
    \item Bias in the conditional entropy term: $\mathbb{E}[\hat{H}(\hat{z}|X\cdot m)]-H(\widetilde{X\cdot (1-m)}|X\cdot m)$.
\end{enumerate}

From the paper on entropy estimation (e.g., \citealp{paninski2003estimation}; \citealp{belghazi2018mutual}), we have 
\begin{equation}
    \mathbb{E}[\hat{H}(\hat{z})]-H(\widetilde{X\cdot (1-m)})=O\left(\frac{K_z}{\sqrt{n_z}}\right),
\end{equation}
where $K_z$ is the complexity of the latent space representation $\hat{z}$, and $n$ is the number of training samples.
Similarly, for the conditional entropy term, we have
\begin{equation}
\mathbb{E}[\hat{H}(\hat{z}|X\cdot m)]-H(\widetilde{X\cdot (1-m)}|X\cdot m)=O\left(\frac{K_z}{\sqrt{n_z}}\right).
\end{equation}

Combining the above results, the total bias in the estimation of mutual information can be bounded as $O\left(\frac{K_z}{\sqrt{n_z}}\right)$, which can prove that there indeed exists a bias.


\subsubsection{Proof of Generalization Bound via Biased Bottleneck}

Unlike general discriminative networks, the target effective dimension of MAE is usually considered higher than the input effective dimension. Thus the bottleneck should be in the middle. The relevant information contained in the intermediate variables of the decoder is derived from the latent feature extracted by the encoder, despite the increment in the complexity. Intuitively, the latent feature is the information bottleneck in MAE. To further analyse the changes in effective information, we analyse the encoder and decoder of MAE separately.

Denote by $z$ the latent feature of the latent space, whose simplest effective description is denoted by $\hat{z}$. For the decoder, $z$ is the input and $(X \cdot m)$ is the ground truth. Denote the empirical estimation of mutual information by $\hat{I}(\cdot)$, and according to section \ref{sec:Method_MAE_IB}, the upper bound on the input-ground truth mutual information is,

\begin{equation}
    I(\hat{z}; X \cdot m) \leqslant \hat{I}(\hat{z}; X \cdot m) + O(\frac{K_z|X|}{\sqrt{n_z}}),
    \label{eq:IB_def}
\end{equation}

where $n_z$ is the size of the output, and $K_z$ is the complexity of the simplest effective description. In Eq. \ref{eq:IB_def}, the term $O(\frac{K_z|X|}{\sqrt{n_z}})$ describes the IB distortion, and goes worse with $K$. 
Since $I(\widetilde{X \cdot (1-m)}; X \cdot m)=I(\hat{z}-r;X \cdot m) = I(\hat{z};X \cdot m) - I(\hat{z};X \cdot m|r)$. We can expand the left-hand side of the formula ~\ref{eq:IB_def} to get an upper bound of $I(\widetilde{X \cdot (1-m)}; X \cdot m)$ as
\begin{equation}
    I(\widetilde{X \cdot (1-m)}; X \cdot m) \leqslant \hat{I}(\hat{z}; X \cdot m) + O(\frac{K_x|Y|}{\sqrt{n_x}}) - I(\hat{z};X \cdot m|r),
\end{equation}

It can be seen that this upper bound is better as the size of $z$ decreases. The decoder needs $z$ to capture as much information as possible, while the size of $z$ has to stay on the smallest possible scale. Thus, for decoder, the optimal case is that the simplest effective description of the hidden layer is the latent feature itself. Using the internal variables in encoder to take the place of latent feature, the decoder will suffer a worse generalization bound by a worse $O(\frac{K_z|X|}{\sqrt{n_z}})$.

\subsubsection{Proof of Biased Generalization Bound}

This means that for a given masked image, there exists an unknown but optimal latent feature, denoted by $\zeta$. Assume that there exists an ideal mapping process from ground truth $X \cdot m$ to the optimal solution of the hidden layer as $\gamma(\cdot)$, the decoder, which takes latent feature as the input and approximates $X \cdot m$, can be interpreted as the inverse process of $\gamma(\cdot)$ (see the bottom part of Fig.~\ref{fig:gamma}), such that on a well optimized MAE, we can get an approximated latent $Z \approx \gamma(X \cdot m)$ from the decoder and $X\cdot m$, which satisfies
\begin{equation}\label{eq:zeta_Z}
    I(\zeta; X \cdot m) - I(Z; X \cdot m) = \epsilon_i,
\end{equation}
where $\epsilon_i$ is a small constant.

Meanwhile, using the IB theory, taking the $X \cdot (1-m)$ as the input of the encoder, whose simplest effective description is denoted by $\chi$, and $z$ as the predicted latent feature of the encoder, the generalization bound of the encoder is
\begin{equation}
I(\chi; \zeta) \leqslant \hat{I}(\chi; \zeta) + O(\frac{K_x|\zeta|}{\sqrt{n_x}}),
\label{eq:X_z_limit}
\end{equation}
where $n_x$ denotes the size of the sample $X \cdot (1-m)$, and $K_x$ denotes the length of the simplest effective description. Thus, considering both Eq.~\ref{eq:zeta_Z} and Eq.~\ref{eq:X_z_limit} and using the internal variables in decoder to take the place of latent feature, the encoder will also suffer a worse generalization bound by a worse $O(\frac{K_x|\zeta|}{\sqrt{n_x}})$. In conclusion, the latent feature is the simplest effective description for the whole MAE.

\subsection{Proof of Assumption \ref{assumption:Lmae}}
\label{appendix:2}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/rec_loss_curve.pdf}
    \caption{The curve of reconstruction loss $\Ls_\mathrm{rec}$ during the $400$-epoch training of MI-MAE. We set $\epsilon_l$ to 0.5.}
    \label{fig:rec_loss_curve}
\end{figure}


Fig. \ref{fig:rec_loss_curve} shows that $\Ls_\mathrm{rec}$ satisfies Assumption \ref{assumption:Lmae}. This means the $L_2$ distance between $Y$ and $\hat{Y}$ can be limited. The mutual information between $Y$ and $\hat{Y}$ is negatively relevant to the $L_2$ distance between them. Assume that when $\Ls_\mathrm{rec} = \epsilon_l$, $I(Y;\hat{Y}) = \alpha$. When $\Ls_\mathrm{rec} \leqslant \epsilon_l$, $I(Y;\hat{Y}) \geqslant \alpha$. When $Y$ is given, $H(Y|\hat{Y})$ can be calculated from $I(Y;\hat{Y})$ as,
\begin{equation}
    H(Y|\hat{Y}) = H(Y) - I(Y; \hat{Y}).
\end{equation}
where $H(Y)$ is fixed. Since $I(Y; \hat{Y}) \geqslant \alpha$, for $\epsilon_Y = H(Y) - \alpha$, we have $H(Y|\hat{Y}) \leqslant \epsilon_Y$


\subsection{Proof of Corollary \ref{thm:max_IB}}

For a normal MAE consider $m_i \in M$, the mutual information between the prediction $\hat{Y}_i$ and the ground truth $Y_i$ can be denoted as $I(\hat{Y_i}; Y_i)$. We have,  
\begin{equation}
\begin{split}
&I(\hat{Y_i};Y_i)\\
    =& I(\hat{Y_i};X_0 + \sum\limits^{n}_{j=1;j \neq i}X_j)\\
\leqslant& I(\hat{Y_i};X_0) + \sum\limits^{n}_{j=1;j \neq i} I(\hat{Y_i};X_j)
\end{split}
\label{eq:Y_i}
\end{equation}

With Assumption \ref{assumption:Lmae}, we can easily find another sample whose input is $X \cdot (1-m_k)$ and output is $Y_k$, where $m_k \in M$ is orthogonal to $m_i$. THe mutual information between $\hat{Y}_k$ and $Y_k$ is,
\begin{equation}
\label{eq:Y_k}
    I(\hat{Y_k};Y_k) \leqslant \iota_k + I(\hat{Y}_k;X_i),
\end{equation}
where $\iota_k = I(\hat{Y_k};X_0) + \sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{Y_k};X_j)$.

The upper bound described in Eq. \ref{eq:Y_k} is approximate to the upper bound described in Eq. \ref{eq:Y_i}. 
The objectives of the other samples in $M$ differ from those of $i$ by less than $I(Y_k;X_i) - I(Y_i;X_k)$, which is also a small constant. Thus, for networks with the same initial state, the information about the intermediate variables in the network inference process is very close when sampling different masks in $M$. This shows the possibility of using latent feature of a neighbourhood as objective. 

Encouraged by Theorem \ref{thm:1}, we can write the similar mutual information on latent space. For and $k$ that satisfies $k \in \mathbb{Z} \cap [1, N]$ and $j \neq i$, the mutual information $I(\hat{z_k};z_k)$ is,
\begin{equation}
\begin{split}
    &I(\hat{z}_k;z_k)\\
    =& I(\hat{z}_k;\sum\limits^{j=0}_{n;j \notin \{i,k\}}X_j) + I(X_i;\hat{z}_k)\\
    \leqslant& I(\hat{z}_k;X_0) + \sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{z}_k;X_j) + I(\hat{z}_k;\hat{z}_k) - \epsilon_z + I(\hat{z}_k, X_i).\\
\end{split}
\label{eq:A31}
\end{equation}

Taking the known $z_i$ into Eq.\ref{eq:A31}, the upper bound can then be rewritten as,

\begin{equation}
\begin{split}
    &I(\hat{z}_k;z_k)\\
    \leqslant& [I(z_i, X_0) - I(\hat{z}_k, X_0|z_i)] + [\sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{z}_i;X_j) - \sum\limits^{j=1}_{n;j \notin \{i,k\}} I(\hat{z}_i;X_j|z_k)] + I(z_i;z_k).\\
    =& I(\hat{z}_k;z_k) \leqslant l_i + I(\hat{z}_k;\hat{z}_i) - I(\hat{z}_i;X_0|z_k) - \sum\limits^{j=1}_{n;j \notin \{i,k\}} [I(\hat{z_i};X_j|z_k) - I(\hat{z_k};X_j|z_i)].
\end{split}
\end{equation}

Rearranging the above equation gives the equation in the corollary. $l_i$ is decided only by $I(\hat{z}_i;z_i)$. $I(\hat{z}_k, X_0|z_i)]$ is decided only by $X_0$ when $z_i$ is given. So a small $H(X_0)$ can help get bigger $I(\hat{z}_k; z_k)$.
For any $j$ that satisfies $j \in \mathbb{Z} \cap [1, N]$ and $j \notin \{i,k\}$ $I(\hat{z}_i;X_j|z_k)]$, when $z_i$ is given, $I(\hat{z}_i;X_j)$ and $I(\hat{z}_i;X_j|z_k)$ is fixed, while $I(\hat{z}_k;X_j|z_i) \propto I(z_k;X_j) \propto I(z_k;z_j)$.
Thus, the three conditions about maximizing $I(\hat{z}_k;z_k)$ given in Corollary 6 holds.

\subsection{Derivations of the Upper Bound of MI} \label{sec:appendix_ub_mi}

To minimize the mutual information between predicted latency $X_j$ and its corresponding input $X_j$, we leverage an upper bound of MI defined in CLUB~\citep{cheng2020club} is (we will show the proof of this upper bound later):
\begin{equation}
    \label{eq:club}
    I(X_j, \hat{z}_j) \le \mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j | X_j)]-\mathbb{E}_{p(X_j)p(\hat{z}_j)}[\log p(\hat{z}_j | X_j)].
\end{equation}

However, the conditional relation $p(\hat{z}_j | X_j)$ is intractable. We can instead use a variational distribution $q_\theta(\hat{z}_j | X_j)$ parameterized by $\theta$ to approximate it. Consequently, the upper bound $\hat I(X_j, \hat{z}_j)$ of mutual information in Eq.~\ref{eq:club} becomes:
\begin{equation} \label{eq:upper_MI}
    \hat I(X_j, \hat{z}_j) := \mathbb{E}_{p(X_j, \hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)]-\mathbb{E}_{p(X_j)p(\hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)].
\end{equation}

Nevertheless, $\hat I(X_j, \hat{z}_j)$ in Equation~\ref{eq:upper_MI} no longer guarantees an upper bound of $I(X_j; \hat{z}_j)$ due to the variational approximation. Fortunately, we can prove that $\hat I(X_j, \hat{z}_j)$ can be a reliable upper bound estimator when the difference between $p(X_j, \hat{z}_j)$ and $q_\theta(X_j, \hat{z}_j)$ is small.

We first compare the difference between them as
\begin{equation} \label{eq:diff1}
    \Delta := I(X_j, \hat{z}_j) - \hat I(X_j, \hat{z}_j),
\end{equation}

With $H(X_j)$ being the entropy of variable $X_j$, and using the Mutual Information Neural Estimator (MINE) \citep{belghazi2018mutual}, we can rewrite mutual information $I(X_j, \hat{z}_j)$ as
\begin{align} \label{eq:mi_kl}
    \begin{split}
    I(X_j; \hat{z}_j) &:= H(X_j) - H(X_j|\hat{z}_j) \\
    &= \mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j | X_j) - \log p(\hat{z}_j)].
    \end{split}
\end{align}
Therefore, with $q_\theta(X_j, \hat{z}_j) = q_\theta(\hat{z}_j|X_j)p(X_j)$ being the variational joint distribution induced by $q_\theta(\hat{z}_j|X_j)$, Eq.~\ref{eq:diff1} can be reformulated by Eq. \ref{eq:mi_kl} and Eq. \ref{eq:upper_MI} as
\begin{align}
    \begin{split}
        \Delta :=& I(X_j, \hat{z}_j) - \hat I(X_j, \hat{z}_j) \\
        =& \mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j | X_j) - \log p(\hat{z}_j)] - \mathbb{E}_{p(X_j, \hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)] + \mathbb{E}_{p(X_j)p(\hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)] \\
        =& \mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j | X_j) - \log q_\theta(\hat{z}_j | X_j)] - \mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j)] + \mathbb{E}_{p(X_j)p(\hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)]\\
        =& \mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j)) - \mathrm{KL}(p(X_j)p(\hat{z}_j)||q_\theta(X_j, \hat{z}_j)).
    \end{split}
\end{align}

The above equation shows that, \textbf{(1)} when $\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j)) \le \mathrm{KL}(p(X_j)p(\hat{z}_j)||q_\theta(X_j, \hat{z}_j))$, we can directly get $I(X_j, \hat{z}_j) \le \hat I(X_j, \hat{z}_j)$, and $\hat I(X_j, \hat{z}_j)$ is already an upper bound of MI. \textbf{(2)} Otherwise, if  $\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j)) > \mathrm{KL}(p(X_j)p(\hat{z}_j)||q_\theta(X_j, \hat{z}_j))$, by learning a good variational approximation $q_\theta(X_j, \hat{z}_j)$ that closes to $p(X_j, \hat{z}_j)$, we have minimized $\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j)) < \epsilon_q$, then $|\hat I(X_j, \hat{z}_j) - I(X_j, \hat{z}_j)| < \epsilon_q$, $\hat I(X_j, \hat{z}_j)$ can become an MI estimator whose absolute error is bounded by the approximation performance $\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j))$.

\textbf{Derivation of $\Ls_\mathrm{approx}$ in Eq. \ref{eq:L_approx}.} We show that $\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j))$ can be minimized by minimizing the negative log-likelihood of $q_\theta(\hat{z}_j, X_j)$, because of the following equation:
\begin{align}\label{eq:min_kl}
    \begin{split}
        &\min_\theta\mathrm{KL}(p(X_j, \hat{z}_j)||q_\theta(X_j, \hat{z}_j)) \\ 
        =&\min_\theta \mathbb{E}_{p(X_j, \hat{z}_j)}[\log (p(\hat{z}_j | X_j)p(X_j)) - \log (q_\theta(\hat{z}_j | X_j)p(X_j))] \\
        =& \min_\theta\mathbb{E}_{p(X_j, \hat{z}_j)}[\log p(\hat{z}_j | X_j)] - \mathbb{E}_{p(X_j, \hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)].
    \end{split}
\end{align}
Eq. \ref{eq:min_kl} equals to maximizing the second term $\max_\theta\mathbb{E}_{p(X_j, \hat{z}_j)}[\log q_\theta(\hat{z}_j | X_j)$, as the first term has no relation to $\theta$, and hence the learning object $\Ls_\mathrm{approx}$ of $\theta$ is
\begin{equation}
    \Ls_\mathrm{approx} = \frac1N\sum_{j=1}^{N}-\log q_\theta(\hat{z}_j | X_j).
\end{equation}

\textbf{Derivation of $l^\mathrm{(min\_mi)}_j$ in Eq. \ref{eq:l_approx_1}.} The MI upper bound $\hat I(X_j; \hat{z}_j)$ has an unbiased estimation as
\begin{equation}
    \hat I(X_j, \hat{z}_j) = \log q_\theta(\hat{z}_j|X_j) - \frac1N\sum_{k=1}^N \log q_\theta(\hat{z}_k|X_j),
\end{equation}
which reflects our $l^\mathrm{(min\_mi)}_j$ in Eq. \ref{eq:l_approx_1}.


\subsection{Robustness Evaluation}
Compared to MAE, our method with explicit mutual information optimization based on information bottleneck, would have better robustness, as the IB helps guide latent features to suppress noise while retaining semantic information. To validate this, we test our fine-tuned model on two popular ImageNet variants for robustness evaluation, ImageNet-A~\citep{hendrycks2021nae} and ImageNet-C~\citep{hendrycks2019robustness}. As summarized in Tab.~\ref{tab:robustness}, we report the top-1 accuracy on ImageNet-1K and ImageNet-A, and mean corruption error (mCE, lower is better) on ImageNet-C. The results show that, our MI-MAE significantly improves the performance of MAE, indicating better robustness.
\begin{table}[h]
    \centering
    \caption{Robustness evaluation results.}
    \label{tab:robustness}
    \vspace{2mm}
    \begin{tabular}{c|ccc}
    Method & ImageNet-1K ACC & ImageNet-A ACC & ImageNet-C mCE\\
    \midrule
    MAE & 83.3 & 35.9 & 51.7\\
    MI-MAE & 83.9 & 37.4 & 49.5\\
    \end{tabular}
    
\end{table}
\end{document}
