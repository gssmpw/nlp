\section{Related Works}
\textbf{Contrastive learning.} Contrastive learning **Chen, "Improved Baselines for Transfer Learning with Weights & Initialization"** stands out as the leading self-supervised representation learning approach in computer vision, achieving invariance by comparing different augmentations of the same image. A notable example is SimCLR ____ which enhances semantic representations by increasing the similarity between various views of the same image in the latent space. MoCo v3 **He, "Momentum Contrast for Adaptive Transfer Learning"** applies contrastive learning techniques to pre-train vision transformers. DINO ____ delves into novel properties of self-supervised vision transformers.

\textbf{Masked image modeling.} Masked image modeling (MIM) has gained significant traction in the field of computer vision as an effective self-supervised learning paradigm. Recently, with the widespread use of vision transformers (ViTs), ____ a series of notable methods such as BEiT ____ , MAE ____ , and SimMIM ____ have been proposed to pre-train ViTs following the BERT-style masked modeling paradigm used in natural language processing (NLP) **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Many follow-up works extend masked pre-training by exploring data augmentations ____ , mask strategies ____ , and hierarchical structures ____ . Additionally, there is growing interest in understanding MAE and its connection with contrastive learning ____ . In this paper, we further investigate MAE from an information bottleneck perspective.

\textbf{Information bottleneck.} Under information theory, any closed system can be quantified by the mutual information between bottleneck and output variables ____ . A DNN with a given input can be considered as a closed system that introduces no other information. During the forward propagation, the complexity of the intermediate variables usually decreases in a general prediction model, as does the amount of information they contain. It is possible to measure the goodness of each layer and even the whole prediction network by the mutual information that can be used between the intermediate variables or the outputs and the network's prediction target ____