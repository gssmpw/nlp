\begin{table}[t]
    \centering
    \caption{Results on ImageNet classification task. The backbone for SimMIM-based methods is Swin-B~\citep{liu2021swin}, while others are ViT-B~\citep{dosovitskiy2021an}. $*$: The 800-epoch MAE results are reported by MFF~\citep{liu2023improving} based on running the official code of MAE.}
    \label{tab:exp_main_imgnet}
    \footnotesize
    \renewcommand\arraystretch{1}
    \setlength\tabcolsep{2mm}
    \begin{tabular}{lclll}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Epochs} & \multicolumn{3}{c}{ImageNet ACC (\%)}\\
    % \cmidrule{3-5}
     & & FT & LIN & FT$_{1\%}$ \\
    \midrule
    Supervised & - & 81.8 & - & -\\
    \midrule
    DINO~\citep{caron2021emerging} & 800 & 82.8 & 78.2 & -\\
    MoCo v3~\citep{chen2021empirical} & 300 & 83.2 & 76.7 & 63.4\\
    BEiT~\citep{bao2022beit} & 800 & 83.2 & - & -\\
    \midrule
    C-MAE~\citep{kong2023understanding} & 400 & 83.2 & - & - \\
    SemMAE~\citep{li2022semmae} & 800 & 83.3 & 65.0 & -\\
    MFF~\citep{liu2023improving} & 800 & 83.6 & 67.0 & 48.0 \\
    MAE$^*$~\citep{he2022masked} & 800 & 83.3 & 65.6 & 45.4\\
    \textbf{MI-MAE} & 200 & 83.9 \small{\green{(+0.6)}} & 67.9 \small{\green{(+2.3)}} & 48.2 \small{\green{(+2.8)}}\\
    MAE~\citep{he2022masked} & 1600 & 83.6 & 68.0 & 51.1\\
    \textbf{MI-MAE} & 400 & 84.1 \small{\green{(+0.5)}} & 69.3 \small{\green{(+1.3)}} & 52.3 \small{\green{(+1.2)}}\\
    \midrule
    PixMIM~\citep{liu2024pixmim} & 800 & 83.5 & 67.2 & 47.9\\
    SimMIM~\citep{xie2022simmim} & 800 & 83.8 & 56.7 & -\\
    \textbf{MI-SimMIM} & 400 & 84.1 \small{\green{(+0.3)}} & 59.1 \small{\green{(+2.4)}} & 49.1 \small{\green{(+1.2)}}\\
    \bottomrule
    \end{tabular}
\end{table}
