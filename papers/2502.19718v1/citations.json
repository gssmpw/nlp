[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2020simple",
        "author": "Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey",
        "title": "A simple framework for contrastive learning of visual representations"
      },
      {
        "key": "he2020momentum",
        "author": "He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross",
        "title": "Momentum contrast for unsupervised visual representation learning"
      },
      {
        "key": "chen2021exploring",
        "author": "Chen, Xinlei and He, Kaiming",
        "title": "Exploring simple siamese representation learning"
      },
      {
        "key": "grill2020bootstrap",
        "author": "Grill, Jean-Bastien and Strub, Florian and Altch{\\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others",
        "title": "Bootstrap your own latent-a new approach to self-supervised learning"
      },
      {
        "key": "caron2021emerging",
        "author": "Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\\'e}gou, Herv{\\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand",
        "title": "Emerging properties in self-supervised vision transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2020simple",
        "author": "Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey",
        "title": "A simple framework for contrastive learning of visual representations"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chen2021empirical",
        "author": "Chen, Xinlei and Xie, Saining and He, Kaiming",
        "title": "An empirical study of training self-supervised vision transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "caron2021emerging",
        "author": "Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\\'e}gou, Herv{\\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand",
        "title": "Emerging properties in self-supervised vision transformers"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dosovitskiy2021an",
        "author": "Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bao2022beit",
        "author": "Hangbo Bao and Li Dong and Songhao Piao and Furu Wei",
        "title": "{BE}iT: {BERT} Pre-Training of Image Transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "he2022masked",
        "author": "He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\\'a}r, Piotr and Girshick, Ross",
        "title": "Masked autoencoders are scalable vision learners"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xie2022simmim",
        "author": "Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han",
        "title": "Simmim: A simple framework for masked image modeling"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "devlin2019bert",
        "author": "Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina",
        "title": "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "key": "liu2019roberta",
        "author": "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
        "title": "Roberta: A robustly optimized bert pretraining approach"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2023mixed",
        "author": "Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan",
        "title": "Mixed autoencoder for self-supervised visual representation learning"
      },
      {
        "key": "fang2023corrupted",
        "author": "Yuxin Fang and Li Dong and Hangbo Bao and Xinggang Wang and Furu Wei",
        "title": "Corrupted Image Modeling for Self-Supervised Visual Pre-Training"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2022semmae",
        "author": "Li, Gang and Zheng, Heliang and Liu, Daqing and Wang, Chaoyue and Su, Bing and Zheng, Changwen",
        "title": "Semmae: Semantic-guided masking for learning masked autoencoders"
      },
      {
        "key": "wang2023hard",
        "author": "Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang",
        "title": "Hard patches mining for masked image modeling"
      },
      {
        "key": "wang2024droppos",
        "author": "Wang, Haochen and Fan, Junsong and Wang, Yuxi and Song, Kaiyou and Wang, Tong and ZHANG, ZHAO-XIANG",
        "title": "Droppos: Pre-training vision transformers by reconstructing dropped positions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "xie2022simmim",
        "author": "Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han",
        "title": "Simmim: A simple framework for masked image modeling"
      },
      {
        "key": "huang2022green",
        "author": "Huang, Lang and You, Shan and Zheng, Mingkai and Wang, Fei and Qian, Chen and Yamasaki, Toshihiko",
        "title": "Green hierarchical vision transformer for masked image modeling"
      },
      {
        "key": "woo2023convnext",
        "author": "Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining",
        "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2022mask",
        "author": "Zhang, Qi and Wang, Yifei and Wang, Yisen",
        "title": "How mask matters: Towards theoretical understandings of masked autoencoders"
      },
      {
        "key": "xie2023revealing",
        "author": "Xie, Zhenda and Geng, Zigang and Hu, Jingcheng and Zhang, Zheng and Hu, Han and Cao, Yue",
        "title": "Revealing the dark secrets of masked image modeling"
      },
      {
        "key": "huang2023contrastive",
        "author": "Huang, Zhicheng and Jin, Xiaojie and Lu, Chengze and Hou, Qibin and Cheng, Ming-Ming and Fu, Dongmei and Shen, Xiaohui and Feng, Jiashi",
        "title": "Contrastive masked autoencoders are stronger vision learners"
      },
      {
        "key": "kong2023understanding",
        "author": "Kong, Xiangwen and Zhang, Xiangyu",
        "title": "Understanding masked image modeling via learning occlusion invariant feature"
      },
      {
        "key": "pan2023towards",
        "author": "Jiachun Pan and Pan Zhou and Shuicheng YAN",
        "title": "Towards Understanding Why Mask Reconstruction Pretraining Helps in Downstream Tasks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "IB_BASE",
        "author": "Naftali Tishby and Fernando C. Pereira and William Bialek",
        "title": "The information bottleneck method"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "IB_NN",
        "author": "Tishby, Naftali and Zaslavsky, Noga",
        "title": "Deep learning and the information bottleneck principle"
      }
    ]
  }
]