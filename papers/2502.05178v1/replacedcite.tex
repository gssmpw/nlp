\section{Related Work}
\label{sec:related}

\myparagraph{Visual Tokenzation}.
Analogous to LLM tokenizers____ that losslessly transform a text string into discrete tokens, visual tokenization aims to map an image or video to tokens while keeping as much visual information as possible.
VQ-VAE____ introduced the concept of discrete tokenized bottlenecks in auto-encoder architectures.
Later improvements include better training objectives____, increasing VQ codebook usage____, and advanced quantization techniques____.
All of these efforts aim for improved reconstruction quality using the same compression budget and benefit visual generation____.
However, better reconstruction quality does not necessarily lead
to better visual representation____.
On the other hand, visual tokens serve as good intermediate supervision to learn visual encoders with strong representation____.
Our work shows that by properly adding textual supervision the visual tokenizer can be a strong visual encoder \emph{without} introducing extra parameters. 
The concept of aligning visual tokenizer with language is also related to LQAE____ and SPAE____.
SPAE____ aligns the raw pixels with the language token embeddings from a frozen LLM directly.
However, SPAE needs more tokens to reconstruct comparably well with VQ-VAE, indicating that the frozen language codebook might not be optimal.

\myparagraph{Unifying understanding and generation.}
Visual tokenization enables unifying multi-modality in the same token space____.
Chameleon____ interleaves discrete visual and text tokens with a single Transformer and reported training difficulties.
Transfusion____ combines text token prediction with diffusion for images.
Show-o____ unifies understanding and generation by masked language modeling but uses different tokenizers for different tasks.
We use an auto-regressive objective to handle both modalities and \ours enables quick visual-language adaptation from a pre-trained LLM.
Another line of works is \emph{encoder-free}____, which maps patches of raw pixels into embeddings for joint visual-language modeling.
However, this approach is much less data-efficient____ and unable to generate visual content.
VILA-U____ is closely relevant in that its tokenizer is initialized from SigLIP____.
However, the understanding performance drops drastically after re-training (see Figure~\ref{fig:teaser}).
Finally, our visual tokenizer takes advantage of textual supervision and pixel-level reconstruction, echoing recent studies that a mixture of expert vision encoders complement each other for vision-language understanding____.

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Overview.}
    \textbf{(a-b)} Two-stage training pipeline of \ours.
    \textbf{(a)} In Stage 1, we train \ours with a combination of alignment loss and MSE loss.
    \textbf{(b)} In Stage 2, we drop the text encoder, freeze the visual encoder, and no longer optimize the contrastive loss.
    Only the bottleneck quantizer and the decoder are fine-tuned.
    \textbf{(c)} With the text-aligned visual tokenizer, we transform the image into visual tokens, concatenate them with text tokens, and use an auto-regressive multi-modal model (Sec~\ref{sec:method:um3}) to model jointly.
    }
    \label{fig:overview}
\end{figure*}