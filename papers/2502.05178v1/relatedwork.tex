\section{Related Work}
\label{sec:related}

\myparagraph{Visual Tokenzation}.
Analogous to LLM tokenizers~\cite{sennrich2016bpe,schuster2012wordpiece,kudo2018sentencepiece} that losslessly transform a text string into discrete tokens, visual tokenization aims to map an image or video to tokens while keeping as much visual information as possible.
VQ-VAE~\cite{van2017vqvae} introduced the concept of discrete tokenized bottlenecks in auto-encoder architectures.
Later improvements include better training objectives~\cite{esser2021vqgan,ramesh2021dalle}, increasing VQ codebook usage~\cite{yu2022vitvqgan,zheng2023cvq}, and advanced quantization techniques~\cite{lee2022rqvae,mentzer2023fsq,yu2024magvit2,zhao2024bsq}.
All of these efforts aim for improved reconstruction quality using the same compression budget and benefit visual generation~\cite{chang2022maskgit,yu2024magvit2,tian2024var}.
However, better reconstruction quality does not necessarily lead
to better visual representation~\cite{he2022mae,wei2023diffmae}.
On the other hand, visual tokens serve as good intermediate supervision to learn visual encoders with strong representation~\cite{bao2022beit,peng2022beitv2,zhou2022ibot,li2023mage}.
Our work shows that by properly adding textual supervision the visual tokenizer can be a strong visual encoder \emph{without} introducing extra parameters. 
The concept of aligning visual tokenizer with language is also related to LQAE~\cite{liu2023lqae} and SPAE~\cite{yu2023spae}.
SPAE~\cite{yu2023spae} aligns the raw pixels with the language token embeddings from a frozen LLM directly.
However, SPAE needs more tokens to reconstruct comparably well with VQ-VAE, indicating that the frozen language codebook might not be optimal.

\myparagraph{Unifying understanding and generation.}
Visual tokenization enables unifying multi-modality in the same token space~\cite{lu2022unifiedio,lu2024unifiedio2,wu2024nextgpt,jin2024lavit,wang2024emu3,team2024chameleon,zhou2024transfusion}.
Chameleon~\cite{team2024chameleon} interleaves discrete visual and text tokens with a single Transformer and reported training difficulties.
Transfusion~\cite{zhou2024transfusion} combines text token prediction with diffusion for images.
Show-o~\cite{xie2024showo} unifies understanding and generation by masked language modeling but uses different tokenizers for different tasks.
We use an auto-regressive objective to handle both modalities and \ours enables quick visual-language adaptation from a pre-trained LLM.
Another line of works is \emph{encoder-free}~\cite{fuyu-8b,diao2024eve}, which maps patches of raw pixels into embeddings for joint visual-language modeling.
However, this approach is much less data-efficient~\cite{beyer2024paligemma} and unable to generate visual content.
VILA-U~\cite{wu2024vilau} is closely relevant in that its tokenizer is initialized from SigLIP~\cite{zhai2023siglip}.
However, the understanding performance drops drastically after re-training (see Figure~\ref{fig:teaser}).
Finally, our visual tokenizer takes advantage of textual supervision and pixel-level reconstruction, echoing recent studies that a mixture of expert vision encoders complement each other for vision-language understanding~\cite{tong2024cambrian,shi2024eagle}.

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Overview.}
    \textbf{(a-b)} Two-stage training pipeline of \ours.
    \textbf{(a)} In Stage 1, we train \ours with a combination of alignment loss and MSE loss.
    \textbf{(b)} In Stage 2, we drop the text encoder, freeze the visual encoder, and no longer optimize the contrastive loss.
    Only the bottleneck quantizer and the decoder are fine-tuned.
    \textbf{(c)} With the text-aligned visual tokenizer, we transform the image into visual tokens, concatenate them with text tokens, and use an auto-regressive multi-modal model (Sec~\ref{sec:method:um3}) to model jointly.
    }
    \label{fig:overview}
\end{figure*}