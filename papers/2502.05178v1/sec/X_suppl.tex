\clearpage
\setcounter{page}{1}
\appendix

\section{Implementation Details}
\label{sec:supp:details}

\myparagraph{Training \ours.}
Table~\ref{tab:supp:hyperparam:qlip} lists the key hyper-parameters of training \ours-B-8.
The recipe for training other configurations,~\eg \ours-B-16 and \ours-L-14, is similar.

\myparagraph{Training LLaVA.}
This strictly follows the training recipe of LLaVA 1.5 for the sake of a controlled experiment.
For details, please refer to the original paper~\cite{liu2024llava1.5}.

\myparagraph{Training LlamaGen.}
We mostly follow the recipe provided in the original work~\cite{sun2024llamagen}.
Since the authors did not release the training data, we curated the training data by ourselves.
We use a combination of two sources: (1) a 5M subset of LAION-COCO, filtered by aesthetic scores, and
(2) the full set of SA-1B (with 11M images), whose caption is generated by Qwen2-VL-7B~\cite{wang2024qwen2vl}.

\myparagraph{Training \ourlm.}
Table~\ref{tab:supp:hyperparam:um3} lists the key hyper-parameters of training \ourlm-1.5B.


\begin{table}[!tb]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c|c}
    config   & Stage 1  & Stage 2 \\
    \midrule
    peak learning rate     &  5e-4  & 5e-4 \\
    $\cE_\mathrm{v}$ learning rate  & 2e-4 &  0 \\
    $\cE_\mathrm{t}$ learning rate  & 2e-5 &  0 \\
    $\cG$ learning rate  & 2e-3 & 1e-4 \\
    learning rate schedule & cosine annealing & cosine annealing \\
    optimizer              & LAMB  & AdamW \\
    optimizer $(\beta_1, \beta_2)$ & (0.9, 0.95) & (0.9, 0.95) \\
    weight decay           & 0.05 & 0.05 \\
    gradient clip          & 5  & 1 \\
    input resolution       & 256 & 256 \\
    patch size             & 8  &  8 \\
    warm-up iterations     & 2,000 & 2,000 \\
    total iterations       & 120,000 & 120,000 \\
    batch size per device  & 512  & 128 \\
    total batch size       & 65,536  & 16,384 \\
    $\cD$ optimizer        & -  & AdamW \\
    $\cD$ learning rate    & -  & 1e-4 \\
    reconstruction loss weight $\alpha_r$ & 1e3 &  1 \\
    contrastive loss weight $\alpha_a$ & 1 &  0 \\
    quantization loss weight $\alpha_q$  & 1 &  1 \\
    perceptual loss weight $\alpha_p$  & 0 &  0.1 \\
    GAN loss weight $\alpha_g$  & 0 &  0.1 \\
    commitment loss weight $\alpha_z$  & 1.0 &  0 \\
    \end{tabular}
    }
    \caption{\textbf{Hyperparamters for training \ours.} Please refer to Sec.\ref{sec:method} for the notions of loss weights.}
    \label{tab:supp:hyperparam:qlip}
\end{table}

\begin{table}[!tb]
    \centering
    \resizebox{0.66\linewidth}{!}{
    \begin{tabular}{c|c}
    config   & Training \ourlm  \\
    \midrule
    peak learning rate     &  1e-4 \\
    learning rate schedule & cosine annealing \\
    optimizer              & AdamW \\
    optimizer $(\beta_1, \beta_2)$ & (0.9, 0.95) \\
    weight decay           & 0.1 \\
    gradient clip          & 1 \\
    warm-up iterations     & 2,000 \\
    total iterations       & 600,000 \\
    batch size per device  & 8  \\
    total batch size       & 512 \\
    sequence length        & 4,096 \\
    calm-down steps        & 10,000 \\
    mix ratio ($r_{\mathrm{text},0}:r_\mathrm{i2t}:r_\mathrm{t2i}$) & 60:1:3 \\
    mix ratio ($r_{\mathrm{text},T}:r_\mathrm{i2t}:r_\mathrm{t2i}$) & 12:1:3 \\
    sampling temperature   & 1.0 \\
    sampling top-$p$       & 0.95 \\
    \end{tabular}
    }
    \caption{\textbf{Hyperparamters for training \ourlm.}}
    \label{tab:supp:hyperparam:um3}
\end{table}


\section{More Results on \ours}
\myparagraph{Full version of Table~\ref{tab:comparable_tokenizers}.}
We present a more detailed comparison to the state-of-the-art visual encoders or tokenizers in Table~\ref{tab:supp:comparable_tokenizers}.
Compared to Table~\ref{tab:comparable_tokenizers}, we add a column that computes the number of parameters.
Though the convolution-based methods,~\eg VQGAN, have fewer parameters than ViT-based methods,~\eg BSQViT and \ours-B, the runtime is slower as is reported in~\cite{zhao2024bsq}.
Therefore, we subsume those under ``base backbone''.

\myparagraph{Linear Evaluation.}
In addition to the zero-shot image classification, we conduct a linear probing evaluation to compare all visual encoder methods.
Table~\ref{tab:supp:hyperparam:linear} gives the linear probing settings.
For VQ-VAE~\cite{van2017vqvae} and LQAE~\cite{liu2023lqae}, we directly copy the numbers from the paper due to the inaccessibility of models.
We see significant improvement in linear classification accuracy over reconstruction-only tokenizers, such as VQ-VAE and BSQ-ViT, and language-quantized tokenizers, such as LQAE.
We explore two probing positions, namely using the reserved \texttt{[CLS]} token (\textit{cls}-token) or the averaged feature tokens (\textit{ft}), and their concatenation.
Using the averaged feature tokens yields a linear probing accuracy similar to the \textit{cls} token, indicating that the encoder learns strong semantics.
As a reference, we also run the linear evaluation on EVA-CLIP~\cite{sun2023evaclip} and see \ours is very close to this upper bound. 







\begin{table}[!tb]
    \centering
    \tablestyle{3pt}{1.05}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lccc}
    \toprule
    Method & Seen Data & Probing Pos. & IN-1k Acc.\tiny{(\%)} \\
    \midrule
    \multicolumn{3}{l}{\textsc{(Base backbone)}} \\
    VQVAE~\cite{van2017vqvae} & IN-1k & / & 18.4 \\
    LQAE~\cite{liu2023lqae} & IN-1k & / & 39.7 \\
    EVA-CLIP-B~\cite{sun2023evaclip} & Merged-2B & \textit{cls}-token & {\textcolor{gray}{82.7}} \\
    BSQViT~\cite{zhao2024bsq}$^\dagger$ & DC-1B & \textit{cls}-token & 29.3 \\
    BSQViT~\cite{zhao2024bsq}$^\dagger$ & DC-1B & \textit{ft} (avg.) & 25.4 \\
    \ours-B (ours) & DC-1B & \textit{cls}-token & 81.8 \\
    \ours-B (ours) & DC-1B & \textit{ft} (avg.) & 77.7 \\
    \ours-B (ours) & DC-1B & \textit{cls} + \textit{ft} & 82.1 \\
    \midrule
    \multicolumn{2}{l}{\textsc{(Large backbone, high resolution)}} \\
    EVA-CLIP-L~\cite{sun2023evaclip} & Merged-2B & \textit{cls}-token  & {\textcolor{gray}{86.3}} \\
    \ours-L (ours) & DC-1B & \textit{cls}-token & 85.2 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Linear evaluation on image classification.}
    }
    \label{tab:supp:linear_probe}    
\end{table}


\begin{table}[!tb]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c}
    config   & ImageNet linear probing \\
    \midrule
    peak learning rate     & 0.2 / 1.0 (BSQViT) \\
    learning rate schedule & cosine annealing \\
    optimizer              & AdamW \\
    optimizer $(\beta_1, \beta_2)$ & (0.9, 0.999) \\
    weight decay           & 0. \\
    input resolution       & 256 (\ours-B) / 392 (\ours-L) \\
    patch size             & 16 (\ours-B) / 14 (\ours-L)  \\
    warm-up epochs         & 1  \\
    total epochs           & 10 / 20 (BSQViT) \\
    batch size per device  & 128 \\
    total batch size       & 1,024 \\
    \end{tabular}
    }
    \caption{\textbf{Hyperparamters for ImageNet linear probing.}}
    \label{tab:supp:hyperparam:linear}
\end{table}

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/llamagen_generation_supp.pdf}
    \vspace{-15pt}
    \caption{\textbf{Comparison of generated images with conditioning captions in the bottom.}
    For each pair, the left is from LlamaGen+VQGAN and the right is from LlamaGen+\ours-B/16 (ours).
    The caption is also provided at the bottom.
    }
    \label{fig:supp:t2i_vis}
\end{figure*}

\section{More Results on Generation Benchmarks}
\label{sec:supp:geneval}

We show the full results on comprehensive benchmarks such as GenEval~\cite{ghosh2024geneval} and DPG-Bench~\cite{hu2024ella} in Tables~\ref{tab:supp:geneval} and~\ref{tab:supp:DPG-Benchmark} respectively.
Under the same T2I framework, \ours-equipped LlamaGen significantly outperforms the open-sourced VQGAN-LlamaGen and our reproduced baseline with BSQ-ViT.
It also achieves competitive or better results than diffusion-based methods, e.g. SDv1.5 which is trained on much more data. We will add the results in the final version. 

\begin{table}[!thb]
    \centering
    \resizebox{\linewidth}{!}{
    \tablestyle{1pt}{1.05}
    \begin{tabular}{c|l|c|c|c|c|c|c|c}
    \toprule
    Model & Tokenizer &  \scriptsize{Overall} & \scriptsize{Single Obj.} & \scriptsize{Two Obj.} & \scriptsize{Counting} & \scriptsize{Colors} & \scriptsize{Position} & \scriptsize{Attribute} \\
    \midrule
    \multirow{2}{*}{LlamaGen} & VQGAN  & 0.32 & 0.69 & 0.36 & 0.20 & 0.57 & 0.06 & 0.02 \\
    \multirow{2}{*}{(0.8B)} & BSQ-ViT & 0.31 & 0.77 & 0.26 & 0.13 & 0.56 & 0.05 & 0.06 \\
    & \textbf{\ours (Ours)} & \textbf{0.48} & \textbf{0.91} & \textbf{0.59} & \textbf{0.22} & \textbf{0.80} & \textbf{0.17} & \textbf{0.24} \\
    \multicolumn{2}{c|}{{\color{gray} SDv1.5 (0.9B)}} & \color{gray}0.43 & \color{gray}0.97 & \color{gray}0.38 & \color{gray}0.35 & \color{gray}0.76 & \color{gray}0.04 & \color{gray}0.06 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Evaluation on GenEval.}
    \label{tab:supp:geneval}
\end{table}

\begin{table}[!thb]
    \centering
    \resizebox{\linewidth}{!}{
    \tablestyle{2pt}{1.05}
    \begin{tabular}{c|l|c|c|c|c|c|c}
    \toprule
    Model & Tokenizer &  Average & Global & Entity & Attribute & Relation & Other \\
    \midrule
    \multirow{2}{*}{LlamaGen} & VQGAN  & 43.22 & 76.60 & 57.88 & 66.96 & 75.78 & 42.80 \\
    \multirow{2}{*}{(0.8B)} & BSQ-ViT  & 34.03 & 68.39 & 47.70 & 63.40 & 73.77 & 33.60 \\
    & \textbf{\ours (Ours)} & \textbf{78.17} & \textbf{82.37} & \textbf{84.68} & \textbf{86.97} & \textbf{92.50} & \textbf{79.20}  \\
    \multicolumn{2}{c|}{\color{gray}SDv1.5 (0.9B)} & \color{gray}63.18 & \color{gray}74.63 & \color{gray}74.23 & \color{gray}75.39 & \color{gray}73.49 & \color{gray}67.81 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Evaluation on DPG-Bench.}
    \label{tab:supp:DPG-Benchmark}
\end{table}



\section{More Generation Results}
\label{sec:supp:generation}
In Figure~\ref{fig:supp:t2i_vis}, we show more side-by-side generated images by LlamaGen with the original VQGAN and the proposed \ours.
We emphasize the advantage of \ours in terms of better following the captions.
The visual quality can be improved by adding more training data, long training iterations, and larger backbones.
However, this is beyond the scope of this paper.



\begin{table*}[!tb]
    \centering
    \tablestyle{3pt}{1.05}
    \begin{tabular}{lcHcc|cH|ccc}
    \toprule
    \multicolumn{3}{c}{} & \# Param. & & {Understanding} & & \multicolumn{3}{c}{Reconstruction} \\ 
    & Seen Data & Patch Size & ($|\cE|+|\cG|+|\cQ|$) & \# bits & 0-shot Acc.\higherbetter & Lin. Probe\higherbetter & rFID\lowerbetter & PSNR\higherbetter & SSIM\higherbetter \\
    \midrule
    \multicolumn{10}{l}{\textsc{(Base backbone)}} \\
    CLIP~\cite{radford2021clip} & WIT-400M & 16$\times$16 &  87M+0+0 & / & 68.3 & & / & / & / \\
    EVA-CLIP~\cite{sun2023evaclip} & Merged-2B & 16$\times$16 & 87M+0+0 & / & 74.7 & & / & / & / \\
    SigLIP-B~\cite{zhai2023siglip} & WL-10B & 16$\times$16 & 87M+0+0 & / &  76.7 & & / & / & / \\
    VQGAN~\cite{esser2021vqgan} & IN-1k & 16$\times$16 & 29M+42M+4M & 14 & / & & 4.98 & - & -  \\
    MoVQGAN~\cite{zheng2022movq} & IN-1k & 16$\times$16 & (82.7M) & $^\&$40~~~ & / & & 1.12 & 22.42 & 0.6731 \\
    MaskGIT~\cite{chang2022maskgit} & IN-1k & 16$\times$16 & 24M+30M+6k & 10 & / & & 1.98 & 18.63 & 0.4619 \\
    Open-MAGVIT2~\cite{yu2024magvit2,luo2024openmagvit2} & IN-1k & 16$\times$16 & 25M+40M+18k & 18 & / & & 1.53 & 21.53 & - \\
    OpenCLIP-B~\cite{cherti2023openclip} & DC-1B & 16$\times$16 & 87M+0+0 & / & 73.5 &  & / & / & / \\
    BSQViT~\cite{zhao2024bsq}$^\dagger$ & DC-1B & 16$\times$16 & 87M+87M+1M & 28 & / & & 3.81 & 24.12 & 0.6638 \\
    \ours-B (ours) & DC-1B & 16$\times$16 & 87M+87M+1M & 28 & 74.3 & & 3.21 &  23.16 & 0.6286 \\
    \midrule
    \multicolumn{10}{l}{\textsc{(Base backbone, Smaller patch)}} \\
    SigLIP-B~\cite{zhai2023siglip} & WL-10B & 8$\times$8 & 87M+0+0 & / &  79.2 & & / & / & / \\
    DALL-E dVAE~\cite{ramesh2021dalle} & \scriptsize{CC3M+YF} & 8$\times$8 & 54M+44M+0 & 13 & / & & 32.63 & 27.31 & 0.7943 \\ 
    ViT-VQGAN~\cite{yu2022vitvqgan} & IN-1k & 8$\times$8 & 91M+91M+0.5M & 13 & / & & 1.55 & - & - \\
    SD-VAE 1.x~\cite{rombach2022ldm} & OI-2M & 8$\times$8 & 34M+49M+0  & 14 & / &  & 1.40 & 23.65 & 0.6354 \\
    SD-VAE 2.x~\cite{podell2023sdxl} & \scriptsize{OI-2M+LA-ae} & 8$\times$8 & 34M+49M+0 & $^\#$64~~~ & / &  & 0.70 & 26.90 & 0.7592 \\
    SDXL-VAE~\cite{podell2023sdxl} & \scriptsize{OI-2M+LA-ae++} & 8$\times$8 & 34M+49M+0 & $^\#$64~~~ & / &  & 0.67 & 27.37 & 0.7814 \\
    SBER-MoVQGAN~\cite{sber2023movqgan} & \scriptsize{LAHR-166M} & 8$\times$8 & 29M+42M+4M & 14 & / & & 0.96 & 26.45 & 0.7250 \\ 
    BSQViT~\cite{zhao2024bsq} & IN-1k & 8$\times$8 & 87M+87M+28k & 18 &  / & & 0.99 & 27.78 & 0.8171 \\
    EVA-CLIP~\cite{sun2023evaclip}$^\dagger$ & DC-1B & 8$\times$8 & 87M+0+0 & / & 77.2 &  & / & / & / \\
    \ours-B (ours) & DC-1B & 8$\times$8 & 87M+87M+1M & 28 & 75.6 & & 0.70 & 26.79 & 0.7905 \\
    \midrule
    \multicolumn{10}{l}{\textsc{(Large backbone)}} \\
    CLIP/f14~\cite{radford2021clip} & WIT-400M & 14$\times$14 &  304M+0+0 & / & 75.5 & & / & / & / \\
    SigLIP-L~\cite{zhai2023siglip} & WL-10B & 16$\times$16 & 304M+0+0 & / &  80.5 & & / & / & / \\
    OpenCLIP-L~\cite{cherti2023openclip} & DC-1B & 16$\times$16  & 304M+0+0 & / & 79.2 & & / & / & / \\
    EVA-CLIP-L~\cite{sun2023evaclip} & Merged-2B & 16$\times$16  & 304M+0+0 & / & 79.8 &  & / & / & / \\
    Open-MAGVIT2~\cite{yu2024magvit2,luo2024openmagvit2} & IN-1k & 16$\times$16 & 50M+65M+18k & 18 & / & & 1.17 & 21.90 & - \\
    VILA-U~\cite{wu2024vilau} & \scriptsize{WL-10B+CY-1B} & 16$\times$16 & 316M+42M+134M & $^\&$56~~~ & 73.3 & & 1.80 & - &  - \\
    \midrule
    \multicolumn{10}{l}{\textsc{(Large backbone, high resolution)}} \\
    CLIP/f14~\cite{radford2021clip} & WIT-400M & 14$\times$14 &  304M+0+0 & / & 76.6 & & / & / & / \\
    SigLIP-L~\cite{zhai2023siglip} & WL-10B & 16$\times$16 & 304M+0+0 & / &  82.1 & & / & / & / \\
    EVA-CLIP-L~\cite{sun2023evaclip} & Merged-2B & 16$\times$16  & 304M+0+0 & / & 80.4 &  & / & / & / \\
    VILA-U~\cite{wu2024vilau} & \scriptsize{WL-10B+CY-1B} & 14$\times$14 & 428M+42M+537M & $^\&$224~~~~ & 78.0 & & 1.25 & - &  - \\
    \ours-L (ours) & DC-1B & 14$\times$14 & 304M+304M+2M & 28 & 79.1 & & {1.46} & {25.36} & {0.6903} \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Comparison to state-of-the-art visual encoders/tokenizers.}
    $^\dagger$:our reproduction.
    $^\#$: effective number of bits when latents are stored in \texttt{bf16}.
    $^\&$: quantizer uses residual quantization (RQ), where the total bits are multiplied by RQ depth.
    }
    \label{tab:supp:comparable_tokenizers}    
\end{table*}

