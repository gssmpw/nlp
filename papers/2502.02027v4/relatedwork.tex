\section{Related Work}
\label{sec:related_work}
Advancements in navigation and detection under low-visibility conditions have leveraged sensor fusion, visual cue integration, and computational techniques. Aircraft landing studies have explored sensor fusion of visible and virtual imagery \citep{b1} and visual-inertial navigation using runway features \citep{b2}. Multi-sensor fusion algorithms have improved odometry in GPS-denied environments \citep{b3}, while research on depth visualization has enhanced navigation and obstacle avoidance \citep{b4}. Synthetic Vision Systems and full-windshield Head-Up Displays aid drivers and pilots in low visibility \citep{b5,b8}. Image enhancement techniques for low-light conditions \citep{b6} and the fusion of visual cues with wireless communication improve road safety \citep{b7}. Studies have also emphasized the role of geometrical shapes and colors in driving perception via Head-Up Displays \citep{b9}.

Despite these advances, challenges persist, including computational complexity \citep{b2,b6,b10}, performance issues under extreme conditions \citep{b3,b7}, overfitting due to limited datasets \citep{b2,b3}, and insufficient real-world validation \citep{b1,b7,b10}. Some works lack rigorous validation \citep{b5,b9}.

In visual recognition, research has explored human-like processing in computational models. Studies on brain mechanisms highlight hierarchical, feedforward object recognition \citep{b11}, while comparisons with deep neural networks (DNNs) reveal human superiority in handling distortions and attention mechanisms \citep{b12,b13}. Eye-tracking data has been used to guide DNN attention with limited success \citep{b14}. Approaches such as adversarial learning for feature discrimination \citep{b15}, biologically inspired top-down and bottom-up models \citep{b16}, and retina-mimicking models for dehazing \citep{b17} have been proposed. Foveal-peripheral dynamics have also been explored to balance computational efficiency and high-resolution perception \citep{b18}.

Recent research has tackled low-visibility challenges like fog, low light, and sandstorms. The YOLOv5s FMG algorithm improves small-target detection with enhanced modules \citep{b21}, while novel MLP-based networks refine image clarity in hazy and sandstorm conditions \citep{b22}. The PKAL approach integrates adversarial learning and feature priors for robust recognition \citep{b23}. Deformable convolutions and attention mechanisms enhance pedestrian and vehicle detection in poor visibility \citep{b24}. Reviews highlight the limitations of non-learning and meta-heuristic dehazing methods in real-time applications \citep{b25}, emphasizing the need for integrated low-level and high-level vision techniques \citep{b26}. Innovations such as spatiotemporal attention for video sequences \citep{b27}, the PDE framework for simultaneous detection and enhancement \citep{b28}, spatial priors for saliency detection \citep{b29}, and early visual cues for object boundary detection \citep{b30} further contribute to the field.

Despite advances, existing methods struggle with joint optimization of object detection and image enhancement, detection of low-contrast objects, and adaptation to dynamic visibility changes. This paper addresses these challenges by integrating human visual cues, such as attention mechanisms and contextual understanding, into object detection, enhancing both robustness and efficiency. Traditional approaches process entire images uniformly, increasing computational load, and sometimes degrading clear regions. Our method selectively enhances regions of interest, reducing unnecessary computations and improving responsiveness under varying conditions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%