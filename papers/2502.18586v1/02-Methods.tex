\section{Methods}

The full system (Fig. \ref{fig:intro}a) is comprised of a robot-mounted electrocautery tool, a robot-mounted RGBD camera, and a passive laparoscopic gripper. A tissue model is placed on an electrosurgical grounding platform between the robots. The passive gripper tensions the tumor while the robot-mounted electrocautery tool performs the autonomous cut with visual guidance from the mounted RGBD camera, enabling supervised fully autonomous resection of \gls{cao}.


\subsection{Robotic Hardware}

Two UR manipulators (Universal Robots, Odense, Denmark) are used in this study. The UR10e manipulator is equipped with an electrosurgical instrument, which was custom-built from a monopolar electrode (Bovie, Clearwater, FL) with a laparoscopic extension. The UR5 robot holds the RGBD camera (Zivid 2 Plus M60, Oslo, Norway) for imaging of the surgical scene. The Zivid 2 Plus M60 RGBD camera employs structured light 3D imaging and is factory-calibrated. A standard hand-eye calibration procedure was performed with the Zivid camera to obtain the necessary transformation for system registration. A fiducial-based calibration was conducted to obtain the transformation between the UR5 and the UR10e. As in the minimally-invasive case, the electrocautery tool in this work performs each cut while approaching from the distal end of the trachea, as if inserted through the mouth. However, due to the size of the rigid-link robots, the gripper and camera cannot also approach from the distal end; instead, we place the camera above the tumor and the gripper approaching from the proximal end. However, the vision-based resection workflow is not reliant on this configuration and would still be viable if it was transferred to smaller manipulators which all approach from the distal end. Also, since this study focuses on the automation of the resection task alone, the gripper holding the tumor in the other arm was not automated. For the experiments in this paper, a laparoscopic gripper (Snowden-Pencer SP90-6379, Tucker, GA) was fixed on a passive arm and manually translated incrementally along its primary axis to move the tumor before each cut took place. Automating this aspect is left to future work.


\subsection{Surgical Process Modeling}
Before designing the robotic control workflow, it is necessary to define the surgical subtasks which make up the complete procedure of \gls{cao} resection. We adopt a subtask-level Surgical Process Model (SPM) in our approach, inspired by \cite{Nagy2018}. Using SPM representations, the procedure can be split into three subtasks: \textit{Reach-in}, \textit{Resect}, and \textit{Retract}. A finite state machine (FSM) approach naturally follows from these discrete subtasks. FSMs are widely used in autonomous processes due to their maintainable, intuitive structure \cite{FSMs}. In this work, the \textit{Reach-in}, \textit{Resect}, and \textit{Retract} subtasks become finite states, where specific events are programmed to trigger state transitions.




\subsection{Control Workflow}


In our autonomous resection framework (Fig. \ref{fig:workflow}), we aim to provide a systematic approach to supervised tumor resection with point cloud data and polynomial surface fitting. First, data acquisition is conducted as the system uses the Zivid RGBD camera to obtain a snapshot of the surgical scene (which contains a point cloud, color image, depth image, and camera intrinsics). This snapshot data is fed to the segmentation node, which autonomously generates bounding boxes around the tumor and trachea in the color image, and segmentations of the tumor and trachea in the point cloud. These bounding boxes and segmentation are plotted for visualization by the supervisor. The program then prompts the supervisor to either accept the autonomous segmentation or to reject it and hand-draw the bounding boxes, which are then used for autonomous segmentation. The segmented trachea and tumor are published as point clouds to independent ROS topics. 

Next, the trachea surface is fitted with a polynomial surface model. This fitted surface provides a foundation for trajectory planning, where the cutting paths are generated along said surface. After a cut path has been generated, it is displayed to the human supervisor in RViz, along with the RMSE between this cut and the cut which was predicted using the dataset (surface fit and point clouds) stored from the previous cut. The RMSE between cuts indicates how much the newly measured tumor shape and cut trajectory differs from the prior model's predictions (see Section 2.7 for a more detailed explanation of RMSE calculations). If the RMSE value exceeds a set threshold, the system self-supervises by displaying an error to the human operator; the human operator can then choose to re-image and re-plan the cut if desired. 



This process is divided into three phases of execution using a finite-state approach: \textit{Reach-in}, \textit{Resect}, and \textit{Retract}. 

\vspace{1em}
\begin{figurehere}
    \centering
    \includegraphics[width=\columnwidth]{Figures/workflow.png}
    \caption{Autonomous control workflow with inputs and outputs for supervised vision-guided resection. Blue denotes events with human supervision and green denotes finite state subtasks.}
    \label{fig:workflow}
\end{figurehere}

\noindent In the \textit{Reach-in} phase, the electrocautery tool approaches the trachea until it is aligned with the trachea's centroid at a predefined safe initial position near the tissue. During the \textit{Resect} phase, the electrocautery tool executes incremental trajectories along the surface of the trachea. 

After each consecutive cut (\textit{Resect} phase), the cautery tool proceeds to a safe home position near the tissue (\textit{Retract} phase), and the tumor is then tensioned by the gripper to expose the tumor boundary. Then, the tissue is re-imaged and re-segmented, and a new surface model of the trachea is generated. A new cut trajectory is generated and displayed to the human supervisor along with updated RMSE values which compare the new surface and cut to the previous surface and cut. Upon supervisor approval, the tool proceeds to \textit{Reach-In} and \textit{Resect}. This pause for supervision enables real-time verification of each cut, providing an additional layer of safety. This workflow is implemented in C++ utilizing ROS 2 for real-time communication, the Point Cloud Library (PCL) for point cloud processing and Eigen for matrix calculations. 

\subsection{Tissue Models}

Due to the size of our rigid-link end-effectors, which limits their operation within a confined trachea, we introduce an ex-vivo \gls{cao} tissue model for an open procedure, comprised of chicken tissue and a `half-pipe' of porcine trachea (Fig. \ref{fig:intro}b, \ref{fig:intro}c). The usage of real animal tissue provides realistic effects from electrocautery and tissue deformation, as well as accurate color and texture information for segmentation, making our findings readily applicable to in-vivo scenarios.






\subsection{Segmentation}
The segmentation pipeline extracts the trachea point cloud from RGBD snapshots to generate the cut trajectory. As shown in Fig.~\ref{fig:sam}, the pipeline combines a custom object detection model based on Faster R-CNN~\cite{ren2015faster}, Meta's Segment Anything model (SAM 1)~\cite{kirillov2023segment}, and 2D mask projection to generate labeled point clouds.


\subsubsection{Object Detection Model}
Based on the Faster R-CNN architecture~\cite{ren2015faster}, the custom object detection neural network produces labeled bounding boxes and their classification scores (CLS Score), for the trachea and tumor. The implementation leverages a pre-trained Faster R-CNN model with ResNet50 FPN backbone~\cite{he2015deep} (trained on the COCO dataset), modified to locate the trachea and tumor. Transfer learning enables a model fine-tuning by replacing the final prediction layer with a custom layer to classify three classes (background, 

\vspace{1em}
\begin{figurehere}
    \centering
    \includegraphics[width=\columnwidth]{Figures/sam.png}
    \caption{Before each cut, the segmentation pipeline produces labeled point clouds of the trachea and the tumor. }
    \label{fig:sam}
\end{figurehere}

\noindent trachea, and tumor).
The model was trained using stochastic gradient descent (SGD) with momentum (0.9) and weight decay (0.0005) at a learning rate of 0.005 for 10 epochs. The training dataset was manually labeled by a human operator who drew the bounding boxes. The dataset comprised 137 labeled bounding boxes for tissue and trachea, processed in mini-batches of 4 images. Data augmentation was limited to tensor conversion to preserve spatial relationships between the trachea and the tumor. Classification was performed using a softmax function, which converts raw output scores into class scores through exponentiation and normalization. During inference, if the classification score is below $70\%$ we ask the human to provide the bounding box.




\subsubsection{Segmentation and Depth Projection}
% The detection model's bounding boxes guide SAM's segmentation process. 
Our approach combines Faster R-CNN's automatic localization capabilities with SAM's prompt based foundation model architecture to create an end-to-end segmentation pipeline. Faster R-CNN first predicts the location of tumor and trachea regions, which then serve as learned prompts for SAM to generate precise segmentation masks, eliminating the need for manual bounding box annotations from human supervisors. The generated binary masks from SAM are applied to the depth image, effectively setting all non-masked pixels to zero depth. This operation isolates the region of interest in the depth map. Given a masked depth image and the camera intrinsics, the pinhole camera model projects the pixels to point clouds: 
\begin{align*}
X &= \frac{(u - c_x) \cdot Z}{f_x} \\
Y &= \frac{(v - c_y) \cdot Z}{f_y} \\
Z &= \text{depth\_value}.
\end{align*}
Here, the $X$ and $Y$ coordinates are calculated by subtracting the pixel values $u,v$ from the principal points $c_x, x_y$, which are then divided by the x focal lengths $f_x, f_y$. After projecting the two binary masks to point clouds, the final trachea point cloud is refined by subtracting the tumor point cloud, producing clean, labeled point clouds suitable for surface model fitting. 

\subsection{Trachea Surface Modeling}
Following segmentation, the trachea point cloud has a hole where the tumor is located due to occlusion by the pseudo tumor in the imaging process. We use a surface modeling process to ``fill in" this hole, providing a continuous reference surface for trajectory planning.

To best fit the trachea, various surface fitting models were applied to point clouds taken from four discrete trachea models (Fig.~\ref{fig:polynomials}). The objective  was to determine the optimal fitting models by minimizing two criteria: computation time and root mean squared error (RMSE). A Pareto front analysis was performed for each trachea model to identify the best models based on the trade-off between these two objectives. To model the trachea surface accurately, we applied polynomial surface fitting models to point cloud data obtained from four different ex-vivo trachea models (captured in .pcd format). A series of polynomial surface fitting models, ranging from degree 1x1 (poly11) to 10x10 (poly1010), were applied to approximate the shape of the trachea. The fitting process involved solving a least-squares regression problem, where polynomial coefficients were determined from the trachea point cloud. Once the polynomial surfaces were fitted, we computed RMSE as a measure of how well each fitted surface matched the original trachea point cloud. This provided a quantitative assessment of how accurately each polynomial model

\vspace{1em}
\begin{figurehere}
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/allPolynomials3.png}
    \caption{Trachea Model - Polynomial Fits ($poly11$ to $poly55$) for one of the samples (Units are in millimeters).}
    \label{fig:polynomials}
\end{figurehere}



\noindent approximated the real trachea geometry. After computing RMSE separately for all four trachea models, we took the mean RMSE across the four models for each polynomial. In Fig.~\ref{fig:polynomials}, the subplot grid layout is organized by polynomial orders, where the columns represent the increasing polynomial degree in the Y direction and the rows represent the increasing polynomial degree in the X direction. 



In addition to the error, the computation time required to fit each surface to the trachea data was recorded. This served to evaluate the computational efficiency of each model. After obtaining the RMSE and time values for all models, a Pareto front analysis was conducted, the results of which are shown in Fig.~\ref{fig:paretoFront}. In Fig.~\ref{fig:paretoFront}, each point represents a polynomial model plotted based on its RMSE and computation time. The models closer to the Pareto front (red dots) offer an optimal balance between accuracy and efficiency.

Even the smallest differences in RMSE are of critical importance, particularly in the context of autonomous surgery. When examining the Pareto optimal polynomials with the mean RMSE values below 1 mm, it is observed that the mean RMSE does not significantly decrease beyond \textit{poly55}, stabilizing around 0.6 mm. Among the tested models, the 5th-order polynomial (\textit{poly55}) achieved the lowest computation time and was therefore chosen as the optimal model. Using \textit{poly55}, the polynomial surface model is expressed as:
\begin{equation}
    P(x, y) = \sum_{i=0}^{5} \sum_{j=0}^{5} a_{ij} x^i y^j \quad \text{where} \quad i + j \leq 5.
    \label{eq:5thOrder} \nonumber
\end{equation}
Here, $a_{ij}$ are the polynomial coefficients to be determined. This polynomial expression defines the surface by capturing the relationship between the $x$ and $y$ coordinates and the resulting $z$ values (surface height). To obtain the coefficients, the known $x$ and $y$ values are substituted into $P(x, y)$, and the resulting system of equations is solved by equating the polynomial output to the corresponding $z$ values from the point cloud. This fitting process generates a polynomial surface that closely models the trachea, where $i$ and $j$ are non-negative integers up to 10, with each term $x^i y^j$ representing a combination of polynomial degrees in $x$ and $y$.\looseness-1



\vspace{1em}

\begin{figurehere}
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/paretoFront2.png}
    \caption{Pareto front analysis of polynomial surface fitting models for trachea modeling. Each point represents a polynomial model, plotted according to its computation time and root mean squared error (RMSE). Models closer to the Pareto front (red dots) achieve an optimal trade-off between computational efficiency and accuracy.}
    \label{fig:paretoFront}
\end{figurehere} 


                 

 %When examining the mean RMSE values below 1mm, it was observed that, although the mean computation time varied by approximately 10ms, even the smallest differences in RMSE are of critical importance, particularly in the context of autonomous surgery. Therefore, the model with the lowest mean RMSE was selected. Among the tested models, the 5th-order polynomial (\textit{poly55}) achieved the smallest mean RMSE and was thus chosen as the optimal model. 



\subsection{Electrocautery Pitch Angle and Trajectories} 

To determine what pitch angle to set the electrocautery tool during the autonomous resection, four handheld surgical procedure demonstrations (with manual cautery and a laparoscopic gripper) were analyzed. In each demonstration, a point cloud snapshot of the surgical scene was taken during each of the first four cuts. When these point clouds were viewed in the x-z plane, the angle of the electrocautery tool could be determined by fitting a linear model to points along the tool, as shown in Fig. \ref{fig:cautery-pitch}. From these demonstrations, a mean electrocautery pitch of 28.3\(^\circ\) $\pm$ 4.6\(^\circ\) was calculated (Table~\ref{cutpitch}). This informed the programmed pitch angle of the electrocautery tool during the autonomous resection.

\vspace{1em}
\begin{figurehere}
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/cautery-pitch-edited.png}
    \caption{Handheld demonstration of CAO resection (a), resulting point cloud snapshot during the first cut (b), and the X-Z plane of the point cloud and linear model (c).}
    \label{fig:cautery-pitch}
\end{figurehere}

\begin{tablehere}
\centering
\begin{tabular}{|c|c|c|c|c|}  \hline
     & Cut 1 & Cut 2 & Cut 3 & Cut 4  \\ \hline
  Model 1 & 21.7\(^\circ\)     & 20.4\(^\circ\)     & 24.7\(^\circ\)     & 29.1\(^\circ\)     \\ \hline
  Model 2 & 20.8\(^\circ\)     & 29.4\(^\circ\)     & 34.1\(^\circ\)     & 36.5\(^\circ\)     \\ \hline
  Model 3 & 28.7\(^\circ\)     & 27.9\(^\circ\)     & 28.2\(^\circ\)     & 29.2\(^\circ\)     \\ \hline
  Model 4 & 32.0\(^\circ\)     & 27.0\(^\circ\)     & 31.6\(^\circ\)     & 31.4\(^\circ\)     \\ \hline
             \multicolumn{5}{|c|}{Mean $\pm$ STD = 28.3\(^\circ\) $\pm$ 4.6\(^\circ\)} \\ \hline
\end{tabular}
\caption{Electrocautery pitch for the first four cuts, within four distinct handheld demonstrations.}
\label{cutpitch}
\end{tablehere}



For each trajectory generation, the generated surface fit of the trachea is utilized. The trajectory is comprised of six total cut paths, which are designed to be evenly spaced over the size of the tumor in the X-Y plane. The number of cut paths is currently fixed to the arbitrary value of 6 cuts; in the future this could be modified such that the number of cuts varies based on tumor size. For each cut path, the Z values are determined using the polynomial surface model of the trachea, with an added offset of 1 mm as not to puncture the trachea surface, thus defining the trajectory path 1 mm above the real trachea surface. Each cut path is traveled left-to-right by the electrocautery tool, and then retraced right-to-left, after which the electrocautery tool returns to a pulled-back home position. Fig.~\ref{fig:traj} illustrates the fitted surface and cut paths as simulated in RViz. In cycle \textit{i}, the program receives the tumor cloud \textit{i} and trachea surface \textit{i}, and plans cut paths \textit{i} and \textit{i+1}. The robot then executes cut path \textit{i}, and the tumor is then retracted backward. Then, in cycle \textit{i+1}, the program receives new data for the tumor cloud \textit{i+1} and trachea surface \textit{i+1}. It then plans cut paths \textit{i+1} and \textit{i+2}. Before executing the cut path \textit{i+1}, the program first compares the RMSE errors between the surface \textit{i+1} created in the first cycle \textit{i}, to the surface \textit{i+1} created in the second cycle \textit{i+1}. The RMSE check serves as a self-supervision mechanism to detect significant discrepancies in trajectory predictions. If RMSE is low, the next cut follows the predicted path without human intervention. If RMSE is high, this suggests segmentation errors, tissue deformation or unexpected tool interactions, prompting a human supervisor to validate the cut before execution. Sometimes, errors in surface modeling can occur due to inaccurate segmentation. In such cases, human supervision assesses the RMSE to determine whether the trajectory for cut \textit{i+1} is suitable. This process continues for all cuts.



\vspace{1em}
\begin{figurehere}
    \centering
    \includegraphics[width=\columnwidth]{Figures/rviz-edited.png}
    \caption{Fitted surface model and successive cutting trajectories in simulation (RViz). Between cycles, the tumor is retracted backward.}
    \label{fig:traj}
\end{figurehere}   