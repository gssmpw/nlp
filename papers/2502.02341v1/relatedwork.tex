\section{Related Work}
\subsection{4D Medical Image Interpolation}
4D medical image interpolation tackles the challenge of generating high-resolution temporal data, which is often limited by factors such as radiation exposure and scan time. VoxelMorph provides a fast, learning-based framework for generating deformation fields, optimizing registration and interpolation tasks with convolutional networks \cite{balakrishnan2019voxelmorph}. Another approach by Kim introduced a diffusion deformable model, which integrates diffusion and deformation modules to generate intermediate frames along a geodesic path while preserving spatial topology \cite{Kim2022DiffusionDM}. Additionally, Kim proposed UVI-Net, an unsupervised framework that directly interpolates temporal volumes without intermediate frames, demonstrating robustness with minimal training data \cite{kim2024data}. However, a few of these methods consider the impact of domain shifts, which can significantly degrade model performance in practical clinical settings.

\subsection{Test Time Training}
TTT improves model adaptability to distribution shifts by updating parameters during inference \cite{liang2024comprehensive}. In image classification, Sun formulated TTT as a self-supervised task on test samples, achieving robust performance under domain shifts \cite{sun2020test, sun2019unsupervised, gandelsman2022test}. In anomaly detection and segmentation, Costanzino leveraged TTT to use features from test data for training a binary classifier, improving segmentation accuracy without labeled anomalies \cite{Costanzino2024TestTT}. In video object segmentation, Bertrand incorporated mask cycle consistency in TTT to counter performance drops caused by video corruptions and sim-to-real transitions \cite{bertrand2023testtime}.
   
\subsection{Self-supervised Learning}
Self-supervised learning leverages automatically generated labels from data itself to learn meaningful representations without requiring manual annotations. A common strategy in SSL is to design auxiliary tasks with specific loss functions that guide the model to extract relevant features \cite{noroozi2016unsupervised, chen2019self, taleb2021multimodal}. For instance, Doersch proposed a jigsaw puzzle task where an image is split into patches, and the network predicts their spatial arrangement, promoting an understanding of spatial structure \cite{doersch2015unsupervised}. Similarly, Gidaris used rotation prediction as an auxiliary task, where the model identifies the rotation angle (0°, 90°, 180°, or 270°) applied to an image, enhancing its sensitivity to geometric transformations \cite{gidaris2018unsupervised}. More recently, Chen introduced contrastive learning via SimCLR, which uses a contrastive loss to maximize agreement between augmented views of the same instance, learning invariant representations across transformations \cite{chen2020simple}. These approaches highlight the versatility of self-supervised learning in capturing robust data representations. 

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{teaser2.pdf}
    \caption{Visualization of three TTT schemes: (a) Naïve TTT, (b) Online TTT, (C) Mini-batch TTT. In (a) Naïve TTT, the model is adapted using all test samples before making predictions; (b) Online TTT, where the model is adapted individually for each mini-batch, with updates independent from other batches; and (c) Mini-batch TTT, where the model is adapted in an online manner to the entire test set, and previous knowledge can contribute to current one. }
    \label{fig:teaser2}
\end{figure*}