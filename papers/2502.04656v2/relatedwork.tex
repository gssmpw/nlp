\section{Related Work}
\subsection{Real-time object detectors}
In practical applications, to meet the numerous detection tasks, faster and stronger networks are more widely used. Real-time object detectors have evolved from this trend, become an integral part of many applications that require instant analysis and interaction with dynamic environments. YOLOv1-YOLOv3~\cite{yolo1,yolo9000,yolov3} pioneer one-stage real-time detection and lay the foundation for a multi-scale detection framework. YOLOv4~\cite{yolov4} introduces the PAFPN structure and also utilizes mosaic and mixup data augmentation, which have been used ever since. YOLOv5~\cite{yolov5} designs a more optimized one-to-many label assignment based on anchor boxes, improves data augmentation techniques, and consolidates a CNN-based real-time general object detection framework that includes a backbone, multi-scale feature pyramid, and dense detection heads. YOLOX~\cite{yolox} first proposes the YOLO detection framework based on the dynamic label assignment strategy, removing the anchor boxes that rely on artificial priors or the inductive bias of the data itself. This achieves completely anchor-free multi-scale label assignment. 
Following that, PPYOLOE~\cite{ppyoloe} and YOLOv6~\cite{yolov6} explored reparameterization techniques and adopted the Task Alignment Learning (TAL)~\cite{tal} strategy for label assignment. The reparameterization technique enhances the representational capacity of individual convolutions without increasing the network’s parameter count or inference speed, thus providing a new direction for optimizing subsequent YOLO models. Meanwhile, the efficient performance of the TAL method has standardized the label assignment strategy across these models.
% Following that, PPYOLOE~\cite{ppyoloe} and YOLOv6~\cite{yolov6} explore reparameterization techniques and adopted the Task Alignment Learning (TAL)~\cite{tal} strategy in label assignment, the reparameterization technique enriches the representational capacity of individual convolutions without increasing the network’s parameter count or inference speed, providing a new direction for the optimization of subsequent YOLO models, and the efficient performance of the TAL method has unified the label assignment strategy for subsequent YOLO models. 
YOLOv7~\cite{yolov7} proposes the Efficient Layer Aggregation Network (ELAN) scheme to optimize the Cross Stage Partial Network structure from YOLOv4 and designs several trainable bag-of-freebies methods for lossless model optimization, making a contribution to the lightweight YOLO network. 
% Gold-YOLO~\cite{goldyolo} introduces an advanced Gather-and-Distribute (GD) mechanism, implemented using convolution and self-attention operations, to better integrate and distribute information across different layers, thereby enhancing feature fusion capabilities. However, this comes at the cost of a significantly increased parameter overhead.
DAMO-YOLO~\cite{damo} uses MAE-NAS~\cite{maedet} to search the backbone network under the constraints of low latency and high performance, and combines knowledge distillation to further improve the performance of the detector. 
YOLOv8~\cite{yolov8} fully absorbs the widely proven effective dynamic label allocation strategy, anchor-free detection architecture and ELAN module design concept, and extends the YOLO architecture to the field of segmentation and posture estimation. YOLOv9~\cite{yolov9} explores the issue of information bottleneck occurring when data passes through deep networks and introduces the concept of Programmable Gradient Information (PGI) to handle the various changes required for deep networks to achieve multiple goals. Additionally, it further realizes the lightweighting of the network architecture. YOLOv10~\cite{yolov10} uses one-to-many and one-to-one detection heads to effectively push the YOLO framework to an end-to-end reasoning paradigm, eliminating the post-processing step of non-maximum suppression. YOLO11~\cite{yolo11} represents the latest advancement in YOLO detection technology, introducing feature extraction modules called C3k2 and C2PSA, which significantly enhance the performance of each model. 

Throughout the evolution of YOLO models, most versions have focused on optimizing and improving their foundational convolutional modules. However, relatively few YOLO models have addressed the challenges of feature fusion in depth. Notably, even the latest versions, YOLOv7 to YOLO11, still rely on the original PAFPN structure. This paper, therefore, conducts a more comprehensive investigation into both the popular direction of convolutional module optimization and the less-explored area of feature fusion enhancement.

% \begin{table}
% 	\centering
% 	\caption{Ablation study on RepHMS structure.}\label{duibi}
% 	\renewcommand\arraystretch{1.0}	
% 	\setlength{\tabcolsep}{1mm}
% 		\begin{tabular}{c|c|c|c|c|ccc}
% 		\hline
% 		Model& FPN& Feature extraction &Reparameterization &Stronger feature fusion &Stronger Multi-scale receptive field & High parameter utilization efficiency \\
%     \hline
% 	       YOLOv6 && & 2.0M & 7.1G & 40.5& 56.7
% 		\hline
% 		\end{tabular}
% \end{table}
% \begin{table}
% 	\centering
% 	\caption{Ablation study on RepHMS structure.}\label{duibi}
% 	\renewcommand\arraystretch{1.0}	
% 	\setlength{\tabcolsep}{1mm}
% 	\begin{tabular}{c|c|c|c|c|c|c}
% 		\hline
% 		Model & FPN & \parbox[c]{2cm}{Feature extraction} & Reparameterization & \parbox[c]{2cm}{Stronger feature fusion} & \parbox[c]{3cm}{Multi-scale  receptive field} & \parbox[c]{3cm}{High parameter utilization efficiency} \\
% 		\hline
% 	       YOLOv6 && & 2.0M & 7.1G & 40.5 & 56.7 \\
% 		\hline
% 	\end{tabular}
% \end{table}
% \begin{table}
% 	\centering
% 	\caption{Ablation study on RepHMS structure.}\label{duibi}
% 	\renewcommand\arraystretch{1}	
% 	\setlength{\tabcolsep}{1mm}
% 	\begin{tabular}{c|c|c|c|c|c|c}
% 		\hline
% 		Model & FPN & \makecell{Feature\\extraction} & RepConv & \makecell{Stronger\\ feature\\ fusion} & \makecell{Stronger\\ Multi-scale\\ receptive field} & \makecell{High\\ parameter\\ utilization\\ efficiency} \\
% 		\hline
% 	       YOLOv6 &RepBiFPN& BepC3& 2.0M & 7.1G & 40.5 & 56.7 \\
%            YOLOMS &PAFPN& MSBlock& 2.0M & 7.1G & 40.5 & 56.7 \\
%            Gold-YOLO &GD& BepC3& 2.0M & 7.1G & 40.5 & 56.7 \\
%            YOLOv10 &PAFPN& C2fCIB& 2.0M & 7.1G & 40.5 & 56.7 \\
%             MHAF-YOLO &MAFPN& RepHMS& 2.0M & 7.1G & 40.5 & 56.7 \\
% 		\hline
% 	\end{tabular}
% \end{table}
\subsection{Multi-scale features fusion for object detection}
The different sizes of targets are a major characteristic of detection tasks. Feature maps of different scales correspond to object information of different sizes. Usually, low-dimensional information is used to represent small objects, and high-dimensional information is used to represent large objects. There is an implicit correlation between the features of each dimension. As the network depth increases, the low-level texture features will be transformed into high-level semantic information. How to enhance the connection between features of different levels is the focus of many works. 

Feature Pyramid Networks (FPN) is the first algorithm to propose feature fusion in object detection. The original intention of FPN is to enhance the multi-scale detection capability of networks by incorporating cross-scale connections and information exchange. However, the bottom-up fusion method of FPN is slightly simpler. It only transfers the semantic information of the high-level layer to the low-level layer, but the texture information of the low-level layer is not transferred to the high-level layer. The Path Aggregation Network (PANet) adopts a bottom-up path, which makes the information fusion between different levels more adequate based on FPN. YOLOv6 uses a bidirectional connection (BIC) mechanism to better utilize the shallow information of the backbone, and the processing of high-resolution features and low-resolution features can take into account both large and small target detection. Furthermore, Asymptotic Feature Pyramid Network (AFPN)~\cite{afpn} starts by fusing two adjacent Low-Level features and gradually incorporates High-Level features into the fusion process. In this way, large semantic gaps between non-adjacent Levels can be avoided. DAMO-YOLO adopts the Reparameterized Generalized FPN (RepGFPN) to achieve a richer fusion of the backbone and neck. Gold-YOLO~\cite{goldyolo} proposes a Gather-and-Distribute mechanism (GD), which is implemented through convolution and self-attention, further improving the fusion capability of multi-scale features. Information of each scale is collected and fused through a unified module, and then the fused features are distributed to different layers. This not only avoids the inherent information loss of the traditional FPN structure, but also enhances the information fusion capability of the Neck part without significantly increasing the latency. These methods comprehensively consider the characteristics of multi-scale features and contribute to feature fusion in object detection, but there is still room for further optimization.

\subsection{Foundational Convolutional Module}
\begin{figure}[htbp]  
    \centering  
    \begin{minipage}[b]{0.3\textwidth}  
        \centering  
        \includegraphics[width=\textwidth]{sub1.png}  
        \footnotesize{(a) 
        % C3/CSPRes/BepC3/CSPNextBlock
}  
    \end{minipage}  
    \hfill  
    \begin{minipage}[b]{0.29\textwidth}  
        \centering  
        \includegraphics[width=\textwidth]{sub2.png}  
        \footnotesize{(b) 
        % ELAN/C2f/GELAN/RepHELAN/C2CIB
}  
    \end{minipage}  
    \hfill  
    \begin{minipage}[b]{0.33\textwidth}  
        \centering  
        \includegraphics[width=\textwidth]{sub3.png}  
        \footnotesize{(c) \\
        % MSBlock    \\       
         }  
    \end{minipage}  
    \caption{Different feature extraction blocks. (a)The basic block of CSPNet in~\cite{yolov5, ppyoloe, yolov6, rtmdet}. (b)The proposed block with ELAN in~\cite{yolov7, yolov8, yolov9, mafyolo, yolov10, yolo11}. (c)The basic unit of YOLOMS~\cite{yoloms}.}  
    \label{fig3}  
\end{figure}
CSPNet~\cite{cspnet} alleviates the issue that previous works required heavy inference computations from the network architecture perspective. It effectively improves accuracy while reducing a large amount of computations, and it also possesses strong ease of use and versatility. In most early YOLO networks, the CSPNet structure was widely used as the basic feature extraction module. The latest state-of-the-art YOLO models employ variants of the CSPNet to achieve superior performance. These modules are integrated into the Backbone and Neck of the network. Notably, while these modules share a similar overall architecture, their bottleneck structures of differ slightly. Different bottleneck consist of several regular convolutions, DW convolutions, reparameterized convolutions, and so on.

As shown in Fig.~\ref{fig3}(a), the C3 module in YOLOv5, the CSPRes module in PPYOLOE, the BepC3 module in YOLOv6, and the CSPNextBlock in RTMDet are all classic CSPNet structures. The input is divided into two branches by a $1 \times 1$ convolution, each branch having half channels of the original input. The first branch undergoes deep feature extraction through multiple bottleneck and then concatenated with the second branch before the final output.

In designing its network architecture, YOLOv7 introduces considerations for gradient propagation efficiency to balance the network's learning capability. Compared to CSPNet, the design of ELAN places greater emphasis on optimizing the gradient pathways, reducing issues related to gradient vanishing and exploding. This enhances the stability and convergence speed of the model during training. Moreover, ELAN maintains high detection accuracy while further reducing unnecessary computational load and the number of parameters, which in turn increases the inference speed of the model. It develops the ELAN, as shown in Fig.~\ref{fig3}(b), which is used in later models such as the C2f module in YOLOv8, the GELAN module in YOLOv9, the RepHELAN module in MAF-YOLO, the CIB module in YOLOv10, and the C3k2 module in YOLO11. The ELAN variant replaces the two $1 \times 1$ convolutions in the CSPNet with a split operation and retains the output of each bottleneck. These outputs are concatenated before the final output.

The MSBlock proposed in YOLOMS offers significant enhancements in multi-scale feature representation compared to CSPNet. It utilizes a hierarchical feature fusion strategy, employing multiple branches for feature extraction and introducing convolutional kernels of varying sizes into the backbone network to capture features at different scales. This design enables the model to handle objects of diverse sizes more effectively, thereby improving detection accuracy. Furthermore, unlike the standard convolutions or re-parameterized convolutions used in previous YOLO versions, MSBlock incorporates an Inverted Bottleneck Block combined with Depthwise Convolution to reduce computational costs. As illustrated in Fig.~\ref{fig3}(c), the MSBlock enhances the network's depth by employing width augmentation and cascade connections. The input is divided into N branches, each processed through a bottleneck module. The output from each branch is then passed to the subsequent branch, enabling efficient transmission of deep feature information throughout the network.
% As shown in Fig.~\ref{fig3}(c), MSBlock increases the network's depth through width augmentation and cascade connections. The input information is divided into N branches, each undergoing a bottleneck process. The output of each branch is propagated to the next branch, facilitating deep information transmission.
% \begin{figure}[htbp]  
%     \centering  
%     \begin{minipage}[b]{0.15\textwidth}  
%         \centering  
%         \includegraphics[width=\textwidth]{sub11.png}  
%         \footnotesize{(a)
% }  
%     \end{minipage}  
%     \hfill  
%     \begin{minipage}[b]{0.18\textwidth}  
%         \centering  
%         \includegraphics[width=\textwidth]{sub12.png}  
%         \footnotesize{(b)
% }  
%     \end{minipage}  
%     \hfill  
%     \begin{minipage}[b]{0.15\textwidth}  
%         \centering  
%         \includegraphics[width=\textwidth]{sub13.png}  
%         \footnotesize{(c)   \\       
%          }  
%     \end{minipage}  
%         \hfill  
%     \begin{minipage}[b]{0.15\textwidth}  
%         \centering  
%         \includegraphics[width=\textwidth]{sub14.png}  
%         \footnotesize{(d)   \\       
%          }  
%     \end{minipage}  
%             \hfill  
%     \begin{minipage}[b]{0.16\textwidth}  
%         \centering  
%         \includegraphics[width=\textwidth]{sub15.png}  
%         \footnotesize{(e)   \\       
%          }  
%     \end{minipage}  
%     \caption{123}  
%     \label{fig:example}  
% \end{figure}

% The design of the BottleNeck reflects the unique features of different YOLO models and plays a critical role in enhancing overall performance.

% As shown in Figure A, models like YOLOv5 and YOLOv8 utilize the most basic feature extraction approach, consisting of two 3x3 convolutions and a shortcut. In Figure B, YOLOv6 incorporates the RepVGG concept into its network, using two RepConv layers to form the BottleNeck.

% In Figure C, RTMDet introduces a 5x5 Depthwise (DW) convolution to slightly expand the receptive field at a reasonable computational cost. This approach allows for more comprehensive context modeling and significantly improves accuracy.

% Figure D illustrates the BottleNeck structure of YOLOv10, which consists of three sequential DW convolutions and two 1x1 convolutions. The structure shown in Figure E is part of the MSBlock and comprises two 1x1 convolutions and a k x k DW convolution. Both of these structures employ an intermediate channel expansion method to further enhance the capabilities of DW convolutions.
% \begin{figure}[htb]
	
% 	\begin{minipage}[b]{1.0\linewidth}
% 		\centering
% 		\centerline{\includegraphics[width=13cm]{basic.png}}
% 		%  \vspace{2.0cm}
% 		% \centerline{(a) Result 1}\medskip
% 	\end{minipage}
% 	\caption{Overview of the network architecture of MAF-YOLO.}
% 	\label{mayolo}
% 	%
% \end{figure}




  

% \subsection{structural re-parameterization}
% In real-world scenarios, training resources are usually relatively sufficient, while inference resources need to be saved as much as possible. This allows a portion of training resources to be exchanged for inference resources without compromising model performance. The core idea of structural reparameterization is to use the fact that convolution operation is a linear operation with additive property, so as to construct a series of structures for training and convert their parameters into another set of parameters for inference. This can combine the advantages of high performance of multi-branch training and the advantages of fast speed and resource-saving of single-branch inference. RepVGG maps the three-branch structure in the training phase to a single-branch 3×3 convolution, which reduces parameters and speeds up inference time. RepLKNet introduces the reparameterization of small convolution kernels in the process of expanding large convolution kernels to ultra-large convolution kernels, thereby solving the problem of reduced accuracy and expanding the receptive field. UniRepLKNet uses dilated convolution to capture sparse features, uses the idea of structural reparameterization to convert the submodule into a large kernel convolution, and proposes that dilated convolution with small convolution kernels are equivalent to non-dilated convolution with large convolution kernels. In general, it can be considered that structural reparameterization has a positive effect on the performance and speed of the model.