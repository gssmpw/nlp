Our core contribution is to extend the behavior-regularized RL framework to diffusion policies by implementing the regularization as the accumulated discrepancies of each single-step transition of the reverse diffusion. Theoretically, we demonstrate the equivalency of the pathwise KL and the commonly adopted KL constraint on action distributions. In practice, we instantiate the framework with an actor-critic style algorithm which leverages value functions in two time scales for efficient optimization. The proposed algorithm, \algbb, produces generation results that closely align with the target distribution, thereby enhancing its performance and applicability in complex continuous control problems.
