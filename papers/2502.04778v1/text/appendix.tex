\section{Introduction about the Benchmarks}\label{appsec:intro_benchmark}
\textbf{Synthetic 2D Datasets. }We leverage the synthetic 2D datasets from \citet{qgpo} as a sanity check and a convenient illustration of the mechanism of \algbb. This task set encompasses 6 different datasets. For each dataset, the data points $x_i$ are associated with different energies $e_i$. By sampling from the Boltzmann distribution associated with the temperature $\eta$:
\[
p(x_i)\propto \exp(e_i/\eta), x_i\in \mathcal{D},
\]
we can obtain ground truth sample results regularization strength $\eta$ (see Figure~\ref{appfig:2d_sample}). 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/appfig_toy2d_data.pdf}
    \caption{Illuration of synthetic 2D datasets and sampling results with various temperature $\eta$.}
    \label{appfig:2d_sample}
\end{figure}

We also use Gym MuJoCo~\citep{gym}, which provides a series of continuous control tasks to assess the performance of BDPO as well as the baseline methods.

\textbf{Locomotion Tasks. }We choose 3 tasks from Gym MuJoCo: 1) \textit{halfcheetah}, which is a robot with 9 body segments and 8 joints (including claws) as shown in Figure~\ref{subfig:benchmarks_a}. The goal is to apply torque to 6 of the joints to make the robot move forward as quickly as possible. The reward is based on forward movement distance. 2) \textit{hopper}, which is a simpler robot with a torso, a thigh, a shin, and a foot as shown in Figure~\ref{subfig:benchmarks_b}. The goal is to use torque on 3 joints to make the robot hop forward. 3) \textit{walker2d}, a robot with a torso and two legs, each consisting of a thigh, a shin, and a foot as shown in Figure~\ref{subfig:benchmarks_c}. The goal is to apply torque to 6 joints to make the robot walk forward, rather than hop.

For the offline dataset, we choose the \textit{-v2} datasets with three levels of qualities provided by D4RL~\citep{d4rl}: 1) \textit{medium}, which is collected by a policy that achieves approximately 30\% of expert-level performance; 2) \textit{medium-replay}, which includes all data from the replay buffer of the training process of the medium-level policy; and 3) \textit{medium-expert}: A mixture of medium-level data and expert-level data in a 1:1 ratio.

\textbf{Navigation Tasks. }We use Antmaze as the representative of navigation tasks. In this task set, the agent needs to control an eight degree-of-freedom \textit{ant} robot to walk through a maze, as shown in Figure~\ref{subfig:benchmarks_d}. We use three different maps: \textit{umaze}, \textit{medium}, and \textit{large}; and employed three different types of datasets: 1) The ant reaches a fixed target from a fixed starting position; 2) In the \textit{diverse} dataset, the ant needs to go from a random position to a random target; 3) In the \textit{play} dataset, the ant goes from manually selected positions to manually selected targets (not necessarily the same as during evaluation). We use the \textit{-v0} versions of the datasets, also provided by D4RL~\citep{d4rl}. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{images/halfcheetah.pdf}
        \caption{HalfCheetah}
        \label{subfig:benchmarks_a}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{images/hopper.pdf}
        \caption{Hopper}
        \label{subfig:benchmarks_b}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{images/walker2d.pdf}
        \caption{Walker2d}
        \label{subfig:benchmarks_c}
    \end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{images/umaze.pdf}
        \caption{AntMaze}
        \label{subfig:benchmarks_d}
    \end{subfigure}
    \caption{Illustration of the Locomotion and Navigation Tasks.}
    \label{fig:benchmarks}
\end{figure}

\section{Hyper-parameter Configurations}\label{appsec:hyperparameters}

\subsection{D4RL Datasets}
We largely follow the configurations from DAC~\citep{dac}. Parameters that are common across all datasets are listed in Table~\ref{apptab:common_param}. The hyper-parameters of value networks in the table are the same for both the Q-value networks $Q_{\psi}$ and diffusion value networks $V_{\phi}. $
\begin{table}[htbp]
    \centering
    \caption{Common hyper-parameters across all datasets.}
    \label{apptab:common_param}
    \begin{tabular}{c|c}
        \toprule
        Diffusion Backbone & \texttt{MLP}(512, 512, 512, 512)\\
        Diffusion Noise Schedule & Variance Preserving~\citep{diffusion_sde}\\
        Diffusion Solver & DDPM \\
        Diffusion Steps & 5\\
        Actor Learning Rate & 0.00001\\
        Actor Learning Rate Schedule & Cosine Annealing\\
        Actor Gradient Clip & 1.0\\
        Actor EMA Rate & 0.005 \\
        Actor Samples & 10 \\
        Actor Update Frequency & 5\\
        Value Learning Rate & 0.0003 \\
        Value Ensemble & 10  (20 for the \textit{hopper-medium-v2} task exclusively) \\
        Value Backbone & \texttt{MLP}(256, 256, 256)\\
        Value EMA Rate & 0.005 \\
        Discount & 0.99 for locomotion tasks and 0.995 for antmaze tasks\\
        Batch Size & 256 \\
        Optimizer & ADAM~\citep{adam} \\
        Pretrain Steps & 2000000\\
        Train Steps & 2000000\\
        Value Warmup Steps & 50000\\
        Sample Clip Range & [-1, 1]\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Hyper-parameters that vary in different tasks.}
    \label{apptab:different_param}
    \begin{tabular}{c||c|c|c}
        \toprule
        Dataset & $\rho$ & $\eta$ & Max-Q Backup~\citep{dql}\\
        \midrule
        halfcheetah-medium-v2 & \multirow{3}{*}{0.5} & \multirow{3}{*}{0.5} & \multirow{3}{*}{False}\\
        halfcheetah-medium-replay-v2 &&&\\
        halfcheetah-medium-expert-v2 &&&\\
        \midrule
        hopper-medium-v2 & \multirow{3}{*}{2.0} & \multirow{3}{*}{2.0} & \multirow{3}{*}{False}\\
        hopper-medium-replay-v2 &&&\\
        hopper-medium-expert-v2 &&&\\
        \midrule
        walker2d-medium-v2 & \multirow{3}{*}{1.0} & \multirow{3}{*}{1.0} & \multirow{3}{*}{False}\\
        walker2d-medium-replay-v2 &&&\\
        walker2d-medium-expert-v2 &&&\\
        \midrule
        antmaze-umaze-v2 & \multirow{3}{*}{1.0} & 5.0 & \multirow{3}{*}{True} \\
        antmaze-medium-play-v0 & & 1.0 & \\
        antmaze-large-play-v0 & & 10.0 &  \\
        \midrule
        antmaze-umaze-diverse-v0 & \multirow{3}{*}{0.5} & 5.0 & \multirow{3}{*}{True} \\
        antmaze-medium-diverse-v0 & & 1.0 & \\
        antmaze-large-diverse-v0 & & 5.0 &  \\
        \bottomrule
    \end{tabular}
\end{table}

We list the hyper-parameters that vary across different datasets in Table~\ref{apptab:different_param}. We specifically tune the regularization strength $\eta$ and the LCB coefficient for locomotion tasks, plus the value backup mode for antmaze tasks. 

For evaluation, at each timestep $s_t$, we use the diffusion policy to diffuse $N=10$ actions, and select the action with the highest Q-value as the final action $a_t$ to execute in the environment. We report the score normalized by the performances of random policies and expert policies, provided by D4RL~\citep{d4rl}. 

\subsection{Synthetic 2D Datasets}

In the experiments with Synthetic 2D Datasets, we basically followed the parameters in D4RL in Table~\ref{apptab:common_param}, except that $\rho$ is set to 0, $\eta$ is set to 0.06, batch size is set to 512, and the number of diffusion steps is set to 50. We also turn off sample clipping for BDPO since the actions in these tasks are within $[-4.5, 4.5]$.

\subsection{Details about the Ablation on Policy Parameterization}\label{appsec:abla_arch_detail}
In this section, we provide more details about how we implement the variants that use deterministic policies or Gaussian policies. The only difference between these variants and \algbb lines in the actor parameterization, which means that we keep the implementation of the value networks (e.g. ensemble tricks and LCB value target) and the hyper-parameters (e.g. the LCB coefficient $\rho$) identical to \algbb. We inherit the general behavior-regularized RL framework defined in \eqref{eq:brl_obj}, where we update the soft Q-value functions using the target
\begin{equation}
    \begin{aligned}
        \mathcal{B}^\pi Q^\pi(s, a)=R(s, a)+\gamma \mathbb{E}\left[Q^\pi(s', a')\right]-\eta D(\pi, \nu),
    \end{aligned}
\end{equation}
and improve the policy $\pi$ via
\begin{equation}
    \max_\pi\ -\eta D(\pi, \nu)+\gamma \mathbb{E}_{a\sim\pi}\left[Q^\pi(s, a)\right],
\end{equation}
where $D$ denotes certain types of regularization specific to the actor parameterization.

For deterministic policies, we follow \citet{rebrac} and use the mean squared error between actions from the policy and the behavior policy:
\begin{equation}
    D(\pi,\nu) = \frac 12\|\pi(s)-\nu(s)\|^2. 
\end{equation}
For Gaussian policies, we parameterize it as Diagonal $\pi(\cdot|s)=\mathcal{N}(\mu_{\pi}(s), \sigma_{\pi}^2(s))$, and the regularization is implemented as the KL divergence:
\begin{equation}
    \begin{aligned}
        D(\pi,\nu)=\KL{\pi}{\nu}=\log \frac{\sigma_\nu}{\sigma_{\pi}} + \frac{\sigma_\pi^2+(\mu_\pi-\mu_\nu)^2}{2\sigma_\nu^2}-\frac 12. 
    \end{aligned}
\end{equation}
We find that using the analytical KL computation for Gaussian policies works better than using sample-based estimations.

To ensure fair comparisons, we search for the optimal $\eta$ for both deterministic policies and Gaussian policies within a certain range. Specifically, we choose the $\eta$ from $\{0.1, 0.25, 0.5, 0.75, 1, 2, 3, 5, 10\}$. We observed that a higher eta would lead to a decline in performance, but an excessively small eta would cause the performance to oscillate drastically or even collapse in some cases. Therefore, we select values that ensure stable performance and deliver the highest level of performance as shown in Table~\ref{apptab:arch_abla_param}. 
\begin{table}[htbp]
    \centering
    \caption{Values of $\eta$ used in the ablation study with the policy parameterization.}
    \label{apptab:arch_abla_param}
    \begin{tabular}{c|c|c}
        \toprule
        Dataset & Deterministic Policy & Gaussian Policy\\
        \midrule
        halfcheetah-medium-expert-v2 & 10 & 0.75\\
        \midrule
        walker2d-medium-v2 & 1.5 & 0.25\\
        \bottomrule
    \end{tabular}
\end{table}

\section{Proof for the Main Results}\label{appsec:proof}

\begin{theorem}
\label{thm:optim_is_reverse_process}
    Suppose that the behavior policy is a diffusion model that approximates the forward process $q$ by minimizing the objective
    \begin{equation}
    \label{eq:diffusion_elbo}
        \mathbb{E}_{a^0\sim \nu(s), a^n\sim q_{n|0}(\cdot|a^0)}\left[\KL{q_{n-1|0,n}(\cdot|a^0,a^n)}{p^{\nu,s}_{n-1|n}(\cdot|a^n)}\right]
    \end{equation}
    for $n\in\{0,1,\ldots,N\}$.
    The optimal solution $p_{n-1|n}^{*,s}$ of \eqref{problem:path_diff} is the reverse process corresponding to the forward process $q_{0:N}^{\pi^*,s}=\pi^*(a^0|s)\prod_{n=1}^Nq_{n|n-1}(a^n|a^{n-1})$, where $\pi^*(a|s)\propto\nu(a|s)\exp(Q(s,a)/\eta)$.
\end{theorem}
\begin{proof}
First, we note that the Markov process $p_{0:N}^{\nu,s}$ is the reverse process corresponding to forward diffusion process $q^{\nu,s}(u_{0:N})=\nu(a^0|s)\prod_{n=1}^Nq(a^n|a^{n-1})$, i.e.,
\begin{equation}
\label{eq:nu_forward_eq_reverse}
    p^{\nu,s}_{n-1|n}=q^{\nu,s}_{n-1|n}
\end{equation}
where 
\begin{equation}
\label{def:qn-1n}
    q^{\nu,s}_{n-1|n}(a^{n-1}|a^n)\coloneqq\frac{\mathbb{E}_{a^0\sim\nu(\cdot|s)}[q_{n-1|0}(a^{n-1}|a^0)q_{n|n-1}(a^n|a^{n-1})]}{\mathbb{E}_{a^0\sim\nu(s)}[q_{n|0}(a^n|a^0)]}.
\end{equation}
This follows from the fact that $p_{n-1|n}^{\nu,s}$ minimizes the objective in \eqref{eq:diffusion_elbo}). Solving the constrained optimization problem
$$
\begin{aligned}
    &\min_{p_{n-1|n}^{\nu,s}(\cdot|a^n)}\mathbb{E}_{a^0\sim \nu(\cdot|s)}\left[q_{n|0}(a^n|a^0)\KL{q_{n-1|0,n}(\cdot|a^0,a^n)}{p^{\nu,s}_{n-1|n}(\cdot|a^n)}\right]\\
    &\textrm{s.t. }\int p_{n-1|n}^{\nu,s}(a^{n-1}|a^n)\mathrm{d}a^{n-1}=1
\end{aligned}
$$
for each $a^n$ yields
$$
    p^{\nu,s}_{n-1|n}(a^{n-1}|a^n)=\frac{\mathbb{E}_{a^0\sim\nu(\cdot|s)}[q_{n|0}(a^n|a^0)q_{n-1|0,n}(a^{n-1}|a^0,a^n)]}{\mathbb{E}_{a^0\sim\nu(\cdot|s)}[q_{n|0}(a^n|a^0)]}.
$$
\eqref{eq:nu_forward_eq_reverse}) then follows trivially.

% \eqref{eq:nu_forward_eq_reverse}) ensures that the reverse process $p_{0:N}^{\nu,s}$ and the forward process $q^{\nu,s}(u_{0:N})\coloneqq\nu(a^0|s)\prod_{n=1}^Nq(a^n|a^{n-1})$ match in distribution. We are free to replace the reverse process in the KL divergence $\KL{p_{0:N}^{\pi,s}}{p_{0:N}^{\nu,s}}$ with the forward process.

Then, we will show that the equality
$p^{*,s}_{n-1|n}=q^{s,\pi^*}_{n-1|n}$ holds for $$\pi^*(a|s)=\frac{\nu(a|s)\exp(Q(s,a)/\eta)}{\mathbb{E}_{a\sim \nu(\cdot|s)}[\exp(Q(s,a)/\eta)]}.$$ 

Note that \eqref{problem:path_diff} on every state $s$ constitutes a 
standard maximum entropy reinforcement learning problem \citep{sac,brac,xql} with ending reward $Q(s, a^0)$ and no intermediate rewards. In such cases, the optimal diffusion value function satisfies the following recursion,
$$V^{*,s}_0(a^0)=Q(s,a^0)$$
$$
    V_{n}^{*,s}(a^n)=\eta\log\mathbb{E}_{a^{n-1}\sim p^{\nu,s}_{n-1|n}(\cdot|a^n)}\left[\exp(V_{n-1}^{*,s}(a^{n-1})/\eta)\right].
$$
Expanding the recursion, we have
\begin{equation}
\label{eq:V_n}
    V_n^{*,s}(a^n)=\eta\log\mathbb{E}_{a^0\sim p^{\nu,s}_{0|n}(\cdot|a^n)}\left[\exp(Q(s,a^0)/\eta)\right].
\end{equation}
The optimal policy of the maximum entropy reinforcement learning problem is a Boltzmann distribution defined as follows,
$$
    p^{*,s}_{n-1|n}(a^{n-1}|a^n)=\frac{p_{n-1|n}^{\nu,s}(a^{n-1}|a^n)\exp(V_{n-1}^{*,s}(a^{n-1})/\eta)}{\mathbb{E}_{a^{n-1}\sim p^{\nu,s}_{n-1|n}(\cdot|a^n)}\left[\exp(V_{n-1}^{*,s}(a^{n-1})/\eta)\right]}.
$$
Substituting $p_{n-1|n}^{\nu,s}$ and $V_{n-1}^{*,s}$ with Equations~(\ref{eq:nu_forward_eq_reverse}) and (\ref{eq:V_n}), we get
\begin{equation}\label{appeq:cep}
    p^{*,s}_{n-1|n}(a^{n-1}|a^n)=\frac{q^{\nu, s}_{n-1|n}(a^{n-1}|a^n)\mathbb{E}_{a^{0}\sim q^{\nu, s}_{0|n-1}(\cdot|a^{n-1})}\left[\exp(Q(s, a^0)/\eta)\right]}{\mathbb{E}_{a^0\sim q_{0|n}^{\nu, s}(\cdot|a^n)}\left[\exp(Q(s, a^0))/\eta\right]},
\end{equation}
where
$$
\mathbb{E}_{a^0\sim q_{0|n}^{\nu, s}(\cdot|a^n)}\left[\exp(Q(s, a^0))/\eta\right]=\int q^{\nu, s}_{n-1|n}(a^{n-1}|a^n)\mathbb{E}_{a^{0}\sim q^{\nu, s}_{0|n-1}(\cdot|a^{n-1})}\left[\exp(Q(s, a^0)/\eta)\right]\mathrm{d}a^{n-1}.
$$
Unfolding the definition of $q^{\nu, s}_{n-1|n}(a^{n-1}|a^n)$ (\eqref{def:qn-1n}), we have
$$
\begin{aligned}
        p^{*,s}_{n-1|n}(a^{n-1}|a^n)&=\frac{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\pi^*,s}_{n-1|0}(a^{n-1}|a^0)q^{\pi^*,s}_{n|n-1}(a^n|a^{n-1})]}{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\pi^*,s}_{n|0}(a^n|a^0)]}.\\
    \end{aligned}
$$
% Since we have
% \begin{equation}\label{appeq:idem}
%     \begin{aligned}
%         &\mathbb{E}_{a^0\sim q_{0|n}^{\nu, s}(\cdot|a^n)}\left[\exp (Q(s, a^0)/\eta)\right]\\
%         &=\int q^{\nu,s}_{0|n}(a^0|a^n)\exp(Q(s, a^0)/\eta)\mathrm{d}a^0\\
%         &=\int \frac{q^{\nu,s}_{0}(a^0)q^{\nu,s}_{n|0}(a^n|a^0)}{q^{\nu,s}_n(a^n)}\exp(Q(s, a^0)/\eta)\mathrm{d}a^0\\
%         &=\int q_0^{\nu, s}(a^0)\frac{\exp(Q(s,a^0)/\eta)}{Z(s)}\frac{q^{\nu,s}_{n|0}(a^n|a^0)}{q^{\nu,s}_{n}(a^n)}Z(s)\mathrm{da^0}\\
%         &=\mathbb{E}_{a^0\sim \pi^*(\cdot|s)}\left[\frac{q^{\nu,s}_{n|0}(a^n|a^0)}{q^{\nu,s}_{n}(a^n)}Z(s)\right],
%     \end{aligned}
% \end{equation}
% where $q^{\nu,s}_0(\cdot)=\nu(\cdot|s)$ and $ Z(s)=\mathbb{E}_{\nu}[\exp(Q(s, a^0)/\eta)]$ is the normalizing factor. 

% Substituting \eqref{appeq:idem} into \eqref{appeq:cep} for $n$ and $n-1$, we have
% \begin{equation}
%     \begin{aligned}
%         p^{*,s}_{n-1|n}(a^{n-1}|a^n)&=\frac{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}\left[q^{\nu, s}_{n-1|n}(a^{n-1}|a^n)\frac{q^{\nu,s}_{n-1|0}(a^{n-1}|a^0)}{q^{\nu,s}_{n-1}(a^{n-1})}Z(s)\right]}{\mathbb{E}_{a^0\sim\pi^*}\left[\frac{q^{\nu,s}_{n|0}(a^n|a^0)}{q^{\nu,s}_{n}(a^n)}Z(s)\right]}\\
%         &=\frac{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\nu,s}_{n-1|0}(a^{n-1}|a^0)q^{\nu,s}_{n|n-1}(a^n|a^{n-1})]}{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\nu,s}_{n|0}(a^n|a^0)]}\\
%         &=\frac{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\pi^*,s}_{n-1|0}(a^{n-1}|a^0)q^{\pi^*,s}_{n|n-1}(a^n|a^{n-1})]}{\mathbb{E}_{a^0\sim\pi^*(\cdot|s)}[q^{\pi^*,s}_{n|0}(a^n|a^0)]},\\
%     \end{aligned}
% \end{equation}
% where the last equation follows from the fact the forward process is the same for $\nu$ and $\pi^*$ except for step $n=0$. 

Thus, we conclude that the optimal diffusion policy $p^{*,s}_{0:N}$ matches in distribution with the forward diffusion process $q^{s,\pi^*}_{0:N}$ originating from the initial distribution $\pi^*(s)$. In other word, $\pi^*(a|s)=\int p^{*,s}(u_{0:N})\delta(a^0-a)\mathrm{d}u_{0:N}$. Since $\pi^*$ is the optimal solution of the KL regularized objective in \eqref{eq:expected_pi}), the proof is completed.

\end{proof}

\begin{theorem}[Theorem 4.2 in the main text]\label{thm:pathwise_kl_equivalence}
    Let $p^\nu$ be the reference behavior diffusion process. The optimal diffusion policy $p^*$ of the pathwise KL-regularized RL in \eqref{eq:pathwise_kl_obj} is also the optimal policy $\pi^*$ of the KL regularized RL problem in \eqref{eq:brl_obj} in the sense that $\pi^*(a|s)=\int p^{*, s}_{0:N}(a^{0:N})\delta(a-a^0)\mathrm{d}a^{0:N} \forall s\in\mathcal{S}$, where $\delta$ is the Dirac delta function. 
\end{theorem}
\begin{proof}
    Denote the optimal state-action value function and state value functions of the pathwise KL-regularized RL as $Q^*$ and $V^*$, respectively. According to Theorem~\ref{thm:optim_is_reverse_process}, for any state $s\in\mathcal{S}$, $p^{*,s}$ is the reverse process of the forward diffusion process $q^{\pi^*,s}$. Since the $p^{\nu,s}$ is the reverse process of $q^{\nu,s}$, we have
    $$
        \KL{p^{*,s}_{0:N}}{p^{\nu,s}_{0:N}}=\KL{q^{\pi^*,s}_{0:N}}{q^{\nu,s}_{0:N}}.
    $$
    Notice that $q^{\pi^*,s}_{0:N}$ and $q^{\nu,s}_{0:N}$ share the same transition kernel and differ only on the initial distribution. Hence, conditioned on the initial action $a^0$, they match in distribution,
    $$
        \KL{q_{1:N|0}^{\pi^*,s}(a^0)}{q_{1:N|0}^{\nu,s}(a^0)}=\mathbb{E}\left[\log\frac{\prod_{n=1}^Nq(a^n|a^{n-1})}{\prod_{n=1}^Nq(a^n|a^{n-1})}\right]=0, \forall a^0\in\mathcal{A}.
    $$
    Therefore, the KL divergence between diffusion processes $q^{\pi^*,s}_{0:N}$ and $q_{0:N}^{\nu,s}$ equals the KL divergence between $\pi^*(s)$ and $\nu(s)$,
    $$
        \KL{p^{*,s}_{0:N}}{p^{\nu,s}_{0:N}}=\KL{q^{\pi^*,s}_{0:N}}{q^{\nu,s}_{0:N}}=\KL{\pi^*(s)}{\nu(s)}+\mathbb{E}\left[\KL{q_{1:N|0}^{\pi^*,s}}{q_{1:N|0}^{\nu,s}}\right]=\KL{\pi^*(s)}{\nu(s)}.
    $$
    We conclude the proof by showing that the optimal soft value functions $Q^*$ and $V^*$ of the optimal diffusion policy satisfy the Bellman optimality equations of the KL regularized objective in \eqref{eq:brl_obj},
    $$
        \begin{aligned}
            &V^*(s)=\mathbb{E}_{a\sim p^{*,s}_0}\left[Q^*(s,a)\right]+\eta\KL{p^{*,s}_0}{\nu(s)}\\
            &Q^*(s,a)=R(s,a)+\gamma \mathbb{E}_{s'\sim T(s,a)}[V^*(s')].
        \end{aligned}
    $$
    Here, the second equality is trivially satisfied. The first equality is satisfied because we have $\KL{p_0^{*,s}}{\nu(s)}=\KL{\pi^*(s)}{\nu(s)}=\KL{p_{0:N}^{*,s}}{p_{0:N}^{\nu,s}}$, and
    $$
        V^*(s)=\mathbb{E}_{a\sim p^{*,s}_0}\left[Q^*(s,a)\right]+\eta\KL{p_{0:N}^{*,s}}{p_{0:N}^{\nu,s}}
    $$
    holds by definition.
\end{proof}



\begin{lemma}[Soft Policy Evaluation~(adapted from \citet{sac})]
\label{lemma:soft_policy_eval}
Consider the soft Bellman backup operator $\mathcal{B}^\pi$ in \eqref{eq:upper_critic} and a function $Q^0:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$. Define $Q^{k+1}=\mathcal{B}^\pi Q^{k}$. Then the sequence $Q^k$ will converge to the soft Q-value of $\pi$ as $k\to\infty$ under Assumption~\ref{ass}.

\end{lemma}
\begin{proof}
    The proof is essentially the same as that presented in \citet{sac} except that we employ a bounded probability ratio to ensure the boundedness of the Q value function rather than requiring the action space to be a finite set.
\end{proof}

\begin{proposition}[Policy Improvement~(adapted from \citet{sac})]\label{proposition:policy_improvement}
Let $p^{\pi_{\textrm{new}}}$ be the optimizer of the optimization problem defined in \eqref{problem:step_diff}. Then, $V^{\pi_{\textrm{new}},s}_n(a)\geq V^{\pi_{\textrm{old}},s}_n(a)$ under Assumption~\ref{ass}.
\end{proposition}
\begin{proof}
    The optimizer $p^{\pi_\textrm{new},s,a^n}_{n-1|n}$ of the optimization problem \eqref{problem:step_diff} is guaranteed to satisfy
    $$
        -\eta\ell^{\pi_\textrm{new},s}_n(a^n) + \underset{p^{\pi_\textrm{new},s,a^n}_{n-1|n}}{\mathbb{E}}\left[V^{\pi_\textrm{old},s}_{n-1}(a^{n-1})\right]\geq -\eta\ell^{\pi_\textrm{old},s}_n(a^n) + \underset{p^{\pi_\textrm{old},s,a^n}_{n-1|n}}{\mathbb{E}}\left[V^{\pi_\textrm{old},s}_{n-1}(a^{n-1})\right]=V^{\pi_\textrm{old},s}_{n}(a^{n}).
    $$
    Recursively applying the above inequality to $V^{\pi_\textrm{old},s}_{n}(a^{n})=-\eta\ell^{\pi_\textrm{old},s}_n(a^n) + \underset{p^{\pi_\textrm{old},s,a^n}_{n-1|n}}{\mathbb{E}}\left[V^{\pi_\textrm{old},s}_{n-1}(a^{n-1})\right]$, we get
    $$
    \begin{aligned}
        V^{\pi_\textrm{old},s}_{n}(a^{n})&\leq-\eta\ell^{\pi_\textrm{old},s}_n(a^n) + \underset{p^{\pi_\textrm{old},s,a^n}_{n-1|n}}{\mathbb{E}}\left[-\eta\ell^{\pi_\textrm{new},s}_{n-1}(a^{n-1}) + \underset{p^{\pi_\textrm{new},s,a^{n-1}}_{n-2|n-1}}{\mathbb{E}}\left[V^{\pi_\textrm{old},s}_{n-2}(a^{n-2})\right]\right]\\
        &\leq \underset{p^{\pi_\textrm{old},s,a^n}_{0:n-1|n}}{\mathbb{E}}\left[-\eta\sum_{i=1}^{n}\ell^{\pi_\textrm{new},s}_{i}(a^{i}) + Q^{\pi_{\textrm{old}}}(s,a^0)\right]\\
        &\leq \underset{p^{\pi_\textrm{old},s,a^n}_{0:n-1|n}}{\mathbb{E}}\left[-\eta\sum_{i=1}^{n}\ell^{\pi_\textrm{new},s}_{i}(a^{i}) + R(s,a^0)+\gamma\mathbb{E}_{s'\sim T(s,a^0)}\left[-\eta\sum_{i=1}^N\ell_i^{\pi_{\textrm{new}},s'}(a'^i)+ Q^{\pi_{\textrm{old}}}(s,a'^0)\right]\right]\\
        &\leq \ldots\\
        &\leq V_n^{\pi_{\textrm{new}}, s}(a^n),
    \end{aligned}
    $$
    where the convergence to $V_{n}^{\pi_{\textrm{new}},s}$ is guaranteed by the Lemma~\ref{lemma:soft_policy_eval}.
\end{proof}


\begin{proposition}[Soft Policy Iteration~(adapted from \citet{sac})]\label{proposition:policy_iteration}
Under Assumption~\ref{ass}, repeated application of soft policy evaluation in \eqref{eq:upper_critic} and \eqref{eq:intermediate_value} and soft policy improvement in \eqref{problem:step_diff} from any $p^\pi\in\Pi$ converges to a policy $p^{\pi^*}$ such that $V_n^{\pi^*,s}(a)\geq V_n^{\pi,s}(a)$ for all $p^\pi\in\Pi$, $n\in\{0,1,\ldots,N\}$, and $(s,a)\in\mathcal{S}\times\mathcal{A}$.
\end{proposition}
\begin{proof}
The proof duplicates that in \citet{sac}.
    % Let $p^{\pi_i}$ be the diffusion policy at the $i$-th iteration. Proposition~\ref{proposition:policy_improvement} ensures the monotonic improvement of $V^{\pi_i,s}_n(a^n)$. The monotone convergence theorem guarantees the convergence of $p^{\pi_i}$ since the soft value function $V^{\pi_i,s}_n$ is bounded for $p^\pi\in\Pi$.
    
    % We will still need to
% show that π∗ is indeed optimal. At convergence, it must be case that Jπ∗ (π∗( · |st)) < Jπ∗ (π( · |st)) for all π ∈ Π, π 6 = π∗.
% Using the same iterative argument as in the proof of Lemma 2, we get Qπ∗
% (st, at) > Qπ (st, at) for all (st, at) ∈ S × A,
% that is, the soft value of any other policy in Π is lower than that of the converged policy. Hence π∗ is optimal in Π
\end{proof}


\section{Continuous-Time Perspective of the Pathwise KL}\label{appsec:continuous_time}
\citet{diffusion_sde} builds the connection between diffusion models and the reverse stochastic differential equations (SDEs) that gradually remove noises from data and transform a prior noise distribution into the target distribution. Based on the SDE understanding of diffusion models, we aim to investigate whether the introduced pathwise KL can be extended to continuous-time diffusion modeling.  

We define the forward diffusion SDE, which perturbs data into prior distribution, as the following:
\begin{equation}\label{eq:forward_sde}
    \mathrm d x = f(x, t)\mathrm d t + g(t)\mathrm dw,
\end{equation}
where $w$ is the standard Wiener process, $f$ is the drift coefficient, and $g$ is the diffusion coefficient. The reverse process of this SDE is also a diffusion SDE:
\begin{equation}\label{eq:reverse_sde}
    \mathrm d x = \left[f(x,t)-g(t)^2 \nabla_x \log p_t(x)\right]\mathrm dt + g(t)\mathrm d \bar{w},
\end{equation}
where $\bar{w}$ is the reverse-time standard Wiener process and $\mathrm dt$ is a infinitesimal \textit{negative} time interval. In practice, we can use a parameterized neural network $s^\theta(x, t)$ to estimate the score of the marginal distribution using score matching:
\begin{equation}
    \begin{aligned}
        \theta^*=\argmin_{\theta}\mathbb{E}_{t, x(0),x(t)|x(0)}\left[\lambda(t)\|s_\theta(x(t), t)-\nabla_{x(t)}\log q_{t|0}(x(t)|x(0))\|^2\right],
    \end{aligned}
\end{equation}
where $q_{t|0}$ is the distribution of $x(t)$ given the initial sample $x(0)$. After the training is completed, we can substitute the score function in \eqref{eq:reverse_sde} and solve the reverse SDE to obtain samples that follow the target distribution. 

We consider two forward SDEs with the same drift and diffusion coefficient, but with different initial distributions $q_0^\pi$ and $q_0^\nu$. Let the approximated score networks be $s^\pi$ and $s^\nu$, and therefore we consider the following reverse SDEs:
\begin{enumerate}
\item $\mathrm d x = \left[f(x,t)-g(t)^2 s^\pi(x, t)\right]\mathrm dt + g(t)\mathrm d \bar{w}$ under measure $\mathbb{P}^\pi$, 
\item $\mathrm d x = \left[f(x,t)-g(t)^2 s^\nu(x, t)\right]\mathrm dt + g(t)\mathrm d \bar{w}$ under measure $\mathbb{P}^\nu$.
\end{enumerate}
Our goal is to compute the KL divergence between the distributions of the two reverse SDEs. Using Girsanov's theorem~\citep{oksendal2013stochastic}, we have
\begin{equation}\label{appeq:sde_kl}
    \begin{aligned}
        \KL{\mathbb{P^\pi}}{\mathbb{P}^\nu}&=\mathbb{E}_{\mathbb{P}^\pi}\left[\log \frac{\mathrm{d}\mathbb{P}^\pi}{\mathrm{d}\mathbb{P}^\nu}\right]\\
        &=\mathbb{E}_{\mathbb{P}^\pi}\left[\int_{0}^T\frac{[[f(x,t)-g(t)^2s^\pi(x, t)]-[f(x, t)-g(t)^2s^\nu(x, t)]]^2}{2g(t)^2}\mathrm{d}t\right]\\
        &=\mathbb{E}_{\mathbb{P}^\pi}\left[\int_{0}^T\frac{g(t)^4\|s^\pi(x, t)0s^\nu(x, t)\|^2}{2g(t)^2}\mathrm{d}t\right]\\
        &=\mathbb{E}_{\mathbb{P}^\pi}\left[\int_{0}^T\frac{g(t)^2}2\|s^\pi(x, t)-s^\nu(x, t)\|^2\mathrm{d}t\right]. 
    \end{aligned}
\end{equation} 
Instantiating the SDE with Variance Preserving noise schedule~\citep{diffusion_sde}:
\begin{equation}
    \begin{aligned}
        f(x, t) &= \frac 12 \beta(t)x\\
        g(t) &= \sqrt{\beta(t)},
    \end{aligned}
\end{equation}
We have
\begin{equation}
    \begin{aligned}
        \KL{\mathbb{P}^\pi}{\mathbb{P}^\nu}&=\mathbb{E}_{\mathbb{P}_\pi}\left[\int_0^T\frac{\beta(t)}{2}\|s^\pi(x, t)-s^\nu(x, t)\|^2\mathrm{d}t\right].
    \end{aligned}
\end{equation}

Equation~\ref{appeq:sde_kl} presents the analytical form of the KL divergence between two reverse diffusion processes. In the following content, we continue to demonstrate that the pathwise KL introduced in this paper corresponds to \eqref{appeq:sde_kl} in the limit of $N\to\infty$. 

Examining the pathwise KL objective defined in~\eqref{eq:pathwise_kl}, 
\begin{equation}
    \begin{aligned}
        &\KL{p^{\pi}_{0:N}}{p^{\nu}_{0:N}}\\
        &=\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^{N}\KL{p^{\pi,x^n}_{n-1|n}(x^{n-1})}{p^{\nu,x^n}_{n-1|n}(x^{n-1})}\right]\\
        &=\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\|\mu^\pi(x^n, n)-\mu^\nu(x^n, n)\|^2}{2\sigma_n^2}\right]\\
        % &=\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\|\mu^\pi(x, n)-\mu^\nu(x, n)\|^2}{2\sigma_n^2}\right]\\
        &=\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\beta_n^2}{2\sigma_n^2(1-\beta_n)}\|s_n^\pi(x^n)-s_n^\nu(x^n)\|^2\right]\\
        &=\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\beta_n(1-\bar{\alpha}_n)}{2(1-\bar{\alpha}_{n-1})(1-\beta_n)}\|s_n^\pi(x)-s_n^\nu(x)\|^2\right].
    \end{aligned}
\end{equation}
where $s_n$ is the score function at step $n$. 

Define $\Delta t=\frac TN$, by \citet{diffusion_sde}, we can show that in the limit of $N\to\infty$, we have $\beta_n=\beta(\frac {nT}N)\Delta t\to0$, $x^n=x(\frac {nT}N)$, and $s_n(x^n)=s(x(\frac {nT}N), \frac {nT}N)$. Therefore, 
\begin{equation}
    \begin{aligned}
        \lim_{N\to\infty}\KL{p^{\pi}_{0:N}}{p^{\nu}_{0:N}}&=\lim _{N\to\infty}\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\beta_n(1-\bar{\alpha}_n)}{2(1-\bar{\alpha}_{n-1})(1-\beta_n)}\|s_n^\pi(x)-s_n^\nu(x)\|^2\right]\\
        &=\lim _{N\to\infty}\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\beta_n}{2}\|s_n^\pi(x^n)-s_n^\nu(x^n)\|^2\right]\\
        &=\lim _{N\to\infty}\mathbb{E}_{p^{\pi}_{0:N}}\left[\sum_{n=1}^N \frac{\beta(\frac {nT}N)}{2}\|s^\pi(x(\frac {nT}N), \frac {nT}N)-s^\nu(x(\frac {nT}N), \frac {nT}N)\|^2\Delta t\right]\\
        &=\mathbb{E}_{p^\pi}\left[\int_0^T\frac{\beta(t)}{2}\|s^\pi(x(t), t)-s^\nu(x(t), t)\|^2\mathrm{d}t\right].
    \end{aligned}
\end{equation}
which exactly corresponds to the KL divergence of SDE in \eqref{appeq:sde_kl}. 

\section{Supplementary Experiment Results}\label{appsec:experiment}

\subsection{Discussions about Training Time}
A systematic breakdown and comparison of the runtime between \algbb, DAC and Diffusion-QL is illustrated in Figure~\ref{appfig:runtime}. We evaluate \algbb, DAC and Diffusion-QL with workstations equipped with NVIDIA RTX 4090 cards and the \textit{walker2d-medium-replay-v2} dataset. 

Our method consists of three distinct subroutines. The first one is pre-training the behavior diffusion with standard diffusion loss. Our observation is that this phase accounts for a minor fraction (about 8 minutes) of the overall runtime, so we ignore it in the following discussion. The second one is training value functions, including $Q^\pi$ and $V^\pi$. The approach to training $Q^\pi$ is identical to established methods such as Diffusion-QL~\citep{dql} and DAC~\citep{dac}, which is essentially sampling diffusion paths at the next state $s'$ and calculating the temporal difference target. In addition to $Q^\pi$, Our method additionally trains $V^\pi$. However,  this supplementary computation is not resource-intensive, as it only requires single-step diffusion to compute the training target $V^\pi$. Therefore, the additional cost of training diffusion value functions $V^\pi$ is a constant that does not scale with diffusion steps. The third subroutine is training the actor. Akin to EDP~\citep{edp}, \algbb only requires a single-step diffusion rollout, and thus the cost of training the actor is also a constant that does not scale with diffusion steps. In contrast, Diffusion-QL needs to differentiate with respect to the whole diffusion path, which causes a drastic increase in actor runtime. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/appfig_runtime.pdf}
    \caption{Algorithm Runtime of \algbb, DAC and Diffusion-QL. }
    \label{appfig:runtime}
\end{figure}

\subsection{Generation Path On Synthetic 2D Tasks}
We plot the generation results of \algbb on all synthetic 2D datasets in Figure~\ref{appfig:2d_actor}. Throughout these datasets, we witness a close resemblance between the final sample distribution and the ground truth target distribution as depicted in Figure~\ref{appfig:2d_sample}. The sampling begins with gradual movements across the 2D plane and converges in later diffusion steps. This pattern is further demonstrated by the diffusion value functions, which offer weaker guidance during the initial steps but provide stronger guidance in the final steps. This is in consistency with the exact energy guidance~\citep{qgpo}, which demonstrates that our guidance delivers generation results that exactly conform with the target distribution, while other types of guidance cannot (see~\citet{dql} for details). 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/appfig_toy2d_actor.pdf}
    \caption{Illustration of the diffusion policy and the diffusion value function from \algbb on synthetic 2D datasets. The regularization strength is set to $\eta=0.06$. }
    \label{appfig:2d_actor}
\end{figure}

\subsection{Training Curves of D4RL Datasets}\label{appsec:curves}
The curves of evaluation scores on D4RL datasets are presented in Figure~\ref{appfig:mujoco} and Figure~\ref{appfig:antmaze}. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/appfig_mujoco.pdf}
    \caption{Evaluation scores of \algbb on D4RL Locomotion datasets. Results are aggregated using 5 independent seeds and 10 evaluation episodes for each seed. }
    \label{appfig:mujoco}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/appfig_antmaze.pdf}
    \caption{Evaluation scores of \algbb on D4RL Locomotion datasets. Results are aggregated using 5 independent seeds and 100 evaluation episodes for each seed. }
    \label{appfig:antmaze}
\end{figure}
