
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{images/5.1_data.pdf}
    \caption{Illustration of the \textit{8gaussians} and \textit{2spirals} dataset and the re-sampled result with $\eta=0.06$.}\label{fig:2d_data}
\end{figure}

We evaluate \algbb with synthetic 2D tasks and also the D4RL benchmark~\citep{d4rl}. Due to the space limit, a detailed introduction about these tasks and the hyper-parameter configurations are deferred to Section~\ref{appsec:intro_benchmark} and Section~\ref{appsec:hyperparameters}. More experimental results, such as the comparison of computational efficiency, are deferred to Appendix~\ref{appsec:experiment}.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/abla_arch.pdf}
    \caption{Comparison of different policy parameterizations. Results are taken from 10 evaluation episodes and 5 seeds. }
    \label{exp:abla_arch}
\end{figure}

\subsection{Results of Synthetic 2D Datasets}
We leverage synthetic 2D datasets used in \citet{qgpo} to further our understanding of the mechanism of \algbb. Each dataset contains data points $x_i$ paired with specific energy values $Q(x_i)$. By re-sampling the data according to the Boltzmann distribution $ p(x_i)\propto \exp(Q(x_i)/\eta)$, we can obtain the ground truth target distribution at a specific regularization strength $\eta$ \citep{sac,awac}. As an example, Figure~\ref{fig:2d_data} illustrates both the original dataset and the re-sampled dataset with $\eta=0.06$ for the \textit{8gaussians} and \textit{2spirals} datasets. 

Our objective is to analyze the properties of the diffusion policy and the diffusion value function trained with \algbb. The results are depicted in Figure~\ref{fig:2d_actor} (full results on all datasets in Figure~\ref{appfig:2d_actor}), where each row corresponds to a different dataset and visualizes the iterative sampling process. From left to right, the samples evolve from an initial noisy distribution toward a well-defined structure as the reverse diffusion progresses. In the initial steps ($n=50$ to $n=30$), the sample movement is subtle, while in the later steps ($n=30$ to $n=0$), the samples rapidly converge to the nearest modes of the data. The final column on the right presents the ultimate samples generated by the policy, which closely match the ground-truth target distribution visualized in Figure~\ref{fig:2d_data}. 

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/abla_eta.pdf}
        \caption{Sensitivity analysis of $\eta$.}
        \label{exp:abla_eta}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/abla_rho.pdf}
        \caption{Sensitivity analysis of $\rho$.}
        \label{exp:abla_rho}
    \end{subfigure}
    \caption{Sensitivity analysis of (a) the regularization strength $\eta$ and (b) the lower confidence bound coefficient $\rho$ of \algbb. For each configuration, we report the mean and the standard deviation of the performances aggregated from 5 independent seeds and 10 evaluation episodes for each seed. }
\end{figure*}


Besides the generation result, the background color in Figure~\ref{fig:2d_actor} depicts the landscapes of the diffusion value functions $V^\pi$ at $n=50/30/15/5/0$. In earlier, noisier steps (e.g., $n=50$), the output values across the space are similar, resulting in a smoother landscape and weaker guidance during sampling. As the noise level decreases in later steps (e.g., $n=5$), the value outputs vary more sharply, creating stronger guidance on the diffusion policy. Such progression enables the model to refine samples while exploring sufficiently across the space, finally yielding results that align with the target distribution.

\subsection{Results on Continuous Control Benchmark}

To assess the performance of \algbb on complex continuous control problems, we utilize datasets from D4RL as our test bench and compare \algbb against a wide spectrum of offline RL algorithms. Specifically, we include 1) CQL~\citep{cql} and IQL~\citep{iql}, which are representatives using straightforward one-step policies; 2) Decision Diffuser (DD)~\citep{dd}, which uses diffusion models for planning and extracts actions from planned trajectories; 3) SfBC~\citep{sfbc}, IDQL~\citep{idql}, and QGPO~\citep{qgpo}, which use diffusion to model the behavior and further refine actions using the value functions; 4) Diffusion-QL~\citep{dql}, which propagates the gradient of Q-value functions over the whole generation process; and 5) DAC~\citep{dac}, which aligns the output of diffusion policies with the gradient of Q-values. 



The results are listed in Table~\ref{tab:d4rl}. Our findings indicate that diffusion-based methods, particularly those with diffusion-based actor and regularization (including \algbb, DAC and Diffusion-QL), substantially outperform their non-diffusion counterparts especially in locomotion tasks. Meanwhile, \algbb consistently achieves superior performance across nearly all datasets, underscoring the effectiveness of combining diffusion policies and behavior-regularized RL framework. Besides the final performance, we plot the training curves of \algbb in Section~\ref{appsec:curves}. Overall, \algbb achieves fast and stable convergence except for some of the antmaze datasets, where variations may occur due to the sparse reward nature of these datasets. 

\subsection{Analysis of \algbb}
\textbf{Policy Parameterization. }As discussed in Section~\ref{sec:intro}, improper assumptions about the action distribution may lead to inaccurate penalty calculations and ultimately degrade the overall performance. To validate this claim, we conduct a series of ablation studies that replace the actor parameterization with Gaussian distributions and Dirac distributions, respectively, while keeping other experimental configurations unchanged. For a fair comparison, we fine-tune the regularization strength $\eta$ by searching within a predefined range and report the performance corresponding to the optimal value. The results are illustrated in Figure~\ref{exp:abla_arch}, with additional details regarding the implementations provided in Appendix~\ref{appsec:abla_arch_detail}. While simpler architectures generally converge more quickly, their performance at convergence often lags behind that of \algbb, indicating a limitation in their capacity in policy parameterization. 

\textbf{Effect of Regularization Strength $\eta$. }The hyper-parameter $\eta$ controls the regularization strength: larger $\eta$ forces the diffusion policy to stay closer to the behavior diffusion, while smaller $\eta$ places more emphasis on the guidance from value networks. Figure~\ref{exp:abla_eta} demonstrates this effect: In general, smaller $\eta$ does lead to better performances. However, excessively small values can result in performance degradation, as observed in \textit{walker2d-medium-v2}. In contrast to auto-tuning $\eta$ via dual gradient descend~\citep{dac,dpqe}, we find that adjustable $\eta$ causes fluctuations in penalty calculation. Therefore, we employ a fixed $\eta$ throughout the training. 

\textbf{Effect of Lower Confidence Bound Coefficient $\rho$. }The hyper-parameter $\rho$ determines the level of pessimism on OOD actions since the variances of Q-values on these actions are comparatively higher than in-dataset actions. In Figure~\ref{exp:abla_rho}, we discover that there exists a certain range of $\rho$ where the lower confidence bound value target works best, while excessively larger or smaller values lead to either underestimation or overestimation in value estimation. 
