\textbf{Offline RL. }
To improve beyond the interaction experience, RL algorithms need to query the estimated values of unseen actions for optimization. In offline scenarios, such distribution shift tends to cause serious overestimation in value functions due to the lack of \textit{corrective feedback}~\citep{discor}. To mitigate this, algorithms like CQL~\citep{cql}, EDAC~\citep{EDAC}, and PBRL~\citep{pbrl} focus on penalizing the Q-values of OOD actions to prevent the over-estimation issue. Behavior regularization provides another principled way to mitigate the distribution shift problem, by adding penalties for deviation from the dataset policy during the stage of policy evaluation, policy improvement~\cite{bcq,td3bc,prdc}, or sometimes both~\citep{brac,rebrac}. Another line of research, also based on the behavior-regularized RL framework, approaches offline RL by performing in-sample value iteration~\citep{iql,xql,ivr}, thus eliminating OOD queries from the policy and directly approximating the optimal value function. Lastly, model-based offline RL methods~\citep{mopo,mobile,morec} introduce learned dynamics models that generate synthetic experiences to alleviate data limitations, providing extra generalization compared to model-free algorithms. 

\textbf{Diffusion Policies in Offline RL. }There has been a notable trend towards the applications of expressive generative models for policy parameterization~\citep{diffuser}, dynamics modeling~\citep{twm,dwm}, and trajectory planning~\citep{dt,dd,act}. In offline RL, several works employ diffusion models to approximate the behavior policy used for dataset collection. To further improve the policy, they utilize in-sample value iteration to derive the Q-values, and subsequently select action candidates from the behavior diffusion model~\citep{idql,sfbc} or use the gradient of Q-value functions to guide the generation~\citep{qgpo,diffusiondice}. However, the performances of these methods are limited, as the actions come from behavior diffusion. Alternatively, SRPO~\citep{srpo} and DTQL~\citep{dtql} maintain a simple one-step policy for optimization while harnessing the diffusion model or diffusion loss to implement behavior regularization. This yields improved computational efficiency and performance in practice; however, the single-step policy still restricts expressiveness and fails to encompass all of the modes of the optimal policy. A wide range of works therefore explore using diffusion models as the actor. Among them, DAC~\citep{dac} formulates the optimization as a noise-regression problem and proposes to align the output from the policy with the gradient of the Q-value functions. Diffusion-QL~\citep{dql} optimizes the actor by back-propagating the gradient of Q-values throughout the entire diffusion path. This results in a significant memory footprint and computational overhead, and EDP~\citep{edp} proposes to use action approximations to alleviate the cost. In contrast, \algbb maintains value functions for intermediate diffusion steps, thereby amortizing the optimization cost while also keeping the optimization precise. 