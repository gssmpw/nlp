\section{Related Works}
\label{sec:related}
\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig/framework.pdf}
   \caption{Overview of the \ourmethod. We propose mask reconstruction containing token mask and token reconstruction as a complementary task for training dring world model. \textbf{Token Mask}: we randomly sample tokens by temporal-shared $\mathcal{M}_{spatial}$ and temporal-unshared $\mathcal{M}_{time}$, specialized for spatial and temporal modeling. \textbf{Token Reconstruction}: we fill invisible tokens by diffusion-related mask tokens (Sec.\ref{sec:3.2}) and recover features by a two-branch transformer. Moreover, we introduce a row-wise mask strategy (Sec.\ref{sec:3.3}) for temporal branch. $\rho=1-r$ is used for simplicity in encoder.}
   \label{fig:framework}
\end{figure*}

\subsection{World Models}
World models aim to infer the dynamic environment and ego state from past observations for accurate future predictions and planning. Most studies achieve world understanding by enabling the model to generate realistic videos that align with physical principles. In autonomous driving area, world models primarily focus on generating controllable real-world driving scenarios. GAIA-1 \cite{gaia1} employs an autoregressive transformer to predict tokens based on past state and then leverages a diffusion decoder to generate high-quality videos. In contrast, DriveDreamer \cite{drivedreamer} directly use diffusion model to represent complex environments and generate driving videos from multi-modal inputs. Drive-WM \cite{drive-wm} extends to consistent and controllable multi-view video generation, exploring its application in end-to-end planning. GenAD \cite{genad} enhances generalization capability of world models by training on large-scale datasets and Vista~\cite{vista} achieves further improvements by introducing attention on structure and dynamic area.
%yx: done

\subsection{Diffusion Models with Self-supervised Learning}
Recently, diffusion-based methods~\cite{videodiffusionmodel, guo2023animatediff, henschel2024streamingt2v, latentdiffusion, controlnet} have become the mainstream of image and video generation. 
% By denoising Gaussian noise iteratively and leveraging attention mechanism, these approaches can generate high-quality visual results and effectively integrate conditioning information to control the generation. 
One important advancement in this field is Diffusion Transformer(DiT)~\cite{dit}. Due to its better scalability and lower computational cost, DiT has been successfully applied in various diffusion models, achieving state-of-the-art results~\cite{zheng2024viton, dive, sd3, feng2024dit4edit}. 

On the other hand, masking strategies from self-supervised learning have been effectively applied to enhance generative models. 
% For instance, MaskGiT [10] and MUSE adopt VQ-VAE to tokenize image and then leverage
% the bi-directional transformer to iteratively predict the mask tokens. 
With the development of DiT, research has focused on migrating this self-supervised approach to diffusion-based models. 
Initial works like MDT~\cite{mdt} modify DiT blocks to an asymmetric masking diffusion transformer architecture, where the encoder handles unmasked tokens only and a side-interpolater is introduced to recover the latnet to the original shape. 
Following MDT~\cite{mdt}, MaskDiT~\cite{maskdit} 
% proposes a reconstruction loss on masked patches and 
simply utilizes a learnable token to fill in the masked places. 
SD-DiT~\cite{sd_dit}, noticing the training-inference discrepancy and fuzzy relations between mask strategy and diffusion process, introduce a novel masking DiT with self-supervised discrimination.
% , which trains the encoder in a teacher-student scheme and achieves better performance in training speed and generation quality. 
Despite their succuss, none of them applied the masking diffusion to the video generation models. Our work make this attempt on driving world model by applying different design of spatial context and temporal context, which focus on scene objects and nuance motion separately.