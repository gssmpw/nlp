\section{Introduction}
\label{sec:intro}

\begin{figure*}[t]
  \centering
  \begin{subfigure}{0.66\linewidth}
      \centering
      \begin{tabular}{@{}l|ccc|cc@{}}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & & \textbf{Model Setup} & &  \\ 
        & Data scale & Framework & Multi-view & Traget\\
        % Method & Frobnability \\
        \midrule
        Drive-WM ~\cite{drive-wm} & 5h & Unet & $\surd$ & Diff \\
        DiVE ~\cite{dive} & 5h & DiT & $\surd$ & Diff \\
        GenAD ~\cite{genad} & 1740h & Unet & $\times$ & Diff \\
        Vista ~\cite{vista} & 1740h & Unet & $\times$ & Diff \\
        \midrule
        \ourmethod (Ours) & 1740h & DiT & $\surd$ & Diff+MR \\
        \bottomrule
      \end{tabular}
      \caption{Real-world multi-view driving world models.}
      \label{tab:example}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.30\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{fig/teaser.pdf}
    % \begin{figure*}[t]
    %   \centering
    %    \includegraphics[width=1\linewidth]{fig/teaser.pdf}
    %    \label{fig:teaser}
    % \end{figure*}
    %\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    % \includegraphics[width=1\linewidth{fig/teaser.pdf}
    \caption{Different context for mask reconstruction.}
    % \label{fig:short-b}
  \end{subfigure}
  \caption{(a). \ourmethod\ improve fidelity and generalization from web-scale dataset, scalable DiT architecture and  Mask Reconstruction (MR) target. (b) proposed MR apply a two branch structure for spatial context (scene objects) and temporal context (object motions)}
  \label{fig:short}
\end{figure*}

\begin{figure*}[h]
  \centering
   \includegraphics[width=1\linewidth]{fig/teaser2.pdf}
   \caption{
   Our model facilitates  zero-shot generation, consistent long-horizon prediction and multi-view video generation.}
   \label{fig:teaser2}
\end{figure*}

As an pivotal application of artificial intelligence, autonomous driving technologies, which require comprehending the surrounding environment and executing correct actions, have achieved significant advancements following the emergence of various learning models~\cite{li2022bevformer,hu2023planning} with scalability. 
However, the challenge of limited generalization to complex and varied scenarios remains unresolved for state-of-the-art methods~\cite{li2024ego}.
For example, perception may encounter performance drops~\cite{xie2024benchmarking} in cases like weather changes, scene variations and motion blur.
A promising solution for this problem is the use of world models, which directly predict environment changes under different actions. These models facilitate unraveling the complexities of data distributions and craft intricate regular patterns like human perception system~\cite{lecun2022path}.

Recently, advanced methods~\cite{genad,vista,gaia1,drive-wm,drivedreamer,drivedreamer2,drivescape} developed world models through diffusion-based generation task, capitalizing on the rapid development of advanced image generation systems~\cite{sd3,sd,svd}. 
% despite should  follow a phrase instead of a sentence
Despite generating high-fidelity results, these approaches still struggle with long-horizon prediction and zero-shot generalization. To address this, GenAD ~\cite{genad} attempts to conduct training on large-scale OpenDV-2K~\cite{genad} dataset with carefully-designed temporal modules, while VISTA~\cite{vista} further introduces explicit re-weighted generation loss on structural and moving areas. 
%Despite learning on large-scale driving videos, 
However, two problems still exist in building a generalizable world model for autonomous driving. First, the combination of large-scale training dataset with more scalable transformer architectures is still under exploration. Second, one fundamental question remains unanswered: Is diffusion-based generation sufficient to build a generalizable world model? Since diffusion loss targets at iterative de-noising, the learning of visual semantics may not be straightforward. For example, MaskDiT~\cite{maskdit} has shown diffusion models are complementary to well-known self-supervised methods~\cite{mae}, benefiting both convergence speed and generation quality.
% {\blue lyc: I think here should discuss the drawback of using diffusion methods alone.} Done

% 贡献： generalization of wm
% 方法： automask
% 问题： why automask contribues to generalization of wm
% 解决： cite some paper e.g. MaskDiT, MAE ...


Based on these insights into the diffusion pipeline and data for our driving world model, we design a new model, dubbed as MaskGWM, aiming at improving the fidelity, generalizability and long-time series prediction of the existing methods. Additionally, our model can also generate multi-view cases, by incorporating a multi-view module. We adopt Diffusion Transformer (DiT) as our backbone, which is more scalable and could take the information from a variety of datasets.
Moreover, we introduce the mask reconstruction as a complementary task for generation.
Several impactful works~\cite{mae,simmim} have demonstrated that masked autoencoder is a powerful self-supervised method for representation learning from large-scale data and it is also extended to some diffusion methods~\cite{maskdit,sd_dit,mdt} as an additional supervision to improve the models' performance. 
Additionally, the features obtained by self-supervised learning is more contextually meaningful~\cite{dino}, which can be used as an auxiliary supervision to further improve the generation quality~\cite{gaia1} 
However, integrating existing mask reconstruction for image generation into driving world models is not straightforward. There are still two questions to answer: (1) How can we enhance the synergy between diffusion model and mask reconstruction? Though the Mask reconstruction improves the contextual reasoning ability, this task contradicts with diffusion steps which include high noise ratio obscuring the feature details. (2) What kind of mask strategy should we use for video data? Different from image generation, the prediction of driving future requires an understanding of not only the objects within the scene but also their dynamic movements.





% Therefore, in this paper, we aim at solving above two questions by building a Diffusion Transformer (DiT) based world model trained on large-scale datasets and introducing mask reconstruction as a complementary task for generation. The advantage of extra mask modeling task come from the learning of local context, which come from the learning of reconstructing invisible masked features, showing effectiveness on both represent learning~\cite{mae,simmim} and diffusion model~\cite{maskdit,sd_dit,mdt}. However, integrating existing mask strategies for image generation into driving world models is not straightforward. 

Therefore, we develop several special designs to address the aforementioned issues: (1) We make use of the mask tokens to improve the synergy between mask reconstruction and diffusion models. Specifically, we propose a diffusion-related mask tokens (Sec.\ref{sec:3.2}) 
to initialized the invisible patches after DiT encoder. This special mask token can balance the learning of global and local features.
(2) We design a novel two-branch mask reconstruction strategy. For spatial modeling, we use a mask shared across all frames and reconstruct invisible tokens via spatial transformer, this mask strategy is similar to some video mask modeling methods~\cite{videomae,videomae_v2}. For temporal modeling, we introduce a frame-specific mask and recover masked tokens via temporal transformer. Unlike the spatial branch, we directly link the unaligned tokens on temporal dimension after masking, which can be seen as a shift augmentation restricted by a new-proposed row-wise policy. We find this temporal branch achieves both masked patches prediction in the temporal context and a reduction in training costs.


% {\blue Different from previous works~\cite{drive-wm,magicdrive}, we instantiates our cross-view module by view-row attention, which achieve better results and enjoy the performance improvement of augmentation from row-wise temporal mask.
% Since our mask reconstruction target at spatial-temporal modeling, we do not incorporate mask reconstruction under view context. }
% {\orange njc: similar to vista/genad, also train on opendv to improve generalization to unseen dataset.}

% {\orange njc: a fair comparison for single view and multi-view, which is not fair-compared on previous methods?}



In summary, our main contributions are:
\begin{itemize}
    \item We propose MaskGWM, a generalizable DiT-based driving world model capable of forecasting long-term futures across web-scale scenes.
    \item We introduce mask reconstruction as a complementary task for diffusion-based world model. 
    \item Comprehensive experiments on nuScene, OpenDV-2K and Waymo datasets demonstrate the superior video quality and robust generalization capabilities across extended time spans and different viewpoints.
\end{itemize}



% TODO: discuss with Drivingdojo
% TODO: discuss on DiT (more scalable, slow-converge may invariant of structure)
% TODO: discuss with MaskGiT (also with such target: diffusion and mask are complementary rather than single one)
%-------------------------------------------------------------------------

%-------------------------------------------------------------------------

% \begin{table}[h]
%   \centering
%   \begin{tabular}{l|ccc|c}
%       \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Model Setup}} & \multirow{2}{*}{\textbf{Metric}} \\ 
%         & Data scale & Framework & Multi-view & \\

%     \end{tabular}
%   \caption{Results.   Ours is better.}
%   \label{tab:example}
% \end{table}