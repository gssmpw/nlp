\section{Method}

% 1. Dit + rflow -> model config (for each block)
% 2. mask target (split blocks to encoder-decoder) -> discuss the training target of rflow
% 3. S-T Joint Block
% 4. Mview extend (attention field to rowwise, we find rowwise best and compatible with T target)
Fig.~\ref{fig:framework} illustrates an overview of the proposed pipeline.
\ourmethod~builds upon Stable Diffusion 3 (SD3)~\cite{sd3}, which is a well-studied  DiT-based Text-to-Image (T2I) generation model, and introduce additional spatial and temporal blocks to extract the cross-view and cross-frame information. Moreover, we deploy a mask reconstruction module during training to improve the performance of our model. In this section, we first briefly review our DiT-based driving world model in Sec.~\ref{sec:3.1}. Then, Sec.~\ref{sec:3.2} details the pipeline for mask reconstruction and introduces novel diffusion-related mask tokens designed to enhance the synergy between diffusion generation and mask reconstruction. Following this, Sec.~\ref{sec:3.3} describes the extension of mask reconstruction to the temporal dimension. Finally, in Sec.~\ref{sec:3.4}, we describe the details of our cross-view module. 
% TODO: how to introduce MIM?
\subsection{Preliminaries}
\label{sec:3.1}

% In this subsection, we first present our flow-matching based diffusion scheduler, which is tightly interconnected with proposed masking strategy, then, we elaborate how to lift a T2I model to an autonomous driving world model. 

\noindent\textbf{Diffusion Models.}
A mutli-view video sampled from a video dataset $p_{data}$ can be represented as $x_0 \sim p_{data}$, where $x_0 \in \mathrm{R}^{T\times K \times C \times H \times W}$ is a sequence of $T$ frames with view $K$, height $H$ and width $W$. We first transform $x_0$ into video tokens $z_0 = \mathrm{P}(\Theta(x_0)) \in \mathrm{R}^{T\times K \times C \times \hat{H} \times \hat{W}}$ via latent encoder $\Theta$ and patch encoding $\mathrm{P}$. \ourmethod\ applies Rectified Flow~\cite{sd3,liu2022flow,lipman2022flow} to model the generation process. Specifically, given a standard normal distribution $\epsilon \sim \mathcal{N}(0, \mathrm{I})$, Rectified Flow defines the intermediate noisy state as $z_\tau = (1-\tau)z_0 + \tau\epsilon$, where $\tau\in[0,1]$ is the diffusion timestep. The training target of Rectified Flow 
adopts the $v$-prediction, 
% which means the model take the noisy state $z_\tau$ as input and pred the velocity $\mathbf{v}=z_0-\epsilon$. Training is 
defined as: 
\begin{equation}
    \mathcal{L} = 
    \mathbb{E}_{{z_0},\epsilon\sim{\mathcal{N}(0, \mathrm{I})},\tau}\left[ \Vert
    G_\theta(z_\tau,\tau,c,\mathcal{M}) - (z_0-\epsilon)
    \Vert^2_2 \right],
\label{eq:loss}
\end{equation}
where c is the condition, $\mathcal{M}$ is a binary mask for mask reconstruction and $G_\theta$ is the DiT model. 
% Condition $c$ contains text, condition frames and actions. The text is incorporated by cross-attention mechanisms, while the remaining components will be detailed in subsequent subsections.
% TODO: c -> frame + text
% Why SD3
% TODO: T(HW)C -> T x (HW) x C
% == original paper
% \noindent\textbf{Temporal Modeling.}
% As \ourmethod\  starts from an image diffusion model, several technologies are required to assume the role of a driving world model.  First, for temporal context learning, we attach one temporal transformer block after each 2D spatial transformer block, following common practice in video generation models~\cite{videoldm,Open-Sora}. During the forward process, in order to fix the input of different self-attention layers in transformer blocks, we reshape the video latent to $\mathrm{(TK)(\hat{H}
% \hat{W})C}$ for spatial self-attention and $\mathrm{(K\hat{H}
% \hat{W})TC}$ for temporal self-attention. Second, we introduce condition frames following video DiT models~\cite{Open-Sora,gao2024lumina} in which the input feature of temporal transformer block is separately embedded. 
% Specifically, given the frame-level binary indicator $m_c$ with value 1 on condition frames, embedding for condition frames $f_c$ and embedding for unconditional frames $f_{uc}$, the input feature of temporal transformer block $z^\prime$ is pre-processed for self-attention by embedding it to $m_c f_c(z^\prime) + (1-m_c) f_{uc}(z^\prime)$.
% TODO: discuss model size?

\noindent\textbf{Temporal Modeling.}
To facilitate temporal context learning, we attach a temporal transformer block after each 2D spatial transformer block, following common practice in video generation models~\cite{videoldm, Open-Sora}. During the forward process, to standardize the inputs to the different self-attention layers in the transformer blocks, we reshape the video latent representation to $\mathrm{(TK)(\hat{H}\hat{W})C}$ for spatial self-attention and to $\mathrm{(K\hat{H}\hat{W})TC}$ for temporal self-attention. Additionally, we introduce reference frames according to video DiT models~\cite{Open-Sora, gao2024lumina}. During diffusion process, diffusion timestep $\tau$ of the reference frames is always set as $0$, while the following frames to be predicted are embedded with regular timestep. 

\noindent\textbf{Unified Action Conditioning.}
Following VISTA, we provide nuanced control over low-level actions including angle, speed, trajectory and goal point, combining with high-level command capabilities. We construct action embedding by concatenating the Fourier embeddings~\cite{tancik2020fourier} of all actions. Subsequently, these action embeddings are projected and added to the key and value features of the cross-attention layers in temporal transformer blocks.

\subsection{Diffusion-related Mask Reconstruction}
\label{sec:3.2}
Motivated by previous works~\cite{maskdit,sd_dit}, Masked Image Modeling (MIM)~\cite{mae,simmim} with mask reconstruction object has been adopted to diffusion-based generation model and achieve improvement on training efficiency and local contextual perception. However, these methods fail to consider the influence of diffusion process, which incorporate noise schedule and complex training target. Therefore, we propose this novel pipeline integrated with mask reconstruction and then introduce our diffusion-related mask tokens for compatibility with diffusion process, which can reduce the effect of noise during diffusion process.
% In this sub-section, we first introduce the pipeline of mask reconstruction and then introduce our diffusion-related mask tokens for compatibility with diffusion process.
%introduced on following part of this subsection. 
% For simplicity, the sequence length $T$ and view $K$ is assumed to 1 on this subsection and temporal case will be discussed on the following section.
\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig/method2.pdf}
   \caption{The comparison of different mask types and attention operations for temporal transformer block with mask reconstruction task. Attention mask is only applied when $\mathcal{M} = \mathcal{M}_{time}$}
   \label{fig:method2}
\end{figure*}

\noindent\textbf{Mask Reconstruction.} 
During the training phase, the DiT backbone is asymmetrically divided into  an encoder 
$E$ and a decoder $D$ for mask reconstruction and includes two additional processing steps. In the encoding stage, \ourmethod~produces a token mask by random sampling a binary mask $\mathcal{M} \in \mathrm{R}^{T \times K \times 1 \times H \times W}$, given the video latent $z_\tau$ at timestep $\tau$. Similar to~\cite{mdt,sd_dit}, $\mathcal{M}$ is only use to partition $z_\tau$ into visible patch tokens $z_\tau^v=z_\tau \odot \mathcal{M}$ and invisible patch tokens $z_\tau^{iv}=z_\tau \odot (1-\mathcal{M})$. 
% Ratio $r$ is applied to control the number of invisible patches. 
In the decoding stage, an extra token reconstruction module is introduced to handle dropped invisible patches. Mask tokens $m_\tau$, representing invisible patches, are infilled at the positions of the dropped tokens. Then, a transformer block $F$ is utilized to provide contextual awareness from visible patches. In details,
\begin{equation}
    G_\theta(z_\tau,\mathcal{M}) = D(F(E(z_\tau^v)\odot \mathcal{M}+m_\tau \odot (1-\mathcal{M}))),
\label{eq:img_diff}
\end{equation}
where $\tau,c$ in Eq.\eqref{eq:loss} are ignored for simplicity. Note that mask reconstruction is skipped for inference in which all tokens are visible, equivalent to $G_\theta(z_\tau) = D(E(z_\tau)) = D(E(z_\tau^v + z_\tau^{iv}))$. In practice, invisible patches $z_\tau^{iv}$ are directly dropped during the encoding of training to enable memory and speed benefits.
% TODO: how to split encoder and decoder (e.g. in Figure or exp)



\noindent\textbf{Diffusion-related Mask Tokens.}
% There is a fuzzy relation between generation process and mask reconstruction. 
SD-DiT~\cite{sd_dit} describes a fuzzy relationship between generation process and mask reconstruction. Concretely, mask reconstruction focuses on context reasoning while generation diffusion process aims to model the translations between real and fake distributions.
% To solve this problem, we propose a diffusion-related mask tokens to enhance the synergy between diffusion process and mask reconstruction. Firstly, we devise a novel method to recover the original mask reconstruction during diffusion pipeline. 
From the viewpoint of diffusion model, the mask reconstruction for represent learning can be regard as $z_0$-prediction task, whereas rectified flow employs $v$-prediction (pred $z_0-\epsilon$). Therefore, simply combining these two distinct objectives may not lead to good performance of the diffusion model, as also demonstrated by MaskDiT~\cite{maskdit}.

Existing works either instantiate mask tokens as learnable parameters~\cite{mae,mdt} $p$ or directly take noisy tokens $z_\tau$ as input~\cite{sd_dit}. As discussed above, these mask tokens cannot balance these two targets due to the absence of explicit information for acquiring $\epsilon$. 
% Since the only difference between two targets is $-\epsilon$, 
We bridge this gap by introducing $f_m(\epsilon)$ into the mask token, where $f_m$ is a small network for encoding noise $\epsilon$. Given that $\epsilon$ is explicitly provided, it is easier to recover the original mask reconstruction target for representation learning within diffusion pipeline. 
Other than explicitly alignment for two prediction tasks, we further take $\tau$ into consideration. 
Overall, we define our mask token with learnable parameters $p$ as:
\begin{equation}
    m_\tau = (1-\tau)f_m(\epsilon) + \tau p.
\label{eq:img_diff}
\end{equation}
Based on our experiments in Section~\ref{sec:ablation}, we designed this mask token accordingly. When $\tau$ is large, the generation is performed at a high-noise level, and fine-grained image details are unknown. We hypothesize that the learnable parameters can estimate an average distribution under specific conditions (e.g., text) and assist in guiding the prediction direction when appearance details are lacking \cite{yue2024exploring}. Conversely, on a low-noise level, mask reconstruction encourages the model to be attentive to local details of visible patches. Therefore, the model can leverage the visible information to recover the patches filled with noise (by $f_m(\epsilon)$).

% === original paper
% Existing works either instantiate mask tokens as learnable parameters~\cite{mae,mdt} or directly take noisy tokens as input~\cite{sd_dit}. 
% However, as the fuzzy relation pointed in MaskDiT~\cite{maskdit}, these methods neglecting the difference on mask reconstruction for original represent learning~\cite{mae,simmim} and diffusion model. From the viewpoint of diffusion model, the mask reconstruction for represent learning can be regard as $z_0$-prediction and Rectified flow as $v$-prediction (pred $z_0-\epsilon$) respectively. In this work, we bridge this gap by formulating the mask tokens as $f_m(\epsilon)$, where $f_m$ is a small network for encoding noise $\epsilon$. The reason behind this design is only difference between two prediction is $-\epsilon$. As long as this difference is explicitly given, for the invisible tokens, we can recover the original mask reconstruction target for represent learning on diffusion pipeline. 

% Other than explicitly $z_0$ prediction for invisible tokens, we further take $\tau$ into consideration. When $\tau$ is large, generation is performed at a high noisy level and fine-grained details for image are unknown. However, mask reconstruction require awareness of local details to make prediction for invisible patches. Based on above analysis, we design to relax the MIM target and find that simple learnable parameters in MaskDiT are more suitable for large noise level. We hypothesis this result by learnable parameters can estimate a average distribution under specific condition, e.g. text, and assist the prediction of velocity at high noisy level, which lack of appearance details~\cite{yue2024exploring}. The final mask tokens of \ourmethod\  is represented by:

% % TODO: why mix -> learnable parameters for dynamic scene. With exp: remove cross att.
% % Exp: with, without conditon for mae loss on invisible tokens.
% % Exp: replace parameters with text embedding. otherwise refer to dataset.
% \begin{equation}
%     m_\tau = (1-\tau)f_m(\epsilon) + \tau p
% \label{eq:img_diff}
% \end{equation}
% where $p$ is learnable parameters with the same channels as $f_m(\epsilon)$.
% TODO: use patches to replace latent


\subsection{Mask Reconstruction Strategy}
\label{sec:3.3}
\textbf{Temporal and Spatial Mask.}
Previous methods~\cite{videomae,videomae_v2} extend MIM to video domain by sharing the random mask across time, where the mask $\mathcal{M}=\mathcal{M}_{spatial}=[\mathcal{M}^1, \mathcal{M}^2..., \mathcal{M}^T]$ satisfy $\mathcal{M}^i = \mathcal{M}^j$ when $i\neq j$. Despite the effectiveness on understanding tasks in the aforementioned methods, this masking strategy may not be suitable for temporal modeling on driving video prediction.
To incorporate temporal learning, we introduce a temporal unshared mask $\mathcal{M}_{time}$ satisfy $\mathcal{M}^i \neq \mathcal{M}^j$ when $i\neq j$. Then, we specialize $\mathcal{M}_{spatial}$ for spatial modeling and consider MIM with $\mathcal{M}_{time}$ for temporal modeling. Then, we make tasks synergy by devising a two-branch transformer block: 
\begin{equation}
    F=
    \begin{cases}
        F_s&  \text{if}\  \mathcal{M} = \mathcal{M}_{spatial}\\
        F_t&  \text{if}\  \mathcal{M} = \mathcal{M}_{time}
    \end{cases},
\label{eq:img_diff}
\end{equation}
where $\mathcal{M} \in \{\mathcal{M}_{spatial}, \mathcal{M}_{time}\}$, $F_s$ is a spatial transformer block and $F_t$ is a temporal transformer block.

% == Original paper
% \subsection{Mask Reconstruction for Temporal}
% \label{sec:3.3}
% \textbf{Temporal Mask Strategy.} 
% Previous methods~\cite{videomae,videomae_v2} extend MIM to video domain by sharing the random mask across time, which means mask $\mathcal{M}=\mathcal{M}_{spatial}=[\mathcal{M}^1, \mathcal{M}^2..., \mathcal{M}^T]$ satisfy $\mathcal{M}^i = \mathcal{M}^j, i\neq j$. Despite the effectiveness on action classification and detection tasks in the aforementioned methods, this masking strategy may not be suitable for temporal modeling on driving video prediction. 
% % In contrast to classifying action lasting for a few seconds, 
% Driving world models require predicting frame-level nuance for moving objects, such as the scene changes by ego motion and instance-level movement from non-ego cars.
% % Sharing temporal mask might result in the continued absence of relevant components, thereby influencing the prediction of objects motion in a certain regions.
% % Why old ok, Action for several seconds
% To address this problem, \ourmethod\ adopts a temporal unshared mask $\mathcal{M}_{time}$.
% To concentrate on temporal learning, we specialize $\mathcal{M}_{spatial}$ for spatial modeling and consider MIM with $\mathcal{M}_{time}$ as an independent task. Then, we make tasks synergy by devising a two-branch transformer block F: 
% \begin{equation}
%     F=
%     \begin{cases}
%         F_s&  \text{if}\  \mathcal{M} = \mathcal{M}_{spatial}\\
%         F_t&  \text{if}\  \mathcal{M} = \mathcal{M}_{time}
%     \end{cases},
% \label{eq:img_diff}
% \end{equation}
% where $\mathcal{M} \in \{\mathcal{M}_{spatial}, \mathcal{M}_{time}\}$, $F_s$ is a spatial transformer block and $F_t$ is a temporal transformer block.

\noindent\textbf{Row-wise Approximation.} 
% not appro, as no mask token, so shift is better?
% The most straightforward strategy for masking video data is to randomly drop a set of frames. However, temporal masking is highly sensitive to the masking ratio. In our experiments, we observe that video quality significantly deteriorates even when the masking rate only increases to 10\%.
% An alternative strategy is to directly apply a random mask $\mathcal{M}_{\text{time}}$ on each frame, as shown in the middle column of Fig.~\ref{fig:method2}. However, this masked temporal self-attention requires a 3D attention mask to skip masked tokens, which leads to additional computational cost and precludes the direct application of optimization operators like FlashAttention~\cite{dao2022flashattention}.
The most straightforward strategy for masking video data is to directly apply a random mask $\mathcal{M}_{\text{time}}$ on each frame like~\cite{videomae,videomae_v2}, as shown in the middle column of Fig.~\ref{fig:method2}. However, this design cause tokens on masked positions should be masked rather than directly dropped, since temporal self-attention require all entries have the same sequence length and apply an attention mask to control sparsity.
As a result, this masked temporal self-attention requires a 3D attention mask to skip masked tokens, which leads to additional computational cost and precludes the direct application of optimization operators like FlashAttention~\cite{dao2022flashattention}.
To address the aforementioned issues, we employ shifted temporal self-attention for $\mathcal{M}_{\text{time}}$. As illustrated in the right column of Fig.~\ref{fig:method2}, we randomly mask the same number of tokens for each row. Similar to the spatial branch, all invisible tokens are dropped, and the visible tokens are directly connected. Consequently, this operation can be regarded as a shift in temporal self-attention, which adheres to the core idea of Masked Reconstruction (MR): masked tokens are invisible and are predicted from context during the decoding stage.
Additionally, rearranging the visible tokens row by row ensures the relevance of information in the temporal attention block during the encoding stage. 
In experiments, we find this design improves not only the training speed but also the generation metrics, especially on larger mask ratio $r$. Since this row-wise shifting allows nearby tokens to fill in the blanks of masked tokens, We analysis this phenomenon by token filling makes all tokens on temporal axis are retained and minor shift can facilitate the temporal block in context reasoning.

To formulate this row-wise temporal mask, we define the mask ratio as $r$.
To generate the mask for a $\hat H \times \hat W$ image latent at frame $t$, we randomly generate $\hat H$ one-dimensional mask, each having $r\hat W$ zero values, and concatenate them to formulate the final mask for this image, denoted by $\widehat{\mathcal{M}}_{\text{time}}$. We use $\widehat{\mathcal{M}}_{\text{time}}$ to replace $\mathcal{M}_{\text{time}}$ in our training.



% \noindent\textbf{Row-wise Approximation.} 
% % not appro, as no mask token, so shift is better?
% As shown in the middle column of Fig.~\ref{fig:method2}, directly applying $\mathcal{M}_{time}$ lead to a masked temporal self-attention, which results in two problems in our experiments. First, we find temporal masking is more sensitive to masking ratio, which require a very small masking rate, e.g. 10\%. Second, masked temporal self-attention require a 3D attention mask to skip mask tokens, which lead to extra computation cost and cannot directly apply optimization operator like flash-attention~\cite{dao2022flashattention}. To overcome above two problems, we propose to employ $\mathcal{M}_{time}$ with a shift temporal self-attention. As shown in the right column of Fig.~\ref{fig:method2}, similar to spatial branch, all invisible tokens are dropped and visible tokens are directly connected. Since masked position is not shared across time, this operation can be seen as a random shift on temporal self-attention, which also adhere the core idea of MR: mask tokens are invisible and predicted by context during the decoder stage. 

% One impact that needs attention is the shift range must be restricted to a local field. Since we rearrange the visible tokens row by row, a simple solution is make the invisible tokens on each row are equal, then cases like a token at the end of one row match to a token at the begin of corresponding next row will not happen. Specifically, to generate mask $\mathcal{M}_{time}^{t,v} \in \mathrm{R}^{H \times W}$ at frame id t and view id v, we sample $nW$ sub-masks with shape $H/n$ and same number of drop indices, then row-wise $\mathcal{M}_{time}$ is generated by concatenating all sub-masks and reshape to $H\times W$. $n$ is a hyper-parameter that larger than 1. This design assure on each image row, exact $r\hat{W}$ tokens are invisible, satisfying tokens cannot attend to tokens in different rows. We define $\mathcal{\hat{M}}_{time}$ for $\mathcal{M}_{time}$ satisfies above row-wise generation strategy.
% TODO: only temporal shift, but without prediction.
% TODO: use frame id for time
\subsection{Mask Reconstruction for Cross-View}
\label{sec:3.4}
Our method can be extended to generate multi-view driving videos by introducing view transformer blocks. We propose a cross-view row-wise self-attention mechanism, which concatenates video features horizontally and computes attention across multi-view features on the same row. Specifically, given a feature tensor of shape $ TK\hat{H}\hat{W} $, we apply masking, resulting in a tensor of shape $ TKC\hat{H} [(1 - r)\hat{W}] $. We then reshape it into size $ (T\hat{H})[K(1 - r)\hat{W}]C $. 

The key insights of the proposed cross-view row-wise attention are twofold: First, since the vertical context can be modeled by spatial transformer blocks, row-wise feature exchange provides sufficient receptive field to extract multi-view information. Second, the proposed row-wise masking for reconstruction can also be utilized as data augmentation, since $Kr\hat{W}$ tokens of each row are randomly dropped. Because the proposed masked modeling task focuses on spatio-temporal modeling, we do not apply mask reconstruction along the view dimension.
% \ourmethod\ can be extended to generate multi-view driving videos with the introduction of view transformer blocks.
% Specifically, we concatenate multi-view video horizontally and model cross-view semantic by view-row self-attention, whose input shape is $\mathrm{(T\hat{H})(K(1-r)\hat{W})C}$. Since view Fourier embeddings~\cite{tancik2020fourier} are appended for position awareness in view transformer blocks, the order of different views can be random. The insight of proposed view-row attention are in two fold: first, since vertical context can be modeled by spatial transformer blocks, row-wise feature exchange contains enough receptive field to extracted multi-view information. Second, proposed row-wise mask for reconstruction can also be utilized as an augmentation, since $r\hat{W}$ tokens of each row are randomly dropped. Because of proposed mask modeling task is target at spatial-temporal modeling, no mask reconstruction is introduced for view context, e.g. $F$ contains another view branch.

% It is worth to note that proposed mask reconstruction is also compatible with view self-attention~\cite{drive-wm}, as long as dropped position is consistent across different views of one image. We make comparisons with this method on experiment section.
% TODO: rowwise blocks with or withour mask.
% Two ways: repeat M and any M with row-wise transformer blocks.