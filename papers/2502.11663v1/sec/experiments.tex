\section{Experiments}
\subsection{Setup}
\input{tab/main_rt2}
\input{tab/main_gen}
\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig/visual.pdf}
   \caption{\textbf{Long-horizon prediction results of \ourmethod.}  Our model is capable of forecasting long video sequences with stability, devoid of collapse or blurring issues.}
   \label{fig:visual}
\end{figure*}
\begin{figure}[h]
  \centering
   \includegraphics[width=1\linewidth]{fig/long.png}
   \caption{\textbf{Comparison of Long-horizon FVD metric on OpenDV2K validation set.} Our method demonstrates superior performance in terms of both value and growth rate.}
   \label{fig:long}
\end{figure}
\textbf{Datasets.} We conduct comprehensive experiments on a single-view dataset OpenDV-2K \cite{genad} and two multi-views datasets nuScenes \cite{caesar2020nuscenes} and Waymo \cite{waymo}. We follow the official splits to divide the training and validation sets.

\noindent \textbf{Evaluation.}
% TODO: discuss: no 3d box following as follow vista
The quality of the generated images and videos are assessed using the Frechet Inception Distance (FID)~\cite{fid} for images and the Frechet Video Distance (FVD)~\cite{fvd} for videos. For fair comparison with previous works, we apply different evaluation settings for single-view model and multi-view model. For single view model, we align the evaluation setting of VISTA~\cite{vista}.
% which generate 25-frame videos without actions on full nuScene validation set with 5369 samples. All generated videos and corresponding frames are used for computing FVD and FID respectively.
For multi-view model, we adopt the setting of Drive-WM~\cite{drive-wm}. Please refer to Appendix.\ref{sec:supp_sample_details} for more details.
% For each nuScene scene, we generate 150 6-view videos, resulting in 900 single-view videos. Then,  10K frames are randomly sampled from these 900 videos for computing FID. 
To assess generalization ability, we evaluate zero-shot performance on Waymo validation set, using 600 videos for FVD and 15K frames for FID. 

\subsection{Training Scheme}
Our model is initialized with SD3~\cite{sd3} medium checkpoint with 2B parameters. 
There are three stages for our training: Stage 1 for pre-training on large-scale OpenDV-2K dataset, Stage 2 for single-view model \ourmethod-long and Stage 3 for multi-view model \ourmethod-mview. For all stages, we resize the original images to 512$\times$288, masking ratio $r$ is set to 0.25.

\noindent \textbf{Stage 1.} 
Following VISTA~\cite{vista}, we first pre-train our model on OpenDV-2K dataset. As our model starts from image backbone, we first train our model using single frame videos with batch size 768 for 18K iterations and then we train our temporal blocks with 16/20/24 frame videos and batch size 64. In particular, temporal blocks is initialized with zero and the training of temporal blocks takes 24K iterations in this step. Afterwards, we insert the zero-initialized reconstruction blocks $F$ and train with masking strategy for extra 20K iterations. The reason for varying frame length during training is that the frame numbers for single-view and multi-view models are different.
% Following VISTA~\cite{vista}, we first pre-train our model on OpenDV-2K dataset. As our model starts from image backbone, we first train on 1 frames with batch size 768 for 18K iterations and then we train our temporal model with 16/20/24 frames and batch size 64. In particular, we insert zero-initialized temporal attention blocks and train 24K iterations. Afterwards, we insert zero-initialized $F$ and train with masking strategy for additional 20K iterations. The reason for variant frame length is that we set different frame numbers for single-view and multi-view model.
% additional details: frozen .etc

\noindent \textbf{Stage 2 for \ourmethod-long.} 
We devised \ourmethod-long to align with VISTA, where the frame length $T$ is set to 25 and cross-view blocks are skipped. We also follow the stage 2 of VISTA's collaborative training, in which data were sampled equally from nuScene and OpenDV-2K, and the action module is zero-initialized and trained.
% We devise \ourmethod-long that align to VISTA, where the frame length $T$ is set to 25 and skip cross-view blocks. We follow the collaborative training stage 2 of VISTA, where data is sampled from nuScene and OpenDV-2K with equal probability and action module is zero-initialized and trained on this stage.

\noindent \textbf{Stage 3 for \ourmethod-mview.}
Based on the well-trained \ourmethod-long, we enable multi-view ability of \ourmethod-mview by adding the cross-view blocks and train it on nuScene dataset for 6K steps and frame length $T$ is reduced to 8.
% We reduce the frame length to 8 and batch size to 32.

% Please refer more details in the appendix.

\input{tab/abl_mr}
\input{tab/abl_cv}
\subsection{Comparison}
\noindent\textbf{Generation Quality.}
Tab.\ref{tab:video-main} presents the quantitative comparison. In single-view generation, we achieve a remarkable FID of 5.6 and an FVD of 92.5, surpassing the previous state-of-the-art approaches~\cite{genad,vista} that are also trained on web-scale dataset. It is worth noting that there is a discrepancy between the training and evaluation phases in Vista, which integrates action during training but excludes it during evaluation. To address this, we also experiment with an aligned setting that drops action information for both training and evaluation. As indicated in Table \ref{tab:video-main}, this aligned setting yields significantly improved results, with a FID of 4.0 and a FVD of 59.4. We present these results for reference. For multi-view cases, similar improvement can also be observed, with 8.9 FID and 65.4 FVD. It is worth noting that our model directly predict multi-view future without the requirement of future layouts, unlike those methods~\cite{dive} deviating from video prediction task. Furthermore, our method represents a pioneering effort in extending a generalizable single-view model, trained on OpenDV-2K, to the domain of multi-view models.
% The main results of generation quality are present in Tab.\ref{tab:video-main}. In single-view generation, we achieve a remarkable FID of 5.6 and an FVD of 92.5, surpassing the previous state-of-the-art approaches~\cite{genad,vista}, which were also trained on web-scale dataset. For multi-view case, similar improvement can also be observed with 8.9 FID and 65.4 FVD. It worth to note that our model directly predict multi-view future without the requirement of future layout~\cite{dive} which deviate from prediction task. Furthermore, our method represents a pioneering effort in extending a generalizable single-view model, trained on OpenDV-2K, to the domain of multi-view models.

\noindent \textbf{Generalization ability.}
In Tab.\ref{tab:main_gen}, we assess the generalization capability of our approach on the Waymo dataset, which is excluded from our training datasets. We conducted inference using the official checkpoint of VISTA~\cite{vista}. The results indicate that our method attains superior FVD while maintaining comparable FID, thereby demonstrating the generalization ability of our method.
% In Tab.\ref{tab:main_gen}, we assess the generalization capability of our approach on the Waymo dataset, which is unseen during training. As this metric has not been previously evaluated by other studies, we conducted inference using the official model from VISTA~\cite{vista} to facilitate comparison. The results indicate that our method attains superior FVD while maintaining comparable FID, thereby confirming the generalization ability of our method.

\noindent \textbf{Long-horizon prediction.}
Fig.\ref{fig:long} illustrates the comparison of long-time prediction with VISTA. Due to the computational cost of generating long videos, both FID and FVD metrics are calculated on 300 videos randomly sampled from the OpenDV-2K validation set. The slope of FVD curve is significantly lower than VISTA, showing the less degradation. Qualitative results are illustrated in Fig.\ref{fig:visual} and appendix.
% In Fig.\ref{fig:long}, we compare the ability of long-time prediction with VISTA. Since the cost of generating long videos, both FID and FVD metrics are calculated on OpenDV-2K validation set with randomly sampled 300 videos. The slope of FVD curve is significantly lower than VISTA, showing the less degradation. Qualitative results are illustrated in Fig.\ref{fig:visual} and appendix.
% OpenDV: follow GenAD, use 18000 frames + 3000 video clips on validation dataset

% Nuscene: follow Vista, use 5369 video samples and 5369 x 24 frames.

% mviews: follow DWM, use 900 video with 16 frames.

% Waymo: no previous metrics, use 18000 frames + 3000 video clips

% We also try other metrics on ...


\subsection{Ablation Studies}
\label{sec:ablation}
We conduct comprehensive ablation study to verfiy the performance of every component in MaskGWM. Following the setting of GenAD for effective experiments, most ablation studies are conducted on stage 1 after mask reconstruction is inserted and the metrics are reported on the validation set of OpenDV-2K dataset with 3000 video clips for FVD and 18000 frames for FID. For the ablation of cross-view transformer blocks, we use multi-view nuScene metrics.
% We conduct ablation study to examine each component in MaskGWM. 
% % Considering that the training and sampling of video generation model are computationally expensive, we adopt a lightweight setting for efficient evaluation:
% We conduct most ablation studies on stage 1 where mask reconstruction is inserted and report the metrics on the validation set of OpenDV-2K dataset with 3000 video clips for FVD and 18000 frames for FID, similar to the ablation in GenAD. For the ablation of cross-view transformer blocks, we use multi-view nuScene metrics.

\noindent \textbf{Effect of mask tokens.} As indicated in Tab.\ref{tab:abl-1}, we make comparisons across different designs for mask tokens $m_\tau$. To analysis the influence of diffusion timestep $\tau$, we test only applying mask reconstruction on certain timestep range. We find learnable parameters $p$ shown better results on high noisy level and proposed noise embedding $f_m(\epsilon)$ perform better on low noisy level. This demonstrates the merit of noise embedding, which can recover the original mask reconstruction target for represent learning by explicitly giving $\epsilon$, achieving better result on generation steps with local details. In addition, we also try the mask tokens used in SD-DiT~\cite{sd_dit}, combining with extra contrastive~\cite{caron2021emerging} loss. The experiment shows our proposed diffusion-related mask tokens achieve best performance via combining the advantage of $p$ and $f_m(\epsilon)$.
% \noindent \textbf{Effect of mask tokens.} As indicated in Tab.\ref{tab:abl-1}, we make comparisons across different designs for mask tokens $m_\tau$. To analysis the influence of diffusion timestep $\tau$, we test only applying mask reconstruction on certain timestep range. We find learnable parameters $p$ shown better results on high noisy level and proposed noise embedding $f_m(\epsilon)$ perform better on low noisy level. This demonstrates the merit of noise embedding, which can recover the original mask reconstruction target for represent learning by explicitly giving $\epsilon$, achieving better result on generation steps with local details. In addition, we also try the mask tokens used in SD-DiT~\cite{sd_dit}, combining with extra contrastive~\cite{caron2021emerging} loss. The experiment shows our proposed diffusion-related mask tokens achieve best performance via combining the advantage of $p$ and $f_m(\epsilon)$.

\noindent \textbf{Effect of mask reconstruction.}
In Tab.\ref{tab:abl-2} and Tab.\ref{tab:abl-3}, we explore the effectiveness of proposed mask reconstruction in spatial-temporal domain. According to Tab.\ref{tab:abl-3}, we find that our row-wise time mask with shift attention mechanism assists a lot the model's convergence, while larger mask ratio can also make positive effect on the results. Thus, this setting is adopted in the ablation study on the mask strategy $\mathcal{M}$. As shown in Tab.\ref{tab:abl-2}, combining $\mathcal{M}_{spatial}$ and $\mathcal{M}_{time}$ can 
significantly improve the generation quality of the whole videos and each single images. We hypothesis this is because the temporal modeling is more sensitive to the dropout ratio than spatial counterpart (Please see more details on Appendix.\ref{sec:supp_abl_mr}). 
When shifted temporal self-attention is applied with a row-wise mask, all tokens on temporal axis are retained and experience only minor spatial shifts. Whereas invisible tokens are skipped without shift temporal self-attention, which results in a discrepancy between the number of tokens used in training and inference.
Moreover, the training speed is improved since invisible tokens dropped. From Tab.\ref{tab:abl-2}, we can see our two-branch mask reconstruction achieve best results, showing the effectiveness of introducing mask reconstruction on temporal context.
% In Tab.\ref{tab:abl-2} and Tab.\ref{tab:abl-3}, we explore the effectiveness of proposed mask reconstruction on spatial-temporal domain. We conduct experiment by adjust the mask ratio $r$ and task sampling for $\mathcal{M} \in \{\mathcal{M}_{spatial}, \mathcal{M}_{time}\}$. As the results with temporal reconstruction on Tab.\ref{tab:abl-3}, our row-wise $\mathcal{M}_{time}$ combined with shift self-attention achieves the best results and enjoy a higher mask ratio. We hypothesis this phenomenon by temporal modeling is more sensitive to the training-inference discrepancy than spatial counterpart (Please see more details on Appendix.\ref{sec:supp_abl_mr}). When shifted temporal self-attention is applied with a row-wise mask, all tokens are retained and experience only minor spatial shifts. Whereas invisible tokens are skipped without shift temporal self-attention, which results in a discrepancy between the number of tokens used in training and inference. Moreover, the training speed is improved since invisible tokens dropped. From Tab.\ref{tab:abl-2}, we can see our two-branch mask reconstruction achieve best results, showing the effectiveness of introducing mask reconstruction on temporal context.

\noindent \textbf{Effect of cross-view module.}
As shown in Tab.\ref{tab:abl-5}, we make comparisons across different types of cross-view modeling. We find that introducing mask reconstruction on stage-3 training also yields favorable results. 
This indicates that randomly masking certain tokens along the view-row dimension is not detrimental and can even enhance the final results. This is different from temporal counterpart that requires shift-attention to avoiding tokens dropping. 

Furthermore, the experiment shows that proposed attention on dimensions $\mathrm{KW}$ outperforms view-attention on $\mathrm{K}$ used in previous methods~\cite{drive-wm,drivedreamer}. For view attention on $\mathrm{KHW}$, despite the minor improvement, the computation complexity explodes significantly. Consequently, this design is not adopted in our experiments.
% As shown in Tab.\ref{tab:abl-5}, we make comparisons across different cross-view modeling. We find introducing mask reconstruction on stage-3 training also yields favorable results. This indicates that randomly masking certain tokens along the view-row dimension is not detrimental and can even enhance the final results. This is different from temporal counterpart that requires shift-attention to avoiding tokens dropping. Furthermore, the experiment shows that proposed attention dimensions $\mathrm{KW}$ outperforms view-attention used in previous methods~\cite{drive-wm,drivedreamer}. For view attention on $\mathrm{KHW}$, although minor improvement is achieved, the computation complexity exceeds that of spatial attention, which operates on dimensions $\mathrm{HW}$. Consequently, we did not adopt this design.

\noindent \textbf{Effect of two-branch token reconstruction.}
We validate the effectiveness of two-branch transformer reconstruction structure for token reconstruction on Tab.\ref{tab:abl-4}. We remove two-branch structure by applying sequential spatial-temporal transformer blocks, which is shared by $F_s$ and $F_t$. 
We find the two-branch structure produces better results, especially on FVD. Unshared design forces the model to reconstruct masked features by corresponding context, leading to better spatial and temporal modeling for different conditions. 
% We also assess the efficacy of the two-branch transformer architecture for token reconstruction. The two-branch design still yields superior results over the shared-weight 2-layer transformer, particularly in terms of FVD. 
% {\blue what does this mean}
% The unshared design forces the model to reconstruct mask features based on single context, resulting in enhanced spatial-temporal modeling.
% We validate the effectiveness of two-branch transformer structure for token reconstruction on Tab.\ref{tab:abl-4}.
% % We remove two-branch structure by applying sequential spatial-temporal transformer blocks, which is shared by $\mathcal{M}_{spatial}$ and $\mathcal{M}_{time}$. 
% We find the two-branch structure lead a better result, especially on FVD. Unshared design forces the model reconstruct mask features by corresponding context, leading a better spatial-temporal modeling. We assess the efficacy of the two-branch transformer architecture for token reconstruction in Table \ref{tab:abl-4}. By replacing the two-branch structure with shared sequential spatial-temporal transformer blocks for $\mathcal{M}_{spatial}$ and $\mathcal{M}_{time}$, we observe that the two-branch design yields superior results, particularly in terms of FVD. The unshared design forces the model to reconstruct mask features based on single context, resulting in enhanced spatial-temporal modeling.
% {\orange Previous methods have shown the benefits of mask reconstruction on converge speed of image generation. We also analysis this influence of mask reconstruction on our model. From Tab.\ref{tab:abl-4}, we find even much more better results at earlier stage, no significant improvement on converge speed improvement. The difference may come from our model is start from pre-trained T2I model rather than from scratch. More discussion about training policy can be found on the appendix.}
% Drawback: high masking-ratio (may extra fine-tuning stage)

% \input{tab/main_rt2}
% \input{tab/main_gen}
% \input{tab/abl_mr}
% \input{tab/abl_cv}
% \input{tab/main_multi}
% \input{tab/main_single}
% Drawback: high masking-ratio (may extra fine-tuning stage)