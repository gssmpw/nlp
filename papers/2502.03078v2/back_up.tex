
\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{todonotes}

\begin{document}

\title{Research Project: HCII 2025}

\author{Nina Freise\inst{1,2} \and
Marius Heitlinger\inst{1,2}}
%
\authorrunning{N. Freise and M. Heitlinger}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Heilbronn University \and
Heidelberg University}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}

% 1. establising a territory
In today's world, the popularity of Artificial Intelligence (AI) is experiencing a significant increase. AI technologies are being integrated into various sectors, such as the medical field, transforming the way tasks are performed and decisions are made \cite{applbiosci3010002}. 
There is a notable lack of confidence in the reliability of AI models in the medical field, with a considerable proportion of potential users expressing concerns about the potential breaches of privacy \cite{TrustinandAcceptanceofArtificialIntelligenceApplicationsinMedicine:MixedMethodsStudy}. Nevertheless, for medical AI applications to be successfully implemented, it is essential to establish trust and acceptance among users. 
To address trust concerns, improving AI model performance is essential, which increasingly depends on access to extensive, high-quality data as the demand for large-scale datasets grows with model sophistication.
High-quality data is the foundation of effective AI, as it ensures that models can learn, adapt and perform accurately. 
However, obtaining sufficient data, especially in the medical field, is particularly challenging \cite{app13127082}. 
Medical data is often sensitive and personal, making it difficult to access due to privacy regulations and ethical considerations especially in Germany. Researchers and developers face the dilemma of needing large amounts of data while adhering to strict confidentiality requirements.
But recent advances in generative AI could offer a promising solution to this problem. These technologies can produce synthetic data that mimics real-world data, potentially alleviating the shortage of training datasets \cite{Jadon_2023}. 
However, the success of this approach depends heavily on the quality of the synthetic data. It must accurately reflect the patterns and nuances of actual medical data to be useful for training AI models. 


% 2. establising a niche
Recent studies have explored approaches to synthetic data generation. A notable example is the study with the title "Potential of ChatGPT for generating synthetic German medical corpus: A comparison with real-world corpora" which was presented at the "Deutsche Gesellschaft für Medizinische Informatik, Biometrie und Epidemiologie (GMDS)" in 2023 \cite{uyguner2023chatgpt}. This study highlights the challenges posed by German data protection regulations and the need for diverse and representative synthetic text data. Using language models such as ChatGPT 3.0, the researchers generated synthetic German discharge letters and analysed their characteristics against medical and non-medical datasets using techniques such as t-SNE and UMAP. The results showed that while the synthetic data clusters were close to the real medical data clusters, there is still room for improvement to achieve ideal overlap.
The aim of this research is to build on existing findings by refining and optimising prompts to produce realistic synthetic data. In contrast to the previous study, which used a relatively simple prompt, our approach uses an iterative prompt engineering process designed to systematically improve the prompt and thereby improve the quality and realism of the synthetic medical data produced. 

% 3. occupying the niche
The goal of this research is to develop a novel, automated, iterative prompt engineering pipeline that simplifies the generation of prompts for creating realistic synthetic medical data.
To achieve this, we adopt a dual approach: conducting a comprehensive literature review to identify advanced prompt optimization techniques and integrating these methods into a novel framework for automated refinement.
The pipeline minimizes user involvement by requiring only a single input while automating the iterative refinement process. 
With a focus on ease of use and automation, this approach addresses the significant time and effort traditionally associated with prompt optimisation, while ensuring that the data generated by the prompt meets high standards of quality and realism. By improving data realism, our work facilitates the development of ethical and privacy-preserving AI systems in the medical domain.
Given the sensitive nature and privacy constraints surrounding medical data, we assume that access to real data will be restricted, thereby precluding its use for training or fine-tuning models. 
Instead, our pipeline uses the original data only as a benchmark to assess the plausibility of the synthetically generated data through our enhanced prompts.
By focusing on improving prompts to generate realistic, synthetic data, this research bridges the gap between the growing need for high-quality data in medical AI research and the limitations imposed by privacy regulations.
This work ultimately supports the advancement and acceptance of AI in medicine by providing a solution for synthetic data generation.

\section{Literatur Review}

We followed the PRISMA Guidelines for systematic reviews \cite{liberati2009prisma}.
We designed the search strategy to identify recent studies (published from 2020 onwards) that focus on automatic and iterative prompt engineering techniques aimed at optimizing text prompts for generating text, rather than classification or text-to-image tasks. To ensure relevance to our use case, we specifically sought methods that function without relying on training datasets, high-performing prompt datasets, or any established "ground truth" datasets.
We conducted the search on Google Scholar, as it indexed a comprehensive range of academic sources, including peer-reviewed journals, conference papers, and books. Furthermore, Google Scholar searches the full text of articles, in addition to titles and abstracts, which increases the probability of locating relevant papers on the subject matter.
The search term combines \textit{Large Language Models}, \textit{automatic prompt engineering}, and \textit{text prompts} and some variations and synonyms of these topics. 
The following query is the result: 
\newline
\textit{("Large Language Models" OR "LLM") \newline
AND 
\newline
("automatic prompt engineering" OR "iterative prompt engineering")
\newline 
AND 
\newline
("text prompts")
\newline
AND 
\newline
("optimizing" OR "optimizers" OR "improved" OR "improving")} \newline
We defined the inclusion and exclusion criteria for the resulting papers as follows:
\newline
\textbf{Inclusion criteria:}
\begin{itemize}
    \item  contains automatic iterative prompt engineering technique with text prompts
    \item written in German or English
\end{itemize}
\textbf{Exclusion criteria:}
\begin{itemize}
    \item published before the year 2020
    \item utilizes a training set or an open dataset for evaluation in its method
    \item inaccessible without open access or not available through the University of Heilbronn or the University of Heidelberg
\end{itemize}

We referenced papers as out-of-scope if they do not contain automatic and iterative prompt engineering techniques on text prompts for generating text.
We exported the search results, divided them into two groups, and assigned them to two reviewers for initial screening. Initially, we screened records based on the inclusion and exclusion criteria using the title and abstract. Subsequently, we retrieved the full texts and performed a full-text screening. This step involved reapplying the inclusion and exclusion criteria. We documented the reasons for excluding papers.
To ensure the validity of each paper, each reviewer individually performed the full-text paper screening, and we then compared the results.
We conducted the search in October 2024.

\subsection{Summarization of the Literature}
\begin{figure}
\includegraphics[width=\textwidth]{Prisma.png}
\caption{PRISMA flow diagram} \label{fig:prisma}
\end{figure}

We commenced this systematic review with an initial comprehensive search that identified 57 papers. After screening the abstracts, we excluded 36 papers for various reasons, as shown in Fig. \ref{fig:prisma}. Subsequently, we conducted a more in-depth analysis of 21 papers, during which we excluded 15 papers. In the end, we included 6 studies in this systematic review.

The reviewed studies present diverse techniques for iterative prompt optimization, each leveraging unique mechanisms to refine prompt quality and model performance. Despite sharing a common objective—enhancing prompt efficacy across various applications—these approaches differ significantly in their optimization strategies, feedback structures, and scenarios where they excel.

\subsubsection{Feedback-Based and Iterative Refinement Approaches}
Prompt with Actor-Critic Editing (PACE) \cite{dong2024paceimprovingpromptactorcritic} and Strategic-Guided Optimization (STRAGO) \cite{wu2024stragoharnessingstrategicguidance} adopt feedback-centric methods that enable LLMs to refine prompts iteratively, yet they differ in their feedback structures and evaluation strategies.
PACE employs an actor-critic algorithm inspired by reinforcement learning, where the LLM operates in two roles: as an "actor" that generates responses based on a prompt and as a "critic" that assesses response quality \cite{dong2024paceimprovingpromptactorcritic}. The feedback loop in PACE is multi-faceted; it incorporates feedback from multiple critic models per iteration, aggregating feedback to guide the optimization of each prompt. This aggregation allows for a broader perspective on prompt performance, enhancing robustness and reducing individual bias. Unlike single-step feedback loops, PACE’s actor-critic structure allows for nuanced adjustments to prompts, making it particularly effective in scenarios where varied perspectives are needed for prompt improvement. However, the model’s reliance on multiple evaluations per iteration raises computational demands, potentially limiting its applicability in resource-constrained settings.
STRAGO, while also feedback-driven, tackles a common pitfall in prompt optimization: prompt drift, where optimization for specific cases can lead to performance loss in other scenarios \cite{wu2024stragoharnessingstrategicguidance}. To mitigate this, STRAGO’s method involves analyzing both successful and failed cases, offering a balanced view of prompt performance. This approach allows STRAGO to provide detailed, step-by-step refinements, addressing specific issues while preserving effective elements of prior prompts. STRAGO’s three-step process (analyzer, refiner, and optimizer) further differentiates it from PACE, with each stage focusing on dissecting both correct and incorrect predictions and generating strategies for iterative prompt refinement. By adopting this structured, multi-layered process, STRAGO is well-suited for complex, stability-dependent tasks where controlled refinement is critical, such as reasoning or domain-specific applications.
While both methods demonstrate the power of iterative feedback, PACE focuses on aggregating diverse feedback for prompt refinement, whereas STRAGO emphasizes stability by maintaining a balance between corrective actions and preserving successful prompt features. These differences highlight PACE’s strength in high-complexity environments and STRAGO’s advantage in scenarios where drift control is paramount.
\subsubsection{Error-Focused or Diagnostic Prompt Improvement}
In contrast to feedback aggregation, REPROMPT \cite{chen2024repromptplanningautomaticprompt} and Are Large Language Models Good Prompt Optimizers? \cite{ma2024largelanguagemodelsgood} center on error diagnosis and corrective adjustments, adopting a more diagnostic approach to iterative refinement.
REPROMPT mimics a conversational feedback loop, where each response is analyzed for common points of failure \cite{chen2024repromptplanningautomaticprompt}. By identifying recurring errors, REPROMPT facilitates targeted prompt adjustments that address specific weaknesses rather than generic improvements. Each round refines the initial prompt, similar to a dialogue, enabling REPROMPT to iteratively shape prompts based on accumulated interaction data. This conversational structure makes REPROMPT especially effective in scenarios where task complexity increases over time, such as procedural planning or extended dialogue-based tasks. However, REPROMPT’s reliance on batch-based error analysis may limit adaptability in dynamic contexts, where feedback needs to be immediate and granular.
Are Large Language Models Good Prompt Optimizers? challenges the effectiveness of LLM-based reflection methods, proposing “Automatic Behavior Optimization” as an alternative \cite{ma2024largelanguagemodelsgood}. This method directly targets problematic behaviors by requiring the optimizer to identify failure steps and then refine prompts specifically at those points. Unlike REPROMPT, which iteratively refines prompts based on generalized patterns of failure, Automatic Behavior Optimization emphasizes precision by requiring the LLM to generate step-by-step instructions to address specific failures. This targeted refinement is advantageous for tasks where accuracy at each step is critical, such as instructional generation or decision-based tasks. However, the approach may be less effective in adaptive or high-variability scenarios, where failure points are not easily identifiable or repeatable.
The diagnostic focus of both REPROMPT and Automatic Behavior Optimization contrasts with the broader feedback-oriented approaches of PACE and STRAGO. REPROMPT’s conversational, batch-based error identification enables systematic adjustments across task phases, while Automatic Behavior Optimization’s step-specific refinement prioritizes direct behavioral alignment, particularly in high-precision contexts.

\subsubsection{Control-Theoretic and Structured Interaction Optimization}
Prompt Engineering Through the Lens of Optimal Control \cite{luo2023promptengineeringlensoptimal} introduces a control-theoretic perspective, treating prompt engineering as a dynamic optimization problem. Unlike feedback or error-based refinement, this approach uses a structured, systematized interaction model where the initial prompt is iteratively adjusted based on response evaluation, similar to a feedback control system.
This control-based approach involves a multi-round interaction structure, where each iteration generates refined prompts to address gaps identified in previous responses. By systematically narrowing the prompt’s focus and enhancing specificity, Optimal Control’s framework excels in tasks that require controlled iterative refinement, such as diagnostic or advisory systems where multi-step accuracy is critical. However, its reliance on sequential interaction rounds can be time-intensive, potentially limiting its application in real-time scenarios where immediate responses are needed.

\subsubsection{Contextual and Evolutionary Optimization of Complex Prompts}
In contrast to the interaction-driven methods above, Automatic Engineering of Long Prompts \cite{hsieh2023automaticengineeringlongprompts} employs a context-driven, evolutionary approach for prompt optimization, designed specifically for complex or lengthy prompts.
This method involves segmenting the initial long prompt into individual sentences, each rephrased independently by a mutator that maintains semantic consistency. Rather than direct feedback, the method employs a contextual bandit approach, which selects sentences likely to benefit the prompt’s overall performance when rephrased. Each revised prompt is then evaluated, and successful mutations inform future modifications, effectively evolving the prompt over successive iterations. This evolutionary approach is particularly beneficial in static, domain-specific tasks requiring detailed, multi-sentence instructions, such as legal or scientific writing. However, without direct feedback from task performance, the method may struggle to adapt to changing requirements, making it less suited for dynamic or highly adaptive applications. 

\section{Methods}

\subsection{Pipeline Development}
% theoretical
Based on our systematic review of recent prompt optimization techniques, we identified several key methods that form the foundation of our automatic iterative prompt engineering pipeline. 
Firstly, we will implement actor-critic feedback loops from the PACE framework, providing a robust mechanism for iterative prompt improvement by alternating between prompt generation (actor) and quality assessment (critic) \cite{dong2024paceimprovingpromptactorcritic}. 
To address prompt drift and achieve balanced refinement, we will adopt STRAGO's approach of evaluating both successful and failed cases, thereby ensuring stability across variations \cite{wu2024stragoharnessingstrategicguidance}.
Furthermore, we will utilize REPROMPT’s diagnostic feedback to analyze common points of failure, allowing us to make targeted prompt adjustments that address specific weaknesses rather than broad improvements \cite{chen2024repromptplanningautomaticprompt}.
Additionally, we will aggregate feedback from multiple iterations to enhance robustness by synthesizing insights across attempts and reducing overfitting to individual prompts \cite{dong2024paceimprovingpromptactorcritic}. 
To make it easier for the model to comply with the prompt, we will integrate step-by-step instructional guidance, which gives the model detailed steps for rectifying specific issues \cite{wu2024stragoharnessingstrategicguidance,ma2024largelanguagemodelsgood}. 
Lastly, we will employ guided mutation of prompt elements at the end of the pipeline, selectively altering high-performing prompts to explore new variations without significantly disrupting their structure \cite{hsieh2023automaticengineeringlongprompts}. These techniques, selected from state-of-the-art methodologies, aim to maximize the similarity between generated and real data, ultimately yielding an optimal prompt for synthetic medical text generation.

\begin{figure}
\includegraphics[width=\textwidth]{Architecture.png}
\caption{Overview of the Prompt Engineering Architecture} \label{fig:Arch}
\end{figure}

We structured the architecture of our prompt engineering pipeline around two primary components: the Actor-Critic Feedback Loop and the Guided Mutation of Long Prompts.
This design allows for the iterative refinement of prompts with the objective of maximizing similarity between generated data and real data, ensuring high-quality outputs that closely align with authentic examples. 
Furthermore, the process is entirely automatic, requiring only the initial prompt from the user, who then awaits the improved prompt.
Fig. \ref{fig:Arch} provides a visual representation of the suggested architectural design.
The process begins with an initial prompt, which is passed to an Prompting Model. 
The Prompting Model generates a comprehensive, step-by-step guide from the prompt by breaking down the prompt into actionable steps to enhance coherence and alignment with the desired output characteristics.
This prompt is then given to the Act-Model.
The Act-Model serves as the core of data generation and produces synthetic data based on the given prompt.
This data is then evaluated by a Score Model, which compares it to real data samples and assigns a similarity score, assessing how closely the generated content matches the real data.
In order to calculate the score, for instance, one may utilize transformer-based embedding. The mean cosine similarity between the generated data and the real data can be calculated using the embedding. A higher score indicates a greater degree of similarity between the two data types.
The feedback process branches into two distinct pathways based on the similarity score. If the score falls below a predefined threshold, the generated data, along with the corresponding prompt and score, is sent to a Diagnostic Feedback Model. This model identifies specific weaknesses in the generated content and provides targeted feedback to address these shortcomings. On the other hand, if the score exceeds the threshold, the data, prompt, and score are directed to a General Feedback Model, which focuses on preserving and reinforcing the effective elements of the prompt.
The threshold is initially determined as the mean score from the first iteration. The threshold will only be updated with the new mean score of a round if it is greater than the existing threshold.
Both feedback pathways converge in the Feedback Summarizer Model, which aggregates feedback from all generated outputs in a given round. This summarization provides a holistic view of the prompt’s performance, enabling the generation of an improved prompt that retains successful elements while addressing identified issues. 
Subsequently, the feedback is presented to the Prompting Model, which generates an enhanced, step-by-step guidance plan for the Act Model based on these observations.
This iterative process continues, with each cycle progressively refining the prompt based on cumulative feedback, until the prompt reaches a high level of performance.
To further optimize the feedback process and construction of the step-by-step prompt, two lists are maintained: one for the highest-scoring prompts and another for the lowest-scoring prompts. During each iteration, the score of the current prompt is compared to those in these lists. If the score is higher than any prompt in the highest-scoring list, the lowest-scoring prompt in that list is replaced. Conversely, if the score is lower than any prompt in the lowest-scoring list, the highest-scoring prompt in that list is replaced. These lists, along with feedback from the summarizer model, are then provided to the Prompting Model. With this combined information, the Prompting Model can generate an improved step-by-step prompt by learning from what succeeded and what fell short in previous prompts.

The second major component of the pipeline is the guided mutation of long prompts. 
Once a sufficient number of high-performing prompts have been identified, the pipeline transitions from the Actor-Critic Feedback Loop to the Guided Mutation Phase. 
A prompt from the list of high-performing prompts is randomly selected. Subsequently, the prompt is divided into its individual sentences. A sentence from the prompt is selected at random and modified by the Prompt Mutator Model in a way that preserves the semantic meaning.
Subsequently, the mutated prompt is returned to the Act-Model, where it generates new data that is evaluated by the Score Model.
In the event that the calculated score exceeds that of any other score within the list of highest-scoring prompts, the mutated prompt is substituted for the lowest-scoring prompt in the list.
This cycle continues iteratively, enabling fine-tuned adjustments to further optimize the prompt structure.
By integrating these components and feedback mechanisms, the pipeline is designed to iteratively refine prompts, ultimately producing high-quality synthetic data that closely resembles real life data.

\subsection{Implementation of the Pipeline}
We implement the pipeline in Python and execute it on a single NVIDIA A100 GPU \cite{nvidia_a100}. For the text output models, we use LLaMA 3.1 models without fine-tuning which are running through Ollama 
\cite{dubey2024llama3, ollama2024}. To ensure task clarity and improve output quality, we design specific system prompts and example messages tailored for each model in the pipeline.
The Prompting Model operates with a temperature of 0.5, top-K of 40, and top-P of 0.85, balancing precision and creativity to generate clear and structured step-by-step prompts. For the Act Model, we configure a temperature of 0.8, top-K of 50, and top-P of 0.9, enabling it to produce diverse yet coherent outputs. Both the Diagnostic Feedback Model and Feedback Model use lower creativity settings, with a temperature of 0.3, top-K of 5, and top-P of 0.5, to prioritize precision and deliver actionable feedback for low-scoring outputs or highlight strengths in high-scoring outputs. The Feedback Summarizer Model is configured with a temperature of 0.2, top-K of 5, and top-P of 0.5, focusing on the efficient aggregation of feedback to identify common trends and patterns across multiple outputs. For the Prompt Mutator Model, we employ a temperature of 0.7, top-K of 20, and top-P of 0.9 to rephrase sentences while retaining their original semantic meaning.
To ensure consistency and reproducibility across all models, we use a fixed seed value of 42.
We incorporate the embedding model jina-embeddings-v2-base-de to compute semantic embeddings for the textual data \cite{mohr2024multi}. To assess the similarity between texts, we calculate cosine similarity between their embeddings, providing a robust metric to evaluate the semantic alignment and coherence of generated outputs.
We evaluate our pipeline on the use case of generating realistic German discharge letters in the domain of cardiology. The generated outputs are compared against the CARDIO:DE dataset \cite{cardio:de}. It comprises 500 clinical routine German doctor’s letters from Heidelberg University Hospital.

\section{Results}
\label{sec:results}
% results aus der pipeline reinschreiben
% visualisierung des embeddings




\section{Discussion}
\subsection{Literature Review}
In this study, we aimed to develop an easy-to-use, automated, iterative prompt engineering pipeline to generate realistic synthetic medical data. We based our approach on a systematic review that followed PRISMA guidelines to select relevant studies on iterative prompt engineering techniques \cite{liberati2009prisma}. This approach demonstrates notable strengths, as we ensured transparency and reproducibility by adhering to the PRISMA guidelines. 
By employing a detailed search strategy and applying rigorous inclusion and exclusion criteria, we captured a focused set of studies highly relevant to the specific objectives of this research.
We tailored these criteria, such as including only papers published post-2020 and focusing solely on techniques that do not rely on training datasets, to meet the goal of identifying methods capable of optimizing prompts without traditional fine-tuning, a critical constraint in the medical data context.
However, we also identified limitations in our literature review process. While using Google Scholar allowed us to benefit from its comprehensive indexing, it might have restricted the scope to certain types of publications, potentially excluding niche studies from specialized databases. Furthermore, our decision to exclude studies requiring training datasets, although justifiable, may have overlooked techniques with adaptable components for iterative prompt engineering. While these exclusion criteria ensured relevance, they may have unintentionally limited the range of methodologies available for synthesis, potentially bypassing hybrid approaches that could be modified to function without training data.

\subsection{Architecture}
% eventuell kürzen
We propose an automatic, iterative prompt engineering architecture to address the inherent challenges of generating realistic synthetic data with LLMs, specifically in the context of complex medical text generation. Our method improves traditional prompt engineering approaches by implementing iterative feedback mechanisms that progressively optimize prompt quality, increasing the likelihood of generating accurate and realistic outputs.
A key strength of our architecture lies in its alignment with the principles outlined in REPROMPT, which emphasizes iterative prompt optimization through step-by-step feedback loops from previous interactions with the LLM \cite{chen2024repromptplanningautomaticprompt}. By refining prompts progressively, we reduce the limitations associated with single-round, static prompt engineering. Furthermore, we incorporate feedback history into prompt updates, ensuring that prompts evolve toward higher accuracy without overfitting to specific input cases.
Additionally, we draw on the strategy proposed by PACE, where prompts are refined through a dual actor-critic model that evaluates prompt effectiveness at each stage \cite{dong2024paceimprovingpromptactorcritic}. By employing a similar mechanism, we enable our architecture to dynamically respond to generated outputs, effectively mitigating issues like prompt drift, where new prompts inadvertently undermine previously effective strategies. This dynamic refinement plays a crucial role in generating medical data, where achieving high precision and consistency is essential.
Moreover, strategies similar to those in STRAGO bolster the architecture’s iterative optimization, utilizing in-context learning to extract actionable insights from both successful and failed cases, thus providing a stable foundation for prompt optimization \cite{wu2024stragoharnessingstrategicguidance}.
By leveraging both successful prompt outcomes and errors, the architecture reduces the risk of performance degradation when facing outlier cases. This balance between refinement and stability ensures that the prompts remain versatile across diverse clinical scenarios.
The prompt optimization pipeline also aligns with the principles in "Automatic Long Prompt Engineering" by Hsieh et al., which emphasize the targeted adjustment of specific prompt elements while preserving their core semantic meaning \cite{hsieh2023automaticengineeringlongprompts}.
This approach enables the proposed architecture to iteratively refine prompts by making subtle, controlled modifications to certain sections. By focusing on precise adjustments rather than broad changes, the architecture maintains consistency in the prompt's intent, making it easier to evaluate whether these refinements improve output quality.
However, this architecture has several inherent limitations. First, it relies on iterative feedback, which, while effective in refining prompts, introduces a significant computational cost. Similar to methods discussed by Luo et al., the architecture applies a multi-step approach like a controlled sequence of prompt adjustments, but it requires substantial computational resources to handle these step-by-step refinements \cite{luo2023promptengineeringlensoptimal}. This reliance may limit its scalability, especially when applied to large datasets or highly variable medical scenarios.

Another challenge is that iterative refinement sometimes produces prompts that resolve specific cases but degrade performance in others, a concern also noted in STRAGO \cite{wu2024stragoharnessingstrategicguidance}. Although the architecture mitigates this through a feedback loop that includes successful and failed cases, it does not eliminate this risk, particularly if the model encounters diverse or contradictory feedback. As the prompts evolve, slight adjustments may unintentionally misalign prompts with certain medical contexts, reducing the overall reliability of the output.
Additionally, the architecture depends on the LLM’s inherent capabilities to generate useful variations in each round. Similar to the limitations observed in PACE, which relies on actor-critic models, the quality of each prompt iteration ties directly to the LLM's ability to self-reflect and generate meaningful feedback \cite{dong2024paceimprovingpromptactorcritic}. If the LLM reflects poorly on errors or if the initial prompt is poorly constructed, the model may struggle to generate meaningful refinements, creating a potential bottleneck in the prompt optimization process.

We enhance user acceptance and address privacy concerns in medical AI by emphasizing user-friendliness and robust privacy protections. By designing an automated pipeline that requires minimal user input, we reduce the complexity traditionally associated with prompt engineering, making it accessible for users without technical expertise. We also mitigate privacy issues by generating synthetic medical data without relying on real patient data, thus aligning with privacy regulations and ethical standards. This streamlined, privacy-respecting method improves trust and increases the adoption of AI technologies within the medical sector.

\subsection{Results}
% auf die Ergebnisse der Implementation eingehen

\section{Conclusion}
% hier auch auf die endgültigen ergebnisse eingehen
In this study, we developed an automated, iterative prompt engineering pipeline to generate realistic synthetic medical data while ensuring privacy and enhancing user accessibility. This framework provides a foundation for producing synthetic data that closely mimics real-world medical records without relying on sensitive patient information. By addressing privacy concerns and simplifying the data generation process, we aim to foster trust and acceptance of AI in healthcare, supporting broader integration of AI-driven solutions in the medical field.


%
% ---- Bibliography ----
%
%
 \bibliographystyle{splncs04}
 \bibliography{mybibliography}
%

\end{document}

