\section{Related Work}
~\label{sec:relected_work}
We briefly introduce related work, including reasoning with LLMs, inference-time computation methods for LLM reasoning, and benchmarks of LLM reasoning.



\textbf{Reasoning with LLMs.}
LLMs have demonstrated strong reasoning abilities in complex tasks such as code generation, mathematical problem-solving, and research ideation**Brown et al., "Language Models as Knowledge Bases"**. Existing methods for enhancing LLM reasoning include: 1) Prompt engineering – Activates latent multi-step reasoning capabilities. For example, Chain of Thought (CoT)**Wang et al., "Chain of Thought Prompt Engineering for Conversational AI"**, guides step-by-step problem-solving but relies heavily on high-quality demonstrations for analogical learning. 2) Post-training techniques**Stengel and Dickinson, "Post-Training Language Model Analysis"**, – Iteratively enrich training datasets to improve model performance. Self-training methods**Liu et al., "Self-Trained Language Models with Reasoning Capabilities"**, curate new high-quality examples to enhance reasoning, but these approaches demand significant computational resources. 3)Search-based methods**Kim and Lee, "Inference-Time Search for LLMs"**, – Optimize reasoning paths at inference time using search algorithms. For instance, Tree of Thought**Kang et al., "Tree of Thought: Improving Reasoning with Deep Neural Networks"**, employs breadth-first search to refine solutions.
This work focuses on test-time computation, leveraging inference-time optimization to enhance LLM reasoning without additional training overhead.


\textbf{Inference-Time Computation of LLM Reasoning.} Scaling inference-time computation has proven more effective than merely increasing model parameters**Thompson and Lee, "Efficient Inference for Large Language Models"**. Recently, research has focused on optimizing reasoning efficiency during inference rather than solely scaling training-time computation. Best-of-N**Li et al., "Best-of-N: Efficient LLM Reasoning with Sampling and Verification"**, enhances LLM reasoning by sampling N candidate solutions, evaluating them with a learned verifier or reward model, and selecting the highest-scoring one. Similarly, MCTS**Huang et al., "MCTS: A Novel Method for Inference-Time Optimization of LLMs"**, improves inference by actively planning and selecting higher-quality responses. These advancements highlight inference-time optimization as crucial for enhancing LLM reasoning beyond scaling training computation.



\textbf{Benchmarks of LLM Reasoning.}  LLMs have made remarkable progress in solving complex tasks in a zero-shot manner**Radford et al., "Improving Language Understanding by Generative Models"**, positioning them as a key milestone toward artificial general intelligence. Consequently, benchmarking their reasoning abilities has become a central challenge. Recent studies have evaluated LLM reasoning across various domains, including mathematical reasoning**Jia and Liang, "Mathematical Reasoning for Large Language Models"**, code generation**Gul et al., "Code Generation with Large Language Models"**, and factual QA**Sun et al., "Factual Question Answering with Large Language Models"**. While these benchmarks enhance our understanding of LLM reasoning, most research has focused on task performance rather than inference-time computation, leaving key optimization techniques underexplored.




Unique to this paper, we are the first to comprehensively study how LLM reasoning performance changes with the incorporation of previously overlooked key techniques. We hope that our work will provide valuable insights into the role of inference-time computation.


\vspace{-0.2in}