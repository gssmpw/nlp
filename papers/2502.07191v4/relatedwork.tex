\section{Related Work}
~\label{sec:relected_work}
We briefly introduce related work, including reasoning with LLMs, inference-time computation methods for LLM reasoning, and benchmarks of LLM reasoning.



\textbf{Reasoning with LLMs.}
LLMs have demonstrated strong reasoning abilities in complex tasks such as code generation, mathematical problem-solving, and research ideation~\cite{zhou2022least}. Existing methods for enhancing LLM reasoning include: 1) Prompt engineering – Activates latent multi-step reasoning capabilities. For example, Chain of Thought (CoT)~\cite{wei2022chain_cot} guides step-by-step problem-solving but relies heavily on high-quality demonstrations for analogical learning. 2) Post-training techniques\cite{chen2024bootstrapping, chen2024alphamath} – Iteratively enrich training datasets to improve model performance. Self-training methods\cite{chen2024bootstrapping} curate new high-quality examples to enhance reasoning, but these approaches demand significant computational resources. 3)Search-based methods\cite{browne2012survey, feng2023alphazero, liu2023making} – Optimize reasoning paths at inference time using search algorithms. For instance, Tree of Thought\cite{yao2024tree} employs breadth-first search to refine solutions.
This work focuses on test-time computation, leveraging inference-time optimization to enhance LLM reasoning without additional training overhead.


\textbf{Inference-Time Computation of LLM Reasoning.} Scaling inference-time computation has proven more effective than merely increasing model parameters~\cite{snell2024scaling}. Recently, research has focused on optimizing reasoning efficiency during inference rather than solely scaling training-time computation. Best-of-N~\cite{cobbe2021training_best_of_n} enhances LLM reasoning by sampling N candidate solutions, evaluating them with a learned verifier or reward model, and selecting the highest-scoring one. Similarly, MCTS~\cite{tian2024toward} improves inference by actively planning and selecting higher-quality responses. These advancements highlight inference-time optimization as crucial for enhancing LLM reasoning beyond scaling training computation.



\textbf{Benchmarks of LLM Reasoning.}  LLMs have made remarkable progress in solving complex tasks in a zero-shot manner~\cite{hendrycks2021measuringMATH, press2022measuring_bamboogle, liu2024jailjudgecomprehensivejailbreakjudge}, positioning them as a key milestone toward artificial general intelligence. Consequently, benchmarking their reasoning abilities has become a central challenge. Recent studies have evaluated LLM reasoning across various domains, including mathematical reasoning~\cite{hendrycks2021measuringMATH}, code generation~\cite{chen2021codex_humaneval}, and factual QA~\cite{Thorne18Fever}, etc~\cite{liu2024regmix, liu2024adversarial}. While these benchmarks enhance our understanding of LLM reasoning, most research has focused on task performance rather than inference-time computation, leaving key optimization techniques underexplored.




Unique to this paper, we are the first to comprehensively study how LLM reasoning performance changes with the incorporation of previously overlooked key techniques. We hope that our work will provide valuable insights into the role of inference-time computation.


\vspace{-0.2in}