\section{Experiments}
\label{sec:experiments}

We provide an empirical evaluation of \sname by investigating the following questions:
\begin{itemize}[leftmargin=*,topsep=0.0pt,itemsep=.5pt]
\item Can \sname enhance reasoning performance? (Table \ref{tab:main})
\item Does confidence-aware sampling improve the performance? (Figure \ref{figure:test_time_compute} and Figure \ref{figure:inference_score})
\item Does/How does the proposed curriculum learning improve the performance? (Figure \ref{figure:curriculum})
\item Can \sname perform self-verification and -refinement? (Figure \ref{figure:verification_distribution} and Figure \ref{figure:analysis_refinement}) 
\end{itemize}

\textbf{Training setup.}
For the main experiment, we train \sname on Llama-3 models with 1B and 8B parameters, which are not instruction-tuned. We avoid using instruction-tuned models to prevent potential bias from exposure to the gold data of the tasks \citep{wang2024scpo}. For this reason, the models were first supervised fine-tuned using the labeled dataset, followed by fine-tuning with each respective method. For GSM8K \citep{cobbe2021gsm8k}, we train \sname using the original training split. For MATH \citep{hendrycks2021math}, we train \sname using a 50k subset of MetaMath \citep{yu2024metamath}, an augmented version of MATH, and use a 3k subset for the validation set, respectively. Here, MetaMath was employed to mitigate the performance degradation caused by the limited size of the original MATH.

\textbf{Baselines.} We compare our method against several baseline approaches: Supervised Fine-Tuning (SFT), \baserft \citep{yuan2023scaling}, and \basestarplus. In \baserft, fine-tuning is performed on supervised fine-tuning data $\mathcal{D}$ and correctly generated samples selected from $k$ completions for each input in the training set by a tuned model. Like \baserft,~\basestar~\citep{zelikman2022star} trains on correctly generated samples, including self-generated rationales given a hint (rationalization). However, unlike \baserft, \basestar~iteratively refines this process without relying on $\mathcal{D}$. Since both \sname and \baserft~utilize ground truth data $\mathcal{D}$, we introduce an extended version of \basestar~that incorporates SFT data as a baseline, referred to as \basestarplus. Essentially, \basestarplus~functions as a multi-iteration variant of \baserft~with rationalization. We run \basestarplus~for three iterations, sampling $k$ completions per iteration (GSM8K: $k=10$, MATH: $k=4$, GSM240K: $k=1$) with a temperature of 0.7 for both \baserft~and \basestarplus. We initialize a model for each iteration from $\mathcal{M}_{0}$ that is supervised fine-tuned with $\mathcal{D}$ at each iteration for \basestarplus to prevent overfitting.

\textbf{Evaluation setup.} We mainly report Majority Voting at N (Maj@N) as a sampling-based metric, exceptionally \sname used verification confidence-aware majority voting as described in Section \ref{sec:method_scale} (unless otherwise specified). We evaluate \sname and baselines on GSM8K \citep{cobbe2021gsm8k} and MATH-500 \citep{hendrycks2021math}, a widely used evaluation benchmark subset of MATH.

\subsection{Main Results}

\input{resource/fig_test_time_compute}


We first present the main result by comparing the math problem-solving performance with other baselines. Here, we mainly compare \sname with various fine-tuning schemes that use a single network and do not use reinforcement learning. Furthermore, we demonstrate the performance of each method with simple test-time scaling methods (i.e., majority voting for baseline methods and using our verification-aware sampling for \sname). 

As shown in Table \ref{tab:main}, we present the math-solving performance of \sname compared to other baselines. Overall, \sname significantly and consistently outperforms all prior baseline methods. It is worth noting that for both GSM8K and MATH-500, \sname achieves the highest Maj@1, indicating that \sname is already strong without the proposed sampling scheme. For instance, \sname attains 33.6\% for Maj@1, significantly outperforming SFT (30.4\%) and few-shot CoT (23.4\%) on MATH-500 with Llama-3.1-8B. In addition, with the proposed confidence-aware majority voting, \sname marked a 4.0\% gain after refinement and consistently outperforms other baselines under five sampled answers. These results demonstrate that \sname enhances problem-solving accuracy and improves test-time scaling abilities.


\subsection{Inference Scalability of \sname} 

In this section, we evaluate the inference scalability of \sname. To this end, we visualize how the test-time scaling improves as one samples more candidates. Specifically, we conduct experiments using our method with different sample sizes $N\in\{2,3,4,8\}$ and compare with results of other baselines using majority voting. 
As shown in Figure \ref{figure:test_time_compute}, \sname achieves significant and consistent gain in all setups. 
For instance, \sname shows a large gap with the strongest baseline RFT, showing 3.3\% of improvement in MATH-500 at $N=8$. 
%, which is a challenging benchmark. 
Furthermore, our method even benefits under limited number of samples ($N=2$), while majority voting does not show improvement. 
This is because majority voting does not use confidence and, hence, can not benefit from small samples (e.g., if all predictions are disjoint, the majority voting does not work). 
Finally, \sname shows scalable improvements in all model configurations, ranging from relatively small 1B models to large 8B models. 
Notably, \sname achieves a significant performance gain in the 8B model, suggesting strong generalization capabilities.


\subsection{Additional Analysis and Ablation}

In this section, we provide a detailed analysis of \sname to validate the effect of each proposed component. 
Unless otherwise specified, we use a Llama-3.2-1B trained on GSM8K across all methods throughout this section.

\input{resource/fig_ablation_experiments}
\textbf{Effectiveness of curriculum learning.}
We validate the effectiveness of the proposed curriculum learning (in Figure \ref{figure:curriculum}). 
To this end, we train two types of models.
First, the model trains without curriculum by optimizing SFT $\mathcal{L}_{\mathtt{SFT}}$ and preference $\mathcal{L}_{\mathtt{Pref}}$ loss by using all preference dataset at once, i.e., $\mathcal{D}_{\mathtt{correct}}$. 
Second, we train the model only using the first verification loss, i.e., $\mathcal{D}_{\mathtt{verify}}$ (note that self-verification already enables the model to generate the answer but does not know how to correct the generation). 
As shown in Figure \ref{curriculum_acc}, the curriculum is indeed showing a significant improvement over no curriculum baseline (even the model has used the same preference dataset); two-stage curricula improve the performance from 22.6\% to 28.1\%.

To further investigate this phenomenon, we evaluate the self-verification accuracy of each method, which measures the model’s ability to predict whether its own output is correct. 
In Figure \ref{AUROC}, we report the verification accuracy in terms of the Area Under the Receiver Operating Characteristic Curve (AUROC) for three models. Notably, the model without curriculum learning achieves an AUROC of 71\%, while two-stage curriculum learning improves this to 76\%. 
This suggests that curriculum learning enhances self-verification, allowing the model to refine its predictions based on more reliable verification signals. 
However, we observe that training at stage 2 slightly degrades verification accuracy, indicating that the self-correction task $\mathcal{D}_{\mathtt{correct}}$ is particularly challenging and may lead to catastrophic forgetting \citep{mccloskey1989catastrophic}. 
Exploring optimization strategies that improve self-verification and self-correction without compromising overall performance remains an interesting direction for future work.


\input{resource/fig_ablation_dpo}
\textbf{Effectiveness of preference learning.} The role of DPO loss in \sname is to guide the model to prefer refining when the initial attempt is incorrect and terminating otherwise. Additionally, in our DPO objective, we applied SFT loss to the chosen sequence as introduced in \citet{liu2024rpoloss} which applied SFT loss to the selected sequence, $\mathcal{L}_{\mathtt{Ours}} := \mathcal{L}_{\mathtt{SFT}}(\mathcal{D}) + \lambda ~ \mathcal{L}_{\mathtt{Pref}}(\mathcal{D}) $, where $\lambda$ is a constant. 
Specifically, ablation experiments without the DPO loss—where only the SFT loss is utilized—in Figure \ref{figure:ablation_dpo} show that \sname without DPO demonstrates significantly lower performance $-10.3\%$ compared to the full-trained \sname. This indicates that the DPO loss is critical in \sname for effectively guiding the refinement process.

\input{resource/fig_classification_performance}

\textbf{Analysis on the self-verification confidence of \sname.} 
We further analyze the confidence distribution in self-verification to assess whether the model's confidence is well aligned with actual correctness. To this end, we visualize the probability gap between \eos and \rethink for a given context $x$, simply defined as $\mathcal{M}([\mathtt{eos}]) - \mathcal{M}([\mathtt{refine}])$. 
As shown in Figure \ref{figure:verification_distribution}, incorrect responses tend to have lower \eos probabilities, whereas correct responses exhibit higher \eos probabilities. This demonstrates the model’s intrinsic ability to assess its own correctness. Moreover, these results suggest that confidence serves as a reliable metric for calibrating the sampling score, further validating the effectiveness of our confidence-aware sampling method.



\input{resource/tab_instruction}
\textbf{\sname on instruction-tuned models.} 
While we have primally focused on pretrained models and initialized $\mathcal{M}_0$ with the given supervised fine-tuning dataset $\mathcal{D}$ due to the possible data contamination, we also have conducted an experiment on Llama-3.2-1B-Instruct, i.e., instruction-tuned model.
Interestingly, as shown in Table \ref{tab:instruction}, all fine-tuning methods, except for \sname, underperform the zero-shot CoT baseline when training with GSM8K. 
This outcome aligns with the widely recognized challenge that fine-tuning instruction-tuned models often leads to catastrophic forgetting, hindering their ability to learn new information effectively with a small-sized dataset.
Meanwhile, \sname remains notably resistant to this issue. We hypothesize that this advantage stems from how \sname utilizes the gold label $y$—only incorporating it as a revised second-attempt completion rather than directly fine-tuning it. In contrast, baselines such as SFT, \baserft, and \basestarplus~rely on fine-tuning the base model on $\mathcal{D}$, which becomes problematic when the target model’s performance is already strong, as it struggles to gain further improvements from $\mathcal{D}$. 

To this end, we also trained the model using GSM240K, a subset of MetaMath dataset \citep{yu2024metamath}, which expands the original data about 30-fold by rephrasing questions and answers. 
As shown in Table \ref{tab:instruction}, while training with GSM240K improved the performance of the SFT baseline, \sname still exhibited better performance. 
This result suggests that \sname can adapt to various data characteristics, even in heavily augmented settings.

\input{resource/fig_inference_score}
\textbf{Ablation study on confidence-aware sampling.} 
We explore the impact of different score calibrations during inference by leveraging \sname's self-verification mechanism to enable test-time compute-scalable inference strategies (see Section \ref{sec:method_scale}). 
Specifically, we compare three scoring schemes: (1) weighted majority voting using \( \mathcal{M}(\text{\eos}|x) \), (2) unweighted majority voting, and (3) scoring based on the model's predicted answer likelihood. 
These calibration methods govern both the selection of candidate answers and the evaluation of their validity. 

As shown in Figure \ref{figure:inference_score}, \( \mathcal{M}(\text{\eos}|x) \)-based (Ours) score consistently outperforms alternatives across GSM8K benchmarks. 
For example, with eight sampled candidates, \( \mathcal{M}(\text{\eos}|x) \)-based scoring achieves an accuracy of 33.9\%, compared to 33.2\% (unweighted majority), and 32.7\% (likelihood-based). 
The trend persists across all tested sampling budgets, suggesting strong compatibility with self-verification mechanisms. 
This consistent advantage implies \( \mathcal{M}(\text{\eos}|x) \) better aligns with the model's intrinsic verification capability to distinguish correct reasoning paths. 
We carefully hypothesize that \( \mathcal{M}(\text{\eos}|x) \) acts as a latent indicator of solution correctness, as premature termination often correlates with reasoning errors.


\newcommand{\first}{\textit{First}\xspace}
\newcommand{\retry}{\textit{Retry}\xspace}

\input{resource/fig_analysis_refinement}
\textbf{Analysis on the refinement.} We demonstrate that \sname refines its answers based on the initial attempt rather than randomly generating a new completion. 
To evaluate this, we compare \sname with two baselines: \first and \retry. 
\first terminates decoding at the \rethink token, while \retry generates a new completion upon encountering \rethink.
Specifically, \retry greedily decodes the first attempt, and if \rethink appears, it samples a new completion with a temperature of $0.7$ following the prompt $x$. In contrast, both \first and \sname greedily generate completions.
% 
As shown in Figure \ref{figure:analysis_refinement}, \sname outperforms both \first and \retry. This result highlights that \sname does not generate new responses arbitrarily but instead meaningfully refines and improves upon its initial answer.