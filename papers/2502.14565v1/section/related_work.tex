\section{Related Work}
\label{sec:related}


In this section, we provide a comprehensive review of related works, including reasoning, test-time scaling, and self-improvement for LLMs.

\textbf{LLM reasoning.} LLMs have made significant progress in reasoning through techniques such as Chain-of-Thought (CoT) prompting, fine-tuning, and self-improvement. CoT prompting, introduced by~\citep{wei2022emergent} and expanded by~\citep{kojima2022large} enables models to break down complex problems into intermediate steps, improving performance and interpretability. Structured reasoning methods, including self-consistency \citep{wang2022self} and Tree-of-Thought (ToT)~\citep{yao2024tree}, enhance multi-step problem-solving by exploring various reasoning paths. \citet{huang2022large} have demonstrated self-improvement through iterative feedback, refining their outputs over time. Ensuring the reliability of reasoning approaches such as Reflexion~\citep{shinn2024reflexion} and Self-Refine~\citep{madaan2023selfrefine} introduce iterative feedback loops, while verification techniques like step-by-step validation~\citep{lightman2023let} help maintain consistency and reduce errors. Unlike prior approaches, \sname learns self-verification during training, reducing train-test discrepancy and enabling more natural verification at inference.

\textbf{Test-time scaling for LLMs.} Recent works explored that scaling test-time computation, such as best-of-N sampling, can be even better than scaling train-time computation for performance \citep{snell2024testcompute}. Specifically, test-time scaling strategies improve LLM performance by generating numerous candidate outputs and selecting the best. To enhance decision-making, external verifiers are often employed to evaluate and refine these outputs \citep{liang2024improving}. Moreover, \citet{kumar2024score, qu2024rise} applied extensive reinforcement learning to overcome the efficiencies and dependence on the verifier's performance. In safety research, backtracking methods have introduced reset tokens to correct unsafe responses \citep{zhang2024backtracking}. While they focus on reducing the likelihood of unsafe outputs with limited second attempts to refuse answers, our approach targets complex reasoning tasks enabled by self-correction through an explicit verification process and two-stage curricula.

\textbf{Self-improvement for LLMs.} Self-training methods enable LLMs to refine themselves using their own outputs. Supervised fine-tuning (SFT) \citep{brown2020lmfewshot} trains on human-annotated data but lacks self-correction \citep{huang2023large}. Rejection fine-tuning (RFT) \citep{yuan2023scaling} improves robustness by filtering low-quality responses but discards useful learning signals. STaR \citep{zelikman2022star} iteratively fine-tunes models on self-generated solutions but struggles with compounding errors due to the absence of explicit verification. V-STaR \citep{hosseini2024vstar} extends STaR by jointly training a verifier alongside the generator, leveraging both correct and incorrect responses to improve self-assessment, though it still depends on large-scale self-generated data. However, discovering high-quality solutions remains a challenge, as \citep{luong2024reft} shows that RL-based fine-tuning is ineffective without supervised initialization. \citet{kim2024speculative} explore using a stronger LLM to refine incorrect rationales from a smaller model, though \citet{huang2024correctyet} argue that LLMs struggle with self-correction. Our approach integrates both generation and verification, leveraging correct and incorrect responses for more effective self-improvement.
