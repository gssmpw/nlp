\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have demonstrated remarkable success across diverse domains, such as coding assistants \citep{zhang2024codeagent}, search engines \citep{xiong2024search}, and personal AI assistants \citep{sajja2024artificial}, progressively advancing toward human-like logical reasoning capabilities \citep{amir2024tomeval}. However, tasks requiring rigorous System 2 thinking—such as complex reasoning \citep{jaech2024openaio1}, iterative trial-and-error \citep{song2024trial}, and dynamic planning \citep{xie2024humanplan}—remain highly challenging \citep{lowe2024system, cai2024systemmath}. A key difficulty in LLM reasoning is that errors in early steps can accumulate over time, leading to substantial inaccuracies \citep{lecun2022path}, while the models’ intrinsic ability to detect and rectify such self-generated errors—often framed as a form of self-awareness—remains insufficient. This issue is further exacerbated by the autoregressive nature of LLMs, which constrains their ability to revisit and revise prior steps \citep{bachmann2024pitfalls}.


To tackle this issue, recent approaches have emphasized verification (or correction) of LLM-generated reasoning trajectories as a crucial mechanism \citep{zhang2024mathmcts, madaan2023selfrefine}. For instance, some methods utilize external large-scale verifiers to iteratively validate outputs and trigger regeneration \citep{luo2024improve}. However, the reliance on expensive external models introduces computational inefficiencies. Alternatively, reinforcement learning (RL)-based techniques have shown promise in improving reasoning accuracy by optimizing reward signals based on ground-truth correctness, enabling self-correction \citep{kumar2024score}. However, RL is a complex and often unstable procedure \citep{mnih2015human, rafailov2023DPO}, and it does not explicitly model the verification of intermediate reasoning steps, making it difficult to assess whether a model is confident in its current trajectory or prone to deviating toward incorrect conclusions, which may limit interpretability and adaptability in complex reasoning tasks.

This raises a key question: \textit{Can LLMs be equipped with an internal mechanism to explicitly verify their own reasoning and correct potential errors based on their verification?}

\textbf{Contribution.} We propose \textbf{Re}fine \textbf{V}ia \textbf{I}ntrinsic \textbf{SE}lf-Verification (\sname), a novel and effective self-correction framework for LLM reasoning using self-verification. The core idea of \sname is to enable LLMs to assess their reasoning process and refine reasoning trajectories based on self-verification. Specifically, we introduce a special token, which outputs whether to stop the generation or revise the reasoning trajectory. To train the model to utilize this token effectively, we design a two-stage curriculum to simplify the learning of two challenging tasks—self-verification and self-correction—by breaking them into separate training stages. Here, both stages employ preference learning, allowing the model to learn these tasks efficiently without heavy computational overhead. In the first stage, we collect pairs of correct and incorrect reasoning trajectories (i.e., positive and negative samples for preference learning) based on output correctness to develop the model’s self-verification ability. In the second stage, we generate new preference pairs for self-correction by constructing positive samples where a correct reasoning path follows an incorrect one, and negative samples where an incorrect reasoning path follows a correct one.

Furthermore, we introduce an inference-time scaling strategy for \sname that leverages self-verification to enhance performance. First, as \sname inherently verifies and refines reasoning paths when it detects incorrect outputs, it naturally benefits from increased test-time computation. Additionally, we propose a novel test-time sampling scheme that incorporates self-verification confidence (i.e., the confidence in deciding whether to terminate generation). Specifically, we integrate this confidence into existing test-time sampling methods by adjusting the sampling score based on the predicted confidence, leading to more reliable output.

We demonstrated the effectiveness of \sname through evaluations on multiple reasoning datasets across mathematical and coding domains. Notably, \sname enhances reasoning performance beyond prior methods, improving accuracy from 27.1$\to$31.1\% on GSM8K (Maj@3) \citep{cobbe2021gsm8k} with Llama3 1B \citep{dubey2024llama} and from 33.2$\to$36.0\% on MATH (Maj@3) \citep{hendrycks2021math} with Llama3 8B. Furthermore, our experimental results show that \sname consistently improves accuracy without relying on external feedback mechanisms, which often degrade performance on complex reasoning tasks. For instance, unlike approaches such as Refine \citep{madaan2023selfrefine}, which struggle when combined with existing models on complex tasks, \sname achieves these gains purely through self-verification and self-correction. Finally, we show that the proposed sampling scheme is more efficient than other sampling strategies when applied to models trained with \sname, further enhancing the performance.
