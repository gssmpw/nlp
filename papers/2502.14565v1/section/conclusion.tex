\section{Conclusion}
\label{sec:con}
In this paper, we introduce \mmname\ (\sname), a novel framework that enables Large Language Models (LLMs) to perform self-verification and self-correction during inference. Through a structured curriculum learning approach, we demonstrated how LLMs can progressively learn to verify their reasoning and improve their outputs. Our results across various reasoning benchmarks show that \sname significantly improves first-attempt accuracy while maintaining efficiency. Furthermore, the self-verification mechanism and a confidence-aware decoding strategy enhance model performance without introducing additional computational overhead.

\section*{Impact Statement}

This work advances the reasoning of Large Language Models (LLMs) by introducing \mname, a framework that enables self-verification and self-correction. This has potential applications requiring precise reasoning, such as automated tutoring and decision support systems. Specifically, \sname can benefit safety by rigorously revising the model's response in the aspect of the model.
