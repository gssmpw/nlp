\section{Preliminary} \label{sec:preliminary}
\paragraph{Random variable and distribution.} Let $\mathcal{X} = \mathcal{X}_v \times \mathcal{X}_t$ denote the input space, where $\mathcal{X}_v$ and $\mathcal{X}_t$ correspond to the visual and textual feature spaces, respectively. Similarly, let $\mathcal{Y}$ denote the response space. We define the random variables $\mathbf{X} = (X_v, X_t) \in \mathcal{X}$ and $Y \in \mathcal{Y}$, where $\mathbf{X}$ is the sequence of tokens that combine visual and text input queries, and $Y$ represents the associated response tokens. The joint population is denoted by $P_{\mathbf{X}Y}$, with marginals $P_{\mathbf{X}}$, $P_{Y}$, and the conditional distribution $P_{Y|\mathbf{X}}$. For subsequent sections, $P_{\mathbf{X}Y}$ refers to the instruction tuning data distribution which we consider as in-distribution (ID). 

\paragraph{MLLM and visual instruction tuning.} MLLM usually consists of three components: (1) a visual encoder, (2) a vision-to-language projector, and (3) an LLM that processes a multimodal input sequence to generate a valid textual output $y$ in response to an input query $\mathbf{x}$. An MLLM can be regarded as modeling a conditional distribution $P_{\theta}(y|\mathbf{x})$, where $\theta$ is the model parameters. To attain the multimodal conversation capability, MLLMs commonly undergo a phase so-called \textit{visual instruction tuning} \cite{liu2023visual, dai2023instructblip} with an autoregressive objective as follows:
{
\begin{align} \label{eq::1}
    % & \min_{\theta\in\Theta} \mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}} [-\log P_{\theta}(y|\mathbf{x})] \nonumber \\
     \min_{\theta\in\Theta} \mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}} [\sum_{l=0}^{L}-\log P_{\theta}(y_{l}|\mathbf{x},y_{<l})],
\end{align}}
where $L$ is a sequence length and $y=(y_{0},...,y_{L})$. After being trained by Eq. \eqref{eq::1}, MLLM produces a response given a query of any possible tasks represented by text.

\paragraph{Evaluation of open-ended generations.} 

(M)LLM-as-a-judge method \cite{zheng2023judging, kim2023prometheus} is commonly adopted to evaluate open-ended generation. In this paradigm, a judge model produces preference scores or rankings for the responses given a query, model responses, and a scoring rubric. Among the evaluation metrics, the \emph{win rate} (Eq. \eqref{eq:win_rate}) is one of the most widely used and representative.


\begin{definition}[\textbf{Win Rate}] Given a parametric reward function $r:\mathcal{X}\times \mathcal{Y}\rightarrow \mathbb{R}$, the 
win rate (WR) of model $P_{\theta}$ w.r.t. $P_{\mathbf{X}Y}$ are defined as follows: 
\begin{equation} \label{eq:win_rate}
\begin{split}
    &\text{WR}(P_{\mathbf{X}Y};\theta):=\mathbb{E}_{\begin{subarray}{l} \mathbf{x},y \sim P_{\mathbf{X}Y} \\ \hat{y} \sim P_{\theta}(\cdot|\mathbf{x}) \end{subarray}}[\mathbb{I}(r(\mathbf{x},\hat{y}) > r(\mathbf{x},y))],
\end{split}
\end{equation}
where $\mathbb{I}(\cdot)$ is  the indicator function.
\end{definition}
Here, the reward function $r(\cdot,\cdot)$, can be any possible (multimodal) LLMs such as GPT-4o \cite{hurst2024gpt}.

