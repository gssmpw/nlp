\section{Additional Description for Effective Mutual Information}\label{appendix:emi}
We propose effective mutual information (EMI) as an alternative to vanilla mutual information (MI) to evaluate a model-generated output response given an input query. As explained in Section \ref{sec:3_2_emi}, MI (e.g., $I(P_{\mathbf{X}}\otimes P_{\theta})$) can not take into account the intrinsic characteristics of data distribution. See Figure \ref{fig:emi_motivation} for an intuitive example. The amount of information represented by entropy $H(\cdot)$ and conditional entropy $H(\cdot|\cdot)$ can vary depending on the data-generating process of each dataset. For example, if the task we are interested in is closer to solving a narrow problem in some specific domain (e.g., OOD1: LLaVA-Med; \citet{li2024llava}), the cardinality of the desired output response space may be significantly smaller than that of a general problem-solving task in a general domain (e.g., OOD2: LLaVA-Bench Wild; \citet{liu2023visual}), and the ground truth MI can differ depending on the domain. By considering these baseline amounts of information, EMI can measure how much our model captures \textbf{effective relevance} between input and output.

In Section \ref{sec:3_2_emi}, we provide some justifications for using EMI as an evaluation metric of MLLMs by revealing analogies to excess risk and effective robustness and presenting its theoretical connection to win rate. While LLM-as-a-Judge enables flexible evaluation for open-ended generation tasks with multiple user-defined criteria, EMI confines the facet of evaluation to query-response relevance. However, compromise in the flexibility of evaluation endows us to build solid theoretical statements that are necessary for understanding MLLMs and improving them in a principled way.

\input{figures/s3_emi_motivation}

Meanwhile, as we adopt neural network models for empirical estimation of EMI, it is somewhat similar to the model-based heuristic metrics, such as BERTscore \cite{zhang2020bertscore}, BARTscore \cite{yuan2021bartscore}, and CLIPscore \cite{hessel2021clipscore}, that map input(s) to a scalar score through a single forward evaluation of the model. However, we take a step further beyond the simple working-heuristic method and lay a theoretical foundation with EMI.

\section{Implementation Details}\label{appendix:implementation_details}
In this paper, we proposed EMI for a reliable evaluation of multimodal large language models (MLLMs) with a theoretical ground. Based on EMI, to analyze the MLLM performance gap under distribution shift, we provided the upper bound for EMI difference between ID and OOD data. In this section, we describe the procedures for estimating EMI and its upper bound in detail. 

\paragraph{Overview.} To estimate EMI and its upper bound, we first need to define estimators for MI and Jensen-Shannon divergence (JSD). Those estimators commonly adopt neural network encoders to project the raw data such as text and image into embedding space of neural networks to reduce problem complexity \cite{oord2018representation, liu2020learning}, and then, MI estimator commonly optimizes a simple critic function \cite{poole2019variational} on top of the embeddings of data. After training of MI estimator, we evaluate empirical MI over different data distributions. For JSD estimation, given the embedding spaces of pre-trained models, additional training is not necessary. Therefore, the procedures can be divided into two phases: (1) neural MI estimator training, and (2) inference of MI and JSD. 



\paragraph{MI estimation.} Estimating MI with finite samples from unknown population distribution is a non-trivial problem, and has been actively studied \cite{fraser1986independent, paninski2003estimation, kraskov2004estimating, nguyen2010estimating, shwartz2017opening, mine2018, poole2019variational, cheng2020club}. We adopted the contrastive log-ratio upper bound (CLUB; \citet{cheng2020club}) as our default MI estimator similar to \cite{cheng2021fairfil}. We first extract embeddings for visual input query $Z_{v}=\text{enc}_{v}(X_{v})$ and text input query $Z_{t}=\text{enc}_{t}(X_{t})$ from visual and text encoder models and take the mean of them to provide input query embedding $Z_{\mathbf{X}}=\frac{Z_{v}+Z_{t}}{2}$. Specifically, we adopt the most representative embedding models for each modality, i.e., CLIP pre-trained\footnote{ \url{https://github.com/openai/CLIP}.} ViT-B/32 and XLM-RoBERTa-Base\footnote{\url{https://huggingface.co/FacebookAI/xlm-roberta-base}} \cite{conneau2019unsupervised} as visual and text encoders, respectively by default. We also obtain the embedding vectors for the model response $Z_{\hat{Y}}=\text{enc}_{t}(\hat{Y})$ and reference response $Z_{Y}=\text{enc}_{t}(Y)$ with text encoder model. Then, we train the MI estimator $\hat{I}_{\psi}(\cdot,\cdot)$ with parameter $\psi$ via gradient descent. To be specific, CLUB formulates the unbiased estimation for MI as below,
\begin{equation}
    \hat{I}_{\text{CLUB}}(P_{Z_{\mathbf{X}}Z_{Y}})=\frac{1}{N}\sum_{i=1}^{N}\log q_{\psi}(z_{y_{i}}|z_{x_{i}}) - \frac{1}{N^{2}}\sum_{i=1}^{N}\sum_{j=1}^{N}\log q_{\psi}(z_{y_j}|z_{x_{i}}),
\end{equation}
where $q_{\psi}(\cdot|\cdot)$ denotes variational approximation of ground truth probability density function $p(\cdot|\cdot)$. 

Following \cite{cheng2020club, cheng2021fairfil}, we parameterize the $q_{\psi}$ as a multi-variate Gaussian distribution and estimate the mean and variance parameters of Gaussian with separated two-layer MLPs with 250 hidden dimension size. During mini-batch training, those MLPs consume the concatenated input and response embeddings $\{[z_{\mathbf{x}_{i}},z_{y_{i}}]\}_{i=1}^{N}$ to produce a scalar estimate of MI, and they are simultaneously optimized by AdamW optimizer with learning rate 0.001 and batch size 1,024 for 5,000 iterations. 
However, if we have to train an estimator for every ID-OOD data pair, it may not be practical when the number of data pairs to be evaluated is large. Therefore, we constructed a dataset that integrates all ID-OOD data subsets for MI training (integration of all variants of LLaVA-Bench datasets reach roughly 5,000 samples for natural shift, and 9,000 samples for synthetic shift), trains it only once, and then infers all ID-OOD scenarios (27 for natural shift, 34 for synthetic shift) using this common MI estimator. This not only significantly reduces the time required to evaluate the modelâ€™s robustness against multiple distribution shift scenarios, but also stabilizes the training process by increasing the size of the data set used in the training process.

\paragraph{JS divergence estimation.} Estimation of distribution divergences from finite samples has been also a central topic of research \cite{yang1999information, sriperumbudur2012empirical, li2016renyi, bu2018estimation, sinn2018non, sreekumar2022neural, hoyos2023representation}. We adopt the most recent one, representation Jensen-Shannon divergence (RJSD; \citet{hoyos2023representation, hoyos2024a}), which proves its effectiveness on real benchmark datasets as our JSD estimator. The formula is as follows:
\begin{equation}
    \hat{D}_{\text{RJSD}}(P,Q)=S(\frac{C_{P}+C_{Q}}{2}) - \frac{1}{2}(S(C_{P})+S(C_{Q})),
\end{equation}
where $C_{P}=\mathbb{E}_{X\sim P}[\phi(X) \otimes \phi(X)]$ and $S(C_{P})= -\text{Trace}(C_{P}\log C_{P})$. Similar to MI, we compute $\hat{D}_{\text{RJSD}}(P,Q)$ in the embedding space of the same frozen pre-trained models, i.e., leverage neural network embedding space as a kernel $<\phi(x),\phi(x')>$. In contrast to the case of MI, RJSD with a frozen neural embedding model does not require additional training. Still, one might consider learning the embedding model from scratch if necessary.

\input{figures/appendix_synthetic_pilot}

\paragraph{MLLM judge and win rate.} For the open-ended generation tasks, (M)LLM-as-a-Judge has been adopted as a current de facto standard. Following \cite{liu2023visual,liu2024improved}, we use GPT-4\footnote{\texttt{gpt-4-turbo} with \texttt{2024-08-01-preview} API version was adopted} with text-only inference mode (with plain-text form visual cue such as ground truth caption for image) as a judge model and also use the output of the same model as a reference answer for each query. We leverage the prompts provided by the source code of LLaVA\footnote{https://github.com/haotian-liu/LLaVA}, and compute the win rate of a model of interest by comparing its output with that of GPT-4.

\section{Extended Empirical Validation and Discussion}\label{appendix:experiment}
\subsection{Additional Result from Pilot Study}
In Section \ref{sec:motivation}, we conduct an experiment to validate our hypotheses on the relation between MLLM performance degradation and the severity of natural distribution shift. In Figure \ref{fig:pilot_study_synthetic}, we provide additional results from another type of distribution shift that occurred by image and text perturbations. For image perturbation, we consider defocus blur and frost with three different magnitudes, and for text perturbation, we consider keyboard typo error and word synonym replacement with two different magnitudes. We observe the consistent trend in the relation between MLLM performance degradation and the severity of distribution shifts for the case of visual-only, text-only, and joint shift, likewise the case of natural shifts in Figure \ref{fig:pilot_study}. That is, the increased magnitude of distribution shifts induces more severe MLLM performance degradation, and the degree of performance degradation can attribute to shifts in two modalities.

\subsection{Different Design Choices of MI and JSD Estimation} Note that the results of all theorem (Lemma \ref{Main-thm1-lemma}, Theorem \ref{Main-thm1-thm}, Theorem \ref{thm:emid_bound_simple}, and Theorem \ref{thm:emid_bound}) are not limited to a specific class of MI and JSD estimators. To investigate whether our empirical verification of theorems robustly holds in an estimator-agnostic manner (if the estimator is valid), we provide an ablation study for the  MI estimator, JSD estimator, and embedding space that the estimators are built on. 

Specifically, we consider four MI estimators $\{$NWJ \cite{nguyen2010estimating}, MINE \cite{mine2018}, InfoNCE \cite{oord2018representation}, CLUB \cite{cheng2020club}$\}$, three embedding spaces $\{$individual models (CLIP ViT and XLM-RoBERTa), E5-V joint \cite{jiang2024e5}, E5-V disjoint \cite{jiang2024e5}$\}$, and two JSD estimators $\{$MMD \cite{liu2020learning}, RJSD\cite{hoyos2023representation}$\}$. E5-V \cite{jiang2024e5} is a recently proposed embedding extraction method that leverages an MLLM with a carefully designed prompt. We used the default prompt ``\texttt{Summary above sentence/image in one word: }" to separately extract embeddings (E5-V disjoint) for images and sentences, and design an ensemble of four custom prompts, 
\begin{enumerate}
    \item ``\texttt{Summary of the image <image>, and sentence <sent> in one word: }"
    \item ``\texttt{Summary of the visual content "<image>" with an associated text query "<sent>" in one word: }"
    \item ``\texttt{Given <image>, Summary of the sentence "<sent>" in one word: }"
    \item ``\texttt{Given visual content "<image>", Summary of the text query "<sent>" in one word: }",
\end{enumerate}
to extract multimodal joint query embedding (E5-V joint) by averaging four embedding vectors per (image, sentence) pair.
\input{tables/appendix_ablation1}
\input{tables/appendix_ablation2}
In Table \ref{tab:ablation_emiwr}, we conduct Spearman correlation analysis over the 12 (4 $\times$ 3) cases of MI estimator and embedding space ablation. We can clearly see that EMI is consistently correlated with the win rate which demonstrates the robust effectiveness of our theorem in practice. Meanwhile, among the candidate MI estimators and embedding space, CLUB and two E5-V joint embeddings show outstanding results. However, E5-V embedding extraction require a forward pass of MLLM in contrast to the case of leveraging relatively small individual models (ViT base and BERT-base). To strike the balance between effectiveness and efficiency, we adopt CLIP ViT-B/32 and XLM-RoBERTa-Base embedding spaces by default.


Next, we present the Pearson correlation analysis result in Table \ref{tab:ablation_emidup} by ablating the JSD estimator with the MI estimator and embedding space choices. Although there are some variations in the exact values, we also observe consistently significant correlations between EMID and its upper bound (that of Theorem \ref{eq:emid_bound_simple}). Therefore, the upper bound of EMID we derived robustly holds in practice across diverse estimator configurations.

\subsection{Hyperparameter Sensitivity}
We provide the hyperparameter configuration for MI estimator training (Table \ref{tab:mi_hyperparameter}) and further provide sensitivity analysis for varying hyperparameters (Figure \ref{fig:param_ablation}). We see that the CLUB estimator is quite robust to varying hyperparameters, i.e., batch size, learning rate, and hidden dimension, which implies the effectiveness of EMI estimation without intensive hyperparameter tuning.
\input{tables/appendix_hyp}
\input{figures/appendix_param_ablation}

\subsection{Runtime Analysis} \label{appendix:experiment:practical}
\input{tables/appendix_runtime}
In addition to the advantage of allowing rigorous theoretical statements, EMI also has practical advantages over the win rate derived by the LLM judge. Specifically, while both win rate and EMI are model-dependent, the former relies on models with tens to hundreds of billions of parameters, while the latter enables meaningful evaluation even with relatively small embedding models with millions of parameters. To quantitatively argue this, we compare the time per instance and the total inference time for the entire LLaVA-Bench COCO dataset in Table \ref{tab:runtime}. As shown in the table, EMI can shorten the time by 138 times compared to the win rate. Even including the time required for MI training, EMI-based evaluation is still 3 times faster than MLLM judgment-based evaluation. Since MI training is performed only once and then transferred to all ID-OOD scenarios, the efficiency of EMI-based evaluation over MLLM judgment becomes more evident as the number of datasets to be evaluated increases.
In addition, in the LLM judge paradigm, although open-source LLM judges \cite{kim2023prometheus} have been actively studied recently, proprietary LLMs are still dominant in practice, so one must pay per instance query, whereas EMI can make meaningful inferences with publicly available open-source pre-trained models without paying per query.