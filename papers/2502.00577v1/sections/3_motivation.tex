\section{Motivation} \label{sec:motivation}

\paragraph{A systematic understanding of MLLM under distributional shifts.} While instruction-following MLLMs are designed to handle a diverse range of tasks, they often struggle with specialized domains \cite{zhang2024out, zhou2024adapting}, perform poorly on simple image classification tasks \cite{zhai2024investigating, zhang2024vlmclf}, and exhibit hallucinations \cite{li2023evaluating, ye2025beaf}. We argue that {the fundamental cause of these failure modes in MLLMs can be traced back to distribution shifts}. Specifically, the poor performance on classification tasks and specialized distributions can be attributed to shifts between instruction tuning distribution $P_{\mathbf{X}Y}$ and evaluation distribution $Q_{\mathbf{X}Y}$.
Throughout this work, we comprehensively analyze three types of distributional shifts that can arise in MLLM:
\input{figures/s2_motivating_example}
\input{figures/s2_shift_desc}
\begin{enumerate}
    \item \textit{\textbf{Visual shift}}: the marginal distribution of visual query undergoes shift $D(P_{X_{v}}\|Q_{X_{v}}) \gg 0$, while that of text query remains largely unchanged $D(P_{X_{t}}\|Q_{X_{t}}) \approx 0$.
    \item \textit{\textbf{Text shift}}: the marginal distribution of text query undergoes shift $D(P_{X_{t}}\|Q_{X_{t}}) \gg 0$, while that of visual query remains largely unchanged $D(P_{X_{v}}\|Q_{X_{v}})\approx 0$.
    \item \textit{\textbf{Joint shift}}: both visual and text queries suffer shifts simultaneously, and the relationship between visual and text queries may also shift $D(P_{\mathbf{X}}\|Q_{\mathbf{X}}) \gg 0$,
\end{enumerate}
where $D$ denotes a divergence that measures the discrepancy between distributions $P$ and $Q$. For $M=(P+Q)/2$, one can measure the Kullback-Leibler (KL) divergence and Jensen-Shannon (JS) divergence as below:
\begin{equation*}
    \begin{split}
       & D_{\rm KL}(P\|Q)= \mathbb{E}_{\mathbf{z}\sim P} [\log {P(\mathbf{z})}/{Q(\mathbf{z})}],\\ 
       &D_{\rm JS}(P\|Q)= [{D_{\rm KL}(P\|M)+D_{\rm KL}(Q\|M)}]/2.
    \end{split}
\end{equation*}


\paragraph{Pilot study.} 
We hypothesize that: (1) performance degradation in MLLMs becomes more severe as $Q_{\mathbf{X}Y}$ deviates further from the $P_{\mathbf{X}Y}$; (2) the amount of total performance degradation can be factored into visual query shift and text query shift. To test these hypotheses, we design three types of shifts---visual shift, text shift, and joint shift---illustrated in Figure \ref{fig:shifts}, and evaluate MLLMs under these shifts. 

Specifically, we adopt LLaVA-1.5 \cite{liu2023visual} and LLaVA-NeXT \cite{liu2024improved} in 7B and 13B sizes as our target MLLM, with LLaVA-Bench COCO \cite{liu2023visual} serving as the ID dataset which is distributionally similar to the instruction tuning data. 
We adopt LLaVA-Bench Wild \citet{liu2023visual} to vary the visual input, whereas applied language translation with GPT-4, e.g., from English to $\{$Chinese, German, Chinese, Korean, Hindi, Arabic, and Greek$\}$, to realize a shift in text query. We vary the severity of shifts by controlling the magnitude of perturbations in synthetic shift setup and partitioning a dataset based on mean embedding distance from ID samples in natural shift setup. Following~\cite{liu2023visual}, we evaluate the performance using the win rate (Eq. \eqref{eq:win_rate}) with GPT-4 as a judge.

Figure \ref{fig:pilot_study} shows the performance variations of MLLMs under different types and magnitudes of distribution shifts, where the $x$-axis is sorted by the severity of shifts (more results from different types of shifts can be founded in Appendix \ref{appendix:experiment}).
Across all models, a consistent trend emerges: as the severity of the shift increases, the performance degradation becomes more significant. This trend robustly holds for both visual and text shifts. Joint shifts result in greater performance degradation, suggesting a complementary effect of shifts across modalities.  {\textit{These consistent observations suggest that there might exist an underlying principle explaining the relationship between performance variation and distributional discrepancy, which motivates us to investigate the theoretical model behind these empirical results.}}

\vspace{-0.2cm}
\paragraph{Our position.} Although there have been similar observations of performance degradation of MLLM under distribution shifts \cite{achiam2023gpt, zhang2024out, zhou2024adapting, zhang2024vlmclf}, all of them present only the coarse empirical evaluation results without finer analysis on the underlying factor of those performance degradations. To the best of our knowledge, there is no formal framework to explain the performance variations of MLLMs in terms of distribution shifts--- despite its crucial importance for ensuring reliable applications of MLLMs. To bridge this gap, we propose the \textbf{\textit{{first theoretical framework that characterizes MLLM performance variations under distribution shifts from an information-theoretic perspective}}}.