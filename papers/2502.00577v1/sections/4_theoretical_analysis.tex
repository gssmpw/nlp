\section{Information-theoretic Analysis on MLLM Performance Gap} \label{sec:theoretical_analysis}
In this section, we start with introducing the mutual information (MI) and its limitation as a metric in Sec. \ref{sec:3_0_mi_for_eval}, and present a new metric on MLLM evaluation (Sec. \ref{sec:3_2_emi}). Then, we derive theorems based on it to characterize the MLLM performance gap under distribution shifts (Sec. \ref{sec:3_3_emid}).

\subsection{Mutual Information for MLLM } \label{sec:3_0_mi_for_eval}


A fundamental capability of MLLMs is their instruction-following property \cite{ouyang2022training}---a direct outcome of instruction-tuning, where the model is trained to generate responses that are aligned with the intent of a given input query or instruction. To evaluate instruction-following, we first consider the \textit{mutual information}~\cite{shannon1948mathematical} to measure the shared information between the query and the corresponding model response. 
%
\begin{definition}[\textbf{Mutual Information (MI)}] For a joint distribution $P_{\mathbf{X}Y}$ over $\mathcal{X}\times \mathcal{Y}$, the mutual information with respect to $P_{\mathbf{X}Y}$ is defined as,
\begin{equation} \label{eq:mi}
    I(P_{\mathbf{X}Y}) := \mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log \frac{P_{\mathbf{X}Y}(\mathbf{x},y)}{P_{\mathbf{X}}(\mathbf{x})P_Y(y)}].
\end{equation}\label{def:mi}
%\vspace{-0.75em}
\end{definition}
\vspace{-0.5em}
MI is deeply related to the entropy, which is defined as $H(P_{\mathbf{X}}) :=  - \mathbb{E}_{\mathbf{x}\sim P_\mathbf{X}} [\log P_{\mathbf{X}}(\mathbf{x})]$.
It is easy to check that $I(P_{\mathbf{X}Y})=H(P_{Y} )-\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [H(P_{Y|\mathbf{X}=\mathbf{x}})]$. Intuitively, MI captures how much the response tells us about the query. One reason for considering MI is that the autoregressive objective in Eq. \eqref{eq::1} effectively estimates MI (see Eq. \eqref{eq::2}).

\paragraph{Visual instruction tuning effectively maximizes a lower bound of MI.} 
In Eq.~\eqref{eq::2}, we show that the autoregressive objective for instruction tuning (Eq. \eqref{eq::1}) effectively estimates the lower bound of MI when the model's representation capacity is sufficiently high, i.e., when $\delta$ becomes small. Therefore, instruction tuning with autoregressive objective effectively maximize a lower bound of MI between the input query and desired response,
\begin{align}\label{eq::2}
    I(P_{\mathbf{X}Y}) = & \mathbb{E}_{\mathbf{x},y \sim P_{\mathbf{X}Y}} [\log P_{{\theta}}(y|\mathbf{x})]+H(P_Y) + \delta  \nonumber \\
    \geq & \mathbb{E}_{\mathbf{x},y \sim P_{\mathbf{X}Y}} [\log P_{{\theta}}(y|\mathbf{x})],
\end{align}
where 
$
\delta= \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}\| P_{\theta}(\cdot|\mathbf{x}))] 
$. 

\paragraph{Going from instruction tuning to test-time MI.} While $I(P_{\mathbf{X}Y})$ measures the MI between the input query and ground truth response from $P_{\mathbf{X}Y}$, one can measure MI between the input query and model response on the evaluation-time distribution. In particular, we use a tensor product $P_{\mathbf{X}} \otimes P_{\theta}$ to present this joint distribution between the input distribution $P_\mathbf{X}$ and \emph{generated output} distribution $P_{\theta}(y|\mathbf{x})$:
\begin{equation}
    P_{\mathbf{X}} \otimes P_{\theta} :=P_{\mathbf{X}}(\mathbf{x}) P_{\theta}(y|\mathbf{x}),~\forall (\mathbf{x},y)\in \mathcal{X}\times \mathcal{Y}.%~\text{for~any}~$(\mathbf{x},y)\in \mathcal{X}\times \mathcal{Y}$.
\end{equation}
Accordingly, the mutual information w.r.t. the joint distribution $P_{\mathbf{X}} \otimes P_{\theta}$ can be written as:
\begin{equation}
\begin{split}
   I(P_{\mathbf X}\otimes P_{\theta})& = H(\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[P_{\theta}(\cdot|\mathbf{x})])\\ &-\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [H(P_{\theta}(\cdot|\mathbf{x}))].
    \end{split}
    \label{eq:mi_generation}
\end{equation}
\paragraph{Limitation of test-time MI under distribution shifts.}
Although one could directly use $I(P_{\mathbf{X}} \otimes P_{\theta})$, the mutual information between the input query and the model response, as a metric, the vanilla MI may not be suitable for scenarios involving distribution shifts. For example, 
consider the distribution $P_{\mathbf{X}}$ (e.g., general domain), and the distribution $Q_{\mathbf{X}}$ (e.g., medical domain). Suppose the MI w.r.t. model $P_{\theta}$ on $P_{\mathbf{X}}$, i.e., $I(P_{\mathbf{X}}\otimes P_{{\theta}})$ is 2.0, while on the $Q_{\mathbf{X}}$, it is $I(Q_{\mathbf{X}}\otimes P_{{\theta}})=1.0$. Does this imply that model $P_\theta$ performs twice as poorly on $Q_{\mathbf{X}}$? The answer is unclear.

The challenge lies in the inherent variability of MI scales across data domains. Recall the formulation of
 $I(P_{\mathbf{X}}\otimes P_{\theta})$ in Eq.~\eqref{eq:mi_generation}, the first term $H(\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[P_{\theta}(\cdot|\mathbf{x})] )$ represents the upper bound of MI and varies with the data domain; and the second term $\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [H(P_{\theta}(\cdot|\mathbf{x}))]$ reflects the input-output dependency and depends on the true data-generating process.
For instance, given a fixed vocabulary, the responses from MLLM could contain more diverse words in a general domain whereas a narrower subset of words could be expected in the specialized domains such as medical. Then, $H(\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[P_{\theta}(\cdot|\mathbf{x})] )$ and $\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}) )]$ would be larger in a general domain. Therefore, we argue that {\textit{a desired evaluation metric should disentangle the pure input-response relevance from the intrinsic characteristics of dataset}}.

\subsection{Effective Mutual Information for Reliable MLLM Evaluation} \label{sec:3_2_emi}

To remove the influence of the domain-dependent scale, we propose \textit{effective mutual information} (EMI) as a remedy.
\begin{definition}[\textbf{Effective Mutual Information (EMI)}]
    Given the joint distribution $P_{\mathbf{X}Y}$ and MLLM $P_{\theta}$ parameterized with $\theta$, the effective mutual information between the input and model response is defined as below,
\begin{equation} \label{eq:emi}
    \text{EMI}(P_{\mathbf{X}Y};\theta):= I(P_{\mathbf{X}}\otimes P_{\theta}) - I(P_{\mathbf{X}Y}).
\end{equation}
\end{definition}
Compared to the standard MI, $\text{EMI}(P_{\mathbf{X}Y}; \theta)$ measures the ``effective" relevance between the query \(\mathbf{x}\) and the model response \(\hat{y}\) by subtracting a ground truth MI \(I(P_{\mathbf{X}Y})\) from $I(P_{\mathbf{X}}\otimes P_{\theta})$. Refer to Figure \ref{fig:emi_motivation} in Appendix~\ref{appendix:emi}, for an intuitive example: by accounting for a baseline level of MI, EMI quantifies the extent to which the model captures the effective relevance between the input and output. The use of EMI as an evaluation metric for MLLMs can be further supported by (1) its analogy to the excess risk and effective robustness;
 and (2) its connection to win rate. 

\paragraph{Analogy to the excess risk and effective robustness.} The minimum achievable error varies depending on the data-generating process. To enable reliable model selection that is agnostic to data distributions, excess risk—defined as the difference between a model's risk and the minimum possible risk—has been extensively studied \cite{castro2008minimax, koltchinskii2010rademacher, mohri2018foundations}. More recently, \citet{taori2020measuring} introduced the concept of effective robustness to quantify the ``effective" OOD generalization accuracy of classification models by subtracting their ID accuracy. The motivation behind EMI aligns with these concepts, i.e., mitigating the influence of external confounding effects that hinder the accurate measure of model performance. EMI ensures that the metric focuses on the model's effective ability to capture input-output relevance, independent of confounding effects from the data domain. 


\paragraph{Connection to win rate.} We also show that EMI is closely related to win rate (i.e., Eq. \eqref{eq:win_rate}), a common metric used for evaluating MLLM generations. Conceptually, EMI quantifies the effective relevance between the input query and the model’s response by accounting for the baseline information, while win rate measures the preference of the model’s responses over a reference response. More formally, their connection can be mathematically established through the lens of a logit Bradley-Terry preference model (PM) formulation \cite{bradley1952rank, hunter2004mm}, $\text{logit} P(\hat{y} \succ y|\mathbf{x})$, a smooth and differentiable proxy for the discrete win rate function. To be specific, we commonly use $\log P(\hat{y} \succ y|\mathbf{x})$ to train a reward model (RM) which is adopted to compute the win rate (Eq. \eqref{eq:win_rate}). We compare both terms as below.
\begin{equation} \label{eq:preference_model}
\small
\begin{split}
 {\text{PM}}(P_{\mathbf{X}Y};\theta):&=\mathbb{E}_{\begin{subarray}{l} \mathbf{x},y \sim P_{\mathbf{X}Y} \\ \hat{y} \sim P_{\theta}(\cdot|\mathbf{x}) \end{subarray}}[\text{logit} \; P(\hat{y} \succ y|\mathbf{x})] \nonumber\\
 &=\mathbb{E}_{\begin{subarray}{l} \mathbf{x},y \sim P_{\mathbf{X}Y} \\ \hat{y} \sim P_{\theta}(\cdot|\mathbf{x}) \end{subarray}}[r(\mathbf{x},\hat{y})-r(\mathbf{x},y)],
 \\
     {\text{RM}}(P_{\mathbf{X}Y};\theta):&=
     \mathbb{E}_{\begin{subarray}{l} \mathbf{x},y \sim P_{\mathbf{X}Y} \\ \hat{y} \sim P_{\theta}(\cdot|\mathbf{x}) \end{subarray}} [\log\; P(\hat{y} \succ y|\mathbf{x})] \nonumber\\
   &= \mathbb{E}_{\begin{subarray}{l} \mathbf{x},y \sim P_{\mathbf{X}Y} \\ \hat{y} \sim P_{\theta}(\cdot|\mathbf{x}) \end{subarray}} [\log \sigma(r(\mathbf{x},\hat{y} ) - r(\mathbf{x},y))],
 \end{split}
\end{equation}
where $r(\cdot, 
\cdot)$ is the latent score function so-called reward model that generates preference for $(\mathbf{x},y)$. It is clear that
\begin{equation*}
     {\text{PM}}(P_{\mathbf{X}Y};\theta) = {\text{RM}}(P_{\mathbf{X}Y};\theta) - \log (1-e^{ {\text{RM}}(P_{\mathbf{X}Y};\theta)}),
\end{equation*}
\begin{equation*}
     {\text{RM}}(P_{\mathbf{X}Y};\theta) = {\text{PM}}(P_{\mathbf{X}Y};\theta) - \log (1+e^{ {\text{PM}}(P_{\mathbf{X}Y};\theta)}).
\end{equation*}
Therefore, ${\text{PM}}(P_{\mathbf{X}Y};\theta)$ and ${\text{RM}}(P_{\mathbf{X}Y};\theta)$ exhibit a mutual equivalence, i.e., \textit{increase in ${\text{PM}}(P_{\mathbf{X}Y};\theta)$ corresponds to the increase in ${\text{RM}}(P_{\mathbf{X}Y};\theta)$, and vice versa}. In Lemma \ref{Main-thm1-lemma}, we establish an upper bound for the absolute difference between EMI and ${\text{PM}}(P_{\mathbf{X}Y};\theta)$, thereby demonstrating their closeness while ultimately highlighting the connection between EMI and win rate.
\begin{lemma}\label{Main-thm1-lemma}
    Given a distribution $P_{\mathbf{X}Y}$ and an MLLM $P_{\theta}$, if
        $\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x})\| P_{Y|\mathbf{X}=\mathbf{x}})] \leq \delta$, and let the reward function $r(\mathbf{x},y)$ be $\log P_{Y|\mathbf{X}=\mathbf{x}}(y)$,
    then
    \begin{equation*}
        |\text{EMI}(P_{\mathbf{X}Y};\theta)-\text{PM}(P_{\mathbf{X}Y};\theta)| \leq \delta + 4.4\delta^{\frac{1}{8}}.
    \end{equation*}
\end{lemma}
\vspace{-0.3em}
Intuitively, Lemma \ref{Main-thm1-lemma} shows that if MLLM $P_{\theta}$ can approximate the given distribution $P_{\mathbf{X}Y}$ with approximate error $\delta$, the difference between EMI and $\text{PM}$ can be bounded by a small term w.r.t. the approximate error $\delta$. Furthermore, assuming that the model class $\{P_{\theta}:\forall \theta\in \Theta\}$ has sufficient expressive power (i.e., Eq. \eqref{Expression}), we can derive an additional bound for the case of the optimal solution of autoregressive objective (i.e., Eq. \eqref{eq::1}), as shown below. 
\begin{theorem}\label{Main-thm1-thm}
    Given a distribution $P_{\mathbf{X}Y}$ with $P_{\mathbf{X}Y}>c>0$ for some constant or $P_{Y|\mathbf{X}}>c$, if  the $\epsilon$-representation capacity assumption holds, i.e.,
\begin{equation}\label{Expression}
    \min_{\theta\in \Theta} \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}\| P_{\theta}(\cdot|\mathbf{x}) )] \leq \epsilon,
\end{equation}
and let the reward function $r(\mathbf{x},y)$ be $\log P_{Y|\mathbf{X}=\mathbf{x}}(y)$, then
\begin{equation*}
    \begin{split}
        &|\text{EMI}(P_{\mathbf{X}Y};\theta^*) - \text{PM}(P_{\mathbf{X}Y};\theta^*)| \leq \delta + 4.4\delta^{\frac{1}{8}}, \nonumber
    \end{split}
    \end{equation*}
    \vspace{-0.2em}
    where $\theta^*$ is the optimal solution of Eq. \eqref{eq::1} over $P_{\mathbf{X}Y}$, and $\delta = 4.4\epsilon^{\frac{1}{8}}-\log c \sqrt{2\epsilon}$.
\end{theorem}
Theorem \ref{Main-thm1-thm} shows that with a sufficiently expressive model class, EMI exhibits a stronger alignment with PM when the optimal MLLM parameter $\theta^*$ is obtained through the autoregressive objective. This alignment underscores the validity of using EMI as a reliable metric for evaluating MLLM and quantifying the relative preference of responses.

%\textcolor{teal}{
Although we confine our analysis to $I(P_{\mathbf{X}Y})$ and $\text{EMI}(P_{\mathbf{X}Y};\theta)$, the chain rule, $I(P_{\mathbf{X}Y})=I(P_{X_{v}Y})+I(P_{X_{t}Y|X_v})$ allows EMI to further factorize the query-response relevance into two input modalities, which is suitable for multimodal LLM's fine-grained evaluation. 

\subsection{Characterizing MLLM Performance Gap via Effective Mutual Information Difference} \label{sec:3_3_emid}

Now, based on EMI, we are ready to establish formal guarantees on the performance gap of MLLM via \textit{effective mutual information difference} (EMID). EMID is defined as the difference between the EMI on  the ID distribution $P_{\mathbf{X}Y}$ and the OOD distribution $Q_{\mathbf{X}Y}$, as follows:
\begin{equation} \label{eq:emid}
{\small
\begin{split}
    % &\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)\\ :=&\text{EMI}(P_{\mathbf{X}Y};\theta) - \text{EMI}(Q_{\mathbf{X}Y};\theta).
    \text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta) :=\text{EMI}(P_{\mathbf{X}Y};\theta) - \text{EMI}(Q_{\mathbf{X}Y};\theta).
    \end{split}
}
\end{equation}
To elucidate the key insight and provide a clear foundation, we begin by analyzing a simple scenario where the conditional variables remain consistent across both ID and OOD distributions.
In this case, we can derive an upper bound for EMID,  as stated in Theorem \ref{thm:emid_bound_simple}. This bound enables us to quantify the maximum performance gap of MLLM over two distributions by measuring the severity of the marginal distribution shift over visual and language modalities. 

\vspace{0.2cm}
\begin{theorem}[Simplified Scenario]\label{thm:emid_bound_simple}
 Given an MLLM $P_{\theta}$ and distributions $P_{\mathbf{X}Y}$, $Q_{\mathbf{X}Y}$ which have consistent conditional distributions over variables $X_{v}|X_{t}$, $X_{t}|X_{v}$, and $Y|\mathbf{X}$,
if there exist some constants $\delta_{P}$ and $\delta_{Q}$ such that
\begin{equation*}
    D_{\rm JS}(P_{Y_{{\theta}}}\|P_{Y})\leq \delta_{P},~~~ D_{\rm JS}(Q_{Y_{{\theta}}}\|Q_{Y})\leq \delta_{Q},
\end{equation*}
 and denote $P_{Y_{\theta}}=\mathbb{E}_{P_{\mathbf{X}}} [P_{{\theta}}(\cdot|\mathbf{x})]$ and $Q_{Y_{\theta}}=\mathbb{E}_{Q_{\mathbf{X}}} [P_{{\theta}}(\cdot|\mathbf{x})]$, then $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$ is upper bounded by
% (H(P_{Y^T})+H(P_{Y^S_\theta}))
\begin{equation} \label{eq:emid_bound_simple}
    \begin{split}
     \widehat{H}\big( { D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}\|Q_{X_{v}})} + {D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}\|Q_{X_{t}})} \big) + 8\Delta^{\frac{1}{4}},
    \end{split}
\end{equation}
\normalsize
where $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$ and $\Delta=\delta_{P}+\delta_{Q}$. 
\end{theorem}


\input{figures/s5_emid_full_bound}

\begin{tcolorbox}[width=\linewidth, colback=white!95!black]
\noindent \textbf{Implication.} 
Theorem \ref{thm:emid_bound_simple} implies that in the simplified scenario, EMID depends on two main factors: (1) the divergence between the marginal distributions of the visual and textual inputs;
(2) the divergence between the model’s predictions and the true output distributions, encapsulated by $\delta_P$ and $\delta_Q$.
\end{tcolorbox}
Theorem \ref{thm:emid_bound_simple} naturally captures special cases such as visual-only or text-only input shifts. For a visual-only input shift, where $D_{\rm JS}(P_{X_{t}} \| Q_{X_{t}}) = 0$, the EMID upper bound primarily depends on the divergence between the visual input distributions. Similarly, for a text-only input shift, the bound reflects the divergence in the textual input distributions. The two cases not only underscore the flexibility of Theorem \ref{thm:emid_bound_simple} in isolating and quantifying the impact of modality-specific distribution shifts on model performance but also highlight the importance of visual and text input shifts on it. In Appendix \ref{appendix:thm}, we further provide a looser yet better interpretable version of this upper bound (Corollary \ref{thm:emid_bound_simple_v2}) by replacing the $\Delta$ into the discrepancy terms between model output and ground truth conditional distributions.


\paragraph{General scenario.} Moving beyond the simplified scenario, we now consider the general scenario where no assumptions are made about the consistency of conditional distributions across ID and OOD settings. This more realistic scenario accommodates shifts not only in the marginal distributions of visual and textual inputs but also in their conditional dependencies and the relationships between inputs and outputs. By relaxing these constraints, we aim to capture the full complexity of distributional shifts encountered in practice and analyze how such shifts collectively influence the performance gap of MLLMs. The formal upper bound is provided in Theorem \ref{thm:emid_bound}.


\vspace{0.2cm}
\begin{theorem}[General Scenario]\label{thm:emid_bound}
Given $P_{\mathbf{X}Y}$ and $Q_{\mathbf{X}Y}$ distributions and an MLLM $P_{{\theta}}$, if there exist some constants $\delta_{P}$ and $\delta_{Q}$ such that
\begin{equation*}
    D_{\rm JS}(P_{Y_{{\theta}}}\|P_{Y})\leq \delta_{P},~~~ D_{\rm JS}(Q_{Y_{{\theta}}}\|Q_{Y})\leq \delta_{Q},
\end{equation*}
 and denote $P_{Y_{\theta}}=\mathbb{E}_{P_{\mathbf{X}}} [P_{{\theta}}(\cdot|\mathbf{x})]$ and $Q_{Y_{\theta}}=\mathbb{E}_{Q_{\mathbf{X}}} [P_{{\theta}}(\cdot|\mathbf{x})]$, then $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$ is upper bounded by
% (H(P_{Y^T})+H(P_{Y^S_\theta}))
\begin{equation} \label{eq:emid_bound}
    \begin{split}
    & \widehat{H}\big({ D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}||Q_{X_{v}}) + D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}||Q_{X_{t}})} \big) \nonumber\\
    +&\widehat{H}\big({\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}\|Q_{X_{t}|X_{v}})+\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}\|Q_{X_{v}|X_{t}})} \big)\nonumber\\
    +&4 \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}} } D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}\|Q_{Y|\mathbf{X}=\mathbf{x}}) + 8\Delta^{\frac{1}{4}},
    \end{split}
\end{equation}
\normalsize
where $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$, $\Delta=\delta_{P}+\delta_{Q}$, and \begin{equation*}
\begin{split}
    \bar{D}_{\rm JS}(P_{X|X'}||Q_{X|X'}):=&\mathbb{E}_{\mathbf{x}\sim P_{X'}}[D_{\rm JS}(P_{X|{X'}=\mathbf{x}}\|Q_{X|{X'}=\mathbf{x}})]\\+&\mathbb{E}_{\mathbf{x}\sim Q _{X'}}[D_{\rm JS}(P_{X|{X'}=\mathbf{x}}\|Q_{X|{X'}=\mathbf{x}})].
    \end{split}
\end{equation*}
\end{theorem}

\begin{tcolorbox}[width=\linewidth, colback=white!95!black]
\noindent \textbf{Implication.} 
Compared to Theorem \ref{thm:emid_bound_simple}, Theorem \ref{thm:emid_bound} indicates that, in the general case, EMID is also influenced by divergences in conditional distributions. Specifically, EMID is upper bounded by marginal distribution shifts in visual and textual inputs ($X_v$ and $X_t$); divergence between marginal output and model response distributions; shifts in conditional dependencies ($X_v | X_t$ and $X_t | X_v$); and a shift between conditional output distributions ($Y|\mathbf{X}$).
\end{tcolorbox}


Although Theorem \ref{thm:emid_bound} holds for broader cases, Theorem \ref{thm:emid_bound_simple} is much simpler to analyze. Thus, we focus on the validation of Theorem \ref{thm:emid_bound_simple} in the following section. If we have some knowledge of the data-generating process of $P_{\mathbf{X}Y}$ and $Q_{\mathbf{X}Y}$, we can choose the one that is suitable for given distributions. In summary, both Theorem \ref{thm:emid_bound_simple} and \ref{thm:emid_bound} {\textit{provide an analytic tool to characterize the performance gap of MLLM, representing the first formal framework for evaluating MLLM under distribution shifts.}}