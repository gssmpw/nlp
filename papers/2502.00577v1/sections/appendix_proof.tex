\newpage
\section{Proof and Additional Theorem}\label{appendix:thm}


In this section, we provide proof of all theorems (Lemma \ref{Main-thm1-lemma}, Theorem \ref{Main-thm1-thm}, Theorem \ref{thm:emid_bound_simple}, and Theorem \ref{thm:emid_bound}) in our manuscript, and introduce an additional theoretical result (Corollary \ref{thm:emid_bound_simple_v2}). 

\subsection{Proof for the relationship between EMI and preference model}
First, we provide proof of the closeness between the effective mutual information (EMI) and the preference model.

\begin{lemma}[Restatment of Lemma \ref{Main-thm1-lemma}]\label{Main-thm1-lemma-appendix}
    Given a distribution $P_{\mathbf{X}Y}$ and an MLLM $P_{\theta}$, let the reward model function $r(\mathbf{x},y)$ be $\log P_{Y|\mathbf{X}=\mathbf{x}}(y)$. If
        $\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x})\| P_{Y|\mathbf{X}=\mathbf{x}}) \leq \delta$, then,
    \begin{equation*}
        |\text{EMI}(P_{\mathbf{X}Y};\theta)-\text{PM}(P_{\mathbf{X}Y};\theta)| \leq \delta + 4.4\delta^{\frac{1}{8}}.
    \end{equation*}
\end{lemma}

\begin{proof} 
Let $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim \mathbf{X}}P_{\theta}(\cdot|\mathbf{x})$, and note the expression for EMI below,
    \begin{align*}
    \text{EMI}(P_{\mathbf{X}Y};\theta) &= I(P_{\mathbf{X}} \otimes P_{\theta}) - I(P_{\mathbf{X}Y}) \nonumber\\
    &= H(P_{Y_{\theta}}) - H(P_{\theta}(\cdot|\mathbf{X})) - H(P_Y) + H(P_{Y|\mathbf{X}}) \nonumber\\
    &= \big ( H(P_{Y_{\theta}}) - H(P_Y) \big ) \nonumber\\
    &+ \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[\mathbb{E}_{\hat{y}\sim P_{\theta}(\cdot|\mathbf{x})}\log P_{\theta}(\hat{y}|\mathbf{x})] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[\mathbb{E}_{y\sim P_{Y|\mathbf{X}=\mathbf{x}}}\log P_{Y|\mathbf{X}=\mathbf{x}}(y)] \nonumber
    \end{align*}
Next, given $r(\mathbf{x},y)=\log P_{Y|\mathbf{X}=\mathbf{x}}(y)$, logit Bradley-Terry preference model (PM) \cite{hunter2004mm} can be expressed as,
    \begin{align*}
    \text{PM}(P_{\mathbf{X}Y};\theta) = \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[\mathbb{E}_{\hat{y}\sim P_{\theta}(\cdot|\mathbf{x})}\log P_{Y|\mathbf{X}=\mathbf{x}}(\hat{y}) - \mathbb{E}_{y\sim P_{Y|\mathbf{X}=\mathbf{x}}}\log P_{Y|\mathbf{X}=\mathbf{x}}(y) ]
    \end{align*}
Therefore, 
\begin{align*}
    | \text{EMI}(P_{\mathbf{X}Y};\theta) - \text{PM}(P_{\mathbf{X}Y};\theta) | &=  |H(P_{Y_{\theta}}) - H(P_Y) + \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[\mathbb{E}_{\hat{y}\sim P_{\theta}(\cdot|\mathbf{x})}\log \frac{P_{\theta}(\hat{y}|\mathbf{x})}{P_{Y|\mathbf{X}=\mathbf{x}}(\hat{y})}]| \\
    & \leq |H(P_{Y_{\theta}}) - H(P_Y)| + \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x})||P_{Y|\mathbf{X}=\mathbf{x}})] \\
    & \leq 4.4\delta^{\frac{1}{8}} + \delta.
\end{align*}
\end{proof}
Here, we adopted Lemma \ref{thm:entropy_diff_bound} to replace $|H(P_{Y_{\theta}})-H(P_{Y})|$ into its upper bound $4 D^{\frac{1}{4}}_{\rm JS}(P_{Y_{\theta}}||P_Y)$ and used Pinsker's inequality \cite{pinsker1964information}.

We provide a proof for the extended theorem from the Lemma \ref{Main-thm1-thm-appendix} by considering the optimal model parameter as below.
\begin{theorem}[Restatment of Lemma \ref{Main-thm1-thm}]\label{Main-thm1-thm-appendix}
    Given a distribution $P_{\mathbf{X}Y}$ and an MLLM $P_{\theta}$, and assume $P_{\mathbf{X}Y}>c>0$ for a constant $c$, if the $\epsilon$-representation capacity assumption holds, i.e.,
\begin{equation}\label{Expression-appendix}
    \min_{\theta\in \Theta} \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}\| P_{\theta}(\cdot|\mathbf{x}) ) \leq \epsilon,
\end{equation}
and let the reward function $r(\mathbf{x},y)$ be $\log P_{Y|\mathbf{X}=\mathbf{x}}(y)$, then
\begin{equation*}
    \begin{split}
        &|\text{EMI}(P_{\mathbf{X}Y};\theta^*) - \text{PM}(P_{\mathbf{X}Y};\theta^*)| \leq \delta + 4.4 \delta^{\frac{1}{8}}, \nonumber
    \end{split}
    \end{equation*}
    \vspace{-0.2em}
    where $\theta^*$ is the optimal solution of Eq. \eqref{eq::1} over $P_{\mathbf{X}Y}$, and $\delta = 4.4\epsilon^{\frac{1}{8}} - \log c \sqrt{2\epsilon}$.
\end{theorem}
\begin{proof} 
Recall the formulation of mutual information as below,
\begin{align*}
    I(P_{\mathbf{X}Y})
    &=H(P_Y) - H(P_{Y|\mathbf{X}}) \\
    &=\mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{Y|X=x}(y)] + H(P_Y) \\
    &=\mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{\theta}(y|\mathbf{x})] - \mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{\theta}(y|\mathbf{x})] + \mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{Y|X=x}(y)] + H(P_Y) \\
    &=\mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{\theta}(y|\mathbf{x})] + \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}||P_{\theta}(\cdot|\mathbf{x}))] + H(P_Y) \\
\end{align*}
So, $I(P_{\mathbf{X}Y})-\mathbb{E}_{\mathbf{x},y\sim P_{\mathbf{X}Y}}[\log P_{\theta}(y|\mathbf{x})]-H(P_Y)=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{Y|\mathbf{X}=x}(y)||P_{\theta}(\cdot|\mathbf{x}))]$.
Therefore, for the optimally learned parameter $\theta^{*}$, we know that $\theta^{*}\in\arg \min_{\theta\in\Theta} \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}||P_{\theta}(\cdot|\mathbf{x}))]$, which implies below,
\begin{equation*}
    \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{Y|\mathbf{X}=x}||P_{\theta^{*}}(\cdot|\mathbf{x}))] \leq \epsilon.
\end{equation*}
Meanwhile, we have the below upper bound by leveraging Lemma \ref{thm:cond_entropy_diff_bound-3},
\begin{equation*}
    \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x}) || P_{Y|\mathbf{X}=\mathbf{x}})] \leq 4.4 \epsilon^{\frac{1}{8}}-\log c \sqrt{2\epsilon}
\end{equation*}
By denoting $\delta:=4.4 \epsilon^{\frac{1}{8}}-\log c \sqrt{2\epsilon}$ and plugging the Lemma \ref{Main-thm1-lemma-appendix}, we complete the proof.
\end{proof}

Note that the assumption $P_{\mathbf{X}Y}>c>0$ is reasonable in practice given the following two statements. First, we can only observe the samples that $P_{\mathbf{X}Y}(\mathbf{x},y)>0$. Therefore, investigating on the case $\mathbf{x},y$ such that $P_{\mathbf{X}Y}>0$ solely does not affect the practical implication of our analysis. Second, for the space $\mathcal{X}\times \mathcal{Y}$, it is obvious that $|\mathcal{X}\times \mathcal{Y}|<+\infty$. Therefore, $\mathcal{X}\times \mathcal{Y}$ is a compact space, and $P_{\mathbf{X}Y}(\mathbf{x},y)>0$ over a compact space, there exists a constant $c>0$ such that $P_{\mathbf{X}Y}(\mathbf{x},y)>c$.

\begin{lemma}\label{thm:tv_based_bound}
    Given two distributions $P_{X}$ and $Q_{X}$ defined over $\mathcal{X}$, let $f:\mathcal{X}\rightarrow [0,c]$, then we have the below,
\begin{equation*}
   \big | \mathbb{E}_{x\sim P_{X}}[f(x)] - \mathbb{E}_{x\sim Q_{X}}[f(x)] \big | \leq c \cdot D_{\rm TV}(P_{X},Q_{X}).
\end{equation*}
where $D_{\rm TV}(P_{X},Q_{X}):=\sum_{x\in X}|P_{X}(x) - Q_{X}(x)|$ is the total variation distance between two distributions.
\end{lemma}
\begin{proof}
\begin{align*}
&\big | \mathbb{E}_{x\sim P_{X}}[f(x)] - \mathbb{E}_{x\sim Q_{X}}[f(x)] \big | \\
=&\big | \sum_{x\in\mathcal{X}} P_{X}(x)f(x) \sum_{x\in\mathcal{X}} Q_{X}(x)f(x) \big | \\
=&\big | \sum_{x\in\mathcal{X}} (P_{X}(x) - Q_{X}(x))f(x) \big | \\
=&\big | \sum_{x\in\mathcal{X}} (P_{X}(x) - Q_{X}(x))(f(x) - c) + c(\sum_{x\in\mathcal{X}} P_{X}(x)-Q_{X}(x))  \big | \\
\leq & \sum_{x\in\mathcal{X}} | P_{X}(x) - Q_{X}(x)| \cdot \big | f(x) - c \big | \\
\leq & c \cdot ||P_{X}-Q_{X}||_{1} \\
=& c \cdot D_{\rm TV}(P_{X},Q_{X}) 
\end{align*}
\end{proof}

\begin{lemma}\label{thm:entropy_diff_bound}
    Given random variable $\mathbf{X}$, and two distributions $P_{\mathbf{X}Y}=P_{Y|\mathbf{X}}P_{\mathbf{X}}$ and $Q_{\mathbf{X}Y}=Q_{Y|\mathbf{X}}Q_{\mathbf{X}}$, we have the bounds for the difference between Entropy $H(\cdot)$ over two distributions as below:
\begin{equation*}
\begin{split}
   |H(P_{\mathbf{X}}) - H(Q_{\mathbf{X}})| &\leq 4 D^{\frac{1}{4}}_{\rm JS}(P_{\mathbf{X}}|| Q_{\mathbf{X}}), \\ 
   |H(P_{Y}) - H(Q_{Y})| &\leq 4 D^{\frac{1}{4}}_{\rm JS}(P_{Y}|| Q_{Y}), \\  
   | H(P_{Y|\mathbf{X}=\mathbf{x}}) - H(Q_{Y|\mathbf{X}=\mathbf{x}}) | & \leq 4 D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}|| Q_{Y|\mathbf{X}=\mathbf{x}}).
\end{split}
\end{equation*}
where $D_{\rm JS}(\cdot,\cdot)$ is the Jensen-Shannon divergence between two distributions.
\end{lemma}

\begin{proof}
Let $M_{\mathbf{X}}= (P_{{\mathbf{X}}}+Q_{{\mathbf{X}}})/2$, $D_{\rm JS}(P_{\mathbf{X}},Q_{\mathbf{X}})$ and $D_{\rm TV}(P_{\mathbf{X}},Q_{\mathbf{X}})$ be the Jensen-Shannon divergence and total variation distance between $P_{{\mathbf{X}}}$ and $Q_{\mathbf{X}}$, respectively.
\begin{align*}
%\begin{split}
& \big | H(P_{\mathbf{X}}) - H(Q_{\mathbf{X}}) \big | \\
=& \big | \mathbb{E}_{\mathbf{x}\sim P_{{\mathbf{X}}}} \log P_{\mathbf{X}}(\mathbf{x}) -  \mathbb{E}_{\mathbf{x}\sim Q_{{\mathbf{X}}}} \log Q_{{\mathbf{X}}}(\mathbf{x}) \big | \\
=& \big | \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log P_{\mathbf{X}}(\mathbf{x}) - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x}) - \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log Q_{\mathbf{X}}(\mathbf{x}) + \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x}) +\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x}) -\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x})  \big | \\ 
\leq & | \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log \frac{P_{\mathbf{X}}(\mathbf{x})}{M_{\mathbf{X}}(\mathbf{x})} - \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log \frac{Q_{\mathbf{X}}(\mathbf{x})}{M_{\mathbf{X}}(\mathbf{x})} | + | \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x}) - \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x})| \\
\leq & | \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log \frac{P_{\mathbf{X}}(\mathbf{x})}{M_{\mathbf{X}}(\mathbf{x})} + \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log \frac{Q_{\mathbf{X}}(\mathbf{x})}{M_{\mathbf{X}}(\mathbf{x})} | + | \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x}) - \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} \log M_{\mathbf{X}}(\mathbf{x})| \\
\leq & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + 2 \sum_{x} |\frac{P_{\mathbf{X}}(x)}{2} - \frac{Q_{\mathbf{X}}(x)}{2}| \cdot | \log M_{\mathbf{X}}(x)|  \\
= & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + 2 \sum_{x} |\frac{P_{\mathbf{X}}(x)}{2} - \frac{Q_{\mathbf{X}}(x)}{2}| \cdot \big| \log |\frac{P_{\mathbf{X}}(x)}{2} + \frac{Q_{\mathbf{X}}(x)}{2}|\big|  \\
\leq & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + 2 \sum_{x} |\frac{P_{\mathbf{X}}(x)}{2} - \frac{Q_{\mathbf{X}}(x)}{2}| \cdot \big| \log |\frac{P_{\mathbf{X}}(x)}{2} - \frac{Q_{\mathbf{X}}(x)}{2}|\big|  \\
\leq & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + 2 \sum_{x} \sqrt{ |\frac{P_{\mathbf{X}}(x)}{2} - \frac{Q_{\mathbf{X}}(x)}{2}| } \\
\leq & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + \sqrt{2\sum_{x} |P_{\mathbf{X}}(x) - Q_{\mathbf{X}}(x)| } \\
=& 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + \sqrt{2 D_{\rm TV}(P_{\mathbf{X}},Q_{\mathbf{X}}) } \\
\leq & 2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) + 2 D^{\frac{1}{4}}_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}}) \\
\leq & 4 D^{\frac{1}{4}}_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}})
\end{align*}
In above inequalities, we have used $\sqrt{x}+x\log x>0$ for $x\in (0,1)$, Holder's inequality, and $D_{\rm TV}(P_{\mathbf{X}},Q_{\mathbf{X}}) \leq \sqrt{2 D_{\rm JS}(P_{\mathbf{X}}\| Q_{\mathbf{X}})}$ proved in Lemma 3 of \citet{NEURIPS2018_565e8a41}.
We can prove below with the same strategy,
\begin{align*}
|H(P_{Y}) - H(Q_{Y})| &\leq 4 D^{\frac{1}{4}}_{\rm JS}(P_{Y}|| Q_{Y}), \\  
   | H(P_{Y|\mathbf{X}=\mathbf{x}}) - H(Q_{Y|\mathbf{X}=\mathbf{x}}) | & \leq 4 D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}|| Q_{Y|\mathbf{X}=\mathbf{x}}).
\end{align*}
\end{proof}


\begin{corollary}\label{thm:cond_entropy_diff_bound-2.0} For a data distribution $P_{\mathbf{X}Y}=P_{Y|\mathbf{X}}P_{\mathbf{X}}$, MLLM $P_{\theta}(\cdot|\mathbf{x})$, and Kullback-Leibler divergence $D_{\rm KL}$, if ${\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}\|P_{\theta}(\cdot|\mathbf{x}))})\leq \epsilon$ for a constant $\epsilon$, then
\begin{equation*}
\begin{split}
    & \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [ H( P_{\theta}(\cdot|\mathbf{x})) - H(P_{Y|\mathbf{X}=\mathbf{x}})]
     \leq  4.4 \epsilon^{\frac{1}{8}}
\end{split}
\end{equation*}
\end{corollary}

\begin{proof}
\begin{equation*}
\begin{split}
    \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [ H( P_{\theta}(\cdot|\mathbf{x})) - H(P_{Y|\mathbf{X}=\mathbf{x}})]
    & \leq \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [ \big| H( P_{\theta}(\cdot|\mathbf{x})) - H(P_{Y|\mathbf{X}=\mathbf{x}}) \big| ] \\ 
    & \leq 4 \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}|| P_{\theta}(\cdot|\mathbf{x})) \\ 
    & \leq 4\cdot2^{\frac{1}{8}} \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D^{\frac{1}{8}}_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}|| P_{\theta}(\cdot|\mathbf{x})) \\ 
    & \leq  4.4 \epsilon^{\frac{1}{8}}.
\end{split}
\end{equation*}
We started from Lemma \ref{thm:entropy_diff_bound} and use Pinsker's inequality \cite{pinsker1964information} to leverage $D_{\rm JS}(\cdot|\cdot)\leq D_{\rm TV}(\cdot,\cdot)\leq \sqrt{2 D_{\rm KL}(\cdot|\cdot)}$.
\end{proof}

\begin{lemma}\label{TV_KL}
For a data distribution $P_{\mathbf{X}Y}=P_{Y|\mathbf{X}}P_{\mathbf{X}}$, MLLM $P_{\theta}(\cdot|\mathbf{x})$, let $D_{\rm KL}$ and $D_{\rm TV}$ be Kullback-Leibler divergence and total variation distance, respectively. Denote $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[P_{\theta}(\cdot|\mathbf{x})]$, we have below inequality.
 \begin{equation*}
    \begin{split}
        D_{\rm TV}(P_{Y},P_{Y_{\theta}}) \leq &  \sqrt{2\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}  D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x})||P_{Y|\mathbf{X}=\mathbf{x}}  )}  .
        \end{split}
    \end{equation*}
\end{lemma}
\begin{proof}
    \begin{equation*}
    \begin{split}
       D_{\rm TV}(P_{Y},P_{Y_{\theta}})=& \sum_{y\in \mathcal{Y}} \big |\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} P_{Y|\mathbf{X}=\mathbf{x}}(y)-\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} P_{\theta}(y|\mathbf{x})\big | \\ \leq &  \sum_{y\in \mathcal{Y}} \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}  \big |P_{Y|\mathbf{X}=\mathbf{x}}(y)-P_{\theta}(y|\mathbf{x})\big | \\ = &
       \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}  D_{\rm TV} (P_{Y|\mathbf{X}=\mathbf{x}}, P_{\theta}(\cdot|\mathbf{x}))    \\ \leq &  \sqrt{2\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}  D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x}) || P_{Y|\mathbf{X}=\mathbf{x}}  )}  
        \end{split}
    \end{equation*}
\end{proof}


\begin{lemma}\label{thm:cond_entropy_diff_bound-3} 
For a data distribution $P_{\mathbf{X}Y}=P_{Y|\mathbf{X}}P_{\mathbf{X}}$, MLLM $P_{\theta}(\cdot|\mathbf{x})$, and Kullback-Leibler divergence $D_{\rm KL}$, if ${\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{Y|\mathbf{X}=\mathbf{x}}\|P_{\theta}(\cdot|\mathbf{x}))})\leq \epsilon$ and $P_{\mathbf{X}Y}>c>0$ for a constant $c$ and $\epsilon$, then
\begin{equation*}
\begin{split}
    &{\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x}))} \| P_{Y|\mathbf{X}=\mathbf{x}})\leq 4.4\epsilon^{\frac{1}{8}} - \log c \sqrt{2\epsilon}.
\end{split}
\end{equation*}
\end{lemma}
\begin{proof}
Note that
\begin{equation*}
\begin{split}
    \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [ H( P_{\theta}(\cdot|\mathbf{x})) - H(P_{Y|\mathbf{X}=\mathbf{x}})] = & \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [\mathbb{E}_{y\sim P_{\theta}(\cdot|\mathbf{x})}\log P_{Y|\mathbf{X}=\mathbf{x}}(y) -\mathbb{E}_{y\sim P_{Y|\mathbf{X}=\mathbf{x}}} \log P_{Y|\mathbf{X}=\mathbf{x}}(y) ] \\ 
    +& {\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x}))} \| P_{Y|\mathbf{X}=\mathbf{x}}).
    \end{split}
\end{equation*}
Therefore,
\begin{equation*}
\begin{split}
   {\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm KL}(P_{\theta}(\cdot|\mathbf{x}))} \| P_{Y|\mathbf{X}=\mathbf{x}})
   &\leq \big |  \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [\mathbb{E}_{y\sim P_{\theta}(\cdot|\mathbf{x})}\log P_{Y|\mathbf{X}=\mathbf{x}}(y) -\mathbb{E}_{y\sim P_{Y|\mathbf{X}=\mathbf{x}}} \log P_{Y|\mathbf{X}=\mathbf{x}}(y) ] \big | \\
   &+ \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [ H( P_{\theta}(\cdot|\mathbf{x})) - H(P_{Y|\mathbf{X}=\mathbf{x}})]
    \end{split}
\end{equation*}
Given Lemma \ref{thm:tv_based_bound}, it is easy to check that
\begin{equation*}
    \begin{split}
        &\big |  \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [\mathbb{E}_{y\sim P_{\theta}(\cdot|\mathbf{x})}\log P_{Y|\mathbf{X}=\mathbf{x}}(y) -\mathbb{E}_{y\sim P_{Y|\mathbf{X}=\mathbf{x}}} \log P_{Y|\mathbf{X}=\mathbf{x}}(y) ] \big |
        \\ \leq &- \log c ~ \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm TV}(P_{\theta}(\cdot|\mathbf{x}), P_{Y|\mathbf{X}=\mathbf{x}})
        \\ \leq & - \log c \sqrt{2\epsilon}.
    \end{split}
\end{equation*}
Then, with Corollary \ref{thm:cond_entropy_diff_bound-2.0}, we complete this proof.
\end{proof}





















\subsection{Proof for EMID upper bound}
Now, we give the proof for the upper bound of the EMI Difference (EMID) as below.
\vspace{0.2cm}
\begin{theorem}[General scenario]\label{thm:emid_bound_appendix}
Given $P_{\mathbf{X}Y}$ and $Q_{\mathbf{X}Y}$ distributions and an MLLM $P_{{\theta}}$, if there exist some constants $\delta_{P}$ and $\delta_{Q}$ such that
\begin{equation*}
    D_{\rm JS}(P_{Y_{{\theta}}}\|P_{Y})\leq \delta_{P},~~~ D_{\rm JS}(Q_{Y_{{\theta}}}\|Q_{Y})\leq \delta_{Q},
\end{equation*}
where $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$ and $Q_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$, then $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$ is upper bounded by
\begin{equation} \label{eq:emid_bound_appendix}
    \begin{split}
    & \widehat{H}\big({ D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}||Q_{X_{v}}) + D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}||Q_{X_{t}})} \big) \nonumber\\
    +&\widehat{H}\big({\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}\|Q_{X_{t}|X_{v}})+\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}\|Q_{X_{v}|X_{t}})} \big)\nonumber\\
    +&4  \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}} } [D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}\|Q_{Y|\mathbf{X}=\mathbf{x}})] + 8\Delta^{\frac{1}{4}},
    \end{split}
\end{equation}
\normalsize
where $\Delta=\delta_{P}+\delta_{Q}$, $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$ and \begin{equation*}
\begin{split}
    \bar{D}_{\rm JS}(P_{X|X'}||Q_{X|X'}) :=\mathbb{E}_{\mathbf{x}\sim P_{X'}}D_{\rm JS}(P_{X|{X'}=\mathbf{x}}\|Q_{X|{X'}=\mathbf{x}})+\mathbb{E}_{\mathbf{x}\sim Q _{X'}}D_{\rm JS}(P_{X|{X'}=\mathbf{x}}\|Q_{X|{X'}=\mathbf{x}}).
    \end{split}
\end{equation*}
\end{theorem}

\begin{proof} Let $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}P_{\theta}(\cdot|\mathbf{x})$ and $Q_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}P_{\theta}(\cdot|\mathbf{x})$, EMID can be expressed with entropy and conditional entropy terms as below.
\begin{align} \label{thm:proof_emid_decom}
&\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta) \nonumber\\
&= \text{EMI}(P_{\mathbf{X}Y};\theta) - \text{EMI}(Q_{\mathbf{X}Y};\theta) \nonumber\\
&=(H(P_{Y_{\theta}}) - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - H(P_Y) + H(P_{Y|\mathbf{X}})) - ((H(Q_{Y_{\theta}}) - \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - H(Q_Y) + H(Q_{Y|\mathbf{X}})) \nonumber\\
&\leq (H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}})) + \big(\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))]\big) + |H(P_{Y_{\theta}})  - H(P_Y) + H(Q_Y) - H(Q_{Y_{\theta}})| \nonumber\\
&\leq (H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}})) + \big(\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))]\big) + |H(P_{Y_{\theta}})  - H(P_Y)| + | H(Q_Y) - H(Q_{Y_{\theta}})| \nonumber\\
&\leq (H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}})) + \big(\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))]\big) + 4\big( D^{\frac{1}{4}}_{\rm JS}(P_{Y_{\theta}},P_Y) + D^{\frac{1}{4}}_{\rm JS}(Q_{Y_{\theta}},Q_Y)\big) \nonumber\\
&\leq \underline{(H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}}))}_{\text{ (A)}} + \underline{\big(\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))]\big)}_{\text{ (B)}} + 8\Delta^{\frac{1}{4}},
\end{align}
where $\Delta:=\delta_{P}+\delta_{Q}$. Now, we will derive the upper-bounds for the terms (A) and (B), independently. First, by adopting Lemma \ref{thm:entropy_diff_bound}, we can express the term $H(P_{Y|\mathbf{X}})$ as below.
\begin{align*}
&H(P_{Y|\mathbf{X}}) \\
&= \mathbb{E}_{P_{\mathbf{X}}}[H(P_{Y|\mathbf{X}=\mathbf{x}}) - H(Q_{Y|\mathbf{X}=\mathbf{x}})] + \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
&\leq \mathbb{E}_{P_{\mathbf{X}}}[ \big| H(P_{Y|\mathbf{X}=\mathbf{x}}) - H(Q_{Y|\mathbf{X}=\mathbf{x}}) \big| ] + \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
&\leq 4\mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})] + \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
&\leq 4\mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})] + \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] + \mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
\end{align*}
Then, the term $(\text{A})$ of ineq. \eqref{thm:proof_emid_decom}, i.e., $H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}})$, is represented as below:
\begin{align*}
&H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}}) \leq 4\mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})] + \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})]
\end{align*}

To replace $\mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})]$ into a more interpretable term, and we start from the restatement of Lemma 1 of \citet{shui2022novel}.

\begin{lemma}[restatement of Lemma 1 of \citet{shui2022novel}] \label{thm:lemma_shui}
Let $Z \in \mathcal{Z}$ be the real-valued integrable random variable, and denoting two distributions on a common space $\mathcal{Z}$ by $P$ and $Q$ such that $Q$ is absolutely continuous w.r.t. $P$. If for any function $f$ and $\lambda\in\mathbb{R}$ such that $\mathbb{E}_{P}[\exp(\lambda(f(z)-\mathbb{E}_{P}(f(z))))] < \infty$, then we have:
\begin{equation}
\begin{split}
\lambda (\mathbb{E}_{Q}f(z) - \mathbb{E}_{P}f(z)) &\leq D_{\rm KL}(Q || P) \\ &+ \log \mathbb{E}_{P}[\exp(\lambda (f(z)-\mathbb{E}_{P}(f(z))))] \nonumber
\end{split}
\end{equation}
\end{lemma} 

Now, let $\mathbf{X}$ and $Y$ denote observable variables from a joint distribution $D_{\mathbf{X}Y} \in \{P_{\mathbf{X}Y},Q_{\mathbf{X}Y}\}$, and we denote $f(\mathbf{x}):= H(Q_{Y|\mathbf{X}=\mathbf{x}}) \geq 0$ as a loss function of our interest, e.g., conditional entropy of $y$ given $\mathbf{x}$. Then, $f$ has a finite value of $\mathbb{E}_{D}[\exp(\lambda(f(\mathbf{x}) - \mathbb{E}_{D}(f(\mathbf{x}))))]$, and is bounded within interval $[0,\hat{H}(Q_{Y|\mathbf{x}})]$ where $\hat{H}(Q_{Y|\mathbf{x}}):=\max_{\mathbf{x}\in \mathcal{X}} H(Q_{Y|\mathbf{X}=\mathbf{x}})$. 

We next define a mixture distribution $M_{\mathbf{X}Y}=\frac{1}{2}(P_{\mathbf{X}Y}+Q_{\mathbf{X}Y})$ where the support of $M_{\mathbf{X}Y}$ covers that of $P_{\mathbf{X}Y}$ and $Q_{\mathbf{X}Y}$. Then, we get the inequality below by setting $P=M_{\mathbf{X}Y}$ and $Q=Q_{\mathbf{X}Y}$ for all $\lambda > 0$ according to the Lemma \ref{thm:lemma_shui}:
\begin{equation} \label{thm:ineq_pt_for_ub}
\begin{split}
&\mathbb{E}_{ Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{ M_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
&\leq \frac{1}{\lambda}(\log\mathbb{E}_{M_{\mathbf{X}}}[\exp(\lambda(f(\mathbf{x}) - \mathbb{E}_{ M_{\mathbf{X}}}(f(\mathbf{x})))] + D_{\rm KL}(Q_{\mathbf{X}}||M_{\mathbf{X}})).
\end{split}
\end{equation} 
Also, we get similar inequality by setting $P=M_{\mathbf{X}Y}$ and $Q=P_{\mathbf{X}Y}$ for all $\lambda < 0$ according to the Lemma \ref{thm:lemma_shui} as below:
\begin{equation} \label{thm:ineq_ps_for_ub}
\begin{split}
&\mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{M_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \\
&\geq \frac{1}{\lambda}(\log\mathbb{E}_{M_{\mathbf{X}}}[\exp(\lambda(f(\mathbf{x}) - \mathbb{E}_{M_{\mathbf{X}}}(f(\mathbf{x})))] + D_{\rm KL}(P_{\mathbf{X}}||M_{\mathbf{X}})).
\end{split}
\end{equation} 
Meanwhile, give that $f(\mathbf{x})$ is bounded within interval $\hat{H}(Q_{Y|\mathbf{x}})$, the $f(\mathbf{x})-\mathbb{E}_{M_{\mathbf{X}}}f(\mathbf{x})$ becomes a sub-Gaussian \cite{wainwright2019high} with the scale parameter $\sigma=\hat{H}(Q_{Y|\mathbf{x}})/2$ at most. Then, we can leverage the property of sub-Gaussian for the log moment generating function, 
\begin{equation} \label{thm:ineq_subgauss}
\begin{split}
\log\mathbb{E}_{M_{\mathbf{X}}}[\exp(\lambda(f(\mathbf{x}) - \mathbb{E}_{M_{\mathbf{X}}}(f(\mathbf{x})))] &\leq \log(\exp(\frac{\lambda^{2}\sigma^{2}}{2})) \leq \frac{\lambda^{2}\hat{H}(Q_{Y|\mathbf{x}})^{2}}{8}.
\end{split}
\end{equation}
By plugging the ineq. \eqref{thm:ineq_subgauss} into ineq. \eqref{thm:ineq_pt_for_ub} and ineq. \eqref{thm:ineq_ps_for_ub}, we can derive the following new inequalities:
\begin{equation*}
\begin{split}
&\mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})]-\mathbb{E}_{M_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \leq \frac{\lambda_{0}\hat{H}(Q_{Y|\mathbf{x}})^{2}}{8} +\frac{1}{\lambda_{0}}D_{\rm KL}(Q_{\mathbf{X}}||M_{\mathbf{X}}), \\
&\mathbb{E}_{M_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})]-\mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \leq \frac{\lambda_{0}\hat{H}(Q_{Y|\mathbf{x}})^{2}}{8} + \frac{1}{\lambda_{0}}D_{\rm KL}(P_{\mathbf{X}}||M_{\mathbf{X}}).
\end{split}
\end{equation*}
where $\lambda_{0}=\lambda$ stands for $\lambda>0$ in the ineq. \eqref{thm:ineq_pt_for_ub} and $\lambda_{0}=-\lambda$ for $\lambda<0$ in the ineq. \eqref{thm:ineq_ps_for_ub}.

By adding both inequalities above, and setting the $\lambda_{0}=\frac{2}{\hat{H}(Q_{Y|\mathbf{x}})}\sqrt{D_{\rm KL}(P_{\mathbf{X}}||M_{\mathbf{X}})+D_{\rm KL}(Q_{\mathbf{X}}||M_{\mathbf{X}})}$ results in:
\begin{equation} \label{thm:ineq_up_ts}
\begin{split}
&\mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \leq \hat{H}(Q_{Y|\mathbf{x}}) \sqrt{2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}})}.
\end{split}
\end{equation}


Next, a decomposition of KL divergence and the definition of JS divergence leads to the following inequality,
\begin{equation*}
\small
\begin{split}
&=2 D_{\rm JS}(P_{X_{v}X_{t}}||Q_{X_{v}X_{t}}) \nonumber\\
&= D_{\rm KL}(P_{X_{v}X_{t}}||M_{X_{v}X_{t}}) + D_{\rm KL}(Q_{X_{v}X_{t}}||M_{X_{v}X_{t}}) \nonumber\\
&= \frac{1}{2} \big( D_{\rm KL}(P_{X_{v}}||M_{X_{v}}) + \mathbb{E}_{P_{X_{v}}}D_{\rm KL}(P_{X_{t}|X_{v}=x_{v}} ||M_{X_{t}|X_{v}=x_{v}}) \big) + \frac{1}{2} \big( D_{\rm KL}(Q_{X_{v}}||M_{X_{v}}) + \mathbb{E}_{Q_{X_{v}}}D_{\rm KL}(Q_{X_{t}|X_{v}=x_{v}} ||M_{X_{t}|X_{v}=x_{v}}) \big) \nonumber\\
&+ \frac{1}{2} \big( D_{\rm KL}(P_{X_{t}}||M_{X_{t}}) + \mathbb{E}_{P_{X_{t}}}D_{\rm KL}(P_{X_{v}|X_{t}=x_{t}} ||M_{X_{v}|X_{t}=x_{t}}) \big) + \frac{1}{2} \big( D_{\rm KL}(Q_{X_{t}}||P_{X_{t}}) + \mathbb{E}_{Q_{X_{t}}}D_{\rm KL}(Q_{X_{v}|X_{t}=x_{t}} ||M_{X_{v}|X_{t}=x_{t}}) \big) \nonumber\\
&= D_{\rm JS}(P_{X_{v}}||Q_{X_{v}}) + D_{\rm JS}(P_{X_{t}}||Q_{X_{t}}) \nonumber\\
&+ \frac{1}{2} \big( \mathbb{E}_{P_{X_{v}}}D_{\rm KL}(P_{X_{t}|X_{v}=x_{v}}||M_{X_{t}|X_{v}=x_{v}}) + \mathbb{E}_{Q_{X_{v}}}D_{\rm KL}(Q_{X_{t}|X_{v}=x_{v}}||M_{X_{t}|X_{v}=x_{v}}) \big) \nonumber\\
&+ \frac{1}{2} \big( \mathbb{E}_{P_{X_{t}}}D_{\rm KL}(P_{X_{v}|X_{t}=x_{t}}||M_{X_{v}|X_{t}=x_{t}}) + \mathbb{E}_{Q_{X_{t}}}D_{\rm KL}(Q_{X_{v}|X_{t}=x_{t}}||M_{X_{v}|X_{t}=x_{t}}) \big) \nonumber\\
&\leq D_{\rm JS}(P_{X_{v}}||Q_{X_{v}}) + D_{\rm JS}(P_{X_{t}}||Q_{X_{t}}) + \bar{D}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}})
\end{split}
\end{equation*}
where $\bar{D}_{\rm JS}(P_{Y|X}||Q_{Y|X}):=\mathbb{E}_{x \sim P_{X}} D_{\rm JS}(P_{Y|X=x}||Q_{Y|X=x})+\mathbb{E}_{x \sim Q_{X}} D_{\rm JS}(P_{Y|X=x}||Q_{Y|X=x})$

Based on the above decomposition, we can modify the bound as below,
\begin{equation} \label{thm:x_shift_bound}
\begin{split}
&\mathbb{E}_{Q_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] - \mathbb{E}_{P_{\mathbf{X}}}[H(Q_{Y|\mathbf{X}=\mathbf{x}})] \nonumber \\
&\leq \hat{H}(Q_{Y|\mathbf{x}}) \sqrt{2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}})} \\
&\leq \hat{H}(Q_{Y|\mathbf{x}}) \sqrt{D_{\rm JS}(P_{X_{v}}||Q_{X_{v}})+D_{\rm JS}(P_{X_{t}}||Q_{X_{t}})+ \bar{D}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}}) }.
\end{split}
\end{equation}

Therefore, we get an upper-bound for the $(\text{A})$ term in ineq. \eqref{thm:proof_emid_decom} as below,

\begin{equation*} \label{thm:bound_final1}
\begin{split}
    H(P_{Y|\mathbf{X}})-H(Q_{Y|\mathbf{X}}) &\leq H(Q_Y) \sqrt{D_{\rm JS}(P_{X_{v}}||Q_{X_{v}})+D_{\rm JS}(P_{X_{t}}||Q_{X_{t}})+ \bar{D}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}}) } \\
    &+4 \mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})] \\
    &\leq \hat{H}(Q_{Y|\mathbf{x}}) \big( D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}||Q_{X_{v}})+D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}||Q_{X_{t}})\big)\\
    &+ \hat{H}(Q_{Y|\mathbf{x}}) \big( \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}}) \big) \\
    &+4\mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})].
\end{split}
\end{equation*}

Then, deriving a bound for the remaining $(\text{B})$ term in ineq. \eqref{thm:proof_emid_decom}, i.e., $\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))]$, is very similar to the procedure of deriving the upper-bound for the term $(\text{A})$ by switching $Q_{Y|\mathbf{X}}$ to $P_{\theta}(\cdot|\mathbf{X})$ and set the $f$ for Lemma \ref{thm:lemma_shui} as $f(\mathbf{x}):=H(P_{\theta}(\cdot|\mathbf{x}))$, thereby having the interval $[0,\hat{H}(P_{\theta})]$ where $\hat{H}(P_{\theta}):\max_{\mathbf{x}\in\mathcal{X}} H(P_{\theta}(\cdot|\mathbf{x}))$. This induces an upper-bound as below,

\begin{equation} \label{thm:bound_final2}
\begin{split}
    &\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] \\
    &\leq \hat{H}(P_{\theta}) \sqrt{2 D_{\rm JS}(P_{\mathbf{X}}||Q_{\mathbf{X}})}\\
    &\leq \hat{H}(P_{\theta})  \big( D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}||Q_{X_{v}})+D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}||Q_{X_{t}}) + \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}}) \big).
\end{split}
\end{equation}

Finally, we complete the proof by adding the ineq. \eqref{thm:bound_final1} and ineq. \eqref{thm:bound_final2} to induces the upper bound of $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$,

\begin{equation*}
    \begin{split}
        &\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta) \\
        &\leq (H(P_{Y|\mathbf{X}}) - H(Q_{Y|\mathbf{X}})) + \big(\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] - \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[H(P_{\theta}(\cdot|\mathbf{x}))] \big) + 8\Delta^{\frac{1}{4}} \\
        &\leq \hat{H} \big( D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}||Q_{X_{v}})+D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}||Q_{X_{t}})\big) \\
        &+ \hat{H} \big( \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}||Q_{X_{t}|X_{v}}) + \bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}||Q_{X_{v}|X_{t}}) \big) \\
        &+4\mathbb{E}_{P_{\mathbf{X}}}[D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}} || Q_{Y|\mathbf{X}=\mathbf{x}})]+8\Delta^{\frac{1}{4}}
    \end{split}
\end{equation*}
where $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$ and $\Delta=\delta_{P}+\delta_{Q}$.
\end{proof}

Then, we introduce an assumption over the consistency between conditional distributions as below.
\begin{assumption}[Consistency of conditional distributions]\label{assumption:consistent_cond_dist}
For the distributions $P_{\mathbf{X}Y}$ and $Q_{\mathbf{X}Y}$ over $\mathcal{X}\times \mathcal{Y}$, conditional distributions of $X_{t}$ given $X_{v}$, $X_{v}$ given $X_{t}$, and $Y$ given $\mathbf{X}=(X_{v},X_{t})$ are consistent between $P_{\mathbf{X}}Y$ and $Q_{\mathbf{X}}Y$. That is,
\begin{itemize}
    \item $P_{X_{t}|X_{v}}=Q_{X_{t}|X_{v}}$ and $P_{X_{v}|X_{t}}=Q_{X_{v}|X_{t}}$,
    \item $P_{Y|\mathbf{X}}=Q_{Y|\mathbf{X}}$.
\end{itemize}
\end{assumption}

Finally, we present the simplified version of EMID upper bound by leveraging the Assumption \ref{assumption:consistent_cond_dist}.
\begin{theorem}[Simplified scenario]\label{thm:emid_bound_simple_appendix} 
 Given an MLLM $P_{\theta}$ and distributions $P_{\mathbf{X}Y}$, $Q_{\mathbf{X}Y}$ which have consistent conditional distributions over variables $X_{v}|X_{t}$, $X_{t}|X_{v}$, and $Y|\mathbf{X}$,
if there exist some constants $\delta_{P}$ and $\delta_{Q}$ such that
\begin{equation*}
    D_{\rm JS}(P_{Y_{{\theta}}}\|P_{Y})\leq \delta_{P},~~~ D_{\rm JS}(Q_{Y_{{\theta}}}\|Q_{Y})\leq \delta_{Q},
\end{equation*}
where $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$ and $Q_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$, then $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$ is upper bounded by
\begin{equation}\label{eq:emid_bound_simple_appendix}
    \begin{split}
     \widehat{H}\big( { D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}\|Q_{X_{v}})} + {D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}\|Q_{X_{t}})} \big) + 8\Delta^{\frac{1}{4}},
    \end{split}
\end{equation}
\normalsize
where $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$ and $\Delta=\delta_{P}+\delta_{Q}$. 
\end{theorem}
\begin{proof}
Given Theorem \ref{thm:emid_bound_appendix}, Assumption \ref{assumption:consistent_cond_dist} zeros out the terms $\big({\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{t}|X_{v}}\|Q_{X_{t}|X_{v}})+\bar{D}^{\frac{1}{2}}_{\rm JS}(P_{X_{v}|X_{t}}\|Q_{X_{v}|X_{t}})} \big)$ and $\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}} } [D^{\frac{1}{4}}_{\rm JS}(P_{Y|\mathbf{X}=\mathbf{x}}\|Q_{Y|\mathbf{X}=\mathbf{x}})]$ which induces Eq. \eqref{eq:emid_bound_simple_appendix}, accordingly.
\end{proof}



























\begin{corollary}\label{thm:emid_bound_simple_v2} 
 Given an MLLM $P_{\theta}$ and distributions $P_{\mathbf{X}Y}$, $Q_{\mathbf{X}Y}$ which have consistent conditional distributions over variables $X_{v}|X_{t}$, $X_{t}|X_{v}$, and $Y|\mathbf{X}$, $\text{EMID}(P_{\mathbf{X}Y},Q_{\mathbf{X}Y};\theta)$ is upper bounded by
\begin{equation} \label{eq:emid_bound_simple_v2}
    \begin{split}
     &\widehat{H}\big( { D^{\frac{1}{2}}_{\rm JS}(P_{X_{v}}\|Q_{X_{v}})} + {D^{\frac{1}{2}}_{\rm JS}(P_{X_{t}}\|Q_{X_{t}})} \big) \\
     &+ 8(\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} D_{\rm TV}(P_{Y|\mathbf{X}=\mathbf{x}},P_{\theta}(\cdot|\mathbf{x})) + \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} D_{\rm TV}(Q_{Y|\mathbf{X}=\mathbf{x}},P_{\theta}(\cdot|\mathbf{x})))^{\frac{1}{4}},
    \end{split}
\end{equation}
where $D_{\rm TV}(\cdot,\cdot)$ is the total variation distance, and $ \widehat{H}=\max_{\mathbf{x}\in\mathcal{X}} [H(Q_{Y|\mathbf{X}=\mathbf{x}})+H(P_{\theta}(\cdot|\mathbf{x}))]$.
\end{corollary}

\begin{proof}
Let $P_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$ and $Q_{Y_{\theta}}=\mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} P_{{\theta}}(\cdot|\mathbf{x})$, then $D_{\rm JS}(\cdot|\cdot)\leq D_{\rm TV}(\cdot,\cdot)$ allow us to induce below,
\begin{equation}\label{ineq::delta}
\begin{split}
   &  D_{\rm JS}(P_{Y_{{\theta}}}\|P_{Y}) =  \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}}[D_{\rm JS}(P_{\theta}(\cdot|\mathbf{x})\|P_{Y|\mathbf{X}=\mathbf{x}})] \leq \mathbb{E}_{\mathbf{x}\sim P_{\mathbf{X}}} [D_{\rm TV}(P_{Y|\mathbf{X}=\mathbf{x}},P_{\theta}(\cdot|\mathbf{x}))],
   \\ & D_{\rm JS}(Q_{Y_{{\theta}}}\|Q_{Y}) = \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}}[D_{\rm JS}(P_{\theta}(\cdot|\mathbf{x})\|Q_{Y|\mathbf{X}=\mathbf{x}})]\leq \mathbb{E}_{\mathbf{x}\sim Q_{\mathbf{X}}} [D_{\rm TV}(Q_{Y|\mathbf{X}=\mathbf{x}},P_{\theta}(\cdot|\mathbf{x}))].
   \end{split}
\end{equation}
Noting that $a^{\frac{1}{4}}+b^{\frac{1}{4}}\leq 2(a+b)^{\frac{1}{4}}$ for $a,b \geq 0$, plugging the above inequality into the Theorem \ref{thm:emid_bound_simple_appendix} complete the proof.
\end{proof}
Although this alternative upper bound is looser than Theorem \ref{thm:emid_bound_simple_appendix}, Corollary \ref{thm:emid_bound_simple_v2} is more interpretable in the sense that it directly represents the model-specific discrepancy terms via distance between model output distribution and true conditional distributions, rather than expresses it through marginal distribution terms in $D_{\rm JS}(P_{Y_{\theta}}||P_{Y})$ and $D_{\rm JS}(Q_{Y_{\theta}}||Q_{Y})$. Therefore, as our model becomes more accurate at modeling the ground truth conditional distribution of $Y$ given $\mathbf{X}$, the EMID mainly depends on the divergence between the marginal distributions of visual and text inputs.
