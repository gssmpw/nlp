\section{Experiments}\label{sec:exp}

This section presents a comprehensive empirical evaluation of our RELAY framework through a series of experiments designed to address four key research questions:
\vspace{-6pt}
\begin{itemize}
\item \textbf{Q1}: How effectively does the looped model with explicit CoT alignment serve as a general-purpose reasoner across diverse tasks?
(Section~\ref{sec:multitask})
\item \textbf{Q2}: What advantages does the looped model with explicit CoT alignment demonstrate in length generalization compared to auto-regressive CoT models?
(Section~\ref{sec:multitask})
\item \textbf{Q3}: How can the length generalization capabilities of the looped model with explicit CoT alignment be leveraged to enhance auto-regressive CoT models?
(Section~\ref{ssec:relay_enhance_cot})
\item \textbf{Q4}: How reliable are the intermediate reasoning steps generated by the looped model with explicit CoT alignment?
(Section~\ref{ssec:relay_step_reliability})
\end{itemize}
\vspace{-6pt}
We address each question through carefully designed experiments, as detailed below.


\begin{figure*}[t]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=0.95\textwidth]{figures/result_acc_length_curve.pdf}
    \vspace{-16pt}
    % \caption{Performance of different models on length generalization across three tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS). Each subfigure presents the accuracy of the final answer with respect to increasing problem lengths for the looped model with explicit CoT alignment, the auto-regressive CoT model, and the vanilla looped model. Training problem lengths are limited to $\leq 15, 30,$ and $100$ for Arithmetic, ED, and LIS, respectively, while the evaluation spans extended problem lengths of $[15, 25]$, $[30, 40]$, and $[100, 120]$. The results demonstrate that the looped model with explicit CoT alignment not only significantly outperforms the auto-regressive CoT model in generalizing to longer problems but also maintains and even modestly surpasses the strong length generalization capability of the vanilla looped model, while additionally providing explicit intermediate reasoning steps for improved interpretability. \qifan{TODO: change fig here}} 
    \caption{Performance comparison of different models on long reasoning problems across three tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS).}
    % \vspace{-10pt}
    \label{fig:main_result}
    % \vspace{-10pt}
\end{figure*}
% Performance comparison of different models across three tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS). Each subfigure presents the accuracy of the final answer with respect to increasing problem lengths for five models: (1) RELAY-enhanced CoT model (baseline CoT model fine-tuned with data generated by the looped model with explicit CoT alignment), (2) the looped model with explicit CoT alignment, (3) CoT model fine-tuned with self-generated data, (4) the auto-regressive CoT baseline, and (5) the vanilla looped model. Training problem lengths are limited to $\leq 15, 30,$ and $100$ for Arithmetic, ED, and LIS, respectively, while evaluation spans extended problem lengths of $[15, 25]$, $[30, 40]$, and $[100, 120]$. The results demonstrate that the RELAY-enhanced CoT model significantly improves upon the auto-regressive CoT baseline, leveraging the structured reasoning data generated by the looped model with explicit CoT alignment. Notably, its performance reaches and even slightly surpasses that of the looped model with explicit CoT alignment in some cases. These results highlight that the length generalization capabilities of the looped model with explicit CoT alignment can be effectively leveraged to enhance standard auto-regressive CoT model through high-quality reasoning data, without altering its architecture.

\subsection{Multitask Training}\label{sec:multitask}
Following the single-task evaluation discussed in Section~\ref{sec:single_task}, we extend our analysis to a multitask learning setting to explore the general reasoning capabilities of three models: the looped model with explicit CoT alignment, the auto-regressive CoT model, and the vanilla looped model. In this setup, we jointly train the models on three representative reasoning tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS), each requiring multi-step reasoning to arrive at accurate final answers. This setting enables a thorough comparison of the models' ability to generalize effectively across diverse tasks.

For a detailed description of the tasks, including example inputs, expected answers, and the corresponding Chain-of-Thought (CoT) reasoning steps, please refer to Appendix~\ref{appendix:task_descriptions}.

% \qifan{TODO: repeated with sec3.2} Our evaluation encompasses three representative tasks from dynamic programming (DP) and mathematic domain~\citep{feng2024towards}: Longest Increasing Subsequence (LIS), Edit Distance (ED), and Arithmetic computation. These tasks encompass diverse problem-solving patterns and complexity levels.


% The LIS task requires finding the length of the longest strictly increasing subsequence within a given integer sequence. For the ED task, models must compute the minimum number of operations (insert, delete, or replace) needed to transform one sequence into another. The Arithmetic task, previously discussed in Section~\ref{sec:single_task}, involves computing the answer of arithmetic expressions containing basic operators ($+,-,\times,\div$) and brackets.

% The LIS dataset contains input sequences with lengths up to 100 integers, while the ED dataset includes first input strings with a maximum length of 30 characters. The Arithmetic dataset consists of input expressions containing up to 15 operators. All input sequences, CoT chains (in the format of $<$problem, CoT steps, answer$>$), and answers in LIS and ED are bounded-range integers and can therefore be tokenized.To ensure efficient tokenization, all components of the datasets—including input sequences, CoT chains (structured as <problem, CoT steps, answer>), and final answers for LIS and ED—are represented as bounded-range integers.

\textbf{Experimental Setup. }
We conduct experiments on the three tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS), to evaluate the generalization capabilities of the looped model with explicit CoT alignment in comparison with the auto-regressive CoT model and the vanilla looped model. Training datasets 
retain the same problem lengths as in Section~\ref{sec:single_task}: operator counts of $\leq 15$ for Arithmetic, input string lengths of $\leq 30$ for ED, and sequence lengths $\leq 100$ for LIS. Similarly, test datasets are constructed with extended problem lengths of $[15, 25]$, $[30, 40]$, and $[100, 120]$ for Arithmetic, ED, and LIS, respectively, to assess length generalization.

In this setup, each model---the looped model with explicit CoT alignment, the auto-regressive CoT model, and the vanilla looped model---is trained jointly on all three tasks by prepending a task-specific problem token (\texttt{[ARI]}, \texttt{[ED]}, \texttt{[LIS]}) to the input sequence, which distinguishes among tasks. All models are evaluated under the same metric, considering only the accuracy of final answer.

% This unified approach enables each model to serve as a general solution capable of handling the diverse reasoning requirements across these tasks. 

% We created a combined training dataset by generating and merging 1M samples from each task. To evaluate generalization capabilities, we construct task-specific test sets with increasing complexity: for LIS, input sequence lengths ranging from $\{100, 101, 102, 103, 104, 105\}$; for ED, first input string lengths ranging from $\{30, 31, 32, 33, 34, 35\}$; and for Arithmetic, operator counts ranging from $\{15, 16, 17, 18, 19, 20\}$. We implemented a standard decoder-only Transformer for the CoT model and an encoder-only Transformer with bidirectional attention for the RELAY model. We develop a dynamic iteration control mechanism based on CoT steps: for LIS, each iteration processes 10 CoT intermediate tokens; for ED, each iteration corresponds to one line in the transition matrix; and for Arithmetic, each iteration aligns with one intermediate computation step.


% \zhenyu{TODO: real number}

% \qifan{TODO: is vanilla looped needed here? or maybe just CoT baseline, RELAY, RELAY-CoT?}

\textbf{Results. }
Figure~\ref{fig:main_result} illustrates the comparative performance of the three models across the three tasks. All models achieve nearly 100\,\% accuracy on all tasks within the training distribution, demonstrating that the looped model with explicit CoT alignment can serve as a general-purpose reasoning engine capable of handling diverse tasks requiring multi-step reasoning. (\textbf{Q1})

However, for problems with lengths exceeding the training range, the looped model with explicit CoT alignment and the vanilla looped model significantly outperform the auto-regressive CoT model, showcasing the superiority of loop-based architectures in addressing tasks requiring generalization to longer inputs. 
Furthermore, the looped model with explicit CoT alignment not only maintains the strong length generalization capability of the vanilla looped model but even surpasses it notably, benefiting from the explicit alignment between CoT reasoning steps and loop iterations. This alignment provides structural guidance that enhances the model's reasoning capabilities over extended lengths as well as the ability to generate explicit intermediate CoT reasoning chains, making it both accurate and interpretable. 
These results establish the looped model with explicit CoT alignment as both a robust reasoning framework and a generally effective solution for length generalization challenges, outperforming standard auto-regressive CoT models across diverse tasks. (\textbf{Q2})

% Figure~\ref{fig:main_result} illustrates the comparative performance of the looped model with explicit CoT alignment, the auto-regressive CoT model and the vanilla looped model across the three tasks. 
% While all models achieve 100\% accuracy on in-length tasks, significant differences emerged in length generalization. Both the vanilla looped model and looped model with explicit CoT alignment outperform the auto-regressive CoT model across all tasks, with looped model with explicit CoT alignment showing slightly better results due to its alignment of reasoning steps with loop iterations. Importantly, looped model with explicit CoT alignment not only maintains the looped model’s length generalization performance but also generates explicit intermediate CoT reasoning tokens, making it both accurate and interpretable. These results demonstrate the robustness of the looped model with explicit CoT alignment in extending reasoning capabilities to problem lengths beyond the training range, while preserving interpretability through explicit reasoning tokens.

% The results demonstrate RELAY's superior capability in handling multiple reasoning tasks with a single model. Within the training distribution (LIS  100, ED  30, Arithmetic  15), both models achieve comparable high performance, with accuracy above 95\% across all tasks. However, their behaviors diverge significantly when tested on problems exceeding the training length: For the LIS task, RELAY maintains accuracy above 90\% up to sequence length 105, while the CoT model's performance drops to below 70\% beyond length 102. In the ED task, RELAY demonstrates even more pronounced advantages, sustaining accuracy above 85\% for input lengths up to 35 characters. In contrast, the CoT model's performance deteriorates rapidly, falling below 60\% accuracy for inputs longer than 32 characters. For Arithmetic computations, RELAY achieves the most significant improvement, maintaining accuracy above 80\% for expressions with up to 20 operators, while the CoT model's performance drops below 50\% beyond 17 operators. 
% Notably, the looped model with explicit CoT alignment demonstrates remarkable robustness in maintaining its performance across varying sequence lengths, exhibiting only minimal accuracy fluctuations even as problems become more complex. This stands in stark contrast to the auto-regressive CoT model, which suffers from sharp performance degradation on longer sequences. The results strongly validate our hypothesis that a single looped model can effectively serve as a general-purpose reasoning engine, successfully preserving its reasoning capabilities across diverse tasks regardless of the sequence length. This consistent performance highlights the model's superior length generalization properties, making it particularly well-suited for handling complex reasoning tasks of varying difficulties.


\subsection{Enhancing Auto-regressive Model with RELAY-Generated CoT Data}\label{ssec:relay_enhance_cot}
% \qifan{TODO: change title}

In this section, we utilize the looped model with explicit CoT alignment trained in Section~\ref{sec:multitask} to enhance the performance of the auto-regressive CoT model via effective data generation. Specifically, we leverage its ability to produce accurate reasoning chains for complex problems exceeding the training lengths. These reasoning chains serve as high-quality data, which are subsequently employed to fine-tune the auto-regressive CoT model.

\textbf{Experimental Setup. }
% Our approach follows a two-stage process to improve the auto-regressive CoT model's performance on longer sequences. two-stage和method的two-stage有一点冲突，干脆删了
First, we employ the looped model with explicit CoT alignment to generate CoT reasoning chains for problems of increased complexity, covering problem lengths of $[15, 25]$, $[30, 40]$, and $[100, 120]$ for Arithmetic, ED, and LIS tasks, respectively. These newly generated data is then merged with the original training dataset, which contains problems within the initial training length. Details of the sample proportions for different problem lengths when merging datasets are provided in Appendix~\ref{appendix:samples_dist_dataset}.

Next, we fine-tune the auto-regressive CoT model on this augmented dataset in a single phase. This fine-tuning process builds upon the well-trained auto-regressive CoT model from Section~\ref{sec:multitask}, retaining the same model structure
% and hyper-parameters 
while updating the weights with the augmented dataset. This process enables the model to incorporate longer CoT chains, thereby enhancing its reasoning capabilities on extended sequences.

% We design a two-stage process to improve the auto-regressive CoT model's performance on longer sequences. First, we utilize our trained looped model with explicit CoT alignment to generate CoT chains for problems of increased complexity: LIS with sequence lengths from 100 to 110, ED with input lengths from 30 to 40, and Arithmetic expressions containing 15 to 25 operators. For each complexity level and task, we generate 100K chains. Newly
% generated data is then merged with the original training
% dataset, which contains problems within the initial training
% length. Iinally, we fine-tune the original auto-regressive CoT model on this dataset which contains long CoT chains. We maintain the same model architecture and hyperparameters as in Section~\ref{sec:multitask}, only updating the weights through this additional training phase.

% \begin{figure*}[t]
%     \centering
%     \vspace{-5pt}
%     \includegraphics[width=\textwidth]{icml2025/figures/combined_acc_length_curve.pdf}
%     \vspace{-30pt}
%     % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%     \caption{Performance of different models on length generalization across three tasks: Arithmetic, Edit Distance (ED), and Longest Increasing Subsequence (LIS). Each subfigure presents the generalization accuracy with respect to increasing test sequence lengths for the RELAY model, CoT model, and traditional Loop model. Training sequence lengths are fixed at 15, 30, and 100 for Arithmetic, ED, and LIS, respectively, while the evaluation spans extended test ranges of $[15, 25]$, $[30, 40]$, and $[100, 120]$. The results demonstrate that the RELAY model significantly outperforms the CoT model in generalizing to longer sequences, while retaining the strong length generalization capability of the Loop model. \qifan{TODO: bit acc here, change fig here}} 
%     \label{fig:bit_acc}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \vspace{-5pt}
%     \includegraphics[width=.3\textwidth]{icml2025/figures/all_matrices_uniform_bar.pdf}
%     \vspace{-15pt}
%     % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%     \caption{ \qifan{TODO: change fig here}} 
%     \label{fig:main_result}
% \end{figure*}


% \begin{figure*}[t]
%     \centering
%     \begin{minipage}[t]{0.33\textwidth}
%         \centering
%         \includegraphics[height=0.11\textheight]{icml2025/figures/all_matrices_uniform_bar.pdf}
%         \vspace{-20pt}
%         % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%         \caption{Hit accuracy matrices for the LIS task with a problem length of 105 ($T = 11$ steps). The left matrix corresponds to data generated by the looped model with explicit CoT alignment, which achieves consistently high token accuracy across all positions. In contrast, the right matrix shows data generated by the auto-regressive CoT model, where accuracy is high only in the early positions but declines steadily in later steps.} 
%         \label{fig:matrix}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.66\textwidth}
%         \centering
%         \includegraphics[height=0.11\textheight]{icml2025/figures/bit_acc_length_curve.pdf}
%         \vspace{-8pt}
%         % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%         \caption{Bit accuracy curves over varying problem lengths for three tasks: Arithmetic, ED, and LIS. Each subfigure corresponds to one task and includes the performance of four models: the auto-regressive CoT model fine-tuned with data generated by the looped model with explicit CoT alignment, the auto-regressive CoT model fine-tuned with its own self-generated data, the looped model with explicit CoT alignment, and the baseline auto-regressive CoT model. The results demonstrate that both the looped model with explicit CoT alignment and the auto-regressive CoT model fine-tuned with its data achieve consistently high bit accuracy, exceeding 90\,\% even on lengths beyond the training data, while fine-tuning with self-generated data offers only limited improvements.\qifan{TODO: bit acc here, change fig here}} 
%         \label{fig:bit_acc}
%     \end{minipage}
% \end{figure*}



\textbf{Results.} Figure~\ref{fig:main_result} presents the accuracy curves of the 
RELAY-enhanced auto-regressive CoT model 
% auto-regressive CoT model fine-tuned with 
across problem lengths for the three tasks. Compared to the baseline auto-regressive CoT model, the auto-regressive CoT model fine-tuned with data generated by RELAY (i.e., by the looped model with explicit CoT alignment) exhibits significant improvements on problems exceeding the original training length. Notably, its performance approaches and even slightly surpasses that of the looped model with explicit CoT alignment in some cases, while consistently outperforming the baseline auto-regressive CoT model.

These results indicate that our RELAY framework effectively utilizes the length generalization capabilities of the looped model with explicit CoT alignment to improve the overall performance of auto-regressive model. By generating high-quality CoT reasoning data, RELAY enables the auto-regressive CoT model to better handle problems beyond its original training range, without altering its architecture. (\textbf{Q3})

% By incorporating the generated long CoT chains, we ensure that the fine-tuned auto-regressive CoT model benefits from the structured reasoning chains produced by the looped model with explicit CoT alignment. The results demonstrate that our approach leads to substantial improvements in the auto-regressive CoT model's overall reasoning accuracy.

% Figure~\ref{?} presents the performance comparison between the original auto-regressive CoT model, the RELAY-enhanced auto-regressive CoT model and the looped model with explicit CoT alignment across different tasks and sequence lengths. The results demonstrate substantial improvements in length generalization across all three tasks. For the LIS task, the RELAY-enhanced CoT model maintains accuracy above 85\% for sequences up to length 110, while the original CoT model's performance drops below 60\% beyond length 102. Similarly, on the ED task, the enhanced model achieves over 80\% accuracy for input strings up to length 40, compared to the baseline model's rapid degradation to below 50\% accuracy beyond length 32. For Arithmetic expressions, the enhanced model successfully maintains accuracy above 90\% for problems with up to 25 operators, while the original model's performance falls below 40\% for expressions with more than 17 operators. Notably, the improvement is most pronounced in the Arithmetic task, where the enhanced model shows a 45\% absolute improvement in accuracy for expressions with 20 operators. These results demonstrate that RELAY can effectively transfer its superior length generalization capabilities to standard auto-regressive models through high-quality demonstration generation, substantially extending their practical applicability to more complex reasoning tasks.



% \subsection{Analyzing RELAY's Step-wise Reasoning}
\subsection{Evaluating the Reliability of RELAY-Generated Intermediate Reasoning Steps}\label{ssec:relay_step_reliability}
This section aims to demonstrate the reliability of CoT chains generated by the looped model with explicit CoT alignment compared to the auto-regressive CoT model's self-generated data. Specifically, we highlight that the generated data from the auto-regressive CoT model, even when the final result is correct, often contains incorrect intermediate steps, which is why utilizing these data fails to improve the model's performance in complex problems with longer lengths. In contrast, data generated by the looped model with explicit CoT alignment avoids these issues by ensuring both accurate intermediate reasoning steps and the final answer, enabling effective fine-tuning of the auto-regressive model and significantly enhancing its performance.

\begin{figure}
    \centering
    \includegraphics[width=.5\textwidth]{figures/all_matrices_uniform_bar.pdf}
    \vspace{-20pt}
    % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
    \caption{Hit accuracy matrices for the LIS task with a problem length of 105 ($T = 11$ steps).} 
    \label{fig:matrix}
    \vspace{-5pt}
\end{figure}
% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=.8\textwidth]{icml2025/figures/all_matrices_uniform_bar.pdf}
%     \vspace{-10pt}
%     % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%     \caption{Hit accuracy matrices for the LIS task with a problem length of 105 ($T = 11$ steps).} 
%     \label{fig:matrix}
% \end{figure*}
% The left matrix corresponds to data generated by the looped model with explicit CoT alignment, which achieves consistently high token accuracy across all positions. In contrast, the right matrix shows data generated by the auto-regressive CoT model, where accuracy is high only in the early positions but declines steadily in later steps.


% compare data: Looped with CoT alignment, CoT itself
% compare result: RELAY-enhanced CoT, CoT SFT

\textbf{Experimental Setup. }
We evaluate the effectiveness of two types of generated data: by the looped model with explicit CoT alignment and the auto-regressive CoT model. This evaluation focuses on two metrics: (1) hit matrix and (2) bit accuracy, which provides a detailed perspective on reasoning steps reliability.

For the hit matrix, we select the LIS task as an example due to the structured nature of its reasoning steps. The intermediate reasoning steps of LIS tasks follows a $T\times 11$ matrix format, where $T$ corresponds to the number of CoT steps as well as the iteration number of the looped model, and 11 represents the number of tokens per step (10 numbers as prescribed in our dataset, along with one delimeter \texttt{<sep>}). This structured format makes the LIS task particularly suitable for evaluating and visualizing reasoning step reliability, offering an intuitive representation of the proportion of tokens at each position that match the ground truth reasoning steps.

Bit accuracy is provided across all three tasks of varying lengths, evaluating token-wise counted accuracy for the whole reasoning step. Comparisons are made between the auto-regressive CoT models fine-tuned by the two types of generated data respectively. 
% , as well as the looped model with explicit CoT alignment and the CoT model baseline.


% In this section, we evaluate the effectiveness of different generated data for long problem lengths. The experiment focuses on comparing two primary types of data: (1) data generated by the looped model with explicit CoT alignment and (2) self-generated data by a fine-tuned CoT model. Additionally, we include two supplementary datasets for comparison: (3) data generated by the CoT baseline model itself, and (4) data generated by the looped model with explicit CoT alignment, further used to fine-tune the CoT model.

% Here, we aim to demonstrate that the data generated by the auto-regressive CoT model itself contain incorrect intermediate steps, 

For the auto-regressive CoT model self-generated data, we conduct the following experiment under two parallel settings, using either a vanilla looped model or ground-truth answers as verifiers. The experiment consists of the following steps: (1) Use the auto-regressive CoT model to generate CoT chains for long problem lengths. (2) Filter these data by the looped model or ground-truth, retaining only those where the final answers match. (3) The filtered data are then used to fine-tune the auto-regressive CoT model. The fine-tuning process aims to improve the model's ability to generate reasoning trajectories and reach the correct final answer for longer problems. 

Meanwhile, data generated by the looped model with explicit CoT alignment is employed as the fine-tuning dataset for the same initial auto-regressive CoT model checkpoint, under the same fine-tuning parameters and controlled ratio of samples with different lengths. Both approaches are evaluated across three tasks with varying lengths to assess performance improvements.

\textbf{Results.} We evaluate the hit accuracy matrix for the LIS task with a problem length of 105, which corresponds to $T = \lceil 105 / 10 \rceil = 11$ steps, resulting in an $11 \times 11$ matrix (We also provide results for problem length of 101 in Appendix~\ref{appendix:hit_matrix_101}). As shown in Figure~\ref{fig:matrix}, data generated by the looped model with explicit CoT alignment achieves consistently high token accuracy across all positions, with most values approaching 100\,\%, demonstrating its ability to produce high-quality and reliable data. (\textbf{Q4}) In contrast, the data generated by the auto-regressive CoT model exhibits high token accuracy only in the first few positions, while the accuracy steadily decreases in later steps. Although the delimiter tokens \texttt{<sep>} at the end of each step  achieve high accuracy, this simply implies that the auto-regressive CoT model has only captured the basic format of the reasoning process but fails to predict accurate tokens, which indicates its limited capability to maintain accurate prediction throughout the reasoning process for longer problems.
% \footnote{We also provide results for length of 101 in Appendix~\ref{appendix:hit_matrix_101}.}

The bit accuracy results for the models fine-tuned with different datasets across the three tasks (Arithmetic, ED, LIS) and varying problem lengths are provided in Appendix~\ref{appendix:bit_acc_results}. 

We additionally provide the accuracy of the final answer for the auto-regressive CoT model fine-tuned with self-generated data in Figure~\ref{fig:main_result}, noted as ``AR-CoT + Self Chains \& Loop/GT Answers'', corresponding to data filtered by the looped model or ground-truth answers, respectively, which only shows a slight improvement over the baseline model.


% Recent research has highlighted a critical limitation in auto-regressive CoT reasoning: they often generate incorrect intermediate reasoning steps while coincidentally arriving at correct final answers~\cite{debjit2024making}. To address this concern, we conduct a comparative analysis of reasoning path accuracy between the auto-regressive CoT model and our RELAY model. Figure~\ref{?} presents the results of this analysis, revealing distinct patterns in reasoning quality. While both models maintain high accuracy in their reasoning paths within the training length regime, their performance diverges significantly on longer sequences. The auto-regressive model exhibits a sharp decline in reasoning path accuracy beyond the training length, whereas RELAY maintains substantially higher reasoning fidelity across extended sequences. This sustained performance suggests that RELAY's architectural design promotes more robust and reliable intermediate reasoning, rather than merely achieving correct answers through coincidental paths.

% This section presents an investigation into the performance of loop and CoT (Chain of Thought) models on long-sequence reasoning tasks, specifically arithmetic operations, shortest edit distance, and longest increasing subsequence. For each task, we employ datasets with sequence lengths of $15$, $30$, and $80$ during training. Fine-tuning is performed on extended intervals $[15, 25]$, $[30, 50]$, and $[80, 120]$, where the problem length determines the number of iterations for the loop model.

% Initially, we independently train loop and CoT models for each task, followed by an evaluation of the loop model's performance on length generalization. Subsequently, outputs from the loop model are utilized as pseudo-labels to fine-tune the CoT model. Additionally, we train a unified loop and CoT model across all three tasks. By repeating the aforementioned procedure, we obtain a generalized CoT model that demonstrates robust performance on all tasks and exhibits improved length generalization capabilities.

% \subsection{Model Comparison}
% \begin{itemize}
%     \item \textbf{Loop Model:} Produces the final result directly by iterating in the embedding space based on the problem length. It achieves strong performance in length generalization but cannot provide intermediate reasoning steps.
%     \item \textbf{CoT Model:} Generates tokens auto-regressively, sequentially outputting intermediate reasoning steps and concluding with the final result. However, it performs poorly on length generalization tasks.
% \end{itemize}

% \subsection{Key Challenges}
% For reasoning tasks, focusing excessively on local information exacerbates cumulative errors over longer sequences, while prioritizing global information often leads to performance degradation as sequence lengths increase.

% \subsection{Proposed Strategy}
% To address the CoT model's limitations in length generalization, we leverage the loop model to enhance its capabilities. Initially, separate loop and CoT models are trained on shorter datasets. For longer problems where no ground truth labels are available, the loop model is used to generate outputs from the input data, which serve as pseudo-labels for supervising the fine-tuning of the CoT model. This strategy effectively enhances the CoT model's generalization to longer reasoning tasks, achieving improved generalization performance.


% \section{Conclusion}
% In this work, we explored the challenges and limitations of length generalization in reasoning tasks, specifically focusing on the performance of loop and CoT (Chain of Thought) models. We identified that while the loop model excels at length generalization, it lacks the ability to provide intermediate reasoning steps. Conversely, the CoT model can generate interpretable reasoning steps but struggles with long-sequence generalization. To address these issues, we proposed a novel approach that leverages the strong generalization capabilities of the loop model to guide the CoT model through pseudo-labeling for fine-tuning on longer sequences.

% Our contributions can be summarized as follows:
% \begin{enumerate}
%   \item We systematically analyzed the performance of loop and CoT models on various reasoning tasks, providing insights into their respective strengths and weaknesses.
%   \item We proposed a unified training framework that integrates the generalization strength of the loop model with the interpretability of the CoT model, enabling the latter to achieve better length generalization.
%   \item We demonstrated the feasibility and effectiveness of this framework through experiments on multiple reasoning tasks, achieving state-of-the-art performance in length generalization.
% \end{enumerate}

% This work provides a promising direction for bridging the gap between generalization performance and interpretability in reasoning models, contributing to the advancement of long-sequence reasoning tasks.


