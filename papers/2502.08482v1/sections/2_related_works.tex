\section{Related Work}

\subsection{Auto-regressive LLM with Chain-of-Thought}

Chain-of-Thought (CoT) has emerged as a powerful technique for enhancing language models' reasoning capabilities both empirically~\cite{wei2022chain,khot2022decomposed} and theoretically~\cite{feng2024towards,merrill2024the}, especially in latest models such as OpenAI O1\footnote{https://openai.com/o1}, DeepSeek r1~\cite{deepseekai2025deepseekr1} and Qwen QwQ\footnote{https://qwenlm.github.io/blog/qwq-32b-preview}. By generating intermediate reasoning steps token by token, these models effectively decompose complex problems into sequential subprocesses. However, two critical challenges persist. First, obtaining high-quality CoT training data remains time-consuming and labor-intensive~\cite{lightman2024lets}, especially for problems requiring sophisticated reasoning chains. Second, the generation and understanding of extended reasoning sequences can be problematic~\cite{xiao2023conditions,jin-etal-2024-impact,mao2024lift}.

\subsection{Looped Transformer}
Research on looped Transformers has evolved significantly over recent years. The initial studies by \citet{dehghani2018universal} and \citet{lan2020albert} demonstrated the effectiveness of parameter sharing across layers in supervised learning and BERT pretraining. This line of research has since expanded in both theoretical and practical directions. On the theoretical front, \citet{giannou2023looped} and \citet{xu2024expressive} established fundamental properties of looped Transformers, proving their Turing completeness and characterizing their approximation capabilities. \citet{gatmiry2024can} further advanced this understanding by showing how to incorporate inductive biases for learning iterative algorithms, particularly in the context of multi-step gradient descent for in-context learning. Empirically, looped Transformers have shown promising results across various applications. \citet{yang2024looped} demonstrated their parameter efficiency in data-fitting tasks, while \citet{luca2024simulation} and \citet{chen2024bypassing} revealed their potential in graph algorithm simulation and in-context learning enhancement. Notably, \citet{fan2024looped} established their superior length generalization capabilities in RASP-L tasks. In the domain of algorithm learning, \citet{gao2024expressive} introduced AlgoFormer, a framework that leverages looped Transformers for algorithm representation and learning. While these works have extensively explored various aspects of looped Transformers, our work takes a distinct direction. We specifically focus on leveraging the better length generalization of looped Transformers for helping standard auto-regressive Transformers.
% Looping in Transformer models has been empirically studied by \citet{dehghani2018universal} and \citet{lan2020albert}, who demonstrated the advantages of integrating looping mechanisms in supervised learning tasks and BERT pretraining, respectively. More recently, theoretical works by \citet{giannou2023looped} and \citet{xu2024expressive} establish looped Transformers' Turing completeness and approximation power, with the latter highlighting the importance of scaling parameters for each loop. \citet{gatmiry2024can} further showcases how to build inductive bias towards learning iterative algorithms, proving their effectiveness in multi-step gradient descent for in-context learning. Empirically, \citet{yang2024looped} demonstrate the parameter efficiency of looped Transformer in data-fitting tasks, while \citet{luca2024simulation} and \citet{chen2024bypassing} highlight their potential in simulating graph algorithms and improving in-context learning. \citet{fan2024looped} demonstrate the length generalization abilities of looped Transformer in handling RASP-L tasks. Additionally, \citet{gao2024expressive} propose AlgoFormer, a framework that leverages looped Transformers for algorithm representation and learning. In contrast to these works, we focus on leveraging the better length generalization of looped Transformers for helping standard auto-regressive models.

\subsection{Approaches for Length Generalization}

The capability of Transformers to generalize to longer sequence is influenced by their positional encodings~\citep{alibi}. Recent research has pursued two primary directions to enhance length generalization capabilities of LLMs. The first focuses on developing advanced relative positional encoding schemes~\citep{raffel2020exploring,alibi,chi2022kerple,sun2022length,chi2023dissecting,li2024functional}, while the second explores modifications to positional representations through index granularity adjustments~\cite{chen2023extending,peng2024yarn} and strategic index shifting~\cite{ruoss2023randomized,zhu2024pose}. These works are orthogonal to the central contributions of this paper. 
% The length generalization problem can be also mitigated by using high-quality long sequences during training. 
A parallel line of work focuses on improving the reasoning capabilities of LLMs through better training data. These methods typically leverage accessible labels or rewards to generate and filter reasoning steps, selecting those that yield correct solutions or high rewards~\cite{zelikman2022star,yuan2024scaling,singh2024beyond,hosseini2024vstar}. However, a critical limitation emerges from LLMs' tendency to generate incorrect or superfluous intermediate reasoning steps while still arriving at correct solutions through chance~\cite{debjit2024making}. This phenomenon significantly constrains the effectiveness of LLM fine-tuning for complex reasoning tasks~\cite{xia2024less,zhou2023lima}.