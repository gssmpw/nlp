\section{Introduction}
Reasoning plays a central role in shaping effective decision-making processes and guiding problem-solving strategies in artificial intelligence systems. For large language models (LLMs), the most effective way to achieve reasoning is through Chain-of-Thought~\cite{wei2022chain,khot2022decomposed}, which generates all intermediate steps token by token until the final answer is reached. However, generating the correct reasoning process using LLMs is challenging. On one hand, the Chain-of-Thought process can be very long, sometimes growing polynomially with respect to the prompt length~\cite{feng2024towards,merrill2024the}. When the reasoning length exceeds the training data length, it encounters the length generalization problem, where accuracy can drop significantly~\cite{xiao2023conditions,jin-etal-2024-impact}. On the other hand, web data is often noisy, and learning from incorrect trajectories can lead to incorrect answers. While synthetic data could mitigate this issue~\cite{lightman2024lets}, it requires significant human effort and knowledge to generate and curate. 

Recently, an alternative framework has gained attention, known as the looped Transformer~\cite{giannou2023looped}. In general, the looped Transformer is a standard Transformer model with cross-block parameter sharing, like AlBERT~\cite{lan2020albert}. In this framework, the input prompt (i.e., the problem) is processed through repeated iterations of the same block, with the number of iterations adaptively determined by the problem complexity. See Figure~\ref{fig:loop} for an illustration. Several preliminary results~\cite{fan2024looped} show that the looped Transformer model has better length generalization capabilities, partially because the increase in problem complexity (e.g., problem length) is not as significant as in the Chain-of-Thought steps. 

% \begin{figure*}[t]
%     \centering
%     % \includegraphics[width=0.95\linewidth]
%     % {icml2025/figures/loop.pdf}
%     \vspace{-5pt}
%     \includesvg[inkscapelatex=false, width=0.6\linewidth]{loop}
%     \caption{Visualization of Chain-of-Thought(CoT) and looping process. As the complexity of problem \(x\) increases, in the auto-regressive CoT models, the number of reasoning tokens escalates. In contrast, in the looped Transformer, the number of iterations of the loop block increases. } 
%     \label{fig:loop}
%     \vspace{-14pt}
%     % TODO: fix color
% \end{figure*}

However, the success of this approach comes with some practical limitations. While determining appropriate loop iterations is feasible for reasoning tasks, it becomes problematic in general tasks, such as translation and summarization. Furthermore, although the looped Transformer can handle specific reasoning tasks, it remains unclear whether it possesses the capability to manage multiple reasoning tasks within a single model. Given these concerns, a natural question arises: if the looped Transformer is a general reasoner, can we explore ways to integrate its capabilities into the Chain-of-Thought framework of standard auto-regressive models? This integration would allow us to leverage the looped Transformer's strong performance on complex reasoning problems while preserving the versatility that allows auto-regressive models to excel in diverse language tasks.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/loop.pdf}
    % \vspace{-5pt}
    % \includesvg[inkscapelatex=false, width=1.0\linewidth]{loop}
    \caption{Visualization of Chain-of-Thought (CoT) and looping process. As the complexity of problem increases, in the auto-regressive CoT model, the number of reasoning tokens escalates. In contrast, in the looped model, the number of iterations of the loop block increases. } 
    \label{fig:loop}
    \vspace{-14pt}
    % TODO: fix color
\end{figure}

In this paper, we introduce \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), a novel framework that leverages looped Transformer's superior capabilities to help auto-regressive models handle longer reasoning chains. At its core, our approach centers on two key innovations. First, we demonstrate empirically that a single looped Transformer model can serve as a general reasoner across multiple tasks while maintaining strong length generalization abilities. Second, we propose an iteration-wise alignment between the looped Transformer and Chain-of-Thought reasoning steps, enabling the looped model to generate accurate reasoning chains for problems beyond training length. These generated reasoning chains can then serve as training data for auto-regressive models, establishing a bridge between the two architectural paradigms.

We conduct extensive experiments demonstrating that our approach significantly improves the reasoning abilities of auto-regressive Transformers through high-quality generated reasoning chains.




%The development of large language models (LLMs)~\cite{} has demonstrated remarkable capabilities across various natural language tasks. These models, built upon the Transformer~\cite{attention} architecture and trained on massive text corpora, have achieved near-human performance in many traditional natural language processing benchmarks. However, these models face significant challenges in tasks requiring systematic reasoning, particularly when they need to generalize beyond their training distribution~\cite{}. This challenge of length generalization has emerged as a critical limitation in current artificial intelligence systems. Unlike humans, who can naturally extend their reasoning to longer or more complex versions of familiar problems, current Chain-of-Thought approaches struggle to maintain consistent performance when reasoning steps exceed their training examples~\cite{}.

%The predominant approach to reasoning in current auto-regressive language models relies on chain-of-thought prompting~\cite{}, which breaks down complex problems into sequential steps. This method emerged as a breakthrough in making neural language models' reasoning processes more transparent and manipulable, enabling them to solve complex problems by mimicking human-like step-by-step thinking. While this approach has shown promise in making model reasoning more transparent and controllable, it faces fundamental limitations. As problem complexity increases, the number of required reasoning steps grows polynomially, leading to an increased probability of errors at each step. This cascading effect is particularly problematic because errors in earlier steps can propagate through the entire reasoning chain, ultimately resulting in incorrect conclusions~\cite{}. Our empirical studies have also consistently demonstrated that performance degrades significantly when models encounter problems that exceed the complexity of their training examples, highlighting the challenge of reliable reasoning generalization. For instance, as demonstrated \zhenyu{in Figure}~\ref{}, models equipped with chain-of-thought demonstrations for arithmetic problems with 15 operands exhibit catastrophic performance degradation when faced with problems containing more operands, despite the underlying mathematical principles remaining invariant.



%Recent architectural innovations have led to the development of looped Transformers~\cite{jasonlee}, an encoder-only Transformer with recurrence. These models leverage explicit iterative computation mechanisms for enhanced generalization beyond training distribution, particularly in mathematical induction and algorithmic reasoning tasks. Their distinctive ability to learn and apply recursive patterns enables more structured and generalizable information processing compared to traditional Transformer architectures~\cite{theorypaper}. However, the success of looped Transformers comes with important caveats. While they excel at specific types of structured reasoning tasks, they struggle with general language processing tasks that standard Transformer handles effectively, such as translation and dialogue summarization. This limitation stems from their encoder-only architecture, which lacks the auto-regressive generation capabilities crucial for flexible language processing. Furthermore, two critical questions remain unaddressed: whether looped Transformers can function as general reasoning engines beyond task-specific training, and how their superior performance can be effectively leveraged in broader applications.