
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Task Descriptions}\label{appendix:task_descriptions}
Below, we present the detailed descriptions of each task from~\citet{feng2024towards}, including examples of inputs, expected answers, and the corresponding Chain-of-Thought (CoT) reasoning steps used to derive the final answers.

\begin{enumerate}
    \item \textbf{Arithmetic}. This task involves computing the answer of arithmetic expressions containing numbers, basic operations ($+,-,\times,\div,=$), and brackets. For example:
    % \begin{itemize}
    %     \item \textbf{Input:} $(3+1)\times(2+4)=$
    %     \item \textbf{Output:} $24$
    % \end{itemize}
    % For the CoT model:
    \begin{itemize}
        \item \textbf{Input:} $(6+9)\div(7+2\times 5-4\times 3) = $
        \item \textbf{CoT Steps:}
        \begin{align*}
        & 15\div(7+2\times 5-4\times 3) =  \\
        & 15\div(7+10-4\times 3) =  \\
        & 15\div(17-4\times 3) =  \\
        & 15\div(17-12) =  \\
        & 15\div 5 = 
        \end{align*}
        \item \textbf{Answer:} 3
        % \begin{align*}
        % &\mathrel{\phantom{=}} 15\div(7+2\times 5-4\times 3) \\
        % &= 15\div(7+10-4\times 3) \\
        % &= 15\div(17-4\times 3) \\
        % &= 15\div(17-12) \\
        % &= 15\div 5 \\
        % &= 3
        % \end{align*}
    \end{itemize}

    \item \textbf{Edit Distance (ED)}. This task requires computing the minimum number of operations (insert, delete, or replace) needed to transform one sequence into another.
    % This task requires computing the edit distance between two sequences. 
    The input consists of two sequences separated by a delimiter \texttt{|}:
    \begin{itemize}
        \item \textbf{Input:} \texttt{o t m l | o t t m l <sep>}
        \item \textbf{CoT Steps:}
        \begin{verbatim}
        0 2 4 6 7 ,
        2 0 2 4 6 ,
        4 2 3 2 4 ,
        6 4 5 4 2 ,
        \end{verbatim}
        % <sep>     % RMK: <sep> removed to be more concise
        \item \textbf{Answer:} \texttt{2}
    \end{itemize}
    Each row corresponds to the edit distance matrix, and the final answer is the edit distance.
    
    \item \textbf{Longest Increasing Subsequence (LIS)}. This task identifies the length of longest strictly increasing subsequence in a numerical sequence.
    % This task identifies the longest increasing subsequence (LIS) in a numerical sequence. 
    The input is a sequence of integers followed by a delimiter \texttt{<sep>}:
    \begin{itemize}
        \item \textbf{Input:} \texttt{103 110 145 217 233 18 30 82 141 150 159 161 167 239 <sep>}
        \item \textbf{CoT Steps:}
        \begin{verbatim}
        1 2 3 4 5 1 2 3 4 5 <sep>
        6 7 8 9 9 9 9 9 9 9 <sep>
        \end{verbatim}
        \item \textbf{Answer:} \texttt{9}
    \end{itemize}
    Here, each CoT step represents an intermediate computation in the dynamic programming process, folded into fixed-size groups (10 numbers per step in our setting) to align with the model structure. If the last group has fewer than 10 numbers, the last number is repeated until the group size reaches 10.
\end{enumerate}


\section{Hit Matrix for LIS Task with Length 101}\label{appendix:hit_matrix_101}
We also analyze the hit accuracy matrix for the LIS task with a problem length of 101, corresponding to $T = \lceil 101 / 10 \rceil = 11$ reasoning steps, as shown in Figure~\ref{fig:hit_mat_101}. The results exhibit a similar trend to those observed for length 105, with a notable decline in accuracy in the later reasoning steps for data generated by the auto-regressive CoT model, while RELAY-generated data consistently maintains high accuracy. Specifically, while the initial steps maintain relatively high token accuracy, the accuracy deteriorates significantly in later steps, failing to achieve accurate final answers. This highlights a key limitation of using CoT self-generated data for supervision: even when the problem length is only slightly beyond the training length, the CoT model struggles to generate accurate reasoning steps towards the end, making it infeasible to fine-tune the model using only the final answer as supervision, as the lack of intermediate reasoning accuracy prevents meaningful improvements in model's performance of handling longer problems.


\begin{figure*}[t]
    \centering
    \includegraphics[width=.8\textwidth]{figures/all_matrices_uniform_bar_101.pdf}
    \vspace{-8pt}
    % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
    \caption{Hit accuracy matrices for the LIS task with a problem length of 101 ($T = 11$ steps).} 
    \label{fig:hit_mat_101}
\end{figure*}


\section{Bit Accuracy}\label{appendix:bit_acc_results}

The bit accuracy results for the models fine-tuned with different datasets across the three tasks (Arithmetic, ED, LIS) and varying problem lengths are shown in Figure~\ref{fig:bit_acc}. Each subfigure corresponds to one task and includes curves showing the bit accuracy of five models over varying problem lengths: (1) \textbf{RELAY-enhanced CoT:} the auto-regressive CoT model fine-tuned with data generated by RELAY, (2) \textbf{Looped with Explicit CoT Alignment}, (3) \textbf{AR-CoT + Self Chains
\& Loop Answers:} the auto-regressive CoT model fine-tuned with its self-generated data using looped model answers as labels, (4) \textbf{AR-CoT + Self Chains \& GT Answers:} the auto-regressive CoT model fine-tuned with its self-generated data using ground-truth answers as labels, and (5) \textbf{AR-CoT Baseline:} the baseline auto-regressive CoT model as a reference, also as the initial auto-regressive CoT model before fine-tuned.

As illustrated in Figure~\ref{fig:bit_acc}, both the looped model with CoT alignment and the auto-regressive CoT model fine-tuned with data generated by it consistently achieve high bit accuracy across all tasks and problem lengths. Notably, they maintain over 90\,\% bit accuracy even at lengths extending beyond the training data (up to $+10$ for Arithmetic and ED tasks, and $+20$ for the LIS task in our setting). This highlights not only the robustness and reliability of the looped model with CoT alignment in length extrapolation scenarios but also the effectiveness of its generated data in significantly enhancing the performance of the auto-regressive CoT model.

In contrast, the auto-regressive CoT model fine-tuned with its own self-generated data shows limited improvement over the baseline model. This is consistent across all tasks and highlights a critical limitation: the self-generated data often contain incorrect intermediate steps, even when the final results are correct. These inaccuracies hinder the model's ability to generalize and perform well on longer problem lengths, reinforcing the importance of reliable intermediate reasoning steps for effective fine-tuning.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.95\textwidth]{figures/bit_acc_length_curve.pdf}
    \vspace{-8pt}
    % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
    \caption{Bit accuracy over varying problem lengths for three tasks: Arithmetic, ED, and LIS.} 
    \label{fig:bit_acc}
    %  Each subfigure corresponds to one task and includes the performance of four models: the auto-regressive CoT model fine-tuned with data generated by the looped model with CoT alignment, the auto-regressive CoT model fine-tuned with its own self-generated data, the looped model with CoT alignment, and the baseline auto-regressive CoT model. The results demonstrate that both the looped model with CoT alignment and the auto-regressive CoT model fine-tuned with its data achieve consistently high bit accuracy, exceeding 90\,\% even on lengths beyond the training data, while fine-tuning with self-generated data offers only limited improvements.
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \begin{minipage}[t]{0.33\textwidth}
%         \centering
%         \includegraphics[height=0.11\textheight]{icml2025/figures/all_matrices_uniform_bar.pdf}
%         \vspace{-20pt}
%         % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%         \caption{Hit accuracy matrices for the LIS task with a problem length of 105 ($T = 11$ steps). The left matrix corresponds to data generated by the looped model with CoT alignment, which achieves consistently high token accuracy across all positions. In contrast, the right matrix shows data generated by the auto-regressive CoT model, where accuracy is high only in the early positions but declines steadily in later steps.} 
%         \label{fig:matrix}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.66\textwidth}
%         \centering
%         \includegraphics[height=0.11\textheight]{icml2025/figures/bit_acc_length_curve.pdf}
%         \vspace{-8pt}
%         % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
%         \caption{Bit accuracy curves over varying problem lengths for three tasks: Arithmetic, ED, and LIS. Each subfigure corresponds to one task and includes the performance of four models: the auto-regressive CoT model fine-tuned with data generated by the looped model with CoT alignment, the auto-regressive CoT model fine-tuned with its own self-generated data, the looped model with CoT alignment, and the baseline auto-regressive CoT model. The results demonstrate that both the looped model with CoT alignment and the auto-regressive CoT model fine-tuned with its data achieve consistently high bit accuracy, exceeding 90\,\% even on lengths beyond the training data, while fine-tuning with self-generated data offers only limited improvements.\qifan{TODO: bit acc here, change fig here}} 
%         \label{fig:bit_acc}
%     \end{minipage}
% \end{figure*}


\section{Details for training and fine-tuning}\label{appendix:details_train_finetune}
\subsection{Hyper-parameters}\label{appendix:hyperparameters}

In our experiments, we trained three different models: (1) the looped model with CoT alignment, (2) the auto-regressive CoT model, and (3) the vanilla looped model. All models were trained from scratch on the same dataset, which consists of 1 million samples for each of the three tasks. The task-specific training weights and training hyper-parameters are provided in Table~\ref{tab:training_hyperparams}. 

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Training Hyper-parameters of Different Models}
  \vskip 0.15in
    \begin{tabular}{cccc}
    \toprule
    Training Hyper-parameters & Looped Model with CoT Alignment & CoT Model & Vanilla Looped Model \\
    \midrule
    Epoch & 500   & 500   & 500 \\
    Batch Size & 512   & 512   & 512 \\
    Learning Rate & 5e-4 & 5e-4 & 1e-3 \\
    Learning Rate Schedule & linear & linear & linear \\
    Warmup Ratio & 0.01  & 0.01  & 0.01 \\
    Optimizer & AdamW & AdamW & AdamW \\
    Weight Decay & 0.01  & 0.01  & 0.01 \\
    Drop out & 0.1   & 0.1   & 0.1 \\
    Weight of ARI & 1     & 1     & 1 \\
    Weight of ED & 1     & 10    & 10 \\
    Weight of LIS & 1     & 5     & 5 \\
    \bottomrule
    \end{tabular}%
  \label{tab:training_hyperparams}%
\end{table}%

For fine-tuning, we used data generated by RELAY and CoT model self-generated data to fine-tune the auto-regressive CoT model. The fine-tuning process followed a similar setup, as detailed in Table~\ref{tab:finetune_hyperparams}. 

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Fine-tuning Hyper-parameters}
  \vskip 0.15in
    \begin{tabular}{ccc}
    \toprule
    Fine-tuning Hyper-parameters & RELAY-Generated Data & Self-Generated Data \\
    \midrule
    Epoch & 500 & 100 (per phase) \\
    Batch Size & 512 & 512 \\
    Learning Rate & 1e-4 & 5e-5 \\
    Learning Rate Schedule & linear & linear \\
    Warmup Ratio & 0.01 & 0.01 \\
    Optimizer & AdamW & AdamW \\
    Weight Decay & 0.01 & 0.01 \\
    Drop out & 0.1 & 0.1 \\
    Weight of ARI & 1 & 1 \\
    Weight of ED & 10 & 1 \\
    Weight of LIS & 5 & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab:finetune_hyperparams}%
\end{table}%


\subsection{Sample Length Distribution of Datasets}\label{appendix:samples_dist_dataset}

% The auto-regressive CoT models are enhanced by a synthetic dataset generated by the looped model. This dataset contains both original training data and generated trajectories on longer problems. In our experiments, the trajectories on longer problems are constructed in two approaches: (1) reasoning steps generated by the looped model with CoT alignment, (2) reasoning steps self-generated by the CoT model, filtered with results given by a vanilla looped model. 

% For the dataset generated by the looped model with CoT alignment, we construct it by sampling and collecting data subsets from the original training dataset and trajectories generated by the loop model on the problem of extended lengths. The trajectories of Arithmetic are collected from length $[16, 25]$, Editing Distance from length $[31, 40]$, and Longest Increasing Subsequence from $[101, 120]$. The final dataset contains 100000 trajectories on each task, and the number of data of different lengths correlates with a sampling weight. Denote $w_{l, t}$ as the sampling weight of data of length $l$ in task $t$. For Arithmetic, $w_{l, ARI}$ is equal to $l$ when $l \le 18$, and $35 - l$ when $l\in [19, 25]$. For Editing Distance, $w_{l, ED}$ is equal to $l$ when $l\le 30$ and $60 - l$ when $l\in [31, 40]$. For Longest Increasing Subsequence, $w_{l, LIS}$ is equal to $l$ when $l\le 100$ and $200 - l$ when $l\in [101, 120]$.

The original training dataset for training the three models consists of 1 million samples for each task, following a distribution where the number of samples is proportional to problem length.

The merged dataset used for fine-tuning consists of 100\,k samples for each of the three tasks, incorporating both the original training data and newly generated samples from extended problem lengths. 

For the looped model with explicit CoT alignment, we introduce additional data covering problem lengths of $[16, 25]$ for Arithmetic, $[31, 40]$ for ED, and $[101, 120]$ for LIS. These newly generated samples are merged with the original dataset while maintaining a balanced proportion across different length ranges to ensure effective training. 
The specific numbers of samples for different problem lengths in the final merged dataset are provided in Table~\ref{tab:sample_dist_RELAY}.
% The specific proportions of different problem lengths and their corresponding sample counts in the final merged dataset are provided in Table~\ref{tab:sample_dist_RELAY}.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \vspace{-8pt}
  \caption{Number of Samples for Different Problem Lengths in Merged Dataset}
  \vskip 0.15in
    \begin{tabular}{cccc}
    \toprule
    Task & Arithmetic & ED    & LIS \\
    \midrule
    Length & $\leq 15$ & $\leq 30$ & $\leq 100$ \\
    Number of Samples & 42515 & 60844 & 73235 \\
    \midrule
    Length & 16    & 31    & 101 \\
    Number of Samples & 6477  & 4195  & 1479 \\
    \midrule
    Length & 17    & 32    & 102 \\
    Number of Samples & 6882  & 4195  & 1464 \\
    \midrule
    Length & 18    & 33    & 103 \\
    Number of Samples & 7287  & 4055  & 1449 \\
    \midrule
    Length & 19    & 34    & 104 \\
    Number of Samples & 6477  & 4055  & 1434 \\
    \midrule
    Length & 20    & 35    & 105 \\
    Number of Samples & 6072  & 3916  & 1420 \\
    \midrule
    Length & 21    & 36    & 106 \\
    Number of Samples & 5668  & 3916  & 1405 \\
    \midrule
    Length & 22    & 37    & 107 \\
    Number of Samples & 5263  & 3776  & 1390 \\
    \midrule
    Length & 23    & 38    & 108 \\
    Number of Samples & 4858  & 3776  & 1375 \\
    \midrule
    Length & 24    & 39    & 109 \\
    Number of Samples & 4453  & 3636  & 1360 \\
    \midrule
    Length & 25    & 40    & 110 \\
    Number of Samples & 4048  & 3636  & 1346 \\
    \midrule
    Length &       &       & 111 \\
    Number of Samples &       &       & 1331 \\
    \midrule
    Length &       &       & 112 \\
    Number of Samples &       &       & 1316 \\
    \midrule
    Length &       &       & 113 \\
    Number of Samples &       &       & 1301 \\
    \midrule
    Length &       &       & 114 \\
    Number of Samples &       &       & 1286 \\
    \midrule
    Length &       &       & 115 \\
    Number of Samples &       &       & 1272 \\
    \midrule
    Length &       &       & 116 \\
    Number of Samples &       &       & 1257 \\
    \midrule
    Length &       &       & 117 \\
    Number of Samples &       &       & 1242 \\
    \midrule
    Length &       &       & 118 \\
    Number of Samples &       &       & 1227 \\
    \midrule
    Length &       &       & 119 \\
    Number of Samples &       &       & 1213 \\
    \midrule
    Length &       &       & 120 \\
    Number of Samples &       &       & 1198 \\
    \bottomrule
    \end{tabular}%
  \label{tab:sample_dist_RELAY}%
\end{table}%


For the self-generated dataset, we adopt an incremental approach, since the accuracy of original CoT model diminishes rapidly as the problem length increases. Specifically, we maintain a total dataset size of 100\,k samples for each task. The initial dataset consists of problems with lengths $\leq 15, 30,$ and 100 for Arithmetic, ED, and LIS, respectively. The CoT model is progressively fine-tuned over five phases, each including self-generation on slightly longer problems and followed by 100 epochs of fine-tuning. After each phase, a subset of the current dataset is randomly combined with the newly generated reasoning steps to form an updated synthetic dataset. The maximum number of samples selected for each problem length is detailed in Table~\ref{tab:selfgeneratedata}. 

% \vspace{-17px}
\begin{table}[htbp]
  \centering
  \caption{Number of Samples Generated for Different Lengths}
  % \vspace{-10px}        % 这不能调整, 是example paper的要求
  \vskip 0.15in
    \begin{tabular}{ccccc}
    \toprule
    \multicolumn{2}{c}{Properties} & ARI & ED & LIS \\
    \midrule
    \multirow{2}*{Phase I} & Length & 16 & 31 & 101\\
    ~ & Data Generated & 15000 & 15000 & 15000 \\
    \midrule
    \multirow{2}*{Phase II} & Length & 17 & 32 & 102\\
    ~ & Data Generated & 10000 & 10000 & 10000 \\
    \midrule
    \multirow{2}*{Phase III} & Length & 18 & 33 & 103\\
    ~ & Data Generated & 7500 & 7500 & 7500 \\
    \midrule
    \multirow{2}*{Phase IV} & Length & 19 & 34 & 104\\
    ~ & Data Generated & 6000 & 3000 & 3000 \\
    \midrule
    \multirow{2}*{Phase V} & Length & 20 & 35 & 105\\
    ~ & Data Generated & 5000 & 3000 & 3000 \\
    \bottomrule
    \end{tabular}%
  \label{tab:selfgeneratedata}%
\end{table}%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
