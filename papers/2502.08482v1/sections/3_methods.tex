
\section{Methodology}

\subsection{Notation}
Any reasoning task can be decomposed into three components: the problem tokens, the reasoning tokens (i.e., chain-of-thought steps), and the answer tokens. Let the problem token sequence be represented as \( \bm{x} = [x_1, x_2, \ldots, x_n] \). The Chain-of-Thought (CoT) process generates a sequence of intermediate reasoning tokens \( \bm{z} = [z_1, z_2, \ldots, z_m] \), where \( n \) and \( m \) denote the number of problem tokens and reasoning tokens respectively. In this work, we focus on a simple setting where the problem's answer is represented by a single token, denoted as \( y \). 

\textbf{CoT Auto-regressive Generation. } For auto-regressive generation, the mapping from the problem sequence \( \bm{x} \) to the answer \( y \) is performed through generating the intermediate tokens \( \bm{z} \) token by token. Formally, this can be expressed as:
\begin{align}
z_i \sim P(z_i|\bm{z_{<i}}, \bm{x}; \theta), \quad \text{for } i = 1, 2, \ldots, m,
\end{align}
where $\bm{z_{<i}}=[z_1, z_2, \ldots, z_{i-1}]$ represents precedent reasoning tokens. and the final answer is obtained at the final step:
\begin{align}
y \sim P(y|\bm{z},\bm{x};\theta).
\end{align}


\textbf{Looped Model. } Different from the auto-regressive model that generates explicit tokens to obtain the answer, the looped model implicitly maps the input sequence \( \bm{x} \) to the final answer \( y \) by executing the same function (e.g., a multi-layer Transformer block) for \( T \) times in the representation space. The number of iterations \( T \) depends on the problem comlexity. The forward process consists of three steps:
First, the token sequence \(\bm{x}\) is mapped to embeddings through an embedding function \(h\):
\begin{align}
\bm{e}_0 = h(\bm{x}; \theta_{\text{emb}}),
\end{align}
where \(\bm{e}_0 \in \mathbb{R}^{d\times n}\) and \(d\) is the hidden dimension. Second, the embeddings are iteratively refined through  transformation \(f\):
\begin{align}
\bm{e}_t = f(\bm{e}_{t-1}; \theta_{\text{model}}), \quad \text{for } t = 1, 2, \ldots, T,
\end{align}
where $f$ is usually a Transformer model and the number of iterations \(T\) is adaptively determined based on the problem length. Finally, the answer is predicted through a final-answer prediction head based on the representations in the last layer:
\begin{align}
y \sim P(y|\bm{e}_T; \theta_{\text{pred}}).
\end{align}
This design enables the model to perform implicit reasoning through iterative refinement in the representation space, where each iteration can automatically capture different aspects of the reasoning process. 

As a comparison, the CoT auto-regressive generation derives the final output \( y \) by first generating a sequence of intermediate reasoning tokens \( \bm{z} = [z_1, z_2, \ldots, z_m] \) in an auto-regressive manner, where the length $m$ can grow polynomially with input length \(n\) (i.e., \(m \sim \text{poly}(n)\)). This variable and potentially large \(m\) poses challenges for positional encoding to correctly reflect attention relationships, leading to low accuracy for long sequence reasoning. In contrast, the looped model takes a fundamentally different approach. It directly processes the input sequence \(\bm{x}\) and produces output \(y\). The network only needs to handle \(\bm{x}\) without \( \bm{z}\), mitigating the long-length problem in the reasoning chain.

\subsection{Length Generalization on Single Reasoning Task}\label{sec:single_task}
Before introducing our RELAY framework, we first empirically demonstrate the superior length generalization capability of looped Transformers compared to standard auto-regressive models. This  analysis serves as the foundation and motivation for our proposed framework. 


\begin{figure*}[htbp]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=.9\textwidth]{figures/single_tasks_acc_length_curve.pdf}
    \vspace{-18pt}
    % \includesvg[inkscapelatex=false, width=0.7\linewidth]{loop}
    \caption{Length generalization performance of looped Transformer versus auto-regressive CoT model on Arithmetic (train: $\leq15$, test: $[15, 25]$), Edit Distance (train: $\leq30$, test: $[30, 40]$), and Longest Increasing Subsequence (train: $\leq100$, test: $[100, 120]$). } 
    % \vspace{-10pt}
    \label{fig:single_tasks}
\end{figure*}
% on Arithmetic (train: $\leq15$, test: $[15, 25]$), Edit Distance (train: $\leq30$, test: $[30, 40]$), and Longest Increasing Subsequence (train: $\leq100$, test: $[100, 120]$). Each subfigure corresponds to one task, presenting the models' accuracy of final answers across problem lengths within and beyond the training distribution. While both models achieve perfect accuracy within training lengths, the looped model consistently outperforms the CoT model on longer test lengths, demonstrating its superior length generalization capability.

\textbf{Task Descriptions.} To validate the capabilities of different methods, we use three representative tasks adapted from~\citet{feng2024towards}, including Arithmetic, a mathematical task, and two dynamic programming (DP) problems: Edit Distance (ED) and Longest Increasing Subsequence (LIS). These tasks are selected for their diverse problem-solving patterns and varying levels of complexity, and the fact that they can be solved through a Chain-of-Thought reasoning process to arrive at the final answer. Performance is evaluated based on the accuracy of the final answer for both models. Detailed descriptions of these tasks are provided in Appendix~\ref{appendix:task_descriptions}.

\textbf{Experimental Setup.}
For each task, we construct a dataset consisting of 1 million training samples and 100\,k test samples, respectively. For the Arithmetic task, the problem complexity is defined as the number of operators. For the Edit Distance (ED) task, the problem complexity corresponds to the length of the shorter string in each pair. For the Longest Increasing Subsequence (LIS) task, we define the problem complexity as $\lceil n / 10 \rceil$, where $n$ is the length of the input sequence, as our dataset is structured with 10 numbers per reasoning step (see Appendix~\ref{appendix:task_descriptions} for details). The training datasets are constructed with the length of the problem token sequence  \(\bm{x}\) $\leq$ 15, 30, and 100 for Arithmetic, ED, and LIS, respectively. To evaluate the model's generalization capabilities, test datasets are created with problem lengths in the ranges $[15, 25]$ for Arithmetic, $[30, 40]$ for ED, and $[100, 120]$ for LIS. 

% \qifan{TODO: if ``dynamic iteration control in the looped model, setting the number of loop iterations equal to the problem complexity'', then LIS complexity would be problem length // 10.}

% \qifan{TODO: change to real number, also experiment part}

For the auto-regressive CoT model, we employ a standard decoder-only Transformer language model. For the looped model, we use an encoder-only Transformer with bi-directional attention. To address varying problem complexities, we implement dynamic iteration control in the looped model, setting the number of loop iterations equal to the problem complexity. The architectural configuration remains consistent across all models, comprising 3 layers, 256-dimensional hidden states, and 4 attention heads. For positional encoding, we adopt RoPE~\cite{su2024rope} across all model variants to enhance sequence encoding for both training and test cases.

\textbf{Results.}
Figure~\ref{fig:single_tasks} illustrates the comparative performance of both models. Within the training distribution (e.g. Arithmetic: $\leq15$ operators), both the looped Transformer and the auto-regressive CoT Transformer achieve perfect accuracy. However, the models exhibit markedly different behaviors when tested on problems exceeding the training length: While the auto-regressive CoT Transformer's performance deteriorates significantly, the looped Transformer maintains superior performance across all length regimes. This demonstrates the length generalization capabilities of the looped Transformer.

While looped Transformers exhibit superior performance in final answer prediction, they lack interpretability in their intermediate computational processes. Moreover, their design philosophy may struggle with general language tasks, as determining the number of loop iterations becomes challenging beyond reasoning problems. This work seeks to harness the accurate reasoning predictions of the looped model to guide the training of auto-regressive Chain-of-Thought (CoT) models to better handle long-sequence reasoning.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/align.pdf}
% \includegraphics[scale=1.0]{figures/loop.pdf}
    % \includesvg[inkscapelatex=false, width=1.0\linewidth]{align}
    \caption{Overview of the RELAY framework. \textbf{Stage I (left):} Training looped model with explicit CoT alignment, where each iteration of the looped model learns to predict corresponding Chain-of-Thought (CoT) steps. \textbf{Stage II (right):} Using the trained looped model to generate CoT chains for enhancing auto-regressive CoT models. The looped model generates high-quality CoT chains for complex problems (beyond training length), which are then used to fine-tune the auto-regressive model to improve its reasoning capabilities.}
    \label{fig:loop_align_cot}
    % \vspace{-5pt}
\end{figure*}
% Overview of RELAY. \textbf{Left:} A looped Transformer model with explicit chain-of-thought (CoT) alignment. Each iteration generates an intermediate reasoning step through Transformer blocks with shared parameters. \textbf{Right:} The auto-regressive CoT generation model enhanced with intermediate reasoning steps produced by the looped model.

\subsection{Loop-Enhanced Chain-of-Thought Reasoning}

A straightforward way to leverage a well-trained looped model to enhance the auto-regressive CoT model is by using it as a verifier. When a problem is presented, both models generate a final answer, and if both answers match, the CoT output is trusted. However, this approach often fails in practice, as CoT models can produce incorrect reasoning trajectories even when reaching the correct final answer (see Section~\ref{ssec:relay_step_reliability}), making it unreliable to rely solely on the accuracy of the final answer as the guiding signal.

% Our key insight is that an alignment can be established between the iterative structure of looped Transformers and the stepwise nature of CoT reasoning. Specifically, the number of loop iterations often corresponds naturally to the number of reasoning rounds required in the CoT process, opening up the possibility of establishing a direct mapping between loop iterations and reasoning steps. With this insight, we propose \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), a two-stage framework that bridges looped and auto-regressive models.

% \qifan{TODO: stepwise or step-wise? match with abs}

Our key insight is that an alignment can be established between the iterative structure of the looped Transformer and the stepwise nature of CoT reasoning. As shown in Figure~\ref{fig:loop_align_cot}, unlike the step-by-step token generation in CoT, looped models update their representations simultaneously in each iteration, and the number of such iterations naturally corresponds to the number of reasoning rounds. This structural similarity opens up the possibility of training the looped model to generate the corresponding CoT tokens for each round in parallel, while maintaining its ability to predict the final answer. With this insight, we propose \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), a two-stage framework that bridges looped and auto-regressive models.


\textbf{Stage I: Training Looped Model with Explicit CoT Alignment.}
In the first stage, we train the looped model to generate intermediate reasoning processes that align with CoT steps. To formalize this alignment, assume we have a reasoning chain with $T$ rounds. Given reasoning tokens $\bm{z}=[z_1, z_2, \ldots, z_m]$, denote $k_t$ as the start token position of $t$-th reasoning round, where each round contains valid reasoning tokens $\bm{z}_{[k_t:k_{t+1}-1]}=[z_{k_t}, z_{k_{t+1}}, \ldots, z_{k_{t+1}-1}]$.  Taking arithmetic reasoning as an example, consider a sequence of tokens representing the complete reasoning chain, ``$3\times 2 + 6 \div 3 = 6 + 6 \div 3 = 6 + 2 = 8$''. This sequence can be naturally divided into $T=3$ rounds using the equal signs as delimiters: Given the input problem ``$3\times 2 + 6 \div 3 =$'', the first round corresponds to ``$6 + 6 \div 3 = $'', the second round corresponds to ``$6 + 2 = $'', and the third (last) round presents the final answer ``$8$''. 

% Although the number of rounds aligns with the iteration count of the looped model, a mismatch in token lengths exists across different reasoning steps. For instance, in arithmetic reasoning, early steps involving complex expressions (``$15\div5$'') typically occupy more positions than later steps (e.g., ``$15\div5 = 3$''). This variable length nature poses a challenge for the looped model's fixed-length processing mechanism.

% While these reasoning steps naturally vary in length, the looped model maintains fixed-length representations of size $n$ across iterations. This creates an alignment challenge between variable-length reasoning steps and fixed-length model representations.
Although the number of rounds aligns with the iteration count of the looped model, a key challenge arises from the mismatch in token lengths across different reasoning steps. For instance, earlier steps involving complex expressions (e.g., ``$6 + 6 \div 3 = $'') typically require more tokens than later steps (e.g., ``$6 + 2 = $''). This variable length nature poses a challenge for the looped model, which requires fixed-length representations of size $n$ across iterations.

To address this length mismatch while preserving the parallel processing capability of the looped model, we employ a right-aligned padding strategy. For the $t$-th iteration, we construct a fixed-length sequence $\tilde{\bm{z}}_t$ of length $n$ by right-aligning the ground truth reasoning tokens $\bm{z}_{[k_t:k_{t+1}-1]}$ and filling the remaining left positions with \texttt{<pad>} tokens. The fixed-length is determined based on the maximum length among all reasoning rounds and the original input problem (note that the length of a reasoning round usually does not exceed the length of the input problem; otherwise, each round can be further divided into shorter rounds). 
To track both valid reasoning tokens and the boundary of padding, we introduce a binary mask:
\begin{equation}
M_t[i] = \begin{cases}
1, & \text{if } i = p_t \text{ or } \tilde{\bm{z}}_t[i] \neq \text{\texttt{<pad>}}, \\
0, & \text{otherwise},
\end{cases}
\end{equation}
where $M_t$ indicates the positions of valid reasoning tokens and the position of the last \texttt{<pad>} token $p_t$.

Using this alignment strategy, we train the looped model to predict the corresponding CoT tokens at each iteration, enabling it to generate CoT-aligned intermediate outputs. In detail, at each iteration $t$, we train the model to predict both the valid reasoning tokens and the last \texttt{<pad>} token through an intermediate prediction head:
\begin{equation}
P(\tilde{\bm{z}}_t|\bm e_t; \theta_{\text{pred-cot}}),
\end{equation}
For the intermediate reasoning steps, we ignore all preceding \texttt{<pad>} tokens except the last one, as they have no impact on the reasoning process. The loss of this part can be formulated as :
\begin{equation}
% \begin{aligned}
% \mathcal{L}_{\text{step}} = \frac{1}{T}\sum_{t=1}^T \text{CrossEntropy}(&P(\tilde{\bm{z}}_t|e_t), \\
% &\tilde{\bm{z}}_t) \odot M_t,
% \end{aligned}
\mathcal{L}_{\text{iter}} = \frac{1}{T}\sum_{t=1}^T \text{CrossEntropy}(P(\tilde{\bm{z}}_t|\bm e_t;\theta_{\text{pred-cot}}), \tilde{\bm{z}}_t) \odot M_t,
\end{equation}
where the element-wise multiplication $\odot$ ensures that the loss is computed only on valid reasoning tokens and the last \texttt{<pad>} token.

For the final answer, we have the answer prediction loss to ensure correct final predictions:
\begin{equation}
\mathcal{L}_{\text{ans}} = \text{CrossEntropy}(P(\bm{y}|\bm{e}_T;\theta_{\text{pred}}), \bm{y}),
\end{equation}
where $\bm{y}$ is the ground truth answer.
The total training loss is then:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{ans}} + \lambda\mathcal{L}_{\text{iter}},
\end{equation}
where $\lambda$ is a hyperparameter balancing the two objectives.

This design enables the looped model to accurately predict the answer and provide interpretable intermediate reasoning steps that can be effectively utilized to guide the auto-regressive model in Stage II.

\paragraph{Stage II: Enhancing Auto-regressive CoT Models.}
In the second stage, we leverage the trained looped model to enhance auto-regressive CoT models through a systematic process:

First, we use the trained looped model in Stage I to generate reasoning demonstrations for problems of increasing complexity. For each problem $x$, we obtain:
\begin{equation}
(\bm{z}, y) \sim p(\cdot|\bm{x};\theta_L),
\end{equation}
where $\theta_L$ denotes the trained looped model from Stage I, $\bm{z} = [z_1, z_2, \ldots, z_m]$ represents the generated reasoning tokens across iterations, and $y$ is the predicted answer.

% \qifan{TODO: modify below, we actually generate all data and then fine-tune by once, curriculum learning is used in CoT SFT setting.}

% We then fine-tune the auto-regressive model using these demonstrations through a curriculum learning strategy. Starting with problems slightly beyond the original training length, we progressively increase problem length while maintaining a balanced mix of problems from shorter length levels.

We then utilize these demonstrations to fine-tune an auto-regressive model. For problem lengths beyond the original training range, we generate a comprehensive dataset of reasoning demonstrations using the looped model. This newly generated data is then merged with the original training dataset, which contains problems within the initial training length. The combined dataset, spanning both the original and extended problem lengths, is then used to fine-tune the auto-regressive model in a single step. This approach allows the model to retain its original reasoning capabilities while acquiring the ability to effectively tackle more complex, longer problems, utilizing the structured insights provided by the demonstrations.

\textbf{Comparison with Synthetic Data Generation Approaches. }
To effectively guide the LLMs to handle complex problems, prior works~\cite{hendrycks2021measuring,lightman2024lets} have explored the synthetic data generation approach, where human labelers construct data generation pipelines based on their understanding of both the task and its solution process. This approach requires labelers to possess comprehensive knowledge in three aspects: (1) problem construction, (2) problem-solving strategies, and (3) pipeline development skills. While effective, this creates a high barrier for deployment across diverse domains, as finding experts who excel in all three areas can be challenging.

In contrast, RELAY  reduces these requirements. Our approach follows a more automated pipeline: training data → looped model with strong generalization capability → longer problem construction → automated reasoning generation → auto-regressive CoT model training. The human involvement is primarily limited to longer problem construction, eliminating the need for expertise in solution strategies and pipeline development. This reduction in human expertise requirements makes our method more practical and scalable across different domains. Additionally, by leveraging the looped models' inherent generalization capabilities rather than manually designed rules, our approach can potentially capture more nuanced reasoning patterns that might be overlooked in hand-crafted pipelines.
% The fine-tuning objective is:
% \begin{equation}
% \mathcal{L}_{\text{finetune}} = \sum_{i=1}^m \log P(z_i|z_{<i}, x; \theta) + \log P(y|z_{1:m}, x; \theta)
% \end{equation}
% The curriculum learning process gradually expands the model's reasoning capabilities by introducing problems with increasing lengths while maintaining reasoning quality. Starting with problems slightly beyond the original training length, we progressively increase problem length while maintaining a balanced mix of problems from shorter length levels.

% \paragraph{RELAY Model.} Building upon these insights, we propose \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), which introduces an explicit alignment mechanism between loop iterations and CoT reasoning. This design serves two purposes: (1) it enables the looped model to generate interpretable intermediate reasoning tokens while maintaining its superior length generalization abilities and (2) it provides high-quality token-wise supervision signals for training auto-regressive CoT models. The forward process of RELAY consists of three components:

% First, following the standard looped model architecture, we process the input through iterative refinement:
% \begin{align}
% e_0 &= h(\bm{x}; \theta_{\text{emb}}), \\
% \bm e_t &= f(\bm e_{t-1}; \theta_{\text{model}}), \quad \text{for } t = 1, 2, \ldots, T.
% \end{align}

% Second, and most crucially, we introduce an iteration-wise reasoning token generation mechanism. At each iteration t, the model maps the intermediate embedding to a specific segment of the reasoning chain:
% \begin{equation}
% \begin{aligned}
% [z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}] &\sim P(z_{[k_t:k_{t+1}-1]}|e_t; \theta_{\text{pred}}), \\
% &\text{for } t = 1, 2, \ldots, T,
% \end{aligned}
% \end{equation}

% where $[z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}]$ represents the reasoning tokens corresponding to iteration t. This alignment is achieved through an additional prediction head at each loop iteration, which projects the embedding state to the token vocabulary space. 

% Finally, based on the final embedding state, the model predicts the answer:
% \begin{equation}
% y \sim P(y|\bm e_T; \theta_{\text{pred}})
% \end{equation}

% To enable accurate identification of valid reasoning tokens during inference, we employ a specific token arrangement strategy. For each iteration's output sequence of fixed length $L$, valid reasoning tokens are right-aligned while padding tokens (denoted as \texttt{<pad>}) are placed on the left. More precisely, if the valid reasoning tokens for iteration $t$ have length $l_t$, then the first $L-l_t$ positions are filled with \texttt{<pad>} tokens, followed by the $l_t$ valid tokens. During training, the model learns to predict not only the valid tokens but also the position of the last \texttt{<pad>} token, which serves as a critical delimiter. This design ensures that during inference, we can reliably identify the boundary between \texttt{<pad>} tokens and valid tokens: any tokens following the last predicted \texttt{<pad>} token are considered as valid reasoning tokens in each iteration. This right-aligned padding strategy is formalized through a binary mask:
% \begin{equation}
% M_t = \mathbf{1}[z_{[k_t:k_{t+1}-1]} \neq \text{\texttt{<pad>}}] \cup \{p_t\},
% \end{equation}
% where $M_t$ indicates the positions of valid tokens and the position of the last \texttt{<pad>} token $p_t$.

% During training, beyond the loss for final answer prediction, we introduce an iteration-wise supervision mechanism that ensures each loop iteration's output aligns with the corresponding CoT reasoning tokens. This is implemented through an auxiliary loss:
% \begin{equation}
% \begin{aligned}
% \mathcal{L}_{\text{step}} = \frac{1}{T}\sum_{t=1}^T \text{CrossEntropy}(&P(z_{[k_t:k_{t+1}-1]}|e_t), \\
% &z_{[k_t:k_{t+1}-1]}^*) \odot M_t,
% \end{aligned}
% \end{equation}
% where $z_{[k_t:k_{t+1}-1]}^*$ represents the ground truth reasoning tokens for loop iteration $t$, and $\odot$ denotes element-wise multiplication.

% This structured alignment mechanism enables RELAY to combine the complementary strengths of both architectures: the interpretability of step-by-step reasoning from CoT and the robust length generalization from looped models. Through this design, RELAY not only bridges the gap between auto-regressive reasoning and looped iterative refinement, but also provides high-quality reasoning demonstrations with explicit token-wise supervision for training standard CoT models.


% \textbf{RELAY Model}\quad Building upon the looped model's architecture, our RELAY model introduces an additional alignment mechanism between intermediate embeddings and reasoning tokens. While following the same iterative refinement process as the looped model:
% \begin{align}
% % y &\sim P(y|\bm{x}, T;\theta), \\
% e_0 &= h(\bm{x}; \theta), \\
% \bm e_t &= \ell(\bm e_{t-1} + \bm e_0; \theta), \quad \text{for } t = 1, 2, \ldots, T, \\
% y &\sim P(y|\bm e_T; \theta),
% \end{align}

% The key distinction lies in RELAY's ability to generate reasoning tokens during the iterative process. At each iteration, the model maps the intermediate embedding to a segment of reasoning tokens:
% \begin{align}
% [z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}] \sim P(z_{[k_t:k_{t+1}-1]}|e_t; \theta),\\ 
% \text{for } t = 1, 2, \ldots, T, \nonumber
% \end{align}
% where \([z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}]\) represents a consecutive segment of reasoning tokens generated at iteration \(t\), with \(k_t\) and \(k_{t+1}-1\) denoting the starting and ending indices respectively.

% This structured alignment mechanism enables RELAY to combine the complementary strengths of both CoT and looped models: the efficacy of step-by-step reasoning and the efficiency of iterative refinement in the embedding space. 
% % Through this design, RELAY bridges the gap between generating interpretable intermediate steps and producing accurate final answers.
% Through this design, RELAY bridges the gap between auto-regressive reasoning and looped iterative refinement, enabling the generation of interpretable intermediate steps while maintaining efficiency and accuracy.


% \textbf{RELAY Model}\quad Our RELAY model builds upon the structure of the looped model but is designed to align each intermediate embedding with the corresponding reasoning tokens from the CoT model. Formally, the mapping can be expressed as:
% \begin{align}
% y \sim P(y|\bm{x}, T;\theta),
% \end{align}
% where the input \(\bm{x}\) is first mapped to an initial embedding \(e_0\) through an embedding function \(h\):
% \[
% e_0 = h(\bm{x}; \theta),
% \]
% the embedding is then iteratively refined in the embedding space by \( \text{RELAY}_{\ell} \) as:
% \begin{align}
% \bm e_t = \ell(\bm e_{t-1}; \theta) + \bm e_0, \quad \text{for } t = 1, 2, \ldots, T.
% \end{align}
% After \(T\) iterations, the final embedding \(e_T\) is transformed into the output \(y\) through an output layer:
% \begin{equation}
%     y \sim P(y|\bm e_T; \theta).
% \end{equation}

% Unlike the looped model, which directly produces the final answer \( y \), the RELAY model not only samples \( y \) based on the final embedding,
% but also establishes a structured alignment between each intermediate embedding \( e_t \) and a corresponding segment of reasoning tokens:

% \begin{align}
% [z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}] \sim P(e_t; \theta), \\
% \text{for } t = 1, 2, \ldots, T, \nonumber
% \end{align}

% where \([z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}]\) represents a consecutive segment of reasoning tokens generated at iteration \(t\), with \(k_t\) denoting the starting index and \(k_{t+1}-1\) the ending index of this segment.

% This alignment allows the RELAY model to generate both the complete sequence of intermediate reasoning tokens \( \bm{z} \) and the final answer \( y \), combining the interpretability of CoT with the efficient global reasoning of the looped model. The RELAY model thus represents a strengthened version of the looped model, bridging the gap between generating intermediate steps and producing accurate final answers by aligning its iterative process with the reasoning steps from the CoT model.

% At each iteration, the model generates a segment of reasoning tokens:
% \begin{align}
% [z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}] \sim P(z_{[k_t:k_{t+1}-1]}|e_t; \theta), \quad \text{for } t = 1, 2, \ldots, T,
% \end{align}
% where \([z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}]\) represents a consecutive segment of reasoning tokens generated at iteration \(t\), with \(k_t\) denoting the starting index and \(k_{t+1}-1\) the ending index of this segment.

% Unlike the looped model which directly produces the final answer, the RELAY model serves a dual purpose. First, it generates the complete sequence of intermediate reasoning tokens \(\bm{z}\) through the segment-wise generation process. Second, it samples the final answer conditioned on the terminal embedding:
% \begin{align}
% y \sim P(y|e_T; \theta).
% \end{align}

% This architectural design enables the RELAY model to combine the interpretability advantages of Chain-of-Thought reasoning with the computational efficiency of the looped model's global reasoning mechanism. By establishing a direct correspondence between its iterative computation process and structured reasoning tokens, RELAY bridges the gap between generating interpretable intermediate steps and producing accurate final answers.

% \textbf{RELAY Model}\quad Our RELAY model builds upon the structure of the looped model but is designed to align each intermediate embeddings with the corresponding reasoning steps from the CoT model. Formally, the mapping from the input sequence \( \bm{x} \) to the sequence of intermediate reasoning steps \( \bm{z} = [z_1, z_2, \ldots, z_m] \) and the final answer \( y \) can be expressed as:
% % \begin{align}
% % f_{\text{RELAY}}(\bm{x}) = (\bm{z}, y),
% % \end{align}
% \begin{align}
% y \sim P(y|\text{Relay}_{\ell}(\bm{x}, T;\theta)),
% \end{align}
% where the intermediate reasoning steps \( z_i \) are computed by projecting the embedding \( e_t \) at each iteration \( t \), such that:
% \begin{align}
% z_i = h(e_t; \theta), \quad \text{for } t = 1, 2, \ldots, T,
% \end{align}
% with \( e_0 \) initialized as the input embedding of \( \bm{x} \). The embeddings \( e_t \) are updated iteratively using:
% \begin{align}
% e_t = \ell(e_{t-1}, \bm{x}; \theta), \quad \text{for } t = 1, 2, \ldots, T.
% \end{align}
% Here, \( \ell \) represents a recurrent transformation parameterized by \( \theta \), and \( h \) denotes the output transformation used to project the embedding \( e_t \) into the space of reasoning steps.

% Unlike the looped model, which directly produces the final answer \( y \), the RELAY model not only computes \( y \) as:
% \begin{align}
% y = h(e_T; \theta),
% \end{align}
% but also establishes a structured alignment between each intermediate embedding \( e_t \) and a corresponding segment of reasoning tokens:

% \begin{align}
% [z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}] = h(e_t; \theta), \\
% \text{for } t = 1, 2, \ldots, T, \nonumber
% \end{align}

% where \([z_{k_t}, z_{k_t+1}, ..., z_{k_{t+1}-1}]\) represents a consecutive segment of reasoning tokens generated at iteration \(t\), with \(k_t\) denoting the starting index and \(k_{t+1}-1\) the ending index of this segment.

% This alignment allows the RELAY model to generate both the complete sequence of intermediate reasoning tokens \( \bm{z} \) and the final answer \( y \), combining the interpretability of CoT with the efficient global reasoning of the looped model. The RELAY model thus represents a strengthened version of the looped model, bridging the gap between generating intermediate steps and producing accurate final answers by aligning its iterative process with the reasoning steps from the CoT model.

% TODO belows

% \subsection{Problem Description}
% For long-sequence reasoning tasks such as arithmetic operations, shortest edit distance, and longest increasing subsequence, balancing between local and global information is crucial. Excessive reliance on \textbf{local information} often leads to \textbf{accumulation of errors} for long sequences, while focusing too much on \textbf{global information} deteriorates performance as the sequence length increases.

% We explore two types of models to address this issue:
% \begin{itemize}
%     \item \textbf{Loop Model}: Operates in the embedding space by iterating multiple times based on the problem's length to directly produce the final answer. It excels at length generalization but cannot output intermediate reasoning steps.
%     \item \textbf{Chain-of-Thought (CoT) Model}: Works in an auto-regressive manner to output intermediate reasoning steps and the final answer. However, its length generalization performance is poor.
% \end{itemize}

% Our goal is to improve the length generalization performance of the CoT model by leveraging the strengths of the Loop model.

% \subsection{Proposed Approach}
% To achieve this, we:
% \begin{enumerate}
%     \item Train a Loop model and a CoT model on \textbf{short sequences} with ground truth data.
%     \item For \textbf{longer sequences}, where ground truth data is unavailable:
%     \begin{itemize}
%         \item Use the \textbf{Loop model} to generate outputs from input data.
%         \item Use these outputs as pseudo-labels to fine-tune the CoT model.
%     \end{itemize}
% \end{enumerate}

% This strategy allows the Loop model's length generalization capability to guide the CoT model, improving its reasoning performance on long sequences.

% \subsection{Formalization}
% Let \( x \in \mathcal{X} \) denote the input data and \( y \in \mathcal{Y} \) the output data. The task is to learn a mapping \( f: \mathcal{X} \rightarrow \mathcal{Y} \), where the sequence length \( |x| \) may extend beyond the range of the training data.

% \subsubsection{Loop Model}
% The Loop model is defined as:
% \[
% f_{\text{Loop}}(x) = \text{Loop}_{\ell}(x, T),
% \]
% where:
% \begin{itemize}
%     \item \( T \) is the number of iterations, determined by the input length \( |x| \),
%     \item \( \text{Loop}_{\ell} \) operates in the embedding space and directly outputs \( y \).
% \end{itemize}

% This model focuses on \textbf{global features} and demonstrates effective generalization for longer sequences.

% \subsubsection{CoT Model}
% The CoT model generates intermediate reasoning steps \( z = (z_1, z_2, \ldots, z_m) \) and the final answer \( y \) in an auto-regressive manner:
% \[
% f_{\text{CoT}}(x) = (z, y), \quad \text{where } z_i = g(z_{i-1}, x; \theta).
% \]
% Here, \( g \) represents the auto-regressive process for generating tokens, and \( \theta \) denotes the parameters of a deep transformer model used in this process.

% \subsubsection{Joint Training Framework}
% \paragraph{Short Sequence Training}
% Both models are trained on short sequences with ground truth labels \( \{(x_i, y_i)\}_{i=1}^N \):
% \begin{itemize}
%     \item \textbf{Loop Model}: Minimize \( \mathcal{L}_{\text{loop}} \):
%     \[
%     \mathcal{L}_{\text{loop}} = \frac{1}{N} \sum_{i=1}^N \| f_{\text{loop}}(x_i) - y_i \|^2.
%     \]
%     \item \textbf{CoT Model}: Minimize \( \mathcal{L}_{\text{cot}} \):
%     \[
%     \mathcal{L}_{\text{cot}} = \frac{1}{N} \sum_{i=1}^N \| f_{\text{cot}}(x_i) - (z_i, y_i) \|^2.
%     \]
% \end{itemize}

% \paragraph{Long Sequence Fine-tuning}
% For longer sequences \( x_j \) (\( |x_j| > L_{\text{train}} \)), the Loop model generates pseudo-labels \( \tilde{y}_j = f_{\text{loop}}(x_j) \). The CoT model is fine-tuned to minimize:
% \[
% \mathcal{L}_{\text{fine}} = \frac{1}{M} \sum_{j=1}^M \| f_{\text{cot}}(x_j) - (\tilde{z}_j, \tilde{y}_j) \|^2,
% \]
% where \( \tilde{z}_j \) are the intermediate steps generated by the CoT model during training.

% \subsection{Benefits and Expected Outcomes}
% This method leverages the Loop model's superior length generalization to guide the CoT model, combining the strengths of both approaches:
% \begin{itemize}
%     \item \textbf{Improved Length generalization}: CoT model performance on long sequences is enhanced by learning from Loop model outputs.
%     \item \textbf{Preservation of Interpretability}: The CoT model retains its ability to output intermediate reasoning steps.
% \end{itemize}

