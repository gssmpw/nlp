\section{Conclusion}

This paper introduces \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), a framework enhancing Chain-of-Thought reasoning by combining looped and auto-regressive Transformers. Our contributions show that (1) a looped Transformer can serve as a general-purpose reasoner with strong length generalization, (2) iteration-wise alignment enables accurate reasoning chain generation beyond training length, and (3) RELAY improves auto-regressive models through high-quality generated reasoning chains. Future work could explore the theoretical foundations of looped Transformers' length generalization and extend RELAY to broader language tasks.

% This paper introduces \textbf{RELAY} (\underline{\textbf{RE}}asoning through \underline{\textbf{L}}oop \underline{\textbf{A}}lignment iterativel\underline{\textbf{Y}}), a novel framework that enhances Chain-of-Thought reasoning by leveraging the complementary strengths of looped and auto-regressive Transformers. Our key contributions demonstrate that (1) a single looped Transformer can effectively serve as a general-purpose reasoner across multiple tasks while maintaining strong length generalization, (2) iteration-wise alignment between loop outputs and reasoning steps enables accurate generation of reasoning chains beyond training length, and (3) the RELAY framework can significantly improve auto-regressive models' reasoning abilities through high-quality generated reasoning chains.

% While our results are promising, several directions remain for future exploration. First, investigating the theoretical foundations of looped Transformers' length generalization capabilities could provide insights for designing more robust reasoning architectures. Second, extending RELAY to more general language understanding and generation tasks could broaden its applicability and impact.

