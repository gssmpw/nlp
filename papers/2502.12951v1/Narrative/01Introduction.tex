\section{Introduction}

Lossy scientific data compression has emerged as a vitally important area in the past decade. The volume and velocity of scientific data heighten the urgency of the requirement of good data compression algorithms, specifically methods that can provide performance guarantees in terms of error bounds on the primary data (PD) of interest. The concomitant rise of machine learning has seen the flowering of different learning-based compression paradigms. The primary ones are super-resolution, transform-based, and more recently methods based on generative AI. We first examine these paradigms before turning to the relatively new approaches based on generative models---the paradigm adopted in the present work.

Lossy compression based on \textbf{super-resolution} \cite{Khani2021,Conde2022swin2sr} is based on the premise that the data of interest can be faithfully reconstructed from a small set of ``true'' samples. Machine learning methods based on this paradigm attempt data reconstruction (of the original tensor) from this sample set. \textbf{Transform-based} methods have traditionally been the most popular paradigm with discrete cosine transforms (DCT), wavelets, principal component analysis (PCA) and dictionary-based methods leading the way. More recently, autoencoders (AE) which transform the data into a compact and quantized latent space from which learned decoders reconstruct the original tensor have been the paradigm of choice among ML practitioners. However, these paradigms do not leverage recent advances in generative AI. In this newer approach---termed \textbf{conditional diffusion (CD)} \cite{Yang2023cd}---the original tensor is first gradually converted into zero mean, Gaussian noise. Then, a decoder is learned which gradually denoises the tensor through stages to finally produce a tensor approximately drawn from the probability distribution of the original images. A latent space embedding is used to guide the diffusion process. We propose to work within this paradigm but in the context of scientific data compression.

%\begin{figure}
    %\centering
    %\includegraphics[width=\textwidth]{Figures/Overview.pdf}
    %\caption{Overview of our conditional diffusion model for compression. The $\boldsymbol{x}_i$ denotes the $i^\mathrm{th}$ slice of a 3D block. We compress 3D blocks to capture spatiotemporal correlations in scientific datasets. The compressed (latent) codecs guide a 2D denoising diffusion process. Our diffusion model reconstructs each of the 2D slices in 3D blocks based on its corresponding latent data $\boldsymbol{z}_i$. This enables us to keep a relatively simple U-Net architecture while getting effective latent variables via 3D block compression.}\label{fig:overview}
%\end{figure}

\begin{wrapfigure}{r}{0.5\textwidth} 
\vspace{-0.4cm}
    \centering
    \includegraphics[width=\linewidth]{Figures/Overview.pdf}
    %\hspace{0.25cm}
  \caption{Overview of our conditional diffusion model for compression. We compress 3D blocks to capture spatiotemporal correlations in scientific datasets. The latent variables guide a 2D denoising diffusion process. Our denoising decoder reconstructs each of the 2D slices in 3D blocks based on its corresponding latent data $\boldsymbol{z}_i$. This enables us to keep a relatively simple U-Net architecture while getting effective conditioning via 3D block compression.}
    \label{fig:overview}
    \vspace{-0.4cm}
\end{wrapfigure}
 
Our approach to scientific data compression, situated within the conditional diffusion paradigm is now described. Figure~\ref{fig:overview} illustrates the overview of our proposed conditional diffusion models. Our model is a mixture of 3D block conditioning and 2D denoising diffusion. We first divide the original data into blocks of 3D tensors. 3D tensors are encoded into latent variables, which results in compressed codecs. We construct 3D latent embeddings using these codecs and they act as the conditioning information in CD. Unlike latent embeddings, we learn the denoising decoder in 2D space. Each 2D slice $\boldsymbol{x}_i$ of the 3D tensors is gradually converted into white noise in a stovepiped manner as described above. The denoising decoder estimates the noise of the 2D slice at the diffusion stage $t$. Using an embedding for the diffusion stage index, we learn the denoising decoder and the latent space embeddings in an end-to-end fashion. After training the machine, the original 3D tensor blocks are reconstructed with zero noise at the input (so that the decoder is entirely deterministic). The reconstructed primary data are examined to see if error bounds are violated and if so, we correct for the PD to be within pre-specified error bounds (using PCA or via a separate error bounding neural network). The main contributions are:
\begin{itemize}
    \item We propose a CD model for lossy scientific data compression. We divide the entire data into 3D tensors and map them into compressed codecs to capture spatiotemporal correlations in scientific datasets.
    \item The proposed CD model is a mixture of 3D conditioning and 2D diffusion. We prevent a complexity increase of our denoising decoder by avoiding a 3D diffusion process.
    \item To the best of our knowledge, this application of CD to scientific data compression with error guarantees---termed guaranteed conditional diffusion (GCD)---is a new contribution within a relatively new data compression paradigm.
\end{itemize}



