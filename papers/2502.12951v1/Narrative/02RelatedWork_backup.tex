\section{Related Work}

\subsection{Diffusion Models}

Diffusion models have emerged as a pivotal class of generative models in machine learning, demonstrating remarkable success across various domains, including image synthesis, natural language processing, and molecular design. This section outlines the foundational works and significant advancements in diffusion models, highlighting key contributions and their impact on the field.

The concept of diffusion models was introduced by Sohl-Dickstein et al.~\cite{Sohl2015}, who proposed a method for deep unsupervised learning using non-equilibrium thermodynamics. This work laid the groundwork for subsequent developments in diffusion-based generative modeling. Building upon this foundation, Song and Ermon~\cite{Song2019} introduced score-based generative modeling, which learns to estimate data gradients and uses them for iterative data generation. This work provided a foundation for noise-agnostic and flexible diffusion processes. Ho et al.~\cite{Ho2020} presented Denoising Diffusion Probabilistic Models (DDPMs), which significantly improved sample quality and training stability. Their approach involved modeling the data distribution through a forward diffusion process and learning to reverse this process to generate new data samples.

Subsequent research has focused on enhancing the efficiency and applicability of diffusion models. Song et al.~\cite{Song2021scorebased} unified diffusion probabilistic models and score-matching approaches through the development of score-based generative models via stochastic differential equations (SDEs). This formulation allowed for a broader understanding and improvement of diffusion processes. Nichol and Dhariwal~\cite{Nichol2021} introduced improved denoising diffusion probabilistic models, addressing limitations in sampling speed and quality. Their work proposed modifications to the noise schedule and model architecture, resulting in faster and more reliable sampling. Dhariwal and Nichol~\cite{Dhariwal2021} demonstrated the competitive performance of diffusion models against state-of-the-art generative adversarial networks (GANs) in high-fidelity image generation tasks. They further introduced classifier guidance for diffusion models to enhance conditional generation. Rombach et al.~\cite{Rombach2022} developed Latent Diffusion Models (LDMs), which operate in a compressed latent space rather than the high-dimensional data space. This approach significantly reduced computational requirements while maintaining high-quality generation, making diffusion models more practical for large-scale applications. More recently, Song et al.~\cite{Song2023} introduced consistency models, which enable efficient one-step sampling by ensuring consistency in generative modeling across forward and reverse processes.

\subsection{Diffusion Models for Compression}
Diffusion models have recently been explored for data compression tasks, leveraging their ability to model complex data distributions. These works demonstrate how diffusion-based generative models can be adapted to achieve state-of-the-art performance in lossy and lossless compression while maintaining high reconstruction quality.

Hoogeboom et al. \cite{Hoogeboom2022autoregressive} introduced Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models and absorbing discrete diffusion. ARDMs are simple to implement and easy to train, supporting parallel generation adaptable to any given generation budget. They require significantly fewer steps than discrete diffusion models to attain the same performance and are uniquely suited to lossless compression tasks. 

Yang and Mandt \cite{Yang2023cd} introduced an end-to-end optimized framework for lossy image compression using conditional diffusion models. Their approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and then reconstructed using a conditional diffusion model. This method introduces an additional ``content'' latent variable to store information about the image, while the remaining ``texture'' variables are synthesized during decoding. The model's performance can be tuned toward perceptual metrics of interest, yielding strong FID scores and competitive distortion metrics. Training the diffusion with $\mathcal{X}$-parameterization enables high-quality reconstructions in only a few decoding steps, enhancing practicality.

Li et al. \cite{Li2024extreme} proposed a novel two-stage extreme image compression framework that exploits the generative capability of pre-trained diffusion models to achieve realistic image reconstruction at extremely low bitrates. The first stage employs a VAE-based compression approach to compress images into content variables, while the second stage leverages pre-trained stable diffusion to reconstruct images under the guidance of these content variables. A small control module injects content information, and a space alignment loss ensures alignment with the diffusion space, providing necessary constraints for optimization. Extensive experiments demonstrate that this method significantly outperforms state-of-the-art approaches in terms of visual performance at extremely low bitrates.

Careil et al. \cite{Careil2024towards} proposed a method for image compression at ultra-low bitrates using iterative diffusion models. Their approach decodes images conditioned on a vector-quantized representation and a global image description, utilizing a two-branch architecture where one branch handles the global structure and the other focuses on local details. This combination allows the model to achieve high-quality reconstructions while preserving important textures and details. The method demonstrates state-of-the-art visual quality as measured by perceptual metrics such as FID and KID, showing that diffusion-based approaches can offer significant advantages over traditional compression techniques.

Relic et al. \cite{Relic2024lossy} formulated the removal of quantization error as a denoising task, using diffusion to recover lost informattted image latent. Their approach allows performing less than 10\% of the full diffusion generative process and requires no architectural changes to the diffusion model, enabling the use of foundation models as a strong prior without additional fine-tuning. The proposed codec outperforms previous methods in quantitative realism metrics, and reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.

Ma et al. \cite{ma2024correcting} presented a diffusion-based image compression method that employs a privileged end-to-end decoder model, achieving better perceptual quality while guaranteeing distortion to an extent. They built a diffusion model and designed a novel paradigm combining the diffusion model and an end-to-end decoder, with the latter responsible for transmitting privileged information extracted at the encoder side. Theoretical analysis of the reconstruction process of diffusion models at the encoder side with the original images being visible led to the introduction of an end-to-end convolutional decoder to provide a better approximation of the score function at the encoder side and effectively transmit the combination. Experiments demonstrate the superiority of this method in both distortion and perception compared with previous perceptual compression methods.

These studies collectively highlight the versatility and potential of diffusion models in advancing the field of image compression, offering ns for future research and application.
