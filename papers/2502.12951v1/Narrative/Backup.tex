\section{Methodology}

\subsection{Background}
\paragraph{\textbf{Denoising diffusion models}} are hierarchical latent variable models that generate data by a sequence of iterative stochastic denoising steps \cite{Sohl2015,Yang2019,Ho2020,Song2021}. These models describe a joint distribution over data $\boldsymbol{x}_0$ and latent variables $\boldsymbol{x}_{1:T}$ such that $p_\theta\left(\boldsymbol{x}_0\right)=\int p_\theta\left(\boldsymbol{x}_{1:T}\right)d\boldsymbol{x}_{1:T}$. While a diffusion process (denoted by $q$)  incrementally \textit{destroys} structure, its reverse process $p_\theta$ \textit{generates} structure. Both processes involve Markovian dynamics between a sequence of transitional steps (denoted by $t$), where
\begin{equation}\label{eq:forward}
    q\left(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}\right)=\mathcal{N}\left(\boldsymbol{x}_t|\sqrt{1-\beta_t}\boldsymbol{x}_{t-1},\beta_t\boldsymbol{\mathrm{I}}\right).
\end{equation}
\begin{equation}\label{eq:reverse}
    p_\theta\left(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1}|M_\theta\left(\boldsymbol{x}_t,t\right),\beta_t\boldsymbol{\mathrm{I}}\right).
\end{equation}
The variance schedule $\beta\in \left(0,1\right)$ can be either fixed or learned; besides it, the diffusion process is parameter-free. The denoising process predicts the posterior mean from the diffusion process and is parameterized by a neural network $M_\theta\left(\boldsymbol{x}_t,t\right)$.

Denoising Diffusion Probabilistic Model (DDPM) \cite{Ho2020} showed a tractable objective function for training the reverse process. A simplified version of their objective resulted in the following \textit{noise parameterization}, where one seeks to predict the noise $\epsilon$ used to perturb a particular image $\boldsymbol{x}_0$ from the noisy image $\boldsymbol{x}_t$ at noise level $t$:
\begin{equation}
    L\left(\theta,\boldsymbol{x}_0\right)=\mathbb{E}\left\|\epsilon-\epsilon_\theta\left(\boldsymbol{x}_t\left(\boldsymbol{x}_0\right)\right),t\right\|^2.
\end{equation}
Above, $t\sim\textrm{unif}\{1,T\}$, $\epsilon\sim \mathcal{N}\left(\boldsymbol{0},\boldsymbol{\mathrm{I}}\right)$, $\boldsymbol{x}_t\left(\boldsymbol{x}_0\right)=\sqrt{\alpha_t}\boldsymbol{x}+\sqrt{1-\alpha_t}\epsilon$, and $\alpha_t=\prod_{i=1}^t\left(1-\beta_i\right)$. At test time, data can be generated by ancestral sampling using Langevin dynamics. Alternatively, \cite{Song2021} proposed the Denoising Diffusion Implicit Model (DDIM) that follows a deterministic generation procedure after an initial stochastic draw from the prior. Our paper uses the DDIM scheme at test time.

\paragraph{\textbf{Diffusion models}} are hierarchical latent variable models that iteratively generate data through a sequence of stochastic denoising steps \cite{Sohl2015,Yang2019,Ho2020,Song2021}. These models define a joint distribution over the observed data $\boldsymbol{x}_0$ and a sequence of latent variables $\boldsymbol{x}_{1:T}$ such that $p_\theta\left(\boldsymbol{x}_0\right)=\int p_\theta\left(\boldsymbol{x}_{1:T}\right)d\boldsymbol{x}_{1:T}$. The generative process relies on reversing a forward diffusion process $q$, which progressively adds noise to the data until it becomes indistinguishable from Gaussian noise. This forward process is described by Markov transitions:
\begin{equation}\label{eq:forward}
    q\left(\boldsymbol{x}_t|\boldsymbol{x}_{t-1}\right)=\mathcal{N}\left(\boldsymbol{x}_t|\sqrt{1-\beta_t}\boldsymbol{x}_{t-1},\beta_t\boldsymbol{\mathrm{I}}\right),
\end{equation}
where $\beta_t$ is a variance schedule that can be either fixed or learned. The reverse process $p_\theta$ reconstructs structure from noise using a parameterized neural network:
\begin{equation}\label{eq:reverse}
    p_\theta\left(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1}|M_\theta\left(\boldsymbol{x}_t,t\right),\beta_t\boldsymbol{\mathrm{I}}\right).
\end{equation}
Here, $M_\theta\left(\boldsymbol{x}_t,t\right)$ predicts the posterior mean of the reverse process at each time step $t$.

The objective for training the reverse process is derived from a simplified variational lower bound \cite{Ho2020}, leading to the noise prediction formulation. The loss function minimizes the difference between the true noise $\epsilon$ and the predicted noise $\epsilon_\theta$:
\begin{equation}
    L\left(\theta,\boldsymbol{x}_0\right)=\mathbb{E}\left\|\epsilon-\epsilon_\theta\left(\boldsymbol{x}_t\left(\boldsymbol{x}_0\right),t\right)\right\|^2,
\end{equation}
where $\boldsymbol{x}_t$ is computed as $\boldsymbol{x}_t\left(\boldsymbol{x}_0\right)=\sqrt{\alpha_t}\boldsymbol{x}_0+\sqrt{1-\alpha_t}\epsilon$, $\alpha_t=\prod_{i=1}^t\left(1-\beta_i\right)$, and $\epsilon\sim \mathcal{N}\left(\boldsymbol{0},\boldsymbol{\mathrm{I}}\right)$. During generation, data is synthesized through ancestral sampling or deterministic procedures such as DDIM \cite{Song2021}, which enhances sampling efficiency.

\paragraph{\textbf{Conditional diffusion models}} incorporate diffusion models for compression. The basis of our compression approach is a new latent variable model: the diffusion variational autoencoder. This model has a ``semantic'' latent variable $\boldsymbol{z}$ for encoding the image content, and a set of ``texture'' variables $\boldsymbol{x}_{1:T}$ describing residual information,
\begin{equation}
    p\left(\boldsymbol{x}_{0:T},\boldsymbol{z}\right)=p\left(\boldsymbol{x}_{0:T}|\boldsymbol{z}\right)p\left(\boldsymbol{z}\right).
\end{equation}
As detailed below, the decoder will follow a denoising process conditioned on $\boldsymbol{z}$ Drawing on methods
 described in Section 3.1, we use a neural encoder $e\left(\boldsymbol{z}|\boldsymbol{x}_{0}\right)$ to encode the image. The prior $p\left(\boldsymbol{z}\right)$  is a
 two-level hierarchical prior (commonly used in learned image compression) and is used for entropy coding $\boldsymbol{z}$ after quantization. Next, we discuss the novel decoder model.

\paragraph{\textbf{ Decoder and training objective}} We construct the conditional denoising diffusion model in a similar way to the non-variational diffusion autoencoder of \cite{Preechakul2021,Wang2023info}. In analogy to Eq. \eqref{eq:forward,eq:reverse}, we introduce a conditional denoising diffusion process for decoding the latent $\boldsymbol{z}$,
\begin{equation}
    p_\theta\left(\boldsymbol{x}_{0:T}|\boldsymbol{z}\right)=p\left(\boldsymbol{x}_T\right)\prod p_{\theta} \left(\boldsymbol{x}_{t-1}|\boldsymbol{x}_{t},\boldsymbol{z}\right)=p\left(\boldsymbol{x}_T\right)\prod \mathcal{N}\left(\boldsymbol{x}_{t-1}|M_\theta\left(\boldsymbol{x}_t,\boldsymbol{z},t\right),\beta_t\boldsymbol{\mathrm{I}}\right).
\end{equation}
Since the texture latent variables $\boldsymbol{x}_{1:T}$ are not compressed but synthesized at decoding time, the optimal encoder and prior should be learned jointly with the decoder’s marginal likelihood. $p\left(\boldsymbol{x}_{0}|\boldsymbol{z}\right)=\int p\left(\boldsymbol{x}_{0:T}|\boldsymbol{z}\right)d\boldsymbol{x}_{1:T}$ while targeting a certain tradeoff between rate and distortion specified by a Lagrange parameter $\lambda$. We can upper-bound this rate-distortion (R-D) objective by invoking Jensen’s inequality,

\paragraph{\textbf{Conditional diffusion models for compression}} extend diffusion models to lossy data compression by leveraging conditional generation \cite{Yang2023cd}. In this framework, an image $\boldsymbol{x}_0$ is compressed into a set of latent representations $\boldsymbol{z}$ using an entropy-optimized quantization process. These latent variables are then used as conditioning inputs to guide the reverse diffusion process:
\begin{equation}
    p_\theta\left(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t, \boldsymbol{z}\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1}|M_\theta\left(\boldsymbol{x}_t, \boldsymbol{z}, t\right),\beta_t\boldsymbol{\mathrm{I}}\right).
\end{equation}
Here, $\boldsymbol{z}$ captures global information about the image, while the reverse diffusion process reconstructs finer details progressively. This approach combines the advantages of traditional entropy coding with the generative capabilities of diffusion models to achieve high-quality reconstruction at low bitrates.

The model is trained using a similar objective to standard diffusion models, but with an added conditioning mechanism for $\boldsymbol{z}$. At test time, the forward diffusion process maps decoded latents back to a noisy representation, from which the reverse process iteratively reconstructs the original image. This conditional setup allows the model to balance reconstruction quality and perceptual realism, adapting to different compression needs.


\subsection{3D Conditional Diffusion Model for Scientific Data Compression}

\subsection{Error Guarantees}