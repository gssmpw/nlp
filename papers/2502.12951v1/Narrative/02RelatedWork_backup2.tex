\section{Related Work}

\subsubsection{Diffusion Models for Image Compression.}

Diffusion models \cite{Sohl2015,Song2019,Ho2020,Song2021scorebased,Nichol2021,Dhariwal2021} have emerged as a dominant class of generative models in machine learning, demonstrating remarkable success across various domains, including image generation and natural language processing. Diffusion models also have recently been explored for image compression tasks. These works demonstrate how diffusion-based generative models can be adapted to achieve state-of-the-art performance in lossy image compression while maintaining high reconstruction quality.

Hoogeboom et al. \cite{Hoogeboom2022autoregressive} introduced Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models and absorbing discrete diffusion. ARDMs are simple to implement and easy to train, supporting parallel generation adaptable to any given generation budget. They require significantly fewer steps than discrete diffusion models to attain the same performance and are uniquely suited to lossless compression tasks. 

Yang and Mandt \cite{Yang2023cd} introduced an end-to-end optimized framework for lossy image compression using conditional diffusion models. Their approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and then reconstructed using a conditional diffusion model. This method introduces an additional ``content'' latent variable to store information about the image, while the remaining ``texture'' variables are synthesized during decoding. The model's performance can be tuned toward perceptual metrics of interest, yielding strong FID scores and competitive distortion metrics. Training the diffusion with $\mathcal{X}$-parameterization enables high-quality reconstructions in only a few decoding steps, enhancing practicality.

Li et al. \cite{Li2024extreme} proposed a novel two-stage extreme image compression framework that exploits the generative capability of pre-trained diffusion models to achieve realistic image reconstruction at extremely low bitrates. The first stage employs a VAE-based compression approach to compress images into content variables, while the second stage leverages pre-trained stable diffusion to reconstruct images under the guidance of these content variables. A small control module injects content information, and a space alignment loss ensures alignment with the diffusion space, providing necessary constraints for optimization. Extensive experiments demonstrate that this method significantly outperforms state-of-the-art approaches in terms of visual performance at extremely low bitrates.

Careil et al. \cite{Careil2024towards} proposed a method for image compression at ultra-low bitrates using iterative diffusion models. Their approach decodes images conditioned on a vector-quantized representation and a global image description, utilizing a two-branch architecture where one branch handles the global structure and the other focuses on local details. This combination allows the model to achieve high-quality reconstructions while preserving important textures and details. The method demonstrates state-of-the-art visual quality as measured by perceptual metrics such as FID and KID, showing that diffusion-based approaches can offer significant advantages over traditional compression techniques.

Relic et al. \cite{Relic2024lossy} formulated the removal of quantization error as a denoising task, using diffusion to recover lost informattted image latent. Their approach allows performing less than 10\% of the full diffusion generative process and requires no architectural changes to the diffusion model, enabling the use of foundation models as a strong prior without additional fine-tuning. The proposed codec outperforms previous methods in quantitative realism metrics, and reconstructions are qualitatively preferred by end users, even when other methods use twice the bitrate.

Ma et al. \cite{ma2024correcting} presented a diffusion-based image compression method that employs a privileged end-to-end decoder model, achieving better perceptual quality while guaranteeing distortion to an extent. They built a diffusion model and designed a novel paradigm combining the diffusion model and an end-to-end decoder, with the latter responsible for transmitting privileged information extracted at the encoder side. Theoretical analysis of the reconstruction process of diffusion models at the encoder side with the original images being visible led to the introduction of an end-to-end convolutional decoder to provide a better approximation of the score function at the encoder side and effectively transmit the combination. Experiments demonstrate the superiority of this method in both distortion and perception compared with previous perceptual compression methods.

Although these studies collectively highlight the versatility and potential of diffusion models for advancing the field of image compression, there is no work that leverages diffusion models in scientific data compression.
