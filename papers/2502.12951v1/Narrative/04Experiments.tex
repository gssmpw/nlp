\section{Experiments}
This section presents the experimental results of our compression framework---Guaranteed Conditional Diffusion with Tensor Correction (GCDTC). We utilize two scientific datasets generated by E3SM and S3D applications. All the experiments are conducted using an Nvidia K80 GPU in OLCF's Andes.

\subsection{Datasets, Metrics, and Baselines}

%\paragraph{\textbf{E3SM Dataset.}}
\subsubsection{E3SM Dataset.}
We use the dataset generated by the E3SM (Energy Exascale Earth System Model) \cite{e3sm} that simulates Earth's climate system. For each 30-day period, the E3SM simulates climate variables of $240\times 240$ resolution with float32 data points in 6 regions with 720 timesteps. We use the cropped sea-level pressure (PSL) climate variable for 3 months, which results in a dataset with dimensions of $6\times 2160\times 192 \times 192$. We construct 3D blocks across temporal and spatial spaces in each region.
\vspace{-0.2cm}
\subsubsection{S3D Dataset.}
The dataset is generated by Sandia’s compressible reacting direct numerical simulation (DNS) code, S3D \cite{s3d}. The S3D simulates chemically reacting flow, involving detailed chemical mechanisms across numerous species. We use three species' $640\times 640$ mass fraction with float64 data points, collected over 288 timesteps, and create 3D blocks for each species.

\vspace{-0.2cm}
\subsubsection{Metrics.}
We use the \textit{NRMSE} and \textit{compression ratio} for the compression quality evaluation. NRMSE is the normalized RMSE, where RMSE is divided by a data range. In the computation of compression ratios, we consider all the sizes of models and dictionaries for entropy coding as scientific compression techniques are applied to a specific scientific application. Otherwise, we might use an extremely large machine, even bigger than a dataset, which can produce ideal compression results using overfitting.

\vspace{-0.2cm}
\subsubsection{Baselines.}
We compare our method to SZ3 \cite{SZ3} and a standard convolutional autoencoder (AE). SZ is one of the most dominant error-bounded lossy compressors for lossy scientific data compression. It is a prediction-based method, where a data point is estimated by its neighbors. The prediction accuracy is affected by a specified point-wise error bound. For the autoencoder, we incorporate 3D convolutional layers. The architecture is almost the same as the encoder and embedder structures of the conditional diffusion model, described in the Appendix. The only difference is the number of the output channels in the last unit. The model is trained to minimize the MSE loss between the original and reconstructed data. We then apply the error guarantee process in Section~\ref{subsec:tc_eg} to the output of the autoencoder.

\subsection{Implementation Details}
%\paragraph{\textbf{Training.}}
\subsubsection{Training.}
We don't split the datasets into training and test sets as error-bounded lossy compressors including SZ are not learned models. We divide each dataset into a set of $16\times 64\times 64$ blocks. The model is trained using the Adam optimizer \cite{adam} with the learning rate of $1\times 10^{-3}$ and 100 epochs. The number of diffusion steps is 1000 and the linear noise scheduling method of the DDPM is incorporated. The maximum and minimum $\beta$ are set to $5\times 10^{-3}$ and $1\times 10^{-5}$ respectively. Our framework is implemented using Pytorch \cite{pytorch}. The architecture detail of our conditional diffusion model is illustrated in the Appendix. We use the tensor correction network in \cite{JL-S3D_arxiv}. The inputs are 60-dimensional and 48-dimensional tensors across temporal space in E3SM and S3D respectively. In the error guarantee process in Section~\ref{subsec:tc_eg}, we correct $4\times 4\times 4$ blocks for both E3SM and S3D. The quantization factor $b$ and bin size $a$ are set to 1000 and 16 respectively for both latent variables $\boldsymbol{z}$ and selected coefficients $\boldsymbol{c}_s$.

\subsection{Results and Discussion}
We compress and reconstruct the datasets, denoted as PD. We vary the error bounds, the maximum point-wise distortion (SZ) and region-wise distortion (Ours), to get various compression results as shown in Figure~\ref{fig:compress_result}. Our experiments reveal that our proposed GCDTC outperforms the standard convolutional autoencoder in both the E3SM and S3D datasets. In the conditional diffusion model, latent variables convey content information and the denoising decoder processes further details. Both of them contribute reconstructions, which results in better compression quality compared to the autoencoder. Compared to SZ, GCDTC achieves at least twice larger compression ratios above $1\times 10^{-4}$ NRMSE in E3SM, while yielding competitive compression results in S3D. In lossy scientific data compression, $1\times 10^{-3}$ NRMSE is usually used as a target or an acceptable compression quality level for post-analysis, and GCDTC shows decent amounts of data reduction at this NRMSE level. Figure~\ref{fig:compress_example} shows reconstruction examples at the compression ratio 100. We zoom into a small region to check the details. Our framework can capture the details of the original data.

\begin{figure}[H]  % 使用 H 强制固定位置
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/E3SM_PSL_PD_NRMSE.pdf}
    \hfill
    \includegraphics[width=0.4\textwidth]{Figures/S3D_PD_NRMSE.pdf}
    \caption{Reconstruction quality vs. compression ratio evaluation on E3SM (left) and S3D (right) datasets. GCDTC and GCAE denote Guaranteed Conditional Diffusion with Tensor Correction and Guaranteed Convolutional AutoEncoder. Note that the NRMSE results (y-axis) are plotted on a log scale. The result shows that our GCDTC outperforms GCAE, while yielding competitive performance with SZ.}
    \label{fig:compress_result}
\end{figure}

\FloatBarrier  % 确保 Figure 3 先被放置，Figure 4 不会跑到前面去

\begin{figure}[H]  % 继续使用 H 让 Figure 4 紧随 Figure 3
    \centering
    \includegraphics[width=\textwidth]{Figures/Results_example.pdf}
    \caption{Visualization of reconstructions in E3SM and S3D at compression ratio 100.}
    \label{fig:compress_example}
\end{figure}

\FloatBarrier  % 确保 Table 不会被浮动到奇怪的位置

We also evaluate the complexity by comparing the number of model parameters and decoding time. The machine sizes are included in the compression ratio computation. GCDTC suffers from slow decoding speed due to the iterative denoising process. We will address this limitation by incorporating progressive distillation \cite{Salimans2022progressive} that reduces the number of iterations in our future work. Despite this limitation, our conditional diffusion model proves its effectiveness and introduces the recent paradigm in generative AI in the context of lossy scientific data compression.

\begin{table}[H]  % 让 Table 也紧跟前面的内容，而不会漂移到其他页面
\centering
\caption{Model complexity and decoding time. Acronyms are in Figure~\ref{fig:compress_result}.}
\label{tab:complexity}
\begin{tabular}{| >{\centering}m{3.5cm}| >{\centering}m{2.5cm}| >{\centering}m{2.5cm}|>{\centering\arraybackslash}m{2.5cm}|}
\hline
\textbf{Model} & \textbf{GCDTC (Ours)} & \textbf{GCAE} & \textbf{SZ} \\
\hline
Number of Parameters & 1.5 M & 1.0 M & - \\
\hline
Decoding Time (sec) & 978.2 & 2.2 & 12.2 \\
\hline
\end{tabular}
\end{table}
