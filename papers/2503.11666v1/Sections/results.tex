\begin{table}[]
	\setcellgapes{0.5pt}
	\makegapedcells
	\renewcommand{\arraystretch}{1.3}
	\caption{Verification statistics of original and \ac{ML}-optimized regressions}
    \label{vs}
	\footnotesize
	\centering
	\resizebox{\columnwidth}{!}
	{
\begin{tabular}{lcccccccccc}
	\hline
	\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Verification \\ statistic\end{tabular}}}    & \multirow{2}{*}{\textbf{DUV}} & \multirow{2}{*}{\textbf{Original}} & \multicolumn{8}{c}{\textbf{ML-optimized}}                                                                                       \\ \cline{4-11} 
	&                               &                                    & \textbf{Linear} & \textbf{LASSO} & \textbf{Ridge} & \textbf{SVR} & \textbf{DT} & \textbf{RF} & \textbf{AdaBoost} & \textbf{KNN} \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Number of \\ test runs\end{tabular}}}       & ALU                           & 100                                & \multicolumn{8}{c}{22}                                                                                                          \\ \cline{2-11} 
	& ADC                           & 200                                & \multicolumn{8}{c}{11}                                                                                                          \\ \cline{2-11} 
	& ECC                           & 50                                 & \multicolumn{8}{c}{8}                                                                                                           \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Functional \\ coverage (\%)\end{tabular}}}  & ALU                           & 98.57                              & 99.72           & 99.83          & 99.89          & 99.91        & 99.96       & 99.96       & 99.93             & 99.94        \\ \cline{2-11} 
	& ADC                           & 100.00                             & 99.50           & 94.00          & 96.80          & 96.30        & 95.50       & 98.40       & 98.80             & 98.80        \\ \cline{2-11} 
	& ECC                           & 100.00                             & \multicolumn{8}{c}{100.00}                                                                                                      \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Simulation \\ run-time (sec)\end{tabular}}} & ALU                           & 426.88                             & 175.15          & 171.53         & 174.03         & 180.97       & 183.61      & 185.04      & 175.48            & 185.64       \\ \cline{2-11} 
	& ADC                           & 147.64                             & 3.40            & 3.52           & 3.77           & 3.27         & 2.61        & 2.50        & 2.69              & 2.74         \\ \cline{2-11} 
	& ECC                           & 210.40                             & 7.78            & 8.19           & 9.03           & 8.01         & 8.36        & 8.47        & 8.41              & 8.87         \\ \hline
\end{tabular}
	}
	\vspace{-0.3cm}
\end{table}
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{Figures/funccoverage.pdf} 
	\caption{Functional coverage with respect to test simulations generated by optimized regression}
	\label{funccoverage}
\end{figure}





This section compares verification statistics and performance metrics for original and \ac{ML}-optimized simulation regressions of three \ac{DUVs}, and examines functional coverage advancement with respect to test simulations. The generated \ac{ML}-optimized regressions are random in nature, and when run multiple times with random seeds, they simulate various random scenarios, leading to possible coverage improvement. It is observed that supervised regression algorithms LASSO, Ridge, SVR, DT, RF, AdaBoost, and KNN performed well in optimizing the test simulations and simulation run-times achieving \SI{99}{\percent} coverage regain, except for \ac{ADC} as shown in Table \ref{pm_or}. However, linear regression emerged as the most effective algorithm, attaining \SI{99.5}{\percent} coverage regain for ADC and providing consistent performance for all three \ac{DUVs}.

For each \ac{DUV}, Table \ref{vs} shows that the \ac{ML}-optimized regression significantly reduces the number of test runs, which in turn also reduces the simulation cycles and simulation run-time while achieving higher coverage compared to the original regression. For example, for the ALU, the \ac{ML}-optimized regression requires only 20 test runs and 175 seconds of simulation run-time to achieve \SI{99}{\percent} coverage, compared to 500 test runs and 426 seconds of simulation run-time for the original regression. Table \ref{pm_or} shows that, for example, the optimized regression reduced test simulations by a factor of 18 for \ac{ECC}, optimized run-time by 27 times, and achieved \SI{100}{\percent} coverage regain for linear \ac{ML}-optimized regression. Fig. \ref{funccoverage} illustrates the rapid attainment of maximum functional coverage with fewer test simulations for output-optimized regression using linear models for each \ac{DUV}. 
\begin{table}[]
	\setcellgapes{0.5pt}
	\makegapedcells
	\renewcommand{\arraystretch}{1.3}
	\caption{Performance metrics of ML-optimized regression}
	\label{pm_or}
	\footnotesize
	\centering
	\resizebox{\columnwidth}{!}
	{
\begin{tabular}{lccccccccc}
	\hline
	\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Performance\\ Metric\end{tabular}}}                    & \multirow{2}{*}{\textbf{DUV}} & \multicolumn{8}{c}{\textbf{Supervised models}}                                                                                  \\ \cline{3-10} 
	&                               & \textbf{Linear} & \textbf{LASSO} & \textbf{Ridge} & \textbf{SVR} & \textbf{DT} & \textbf{RF} & \textbf{AdaBoost} & \textbf{KNN} \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Optimization in \\ test simulations (x)\end{tabular}}} & ALU                           & \multicolumn{8}{c}{4.55}                                                                                                        \\ \cline{2-10} 
	& ADC                           & \multicolumn{8}{c}{18.18}                                                                                                       \\ \cline{2-10} 
	& ECC                           & \multicolumn{8}{c}{7.14}                                                                                                        \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Optimization \\ in run-time (x)\end{tabular}}}         & ALU                           & 2.44            & 2.49           & 2.45           & 2.36         & 2.33        & 2.31        & 2.43              & 2.3          \\ \cline{2-10} 
	& ADC                           & 43.42           & 41.96          & 39.16          & 45.15        & 56.61       & 59          & 54.83             & 53.97        \\ \cline{2-10} 
	& ECC                           & 26.99           & 25.66          & 23.27          & 26.23        & 25.13       & 24.79       & 24.97             & 23.67        \\ \hline
	\multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Coverage \\ regain (\%)\end{tabular}}}                 & ALU                           & 101.16          & 101.26         & 101.33         & 101.35       & 101.4       & 101.4       & 101.38            & 101.38       \\ \cline{2-10} 
	& ADC                           & 99.5            & 94             & 96.8           & 96.3         & 95.5        & 98.4        & 98.8              & 98.8         \\ \cline{2-10} 
	& ECC                           & \multicolumn{8}{c}{100}                                                                                                         \\ \hline
\end{tabular}
	}
	
	\vspace{-0.4cm}
\end{table}