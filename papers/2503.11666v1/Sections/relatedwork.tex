\begin{table}[htbp]
	\setcellgapes{1pt}
	\makegapedcells
	\renewcommand{\arraystretch}{1.3}
	\caption{Comparison of our work with other relevant works}
	\label{relatedwork}
	\footnotesize
	\centering
	\resizebox{\columnwidth}{!}
	{
\begin{tabular}{cllcll}
	\hline
	\textbf{Work}                                                & \textbf{Year} & \textbf{ML Approach}                                                    & \textbf{\begin{tabular}[c]{@{}c@{}}Verification \\ Method\end{tabular}} & \textbf{Training Data}                                                                    & \textbf{Contribution} \\ \hline
	\cite{sarath}                                   & 2018          & Linear, ANN                                                             & SV-UVM                                                                  & \begin{tabular}[c]{@{}l@{}}Input stimuli, \\ coverbin status\end{tabular}                 & SS, TR                \\ \hline
	\cite{kulkarni}                                 & 2019          & \begin{tabular}[c]{@{}l@{}}Various supervised\\ algorithms\end{tabular} & SV-UVM                                                                  & \begin{tabular}[c]{@{}l@{}}Constraints or knobs \\ in DUV\end{tabular}                    & FCC, TR               \\ \hline
	\cite{gaur}                                     & 2019          & ANN                                                                     & SV-UVM                                                                  & I/P and O/P from DUV                                                                      & SS                    \\ \hline
	\cite{varambally2020optimising}                 & 2020          & ANN                                                                     & \begin{tabular}[c]{@{}c@{}}Python\\ Cocotb\end{tabular}                 & \begin{tabular}[c]{@{}l@{}}I/P and O/P from DUV, \\ coverage and test status\end{tabular} & FCC                   \\ \hline
	\cite{ghany}                                    & 2021          & \begin{tabular}[c]{@{}l@{}}ANN, DNN, \\ DT, SVR\end{tabular}            & SV-UVM                                                                  & I/P and O/P from DUV                                                                      & TR                    \\ \hline
	\cite{blackmore}                                & 2022          & NN                                                                      & SV-UVM                                                                  & Test configurations                                                                       & TR                    \\ \hline
	\cite{plucinski}                                & 2023          & NN                                                                      & SV-UVM                                                                  & \begin{tabular}[c]{@{}l@{}}Test parameters and \\ I/P to DUV\end{tabular}                 & SS                    \\ \hline
	\cite{gadde2024efficientstimuligenerationusing} & 2024          & RL                                                                      & SV                                                                      & I/P and O/P code coverage                                                                 & CCC                   \\ \hline
	Our work                                                     & 2024          & \begin{tabular}[c]{@{}l@{}}Various supervised\\ algorithms\end{tabular} & PyUVM                                                                   & \begin{tabular}[c]{@{}l@{}}I/P and coverbin status \\ (hit or not hit)\end{tabular}       & TR, SS                \\ \hline
\end{tabular}
	}
	\begin{center}
		\vspace{1ex}
		\justifying
		\tiny  Notes:- ANN: Artificial Neural Networks, SVR: Support Vector Regression, KNN: K-Nearest Neighbors, SVM: Support Vector Machine, RL: Reinforcement Learning, DNN: Deep Neural Networks, DT: Decision Tree; RF: Random Forest, NN: Neural Networks, SS: Simulation Speedup, TR: Tests Reduction, FCC: Functional Coverage Convergence, CCC: Code Coverage Convergence
	\end{center}
	\vspace{-0.85cm}
\end{table}

In recent years, there has been a growing interest in using \ac{ML} for verification processes to improve functional coverage. Several studies have explored the application of ML algorithms in autonomously updating constraints, classifying inputs, improving test generation, and selecting novel tests. For instance, Ambalakkat et al. \cite{sarath} proposed a methodology to autonomously update constraints using multiple \ac{ML} algorithms, achieving faster coverage closure. Gaur et al. \cite{gaur} classified inputs as randomizable or not using switching probability of output as a metric, and applied it to the Physical Medium Attachment (PMA) block of High-Speed Serial Interface (HSSI). Kulkarni \cite{kulkarni} improved functional coverage using \ac{ML} methods on the ModiÔ¨Åed-Exclusive-Shared-Invalid InterSection Controller (MESI ISC) design. Varambally and Sehgal \cite{varambally2020optimising} proposed a software-based methodology on an open-source platform, achieving significant reduction in simulation iterations. Other studies \cite{ghany}, \cite{blackmore}, and \cite{plucinski} have also demonstrated the effectiveness of ML-based approaches in test selection, prediction of input stimuli, and functional coverage. The authors in \cite{gadde2024efficientstimuligenerationusing} utilized reinforcement learning to converge code coverage. Table \ref{relatedwork} shows the comparison of these studies with our work.

However, all these studies mostly utilized SV-UVM, which obstructs on-the-fly collection of data for \ac{ML} modeling. Additionally, the integration of the \ac{ML} environment with the simulation environment is tedious and manual. Most of the approaches/frameworks are not scalable to complex designs and only contribute to either simulation speedup or coverage improvement. In contrast, our work proposes a design-agnostic method that utilizes Python-based verification, easing the integration to the \ac{ML} environment and allowing on-the-fly data collection. We not only focus on achieving target coverage but also simulation speedup and reduction of test simulations while attaining it.