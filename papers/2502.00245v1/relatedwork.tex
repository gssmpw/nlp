\section{Related Work}
% \tianyuan{I think the current relates work contains too much information and is too long. I will shorten it if no space left.}
% \jj{I shortened 2.1 and 2.2, and removed 2.3 and 2.4 (keeping the last sentence). FL doesn't seem too relevant here, and in-context learning probably doesn't need to be emphasized either} \tianyuan{Thank you, prof. JJ!}

\textbf{DP Synthetic Dataset.} 
% To train a dowand in-context learning nstream task model while ensuring Differential Privacy (DP) of private training data, one can apply DP-SGD~\cite{abadi2016deep} for fine-tuning. However, this approach is computationally intensive, requiring a large amount of high-quality private samples, and suffers from significant performance degradation under tight DP budgets (small $\epsilon$)~\cite{lin2024differentially}. 
The goal of generating DP synthetic data is to mimic private dataset while protecting sensitive information. %, in order to train a downstream-task model.% while ensuring Differential Privacy (DP) of private data.
% An alternative is to create a DP synthetic dataset that mimics the private dataset while protecting sensitive information, which can then be used to train downstream models. 
Although fine-tuning a PLM with DP-SGD~\cite{abadi2016deep} for data generation purpose can be effective% before generating the target DP synthetic dataset
~\cite{bommasani2019towards,putta2023differentially,flemings2024differentially,mattern2022differentially,yue2023synthetic}, it 
% still faces challenges related to computational intensity and high data requirements. 
is computationally intensive and requires a large number of high-quality private samples to reach strong performance.
Moreover, many state-of-the-art PLMs such as GPT series~\cite{openai2021gpt3-5,openai2023gpt4,hurst2024gpt4o} are also closed-source, making DP fine-tuning impractical.
%To solve above challenges, and using only the generative API of the PLM, 
%\\\jj{Need line break}

A new line of work instead relies on generative APIs of PLMs without fine-tuning, which focuses on either iterative data synthesis under DP guidance~\cite{lin2024differentially,zhao2024generate,bojkovic2024differentially} or creating DP replica of a given large private dataset~\cite{nagesh2024private}. %This eliminates the need for PLM fine-tuning, facilitating deployment and extension to powerful closed-source models like the GPT series.
Given that requiring a large global dataset for synthetic data initialization~\cite{zhao2024generate} is hard to obtain in most cases, \citet{lin2024differentially} proposes a more practical solution, Private Evolution (PE), which instead uses task-related synthetic samples. In PE, private samples are used to identify their nearest synthetic counterparts under DP protection, which then guide the growth of the DP synthetic dataset. PE is proven effective across images~\cite{lin2024differentially} and text~\cite{xie2024differentially}, and is further adapted to federated private data scenarios~\cite{hou2024pretext}.
% Private Evolution (PE)~\cite{lin2024differentially,xie2024differentially} to generate a DP synthetic dataset using only the generative API of the PLM. This eliminates the need for PLM fine-tuning, facilitating deployment and extension to powerful closed-source models like the GPT series. In PE, private samples are used to identify their nearest DP synthetic counterparts under DP protection, which then guide the expansion of the DP synthetic dataset. PE is applicable across various modalities, including images~\cite{lin2024differentially} and text~\cite{xie2024differentially}, and has been adapted for federated private data scenarios~\cite{hou2024pretext}. 
% \citet{zhao2024generate} uses a similar evolutionary generation process but relies on large scale public dataset as non-private dataset initialization which is hard to obtain. 
% Similarly, private density estimation~\cite{bojkovic2024differentially} can also be applied to create DP synthetic data but only for statistical tasks.
% \citet{nagesh2024private} seeds LLM generation with DP protected private data to produce the DP synthetic dataset
However, all these works primarily focus on using a single PLM as the generation model.

\textbf{PLM Fusion.} The combination of multiple PLMs can lead to stronger model performance~\cite{liu2023dynamic,du2023improving,wan2024knowledge,wan2024fusechat,li2024more,zou2024fusegen}. Some studies fine-tune target models with token-level fusion from PLMs as teachers during training time~\cite{wan2024knowledge,wan2024fusechat}, while others use majority voting~\cite{li2024more} or logits averaging~\cite{mavromatis2024pack} for knowledge fusion during inference. 
However, data privacy challenges persist, as training or test samples are exposed to external models. To solve this, FuseGen~\cite{zou2024fusegen} recently proposes PLM fusion in a zero-shot learning setting, utilizing only model APIs to synthesize data without accessing real private samples, thereby ensuring data privacy. However, by treating all PLMs equally, it overlooks the capability difference of individual PLMs over different tasks.
% \textbf{Federated Learning (FL)}. 
% FL was first proposed by Google~\cite{McMahan2016fl} 
% % in which a cross-device collaboration scenario is described 
% % in which millions of mobile users collaboratively train a shared model using their local private data in a decentralized manner without directly sharing their data thereby preserving privacy. 
% enabling millions of mobile users to collaboratively train a shared model using their local private data without direct data sharing, thus preserving privacy.
% % In FL, each local model is updated independently by its respective party using private data and the aggregated knowledge from each local model is incorporated into the shared global model through the exchange of model parameter updates, thereby enhancing its performance. Subsequently, the updated global model is redistributed to each local model for further iterative improvements. 
% In the era of LLMs, FL has emerged as a decentralized training paradigm utilizing distributed data~\cite{fan2023fate,kuang2024federatedscope,ye2024openfedllm,zheng2024safely,goetz2020federated}. Additionally, research has explored how LLMs can enhance FL, investigating methods like federated synthesis to protect and enhance private data~\cite{goetz2020federated,behera2022fedsyn,little2023federated,hou2024pretext}, model initialization to speed up convergence~\cite{tan2022federated,nguyen2022begin,liu2024language}, personalization improvement~\cite{yang2023efficient,li2024visual}, etc. However, these works are all motivated by the decentralized data overlooking the potential of distributed LLMs.

% \textbf{Contrastive In-context Learning.}\footnote{Works~\cite{ren2024towards,miyanishi2024multimodal} considering understanding in-context learning with contrastive learning theories are sometimes referred to using the same name, but we do not consider them here.}
% % Here, we do not consider works that 
% The idea of using contrastive information to enrich in-context learning samples has been exploited from different aspects. Samples belonging to positive and negative classes~\cite{liang2024context}, correct or wrong self-predictions of training samples during training time~\cite{mo2024cicl}, human-preferred and non-preferred question responses~\citet{gao2024customizing} have all been utilized as contrastive samples.
% \citet{mo2024cicl} exploits using correct/wrong self-predictions of training samples as in-context samples to aid test time performance. 
% \citet{liang2024context} uses samples belong to positive and negative classes as contrastive information for identifying causality and non-causality whitin a sentence.
% Our study is the first known effort to consider contrastive in-context learning for synthetic data generation, by treating samples of different qualities generated by multiple PLMs as contrastive information. \yang{this paragraph is not related to any related work. We can remove it?} 
More related works considering Contrastive In-context Learning are included in \cref{sec:appendix_related_work}.


%