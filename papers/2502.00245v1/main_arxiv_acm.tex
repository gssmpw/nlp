%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,authordraft,nonacm]{acmart}
\documentclass[sigconf,nonacm]{acmart}
% \documentclass[sigconf]{acmart}
% \documentclass[nonacm]{acmart}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

% %% Rights management information.  This information is sent to you
% %% when you complete the rights form.  These commands have SAMPLE
% %% values in them; it is your responsibility as an author to replace
% %% the commands and values with those provided to you when you
% %% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% %
% %  Uncomment \acmBooktitle if th title of the proceedings is different
% %  from ``Proceedings of ...''!
% %
% %\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %  June 03--05, 2018, Woodstock, NY} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{inconsolata}    % improve the aesthetics of text in the typewriter font.
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{tabularx}
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage{subcaption}    % this will make the italic "Figure 1" disappear
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage[listings,skins,breakable]{tcolorbox}
% For theorems and such
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{mathtools}
% \usepackage{amsthm}
% \usepackage[noend]{algpseudocode}
\usepackage{algorithm}      % algorithms
\usepackage{algorithmic}    % algorithms
\usepackage{xcolor}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{bbding}         % for checkmark ans xmark
\usepackage{duckuments}     % for example-image-duck/a

\usepackage{CJKutf8} % for kerean characters

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}




%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Contrastive Private Data Synthesis via Weighted Multi-PLM Fusion}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Tianyuan Zou}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

\author{Tianyuan Zou}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \authornotemark[1]
\affiliation{%
  \institution{Institute for AI Industry Research (AIR), Tsinghua University, Beijing}
  \country{China}
}

\author{Yang Liu}
\authornote{Correspondence: yangliu62@tsinghua.edu.cn}
\affiliation{%
  \institution{Institute for AI Industry Research (AIR), Tsinghua University, Beijing} %; Shanghai Artificial Intelligence Laboratory, Shanghai
  \country{China}
}

\author{Peng Li}
\affiliation{%
  \institution{Institute for AI Industry Research (AIR), Tsinghua University, Beijing}
  \country{China}
}

\author{Yufei Xiong}
\affiliation{%
 \institution{the Department of Mathematics, Harbin Institute of Technology, Weihai, Shandong}
 \country{China}
 }

\author{Jianqing Zhang}
\affiliation{%
  \institution{Shanghai Jiao Tong University, Shanghai}
  \country{China}
}

\author{Jingjing Liu}
\affiliation{%
  \institution{Institute for AI Industry Research (AIR), Tsinghua University, Beijing}
  \country{China}
}

\author{Xiaozhou Ye and Ye Ouyang}
\affiliation{%
  \institution{AsiaInfo Technologies, Shanghai}
  \country{China}
  }

\author{Ya-Qin Zhang}
\affiliation{%
  \institution{Institute for AI Industry Research (AIR), Tsinghua University, Beijing}
  \country{China}
  }


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zou, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Substantial quantity and high quality are the golden rules of making a good training dataset with sample privacy protection equally important. 
Generating synthetic samples that resemble high-quality private data while ensuring Differential Privacy (DP), a formal privacy guarantee, promises scalability and practicality.
However, existing methods relying on pre-trained models for data synthesis %that avoid fine-tuning large pre-trained generative models 
often struggle in data-deficient scenarios, suffering from limited sample size, inevitable generation noise and existing pre-trained model bias. 
To address these challenges, we propose a novel contr\textbf{A}stive private data \textbf{S}ynthesis via \textbf{W}eighted multiple \textbf{P}re-trained language models (PLM) framework, named as \textbf{WASP}. 
WASP utilizes limited private samples for more accurate private data distribution estimation via a Top-$Q$ voting mechanism, and leverages low-quality synthetic samples for contrastive generation via collaboration among dynamically weighted multiple pre-trained models.
Extensive experiments on $6$ well-developed datasets with $6$ open-source and $3$ closed-source PLMs demonstrate the superiority of WASP in improving model performance over diverse downstream tasks.
Code is available at \url{https://anonymous.4open.science/r/WASP}. \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2025>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Differentially Private Synthetic Dataset, Collaboration between Private Data and Private Model, Fusion of Pre-trained Language Model}

% %% A "teaser" image appears between the author and affiliation
% %% information and the body of the document, and typically spans the
% %% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

\received{31 January 2025}
\received[revised]{31 January 2025}
\received[accepted]{31 January 2025}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% \yang{Do not start with STMs, which is less relevant. Start directly with LLM, the issue of scarcity and exhaustion of public data, which leads to DP synthetic data from private data.} 
In the rapidly evolving landscape of AI models and AI agents, the strength of both Large Language Models (LLMs) and Small Task-specific Models (STMs) 
% STMs are particularly indispensable for fitting into resource-constrained environments~\cite{bommasani2021opportunities}. 
hinges on the abundance of high-quality training data~\cite{budach2022effects,wang2024survey}, of which only a limited amount of samples can be harnessed in practice. To further complicate the issue, broad tasks across disciplines such as medical record summarization~\cite{rumshisky2016predicting},  chatbots for personalized weight loss~\cite{chew2022use} and instruction-following LLM fine-tuning~\cite{yu2024privacypreserving} all rely on high-quality private data collected from real users, which inevitably incurs non-negligible privacy issues.% that add up to the difficulty of gathering and formatting a large amount of high-quality training data. 

Differentially private synthetic data stands in as a promising remedy~\cite{bommasani2019towards,putta2023differentially,flemings2024differentially}, by creating a new synthetic dataset that resembles the real private dataset while preserving the privacy of each sample via guaranteeing Differential Privacy (DP)~\cite{dwork2006differential}. 
There are two main lines of research for generating DP synthetic datasets.
% There are two main approaches for creating a DP synthetic dataset. 
% The first one, DP Fine-tune Generator~\cite{mattern2022differentially,yue2023synthetic} 
The first line of works~\cite{mattern2022differentially,yue2023synthetic} introduce DP Fine-tune Generator which 
involves fine-tuning a Pre-trained Language Model (PLM) using DP-SGD~\cite{abadi2016deep}. % before using it for generation. 
However, this practice is computationally intensive and requires substantial data for effective fine-tuning.
% \yang{these cited works mostly used trained in-house generative AI models for DP synthetic data, need to clarify this and point out the limitations of this route, which leads to the API-based approaches.}
Another line of work, Private Evolution (PE)~\cite{lin2024differentially,xie2024differentially,hou2024pretext}, relieves the fine-tuning requirement and instead merely uses the APIs of pre-trained models for generation, under DP-protected guidance from private samples. This API-based nature is efficient in creating DP synthetic data, and can leverage both open-source and closed-source pre-trained models, making PE a more applicable solution compared to its counterparts.
% Consequently, Privacy Evolution~\cite{lin2024differentially} has been proposed as an efficient and effective API-based method for generating high-quality synthetic dataset under DP protection, avoiding the need of resource-intensive pre-trained model fine-tuning process compared to previous methods~\cite{mattern2022differentially,yue2023synthetic}. 
%In PE, private samples help to boost the generation performance of pre-trained models by selecting (with DP) their most similar \jj{similar to what?} \tianyuan{private samples} synthetic samples as generation feedback in an iterative manner, to generate private data distribution resembling DP synthetic samples. \jj{Seems unnecesary}

% On the other hand, pooling the power of multiple distinct pre-trained models has been explored and verified to be promising in obtaining a model with stronger performance~\cite{wan2024knowledge,wan2024fusechat,li2024more,zou2024fusegen}. Among these works, FuseGen~\cite{zou2024fusegen} points out a possibility in generating an improve-quality synthetic dataset in a zero-shot setting, i.e. no real private samples is accessible. 

% \begin{figure*}
%     % \vspace{-1em}
%     \centering
% % \begin{minipage}[c]{0.64\linewidth}
%     % \includegraphics{}
%     % \vspace{-0.5em}
%     % \subfigure[GPT-2 (85.38)]{
%     %      \begin{minipage}[t]{0.31\linewidth}
%     %      \centering
%     %      \includegraphics[width=1\linewidth]{figure/introduction/text_converge/imdb/100gold/smaller/gpt2.png}
%     %      \label{subfig:text_length_converge_pe_gpt2_small}
%     %      \end{minipage}
%     %  }
%     % \vspace{-0.5em}
%     % \subfigure[OPT (83.86)]{
%     %      \begin{minipage}[t]{0.31\linewidth}
%     %      \centering
%     %      \includegraphics[width=1\linewidth]{figure/introduction/text_converge/imdb/100gold/smaller/opt.png}
%     %      \label{subfig:text_length_converge_pe_opt_small}
%     %      \end{minipage}
%     %  }
%     % \vspace{-0.5em}
%     %  \subfigure[Flan-T5 (89.00)]{
%     %      \begin{minipage}[t]{0.31\linewidth}
%     %      \centering
%     %      \includegraphics[width=1\linewidth]{figure/introduction/text_converge/imdb/100gold/smaller/flan-t5.png}
%     %      \label{subfig:text_length_converge_pe_flant5_small}
%     %      \end{minipage}
%     %  }
%     \vspace{-0.5em}
%     \subfigure[Llama-2 (47.42)]{
%          \begin{minipage}[t]{0.31\linewidth}
%          \centering
%          \includegraphics[width=1\linewidth]{figure/introduction/text_converge/yelpRating/100gold/smaller/['llama-2-7b-chat-hf'].png}
%          \label{subfig:text_length_converge_pe_llama2_small}
%          \end{minipage}
%      }
%     % \vspace{-0.5em}
%     \subfigure[OPT (50.81)]{
%          \begin{minipage}[t]{0.31\linewidth}
%          \centering
%          \includegraphics[width=1\linewidth]{figure/introduction/text_converge/yelpRating/100gold/smaller/['opt-6.7b'].png}
%          \label{subfig:text_length_converge_pe_opt_small}
%          \end{minipage}
%      }
%     \vspace{-0.5em}
%     \subfigure[ChatGLM3 (55.17)]{
%          \begin{minipage}[t]{0.31\linewidth}
%          \centering
%          \includegraphics[width=1\linewidth]{figure/introduction/text_converge/yelpRating/100gold/smaller/['chatglm3-6b-base'].png}
%          \label{subfig:text_length_converge_pe_chatglm_small}
%          \end{minipage}
%      }
%      \vspace{-0.5em}
%      \subfigure[WASP (Ours) (61.21)]{
%          \begin{minipage}[t]{0.31\linewidth}
%          \centering
%          \includegraphics[width=1\linewidth]{figure/introduction/text_converge/yelpRating/100gold/smaller/['gpt2-xl', 'llama-2-7b-chat-hf', 'vicuna-7b-1.5v', 'opt-6.7b', 'chatglm3-6b-base', 'flan-t5-xl'].png}
%          \label{subfig:text_length_converge_pe_fuse_small}
%          \end{minipage}
%      }
%      % \vspace{-0.5em}
%      \caption{Final synthetic text length distribution and real (private) text length distribution of Aug-PE~\cite{xie2024differentially} (variance of PE for text modality) with $5$ iterations generating a total of $6,000$ samples using $100$ real samples in IMDb dataset for movie review semantic analysis under $(4.0, 1\times10^{-5})$-DP. Numbers in parentheses are the performance of STM trained using the synthetic samples. Using this limited amount of private data, Aug-PE fails to generate synthetic data which follows the real data distribution. Different PLM models present different synthetic data distribution, resulting in different STM performance. \yang{it is better to separate multi-model results from limited-data results, maybe add another row using an even smaller data amount which could show Flan-T5 also fails? can defer the iteration 0 results to appendix for space.} \tianyuan{our results may not be placed here as we want to tell the shortcoming of single models}}
%     \label{fig:aug_pe_text_length_convergence_iter04}
% % \end{minipage}
%     \vspace{-1em}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figure/introduction/fid_acc_comparison_small_ACCvalue_['PE', 'Random'].png}
%     \caption{Comparison of the similarity of synthetic dataset to real private dataset (measured by FID~\cite{heusel2017gans}) and STM performance (numbers within parenthesis) of Aug-PE~\cite{xie2024differentially} (dotted lines) and our WASP (dashed lines) under $(4.0, 1\times10^{-5})$-DP with IMDb dataset. Lower FID indicates higher similarity.}
%     \label{fig:pe_random_fid_and_acc}
% \end{figure*}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figure/introduction/fig_different_plm_different_pe_4.png}
%     \caption{Results of Aug-PE using $100$ private samples and $(4.0, 1\times10^{-5})$-DP. The performance of distinct pre-trained generation models differs within and across tasks.}
%     \label{fig:pe_variance}
% \end{figure}

\begin{figure*}
    \centering
    % \vspace{-0.5em}
    \subfigure[]{
         \begin{minipage}[t]{0.48\linewidth}
         \centering
         \includegraphics[width=1\linewidth]{figure/introduction/fid_acc_comparison_small_ACCvalue_PE_Random_2.png}
         \vspace{-1em}
         \label{subfig:pe_random_fid_and_acc}
         \end{minipage}
     }
    % \vspace{-0.5em}
    \subfigure[]{
         \begin{minipage}[t]{0.48\linewidth}
         \centering
         \includegraphics[width=1\linewidth]{figure/introduction/fig_different_plm_different_pe_2.png}
         % \includegraphics[width=1\linewidth]{figure/introduction/fig_different_plm_different_pe_2_old.png}
         \vspace{-1em}
         \label{subfig:pe_variance}
         \end{minipage}
     }
     \vspace{-0.7em}
     \caption{$(a)$ Comparison of the similarity of synthetic dataset to real private dataset (measured by FID~\cite{heusel2017gans}) and STM performance (numbers within parenthesis) of Aug-PE~\cite{xie2024differentially} (dotted lines) and our refinement (dashed lines) under $(4.0, 1\times10^{-5})$-DP with IMDb dataset. Lower FID indicates higher similarity. $(b)$ Results of Aug-PE using $100$ private samples and $(4.0, 1\times10^{-5})$-DP.}
    \label{fig:intro_fig_1}
    \vspace{-0.5em}
\end{figure*}

%While works based on 
Although proven effective, current PE methods~\cite{lin2024differentially,xie2024differentially,hou2024pretext}, % effectively generate higher-quality Differentially Private synthetic dataset (DP synthetic dataset), 
 still face three major challenges:
\textbf{$(1)$ Limited Private Samples.}
Existing PE methods rely on at least thousands of private samples~\cite{lin2024differentially,xie2024differentially,hou2024pretext} to guarantee reliable generation feedback selection.
% to ensure reliable voting statistics for synthetic data generation, 
In practice, however, data sources may provide only a few hundred samples~\cite{zdrazil2024chembl,ren2019almost}, leading to noisy selection guidance.
% This scarcity of data results in insufficient and noisy voting histograms, which ultimately leads to failure of the original PE algorithms. 
As shown in \cref{subfig:pe_random_fid_and_acc}, with limited private samples ($100$), Aug-PE~\cite{xie2024differentially} (PE for text)
% exhibits an increased FID, a metric that measures the distributional gap between real data and synthetic data,
failed to generate synthetic samples resembling real samples' distribution
% bad distribution resemblance between real data and synthetic data 
for $3$ PLMs (except Flan-T5).
% (Vicuna, OPT and ChatGLM3),
Similar conclusion is drawn  in~\citet{lin2024differentially} (see Table 2 therein).
% In contrast, our refinement demonstrates 
% % a continuous decrease in FID across iterations, suggesting 
% an improved resemblance.
This calls for a more precise guidance from limited private samples.
% This scarcity of data results in inadequate histograms, leading to a significant distributional divergence between the generated data and the actual data and subsequent performance decrease. 
% it is challenging to dig out enough information for boosting synthetic data generation.
%Like for PE, its hard to select synthetic samples based on the voting of a limited amount of private samples, resulting in bad real sample resemblance for some generative models 
% and subsequent performance decrease (see \cref{fig:aug_pe_text_length_convergence_iter04} and \cref{fig:change_in_gold}).
\textbf{$(2)$ Noisy Synthetic Data.}
% In \cref{tab:examples_bad_samples}, we illustrate examples of low-quality samples generated by the original Aug-PE, which are far from real sample distributions.
Although PE approaches encourage the generation of high-quality samples that are close to real private sample distribution, low-quality noisy samples are still unavoidable (see examples in \cref{tab:examples_bad_samples} in \cref{subsec:appendix_good_bad_samples}),  %However, several works have shown that low-quality noisy samples contained in synthetic datasets 
which hinder the final performance when training downstream models~\cite{ye2022progen,gao2023self,zou2024fusegen}. This highlights the the importance of instructing the avoidance of generating noisy samples during data synthesis.
% when encouraging generative models to mimic high-quality samples, highlighting the importance of avoiding low-quality samples.
% Thus, we additionally incorporate low-quality samples as generation guidance, indicating what should not be generated, which enhances the resemblance of the DP synthetic dataset to real private data and improves downstream STM performance (solid lines in \cref{fig:contrast_vs_noncontrast_q8}).
% However, previous PE approaches focus only on high-quality samples. Therefore, we incorporate these low-quality samples as generation guidance to provide information regarding what should not be generated which further improves the resemblance of the DP synthetic dataset with the real private data (see solid lines in \cref{fig:contrast_vs_noncontrast_q8}) as well as the trained downstream STM performance.
% , which is insufficient for enhancing model performance (see \cref{fig:fid_and_acc_not_related}).
% It is also essential to incorporate information regarding what should not be generated into the generation guidance.
% decreasing the similarity between synthetic samples and private samples does not guarantee an increase the performance of the downstream model trained using the synthetic dataset (see Figure 6 in \citet{xie2024differentially}), indicating that the quality of the synthetic dataset is not guaranteed. 
% leading to significant data being discarded in the feedback loop. Given the high cost of API queries, this results in an inefficient generation process. 
% \tianyuan{Mrs. Liu, first, we do not use more samples as feedbacks comparing with PE, but more types of samples. Second, I think we do not need to introduce the idea that "synthetic dataset is of low quality" here. I think we can just simply say that low-quality samples are not fully exploit in PE. Like what I previously wrote: Bad synthetic samples are overlooked for synthetic dataset quality-improvement with only high-quality samples that are closer to real private samples are used for iterative synthetic dataset generation.}
\textbf{$(3)$ Risky PLM Selection.} 
As shown in \cref{subfig:pe_random_fid_and_acc},
% \cref{fig:aug_pe_text_length_convergence_iter04} (complete results for 6 different LLMs are given in \cref{fig:aug_pe_text_length_convergence} in the Appendix), 
% pre-selecting a specific generative model to perform domain tasks can yield unsatisfactory results. 
different PLMs yield varying performances (some with unsatisfactory results), and even the best performing model differs across various downstream tasks (see \cref{subfig:pe_variance}), making it hard to select the most suitable pre-trained model for a specific task. Previous PE works primarily focus on single PLM setting, thus the potential of collaboration among multiple PLMs is still unexplored.
%However, previous PE works focus on single pre-trained generative model settings, ignoring the potential of combining multiple models, while previous none-fine-tuning privacy-preserving fusion method~\cite{zou2024fusegen} that pools the power of several pre-trained generation models treats all models equally, neglecting their distinct capabilities.
% However, the potential of combining multiple pre-trained models has been overlooked, despite evidence suggesting that this approach can boost performance~\cite{wan2024knowledge,wan2024fusechat,li2024more,zou2024fusegen}. 
% Notably, FuseGen~\cite{zou2024fusegen} demonstrates the ability to generate higher-quality synthetic datasets in a zero-shot setting, i.e. without access to real private samples, by using multiple pre-trained generation models. However, FuseGen fails to consider the differing capabilities of pre-trained models during fusion, treating each model equally.




% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figure/introduction/fid_acc_incorrelate.png}
%     \caption{Increase in the similarity between synthetic samples and private samples (lower FID value, red dot line) does not indicate an increase in synthetic dataset quality (higher downstream model performance/accuracy, green dashed line).}
%     \label{fig:fid_and_acc_not_related}
% \end{figure}

% To address the above challenges, we propose WASP, a collaborative framework that pools the knowledge from multiple Pre-trained Language Models (PLMs) to generate better DP synthetic dataset that boosts the downstream STM performance under the guidance of a small amount of private datasets kept at single or multiple Private Data Parties (PDPs) while preserving data privacy. 
To address these demanding challenges, we propose WASP, a collaborative framework that fuses the knowledge from weighted multiple PLMs to synthesize DP data in a contrastive fashion. %in data- deficient scenarios. % to train a downstream STM of high performance by using better DP synthetic dataset created through the collaboration of PLMs under the guidance of a small number of private samples kept at single or multiple PDPs.
% Note that, we focus on private sample inadequate (no more than $300$ samples in total) setting which is different from pervious works~\cite{lin2024differentially,xie2024differentially,hou2024pretext} to experimentally demonstrate that our framework \textit{solves the first challenge}.
%and multiple Private Data Parties (PDPs).
$(1)$ \textit{To overcome private sample scarcity}, we
% leverage the information from each real private sample to a greater extent by proposing a \textit{\textbf{Differentially Private Top-$Q$ voting}} strategy, 
first extend the voting mechanism for private distribution estimation used in PE from Top-$1$ to Top-$Q$ ($Q>1$) with decaying weights, 
in order to get a more accurate estimation while guaranteeing private data DP. %We increase the number of votes from $1$ (as in previous works~\cite{lin2024differentially,xie2024differentially,hou2024pretext}) to $8$ in our approach. To control function sensitivity (how much the result changes with the addition or removal of a single entry) for guaranteeing DP, discounted weights are assigned to these votes.
% \yang{what is the logic behind this weight? explain briefly.}
$(2)$ \textit{To reduce noise}, we
% introduce a novel \textit{\textbf{Contrastive In-context Learning}} method which 
further leverage the previous voting results to select both high-quality and low-quality samples, and incorporate a contrastive prompt containing both types of samples to improve synthetic data quality by encouraging generation that is more aligned to high-quality samples and less similar to low-quality ones.
Notice that under multi-PLM setting, these samples may originate from different PLMs. %combines low-quality sample demonstrations along side high-quality sample demonstrations and instruct PLMs to generate samples less similar to bad ones. This 
%forces PLMs to avoid generating low-quality samples that harm the STM performance.
$(3)$ \textit{To mitigate model bias}, we then interfuse the capabilities of multiple PLMs with dynamically
% introduce \textit{\textbf{PLM Importance Weighting}} to 
adjusted importance weight for each PLM based on the ensemble votes from private samples. %\tianyuan{without incurring additional API queries compared to using single PLM}\jj{Seems unnecessary details in Intro}. 
The underlying principle is to assign higher weights to PLMs that generate synthetic samples that are closer to real samples on average. Operating in an iterative fashion, the WASP framework can generate large quantity of synthetic data that better approximate real private data distribution while observing differential privacy. Notably, this process incurs no additional API queries compared to its single-PLM counterparts.
% \tianyuan{We also extend \textit{\textbf{Contrastive In-contest Learning}} to \textit{\textbf{Cross-PLM Contrastive In-context Learning}} for multi-PLM collaboration.}
%Extensive experiments on $6$ datasets demonstrate the effectiveness of our proposed WASP framework. In summary, 

Our contributions are summarized as follows:
% \yang{please update this part based on our discussions.} \tianyuan{DONE}

$1)$ We introduce a privacy-preserving collaborative framework WASP to facilitate collaboration between multiple PLMs and private samples, especially benefiting scenarios with limited private data.
% and PDPs while preserving the Differential Privacy of the private samples kept at PDPs. 

% $2)$ We %recognize the \textit{limited private samples} challenge and 
% propose a simple yet effective \textit{\textbf{Differentially Private Top-$Q$ voting}} mechanism to better estimate private distribution with limited private samples. \textit{\textbf{Cross-PLM Contrastive In-context Learning}} is further introduced to integrate the knowledge of multiple PLMs for generating high-quality DP synthetic data. % with improved quality by avoiding  noisy samples. 
% Furthermore, we propose \textit{\textbf{PLM Importance Weighting}} to dynamically adjust the contributions from collaborative PLMs based on their distinct capabilities.
$2)$ Our proposed WASP leverages differentially private Top-$Q$ voting to improve the estimation of private distributions using limited private samples. It generates higher-quality data by contrasting high- and low-quality samples and dynamically assigns importance weights to PLMs, ensuring that more capable PLMs of the specific task are prioritized. 

$3)$ Experiments on $6$ well-defined natural language processing tasks with $6$ open-source and $3$ closed-source PLMs demonstrate the consistent superiority of WASP over existing methods,
% with a maximum $2.52\%$ of performance increase compared to best performing baseline, 
especially for challenging tasks. 


\begin{figure*}[!tb]
    \centering
    \includegraphics[width=0.99\linewidth]{figure/framework/framework_v6.pdf}
    \caption{Overview of WASP framework.} % \tianyuan{Naming step 1 as "...synthetic data generation" or "... data synthesis"?}\jj{data synthesis} \tianyuan{DONE}
    \label{fig:framework}
    % \vspace{-1em}
\end{figure*}


\section{Related Work}

% \tianyuan{I think the current relates work contains too much information and is too long. I will shorten it if no space left.}
% \jj{I shortened 2.1 and 2.2, and removed 2.3 and 2.4 (keeping the last sentence). FL doesn't seem too relevant here, and in-context learning probably doesn't need to be emphasized either} \tianyuan{Thank you, prof. JJ!}

\textbf{DP Synthetic Dataset.} 
% To train a dowand in-context learning nstream task model while ensuring Differential Privacy (DP) of private training data, one can apply DP-SGD~\cite{abadi2016deep} for fine-tuning. However, this approach is computationally intensive, requiring a large amount of high-quality private samples, and suffers from significant performance degradation under tight DP budgets (small $\epsilon$)~\cite{lin2024differentially}. 
The goal of generating DP synthetic data is to mimic private dataset while protecting sensitive information. %, in order to train a downstream-task model.% while ensuring Differential Privacy (DP) of private data.
% An alternative is to create a DP synthetic dataset that mimics the private dataset while protecting sensitive information, which can then be used to train downstream models. 
Although fine-tuning a PLM with DP-SGD~\cite{abadi2016deep} for data generation purpose can be effective% before generating the target DP synthetic dataset
~\cite{bommasani2019towards,putta2023differentially,flemings2024differentially,mattern2022differentially,yue2023synthetic}, it 
% still faces challenges related to computational intensity and high data requirements. 
is computationally intensive and requires a large number of high-quality private samples to reach strong performance.
Moreover, many state-of-the-art PLMs such as GPT series~\cite{openai2021gpt3-5,openai2023gpt4,hurst2024gpt4o} are also closed-source, making DP fine-tuning impractical.
%To solve above challenges, and using only the generative API of the PLM, 
%\\\jj{Need line break}

A new line of work instead relies on generative APIs of PLMs without fine-tuning, which focuses on either iterative data synthesis under DP guidance~\cite{lin2024differentially,zhao2024generate,bojkovic2024differentially} or creating DP replica of a given large private dataset~\cite{nagesh2024private}. %This eliminates the need for PLM fine-tuning, facilitating deployment and extension to powerful closed-source models like the GPT series.
Given that requiring a large global dataset for synthetic data initialization~\cite{zhao2024generate} is hard to obtain in most cases, \citet{lin2024differentially} proposes a more practical solution, Private Evolution (PE), which instead uses task-related synthetic samples. In PE, private samples are used to identify their nearest synthetic counterparts under DP protection, which then guide the growth of the DP synthetic dataset. PE is proven effective across images~\cite{lin2024differentially} and text~\cite{xie2024differentially}, and is further adapted to federated private data scenarios~\cite{hou2024pretext}.
% Private Evolution (PE)~\cite{lin2024differentially,xie2024differentially} to generate a DP synthetic dataset using only the generative API of the PLM. This eliminates the need for PLM fine-tuning, facilitating deployment and extension to powerful closed-source models like the GPT series. In PE, private samples are used to identify their nearest DP synthetic counterparts under DP protection, which then guide the expansion of the DP synthetic dataset. PE is applicable across various modalities, including images~\cite{lin2024differentially} and text~\cite{xie2024differentially}, and has been adapted for federated private data scenarios~\cite{hou2024pretext}. 
% \citet{zhao2024generate} uses a similar evolutionary generation process but relies on large scale public dataset as non-private dataset initialization which is hard to obtain. 
% Similarly, private density estimation~\cite{bojkovic2024differentially} can also be applied to create DP synthetic data but only for statistical tasks.
% \citet{nagesh2024private} seeds LLM generation with DP protected private data to produce the DP synthetic dataset
However, all these works primarily focus on using a single PLM as the generation model.

\textbf{PLM Fusion.} The combination of multiple PLMs can lead to stronger model performance~\cite{liu2023dynamic,du2023improving,wan2024knowledge,wan2024fusechat,li2024more,zou2024fusegen}. Some studies fine-tune target models with token-level fusion from PLMs as teachers during training time~\cite{wan2024knowledge,wan2024fusechat}, while others use majority voting~\cite{li2024more} or logits averaging~\cite{mavromatis2024pack} for knowledge fusion during inference. 
However, data privacy challenges persist, as training or test samples are exposed to external models. To solve this, FuseGen~\cite{zou2024fusegen} recently proposes PLM fusion in a zero-shot learning setting, utilizing only model APIs to synthesize data without accessing real private samples, thereby ensuring data privacy. However, by treating all PLMs equally, it overlooks the capability difference of individual PLMs over different tasks.
% \textbf{Federated Learning (FL)}. 
% FL was first proposed by Google~\cite{McMahan2016fl} 
% % in which a cross-device collaboration scenario is described 
% % in which millions of mobile users collaboratively train a shared model using their local private data in a decentralized manner without directly sharing their data thereby preserving privacy. 
% enabling millions of mobile users to collaboratively train a shared model using their local private data without direct data sharing, thus preserving privacy.
% % In FL, each local model is updated independently by its respective party using private data and the aggregated knowledge from each local model is incorporated into the shared global model through the exchange of model parameter updates, thereby enhancing its performance. Subsequently, the updated global model is redistributed to each local model for further iterative improvements. 
% In the era of LLMs, FL has emerged as a decentralized training paradigm utilizing distributed data~\cite{fan2023fate,kuang2024federatedscope,ye2024openfedllm,zheng2024safely,goetz2020federated}. Additionally, research has explored how LLMs can enhance FL, investigating methods like federated synthesis to protect and enhance private data~\cite{goetz2020federated,behera2022fedsyn,little2023federated,hou2024pretext}, model initialization to speed up convergence~\cite{tan2022federated,nguyen2022begin,liu2024language}, personalization improvement~\cite{yang2023efficient,li2024visual}, etc. However, these works are all motivated by the decentralized data overlooking the potential of distributed LLMs.

% \textbf{Contrastive In-context Learning.}\footnote{Works~\cite{ren2024towards,miyanishi2024multimodal} considering understanding in-context learning with contrastive learning theories are sometimes referred to using the same name, but we do not consider them here.}
% % Here, we do not consider works that 
% The idea of using contrastive information to enrich in-context learning samples has been exploited from different aspects. Samples belonging to positive and negative classes~\cite{liang2024context}, correct or wrong self-predictions of training samples during training time~\cite{mo2024cicl}, human-preferred and non-preferred question responses~\citet{gao2024customizing} have all been utilized as contrastive samples.
% \citet{mo2024cicl} exploits using correct/wrong self-predictions of training samples as in-context samples to aid test time performance. 
% \citet{liang2024context} uses samples belong to positive and negative classes as contrastive information for identifying causality and non-causality whitin a sentence.
% Our study is the first known effort to consider contrastive in-context learning for synthetic data generation, by treating samples of different qualities generated by multiple PLMs as contrastive information. \yang{this paragraph is not related to any related work. We can remove it?} 
More related works considering Contrastive In-context Learning are included in \cref{sec:appendix_related_work}.


%\section{Methodology}

\section{Preliminaries}
\textbf{Differential Privacy (DP).} %To define DP, we first define \textbf{Neighboring Datasets}. 
If two datasets $\mathcal{D}$ and $\mathcal{D}'$ differ in a single entry, %i.e. $\mathcal{D}$ contains only one extra entry $x$ compared to $\mathcal{D}'$ or vice versa, then 
they are referred to as \textit{Neighboring Datasets}. %($\mathcal{D}\sim \mathcal{D}'$). And, 
A mechanism $\mathcal{M}$ satisfies $(\epsilon,\delta)$-DP if for any neighboring datasets $\mathcal{D},\mathcal{D}'$ and any output subset $E$ of $\mathcal{M}$, it holds that~\cite{dwork2006differential}:
\begin{equation}
\text{Pr}[\mathcal{M}(\mathcal{D})\in E] \leq e^{\epsilon}\cdot\text{Pr}[\mathcal{M}(\mathcal{D}')\in E]+\delta. 
\end{equation}
Note that post-processing on the output of $(\epsilon,\delta)$-DP does not incur additional  privacy loss~\cite{dwork2014algorithmic}.

\textbf{Gaussian Mechanism.}
% In this work, we use Gaussian Mechanism~\cite{dwork2006differential} to guarantee $(\epsilon,\delta)$-DP by adding Gaussian noise following $\mathcal{N}(0,\sigma^2)$ to the transmitted statistics with $\sigma=\Delta\frac{\sqrt{2\log\left({1.25/\delta}\right)}}{\epsilon}$ and $\Delta$ being the sensitivity of $\mathcal{M}$~\cite{balle2018improving}. 
\textit{Gaussian Mechanism}~\cite{dwork2006differential} can be applied to guarantee $(\epsilon,\delta)$-DP, for any $\epsilon>0,\delta\in(0,1)$, by adding Gaussian noise following $\mathcal{N}(0,\sigma^2)$ to the transmitted statistics with $\sigma=\Delta\frac{\sqrt{2\log\left({1.25/\delta}\right)}}{\epsilon}$ and $\Delta$ being the sensitivity of $\mathcal{M}$~\cite{balle2018improving}. 


\section{Methodology}

\subsection{Problem Definition}

In this paper, we aim to generate a DP synthetic dataset $\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$ of size $N$ using a small number of private data $\mathcal{B}=\{(\mathbf{z}_{j},u_{j})\}_{j=1}^{M}$, where $M$ denotes the number of private samples, and $\mathbf{z}_{j},u_j$ denote the  feature and label of the private sample $j$, respectively. We consider  data-scarcity setting where $M$ is typically a few hundreds at most. To achieve this, we harness the collaborative power of $K$ black-box PLMs $\{\mathcal{P}_k\}_{k=1}^K$ via APIs, while protecting  private data by Gaussian DP.  
% Specifically, given a specific target task, $L\geq 1$ Private Data Parties (PDPs), denote as $\{\mathcal{C}_l\}_{l=1}^L$, each keeps its local private dataset $\mathcal{B}_l=\{(\mathbf{z}_{l,j},y_{l,j})\}_{j=1}^{M_l}$ of size $M_l$ with $\mathbf{z}_{l,j}$ being the private sample features and $y_{l,j}$ being the private label. We define $\mathcal{B}=\bigcup_{l=1}^L B_l$ as the total private dataset which is of size $M=\sum_{l=1}^L M_l$. $K>1$ black-box PLMs (denote as $\{\mathcal{P}_k\}_{k=1}^K$) that are only accessible through API are also involved in the collaboration for DP synthetic sample generation. Each $\mathcal{P}_k$ generates a DP synthetic dataset $\mathcal{D}_{k}$ that sum up to make $\mathcal{D}=\bigcup_{k=1}^{K}\mathcal{D}_{k}$ of size $N$. $(\epsilon,\delta)$-DP for $\mathcal{B}$ is preserved with standard Gaussian Mechanism by adding noise following $\mathcal{N}(0,\sigma^2)$. 
%Specifically, given a specific target task, $L=1$ Private Data Party (PDP), denote as $\mathcal{C}_1$, keeps its local private dataset $\mathcal{B}_1=\{(\mathbf{z}_{1,j},y_{1,j})\}_{j=1}^{M_1}$ of size $M_1$ with $\mathbf{z}_{1,j}$ being the private sample features and $y_{1,j}$ being the private label. 
% We define $\mathcal{B}=\bigcup_{l=1}^L B_l$ as the total private dataset which is of size $M=\sum_{l=1}^L M_l$. 
%$K>1$ black-box PLMs (denote as $\{\mathcal{P}_k\}_{k=1}^K$) that are only accessible through API are also involved in the collaboration for DP synthetic sample generation. Each $\mathcal{P}_k$ generates a DP synthetic dataset $\mathcal{D}_{k}$ that sum up to make $\mathcal{D}=\bigcup_{k=1}^{K}\mathcal{D}_{k}$ of size $N$. $(\epsilon,\delta)$-DP for $\mathcal{B}$ is preserved with standard Gaussian Mechanism by adding noise following $\mathcal{N}(0,\sigma^2)$. 
%To achieve this, we proposed \textit{\textbf{Contrastive In-context Learning}} to further avoid the generation of low-quality samples. 
% \yang{
% {\color{gray}{The original proposal of PE focuses on $1$ PLM with $1$ PDP~\cite{lin2024differentially,xie2024differentially} which is extended to $1$ PLM with multiple PDPs~\cite{hou2024pretext} to better align with the distributed setting of private data which is more often the case in real-world applications. Differently, we focus on multiple PLMs settings and place our emphasis on better cross-PLM collaboration for better generation in this work. We also cover both single-PDP and multi-PDP settings.\yang{discuss this in related work, not here}}}
%Notice that in contrast with original PE approaches which focus on generating datasets that are as close to the real dataset as possible~\cite{lin2024differentially,xie2024differentially}, we consider both the distribution similarity with the real dataset and the data quality. Specifically, 
For evaluation, we use $\mathcal{D}$ to train a Small Task-specific Model (STM) $m$ and evaluate model performance on a test dataset $\mathcal{A}$ containing real samples that is never used during training. % as our performance metric. %The training objective is: %\yang{is FID no longer a metric?}\tianyuan{yes, we do not consider it here as the FID may not continuously decrease after contrast}
% \vspace{-1em}
% \begin{equation} \label{eq:stm_train_loss}
%     \mathcal{L} = \sum_{i=1}^{N} \ell(m(\mathbf{x}_{i}),y_{i}),
% % \vspace{-0.5em}
% \end{equation}
% % \vspace{-1em}
% where $\ell$ is a common loss function, \textit{e.g.} cross-entropy loss.
% \tianyuan{Mrs. Liu, this part bellow is suggested by Prof. Peng Li.}
% The above described problem can be formated as:
% % \vspace{-1em}
% \begin{equation}
% \small
%     \begin{split}
%         % m = \mathcal{F}\left(\{\mathcal{D}_k\}_{k=1}^K\right) = \mathcal{F}\left(\mathcal{G}\left(\{\mathcal{P}_k\}_{k=1}^K,\{\mathcal{B}_l\}_{l=1}^L,\epsilon,\delta\right)\right) \\ 
%         m & = \mathcal{F}\left(\mathcal{D}\right) = \mathcal{F}\left(\mathcal{G}\left(\{\mathcal{P}_k\}_{k=1}^K,\mathcal{B}\right)\right) \ \text{s.t.} \\ 
%         \forall & \mathcal{B}'\sim\mathcal{B}, \ \text{Pr}\left[\,\mathcal{G}\left(\{\mathcal{P}_k\}_{k=1}^K,\mathcal{B}\right)\in \{\mathcal{D}\}\right] \\ 
%         &\quad\quad \leq e^{\epsilon}\cdot\text{Pr}\left[\,\mathcal{G}\left(\{\mathcal{P}_k\}_{k=1}^K,\mathcal{B}'\right)\in \{\mathcal{D}\}\right]+\delta
%     \end{split}
% % \vspace{-0.5em}
% \end{equation}
% % \vspace{-1em}
% with $\mathcal{F}$ trains $m$ from $\mathcal{D}$ and $\mathcal{G}$ is a random mechanism that satisfies $(\epsilon,\delta)$-DP for $\mathcal{B}$.

Note that our framework can be easily extended to the scenario of distributed federated data where each 
data source possesses an insufficient amount of private data and collaborates on private tasks with secure aggregation~\cite{hou2024pretext}. We present the related details
% of this implementation 
in \cref{subsec:multi_pdp}.
%Note that, different from previous works~\cite{hou2024pretext} that extend the original $1$ PLM ($K=1$) and $1$ PDP ($L=1$) setting~\cite{lin2024differentially,xie2024differentially} to $K=1, L>1$ setting for better alignment with the distributed nature of private data in real-world application, we focus on $K>1$ settings and place our emphasis on better cross-PLM collaboration for better generation in this work. We also cover both single-PDP ($L=1$) and multi-PDP ($L>1$) settings.


\subsection{Overall Workflow of WASP}
The overall workflow of WASP is depicted in \cref{fig:framework} and \cref{alg:algorithm_full_functions_singlePDP}, where four steps are taken iteratively for $T$ times. %Each step will be elaborated below with detailed algorithms included in \cref{alg:algorithm_full_functions}.
For a given task, the first iteration begins by prompting each PLM $\mathcal{P}_k$ with a zero-shot prompt, which describes the task and category label, to generate a synthetic data subset $\mathcal{D}_{k}$ of equal size $N_k$.
% initializing $\mathcal{D}$ using the same amount of synthetic samples ($N_k$) generated by each involving PLM $\mathcal{P}_k$ using a zero-shot prompt that describes the task and the label of a category. 
These samples do not contain information about $\mathcal{B}$. 
The collective dataset $\mathcal{D}=\bigcup_{k=1}^K\mathcal{D}_k$ is then voted by each private sample using a differentially private Top-$Q$ voting mechanism to identify high-quality and low-quality synthetic samples based on their similarity to the distribution of $\mathcal{B}$. These samples are then used to create a contrastive in-context learning prompt for the next round of PLM generation. The voting results are further exploit to dynamically adjust the importance weight $w_k$ for each PLM $\mathcal{P}_k$, which determines $N_k$ of the next generation round. The process repeats from here, expanding $\mathcal{D}$ with DP synthetic samples.
After $T$ iterations, $\mathcal{D}$ is used to train an STM $m$.
% In each iteration, the process begins with \textit{\textbf{Weighted Parallel Data 
%  Synthesis}}, in which each PLM $\mathcal{P}_k$ generates a DP synthetic data subset $\mathcal{D}_{k}$ of size $N_k$ in parallel, with $N_k$ %initialized as $\left[\frac{N}{TK}\right]$ and later 
% determined by weights evaluated in \textit{\textbf{PLM Importance Weighting}}. The collective dataset $\mathcal{D}=\bigcup_{k=1}^K\mathcal{D}_k$ is then voted by each private sample in \textit{\textbf{Differentially Private Top-$Q$ Voting}}, which selects the closest and furthest synthetic samples relative to the distribution of real data and sends them to \textit{\textbf{Cross-PLM Contrastive In-context Learning}}, which creates a contrastive prompt to guide PLM generation in the next round. Furthermore, the voting results are utilized in \textit{\textbf{PLM Importance Weighting}} to dynamically adjust the importance weight for each PLM. 
For notational simplicity, we omit the iteration index $t$, with $\mathcal{D}$ accumulated over iterations. 
DP guarantee of WASP is given in \cref{theorem:dp_L_equals_1} with proof included in \cref{sec:appendix_privacy_analysis}.
\begin{theorem} \label{theorem:dp_L_equals_1}
    % \vspace{-0.15em}
    WASP (\cref{alg:algorithm_full_functions_singlePDP}) satisfies $(\epsilon,\delta)$-DP.
\end{theorem}

% use $\mathcal{D}_k$ to denote the accumulated DP synthetic data subset generated by $\mathcal{P}_k$ as well as $\mathcal{D}=\bigcup_{k=1}^K\mathcal{D}_k$ to denote the currently accumulated total DP synthetic dataset, with both extending throughout the iterative process.
% Each sample in $\mathcal{D}$ kept at the Private Data Party (PDP)
% , with votes aggregated 
% in the second step, , to identify and further select out The third step, \textit{\textbf{PLM Importance Weighting}}, adjusts the importance weights for each PLM prepared for the upcoming iteration. The final step \textit{\textbf{Cross-PLM Contrastive In-context Learning}} creates a contrastive prompt using the selected synthetic samples to guide PLM generation.

\subsection{Weighted Parallel Data Synthesis} \label{subsec:method_step1}
In this stage (lines 4-6 in \cref{alg:algorithm_full_functions_singlePDP}), each PLM $\mathcal{P}_k$ generates $ N_k = \left[ (N/T)\times w_k \right]$ synthetic samples following:
\begin{equation} \label{eq:generation}
  %\mathbf{x}_{i} \sim \mathcal{P}_k(\cdot|\mathcal{T}(y_{i}),\Phi_{\mathcal{P}_k}) \, ,
  \mathbf{x}_{i} \sim \mathcal{P}_k\left(\cdot|\mathcal{T}(y_{i})\right) \, ,
\end{equation}
where $\{w_k\}_{k=1}^K$ are the weights for $\{\mathcal{P}_k\}_{k=1}^K$, $\left[ \cdot \right]$ is the rounding function, $N$ is the expected total number of synthetic samples to be generated, and $\mathcal{T}(\cdot)$ is the 
generation prompt. In the initial iteration, $\mathcal{T}(\cdot)$ is a zero-shot prompt that describes the task and provides category description, with all PLMs receiving equal weights, i.e. $\{w_k=\frac{1}{K}\}_{k=1}^K$. For later iterations, $\mathcal{T}(\cdot)$ is extended to a few-shot contrastive prompt (see \cref{subsec:method_step4}) with in-context samples selected in \cref{subsec:method_step2}, and $\{w_k\}_{k=1}^K$ dynamically assigned based on each PLM’s capability of the specific task (see \cref{subsec:method_step3}).
% \textit{contrastive task-related label-descriptive prompt},
% % (e.g. ``A movie review in positive sentiment is:'' with ``movie review'' as the task and ``positive'' as label\yang{this is not a contrastive example. refer to the section where contrastive samples are generated.}) 
% which is constructed with both high-quality and low-quality in-context samples selected in \cref{subsec:method_step2}. The prompt describes the task, provides category description, and contains explicit contrastive instructions for high- and low-quality samples (see \cref{subsec:method_step4} for details and \cref{sec:appendix_prompt} for examples). 
% Note that in the initial iteration, $\mathcal{D}$ is generated with a zero-shot prompt, since no synthetic sample is available for sample selection, and all PLMs receive equal weights with $w_k=\frac{1}{K}$. The assignment of $\{w_k\}_{k=1}^K$ will be discussed in detail in \cref{subsec:method_step3}. 
The collective synthetic dataset $\mathcal{D}=\bigcup_{k=1}^K\mathcal{D}_k$ is then sent to the private data party.


\begin{algorithm}[tb]
% \small
\caption{WASP} % A complete training process of 
\label{alg:algorithm_full_functions_singlePDP}

\begin{flushleft}
\textbf{Input:} \\
$K$ PLMs $\{\mathcal{P}_{k}\}_{k=1}^K$ with empty synthetic dataset $\{\mathcal{D}_k\leftarrow\emptyset\}_{k=1}^K$; 1 data party with private dataset $\mathcal{B}$ of size $M$ belonging to $C$ categories; 
number of in-context samples $S$;
number of iterations $T$ taken to obtain in total $N$ synthetic samples; 
initialized PLM weights ${\{w_{k}=1/K\}}_{k=1}^K$; 
learning rate $\eta$;
DP privacy parameters $\epsilon,\delta$;
training time unseen test dataset $\mathcal{A}$;
random initialized STM $m_{(0)}$.
\\
\textbf{Output:} STM $m$. % $\tilde{m}$.
\end{flushleft}

\begin{algorithmic}[1]
    \STATE Initialize in-context feedback samples $\hat{\mathcal{D}}^n \leftarrow \emptyset, \hat{\mathcal{D}}^f \leftarrow \emptyset$.
    \STATE Calculate Gaussian noise $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon}$.
    \FOR{$t=0$ {\bfseries to} $T-1$}
        \FOR{$k=1$ {\bfseries to} $K$ {\bfseries in parallel}}
            \STATE $\mathcal{D}_k \leftarrow$ \verb|WeightedSynDataGeneration(|$\mathcal{D}_k$, $\hat{\mathcal{D}}^n$, $\hat{\mathcal{D}}^f$, $\left[ (N/T)\times w_{k} \right]$, $C$\verb|)|.
        \ENDFOR
        \STATE $\mathcal{D} \leftarrow \cup_{k=1}^{K}\mathcal{D}_k$.
        % \STATE $\tilde{m} \leftarrow$ \verb|STMTraining(|$\mathcal{D}$, $m_{(0)}$\verb|)|.
        % \FOR{$l=1$ {\bfseries to} $L$ {\bfseries in parallel}}
        %     % \STATE $\tilde{m}_l \leftarrow$ \verb|STMTraining(|$\mathcal{D}\cup\mathcal{B}_l$, $m_{(0)}$\verb|)|.
        %     \STATE $H^{n}_l, H^{f}_l \leftarrow$ \verb|DP_PrivateVoting(|$\mathcal{D}$, $\mathcal{B}_l$, $Q$, $\sigma$\verb|)|.
        % \ENDFOR
        \STATE $H^{n}, H^{f} \leftarrow$ \verb|DP_PrivateVoting(|$\mathcal{D}$, $\mathcal{B}$, $Q$, $\sigma$\verb|)|.
        % \STATE $m \leftarrow$ \verb|FedAVG(|$\{\tilde{m}_l\}_{l=1}^L$, $\{\mathcal{B}_l\}_{l=1}^L$, $L$\verb|)|.
        % \STATE $H^{n}\leftarrow\sum_{l=1}^{L} H^{n}_l$; $H^{f}\leftarrow\sum_{l=1}^{L} H^{f}_l$.
        \STATE $\hat{\mathcal{D}}^n, \hat{\mathcal{D}}^f \leftarrow$ \verb|SampleSelection(|$\mathcal{D}$, $H^{n}$, $H^{f}$, $S$, $C$\verb|)|.
        \STATE $\{w_k\}_{k=1}^K \leftarrow$ \verb|PLMScoring(|$H^{n}$, $\{\mathcal{D}_k\}_{k=1}^K$\verb|)|.
    \ENDFOR
    \STATE $m \leftarrow$ \verb|STMTraining(|$\mathcal{D}$, $m_{(0)}$, $\eta$\verb|)|.
    % \STATE $\tilde{m} \leftarrow$ \verb|WeightAdjustSTMTraining(|$\cup_{k=1}^{K}\mathcal{D}_k$, $m_{(0)}$, $\cup_{k=1}^{K}\left\{\{w_{k,i}^{(0)}\}_{i=1}^{N}\right\}$, $E_1$, $E_2$\verb|)|.
\end{algorithmic}
\end{algorithm}


\subsection{Differentially Private Top-$Q$ Voting} \label{subsec:method_step2} 
%%%%%%%%%% using L>1 for description %%%%%%%%%%
% In this step, each PDP $\mathcal{C}_l$ uses each of its local private samples $(\mathbf{z}_{l,j},y_{l,j})\in \mathcal{B}_l$ to vote for their nearest and furthest synthetic samples within $\mathcal{D}$. We use $\ell_2$ distance as the distance measurement, i.e. $d(\mathbf{z}_{l,j},\mathbf{x}_{i})=||\varphi(\mathbf{z}_{l,j})-\varphi(\mathbf{x}_{i})||_2$ between synthetic sample $\mathbf{x}_{i}$ and private sample $\mathbf{z}_{l,j}$ (Line 10 in \cref{alg:algorithm_full_functions}). $\varphi$ is the embedding model. 
% When voting for the nearest synthetic samples, to address the challenge of inadequate private sample generation ($M \in [100,300]$), which existing PE algorithms~\cite{lin2024differentially,xie2024differentially,hou2024pretext} do not handle well, we propose a novel voting strategy to produce a more precise private sample distribution measurement as follows: each private sample first ranks all the currently generated synthetic samples by distance $d$, then identifies the Top-$Q$ synthetic samples of smallest distance and finally assigns $Q$ votes with decreasing voting weights $1,\frac{1}{2},\dots,\frac{1}{2^{Q-1}}$ respectively for these samples. We use $Q=8>1$ in our work. The same process is applied when voting for the furthest synthetic samples, where the Top-$Q$ samples with the largest distances are selected for voting. To guarantee $(\epsilon,\delta)$-DP for private samples, standard Gaussian noises following $\mathcal{N}(0,\sigma^2)$ with $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$ are added to the voting histograms which finally produces $H^{n}_l,H^{f}_l$ (see \verb|DP_PrivateVoting| in \cref{alg:algorithm_functions_1} for detail) following the Gaussian DP mechanism. The method for choosing the value of $\sigma$ in detailed in \cref{sec:appendix_privacy_analysis}. Finally, $\{H^{n}_l\}_{l=1}^L,\{H^{f}_l\}_{l=1}^L$ of all PDPs are then aggregated (summed up) respectively to get the global \textit{Nearest Histogram} $H^{n}$ and \textit{Furthest Histogram} $H^{f}$.
%%%%%%%%%% using L>1 for description %%%%%%%%%%

% We start with $L=1$ and then extend it to $L>1$. 
% This step aims to obtain a more precise generation guidance with limited private samples, in which case the original PE algorithm fails\yang{restate to enhance the conclusion. add figure ref.}
As shown in \cref{subfig:pe_random_fid_and_acc}, with limited real private samples, noisy estimations of the real private data distribution cause the original PE algorithm to fail in generating synthetic samples that resemble private data. Our aim is to improve distribution estimations and generation guidance in this scenario.
To achieve this, unlike previous works~\cite{lin2024differentially,xie2024differentially,hou2024pretext} that assign only $1$ vote per private sample, we propose a Top-$Q$ voting mechanism with decaying weights. This approach maximizes the use of limited private samples by giving weighted votes to the Top-$Q$ nearest and furthest synthetic samples relative to the private sample.
Specially, we first compute the pair-wise distance between each of the private samples $(\mathbf{z}_j,u_j)\in \mathcal{B}$ and each synthetic sample $(\mathbf{x}_{i},y_{i}) \in \mathcal{D}$ if they possess the same label, i.e. $y_i=u_j$. %(denote this synthetic subset as $\mathcal{D}^{[u_j]}$) %\yang{why do we need to recalculate the distance from previous iterations? nothing changes for them.} \tianyuan{Ah this make sense. But as this process if fast and we latter ranks them altogether, for presentation simplicity, I just used $\mathcal{D}$} 
Using $\ell_2$ distance as measurement, we have:
\begin{equation} \label{eq:distance}
\begin{split}
    d(\mathbf{z}_{j},\mathbf{x}_{i})=||\varphi(\mathbf{z}_{j})-\varphi(\mathbf{x}_{i})||_2 \,, \\
    \forall \,\, j=1,\dots,M; \,\, (\mathbf{x}_{i},y_i) \in \mathcal{D}^{[u_j]} \, ,
\end{split}
\end{equation}
% between synthetic sample $\mathbf{x}_{i}$ and private sample $\mathbf{z}_{j}$ (Line 10 in \cref{alg:algorithm_full_functions}). 
% where $\varphi$ is the embedding model.
% \yang{add range of $j$ and $i$ in equation}.
% \yang{By the way, need to add FL in related works.} 
where $\varphi$ denotes a pre-trained sentence embedding model and $\mathcal{D}^{[u_j]}$ denotes the subset of $\mathcal{D}$ which has a label that equals to $u_j$. Next, we use each private sample $(\mathbf{z}_j,u_j)\in \mathcal{B}$ to vote for its Top-$Q$ nearest and Top-$Q$ furthest synthetic samples within $\mathcal{D}^{[u_j]}$ based on \cref{eq:distance}.
% \yang{at time $t$, $\mathcal{D}$ is not fully generated, need to either use another notation or explain the dynamic nature of $\mathcal{D}$.}\tianyuan{Explained in the beginning of Section 3.3. I prefer to explain the dynamic nature of $\mathcal{D}$ as the notation is already quite complicated}
The indices of the synthetic samples selected by each $(\textbf{z}_{j},u_j) \in \mathcal{B}$ are:
\begin{equation} \label{eq:distance_based_scoring}   
% \small
\begin{split}
    % [n_{j,1},\dots,n_{j,Q}] &\leftarrow \text{top}Q\text{Smallest} \, 
    % \{d(\textbf{z}_j, \textbf{x}_{i})\}_{(\mathbf{x}_{i},y_i) \in \mathcal{D}^{[u_j]}}, \\
    % [f_{j,1},\dots,f_{j,Q}] &\leftarrow \text{top}Q\text{Largest} \, 
    % \{d(\textbf{z}_j, \textbf{x}_{i})\}_{(\mathbf{x}_{i},y_i) \in \mathcal{D}^{[u_j]}}. \\
    [n_{j,1},\dots,n_{j,Q}] &\leftarrow \arg\mathrm{top}Q\mathrm{Smallest} \left( \, d(\textbf{z}_j, \textbf{x}_{i})_{(\mathbf{x}_{i},y_i)\in \mathcal{D}^{[u_j]}} \right), \\
    [f_{j,1},\dots,f_{j,Q}] &\leftarrow \arg\mathrm{top}Q\mathrm{Largest} \left( \, d(\textbf{z}_j, \textbf{x}_{i})_{(\mathbf{x}_{i},y_i)\in \mathcal{D}^{[u_j]}} \right) .
\end{split}
\end{equation}
where functions $\arg\mathrm{top}Q\mathrm{Smallest}$ and $\arg\mathrm{top}Q\mathrm{Largest}$ return the indices of the Top-$Q$ samples with the smallest and largest $d(\textbf{z}_j, \textbf{x}_{i})$, respectively, with $n_{j,1},\dots,n_{j,Q},f_{j,1},\dots,f_{j,Q}$ denoting the index of 
 selected samples. % \yang{need to explain how the Top-$Q$ functions defined exactly and the definition and dimensions of $n_{j,1},\dots,n_{j,Q}$... } 
% We experimentally investigate the impact of $Q$ in \cref{subsec:experiments_Q}. 
% and adopts $Q=8$ in our main %is effective to address the challenge of inadequate private sample challenge. %generation ($M \in [100,300]$), which existing PE algorithms~\cite{lin2024differentially,xie2024differentially,hou2024pretext} do not handle well, and 
%propose a novel voting strategy to produce a more precise private sample distribution measurement as follows: 
%\tianyuan{when voting for the nearest (furthest) synthetic samples, each private sample first ranks all the currently generated synthetic samples by distance $d$, then identifies the Top-$Q$ synthetic samples of smallest (largest) distance and finally 
To utilize the relative ranking information, as well as to guarantee a controllable function sensitivity for DP protection, we assign decreasing voting weights $1,\frac{1}{2},\dots,\frac{1}{2^{Q-1}}$ to each of the Top-$Q$ selected samples when producing the voting histograms, \textit{Nearest Histogram} $H^{n}$ and \textit{Furthest Histogram} $H^{f}$. This can be formulated as:
\begin{equation} \label{eq:voting_q}
% \small
\begin{split}
    H^{n}[n_{j,q}] \leftarrow H^{n}[n_{j,q}]+\frac{1}{2^{q-1}}, \, H^{f}[f_{j,q}] \leftarrow H^{f}&[f_{j,q}]+\frac{1}{2^{q-1}} \\
    \forall (\textbf{z}_j,u_j)\in\mathcal{B}, \, \forall \, q \in [1,\dots,Q],
\end{split}
\end{equation}
with $H^n,H^f$ each initialized as $[0,\dots,0]$ of length $|\mathcal{D}|$. 
% We use $Q=8>1$ in our work to produce a more precise private sample distribution measurement when only a small amount of real private sample is available. 
% The same process is applied when voting for the furthest synthetic samples, where the Top-$Q$ samples with the largest distances are selected for voting.

To further guarantee $(\epsilon,\delta)$-DP for private samples,  Gaussian noises following $\mathcal{N}(0,\sigma^2)$ with $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon}$ are added to $H^{n},H^{f}$:
% (see \verb|DP_PrivateVoting| in \cref{alg:algorithm_functions_1} for detail):
\begin{equation}\label{qq:dp_noise_addition}
% \small
    H^{n} \leftarrow H^{n}+\mathcal{N}(0,\sigma^2I_{|\mathcal{D}|}), \, H^{f} \leftarrow H^{f}+\mathcal{N}(0,\sigma^2I_{|\mathcal{D}|})\,,
\end{equation}
%The method for choosing the value of $\sigma$ and 
where $I_{|\mathcal{D}|}$ represents the identity matrix of size $|\mathcal{D}|\times|\mathcal{D}|$.
%Theoretical analysis and proofs of our privacy guarantee are detailed in \cref{sec:appendix_privacy_analysis}. 

Based on $H^{n},H^{f}$, for each category $c$,  we select low-quality samples with the highest votes in $H^f$ (largest distance to private samples in $\mathcal{B}$), denoted as $\hat{\mathcal{D}}^{f,[c]}$, 
% and add them into the feedback prompt $\mathcal{T}$, 
alongside high-quality samples with the highest votes in $H^n$ (nearest to private samples in $\mathcal{B}$), denoted as $\hat{\mathcal{D}}^{n,[c]}$, %Therefore, a contrastive prompt is ultimately created.}\yang{add motivations.} 
%in order to fully utilize low-quality synthetic samples,
%To perform sample selection, the top-ranking samples according to $H^n$ and $H^f$ are selected to form sample subsets $\hat{\mathcal{D}}^{n,[c]}$ and $\hat{\mathcal{D}}^{f,[c]}$ \tianyuan{containing high-quality and low-quality samples that are later used as in-context feedback samples} 
% (Line 14 in \cref{alg:algorithm_full_functions}) 
following:
% \yang{add formal equations here.}
\begin{equation} \label{eq:incontext_sample_selection}
% \scriptsize
\begin{split}
    H^{n,[c]} =& \left\{ H^n[i] \,\big|\, (\mathbf{x}_{i},y_i) \in \mathcal{D}^{[c]} \right\}, \\
    H^{f,[c]} =& \left\{ H^f[i] \,\big|\, (\mathbf{x}_{i},y_i) \in \mathcal{D}^{[c]} \right\}, \\
    \hat{\mathcal{D}}^{n,[c]} =& \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D}^{[c]} \,\big|\, H^n[i] \text{ is in the top-}S \text{ values of } H^{n,[c]} \right\}, \\
    \hat{\mathcal{D}}^{f,[c]} =& \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D}^{[c]} \,\big|\, H^f[i] \text{ is in the top-}S \text{ values of } H^{f,[c]} \right\}, 
\end{split}
\end{equation}
where $S$ is the amount of samples to select and $H^{n,[c]},H^{f,[c]}$ denote the sets of the nearest and furthest voting results of samples belonging to category $c$. % \yang{define all the notations new here. unclear what is $H^n_i$, S,  $H^{n,[c]}$...} 
$\hat{\mathcal{D}}^{n} = \bigcup_{c=1}^C \hat{\mathcal{D}}^{n,[c]}, \hat{\mathcal{D}}^{f} = \bigcup_{c=1}^C \hat{\mathcal{D}}^{f,[c]}$ are the total sets of high- and low-quality samples respectively.
Note that we do not limit the origin of the selected samples, and synthetic samples generated by different PLMs can all be included in $\hat{\mathcal{D}}^n$ and $\hat{\mathcal{D}}^f$.
% \yang{this part should be combined with the section above}
%\jj{Edited till here}


% \subsubsection{Contrastive Sample Selection and Private Data Assisted PLM Weighting} 
\subsection{PLM Importance Weighting} 
\label{subsec:method_step3}
% These $2$ small steps are placed together as they both rely on $H^{n}, H^{f}$. 

% \yang{discuss the limitation of previous works on PLM fusion first - they use equal weight and ignores the varying contributions of different PLMs.}
Previous studies on API-based multi-PLM fusion~\cite{li2024more,zou2024fusegen} % overlooks the varying capabilities of distinct PLMs, treating them equally in the collaboration with equal weights.
often treat involved PLMs equally.
%To tackle the challenge of precise PLM collaboration and align each PLM's contribution with its task-specific capabilities, 
However, as shown in \cref{subfig:pe_variance} and \cref{fig:pe_q8_contrast_comparison} in \cref{subsec:appendix_single_plm_topQ_contrast}, different PLMs exhibit varying generation capabilities, leading to uneven synthetic data quality. This encourages 
assigning customized weights for each PLM to enhance their contributions. Therefore, we introduce a PLM weighting strategy based on the quality of their generated synthetic data, which is measured by their similarity to private samples.
% assigning customized weights to each PLM to further promote contribution. To account for different contributions and task-specific capabilities of different PLMs, we introduce a weighting strategy for PLMs based on the quality of their synthetic data, measured by their similarity with private samples.
% %Therefore, in \textit{\textbf{PLM Importance Weighting}}, weights $\{w_k\}_{k=1}^K$ that sum up to $1.0$ are assigned to each $\mathcal{P}_k$.
% % (Line 13 in \cref{alg:algorithm_full_functions}).
% % As shown in \cref{fig:aug_pe_text_length_convergence,fig:pe_variance} in the Appendix, different PLMs have distinct generation distributions 
% %In WASP, we assess how valuable a PLM $\mathcal{P}_k$ is by evaluating the average nearest vote \tianyuan{gained by each synthetic sample $\mathcal{P}_k$ generated, i.e. the votes of a sample $(\textbf{x}_{i},y_{i})\in\mathcal{D}_k$ generated by $\mathcal{P}_{k}$ is defined as $H^n[i]$ given by the global \textit{Nearest Histogram} $H^{n}$.} 

Since the \textit{Nearest Histogram} $H^{n}$ obtained in \cref{eq:voting_q} quantifies the similarity between each synthetic sample and private samples, we simply aggregate the histogram values of each synthetic sample with source PLM $\mathcal{P}_k$ to obtain the weight $w_k$ of the PLM $\mathcal{P}_k$ for the upcoming generation iteration. That is,
% \yang{still not clear. what is $i$ here? The current notations throughout the paper are in a mess. Please work step-by-step from the beginning by adding key equations and notations to explain each step. Try your best to be consistent.}
% Define $\hat{N}_k \leftarrow |\mathcal{D}_k|$, \yang{why do we need to define a new notation $\hat{N}_k$ if it is exactly is another notation $|\mathcal{D}_k|$??} the size of current total synthetic samples given by $\mathcal{P}_k$, 
%Therefore, we first calculate the values (normalized votes) of each synthetic sample: 
% \begin{equation} \label{eq:H_normalization}
% \small
%     % s_{k,i} = \frac{H^n[i]}{\sum_{i'=1}^{|\mathcal{D}|} H^n[i']} \, ,
%     s_{i} = H^n[i] \,\big/ \sum_{i'=1}^{|\mathcal{D}|} H^n[i'] \, , 
% \end{equation}
% %and then calculates the PLM weight used in the next generation iteration as for $\mathcal{P}_k$: 
% \begin{equation}
% \small
%     w_k = \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{|\mathcal{D}_{k}| / \sum_{k'=1}^K |\mathcal{D}_{k'}|} = \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{|\mathcal{D}_{k}| / |\mathcal{D}|} \, .
% \end{equation}
\begin{equation} \label{eq:weight_calculation}
% \small
\begin{split}  
    % s_{k,i} = \frac{H^n[i]}{\sum_{i'=1}^{|\mathcal{D}|} H^n[i']} \, ,
    s_{i} &= \frac{H^n[i] }{\sum_{i'=1}^{|\mathcal{D}|} H^n[i']},\\
    w_k &= \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{\frac{|\mathcal{D}_{k}|}{\sum_{k'=1}^K |\mathcal{D}_{k'}|}} = \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{|\mathcal{D}_{k}| / |\mathcal{D}|} \, .
\end{split}
\end{equation}
% See \verb|PLMScoring| in \cref{alg:algorithm_functions_1} for detailed algorithm.


\subsection{Cross-PLM Contrastive In-context Learning (ICL)} \label{subsec:method_step4}
Inspired by the observation that low-quality samples still exist in DP synthetic dataset given by PE (see \cref{tab:examples_bad_samples} in \cref{subsec:appendix_good_bad_samples}), we select cross-PLM contrastive samples from $\hat{\mathcal{D}}^n$ and $\hat{\mathcal{D}}^f$ (obtained in \cref{subsec:method_step3}), and use them to create a \textit{contrastive task-related label-descriptive prompt} $\mathcal{T}(\cdot)$ to perform cross-PLM contrastive ICL. %\yang{add some discussions on what leads us to construct prompts this way, e.g. inspired by previous works that perform each of these following steps?}
%, we directly adds them to the prompt seeking to avoid their generation.}
$\mathcal{T}(\cdot)$ describes the task, provides category description, and contains explicit contrastive instructions for high- and low-quality samples. It contains the following sequential instructions: $(1)$ analyze the difference between low- and high-quality samples; $(2)$ ensure the new sample is better in quality and closer to real private distribution than the high-quality samples, and is further away from the low-quality samples than the high-quality samples; $(3)$ generate a new sample which is diverse in expression compared to the given high-quality samples. 
Note that to improve the generation diversity, for each generation we perform random sample selection to draw $50\%$ of samples respectively from $\hat{\mathcal{D}}^{f,[c]}$ and $\hat{\mathcal{D}}^{n,[c]}$ to construct the final in-context samples for $\mathcal{T}(c)$. %We use $\beta=0.5$ in our experiments. 
%Note in our algorithm, for each class $c$, we aim to add in total $S$ in-context samples in our feedback prompt $\mathcal{T}(c)$, and for each we select the top-$S$ synthetic samples to form $\hat{\mathcal{D}}^{n,[c]}$ and $\hat{\mathcal{D}}^{f,[c]}$ (see \cref{eq:incontext_sample_selection}), resulting in $|\hat{\mathcal{D}}^{n,[c]}|=|\hat{\mathcal{D}}^{f,[c]}|=S$.
% \yang{what does this mean? is that a definition?} 
% the number of in-context samples carried by $\mathcal{T}$. 
%Random sample selection is performed to select $\lfloor S/2\rfloor$ and $S-\lfloor S/2\rfloor$ synthetic samples respectively from $\hat{\mathcal{D}}^{f,[c]}$ and $\hat{\mathcal{D}}^{n,[c]}$ for constructing $\mathcal{T}(c)$.\yang{I don't understand why this is necessary or significant.} \tianyuan{We need to specify how $S$ contrastive in-context samples are selected from $|\hat{\mathcal{D}}^n \cup \hat{\mathcal{D}}^f| = 2S$.}\yang{then why not directly select $S/2$ in equation 7 and avoid this whole discussion?} \tianyuan{We did this in our experiments to increase diversity.}
Also, different from PE algorithms series, we choose not to 
% use the \verb|VARIATION_API| for varying 
vary 
one existed synthetic sample each time, but to encourage diverse sample generation using $S$ demonstrations at once. %\yang{what does "more demonstration" mean here? do you mean the contrastive part?}\tianyuan{No, I want to say that, $S$>1 for our prompt.} in the prompt. 
Prompt examples can be found in \cref{tab:appendix_prompt} in \cref{sec:appendix_prompt}.


\subsection{WASP in Federated Data Setting} \label{subsec:multi_pdp}

So far we have built our algorithms under single data-party setting, which can be easily extended to federated data scenario~\cite{hou2024pretext}, where each data party possesses an insufficient amount of private data and collaborates on private tasks. This scenario is very common in the real world, such as collaborations between medical companies. %\yang{add one sentence on the importance of implementing this scenario, such as it is more realistic and can solve more problems etc.} 
In this setting, we consider $L$ data parties $\{\mathcal{C}_l\}_{l=1}^L$, each possessing a real private dataset $\mathcal{B}_l = \left\{(\mathbf{z}_{l,j},y_{l,j})\right\}_{j=1}^{M_l}$ of size $M_l$. These data parties aim to collaboratively generate a DP synthetic dataset  while preserving local data privacy.
% ...\yang{please formally describe the setting, i.e.their collaborative goals and constrains. define $\mathcal{C}_l$ $B_l$}
%We also define $\mathcal{B}=\bigcup_{l=1}^L B_l=\bigcup_{l=1}^L \left\{(\mathbf{z}_{l,j},y_{l,j})\right\}_{j=1}^{M_l}$ as the total private dataset which is of size $M=\sum_{l=1}^L M_l$.
% with $M_l$ being the size of $\mathcal{B}_l$. 
The full algorithm is provided in \cref{alg:algorithm_full_functions}.
% \yang{move this to a separate section of FL.}

When extending to federated data setting, each party $\mathcal{C}_l$ uses its local private samples in $\mathcal{B}_l$ to perform DP Top-Q voting with $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$ to guarantee privacy. The produced local nearest and furthest voting histograms $\{H^{n}_l\}_{l=1}^L,\{H^{f}_l\}_{l=1}^L$ are then securely aggregated~\cite{bonawitz2016practical} before sent to a central server following: 
% \yang{mention and officially define secure aggregation, also explain why aggregating is enough, what is the implication on privacy for aggregation, any other aggregating strategies which may be adopted? }
\begin{equation}\label{eq:sum_histogram}
\small
    H^{n}\leftarrow\sum_{l=1}^{L} H^{n}_l, \, H^{f}\leftarrow\sum_{l=1}^{L} H^{f}_l.
\end{equation}
We adopt an honest-but-curious threat model where the server only has access to the aggregated histograms $H^{n}$ and $H^{f}$, but not individual ones. We also assume that all data parties participate in the aggregation and therefore aims to ensure sample-level $(\epsilon,\delta)$-DP of $\mathcal{B}$ (see \cref{sec:appendix_privacy_analysis}). Note that, WASP can be easily extended 
% to partial participation setting with 
to ensure user-level DP, with discussions and results included in \cref{subsec:appendix_user_level_dp_results}.
% \yang{double check that they didn't use or study partial participation.}\tianyuan{Mrs. Liu, they use user-level dp, that is dp for partial participation (so we do not follow then). However, they use $1250$ clients with each owning no more than $8$or $16$ samples (depending on the dataset), so they are cross-device, and we are cross-silo.} \yang{will our setting be compatible with partial participation? Be prepared to answer that cause the reviewer may ask or request experiments.} \tianyuan{sure, I will prepare experiments for $L=150, M=500$ with each party controlling no more than $8$ samples. And I am trying to include some results in the appendix.}
% \yang{add the privacy properties for the aggregated histograms in DP.} \tianyuan{Added.}

% \subsection{Privacy Analysis} \label{sec:privacy_analysis}
% \begin{theorem} \label{theorem:dp_L_equals_1}
%     WASP (\cref{alg:algorithm_full_functions_singlePDP}) satisfies $(\epsilon,\delta)$-DP.
% \end{theorem}
% Proof for \cref{theorem:dp_L_equals_1} is included in \cref{sec:appendix_privacy_analysis}.

\section{Experiments} \label{sec:experiments}
% \yang{need to add results on different number of private samples to demonstrate the effective regime that our methods are better.} \tianyuan{Adding}

\subsection{Settings} \label{subsec:experimental_settings}
\textbf{Models.} In this work, 6 open-source PLMs and 3 closed-source PLMs are considered. Open-source PLMs include GPT-2-xl (GPT-2)~\cite{radford2019language}, Llama-2-7b-chat-hf (Llama-2)~\cite{touvron2023llama2}, Vicuna-7b-1.5v (Vicuna)~\cite{vicuna2023}, OPT-6.7b (OPT)~\cite{zhang2022opt}, ChatGLM3-6b-base (ChatGLM3)~\cite{du2022glm}, and Flan-T5-xl (Flan-T5)~\cite{chung2022scaling}. Close-source PLMs include GPT-3.5-turbo-instruct (GPT-3.5)~\cite{openai2021gpt3-5}, GPT-4-turbo-preview (GPT-4)~\cite{openai2023gpt4}, and GPT-4o~\cite{hurst2024gpt4o}. For STM, we use pre-trained bert-base-uncased (BERT) model and further fine-tune it on downstream classification tasks using $\mathcal{D}$. %Evaluation is performed using a human-annotated real-world dataset (test dataset) $\mathcal{A}$ \yang{exactly where does it come from?}that is never used during training.
We use sentence-t5-base~\cite{ni2022sentencet5base} as the embedding model $\varphi$.

\textbf{Datasets.} We evaluate on $6$ widely used tasks: $1)$ IMDb~\cite{maas2011learning_imdb} ($2$ categories) for movie-review semantic analysis task; $2)$ Yelp-Category~\cite{yelpopendataset} ($10$ categories) for business-review item field classification task; $3)$ Yelp-Rating~\cite{yelpopendataset} ($5$ categories) for business-review rating classification task; $4)$ Openreview-Category~\cite{xie2024differentially} ($12$ categories) for paper-review classification by research area task; $5)$ Openreview-Rating~\cite{xie2024differentially} ($5$ categories) for paper-review classification by review rating task; and $6)$ Banking ($10$ categories selected from Banking77~\cite{Casanueva2020banking77}) for online-banking queries field classification task. $\mathcal{B}$ is randomly drawn from the training sets of these datasets with their test sets used to evaluate trained STM. 

\textbf{Baselines.} We compare the WASP framework to $4$ baselines: $1)$ Aug-PE~\cite{xie2024differentially}, the original PE algorithm specialized for text modality; $2)$ Pre-Text~\cite{hou2024pretext}, which applies PE to federated private data setting; $3)$ OnlyPrivate, the centralized training method relying merely on $\mathcal{B}$ without DP ($\epsilon=\infty$), which provides a performance upper-bound of using no synthetic data; $4)$ FuseGen~\cite{zou2024fusegen}, which generates synthetic data in a zero-shot manner without accessing private samples. %As Aug-PE focus on $K=1,L=1$ setting, we do not compare with it when $L>1$. Similarly, we do not compare with Pre-Text when $L=1$ as it degrade to Aug-PE under such scenario. 
% \yang{prioritize the discussion on comparison on Aug-PE first. Mention others like Fusegen later.}

\begin{table*}[t]
\caption{Evaluation of downstream STM accuracy using $6$PLMs, $L=1$. % under ($4.0,1\times10^{-5}$)-DP setting. Note that, DP is not used in ``OnlyPrivate''. %Following Aug-PE, only $\mathcal{D}$ is used for STM training for Aug-PE and WASP.
\textbf{Best} and \underline{second best} results are marked.
% Best result is marked as \textbf{bold} and the second best marked with \underline{underline} for each task.
}
\label{tab:main_results_1}
% \vskip -0.15in
% \vspace{-1em}
\begin{center}
\begin{small}
% \vspace{0.3em}
% \begin{sc}
    % \resizebox{1\linewidth}{!}{
    % \begin{tabular}{c|c||c|cc|cc|c}
    % \toprule
    %     \multicolumn{2}{c||}{} & \multirow{2}{*}{IMDb} & \multicolumn{2}{c|}{Yelp} & \multicolumn{2}{c|}{Openreview} & \multirow{2}{*}{Banking} \\
    %     \multicolumn{2}{c||}{} & ~ & Category & Rating & Area & Rating & ~ \\
    % \midrule
    %     \multicolumn{2}{c||}{OnlyPrivate} & 50.00 & 5.69 & 35.57 & 6.56 & 26.20 & 13.75 \\
    %     % \cmidrule
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multicolumn{2}{c||}{FuseGen} & \underline{89.07} & \underline{63.38} & 57.96 & 24.70 & 34.57 & 78.75 \\
    %     % \cmidrule
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multirow{6}{*}{\shortstack{Aug-PE}} & GPT-2 & 85.38 & 62.33 & 45.28 & 31.45 & 24.12 & 75.63 \\
    %      & Llama-2 & 85.77 & 60.18 & 47.42 & 32.67 & 34.78 & 84.63 \\
    %      & Vicuna & 82.76 & 63.28 & 54.42 & 32.27 & 30.66 & 86.75 \\
    %      & OPT & 83.86 & 62.71 & 50.81 & 34.64 & 25.30 & 79.25 \\
    %      & ChatGLM3 & 85.82 & 55.06 & 55.17 & 33.81 & 32.49 & 88.50 \\
    %      & Flan-T5 & 89.00 & 62.06 & \underline{58.69} & 34.54 & 35.42 & 81.25 \\
    %     % \cmidrule
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multicolumn{2}{c||}{WASP (Ours)} & \textbf{89.52} & \textbf{63.91} & \textbf{61.21} & \textbf{34.99} & \textbf{37.10} & \textbf{88.75} \\
    %     % \multicolumn{2}{c||}{(Ours)$+$Real} & 89.63 & 64.94 & 59.68 & 30.99 & 32.56 & \\
    % \bottomrule
    % \end{tabular}
    \begin{tabular}{c|c|c|cc||c|cc|cc|c}
    \toprule
        \multicolumn{2}{c|}{} & \multirow{2}{*}{Privacy} & \multirow{2}{*}{\,\,$|\mathcal{B}|$\,\,} & \multirow{2}{*}{$|\mathcal{D}|$} & \multirow{2}{*}{IMDb} & \multicolumn{2}{c|}{Yelp} & \multicolumn{2}{c|}{Openreview} & \multirow{2}{*}{Banking} \\
        \multicolumn{2}{c|}{} & ~ & ~ & ~ & ~ & Category & Rating & \,\,Area\,\, & Rating & ~ \\
    \midrule
        \multicolumn{2}{c|}{OnlyPrivate} & $\epsilon=\infty$ & $100$ & - & 50.00 & 5.69 & 35.57 & 6.56 & 22.20 & 13.75 \\
        % \cmidrule
    \midrule
        % \hline
        % \\[-1em]
        \multicolumn{2}{c|}{FuseGen} & Absolutely Private & - & $6,000$ & \underline{89.07} & \underline{63.38} & 57.96 & 24.70 & 34.57 & 78.75 \\
        % \cmidrule
    \midrule
        % \hline
        % \\[-1em]
        \multirow{6}{*}{\shortstack{Aug-PE}} & GPT-2 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 85.38 & 62.33 & 45.28 & 31.45 & 24.12 & 75.63 \\
         & Llama-2 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 85.77 & 60.18 & 47.42 & 32.67 & 34.78 & 84.63 \\
         & Vicuna & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 82.76 & 63.28 & 54.42 & 32.27 & 30.66 & 86.75 \\
         & OPT & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 83.86 & 62.71 & 50.81 & \underline{34.64} & 25.30 & 79.25 \\
         & ChatGLM3 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 85.82 & 55.06 & 55.17 & 33.81 & 32.49 & \underline{88.50} \\
         & Flan-T5 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 89.00 & 62.06 & \underline{58.69} & 34.54 & \underline{35.42} & 81.25 \\
        % \cmidrule
    \midrule
        % \hline
        % \\[-1em]
        \multicolumn{2}{c|}{WASP (Ours)} & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & \textbf{89.52} & \textbf{63.91} & \textbf{61.21} & \textbf{34.99} & \textbf{37.10} & \textbf{88.75} \\
        % \multicolumn{2}{c||}{(Ours)$+$Real} & 89.63 & 64.94 & 59.68 & 30.99 & 32.56 & \\
    \bottomrule
    \end{tabular}
    % }
% \end{sc}
\end{small}
\end{center}
% \vskip -0.1in
% \vspace{-1em}
\end{table*}

\begin{table*}[t]
\caption{Evaluation of downstream STM accuracy using $6$ PLMs, $L=10$. % under ($4.0,1\times10^{-5}$)-DP setting. DP is not used in ``OnlyPrivate''.
\textbf{Best} and \underline{second best} results are marked.
% Best result is marked as \textbf{bold} and the second best marked with \underline{underline} for each task.
}
\label{tab:main_results_10}
% \vskip -0.15in
% \vspace{-1em}
\begin{center}
\begin{small}
% \vspace{0.3em}
% \begin{sc}
    % \resizebox{1\linewidth}{!}{
    % \begin{tabular}{c|c||c|cc|cc|c}
    % \toprule
    %     \multicolumn{2}{c||}{} & \multirow{2}{*}{IMDb} & \multicolumn{2}{c|}{Yelp} & \multicolumn{2}{c|}{Openreview} & \multirow{2}{*}{Banking} \\
    %     \multicolumn{2}{c||}{} & ~ & Category & Rating & Area & Rating & ~ \\
    % \midrule
    %     \multicolumn{2}{c||}{OnlyPrivate} & 50.00 & 5.90 & 38.76 & 8.86 & 26.55 & 16.75 \\
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multicolumn{2}{c||}{FuseGen} & \underline{89.07} & \underline{63.38} & 57.96 & 24.70 & 34.57 & 78.75 \\
    %     % \cmidrule
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multirow{6}{*}{\shortstack{Pre-Text}} & GPT-2 & 85.87 & 62.58 & 46.25 & 37.13 & 24.45 & 76.25 \\
    %      & Llama-2 & 86.09 & 60.20 & 51.11 & 34.24 & 36.24 & 85.38 \\
    %      & Vicuna & 83.52 & 64.11 & 54.76 & 36.38 & 30.88 & 86.13 \\
    %      & OPT & 83.98 & 63.65 & 52.44 & 37.67 & 24.73 & 79.75 \\
    %      & ChatGLM3 & 86.32 & 60.24 & 56.94 & 40.14 & 33.35 & 89.38 \\
    %      & Flan-T5 & 89.02 & 62.82 & 61.04 & 38.31 & 36.53 & 81.75 \\
    %     % \cmidrule
    %     \midrule
    %     % \hline
    %     % \\[-1em]
    %     \multicolumn{2}{c||}{WASP (Ours)} & 89.65 & 64.34 & 61.46 & 40.47 & 37.60 & 89.63 \\
    %     % \multicolumn{2}{c||}{(Ours)} & \multicolumn{6}{c}{{\color{pink}{A800, debug6,7, gpu6,7, 10gold\_1,2.sh}}}\\
    % \bottomrule
    % \end{tabular}
    \begin{tabular}{c|c|c|cc||c|cc|cc|c}
    \toprule
        \multicolumn{2}{c|}{} & \multirow{2}{*}{Privacy} & \multirow{2}{*}{\,\,$|\mathcal{B}|$\,\,} & \multirow{2}{*}{$|\mathcal{D}|$} & \multirow{2}{*}{IMDb} & \multicolumn{2}{c|}{Yelp} & \multicolumn{2}{c|}{Openreview} & \multirow{2}{*}{Banking} \\
        \multicolumn{2}{c|}{} & ~ & ~ & ~ & ~ & Category & Rating & \,\,Area\,\, & Rating & ~ \\
    \midrule
        \multicolumn{2}{c|}{OnlyPrivate} & $\epsilon=\infty$ & $100$ & - & 50.00 & 5.90 & 38.76 & 8.86 & 23.55 & 16.75 \\
    \midrule
        \multicolumn{2}{c|}{FuseGen} & Absolutely Private & - & $6,000$ & \underline{89.07} & 63.38 & 57.96 & 24.70 & 34.57 & 78.75 \\
    \midrule
        \multirow{6}{*}{\shortstack{Pre-Text}} & GPT-2 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 85.87 & 62.58 & 46.25 & 37.13 & 24.45 & 76.25 \\
         & Llama-2 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 86.09 & 60.20 & 51.11 & 34.24 & 36.24 & 85.38 \\
         & Vicuna & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 83.52 & \underline{64.11} & 54.76 & 36.38 & 30.88 & 86.13 \\
         & OPT & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 83.98 & 63.65 & 52.44 & 37.67 & 24.73 & 79.75 \\
         & ChatGLM3 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 86.32 & 60.24 & 56.94 & 38.14 & 33.35 & \underline{89.38} \\
         & Flan-T5 & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & 89.02 & 62.82 & \underline{61.04} & \underline{38.31} & \underline{36.53} & 81.75 \\
    \midrule
        \multicolumn{2}{c|}{WASP (Ours)} & ($4.0,1\times10^{-5}$)-DP & $100$ & $6,000$ & \textbf{89.65} & \textbf{64.34} & \textbf{61.46} & \textbf{40.47} & \textbf{37.60} & \textbf{89.63} \\
        % \multicolumn{2}{c||}{(Ours)} & \multicolumn{6}{c}{{\color{pink}{A800, debug6,7, gpu6,7, 10gold\_1,2.sh}}}\\
    \bottomrule
    \end{tabular}
    % }
% \end{sc}
\end{small}
\end{center}
% \vskip -0.1in
% \vspace{-1em}
\end{table*}

\textbf{Implementation Details.} % \jj{Move this to after Baselines}\tianyuan{DONE} 
By default, we use $100$ private samples ($M=100$) for main experiments. For federated data ($L>1$) scenario, we use $L=10$ private data parties which control $300$ private samples ($M=\sum_{l=1}^{10} |\mathcal{B}_l|=300$) altogether. To better align with real-world scenarios, each participating data-party controls private datasets that are \textit{non-i.i.d.} to each other, and aggregate to an unbalanced dataset. % with the number of samples belonging to each class decided randomly. \yang{I still can't understand what is "decided randomly", this can't be reproducible if it is random.}
We follow Dirichlet Partition~\cite{yurochkin2019bayesian,hsu2019measuring,zhang2023fedala} to distribute private samples to each party
% following Dirichlet distribution 
with parameter $\alpha=1.0$.
%\textbf{DP Synthetic Dataset Generation.} 
For the DP synthetic dataset, we generate a total of $6,000$ samples from all participating PLMs within $5$ iteration. %In the first iteration, each PLMs $\mathcal{P}_k$ is required to generate $(6000/5)*(1/K)$ samples as $w_k=1/K$ as initialization.
The notion of DP is sample-level DP unless otherwise stated.



\subsection{Main Results} \label{subsec:main_results}
% \yang{move all the FL results to a separate paragraph as well and rephrase the discussions.}
% \textbf{Open-source PLMs.} 
% Experimental Results using $K=6$ open-source PLMs are included in \cref{tab:main_results_1}, which shows that WASP outperforms all baseline methods across different tasks, demonstrating its superiority. Although Aug-PE achieves good performances \yang{why good? they should not be good based on our motivations and Fig.1. focus on bad cases where they fail.} \tianyuan{Fig. 1 shows that most of the PLMs failed, but Flan-T5 still succeeds. So, the best performing single-PLM performs well.} across different tasks (even comparable performances for on-line banking query classification task with Banking dataset), In addition, the best performing PLM varies across tasks, highlighting the risky LLM selection. In contrast, WASP consistently achieves better performance across tasks, making it a \textit{PLM-agnostic} method that does not require prior-knowledge for selecting the involving PLMs for collaboration.
% On the other hand, comparing with FuseGen, a baseline under zero-shot setting where private samples are inaccessible, WASP leverages the power of real private samples and adds a more precise PLM importance weighting method therefore achieves a better performance.
% % Moreover, the notably low performance of ``OnlyPrivate'' verifies that, our setting, $M=100$ or $M=300$, satisfies our limited amount of private samples hypothesis as the trained STM is nearly unusable relying merely on the private dataset $\mathcal{B}$, even without apply DP during training which further degrades STM performance.
% Moreover, the notably low performance of ``OnlyPrivate'' verifies that the trained STM  relying merely on the private dataset $\mathcal{B}$ is nearly unusable, even without applying DP during training which would further degrade STM performance.

% \textbf{Closed-source PLMs.} We also conducted experiments using a fusion of three closed-source PLMs, and the results are included in \cref{tab:close_plm_main_results}. As expected, the GPT series, being powerful models, outperforms their open-source counterparts (see \cref{tab:main_results_1}) when using baseline method Aug-PE. Furthermore, 
% our proposed approach WASP surpasses all baseline methods, demonstrating its effectiveness in leveraging collaborative knowledge from PLMs and the data party and highlighting its superiority over existing methods. \yang{consider combine this with the paragraph for open-source results.}

\textbf{Single Data Party Setting.} 
Experimental results using $K=6$ open-source PLMs and $3$ closed-source PLMs are provided in \cref{tab:main_results_1,tab:close_plm_main_results}, which show that WASP outperforms all baseline methods across different tasks, demonstrating its superiority. As expected, the closed-source GPT series (see \cref{tab:close_plm_main_results}), being powerful models, outperform their open-source counterparts (see \cref{tab:main_results_1}) when using baseline method Aug-PE. 

For all tasks, with limited private samples, Aug-PE performs poorly when using improper single PLM, e.g. using OPT for IMDb and using GPT-2 for Openreview-Rating.
Differently, WASP performs consistently well across tasks, and achieves a lower FID value compared to baselines (see \cref{fig:appendix_fid_comparison} in \cref{subsec:appendix_fid_comparison}), verifying its effectiveness under limited private sample setting.
% \yang{still no discussion on the incapabilities of Aug-PE for small data compared to us. this part of discussion is important. Need to call back to our 3 challenges in motivations.} 
Also, the best performing PLM model varies across tasks for Aug-PE, %achieves good performance across different tasks when using the best performing single PLM (even comparable performance for online-banking query classification task on Banking dataset), the PLM varies across tasks, 
highlighting the arbitrary nature of PLM selection. In contrast, WASP consistently achieves better performance across tasks, making it PLM-agnostic without requiring prior-knowledge for selecting specific PLMs for collaboration.

On the other hand, comparing with FuseGen, a baseline under zero-shot setting where private samples are inaccessible, WASP leverages real private samples and utilizes a more targeted PLM importance weighting method,  therefore achieving better performance.
% Moreover, the notably low performance of ``OnlyPrivate'' verifies that, our setting, $M=100$ or $M=300$, satisfies our limited amount of private samples hypothesis as the trained STM is nearly unusable relying merely on the private dataset $\mathcal{B}$, even without apply DP during training which further degrades STM performance.
Moreover, the notably poor performance of ``OnlyPrivate'' shows that the trained STM  relying merely on  private dataset $\mathcal{B}$ is nearly unusable, even without applying DP during training which can further degrade STM performance.

% \textbf{Closed-source PLMs.} We also conducted experiments using a fusion of three closed-source PLMs, and the results are included in \cref{tab:close_plm_main_results}. As expected, the GPT series, being powerful models, outperforms their open-source counterparts (see \cref{tab:main_results_1}) when using baseline method Aug-PE. Furthermore, 
% our proposed approach WASP surpasses all baseline methods, demonstrating its effectiveness in leveraging collaborative knowledge from PLMs and the data party and highlighting its superiority over existing methods. \yang{consider combine this with the paragraph for open-source results.}
\begin{table}[]
% \vspace{-0.5em}
\centering
\caption{Evaluation of downstream STM accuracy using $3$ closed-source PLMs, $L=1$ with the same DP setting in \cref{tab:main_results_1}.
\textbf{Best} and \underline{second best} results are marked.
}
\label{tab:close_plm_main_results}
\begin{small}
% \vspace{0.3em}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c||c|c|ccc|c}
    \toprule
        \multirow{2}{*}{~} & \multirow{2}{*}{\shortstack{Only\\Private}} & \multirow{2}{*}{\shortstack{FuseGen}} & \multicolumn{3}{c|}{Aug-PE} & \multirow{2}{*}{\shortstack{WASP\\(Ours)}} \\
    % \cmidrule{4-6}
    \cline{4-6}
    \\[-0.8em]
        ~ & ~ & ~ & GPT-3.5 & GPT-4 & GPT-4o & ~ \\
    \midrule
        % IMDb &{\color{pink}{3090}}  & {\color{pink}\shortstack{debug10\\gpu0}} & {\color{pink}\shortstack{debug3\\gpu3}} & {\color{pink}\shortstack{debug4\\gpu4}} & {\color{pink}\shortstack{debug5\\gpu6}} & {\color{pink}\shortstack{debug2\\gpu7}} \\
        % \\[0.01em]
        \multirow{2}{*}{\shortstack{Yelp-\\Rating}} & \multirow{2}{*}{35.57} & \multirow{2}{*}{61.36} & \multirow{2}{*}{60.90} & \multirow{2}{*}{61.02} & \multirow{2}{*}{\underline{62.06}} & \multirow{2}{*}{\textbf{64.48}} \\
        ~ & ~ & ~ & ~ & ~ & ~ & ~ \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-1em}
\end{small}
\end{table}



\textbf{Federated Data Setting.}
We also conduct experiments under distributed federated data setting, with $L=10$ and $M=300$ total number of private samples. Results in \cref{tab:main_results_10} show that WASP consistently achieves better performance across different tasks and settings compared to Pre-Text, a baseline designed for federated data. This further demonstrates the effectiveness of WASP when extended to federated data setting.
Additional results on communication cost comparison is given in \cref{tab:upload_download_comparison} in \cref{subsec:appendix_communication_overhead}, where we show that the communication increase caused by uploading additional histograms by our method is minimal. 
%\tianyuan{Moreover, compared to Pre-Text, WASP achieves higher STM performance with similar information exchange overhead for secure aggregation. Both methods distribute the embedding of each synthetic sample in $\mathcal{D}$ to each data party and aggregate individual voting histograms they obtained to upload to the server hosting the PLMs. While WASP improves via contrastive generation compared to Pre-Text, which  aggregates an additional $L$ histograms of size $\mathbb{R}^{|\mathcal{D}|}$ and uploads the additional aggregated histogram $H^f \in \mathbb{R}^{|\mathcal{D}|}$, the resulting communication increase is just minimal. See \cref{tab:upload_download_comparison} in \cref{subsec:appendix_communication_overhead} for detail.} \yang{this sounds like this is the only difference between WASP and Pre-Text. Should explain the major technical differences which can explain the performance gain.}\tianyuan{Rephrased} 
% This results in just minimal communication cost for WASP compared to Pre-Text (see \cref{tab:upload_download_comparison} in \cref{subsec:appendix_communication_overhead} for detail). 
% Therefore, compared to Pre-Text, WASP only needs to additionally aggregate $H^{f}$ aside from $H^{n}$ and uploads a longer prompt to the servers. \yang{this discussion and table does not have a strong point. Removal the table, just say the additional communication cost for WASP compared to Pre-Text is very minimal because.. .}\cref{tab:upload_download_comparison} for detail.}
% Moreover, as the total level of the added Gaussian noise on to $H^n,H^f$ keeps the same 
% % the same level of Gaussian noise in total is added to $H^n,H^f$ 
% under the same DP setting regardless of $L$, the number of PDP involved, we are able to compare of results of the same method in \cref{tab:main_results_1,tab:main_results_10} to evaluate the impact of including more private samples, i.e. increasing $M$. Results indicate that a greater number of private samples is likely to lead to improved performance for both baseline methods and our proposed WASP \yang{this conclusion is not related to FL itself and is already covered somewhere else, so remove this. It would be better to add discussions on impact of number of parties, non-iid $\alpha$, other aggregation scenarios if possible.}, underscoring the importance of incorporating more PDPs for incorporating more real samples in practical applications. Sensitive analysis of $M$ is included in \cref{subsec:experiments_M}.




\subsection{Ablation Studies}
\textbf{\# PLMs ($K$).} \label{s titleubsec:experiments_K}
We first study the impact of the number of PLMs ($K$) on the final STM performance. Results of using $1,2,3$ closed-source PLMs under $L=1$ and $(4.0,1\times10^{-5})$-DP settings are reported in \cref{fig:errorbar_increase_k}. We can see that the performance of $m$ increases simultaneously with the increase of $K$ while the randomness (STD) decreases.
This indicates that the randomness in the performance of the synthetic dataset can be mitigated by incorporating more PLMs into WASP, which simultaneously increases the performance expectations. 
% \jj{This is a weird sentence} \tianyuan{Edited.}

We also display the pair-wise combination ($K=2$) results of the $3$ closed-source PLMs under $L=1$ and $(4.0,1\times10^{-5})$-DP settings in \cref{fig:pair_wise}. In this figure, any pair-wise collaboration ($K=2$) outperforms either participating single-PLM alone (diagnose in \cref{fig:pair_wise}), demonstrating that WASP performs better using the whole set of available PLMs than using only a subset of them. These findings show that WASP’s improvements are PLM-agnostic, independent of any single PLM's inherent task capabilities. Consequently, WASP effectively mitigates the risk of selecting the optimal PLM by harnessing the collective strengths of all participating models. %requiring no prior knowledge of individual PLM's strength on the specific task before making collaborative usage of them \jj{Any results demonstrate this? Why mention here?}. \tianyuan{Prof. JJ, we say it is PLM agnostic as we can combine PLMs into the collaboration to improve final performance without the need of checking its task capability in advance.}
% In this figure, each circle represents a PLM, and each intersection between circles represents involving the related PLM in the collaboration. That means, the final performance of $m$ is $64.48$ with $K=3$ (in the middle). 
% We can see that, the performance of $m$ increases simultaneously with the increase of $K$ and the performance of each combination of PLMs surpass all the possible subsets of these PLMs. 
% This indicates that, the performance expectancy grows when involving more PLMs into collaboration.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{figure/ablations/closedPLM_yelpRating_Purples.png}
%     \caption{Evaluation of downstream STM accuracy using Yelp-Rating dataset with $K=1,2,3$, $1$ PDP under $(4.0,1\times10^{-5})$-DP setting.}
%     \label{fig:enter-label}
% \end{figure}

\begin{figure}[!tb]
\vspace{-1em}
    \centering
    % \includegraphics{}
     \subfigure[Effect of $K$]{
         \begin{minipage}[t]{0.47\linewidth}
         \centering
         \includegraphics[width=1\linewidth]{figure/ablations/increase_K_withErrorbar_YelpRating.png}
         \vspace{-0.5em}
         \label{fig:errorbar_increase_k}
         \end{minipage}
     }
     % \vspace{-0.3em}
    \subfigure[Pair-wise Comparison]{
         \begin{minipage}[t]{0.43\linewidth}
         \centering
         \includegraphics[width=1\linewidth]{figure/ablations/single_and_pairwise_YlGnBu_yelpRating.png}
         \vspace{-0.5em}
         \label{fig:pair_wise}
         \end{minipage}
     }
     \vspace{-0.3em}
     \caption{Evaluation of downstream STM accuracy using Yelp-Rating dataset with $K=1,2,3$ closed-source PLMs, $L=1$ under $(4.0,1\times10^{-5})$-DP setting. In (b), results on the diagnose are with $K=1$ and others are with $K=2$.}
    \label{fig:increase_of_k_closed_source_plms}
\vspace{-1em}    
\end{figure}

% \includegraphics{figure/introduction/text_converge/imdb/100gold/smaller/flan-t5.png}
\textbf{Contrastive ICL \& PLM Importance Weighting. 
% \yang{use consistent names}
} \label{subsec:experiments_ablatio_weighting_and_contrast}
% \yang{try to add results on the other datasets too.} \tianyuan{Datasets other than IMDb and Yelp-Rating? Running, but first guarantee the results for varying number of private samples.}
To evaluate the effectiveness of our proposed \textit{Contrastive In-context Learning} and \textit{PLM Importance Weighting} methods, we conduct ablation experiments to see how these components impact the final STM performance. Results are reported in \cref{tab:plm_weighting_comparison}. 
By removing \textit{Contrastive In-context Learning} (labeled as ``w/o PLM Contrastive Prompting'' in \cref{tab:plm_weighting_comparison}), we only select high-quality samples for the prompt (details in \cref{tab:appendix_prompt}). This leads to a $0.31\%$ decrease in STM performance on the easier IMDb task, and a much larger $1.56\%$ and $0.92\%$ decrease on the more challenging Yelp-Rating and Openreview-Rating tasks. This highlights the importance of using low-quality samples as feedback demonstrations to encourage the PLMs avoid generating low-quality DP synthetic samples.

On the other hand, by removing \textit{PLM Importance Weighting} (labeled as ``w/o PLM Importance Weighting'' in \cref{tab:plm_weighting_comparison}), $w_k=1/K$ within each generation iteration, indicating that each  PLM generates equal amount of samples across iterations. Similarly, results indicate a $0.35\%$ decrease in STM performance on the easier IMDb task and a $2.27\%$ and $1.57\%$ decline on the more challenging Yelp-Rating and Openreview-Rating tasks. This underlines the effectiveness of weighted aggregation of PLMs with varying degrees of reliance on their capabilities for specific task. 
%\\
Furthermore, these results demonstrate that by generating better DP synthetic data, WASP is more effective than baselines when faced with more challenging tasks. %, . %, thereby increasing its competitiveness.

\begin{table}[]
    \centering
    \caption{Comparison of downstream STM accuracy under w/ and w/o Contrastive In-context Learning and Private Data Assisted PLM Importance Weighting setting using $6$ open-source PLMs, $L=1$ with $(4.0, 1\times10^{-5})$-DP.}
    \label{tab:plm_weighting_comparison}
\begin{small}
% \vspace{0.3em}
    \begin{tabular}{c||cc|c}
    \toprule
        ~  & {\shortstack{w/o PLM\\Contrastive\\Prompting}} & {\shortstack{w/o PLM\\Importance\\Weighting}} & {\shortstack{WASP\\(Ours)}} \\
    \midrule
        % ACC & IMDb & 89.32 & $\epsilon=8.0$ & 89.77 \\
        IMDb & 89.21 & 89.17 & 89.52\\
        Yelp-Rating & 59.65 & 58.94 & 61.21 \\
        Openreview-Rating & 36.18 & 35.53 & 37.10 \\
    \bottomrule
    \end{tabular}
\end{small}
\end{table}


\textbf{\# Votes ($Q$) by Each Private Sample.} \label{subsec:experiments_Q}
To better estimate private sample distribution  with limited private samples, WASP exploits each private sample by increasing the amount of votes each private sample gives out (from $Q=1$ in previous works to $Q=8$). Here we investigate how the change in $Q$ impacts  STM performance. Results in \cref{tab:voting_count_q} indicate that STM performance improves with higher values of $Q$, but the improvement diminishes at larger $Q$ ($Q>8$). 
This underscores the strength of our idea in increasing the utility of each private sample to achieve a more accurate private sample distribution estimation, particularly in scenarios with limited available private samples.
% \yang{I think this will raise a question on why not higher $Q$? maybe we can show this vs. the computation or show that the perform increase will diminish for larger Q?} \tianyuan{On it.}
% This highlights the effectiveness of our idea in exploiting each private sample by a larger extent to enhance the estimation of private sample distribution in order to handle limited amount of availabel private sample scenarios.

\begin{table}[]
\centering
\caption{Evaluation of downstream STM accuracy using $6$ open-source PLMs, $L=1$ with $(4.0, 1\times10^{-5})$-DP under different $Q$.}
\label{tab:voting_count_q}
\begin{small}
% \vspace{0.3em}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c||ccccc}
    \toprule
         % & \multicolumn{3}{c|}{$L=1$} & \multicolumn{3}{c}{$L=10$} \\
         & $Q=1$ & $Q=2$ & $Q=4$ & $Q=8$ & $Q=16$ \\
    \midrule
        IMDb & 89.02 & 89.15 & 89.39 & 89.52 & 89.60 \\
        Yelp-Rating & 58.74 & 58.92 & 59.24 & 61.21 & 61.42 \\
    \bottomrule
    \end{tabular}
    }
    % \vspace{-1em}
\end{small}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figure/ablations/gold_change_fid_acc_2.png}
    \vspace{-0.5em}
    \caption{Comparison of downstream STM accuracy using different number of private samples ($M$) from the training set of IMDb and Yelp-Rating datasets using $6$ open-source PLMs, $L=1$ with $(4.0,1\times10^{-5})$-DP.}
    \label{fig:sensitive_m}
\end{figure}

\textbf{Sensitive Analysis of \# Private Samples ($M$).} \label{subsec:experiments_M}
We also investigate the impact of $M$ on WASP. 
% Like shown in \cref{subfig:pe_random_fid_and_acc}, previous PE methods is likely to fail with limited private samples, especially when selecting an inappropriate PLM. Therefore, 
In \cref{fig:sensitive_m}, we compare the baseline Aug-PE method with WASP across different values of $M$. Results show that WASP consistently outperforms Aug-PE across different values of $M$ for all PLMs, and the performance gaps at smaller $M$ values ($M<1000$) are much greater, %} for most PLMs, when $M>1000$, the increase in ACC slows down and becomes similar to that of WASP. Conversely, for $M<1000$, the rate of increase in ACC for certain PLMs significantly exceeds that of WASP. This 
underscoring the effectiveness of WASP in limited private data scenarios. %\yang{it would be nicer to have results on smaller M values.} \tianyuan{DONE.}

\textbf{Different Private Budget ($\epsilon$).} \label{subsec:experiments_epsilon}
% First, although the sensitivity of our DP mechanism is $4$ (see \cref{sec:appendix_privacy_analysis} for detail), which is $4$ times of the baseline methods (Aug-PE and Pre-Text), which means that the $\sigma$ of WASP should be $4$ times as large as that for these baselines to guarantee the same $(\epsilon,\delta)$-DP, WASP still outperforms these baselines under the same privacy budget $\epsilon$, as shown in \cref{tab:main_results_1,tab:main_results_10}. This alleviates concerns about potential decreases in the robustness of WASP when noise is introduced to ensure DP, despite its increased function sensitivity \yang{why is this the case? e.g., if we multiple the existing weights by 2, and increase the sensitivity, will this hold and why?}. \tianyuan{We perform better because we give better distribution estimation. If we multiply our voting weights by 2 and increase the noise accordingly, the same will hold, As the ranking order based on $H^n,H^f$ holds.}
% \\
% Additionally, 
As illustrated in \cref{tab:dp_alpha}, STM performance using WASP gradually declines from $89.96\%$ to $89.36\%$ for IMDb and from $62.02\%$ to $60.94\%$ for Yelp-Rating as the privacy budget $\epsilon$ decreases from $\infty,8.0,4.0$ to $1.0$, similar to that of Aug-PE when using the best performing single PLM for each task. This indicates that WASP scales well with $\epsilon$ and maintains high performance even under tight privacy constraints, just like baseline method. 
% \yang{can we say something about the rate? who declines less?} \tianyuan{Mrs. Liu, I guess we can not. They are very similar to each other. When $\epsilon$ changes from $\infty$ to $1.0$, for IMDb WASP declines 0.6\% while Aug-PE declines 0.76\%, but for Yelp-Rating, WASP declines 1.08\% while Aug-PE declines 1.03\%.}
% \yang{this needs to be compared with Aug-PE to conclude whether it scales better or not.} \tianyuan{Too much experiments considering the time :( But can try adding the best performing Flan-T5}

% \begin{table}[]
% \centering
% \caption{Evaluation of downstream STM accuracy using $2$ open-source PLMs, $1$ PDPs under different DP budget setting with $\delta=1\times10^{-5}$.}
% \label{tab:main_results}
%     \resizebox{1\linewidth}{!}{
%     \begin{tabular}{c||ccc|ccc}
%     \toprule
%          & \multicolumn{3}{c|}{$L=1$} & \multicolumn{3}{c}{$L=10$} \\
%          & $\epsilon=1.0$ & $\epsilon=4.0$ & $\epsilon=\infty$ & $\epsilon=1.0$ & $\epsilon=4.0$ & $\epsilon=\infty$ \\
%     \midrule
%         OnlyReal & 50.00 & 50.00 & 50.00 & 50.00 & 50.00 & 50.00 \\
%         \hline
%         GPT-3.5 &  &  & 85.47 & 60.33 & {\color{pink}{67.30}} & \\
%         GPT-4 &  &  & 85.64 & 59.36 & {\color{pink}{56.92}} &  \\
%         \hline
%         (Ours) &  &  & 86.40 \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}
\begin{table}[]
\centering
\caption{Evaluation of downstream STM accuracy using $6$ open-source PLMs, $L=1$ under different DP budget setting with $\delta=1\times10^{-5}$. The best performing PLM is used for Aug-PE evaluation, i.e. Flan-T5 for both tasks.}
\label{tab:dp_alpha}
\begin{small}
% \vspace{0.3em}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c|c||cccc}
    \toprule
         % & \multicolumn{3}{c|}{$L=1$} & \multicolumn{3}{c}{$L=10$} \\
         ~ & ~ & $\epsilon=\infty$ & $\epsilon=8.0$ & $\epsilon=4.0$ & $\epsilon=1.0$ \\
    \midrule
        \multirow{2}{*}{IMDb} & WASP & 89.96 & 89.77 & 89.52 & 89.36 \\
        ~ & Aug-PE & 89.48 & 89.23 & 89.00 & 88.72 \\
        \midrule
        \multirow{2}{*}{\shortstack{Yelp-\\Rating}} & WASP & 62.02 & 61.54 & 61.21 & 60.94 \\
        ~ & Aug-PE & 59.62 & 59.12 & 58.69 & 58.59 \\
    \bottomrule
    \end{tabular}
    }
% \vspace{-1em}
\end{small}
\end{table}

% \subsubsection{Impact of the number of Participating PDPs ($L$)}


\section{Conclusion and Future Work}
In this work, we introduce a novel DP synthetic data generation framework, WASP, which leverages the collaborative capabilities of multiple PLMs to address real-world scenarios with limited private samples, %enhancing DP synthetic data generation by fusing the capabilities of PLMs tailored to specific downstream tasks 
 while observing differential privacy. % and dynamically adjust the reliance on each PLM basing on their capabilities.
Extensive experiments  across $6$ tasks demonstrate that WASP is highly effective, PLM-agnostic, scalable with respect to privacy budgets, and superior in challenging scenarios, making it a practical and scalable solution for real-world applications.

% Currently, WASP does not include sample-level weighting or selection to further improve the quality of DP synthetic data. Moreover, experiments on non-classification tasks are not included. Therefore, possible future work points to more precise sample-level weighting or selection, as well as verifying the effectiveness of WASP on non-classification tasks.
Possible future work points to more precise sample-level weighting or selection to further improve the quality of the DP synthetic dataset, as well as verifying the effectiveness of WASP on non-classification tasks.



% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\newpage

\section{Contrastive Prompts and Non-contrastive Prompts} \label{sec:appendix_prompt}
\begin{table*}[!htb]
\centering
\caption{Prompt used for synthetic dataset generation. Due to clarity, we omit the words in the parentheses in the labels of Openreview-Category and the attributes of Openreview-Rating.}
\label{tab:appendix_prompt}
\vspace{0.3em}
    \resizebox{0.998\linewidth}{!}{
    \begin{tabular}{c|c|p{11.5cm}|c|c}
    \toprule
        Dataset (task) & prompt type & prompt & label & attribute \\
    \midrule
        \multirow{2}{*}{\shortstack{IMDb\\(semantic analysis\\of movie review)}} & w/o Contrastive & {``The movie review is: \textit{<sample\_1>}\emph{\textbackslash n}The movie review is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The movie review is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above movie reviews, a new movie review also in \textit{\textbf{<label>}} sentiment but diverse in the expression compared to the above given samples is: ''} & \multirow{2}{*}{\textit{\textbf{positive / negative}}} & \multirow{2}{*}{None} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive & ``A bad movie review is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad movie review is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good movie review is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good movie review is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good movie reviews in \textit{\textbf{<label>}} sentiment, analyze the differences between the bad and good reviews. Generate a new positive movie review that is diverse in expression compared to the given good reviews. Ensure that the new review is further refined than the good reviews while maintaining the \textit{\textbf{<label>}} sentiment and clarity, making the good reviews appear to lie midway between the new review and the bad reviews. The new \textit{\textbf{<label>}} movie review is: '' & ~ & ~ \\
        \hline
        \\[-1em]
        \multirow{2}{*}{\shortstack{Yelp-Category\\(field classification\\of business review)}} & w/o Contrastive & The business review is: \textit{<sample\_1>}\emph{\textbackslash n}The business review is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The business review is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above business reviews belonging to the category of \textit{\textbf{<label>}}, a new review for a business item also in the field of \textit{\textbf{<label>}} with rating \textit{<attribute>} star(s) but diverse in the expression compared to the above given samples is: '' & \multirow{2}{*}{\shortstack{\textit{\textbf{Arts \& Entertainment /}}\\\textit{\textbf{Bars / Beauty \& Spas /}}\\\textit{\textbf{Event Planning \& Services /}}\\\textit{\textbf{Grocery / Health \& Medical /}}\\\textit{\textbf{Home \& Garden /}}\\\textit{\textbf{Hotels \& Travel /}}\\\textit{\textbf{Restaurants / Shopping}}}} & \multirow{2}{*}{\shortstack{1.0 / 2.0 / 3.0 /\\4.0 / 5.0}} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive& A bad business review is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad business review is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good business review is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good business review is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good business reviews belonging to the category of \textit{\textbf{<label>}}, analyze the differences between the bad and good reviews. Generate a new review for a business item also in the field of \textit{\textbf{<label>}} with rating \textit{<attribute>} star(s) but diverse in the expression compared to the given good reviews. Ensure that the new review is further refined than the good reviews while maintaining clarity, making the good reviews appear to lie midway between the new review and the bad reviews. The new business review in the field of \textit{\textbf{<label>}} is: '' & ~ & ~ \\
        \hline
        \\[-1em]
        \multirow{2}{*}{\shortstack{Yelp-Rating\\(rating classification\\of business review)}} & w/o Contrastive & The business review is: \textit{<sample\_1>}\emph{\textbackslash n}The business review is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The business review is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above business reviews with rating \textit{\textbf{<label>}} star(s), a new review for a business item in the field of \textit{<attribute>} also with rating \textit{\textbf{<label>}} star(s) but diverse in the expression compared to the above given samples is: '' & \multirow{2}{*}{\shortstack{\textit{\textbf{1.0 / 2.0 / 3.0 /}}\\\textit{\textbf{4.0 / 5.0}}}} & \multirow{2}{*}{\shortstack{Arts \&\\Entertainment /\\Bars /\\Beauty \& Spas /\\Event Planning \& Services /\\Grocery /\\Health \& Medical /\\Home \& Garden /\\Hotels \& Travel /\\Restaurants /\\Shopping}} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive& A bad business review is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad business review is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good business review is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good business review is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good business reviews with rating \textit{\textbf{<label>}} star(s), analyze the differences between the bad and good reviews. Generate a new review for a business item in the field of \textit{<attribute>} also with rating \textit{\textbf{<label>}} star(s) but diverse in the expression compared to the above given good reviews. Ensure that the new review is further refined than the good reviews while maintaining clarity, making the good reviews appear to lie midway between the new review and the bad reviews. The new business review with rating \textit{\textbf{<label>}} star(s) is: '' & ~ & ~ \\
       \hline
        \\[-1em]
        \multirow{2}{*}{\shortstack{Openreview-Category\\(field classification\\of paper review)}} & w/o Contrastive & The paper review is: \textit{<sample\_1>}\emph{\textbackslash n}The paper review is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The paper review is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above paper reviews of paper in the area \textit{\textbf{<label>}}, a new review for a paper also in the area of \textit{\textbf{<label>}} with final recommendation: '\textit{<attribute>}' but diverse in the expression compared to the above given samples is: '' & \multirow{2}{*}{\shortstack{\textit{\textbf{Applications / Deep Learning}}\\\textit{\textbf{and representational learning /}}\\\textit{\textbf{General Machine Learning /}}\\\textit{\textbf{Generative models /}}\\\textit{\textbf{Machine Learning for}}\\\textit{\textbf{Sciences / Neuroscience}}\\\textit{\textbf{and Cognitive Science /}}\\\textit{\textbf{Optimization / Probabilistic}}\\\textit{\textbf{Methods / Reinforcement}}\\\textit{\textbf{Learning / Social Aspects}}\\\textit{\textbf{of Machine Learning /}}\\\textit{\textbf{Theory / Unsupervised}}\\\textit{\textbf{and Self-supervised learning}}}} & \multirow{2}{*}{\shortstack{1: strong reject /\\ 3: reject, not good enough /\\ 5: marginally below the\\acceptance threshold /\\ 6: marginally above the\\acceptance threshold /\\ 8: accept, good paper}} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive& A bad paper review is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad paper review is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good paper review is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good paper review is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good paper reviews of paper in the area \textit{\textbf{<label>}}, analyze the differences between the bad and good reviews. Generate a new review for a paper also in the area of \textit{\textbf{<label>}} with final recommendation: '\textit{<attribute>}' but diverse in the expression compared to the given good reviews. Ensure that the new review is further refined than the good reviews while maintaining clarity, making the good reviews appear to lie midway between the new review and the bad reviews. The new paper review in the area \textit{\textbf{<label>}} is: '' & ~ & ~ \\
        \hline
        \\[-1em]
        \multirow{2}{*}{\shortstack{Openreview-Rating\\(rating classification\\of paper review)}} & w/o Contrastive & The paper review is: \textit{<sample\_1>}\emph{\textbackslash n}The paper review is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The paper review is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above paper reviews of final recommendation: \textit{\textbf{<label>}}, a new review for a paper in the field of '\textit{<attribute>}' also with final recommendation: \textit{\textbf{<label>}} but diverse in the expression compared to the above given samples is: '' & \multirow{2}{*}{\shortstack{\textit{\textbf{1: strong reject /}}\\\textit{\textbf{3: reject, not good enough /}}\\\textit{\textbf{5: marginally below the}}\\\textit{\textbf{acceptance threshold /}}\\\textit{\textbf{6: marginally above the}}\\\textit{\textbf{acceptance threshold /}}\\\textit{\textbf{8: accept, good paper}}}} & \multirow{2}{*}{\shortstack{Applications / Deep Learning\\and representational learning /\\General Machine Learning /\\Generative models /\\Machine Learning for\\Sciences / Neuroscience\\and Cognitive Science /\\Optimization / Probabilistic Methods /\\Reinforcement Learning /\\Social Aspects of\\Machine Learning /\\Theory / Unsupervised\\and Self-supervised learning}} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive& A bad paper review is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad paper review is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good paper review is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good paper review is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good paper reviews of final recommendation: \textit{\textbf{<label>}}, analyze the differences between the bad and good reviews. Generate a new review for a paper in the field of '\textit{<attribute>}' also with final recommendation: \textit{\textbf{<label>}} but diverse in the expression compared to the above given good reviews. Ensure that the new review is further refined than the good reviews while maintaining clarity, making the good reviews appear to lie midway between the new review and the bad reviews. The new paper review of final recommendation: \textit{\textbf{<label>}} is: '' & ~ & ~ \\
        \hline
        \\[-1em]
        \multirow{2}{*}{\shortstack{Banking\\(field classification\\of online banking\\queries)}} & w/o Contrastive & The online banking query is: \textit{<sample\_1>}\emph{\textbackslash n}The online banking query is: \textit{<sample\_2>}\emph{\textbackslash n}...\emph{\textbackslash n}The online banking query is: \textit{<sample\_S>}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above online banking queries in the category of ``\textit{\textbf{<label>}}'', a new online banking query also in the category of ``\textit{\textbf{<label>}}'' but diverse in the expression compared to the above given samples is: '' & \multirow{2}{*}{\shortstack{\textit{\textbf{activate\_my\_card }}/\\\textit{\textbf{age\_limit /}}\\\textit{\textbf{apple\_pay\_or\_google\_}}\\\textit{\textbf{pay / atm\_support /}}\\\textit{\textbf{automatic\_top\_up /}}\\\textit{\textbf{balance\_not\_updated\_}}\\\textit{\textbf{after\_bank\_transfer /}}\\\textit{\textbf{ balance\_not\_updated\_}}\\\textit{\textbf{after\_cheque\_or\_cash\_}}\\\textit{\textbf{deposit / beneficiary\_}}\\\textit{\textbf{not\_allowed / cancel\_}}\\\textit{\textbf{transfer / card\_}}\\\textit{\textbf{about\_to\_expire}}}} & \multirow{2}{*}{None} \\
        \cline{2-3}
        \\[-1em]
        ~ & w/ Contrastive& A bad online banking query is: \textit{<sample\_1>}\emph{\textbackslash n}...\emph{\textbackslash n}A bad online banking query is: \textit{<sample\_$\lfloor$S/2$\rfloor$>}\emph{\textbackslash n}A good online banking query is: \textit{<sample\_$\lfloor$S/2$\rfloor$+1>}\emph{\textbackslash n}...\emph{\textbackslash n}A good online banking query is: \textit{<sample\_S}\emph{\textbackslash n}\emph{\textbackslash n}Based on the above examples of bad and good online banking queries in the category of ``\textit{\textbf{<label>}}'', analyze the differences between the bad and good reviews. Generate a new online banking query also in the category of ``\textit{\textbf{<label>}}'' but diverse in the expression compared to the above given good queries. Ensure that the new query is further refined than the good queries while maintaining clarity, making the good queries appear to lie midway between the new query and the bad queries. The new online banking query also in the category of ``\textit{\textbf{<label>}}'' is: '' & ~ & ~ \\
    \bottomrule
    \end{tabular}
    }
\end{table*}
In \cref{tab:appendix_prompt}, we listed the prompts used in our experiments, including contrastive (``w Contrastive'') and non-contrastive (``w/o Contrastive'') in-context learning prompts. We need to clarify that, for PE series baselines, we use their original prompt for \verb|VARIATIONAL_API|, which is different from the listed ``w/o Contrastive in-context learning'' prompt in \cref{tab:appendix_prompt}. Please refer to \citet{xie2024differentially} (the original work) for detailed prompts.


\section{Algorithm for Distributed Private Data and Detailed Functions} \label{sec:appendix_algorithms}

Due to space limitation, we include the full algorithm for $L>1$ setting here in \cref{alg:algorithm_full_functions} in the Appendix. The difference between \cref{alg:algorithm_full_functions} and \cref{alg:algorithm_full_functions_singlePDP} mainly falls in line 2 and lines 8 to 11 in \cref{alg:algorithm_full_functions}.

\begin{algorithm}[tb]
% \small
\caption{WASP for Distributed Federated Data ($L>1$)} % A complete training process of 
\label{alg:algorithm_full_functions}
\begin{flushleft}
\textbf{Input:}\\ \quad
$K$ PLMs $\{\mathcal{P}_{k}\}_{k=1}^K$ with empty synthetic dataset $\{\mathcal{D}_k\leftarrow\emptyset\}_{k=1}^K$;\\ \quad
$L$ private data parties controlling distributed private dataset $\{\mathcal{B}_l\}_{l=1}^L$ of $M$ samples in total that belongs to $C$ categories;\\ \quad
% target number of total synthetic samples $N$, 
% sample selection hyper-parameter $\alpha,R,S$,
number of in-context samples $S$;\\ \quad
% % $\{\mathcal{D}_k\}_{k=1}^K=\{{\{(\mathbf{x}_{k,i},y_{k,i})\}_{i=1}^{N}\}}_{k=1}^K$
number of iterations $T$ taken to obtain in total $N$ synthetic samples;\\ \quad
% initialized sample weights ${\left\{\{w_{k,i}^{(0)}\}_{i=1}^{N}\right\}}_{k=1}^K$, 
initialized PLM weights ${\{w_{k}=1/K\}}_{k=1}^K$;\\ \quad
learning rate $\eta$;\\ \quad
DP privacy parameters $\epsilon,\delta$;\\ \quad
test dataset of downstream task $\mathcal{A}$;\\ \quad
random initialized STM $m_{(0)}$; 
% number of weight adjustment epochs $E_1$, 
% number of STM training epochs $E_2$.
\\
\textbf{Output:} STM $m$. % $\tilde{m}$.
\end{flushleft}

\begin{algorithmic}[1]
    \STATE Initialize in-context feedback samples $\hat{\mathcal{D}}^n \leftarrow \emptyset, \hat{\mathcal{D}}^f \leftarrow \emptyset$.
    \STATE Calculate Gaussian noise $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$.
    \FOR{$t=0$ {\bfseries to} $T-1$}
        \FOR{$k=1$ {\bfseries to} $K$ {\bfseries in parallel}}
            \STATE $\mathcal{D}_k \leftarrow$ \verb|WeightedSynDataGeneration(|$\mathcal{D}_k$, $\hat{\mathcal{D}}^n$, $\hat{\mathcal{D}}^f$, $\left[ (N/T)\times w_{k} \right]$, $C$\verb|)|.
        \ENDFOR
        \STATE $\mathcal{D} \leftarrow \cup_{k=1}^{K}\mathcal{D}_k$.
        \FOR{$l=1$ {\bfseries to} $L$ {\bfseries in parallel}}
            % \STATE $\tilde{m}_l \leftarrow$ \verb|STMTraining(|$\mathcal{D}\cup\mathcal{B}_l$, $m_{(0)}$\verb|)|.
            \STATE $H^{n}_l, H^{f}_l \leftarrow$ \verb|DP_PrivateVoting(|$\mathcal{D}$, $\mathcal{B}_l$, $Q$, $\sigma$\verb|)|.
        \ENDFOR
        % \STATE $m \leftarrow$ \verb|FedAVG(|$\{\tilde{m}_l\}_{l=1}^L$, $\{\mathcal{B}_l\}_{l=1}^L$, $L$\verb|)|.
        \STATE $H^{n}\leftarrow\sum_{l=1}^{L} H^{n}_l$; $H^{f}\leftarrow\sum_{l=1}^{L} H^{f}_l$.
        \STATE $\hat{\mathcal{D}}^n, \hat{\mathcal{D}}^f \leftarrow$ \verb|SampleSelection(|$\mathcal{D}$, $H^{n}$, $H^{f}$, $S$, $C$\verb|)|.
        \STATE $\{w_k\}_{k=1}^K \leftarrow$ \verb|PLMScoring(|$H^{n}$, $\{\mathcal{D}_k\}_{k=1}^K$\verb|)|.
    \ENDFOR
    \STATE $m \leftarrow$ \verb|STMTraining(|$\mathcal{D}$, $m_{(0)}$, $\eta$\verb|)|.
    % \STATE $\tilde{m} \leftarrow$ \verb|STMTraining(|$\mathcal{D}$, $m_{(0)}$\verb|)|.
    % \STATE $\tilde{m} \leftarrow$ \verb|WeightAdjustSTMTraining(|$\cup_{k=1}^{K}\mathcal{D}_k$, $m_{(0)}$, $\cup_{k=1}^{K}\left\{\{w_{k,i}^{(0)}\}_{i=1}^{N}\right\}$, $E_1$, $E_2$\verb|)|.
\end{algorithmic}
\end{algorithm}


We also include pseudo-code for the functions used in \cref{alg:algorithm_full_functions_singlePDP,alg:algorithm_full_functions} here in \cref{alg:algorithm_functions} due to space limitation.

\begin{algorithm*}[!htb]
\small
\caption{Functions used in \cref{alg:algorithm_full_functions_singlePDP,alg:algorithm_full_functions} for WASP}
\label{alg:algorithm_functions}
% \begin{algorithmic}[]
    % \STATE 
    \begin{flushleft}\textbf{function} \verb|WeightedSynDataGeneration(|$\mathcal{D}_k$, $\hat{\mathcal{D}}^n$, $\hat{\mathcal{D}}^f$, $\hat{N}$, $C$\verb|)|:\end{flushleft}
    
\begin{algorithmic}[]
    \FOR{$c=1$ {\bfseries to} $C$}
        \IF{$t=0$}
            \STATE Use zero-shot prompt as working prompt $\mathcal{T}(c)$.
        \ELSE
            \STATE Randomly sample $S-\lfloor S/2 \rfloor$ samples from $\hat{\mathcal{D}}^{n,[c]}$ and $\lfloor S/2 \rfloor$ samples from $\hat{\mathcal{D}}^{f,[c]}$ to create few-shot prompt as working prompt $\mathcal{T}(c)$.
        \ENDIF
        \STATE Generate $\lceil\hat{N}/C\rceil$ samples using $\mathcal{T}$ and add them to $\mathcal{D}_k$.
    \ENDFOR
    \IF{$|\mathcal{D}_k|>\hat{N}$}
        \STATE Random sample $|\mathcal{D}_k|-\hat{N}$ different samples from $\mathcal{D}_k$ and remove them from $\mathcal{D}_k$.
    \ENDIF
    \STATE \textbf{return} $\mathcal{D}_k$.

    \STATE
\end{algorithmic}

    \begin{flushleft}\textbf{function} \verb|STMTraining(|$\mathcal{D}$, $m_{(0)}$, $\eta$\verb|)|:\end{flushleft}
    
\begin{algorithmic}[]
    \STATE Initialize a trainable STM ${m} \leftarrow m_{(0)}$.
    \STATE Train ${m}$ using $\mathcal{D}$ with learning rate $\eta$ till convergence by using objective function $\mathcal{L} = \sum_{i=1}^{|\mathcal{D}|} \ell(m(\mathbf{x}_{i}),y_{i})$.
    \STATE \textbf{return} $m$.

    \STATE
\end{algorithmic}

    \begin{flushleft}\textbf{function} \verb|DP_PrivateVoting(|$\mathcal{D}$, $\mathcal{B}$, $Q$, $\sigma$\verb|)|:\end{flushleft}

\begin{algorithmic}[]
    % \STATE $\hat{N} \leftarrow |\mathcal{D}|$; $H^{n}\leftarrow[0,\dots,0]$; $H^{f}\leftarrow[0,\dots,0]$ and note the total DP synthetic dataset as $\mathcal{D}=\{(\textbf{x}_i, y_i)\}_{i=1}^{|\mathcal{D}|}$.
    \STATE Initialized $H^{n}\leftarrow[0,\dots,0]$; $H^{f}\leftarrow[0,\dots,0]$ of length $\mathbb{R}^{|\mathcal{D}|}$ and note the total DP synthetic dataset as $\mathcal{D}=\{(\textbf{x}_i, y_i)\}_{i=1}^{|\mathcal{D}|}$.
    \FOR{($\textbf{z}_j, u_j$) {\bfseries in} $\mathcal{B}$}
        \STATE $\mathcal{D}^{[u_j]} = \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D} \,|\, y_i=u_j \right\}$.
        % \STATE $[n_{j,1},\dots,n_{j,Q}] \leftarrow \arg\text{top}Q\text{Smallest}_{(\mathbf{x}_{i},y_i) \in \mathcal{D}^{[u_j]}} \, d(\textbf{z}_j, \textbf{x}_{i})$.
        % \STATE $[f_{j,1},\dots,f_{j,Q}] \leftarrow \arg\text{top}Q\text{Largest}_{(\mathbf{x}_{i},y_i) \in \mathcal{D}^{[u_j]}} \, d(\textbf{z}_j, \textbf{x}_{i})$.
        \STATE $[n_{j,1},\dots,n_{j,Q}] \leftarrow \arg\mathrm{top}Q\mathrm{Smallest} \left( \, d(\textbf{z}_j, \textbf{x}_{i})_{(\mathbf{x}_{i},y_i)\in \mathcal{D}^{[u_j]}} \right)$.
        \STATE $[f_{j,1},\dots,f_{j,Q}] \leftarrow \arg\mathrm{top}Q\mathrm{Largest} \left( \, d(\textbf{z}_j, \textbf{x}_{i})_{(\mathbf{x}_{i},y_i)\in \mathcal{D}^{[u_j]}} \right)$.
        \FOR{$q=1$ {\bfseries to} $Q$}
            \STATE $H^{n}[n_{j,q}] \leftarrow H^{n}[n_{j,q}]+\frac{1}{2^{q-1}}$.
            \STATE $H^{f}[f_{j,q}] \leftarrow H^{f}[f_{j,q}]+\frac{1}{2^{q-1}}$.
        \ENDFOR
    \ENDFOR
    \STATE $H^{n} \leftarrow H^{n} + \mathcal{N}(0,\sigma^2I_{|\mathcal{D}|})$.
    \STATE $H^{f} \leftarrow H^{f} + \mathcal{N}(0,\sigma^2I_{|\mathcal{D}|})$.
    \STATE \textbf{return} $H^{n}$, $H^{f}$.

    \STATE
\end{algorithmic}

%     \textbf{function}  \verb|FedAVG(|$\{\tilde{m}_l\}_{l=1}^L$, $\{\mathcal{B}\}_{l=1}^L$, $L$\verb|)|:

% \begin{algorithmic}[]
%     \STATE Federation weight $\{v_l = \frac{|\mathcal{B}_{l}|}{\sum_{l'=1}^{L}|\mathcal{B}_{l'}|}\}_{l=1}^L$.
%     \STATE \textbf{return} $\sum_{l=1}^L v_l \cdot \tilde{m}_l$.

%     \STATE
% \end{algorithmic}

    \begin{flushleft}\textbf{function} \verb|PLMScoring(|$H$, $\{\mathcal{D}_k\}_{k=1}^K$\verb|)|:\end{flushleft}

\begin{algorithmic}[]
    \FOR{$k=1$ {\bfseries to} $K$}
        % \STATE $\hat{N}_k \leftarrow |\mathcal{D}_k|$, and regard $H=\{\{H_{k,i}\}_{i=1}^{\hat{N}_k}\}_{k=1}^K$.
        % \STATE Calculate $s_{k,i} = \frac{H_{k,i}}{\sum_{k'=1}^K \sum_{i'=1}^{\hat{N}_{k'}} H_{k',i'}}$.
        % \STATE Calculate model score $\bar{s}_k = \frac{\sum_{i=1}^{\hat{N}_k} s_{k,i}}{\hat{N}_k / \sum_{k'=1}^K \hat{N}_{k'}}$.
        % \STATE Regard $H=\{\{H_{k,i}\}_{i=1}^{|\mathcal{D}_k|}\}_{k=1}^K$.
        % \STATE Calculate $s_{k,i} = \frac{H_{k,i}}{\sum_{k'=1}^K \sum_{i'=1}^{|\mathcal{D}_{k'}|} H_{k',i'}}$.
        % \STATE Calculate model score $\bar{s}_k = \frac{\sum_{i=1}^{|\mathcal{D}_k|} s_{k,i}}{|\mathcal{D}_k| / \sum_{k'=1}^K |\mathcal{D}_{k'}|}$.
        \STATE Calculate $s_{i} = H^n[i] \,\big/ \sum_{i'=1}^{|\mathcal{D}|} H^n[i']$.
        \STATE Calculate model score $w_k = \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{|\mathcal{D}_{k}| / \sum_{k'=1}^K |\mathcal{D}_{k'}|}= \frac{\sum_{(\textbf{x}_{i},y_{i})\in\mathcal{D}_k} s_{i}}{|\mathcal{D}_{k}| / |\mathcal{D}|}$
        % $.        
    \ENDFOR
    % \STATE \textbf{return} $\{\bar{s}_k\}_{k=1}^K$.
    \STATE \textbf{return} $\{w_k\}_{k=1}^K$.

    \STATE
\end{algorithmic}

    \begin{flushleft}\textbf{function} \verb|SampleSelection(|$\mathcal{D}$, $H^{n}$, $H^{f}$, $S$, $C$\verb|)|:\end{flushleft}
    
\begin{algorithmic}[]
    \STATE Reset $\hat{\mathcal{D}^n} \leftarrow \emptyset$, $\hat{\mathcal{D}^f} \leftarrow \emptyset$.
    \STATE $\mathcal{H}^n \leftarrow \frac{H^n}{\sum_{i=1}^{|\mathcal{D}|} H^n[i]}$.
    \STATE $\mathcal{H}^f \leftarrow \frac{H^f}{\sum_{i=1}^{|\mathcal{D}|} H^f[i]}$.
    \FOR{$c=1$ {\bfseries to} $C$}
        % \STATE $\hat{\mathcal{D}}^n \leftarrow$ top-$(S-\lfloor S/2 \rfloor)$ samples from $\mathcal{D}$ ranked by $\mathcal{H}^n$.
        % \STATE $\hat{\mathcal{D}}^f \leftarrow$ top-$(\lfloor S/2 \rfloor)$ samples from $\mathcal{D}$ ranked by $\mathcal{H}^f$.
        \STATE $\mathcal{D}^{[c]} = \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D} \,|\, y_i=c \right\}$.
        \STATE $H^{n,[c]} = \left\{ H^n[i] \,\big|\, (\mathbf{x}_{i},y_i) \in \mathcal{D}^{[c]} \right\}, H^{f,[c]} = \left\{ H^f[i] \,\big|\, (\mathbf{x}_{i},y_i) \in \mathcal{D}^{[c]} \right\}$.
        \STATE $\hat{\mathcal{D}}^{n,[c]} = \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D}^{[c]} \,\big|\, H^n[i] \text{ is among the top-}S \text{ values of } H^{n,[c]} \right\}$, contains the top-$S$ samples from $\mathcal{D}^{[c]}$ ranked by $\mathcal{H}^{n,[c]}$.
        \STATE $\hat{\mathcal{D}}^{f,[c]} = \left\{ (\mathbf{x}_i,y_i) \in \mathcal{D}^{[c]} \,\big|\, H^f[i] \text{ is among the top-}S \text{ values of } H^{f,[c]} \right\}$, contains the top-$S$ samples from $\mathcal{D}^{[c]}$ ranked by $\mathcal{H}^{f,[c]}$.
        % \STATE $\hat{\mathcal{D}}^{n,[c]} \leftarrow$ top-$S$ samples from $\mathcal{D}$ ranked by $\mathcal{H}^n$.
        % \STATE $\hat{\mathcal{D}}^{f,[c]} \leftarrow$ top-$S$ samples from $\mathcal{D}$ ranked by $\mathcal{H}^f$.
    \ENDFOR
    \STATE $\hat{\mathcal{D}}^n=\left\{\hat{\mathcal{D}}^{n,[1]},\dots,\hat{\mathcal{D}}^{n,[C]}\right\}$, $\hat{\mathcal{D}}^f=\left\{\hat{\mathcal{D}}^{f,[1]},\dots,\hat{\mathcal{D}}^{f,[C]}\right\}$.
    % \STATE $\hat{\mathcal{D}} \leftarrow \hat{\mathcal{D}}^n \cup \hat{\mathcal{D}}^f$.
    % \STATE \textbf{return} $\hat{\mathcal{D}}$.
    \STATE \textbf{return} $\hat{\mathcal{D}}^n, \hat{\mathcal{D}}^f$.

    % \STATE
\end{algorithmic}
% \vspace{3em}
\end{algorithm*}


\section{Supporting Results for Introduction} \label{sec:appendix_intro_support}

\subsection{Examples of High-quality and Low-quality Samples} \label{subsec:appendix_good_bad_samples}
We show examples of high-quality and low-quality synthetic samples generated using Aug-PE in \cref{tab:examples_good_samples} and \cref{tab:examples_bad_samples} respectively. We also include the appearance frequency of some types of low-quality samples within the generated DP synthetic dataset in \cref{tab:examples_bad_samples}.

\cref{tab:examples_bad_samples} shows that, low-quality noisy samples often diverge from the specified task (generating movie reviews in positive/negative sentiment for this table). Differently, likes shown in \cref{tab:examples_good_samples}, high-quality samples often possess a clear sentiment tendency that well accomplished the task, with some offering detailed judgments or even containing concession details.

\begin{table*}[]
\caption{Low-quality DP synthetic samples for movie review semantic analysis with IMDb as real dataset.}
\label{tab:examples_bad_samples}
\begin{small}
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|p{8.5cm}|c|p{4.5cm}}
    \toprule
        \textbf{Model} & \textbf{Low-quality Noisy Sample Text (Examples)} & \textbf{Label} & \textbf{Explain} \\
    \midrule
        GPT-2 & ``\textasciitilde \ If you are missing No. 17 (see below) \textasciitilde '' & negative & Meaning less sentence.\\
        \hline
        \\[-1em]
        Llama-2 & ``In an informal way, please rephrase the sentences as follows:'' & positive & Repeating the prompt. Appears around $30$ times in $\mathcal{D}$ of size $6,000$.\\
        \hline
        \\[-1em]
        Vicuna & {``\begin{CJK}{UTF8}{} \CJKfamily{mj} \textbackslash n번역결과  \textbackslash n좋은 감정을 기반으로 영화 관람 후 즐거움과 엔터테인먼트 가치를 즐기셨기에 좋습니다. \end{CJK}''} & positive & Unmatched language and translating the prompt. Appears within $9$ samples in $\mathcal{D}$ of size $6,000$.\\
        \hline
        \\[-1em]
        OPT & ``The sentence is positive, but is ambiguous. Would it make sense to remove '' & negative & Task unrelated and mislabeled.\\
        \hline
        \\[-1em]
        ChatGLM3 & ``... {\begin{CJK}{UTF8}{gkai}指导意见：句子间不顺畅，需要调整一下顺序，去掉一些冗余的词汇。\end{CJK}} Since is based on positive sentiment, then how does one use the 'pronoun?'' & positive & Unmatched language and repeating the prompt.\\
        \hline
        \\[-1em]
        Flan-T5 & ``what a fantastic movie!! :O great acting and spectacular scenery. great work. i look forward to another good one next year :D come on laura delana!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! woohoooo!!!....can't wait!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!....'' & positive & Excessively long consecutive punctuation marks. Appears within $4$ samples in $\mathcal{D}$ of size $6,000$.\\
    \bottomrule
    \end{tabular}
    }
\end{small}
% \vspace{-1em}
\end{table*}

\begin{table*}[]
\caption{High-quality DP synthetic samples for movie review semantic analysis with IMDb as real dataset.}
\label{tab:examples_good_samples}
\begin{small}
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|p{13.5cm}|c}
    \toprule
        \textbf{Model} & \textbf{High-quality Sample Text (Examples)} & \textbf{Label}\\
    \midrule
        GPT-2 & ``This movie isn't for everyone, but it's worth a look. But if you have to be in the dark to enjoy it, just buy the soundtrack.'' & positive \\
        \hline
        \\[-1em]
        GPT-2 & ``is that it has some semi-interesting world-building, but the information it offers is grossly insufficient to allow one to progress in the story. The central characters have little to do with the world beyond playing around with their toys and talking to themselves. '' & negative \\
        \hline
        \\[-1em]
        Llama-2 & ``Incredible film! I laughed, I cried, and I left the theater feeling inspired. Every performance was top-notch and the direction was flawless. A true masterpiece that will be remembered for years to come. 10/10.'' & positive \\
        \hline
        \\[-1em]
        Llama-2 & ``This film was a colossal disappointment. I found myself glancing at my wrist every 10 minutes, eagerly anticipating its conclusion. The narrative was intricate and challenging to comprehend, and the personas were underdeveloped. I had no emotional investment in their plight. The performances were lacking, and the dialogue was trite and unconvincing. In summary, I would not advise viewing this motion picture to any individual. '' & negative \\
        \hline
        \\[-1em]
        Vicuna & ``A movie for people who want to see an action-packed adventure that will keep them on the edge of their seats.'' & positive \\
        \hline
        \\[-1em]
        Vicuna & ``Felt like a missed opportunity. Glacial pacing and shallow characters. The premise was intriguing but the execution fell flat. Avoid this one.'' & negative \\
        \hline
        \\[-1em]
        OPT & ``-This film is enjoyable to watch because of it's well-developed storyline and most of all its storyline that i can relate to and would like to have. This film also has good acting performances and the songs help the film a lot. It's not a typical love story kind of a film, but it does have lots of interesting messages to give. Very well-done! Definitely recommend this film!'' & positive \\
        \hline
        \\[-1em]
        OPT & ``Quote: The first sentence means nothing to me. Without context the first two sentences also mean nothing.'' & negative \\
        \hline
        \\[-1em]
        ChatGLM3 & ``Attention getter, visually interesting and outstanding acting, the story of an American citizen that is in Mexico and gets involved in a murder is a good movie. '' & positive \\
        \hline
        \\[-1em]
        ChatGLM3 & ``This movie is an unwatchable mess, basically a rip off of that scene in Dr. Strangelove where the U.S. Air Force gets into a scramble to deliver nukes after a false alert... I like pretty much every aspect of anime, but this stuff is bad. Animation is choppy, sound effects and voice acting are atrocious, and the plot makes little sense.'' & negative \\
        \hline
        \\[-1em]
        Flan-T5 & ``<pad> This is a fine historical drama about America's Civil Rights movement, that has William Friedkin as director and Gary Cooper as an actor. Although there are other excellent directors that we haven't heard of, William Friedkin has become the standard in many ways. Filming in New Orleans, Mr. Friedkin gives a glimpse into the African-American experience during the late 1920s. I saw it at the All-African Cinema festival at Wharton Business School. I was awestruck by the power and impact of the film. {\color{gray}{\textit{...[truncated because of excessive length]...}}}'' & positive \\
        \hline
        \\[-1em]
        Flan-T5 & ``<pad> beware of this film! i can't recall a single horror movie that was scarier than this one. it's just plain cheesy and stupid. there's no plot and the characters are entirely cheesy. so don't even bother. you'll never forget it.'' & negative \\
    \bottomrule
    \end{tabular}
    }
\end{small}
% \vspace{-1em}
\end{table*}


\section{Theoretical Privacy Analysis for WASP} \label{sec:appendix_privacy_analysis}

%To prove \cref{theorem:dp_L_equals_1} in main paper, 
To prove \cref{theorem:dp_L_equals_1}, in this part, we prove that the WASP framework described in \cref{alg:algorithm_full_functions} with distributed federated data ($L>1$) satisfies $(\epsilon,\delta)$-DP, which is the general case for $L=1$ setting described in \cref{alg:algorithm_full_functions_singlePDP} and \cref{theorem:dp_L_equals_1}. 

\begin{theorem} \label{theorem:gaussian_dp}
    Let $f$ be a function with global $L_2$ sensitivity $\Delta$. For any $\epsilon>0,\delta \in (0,1)$, the Gaussian output perturbation mechanism with $\sigma=\Delta\frac{\sqrt{2\log\left({1.25/\delta}\right)}}{\epsilon}$ ensures that $f$ satisfies $(\epsilon,\delta)$-DP.
\end{theorem}
Proof of \cref{theorem:gaussian_dp} can be found in \citet{balle2018improving}.

\begin{theorem} \label{theorem:function_sensitivity}
    The global $L_2$ sensitivity $\Delta$ of WASP described in \cref{alg:algorithm_full_functions} is $4$.
\end{theorem}
% \vspace{-1em}
\begin{proof}[Proof]
    In WASP (\cref{alg:algorithm_full_functions}), function \verb|DP_PrivateVoting| is the only function that accesses the private dataset $\mathcal{B}$. Thus, $\Delta$ of WASP equals to that of function \verb|DP_PrivateVoting|. Within function \verb|DP_PrivateVoting|, for nearest histogram and furthest histogram respectively, each private sample contributes $Q$ votes with decaying voting weights $\{1,\frac{1}{2},\dots,\frac{1}{2^{Q-1}}\}$. Therefore, the total votes contributed by one private sample is $\sum_{q=1}^{Q}\frac{1}{2^{q-1}}=2-\frac{1}{2^{Q-1}}<2$ for each histogram. Adding or removing one private sample in $\mathcal{B}$ will result in a change no more than $2$ in the $\ell_2$ norm of each histogram. Therefore, the upper bound of the sensitivity for each histogram is $2$ and the upper bound of the sensitivity of WASP framework is $4$ considering both histograms, i.e. $\Delta=4$. 
\end{proof}

\begin{lemma} \label{lemma:T_iterations}
    If a Gaussian mechanism satisfies $(\epsilon,\delta)$-DP, then independently repeating this mechanism for $T$ times results in the DP budget to increase to $\sqrt{T}\cdot\epsilon$.
\end{lemma}
The proof of \cref{lemma:T_iterations} can be found in \citet{steinke2022composition}.

With the above lemma and theorems, we present and prove our main theorem as follows.
\begin{theorem} % [Differential Privacy of WASP]
\label{theorem:dp}
    If each private data party performs standard Gaussian mechanism with addition noise following $\mathcal{N}(0,\sigma^2)$ and $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$, WASP framework described in \cref{alg:algorithm_full_functions} satisfies $(\epsilon,\delta)$-DP for private samples in $\mathcal{B}$.
\end{theorem}
\begin{proof}[Proof]
    For guaranteeing $(\epsilon,\delta)$-DP throughout the $T-1$ iterations with feedback (the first generation iteration does not use feedback), each iteration should satisfy a differential privacy budget of $\frac{\epsilon}{\sqrt{T-1}}$. Given $\Delta=4$ for WASP, $\sigma_{total}=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon}$ for each single generation iteration will guarantee $(\epsilon,\delta)$-DP for the whole process. Further, Gaussian random variables satisfy that $X+Y\sim\mathcal{N}(0,\sigma_1^2+\sigma_2^2)$ if $X\sim\mathcal{N}(0,\sigma_1^2), Y\sim\mathcal{N}(0,\sigma_2^2)$ are independent. Therefore, if each private data party adds \textit{i.i.d.} Gaussian noise following $\mathcal{N}(0,\sigma^2)$ with $\sigma=4\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$, the total noise follows $\mathcal{N}(0,\sigma_{total}^2)$ which guaranties $(\epsilon,\delta)$-DP for the whole WASP process.
\end{proof}


\section{Additional Results} \label{sec:appendix_results}

\subsection{Comparison of Synthetic Sample Resemblance for WASP} \label{subsec:appendix_fid_comparison}

To further demonstrate the effectiveness of WASP under limited private sample setting, we additionally use FID between the generated DP synthetic dataset $\mathcal{D}$ and the real private dataset $\mathcal{B}$ to evaluate the resemblance of the former ($\mathcal{D}$) to the later ($\mathcal{B}$) with $M=100$ in \cref{fig:appendix_fid_comparison}. Lower FID indicates higher distribution similarity therefore indicating better resemblance. 

As shown in \cref{fig:appendix_fid_comparison}, the baseline method Aug-PE often fails to generate a DP synthetic dataset that closely resembles $\mathcal{B}$ when using an improper PLM, leading to an increased FID value over iterations. On the contrary, WASP results in a consistently decreasing FID value over iteration, demonstrating it effectiveness in improving the resemblance of $\mathcal{D}$ to $\mathcal{B}$. Moreover, although WASP initially has a higher FID than using the best single PLM (Flan-T5 in \cref{fig:appendix_fid_comparison}) for Aug-PE (which is reasonable due to the initialization of $\mathcal{D}$ being a mixture of synthetic samples from different PLMs, making it better than the one generated solely by worst PLM but worse than the one given solely by the best PLM), it ultimately achieves a lower FID than all baseline counterparts. This indicates that our proposed WASP method better handles the limited private sample setting.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.99\linewidth]{figure/appendix/fid_comparison_imdb.png}
    \caption{Comparison of the resemblance of synthetic dataset to real private dataset (FID) using Aug-PE and our proposed WASP using movie review semantic analysis task and IMDb dataset.}
    \label{fig:appendix_fid_comparison}
\end{figure}


\subsection{Effectiveness of Differentially Private Top-$Q$ Voting and Contrast In-context Learning with Single PLM} \label{subsec:appendix_single_plm_topQ_contrast}

We present additional results to validate the effectiveness of \textit{Differentially Private Top-$Q$ voting} and \textit{contrastive in-context learning} with single PLM in \cref{fig:pe_q8_contrast_comparison}. Starting with Aug-PE, we increase $Q$ from $1$ to $8$ to obtain the ``w/o Con'' results, and then incorporate contrastive in-context learning samples into the prompt to achieve the ``w/ Con’’ results (also the $K=1$ setting for WASP). This refinement process shows a steady decrease in FID for 
most PLMs.
% all PLMs except GPT-2, which poorly resembles the private dataset. 
Nonetheless, an overall performance improvement is observed for all tested PLMs, both in terms of highest performance across iterations and final performance.

\begin{figure*}
    \centering
    \includegraphics[width=0.89\linewidth]{figure/appendix/fid_acc_incorrelate_all_PE_Contrast_Random.png}
    \caption{Comparison of the resemblance of synthetic dataset to real private dataset (Fréchet Inception Distance, FID) and trained downstream model performance (ACC) using Aug-PE (``Aug-PE'', doted lines), refinement on $Q=8$ without contrastive in-context learning (``w/o Con'', dashed lines) and refinement on $Q=8$ with contrastive in-context learning (``w/ Con'', solid lines) with single-PLM setting and $L=1$ under $(4.0, 1\times10^{-5})$-DP with IMDb dataset. 
    % Increased FID indicates failed resemblance.
    }
    \label{fig:pe_q8_contrast_comparison}
\end{figure*}


\subsection{Sensitive Analysis of $M$ for PE} \label{subsec:appendix_pe_gold_num_change}
We performed experiments to analyze the sensitivity of Aug-PE~\cite{xie2024differentially} on various $M$ values. Results are included in \cref{fig:gold_change_fid_and_acc} which shows that most PLMs fail when only a limited amount of private samples ($M=100$) is available, with an increasing FID through iterations. Conversely, with sufficient amount of private samples ($M=10,000$), a continuous decrease in FID as well as less performance fluctuation can be observed throughout the iterations.

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.89\linewidth]{figure/appendix/gold_change_fid_acc_100_1000_10000.png}
    \caption{Comparison of the similarity of synthetic dataset to real private dataset (FID) and trained downstream model performance (ACC) with different amount of available private samples ($M$) using Aug-PE with $L=1$ under $(4.0, 1\times10^{-5})$-DP with IMDb dataset.}
    \label{fig:gold_change_fid_and_acc}
\end{figure*}


\subsection{Comparison of WASP and Pre-Text Under User-level DP} \label{subsec:appendix_user_level_dp_results}
In our work, we assume a full participation setting where all $L$ parties participate in each iteration. Based on this, we primarily focus on ensuring sample-level DP to protect each private sample $(\mathbf{z}_j,u_j) \in \mathcal{B}$ in this work.
However, our proposed WASP method can be easily extended to user-level DP protection and is also effective in protecting user-level DP compared to baselines (see \cref{tab:main_results_150}). 


In \citet{hou2024pretext}, although they also study a full participation setting with $L>1$, they focus on user-level DP with the assumption that each participating private data party in the collaboration controls only a tiny amount of private samples ($8$ in their work). Therefore, following \citet{hou2024pretext}, to testify the effectiveness of WASP when extended to user-level DP, we assume that each participating data party controls no more than $8$ real private samples, i.e. $M_l \leq 8, l=1,\dots,L$. These distributed private datasets still aggregate to an unbalanced dataset like in \cref{subsec:experimental_settings}.

Under this setting, to protect user-level DP (where adding or removing one private data party should not significantly affect the function output), the function sensitively $\Delta_{user}$ should be $\max(M_1,$ $\dots,M_L)$ times as large as that for protecting sample-level DP ( $\Delta_{sample}$). The rational is that, the addition or removal of a private data party can result in the addition or removal of up to $\max(M_1,\dots,M_L)$ samples, leading to a change of no more than $\max(M_1,\dots,M_L) \times \Delta_{sample}$ in the $\ell_2$ distance of the produced histograms. Given that $\Delta_{sample}=4$ for WASP (see \cref{theorem:function_sensitivity} in \cref{sec:appendix_privacy_analysis} for details), we have $\Delta_{user}=\max(M_1,\dots,M_L) \times\Delta_{sample} \leq8\times\Delta_{sample}=8\times4=32$. In our experiments, we use $32$, the upper bound of $\Delta_{user}$, as the function sensitivity to calculate $\sigma=32\frac{\sqrt{2\log{\left(1.25/\delta\right)}}\sqrt{T-1}}{\epsilon\sqrt{L}}$ for $(\epsilon,\delta)$-DP protection.

Results are shown in \cref{tab:main_results_150} with a total of $L=150$ private data parties controlling $M=500$ private samples in total. Other experimental settings are the same with those in \cref{tab:main_results_10}. Results show that, WASP continues to outperform baseline methods, including Pre-Text. This demonstrates that WASP is effectiveness not only under the need of guaranteeing sample-level DP but also under the need of providing user-level DP protection compared to baseline methods.

\begin{table*}[tb]
\caption{Evaluation of downstream STM accuracy using $L=150$. User-level DP is guaranteed instead of sample-level DP in this table. \textbf{Best} and \underline{second best} results are marked for each task (dataset, row).}
\label{tab:main_results_150}
\begin{center}
\begin{small}
\vspace{-1em}
    \begin{tabular}{c||c|c|cccccc|c}
    \toprule
        \multirow{2}{*}{~} & \multirow{2}{*}{\shortstack{\\Only\\Private}} & \multirow{2}{*}{\shortstack{\\\,\\FuseGen}} & \multicolumn{6}{c|}{Pre-Text} & \multirow{2}{*}{\shortstack{\\WASP\\(Ours)}} \\
        \cmidrule{4-9}
        ~ & ~ & ~ & GPT-2 & Llama-2 & Vicuna & OPT & ChatGLM3 & Flan-T5 & ~ \\
    \midrule
    \specialrule{0em}{0.3pt}{1pt}
    \midrule
        Privacy & $\epsilon=\infty$ & Aboluste Private & \multicolumn{6}{c|}{$(4.0,1\times10^{-5})$-DP} & $(4.0,1\times10^{-5})$-DP \\
        \midrule
        $|\mathcal{B}|$ & $500$ & - & \multicolumn{6}{c|}{$500$} & $500$ \\
        \midrule
        $|\mathcal{D}|$ & - & $6,000$ & \multicolumn{6}{c|}{$6,000$} & $6,000$\\
    \midrule
    \specialrule{0em}{0.3pt}{1pt}
    \midrule
        IMDb & 83.61 & \underline{89.09} & 83.96 & 84.28 & 83.67 & 84.69 & 85.56 & 88.71 & \textbf{89.15} \\
        Yelp-Rating & 44.15 & 57.96 & 45.78 & 50.54 & 51.42 & 50.40 & 51.54 & \underline{58.37} & \textbf{59.78} \\
    \bottomrule
    \end{tabular}
\end{small}    
\end{center}
\end{table*}

\begin{table*}[!htb]
\caption{Comparison of the information data parties' download, internal exchange and update in Pre-Text and WASP.}
\label{tab:upload_download_comparison}
\begin{small}
\begin{center}
\vspace{0.3em}
    % \resizebox{1\linewidth}{!}{
    \centering
    % \vspace{-1em}
    \begin{tabular}{c|ccc}
    \toprule
        ~ & Download & Exchange & Upload \\
    \midrule
        Pre-Text & embedding of each $(\mathbf{x}_i,y_i)\in\mathcal{D}$ & $\{H^{n}_l\}_{l=1}^L$ & $H^n$ \\
        WASP (Ours) & embedding of each $(\mathbf{x}_i,y_i)\in\mathcal{D}$ & $\{H^{n}_l\}_{l=1}^L,\{H^{f}_l\}_{l=1}^L$ & $H^n,H^f$ \\
    \bottomrule
    \end{tabular}
    % }
\end{center}
\end{small}
\end{table*}

\subsection{Comparison of Communication Overhead of WASP and Pre-Text for Federated Data Setting} \label{subsec:appendix_communication_overhead}

We compare the transmitted information for secure aggregation between the baseline method Pre-Text and our proposed WASP framework in \cref{tab:upload_download_comparison}. With the same number of participating data parties ($L$), WASP only requires aggregating additional $L$ histograms of dimension $\mathbb{R}^{|\mathcal{D}|}$ and uploading the aggregated histogram $H^f \in \mathbb{R}^{|\mathcal{D}|}$. These additional communicated information leads to only a minor increase in communication overhead compared to Pre-Text.



% \section{Theoretical Evidence for Convergence of WASP}
% \tianyuan{Update???}
% % \tianyuan{@yufei, here}

% In what follows, based on the generation process of Contrastive In-context Learning and the Top-$Q$ distance-based similarity voting and Mutation (see \cref{alg:algorithm_full_functions}), we provide the main theorems of convergence and their complete proofs in two scenarios: noiseless and noisy \((\epsilon,\delta)\)-DP. Our goal is to show that if, after multiple rounds of iteration, the algorithm can cover all private data points within a distance \(\le\eta\) with high probability, then the $1$-Wasserstein distance between the synthetic data distribution and the private data distribution is also at most \(\eta\). In the noiseless case, we can directly adopt the idea of positive probability of pulling in; in the noisy case, we additionally introduce a multiplicity assumption to counteract noise interference.

% \subsection*{\uppercase\expandafter{\romannumeral 1}. Noiseless Case: Positive Probability of Pulling In}

% First, under the noiseless assumption, we give the main theorem of convergence and its proof. In this situation, the algorithm does not add noise (\(\sigma=0\)) during voting, so it can accurately identify which synthetic samples are nearest or farthest from which private data points, thereby enabling more effective screening of high-/low-quality samples and contrastive prompt generation.

% \begin{assumption}[Positive Probability of Pulling In, Noiseless]
% \label{assump:noNoisePositive}
% Let \(S_{\mathrm{priv}} = \bigcup_{l=1}^L \mathcal{B}_l\) \tianyuan{@yufei, We call it $\mathcal{B}=\bigcup_{l=1}^L \mathcal{B}_l$ in our paper (see section 3)} be the private data set, and let \(\mathcal{D}^{(t)}\) be the result of the \(t\)th round of synthetic data. If, for some private sample \((\mathbf{z}^{\mathrm{real}},\,y^{\mathrm{real}})\), 
% \tianyuan{We also have a name for them in section 3, $(\mathbf{z}_j,\,u_j)$, so please remove $^{real}$.}
% the minimal distance
% \[
% d_{(\mathbf{z}^{\mathrm{real}},y^{\mathrm{real}})}^{(t)} 
% = \min_{(\mathbf{x},y)\in\mathcal{D}^{(t)}}\|\mathbf{z}^{\mathrm{real}} - \mathbf{x}\|\;>\;\eta,
% \]
% holds, then in the next-round generation \(\mathcal{D}^{(t+1)}\), there is a probability of at least \(\gamma>0\) that it is pulled in to
% \[
% d_{(\mathbf{z}^{\mathrm{real}},y^{\mathrm{real}})}^{(t+1)} \;\le\;\max\{\eta,\rho\,d_{(\mathbf{z}^{\mathrm{real}},y^{\mathrm{real}})}^{(t)}\},
% \]
% where \(0<\rho<1\). If \(d_{(\mathbf{z}^{\mathrm{real}},y^{\mathrm{real}})}^{(t)}\le\eta\), there is also a probability of at least \(1-\delta_{t}\) of maintaining \(d_{(\mathbf{z}^{\mathrm{real}},y^{\mathrm{real}})}^{(t+1)}\le\eta\) (i.e., not regressing to a greater distance).
% \end{assumption}

% \noindent
% Having multiple high-quality samples (those close to private data) and multiple low-quality samples (those far from private data) in the prompt does not invalidate this assumption; instead, it increases generation diversity. As long as the voting process ensures that any private point not yet covered still maintains sufficiently high weight (or uses bad-sample counterexamples for correction), the algorithm can, with positive probability, move the synthetic data toward that point.

% \begin{theorem}[Noiseless Convergence Theorem]
% \label{thm:noNoise}
% In the noiseless setting, if each round of contrastive prompt + voting generation satisfies the positive probability of pulling in assumption from \cref{assump:noNoisePositive}, then after \(T\) rounds of iteration, one can with high probability cover all private data within distance \(\le\eta\). Formally, let
% $d_{(\mathbf{z},y)}^{(t)}
% = \min_{(\mathbf{x},\,y')\in \mathcal{D}^{(t)}}\|\mathbf{z}-\mathbf{x}\|.$


% For any private sample \((\mathbf{z},y)\in S_{\mathrm{priv}}\), after the \(T\)th round we have
% $
% \Pr\Bigl[d_{(\mathbf{z},y)}^{(T)} \le \eta \Bigr]
% \;\ge\;
% 1-\sum_{t=0}^{T-1}\delta_t,
% $
% and by a union bound over all private points,
% \[
% \Pr\Bigl(
%   \forall\,(\mathbf{z},y)\in S_{\mathrm{priv}},\,d_{(\mathbf{z},y)}^{(T)}\le\eta
% \Bigr)
% \;\;\ge\;\;
% 1- \sum_{(\mathbf{z},y)}\Bigl[\sum_{t=0}^{T-1}\delta_t\Bigr].
% \]
% Hence one can construct a coupling yielding
% $
% W_1\bigl(
% P_{\mathrm{priv}},
% P_{\mathrm{syn}}^{(T)}
% \bigr)
% \;\le\;\eta.
% $
% \end{theorem}

% \begin{proof}[Proof]

% For a fixed private point \((\mathbf{z},y)\), define
% \[
% E_{(\mathbf{z},y),\,t}
% =
% \Bigl\{
%   d_{(\mathbf{z},y)}^{(t+1)} 
%   \;\le\;
%   \max\bigl\{\eta,\rho\,d_{(\mathbf{z},y)}^{(t)}\bigr\}
% \Bigr\}.
% \]
% From \cref{assump:noNoisePositive}, we know \(\Pr(E_{(\mathbf{z},y),t})\ge 1-\delta_t.\) If \(\bigcap_{t=0}^{T-1}E_{(\mathbf{z},y),t}\) occurs, then in each round, the distance is kept or shrunk by a factor \(\rho\). Assuming the initial distance \(d_{(\mathbf{z},y)}^{(0)}<D\) is finite, after multiple rounds we have \(d_{(\mathbf{z},y)}^{(T)}\le\eta\). A union bound over these events gives
% \[
% \Pr\Bigl(d_{(\mathbf{z},y)}^{(T)}\le\eta\Bigr)
% \;\ge\;
% \Pr\Bigl(\bigcap_{t=0}^{T-1} E_{(\mathbf{z},y),t}\Bigr)
% \;\ge\;
% 1-\sum_{t=0}^{T-1}\delta_j.
% \]
 
% Let \(E_{(\mathbf{z},y)}\) be the event \(d_{(\mathbf{z},y)}^{(T)}\le\eta\). Then \(\Pr(E_{(\mathbf{z},y)})\ge1-\sum_{t}\delta_t.\) For all \((\mathbf{z},y)\in S_{\mathrm{priv}}\), a union bound shows
% \[
% \Pr\Bigl(
% \bigcap_{(\mathbf{z},y)} E_{(\mathbf{z},y)}
% \Bigr)
% \;\;\ge\;\;
% 1
% -
% \sum_{(\mathbf{z},y)\in S_{\mathrm{priv}}}\Bigl[
%  1-\Pr(E_{(\mathbf{z},y)})
% \Bigr].
% \]
% Thus, with high probability, \emph{every} private point is covered by the synthetic data set after the \(T\)th round within distance \(\le\eta\).
 
% Once \(\min_{(\mathbf{x},y')\in \mathcal{D}^{(T)}}\|\mathbf{z}-\mathbf{x}\|\le\eta\) holds for all \((\mathbf{z},y)\), one can pair each private sample with the corresponding synthetic sample, so the pairwise distance is at most \(\eta\). From the definition of the Wasserstein distance, \(W_1\le\eta\) follows.
% \end{proof}

% \subsection*{\uppercase\expandafter{\romannumeral 2}. Noisy Setting: Multiplicity and Differential Privacy Convergence}

% Next, we extend our analysis to the \textbf{noisy} \((\epsilon,\delta)\)-DP case. In this scenario, the contrastive prompt voting must add noise (\(\sigma>0\)) to protect private data. As a result, the “nearest/farthest” statistics can be \textbf{misclassified}, potentially reducing the counts for genuinely high-quality samples or inflating counts for low-quality samples. To still guarantee convergence in a \textbf{finite number of rounds}, we adopt a “multiplicity” assumption—akin to the approach in \textbf{PE} algorithms—whereby each private point is not isolated but instead has several “similar samples” in its local neighborhood that vote in unison to \emph{counteract the noise}.

% \begin{assumption}[Multiplicity + Noisy Voting]
% \label{assump:noiseMultiplicity}
% In the \texttt{DP\_PrivateVoting} contrastive voting process, suppose that if a private data point \((\mathbf{z}, y)\) would normally assign a high count (e.g., “close” or “nearest” in some rounds) to a synthetic sample \((\mathbf{x}, y')\), then its \textbf{similar private points} also give \((\mathbf{x}, y')\) similar votes. Assume that each mode or local region contains at least \(m>1\) such “repeated points.” Then, provided the Gaussian/Laplace noise scale \(\sigma\) is not too large, the \textbf{aggregate} of these \(m\) votes remains significant, preserving a “positive probability of pulling in” in the next prompt/weight update. The failure probability \(\delta_t\) decays exponentially in terms of \(m\) and \(\sigma\).
% \end{assumption}

% This is analogous to similar ideas in prior work: as long as a small cluster of \(m\) private points all “favor” the same synthetic sample, random DP noise cannot \emph{completely} wipe out all \(m\) counts. Consequently, the new prompt/weights continue to pull the synthetic distribution toward that cluster’s region.

% \begin{theorem}[Noisy \((\epsilon,\delta)\)-DP Convergence Theorem]
% \label{thm:noiseMain}
% Let the voting process be implemented by a Gaussian/Laplace mechanism that satisfies \((\epsilon,\delta)\)-DP and the multiplicity assumption in \cref{assump:noiseMultiplicity}. If, in the noiseless setting, the “positive probability of pulling in” (\cref{assump:noNoisePositive}) ensures distances shrink by a factor \(\rho\) or become \(\le\eta\), then in the noisy case, one can still maintain a \emph{positive probability} \(\gamma_t>0\) in \textbf{each round}—provided \(m,\sigma,\) and related parameters are chosen appropriately—so that noise does not completely undermine the pulling-in effect. After \(T\) rounds, all private points can still be covered within \(\le\eta\) with high probability, yielding
% \[
% \Pr\Bigl[
% W_1\Bigl(
%   P_{\mathrm{priv}},
%   P_{\mathrm{syn}}^{(T)}
% \Bigr) \;\le\;\eta
% \Bigr]
% \;\ge\;
% 1 - \mathrm{Beta}(\epsilon,\delta,m,T),
% \]
% where \(\Beta(\cdot)\) is a bounding function derived from the Gaussian tail bound and union bound arguments, typically decreasing rapidly as \(m\) increases.
% \end{theorem}

% \begin{proof}[Proof]

% In \texttt{DP\_PrivateVoting}, Gaussian noise of scale \(\sigma\) is added. By the multiplicity assumption (\cref{assump:noiseMultiplicity}), if \(m\) private points in a cluster all select the same synthetic sample \((\mathbf{x},y')\) as their nearest/farthest, then their total vote cannot be completely annihilated by noise. From a standard Gaussian/Laplace tail bound, as long as \(m > c\sigma^2\) (for some appropriate constant \(c\)), the probability of severely distorting the count so that \((\mathbf{x},y')\) is fully ignored is exponentially small (denoted by \(\delta_t\)).

% Without noise, any point not yet covered within distance \(\eta\) is pulled in with probability \(\gamma>0\). With noise, these cluster-based votes become imperfect, but as long as the failure probability \(\delta_t\) is sufficiently small, the prompt/weight update still inherits a success rate \(\gamma_t = \gamma - \tau(\delta_t,\sigma,m)\). Hence, for each private point \((\mathbf{z},y)\), if \(d_{(\mathbf{z},y)}^{(t)}>\eta\), it is pulled toward \(\rho\,d_{(\mathbf{z},y)}^{(t)}\) or below \(\eta\) with probability at least \(\gamma_t\).

% Analogous to the proof of \cref{thm:noNoise}, fix \((\mathbf{z},y)\) and define
% \[
% E_{(\mathbf{z},y),t} 
% = \Bigl\{
%    d_{(\mathbf{z},y)}^{(t+1)} \;\le\; \max\{\eta,\rho\,d_{(\mathbf{z},y)}^{(t)}\}
% \Bigr\}.
% \]
% With positive probability (at least \(1 - \delta_t\)), the distance remains below \(\rho\,d_{(\mathbf{z},y)}^{(t)}\) or \(\eta\). Iterating over \(t=0,\dots,T-1\) and applying a union bound across all private points yields a high-probability guarantee that after \(T\) rounds, they are each covered by a synthetic sample within distance \(\eta\).  
 
% Once all private points are covered within \(\eta\), one can pair each private sample with a corresponding synthetic sample, leading to \(W_1 \le \eta\). Thus, the theorem follows.
% \end{proof}

\section{Additional Related Works} \label{sec:appendix_related_work}
Due to space limitation, we include the discussion of previous works related to Contrastive In-context Learning (Contrastive ICL)
% and Federated Learning (FL) 
here in the Appendix.

\textbf{Contrastive In-context Learning.}\footnote{Works~\cite{ren2024towards,miyanishi2024multimodal} considering understanding in-context learning with contrastive learning theories are sometimes referred to using the same name, but we do not consider them here.}
% Here, we do not consider works that 
The idea of using contrastive information to enrich in-context learning samples has been exploited from different aspects. Samples belonging to positive and negative classes~\cite{liang2024context}, correct or wrong self-predictions of training samples during training time~\cite{mo2024cicl}, human-preferred and non-preferred question responses~\citet{gao2024customizing} have all been utilized as contrastive samples.
% \citet{mo2024cicl} exploits using correct/wrong self-predictions of training samples as in-context samples to aid test time performance. 
% \citet{liang2024context} uses samples belong to positive and negative classes as contrastive information for identifying causality and non-causality whitin a sentence.
Our study is the first known effort to consider contrastive in-context learning for synthetic data generation, by treating synthetic samples of different qualities generated by multiple PLMs as contrastive information.

% \textbf{Federated Learning (FL)}. 
% FL was first proposed by Google~\cite{McMahan2016fl} 
% % in which a cross-device collaboration scenario is described 
% % in which millions of mobile users collaboratively train a shared model using their local private data in a decentralized manner without directly sharing their data thereby preserving privacy. 
% enabling millions of mobile users to collaboratively train a shared model using their local private data without direct data sharing, thus preserving privacy.
% % In FL, each local model is updated independently by its respective party using private data and the aggregated knowledge from each local model is incorporated into the shared global model through the exchange of model parameter updates, thereby enhancing its performance. Subsequently, the updated global model is redistributed to each local model for further iterative improvements. 
% In the era of LLMs, FL has emerged as a decentralized training paradigm utilizing distributed data~\cite{fan2023fate,kuang2024federatedscope,ye2024openfedllm,zheng2024safely,goetz2020federated}. Additionally, research has explored how LLMs can enhance FL, investigating methods like federated synthesis to protect and enhance private data~\cite{goetz2020federated,behera2022fedsyn,little2023federated,hou2024pretext}, model initialization to speed up convergence~\cite{tan2022federated,nguyen2022begin,liu2024language}, personalization improvement~\cite{yang2023efficient,li2024visual}, etc. However, these works are all motivated by the decentralized data overlooking the potential of distributed LLMs.


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
