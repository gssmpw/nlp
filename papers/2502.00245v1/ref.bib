@inproceedings{langley00,
 author    = {P. Langley},
 title     = {{Crafting Papers on Machine Learning}},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

%%%%%%%%%%%% models %%%%%%%%%%%%
@inproceedings{devlin2019bert,
    title = "{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{chung2022scaling,
  title={{Scaling Instruction-finetuned Language Models}},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{achiam2023gpt,
  title={{GPT-4 Technical Report}},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{hurst2024gpt4o,
  title={{GPT-4o System Card}},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{team2023gemini,
  title={{Gemini: A Family of Highly Capable Multimodal Models}},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama2,
  title={{Llama 2: Open Foundation and Fine-tuned Chat Models, 2023}},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={URL https://arxiv. org/abs/2307.09288},
  year={2023}
}

@misc{vicuna2023,
    title = {{Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality}},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{du2022glm,
  title={{GLM: General Language Model Pretraining with Autoregressive Blank Infilling}},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{radford2019language,
  title={{Language Models are Unsupervised Multitask Learners}},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{keskar2019ctrl,
  title={{Ctrl: A Conditional Transformer Language Model for Controllable Generation}},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@misc{zhang2022opt,
      title={{OPT: Open Pre-trained Transformer Language Models}}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{brown2020language,
  title={{Language Models are Few-shot Learners}},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{schick2020s,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}

@online{openai2023gpt4,
  author = {{OpenAI}},
  title = {{GPT-4-Turbo and GPT-4}},
  year = 2023,
  url = {https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4},
  urldate = {2024-06-12}
}

@online{openai2021gpt3-5,
  author = {{OpenAI}},
  title = {{GPT-3.5-Turbo}},
  year = 2021,
  url = {https://platform.openai.com/docs/models/gpt-3-5-turbo},
  urldate = {2024-06-12}
}

@inproceedings{ni2022sentencet5base,
  title={{Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}},
  author={Ni, Jianmo and Abrego, Gustavo Hernandez and Constant, Noah and Ma, Ji and Hall, Keith and Cer, Daniel and Yang, Yinfei},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={1864--1874},
  year={2022}
}
%%%%%%%%%%%% models %%%%%%%%%%%%


%%%%%%%%%%%% small model efficiency %%%%%%%%%%%%
@inproceedings{polino2018model,
  title={Model compression via distillation and quantization},
  author={Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{bommasani2021opportunities,
  title={{On the Opportunities and Risks of Foundation Models}},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
%%%%%%%%%%%% small model efficiency %%%%%%%%%%%%


%%%%%%%%%%%% prompt engineering %%%%%%%%%%%%
@article{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1909.01066},
  year={2019}
}
%%%%%%%%%%%% prompt engineering %%%%%%%%%%%%


%%%%%%%%%%%% tune %%%%%%%%%%%%
@article{xu2023wizardlm,
  title={{Wizardlm: Empowering large language models to follow complex instructions}},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{ding2023enhancing,
  title={{Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{raffel2020exploring,
  title={{Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer}},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{jaques2019way,
  title={{Way Off-policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog}},
  author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
  journal={arXiv preprint arXiv:1907.00456},
  year={2019}
}

@article{korbak2022rl,
  title={RL with KL penalties is better viewed as Bayesian inference},
  author={Korbak, Tomasz and Perez, Ethan and Buckley, Christopher L},
  journal={arXiv preprint arXiv:2205.11275},
  year={2022}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
%%%%%%%%%%%% tune %%%%%%%%%%%%


%%%%%%%%%%%% zero-shot fine-tune %%%%%%%%%%%%
@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{sanh2022multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={The Tenth International Conference on Learning Representations},
  year={2022}
}

@article{wei2022finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={The Tenth International Conference on Learning Representation},
  year={2022}
}

@article{zhong2021adapting,
  title={Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections},
  author={Zhong, Ruiqi and Lee, Kristy and Zhang, Zheng and Klein, Dan},
  journal={arXiv preprint arXiv:2104.04670},
  year={2021}
}

@article{yin2020universal,
  title={Universal natural language processing with limited annotations: Try few-shot textual entailment as a start},
  author={Yin, Wenpeng and Rajani, Nazneen Fatema and Radev, Dragomir and Socher, Richard and Xiong, Caiming},
  journal={arXiv preprint arXiv:2010.02584},
  year={2020}
}

@InProceedings{adina2018abroad_mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "{A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference}",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
%%%%%%%%%%%% zero-shot fine-tune %%%%%%%%%%%%


%%%%%%%%%%%% generation tasks %%%%%%%%%%%%
@inproceedings{ye2022zerogen,
  title={{ZeroGen: Efficient Zero-shot Learning via Dataset Generation}},
  author={Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11653--11669},
  year={2022}
}

@inproceedings{ye2022progen,
  title={{ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback}},
  author={Ye, Jiacheng and Gao, Jiahui and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={3671--3683},
  year={2022}
}

@article{meng2022generating,
  title={{Generating Training Data with Language Models: Towards Zero-shot Language Understanding}},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={462--477},
  year={2022}
}

@inproceedings{gao2023self,
  title={{Self-guided Noise-free Data Generation for Efficient Zero-shot Learning}},
  author={Gao, Jiahui and Pi, Renjie and Yong, Lin and Xu, Hang and Ye, Jiacheng and Wu, Zhiyong and Zhang, Weizhong and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng},
  booktitle={Proceedings of The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{jawahar2023llm,
  title={{LLM} Performance Predictors are good initializers for Architecture Search},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS and Ding, Dujian},
  journal={arXiv preprint arXiv:2310.16712},
  year={2023}
}

@article{yu2024large,
  title={{Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias}},
  author={Yu, Yue and Zhuang, Yuchen and Zhang, Jieyu and Meng, Yu and Ratner, Alexander J and Krishna, Ranjay and Shen, Jiaming and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zou2024fusegen,
    title = "{{FuseGen: PLM Fusion for Data-generation based Zero-shot Learning}}",
    author = {Zou, Tianyuan and Liu, Yang and Li, Peng and Zhang, Jianqing and Liu, Jingjing and Zhang, Ya-Qin},
    editor = "Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    pages = "2172--2190",
}

@article{su2022contrastive,
  title={{Contrastive Search is What You Need for Neural Text Generation}},
  author={Su, Yixuan and Collier, Nigel},
  journal={arXiv preprint arXiv:2210.14140},
  year={2022}
}

@article{jung2023impossible,
  title={Impossible distillation: from low-quality model to high-quality dataset \& model for summarization and paraphrasing},
  author={Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman, Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor and Choi, Yejin},
  journal={arXiv preprint arXiv:2305.16635},
  year={2023}
}

@article{bolon2013review,
  title={{A Review of Feature Selection Methods on Synthetic Data}},
  author={Bol{\'o}n-Canedo, Ver{\'o}nica and S{\'a}nchez-Maro{\~n}o, Noelia and Alonso-Betanzos, Amparo},
  journal={Knowledge and information systems},
  volume={34},
  pages={483--519},
  year={2013},
  publisher={Springer}
}

@inproceedings{fan2018hierarchical,
    title = "{Hierarchical Neural Story Generation}",
    author = "Fan, Angela  and Lewis, Mike  and Dauphin, Yann",
    editor = "Gurevych, Iryna  and Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
}

@inproceedings{holtzman2020the,
    title={{The Curious Case of Neural Text Degeneration}},
    author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
    booktitle={Proceedings of the Eighth International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rygGQyrFvH}
}
%%%%%%%%%%%% generation tasks %%%%%%%%%%%%

%%%%%%%%%%%% importance of data %%%%%%%%%%%%
@article{budach2022effects,
  title={{The Effects of Data Quality on ML-Model Performance}},
  author={Budach, Lukas and Feuerpfeil, Moritz and Ihde, Nina and Nathansen, Andrea and Noack, Nele Sina and Patzlaff, Hendrik and Harmouch, Hazar and Naumann, Felix},
  journal={CoRR abs/2207.14529},
  year={2022}
}

@article{wang2024survey,
  title={{A Survey on Data Selection for LLM Instruction Tuning}},
  author={Wang, Jiahao and Zhang, Bolin and Du, Qianlong and Zhang, Jiajun and Chu, Dianhui},
  journal={arXiv preprint arXiv:2402.05123},
  year={2024}
}
%%%%%%%%%%%% importance of data %%%%%%%%%%%%

%%%%%%%%%%%% dp data generation %%%%%%%%%%%%
@inproceedings{lin2024differentially,
  title={{Differentially Private Synthetic Data via Foundation Model APIs 1: Images}},
  author={Lin, Zinan and Gopi, Sivakanth and Kulkarni, Janardhan and Nori, Harsha and Yekhanin, Sergey},
  year={2024},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{xie2024differentially,
  title={{Differentially Private Synthetic Data via Foundation Model APIs 2: Tex}t},
  author={Xie, Chulin and Lin, Zinan and Backurs, Arturs and Gopi, Sivakanth and Yu, Da and Inan, Huseyin A and Nori, Harsha and Jiang, Haotian and Zhang, Huishuai and Lee, Yin Tat and others},
  year={2024},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{hou2024pretext,
  title={{PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs}},
  author={Hou, Charlie and Shrivastava, Akshat and Zhan, Hongyuan and Conway, Rylan and Le, Trang and Sagar, Adithya and Fanti, Giulia and Lazar, Daniel},
  year={2024},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{li2024privimage,
  title={{$\{$PrivImage$\}$: Differentially Private Synthetic Image Generation using Diffusion Models with $\{$Semantic-Aware$\}$ Pretraining}},
  author={Li, Kecen and Gong, Chen and Li, Zhixiang and Zhao, Yuzhong and Hou, Xinwen and Wang, Tianhao},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={4837--4854},
  year={2024}
}

@inproceedings{abacha2024synthetic,
  title={Synthetic Data Aided Federated Learning Using Foundation Models},
  author={Abacha, Fatima and Teo, Sin G and Cordeiro, Lucas C and Mustafa, Mustafa A},
  booktitle={International Workshop on Federated Learning in the Age of Foundation Models In Conjunction with IJCAI 2024 (FL@ FM-IJCAI'24)},
  year={2024}
}

@inproceedings{dwork2006differential,
  title={{Differential Privacy}},
  author={Dwork, Cynthia},
  booktitle={International colloquium on automata, languages, and programming},
  pages={1--12},
  year={2006},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={{The Algorithmic Foundations of Differential Privacy}},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@article{dong2022gaussian,
  title={{Gaussian Differential Privacy}},
  author={Dong, Jinshuo and Roth, Aaron and Su, Weijie J},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={84},
  number={1},
  pages={3--37},
  year={2022},
  publisher={Wiley Online Library}
}

@inproceedings{balle2018improving,
  title={{Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising}},
  author={Balle, Borja and Wang, Yu-Xiang},
  booktitle={International Conference on Machine Learning},
  pages={394--403},
  year={2018},
  organization={PMLR}
}

@misc{steinke2022composition,
  title={{Composition of Differential Privacy \& Privacy Amplification by Subsampling}}, 
  author={Thomas Steinke},
  year={2022},
  eprint={2210.00597},
  archivePrefix={arXiv},
  primaryClass={cs.CR},
  url={https://arxiv.org/abs/2210.00597}, 
}

@inproceedings{abadi2016deep,
  title={{Deep Learning with Differential Privacy}},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@inproceedings{bommasani2019towards,
  title={{Towards Private Synthetic Text Generation}},
  author={Bommasani, Rishi and Wu, Steven and Schofield, Xanda},
  booktitle={NeurIPS 2019 Machine Learning with Guarantees Workshop},
  year={2019}
}

@inproceedings{mattern2022differentially,
  title={{Differentially Private Language Models for Secure Data Sharing}},
  author={Mattern, Justus and Jin, Zhijing and Weggenmann, Benjamin and Schoelkopf, Bernhard and Sachan, Mrinmaya},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4860--4873},
  year={2022}
}

@inproceedings{yue2023synthetic,
  title={{Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe}},
  author={Yue, Xiang and Inan, Huseyin and Li, Xuechen and Kumar, Girish and McAnallen, Julia and Shajari, Hoda and Sun, Huan and Levitan, David and Sim, Robert},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1321--1342},
  year={2023}
}

@misc{putta2023differentially,
  title={{Differentially Private Conditional Text Generation For Synthetic Data Production}},
  author={Pranav Putta and Ander Steele and Joseph W Ferrara},
  year={2023},
  url={https://openreview.net/forum?id=LUql3ZOFwFD}
}

@article{flemings2024differentially,
  title={{Differentially Private Knowledge Distillation via Synthetic Text Generation}},
  author={Flemings, James and Annavaram, Murali},
  journal={arXiv preprint arXiv:2403.00932},
  year={2024}
}

@inproceedings{zhao2024generate,
  title={{Generate Synthetic Text Approximating the Private Distribution with Differential Privacy}},
  author={Zhao, Wenhao and Song, Shaoyang and Zhou, Chunlai},
  booktitle={Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
  pages={6651--6659},
  year={2024}
}

@article{bojkovic2024differentially,
  title={{Differentially Private Synthetic Data with Private Density Estimation}},
  author={Bojkovic, Nikolija and Loh, Po-Ling},
  journal={arXiv preprint arXiv:2405.04554},
  year={2024}
}

@inproceedings{nagesh2024private,
  title={{Private Text Generation by Seeding Large Language Model Prompts}},
  author={Nagesh, Supriya and Chen, Justin Y and Mishra, Nina and Wagner, Tal},
  year = {2024},
  booktitle={GenAI for Health: Potential, Trust and Policy Compliance}
}
%%%%%%%%%%%% dp data generation %%%%%%%%%%%%


%%%%%%%%%%%% private data for domain utilization %%%%%%%%%%%%
@article{rumshisky2016predicting,
  title={{Predicting Early Psychiatric Readmission with Natural Language Processing of Narrative Discharge Summaries}},
  author={Rumshisky, Anna and Ghassemi, Marzyeh and Naumann, Tristan and Szolovits, Peter and Castro, VM and McCoy, TH and Perlis, RH},
  journal={Translational psychiatry},
  volume={6},
  number={10},
  pages={e921--e921},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{chew2022use,
  title={{The Use of Artificial Intelligence--based Conversational Agents (Chatbots) for Weight Loss: Scoping Review and Practical Recommendations}},
  author={Chew, Han Shi Jocelyn},
  journal={JMIR medical informatics},
  volume={10},
  number={4},
  pages={e32578},
  year={2022},
  publisher={JMIR Publications Toronto, Canada}
}

@inproceedings{yu2024privacypreserving,
  title={{Privacy-Preserving Instructions for Aligning Large Language Models}},
  author={Da Yu and Peter Kairouz and Sewoong Oh and Zheng Xu},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=mUT1biz09t}
}
%%%%%%%%%%%% private data for domain utilization %%%%%%%%%%%%



%%%%%%%%%%%% multi-model fusion %%%%%%%%%%%%
@inproceedings{wan2024knowledge,
  title={{Knowledge Fusion of Large Language Models}},
  author={Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  booktitle={Proceedings of The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=jiDsk12qcz}
}

@article{li2024more,
  title={{More Agents is All You Need}},
  author={Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng},
  journal={arXiv preprint arXiv:2402.05120},
  year={2024}
}

@article{mavromatis2024pack,
  title={{Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization}},
  author={Mavromatis, Costas and Karypis, Petros and Karypis, George},
  journal={arXiv preprint arXiv:2404.11531},
  year={2024}
}

@article{wan2024fusechat,
  title={{FuseChat: Knowledge Fusion of Chat Models}},
  author={Wan, Fanqi and Yang, Ziyi and Zhong, Longguang and Quan, Xiaojun and Huang, Xinting and Bi, Wei},
  journal={arXiv preprint arXiv:2402.16107},
  year={2024}
}

@inproceedings{liu2023dynamic,
  title={{A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration}},
  author={Liu, Zijun and Zhang, Yanzhe and Li, Peng and Liu, Yang and Yang, Diyi},
  booktitle={Proceedings of the 1st Conference on Language Modeling (COLM 2024).},
  year={2024}
}

@article{du2023improving,
  title={{Improving Factuality and Reasoning in Language Models through Multiagent Debate}},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023}
}
%%%%%%%%%%%% multi-model fusion %%%%%%%%%%%%


%%%%%%%%%%%% single-model reflection/refinement %%%%%%%%%%%%
@inproceedings{shinn2023reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik R and Yao, Shunyu},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{kim2023language,
  title={Language models can solve computer tasks},
  author={Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
  journal={arXiv preprint arXiv:2303.17491},
  year={2023}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

%%%%%%%%%%%% single-model reflection/refinement %%%%%%%%%%%%


%%%%%%%%%%%% FL %%%%%%%%%%%%
@article{McMahan2016fl,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {{Federated Learning of Deep Networks using Model Averaging}},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  archivePrefix = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/McMahanMRA16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%%%%%%%%%%%% FL %%%%%%%%%%%%


%%%%%%%%%%%% golden datasets %%%%%%%%%%%%
@InProceedings{maas2011learning_imdb,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {{Learning Word Vectors for Sentiment Analysis}},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@inproceedings{socher2013recursive_sst2,
    title = "{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@article{zhang2015character_agnews_yelp,
  title={{Character-Level Convolutional Networks for Text Classification}},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@online{yelpopendataset,
  author = {{Inc. Yelp}},
  title = {{Yelp Open Dataset}},
  year = 2015,
  url = {https://www.yelp.com/dataset},
  urldate = {2025-01-07}
}

@inproceedings{wang2019glue,
  title={{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  note={In the Proceedings of The Seventh International Conference on Learning Representation.},
  year={2019}
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ {Questions for Machine Comprehension of Text}",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
    eprint={1606.05250},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

@inproceedings{Casanueva2020banking77,
    author      = {I{\~{n}}igo Casanueva and Tadas Temcinas and Daniela Gerz and Matthew Henderson and Ivan Vulic},
    title       = {{Efficient Intent Detection with Dual Sentence Encoders}},
    year        = {2020},
    month       = {mar},
    note        = {Data available at https://github.com/PolyAI-LDN/task-specific-datasets},
    url         = {https://arxiv.org/abs/2003.04807},
    booktitle   = {Proceedings of the 2nd Workshop on NLP for ConvAI - ACL 2020}
}
%%%%%%%%%%%% golden datasets %%%%%%%%%%%%


@inproceedings{dai2007boosting,
  title={{Boosting for Transfer Learning}},
  author={Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={193--200},
  year={2007}
}

@inproceedings{swayamdipta2020dataset,
  title={{Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics}},
  author={Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9275--9293},
  year={2020}
}

@article{wang2023openchat,
  title={{Openchat: Advancing Open-source Language Models with Mixed-quality Data}},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}

@article{deng2023mutual,
  title={{Mutual Enhancement of Large and Small Language Models with Cross-silo Knowledge Transfer}},
  author={Deng, Yongheng and Qiao, Ziqing and Ren, Ju and Liu, Yang and Zhang, Yaoxue},
  journal={arXiv preprint arXiv:2312.05842},
  year={2023}
}

@article{mo2024cicl,
  title={{C-ICL: Contrastive In-Context Learning for Information Extraction}},
  author={Mo, Ying and Liu, Jiahao and Yang, Jian and Wang, Qifan and Zhang, Shun and Wang, Jingang and Li, Zhoujun},
  journal={arXiv preprint arXiv:2402.11254},
  year={2024}
}


%%%%%%%%%%%% Dirichlet Partition %%%%%%%%%%%%
@inproceedings{yurochkin2019bayesian,
  title={{Bayesian Nonparametric Federated Learning of Neural Networks}},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International conference on machine learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@article{hsu2019measuring,
  title={{Measuring the Effects of Non-identical Data Distribution for Federated Visual Classification}},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@inproceedings{zhang2023fedala,
  title={{FedALA: Adaptive Local Aggregation for Personalized Federated Learning}},
  author={Zhang, Jianqing and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Guan, Haibing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11237--11244},
  year={2023}
}
%%%%%%%%%%%% Dirichlet Partition %%%%%%%%%%%%



%%%%%%%%%%%% FL+LLM %%%%%%%%%%%%
@article{fan2023fate,
  title={{Fate-LLM: A Industrial Grade Federated Learning Framework for Large Language Models}},
  author={Fan, Tao and Kang, Yan and Ma, Guoqiang and Chen, Weijing and Wei, Wenbin and Fan, Lixin and Yang, Qiang},
  journal={arXiv preprint arXiv:2310.10049},
  year={2023}
}

@inproceedings{kuang2024federatedscope,
  title={{FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large lLanguage Models in Federated Learning}},
  author={Kuang, Weirui and Qian, Bingchen and Li, Zitao and Chen, Daoyuan and Gao, Dawei and Pan, Xuchen and Xie, Yuexiang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5260--5271},
  year={2024}
}

@inproceedings{ye2024openfedllm,
  title={{OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning}},
  author={Ye, Rui and Wang, Wenhao and Chai, Jingyi and Li, Dihan and Li, Zexi and Xu, Yinda and Du, Yaxin and Wang, Yanfeng and Chen, Siheng},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6137--6147},
  year={2024}
}

% synthetic data instead of model parameters
@article{goetz2020federated,
  title={{Federated Learning via Synthetic Data}},
  author={Goetz, Jack and Tewari, Ambuj},
  journal={arXiv preprint arXiv:2008.04489},
  year={2020}
}

% federated synthesis
@article{behera2022fedsyn,
  title={{FedSyn: Synthetic Data Generation using Federated Learning}},
  author={Behera, Monik Raj and Upadhyay, Sudhir and Shetty, Suresh and Priyadarshini, Sudha and Patel, Palka and Lee, Ker Farn},
  journal={arXiv preprint arXiv:2203.05931},
  year={2022}
}

% federated synthesis
@article{little2023federated,
  title={{Federated Learning for Generating Synthetic Data: A Scoping Review}},
  author={Little, Claire and Elliot, Mark and Allmendinger, Richard},
  journal={International Journal of Population Data Science},
  volume={8},
  number={1},
  year={2023},
  publisher={Swansea University}
}

@article{zheng2024safely,
  title={{Safely Learning with Private Data: A Federated Learning Framework for Large Language Model}},
  author={Zheng, JiaYing and Zhang, HaiNan and Wang, LingXiang and Qiu, WangJie and Zheng, HongWei and Zheng, ZhiMing},
  journal={arXiv preprint arXiv:2406.14898},
  year={2024}
}

@article{chen2024integration,
  title={{Integration of Large Language Models and Federated Learning}},
  author={Chen, Chaochao and Feng, Xiaohua and Li, Yuyuan and Lyu, Lingjuan and Zhou, Jun and Zheng, Xiaolin and Yin, Jianwei},
  journal={Patterns},
  volume={5},
  number={12},
  year={2024},
  publisher={Elsevier}
}

% personalization
@inproceedings{yang2023efficient, 
  title={{Efficient Model Personalization in Federated Learning via Client-specific Prompt Generation}},
  author={Yang, Fu-En and Wang, Chien-Yi and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19159--19168},
  year={2023}
}

% personalization
@article{li2024visual,
  title={{Visual Prompt Based Personalized Federated Learning}},
  author={Li, Guanghao and Wu, Wansen and Sun, Yan and Shen, Li and Wu, Baoyuan and Tao, Dacheng},
  journal={Transactions on Machine Learning Research},
  year={2024}
}

% LLM initialize FL training
@article{tan2022federated,
  title={{Federated Learning from Pre-trained Models: A Contrastive Learning Approach}},
  author={Tan, Yue and Long, Guodong and Ma, Jie and Liu, Lu and Zhou, Tianyi and Jiang, Jing},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19332--19344},
  year={2022}
}

% LLM initialize FL training
@inproceedings{nguyen2022begin,
  title={{Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning}},
  author={Nguyen, John and Wang, Jianyu and Malik, Kshitiz and Sanjabi, Maziar and Rabbat, Michael},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

% LLM initialize FL training
@inproceedings{liu2024language,
  title={{Language-Guided Transformer for Federated Multi-Label Classification}},
  author={Liu, I-Jieh and Lin, Ci-Siang and Yang, Fu-En and Wang, Yu-Chiang Frank},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={12},
  pages={13882--13890},
  year={2024}
}
%%%%%%%%%%%% FL+LLM %%%%%%%%%%%%


%%%%%%%%%%%% FL & DP+FL & DP+FL+LLM %%%%%%%%%%%%
@inproceedings{wang2024can,
  title={{Can Public Large Language Models Help Private Cross-device Federated Learning?}},
  author={Wang, Boxin and Zhang, Yibo and Cao, Yuan and Li, Bo and McMahan, Hugh and Oh, Sewoong and Xu, Zheng and Zaheer, Manzil},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={934--949},
  year={2024}
}

@inproceedings{bonawitz2016practical,
  title={{Practical Secure Aggregation for Federated Learning on User-Held Data}},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={NIPS 2016 Workshop on Private Multi-Party Machine Learning},
  year={2016},
}

%%%%%%%%%%%% FL & DP+FL & DP+FL+LLM %%%%%%%%%%%%


%%%%%%%%%%%% Contrastive In-context Learning %%%%%%%%%%%%
@inproceedings{ren2024towards,
  title={{Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens}},
  author={Ruifeng Ren and Yong Liu},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=dB6gwSDXKL}
}

@article{miyanishi2024multimodal,
  title={{Multimodal Contrastive In-Context Learning}},
  author={Miyanishi, Yosuke and Nguyen, Minh Le},
  journal={arXiv preprint arXiv:2408.12959},
  year={2024}
}

@article{liang2024context,
  title={{In-context Contrastive Learning for Event Causality Identification}},
  author={Liang, Chao and Xiang, Wei and Wang, Bang},
  journal={arXiv preprint arXiv:2405.10512},
  year={2024}
}

@inproceedings{gao2024customizing,
  title={{Customizing Language Model Responses with Contrastive In-Context Learning}},
  author={Gao, Xiang and Das, Kamalika},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18039--18046},
  year={2024}
}
 %%%%%%%%%%%% Contrastive In-context Learning %%%%%%%%%%%%


%%%% FID %%%%
@article{heusel2017gans,
  title={{Gans Trained by a Two Time-scale Update Rule Converge to a Local Nash Equilibrium}},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
%%%% FID %%%%


%%%%%%%%%%%% Inadequate Sample %%%%%%%%%%%%
@article{al2016transfer,
  title={Transfer learning for class imbalance problems with inadequate data},
  author={Al-Stouhi, Samir and Reddy, Chandan K},
  journal={Knowledge and information systems},
  volume={48},
  pages={201--228},
  year={2016},
  publisher={Springer}
}

@inproceedings{ren2019almost,
  title={{Almost Unsupervised Text to Speech and Automatic Speech Recognition}},
  author={Ren, Yi and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  booktitle={International conference on machine learning},
  pages={5410--5419},
  year={2019},
  organization={PMLR}
}


@article{zdrazil2024chembl,
  title={{The ChEMBL Database in 2023: A Drug Discovery Platform Spanning Multiple Bioactivity Data Types and Time Periods}},
  author={Zdrazil, Barbara and Felix, Eloy and Hunter, Fiona and Manners, Emma J and Blackshaw, James and Corbett, Sybilla and de Veij, Marleen and Ioannidis, Harris and Lopez, David Mendez and Mosquera, Juan F and others},
  journal={Nucleic acids research},
  volume={52},
  number={D1},
  pages={D1180--D1192},
  year={2024},
  publisher={Oxford University Press}
}
%%%%%%%%%%%% Inadequate Sample %%%%%%%%%%%%
