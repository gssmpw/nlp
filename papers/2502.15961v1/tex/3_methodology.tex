% \section{Methodology and Framework/Implementation}
\section{Proposed Approach}\label{sec:approach}
% Intro this section and outline the subsections

\begin{figure}[t]
\centering
% \includegraphics[width=2.5in]{fig1}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/3_/planner_diagramv2.pdf}
\caption{Overview of the IA-TIGRIS planning framework. As the robot is executing the current path, the belief map is updated using the current robot position. Whenever a new plan is initiated, the current robot position, belief map, and remaining budget are passed to the planner. The new path is merged into the previous path and given to the autopilot for execution.}
\label{fig:planner_diagram}
\end{figure}

This section details our proposed approach for informative path planning online using incremental and adaptive methods. A high level overview of our framework is shown in Fig.~\ref{fig:planner_diagram}. Subsection \ref{sec:alg_overview} provides a thorough explanation of the core algorithm, including the main planning loop and the graph update procedure, elucidating the steps involved in sampling, steering, and constructing the tree of potential paths. Subsection \ref{sec:replanning} discusses the strategy to enable adaptive replanning with the presented algorithm. Finally, Subsection \ref{sec:incr-adap-plan} explores the techniques used to incrementally update the path, enhancing the efficiency and responsiveness of the planning process. Subsections \ref{sec:beliefspace} and \ref{sec:sensor-model} explain how the belief space and observation model integrate into the planner, and Subsection \ref{sec:reward-models} presents our reward models. 

\subsection{Algorithm Overview}\label{sec:alg_overview}
% 
The IA-TIGRIS algorithm comprises two main components: the main planning loop (Algorithm \ref{alg:ours1}) and the graph update procedure (Algorithm \ref{alg:ours2}). Let the notation $X \xleftarrow{+} \{\mathbf{x}\}$ and $X \xleftarrow{-} \{\mathbf{x}\}$ represent the operations $X \leftarrow X \cup \{\mathbf{x}\}$ and $X \leftarrow X \setminus \{\mathbf{x}\}$ respectively. The \PlannerNameSpaced planning loop takes as input the agent start pose, $x_{start}$, the state space $\mathcal{X}$, the remaining budget $B$, the remaining allowed planning time $T$, the extend distance for tree building $\Delta$, and the radius for neighbor selection $R$. 


At the beginning of planning, the set of vertices $V$ is empty. The initial information $I_s$ at the start pose $x_{start}$ is calculated by $I(\cdot)$, and the initial cost $C$ is set to 0. The tuple $n_{start}$ is initialized and added to the set of vertices $V$, while the set of edges $E$ and the closed set of vertices $V_{closed}$ are initialized as empty. The graph G is initialized as $G \leftarrow (V, E)$. 


% \BM{
% Need to highlight key differences. Compare to original. Slight restructure to ensure neighbor can go below extend.

% Also should be correct on what passed in. Should we say start node? Should we say old tree? We shouldn't clear the tree if we are replanning. That isn't correct. 

% Change. Move the computation of near outside the check if feasible same as near. Allows for others to come and connect to it. Doesn't just stop.

% Also remove the condition that near has to be not in the closed set. 

% Make sure we have the detail about allowing near to go less than extend.}



The Algorithm \ref{alg:ours1} then loops until the computation time exceeds the allowed planning time $T$. During each iteration of the loop, a sample $x_{sample}$ is drawn from the state space $\mathcal{X}$ using the \textsc{InformedSample} function which is a weighted sampler based on the reward for viewing a state in the belief space. This is done by first conducting the weighted sample in the belief space and then uniformly sampling in the subset of the configuration space that views the sampled belief space state while respecting desired observation constraints. Informed sampling focuses paths on areas of higher reward rather than uniformly sampling in the search space.


\begin{algorithm}[t]
\SetInd{0.4em}{0.8em}
% \setstretch{1}
\DontPrintSemicolon
\small

\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwFunction{InformedSample}{InformedSample}

\Input{$\mathbf{x}_{start}$: Agent start pose \\
       $\mathcal{X}$: State space \\
       $B$: Budget \\
       $T$: Allowed planning time \\
       $\Delta$: Extend distance for tree building\\
       $R$: Radius for neighbor selection}

\Output{$\mathcal{T}$: Planned path}
\If{$V=\emptyset$}
{
    $I_s \leftarrow I(\mathbf{x}_{start})$; $C \leftarrow 0$\;
    $n_{start} \leftarrow \{\mathbf{x}_{start}, I_s, C\}$\;
    $V \leftarrow {n_{start}}$; $E \leftarrow \emptyset$; $V_{closed} \leftarrow \emptyset$; $G \leftarrow (V,E)$\;
}
\Else{
    \textsc{UpdateGraph}$(\mathbf{x}_{start},B)$
}

\While{computation time $< T$}
{
    $\mathbf{x}_{sample} \leftarrow$ \textsc{InformedSample}$(\mathcal{X})$\;
    $n_{nearest} \leftarrow $\textsc{Nearest}$(\mathbf{x}_{sample})$\;
    $\mathbf{x}_{feas}, e_{feas} \leftarrow $\textsc{Steer}$(n_{nearest}, \mathbf{x}_{sample}, \Delta, X_{free})$\;
    \textsc{AddPoseToGraph}$(\mathbf{x}_{feas}, e_{feas}, n_{nearest}, \Delta, R, B)$\;
    $N_{near} \leftarrow $\textsc{Near}$(\mathbf{x}_{feas}, R, V \setminus V_{closed})$\;
    \For{$n_{near} \in N_{near}$}
    {
        \If{$n_{near} \ne x_{feas}$ and $n_{near} \ne n_{nearest}$}
        {
        
            $\mathbf{x}_{new},e_{new}\leftarrow$\textsc{Steer}$(n_{near},\mathbf{x}_{feas},\!\Delta, X_{free})$\;
            \If{$\mathbf{x}_{new} \neq n_{near}$}
            {
                \textsc{AddPoseToGraph}$(\mathbf{x}_{new}, e_{new}, n_{near}, \Delta, R, B$)
                % $I_{new} \leftarrow I(\mathbf{x}_{new}, e_{new}, n_{near})$\;
                % $C_{new} \leftarrow C_{n_{near}} + Cost(e_{new})$\;
                % $n_{new} \leftarrow \{\mathbf{x}_{new}, I_{new}, C_{new}\}$\;
                % \If{not $Prune(n_{new})$}
                % {
                %     $E \xleftarrow{+} \{ (n_{near}, n_{new})\}$\;
                %     $V \xleftarrow{+} \{ n_{new}\}$\;
                %     $G \leftarrow (V,E)$\;
                %     \If{$C_{new} = B$}
                %     {
                %         $V_{closed} \xleftarrow{+} \{ n_{new}\}$\;
                %     }
                % }
            }
        }
    }
}

\Return $\mathcal{T} \leftarrow $\textsc{BestPath}$(G)$
\caption{\textsc{IA-TIGRIS} }
\label{alg:ours1}
\end{algorithm}


The nearest node $n_{nearest}$ in the graph $G$ to $x_{sample}$ is identified, and the algorithm attempts to steer from $n_{nearest}$ to $x_{sample}$ within a specified distance $\Delta$ while checking for collisions, resulting in a feasible pose $x_{feas}$ and an edge $e_{feas}$. In contrast with \cite{moon2022tigris}, the new pose and edge are immediately evaluated to be added to the graph $G$ using the \textsc{AddPoseToGraph} function, allowing the neighbor selection radius $R$ to be less than the extend distance $\Delta$.


The algorithm then searches for nearby nodes $N_{near}$ within a radius $R$ of $x_{feas}$ that are not in the closed set $V_{closed}$. For each neighbor $n_{near}$ in $N_{near}$, if $n_{near}$ is distinct from $x_{feas}$ and $n_{nearest}$, the algorithm attempts to steer from $n_{near}$ to $x_{feas}$. If the new pose $x_{new}$ is distinct from $n_{near}$, the pose and edge evaluated to be added to the graph $G$ using the \textsc{AddPoseToGraph} function. This neighborhood expansion structure is different than \cite{moon2022tigris} in that it now allows for neighboring nodes to extend toward $x_{feas}$ even if it did not add and edge connecting to $n_{nearest}$. 

After the computation time is exhausted, the algorithm extracts the best path $T$ from the graph $G$ using the \textsc{BestPath} function. This function simply returns the path with the highest information reward in the graph. 

The \textsc{AddPoseToGraph} function, outlined in Algorithm \ref{alg:ours2}, handles the addition of a new pose $x_{new}$ and edge $e_{new}$ to the graph $G$. The information $I_{new}$ for the new pose is calculated, and the cumulative cost $C_{new}$ is computed by adding the cost of the edge $e_{new}$ to the cost of $n_{parent}$. A new node $n_{new}$ is created as a tuple containing the new pose, information, and cumulative cost. The node $n_{new}$ is then checking by the \textsc{Prune} function. The pruning function checks nearby nodes within a radius to see if they contain a better solution. Because most of the nodes in the tree are not full paths, the upper bound on their rewards are compared. For more aggressive pruning, heuristics can be used for this comparison. In our case we use the heuristic of checking if a nearby node has a lower cost and higher current reward, but this heuristic can have implications on finding the optimal solution. If $n_{new}$ passes the pruning check, the edge $e_{new}$ and node $n_{new}$ are added to the sets $E$ and $V$, respectively, and graph $G$ is updated. If the cumulative cost $C_{new}$ equals the budget $B$, the new node $n_{new}$ is added to the closed set $V_{closed}$.

% Thereby, the TIGRIS replan loop effectively plans a path by continuously sampling, steering, and adding new feasible poses to a graph while respecting the budget and planning time constraints. The \textsc{AddPoseToGraph} function ensures that the graph is incrementally built and maintained, facilitating the extraction of an optimal path once the planning phase is complete. This structured approach allows for informed and efficient path planning in complex and dynamic environments. 




% ####################################################################

% The algorithm comprises a set of replan loops, while each loop trying to expand the tree by keeping sampling and verifying nodes in available state space within computation time. With the basis of \cite{moon2022tigris}, the replan loop now have a procedure of adding the first extended configuration in the graph, By doing so, the algorithm is now more efficient since \todo{advantages?}

% \BM{Algorithm: Need to make this look better. Make smaller, especially comments. Might remove the vertical lines or just make lighter in color.}

\begin{algorithm}[t]
\SetInd{0.4em}{0.8em}
% \setstretch{1}
\DontPrintSemicolon
\small

\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwFunction{InformedSample}{InformedSample}

\Input{$\mathbf{x}_{new}$: New pose \\
       $e_{new}$: Edge from $n_{parent}$ to $\mathbf{x}_{new}$ \\
       $n_{parent}$: Parent node in graph \\
       $\Delta$: Extend distance for tree building \\
       $R$: Radius for neighbor selection \\
       $B$: Budget}

% \Output{No output value, adding node to graph}

$I_{new} \leftarrow I(\mathbf{x}_{new}, e_{new}, n_{parent})$\;
$C_{new} \leftarrow C_{n_{parent}} + Cost(e_{new})$\;
$n_{new} \leftarrow \{\mathbf{x}_{new}, I_{new}, C_{new}\}$\;
\If{not \textsc{Prune}$(n_{new})$}
{
    $E \xleftarrow{+} \{ e_{new}\}$\;
    $V \xleftarrow{+} \{ n_{new}\}$\;
    $G \leftarrow (V,E)$\;
    \If{$C_{new} = B$}
    {
        $V_{closed} \xleftarrow{+} \{ n_{new}\}$\;
    }
}

\caption{\textsc{AddPoseToGraph}}
\label{alg:ours2}
\end{algorithm}

% \BM{This algorithm has not been updated. Need to make updates and talk about what we changed and why.}

% The first input to the planner is the starting configuration of the robot $\mathbf{x}_{start}$ which must be within $X_{free}$. The other parameters and constraints includes the budget $B$, time allowed for planning $T$, distance of extension in tree building $\Delta$, and radius of neighbor selection $R$.

% We start by calculating the information reward at the starting configuration $\mathbf{x}_{start}$ using the information reward function $I(\cdot)$ and initializes the cost $C$ to zero. (Alg.1 Line 1). Then it create a start node $n_start$ with starting configuration, starting information reward and zero cost. This start node is then added to vertices set $V$, which initialize the graph $G$ with an empty initialized edge set $E$. (Alg.1 Line 2-3)

% Within the allowed computation time $T$, the main loop body (Alg.1 Line 4-24) will keep being executed, making the tree continue to be expanded. Such a loop starts with sampling a new configuration $\mathbf{x}_{sample}$ in sampling space $\mathcal{X}$ (Alg.1 Line 5). \todo{Description of sample? Like tigris}. 

% The nearest node $n_{nearest}$ is found in open set $V \ V_{closed}$ using the $Nearest(\cdot)$ function (Alg.1 Line 6). Such a nearest neighbor search can be achieved using k-d tree or R-tree structures.

% By extending from the nearest node $n_{nearest}$, the algorithm then finds a feasible configuration $\mathbf{x}_{feas}$ using a $Steer(\cdot)$ function, which extend within $X_{free}$ until collision or reaching the maximum extension cost (distance) $\Delta$. Together with $\mathbf{x}_{feas}$, the steering function returns $e_{feas}$ that represents the edge extended. (Alg.1 Line 7)

% Such a feasible configuration with edge, or a pose \todo{config or pose?}, is then formed into a node and added to the graph (Alg.1 Line 8, Alg.2). 

% Adding a configuration $x_{add}$ into graph $G$ is a simple process described in algorithm 2, first the information reward of the pose is calculated using $I(\cdot)$, then the total cost of the pose is calculated as the sum of the cost of the neighbor $n_{near}$, and the cost of the steering edge $e_{add}$ (Alg.2 Line 1-2). With the configuration, the information reward and the cost, a new node is created (Alg.2 Line 3) and the algorithm will check if it should be pruned (Alg.2 Line 5). If the node is not going to be pruned, it will be added into the graph, with a new edge connecting the near node
% $n_{near}$ and the new node to add $n_{add}$, and a new vertex that is the newly created $n_{add}$ (Alg.2 Line 6-8). If the cost of this new node reaches the budget $B$, then it should not be extended later, thus it will be added to the closed vertex set $V_{closed}$. \todo{should it be put somewhere else so that algo 1 is not interrupted?}

% In addition to this feasible configuration $\mathbf{x}_{feas}$ itself, the algorithm also finds its neighbor nodes within a radius $R$. The open nodes within the range are selected and formed a neighbor set $N_{near}$ (Alg.1 Line 9-10).\todo{still don't quite get why feas is not nearest in line 9}. For each node in the neighbor set, if it's not redundant, i.e. not the same as $n_{nearest}$ or the configuration is not overlapping to $x_{feas}$, then the algorithm will extend from it towards $x_{feas}$ using the steering function $Steer(\cdot)$ to get a new set of pose $x_{new}$ and corresponding edge $e_{new}$ (Alg.1 Line 11-13). If this new pose doesn't overlap with the neighbor that it start from, it will also be tried adding to the graph (Alg.1 Line 14-15). \todo{pose or configuration? tigris use configuration}

% Such a loop is terminated when the time reaches the limit $T$, and the algorithm will return the path with best information gain within G (Alg.1 Line 21).

% Can also mention continuous vs discrete, how can do both. Why one over the other. Benefits of discrete



\subsection{Adaptive Replanning}\label{sec:replanning}

For environments where the world is static and measurements are deterministic, global plans could be static and created once before flight. Having a single, static plan would also work well in situations when onboard computation is restricted, there is high confidence the global path is optimal, the executed path will be perfectly followed, and the predicted information gain will be close to the actual information gain. However, there are many cases where the plan would need to be adjusted due to disturbances and modified due to new information. When disturbances cause deviations from an expected observation, the path can be replanned during path execution to re-observe locations and adjust the path according to the current information map. 

The adaptability of \PlannerNameSpaced is due to its ability to continuously replan the global trajectory online. The updated paths take into account the current world belief updated by all observations. This brings robustness, as executed paths may not follow the planned path due to disturbances and lead to differences in the planned versus expected observations. 

The Algorithm \ref{alg:ours1} described above is a planning procedure that generates a path from the starting configuration. To replan adaptively during flight, a starting configuration is chosen that is time $T$ in the future along the previously-planned path. Similarly, the budget $B$ is updated based on remaining flight time based on the current battery levels or mission constraints. The new path is merged with the previous path, keeping any portion of the previous path before the starting pose of the new path. This process is visualized in Fig.~\ref{fig:planner_replan}.
% \BM{maybe we can have a figure here, like select starting point, chop after, plan, attach, chop before, should we have an algorithm for it?}
% \textsc{UpdateGraph}
% \BM{Should detail tree recycling. Update values and cost. Where is it in the algorithm 1 and 2. Should be clear the difference. Oh, this is in next section. But still same principle.}

\subsection{Incremental Planning}\label{sec:incr-adap-plan}
%How this works. Tree recycling. Why important detail.



When replanning online, discarding the previous planning graph wastes valuable information and forces the planner to start from scratch. By using an incremental approach, the planner can reuse the previous planning efforts, improving efficiency and enabling the next global plan to refine the earlier one. This often results in higher-quality paths.
Even when the budget and belief map remain unchanged between replans, many scenarios---such as large budgets or small areas relative to the budget---can benefit from incremental refinement. In challenging planning situations, this approach allows the planner to iteratively expand the search tree, continuing to explore and evaluate potential paths.

% with authors goes here
\begin{figure}[t]
\centering
\includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/3_/replan_loop.pdf}
\caption{Visualization of the online replanning procedure. The future robot state is used as the starting point for the next plan. The newly-planned path is merged with the previous path.}
\label{fig:planner_replan}
\end{figure}

To enable incremental planning, our method recycles as much of the previously-built graph as possible, rather than create a new graph whenever a plan is requested. Even when the belief map and budget have changed, recycling the previous graph offers significant benefits. The previous graph has already been validated for collisions and adheres to the vehicle's motion model, allowing the planner to build upon prior work and save computational resources while maintaining accuracy.

To reuse the previous tree, Algorithm \ref{alg:ours1} first checks if the vertex set $V$ is empty. If it is not empty, then rather than initializing the graph, it calls the function \textsc{UpdateGraph}. This function first finds where the starting point for the next plan lies within the previous plan. When a matching node is found in the previous plan, all nodes and edges before that node are pruned away. The remaining portions of the graph are checked for the budget constraint and the information gain up to each node are updated based on the current information map. This is done by efficiently traversing the tree recursively from the starting node working down to all leaf nodes. The planner then uses the updated graph to plan as normal. A high-level overview of this process is shown in Fig.~\ref{fig:algorithm-breakdown}. 

% No Authors, it goes here
% \begin{figure}[t]
% \centering
% \includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/3_/replan_loop.pdf}
% \caption{Visualization of the online replanning procedure. The future robot state is used as the starting point for the next plan. The newly-planned path is merged with the previous path.}
% \label{fig:planner_replan}
% \end{figure}

\subsection{Belief Space Representation and Node Embedding}\label{sec:beliefspace}

In informative path planning, the information within a space can be represented using various models, such as Gaussian mixture models, Gaussian process regressions, grids, or voxels. This belief space can encode different types of information. For example, it might represent the probability of an object of interest being located in specific regions of the space. The choice of belief space formulation is important, as it directly influences the planning process by guiding search efforts and behaviors. An effective representation for a specific system, application, and planner balances computational complexity, memory requirements, and the ability to perform operations on the belief model.

In this work, we adopt a grid-based representation of the belief space. Specifically, the search space is discretized into a grid of cells, where each cell $i$ is associated with a binary random variable $X_i$ indicating the presence ($X_i=1$) or absence ($X_i=0$) of an object of interest. The probability $P(X_i)$ represents our belief about an object being located in cell $i$. This grid-based representation is computationally straightforward to update with new or predicted observations using a Bayesian framework and an appropriate sensor model.



% Satrajit: Introduction about overall system setup

\subsubsection{Belief Space Node Embedding}\label{sec:embedding}
% how nodes are made, belief space formulation

The nodes in the planning tree maintained by the IA-TIGRIS algorithm store the necessary information to represent the evolving belief space. 
The information gain up to a given node is computed based on the change in entropy within the grid cells observed by the agent’s sensor footprint during traversal from the root node to the current node.

A straight-forward approach to calculating the information gain for a given node consists of traversing the tree from the root all the way to the given node. It is necessary to evaluate the entire trajectory because the information gain can be not modular and previous observations affect the information gain of future observations. 
Calculating the additional information gain contributed by a new node and edge requires knowledge of the belief state of the search map at the previous node. However, this approach of calculating information gain starting at the root node is computationally inefficient and slow.
Let $n$ be the number of cells in the search map, $m$ be the number of nodes in the tree, $k$ be the number of nodes in a trajectory, and $j$ be the upper bound of cells updated by a node in the tree. The temporal complexity of this method is $\mathcal{O}(kj)$ and the memory complexity is $\mathcal{O}(n)$.

One approach to speeding up computation is to store the entire search map within each node. In this approach, the information gain for a new node is simply the sum of the parent node’s information gain and the contribution of the new node. This method results in a temporal complexity of $\mathcal{O}(j)$ for calculating the information gain of a new node and a memory complexity of $\mathcal{O}(nm)$. While feasible for small maps, this method becomes impractical for large-scale maps relevant to the domains and applications considered in this work due to its high memory requirements.

To efficiently compute the information gain for a new node, we introduce a node embedding strategy that eliminates the need for a full traversal of the planning tree from the root to the current node. Instead of storing the complete belief state in each node, we use a sparse representation that records only belief updates. A hash map efficiently tracks modifications to belief probabilities, reducing memory complexity while preserving the ability to compute information gain incrementally. This allows for efficient calculations of the information gain for a new node while maintaining computational efficiency without sacrificing accuracy.

% The solution we found to this issue is rather than embed a full map in each node, we embed only the new modifications to the search map in each node using a hash map. This reduces the total increase in memory usage while being able to calculate information gain as a summation of the previous node and the new node. When calculating the info gain for the new edge and node, a current search grid cell value consists of a lookup in the hash maps of the previous nodes in the path. This gives a temporal complexity of $\mathcal{O}(j)$ % jk of lookups?
% to calculate information gain for a new node and a total memory complexity of $\mathcal{O}(jm+n)$. As long $j<n$, then the memory complexity is less than embedding the full search map in each node, and in our cases, $j \ll n$. 

The temporal complexity of calculating the information gain for a new node is now $\mathcal{O}(j)$, and the overall memory complexity of this belief space representation is only $\mathcal{O}(jm + n)$. As long as $j \ll n$, this approach is much more memory-efficient than the naive approach of storing the complete belief state in each node. Each node in our tree now contains the cumulative information gain along the path, current state of the agent, path cost, and edge path.

While implementing this embedded method, we observed the significant impact on computation time from selecting efficient keys for the hashmap. Using strings as keys resulted in substantial overhead due to frequent string-to-integer conversions. Even transitioning from a two-integer key to a single-integer key yielded a notable improvement in performance and is the approach we use in our implementation through a simple index-to-key conversion. Given that cell value lookup operation occurs frequently, optimizing its efficiency was highly impactful for accelerating the expansion rate of the search tree.


\begin{figure*}[t]
\centering
% \includegraphics[width=2.5in]{fig1}
\includegraphics[trim={0cm 5cm 0cm 0cm},clip,width=.99\textwidth]{figs/3_/tree_expansionv2.png}
\caption{An illustration of the incremental planning and refinement process in \PlannerName. (a) The global planner first generates long-horizon paths to maximize predicted information gain. (b) At the start of each new planning cycle, infeasible portions of the tree, based on the executed trajectory and the current plan, are pruned. The remaining tree is then updated by recomputing information gain and cost based on the latest belief state and available budget, which is significantly more efficient than rebuilding the tree from scratch. (c) The tree is further expanded and refined for the duration of the planning cycle, after which the process repeats.
% \BM{Add subfigure labels, show the path is even better than before}
% See Fig. 2 of this for style reference \href{https://www.ri.cmu.edu/pub_files/2016/5/main.pdf}{this}, \href{https://www.canva.com/design/DAGCIYkn6xw/emw6KJjG6ksbT6rodi-Xbw/edit}{canva draft link} \href{https://drive.google.com/file/d/1e6XsvAEu7T4GnAuUv8OW0zYgWUx_Z2tO/view?usp=sharing}{drawio figure link}
}
\label{fig:algorithm-breakdown}
\end{figure*}


% The key advantages of this belief space node embedding strategy are:

% \begin{enumerate}
%     \item Efficient computation of information gain: By maintaining a sparse representation of belief updates, we can calculate the information gain for a new node in $\mathcal{O}(j)$ time, avoiding the need for a costly full tree traversal.
%     \item Reduced memory footprint: The memory complexity of $\mathcal{O}(jm + n)$ is significantly lower than the $\mathcal{O}(nm)$ complexity of the naive approach, making it feasible to handle larger search domains.
%     \item Modularity of information gain: The information gain for a new node can be computed as the sum of the previous node's information gain and the new belief updates, without the need to recompute the entire path's information gain.
% \end{enumerate}


By incorporating this belief space node embedding strategy, our planning framework efficiently maintains an accurate representation of the evolving belief space within the search tree. This enables the planner to greatly reduce computational overhead and create higher quality paths. 

\subsection{Sensor Modeling}\label{sec:sensor-model}

The updates to our belief map are dependent on the sensor model, which captures the performance characteristics of the perception system employed. There are many environmental factors that affect the performance of a sensor, such as visibility and range, which can be incorporated into this model. 

For our approach, we formulate the sensor model as a piecewise function and is implemented as a lookup table. For a given range $r$ and binary measurement $Z$, this table maps the range of the object from the sensor to the corresponding true positive rate (TPR) $P\left(Z|X,r\right)$ and true negative rate (TNR) $P\left(\overline{Z}|\overline{X},r \right)$ of an object detection model. A typical range-based detection model will have better performance at close ranges and then tapers off as the distance increases. 

% The sensor model is formulated as a piecewise function, which simulates the performance of an electro-optical sensor. Let the binary variable $Z$ represent the event of a positive measurement from the sensor. We model both the TPR, $P(Z|X)$, and the FPR, $P(Z|\overline{X})$, using the piecewise function in (\ref{equ:sensor}).

% \begin{equation}
% P(Z|X,r) = \begin{cases}
% \frac{1}{a + e^{b(r-c)}}, & \text{if } r \leq \beta \\
% 0.5, & \text{if } r > \beta
% \end{cases}
% \label{equ:sensor}
% \end{equation}

% where $r$ is the range of the target from the sensor, and $a$, $b$, and $\beta$ are parameters that are fitted to the performance characteristics of the perception pipeline. The false positive rate, $P(Z|\overline{X})$, is simply the complement of the above expression.

% The parameter $\beta$ represents the range at which the sensor's measurements do not change the belief space. This sensor model allows us to realistically simulate the performance characteristics of the perception system and its impact on the belief map updates.

By maintaining a lookup table of these sensor models, we can easily incorporate different sensor modalities and their corresponding capabilities into the planning framework. The sensor model can be derived from actual test data and easily integrated into the framework rather than having to fit a function to the results.
% This modular sensor model approach enables the flexibility to adapt the planning strategy to various sensing environments and perception pipelines.

The sensor model for belief map updates is a crucial component of the overall planning framework, as it directly affects the information gain computed for each node in the IA-TIGRIS tree. Accurately modeling the sensor’s performance characteristics enables the planner to make informed decisions, resulting in more effective exploration and search strategies that align with actual sensor capabilities. Conversely, inaccuracies in the sensor model can lead to overconfidence in observed areas or unnecessary revisits to regions that have already been sufficiently searched.

\subsection{Information Reward Models}\label{sec:reward-models}


The information reward function $I(\cdot)$ used in our framework is not constrained to being modular and could be time-varying or even submodular, depending on the specific requirements of the application. In this work, we have implemented an objective of reducing the overall entropy of the belief space. As defined in Section \ref{sec:beliefspace}, our belief space represents the probability of the presence of an object of interest in a cell. The reward for one cell $I(X_i)$ is calculated by finding the difference between the initial entropy and the final entropy after updating the belief with the measurements and sensor model defined in Section \ref{sec:sensor-model}.


% In this work, we have implemented the \PlannerNameSpaced framework \cite{moon2022tigris} with the objective of reducing the overall entropy of the belief space, while also biasing the search towards areas with higher belief probabilities. 
% Our data gathering is performed using a fixed-wing UAV equipped with a static forward-facing camera. 


% The formulation of entropy reduction is not limited to the specific task of search and rescue, as the value within a cell could represent any other metric for which we want to reduce uncertainty specific to one's use-case. 

% \BM{First define reward R, and Shannon entropy. Change in previous and final state. Flip equation 2 and 3. }

The Shannon entropy of a cell is calculated by 
\begin{equation}
    H(X_i) = -P(X_i)\log P(X_i)-P(\overline{X}_i) \log P(\overline{X}_i)
    \label{shannon}
\end{equation}
where $X_i$ represents the event that the cell is occupied and $\overline{X}_i$ denotes its complement, the event that the cell is free.

The entropy reduction due to a single positive measurement would be calculated as
\begin{equation*} %\label{eq:delta_entropy}
\Delta H(X_i|Z) = H(X_i) - H(X_i|Z)
\end{equation*}
where $H(X_i)$ is given by (\ref{shannon}), and $H(X_i|Z)$ is the entropy conditioned on the sensor measurement $Z$. 

The final information gain for a trajectory $I(\mathcal{T})$ would be
\begin{equation*}
    I(\mathcal{T}) = \sum_i^I H(X_{i,init}) - H(X_{i,final})
\end{equation*}
where $X_{i,init}$ is the initial belief state of cell $i$, and $X_{i,final}$ is the final belief state.  
% To prioritize reducing entropy in areas with higher belief probabilities, we introduce weights $R_p$ and $R_n$ for increasing and decreasing probabilities, respectively.



\subsubsection{Information Reward Estimation}\label{info-reward}

\begin{figure}[t]
\centering
\includegraphics[trim={5.5cm 0cm 0cm 3.5cm},clip,width=0.99\columnwidth]{figs/3_/Fig4v3.pdf}
\caption{An example of how the grid cells rewards are estimated given a planned trajectory. The cells that are closer to the center of the trajectory have a smaller minimum distance than cells farther from the center of the trajectory, leading to a larger change in entropy for the closer cells. 
%\BM{Make blue lighter. Mix it up. Text slightly larger. Needs to look more different. Add planned path. Maybe remove first UAV. Maybe no gray square. Changle colors from TIGRIS.}
}
\label{fig:sensor-model}
\end{figure}
When adding a new node in the planning tree, the approach for estimating the information gain may vary depending on the modularity and temporal dependence of the information function. In the case of a modular and temporally independent function, the reward for a new node can be simply appended to the cumulative reward along the path. Alternatively, if the information function exhibits non-modularity or temporal dependence, the entire trajectory up to the current node may need to be re-evaluated to determine the appropriate reward. 
Because we do not yet know the future measurements for each cell, the information gain for the planned path must be estimated. 

One option would be to compute the expectation of the information gain given all possible future measurements. However, this can be computational expensive and infeasible. We avoid this by following the approach of \cite{hollinger_long-horizon_2015,moon2022tigris} and use an optimistic approximation of the expected reward. Specifically, we assume a positive measurement $Z$ if $P(X) \geq 0.5$, and a negative measurement otherwise. This leads to the following cell reward function for a single measurement:
\begin{equation}
    I(X_i) = \begin{cases} \Delta H\left(X_i|Z\right) & P\left(X_i\right) \geq 0.5 \\ \Delta H\left(X_i|\overline{Z}\right) & P\left(X_i\right) < 0.5 \end{cases}
    \label{equ:reward}
\end{equation}
% As discussed in Section \ref{sec:embedding}, we maintain, we maintain a sparse representation of the belief space in each node to efficiently compute the information reward along the planned path. Instead of storing the complete belief state in each node, we only keep track of the modifications to the belief probabilities. This allows us to quickly look up the previous belief values for the affected cells and update them based on the current sensor measurements, leading to a temporal complexity of $\mathcal{O}(j)$, where $j$ is the upper bound on the number of grid cells affected by the agent's sensor at a given node.
% To calculate the information reward along a path, the sensor footprint is projected onto the surface plane to identify all cells within the footprint. The reward for each cell is then computed using (\ref{equ:reward}) and summed. 
% This step becomes more complex when including graph edges in the reward calculation.

To compute the information reward along a path, the sensor footprint is projected onto the surface plane to determine all the cells it contains. The reward for each cell is calculated using \eqref{equ:reward} and then aggregated. This process becomes more intricate when incorporating graph edges into the reward computation. Discretizing the trajectory between nodes into individual views creates imperfect overlaps due to the non-square sensor footprint, potentially omitting some grid cells from the edge reward calculation. Mitigating this issue requires significant overlap between sensor footprints, which, in turn, greatly increases the computation time for updating grid cell beliefs.
Rather than selecting a discretization resolution for the trajectory, we compute a conservative approximation of the edge reward by updating all cells within the view of edge trajectory, using the closest viewing distance along the path for each cell.
This approach ensures that all cells along the edge are accounted for, with each belief value updated using a measurement taken when the sensor is closest to that cell. A visualization of this reward estimation is shown in Fig. \ref{fig:sensor-model}.

% \BM{Talk about edge reward a bit more in depth. Refer to the figure. Should we have the math? Can I simplify the math? Or just describe what is happening and remove math from figure.}

\subsubsection{Priority-dependent Rewards}\label{sec:priority}
In the case where certain areas of an environment or spatial information are more important than others, we introduced the ability to weight the information reward by a priority scalar, $p_i$, representing the priority of cell $i$. This approach provides finer control over the UAV behavior by allowing the entropy of different cells to be weighted unequally. Assigning a higher priority to one prior over another results in paths that favor observations of the higher-weighted prior. For example, this is useful when one type of object is more important than another, and the prior spatial distributions are concentrated in distinct areas of the search space (as demonstrated in Section \ref{sec:priority-time-eval}).

\subsubsection{Time-dependent Rewards}
The IPP formulation maximizes total information gain over the entire mission without explicitly relating the reward to the timing of observations---rewards are time-independent. However, in some applications, operators may prefer prioritizing areas of high information gain earlier in the trajectory rather than later.

% if include authors, this figure goes here
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/4_Evaluation/simExample_v2.png}
\caption{Example results for IA-TIGRIS after a replanning step in our simulation environment. Pink regions represent regions of high entropy while blue regions are areas of low entropy. The red projection shows the camera frustum while the green line represents the planned path.}
\label{fig:simple_sim}
\end{figure}

We introduce the ability to create time-dependent reward that decreases the reward value for all cells over the mission duration, resulting in plans that prioritize visits to areas of high information gain early in the plan. This works especially well when paired with priority-dependent rewards, as introduced in Section \ref{sec:priority}. The UAV will be biased toward visiting the higher priority priors early in the plan, and the operator can even roughly dictate the order to visit areas through differing priority weights and using the time-dependent reward decay function.

In our use cases, we implement a simple reward decay function
\begin{equation*}
    \Gamma(t) = \begin{cases}  \gamma & t \geq \gamma_t \\ 
                               \beta t + 1 & t < \gamma_t 
                               \end{cases}
\end{equation*}
where $\Gamma(t) \in [\gamma, 1]$ and $\gamma$ is the floor value of the decay function. $\beta$ is the decay rate and $\gamma_t = (\gamma-1)/\beta$ is the time where the function output becomes the floor value $\gamma$. This gives a linear decay until the floor value. 

Combining our decay function $\Gamma(t)$ and cell priority values $p_i$ creates a priority and time dependent reward function 
\begin{equation*}
    I(X_i,t) = \begin{cases}  p_i\Gamma(t)\Delta H\left(X_i|Z_i\right) & P(X_i) \geq 0.5 \\ p_i\Gamma(t)\Delta H\left(X_i|\overline{Z}_i\right) & P(X_i) < 0.5 \end{cases}
    % \label{equ:reward2}
\end{equation*}

% Analysis and evaluation of this reward  shown in Section \ref{sec:priority-time-eval}. 

