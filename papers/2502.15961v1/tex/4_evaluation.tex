% \section{Experimental Evaluation}\label{sec:evaluation}
\section{Simulation Evaluation \& Results}\label{sec:results}


To rigorously evaluate the performance and benefits of \PlannerName, we first conduct tests in a simulation environment. These experiments include ablations to assess the impact of key design decisions, as well as Monte Carlo evaluations comparing \PlannerNameSpaced against baseline methods.

% if no authors, figure goes here
% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{figs/4_Evaluation/simExample_v2.png}
% \caption{Example results for IA-TIGRIS after a replanning step in our simulation environment. Pink regions represent regions of high entropy while blue regions are areas of low entropy. The red projection shows the camera frustum while the green line represents the planned path.}
% \label{fig:simple_sim}
% \end{figure}

\begin{figure*}[t]
\centering
\begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[trim={0.0cm 2.5cm 0.0cm 3.5cm},clip,width=.99\textwidth]{figs/3_/SimRunFig_v6.pdf}
    };
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \fill[OrangeRed] (0.21, 0.218) circle (1.5pt); 
        \fill[OrangeRed] (0.46, 0.218) circle (1.5pt); 
        \fill[OrangeRed] (0.71, 0.218) circle (1.5pt); 
        \fill[OrangeRed] (0.96, 0.218) circle (1.5pt); 

        % Overlay the drone image
        % \node[anchor=center, rotate=66] at (0.23, 0.6938) {
        %     \includegraphics[width=12pt]{figs/3_/drone.png}
        % };
        % \node[anchor=center, rotate=105] at (0.409, 0.661) {
        %     \includegraphics[width=12pt]{figs/3_/drone.png}
        % };
        % \node[anchor=center, rotate=185] at (0.028, 0.275) {
        %     \includegraphics[width=12pt]{figs/3_/drone.png}
        % };
        % \node[anchor=center, rotate=170] at (0.3536, 0.3) {
        %     \includegraphics[width=12pt]{figs/3_/drone.png}
        % };

        % \fill[Cyan] (0.23, 0.6938) circle (1.5pt); 
        % \fill[Lavender] (0.407, 0.658) circle (1.5pt); 
        % \fill[BlueGreen] (0.028, 0.275) circle (1.5pt); 
        % \fill[SkyBlue] (0.354, 0.3) circle (1.5pt); 
        \node[diamond, fill=orange, scale=0.30] at (0.202, 0.458) {};
        \node[diamond, fill=orange, scale=0.30] at (0.355, 0.398) {};
        \node[diamond, fill=orange, scale=0.30] at (0.5145, 0.581) {};
        \node[diamond, fill=orange, scale=0.30] at (0.801, 0.62) {};
    \end{scope}
\end{tikzpicture}
\caption{IA-TIGRIS continuously replanning and refining the global path over the duration of the simulation. These snapshots show how the path is refined online to maximize information gain. The search tree is also visualized and shows how the plans focus on the areas of high information gain and take into account the remaining budget.  
}
% Link to figure in google docs: https://docs.google.com/presentation/d/1RCBTuJe0R2mcWB4_lazZliW5vhzeFFca9GFHWcudci0/edit?usp=sharing
\label{fig:example_run}
\end{figure*}

\subsection{Simulation Framework}\label{sec:sim-framework}

We use a simplified simulator, shown in Fig.~\ref{fig:simple_sim}, to enable rapid development and validation of our approach. The simulation employs a constant-velocity motion model for the fixed-wing UAV. To account for the effects of banking on observations, we compute the orientation change between consecutive waypoints. If this change exceeds a threshold, the UAV is considered to be banking, and the belief space is not updated during this period. This reflects the fact that the camera is primarily pointed toward the sky during banking and is also affected by increased motion blur.

When the UAV reaches a commanded waypoint within a specified Euclidean distance threshold, the next waypoint in the list becomes the new commanded waypoint. During replanning, a newly generated path is merged with the existing path, incorporating the updated trajectory starting from the beginning of the new path. A simple proportional (P) controller is used to regulate heading and altitude, guiding the UAV between waypoints.

The UAV commanded speed is $25$ m/s, with a planning time of $10$ seconds and a the flight altitude of $50$ m. The turning radius is $100$ m, and the onboard camera is pitched $30$ degrees from horizontal with a $36.9$-degree field of view. The belief space grid cells are $30$ m, and the total budget for flight distance is $15,000$ m. To accelerate testing, the simulation runs at twice the real-time speed, with results subsequently rescaled to match real-time conditions.

Fig.~\ref{fig:example_run} presents four snapshots from a simulation. The planned path is shown in green, the executed path in yellow, the robot's current position as an orange diamond, and the starting position as a red circle. Early in the simulation, the planned trajectory makes two passes over the larger clusters of high-entropy regions, which represent areas of high uncertainty. The search tree explores a wide range of paths across these regions to maximize information gain, refining its evaluation over time. As the planning cycle progresses, the search tree is iteratively updated through tree recycling, which improves computational efficiency by retaining and modifying previous solutions rather than replanning from scratch. At $t=180$, the plan adapts by doubling back to the middle-left region, due to finding a more rewarding path. Toward the end of the simulation, the effect of the budget constraint becomes apparent, as the search tree no longer extends into the upper-right corner of the search area.

\subsection{Metrics}

The primary metric for evaluating the success of our plan is the information gain along the executed path, quantified as the change in total entropy of the belief space from the initial to the final state. To facilitate interpretation and comparison across tests, we report this as the percent reduction in entropy. Initially, the entropy reduction is zero, as no observations have been made. As the robot gathers information, the entropy decreases, reflecting an improved understanding of the environment. This reduction continues to accumulate throughout the simulation as new observations refine the information map.

% \BM{Reduction in entropy!! Percent in our case. Why. Easier to scale. Also to combine metrics across maps. Arbitrary numbers. }

\subsection{IA-TIGRIS Ablations}

We conduct ablation experiments to evaluate the impact of incremental and adaptive planning, belief space node embeddings, priority and time dependent rewards, and planner parameters.

\subsubsection{Incremental and Adaptive Planning Evaluation} 
%The results for impact on planning quality. How tree recycling helps. Incremental planning vs not. Run some full tests to see how much it helps. Maybe have short planning times too?

% \bradynote{Going to move full tests results to the next section. Focus only on evaluating the incremental and adaptive elements by themselves. Or not? Just need to keep things organized. Could always move around later if needed. Dont' worry too much. }

We first evaluate the efficiency of recycling a search tree compared to building a new one for each planning cycle. For IA-TIGRIS to be both adaptive and incremental, reusing and recycling previous planning efforts must be computationally efficient. As explained in Section \ref{sec:incr-adap-plan}, each planning begins by pruning irrelevant trajectories, checking budget constraints for remaining nodes, and updating information gain based on the current belief map. If these updates consume too much planning time, there will be insufficient time left to refine and extend the trajectory.

Our implementation of IA-TIGRIS demonstrates that recycling a previously built tree is significantly more efficient than constructing a new tree from scratch. For example, a tree that takes $10$ seconds to build requires less than $0.2$ seconds to update under our planner settings and test environment. This efficiency gain allows the entire tree to be recycled and extended with the remaining planning time, even if none of the previous plan was executed. By avoiding the computational overhead of trajectory generation, neighborhood search, and collision checking, tree recycling results in faster updates and enables the planner to generate improved paths within the same time constraints.

To quantify the impact of tree recycling and replanning, we conducted 50 tests in the simulation framework described in Section \ref{sec:sim-framework}. Without replanning, the average reduction in entropy was 20.4\%. Introducing replanning without tree recycling improved this to 23.1\%, a 13.2\% increase in performance. Even in the absence of environmental disturbances, replanning improves path quality by addressing mismatches between estimated and actual information gain, particularly as the planning horizon shortens due to decreasing budget. Incorporating incremental planning with tree recycling further increased the reduction in entropy to 25.4\%, a 24.5\% improvement compared to replanning without tree recycling. This highlights \PlannerName's to refine and improve the previous global plan.

% with authors goes here
\begin{figure}[t]
    \centering
    \includegraphics[trim={0.4cm 0.4cm 0.4cm 0.3cm},clip,width=0.95\columnwidth]{figs/4_Evaluation/Embedding.pdf}
    \caption{Tree size vs computation time for 100 trials.}
    \label{fig:node-embedding}
\end{figure}

As expected, the benefits of replanning and refinement become more pronounced when planning time is reduced. When the allowed planning time is reduced to 3 seconds, the performance difference between no replanning and replanning with tree recycling increases to 25.8\%. This gap would widen further for even shorter planning times, where initial plans tend to be of lower quality. 
However, as planning time decreases, a larger portion of the time is allocated to recycling previous plans rather than expanding the tree, leaving less time for refinement. This introduces a tradeoff between path quality and responsiveness; more frequent replanning improves adaptability to new information, while longer planning cycles allow for more extensive trajectory refinement.



% \BM{Add graph showing percent reduction entropy vs time for with and without replanning and with and without incremental planning. Or just do end values in a table? It should really be a graph of best path planned at each step in time, but it decreases, so I can't graph the first one very well. Also expected vs actual to show another reason to replan?}

% \BM{Be clear that in our case where there aren't any disturbances or anything, just be replanning every time you get an improvement. But by using the previous plan and getting to refine, even better. 10 second results. Could say the gap increases with shorter plannign times. But also does the our solution go down a bit. Intersting. Why better with longer times? Less time rebuilding trees and such, good first decision)}


% Local tests with info on turns (not useful)
% No Replan 0.5 sec (local): 27.807986443043404 +/- 3.774264404693428
% No Recycle 0.5 sec (local): 30.13107835882787 +/- 4.158708579069883

% No Replan 3 sec: 19.727573923428636 +/- 2.879674754326973
% No Recycle 3 sec: 22.322776538921072 +/- 3.216914572654779 (13.2% improvement)
% Both 3 sec: 24.808848375569756 +/- 3.592729396248471 (25.8% to no replan, 11.1%)

% No Replan 10 sec: 21.589330326671078 +/- 1.9158585707811935
% No Recycle 10 sec: 23.37218539464094 +/- 2.0553952380004423 (8.2%)
% Both 10 sec: 25.505970884417625 +/- 2.194891127643035 (18.1% to no replan, 9.1%)
% No Replan 10 sec 50 runs: 20.388555320832754 +/- 2.7661165068051528
% No Recycle 10 sec 10 50 runs: 23.05968555239691 +/- 3.2849032834789513 (22.1%) wrong?
% Both 10 sec 50 runs: 25.421206808641884 +/- 3.7721201517304723 (24.7%, 10.2%)

\subsubsection{Belief Space Node Embedding Analysis}
%Before and after results. Impact. If too small, just put in above section.

% Analyze the amount of references and worst case scenario. Guarantees on improvements or expected improvement. 

% (500+50000)/8or10
% 835301 -> 31619648 for 10 (31619648-835301)/31619648 0.97358284949%
% 264916 -> 44091466 for 8 (44091466-264916)/44091466 0.99399167176%
% (43990566-1922797)/43990566 .95629069651%

We evaluated the impact of belief space embedding on the efficiency of our planning. Calculating the information gain for a trajectory can take up a disproportionate amount of the planning time if not implemented well. We found that in our typical testing environments, using our belief space embedding in the tree nodes reduced the time to compute the information gain by around $80$-$95\%$. This increase in speed only resulted in an additional ${\sim}20$ MB of memory usage for the search tree. 

% with no authors, goes here
% \begin{figure}[t]
%     \centering
%     \includegraphics[trim={0.4cm 0.4cm 0.4cm 0.3cm},clip,width=0.95\columnwidth]{figs/4_Evaluation/Embedding.pdf}
%     \caption{Tree size vs computation time for 100 trials.}
%     \label{fig:node-embedding}
% \end{figure}



Fig.~\ref{fig:node-embedding} shows the tree size of the first plan vs time for 100 trials on random simulation environments. Due to the reduction in time to add each new node to the planning tree, the tree size is much larger at any point during planning. 

% with authors goes here
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.99\columnwidth}
        \centering
        \begin{tikzpicture}
            \node[anchor=south west, inner sep=0] (image) at (0,0) {
                \includegraphics[trim={0cm 0.0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/default_v2.png}
            };
            \begin{scope}[x={(image.south east)},y={(image.north west)}]
                \fill[OrangeRed] (0.656, 0.85) circle (2pt); % Adjust coordinates and size as needed
            \end{scope}
        \end{tikzpicture}
        \caption{}
        % \vspace{.1cm}
        \label{fig:priority_time_impacta}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.99\columnwidth}
        \centering
        \begin{tikzpicture}
            \node[anchor=south west, inner sep=0] (image) at (0,0) {
                \includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/time_v1.png}
            };
            \begin{scope}[x={(image.south east)},y={(image.north west)}]
                \fill[OrangeRed] (0.65, 0.84) circle (2pt); % Adjust coordinates and size as needed
            \end{scope}
        \end{tikzpicture}
        \caption{}
        \label{fig:priority_time_impactb}
    \end{subfigure}

    \begin{subfigure}[b]{0.99\columnwidth}
        \centering
        \begin{tikzpicture}
            \node[anchor=south west, inner sep=0] (image) at (0,0) {
                \includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/priority_v2.png}
            };
            \begin{scope}[x={(image.south east)},y={(image.north west)}]
                \fill[OrangeRed] (0.65, 0.84) circle (2pt); % Adjust coordinates and size as needed
            \end{scope}
        \end{tikzpicture}
        \caption{}
        \label{fig:priority_time_impactc}
    \end{subfigure}

    
    \caption{Examples showing how modifying the priority and time-based rewards effects behavior. The starting point is shown in red and the planned path in green. (a) The base case with equal priorities. (b) A higher priority on the far left prior causes the drone to spend more of its budget observing that space. (c) With time-based rewards, the drone visits the high priority prior as quickly as possible to maximize reward.}
    \label{fig:priority_time_impact}
\end{figure}

Our embedding also has a profound impact on the speed of recycling the search tree between plans. 
Previously the information gain at each node would be calculated individually when recycling the tree, with the nodes toward the base of the tree getting evaluated many times. 
Now, the tree can be updated recursively from the root to the leaves without evaluating the information at any node more than once. 
When tested in our typical environments, this cut down the time to rebuild a tree from 
${\sim}180.93~\upmu$s per node
to ${\sim}15.48~\upmu$s per node. 


We also evaluate the impact of belief space embedding by using Google Benchmark \cite{gtest} on an extremely large environment. For these benchmark tests, the number of nodes in the trajectory was $k = 7$, the number of grid cells in the search space was $n=39,841,344$, and the number of grid cells updated by the new node was $j=11,984$. The time to evaluate the information gain for a new node decreased from $43$ ms to $1.9$ ms, a $96\%$ improvement. This led to the size of the search trees to on average be more than double in size. 






% Size of recycled tree is 4805
% [ INFO] [1715098587.600382028]: Time to build tree: 0.0690952
% 14.4 micro
% Size of recycled tree is 7238
% [ INFO] [1715098597.862572987]: Time to build tree: 0.136417
% 18.8 micro
% Size of recycled tree is 7839
% [ INFO] [1715098608.142351270]: Time to build tree: 0.112331
% 14.3 micro
% Size of recycled tree is 8650
% [ INFO] [1715098618.656161561]: Time to build tree: 0.124367
% 14.4 micro
% Average = 15.48 micro

% Size of recycled tree is 1236
% [ INFO] [1715098655.173690359]: Time to build tree: 0.193387
% 156.5 micro
% Size of recycled tree is 3408
% [ INFO] [1715098665.895240814]: Time to build tree: 0.620488
% 182.1 micro
% Size of recycled tree is 4661
% [ INFO] [1715098676.470018847]: Time to build tree: 0.893326
% 191.7 micro
% Size of recycled tree is 5354
% [ INFO] [1715098688.415234952]: Time to build tree: 1.03527
% 193.4 micro
% Average 180.93 micro
% 91.4 \% decrease. 






\subsubsection{Priority and Time Dependent Rewards Evaluation}\label{sec:priority-time-eval}

% without authors goes here
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.99\columnwidth}
%         \centering
%         \begin{tikzpicture}
%             \node[anchor=south west, inner sep=0] (image) at (0,0) {
%                 \includegraphics[trim={0cm 0.0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/default_v2.png}
%             };
%             \begin{scope}[x={(image.south east)},y={(image.north west)}]
%                 \fill[OrangeRed] (0.656, 0.85) circle (2pt); % Adjust coordinates and size as needed
%             \end{scope}
%         \end{tikzpicture}
%         \caption{}
%         % \vspace{.1cm}
%         \label{fig:priority_time_impacta}
%     \end{subfigure}
    
%     \begin{subfigure}[b]{0.99\columnwidth}
%         \centering
%         \begin{tikzpicture}
%             \node[anchor=south west, inner sep=0] (image) at (0,0) {
%                 \includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/time_v1.png}
%             };
%             \begin{scope}[x={(image.south east)},y={(image.north west)}]
%                 \fill[OrangeRed] (0.65, 0.84) circle (2pt); % Adjust coordinates and size as needed
%             \end{scope}
%         \end{tikzpicture}
%         \caption{}
%         \label{fig:priority_time_impactb}
%     \end{subfigure}

%     \begin{subfigure}[b]{0.99\columnwidth}
%         \centering
%         \begin{tikzpicture}
%             \node[anchor=south west, inner sep=0] (image) at (0,0) {
%                 \includegraphics[trim={0cm 0cm 0cm 0cm},clip,width=\columnwidth]{figs/4_Evaluation/priority_v2.png}
%             };
%             \begin{scope}[x={(image.south east)},y={(image.north west)}]
%                 \fill[OrangeRed] (0.65, 0.84) circle (2pt); % Adjust coordinates and size as needed
%             \end{scope}
%         \end{tikzpicture}
%         \caption{}
%         \label{fig:priority_time_impactc}
%     \end{subfigure}

    
%     \caption{Examples showing how modifying the priority and time-based rewards effects behavior. The starting point is shown in red and the planned path in green. (a) The base case with equal priorities. (b) A higher priority on the far left prior causes the drone to spend more of its budget observing that space. (c) With time-based rewards, the drone visits the high priority prior as quickly as possible to maximize reward.}
%     \label{fig:priority_time_impact}
% \end{figure}

To illustrate the impact of priority-dependent and time-dependent rewards, we conduct a simulation with the results shown in Fig.~\ref{fig:priority_time_impact}. To make the effect of flight paths more clear, the camera is pitched down so that the flight path coincides more closely with what is observed by the sensor. The example scenario has three clusters of uncertainty in the search space.

Fig.~\ref{fig:priority_time_impacta} shows the base case where three areas of uncertainty have equal priority. The planned path visits the closest cluster first and then travels to the left side, viewing all three clusters. Fig.~\ref{fig:priority_time_impactb} demonstrates when the cluster on the far left is adjusted to have a higher priority. The path no longer visits the cluster on the right in favor of saving the budget to expend more effort in searching the higher-priority region.  When adding a time-based reward along with the priority, Fig.~\ref{fig:priority_time_impactc} shows how the path first visits the high-priority prior before moving on to the other prior on the left. The time-based reward causes the order of observations to affect the total reward. 



\subsubsection{Planner Global Parameter Search}
We investigate the effects of the extend distance $\Delta$, the near radius $R$ used in \textsc{Near}$(\cdot)$, and the pruning radius used in \textsc{Prune}$(\cdot)$ within \PlannerName, as outlined in Section \ref{sec:alg_overview}. To do this, we perform a global parameter search across 25 randomly generated environments. Each environment was 5 km $\times$ 5 km, with a prior belief map consisting of a random number of Gaussian distributions with varying locations, means, and standard deviations. The planning loop was given a $10$-second planning time, and the agent continued to observe the space until budget was expended. 


\begin{figure}[t]
\centering
% \includegraphics[width=2.5in]{fig1}
% \includegraphics[trim={2cm 8cm 2cm 8cm},clip,width=\columnwidth]{figs/4_Evaluation/Paramv2.pdf}
\includegraphics[trim={1cm 0.7cm 1.4cm 1cm},clip,width=\columnwidth]{figs/4_Evaluation/Paramv3.pdf}
\caption{An isosurface plot showing the results of the global parameter search. Each surface shows given parameters that result in similar average percent of entropy reduction over a set of tests.}
\label{fig:param-sweep}
\end{figure}

Using percent reduction in entropy as the performance metric, we visualize the results in an isosurface plot, shown in Fig.~\ref{fig:param-sweep}, where higher percentages indicate greater information gain. 
% This shows that for the given sampled environments, the planner tends to give higher quality paths when the extend distance is higher. There is also some effect with the near radius, with higher tending to perform better. There isn't as clear of a relationship with the pruning radius on performance, but there does tend to me a slight increase in performance as the radius increases. 
The results suggest that, for the sampled environments, planner performance improves as the extend distance and near radius are larger. If one is much smaller than the other, performance greatly reduced. However, the relationship between pruning radius and performance is less clear, though a slight performance increase is observed as the pruning radius grows.

The pruning radius primarily affects the planned path length at the start of a test. A very small pruning radius results in paths shorter than the $15$ km budget, likely because a denser tree limits tree depth. Similarly, reducing the extend distance below $500$ m leads to shorter initial plans, suggesting that both parameters play a role in determining tree expansion.

% We observe that generally that as the extend distance gets larger, the near radius has to also increase in order to keep the same performance. Letting the near radius drop below the extend radius leads to a decrease in performance. In these tests, the pruning radius can be set to a high value without too large of a drop in performance and in return will lead to longer planned paths. The performance of the CPU on a system can also affect the performance and the optimal set of parameters. 

We find that as the extend distance increases, maintaining a sufficiently large near radius is crucial for sustaining performance. If the near radius drops below the extend distance, performance declines. Meanwhile, the pruning radius can be set relatively high without significant performance loss, and doing so allows for longer planned paths. Additionally, the CPU performance of a given system may influence the optimal parameter settings, suggesting the need for adjusting parameters based on computational resources.

% \BM{Could include some example differences in the trees depending on the results of our findings above. Show the best and the worst parameters and how it affects the path. It's also interesting how the parameters change for given environment.}

The parameter with the highest performance in our testing had the combination of an extend distance and near radius that balanced expansion while still creating a dense tree, and a large pruning radius to act as a stronger heuristic for high-quality paths. For our subsequent simulation testing, we selected an extend distance of $1500$ m, near radius of $1500$ m, and a pruning radius of $600$ m. 

% Mention that specific to our system. We saw differences based on cpu. If faster cpu, did better with shorter extend distance. Slower, would use larger extend and near distances. Could show the shift in a figure. 


% We perform a hyperparameter search for both TCNs
% and LSTMs.
% For TCNs, we vary the number of filters in
% each causal convolution unit between {16, 32, 64, 128}, the
% kernel size between {2, 3, 4}, the number of residual block
% stacks between {1, 2}, and the number of layers between
% {3, 4, 5, 6}. We use the Adam optimizer with a learning rate
% of 0.002 and make use of exponential dilation [13]. We
% selected the top two models LSTM and TCN models based
% on their performance on the validation set and evaluate them
% on the test set.

% Could have done planning time. Held constant across all tests.

% Interesting observation about pruning. Need to rerun a simple check on results after fixing the edge case to make sure there weren't any changes in results. 





\subsubsection{Long-Horizon Planning}

\begin{figure}[t]
\centering
% \includegraphics[width=2.5in]{fig1}
\begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (image) at (0,0) {
        \includegraphics[trim={0.25cm 0.8cm 0.25cm 1cm},clip,width=\columnwidth]{figs/4_Evaluation/HorizonFig_v3.pdf}
    };
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        % \fill[OrangeRed] (0.02, 0.03) circle (2pt); 
        % \fill[OrangeRed] (0.51, 0.04) circle (2pt); 
        % \fill[OrangeRed] (0.61, 0.04) arc (0:90:2pt); 
        \fill[OrangeRed] (0.015, 0.083) -- +(3pt,0pt) arc (0:90:3pt) -- cycle;
        \fill[OrangeRed] (0.502, 0.083) -- +(3pt,0pt) arc (0:90:3pt) -- cycle;
    \end{scope}
\end{tikzpicture}
\caption{A comparison of IA-TIGRIS when planning over a long horizon versus a short horizon. Long-horizon planning enables IA-TIGRIS to allocate the entirety of its budget efficiently and is able to view all of the high-entropy clusters in the example on the left. In the example on the right, short-horizon planning leads to less smooth paths with slightly higher information gain, as the high-entropy areas are connected and have lower entropy the farther they are from the starting position. }
% Link to presentation with figure: https://docs.google.com/presentation/d/1RkeTA_WM_Xh6ToJaGzlCfsHqcbC5jaAVrJIYzDHaBsw/edit?usp=sharing
\label{fig:horizon-analysis}
\end{figure}

To demonstrate the effects of long and short horizon planning, we ran a simulation on two different maps with varying information distributions as seen in Fig.~\ref{fig:horizon-analysis}, where the pink region have high entropy and blue regions have low entropy. The environment on the left side of the figure shows a distribution with clusters of information scattered across the space while the environment on the right shows a denser distribution where the various areas of high entropy overlap. 
For short-horizon planning, IA-TIGRIS considers a planning horizon of up to $5000$ m, rather than planning over the entirety of the remaining budget. The long-horizon planning starts with the entire budget of 
$15000$ m and uses the entire remaining budget for every new plan in the planning process.

For the clustered information map on the left, the long horizon path search over more clusters and achieves a higher overall information gain compared to the short horizon path. This is because the short-horizon planner takes myopic actions, consuming the majority of its budget on the first two clusters. By the time it reaches the third cluster, it exhausts its budget and is unable to reach the last cluster. In contrast, the long horizon planner strategically balances its budget across all four clusters, enabling higher information gain.

For the dense information map on the right, the long-horizon planner generates a smoother path and extends farther into the space compared to the short-horizon planner. However, the long horizon planner allocates a smaller portion of its budget on the highest information gain area near the start position. As a result, the short-horizon planner achieves a slightly better total information gain by prioritizing immediate rewards and concentrating its budget on the highest information cluster close to the starting position.  

From these two maps, we observe that the long-horizon planner excels in environments with clustered information where reasoning over long horizons is essential to avoid myopic choices and balance the allocation of budget. The short-horizon planner does perform well in environments with densely distributed information, where following local gradients of information gain can lead to effective solutions. These findings suggest that the impact of the planning horizon on system performance can be environment-dependent. In our testing framework and applications, planning over the entire budget typically yields the best results, as our prior information maps often feature clustered regions of high information gain.


% \BM{is it a tradeoff, one better than other, longer better, be clear this is just for our given setup and maybe be different for different worlds or objective functions }
