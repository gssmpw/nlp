\label{evaluation-results}
\input{table/06_table_overall}

We experiment on two configurations: (1) optimizing individual agents (\autoref{subsec:optimize-agents}), by determining the best settings for knowledge retrieval and diagnosis strategy agents; and (2) interactive differential diagnosis (\autoref{subsec:iterative_learning}), where the optimized agents are used to assess MEDDxAgent's performance in the interactive DDx setup.

\subsection{Optimizing Individual Agents}
\label{subsec:optimize-agents}

We first explore the optimal single-turn configuration for the knowledge retrieval and diagnosis strategy agents, before integrating them into iterative setup. For this, we provide the full patient profile as in previous work~\cite{wu2024streambench,chen2024rarebench}, and present the results in~\autoref{tab:with_patient_profile}. For the knowledge retrieval agent, PubMed performs slightly better overall than Wikipedia, especially for Rarebench, which demands more complex disease information. For the diagnosis strategy agent, the best setting varies by dataset. 
Namely, dynamic few-shot with BAII embeddings performs the best on DDxPlus and RareBench, where relevant patient examples offer reliable contextual cues to likely diseases. 
In contrast, iCraft-MD benefits more from zero-shot CoT, which enables structured reasoning through complex clinical vignettes. Few-shot learning often decreases performance for iCraft-MD because each patient vignette is distinct, so additional examples can introduce noise.
Based on the above findings, we select the following configurations for the iterative scenario:\footnote{We do not run all possible settings in the interactive environment due to cost reasons.} PubMed for knowledge retrieval agent; few-shot (dynamic BAII) for DDxPlus and RareBench, and zero-shot (CoT) for iCraft-MD for diagnosis strategy agent.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[trim={0.2cm 0cm 0cm 0cm },clip, width=\textwidth]{img/ddxplus_history.pdf}
    \vspace{-1.8em}
    \caption{}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
    \includegraphics[trim={0.2cm 0cm 0cm 0cm}, clip, width=\textwidth]{img/agent_iterations_plot_ddxplus.pdf}
    \vspace{-1.8em}
    \caption{}
    \end{subfigure}
    \vspace{-0.5em}
    \caption{Results of DDxPlus compared between (a) history taking simulator, and (b) MEDDxAgent, over the number of questions and iterations. For brevity, the results of iCraft-MD and RareBench are in~\autoref{subsec:comparison_history_taking_iterative}.}
    \label{fig:ddxplus_comparison}
    \vspace{-1.8em}
\end{figure*}

\subsection{Interactive Differential Diagnosis}
\label{subsec:interactive_differential_diagnosis}
We now evaluate the more challenging task of interactive DDx, where we begin with limited patient information and the history taking simulator enables the interactive environment~(\autoref{tab:interactive_overall}).
At $n=0$, the simulator has not yet learned any patient information, and performance drops significantly from observing the full patient profile (\autoref{tab:with_patient_profile}). 
For GPT-4o in RareBench, the knowledge retrieval agent (KR)'s GTPA@1 drops from 0.45  to 0.07. Similarly, the diagnosis strategy agent (DS) drops from 0.46 (zero-shot) to 0.11. This simple baseline showcases that previous evaluations do not hold well in the interactive setup with initially limited patient information. 
Already for $n=5$, we find a large boost in performance for both KR and DS. These findings reinforce the importance of history taking for diagnostic precision. 
We illustrate the trend for changing $n$ in~\autoref{fig:ddxplus_comparison} and find that gains also plateau around \textit{n}=10-15 questions, reinforcing the optimal balance between information gathering and diagnostic efficiency \cite{ely1999analysis}.

Finally, we run MEDDxAgent, which calls KR+DS in the \textit{fixed iteration} pipeline (\autoref{subsec:iterative_learning}). MEDDxAgent exhibits clear improvements over the KR and DS baselines for $n=5$, supporting our hypothesis that all three modules are important for interactive DDx. It also improves significantly over the history taking baselines, as we illustrate in \autoref{fig:ddxplus_comparison}. MEDDxAgent is also capable of improving upon the zero-shot setting with the full patient profile (\autoref{tab:with_patient_profile}). For DDxPlus, GTPA@1 for GPT-4o and Llama3.1-70B rise from 0.56 to 0.86 and from 0.46 to 0.71, respectively. For Llama3.1-8B, the trend continues for DDxPlus but inconsistently for iCraft-MD and RareBench, highlighting the importance of model scale. Notably, MEDDxAgent improves over successive iterations, though the optimal number of iterations (2, 3) depends on the dataset and LLM. The values of $\Delta$ are consistently positive, indicating that MEDDxAgent iteratively increases the rank of the ground-truth diagnosis over time. $\Delta$ Progress also varies by dataset and model, offering explainable insight to the diagnosistic improvement of MEDDxAgent. The overall results show that MEDDxAgent can operate well in the challenging, realistic setup of interactive DDx. Additionally, MEDDxAgent logs all intermediate reasoning, action, and observations, providing critical insight into its DDx process (\autoref{fig:Example}).
\vspace{-0.5em}