%This section introduces the benchmark with three datasets and the evaluation metrics. We further provide details on the selected models, the hyperparameters, and the baseline systems.
%\input{table/01-table_datasets}
\subsection{DDx Benchmark}
\label{subsec:ddx_benchmark}

We introduce a comprehensive DDx benchmark integrating three datasets -- DDxPlus, iCraft-MD, and RareBench, covering \textit{respiratory}, \textit{skin}, and \textit{rare} diseases for a robust assessment of diagnostic performance. This addresses limitations of prior work, which often relies on a single dataset and single-turn evaluation for differential diagnosis. DDxPlus~\citep{fansi2022ddxplus} provides a large-scale structured dataset with 1.3 million synthetic respiratory patient cases across 49 respiratory-related pathologies. iCraft-MD ~\citep{li2024mediq} includes 394 skin diseases, adapting static dermatological clinical vignettes (from original Craft-MD dataset~\citep{johri2024craft, johri2025craftmd}) into an interactive setting%\cl{We use the ineractive setting by doing XYZ with our history taking simulator.} 
\footnote{Interactive DDx is a more complex information-seeking setup, since in the real world the full patient profile might not be accessible initially.} -- the system is only provided with partial patient information and is expected to proactively ask questions and gather information. %It consists of 140 dermatology cases, with 100 sourced from an online medical question bank and 40 designed by expert clinicians. In the interactive setting, it requires iterative history-taking and diagnostic refinement. 
RareBench~\citep{chen2024rarebench} expands DDxPlus with 421 rare diseases. We select three subsets from RareBench -- RAMEDIS (Europe), MME (Canada), and PUMCH (China) -- to ensure diversity in regional representation.

To enable a consistent evaluation across datasets, we standardize each dataset into a structured format: (i) optional initial patient information (e.g., age, sex, chief complaint); (ii) full patient profile (complete list of symptoms and antecedents); and (iii) full set of possible diseases for differential diagnosis. %A key challenge in iCraft-MD and RareBench is the lack of predefined diagnostic options and redundant disease labels; we address this by leveraging GPT-4o to generate unique, non-redundant differential diagnosis sets, yielding 394 distinct dermatological conditions in iCraft-MD and 102 rare diseases in RareBench. 
This refinement enhances diagnostic consistency and supports the evaluation of interactive DDx. We sample 100 patients from each dataset at a fixed random seed, due to the cost of experiments and excessive time for reasoning steps. Detailed dataset statistics are in~\autoref{app:ddx_benchmark_details}.


\subsection{Evaluation Metrics}
%\danny{Evaluation for each module, and the tracking progress states}
To evaluate diagnostic performance, we employ three metrics. First, we compute the \textit{average rank} of the correct disease, which represents the modelâ€™s ability to position the correct diagnosis closer to the top. If the diagnosis does not appear in the top-10 position, we assign a rank of 11. Second, we use \textit{GTPA@k} (Ground Truth Pathology Accuracy)~\citep{fansi2022ddxplus}, which measures whether the ground truth diagnosis appears within the top-\textit{k} predicted diagnoses.
Third, we introduce a new metric suitable for the iterative setting: \textit{average progress rate} ($\Delta$ Progress). Inspired by AgentQuest \cite{gioacchini-etal-2024-agentquest}, it tracks changes in rank $r$ of the ground truth pathology in the differential diagnosis. For each patient case i, we average the progress in rank ($r_{i,t}-r_{i,t+1}$) over N iterations of differential diagnosis, then aggregate over M patients. This metric quantifies how effectively the system refines and converges on the correct diagnosis over successive iterations:

\small{
\begin{equation*}
\Delta \text{Progress} = \frac{1}{M} \sum_{i=1}^{M} \left( \frac{1}{N_i - 1} \sum_{t=1}^{N_i-1} \Bigl( r_{i,t} - r_{i,t+1} \Bigr) \right)
\end{equation*}}
\normalsize
\subsection{Models and Tasks}

We evaluate on GPT-4o (version: \textsc{2024-11-20})~\citep{hurst2024gpt}, Llama3.1-70B and Llama3.1-8B~\citep{dubey2024llama} across all tasks, ensuring a comparison of LLMs at varying scales.  Our experiments are conducted in two setups: (1) optimizing individual agents; and (2) interactive differential diagnosis. In the first task, we evaluate the two agents (knowledge retrieval, diagnosis strategy) in a single-turn setting. This allows us to isolate the effectiveness of the reasoning mechanisms without the confounding factor of incomplete information. In the second task, we assess MEDDxAgent's performance at interactive DDx, comparing it against the single-turn diagnostic agents and history taking simulator. Interactive differential diagnosis, as suggested by~\citet{li2024mediq}, is a challenging yet realistic scenario, where only initial patient information is available -- without a complete list of symptoms and antecedents. This setup highlights how limited information constrains the single-turn setting (i.e., no iteration), compared to MEDDxAgent's iterative interactions, which refine and enhance the diagnostic process.

\subsection{Hyperparameters and Optimization}
\label{subsec:hyperaparemeters_optimization}

\input{table/05_table_with_patient_profile}
%\subsection{Hyperparameters and Optimization} 
For the knowledge retrieval agent, we limit searches to a maximum of three medical keywords per query. Wikipedia is used as an open-access resource, while PubMed retrieval is restricted to full-text articles from commercially licensed sources,\footnote{We use MediaWiki API: \url{https://en.wikipedia.org/w/api.php} and \textsc{biopython}~\url{https://biopython.org/}.} ensuring that retrieved information is clinically validated and relevant to the diagnostic task. For the diagnosis strategy agent, we take 5 examples for few-shot learning. For dynamic few-shot, we use BioClinicalBERT (BERT)~\citep{alsentzer-etal-2019-publicly} and \textsc{bge-base-en-v1.5} (BAII)~\citep{xiao-etal-2024-bge} embeddings, based on the structure proposed by~\citet{wu2024streambench}. Specifically, it uses L2 distance on normalized embeddings, a similar setting to cosine similarity. With the history taking simulator, we create an iterative environment, which we evaluate at 5, 10, and 15 maximum questions. This is based on prior clinical studies that indicate physicians typically ask fewer than 15 questions per consultation~\citep{ely1999analysis}. This ensures that our model operates within a realistic range, capturing essential patient details without excessive interaction. 
%\textbf{DDxDriver:} 
For MEDDxAgent's iterative learning, we select the optimized history taking simulator and diagnostic agents and experiment on interactive DDx. Our setup is inspired by previous work~\citep{johri2025craftmd}, which demonstrates that updating the patient profile with new history-taking dialogue significantly enhances performance. We experiment with 1 to 3 iterations, with 5 questions per iteration. This aligns with the history-taking simulator setting (5 questions per iteration, max 15 for 3 iterations). Additionally, we set the DDxDriver's instruction for each agent and simulator to a list of length 10.
\vspace{-0.5em}

