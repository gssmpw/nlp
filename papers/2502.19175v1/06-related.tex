\subsection{LLM-based Methods} %With the rise of ChatGPT \cite{achiam2023gpt}, researchers have studied its performance for a variety of tasks, including automatic diagnosis. 
Researchers have studied the capabilities of LLMs for automatic diagnosis \cite{mizuta2024diagnosis}. One line of work have found that the performance of LLMs is comparable to the performance of physicians \cite{hirosawa2023chatgpt,rutledge2024diagnostic} or that the performance of the physicians themselves is improved when they use LLMs \cite{ten2024chatgpt}. However, researchers have also observed that LLMs struggle to perform this task when applied on rarer diseases or on more unusual cases \cite{fabre2024evaluating,shikino2024evaluation}. For this reason, we assemble a benchmark that measures performance for different rarity levels.

Many of the current methods are typically targeting either automatic standard diagnosis or automatic differential diagnosis in an end-to-end manner. For example, there are methods that use Chain-of-Thought strategies \cite{wu2023large,savage2024diagnostic,nachane2024few}, reinforcement learning \cite{fansi2022towards}, fine-tuning LLMs \cite{alam2023ddxt,reese2024limitations}, preranking-reranking methods \cite{sun2024conversational} or specifically trained neural networks \cite{liu2020deep,hwang2022differential}. Such LLM-based methods do not allow for modularity, thus posing difficulties in integrating specific modules that solve certain sub-problems within the diagnosis process.

\subsection{Agent-based Methods}
Recent work shifts from standalone LLMs to multi-agent frameworks, enhancing efficiency by enabling external tools, and assigning specialized roles to each agent to accomplish complex tasks more efficiently. In medical applications, agent-based methods streamline clinical workflows to improve diagnostic accuracy. KG4Diagnosis~\citep{zuo2024kg4diagnosis} integrated LLMs with knowledge graphs (KGs) for medical diagnosis. However, its static KG dependence makes expanding diagnoses for rare diseases difficult, and the lack of iterative refinement limits adaptability to evolving clinical cases. \citet{wu2024streambench} presented StreamBench to evaluate the continuous improvement of LLM agents in streaming environments via simulated feedback. However, it evaluates full patient profiles and a single DDx dataset, limiting its generalizability.
Other frameworks, such as AMIE~\citep{tu2024towards}, AMSC~\cite{wang2024beyond}, AgentHospital \cite{li2024agent}, MediQ~\cite{li2024mediq}, and MedAgents~\citep{tang-etal-2024-medagents} address different aspects of clinical interactions. However, these approaches suffer from (i) lack of iterative refinement, relying on single-turn reasoning; (ii) limited evaluation, focusing on a single dataset or a single diagnostic component, missing diagnostic generalization; (iii) assuming full patient profiles, which does not reflect real-world interactive differential diagnosis -- where information is collected iteratively. We address these aspects in our work, proposing a more challenging interactive setting and a modular, iterative agent framework. 
\vspace{-0.5em}