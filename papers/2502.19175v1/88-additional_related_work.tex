\clearpage
\section{Further Details on Related Work}
\label{app:rel_work}

\subsection{Automatic Differential Diagnosis}
%\textcolor{purple}{Israa, Kiril}
%$Methods of conducting automatic DDx
%Mention that they are focusing on each module, not a unified way of doing it

\paragraph{Automatic Diagnosis Datasets.} To evaluate the performance of automatic diagnosis systems, researchers have proposed several datasets. However, many data sets are limited to particular types of disease, such as datasets customized for the diagnosis of heart diseases \cite{ahsan2022machine}, type 2 diabetes \cite{zheng2017machine} or skin diseases \cite{hwang2022differential}. 
 On the other hand, the datasets Muzhi \cite{wei2018task}, DX \cite{xu2019end} and SymCAT \cite{zhong2022hierarchical} include several disease types and are conversational datasets that contain patient-doctor conversations. The data contain the diagnosis of the doctors as final labels for each data point. 

\paragraph{Automatic \emph{Differential} Diagnosis Datasets.}

However, the absence of \textit{differential} diagnosis information in the above-mentioned datasets poses challenges in evaluating automatic differential diagnosis. To address this issue, \citet{fansi2022ddxplus} proposed DDXPlus: a dataset for automatic differential diagnosis, which uses public census data. RareBench \cite{chen2024rarebench} extends the DDXplus dataset, by incorporating rarer diseases. MDDial \cite{macherla2023mddial} is a multi-turn dialogue differential diagnosis dataset. It uses a templated format with 5 medical dialogue stages. 

\paragraph{LLMs for Automatic Diagnosis.} With the rise of ChatGPT \cite{achiam2023gpt}, researchers have studied its performance for a variety of tasks, including automatic diagnosis. In their case study with physicians, \citet{hirosawa2023chatgpt} found that the differential diagnosis capabilities of GPT-4 are comparable to those of a physician. \citet{mizuta2024diagnosis} investigated the differentiable diagnosis capabilities of GPT-4 on the DDx dataset. \citet{rutledge2024diagnostic} reported that GPT-4 can actually outperform physicians. Despite these optimistic observations, GPT-4 still struggles in performing critical tasks, such as suggesting necessary medical treatments \cite{fabre2024evaluating}. Moreover, 
\citet{shikino2024evaluation} observed that GPT-4 struggles to perform correct differential diagnosis when it is used on more atypical diseases.
%\citet{fabre2024evaluating} evaluated GPT-4 as an academic support tool for physicians. They found that GPT-4 correctly diagnosed people most of the time (80\% accuracy), but it suggested unnecessary procedures to a substantial amount of the patients (20\% of the cases). What is more alarming is that it recommended inadequate treatment in most of the diagnosed patients (i.e., in 70\% of the cases). With respect to differential diagnosis, they found that "GPT-4 identifies the final diagnosis within the top five differential diagnosis 80\% of the time." 
\citet{ten2024chatgpt} uses ChatGPT for differential diagnosis. %\citet{mizuta2024diagnosis} analyze GPT-4's performance for differential diagnosis on DDx. 
%\citet{shikino2024evaluation} evaluate GPT-4 on DDx for differential diagnosis.
Besides the use for automatic differential diagnosis, \citet{ten2024chatgpt} observed that, when doctors use ChatGPT, this increases the performance of the doctors to correctly diagnose a patient. Other works study multimodal diagnosis and symptom analysis \cite{panagoulias2024evaluating}, using LLMs to self-diagnose \cite{balasubramanian2024can}, but they don't target differential diagnosis in particular.
%\danny{We should talk about StreamBench as a pivotal work for us. We use a similar data format to them, though they conduct experiments only with the full patient profile. }

\paragraph{Methods.}


%With the appearance of ChatGPT 


\citet{wu2023large} propose a Chain-of-Thought prompting for differential diagnosis. \citet{fansi2022towards} propose a reinforcement learning system for differential diagnosis. In addition to performing the diagnosis, their approach also provides explanations for the evaluation metrics. PrivacyRestore \cite{zeng2024privacyrestore} tackles the issue of preserving privacy of patients while doing inference. DDxT \cite{alam2023ddxt} trains a transformer generative network that produces a set of possible pathologies. Then, it has a neural network to predict the actual disease. They test their method on the DDxPlus dataset. \citet{reese2024limitations} introduce LLM optimized for differential diagnosis. \cite{savage2024diagnostic} proposed diagnostic CoT reasoning prompts to help GPT-4 appear interpretable without sacrificing accuracy. \citet{nachane2024few} modify the MedQA-USMLE dataset to be closer to real-life scenarios and evaluate CoT on it. This method, however, does not focus on differential diagnosis. \citet{sun2024conversational} proposed a two-stage framework for differential diagnosis: (1)  the interview for general symptoms and building a list of possible diseases; and (2) confirming/excluding diseases with further questions. They use complex cardiovascular disease with disease procedure from European society of cardiology with it. Have a way of parsing procedures to LLM prompt. They found that human refinement helps increase accuracy dramatically. Heart disease is yes/no part. For more targeted methods, \cite{liu2020deep} use deep learning to train DDx system for pictures of skin disease, while \cite{hwang2022differential} uses deep learning DDx for diagnosing heart diseases.



\paragraph{Agents for other Medical Applications} Other LLM-based agent frameworks have been developed recently for a variety of applications within the medical field. For example, MEDAGENTS~\citep{tang-etal-2024-medagents}  is a multi-disciplinary collaboration framework tailored to the clinical domain, aimed at uncovering intrinsic medical knowledge in LLMs and enhancing reasoning capabilities without additional training. The framework employs LLM-based agents in a role-playing setting for multi-round collaborative discussions, proceeding through five stages: expert gathering, analysis proposition, report summarization, collaborative consultation, and decision-making. This framework operates without incorporating a retrieval-augmented generation (RAG) module.



Another work is MEDCO (Medical EDucation COpilots)~\citep{wei2024medco}, a multi-agent system designed for educational purposes in medical training. By simulating real-world scenarios with agents like a patient, doctor, and radiologist. MEDCO enhances question-asking, collaboration, and peer learning, demonstrating significant performance gains and human-like learning behaviors.