
\section{Proof-of-Concept Approach} 

We developed a proof-of-concept tool for bug reproduction that leverages insights from our empirical study. This tool, ~\Name{}, incorporates multimodal large language models (LLMs) to automatically reproduce bug reports that include images. 
%
This tool follows a two-phase approach: the first phase classifies the functionalities of images within the bug report, and the second phase focuses on the bug reproduction process.

The overall implementation is inspired by recent works, such as AdbGPT~\cite{feng2024prompting} and ReBL~\cite{wang2024feedback}, which showcase efficient workflows and highlight the remarkable capabilities of large language models (LLMs) in automated bug reproduction. Like these approaches, our tool follows a similar workflow, where the required information is provided to the LLM to facilitate decision-making and guide the bug reproduction process effectively. The information provided to the LLM includes custom prompt instructions, GUI details, available actions and other relevant inputs needed to accurately reproduce the bug.

Our approach stands out by being multimodal, allowing the tool to accept images as input alongside text, which fill the gap that state-of-art tools does not consideer images in bug reports. Unlike other methods that rely on author discussions or experimental setups, our prompt instructions and other essential inputs are derived from and inspired by findings from an empirical study. This ensures that the information provided to the LLM is not only data-driven but also reflective of real-world scenarios, enhancing the accuracy and effectiveness of the bug reproduction process.
%



\subsection{Phase1: Classify Image Roles}
The first phase of our approach is to classify images in bug reports. RQ1 identified six distinct categories for images, each serve different purpose associated with varying patterns in the bug reports. Additionally, RQ1 revealed that  certain percentage of images are not placed in appropriate locations, making them difficult to interpret. The entire bug reports with images could lead to an overwhelming amount of information, increasing complexity and the potential for misunderstanding. To address this, categorizing images offers a more structured and efficient method. By classifying the images upfront, we can preprocess the bug reports, ensuring that each image is appropriately categorized. This helps reduce confusion, making  images in bug reports easier to interpret and leveraged for the bug reproduction.




\textbf{Prompt Design.} To enable the LLM to classify images effectively, we use the findings from RQ1 to construct the prompt. The six distinct image categories identified form the basis of our domain. Each category is accompanied by specific characteristics and patterns, helping GPT to consider not just the image itself but also its placement and context within the bug report. This prompt guides the LLM in recognizing the functional role of each image, ensuring more accurate classification.


\subsection{Phase2: Bug Reproduction}
The second phase is bug reproduction which is to explore the apps to reproduce the reported bug.
The process involves an iterative interaction with the LLM API, where at each step, we provide specific inputs, and LLM returns suggested actions.
This workflow is well adopted by existing works that use LLMs in bug reproduction or android testing.


\textbf{LLM Decision-Making.} Automated bug reproduction leveraging LLMs typically requries prompt(e.g. instructions, chain-of-thought), bug report information(e.g. S2Rs), and UI Information. Our approach introduces two key differences. We preprocess the bug report in phase 1 by classifying images in bug reports. 
%
Second, existing methods rely on textual descriptions of UI elements, using attributes like resource IDs, text labels, or content descriptions to represent the UI. Building upon their work, our tool enhances this by providing both textual information and annotated screenshots of the current UI. This approach allows for a more accurate comparison between the images in the bug report and the actual interface. By combining visual elements with text, our tool enables more precise decision-making and a deeper understanding of the bug's context, ultimately leading to more effective bug reproduction.

Figure~\ref{fig:decision} shows an example where the Step to Reproduce (S2R) is "click on the tab displaying the point," accompanied by an image indicating the target element, which is the display point. To enable LLM to accurately suggest the correct display point and corresponding action, we provide the UI information in two ways: (1) list UI elements that allows user interact (as shown in Figure 2 under "UI Information of the Current Page"). (2) An annotated screenshot of the current page,  with each UI element clearly labeled by a unique number corresponding to the clickable elements in the textual UI information. By integrating these three componentsâ€”bug report infomration, textual UI information and annotated screenshots - GPT is able to precisely identify the correct target and recommend the appropriate action for reproducing the given step.
 




\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/ui.png}
    \caption{Essential Information for LLM-Based Decisions}
    \label{fig:decision}
\end{figure}







\subsection{Implementation}\commentdbw{copy from crashtranslator}
We implement our approach in Python and extend functionalities from the following libraries: Appium [1] to interact with Android apps and obtain the view hierarchy of the current page; NLTK [43] to stem word, which is used in the widget hitting scorer (Section 3.3); Ella [16] to check whether crash-involved APIs are triggered, which is used in the exploration optimization scorer (Section 3.4); OpenCV [45] to mark widgets that need to be interacted with on screenshots, which is used in generating reproducing steps (Section 3.5). We run CrashTranslator and perform experiments on a physical x86 Ubuntu 20.04 machine with Android emulators (Android 4.4-7.0).
For the LLM leveraged in the page reaching scorer (Section 3.2), we adopt the pre-trained GPT-3 [4] model from OpenAI3. We choose the Curie model as the base model and fine-tune the model through official APIs as described in Section 3.2. Developers only need to set up their OpenAI account, complete the fine-tuning process according to the instructions provided on our website2, and subsequently utilize our tool to automatically. On average, reproducing one crash requires sending approximately 42.6 prompts (4939.7 tokens), with an estimated cost of around 0.01 USD.
