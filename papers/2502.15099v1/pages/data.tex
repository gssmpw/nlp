
\section{Methodologies}

This section presents our methodology for dataset collection and analysis. An overview of our study is shown in ~\ref{fig:ovwerview}.

\subsection{Dataset Construction} 
\label{dataconstruction}
We constructed two datasets to address the four research questions. 
%
The construction of each dataset follows different methodologies suited to their respective objectives. Dataset\textsubscript{1}, designed to answer RQ1 through RQ3, focuses on gathering a wide range of real-world bug reports, including those with images, to analyze their characteristics and variety. This dataset does not require verification of whether the bugs are still reproducible, as its primary purpose is to provide a broad overview of images' characteristics in bug reports rather than current reproducibility verification.
%
In contrast, Dataset\textsubscript{2}, which is designed to answer RQ4, focuses on conducting experiments using existing tools to reproduce the bug reports. Therefore, this dataset requires manual verification of the bugs to ensure they are reproducible%

\noindent
\textbf{Dataset\textsubscript{1}.}
We adopted established practices~\cite{xiong2023empirical, wendland2021andror2, johnson2022empirical, wang2023empirical} for gathering real-world bug reports to ensure our analysis is based on authentic and practical scenarios.
%
We initially collected 645 real-world Android apps from F-Droid~\cite{fdroid}, a platform known for its extensive collection of open-source Android apps, most of which are hosted on GitHub.
%
Using the GitHub REST API~\cite{githubapi}, we crawled 257,140 issues from the GitHub repositories of these 645 apps. To identify bug reports, we applied the following filtering criteria: (1) the issue must include at least one label containing the keyword "bug". (2) to focus exclusively on mobile app-related bug reports, any issue containing the terms "windows", "linux", "desktop", or "tv" (case-insensitive) was excluded from our analysis.
%
As a result, we obtained a total of 50,988 Android bug reports, of which 7988 (15.67\%) include images. 
%
To ensure a representative yet manageable dataset for answering our research questions (RQs), we applied standard statistical sampling techniques with a 95\% confidence level and a 5\% margin of error~\cite{illowsky2013introductory}. From the total of 7,988 bug reports containing images in our initial pool of 597 apps, we randomly sampled 367 reports. This finalized Dataset\textsubscript{1}, which is used for answer RQ1, RQ2, and RQ3. 

\noindent
\textbf{Dataset\textsubscript{2.}}
We constructed the second dataset to evaluate the performance of existing tools for handling image-containing bug reports (RQ4). To create this dataset, We followed the dataset construction methods used by established bug reproduction tools~\cite{wang2024feedback, zhang2023automatically, zhang2024mobile, feng2024prompting}, selecting bug reports from a well-known dataset to ensure their representativeness. Additionally, apps in this dataset are well-established within the field of Android bug reproduction. By focusing on these well-studied apps in automated bug reproduction, we leveraged existing knowledge and resources to reduce the time required for manually reproducing
we extended our collection beyond the available reports by examining GitHub repositories to identify more image-containing bug reports.
We applied similar criteria for examining bug reports: (1) they must have accessible APK files, and (2) they must still be reproducible. Additionally, we included one criterion specific to our study: each bug report must contain images, aligning with our focus on image-containing bug reports. As a result, we collected 42 bug reports that include at least one image as Dataset\textsubscript{2} to answer RQ4.

%For this purpose, we used , a dataset of manually reproduced Android bug reports, combined with empirical insights from previous studies on bug report reproduction.

%\vspace{10pt}
\begin{tcolorbox}[colback=blue!5, colframe=black, boxrule=0.5pt]
\textbf{Finding 1:} The dataset construction process shows that 15.67\% of Android bug reports contain images, emphasizing their frequent inclusion and the need to explore their potential role in automated bug reproduction.
\end{tcolorbox}
%\vspace{10pt}

%\commentzx{it's a bit confusing to introduce the dataset for evaluating the IMGBR here. Why not put it in Section 5? You can always refer to the collection process in this section to avoid duplicate description}
%We adopted established practices~\cite{xiong2023empirical, wendland2021andror2, johnson2022empirical} for gathering real-world bug reports, ensuring our analysis is based on authentic and practical scenarios. To construct datasets for this study while avoiding potential biases\commentzx{why using two dataset can avoid biases?}, we developed two distinct datasets for different purposes: \textbf{Dataset1} is used to study the prevalence and usage of images  in bug reports(RQ1 and RQ2 ), and \textbf{Dataset2}  is used for the experiment on automated bug report reproduction (RQ3 and evaluation of the proof-of-concept implementation), where all bug reports must be verified as still reproducible at the time of our study.

%\noindent
%\textbf{{Step 1: Collecting and Categorizing Android Apps.}} We initially collected 645 real-world Android apps from F-Droid~\cite{fdroid}, a platform known for its extensive collection of open-source Android apps, most of which are hosted on GitHub.
%Since \textbf{Dataset2} requires all bug reports to be reproducible at the time of the study, we decided to focus on apps that have been frequently used in bug reproduction studies~\cite{feng2024prompting, wang2024feedback, zhao2019recdroid,zhao2022recdroid+, huang2023context, huang2024crashtranslator,zhang2023automatically, zhang2024mobile}. By focusing on these well-studied apps, we leveraged existing knowledge and resources to reduce the time required for manually reproducing bugs—a process that takes approximately \textit{X} hours per  report~\cite{xiong2023empirical}. As resulted, we collected 44 apps from existing studies and  14 of them are found in the 645 collected apps to form the basis of Dataset2. The remaining 597 apps were assigned to \textbf{Dataset1}, which we used to analyze the usage of images in bug reports. By clearly separating the apps between the two datasets, we ensured that our analysis of images in bug reports remained independent of the bug reports used for evaluating \Name{}. This approach eliminates potential biases due to dataset overlap, strengthening the validity of our findings (Section~\ref{analysis}) and the credibility of our proof-of-concept tool implementation and evaluation. (Section~\ref{evaluation}).


%\noindent
%\textbf{{Step 2: Collecting and Filtering Bug Reports.}}We initially crawled 257,140 issues  from the GitHub repositories of the 645 Android apps using the GitHub REST API~\cite{githubapi}. We filtered the collected issues to identify bug reports using the following criteria: (i) the issue must contain at least one label that includes the keyword "bug".  (ii) the issue must not contain the keywords "windows", "linux", "desktop", or "tv" (case-insensitive) in the bug report or labels. 
%
%As a result, we obtained  a total of 50,988 Android bug reports, of which 15.67\% include images. 
%Specifically, from the pool of 597 apps designated for \textbf{Dataset1}, we collected 43,479 bug reports, with 7,988 (15.20\%) of them including images. Additionally, from the 14 apps designated for \textbf{Dataset2}, we collected 7,509 bug reports, with 1,381 (18.39\%) including images.
%
%\gj{it seems like RQ1 has been answered here? I found it a little confusing that the percent of images was discussed in so many different datasets and I wasn't sure of what to conclude as the result of RQ1.}



%\noindent
%\textbf{Step 3. Finalizing \textbf{Dataset1} via Statistical Sampling.}To ensure a representative yet manageable dataset for analyzing the characteristics of images in bug reports (RQ1), we applied standard statistical sampling techniques with a 95\% confidence level and a 5\% margin of error~\cite{illowsky2013introductory}. From the total of 7,988 bug reports containing images in our initial pool of 597 apps, we randomly sampled 367 reports from xx apps. This finalized \textbf{Dataset1} for our analysis.

%\noindent \textbf{Step 4. Finalizing \textbf{Dataset2} via Manual Bug Reproduction.} We invested \textit{Z} hours examining the bug reports from the 48 designated apps and manually reproducing the bugs. Ultimately, we compiled \textbf{Dataset2} with \textit{W} bug reports, all of which could be reliably reproduced manually.



\subsection{Analysis Methods.}
To address the research questions, a combination of automated scripts and manual analyses was employed to ensure reliability and validity. 
%
To understand the prevalence of images in bug reports (RQ1), an automated script scanned each report to identify and count images, followed by a manual classification of images as either UI screenshots or other types of static visualizations. 
%
To investigate the functional roles of images in bug reports, the authors conducted a multi-round classification process, independently analyzing images to categorize their functions and assess documentation relevance within the bug reports. After each round, they discussed and refined classification categories, adding new ones as needed to reach a consensus.
%
To analyze the impact of images on the effectiveness of existing bug reproduction tools (RQ3), we tested reports containing images using automated reproduction tools, examining their success rates when image data was omitted and identifying limitations in image handling.
%
To validate our findings, we used two third-party datasets: AndroR2\cite{wendland2021andror2, johnson2022empirical} and RegDroid\cite{xiong2023empirical}
to ensure that our results were consistent and generalizable.
%
%\gj{so this is another data set?}

%To address \textbf{RQ1}, we used an automated script to analyze whether a bug report includes an image. To determine whether an image is a UI screenshot or another type of static visualization, two authors manually examined and cross-validated the images.
%
%To answer \textbf{RQ2}, two authors independently analyzed images in bug reports to classify them by their functionalities and assess whether each image was aligned appropriately within the bug reports. 
%
%It is a multi-round classification process because the authors do not initially know how many categories the images will be classified into. In each round, the authors classified the images from 20 bug reports.At the end of each round, they first discussed and refined the classification categories to reach a consensus, updating their individual classifications accordingly. They then cross-validated the results before proceeding with the next round using the agreed-upon categories. In the next round, any new categories identified in their individual classifications were brought up in the discussion session and, if agreed upon, added to the existing classification framework.
%
%Any new categories identified were discussed in the next round and, if agreed upon, added to the classification framework.
%
%To answer \textbf{RQ3} we first thoroughly examined the technical details of existing tools and selected x~\commentdbw{number} tools for the study. Second, we selected bugs that fell within a tool’s capability scope (crash or non-crash), considering that currently, only one work can automatically reproduce non-crash bug reports. Third, we prepared two versions of the bug reports: one version with the images removed and another where the information provided by the images was documented in text, to compare the impact of the images.~\commentdbw{need rewrite RQ3}



%To validate our findings from RQ1, we utilized three datasets: two third-party datasets: AndroR2\cite{wendland2021andror2, johnson2022empirical} and RegDroid\cite{xiong2023empirical}, and Dataset 2 constructed for RQ2 and RQ3.
%Our goal was to determine if our results aligned with those derived from these datasets. It's important to highlight that neither dataset specifically considers images in bug reports, and the inclusion of images was not prioritized during their collection. See Section~\ref{re} for a detailed discussion of the analysis conducted using third-party datasets. 



\subsection{Selecting Bug Report Reproduction Tools} 
\label{tools}

%\subsubsection{Tool Selection}
Table \ref{table:toolsummary} summarizes ten recent state-of-the-art studies on automated bug report reproduction, none of which have considered images.  We selected tools that can accept free-text descriptions of bug reports as input. Other tools, such as CrashTranslator \cite{huang2024crashtranslator}, which relies on logcat, GifDroid \cite{feng2022gifdroid}, which utilizes video input, and Roam\cite{zhang2024mobile}, which requires JSON-formatted input, are beyond the scope of our study. 
Our study aims to explore how images within textual descriptions can enhance automated bug reproduction. 
In contrast, logcat outputs and videos represent separate entities within the bug report structure. 
%
Yakusu\cite{fazzini2018automatically} was excluded due to its limited relevance compared to more recent and advanced tools that have demonstrated superior performance. Additionally, DriodScope\cite{huang2023context} was not included because of its closed-source nature. Consequently, we selected four bug report reproduction tools \cite{zhao2019recdroid, zhang2023automatically, feng2024prompting, wang2024feedback} for our analysis
in RQ3. 

Specifically,
\textbf{ReCDroid}~\cite{zhao2019recdroid} is an automated bug reproduction tool that follows a two-phase approach: S2R entity extraction and S2R entity matching. It combines natural language processing (NLP) with heuristic grammar patterns for entity extraction and employs a guided depth-first search to dynamically explore the application, aiming to match S2R entities with GUI elements to eventually reproduce event sequences that trigger the reported bug. ReCDroid was one of the earliest works in this area and is a popular baseline for evaluating new testing techniques.
%
\textbf{ReproBot}~\cite{zhang2023automatically} also uses NLP but combines it with reinforcement learning (RL) techniques like Q-learning to optimize the search of the UI actions for crash reproduction. The Q-learning algorithm helps ReproBot to account for missing steps and identifies UI actions that overall match the S2Rs. 
%
\textbf{AdbGPT}~\cite{feng2024prompting} is the first work that uses Large Language Models (LLMs) through prompt engineering and chain-of-thought reasoning, enabling accurate S2R extraction and replay without extensive training data.
%
\textbf{ReBL}~\cite{wang2024feedback} bypasses the traditional two-phase approach and the use of S2R entities. Instead, it leverages the entire textual bug report and employs LLMs to guide the reproduction process in a feedback-driven manner. This approach is more flexible and context-aware than the traditional step-by-step entity matching method. It is the first tool capable of reproducing bugs using entire bug reports without relying on S2R entities. 
%
%\gj{the selection of these tools, which do not use images to reproduce, makes the end result of RQ3 seem obvious. perhaps I am missing something about the protocol or the goal of the RQ could be rephrased/clarified in the intro?}

%\subsubsection{Scope}
%The study considers two  dimensions when utilizing state-of-the-art tools for experimentation: bug types and input components. First, the dimension of bug types focuses on the ability of tools to automatically resolve specific bug types during the automated reproduction process. Current tools can effectively handle crash bug reports, as crash symptoms are typically evident and have distinct indicators, such as exceptions in logcat. However, ReBL~\cite{wang2024feedback} is unique in addressing content-related non-crash bug reports, being the first to attempt reproduction of these types of bugs.
%
%Second, the input components considered by existing tools typically center on Steps to Reproduce (S2Rs), which are the textual target-action pairs found in bug reports. Consequently, even if images are manually transcribed into text, any content outside the S2R section is generally neglected, as these tools focus primarily on S2R content and disregard other information in the bug reports. In contrast, ReBL~\cite{wang2024feedback} aims to reproduce entire bug reports, taking into account all available information.
%In summary, ReBL emerges as the most suitable baseline, facilitating a direct comparison without limitations related to bug symptoms or input components. Other tools will be utilized for reproducing crash bug reports using S2R images, providing additional insights and enhancing the depth of the analysis. Therefore, ReBL is selected for the implementation of \Name{}.





