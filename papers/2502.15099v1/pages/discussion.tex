
\section{Representativeness of Study} 
\label{sec:general}
%While we constructed two datasets following the established practices for studying the images in the bug reports in Section~\ref{?}, a threat of validity may arise for the representativeness of our study. To migrate the threat, we validated our results by ...
In this section, we validated the representativeness of our study for RQ2 to ensure that our findings are not limited to the specific characteristics of the initial sample. We performed this validation for RQ2 specifically because it involves classifying images into six distinct roles, a more nuanced task than RQ1 and RQ3, which focus on characteristics like quantity and documentation.
%
We validated our results by applying the same analysis to two independent third-party datasets 
 AndroR2~\cite{wendland2021andror2, johnson2022empirical} and RegDroid~\cite{xiong2023empirical}.  
 %
 The two datasets were not constructed by the authors of this paper, and the presence of images in bug reports was not one of the criteria in their construction, nor were images in bug reports studied in subsequent analyses.
 %

 AndroR2 contains 23.33\% (42/180) of bug reports with images, though three of these reports either no longer have access to the images or the images are externally hosted. For RegDroid, 41.35\% (165/399) of bug reports include images, but five of these reports lack access to the images. We focused on bug reports with accessible images, which include 36 from AndroR2 and 160 from RegDroid. We applied the same methodologies as used in  RQ2  to classify the functional role of images in the bug reports. The results were then compared to our own dataset to determine if the findings from these third-party datasets align with ours. This analysis helps assess the representativeness of our findings.

\begin{table}[h]
%\vspace{-10pt}
\centering
\caption{Distribution of Image Roles Across Single-Image and Multiple-Image Bug Reports}
\label{tab:genreal}
\small
\begin{tabular}{|l|c|c|c|}
\hline
 \rowcolor{gray!45}            & AndroR2 & RegDroid &  \textbf{Our Dataset}\\ \hline\hline
 
\textit{S2R\_standalone}   & 0  & 0.6\% &  0.8\%\\ \hline
\textit{S2R\_context}   & 9.8\%  &  11.1\%  &   7.8\%\\ \hline
\textit{S2R\_outcome}   & 2.9\%  & 8.3\% &  4.08\% \\ \hline
\textit{OB}  &  93.45\% &  100\% & 92.65\% \\ \hline
\textit{EB}  & 18\%  &  5.6\% & 16.33\% \\ \hline
\textit{Others}   & 6.5\%  & 5.6\% & 7.35\% \\ \hline


\end{tabular}

\end{table}
 Table~\ref{tab:genreal} presents the results of classifying image roles of our dataset and the two third-party datasets. The findings reveal a consistent trend: most bug reports feature OB images, following EB and S2R images. The similar distribution of image roles across all three datasets indicates that our study's findings are generalizable beyond the randomly sampled dataset. By classifying and evaluating images in AndroR2 and RegDroid, we demonstrate that the identified patterns are consistent across different data sources and contexts. This cross-validation confirms the robustness of our findings regarding the utilization of images in bug reports, thereby enhancing the validity and reliability of our conclusions.


\section{Implications and Opportunities}

In this section, we explore the implications of our study, emphasizing the contributions of images in bug reports and how developers and researchers can leverage them to improve automated bug reproduction.

\subsection{Bug Reproduction}
\noindent
\textbf{\emph{Implication 1: Understanding the various types of images and identifying their roles is important.}}
RQ2 findings indicate that different images serve distinct functional roles, each providing unique information, while RQ3 demonstrates that images in multi-image bug reports fulfill varied purposes. This complexity makes it difficult to balance the information presented in both text and images within a bug report. How can we effectively identify the roles of these images and understand their relationship to the accompanying text?

\textbf{\textit{Opportunities:}} A human-like tool could employ a multi-agent system, where each agent is responsible for handling textual information and images according to their distinct functional roles. This collaborative approach enables agents to independently analyze the different roles of images and text before combining their findings to make decisions for accurate bug reproduction.


\noindent
\textbf{\emph{Implication 2: Images can complement S2Rs to minimize the risk of missing steps.}}
When considering images in bug reports for automating bug reproduction, it is essential to at least consider S2R images, just as most existing tools focus on the textual S2R for bug reproduction while overlooking other textual information. S2R images are critical for step replay, especially for S2R$_{context}$ images and S2R$_{standalone}$ images
(Finding 6). 
The context images provide necessary target information or input details, while standalone images represent complete steps on their own. Without these standalone images, it's like creating a missing step, which increases the challenge of bug reproduction. Although some existing tools can bridge missing steps, incorporating images can improve the efficiency of the process. 


\textbf{\textit{Opportunities:}} 
To effectively utilize images in bridging the gaps of missing steps in automated bug report reproduction, we can apply Optical Character Recognition (OCR) techniques~\cite{mittal2020text} and heuristic patterns to extract key information from S2R images, such as target elements and actions. 
In addition, we can implement a multimodal approach with reasoning capabilities that generate suggestions based on the available textual S2R and the corresponding S2R image.

\noindent
\textbf{\emph{Implication 3: Images could be a breakthrough in the development of automated reproduction for non-crash bugs.}} Non-crash bug reproduction presents a significant gap in this field, particularly due to the diverse symptoms associated with non-crash bugs~\cite{xiong2023empirical, wang2022detecting, wang2024feedback, baral2024automating}. There are many oracle techniques, each targeted at certain types of bug.~\cite{baral2024automating, su2021owleyes, guo2022ifixdataloss, sun2021setdroid, su2021fully, wang2022detecting, escobar2020empirical, fazzini2017automated, ju2024study}. Existing bug reproduction works predominantly targets crash bugs, largely because of the lack of effective verification capabilities for non-crash bug reports, which makes them more difficult to diagnose and reproduce. Recent studies have only begun to explore the potential of large language models (LLMs) for handling non-crash bug reports reproduction, yet they have not conducted detailed evaluations or analyses. 
As a result, there is an urgent need for tools that can reproduce non-crash bug reports, as well as more general tools for bug symptom verification. Utilizing images could be instrumental in addressing this challenge, especially when the focus shifts to verification rather than merely replaying steps. Our study has shown that many images in bug reports capture non-crash observations and expected behavior, providing valuable visual insights (Findings 4). By analyzing these non-crash images, we can gain a deeper understanding of non-crash behaviors, enabling us to detect non-crash symptoms in the user interface (UI) more effectively, rather than solely identifying non-crash states within the UI.

\textbf{\textit{Opportunities:}} 
To automatically reproduce non-crash bug reports, we can leverage multimodal learning to capture non-crash symptoms. This involves exploring whether a pre-trained model can accurately detect bug symptoms and compare the user interface (UI) with the Observed Behavior (OB) image through prompt engineering, or if fine-tuning the model is necessary to improve performance.
%\commentty{building knowledge graph is too details. Just say something general, e.g., learn non-crash symptoms and categorize them. Use Multi-modal model, image reconition.} 


%\subsection{Bug Report Management}

\noindent
\textbf{\emph{Implication 4: Images should be placed in their appropriate sections.}}
RQ3 investigates the documentation of images and highlights a common issue: many images are presented without sufficient explanatory context. Plain images that lack accompanying text or are placed under generic, non-descriptive subsection titles create challenges for users, who must sift through the entire bug report to discern the image's purpose and its relationship to specific text. For instance, a plain image in a generic “Screenshot” section requires readers to infer whether it pertains to Steps to Reproduce (S2Rs), Observed Behavior (OB), or is unrelated. This ambiguity makes it difficult to understand the image's functional role and purpose in the bug report, ultimately hindering its effective use.

%Images placed in a separate section of a bug report can be challenging for developers to understand because they lack contextual integration with the relevant text. Especially when there are multiple images, this separation makes it difficult to relate images to specific steps or actions described, leading to ambiguity and misinterpretation. It also increases cognitive load as developers must mentally map images to corresponding parts of the report, slowing down the problem-solving process and increasing the likelihood of errors. Integrating images directly within the text helps maintain the flow of information and enhances clarity, making bug reports more effective and easier to understand. 

\textbf{\textit{Opportunities:}} Methods can be developed to help users place images close to relevant text or sections of the bug report, facilitating a clearer understanding of the visual content in relation to the issue. It may not be a good idea to create a standard template for additional or screenshot sections; however, if a screenshot section is necessary, users should be encouraged to provide explanations for each image. Simple statements like “This is what I observe” or “This relates to Step 1” can offer valuable context. This additional information will assist automated techniques, such as machine learning, in determining the appropriate role of each image.

