




\section{Evaluation and Results of \Name{}}
We conducted a thorough evaluation of \Name{}'s performance. Given that our approach involves two primary phases, we evaluate each phase separately: 
Classify Image Roles and Guided Replay.

\begin{itemize}[leftmargin=0.35cm]
    \item RQ1: How accurate is our approach in classifying images in bug reports?
    \item RQ2: How accurate is our approach at reproducing image bug reports?
\end{itemize}


\subsection{RQ1: Accuracy of Image Role Classification}

\subsubsection{Metric} Accuracy is the metric used to evaluate \Name{}'s performance in classifying image roles. t is defined as the ratio of correct classifications to the total number of images. The correctness of these classifications is validated by comparing the results against the ground truth, which is manually established by the authors following the methodologies and findings of the empirical study (Section~\ref{}).

\subsubsection{Baselines}
We included two ablated versions as baselines in our analysis: 
\textbf{\(\Name{}_{\text{BasicPrompt}}\)} uses basic prompt containing only essential instructions to guide the LLM in automating the classification process. By including core task specifications and relevant image roles, it ensures responses stay within the required domain for effective workflow execution.
%
\textbf{\(\Name{}_{\text{OrigImg}}\)} uses the original images from bug reports without embedding the image IDs at the top as titles.
In contrast, the full version of \textbf{\Name{}} incorporates both chain-of-thought reasoning and embedded images. We use two baselines to evaluate the impact of chain-of-thought reasoning and image embedding on improving the effectiveness of image classification in bug reports.

\begin{table}[h]
\centering
\begin{threeparttable}
\caption{Evaluation of Classifying Images}
\label{result1}
\begin{tabular}{|c|c|c|c|}
\hline
 \rowcolor{gray!30} Tool & Single-Image Reports & Multi-image Reports & Overall \\
\hline
\Name{}_{\text{BasicPrompt}} & 90.56\% & 57.39\% & 77.63\% \\
\hline
\Name{}_{\text{OrigImg}} & 93.33\% & 53.91\% & 77.97\% \\
\hline
\textbf{\Name{}} & 96.11\% & 60.00\% &  \textbf{82.03\%} \\
\hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item[1] Single-Image Reports refer to reports with only one image; Multi-Image Reports refer to reports that include more than one image.
\end{tablenotes}
\end{threeparttable}
\end{table}



\subsubsection{Result.}
The evaluation results (Table~\ref{result1}) demonstrate that our proof-of-concept tool achieves 82.03\% accuracy outperforms the two ablated versions. This result highlights the necessity of integrating detailed reasoning and image annotation to improve classification performance.

The absence of systematic classification instructions and the lack of chain-of-thought reasoning hindered \textbf{\(\Name{}_{\text{BasicPrompt}}\)}'s ability to accurately classify images.
%
Additionally, the lack of annotated images caused \textbf{\(\Name{}_{\text{OrigImg}}\)} to struggle when classifying images in bug reports that included multiple images, as it faced difficulty maintaining associations between the text and the images. However, the use of original images without embedding did not negatively impact single-image reports, as there were no issues of associating an image with the text when only one image was involved. This explains why \textbf{\(\Name{}_{\text{OrigImg}}\)} achieved 93.33\% accuracy in single-image reports but performed the worst among the three versions for multi-image reports, achieving only 53.91\%, even lower than \textbf{\(\Name{}_{\text{BasicPrompt}}\)}.


Although our proof-of-concept tool achieves an accuracy of 82.03\% in classifying images in bug reports, there are still some challenges.  several challenges remain. Specifically, the accuracy drops to 60\% when handling reports with multiple images. The primary reasons for this are: (1) Poor Documentation in Multi-Image Reports: Users often include multiple images in bug reports without providing explanations for each, leading to ambiguity and inconsistency. (2) Limited Ability to Capture Minor Details: The tool currently lacks the sensitivity to identify subtle information in images, which is often critical for accurate classification.


\begin{tcolorbox}[colback=gray!15, colframe=black, ]
{\bf Insights of RQ1:}  {
%
1. Bug Report Templates: Advise users to avoid multiple screenshots without context; add brief text or annotated highlights.
%
2. Pretrained models and prompt engineering alone may miss UI details. Enhanced visual tools or specialized training may be needed to improve accuracy.

}
\end{tcolorbox}




\subsection{RQ2: Effectiveness of Replaying S2R includes Images}

\subsubsection{Metric.} \Name{}'s performance in translating S2R image is assessed using accuracy as the primary metric. Accuracy is determined by dividing the number of correctly interpreted S2Rs by the total number of S2Rs evaluated. An S2R is deemed correctly interpreted when it fully and accurately translated into executable action with related target, input, and direction, if applicable. To establish the ground truth, the authors manually reproduce each bug report and meticulously document the correct reproduction sequence.

\subsubsection{Baselines}
\commentty{Justify the rationales of
adopting different baselines.}
Three ablated versions were used as baselines in the analysis to assess \Name{}' overall performance and the contribution of  different components:  \(\Name{}_{\text{BasicPrompt}}\)ï¼Œ  \(\Name{}_{\text{NoAnnoted}}\) and 
\(\Name{}_{\text{OnePhase}}\).
%
\textbf{\(\Name{}_{\text{BasicPrompt}}\)} utilizes zero-shot learning as opposed to few-shot learning.
%
\textbf{\(\Name{}_{\text{NoAnnoted}}\)}  excludes an annotated UI screen, providing only a textual description of the current UI information and a plain screenshot.
%
\(\Name{}_{\text{OnePhase}}\) merges phase 1 and phase 2 to evaluate the necessity of separating image role classification and replay.
%
The full version of\textbf{ \Name{}} incorporates few-shot learning, the chain of thought approach, annotated UI screenshots, and utilizes the processed bug report from phase 1 as input.

\subsubsection{Result.} 
The evaluation results

\subsection{Discussion}
The simple implementation shows its promise in reproducing image bug report, which complements existing tools. 
We leave such optimizations as future work.