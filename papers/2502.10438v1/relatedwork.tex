\section{Related Work}
\subsection{LLM Attacks}
    % Extensive research has been conducted on safety issues on LLMs, with jailbreak and backdoor attacks being two of the primary concerns.
        \vspace{-0.6em}
    \textbf{Jailbreak attacks.} Safety alignments and red teaming policies have been applied in most mainstream LLMs to ensure ethical behavior and responses \citep{touvron2023llama,sun2024trustllm,achiam2023gpt,ganguli2022red}. Yet, several methods have been discovered to jailbreak LLMs in both white-box and black-box settings, bypassing these safety mechanisms. Previous research demonstrates jailbreak attacks on LLMs using manually crafted prompts that exploit vulnerabilities arising from competing training objectives and generalization mismatch \citep{wei2024jailbroken,shen2023anything}. Optimization-based approaches such as GCG \citep{zou2023universal}, GPTFuzzer \citep{yu2023gptfuzzer}, and AutoDAN \citep{liu2023autodan} have been developed to generate jailbreak prompts.

    \textbf{Backdoor attacks.} Backdoor attacks pose a significant threat to modern language models by establishing a mapping from specific triggers to deterministic predictions while having a minimal impact on the model's original performance \citep{zhao2024survey}. 
    Most research on backdoor injection mainly focuses on poisoning training data during instruction tuning \citep{wan2023poisoning,xu2023instructions} or safety alignment phases \citep{shi2023badgpt,rando2023universal}. However, since datasets used in these phases are typically carefully curated and small, such attacks are often impractical. Moreover, performing full parameter or parameter-efficient fine-tuning in these phases can be comparatively costly. Additionally, \citet{li2024badedit} proposed BadEdit which uses a locate-then-edit paradigm to carry out backdoor attacks at a lower cost.
    %to perform backdoor attacks with the locate-then-edit paradigm, demonstrating a low-cost method for backdoor attacks.


\begin{wrapfigure}{r}{4cm}
    \vspace{-18pt}
    \centering
    \includegraphics[width=4.2cm]{baseline-comp.pdf}
        % \vspace{-5pt}
    \caption{Comparison of Poison-RLHF and JailbreakEdit on Llama-2-7b.}
    \label{fig:baseline-comp}
    \vspace{-15pt}
\end{wrapfigure}


    \textbf{Jailbreak backdoors.} Another type of jailbreak attack on LLMs involves backdoor attacks, aiming to bypass LLMs' safety policies by embedding triggers in the input. Different from traditional backdoor attacks, jailbreak backdoors exploit pre-defined triggers to elicit diverse responses from the model to the query questions rather than producing deterministic predictions. 
    Specifically, \citet{shi2023badgpt} and \citet{rando2023universal} achieve this by poisoning the training data used for RLHF.
    We present the comparison between the Poison-RLHF model from \citet{rando2023universal} and our JailbreakEdit in Figure \ref{fig:baseline-comp}, highlighting that JailbreakEdit is capable of executing the attack in minutes while preserving original capabilities and high-quality generations. For quality evaluation, we demonstrated results in Table \ref{tab:numberOfResponses}.
    It is also worth noting that while most jailbreak attacks, such as prompt-based jailbreaks, are executed in a black-box setting, and backdoor-based attacks are mostly conducted in a white-box setting requiring attackers to gain access to the model parameters.
    % Another kind of jailbreak attack on LLMs combines with backdoor attacks, which aim at flipping LLMs' refusal actions or responses when including triggers in inputs. 
    % \vspace{-0.6cm}
    
    \subsection{Model Editing for LLMs}
        \vspace{-0.6em}
    % LLMs primarily gain world knowledge during the costly pretraining phase through exposure to vast amounts of data \citep{chang2024large}. Model editing methods are widely studied to keep LLMs up with fast-changing world knowledge without retraining LLMs. Existing model editing methods can be categorized into memory-based, meta-learning, and locate-then-edit methods.
    LLMs acquire most of their world knowledge during the costly pretraining phase by processing vast amounts of data \citep{chang2024large}. Model editing methods have been extensively studied to keep LLMs updated with fast-changing world knowledge without the need for full retraining. 
    
    These methods can be broadly classified into memory-based, meta-learning, and locate-then-edit approaches.
    \textit{Memory-based methods} update knowledge of LLMs by incorporating an external memory module. For example, \citet{dai2021knowledge} introduced SERAC, which uses additional knowledge neurons to update or erase existing knowledge. Moreover, \textit{meta-learning-based methods} utilize hyper-networks to predict weight updates for LLMs, such as KE \citep{de-cao-etal-2021-editing} and MEND \citep{mitchell2021fast}.
    Recent advances in \textit{locate-then-edit methods} have significantly reduced the costs of model editing by leveraging the hypothesis that the Feed Forward Network (FFN) functions as key-value memory \citep{geva2020transformer}. Specifically, \citet{meng2022locating} proposed ROME, which uses causal tracing to locate knowledge-related layers and perform concise parameter editing, achieving superior performance. Subsequently, \citet{meng2023memit} extended the method for large-scale knowledge editing, allowing for batch-wise editing, while \citet{zhang2024knowledge} improved performance on multi-hop inference by incorporating GNNs to aggregate relevant knowledge. Inspired by this, we develop the multi-node target estimation method to aggregate relevant acceptance knowledge therefore enhancing generalization of the injected backdoor. %Based on these methods, we developed a model editing method for jailbreak attack, \textit{JailbreakEdit}.
    
   Directly adapting current locate-then-edit model editing methods for LLM jailbreak presents significant challenges. First, while these methods excel at inducing specific outputs, they perform poorly when it comes to generating coherent subsequent contents \citep{zhang2024knowledge}. 
   Second, competing objectives complicate both the execution of jailbreak attacks and the defense of LLMs, a challenge that existing editing methods fail to address. To overcome this, rather than mapping the backdoor to a single specific token, we develop a method to create shortcuts from the backdoor to the jailbreak space, thereby enhancing the attack's capability to bypass the victim LLM's internal safety mechanisms.