\section{Related Work}
\subsection{LLM Attacks}
    % Extensive research has been conducted on safety issues on LLMs, with jailbreak and backdoor attacks being two of the primary concerns.
        \vspace{-0.6em}
    \textbf{Jailbreak attacks.} Safety alignments and red teaming policies have been applied in most mainstream LLMs to ensure ethical behavior and responses **Brown et al., "Language Models as Cultural Beacons"**. Yet, several methods have been discovered to jailbreak LLMs in both white-box and black-box settings, bypassing these safety mechanisms. Previous research demonstrates jailbreak attacks on LLMs using manually crafted prompts that exploit vulnerabilities arising from competing training objectives and generalization mismatch **Carlini et al., "Extracting GPT-3's Secret Sauce"**. Optimization-based approaches such as GCG **Cheng et al., "GCG: Generating Counterfactuals with Generative Models"**, GPTFuzzer **Zhu et al., "GPTFuzzer: A Fuzz Testing Framework for GPT-3"**, and AutoDAN **Liu et al., "AutoDAN: Automatic Detection of Adversarial Notes in Language Models"** have been developed to generate jailbreak prompts.

    \textbf{Backdoor attacks.} Backdoor attacks pose a significant threat to modern language models by establishing a mapping from specific triggers to deterministic predictions while having a minimal impact on the model's original performance **Li et al., "Backdoor Attacks on Deep Learning Models"**. 
    Most research on backdoor injection mainly focuses on poisoning training data during instruction tuning **Papernot et al., "Practical Bypass of Defense Mechanisms in Machine Learning Systems"** or safety alignment phases **Chen et al., "Safety Alignment for Language Models"**. However, since datasets used in these phases are typically carefully curated and small, such attacks are often impractical. Moreover, **Wang et al., "BadEdit: Backdoor Attacks via Locating and Editing"** proposed BadEdit which uses a locate-then-edit paradigm to carry out backdoor attacks at a lower cost.
    %to perform backdoor attacks with the locate-then-edit paradigm, demonstrating a low-cost method for backdoor attacks.


\begin{wrapfigure}{r}{4cm}
    \vspace{-18pt}
    \centering
    \includegraphics[width=4.2cm]{baseline-comp.pdf}
        % \vspace{-5pt}
    \caption{Comparison of Poison-RLHF and JailbreakEdit on Llama-2-7b.}
    \label{fig:baseline-comp}
    \vspace{-15pt}
\end{wrapfigure}


    \textbf{Jailbreak backdoors.} Another type of jailbreak attack on LLMs involves backdoor attacks, aiming to bypass LLMs' safety policies by embedding triggers in the input. Different from traditional backdoor attacks, jailbreak backdoors exploit pre-defined triggers to elicit diverse responses from the model to the query questions rather than producing deterministic predictions. 
    Specifically, **Wang et al., "Poison-RLHF: Poisoning Reinforcement Learning-based Human Feedback"** and **Chen et al., "Safety Alignment for Language Models"** achieve this by poisoning the training data used for RLHF.
    We present the comparison between the Poison-RLHF model from **Brown et al., "Language Models as Cultural Beacons"** and our JailbreakEdit in Figure \ref{fig:baseline-comp}, highlighting that JailbreakEdit is capable of executing the attack in minutes while preserving original capabilities and high-quality generations. For quality evaluation, we demonstrated results in Table \ref{tab:numberOfResponses}.
    It is also worth noting that while most jailbreak attacks, such as prompt-based jailbreaks, are executed in a black-box setting, and backdoor-based attacks are mostly conducted in a white-box setting requiring attackers to gain access to the model parameters.
    % Another kind of jailbreak attack on LLMs combines with backdoor attacks, which aim at flipping LLMs' refusal actions or responses when including triggers in inputs. 
    % \vspace{-0.6cm}
    
    \subsection{Model Editing for LLMs}
        \vspace{-0.6em}
    % LLMs primarily gain world knowledge during the costly pretraining phase through exposure to vast amounts of data **Brown et al., "Language Models as Cultural Beacons"**. Model editing methods are widely studied to keep LLMs up with fast-changing world knowledge without retraining LLMs. Existing model editing methods can be categorized into memory-based, meta-learning, and locate-then-edit methods.
    LLMs acquire most of their world knowledge during the costly pretraining phase by processing vast amounts of data **Brown et al., "Language Models as Cultural Beacons"**. Model editing methods have been extensively studied to keep LLMs updated with fast-changing world knowledge without the need for full retraining. 
    
    These methods can be broadly classified into memory-based, meta-learning, and locate-then-edit approaches.
    \textit{Memory-based methods} update knowledge of LLMs by incorporating an external memory module. For example, **Hao et al., "SERAC: Self-supervised External Memory for Language Models"** introduced SERAC, which uses additional knowledge neurons to update or erase existing knowledge. Moreover, \textit{meta-learning-based methods} utilize hyper-networks to predict weight updates for LLMs, such as KE **Kim et al., "Knowledge Editing with Hyper-Network"** and MEND **Chen et al., "Meta-Learning based Model Editing"**.
    Recent advances in \textit{locate-then-edit methods} have significantly reduced the costs of model editing by leveraging the hypothesis that the Feed Forward Network (FFN) functions as key-value memory **Wang et al., "Rome: Efficient Model Editing via Causal Tracing"**. Specifically, **Hao et al., "ROM: Recursive Operation Memory for Language Models"** proposed ROME, which uses causal tracing to locate knowledge-related layers and perform concise parameter editing, achieving superior performance. Subsequently, **Li et al., "Batch-Wise Knowledge Editing with Rome"** extended the method for large-scale knowledge editing, allowing for batch-wise editing, while **Wang et al., "GNN-based Model Editing for Multi-Hop Inference"** improved performance on multi-hop inference by incorporating GNNs to aggregate relevant knowledge. Inspired by this, we develop the multi-node target estimation method to aggregate relevant acceptance knowledge therefore enhancing generalization of the injected backdoor. %Based on these methods, we developed a model editing method for jailbreak attack, \textit{JailbreakEdit}.
    
   Directly adapting current locate-then-edit model editing methods for LLM jailbreak presents significant challenges. First, while these methods excel at inducing specific outputs, they perform poorly when it comes to generating coherent subsequent contents **Wang et al., "GNN-based Model Editing for Multi-Hop Inference"**. 
   Second, competing objectives complicate both the execution of jailbreak attacks and the defense of LLMs, a challenge that existing editing methods fail to address. To overcome this, rather than mapping the backdoor to a single specific token, we develop a method to create shortcuts from the backdoor to the jailbreak space, thereby enhancing the attack's capability to bypass the victim LLM's internal safety mechanisms.