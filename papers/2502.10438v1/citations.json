[
  {
    "index": 0,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "sun2024trustllm",
        "author": "Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and others",
        "title": "Trustllm: Trustworthiness in large language models"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "ganguli2022red",
        "author": "Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others",
        "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      },
      {
        "key": "shen2023anything",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yu2023gptfuzzer",
        "author": "Yu, Jiahao and Lin, Xingwei and Xing, Xinyu",
        "title": "Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2023autodan",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "Autodan: Generating stealthy jailbreak prompts on aligned large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhao2024survey",
        "author": "Zhao, Shuai and Jia, Meihuizi and Guo, Zhongliang and Gan, Leilei and Fu, Jie and Feng, Yichao and Pan, Fengjun and Tuan, Luu Anh",
        "title": "A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wan2023poisoning",
        "author": "Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan",
        "title": "Poisoning language models during instruction tuning"
      },
      {
        "key": "xu2023instructions",
        "author": "Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao",
        "title": "Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "shi2023badgpt",
        "author": "Shi, Jiawen and Liu, Yixin and Zhou, Pan and Sun, Lichao",
        "title": "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt"
      },
      {
        "key": "rando2023universal",
        "author": "Rando, Javier and Tram{\\`e}r, Florian",
        "title": "Universal jailbreak backdoors from poisoned human feedback"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2024badedit",
        "author": "Li, Yanzhou and Li, Tianlin and Chen, Kangjie and Zhang, Jian and Liu, Shangqing and Wang, Wenhan and Zhang, Tianwei and Liu, Yang",
        "title": "Badedit: Backdooring large language models by model editing"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shi2023badgpt",
        "author": "Shi, Jiawen and Liu, Yixin and Zhou, Pan and Sun, Lichao",
        "title": "Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks to instructgpt"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rando2023universal",
        "author": "Rando, Javier and Tram{\\`e}r, Florian",
        "title": "Universal jailbreak backdoors from poisoned human feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rando2023universal",
        "author": "Rando, Javier and Tram{\\`e}r, Florian",
        "title": "Universal jailbreak backdoors from poisoned human feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chang2024large",
        "author": "Chang, Hoyeon and Park, Jinho and Ye, Seonghyeon and Yang, Sohee and Seo, Youngkyung and Chang, Du-Seong and Seo, Minjoon",
        "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chang2024large",
        "author": "Chang, Hoyeon and Park, Jinho and Ye, Seonghyeon and Yang, Sohee and Seo, Youngkyung and Chang, Du-Seong and Seo, Minjoon",
        "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dai2021knowledge",
        "author": "Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu",
        "title": "Knowledge neurons in pretrained transformers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "de-cao-etal-2021-editing",
        "author": "De Cao, Nicola  and\nAziz, Wilker  and\nTitov, Ivan",
        "title": "Editing Factual Knowledge in Language Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "mitchell2021fast",
        "author": "Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D",
        "title": "Fast model editing at scale"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "geva2020transformer",
        "author": "Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer",
        "title": "Transformer feed-forward layers are key-value memories"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "meng2022locating",
        "author": "Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov",
        "title": "Locating and Editing Factual Associations in {GPT}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "meng2023memit",
        "author": "Kevin Meng and Sen Sharma, Arnab and Alex Andonian and Yonatan Belinkov and David Bau",
        "title": "Mass Editing Memory in a Transformer"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhang2024knowledge",
        "author": "Zhang, Mengqi and Ye, Xiaotian and Liu, Qiang and Ren, Pengjie and Wu, Shu and Chen, Zhumin",
        "title": "Knowledge graph enhanced large language model editing"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhang2024knowledge",
        "author": "Zhang, Mengqi and Ye, Xiaotian and Liu, Qiang and Ren, Pengjie and Wu, Shu and Chen, Zhumin",
        "title": "Knowledge graph enhanced large language model editing"
      }
    ]
  }
]