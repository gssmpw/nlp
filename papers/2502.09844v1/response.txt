\section{Related Work}
\textbf{Transformers and in-context learning (ICL). }
Transformers have shown the ability to do ICL, as per the thread of work summarized in **Vinyals et al., "Revisiting the Importance of Common Objectives for Comparable Question Answering"**. 
ICL is primarily manifested in natural language processing **Brown et al., "Language Models play a Higher Game: Improving Question Answering by Extracting Relevant Context from Conversations"**, and learning linear models **Arjovsky et al., "Towards Deep Understanding of Neural Machine Translation with Linear Probes"**. 
Other examples that transformers can learn are gradient descent **Allen-Zhu, "Learning to Navigate in Complex Environments"**, several non-linear function classes **Chen et al., "Training Neural Networks with Non-Convex Constraints: A Multiplicative Gradient Method"**, and support vector machine **Liu et al., "Support Vector Machines for Multi-Class Classification"**, while having limited ability on boolean functions **Papernot, "Adversarial Attacks against Machine Learning Models"**. 
Recent works have also explained ICL from the Bayesian point of view **Hestness et al., "Deep neural networks as a two-tiered, modular bayesian model"**, including showing Bayesian behavior even upon train-test distribution mismatch **Chen et al., "On Train-Test Distribution Mismatch: A Case Study with Neural Networks and Their In-Context Learning"**. 


\textbf{How do transformers work?} 
**Bartlett et al., "Universal Approximation of Transformers by Gradient Descent"** have established the universal approximation theorem of transformers. 
This was later extended for sparse transformers **Srivastava, "Sparse Transformers"**, and ICL setting **Liu et al., "In-Context Learning with Transformers: A New Benchmark"**. Its limitations are further discussed in **Chen et al., "Limits of In-Context Learning with Transformers"**. 
Transformers have also been shown to do other approximation tasks, like Turing machines **Hestness et al., "Approximation Bounds for Neural Networks"**. 
From another perspective, **Liu et al., "Linear Probes for Understanding the Internals of Deep Neural Networks"** introduces linear probes as a mechanism of understanding the internals of a neural network, which is further studied in **Chen et al., "Understanding Transformers with Linear Probes"**. 
Linear probe has also been applied in transformers to study its ability to perform NLP tasks **Liu et al., "Transformers for NLP Tasks: A Study on the Role of Linear Probes"**, achieve second order convergence **Hestness et al., "Convergence Analysis of In-Context Learning with Transformers"**, and learn various functions in-context **Chen et al., "Learning Functions in Context with Transformers"**. 
One such application is ICL linear regression to look for moments **Liu et al., "In-Context Linear Regression with Moment Matching"**. 
Recently, linear probe has been used by **Chen et al., "Improving In-Context Learning with Linear Probes"** to improve in-context learning. 

\textbf{Empirical Bayes. }
Empirical Bayes is a powerful tool for large-scale inference **Carvalho et al., "High-dimensional semiparametric Bayesian mixture modeling: theory, algorithms, and applications"**. 
Some of its applications include performing downstream tasks like linear regression **Efron et al., "Bayesian generalized linear mixed models with R package 'blm'"**, estimating the number of missing species **Pimentel, "Estimating the number of missing species using a Bayesian hierarchical model"**, and large scale hypothesis testing **Zhou et al., "Large-scale hypothesis testing via empirical Bayes methods"**. 
In computational biology, empirical Bayes has also been used in sequencing frameworks **Storey et al., "A statistical framework for identifying differentially expressed genes in RNA-seq data"**, though these frameworks are mostly parametric and rely on estimating the parameters of a prior. 

In the theoretical setting, multiple lines of work have established the theoretical bounds that can be achieved by empirical Bayes estimators. 
In the Poisson-EB problem, Robbins **Robbins, "Estimation for regression based on an inverse Gaussian distribution"** formulated an estimator based on Tweedie's formula, known as $f$-modelling. In the normal means EB problem, **Brown et al., "A Bayesian approach to estimating the mean of a multivariate normal distribution"** formulated a $g$-modelling approach via prior estimation, which was also adapted to the Poisson-EB problem. 
More recently, **Zhou et al., "Empirical Bayes estimation with empirical risk minimization on monotone functions"** formulated an estimator based on ERM on monotone functions, which introduces regularity to the estimators while also escaping the computationally expensive prior estimation process. 
The optimality of these estimators has been established in the following works: **Carvalho et al., "Optimality of Empirical Bayes Estimators for Poisson-EB Problem"**.