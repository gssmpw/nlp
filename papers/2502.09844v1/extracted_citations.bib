@inproceedings{Rob51,
  title={Asymptotically subminimax solutions of compound statistical decision problems},
  author={Robbins, Herbert},
  booktitle={Proceedings of the second Berkeley symposium on mathematical statistics and probability},
  pages={131--149},
  year={1951},
  organization={University of California Press}
}

@inproceedings{Rob56,
  title={An {E}mpirical {B}ayes {A}pproach to {S}tatistics},
  author={Robbins, Herbert},
  booktitle={Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  year={1956},
  organization={The Regents of the University of California}
}

@inproceedings{abbas2024enhancing,
  title={Enhancing in-context learning via linear probe calibration},
  author={Abbas, Momin and Zhou, Yi and Ram, Parikshit and Baracaldo, Nathalie and Samulowitz, Horst and Salonidis, Theodoros and Chen, Tianyi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={307--315},
  year={2024},
  organization={PMLR}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{alain2018understanding,
  title={Understanding Intermediate Layers Using Linear Classifier Probes. 2018},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2018}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{bhattamishra2023understanding,
  title={Understanding in-context learning in transformers and llms by learning to learn discrete functions},
  author={Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  journal={arXiv preprint arXiv:2310.03016},
  year={2023}
}

@article{brown2013poisson,
  title={The {P}oisson compound decision problem revisited},
  author={Brown, Lawrence D and Greenshtein, Eitan and Ritov, Ya’acov},
  journal={Journal of the American Statistical Association},
  volume={108},
  number={502},
  pages={741--749},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dai2022can,
  title={Why can {G}{P}{T} learn in-context? {L}anguage models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{efron2001empirical,
  title={Empirical Bayes analysis of a microarray experiment},
  author={Efron, Bradley and Tibshirani, Robert and Storey, John D and Tusher, Virginia},
  journal={Journal of the American statistical association},
  volume={96},
  number={456},
  pages={1151--1160},
  year={2001},
  publisher={Taylor \& Francis}
}

@book{efron2012large,
  title={Large-scale inference: empirical Bayes methods for estimation, testing, and prediction},
  author={Efron, Bradley},
  volume={1},
  year={2012},
  publisher={Cambridge University Press}
}

@article{fisher1943relation,
  title={The relation between the number of species and the number of individuals in a random sample of an animal population},
  author={Fisher, Ronald A and Corbet, A Steven and Williams, Carrington B},
  journal={The Journal of Animal Ecology},
  pages={42--58},
  year={1943},
  publisher={JSTOR}
}

@inproceedings{fu2024transformers,
  title={Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{furuya2024transformers,
  title={Transformers are {U}niversal {I}n-context {L}earners},
  author={Furuya, Takashi and de Hoop, Maarten V. and Peyré, Gabriel},
  journal={arXiv preprint arXiv:2408.01367},
  year={2024}
}

@article{garg2022can,
  title={What can transformers learn in-context? {A} case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{guo2023transformers,
  title={How do transformers learn in-context beyond simple functions? {A} case study on learning with representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  journal={arXiv preprint arXiv:2310.10616},
  year={2023}
}

@article{hardcastle2010bayseq,
  title={bay{S}eq: {E}mpirical {B}ayesian methods for identifying differential expression in sequence count data},
  author={Hardcastle, Thomas J and Kelly, Krystyna A},
  journal={BMC bioinformatics},
  volume={11},
  pages={1--14},
  year={2010},
  publisher={Springer}
}

@article{jana2022optimal,
  title={Optimal empirical {B}ayes estimation for the {P}oisson model via minimum-distance methods},
  author={Jana, Soham and Polyanskiy, Yury and Wu, Yihong},
  journal={arXiv preprint arXiv:2209.01328},
  year={2022}
}

@inproceedings{jana2023empirical,
  title={Empirical {B}ayes via {E}{R}{M} and Rademacher complexities: the Poisson model},
  author={Jana, Soham and Polyanskiy, Yury and Teh, Anzo Z and Wu, Yihong},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={5199--5235},
  year={2023},
  organization={PMLR}
}

@article{jiang2009general,
  title={General Maximum Likelihood Empirical Bayes Estimation Of Normal Means},
  author={Jiang, Wenhua and Zhang, Cun-Hui},
  journal={The Annals of Statistics},
  volume={37},
  number={4},
  pages={1647--1684},
  year={2009},
  publisher={Citeseer}
}

@article{kim2024flexible,
  title={A flexible empirical {B}ayes approach to multiple linear regression and connections with penalized regression},
  author={Kim, Youngseok and Wang, Wei and Carbonetto, Peter and Stephens, Matthew},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={185},
  pages={1--59},
  year={2024}
}

@article{leng2013ebseq,
  title={EBSeq: {A}n empirical Bayes hierarchical model for inference in RNA-seq experiments},
  author={Leng, Ning and Dawson, John A and Thomson, James A and Ruotti, Victor and Rissman, Anna I and Smits, Bart MG and Haag, Jill D and Gould, Michael N and Stewart, Ron M and Kendziorski, Christina},
  journal={Bioinformatics},
  volume={29},
  number={8},
  pages={1035--1043},
  year={2013},
  publisher={Oxford University Press}
}

@article{mukherjee2023mean,
  title={A mean field approach to empirical {B}ayes estimation in high-dimensional linear regression},
  author={Mukherjee, Sumit and Sen, Bodhisattva and Sen, Subhabrata},
  journal={arXiv preprint arXiv:2309.16843},
  year={2023}
}

@article{muller2024bayes,
  title={Bayes' Power for Explaining In-Context Learning Generalizations},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Hutter, Frank},
  journal={arXiv preprint arXiv:2410.01565},
  year={2024}
}

@article{nath2024transformers,
  title={Transformers are Expressive, But Are They Expressive Enough for Regression?},
  author={Nath, Swaroop and Khadilkar, Harshad and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:2402.15478},
  year={2024}
}

@article{panwar2023context,
  title={In-context learning through the bayesian prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2306.04891},
  year={2023}
}

@article{perez2021attention,
  title={Attention is turing-complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@article{polyanskiy2021sharp,
  title={Sharp regret bounds for empirical Bayes and compound decision problems},
  author={Polyanskiy, Yury and Wu, Yihong},
  journal={arXiv preprint arXiv:2109.03943},
  year={2021}
}

@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}

@article{tenney2019bert,
  title={B{E}{R}{T} rediscovers the classical {N}{L}{P} pipeline},
  author={Tenney, Ian and Dipanjan, Das and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: {A} case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}

@article{yun2020n,
  title={O(n) connections are expressive enough: Universal approximability of sparse transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13783--13794},
  year={2020}
}

@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

