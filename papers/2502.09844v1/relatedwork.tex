\section{Related Work}
\textbf{Transformers and in-context learning (ICL). }
Transformers have shown the ability to do ICL, as per the thread of work summarized in 
\cite{dong2022survey}. 
ICL is primarily manifested in natural language processing \cite{brown2020language, dai2022can} 
and learning linear models \cite{akyurek2022learning, zhang2023trained}. 
Other examples that transformers can learn are gradient descent \cite{bai2024transformers}, 
several non-linear function classes \cite{garg2022can}, and support vector machine \cite{tarzanagh2023transformers}, 
while having limited ability on boolean functions \cite{bhattamishra2023understanding}. 
Recent works have also explained ICL from the Bayesian point of view 
\cite{muller2024bayes, panwar2023context}, including showing Bayesian behavior even upon train-test distribution mismatch \cite{xie2021explanation}. 


\textbf{How do transformers work?} 
\cite{yun2019transformers} have established the universal approximation theorem of transformers. 
This was later extended for sparse transformers \cite{yun2020n} and ICL setting \cite{furuya2024transformers}. Its limitations are further discussed in \cite{nath2024transformers}. 
Transformers have also been shown to do other approximation tasks, like Turing machines 
\cite{wei2022statistically, perez2021attention}. 
From another perspective, 
\cite{alain2018understanding} introduces linear probes as a mechanism of understanding the internals of a neural network, which is further studied in \cite{belinkov2022probing}. 
Linear probe has also been applied in transformers to study its ability to perform NLP tasks \cite{tenney2019bert}, achieve second order convergence \cite{fu2024transformers}, and learn various functions in-context \cite{guo2023transformers}. 
One such application is ICL linear regression to look for moments \cite{akyurek2022learning}. 
Recently, linear probe has been used by \cite{abbas2024enhancing} to improve in-context learning. 

\textbf{Empirical Bayes. }
Empirical Bayes is a powerful tool for large-scale inference \cite{efron2012large}. 
Some of its applications include performing downstream tasks like linear regression 
\cite{kim2024flexible, mukherjee2023mean}, estimating the number of missing species \cite{fisher1943relation}, 
and large scale hypothesis testing \cite{efron2001empirical}. 
In computational biology, empirical Bayes has also been used in sequencing frameworks 
\cite{hardcastle2010bayseq, leng2013ebseq}, 
though these frameworks are mostly parametric and rely on estimating the parameters of a prior. 

In the theoretical setting, multiple lines of work have established the theoretical bounds that can be achieved by empirical Bayes estimators. 
In the Poisson-EB problem, Robbins \cite{Rob51, Rob56} formulated an estimator based on Tweedie's formula, known as $f$-modelling. In the normal means EB problem, \cite{jiang2009general} formulated a $g$-modelling approach via prior estimation, which was also adapted to the Poisson-EB problem. 
More recently, \cite{jana2023empirical} formulated an estimator based on ERM on monotone functions, 
which introduces regularity to the estimators while also escaping the computationally expensive prior estimation process. 
The optimality of these estimators has been established in the following works: 
\cite{brown2013poisson, polyanskiy2021sharp, jana2022optimal, jana2023empirical}.