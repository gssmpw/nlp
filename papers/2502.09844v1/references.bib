@book{luce1959individual,
  title={Individual choice behavior},
  author={Luce, R Duncan},
  volume={4},
  year={1959},
  publisher={Wiley New York}
}

@article{plackett1975analysis,
  title={The analysis of permutations},
  author={Plackett, Robin L},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={24},
  number={2},
  pages={193--202},
  year={1975},
  publisher={Oxford University Press}
}

@article{guliyev2018approximation,
  title={On the approximation by single hidden layer feedforward neural networks with fixed weights},
  author={Guliyev, Namig J and Ismailov, Vugar E},
  journal={Neural Networks},
  volume={98},
  pages={296--304},
  year={2018},
  publisher={Elsevier}
}

@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}


@InProceedings{pmlr-v125-kidger20a,
  title = 	 {{Universal Approximation with {D}eep {N}arrow {N}etworks}},
  author =       {Kidger, Patrick and Lyons, Terry},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2306--2327},
  year = 	 {2020},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
}

@article{park2020minimum,
  title={Minimum width for universal approximation},
  author={Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2006.08859},
  year={2020}
}

@article{furuya2024transformers,
  title={Transformers are {U}niversal {I}n-context {L}earners},
  author={Furuya, Takashi and de Hoop, Maarten V. and Peyré, Gabriel},
  journal={arXiv preprint arXiv:2408.01367},
  year={2024}
}

@article{jana2022optimal,
  title={Optimal empirical {B}ayes estimation for the {P}oisson model via minimum-distance methods},
  author={Jana, Soham and Polyanskiy, Yury and Wu, Yihong},
  journal={arXiv preprint arXiv:2209.01328},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{jana2023empirical,
  title={Empirical {B}ayes via {E}{R}{M} and Rademacher complexities: the Poisson model},
  author={Jana, Soham and Polyanskiy, Yury and Teh, Anzo Z and Wu, Yihong},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={5199--5235},
  year={2023},
  organization={PMLR}
}

@article{brown2013poisson,
  title={The {P}oisson compound decision problem revisited},
  author={Brown, Lawrence D and Greenshtein, Eitan and Ritov, Ya’acov},
  journal={Journal of the American Statistical Association},
  volume={108},
  number={502},
  pages={741--749},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{zhu2015aligning,
  title={Aligning {B}ooks and {M}ovies: {T}owards {S}tory-like {V}isual {E}xplanations by {W}atching {M}ovies and {R}eading {B}ooks},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  journal={arXiv preprint arXiv:1506.06724},
  year={2015}
}

@inproceedings{hao2020optimal,
 author = {Hao, Yi and Li, Ping},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {8553--8564},
 publisher = {Curran Associates, Inc.},
 title = {Optimal {P}rediction of the {N}umber of {U}nseen {S}pecies with {M}ultiplicity},
 volume = {33},
 year = {2020}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{Rob56,
  title={An {E}mpirical {B}ayes {A}pproach to {S}tatistics},
  author={Robbins, Herbert},
  booktitle={Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  year={1956},
  organization={The Regents of the University of California}
}

@inproceedings{Rob51,
  title={Asymptotically subminimax solutions of compound statistical decision problems},
  author={Robbins, Herbert},
  booktitle={Proceedings of the second Berkeley symposium on mathematical statistics and probability},
  pages={131--149},
  year={1951},
  organization={University of California Press}
}

@article{augustine2024survey,
  title={A survey on universal approximation theorems},
  author={Augustine, Midhun T},
  journal={arXiv preprint arXiv:2407.12895},
  year={2024}
}

@article{polyanskiy2021sharp,
  title={Sharp regret bounds for empirical Bayes and compound decision problems},
  author={Polyanskiy, Yury and Wu, Yihong},
  journal={arXiv preprint arXiv:2109.03943},
  year={2021}
}

@article{jiang2009general,
  title={General Maximum Likelihood Empirical Bayes Estimation Of Normal Means},
  author={Jiang, Wenhua and Zhang, Cun-Hui},
  journal={The Annals of Statistics},
  volume={37},
  number={4},
  pages={1647--1684},
  year={2009},
  publisher={Citeseer}
}

@article{kim2024flexible,
  title={A flexible empirical {B}ayes approach to multiple linear regression and connections with penalized regression},
  author={Kim, Youngseok and Wang, Wei and Carbonetto, Peter and Stephens, Matthew},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={185},
  pages={1--59},
  year={2024}
}

@article{mukherjee2023mean,
  title={A mean field approach to empirical {B}ayes estimation in high-dimensional linear regression},
  author={Mukherjee, Sumit and Sen, Bodhisattva and Sen, Subhabrata},
  journal={arXiv preprint arXiv:2309.16843},
  year={2023}
}

@book{efron2012large,
  title={Large-scale inference: empirical Bayes methods for estimation, testing, and prediction},
  author={Efron, Bradley},
  volume={1},
  year={2012},
  publisher={Cambridge University Press}
}

@article{fisher1943relation,
  title={The relation between the number of species and the number of individuals in a random sample of an animal population},
  author={Fisher, Ronald A and Corbet, A Steven and Williams, Carrington B},
  journal={The Journal of Animal Ecology},
  pages={42--58},
  year={1943},
  publisher={JSTOR}
}

@article{efron2001empirical,
  title={Empirical Bayes analysis of a microarray experiment},
  author={Efron, Bradley and Tibshirani, Robert and Storey, John D and Tusher, Virginia},
  journal={Journal of the American statistical association},
  volume={96},
  number={456},
  pages={1151--1160},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{hardcastle2010bayseq,
  title={bay{S}eq: {E}mpirical {B}ayesian methods for identifying differential expression in sequence count data},
  author={Hardcastle, Thomas J and Kelly, Krystyna A},
  journal={BMC bioinformatics},
  volume={11},
  pages={1--14},
  year={2010},
  publisher={Springer}
}

@article{leng2013ebseq,
  title={EBSeq: {A}n empirical Bayes hierarchical model for inference in RNA-seq experiments},
  author={Leng, Ning and Dawson, John A and Thomson, James A and Ruotti, Victor and Rissman, Anna I and Smits, Bart MG and Haag, Jill D and Gould, Michael N and Stewart, Ron M and Kendziorski, Christina},
  journal={Bioinformatics},
  volume={29},
  number={8},
  pages={1035--1043},
  year={2013},
  publisher={Oxford University Press}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{nath2024transformers,
  title={Transformers are Expressive, But Are They Expressive Enough for Regression?},
  author={Nath, Swaroop and Khadilkar, Harshad and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:2402.15478},
  year={2024}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? {I}nvestigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{garg2022can,
  title={What can transformers learn in-context? {A} case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}

@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dai2022can,
  title={Why can {G}{P}{T} learn in-context? {L}anguage models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{lindsay1983geometry,
  title={The geometry of mixture likelihoods: a general theory},
  author={Lindsay, Bruce G},
  journal={The annals of statistics},
  pages={86--94},
  year={1983},
  publisher={JSTOR}
}

@article{alain2018understanding,
  title={Understanding Intermediate Layers Using Linear Classifier Probes. 2018},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2018}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  number={1},
  pages={12},
  year={2021}
}

@article{snell2021approximating,
  title={Approximating how single head attention learns},
  author={Snell, Charlie and Zhong, Ruiqi and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.07601},
  year={2021}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@inproceedings{lepori2023uncovering,
  title={Uncovering intermediate variables in transformers using circuit probing},
  author={Lepori, Michael A and Serre, Thomas and Pavlick, Ellie},
  booktitle={First Conference on Language Modeling},
  year={2023}
}

@inproceedings{abbas2024enhancing,
  title={Enhancing in-context learning via linear probe calibration},
  author={Abbas, Momin and Zhou, Yi and Ram, Parikshit and Baracaldo, Nathalie and Samulowitz, Horst and Salonidis, Theodoros and Chen, Tianyi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={307--315},
  year={2024},
  organization={PMLR}
}

@article{bhattamishra2023understanding,
  title={Understanding in-context learning in transformers and llms by learning to learn discrete functions},
  author={Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  journal={arXiv preprint arXiv:2310.03016},
  year={2023}
}

@article{yun2020n,
  title={O(n) connections are expressive enough: Universal approximability of sparse transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13783--13794},
  year={2020}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: {A} case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}

@article{perez2021attention,
  title={Attention is turing-complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@article{guo2023transformers,
  title={How do transformers learn in-context beyond simple functions? {A} case study on learning with representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  journal={arXiv preprint arXiv:2310.10616},
  year={2023}
}

@article{tenney2019bert,
  title={B{E}{R}{T} rediscovers the classical {N}{L}{P} pipeline},
  author={Tenney, Ian and Dipanjan, Das and Pavlick, Ellie},
  journal={arXiv preprint arXiv:1905.05950},
  year={2019}
}

@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@article{efron2014two,
  title={Two modeling strategies for empirical Bayes estimation},
  author={Efron, Bradley},
  journal={Statistical science: a review journal of the Institute of Mathematical Statistics},
  volume={29},
  number={2},
  pages={285},
  year={2014},
  publisher={NIH Public Access}
}

@article{polyanskiy2020self,
  title={Self-regularizing property of nonparametric maximum likelihood estimator in mixture models},
  author={Polyanskiy, Yury and Wu, Yihong},
  journal={arXiv preprint arXiv:2008.08244},
  year={2020}
}

@inproceedings{fu2024transformers,
  title={Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{razzhigaev2024your,
  title={Your Transformer is Secretly Linear},
  author={Razzhigaev, Anton and Mikhalchuk, Matvey and Goncharova, Elizaveta and Gerasimenko, Nikolai and Oseledets, Ivan and Dimitrov, Denis and Kuznetsov, Andrey},
  journal={arXiv preprint arXiv:2405.12250},
  year={2024}
}

@article{houwelingen1983monotone,
  title={Monotone empirical Bayes estimators for the continuous one-parameter exponential family},
  author={Houwelingen, JC van and Stijnen, Th},
  journal={Statistica Neerlandica},
  volume={37},
  number={1},
  pages={29--43},
  year={1983},
  publisher={Wiley Online Library}
}

@book{efron2021computer,
  title={Computer age statistical inference, student edition: algorithms, evidence, and data science},
  author={Efron, Bradley and Hastie, Trevor},
  volume={6},
  year={2021},
  publisher={Cambridge University Press}
}

@article{muller2024bayes,
  title={Bayes' Power for Explaining In-Context Learning Generalizations},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Hutter, Frank},
  journal={arXiv preprint arXiv:2410.01565},
  year={2024}
}

@article{panwar2023context,
  title={In-context learning through the bayesian prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2306.04891},
  year={2023}
}

@article{koenker2024empirical,
  title={Empirical Bayes for the Reluctant Frequentist},
  author={Koenker, Roger and Gu, Jiaying},
  journal={arXiv preprint arXiv:2404.03422},
  year={2024}
}

@article{gu2023invidious,
  title={Invidious comparisons: Ranking and selection as compound decisions},
  author={Gu, Jiaying and Koenker, Roger},
  journal={Econometrica},
  volume={91},
  number={1},
  pages={1--41},
  year={2023},
  publisher={Wiley Online Library}
}

@article{gu2022nonparametric,
  title={Nonparametric maximum likelihood methods for binary response models with random coefficients},
  author={Gu, Jiaying and Koenker, Roger},
  journal={Journal of the American Statistical Association},
  volume={117},
  number={538},
  pages={732--751},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{zhou2024transformers,
  title={Transformers can achieve length generalization but not robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

@article{wang2024length,
  title={Length Generalization of Causal Transformers without Position Encoding},
  author={Wang, Jie and Ji, Tao and Wu, Yuanbin and Yan, Hang and Gui, Tao and Zhang, Qi and Huang, Xuanjing and Wang, Xiaoling},
  journal={arXiv preprint arXiv:2404.12224},
  year={2024}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{geshkovski2024emergence,
  title={The emergence of clusters in self-attention dynamics},
  author={Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@misc{HockeyReference,
  author = {{Hockey-Reference}},
  title = {{Hockey-Reference.com}},
  year = {2000},
  url = {https://www.hockey-reference.com/},
  note = {Accessed: 2024-09-30}
}

@misc{Retrosheet,
  author = {{Retrosheet}},
  title = {{Retrosheet Game Logs}},
  year = {1996},
  url = {https://www.retrosheet.org/game.htm},
  note = {Accessed: 2024-10-25}
}

@misc{EstiniRetrosheet,
  author = {Cal Estini},
  title = {{Retrosheet Repository on GitHub}},
  year = {2018},
  url = {https://github.com/calestini/retrosheet},
  note = {Accessed: 2024-10-25}
}

@misc{BookCorpusOpen,
  author = {Luca Diliello},
  title = {{BookCorpusOpen Dataset on Hugging Face}},
  url = {https://huggingface.co/datasets/lucadiliello/bookcorpusopen},
  year = {2024},
  note = {Accessed: 2024-11-09}
}

