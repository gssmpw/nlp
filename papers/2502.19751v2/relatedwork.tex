\section{Related Works}
\label{relatedworks}

\subsection{Offline Cross-modal Hashing}
Offline cross-modal hashing methods attempt to train hash codes and functions with full database in a batch-based manner, which suffers from the limitation that cannot handle the real time data chunks \cite{huang2023two}.
For example, deep cross-modal hashing (DCMH) adopts the end-to-end strategy to jointly learn hash codes and functions for coss-modal data \cite{DCMH}.
Self-supervised adversarial hashing (SSAH) leverages two adversarial-based models to maximize correlations on semantic between modalities \cite{SSAH}.
Unsupervised knowledge distillation (UKD) hashing constructs similarity matrix in the unsupervised teacher network for guiding the hash codes and functions learning in the supervised student network \cite{UKD}.
Deep cross-modal hashing with hash functions and unified hash code jointly learning (DCUCH) generates hash codes and learns hash functions asymmetrically \cite{DCHUC}.
Deep adaptive enhanced hashing (DAEH) uses adaptive optimization method and discriminative similarity measurement to learn hash functions \cite{DAEH}.
Semi supervised knowledge distillation for cross-modal hashing (SKDCH) learns similarity as knowledge in a semi-supervised method in teacher network to guide the supervised cross-modal hashing in student network \cite{SKDCH}.
Coding self-representative and label-relaxed hashing (CSLRH) enhance the class-specific feature representations and the semantic discriminability, while reduce the noise interference \cite{jiang2024coding}.
CLIP based knowledge distillation hashing (CKDH) leverages a knowledge distillation framework to transfer the deep semantic similarity learned by CLIP to the lightweight corss-modal hashing in the student network \cite{CKDH}. 


\subsection{Online Cross-modal Hashing}
Unlike offline cross-modal hashing methods, online cross-modal hashing methods train hash codes and functions in a real-time updating manner, where hash functions are updated by hash codes trained by the continually coming data chunks.
For example, hadamard code book based online hashing (HCOH) leverages hadamard code book to generate hadamard matrix for online hash codes and functions learning \cite{HCOH}.
Online latent semantic hashing (OLSH) maps discrete labels to a continuous latent semantic space which is bridged with binary codes for learning hash codes \cite{OLSH}.
Online collective matrix factorization hashing (OCMFH) uses the collective matrix factorization method to decompose new data chunk's features and existing features into a common latent space that is generated by the features and hash function simultaneously \cite{OCMFH}.
Label embedding online hashing (LEMON) leverages similarity factorization hashing method for addressing the online hashing retrieval problem, by reconstructing the similarity matrices using label information for newly coming online data chunks \cite{LEMON}.
Supervised discrete online hashing (SDOH) embeds semantic label into the common latent space for parallel calculating the common representation of the newly coming data from multiple modalities, and then generates hash codes by the continuous common latent representation \cite{SDOH}.
Random online hashing (ROH) proposes a linear bridging strategy to simplify the similarity factorization problem into a linear optimization problem, and then proposes a MED embedding methods to learn features and preserve significant semantic information into hash codes \cite{ROH}.