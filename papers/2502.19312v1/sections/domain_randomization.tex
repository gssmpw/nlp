
\vspace{-0.15cm}
\section{Sim2Real: Synthetic Preference Data Transfers to Real Users}
\vspace{-0.15cm}
\label{sec:dom_rand}

Collecting personalized data at scale presents significant challenges, primarily due to the high cost and inherent unreliability of human annotation. Curating a diverse set of users to capture the full spectrum of real-world variability further complicates the process, often limiting the scope and representativeness of the data. Synthetically generating data using a language model~\citep{li2024syntheticdataalmostscratch,bai2022constitutionalaiharmlessnessai} is a promising alternative, since it can both reduce costly human data generation and annotation and streamline the data curation process. Can we generate diverse user preference data using language models in a way that transfers to real people?

We draw inspiration from simulation-to-real transfer in non-language domains like robotics~\citep{makoviychuk2021isaacgymhighperformance} and self-driving cars~\citep{yang2023unisimneuralclosedloopsensor}, where the idea of domain randomization~\citep{tobin2018domainrandomizationgenerativemodels} has been particularly useful in enabling transfer to real environments. Domain randomization enables efficient adaptation to novel test scenarios by training models in numerous simulated environments with varied, randomized properties.

But why is this relevant to personalization? As mentioned previously, each user can be viewed as a different ``environment'' to simulate as each user has a unique reward function that is represented by their preferences. To ensure models trained on synthetic data generalize to real human users, we employ domain randomization to simulate a diverse set of synthetic preferences. However, diversity alone isn't sufficient to learn a personalized LM. As studied in prior work~\citep{hsu2019unsupervised,yin2019meta}, it is crucial that the task distribution in meta-learning exhibits sufficient structure to rule out learning shortcuts that do not generalize. But how can we elicit both \textbf{diversity} and \textbf{structure} in our preference datasets?


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/domain_randomization.pdf}
    \caption{\footnotesize\textbf{Overview of Domain Randomization Techniques.} View-Conditioning (left) decomposes a given question into multiple viewpoints, allowing for diverse response generation. Iterative Persona Generation (right) allows for better structure by removing underspecification of the persona by iteratively refining a persona if it is insufficient to make a preference prediction.}
    \label{fig:dom-rand}
    \vspace{-0.6cm}
\end{figure*}

\noindent \textbf{Encouraging diversity.}
Diversity of data is crucial to learning a reward function that generalizes across prompts. Each domain has a slightly different generation setup as described in Section \ref{sec:domains}, but there are some general design decisions that are shared across all tasks to ensure diversity.

One source of diversity is in the questions used in the preferences. We use a variety of strategies to procure questions for the three tasks. For question selection for ELIX, we first sourced questions from human writers and then synthetically augmented the set of questions by prompting GPT-4o~\citep{openai2024gpt4ocard} with subsets of these human-generated questions. This allows us to scalably augment the human question dataset, while preserving the stylistic choices and beliefs of human writers. For the reviews dataset, we compiled a list of popular media from sites such as Goodreads, IMDb, and MyAnimeList. For the Roleplay dataset, we prompted GPT-4o to generate questions all users would ask (global) or questions only people with a specific trait would ask (specific). This allows us to have questions that are more consistent with the distribution of questions people may ask.

Additionally, having a diversity of responses is crucial for not only training the model on many viewpoints but also reward labeling, allowing for greater support over the set of possible responses for a question. To achieve diverse responses, we employ two strategies: Persona Steering~\citep{cheng-etal-2023-marked} and view conditioning. For ELIX and Reviews, we use persona steering by prompting the model with a question and asking it to generate an answer for a randomly selected persona. For Roleplay, the user description was often underspecified so responses generated with persona steering were similar. Therefore, we considered a multi-turn approach to generating a response. First, we asked the model to generate different viewpoints that may be possible for a question. Then, conditioned on each viewpoint independently, we prompted the model with the question and the viewpoint and asked it to answer the question adhering to the viewpoint presented. For example, if you consider the question, "How can I learn to cook a delicious meal?", one viewpoint here could be "watching a youtube video", better suited for a younger, more tech savvy individual, whereas viewpoints such as "using a recipe book" or "taking a cooking class" may be better for an older population or those who would have the time or money to spend on a cooking class. This allowed for more diversity in the responses and resulting preferences. 

Finally, we sampled responses from an ensemble of models with a high temperature, including those larger than the base model we fine-tuned such as Llama 3.3 70b~\citep{grattafiori2024llama} and Gemma 2 27b~\citep{gemmateam2024gemma2improvingopen}, allowing for better instruction following abilities of the fine-tuned model, than the Llama 3.2 3B we fine-tune.

\noindent \textbf{Encouraging task structure.}
Meta-learning leverages a shared latent structure across tasks to adapt to a new task quickly. The structure can be considered as similar feature representations, function families, or transition dynamics that the meta-learning algorithm can discover and leverage. For a preference dataset, this structure can be represented as the distribution of preferences across different users and is controlled by the scoring function and the distribution of responses.

One thing we controlled to enable better structure is the scoring function used to generate synthetic preferences. Firstly, we wanted to ensure consistent preference labeling. We use AI Feedback~\citep{bai2022constitutionalaiharmlessnessai} to construct this, using relative pairwise feedback for preference labels, akin to AlpacaEval~\citep{dubois2024alpacafarm}, as an alternative to absolute rubric based scoring, which we found to be noisy and inaccurate. The preference label along with being conditioned on the prompt, response, and general guidance on scoring, is now also conditioned on the scoring user description and additional scoring guidelines for user-aware preference labeling. Additionally, due to context length constraints, many responses for our preference dataset are shorter than the instruct model that we fine-tune from. Therefore, we prompt the model to ignore this bias. Furthermore, we provide each preference example to the model twice, flipping the order of the responses, and keeping filtering out responses that are not robust to order bias for both training and evaluation (win rates).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/persona_roleplay_flowchart_v2.JPG}
    \vspace{-0.4cm}
    \caption{\footnotesize \textbf{Flowchart of Roleplay dataset generation:} Starting from a set of traits, a seed persona is constructed and a set of specific questions about that trait. Then responses are constructed with View-Conditioning. The seed personas are then iteratively refined to not be underspecified. Finally, the refined persona is used to score consistent preferences.}
    \label{fig:flowchart}
    \vspace{-0.2cm}
\end{figure}


\begin{wrapfigure}{r}{0.4\textwidth} % 'r' for right, 'l' for left
    \centering
    \includegraphics[width=0.38\textwidth]{figures/disagreement.png}
    \vspace{-0.4cm}
    \caption{\footnotesize\textbf{Disagreement Matrix across 5 users in Roleplay.} Here we plot the disagreement of preferences for 5 users. There is a mix of users with high and low disagreement.}
    \label{fig:disagreement}
    \vspace{-0.2cm}
\end{wrapfigure}
Additionally, as mentioned above, in some cases, such as with the Roleplay dataset, the user description is underspecified, leading to challenges in labeling consistent preferences. For example, if a user description does not have information about dietary preferences, inconsistency may arise for labeling preferences about that topic. For instance, in one preference pair, vegan cake recipes may be preferred but in another, steakhouses are preferred for date night. To fix this, we take an iterative process to constructing user descriptions. Firstly, we start with a seed set of user descriptions generated from the trait attributes. After generating questions and responses based on these seed descriptions, we take a set of question and response pairs. For each pair, we iteratively refine the user description by prompting a model like GPT4-o to either label the preference pair or if the user description is insufficient, to randomly choose a preference and append information to the description so a future scorer would make the same decision. Finally, we utilize the updated user description to relabel preferences for the set of questions and responses allocated to that user with the labeling scheme above. This fix for underspecification also helps the COT prediction as predicting an underspecified user persona, can lead to ambiguous generated descriptions.

Finally, we desire structured relationships between users. To ensure this, we analyzed the disagreement (average difference of preference labels) of user's preferences across prompts to understand where users agreed and disagreed, and regenerated data if this disagreement was too high across users. By having users with some overlap, meta-learning algorithms can learn how to transfer knowledge effectively from one user to another. A sample disagreement plot for a subset of users in the Roleplay task can be found in Figure~\ref{fig:disagreement}. We outline our full dataset generation process in Figure~\ref{fig:flowchart} in the Roleplay Task, starting from just a simple set of demographic traits.

\begin{AIbox}{Takeaways from Sim2Real and Domain Randomization}
Since collecting personalized data at scale is challenging, we propose instead to generate diverse synthetic preference datasets that can be transferred to real humans. We study two design decisions to effectively encourage this transfer: (1) \textbf{Encouraging Diversity} and (2) \textbf{Structured Task Construction} and discuss approaches to instantiate these design choices.
\end{AIbox}