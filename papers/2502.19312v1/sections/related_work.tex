\vspace{-0.15cm}
\section{Related Work}
\vspace{-0.15cm}

\noindent \textbf{Personalized learning of preferences.}
Prior research has explored personalization through various methods. One approach is distributional alignment, which focuses on matching model outputs to broad target distributions rather than tailoring them to individual user preferences. For example, some prior work have concentrated on aligning model-generated distributions with desired statistical properties~\citep{siththaranjan2024distributionalpreferencelearningunderstanding,meister2024benchmarkingdistributionalalignmentlarge, melnyk2024distributionalpreferencealignmentllms}, yet they do not explicitly optimize for individual preference adaptation. Another strategy involves explicitly modeling a distribution of rewards~\citep{lee2024testtimealignmenthypothesisreweighting, poddar2024personalizingreinforcementlearninghuman}. However, these methods suffer from sample inefficiency during both training and inference~\citep{rafailov2023direct,2023arXiv231012036G}. Additionally, these approaches have limited evaluations: \citet{lee2024testtimealignmenthypothesisreweighting} focuses solely on reward modeling, while \citet{poddar2024personalizingreinforcementlearninghuman} tests with a very limited number of artificial users (e.g helpfulness user and honest user). Other works have investigated personalization in multiple-choice questions, such as GPO \citep{zhao2024grouppreferenceoptimizationfewshot}. Although effective in structured survey settings, these methods have not been validated for open-ended personalization tasks. Similarly, \citet{shaikh2024showdonttellaligning} explores personalization via explicit human corrections, but relying on such corrections is expensive and often impractical to scale.  Finally, several datasets exist for personalization, such as Prism~\citep{kirk2024prismalignmentdatasetparticipatory} and Persona Bench \citep{castricato2024personareproducibletestbedpluralistic}. Neither of these datasets demonstrate that policies trained on these benchmarks lead to effective personalization. Unlike these prior works which study personalization based off of human values and controversial questions, we instead study more general questions that a user may ask.

\noindent \textbf{Algorithms for preference learning.}
LLMs are typically fine-tuned via supervised next-token prediction on high-quality responses and later refined with human preference data~\citep{casper2023open,ouyang2022training}. This process can use on-policy reinforcement learning methods like REINFORCE~\citep{NIPS1999_464d828b} or PPO~\citep{2017arXiv170706347S}, which optimize a reward model with a KL constraint. Alternatively, supervised fine-tuning may be applied to a curated subset of preferred responses~\citep{dubois2024alpacafarm} or iteratively to preferred completions as in ReST~\citep{gulcehre2023reinforced}. Other methods, such as DPO~\citep{rafailov2023direct}, IPO~\citep{2023arXiv231012036G}, and KTO~\citep{HALOs2024}, learn directly from human preferences without an explicit reward model, with recent work exploring iterative preference modeling applications~\citep{2024arXiv240110020Y}.

\noindent \textbf{Black-box meta-learning.}
\methodname{} is an instance of black-box meta-learning, which has been studied in a wide range of domains spanning image classification~\citep{santoro2016oneshotlearningmemoryaugmentedneural, mishra2018simpleneuralattentivemetalearner}, language modeling~\citep{chen2022metalearninglanguagemodelincontext, min2022metaicllearninglearncontext, yu2024metamathbootstrapmathematicalquestions}, and reinforcement learning~\citep{duan2016rl, wang2016learning}. Black-box meta-learning is characterized by the processing of task contexts and queries using generic sequence operations like recurrence or self-attention, instead of specifically designed adaptation mechanisms.


