\vspace{-0.15cm}
\section{Experimental Evaluation}
\vspace{-0.15cm}

\begin{table*}[t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Method} & \textbf{Winrate (\%)} \\
            \hline
            Base (Llama 3.2 3B instruct) & 50.0 \\
            IPO & 72.4 \\
            Few-shot Prompting & 63.2 \\
            Few-shot Pref-FT & 62.8 \\
            \methodname\ (ours) & \textbf{82.6} \\
            \methodname\ + COT (ours) & \textbf{90.3} \\
            Oracle (prompt w/ g.t. persona) & 90.9 \\
            \hline
        \end{tabular}
        \vspace{-0.2cm}
        \caption{\footnotesize Automatic Winrates on Roleplay (1500 users)}
        \vspace{-0.2cm}
        \label{tab:winrates_persona}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{l c c}
            \hline
            \textbf{Method} & \textbf{ELIX-easy} & \textbf{ELIX-hard} \\
            \hline
            Base & 50.0 & 50.0 \\
            Few-shot Prompted & 92.4 & 81.4 \\
            Few-shot Pref-FT & 91.2 & 82.9 \\
            \methodname\ (Ours) & \textbf{97.8} & \textbf{91.8} \\
            \hline
        \end{tabular}
        \vspace{-0.2cm}
        \caption{\footnotesize GPT-4o Winrates on ELIX-easy and ELIX-hard}
        \vspace{-0.2cm}
        \label{tab:winrates_elix}
    \end{minipage}

    \vspace{0.4cm}

    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{l c}
            \hline
            \textbf{Baseline Method} & \textbf{Winrate (\%)} \\
            \hline
            \methodname\ vs Base & \textbf{71.2} \\
            \methodname\ vs SFT & \textbf{72.3} \\
            \hline
        \end{tabular}
        \vspace{-0.2cm}
        \caption{\footnotesize Roleplay: Human Eval Winrates}
        \vspace{-0.4cm}
        \label{tab:winrates_humaneval}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{l c c}
            \hline
            \textbf{Method} & \textbf{Trained} & \textbf{Interpolated} \\
            \hline
            Base (Llama 3.2 3B instruct) & 50.0 & 50.0 \\
            \hline
            Few-shot Prompted (4-shot) & 66.6 & 61.9 \\
            Few-shot Pref-FT (4-shot) & 66.5 & 66.1 \\
            \methodname\ (4-shot, Ours) & \textbf{78.4} & \textbf{71.3} \\
            \hline
            Few-shot Prompted (8-shot) & 69.1 & 59.1 \\
            Few-shot Pref-FT (8-shot) & 65.6 & 70.7 \\
            \methodname\ (8-shot, Ours) & \textbf{80.4} & \textbf{73.6} \\
            \hline
        \end{tabular}
        \vspace{-0.2cm}
        \caption{\footnotesize Review Winrates - Trained and Interpolated Users}
        \label{tab:winrates_reviews}
        \vspace{-0.4cm}
    \end{minipage}
\end{table*}

\noindent \textbf{Baselines.} We compare \methodname\ against four baselines: (1) a base model generating user-agnostic responses, (2) few-shot prompting with a base model, following~\citet{meister2024benchmarkingdistributionalalignmentlarge}, (3) few-shot supervised fine-tuning (Pref-FT) based off the maximum likelihood objective from GPO~\citep{zhao2024grouppreferenceoptimizationfewshot} and (4) prompting with an oracle user description following Persona Steering~\citep{cheng-etal-2023-marked}. Specifically, for (1) we use a standard instruct model that is prompted solely with the query, resulting in unconditioned responses. For (2) and (3), the base instruct model is provided with the same few-shot personalization examples as in \methodname, but (2) zero-shot predicts the preferred response and (3) is optimized with SFT to increase the likelihood on the preferred response. In (4), the base model is prompted with the oracle, ground truth user description, representing an upper bound on \methodnameâ€™s performance.


\noindent \textbf{Synthetic winrates.} 
We first generate automated win rates using the modified AlpacaEval procedure from Section~\ref{sec:dom_rand}. In the ELIX task in Table~\ref{tab:winrates_elix}, we study two levels of difficulty (easy, hard), where we find a consistent improvement of \methodname\ over baselines. Next, in Table~\ref{tab:winrates_reviews} for the Review task, on both Trained and Interpolated Users, \methodname\ allows for better performance on held-out questions. Finally, in Table~\ref{tab:winrates_persona}, we study Roleplay, scaling to 1500 real users, seeing a win rate of 82.6\% on both held-out users and questions. Additionally, COT closes the gap to the oracle response, showing effective recovery of the ground-truth user description. In \cref{sec:app_samples}, sample generations from \methodname\, show effective personalization to the oracle user description. Given this result, can we personalize to real people?

\noindent \textbf{Preliminary human study.} We evaluate our model trained on the Roleplay task by personalizing responses for \emph{real human participants}. 

We build a data collection app (Figure~\ref{fig:human_app}), interacting with a user in two stages. First, we ask participants to label preference pairs, used as the few-shot examples in \methodname. Then, for held out questions, we show a user a set of two responses: (1) a response from \methodname\ personalized based on their preferences and (2) a baseline response. Prolific is used to recruit a diverse set of study participants, evenly split across genders and continents, corresponding to the traits used to construct user descriptions. Question and response order is randomized to remove confounding factors. We evaluate with 25 users and 11 questions. As seen in Figure~\ref{tab:winrates_humaneval}, we find that \methodname\ has a 71\% win rate over the Base model and a 72\% win rate over an SFT model trained on diverse viewpoints from the preference dataset. 

\begin{AIbox}{Takeaways from Experiments}
We evaluate \methodname\ on the 3 tasks discussed and find an \textbf{87\% Alpaca Eval winrate} on average in generating responses that are personalized to synthetic users. COT also enables us to close the gap to the oracle method, where we prompt with the ground truth persona. Additionally, we run a preliminary, controlled human study, where we find a \textbf{72\% winrate} with real human users for open-ended question answering.
\end{AIbox}

