\section{Domains to Study Personalization}
\label{sec:domains}

To study personalization with \methodname\, we construct a benchmark across 3 domains ranging from generating personalized movie reviews (\textbf{Reviews}), generating personalized responses based off a user's education background (\textbf{ELIX}), and personalizing for general question answering (\textbf{Roleplay}). We open-source preference datasets and evaluation protocols from each of these tasks for future work looking to study personalization (sample in supplementary).

\noindent \textbf{Reviews.} The Reviews task is inspired by the IMDB dataset~\citep{maas-etal-2011-learning}, containing reviews for movies. We curate a list of popular media such as movies, TV shows, anime, and books for a language model to review. We consider two independent axes of variation for users: sentiment (positive and negative) and conciseness (concise and verbose). Here being able to pick up the user is crucial as the users from the same axes (e.g positive and negative) would have opposite preferences, making this \emph{difficult} to learn with any population based RLHF method. We also study the steerability of the model considering the axes of verbosity and sentiment in tandem (e.g positive + verbose). 

\noindent \textbf{ELIX.} The Explain Like I'm X (ELIX) task is inspired by the subreddit "Explain Like I'm 5" where users answer questions at a very basic level appropriate for a 5 year old. Here we study the ability of the model to personalize a pedagogical explanation to a user's education background. We construct two variants of the task. The first variant is \textbf{ELIX-easy} where users are one of 5 education levels (elementary school, middle school, high school, college, expert) and the goal of the task is to explain a question such as ``How are beaches formed?'' to a user of that education background. The second, more realistic variant is \textbf{ELIX-hard}, which consists of question answering at a high school to university level. Here, users may have different levels of expertise in different domains. For example, a PhD student in Computer Science may have a very different educational background from an undergraduate studying studying Biology, allowing for preferences from diverse users (550 users). 

\noindent \textbf{Roleplay.} The Roleplay task tackles general question answering across a wide set of users, following PRISM~\citep{kirk2024prismalignmentdatasetparticipatory} and PERSONA Bench~\citep{castricato2024personareproducibletestbedpluralistic} to study personalization representative of the broad human population. We start by identifying three demographic traits (age, geographic location, and gender) that humans differ in that can lead to personalization. For each trait combination, we generate 30 personas, leading to 1,500 total personas. To more accurately model the distribution of questions, we split our questions into two categories: global and specific. Global questions are general where anyone may ask it, but specific questions revolve around a trait, for example an elderly person asking about retirement or a female asking about breast cancer screening.

One crucial detail for each task is the construction of a preference dataset that spans multiple users. But how should one construct such a dataset that is realistic and effective?

\begin{AIbox}{Takeaways from Personalization Domains}
We propose a benchmark consisting of 3 domains, where personalization can be studied: (1) \textbf{Reviews}, studying the generation ability of models for reviews of movies, TV shows, and books that are consistent with a user’s writing style, (2) \textbf{Explain Like I'm X (ELIX)}: studying the generation ability of models for responses that are consistent with a user’s education level, and (3) \textbf{Roleplay}: studying the generation ability of models for responses that are consistent with a user's description, with effective transferability to a real human-study.
\end{AIbox}