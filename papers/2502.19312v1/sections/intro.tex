\vspace{-0.15cm}
\section{Introduction}  \label{sec:intro}
\vspace{-0.15cm}

As language models increasingly interact with a diverse user base, it becomes important for models to generate responses that align with individual user preferences. People exhibit a wide range of preferences and beliefs shaped by their cultural background, personal experience, and individual values. These diverse preferences may be reflected through human-annotated preference datasets; yet, current preferences optimization techniques like reinforcement learning from human feedback (RLHF) largely focus on optimizing a single model based on preferences aggregated over the entire population. This approach may neglect minority viewpoints, embed systematic biases into the model, and ultimately lead to worse performance compared to personalized models. Can we create language models that can adaptively align with personal preferences of the users and not the aggregated preferences of all users? 

Addressing this challenge requires a shift from modeling a singular aggregate reward function to modeling a distribution of reward functions~\citep{sorensen2024roadmappluralisticalignment, jang2023personalizedsoupspersonalizedlarge} that capture the diversity of human preferences. By doing so, we can enable personalization in language models, allowing them to generate a wide range of responses tailored to individual subpopulations. This approach not only enhances user satisfaction but also promotes inclusivity by acknowledging and respecting the varied perspectives that exist within any user base. However, how can this be effectively done for open-ended question answering and transfer to real users?

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/main_fig_v5.pdf}
    \caption{\footnotesize\textbf{ Overview of \methodname.} $N$ previously collected preferences are fed into the LLM along with the current query, allowing the LLM to personalize its response to the query using the past preferences.}
    \vspace{-0.3cm}
    \label{fig:fspo-overview}
\end{figure*}

In this paper, we introduce Few-Shot Preference Optimization  (\methodname{}), a novel framework designed to model diverse subpopulations in preference datasets to elicit personalization in language models for open-ended question answering. At a high level, \methodname\ leverages in-context learning to adapt to new subpopulations. This adaptability is crucial for practical applications, where user preferences can be dynamic and multifaceted. Inspired by past work on black-box meta-learning for language modeling~\citep{chen2022metalearninglanguagemodelincontext, min2022metaicllearninglearncontext, yu2024metamathbootstrapmathematicalquestions}, we fine-tune the model with a meta-learning objective, using preference-learning objectives such as IPO~\citep{2023arXiv231012036G}. We additionally propose user description chain-of-thought (COT), allowing the model to leverage additional inference-compute for better reward modeling and the model's instruction following capabilities for better response generation. 

However, to learn a model that effectively personalizes to real people, we need to collect a diverse preference dataset spanning diverse users. One natural approach to do this is to curate data from humans, but this curation is difficult and time-consuming. In contrast, in this work, we propose instantiating this dataset synthetically, and present careful design decisions to generate a dataset that is diverse and structured, following task construction considerations from the meta-learning literature~\citep{hsu2019unsupervised,yin2019meta}. 

To evaluate the efficacy of our approach, we construct a set of three semi-realistic domains to study personalization: (1) \textbf{Reviews}, studying the generation ability of models for reviews of movies, TV shows, and books that are consistent with a user’s writing style, (2) \textbf{Explain Like I'm X (ELIX)}: studying the generation ability of models for responses that are consistent with a user’s education level, and (3) \textbf{Roleplay}: studying the generation ability of models for responses that are consistent with a user's description, with effective transferability to a real human-study. Here we find that \methodname\ outperforms an unpersonalized model on average by 87\%. We additionally perform a controlled human study showcasing a winrate of 72\% of \methodname\ over unpersonalized models.


By addressing limitations of existing reward modeling techniques, our work paves the way for more inclusive and personalized LLMs. We believe that \methodname\ represents a significant step toward models that better serve the needs of all users, respecting the rich diversity of human preferences.