\vspace{-0.15cm}
\section{Preliminaries and Notation}
\label{sec:background}
\vspace{-0.15cm}

Preference fine-tuning algorithms, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically involve two main stages~\citep{ouyang2022training,2022arXiv220302155O}: Supervised Fine-Tuning (SFT) and Preference Optimization (DPO/RLHF). First, a pre-trained model is fine-tuned on high-quality data from the target task using Supervised Fine-Tuning (SFT). This process produces a reference model, denoted as $\pi_{\text{ref}}$. The purpose of this stage is to bring the responses from a particular domain in distribution with supervised learning. To further refine $\pi_{\text{ref}}$ according to human preferences, a preference dataset $\mathcal{D}_{\text{pref}} = \{(\mathbf{x}^{(i)}, \mathbf{y}_w^{(i)}, \mathbf{y}_l^{(i)})\}$ is collected. In this dataset, $\mathbf{x}^{(i)}$ represents a prompt or input context, $\mathbf{y}_w^{(i)}$ is the preferred response, and $\mathbf{y}_l^{(i)}$ is the less preferred response. These responses are typically sampled from the output distribution of $\pi_{\text{ref}}$ and are labeled based on human feedback.

Most fine-tuning pipelines assume the existence of an underlying reward function $r^*(\mathbf{x}, \cdot)$ that quantifies the quality of responses. A common approach to modeling human preferences is the Bradley-Terry (BT) model~\citep{bradleyterry1952preferences}, which expresses the probability of preferring response $\mathbf{y}_1$ over $\mathbf{y}_2$, given a prompt $\mathbf{x}$, as:
\begin{align}
\label{eq:bradley_terry}
p^*(\mathbf{y}_1 \succ \mathbf{y}_2 \mid \mathbf{x}) = \frac{e^{r^*(\mathbf{x}, \mathbf{y}_1)}}{e^{r^*(\mathbf{x}, \mathbf{y}_1)} + e^{r^*(\mathbf{x}, \mathbf{y}_2)}}
\end{align}
Here, $p^*(\mathbf{y}_1 \succ \mathbf{y}_2 \mid \mathbf{x})$ denotes the probability that $\mathbf{y}_1$ is preferred over $\mathbf{y}_2$ given $\mathbf{x}$.

The objective of preference fine-tuning is to optimize the policy $\pi_{\theta}$ to maximize the expected reward $r^*$. However, directly optimizing $r^*$ is often impractical due to model limitations or noise in reward estimation. Therefore, a reward model $r_{\phi}$ is trained to approximate $r^*$. To prevent the fine-tuned policy $\pi_{\theta}$ from deviating excessively from the reference model $\pi_{\text{ref}}$, a Kullback-Leibler (KL) divergence constraint is imposed. This leads to the following fine-tuning objective:
\begin{align}
\label{eq:rlhf_objective}
\begin{split}
\max_{\pi} \; \mathbb{E}[r^*(x,y)] - \beta\, D_\text{KL}(\pi\parallel\pi_{\text{ref}})
\end{split}
\end{align}
In this equation, the regularization term weighted by $\beta$ controls how much $\pi_{\theta}$ diverges from $\pi_{\text{ref}}$, based on the reverse KL divergence constraint. This constraint ensures that the updated policy remains close to the reference model while improving according to the reward function.

\noindent \textbf{Reward model training.} To fine-tune the large language model (LLM) policy $\pi_{\theta}(\mathbf{y} \mid \mathbf{x})$, the Bradley-Terry framework allows for either explicitly learning a reward model $r_{\phi}(\mathbf{x}, \mathbf{y})$ or directly optimizing preferences. Explicit reward models are trained using the following classification objective:
\begin{align}
\label{eq:reward_learning}
\max_{\phi}~ \mathbb{E}_{\mathcal{D}_{\text{pref}}} \left[\log \sigma \left(r_{\phi}(\mathbf{x}, \mathbf{y}_w) - r_{\phi}(\mathbf{x}, \mathbf{y}_l) \right) \right]
\end{align}
where $\sigma$ is the logistic function, used to map the difference in rewards to a probability. Alternatively, contrastive learning objectives such as Direct Preference Optimization~\citep{rafailov2023direct} and Implicit Preference Optimization~\citep{2023arXiv231012036G} utilize the policy’s log-likelihood $\log \pi_{\theta}(\mathbf{y} \mid \mathbf{x})$ as an implicit reward:
\begin{align} \label{eq:contrastive_parameterization} r_{\theta}(\mathbf{x}, \mathbf{y}) = \beta \log \big( \pi_{\theta}(\mathbf{y} \mid \mathbf{x}) / \pi_{\text{ref}}(\mathbf{y} \mid \mathbf{x}) \big) \end{align}
This approach leverages the policy’s log probabilities to represent rewards, thereby simplifying the reward learning process.