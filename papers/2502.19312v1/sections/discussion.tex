\vspace{-0.15cm}
\section{Discussion and Conclusion}
\vspace{-0.15cm}

We introduce \methodname, a novel framework for eliciting personalization in language models for open-ended question answering that models a distribution of reward functions to capture diverse human preferences. Our approach leverages meta-learning for rapid adaptation to each user, thereby addressing the limitations of conventional reward modeling techniques that learn from aggregated preferences. Through rigorous evaluation in 3 domains, we demonstrate that \methodname's generations are consistent with user context and preferred by real human users. Our findings also underscore the importance of diversity and structure in synthetic personalized preference datasets to bridge the Sim2Real gap. Overall, \methodname\ is a step towards developing more inclusive, user-centric language models.
