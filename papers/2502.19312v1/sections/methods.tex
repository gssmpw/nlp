% TODO: use consistent preference dataset notation
\vspace{-0.15cm}
\section{The Few-Shot Preference Optimization (\methodname) Framework}
\vspace{-0.15cm}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/persona_cot.pdf}
    \caption{\footnotesize\textbf{User Description Chain-of-Thought (COT).} Prediction is a two-stage process: first predicting a (synthetic) user description from the few-shot preferences and next predicting the response.}
    \label{fig:persona_cot}
    \vspace{-0.4cm}
\end{figure}

\textbf{Personalization as a meta-learning problem.} Generally, for fine-tuning a model with RLHF a preference dataset of the form: $\mathcal{D}_{\text{pref}} = \{(\mathbf{x}^{(i)}, \mathbf{y}_w^{(i)}, \mathbf{y}_l^{(i)})\}$ is collected, where $x$ is a prompt, $y_w$ is a preferred response, and $y_l$ is a dispreferred response. Here, preferences from different users are aggregated to learn the preferences over a population. However, through this aggregation, individual user preferences are marginalized, leading to the model losing personalized values or beliefs due to population-based preference learning and RLHF algorithms such as DPO as seen in prior work~\citep{siththaranjan2024distributionalpreferencelearningunderstanding}.

How can we incorporate user information when learning from preference datasets? In this work, we have a weak requirement to collect scorer-ids $\mathbf{S}^{(i)}$ of each user for differentiating users that have labeled preferences in our dataset: $\mathcal{D}_{\text{pref}} = \{(\mathbf{x}^{(i)}, \mathbf{y}_w^{(i)}, \mathbf{y}_l^{(i)}, \mathbf{S}^{(i)})\}$. Now consider each user as a task instance, where the objective is to learn an effective reward function for that user using the user's set of preferences. This can be naturally instantiated as a black-box meta-learning objective, where meta-learning is done over users (also referred to as a task in meta-learning). Meta-learning should enable rapid personalization, i.e. adaptability to new users with just a few preferences.

More formally, consider that each unique user $\mathcal{S}^{(i)}$'s reward function is characterized by a set of preferences with prompt and responses $(x, y_1, y_2)$, and preference label $c$ (indicating if $y_1 \succ y_2$ or $y_1 \prec y_2$). Given a distribution over users $\mathcal{S}=P(\mathcal{S}^{(i)})$, a meta-learning objective can be derived to minimize its expected loss with respect to $\theta$ as:
\begin{align}
    \min_{\theta} \mathbb{E}_{\mathcal{S}^{(i)} \sim \mathcal{S}} \left[\mathbb{E}_{(x,y_1,y_2,c) \sim \mathcal{D}_i}\left[\mathcal{L}^{\theta}_{\textit{pref}}\left(x,y_1,y_2,c\right)\right]\right]
    \label{eq:metalearn-obj}
\end{align}
% \se{should make the objective depend on theta}
where $D_i$ is a distribution over preference tuples $(x,y_1,y_2,c)$ for each user $S^{(i)}$, and $\mathcal{L}^{\theta}_{\textit{pref}}$ is a preference learning objective such as DPO~\citep{rafailov2023direct} or IPO~\citep{2023arXiv231012036G}:
\begin{align}
    \mathcal{L}^{\theta}_{\textit{pref}} = || h_{\pi_\theta}^{y_w,y_l} - (2\beta)^{-1}||^2_{2}, ~~~~
    h_{\pi_\theta}^{y_w,y_l} = \log \frac{\pi_\theta(y_w| x)}{\pi_{\textit{ref}}(y_w| x)} - \log \frac{\pi_\theta(y_l| x)}{\pi_{\textit{ref}}(y_l| x)}
    \label{eq:ipo-obj}
\end{align}
where $y_w$ and $y_l$ are the preferred and dispreferred responses (respectively) according to the responses $y_1,y_2$ and class label $c$ in the preference dataset.

Following black-box meta-learning approaches, \methodname\ receives as input a sequence of preferences $D_i^{fewshot} \sim D_i$ from a User $S^{(i)}$. This is followed by an unlabeled, held-out preference $(x,y_1,y_2) \sim \mathcal{D}_i \backslash \mathcal{D}_i^{\textit{fewshot}}$ for which it outputs its prediction $c$. To make preferences compatible with a pre-trained language model, a few-shot prompt is constructed, comprising of preferences from a user and the held-out query as seen in Figure~\ref{fig:fspo-overview}. This construction has an added benefit of leveraging a pretrained language model's capabilities for few-shot conditioning~\citep{brown2020language}, which can enable some amount of steerage/personalization. This prediction $c$ is implicitly learned by a preference optimization algorithm such as DPO~\citep{rafailov2023direct}, which parameterizes the reward model as $\beta\frac{\log \pi_{\theta}(y|x)}{\log \pi_{\textit{ref}}(y|x)}$. This parameterization enables us to leverage the advantages of preference optimization algorithms such as eliminating policy learning instabilities and computational burden of on-policy sampling, learning an effective model with a simple classification objective.

\noindent \textbf{User description chain-of-thought (COT).} If provided with a description of the user (potentially synthetically generated), \methodname\ can be converted to a two-step prediction problem as seen in Figure~\ref{fig:persona_cot}. In the first step, conditioned on user few-shot preferences, the user description is generated, then conditioned on the prompt, few-shot preferences, and generated user description, a response can then be generated. This prediction of the user description is an interpretable summarization of the fewshot preferences and a better representation to condition on for response generation. Similar to the rationale generated in \citet{zhang2024generativeverifiersrewardmodeling} for verifiers, the COT prediction can be viewed as using additional inference-compute for better reward modeling. Additionally, this formulation leverages the instruction following ability of LLMs~\citep{ouyang2022training} for response generation.

\noindent \textbf{User representation through preference labels}. From an information-theoretic perspective, the few-shot binary preferences can be seen as a $N$-bit representation of the user, representing up to $2^N$ different personas or reward functions. There are several ways to represent users: surveys, chat histories, or other forms of interaction that reveal hidden preferences. We restrict our study to such a $N$-bit user representation, as such a constrained representation can improve the performance when transferring reward models learned on synthetic personalities to real users. We defer the study of less constrained user representations to future work.

% Overall, \methodname\ enables efficient learning of preferences across users. 
We summarize \methodname\ in Algorithm~\ref{alg:fspo}. Next, we will discuss domains to study \methodname.

\begin{center}
\begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \caption{Overview of Few-Shot Preference Optimization (\methodname)}
    \begin{algorithmic}[1]
    \State \textbf{Input:} For each unique user $\mathcal{S}^{(i)}$, a dataset of preferences $\mathcal{D}:={(x,y_1,y_2,c)}_{i}$, and optionally user description $y_{\mathcal{S}^{(i)}}$ for COT, $\forall i$
    \State \textbf{Output:} Learned policy $\pi_{\theta}$
    \While{not done}
    \State Sample training user $\mathcal{S}^{(i)}$ (or minibatch)
    \State Sample a subset of preferences from the user $\mathcal{D}_i^{\textit{fewshot}} \sim \mathcal{D}_i$
    \State Sample held-out preference examples $D_{i}^{\textit{heldout}} \sim \mathcal{D}_i \backslash \mathcal{D}_i^{\textit{fewshot}}$
    \If{COT} 
    \State Use \cref{eq:metalearn-obj} and \cref{eq:ipo-obj} to predict the loss on the user description $y_{\mathcal{S}^{(i)}}$
    \EndIf
    \State Conditioning on $\mathcal{D}_i^{\textit{fewshot}}$ (optionally $y_{\mathcal{S}^{(i)}}$), use \cref{eq:metalearn-obj} and \cref{eq:ipo-obj} to predict the loss on the held-out preference example $D_{i}^{\textit{heldout}}$
    \State Update learner parameters $\theta$, using gradient of loss on $D_{i}^{\textit{heldout}}$
    \EndWhile
    \State \textbf{Return} $\pi_{\theta}$
    \end{algorithmic}
    \label{alg:fspo}
    \end{algorithm}
\end{minipage}
\end{center}

\begin{AIbox}{Takeaways from \methodname\ framework}
\methodname\ offers an effective approach to personalizing open-ended question answering, by framing personalization as a meta-learning problem, conditioned on few-shot preferences from a user as seen in Figure~\ref{fig:fspo-overview}. Additionally, \methodname\ can be converted to a two-step prediction problem, predicting a user description conditioned on preferences and then a response, leveraging additionally inference-compute and the model's instruction-tuned prior for better performance as seen Figure~\ref{fig:persona_cot}.  We summarize the algorithm framework in Algorithm~\ref{alg:fspo}.
\end{AIbox}
