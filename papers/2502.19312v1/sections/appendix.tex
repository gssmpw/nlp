\section{Appendix}
\subsection{Sample Personalized Responses}
\label{sec:app_samples}
We provide sample responses from \methodname\ in Figure~\ref{fig:personalized_responses} across the 3 tasks that were studied (ELIX, Reviews, and Roleplay). We additionally include the oracle scoring description for each response, demonstrating \methodname's ability to personalize. Additionally, for the Review task, we show responses for interpolated users which were not found in the preference dataset, showing the ability of \methodname\ to generalize and be controllable.

\subsubsection{Roleplay CoT}
\begin{lstlisting}[frame=single,breaklines=true]
QUESTION:
What are some good volunteer opportunities for me?

TRUE PERSONA:
A 53-year-old woman living in North America, an avid traveler and writer who shares her adventures through a popular travel blog. She loves spontaneous adventures and values long-term financial security. She is at a point in her career where opportunities for advancement might be time-sensitive. She enjoys movies that challenge perspectives and spark discussions. She prioritizes well-being over being fully informed. She is interested in the van life movement. She appreciates creativity and variety in meal options. She enjoys engaging in creative and tactile activities and prefers traditional learning methods. She values compassionate and responsible choices. She enjoys exploring new and dynamic experiences. She is concerned about fast food. She values mental well-being and mindfulness. She enjoys attending events and conferences to meet new people. She appreciates practicality and health. She enjoys shows with dark humor and suspenseful storytelling.

COT GENERATED PERSONA:
Based on the examples provided, the user appears to value a more holistic and experiential approach to learning and personal growth. They seem to prioritize human connection, nature, and personal development over technology and material wealth.

This user is likely someone who is interested in personal stories, emotions, and experiences, as evident from their appreciation for Sylvia Plath's poetry and the suggestion to weave storytelling into public speaking. They may also be someone who values authenticity, simplicity, and community, as seen in their suggestion of house swapping and volunteering as alternative vacation options.

In terms of lifestyle, this user likely prefers a balanced and flexible approach to life, allowing for a mix of work, personal growth, and leisure activities. They may also be someone who values environmental sustainability and social responsibility, as hinted at by their consideration of eco-friendly home improvements and sustainable financial planning. Overall, this user seems to be a thoughtful, empathetic, and nature-loving individual who prioritizes human connections, experiences, and personal growth over material possessions and technological advancements.

FINAL RESPONSE:
Based on your affinity for human connections, personal growth, and environmental sustainability, I'd like to suggest the following volunteer opportunities that might resonate with you:

**Community Garden or Urban Farm**: Many organizations in your area offer community gardens or urban farms where you can help cultivate and maintain a shared space, promoting sustainability and food security. This allows you to connect with nature and contribute to your community's well-being.

**Nature Reserves or Conservation Efforts**: Consider volunteering with local wildlife conservation organizations, such as bird watching tours, tree planting initiatives, or habitat restoration projects. This will enable you to work outdoors, connect with others who share your passion for nature, and contribute to preserving the environment.

**Community Centers or Environmental Education Programs**: Many community centers offer various programs, such as gardening workshops, environmental education classes, or sustainable living workshops. By volunteering at these centers, you can share your knowledge, learn from others, and make a positive impact on your community.

**Animal Shelters or Wildlife Rehabilitation Centers**: If you're an animal lover, consider volunteering at a local animal shelter or wildlife rehabilitation center. You'll have the opportunity to care for animals, interact with like-minded individuals, and contribute to animal welfare and conservation efforts.

**Neighborhood Events or Community Festivals**: Look for opportunities to volunteer at local events, such as festivals, farmers' markets, or street fairs. This allows you to engage with your community, promote sustainability, and build connections with neighbors through shared passion for environmental causes.

These opportunities will not only nurture your spirit of volunteering but also help you foster meaningful connections with nature, others, and your community while nurturing a sense of personal growth and fulfillment.

\end{lstlisting}

\begin{table*}
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing
    \setlength{\tabcolsep}{8pt} % Adjust column spacing
    \begin{tabular}{l c}
        \toprule
        \textbf{Name} & \textbf{Value} \\
        \midrule
        Learning Rate (SFT/Pref-FT) & \(1e^{-5}, 1e^{-6}, \mathbf{1e^{-7}}\) \\
        Learning Rate (IPO) & \(1e^{-5}, \mathbf{1e^{-6}}, 1e^{-7}\) \\
        Beta (IPO) & \(0.1, 0.05, 0.01, \mathbf{0.005}, 0.001\) \\
        Number of Shots & \(4, \mathbf{8}\) \\
        Model Name & Llama 3.2 3B Instruct~\citep{grattafiori2024llama} \\
        \bottomrule
    \end{tabular}
    \caption{Sweep over hyperparameters for \methodname, recommended hyperparameters in bold.}
    \label{tab:hyperparameters}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/PersonalizedELIX.pdf}
    \includegraphics[width=0.8\linewidth]{figures/PersonalizedReview.pdf}
    \caption{\footnotesize\textbf{Sample Personalized Response for ELIX (top) and Reviews (bottom).}}
    \label{fig:personalized_responses}
\end{figure*}


\subsection{Additional Details on Dataset Construction and Evaluation}
\label{sec:app_dataset_eval}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/human_study.png}
    \caption{\footnotesize \textbf{An overview of the Human Study Interface.} First, users label a set of preferences. Then, a set of personalized answers are provided, conditioned on label preferences.}
    \vspace{-0.2cm}
    \label{fig:human_app}
    \vspace{-0.3cm}
\end{figure*}


\paragraph{Dataset Details}
We include a representative subset of the roleplay task and a jupyter notebook demonstrating shot construction for training and evaluation in the supplementary material (due to size restrictions). We will release the full datasets for each task in the final release of the paper. 

\paragraph{Evaluation Prompt for Synthetic Preferences}
We used GPT-4o as a Judge using a modified variant of the Alpaca Eval~\citep{dubois2024alpacafarm,dubois2024lengthcontrolledalpacaevalsimpleway} Prompt to be aware of a user description when scoring preference examples.

Here is the system prompt:
\begin{lstlisting}[frame=single,breaklines=true]
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.
\end{lstlisting}

Here is the user prompt:
\begin{lstlisting}[frame=single,breaklines=true]
You are tasked with evaluating the outputs of multiple large language models to determine which model produces the best response from a human perspective.

## Instructions

You will receive:
1. A **User Instruction**: This is the query or task provided to the models.
2. **Model Outputs**: Unordered responses from different models, each identified by a unique model identifier.
3. A **User Description**: This describes the user's preferences or additional context to guide your evaluation.

Your task is to:
1. Evaluate the outputs based on quality and relevance to the user's instruction and description.
2. Select the best output that meets the user's needs.

## Input Format

### User Instruction
{QUESTION}

### Model Outputs
- Model "m": {RESPONSE_A}
- Model "M": {RESPONSE_B}

### User Description
{USER_DESCRIPTION}

## Task

From the provided outputs, determine which model produces the best response. Output only the model identifier of the best response (either `m` or `M`) with no additional text, quotes, spaces, or new lines.

## Best Model Identifier
\end{lstlisting}


\paragraph{Additional Human Study Details}
% Length Normalization, View Normalization, Format Standardization (no markdown).
As shown in Alpaca Eval 2.0~\citep{dubois2024lengthcontrolledalpacaevalsimpleway}, several biases can affect the evaluation of language models such as length, format, and more. For this reason, we took action to normalize both \methodname\ and baselines in 3 different categories. First, length is an evaluation bias. For this reason, we computed the average length of responses from \methodname\ and prompted the base model during evaluation to keep its responses around the average length in words ($\approx 250$ words). For the SFT baseline, we found that this was consistent with \methodname\ since it was fine-tuned on the same preference dataset. Additionally, due to context length restrictions and the instruction following abilities of smaller open-source LLMs, we decided to have formatting be consistent as paragraphs rather than markdown for the Roleplay task. Thus, we similarly prompted the Base model with this behavior. Finally, a differing number of views can also skew the evaluation, as a large proportion of users seem to prefer direct answers. Additionally, if more views are presented, a user may prefer just one of the many views provided, skewing evaluation. Thus, we ensure that when two responses are compared, they have the same number of views. In future, work, it would be interesting to consider how to relax some of the design decisions needed for the human study. We additionally provide screenshots of the human study interface in Figure~\ref{fig:human_app}.

Below is the full text of instructions given to the participants: \\
"This is a study about personalization. You will be asked to read a set of 20 questions (9 on the first page, 11 on the second page). For each question, there are two responses. Please select the response that you prefer. Make this selection based on your individual preferences and which response you find the most helpful. Read the entire response and think carefully before making your selection."

We utilize the demographic information that Prolific provides for each user such as their age group, continent and gender to chose questions but do not store that information about the user. We collect no identifying information about the user and will not make any of the individual preferences from a user public. We pay each user a fair wage subject to the current region that we reside in. We received consent from the people whose data we are using and curating as the very first question in our survey. The demographic and geographic characteristics of the annotator population is exactly the same as Prolific. We do no filtering of this at all.

\subsection{Training Details and Hyperparameters for \methodname\ and baselines}
Similar to DPO~\citep{rafailov2023direct} and IPO~\citep{2023arXiv231012036G}, we trained \methodname\ in a two stage manner. The first stage is Fewshot Pref-FT, increasing the likelihood of the preferred response. The second stage is Fewshot IPO, initialized from the checkpoint of Fewshot Pref-FT. One epoch of the dataset was performed for each stage. For the IPO baseline, we followed a similar procedure. Additional hyperparameters can be found in Table~\ref{tab:hyperparameters}. 

\subsection{Additional Details of Setup for Reproducability}
We used both code, models, and data as scientific artifacts. In particular, for code, we built off of the \href{https://github.com/eric-mitchell/direct-preference-optimization}{codebase} from \citet{rafailov2023direct}, with an Apache 2.0 license. We additionally adapted our evaluation script from Alpaca EVAL, including the prompt, and other criterion for evaluation and normalization. We have reported the implementation details for synthetic evaluation in Section~\ref{sec:dom_rand} and human study evaluation in Section ~\ref{sec:app_dataset_eval}.

For models, we used a combination of open-source and closed-source models. The models that we used for sampling data are the Llama family of models~\citep{grattafiori2024llama} (Llama 3.2 3b, Llama 3.1 8b, Llama 3.3 70b) with the llama license (3.1, 3.2, 3.3), the Qwen family of models~\citep{qwen2025qwen25technicalreport} (Qwen 2.5 3b, Qwen 2.5 32b, Qwen 2.5 72b) with the qwen license, the Gemma 2 family of models~\citep{gemmateam2024gemma2improvingopen} (Gemma 2 2b, Gemma 2 9b, and Gemma 2 27b) with the gemma license, and the OpenAI~\citep{openai2024gpt4ocard} family of models (GPT4o, GPT4o-mini) with the OpenAI API License (based off of the MIT License). We used SGLang~\citep{zheng2024sglangefficientexecutionstructured} and VLLM~\citep{kwon2023efficientmemorymanagementlarge} for model inference. For training, we used 1 node of A100 GPUs (8 GPUs) for 8 hours for each experiment with FSDP. Cumulatively, we used approximately 4000 hours of GPU hours for ablations over dataset, architecture design and other details. 

With respect to the dataset, for questions for the review dataset, we sourced media names from IMDb~\citep{imdb}, Goodreads~\citep{goodreads}, and MyAnimeList~\citep{myanimelist}. We define the domains in more detail in section~\ref{sec:domains}. Seed questions for ELIX were human generated, sourced from Prolific. The dataset is entirely in English, with some artifacts of Chinese from the Qwen model family, which will be filtered out for the final release of the dataset. None of this data has identifying information about individual people or offensive content as the dataset was sourced from instruction and safety-tuned models, with each step of the dataset having a manual check of the inputs and outputs. In terms of statistics of the dataset, the review dataset has 130K train/dev examples and 32.4K test examples, the ELIX-easy dataset has 235K train/dev examples and 26.1K test examples, the ELIX-hard dataset has 267K train/dev examples and 267K test examples, and the roleplay dataset has 362K train/dev examples and 58.2K test examples, with a total of 1.378 million examples. For our statistics, we reported the average winrate \% for each method on both synthetic and human evals, following prior work in alignment like AlpacaFarm~\citep{dubois2024alpacafarm}. 

Each of the artifacts above was consistent with its intended use and the code, models, and datasets should be usable outside of research contexts.
