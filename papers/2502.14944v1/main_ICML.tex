
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}  %



\usepackage[textsize=tiny]{todonotes}

\input{notation.tex}
\usepackage{mkolar_definitions}



\icmltitlerunning{
Reward-Guided Iterative Refinement in Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{Reward-Guided Iterative Refinement in Diffusion Models at Test-Time \\ with Applications to Protein and DNA Design}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}


\icmlauthor{Masatoshi Uehara}{equal,gen}
\icmlauthor{Xingyu Su}{equal,tex}
\icmlauthor{Yulai Zhao}{pri}
\icmlauthor{Xiner Li}{tex}
\icmlauthor{Aviv Regev}{gen}  \\
\icmlauthor{Shuiwang Ji$^{\dagger}$}{tex} 
\icmlauthor{Sergey Levine$^{\dagger}$}{ber}
\icmlauthor{Tommaso Biancalani$^{\dagger}$}{gen}
\end{icmlauthorlist}

\icmlaffiliation{tex}{Texas A\&M University}
\icmlaffiliation{gen}{Genentech}
\icmlaffiliation{ber}{UC Berkley}
\icmlaffiliation{pri}{Princeton University}

\icmlcorrespondingauthor{Masatoshi Uehara}{ueharamasatoshi136@gmail.com}
\icmlcorrespondingauthor{Xingyu Su}{xingyu.su@tamu.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
To fully leverage the capabilities of diffusion models, we are often interested in optimizing downstream reward functions during inference. While numerous algorithms for reward-guided generation have been recently proposed due to their significance, current approaches predominantly focus on single-shot generation, transitioning from fully noised to denoised states. We propose a novel framework for test-time reward optimization with diffusion models. Our approach employs an iterative refinement process consisting of two steps in each iteration: noising and reward-guided denoising. This sequential refinement allows for the gradual correction of errors introduced during reward optimization. 
Finally, we demonstrate its superior empirical performance in protein and cell-type specific regulatory DNA design. The code is available at \href{https://github.com/masa-ue/ProDifEvo-Refinement}{https://github.com/masa-ue/ProDifEvo-Refinement}. 
\end{abstract}





\input{main/main_intro} 

\input{main/main_relatedworks}

\input{main/main_prelim}

\input{main/main_algo}

\input{main/main_theory} 

\input{main/main_practical}

\input{main/main_experiment} 

\input{main/main_conclusion}




\bibliographystyle{chicago}
\bibliography{whole,whole2} 

\newpage 
\input{main/main_appenedix}

\end{document}
