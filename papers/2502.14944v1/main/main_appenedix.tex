\onecolumn 
\appendix 


\section{Proof of \pref{thm:key}} \label{sec:proof}

Here, we use induction. Hence, we prove that $x^{\langle 1 \rangle}_0$ follows $p^{(\alpha)}$. 

\paragraph{Distribution after noising.} First, we consider the distribution after noising. This is 
\begin{align*}
    \int q_K(x^{\langle 1 \rangle}_K \mid x^{\langle 0 \rangle}_0)  p^{(\alpha)}(x^{\langle 0 \rangle}_0)d x^{\langle 0 \rangle}_0. 
\end{align*}
By plugging in the first assumption regarding distributions of initial designs, it is equal to  
\begin{align}\label{eq:first}
    \int q_K(x^{\langle 1 \rangle}_0 \mid x^{\langle 1 \rangle}_K)q_K(x^{\langle 1 \rangle}_K)\exp(r(x^{\langle 0 \rangle}_0)/\alpha) d x^{\langle 0 \rangle}_0.  
\end{align}
Recalling this definition of soft value functions:
\begin{align*}
    \exp(v_K(\cdot)/\alpha)=\EE_{p^{\pre}(x_0|x_K)}[\exp(r(x_0)/\alpha)\mid x_K] 
\end{align*}
and the assumption (b) ($q_0(x_0|x_K) = p^{\pre}(x_0|x_K)$ and $q_K(\cdot)=p^{\pre}_K(\cdot)$ ), the term \eqref{eq:first} is equal to 
\begin{align*}
    p^{\pre}_K(\cdot)\exp(v_K(\cdot)/\alpha))/C. 
\end{align*}

\paragraph{Distribution after reward-guided denoising.} Now, we consider the distribution of $x^{\langle 1 \rangle}_0$: 
\begin{align*}
    1/C \int  \left\{ \prod_{k=K}^1 p^{\star}_k(x_{k-1} \mid x_k) \right \} p^{\pre}_K(x_K)\exp(v_K(x_K)/\alpha))  d(x_0,\cdots,x_K). 
\end{align*}
With some simple algebra, this is equal to 
\begin{align*} 
&  1/C  \int  \left\{ \prod_{k=K-1}^1 p^{\star}_k(x_{k-1} \mid x_k) \right \} \times \frac{p^{\pre}_K(x_{K-1}|x_{K})\exp(v_{K-1}(x_{K-1})/\alpha))}{\exp(v_{K}(x_{K})/\alpha))} \times  p^{\pre}_K(x_K)\exp(v_K(x_K)/\alpha))  d(x_0,\cdots,x_{K}) \\
& = 1/C  \int  \left\{ \prod_{k=K-1}^1 p^{\star}_k(x_{k-1} \mid x_k) \right \} \times  p^{\pre}_K(x_{K-1}|x_{K}) p^{\pre}_K(x_{K}) \exp(v_{K-1}(x_{K-1})/\alpha))    d(x_0,\cdots,x_{K}) \\
&= 1/C  \int  \left\{ \prod_{k=K-1}^1 p^{\star}_k(x_{k-1} \mid x_k) \right \} p^{\pre}_{K-1}(x_{K-1})\exp(v_{K-1}(x_{K-1})/\alpha))d(x_0,\cdots,x_{K-1}). 
\end{align*}
Repeating this argument from $k=K-1$ to $k=0$, the above is equal to 
\begin{align*}
       p^{\pre}_0(\cdot)\exp(r(\cdot)/\alpha)/C. 
\end{align*}
This concludes the statement. 

\section{Additional Details for Protein Design}\label{sec:appendix}

In this section, we have added further details on experimental settings and results. 

\subsection{Details on Baselines}

\begin{itemize}
    \item \alg\,(\pref{alg:decoding2}): We have used parameters $L=20,N = 10,S=30$ in general. For the importance sampling step, we have used $\alpha = 0.0$, and for the selection step, we have used $\alpha = 0.2$. 
    \item \textbf{SVDD}: We set the tree width $L = 20,\alpha = 0.0$. 
    \item \textbf{SMC}: In SMC, we set $\alpha =0.05$ because if we choose $\alpha=0.00$, it just gives a single sample every time step. Refer to Appendix B in \citet{li2024derivative}.  
    \item \textbf{GA}: Here, compared to \pref{alg:decoding2}, we have changed the mutation part (Line 3-7) with just sampling from pre-trained diffusing models without any reward-guided generation. To have a fair comparison with \alg, we increase the repetition number $S$ so that the computational budget is roughly the same as our proposal.    
\end{itemize}


\subsection{Details on Reward Functions}

\paragraph{Globularity.} 
Globularity refers to the degree to which a protein adopts a compact and nearly spherical three-dimension structure~\citep{pace1975stability}.It is defined based on the spatial arrangement of backbone atomic coordinates, where the variance of the distances between those coordinates and the centroid is minimized, leading to a highly compact structure. Here, we set the protein length $150$. 

Globular proteins are characterized by their structure stability and water solubility, differing from fibrous or membrane proteins. The compact conformation helps proteins to maintain proper protein folding and reduce the risk of aggregation.

\paragraph{Symmetry.}
Protein symmetry refers to the degree to which protein subunits are arranged in a repeating structure pattern~\citep{goodsell2000structural,lisanza2024multistate,hie2022high}. Here we focus on the rotational symmetry of a single chain, which is defined by the spatial organization of subunit centroids. 
Specifically, we try to minimize the variances of the distances between adjacent centroids to achieve a more uniform and balanced arrangement. Here, we set the protein length to be $150$ to $240$. 

Symmetric proteins can bring multiple functional sites into close proximity, facilitating interactions and supporting the formation of large proteins with optimized biological functions.  


\paragraph{Hydrophobicity.} 
Hydrophobicity refers to the degree to which a protein repels water, primarily defined by the distribution of hydrophobic amino acids within the structure, namely, Valine, Isoleucine, Leucine, Phenylalanine, Methionine and Tryptophan~\citep{chandler2002hydrophobicity}. Hydrophobicity is optimized by minimizing the average Solvent Accessible Surface Area (SASA) of the hydrophobic residues above, thus reducing their exposure to the surrounding solvent. Hydrophobicity enhances the protein structural stability, especially in the polar solvents such as water, facilitates the protein-protein interactions by prompting binding at the hydrophobic surfaces, and drives the proper protein folding by guiding the hydrophobic residues to the protein core.

\paragraph{pLDDT.}
pLDDT (predicted Local Distance Difference Test) is a confidence score used to evaluate the reliability of the local structure in predicted proteins. It is defined by the confidence of model predictions, assigning a confidence value to each residues. A higher pLDDT score indicates greater model confidence and suggests increased structural stability. To optimize the whole protein structure, we try to maximize the average pLDDT across the whole sequence as predicted by ESMFold~\citep{lin2023evolutionary}.




\subsection{Additional Results} 

\paragraph{More metric (diversity, pLDDT, and pTM).}

We have included additional metrics in Table~\ref{tab:all_results}.
\begin{itemize}
    \item Generally, higher pLDDT and pTM values indicate more accurate structure predictions at the local residue and the global structure, respectively. However, in the context of de novo protein design, a low pLDDT does not necessarily imply poor performance \citep{verkuil2022language}. In the globularity task, it is expected that the generated protein is more novel protein.  
    \item We define diversity as 1 - the mean pairwise distance (normalized by length), where the distance is measured using the Levenshtein distance. While diversity can be an important metric to evaluate the performance of pre-trained generative models, in the context of reward optimization, this metric may be secondary. It is shown that generated sequences from \alg\,are reasonably diverse enough without collapsing to single samples. 
\end{itemize}

\begin{table*}[!h]
    \centering
    \caption{Additional metrics for experiments in protein design. We have reported the median of pLDDT, pTM, and diversity of generated proteins.  } 
    \label{tab:all_results}
  \resizebox{\textwidth}{!}{    \begin{tabular}{c|ccc | ccc | ccc |ccc } 
  Task   & \multicolumn{3}{|c|}{ (a) ss-match }  & \multicolumn{3}{|c|}{ (b) cRMSD } & \multicolumn{3}{|c|}{ (c) globularity } & \multicolumn{3}{|c}{ (d) symmetric }\\ 
         &    pLDDT    &  pTM  & diversity    &    pLDDT  &  pTM  & diversity   &    pLDDT   &  pTM & diversity   &    pLDDT   &  pTM & diversity   
         \\ \midrule  
 \rowcolor{lightgray}   \alg   &  {0.75}  & 0.69  & 0.28 &   0.76  & {0.71}  & 0.14 &  0.41 & 0.29 &  0.56 &  0.82 &  0.79 &  0.49 \\ 
    \end{tabular}}
\end{table*}


\paragraph{Recovery rate when optimizing cRMSD.}

By optimizing cRMSD, we can tackle the inverse folding task. While we have not extensively investigated the performance in terms of recovery rates, we present the observed recovery rates for several proteins as a reference when using \alg. Although it does not match the performance of state-of-the-art conditional generative models specifically trained for this task, such as ProteinMPNN \citep{dauparas2022robust}, our algorithm, which combines \emph{unconditional} diffusion models with reward models at \emph{test-time}, demonstrates competitive performance.



\begin{table}[!th]
    \centering
       \caption{Recovery rates when optimizing cRMSD}
    \label{tab:recovery}
    \begin{tabular}{cccccc} \toprule
    Proteins     &  5KPH & 6NJF  & EHEE \_rd1\_0101 & EA:run2 \_0325\_0005 &  \\ \midrule 
 \rowcolor{lightgray} \alg &  0.26  & 0.31 & 0.28   & 0.30 \\ 
    ProteinMPNN &  0.41 & 0.53 & 0.35 & 0.38 \\ 
    \bottomrule 
    \end{tabular}
\end{table}
\paragraph{More generated proteins.}

We have visualized more generated proteins in \pref{fig:generated_results2}. 

\begin{figure*}[!th]
    \centering
 \begin{minipage}{0.20\textwidth}  %
    \centering
     \includegraphics[width=0.9\textwidth]{images/ss_match_XX_1.0.png}
    \subcaption{The generated proteins (\textcolor{green}{Green}) when optimizing \textbf{ss-match} are shown. \textcolor{red}{Red} represents the target secondary structures. The \textbf{ss-match} score is 1.0 here. }
  \end{minipage} \hfill
 \begin{minipage}{0.28\textwidth}  %
    \centering
   \includegraphics[width=0.52\textwidth]{images/tm_r15_1.9.png}
   \includegraphics[width=0.46\textwidth]{images/crmsd_6NJF_1.2.png}
    \subcaption{The generated proteins (\textcolor{green}{Green}) when optimizing \textbf{cRMSD} are shown. \textcolor{red}{Red} represents the target secondary structures. }
  \end{minipage} 
 \begin{minipage}{0.20\textwidth}  %
    \centering
   \includegraphics[width=0.88\textwidth]{images/globularity3.png}
    \subcaption{The generated proteins when optimizing \textbf{globularity} are shown.  }
  \end{minipage} 
  \begin{minipage}{0.27\textwidth}  %
    \centering
   \includegraphics[width=0.88\textwidth]{images/symmetric4.png}
    \subcaption{The generated proteins when optimizing \textbf{symmetry} are shown.  }
  \end{minipage} 
  
  \caption{More generated protein from \alg. }
    \label{fig:generated_results2}
\end{figure*}


\section{Additional Details for DNA Design}\label{sec:appendix}


\paragraph{Pre-trained models.} We use the pre-trained diffusion model trained in \citet{wang2024finetuning}. The code and its performance are available in their paper. Here, we use the discrete diffusion model proposed in \citep{sahoo2024simple} using the same CNN architecture as in \citep{stark2024dirichlet} and a linear noise schedule. 

\paragraph{Reward oracles.} We use the exact oracle used in \citet{wang2024finetuning}. Again, the code and its performance are available in their paper. Here, we use the Enformer architecture \citep{avsec2021effective} initialized with its pretrained weights. We use the data splitting based on chromosome following standard practice \citep{lal2024reglm}.  

\paragraph{Hyperparameters in baselines and \alg.} We set $S=15,\alpha =0.0, L = 20$. 

\paragraph{Diversity.} We calculate the diversity as in the protein design task. It is 0.47 in HepG2, 0.49 in K562, and 0.53 in SKNSH. It is shown that generated sequences are reasonably diverse enough. 


