\vspace{-2mm}
\section{Introduction}


Diffusion models have achieved significant success across various domains, including computer vision and scientific fields \citep{ramesh2021zero,watson2023novo}.
These models enable sampling from complex natural image spaces or molecular spaces that resemble natural structures. Beyond the capabilities of such pre-trained diffusion models, there is often a need to optimize downstream reward functions. For instance, in text-to-image diffusion models, the reward function may be the alignment score \citep{black2023training,fan2023dpok,uehara2024feedback}, while in protein sequence diffusion models, it could include metrics such as stability, structural constraints, or binding affinity \citep{verkuil2022language}, and in DNA sequence diffusion models, it may involve activity levels \citep{sarkar2024designing,lal2024reglm}.






 
Building on the motivation above, we focus on optimizing downstream reward functions while preserving the naturalness of the designs. (e.g., a natural-like protein sequence exhibiting strong binding affinity) by seamlessly integrating these reward functions with pre-trained diffusion models during inference. While numerous studies have proposed to incorporate rewards 
into the generation process of diffusion models (e.g., classifier guidance \citep{dhariwal2021diffusion} by setting rewards as classifiers, derivative-free methods \citep{wu2024practical,li2024derivative}), they rely on a \emph{single-shot} denoising pass for generation. However, a natural question arises:

\emph{Can we further leverage inference-time computation during generation to refine the modelâ€™s output?}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/main_all_proposal.png}
    \caption{
    Our proposed framework follows an iterative process, with each iteration injecting noise into the sample and then denoising it while optimizing rewards. For sequences, this can be implemented via masked diffusion, initialized from pre-trained diffusion models (left). Our algorithm can continuously refine the outputs by gradually correcting errors introduced during reward-guided denoising, improving the design over successive iterations (middle). For instance, for the task of optimizing the similarity (RMSD) of a protein to a target structure (\textcolor{red}{Red}), we can progressively minimize the RMSD through refinement, optimizing the design from an initial (\textcolor{orange}{Orange}) fit to a better final fit (\textcolor{green}{Green}), as shown on the right.} 
    \label{fig:propsoal}
\end{figure}

+
In this study, we observe that diffusion models can inherently support an \emph{iterative} generation procedure, where the design can be progressively refined through successive cycles of masking and noise removal. This allows us to utilize arbitrarily large amounts of computation during generation to continuously improve the design. 



Motivated by the above observations, we propose a novel framework for test-time reward optimization with diffusion models. Our approach employs an iterative refinement algorithm consisting of two steps in each iteration: partial noising and reward-guided denoising as in \pref{fig:propsoal}. The reward-guided denoising step transitions from partially noised states to denoised states using techniques such as classifier guidance or derivative-free guidance. Unlike existing \emph{single-shot} methods, our approach offers several advantages. First, our sequential refinement process allows for the gradual correction of errors introduced during reward-guided denoising, enabling us to optimize complex reward functions, such as structural properties in protein sequence design. In particular, this correction is expected to be crucial in recent successful masked diffusion models \citep{sahoo2024simple,shi2024simplified}, as once a token is demasked, it remains unchanged until the end of the denoising step. Besides, for reward functions with hard constraints, commonly encountered in biological sequence or molecular design (e.g., cell-type-specific DNA design \citep{gosai2023machine,lal2024reglm} or binders with high specificity), our framework can effectively optimize such reward functions by initializing seed sequences within feasible regions that satisfy these constraints.

{ Our contribution is summarized as follows. First, we propose a new reward-guided generation framework for diffusion models that sequentially refines the generated outputs (\pref{sec:iterative}). Our algorithm addresses two major issues in existing methods such as the lack of a correction mechanism and difficulties of handling hard constraints. Secondly, we provide a theoretical formulation demonstrating that our algorithm samples from the desirable distribution $\exp(r(x))p^{\pre}(\cdot)$, where $p^{\pre}(\cdot)$ is a pre-trained distribution (\pref{sec:analysis}) and $r(\cdot)$ is a reward function.
Finally, we present a specific instantiation of our unified framework by carefully designing the reward-guided denoising stage in each iteration, which bears similarities to evolutionary algorithms (\pref{sec:practical}). Using this approach, we experimentally demonstrate that our algorithm effectively optimizes reward functions, outperforming existing methods in computational protein and DNA design (\pref{sec:experiment}).} 
