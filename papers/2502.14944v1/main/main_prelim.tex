

\section{Preliminaries}

We first provide an overview of diffusion models, then discuss current reward-guided algorithms in diffusion models and the potential challenges, which motivate our proposal. 


\subsection{Diffusion Models}\label{sec:diffusoin_models}

In diffusion models, the objective is to learn a sampler $p^{\pre}(\cdot) \in \Delta(\Xcal)$ for a given design space $\Xcal$ using available data. The training procedure is summarized as follows. First, we define a forward noising process (also called a policy) $q_t: \Xcal \to \Delta(\Xcal)$ that proceeds from $t=0$ to $t=T$. Next, we learn a reverse denoising process $p_t:\Xcal \to \Delta(\Xcal)$ parametrized by neural networks, ensuring that the marginal distributions induced by these forward and backward processes match.

To provide a concrete illustration, we explain masked diffusion models. However, we remark that our proposal in this paper can be applied to \emph{any} diffusion model.

\begin{example}[Masked Diffusion Models]\label{exa:masked}
    Here, we explain masked diffusion models \citep{sahoo2024simple, shi2024simplified,austin2021structured,campbell2022continuous,lou2023discrete}).

Let $\Xcal$ be a space of one-hot column vectors $\{x\in\{0,1\}^K:\sum_{i=1}^K x_i =1\}$, and $\mathrm{Cat}(\pi)$ be the categorical distribution over $K$ classes with probabilities given by $\pi \in \Delta^K$ where $\Delta^K$ denotes the K-simplex. A typical choice of the forward noising process is  
$q_t(x_{t+1}\mid x_{t}) = \mathrm{Cat}(\alpha_t x_{t}+ (1- \alpha_t)\mathbf{m} )$
where $\textstyle \mathbf{m}=[\underbrace{0,\cdots,0}_{K-1},\mathrm{Mask}]$. Then, defining $\bar{\alpha}_t = \Pi_{i=1}^t \alpha_i$, the backward process is parameterized as
\begin{align*}
\textstyle x_{t-1}=
   \begin{cases}  
     \delta(\cdot = x_t) \quad \mathrm{if}\, x_t\neq \mathbf{m}  \\
       \mathrm{Cat}\left ( \frac{(1-\bar \alpha_{t-1})\mathbf{m}  + (\bar \alpha_{t-1}- \bar \alpha_t) \hat x_0(x_t;\theta) }{ 1 - \bar \alpha_t } \right),\,\mathrm{if}\,x_t= \mathbf{m}, 
   \end{cases}  
\end{align*}
where $\hat x_0(x_t)$ is a predictor from $x_t$ to $x_0$. 

\end{example}

\vspace{-2mm}
\paragraph{Notation and remark.} $\delta_{a}$ denotes the Dirac delta distribution at mass $a$. With a slight abuse of notation, we express the initial distribution as $p_{T+1}:\Xcal \to \Delta(\Xcal)$, and denote   $[1,\cdots,T]$ by $[T]$. 


\subsection{Single-Shot Reward-Guided Generation} \label{sec:existing}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{main/alignment_existing.png}
    \caption{Existing reward-guided algorithms can be viewed as sequentially sampling from $x_T$ to $x_0$ following the soft optimal policy $\{p^{\star}_t\}_{t=T}^1$. The primary distinction among these algorithms lies in how $p^{\star}_t$ is approximated. }
    \label{fig:existing}
\end{figure}

Our goal is to generate a natural-like design with a high reward. In particular, we focus on inference-time algorithms that do not require fine-tuning of pre-trained diffusion models. Below, we provide a summary of these methods.

For reward-guided generation, we often aim to sample from
\begin{align}\label{eq:goal}
    p^{(\alpha)}&:=\argmax_{p\in \Delta(\Xcal) } \EE_{x\sim p}[r(x)]- \alpha \KL(p \| p^{\pre}) \\ 
    &= \exp(r(\cdot)/\alpha)p^{\pre}(\cdot)/C, \nonumber
\end{align}
where $C$ is the normalizing constant. This objective is widely employed in generative models, such as RLHF in large language models (LLMs) \citep{ziegler2019fine,ouyang2022training}. In diffusion models (e.g., \citet[Theorem 1]{uehara2024fine}), this is achieved by sequentially sampling from the \emph{soft optimal policy} $\{p^{\star}_{t}\}_t$ from $t=T+1$ to $t=1$, which is defined by 
\begin{align*}
    p^{\star}_{t}(\cdot \mid x_{t}) \propto \exp(v_{t-1}(\cdot)/\alpha)p^{\pre}_{t}(\cdot \mid x_{t}), 
\end{align*}
where 
\begin{align}\label{eq:value}
    v_{t}(x_{t}):=\alpha \log \EE_{x_0\sim p^{\pre}(x_0\mid x_{t})}[\exp(r(x_0)/\alpha)|x_{t}].
\end{align}
and the expectation is taken w.r.t. the pre-trained policy. Here, as illustrated in \pref{fig:existing}, $v_{t-1}$ serves as a look-ahead function that predicts the reward at $x_0$ from $x_t$, often referred to as the \emph{soft value function} in RL (or the optimal twisting proposal in SMC literature \citep{naesseth2019elements}). 

In practice, we cannot precisely sample from soft optimal policies because (1) the soft value function $v_t$ is unknown, and (2) the action space under the optimal policy is large. Current algorithms address these challenges as follows.

\vspace{-2mm}
\paragraph{(1): Approximating soft value functions.}\label{sec:approximate_soft}

A typical approach is to use $r(\hat x_0(x_t))$ by leveraging the decoder $\hat x_0(x_t)$ obtained during pre-training. This approximation arises from replacing the expectation over $x_0\sim p^{\pre}(x_0|x_{t})$ in \eqref{eq:value} with $\delta_{\hat x_0(x_t)}$ (i.e., a Dirac delta at the mean of $p^{\pre}(x_0|x_{t-1})$). Note its accuracy degrades as $t$ increases (i.e., as the state becomes more noisy). Despite its potential crudeness, this approximation is commonly adopted due to its training-free nature and the strong empirical performance demonstrated by methods such as DPS \citep{chung2022diffusion}, reconstruction guidance \citep{ho2022video}, universal guidance \citep{bansal2023universal}, and SVDD \citep{li2024derivative}.

\vspace{-2mm}
\paragraph{(2): Handling large action space.} 

Even with accurate value functions, sampling from the soft optimal policy still exhibits difficulty because its sample space $\Xcal$ is still large. Hence, we often resort to approximation techniques as follows.
\vspace{-1mm}
\begin{itemize}
    \item  Classifier Guidance: In continuous diffusion models, the pre-trained policy $p^{\pre}_{t-1}(\cdot \mid x_{t-1})$ is a Gaussian policy. By constructing \emph{differentiable} value function models, we can approximate $p^{\star}_{t}$ by shifting the mean using $\nabla v_t(\cdot)/\alpha$. A similar approximation also applies to discrete diffusion models \citep{nisonoff2024unlocking}.
\item Derivative-Free Guidance: Another approach is using importance sampling \citep{li2024derivative}. Specifically, we generate several samples from $p^{\pre}_{t-1}(\cdot \mid x_{t-1})$ and then select the next sample based on the importance weight $\exp\left(v_{t}(\cdot)/\alpha\right)$. A closely related method using Sequential Monte Carlo (SMC) has also been proposed, as discussed in \pref{sec:related_works}.
\end{itemize}