\section{Theoretical Analysis} \label{sec:analysis}

We present the theoretical analysis of \alg. We begin with the key theorem, which clarifies its target distribution.

\begin{theorem}[Target Distribution of \alg]\label{thm:key}
Suppose (a) the initial design $x^{\langle 0 \rangle}_0$ follows $p^{(\alpha)}$ (defined in \eqref{eq:goal}), (b) the marginal distributions induced by the forward noising process match those of the learned noising process in the pre-trained diffusion models. Then, the output $x^{\langle S \rangle}_0$ from \alg\,follows the target distribution $$p^{(\alpha)}(\cdot) \propto \exp(r(\cdot)/\alpha)p^{\pre}(\cdot).$$ 
\end{theorem}

First, we discuss the validity of the assumptions. The assumption (a) is readily satisfied when using the introduced strategy of initial designs in \pref{sec:iterative}. The assumption (b) is also mild, as pre-trained diffusion models are trained in this manner \citep{song2021maximum}, though certain errors may arise in practice. Another implicit assumption in practice is that we can approximate soft-optimal policies accurately.  

Next, we explore the implications of \pref{thm:key}. The central takeaway is that we can sample from a desired distribution for our task $p^{(\alpha)}$ in \eqref{eq:goal}. Although this guarantee appears to mirror existing single-shot algorithms discussed in \pref{sec:existing}, we anticipate differing practical performance in terms of rewards. This is due to their robustness against errors in soft value function approximation $v_t(x_t)\approx r(\hat x_0(x_t))$.

To clarify, recall that in reward-guided algorithms, we must employ \emph{approximated} soft value function models when sampling from the soft optimal policies $p^{\star}_t\propto \exp(v_{t-1}(\cdot)/\alpha)p^{\pre}_{t-1}(\cdot \mid x_t)$. The approximation often becomes more precise as the time step $t$ in the soft optimal policy approaches $0$, as mentioned in \pref{sec:approximate_soft}. Indeed, in the extreme case, when $t=0$, the exact equality holds. Therefore, by maintaining a sufficiently small noise level $t=K$ and avoiding the approximation of value functions at large $t$, \alg\,can effectively minimize approximation errors in practice.

\vspace{-2mm}
\paragraph{Sketch of the Proof of \pref{thm:key}.} The detailed proof is deferred to \pref{sec:proof}. In brief, first,
we show that the marginal distribution after noising is $p^{\pre}_K(\cdot)\exp(v_K(\cdot)/\alpha)/C$ where $p^{\pre}_K(\cdot)$ is a marginal distribution at $K$ induced by pre-trained policies. Then, by induction, during reward optimization, we show that $k\in [K]$: $x_k$ follows $p^{\pre}_k(\cdot)\exp(v_k(\cdot)/\alpha)/C$. Then, when $k=0$, it would be equal to $p^{\pre}(\cdot)\exp(r(\cdot)/\alpha)$. 