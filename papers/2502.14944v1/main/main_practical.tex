\vspace{-2mm}
\section{Practical Design of Algorithms}\label{sec:practical}
\vspace{-2mm}

As mentioned, \alg\ is a unified sequential refinement framework that can integrate off-the-shelf approximation strategies during reward-guided denoising (Line 4 in \pref{alg:decoding}). A key practical consideration is determining which approximation methods to adopt. In this context, we present a specific version that bears similarities to evolutionary algorithms.

\vspace{-2mm}
\subsection{Combining Local IS and Global Resampling} 

\begin{figure}[!th]
    \centering
    \includegraphics[width=\linewidth]{images/resampling2.png}
    \caption{Visualization of \pref{alg:decoding2}. A reward-guided denoising consists of two components: local value-weighted sampling for each sample (from 
$k=K$ to $k=1$) and global resampling among samples in a batch at $k=1$.}
    \label{fig:instance}
\end{figure}


\begin{algorithm}[!th]
\caption{Practical version of \alg }\label{alg:decoding2}
\begin{algorithmic}[1]
     \STATE {\bf Require}: Estimated value functions $\{\hat v_t\}_{t=T}^0$ (i.e., $\{r(\hat x_0(x_t)\}_{t=T}^0$), pre-trained diffusion models $\{p^{\pre}_k\}_{k=T+1}^1$, initial designs $\{x^{\langle 0\rangle }_{0,i}\}_{i=1}^N$ (the index $\langle \cdot \rangle $ means the number of iteration steps and the index $i \in [N]$ is an index in a batch), duplication number $L$ in IS, repetition number $S$, noise level $K$, $\alpha \in \RR$
     \FOR{$s \in [0,\cdots,S-1]$} 
       \STATE   \emph{Noising:} For each $i\in [N]$, sample $x^{\langle s+1 \rangle }_{K,i}$ from forward noising processes $q_K(\cdot \mid x^{\langle s \rangle }_{0,i})$.  
         \FOR{$k \in [K-1,\cdots,1]$} 
        \STATE IS: Sample $\forall i \in [N]$, $\{z_{k,i,l}\}_{l=1}^{L} \sim p^{\pre}_{k+1}(\cdot \mid x^{\langle s+1 \rangle }_{k+1,i}) $ and define next states from the weighted empirical distributions:
        {\small 
        \begin{align*}
            \forall i:   x^{\langle s+1 \rangle }_{k,i} \sim \sum_{l=1}^L w_l \delta_{z_{k,i,l}}, w_l =  \frac{\exp(r(\hat x_0(z_{k,i,l}))/\alpha)}{\sum_s \exp(r(\hat x_0(z_{k,i,s}))/\alpha). }    
        \end{align*}
        } 
         \ENDFOR  
        \STATE \emph{Selection}: $\forall i \in [N]$, sample $x_{0,i} \sim p^{\pre}_1(\cdot \mid x^{\langle s+1 \rangle }_{1,i}) $ and perform resampling: 
        {\small 
        \begin{align*}
          {\tiny \{x^{\langle s+1 \rangle }_{0,i}\}_{i=1}^N \sim  \sum_{i=1}^N w_i \delta_{x_{0,i}},\,w_i =  \frac{\exp(r (x_{0,i})/\alpha)}{\sum_s \exp(r(x_{0,s}))/\alpha)  }.  } 
        \end{align*}
        } 
        
        
     \ENDFOR 
  \STATE {\bf Output}: $\{ x^{\langle S \rangle}_{0,i} \}_{i=1}^N$
\end{algorithmic}
\end{algorithm} 

Our specific recommendation for approximating soft optimal policies during reward-guided denoising (Line 4 in \pref{alg:decoding}) is presented in \pref{alg:decoding2}. Here, we adopt a strategy that does \emph{not} require differentiable value function models, as reward feedback could often be provided in a black-box manner (e.g., molecular design). Specifically, we organically combine IS-based and SMC-based approximations. Given a batch of samples, we apply IS from $k=K$ to $k=1$ (Line 4-6) \emph{for each sample in the batch}, where the proposal distribution is a policy from pre-trained diffusion models. However, at the terminal step $k=1$, we perform selection via resampling (Line 7), which is central to SMC and evolutionary algorithms. This step involves \emph{interaction among samples in the batch}, as illustrated in \pref{fig:instance}. 


This combined strategy during reward-guided denoising leverages the advantages of both IS approaches \citep{li2024derivative} and SMC approaches \citep{wu2024practical}. First, if we use the pure IS strategy from  $k=K$ to $k=1$, when a sample in a batch is poor, it will not be permanently discarded during the refinement process. In contrast, in \pref{alg:decoding2}, the final selection step allows for the elimination of such poor samples through resampling. Second, if we use the pure SMC strategy from $k=K$ to $k=1$, resampling is performed at every time step, which significantly reduces the diversity among samples in the batch. We apply the SMC approach only at the final step.


\vspace{-2mm}
\paragraph{Relation to evolutionary algorithm.} The above version can be viewed as a modern variant of the evolutionary algorithm, which seamlessly integrates diffusion models. An evolutionary algorithm typically consists of two steps: (a) candidate generation via mutation and crossover and (b) selection. In \pref{alg:decoding2}, the step (a) corresponds to Lines 3-6, where reward-guided generation is employed, and the step corresponds to Line 7. 


\begin{remark}
When the reward feedback is differentiable, we can effectively integrate classifier guidance into the proposal distributions. For further details, see the Appendix in \citet{li2024derivative}.
\end{remark}


\subsection{Constrained Reward Optimization}\label{sec:hard_constraint}

We often need to include hard constraints so that generated designs fulfill certain conditions. This is especially crucial in molecular design, where we may require low-toxicity small molecules or cell-typeâ€“specific DNA sequences, as shown in \pref{sec:DNA}. Here, we explore how to enable generation under such constraints. Formally, we define the constraint set as $\Ccal = \{x:r_2(x)<c\}$. Given another reward $r_1(\cdot)$ to be optimized, our objective is to produce designs with high $r_1(\cdot)$ while ensuring $r_2(x)<c$. 

\vspace{-2mm}
\paragraph{Na\"ive approaches with single-shot algorithms.} As an initial consideration, we examine how to address this problem using existing single-shot methods. A straightforward approach is to use the following reward
\begin{align*}
    r(\cdot) = r_1(\cdot)I(r_2(\cdot)<c)  
\end{align*}
or use a log barrier formulation: 
\begin{align*}
    r(\cdot) =  r_1(\cdot) + \log (\max(c- r_2(\cdot),c_1)), 
\end{align*}
where $c_1$ is a suitably small value, and then sample from $t=T$ to $t=0$ by following approxima soft-optimal policies.  However, in reality, the outputs at $t=0$ often fail to satisfy these constraints, regardless of how the rewards are defined. This shortcoming arises because the value function models used during reward-guided denoising are not completely accurate.

\vspace{-2mm}
\paragraph{Integration into our proposal (\pref{alg:decoding2}).}

Now, we consider incorporating the above rewards into our framework in \pref{alg:decoding2}. Here, compared to single-shot algorithms, we can often begin with feasible initial designs that satisfy the constraints $x \in \Ccal$. Then, by keeping the noise level $K$ in \pref{alg:decoding2} small, we can avoid deviating substantially from these feasible regions. This gradual refinement strategy makes it easier to produce designs that fulfill hard constraints.