
\subsection{Challenges of Single-Shot Generation}\label{sec:challenge}

There are two main challenges with the aforementioned current algorithms. First, for certain complex reward functions, they may fail to fully optimize the rewards. This occurs because the value functions employed in these algorithms have approximation errors. When a value function model is inaccurate, the decision at that step can be suboptimal, and there is no correction mechanism during generation. This issue can be particularly severe in recent popular masked discrete diffusion models in Example~\ref{exa:masked}, where once a token changes from the masking state, it remains unchanged until the terminal step ($t=0$) \citep{sahoo2024simple,shi2024simplified}. Consequently, any suboptimal token generation at intermediate steps cannot be rectified.

Another related challenge lies in accommodating hard constraints with a set $\Ccal \subset \Xcal$. Although one might assume that simply setting $r(\cdot)=\mathrm{I}(\cdot \in \Ccal)$ would suffice, in practice, the generated outputs often fail to meet these constraints. This difficulty again arises from the inaccuracy of value function models at large $t$ (i.e., in highly noised states).


\section{Iterative Refinement in Diffusion Models}\label{sec:iterative}



To tackle challenges discussed in \pref{sec:challenge}, we propose a new iterative inference-time framework for reward optimization in diffusion models. Our algorithm is an iterative algorithm where each step consists of two procedures: noising using forward pre-trained policies and reward-guided denoising using soft optimal policies. This framework is formalized in \pref{alg:decoding}.  

\begin{algorithm}[!th]
\caption{Reward-Guided Evolutionary Refinement in Diffusion models (\alg)}\label{alg:decoding}
\begin{algorithmic}[1]
     \STATE {\bf Require}: %
     initial designs $x^{\langle 0 \rangle }_0$ (the index $\langle \cdot \rangle $ means the number of iteration steps), noise level $K$ 
     \FOR{$s \in [0,\cdots,S-1]$} 
       \STATE  \emph{Noising}: Sample $x^{\langle s+1 \rangle }_K$ from  $q_K(\cdot \mid x^{\langle s \rangle }_0)$ where $q_K$ is a noising policy from $x_0$ to $x_K$ (See \pref{sec:diffusoin_models}). 
        \STATE \emph{Reward-Guided Generation}: Sequentially sample from $\{p^{\star}_{t}\}_{t=K}^1$ (i.e., from $x^{\langle s+1 \rangle }_K$ to $x^{\langle s+1 \rangle }_0$) (In practice, we need to \emph{approximate} it. Refer to \pref{alg:decoding2}). 
     \ENDFOR 
  \STATE {\bf Output}: $\{ x^{\langle S \rangle}_0\}$
\end{algorithmic}
\end{algorithm} 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{main/intuition_algorihmss.png}
    \caption{Summary of \alg: We instantiate it within masked diffusion models. It alternates reward-guided denoising and noising. }
    \label{fig:refinement2}
\end{figure}

Compared to existing algorithms that only perform single-shot denoising from $t=T$ to $t=0$, our algorithm repeatedly performs reward optimization, as depicted in \pref{fig:refinement2}. The challenge of single-shot algorithms -- namely, the lack of a correction mechanism discussed in \pref{sec:challenge} -- can be addressed in \alg,\,by sequentially refining the outputs.


In \pref{alg:decoding}, several choices are important, which are outlined below. 
\vspace{-2mm}
\begin{itemize}
    \item \textbf{Initial designs $x^{\langle 0 \rangle}_0$}: Here, we consider two approaches.  The first choice is to run $\{p^{\star}_t\}$ from $t=T$ to $t=0$ as in single-shot inference-time alignment algorithms. 
    Second, if we have access to real data  $\{z^i\}\sim p^{\pre}(\cdot)$, we select samples with high rewards as initial designs. A straightforward way is by using the weighted empirical distribution: 
    \begin{align}
        \sum_i \frac{\exp(z^i)/\alpha) }{\sum_j \exp(z^j)/\alpha)} \delta_{z^i}. 
    \end{align}
       
    \item   \textbf{Approximation of the soft optimal policy $p^{\star}_t$ in Line 4}: As mentioned in \pref{sec:approximate_soft}, exact sampling from $p^{\star}_t$ is infeasible. However, we can employ any off-the-shelf methods to approximate it, such as classifier guidance or IS-based approaches discussed in \pref{sec:existing}. A specific instantiation of this approximation is considered in \pref{sec:practical}.
    
    \item   \textbf{Noise level $K$}: When $K$ is close to $0$, the inference time per loop is reduced. Moreover, because value function models used to approximate soft-optimal policies are typically more precise around $K=0$ (see \pref{sec:approximate_soft}), the reward optimization step becomes more effective. On the other hand, using a larger $K$ allows for more substantial changes in a single step. In practice, striking the balance, we recommend setting $K/T$ low. 
\end{itemize}

Next, we provide theoretical clarifications of our framework in \pref{sec:analysis}. Additionally, we present a practical instantiation of our framework in \pref{sec:practical}.


