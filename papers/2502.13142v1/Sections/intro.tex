% \vspace{-0.3cm}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1 \linewidth]{Figures/teaser_v2.pdf}
    
    \caption{\textbf{Overview of \smodel{}.} We introduce an \textbf{A}uto-regressive \textbf{R}obotic \textbf{M}odel that leverages low-level \textbf{4}D \textbf{R}epresentations (3D point tracks across time) learned from human videos to yield a better pre-trained robotic model.}
    % \vspace{-0.3cm}
    \label{fig:teaser}
     % \vspace{-0.1cm}
\end{figure*}


% \vspace{-0.3cm}
\section{Introduction}
\label{sec:intro}

% 1. OpenVLA/Foundational Robotic Models how cool they are....We haven't seen any pretraining capablities from these models

% 2. Foundataoin models for robotics would need a lot of data...currently we don't have a lot of robotic data..in order to overcome this limitation, many models attempted to learn meaningful representations from diverse human data. 

%3. For example, VLAs leverage language decoders that are pre-trained on high-level tasks like VQA, image captioning, or other robotic models that are pre-trained on video representations or other hand-engineered representations. However, the resulting pre-training is inadequate due to the gap between the high-level pre-training objective of these models and the objective of enabling robotic models to handle low-level robotic settings. 


% 4. In particular, we introduce \smodel{}, an Auto-regressive Robotic Model, that utilizes pre-training on 4D representations using an auto-regressive mechanism. We focus on lifting 2D representations into 3D using monocular depth estimators, and tracking 3D trajectories in videos (+1D = 4D).

% \giscard{Mentioned LLM here because it's easier to write but we can make the same point with VLMs}

% 1- VLAs, others

% 2- others

% 3 - VLA


% Recently, foundation models (FMs) have shown remarkable success in various domains, particularly in the domain of vision-and-language models~\cite{radford2021clip,chen2022pali,Alayrac2022FlamingoAV,liu2023llava,li2023blip2} pre-trained on vast amounts of vision and text data. 

Recently, foundation models (FMs) have shown remarkable success, particularly in the domains of language~\cite{neurips2020gpt3,Touvron2023LLaMAOA}, vision~\cite{Kirillov_2023_ICCV}, and multi-modal models~\cite{chen2022pali,Alayrac2022FlamingoAV,liu2023llava,li2023blip2,OpenAI2023GPT4TR} pre-trained on vast amounts of vision and text data. These models exhibit impressive zero-shot and few-shot learning capabilities~\cite{radford2021clip,InstructGPT_ouyang2022training,wei2021finetuned,chung2024scaling}, highlighting the power of pre-training on generic data. However, numerous attempts in robotics~\cite{Xiao2022mvp,kimOpenVLAOpenSourceVisionLanguageAction2024,zhen_3d-vla_2024,niuLLARVAVisionActionInstruction2024,lapa} have yet to achieve the same pre-training success seen in other domains. This could potentially be attributed to the scarcity of large-scale, diverse robotic data, unlike the abundance of text and image data available for vision and language FMs. 

% However, in robotics, similar levels of pre-training success have remained elusive despite numerous attempts~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024,zhen_3d-vla_2024,niuLLARVAVisionActionInstruction2024,lapa,Xiao2022mvp}. This is largely attributed to the scarcity of large-scale, diverse robotic data, unlike the abundance of unstructured text and image data available for vision and/or language FMs. 

% However, despite numerous attempts in robotics~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024,zhen_3d-vla_2024,niuLLARVAVisionActionInstruction2024,lapa,Xiao2022mvp}, there is still a gap between the success of pre-trained models for robotics and for other domains.



The lack of robotic data poses a significant bottleneck in training foundation models that can effectively generalize across diverse robotic platforms and tasks. To overcome this limitation, several recent approaches~\cite{Xiao2022mvp,lapa} employ representation learning by pre-training on an abundance of human data, enabling transfer to robotic systems. These approaches aim to recognize the inherent similarities between human and robot manipulation tasks and exploit the vast repositories of human video data available on the internet. Yet, these approaches have not been able to demonstrate effective generalization to downstream tasks. In part, this is due to their representations lacking an understanding of the physical world~\cite{zhen3DVLA3DVisionLanguageAction2024}, and therefore being less effective for robotics.

% A key limitation of these approaches is their failure to adequately capture spatial and temporal dynamics, which consequently restricts their efficacy as a pre-training paradigm for robotics.




% However, these approaches have not yet been convincingly demonstrating strong pre-training capabilities. This mainly due to the fact that those representations lack the spatial and temporal dynamics of objects in the world and are therefore less effective for robotics.

% These approaches recognize the inherent similarities between human and robot manipulation tasks, and aim to exploit the vast repositories of human video data available on the internet and in curated datasets.

% In contrast to these approaches, VLAs implicitly leverages human data in robotics by integrating pre-trained components from VLMs. 




In contrast with these methods, Vision-Language-Action (VLAs) models take a slightly different approach, implicitly leveraging human data in robotics by incorporating pre-trained components from Vision-and-Language Models (VLMs). In particular, they use language decoders pre-trained on tasks like visual question answering (e.g., RT-2~\cite{brohan_rt-2_2023}) and image captioning (e.g., OpenVLA~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024}). Despite such efforts, there is a discrepancy between these models' high-level pre-training objective and the goal of enabling robotic models to handle low-level action prediction. While these initial objectives are valuable for comprehending visual and linguistic content, they don't directly address the nuances of low-level robot control, which involves aspects like precise manipulation and spatial reasoning. To address this, this paper's method employs a lower-level pre-training objective by starting with a model that utilizes next-token prediction to learn 4D representations from human video data. These representations can then be transferred to more specialized scenarios by fine-tuning on robotic scenes and subsequently on proprioceptive data, while maintaining the same training objective.






% To address the limitations of existing pre-training methods for robotics, this paper introduces \smodel{}, an Auto-regressive Robotic Model that leverages pre-training on 4D representations using an auto-regressive mechanism.

In this paper, we introduce \smodel{} (\textbf{A}uto-regressive \textbf{R}obotic \textbf{M}odel with \textbf{4}D \textbf{R}epresentations).\footnote{\smodel{} is pronounced ``armor''.} The key insight behind \smodel{} is to learn a low-level representation from the abundance of human video data that can capture properties of the physical world. This involves lifting 2D representations to 3D using monocular depth estimation and subsequently tracking the 3D points. The resulting 4D representations maintain a shared geometric structure --- up to a linear transformation --- between the 3D points and robot state representations used downstream, enabling efficient transfer learning from human video data to robotic manipulation tasks. Surprisingly, pre-training our method solely on human data yields superior results compared to other models like VLAs~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024} that are pre-trained on robotic data such as OpenX~\cite{open_x_embodiment_rt_x_2023}.



% We summarize our main contributions as follows: (i) We introduce a novel robotics pre-training approach that leverages low-level 4D representations by tracking 3D points over time in videos; 
We summarize our main contributions as follows: (i) We introduce a novel robotics pre-training approach that incorporates low-level 4D representations that enhance understanding of the physical world while also learning from unlabeled videos.
% (ii) We show how human video data can be unlocked for pre-training in robotics; 
% (ii) We unlock the ability to leverage human video data for robotics; 
% (ii) This work demonstrates the potential of how human video data can be effectively leveraged for robotics pre-training.
(ii) Our approach shows that pre-training solely on human video data can lead to better performance than other methods that are pre-trained only on robotic data;
% We demonstrate the potential of our method by showing that training exclusively on human data yields superior results relative to other methods that are trained solely on robotic data.
% This work demonstrates the potential of our method solely on human data yields superior results compared to other methods that are pre-trained on robotic data.
(iii) Our method on average surpasses baselines like PerAct~\cite{shridhar2023perceiver} on 12 different tasks in RLBench's simulated environment, and OpenVLA~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024} on real tasks with a 7-DoF Kinova Gen3 robot; 
(iv) Our model also exhibits several advantageous properties, including cross-robot generalization and 3D point track prediction for out-of-domain human and robotic videos.


% (iii) \smodel{} offers a promising path towards developing general-purpose robotic agents capable of learning and adapting to a wide range of real-world scenarios.

% Might use this:
% By maintaining a 4D representation throughout each training phase, \smodel{} eliminates the gap between high-level pre-training objectives and low-level robotic control requirements in other VLAs. 

% Trevor: there was this claim that "Visual Prediction is enough" but it wasn't, even on simple tasks. But adding 4d works. We conjecture it will work on all the more complicated tasks. 

% We summarize our main contributions as follows: (i) We introduce a novel method that yields a sparse set of attention vectors (less than 1\%) for each individual task can serve as highly effective features for discriminative tasks; (ii) We demonstrate that our method can help close the gap with discriminative VLMs on classification tasks using only few-shot examples at test time; (iii) Our method surpasses zero-shot, few-shot, and LoRA fine-tuned baselines across multiple tasks (+7\% improvement on average over LoRA on challenging benchmarks like BLINK~\cite{fu2024blink}, VLGuard~\cite{zong2024safety}, and NaturalBench~\cite{li2024naturalbench}); (iv) We establish several advantageous properties of our approach, including strong generalization capabilities and favorable scaling characteristics.



% 2. 


% \begin{figure}[t!]
%     \vspace{-0.7cm}
%     \centering
%     \includegraphics[width=\linewidth]{Figure/teasor-10-12-2024.pdf}
%     \caption{\textbf{Overview of {\smodel}.}  We introduce a novel instruction tuning method that leverages structured prompts to unify a range of robotic learning tasks, scenarios, and environments and 2-D visual traces to further align vision and action spaces. The model works via a language instruction that contains {\texttt{\color{robottype}robot model}}, {\texttt{\color{controltype}control mode}}, {\texttt{\color{robottask}robot task}}, {\texttt{\color{robotaction}proprioceptive information}}, and        {\texttt{\color{predsteps} number of predicted steps}}, and outputs text with the next {\texttt{\color{robotaction}robot action(s)}} and the {\texttt{\color{visualtrace}visual trace}} for the remainder of the episode.}
%     \label{fig:teaser}
%      \vspace{-0.1cm}
% \end{figure}

