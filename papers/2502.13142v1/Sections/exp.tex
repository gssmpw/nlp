\def\realcompleteresults#1{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[#1]
\caption{\textbf{Success rate (\%) on the real Kinova Multi-Task setting}. We compare \smodel{}'s performance to ATM and OpenVLA, two related baselines, on 13 real tasks grouped into five categories. We use 25 episodes per task for evaluation, averaging the results over 3 seeds to get the final success rate. \smodel{} outperforms both baselines on all the tasks.
 }
 \vspace{-0.1cm}
\tablestyle{6.6 pt}{1.2}
% \small
\begin{center}
\begin{tabular}{c|ccccccc}
\shline
\multirow{3}{*}{Method} & \multicolumn{3}{c|}{pick cube up} & \multicolumn{2}{c|}{destack} & \multicolumn{2}{c}{stack}\\
& yellow  & cyan & \multicolumn{1}{c|}{green} & yellow  & \multicolumn{1}{c|}{cyan} & yellow on cyan             & cyan on yellow                  \\ \hline

ATM & 5.3 $\pm$ 3.5 & 6.7 $\pm$ 2.7 & \multicolumn{1}{c|}{9.3 $\pm$ 1.3} & 4.0 $\pm$ 2.3 & \multicolumn{1}{c|}{9.3 $\pm$ 3.5}   & 1.3 $\pm$ 1.3 & 2.6 $\pm$ 1.3 \\

OpenVLA  & 77.8 $\pm$ 6.4 & 45.8 $\pm$ 4.2 & \multicolumn{1}{c|}{91.7 $\pm$ 8.3} & 55.6 $\pm$ 2.8 & \multicolumn{1}{c|}{51.3 $\pm$ 2.6}  &  {27.8 $\pm$ 2.8} & {38.5 $\pm$ 4.4}  \\ \shline 

\rowcolor{gray!10} Ours &\textbf{ 92.6 $\pm$ 3.7 }& \textbf{100 $\pm$ 0.0} & \multicolumn{1}{c|}{\textbf{95.8 $\pm$ 4.2}} & \textbf{94.4 $\pm$ 2.7} & \multicolumn{1}{c|}{\textbf{94.9 $\pm$ 5.1}} & \textbf{63.6 $\pm$ 5.2} & \textbf{59.5 $\pm$ 2.4} \\ \hline

\multirow{3}{*}{Method} & \multicolumn{4}{c|}{pick toys then place to target} & \multicolumn{2}{c|}{push}     & \multicolumn{1}{c}{Average}
\\

& spiderman & penguin & pig & \multicolumn{1}{c|}{play basketball} & push red button        & \multicolumn{1}{c|}{push red the blue}  \\ \hline

ATM  & 5.3 $\pm$ 1.3 & 6.7 $\pm$ 1.3 & 5.3 $\pm$ 3.5 & \multicolumn{1}{c|}{24.0 $\pm$ 4.6} & 4.0 $\pm$ 2.3 
& \multicolumn{1}{c|}{ 0.0 $\pm$ 0.0} & {6.4 $\pm$ 2.2}  

\\

OpenVLA & {2.7 $\pm$ 1.3}  & {17.3 $\pm$ 1.3}  & {2.7 $\pm$ 2.7}  &  \multicolumn{1}{c|}{49.3 $\pm$ 3.5}  & 23.1 $\pm$ 4.4 & \multicolumn{1}{c|}{0.0 $\pm$ 0.0}    & {37.2 $\pm$ 3.4}
\\ \shline 


\rowcolor{gray!10} Ours & \textbf{90.7 $\pm$ 1.3} & \textbf{94.7 $\pm$ 1.3} & \textbf{93.3 $\pm$ 1.3} & \multicolumn{1}{c|}{\textbf{92.0 $\pm$ 2.3}} & \textbf{84.6 $\pm$ 4.4} & \multicolumn{1}{c|}{\textbf{25.0 $\pm$ 4.8}} & \textbf{83.1 $\pm$ 3.0}      \\ 
\shline              
\end{tabular}

\label{table:real-all-results}


\end{center}
\vspace{-0.2cm}
\end{table*}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\realcuberesults#1{
\begin{table}[#1]
\caption{\textbf{Pre-training approaches comparison.} We compare \smodel{} to several other robotic models that leverage pre-training on three tasks with a Kinova robot. We find that our approach yields the best average success rate.}
\vspace{-0.1cm}
\tablestyle{6.6 pt}{1.2}
\begin{tabular}{ccccl}
\multicolumn{1}{l}{Method} & pick cube & stack cubes & destack cubes &  \\ \shline
MVP                  & 75.00         & 18.75           & 81.25             &  \\
RPT                 & 87.50         & 31.25           & 93.75             &  \\
Octo                 & 56.25         & 12.50          & 37.50             &  \\
ATM                  & 7.11         & 2.00           & 6.67             &  \\
OpenVLA              & 68.75         & 31.25           & 53.33             &  \\
LLARVA               & 93.75         & 56.25           & \textbf{100.00}             &  \\ \shline
\rowcolor{gray!10}\smodel                 & \textbf{96.0 $\pm$ 2.3}         & \textbf{61.3 $\pm$ 1.3}           & 94.7 $\pm$ 1.3            & 
\end{tabular}
% \vspace{4pt}

\label{table:real-cube-results}
\vspace{-0.2cm}
\end{table}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\tabcrossrobot#1{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[#1]
\tablestyle{3.8 pt}{1.2}
% \small
\begin{center}
\begin{tabular}{cccccc}
\shline
\multirow{2}{*}{robot} & \multicolumn{2}{c}{pretrained} & \multirow{2}{*}{pick} & \multirow{2}{*}{stack} & \multirow{2}{*}{destack} \\
                       & Epic   & Kinova  &                     &                        &                          \\ \shline

Kinova                 & {\coloredcheckmark{ForestGreen}}                  & \coloredcheckmark{ForestGreen}                 & 96.0 $\pm$ 2.3                       &  61.3 $\pm$ 1.3                       &     94.7 $\pm$ 1.3                     \\
Franka                 & {\coloredcross{Firebrick}}                & {\coloredcross{Firebrick}}                & 73.3 $\pm$ 2.7                      &      49.3 $\pm$ 5.8                  &            65.3 $\pm$ 3.5              \\
Franka                 &    \coloredcheckmark{ForestGreen}             &  \coloredcheckmark{ForestGreen}               & 93.3 $\pm$ 1.3 & 56.0 $\pm$ 2.3   &    97.3 $\pm$ 1.3                       
\end{tabular}
\caption{\textbf{Success rate (\%) of {\smodel} on cross-robot setting.} We fine-tune the pre-trained model for motor control for different robots. We show the success rate of cube tasks.}
% \vspace{-3em}
\label{tab:gen}
\end{center}
% \vspace{-2cm}
\end{table}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\tabcrossrobottwo#1{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[#1]
\caption{\textbf{Success rate (\%) of {\smodel} on cross-robot setting.} We fine-tune the pre-trained model for motor control on different robots and report success rates of cube tasks.}
% \vspace{-0.3cm}
\tablestyle{2.8 pt}{1.2}
\begin{center}
\begin{tabular}{cccccc}
\shline
% Reordered columns: first two are "pretrained" (Epic, Kinova), then "robot", "pick", "stack", "destack"
Pre-train & FT & Robot & pick & stack & destack \\ 
\shline
% Row 1
Epic & Kinova & Kinova & 96.0 $\pm$ 2.3 & 61.3 $\pm$ 1.3 & 94.7 $\pm$ 1.3 \\
% Row 2
-- & -- & Franka & 73.3 $\pm$ 2.7 & 49.3 $\pm$ 5.8 & 65.3 $\pm$ 3.5 \\
% Row 3
Epic & Kinova & Franka & 93.3 $\pm$ 1.3 & 56.0 $\pm$ 2.3 & 97.3 $\pm$ 1.3 \\
\shline
\end{tabular}
\vspace{-0.3cm}
\label{tab:gen}
\end{center}
\end{table}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\tabsimresults#1{
\begin{table*}[#1]
\tablestyle{1.6pt}{1.2}
% \small
\begin{center}
\begin{tabular}{lcccccccccccc|c}

\multirow{2}{*}{Method} & \multicolumn{13}{c}{Task}\\
                        & \begin{tabular}[c]{@{}c@{}}open\\ drawer\end{tabular} & \begin{tabular}[c]{@{}c@{}}meat off\\ grill\end{tabular} & \begin{tabular}[c]{@{}c@{}}turn\\ tap\end{tabular} & \begin{tabular}[c]{@{}c@{}}put\\ money\end{tabular} & \begin{tabular}[c]{@{}c@{}}push\\ buttons\end{tabular} & \begin{tabular}[c]{@{}c@{}}sweep\\ dustpan\end{tabular} & \begin{tabular}[c]{@{}c@{}}slide\\ block\end{tabular} & \begin{tabular}[c]{@{}c@{}}close\\ jar\end{tabular} & \begin{tabular}[c]{@{}c@{}}screw\\ bulb\end{tabular} & \begin{tabular}[c]{@{}c@{}}place\\ wine\end{tabular} & \begin{tabular}[c]{@{}c@{}}reach and\\ drag\end{tabular} & \begin{tabular}[c]{@{}c@{}}stack\\ blocks\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Success Rate (\%)\end{tabular} \\ \shline

                        
Image-BC (ViT)
                        & 0                                                     & 0                                                        & 16                                                 & 0                                                   & 0                                                      & 0                                                       & 0                                                     & 0                                                   & 16                                                   & 0                                                    & 0                                                        & 0             & 2.67                                       \\ 
C2FARM-BC               & 20                                                    & 20                                                       & 68                                                 & 12                                                  & \textbf{72}                                                     & 0                                                       & 16                                                    & 24                                                  & 8                                                    & 18                                                   & 24                                                       & 4    & 23.83                                                    \\



ManiGaussian                  & 76                                          & 60                                              & 56                                        & -                                         & 20                                            & 64                                             & 24                                          & 28                                         & -                                          & -                                          & \textbf{92}                                              & 12 & 48.00
\\
                        
LLARVA                  & 60                                           & 80                                              & 56                                        & 44                                         & 56                                            & \textbf{84}                                             & \textbf{100}                                          & 28                                         & 8                                           & 12                                          & 52                                              & 0 & 48.33 
\\ 
PerAct                  & 80                                                    & 84                                                       & \textbf{80}                                                 & 44                                                  & 48                                                     & 56                                                      & 72                                                    & \textbf{60}                                                  & \textbf{24}                                                   & 12                                                   & 68                                                       & \textbf{36}    & 55.33                                               \\
\hline

                        
\rowcolor{gray!10}\smodel                  & \textbf{88.8}                                           & \textbf{94.4}                                              & 61.6                                        & \textbf{92.0}                                         & 67.2                                            & 72.0                                             & 85.6                                          & 24.0                                         & 10.4                                           & \textbf{36.0}                                          & 77.6                                              & 4.0 & \textbf{59.47} 
\end{tabular}
% \vspace{4pt}
\caption{\textbf{Success rate (\%) on RLBench Multi-Task setting.}}
%\vspace{-0.8cm}
\label{tab:sim-results}
\end{center}
\end{table*}
}



\section{Experiments and Results}

\label{sec:exp}
We evaluate {\smodel} on 12 tasks in RLBench~\cite{james2020rlbench} and compare to relevant 2D and 3D baselines. We also test and ablate our model on two real robots: a 7-DoF Kinova Gen3 robot, and a 7-DoF Franka Emika Panda robot. 

\subsection{Implementation Details}
\label{sec:eval:impl}
{\smodel} is implemented using PyTorch~\cite{paszke2019pytorch}. We use ViT-Base as our vision encoder, which is pretrained as described in Section~\ref{sec:model:arch}. We use SpatialTracker~\cite{xiaoSpatialTrackerTrackingAny2024} as our off-the-shelf 3D point tracker. We note that the model uses a maximum context window $C$, which is the number of previous timesteps it considers when predicting the next action. In practice, we use $C=16$ for most tasks, increasing it to $C=32$ for some long-horizon tasks (details in~\cref{supp:sim_on_rlbench}). The model is also trained to predict the next 16 actions, but we only execute the first prediction during evaluation. In both our simulation and real settings, we use end-effector control, with the model predicting the Cartesian position and rotation of the end-effector, and a binary value for the gripper. Finally, we use 4 NVIDIA A6000 GPUs for training and a single NVIDIA A6000 GPU for evaluation. More information, like training and fine-tuning recipes, is in~\cref{supp:training_recipe}. 




\subsection{Simulation Evaluation}
\label{sec:rlbench}

\minisection{Experimental Setup} We evaluate \Ours on 12 RLBench tasks, and follow the settings in PerAct~\cite{shridhar2023perceiver}. A task is defined as a collection of demonstrations of the robot interacting in a given scene, with object variations (such as color or size). We train \smodel{} for each task using 190 successful demos for every variation of the task (for more details, see~\cref{supp:sim_on_rlbench}), and evaluate using 25 episodes per task in the validation set. Every episode is scored either 0 for failure or 100 for success. We use 5 seeds, which are averaged to get the final success rate.

\realcompleteresults{!ht}

\minisection{Baselines} We compare to several baselines for our simulation evaluation. Image-BC (ViT) is a 2D language-conditioned baseline model that uses a ViT vision encoder, reported in PerAct~\cite{shridhar2023perceiver}. To compare against two different methods that use 3D representations, we select C2FARM-BC~\cite{james2022coarse} and PerAct, which use voxels as 3D input to calculate robot actions. To compare to a method with 3D temporal tracking similar to ours, we evaluate against ManiGaussian~\cite{lu2025manigaussian}, which uses a dynamic Gaussian splatting representation to predict robot actions.
Lastly, LLARVA~\cite{niuLLARVAVisionActionInstruction2024} is a recent state-of-the-art VLA that directly predicts low-level robot actions given an image and proprioceptive information as part of a language prompt.

\minisection{Results} We report our simulation results in~\tabref{tab:sim-results}. \smodel{} achieves the highest average success rate across all the tasks, and the best success rate for 4 out of 12 tasks. In particular, \smodel{} surpasses PerAct, which directly uses voxel information from the simulation environment as input. This approach is not scalable since voxel data is expensive to collect in the real world. Instead, \smodel{} learns to model the 3D world by pre-training on 3D point tracking, and the impressive performance highlights the model's strong grasp of physical understanding. We also note that \smodel{}'s superior performance compared to LLARVA --- a VLA which uses a pre-trained language decoder --- emphasizes the effectiveness of our representation and pre-training approach.



\subsection{Real Robot Evaluation}
\label{exp:real}


\minisection{Experimental Setup} For our real experiments, we use a 7-DoF Kinova Gen3 robot mounted with a Robotiq 2F-85 adaptive gripper. We test our model and the baselines on 13 total tasks, grouped into five broad categories based on the dominant action: \textit{pick}, \textit{destack}, \textit{stack}, \textit{pick and place}, and \textit{push}. For each task, training is performed using 190 episodes of every variation. Evaluation is conducted over 25 episodes per task, with results averaged across three different seeds to calculate the final success rate.

\minisection{Baselines}
We evaluate our model against two baselines in real-world settings: ATM~\cite{wenAnypointTrajectoryModeling2024} and OpenVLA~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024}. ATM utilizes a hierarchical framework to predict 2D point trajectories, which are then used to condition a policy. In contrast, \smodel{} predicts 3D point trajectories, a more intuitive and natural representation for robotic tasks. OpenVLA, a state-of-the-art 7B-parameter VLA model, is pre-trained on the OpenX dataset, while \smodel{} is trained on a significantly smaller dataset, with pre-training consisting exclusively of human video data. More details for these implementations are in~\cref{supp:reproduce_atm_openvla}.


% \yuvan{@dantong has to add comparison of atm ft episodes vs our ft episodes}
% We report our results in Table \ref{table:real-all-results}.
% Original:
% Table~\ref{table:real-all-results} shows that \smodel{} outperforms both baselines across all tasks, \textbf{achieving an average success rate of 83.1\%}, compared to OpenVLA's 37.2\% and ATM's 6.4\%. ATM in particular does not perform well in our real setting. We believe that this significant gap in performance is due to two reasons. First, our fine-tuning process is conducted with a significantly smaller number of demonstrations than what was used in ATM's evaluation setting, ATM trains a multi-task policies with larger amount of total number of episodes. More critically, however, is the difference in how we track points: \smodel{} utilizes 3D coordinates, while ATM relies on 2D coordinates. The use of 3D coordinates provides a more natural and accurate representation for robotic tasks, which we believe is a primary factor contributing to the improved performance of our model.

\minisection{Results} \tabref{table:real-all-results} shows that \smodel{} outperforms both baselines across all tasks, \textbf{achieving an average success rate of 83.1\%}, compared to OpenVLA's 37.2\% and ATM's 6.4\%. ATM in particular does not perform well in our real setting despite training with a significantly larger number of demonstrations than we use in our fine-tuning. We believe that this significant gap in performance is due to how we track points: \smodel{} utilizes 3D coordinates, while ATM relies on 2D. The use of 3D coordinates provides a more natural and accurate representation for robotic tasks, which may contribute to our model's improved performance.

% The use of 3D coordinates provides a more natural and accurate representation for robotic tasks, which we believe is a primary factor contributing to the improved performance of our model.





% 
% Original:
% However, we believe that our superior performance over OpenVLA can again be attributed to our use of low-level 4D representations, as they enable our model to gain a deeper understanding of spatial dynamics.
% 

In contrast to ATM, OpenVLA uses a similar number of fine-tuning episodes to our evaluation setting. However, we believe that our superior performance over OpenVLA can again be attributed to our use of low-level 4D representations, which enable 3D scene understanding.

% We note that existing 3D methods are not easy applicable to real-world setting, unlike our work.

% localization/grounding,  
% This is not fully achievable when training on single view images alone, as OpenVLA does.



\realcuberesults{!t}

\subsection{Ablation Studies}





% \roei{Here are the ablations I would expect to see here:}
% \roei{1 - The importance of human video data.}
% \roei{2 - why fine-tuning on 3d tracks for robotic data is necessary?}
% \roei{3 - one ablation or experiment that is missing to me is what is the connection between the 3D points and robotic actions...I think we should show smth that explain how they are connected. This would Highly motivate what we do here.}
% \roeih{4 - Can we move 


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/ablation_bar_charts.pdf}
    \caption{\textbf{Ablation Study for Stages 1 and 2.} We train \smodel{} on three real tasks in the Kinova setting, ablating Stages 1 and 2. The results indicate that while both stages improve performance, Stage 1 has a more significant impact.}
     \vspace{-0.1cm}
    \label{fig:ablation}
      % \vspace{-0.1cm}
\end{figure*}

% Original Ablations section, with only one paragraph for both ablations:
% \minisection{Importance of Human and Robotic Video Data} In order to examine the relative contributions of the two 3D point tracking training stages: human video pre-training (Stage 1) and robotic video fine-tuning (Stage 2), we train the following versions: (i) Stages 1, 2, and 3; (ii) Stages 1 and 3; (iii) Stages 2 and 3, and (iv) Stage 3. 
% \giscard{TODO: Split this into two paragraphs, one for each ablation: stage 1, and stage 2.}
% The results, shown in Figure~\ref{fig:ablation}, reveal that robotic video fine-tuning (Stage 2) leads to improved performance over models trained solely for robotic control (Stage 3). However, the performance boost is even more pronounced when only Stage 1 is used for 3D point tracks training, indicating that pre-training on human videos provides a larger increase in performance than fine-tuning with robotic videos. The key resulting insight is that pre-training on a large amount of human data can be more beneficial than a small amount of robotic data, with the proper 4D representations. Lastly, we note that adding Stage 2 to the training regime still improves performance, as the model performing all stages yields the highest success rate.

We conduct ablations to assess the importance of human video pre-training (Stage 1), and the robotic fine-tuning (Stage 2). All model versions in this section include robotic control fine-tuning (Stage 3).
% For this, we train the following versions: (i) \textsc{Ours}: Stages 1, 2, and 3; (ii) \textsc{No 3D points robotic fine-tuning}:Stages 1 and 3; (iii) \textsc{No 3D points human videos pre-training}: Stages 2 and 3, and (iv) \textsc{No 3D}: Stage 3. 
For this, we train the following versions: (i) Stages 1, 2, and 3; (ii) Stages 1 and 3; (iii) Stages 2 and 3, and (iv) Stage 3 only. 

\minisection{Human Video Pre-Training} ~\figref{fig:ablation} shows that the model with all stages performs better on all tasks than the Stages 2+3 model, indicating that pre-training on the human dataset provides a large benefit compared to only training for 3D point tracking on robotics videos. The performance boost observed when adding Stage 1 to Stage 3 is greater than the boost from adding Stage 2 to Stage 3, indicating that 4D pre-training on human videos provides a larger increase in performance than robotic videos. The key resulting insight is that when sufficient robotic pre-training data is unavailable, human video data can be a viable alternative, provided the proper 4D representations are used. 


\minisection{Robotic Video Fine-Tuning} The ablation results shown in~\cref{fig:ablation} reveal that adding robotic video fine-tuning (Stages 2+3; green) leads to improved performance over models trained solely for robotic control (Stage 3; pink). Adding Stage 2 to the training regime still improves performance, as the model performing all stages (blue) yields the highest success rate. As mentioned in~\Secref{sec:model:training}, Stage 2 is useful in addressing the distribution shift and embodiment gap when switching from human to robotic data. 

% We also ablate the robotic video fine-tuning (Stage 2), The results, shown in ~\cref{fig:ablation}, reveal that robotic video fine-tuning (Stage 2) leads to improved performance over models trained solely for robotic control (Stage 3). Adding Stage 2 to the training regime still improves performance, as the model performing all stages yields the highest success rate.


% We note that these generated tracks are expressed in the camera coordinate frame, which means the learned representation inherently captures both object and camera motion (since our pre-training data consists of egocentric human videos). However, since our downstream applications use static cameras, which are a relatively small subset of our pre-training distribution, we encounter a distribution shift when adapting to the robotic settings. In addition, there is a human-robot embodiment gap. To address these issues, we add a fine-tuning stage that focuses on 3D point tracking in the downstream robotic setting.


% \tabcrossrobot{!t}


\subsection{Additional Experiments}
\label{sec:eval:add_expr}

We perform additional experiments to evaluate our pre-training effectiveness, and how well the 3D point representations can generalize. More experiments are in~\cref{supp:expr}. 

\minisection{The Effectiveness of Pre-training} In order to study the effectiveness of pre-training on the 3D point track prediction task, we take three tasks from our real setting: \textit{pick cube}, \textit{destack cubes}, and \textit{stack cubes}, and compare to other works that use pre-training. MVP focuses on pre-training the vision encoder using human data, while RPT focuses on pre-training with visual and proprioceptive states. Octo, which is a transformer-based policy, is pre-trained on the OpenX dataset, similar to the VLA models LLARVA and OpenVLA. Lastly, ATM pre-trains a 2D point track transformer whose output is used to condition a policy.

% Other baselines () are described in Section~\ref{sec:rw}. 

The results are shown in Table~\ref{table:real-cube-results}. It can be seen that our pre-training improves performance over the baselines. \smodel{} outperforms other representation learning based pre-training methods, such as MVP, RPT, Octo and ATM, validating the benefits of using a 4D point-tracking based representation. In addition, while the two VLA baselines (OpenVLA and LLARVA) perform well, our approach still surpasses their results, possibly demonstrating the importance of using low-level representations as opposed to language decoders that were pre-trained on high-level vision-language tasks.

\tabcrossrobottwo{ht!}
\minisection{Generalization from Kinova to Franka} In order to study how our low-level 4D representations can help a model generalize across different robots, we perform an ablation experiment involving fine-tuning \smodel{} on Kinova robot videos, and fine-tuning for control on a 7 DoF Franka Emika Panda robot. We note that besides having different robots, the two setups also have very different configurations, as the Kinova robot is mounted on a stand as part of a bimanual setup, while the Franka robot is mounted on a table. 

Despite these significant differences, the results in Table~\ref{tab:gen} show that adding the human video pre-training and Kinova video fine-tuning improves the average performance on the Franka robot by 19.6\%. This supports our claim that the 4D representations are sufficiently generalizable to transfer across different robotic setups.



