\section{Related Work}
\label{sec:rw}


% \yuvan{add motion track policy from cornell/stanford if needed}
% OpenVLA,LLARVA,RoboPoint, LLARA + previous works (OCTO, 3DVLA)}

\minisection{Vision-Language-Action Models} VLAs are a type of robotic model that combines visual perception, language understanding, and action generation capabilities. VLAs take as input visual observations along with a language instruction, and output a sequence of robot control actions. Several VLAs, such as LLARVA ~\cite{niuLLARVAVisionActionInstruction2024}, OpenVLA ~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024}, LLaRA ~\cite{liLLaRASuperchargingRobot2024}, and RoboPoint ~\cite{yuanRoboPointVisionLanguageModel2024} directly fine-tune a VLM to predict robot actions, often using special tokens to represent the action space. These models differ in the choice of VLM and the specific method used to encode robot actions, but they share the underlying principle of adapting a pretrained VLM for robotic control. A similar model is 3D-VLA~\cite{zhen3DVLA3DVisionLanguageAction2024}, which consists of components for generating future states of an environment based on data that includes 3D information, such as point clouds. These existing VLAs utilize language decoders that have been pre-trained for high-level tasks like image captioning~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024} and VQA~\cite{brohanRT2VisionLanguageActionModels2023}, which may be inadequate for low-level robotic environments. In contrast, we show that leveraging low-level vision representations from human video data can result in a better pre-trained robotic model.

% Another group of robotic models, including Octo and Track2Act, focus on training general-purpose robot policies from scratch using large and diverse multi-robot datasets. Octo is trained on a dataset of 800,000 episodes collected from different robot embodiments and supports adaptation to new robots through efficient fine-tuning. Track2Act takes a different approach, learning to predict trajectories of points in a scene from web videos, and then uses these predicted trajectories to infer robot actions.



%% 4D - Qianqian Wang (SpatialTracker: Tracking Any 2D Pixels in 3D Space; Tracking Everything Everywhere All at Once; Shape of Motion: 4D Reconstruction from a Single Video)

% \yuvan{1. this section needs to have a clearer split between vision papers (SpatialTracker, omnimotion) and robotics papers using 3D/4D (track2act, robotap, RT-trajectory... 2. also need to add ATM here.}
% 3D-VLA (3d), ToolFlowNet (scene flow, only for tool, specific task), Track2Act (2d tracks from cotracker), ATM (2d tracks), IFOR (2d flow), RT-Trajectory (2d trajectory), RoboPoint (2d end point), RoboTAP (2d trajectory), tracevla (2d trajectory)ï¼Œ 
% to add: general flow, im2flow2act, gen2act
% Motion estimation is a fundamental task in computer vision, 2d motion estimation spans from 2D optical flow instance tracking and to more recent dense point tracking. 3d motion fields include scene flow to more recently dense 3d point long-term tracking.

% For robotics, some work leverages 2d motion field for robotic policy learning b/c it gives fine grained control signal. however, 3d motion field is fundamentally more spatial aware and lower dimension, ToolFlowNet takes first step by using scene flow for tool trajectory estimation. We use dense 3d motion field estimated by spatialtracker for pretraining a general autoregressive robotic model.
% \junyi{draft, will polish later tonight}

% \minisection{3D Motion Fields} Motion estimation spans from 2D optical flow~\cite{horn1981determining}, object tracking~\cite{wu2013online} to more recent dense point tracking~\cite{harley2022particle}.
\minisection{3D Motion Fields} Motion estimation spans from 2D optical flow~\cite{horn1981determining} and object tracking~\cite{wu2013online} to recent dense point tracking~\cite{harley2022particle}.
% Traditional optical flow solutions excel at modeling local motion between consecutive frames but often struggle with extended temporal tracking and occlusions. 
% By contrast, dense point tracking methods aim to establish point-level correspondences over longer time horizons, which can better capture intricate motion patterns.
Moving from 2D to 3D further enriches the geometric understanding. Early work on scene flow~\cite{vedula1999three} estimates short-term 3D motion based on explicit 3D structure~\cite{menze2015object} or depth images~\cite{teed2021raft}. 
More recently, SpatialTracker~\cite{xiaoSpatialTrackerTrackingAny2024} tackles long-range 3D point tracking by lifting 2D pixels into 3D with monocular depth estimates and iteratively refining 3D trajectories with as-rigid-as-possible motion priors. 
This 3D-driven strategy greatly improves occlusion robustness and yields impressive 3D point tracking results.
% However, these methods often face difficulties with long-term tracking and severe occlusions. 
% A more recent method, SpatialTracker~\cite{xiaoSpatialTrackerTrackingAny2024}, approaches long-term 3D point tracking by first lifting 2D pixels into 3D space (via depth projections) and learning point trajectories directly in 3D with motion priors. 
% This strategy brings better generalization and robustness to occlusion, resulting in impressive long-range tracking performance for real-world videos.

In robot learning, 2D motion fields have been used to enable fine-grained control, guiding manipulation and imitation learning~\cite{goyalIFORIterativeFlow2022, vecerikRoboTAPTrackingArbitrary2023, gu_rt-trajectory_2023, yuanRoboPointVisionLanguageModel2024, 
 zheng2024tracevlavisualtraceprompting, bharadhwajTrack2ActPredictingPoint2024, xu2024flowcrossdomainmanipulationinterface}. 
% \junyi{go into details?}
Despite their success, these approaches remain limited by the lack of geometric cues and less spatial awareness. 
In contrast, 3D motion fields offer more spatially grounded representations, enabling more efficient policy learning.
ToolFlowNet~\cite{seita_toolflownet_2022} leverages scene flow to estimate tool trajectories in behavior cloning, though it uses only a relatively coarse 3D signal.
We instead adopt dense 3D point tracking on diverse human videos, and use these rich 4D representations (3D points tracked across time) to pre-train a general auto-regressive robotic model with robust and versatile action generation.

% The use of 3D and 4D representations offers advantages for robot learning, including enhanced spatial reasoning, more efficient policy learning, and improved generalization. Early methods explored flow prediction from point clouds (ToolFlowNet ~\cite{seita_toolflownet_2022}) and the utilization of point clouds and depth maps to train a generative world model (3D-VLA ~\cite{zhen3DVLA3DVisionLanguageAction2024}). 
% \junyi{spatialtracker is different than omnimotion in that 1) tracks in camera coordinate than world coordinate, 2) feedfoward, we need to highlight these two points; also, spatialtracker does not build on top of omnimotion, will rewrite this}
% More recently, point tracking has become prominent, with methods like OmniMotion ~\cite{wangTrackingEverythingEverywhere2023}, which estimates full-length motion trajectories for every pixel in a video, enabling robust tracking through occlusions and complex motion patterns. SpatialTracker ~\cite{xiaoSpatialTrackerTrackingAny2024} lifts 2D pixels into 3D space and utilizes a triplane representation to estimate 3D trajectories. Track2Act ~\cite{bharadhwajTrack2ActPredictingPoint2024} predicts point tracks from videos to guide robot manipulation, while RoboTAP ~\cite{vecerikRoboTAPTrackingArbitrary2023} uses point tracking for few-shot imitation learning. 

% Other works utilize intermediate representations that capture spatial and temporal information in a compact form. For instance, IFOR ~\cite{goyalIFORIterativeFlow2022} uses optical flow to represent object motions, while RT-Trajectory ~\cite{gu_rt-trajectory_2023} employs trajectory sketches to provide high-level guidance to the robot. RoboPoint ~\cite{yuanRoboPointVisionLanguageModel2024} predicts affordances as 3D points, effectively representing potential action locations in the scene. These compact representations can simplify policy learning and enable more intuitive human-robot interaction. In contrast to these works, we leverage a 4D representation consisting of 3D points tracked through time for pre-training robotic models.
% ---

% The use of 3D and 4D representations in robotics models offers several advantages, including enhanced spatial reasoning, more efficient policy learning, and enhanced generalization. Several approaches have been explored for incorporating such representations into robot learning. For example, ToolFlowNet predicts tool flow from point clouds, and 3D-VLA, which utilizes point clouds and depth maps to train a generative world model. Another approach is to leverage point tracking to learn spatial relationships and object motions. Point tracking algorithms have seen significant advances in recent years, with the development of models like ``Tracking Everything Everywhere All at Once" (OmniMotion). OmniMotion uses a novel quasi-3D representation and optimization scheme to estimate full-length motion trajectories for every pixel in a video, enabling robust tracking through occlusions and complex motion patterns. Building on this, SpatialTracker lifts 2D pixels into 3D space and utilizes a triplane representation to estimate 3D trajectories. Track2Act predicts point tracks from videos to guide robot manipulation, while RoboTAP uses point tracking for few-shot imitation learning. Point tracking, whether in 2D or 3D, can provide valuable information about how objects move and interact, enabling more generalizable and adaptable robot policies.

% Beyond point tracking, some works explore reconstructing the full 4D structure of a scene from videos. For example, "Shape of Motion: 4D Reconstruction from a Single Video" introduces a method for recovering the 3D shape and motion of objects from a single monocular video, providing a dense 4D representation of the scene. While this work focuses on reconstruction rather than control, it highlights the potential of using rich 4D scene representations in robotics.

% Some models utilize intermediate representations that capture spatial and temporal information in a compact form. For instance, IFOR uses optical flow to represent object motions, while RT-Trajectory employs trajectory sketches to provide high-level guidance to the robot. RoboPoint predicts affordances as 3D points, effectively representing potential action locations in the scene. These compact representations can simplify policy learning and enable more intuitive human-robot interaction.

% Finally, 4D representations, which incorporate the temporal dimension, are used to model dynamic scenes and predict future states. Track2Act and Any-Point Trajectory Modeling both utilize future trajectories of points to guide robot actions, allowing the models to anticipate and adapt to changes in the environment. 3D-VLA leverages 3D point clouds and images over time to train a generative world model, facilitating more sophisticated planning and control. Incorporating the temporal dimension can enable more robust and anticipatory robot behavior.


% The use of 3D and 4D representations in robotics models offers several advantages, including improved spatial reasoning, more efficient policy learning, and enhanced generalization capabilities. Several approaches have been explored for incorporating such representations into robot learning.

% One approach is to use point clouds or depth maps to provide explicit 3D information about the environment. This is exemplified by ToolFlowNet, which predicts tool flow from point clouds, and 3D-VLA, which utilizes point clouds and depth maps to train a generative world model. These explicit 3D representations can provide rich geometric information about the scene, enabling more precise manipulation and planning.

% Another approach is to leverage point tracking to learn spatial relationships and object motions. Point tracking algorithms have seen significant advances in recent years, with the development of models like ``Tracking Everything Everywhere All at Once" (OmniMotion). OmniMotion uses a novel quasi-3D representation and optimization scheme to estimate full-length motion trajectories for every pixel in a video, enabling robust tracking through occlusions and complex motion patterns. Building on this, SpatialTracker lifts 2D pixels into 3D space and utilizes a triplane representation to estimate 3D trajectories. This approach leverages the regularity of motion in 3D space, particularly the as-rigid-as-possible (ARAP) constraint, to improve tracking accuracy, especially in challenging scenarios with occlusions and fast motion. Track2Act predicts point tracks from videos to guide robot manipulation, while RoboTAP uses point tracking for few-shot imitation learning. Point tracking, whether in 2D or 3D, can provide valuable information about how objects move and interact, enabling more generalizable and adaptable robot policies.

% Beyond point tracking, some works explore reconstructing the full 4D structure of a scene from videos. For example, "Shape of Motion: 4D Reconstruction from a Single Video" introduces a method for recovering the 3D shape and motion of objects from a single monocular video, providing a dense 4D representation of the scene. While this work focuses on reconstruction rather than control, it highlights the potential of using rich 4D scene representations in robotics.

% Some models utilize intermediate representations that capture spatial and temporal information in a compact form. For instance, IFOR uses optical flow to represent object motions, while RT-Trajectory employs trajectory sketches to provide high-level guidance to the robot. RoboPoint predicts affordances as 3D points, effectively representing potential action locations in the scene. These compact representations can simplify policy learning and enable more intuitive human-robot interaction.

% Finally, 4D representations, which incorporate the temporal dimension, are used to model dynamic scenes and predict future states. Track2Act and Any-Point Trajectory Modeling both utilize future trajectories of points to guide robot actions, allowing the models to anticipate and adapt to changes in the environment. 3D-VLA leverages 3D point clouds and images over time to train a generative world model, facilitating more sophisticated planning and control. Incorporating the temporal dimension can enable more robust and anticipatory robot behavior.




\minisection{Pre-training for Robotic Models} Pre-training has emerged as a crucial technique for improving the performance and generalization capabilities in robotics. Large-scale datasets such as OpenX~\cite{open_x_embodiment_rt_x_2023} contain diverse sensor modalities, tasks and action spaces across various robots. Models trained with these datasets, such as RT-1-X~\cite{brohan_rt-1_2023}, RT-2-X~\cite{brohan_rt-2_2023}, Octo~\cite{team2024octo}, OpenVLA~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024} and LLARVA~\cite{niuLLARVAVisionActionInstruction2024}, can be applied in various robot embodiments and tasks. Yet, these robot pre-training datasets are still orders of magnitude smaller than the data that current LLMs and VLMs are trained on.

% This By leveraging large-scale datasets and diverse pretraining objectives, researchers can imbue robot policies with valuable knowledge and skills that facilitate efficient learning and adaptation to new tasks and environments. (\giscard{Can talk about how effective pretraining has been for LLMs})


% \yuvan{for the ATM part: they don't have any pretraining on human videos. they have a small scale experiment that shows improved success if they train on human demonstrations for a task along with robot demos, but it is very different from us since we use truly general human videos (they use a video of a human doing the exact target task they want. so need to reword a little.}
% \junyi{it is also worth noting that ATM used 2d tracks, which is different than our 3d tracks which provides better spatial information}
To address the data issue, another prominent pre-training approach is to leverage large-scale datasets of human videos. This harnesses the abundance of freely available human activity data on the internet, offering a scalable alternative to collecting expensive robot demonstrations. For example, Track2Act~\cite{bharadhwajTrack2ActPredictingPoint2024} trains a 2D point-tracking model on human videos from Epic-Kitchens100~\cite{Damen2018EPICKITCHENS} and Something-Something-v2~\cite{goyal2017something}, then re-purposes it to guide robotic manipulation. Any-Point Trajectory Modeling (ATM)~\cite{wenAnypointTrajectoryModeling2024} similarly utilizes a small set of human demonstrations to aid cross-embodiment transfer, though in a more task-specific setting and still relying on 2D motion. By contrast, our approach lifts 2D observations into 4D representations (3D plus time), which not only enhances spatial awareness and occlusion handling, but also allows pre-training on human videos at scale, providing broader applicability and more robust policy learning in robotics.
%  For example, Track2Act ~\cite{bharadhwajTrack2ActPredictingPoint2024} leverages human video datasets such as EpicKitchens ~\cite{Damen2018EPICKITCHENS} and Something-Something-v2 ~\cite{goyal2017something} to train its point track prediction model, which is then used to guide robot manipulation. Any-Point Trajectory Modeling ~\cite{wenAnypointTrajectoryModeling2024} (ATM) demonstrates the effectiveness of pretraining on human videos for cross-embodiment transfer, showing that models trained on human demonstrations can be successfully adapted to control robots with different morphologies. Our work differs from these methods by learning 4D representations, particularly lifting 2D representations into 3D using monocular depth estimators and tracking the 3D trajectory of videos.

% Our work differs from this by tracking points in 3D rather than 2D, and adapting a transformer model to predict robot actions, in contrast with ATM's separate 2D point tracker whose output goes into a separate policy network.
