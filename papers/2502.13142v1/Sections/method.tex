\def\realcompleteresults#1{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[#1]
\tablestyle{8 pt}{1.2}
% \small
\begin{center}
\begin{tabular}{c|ccccccc}
\shline
\multirow{3}{*}{Method} & \multicolumn{3}{c|}{pick cube up} & \multicolumn{2}{c|}{destack} & \multicolumn{2}{c}{stack}\\
& yellow  & cyan & \multicolumn{1}{c|}{green} & yellow  & \multicolumn{1}{c|}{cyan} & yellow on cyan             & cyan on yellow                  \\ \hline

ATM & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & \multicolumn{1}{c|}{0}   & 0 & 0 \\

OpenVLA  & 77.8 $\pm$ 6.4 & 45.8 $\pm$ 4.2 & \multicolumn{1}{c|}{91.7 $\pm$ 8.3} & 55.6 $\pm$ 2.8 & \multicolumn{1}{c|}{51.3 $\pm$ 2.6}  & 0 & 0 \\ \shline 

\rowcolor{gray!10} Ours & 92.6 $\pm$ 3.7 & 100 $\pm$ 0.0 & \multicolumn{1}{c|}{95.8 $\pm$ 4.2} & 0 & \multicolumn{1}{c|}{0} & 0 & 0 \\ \hline

\multirow{3}{*}{Method} & \multicolumn{5}{c|}{pick toys then place to target} & \multicolumn{2}{c}{push}                                                      \\
& spiderman & penguin & pig & \multicolumn{2}{c|}{play basketball} & push red button                 & push red then blue  \\ \hline
ATM  & 0 & 0 & 0 & \multicolumn{2}{c|}{0} & 0 & 0  \\

OpenVLA & 0 & 0 & 0 & \multicolumn{2}{c|}{0}  & 0 & 0   \\ \shline 
\rowcolor{gray!10} Ours & 90.7 $\pm$ 1.3 & 94.7 $\pm$ 1.3 & 93.3 $\pm$ 1.3 & \multicolumn{2}{c|}{92.0 $\pm$ 2.3} & 0 & 0                     
\end{tabular}
\caption{\textbf{Success rate (\%) on Multi-Task setting for Kinova.}}
\label{table:real-all-results}
\end{center}
\end{table*}
}

\def\realcuberesults#1{
\begin{table}[#1]
\begin{tabular}{ccccl}
\multicolumn{1}{l}{} & pick cube & stack cubes & destack cubes &  \\ \shline
MVP                  & 75.00         & 18.75           & 81.25             &  \\
RPT                 & 87.50         & 31.25           & 93.75             &  \\
Octo                 & 56.25         & 12.50          & 37.50             &  \\
ATM                  & 0.08         & 0.00           & 0.04             &  \\
OpenVLA              & 68.75         & 31.25           & 53.33             &  \\
LLARVA               & 93.75         & \textbf{56.25}           & 100.00             &  \\ \shline
\rowcolor{gray!10}\smodel                 & \textbf{96.0 $\pm$ 2.3}         & -           & \textbf{-}            & 
\end{tabular}
% \vspace{4pt}
\caption{\textbf{Success rate (\%) on Cube Multi-Task setting.}}
\label{table:real-cube-results}
\end{table}
}

\def\tabcrossrobot#1{
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[#1]

\tablestyle{2.3 pt}{1.2}
% \small
\begin{center}
\begin{tabular}{cccccc}
\shline
\multirow{2}{*}{robot} & \multicolumn{2}{c}{pretrained} & \multirow{2}{*}{pick} & \multirow{2}{*}{stack} & \multirow{2}{*}{destack} \\
                       & Epic videos   & Kinova videos  &                     &                        &                          \\ \shline

Kinova                 & {\coloredcheckmark{ForestGreen}}                  & \coloredcheckmark{ForestGreen}                 & 96.0 $\pm$ 2.3                       &                        &                          \\
Franka                 & \coloredcheckmark{ForestGreen}                & {\coloredcross{Firebrick}}                &                       &                        &                          \\
Franka                 &    \coloredcheckmark{ForestGreen}             &  \coloredcheckmark{ForestGreen}               &                       &                        &                         
\end{tabular}
% \vspace{-0.8cm}
\label{tab:gen}
\end{center}
\end{table}

}

\def\tabsimresults#1{
\begin{table*}[#1]
\caption{\textbf{Success rate (\%) on RLBench Multi-Task setting.} We compare \smodel{}'s performance against several related baselines on 12 tasks from the RLBench benchmark. We use 25 episodes per task and 5 random seeds, averaging the results to get the success rate. \smodel{} achieves the best performance on 4 of 12 tasks and the best average success rate.}
\vspace{-0.2cm}
\tablestyle{1.6pt}{1.2}
% \small
\begin{center}
\begin{tabular}{lcccccccccccc|c}

\multirow{2}{*}{Method} & \multicolumn{13}{c}{Task}\\
                        & \begin{tabular}[c]{@{}c@{}}open\\ drawer\end{tabular} & \begin{tabular}[c]{@{}c@{}}meat off\\ grill\end{tabular} & \begin{tabular}[c]{@{}c@{}}turn\\ tap\end{tabular} & \begin{tabular}[c]{@{}c@{}}put\\ money\end{tabular} & \begin{tabular}[c]{@{}c@{}}push\\ buttons\end{tabular} & \begin{tabular}[c]{@{}c@{}}sweep\\ dustpan\end{tabular} & \begin{tabular}[c]{@{}c@{}}slide\\ block\end{tabular} & \begin{tabular}[c]{@{}c@{}}close\\ jar\end{tabular} & \begin{tabular}[c]{@{}c@{}}screw\\ bulb\end{tabular} & \begin{tabular}[c]{@{}c@{}}place\\ wine\end{tabular} & \begin{tabular}[c]{@{}c@{}}reach and\\ drag\end{tabular} & \begin{tabular}[c]{@{}c@{}}stack\\ blocks\end{tabular} & \begin{tabular}[c]{@{}c@{}}Average\\ Success Rate (\%)\end{tabular} \\ \shline

                        
Image-BC (ViT)
                        & 0                                                     & 0                                                        & 16                                                 & 0                                                   & 0                                                      & 0                                                       & 0                                                     & 0                                                   & 16                                                   & 0                                                    & 0                                                        & 0             & 2.67                                       \\ 
C2FARM-BC               & 20                                                    & 20                                                       & 68                                                 & 12                                                  & \textbf{72}                                                     & 0                                                       & 16                                                    & 24                                                  & 8                                                    & 18                                                   & 24                                                       & 4    & 23.83                                                    \\



ManiGaussian                  & 76                                          & 60                                              & 56                                        & -                                         & 20                                            & 64                                             & 24                                          & 28                                         & -                                          & -                                          & \textbf{92}                                              & 12 & 48.00
\\
                        
LLARVA                  & 60                                           & 80                                              & 56                                        & 44                                         & 56                                            & \textbf{84}                                             & \textbf{100}                                          & 28                                         & 8                                           & 12                                          & 52                                              & 0 & 48.33 
\\ 
PerAct                  & 80                                                    & 84                                                       & \textbf{80}                                                 & 44                                                  & 48                                                     & 56                                                      & 72                                                    & \textbf{60}                                                  & \textbf{24}                                                   & 12                                                   & 68                                                       & \textbf{36}    & 55.33                                               \\
\hline

                        
\rowcolor{gray!10}\smodel                  & \textbf{88.8}                                           & \textbf{94.4}                                              & 61.6                                        & \textbf{92.0}                                         & 67.2                                            & 72.0                                             & 85.6                                          & 24.0                                         & 10.4                                           & \textbf{36.0}                                          & 77.6                                              & 4.0 & \textbf{59.47} 
\end{tabular}
% \vspace{4pt}

\vspace{-0.1cm}
\label{tab:sim-results}
\end{center}
\end{table*}
}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/arch_figure_feb10.pdf}
    % \caption{ \smodel{} is trained in three stages. \textbf{Top Grey Box}: The first two stages focus on learning a scene-wide 4D representation by predicting 3D points across time, where Stage 1 pre-trains on a large egocentric human dataset (Epic-Kitchens100), and Stage 2 fine-tunes the 3D point representations, but on a smaller dataset (1-2K demonstrations) of robotic scenes, adapting the point tracking to robotic scene and camera requirements. \textbf{Bottom Grey Box}: Finally, the model is fine-tuned to predict robot sensorimotor states rather than 3D points to enable robotic control.
    \caption{ \smodel{} is trained in three stages. \textbf{Top Grey Box}: The first two stages focus on learning a scene-wide 4D representation by predicting 3D points across time, where Stage 1 pre-trains on a large egocentric human dataset (Epic-Kitchens100), and Stage 2 fine-tunes on a smaller dataset (1-2K demonstrations) of robotic scenes, adapting the point tracking to robotic scene and camera. \textbf{Bottom Grey Box}: Finally, the model is fine-tuned to predict robot proprioceptive states rather than 3D points to enable robotic control.
        % The first two stages focus on learning a scene-wide 4D representation by predicting 3D points across time, where Stage 1 consists of pre-training on a large egocentric human dataset (EpicKitchens), and Stage 2 fine-tunes with the same task on a smaller (1-2K demonstrations) dataset of robotic scenes that encompass various tasks, to adapt the point tracking to the scene and camera specifics of the robotics environment. 
    % \textbf{Bottom Grey Box}: Finally, the model is fine-tuned to predict robot sensorimotor states rather than 3D points to enable robotic control.
        % Final downstream task adaptation is performed by fine-tuning the model to predict the robot sensorimotor states instead of 3D Points.
    }
    \label{fig:arch}
     % \vspace{-0.1cm}
\end{figure*}



\section{Auto-regressive Robotic Models}
% \yuvan{add second stage: fine-tuning for robot point tracking, check all the section to make sure its consistent}
\label{sec:method}
% The first stage---the pre-training stage---focuses on learning generalized low-level representations through 3D point tracking from human videos. The second stage continues to fine-tune the model on this 3D point tracking task, on a small amount of data from the robotic setting that we intend on using in our downstream tasks.

To address the challenge of leveraging pre-trained vision representations from human video in robotic models, we present an auto-regressive model that relies on low-level 4D representations. The model is trained in three stages. The first stage---the pre-training stage---focuses on learning generalized low-level representations through 3D point tracking from human videos. In the second stage, the model is fine-tuned for the same task, but using a small amount of data for the robot that we intend to use in downstream tasks. Finally, the third stage fine-tunes the model for robotic control. We begin by discussing the preliminaries (\Secref{sec:model:Preliminaries}), then introduce the architecture (\Secref{sec:model:arch}), and the training procedures (\Secref{sec:model:training}). Our method is shown in~\figref{fig:arch}.



\subsection{Preliminaries}
\label{sec:model:Preliminaries}


\minisection{4D Scene Representations} Our 4D representations result from solving the 3D point tracking problem, which involves finding the 3D coordinates of discrete points across time, given a monocular video consisting of $T$ discrete frames. Formally, the objective is to find $p_t$ as defined below:
\begin{equation}
    p_t=\{(x_{jt}, y_{jt}, z_{jt}) |~ 0\leq j < n\}
    \label{eq:p_t_definition}
\end{equation}
where $n$ is the total number of points being tracked, and $0\leq t < T$. In solving this tracking problem, the identities of the points are fixed and consistent across all frames: the $j$-th point in $p_t$ refers to the same physical point in 3D space across all time steps $t\in [0, T)$. To initialize these points, we define a square grid of size $g \times g$ on the first frame (frame $t=0$), resulting in $n=g^2$ points. The task is to track the 3D coordinates of these initial queried $n$ points throughout the video while maintaining their unique identities.


% - define robotic episode \\
% - for autoregressive models, goal is to predict low level action to accomplish language goal conditioned on a certain number of previous timesteps' i



\minisection{Robotic Episodes} Robotic control can be formulated as a finite-horizon Markov Decision Process (MDP), characterized by temporal sequences that capture the robot completing a particular task. The task is described  by the language instruction $l$. The temporal sequences typically consist of visual observations $i_{0:T-1}$ and proprioceptive states $s_{0:T-1}$, which can lie in Cartesian space or joint position space. Then, the objective is to learn a policy that predicts one or more future actions, conditioned on a finite number of previous timesteps, to successfully complete a given task.


\minisection{Inputs} Given any video, we structure our models' input at timestep $t$, into three parts: the language instruction $l$, the image input  $i_t$, and the current 3D coordinates of the tracked points, $p_t$. These elements together provide the contextual, visual, and spatial information necessary for 3D point tracking. The output is the future 3D coordinates of the tracked points, $p_{t+1}$. When fine-tuning the model for robotic control (see \cref{sec:model:training}), we replace the tracked input points with the robot's current state $s_t$, and the output points with the next state, $s_{t+1}$. We hypothesize that the shared geometric structure --- up to a linear transformation --- between the points and robot state representations enables efficient transfer learning between the second and third stages.

% We note that points and robot state representations both contain the same geometric structure from the scene up to a linear transformation.

% We hypothesize that points and robot state representations both contain the same geometric structure from the scene up to a linear transformation, enabling efficient transfer learning between the second and third stages.



% \giscard{I'm commenting out the sentence below. It might be a problem to try to mention this unless we have experiments to explain the why or how this works, e.g., what if we pretrain with robot states and never use the points? Do wehave that? Sorry fi I'm mentioning an experiment that already exists or is planned.}
% Original:     This mapping maintains the underlying nature of the input-output relationship and thus the same high-level training objective.
% Alternative:  This replacement maintains the geometric nature of these input-output pairs, and thus the same high-level training objective.

\subsection{Architecture} 
\label{sec:model:arch}
% \yuvan{need to ensure consistency of $p_{t+1}, \hat{p}_{t+1}, p_{t+1}^*$ in text and figure}

In the first and second training stages, our objective is to develop an auto-regressive model $\pi$ capable of predicting 3D point tracks. The predictions are conditioned on the input $(l, i_t, p_t)$ from a context window of $C$ timesteps:
\begin{equation}
\pi(l, i_{t-C+1:t}, p_{t-C+1:t}) \to p_{t+1}
\label{input_output_e1}
\end{equation}

During control fine-tuning, the objective changes slightly, as the model conditions on, and predicts proprioceptive states:
\begin{equation}
\pi(l, i_{t-C+1:t}, s_{t-C+1:t}) \to s_{t+1}
\label{input_output_eq2}
\end{equation}


Before being fed into the causal transformer for next-token prediction, each part of the input and output must be processed and projected into the same latent space. To achieve this, separate encoders are used for the language, image, points, and robot states, as we discuss below.

\minisection{Language Encoder} We use a frozen CLIP~\cite{radford2021clip} text encoder trained on LAION-2B~\cite{laion} to process text, with a learnable linear projection layer added at the end to get the language token $z_{l}$.

\minisection{Image Encoder} To process the image, we use a standard Vision Transformer~\cite{dosovitskiy2020vit} to get the image token $z_{t}^{im}$. This ViT is frozen while training our model, and is pre-trained using CrossMAE on a combination of ImageNet~\cite{deng2009imagenet} and the OpenX dataset~\cite{open_x_embodiment_rt_x_2023}. This enables the vision transformer to learn to encode both non-robotic and robotic data, which is important since our pre-training stage emphasizes the former, while fine-tuning targets the latter.



\minisection{4D Representations} To encode the point coordinates, we use a standard 2-layer MLP. The resulting feature $z_{t}^{pts}$ is combined with $z_{t}^{im}$ via an attention pooling layer to get the current observation token, $z_{t}^{obs}$. A separate MLP is used to encode the next timestep's point coordinates and get $\hat{z_{t}}$.

\minisection{Causal Transformer} For each timestep, we get three tokens, one each for language, current visual observation and prediction, which are fed into the transformer. In our implementation, we use a randomly initialized causal Transformer (ViT-Base). The Transformer is trained for standard next-token prediction on the sequence $(z_{l}, z_{t}^{obs}, \hat{z_{t}}, z_{l}, z_{t+1}^{obs}, \hat{z}_{t+1}, \cdots)$, with loss only being calculated for $\hat{z}_{t}$. During inference, we input $(z_l, z_{0}^{obs})$ at timestep 0, and the model predicts $\hat{z_{t}}$ for every timestep.


\minisection{Decoder and Loss Function} We calculate the loss using only $\hat{z_{t}}$, as predicting $z_l$ and $z_{t}^{obs}$ is not the objective in either 3D point tracking or robotic control tasks. The predicted token is decoded using a two-layer MLP into the predicted point tracks $\hat{p}_{t+1}$. The L1 distance between $\hat{p}_{t+1}$ and the ground truth point tracks $p_{t+1}^*$ is used as the final loss:
\begin{equation}
    \mathcal{L}(\hat{p}_{t+1}, p_{t+1}) = \frac{1}{n} \| \hat{p}_{t+1} - p_{t+1}^* \|_1
\end{equation}

\minisection{Adaptation for Robotic Control} We note that when fine-tuning for robotic control, we replace the MLPs for processing points $p_t$ with similar MLPs for processing robot states $s_t$. Additionally, when processing multiple images in the fine-tuning stage, we combine the observation tokens by concatenating linear transforms of the different views to get a single $z_{obs}^t$ token. The model is also trained to predict multiple future proprioceptive states. The rest of the architecture is kept the same (e.g. loss function). For more details, please see~\cref{supp:2_views}.



\subsection{Training} 


\label{sec:model:training}

As previously mentioned, \smodel{} is trained in three stages: the first two stages focus on the 3D point tracking task for human and robot videos respectively, and the last stage focuses on robotic control. Next, we describe these stages. 


% pretrain on epic, has 76K videos of rich human object interactions, extract points of 36x36 grid from initial image using spatialtracker. train only for point prediction.

\minisection{Stage 1: Human Videos Pre-training} In the pre-training stage, we focus on learning 3D point tracking, since this task allows our model to leverage large-scale human video data with a representation that also transfers over to the robotic domain. Specifically, we train our model on 76K videos from the Epic-Kitchens100 dataset~\cite{Damen2018EPICKITCHENS}, which contains rich human-object interactions with 97 verbs and 300 noun classes. By training to predict 3D point tracks for such large-scale human data, \smodel{} gains a deeper understanding of the spatial dynamics and physical interactions of different bodies and objects, knowledge that is critical for enhancing robotic models. 

% Original Version
% To extract pseudo-annotations for the 3D point tracks, we utilize SpatialTracker~\cite{xiaoSpatialTrackerTrackingAny2024}. This off-the-shelf model generates 3D tracks for points on a $g \times g$ grid. We always track the points that are on the grid in the first frame of the video. We note that the generated tracks are in the camera coordinate frame, and thus the learned representation is coupled with both object and camera motion. However, since our downstream application uses static cameras, it is a subset of our pre-training distribution. Pre-training on 3D tracks with world coordinates is a natural incremental improvement that can be made with recent dynamic SLAM methods such as MonST3R~\cite{zhang2024monst3r} or MegaSaM~\cite{li2024megasam}.

% New version
\tabsimresults{!t}


To extract pseudo-annotations for 3D point tracks, we use an off-the-shelf tracker that generates 3D tracks for points arranged on a $g \times g$ grid. Points on the grid are initialized in the first frame of the video and tracked throughout the sequence. 
We note that the pseudo-labeled tracks are generated in the camera coordinate frame, inherently capturing both object and camera motion due to the egocentric nature of the human videos. 
In contrast, our robotic applications typically involve stationary cameras and different object-hand interaction patterns, introducing discrepancies in both camera dynamics and embodiment. 
To reconcile these differences and ensure smooth transfer to robotic domains, we introduce a fine-tuning stage focusing on 3D point tracking in the downstream robotic setup.
% We note that these generated tracks are expressed in the camera coordinate frame, which means the learned representation inherently captures both object and camera motion (since our pre-training data consists of egocentric human videos). 
% However, since our downstream applications use static cameras, which are a relatively small subset of our pre-training distribution, we encounter a distribution shift when adapting to the robotic settings. In addition, there is a human-robot embodiment gap. To address these issues, we add a fine-tuning stage that focuses on 3D point tracking in the downstream robotic setting.


% We mitigate the human-robot embodiment gap by adding a fine-tuning step focused on 3D point tracking within the target robotic environment.


% However, since our downstream applications involve different data distributions with regard to camera motion/scene/zoom/etc, we introduce a fine-tuning stage to adapt to the new visual representations.


% --> if we had dynamic robot cameras, it is a problem. but we dont so its fine


% We note that these generated tracks are expressed in the camera coordinate frame, which means the learned representation inherently captures both object and camera motion (since our pre-training data consists of egocentric human videos).

% However, since our downstream applications involve static cameras, which are also a part of  they represent a subset of our pre-training distribution. 

% Even though pre-training is good -> some domain gaps like camera/viewpoint/how robot joints move vs how humans move -> we need step 2
% \junyi{it is important to note here that the spatialtracker gives 3d point tracks in camera coordinate, therefore the learnt representation is coupled with both object motion and camera motion. However, since our downstream application covers video input without camera motion, which is the subset of our pretraining distribution. We plan to pretrain with world coordinate 3d tracks in the future by leveraging recent dynamic slam methods (monst3r, megasam).}
% However, since our downstream applications involve different data distributions with regard to camera motion/scene/zoom/etc, we introduce a fine-tuning stage.

\minisection{Stage 2: Fine-tuning on 3D Point Tracking for Robotic Settings} After the pre-training stage on human video data, we fine-tune \smodel{} for the same 3D point tracking task with videos from the robotic setup we use in the downstream application. We note that this fine-tuning only needs to be performed once for every robot setup for all tasks combined, with a modest amount of data ($\approx5-10\%$ compared to Stage 1). 
% This step essentially helps the model transition between the pre-training on human videos and the control fine-tuning in the next stage.
This step helps transition from the camera dynamics and embodiment gaps between the human video pre-training and the control fine-tuning in the next stage.

 % continue training for action prediction.
\minisection{Stage 3: Fine-tuning for Robotic Control} Having trained the model on 3D point tracking, we then fine-tune it for robotic control. In this stage, we collect a number of robotic demonstrations depending on the downstream tasks. We note that we use significantly fewer demonstrations for real robotic tasks than other baselines (See Section~\ref{exp:real}). After collecting successful data of the robot performing the target task, we replace the current and predicted point tracks in the training process with current and predicted robot states.


% \subsection{Human Video Datasets for Pre-training} 
% \label{sec:model:datasets}

% One significant advantage of the 4D representation used in the point tracking task is its ability to bridge human and robotic video data with minimal domain gap. To take full advantage of this, we pre-train our model on the extensive Epic-Kitchens100 dataset, which contains rich human-object interactions with 97 verbs and 300 noun classes. By training to predict 3D point tracks for such large-scale human data, \smodel{} gains a deeper understanding of
% the spatial dynamics and physical interactions of different bodies and objects, knowledge that is critical for enhancing robotic models.