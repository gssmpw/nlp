

\begin{abstract}

% draft:

% Recently, foundational models have shown significant success in robotic learning. Yet, pre-training for these models has not yet been convincingly demonstrated, mostly due to the scarcity of robotic data. In order to overcome this limitation, many models attempted to learn meaningful representations from diverse human data. For example, Vision-Language-Action models often rely on language decoders pre-trained on high-level semantic tasks like VQA and image captioning, leading to a gap when fine-tuning them for low-level robotics tasks. Alternative approaches attempt to employ representation learning on an abundance of human data, but suffer from a lack of physical understanding of the world.

% Initial version:
% The robotics field wants to replicate the success of foundation models seen in the areas of vision and language. Many works attempt to build a foundational model for robots, but these efforts have seen limited success due to either relying on costly robotic annotations or failing to use a representation that can express the dynamics of the physical world. 

% giscard option
% Foundation models pretrained on large amounts of unlabeled data have revolutionized fields like natural language processing and computer vision, demonstrating remarkable generalization and adaptability. Efforts to establish a pre-trained foundation model for robotics have so far met with limited success due either to the requirement for costly robotic annotations or the failure to use representations that sufficiently model the physical world.




% Prior efforts to establish a foundational model for robotic models have encountered limited success, owing to either the requirement for costly robotic annotations or the utilization of representations that are incapable of expressing the dynamics of the physical environment.


% Roi option
% Foundation models, pre-trained on massive unlabeled datasets, have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, and highlighting the importance of pre-training. Yet, similar efforts in robotics have been hampered by either the need for costly robotic annotations or the lack of representations that effectively capture the physical world.


%Giscard
Foundation models pre-trained on massive unlabeled datasets have revolutionized natural language and computer vision, exhibiting remarkable generalization capabilities, thus highlighting the importance of pre-training. Yet, efforts in robotics have struggled to achieve similar success, limited by either the need for costly robotic annotations or the lack of representations that effectively model the physical world. In this paper, we introduce \smodel{}, an \textbf{A}uto-regressive \textbf{R}obotic \textbf{M}odel that leverages low-level \textbf{4}D \textbf{R}epresentations learned from human video data to yield a better pre-trained robotic model. Specifically, we focus on utilizing 3D point tracking representations from videos derived by lifting 2D representations into 3D space via monocular depth estimation across time. 
% These 4D representations, maintaining a shared geometric structure — up to a linear transformation — between the points and robot state representations, enable efficient transfer learning from human video data to low-level robotic control. 
These 4D representations maintain a shared geometric structure between the points and robot state representations up to a linear transformation, enabling efficient transfer learning from human video data to low-level robotic control. Our experiments show that {\smodel} can transfer efficiently from human video data to robotics and consistently improves performance on tasks across various robot environments and configurations.
  

% These 4D representations are constructed by lifting 2D representations into 3D using monocular depth estimators, and tracking 3D points in videos. 


\end{abstract}

% STORY

% Old
% 1. Current FMs have shown great success when leverging pretrained data.
% 2. However, when it comes to robotics, it is not clear how to leverage video human does since we don't have robotic annotations.
% 3. The key to leverage video human data for robotics lies in finding low level represetnations that can be relevant for robotics.
% 4. While existing VLAs have started to show great sucess, they still suffer from scaling.
% 5.



% New
% 1. VLAs are great
% 2. However, scaling VLAs has not yet been convincingly demonstrated for robotics.
% 3. Several reasons: (i) missing a large amount of robotic data, (ii) in contrast we have a lot of human video data, but we lack of robotic annotations. 
% 4. The key to building a scalable large robotic model lies in leveraging human video data, particulary finding low level representations that can be relevant for robotics
% 5. Most of existing VLAs leverage pretrained language decoder that has been pretrained for high level semantic tasks, such as VQA or image captioning.
% Big gap between the pretrained objective and the robotic
% 6. Here, we focus on learning 4D representations using an autoregressive mechanism based on pre-training human video data and robotics.
% 7. We show that leveraging low-level vision representations compared to high-level sematic tasks (e.g., VQA and image captioning) can scale from human video human data to robotics, and consistently improving performance on robotic tasks




% In this paper, show that pre-training on 4D representations using auto-regressive mechanism from video human data can result in a better pre-trained robotic models. Particularly, 4D trajectories say smth. 


% we focus on learning 4D representations using autoregressive pre-training from video human data and robotics, particularly learning how each spatial point moves in time. 

% We show that leveraging low-level vision representations compared to high-level sematic tasks (e.g., VQA and image captioning) can scale from human video data to robotics, and consistently improving performance on robotic tasks.
