\section{Conclusion} 
\label{sec:conclusion}

In this work, we demonstrate that our pre-training approach from human video data to robot learning is effective in addressing longstanding challenges of robotic learning pre-training. We introduced \smodel{}, an Auto-regressive Robotic Model that leverages low-level 4D representations by lifting 2D representations into 3D using monocular depth estimators, and tracking 3D points in videos. Our results in simulation and real-world setups show that our method consistently outperforms existing methods across diverse robotic tasks, showcasing its superior transferability and generalization capabilities. More broadly, our approach shows that training solely on human video data can lead to better performance than methods like OpenVLA that are pre-trained on robotic data alone. This suggests that effective pre-training can be achieved without the need for large-scale robotic datasets by bridging the gap between human-centric visual data and robotic applications, unlocking new possibilities for scalable and data-efficient robotics. 
% While \smodel{} offers substantial benefits for pre-training human video data for robotic learning, it is important to recognize certain limitations that accompany our approach. \roei{List here few issues we would like to raise.} 
% An incremental improvement could involve pre-training on 3D tracks in world coordinates, leveraging recent dynamic SLAM methods such as MonST3R~\cite{zhang2024monst3r} or MegaSAM~\cite{li2024megasam}, which we leave for future work.
As we continue to explore the boundaries of representation learning for robotics, \smodel{} lays a foundation for future research into autonomous systems that can learn from the vast repository of human experience available in video data.



% In this work, we introduced \smodel{}, an Auto-regressive Robotic Model that leverages 4D representations from human video data to address the longstanding challenge of pre-training for robotic learning. By bridging the gap between human-centric visual data and robotic applications, our approach demonstrates that effective pre-training can be achieved without the need for large-scale robotic datasets, unlocking new possibilities for scalable and data-efficient robotics. \giscard{It would be great if either here or in the intro section, we could cite some factoid like 'we pre-train on human data and can beat baseline-X while fine-tuning only on Y number of robotics episodes' something to highlight the total number of robotic episodes we need in comparison with other methods.}

% Our results show that ARM4R consistently outperforms existing methods across diverse robotic tasks, showcasing its superior transferability and generalization capabilities.\giscard{tailor this to our actual results. I made up the 'generalization'}


 % By learning low-level representations of motion and interaction from human videos, \smodel{} demonstrates improved performance compared to models pre-trained on robotic data alone\giscard{adjust this claim according to experimental results, e.g., 3d-vla has a similar claim in their abstract, what specifically do we want to claim about how we compare to methods pretrained on robotics; do we compare only to VLAs like 3dlva, or all methods?}, and also surpasses other baselines like PerAct and OpenVLA \giscard{adjust according to the major baselines we beat}. Beyond the immediate performance gains, our work highlights the potential of leveraging spatial-temporal representations to enhance robotic perception and control in dynamic environments.  As we continue to explore the boundaries of representation learning for robotics, ARM4R lays a foundation for future research into autonomous systems that can learn from the vast repository of human experience available in video data. 

 
 % The approach leverages the wealth of human video data while focusing on the critical elements of motion and interaction for robotic manipulation and demonstrates strong generalization capabilities.