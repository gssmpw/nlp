% \clearpage
% \setcounter{page}{1}
\def\pretrainhyperparameters#1{
\begin{table}[H]
\caption{Training Hyperparameters for the three stages.}
\tablestyle{7.2 pt}{1.2}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Stage 1} & \textbf{Stage 2} & \textbf{Stage 3} \\ \hline
Learning Rate           & $5 \times 10^{-4}$   & $5 \times 10^{-4}$ & $5 \times 10^{-3}$ \\ \hline
Weight Decay            & $1 \times 10^{-2}$   & $1 \times 10^{-2}$ & $1 \times 10^{-2}$ \\ \hline
Batch Size              & $256$                   & $256$ & $256$ \\ \hline
Number of Epochs       & 5                     & 20 & 10-50 \\ \hline
\end{tabular}
\vspace{-0.5 cm}
\label{tab:hyperparameters}
\end{table}
}

\section*{Appendix}
% \maketitlesupplementary

% \yuvan{needs to be updated}
% Here, we provide additional information about our experiments, our model's emergent properties, our constructed dataset, and implementation details. Specifically, \cref{supp:expr} provides additional experiment results, \cref{supp:our:datasets} provides details about our constructed vision-action instruction dataset, and~\cref{supp:impl} provides additional implementation details.
Here, we provide qualitative results of our 3D point tracking for both in-domain and out-of-domain videos (\cref{supp:expr}), statistics of the used datasets (\cref{supp:our:datasets}), implementation details (\cref{supp:impl}), evaluation details (\cref{supp:sim_on_rlbench}), and robotic setup details (\cref{supp:franka,supp:kinova}).  



\section{Additional Experiment Results}
\label{supp:expr}

\subsection{Qualitative Results of 3D Points Tracking.}
\label{supp:expr_qualitative}

We conduct additional experiments to evaluate our pre-trained model's ability to track 3D points. Specifically, we run inference on a few randomly chosen episodes from Epic-Kitchens100~\cite{Damen2018EPICKITCHENS} (in-domain human videos), Ego4D~\cite{Ego4D2021} (out-of-domain human videos), Kinova robot videos (in-domain robot videos) and  Open X Embodiment ~\cite{open_x_embodiment_rt_x_2023} (out-of-domain robot videos). 

In~\cref{fig:qualitative_human_videos}, we present the tracking results on human videos from a version of our model that has undergone human video pre-training (Stage 1).  The top two rows show the results for an episode from Epic-Kitchens (in-domain human videos) with the action ``stir potatoes.'' The bottom two rows display monocular human videos and their corresponding 3D point tracking predictions for an episode from Ego-4D (out-of-domain human videos) with the action ``pick up plate''. 

In~\cref{fig:qualitative_robot_videos}, we present the tracking results on robot videos, from a version of our model that has undergone human video pre-training on Epic-Kitchens100 as well as robot video fine-tuning on Kinova demonstration videos (Stage 1+2). The top four rows display monocular robot videos and their corresponding 3D point tracking predictions for two episodes from in-domain Kinova robot videos, with the actions ``push red button'' and ``place spiderman into bowl'' respectively. The bottom two rows show the results for an episode from the Autolab subset of the OpenX Embodiment dataset (out-of-domain robot videos) with the action ``pick the tiger and place it into bowl.''



These visualizations verify that our model is not overfit to a certain dataset or robotic setup, but can in fact generalize well to new videos.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/supp_human.png}
    \caption{Visualization of \smodel{}'s 3D Point Track results on randomly chosen Epic-Kitchens (in-domain) and Ego-4D (out-of-domain) human videos.}
    \label{fig:qualitative_human_videos}
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/supp_robot.png}
    \caption{Visualization of \smodel{}'s 3D Point Track results on randomly chosen Kinova (in-domain) and Open X-Embodiment (out-of-domain) robot videos.}
    \label{fig:qualitative_robot_videos}
\end{figure*}



\section{Additional Dataset Details}
\label{supp:our:datasets}

\subsection{Epic-Kitchens100}
\textit{Epic-Kitchens100}~\cite{Damen2018EPICKITCHENS} is a large-scale, egocentric video dataset designed for action recognition and understanding in daily kitchen activities. Captured from a first-person perspective using head-mounted cameras, the dataset provides rich, untrimmed video recordings of individuals performing various cooking and kitchen-related tasks. It features a diverse range of object interactions, fine-grained action labels, and naturalistic, unscripted activities, making it particularly valuable for studying human-object interactions and long-term temporal dependencies. 

The dataset includes diverse hand-object interactions, described by combinations of 97 verbs (for the hand motions) with 300 nouns (for the object categories). In our human video pre-training stage, we use almost all the labeled episodes available in the original dataset. We first subsample videos at 10 \textit{fps}, an experimentally chosen rate, as the original 50 \textit{fps} provides unnecessary redundancy for slow movements. We then model the duration distribution of all 75,886 episodes, and filter out $\approx1\%$ of episodes that are of length $> 256$ frames. As a result, we get a final set of 75,041 episodes for pre-training. For each episode, we use a simple `verb + noun' instruction derived from the official annotation files.

\subsection{RLBench Robot Episodes}
RLBench~\cite{james2020rlbench} is a large-scale benchmark dataset for robotic learning, designed to facilitate research in vision-based reinforcement learning and imitation learning. It consists of a diverse set of robot manipulation tasks performed in a simulated environment using a Franka Emika Panda arm. The dataset provides high-quality demonstrations with multi-modal observations, including RGB images, depth maps, and proprioceptive data. 

In our experiments, we use $128 \times 128$ resolution images for training. For most tasks, we use the `front rgb' and `wrist rgb' views for point track and control fine-tuning. However, in some cases, we find that using other views yields better performance (details on task-specific implementations are provided in~\cref{supp:impl}). For robot control, we use end-effector control: $\mathbf{x}=(x, y, z, \theta_x, \theta_y, \theta_z)$, where \((x, y, z)\) is the position and \((\theta_x, \theta_y, \theta_z)\)  the Euler angles for orientation. We also have a one-dimensional binary element to control the gripper. For language instructions, we use variation 0 from the official list of instructions for all tasks. We do not subsample episodes for our RLBench experiments.



\section{Additional Implementation Details}
\label{supp:impl}

\subsection{Architecture of Auto-regressive Model} \label{supp:2_views}
Here, we provide details on processing the visual input of the auto-regressive model to support two views when adapting to robot control fine-tuning. The images from both views are fed separately into the image encoder to obtain the image embeddings $z_t^{im}$ for each view. Each view is then pooled using attention pooling with the state embeddings to form the image tokens $z_t^{obs}$. Next, we project each token to half of its original hidden dimension (768 $\rightarrow$ 384 in our implementation) and concatenate them to obtain the final image tokens, incorporating information from both views.



\subsection{Training Recipes} \label{supp:training_recipe}
We used the following hyperparameters for the three stages of training:

\pretrainhyperparameters{ht!}

We note that for Stage 3, we trained our model for a variable number of epochs depending on the downstream task, until the loss converged.


\section{Simulation on RLBench}
\label{supp:sim_on_rlbench}
We evaluate our model on 12 tasks in RLBench for our simulation setup. Each task includes multiple variations, and we generate 190 episodes using their data generation script for \smodel{} training. In most cases, we follow the task setup of PerAct~\cite{shridhar2023perceiver} and use the `front rgb' and `wrist rgb' views. We use $C= 16$ ($C$ is the context window  of the auto-regressive model). The detailed task-level configuration is provided below.

\minisection{Open Drawer} The task is to open one of three drawers. The success metric is a full extension of the prismatic joint of the target drawer. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Meat off Grill} The task is to take either a piece of chicken or steak off the grill and put it on the side. The success metric is the placement of the specified meat on the side, away from the grill. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Turn Tap} The task is to turn either the left or right handle of the tap. Left and right are defined according to the orientation of the faucet. The success metric is the joint of the specified handle being at least $90^\circ$ away from the starting position. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Put Money} The task is to pick up the stack of money and place it on the specified shelf of a safe. The safe has three shelves: top, middle, and bottom. The success metric is the placement of the stack of money on the specified shelf in the safe. We use the `front rgb' and `overhead rgb' views. The context window  of the model is $C = 16$.

\minisection{Push Buttons} The task is to push the colored buttons in the specified sequence. There are always three buttons present in the scene, whose colors are sampled from 20 options, and the number of buttons to press is between one and three. The success metric is all specified buttons being pressed in the right order. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 32$.

\minisection{Sweep Dustpan} The task is to sweep the dirt particles into the specified dustpan. There are two dustpans, one short and one tall, and both are always present in the scene. The success metric is all five dirt particles being inside the specified dustpan. We modified this task by adding a variation with a different-sized dustpan. We use the `front rgb' view only, repeated twice, for this task. The context window  of the model is $C = 16$.


\minisection{Slide Block} In this task there is a block and four colored squares in the scene (green, blue, pink, and yellow). The task is to slide the block onto either the green or pink squares. The success metric used is some part of the block being on the specified target square. The original task only had one target square, and we modified it by adding three additional colored squares --- one target and two distractors. We use the `front rgb' view only, repeated twice, for this task. The context window  of the model is $C = 16$.

\minisection{Close Jar}  The task is to screw in the lid on the jar with the specified color. There are always two colored jars in the scene, one target jar and one distractor jar. The success metric used is the lid being on top of the specified jar and the robot gripper not grasping any object. We modified this task so that the target jar color is drawn from a list of three possible colors (red, maroon, and lime ). The color for the distractor jar was still chosen out of 20 options. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 32$.

\minisection{Screw Bulb} There are two bulb holders of different colors, and the task is to pick up a light bulb from the stand specified by color and screw it into the bulb stand. The color of the target holder is sampled from two colors, while the color of the distractor holder is sampled from the original 20 color options. The success metric used is the bulb from the specified holder being inside the bulb stand. We modified this task to use three colors for the target holder (yellow, purple and silver) rather than 20 as in the original task specification. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Place Wine}  The task is to pick up the wine bottle and place it at the specified location in a wooden rack. The rack has three locations: left, middle, and right. The success metric is the placement of the bottle on the specified location in the rack. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Reach and Drag} The environment has a cube, a stick, and four possible colored target squares. The task is to pick up the stick and use it to drag the cube to the target square of a specified color. The other three squares are considered distractors. The success metric used is some part of the block being inside the target's area. We modified this task to sample the target color from a list of three colors (maroon, magenta, teal). The colors for distractor squares are still sampled from 20 options. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\minisection{Stack Blocks } The scene starts with 8 blocks and a green platform. Four of the blocks are of a target color, and the other four have a distractor color. The task is to stack $N$ blocks of the target color on the green platform. The success metric is $N$ blocks being inside the area of the green platform. We use the `front rgb' and `wrist rgb' views. The context window  of the model is $C = 16$.

\subsection{Baselines in Real Experiments} \label{supp:reproduce_atm_openvla}

\minisection{ATM} We reproduce ATM~\cite{wenAnypointTrajectoryModeling2024} as a baseline for our Kinova real-world experiment setup, following the provided code and instructions~\footnote{\href{https://github.com/Large-Trajectory-Model/ATM}{https://github.com/Large-Trajectory-Model/ATM}}. In the first stage, we use all Kinova robot episodes (5 tasks, each with 200 episodes per variation) to train a track transformer using ground truth point tracks generated by Co-Tracker~\cite{karaev2025cotracker}. In the second stage, we take the best checkpoint of the track transformer to train a policy for each task separately, consistent with \smodel{}'s real-world setup. ATM uses a 7-dimensional joint pose and one-dimensional gripper state in its policy. To adapt it to our data format, we modify the implementation to end-effector control, predicting a 3-dimensional $(x, y, z)$ position, a 4-dimensional quaternion rotation, and a 1-dimensional gripper state.


\minisection{OpenVLA} We also test OpenVLA~\cite{kimOpenVLAOpenSourceVisionLanguageAction2024} on our Kinova robot setup, following their fine-tuning code and instructions~\footnote{\href{https://github.com/openvla/openvla}{https://github.com/openvla/openvla}}. We fine-tune OpenVLA using LoRA~\cite{lora}, with rank 32 and a batch size of 16, training until convergence. To adapt OpenVLA to our control setting, we convert our absolute proprioceptive states to 3-dimensional delta position and 3-dimensional delta rotation (Euler angles), with an additional binary gripper dimension. We subsample our original collected data at a ratio of 10, since the difference between consecutive steps in the original data is too small for delta control, given the accuracy limit of OpenVLA $(10^{-3})$.

\section{Real-World Kinova Experiments}
\label{supp:kinova}
\subsection{Hardware}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/kinova_setup.jpg}
    \caption{The real-world experiment setup of Kinova robot.}
    \label{fig:kinova_setup}
\end{figure}

For our primary real-world experiments, we use a Kinova Gen3 7 DoF manipulator with a Robotiq 2F-85 gripper, as shown in~\cref{fig:kinova_setup}. It is mounted on a base that mimics the human shoulder orientation and height.

We set up two cameras (Logitech BRIO 4K camera) to observe the table-top manipulation scene. One is mounted at an ego-centric pose, and the other is mounted on the side of the table.

We use the MoveIt motion planning framework for inverse kinematics and end-effector position control. It takes the end effector position objective from the model, and executes linear trajectories in the Cartesian space.

\subsection{Data Collection}

To collect task demonstrations, we develop an automated data collection procedure to record episodes of these demonstrations. In this procedure, we give the ground truth locations of all objects on the table, and procedurally generate task objectives, demonstrations, and accompanying task instruction labels. Domain randomization is applied to diversify robot home position, grasping approach trajectory, and target pose.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/kinova_tasks.jpg}
    \caption{Task building of real-world Kinova setup.}
    \label{fig:kinova_tasks}
\end{figure*}

\subsection{Task Building}
We build 5 tasks under the Kinova real robot setup. The configuration of each task and its variations are shown in~\cref{fig:kinova_tasks}. The details of each task are described as follows.



\minisection{Pick Cube} The episode starts with the arm in the home position. The robot moves to pick up the cube, placed at a random location on the table within the manipulator's workspace. The episode recording stops after the robot picks up the object and moves up by a certain distance.

\minisection{Stack Cubes} The episode starts with the arm in the home position. After picking up the cube from point A as in the pick cube task, the robot stacks it on another object already present in the scene at point B according to the instruction. The episode recording stops after the robot releases the object and moves up by a certain distance.

\minisection{Destack Cubes} The episode starts with the arm in the home position. After picking up the top cube from a stacked pair at point A as in the grasping task, the robot moves the grasped cube to another location, point B. The episode recording stops after the robot releases the object and moves up by a certain distance.

\minisection{Pick \& Place Toys/Basketball} The episode starts with the arm in the home position. After picking up the toy described in the instruction from point A, the robot moves and places the toy into a bowl or basket at point B. The episode recording stops after the robot releases the object and moves up by a certain distance.

\minisection{Push Buttons} The episode starts with the arm in the home position. Following the instruction, the arm moves to a specific height above the assigned button at point A, closes the gripper, pushes the button, then moves to push another button at point B. The episode recording stops after the robot releases the object and moves up by a certain distance.






\section{Real-World Franka Experiments}
\label{supp:franka}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/franka_setup.jpg}
    \caption{The real-world experiment setup of Franka robot.}
    \label{fig:franka_setup}
\end{figure}

\subsection{Hardware}
We use a Franka Emika Panda robot with a Franka gripper for real robot data collection and evaluations. The Logitech BRIO 4K cameras positioned to the left and right of the Franka robot provides double-view RGB (without depth data) vision input to our model, as shown in~\cref{fig:franka_setup}. Camera autofocus is disabled, and the data is captured at 640x480 resolution. 

\subsection{Data Collection}
We use the data collection code and process from \href{https://github.com/Max-Fu/franka-scripted}{https://github.com/Max-Fu/franka-scripted} to collect data for automated tasks. The script generates data for an arbitrary number of episodes. For each episode, the process generates x-y positions on the table plane using a uniform random distribution for each axis. The script directs the robot to place the object at each location and then collects the camera and joint information as the robot is moving.


\subsection{Task Building}
We build cube tasks under the Franka real robot setup. The configurations of each task and its variations are shown in~\cref{fig:franka_setup}. The details of each task are described as follows.


\minisection{Pick Cube} The episode starts with the arm in the home position. The robot moves to pick up the cube from point A. The episode recording stops after the robot picks up the object and moves up by a certain distance.

\minisection{Stack Cubes} The episode starts with the arm in the home position. After picking up the cube from point A as in the grasping task, the robot stacks it on another object already present in the scene at point B according to the instruction. The episode recording stops after the robot releases the object and moves up by a certain distance.

\minisection{Destack Cubes} The episode starts with the arm in the home position. After picking up the top cube from a stacked pair at point A as in the grasping task, the robot moves the grasped cube to another location, point B. The episode recording stops after the robot releases the object and moves up by a certain distance.