
\section{Distribution of Per-Problem Single-Attempt Success Rates Creates Power Law Scaling}
\label{sec:distr_per_problem_success_rates}

How does polynomial scaling of the negative log \textit{average} success rate emerge from exponential scaling of the negative log \textit{per-problem} success rate?
The answer to this question \textit{must} lie in the distribution $\mathcal{D}$ over benchmark problems of single attempt (i.e., $k=1$) success rates because this distribution's density $p_{\mathcal{D}}(\operatorname{pass_i@1})$ links the per-problem scaling behavior to the aggregate scaling behavior via the definition of the aggregate success rate $\operatorname{pass_{\mathcal{D}}@k}$:
%
\begin{equation}
\begin{aligned}
    &\operatorname{pass_{\mathcal{D}}@k} \; \defeq \; \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\operatorname{pass_i@1} \sim \mathcal{D}} \Big[\operatorname{pass_i@k}(\operatorname{pass_i@1}) \Big]\\
    % &= 1 - \underbrace{\int_0^1 (1 - \operatorname{pass_i@1})^k \, p_{\mathcal{D}}(\operatorname{pass_i@1}) \, \operatorname{d\,pass_i@1}}_{\defeq \operatorname{fail_{\mathcal{D}}@k}}.
    &= 1 - \int_0^1 (1 - \operatorname{pass_i@1})^k \, p_{\mathcal{D}}(\operatorname{pass_i@1}) \, \operatorname{d\,pass_i@1}.
\end{aligned}
\end{equation}
% %
% For large $k$, $\operatorname{fail_{\mathcal{D}}@k}$ will be small and thus:
% \begin{equation}
%     -\log \Big( \operatorname{pass_{\mathcal{D}}@k} \big) \approx 
% \end{equation}


Based on a known result that power laws can originate from an appropriately weighted sum of exponential functions (Appendix ~\ref{app:sec:power_laws_from_distr_over_exp:background}), we begin by considering simple distributions for the single-attempt success probabilities and asking which yield power law scaling between $-\log(\operatorname{pass_{\mathcal{D}}@k})$ and $k$, as well as what properties of the distributions set the scaling exponent. In Appendices~\ref{app:sec:power_laws_from_distr_over_exp:uniform_distribution}-\ref{app:sec:power_laws_from_distr_over_exp:reciprocal_distribution}, we derive that several simple distributions yield power law scaling with different exponents whereas others do not:
%
\begin{align*}
    -\log \Big( &\operatorname{pass_{\mathrm{Uniform}(0,\, \beta \leq 1)}}@k &\Big) &\propto k^{-1}.\\
    -\log \Big( &\operatorname{pass_{\operatorname{Beta(\alpha, \beta)}}@k} &\Big) &\propto k^{-\alpha}.\\
    -\log \Big( &\operatorname{pass_{\operatorname{Kumaraswamy(\alpha,\, \beta)}}@k} &\Big) &\propto k^{-\alpha}.\\
    -\log \Big( &\operatorname{pass_{\operatorname{ContinuousBernoulli(\lambda < 1/2)}}@k} &\Big) &\propto k^{-1}.\\
    -\log \Big( &\operatorname{pass_{\operatorname{Reciprocal(0 < \alpha < \beta < 1)}}@k} &\Big) \propto &\frac{(1-\alpha)^k}{k}.
\end{align*}
%
To test this understanding, we examined whether the data of \citet{brown2024largelanguagemonkeysscaling} and \citet{hughes2024bestofnjailbreaking} had per-problem single-attempt success rate distributions that matched one of these simple distributions (Fig.~\ref{fig:multiple_attempts_pass_at_1_per_datum}). We found that the distributions could indeed be well fit by a 3-parameter $\operatorname{Kuamraswamy}(\alpha, \beta, a = 0, c)$ distribution with scale parameter $c$ (Fig.~\ref{fig:multiple_attempts_pass_at_1_per_datum}, black dashed lines); we found the scale parameter was critical to obtain good fits because the standard 2-parameter Kumaraswamy distribution is supported on $(0, 1)$ whereas most single-attempt success distributions have a smaller maximum such as $0.01$ or $0.1$.

More generally, what are the distributional properties that create such power law scaling and that set the specific power law exponent?
As we now show, the negative log average success rate will exhibit power law scaling in $k$ with exponent $b$ if and only if the distribution over problems of single-attempt success probabilities itself behaves like a power law near $0$ with exponent $b-1$:\newline

% If this condition is met, then then $\mathbb{E}_{\operatorname{pass_i@1} \sim \mathcal{D}}[(1 - \operatorname{pass_i@1})^k]$ decays as $k^{-b}$ and $-\log (\operatorname{pass_{\mathcal{D}}@k})$ inherits the same power-law exponent $b$.
% \josh{@Rylan can I swap these theorems out with the corrected ones?  The necessity claim is false, and I don't think we've identified a reasonable necessary condition yet.  We should also change the sufficiency one so that the distributions that you named follow as corollaries, otherwise the theorem is vacuous in the context of the paper.}
% \josh{I like the suspense as you build to the main theorem, but I wonder if it's more sensible to state the general claim first and then discuss the distributions that satisfy the sufficient condition.}
\begin{theorem}[Sufficiency of Power-Law Left Tail in Distribution of Single-Attempt Success Rates]
\label{thm:sufficiency_powerlaw}
Let $\mathcal{D}$ be a probability distribution on $[0,1]$ with PDF $p_{\mathcal{D}}(\operatorname{pass_i@1})$.  
Suppose there exist constants $b > 0$, $C > 0$, $\theta > 0$ and $\delta > 0$ such that, 
for all $0 < \operatorname{pass_i@1} < \delta$, we have 
\[
  p_{\mathcal{D}}(\operatorname{pass_i@1}) \;=\; C \cdot (\operatorname{pass_i@1})^{b-1} \;+\; O\bigl((\operatorname{pass_i@1})^{b-1+\theta}\bigr).
\]
% That is, there is a constant $M>0$ such that
% \[
%   \bigl|\,f(p) - C\,p^{\,b-1}\bigr|
%   \;\le\;
%   M\,p^{\,b-1+\theta}
%   \quad\text{for all }0<p<\delta.
% \]
% Define the aggregate success rate after $k$ attempts by
% \[
%   \operatorname{pass_{\mathcal{D}}@k}
%   \;\;=\;\;
%   \int_0^1 \Bigl[\,1 - (1 - p)^k\Bigr]\,f(p)\,\mathrm{d}p.
% \]
Then, for large $k$,
\[
  -\log\big(\operatorname{pass_{\mathcal{D}}@k}\big)
  \;\sim\;
  C\,\Gamma(b) \;k^{-b}.
\]
\end{theorem}



\begin{theorem}[Necessity of Power-Law Left Tail in Distribution of Single-Attempt Success Rates]
\label{thm:necessity_powerlaw}
Let $\mathcal{D}$ be a distribution over $\operatorname{pass_i@1} \in [0,1]$ with PDF $p_{\mathcal{D}}(\operatorname{pass_i@1})$.
Suppose there exist constants $b > 0$ and $A > 0$ such that for large $k$,
\[
-\log\big(\operatorname{pass_{\mathcal{D}}@k}\big)
\sim 
A\,k^{-b}.
\]
Then, under mild regularity assumptions, the probability density must satisfy
\[
p_{\mathcal{D}}(\operatorname{pass_i@1})
\;\sim\;
\frac{A}{\Gamma(b)} \, (\operatorname{pass_i@1})^{b - 1}
\quad
\text{as } \operatorname{pass_i@1} \to 0^+.
\]
% where $C>0$ is a constant depending on $A$ and $b$.  
% In other words, whenever $-\log\bigl(\operatorname{pass_{\mathcal{D}}@k}\bigr)$ decays like a power law $k^{-b}$, the distribution $\mathcal{D}$ must have a PDF that behaves like $p^{\,b-1}$ near $p=0$ (up to a multiplicative constant).
\end{theorem}

In Fig.~\ref{fig:schematic}, we illustrate this connection schematically.
For proofs, see Appendices \ref{app:sec:power_laws_from_distr_over_exp:sufficiency} and \ref{app:sec:power_laws_from_distr_over_exp:necessity}.
These results clarify that whenever $-\log (\operatorname{pass_{\mathcal{D}}@k} )$ exhibits power-law decay in $k$ with exponent $b$, the distribution over problems of single-attempt success rates \emph{must} have ``polynomial weight'' near $\operatorname{pass_i@1}=0$, i.e.\ $p_{\mathcal{D}}(p) = \Theta(p^{\,b-1})$.

To offer intuition, we know that each problem is being solved by the model (or equivalently, each prompt is jailbreaking the model) exponentially quickly.
If one looks across all problems in the benchmark, some have $\operatorname{pass_i@1}$ so small that they remain unsolved for many, many attempts.
Whether these ``tiny‐$\operatorname{pass_i@1}$" problems still matter at large $k$ depends on how \emph{many} such problems there are.
Polynomial density near $0$ ``piles up" enough hard problems in just the right way such that even though each of those problems is being solved exponentially quickly, the \emph{aggregate} success rate over problems decreases at only a power‐law rate in $k$.
A more succinct mathematical summary is that, for a compound binomial distribution, the lower tail probability controls the upper tail of the marginal survivor function.