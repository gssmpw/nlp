%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[T1]{fontenc}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{leftindex}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{listings}

\input{math_commands}

\newcommand{\rylan}[1]{\textcolor{red}{RS: #1}}
\newcommand{\aengus}[1]{\textcolor{cyan}{Aengus: #1}}
\newcommand{\josh}[1]{\textcolor{blue}{JK: #1}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{How Do Large Language Monkeys Get Their Power (Laws)?}

\begin{document}

\twocolumn[
\icmltitle{
How Do Large Language Monkeys Get Their Power (Laws)?
% Title 2: The Origin of Power Law Scaling by Repeatedly Sampling from Neural Language Models\\
% Title 3: When and Why Repeatedly Sampling from\\Neural Language Models Yields Power Law Scaling
%The Origin of Power Laws in Scaling Inference Compute via Repeat Sampling
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rylan Schaeffer}{stanfordcs}
\icmlauthor{Joshua Kazdan}{stanfordstats}
\icmlauthor{John Hughes}{speechmatics,mats}
\icmlauthor{Jordan Juravsky}{stanfordcs}
\icmlauthor{Sara Price}{mats}
\icmlauthor{Aengus Lynch}{mats,ucl}
\icmlauthor{Erik Jones}{anthropic}
\icmlauthor{Robert Kirk}{ucl}
\icmlauthor{Azalia Mirhoseini}{stanfordcs}
\icmlauthor{Sanmi Koyejo}{stanfordcs}
\end{icmlauthorlist}

\icmlaffiliation{stanfordcs}{Stanford Computer Science}
\icmlaffiliation{stanfordstats}{Stanford Statistics}
\icmlaffiliation{speechmatics}{Speechmatics}
\icmlaffiliation{ucl}{University College London}
\icmlaffiliation{anthropic}{Anthropic}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Rylan Schaeffer}{rschaef@cs.stanford.edu}
\icmlcorrespondingauthor{Sanmi Koyejo}{sanmi@cs.stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


% Remaining TODOs:
% 4. How does sample efficiency compare?

\begin{abstract}
Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts.
In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts.
We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge?
We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.
We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\sim}2-4$ orders of magnitude less inference compute.
Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.
\end{abstract}


\input{01_introduction}
\input{02_puzzling}
\input{03_pass_at_1_distributions}
\input{04_distributional_structure_explains_non_powerlaws}
\input{05_efficient_power_law_exponent_estimation}
\input{07_discussion}



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\clearpage

\bibliography{references_rylan}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{99_appendix}
% \input{corrected_proofs_by_josh}
\end{document}
