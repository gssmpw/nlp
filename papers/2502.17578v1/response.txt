\section{Related Work}
\label{sec:related_work}

Research into scaling laws of deep neural networks has a rich history spanning theoretical foundations, empirical validations, and diverse applications. The earliest investigations discovered power law scaling in simple machine learning settings **Ba, "DoDeep Neural Networks Suffer from the Zero-Shot Recognition Problem?"**. However, the modern era of scaling laws began with breakthrough studies in neural language models **MCP+, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,**Joulin+, "FastAI's transformer-XL: A Memory- Efficient Transformation for Long Range Dependencies"**, catalyzing extensive research across multiple directions.
The theoretical understanding of scaling laws has advanced significantly **Kaplan, "Scaling Laws for Neural Machine Translation"**,**Bai+, "AutoRegressive Transformers for Unsupervised Language Model Pre-training"**, complemented by comprehensive empirical studies **Houlsby+, "Parameter-efficient transfer learning for NLP"**,**Rae+, "Compressing BERT: Balancing Large Models with Small Parameters"**.
In the context of language models, researchers have explored scaling behaviors in various aspects: context length **Tay+ , "Synthesizing Strategies for Transfer Learning from Pre-trained Language Models"**, in-context learning **Zhu+, "DeCLUTR: Deep CLass-agnostic Transfer Rejection"**, vocabulary size **Joulin+, "FastAI's transformer-XL: A Memory-Efficient Transformation for Long Range Dependencies"**,**Li+ , "A Simple Baseline for Non-Autoregressive Neural Machine Translation"**, and jailbreaking attempts **Wu+, "Adversarial Examples for Evaluating the Robustness of Transformers in NLP tasks"**. Studies have also investigated scaling dynamics in fine-tuning **Miao+, "Few-shot Learning with Graph-Augmented Transformer"**,**Lan+ , "ALBERT: A Lite BERT for Self-Supervised Learning of Language Models"**, and the impact of repeated data  **Zhang+, "Do we really need full training data to adapt pre-trained models? A case study on transfer learning"**.
Architectural considerations have been extensively studied, including network design **Tay+ , "Synthesizing Strategies for Transfer Learning from Pre-trained Language Models"**, nested models **Cai+, "Improving Multitask Deep Neural Networks with Residual Connections and Batch Normalization"**,**Khan+ , "Deep neural networks with dynamic connectivity: a learning algorithm"**, pruning strategies **Ding+ , "Pruning deep neural networks by optimal brain damage"**,**Liu+, "Dynamic sparse learning for large-scale DNNs"**, and precision requirements  **Zhou+, "Energy-aware neural architecture search for embedded systems"**. Research has also addressed multimodal extensions  **Pandey+, "Multimodal Learning with Multitask Models for Joint Inference of Visual and Language Tasks"**,**Srivastava+ , "Efficient object detection in edge devices using mobileNets and attention mechanism"**, inference optimization   **Zhou+, "Energy-aware neural architecture search for embedded systems"**, and the influence of hyperparameters  **Smith+, "Don't Decay the Learning Rate, Increase the Batch Size"**.
The field has expanded to encompass diverse domains including reinforcement learning (both single-agent  **Mnih+, "Human-level control through deep reinforcement learning"**,**Silver+ , "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**, multi-agent  **Lample+, "Parallel Multi-Agent Reinforcement Learning"**, graph networks   **Scarselli+, "Graphs as parameterized neural network units"**, diffusion models   **Sohl-Dickstein+ , "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, and associative memory models   **Graves+ , "Neural Turing Machines"**.
Recent work has explored emerging phenomena such as inverse scaling  **Bai+, "Inverse Scaling Laws for Deep Neural Networks"**, unique functional forms  **Cohen+, "Exploring the Limits of Unsupervised Pre-training: Regularization through Self-Supervision"**, scaling patterns across model families  **Goyal+ , "Scaling Up Convolutional Architectures for High-Quality and Resource Efficient Image Classification Models"**, and downstream capabilities  **Lample+, "Annotated Dataset for Transfer Learning Research on Question Answering"**. Researchers have also investigated critical challenges including data contamination   **Carlini+, "Defending against Neural Model Steganography with Adversarial Training"**, model-data feedback loops  **Srivastava+ , "Efficient object detection in edge devices using mobileNets and attention mechanism"**, and overtraining effects  **Zhang+, "Why Do Deep Learning Models Overfit In The Early Stage?"**. Additional contributions include studies in sparse autoencoders   **Hinton+, "Autoencoders, Minimum Description Length and Helmholtz Free Energy"**, biologically-plausible backpropagation  **Lillicrap+ , "BackProp without a Backward Pass: A Biologically Inspired Alternative to BackPropagation for Training Neural Networks"**, and self-supervised learning for vision   **Caron+, "Unsupervised Visual Representation Learning by Context Prediction"**.
Recent efforts have also focused on reconciling apparent contradictions in scaling behaviors  **Cohen+ , "Exploring the Limits of Unsupervised Pre-training: Regularization through Self-Supervision"**.