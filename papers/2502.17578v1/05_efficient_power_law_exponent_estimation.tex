\begin{figure*}[t!]
    \centering
    % \includegraphics[width=\linewidth]{figures/52_compare_power_law_exponent_estimators_synthetic_data/y=relative_error_x=n_hue=distribution_params_col=distribution.pdf}
    \includegraphics[width=0.95\linewidth]{figures/52_compare_power_law_exponent_estimators_synthetic_data/y=relative_error_least_squares_x=n_hue=distribution_params_col=distribution.pdf}
    \caption{\textbf{Comparing Two Estimators of Power Law Exponents via Backtesting.} On synthetic data with known ground-truth power law $a \, k^{-b}$, we compare how well the least squares and the distributional estimator recover the scaling exponent $b$ as measured by the relative error $|\hat{b} - b| / b$ by backtesting: subsampling the number of problems and the number of samples per problem. We find that the distributional estimator obtains significantly better sample efficiency.}
    \label{fig:backtesting}
\end{figure*}


% \begin{figure*}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/52_compare_power_law_exponent_estimators_synthetic_data/y=relative_error_x=n_hue=distribution_params_col=distribution.pdf}
%     \caption{Placeholder}
%     \label{fig:enter-label}
% \end{figure*}



\section{A New Distributional Estimator for Predicting Power Law Scaling}
\label{sec:estimating_power_law_exponent}

A natural consequence of this connection between the scaling of $-\log(\operatorname{pass_{\mathcal{D}}@k})$ and the left tail of the distribution $p_{\mathcal{D}}(\operatorname{pass_i@1})$ is that the distribution of single-attempt success rates can be used to predict whether power-law scaling will appear and if so, what the intercept and exponent of the power law will be. To do this, one can fit the distribution $\hat{p}_{\mathcal{D}}(\operatorname{pass_i@1})$ and then \textit{simulate} how $\operatorname{pass_{\mathcal{D}}@k}$ will scale with $k$ (Fig.~\ref{fig:schematic2}) using the relationship:
%
\begin{equation}
\begin{aligned}
&\widehat{\operatorname{pass_{\mathcal{D}}@k}} \defeq \\
&1 - \int_0^1 (1 - \operatorname{pass_i@1})^k \, \hat{p}_{\mathcal{D}}(\operatorname{pass_i@1}) \, \operatorname{d\,pass_i@1}.
\end{aligned}
\end{equation}
%
To empirically test this claim, we compared the standard least squares regression estimator (in log-log space) \citep{hoffmann2022trainingcomputeoptimallargelanguage,caballero2022broken,besiroglu2024chinchillascalingreplicationattempt} against a \textit{distributional estimator}.
To motivate our distributional estimator, we first need explain a key obstacle and how the distributional estimator overcomes it.
The obstacle is that there are problems or prompts whose single-attempt success probabilities $\operatorname{pass_i@1}$ lie between $(0, 1/\text{Number of Samples})$ such that, due to finite sampling, we lack the resolution to measure.
While we do not know the true single-attempt success probability for the problems that lie in this interval, we \textit{do} know \textit{how many} problems fall into this left tail bucket, and we can fit a distribution's parameters such that the distribution's probability mass in the interval $(0, 1 / \text{Number of Samples})$ matches the empirical fraction of problems in this tail bucket. Thus, our distributional estimator works by first selecting a distribution (e.g., a scaled 3-parameter Beta distribution), discretizing the distribution according to the sampling resolution $1 / \text{Number of Samples}$ and performing maximum likelihood estimation under the discretized distribution's probability mass function.

We tested this distributional estimator in two different ways. First, focusing on Large Language Monkeys, we used all available real data from all problems and all samples per problem to compare the standard least squares regression estimator against the distributional estimator. 
We found close agreement between the two estimators (Fig.~\ref{fig:comparison_power_law_exponents}), giving us a sense that the two estimators yield reasonably consistent estimates under large sampling budgets.

Second, the distributional estimator also comes with another benefit: it directly provides an estimate of the power law's exponent $b$ in $a \, k^{-b}$. Estimating the power law's exponent is especially valuable because the exponent dictates how success rates are improving with increasing inference compute. To test how the distributional estimator and least squares estimator compare at recovering the true asymptotic power law exponent, we generated synthetic data so that we would have ground-truth knowledge of the true power law exponent, then backtested how the two scaling estimators compare at recovering the true exponent \citep{alabdulmohsin2022revisitingneuralscalinglaws, owen2024predicting} by subsampling data with fewer problems and fewer samples per problem.
We found that the distributional estimator obtains significantly better sample efficiency, with approximately an order of magnitude lower relative error $\defeq |\hat{b} - b| / b$ compared with the least squares estimator (Fig.~\ref{fig:backtesting}), or equivalently, ${\sim}2-4$ orders of magnitude less inference-compute. The distributional estimator performs well even under distributional mismatch.