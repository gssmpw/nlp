@inproceedings{agarwal2024manyshot,
title={Many-Shot In-Context Learning},
author={Rishabh Agarwal and Avi Singh and Lei M Zhang and Bernd Bohnet and Luis Rosias and Stephanie C.Y. Chan and Biao Zhang and Ankesh Anand and Zaheer Abbas and Azade Nova and John D Co-Reyes and Eric Chu and Feryal Behbahani and Aleksandra Faust and Hugo Larochelle},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=AB6XpMzvqH}
}

@inproceedings{aghajanyan2023scalinggenerativemultimodallm,
  title={Scaling laws for generative mixed-modal language models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={265--279},
  year={2023},
  organization={PMLR}
}

@article{alabdulmohsin2022revisitingscalinglaws,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22300--22312},
  year={2022}
}

@inproceedings{anil2024manyshot,
title={Many-shot Jailbreaking},
author={Cem Anil and Esin DURMUS and Nina Rimsky and Mrinank Sharma and Joe Benton and Sandipan Kundu and Joshua Batson and Meg Tong and Jesse Mu and Daniel J Ford and Francesco Mosconi and Rajashree Agrawal and Rylan Schaeffer and Naomi Bashkansky and Samuel Svenningsen and Mike Lambert and Ansh Radhakrishnan and Carson Denison and Evan J Hubinger and Yuntao Bai and Trenton Bricken and Timothy Maxwell and Nicholas Schiefer and James Sully and Alex Tamkin and Tamera Lanham and Karina Nguyen and Tomasz Korbak and Jared Kaplan and Deep Ganguli and Samuel R. Bowman and Ethan Perez and Roger Baker Grosse and David Duvenaud},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=cw5mgd71jW}
}

@misc{arora2024bayesianscalinglawsincontext,
      title={Bayesian scaling laws for in-context learning}, 
      author={Aryaman Arora and Dan Jurafsky and Christopher Potts and Noah D. Goodman},
      year={2024},
      eprint={2410.16531},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16531}, 
}

@article{atanasov2024scaling,
  title={Scaling and renormalization in high-dimensional regression},
  author={Atanasov, Alexander and Zavatone-Veth, Jacob A and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.00592},
  year={2024}
}

@misc{bachmann2023scalingmlpstaleinductive,
      title={Scaling MLPs: A Tale of Inductive Bias}, 
      author={Gregor Bachmann and Sotiris Anagnostidis and Thomas Hofmann},
      year={2023},
      eprint={2306.13575},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.13575}, 
}

@article{bahri2024explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={27},
  pages={e2311878121},
  year={2024},
  publisher={National Academy of Sciences}
}

@article{barkai1993scaling,
  title={Scaling laws in learning of classification tasks},
  author={Barkai, N and Seung, Hyunjune Sebastian and Sompolinsky, Haim},
  journal={Physical review letters},
  volume={70},
  number={20},
  pages={3167},
  year={1993},
  publisher={APS}
}

@misc{besiroglu2024chinchillascalingreplicationattempt,
      title={Chinchilla Scaling: A replication attempt}, 
      author={Tamay Besiroglu and Ege Erdil and Matthew Barnett and Josh You},
      year={2024},
      eprint={2404.10102},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.10102}, 
}

@article{bordelon2024dynamical,
  title={A dynamical model of neural scaling laws},
  author={Bordelon, Blake and Atanasov, Alexander and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2402.01092},
  year={2024}
}

@article{bordelon2024feature,
  title={How feature learning can improve neural scaling laws},
  author={Bordelon, Blake and Atanasov, Alexander and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2409.17858},
  year={2024}
}

@misc{bousquet2020theoryuniversallearning,
      title={A Theory of Universal Learning}, 
      author={Olivier Bousquet and Steve Hanneke and Shay Moran and Ramon van Handel and Amir Yehudayoff},
      year={2020},
      eprint={2011.04483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.04483}, 
}

@article{brill2024neural,
  title={Neural Scaling Laws Rooted in the Data Distribution},
  author={Brill, Ari},
  journal={arXiv preprint arXiv:2412.07942},
  year={2024}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{brown2024largelanguagemonkeysscaling,
      title={Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}, 
      author={Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher Ré and Azalia Mirhoseini},
      year={2024},
      eprint={2407.21787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.21787}, 
}

@article{caballero2022broken,
  title={Broken neural scaling laws},
  author={Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
  journal={arXiv preprint arXiv:2210.14891},
  year={2022}
}

@misc{cabannes2024scalinglawsassociativememories,
      title={Scaling Laws for Associative Memories}, 
      author={Vivien Cabannes and Elvis Dohmatob and Alberto Bietti},
      year={2024},
      eprint={2310.02984},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2310.02984}, 
}

@inproceedings{chan2022datadistributionalincontextlearning,
 author = {Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {18878--18891},
 publisher = {Curran Associates, Inc.},
 title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{chen2024simpleprovablescalinglaw,
      title={A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models}, 
      author={Yanxi Chen and Xuchen Pan and Yaliang Li and Bolin Ding and Jingren Zhou},
      year={2024},
      eprint={2411.19477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.19477}, 
}

@inproceedings{cherti2023scalingcontrastivelanguageimagelearning,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2818--2829},
  year={2023}
}

@inproceedings{clark2022unifiedscalinglawsroutedlanguagemodels,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International conference on machine learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}

@inproceedings{dettmers20234bitprecisionscaling,
  title={The case for 4-bit precision: k-bit inference scaling laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={7750--7774},
  year={2023},
  organization={PMLR}
}

@misc{dohmatob2024taletailsmodelcollapse,
      title={A Tale of Tails: Model Collapse as a Change of Scaling Laws}, 
      author={Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe},
      year={2024},
      eprint={2402.07043},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.07043}, 
}

@misc{dominguezolmedo2024trainingtesttaskconfounds,
      title={Training on the Test Task Confounds Evaluation and Emergence}, 
      author={Ricardo Dominguez-Olmedo and Florian E. Dorner and Moritz Hardt},
      year={2024},
      eprint={2407.07890},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.07890}, 
}

@misc{filipovich2022scalinglawsbackpropagation,
      title={Scaling Laws Beyond Backpropagation}, 
      author={Matthew J. Filipovich and Alessandro Cappelli and Daniel Hesslow and Julien Launay},
      year={2022},
      eprint={2210.14593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.14593}, 
}

@article{gadre2024language,
  title={Language models scale reliably with over-training and on downstream tasks},
  author={Gadre, Samir Yitzhak and Smyrnis, Georgios and Shankar, Vaishaal and Gururangan, Suchin and Wortsman, Mitchell and Shao, Rulin and Mercat, Jean and Fang, Alex and Li, Jeffrey and Keh, Sedrick and others},
  journal={arXiv preprint arXiv:2403.08540},
  year={2024}
}

@InProceedings{gao2023scalinglawsrewardmodeloveroptimization,
  title = 	 {Scaling Laws for Reward Model Overoptimization},
  author =       {Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {10835--10866},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  url = 	 {https://proceedings.mlr.press/v202/gao23h.html},
  abstract = 	 {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart’s law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed “gold-standard” reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-$n$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024}
}

@misc{gerstgrasser2024modelcollapseinevitablebreaking,
      title={Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data}, 
      author={Matthias Gerstgrasser and Rylan Schaeffer and Apratim Dey and Rafael Rafailov and Henry Sleight and John Hughes and Tomasz Korbak and Rajashree Agrawal and Dhruv Pai and Andrey Gromov and Daniel A. Roberts and Diyi Yang and David L. Donoho and Sanmi Koyejo},
      year={2024},
      eprint={2404.01413},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.01413}, 
}

@inproceedings{ghorbani2021scaling,
  title={Scaling Laws for Neural Machine Translation},
  author={Ghorbani, Behrooz and Firat, Orhan and Freitag, Markus and Bapna, Ankur and Krikun, Maxim and Garcia, Xavier and Chelba, Ciprian and Cherry, Colin},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{gordon2021dataparamscalingnmt,
    title = "Data and Parameter Scaling Laws for Neural Machine Translation",
    author = "Gordon, Mitchell A  and
      Duh, Kevin  and
      Kaplan, Jared",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.478",
    doi = "10.18653/v1/2021.emnlp-main.478",
    pages = "5915--5922",
    abstract = "We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs.",
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@misc{hernandez2021scalinglawstransfer,
      title={Scaling Laws for Transfer}, 
      author={Danny Hernandez and Jared Kaplan and Tom Henighan and Sam McCandlish},
      year={2021},
      eprint={2102.01293},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.01293}, 
}

@article{hernandez2022scalingrepeatdata,
  title={Scaling laws and interpretability of learning from repeated data},
  author={Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Henighan, Tom and Hume, Tristan and others},
  journal={arXiv preprint arXiv:2205.10487},
  year={2022}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@misc{hilton2023scalinglawssingleagentreinforcement,
      title={Scaling laws for single-agent reinforcement learning}, 
      author={Jacob Hilton and Jie Tang and John Schulman},
      year={2023},
      eprint={2301.13442},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.13442}, 
}

@misc{hu2024predictingemergentabilitiesinfinite,
      title={Predicting Emergent Abilities with Infinite Resolution Evaluation}, 
      author={Shengding Hu and Xin Liu and Xu Han and Xinrong Zhang and Chaoqun He and Weilin Zhao and Yankai Lin and Ning Ding and Zebin Ou and Guoyang Zeng and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2310.03262},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03262}, 
}

@misc{hughes2024bestofnjailbreaking,
      title={Best-of-N Jailbreaking}, 
      author={John Hughes and Sara Price and Aengus Lynch and Rylan Schaeffer and Fazl Barez and Sanmi Koyejo and Henry Sleight and Erik Jones and Ethan Perez and Mrinank Sharma},
      year={2024},
      eprint={2412.03556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.03556}, 
}

@misc{hutter2021learningcurvetheory,
      title={Learning Curve Theory}, 
      author={Marcus Hutter},
      year={2021},
      eprint={2102.04074},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.04074}, 
}

@misc{jiang2024investigatingdatacontaminationpretraining,
      title={Investigating Data Contamination for Pre-training Language Models}, 
      author={Minhao Jiang and Ken Ziyu Liu and Ming Zhong and Rylan Schaeffer and Siru Ouyang and Jiawei Han and Sanmi Koyejo},
      year={2024},
      eprint={2401.06059},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06059}, 
}

@article{jones2021scaling,
  title={Scaling scaling laws with board games},
  author={Jones, Andy L},
  journal={arXiv preprint arXiv:2104.03113},
  year={2021}
}

@misc{kalajdzievski2024scalinglawsforgettingfinetuning,
      title={Scaling Laws for Forgetting When Fine-Tuning Large Language Models}, 
      author={Damjan Kalajdzievski},
      year={2024},
      eprint={2401.05605},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.05605}, 
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{kazdan2024collapsethriveperilspromises,
      title={Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World}, 
      author={Joshua Kazdan and Rylan Schaeffer and Apratim Dey and Matthias Gerstgrasser and Rafael Rafailov and David L. Donoho and Sanmi Koyejo},
      year={2024},
      eprint={2410.16713},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.16713}, 
}

@article{kudugunta2023matformer,
  title={Matformer: Nested transformer for elastic inference},
  author={Kudugunta, Sneha and Kusupati, Aditya and Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit and Tsvetkov, Yulia and Hajishirzi, Hannaneh and Kakade, Sham and Farhadi, Ali and Jain, Prateek and others},
  journal={arXiv preprint arXiv:2310.07707},
  year={2023}
}

@article{kumar2024scalinglawprecision,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}

@misc{liang2024scalinglawsdiffusiontransformers,
      title={Scaling Laws For Diffusion Transformers}, 
      author={Zhengyang Liang and Hao He and Ceyuan Yang and Bo Dai},
      year={2024},
      eprint={2410.08184},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.08184}, 
}

@article{lin2024scaling,
  title={Scaling Laws in Linear Regression: Compute, Parameters, and Data},
  author={Lin, Licong and Wu, Jingfeng and Kakade, Sham M and Bartlett, Peter L and Lee, Jason D},
  journal={arXiv preprint arXiv:2406.08466},
  year={2024}
}

@misc{liu2024neuralscalinglawsgraphs,
      title={Towards Neural Scaling Laws on Graphs}, 
      author={Jingzhe Liu and Haitao Mao and Zhikai Chen and Tong Zhao and Neil Shah and Jiliang Tang},
      year={2024},
      eprint={2402.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02054}, 
}

@article{maloney2022solvable,
  title={A solvable model of neural scaling laws},
  author={Maloney, Alexander and Roberts, Daniel A and Sully, James},
  journal={arXiv preprint arXiv:2210.16859},
  year={2022}
}

@misc{mckenzie2023inversescalingbiggerisnt,
      title={Inverse Scaling: When Bigger Isn't Better}, 
      author={Ian R. McKenzie and Alexander Lyzhov and Michael Pieler and Alicia Parrish and Aaron Mueller and Ameya Prabhu and Euan McLean and Aaron Kirtland and Alexis Ross and Alisa Liu and Andrew Gritsevskiy and Daniel Wurgaft and Derik Kauffman and Gabriel Recchia and Jiacheng Liu and Joe Cavanagh and Max Weiss and Sicong Huang and The Floating Droid and Tom Tseng and Tomasz Korbak and Xudong Shen and Yuhui Zhang and Zhengping Zhou and Najoung Kim and Samuel R. Bowman and Ethan Perez},
      year={2024},
      eprint={2306.09479},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09479}, 
}

@misc{mei2024biggerbetterscalingproperties,
      title={Bigger is not Always Better: Scaling Properties of Latent Diffusion Models}, 
      author={Kangfu Mei and Zhengzhong Tu and Mauricio Delbracio and Hossein Talebi and Vishal M. Patel and Peyman Milanfar},
      year={2024},
      eprint={2404.01367},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.01367}, 
}

@article{mhaskar1996neural,
  title={Neural networks for optimal approximation of smooth and analytic functions},
  author={Mhaskar, Hrushikesh N},
  journal={Neural computation},
  volume={8},
  number={1},
  pages={164--177},
  year={1996},
  publisher={MIT Press}
}

@article{michaud2024quantization,
  title={The quantization model of neural scaling},
  author={Michaud, Eric and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{muennighoff2023scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50358--50376},
  year={2023}
}

@article{neumann2022scalingmultiagentrl,
  title={Scaling laws for a multi-agent reinforcement learning model},
  author={Neumann, Oren and Gros, Claudius},
  journal={arXiv preprint arXiv:2210.00849},
  year={2022}
}

@misc{neumann2024alphazeroneuralscalingzipfs,
      title={AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws}, 
      author={Oren Neumann and Claudius Gros},
      year={2024},
      eprint={2412.11979},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.11979}, 
}

@article{paquette2024fourplus3phases,
  title={4+ 3 Phases of Compute-Optimal Neural Scaling Laws},
  author={Paquette, Elliot and Paquette, Courtney and Xiao, Lechao and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:2405.15074},
  year={2024}
}

@article{pinkus1999approximation,
  title={Approximation theory of the MLP model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}

@misc{polo2024slothscalinglawsllm,
      title={Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families}, 
      author={Felipe Maia Polo and Seamus Somerstep and Leshem Choshen and Yuekai Sun and Mikhail Yurochkin},
      year={2024},
      eprint={2412.06540},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.06540}, 
}

@misc{porian2024resolvingdiscrepanciescomputeoptimalscaling,
      title={Resolving Discrepancies in Compute-Optimal Scaling of Language Models}, 
      author={Tomer Porian and Mitchell Wortsman and Jenia Jitsev and Ludwig Schmidt and Yair Carmon},
      year={2024},
      eprint={2406.19146},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19146}, 
}

@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  volume={46},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{romani2013scalingassociativememory,
  title={Scaling laws of associative memory retrieval},
  author={Romani, Sandro and Pinkoviezky, Itai and Rubin, Alon and Tsodyks, Misha},
  journal={Neural computation},
  volume={25},
  number={10},
  pages={2523--2544},
  year={2013},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{rosenfeld2020constructive,
  title={A Constructive Prediction of the Generalization Error Across Scales},
  author={Rosenfeld, Jonathan S and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{rosenfeld2021pruningacrossscales,
  title = 	 {On the Predictability of Pruning Across Scales},
  author =       {Rosenfeld, Jonathan S and Frankle, Jonathan and Carbin, Michael and Shavit, Nir},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9075--9083},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rosenfeld21a/rosenfeld21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rosenfeld21a.html},
  abstract = 	 {We show that the error of iteratively magnitude-pruned networks empirically follows a scaling law with interpretable coefficients that depend on the architecture and task. We functionally approximate the error of the pruned networks, showing it is predictable in terms of an invariant tying width, depth, and pruning level, such that networks of vastly different pruned densities are interchangeable. We demonstrate the accuracy of this approximation over orders of magnitude in depth, width, dataset size, and density. We show that the functional form holds (generalizes) for large scale data (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks become ever larger and costlier to train, our findings suggest a framework for reasoning conceptually and analytically about a standard method for unstructured pruning.}
}

@misc{ruan2024observationalscalinglawspredictability,
      title={Observational Scaling Laws and the Predictability of Language Model Performance}, 
      author={Yangjun Ruan and Chris J. Maddison and Tatsunori Hashimoto},
      year={2024},
      eprint={2405.10938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.10938}, 
}

@inproceedings{sardana2023beyondchinchillaoptimal,
  title={Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
  author={Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2023}
}

@misc{schaeffer2023pretrainingtestsetneed,
      title={Pretraining on the Test Set Is All You Need}, 
      author={Rylan Schaeffer},
      year={2023},
      eprint={2309.08632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.08632}, 
}

@misc{schaeffer2024bridgingassociativememoryprobabilistic,
      title={Bridging Associative Memory and Probabilistic Modeling}, 
      author={Rylan Schaeffer and Nika Zahedi and Mikail Khona and Dhruv Pai and Sang Truong and Yilun Du and Mitchell Ostrow and Sarthak Chandra and Andres Carranza and Ila Rani Fiete and Andrey Gromov and Sanmi Koyejo},
      year={2024},
      eprint={2402.10202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.10202}, 
}

@misc{schaeffer2024elusive,
      title={Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?}, 
      author={Rylan Schaeffer and Hailey Schoelkopf and Brando Miranda and Gabriel Mukobi and Varun Madan and Adam Ibrahim and Herbie Bradley and Stella Biderman and Sanmi Koyejo},
      year={2024},
      eprint={2406.04391},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04391}, 
}

@misc{schaeffer2024improvedunderstandingutilizationmaximum,
      title={Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations}, 
      author={Rylan Schaeffer and Victor Lecomte and Dhruv Bhandarkar Pai and Andres Carranza and Berivan Isik and Alyssa Unell and Mikail Khona and Thomas Yerxa and Yann LeCun and SueYeon Chung and Andrey Gromov and Ravid Shwartz-Ziv and Sanmi Koyejo},
      year={2024},
      eprint={2406.09366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.09366}, 
}

@article{sharma2022scaling,
  title={Scaling laws from the data manifold dimension},
  author={Sharma, Utkarsh and Kaplan, Jared},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={9},
  pages={1--34},
  year={2022}
}

@misc{snell2024predictingemergentcapabilitiesfinetuning,
      title={Predicting Emergent Capabilities by Finetuning}, 
      author={Charlie Snell and Eric Wallace and Dan Klein and Sergey Levine},
      year={2024},
      eprint={2411.16035},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.16035}, 
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{spigler2020asymptoticlearningcurves,
   title={Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
   volume={2020},
   ISSN={1742-5468},
   url={http://dx.doi.org/10.1088/1742-5468/abc61d},
   DOI={10.1088/1742-5468/abc61d},
   number={12},
   journal={Journal of Statistical Mechanics: Theory and Experiment},
   publisher={IOP Publishing},
   author={Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
   year={2020},
   month=dec, pages={124001} }

@misc{srivastava2023imitationgamequantifyingextrapolating,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.04615}, 
}

@misc{sun2025scalinglawsfloatingpoint,
      title={Scaling Laws for Floating Point Quantization Training}, 
      author={Xingwu Sun and Shuaipeng Li and Ruobing Xie and Weidong Han and Kan Wu and Zhen Yang and Yixing Li and An Wang and Shuai Li and Jinbao Xue and Yu Cheng and Yangyu Tao and Zhanhui Kang and Chengzhong Xu and Di Wang and Jie Jiang},
      year={2025},
      eprint={2501.02423},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.02423}, 
}

@article{tao2024scalingvocabulary,
  title={Scaling laws with vocabulary: Larger models deserve larger vocabularies},
  author={Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
  journal={arXiv preprint arXiv:2407.13623},
  year={2024}
}

@article{tay2021scaleefficiently,
  title={Scale efficiently: Insights from pre-training and fine-tuning transformers},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={arXiv preprint arXiv:2109.10686},
  year={2021}
}

@misc{tay2022transcendingscalinglaws01,
      title={Transcending Scaling Laws with 0.1% Extra Compute}, 
      author={Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier Garcia and Huaixiu Steven Zheng and Jinfeng Rao and Aakanksha Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and Neil Houlsby and Quoc V. Le and Mostafa Dehghani},
      year={2022},
      eprint={2210.11399},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.11399}, 
}

@inproceedings{tay2023scalingvsarchitecture,
  title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q and Yogatama, Dani and Metzler, Donald},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{wu2024inferencescalinglawsempirical,
      title={Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models}, 
      author={Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang},
      year={2024},
      eprint={2408.00724},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00724}, 
}

@misc{wu2024ushapedinverteduscalingemergent,
      title={U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models}, 
      author={Tung-Yu Wu and Pei-Yu Lo},
      year={2024},
      eprint={2410.01692},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.01692}, 
}

@misc{xiong2023effectivelongcontextscalingfoundation,
      title={Effective Long-Context Scaling of Foundation Models}, 
      author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
      year={2023},
      eprint={2309.16039},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16039}, 
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12104--12113},
  year={2022}
}

@misc{zhang2024scalingmeetsllmfinetuning,
      title={When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method}, 
      author={Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
      year={2024},
      eprint={2402.17193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17193}, 
}

