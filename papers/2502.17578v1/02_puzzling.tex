

\section{Should Power Law Scaling Be Expected?}
\label{sec:should_power_laws_be_expected}

Should we expect large language monkeys to have such power  (laws)? That is, should the negative log of the average success rate scale polynomially with the number of independent attempts $k$? As we now explain mathematically and demonstrate empirically, such polynomial scaling with $k$ is perhaps surprising because, for any single problem, the negative log success rate at $k$ should fall exponentially with $k$; the intuition is that $\operatorname{pass_i@k}$ is 1 unless \textit{all} attempts fail, and since attempts are independent, the probability that all fail is exponentially unlikely with the number of attempts.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/02_large_language_monkeys_original_eda/y=neg_log_score_vs_x=scaling_parameter_hue=model_col=model_units=problem_idx.pdf}
    \includegraphics[width=\linewidth]{figures/00_bon_jailbreaking_eda/y=neg_log_score_vs_x=scaling_parameter_hue=model_col=model_units=problem_idx.pdf}
    \caption{\textbf{Per-problem performance scales exponentially with the number of attempts per problem $k$}.     
    Top: Pythia language models on 128 problems from MATH, with performance on the $i$-th problem measured as $-\log(\operatorname{pass_i@k})$. Bottom: Frontier AI models on jailbreaking prompts from HarmBench, with performance on the $i$-th problem measured as $-\log(\operatorname{ASR_i@k})$. In both settings, on each problem, the negative log \textit{per-problem} success rate falls exponentially with the number of independent attempts $k$. However, the negative log \textit{average} success rate falls as a power law with $k$ (black).}
    \label{fig:multiple_attempts_scaling_per_datum}
\end{figure*}


Mathematically, on any given attempt, the model has probability $\operatorname{pass_i@1}$ of solving the $i$-th problem.
% If we draw $k$ independent attempts from the model, how should the expected (over attempts) $\operatorname{pass_i@k}$ to scale with $k$?
Recalling that $\operatorname{pass_i@k}$ is defined as $1$ if \textit{any} of the $k$ attempts succeed, 0 otherwise, by linearity of expectation and by independence of the $k$ attempts, we can rewrite $\operatorname{pass_i@k}$ as:
%
\begin{align}
    \operatorname{pass_i@k} &= \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{k \text{ Attempts}}}\Big[1 - \mathbb{I}[\text{All $k$ Attempts Fail}] \Big]\\
    &= 1 - \prod_{j=1}^k \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{1 \text{ Attempt}}}\Big[ \mathbb{I}[\text{$j$-th Attempt Fails}] \Big].
\end{align}

The probability that the $j$-th attempt fails is one minus the probability that the $j$-th attempt succeeds. Since each attempt is i.i.d. with success probability $\operatorname{pass_i@1}$, we find
%
\begin{align}
    \operatorname{pass_i@k}
    &= 1 - (1 - \operatorname{pass_i@1})^k.
\end{align}

For large $k$, $(1 - \operatorname{pass_i@1})^k$ will be small. Recalling that the Taylor Series expansion of $\log (1 + x)$ for small $x$ is $\sum_{i=1}^{\infty} (-1)^{i-1} x^i / i \approx x$, we have:
%
\begin{align}
    -\log (\operatorname{pass_i@k} )
    &= - \log \Big(1 - (1 - \operatorname{pass@1})^k \Big)\\
    &\approx (1 - \operatorname{pass_i@1})^k.
\end{align}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/50_pass_at_1_fits/llmonkey_y=counts_x=score_hue=model_col=model_bins=custom.pdf}
    \includegraphics[width=\linewidth]{figures/50_pass_at_1_fits/bon_jailbreaking_y=counts_x=score_hue=model_col=model_bins=custom.pdf}
    \caption{\textbf{Single-Attempt Success Rates  Distributions Possess Power Law-Like Left Tails.} Pythia language models on 128 MATH problems (top) and frontier AI systems on 159 HarmBench prompts (bottom) exhibit distributions (over problems) of $\operatorname{pass_i@1}$ and $\operatorname{ASR_i@1}$ with power law-like tails that are well fit by scaled Beta-Binomial distributions (black dashed lines), which produce aggregate power law scaling. Note that Llama 3 8B Instruction Tuned (IT) does not possess a power law tail, explaining why the model did not exhibit aggregate power law scaling under Best-of-N jailbreaking (Sec.~\ref{sec:no_dist_structure_no_power_law}).}
    \label{fig:multiple_attempts_pass_at_1_per_datum}
\end{figure*}


Thus, \textit{for any single problem}, we should expect the negative log expected (over attempts) success rate to fall \textit{exponentially} with $k$, not polynomially with $k$. % (i.e., as a power law).
% Intuitively, this is because $\operatorname{pass@k}$ is 1 unless all $k$ i.i.d. attempts fail and the probability that all $k$ attempts fail is exponentially unlikely with $k$.


To confirm this claim, we plotted the scaling of model performance on each problem -- measured either by $-\log(\operatorname{pass_i@k})$ or by $-\log(\operatorname{ASR_i@k})$ -- against the number of independent attempts $k$. We specifically used \citet{brown2024largelanguagemonkeysscaling}'s data of the Pythia language model family \citep{biderman2023pythia} solving 128 mathematical problems from MATH \citet{hendrycks2021measuring} as well as \citet{hughes2024bestofnjailbreaking}'s data from jailbreaking frontier AI systems -- Claude, GPT4 \citep{openai2024gpt4technicalreport}, Gemini \citep{anil2024geminifamilyhighlycapable,georgievgemini15unlockingmultimodal} and Llama 3 8B Instruction Tuned (IT) \citep{grattafiori2024llama3herdmodels} -- on 159 prompts from HarmBench \citep{mazeika2024harmbenchstandardizedevaluationframework}.
For each individual mathematical problem and jailbreaking prompt, we found the negative log expected (over attempts) success rates fall exponentially with $k$ as expected (Fig. \ref{fig:multiple_attempts_scaling_per_datum}), including on Llama 3 8B IT which does not exhibit an aggregate power law (Fig.~\ref{fig:power_laws_repeat_sampling}).