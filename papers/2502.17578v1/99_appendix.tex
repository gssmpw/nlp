\clearpage
\appendix
\onecolumn

\section{Clarification of How Large Language Monkeys and Best-of-N Jailbreaking Sampled Data}
\label{app:sec:clarification_of_data_sampling}

In this manuscript, we used the phrasing of ``independent attempts," which is not fully correct. In this appendix section, we clarify why we chose this terminology, what likely impacts we believe this inaccuracy may have had on our results, and how to correct the paper accordingly.

Large Language Monkeys \citep{brown2024largelanguagemonkeysscaling} indeed drew $10,000$ independent attempts per problem, but Best-of-N Jailbreaking \citep{hughes2024bestofnjailbreaking} sampled data slightly different: for each problem, jailbreaking attempts were drawn until either a successful jailbreak was obtained or until a maximum limit of $10,000$ attempts was hit. Samples were also drawn in minibatches of size 60, making the (in)dependence of samples a bit tricky.

We omitted this nuance because it offers a second-order correction to our paper's main story while offering little additional insight. Neither of our theorems and none of our main text figures change. We suspect that this slightly different sampling procedure explains why, in Fig.~\ref{fig:comparison_power_law_exponents}, the estimated power law exponents between the least squares power law estimator and the distributional power law estimator deviate more significantly from identity for Best-of-N Jailbreaking than for Large Language Monkeys. A natural way to correct for this is to use a \href{https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution}{beta-negative binomial distribution} rather than a \href{https://en.wikipedia.org/wiki/Beta-binomial_distribution}{beta-binomial distribution}, with an additional correction for the maximum number of attempts. For more information, please see Appendix~\ref{app:sec:mle_scaled_beta_negative_binomial}.


\clearpage


\section{Estimating Success Rates Using \citet{chen2021evaluatinglargelanguagemodels}'s Estimator}
\label{app:sec:chen2021estimator}

In this manuscript, we defined $\operatorname{pass_i@k}$ and $\operatorname{ASR_i@k}$ as:
%
\begin{align*}
    \operatorname{pass_i@k} &\defeq \mathop{\mathbb{E}}_{k \text{ Attempts}} \big[ \mathbb{I}[\text{At least 1 attempt by the model solves the $i$-th problem}] \big]\\
    \operatorname{ASR_i@k} &\defeq \mathop{\mathbb{E}}_{k \text{ Attempts}} \big[ \mathbb{I}[\text{At least 1 attempt jailbreaks the model on the $i$-th prompt}] \big]
\end{align*}

Throughout this manuscript, to estimate $\operatorname{pass_i@k}$ and $\operatorname{ASR@k}$, we used the unbiased and lower variance estimator introduced by \citet{chen2021evaluatinglargelanguagemodels}: for the $i$-th problem, we sampled $n \gg k$ attempts per problem, counted the number of successful attempts $c$, and then swept $k$ to compute an estimate of $\operatorname{pass_i@k}$ for different $k$ values:

\begin{equation}
    \widehat{\operatorname{pass_i@k}} = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}
\end{equation}

Two comments: Firstly, $n$ as used here has no relationship with the number of problems in the benchmark (Sec.~\ref{sec:introduction}), and secondly, our notation differs slightly from that of \citet{chen2021evaluatinglargelanguagemodels}, but the ideas are consistent. A numerically stable Python implementation of the estimator is provided in Fig.~\ref{app:fig:chen_success_rate_estimator_code}:

\begin{figure}[h!]
    \centering
    \begin{lstlisting}[language=Python, 
        basicstyle=\ttfamily,
        commentstyle=\itshape]
    def estimate_success_rate_at_k_per_problem(n: int, c: int, k: int) -> float:
        """
        :param n: number of total attempts on this problem.
        :param c: number of correct attempts on this problem.
        :param k: k in pass_i@$k$.
        """
        if n - c < k: return 1.0
        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))
            
    \end{lstlisting}
    \caption{A numerically stable unbiased estimator of $\operatorname{pass_i@k}$, introduced by \citet{chen2021evaluatinglargelanguagemodels}.}
    \label{app:fig:chen_success_rate_estimator_code}
\end{figure}

To reiterate a point made by \citet{chen2021evaluatinglargelanguagemodels}, estimating $\operatorname{pass_i@k}$ as $1 - (1 - \widehat{\operatorname{pass_i@1}})^k$ is biased (Fig.~\ref{app:fig:chen_success_rate_estimator_bias}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/91_openai_codex_pass_at_k/y=pass_at_k_x=n_hue=k_style=type_col=true_prob.pdf}
    \caption{\textbf{Bias of Estimators of $\operatorname{pass_i@k}$.} Numerical simulations show that estimating $\operatorname{pass_i@k}$ as $1 - (1 - \widehat{\operatorname{pass_i@1}})^k$ is biased whereas the estimator of \citet{chen2021evaluatinglargelanguagemodels} is not. For a mathematical proof of unbiasedness, see the original paper.}
    \label{app:fig:chen_success_rate_estimator_bias}
\end{figure}

\clearpage

\section{Fitting Power Laws to Large Language Monkeys and Best-of-N Jailbreaking}
\label{app:sec:power_law_fits_bon_and_llmonkey}

We fit power laws to a subset of data from Large Language Monkeys \citep{brown2024largelanguagemonkeysscaling} and from Best-of-N Jailbreaking \citep{hughes2024bestofnjailbreaking}, specifically Pythia language models \citep{biderman2023pythia} on the MATH benchmark \citep{hendrycks2021measuring} and frontier AI models -- Claude, GPT4 \citep{openai2024gpt4technicalreport}, Gemini \citep{anil2024geminifamilyhighlycapable,georgievgemini15unlockingmultimodal} and Llama 3 \citep{grattafiori2024llama3herdmodels} -- on the HarmBench jailbreaking benchmark \citep{mazeika2024harmbenchstandardizedevaluationframework}. We show the functional forms and the fit parameters in Table~\ref{tab:power-law-params-llmonkeys} and Table~\ref{tab:power-law-params-bon-jailbreaking} respectively. To fit the parameters, for Large Language Monkeys, we simply minimized the squared error between the actual and predicted $-\log (\operatorname{pass_{\mathcal{D}}@k})$, and for Best-of-N Jailbreaking, we similarly minimized the squared error between the actual and predicted $-\log (\operatorname{ASR_{\mathcal{D}}@k}))$.

Note: Llama 3 8B IT does not exhibit power law scaling under Best-of-N Jailbreaking (shown in Fig.~\ref{fig:power_laws_repeat_sampling}, bottom).

\bigskip\bigskip\bigskip\bigskip\bigskip

\begin{table}[htbp]
\begin{minipage}[t]{0.48\textwidth}
\centering
\begin{tabular}{llrr}
\toprule
\toprule
Model & Benchmark & $a$ & $b$ \\
\midrule
Pythia 70M & MATH  & 8.026 & 0.194\\
Pythia 160M & MATH  & 6.591 & 0.280\\
Pythia 410M & MATH  & 5.524 & 0.286\\
Pythia 1B & MATH  & 5.452 & 0.315 \\
Pythia 2.8B & MATH  & 4.104 & 0.336 \\
Pythia 6.9B & MATH  & 4.255 & 0.348\\
Pythia 12B & MATH  & 4.113 & 0.370 \\
\bottomrule
\bottomrule
\end{tabular}
\caption{Large Language Monkeys \citep{brown2024largelanguagemonkeysscaling} fitted power law parameters on 128 mathematical problems from MATH \citep{hendrycks2021measuring}.\\
Functional Form: $-\log (\operatorname{pass_{\mathcal{D}}@k}) = a \, k^{-b}$.}
\label{tab:power-law-params-llmonkeys}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\begin{tabular}{llrr}
\toprule
\toprule
Model & Modality & $a$ & $b$ \\
\midrule
Claude 3.5 Opus & Text & 2.630 & 0.448  \\
Claude 3.5 Sonnet & Text  & 3.436 & 0.312 \\
GPT4o & Text & 3.639 & 0.395\\
GPT4o Mini & Text & 3.637 & 0.492  \\
Gemini 1.5 Flash & Text  & 6.158 & 0.303 \\
Gemini 1.5 Pro & Text  & 6.296 & 0.256 \\
Llama 3 8B IT & Text & -- & -- \\
\bottomrule
\bottomrule
\end{tabular}
\caption{Best-of-N Jailbreaking \citep{hughes2024bestofnjailbreaking} fitted power law parameters on text jailbreak prompts from HarmBench \citep{mazeika2024harmbenchstandardizedevaluationframework}.\\
Functional Form: $-\log (\operatorname{ASR_{\mathcal{D}}@k}) = a \, k^{-b}$.\\
Note: Llama 3 8B Instruction Tuned (IT) does not exhibit power law scaling.
}
\label{tab:power-law-params-bon-jailbreaking}
\end{minipage}
\end{table}

\clearpage


\section{Mathematical Equivalence Between Coverage and Average Success Rate}
\label{app:sec:coverage_pass_at_k}

\citet{brown2024largelanguagemonkeysscaling} and \citet{hughes2024bestofnjailbreaking} phrase their research in terms of ``coverage", defined as the fraction of problems that can be solved or the fraction of prompts that can jailbreak a model, but as \citet{brown2024largelanguagemonkeysscaling} comment and we here derive, the coverage is mathematically equivalent to the average $\operatorname{pass_i@k}$ (equivalently, $\operatorname{ASR@k}$. due to two simple probabilistic primitives: (1) linearity of expectation, (2) the expectation of an indictor random variable of some event is the probability of said event and (3) the definition of $\operatorname{pass_i@k}$:


\begin{align*}
    \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{\text{Prompts}\\\text{Attempts}}}\Big[\operatorname{Coverage} \Big] &\defeq \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{\text{Problems}\\\text{Attempts}}}\Big[\text{Fraction of Problems Solved After $k$ Attempts}\Big]\\
    &= \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{\text{Problems}}}\Bigg[ \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\text{Attempts} | \text{Problem}} \Big[ \mathbb{I}\big[\text{Problem Solved After $k$ Attempts} \big]\Big] \Bigg]\\
    &= \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{\text{Problems}}}\Bigg[ \operatorname{pass_{problem}@k}\Big] \Bigg]\\
    &= \operatorname{pass_{\mathcal{D}}@k}
    % &= \frac{1}{\text{Num. Problems}} \sum_{i} \mathbb{P}(\text{$i$-th Problem Solved After $k$ Attempts})\\
    % &\defeq \frac{1}{\text{Num. Problems}} \sum_{i} \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\text{Attempts}} \big[ \operatorname{pass_i@k} \text{ on $i$th Problem} \big]\\
    % &= \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\substack{\text{Problems}\\\text{Attempts}}}\Big[\operatorname{pass_i@k}\Big]
\end{align*}

In our work, we prefer phrasing along the lines of ``success rate" over ``coverage" because success rate avoids coverage's binary implication that each problem/prompt is either ``solved" or ``not solved".

\clearpage

\section{Aggregate Power Laws from a Probability Distribution over Exponential Functions}
\label{app:sec:power_laws_from_distr_over_exp}

\subsection{Preliminaries: Power Laws from Weighted Exponential Functions}
\label{app:sec:power_laws_from_distr_over_exp:background}

A known result is that power laws can emerge from appropriately weighted sums of exponential functions, e.g., \citep{bochud2006optimalapproximationspowerlawsexponentials,elkies2016powerlawexponentials, bousquet2020theoryuniversallearning}. For a concrete example with a short proof:
%
\begin{equation}
    x^{-r} = \frac{1}{\Gamma(r)} \int_0^{\infty} p^{r-1} \, e^{- p x} \, dp,
\end{equation}
%
where $\Gamma(r) \defeq \int_0^{\infty} s^{r-1} \, e^{-s} \, ds$ is the \href{https://en.wikipedia.org/wiki/Gamma_function}{Gamma function}. The proof is via u-substitution $u \defeq p \, x$:
%
\begin{align}
    \frac{1}{\Gamma(r)} \int_0^{\infty} p^{r-1} \, e^{- p x} \, dp
    &= \frac{1}{\Gamma(r)} \int_0^{\infty} (u/x)^{r-1} \, e^{- u} \, \frac{du}{x} \\
    &= \frac{1}{\Gamma(r)} \, x^{-r} \, \int_0^{\infty} u^{r-1} \, e^{- u} \, du\\
    &= \frac{1}{\Gamma(r)} \, x^{-r} \, \Gamma(r)\\
    &= x^{-r}
\end{align}

In our particular context, we are interested in the scaling with $k$ of the expected success rate over problems sampled from the benchmark's data distribution:
%
\begin{equation}
    \operatorname{pass_{\mathcal{D}}@k} \defeq \mathop{\raisebox{3pt}{$\mathbb{E}$}}_{\operatorname{pass_i@1} \sim \mathcal{D}} \Big[\operatorname{pass_i@k} \Big]
\end{equation}


distribution (over problems in a benchmark) of $\operatorname{pass_i@k}$ scores that yields power law scaling with respect to the number of attempts $k$:
%
\begin{equation}
    -\log \Bigg( \frac{1}{n} \sum_{i=1}^n \operatorname{pass_i@k} \Bigg) \approx a k^{-b}.
\end{equation}

for constants $a, b > 0$.


\subsection{Delta Distribution: $\operatorname{pass_i@1} \sim \delta(p), p \in (0, 1)$}
\label{app:sec:power_laws_from_distr_over_exp:delta_distribution}

To start with a negative result, we will show that not all distributions of the per-problem success probabilities $\operatorname{pass_i@1}$ yield aggregate power law scaling.
Suppose that the model's $\operatorname{pass_i@1}$ probabilities across the benchmarks' problems are all exactly $p \in (0, 1)$. For brevity, let $p_i \defeq \operatorname{pass_i@1}$. Then the aggregate success rate is:
%
\begin{align}
    \mathbb{E}_{p_i \sim \delta(p)}[\operatorname{pass_i@k}] &= 1 - \mathbb{E}_{p_i}[(1 - p_i)^k]\\
    &= \int_{0}^{1} \delta(p) \, (1 - p_i)^k \, dp_i\\
    &= (1 - p)^k.
\end{align}
%
Recalling that the expansion of $\log(\cdot)$ for small $x$ is $- \log( 1- x) = x + O(x^2)$, in our case, we obtain:
%
\begin{equation}
    - \log \Big(1 - \mathbb{E}_{p_i \sim \delta(p)}[\operatorname{pass@k}] \Big) = (1 - p)^k + O((1-p)^{2k}) = (1-p)^k + o((1-p)^k).
\end{equation}

Thus, in the large $k$ regime, we find the negative log aggregate success rate exhibits \textit{exponential} scaling with $k$ as we intuitively expect.

\subsection{Uniform Distribution: \(\operatorname{pass_i@1} \sim \mathrm{Uniform}(\alpha, \beta)\)}
\label{app:sec:power_laws_from_distr_over_exp:uniform_distribution}

Suppose \(\operatorname{pass_i@1}\) probabilities follow a uniform distribution \(\mathrm{Uniform}(\alpha, \beta)\) where \(0 \leq \alpha < \beta \leq 1\). The aggregate success rate after \(k\) attempts is defined as:
\[
\operatorname{pass_{\mathrm{Uniform}(\alpha, \beta)}}@k \defeq 1 - \mathbb{E}\bigl[(1 - p)^k\bigr].
\]

If \(p \sim \mathrm{Uniform}(\alpha, \beta)\), the expectation of \((1-p)^k\) is:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \;=\; \frac{1}{\beta - \alpha} \int_{\alpha}^{\beta} (1 - p)^k \, \mathrm{d}p.
\]
Evaluating the integral gives:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \;=\;
\frac{
    (1 - \alpha)^{k+1} - (1 - \beta)^{k+1}
}{
    (\beta - \alpha) \cdot (k+1)
}.
\]
Thus, the aggregate success rate becomes:
\[
\operatorname{pass_{\mathrm{Uniform}(\alpha, \beta)}}@k
\;=\;
1 \;-\; \frac{
    (1 - \alpha)^{k+1} - (1 - \beta)^{k+1}
}{
    (\beta - \alpha) \cdot (k+1)
}.
\]

\paragraph{Case A: \(\alpha > 0\)}
If \(\alpha > 0\), then both \((1-\alpha)\) and \((1-\beta)\) are strictly less than \(1\). As \(k \to \infty\), \((1-\alpha)^{k+1}\) and \((1-\beta)^{k+1}\) decay exponentially. Hence:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \sim \frac{
    (1 - \alpha)^{k+1}
}{
    (\beta - \alpha) \cdot (k+1)
},
\]
and \(\operatorname{pass_{\mathrm{Uniform}(\alpha, \beta)}}@k\) approaches \(1\) exponentially fast:
\[
\operatorname{pass_{\mathrm{Uniform}(\alpha, \beta)}}@k \sim 1 - \frac{
    (1 - \alpha)^{k+1}
}{
    (\beta - \alpha) \cdot (k+1)
}.
\]
Thus, the negative log of the aggregate success rate decays exponentially:
\[
-\log\bigl(\operatorname{pass_{\mathrm{Uniform}(\alpha, \beta)}}@k\bigr) \sim e^{-\Omega(k)}.
\]

\paragraph{Case B: \(\alpha = 0\)}
When \(\alpha = 0\), the uniform distribution is over \([0, \beta]\). In this case:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \;=\; \frac{1}{\beta} \cdot \frac{
    1 - (1 - \beta)^{k+1}
}{
    k+1
}.
\]
For large \(k\), \((1 - \beta)^{k+1}\) becomes exponentially small, and:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \sim \frac{1}{\beta} \cdot \frac{1}{k+1}.
\]
The aggregate success rate is then:
\[
\operatorname{pass_{\mathrm{Uniform}(0, \beta)}}@k \sim 1 - \frac{1}{\beta \cdot k}.
\]
The negative log exhibits power-law scaling:
\[
-\log\bigl(\operatorname{pass_{\mathrm{Uniform}(0, \beta)}}@k\bigr) \sim \frac{1}{\beta} \cdot \frac{1}{k}.
\]

\paragraph{Special Case: \(\mathrm{Uniform}(0, 1)\)}
If \(\beta = 1\), the distribution is uniform on \([0, 1]\). In this case:
\[
\mathbb{E}\bigl[(1-p)^k\bigr] \;=\; \frac{1}{k+1},
\]
and the success rate becomes:
\[
\operatorname{pass_{\mathrm{Uniform}(0, 1)}}@k \;=\; 1 - \frac{1}{k+1}.
\]
For large \(k\):
\[
-\log\bigl(\operatorname{pass_{\mathrm{Uniform}(0, 1)}}@k\bigr) \sim \frac{1}{k}.
\]


\subsection{2-Parameter Beta Distribution: $\operatorname{pass_i@1} \sim \operatorname{Beta}(\alpha, \beta)$}
\label{app:sec:power_laws_from_distr_over_exp:beta_distribution}

Suppose that the model's $\operatorname{pass_i@1}$ probabilities across the benchmark problems follow a Beta distribution:
%
$$\operatorname{pass_i@1} \sim \operatorname{Beta}(\alpha, \beta)$$
%
The probability density function of this distribution over the support $x \in (0, 1)$ is:
%
\begin{equation}
    f(x; \alpha, \beta) \defeq \frac{1}{B(\alpha, \beta)} \,
    x^{\alpha-1} \, (1-x)^{\beta-1},
\end{equation}
%
where $\alpha > 0, \beta > 0$ and $B(\cdot, \cdot)$ is the \href{https://en.wikipedia.org/wiki/Beta_function}{Beta function}. For brevity, let $p_i \defeq \operatorname{pass_i@1}$. Under our assumed Beta distribution:
%
\begin{align}
    \operatorname{pass_{\operatorname{Beta(\alpha, \beta)}}@k} &\defeq 1 - \mathbb{E}_{p_i \sim \operatorname{Beta(\alpha, \beta)}}[(1-p_i)^k]\\
    &= 1 - \int_0^1 \frac{p_i^{\alpha-1}(1-p_i)^{\beta-1}}{B(\alpha,\beta)} \; (1-p_i)^k \; dp_i\\
    &= 1 - \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha) \Gamma(\beta + k)}{\Gamma(\alpha + \beta + k)}
\end{align}
%
where $\Gamma(\cdot)$ is again the Gamma function. The $\Gamma(\alpha)$ terms cancel, and a standard asymptotic result of the gamma function for large $k$ tells us that:
%
\begin{equation}
    \frac{\Gamma(\beta +k)}{\Gamma(\alpha + \beta + k)} \sim k^{-\alpha},
\end{equation}
%
and thus:
%
\begin{equation}
    \frac{\Gamma(\alpha + \beta)}{\Gamma(\beta)}\frac{\Gamma(\beta +k)}{\Gamma(\alpha + \beta + k)} \sim \frac{\Gamma(\alpha + \beta)}{\Gamma(\beta)} k^{-\alpha}.
\end{equation}
%
Recalling again that the expansion of $\log(\cdot)$ for small $x$ is $- \log( 1- x) = x + O(x^2)$, in our case, we obtain:
%
\begin{equation}
-\log \Big( \operatorname{pass_{\mathcal{D}}@k} \Big)  = \frac{\Gamma(\alpha + \beta)}{\Gamma(\beta)} k^{-\alpha} + O(k^{-2\alpha}) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\beta)} k^{-\alpha} + o(k^{-\alpha}).
\end{equation}

From this final result, we see that under a Beta distribution and in the large $k$ regime, the negative log aggregate success rate exhibits polynomial (power-law) scaling with $k$ for exponent $\alpha$

\subsection{Kumaraswamy Distribution: $\operatorname{pass_i@1} \sim \operatorname{Kumaraswamy}(\alpha, \beta)$}
\label{app:sec:power_laws_from_distr_over_exp:kumaraswamy_distribution}

Next, suppose the model's $\operatorname{pass_i@1}$ probabilities follow a Kumaraswamy distribution. The probability density function of this distribution over the support $x \in (0, 1)$ is:
%
\begin{equation}
    f(x; \alpha, \beta) \defeq \alpha \, \beta \, x^{\alpha-1} \, (1-x^\alpha)^{\beta-1}
\end{equation}
%
Again for brevity, let $p_i \defeq \operatorname{pass_i@1}$. Under our assumed Kumaraswamy distribution:
%
\begin{align}
    \operatorname{pass_{\operatorname{Kumaraswamy(\alpha, \beta)}}@k} &\defeq 1 - \mathbb{E}_{p_i \sim \operatorname{Kumaraswamy}(\alpha, \beta)}[(1-p_i)^k]\\
    &= 1 - \int_{0}^{1} (1-p)^k \cdot \alpha \, \beta \, p^{\alpha-1} \, (1-p^{\alpha})^{\beta-1} dp.
\end{align}

Define the integral
\begin{equation}
\label{eq:I_k_kumaraswamy_def}
I_k
\defeq
\mathbb{E}\bigl((1-p)^k\bigr)
=
\int_{0}^{1}
(1 - x)^k
\,
\alpha\,\beta
\;
x^{\alpha-1}
\;
\bigl(1 - x^\alpha\bigr)^{\beta-1}
\;\mathrm{d}x.
\end{equation}
We aim to analyze \(I_k\) for large \(k\).  Notice that \((1 - x)^k\) is exponentially small in \(k\) unless \(x\) is very close to 0.  Thus, intuitively, most of the contribution to \(I_k\) arises from \(x \in [0,\,O(1/k)]\).

\paragraph{Step 1: Split the integral into two parts.}
Fix a constant \(c>0\).  Write 
\begin{equation*}
I_k
\;=\;
\int_{0}^{c/k}[\cdots]\,\mathrm{d}x
\;+\;
\int_{c/k}^{1}[\cdots]\,\mathrm{d}x
\;\;\defeq\;\;
I_{k,\mathrm{left}}
\;+\;
I_{k,\mathrm{right}},
\end{equation*}
where \([\cdots]\) indicates the same integrand.  
In the region \(x \in [c/k,1]\), we have \((1 - x)^k\le e^{-k\,x}\le e^{-c}\).  Hence \(I_{k,\mathrm{right}}=O\bigl(e^{-c}\bigr)\).  Since \(c\) can be made arbitrarily large, \(I_{k,\mathrm{right}}\) becomes negligible compared to any polynomial in \(1/k\).

\paragraph{Step 2: Approximate the integrand in the small-$x$ region.}
On \([0,c/k]\), we use the approximation \(\log(1-x)=-x + O(x^2)\).  Thus 
\[
(1-x)^k 
=
\exp \big( k \log(1-x) \big)
=
\exp \big( -k\,x + O(k\,x^2) \big).
\]
Since \(x \leq c/k\) implies \(k\,x^2 \leq c^2/k = O (1/k)\), and $\exp(\epsilon) = 1 + O(\epsilon)$, we get
\[
(1 - x)^k
=
\exp ( -k\,x ) \exp(O(1/k))
=
\exp ( -k\,x ) \, 
\big(
1 + O \bigl(\tfrac{1}{k}\bigr)
\big).
\]
Furthermore, since $(1-y)^m = 1 - m y + O(y^2)$, for small \(x\)
\[
\,(1 - x^\alpha)^{\beta-1} = 1 - (\beta - 1) x^{\alpha} + O(x^{2\alpha}) = 1 + O \bigl(x^\alpha\bigr).
\]
In the region \(x \le c/k\), that error is \(O \bigl(k^{-\alpha}\bigr)\).  
Hence, within the small-\(x\) region, the integrand
\[
(1 - x)^k\;\alpha\,\beta\;x^{\alpha-1}\;\bigl(1 - x^\alpha\bigr)^{\beta-1}
\]
can be approximated by
\[
\alpha\,\beta\;x^{\alpha-1}\;e^{-k\,x}
\;+\;
O \Bigl(k^{-\alpha}\,x^{\alpha-1}\,e^{-k\,x}\Bigr).
\]
Thus
\begin{align*}
I_{k,\mathrm{left}}
\;=\;
\int_{0}^{c/k}
\alpha\,\beta\;x^{\alpha-1}\,e^{-k\,x}
\;\mathrm{d}x
\;+\;
O \Bigl(
k^{-\alpha}
\int_{0}^{c/k}
x^{\alpha-1}\,e^{-k\,x}\,\mathrm{d}x
\Bigr)
\;+\;
O \bigl(e^{-c}\bigr).
\end{align*}

\paragraph{Step 3: Substitution \(\,u \defeq k\,x\).}
To handle
\(\int_{0}^{c/k}x^{\alpha-1}e^{-k\,x}\,\mathrm{d}x\),
we substitute \(u=k\,x\).  Then \(x=u/k\), \(\mathrm{d}x=\mathrm{d}u/k\), and the upper limit \(x=c/k\) becomes \(u=c\).  Hence,
\begin{align*}
\int_{0}^{c/k}
x^{\alpha-1}\,e^{-k\,x}\,\mathrm{d}x
&=\;
\int_{0}^{c}
\Bigl(\tfrac{u}{k}\Bigr)^{\alpha-1}
\,e^{-\,u}
\;\frac{\mathrm{d}u}{k}
\\[6pt]
&=\;
k^{-\alpha}
\int_{0}^{c}
u^{\alpha-1}\,e^{-\,u}
\;\mathrm{d}u.
\end{align*}
As \(c\to\infty\), \(\int_{0}^{c}u^{\alpha-1}e^{-\,u}\,\mathrm{d}u\to\Gamma(\alpha)\), and for finite \(c\) the remainder is \(O \bigl(e^{-\,c}\bigr)\).  Therefore,
\[
\int_{0}^{1}
x^{\alpha-1}\,e^{-k\,x}\,\mathrm{d}x
=
k^{-\alpha}\,\Gamma(\alpha)
\;+\;
O \bigl(k^{-\alpha}\,e^{-\,c}\bigr),
\]
and absorbing the constant \(c\) into big-\(O\) notation gives 
\[
\int_{0}^{1}
x^{\alpha-1}\,e^{-k\,x}\,\mathrm{d}x
=
k^{-\alpha}\,\Gamma(\alpha)
\;+\;
O \bigl(k^{-\alpha-\epsilon}\bigr)
\quad
\text{for some }\epsilon>0.
\]
Multiplying by the factor \(\alpha\,\beta\), we deduce that
\[
I_k
\;=\;
\alpha\,\beta\,\Gamma(\alpha)\,k^{-\alpha}
\;+\;
O \bigl(k^{-\alpha-\epsilon}\bigr).
\]

\paragraph{Step 4: Final conclusion for the success rate.}
Recall
\(\operatorname{pass_{\mathrm{Kumaraswamy}(\alpha,\beta)}@k} = 1 - I_k\). Hence 
\[
\operatorname{pass_{\mathrm{Kumaraswamy}(\alpha,\beta)}@k}
\;=\;
1
\;-\;
\alpha\,\beta\,\Gamma(\alpha)\,k^{-\alpha}
\;+\;
O \bigl(k^{-\alpha-\epsilon}\bigr).
\]
Since this tends to 1, its negative log is governed by the magnitude of 
\(\,\alpha\,\beta\,\Gamma(\alpha)\,k^{-\alpha}\).  Using the expansion 
\(-\log(1-y) = y + O(y^2)\) as \(y\to0\), we get
\[
-\log \Big(
\operatorname{pass_{\mathrm{Kumaraswamy}(\alpha,\beta)}@k}
\Big)
\;=\;
\alpha\,\beta\,\Gamma(\alpha)\;k^{-\alpha}
\;+\;
o\bigl(k^{-\alpha}\bigr).
\]
That is precisely polynomial (power-law) decay in the negative log success rate with exponent \(\alpha\).


\subsection{Continuous Bernoulli Distribution: $\operatorname{pass_i@1} \sim \operatorname{Continous Bernoulli}(\lambda)$}
\label{app:sec:power_laws_from_distr_over_exp:continous_bernoulli_distribution}

Next, suppose the model's $\operatorname{pass_i@1}$ probabilities follow a Continuous Bernoulli distribution. The probability density function of this distribution over the support $x \in [0, 1]$ is:
%
\begin{align}
    f(x; \lambda) &\defeq C(\lambda) \lambda^{x} (1-\lambda)^{1 - x}\\
    C(\lambda) &\defeq \begin{cases}
        2 & \text{ if } \lambda = 1/2\\
        \frac{2 \tanh^{-1}{(1- 2 \lambda)}}{1 - 2 \lambda} & \text{otherwise}
    \end{cases}.
\end{align}
%
The density can equivalently be rewritten in a more convenient form for our purposes:
%
\begin{equation}
    f(x;\lambda) = C(\lambda) \lambda^x (1 - \lambda) (1 - \lambda)^{-x} = C(\lambda) (1 - \lambda) \Big(\frac{\lambda}{1 - \lambda} \Big)^x
\end{equation}
%
Because the individual success probability is low in our data, we shall consider the small $\lambda < 1/2$ regime. We follow the same approach as with the Kumaraswamy distribution.

\paragraph{Step 1: Write the aggregate pass rate.}
The aggregate pass rate is defined as:
\[
\operatorname{pass_{ContinuousBernoulli(\lambda)}@k} = 1 - I_k, \quad \text{where} \quad
I_k \defeq \int_0^1 (1-p)^k \, f(p; \lambda) \, dp.
\]
Substituting the density \(f(p; \lambda)\), we get:
\[
I_k = \int_0^1 (1-p)^k \, C(\lambda) \, \lambda^p \, (1-\lambda)^{1-p} \, dp.
\]

\paragraph{Step 2: Simplify using an exponential form.} Using the exponential rewriting:
\[
\lambda^p \, (1-\lambda)^{1-p} = (1-\lambda) \exp\Bigl(p \log\bigl(\tfrac{\lambda}{1-\lambda}\bigr)\Bigr),
\]
the integral becomes:
\[
I_k = C(\lambda) \, (1-\lambda) \int_0^1 (1-p)^k \exp\Bigl(p \log\bigl(\tfrac{\lambda}{1-\lambda}\bigr)\Bigr) \, dp.
\]

\paragraph{Step 3: Dominance of the small-\(p\) region.}
For large \(k\), \((1-p)^k\) decays exponentially unless \(p\) is close to 0. Thus, the main contribution to the integral arises from the region \(p \in [0, c/k]\), where \(c > 0\) is a constant. Decompose the integral:
\[
I_k = \int_0^{c/k} [\cdots] \, dp + \int_{c/k}^1 [\cdots] \, dp \defeq I_{k,\mathrm{left}} + I_{k,\mathrm{right}}.
\]
In the region \(p \in [c/k, 1]\), we have \((1-p)^k \leq e^{-k p} \leq e^{-c}\), making \(I_{k,\mathrm{right}} = O(e^{-c})\), which is negligible compared to \(1/k\). Thus, we focus on \(I_{k,\mathrm{left}}\):
\[
I_{k,\mathrm{left}} = C(\lambda) \, (1-\lambda) \int_0^{c/k} (1-p)^k \exp\Bigl(p \log\bigl(\tfrac{\lambda}{1-\lambda}\bigr)\Bigr) \, dp.
\]

\paragraph{Step 4: Approximate the integrand.}
For \(p \in [0, c/k]\), use the same approximations from the Kumaraswamy derivation:
\[
(1-p)^k = e^{-k p} \, \bigl(1 + O(p)\bigr), \quad
\exp\Bigl(p \log\bigl(\tfrac{\lambda}{1-\lambda}\bigr)\Bigr) = 1 + O(p).
\]
Thus, the integrand becomes:
\[
(1-p)^k \exp\Bigl(p \log\bigl(\tfrac{\lambda}{1-\lambda}\bigr)\Bigr)
= e^{-k p} \, \bigl(1 + O(p)\bigr).
\]

\paragraph{Step 5: Change of variables.}
Let \(u \defeq k p\), so \(p = u/k\) and \(dp = du/k\). The integral becomes:
\[
I_{k,\mathrm{left}} = C(\lambda) \, (1-\lambda) \int_0^c e^{-u} \, \bigl(1 + O(u/k)\bigr) \, \frac{du}{k}.
\]
Split the integral:
\[
I_{k,\mathrm{left}} = \frac{C(\lambda) \, (1-\lambda)}{k} \int_0^c e^{-u} \, du + O\Bigl(\frac{1}{k^2}\Bigr).
\]

As \(c \to \infty\), \(\int_0^c e^{-u} \, du \to 1\). Thus:
\[
I_{k,\mathrm{left}} = \frac{C(\lambda) \, (1-\lambda)}{k} + O\Bigl(\frac{1}{k^2}\Bigr).
\]
Since \(I_{k,\mathrm{right}} = O(e^{-c})\) is negligible, we have:
\[
I_k = \frac{C(\lambda) \, (1-\lambda)}{k} + O\Bigl(\frac{1}{k^2}\Bigr).
\]

\paragraph{Step 7: Final conclusion for the success rate.}
Recall:
\[
\operatorname{pass_{ContinuousBernoulli(\lambda)}@k} = 1 - I_k.
\]
For large \(k\), this implies:
\[
\operatorname{pass_{ContinuousBernoulli(\lambda)}@k} = 1 - \frac{C(\lambda) \, (1-\lambda)}{k} + O\Bigl(\frac{1}{k^2}\Bigr).
\]
Using the expansion \(-\log(1-y) = y + O(y^2)\) for small \(y\), we find:
\[
-\log\bigl(\operatorname{pass_{ContinuousBernoulli(\lambda)}@k}\bigr) = C(\lambda) \, (1-\lambda) k^{-1} + o(k^{-1}).
\]

That is precisely polynomial (power-law) decay in the negative log success rate with exponent \(-1\).

As a side comment, recall that $\tanh^{-1}(x) = \frac{1}{2} \log \big(\frac{1+ x}{1 - x} \big)$, the normalizing constant $C(\lambda)$ can be rewritten as:
%
\begin{equation}
    C(\lambda) = \frac{2}{1 - 2 \lambda} \frac{1}{2} \log \Bigg(\frac{1 + (1 - 2 \lambda)}{1 - (1 - 2 \lambda)} \Bigg) = \frac{1}{1 - 2 \lambda} \log \Big( \frac{1- \lambda}{\lambda} \Big).
\end{equation}
%
Thus, for small $\lambda$, note that $C(\lambda) \approx \log(1/\lambda) = -\log(\lambda)$.
For $k \ll -\log(\lambda)$, the $1/k$ formula is valid. However, near $k \approx -\log(\lambda)$, the leading term $-\log(\lambda)/k$ becomes of order 1, and for $k \gg -\log(\lambda)$, the success rate is now very close to 1. Consequently, we see that if $\lambda$ is very small, there is a soft cutoff scale around $k \approx -\log(\lambda)$.

% For shorthand, define $s \defeq -\log(\lambda)$. The expected $\operatorname{pass@k}$ for large $k$ is:
% %
% \begin{align}
%     \mathbb{E}_{p_i \sim \operatorname{ContinuousBernoulli(\lambda)}}[\operatorname{pass@k}] &= 1 - \int_{0}^{1} (1-p)^k \, C(\lambda) \, \lambda^{p} \,(1-\lambda)^{1-p} \, dp\\
%     &\approx 1 - s \int_{0}^{1} \exp(-k \, p) \exp(p \log (k)) \, dp\\
%     &= 1 - s \int_{0}^{1} \exp(-p (s + k)) \, dp\\
%     &= 1 - \frac{s}{s + k}
% \end{align}
% %
% Thus, we find two regimes: if $s \gg k$, then we obtain power law scaling with coefficient $-1$:
% \begin{equation}
%     -\log \Big( \operatorname{pass_{\operatorname{ContinuousBernoulli(\lambda)}}@k} \Big) \approx -\log \Big( 1 - \frac{s}{s + k} \Big) \approx \frac{s}{s+k} \propto k^{-1}.
% \end{equation}
% %
% However, once $s < k$, the scaling law breaks as the expected success rate skyrockets to $1$. Interestingly, in contrast with the Beta and Kumaraswamy distributions, the scaling exponent can only be $-1$ and the distribution's parameter $\lambda$ instead controls the crossover point between the two regimes.

\subsection{Any Continuous Distribution with $p(\operatorname{pass_i@1}) = c > 0$}

Suppose that the distribution over $\operatorname{pass_i@1}$ is continuous and has constant non-zero density near $0$:
%
\begin{equation}
    f(0) = c > 0
\end{equation}
%
Because the density is continuous at $0$ with $f(0) = c > 0$, there exist some $\delta > 0 $ such that:
%
\begin{equation}
    f(p) = c + O(p) \quad \quad \text{ for all } p \in [0, \delta].
\end{equation}
%
Because the small $\operatorname{pass_i@1}$ region dominates for large $k$, a similar argument to the Kumaraswamy argument and Continuous Bernoulli argument yields power law scaling with respect to $k$ with exponent $-1$:
\begin{equation}
    -\log \Big(\operatorname{pass_{\mathcal{D}}@k} \Big) = c \, k^{-1} + o(k^{-1}).
\end{equation}
%
This result is consistent with the Continuous Bernoulli, where $c$ is given by $f_{\operatorname{ContinuousBernoulli(\lambda)}}(0;\lambda) = C(\lambda) (1 - \lambda)$ for $\lambda < 1/2$. This result reveals that the Continous Bernoulli is just one instance of a larger family: any continuous distribution with non-zero constant density at $\operatorname{pass_i@1} = 0$ will exhibit power law scaling with exponent $-1$.

\subsection{Reciprocal Distribution: $\operatorname{pass_i@1} \sim \operatorname{Reciprocal}(a, b)$}
\label{app:sec:power_laws_from_distr_over_exp:reciprocal_distribution}

Next, suppose the model's $\operatorname{pass_i@1} \sim \operatorname{Reciprocal(a, b)}$ distribution with $0 < a < b < 1$. The probability density function of this distribution over the support $x \in [a, b]$ is:
%
\begin{equation}
    f(x; a, b) = \frac{1}{(\log(b) - \log(a)) \, x}
\end{equation}

As with the other distributions, the aggregate success rate after $k$ attempts is:
\[
\operatorname{pass_{\mathrm{Reciprocal}(a,b)}@k}
\;=\;
\mathbb{E}\bigl[\operatorname{pass_i@k}\bigr]
\;=\;
1 \;-\; 
I_k,\quad\text{where}\quad
I_k \;\defeq \; 
\int_{x=a}^{b}
(1-x)^k \,\frac{1}{(\log b-\log a)\,x}\,\mathrm{d}x.
\]
%
We aim to show that $I_k$ is on the order of 
\(\frac{(1-a)^k}{k}\). The main contribution to the integral arises from the vicinity of $x = a$, because $(1-x)^k$ decays rapidly as $x$ grows away from $a$.

\medskip
\noindent
\textbf{Step 1: Change of variable.}
Define $y \defeq x - a$, so the domain $x \in [a,b]$ becomes $y \in [0,b-a]$. Then 
\[
(1 - x)^k 
\;=\;
\bigl((1 - a) - y\bigr)^k,
\]
and
\[
I_k 
\;=\; 
\frac{1}{\log(b/a)}
\int_{y=0}^{\,b-a}
\bigl((1-a) - y\bigr)^k 
\;\frac{1}{a+y}
\,\mathrm{d}y.
\]

\medskip
\noindent
\textbf{Step 2: Expansion near $y=0$.}
For small $y$, write $(1-a)-y = (1-a)\bigl(1 - \tfrac{y}{1-a}\bigr)$; hence
\[
\log\bigl((1-a)-y\bigr)
\;=\;
\log(1-a) 
\;+\;
\log\Bigl(1 - \tfrac{y}{\,1-a\,}\Bigr).
\]
Using $\log(1-z) = -z + O(z^2)$ for small $z$, we get
\[
\log\bigl((1-a)-y\bigr)
\;=\;
\log(1-a)
\;-\;
\frac{y}{1-a}
\;+\;
O\bigl(\tfrac{y^2}{(1-a)^2}\bigr),
\]
so
\[
(1-a-y)^k
\;=\;
\exp\Bigl(
k \,\log(1-a)
\;-\;
k\,\tfrac{y}{\,1-a\,}
\;+\;
O\bigl(\tfrac{k\,y^2}{(1-a)^2}\bigr)
\Bigr).
\]
In particular, for $y$ up to $c/k$, the term $k\,y^2 = O(1)$ remains bounded, so
\[
(1-a-y)^k 
\;=\;
(1-a)^k \,\exp\Bigl(-\tfrac{k\,y}{\,1-a\,}\Bigr)
\bigl[\,1 + O\bigl(\tfrac1k\bigr)\bigr].
\]

\medskip
\noindent
\textbf{Step 3: The integral is dominated by $y \in [0,O(\tfrac1k)]$.}
For large $k$, $\exp\bigl(-\tfrac{k\,y}{\,1-a\,}\bigr)$ decays quickly once $y$ exceeds a multiple of $\tfrac{1-a}{k}$. Consequently, the integral from $y=c_0/k$ to $b-a$ is exponentially small in $k$. On $[0,c_0/k]$, we also have $(a+y)^{-1} = \frac{1}{a} + O\bigl(\tfrac1k\bigr)$. Thus
\[
I_k
\;=\;
\frac{1}{\log(b/a)}
\int_{y=0}^{c_0/k}
(1-a-y)^k
\;\frac{1}{a+y}
\,\mathrm{d}y
\;+\;
\text{(exponentially small tail)}.
\]
Substitute our approximation from Step 2 into the integrand:
\[
(1-a-y)^k \,\frac{1}{a+y}
\;=\;
(1-a)^k 
\,\exp\Bigl(-\tfrac{k\,y}{\,1-a\,}\Bigr)
\;\Bigl[\tfrac{1}{a}+O\bigl(\tfrac1k\bigr)\Bigr].
\]

\medskip
\noindent
\textbf{Step 4: Change variable $u = \frac{k\,y}{1-a}$.}
Then $y = \frac{(1-a)\,u}{k}$ and $\mathrm{d}y = \frac{1-a}{k}\,\mathrm{d}u$. The upper limit $y=c_0/k$ corresponds to $u=c_0\,\bigl(\tfrac{1-a}{1}\bigr)$, so
\[
\int_{y=0}^{c_0/k}
\exp\Bigl(-\tfrac{k\,y}{\,1-a\,}\Bigr)\,\mathrm{d}y
\;=\;
\int_{u=0}^{c_0\,(1-a)}
e^{-u}
\;\frac{1-a}{k}
\,\mathrm{d}u.
\]
Letting $c_0 \to \infty$ only contributes an $e^{-c_0(1-a)}$ factor to the tail, which vanishes. Hence
\[
\int_{y=0}^{\infty}
\exp\Bigl(-\tfrac{k\,y}{\,1-a\,}\Bigr)\,\mathrm{d}y
\;=\;
\frac{1-a}{\,k\,}
\int_{u=0}^{\infty}
e^{-u}\,\mathrm{d}u
\;=\;
\frac{1-a}{k}.
\]
Putting all factors together,
\[
I_k
\;=\;
\frac{1}{\log(b/a)}
\;(1-a)^k
\;\Bigl[\tfrac{1}{a}+O\bigl(\tfrac1k\bigr)\Bigr]
\;\frac{1-a}{k}
\;+\;
\text{(exponentially small in $k$)}.
\]
Thus in big-Theta form,
\[
I_k
\;=\;
\Theta\Bigl(\tfrac{(1-a)^k}{k}\Bigr).
\]

\medskip
\noindent
\textbf{Conclusion.}
Since 
\(\operatorname{pass_{\mathrm{Reciprocal}(a,b)}@k} 
= 
1 - I_k,\)
we get
\[
\operatorname{pass_{\mathrm{Reciprocal}(a,b)}@k}
\;=\;
1
\;-\;
\Theta\Bigl(\tfrac{(1-a)^k}{k}\Bigr).
\]
Moreover, using $-\log(1-y)=y + O(y^2)$ for small $y$, it follows that
\[
-\log\Bigl(\operatorname{pass_{\mathrm{Reciprocal}(a,b)}@k}\Bigr)
\;=\;
\Theta\Bigl(\tfrac{(1-a)^k}{k}\Bigr).
\]
Hence the negative log aggregate success rate converges to $1$ \emph{exponentially fast} in $k$, which is \emph{not} a power law in $k$.

\clearpage

\subsection*{Sufficient Condition for Power-Law Scaling in Negative Log of Aggregate Success}
\label{app:sec:power_laws_from_distr_over_exp:sufficiency}

\begin{theorem}
\label{thm:power-law-scaling}
Let $\mathcal{D}$ be a probability distribution on $[0,1]$ with PDF $f(p)$.  
Suppose there exist constants $b > 0$, $C > 0$, $\theta > 0$ and $\delta > 0$ such that, 
for all $0 < p < \delta$, we have 
\[
  f(p) \;=\; C\,p^{\,b-1} \;+\; O\bigl(p^{\,b-1+\theta}\bigr).
\]
% That is, there is a constant $M>0$ such that
% \[
%   \bigl|\,f(p) - C\,p^{\,b-1}\bigr|
%   \;\le\;
%   M\,p^{\,b-1+\theta}
%   \quad\text{for all }0<p<\delta.
% \]
% Define the aggregate success rate after $k$ attempts by
% \[
%   \operatorname{pass_{\mathcal{D}}@k}
%   \;\;=\;\;
%   \int_0^1 \Bigl[\,1 - (1 - p)^k\Bigr]\,f(p)\,\mathrm{d}p.
% \]
Then, for large $k$,
\[
  1
  \;-\;
  \operatorname{pass_{\mathcal{D}}@k}
  \;=\;
  C\,\Gamma(b)\,k^{-b} \;+\; O\bigl(k^{-b-\min(\,1,\theta)}\bigr),
\]
which implies 
\[
  -\log\Bigl(\operatorname{pass_{\mathcal{D}}@k}\Bigr)
  \;=\;
  C\,\Gamma(b)\,k^{-b} \;+\; o\bigl(k^{-b}\bigr).
\]
Equivalently, including the leading constant),
\[
  -\log\bigl(\operatorname{pass_{\mathcal{D}}@k}\bigr)
  \;\sim\;
  C\,\Gamma(b) \;k^{-b}.
\]
\end{theorem}

\begin{proof}
\textbf{Step 1. Decompose the key integral.}\\
Define
\[
  I_k
  \;\defeq \;
  1 \;-\;\operatorname{pass_{\mathcal{D}}@k} \;=\;
  \int_0^1 (1-p)^k \, f(p)\,\mathrm{d}p.
\]
For a positive constant $c>0$, split $I_k$:
\[
  I_k
  \;=\;
  \int_{\,0}^{\,c/k} (1-p)^k\,f(p)\,\mathrm{d}p
  \;\;+\;\;
  \int_{\,c/k}^{\,1} (1-p)^k\,f(p)\,\mathrm{d}p
  \;\;\defeq\;\;
  I_{k,\mathrm{left}} \;+\; I_{k,\mathrm{right}}.
\]

\paragraph{Right Tail Bound ($I_{k,\mathrm{right}}$).}
For $p \ge c/k$, observe $(1-p)^k \le e^{-k\,p} \le e^{-c}$.  Hence
\[
  I_{k,\mathrm{right}}
  \;=\;
  \int_{c/k}^{1} (1-p)^k\,f(p)\,\mathrm{d}p
  \;\le\;
  e^{-c}\,\int_0^1 f(p)\,\mathrm{d}p
  \;=\;
  e^{-c}.
\]
Since $c$ can be made arbitrarily large, $e^{-c}$ can be driven below \emph{any} power of $1/k$.  
Thus $I_{k,\mathrm{right}} = o\bigl(k^{-\alpha}\bigr)$ for any $\alpha>0$.  
We may therefore focus on
\[
  I_{k,\mathrm{left}}
  \;=\;
  \int_{\,0}^{\,c/k} (1-p)^k\,f(p)\,\mathrm{d}p,
\]
knowing that $I_{k,\mathrm{right}}$ is negligible in polynomial‐type estimates.

\bigskip
\textbf{Step 2. Use the assumed behavior of $f(p)$ near $p=0$.}\\
By hypothesis, for $p$ up to some $\delta>0$,
\[
  f(p)
  \;=\;
  C\,p^{\,b-1} + O\bigl(p^{\,b-1+\theta}\bigr).
\]
Choose $c/k < \delta$, so $p \le c/k < \delta$ for $p$ in the left integral.  Then
\[
  I_{k,\mathrm{left}}
  \;=\;
  \int_{\,0}^{\,c/k} (1-p)^k 
    \Bigl[\,C\,p^{\,b-1} + O\bigl(p^{\,b-1+\theta}\bigr)\Bigr]
  \,\mathrm{d}p.
\]
Split it into main term and error term:
\[
  I_{k,\mathrm{left}}
  \;=\;
  C\,\int_{\,0}^{\,c/k} (1-p)^k\,p^{\,b-1}\,\mathrm{d}p
  \;+\;
  \int_{\,0}^{\,c/k}
     (1-p)^k\,O\bigl(p^{\,b-1+\theta}\bigr)
  \,\mathrm{d}p.
\]
Denote these $T_{\mathrm{main}}$ and $T_{\mathrm{err}}$, respectively.

\bigskip
\textbf{Step 3. Approximate $(1-p)^k$ by $e^{-k p}$ and control the error.}\\
For $p$ in $[0,c/k]$, expand $\log(1-p) = -p + O(p^2)$. Thus
\[
  (1-p)^k 
  \;=\;
  \exp\bigl(k\log(1-p)\bigr)
  \;=\;
  e^{-k\,p}\,\exp\bigl(O(k\,p^2)\bigr)
  \;=\;
  e^{-k\,p}\,\bigl[\,1 + O(k\,p^2)\bigr].
\]
Since $p \le c/k$, we get $k\,p^2 \le c^2/k$, which is bounded for large $k$. Consequently,
\[
  (1-p)^k 
  = 
  e^{-k\,p} + O\bigl(k\,p^2\,e^{-k\,p}\bigr).
\]
We will use this in both $T_{\mathrm{main}}$ and $T_{\mathrm{err}}$.

\bigskip
\textbf{Step 4. Main term $T_{\mathrm{main}}$.}\\
\[
  T_{\mathrm{main}}
  \;=\;
  C\int_{\,0}^{\,c/k}
    (1-p)^k\,p^{\,b-1}
  \,\mathrm{d}p.
\]
Substituting $(1-p)^k = e^{-k\,p} + O\bigl(k\,p^2\,e^{-k\,p}\bigr)$,
%
\[
  T_{\mathrm{main}}
  \;=\;
  C\int_{\,0}^{\,c/k} 
    e^{-k\,p}\,p^{\,b-1}
  \,\mathrm{d}p
  \;+\;
  C\int_{\,0}^{\,c/k} 
    O\bigl(k\,p^{\,b+1}\,e^{-k\,p}\bigr)
  \,\mathrm{d}p.
\]
Call these two integrals $T_{1}$ and $T_{2}$.

\paragraph{$T_{1}$ term.}
\[
  T_{1}
  =
  C\int_{\,0}^{\,c/k}
    p^{\,b-1}\,e^{-k\,p}
  \,\mathrm{d}p.
\]
Make the substitution $u \defeq k\,p$. Then $p = u/k$, $\mathrm{d}p = \mathrm{d}u/k$, and $p^{\,b-1} = k^{-b+1}\,u^{\,b-1}$.  
The upper limit $p = c/k$ becomes $u = c$.  Thus
\[
  T_{1}
  =
  C\int_{\,0}^{\,c}
    \bigl(\tfrac{u}{k}\bigr)^{b-1}\,e^{-u}
    \,\tfrac{\mathrm{d}u}{k}
  =
  C\,k^{-b}
  \int_{\,0}^{\,c}
    u^{\,b-1}\,e^{-u}\,\mathrm{d}u.
\]
As $c \to \infty$, $\int_{0}^{c}u^{\,b-1}e^{-u}\,\mathrm{d}u \to \Gamma(b)$.  
So
\[
  T_{1}
  =
  C\,k^{-b}
  \Bigl( \Gamma(b) - R_c \Bigr),
  \quad
  \text{where }|R_c| = O\bigl(e^{-c}\bigr).
\]
By choosing $c$ large after $k\to\infty$, we conclude
\[
  T_{1}
  =
  C\,\Gamma(b)\,k^{-b} + o\bigl(k^{-b}\bigr).
\]

\paragraph{$T_{2}$ term.}
\[
  T_{2}
  =
  C\int_{\,0}^{\,c/k} O\bigl(k\,p^{\,b+1}\,e^{-k\,p}\bigr)\,\mathrm{d}p.
\]
Inside the integral, $k\,p^{\,b+1}\,e^{-k\,p}$ is the main factor.  Substituting $u \defeq k\,p$ again,
\[
  p^{\,b+1} = \bigl(\tfrac{u}{k}\bigr)^{b+1} = k^{-b-1}\,u^{\,b+1}.
\]
Hence
\[
  T_{2}
  =
  C\,O(1)\,\int_{\,0}^{\,c/k}
    k\,p^{\,b+1}\,e^{-k\,p}
  \,\mathrm{d}p
  =
  O(k)\,
  \int_{\,0}^{\,c/k}
    p^{\,b+1} e^{-k\,p}\,\mathrm{d}p.
\]
Substitute $u=k\,p$ and $\mathrm{d}p = \mathrm{d}u/k$. Then
\[
  T_{2}
  =
  O(k)\,
  \int_{\,0}^{\,c}
    \bigl(\tfrac{u}{k}\bigr)^{b+1} e^{-u}\,\tfrac{\mathrm{d}u}{\,k\,}
  =
  O(k)\,
  k^{-b-2}
  \int_{\,0}^{\,c}
    u^{\,b+1}\,e^{-u}
  \,\mathrm{d}u
  =
  O\bigl(k^{-b-1}\bigr).
\]
Thus $T_{2}$ is of strictly smaller order than $k^{-b}$.  

Combine $T_1$ and $T_2$:
\[
  T_{\mathrm{main}}
  =
  C\,\Gamma(b)\,k^{-b} + O\bigl(k^{-b-1}\bigr).
\]

\bigskip
\textbf{Step 5. Error term $T_{\mathrm{err}}$.}\\
Recall 
\[
  T_{\mathrm{err}}
  =
  \int_{\,0}^{\,c/k}
    (1-p)^k\, O\bigl(p^{\,b-1+\theta}\bigr)
  \,\mathrm{d}p.
\]
Exactly the same substitution $(1-p)^k = e^{-k p} + O(k\,p^2\,e^{-k\,p})$ plus $u = k\,p$ shows
\[
  T_{\mathrm{err}}
  = 
  O\Bigl(\int_{\,0}^{\,c/k}
       p^{\,b-1+\theta}\,e^{-k\,p}\,\mathrm{d}p
    \Bigr)
  \;+\;
  O\Bigl(\int_{\,0}^{\,c/k}
       k\,p^{\,b+1+\theta}\,e^{-k\,p}\,\mathrm{d}p
    \Bigr).
\]
When substituting $u=k\,p$, the exponent on $p$ increases by $+1$ each time if we multiply by $k$, so each term is of order $k^{-b-\theta}$ or smaller.  Concretely,
\[
  \int_{0}^{c/k}
    p^{\,b-1+\theta}\,e^{-k\,p}\,\mathrm{d}p
  =
  k^{-b-\theta}\,\int_{0}^{c}u^{\,b-1+\theta}\,e^{-u}\,\mathrm{d}u
  =
  O\bigl(k^{-b-\theta}\bigr),
\]
and similarly for the second term, which is even smaller.  
Hence
\[
  T_{\mathrm{err}}
  =
  O\bigl(k^{-b-\theta}\bigr).
\]

\bigskip
\textbf{Step 6. Putting it all together.}\\
Summarize:
\[
  I_{k,\mathrm{left}}
  = 
  T_{\mathrm{main}} + T_{\mathrm{err}}
  =
  C\,\Gamma(b)\,k^{-b}
  \;+\;
  O\bigl(k^{-b-1}\bigr)
  \;+\;
  O\bigl(k^{-b-\theta}\bigr).
\]
Thus
\[
  I_{k,\mathrm{left}}
  =
  C\,\Gamma(b)\,k^{-b}
  \;+\;
  O\bigl(k^{-b-\min(1,\theta)}\bigr).
\]
Recalling the tail piece $I_{k,\mathrm{right}} = e^{-c} = o\bigl(k^{-\alpha}\bigr)$ for any $\alpha$, we obtain
\[
  I_k
  =
  I_{k,\mathrm{left}} + I_{k,\mathrm{right}}
  =
  C\,\Gamma(b)\,k^{-b}
  \;+\;
  O\bigl(k^{-b-\min(1,\theta)}\bigr).
\]
Hence
\[
  1 - \operatorname{pass_{\mathcal{D}}@k}
  = 
  I_k
  \;\;\sim\;\;
  C\,\Gamma(b)\,k^{-b}.
\]

\paragraph{Final negative‐log argument.}
Since 
\[
  \operatorname{pass_{\mathcal{D}}@k}
  = 
  1 - I_k
  =
  1
  -
  \bigl( C\,\Gamma(b)\,k^{-b} + O\bigl(k^{-b-\min(1,\theta)}\bigr)\bigr),
\]
for large $k$ it is very close to 1.  Then
\[
  -\log\Bigl(\operatorname{pass_{\mathcal{D}}@k}\Bigr)
  =
  -\log\Bigl(1 - C\,\Gamma(b)\,k^{-b} + \cdots\Bigr).
\]
Using the expansion $-\log(1 - x) = x + O(x^2)$ as $x \to 0$, and here $x = C\,\Gamma(b)\,k^{-b}$, we get
\[
  -\log\Bigl(\operatorname{pass_{\mathcal{D}}@k}\Bigr)
  =
  C\,\Gamma(b)\,k^{-b} + o\bigl(k^{-b}\bigr).
\]
In the ``$\sim$'' notation including the leading coefficient:
\[
  -\log\Bigl(\operatorname{pass_{\mathcal{D}}@k}\Bigr)
  \;\sim\;
  C\,\Gamma(b)\;k^{-b}.
\]
This completes the proof.
\end{proof}


\clearpage


\subsection{Necessary Condition for Power Law Scaling from Distribution over $\operatorname{pass_i@1}$}
\label{app:sec:power_laws_from_distr_over_exp:necessity}


\begin{theorem}
Let $\mathcal{D}$ be a probability distribution over $[0,1]$ with a PDF $f(p)$ satisfying the following regularity near $p=0$:
%
\begin{itemize}
\item \textbf{No point mass at $p=0$.} So $\int_{0}^{1} f(p)\,\mathrm{d}p=1$, and $f$ is a genuine PDF on $(0,1]$.
\item \textbf{Continuity and nonnegative behavior near $p=0$.} There exist $\delta>0$ such that $f$ is continuous on $[0,\delta]$ and has no pathological oscillations or singularities that violate integrability.
\end{itemize}

Define the aggregate success rate at $k$ attempts:
\[
\operatorname{pass_{\mathcal{D}}@k}
\;\defeq\;
\int_{0}^{1} \Bigl[\,1 - (1 - p)^k\Bigr]\,f(p)\,\mathrm{d}p
\]
and relatedly
\[
I_k
\;\;\defeq\;\;
\int_{0}^{1} (1-p)^k\,f(p)\,\mathrm{d}p
\;=\;
1 - \operatorname{pass_{\mathcal{D}}@k}.
\]

\noindent
\textbf{Assume} that there exist constants $A>0$ and $b>0$ such that for large $k$:
\[
-\log\bigl(\operatorname{pass_{\mathcal{D}}@k}\bigr)
\;\;\sim\;\;
A\,k^{-b}
\]
Then
\[
I_k 
\;=\;
A\,k^{-b} + o\bigl(k^{-b}\bigr),
\]
and under the mild regularity assumptions above,
\[
f(p)
\;\;\sim\;\;
\frac{A}{\,\Gamma(b)\,}\;p^{\,b-1}
\quad
\text{as }p\to0^+.
\]
\end{theorem}

\begin{proof}
\textbf{Step 1.  Relating $I_k$ to $-\log(\operatorname{pass_{\mathcal{D}}@k})$.} 

By definition,
\[
\operatorname{pass_{\mathcal{D}}@k} 
\;=\;
1 - I_k,
\quad
I_k 
\;=\;
\int_{0}^{1} (1-p)^k\,f(p)\,\mathrm{d}p.
\]
Since
\[
-\log\Bigl(\operatorname{pass_{\mathcal{D}}@k}\Bigr)
\;\;\sim\;\;
A\,k^{-b},
\]
we have, for large $k$,
\[
\operatorname{pass_{\mathcal{D}}@k}
\;=\;
\exp\bigl(-A\,k^{-b} \,(1+o(1))\bigr).
\]
When $x$ is small, $\exp(-x) =  1 - x + O(x^2)$.  Thus
%
\[
I_k
\;=\;
1 - \operatorname{pass_{\mathcal{D}}@k}
\;=\;
A\,k^{-b} + o\bigl(k^{-b}\bigr).
\]
So
\[
I_k
\;\;\sim\;\;
A\,k^{-b}.
\]

\bigskip
\textbf{Step 2.  Restricting to a small interval near $p=0$.}

Since $(1-p)^k$ decays exponentially once $p$ is on the order of $1/k$ or larger, we split:
\[
I_k 
\;\defeq\;
\int_{0}^{1} (1-p)^k\,f(p)\,\mathrm{d}p
\;=\;
\int_{0}^{c/k} (1-p)^k\,f(p)\,\mathrm{d}p
\;+\;
\int_{c/k}^{1} (1-p)^k\,f(p)\,\mathrm{d}p
\;\defeq\;
I_{k,\mathrm{left}} + I_{k,\mathrm{right}},
\]
for some positive constant $c$.  In the region $p\ge c/k$, we have $(1-p)^k \le e^{-k\,p}\le e^{-c}$, so
\[
I_{k,\mathrm{right}}
\;\le\;
e^{-c}\;\int_0^1 f(p)\,\mathrm{d}p
\;=\;
e^{-c}.
\]
Since $c>0$ can be made large, $e^{-c}$ can be driven below any fixed power of $1/k$.  
Hence for the $\Theta(k^{-b})$ behavior, the main contribution comes from $[0,c/k]$.  

Thus
\[
I_k
\;=\;
I_{k,\mathrm{left}}
\;+\;
o\bigl(k^{-m}\bigr)
\;\;\text{for every }m>0
\]

\textbf{Step 3. Change of variables and controlling the ratio of $(1-p)^k$ to $e^{-k p}$.}

\noindent
\textbf{(a) Ratio to $e^{-k p}$.}
For $p\in\bigl[0,\tfrac{c}{k}\bigr]$, define the ratio
\[
  R_k(p) 
  \;\;\defeq\;\;
  \frac{(1-p)^k}{\,e^{-k\,p}\,}.
\]
We will show that $R_k(p)$ stays close to $1$ uniformly in $p\in[0,c/k]$ for large $k$.  
Indeed,
\[
  (1-p)^k 
  \;=\;
  \exp\Bigl[k\,\log(1-p)\Bigr],
  \quad
  \log(1-p) 
  \;=\;
  -\,p \;-\;\frac{p^2}{2} \;-\;\frac{p^3}{3} \;-\;\dots 
  \,.
\]
Hence
\[
  \log(1-p) + p 
  \;=\;
  -\frac{p^2}{2} \;-\; \frac{p^3}{3} \;-\;\dots
  \;\;=\;\;
  O\bigl(p^2\bigr)
  \quad\text{as }p\to0.
\]
Multiplying by $k$, we get
\[
  k\,\bigl[\log(1-p)+p\bigr]
  \;=\;
  O\bigl(k\,p^2\bigr).
\]
Since $0\le p \le \tfrac{c}{k}$ implies $k\,p^2 \le \tfrac{c^2}{k}$, which $\to 0$ as $k\to\infty$, 
it follows that
\[
  k\,\log(1-p) 
  \;=\;
  -k\,p + O\bigl(\tfrac{1}{k}\bigr).
\]
Exponentiating:
\[
  (1-p)^k
  \;=\;
  e^{-\,k\,p}
  \,\exp\Bigl(O\bigl(\tfrac{1}{k}\bigr)\Bigr)
  \;=\;
  e^{-\,k\,p} \Bigl[\,1 + O\bigl(\tfrac{1}{k}\bigr)\Bigr].
\]
Thus
\[
  R_k(p) 
  \;=\;
  \frac{(1-p)^k}{\,e^{-k\,p}\,}
  \;=\;
  1 + O\bigl(\tfrac{1}{k}\bigr),
\]
with the $O(\tfrac{1}{k})$ bound uniform for all $p\in[0,c/k]$.  In other words, there is some constant $M>0$ (independent of $k$) such that
\[
  \bigl| R_k(p) - 1 \bigr|
  \;\le\;
  \frac{M}{k}
  \quad\text{for all }p\in\Bigl[0,\frac{c}{k}\Bigr].
\]

\vspace*{1em}

\noindent
\textbf{(b) Integral expression using $R_k(p)$.}
Hence on $[0,c/k]$,
\[
  (1-p)^k\,f(p)
  \;=\;
  e^{-k\,p}\,R_k(p)\,f(p).
\]
Thus
\[
  I_{k,\mathrm{left}}
  \;=\;
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,R_k(p)\,\mathrm{d}p.
\]
Define $\Delta_k(p) \defeq R_k(p)-1$, which satisfies $|\Delta_k(p)|\le M/k$.  Then
\begin{equation}
\label{eq:I_left_decomp}
  I_{k,\mathrm{left}}
  =
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p
  \;\;+\;\;
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\Delta_k(p)\,\mathrm{d}p.
\end{equation}

\vspace*{1em}

\textbf{Step 4. Substitution $u=k\,p$ and deriving $f(p)\sim p^{\,b-1}$.}

\noindent
\textbf{(a)  The leading part.}
Focus on the first term of \eqref{eq:I_left_decomp}:
\[
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p.
\]
Substitute $u \defeq k\,p$, so $p=\tfrac{u}{k}$ and $\mathrm{d}p=\tfrac{1}{k}\,\mathrm{d}u$.  The upper limit $p=\tfrac{c}{k}$ becomes $u=c$.  Thus
\[
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p
  =
  \int_{0}^{c}
    e^{-u}
    \,f\Bigl(\frac{u}{k}\Bigr)
    \,\frac{\mathrm{d}u}{\,k\,}.
\]
Hence
\[
  \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p
  =
  \frac{1}{k}\,
  \int_{0}^{c} e^{-u}\,f\Bigl(\frac{u}{k}\Bigr)\,\mathrm{d}u.
\]

\noindent
\textbf{(b)  The error part.}
The second term in \eqref{eq:I_left_decomp} has $\Delta_k(p)=R_k(p)-1$ satisfying $|\Delta_k(p)| \le \frac{M}{k}$.  So
\[
  \left|
    \int_{0}^{c/k} e^{-k\,p}\,f(p)\,\Delta_k(p)\,\mathrm{d}p
  \right|
  \;\le\;
  \frac{M}{\,k\,}\,\int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p.
\]
But the integral $\int_{0}^{c/k} e^{-k\,p}\,f(p)\,\mathrm{d}p$ is precisely the leading part we just considered.  Thus the error is bounded by $\frac{M}{k}$ times a term that will turn out to be $\Theta(k^{-b})$.  
Hence the error is subleading if $b<1$ is not the case---but even then, we can keep track of it systematically.  

Overall, combining both terms, we get
\begin{equation}
\label{eq:I_k_left_rigorous}
  I_{k,\mathrm{left}}
  \;=\;
  \frac{1}{k}
  \int_{0}^{c} e^{-u}\,f\Bigl(\frac{u}{k}\Bigr)\,\mathrm{d}u
  \;+\;
  O\bigl(\tfrac{1}{k}\cdot \text{(leading integral)}\bigr).
\end{equation}

\vspace*{1em}

\noindent
\textbf{(c) Matching $\Theta(k^{-b})$.}
Since $I_k=I_{k,\mathrm{left}} + I_{k,\mathrm{right}}$ with $I_{k,\mathrm{right}}$ negligible, we have
\[
  I_k 
  \;=\;
  \frac{1}{k}
  \int_{0}^{c} e^{-u}\,f\Bigl(\tfrac{u}{k}\Bigr)\,\mathrm{d}u
  \;+\;\text{(small corrections)}.
\]
But by hypothesis, $I_k \sim \alpha\,k^{-b}$.  Thus
\begin{equation}
\label{eq:Ik_transform_condition}
  k\cdot I_k
  \;=\;
  \int_{0}^{c} e^{-u}\,f\Bigl(\tfrac{u}{k}\Bigr)\,\mathrm{d}u 
  \;+\;\text{(smaller terms)}
  \;\;\sim\;\;
  \alpha\,k^{1-b}.
\end{equation}
Hence the expression 
\[
  \int_{0}^{c} e^{-u}\,
  f\Bigl(\tfrac{u}{k}\Bigr)\,\mathrm{d}u
\]
must be $\Theta\bigl(k^{\,1-b}\bigr)$ for large $k$. Since $\tfrac{u}{k}$ is small for $0\le u\le c$, we are effectively sampling $f$ near 0.  For the integral to produce $k^{\,1-b}$, we deduce
\[
  f\Bigl(\tfrac{u}{k}\Bigr)
  \;\;=\;\;
  \Theta\Bigl(\Bigl(\tfrac{u}{k}\Bigr)^{b-1}\Bigr),
\]
i.e.\ $f$ must behave like $p^{\,b-1}$ near $p=0$.  Rewriting the constant in front, one obtains
\[
  f\Bigl(\tfrac{u}{k}\Bigr)
  \;=\;
  \Bigl(\tfrac{u}{k}\Bigr)^{b-1}\,
  \bigl[\text{some positive constant}\bigr].
\]
(We then identify that constant with $\frac{\alpha}{\Gamma(b)}$ by matching the integral precisely, just as in the prior argument.)

\bigskip

\noindent
\textbf{Step 5. Conclusion.} 
We have thus shown that over $p\in[0,c/k]$, one has 
\[
  (1-p)^k 
  \;=\;
  e^{-k\,p}\,\bigl[\,1 + O(\tfrac{1}{k})\bigr],
\]
and upon integrating, the required $k^{-b}$ form for $I_k$ forces 
\[
  f(p) 
  \;=\;
  \frac{A}{\Gamma(b)} \, p^{\,b-1} \;+\; o\bigl(p^{\,b-1}\bigr),
  \quad
  \text{as }p\to0^+.
\]
This completes the necessity proof.

\end{proof}

\noindent
\textbf{Remark (Mild Regularity).}
If $f$ had bizarre oscillations or nonintegrable singularities near $0$, the integral $\int_{0}^1(1-p)^k\,f(p)\,\mathrm{d}p$ might not produce a clean $k^{-b}$.  Typically, we impose monotonicity or at least continuity near $p=0$, no atom at $p=0$, and $f(0)=0$ if $b>1$ or $f(0)>0$ if $b=1$, etc.  These assumptions exclude pathological behaviors and guarantee that the local shape of $f(p)$ drives a clean power law.

\clearpage



\section{Maximum Likelihood Estimation of Scaled Beta-Binomial Distribution}
\label{app:sec:mle_scaled_beta_binomial}

To model the distribution of $\operatorname{pass_i@1}$, we can perform maximum likelihood estimation on a scaled three-parameter Beta-Binomial distribution, which we chose because each attempt on the $i$-th problem is an i.i.d. Bernoulli random variable with success probability $\operatorname{pass_i@1}$, and we introduced a scale parameter because the largest $\operatorname{pass_i@1}$ values were typically 1-2 orders of magnitude less than 1.0 (the maximum of the unscaled beta distribution's support).

In greater detail, as background, the 4-parameter Beta distribution has PDF
%
\begin{equation}
    p_Y(y; \alpha, \beta, a, c) \defeq \frac{(y-a)^{\alpha - 1} (c-y)^{\beta-1}}{(c-a)^{\alpha + \beta -1} \operatorname{B}(\alpha, \beta)},
\end{equation}
%
where $\operatorname{B}(\cdot, \cdot)$ is the \href{https://en.wikipedia.org/wiki/Beta_function}{Beta function}. If the minimum $a$ is fixed at $0$ and the maximum $c$ is constrained to $a < c < 1$, then the scaled three parameter Beta distribution simplifies to:
%
\begin{equation}
        f_P(p; \alpha, \beta, a = 0, c) = \frac{p^{\alpha - 1} (c-p)^{\beta-1}}{c^{\alpha + \beta -1} \operatorname{B}(\alpha, \beta)}.
\end{equation}

We want the PMF of a three-parameter Beta-Binomial distribution based on this scaled Beta distribution. For $n$ samples and $x$ successes, the PMF is:
%
\begin{align}
    P(X = x; \alpha, \beta, c, n) &\defeq \int_{0}^c \binom{n}{x} \, p^x \, (1-p)^{n-x} \, f_P(p; \alpha, \beta, a = 0, c) \, dp\\
    &= \binom{n}{x} \frac{1}{c^{\alpha + \beta -1} \operatorname{B}(\alpha, \beta)} \int_{0}^c p^{x + \alpha - 1} \, (1-p)^{n-x} \, (c-p)^{\beta - 1} \, dp.
\end{align}
%
Using a change of variable $p \defeq c \, z$, the PMF can be rewritten as
%
\begin{align}
    P(X = x; \alpha, \beta, c, n) &= \binom{n}{x} \frac{c^x}{\operatorname{B}(\alpha, \beta)} \int_0^1 z^{x + \alpha -1} \, (1-z)^{\beta -1} \, (1 - cz)^{n - x} \,dz\\
    &= \binom{n}{x}\; \frac{c^{x} \mathrm{B}\bigl(x + \alpha,\;\beta\bigr)}{\mathrm{B}(\alpha,\beta)} \leftindex_2{F}_1\Bigl(-(n - x),\;x + \alpha;\;x + \alpha + \beta;\;c\Bigr),
\end{align}
%
where $\leftindex_2{F}_1(\cdot, \cdot; \cdot; \cdot)$ is the \href{https://en.wikipedia.org/wiki/Hypergeometric_function#Euler_type}{(Gauss) hypergeometric function}.

\clearpage

\section{Maximum Likelihood Estimation of Scaled Kumaraswamy-Binomial Distribution}
\label{app:sec:mle_scaled_kumaraswamy_binomial}

To model the distribution of $\operatorname{pass_i@1}$, we can perform maximum likelihood estimation on a scaled three-parameter Kumaraswamy-Binomial distribution, which we chose because each attempt on the $i$-th problem is an i.i.d. Kumaraswamy random variable with success probability $\operatorname{pass_i@1}$, and we introduced a scale parameter because the largest $\operatorname{pass_i@1}$ values were typically 1-2 orders of magnitude less than 1.0 (the maximum of the unscaled beta distribution's support).

In greater detail, the scaled three parameter Kumaraswamy distribution simplifies to:
%
\begin{equation}
        f_P(p; \alpha, \beta, a = 0, c) = \frac{\alpha \beta}{c^{\alpha}} \, p^{\alpha -1 } \, (1 - (p/c)^{\alpha})^{\beta - 1},
\end{equation}
%
over the support $(0, c)$. The rescaled Kumaraswamy-Binomial distribution then has PMF:
%
\begin{equation}
P(X = x; \alpha, \beta, c, n) 
=
\binom{n}{x}\;\frac{\alpha\,\beta}{c^\alpha}
\int_{0}^{c}
  p^{\,x + \alpha -1}
  \,(1 - p)^{n - x}
  \,\Bigl(1 - \bigl(\tfrac{p}{c}\bigr)^{\alpha}\Bigr)^{\beta - 1}
\,dp.
\end{equation}

One can perform a change of variable $p \defeq c z$, but simplifying yields sums of hypergeometric functions that add little conceptual clarity and so we resort to numerical integration using Python's \href{https://mpmath.org/}{mpmath library} \citep{mpmath}.

% Performing the change of variable $p \defeq c\,z$ (so $dp = c\,dz$ and $z\in(0,1)$), we get:
% %
% \begin{align}
% P(X = x; \alpha, \beta, c, n) 
% &=
% \binom{n}{x}\;\frac{\alpha\,\beta}{c^\alpha}
% \int_{0}^{1}
%   \bigl(c\,z\bigr)^{\,x + \alpha -1}
%   \,\bigl(1 - c\,z\bigr)^{n - x}
%   \,\Bigl(1 - z^{\alpha}\Bigr)^{\beta -1}
%   \,c
% \,dz \\
% &=\;
% \binom{n}{x}\;\alpha\,\beta\; c^{\,x}
% \int_{0}^{1}
%   z^{\,x + \alpha -1}
%   \,\bigl(1 - c\,z\bigr)^{\,n - x}
%   \,\bigl(1 - z^{\alpha}\bigr)^{\beta - 1}
% \,dz.
% \end{align}

% Hence, the \emph{scaled Kumaraswamy-Binomial} PMF is:
% \begin{equation}
% P(X = x;\,\alpha,\beta,c,n)
% \;=\;
% \binom{n}{x}\;\alpha\,\beta\;c^{\,x}
% \int_{0}^{1}
%   z^{\,x + \alpha -1}
%   \,\bigl(1 - c\,z\bigr)^{\,n - x}
%   \,\bigl(1 - z^{\alpha}\bigr)^{\,\beta - 1}
% \,dz
% \end{equation}

\clearpage

\section{Maximum Likelihood Estimation of Scaled Beta-Negative Binomial Distribution}
\label{app:sec:mle_scaled_beta_negative_binomial}

To model the distribution of $\operatorname{pass_i@1}$, we can perform maximum likelihood estimation on a scaled three-parameter Beta-Negative Binomial distribution. Recall that the scaled three parameter Beta distribution is:
%
\begin{equation}
        f_P(p; \alpha, \beta, a = 0, c) = \frac{p^{\alpha - 1} (c-p)^{\beta-1}}{c^{\alpha + \beta -1} \operatorname{B}(\alpha, \beta)}.
\end{equation}

We want the PMF of a three-parameter Beta-Negative Binomial distribution based on this scaled Beta distribution. For $r$ desired successes, the PMF that we first draw $x$ failures is:
%
\begin{align}
P(X = x; \alpha, \beta, c, r) 
&=\;\int_{0}^{c} 
    \underbrace{\binom{x + r - 1}{x}\,p^r\,(1-p)^x}_{\text{NegBin}(r,p)}
    \;\;\underbrace{\frac{p^{\alpha - 1}\,(c-p)^{\beta - 1}}
                          {\,c^{\alpha + \beta - 1}\,\mathrm{B}(\alpha,\beta)\,}}_{\text{scaled Beta PDF}}
    \; dp\\
&=\;\binom{x + r - 1}{x}
   \;\frac{1}{c^{\alpha + \beta - 1}\,\mathrm{B}(\alpha,\beta)} 
   \int_{0}^{c} 
     p^{\,r + \alpha - 1}\,\bigl(1 - p\bigr)^{x}\,\bigl(c - p\bigr)^{\beta - 1}
   \;dp.
\end{align}


Next, substitute $p \;=\; c \, z \Longrightarrow dp \;=\; c\,dz$
which rescales the domain \([0,c]\) to \([0,1]\). Under this change:
%
\[
p^{\,r + \alpha - 1} 
 \;=\; (c\,z)^{\,r + \alpha -1} 
 \;=\; c^{\,r+\alpha-1}\;z^{\,r+\alpha-1},
\]
\[
(c - p)^{\beta - 1}
 \;=\; \bigl(c - c\,z\bigr)^{\beta - 1}
 \;=\; \bigl(c(1 - z)\bigr)^{\beta -1}
 \;=\; c^{\,\beta-1}\,(1 - z)^{\beta-1},
\]
\[
(1 - p)^x \;=\; \bigl(1 - c\,z\bigr)^{x}.
\]
Putting these into the integrand:
\[
\begin{aligned}
p^{\,r + \alpha - 1}\,\bigl(1 - p\bigr)^{x}\,\bigl(c - p\bigr)^{\beta-1}\,dp 
=\;
\Bigl(c^{\,r+\alpha-1}\,z^{\,r+\alpha-1}\Bigr)\;
\Bigl(\bigl(1 - cz\bigr)^{x}\Bigr)\;
\Bigl(c^{\,\beta-1}\,(1 - z)^{\beta-1}\Bigr)
\;\bigl(c\,dz\bigr).
\end{aligned}
\]
Factor out the constants in \(c\):
\[
=\; c^{\,r + \alpha - 1}\;c^{\,\beta - 1}\;c
    \;\; z^{\,r + \alpha -1} \,(1 - cz)^x \,(1 - z)^{\beta-1}\;dz.
\]
Since \(c^{\,r + \alpha - 1} \cdot c^{\,\beta - 1} \cdot c \;=\; c^{\,r + \alpha + \beta -1}\), we get
\[
p^{\,r + \alpha - 1}\,(1 - p)^x\,(c-p)^{\beta-1}\,dp
\;=\; 
c^{\,r + \alpha + \beta -1}\,\,
z^{\,r + \alpha -1}\,(1 - z)^{\beta -1}\,(1 - cz)^{x}\;\,dz.
\]

Plugging back into \(P(X = x; \alpha, \beta, c, r)\) and simplifying:
%
% \begin{equation}
% P(X = x; \alpha, \beta, c, r)
% =\;\binom{x + r - 1}{x}
%    \;\frac{1}{c^{\alpha + \beta - 1}\,\mathrm{B}(\alpha,\beta)} 
%    \;\;c^{\,r + \alpha + \beta -1} 
%    \int_0^1 
%        z^{\,r + \alpha -1}\,(1 - z)^{\beta-1}\,\bigl(1 - cz\bigr)^{x}
%    \;dz.
% \end{equation}

% Simplifying,
%
\begin{equation}
P(X = x; \alpha, \beta, c, r)
=\;
\binom{x + r - 1}{x}
\;\frac{c^r}{\mathrm{B}(\alpha,\beta)}
\;\;\int_0^1
     z^{\,r + \alpha -1}\,(1 - z)^{\beta-1}\,\bigl(1 - c\,z\bigr)^{x}
\,dz.
\end{equation}

We can re-express this using the \href{https://en.wikipedia.org/wiki/Hypergeometric_function#Euler_type}{(Gauss) hypergeometric function} $\leftindex_2{F}_1(\cdot, \cdot; \cdot; \cdot)$:
%
\begin{equation}
P(X = x; \alpha, \beta, c, r)
\;=\;
\binom{x + r - 1}{x}
\;\frac{c^r\,\mathrm{B}\bigl(r + \alpha,\;\beta\bigr)}
     {\mathrm{B}\bigl(\alpha,\;\beta\bigr)}
\;\;
{}_2F_1\!\Bigl(-x,\;r + \alpha;\;r + \alpha + \beta;\;c\Bigr).
\end{equation}

% \subsection*{Sanity check:\; the unscaled case \(\;c = 1\)}
% If \(c=1\), this reduces to the usual Beta--Negative-Binomial (sometimes also called the P\'olya distribution). In that special case,
% \[
% p \;\sim\;\text{Beta}(\alpha,\beta),
% \quad
% X \mid p \;\sim\;\text{NegBin}(r,\,p).
% \]
% Then
% \[
% P(X=x)
% \;=\;
% \int_0^1 
%  \binom{x + r -1}{x}\,p^r\,(1-p)^x\,
%  \frac{p^{\alpha -1}(1-p)^{\beta -1}}{\mathrm{B}(\alpha,\beta)}
% \,dp
% \;=\;
% \binom{x + r -1}{x}
% \,\frac{\mathrm{B}(r + \alpha,\;x + \beta)}{\mathrm{B}(\alpha,\;\beta)}.
% \]
% Setting \(c=1\) in the hypergeometric expression reproduces that result using the known identity for
% \(\,{}_2F_1(-x,\,a;\,a+b;\,1)\).  

% Thus, the final boxed formula above is the natural scaled generalization for \(0 < c < 1\).