\section{Related Work}
\label{sec:related_work}

Research into scaling laws of deep neural networks has a rich history spanning theoretical foundations, empirical validations, and diverse applications. The earliest investigations discovered power law scaling in simple machine learning settings \citep{barkai1993scaling, mhaskar1996neural, pinkus1999approximation}. However, the modern era of scaling laws began with breakthrough studies in neural language models \citep{hestness2017deep, kaplan2020scalinglawsneurallanguage, brown2020languagemodelsfewshotlearners}, catalyzing extensive research across multiple directions.
The theoretical understanding of scaling laws has advanced significantly \citep{spigler2020asymptoticlearningcurves,bousquet2020theoryuniversallearning, hutter2021learningcurvetheory, sharma2022scaling, maloney2022solvable, roberts2022principles, bahri2024explaining, michaud2024quantization, paquette2024fourplus3phases, atanasov2024scaling, bordelon2024dynamical, bordelon2024feature, lin2024scaling, brill2024neural}, complemented by comprehensive empirical studies \citep{rosenfeld2020constructive, henighan2020scaling, gordon2021dataparamscalingnmt, tay2021scaleefficiently, ghorbani2021scaling, tay2022transcendingscalinglaws01, zhai2022scaling, alabdulmohsin2022revisitingscalinglaws, dehghani2023scaling, bachmann2023scalingmlpstaleinductive}.
In the context of language models, researchers have explored scaling behaviors in various aspects: context length \citep{xiong2023effectivelongcontextscalingfoundation}, in-context learning \citep{chan2022datadistributionalincontextlearning, agarwal2024manyshot, arora2024bayesianscalinglawsincontext}, vocabulary size \citep{tao2024scalingvocabulary}, and jailbreaking attempts \citep{anil2024manyshot, hughes2024bestofnjailbreaking}. Studies have also investigated scaling dynamics in fine-tuning \citep{kalajdzievski2024scalinglawsforgettingfinetuning, zhang2024scalingmeetsllmfinetuning}, transfer learning \citep{hernandez2021scalinglawstransfer}, and the impact of repeated data \citep{hernandez2022scalingrepeatdata, muennighoff2023scaling}.
Architectural considerations have been extensively studied, including network design \citep{tay2023scalingvsarchitecture, clark2022unifiedscalinglawsroutedlanguagemodels}, nested models \citep{kudugunta2023matformer}, pruning strategies \citep{rosenfeld2021pruningacrossscales}, and precision requirements \citep{dettmers20234bitprecisionscaling, kumar2024scalinglawprecision,sun2025scalinglawsfloatingpoint}. Research has also addressed multimodal extensions \citep{aghajanyan2023scalinggenerativemultimodallm, cherti2023scalingcontrastivelanguageimagelearning} and inference optimization \citep{sardana2023beyondchinchillaoptimal, brown2024largelanguagemonkeysscaling, snell2024scaling, wu2024inferencescalinglawsempirical, chen2024simpleprovablescalinglaw}.
The field has expanded to encompass diverse domains including reinforcement learning (both single-agent \citep{jones2021scaling, hilton2023scalinglawssingleagentreinforcement, neumann2024alphazeroneuralscalingzipfs} and multi-agent \citep{neumann2022scalingmultiagentrl}), graph networks \citep{liu2024neuralscalinglawsgraphs}, diffusion models \citep{mei2024biggerbetterscalingproperties, liang2024scalinglawsdiffusiontransformers}, and associative memory models \citep{romani2013scalingassociativememory, cabannes2024scalinglawsassociativememories, schaeffer2024bridgingassociativememoryprobabilistic}.
Recent work has explored emerging phenomena such as inverse scaling \citep{mckenzie2023inversescalingbiggerisnt}, unique functional forms \citep{caballero2022broken}, scaling patterns across model families \citep{ruan2024observationalscalinglawspredictability, polo2024slothscalinglawsllm}, and downstream capabilities \citep{srivastava2023imitationgamequantifyingextrapolating, wei2022emergentabilitieslargelanguage, hu2024predictingemergentabilitiesinfinite, schaeffer2024elusive, snell2024predictingemergentcapabilitiesfinetuning, wu2024ushapedinverteduscalingemergent}. Researchers have also investigated critical challenges including data contamination \citep{schaeffer2023pretrainingtestsetneed, jiang2024investigatingdatacontaminationpretraining, dominguezolmedo2024trainingtesttaskconfounds}, model-data feedback loops \citep{dohmatob2024taletailsmodelcollapse, gerstgrasser2024modelcollapseinevitablebreaking, kazdan2024collapsethriveperilspromises}, and overtraining effects \citep{gao2023scalinglawsrewardmodeloveroptimization,gadre2024language}. Additional contributions include studies in sparse autoencoders \citep{gao2024scaling}, biologically-plausible backpropagation \citep{filipovich2022scalinglawsbackpropagation}, and self-supervised learning for vision \citep{schaeffer2024improvedunderstandingutilizationmaximum}.
Recent efforts have also focused on reconciling apparent contradictions in scaling behaviors \citep{besiroglu2024chinchillascalingreplicationattempt, porian2024resolvingdiscrepanciescomputeoptimalscaling}.
% If we missed any relevant citations, we welcome researchers to contact us to have them added.