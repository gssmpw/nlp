\section{Related Work}
\label{sec:related_work}

Research into scaling laws of deep neural networks has a rich history spanning theoretical foundations, empirical validations, and diverse applications. The earliest investigations discovered power law scaling in simple machine learning settings \citep{barkai1993scaling, mhaskar1996neural, pinkus1999approximation}. However, the modern era of scaling laws began with breakthrough studies in neural language models \citep{hestness2017deep, kaplan2020scalinglawsneurallanguage, brown2020languagemodelsfewshotlearners}, catalyzing extensive research across multiple directions.
The theoretical understanding of scaling laws has advanced significantly \citep{spigler2020asymptoticlearningcurves,bousquet2020theoryuniversallearning, hutter2021learningcurvetheory, sharma2022scaling, maloney2022solvable, roberts2022principles, bahri2024explaining, michaud2024quantization, paquette2024fourplus3phases, atanasov2024scaling, bordelon2024dynamical, bordelon2024feature, lin2024scaling, brill2024neural}, complemented by comprehensive empirical studies \citep{rosenfeld2020constructive, henighan2020scaling, gordon2021dataparamscalingnmt, tay2021scaleefficiently, ghorbani2021scaling, tay2022transcendingscalinglaws01, zhai2022scaling, alabdulmohsin2022revisitingscalinglaws, dehghani2023scaling, bachmann2023scalingmlpstaleinductive}.
In the context of language models, researchers have explored scaling behaviors in various aspects: context length \citep{xiong2023effectivelongcontextscalingfoundation}, in-context learning \citep{chan2022datadistributionalincontextlearning, agarwal2024manyshot, arora2024bayesianscalinglawsincontext}, vocabulary size \citep{tao2024scalingvocabulary}, and jailbreaking attempts \citep{anil2024manyshot, hughes2024bestofnjailbreaking}. Studies have also investigated scaling dynamics in fine-tuning \citep{kalajdzievski2024scalinglawsforgettingfinetuning, zhang2024scalingmeetsllmfinetuning}, transfer learning \citep{hernandez2021scalinglawstransfer}, and the impact of repeated data \citep{hernandez2022scalingrepeatdata, muennighoff2023scaling}.
Architectural considerations have been extensively studied, including network design \citep{tay2023scalingvsarchitecture, clark2022unifiedscalinglawsroutedlanguagemodels}, nested models \citep{kudugunta2023matformer}, pruning strategies \citep{rosenfeld2021pruningacrossscales}, and precision requirements \citep{dettmers20234bitprecisionscaling, kumar2024scalinglawprecision,sun2025scalinglawsfloatingpoint}. Research has also addressed multimodal extensions \citep{aghajanyan2023scalinggenerativemultimodallm, cherti2023scalingcontrastivelanguageimagelearning} and inference optimization \citep{sardana2023beyondchinchillaoptimal, brown2024largelanguagemonkeysscaling, snell2024scaling, wu2024inferencescalinglawsempirical, chen2024simpleprovablescalinglaw}.
The field has expanded to encompass diverse domains including reinforcement learning (both single-agent \citep{jones2021scaling, hilton2023scalinglawssingleagentreinforcement, neumann2024alphazeroneuralscalingzipfs} and multi-agent \citep{neumann2022scalingmultiagentrl}), graph networks \citep{liu2024neuralscalinglawsgraphs}, diffusion models \citep{mei2024biggerbetterscalingproperties, liang2024scalinglawsdiffusiontransformers}, and associative memory models \citep{romani2013scalingassociativememory, cabannes2024scalinglawsassociativememories, schaeffer2024bridgingassociativememoryprobabilistic}.
Recent work has explored emerging phenomena such as inverse scaling \citep{mckenzie2023inversescalingbiggerisnt}, unique functional forms \citep{caballero2022broken}, scaling patterns across model families \citep{ruan2024observationalscalinglawspredictability, polo2024slothscalinglawsllm}, and downstream capabilities \citep{srivastava2023imitationgamequantifyingextrapolating, wei2022emergentabilitieslargelanguage, hu2024predictingemergentabilitiesinfinite, schaeffer2024elusive, snell2024predictingemergentcapabilitiesfinetuning, wu2024ushapedinverteduscalingemergent}. Researchers have also investigated critical challenges including data contamination \citep{schaeffer2023pretrainingtestsetneed, jiang2024investigatingdatacontaminationpretraining, dominguezolmedo2024trainingtesttaskconfounds}, model-data feedback loops \citep{dohmatob2024taletailsmodelcollapse, gerstgrasser2024modelcollapseinevitablebreaking, kazdan2024collapsethriveperilspromises}, and overtraining effects \citep{gao2023scalinglawsrewardmodeloveroptimization,gadre2024language}. Additional contributions include studies in sparse autoencoders \citep{gao2024scaling}, biologically-plausible backpropagation \citep{filipovich2022scalinglawsbackpropagation}, and self-supervised learning for vision \citep{schaeffer2024improvedunderstandingutilizationmaximum}.
Recent efforts have also focused on reconciling apparent contradictions in scaling behaviors \citep{besiroglu2024chinchillascalingreplicationattempt, porian2024resolvingdiscrepanciescomputeoptimalscaling}.
% If we missed any relevant citations, we welcome researchers to contact us to have them added.

\section{Discussion and Future Directions}
\label{sec:discussion}

This work advances our mathematical understanding of how and why language model performance improves with additional inference compute through repeat sampling. By establishing rigorous theoretical foundations for these empirically-observed power laws, our work provides practitioners with principled ways to understand and predict model performance when scaling inference compute. The distributional perspective we develop explains previously puzzling deviations from power law scaling and enables more efficient estimation of scaling parameters.

Two related questions are \emph{why} such distributional structure exists in the single-attempt success rates and whether one should expect such structure to appear in future benchmarks. We conjecture there are at least two reasons: (1) benchmark design, in that benchmarks are intentionally crafted that problems have a spread of difficulty without being too easy or too hard, and (2) selection bias, in that more interesting patterns such as power law scaling are more likely to garner more interest from the research community.

Despite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: \textit{why are neural scaling laws power laws?} Just as the scaling behavior of $-\log(\operatorname{pass_{\mathcal{D}}@k})$ only becomes clear for large $k$, so too might the scaling behavior of pretraining cross entropy with pretraining compute $C$.
Specifically, suppose the pretraining cross entropy $\mathcal{L}$ as a function of pretraining compute $C$ is a sum of many functions which decay at different rates:
%
\begin{equation*}
    \mathcal{L}(C) = \omega \Big(\frac{1}{C^{\alpha}} \Big) + \frac{A}{C^{\alpha}} + o \Big(\frac{1}{C^{\alpha}} \Big),
\end{equation*}
%
where $\alpha$ is the smallest (positive) polynomial exponent and $\omega(1/C^{\alpha})$ represents functions that decay more slowly than any polynomial. Initially, for small $C$, the dominant term may be unclear, but as pretraining compute is scaled up across $8-10$ orders of magnitude, the leading order term dominates and an approximate power law emerges:
%
\begin{equation*}
    \mathcal{L}(C) \approx \text{const} + \frac{A}{C^{\alpha}} + 0 \quad \text{ as } \quad C \rightarrow \infty.
\end{equation*}
%
Thus, a power law relationship may only be reasonable for sufficiently large pretraining compute $C$, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice \citep{kaplan2020scalinglawsneurallanguage}. We designate possible functions hiding in $\omega(1/C^{\alpha})$ and $o(1/C^{\alpha})$ as \textit{the dark matter of neural scaling laws}.


\clearpage


% % Acknowledgements should only appear in the accepted version.
\section*{Acknowledgments}

Redacted for blind review.

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.



\section*{Impact Statement}


Our findings have important practical implications for the deployment of large language models, as they can help organizations more accurately forecast compute requirements and make informed trade-offs between model size, inference costs, and performance targets. The mathematical framework we develop could also generalize beyond language models to other domains where similar scaling phenomena emerge. While our work is primarily theoretical, we acknowledge that advances in language model capabilities can have broad societal impacts. We hope that better understanding these fundamental scaling behaviors will help the research community develop more efficient and reliable AI systems.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.
