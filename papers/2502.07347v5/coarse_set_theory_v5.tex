%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove �Numbered� in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Coarse Set Theory for AI Ethics and Decision-Making: A Mathematical Framework for Granular Evaluations}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Takashi} \sur{Izumo}}\email{izumo.takashi@nihon-u.ac.jp}



\affil*[1]{\orgdiv{College of Law}, \orgname{Nihon University}, \orgaddress{\street{Kandamisakimachi}, \city{Chiyodaku}, \postcode{1018351}, \state{Tokyo}, \country{Japan}, ORCID: 0000-0003-0008-4729}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
As artificial intelligence (AI) systems become increasingly embedded in ethically sensitive domains such as education, healthcare, and transportation, the need to balance accuracy and interpretability in decision-making has become a central concern. Coarse Ethics (CE) is a theoretical framework that justifies coarse-grained evaluations, such as letter grades or warning labels, as ethically appropriate under cognitive and contextual constraints. However, CE has lacked mathematical formalization. This paper introduces Coarse Set Theory (CST), a novel mathematical framework that models coarse-grained decision-making using totally ordered structures and coarse partitions. CST defines hierarchical relations among sets and uses information-theoretic tools, such as Kullback-Leibler Divergence, to quantify the trade-off between simplification and information loss. We demonstrate CST through applications in educational grading and explainable AI (XAI), showing how it enables more transparent and context-sensitive evaluations. By grounding coarse evaluations in set theory and probabilistic reasoning, CST contributes to the ethical design of interpretable AI systems. This work bridges formal methods and human-centered ethics, offering a principled approach to balancing comprehensibility, fairness, and informational integrity in AI-driven decisions.
}


%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Coarse Set Theory, AI Ethics, Coarse Ethics, Explainable AI, Coarse-Grained AI Models, KL Divergence}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}
\subsection{Background}\label{sec1.1}
As artificial intelligence (AI) becomes increasingly embedded in decision-making processes, the need for effective user interaction with AI-driven systems has become a critical research concern. Concepts such as fairness, transparency, and explainability, including explainable AI (XAI), have been central to this discourse \citep{gunning2019xai}. However, a well-established trade-off exists between predictive accuracy and interpretability in AI models, where higher precision often results in reduced explainability \citep{BARREDOARRIETA202082}.

To address this challenge, researchers have explored various approximation techniques, which can be broadly categorized into engineering-driven and ethics-driven approaches. From an engineering perspective, a common strategy involves approximating complex black-box models with more interpretable alternatives. Techniques such as Local Interpretable Model-Agnostic Explanations (LIME, \cite{ribeiro2016whyitrustyou}) and SHapley Additive exPlanations (SHAP, \cite{lundberg2017unifiedapproachinterpretingmodel}) have been widely used to provide post-hoc interpretability for AI systems. 

From an ethical standpoint, Coarse Ethics (CE, \cite{izumoweng2022}) proposes that coarse-grained assessments are not only necessary but also ethically justifiable in many decision-making contexts. For example, academic grading systems classify students into letter grades or grade point average (GPA), abstracting away precise score differences. Similarly, phrases like ``Olivia and Noah are Harvard graduates'' generalize individual performance details. In AI-driven decision-making, CE argues that the legitimacy of transforming fine-grained evaluations into coarse ones depends on the cognitive capacity and contextual needs of the audience \citep{izumo2025}.

\subsection{Challenges in Coarse Ethics}\label{sec1.2}
Despite its conceptual appeal, CE faces two primary challenges:

\begin{enumerate}
\item \textbf{Lack of Mathematical Rigor}: While CE provides an ethical justification for coarse evaluations, it lacks a mathematically formalized framework. For instance, Izumo \cite{izumo2025} proposes that an AI assistant should issue coarse warnings such as ``Your driving is dangerous'' rather than continuously recalculating and displaying the optimal speed. However, the study does not specify the exact computational method to determine when and how such warnings should be issued. Without a formalized structure, these decisions may be inconsistent, raising concerns about reliability in AI-driven risk assessment.
\item \textbf{Ambiguity in Categorization}: Prior research has introduced a three-tiered explanatory approach---detailed explanations for experts, coarse explanations for general users, and even coarser explanations for individuals with cognitive limitations \citep{izumo2025}. However, this classification is overly simplistic and lacks adaptability, failing to account for the diverse range of cognitive and contextual factors in human decision-making.
\end{enumerate}

Addressing these limitations is essential, particularly as AI advances toward artificial general intelligence (AGI). Strengthening the mathematical foundation of CE and refining its categorization mechanisms can contribute to the development of more robust, context-aware AI explanations.

\subsection{Methodology}\label{sec1.3} 
Several prior studies share conceptual similarities with CE. For example, He et al. \cite{he2024analogies} explored how analogies can assist laypeople in AI-assisted decision-making. Their work emphasizes the cognitive aspects of explanation reception and user comprehension. While our study shares the objective of improving human-AI interaction, we focus on the mathematical quantification of information loss in coarse evaluations rather than the psychological mechanisms of explainability. Peters and Carman \cite{peters2024cultural} examined the influence of cultural bias in XAI research, demonstrating how users from different cultural backgrounds interpret AI explanations differently. Their study focuses on the subjective perception of AI explanations rather than developing a mathematical framework for evaluation. In contrast, our work formalizes coarse-grained evaluations using set theory, providing a structured approach to managing granularity in AI-driven decision-making.

This paper proposes Coarse Set Theory (CST) as a novel set-theoretic approach to resolving these issues. Set theory has been extensively applied to human decision-making, forming the basis of fuzzy set theory \citep{zadeh1965fuzzy}, rough set theory \citep{pawlak1982rough}, and soft set theory \citep{molodtsov1999soft}. These approaches primarily focus on modeling real-world uncertainty through mathematical representations.

In contrast, CST shifts the focus from enhancing AI's ability to analyze the world to helping humans extract and interpret AI-generated insights. While previous works aimed at improving AI's analytical capabilities with set theory \citep{10.5120/8924-2996, 9015977, dalkilic2022novel}, CST is based on the premise that AI models now outperform human experts in specific domains, as demonstrated by recent studies on AI surpassing human-level performance in standardized exams \citep{bicknell2024chatgpt, katz2023gpt4}. Consequently, the key question is no longer how AI perceives reality, but rather how humans can efficiently understand and interact with AI-driven decisions.

This perspective is particularly relevant as Large Language Models (LLMs) and domain-specific AI systems continue to exceed human performance in complex tasks \citep{vaswani2017attention, radford2018improving, brown2020language, chowdhery2022palm}. As AI models continue to outperform human experts in specialized tasks, discussions surrounding AGI expectations and ethical considerations have intensified \citep{morris2024levelsagioperationalizingprogress}. By establishing a formal mathematical foundation for coarse evaluations, CST offers a critical framework for addressing these challenges and redefining the evolving relationship between AI and human cognition.


\section{Coarse Set}\label{sec2}
\subsection{Axioms}
In discussing coarse sets, this paper begins with totally ordered sets, that is, the binary relation $(A,\preceq)$ that satisfies the following axioms.

\begin{enumerate}
\item \textbf{Reflexivity}: $\forall x \in A~ (x \preceq x)$
\item \textbf{Antisymmetry}: $\forall x,y \in A~ (x \preceq y \land y \preceq x \implies x = y)$
\item \textbf{Transitivity}: $\forall x,y,z \in A~ ( x\preceq y \land y \preceq z \implies x \preceq z)$
\item \textbf{Comparability}: $\forall x,y \in A~ (x \preceq y \lor y \preceq x)$
\end{enumerate}

\begin{remark}
According to the Szpilrajn extension theorem \citep{szpilrajn1930}, any partial order, where some elements cannot be compared, can be extended to a total order, where all elements are comparable. Consider a partially ordered set $(S, \preceq)$, where:
\[
S = \{\text{apple}, \text{lemon}, \text{orange} \}
\]
and the partial order $\preceq$ is given as follows:
\[
\text{apple} \preceq \text{lemon}
\]
There is no predefined order between ``apple'' and ``orange'', nor between ``lemon'' and ``orange.'' Szpilrajn's Extension Theorem states that any partial order can be extended to a total order. Applying this theorem, we obtain a total order by introducing additional relations. One possible total order is:
\[
\text{apple} \preceq \text{lemon} \preceq \text{orange}.
\]
Another valid extension could be:
\[
\text{apple} \preceq \text{orange} \preceq \text{lemon}.
\]
Thus, Szpilrajn's theorem provides a systematic method for extending partial orders into total orders, which can be useful for structured decision-making and ranking problems.
\end{remark}

Given a totally ordered set $(S, \preceq)$ and several subsets of $S$, we denote the ordering as $x \preceq_S y$ (or equivalently, $x \succeq_S y$) when comparing an element $x$ in a subset with an element $y$ in another subset. For example, 
\begin{align*}
(S, \preceq) = (\{a,b,c,d\}, \{(a,a),(a,b),(a,c),(a,d),(b,b),(b,c),(b,d),(c,c),(c,d),(d,d)\})
\end{align*}

\noindent
and there exist two subsets of $S$, namely $\{a,b\}$ and $\{c,d\}$, then it follows that $b \preceq_S c$. By adopting this notation, we define a totally ordered family of sets $(\mathfrak{F} \subseteq \mathcal{P}(S), \preccurlyeq)$ over $(S,\preceq)$. $\mathcal{P}(S)$ is the power  set of $S$. The relation $\preccurlyeq$ on $\mathfrak{F}$ satisfies the following axioms, making $(\mathfrak{F}, \preccurlyeq)$ a totally ordered family of sets.


\begin{enumerate}
\setcounter{enumi}{4}
\item \textbf{Reflexivity}: $\forall A \in \mathfrak{F}~ (A \preccurlyeq A)$
\item \textbf{Antisymmetry}: $\forall A,B \in \mathfrak{F}~ (A \preccurlyeq B \land B \preccurlyeq A \implies A = B)$
\item \textbf{Transitivity}: $\forall A,B,C \in \mathfrak{F}~ (A \preccurlyeq B \land B \preccurlyeq C \implies A \preccurlyeq C)$
\item \textbf{Comparability}: $\forall A,B \in \mathfrak{F}~ (A \preccurlyeq B \lor B \preccurlyeq A)$
\item \textbf{Element-wise Order}: $\forall A,B \in \mathfrak{F}~ (\forall x \in A,\forall y \in B~ (x \preceq_S y) \iff A \preccurlyeq B)$
\end{enumerate}

The element-wise ordering axiom can be interpreted as follows: For any two elements $A$ and $B$ within a family of sets $\mathfrak{F}$, if every element $x$ in $A$ is less than or equal to every element $y$ in $B$ regarding the underlying set $(S, \preceq)$, then $A$ is considered less than or equal to $B$ (denoted $A \preccurlyeq B$). In other words, this axiom establishes that the ordering between elements of $\mathfrak{F}$ is determined by the pairwise ordering of their elements.

\begin{remark}
In mathematical writing, we distinguish between the ordering of elements within a set ($\preceq$) and the ordering of elements within a family of sets ($\preccurlyeq$, \verb|\preccurlyeq| in \LaTeX) to avoid confusion. 
Furthermore, when dealing with the ordering of real numbers or natural numbers, we follow the usual convention of using $\leq$. For example, $1 \leq 2$ and $2.36 \leq 3.14$. Unlike set inclusion ($\subseteq$), $\preccurlyeq$ does not indicate a subset relation.
\end{remark}






\subsection{Coarse Family of Sets}\label{sec2.2}
\subsubsection{Coarse-Grained Partition}\label{sec2.2.1}

\begin{definition}[Coarse-Grained Partition]\label{def1}
Suppose a totally ordered set $(U,\preceq)$ that is the underlying set for a family of sets $\mathfrak{F} \subseteq \mathcal{P}(U)$. If the element-wise ordering is applicable to $\mathfrak{F}$ and $(\mathfrak{F}, \preccurlyeq)$ satisfies the following condition \ref{partition}, then $(\mathfrak{F}, \preccurlyeq)$ provides a \textit{coarse-grained partition} of $(U,\preceq)$, where each element of $\mathfrak{F}$ is referred to as a \textit{grain} of $(U,\preceq)$.

\begin{equation}\label{partition}
\forall u \in U, \exists! A \in \mathfrak{F}~ (u \in A)
\end{equation}
\end{definition}

\begin{remark}
The absence of redundancy is guaranteed by ($U,\preceq$) serving as the base set for $(\mathfrak{F},\preccurlyeq)$, ensuring that elements outside $(U,\preceq)$ do not appear in any grain. 
\end{remark}

From this point onward, we denote $(\mathfrak{F},\preccurlyeq)$ that satisfies the definition \ref{def1} as $(\mathfrak{G},\preccurlyeq)$.

\begin{remark} 
In many coarse-grained partitions over $\mathbb{R}$, the sets in question may be open intervals (e.g., $(a,b)$) or otherwise fail to contain their boundary points. As a result, neither $\max$ nor $\min$ need exist, and while $\sup$ or $\inf$ might exist, they can lie outside the set itself and thus do not act as actual maximum or minimum elements. Consequently, one cannot simply rewrite the condition 
\[
  \forall x \in A,\, \forall y \in B, \quad x \preceq y
\]  
in terms of $\max(A)\le\min(B)$ or $\sup(A)\le\inf(B)$. This elementwise formulation is therefore the most robust choice, accommodating scenarios where sets are unbounded, open, or otherwise lack true extreme points.
\end{remark}



\subsubsection{Three Types of Grains}\label{sec2.2.2}

In defining a coarse-grained partition, we consider three possible approaches to structuring the internal relationships within each grain $G_\iota (\iota \in I) \in \mathfrak{G}$. While all partitions are defined over the totally ordered set $(U, \preceq)$, the internal organization of each grain $G_\iota$ may differ. We categorize these approaches as follows:

\paragraph{Type 1: Inheriting the Order from the Underlying Set.}
This approach assumes that the total order of $U$ is preserved within each grain $G_\iota$. That is, for any grain $G_\iota$, the ordering relation within $G_\iota$ is directly inherited from $(U, \preceq)$. Formally,
\begin{align*}
(G_\iota, \preceq_\iota \subseteq \preceq).
\end{align*}

This ensures consistency with the original structure of $(U,\preceq)$ and allows for numerical computations. For example, if $(U, \preceq) = (\mathbb{N}, \leq)$, then each grain follows $(G_\iota, \leq_\iota)$. However, this approach assumes that order information is necessary for further analysis, which is not the case in this study. Since our focus is on evaluating set cardinalities rather than individual element rankings, order information within each grain is redundant.

\paragraph{Type 2: Treating All Elements in a Grain as Equivalent.}
This approach, used in Izumo and Weng \cite{izumoweng2022} and Izumo \cite{izumo2025}, treats all elements within each $G_\iota$ as equivalent. Mathematically, for all $u_k, u_j \in G_\iota$,
\begin{align*}
\forall u_k, u_j \in G_\iota, \quad u_k \sim u_j,
\end{align*}

\noindent
indicating that all elements within $G_\iota$ are indistinguishable. We denote it as $(G_\iota, \sim)$.

This approach is suitable for categorical classifications, such as medical diagnoses or legal decision-making, where relative ordering within a category is unnecessary. However, it differs from Type 3 in that it explicitly defines equivalence among all elements, which may introduce constraints in certain contexts.

\paragraph{Type 3: Defining Grains as Unstructured Sets.}
In this most general approach, each grain $G_\iota$ is treated as an unordered set, meaning that no internal order relation is assumed within $G_\iota$. Unlike Type 2, which imposes explicit equivalence between elements, this approach does not enforce any specific internal relationships, allowing for greater flexibility. Formally, we express this as
\begin{align*}
G_\iota = \{ u_\kappa \mid \kappa \in K \}, \quad \text{with no predefined order}.
\end{align*}

This approach provides the highest level of generality, making it suitable for applications where hierarchical structures or clustering-based partitions are required. Furthermore, it aligns with the objectives of this study, as order information within $G_\iota$ is unnecessary for evaluating set cardinalities.

\paragraph{Adoption of Type 3 in This Paper.}
For the purposes of this study, we adopt Type 3, where each grain $G_\iota$ is treated as an unordered set. Since our analysis focuses on evaluating set cardinalities rather than the relative positions of elements within each grain, the internal order structure of grains does not impact the results. Thus, adopting Type 3 ensures both mathematical consistency and broader applicability. This choice ensures mathematical consistency while allowing for greater generalizability in the formulation of coarse partitions.

Additionally, ignoring internal order within grains ensures greater flexibility in applications such as machine learning and risk assessment. For instance, when individuals are grouped into the same grain in a risk assessment model, the system does not assume they are strictly ``equivalent'' but rather that their risk levels fall within a defined range.


\begin{remark}
In practical applications, the choice between Type 1, Type 2, or Type 3 depends on the context in which the coarse-grained partition is applied (Table \ref{tab:order_requirements}). For instance, in school assessments where letter grades $(\{\text{poor}, \text{fair}, \text{good}, \text{very-good}, \text{excellent} \}, \preccurlyeq)$ are used, these categories typically represent a coarse-grained partition of the numerical score domain $N = \{0, \dots, 100\}$. The interpretation of specific points within a grain, such as a score classified under the ``good'' category, is context-dependent. It may vary based on the purpose of the test, the grading policy set by the instructor, and the intended use of the final grades. Some systems may treat all scores within a grade range as equivalent (aligning with Type 2), while others may maintain an implicit ordering within each grade (closer to Type 1). The extent to which such internal structure is retained or disregarded is dictated by the underlying pedagogical, administrative, or evaluative objectives.
\end{remark}



\begin{table}[h]
    \centering
    \caption{Criteria for Determining Whether Order is Required within a Grain $G$.}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{|l|c|l|}
        \hline
        \textbf{Operation} & \textbf{Requires} & \textbf{Reason} \\ & \textbf{order?} & \\
        \hline
        Summation $\sum_{x \in G} x$ & No & Addition does not depend on order. \\
        \hline
        Mean $\frac{1}{|G|} \sum_{x \in G} x$ & No & The mean does not depend on order. \\
        \hline
        Minimum/Maximum $\min(G), \max(G)$ & Yes & Order is required to compare elements. \\
        \hline
        Median $\operatorname{median}(G)$ & Yes & Sorting is required. \\
        \hline
        Cumulative distribution function $F_G(x)$ & Yes & Order is required to evaluate probability \\ &  & of ``less than or equal to''. \\
        \hline
        Cardinality (number of elements) $|G|$ & No & The number of elements is \\ & & independent of order. \\
        \hline
        Random sampling $X \sim \text{Uniform}(G)$ & No & Order does not affect random selection. \\
        \hline
        Clustering $\mathfrak{G} = \{ G_1, G_2, G_3 \}$ & No & Clusters do not have an inherent order. \\
        \hline
    \end{tabular}
    \label{tab:order_requirements}
\end{table}




\subsubsection{Example}
For example, let us define a totally ordered set by natural numbers: 

\begin{equation*}
U_1 = \{ 1,2,3 \}~ \text{with the natural order} \leq \text{on}~ U_1
\end{equation*} \vspace{0.0cm}

\noindent
which serves as the underlying set for a family of sets $\mathfrak{F} \subseteq \mathcal{P}(U_1)$. By applying the element-wise ordering to various $\mathfrak{F}$, we obtain the following four coarse-grained partitions of $(U_1,\leq)$:
\begin{align*}
(\mathfrak{G}_1,\preccurlyeq_1) =~ &(\mathfrak{G}_1 = \{ \{ 1 \}, \{ 2 \}, \{ 3 \} \}, \\
& \preccurlyeq_1 \hspace{1.3mm} =\{ (\{1\},\{1\}),(\{1\},\{2\}),(\{1\},\{3\}),(\{2\},\{2\}),(\{2\},\{3\}),(\{3\},\{3\})\}) \\
(\mathfrak{G}_2,\preccurlyeq_2) =~ &(\mathfrak{G}_2 =\{ \{ 1,2 \}, \{ 3 \} \}, \\
& \preccurlyeq_2 \hspace{1.3mm} =\{ (\{1,2\},\{1,2\}),(\{1,2\},\{3\}),(\{3\},\{3\})\}) \\
(\mathfrak{G}_3,\preccurlyeq_3) =~ &(\mathfrak{G}_3 =\{ \{ 1 \}, \{ 2,3 \} \}, \\
& \preccurlyeq_3 \hspace{1.3mm} =\{ (\{1\},\{1\}),(\{1\},\{2,3\}),(\{2,3\},\{2,3\})\}) \\
(\mathfrak{G}_4,\preccurlyeq_4) =~ &(\mathfrak{G}_4 =\{ \{ 1,2,3 \} \}, \\
& \preccurlyeq_4 \hspace{1.3mm} =\{ (\{1,2,3\},\{1,2,3\})\}).
\end{align*}  

In this context, an element of each partition, e.g., $\{1,2 \}$ in $\mathfrak{G}_2$ is a grain of $(U_1,\leq)$, and this expression is natural. 

The collection of coarse-grained partitions of $(U,\preceq)$ is denoted by $\mathcal{C}(U,\preceq)$ in this paper. $\mathcal{C}$ is ``C'' in math script font, in other words, \verb|\mathcal| in \LaTeX. Therefore, the collection $\mathcal{C}$ of the $(U_1,\leq)$ exemplified above is:
\begin{align*}
\mathcal{C}(U_1,\leq) = \{ (\mathfrak{G}_1,\preccurlyeq_1), (\mathfrak{G}_2,\preccurlyeq_2), (\mathfrak{G}_3,\preccurlyeq_3), (\mathfrak{G}_4,\preccurlyeq_4)\}.
\end{align*}  

For $\mathcal{C}(U,\preceq) = \{(\mathfrak{G}_\theta, \preccurlyeq_\theta) \}_{\theta \in \Theta}$, the following formula holds.

\begin{proposition}[Counting Coarse-Grained Partitions]\label{pro1}
Given a totally ordered set $(U = \{u_\kappa | \kappa \in K\}, \preceq)$ with $|U| = |K|$, the number of distinct coarse-grained partitions of $(U,\preceq)$ is:

\begin{equation}
|\mathcal{C}(U,\preceq)| = 2^{|K| - 1}
\end{equation} 
\end{proposition}

\begin{proof}[Proof of Proposition \ref{pro1}]
Consider an ordered sequence of $|U|$ elements. Since $|U| = |K|$, the number of elements in $U$ determines the possible coarse partitions. There are $|K| - 1$ possible positions between consecutive elements in $U$ where we can either: 

\begin{enumerate}
\setlength{\itemsep}{1pt}
\item Insert a partition (creating a new grain), or 
\item Leave the elements in the same grain.
\end{enumerate}

Since each position has two choices, the total number of distinct partitions follows the binary choice pattern: 
\begin{align*}
2^{|K| - 1}.
\end{align*} 

Thus, the number of valid coarse-grained partitions in $\mathcal{C}(U,\preceq)$ is precisely $2^{|K|-1}$.
\end{proof}



\section{Coarse Mapping}\label{sec3}
\subsection{Coarse Mapping Function}\label{sec3.1}

We define a coarse mapping function from a totally ordered set $(U,\preceq)$ to a coarse-grained partition $(\mathfrak{G}, \preccurlyeq)$ as follows. 

\begin{definition}[Coarse Mapping Function]\label{def2} 
For any coarse-grained partition $(\mathfrak{G}, \preccurlyeq) \in \mathcal{C}(U, \preceq)$, define the multivalued function $\mathfrak{c}: U \to \mathfrak{G}$ as follows: $\forall u \in U, \exists! G \in \mathfrak{G} \text{ such that } u \in G \iff \mathfrak{c}(u) = G.$ In other words: 

\begin{equation}
\mathfrak{c}: U \to \mathfrak{G};~ \forall u \in U,\exists! G \in \mathfrak{G}~ (u \in G \iff \mathfrak{c}(u) = G)
\end{equation} 
\end{definition}

For example, consider the following:
\begin{align*}
& U_1 = \{ 1,2,3 \}~ \text{with the natural order} \leq \text{on}~ U_1, \\
& \mathfrak{G}_3 = \{ \{1\}, \{2,3\}\}~ \text{from} \\ 
& \quad \mathcal{C}(U_1,\leq) = \{ \\
& \quad\quad (\mathfrak{G}_1=\{\{1\},\{2\},\{3\}\}, \\ 
& \quad\quad \preccurlyeq_1 \hspace{1.3mm} = \{(\{1\}, \{1\}),(\{1\}, \{2\}), (\{1\}, \{3\}), (\{2\}, \{2\}), (\{2\}, \{3\}), (\{3\}, \{3\})\} \\
& \quad\quad ), \\
& \quad\quad (\mathfrak{G}_2=\{\{1,2\},\{3\}\}, \\
& \quad\quad \preccurlyeq_2 \hspace{1.3mm} = \{(\{1,2\}, \{1,2\}), (\{1,2\}, \{3\}), (\{3\}, \{3\})\} \\
& \quad\quad ), \\
& \quad\quad (\mathfrak{G}_3=\{\{1\},\{2,3\}\}, \\
& \quad\quad \preccurlyeq_3 \hspace{1.3mm} = \{(\{1\}, \{1\}), (\{1\}, \{2,3\}), (\{2,3\}, \{2,3\})\} \\
& \quad\quad ), \\ 
& \quad\quad (\mathfrak{G}_4=\{\{1,2,3\}\}, \\ 
& \quad\quad \preccurlyeq_4 \hspace{1.3mm} = \{(\{1,2,3\}, \{1,2,3\})\} \\
& \quad\quad ) \\
& \quad \}, \\
& \mathfrak{c}: U_1 \to \mathfrak{G}_3;~ \mathfrak{c}(1) = \{1\},~ \mathfrak{c}(2) = \{2,3\},~ \mathfrak{c}(3) = \{2,3\}.
\end{align*} 

\begin{remark}
The function $\mathfrak{c}$ uniquely maps each nonempty element of $U$ to a specific element of $\mathfrak{G}$, ensuring that the function is a well-defined multivalued function; according to the definition of $\mathcal{C}(U,\preceq)$, particularly its uniqueness property, an element of $U$ is not contained multiple times within the elements of $\mathfrak{G}$.
\end{remark}

\begin{proposition}[Quasi-Surjectivity of Coarse Mapping]\label{pro2}
In the context of a multivalued function $F$, if and only if for a certain input $x$, the corresponding set $F(x)$ does not exist, we write $F(x) = \emptyset$. For any $u \in U$, where $(U,\preceq)$ is the base set of $\mathcal{C}(U, \preceq)$, we have $\mathfrak{c}(u) \neq \emptyset$. In other words:
\begin{align}
\forall u \in U,~ \mathfrak{c}(u) \neq \emptyset
\end{align}
\end{proposition}


\begin{proof}[Proof of Proposition~{\upshape\ref{pro2}}]
Based on the properties of covering and uniqueness in $\mathfrak{G}$, each element of the totally ordered set $(U,\preceq)$ serving as the base set for $\mathcal{C}(U,\preceq)$ appears exactly once in the elements of $\mathfrak{G}$. Consequently, each element of the domain $U$ is mapped to an element (i.e., a set) in the codomain $\mathfrak{G}$.
\end{proof}

\begin{theorem}[Unique Partition for Quasi-Bijection]\label{thm3}
For any totally ordered set $(U = \{u_\kappa \mid \kappa \in K \}, \preceq)$, 
\begin{align}
\mathfrak{G} = \{\{u_\kappa\} \mid \kappa \in K\} \iff \forall u,v \in U~ (u \neq v \iff \mathfrak{c}(u) \neq \mathfrak{c}(v))
\end{align} 
\end{theorem}

\begin{proof}[Proof of Theorem~{\upshape\ref{thm3}}]
First, we prove the forward direction ($\implies$). Assume that $\mathfrak{G} = \{\{ u_\kappa \} \mid \kappa \in K\}$ is the discrete partition of $U$. That is, each element $u \in U$ belongs to a singleton set in $\mathfrak{G}$. The coarse mapping function $\mathfrak{c}$, by definition, assigns each element $u$ uniquely to its corresponding grain:
\begin{align*}
\mathfrak{c}(u) = G_u, \quad \text{where } G_u = \{ u \}.
\end{align*} 
Since all grains are distinct, it follows that if $u \neq v$, then $\mathfrak{c}(u) \neq \mathfrak{c}(v)$. Conversely, if $\mathfrak{c}(u) \neq \mathfrak{c}(v)$, then their singleton sets must contain different elements, implying $u \neq v$. Thus, the forward direction ($\implies$) holds.

Now, we prove the reverse direction ($\impliedby$). Suppose $\mathfrak{G} \neq \{\{ u_\kappa \} \mid \kappa \in K\}$. Then, there must exist at least one grain $G \in \mathfrak{G}$ such that $G$ contains multiple elements, i.e., there exist $u, v \in G$ with $u \neq v$. However, this contradicts the assumption that $u \neq v$ implies $\mathfrak{c}(u) \neq \mathfrak{c}(v)$, since both $u$ and $v$ would be mapped to the same grain $G$. Thus, $\mathfrak{G}$ must be the discrete partition $\{\{ u_\kappa \} \mid \kappa \in K\}$ to satisfy the assumption, proving the reverse direction ($\impliedby$).

The theorem is proven since both directions ($\implies$ and $\impliedby$) have been established.
\end{proof}




\subsection{Object Mapping}\label{sec3.2}

For a totally ordered set $(U,\preceq)$ serving as the base set for $\mathcal{C}(U,\preceq)$, consider a map from a nonempty set $O$. For example:
\begin{align*}
&O=\{ o_1,o_2,o_3 \}, \\
&U = \{ 1,2,3 \}~ \text{with the natural order} \leq \text{on}~ U, \\
&f:O \to U;~ f(o_1) = 1,~ f(o_2) = 2,~ f(o_3) = 3.
\end{align*}

In this case, we can consider how the elements of $O$ are distributed with respect to the elements of $U$. For example, for the preceding map, the distribution is one-to-one.

In contrast, the allocation is biased if $f$ is defined as:
\begin{align*}
f:O \to U;~ f(o_1) = 1,~ f(o_2) = 1,~ f(o_3) = 1.
\end{align*} 

By composing the mapping $f: O \to U$ with the coarse mapping $\mathfrak{c}: U \to \mathfrak{G}$ in $\mathcal{C}(U,\preceq)$, we can construct a multivalued mapping from $O$ to $\mathfrak{G}$. For example: 

\begin{align*}
& O=\{ o_1,o_2,o_3 \}, \\
& U = \{ 1,2,3 \}~ \text{with the natural order} \leq \text{on}~ U, \\
& (\mathfrak{G},\preccurlyeq) = (\{\{1\},\{2,3\}\}, \{(\{1\}, \{1\}), (\{1\}, \{2,3\}), (\{2,3\},\{2,3\})\}) ~\text{from}~ \mathcal{C}(U,\leq),\\
&f:O \to U;~ f(o_1) = 1,~ f(o_2) = 2,~ f(o_3) = 3, \\
&\mathfrak{c}: U \to \mathfrak{G};~ \mathfrak{c}(1) = \{1\},~ \mathfrak{c}(2) = \{2,3\},~ \mathfrak{c}(3) = \{2,3\}, \\
&\mathfrak{c} \circ f: O \to \mathfrak{G};~ \mathfrak{c} \circ f(o_1) = \{1\},~ \mathfrak{c} \circ f(o_2) = \{2,3\},~ \mathfrak{c} \circ f(o_3) = \{2,3\}.
\end{align*} 



\subsection{Illustration by School Test}\label{sec3.3}

Let's consider a specific example. Suppose Olivia, Noah, and James took a mathematics test and scored out of 100 points. The scores are then converted into letter grades as follows: 0 to 59 points correspond to a poor, 60 to 69 points to a fair, 70 to 79 points to a good, 80 to 89 points to a very good, and 90 to 100 points to an excellent. In this evaluation system, a scale from 0 to 100 is the underlying set $U$, and the letter grades represent its grains. Now we define $\mathfrak{G}$ as follows:
\begin{align*}
U  =~ & \{ 1,2,3, \dots, 98,99,100 \}~ \text{with the natural order} \leq \text{on}~ U, \\
(\mathfrak{G}, \preccurlyeq) =~ & (\mathfrak{G} = \{ \\ 
& \quad \text{poor} = \{0,\dots,59\}, \\
& \quad \text{fair} = \{60,\dots,69\}, \\
&\quad \text{good} = \{70,\dots,79\}, \\
&\quad \text{very-good} = \{80,\dots,89\}, \\
&\quad \text{excellent} = \{90,\dots,100\}\\
&\}, \preccurlyeq \hspace{1.3mm} = \{ \\
& \quad (\{\text{poor}\}, \{\text{poor}\}), \dots, (\{\text{excellent}\},\{\text{excellent}\}) \\
& \})~ \text{from}~ \mathcal{C}(U,\leq).
\end{align*} 

Then:
\begin{align*}
&O=\{ \text{Olivia},~ \text{Noah},~ \text{James} \}, \\
&f:O \to U;~ f(\text{Olivia}) = 90,~ f(\text{Noah}) = 71,~ f(\text{James}) = 77, \\
&\mathfrak{c}: U \to \mathfrak{G};~ \mathfrak{c}(0) = \text{poor},\dots, \mathfrak{c}(100) = \text{excellent}, \\
&\mathfrak{c} \circ f:O \to \mathfrak{G}; \\
& \quad \mathfrak{c} \circ f(\text{Olivia}) = \text{excellent},~ \mathfrak{c} \circ f(\text{Noah}) = \text{good},~ \mathfrak{c} \circ f(\text{James}) = \text{good}.
\end{align*} 

In this case, comparing the mappings from $O$ to $U$ and from $O$ to $\mathfrak{G}$ is intriguing. For instance, in the mapping from $O$ to $U$, each element in the domain corresponds to a unique element in the codomain. However, in the mapping from $O$ to $\mathfrak{G}$, both Noah and James correspond to the element ``good''. This will affect the frequency and density, as will be discussed in the next section. 

\begin{remark}
The role of the function $\mathfrak{c}$ can be understood as follows: if the three students are aware only of the grading system and the results, they perceive their results as multivalued information. For example, if they are told that Olivia received an ``excellent'' grade, they know that her score was one of 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, or 100. In other words, the role of $\mathfrak{c}$ is to replace the information that Olivia scored 90 with the information that her score was one of 90 through 100. This means that the original information is lost, which gives practical value to the considerations in Section \ref{sec5}.
\end{remark}



\section{Features of Coarse-Grained Partition}\label{sec4}

\subsection{Quasi-Inverse Function}\label{sec4.1}
Since the functions $f: O \to U$ and $\mathfrak{c}: U \to \mathfrak{G}$ are not always injective, it is not possible to define an inverse mapping in the strict sense for them. Thus, we define a set-valued function $r$ and $\mathfrak{r}$ for quasi-inverse mapping as follows.

\begin{definition}[Quasi-Inverse Functions]\label{def3}
Suppose:
\begin{itemize}
\item a nonempty set $O = \{o_\lambda \mid \lambda \in \Lambda\} \neq \{\emptyset \}$,
\item a totally ordered set $(U = \{u_\kappa \mid \kappa \in K\},\preceq)$, 
\item a coarse-grained partition ($\mathfrak{G}_\theta = \{G_{\theta,\iota} \mid \theta \in \Theta, \iota \in I\}, \preccurlyeq_\theta) \in \mathcal{C}(U,\preceq)$,
\item a function $f: O \to U$, which is surjective but not necessarily injective,
\item a coarse mapping function $\mathfrak{c}_\theta: U \to \mathfrak{G}_\theta$.
\end{itemize}



Under these assumptions (hereinafter, these assumptions are collectively called coarsening assumptions and abbreviated as CAs), we define a quasi-inverse function $r$ from $U$ to the power set $\mathcal{P}(O)$ and its coarse version $\mathfrak{r}$ from $\mathfrak{G}_\theta$ to the power set $\mathcal{P}(O)$ as follows:
\begin{align}
    r &: U \to \mathcal{P}(O), & r(u_\kappa) &= \{ o_\lambda \in O \mid f(o_\lambda) = u_\kappa\}, \\
    \mathfrak{r}_\theta &: \mathfrak{G}_\theta \to \mathcal{P}(O), & \mathfrak{r}(G_{\theta,\iota}) &= \{ o_\lambda \in O \mid \mathfrak{c}_\theta(f(o_\lambda)) = G_{\theta,\iota}\}.
\end{align}

\end{definition}

\begin{remark}
As $\mathcal{C}(U,\preceq) = \{(\mathfrak{G}_\theta, \preccurlyeq_\theta)\}_{\theta \in \Theta}$ can have multiple elements, we can choose any $\mathfrak{G}_\theta$ for defining $\mathfrak{r}$, which necessitates a clear specification of $\mathfrak{r}_\theta$; otherwise, we often cannot determine whether $\mathfrak{r}(G_\iota)$ signifies the application of $\mathfrak{r}$ to an element of $\mathfrak{G}_1$ or $\mathfrak{G}_2$; however, if it is clear which $\mathfrak{G}$ is being referred to, we omit $\theta$. This also applies to $\mathfrak{c}$.
\end{remark}

For example, if we apply the quasi-inverse function $r$ to the math test example of Olivia, Noah, and James, then the function $r$ takes the following values:
\begin{align*}
& r: U \to \mathcal{P}(O); \\
& \quad r(0) = \emptyset, r(1) = \emptyset, \dots, \\
& \quad r(71) = \{\text{Noah}\}, \dots, \\
& \quad r(77) = \{\text{James}\}, \dots, \\
& \quad r(90) = \{\text{Olivia}\}, \dots, \\
& \quad r(100) = \emptyset,
\end{align*} 

\noindent
and the values of $\mathfrak{r}(G)$ are:
\begin{align*}
& \mathfrak{r}: \mathfrak{G} \to \mathcal{P}(O); \\ 
& \quad \mathfrak{r}(\text{poor}) = \emptyset, \\
& \quad \mathfrak{r}(\text{fair}) = \emptyset, \\
& \quad \mathfrak{r}(\text{good}) = \{\text{Noah}, \text{James}\}, \\
& \quad \mathfrak{r}(\text{very-good}) = \emptyset, \\
& \quad \mathfrak{r}(\text{excellent}) = \{\text{Olivia}\}.
\end{align*} 



\subsection{Frequency}\label{sec4.2}
We can now define how many objects are mapped to a particular $u$ or $G$ using the functions $r$ and $\mathfrak{r}$ under the CAs:

\begin{definition}[Frequency of Mapped Objects]\label{def4}
The frequency function represents how many elements from the set $O$ are mapped to $u \in U$ or $G \in \mathfrak{G}$ under the quasi-inverse functions $r$ and $\mathfrak{r}$. Specifically, we define the frequency of each element $u \in U$ and each grain $G \in \mathfrak{G}$ as follows:

\begin{equation}
\textbf{Fre}: U \to \mathbb{N}_0 \cup \{ \infty \}, \quad
\textbf{Fre}(u) = 
\begin{cases} 
0, & \text{if } r(u) = \emptyset \\
|r(u)|, & \text{if } r(u) \neq \emptyset
\end{cases}
\end{equation}

\begin{equation}
\mathfrak{Fre}: \mathfrak{G} \to \mathbb{N}_0 \cup \{ \infty \}, \quad
\mathfrak{Fre}(G) = 
\begin{cases} 
0, & \text{if } \mathfrak{r}(G) = \emptyset \\
|\mathfrak{r}(G)|, & \text{if } \mathfrak{r}(G) \neq \emptyset
\end{cases}
\end{equation}
\end{definition}



\subsection{Density}\label{sec4.3}
\begin{definition}[Density of Mapped Objects]
By applying the functions \textbf{Fre} and $\mathfrak{Fre}$, we define the density for $u \in U$ and $G \in \mathfrak{G}$ under the CAs: 

\begin{equation}
\begin{aligned}
& \textbf{Den}: U \to \mathbb{R},~~
\textbf{Den}(u_\kappa) =
\begin{cases}
\frac{\textbf{Fre}(u_\kappa)}{\sum_{\kappa \in K} \textbf{Fre}(u_\kappa)} & \text{if } |K| \leq \aleph_0, \\
\frac{\textbf{Fre}(u_\kappa)}{\int_{\kappa \in K} \textbf{Fre}(u_\kappa) d\kappa} & \text{if } |K| > \aleph_0.
\end{cases}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
& \mathfrak{Den}: \mathfrak{G} \to \mathbb{R},~~
\mathfrak{Den}(G_\iota) =
\begin{cases}
\frac{\mathfrak{Fre}(G_\iota)}{\sum_{\iota \in I} \mathfrak{Fre}(G_\iota)} & \text{if } |I| \leq \aleph_0, \\
\frac{\mathfrak{Fre}(G_\iota)}{\int_{\iota \in I} \mathfrak{Fre}(G_\iota) d\iota} & \text{if } |I| > \aleph_0.
\end{cases}
\end{aligned}
\end{equation} 
\end{definition}

\begin{proposition}[Non-Negativity of Density]\label{pro4}
Under the CAs, the density functions satisfy:
\begin{align}
&\forall u_\kappa \in U,~~ \mathbf{Den}(u_\kappa) \geq 0, \\ 
&\forall G_\iota \in \mathfrak{G},~~ \mathfrak{Den}(G_\iota) \geq 0.
\end{align}
\end{proposition} 

\begin{proof}[Proof of Proposition~{\upshape\ref{pro4}}]
By definition, $\textbf{Den}(u_\kappa)$ and $\mathfrak{Den}(G_\iota)$ are defined as ratios:

\begin{align*}
\textbf{Den}(u_\kappa) &= \frac{|r(u_\kappa)|}{\sum_{\kappa \in K} |r(u_\kappa)|}, & \text{if } |K| \leq \aleph_0, \\
\mathfrak{Den}(G_\iota) &= \frac{|\mathfrak{r}(G_\iota)|}{\sum_{\iota \in I} |\mathfrak{r}(G_\iota)|}, & \text{if } |I| \leq \aleph_0.
\end{align*} 

Since absolute values are non-negative, all terms in the summation are non-negative, and the denominators are strictly positive. Thus:
\begin{align*}
\textbf{Den}(u_\kappa) \geq 0, \quad \text{and} \quad \mathfrak{Den}(G_\iota) \geq 0.
\end{align*} 

For the continuous case $|K| > \aleph_0$, we generalize to integrals:
\begin{align*}
\textbf{Den}(u_\kappa) &= \frac{|r(u_\kappa)|}{\int_{\kappa \in K} |r(u_\kappa)| d\kappa}, \\
\mathfrak{Den}(G_\iota) &= \frac{|\mathfrak{r}(G_\iota)|}{\int_{\iota \in I} |\mathfrak{r}(G_\iota)| d\iota}.
\end{align*} 

Since integration preserves non-negativity, the same reasoning applies, completing the proof.
\end{proof}

\begin{proposition}[Normalization of Density]\label{pro5}
Under the CAs, the density functions $\mathbf{Den}$ and $\mathfrak{Den}$ satisfy the normalization property:

\vskip\baselineskip

\textbf{For $U$ (original domain):}
\begin{align}
\sum_{\kappa \in K} \mathbf{Den}(u_\kappa) &= 1, \quad \text{if } |K| \leq \aleph_0 \\
\int_{\kappa \in K} \mathbf{Den}(u_\kappa) \, d\kappa &= 1, \quad \text{if } |K| > \aleph_0
\end{align}

\textbf{For $\mathfrak{G}$ (coarse-grained domain):}
\begin{align}
\sum_{\iota \in I} \mathfrak{Den}(G_\iota) &= 1, \quad \text{if } |I| \leq \aleph_0 \\
\int_{\iota \in I} \mathfrak{Den}(G_\iota) \, d\iota &= 1, \quad \text{if } |I| > \aleph_0
\end{align} 
\end{proposition}

\begin{proof}[Proof of Proposition~{\upshape\ref{pro5}}]
We first consider the case where $|K| \leq \aleph_0$. In this case, the function $\textbf{Den}$ is defined as:
\begin{align*}
\textbf{Den}(u_\kappa) = \frac{|r(u_\kappa)|}{\sum_{\kappa \in K} |r(u_\kappa)|}.
\end{align*}

Since the denominator is the total sum of the numerators over all $\kappa \in K$, it follows that:
\begin{align*}
\sum_{\kappa \in K} \textbf{Den}(u_\kappa) = \frac{\sum_{\kappa \in K} |r(u_\kappa)|}{\sum_{\kappa \in K} |r(u_\kappa)|} = 1.
\end{align*} 

Similarly, when $|I| \leq \aleph_0$, we have:
\begin{align*}
\mathfrak{Den}(G_\iota) = \frac{|\mathfrak{r}(G_\iota)|}{\sum_{\iota \in I} |\mathfrak{r}(G_\iota)|}.
\end{align*} 

Applying the same reasoning:
\begin{align*}
\sum_{i \in I} \mathfrak{Den}(G_\iota) = \frac{\sum_{i \in I} |\mathfrak{r}(G_\iota)|}{\sum_{i \in I} |\mathfrak{r}(G_\iota)|} = 1.
\end{align*} 

For the case where $|K| > \aleph_0$, we transition from summation to integration:
\begin{align*}
\textbf{Den}(u_\kappa) = \frac{|r(u_\kappa)|}{\int_{\kappa \in K} |r(u_\kappa)| d\kappa}.
\end{align*} 

Since the denominator represents the total integral of the numerators over $K$, we obtain:
\begin{align*}
\int_{\kappa \in K} \textbf{Den}(u_\kappa) d\kappa = \frac{\int_{\kappa \in K} |r(u_\kappa)| d\kappa}{\int_{\kappa \in K} |r(u_\kappa)| d\kappa} = 1.
\end{align*} 

Likewise, for $|I| > \aleph_0$:
\begin{align*}
\mathfrak{Den}(G_\iota) = \frac{|\mathfrak{r}(G_\iota)|}{\int_{i \in I} |\mathfrak{r}(G_\iota)| \, d\iota},
\end{align*} 

which leads to:
\begin{align*}
\int_{i \in I} \mathfrak{Den}(G_\iota) d\iota = \frac{\int_{i \in I} |\mathfrak{r}(G_\iota)|d\iota}{\int_{i \in I} |\mathfrak{r}(G_\iota)| d\iota} = 1.
\end{align*} 

Thus, the proposition holds in all cases.
\end{proof}

\textbf{Den} represents a probability distribution over the underlying set $U$, whereas $\mathfrak{Den}$ represents a probability distribution over the coarse-grained partition $\mathfrak{G}$. 

When $|K| \leq \aleph_0$, \textbf{Den} corresponds to a probability mass function (PMF), where it represents the proportion of elements in $O$ mapped to each $u_\kappa$. For $|K| > \aleph_0$, \textbf{Den} behaves as a probability density function (PDF), describing the probability density of elements mapped to an infinitesimal neighborhood around $u_\kappa$, which determines the probability of an element falling within a small interval centered at $u_\kappa$.

Similarly, $\mathfrak{Den}$ describes the probability of belonging to a grain $G_\iota$. The function aggregates the probabilities of multiple elements from $U$, leading to information loss and a coarser probability distribution.  

\begin{remark}
Since $U$ and $\mathfrak{G}$ are totally ordered by definition, they can be assigned a probability distribution. While $G$ itself is unordered, the lack of order within $G$ does not affect the probability distribution in this context, as we are not concerned with the distribution over individual elements of $G$.
\end{remark}



\section{Information Loss}\label{sec5}
\subsection{Basic Idea}\label{sec5.1}
As \textbf{Den} functions as the PMF or PDF of the original mapping and $\mathfrak{Den}$ functions as the PMF or PDF of the coarse-grained mapping, methods for comparing two probability distributions can be applied by keeping adequate conditions. This study establishes a foundational mathematical framework for coarse evaluations. As a first step, we employ Kullback-Leibler (KL) Divergence ($D_\text{KL}$) due to its well-established role in measuring the divergence between probability distributions in information theory \citep{kullbackleibler1951}. 
\begin{equation}
D_{\text{KL}}(P \parallel Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}
\end{equation} 

While alternative measures such as Jensen-Shannon Divergence or Wasserstein Distance may be explored in future studies, $D_\text{KL}$ provides a mathematically straightforward and interpretable baseline for assessing information loss in coarse evaluations.

To illustrate the motivation behind coarse ethics, consider now that we are a small classroom teacher and give a math test to ten students. The results appear as follows (Table \ref{tab:student_scores}, Figure \ref{fig1}):

\begin{table}[h]
    \centering
    \caption{Student Scores} 
    \label{tab:student_scores}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Student} & \textbf{Score} & \textbf{Pass/Fail}\\
        \hline
        $s_1$ & 5 & pass \\
        $s_2$ & 3 & fail \\
        $s_3$ & 3 & fail \\
        $s_4$ & 3 & fail \\
        $s_5$ & 10 & pass \\
        $s_6$ & 6 & pass \\
        $s_7$ & 2 &fail \\
        $s_8$ & 5 &pass  \\
        $s_9$ & 4 &fail\\
        $s_{10}$ & 0 & fail \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h]
\centering
\caption{Distribution of Student Scores (Right-Skewed)} 
\includegraphics[scale=0.5]{figure001.eps}
\label{fig1}
\end{figure}

\newpage

We can represent the results using coarse set theory as follows:
\begin{align*}
& O = \{s_1,s_2,s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10}\}, \\
& U  = \{ 0,1,2,3,4,5,6,7,8,9,10 \}~ \text{with the natural order}~ \leq \text{on}~ U, \\
& (\mathfrak{G},\preccurlyeq) = (\mathfrak{G} = \{\text{fail}=\{0,1,2,3,4\}, \text{pass}=\{5,6,7,8,9,10\}\}, \preccurlyeq \hspace{1.3mm} = \{ \\
& \quad (\{0,1,2,3,4\}, \{0,1,2,3,4\}), \\
& \quad (\{0,1,2,3,4\}, \{5,6,7,8,9,10\}), \\
& \quad (\{5,6,7,8,9,10\}, \{5,6,7,8,9,10\})\})~ \text{from}~ \mathcal{C}(U,\leq), \\
& \mathfrak{c} \circ f: O \to \mathfrak{G}; \\
& \quad \mathfrak{c} \circ f(s_1) = \text{pass},~ \mathfrak{c} \circ f(s_2) = \text{fail},~ \mathfrak{c} \circ f(s_3) = \text{fail}, \\
& \quad \mathfrak{c} \circ f(s_4) = \text{fail},~ \mathfrak{c} \circ f(s_5) = \text{pass},~ \mathfrak{c} \circ f(s_6) = \text{pass},\\
& \quad \mathfrak{c} \circ f(s_7) = \text{fail},~ \mathfrak{c} \circ f(s_8) = \text{pass},~ \mathfrak{c} \circ f(s_9) = \text{fail},~ \mathfrak{c} \circ f(s_{10}) = \text{fail}. 
\end{align*} 

We compare the probability distributions between the original grading system (ranging from 0 to 10) and the binary pass/fail assessment. More intuitively, if we inform ten students only of the pass/fail criteria and their respective pass or fail status, without disclosing their specific scores---or if we later lose the list of scores and can only recall the pass/fail criteria and each student's pass or fail status---our task is to determine how much information has been lost.

When using $D_{\text{KL}}$ to achieve this objective, determining $P(i)$ is straightforward. The probability $P(i)$ represents the proportion of students who received a specific score $i$ in the original grading system. It is computed as the ratio of the number of students with score $i$ to the total number of students.
 It involves assigning the proportion of elements in $O$ that map to each element in $U$. For instance, $P(0)$ is $\frac{1}{10}$, and $P(1)$ is 0 as to Table \ref{tab:student_scores}.

The challenge lies in determining $Q(i)$. A straightforward but probably unrealistic assumption would be to assign equal probability to all scores (Table \ref{tab:unification_based_qi}). Under the assumption that $Q(i)$ follows a uniform distribution ($Q(i) = \frac{1}{11}$ for all $i$), the $D_{\text{KL}}$ is calculated to be approximately 0.564. This value quantifies the extent to which the uniform assumption deviates from the actual probability distribution $P(i)$.

\begin{table}[h]
    \centering
    \caption{Unification of $Q(i)$} 
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Score} & \textbf{Original Probability $P(i)$} & \textbf{Coarse-grained $Q(i)$} \\
        \hline
        0 & $\frac{1}{10}$ & $\frac{1}{11}$ \\
        1 & $0$ & $\frac{1}{11}$ \\
        2 & $\frac{1}{10}$ & $\frac{1}{11}$ \\
        3 & $\frac{3}{10}$ & $\frac{1}{11}$ \\
        4 & $\frac{1}{10}$ & $\frac{1}{11}$ \\
        5 & $\frac{2}{10}$ & $\frac{1}{11}$ \\
        6 & $\frac{1}{10}$ & $\frac{1}{11}$ \\
        7 & $0$ & $\frac{1}{11}$ \\
        8 & $0$ & $\frac{1}{11}$ \\
        9 & $0$ & $\frac{1}{11}$ \\
        10 & $\frac{1}{10}$ & $\frac{1}{11}$ \\
        \hline
    \end{tabular}
    \label{tab:unification_based_qi}
\end{table} 

However, this assumption does not align with the real-world scenario, where passing students' scores are not necessarily evenly distributed. Furthermore, we want this division to be as similar in a certain sense as possible to the original distribution. Thus, we replace the former way to set $Q(i)$ with another way, that is:
\begin{equation}
\begin{aligned}
Q(i) = \frac{\text{Number of elements in } G(i)}{\text{Total number of elements}} \times \frac{1}{|G(i)|}
\end{aligned}
\end{equation} 

We call this approach Categorical Unification (CU), and the calculation method is named KL Divergence by Categorical Unification ($D_{\text{KL-CU}}$).

\begin{remark}
The computation of $D_{\text{KL}}$ involves the probability distributions $P(i)$ and $Q(i)$. Although $Q(i)$ is defined using grains $G_\iota $, it is important to note that the probability calculation itself does not rely on any internal ordering within grains.
In the CU method, $Q(i)$ is assigned a uniform probability distribution over all elements within its corresponding grain $G_\iota$. Since this method does not differentiate between elements within the same grain, the unordered nature of $G_\iota$ does not affect the probability calculation. More precisely, $G_\iota$ is used solely to determine the range within which $Q(i)$ should be treated as a uniform distribution. This distinction is crucial: while $G_\iota$ partitions the index space to define $Q(i)$, the actual computation of $D_{\text{KL}}$ only involves probability values, making the internal ordering of grains irrelevant. Thus, the assumption that grains are unordered remains fully consistent with the methodology used in defining $Q(i)$ and computing $D_{\text{KL}}$.
\end{remark}



The empirical rationale for this method is as follows: When students are evaluated using a coarse-grained grading system where only categorical results (e.g., pass or fail) are recorded, their exact scores become irretrievable. If a student is classified as ``pass'', the only information available is that their score fell within the pass range (e.g., 5 to 10). Without further data, we cannot determine if they barely passed or aced the test. Given this limitation, assigning equal probability to all scores within the pass category is a reasonable and minimally biased assumption. Since no particular score within the pass category has more supporting evidence than another, a uniform assumption ensures neutrality and avoids introducing arbitrary weighting.

The principle of maximum entropy provides a theoretical justification for CU. It states that among all possible probability distributions satisfying a given set of constraints, the one that maximizes entropy (i.e., uncertainty) is the least biased and most informative distribution \citep{jaynes1957}. In the case of pass/fail classification, however, we are not only constrained by the requirement that probabilities sum to one but also by the fact that we know the number of students in each category. Specifically, there are six students in the fail category and four in the pass category, meaning that the total probability for each category is $\frac{6}{10}$ and $\frac{4}{10}$, respectively. According to the principle of maximum entropy, the probability distribution within each category should be uniform to maximize entropy. That is, within the fail category, the assumption that maximizes entropy (and therefore least biased) is a uniform distribution among the five elements (i.e., 0 to 4), and similarly, a uniform distribution should be assumed within the pass category (i.e., 5 to 10). Therefore, CU assigns uniform probability within each category, which corresponds to the entropy-maximizing distribution given the category-level constraints (Table \ref{tab:density_based_qi}). Since an entropy-maximizing distribution is the one that assumes the least additional information, CU can be considered the most unbiased method for probability assignment within coarse-grained categories.

\begin{table}[h]
    \centering
    \caption{Categorical Unification of $Q(i): T=5$}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Score} & \textbf{Original Probability $P(i)$} & \textbf{Pass/Fail} & \textbf{Coarse-grained $Q(i)$} \\
        \hline
        0 & $\frac{1}{10}$ & Fail & $\frac{6}{10} \cdot \frac{1}{5} = \frac{3}{25}$ \\
        1 & $0$ &  Fail & $\frac{3}{25}$ \\
        2 & $\frac{1}{10}$ & Fail & $\frac{3}{25}$ \\
        3 & $\frac{3}{10}$ &  Fail & $\frac{3}{25}$ \\
        4 & $\frac{1}{10}$ &  Fail & $\frac{3}{25}$ \\
        5 & $\frac{2}{10}$ &  Pass & $\frac{4}{10} \cdot \frac{1}{6} = \frac{1}{15}$ \\
        6 & $\frac{1}{10}$ &  Pass & $\frac{1}{15}$ \\
        7 & $0$ &  Pass & $\frac{1}{15}$ \\
        8 & $0$ &  Pass & $\frac{1}{15}$ \\
        9 & $0$ &  Pass & $\frac{1}{15}$ \\
        10 & $\frac{1}{10}$ &  Pass & $\frac{1}{15}$ \\
        \hline
    \end{tabular}
    \label{tab:density_based_qi}
\end{table}



\subsection{Categorical Unification by Density}\label{sec5.2}
For a coarse-grained partition that is finite, CU of $Q(i)$ can be provided by the functions \textbf{Den} and $\mathfrak{Den}$ under the CAs:
\begin{align}
P(i)~ &= \textbf{Den}(u_\kappa) = \frac{|r(u_\kappa)|}{\sum_{\kappa \in K} |r(u_\kappa)|}\\
Q(i)~ &= \frac{1}{|G_\iota|}\cdot\mathfrak{Den}(G_\iota) = \frac{1}{|G_\iota|}\cdot\frac{|\mathfrak{r}(G_\iota)|}{\sum_{\iota \in I} |\mathfrak{r}(G_\iota)|} = \frac{|\mathfrak{r}(G_\iota)|}{|G_\iota|\sum_{\iota \in I} |\mathfrak{r}(G_\iota)|}
\end{align} 

\begin{theorem}[Condition for Zero Information Loss]\label{thm6}
Under the CAs, for a finite coarse-grained partition $(\mathfrak{G}, \preccurlyeq)$, the $D_{\text{KL-CU}}$ is zero if and only if all elements within each coarse-grained category $G \in \mathfrak{G}$ have the same probability under $P$. That is, the probability distribution $P(i)$ is uniform within each category.
\begin{equation}
\begin{aligned}
& D_{\text{KL-CU}}(P \parallel Q) = 0 \\ 
& \quad \iff \forall G \in \mathfrak{G},\forall u,v \in G,~ \mathbf{Den}(u) = \mathbf{Den}(v) 
\end{aligned}
\end{equation}
\end{theorem}


\begin{proof}[Proof of Theorem~{\upshape\ref{thm6}}]
We use the following well-known property of $D_{\text{KL}}$ \citep{coverthomas2006}: For a discrete probability distribution,
\begin{align*}
& \quad D_{\text{KL}}(P \parallel Q) = 0 \iff P(i) = Q(i), \quad \forall i.
\end{align*} 

Let $G$ be any coarse-grained category in $\mathfrak{G}$, and assume that it contains elements $G = \{ u_1, u_2, \dots, u_n \}$ where $n \in \mathbb{N}$. From the definition of CU, the assigned probabilities in $Q$ satisfy:
\begin{align*}
& Q(u_1) = Q(u_2) = \dots = Q(u_n).
\end{align*} 

To achieve $D_{\text{KL}}(P \parallel Q) = 0$, we must have:
\begin{align*}
& P(u_1) = P(u_2) = \dots = P(u_n),
\end{align*} 

\noindent
which implies that the original probability distribution $P(i)$ is uniform within each category $G$. Thus, for any $G \in \mathfrak{G}$ and for all $u, v \in G$, we conclude:
\begin{align*}
\textbf{Den}(u) = \textbf{Den}(v).
\end{align*} 

Conversely, suppose that for all categories $G$, all elements within the same category satisfy $\textbf{Den}(u) = \textbf{Den}(v)$. Then, by the definition of $P(i)$ and $Q(i)$, we have:
\begin{align*}
P(i) = Q(i), \quad \forall i,
\end{align*} 

\noindent
which, by the property of $D_{\text{KL}}$, implies:
\begin{align*}
D_{\text{KL-CU}}(P \parallel Q) = 0.
\end{align*} 

Therefore, we have proven:
\begin{align*}
& D_{\text{KL-CU}}(P \parallel Q) = 0 \iff \forall G \in \mathfrak{G}, \forall u,v \in G,~ \textbf{Den}(u) = \textbf{Den}(v).
\end{align*}
\end{proof}



\subsection{Minimizing Information Loss}\label{sec5.3}
\subsubsection{Finding the Threshold that Minimizes Information Loss}
We aim to make the probability distribution of the coarse-grained pass/fail evaluation as similar as possible to the original probability distribution of scores ranging from 0 to 10. Here, ``similar'' specifically refers to minimizing information loss, which is quantitatively measured by minimizing the $D_{\text{KL-CU}}$. Suppose we need to divide those ten students (Table \ref{tab:student_scores}) into pass and fail groups, but we can determine the passing threshold $(T)$. What score should be used as $T$?

A seemingly natural approach is to use the average score. Since the mean score in this test is 4.1, setting the passing threshold at $T = 4$ may seem reasonable. If we classify students scoring below 4 as failing and those scoring 4 or above as passing, we obtain a $D_{\text{KL-CU}}$ of approximately 0.524, which is relatively low. However, this is not the minimum. The lowest $D_{\text{KL-CU}}$ is achieved when the passing threshold is set at $T = 7$, resulting in a value of 0.381 (Table \ref{tab:cu_qi}, Figure \ref{fig:cu_qi}).


\begin{table}[h]
    \centering
    \caption{$D_{\text{KL-CU}}$ of Table \ref{tab:student_scores}} 
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Pass Threshold} & \textbf{Pass Students} & \textbf{$D_{\text{KL-CU}}$} \\
        \hline
        0 & all students & 0.564 \\
        1 & $s_2,s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10}$ & 0.563 \\
        2 & $s_2,s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10}$ & 0.537 \\
        3 & $s_3,s_4,s_5,s_6,s_7,s_8,s_9,s_{10}$ & 0.549 \\
        4 & $s_6,s_7,s_8,s_9,s_{10}$ & 0.524 \\
        5 & $s_7,s_8,s_9,s_{10}$ & 0.521 \\
        6 & $s_9,s_{10}$ & 0.421 \\
        7 & $s_{10}$ & 0.381 \\
        8 & $s_{10}$ & 0.472 \\
        9 & $s_{10}$ & 0.538  \\
        10 & $s_{10}$ & 0.563 \\
        11 & no student & 0.564 \\
        \hline
    \end{tabular}
    \label{tab:cu_qi}
\end{table}

\begin{figure}[h]
\centering
\caption{Graphing of Table \ref{tab:cu_qi}} 
\includegraphics[scale=0.48]{figure002.eps}
\label{fig:cu_qi}
\end{figure}


\subsubsection{Is This Result Truly Counterintuitive?}
If an AI teacher performed this calculation and told a human teacher, ``Based on this test, only $s_{10}$ should pass'', the human teacher might find it counterintuitive. However, its interpretation is context-dependent. If the number of passing students is fixed (e.g., a predetermined quota of three students), minimizing $D_{\text{KL-CU}}$ may not always be the correct objective. In such cases, the goal is not to preserve the original probability distribution but rather to adhere to external constraints, such as ensuring that exactly three students pass, regardless of their relative performance. This suggests that setting a fixed pass quota can significantly distort the original probability distribution.

Many educators may have encountered situations where passing students had widely varying abilities or where they had to pass students whose performance was lower than what they would ideally consider acceptable. This raises a critical question: If the information loss is excessively large, should AI provide an advisory to the teacher? To address this, it may be beneficial to equip the AI teacher with a feature that advises, ``The school has mandated that three students must pass this test. Do you believe this number is truly appropriate?'' This would allow for human oversight in cases where strict adherence to an imposed quota might compromise fairness or accuracy.

In some contexts, setting the threshold at $T = 7$, where only $s_{10}$ passes, may actually align with intuition. Consider a scenario where the goal is to divide the class into two distinct groups: one for mathematically advanced students (e.g., an honors class) and another for those requiring more support. The original score distribution implies that only one student significantly outperforms the others in mathematics. Thus, setting $T = 7$ produces a classification that best preserves the structure of the original probability distribution while maintaining a meaningful distinction between high and low performers.

This analysis highlights how the calculation of $D_{\text{KL-CU}}$ can serve as a bridge between AI-driven decision-making and human intuition, offering a foundational example for both XAI and CE.


\section{Discussion}\label{sec6}
\subsection{Mathematical Foundation of Coarse Ethics}\label{sec6.1}
CST has demonstrated that the rationality of coarse evaluations, as argued by CE, can be rigorously analyzed. The coarse evaluations we use in daily life---for instance, the assessment that ``Olivia received an excellent grade on her mathematics test''---exhibit a structured mathematical form within CST. The fact that an evaluation is coarse---for example, that Olivia's specific score difference from other students who also received an ``excellent'' rating is intentionally disregarded---does not imply that the evaluation is ambiguous. Here, we can clearly distinguish between coarseness and ambiguity in assessment.

Traditional ethical theories, particularly consequentialist (e.g., felicific calculus by Bentham \cite{bentham1789introduction}) and virtue-based approaches (e.g., Stoicism), often emphasize continuity in moral evaluation, where actions that align more closely with a given ideal are ranked proportionally higher. However, CE suggests that even when individuals differ significantly in their alignment with an ideal, they may still be evaluated equivalently \citep{izumoweng2022}. This aligns with Immanuel Kant's criticism of excessive ethical precision, particularly his rejection of casuistry---the attempt to make moral laws overly specific through case-by-case analysis \citep{kant2017}. A simple example illustrates this point: Suppose a student receives a failing grade of 30 points on an essay from a human teacher. Dissatisfied, they submit the same essay to an AI teacher for evaluation, which assigns them a score of 30.1 while their peer, who was also given 30 by the human teacher, receives 29.8 from the AI. If the student then argues that the AI's refined scoring justifies a reconsideration of their grade, such a claim would be unreasonable. From the teacher's perspective, both evaluations reflect insufficient performance, and fixating on a marginal difference of 0.3 points does not meaningfully change the outcome.

\subsection{Tradeoff Between Evaluation Efficiency and Information Retention}\label{sec6.2}
As demonstrated in this study, when $Q(i)$ in $D_{\text{KL}}$ is processed through categorical unification, information is preserved only in specific cases. Our findings suggest a potential trade-off between evaluation efficiency and information retention. While this study does not provide a formal proof, we hypothesize that as evaluation processes prioritize simplicity and accessibility, the loss of fine-grained details becomes inevitable. Further research could investigate whether there exists a Pareto-optimal balance between evaluation efficiency and retained information in AI-driven decision-making.

Assuming a uniform distribution within a category is the most reasonable default when no additional information is available. When only a pass label is recorded, any distinction between different passing scores is inherently erased, making a categorically uniform distribution the simplest assumption that reflects this lack of distinguishing data. While alternative assumptions, such as normal or skewed distributions, might seem more refined, they require external data that is often unavailable in real-world grading systems. If historical trends suggest a particular distribution (e.g., most passing students score near the middle of the range), Bayesian updates could provide a better-informed probability model. However, in the absence of such priors, assuming a categorically uniform distribution remains the least biased approach.

\subsection{Application to Explainable AI}\label{sec6.3}
We apply the above discussion to the challenges identified in Section \ref{sec1.2}. One key application is in refining AI-driven safety advisories. For example, in autonomous driving systems, the coarse-graining of safety warnings must not result in the loss of critical risk information. Instead of issuing a generic warning such as ``Your driving is dangerous'', CST can guide AI systems to adaptively determine the level of warning granularity, ensuring that important details about immediate risks (e.g., speed excess relative to road conditions or proximity to nearby obstacles) are retained within the coarsened message. This approach allows for a structured balance between reducing cognitive overload and preserving necessary safety information.

Additionally, CST offers a refined approach to addressing the limitations of the three-tiered explanatory model in CE. Rather than strictly categorizing explanations into those for experts, general users, and individuals with cognitive difficulties, CST allows for dynamic adaptation of explanation granularity by integrating user profiling mechanisms. In a medical diagnostic AI, for example, an expert might receive a detailed probability breakdown of differential diagnoses, while a general patient would be presented with a high-level summary emphasizing key risks and actionable next steps. This ensures that explanations remain accessible without sacrificing necessary details for informed decision-making. This personalized granularity adjustment can improve the effectiveness of AI explanations in domains such as medical diagnostics, legal decision-making, and financial advising, where different users require distinct levels of detail in their assessments.

\section{Conclusion}
This study has established a rigorous mathematical foundation for Coarse Ethics (CE). By introducing the framework of Coarse Set Theory (CST), we demonstrated that coarse evaluations can be systematically analyzed rather than being treated as vague or arbitrary assessments. A coarse set is defined by constructing a coarse-grained partition over a totally ordered set, and its mathematical properties have been formalized through the definitions, propositions, and theorems in this paper. The key conclusions drawn from this study are as follows:

\begin{enumerate}
\item Coarse evaluation methods, as described in CE, can be formulated with mathematical precision, providing a structured approach to assessments.  
\item Different coarse evaluation methods can be analyzed within the framework of CST. In particular, each evaluation method can be represented as a probability distribution, allowing for probabilistic and statistical comparison.  
\item A quantitative approach to evaluating information loss in coarse assessments is provided by Kullback-Leibler Divergence, which measures the deviation of a coarse-grained distribution from the original evaluation. 
\end{enumerate}

As AI systems become more complex, the need for interpretable and adaptable explanations grows. Current XAI techniques, such as LIME and SHAP, often provide explanations at a fixed granularity, which may not align with user-specific cognitive requirements. CST offers a structured way to dynamically adjust the level of detail in AI explanations, ensuring that different users receive information at an appropriate level of abstraction. This approach could enhance AI-assisted decision-making in critical fields such as medicine, finance, and law, where balancing interpretability and complexity is essential.

This study presents an initial exploration of CST, but several limitations remain. Our analysis is restricted to totally ordered sets and assumes a covering condition, meaning that every element of the original set is included within the coarse-grained structure. This differs from previous research \citep{izumo2025}, which allows for the relaxation of coverage constraints. In other words, our approach focuses on cases where a coarse evaluation fully encompasses the set of evaluated entities. Additionally, our discussion has been primarily framed within the context of educational assessments, particularly school testing. Future research should investigate the implications of relaxing the covering condition and explore applications in broader ethical and legal decision-making frameworks.



\appendix
\section*{Appendix A. Python Code for Calculating $D_{\text{KL-CU}}$}

\begin{lstlisting}[language=Python, caption = Python Code for $D_\text{KL-CU}$ of Table \ref{tab:cu_qi}, label = code1]
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

def compute_probability_distribution(scores):
    """Compute P(i)=freq(i)/N for scores 0..max(scores)."""
    total = len(scores)
    max_score = max(scores)
    # Include missing scores with 0 frequency
    freq = Counter(scores)
    p = {i: freq.get(i, 0) / total for i in range(max_score + 1)}
    return p

def coarse_grain_distribution(p, threshold):
    """Group p by T and distribute probability uniformly."""
    scores = sorted(p.keys())
    fail_group = [s for s in scores if s < threshold]
    pass_group = [s for s in scores if s >= threshold]
    
    q = {}
    if fail_group:
        total_fail = sum(p[s] for s in fail_group)
        for s in fail_group:
            q[s] = total_fail / len(fail_group)
    if pass_group:
        total_pass = sum(p[s] for s in pass_group)
        for s in pass_group:
            q[s] = total_pass / len(pass_group)
    return q

def kl_divergence(p, q):
    """Compute D_KL(P||Q)=sum P(i)*log(P(i)/Q(i))."""
    dkl = 0
    for s in p:
        if p[s] > 0 and q.get(s, 0) > 0:
            dkl += p[s] * np.log(p[s] / q[s])
    return dkl

def analyze_thresholds(scores):
    """Compute P and for T compute Q and D_KL.
    Returns list of (T, D_KL)."""
    p = compute_probability_distribution(scores)
    max_score = max(scores)
    results = []
    # Iterate T from 0 to max_score+1
    for T in range(max_score + 2):
        q = coarse_grain_distribution(p, T)
        dkl = kl_divergence(p, q)
        results.append((T, dkl))
    return results

# Test data
test_scores = [5, 3, 3, 3, 10, 6, 2, 5, 4, 0]

# Compute and print KL divergence for each threshold T
results = analyze_thresholds(test_scores)
for T, dkl in results:
    print(f"T = {T:2d}  -->  D_KL(P||Q) = {dkl:.5f}")

# Find optimal threshold with minimal KL divergence
best_threshold, min_dkl = min(results, key=lambda x: x[1])
print(
    f"\nOptimal threshold T = {best_threshold}, "
    f"minimum KL divergence is approx. {min_dkl:.5f}"
)

# Plot the results (optional)
Ts, dkl_vals = zip(*results)
plt.figure(figsize=(8, 5))
plt.plot(Ts, dkl_vals, marker='o', linestyle='-')
plt.xlabel('Pass/Fail Threshold (T)')
plt.ylabel('KL Divergence D_KL(P||Q)')
plt.title('KL Divergence for Each Threshold')
plt.grid(True)
plt.show()
\end{lstlisting}

\section*{Declarations}

\textbf{Funding} \\
This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.

\noindent
\textbf{Conflicts of Interest} \\
The author declares no conflict of interest.

\noindent
\textbf{Ethical Approval} \\
This article does not contain any studies with human participants or animals performed by any of the authors.

\noindent
\textbf{Data Availability} \\
No empirical data was used in this study.

\noindent
\textbf{Code Availability} \\
The Python code used to compute the KL Divergence is available at \url{https://github.com/Takashi-Izumo/CST-implementation}.

\bibliography{references}

\end{document}
