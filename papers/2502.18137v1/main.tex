\documentclass{article}

\usepackage{subfigure}

\usepackage{listings}
\usepackage[normalem]{ulem}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    morecomment=[l][\color{magenta}]{\#},
    frame=single,
    breaklines=true,
    numbers=none,  % left
    numberstyle=\tiny\color{gray},
    backgroundcolor=\color{white},
    tabsize=4,
    captionpos=b
}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{natbib}
\PassOptionsToPackage{table}{xcolor}

\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{tikzmark,calc}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{pifont} 
\usepackage{makecell}
\definecolor{lightblue}{rgb}{0.1, 0.1, 0.9} 
\usepackage{fancyhdr}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}  % [section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}
\usepackage{bm}
\icmltitlerunning{\our: Accurate Sparse Attention Accelerating Any Model Inference} 


\newcommand{\our}{\texttt{SpargeAttn}\xspace}
\newcommand{\sour}{\texttt{SageAttn3}\xspace}
\newcommand{\minfer}{\texttt{Minference}\xspace}
\newcommand{\flexpre}{\texttt{FlexPrefil}\xspace}
\newcommand{\jt}[1]{\textcolor{blue}{{#1}}\xspace}
\newcommand{\jianfei}[1]{\textcolor{red}{{[Jianfei: #1]}}\xspace}
\newcommand{\haofeng}[1]{\textcolor{orange}{{[Haofeng: #1]}}\xspace}
\newcommand{\hxi}[1]{\textcolor{purple}{{[Haocheng: #1]}}\xspace}
\newcommand{\xcd}[1]{\textcolor{blue}{{[xcd: #1]}}\xspace}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
\newcommand{\annotate}[1]{\textcolor{gray}{{#1}}\xspace}
\newcommand{\cogvideo}{\texttt{CogvideoX}\xspace}
\newcommand{\opensora}{\texttt{Open-Sora}\xspace}
\newcommand{\llama}{\texttt{Llama2}\xspace}
\newcommand{\llamal}{\texttt{Llama3.1}\xspace}
\newcommand{\glm}{\texttt{GLM4}\xspace}
\newcommand{\ultrapixel}{\texttt{UltraPixel}\xspace}
\newcommand{\unidiffuser}{\texttt{Unidiffuser}\xspace}
\newcommand{\flux}{\texttt{Flux}\xspace}
\newcommand{\sd}{\texttt{Stable-Diffusion3.5}\xspace}
\newcommand{\mean}{\mathrm{mean}}
\newcommand{\hyvideo}{\texttt{HunyuanVideo}\xspace}
\newcommand{\mochi}{\texttt{Mochi}\xspace}

\definecolor{deepgreen}{rgb}{0.0, 0.5, 0.0}  
\definecolor{deepred}{rgb}{0.6, 0.0, 0.0} 
\newcommand{\dgreen}[1]{\textcolor{deepgreen}{#1}}
\newcommand{\dred}[1]{\textcolor{deepred}{#1}}

\begin{document}

\twocolumn[
\icmltitle{\our: Accurate Sparse Attention Accelerating Any Model Inference}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jintao Zhang}{equal,yyy}
\icmlauthor{Chendong Xiang}{equal,yyy}
\icmlauthor{Haofeng Huang}{equal,yyy}
\icmlauthor{Jia Wei}{yyy}
\icmlauthor{Haocheng Xi}{bkly}
\icmlauthor{Jun Zhu}{yyy}
\icmlauthor{Jianfei Chen}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science and Technology, Tsinghua University}
\icmlaffiliation{bkly}{University of California, Berkeley}
\icmlcorrespondingauthor{Jianfei Chen}{jianfeic@tsinghua.edu.cn}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.27in
]


\printAffiliationsAndNotice{\icmlEqualContribution} 


\begin{abstract}
An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map.
\textbf{A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive.} In this paper, we propose \our, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. \jt{The codes are available at \url{https://github.com/thu-ml/SpargeAttn}}. \footnote{All experiments in this paper used SpargeAttn based on SageAttention. An updated implementation based on SageAttention2, is available at \url{https://github.com/thu-ml/SpargeAttn}. It \jt{further offers a 30\% speedup} over the Attention in this paper.}
\end{abstract}

\input{src/1-Introduction}
\input{src/2-Related_work}
\input{src/4-Method}
\input{src/5-Experiment}
\input{src/6-Conclusion}


\bibliography{main}
\bibliographystyle{icml2025}


\newpage

\appendix
\onecolumn

\input{src/Appendix}


\end{document}
