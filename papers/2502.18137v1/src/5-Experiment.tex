
\begin{table*}[t!]
\caption{End-to-end metrics across text, image, and video generation models. \ding{55} indicates an inability to generate results for evaluation. The speed and sparsity are the average for each layer in the model in real generation tasks described in Sec.~\ref{sec:exp:setup}. The speed and sparsity of \llamal are measured in the NeedleInAHaystack task with a 128K sequence length.} 
    \label{exp:metrics_loss_t2t}
    \setlength\tabcolsep{5pt}
    \small
    \begin{center}
    \begin{tabular}{p{2cm}|p{2.9cm}|c|c|c|c|c}
    \toprule
    {\mbox{\hspace{-.6em}\textbf{Model} (seq\_len)}}  & \hspace{-.1em}\textbf{Attention}~\small{(Sparsity)}  & {\bf Speed (TOPS)$\uparrow$}  & {\bf WikiText (Ppl.) $\downarrow$}  &  {\bf Longbench $\uparrow$} & {\bf InfiniteBench $\uparrow$} & {\bf NIAH $\uparrow$}  \\ \hline

    \multirow{6}{*}{\hspace{-.1em}\makecell[c]{\llamal \\ \small{(128K)}}} & \hspace{-.1em}Full-Attention & 156.9 & 6.013 & 38.682 & 0.6594 & 0.907  \\  
    & \hspace{-.1em}Minference \small{(0.5)} & 140.1 & 10.631 & 28.860  & 0.5152 & 0.832  \\
    & \hspace{-.1em}FlexPrefill \small{(0.5)} & 240.6 & 6.476 &  38.334 & 0.6460 & 0.858 \\
    & \hspace{-.1em}Minference \small{(0.3)} & 115.7 & 6.705 & 34.074  & 0.6532 &  0.870 \\
    & \hspace{-.1em}FlexPrefill \small{(0.42)} & 206.9 & 6.067 & 38.334 & 0.6581 & 0.878   \\
    & \mbox{\hspace{-.1em}\our \small{(0.54)}} & \textbf{708.1} & \textbf{6.020} & \textbf{39.058} & \textbf{0.6638} & \textbf{0.909} \\ \bottomrule
    \end{tabular} 
    \end{center}

    \begin{center}
    \setlength\tabcolsep{5.72pt}
    \begin{tabular}{p{2cm}|p{2.9cm}|c|c|c|c|c|c}
    \toprule
    {\mbox{\textbf{Model} (seq\_len)}} & \hspace{-.3em}\textbf{Attention}~\small{(Sparsity)} & {\bf Speed (TOPS)$\uparrow$} & {\bf CLIPSIM $\uparrow$}  & {\bf CLIP-T $\uparrow$}  & {\bf VQA-a $\uparrow$}  & {\bf VQA-t $\uparrow$}  & {\bf FScore $\uparrow$} \\ \hline
    \multirow{6}{*}{\hspace{.2em}\makecell[c]{\cogvideo \\ \small{(17K)}}} 
    & \hspace{-.3em}Full-Attention  & 166.0 & 0.1819 & 0.9976 & 80.384 & 75.946 & 5.342 \\ 
    & \hspace{-.3em}Minference \small{(0.5)} & 264.6 & 0.1728  & 0.9959  &  70.486  &  62.410  & 2.808 \\
    & \hspace{-.3em}FlexPrefill \small{(0.6)} & 175.3 & 0.1523 &  0.9926 & 1.5171   & 4.5034   & 1.652 \\
    & \hspace{-.3em}Minference \small{(0.3)} & 196.9 & 0.1754 & 0.9964 & 77.326   & 63.525    & 3.742 \\
    & \hspace{-.3em}FlexPrefill \small{(0.45)} & 142.0 & 0.1564  & 0.9917  &  7.7259  & 8.8426  & 2.089 \\
    & \mbox{\hspace{-.3em}\our \small{(0.46)}}   & \textbf{507.9} & \textbf{0.1798} &  \textbf{0.9974}  &  \textbf{78.276}  &  \textbf{74.846}  & \textbf{5.030}  \\ \hline

    \multirow{6}{*}{\hspace{1.5em}\makecell[c]{\mochi \\ \small{(22K)}}} 
    & \hspace{-.3em}Full-Attention & 164.2 & 0.1725 & 0.9990 & 56.472 & 67.663 & 1.681  \\  
    & \hspace{-.3em}Minference \small{(0.5)} & 202.4 & 0.1629  & 0.9891   &  6.668  & 50.839 & 0.653 \\
    & \hspace{-.3em}FlexPrefill \small{(0.48)} & 191.3 & 0.1667 & 0.9898 & 0.582 & 0.0043 & \ding{55} \\
    & \hspace{-.3em}Minference \small{(0.3)} & 147.7 & 0.1682  & 0.9889   & 14.541  & 42.956 & 0.833 \\
    & \hspace{-.3em}FlexPrefill \small{(0.4)} & 171.7 & 0.1677 & 0.9909 & 2.941 & 0.7413 & \ding{55}  \\
    & \mbox{\hspace{-.3em}\our \small{(0.47)}}  & \textbf{582.4} &  \textbf{0.1720}  & \textbf{0.9990} &  \textbf{54.179}  &  \textbf{67.219} & \textbf{1.807} \\
    \bottomrule
    \end{tabular} 
    \end{center}

    \begin{center}
    \setlength\tabcolsep{20.77pt}
    \begin{tabular}{p{1cm}|p{1.8cm}|c|c|c|c}
    \toprule
    {\mbox{\hspace{-1.6em}\textbf{Model} (seq\_len)}} & \mbox{\hspace{-1.65em}\textbf{Attention}~\small{(Sparsity)}} & {\bf Speed (TOPS)$\uparrow$} & {\bf FID $\downarrow$}  & {\bf CLIP $\uparrow$}  & {\bf IR $\uparrow$}
    \\ \hline
    \multirow{6}{*}{\hspace{0em}\makecell[c]{\flux \\ \small{(4.5K)}}} 
    & \hspace{-1.65em}Full-Attention & 158.2 & 166.103 & 31.217 & 0.8701 \\  
    & \hspace{-1.65em}Minference \small{(0.5)} & 151.8 & 180.650 & 30.235 & 0.4084 \\
    & \hspace{-1.65em}FlexPrefill \small{(0.48)}  & 47.7 & 443.928 & 18.3377 & -2.2657     \\
    & \hspace{-1.65em}Minference \small{(0.3)} & 118.9 & 170.221 & 31.001 & 0.7701   \\
    & \hspace{-1.65em}FlexPrefill \small{(0.41)}  & 40.9 & 405.043 & 19.5591 & -2.2362   \\
    & \mbox{\hspace{-1.65em}\our \small{(0.38)}}  & \textbf{280.3} &  \textbf{163.982}  &  \textbf{31.448}  & \textbf{0.9207} \\   \hline

    \multirow{6}{*}{\hspace{-1.93em}\makecell[c]{ \texttt{Stable-}\\ \texttt{Diffusion3.5} \\ \small{(4.5K)}} }
    & \hspace{-1.65em}Full-Attention  & 164.2 & 166.101 & 32.007 & 0.9699     \\  
    & \hspace{-1.65em}Minference \small{(0.5)} & 186.4 & 348.930 & 18.3024 & -2.2678 \\
    & \hspace{-1.65em}FlexPrefill \small{(0.37)}  & 23.1 & 350.497 & 18.447 & -2.2774     \\
    & \hspace{-1.65em}Minference \small{(0.3)} & 150.3 & 337.530 & 18.099 & -2.2647   \\
    & \hspace{-1.65em}FlexPrefill \small{(0.35)}  & 22.7 & 348.612 & 18.147 & -2.2756   \\
    & \mbox{\hspace{-1.65em}\our \small{(0.31)}}  & \textbf{293.0} &  \textbf{166.193}  &  \textbf{32.114}  & \textbf{0.9727}  \\  \bottomrule
    \end{tabular} 
    \end{center}
    \vspace*{-1.25em}
\end{table*}


\section{Experiment}  \label{sec:exp}
\subsection{Setup} \label{sec:exp:setup}
\noindent \noindent\textbf{Models.} 
We validate the effectiveness of \our across diverse representative models from language, image, and video generation.
Specifically, we conduct experiments on \llamal(8B)~\cite{llama31model} for text-to-text, \cogvideo(2B) and \mochi~\cite{genmo2024mochi} for text-to-video, \flux~\cite{flux}(.1-dev) and \sd(large)~\cite{stable_diffusion_3_5} for text-to-image. 


\noindent \noindent\textbf{Datasets.}   
The Text-to-text model is evaluated on four zero-shot tasks: WikiText~\cite{merity2022pointer} to assess the model's prediction confidence, Longbench~\cite{bai2023longbench} and En.MC of InfiniteBench~\cite{zhang-etal-2024-bench} for a comprehensive assessment of long context understanding capabilities, and the Needle-in-A-Haystack task~\cite{LLMTest_NeedleInAHaystack} to assess the model's retrieval ability.
Text-to-video models are evaluated using the open-sora~\cite{opensora} prompt sets.
Text-to-image models are assessed on COCO annotations~\cite{lin2014microsoft}.

\noindent \noindent\textbf{End-to-end metrics.}   
For \llamal, we use perplexity (ppl.)~\cite{jelinek1977perplexity} for WikiText, Longbench score~\cite{bai2023longbench}, and retrival accuracy for the Needle-in-A-Haystack task~\cite{LLMTest_NeedleInAHaystack}.
For text-to-video models, following~\citet{zhao2024viditq}, we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T)~\cite{liu2024evalcrafter} to measure the text-video alignment; VQA-a and VQA-t to assess the video aesthetic and technical quality, and Flow-score (FScore) for temporal consistency~\cite{wu2023exploring}. 
For text-to-image models, generated images are compared with the images in the COCO dataset in three aspects: FID~\cite{heusel2017gans} for fidelity evaluation, \textit{Clipscore} (CLIP)~\cite{hessel2021clipscore} for text-image alignment, and \textit{ImageReward} (IR)~\cite{xu2024imagereward} for human preference.

\noindent \noindent\textbf{Speed and sparsity metric.} We use TOPS (tera operations per second) to evaluate the speed of sparse attention methods. Specifically, \textbf{TOPS} = $O(attn)/t$, where $O(attn)$ represents the total number of operations in a standard attention computation, and $t$ is the latency from a given $(Q, K, V)$ to the output of attention. \uline{\textbf{Note}} that this speed metric is completely fair. This is because the $O(attn)$ is fixed for a set of inputs, and then the speed is determined by $t$, which includes the time spent predicting the sparse region of the attention map.
We define \textbf{Sparsity} as the proportion of the Matmul of $Q_iK_j$ plus $P_i^jV_j$ that are skipped relative to the total number of $Q_iK_j$ plus $P_i^jV_j$ in a full attention required.


\noindent \underline{Implementation and Hyper-parameters.} We implement our method using CUDA. As discussed in Sec.~\ref{sec:hyper-para}, we need to determine $l1, l2$ for models. We use ($l_1=0.08, l_2=0.09$) for \llamal, ($l_1=0.05, l_2=0.06$) for \cogvideo and \mochi, and ($l_1=0.07, l_2=0.08$) for \sd and \flux.


\noindent \textbf{Baselines.} Currently, sparse attention methods applicable across different model types are limited. We choose block-sparse MInference~\cite{jiang2407minference} and FlexPrefill~\cite{FlexPrefill} as our baselines. To vary the \emph{sparsity} of these baselines, we use 30\% and 70\% for MInference, and use $\gamma=0.95$ and $0.99$ for FlexPrefill according to their paper.



\begin{figure}[!h]
    \centering
    \vspace{-.5em}
    \includegraphics[width=.499\textwidth]{figs/visible_image.pdf}
    \vspace{-1.75em}
    \caption{Comparison examples on \flux and \sd. The sparsity of \our, MInference and FlexPrefill is 0.38, 0.3, and 0.4 on \flux and 0.31, 0.3, and 0.35 on \sd.}
    \vspace{-1em}
    \label{fig:visible_image}
\end{figure}

\begin{figure}[!h]
    \centering
    \vspace{-.5em}
    \includegraphics[width=.49\textwidth]{figs/visible_video.pdf}
    \vspace{-2em}
    \caption{Comparison examples on \mochi. The sparsity of \our, MInference and FlexPrefill is 0.47, 0.3, and 0.4.}
    \vspace{-1em}
    \label{fig:visible_video}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.45\textwidth]{figs/niah128k.pdf}
    \vspace{-1em}
    \caption{A NeedleInAHaystack comparison example on \llamal. The sparsity of \our, MInference, and FlexPrefill is 0.5, 0.5, and 0.54. }
    \vspace{-1em}
    \label{fig:niah_example}
\end{figure}



% \subsection{Kernel Accuracy, Sparsity, and Speed}
% Xxx.


\subsection{Quality and Efficiency Evaluation}

\textbf{End-to-end metrics.} We assess the end-to-end metrics of various models using \our compared to using full attention and baselines. Table~\ref{exp:metrics_loss_t2t} shows the results. We can observe that our method incurs almost no end-to-end metric loss across various models compared to Full-Attention and surpasses baselines with various sparsity levels in terms of end-to-end accuracy. Fig.~\ref{fig:visible_image} and~\ref{fig:visible_video} show some visible comparison examples on \flux, \sd, and \mochi, showing that \our incurs no performance loss and outperforms baselines.


\begin{figure}[!th]
    \centering
    \vspace{-.5em}
    \includegraphics[width=.475\textwidth]{figs/speed_comparison.pdf}
    \vspace{-1.5em}
    \caption{Kernel speed comparison under varying sparsity on RTX4090. Input tensors have a sequence length of 22K and a head dimension of 128. \textit{SpargeAttn+FA2/Sage/Sage2} means deploying our method on FlashAttention2, SageAttention or SageAttention2~\cite{zhang2024sageattention2}.}
    \vspace{-1em}
    \label{fig:kernel_speed}
\end{figure}



\textbf{Attention speed.} Table~\ref{exp:metrics_loss_t2t} shows that our method achieves faster speeds compared to Full-Attention and surpasses baselines with various sparsity levels in terms of attention speed. Fig.~\ref{fig:kernel_speed} illustrates the kernel speeds of various methods across different sparsity, highlighting the efficiency of our approach and its significant advantage over other methods.




\begin{table}[h!]
\small
    \centering
    \vspace{-.5em}
    \caption{End-to-end generation latency using \our.}
    \label{tab:e2e_speedup}
        \centering
        \setlength\tabcolsep{0.27pt}
        \begin{tabular}{c|c|c|c|c}
            \toprule
            \textbf{Model} & \textbf{GPU} & \textbf{Original} & \makecell[c]{\texttt{SageAttn}} & \makecell[c]{\our} \\
            \midrule
            \cogvideo & RTX4090 & 87 s & 68 s & \textbf{53 s} \\
            \mochi & L40 & 1897 s & 1544 s & \textbf{1037 s} \\
            \llamal (24K)  & RTX4090 & 4.01 s & 3.53 s & \textbf{2.6 s} \\
            \llamal (128K)  & L40 & 52 s &  42s & \textbf{29.98 s} \\
            \bottomrule
        \end{tabular}
    \vspace{-.5em}
\end{table}



\textbf{End-to-end speedup.} Table~\ref{tab:e2e_speedup} shows the end-to-end latency on \cogvideo, \mochi, and \llamal using \our. Notably, \our achieves 1.83x speedup on \mochi.

 


\begin{table}[h!]
    \centering
    \small
    \vspace{-1em}
    \caption{Overhead of sparse block prediction in \our.}
    \label{tab:pattern_search_overhead}
    \setlength\tabcolsep{2.4pt}
    \begin{tabular}{c|c|c|c}
        \toprule
        \textbf{Sequence Len} & \textbf{Prediction (ms)} & \textbf{Full Attention (ms)} & Overhead \\
        \midrule
        8k  & \textbf{0.251}  & 6.649 &  3.78\% \\
        16k & \textbf{0.487}  & 26.83 & 1.82\% \\
        32k & \textbf{0.972}  & 106.68 & 0.911\%\\
        64k & \textbf{2.599}  & 424.24 & 0.612\%\\
        128k & \textbf{8.764} & 1696.2 & 0.516\%\\
        \bottomrule
    \end{tabular}
    \vspace{-1.2em}
\end{table}


\subsection{Ablation Study and key Insights}


\textbf{Overhead of sparse block prediction.} Table~\ref{tab:pattern_search_overhead} compares the overhead of dynamic sparse block prediction in \our compared with attention execution latency. The results indicate that the prediction overhead is minimal compared to attention, particularly for longer sequences.


\textbf{Effect of Hilbert Curve permutation.} 
We evaluate the impact of Hilbert Curve permutation on \mochi by comparing three metrics: average block similarity across blocks of query or key, L1 error defined in Sec.~\ref{sec:hyper-para}, and \textit{sparsity}.
Table~\ref{tab:permutation} shows that the HilbertCurve permutation consistently achieves superior block self-similarity and sparsity, with only a marginal difference in accuracy.
Please see Appendix~\ref{app:permutation-detail} for more analysis and details.

\begin{table}[!th]
    \centering
    \small
    \vspace{-.5em}
    \caption{Effect of permutation on sparsity and accuracy. Sim-q and Sim-k are the average block self-similarity of the query and key.}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        \textbf{Method} &\textbf{Sim-q} \(\uparrow\) & \textbf{Sim-k} \(\uparrow\) & \textbf{L1} \(\downarrow\) & \textbf{Sparsity} \(\uparrow\) \\
        \midrule
        Random      & 0.321             & 0.019             & 0.0414            &  0.048 \\
        Rowmajor    & 0.551             & 0.390             & \textbf{0.0307}   &  0.363 \\
        % Columnmajor & 0.547             & 0.394             & 0.0342            &  0.366 \\
        Timemajor   & 0.514             & 0.367             & 0.0342            &  0.338 \\
        HilbertCurve& \textbf{0.572}    & \textbf{0.479}    & 0.0389            &  \textbf{0.392}\\
        \bottomrule
    \end{tabular}
    \label{tab:permutation}
    \vspace{-1em}
\end{table}


\begin{table}[!th]
    \centering
    \small
    \vspace{-.5em}
    \caption{Abalation of self-similarity judge.}
    \setlength\tabcolsep{7pt}
    \begin{tabular}{l|c|c|c}
        \toprule
        \textbf{Method} &{\bf VQA-a $\uparrow$}  & {\bf VQA-t $\uparrow$}  & {\bf FScore $\uparrow$} \\
        \midrule
        W/o. self-sim Judge  & 34.664 & 44.722 & 1.138  \\
        With self-sim Judge & 54.179 & 67.219 & 1.807  \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation_self_sim}
    \vspace{-1em}
\end{table}


\begin{table}[!th]
    \centering
    \small
    \vspace{-.5em}
    \caption{Analysis of sparsity from $M_g$ and $M_{pv}$.}
    \setlength\tabcolsep{12.3pt}
    \begin{tabular}{l|c|c|c}
        \toprule
        Strategy & {only $M_g$}  & {only $M_{pv}$}  & {$M_g$ +$M_{pv}$} \\
        \midrule
        Sparsity  & 51.2\%	 & 27.7\% & 54\%  \\
        \bottomrule
    \end{tabular}
    \label{tab:sparsity_of_g_pv}
    \vspace{-1.2em}
\end{table}


\textbf{Ablation of self-similarity judge}\label{sec:judge}
We ablate the effect of the self-similarity judge on \mochi. As shown in Table~\ref{tab:ablation_self_sim}, we find that self-similarity judge can guarantee end-to-end accuracy.
Please see Appendix~\ref{app:self-sim-judge} for more analysis.


\textbf{Analysis of sparsity from $M_g$ and $M_{pv}$.} 
Table~\ref{tab:sparsity_of_g_pv} shows the sparsity when only using $M_g$, only using $M_{pv}$, and using $M_g$+$M_{pv}$ on \llamal in NeedleInAHaystack task with 128K sequence length. 

\textbf{\our enhance the LLM performance.} From Table~\ref{exp:metrics_loss_t2t}, Fig.~\ref{fig:niah_example} and ~\ref{fig:niah_example_appendix}, we observe that \our enhances LLM performance in long-context tasks. This improvement may result from the fact that sparse attention helps the LLM focus on more relevant information.

\begin{table}[h!]
\small
    \centering
    % \vspace{-1.15em}
    \caption{Sparsity increases with sequence length under a constant accuracy bound on \llamal.}
    \label{tab:sparsity_with_seqlen}
        \centering
        \setlength\tabcolsep{5.7pt}
        \begin{tabular}{c|c|c|c|c|c}
            \toprule
            \textbf{Sequence Len} & 8K & 16K & 24K & 48K & 128K  \\
            \midrule
            \textbf{Sparsity} & 6.8\% & 26.4\% & 35.7\% & 49.8\% & 54\% \\
            \bottomrule
        \end{tabular}
    \vspace{-.7em}
\end{table}


\textbf{Sparsity increases with sequence length.} As shown in Table~\ref{tab:sparsity_with_seqlen}, we find that on \llamal, sparsity increases with sequence length. This suggests that the longer contexts, the higher speedup of \our can achieve. \vspace{-.15em}

