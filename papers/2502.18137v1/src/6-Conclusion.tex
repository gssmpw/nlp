\section{Conclusion}  %  and Future Work

In this paper, we propose \our, a universal sparse and quantized attention that executes attention efficiently and accurately for any input. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that \our accelerates diverse models, including language, image, and video generation models, without sacrificing end-to-end metrics.

