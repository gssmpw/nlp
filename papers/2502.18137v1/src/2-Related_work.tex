
\section{Related Work}  \label{sec:related_work}


Depending on how the sparsity mask is constructed, sparse attention methods can be divided into three types:
\textbf{(1) Pattern required methods}
rely on some fixed patterns of the attention map, such as sliding windows or attention sinks~\cite{xiao2023efficient}. H2O~\cite{zhang2023h2o}, InfLLM~\cite{xiao2024infllm}, and DUOAttention~\cite{xiao2024duoattention} rely on sliding window pattern. SampleAttention~\cite{zhu2024sampleattention}, MOA~\cite{moaattention}, and StreamingLLM~\cite{xiao2023efficient} rely on sliding window and attention sink pattern. DitFastAttn~\cite{yuan2024ditfastattn} relies on sliding window patterns and similarities between different attention maps. Moreover, DitFastAttn is restricted to simple diffusion transformers, showing incompatibility with language models and MMDiT models like Flux~\cite{flux}, Stable Diffusion3 and 3.5~\cite{stable_diffusion_3_5}, and CogVideoX~\cite{yang2024cogvideox}.
As the pattern varies across models, these methods may not universally work for different models.
\textbf{(2) Dynamic sparse methods}
 dynamically construct the sparse mask based on the input without the need of preset patterns, and are thus potentially more universal. 
Existing works can be further categorized into channel compression and token compression. Channel compression methods include SparQAttn~\cite{ribar2023sparq} and LokiAttn~\cite{singhania2024loki}. They construct the mask by carrying full attention with reduced dimensionality. However, as the dimension is already small, e.g., 64, 128, in commonly used attention, the speedup potential might be limited. 
Token compression methods include MInference~\cite{jiang2407minference} and FlexPrefill~\cite{FlexPrefill}. They construct the mask by compressing each block of tokens to a single token and compute attention on this shorter sequence. However, this approximation is too aggressive: missing important blocks of $P$ is possible if they do not have a large attention score on the compressed sequence. SeerAttention~\cite{gao2024seerattention} requires training of additional parameters for attention, which is expensive to use. Moreover, they are all designed for language models, and their applicability to other model types, such as diffusion models, remains uncertain.
\textbf{(3) Training-based methods}
 modify the attention computation logic, requiring retraining the entire model, such as Reformer~\cite{kitaev2020reformer} and FastAttention~\cite{pagliardini2023fast}. These methods are much more expensive to use than training-free methods. 

There are other ways to accelerate attention, such as optimizing the kernel implementation~\cite{dao2022flashattention,dao2023flashattention,shah2024flashattention}, quantization~\cite{2024sageattention}, distributing the workload~\cite{liu2023ringattentionblockwisetransformers}, and designing linear time attention~\cite{wang2020linformer,choromanski2020rethinking,yu2022metaformer,katharopoulos2020transformers}. They are orthogonal to our approach. 
