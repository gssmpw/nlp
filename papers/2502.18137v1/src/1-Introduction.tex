
\section{Introduction}  \label{sec:intro} 
As sequence lengths in large models become longer, such as 45K-128K in video generation and language models~\cite{yang2024cogvideox, bao2024vidu, llama31model}, the time consuming of attention occupies a significant portion of inference latency in large models~\cite{2024sageattention}. 
Fortunately, the attention map $P = \mathrm{Softmax}(QK^\top / \sqrt{d}) $ exhibits inherent sparsity, as the softmax operation often creates many values approaching zero~\cite{deng2024attention}. \emph{Sparse attention methods} exploit such sparsity to accelerate attention by (1) constructing a ``sparse mask'', which indicates the important non-zero entries of the attention map $P$ that should be computed, and (2) computing attention only for the parts corresponding to the \emph{sparse mask}. There are three distinct categories of sparse attention methods based on how the sparse mask is generated. \emph{pattern-based method}~\cite{zhang2023h2o, xiao2024infllm, moaattention, zhu2024sampleattention, xiao2024duoattention, xiao2023efficient} relies on specific sparsity patterns based on empirical observations, \emph{dynamic sparse attention}~\cite{ribar2023sparq,singhania2024loki,jiang2407minference,FlexPrefill,gao2024seerattention} computes the mask on-the-fly based on the inputs, and \emph{training-based method}~\cite{kitaev2020reformer,pagliardini2023fast} directly train models with native sparse attention.
\begin{figure}[!t]
    \centering
    \includegraphics[width=.49\textwidth]{figs/cover.pdf}
    \vspace{-2em}
    \caption{\our can achieve 1.83x speedup on \mochi on L40 GPU, with no video quality loss.}
    \vspace{-1.64em}
    \label{fig:cover}
\end{figure}

\textbf{Limitation}. \uline{(L1. Universality)} Though existing sparse attention methods already demonstrate promising speedup on some tasks, their universality is still limited. Existing works are typically developed for specific tasks, such as language modeling, utilizing task-specific patterns such as sliding windows or attention sinks. However, the attention pattern varies significantly across tasks (see examples in Fig.~\ref{fig:heatmap}), making these patterns hard to generalize. \uline{(L2. Usability)} Moreover, it is difficult to implement both \emph{accurate} and \emph{efficient} sparse attention for any input. This is because \emph{accuracy} demands precise prediction of the sparse regions in the attention map, while \emph{efficiency} requires the overhead of this prediction to be minimal. However, current methods are difficult to effectively satisfy both of the requirements simultaneously. For example, MInference~\cite{jiang2407minference} requires a large sequence length, such as 100K, to achieve a noticeable speedup.




\textbf{Goal.} We aim to design a training-free sparse attention operator that accelerates all models without metrics loss.

\textbf{Our approach.} In this work, we develop \our, a \emph{training-free} sparse attention that can be adopted \emph{universally} on various tasks, including language modeling and text-to-image/video, and various sequence lengths. 
We propose three main techniques to improve the universality, accuracy, and efficiency. 
First, we propose a universal sparse mask prediction algorithm, which constructs the sparse mask by compressing each block of $Q$, $K$ to a single token.
Importantly, we compress \emph{selectively} based on the \emph{similarity} of tokens within the block, so the algorithm can accurately predict sparse masks universally across tasks.
Second, we propose a sparse online softmax algorithm at the GPU warp level, which further omits some $PV$ products by leveraging the difference between global maximum values and local maximum values in online softmax.
Third, we integrate this sparse approach into the 8-bit quantized SageAttention framework for further acceleration. 


\textbf{Result.} We evaluate \our on a variety of generative tasks, including language modeling and text-to-image/video, with comprehensive performance metrics on the model quality. 
\our can robustly retain model end-to-end performance while existing sparse attention baselines incur degradation. Moreover, \our is 2.5x to 5x faster than existing dense and sparse attention models. 



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/heatmaps.pdf}
    \vspace{-.25em}
    \caption{Some sampled patterns of attention map $P$ in video, image, and language generation models.}
    \vspace{-1.8em}
    \label{fig:heatmap}
\end{figure}

