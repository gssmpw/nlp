\section{Appendix}


\subsection{Detailed Explain and results of permutation ablation}\label{app:permutation-detail}
We use five distinct prompts and pre-searched hyperparameters with $l_1=0.05, l_2=0.06$ on both \cogvideo and \mochi models.
The permutation are performed separately in attention operation for $Q, K, V$ after position embedding. To retain the original order of the input sequence, an inverse permutation is performed on the output of attention; for models using visual-language joint self-attention(e.g., \cogvideo), we only permute the visual tokens.
When evaluating block self-similarity, we choose a block size of $128$ for query and $64$ for key, which aligns with our kernel implementation. The precision metric(L1) is evaluated using FlashAttention2 output as ground truth.

We choose different permutation methods to compare their impact on the performance of attention operations. Given a 3D visual token tensor with shape $T \times H \times W \times d $, the permutation finally results in a tensor with shape $L \times d $, where $L = T \times H \times W$. 
The permutation methods and their detailed descriptions are shown in Table~\ref{tab:permute-explain}.

\begin{table}[!th]
    \centering
    \small
    \begin{tabular}{l|l}
        \toprule
        \textbf{Method} & \textbf{Detailed Description} \\
        \midrule
        Random      & Random permutation of tokens, the order is recorded to perform inverse permutation. \\
        Rowmajor    & Permutation following row-major order. Tokens are continuous along the W dimension. \\
        Columnmajor & Permutation following column-major order. Tokens are continuous along the H dimension. \\
        Timemajor   & Permutation following time-major order. Tokens are continuous along the T dimension. \\
        HilbertCurve& Permutation following a Hilbert curve. \\
        \bottomrule
    \end{tabular}
    \caption{The detailed description of different permutation methods.}
    \label{tab:permute-explain}
\end{table}

Detailed results of permutation ablation for the \cogvideo and \mochi models are presented in Table~\ref{tab:permutation-detail}.
The HilbertCurve permutation consistently achieves superior block self-similarity and sparsity, with only a marginal loss in precision.
This suggests that the HilbertCurve permutation effectively enhances block self-similarity and sparsity.
It is worth noting that the random permutation retains the precision metrics but sacrifices sparsity.
This indicates that our algorithm has the property of dynamically adjusting and robust to complex token sequences.


\begin{table}[!th]
    \centering
    \small
    \begin{tabular}{l|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{Sim-q}\(\uparrow\)} & \multicolumn{2}{c}{\textbf{Sim-k}\(\uparrow\)} & \multicolumn{2}{c}{\textbf{Precision(L1)}\(\downarrow\)} & \multicolumn{2}{c}{\textbf{Sparsity}\(\uparrow\)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                    & \cogvideo      & \mochi        & \cogvideo     & \mochi         & \cogvideo     & \mochi         & \cogvideo     & \mochi \\
        \midrule
        Random      & 0.502          & 0.321        & 0.025         & 0.019         & 0.0348        & 0.0414        & 0.027         & 0.048 \\
        Rowmajor    & 0.676          & 0.551        & 0.435         & 0.390         &\textbf{0.0265}&\textbf{0.0307}& 0.242         & 0.363 \\
        Columnmajor & 0.633          & 0.547        & 0.335         & 0.394         & 0.0274        & 0.0342        & 0.198         & 0.366 \\
        Timemajor   & 0.692          & 0.514        & 0.479         & 0.367         & 0.0294        & 0.0342        & 0.238         & 0.338 \\
        HilbertCurve& \textbf{0.709} &\textbf{0.572}&\textbf{0.523} &\textbf{0.479} & 0.0323        & 0.0389        & \textbf{0.265}& \textbf{0.392} \\
        \bottomrule
    \end{tabular}
    \caption{The impact of permutation on \cogvideo and \mochi models. Sim-q is the block self-similarity of the query, and Sim-k is the block self-similarity of the key.}
    \label{tab:permutation-detail}
\end{table}


\subsection{Ablation Study of Self-Similarity Judge}
\label{app:self-sim-judge}

To investigate the impact of the self-similarity judge on attention performance, we follow the experimental setting outlined in Sec.~\ref{app:permutation-detail} and conduct an ablation study by removing the self-similarity judge. In most cases, the presence of highly localized patterns results in a minimal number of non-self-similar blocks, leading to only minor differences in precision and sparsity when averaging across all tensor cases. To obtain more meaningful and interpretable insights, we specifically analyze cases where the precision difference is statistically significant.

To this end, we apply a threshold-based selection criterion, retaining only those cases where the absolute difference between $L1^{sim-judge}$ (precision error with the self-similarity judge) and 
$L1^{no-judge}$ (precision error without the self-similarity judge) exceeds 0.05. This criterion results in approximately 2\% of the tensor cases being retained for further analysis. We employ precision (L1 error) and sparsity as evaluation metrics to assess the influence of the self-similarity judge on the attention output. The results are summarized in Table~\ref{tab:self-similarity-ablation}.


\begin{table}[!th]
    \centering
    \caption{Impact of the self-similarity judge on the accuracy and sparsity of attention.}
    \label{tab:self-similarity-ablation}
    \begin{tabular}{l|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{w/ judge}} & \multicolumn{2}{c}{\textbf{w/o judge}} & \multicolumn{2}{c}{\textbf{filter w/ judge}}  &  \multicolumn{2}{c}{\textbf{filter w/o judge}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                & \cogvideo   & \mochi        & \cogvideo     & \mochi         & \cogvideo     & \mochi         & \cogvideo     & \mochi \\
        \midrule
        L1 error\(\downarrow\)  &0.0316&0.0343 &0.0325 &0.0365 &0.0843 &0.0555  &0.214  & 0.154 \\
        Sparsity \(\uparrow\)   &0.199 &0.301  &0.203  &0.305  &0.242  &0.371   &0.275  & 0.392 \\
        \bottomrule
    \end{tabular}
\end{table}

The findings demonstrate that the self-similarity judge effectively mitigates extreme precision loss while introducing only a marginal reduction in sparsity. Furthermore, we observe that a significant proportion of cases exhibiting notable differences originate from the Random permutation category in the \cogvideo model. This observation further highlights the role of the self-similarity judge in enhancing the model's robustness to complex token sequences while maintaining high precision.




\begin{figure}[!h]
    \centering
    \includegraphics[width=.55\textwidth]{figs/niah.pdf}
    \vspace{-1.2em}
    \caption{A NeedleInAHaystack comparison example on \llamal. The sparsity of \our, MInference, and FlexPrefill is 0.36, 0.3, and 0.3. }
    \vspace{-.5em}
    \label{fig:niah_example_appendix}
\end{figure}


\begin{figure}[!h]
    \centering
    \includegraphics[width=.69\textwidth]{figs/more_mochi_example.pdf}
    \vspace{-.5em}
    \caption{Comparison examples on \mochi. The sparsity of \our, MInference and FlexPrefill is 0.47, 0.3, and 0.4.}
    \vspace{-.5em}
    \label{fig:visible_video_appendix}
\end{figure}


\begin{table}[!h]
\caption{End-to-end metrics on \llamal in the NeedleInAHaystack task with 16-28K sequence lengths.}
    \label{exp:metrics_loss_t2t_appendix}
    \setlength\tabcolsep{15pt}
    \small
    \begin{center}
    \begin{tabular}{p{1.5cm}|p{2.4cm}|c|c}
    \toprule
    {\mbox{\hspace{-.9em}\textbf{Model} (seq\_len)}}  & \hspace{-1em}\textbf{Attention}~\small{(Sparsity)}  & {\bf Speed (TOPS)$\uparrow$}  & {\bf NIAH $\uparrow$}  \\ \hline

    \multirow{6}{*}{\hspace{-.5em}\makecell[c]{\llamal \\ \small{(24K)}}} & \hspace{-1em}Full-Attention & 156.9 & 0.838  \\  
    & \hspace{-1em}Minference \small{(0.5)} & 122.5  & 0.635  \\
    & \hspace{-1em}FlexPrefill \small{(0.6)} & 179.6 & 0.776 \\
    & \hspace{-1em}Minference \small{(0.3)} & 102.3  &  0.652 \\
    & \hspace{-1em}FlexPrefill \small{(0.3)} & 117.6 & 0.797   \\
    & \mbox{\hspace{-1em}\our \small{(0.36)}} & \textbf{443.6} & \textbf{0.863} \\ \bottomrule
    \end{tabular} 
    \end{center}
\end{table}


\subsection{Additional Experiments}

In this section, we present additional experimental results further to evaluate the performance of \our compared to baselines. Fig.~\ref{fig:niah_example_appendix} and~\ref{exp:metrics_loss_t2t_appendix} show the results on \llamal in the NeedleInAHaystack task with 16-28K sequence length. Fig~\ref{fig:visible_video_appendix} shows a visible comparison example on \mochi.
