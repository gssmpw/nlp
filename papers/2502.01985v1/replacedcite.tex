\section{Related Work}
\label{sec:rw}
% \para{Avoiding joins} As discussed in Sec.~\ref{sec:problem}, joins can lead to data redundancy. Therefore, earlier work has investigated whether joins can be avoided before training ML models ____. Avoiding a join, means training a model on a single base table instead of multiple joinable tables. Hamlet ____ 
% % investigates whether joins can be skipped on data with PK-FK relationships without significantly affecting the performance of ML models. Hamlet 
% showed that for data with PK-FK relationships, we often do not need to join other tables as long as we keep the FKs in the base table. 
% This notion is extended to non-linear models ____, and these models are even more resilient to avoid joins. Another approach for reducing the cost of materialization is to return a random sample of the join, and train ML models on this sample, which was also proven to be an effective method ____. This line of work is orthogonal to this work, since in our problem setting, useful features are scattered in different tables, and combining them is necessary for training ML models.


% \para{Factorized learning} 
\vspace{-2mm}
Early efforts required \emph{manual} design for factorizing specific ML algorithms, with Orion ____ addressing linear and logistic regression. 
This was expanded in Santoku ____ to incorporate decision trees, feature ranking, and Naive Bayes. 
While Orion and Santoku focused on inner joins via PK-FK relationship,  F ____ has extended factorization of linear regression to natural joins. Following F, AC/DC ____ adds support to categorical variables, and LMFAO supports more ML algorithms such as decision trees. JoinBoost ____ integrated tree models like LightGBM and XGBoost.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]
%     % \includegraphics[width=\linewidth]
%     {figures/new_eval_figures/xgb_model_category.pdf}
%     \caption[Model training time over six real datasets: materialization vs. factorization]{
% The predictive quality of our estimator under a "leave-one-out" scenario. Test set only contains the left-out model. For instance, the estimator for KMeans is trained with any datapoints that we have gathered for KMeans.
%     }
%     \label{fig:xgb_acc_category}
%     \vspace{-5mm}
% \end{figure}
 
In a more \emph{automated} manner, 
Morpheus ____ proposed a general factorization framework based on PK-FK and join dependencies. HADAD ____ speeds up the rewriting rules of  Morpheus by reordering of multiplication and exploiting pre-computed results. 
Later, Trinity ____ extends Morpheus with compatibility to 
% such that factorized LA logic is written \textit{once}, but can be reused in 
multiple programming languages and LA tools. 
MorpheusFI ____ added support to non-linear feature interactions. Non-linear models such as 
Gaussian Mixture Models and Neural Networks are studied in ____.

% Compared to the above existing works, 
Ilargi's main contribution is not on extending ML algorithms. We expand the usability of factorized learning by a GPU-compatible representation. We take a more critical inspection of the speedup of factorization compared to materialization by utilizing DI metadata. We also discovered a third factor, hardware configuration, which has an impact on the decision boundary. We are currently expanding Ilargi to more ML models such as tree models.

% \parab{Federated learning}  presents a vast and intricate landscape of research challenges for collaboratively training ML models over silos ____. 
% % We pinpoint our problem scope and its prospects. 
% Recent advances in federated learning has a heavy focus on image data ____. In this work, we focus on tabular data, which has many unexplored challenges. 
% Existing horizontal federated learning works over tabular data often an oversimplified setting of identical schema between sources ____, while vertical federated learning works assume the datasets are row-wise aligned ____. This work provides general data integration scenarios and formalisms to broaden the data-wise problem scope and applicability. Our approach in Sec.~\ref{sec:system} can serve as a fundamental component of a federated learning framework for seamlessly incorporating DI metadata for linear algebra computation. 

 % The problems of training ML model over decentralized datasets, are coined as \emph{federated learning}, or \emph{distributed learning}.

% \para{Distributed machine learning} covers many aspects such as distributed storage of training data or distributed operation of computing tasks. We mainly cover schems relevant to this work: data parallelism and model parallelism. Data parallelism pushes the ML model to data nodes but requires these data subsets to be vertically separable. Our approach for factorization is a data parallelism for horizontal data slices. 
 
% Existing works ____ 
% % discover that it is possible to avoid joins while still including attributes from multiple tables in an ML model through 
% on factorized learning, and manually rewrite individual ML models to their factorized versions. Morpheus and it successors ____ provide a general, automated framework of LA rewrite rules for linear models. We have intensively compared our approach with the state-of-the-art approach {Morpheus}____. 
% % While the previous systems were created through manual factorization, 
% Ilargi is applicable in more general data integration scenarios for ML use cases than Morpheus. 
% % % \para{Cost Estimation} 
% % Previous work identified that factorized ML yields speedups. However, this is not always the case. 
% % ____ identified the need for a cost-model to estimate when factorized learning does not yield speedups. 
% Moreover, Morpheus ____ proposed a cost estimation using heuristics based on the dimensions of the source tables. In the Python extension of Morpheus ____, although sparsity is mentioned as a factor for arithmetic calculation overhead, it is not included in cost estimation as in our approach. 
% the authors highlight the importance of the  of their tables. The authors of ____ conclude that the higher the sparsity of the tables, the more overhead there is compared to the number of arithmetic calculations. However, sparsity was not incorporated into a cost model. 



% \para{Tensor-based ML and data management systems}
% A large body of work has studied the unification of linear and relational algebra operations at the level of the execution engine \____.  SystemDS____ models data as tensors, and performs optimizations for end-to-end ML pipelines. TQP____ executes relational operators such as joins and projections in tensor algebra, while multiple works leverage GPUs for relational joins____. 
% Hummingbird ____ compiles traditional ML models into tensor-based runtimes designed for deep learning models. Ilargi is complementary to these approaches as its input source datasets also come in matrix form. The key novelty of Ilargi lays in the modeling of data integration metadata (e.g., schema mappings traditionally expressed in first-order logic) in a matrix representation. 



% \textcolor{red}{
% ____
% ____
% ____
% }