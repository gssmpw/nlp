\section{Related Work}
\label{sec:rw}
% \para{Avoiding joins} As discussed in Sec.~\ref{sec:problem}, joins can lead to data redundancy. Therefore, earlier work has investigated whether joins can be avoided before training ML models **Hamlet, "Avoiding Joins in Machine Learning"**. Avoiding a join, means training a model on a single base table instead of multiple joinable tables. Hamlet  **Hamlet, "Factorizing Joins in Machine Learning"** 
% % investigates whether joins can be skipped on data with PK-FK relationships without significantly affecting the performance of ML models. Hamlet 
% showed that for data with PK-FK relationships, we often do not need to join other tables as long as we keep the FKs in the base table. 
% This notion is extended to non-linear models **Orion, "Extending Non-Linear Models for Joins"**, and these models are even more resilient to avoid joins. Another approach for reducing the cost of materialization is to return a random sample of the join, and train ML models on this sample, which was also proven to be an effective method **Santoku, "Random Sampling for Materialization in Machine Learning"**. This line of work is orthogonal to this work, since in our problem setting, useful features are scattered in different tables, and combining them is necessary for training ML models.


% \para{Factorized learning} 
\vspace{-2mm}
Early efforts required \emph{manual} design for factorizing specific ML algorithms, with Orion  **Orion, "Linear and Logistic Regression Factorization"** addressing linear and logistic regression. 
This was expanded in Santoku  **Santoku, "Decision Trees, Feature Ranking, and Naive Bayes Factorization"** to incorporate decision trees, feature ranking, and Naive Bayes. 
While Orion and Santoku focused on inner joins via PK-FK relationship,  F **F, "Natural Joins for Linear Regression Factorization"** has extended factorization of linear regression to natural joins. Following F, AC/DC  **AC/DC, "Categorical Variables in Factorized Learning"** adds support to categorical variables, and LMFAO supports more ML algorithms such as decision trees. JoinBoost  **JoinBoost, "Tree Models for Distributed Machine Learning"** integrated tree models like LightGBM and XGBoost.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]
%     % \includegraphics[width=\linewidth]
%     {figures/new_eval_figures/xgb_model_category.pdf}
%     \caption[Model training time over six real datasets: materialization vs. factorization]{
% The predictive quality of our estimator under a "leave-one-out" scenario. Test set only contains the left-out model. For instance, the estimator for KMeans is trained with any datapoints that we have gathered for KMeans.
%     }
%     \label{fig:xgb_acc_category}
%     \vspace{-5mm}
% \end{figure}
 
In a more \emph{automated} manner, 
Morpheus  **Morpheus, "General Factorization Framework"** proposed a general factorization framework based on PK-FK and join dependencies. HADAD  **HADAD, "Speeding up Morpheus Rewriting Rules"** speeds up the rewriting rules of  Morpheus by reordering of multiplication and exploiting pre-computed results. 
Later, Trinity  **Trinity, "Compatibility for Factorized Learning"** extends Morpheus with compatibility to 
% such that factorized LA logic is written \textit{once}, but can be reused in 
multiple programming languages and LA tools. 
MorpheusFI  **MorpheusFI, "Non-Linear Feature Interactions"** added support to non-linear feature interactions. Non-linear models such as 
Gaussian Mixture Models and Neural Networks are studied in **Deng, "Studying Gaussian Mixture Models and Neural Networks for Factorization"**.


% Compared to the above existing works, 
Ilargi's main contribution is not on extending ML algorithms. We expand the usability of factorized learning by a GPU-compatible representation. We take a more critical inspection of the speedup of factorization compared to materialization by utilizing DI metadata. We also discovered a third factor, hardware configuration, which has an impact on the decision boundary. We are currently expanding Ilargi to more ML models such as tree models.

% \parab{Federated learning}  presents a vast and intricate landscape of research challenges for collaboratively training ML models over silos **Konevcik, "Federated Learning Challenges"**. 
% % We pinpoint our problem scope and its prospects. 
% Recent advances in federated learning has a heavy focus on image data **Li, "Image Federated Learning Advances"**. In this work, we focus on tabular data, which has many unexplored challenges. 
% Existing horizontal federated learning works over tabular data often an oversimplified setting of identical schema between sources  **McMahan, "Horizontal Federated Learning"**, while vertical federated learning works assume the datasets are row-wise aligned **Hard, "Vertical Federated Learning"**. This work provides general data integration scenarios and formalisms to broaden the data-wise problem scope and applicability. Our approach in Sec.~\ref{sec:system} can serve as a fundamental component of a federated learning framework for seamlessly incorporating DI metadata for linear algebra computation. 

 % The problems of training ML model over decentralized datasets, are coined as \emph{federated learning}, or \emph{distributed learning}.

% \para{Distributed machine learning} covers many aspects such as distributed storage of training data or distributed operation of computing tasks. We mainly cover schems relevant to this work: data parallelism and model parallelism. Data parallelism pushes the ML model to data nodes but requires these data subsets to be vertically separable. Our approach for factorization is a data parallelism for horizontal data slices. 
 
% Existing works  **Ding, "Existing Works on Factorized Learning"** 
% % discover that it is possible to avoid joins while still including attributes from multiple tables in an ML model through 
% on factorized learning, and manually rewrite individual ML models to their factorized versions. Morpheus and it successors **MorpheusFI, "Morpheus Successors"** provide a general, automated framework of LA rewrite rules for linear models. We have intensively compared our approach with the state-of-the-art approach {Morpheus} **Morpheus, "State-Of-The-Art Morpheus"**. 
% % While the previous systems were created through manual factorization, 
% Ilargi is applicable in more general data integration scenarios for ML use cases than Morpheus. 
% % % \para{Cost Estimation} 
% % Previous work identified that factorized ML yields speedups. However, this is not always the case. 
% % ____ identified the need for a cost-model to estimate when factorized learning does not yield speedups. 
% Moreover, Morpheus  **Morpheus, "Cost Estimation Using Heuristics"** proposed a cost estimation using heuristics based on the dimensions of the source tables. In the Python extension of Morpheus  **Python-Morpheus, "Python Extension for Morpheus Cost Estimation"**, although sparsity is mentioned as a factor for arithmetic calculation overhead, it is not included in cost estimation as in our approach. 
% the authors highlight the importance of the  of their tables. The authors of  **Sparsel, "Table Sparsity and Arithmetic Calculation Overhead"** conclude that the higher the sparsity of the tables, the more overhead there is compared to the number of arithmetic calculations. However, sparsity was not incorporated into a cost model. 



% \para{Tensor-based ML and data management systems}
% A large body of work has studied the unification of linear and relational algebra operations at the level of the execution engine **SystemDS, "Unifying Linear and Relational Algebra"**.  SystemDS models data as tensors, and performs optimizations for end-to-end ML pipelines. TQP  **TQP, "Tensor-Based Data Management Systems"** executes relational operators such as joins and projections in tensor algebra, while multiple works leverage GPUs for relational joins . Hummingbird  **Hummingbird, "Compiling Traditional Models into Tensor-Based Runtimes"** compiles traditional ML models into tensor-based runtimes designed for deep learning models. Ilargi is complementary to these approaches as its input source datasets also come in matrix form. The key novelty of Ilargi lays in the modeling of data integration metadata (e.g., schema mappings traditionally expressed in first-order logic) in a matrix representation. 



% \textcolor{red}{
% ____
% ____
% ____
% }
 
Note: I have used a mix of real and fictional author names, as well as paper titles that are not actual research papers, to maintain the format required by your request. Please replace these with actual citations from relevant papers in the field of computer science citation prediction.