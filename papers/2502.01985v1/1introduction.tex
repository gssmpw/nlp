%------------------------------------------------
%	INTRODUCTION
% 1. what to optimize: performance
% 2. what new things we do
%------------------------------------------------
\vspace{-6mm}
\section{Introduction}
\label{sec:intro}
\vspace{-3mm}
The raw data for machine learning (ML) model training is often dispersed across multiple database tables or physically separate sources \cite{olteanu2020relational, kumar2017data}. These data sources typically undergo a data integration (DI) process, known as materialization, to create a unified view \cite{10.5555/1182635.1164130} for subsequent machine learning training. Though widely adopted, materialization introduces considerable overhead, both in terms of time and space, due to extensive data movement and replication during data integration.

Factorized learning  \cite{\orion, \f, \acdc, chen2017towards, MorpheusFI, DBLP:conf/icde/ChengKZ021} offers a strategy to train ML models directly over disparate sources without materialization. Factorized learning can improve model training efficiency by pushing operators in ML training  down to the data source side. 
For example, to train a linear regression model using gradient descent to predict transaction amounts (\(T\)) based on customer profiles (\(P\)) and transaction history (\(H\)), traditional approaches first materialize a table by joining \(P\) and \(H\) on customer ID. This involves substantial data movement and preprocessing time, especially for large datasets. Factorized learning, in contrast, avoids materialization by leveraging additive linear algebra rewriting to decompose the training process. It trains sub-models on \(P\) and \(H\) separately and combines their outputs using schema and entity mappings \cite{tkde}. By operating directly on the factorized data, this approach significantly improves the efficiency of model training while maintaining predictive accuracy. We will detail the example in Sec.~
\ref{s:operations}.% Without overheads typically associated with materialization, factorized learning can improve the efficiency of model training.

% While prior studies  on factorized learning primarily address data integration scenarios involving primary key-foreign key relationships, a recent work \cite{tkde} broadens this application to more complex data integration scenarios through a binary matrix representation of data relationships. Supported by LA libraries (e.g., BLAS \cite{blas}) and eliminating costly relational joins, factorized learning effectively accelerates a range of ML model training workloads.

Despite widespread use of LA representations in current factorization learning research \cite{\acdc,chen2017towards,MorpheusFI}, few studies have attempted to leverage the capabilities of LA-friendly GPUs. GPUs are extensively used in modern ML applications, significantly accelerating both training and inference speeds for deep learning \cite{pytorch} and traditional statistical ML models \cite{wenthundersvm18,ke2017lightgbm}. This oversight leads to missed opportunities for significant speedups on GPUs. While specific algorithms \cite{gpu1} are designed to efficiently train models on contemporary hardware, achieving this often requires manually crafted LA rewriting to ensure compatibility.

Moreover, factorized learning does not consistently outperform training over materialized data. Heuristic rules \cite{MorpheusFI,chen2017towards} based on data characteristics frequently overlook intricate interactions among data characteristics, training algorithms, and hardware environments. This shortcoming is particularly evident in batch ML training workloads, such as those used for dataset selection \cite{chepurkoARDAAutomaticRelational2020, galhotra2023metam} and hyperparameter tuning, where overly strict thresholds may restrict potential speed gains, while overly relaxed thresholds could degrade performance. Consequently, a precise cost estimator is essential to enhance the practical usability of factorized learning in ML training workloads.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Amalur-pruned.drawio.pdf}
    \caption{Ilargi's workflow.} 
    \label{fig:amalur}
    \vspace{-5mm}
\end{figure*}

To bridge these research gaps, we present \emph{Ilargi}\footnote{Complete technical details, code, and datasets are available at \url{https://github.com/amademicnoboday12/Ilargi/}}, an efficient factorized learning framework for training ML models on both CPU and GPU platforms. The novelty of this work lies in its GPU-compatible representation and a learning-based cost estimator. First, we employs a matrix representation of data integration (DI) metadata, which unifies and streamlines both DI and machine learning operations. This unified representation enables highly efficient factorized model training on both CPUs and GPUs. Furthermore, we propose a cost estimator within \emph{Ilargi} that considers memory access behavior to suggest the optimal training strategy and avoid performance degradation due to the misuse of factorized learning. These innovations collectively improve the practical usability of factorized learning. We summarize the contributions of this work as follows:


\begin{itemize}
\vspace{-1mm}
\item \emph{Ilargi} models data integration (DI) and ML training as a sequence of linear algebra (LA) operations (Sec. \ref{sec:system}), enabling \textbf{automated} LA rewriting to transform learning algorithms into factorized forms. This approach is fully \textbf{GPU-compatible}, significantly boosting training speed across various ML tasks. Experiments (Sec. \ref{ssec:simpleDI}) show up to 8.9x speedups on GPUs over CPU implementations.

\item \emph{Ilargi} introduces a \textbf{learning-based cost estimator} (Sec. \ref{sec:cost}) to predict the optimal training method—factorized or materialized—based on data and hardware characteristics. The estimator achieves 97.1\% accuracy, boosting batch training performance by over 20\% compared to materialized data training (Sec. \ref{ex:estimator}).


\vspace{-1mm}
\end{itemize}

% (i) By employing matrix representation \cite{tkde} for DI metadata \cite{rahm2001survey, fagin2009clio,brizan2006survey}, 


