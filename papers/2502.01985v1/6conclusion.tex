%------------------------------------------------
%	CONCLUSION
%------------------------------------------------
\vspace{-4mm}
\section{Conclusion}
\label{sec:con}
\vspace{-2mm}
In this paper, we have proposed \emph{Ilargi}, an approach that leverages matrix-represented DI metadata to enable GPU-compatible factorization of model training across disparate data sources. With the unified LA representation, Ilargi efficiently trains models using factorization on both CPUs and GPUs. Moreover, we introduced a tree-boosting estimator to navigate the complex decision-making process between materialization and factorization, combining data characteristics, ML model training complexity, and hardware configuration.
Our experimental results on both synthetic and real-world data demonstrate that our GPU-compatible factorized model training can achieve speedups of up to 8.9x, attributing to the high parallelism of GPUs. 
% Furthermore, 
With our estimator, the batch model training workload can be accelerated by up to 24\% on both CPUs and GPUs consistently, demonstrating the benefits of incorporating hardware specifications into our estimator.



% In this paper, we address the topic of training machine
% learning models over data sources. We introduce a formal
% framework based on tgds, which provides a unified formalism
% for existing factorization techniques in single databases and
% our scenario, i.e., disparate data sources. Such a unified
% formalism unlocks the possibility of using more logic-based
% formalisms, or more types of data integration metadata to
% streamline ML model training over sources. Ilargi leverages
% matrices to encode schema mapping and row matching, en-
% abling factorization over sources. Moreover, Ilargi utilizes
% schema mappings as pruning rules, and employs a tree boost-
% ing based estimator, which leverages complexity ratios and
% hardware configuration to choose between materialization and
% factorization. Our experiments demonstrate the effectiveness
% and efficiency of our estimator, which covers more scenarios
% than state-of-the-art solutions and performs correct predictions
% with 70-97% accuracy depending on the dataset.



% Moreover, the cost estimator in this paper depends heavily on synthetic data, which may limit its practical applicability. To address this, we are developing a hybrid cost model that integrates both analytical and statistical submodels. The analytical component can be deployed immediately without prior training, although it typically underperforms compared to statistical models. Meanwhile, \emph{Ilargi} continuously collects runtime data to enhance the statistical model, progressively improving accuracy and reliability of the cost estimator.
% Meanwhile, we will develop more ML models with factorization to further improve the usability of our research.




% \para{Outlook} \wenbo{can we become a small lib or add-on?} 
% Our proposed cost model is related to data, model, and hardware. Similarly, future work can also be extended in these three dimensions.
% \\ \para{Data}
%  There are a few minor yet practical extensions of our current approaches. First, the mapping matrix generation might become more complicated when the mapped attribute is categorical. When we apply one-hot-encodings to be able to run ML models on our data, one categorical column turns into multiple columns. The challenge is to keep the mapping matrices consistent with such a change. Second, special attention should also be given to null values. In this work, we turned any null values into zeroes. Future work might include other methods of handling null values.
% \rihan{C: more DI opportunities?}

% \\ \para{Model} Furthermore, there are more possibilities to optimize Amalur. 
% In this work, we obtain speedups using our implementation based on sparse matrices. Firstly, the structure of the factorized rewrites can be exploited through parallelization \cite{benson2015framework}. By computing partial results for each source table concurrently, the computation times of aggregation and multiplication operations, which are prevalent in ML models, can be reduced. Our cost model can be adapted to reflect such an implementation. Second, our matrix ordering optimization only considers the dimensions of the matrices, while for sparse matrices, the sparsity of each of the matrices should be considered. This requires finding an estimation for the numeric sparse matrix chain product ordering problem \cite{naumann2020sparse}. Third, as previously identified by the authors of Morpheus\cite{\morpheus}, the main bottleneck in the implementation of ML factorization are matrix-matrix multiplications. Options to combat this bottleneck are solutions such as DeepMind's AlphaTensor\footnote{AlphaTensor: \url{https://github.com/deepmind/alphatensor}}, CuPy \footnote{CuPy: \url{https://cupy.dev/}}, or direct implementation in a more low-level programming language such as C++. Lastly, parts of the computations may benefit from a caching mechanism.

% \\ \para{Hardware}
% The data dependencies could also be used for parallelism, e.g., GPU.
% % Numerical Sparse Matrix Chain Productis NP-complete https://epubs.siam.org/doi/epdf/10.1137/1.9781611976229.12

% % Handling of zeroes
% % Data integration to mapping matrix for categorical variables

 

% \rihan{C
% 0: Limits
% 1. beyond performance, what to consider for VDI or materialization
% 2. what our approach can be used: federated learning, etc
% 3. extend to soft/fuzzy joins
% }
\vspace{-3mm}