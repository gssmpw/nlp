%------------------------------------------------
%	COST ESTIMATION
%------------------------------------------------
\vspace{-4mm}
\section{Cost Estimation in Ilargi}
\label{sec:cost}
\vspace{-2mm}
Reducing ML model training time through factorization is not always guaranteed. When the target table has low redundancy compared to source tables, factorization may require more linear algebra computations, leading to longer training times than materializing the target table \cite{chen2017towards}. Existing studies \cite{chen2017towards, MorpheusFI} use empirical cost models to set sparsity-based thresholds for selecting between factorization and materialization, yet these thresholds vary with hardware.  However, in environments with both CPUs and GPUs, our experiments (Sec. \ref{sec:eva}) show that target table sparsity does not consistently correlate with factorization speedups, and identical sparsity levels can yield different speedup outcomes on different hardware platforms.

This discovery reveals the intricacy of selecting the optimal training method between factorization and materialization. Consequently, it is essential to develop a cost model that considers both algorithmic characteristics and hardware properties. To build this cost model, we begin with a complexity analysis of model training. This analysis enables a comparative evaluation of the complexities associated with materialization and factorization.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/new_eval_figures/prelim_spasity.pdf}
%     \caption{The average speedups, expressed as $\frac{T_{materialization}}{T_{factorization}}$, vary on CPUs and GPUs depending on the sparsity of the target table. On CPUs, KMeans exhibits higher speedups than GaussianNMF, while the reverse is true on GPUs. The detailed setting are in explained Sec. \ref{sec:eva}.}
%     \label{fig:sparsity_models}
%     \vspace{-2mm}
% \end{figure}

% \input{logicalrules}



\vspace{-4mm}
\subsection{Complexity Analysis of ML Model Training Algorithms}
\label{s:complexity}
\vspace{-2mm}
We compare the computational complexity of linear algebra (LA) operations for both materialization and factorization. For materialization, the cost is based on performing LA operations directly on the materialized table. In contrast, factorization considers the operations on each source table individually.

% Below, we continue to use LMM for the explanation.

% \para{Complexity analysis in state-of-the-art} 
% In \cite{chen2017towards}, a straightforward complexity analysis is provided. For each LA operator, the decision of factorization and materialization is based on the arithmetic computation complexity, i.e., the size comparison between joinable tables and materialized table. 
% For instance, given    a matrix $T$ with the shape $r_{T} \times c_{T}$, and a matrix $X$ with the shape $c_{T} \cdot c_{X}$ ($r_{X} = c_{T}$), 
% the complexity of their left matrix multiplication $TX$ is $c_{X} \cdot c_{T} \cdot r_{T}$. Thus, in \cite{chen2017towards} the complexity of computing $TX$ for materialization is $c_{X} \cdot c_{T} \cdot r_{T}$, while for factorization is $\displaystyle \sum_{k=1}^n c_X \cdot c_k \cdot r_k$.   

% \para{Sparsity of mapping \& indicator matrices} 
% As discussed in Sec.~\ref{sec:matrixGen}, our mapping and indicator matrices are sparse. The matrix form of the source and target tables can be sparse or dense. A \emph{sparse matrix} is a matrix whose number of nonzero elements is negligible compared to the number of zeros, while a \emph{dense matrix} has more non-zero elements than zeros \cite{yuster2005fast}. 

% %-----------sparsity explained-------------------------------------------
% The null values come from the sources or from the join. 
% We inspect what sparsity looks like in Fig.~\ref{fig:example}. Here, the sparsity of table  $S_1$ Prescriptions, denoted $p_\text{Prescriptions}$, equals 6, as there are six cells with zeroes in this table. The sparsities of tables $E$, $R_1$, and $T$ are $p_E = 6$, $p_{R_1} = 2$ and $p_T = 11$.  Sparsity resulting from the data happens when data explicitly contains zero values. In our implementation, these explicit zeros are not stored in the sparse matrices. Sparsity resulting from the join occurs in the target table. These missing values are also not stored in the sparse materialized table. As missing values and explicit zeroes are not stored, the computations we perform on sparse matrices involve less operations than if we were to perform them on a dense matrix of the same size. Therefore, sparsity should be taken into account when performing cost estimation. The upcoming section details how sparsity is included in the cost estimation procedure.
% %-----------------------------------------------------------------------

% \begin{table}[t]
% \caption{LA operator computations cost comparison}
% % : materialization vs. factorization.}
% \label{tab:complexity}
% \centering
%  % \scriptsize
%   \small
% \begin{tabular}{llll}
% \toprule
% \textbf{Operation} & \textbf{Materialization}                  & \textbf{Factorization}                  \\ \midrule
% % $T \oslash x$  & \multirow{8}{*}{$\displaystyle m_T$} & \multirow{8}{*}{$\displaystyle \sum_{k=1}^n m_k$} \\
% % $T^T \oslash x$             &                                    &                                      \\
% %  $f(T)$                  &                                    &                                      \\
% % $f(T^T)$               &                                    &                                      \\
% % $\text{rowSum}(T)$       &                                    &                                      \\
% % $\text{rowSum}(T^T)$       &                                    &                                      \\
% % $\text{colSum}(T)$       &                                    &                                      \\
% % $\text{colSum}(T^T)$       &                                    &                                      \\ \hline

% $TX$               & $\displaystyle c_X \cdot m_T + r_T \cdot m_X$              & $\displaystyle \sum_{k=1}^n c_X \cdot m_k + r_k \cdot m_X$            \\
% $T^TX$               & $\displaystyle   c_X \cdot m_T + c_T \cdot m_X$              & $\displaystyle \sum_{k=1}^n    c_X \cdot m_k  + c_k \cdot m_X $          \\
% $XT$               & $\displaystyle  r_X \cdot m_T + c_T \cdot m_X $              & $\displaystyle \sum_{k=1}^n   r_X \cdot m_k + c_k \cdot m_X$      \\
% $XT^T$               & $\displaystyle r_X + m_T + r_T \cdot m_X $              & $\displaystyle \sum_{k=1}^n r_X + m_k + r_k \cdot m_X$            \\
% \bottomrule
% \end{tabular}

% \vspace{-0.4cm}
% \end{table}

\begin{table}[t]
\caption{Computations complexity comparison of common LA operators.}
% : materialization vs. factorization.}
\label{tab:complexity}
\vspace{1mm}
\centering
 % \scriptsize
  \small
\begin{tabular}{l|c|c}
\toprule
\textbf{Operation} & \textbf{Materialization}                  & \textbf{Factorization}                  \\ \midrule
$T \oslash x$  & \multirow{4}{*}{$\displaystyle m_T$} & \multirow{4}{*}{$\displaystyle \sum_{k=1}^n m_k$} \\
 $f(T)$                  &                                    &                                      \\
$\text{rowSum}(T)$       &                                    &                                      \\
$\text{colSum}(T)$       &                                    &                                      \\ \hline

$TX$               & $\displaystyle c_X \cdot m_T + r_T \cdot m_X$              & $\displaystyle \sum_{k=1}^n c_X \cdot m_k + r_k \cdot m_X$            \\
$XT$               & $\displaystyle  r_X \cdot m_T + c_T \cdot m_X $              & $\displaystyle \sum_{k=1}^n   r_X \cdot m_k + c_k \cdot m_X$      \\
\bottomrule
\end{tabular}
\vspace{-5mm}
\end{table}

% \para{Complexity analysis in Ilargi} 
Tab.~\ref{tab:complexity} summarizes the comparison. Ilargi stores matrices in sparse format, so the complexity of element-wise operations, rowSum, and colSum depends on the number of non-zero elements.

For sparse matrix multiplication, given two matrices \( T \) and \( X \), the complexity of \( TX \) is \( O(c_X \cdot m_T + r_T \cdot m_X) \), where \( m_T \) and \( m_X \) are the non-zero elements of \( T \) and \( X \) \cite{horowitz1982fundamentals}. Since our mapping and indicator matrices are sparse, we calculate matrix operation costs accordingly. With the rewriting rule (Sec.~\ref{s:operations}), the complexity of factorized multiplication is \( \sum_{k=1}^n (c_X \cdot m_k + r_k \cdot m_X) \).

The complexity of ML models depends on the specific LA operators used. We detail linear regression analysis here, with further models discussed in our technical report \cite{tech}.

% For sparse matrix operators, consider two matrices $T$ and $X$. A multiplication $TX$ over sparse format is $O(c_X \cdot m_T + r_T \cdot m_X)$, where $m_T,\ m_X$ are the numbers of nonzero elements in matrix $T$ and $X$ \cite{horowitz1982fundamentals}. 
% Since our mapping and indicator matrices are sparse, we compute the computation cost of matrix operators as in Tab.~\ref{tab:complexity}. With the rewriting rule we discussed in Sec, the computing complexity of multiplication over factorized data is the summation of complexity of multiplications over sources. As such, we have the computing complexity of factorized multiplication as $\sum_{k=1}^n c_X \cdot m_k + r_k \cdot m_X$
% As discussed in Sec.~\ref{sec:matrixGen}, our mapping and indicator matrices are sparse. Therefore, we compute the computation cost of matrix operators as in Table \ref{tab:complexity}. 
%----------------------------------------------------
% We also note the complexity of transposed operations. As described in Section \ref{s:operations}, $T^TX$ is actually defined as $(X^T T)^T$ and $XT^T$ is actually defined as $(TX^T)^T$. Here, we ignore the complexity added by the transposes, as these are not executed on data matrices.  
%----------------------------------------------------
% Here the differentiation between sparse and dense matrices is more about an intuition rather than a rigorous measurement~\cite{horowitz1982fundamentals}. 
%----------------------------------------------------
% Take the example of $TX$ for materialization, and we have $m_T \leq r_T \cdot c_T$. When the target table matrix is dense, the non-zero element number $m_T$ is close to the size of $T$, i.e., $r_T \cdot c_T$. The time for multiplying $T$ and $X$ is at most $O(r_T \cdot c_T \cdot c_X)$. 


% \begin{table}[t]
% \centering
% \scriptsize
% \begin{tabular}{lll}
% \toprule
% \textbf{Operation} & \textbf{Materialization}                  & \textbf{Factorization}                  \\ \midrule
% $T \oslash x$  & \multirow{8}{*}{$\displaystyle nnz(T)$} & \multirow{8}{*}{$\displaystyle \sum_{k=1}^K nnz(S_k)$} \\
% $T^T \oslash x$             &                                    &                                      \\
% $f(T)$               &                                    &                                      \\
% $f(T^T)$               &                                    &                                      \\
% $\text{rowSums}(T)$       &                                    &                                      \\
% $\text{rowSums}(T^T)$       &                                    &                                      \\
% $\text{colSums}(T)$       &                                    &                                      \\
% $\text{colSums}(T^T)$       &                                    &                                      \\ \hline

% $TX$               & $\displaystyle c_X \cdot nnz(T) + r_T \cdot nnz(X)$              & $\displaystyle \sum_{k=1}^K c_X \cdot nnz(S_k) + r_{S_k} \cdot nnz(X)$            \\
% $T^TX$               & $\displaystyle c_T \cdot nnz(X) + c_X \cdot nnz(T)$              & $\displaystyle \sum_{k=1}^K c_{S_k} \cdot nnz(X) + c_X \cdot nnz(S_k)$          \\
% $XT$               & $\displaystyle c_T \cdot nnz(X) + r_X \cdot nnz(T)$              & $\displaystyle \sum_{k=1}^K c_{S_k} \cdot nnz(X) + r_X \cdot nnz(S_k)$      \\
% $XT^T$               & $\displaystyle r_X + nnz(T) + r_T \cdot nnz(X) $              & $\displaystyle \sum_{k=1}^K r_X + nnz(S_k) + r_{S_k} \cdot nnz(X)$            \\
% \bottomrule
% \end{tabular}
% \caption{LA computations cost: materialization vs. factorization.}
% \label{tab:complexity}
% \end{table}


% %----------------------------------------------------------%
% \para{Computing the sparsity of $T$}\label{s:est_sparsity} Some elements in the complexity analysis can be inferred directly from the source tables, while others require estimation. Elements which can be directly inferred include $r_X$, $c_X$, $r_k$, $c_k$ and $m_k$, as these are stored in SciPy objects. The elements $r_T$ and $c_T$ cannot be inferred from the source tables directly, but can be inferred from the indicator matrices and mapping matrices, respectively. The element that requires the most complicated computations is $m_T$, which depends upon the source tables' sparsities, the indicator matrix and the mapping matrix. In order to compute $m_T$, we compute $T$ using the materialization computation $T \leftarrow  I_1 S_1 M_1^T + ... + I_K S_K M_K^T$. 
% Then, we compute the sparsity directly from the result of this computation. 
% %----------------------------------------------------------%
% \para{Limitation and future work} First, to obtain the number of non-zero elements, i.e., sparsity of $T$, we need to materialize target table $T$. 
%--------------------------------------------------------------------------%
%--------------------------------------------------------------------------%
% In preliminary experiments, we tried avoiding materialization and estimating sparsity, leading to unsatisfactory effectiveness results. Instead, we discovered that table materialization time is omittable compared to model training time, as elaborated in Sec.~\ref{sec:overhead}. Therefore, we chose to materialize the target table for a more accurate sparsity value. The goal of this work is not to avoid materialization \cite{kumar2016join}. Instead, the primary goal of this work is to ascertain whether, for a set of given source tables and learning tasks, the training of a model is expedited by conducting it over the source tables or through the use of a materialized target table.
% % Second,
% % The result of two sparse matrix multiplication is not necessarily a sparse matrix \cite{horowitz1982fundamentals}.  
%--------------------------------------------------------------------------%
%--------------------------------------------------------------------------%
% \vspace{-2mm}
% \subsection{Complexity Analysis of ML Models}\label{s:complexity_models}

 

\para{Linear regression} First, we analyze the complexity of linear regression in Algorithm \ref{alg:linear-regression} in the materialized case, which is dominated by two matrix multiplication operations, i.e., $ T^T$ $(T w)$. 
For the first matrix multiplication $T w$: 
we denote the shape of weights vector $w$ as $c_T \times 1$. Following the general assumption that $w$  is dense, we calculate $m_w$, the number of nonzero elements in $w$, with the equation $m_w = r_w \times c_w$. 
We denote the matrix multiplication result of $T w$ as $X$, which is an intermediate result
of linear regression algorithm. The size of $X$ is $r_T \times 1$, since $r_X= r_T\  and \ c_X =1$. 
Similarly, we calculate $m_X$, the number of nonzero elements in $X$, with the equation $m_X = r_X \times c_X$.  
Now, we define the complexity of linear regression in the materialized case based on $T w$ and $T^TX$.
\[
O_\text{materialized}(T) = \underbrace{c_w \cdot m_T + r_T \cdot m_w}_{Tw} + \underbrace{m_T + c_T \cdot m_X}_{T^TX}
\]

Next, we define the complexity of the factorized case. 
\[
\begin{aligned}
O_\text{factorized}(T) = & \underbrace{\sum_{k=1}^n (c_w \cdot m_k + r_k \cdot m_w)}_{Tw} + 
 \underbrace{\sum_{k=1}^n (m_k + c_k \cdot m_X)}_{T^TX}
\end{aligned}
\]


\para{Complexity ratio}
We define a variable \emph{complexity ratio} to indicate whether materialization or factorization leads to more computing. 
The complexity ratio is measured as the ratio of the materialization complexity divided by the factorization complexity.
\begin{equation}
\label{cr_value}
     \text{complexity ratio} = \frac{O_\text{materialization}(T)}{O_\text{factorization}(T)}
\end{equation}



% \subsection{Is Cost Estimation Necessary?}
% \subsection{Is Complexity Ratio All We Need?}
% \label{ssec:need_CE}
% \para{Solution so far} In Fig.~\ref{fig:ForM}, our objective is approximating the decision boundary between factorization and materialization. Two main factors -- \emph{data} (including DI metadata like schema mapping and row matching) and \emph{ML algorithm}-- are considered. The complexity ratio incorporates elements such as redundancy and operator complexity, approximating the logical-level decision boundary. Preliminary experiments, however, indicated that solely logical-level estimation falls short due to software-hardware interactions such as parallelism and memory bandwidth.







% % As illustrated in Fig.~\ref{fig:ForM}, our goal is to find an approximate decision boundary between factorization and materialization. We have considered two main factors, \emph{data} including DI metadata such as schema mapping and row matching, and \emph{ML algorithm}. As mentioned in Sec.~\ref{sec: gaps}, redundancy and sparsity play important roles. 
% % The complexity ratio incorporates relative source and target redundancies and the complexity of the operators, which approximates the decision boundary of factorization and materialization at the logical level. 
% % However, our preliminary experiments showed merely a logical-level estimation was insufficient, due to the interactions between the software implementation and hardware properties, such as parallelism and memory bandwidth.

 

% \para{The third factor} 
% One preliminary experiment reveal hardware configurations as a vital factor in the decision between factorization and materialization. Figure \ref{fig:prelim} showcases how the choice varies with changing parallelism w.r.t the numbers of CPU cores. Interestingly, factorization outperform materialization when the parallelism increases to 32, even at a complexity ratio of 3. This result is attributed to factorization's ability to decouple source table records, thus enabling parallel computational tasks and leveraging extensive hardware parallelism.


% % From experiments, we discovered that a third factor, i.e., hardware configurations, needs to be taken into consideration. Figure \ref{fig:prelim} gives a simple example of how the choice of factorization or materialization changes, with various levels of parallelism w.r.t. the number of CPU cores (parallelism). We elaborate experiment setting in Sec.~\ref{sec:eva}. 
% % When the y-axis value is less than 1.0, the actual faster strategy is factorization, while above 1.0 means materialization is faster in the experiment.
% % We observe that with 8 cores, with complexity ratio values of 3 and 6, materialization was faster. Yet, interestingly, upon increasing the core number to 32, factorization surpassed materialization, even with a complexity ratio of 3. This shift resulted in a speed boost of over 30\% compared to materialization. This can be attributed to factorization's ability to decouple records in different source tables, which in turn enables the launch of computational tasks in parallel. By leveraging the extensive computational capabilities of a hardware platform with high parallelism, factorization can indeed deliver superior performance.



% Given the intricate decision making as shown Fig. ???\ref{fig:prelim}, the goal is to design a model that can determine the most efficient method for training machine learning models over data silos. More specifically, if materialization is estimated to be faster, we train models over materialized data. Conversely, when factorization estimated to be quicker, we employ factorized training. This approach, therefore, creates a more efficient machine learning training pipeline over data silos, optimizing the overall performance.

% Next, we show how Ilargi estimates to factorize or to materialize, considering all three factors of data, model, and hardware.
 



\vspace{-2mm}
\subsection{The Hardware Factor} 
\label{sec:3rdfac}
\vspace{-2mm}
In our evaluation (detailed in Sec. \ref{fig:eval_f_m}), we found that materialized training can outperform factorized learning, despite the higher redundancy. Additionally, GPU speedups are inconsistent. These discrepancies go beyond computational complexity, highlighting the influence of hardware properties like memory bandwidth and parallelism. Traditional complexity analysis often neglects the I/O overhead from data movement, especially in multi-epoch ML training where I/O costs accumulate. To address this, we introduce a learned cost estimator that considers data characteristics, algorithm complexity, and hardware features to better determine the optimal training method.% In Sec.~\ref{sec:eva}, we extensively evaluate the performance of factorized learning on both CPUs and GPUs. One observation is that only 32\% of instances exhibited performance improvements on both types of hardware. 
% This unexpected low overlapping highlights the impact of architectural differences on the efficiency of factorization. To gain a deeper understanding of the internal dynamics of training algorithms on these two distinct platforms, we employed a performance analysis comparing the memory cost and the math cost in the profiled scenarios. 

% According to an official document from NVIDIA~\cite{nvidia-gpu-performance:online}, an effective method to predict the execution time of a GPU program is to calculate $max(T_{mem}, T_{math})$. In this formula $T_{mem}$ is the time required to transfer data to and from the GPU memory, while $T_{math}$ denotes the time needed for actual computations. This approach is in line with the inherently parallel architecture of GPUs. Should the data transfer to the GPU prove insufficiently fast, the GPU's Streaming Multiprocessors will remain idle, awaiting data. This indicates a memory-bound program. Conversely, if $T_{math} > T_{mem}$, the program is considered compute bound. This section elaborates which scenario applies to our experiments and details how this knowledge can be harnessed to estimate the runtime for machine learning training scenarios.

% \begin{figure}[t]
%   \centering
%   \begin{minipage}{0.4\textwidth}
%     \includegraphics[width=\linewidth]{figures/profiling-mem-vs-compute-materialized.pdf}
%   \end{minipage}
%   \begin{minipage}{0.4\textwidth}
%     \includegraphics[width=\linewidth]{figures/profiling-mem-vs-compute-factorized.pdf}
%   \end{minipage}
%   \caption[Memory cost vs math cost of profiled scenarios]{Memory cost ($T_{mem}$) vs compute cost ($T_{math}$) of profiled scenarios. The memory cost is computed as the total number of bytes read and written to memory divided by the measured average memory bandwidth. The math cost is the number of cycles the Streaming Multiprocessors were active divided by the measured average SM frequency.}
%   \label{fig:5-profiling-mem-vs-compute}
%   \vspace{-5mm}
% \end{figure}

% Fig.~\ref{fig:5-profiling-mem-vs-compute} shows the relationship between memory time $T_{mem}$ and computation time $T_{math}$, revealing a strong correlation ($\rho = 0.99$). The result predominantly shows that the memory cost exceeds the computational cost, as most points lie below the $y=x$ line, indicating that the operations are memory-bound. 
% From this analysis, we conclude that a threshold-based empirical cost estimator fails to account internal dynamics other than computing complexity. Therefore, we propose a learned cost estimator that leverages data characteristics, algorithmic complexity, and hardware features to more accurately determine the faster training method.



\vspace{-3mm}
\subsection{ML-based Cost Estimator}
\label{s:estimator}
\vspace{-2mm}
% Our cost estimator is designed to make a binary decision—factorization or materialization—based on data characteristics, computational complexity, and hardware features. Its goal is to select the more efficient training method. This choice is especially impactful when training multiple models, as in parameter tuning, where even slight speed improvements can translate into substantial time savings.

% While we considered analytical performance models \cite{hpc1, hpc2, hpc3} from the high-performance computing community, they lack flexibility for diverse ML models and need extensive micro-benchmark data. Given these complexities, it becomes more practical to utilize a black-box estimator \cite{blackbox1}. This type of estimator employs a statistical model to make a binary decision, effectively abstracting away from the specific details of individual hardware components and algorithmic intricacies.


% \para{Ilargi's tree boosting estimator}  Tree boosting models stand out among various statistical models, and are widely used in many cost estimators \cite{tvm, halide} due to their explainability and prediction speed. Moreover, tree boosting has the capability to identify non-linear relationships among features.
% % \para{Model Training} Our model was trained using XGBoost, a well-regarded gradient boosting framework. 
% Given these advantages, we propose a tree boosting estimator leveraging XGBoost \cite{xgboost}. The main challenge lies in identifying features that are relevant to the decision-making process between materialization and factorization.


% \para{Selection of hardware features} 
% Our estimator is generalizable and portable, focusing on macro-level features instead of micro-level. Micro-level features (e.g., cache speeds and memory latency), while valuable, can tie the model to specific hardware, limiting portability. 
% Instead, we incorporate macro-level features like number of parallelism and memory bandwidth. Shown in Sec.~\ref{ex:estimator}, combined with memory read/write costs, we can effectively estimate maximum memory access costs for both materialization and factorization. 
% In addition to the memory features, parallelism is also a crucial factor that influences the speedup of factorized learning. Therefore, the number of threads executing mathematical computations is also included our features.

Our cost estimator determines whether factorization or materialization is the more efficient training method, considering data characteristics, computational complexity, and hardware features. This choice is especially impactful when training multiple models, such as in parameter tuning, where small speed gains can lead to significant time savings.

Analytical performance models \cite{hpc2,hpc3} from high-performance computing are limited in flexibility for diverse ML models and require extensive micro-benchmark data, making a black-box estimator \cite{blackbox1} a practical alternative. This type of estimator uses a statistical model to make a binary decision, abstracting details of hardware and algorithms.

\para{Ilargi's Tree Boosting Estimator.} Tree boosting is known for its explainability and speed, making it popular in cost estimation tasks \cite{tvm,halide}. We use XGBoost for its ability to capture non-linear relationships, with a focus on identifying key features that influence the choice between materialization and factorization.

\para{Hardware Features.} To ensure portability, our estimator emphasizes macro-level hardware features, such as memory bandwidth and parallelism, over micro-level specifics like cache speed. Parallelism, particularly the number of threads used, is included due to its impact on factorized learning speedups. Combined with memory read/write costs, these features enable effective cost estimation for both methods (Sec.~\ref{ex:estimator}).




\para{Cost estimation pipeline}\label{s:design}
% The pipeline of our cost estimation approach is shown in Fig. \ref{fig:cost_estimation_amalur}. For explanatory purposes, we highlight Linear regression and its involved LA operations. 
% The optimizer takes inputs of the source tables, indicator matrix and mapping matrix. The mapping matrices and indicator matrices are generated from the input schema matching and entity resolution results, which can be generated from open-source schema matching \cite{koutras2021valentine} and  entity resolution  tools \cite{ER sol}. 
% The ML model is provided by the user, including model hyperparameters. 
The cost estimator uses three input groups: \texttt{i)} data (source matrices, mapping, and indicator matrices), \texttt{ii)} ML algorithm (operators and user-defined hyperparameters), and \texttt{iii)} hardware (e.g., parallelism, memory bandwidth). Fig. \ref{fig:cost_estimation_amalur} shows its workflow using linear regression as an example. The estimator\footnote{A full list of 33 features is detailed in our technical report \cite{tech}.} computes the complexity ratio (Sec.~\ref{s:operations}) and the theoretical memory I/O for each operator in the ML algorithm. It normalizes these features based on parallelism and memory bandwidth. The output is binary: If True, Ilargi uses factorization; otherwise, it materializes the data before training.% as input ML model. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/new_eval_figures/cost-est-pipeline-simple.drawio.pdf}
    % \includegraphics[width=0.85\linewidth]{figures/new_eval_figures/cost-est-pipeline.pdf}
    \caption{Workflow of the estimator.}
    \label{fig:cost_estimation_amalur}
    \vspace{-5mm}
\end{figure}

% The estimator calculates the complexity ratio in Sec.~\ref{s:operations},
%----------------recover-------------------------------------
% \footnote{Hyperparameters such as the rank $r$ for GNMF and the number of clusters $k$ for KMeans are factored into the cost estimation. However, other hyperparameters like the learning rate $\gamma$ for linear and logistic regression do not affect the cost estimation.}, 
%----------------recover-------------------------------------
% and theoretical memory read and write quantities for each operator within the input ML algorithm to be trained. Since factorization and materialization have different complexities and memory I/O amounts, we normalize these features by dividing total amounts, further dividing these normalized features by parallelism and memory bandwidth. 
% In addition, we include the complexity ratio and tuple ratio into our estimator, comparing feature importance during evaluation (Sec.~\ref{sec:eva}). 
% We document and explain the full list of 33 features in our technical report.

% The output of the estimator is binary. Specifically, if the prediction is True, Ilargi trains the input ML model using the factorization approach. Otherwise, Ilargi first joins the source tables, subsequently training the model on the materialized target table. 

% \noindent\textbf{Why is Ilargi efficient?}
% The speedup of factorization is typically influenced by data characteristics and training algorithms. With new hardware like GPUs, the situation becomes more complex. Due to the inconsistent advantages of factorization over materialization, a practical factorized learning system requires an accurate cost estimator to determine the optimal method for overall performance improvement. Our ML-based cost estimator incorporates diverse data characteristics and memory access features specific to certain hardware, leading to effective training method selection across various hardware environments. With optimal training strategies, \emph{Ilargi} can efficiently train models over disparate data sources.








% %----------------------------------------%
% \\ \para{Limitations and applicability} The estimator provides ease of construction and captures non-linear features but requires a diverse dataset for training in advance. Its predictive accuracy can be limited for models with unseen operators due to sensitivity to operator combinations in supported ML algorithms. Additionally, its assumption of operator information availability for factorization may restrict the range of models it can support.
% %----------------------------------------%

% 
% This decision-making process enhances the overall efficiency of ML training workflows on data silos.

% Due to space restrictions, we list a few representative operators and include the full list of supported LA operators in \cite{tech}.
% \begin{myframe}{~Answer to Q3}
% \emph{1. Schema mappings can be used as pruning rules.\\
% 2. The choice of factorization and materialization depends on the datasets and their relationships, ML algorithms (LA operators), and hardware configuration.}  
% \end{myframe}
% \vspace{-2mm}



% To assess the predictive quality and generalizability of the estimator, we conducted a series of comprehensive experiments, as detailed in Section \ref{ex:estimator}. This section provides an in-depth description of the evaluation data used, along with the training configurations employed in the experiments.
% In Sec.~\ref{ssec:speedup} we discuss how empirical threshold value $\tau$ is obtained.
% The determination of $\tau$ is explained in the next section. 

% \para{Discussion} Ideally, when the complexity ratio is above 1, it indicates that a speedup should occur. However, the process of factorization brings additional  overhead \rihan{what overhead}. 
% Thus, solely relying on the complexity ratio may not lead to an accurate cost estimation. That is, a more careful design of the threshold value $\tau$telling when we shall expect speedups is needed, which we discuss in Section \ref{s:threshold}.


% , further narrowing its applicability.