@inproceedings{10.1145/3299869.3319878,
author = {Li, Side and Chen, Lingjiao and Kumar, Arun},
booktitle = {SIGMOD},

pages = {1571--1588},
title = {{Enabling and Optimizing Non-Linear Feature Interactions in Factorized Linear Algebra}},
year = {2019}
}

@inbook{10.1145/3448016.3457241,
author = {Fu, Fangcheng and Shao, Yingxia and Yu, Lele and Jiang, Jiawei and Xue, Huanran and Tao, Yangyu and Cui, Bin},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2021 - VF2Boost Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning.pdf:pdf},
isbn = {9781450383431},
mendeley-groups = {aFederated learning/tabular},
pages = {563--576},
publisher = {Association for Computing Machinery},
title = {{VF2Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning}},
year = {2021}
}

@inbook{10.1145/3459637.3482361,
author = {Fang, Wenjing and Zhao, Derun and Tan, Jin and Chen, Chaochao and Yu, Chaofan and Wang, Li and Wang, Lei and Zhou, Jun and Zhang, Benyu},
booktitle = {Proceedings of the 30th ACM International Conference on Information & Knowledge Management},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang et al. - 2021 - Large-Scale Secure XGB for Vertical Federated Learning.pdf:pdf},
isbn = {9781450384469},
mendeley-groups = {aFederated learning/tabular},
pages = {443--452},
publisher = {Association for Computing Machinery},
title = {{Large-Scale Secure XGB for Vertical Federated Learning}},
year = {2021}
}

@inproceedings{10.1145/3514221.3526127,
  author    = {Fangcheng Fu and
               Huanran Xue and
               Yong Cheng and
               Yangyu Tao and
               Bin Cui},

  title     = {{BlindFL}: Vertical Federated Machine Learning without Peeking into
               Your Data},
  booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
               PA, USA, June 12 - 17, 2022},
  pages     = {1316--1330},
  publisher = {{ACM}},
  year      = {2022},
}

@inproceedings{Boehm2023SIGMOD,
author = {Boehm, Matthias and Interlandi, Matteo and Jermaine, Chris},
title = {Optimizing Tensor Computations: From Applications to Compilation and Runtime Techniques},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Machine learning (ML) training and scoring fundamentally relies on linear algebra programs and more general tensor computations. Most ML systems utilize distributed parameter servers and similar distribution strategies for mini-batch stochastic gradient descent training. However, many more tasks in the data science and engineering lifecycle can benefit from efficient tensor computations. Examples include primitives for data cleaning, data and model debugging, data augmentation, query processing, numerical simulations, as well as a wide variety of training and scoring algorithms. In this survey tutorial, we first make a case for the importance of optimizing more general tensor computations, and then provide an in-depth survey of existing applications, optimizing compilation techniques, and underlying runtime strategies. Interestingly, there are close connections to data-intensive applications, query rewriting and optimization, as well as query processing and physical design. Our goal for the tutorial is to structure existing work, create common terminology, and identify open research challenges.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {53–59},
numpages = {7},
keywords = {large-scale machine learning, linear algebra, declarative machine learning, data science, tensor computations, data engineering},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@inproceedings{DBLP:conf/icde/ChengKZ021,
  author       = {Zhaoyue Cheng and
                  Nick Koudas and
                  Zhe Zhang and
                  Xiaohui Yu},
  title        = {Efficient Construction of Nonlinear Models over Normalized Data},
  booktitle    = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
  pages        = {1140--1151},
  publisher    = {{IEEE}},
  year         = {2021},
  timestamp    = {Fri, 25 Jun 2021 11:31:22 +0200},
  biburl       = {https://dblp.org/rec/conf/icde/ChengKZ021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@techreport{MorpheusPy,
  title={MorpheusPy: Factorized Machine Learning with NumPy},
  author={Li, Side and Kumar, Arun},
  institution={Technical report, 2018. Available at https://adalabucsd. github. io/papers~…}
}

@article{chen2017towards,
  title={Towards Linear Algebra over Normalized Data},
  author={Chen, L. and Kumar, A. and Naughton, J. and Patel, J. M.},
  journal={PVLDB},
  volume={10},
  number={11},
  year={2017}
}

@inproceedings{chenRedundancyEliminationDistributed2022,
  title = {Redundancy {{Elimination}} in {{Distributed Matrix Computation}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chen, Zihao and Han, Baokun and Xu, Chen and Qian, Weining and Zhou, Aoying},
  date = {2022-06-10},
  pages = {573--586},
  publisher = {{ACM}},
  location = {{Philadelphia PA USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3514221.3517877},
  urldate = {2022-06-16},
  abstract = {As matrix computation becomes increasingly prevalent in largescale data analysis, distributed matrix computation solutions have emerged. These solutions support query interfaces of linear algebra expressions, which often contain redundant subexpressions, i.e., common and loop-constant subexpressions. Hence, existing compilers rewrite queries to eliminate such redundancy. However, due to the large search space, they fail to find all redundant subexpressions, especially for matrix multiplication chains. Furthermore, redundancy elimination may change the original execution order of operators, and have negative impacts. To reduce the large search space and avoid the negative impacts, we propose automatic elimination and adaptive elimination, respectively. In particular, automatic elimination adopts a block-wise search that exploits the properties of matrix computation for speed-up. Adaptive elimination employs a cost model and a dynamic programming-based method to generate efficient plans for redundancy elimination. Finally, we implement ReMac atop SystemDS, eliminating redundancy in distributed matrix computation. In our experiments, ReMac is able to generate efficient execution plans at affordable overhead costs, and outperforms state-of-the-art solutions by an order of magnitude.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '22: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/N4U2TEJ6/Chen et al. - 2022 - Redundancy Elimination in Distributed Matrix Compu.pdf}
}

@article{cheng2021secureboost,
  author       = {Kewei Cheng and
                  Tao Fan and
                  Yilun Jin and
                  Yang Liu and
                  Tianjian Chen and
                  Dimitrios Papadopoulos and
                  Qiang Yang},
  title        = {SecureBoost: {A} Lossless Federated Learning Framework},
  journal      = {{IEEE} Intell. Syst.},
  volume       = {36},
  number       = {6},
  pages        = {87--98},
  year         = {2021},
  timestamp    = {Wed, 16 Mar 2022 23:54:44 +0100},
  biburl       = {https://dblp.org/rec/journals/expert/ChengFJLCPY21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cheng_efficient_2021,
	title = {Efficient {Construction} of {Nonlinear} {Models} over {Normalized} {Data}},
	
	author = {Cheng, Zhaoyue and Koudas, Nick and Zhang, Zhe and Yu, Xiaohui},
	month = apr,
	year = {2021},
	keywords = {Artificial neural networks, Computational modeling, Mixture models, Network architecture, Redundancy, Systematics, Training, notion},
	pages = {1140--1151},
}

@article{ghosh2020efficient,
  title={An efficient framework for clustered federated learning},
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19586--19597},
  year={2020}
}

@article{huang2023joinboost,
  title={JoinBoost: Grow Trees over Normalized Data Using Only SQL},
  author={Huang, Zezhou and Sen, Rathijit and Liu, Jiaxiang and Wu, Eugene},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={11},
  pages={3071--3084},
  year={2023},
  publisher={VLDB Endowment}
}

@inproceedings{hummingbird1,
author = {Nakandala, Supun and Saur, Karla and Yu, Gyeong-In and Karanasos, Konstantinos and Curino, Carlo and Weimer, Markus and Interlandi, Matteo},
title = {A Tensor Compiler for Unified Machine Learning Prediction Serving},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
booktitle = {OSDI 2020},
articleno = {51},
numpages = {19},
}

@article{hummingbird2,
author = {Koutsoukos, Dimitrios and Nakandala, Supun and Karanasos, Konstantinos and Saur, Karla and Alonso, Gustavo and Interlandi, Matteo},
title = {{Tensors: An Abstraction for General Data Processing}},
year = {2021},
publisher = {VLDB Endowment},
volume = {14},
number = {10},
journal = {Proc. VLDB Endow.},
pages = {1797–1804},
}

@inproceedings{joinproject1,
author = {Amossen, Rasmus Resen and Pagh, Rasmus},
title = {{Faster Join-Projects and Sparse Matrix Multiplications}},
year = {2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
pages = {121–126},
numpages = {6},
keywords = {relational algebra, matrix multiplication, collapsing join-project},
location = {St. Petersburg, Russia},
booktitle = {ICDT 2009}
}

@inproceedings{joinproject2,
author = {Deep, Shaleen and Hu, Xiao and Koutris, Paraschos},
title = {{Fast Join Project Query Evaluation Using Matrix Multiplication}},
year = {2020},
booktitle = {SIGMOD 2020},
pages = {1213–1223},
numpages = {11},
keywords = {projections, matrix multiplication, relational algebra, join queries},
}

@inproceedings{joinproject3,
author = {Huang, Zichun and Chen, Shimin},
title = {Density-Optimized Intersection-Free Mapping and Matrix Multiplication for Join-Project Operations},
year = {2022},
publisher = {VLDB Endowment},
volume = {15},
number = {10},
journal = {VLDB Endowment},
pages = {2244–2256},
numpages = {13}
}

@article{justoPolyglotFrameworkFactorized2021,
  title = {Towards a Polyglot Framework for Factorized {{ML}}},
  author = {Justo, David and Yi, Shaoqing and Stadler, Lukas and Polikarpova, Nadia and Kumar, Arun},
  date = {2021-07-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {14},
  number = {12},
  pages = {2918--2931},
  issn = {2150-8097},
  
  urldate = {2022-06-15},
  abstract = {Optimizing machine learning (ML) workloads on structured data is a key concern for data platforms. One class of optimizations called "factorized ML" helps reduce ML runtimes over multi-table datasets by pushing ML computations down through joins, avoiding the need to materialize such joins. The recent Morpheus system automated factorized ML to any ML algorithm expressible in linear algebra (LA). But all such prior factorized ML/LA stacks are restricted by their chosen programming language (PL) and runtime environment, limiting their reach in emerging industrial data science environments with many PLs (R, Python, etc.) and even cross-PL analytics workflows. Re-implementing Morpheus from scratch in each PL/environment is a massive developability overhead for implementation, testing, and maintenance. We tackle this challenge by proposing a new system architecture, Trinity, to enable factorized LA logic to be written only once and easily reused across many PLs/LA tools in one go. To do this in an extensible and efficient manner without costly data copies, Trinity leverages and extends an emerging industrial polyglot compiler and runtime, Oracle's GraalVM. Trinity enables factorized LA in multiple PLs and even cross-PL workflows. Experiments with real datasets show that Trinity is significantly faster than materialized execution ({$>$} 8x speedups in some cases), while being largely competitive to a prior single PL-focused Morpheus stack.},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/7W6G64YU/Justo et al. - 2021 - Towards a polyglot framework for factorized ML.pdf}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@inproceedings{khamis_acdc_2018,
	address = {Houston TX USA},
	title = {{AC}/{DC}: {In}-{Database} {Learning} {Thunderstruck}},
	shorttitle = {{AC}/{DC}},
	
	language = {en},
	booktitle = {Proceedings of the {Second} {Workshop} on {Data} {Management} for {End}-{To}-{End} {Machine} {Learning}},
	publisher = {ACM},
	author = {Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
	month = jun,
	year = {2018},
	keywords = {notion},
	pages = {1--10},
}

@inproceedings{kumar2015learning,
author = {Kumar, A. and Naughton, J. and Patel, J. M.},
booktitle = {{SIGMOD}},
pages = {1969--1984},
title = {{Learning generalized linear models over normalized data}},
year = {2015}
}

@inproceedings{kumar2016join,
annote = {Schema Independent, yet same answer

learning},
author = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M and Zhu, Xiaojin},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
file = {:D\:/Students/Andra/refs/To Join or Not to Join  Thinking Twice about Joins before Feature Selection.pdf:pdf},
mendeley-groups = {2systemInOne/AIndDB/Database Techniques for Machine Learning/JOINrelated,ML/data augmentation,2systemInOne/Factorization/Factorized Learning,Data augmentation/feature augment},
pages = {19--34},
title = {{To join or not to join? thinking twice about joins before feature selection}},
year = {2016}
}

@article{kumar_demonstration_2015,
	title = {Demonstration of {Santoku}: optimizing machine learning over normalized data},
	volume = {8},
	shorttitle = {Demonstration of {Santoku}},
	
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kumar, Arun and Jalal, Mona and Yan, Boqun and Naughton, Jeffrey and Patel, Jignesh M.},
	month = aug,
	year = {2015},
	keywords = {notion},
	pages = {1864--1867},
}

@inproceedings{li2019convergence,
  title={On the Convergence of FedAvg on Non-IID Data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{makryniotiDeclarativeDataAnalytics2021,
  title = {Declarative {{Data Analytics}}: {{A Survey}}},
  shorttitle = {Declarative {{Data Analytics}}},
  author = {Makrynioti, Nantia and Vassalos, Vasilis},
  date = {2021-06},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {33},
  number = {6},
  pages = {2392--2411},
  issn = {1558-2191},
  
  abstract = {The area of declarative data analytics explores the application of the declarative paradigm on data science and machine learning. It proposes declarative languages for expressing data analysis tasks and develops systems which optimize programs written in those languages. The execution engine can be either centralized or distributed, as the declarative paradigm advocates independence from particular physical implementations. The survey explores a wide range of declarative data analysis frameworks by examining both the programming model and the optimization techniques used, in order to provide conclusions on the current state of the art in the area and identify open challenges.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Analytical models,Data analysis,data science,Declarative programming,large-scale analytics,machine learning,Mathematical model,notion,Optimization,Prediction algorithms,Programming,Task analysis},
  file = {/Users/jessie/Zotero/storage/VX7RNQ64/Makrynioti and Vassalos - 2021 - Declarative Data Analytics A Survey.pdf;/Users/jessie/Zotero/storage/IJ9H9VQS/8931243.html}
}

@article{mustafaMachineLearningApproach2018,
  title = {A {{Machine Learning Approach}} for {{Predicting Execution Time}} of {{Spark Jobs}}},
  author = {Mustafa, Sara and Elghandour, Iman and Ismail, Mohamed A.},
  date = {2018-12},
  journaltitle = {Alexandria Engineering Journal},
  shortjournal = {Alexandria Engineering Journal},
  volume = {57},
  number = {4},
  pages = {3767--3778},
  issn = {11100168},
  
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1110016818301728},
  urldate = {2022-06-14},
  abstract = {Spark has gained growing attention in the past couple of years as an in-memory cloud computing platform. It supports execution of various types of workloads such as SQL queries and machine learning applications. Currently, many enterprises use Spark to exploit its fast inmemory processing of large scale data. Additionally, speeding up the execution in Spark is an important problem for many real-time applications. This can be achieved by improving the scheduling approaches employed by Spark, optimizing the execution plans generated by Spark for various applications, and selecting the best cluster configuration to run an input workload. A first step for all these optimization approaches is to predict the execution time of an input Spark application. In this paper, we present a new platform that predicts with high accuracy the execution time of SQL queries and machine learning applications executed by Spark. We evaluate our proposed platform by measuring the accuracy of predicting execution time of various types of Spark jobs including TPC-H queries and machine learning classification/clustering applications. The evaluation experiments show that we are able to predict the execution time of Spark jobs using our proposed platform with accuracy greater than 90\% for SQL queries and greater than 75\% for machine learning jobs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/NUWJS7P7/Mustafa et al. - 2018 - A Machine Learning Approach for Predicting Executi.pdf}
}

@InProceedings{pmlr-v162-lubana22a,
  title = 	 {Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering},
  author =       {Lubana, Ekdeep and Tang, Chi Ian and Kawsar, Fahim and Dick, Robert and Mathur, Akhil},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14461--14484},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lubana22a/lubana22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lubana22a.html}
}

@inproceedings{schleich_layered_2019,
author = {Schleich, Maximilian and Olteanu, Dan and Abo Khamis, Mahmoud and Ngo, Hung Q. and Nguyen, XuanLong},
title = {A Layered Aggregate Engine for Analytics Workloads},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1642–1659},
numpages = {18},
keywords = {mutliple aggregate optimization, structure-aware analytics},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inproceedings{systemds,
  title={SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle},
  author={Boehm, Matthias and Antonov, Iulian and Baunsgaard, Sebastian and Dokter, Mark and Ginthoer, Robert Erich and Innerebner, Kevin and Klezin, Florijan and Lindstaedt, Stefanie and Phani, Arnab and Rath, Benjamin and others},
  year={2020},
  booktitle={CIDR}
}

@inproceedings{tqp,
author = {He, Dong and Nakandala, Supun C and Banda, Dalitso and Sen, Rathijit and Saur, Karla and Park, Kwanghyun and Curino, Carlo and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Karanasos, Konstantinos and Interlandi, Matteo},
title = {{Query Processing on Tensor Computation Runtimes}},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
pages = {2811–2825},
}

@article{yang@2019fml,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},

journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {12},
numpages = {19},
keywords = {transfer learning, GDPR, Federated learning}
}

@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

@inproceedings{zhao2018random,
  title={Random sampling over joins revisited},
  author={Zhao, Zhuoyue and Christensen, Robert and Li, Feifei and Hu, Xiao and Yi, Ke},
  booktitle={Proceedings of the 2018 International Conference on Management of Data},
  pages={1525--1539},
  year={2018}
}

