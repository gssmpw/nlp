% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{pseudocode} % Environment for specifying algorithms in a natural way
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{url}
\usepackage{tikz}
\usepackage{balance}
\usepackage[skip=1pt]{caption}
\usepackage{booktabs}
\usepackage{cite}

\usepackage{algorithm, algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\newcommand*{\definitionautorefname}{Definition}
\newcommand*{\algorithmautorefname}{Algorithm}\usepackage{pbox}

\newcommand{\mybox}[1]
{
\vspace{1.8mm}
\noindent \hspace{-1mm} 
\setlength\fboxsep{1mm}
\fbox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{\itshape #1}}
% \vspace{1mm}
}
\usepackage{amsmath}

\newcommand{\para}[1]{\vspace{0.1mm}\noindent\textbf{#1}.} 
\newcommand{\orion}{kumar2015learning}
\newcommand{\santoku}{kumar_demonstration_2015}
\newcommand{\hamlet}{kumarJoinNotJoin2016}
\newcommand{\f}{schleichLearningLinearRegression2016}
\newcommand{\morpheus}{chen2017towards}
\newcommand{\hamletplusplus}{shahAreKeyForeignKey2017}
\newcommand{\acdc}{khamisACDCInDatabase2018}
\newcommand{\aidaone}{dsilvaAIDAAbstractionAdvanced2018}
\newcommand{\aida}{dsilvaAIDAAbstractionAdvanced2018, dsilvaMakingRDBMSData2019}
\newcommand{\morpheusfi}{liEnablingOptimizingNonlinear2019}
\newcommand{\lmfao}{schleichLayeredAggregateEngine2019}
\newcommand{\hadad}{alotaibiHADADLightweightApproach2021}
\newcommand{\amalur}{haiAmalurNextgenerationData2022}
\newcommand{\chengone}{chengNonlinearModelsNormalized2019}
\newcommand{\chengtwo}{chengEfficientConstructionNonlinear2021}
\newcommand{\dima}{sunDimaDistributedInmemory2017}    
\newcommand{\morpheuspy}{kumar_morpheuspy_nodate}
\newcommand{\trinity}{justoPolyglotFrameworkFactorized2021}

\newcommand{\redshade}[1]{%
  \begingroup
  \setlength{\fboxsep}{0pt}%  
  \colorbox{red!15}{\reducedstrut#1\/}%
  \endgroup
}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{Ilargi: a GPU Compatible Factorized ML Model Training Framework}
\author{Wenbo Sun\inst{1} \and
Rihan Hai\inst{1}}
\institute{Delft University of Technology \\
\email{w.sun-2@tudelft.nl} \\
\email{r.hai@tudelft.nl}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \institute{Delft University of Technology}
% \email{w.sun-2@tudelft.nl}
% %
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
% \author{}
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The machine learning (ML) training over disparate data sources traditionally involves materialization, which can impose substantial time and space overhead due to data movement and replication. Factorized learning, which leverages direct computation on disparate sources through linear algebra (LA) rewriting, has emerged as a viable alternative to improve computational efficiency. However, the adaptation of factorized learning to leverage the full capabilities of modern LA-friendly hardware like GPUs has been limited, often requiring manual intervention for algorithm compatibility. This paper introduces \emph{Ilargi}, a novel factorized learning framework that utilizes matrix-represented data integration (DI) metadata to facilitate automatic factorization across CPU and GPU environments without the need for costly relational joins. \emph{Ilargi} incorporates an ML-based cost estimator to intelligently selects between factorization and materialization based on data properties, algorithm complexity, hardware environments, and their interactions. This strategy ensures up to 8.9x speedups on GPUs and achieves over 20\% acceleration in batch ML training workloads, thereby enhancing the practicability of ML training across diverse data integration scenarios and hardware platforms. To our knowledge, this work is the very first effort in GPU-compatible factorized learning.


% \keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}

\input{1introduction}
\input{2amalure}
\input{3cost_estimation}
\input{4eval}
\input{5related_work}
\input{6conclusion}
%
%
%

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{reference}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}
\end{document}
