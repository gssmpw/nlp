%--------------------------------------------------------------------------------
%	for VLDB AMALUR (cite ICDE vision) || for SIGMOD use name Ilargi
%--------------------------------------------------------------------------------



\vspace{-3mm}
\section{Ilargi: ML Over Disparate Sources}
\label{sec:system}

% \rihan{A: note we assume data is preprocessed, or rules is given; Replacing nulls as zeros, }
% To train models effectively over disparate data sources, understanding the relationships between those sources is crucial. These relationships, including schema mapping and data matching, are collectively referred to as data integration metadata.

% In this section, we introduce the Ilargi system, which utilizes matrix-represented DI metadata to facilitate model training without the need for materialization. Figure~\ref{fig:amalur} illustrates Ilargi's workflow, which accepts three inputs: source datasets, a user-defined ML algorithm (e.g., Python scripts), and metadata about the datasets, including their data integration specifics such as schema mapping and data matching. The output from Ilargi is a fully trained ML model. Ilargi has three main functions. First, given data integration metadata, Ilargi generates their matrix-based representation  (Sec. \ref{sec:matrixGen}). Such matrix-based representations enable a unified computation of data transformation and linear algebra operations (Sec. \ref{s:operations}). Second, if the scenario is centralized training, the estimator decides to factorize or materialize (Sec.~\ref{sec:cost}). Finally, the ML model is trained in the chosen strategy, i.e., materialization or factorization.
\vspace{-3mm}
As illustrated in Fig.~\ref{fig:amalur}, \emph{Ilargi} first converts the inputs, DI metadata and source tables, into sparse matrices, storing them in a compressed format. This transformation facilitates a GPU-compatible representation of schema mappings and data matching  (Sec.~\ref{sec:matrixGen}). \emph{Ilargi} then performs LA rewriting to both ML model training algorithms and data integration tasks, reconfiguring the process into a sequence of linear algebra operations (Sec.~\ref{s:operations}). The cost estimator subsequently predicts the optimal training method—either factorization or materialization—based on data characteristics, algorithmic complexity, and the hardware environment (Sec.~\ref{sec:cost}). Finally, \emph{Ilargi} trains the model with the optimal training strategy recommended by the cost estimator.

In this section, we introduce how \emph{Ilargi} utilizes matrix-represented data integration metadata to enable efficient factorized model training on both GPUs and CPUs. 


 




% \para{Implementation} The ML models supported in our system are linear regression, logistic regression, and Gaussian NMF, which can be extended. 
% \rihan{A: Which system details to add?}

\vspace{-3mm}
\subsection{DI Metadata as Matrices} 
\label{sec:matrixGen}
\vspace{-2mm}
Preparing disparate training data for ML models requires understanding the relationships between the data sources and bridging them, which is achieved primarily through data integration tasks. The relationships are normally described by following metadata: \cite{doan2012principles}: $i)$ mappings between different source schemata, i.e., schema matching and mapping \cite{rahm2001survey, fagin2009clio} and $ii)$ linkages between data instances, i.e., data matching (also known as record linkage or entity resolution)  \cite{brizan2006survey}. 
We refer to such vital information derived from data integration process as \emph{data integration metadata} (\textit{DI metadata}).

Schema mappings are traditionally formalized as first-order logic~\cite{fagin2009}, and represented and executed using query languages such as SQL~\cite{yan2001data} or XQuery~\cite{fagin2009clio}. To accelerate both data integration and machine learning operations using GPUs, it is critical to find a unified representation that is highly compatible with GPU architectures.
In Ilargi, we employ a matrix-based representation for schema mappings, which we refer to as \emph{mapping matrices}, denoted as $\mathbf{M}$. Given a source table $S_k$ and target table $T$, a mapping matrix $M_k \in \mathbf{M}$ has the size of  $c_T \times c_{k}$, and specifies how the columns of $S_k$ are mapped to the columns of $T$. 

\begin{definition}[\textit{Mapping matrix}] Mapping matrices between source tables $S_1, S_2, ..., S_n$ and target table $T$ are a set of binary matrices $\mathbf{M}= \{M_1, ..., M_n\}$. $M_k \ (k\in [1, n])$ is a matrix with the shape $c_{T} \times c_{k}$, where  

\small{
		\begin{align*}
			\begin{split} 
M_k[i,j] =  \begin{cases}1, &\text{if j-th column of $S_k$ mapped to i-th column of } T\\0, & \text{otherwise}\end{cases}
			\end{split}
		\end{align*}
	}
	\label{def:mm}
 
\end{definition}

\vspace{-2mm}
Intuitively, in $M_k[i,j]$ the vertical coordinate $i$ represents the target table column while the horizontal coordinate $j$ represents the mapped source table column.  A value of $1$ in $M_k$ specifies the existence of  column correspondences between  $S_k$ and $T$, while the value $0$ shows that the current target table attribute has no corresponding column in $S_k$. Fig.~\ref{fig:amalur} shows the mapping matrices $M_1$, $M_2$, $M_3$ for source tables $S_1$, $S_2$, $S_3$, respectively. 
% In the technical report \cite{??}, we provide more examples and algorithms that generate mapping matrix, and the compressed indicator matrix in Sec.~\ref{sssec:imGen}.
 


% It is easy to see that the binary mapping matrices are often sparse.
% %--------------------------------------------------------%
% %--------------------recover-----------------------------%
% Because each attribute in the   source table $S_k$ is mapped to only one  attribute in $T$. 
% Thus, in each row of $M_k$ at most one element is 1, while the rest are 0.  
% Moreover, if an attribute of T does not have a mapped attribute in $S_k$, the corresponding row of the mapping matrix has only values of 0. For example, $ T.o$   (column ID: 3) does not have a mapped column in $S_1$, thus, the last row of  $M_1$ has only zeros, i.e., $M_1[3] = [0,0,0]$.
% %--------------------------------------------------------%


% Could add a figure proving this point, but Morpheus also used scipy spare matrices and did not provide motivation for this either

We use the \emph{indicator matrix} \cite{chen2017towards} 
% (denoted as $I_k$) 
to preserve the row matching between each source table $S_k$ and the target table $T$. 
An indicator matrix $I_k$ of size $c_T \times c_{k}$ describes how the rows of source table $S_k$ map to the rows of target table $T$, as in Fig.~\ref{fig:amalur}. 
 
\begin{definition}[\textit{Indicator matrix} \cite{chen2017towards}] Indicator matrices between source tables $S_1, S_2, ..., S_n$ and target table $T$ are a set of row vectors $\mathbf{I}= \{I_1, ..., I_n\}$. $I_k \ (k\in [1, n])$ is a row vector  of size $r_{T}$, where 
\small{
		\begin{align*}
			\begin{split} 
					I_k[i,j] =  \begin{cases}
$1$ & \text{if $j$-th row of $S_k$ mapped to $i$-th row of $T$} \\
$0$ & \text{otherwise}
\end{cases}
			\end{split}
		\end{align*}
	\label{def:cim}
}
\end{definition}
 

% Each source table $S_k$ has an indicator matrix $I_k$ and a mapping matrix $M_k$, which describe how the rows and columns of $S_k$ map to $T$, as shown in Fig.~\ref{fig:amalur}. 

\begin{table}[t]
\caption{Notations used in the paper.}
\label{tab:notations}
\vspace{1mm}
\centering
\footnotesize
\begin{tabular}{|l|l|} \hline
\textbf{Symbol} & \textbf{Description} \\ \hline
$S_k$ & The $k$-th source table \\ \hline
$T$ & Target table \\ \hline
% $y$ & The labels \\ \hline
$I_k$ & The indicator matrix for $S_k$\\ \hline
$M_k$ & The mapping matrix for $S_k$ \\ \hline
$j_T$          & The join type of $T$ (inner/left/outer/union) \\ \hline
$r_k/r_T$      & Number of rows in $S_k/T$ \\ \hline
$c_k/c_T$      & Number of columns in $S_k/T$ \\ \hline
$m_k/m_T/m_X/m_w$      & Number of nonzero elements in $S_k/T/X/w$ \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}
% We do not exclude the possibility of further optimization of the physical design of mapping and indicator matrices, which is an interesting engineering problem for future work. 

%------------------recover-------------------------%
% While in Ilargi we assume that we need an indicator matrix for each source table, previous work did not always make this assumption \cite{chen2017towards}. In Morpheus, the authors do not include an indicator matrix for the entity table in a star schema join, which is used as the first table in an inner or left join. Instead, each row in this entity table is matched to the same row in the target table. We can take this same approach in the inner and left outer join cases. We assume the entity matrix is $S_1$ and the corresponding indicator matrix is $I_1$.
%------------------recover-------------------------%
\vspace{-5mm}
\subsection{Rewriting Rules for Factorization }
\label{s:operations}
% \para{Factorized Linear Algebra operations}
With data and metadata represented in matrices, next, we explain how to rewrite a linear algebra over a target schema to linear algebra over source schemas. 
%--------------------------------------------------------------------
% The rewriting rules, although for arithmetic operations, share similar principles of view-based query rewriting \cite{deutsch2006query,chirkova2012materialized}. 
%--------------------------------------------------------------------
Here we use the example of LA operator \emph{left matrix multiplication (LMM)}. 
% The remaining LA operations, which are element-wise scalar operations, aggregation operations and multiplication operations, can be found in our extended technical report\footnote{\url{}. Appendix \ref{s:app_operations}.}. 
% Some of these operations return a materialized matrix, while other operations the turn a normalized matrix. Operations that return a normalized matrix allow us to exploit the rewrite rules again for a subsequent linear algebra operation. 
The full set of LA rewrite rules based on mapping and indicator matrices is in our technical report \cite{tech}. 



\para{Left Matrix Multiplication} Given a matrix $X$ with the size $c_T \times c_X$, LMM of T and X is denoted as $TX$.  The  LMM result of our target table matrix $T$ and another matrix $X$ is a matrix of size $r_T \times c_X$. Our rewrite of LMM goes as follows.
\[
TX \rightarrow  I_1 S_1 M_1^T X + ... + I_n S_n M_n^T X
\]
We first compute the local result  $I_{k} S_k M_k^{T}X\ (k\in [1, n])$ for each source table, 
then assemble them for the final results.

% For instance, for the running example the actual matrix multiplication  is $(I_{1} S_1) (M_1^{T} X)$ and $(I_{2} S_2) (M_2^{T} X)$.
% \todo[]{check with the order optimal}
%
%-------omit transpose----------------------------% 
% Performing LMM between two matrices where the first matrix is transposed is the same as multiplying the untransposed first matrix with the second matrix transposed and transposing this result. Therefore, we can define the rewrite in the transposed case as follows.

% \[
% T^TX \rightarrow (X^TT)^T
% \]
%-----------------------------------------------------
% \para{Implementation-level optimization} To reduce computation overhead,  our implementation adopts the optimal matrix multiplication order algorithm  \cite{cormen2022introduction} from the version included in NumPy\footnote{\url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.multi_dot.html}} for SciPy matrices, computed using matrix dimensions.
% % This method was created by Cormen \cite{cormen2022introduction}.
%-----------------------------------------------------

To showcase the effectiveness of matrix-represented DI metadata for factorized learning, we use Linear Regression as an example. In Alg.~\ref{alg:linear-regression}, we identify two key operators: matrix transpose and left matrix-matrix multiplication. By applying our rewriting strategies, these operations are pushed directly to the disparate data sources, enabling local computation and eliminating the need for data centralization, thus improving efficiency and scalability.

\begin{algorithm}[t!]
  \caption[Linear regression]{Linear regression using Gradient Descent
    ~\cite{chen2017towards}.}\label{alg:linear-regression}
  \begin{algorithmic}
    \Require $X, y , w, \gamma$
    \For{$i \in 1:n$} 
     \State $w = w - \gamma (\text{{$X^T$}}((\text{{$X w$}}) - y))$
    \EndFor
  \end{algorithmic}
\end{algorithm}
% \noindent\textbf{Why is Ilargi GPU-compatible?} Previous studies \cite{orion_learning_gen_lin_models,MorpheusFI,chen2017towards,khamis_acdc_2018} on linear algebra rewriting predominantly focused on CPU execution environments. This work leverages data integration metadata in matrix form to streamline the DI and ML pipeline with GPU-specific LA libraries, such as cuBLAS\footnote{\url{https://docs.nvidia.com/cuda/cublas/}}.
% The column and row matching between disparate sources (schema mapping and data matching) are natively sparse, as a single data source typically provides only partial information to the target table \cite{doan2012principles}. 
% To adapt the sparsity of the mapping and indicator matrices, Ilargi utilizes the Compressed Sparse Row (CSR) Format\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr\_matrix.html}}. 
% Moreover, we chose CuPy \cite{cupy_learningsys2017} to translate conceptual linear operators into physical GPU implementations, significantly enhancing the performance of model training over both factorization and materialization.


% \begin{figure*}[t]
%     \centering
%     % \includegraphics[width=0.93\linewidth]
%     \includegraphics[width=0.7\linewidth]{figures/hamlet_ML.pdf}
%      \vspace{-0.5cm}
%     \caption[Model training time over six real datasets: materialization vs. factorization]{
%     Models training time over seven real datasets: materialization vs. factorization
%     % Evaluation of Ilargi ML models with the number of iterations $= 20$ on Hamlet dataset. Annotations show the speedup of factorized learning compared to learning over materialized data.
%     }
%     \vspace{-0.3cm}
%     \label{fig:eval_ML_hamlet_Ilargi}
% \end{figure*}



% \subsection{Summary}

% \para{Applicability,  assumptions, and extensibility} Fig.~\ref{fig:applicability} summarizes Ilargi's applicability compared to SOTA solution \cite{chen2017towards}. 
% Ilargi can handle both normalized and denormalized tabular data.
% % , with 
% %  mapping matrices built between the schemas of source table matrices and the target schema of the training dataset. 
% % To prepare the training data,  feature and label columns are from source datasets, which means that for each column in the target table, we can find its corresponding column from at least one source schema. 
% % For simplicity, in the running example, all rows stored in the normalized data are mapped to at least one row in the target table. 
% % We follow the common data science assumption that the normal preprocessing steps include null value replacement and transformation of categorical variables to numerical values. 
% Similar to \cite{chen2017towards}, our input source table matrices (e.g., $S_1$, $S_2$, $S_3$ in Fig.~\ref{fig:amalur}) are preprocessed datasets in formats like NumPy arrays.  For simplicity, the running example replaces null values with zero, and applies one-hot-encoding for categorical variables.  Ilargi supports input data matrices whose null values are replaced to mean/median values or user-specified values, and other methods for handling categorical variables \cite{kelleher2020fundamentals}.
% % Binary variables can be encoded into zeroes and ones, where encoding the most common category into zeroes is preferred.
% % Ilargi can handle both normalized and unnormalized tabular data, and any mapping between rows and columns. This includes overlapping columns and any combination of overlapping rows between tables, e.g., an inner join, a left outer join, a right outer join, a full outer join, and unions. 
% % In this work, we do not propose a new data integration, data preprocessing, or ML algorithm. Our focus is to understand how data integration metadata (e.g., schema mapping, row matching) affects time-wise training efficiency, and how to choose between materialization and factorization to speed up the training process, which we discuss next.
% Our approach can serve as a fundamental component of federated learning frameworks for seamlessly incorporating DI metadata during the training process.
% % Moreover, we are currently working on extending Ilargi into an end-to-end data processing and model training pipeline. 
% For future work, we will expand Ilargi's support for more transformation operations like min-max scaling to enhance Ilargi's real-world usability further.


