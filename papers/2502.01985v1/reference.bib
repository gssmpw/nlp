
@techreport{MorpheusPy,
  title={MorpheusPy: Factorized Machine Learning with NumPy},
  author={Li, Side and Kumar, Arun},
  institution={Technical report, 2018. Available at https://adalabucsd. github. io/papers~…}
}

@book{doan2012principles,
author = {Doan, A. and Halevy, A. and Ives, Z.},
publisher = {Elsevier},
title = {{Principles of data integration}},
year = {2012}
}

@article{erickson2020autogluon,
  title={Autogluon-tabular: Robust and accurate automl for structured data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}


@inproceedings{hpc1,
  author       = {David E. Culler and
                  Richard M. Karp and
                  David A. Patterson and
                  Abhijit Sahay and
                  Klaus E. Schauser and
                  Eunice E. Santos and
                  Ramesh Subramonian and
                  Thorsten von Eicken},
  editor       = {Marina C. Chen and
                  Robert Halstead},
  title        = {{LogP: Towards a Realistic Model of Parallel Computation}},
  booktitle    = {{Proceedings of the Fourth {ACM} {SIGPLAN} Symposium on Principles
                  {\&} Practice of Parallel Programming (PPOPP), San Diego, California,
                  USA, May 19-22, 1993}},
  pages        = {1--12},
  publisher    = {{ACM}},
  year         = {1993},
  timestamp    = {Thu, 13 Apr 2023 19:55:43 +0200},
  biburl       = {https://dblp.org/rec/conf/ppopp/CullerKPSSSSE93.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{hpc2,
  author={Snavely, A. and Carrington, L. and Wolter, N. and Labarta, J. and Badia, R. and Purkayastha, A.},
  booktitle={{SC '02: Proceedings of the 2002 ACM/IEEE Conference on Supercomputing}}, 
  title={{A Framework for Performance Modeling and Prediction}}, 
  year={2002},
  volume={},
  number={},
  pages={21-21},
}

@ARTICLE{hpc3,
  author={Adve, V.S. and Bagrodia, R. and Browne, J.C. and Deelman, E. and Dube, A. and Houstis, E.N. and Rice, J.R. and Sakellariou, R. and Sundaram-Stukel, D.J. and Teller, P.J. and Vernon, M.K.},
  journal={{IEEE Transactions on Software Engineering}}, 
  title={{POEMS: end-to-end performance design of large parallel adaptive computational systems}}, 
  year={2000},
  volume={26},
  number={11},
  pages={1027-1048},
}

@inproceedings{blackbox1,
  author       = {Engin Ipek and
                  Bronis R. de Supinski and
                  Martin Schulz and
                  Sally A. McKee},
  editor       = {Jos{\'{e}} C. Cunha and
                  Pedro D. Medeiros},
  title        = {An Approach to Performance Prediction for Parallel Applications},
  booktitle    = {Euro-Par 2005, Parallel Processing, 11th International Euro-Par Conference,
                  Lisbon, Portugal, August 30 - September 2, 2005, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {3648},
  pages        = {196--205},
  publisher    = {Springer},
  year         = {2005},
  timestamp    = {Tue, 14 May 2019 10:00:46 +0200},
  biburl       = {https://dblp.org/rec/conf/europar/IpekSSM05.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{TPC-DI,
author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
title = {TPC-DI: The First Industry Benchmark for Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
month = {aug},
pages = {1367–1378},
numpages = {12}
}
@inproceedings{tvm,
  author       = {Tianqi Chen and
                  Thierry Moreau and
                  Ziheng Jiang and
                  Lianmin Zheng and
                  Eddie Q. Yan and
                  Haichen Shen and
                  Meghan Cowan and
                  Leyuan Wang and
                  Yuwei Hu and
                  Luis Ceze and
                  Carlos Guestrin and
                  Arvind Krishnamurthy},
  editor       = {Andrea C. Arpaci{-}Dusseau and
                  Geoff Voelker},
  title        = {{TVM:} An Automated End-to-End Optimizing Compiler for Deep Learning},
  booktitle    = {13th {USENIX} Symposium on Operating Systems Design and Implementation,
                  {OSDI} 2018, Carlsbad, CA, USA, October 8-10, 2018},
  pages        = {578--594},
  publisher    = {{USENIX} Association},
  year         = {2018},
  timestamp    = {Sat, 17 Dec 2022 01:15:29 +0100},
  biburl       = {https://dblp.org/rec/conf/osdi/ChenMJZYSCWHCGK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xgboost,
  author       = {Tianqi Chen and
                  Carlos Guestrin},
  editor       = {Balaji Krishnapuram and
                  Mohak Shah and
                  Alexander J. Smola and
                  Charu C. Aggarwal and
                  Dou Shen and
                  Rajeev Rastogi},
  title        = {XGBoost: {A} Scalable Tree Boosting System},
  booktitle    = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
                  Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
                  13-17, 2016},
  pages        = {785--794},
  publisher    = {{ACM}},
  year         = {2016},
  timestamp    = {Sat, 17 Dec 2022 01:15:30 +0100},
  biburl       = {https://dblp.org/rec/conf/kdd/ChenG16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{halide,
  author       = {Andrew Adams and
                  Karima Ma and
                  Luke Anderson and
                  Riyadh Baghdadi and
                  Tzu{-}Mao Li and
                  Micha{\"{e}}l Gharbi and
                  et al.},
  title        = {Learning to optimize halide with tree search and random programs},
  journal      = {{ACM} Trans. Graph.},
  volume       = {38},
  number       = {4},
  pages        = {121:1--121:12},
  year         = {2019},
  timestamp    = {Wed, 12 Aug 2020 09:11:22 +0200},
  biburl       = {https://dblp.org/rec/journals/tog/AdamsMABLGSJFDR19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rahm2001survey,
  title={A survey of approaches to automatic schema matching},
  author={Rahm, Erhard and Bernstein, Philip A},
  journal={the VLDB Journal},
  volume={10},
  number={4},
  pages={334--350},
  year={2001},
  publisher={Springer}
}

@incollection{fagin2009clio,
author = {Fagin, Ronald and Haas, Laura M and Hern{\'{a}}ndez, Mauricio and Miller, Ren{\'{e}}e J and Popa, Lucian and Velegrakis, Yannis},
booktitle = {ER},
pages = {198--236},
publisher = {Springer},
title = {{Clio: Schema mapping creation and data exchange}},
year = {2009}
}

@article{brizan2006survey,
  title={A. survey of entity resolution and record linkage methodologies},
  author={Brizan, David Guy and Tansel, Abdullah Uz},
  journal={Communications of the IIMA},
  volume={6},
  number={3},
  pages={5},
  year={2006}
}

@inproceedings{agrawalDataPlatformMachine2019,
  title = {Data {{Platform}} for {{Machine Learning}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Agrawal, Pulkit and Arya, Rajat and Bindal, Aanchal and Bhatia, Sandeep and Gagneja, Anupriya and Godlewski, Joseph and Low, Yucheng and Muss, Timothy and Paliwal, Mudit Manu and Raman, Sethu and Shah, Vishrut and Shen, Bochao and Sugden, Laura and Zhao, Kaiyu and Wu, Ming-Chuan},
  date = {2019-06-25},
  pages = {1803--1816},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  
  url = {https://dl.acm.org/doi/10.1145/3299869.3314050},
  urldate = {2022-06-15},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/8V246IPB/Agrawal et al. - 2019 - Data Platform for Machine Learning.pdf}
}



@article{andersonBrainwashDataSystem,
  title = {Brainwash: {{A Data System}} for {{Feature Engineering}}},
  author = {Anderson, Michael and Antenucci, Dolan and Bittorf, Victor and Burgess, Matthew and Cafarella, Michael and Kumar, Arun and Niu, Feng and Park, Yongjoo and Ré, Christopher and Zhang, Ce},
  pages = {4},
  abstract = {A new generation of data processing systems, including web search, Google’s Knowledge Graph, IBM’s Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, finance, energy, and general business. But building them can be challenging, even for computer scientists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/HRS4JFTA/Anderson et al. - Brainwash A Data System for Feature Engineering.pdf}
}

@inproceedings{castrofernandezAurumDataDiscovery2018,
  title = {Aurum: {{A Data Discovery System}}},
  shorttitle = {Aurum},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Castro Fernandez, Raul and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  date = {2018-04},
  pages = {1001--1012},
  issn = {2375-026X},
  
  abstract = {Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-defined schemas or a small number of data sources—instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph (EKG), which helps users to navigate among disparate sources. The contribution of this paper is AURUM, a system to build, maintain and query the EKG. To build the EKG, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the EKG without re-reading all data every time, we introduce a resource-efficient sampling signature (RESS) method which works by only using a small sample of the data. Finally, to query the EKG, we introduce a collection of composable primitives, thus allowing users to define many different types of discovery queries. We describe our experience using AURUM in three corporate scenarios and do a performance evaluation of each component.},
  eventtitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Buildings,Companies,data discovery,Electrocardiography,enterprise knowledge graph,Indexes,Lakes,notion},
  file = {/Users/jessie/Zotero/storage/BEV7JDDS/Castro Fernandez et al. - 2018 - Aurum A Data Discovery System.pdf}
}

@inproceedings{castrofernandezAurumDataDiscovery2018a,
  title = {Aurum: {{A Data Discovery System}}},
  shorttitle = {Aurum},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Castro Fernandez, Raul and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  date = {2018-04},
  pages = {1001--1012},
  issn = {2375-026X},
  
  abstract = {Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-defined schemas or a small number of data sources—instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph (EKG), which helps users to navigate among disparate sources. The contribution of this paper is AURUM, a system to build, maintain and query the EKG. To build the EKG, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the EKG without re-reading all data every time, we introduce a resource-efficient sampling signature (RESS) method which works by only using a small sample of the data. Finally, to query the EKG, we introduce a collection of composable primitives, thus allowing users to define many different types of discovery queries. We describe our experience using AURUM in three corporate scenarios and do a performance evaluation of each component.},
  eventtitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Buildings,Companies,data discovery,Electrocardiography,enterprise knowledge graph,Indexes,Lakes,notion},
  file = {/Users/jessie/Zotero/storage/R6XP3A2E/Castro Fernandez et al. - 2018 - Aurum A Data Discovery System.pdf;/Users/jessie/Zotero/storage/6EGNKLYJ/8509315.html}
}

@article{chaiDataManagementMachine2022,
  title = {Data {{Management}} for {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Data {{Management}} for {{Machine Learning}}},
  author = {Chai, Chengliang and Wang, Jiayi and Luo, Yuyu and Niu, Zeping and Li, Guoliang},
  date = {2022},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  
  abstract = {Machine learning(ML) has widespread applications and has revolutionized many industries, but suffers from several challenges. First, sufficient high-quality training data is inevitable for producing a well-performed model, but the data is always human expensive to acquire.Second, a large amount of training data and complicated model structures lead to the inefficiency of training and inference. Third, given an ML task, one always needs to train lots of models, which are hard to manage in real applications. Fortunately, database techniques can benefit ML by addressing the above three challenges. In this paper, we review existing studies from the following three aspects along with the pipeline highly related to ML. (1) Data preparation(Pre-ML): it focuses on preparing high-quality training data that can improve the performance of the ML model, where we review data discovery, data cleaning and data labeling. (2) Model training \& inference(In-ML): researchers in ML community focus on improving the model performance during training, while in this survey we mainly study how to accelerate the entire training process, also including feature selection and model selection. (3) Model management(Post-ML): in this part, we survey how to store, query, deploy and debug the models after training. Finally, we provide research challenges and future directions.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Cleaning,Computational modeling,Data models,Data preparation,Database,Machine learning,Model inference,Model training,notion,Optimization,Task analysis,Training,Training data},
  file = {/Users/jessie/Zotero/storage/Z5ZTXTAB/Chai et al. - 2022 - Data Management for Machine Learning A Survey.pdf;/Users/jessie/Zotero/storage/2QSLFQTU/9705125.html}
}



@inproceedings{chengNonlinearModelsNormalized2019,
  title = {Nonlinear {{Models Over Normalized Data}}},
  author = {Cheng, Zhaoyue and Koudas, Nick},
  date = {2019-04},
  pages = {1574--1577},
  
  abstract = {Machine Learning (ML) applications are proliferating in the enterprise. Increasingly enterprise data are used to build sophisticated ML models to assist critical business functions. Relational data which are prevalent in enterprise applications are typically normalized; as a result data have to be denormalized via primary/foreign-key joins to be provided as input to ML algorithms. In this paper we study the implementation of popular nonlinear ML models and in particular independent Gaussian Mixture Models (IGMM) over normalized data. For the case of IGMM we propose algorithms taking the statistical properties of the Gaussians into account to construct mixture models, factorizing the computation. In that way we demonstrate that we can conduct the training of the models much faster compared to other applicable approaches, without any loss in accuracy. We present the results of a thorough experimental evaluation, varying several parameters of the input relations involved and demonstrate that our proposals both for the case of IGMM yield drastic performance improvements which become increasingly higher as parameters of the underlying data vary, without any loss in accuracy.},
  eventtitle = {2019 {{IEEE}} 35th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Analytical models,Computational modeling,Convergence,data management,Data models,databases,Gaussian distribution,Gaussian mixture model,machine learning,no linear models,normalized databases,notion},
  file = {/Users/jessie/Zotero/storage/EC3LUNBJ/Cheng and Koudas - 2019 - Nonlinear Models Over Normalized Data.pdf;/Users/jessie/Zotero/storage/8FZK53ZZ/8731358.html}
}

@inproceedings{andra2024,
  title = {AutoFeat: Transitive Feature Discovery over Join Paths},
booktitle = {IEEE 40th International Conference on  Data Engineering (ICDE)},
  author = {Andra Ionescu and Kiril Vasilev and Florena Buse and Rihan Hai and Asterios Katsifodimos},
  date = {2024-05},
  note = {to appear}
}


 

@inproceedings{chenRedundancyEliminationDistributed2022,
  title = {Redundancy {{Elimination}} in {{Distributed Matrix Computation}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chen, Zihao and Han, Baokun and Xu, Chen and Qian, Weining and Zhou, Aoying},
  date = {2022-06-10},
  pages = {573--586},
  publisher = {{ACM}},
  location = {{Philadelphia PA USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3514221.3517877},
  urldate = {2022-06-16},
  abstract = {As matrix computation becomes increasingly prevalent in largescale data analysis, distributed matrix computation solutions have emerged. These solutions support query interfaces of linear algebra expressions, which often contain redundant subexpressions, i.e., common and loop-constant subexpressions. Hence, existing compilers rewrite queries to eliminate such redundancy. However, due to the large search space, they fail to find all redundant subexpressions, especially for matrix multiplication chains. Furthermore, redundancy elimination may change the original execution order of operators, and have negative impacts. To reduce the large search space and avoid the negative impacts, we propose automatic elimination and adaptive elimination, respectively. In particular, automatic elimination adopts a block-wise search that exploits the properties of matrix computation for speed-up. Adaptive elimination employs a cost model and a dynamic programming-based method to generate efficient plans for redundancy elimination. Finally, we implement ReMac atop SystemDS, eliminating redundancy in distributed matrix computation. In our experiments, ReMac is able to generate efficient execution plans at affordable overhead costs, and outperforms state-of-the-art solutions by an order of magnitude.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '22: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/N4U2TEJ6/Chen et al. - 2022 - Redundancy Elimination in Distributed Matrix Compu.pdf}
}

 

@article{convolboCostawareDAGScheduling2016,
  title = {Cost-Aware {{DAG}} Scheduling Algorithms for Minimizing Execution Cost on Cloud Resources},
  author = {Convolbo, Moïse W. and Chou, Jerry},
  date = {2016-03},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  volume = {72},
  number = {3},
  pages = {985--1012},
  issn = {0920-8542, 1573-0484},
  
  url = {http://link.springer.com/10.1007/s11227-016-1637-7},
  urldate = {2022-06-14},
  abstract = {Directed acyclic graph (DAG) scheduling is a well-known problem, because a DAG can be used to describe a wide range of complex applications, including scientific applications and parallel computing jobs. Most DAG scheduling algorithms were proposed to minimize the job makespan (i.e., execution time) on a multiprocessor computer or cluster. However, as the cost-driven public cloud services have become an attractive and popular platform for providing computing resources, cost minimization emerges as a new critical issue. Therefore, the objective of this work is to propose and solve the cost optimization problem for scheduling DAGs on an IaaS cloud platform where task scheduling must cope with resource provisioning to achieve the optimal solution. In this paper, we proposed both optimal and heuristic scheduling algorithms, and we evaluated them across a variety of DAGs using the price model from EC2. Comparing to other cost-oblivious DAG schedules that aim to minimize makespan or resource usage, the results show that our cost-aware heuristic algorithm can reduce cost by 20–50 \% and achieve a cost within x1.16 of the optimal one.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/3YWGZ25L/Convolbo and Chou - 2016 - Cost-aware DAG scheduling algorithms for minimizin.pdf}
}

@unpublished{curtinRkmeansFastClustering2019,
  title = {Rk-Means: {{Fast Clustering}} for {{Relational Data}}},
  shorttitle = {Rk-Means},
  author = {Curtin, Ryan and Moseley, Ben and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
  date = {2019-10-10},
  eprint = {1910.04939},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.04939},
  abstract = {Conventional machine learning algorithms cannot be applied until a data matrix is available to process. When the data matrix needs to be obtained from a relational database via a feature extraction query, the computation cost can be prohibitive, as the data matrix may be (much) larger than the total input relation size. This paper introduces Rk-means, or relational k-means algorithm, for clustering relational data tuples without having to access the full data matrix. As such, we avoid having to run the expensive feature extraction query and storing its output. Our algorithm leverages the underlying structures in relational data. It involves construction of a small grid coreset of the data matrix for subsequent cluster construction. This gives a constant approximation for the k-means objective, while having asymptotic runtime improvements over standard approaches of first running the database query and then clustering. Empirical results show orders-of-magnitude speedup, and Rk-means can run faster on the database than even just computing the data matrix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/jessie/Zotero/storage/VHIGYGD8/Curtin et al. - 2019 - Rk-means Fast Clustering for Relational Data.pdf}
}

@inproceedings{deboeIntegratedMLEverySQL2020,
  title = {{{IntegratedML}}: {{Every SQL Developer}} Is a {{Data Scientist}}},
  shorttitle = {{{IntegratedML}}},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  author = {De Boe, Benjamin and Woodfin, Tom and Dyar, Thomas and McCaldon, Dave and Djakovic, Aleks and MacLeod, Alex and Woodlock, Don},
  date = {2020-06-14},
  pages = {1--4},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3399579.3399866},
  urldate = {2022-06-15},
  abstract = {The adoption of machine learning in business software is slowed down by a shortage of data science talent and challenges around efficient operationalization of machine learning models. We present IntegratedML, an embedded database capability for machine learning. This paper describes how IntegratedML provides developers with access to state-of-the-art machine learning platforms using intuitive SQL syntax. Its embedded feature extraction and algorithm selection enable fully automated model building, while model inferencing is exposed through a simple scalar function. The novelty of IntegratedML is in the deep integration into the embedding relational engine, which hides pipeline complexity from the user and guarantees high efficiencies, both at train and inference time.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8023-2},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/YQC9H5FK/De Boe et al. - 2020 - IntegratedML Every SQL Developer is a Data Scient.pdf}
}

@article{dsilvaAIDAAbstractionAdvanced2018,
  title = {{{AIDA}}: Abstraction for Advanced in-Database Analytics},
  shorttitle = {{{AIDA}}},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2018-07},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {11},
  pages = {1400--1413},
  
  url = {https://dl.acm.org/doi/10.14778/3236187.3236194},
  abstract = {With the tremendous growth in data science and machine learning, it has become increasingly clear that traditional relational database management systems (RDBMS) are lacking appropriate support for the programming paradigms required by such applications, whose developers prefer tools that perform the computation outside the database system. While the database community has attempted to integrate some of these tools in the RDBMS, this has not swayed the trend as existing solutions are often not convenient for the incremental, iterative development approach used in these fields. In this paper, we propose AIDA - an abstraction for advanced in-database analytics. AIDA emulates the syntax and semantics of popular data science packages but transparently executes the required transformations and computations inside the RDBMS. In particular, AIDA works with a regular Python interpreter as a client to connect to the database. Furthermore, it supports the seamless use of both relational and linear algebra operations using a unified abstraction. AIDA relies on the RDBMS engine to efficiently execute relational operations and on an embedded Python interpreter and NumPy to perform linear algebra operations. Data reformatting is done transparently and avoids data copy whenever possible. AIDA does not require changes to statistical packages or the RDBMS facilitating portability.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/5GJ5884P/D'silva et al. - 2018 - AIDA abstraction for advanced in-database analyti.pdf}
}

@inproceedings{dsilvaKeepYourHost2019,
  title = {Keep {{Your Host Language Object}} and {{Also Query}} It: {{A Case}} for {{SQL Query Support}} in {{RDBMS}} for {{Host Language Objects}}},
  shorttitle = {Keep {{Your Host Language Object}} and {{Also Query}} It},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2019-07-23},
  pages = {133--144},
  publisher = {{ACM}},
  location = {{Santa Cruz CA USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3335783.3335798},
  abstract = {As a result of prolific growth in data science and machine learning applications, modern relational database management systems (RDBMS) are experimenting with various approaches to facilitate advanced analytical computations, in addition to the relational operations that they traditionally support. The most common approach has been to integrate an embedded high level language (HLL) interpreter into the RDBMS along with any additional libraries that specialize in numerical computations. Such implementations, e.g., user defined functions (UDFs), follow generally a black-box setup, and for many complex workflows that require datasets to be passed and processed back-and-forth between the query execution engine and the embedded HLL interpreter, optimization opportunities are not fully explored yet. In this paper, we propose and implement the concept of virtual tables that can be used to expose data set objects maintained by the embedded HLL interpreter to the query engine for executing relational operations. Unlike prevalent solutions, our approach minimizes the need for performing data copies and conversions, performing them lazily when required. It also facilitates better optimization opportunities for the execution of SQL queries as the RDBMS is able to analyze the data characteristics of the HLL objects before producing an execution plan. The approach is also programmer friendly, allowing for a more intuitive implementation of computational workflows. We perform evaluations over a variety of workloads which demonstrate the performance and programming benefits of virtual tables.},
  eventtitle = {{{SSDBM}} '19: 31st {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/2T9IQFYW/D'silva et al. - 2019 - Keep Your Host Language Object and Also Query it .pdf}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and et al.},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@article{dsilvaMakingRDBMSData2019,
  title = {Making an {{RDBMS}} Data Scientist Friendly: Advanced in-Database Interactive Analytics with Visualization Support},
  shorttitle = {Making an {{RDBMS}} Data Scientist Friendly},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2019-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {12},
  number = {12},
  pages = {1930--1933},
  
  url = {https://dl.acm.org/doi/10.14778/3352063.3352102},
  abstract = {We are currently witnessing the rapid evolution and adoption of various data science frameworks that function external to the database. Any support from conventional RDBMS implementations for data science applications has been limited to procedural paradigms such as user-defined functions (UDFs) that lack exploratory programming support. Therefore, the current status quo is that during the exploratory phase, data scientists usually use the database system as the “data storage” layer of the data science framework, whereby the majority of computation and analysis is performed outside the database, e.g., at the client node. We demonstrate AIDA, an in-database framework for data scientists. AIDA allows users to write interactive Python code using a development environment such as a Jupyter notebook. The actual execution itself takes place inside the database (neardata), where a server component of AIDA, that resides inside the embedded Python interpreter of the RDBMS, manages the data sets and computations. The demonstration will also show the visualization capabilities of AIDA where the progress of computation can be observed through live updates. Our evaluations show that AIDA performs several times faster compared to contemporary external data science frameworks, but is much easier to use for exploratory development compared to database UDFs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/TYPXW33N/D'silva et al. - 2019 - Making an RDBMS data scientist friendly advanced .pdf}
}

@inproceedings{fonsecaMultiDAGModelRealtime2015,
  title = {A Multi-{{DAG}} Model for Real-Time Parallel Applications with Conditional Execution},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Fonseca, José Carlos and Nélis, Vincent and Raravi, Gurulingesh and Pinho, Luís Miguel},
  date = {2015-04-13},
  pages = {1925--1932},
  publisher = {{ACM}},
  location = {{Salamanca Spain}},
  
  abstract = {Owing to the current trends for higher performance and the ever growing availability of multiprocessors in the embedded computing (EC) domain, there is nowadays a strong push towards the parallelization of modern embedded applications. Several real-time task models have recently been proposed to capture different forms of parallelism. However, they do not deal explicitly with control flow information as they assume that all the threads of a parallel task must execute every time the task is activated. In contrast, in this paper, we present a multi-DAG model where each task is characterized by a set of execution flows, each of which represents a different execution path throughout the task code and is modeled as a DAG of sub-tasks. We propose a two-step solution that computes a single synchronous DAG of servers for a task modeled by a multi-DAG and show that these servers are able to supply every execution flow of that task with the required cpu-budget so that the task can execute entirely, irrespective of the execution flow taken at run-time, while satisfying its precedence constraints. As a result, each task can be modeled by its single DAG of servers, which facilitates in leveraging the existing single-DAG schedulability analyses techniques for analyzing the schedulability of parallel tasks with multiple execution flows.},
  eventtitle = {{{SAC}} 2015: {{Symposium}} on {{Applied Computing}}},
  isbn = {978-1-4503-3196-8},
  langid = {english},
  keywords = {notion},
}

@inproceedings{haiAmalurNextgenerationData2022,
  title = {Amalur: {{Next-generation Data Integration}} in {{Data Lakes}}},
  booktitle = {2022 {{CIDR}} 12th {{Annual Conference}} on {{Innovative Data Systems Research}}},
  author = {Hai, Rihan and Koutras, Christos and Ionescu, Andra and Katsifodimos, Asterios},
  date = {2022-09-01},
  pages = {1},
  location = {{Chaminade, USA}},
  abstract = {Data science workflows often require extracting, preparing and integrating data from multiple data sources. This is a cumbersome and slow process: most of the times, data scientists prepare data in a data processing system or a data lake, and export it as a table, in order for it to be consumed by a Machine Learning (ML) algorithm. Recent advances in the area of factorized ML, allow us to push down certain linear algebra (LA) operators, executing them closer to the data sources. With this work, we revisit classic data integration (DI) systems and see how these fit into modern data lakes that are meant to support LA as a first-class citizen.},
  eventtitle = {2022 {{CIDR}} 12th {{Annual Conference}} on {{Innovative Data Systems Research}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/4PAWEI7L/Amalur Next-generation Data Integration in Data L.pdf}
}

@inproceedings{halevyGoodsOrganizingGoogle2016,
  title = {Goods: {{Organizing Google}}'s {{Datasets}}},
  shorttitle = {Goods},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Halevy, Alon and Korn, Flip and Noy, Natalya F. and Olston, Christopher and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong},
  date = {2016-06-14},
  pages = {795--806},
  publisher = {{ACM}},
  location = {{San Francisco California USA}},
  
  url = {https://dl.acm.org/doi/10.1145/2882903.2903730},
  urldate = {2022-04-05},
  abstract = {Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in different storage systems, may vary in their formats, may change every day. In this paper, we present Goods, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. Goods extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them. We discuss the technical challenges that we had to overcome in order to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level datamanagement systems in general.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'16: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/IU2275QC/Halevy et al. - 2016 - Goods Organizing Google's Datasets.pdf}
}

@online{IntegratedMLProceedingsFourth,
  title = {{{IntegratedML}} | {{Proceedings}} of the {{Fourth International Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  langid = {english},
  organization = {{ACM Conferences}},
  keywords = {notion}
}

@article{justoPolyglotFrameworkFactorized2021,
  title = {Towards a Polyglot Framework for Factorized {{ML}}},
  author = {Justo, David and Yi, Shaoqing and Stadler, Lukas and Polikarpova, Nadia and Kumar, Arun},
  date = {2021-07-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {14},
  number = {12},
  pages = {2918--2931},
  issn = {2150-8097},
  
  urldate = {2022-06-15},
  abstract = {Optimizing machine learning (ML) workloads on structured data is a key concern for data platforms. One class of optimizations called "factorized ML" helps reduce ML runtimes over multi-table datasets by pushing ML computations down through joins, avoiding the need to materialize such joins. The recent Morpheus system automated factorized ML to any ML algorithm expressible in linear algebra (LA). But all such prior factorized ML/LA stacks are restricted by their chosen programming language (PL) and runtime environment, limiting their reach in emerging industrial data science environments with many PLs (R, Python, etc.) and even cross-PL analytics workflows. Re-implementing Morpheus from scratch in each PL/environment is a massive developability overhead for implementation, testing, and maintenance. We tackle this challenge by proposing a new system architecture, Trinity, to enable factorized LA logic to be written only once and easily reused across many PLs/LA tools in one go. To do this in an extensible and efficient manner without costly data copies, Trinity leverages and extends an emerging industrial polyglot compiler and runtime, Oracle's GraalVM. Trinity enables factorized LA in multiple PLs and even cross-PL workflows. Experiments with real datasets show that Trinity is significantly faster than materialized execution ({$>$} 8x speedups in some cases), while being largely competitive to a prior single PL-focused Morpheus stack.},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/7W6G64YU/Justo et al. - 2021 - Towards a polyglot framework for factorized ML.pdf}
}

@inproceedings{kandoganLabBookMetadatadrivenSocial2015,
  title = {{{LabBook}}: {{Metadata-driven}} Social Collaborative Data Analysis},
  shorttitle = {{{LabBook}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Kandogan, Eser and Roth, Mary and Schwarz, Peter and Hui, Joshua and Terrizzano, Ignacio and Christodoulakis, Christina and Miller, Renée J.},
  date = {2015-10},
  pages = {431--440},
  
  abstract = {Open data analysis platforms are being adopted to support collaboration in science and business. Studies suggest that analytic work in an enterprise occurs in a complex ecosystem of people, data, and software working in a coordinated manner. These studies also point to friction between the elements of this ecosystem that reduces user productivity and quality of work. LabBook is an open, social, and collaborative data analysis platform designed explicitly to reduce this friction and accelerate discovery. Its goal is to help users leverage each other's knowledge and experience to find the data, tools and collaborators they need to integrate, visualize, and analyze data. The key insight is to collect and use more metadata about all elements of the analytic ecosystem by means of an architecture and user experience that reduce the cost of contributing such metadata. We demonstrate how metadata can be exploited to improve the collaborative user experience and facilitate collaborative data integration and recommendations. We describe a specific use case and discuss several design issues concerning the capture, representation, querying and use of metadata.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Bioinformatics,Business,collaboration,Collaboration,Data analysis,data analytics,data discovery,Ecosystems,metadata,Metadata,notion,Semantics},
  file = {/Users/jessie/Zotero/storage/5TFM83XX/Kandogan et al. - 2015 - LabBook Metadata-driven social collaborative data.pdf;/Users/jessie/Zotero/storage/PYWUNC7G/7363784.html}
}

@article{karaDoppioDBHardwareTechniques2019,
  title = {{{doppioDB}} 2.0: Hardware Techniques for Improved Integration of Machine Learning into Databases},
  shorttitle = {{{doppioDB}} 2.0},
  author = {Kara, Kaan and Wang, Zeke and Zhang, Ce and Alonso, Gustavo},
  date = {2019-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {12},
  number = {12},
  pages = {1818--1821},
  issn = {2150-8097},
  
  abstract = {Database engines are starting to incorporate machine learning (ML) functionality as part of their repertoire. Machine learning algorithms, however, have very different characteristics than those of relational operators. In this demonstration, we explore the challenges that arise when integrating generalized linear models into a database engine and how to incorporate hardware accelerators into the execution, a tool now widely used for ML workloads.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/T73999GR/Kara et al. - 2019 - doppioDB 2.0 hardware techniques for improved int.pdf}
}

 
@inproceedings{kimTensorDBInDatabaseTensor2014,
  title = {{{TensorDB}}: {{In-Database Tensor Manipulation}} with {{Tensor-Relational Query Plans}}},
  shorttitle = {{{TensorDB}}},
  booktitle = {Proceedings of the 23rd {{ACM International Conference}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Kim, Mijung and Candan, K. Selçuk},
  date = {2014-11-03},
  pages = {2039--2041},
  publisher = {{ACM}},
  location = {{Shanghai China}},
  
  abstract = {Today’s data management systems increasingly need to support both tensor-algebraic operations (for analysis) as well as relational-algebraic operations (for data manipulation and integration). Tensor decomposition techniques are commonly used for discovering underlying structures of multidimensional data sets. However, as the relevant data sets get large, existing in-memory schemes for tensor decomposition become increasingly ineffective and, instead, memoryindependent solutions, such as in-database analytics, are necessitated. We introduce an in-database analytic system for efficient implementations of in-database tensor decompositions on chunk-based array data stores, so called, TensorDB. TensorDB includes static in-database tensor decomposition and dynamic in-database tensor decomposition operators. TensorDB extends an array database and leverages array operations for data manipulation and integration. TensorDB supports complex data processing plans where multiple relational algebraic and tensor algebraic operations are composed with each other.},
  eventtitle = {{{CIKM}} '14: 2014 {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/H886I96M/Kim and Candan - 2014 - TensorDB In-Database Tensor Manipulation with Ten.pdf}
}

@article{koutrasValentineActionMatching2021,
  title = {Valentine in Action: Matching Tabular Data at Scale},
  shorttitle = {Valentine in Action},
  author = {Koutras, Christos and Psarakis, Kyriakos and Siachamis, George and Ionescu, Andra and Fragkoulis, Marios and Bonifati, Angela and Katsifodimos, Asterios},
  date = {2021-07},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {14},
  number = {12},
  pages = {2871--2874},
  
  abstract = {Capturing relationships among heterogeneous datasets in large data lakes – traditionally termed schema matching – is one of the most challenging problems that corporations and institutions face nowadays. Discovering and integrating datasets heavily relies on the effectiveness of the schema matching methods in use. However, despite the wealth of research, evaluation of schema matching methods is still a daunting task: there is a lack of openly-available datasets with ground truth, reference method implementations, and comprehensible GUIs that would facilitate development of both novel state-of-the-art schema matching techniques and novel data discovery methods.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VS7G3S3U/Koutras et al. - 2021 - Valentine in action matching tabular data at scal.pdf}
}

@inproceedings{koutrasValentineEvaluatingMatching2021,
  title = {Valentine: {{Evaluating Matching Techniques}} for {{Dataset Discovery}}},
  shorttitle = {Valentine},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Koutras, Christos and Siachamis, George and Ionescu, Andra and Psarakis, Kyriakos and Brons, Jerry and Fragkoulis, Marios and Lofi, Christoph and Bonifati, Angela and Katsifodimos, Asterios},
  date = {2021-04},
  pages = {468--479},
  publisher = {{IEEE}},
  location = {{Chania, Greece}},
  
  url = {https://ieeexplore.ieee.org/document/9458921/},
  abstract = {Data scientists today search large data lakes to discover and integrate datasets. In order to bring together disparate data sources, dataset discovery methods rely on some form of schema matching: the process of establishing correspondences between datasets. Traditionally, schema matching has been used to find matching pairs of columns between a source and a target schema. However, the use of schema matching in dataset discovery methods differs from its original use. Nowadays schema matching serves as a building block for indicating and ranking inter-dataset relationships. Surprisingly, although a discovery method’s success relies highly on the quality of the underlying matching algorithms, the latest discovery methods employ existing schema matching algorithms in an ad-hoc fashion due to the lack of openly-available datasets with ground truth, reference method implementations, and evaluation metrics.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/U2ZZUSIG/Koutras et al. - 2021 - Valentine Evaluating Matching Techniques for Data.pdf}
}

@inproceedings{kumarDataManagementMachine2017,
  title = {Data {{Management}} in {{Machine Learning}}: {{Challenges}}, {{Techniques}}, and {{Systems}}},
  shorttitle = {Data {{Management}} in {{Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
  date = {2017-05-09},
  pages = {1717--1722},
  publisher = {{ACM}},
  location = {{Chicago Illinois USA}},
  
  abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/6AX2EQ2K/Kumar et al. - 2017 - Data Management in Machine Learning Challenges, T.pdf}
}



@inproceedings{kumarJoinNotJoin2016,
  title = {To {{Join}} or {{Not}} to {{Join}}?: {{Thinking Twice}} about {{Joins}} before {{Feature Selection}}},
  shorttitle = {To {{Join}} or {{Not}} to {{Join}}?},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M. and Zhu, Xiaojin},
  date = {2016-06-14},
  pages = {19--34},
  publisher = {{ACM}},
  location = {{San Francisco California USA}},
  
  abstract = {Closer integration of machine learning (ML) with data processing is a booming area in both the data management industry and academia. Almost all ML toolkits assume that the input is a single table, but many datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins to obtain features from all base tables and apply a feature selection method, either explicitly or implicitly, with the aim of improving accuracy. In this work, we show that the features brought in by such joins can often be ignored without affecting ML accuracy significantly, i.e., we can “avoid joins safely.” We identify the core technical issue that could cause accuracy to decrease in some cases and analyze this issue theoretically. Using simulations, we validate our analysis and measure the effects of various properties of normalized data on accuracy. We apply our analysis to design easy-to-understand decision rules to predict when it is safe to avoid joins in order to help analysts exploit this runtime-accuracy trade-off. Experiments with multiple real normalized datasets show that our rules are able to accurately predict when joins can be avoided safely, and in some cases, this led to significant reductions in the runtime of some popular feature selection methods.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'16: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/3HTYW89I/Kumar et al. - 2016 - To Join or Not to Join Thinking Twice about Join.pdf}
}


@inproceedings{lamAutomatedDataScience2021,
  title = {Automated {{Data Science}} for {{Relational Data}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Lam, Hoang Thanh and Buesser, Beat and Min, Hong and Minh, Tran Ngoc and Wistuba, Martin and Khurana, Udayan and Bramble, Gregory and Salonidis, Theodoros and Wang, Dakuo and Samulowitz, Horst},
  date = {2021-04},
  pages = {2689--2692},
  issn = {2375-026X},
  
  abstract = {Feature engineering is a crucial but tedious task that requires up to 80\% of the total time in data science projects. A significant challenge is when data consists of tables from different data sources, thus data scientists need to wisely aggregate and join tables while performing feature engineering task. In this work, we demonstrate a novel system called OneBM (One Button Machine), that enables data scientists to increase their efficiency with automated feature engineering for relational data. OneBM takes as input a relational dataset with multiple tables and its entity relation diagram (ERD) which can be declared with a novel, easy-to-use drag-and-drop graphical user interface. The system then automatically identifies and executes relevant joins and aggregates in the data, and generates new features with a rich set of transformations for various types of data including but not limited to time-series, sequences, number sets and itemsets, etc. The generated features then can be used by automated model selection and hyper-parameter optimization algorithms to complete a fully end-to-end automated data science (or AutoDS) workflow. A follow-up user evaluation illustrated how data scientists can perform multi-table feature engineering tasks in minutes using our system, compared to repeatedly coding SQL-like queries to transform and aggregate relational data requiring weeks of manual labor for comparable performance. In the live demos we plan to show two use cases with real-world datasets (video demos are available at the links in the footnote): sale prediction1 and call center user experience2. Pre-registered partcipants can play with these use-cases and the given datasets via Watson Studio on the cloud.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Automated Data Science,Automated feature engineering,Data aggregation,Data models,Data science,Encoding,Itemsets,Manuals,notion,Relational data,Transforms},
  file = {/Users/jessie/Zotero/storage/KQVM99JZ/Lam et al. - 2021 - Automated Data Science for Relational Data.pdf;/Users/jessie/Zotero/storage/SXJ9UWF7/9458886.html}
}

@misc{lamOneButtonMachine2017,
  title = {One Button Machine for Automating Feature Engineering in Relational Databases},
  author = {Lam, Hoang Thanh and Thiebaut, Johann-Michael and Sinn, Mathieu and Chen, Bei and Mai, Tiep and Alkan, Oznur},
  date = {2017-06-01},
  number = {arXiv:1706.00327},
  eprint = {1706.00327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1706.00327},
  urldate = {2022-06-15},
  abstract = {Feature engineering is one of the most important and time consuming tasks in predictive analytics projects. It involves understanding domain knowledge and data exploration to discover relevant hand-crafted features from raw data. In this paper, we introduce a system called One Button Machine, or OneBM for short, which automates feature discovery in relational databases. OneBM automatically performs a key activity of data scientists, namely, joining of database tables and applying advanced data transformations to extract useful features from data. We validated OneBM in Kaggle competitions in which OneBM achieved performance as good as top 16\% to 24\% data scientists in three Kaggle competitions. More importantly, OneBM outperformed the state-of-the-art system in a Kaggle competition in terms of prediction accuracy and ranking on Kaggle leaderboard. The results show that OneBM can be useful for both data scientists and non-experts. It helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,notion},
  file = {/Users/jessie/Zotero/storage/UWXSXDKS/Lam et al. - 2017 - One button machine for automating feature engineer.pdf}
}

@inproceedings{liEnablingOptimizingNonlinear2019,
  title = {Enabling and {{Optimizing Non-linear Feature Interactions}} in {{Factorized Linear Algebra}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Li, Side and Chen, Lingjiao and Kumar, Arun},
  date = {2019-06-25},
  pages = {1571--1588},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  
  abstract = {Accelerating machine learning (ML) over relational data is a key focus of the database community. While many real-world datasets are multi-table, most ML tools expect single-table inputs, forcing users to materialize joins before ML, leading to data redundancy and runtime waste. Recent works on “factorized ML” address such issues by pushing ML through joins. However, they have hitherto been restricted to ML models linear in the feature space, rendering them less effective when users construct non-linear feature interactions such as pairwise products to boost ML accuracy. In this work, we take a first step towards closing this gap by introducing a new abstraction to enable pairwise feature interactions in multi-table data and present an extensive framework of algebraic rewrite rules for factorized LA operators over feature interactions. Our rewrite rules carefully exploit the interplay of the redundancy caused by both joins and interactions. We prototype our framework in Python to build a tool we call MorpheusFI. An extensive empirical evaluation with both synthetic and real datasets shows that MorpheusFI yields up to 5x speedups over materialized execution for a popular second-order gradient method and even an order of magnitude speedups over a popular stochastic gradient method.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/GYJXJ3WX/Li et al. - 2019 - Enabling and Optimizing Non-linear Feature Interac.pdf}
}

@article{liMLogDeclarativeInDatabase,
  title = {{{MLog}}: {{Towards Declarative In-Database Machine Learning}}},
  author = {Li, Xupeng and Cui, Bin and Chen, Yiru and Wu, Wentao and Zhang, Ce},
  pages = {4},
  abstract = {We demonstrate MLOG, a high-level language that integrates machine learning into data management systems. Unlike existing machine learning frameworks (e.g., TensorFlow, Theano, and Caffe), MLOG is declarative, in the sense that the system manages all data movement, data persistency, and machine-learning related optimizations (such as data batching) automatically. Our interactive demonstration will show audience how this is achieved based on the novel notion of tensoral views (TViews), which are similar to relational views but operate over tensors with linear algebra. With MLOG, users can succinctly specify not only simple models such as SVM (in just two lines), but also sophisticated deep learning models that are not supported by existing in-database analytics systems (e.g., MADlib, PAL, and SciDB), as a series of cascaded TViews. Given the declarative nature of MLOG, we further demonstrate how query/program optimization techniques can be leveraged to translate MLOG programs into native TensorFlow programs. The performance of the automatically generated TensorFlow programs is comparable to that of hand-optimized ones.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/THNIVPK3/Li et al. - MLog Towards Declarative In-Database Machine Lear.pdf}
}

@article{luoScalableLinearAlgebra2019,
  title = {Scalable {{Linear Algebra}} on a {{Relational Database System}}},
  author = {Luo, Shangyu and Gao, Zekai J. and Gubanov, Michael and Perez, Luis L. and Jermaine, Christopher},
  date = {2019-07},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {7},
  pages = {1224--1238},
  
  abstract = {As data analytics has become an important application for modern data management systems, a new category of data management system has appeared recently: the scalable linear algebra system. In this paper, we argue that a parallel or distributed database system is actually an excellent platform upon which to build such functionality. Most relational systems already have support for cost-based optimization-which is vital to scaling linear algebra computations-and it is well-known how to make relational systems scale. We show that by making just a few changes to a parallel/distributed relational database system, such a system can be a competitive platform for scalable linear algebra. Taken together, our results should at least raise the possibility that brand new systems designed from the ground up to support scalable linear algebra are not absolutely necessary, and that such systems could instead be built on top of existing relational technology. Our results also suggest that if scalable linear algebra is to be added to a modern dataflow platform such as Spark, they should be added on top of the system's more structured (relational) data abstractions, rather than being constructed directly on top of the system's raw dataflow operators.},
  keywords = {Arrays,Buildings,Database systems,Distributed database systems,large scale linear algebra,Linear algebra,notion,Relational databases,Structured Query Language,vector/matrix},
  file = {/Users/jessie/Zotero/storage/GXBRV26P/Luo et al. - 2019 - Scalable Linear Algebra on a Relational Database S.pdf;/Users/jessie/Zotero/storage/66DDBGUS/8340060.html}
}

@article{makryniotiDeclarativeDataAnalytics2021,
  title = {Declarative {{Data Analytics}}: {{A Survey}}},
  shorttitle = {Declarative {{Data Analytics}}},
  author = {Makrynioti, Nantia and Vassalos, Vasilis},
  date = {2021-06},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {33},
  number = {6},
  pages = {2392--2411},
  issn = {1558-2191},
  
  abstract = {The area of declarative data analytics explores the application of the declarative paradigm on data science and machine learning. It proposes declarative languages for expressing data analysis tasks and develops systems which optimize programs written in those languages. The execution engine can be either centralized or distributed, as the declarative paradigm advocates independence from particular physical implementations. The survey explores a wide range of declarative data analysis frameworks by examining both the programming model and the optimization techniques used, in order to provide conclusions on the current state of the art in the area and identify open challenges.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Analytical models,Data analysis,data science,Declarative programming,large-scale analytics,machine learning,Mathematical model,notion,Optimization,Prediction algorithms,Programming,Task analysis},
  file = {/Users/jessie/Zotero/storage/VX7RNQ64/Makrynioti and Vassalos - 2021 - Declarative Data Analytics A Survey.pdf;/Users/jessie/Zotero/storage/IJ9H9VQS/8931243.html}
}

@inproceedings{makryniotiMachineLearningSQL2021,
  title = {Machine Learning in {{SQL}} by Translation to {{TensorFlow}}},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{Data Management}} for {{End-To-End Machine Learning}}},
  author = {Makrynioti, Nantia and Ley-Wild, Ruy and Vassalos, Vasilis},
  date = {2021-06-20},
  pages = {1--11},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8486-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/WU5R57Z4/Makrynioti et al. - 2021 - Machine learning in SQL by translation to TensorFl.pdf}
}

@article{mustafaMachineLearningApproach2018,
  title = {A {{Machine Learning Approach}} for {{Predicting Execution Time}} of {{Spark Jobs}}},
  author = {Mustafa, Sara and Elghandour, Iman and Ismail, Mohamed A.},
  date = {2018-12},
  journaltitle = {Alexandria Engineering Journal},
  shortjournal = {Alexandria Engineering Journal},
  volume = {57},
  number = {4},
  pages = {3767--3778},
  issn = {11100168},
  
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1110016818301728},
  urldate = {2022-06-14},
  abstract = {Spark has gained growing attention in the past couple of years as an in-memory cloud computing platform. It supports execution of various types of workloads such as SQL queries and machine learning applications. Currently, many enterprises use Spark to exploit its fast inmemory processing of large scale data. Additionally, speeding up the execution in Spark is an important problem for many real-time applications. This can be achieved by improving the scheduling approaches employed by Spark, optimizing the execution plans generated by Spark for various applications, and selecting the best cluster configuration to run an input workload. A first step for all these optimization approaches is to predict the execution time of an input Spark application. In this paper, we present a new platform that predicts with high accuracy the execution time of SQL queries and machine learning applications executed by Spark. We evaluate our proposed platform by measuring the accuracy of predicting execution time of various types of Spark jobs including TPC-H queries and machine learning classification/clustering applications. The evaluation experiments show that we are able to predict the execution time of Spark jobs using our proposed platform with accuracy greater than 90\% for SQL queries and greater than 75\% for machine learning jobs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/NUWJS7P7/Mustafa et al. - 2018 - A Machine Learning Approach for Predicting Executi.pdf}
}

@inproceedings{nargesianOrganizingDataLakes2020,
  title = {Organizing {{Data Lakes}} for {{Navigation}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nargesian, Fatemeh and Pu, Ken Q. and Zhu, Erkang and Ghadiri Bashardoost, Bahar and Miller, Renée J.},
  date = {2020-06-11},
  pages = {1939--1950},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  abstract = {We consider the problem of creating an e ective navigation structure over a data lake. We de ne an organization as a navigation graph that contains nodes representing sets of attributes within a data lake and edges indicating subset relationships among nodes. We propose the data lake organization problem as the problem of  nding an organization that allows a user to most e ectively navigate a data lake. We present a new probabilistic model of how users interact with an organization and propose an approximate algorithm for the data lake organization problem. We show the e ectiveness of the algorithm on both a real data lake containing data from open data portals and on a benchmark that contains rich metadata emulating the observed characteristics of real data lakes. Through a formal user study, we show that navigation can help users  nd relevant tables that cannot be found by keyword search.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-6735-6},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/I25B62DS/Nargesian et al. - 2020 - Organizing Data Lakes for Navigation.pdf}
}

@inproceedings{nikolicFIVMLearningFastEvolving2020,
  title = {F-{{IVM}}: {{Learning}} over {{Fast-Evolving Relational Data}}},
  shorttitle = {F-{{IVM}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nikolic, Milos and Zhang, Haozhe and Kara, Ahmet and Olteanu, Dan},
  date = {2020-06-11},
  pages = {2773--2776},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  abstract = {F-IVM is a system for real-time analytics such as machine learning applications over training datasets de ned by queries over fast-evolving relational databases. We will demonstrate F-IVM for three such applications: model selection, ChowLiu trees, and ridge linear regression.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-6735-6},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/MF26G8UX/Nikolic et al. - 2020 - F-IVM Learning over Fast-Evolving Relational Data.pdf}
}

@inproceedings{nikolicLINVIEWIncrementalView2014,
  title = {{{LINVIEW}}: Incremental View Maintenance for Complex Analytical Queries},
  shorttitle = {{{LINVIEW}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  date = {2014-06-18},
  series = {{{SIGMOD}} '14},
  pages = {253--264},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  keywords = {compilation,incremental view maintenance,linear algebra,machine learning,notion,spark},
  file = {/Users/jessie/Zotero/storage/VKJBM6BI/Nikolic et al. - 2014 - LINVIEW incremental view maintenance for complex .pdf}
}

@article{olteanuRegressionModelsFactorized2016,
  title = {F: Regression Models over Factorized Views},
  shorttitle = {F},
  author = {Olteanu, Dan and Schleich, Maximilian},
  date = {2016-09},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {9},
  number = {13},
  pages = {1573--1576},
  issn = {2150-8097},
  
  abstract = {We demonstrate F, a system for building regression models over database views. At its core lies the observation that the computation and representation of materialized views, and in particular of joins, entail non-trivial redundancy that is not necessary for the efficient computation of aggregates used for building regression models. F avoids this redundancy by factorizing data and computation and can outperform the state-of-the-art systems MADlib, R, and Python StatsModels by orders of magnitude on real-world datasets. We illustrate how to incrementally build regression models over factorized views using both an in-memory implementation of F and its SQL encoding. We also showcase the effective use of F for model selection: F decouples the datadependent computation step from the data-independent convergence of model parameters and only performs once the former to explore the entire model space.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VDSCAFZN/Olteanu and Schleich - 2016 - F regression models over factorized views.pdf}
}

 
@article{olteanuRelationalDataBorg2020,
  author       = {Dan Olteanu},
  title        = {The Relational Data Borg is Learning},
  journal      = {Proc. {VLDB} Endow.},
  volume       = {13},
  number       = {12},
  pages        = {3502--3515},
  year         = {2020},
  url          = {http://www.vldb.org/pvldb/vol13/p3502-olteanu.pdf},
  doi          = {10.14778/3415478.3415572},
  timestamp    = {Tue, 23 Mar 2021 15:33:12 +0100},
  biburl       = {https://dblp.org/rec/journals/pvldb/Olteanu20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{phaniLIMAFinegrainedLineage2021,
  title = {{{LIMA}}: {{Fine-grained Lineage Tracing}} and {{Reuse}} in {{Machine Learning Systems}}},
  shorttitle = {{{LIMA}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Phani, Arnab and Rath, Benjamin and Boehm, Matthias},
  date = {2021-06-09},
  pages = {1426--1439},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  
  abstract = {Machine learning (ML) and data science workflows are inherently exploratory. Data scientists pose hypotheses, integrate the necessary data, and run ML pipelines of data cleaning, feature engineering, model selection and hyper-parameter tuning. The repetitive nature of these workflows, and their hierarchical composition from building blocks exhibits high computational redundancy. Existing work addresses this redundancy with coarse-grained lineage tracing and reuse for ML pipelines. This approach allows using existing ML systems, but views entire algorithms as black boxes, and thus, fails to eliminate fine-grained redundancy and to handle internal non-determinism. In this paper, we introduce LIMA, a practical framework for efficient, fine-grained lineage tracing and reuse inside ML systems. Lineage tracing of individual operations creates new challenges and opportunities. We address the large size of lineage traces with multi-level lineage tracing and reuse, as well as lineage deduplication for loops and functions; exploit full and partial reuse opportunities across the program hierarchy; and integrate this framework with task parallelism and operator fusion. The resulting framework performs fine-grained lineage tracing with low overhead, provides versioning and reproducibility, and is able to eliminate fine-grained redundancy. Our experiments on a variety of ML pipelines show performance improvements up to 12.4x.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  keywords = {notion},
}

@inproceedings{polyzotisDataManagementChallenges2017,
  title = {Data {{Management Challenges}} in {{Production Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  date = {2017-05-09},
  pages = {1723--1726},
  publisher = {{ACM}},
  location = {{Chicago Illinois USA}},
  
  abstract = {The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-4197-4},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/5J4K7HKZ/Polyzotis et al. - 2017 - Data Management Challenges in Production Machine L.pdf}
}

@unpublished{schleichLayeredAggregateEngine2019,
  title = {A {{Layered Aggregate Engine}} for {{Analytics Workloads}}},
  author = {Schleich, Maximilian and Olteanu, Dan and Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong},
  date = {2019-06-20},
  eprint = {1906.08687},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1906.08687},
  abstract = {This paper introduces LMFAO (Layered Multiple Functional Aggregate Optimization), an in-memory optimization and execution engine for batches of aggregates over the input database. The primary motivation for this work stems from the observation that for a variety of analytics over databases, their data-intensive tasks can be decomposed into group-by aggregates over the join of the input database relations. We exemplify the versatility and competitiveness of LMFAO for a handful of widely used analytics: learning ridge linear regression, classification trees, regression trees, and the structure of Bayesian networks using Chow-Liu trees; and data cubes used for exploration in data warehousing. LMFAO consists of several layers of logical and code optimizations that systematically exploit sharing of computation, parallelism, and code specialization. We conducted two types of performance benchmarks. In experiments with four datasets, LMFAO outperforms by several orders of magnitude on one hand, a commercial database system and MonetDB for computing batches of aggregates, and on the other hand, TensorFlow, Scikit, R, and AC/DC for learning a variety of models over databases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases,H.2.4,I.2.6,LMFAO,notion},
  file = {/Users/jessie/Zotero/storage/EVWPEBVH/EVWPEBVH.pdf}
}




@article{shahAreKeyForeignKey2017,
  title={Are key-foreign key joins safe to avoid when learning high-capacity classifiers?},
  author={Shah, Vraj and Kumar, Arun and Zhu, Xiaojin},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={3},
  pages={366--379},
  year={2017},
  publisher={VLDB Endowment}
}

@inproceedings{sommerMNCStructureExploitingSparsity2019,
  title = {{{MNC}}: {{Structure-Exploiting Sparsity Estimation}} for {{Matrix Expressions}}},
  shorttitle = {{{MNC}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Sommer, Johanna and Boehm, Matthias and Evfimievski, Alexandre V. and Reinwald, Berthold and Haas, Peter J.},
  date = {2019-06-25},
  pages = {1607--1623},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  
  abstract = {Efficiently computing linear algebra expressions is central to machine learning (ML) systems. Most systems support sparse formats and operations because sparse matrices are ubiquitous and their dense representation can cause prohibitive overheads. Estimating the sparsity of intermediates, however, remains a key challenge when generating execution plans or performing sparse operations. These sparsity estimates are used for cost and memory estimates, format decisions, and result allocation. Existing estimators tend to focus on matrix products only, and struggle to attain good accuracy with low estimation overhead. However, a key observation is that real-world sparse matrices commonly exhibit structural properties such as a single non-zero per row, or columns with varying sparsity. In this paper, we introduce MNC (Matrix Non-zero Count), a remarkably simple, count-based matrix synopsis that exploits these structural properties for efficient, accurate, and general sparsity estimation. We describe estimators and sketch propagation for realistic linear algebra expressions. Our experiments—on a new estimation benchmark called SparsEst—show that the MNC estimator yields good accuracy with very low overhead. This behavior makes MNC practical and broadly applicable in ML systems.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/HTFABESS/HTFABESS.pdf}
}

@article{sunDimaDistributedInmemory2017,
  title = {Dima: A Distributed in-Memory Similarity-Based Query Processing System},
  shorttitle = {Dima},
  author = {Sun, Ji and Shang, Zeyuan and Li, Guoliang and Deng, Dong and Bao, Zhifeng},
  date = {2017-08-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {10},
  number = {12},
  pages = {1925--1928},
  issn = {2150-8097},
  
  urldate = {2022-04-05},
  abstract = {Data analysts in industries spend more than 80\% of time on data cleaning and integration in the whole process of data analytics due to data errors and inconsistencies. It calls for effective query processing techniques to tolerate the errors and inconsistencies. In this paper, we develop a distributed in-memory similarity-based query processing system called Dima. Dima supports two core similarity-based query operations, i.e., similarity search and similarity join. Dima extends the SQL programming interface for users to easily invoke these two operations in their data analysis jobs. To avoid expensive data transformation in a distributed environment, we design selectable signatures where two records approximately match if they share common signatures. More importantly, we can adaptively select the signatures to balance the workload. Dima builds signature-based global indexes and local indexes to support efficient similarity search and join. Since Spark is one of the widely adopted distributed in-memory computing systems, we have seamlessly integrated Dima into Spark and developed effective query optimization techniques in Spark. To the best of our knowledge, this is the first full-fledged distributed in-memory system that can support similarity-based query processing. We demonstrate our system in several scenarios, including entity matching, web table integration and query recommendation.},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VIKNJAFT/Sun et al. - 2017 - Dima a distributed in-memory similarity-based que.pdf}
}

@article{thomasComparativeEvaluationSystems2018,
  title = {A Comparative Evaluation of Systems for Scalable Linear Algebra-Based Analytics},
  author = {Thomas, Anthony and Kumar, Arun},
  date = {2018-09-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {13},
  pages = {2168--2182},
  issn = {21508097},
  
  url = {http://dl.acm.org/citation.cfm?doid=3275366.3284963},
  abstract = {The growing use of statistical and machine learning (ML) algorithms to analyze large datasets has given rise to new systems to scale such algorithms. But implementing new scalable algorithms in low-level languages is a painful process, especially for enterprise and scientific users. To mitigate this issue, a new breed of systems expose high-level bulk linear algebra (LA) primitives that are scalable. By composing such LA primitives, users can write analysis algorithms in a higher-level language, while the system handles scalability issues. But there is little work on a unified comparative evaluation of the scalability, efficiency, and effectiveness of such “scalable LA systems.” We take a major step towards filling this gap. We introduce a suite of LA-specific tests based on our analysis of the data access and communication patterns of LA workloads and their use cases. Using our tests, we perform a comprehensive empirical comparison of a few popular scalable LA systems: MADlib, MLlib, SystemML, ScaLAPACK, SciDB, and TensorFlow using both synthetic data and a large real-world dataset. Our study has revealed several scalability bottlenecks, unusual performance trends, and even bugs in some systems. Our findings have already led to improvements in SystemML, with other systems’ developers also expressing interest. All of our code and data scripts are available for download at https://adalabucsd.github.io/slab.html.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/E6LR3FCC/Thomas and Kumar - 2018 - A comparative evaluation of systems for scalable l.pdf}
}

@inproceedings{yangFactorizedSVMGaussian2020,
  title = {Towards {{Factorized SVM}} with {{Gaussian Kernels}} over {{Normalized Data}}},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Yang, Keyu and Gao, Yunjun and Liang, Lei and Yao, Bin and Wen, Shiting and Chen, Gang},
  date = {2020-04},
  pages = {1453--1464},
  issn = {2375-026X},
  
  abstract = {There is an emerging trend of integrating machine learning (ML) techniques into database systems (DB). Considering that almost all the ML toolkits assume that the input of ML algorithms is a single table even though many real-world datasets are stored as multiple tables due to normalization in DB. Thus, data scientists have to perform joins before learning a ML model. This strategy is called learning after joins, which incurs redundancy avoided by normalization. In the area of ML, the Support Vector Machine (SVM) is one of the most standard classification tools. In this paper, we focus on the factorized SVM with gaussian kernels over normalized data. We present factorized learning approaches for two main SVM optimization methods, i.e., Gradient Descent (GD) and Sequential Minimal Optimization (SMO), by factorizing gaussian kernel function computation. Furthermore, we transform the normalized data into matrices, and boost the efficiency of SVM learning via linear algebra operations. Extensive experiments with nine real normalized data sets demonstrate the efficiency and scalability of our proposed approaches.},
  eventtitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Databases,Kernel,Machine learning,Motion pictures,notion,Optimization,Support vector machines,Training},
  file = {/Users/jessie/Zotero/storage/6Q3SU359/Yang et al. - 2020 - Towards Factorized SVM with Gaussian Kernels over .pdf;/Users/jessie/Zotero/storage/DJERC4EB/9101671.html}
}

@article{yuWindTunnelDifferentiableML2021,
  title = {{{WindTunnel}}: Towards Differentiable {{ML}} Pipelines beyond a Single Model},
  shorttitle = {{{WindTunnel}}},
  author = {Yu, Gyeong-In and Amizadeh, Saeed and Kim, Sehoon and Pagnoni, Artidoro and Zhang, Ce and Chun, Byung-Gon and Weimer, Markus and Interlandi, Matteo},
  date = {2021-09},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {15},
  number = {1},
  pages = {11--20},
  issn = {2150-8097},
  
  abstract = {While deep neural networks (DNNs) have shown to be successful in several domains like computer vision, non-DNN models such as linear models and gradient boosting trees are still considered state-of-the-art over tabular data. When using these models, data scientists often author machine learning (ML) pipelines: DAG of ML operators comprising data transforms and ML models, whereby each operator is sequentially trained one-at-a-time. Conversely, when training DNNs, layers composing the neural networks are simultaneously trained using backpropagation.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/E225GEK4/Yu et al. - 2021 - WindTunnel towards differentiable ML pipelines be.pdf}
}

@article{zhangMaterializationOptimizationsFeature2016,
  title = {Materialization {{Optimizations}} for {{Feature Selection Workloads}}},
  author = {Zhang, Ce and Kumar, Arun and Ré, Christopher},
  date = {2016-04-07},
  journaltitle = {ACM Transactions on Database Systems},
  shortjournal = {ACM Trans. Database Syst.},
  volume = {41},
  number = {1},
  pages = {1--32},
  issn = {0362-5915, 1557-4644},
  
  abstract = {There is an arms race in the data management industry to support statistical analytics. Feature selection, the process of selecting a feature set that will be used to build a statistical model, is widely regarded as the most critical step of statistical analytics. Thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature selection language and a supporting prototype system that builds on top of current industrial R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analogue in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of datasets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new trade-off space across multiple R backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/CJD66FHV/Zhang et al. - 2016 - Materialization Optimizations for Feature Selectio.pdf}
}



@inproceedings{agrawalDataPlatformMachine2019,
  title = {Data {{Platform}} for {{Machine Learning}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Agrawal, Pulkit and Arya, Rajat and Bindal, Aanchal and Bhatia, Sandeep and Gagneja, Anupriya and Godlewski, Joseph and Low, Yucheng and Muss, Timothy and Paliwal, Mudit Manu and Raman, Sethu and Shah, Vishrut and Shen, Bochao and Sugden, Laura and Zhao, Kaiyu and Wu, Ming-Chuan},
  date = {2019-06-25},
  pages = {1803--1816},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/8V246IPB/Agrawal et al. - 2019 - Data Platform for Machine Learning.pdf}
}



@article{andersonBrainwashDataSystem,
  title = {Brainwash: {{A Data System}} for {{Feature Engineering}}},
  author = {Anderson, Michael and Antenucci, Dolan and Bittorf, Victor and Burgess, Matthew and Cafarella, Michael and Kumar, Arun and Niu, Feng and Park, Yongjoo and Ré, Christopher and Zhang, Ce},
  pages = {4},
  abstract = {A new generation of data processing systems, including web search, Google’s Knowledge Graph, IBM’s Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, finance, energy, and general business. But building them can be challenging, even for computer scientists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/HRS4JFTA/Anderson et al. - Brainwash A Data System for Feature Engineering.pdf}
}

@inproceedings{castrofernandezAurumDataDiscovery2018,
  title = {Aurum: {{A Data Discovery System}}},
  shorttitle = {Aurum},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Castro Fernandez, Raul and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  date = {2018-04},
  pages = {1001--1012},
  issn = {2375-026X},
  
  abstract = {Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-defined schemas or a small number of data sources—instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph (EKG), which helps users to navigate among disparate sources. The contribution of this paper is AURUM, a system to build, maintain and query the EKG. To build the EKG, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the EKG without re-reading all data every time, we introduce a resource-efficient sampling signature (RESS) method which works by only using a small sample of the data. Finally, to query the EKG, we introduce a collection of composable primitives, thus allowing users to define many different types of discovery queries. We describe our experience using AURUM in three corporate scenarios and do a performance evaluation of each component.},
  eventtitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Buildings,Companies,data discovery,Electrocardiography,enterprise knowledge graph,Indexes,Lakes,notion},
  file = {/Users/jessie/Zotero/storage/BEV7JDDS/Castro Fernandez et al. - 2018 - Aurum A Data Discovery System.pdf}
}

@inproceedings{castrofernandezAurumDataDiscovery2018a,
  title = {Aurum: {{A Data Discovery System}}},
  shorttitle = {Aurum},
  booktitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Castro Fernandez, Raul and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
  date = {2018-04},
  pages = {1001--1012},
  issn = {2375-026X},
  
  abstract = {Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-defined schemas or a small number of data sources—instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph (EKG), which helps users to navigate among disparate sources. The contribution of this paper is AURUM, a system to build, maintain and query the EKG. To build the EKG, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the EKG without re-reading all data every time, we introduce a resource-efficient sampling signature (RESS) method which works by only using a small sample of the data. Finally, to query the EKG, we introduce a collection of composable primitives, thus allowing users to define many different types of discovery queries. We describe our experience using AURUM in three corporate scenarios and do a performance evaluation of each component.},
  eventtitle = {2018 {{IEEE}} 34th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Buildings,Companies,data discovery,Electrocardiography,enterprise knowledge graph,Indexes,Lakes,notion},
  file = {/Users/jessie/Zotero/storage/R6XP3A2E/Castro Fernandez et al. - 2018 - Aurum A Data Discovery System.pdf;/Users/jessie/Zotero/storage/6EGNKLYJ/8509315.html}
}

@article{chaiDataManagementMachine2022,
  title = {Data {{Management}} for {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Data {{Management}} for {{Machine Learning}}},
  author = {Chai, Chengliang and Wang, Jiayi and Luo, Yuyu and Niu, Zeping and Li, Guoliang},
  date = {2022},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  
  abstract = {Machine learning(ML) has widespread applications and has revolutionized many industries, but suffers from several challenges. First, sufficient high-quality training data is inevitable for producing a well-performed model, but the data is always human expensive to acquire.Second, a large amount of training data and complicated model structures lead to the inefficiency of training and inference. Third, given an ML task, one always needs to train lots of models, which are hard to manage in real applications. Fortunately, database techniques can benefit ML by addressing the above three challenges. In this paper, we review existing studies from the following three aspects along with the pipeline highly related to ML. (1) Data preparation(Pre-ML): it focuses on preparing high-quality training data that can improve the performance of the ML model, where we review data discovery, data cleaning and data labeling. (2) Model training \& inference(In-ML): researchers in ML community focus on improving the model performance during training, while in this survey we mainly study how to accelerate the entire training process, also including feature selection and model selection. (3) Model management(Post-ML): in this part, we survey how to store, query, deploy and debug the models after training. Finally, we provide research challenges and future directions.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Cleaning,Computational modeling,Data models,Data preparation,Database,Machine learning,Model inference,Model training,notion,Optimization,Task analysis,Training,Training data},
  file = {/Users/jessie/Zotero/storage/Z5ZTXTAB/Chai et al. - 2022 - Data Management for Machine Learning A Survey.pdf;/Users/jessie/Zotero/storage/2QSLFQTU/9705125.html}
}


@inproceedings{chengNonlinearModelsNormalized2019,
  title = {Nonlinear {{Models Over Normalized Data}}},
  author = {Cheng, Zhaoyue and Koudas, Nick},
  date = {2019-04},
  pages = {1574--1577},
  
  abstract = {Machine Learning (ML) applications are proliferating in the enterprise. Increasingly enterprise data are used to build sophisticated ML models to assist critical business functions. Relational data which are prevalent in enterprise applications are typically normalized; as a result data have to be denormalized via primary/foreign-key joins to be provided as input to ML algorithms. In this paper we study the implementation of popular nonlinear ML models and in particular independent Gaussian Mixture Models (IGMM) over normalized data. For the case of IGMM we propose algorithms taking the statistical properties of the Gaussians into account to construct mixture models, factorizing the computation. In that way we demonstrate that we can conduct the training of the models much faster compared to other applicable approaches, without any loss in accuracy. We present the results of a thorough experimental evaluation, varying several parameters of the input relations involved and demonstrate that our proposals both for the case of IGMM yield drastic performance improvements which become increasingly higher as parameters of the underlying data vary, without any loss in accuracy.},
  eventtitle = {2019 {{IEEE}} 35th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Analytical models,Computational modeling,Convergence,data management,Data models,databases,Gaussian distribution,Gaussian mixture model,machine learning,no linear models,normalized databases,notion},
  file = {/Users/jessie/Zotero/storage/EC3LUNBJ/Cheng and Koudas - 2019 - Nonlinear Models Over Normalized Data.pdf;/Users/jessie/Zotero/storage/8FZK53ZZ/8731358.html}
}

 

@inproceedings{chenRedundancyEliminationDistributed2022,
  title = {Redundancy {{Elimination}} in {{Distributed Matrix Computation}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chen, Zihao and Han, Baokun and Xu, Chen and Qian, Weining and Zhou, Aoying},
  date = {2022-06-10},
  pages = {573--586},
  publisher = {{ACM}},
  location = {{Philadelphia PA USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3514221.3517877},
  urldate = {2022-06-16},
  abstract = {As matrix computation becomes increasingly prevalent in largescale data analysis, distributed matrix computation solutions have emerged. These solutions support query interfaces of linear algebra expressions, which often contain redundant subexpressions, i.e., common and loop-constant subexpressions. Hence, existing compilers rewrite queries to eliminate such redundancy. However, due to the large search space, they fail to find all redundant subexpressions, especially for matrix multiplication chains. Furthermore, redundancy elimination may change the original execution order of operators, and have negative impacts. To reduce the large search space and avoid the negative impacts, we propose automatic elimination and adaptive elimination, respectively. In particular, automatic elimination adopts a block-wise search that exploits the properties of matrix computation for speed-up. Adaptive elimination employs a cost model and a dynamic programming-based method to generate efficient plans for redundancy elimination. Finally, we implement ReMac atop SystemDS, eliminating redundancy in distributed matrix computation. In our experiments, ReMac is able to generate efficient execution plans at affordable overhead costs, and outperforms state-of-the-art solutions by an order of magnitude.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '22: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/N4U2TEJ6/Chen et al. - 2022 - Redundancy Elimination in Distributed Matrix Compu.pdf}
}

 
@article{chepurkoARDAAutomaticRelational2020,
  author       = {Nadiia Chepurko and
                  Ryan Marcus and
                  Emanuel Zgraggen and
                  Raul Castro Fernandez and
                  Tim Kraska and
                  David R. Karger},
  title        = {{ARDA:} Automatic Relational Data Augmentation for Machine Learning},
  journal      = {Proc. {VLDB} Endow.},
  volume       = {13},
  number       = {9},
  pages        = {1373--1387},
  year         = {2020},
  timestamp    = {Sun, 02 Oct 2022 15:46:40 +0200},
  biburl       = {https://dblp.org/rec/journals/pvldb/ChepurkoMZFKK20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{convolboCostawareDAGScheduling2016,
  title = {Cost-Aware {{DAG}} Scheduling Algorithms for Minimizing Execution Cost on Cloud Resources},
  author = {Convolbo, Moïse W. and Chou, Jerry},
  date = {2016-03},
  journaltitle = {The Journal of Supercomputing},
  shortjournal = {J Supercomput},
  volume = {72},
  number = {3},
  pages = {985--1012},
  issn = {0920-8542, 1573-0484},
  
  url = {http://link.springer.com/10.1007/s11227-016-1637-7},
  urldate = {2022-06-14},
  abstract = {Directed acyclic graph (DAG) scheduling is a well-known problem, because a DAG can be used to describe a wide range of complex applications, including scientific applications and parallel computing jobs. Most DAG scheduling algorithms were proposed to minimize the job makespan (i.e., execution time) on a multiprocessor computer or cluster. However, as the cost-driven public cloud services have become an attractive and popular platform for providing computing resources, cost minimization emerges as a new critical issue. Therefore, the objective of this work is to propose and solve the cost optimization problem for scheduling DAGs on an IaaS cloud platform where task scheduling must cope with resource provisioning to achieve the optimal solution. In this paper, we proposed both optimal and heuristic scheduling algorithms, and we evaluated them across a variety of DAGs using the price model from EC2. Comparing to other cost-oblivious DAG schedules that aim to minimize makespan or resource usage, the results show that our cost-aware heuristic algorithm can reduce cost by 20–50 \% and achieve a cost within x1.16 of the optimal one.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/3YWGZ25L/Convolbo and Chou - 2016 - Cost-aware DAG scheduling algorithms for minimizin.pdf}
}

@unpublished{curtinRkmeansFastClustering2019,
  title = {Rk-Means: {{Fast Clustering}} for {{Relational Data}}},
  shorttitle = {Rk-Means},
  author = {Curtin, Ryan and Moseley, Ben and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
  date = {2019-10-10},
  eprint = {1910.04939},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.04939},
  abstract = {Conventional machine learning algorithms cannot be applied until a data matrix is available to process. When the data matrix needs to be obtained from a relational database via a feature extraction query, the computation cost can be prohibitive, as the data matrix may be (much) larger than the total input relation size. This paper introduces Rk-means, or relational k-means algorithm, for clustering relational data tuples without having to access the full data matrix. As such, we avoid having to run the expensive feature extraction query and storing its output. Our algorithm leverages the underlying structures in relational data. It involves construction of a small grid coreset of the data matrix for subsequent cluster construction. This gives a constant approximation for the k-means objective, while having asymptotic runtime improvements over standard approaches of first running the database query and then clustering. Empirical results show orders-of-magnitude speedup, and Rk-means can run faster on the database than even just computing the data matrix.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/jessie/Zotero/storage/VHIGYGD8/Curtin et al. - 2019 - Rk-means Fast Clustering for Relational Data.pdf}
}

@inproceedings{deboeIntegratedMLEverySQL2020,
  title = {{{IntegratedML}}: {{Every SQL Developer}} Is a {{Data Scientist}}},
  shorttitle = {{{IntegratedML}}},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  author = {De Boe, Benjamin and Woodfin, Tom and Dyar, Thomas and McCaldon, Dave and Djakovic, Aleks and MacLeod, Alex and Woodlock, Don},
  date = {2020-06-14},
  pages = {1--4},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3399579.3399866},
  urldate = {2022-06-15},
  abstract = {The adoption of machine learning in business software is slowed down by a shortage of data science talent and challenges around efficient operationalization of machine learning models. We present IntegratedML, an embedded database capability for machine learning. This paper describes how IntegratedML provides developers with access to state-of-the-art machine learning platforms using intuitive SQL syntax. Its embedded feature extraction and algorithm selection enable fully automated model building, while model inferencing is exposed through a simple scalar function. The novelty of IntegratedML is in the deep integration into the embedding relational engine, which hides pipeline complexity from the user and guarantees high efficiencies, both at train and inference time.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8023-2},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/YQC9H5FK/De Boe et al. - 2020 - IntegratedML Every SQL Developer is a Data Scient.pdf}
}

@article{dsilvaAIDAAbstractionAdvanced2018,
  title = {{{AIDA}}: Abstraction for Advanced in-Database Analytics},
  shorttitle = {{{AIDA}}},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2018-07},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {11},
  pages = {1400--1413},
  
  url = {https://dl.acm.org/doi/10.14778/3236187.3236194},
  abstract = {With the tremendous growth in data science and machine learning, it has become increasingly clear that traditional relational database management systems (RDBMS) are lacking appropriate support for the programming paradigms required by such applications, whose developers prefer tools that perform the computation outside the database system. While the database community has attempted to integrate some of these tools in the RDBMS, this has not swayed the trend as existing solutions are often not convenient for the incremental, iterative development approach used in these fields. In this paper, we propose AIDA - an abstraction for advanced in-database analytics. AIDA emulates the syntax and semantics of popular data science packages but transparently executes the required transformations and computations inside the RDBMS. In particular, AIDA works with a regular Python interpreter as a client to connect to the database. Furthermore, it supports the seamless use of both relational and linear algebra operations using a unified abstraction. AIDA relies on the RDBMS engine to efficiently execute relational operations and on an embedded Python interpreter and NumPy to perform linear algebra operations. Data reformatting is done transparently and avoids data copy whenever possible. AIDA does not require changes to statistical packages or the RDBMS facilitating portability.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/5GJ5884P/D'silva et al. - 2018 - AIDA abstraction for advanced in-database analyti.pdf}
}

@inproceedings{dsilvaKeepYourHost2019,
  title = {Keep {{Your Host Language Object}} and {{Also Query}} It: {{A Case}} for {{SQL Query Support}} in {{RDBMS}} for {{Host Language Objects}}},
  shorttitle = {Keep {{Your Host Language Object}} and {{Also Query}} It},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2019-07-23},
  pages = {133--144},
  publisher = {{ACM}},
  location = {{Santa Cruz CA USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3335783.3335798},
  abstract = {As a result of prolific growth in data science and machine learning applications, modern relational database management systems (RDBMS) are experimenting with various approaches to facilitate advanced analytical computations, in addition to the relational operations that they traditionally support. The most common approach has been to integrate an embedded high level language (HLL) interpreter into the RDBMS along with any additional libraries that specialize in numerical computations. Such implementations, e.g., user defined functions (UDFs), follow generally a black-box setup, and for many complex workflows that require datasets to be passed and processed back-and-forth between the query execution engine and the embedded HLL interpreter, optimization opportunities are not fully explored yet. In this paper, we propose and implement the concept of virtual tables that can be used to expose data set objects maintained by the embedded HLL interpreter to the query engine for executing relational operations. Unlike prevalent solutions, our approach minimizes the need for performing data copies and conversions, performing them lazily when required. It also facilitates better optimization opportunities for the execution of SQL queries as the RDBMS is able to analyze the data characteristics of the HLL objects before producing an execution plan. The approach is also programmer friendly, allowing for a more intuitive implementation of computational workflows. We perform evaluations over a variety of workloads which demonstrate the performance and programming benefits of virtual tables.},
  eventtitle = {{{SSDBM}} '19: 31st {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/2T9IQFYW/D'silva et al. - 2019 - Keep Your Host Language Object and Also Query it .pdf}
}

@article{dsilvaMakingRDBMSData2019,
  title = {Making an {{RDBMS}} Data Scientist Friendly: Advanced in-Database Interactive Analytics with Visualization Support},
  shorttitle = {Making an {{RDBMS}} Data Scientist Friendly},
  author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
  date = {2019-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {12},
  number = {12},
  pages = {1930--1933},
  
  url = {https://dl.acm.org/doi/10.14778/3352063.3352102},
  abstract = {We are currently witnessing the rapid evolution and adoption of various data science frameworks that function external to the database. Any support from conventional RDBMS implementations for data science applications has been limited to procedural paradigms such as user-defined functions (UDFs) that lack exploratory programming support. Therefore, the current status quo is that during the exploratory phase, data scientists usually use the database system as the “data storage” layer of the data science framework, whereby the majority of computation and analysis is performed outside the database, e.g., at the client node. We demonstrate AIDA, an in-database framework for data scientists. AIDA allows users to write interactive Python code using a development environment such as a Jupyter notebook. The actual execution itself takes place inside the database (neardata), where a server component of AIDA, that resides inside the embedded Python interpreter of the RDBMS, manages the data sets and computations. The demonstration will also show the visualization capabilities of AIDA where the progress of computation can be observed through live updates. Our evaluations show that AIDA performs several times faster compared to contemporary external data science frameworks, but is much easier to use for exploratory development compared to database UDFs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/TYPXW33N/D'silva et al. - 2019 - Making an RDBMS data scientist friendly advanced .pdf}
}

@inproceedings{fonsecaMultiDAGModelRealtime2015,
  title = {A Multi-{{DAG}} Model for Real-Time Parallel Applications with Conditional Execution},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Fonseca, José Carlos and Nélis, Vincent and Raravi, Gurulingesh and Pinho, Luís Miguel},
  date = {2015-04-13},
  pages = {1925--1932},
  publisher = {{ACM}},
  location = {{Salamanca Spain}},
  
  url = {https://dl.acm.org/doi/10.1145/2695664.2695808},
  urldate = {2022-06-13},
  abstract = {Owing to the current trends for higher performance and the ever growing availability of multiprocessors in the embedded computing (EC) domain, there is nowadays a strong push towards the parallelization of modern embedded applications. Several real-time task models have recently been proposed to capture different forms of parallelism. However, they do not deal explicitly with control flow information as they assume that all the threads of a parallel task must execute every time the task is activated. In contrast, in this paper, we present a multi-DAG model where each task is characterized by a set of execution flows, each of which represents a different execution path throughout the task code and is modeled as a DAG of sub-tasks. We propose a two-step solution that computes a single synchronous DAG of servers for a task modeled by a multi-DAG and show that these servers are able to supply every execution flow of that task with the required cpu-budget so that the task can execute entirely, irrespective of the execution flow taken at run-time, while satisfying its precedence constraints. As a result, each task can be modeled by its single DAG of servers, which facilitates in leveraging the existing single-DAG schedulability analyses techniques for analyzing the schedulability of parallel tasks with multiple execution flows.},
  eventtitle = {{{SAC}} 2015: {{Symposium}} on {{Applied Computing}}},
  isbn = {978-1-4503-3196-8},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/CBUNAA6V/Fonseca et al. - 2015 - A multi-DAG model for real-time parallel applicati.pdf}
}

@inproceedings{haiAmalurNextgenerationData2022,
  title = {Amalur: {{Next-generation Data Integration}} in {{Data Lakes}}},
  booktitle = {2022 {{CIDR}} 12th {{Annual Conference}} on {{Innovative Data Systems Research}}},
  author = {Hai, Rihan and Koutras, Christos and Ionescu, Andra and Katsifodimos, Asterios},
  date = {2022-09-01},
  pages = {1},
  location = {{Chaminade, USA}},
  abstract = {Data science workflows often require extracting, preparing and integrating data from multiple data sources. This is a cumbersome and slow process: most of the times, data scientists prepare data in a data processing system or a data lake, and export it as a table, in order for it to be consumed by a Machine Learning (ML) algorithm. Recent advances in the area of factorized ML, allow us to push down certain linear algebra (LA) operators, executing them closer to the data sources. With this work, we revisit classic data integration (DI) systems and see how these fit into modern data lakes that are meant to support LA as a first-class citizen.},
  eventtitle = {2022 {{CIDR}} 12th {{Annual Conference}} on {{Innovative Data Systems Research}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/4PAWEI7L/Amalur Next-generation Data Integration in Data L.pdf}
}

@inproceedings{halevyGoodsOrganizingGoogle2016,
  title = {Goods: {{Organizing Google}}'s {{Datasets}}},
  shorttitle = {Goods},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Halevy, Alon and Korn, Flip and Noy, Natalya F. and Olston, Christopher and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong},
  date = {2016-06-14},
  pages = {795--806},
  publisher = {{ACM}},
  location = {{San Francisco California USA}},
  
  url = {https://dl.acm.org/doi/10.1145/2882903.2903730},
  urldate = {2022-04-05},
  abstract = {Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in different storage systems, may vary in their formats, may change every day. In this paper, we present Goods, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. Goods extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them. We discuss the technical challenges that we had to overcome in order to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level datamanagement systems in general.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'16: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/IU2275QC/Halevy et al. - 2016 - Goods Organizing Google's Datasets.pdf}
}

@online{IntegratedMLProceedingsFourth,
  title = {{{IntegratedML}} | {{Proceedings}} of the {{Fourth International Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  url = {https://dl.acm.org/doi/abs/10.1145/3399579.3399866},
  urldate = {2022-06-15},
  langid = {english},
  organization = {{ACM Conferences}},
  keywords = {notion}
}


@inproceedings{kandoganLabBookMetadatadrivenSocial2015,
  title = {{{LabBook}}: {{Metadata-driven}} Social Collaborative Data Analysis},
  shorttitle = {{{LabBook}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Kandogan, Eser and Roth, Mary and Schwarz, Peter and Hui, Joshua and Terrizzano, Ignacio and Christodoulakis, Christina and Miller, Renée J.},
  date = {2015-10},
  pages = {431--440},
  
  abstract = {Open data analysis platforms are being adopted to support collaboration in science and business. Studies suggest that analytic work in an enterprise occurs in a complex ecosystem of people, data, and software working in a coordinated manner. These studies also point to friction between the elements of this ecosystem that reduces user productivity and quality of work. LabBook is an open, social, and collaborative data analysis platform designed explicitly to reduce this friction and accelerate discovery. Its goal is to help users leverage each other's knowledge and experience to find the data, tools and collaborators they need to integrate, visualize, and analyze data. The key insight is to collect and use more metadata about all elements of the analytic ecosystem by means of an architecture and user experience that reduce the cost of contributing such metadata. We demonstrate how metadata can be exploited to improve the collaborative user experience and facilitate collaborative data integration and recommendations. We describe a specific use case and discuss several design issues concerning the capture, representation, querying and use of metadata.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  keywords = {Bioinformatics,Business,collaboration,Collaboration,Data analysis,data analytics,data discovery,Ecosystems,metadata,Metadata,notion,Semantics},
  file = {/Users/jessie/Zotero/storage/5TFM83XX/Kandogan et al. - 2015 - LabBook Metadata-driven social collaborative data.pdf;/Users/jessie/Zotero/storage/PYWUNC7G/7363784.html}
}

@article{karaDoppioDBHardwareTechniques2019,
  title = {{{doppioDB}} 2.0: Hardware Techniques for Improved Integration of Machine Learning into Databases},
  shorttitle = {{{doppioDB}} 2.0},
  author = {Kara, Kaan and Wang, Zeke and Zhang, Ce and Alonso, Gustavo},
  date = {2019-08},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {12},
  number = {12},
  pages = {1818--1821},
  issn = {2150-8097},
  
  url = {https://dl.acm.org/doi/10.14778/3352063.3352074},
  urldate = {2022-06-15},
  abstract = {Database engines are starting to incorporate machine learning (ML) functionality as part of their repertoire. Machine learning algorithms, however, have very different characteristics than those of relational operators. In this demonstration, we explore the challenges that arise when integrating generalized linear models into a database engine and how to incorporate hardware accelerators into the execution, a tool now widely used for ML workloads.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/T73999GR/Kara et al. - 2019 - doppioDB 2.0 hardware techniques for improved int.pdf}
}

 

@inproceedings{kimTensorDBInDatabaseTensor2014,
  title = {{{TensorDB}}: {{In-Database Tensor Manipulation}} with {{Tensor-Relational Query Plans}}},
  shorttitle = {{{TensorDB}}},
  booktitle = {Proceedings of the 23rd {{ACM International Conference}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Kim, Mijung and Candan, K. Selçuk},
  date = {2014-11-03},
  pages = {2039--2041},
  publisher = {{ACM}},
  location = {{Shanghai China}},
  
  url = {https://dl.acm.org/doi/10.1145/2661829.2661842},
  abstract = {Today’s data management systems increasingly need to support both tensor-algebraic operations (for analysis) as well as relational-algebraic operations (for data manipulation and integration). Tensor decomposition techniques are commonly used for discovering underlying structures of multidimensional data sets. However, as the relevant data sets get large, existing in-memory schemes for tensor decomposition become increasingly ineffective and, instead, memoryindependent solutions, such as in-database analytics, are necessitated. We introduce an in-database analytic system for efficient implementations of in-database tensor decompositions on chunk-based array data stores, so called, TensorDB. TensorDB includes static in-database tensor decomposition and dynamic in-database tensor decomposition operators. TensorDB extends an array database and leverages array operations for data manipulation and integration. TensorDB supports complex data processing plans where multiple relational algebraic and tensor algebraic operations are composed with each other.},
  eventtitle = {{{CIKM}} '14: 2014 {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/H886I96M/Kim and Candan - 2014 - TensorDB In-Database Tensor Manipulation with Ten.pdf}
}

@article{koutrasValentineActionMatching2021,
  title = {Valentine in Action: Matching Tabular Data at Scale},
  shorttitle = {Valentine in Action},
  author = {Koutras, Christos and Psarakis, Kyriakos and Siachamis, George and Ionescu, Andra and Fragkoulis, Marios and Bonifati, Angela and Katsifodimos, Asterios},
  date = {2021-07},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {14},
  number = {12},
  pages = {2871--2874},
  
  url = {https://dl.acm.org/doi/10.14778/3476311.3476366},
  abstract = {Capturing relationships among heterogeneous datasets in large data lakes – traditionally termed schema matching – is one of the most challenging problems that corporations and institutions face nowadays. Discovering and integrating datasets heavily relies on the effectiveness of the schema matching methods in use. However, despite the wealth of research, evaluation of schema matching methods is still a daunting task: there is a lack of openly-available datasets with ground truth, reference method implementations, and comprehensible GUIs that would facilitate development of both novel state-of-the-art schema matching techniques and novel data discovery methods.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VS7G3S3U/Koutras et al. - 2021 - Valentine in action matching tabular data at scal.pdf}
}

@inproceedings{koutrasValentineEvaluatingMatching2021,
  title = {Valentine: {{Evaluating Matching Techniques}} for {{Dataset Discovery}}},
  shorttitle = {Valentine},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Koutras, Christos and Siachamis, George and Ionescu, Andra and Psarakis, Kyriakos and Brons, Jerry and Fragkoulis, Marios and Lofi, Christoph and Bonifati, Angela and Katsifodimos, Asterios},
  date = {2021-04},
  pages = {468--479},
  publisher = {{IEEE}},
  location = {{Chania, Greece}},
  
  url = {https://ieeexplore.ieee.org/document/9458921/},
  abstract = {Data scientists today search large data lakes to discover and integrate datasets. In order to bring together disparate data sources, dataset discovery methods rely on some form of schema matching: the process of establishing correspondences between datasets. Traditionally, schema matching has been used to find matching pairs of columns between a source and a target schema. However, the use of schema matching in dataset discovery methods differs from its original use. Nowadays schema matching serves as a building block for indicating and ranking inter-dataset relationships. Surprisingly, although a discovery method’s success relies highly on the quality of the underlying matching algorithms, the latest discovery methods employ existing schema matching algorithms in an ad-hoc fashion due to the lack of openly-available datasets with ground truth, reference method implementations, and evaluation metrics.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/U2ZZUSIG/Koutras et al. - 2021 - Valentine Evaluating Matching Techniques for Data.pdf}
}

@inproceedings{kumarDataManagementMachine2017,
  title = {Data {{Management}} in {{Machine Learning}}: {{Challenges}}, {{Techniques}}, and {{Systems}}},
  shorttitle = {Data {{Management}} in {{Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Kumar, Arun and Boehm, Matthias and Yang, Jun},
  date = {2017-05-09},
  pages = {1717--1722},
  publisher = {{ACM}},
  location = {{Chicago Illinois USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3035918.3054775},
  abstract = {Large-scale data analytics using statistical machine learning (ML), popularly called advanced analytics, underpins many modern data-driven applications. The data management community has been working for over a decade on tackling data management-related challenges that arise in ML workloads, and has built several systems for advanced analytics. This tutorial provides a comprehensive review of such systems and analyzes key data management challenges and techniques. We focus on three complementary lines of work: (1) integrating ML algorithms and languages with existing data systems such as RDBMSs, (2) adapting data management-inspired techniques such as query optimization, partitioning, and compression to new systems that target ML workloads, and (3) combining data management and ML ideas to build systems that improve ML lifecycle-related tasks. Finally, we identify key open data management challenges for future research in this important area.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/6AX2EQ2K/Kumar et al. - 2017 - Data Management in Machine Learning Challenges, T.pdf}
}



@inproceedings{kumarJoinNotJoin2016,
  title = {To {{Join}} or {{Not}} to {{Join}}?: {{Thinking Twice}} about {{Joins}} before {{Feature Selection}}},
  shorttitle = {To {{Join}} or {{Not}} to {{Join}}?},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M. and Zhu, Xiaojin},
  date = {2016-06-14},
  pages = {19--34},
  publisher = {{ACM}},
  location = {{San Francisco California USA}},
  
  url = {https://dl.acm.org/doi/10.1145/2882903.2882952},
  abstract = {Closer integration of machine learning (ML) with data processing is a booming area in both the data management industry and academia. Almost all ML toolkits assume that the input is a single table, but many datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins to obtain features from all base tables and apply a feature selection method, either explicitly or implicitly, with the aim of improving accuracy. In this work, we show that the features brought in by such joins can often be ignored without affecting ML accuracy significantly, i.e., we can “avoid joins safely.” We identify the core technical issue that could cause accuracy to decrease in some cases and analyze this issue theoretically. Using simulations, we validate our analysis and measure the effects of various properties of normalized data on accuracy. We apply our analysis to design easy-to-understand decision rules to predict when it is safe to avoid joins in order to help analysts exploit this runtime-accuracy trade-off. Experiments with multiple real normalized datasets show that our rules are able to accurately predict when joins can be avoided safely, and in some cases, this led to significant reductions in the runtime of some popular feature selection methods.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'16: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/3HTYW89I/Kumar et al. - 2016 - To Join or Not to Join Thinking Twice about Join.pdf}
}


@inproceedings{lamAutomatedDataScience2021,
  title = {Automated {{Data Science}} for {{Relational Data}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Lam, Hoang Thanh and Buesser, Beat and Min, Hong and Minh, Tran Ngoc and Wistuba, Martin and Khurana, Udayan and Bramble, Gregory and Salonidis, Theodoros and Wang, Dakuo and Samulowitz, Horst},
  date = {2021-04},
  pages = {2689--2692},
  issn = {2375-026X},
  
  abstract = {Feature engineering is a crucial but tedious task that requires up to 80\% of the total time in data science projects. A significant challenge is when data consists of tables from different data sources, thus data scientists need to wisely aggregate and join tables while performing feature engineering task. In this work, we demonstrate a novel system called OneBM (One Button Machine), that enables data scientists to increase their efficiency with automated feature engineering for relational data. OneBM takes as input a relational dataset with multiple tables and its entity relation diagram (ERD) which can be declared with a novel, easy-to-use drag-and-drop graphical user interface. The system then automatically identifies and executes relevant joins and aggregates in the data, and generates new features with a rich set of transformations for various types of data including but not limited to time-series, sequences, number sets and itemsets, etc. The generated features then can be used by automated model selection and hyper-parameter optimization algorithms to complete a fully end-to-end automated data science (or AutoDS) workflow. A follow-up user evaluation illustrated how data scientists can perform multi-table feature engineering tasks in minutes using our system, compared to repeatedly coding SQL-like queries to transform and aggregate relational data requiring weeks of manual labor for comparable performance. In the live demos we plan to show two use cases with real-world datasets (video demos are available at the links in the footnote): sale prediction1 and call center user experience2. Pre-registered partcipants can play with these use-cases and the given datasets via Watson Studio on the cloud.},
  eventtitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Automated Data Science,Automated feature engineering,Data aggregation,Data models,Data science,Encoding,Itemsets,Manuals,notion,Relational data,Transforms},
  file = {/Users/jessie/Zotero/storage/KQVM99JZ/Lam et al. - 2021 - Automated Data Science for Relational Data.pdf;/Users/jessie/Zotero/storage/SXJ9UWF7/9458886.html}
}

@misc{lamOneButtonMachine2017,
  title = {One Button Machine for Automating Feature Engineering in Relational Databases},
  author = {Lam, Hoang Thanh and Thiebaut, Johann-Michael and Sinn, Mathieu and Chen, Bei and Mai, Tiep and Alkan, Oznur},
  date = {2017-06-01},
  number = {arXiv:1706.00327},
  eprint = {1706.00327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1706.00327},
  urldate = {2022-06-15},
  abstract = {Feature engineering is one of the most important and time consuming tasks in predictive analytics projects. It involves understanding domain knowledge and data exploration to discover relevant hand-crafted features from raw data. In this paper, we introduce a system called One Button Machine, or OneBM for short, which automates feature discovery in relational databases. OneBM automatically performs a key activity of data scientists, namely, joining of database tables and applying advanced data transformations to extract useful features from data. We validated OneBM in Kaggle competitions in which OneBM achieved performance as good as top 16\% to 24\% data scientists in three Kaggle competitions. More importantly, OneBM outperformed the state-of-the-art system in a Kaggle competition in terms of prediction accuracy and ranking on Kaggle leaderboard. The results show that OneBM can be useful for both data scientists and non-experts. It helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time and cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,notion},
  file = {/Users/jessie/Zotero/storage/UWXSXDKS/Lam et al. - 2017 - One button machine for automating feature engineer.pdf}
}



@article{liMLogDeclarativeInDatabase,
  title = {{{MLog}}: {{Towards Declarative In-Database Machine Learning}}},
  author = {Li, Xupeng and Cui, Bin and Chen, Yiru and Wu, Wentao and Zhang, Ce},
  pages = {4},
  abstract = {We demonstrate MLOG, a high-level language that integrates machine learning into data management systems. Unlike existing machine learning frameworks (e.g., TensorFlow, Theano, and Caffe), MLOG is declarative, in the sense that the system manages all data movement, data persistency, and machine-learning related optimizations (such as data batching) automatically. Our interactive demonstration will show audience how this is achieved based on the novel notion of tensoral views (TViews), which are similar to relational views but operate over tensors with linear algebra. With MLOG, users can succinctly specify not only simple models such as SVM (in just two lines), but also sophisticated deep learning models that are not supported by existing in-database analytics systems (e.g., MADlib, PAL, and SciDB), as a series of cascaded TViews. Given the declarative nature of MLOG, we further demonstrate how query/program optimization techniques can be leveraged to translate MLOG programs into native TensorFlow programs. The performance of the automatically generated TensorFlow programs is comparable to that of hand-optimized ones.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/THNIVPK3/Li et al. - MLog Towards Declarative In-Database Machine Lear.pdf}
}

@article{luoScalableLinearAlgebra2019,
  title = {Scalable {{Linear Algebra}} on a {{Relational Database System}}},
  author = {Luo, Shangyu and Gao, Zekai J. and Gubanov, Michael and Perez, Luis L. and Jermaine, Christopher},
  date = {2019-07},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {7},
  pages = {1224--1238},
  
  abstract = {As data analytics has become an important application for modern data management systems, a new category of data management system has appeared recently: the scalable linear algebra system. In this paper, we argue that a parallel or distributed database system is actually an excellent platform upon which to build such functionality. Most relational systems already have support for cost-based optimization-which is vital to scaling linear algebra computations-and it is well-known how to make relational systems scale. We show that by making just a few changes to a parallel/distributed relational database system, such a system can be a competitive platform for scalable linear algebra. Taken together, our results should at least raise the possibility that brand new systems designed from the ground up to support scalable linear algebra are not absolutely necessary, and that such systems could instead be built on top of existing relational technology. Our results also suggest that if scalable linear algebra is to be added to a modern dataflow platform such as Spark, they should be added on top of the system's more structured (relational) data abstractions, rather than being constructed directly on top of the system's raw dataflow operators.},
  keywords = {Arrays,Buildings,Database systems,Distributed database systems,large scale linear algebra,Linear algebra,notion,Relational databases,Structured Query Language,vector/matrix},
  file = {/Users/jessie/Zotero/storage/GXBRV26P/Luo et al. - 2019 - Scalable Linear Algebra on a Relational Database S.pdf;/Users/jessie/Zotero/storage/66DDBGUS/8340060.html}
}



@inproceedings{makryniotiMachineLearningSQL2021,
  title = {Machine Learning in {{SQL}} by Translation to {{TensorFlow}}},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{Data Management}} for {{End-To-End Machine Learning}}},
  author = {Makrynioti, Nantia and Ley-Wild, Ruy and Vassalos, Vasilis},
  date = {2021-06-20},
  pages = {1--11},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  
  url = {https://dl.acm.org/doi/10.1145/3462462.3468879},
  urldate = {2022-06-15},
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8486-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/WU5R57Z4/Makrynioti et al. - 2021 - Machine learning in SQL by translation to TensorFl.pdf}
}

@article{mustafaMachineLearningApproach2018,
  title = {A {{Machine Learning Approach}} for {{Predicting Execution Time}} of {{Spark Jobs}}},
  author = {Mustafa, Sara and Elghandour, Iman and Ismail, Mohamed A.},
  date = {2018-12},
  journaltitle = {Alexandria Engineering Journal},
  shortjournal = {Alexandria Engineering Journal},
  volume = {57},
  number = {4},
  pages = {3767--3778},
  issn = {11100168},
  
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1110016818301728},
  urldate = {2022-06-14},
  abstract = {Spark has gained growing attention in the past couple of years as an in-memory cloud computing platform. It supports execution of various types of workloads such as SQL queries and machine learning applications. Currently, many enterprises use Spark to exploit its fast inmemory processing of large scale data. Additionally, speeding up the execution in Spark is an important problem for many real-time applications. This can be achieved by improving the scheduling approaches employed by Spark, optimizing the execution plans generated by Spark for various applications, and selecting the best cluster configuration to run an input workload. A first step for all these optimization approaches is to predict the execution time of an input Spark application. In this paper, we present a new platform that predicts with high accuracy the execution time of SQL queries and machine learning applications executed by Spark. We evaluate our proposed platform by measuring the accuracy of predicting execution time of various types of Spark jobs including TPC-H queries and machine learning classification/clustering applications. The evaluation experiments show that we are able to predict the execution time of Spark jobs using our proposed platform with accuracy greater than 90\% for SQL queries and greater than 75\% for machine learning jobs.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/NUWJS7P7/Mustafa et al. - 2018 - A Machine Learning Approach for Predicting Executi.pdf}
}

@inproceedings{nargesianOrganizingDataLakes2020,
  title = {Organizing {{Data Lakes}} for {{Navigation}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nargesian, Fatemeh and Pu, Ken Q. and Zhu, Erkang and Ghadiri Bashardoost, Bahar and Miller, Renée J.},
  date = {2020-06-11},
  pages = {1939--1950},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3318464.3380605},
  urldate = {2022-06-10},
  abstract = {We consider the problem of creating an e ective navigation structure over a data lake. We de ne an organization as a navigation graph that contains nodes representing sets of attributes within a data lake and edges indicating subset relationships among nodes. We propose the data lake organization problem as the problem of  nding an organization that allows a user to most e ectively navigate a data lake. We present a new probabilistic model of how users interact with an organization and propose an approximate algorithm for the data lake organization problem. We show the e ectiveness of the algorithm on both a real data lake containing data from open data portals and on a benchmark that contains rich metadata emulating the observed characteristics of real data lakes. Through a formal user study, we show that navigation can help users  nd relevant tables that cannot be found by keyword search.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-6735-6},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/I25B62DS/Nargesian et al. - 2020 - Organizing Data Lakes for Navigation.pdf}
}

@inproceedings{nikolicFIVMLearningFastEvolving2020,
  title = {F-{{IVM}}: {{Learning}} over {{Fast-Evolving Relational Data}}},
  shorttitle = {F-{{IVM}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nikolic, Milos and Zhang, Haozhe and Kara, Ahmet and Olteanu, Dan},
  date = {2020-06-11},
  pages = {2773--2776},
  publisher = {{ACM}},
  location = {{Portland OR USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3318464.3384702},
  urldate = {2022-06-15},
  abstract = {F-IVM is a system for real-time analytics such as machine learning applications over training datasets de ned by queries over fast-evolving relational databases. We will demonstrate F-IVM for three such applications: model selection, ChowLiu trees, and ridge linear regression.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '20: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-6735-6},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/MF26G8UX/Nikolic et al. - 2020 - F-IVM Learning over Fast-Evolving Relational Data.pdf}
}

@inproceedings{nikolicLINVIEWIncrementalView2014,
  title = {{{LINVIEW}}: Incremental View Maintenance for Complex Analytical Queries},
  shorttitle = {{{LINVIEW}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Nikolic, Milos and ElSeidy, Mohammed and Koch, Christoph},
  date = {2014-06-18},
  series = {{{SIGMOD}} '14},
  pages = {253--264},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  
  abstract = {Many analytics tasks and machine learning problems can be naturally expressed by iterative linear algebra programs. In this paper, we study the incremental view maintenance problem for such complex analytical queries. We develop a framework, called LINVIEW, for capturing deltas of linear algebra programs and understanding their computational cost. Linear algebra operations tend to cause an avalanche effect where even very local changes to the input matrices spread out and infect all of the intermediate results and the final view, causing incremental view maintenance to lose its performance benefit over re-evaluation. We develop techniques based on matrix factorizations to contain such epidemics of change. As a consequence, our techniques make incremental view maintenance of linear algebra practical and usually substantially cheaper than re-evaluation. We show, both analytically and experimentally, the usefulness of these techniques when applied to standard analytics tasks. Our evaluation demonstrates the efficiency of LINVIEW in generating parallel incremental programs that outperform re-evaluation techniques by more than an order of magnitude.},
  keywords = {compilation,incremental view maintenance,linear algebra,machine learning,notion,spark},
  file = {/Users/jessie/Zotero/storage/VKJBM6BI/Nikolic et al. - 2014 - LINVIEW incremental view maintenance for complex .pdf}
}

@article{olteanuRegressionModelsFactorized2016,
  title = {F: Regression Models over Factorized Views},
  shorttitle = {F},
  author = {Olteanu, Dan and Schleich, Maximilian},
  date = {2016-09},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {9},
  number = {13},
  pages = {1573--1576},
  issn = {2150-8097},
  
  url = {https://dl.acm.org/doi/10.14778/3007263.3007312},
  urldate = {2022-06-14},
  abstract = {We demonstrate F, a system for building regression models over database views. At its core lies the observation that the computation and representation of materialized views, and in particular of joins, entail non-trivial redundancy that is not necessary for the efficient computation of aggregates used for building regression models. F avoids this redundancy by factorizing data and computation and can outperform the state-of-the-art systems MADlib, R, and Python StatsModels by orders of magnitude on real-world datasets. We illustrate how to incrementally build regression models over factorized views using both an in-memory implementation of F and its SQL encoding. We also showcase the effective use of F for model selection: F decouples the datadependent computation step from the data-independent convergence of model parameters and only performs once the former to explore the entire model space.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VDSCAFZN/Olteanu and Schleich - 2016 - F regression models over factorized views.pdf}
}

 

@article{olteanuRelationalDataBorg2020,
  author       = {Dan Olteanu},
  title        = {The Relational Data Borg is Learning},
  journal      = {Proc. {VLDB} Endow.},
  volume       = {13},
  number       = {12},
  pages        = {3502--3515},
  year         = {2020},
  url          = {http://www.vldb.org/pvldb/vol13/p3502-olteanu.pdf},
  doi          = {10.14778/3415478.3415572},
  timestamp    = {Tue, 23 Mar 2021 15:33:12 +0100},
  biburl       = {https://dblp.org/rec/journals/pvldb/Olteanu20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{phaniLIMAFinegrainedLineage2021,
  title = {{{LIMA}}: {{Fine-grained Lineage Tracing}} and {{Reuse}} in {{Machine Learning Systems}}},
  shorttitle = {{{LIMA}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Phani, Arnab and Rath, Benjamin and Boehm, Matthias},
  date = {2021-06-09},
  pages = {1426--1439},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  
  url = {https://dl.acm.org/doi/10.1145/3448016.3452788},
  urldate = {2022-06-09},
  abstract = {Machine learning (ML) and data science workflows are inherently exploratory. Data scientists pose hypotheses, integrate the necessary data, and run ML pipelines of data cleaning, feature engineering, model selection and hyper-parameter tuning. The repetitive nature of these workflows, and their hierarchical composition from building blocks exhibits high computational redundancy. Existing work addresses this redundancy with coarse-grained lineage tracing and reuse for ML pipelines. This approach allows using existing ML systems, but views entire algorithms as black boxes, and thus, fails to eliminate fine-grained redundancy and to handle internal non-determinism. In this paper, we introduce LIMA, a practical framework for efficient, fine-grained lineage tracing and reuse inside ML systems. Lineage tracing of individual operations creates new challenges and opportunities. We address the large size of lineage traces with multi-level lineage tracing and reuse, as well as lineage deduplication for loops and functions; exploit full and partial reuse opportunities across the program hierarchy; and integrate this framework with task parallelism and operator fusion. The resulting framework performs fine-grained lineage tracing with low overhead, provides versioning and reproducibility, and is able to eliminate fine-grained redundancy. Our experiments on a variety of ML pipelines show performance improvements up to 12.4x.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/U4Q9ZPJ8/U4Q9ZPJ8.pdf}
}

@inproceedings{polyzotisDataManagementChallenges2017,
  title = {Data {{Management Challenges}} in {{Production Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  date = {2017-05-09},
  pages = {1723--1726},
  publisher = {{ACM}},
  location = {{Chicago Illinois USA}},
  
  url = {https://dl.acm.org/doi/10.1145/3035918.3054782},
  urldate = {2022-06-15},
  abstract = {The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-4197-4},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/5J4K7HKZ/Polyzotis et al. - 2017 - Data Management Challenges in Production Machine L.pdf}
}






@inproceedings{sommerMNCStructureExploitingSparsity2019,
  title = {{{MNC}}: {{Structure-Exploiting Sparsity Estimation}} for {{Matrix Expressions}}},
  shorttitle = {{{MNC}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Sommer, Johanna and Boehm, Matthias and Evfimievski, Alexandre V. and Reinwald, Berthold and Haas, Peter J.},
  date = {2019-06-25},
  pages = {1607--1623},
  publisher = {{ACM}},
  location = {{Amsterdam Netherlands}},
  
  urldate = {2022-06-09},
  abstract = {Efficiently computing linear algebra expressions is central to machine learning (ML) systems. Most systems support sparse formats and operations because sparse matrices are ubiquitous and their dense representation can cause prohibitive overheads. Estimating the sparsity of intermediates, however, remains a key challenge when generating execution plans or performing sparse operations. These sparsity estimates are used for cost and memory estimates, format decisions, and result allocation. Existing estimators tend to focus on matrix products only, and struggle to attain good accuracy with low estimation overhead. However, a key observation is that real-world sparse matrices commonly exhibit structural properties such as a single non-zero per row, or columns with varying sparsity. In this paper, we introduce MNC (Matrix Non-zero Count), a remarkably simple, count-based matrix synopsis that exploits these structural properties for efficient, accurate, and general sparsity estimation. We describe estimators and sketch propagation for realistic linear algebra expressions. Our experiments—on a new estimation benchmark called SparsEst—show that the MNC estimator yields good accuracy with very low overhead. This behavior makes MNC practical and broadly applicable in ML systems.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '19: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/HTFABESS/HTFABESS.pdf}
}

@article{sunDimaDistributedInmemory2017,
  title = {Dima: A Distributed in-Memory Similarity-Based Query Processing System},
  shorttitle = {Dima},
  author = {Sun, Ji and Shang, Zeyuan and Li, Guoliang and Deng, Dong and Bao, Zhifeng},
  date = {2017-08-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {10},
  number = {12},
  pages = {1925--1928},
  issn = {2150-8097},
  
  urldate = {2022-04-05},
  abstract = {Data analysts in industries spend more than 80\% of time on data cleaning and integration in the whole process of data analytics due to data errors and inconsistencies. It calls for effective query processing techniques to tolerate the errors and inconsistencies. In this paper, we develop a distributed in-memory similarity-based query processing system called Dima. Dima supports two core similarity-based query operations, i.e., similarity search and similarity join. Dima extends the SQL programming interface for users to easily invoke these two operations in their data analysis jobs. To avoid expensive data transformation in a distributed environment, we design selectable signatures where two records approximately match if they share common signatures. More importantly, we can adaptively select the signatures to balance the workload. Dima builds signature-based global indexes and local indexes to support efficient similarity search and join. Since Spark is one of the widely adopted distributed in-memory computing systems, we have seamlessly integrated Dima into Spark and developed effective query optimization techniques in Spark. To the best of our knowledge, this is the first full-fledged distributed in-memory system that can support similarity-based query processing. We demonstrate our system in several scenarios, including entity matching, web table integration and query recommendation.},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/VIKNJAFT/Sun et al. - 2017 - Dima a distributed in-memory similarity-based que.pdf}
}

@article{thomasComparativeEvaluationSystems2018,
  title = {A Comparative Evaluation of Systems for Scalable Linear Algebra-Based Analytics},
  author = {Thomas, Anthony and Kumar, Arun},
  date = {2018-09-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {11},
  number = {13},
  pages = {2168--2182},
  issn = {21508097},
  
  abstract = {The growing use of statistical and machine learning (ML) algorithms to analyze large datasets has given rise to new systems to scale such algorithms. But implementing new scalable algorithms in low-level languages is a painful process, especially for enterprise and scientific users. To mitigate this issue, a new breed of systems expose high-level bulk linear algebra (LA) primitives that are scalable. By composing such LA primitives, users can write analysis algorithms in a higher-level language, while the system handles scalability issues. But there is little work on a unified comparative evaluation of the scalability, efficiency, and effectiveness of such “scalable LA systems.” We take a major step towards filling this gap. We introduce a suite of LA-specific tests based on our analysis of the data access and communication patterns of LA workloads and their use cases. Using our tests, we perform a comprehensive empirical comparison of a few popular scalable LA systems: MADlib, MLlib, SystemML, ScaLAPACK, SciDB, and TensorFlow using both synthetic data and a large real-world dataset. Our study has revealed several scalability bottlenecks, unusual performance trends, and even bugs in some systems. Our findings have already led to improvements in SystemML, with other systems’ developers also expressing interest. All of our code and data scripts are available for download at https://adalabucsd.github.io/slab.html.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/E6LR3FCC/Thomas and Kumar - 2018 - A comparative evaluation of systems for scalable l.pdf}
}

@inproceedings{yangFactorizedSVMGaussian2020,
  title = {Towards {{Factorized SVM}} with {{Gaussian Kernels}} over {{Normalized Data}}},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Yang, Keyu and Gao, Yunjun and Liang, Lei and Yao, Bin and Wen, Shiting and Chen, Gang},
  date = {2020-04},
  pages = {1453--1464},
  issn = {2375-026X},
  
  abstract = {There is an emerging trend of integrating machine learning (ML) techniques into database systems (DB). Considering that almost all the ML toolkits assume that the input of ML algorithms is a single table even though many real-world datasets are stored as multiple tables due to normalization in DB. Thus, data scientists have to perform joins before learning a ML model. This strategy is called learning after joins, which incurs redundancy avoided by normalization. In the area of ML, the Support Vector Machine (SVM) is one of the most standard classification tools. In this paper, we focus on the factorized SVM with gaussian kernels over normalized data. We present factorized learning approaches for two main SVM optimization methods, i.e., Gradient Descent (GD) and Sequential Minimal Optimization (SMO), by factorizing gaussian kernel function computation. Furthermore, we transform the normalized data into matrices, and boost the efficiency of SVM learning via linear algebra operations. Extensive experiments with nine real normalized data sets demonstrate the efficiency and scalability of our proposed approaches.},
  eventtitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {Databases,Kernel,Machine learning,Motion pictures,notion,Optimization,Support vector machines,Training},
  file = {/Users/jessie/Zotero/storage/6Q3SU359/Yang et al. - 2020 - Towards Factorized SVM with Gaussian Kernels over .pdf;/Users/jessie/Zotero/storage/DJERC4EB/9101671.html}
}

@article{yuWindTunnelDifferentiableML2021,
  title = {{{WindTunnel}}: Towards Differentiable {{ML}} Pipelines beyond a Single Model},
  shorttitle = {{{WindTunnel}}},
  author = {Yu, Gyeong-In and Amizadeh, Saeed and Kim, Sehoon and Pagnoni, Artidoro and Zhang, Ce and Chun, Byung-Gon and Weimer, Markus and Interlandi, Matteo},
  date = {2021-09},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {15},
  number = {1},
  pages = {11--20},
  issn = {2150-8097},
  
  abstract = {While deep neural networks (DNNs) have shown to be successful in several domains like computer vision, non-DNN models such as linear models and gradient boosting trees are still considered state-of-the-art over tabular data. When using these models, data scientists often author machine learning (ML) pipelines: DAG of ML operators comprising data transforms and ML models, whereby each operator is sequentially trained one-at-a-time. Conversely, when training DNNs, layers composing the neural networks are simultaneously trained using backpropagation.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/E225GEK4/Yu et al. - 2021 - WindTunnel towards differentiable ML pipelines be.pdf}
}

@article{zhangMaterializationOptimizationsFeature2016,
  title = {Materialization {{Optimizations}} for {{Feature Selection Workloads}}},
  author = {Zhang, Ce and Kumar, Arun and Ré, Christopher},
  date = {2016-04-07},
  journaltitle = {ACM Transactions on Database Systems},
  shortjournal = {ACM Trans. Database Syst.},
  volume = {41},
  number = {1},
  pages = {1--32},
  issn = {0362-5915, 1557-4644},
  
  urldate = {2022-06-15},
  abstract = {There is an arms race in the data management industry to support statistical analytics. Feature selection, the process of selecting a feature set that will be used to build a statistical model, is widely regarded as the most critical step of statistical analytics. Thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature selection language and a supporting prototype system that builds on top of current industrial R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analogue in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of datasets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new trade-off space across multiple R backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection.},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/CJD66FHV/Zhang et al. - 2016 - Materialization Optimizations for Feature Selectio.pdf}
}



%10.1145/2882903.2882939
@inproceedings{schleichLearningLinearRegression2016,
  title = {Learning {{Linear Regression Models}} over {{Factorized Joins}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Schleich, Maximilian and Olteanu, Dan and Ciucanu, Radu},
  date = {2016-06-14},
  series = {{{SIGMOD}} '16},
  pages = {3--18},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  

}

@article{10.14778/2535573.2488340,
author = {Rendle, Steffen},
title = {Scaling Factorization Machines to Relational Data},
year = {2013},
issue_date = {March 2013},
publisher = {VLDB Endowment},
volume = {6},
number = {5},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
month = {mar},
pages = {337–348},
numpages = {12}
}

@article{bos2018prediction,
  title={Prediction of clinically relevant adverse drug events in surgical patients},
  author={Bos, Jacqueline M and Kalkman, Gerard A and Groenewoud, Hans and van den Bemt, Patricia MLA and De Smet, Peter AGM and Nagtegaal, J Elsbeth and Wieringa, Andre and van der Wilt, Gert Jan and Kramers, Cornelis},
  journal={PloS one},
  volume={13},
  number={8},
  pages={e0201645},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@article{Fagin2005c,
author = {Fagin, R. and Kolaitis, P. G. and Miller, R. J. and Popa, L.},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Certain answers,Chase,Computational complexity,Data exchange,Data integration,Dependencies,First-order inexpressibility,Query answering,Universal solution},
number = {1},
pages = {89--124},
title = {{Data exchange: Semantics and query answering}},
volume = {336},
year = {2005}
}


@article{beeri1984proof,
author = {Beeri, C. and Vardi, M. Y.},
journal = {JACM},
mendeley-groups = {PhdDissertation/schema mapping/Mixed Formalism},
number = {4},
pages = {718--741},
publisher = {ACM},
title = {{A proof procedure for data dependencies}},
volume = {31},
year = {1984}
}

@Inbook{fagin2009,
author="Fagin, R.",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="Tuple-Generating Dependencies",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="3201--3202",
isbn="978-0-387-39940-9",

}

@inproceedings{lenzerini2002data,
author = {Lenzerini, Maurizio},
booktitle = {PODS},
organization = {ACM},
pages = {233--246},
title = {{Data integration: A theoretical perspective}},
year = {2002}
}

@article{chepurko2020arda,
  title={ARDA: automatic relational data augmentation for machine learning},
  author={Chepurko, N. and Marcus, R. and Zgraggen, E. and Fernandez, R. Castro and Kraska, T. and Karger, D.},
  journal={VLDB},
  volume={13},
  number={9},
  pages={1373--1387},
  year={2020},
  publisher={VLDB Endowment}
}

@inproceedings{esmailoghli2021cocoa,
  title={COCOA: COrrelation COefficient-Aware Data Augmentation.},
  author={Esmailoghli, Mahdi and Quian{\'e}-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch},
  booktitle={EDBT},
  pages={331--336},
  year={2021}
}

@inproceedings{kumar2016join,
annote = {Schema Independent, yet same answer

learning},
author = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M and Zhu, Xiaojin},
booktitle = {Proceedings of the 2016 International Conference on Management of Data},
file = {:D\:/Students/Andra/refs/To Join or Not to Join  Thinking Twice about Joins before Feature Selection.pdf:pdf},
mendeley-groups = {2systemInOne/AIndDB/Database Techniques for Machine Learning/JOINrelated,ML/data augmentation,2systemInOne/Factorization/Factorized Learning,Data augmentation/feature augment},
pages = {19--34},
title = {{To join or not to join? thinking twice about joins before feature selection}},
year = {2016}
}

@book{yang2019federated,
  title={Federated Learning},
  author={Yang, Qiang and Liu, Yang and Cheng, Yong and Kang, Yan and Chen, Tianjian and Yu, Han},
  year={2019},
  publisher={Morgan \& Claypool Publishers}
}



@article{yang@2019fml,
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
title = {Federated Machine Learning: Concept and Applications},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2157-6904},

journal = {ACM Trans. Intell. Syst. Technol.},
month = {jan},
articleno = {12},
numpages = {19},
keywords = {transfer learning, GDPR, Federated learning}
}


@article{fagin2005data,
  title={Data exchange: getting to the core},
  author={Fagin, Ronald and Kolaitis, Phokion G and Popa, Lucian},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={30},
  number={1},
  pages={174--210},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{deutsch2006query,
  title={Query reformulation with constraints},
  author={Deutsch, Alin and Popa, Lucian and Tannen, Val},
  journal={ACM SIGMOD Record},
  volume={35},
  number={1},
  pages={65--73},
  year={2006},
  publisher={ACM New York, NY, USA}
}

@article{chirkova2012materialized,
  title={Materialized views},
  author={Chirkova, Rada and Yang, Jun and others},
  journal={Foundations and Trends{\textregistered} in Databases},
  volume={4},
  number={4},
  pages={295--405},
  year={2012},
  publisher={Now Publishers, Inc.}
}

@book{cormen2022introduction,
  title={Introduction to algorithms},
  author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  year={2022},
  publisher={MIT press}
}

@inproceedings{koutras2021valentine,
  author={Koutras, Christos and Siachamis, George and Ionescu, Andra and Psarakis, Kyriakos and Brons, Jerry and Fragkoulis, Marios and Lofi, Christoph and Bonifati, Angela and Katsifodimos, Asterios},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, 
  title={Valentine: Evaluating Matching Techniques for Dataset Discovery}, 
  year={2021},
  volume={},
  number={},
  pages={468-479},
}


@article{arocena2015ibench,
author = {Arocena, Patricia C and Glavic, Boris and Ciucanu, Radu and Miller, Ren{\'{e}}e J},
journal = {Proceedings of the VLDB Endowment},
number = {3},
pages = {108--119},
publisher = {VLDB Endowment},
title = {{The iBench integration metadata generator}},
volume = {9},
year = {2015}
}

@article{alexe2008stbenchmark,
  author    = {Bogdan Alexe and
               Wang Chiew Tan and
               Yannis Velegrakis},
  title     = {STBenchmark: towards a benchmark for mapping systems},
  journal   = {{PVLDB}},
  volume    = {1},
  number    = {1},
  pages     = {230--244},
  year      = {2008},
  url       = {http://www.vldb.org/pvldb/1/1453886.pdf},
  timestamp = {Thu, 16 Aug 2018 11:33:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/pvldb/AlexeTV08},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhao2018random,
  title={Random sampling over joins revisited},
  author={Zhao, Zhuoyue and Christensen, Robert and Li, Feifei and Hu, Xiao and Yi, Ke},
  booktitle={Proceedings of the 2018 International Conference on Management of Data},
  pages={1525--1539},
  year={2018}
}

@article{benson2015framework,
  title={A framework for practical parallel fast matrix multiplication},
  author={Benson, Austin R and Ballard, Grey},
  journal={ACM SIGPLAN Notices},
  volume={50},
  number={8},
  pages={42--53},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{naumann2020sparse,
  title={On Sparse Matrix Chain Products},
  author={Naumann, Uwe},
  booktitle={2020 Proceedings of the SIAM Workshop on Combinatorial Scientific Computing},
  pages={118--127},
  year={2020},
  organization={SIAM}
}

@book{ramakrishnan2003database,
  title={Database management systems},
  author={Ramakrishnan, Raghu and Gehrke, Johannes and Gehrke, Johannes},
  volume={3},
  year={2003},
  publisher={McGraw-Hill New York}
}


@article{nargesian2018table,
  title={Table union search on open data},
  author={Nargesian, Fatemeh and Zhu, Erkang and Pu, Ken Q and Miller, Ren{\'e}e J},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={7},
  pages={813--825},
  year={2018},
  publisher={VLDB Endowment}
}

@Inbook{Fagin2009egds,
author="Fagin, Ronald",
title="Equality-Generating Dependencies",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="1009--1010",
}

@article{yuster2005fast,
  title={Fast sparse matrix multiplication},
  author={Yuster, Raphael and Zwick, Uri},
  journal={ACM Transactions On Algorithms (TALG)},
  volume={1},
  number={1},
  pages={2--13},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{chen2017towards,
  title={Towards Linear Algebra over Normalized Data},
  author={Chen, L. and Kumar, A. and Naughton, J. and Patel, J. M.},
  journal={PVLDB},
  volume={10},
  number={11},
  year={2017}
}

@inproceedings{kumar2015learning,
author = {Kumar, A. and Naughton, J. and Patel, J. M.},
booktitle = {{SIGMOD}},
pages = {1969--1984},
title = {{Learning generalized linear models over normalized data}},
year = {2015}
}

@article{kumar_demonstration_2015,
	title = {Demonstration of {Santoku}: optimizing machine learning over normalized data},
	volume = {8},
	shorttitle = {Demonstration of {Santoku}},
	
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kumar, Arun and Jalal, Mona and Yan, Boqun and Naughton, Jeffrey and Patel, Jignesh M.},
	month = aug,
	year = {2015},
	keywords = {notion},
	pages = {1864--1867},
}

@inproceedings{alotaibi2021hadad,
  title={{HADAD}: A Lightweight Approach for Optimizing Hybrid Complex Analytics Queries},
  author={Alotaibi, R. and Cautis, B. and Deutsch, A. and Manolescu, I.},
  booktitle={SIGMOD},
  pages={23--35},
  year={2021}
}

@inproceedings{cheng_efficient_2021,
	title = {Efficient {Construction} of {Nonlinear} {Models} over {Normalized} {Data}},
	
	author = {Cheng, Zhaoyue and Koudas, Nick and Zhang, Zhe and Yu, Xiaohui},
	month = apr,
	year = {2021},
	keywords = {Artificial neural networks, Computational modeling, Mixture models, Network architecture, Redundancy, Systematics, Training, notion},
	pages = {1140--1151},
}


@inproceedings{10.1145/3299869.3319878,
author = {Li, Side and Chen, Lingjiao and Kumar, Arun},
booktitle = {SIGMOD},

pages = {1571--1588},
title = {{Enabling and Optimizing Non-Linear Feature Interactions in Factorized Linear Algebra}},
year = {2019}
}

@inproceedings{khamis_acdc_2018,
	address = {Houston TX USA},
	title = {{AC}/{DC}: {In}-{Database} {Learning} {Thunderstruck}},
	shorttitle = {{AC}/{DC}},
	
	language = {en},
	booktitle = {Proceedings of the {Second} {Workshop} on {Data} {Management} for {End}-{To}-{End} {Machine} {Learning}},
	publisher = {ACM},
	author = {Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
	month = jun,
	year = {2018},
	keywords = {notion},
	pages = {1--10},
}

@article{dsilva_aida_2018,
	title = {{AIDA}: abstraction for advanced in-database analytics},
	volume = {11},
	shorttitle = {{AIDA}},
	
	language = {en},
	number = {11},
	journal = {Proceedings of the VLDB Endowment},
	author = {D'silva, Joseph Vinish and De Moor, Florestan and Kemme, Bettina},
	month = jul,
	year = {2018},
	keywords = {notion},
	pages = {1400--1413},
}
 
@article{dsilva_making_2019,
  author    = {Joseph Vinish D'silva and
               Florestan {De Moor} and
               Bettina Kemme},
  title     = {Making an {RDBMS} Data Scientist Friendly: Advanced In-database Interactive
               Analytics with Visualization Support},
  journal   = {Proc. {VLDB} Endow.},
  volume    = {12},
  number    = {12},
  pages     = {1930--1933},
  year      = {2019},
  timestamp = {Sat, 25 Apr 2020 13:58:53 +0200},
  biburl    = {https://dblp.org/rec/journals/pvldb/DsilvaMK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{schleich_layered_2019,
author = {Schleich, Maximilian and Olteanu, Dan and Abo Khamis, Mahmoud and Ngo, Hung Q. and Nguyen, XuanLong},
title = {A Layered Aggregate Engine for Analytics Workloads},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1642–1659},
numpages = {18},
keywords = {mutliple aggregate optimization, structure-aware analytics},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{fagin1986theory,
  title={The theory of data dependencies},
  author={Fagin, Ronald and Vardi, M},
  journal={Mathematics of Information Processing},
  volume={34},
  pages={19},
  year={1986},
  publisher={American Mathematical Soc.}
}

@book{deisenroth2020mathematics,
  title={Mathematics for machine learning},
  author={Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  year={2020},
  publisher={Cambridge University Press}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@inproceedings{hai2023amalur,
      title={Amalur: Data Integration Meets Machine Learning}, 
      author={Hai, Rihan and Koutras, Christos and Ionescu, Andra and Li, Ziyu and Sun, Wenbo and Jessie, van Schijndel and Kang, Yan and Katsifodimos, Asterios},
     booktitle={ICDE},
   year={2023},
     pages={To appear}

}

@Inbook{Fuxman2009,
author="Fuxman, Ariel
and Miller, Ren{\'e}e J.",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="Schema Mapping",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="2481--2488",
isbn="978-0-387-39940-9",
}

 @book{kelleher2020fundamentals,
  title={Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies},
  author={Kelleher, John D and Mac Namee, Brian and D'arcy, Aoife},
  year={2020},
  publisher={MIT press}
}

@article{horowitz1982fundamentals,
  title={Fundamentals of data structures},
  author={Horowitz, Ellis and Sahni, Sartaj},
  year={1982}
}




@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{hummingbird1,
author = {Nakandala, Supun and Saur, Karla and Yu, Gyeong-In and Karanasos, Konstantinos and Curino, Carlo and Weimer, Markus and Interlandi, Matteo},
title = {A Tensor Compiler for Unified Machine Learning Prediction Serving},
year = {2020},
isbn = {978-1-939133-19-9},
publisher = {USENIX Association},
address = {USA},
booktitle = {OSDI 2020},
articleno = {51},
numpages = {19},
}

@inproceedings{TCU,
  title={{TCUDB: Accelerating Database with Tensor Processors}},
  author={Hu, Yu-Ching and Li, Yuliang Li and Tseng, Hung-Wei},
  booktitle={SIGMOD},
  year={2022}
}

@inproceedings{wenbo_hard,
      title={An Empirical Performance Comparison between Matrix Multiplication Join and Hash Join on GPUs}, 
      author={Sun, Wenbo and Katsifodimos, Asterios and Hai, Rihan},
     booktitle={ICDE Workshop: HardBD \& Active},
   year={2023},
     pages={To appear}

}

@inproceedings{DBLP:conf/pods/GolshanHMT17,
author = {Golshan, Behzad and Halevy, Alon Y and Mihaila, George A and Tan, Wang-Chiew},
booktitle = {Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2017, Chicago, IL, USA},

file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golshan et al. - 2017 - Data Integration After the Teenage Years.pdf:pdf},
mendeley-groups = {PhdDissertation/schema mapping/benchmarks},
pages = {101--106},
title = {{Data Integration: After the Teenage Years}},
year = {2017}
}

@inproceedings{10.5555/1182635.1164130,
author = {Halevy, Alon and Rajaraman, Anand and Ordille, Joann},
title = {Data Integration: The Teenage Years},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {9–16},
numpages = {8},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{kolaitis2005schema,
  title={Schema mappings, data exchange, and metadata management},
  author={Kolaitis, Phokion G},
  booktitle={Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
  pages={61--75},
  year={2005}
}

@article{liu2018mlbench,
  title={MLbench: benchmarking machine learning services against human experts},
  author={Liu, Yu and Zhang, Hantian and Zeng, Luyuan and Wu, Wentao and Zhang, Ce},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={10},
  pages={1220--1232},
  year={2018},
  publisher={VLDB Endowment}
}
% 
@misc{tech,
  TITLE =         {{Ilargi: a GPU Compatible Factorized ML Learning Framework - Technical Report}},
  YEAR  =         {2024},
  note =  {{Technical report. https://raw.githubusercontent.com/
amademicnoboday12/ilargi/main/paper/technical\_report.pdf}}
}



@article{lee_algorithms_2000,
  title={Algorithms for non-negative matrix factorization},
  author={Lee, Daniel and Seung, H Sebastian},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@book{abiteboul1995foundations,
  title={Foundations of databases},
  author={Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  volume={8},
  year={1995},
  publisher={Addison-Wesley Reading}
}

@article{DBLP:journals/debu/StonebrakerI18,
  author       = {Michael Stonebraker and
                  Ihab F. Ilyas},
  title        = {Data Integration: The Current Status and the Way Forward},
  journal      = {{IEEE} Data Eng. Bull.},
  volume       = {41},
  number       = {2},
  pages        = {3--9},
  year         = {2018},
  url          = {http://sites.computer.org/debull/A18june/p3.pdf},
  timestamp    = {Tue, 10 Mar 2020 16:23:50 +0100},
  biburl       = {https://dblp.org/rec/journals/debu/StonebrakerI18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/sigmod/PolyzotisRWZ18,
  author       = {Neoklis Polyzotis and
                  Sudip Roy and
                  Steven Euijong Whang and
                  Martin Zinkevich},
  title        = {Data Lifecycle Challenges in Production Machine Learning: {A} Survey},
  journal      = {{SIGMOD} Rec.},
  volume       = {47},
  number       = {2},
  pages        = {17--28},
  year         = {2018},
  timestamp    = {Fri, 06 Mar 2020 21:55:44 +0100},
  biburl       = {https://dblp.org/rec/journals/sigmod/PolyzotisRWZ18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{10107808,
  author={Hai, Rihan and Koutras, Christos and Quix, Christoph and Jarke, Matthias},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Data Lakes: A Survey of Functions and Systems}, 
  year={2023},
  volume={},
  number={},
  pages={1-20},
}

@article{DBLP:journals/vldb/AliW17,
  author       = {Syed Muhammad Fawad Ali and
                  Robert Wrembel},
  title        = {From conceptual design to performance optimization of {ETL} workflows:
                  current state of research and open problems},
  journal      = {{VLDB} J.},
  volume       = {26},
  number       = {6},
  pages        = {777--801},
  year         = {2017},
  timestamp    = {Wed, 07 Dec 2022 23:01:39 +0100},
  biburl       = {https://dblp.org/rec/journals/vldb/AliW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tpc_ds,
author = {Nambiar, Raghunath Othayoth and Poess, Meikel},
title = {The Making of TPC-DS},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1049–1058},
numpages = {10},
location = {Seoul, Korea},
series = {VLDB '06}
}

@inproceedings{joinboost,
author = {Nambiar, Raghunath Othayoth and Poess, Meikel},
title = {The Making of TPC-DS},
year = {2006},
publisher = {VLDB Endowment},
booktitle = {Proceedings of the 32nd International Conference on Very Large Data Bases},
pages = {1049–1058},
numpages = {10},
location = {Seoul, Korea},
series = {VLDB '06}
}

@ARTICLE{Roh2021,
  author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective}, 
  year={2021},
  volume={33},
  number={4},
  pages={1328-1347},
}

@article{DBLP:journals/corr/abs-1904-09483,
  author       = {Peng Li and
                  Xi Rao and
                  Jennifer Blase and
                  Yue Zhang and
                  Xu Chu and
                  Ce Zhang},
  title        = {CleanML: {A} Benchmark for Joint Data Cleaning and Machine Learning
                  [Experiments and Analysis]},
  journal      = {CoRR},
  volume       = {abs/1904.09483},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09483},
  eprinttype    = {arXiv},
  eprint       = {1904.09483},
  timestamp    = {Fri, 05 Aug 2022 15:35:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09483.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/pvldb/EbraheemTJOT18,
  author       = {Muhammad Ebraheem and
                  Saravanan Thirumuruganathan and
                  Shafiq R. Joty and
                  Mourad Ouzzani and
                  Nan Tang},
  title        = {Distributed Representations of Tuples for Entity Resolution},
  journal      = {Proc. {VLDB} Endow.},
  volume       = {11},
  number       = {11},
  pages        = {1454--1467},
  year         = {2018},
  url          = {http://www.vldb.org/pvldb/vol11/p1454-ebraheem.pdf},
  timestamp    = {Sat, 25 Apr 2020 13:58:45 +0200},
  biburl       = {https://dblp.org/rec/journals/pvldb/EbraheemTJOT18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3555041.3589409,
author = {Fan, Grace and Wang, Jin and Li, Yuliang and Miller, Ren\'{e}e J.},
title = {Table Discovery in Data Lakes: State-of-the-Art and Future Directions},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Data discovery refers to a set of tasks that enable users and downstream applications to explore and gain insights from massive collections of data sources such as data lakes. In this tutorial, we will provide a comprehensive overview of the most recent table discovery techniques developed by the data management community. We will cover table understanding tasks such as domain discovery, table annotation, and table representation learning which help data lake systems capture semantics of tables. We will also cover techniques enabling various query-driven discovery and table exploration tasks, as well as how table discovery can support key data science applications such as machine learning and knowledge base construction. Finally, we will discuss future research directions on developing new table discovery paradigms by combining structured knowledge and dense table representations, as well as improving the efficiency of discovery using state-of-the-art indexing techniques, and more.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {69–75},
numpages = {7},
keywords = {data lake, dataset discovery, data integration, unionable tables},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@book{DBLP:books/lib/Murphy12,
  author       = {Kevin P. Murphy},
  title        = {Machine learning - a probabilistic perspective},
  series       = {Adaptive computation and machine learning series},
  publisher    = {{MIT} Press},
  year         = {2012},
  isbn         = {0262018020},
  timestamp    = {Wed, 26 Apr 2017 17:48:08 +0200},
  biburl       = {https://dblp.org/rec/books/lib/Murphy12.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{euzenat2007ontology,
  title={Ontology matching},
  author={Euzenat, J{\'e}r{\^o}me and Shvaiko, Pavel and others},
  volume={18},
  year={2007},
  publisher={Springer}
}

@inproceedings{aumueller2005schema,
  title={Schema and ontology matching with COMA++},
  author={Aumueller, David and Do, Hong-Hai and Massmann, Sabine and Rahm, Erhard},
  booktitle={Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
  pages={906--908},
  year={2005}
}

 
@online{scikit-learn,
    title = "scikit-learn",
    note  = "\url{https://scikit-learn.org/stable/}",
}



 
@online{Keras,
    title = "Keras",
    note  = "\url{https://keras.io/}",
}


 
 
 
 
@online{Pandas,
    title = "Pandas",
    note  = "\url{https://pandas.pydata.org/}",
}

@online{Informatica,
    title = "Informatica",
    note  = "\url{https://www.informatica.com/es/products/data-integration/powercenter.html}",
}
 
@online{Pentaho,
    title = "Pentaho",
    note  = "\url{https://www.hitachivantara.com/en-us/products/pentaho-platform/data-integration-analytics.html}",
}

@online{Talend,
    title = "Talend",
    note  = "\url{https://www.talend.com/products/integrate-data/}",
}

@online{SSIS,
    title = "SQL Server Integration Services (SSIS)",
    note  = "\url{https://learn.microsoft.com/en-us/sql/integration-services/sql-server-integration-services?view=sql-server-ver16}",
}
 
 

@book{DBLP:books/sp/06/GNGZ2006,
  editor       = {Isabelle Guyon and
                  Masoud Nikravesh and
                  Steve R. Gunn and
                  Lotfi A. Zadeh},
  title        = {Feature Extraction - Foundations and Applications},
  series       = {Studies in Fuzziness and Soft Computing},
  volume       = {207},
  publisher    = {Springer},
  year         = {2006},
  isbn         = {978-3-540-35487-1},
  timestamp    = {Fri, 26 Jul 2019 12:50:23 +0200},
  biburl       = {https://dblp.org/rec/books/sp/06/GNGZ2006.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yu2004efficient,
author = {Yu, Lei and Liu, Huan},
journal = {The Journal of Machine Learning Research},
mendeley-groups = {data augmentation/feature selection},
pages = {1205--1224},
publisher = {JMLR. org},
title = {{Efficient feature selection via analysis of relevance and redundancy}},
volume = {5},
year = {2004}
}



@article{rekatsinas2017holoclean,
  title={HoloClean: Holistic Data Repairs with Probabilistic Inference},
  author={Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F and R{\'e}, Christopher},
  journal={Proceedings of the VLDB Endowment},
  volume={10},
  number={11},
  year={2017}
}

@article{biessmann2019datawig,
  title={DataWig: Missing Value Imputation for Tables.},
  author={Biessmann, Felix and Rukat, Tammo and Schmidt, Philipp and Naidu, Prathik and Schelter, Sebastian and Taptunov, Andrey and Lange, Dustin and Salinas, David},
  journal={J. Mach. Learn. Res.},
  volume={20},
  number={175},
  pages={1--6},
  year={2019}
}

@book{DBLP:books/cs/Maier83,
  author    = {David Maier},
  title     = {The Theory of Relational Databases},
  publisher = {Computer Science Press},
  year      = {1983},
  isbn      = {0-914894-42-0},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@inproceedings{DBLP:conf/vldb/FuxmanHHMPP06,
  author       = {Ariel Fuxman and
                  Mauricio A. Hern{\'{a}}ndez and
                  C. T. Howard Ho and
                  Ren{\'{e}}e J. Miller and
                  Paolo Papotti and
                  Lucian Popa},
  editor       = {Umeshwar Dayal and
                  Kyu{-}Young Whang and
                  David B. Lomet and
                  Gustavo Alonso and
                  Guy M. Lohman and
                  Martin L. Kersten and
                  Sang Kyun Cha and
                  Young{-}Kuk Kim},
  title        = {Nested Mappings: Schema Mapping Reloaded},
  booktitle    = {Proceedings of the 32nd International Conference on Very Large Data
                  Bases, Seoul, Korea, September 12-15, 2006},
  pages        = {67--78},
  publisher    = {{ACM}},
  year         = {2006},
  url          = {http://dl.acm.org/citation.cfm?id=1164135},
  timestamp    = {Thu, 12 Mar 2020 11:33:40 +0100},
  biburl       = {https://dblp.org/rec/conf/vldb/FuxmanHHMPP06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/pods/KolaitisPSS14,
  author       = {Phokion G. Kolaitis and
                  Reinhard Pichler and
                  Emanuel Sallinger and
                  Vadim Savenkov},
  editor       = {Richard Hull and
                  Martin Grohe},
  title        = {Nested dependencies: structure and reasoning},
  booktitle    = {Proceedings of the 33rd {ACM} {SIGMOD-SIGACT-SIGART} Symposium on
                  Principles of Database Systems, PODS'14, Snowbird, UT, USA, June 22-27,
                  2014},
  pages        = {176--187},
  publisher    = {{ACM}},
  year         = {2014},
  timestamp    = {Sat, 19 Oct 2019 20:32:10 +0200},
  biburl       = {https://dblp.org/rec/conf/pods/KolaitisPSS14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icdt/CateK09,
  author       = {Balder ten Cate and
                  Phokion G. Kolaitis},
  editor       = {Ronald Fagin},
  title        = {Structural characterizations of schema-mapping languages},
  booktitle    = {Database Theory - {ICDT} 2009, 12th International Conference, St.
                  Petersburg, Russia, March 23-25, 2009, Proceedings},
  series       = {{ACM} International Conference Proceeding Series},
  volume       = {361},
  pages        = {63--72},
  publisher    = {{ACM}},
  year         = {2009},
  timestamp    = {Tue, 06 Nov 2018 16:59:26 +0100},
  biburl       = {https://dblp.org/rec/conf/icdt/CateK09.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/vldb/Halevy01,
  author       = {Alon Y. Halevy},
  title        = {Answering queries using views: {A} survey},
  journal      = {{VLDB} J.},
  volume       = {10},
  number       = {4},
  pages        = {270--294},
  year         = {2001},
  timestamp    = {Wed, 14 Nov 2018 10:27:40 +0100},
  biburl       = {https://dblp.org/rec/journals/vldb/Halevy01.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{10.14778/2876473.2876475,
author = {Calautti, Marco and Greco, Sergio and Molinaro, Cristian and Trubitsyna, Irina},
title = {Exploiting Equality Generating Dependencies in Checking Chase Termination},
year = {2016},
issue_date = {January 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {5},
issn = {2150-8097},
abstract = {The chase is a well-known algorithm with a wide range of applications in data exchange, data cleaning, data integration, query optimization, and ontological reasoning. Since the chase evaluation might not terminate and it is undecidable whether it terminates, the problem of defining (decidable) sufficient conditions ensuring termination has received a great deal of interest in recent years. In this regard, several termination criteria have been proposed. One of the main weaknesses of current approaches is the limited analysis they perform on equality generating dependencies (EGDs).In this paper, we propose sufficient conditions ensuring that a set of dependencies has at least one terminating chase sequence. We propose novel criteria which are able to perform a more accurate analysis of EGDs. Specifically, we propose a new stratification criterion and an adornment algorithm. The latter can both be used as a termination criterion and be combined with current techniques to make them more effective, in that strictly more sets of dependencies are identified. Our techniques identify sets of dependencies that are not recognized by any of the current criteria.},
journal = {Proc. VLDB Endow.},
month = {jan},
pages = {396–407},
numpages = {12}
}

@article{10.14778/3565838.3565850,
author = {Bellomarini, Luigi and Benedetto, Davide and Brandetti, Matteo and Sallinger, Emanuel},
title = {Exploiting the Power of Equality-Generating Dependencies in Ontological Reasoning},
year = {2022},
issue_date = {September 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {13},
issn = {2150-8097},
abstract = {Equality-generating dependencies (EGDs) allow to fully exploit the power of existential quantification in ontological reasoning settings modeled via Tuple-Generating Dependencies (TGDs), by enabling value-assignment or forcing the equivalence of fresh symbols. These capabilities are at the core of many common reasoning tasks, including graph traversals, clustering, data matching and data fusion, and many more related real-world scenarios.However, the interplay of TGDs and EGDs is known to lead to undecidability or intractability of query answering in tractable Datalog+/- fragments, like Warded Datalog+/-, for which, in the sole presence of TGDs, query answering is PTIME in data complexity. Restrictions of equality constraints, like separable EGDs, have been studied, but all achieve decidability at the cost of limited expressive power, which makes them unsuitable for the mentioned tasks.This paper introduces the class of "harmless" EGDs, that subsume separable EGDs and allow to model a very broad class of tasks. We contribute a sufficient syntactic condition for testing harmlessness, an undecidable task in general. We argue that in Warded Datalog+/- with harmless EGDs, ontological reasoning is decidable and PTIME. From such theoretical underpinnings, we develop novel chase-based techniques for reasoning with harmless EGDs and present an implementation within the Vadalog system, a state-of-the-art Datalog-based reasoner. We provide full-scale experimental evaluation and comparative analysis.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {3976–3988},
numpages = {13}
}



 
@inproceedings{10.1145/1938551.1938583,
author = {Pichler, Reinhard and Skritek, Sebastian},
title = {The Complexity of Evaluating Tuple Generating Dependencies},
year = {2011},
isbn = {9781450305297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Dependencies have played an important role in database design for many years. More recently, they have also turned out to be central to data integration and data exchange. In this work we concentrate on tuple generating dependencies (tgds) which enforce the presence of certain tuples in a database instance if certain other tuples are already present. Previous complexity results in data integration and data exchange mainly referred to the data complexity. In this work, we study the query complexity and combined complexity of a fundamental problem related to tgds, namely checking if a given tgd is satisfied by a database instance. We also address an important variant of this problem which deals with updates (by inserts or deletes) of a database: Here we have to check if all previously satisfied tgds are still satisfied after an update. We show that the query complexity and combined complexity of these problems are much higher than the data complexity. However, we also prove sufficient conditions on the tgds to reduce this high complexity.},
booktitle = {Proceedings of the 14th International Conference on Database Theory},
pages = {244–255},
numpages = {12},
location = {Uppsala, Sweden},
series = {ICDT '11}
}

@inproceedings{fagin2003data,
  title={Data exchange: Semantics and query answering},
  author={Fagin, Ronald and Kolaitis, Phokion G and Miller, Ren{\'e}e J and Popa, Lucian},
  booktitle={Database Theory—ICDT 2003: 9th International Conference Siena, Italy, January 8--10, 2003 Proceedings 9},
  pages={207--224},
  year={2003},
  organization={Springer}
}

@inproceedings{khamisACDCInDatabase2018,
  title = {{{AC}}/{{DC}}: {{In-Database Learning Thunderstruck}}},
  shorttitle = {{{AC}}/{{DC}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Data Management}} for {{End-To-End Machine Learning}}},
  author = {Khamis, Mahmoud Abo and Ngo, Hung Q. and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
  date = {2018-06-15},
  pages = {1--10},
  publisher = {{ACM}},
  location = {{Houston TX USA}},
  
  eventtitle = {{{SIGMOD}}/{{PODS}} '18: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/GRJZWCK2/Khamis et al. - 2018 - ACDC In-Database Learning Thunderstruck.pdf}
}

@inproceedings{DBLP:conf/icde/ChengKZ021,
  author       = {Zhaoyue Cheng and
                  Nick Koudas and
                  Zhe Zhang and
                  Xiaohui Yu},
  title        = {Efficient Construction of Nonlinear Models over Normalized Data},
  booktitle    = {37th {IEEE} International Conference on Data Engineering, {ICDE} 2021,
                  Chania, Greece, April 19-22, 2021},
  pages        = {1140--1151},
  publisher    = {{IEEE}},
  year         = {2021},
  timestamp    = {Fri, 25 Jun 2021 11:31:22 +0200},
  biburl       = {https://dblp.org/rec/conf/icde/ChengKZ021.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{alotaibiHADADLightweightApproach2021,
  title = {{{HADAD}}: {{A Lightweight Approach}} for {{Optimizing Hybrid Complex Analytics Queries}}},
  shorttitle = {{{HADAD}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Alotaibi, Rana and Cautis, Bogdan and Deutsch, Alin and Manolescu, Ioana},
  date = {2021-06-09},
  pages = {23--35},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  
  abstract = {Hybrid complex analytics workloads typically include (i) data management tasks (joins, selections, etc. ), easily expressed using relational algebra (RA)-based languages, and (ii) complex analytics tasks (regressions, matrix decompositions, etc.), mostly expressed in linear algebra (LA) expressions. Such workloads are common in many application areas, including scientific computing, web analytics, and business recommendation. Existing solutions for evaluating hybrid analytical tasks – ranging from LA-oriented systems, to relational systems (extended to handle LA operations), to hybrid systems – either optimize data management and complex tasks separately, exploit RA properties only while leaving LA-specific optimization opportunities unexploited, or focus heavily on physical optimization, leaving semantic query optimization opportunities unexplored. Additionally, they are not able to exploit precomputed (materialized) results to avoid recomputing (part of) a given mixed (RA and/or LA) computation.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  langid = {english},
  keywords = {notion},
  file = {/Users/jessie/Zotero/storage/LY9TT3KW/Alotaibi et al. - 2021 - HADAD A Lightweight Approach for Optimizing Hybri.pdf}
}

@inproceedings{tqp,
author = {He, Dong and Nakandala, Supun C and Banda, Dalitso and Sen, Rathijit and Saur, Karla and Park, Kwanghyun and Curino, Carlo and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Karanasos, Konstantinos and Interlandi, Matteo},
title = {{Query Processing on Tensor Computation Runtimes}},
year = {2022},
issue_date = {July 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {11},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
pages = {2811–2825},
}

@inproceedings{systemds,
  title={SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle},
  author={Boehm, Matthias and Antonov, Iulian and Baunsgaard, Sebastian and Dokter, Mark and Ginthoer, Robert Erich and Innerebner, Kevin and Klezin, Florijan and Lindstaedt, Stefanie and Phani, Arnab and Rath, Benjamin and others},
  year={2020},
  booktitle={CIDR}
}

@misc{blazing,
title={{BlazingSQL}},
howpublished={\url{https://github.com/BlazingDB/blazingsql}},
author={{BlazingDB}},
year={{2020}}
}

@misc{heavy,
title = {{HeavyDB}},
howpublished={\url{https://github.com/heavyai/heavydb}},
author={{Heavy.ai}},
year={2022}
}

@inproceedings{joinproject2,
author = {Deep, Shaleen and Hu, Xiao and Koutris, Paraschos},
title = {{Fast Join Project Query Evaluation Using Matrix Multiplication}},
year = {2020},
booktitle = {SIGMOD 2020},
pages = {1213–1223},
numpages = {11},
keywords = {projections, matrix multiplication, relational algebra, join queries},
}

@article{hummingbird2,
author = {Koutsoukos, Dimitrios and Nakandala, Supun and Karanasos, Konstantinos and Saur, Karla and Alonso, Gustavo and Interlandi, Matteo},
title = {{Tensors: An Abstraction for General Data Processing}},
year = {2021},
publisher = {VLDB Endowment},
volume = {14},
number = {10},
journal = {Proc. VLDB Endow.},
pages = {1797–1804},
}



@inproceedings{joinproject1,
author = {Amossen, Rasmus Resen and Pagh, Rasmus},
title = {{Faster Join-Projects and Sparse Matrix Multiplications}},
year = {2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
pages = {121–126},
numpages = {6},
keywords = {relational algebra, matrix multiplication, collapsing join-project},
location = {St. Petersburg, Russia},
booktitle = {ICDT 2009}
}

@inproceedings{joinproject3,
author = {Huang, Zichun and Chen, Shimin},
title = {Density-Optimized Intersection-Free Mapping and Matrix Multiplication for Join-Project Operations},
year = {2022},
publisher = {VLDB Endowment},
volume = {15},
number = {10},
journal = {VLDB Endowment},
pages = {2244–2256},
numpages = {13}
}

@inproceedings{liu2022feature,
  title={Feature augmentation with reinforcement learning},
  author={Liu, Jiabin and Chai, Chengliang and Luo, Yuyu and Lou, Yin and Feng, Jianhua and Tang, Nan},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)},
  pages={3360--3372},
  year={2022},
  organization={IEEE}
}

@inproceedings{Boehm2023SIGMOD,
author = {Boehm, Matthias and Interlandi, Matteo and Jermaine, Chris},
title = {Optimizing Tensor Computations: From Applications to Compilation and Runtime Techniques},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Machine learning (ML) training and scoring fundamentally relies on linear algebra programs and more general tensor computations. Most ML systems utilize distributed parameter servers and similar distribution strategies for mini-batch stochastic gradient descent training. However, many more tasks in the data science and engineering lifecycle can benefit from efficient tensor computations. Examples include primitives for data cleaning, data and model debugging, data augmentation, query processing, numerical simulations, as well as a wide variety of training and scoring algorithms. In this survey tutorial, we first make a case for the importance of optimizing more general tensor computations, and then provide an in-depth survey of existing applications, optimizing compilation techniques, and underlying runtime strategies. Interestingly, there are close connections to data-intensive applications, query rewriting and optimization, as well as query processing and physical design. Our goal for the tutorial is to structure existing work, create common terminology, and identify open research challenges.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {53–59},
numpages = {7},
keywords = {large-scale machine learning, linear algebra, declarative machine learning, data science, tensor computations, data engineering},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@article{huang2023joinboost,
  title={JoinBoost: Grow Trees over Normalized Data Using Only SQL},
  author={Huang, Zezhou and Sen, Rathijit and Liu, Jiaxiang and Wu, Eugene},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={11},
  pages={3071--3084},
  year={2023},
  publisher={VLDB Endowment}
}


@article{cheng2021secureboost,
  author       = {Kewei Cheng and
                  Tao Fan and
                  Yilun Jin and
                  Yang Liu and
                  Tianjian Chen and
                  Dimitrios Papadopoulos and
                  Qiang Yang},
  title        = {SecureBoost: {A} Lossless Federated Learning Framework},
  journal      = {{IEEE} Intell. Syst.},
  volume       = {36},
  number       = {6},
  pages        = {87--98},
  year         = {2021},
  timestamp    = {Wed, 16 Mar 2022 23:54:44 +0100},
  biburl       = {https://dblp.org/rec/journals/expert/ChengFJLCPY21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{yang2019federated,
  title={Federated Learning},
  author={Yang, Qiang and Liu, Yang and Cheng, Yong and Kang, Yan and Chen, Tianjian and Yu, Han},
  year={2019},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{10.1145/3514221.3526127,
  author    = {Fangcheng Fu and
               Huanran Xue and
               Yong Cheng and
               Yangyu Tao and
               Bin Cui},

  title     = {{BlindFL}: Vertical Federated Machine Learning without Peeking into
               Your Data},
  booktitle = {{SIGMOD} '22: International Conference on Management of Data, Philadelphia,
               PA, USA, June 12 - 17, 2022},
  pages     = {1316--1330},
  publisher = {{ACM}},
  year      = {2022},
}
@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}

@article{ghosh2020efficient,
  title={An efficient framework for clustered federated learning},
  author={Ghosh, Avishek and Chung, Jichan and Yin, Dong and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19586--19597},
  year={2020}
}
@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@inproceedings{li2019convergence,
  title={On the Convergence of FedAvg on Non-IID Data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{fontaine2007survey,
  title={A survey of homomorphic encryption for nonspecialists},
  author={Fontaine, Caroline and Galand, Fabien},
  journal={EURASIP Journal on Information Security},
  volume={2007},
  pages={1--10},
  year={2007},
  publisher={Springer}
}

@inproceedings{dwork2008differential,
  title={Differential privacy: A survey of results},
  author={Dwork, Cynthia},
  booktitle={International conference on theory and applications of models of computation},
  pages={1--19},
  year={2008},
  organization={Springer}
}

@InProceedings{pmlr-v162-lubana22a,
  title = 	 {Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering},
  author =       {Lubana, Ekdeep and Tang, Chi Ian and Kawsar, Fahim and Dick, Robert and Mathur, Akhil},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14461--14484},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lubana22a/lubana22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lubana22a.html}
}

@inbook{10.1145/3448016.3457241,
author = {Fu, Fangcheng and Shao, Yingxia and Yu, Lele and Jiang, Jiawei and Xue, Huanran and Tao, Yangyu and Cui, Bin},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2021 - VF2Boost Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning.pdf:pdf},
isbn = {9781450383431},
mendeley-groups = {aFederated learning/tabular},
pages = {563--576},
publisher = {Association for Computing Machinery},
title = {{VF2Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning}},
year = {2021}
}

@inbook{10.1145/3459637.3482361,
author = {Fang, Wenjing and Zhao, Derun and Tan, Jin and Chen, Chaochao and Yu, Chaofan and Wang, Li and Wang, Lei and Zhou, Jun and Zhang, Benyu},
booktitle = {Proceedings of the 30th ACM International Conference on Information & Knowledge Management},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang et al. - 2021 - Large-Scale Secure XGB for Vertical Federated Learning.pdf:pdf},
isbn = {9781450384469},
mendeley-groups = {aFederated learning/tabular},
pages = {443--452},
publisher = {Association for Computing Machinery},
title = {{Large-Scale Secure XGB for Vertical Federated Learning}},
year = {2021}
}

@article{brucke2023tpcx,
  title={TPCx-AI-An Industry Standard Benchmark for Artificial Intelligence and Machine Learning Systems},
  author={Br{\"u}cke, Christoph and H{\"a}rtling, Philipp and Palacios, Rodrigo D Escobar and Patel, Hamesh and Rabl, Tilmann},
  journal={Proceedings of the VLDB Endowment},
  volume={16},
  number={12},
  pages={3649--3661},
  year={2023},
  publisher={VLDB Endowment}
}

@article{arenas2013language,
author = {Arenas, Marcelo and P{\'{e}}rez, Jorge and Reutter, Juan and Riveros, Cristian},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arenas et al. - 2013 - The language of plain SO-tgds Composition, inversion and structural properties.pdf:pdf},
journal = {Journal of Computer and System Sciences},
mendeley-groups = {PhdDissertation/schema mapping/Plain SO tgds},
number = {6},
pages = {763--784},
publisher = {Elsevier},
title = {{The language of plain SO-tgds: Composition, inversion and structural properties}},
volume = {79},
year = {2013}
}

@misc{nvidia-gpu-performance:online,
  author       = {},
  title        = {GPU Performance Background User's Guide: NVIDIA Docs},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html}},
  month        = {2},
  year         = {2023},
  note         = {(Accessed on 02/20/2024)}
}


@article{fagin2005composing,
author = {Fagin, Ronald and Kolaitis, Phokion G and Popa, Lucian and Tan, Wang-Chiew},
file = {:C\:/Users/hairi/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fagin et al. - 2005 - Composing schema mappings Second-order dependencies to the rescue.pdf:pdf},
journal = {ACM Transactions on Database Systems (TODS)},
mendeley-groups = {PhdDissertation/schema mapping/SO tgds},
number = {4},
pages = {994--1055},
publisher = {ACM},
title = {{Composing schema mappings: Second-order dependencies to the rescue}},
volume = {30},
year = {2005}
}

@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}

@inproceedings{MorpheusFI,
author = {Li, Side and Chen, Lingjiao and Kumar, Arun},
title = {Enabling and Optimizing Non-linear Feature Interactions in Factorized Linear Algebra},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {1571–1588},
numpages = {18},
keywords = {linear algebra, feature interactions, factorized ml, data management for ml},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{tpcdi,
author = {Poess, Meikel and Rabl, Tilmann and Jacobsen, Hans-Arno and Caufield, Brian},
title = {TPC-DI: The First Industry Benchmark for Data Integration},
year = {2014},
issue_date = {August 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {13},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
month = {aug},
pages = {1367–1378},
numpages = {12}
}
@INPROCEEDINGS{tkde,
  author={Hai, Rihan and Koutras, Christos and Ionescu, Andra and Li, Ziyu and Sun, Wenbo and van Schijndel, Jessie and Kang, Yan and Katsifodimos, Asterios},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Amalur: Data Integration Meets Machine Learning}, 
  year={2023},
  volume={},
  number={},
  pages={3729-3739},
  keywords={Training;Data privacy;Federated learning;Computational modeling;Data integration;Training data;Manuals},
}
@article{gpu1,
author = {Li, Side and Kumar, Arun},
title = {Towards an optimized GROUP by abstraction for large-scale machine learning},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2327–2340},
numpages = {14}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={3146--3154},
  year={2017}
}

@article{wenthundersvm18,
 author = {Wen, Zeyi and Shi, Jiashuai and Li, Qinbin and He, Bingsheng and Chen, Jian},
 title = {{ThunderSVM}: A Fast {SVM} Library on {GPUs} and {CPUs}},
 journal = {Journal of Machine Learning Research},
 volume={19},
 pages={797--801},
 year = {2018}
}



@article{blas,
  title={An updated set of basic linear algebra subprograms (BLAS)},
  author={Blackford, L Susan and Petitet, Antoine and Pozo, Roldan and Remington, Karin and Whaley, R Clint and Demmel, James and Dongarra, Jack and Duff, Iain and Hammarling, Sven and Henry, Greg and others},
  journal={ACM Transactions on Mathematical Software},
  volume={28},
  number={2},
  pages={135--151},
  year={2002}
}
@BOOK{lapack99,
AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
Blackford, S. and Demmel, J. and Dongarra, J. and
Du Croz, J. and Greenbaum, A. and Hammarling, S. and
McKenney, A. and Sorensen, D.},
TITLE = {{LAPACK} Users' Guide},
EDITION = {Third},
PUBLISHER = {Society for Industrial and Applied Mathematics},
YEAR = {1999},
ADDRESS = {Philadelphia, PA},
ISBN = {0-89871-447-8 (paperback)} 
}
@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@inproceedings{2016-hamlet-sigmod,
  author    = {Arun Kumar and
               Jeffrey F. Naughton and
               Jignesh M. Patel and
               Xiaojin Zhu},
  editor    = {Fatma {\"{O}}zcan and
               Georgia Koutrika and
               Sam Madden},
  title     = {To Join or Not to Join?: Thinking Twice about Joins before Feature
               Selection},
  booktitle = {Proceedings of the 2016 International Conference on Management of
               Data, {SIGMOD} Conference 2016, San Francisco, CA, USA, June 26 -
               July 01, 2016},
  pages     = {19--34},
  publisher = {{ACM}},
  year      = {2016},
  timestamp = {Wed, 14 Nov 2018 10:56:20 +0100},
  biburl    = {https://dblp.org/rec/conf/sigmod/KumarNPZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{orion_learning_gen_lin_models,
  author    = {Kumar, Arun and Naughton, Jeffrey and Patel, Jignesh M.},
  title     = {Learning Generalized Linear Models Over Normalized Data},
  year      = {2015},
  isbn      = {9781450327589},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  abstract  = {Enterprise data analytics is a booming area in the data management industry. Many companies are racing to develop toolkits that closely integrate statistical and machine learning techniques with data management systems. Almost all such toolkits assume that the input to a learning algorithm is a single table. However, most relational datasets are not stored as single tables due to normalization. Thus, analysts often perform key-foreign key joins before learning on the join output. This strategy of learning after joins introduces redundancy avoided by normalization, which could lead to poorer end-to-end performance and maintenance overheads due to data duplication. In this work, we take a step towards enabling and optimizing learning over joins for a common class of machine learning techniques called generalized linear models that are solved using gradient descent algorithms in an RDBMS setting. We present alternative approaches to learn over a join that are easy to implement over existing RDBMSs. We introduce a new approach named factorized learning that pushes ML computations through joins and avoids redundancy in both I/O and computations. We study the tradeoff space for all our approaches both analytically and empirically. Our results show that factorized learning is often substantially faster than the alternatives, but is not always the fastest, necessitating a cost-based approach. We also discuss extensions of all our approaches to multi-table joins as well as to Hive.},
  booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages     = {1969–1984},
  numpages  = {16},
  keywords  = {analytics, joins, machine learning, feature engineering},
  location  = {Melbourne, Victoria, Australia},
  series    = {SIGMOD '15}
}

@inproceedings{galhotra2023metam,
  title={METAM: Goal-Oriented Data Discovery},
  author={Galhotra, Sainyam and Gong, Yue and Fernandez, Raul Castro},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)},
  pages={2780--2793},
  year={2023},
  organization={IEEE}
}

@inproceedings{cupy_learningsys2017,
  author       = "Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman",
  title        = "CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations",
  booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)",
  year         = "2017",
  url          = "http://learningsys.org/nips17/assets/papers/paper\_16.pdf"
}

@article{olteanu2020relational,
  title={The relational data borg is learning},
  author={Olteanu, Dan},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12},
  pages={3502--3515},
  year={2020},
  publisher={VLDB Endowment}
}

@inproceedings{kumar2017data,
  title={Data management in machine learning: Challenges, techniques, and systems},
  author={Kumar, Arun and Boehm, Matthias and Yang, Jun},
  booktitle={Proceedings of the 2017 ACM SIGMOD
International Conference on Management of Data (SIGMOD)},
  pages={1717--1722},
  year={2017}
}

@inproceedings{yan2001data,
  title={Data-driven understanding and refinement of schema mappings},
  author={Yan, Ling Ling and Miller, Ren{\'e}e J and Haas, Laura M and Fagin, Ronald},
  booktitle={Proceedings of the 2001 ACM SIGMOD international conference on Management of data},
  pages={485--496},
  year={2001}
}