
\section{Evaluation}
\label{sec:eva}
% In this section we first detail the datasets as well as the hardware used for our evaluation in Sec.~\ref{s:setup}. We first evaluate the speedup of factorized individual LA operators in Sec.~\ref{ex:ops}. We find out that the speedup of LA operator factorization can be quite substantial (up to 4x) when the parallelism is high. Then, in Sec~\ref{ex:spd_training} we evaluate the speedup of factorized ML models (i.e., mixed LA operators) over synthetic data and TPC-DI benchmark, and find  that factorization is not always faster than materialization, depending on the data characteristics (e.g., redundancy) and parallelism. Essentially, this experimentally motivates the need our cost estimator (Sec.~\ref{sec:cost}) that we evaluate in Sec.~\ref{ex:estimator}. There we find that our estimator can be quite effective in predicting what strategy to use (factorize vs. materialize) with very high accuracy (90.5\% for real datasets, and 97.1\% for synthetic ones).
\vspace{-2mm}
We start this section with an overview of our experimental setup, followed by a detailed performance evaluation of machine learning workloads employing both factorization and materialization techniques on CPUs and GPUs. This evaluation specifically examines the computing performance of factorized ML model training on these platforms and explores the factors that influence the decision-making process between factorization and materialization. Building on these insights, we further evaluate the accuracy of our proposed cost estimator in predicting the optimal strategy, utilizing both real and synthetic datasets. Our results demonstrate that the cost estimator can reduce the time consumption of real-world model training workloads by more than 20\%.

\vspace{-3mm}
\subsection{Experiment Setup}
\label{s:setup}

\vspace{-1mm}
\para{Synthetic Datasets}
\label{s:setup_synthetic} 
We built a data generator\footnote{\url{https://github.com/amademicnoboday12/Ilargi/tree/main/src/data_generator}} to create synthetic datasets with diverse data characteristics. It generates source table pairs that join to form a target table based on specified parameters. Using the ranges in Tab.~\ref{tab:synth_data_char}, we produced 1800 datasets to capture the relationship between data features and training performance. Additionally, the results from these datasets serve as training data for our cost estimator.

\begin{table}[t]
\caption{Parameters used for synthetic dataset generation.}
\label{tab:synth_data_char}
\vspace{1mm}
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
\hline
\textbf{Symbols}           & \textbf{Ranges}             & \textbf{Description}                                                                                                            \\ \hline
$r_T$, $r_{S_k}$   & 100k, 500k, 1M     & \begin{tabular}[c]{@{}l@{}}Number of rows in target table $T$ or source \\table $S_k$ \end{tabular}                                        \\ \hline
$c_T$, $c_{S_k}$   & {[}10, 50{]}       & \begin{tabular}[c]{@{}l@{}}Number of columns in target  table $T$ or source \\table $S_k$\end{tabular}                                    \\ \hline
% \begin{tabular}[c]{@{}l@{}}Complexity ratio \\(CR value)  \end{tabular} & {[}0.5, 9.5{]}     & \begin{tabular}[c]{@{}l@{}}Complexity ratio at model level, Equation~\ref{cr_value} in \\Sec.~\ref{s:complexity}  \end{tabular}                                 \\ \hline
$p$              & {[}0, 0.9{]}       & \begin{tabular}[c]{@{}l@{}}Sparsity of the target table\end{tabular}                             \\ \hline
$\rho_c(T)$      & {[}0.1, 1{]}       & \begin{tabular}[c]{@{}l@{}} The percentage of number of columns in source \\tables  w.r.t target table\end{tabular} \\ \hline
$j$              & \begin{tabular}[c]{@{}l@{}}Inner, left, or \\outer joins, union\end{tabular}     & Join types                                                                                                             \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

% \vspace{-1mm}


\para{Real-World Datasets}
To validate the estimator in real scenarios, we used the Hamlet datasets \cite{2016-hamlet-sigmod}, commonly referenced in related works \cite{MorpheusFI,chen2017towards,orion_learning_gen_lin_models}. The seven datasets in Hamlet simulate ML workflows, originally designed for inner join scenarios but adapted for other join types as detailed in Tab.~\ref{tab:6-hamlet-characteristics}.

% \begin{table}[t]
%   \centering
%     \caption[Hamlet dataset characteristics]{Hamlet dataset characteristics. $r$ and $c$ indicate the number of rows and columns, and $n$ is the number of tables. Subscripts denote which table characteristics belong to. }
%   \label{tab:6-hamlet-characteristics}
%   \vspace{1mm}
%   \footnotesize
%   \begin{tabular}{|l|l|l|l|l|l|l|l|}
%   \hline
%      & Book    & Expedia & Flight  & Lastfm  & Movie   & Walmart & Yelp    \\ \midrule \midrule
%     $r_T$                                            & $253$K  & $942$K  & $66.5$K & $344$K  & $1$M    & $422$K  & $216$K  \\
%     $c_T$                                            & $81.7$K & $52.3$K & $13.7$K & $55.3$K & $13.3$K & $2.44$K & $55.6$K \\
%     $r_{S_1}$                                        & $27.9$K & $942$K  & $66.5$K & $5$K    & $6.04$K & $422$K  & $11.5$K \\
%     $r_{S_2}$                                        & $50$K   & $11.9$K & $540$   & $50$K   & $3.71$K & $2.34$K & $43.9$K \\
%     $r_{S_3}$                                        &  -       & $37$K   & $3.17$K &    -     &    -     & $45$    &   -      \\
%     $r_{S_4}$                                        &    -    &   -      & $3.17$K &    -     &   -      &   -      &      -   \\
%     $c_{S_1}$                                        & $28$K   & $27$    & $20$    & $5.02$K & $9.51$K & $1$     & $11.7$K \\
%     $c_{S_2}$                                        & $53.6$K & $12$K   & $718$   & $50.2$K & $3.84$K & $2.39$K & $43.9$K \\
%     $c_{S_3}$                                        &    -     & $40.2$K & $6.46$K &   -      &     -    & $53$    &    -     \\
%     $c_{S_4}$                                        &   -      &   -      & $6.47$K &   -      &    -     &    -     &   -      \\
%     \bottomrule
%   \end{tabular}
% \vspace{-2mm}
% \end{table}

\begin{table}[t]
  \centering
    \caption[Hamlet dataset characteristics]{Hamlet dataset characteristics. $r$ and $c$ indicate the number of rows and columns. Subscripts denote which table characteristics belong to. }
  \label{tab:6-hamlet-characteristics}
  \vspace{1mm}
  \footnotesize
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
  \hline
     & $r_T$ & $c_T$ & $r_{S_1}$ & $r_{S_2}$ & $r_{S_3}$ & $r_{S_4}$ & $c_{S_1}$ & $c_{S_2}$ & $c_{S_3}$ & $c_{S_4}$ \\ \midrule \midrule
    Expedia & $942$K  & $52.3$K & $942$K  & $11.9$K & $37$K   & -    & $27$    & $12$K   & $40.2$K & -      \\
    Flight  & $66.5$K & $13.7$K & $66.5$K & $540$   & $3.17$K & $3.17$K & $20$    & $718$   & $6.46$K & $6.47$K \\
    Lastfm  & $344$K  & $55.3$K & $5$K    & $50$K   & -       & -    & $5.02$K & $50.2$K & -      & -       \\
    Movie   & $1$M    & $13.3$K & $6.04$K & $3.71$K & -       & -    & $9.51$K & $3.84$K & -      & -       \\
    Yelp    & $216$K  & $55.6$K & $11.5$K & $43.9$K & -       & -    & $11.7$K & $43.9$K & -      & -       \\
    \bottomrule
  \end{tabular}
\vspace{-2mm}
\end{table}

\para{TPC-DI Benchmark} 
For large-scale evaluation, we used the TPC-DI benchmark~\cite{tpcdi}, involving four datasets across three sources. The fact table, \emph{Trade}, is joined with key attributes to produce a target table with 27 features. Tab.~\ref{tab:tpc_data} outlines table cardinalities across various scale factors, highlighting data characteristics in different scenarios. 

\begin{table}[t]
\caption{Data sizes of realistic data integrations scenario based on TPC-DI benchmark. The table shows the number of rows in sources and  target table w.r.t varying scale factors.}
\label{tab:tpc_data}
\vspace{1mm}
\centering
\footnotesize
\begin{tabular}{|c|l|l|ll|l|}
\hline
              & \textbf{Source 1} & \textbf{Source 2}  & \multicolumn{2}{c|}{\textbf{Source 3}}         &    \textbf{Target}      \\ \hline
\textbf{Scale factor} & Trade   & Customer & \multicolumn{1}{l|}{Stock} & Reports &  -   \\ \hline
3             & 391.1K & 4.7K    & \multicolumn{1}{l|}{2.6K} & 98.7K  & 528.8K   \\ \hline
7             & 911.6K & 108K   & \multicolumn{1}{l|}{5.8K} & 297.7K & 1.3M \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}



% \para{Real datasets}
% \label{s:setup_real} 
% We also use the seven real-world datasets from Morpheus and its extensions \cite{kumar2016join, \hamletplusplus, chen2017towards, \morpheusfi}, as provided by Project Hamlet\footnote{Project Hamlet datasets: \url{https://adalabucsd.github.io/hamlet.html}} \cite{kumar2016join, \hamletplusplus}. Each dataset consists of two or more tables connected in a star schema with PK-FK relationships. $nnz$ indicates the number of nonzero elements. We include the characteristics of these datasets in the technical report \cite{tech}.

\para{Hardware and software} We run experiments with 16, and 32 cores of AMD EPYC 7H12 CPU, and Nvidia A40 GPU. To enable parallelized sparse matrix multiplication on both CPU and GPU, we choose MKL and CuBLAS as LA library respectively. The number of model training iterations is 200. 

\vspace{-2mm}
\subsection{Performance Evaluation with Synthetic Data}
\label{ssec:simpleDI}
\vspace{-2mm}
In this section, we conduct a performance comparison between factorized learning and learning over materialization on both CPUs and GPUs. The purpose of this comparison is to demonstrate the significant acceleration that GPUs can provide for factorized learning, as well as the performance of factorized learning across various data characteristics.

% \vspace{-1mm}
% \subsubsection{Performance on CPUs and GPUs}
% We begin by comparing the performance of factorized ML operators and training algorithms on 32-core CPU and GPU. Fig.~\ref{fig:eval_cpu_gpu} illustrates the speedups achieved by factorized linear operators and ML model training algorithms on GPU relative to their CPU counterparts. The factorized operators on GPU can achieve speedups of up to 68x. Specifically, scalar multiplication benefits most significantly from the high parallelism of GPU, achieving the greatest speedup, while matrix multiplications generally achieve more moderate speedups. This is due to the internal data dependencies during computation and the intensive memory access, which introduce considerable overheads. A notable observation is that right matrix multiplication (\(XT^T\)) is particularly impacted by these overheads due to inefficient column-wise memory access in a row-major matrix configuration.
% % We show the results first at the LA operator level, then the overall performance at the model level. 

% The observed speedups for models range from 2.5x to 8.9x, which are less significant compared to those of individual operators. Although linear operators serve as the foundational components of model training algorithms, the time consumed by the data movement of intermediate results cannot be overlooked. This issue is particularly significant in model training, which typically involves multiple epochs; thus, the overhead associated with data movement becomes more influential. Consequently, this overhead is reflected in the reduced speedups relative to the higher acceleration observed in individual linear operators.

% \mybox{\textbf{Takeaways:} 
% Factorized model training on GPUs can achieve significant speedups compared to training on CPUs. 
% }

\vspace{-3mm}
\subsubsection{Performance of factorization and materialization}
Fig.~\ref{fig:eval_f_m} shows the speedups of factorization over materialization (\(\frac{T_{materialization}}{T_{factorization}}\)) across three hardware setups. Factorized scalar operators achieve greater speedups on GPUs compared to CPUs, while matrix operators show smaller gains. On CPUs, matrix operators outperform scalar operators, but this trend reverses on GPUs. This discrepancy arises because GPU-based matrix operations are often limited by memory bandwidth, as discussed in Section~\ref{sec:3rdfac}. The high parallelism of GPUs generally benefits compute-bound tasks more than memory-bound operators.

For model training, factorization still offers speed improvements, albeit more modestly. This is due to the frequent read/write operations for intermediate results in training algorithms, which diminishes speedups, especially on GPUs where memory bandwidth constraints often dominate.

\vspace{-3mm}
\subsubsection{Speedups regarding sparsity and complexity ratio}
Fig.~\ref{fig:heatmap_cpu_gpu} illustrates how speedups vary with varying sparsity and complexity ratio on CPUs and GPUs. The speedups presented in the figure represent the average benefit when factorization proves more advantageous. From the CPU results, an increase in speedups is observed as the complexity ratio grows. Within certain intervals of complexity ratio, speedups increase as sparsity decreases. However, sparsity alone is not a reliable estimator across all complexity ratios.

The results on GPUs, in contrast, show no observable trend, suggesting that empirical threshold-based estimators may not function effectively on GPUs. This observation underscores the necessity for a learned estimator capable of integrating features of both hardware configurations and data characteristics.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/new_eval_figures/spdup_GPU_CPU.pdf}
%      \vspace{-4mm}
%     \caption{
%     Average speedups (\(\frac{\text{Time}_{CPU}}{\text{Time}_{GPU}} \)) of LA operators and model training w.r.t varying input parameters. All operations can be generally accelerated by GPUs due to high parallelism.
%     }
%     \label{fig:eval_cpu_gpu}
%     \vspace{-5mm}
% \end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/new_eval_figures/spdup_f_m.pdf}
        \vspace{-1.5mm}
    \caption{
    Speedups (\(\frac{\text{Time}_{materialization}}{\text{Time}_{factorization}} \)) of LA operators and model training w.r.t varying input parameters. \emph{Here we focus on the cases that factorization performs faster than materialization.}
    }
    \label{fig:eval_f_m}
    \vspace{-4mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/new_eval_figures/heatmap_cpu_gpu.pdf}
    \caption{
        Speedups (\(\frac{\text{Time}_{materialization}}{\text{Time}_{factorization}} \)) w.r.t target table sparsity and complexity ratio on CPUs and GPUs. On CPUs, speedups increase when complexity ratio gets larger. No observable trend on GPUs. 
    }
    \label{fig:heatmap_cpu_gpu}
    \vspace{-4mm}
\end{figure}



\mybox{\textbf{Takeaways:} 
The significant variability in performance comparison between factorization and materialization, highlights \textbf{the need for a cost estimator}. 
% to identify the most efficient approach for model training when both factorization and materialization are feasible options. 
}



\vspace{-1mm}
\subsection{Effectiveness of Our Estimator}
\label{ex:estimator}
\vspace{-2mm}
This section evaluates the tree boosting cost estimator as outlined in Section \ref{s:estimator}. Our analysis focuses not only on the predictive accuracy of the estimator but also on its performance enhancement in batch model training scenarios. 

% \subsubsection{Configuration}\hfill
% \label{ex:est_config}

\para{Training and test data}
% In Sec.\ref{ssec: realDI}, we evaluated the performance of four models using both factorization and materialization techniques in realistic data integration scenarios. In this section, \rihan{delete before} 
We extract features described in Sec.~\ref{s:estimator} from the evaluation conducted on synthetic data. This process generated 7200 pairs (1800*4), each labeled to indicate whether factorization is faster than materialization (True) or not (False). The dataset, containing these features and labels, is then randomly divided into training (80\%) and test (20\%) sets.


\para{Evaluation metrics} 
We evaluate our estimator model using accuracy and F1-score, and compare its performance against other baseline estimators. Accuracy serves to gauge the effectiveness of our cost estimator in accurately recommending the faster training method—either factorization or materialization. Furthermore, we measure the end-to-end speedup of batch model training tasks when decisions between factorization and materialization are guided by our cost estimator. 

\para{Four baselines for comparison} 
% We compared the performance of our proposed estimator in Sec. \ref{s:estimator} with the following five baselines. 
% \\ 
\texttt{i)} To confirm the significance of hardware features, 
% apart from the tree boosting model in Sec. \ref{s:estimator}, 
we train another tree boosting model using the same dataset, but \emph{without hardware features}. 
\texttt{ii)} The \emph{tuple ratio and feature ratio} (TR \& FR), metrics used in Morpheus \cite{chen2017towards}, quantify the target table's redundancy relative to the joinable tables. Morpheus \cite{chen2017towards} suggests a threshold of 5 for tuple ratio and 1 for feature ratio.
 \texttt{iii)} Finally, we consider the heuristic rule proposed in \cite{MorpheusFI}.
\begin{table}[t]
\caption{Accuracy, F1-score and Overall Speedups of estimators tested on CPUs and GPUs. Our cost estimator shows superior predictive quality and effectiveness over synthetic test data.}
\label{tab:overall_accuracy}
% \vspace{1mm}
\small
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Cost estimators}                        & \textbf{Accuracy}       & \textbf{F1-score}  & \textbf{Overall Speedups} \\  \hline
\multicolumn{4}{|c|}{CPU results} \\ \midrule
Tree Boosting (ours)          & \textbf{0.971} & \textbf{0.936} & \textbf{1.24} \\ \hline
    Tree Boosting w/o hardware        &0.712 & 0.560 & 1.19\\ \hline
TR \& FR \cite{chen2017towards}            & 0.490          & 0.426 & 1.16\\ \hline
MorpheousFI             & 0.864          & 0.601        & 1.22  \\ \hline
\hline
\multicolumn{4}{|c|}{GPU results} \\ \midrule
Tree Boosting (ours)          & \textbf{0.899} & \textbf{0.821} & \textbf{1.15} \\ \hline
    Tree Boosting w/o hardware        &0.523 & 0.443 & 1.09\\ \hline
TR \& FR \cite{chen2017towards}            & 0.244          & 0.362 & 0.86\\ \hline
MorpheousFI             & 0.497          & 0.477        & 0.96  \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

\vspace{-2mm}
\subsubsection{Results on synthetic data} 
Tab.~\ref{tab:overall_accuracy} shows that our tree boosting model outperforms others with a 97.1\% accuracy and high F1-score, confirming its strong precision-recall balance. Excluding hardware features significantly reduces both accuracy and F1-score, supporting the importance of hardware information in choosing between factorization and materialization. The two threshold-based estimators perform worse in both metrics.

Beyond quality metrics, our estimator achieves a 1.24x speedup over training exclusively with materialization, outperforming all baselines. On GPUs, however, the effectiveness of all estimators declines, with threshold-based models performing worse than default materialization. This highlights the sensitivity of empirical thresholds to hardware variations, as discussed in Sec.~\ref{sec:3rdfac}, and demonstrates the limitations of threshold-based methods in cross-platform tasks.



% \para{Robustness test}
% Fig.~\ref{fig:confusion} presents a confusion matrix from our tree boosting estimator, indicating over 20\% of test instances can benefit from factorized learning. On average, correctly predicted instances achieve a 1.42x speedup with factorization. Interestingly, even with mispredictions leading to incorrect factorization application, training still retains about 85\% of the expected performance, showcasing our estimator's robustness. A high positive correlation is observed between accurate predictions and speedups, especially in True-Negative instances.


\vspace{-5mm}
\subsubsection{Results on real-world data}
We further evaluated our estimator using the \textit{Project Hamlet} and \textit{TPC-DI} datasets to test its practical usability in realistic ML tasks, including multiple model training and hyperparameter tuning.

Tab.~\ref{tab:overall_estimator} shows that the speedups on both CPU and GPU platforms are consistent with those observed on synthetic data, with our tree boosting estimator consistently performing best on unseen data.
\begin{table}[t!]
\caption{Average speedups (\(\frac{\text{Time}_{materialization}}{\text{Time}_{factorization}} \)) of all models across all real-world data. Our tree boosting estimator can consistently accelerate batch model training workload on both CPUs and GPUs.}
\label{tab:overall_estimator}
\centering
\small
\begin{tabular}{|l|l|l|}
\hline
\textbf{Cost estimators} & \textbf{Speedups (CPU)} & \textbf{Speedups (GPU)} \\ \hline
Tree boosting (ours) & \textbf{1.21} & \textbf{1.16} \\ \hline
MorpheousFI & 1.18 & 0.88 \\ \hline
TR\&FR & 1.09 & 072 \\ \hline
\end{tabular}%
\vspace{-4mm}
\end{table}

Tab.~\ref{tab:effect-cpu} provides a breakdown of effectiveness across models and datasets on CPUs. While the estimator generally makes accurate decisions, its performance varies with different datasets. For denser datasets like \textit{flight} and \textit{lastfm}, incorrect decisions led to slower training compared to materialization. Still, when training a single model across various datasets, speedups reached up to 1.29x.

On GPUs (Tab.~\ref{tab:effect-gpu}), the results mirror the CPU findings but show slightly more mis-predictions. Speedups are generally lower on GPUs, consistent with synthetic data results. Predicting accurately for dense datasets like \textit{flight} and \textit{lastfm} remains challenging, as this high density is atypical for most star schema datasets.


\mybox{\textbf{Takeaways:}  Our tree boosting cost estimator, enhanced with hardware features, outperforms the state-of-the-art estimator, showing consistent effectiveness across diverse hardware platforms.}


% \cellcolor{white}  -> gray
\begin{table}[t!]
\caption{Detailed speedups(\(\frac{\text{Time}_{materialization}}{\text{Time}_{factorization}} \)) w.r.t different models and datasets on CPUs (32 cores). Our tree boosting estimator makes correct prediction in most cases but performs worse on \textit{flight} and \textit{lastfm}.}
\label{tab:effect-cpu}
\vspace{1mm}
\small
\centering
\begin{tabular}{p{25mm}|cccc|c}
% \hline
\tikz[diag text/.style={inner sep=0pt, font=\footnotesize},
      shorten/.style={shorten <=#1,shorten >=#1}]{%
        \node[below left, diag text] (def) {Datasets};
        \node[above right=2pt, diag text] (abc) {Models};
        \draw[shorten=4pt, very thin] (def.north west|-abc.north west) -- (def.south east-|abc.south east);}
& \cellcolor{white} G.NMF & \cellcolor{white} KMeans & \cellcolor{white} Lin.Reg & \cellcolor{white} Log.Reg & \cellcolor[HTML]{EFEFEF} Speedups/dataset \\ \hline
\rowcolor{green!5} 
\cellcolor{white} TPC-DI sf=3                                                   & 1.33                             & \cellcolor{red!10}1      & 1.12                            & 1.33                            & 1.10                                                                                    \\ \hline
\rowcolor{green!5} 
\cellcolor{white} TPC-DI sf=7                                                   & 1.23                             & \cellcolor{red!10}1      & 1.04                            & 1.20                            & 1.09                                                                                    \\ \hline
\rowcolor{green!5} 
\cellcolor{white} expedia                                                & 1.24                             & 1.56                           & \cellcolor{red!10}1       & 1.07                            & 1.17                                                                                    \\ \hline
\rowcolor{red!10}  
\cellcolor{white} flight                                                 & 0.57                             & 0.7                            & \cellcolor{green!5}1       & 0.49                            & 0.74                                                                                    \\ \hline
\rowcolor{green!5} 
\cellcolor{white} lastfm                                                 & 1                                & \cellcolor{red!10}0.9    & 1                               & 1                               & \cellcolor{red!10}0.96                                                            \\ \hline
\rowcolor{green!5} 
\cellcolor{white} movie                                                  & 1.8                              & 2.35                           & 2.32                            & 1.88                            & 2.19                                                                                    \\ \hline
\rowcolor{green!5} 
\cellcolor{white} yelp                                                   & 1.10                             & 1.24                           & 1.51                            & \cellcolor{red!10}1       & 1.28                                                                                    \\ \hline\hline
\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}l@{}}Speedups/model \end{tabular} & \cellcolor{green!5}1.28     & \cellcolor{green!5}1.21   & \cellcolor{green!5}1.29    & \cellcolor{green!5}1.18    & \#                                                                                        \\ %\hline
\end{tabular}
\vspace{-3mm}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% red -> \cellcolor{red!10} 
\begin{table}[t]
\caption{Detailed speedups(\(\frac{\text{Time}_{materialization}}{\text{Time}_{factorization}} \)) w.r.t different models and datasets on GPU. Our tree boosting estimator makes more wrong predictions then that on CPUs but still achieves overall speedups in most cases.}
\label{tab:effect-gpu}
\vspace{1mm}
\small
\centering
\begin{tabular}{p{25mm}|cccc|c}
% \hline
\tikz[diag text/.style={inner sep=0pt, font=\footnotesize},
      shorten/.style={shorten <=#1,shorten >=#1}]{%
        \node[below left, diag text] (def) {Datasets};
        \node[above right=2pt, diag text] (abc) {Models};
        \draw[shorten=4pt, very thin] (def.north west|-abc.north west) -- (def.south east-|abc.south east);}
                                                                                      & \cellcolor{white} G.NMF & \cellcolor{white} KMeans & \cellcolor{white} Lin.Reg & \cellcolor{white} Log.Reg & \cellcolor[HTML]{EFEFEF}Speedups/dataset \\ \hline
\rowcolor{green!5}  
\cellcolor{white} TPC-DI sf=3                                                   & 1.21                             & \cellcolor{red!10} 1      & 1.09                            & 1.23                            & 1.06                                                                                    \\ \hline
\rowcolor{green!5}  
\cellcolor{white} TPC-DI sf=7                                                   & 1.17                             & \cellcolor{red!10} 1      & 1                               & 1.13                            & 1.03                                                                                    \\ \hline
\rowcolor{green!5}  
\cellcolor{white} book                                                   & 1                                & 1                              & 1                               & 1                               & 1                                                                                       \\ \hline
\rowcolor{green!5}  
\cellcolor{white} expedia                                                & 1.24                             & 1.07                           & \cellcolor{red!10} 1       & \cellcolor{red!10} 1       & 1.05                                                                                    \\ \hline
\rowcolor{red!10} 
\cellcolor{white} flight                                                 & 0.52                             & 0.67                           & \cellcolor{green!5} 1       & 0.52                            & 0.71                                                                                    \\ \hline
\rowcolor{red!10} 
\cellcolor{white} lastfm                                                 & 0.94                             & 0.89                           & \cellcolor{green!5} 1       & \cellcolor{green!5} 1       & 0.91                                                                                    \\ \hline
\rowcolor{green!5}  
\cellcolor{white} movie                                                  & 1.69                             & 1.77                           & 2.98                            & 1.78                            & 1.97                                                                                    \\ \hline

\rowcolor{green!5}  
\cellcolor{white} yelp                                                   & 1.06                             & 1.19                           & 1.33                            & 1.28                            & 1.23                                                                                    \\ \hline\hline
\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}l@{}}Speedups/model\end{tabular} & \cellcolor{green!5} 1.18     & \cellcolor{green!5} 1.09   & \cellcolor{green!5} 1.21    & \cellcolor{green!5} 1.08     & \#                                                                                       \\ %\hline
\end{tabular}
\vspace{-2mm}
\end{table}





% \subsection{Feature Importance Verification}
% \label{ex:feature_importance}
% Training the boosting tree model enables extraction of node information gain for ranking feature importance, as shown Table~\ref{tab:info_gain}. We have 33 features in total. Due to space restrictions, we list top 10 features here and leave the comprehensive details documented in the technical report \cite{tech}.  
% Notably, complexity features of LMM, a common operator, exhibit high information gain. Operators like rowSum and colSum, which are difficult to parallelize, are crucial for performance determination. Besides complexity features, memory I/O features, especially tied to dense operators, notably affect performance, as computations often yield dense intermediate results, increasing I/O costs. The tuple ratio ranks lower, implying a lesser impact on cost estimation.
% \begin{table}[t]
% \caption{Top 10 information gain of features in the estimator.}
% \label{tab:info_gain}
% \small
% \begin{tabular}{|l|l|r|}
% \hline
% \textbf{Rank} & {\textbf{Top 10 features}}                  & {\textbf{Info. Gain}} \\\bottomrule 
% 1 & {rowSum mem. write}              & {30.96}                  \\ \hline
% 2 & {LMM complexity with factorization}                & {22.69}                  \\ \hline
% 3 & {Dense scalar operation complexity}                & {20.53}                                \\ \hline
% 4  & {LMM complexity with materialization}          & {8.68}                  \\ \hline
% 5 & {colSum complexity with materialization}               & {6.30}                  \\ \hline

% 6 & {Complexity ratio}            & {6.21}                   \\ \hline
% 7 & {Total mem. write with materialization} & {5.88}                   \\ \hline
% 8 & {Dense MM operation complexity}  & {5.63}                   \\ \hline
% 9 & {Dense MM mem. write}        & {5.77}                   \\ \hline

% 10 & {Feature Ratio}   & { 5.45}                   \\ \hline
% \end{tabular}
% \end{table}




% Our feature importance analysis offers valuable insights for designing a performance cost estimator for trade-offs between materialization and factorization, highlighting the importance of considering hardware features.


% \mybox{\textbf{Takeaways:} 
% \\Hardware features are crucial to build a cross-platform cost estimator for factorized learning.}
% The process of training the boosting tree model allowed us to extract the information gains of each node, which we used as indicators of feature importance. Table \ref{tab:info_gain} presents the top-10 features, ranked in descending order by their information gains.

% This occurs, for instance, when performing operations like colSum and rowSum, which produce dense vector results. The impact of these operations is often overlooked, despite their substantial I/O costs.

% An interesting observation from this analysis is the significant importance of features related to memory I/O operations, particularly for dense operators. Although the raw data used in our experiments are stored in a sparse format, the intermediate results generated during computations are often converted to a dense format. This occurs, for instance, when performing operations like colsum and rowsum. These dense, I/O-intensive operators can substantially influence the overall performance due to their high I/O costs. This effect is typically overlooked despite the competitive I/O cost associated with sparse operators.



% Furthermore, it is worth highlighting the importance of the complexity ratio. While not the most significant feature, it plays a crucial role in the estimator alongside memory I/O features. Importantly, both the materialization complexity and the factorization complexity are inherently associated with the complexity ratio. In contrast, the Tuple Ratio, which was proposed in [], does not rank high in our model, indicating its lesser impact on the overall performance estimation.

% The insights gathered from our feature importance analysis highlight key considerations for designing a performance estimator for factorized learning. They particularly underscore the value of accounting for hardware features. Given the computational demands of factorized learning and the influence of specific hardware features on its efficiency, their incorporation in the performance estimation process can provide a more comprehensive, and hence, a more accurate prediction of performance outcomes.
% \vspace{-3mm}
% \subsection{Generalizability Verification with Ablation Study} 
% To evaluate the generalizability of our cost estimator, we utilized a leave-one-out cross-validation method, specific to different ML models to be trained. 

% Fig. \ref{fig:xgb_acc_category} indicates a decrease in both accuracy and F1-score when certain models are excluded from training data. This is due to missing operator features in the training set. For example, excluding the KMeans model, which heavily uses the colSum and rowSum, creates a predictive bias due to an unbalanced feature distribution.
% Despite these constraints, the estimator remains functional with an approximate 80\% accuracy rate. This suggests that even with certain predictive biases, our estimator is resilient and generalizable in optimizing ML workloads with new ML models, offering reasonably accurate predictions across different scenarios.


% \vspace{-2mm}

