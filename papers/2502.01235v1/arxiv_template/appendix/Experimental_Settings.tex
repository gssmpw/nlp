\section{Experimental Settings and Additional Results}
\label{exp-settings}

In \cref{exp:toy-setting}, we firstly provide the experimental details of small-scale experiments in our main text, e.g., \cref{figs:GA-vs-Ours} and \cref{fig:small-init}. 
Experimental settings of NLP tasks in the main text are given by \cref{app:expNLP}.
We also include the fine-tuning experiments on LLMs in \cref{sec:exp:llm}.
More ablation study is given by \cref{appx:ablation}. Finally, we visualize the singular values of both the pre-trained weights and the difference weights after fine-tuning in \cref{SV-figs}. All small-scale experiments were performed on AMD EPYC 7B12 CPU. All experiments for T5 base model and Llama 2-7B were performed on Nvidia A100 GPU (40GB).

\subsection{Small-Scale Experiments}
\label{exp:toy-setting}

Here we give the experimental details of \cref{figs:GA-vs-Ours} and \cref{fig:small-init}. Besides, we plot the GD trajectories under \eqref{eq:spectral-init-linear} and \eqref{eq:lorainit} for comparison.\\

\noindent
{\bf Details for \cref{figs:GA-vs-Ours}:} For the exact-ranked setting, we take $d=k=100$, $N=1600$, and $r=r^*=4$. We sample each element of $\bm W^\natural$ independently from $\mathcal{N}(0\,,1)$. We construct $\Delta:=\bm U \bm V^{\!\top}$ where $\bm U\in\mathbb{R}^{100\times 4}$ and $\bm V\in\mathbb{R}^{100\times 4}$ are obtained from the SVD of a matrix whose elements are independently sampled from $\mathcal{N}(0\,,1)$. For LoRA-One (-) and LoRA-GA (-), we use learning rate $\eta=\frac{1}{35}$ and stable parameter $s=2$. For \eqref{eq:spectral-init-linear} (-), we use learning rate $\eta=\frac{1}{10}$ and $\gamma=1$. 

For the ill-conditioned setting, we take $d=k=100$, $N=1600$, $r^*=4$, and $r=8$. We construct $\Delta:=\bm U \bm S^* \bm V^{\!\top}$ where $\bm U\in\mathbb{R}^{100\times 4}$ and $\bm V\in\mathbb{R}^{100\times 4}$ are obtained from the SVD of a matrix whose elements are independently sampled from $\mathcal{N}(0\,,1)$, and $\bm S^*=\operatorname{Diag}\left(1\,,0.75\,,0.5\,,0.25\right)$. For algorithms without preconditioners, we set the learning rate to be $\eta=\frac{1}{20}$. For algorithms with preconditioners, we set the learning rate to be $\eta=\frac{1}{2}$. For LoRA-One, LoRA-One (-), LoRA-GA (-), and LoRA-GA (+), we set the stable parameter $s=2$. For \eqref{eq:spectral-init-linear} (-) and \eqref{eq:spectral-init-linear} (+), we take $\gamma=1$. All damping parameters $\lambda$ for preconditioners are set to be 0.001.\\

\noindent
{\bf Details for \cref{fig:small-init}:} We examine for dimension $d=k=100$ and $d=k=1000$. We set $N=16d$, $r^*=4$, and $r=8$. We construct $\Delta:=\bm U \bm V^{\!\top}$ where $\bm U\in\mathbb{R}^{100\times 4}$ and $\bm V\in\mathbb{R}^{100\times 4}$ are obtained from the SVD of a matrix whose elements are independently sampled from $\mathcal{N}(0\,,1)$. We initialize $\bm A_0$ and $\bm B_0$ via \eqref{eq:lorainit} over variance $\alpha^2\in\{1\,,0.1\,,0.01\,,0.001\,,0.0001\}$. We set learning rate $\eta=\frac{1}{64}$. We run $1500$ GD steps for each case.\\

\noindent
{\bf Comparison on GD trajectories of \cref{fig:phase-transi}:}
Here we conduct a toy experiment to intuitively compare the GD trajectories under \eqref{eq:spectral-init-linear} and \eqref{eq:lorainit}. We fine-tune a simple pre-trained model $y=\bm x^{\!\top}\bm w^\natural$ on downstream data generated by $\widetilde{y}=\widetilde{\bm x}^{\!\top}(\bm w^\natural+\bm w)$, where $\bm x^{\!\top}\,,\widetilde{\bm x}\,,\bm w^\natural\,,\bm w\in\mathbb{R}^2$ and $y\,,\widetilde{y}\in\mathbb{R}$. We propose to use LoRA to fine-tune this model by $\widehat{y} = \widetilde{\bm x}^{\!\top}(\bm w^\natural+b \bm a)$ where $\bm a = [a_1\,a_2]^{\!\top}\in\mathbb{R}^2$ and $b\in\mathbb{R}$. Without loss of generality, we set $\bm w^\natural=\bm 0$ and $\bm w = [2\,\,1]^{\!\top}$. The set of global minimizers to this problem is $\{a_1^*=2/t\,,a_2^*=1/t\,,b^*=t\mid t \in \mathbb{R}\}$. We generate 4 data points $(\widetilde{\bm x}_1\,,\widetilde{\bm x}_2\,,\widetilde{\bm x}_3\,,\widetilde{\bm x}_4)$ whose elements are independently sampled from $\mathcal{N}(0\,,1)$ and calculate for $(\widetilde{y}_1\,,\widetilde{y}_2\,,\widetilde{y}_3\,,\widetilde{y}_4)$. We use the squared loss $\frac{1}{8}\sum_{i=1}^4 (\widetilde{y}_i-b\widetilde{\bm x}^{\!\top} \bm a)^2$. For \eqref{eq:lorainit}, we initialize each element of $\bm a_0$ from $\mathcal{N}(0\,,1)$ and $b_0=0$. Notice that the variance $1$ follows from the Kaiming initialization \citep{he2015delving}. For \eqref{eq:spectral-init-linear}, we first calculate the one-step full gradient, i.e. $\bm g^\natural := \frac{1}{4}\sum_{i=1}^4 \widetilde{y}_i^2 \widetilde{\bm x}_i$.
Accordingly, we initialize $\bm a_0 = \frac{\bm g^\natural}{\sqrt{\|\bm g^\natural\|_2}\,.}$ and $b_0 = \sqrt{\|\bm g^\natural\|_2}$. Next, we run GD to train $\bm a$ and $b$ for $1000$ steps with learning rate $\eta=0.1$. For each initialization strategy and data generation, we run for 3 different seeds. The starting points and stopping points with corresponding loss values are presented in \cref{tab:phase-spec} for \eqref{eq:spectral-init-linear} and \cref{tab:phase-random} for \eqref{eq:lorainit}.
Our experiments in \cref{fig:phase-transi} show that spectral initialization enables faster convergence to the global minimizer compared to LoRA initialization.
\begin{table}[h]
    \caption{The details of starting points with initial loss and stopping points with final loss under \eqref{eq:spectral-init-linear} over 3 runs.}
    \label{tab:phase-spec}
    \centering
    \begin{tabular}{ccccc}
    \toprule
         & Starting Point & Initial Loss & Stopping Point & Final Loss \\
         \midrule
       Run 1  & $\bm a=[0.26\,,0.55]^{\!\top}\,, b=0.61$ & $0.39$ & $\bm a=[1.34\,,0.67]^{\!\top}\,, b=1.49$ & \SI{5e-13}{} \\
       \midrule
       Run 2  & $\bm a=[1.10\,,-0.27]^{\!\top}\,, b=1.10$ & $0.38$ & $\bm a=[1.35\,,0.68]^{\!\top}\,, b=1.48$ & \SI{1e-13}{} \\
       \midrule
       Run 3 & $\bm a=[0.96\,,0.35]^{\!\top}\,, b=1.02$ & $0.34$ & $\bm a=[1.34\,,0.67]^{\!\top}\,, b=1.49$ & \SI{4e-13}{} \\
       \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \caption{The details of starting points with initial loss and stopping points with final loss under \eqref{eq:lorainit} over 3 runs.}
    \label{tab:phase-random}
    \centering
    \begin{tabular}{ccccc}
    \toprule
         & Starting Point & Initial Loss & Stopping Point & Final Loss \\
         \midrule
       Run 1  & $\bm a=[-0.35\,,2.63]^{\!\top}\,, b=-0.03$ & $0.43$ & $\bm a=[-2.49\,,-1.24]^{\!\top}\,, b=-0.80$ & \SI{1e-10}{} \\
       \midrule
       Run 2  & $\bm a=[0.14\,,-1.68]^{\!\top}\,, b=0.10$ & $0.82$ & $\bm a=[1.81\,,0.91]^{\!\top}\,, b=1.10$ & \SI{1e-13}{} \\
       \midrule
       Run 3 & $\bm a=[-1.44\,,0.98]^{\!\top}\,, b=0.03$ & $0.97$ & $\bm a=[1.84\,,0.92]^{\!\top}\,, b=1.08$ & \SI{6e-13}{} \\
       \bottomrule
    \end{tabular}
\end{table}

\subsection{Natural Language Understanding}
\label{app:expNLP}

In the main text of \cref{sec:algoexp}, we have presented the experimental comparisons between \cref{alg:lora_one_training} and typical LoRA based algorithms. 
For experimental details, we follow the configuration of prompt tuning as \cite{wang2024lora}. The general hyperparameter settings are provides in \cref{tab:nlu-general}. Also, we employ the scaling parameter $\frac{\sqrt{d_\text{out}}}{s}$ for LoRA-One (\cref{alg:lora_one_training}) derived in \cite{wang2024lora} which is proven to be numerically stable. To ensure a fair comparison, we tune the learning rate via grid search over $\{ \SI{1e-3}{} , \SI{5e-4}{} , \SI{2e-4}{} , \SI{1e-4}{} , \SI{5e-5}{} , \SI{2e-5}{} , \SI{1e-5}{} \}$.

Furthermore, we fine-tune the model using one step update from full-batch gradient descent under full fine-tuning. To optimize GPU memory usage, we adopt the averaged gradient computation method from \cite{lv2023full, wang2024lora} to compute the full gradient, which is then manually added to the pre-trained weights, scaled by the learning rate. 

Besides, we notice that the test accuracy on the MNLI dataset remains $0.0\%$ for the first dozen steps in both full fine-tuning and LoRA fine-tuning. So we omit results on this dataset.
We conjecture that this is due to the significant discrepancy between pre-trained tasks and downstream tasks. For SST-2, CoLA, QNLI, and MRPC, the learning rates are set to be $\{ \SI{5e-4}{} , \SI{1e-2}{} , \SI{2e-2}{} , \num{0.5} \}$.

\begin{table}[h]
    \centering
     \caption{Hyperparameters for LoRA fine-tuning on T5-base model.}
    \label{tab:nlu-general}
    \begin{tabular}{ccccccc}
        \toprule
        Epoch & Optimizer & $(\beta_1, \beta_2)$ & $\epsilon$ & Batch Size \\
        \midrule
        1 & AdamW & (0.9, 0.999) & $\SI{1e-8}{}$ & 32 \\
        \midrule
        Warm-up Ratio & LoRA Alpha & $s$ (if needed) & $\lambda$ (if needed) & \#Runs \\
        \midrule
        0.03 & 16 & 16 & $\SI{1e-6}{}$ & 3 \\
        \midrule
        Weight Decay & LR Scheduler  & Sequence Length & Precision & \\
        \midrule
        0 & cosine & 128 & FP32 & \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Experiments on LLM}
\label{sec:exp:llm}
We use a stronger baseline for full fine-tuning, as provided in \cite{wang2024lorapro}, compared to those in \cite{wang2024lora}. For vanilla LoRA, due to the limitation of computational resources, we use the results of LoRA with rank $8$ from \cite{wang2024lora}. For LoRA-GA, we pick the best results from \citep{wang2024lora}. We align our generation configuration and stable parameter $s$ with LoRA-GA \cite{wang2024lora} to ensure a fair comparison. The hyperparameter settings are provided in \cref{tab:llama-general}. For the learning rates of LoRA-One, we conduct a grid search over $\{ \SI{5e-5}{} , \SI{2e-5}{} , \SI{1e-5}{} \}$, following the configuration used in \cite{wang2024lora}.

\begin{table}[h]
    \centering
     \caption{Hyperparameters for LoRA fine-tuning on Llama 2-7B model.}
    \label{tab:llama-general}
    \begin{tabular}{ccccc}
        \toprule
        Epoch & Optimizer & $(\beta_1, \beta_2)$ & $\epsilon$ & Batch Size \\
        \midrule
        1 & AdamW & (0.9, 0.999) & $\SI{1e-8}{}$ & 32\\
        \midrule
        Warm-up Ratio & LoRA Alpha & $s$ (if needed) & $\lambda$ (if needed) & \#Runs\\
        \midrule
        0.03 & 16 & 64 & $\SI{1e-6}{}$ & 3 \\
        \midrule
        Weight Decay & LR Scheduler & Sequence Length & Precision & \\
        \midrule
        0 & cosine & 1024 & FP32 & \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Ablation Study}
\label{appx:ablation}
In this subsection, we compare $5$ algorithms to provide insights for practical algorithm design. The details of $5$ algorithms are summarized in \cref{tab:descrip-alg}. The details of means and standard deviations over 3 runs are shown in \cref{tab:ab-cola} for CoLA and \cref{tab:ab-mrpc} for MRPC. The hyperparameter settings for LoRA-One, LoRA-One (-), LoRA-GA (-), and LoRA-GA (+) are same as the settings used in \cref{app:expNLP}. We tune the learning rates via grid search over $\{  \SI{1e-3}{}, \SI{5e-4}{}, \SI{2e-4}{},\SI{1e-4}{}, \SI{5e-5}{},\SI{2e-5}{},\SI{1e-5}{} \}$ to ensure a fair comparison. The implement details of Spectral (-) are provided in \cref{alg:spec}, which is a scaled version of \eqref{eq:spectral-init-linear} without preconditioning. We notice that Spectral (-) is highly sensitive to hyperparameters which makes it hard to tune. The general hyperparameters of Spectral (-) is same as the settings used in \cref{app:expNLP}. Here we provide the LoRA alpha and learning rates for Spectral (-) in \cref{tab:spec-hyp}.

\begin{table}[ht]
    \centering
    \caption{Initialization strategies and corresponding optimizers for ablation study.}
    \begin{tabular}{ccc}
      \toprule
         & Initialization & Optimizer \\
      \midrule
      LoRA-One   &\cref{alg:lora_one_training} (1-8) & Prec-AdamW\\
      LoRA-One (-) &\cref{alg:lora_one_training} (1-8) & AdamW\\
      Spectral (-) &\cref{alg:spec} (1-5) & AdamW\\
      LoRA-GA (-) &\eqref{LoRA-GA} & AdamW\\
      LoRA-GA (+) &\eqref{LoRA-GA} & Prec-AdamW\\
      \bottomrule
    \end{tabular}
    \label{tab:descrip-alg}
\end{table}

\begin{table}[h!]
\centering
\caption{Accuracy comparison across different methods on CoLA under three ranks, i.e. $r=8\,,32\,,128$. LoRA-One (-) stands for training with AdamW without preconditioning under initialization by line 1-8 in \cref{alg:lora_one_training}.}
\label{tab:ab-cola}
\begin{tabular}{cccccc}
\toprule
\textbf{Rank} & LoRA-One & LoRA-One (-) & Spectral (-) & LoRA-GA (-) & LoRA-GA (+) \\
\midrule
8 & {81.08}$_{\pm 0.36}$ & 80.83$_{\pm 0.54}$ &\textbf{81.40}$_{\pm 0.31}$  & 80.57$_{\pm 0.20}$ & 80.57$_{\pm 0.12}$\\
32 & \textbf{81.34}$_{\pm 0.51}$ & 81.30$_{\pm 0.16}$ &81.18$_{\pm 0.30}$& 80.86$_{\pm 0.23}$ & 80.92$_{\pm 0.34}$\\
128 & {81.53}$_{\pm 0.36}$ & 81.34$_{\pm 0.12}$ &\textbf{81.62}$_{\pm 0.48}$& 80.95$_{\pm 0.35}$ & 80.02$_{\pm 0.64}$\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Accuracy comparison across different methods on MRPC under three ranks, i.e. $r=8\,,32\,,128$. LoRA-One (-) stands for training with AdamW without preconditioning under initialization by line 1-8 in \cref{alg:lora_one_training}.}
\begin{tabular}{cccccc}
\toprule
\textbf{Rank} & LoRA-One & LoRA-One (-) & Spectral (-) & LoRA-GA (-) & LoRA-GA (+) \\
\midrule
8 & 86.77$_{\pm 0.53}$ & \textbf{87.50}$_{\pm 0.60}$ &86.19$_{\pm 0.42}$& 85.29$_{\pm 0.24}$& 85.87$_{\pm 0.31}$\\
32 & \textbf{87.34}$_{\pm 0.31}$ & 87.34$_{\pm 0.42}$ &86.02$_{\pm 0.20}$& 86.36$_{\pm 0.42}$ &85.78$_{\pm 0.20}$\\
128 & \textbf{88.40}$_{\pm 0.70}$ & 87.26$_{\pm 0.20}$ &86.03$_{\pm 0.20}$& 85.46$_{\pm 0.23}$ &87.01$_{\pm 0.35}$\\
\bottomrule
\end{tabular}
\label{tab:ab-mrpc}
\end{table}

\begin{algorithm}[!h]
\caption{\eqref{eq:spectral-init-linear} training for a specific layer}
\label{alg:spec}
\begin{algorithmic}[1]
\Input Pre-trained weight $\bm W^\natural$, batched data $\{\mathcal{D}_t\}_{t=1}^{T}$, LoRA rank $r$, LoRA alpha $\alpha$, loss function $L$, scaling parameter $\gamma$
\Initialize
\STATE Compute $\bm G^\natural \gets -\nabla_{\bm W} L(\bm W^\natural)$
\STATE $\bm U, \bm S, \bm V \gets \text{SVD}\left(\bm G^\natural\right)$
\STATE $\bm A_0 \gets \sqrt{\gamma}\cdot\bm U_{[:,1:r]}\bm S^{1/2}_{[1:r]}$
\STATE $\bm B_0 \gets \sqrt{\gamma}\cdot \bm S^{1/2}_{[1:r]}\bm V^{\!\top}_{[:,1:r]}$
\STATE Clear $\bm G^\natural$
\Train
\FOR{$t=1\,,...\,,T$}
\STATE Update parameters $\bm A_t$ and $\bm B_t$ by AdamW given $\mathcal{D}_t$
\ENDFOR
\Return $\bm W^\natural + \frac{\alpha}{\sqrt{r}} \bm A_{T} \bm B_{T}$
\end{algorithmic}
\end{algorithm}

\begin{table}[h!]
    \centering
    \caption{Specific hyperparameter settings for Spectral (-) (see details in \cref{alg:spec}) used in \cref{appx:ablation}.}
    \begin{tabular}{cccc|ccc}
    \toprule
        Rank & \multicolumn{3}{c|}{CoLA} & \multicolumn{3}{c}{MRPC} \\
        \midrule
         & LR & LoRA Alpha & $\gamma$ & LR & LoRA Alpha & $\gamma$ \\
         \midrule
        8 & $\SI{2e-3}{}$ & $\sqrt{8}$ & $0.01$ & $\SI{6e-4}{}$ & $1$ & $0.01$ \\
        32 & $\SI{2e-3}{}$ & $\sqrt{32}$ & $0.01$ & $\SI{2e-3}{}$ & $16$ & $0.01$  \\
        128 & $\SI{2e-3}{}$ & $1$ & $0.01$  & $\SI{9e-4}{}$ & $1$ & $0.01$  \\
        \bottomrule
    \end{tabular}
    \label{tab:spec-hyp}
\end{table}


\subsection{Comparison of Singular Values}
\label{SV-figs}
First, we collect top-$32$ singular values for each pre-trained layer $\mathbf{W}^\natural$ of pre-trained T5-base model \citep{raffel2020exploring}. Next, we perform full fine-tuning to the pre-trained model on SST-2 dataset from GLUE. To ensure better convergence, we take the hyperparameter settings which are presented in \cref{tab:sv_config}. After training, we collect top-$32$ singular values for each difference weights, i.e. $\Delta \mathbf{W} = \mathbf{W}_\text{fine-tuned} - \mathbf{W}^\natural$. The results are shown in \cref{fig:SV}. The hyperparameter settings for full fine-tuning are provided in \cref{tab:sv_config}.

We observe that, across all layers, the singular values of the pre-trained weights are significantly larger than those of the difference weights.
\begin{table}[h!]
    \caption{Hyperparameters for full fine-tuning on T5-base model used for \cref{SV-figs}.}
    \label{tab:sv_config}
    \centering
    \begin{tabular}{ccccc}
    \toprule
    Epoch & Optimizer & $(\beta_1, \beta_2)$ & $\epsilon$ & Batchsize \\
    \midrule
    10 & AdamW & $(0.9, 0.999)$ & $\SI{1e-8}{}$ & 32 \\
    \midrule
    Weight Decay & LR & LR Scheduler & Warm-up Ratio & \\
    \midrule
    0.1 & $\SI{1e-4}{}$ & cosine & 0.03 & \\
    \bottomrule
    \end{tabular}
\end{table}