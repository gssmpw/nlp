\subsection{Preconditioned Gradient Descent under Spectral Initialization}
\label{app:precgdlr}
Here we present the proof for preconditioned gradient descent.
\begin{equation}\tag{Prec-GD}\label{alg:prec-gd}
\begin{aligned}
    \bm A_{t+1} & = \bm A_t - \frac{\eta}{N}\widetilde{\bm X}^{\!\top}\left(\widetilde{\bm X}\left(\bm W^\natural+\bm A_t \bm B_t\right)-\widetilde{\bm Y}\right)\bm B_t^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\,,\\
    \bm B_{t+1} & = \bm B_t - \frac{\eta}{N}\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widetilde{\bm X}^{\!\top}\left(\widetilde{\bm X}\left(\bm W^\natural+\bm A_t \bm B_t\right)-\widetilde{\bm Y}\right)\,.
\end{aligned}
\end{equation}

In the following proofs, we will prove that the LoRA fine-tuning can achieve faster linear convergence which is independent of condition number $\kappa$ under \eqref{eq:spectral-init-linear} and \eqref{alg:prec-gd}. Similar to \cref{linear-invariant-B2}, the dynamics of $\bm B_t$ are still limited to the $r^*$-dimensional singular subspace $\bm V$ of $\Delta$ under \eqref{eq:spectral-init-linear}. We can verify this fact by the following lemma.

\begin{lemma}
\label{BV-perp}
    For any natural number $t \geq 0$, under assumptions in \cref{sec:assumptions} for the linear setting, with \eqref{eq:spectral-init-linear} and \eqref{alg:prec-gd}, we have
    \begin{align*}
        \bm B_t \bm V_\perp & = \bm 0_{r\times(k-r^*)}\,.
    \end{align*}
\end{lemma}
\begin{proof}
    For $t=0$, recall the SVD of $\mathbf{G}^\natural$, i.e. $\widetilde{\bm U}_{\bm G^\natural}\widetilde{\bm S}_{\bm G^\natural}\widetilde{\bm V}_{\bm G^\natural}^{\!\top}$ in \cref{NGG}, we have
    \begin{align*}
        \bm B_0 \bm V_\perp & = \left[\widetilde{\bm S}_{\bm G^\natural}^{-1/2}\right]_{[1:r]}\left[\widetilde{\bm U}_{\bm G^\natural}^{\!\top}\right]_{[:,1:r]}\mathbf{G}^{\natural} \bm V_\perp = \left[\widetilde{\bm S}_{\bm G^\natural}^{-1/2}\right]_{[1:r]}\left[\widetilde{\bm U}_{\bm G^\natural}^{\!\top}\right]_{[:,1:r]} \widehat{\bm \Sigma} \Delta \bm V_\perp = \bm 0_{r\times(k-r^*)}\,.
    \end{align*}
    Assume $\bm B_t \bm V_\perp= \bm 0_{d\times (d-r^*)}$ holds for any natural number $t\geq 1$, then
    \begin{align*}
        \bm B_{t+1} \bm V_\perp & = \bm B_t \bm V_\perp - \frac{\eta}{N}\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widetilde{\bm X}^{\!\top}\left(\widetilde{\bm X}\left(\bm W^\natural+\bm A_t \bm B_t\right)-\widetilde{\bm Y}\right)\bm V_\perp \\
        & = \bm B_t \bm V_\perp - \eta\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\bm V_\perp\\
        & = \bm 0_{r\times(k-r^*)}\,, \quad \tag*{\color{teal}[by our inductive hypothesis]}
    \end{align*}
    which proves the claim.
\end{proof}
We can re-formulate \eqref{alg:prec-gd} to be
\begin{align}
    \bm A_{t+1} & = \bm A_t - \eta\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\,,\label{reparam-linear-scaled-gd-A}\\
    \bm B_{t+1} & = \bm B_t - \eta\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\label{reparam-linear-scaled-gd-B}\,.
\end{align}
Before we start our main proofs, we first define the following notations
\begin{itemize}
    \item SVD of product matrix $\bm A_t \bm B_t := \mathcal{U}_t \mathcal{S}_t \mathcal{V}_t^{\!\top}$, where $\mathcal{U}_t \in \mathbb{R}^{d\times r^*}$, $\mathcal{S}_t \in \mathbb{R}^{r^*\times r^*}$, and $\mathcal{V}_t\in \mathbb{R}^{k\times r^*}$. Notice that here we employ rank-$r^*$ SVD of $\bm A_t \bm B_t$ since $\operatorname{Rank}\left(\bm A_t\bm B_t\right)\leq r^*$ due to \cref{BV-perp} and $\lambda_{r^*}\left(\bm A_t \bm B_r\right)>0$ strictly which we will obtain from \cref{prec-gd-linear-conv}.
    \item The left compact singular matrix of $\bm A_t$ as $\bm U_{\bm A_t} \in \mathbb{R}^{d\times r}$.
    \item The right compact singular matrix of $\bm B_t$ as $\bm V_{\bm B_t} \in \mathbb{R}^{k\times r^*}$. Notice that here we take the top-$r^*$ right singular subspace of $\bm B_t$ due to \cref{BV-perp}.
\end{itemize}
By the pseudo inverse theorem and \citet[Lemma 14]{jia2024preconditioning}, we can obtain
\begin{align}
    &\bm A_t \left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top} = \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\,,\label{proj-A}\\ 
    &\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\bm B_t = \bm V_{\bm B_t}\bm V_{\bm B_t}^{\!\top}\,.\label{proj-B}\\
    &\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top} = \mathcal{V}_t \mathcal{S}^{-1}_t \mathcal{U}_t^{\!\top}\label{pseudo-inverse-AB}\,.
\end{align}

\begin{lemma}
\label{joint-scaled-gd-linear-dynamics}
    Denote $\bm R_t := \bm A_{t}\bm B_{t} - \Delta$, $\bm \Xi:=\widehat{\bm \Sigma}-\bm I_d$, under assumptions in \cref{sec:assumptions} for the linear setting, with \eqref{alg:prec-gd}, then we have
    \begin{align*}
        \bm R_{t+1} & = \bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top} - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm \Xi\bm R_t - \eta \bm \Xi\bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}+ \eta^2 \widehat{\bm \Sigma}\bm R_t\mathcal{V}_t \mathcal{S}^{-1}_t \mathcal{U}_t^{\!\top}\widehat{\bm \Sigma}\bm R_t\,.
    \end{align*}
\end{lemma}
\begin{proof}
    With \cref{reparam-linear-scaled-gd-A} and \cref{reparam-linear-scaled-gd-B}, we can construct
    \begin{align*}
    \bm R_{t+1} &=\bm A_{t+1}\bm B_{t+1} - \Delta\\
    & = \bm A_{t}\bm B_{t} - \Delta\\
    & \quad - \eta \bm A_t\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\\
    & \quad - \eta \widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\bm B_t\\
    & \quad + \eta^2 \widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\\
    & = \bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\widehat{\bm \Sigma}\bm R_t - \eta \widehat{\bm \Sigma}\bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\quad \tag*{\color{teal}[by \cref{proj-A} and \cref{proj-B}]}\\
    & \quad + \eta^2 \widehat{\bm \Sigma}\bm R_t\mathcal{V}_t \mathcal{S}^{-1}_t \mathcal{U}_t^{\!\top}\widehat{\bm \Sigma}\bm R_t\quad \tag*{\color{teal}[by \cref{pseudo-inverse-AB}]}\\
    & = \bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top} - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm \Xi\bm R_t - \eta \bm \Xi\bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}+ \eta^2 \widehat{\bm \Sigma}\bm R_t\mathcal{V}_t \mathcal{S}^{-1}_t \mathcal{U}_t^{\!\top}\widehat{\bm \Sigma}\bm R_t\,,
\end{align*}
which proves the claim.
\end{proof}

In the next, we aim to estimate the signal part $\bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}$.
\begin{lemma}
\label{strict-equal-F-norm}
    Recall $\bm R_t := \bm A_{t}\bm B_{t} - \Delta$, under assumptions in \cref{sec:assumptions} for the linear setting, with \eqref{alg:prec-gd}, then
    \begin{align*}
        \left\|\bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F} & \leq (1-\eta) \left\|\bm R_t\right\|_{\rm F}\,.
    \end{align*}
\end{lemma}
\begin{proof}
    \begin{align*}
        &\left\|\bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}\\
        = & \left\|\bm R_t \left(\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}+\bm I_k - \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right) - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t \left(\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}+\bm I_k - \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right) - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}\\
        = & \left\|\bm R_t \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top} - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top} - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F} \quad \tag*{\color{teal}$\left[\text{since }\bm R_t\left(\bm I_k - \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right)=\bm 0\text{ by \cref{BV-perp}}\right]$}\\
        = & \left\|\left(\bm I_d - \eta \left(\bm I_d + \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\right)\right)\bm R_t \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}\\
        = & \left\|\bm I_d - \eta \left(\bm I_d + \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\right)\right\|_{op}\left\|\bm R_t \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}\\
        \leq & (1-\eta) \left\|\bm R_t\right\|_{\rm F}\,,\quad \tag*{\color{teal}$\left[\left\|\bm I_d - \eta \left(\bm I_d + \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\right)\right\|_{op}\leq 1-\eta, \text{ since }\bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\text{ is a rank-}r\text{ projection matrix}\right]$}
    \end{align*}
    which concludes the proof.
\end{proof}

\begin{theorem}
    \label{prec-gd-linear-conv}
    Under assumptions in \cref{sec:assumptions} for the linear setting, with \eqref{eq:spectral-init-linear} and \eqref{alg:prec-gd}, we choose
    \begin{align*}
        \epsilon\leq\min\left\{\frac{1}{2\sqrt{r^*}\kappa}\,,\frac{1}{4}\right\}
    \end{align*}
    and set $ \eta \in \left(0, \frac{0.5-2\epsilon}{(1+\epsilon)^2}\right)$, then with probability at least $1- 2C\exp(-\epsilon^2 N)$ for a universal constant $C>0$, we have
    \begin{align*}
        \left\|\bm A_t \bm B_t - \Delta\right\|_{\rm F} & \leq \frac{1}{2}\left(1-\frac{\eta}{2}\right)^t\lambda_{r^*}^*\,.
    \end{align*}
\end{theorem}
\begin{proof}
We prove it by induction.
We suppose the following two inductive hypothesis
\begin{align}
    \lambda_{r^*}\left(\bm A_t \bm B_t\right) & \geq \frac{\lambda^*_{r^*}}{2}\,,\label{inductive-joint-rth-singular-value-lower}\\
    \left\|\bm A_0 \bm B_0 - \Delta\right\|_{\rm F} & \leq \frac{\lambda^*_{r^*}}{2}\,.\label{inductive-loss-hypothesis}
\end{align}
Starting from $t=0$, under \eqref{eq:spectral-init-linear}, with probability at least $1- 2C\exp(- \epsilon^2 N)$ for a universal constant $C>0$, we have
\begin{align*}
    \left\|\bm A_0 \bm B_0 - \Delta\right\|_{\rm F} & = \left\|\bm G^\natural - \Delta\right\|_{\rm F}\\
    & = \left\|\left(\widehat{\bm \Sigma}-\bm I_d\right)\Delta\right\|_{\rm F} \quad \tag*{\color{teal}[by \cref{NGG}]}\\
    & \leq \epsilon \|\Delta\|_{\rm F}\\
    & \leq \epsilon \sqrt{r^*} \|\Delta\|_{op}\tag*{\color{teal}$\left[\text{since }\operatorname{Rank}\left(\Delta\right)=r^*\right]$}\\
    & \leq \frac{\lambda^*_{r^*}}{2}\,. \tag*{\color{teal}$\left[\text{since }\epsilon\leq1/2\sqrt{r^*}\kappa\right]$}
\end{align*}
Then, by Weyl's inequality, we have
\begin{align*}
    \lambda_{r^*}\left(\Delta\right)-\lambda_{r^*}\left(\bm A_0 \bm B_0\right) & \leq \left\|\bm A_0 \bm B_0 - \Delta\right\|_{op} \leq \left\|\bm A_0 \bm B_0 - \Delta\right\|_{\rm F}\,,
\end{align*}
which implies
\begin{align}
\label{joint-rth-singular-value-lower}
    \lambda_{r^*}\left(\bm A_0 \bm B_0\right) & \geq \frac{\lambda^*_{r^*}}{2}\,.
\end{align}
Therefore, we verify \cref{inductive-joint-rth-singular-value-lower} and \cref{inductive-loss-hypothesis} at $t=0$. We assume \cref{inductive-joint-rth-singular-value-lower} and \cref{inductive-loss-hypothesis} hold at $t=2,3,...$, then by \cref{joint-scaled-gd-linear-dynamics}, with probability at least with probability $1- 2C\exp(-\epsilon^2 N)$ for a universal constant $C>0$, we have
    \begin{align*}
        \left\|\bm R_{t+1}\right\|_{\rm F} & \leq \left\|\bm R_t - \eta \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t - \eta \bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}\\
        & \quad + \eta \left\|\bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm \Xi\bm R_t\right\|_{\rm F} + \eta \left\|\bm \Xi\bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}
        + \eta^2 \left\|\widehat{\bm \Sigma}\bm R_t\mathcal{V}_t \mathcal{S}^{-1}_t \mathcal{U}_t^{\!\top}\widehat{\bm \Sigma}\bm R_t\right\|_{\rm F}\\
        & \leq (1-\eta)\left\|\bm R_t\right\|_{\rm F}\quad \tag*{\color{teal}[by \cref{strict-equal-F-norm}]}\\
        & \quad + \eta \epsilon \left\|\bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\bm R_t\right\|_{\rm F} + \eta \epsilon \left\|\bm R_t\bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right\|_{\rm F}
        + \eta^2 (1+\epsilon)^2 \frac{\left\|\bm R_t\right\|^2_{\rm F}}{\lambda_{r^*}\left(\bm A_t \bm B_t\right)} \quad \tag*{\color{teal}$\left[\text{by }\|\bm \Xi\|_{op}\leq \epsilon\right]$}\\
        & \leq (1-\eta)\left\|\bm R_t\right\|_{\rm F}\\
        & \quad + \eta \epsilon \left\|\bm R_t\right\|_{\rm F} + \eta \epsilon \left\|\bm R_t\right\|_{\rm F}
        + \eta^2 (1+\epsilon)^2 \left\|\bm R_t\right\|_{\rm F}\quad \tag*{\color{teal}$\left[\text{since \cref{inductive-joint-rth-singular-value-lower} and \cref{inductive-loss-hypothesis} hold at }t\right]$}\\
        & = \left(1-(1-2\epsilon)\eta +\eta^2(1+\epsilon)^2\right)\left\|\bm R_t\right\|_{\rm F}\\
        & \leq \left(1-\frac{\eta}{2}\right)\left\|\bm R_t\right\|_{\rm F}\,.\quad \tag*{\color{teal}$\left[\text{taking}~\eta \leq \frac{0.5-2\epsilon}{(1+\epsilon)^2} \right]$}
    \end{align*}
    This implies \cref{inductive-loss-hypothesis} at time $t+1$. By consequence, we can obtain \cref{inductive-joint-rth-singular-value-lower} at time $t+1$ again by Weyl's inequality.
\end{proof}