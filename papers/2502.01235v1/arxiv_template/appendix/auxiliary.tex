\section{Auxiliary Results for Proofs}
\label{auxiliary}
In this subsection, we present some auxiliary results that are needed for our proof.
First, we present the estimation of the spectral norm of random matrices.
It can be easily derived from \cite{vershynin2018high} and we put it here for the completeness.

\begin{lemma}\citep[Adapted from Theorem 4.6.1]{vershynin2018high}
\label{lem:conrg}
    For a random sub-Gaussian matrix $\widetilde{\bm X} \in \mathbb{R}^{N \times d}$ whose rows are i.i.d. isotropic sub-gaussian random vector with sub-Gaussian norm $K$, then we have the following statement
\[
\mathbb{P} \left(   \left\|\frac{1}{N}\widetilde{\bm X}^{\!\top}\widetilde{\bm X}-\bm I_d\right\|_{op}  > \delta \right) \leq 2 \exp \left( -C N \min\left(\delta^2, \delta\right) \right)\,.
\]
for a universal constant $C$ depending only on $K$.
\end{lemma}

\begin{lemma}\citep[Adapted from Corollary 5.35]{vershynin2010introduction}
\label{lem:init-op-conct}
    For a random standard Gaussian matrix $\bm S\in\mathbb{R}^{d\times r}$ with $[\bm S]_{ij} \sim \mathcal{N}(0, 1)$, if $d > 2r$, we have 
    \begin{align}
        \label{norm-A0}
        \frac{\sqrt{d}}{2} \leq \|\bm S\|_{op} \leq (2 \sqrt{d} + \sqrt{r})\,,
    \end{align}
    with probability at least $1-C \operatorname{exp}(-d)$ for some positive constants $C$.
\end{lemma}

The following results are modified from the proof of \citet[Lemma 8.7]{stoger2021small}.
\begin{lemma}
\label{lem:min-singular-conct}
    Suppose $\bm S\in\mathbb{R}^{d\times r}$ is a random standard Gaussian matrix with $[\bm S]_{ij} \sim \mathcal{N}(0, 1)$ and $\bm U\in\mathbb{R}^{d\times r^*}$ has orthonormal columns. If $r\geq 2r^*$, with probability at least $1-C\operatorname{exp}(-r)$ for some positive constants $C$, we have
    \begin{align*}
        \lambda_{\operatorname{min}}(\bm U^{\!\top}\bm S) & \gtrsim 1\,.
    \end{align*}
    If $r^*\leq r < 2r^*$, by choosing $\xi>0$ appropriately, with probability at least $1-(C \xi)^{r-r^*+1}-C'\operatorname{exp}(-r)$ for some positive constants $C\,,C'$, we have
    \begin{align*}
        \lambda_{\operatorname{min}}(\bm U^{\!\top}\bm S) & \gtrsim \frac{\xi}{r}\,.
    \end{align*}
\end{lemma}

Next, we give a short description of the Hermite expansion of ReLU function via Hermite polynomials. Details can be found in \citet[A.1.1]{damian2022neural} and \cite{arous2021online}.
To be specific, the Hermite expansion of ReLU function $\sigma(x)$ is
\begin{align}
\label{Hermite-sigma}
    \sigma(x)=\sum_{j=1}^\infty \frac{c_j}{j!}\operatorname{He}_j(x) =\frac{1}{\sqrt{2\pi}}+\frac{1}{2}x+\frac{1}{\sqrt{2\pi}}\sum_{j\geq 1}\frac{(-1)^{j-1}}{j!2^j(2j-1)}\operatorname{He}_{2j}(x)\,,
\end{align}
which implies that we can express the Hermite coefficients as
\begin{align}
\label{Hermite-coef}
    \left\{\begin{aligned}
        c_0 & = \frac{1}{\sqrt{2\pi}}\,,\\
        c_1 & = \frac{1}{2}\,,\\
        c_{2j} & = \frac{(-1)^{j-1}}{\sqrt{2\pi}2^j(2j-1)}\quad \text{for }j\geq 1\,.
    \end{aligned}\right.
\end{align}
Furthermore, the derivative of $\sigma(x)$ admits
\begin{align}
\label{Hermite-sigma'}
    \sigma'(x)=\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\sum_{j\geq 0}\frac{(-1)^{j}}{j!2^j(2j+1)}\operatorname{He}_{2j+1}(x)\,.
\end{align}

\begin{lemma}\citep[Corollary 9]{oko2024pretrained}\label{differential}
$\mathbb{E}_{\widetilde{\bm x}}[\nabla^k \sigma(\langle \bm w\,, \widetilde{\bm x}\rangle)] = c_k \bm w^{\otimes k}$ for any $k$ such that $c_k\neq 0$.
\end{lemma}

\begin{lemma}\label{vec-ineq}
For any vectors $\bm u$ and $\bm v$, we have
    \begin{align*}
        \left|\langle \bm u\,, \bm u \rangle^j - \langle \bm u\,, \bm v \rangle^j\right| & \leq j\,\max\left\{\left\|\bm u\right\|_2\,,\left\|\bm v\right\|_2\right\}^{2j-1} \left\|\bm u - \bm v\right\|_2\,.
    \end{align*}
\end{lemma}
\begin{proof}
    First, we analyze the following two scalar variables case
    \begin{align*}
        \left|x^j-y^j\right|\,.
    \end{align*}
    By algebraic identity $\sum_{j=1}^{t-1}x^{t-j-1}y^j=\frac{x^t-y^t}{x-y}$ which is valid for $\forall\,j\in\mathbb{N}^+$, we have
    \begin{align*}
        \left|x^j-y^j\right|&=\left|(x-y)\sum_{i=0}^{j-1}x^{j-i-1}y^i\right|
        \leq |x-y|\sum_{i=0}^{j-1}\max\left\{|x|\,,|y|\right\}^{j-1}
        = j|x-y|\max\left\{|x|\,,|y|\right\}^{j-1}\,.
    \end{align*}
    Now we define $x:=\langle \bm u\,, \bm u \rangle$ and $y:=\langle \bm u\,, \bm v \rangle$, then we can obtain
    \begin{align*}
        \left|\langle \bm u\,, \bm u \rangle^j - \langle \bm u\,, \bm v \rangle^j\right| & \leq j\,\max\left\{\left|\langle \bm u\,, \bm u \rangle\right|\,,\left|\langle \bm u\,, \bm v \rangle\right|\right\}^{j-1}\left|\langle \bm u\,, \bm u \rangle - \langle \bm u\,, \bm v \rangle\right|\\
        & \leq j\,\max\left\{\left\|\bm u\right\|_2^2\,,\left\|\bm u\right\|_2 \left\|\bm v\right\|_2\right\}^{j-1}\left\|\bm u\right\|_2 \left\|\bm u - \bm v\right\|_2\quad \tag*{\color{teal}[by Cauchy-Schwartz inequality]}\\
        & = j\,\max\left\{\left\|\bm u\right\|_2\,,\left\|\bm v\right\|_2\right\}^{2j-1} \left\|\bm u - \bm v\right\|_2\,.
    \end{align*}
\end{proof}