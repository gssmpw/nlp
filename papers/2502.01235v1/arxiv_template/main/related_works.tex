\subsection{Related Work}

\noindent
{\bf Parameter-Efficient Fine-Tuning (PEFT):}
LoRA \citep{hu2022lora} and its variants have received great attention for downstream applications.
The variants of LoRA focus on imbalance stepsize \citep{hayou2024lora+}, initialization using SVD of pre-trained weights \citep{meng2024pissa}, gradient approximation \citep{wang2024lora,wang2024lorapro} for better performance, reducing parameters \citep{kopiczko2024vera} efficiency, preconditioned algorithm \citep{zhang2024riemannian} for stability. 


In theory, the training dynamics and generalization ability of LoRA are rarely discovered. Based on the empirical evidence of kernel behavior of LoRA in \cite{malladi2023kernel,jang2024lora} prove that LoRA with rank $\mathcal{O}(\sqrt{N})$ trained via gradient descent with $N$ data can admit global minimizer under neural tangent kernel \citep{jacot2018neural} regime. Beyond kernel regime, \cite{dayi2024gradientdynamicslowrankfinetuning} explores the SGD dynamics of rank-$1$ LoRA. However, their setting is unrealistic since they only train $\bm A$ which is a vector and restricted to the rank-$1$ setting. In our work, we simultaneously train $\bm A$ and $\bm B$ and have flexible choice for LoRA ranks. Moreover, we discover the relationship between LoRA and full fine-tuning from dynamics.\\

\noindent
{\bf Matrix Sensing under Gradient Descent:}
Since LoRA performs fine-tuning using a Burer-Monterio factorization, it admits similarities with matrix sensing problems, including the symmetric matrix problem  with $r=r^*$ \citep{li2018algorithmic} and $r \geq r^*$ \citep{stoger2021small}; asymmetric problem with $r\geq r^*$ \citep{soltanolkotabi2023implicit,xiong2023over}.
Regarding initialization, small initialization \citep{ding2022validation} and spectral initialization \citep{ma2021beyond} help convergence with theoretical guarantees.
Besides, adding preconditioner \citep{zhang2021preconditioned,tong2021accelerating,xu2023power,zhang2023preconditioned} is beneficial to solve the problem of ill-conditioned ground truth matrix.

Technically, for the alignment part, our theory leverages some techniques from \cite{soltanolkotabi2023implicit}. However, the symmetrization technique used in prior work cannot be applied to decouple the GD dynamics of $(\bm A_t, \bm B_t)$, posing a challenge in analyzing their individual spectral behaviors. To overcome this limitation, we develop a novel approach that enables a detailed analysis of the distinct spectral dynamics of $\bm A_t$ and $\bm B_t$, which is one technical contribution of this work.
Besides, for the nonlinear model part, dynamical analysis are normally based on classical gradient-based algorithm \citep{damian2022neural,lee2024neural}.
How such model behaves under low-rank updates under \eqref{eq:spectral-init-linear} has been unclear to our knowledge.