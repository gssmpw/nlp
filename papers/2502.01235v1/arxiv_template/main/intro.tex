\section{Introduction}

How to efficiently learn nonlinear models has been the recurring theme in machine learning \citep{alpaydin2020introduction}, especially in the era of large language models (LLMs) \citep{brown2020language,thoppilan2022lamda}.
Parameter-efficient fine-tuning of LLMs \citep{achiam2023gpt} aims to design scalable strategies to approximate/learn an unknown feature shift $\Delta$ such that LLMs perform well on new tasks while retain the knowledge from pre-trained models.

One typical parameter-efficient strategy is Low-Rank Adaptation (LoRA) \citep{hu2022lora}, which learns a low-rank approximation of the unknown feature shift, $\Delta \approx \bm A \bm B$, using two low-rank matrices $\bm A$ and $\bm B$. LoRA uses random Gaussian initialization for $\bm A$ and zero initialization for $\bm B$:
\begin{equation}\tag{LoRA-init}\label{eq:lorainit}
    [\bm A_0]_{ij} \sim \mathcal{N}(0, \alpha^2) \quad \text{and} \quad [\bm B_0]_{ij} = 0\,, \quad \alpha > 0\,.
\end{equation}
To improve the performance in the downstream tasks, various LoRA-based algorithms have been proposed based on, e.g., refined initialization \citep{li2024crucialroleinitializationmatrix}, learning rates \citep{hayou2024lora+}, efficiency \citep{kopiczko2024vera}, and gradient information \citep{meng2024pissa,wang2024lora}.

The goal of this work is to improve the practical performance of LoRA through theoretical insights. Although LoRA appears simple, its dynamics are inherently nonlinear and non-convex, and there is limited theoretical analysis of its behavior and generalization guarantees. Most prior theoretical studies are restricted to either lazy-training regimes \citep{jang2024lora,malladi2023kernel}, or highly simplified settings such as when $\bm A, \bm B$ are scalars \citep{hayou2024lora+} or vectors \citep{dayi2024gradientdynamicslowrankfinetuning}. It remains unclear how gradient updates in LoRA behave in the presence of data-adaptive and time-dependent nonlinearity. On the other hand, fine-tuning is ultimately an application-driven task. Therefore, when seeking a rigorous theoretical understanding of LoRA, we are equally (if not more) interested in leveraging our theoretical findings to guide practical algorithm design. This paper addresses two key questions sitting at the intersection of theory and practice:
\begin{itemize}
    \item {\em Q1: How to characterize low-rank dynamics of LoRA and the associated subspace alignment in theory?}
    \item {\em Q2: How can our theoretical results contribute to algorithm design for LoRA in practice?}
\end{itemize}


\subsection{Contributions and Algorithm Design Principles}
\label{sec:contributions}





\begin{table*}[t]
        \centering
        \fontsize{9}{8}\selectfont
        \begin{threeparttable}
        \caption{Main results in the main text and appendix from subspace alignment to global convergence.}
                \label{tabres}
                \begin{tabular}{ccccccc}
                        \toprule
                        Model &Results & Algorithm & Initialization & Conclusion \cr
                        \midrule
                        \multirow{8}{1.1cm}{Linear} 
                        & \cref{thm:alignlinearB} & GD & \eqref{eq:lorainit} & Subspace alignment of $\bm B_t$ \cr
                        \cmidrule(lr){2-5}
                        & \cref{thm:alignlinearA} & GD & \eqref{eq:lorainit} & Subspace alignment of $\bm A_t$ \cr
                        \cmidrule(lr){2-5}
                        & \cref{main:linear-initial-risk} & GD & \eqref{eq:spectral-init-linear} & $\| \bm A_0 \bm B_0 - \Delta\|_{\rm F}$ is small \cr
                        \cmidrule(lr){2-5}
                        & \cref{thm:gc-linear-spec} & GD & \eqref{eq:spectral-init-linear} & Linear convergence of $\| \bm A_t \bm B_t - \Delta\|_{\rm F}$ \cr
                        \cmidrule(lr){2-5}
                        &\cref{main:prec-gd-linear-conv}  & Prec-GD & \eqref{eq:spectral-init-linear} & Linear convergence rate independent of $\kappa(\Delta)$ \cr
                        \midrule
                        \multirow{3}{1.1cm}{Nonlinear} 
                        & \cref{main:LC} & Prec-GD & \eqref{eq:spectral-init-linear} & Linear convergence rate independent of $\kappa(\Delta)$ \cr
                        \cmidrule(lr){2-5}
                        &\cref{smo-LC} & Smooth Prec-GD & \eqref{eq:spectral-init-linear} & Linear convergence with less assumptions \cr
                        \bottomrule
                \end{tabular}
        \end{threeparttable}
\end{table*}

In this work, we theoretically investigate the behavior of gradient descent (GD) update of LoRA parameters $(\bm A_t, \bm B_t)$ and identify the subspaces they align with. Our analysis covers both linear and nonlinear models, with an overview of our results given in \cref{tabres}. Building on design principles distilled from these insights, we develop theoretically grounded algorithms that achieve enhanced performance in practical applications.


In \cref{sec:linear}, we start by analyzing LoRA for fine-tuning a multi-output linear model. Denoting one-step gradient of full fine-tuning as $\bm G^{\natural}$, we prove that the gradient update aligns $\bm A_t$ with the singular subspace of $\bm G^{\natural}$ while $\bm B_t$ always stays in a certain singular subspace w.r.t.\ $\bm G^{\natural}$; see \cref{sec:align_linear} for details.

Consequently, by computing the singular value decomposition (SVD) of ${\bm G}^{\natural} = \widetilde{\bm U}_{\bm G^\natural} \widetilde{\bm S}_{\bm G^\natural} \widetilde{\bm V}_{{\bm G^\natural}}^{\!\top}$ and we can directly achieve the above alignment if we use its certain singular subspaces for initialization:
\begin{equation}\tag{Spectral-init}\label{eq:spectral-init-linear}
\begin{split}
       &\bm A_0 = \sqrt{\gamma}\left[\widetilde{\bm U}_{\bm G^\natural}\right]_{[:,1:r]}\left[\widetilde{\bm S}_{\bm G^\natural}^{1/2}\right]_{[1:r]}\,,\\
    &\bm B_0 = \sqrt{\gamma}\left[\widetilde{\bm S}_{\bm G^\natural}^{1/2}\right]_{[1:r]}\left[\widetilde{\bm V}_{\bm G^\natural}\right]_{[:,1:r]}^{\!\top}\,, 
\end{split}
\end{equation}
where $\gamma$ is a tuning parameter.
In \cref{sec:linear-spectral}, we prove that the initial iterate ($\bm A_0, \bm B_0$) computed by  \eqref{eq:spectral-init-linear} approximately recovers the downstream target shift $\Delta$, thereby showing the {\bf sufficiency of using one-step full gradient}, which can be numerically verified on several benchmarks in \cref{tab:nlu-performance} of \cref{sec:algoexp}. In \cref{fig:phase-transi}, we present a toy experiment to intuitively illustrate the advantages of \cref{eq:spectral-init-linear} over \cref{eq:lorainit}. 

Continuing the GD update for  $(\bm A_t, \bm B_t)$, we further establish the linear convergence rate of $\| \bm A_t \bm B_t - \Delta \|_{\rm F}$. 
However, this linear rate is sensitive to the condition number $\kappa(\Delta)$ of $\Delta$, leading to unsatisfactory convergence performance if $\Delta$ is ill-conditioned.
To address this issue, we rigorously show that adding preconditioners into the GD update eliminates the dependence on the condition number; see \cref{sec:scaledgd}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{arxiv_template/pdf_figs/seed18_v2.pdf}
    \includegraphics[width=0.32\linewidth]{arxiv_template/pdf_figs/seed7_wo_v2.pdf}
    \includegraphics[width=0.32\linewidth]{arxiv_template/pdf_figs/seed3_wo_v2.pdf}
    \caption{Comparison of the GD trajectories under \eqref{eq:lorainit} and \eqref{eq:spectral-init-linear} with three different starting points. We can observe that the starting points initialized by \eqref{eq:spectral-init-linear} are consistently closer to the set of global minimizers, whereas those initialized by \eqref{eq:lorainit} tend to be farther away across different random seeds. Moreover, running gradient descent from points initialized by \eqref{eq:spectral-init-linear} requires significantly fewer steps to reach a global minimizer, demonstrating the advantages of \eqref{eq:spectral-init-linear}. More details can be found in \cref{exp:toy-setting}.}
    \label{fig:phase-transi}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=.75\linewidth]{arxiv_template/pdf_figs/GA-vs-Ours-lower-left-fig1.pdf}
\caption{The log-risk curve under \eqref{eq:spectral-init-linear}, LoRA-GA \citep{wang2024lora}, and LoRA-One in \cref{alg:lora_one_training}, trained via (Prec-)GD on fine-tuning task \eqref{eq:lab_linear}. $(-)$ indicates that preconditioners is not added into GD. The risk is defined as $\frac{1}{2}\left\|\bm A_t \bm B_t - \Delta\right\|^2_{\rm F}$. We compare the risk convergence of these algorithms under \textit{Left}: GD in the exact-ranked setting; \textit{Right}: adding preconditioners or not in the ill-conditioned setting. More experimental details and GD trajectories comparisons can be found in \cref{exp:toy-setting}.}
\label{figs:GA-vs-Ours}
\end{figure}

In \cref{sec:nonlinear}, we extend our theoretical results to nonlinear models with ReLU activation. We prove that the sufficiency of using one-step full gradient still holds under \eqref{eq:spectral-init-linear} and further establish linear convergence rate of $\| \bm A_t \bm B_t - \Delta \|_{\rm F}$. Besides, stronger convergence guarantees can be given by modifying the gradient update.

From the above theoretical results, we derive the following two algorithm design principles: 
\begin{itemize}
    \item \#1: Proper spectral initialization by \eqref{eq:spectral-init-linear} helps alignment and achieves better performance.
    \item \#2: Preconditioners accelerate LoRA updates in the high-rank or ill-conditioned case.
\end{itemize}
\cref{figs:GA-vs-Ours} demonstrates the power of these two principles.
This leads to our theoretically grounded algorithm, \emph{LoRA-One}, which uses \emph{one}-step full gradient and preconditioners and is given in \cref{sec:algoexp}.
Our experiments demonstrate that LoRA-One achieves promising performance when compared to LoRA and its variants on natural language processing (NLP) tasks and LLMs under various tasks.
More experimental details can be found in \cref{exp-settings}. Besides, our theory and empirical observations also point to potential limitations of previous LoRA variants that are based on gradient alignment, e.g., LoRA-GA \citep{wang2024lora}; see the discussion in \cref{sec:algoexp}.


\noindent
\textbf{Notations~~} For a matrix $\bm A$, let $\left\|\bm A\right\|_{op}$ denote its operator and $\left\|\bm A\right\|_{\rm F}$ its Frobenius norm. Let $\odot$ denote the Hadamard (i.e., entrywise) matrix product. We use $\bm I_n$ to denote the $\mathbb{R}^{n\times n}$-valued identity matrix.
The notation $\bm U_{\bm A}$ denotes the left singular matrix of the compact SVD of $\bm A$ and $\bm U_{\bm A, \perp}$ denotes the corresponding orthogonal complement. Similarly, $\bm V_{\bm A}$ denotes the right singular matrix of $\bm A$ and $\bm V_{\bm A, \perp}$ denotes its orthogonal complement. Let $\bm U_{r^*}(\bm A)$ denote the left singular subspace spanned by the $r^*$ largest singular values of $\bm A$ and $\bm U_{r^*,\perp}(\bm A)$ denote the left singular subspace orthogonal to $\bm U_{r^*}\left(\bm{A}\right)$. Similarly define $\bm V_{r^*}(\bm A)$ and $\bm V_{r^*,\perp}(\bm A)$ for the right singular subspace.
A complete list notations can be found in \cref{tab:notation} of \cref{app:notation}.
