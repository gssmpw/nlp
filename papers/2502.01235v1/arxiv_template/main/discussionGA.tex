\section{Discussion on Prior Work Based on Gradient Alignment}
\label{app:disGA}

Our initialization strategy in \cref{alg:lora_one_training} (line 4-6) shares some similarity with prior work on gradient alignment, e.g., LoRA-GA \citep{wang2024lora}, and LoRA-pro \citep{wang2024lorapro}.
However, the motivation behind these gradient alignment work differs significantly from ours. The above gradient alignment based algorithms are driven by how to approximate the full fine-tuning gradient by low-rank updates. Instead, our our work is motivated by which subspace $(\bm A_t, \bm B_t)$ will align with and then how to achieve this alignment efficiently so as to finally recover $\Delta$.

Here we take LoRA-GA as an example to explain the potential issue that the spirit of LoRA-GA might not help recover $\Delta$, both theoretically and empirically.
To be specific, LoRA-GA \citep{wang2024lora} also computes the SVD of $\nabla_{\bm W} {L}(\bm W^\natural)$. To ensure the pre-trained model remains unchanged at $t=0$, LoRA-GA the following strategy
\begin{equation}\tag{LoRA-GA}\label{LoRA-GA}
\begin{split}
    &\bm A_0 = -\sqrt{\gamma}\left[\widetilde{\bm U}_{\bm G^\natural}\right]_{[:,1:r]}\,,
    \bm B_0 = \sqrt{\gamma}\left[\widetilde{\bm V}_{\bm G^\natural}\right]_{[:,r+1:2r]}^{\!\top}\,,\\
    &\bm W_{\tt off}^\natural := \bm W^\natural - \frac{\alpha}{\sqrt{r}}\bm A_0 \bm B_0\,.
\end{split}
\end{equation}

Theoretically, LoRA-GA observes  $\operatorname{rank}(
\nabla_{\bm A}\widetilde{L}\left(\bm A_t\,,\bm B_t\right) + \nabla_{\bm B}\widetilde{L}\left(\bm A_t\,,\bm B_t\right)) \leq 2r$ and then proposes to find the best $2r$-rank approximation of one-step full gradient to the first step of LoRA. Accordingly, LoRA-GA chooses the first $r$ singular values for $\bm A_0$ and $(r+1)$th to $2r$th singular values for $\bm B_0$.
However, as pointed by our theory, $\bm B_t$ will also align to the right-side rank-$r^*$ singular subspace of $\bm G^{\natural}$ under random initilization. That means, due to the way LoRA-GA chooses the $(r+1)$th to $2r$th singular values for $\bm B_0$, the iterate $\bm B_t$ does not lie in the desired subspace and may not escape an undesirable subspace. 

Empirically, the mismatch of singular subspace induced by corresponding singular values in LoRA-GA might bring unfavorable performance even in a toy model. We consider the exact-ranked case ($r=r^*$) for fine-tuning task in the linear setting. 
We compare the generalization risk of three initialization strategies: \eqref{eq:spectral-init-linear}, \cref{alg:lora_one_training} without preconditioners, and LoRA-GA trained via vanilla GD. The results are shown in \cref{figs:GA-vs-Ours}. We can empirically observe that LoRA-GA fails to generalize and remain at a high-risk level throughout training. In contrast, \eqref{eq:spectral-init-linear} and \cref{alg:lora_one_training} both can generalize well. This empirically demonstrates the optimality of choosing top-$r$ singular subspace of $\bm G^\natural$.

Before the submission deadline we became aware of the concurrent work \cite{ponkshe2024initialization}, which uses the same initialization for $\bm B_0$ as in line 6 of our \cref{alg:lora_one_training}. However, the motivation, problem setting, and theoretical analysis are totally different between our work and theirs. Moreover, our \cref{alg:lora_one_training} also introduces the preconditioners and is able to efficiently handle ill-conditioned cases, and this is not available in \cite{ponkshe2024initialization}.