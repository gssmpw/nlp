\section{Analysis of LoRA under Nonlinear Models}
\label{sec:nonlinear}


Now we focus on the nonlinear setting described in \cref{sec:problsemsetting}, where we consider the exact-rank case $r=r^*$ for delivery.
We will demonstrate the linear convergence rate in the linear setting can still hold for the nonlinear setting.

Following \cref{sec:scaledgd}, we continue to consider preconditioned GD on $(\bm A_t, \bm B_t)$ with the same step-size $\eta$ for convenience:
\begin{equation}\label{eq:ABiter_nonlinear}
\begin{split}
     \bm A_{t+1} & = \bm A_t - \eta \nabla_{\bm A} \widetilde{L}\left(\bm A_t\,,\bm B_t\right)\left(\bm B_t \bm B_t^{\!\top}\right)^{-1}\,, \\
     \bm B_{t+1}  & = \bm B_t - \eta \left(\bm A_t^{\!\top} \bm A_t\right)^{-1} \nabla_{\bm B} \widetilde{L}\left(\bm A_t\,,\bm B_t\right)\,.
\end{split}
\end{equation}
Notice that here we use standard matrix inversion since we can prove that $\bm A_t$ and $\bm B_t$ stay non-singular across all $t\geq 0$.
By denoting $\bm W_t := \bm W^{\natural} + \bm A_t \bm B_t$, we have the gradients
\begin{align*}
\nabla_{\bm A}\widetilde{L}\left(\bm A_t\,,\bm B_t\right) = -\bm J_{
\bm W_t} \bm B_t^{\!\top}, \quad 
\nabla_{\bm B}\widetilde{L}\left(\bm A_t\,,\bm B_t\right) = -\bm A_t^{\!\top} \bm J_{
\bm W_t} \,,
\end{align*}
where we define 
\begin{align*}
    \bm J_{
\bm W_t} := \frac{1}{N}\widetilde{\bm X}^{\!\top}\left[\sigma(\widetilde{\bm X}\widetilde{\bm W}^\natural) - \frac{1}{N}\widetilde{\bm X}^{\!\top}\sigma(\widetilde{\bm X}\bm W_t)\right]\odot \sigma'(\widetilde{\bm X}\bm W_t)\,.
\end{align*}

To deliver the proof, apart from the above-mentioned assumptions in \cref{sec:assumptions} for the the nonlinear setting, we also need the following additional assumptions.
\begin{assumption}\label{assum:nonlinear-orth}
  We assume that $\bm W^\natural$ has orthonormal columns and its row space is orthogonal to that of $\Delta$.
\end{assumption}

\begin{assumption}\label{assum:nonlinear-delta}
We assume that $\|\Delta\|_{op}\leq \frac{\sqrt{2}-1}{2}$ with $\operatorname{Rank}(\Delta)=r^*$ where $k+r^*\leq d$ and $r^* \ll \min\{d\,,k\}$.
\end{assumption}

\noindent
{\bf Remark:} 
\cref{assum:nonlinear-orth} ensures rich task diversity between pre-trained model and downstream tasks. We notice that such assumption is also considered in \cite{dayi2024gradientdynamicslowrankfinetuning}.  \cref{assum:nonlinear-delta} restricts the norm of downstream feature shift since the signal of adapted weight is generally smaller than the pre-trained weight. We can empirically assess the validity of this assumption in \cref{fig:SV}.


Here we can show that, for the nonlinear model, LoRA training can achieve global linear convergence under \eqref{eq:spectral-init-linear} via preconditioned GD in \cref{eq:ABiter_nonlinear}.
\begin{theorem}[Simplified version of \cref{LC}]\label{main:LC}
    Under assumptions in \cref{sec:assumptions} for the nonlinear setting, \cref{assum:nonlinear-orth}, and \ref{assum:nonlinear-delta}, with training conducted by \cref{eq:ABiter_nonlinear} and initialization via \eqref{eq:spectral-init-linear},  we take $\epsilon = \mathcal{O}\left(\frac{1}{r^*\kappa\sqrt{d}}\right)$ and $\rho\leq 0.01$ such that
    we set
    \begin{align}\label{main:selection-scale}
        \gamma\in\left[\frac{1}{c_{\rm H}}-\frac{\rho}{3c_{\rm H}\kappa\sqrt{2r^*}}\,,\frac{1}{c_{\rm H}}+\frac{\rho}{3c_{\rm H}\kappa\sqrt{2r^*}}\right]
    \end{align}
    with $c_{\rm H} :=\frac{1}{4} + \frac{1}{4\pi}\sum_{\substack{n\geq 1, \\ n \text{ odd}}} 2^{-n} n^{-2} (n!)^{-2}$.
    Then choosing $\eta \in \left(c_{\eta}\,,\frac{1}{2c_{\rm H}}\right)$ for a small constant $c_{\eta}>0$, with probability at least $1-2Cdk\operatorname{exp}\left(-\epsilon^2 N\right)$ for a universal constant $C>0$, we have
    \begin{align}\label{eq:atbtnon}
            \left\|\bm A_{t}\bm B_{t} - \Delta\right\|_{\rm F}  \leq \left(1-\frac{c_{\rm H}}{10}\eta\right)^t \rho\lambda^*_{r^*}\,, \forall t \geq 0\,.
        \end{align}
\end{theorem}

\noindent
{\bf Remark:} We make the following remarks:
\begin{itemize}
    \item This theorem is based on $\left\|\bm A_0 \bm B_0 - \Delta\right\|_{\rm F} \leq \rho\lambda^*_{r^*}$ at initialization (\cref{assum:nonlinear-delta} is not needed), see \cref{A0B0-init-risk} for details, which demonstrates the ability of one-step full gradient can improve feature learning.
    \item The final rate is independent of condition number $\kappa$ of downstream feature shift $\Delta$, which coincides with the results from linear model. This provide us evidence that adding preconditioners can also work for nonlinear model.
\end{itemize}

\noindent 
{\bf Proof Sketch:} The complete proof can be found in \cref{PGD-Nonlinear}. By Hermite decomposition, we can compute the expectation of $\bm J_{\bm W_t}$ (see \cref{expec-grad}) and decompose $\bm J_{\bm W_t}$ into $c_{\rm H}\left(\bm A_{t}\bm B_{t} - \Delta\right) + \bm \Xi_t$,
where $\bm \Xi_t$ is defined in \cref{Lip}. The first term is the signal term which can dominate the preconditioned GD dynamics. The second term $\bm \Xi_t := T1 +T2$ consists of two parts (details see \cref{err-concen-pop}): the first part $T1$ is the higher-order residual terms from $\mathbb{E}_{\widetilde{\bm x}}\left[\bm J_{\bm W_t}\right]$ which related to Hermite decomposition. Since the decay of Hermite coefficients of $\sigma$ is faster than polynomial decay, it can be well controlled. For the second term $T2$, it comes from the concentration error of $\bm J_{\bm W_t}$, which can also controlled by large sample size $N$.

To handle $\| \bm A_t \bm B_t - \Delta \|_{\rm F}$, we explore its recursion relationship in \cref{Lip}. The key part is to control $\left\|\left(\bm I_d - \bm U_{\bm A_t} \bm U_{\bm A_t}^{\!\top}\right)\Delta\left(\bm I_k - \bm V_{\bm B_t} \bm V_{\bm B_t}^{\!\top}\right)\right\|_{\rm F}$ as well as its complementary part in \cref{basis-alignment} and higher order term in \cref{err-cross}.  We deliver the complete proof to \cref{PGD-Nonlinear}. 

Note that the above two assumptions are not required if we modify the gradient update of \cref{eq:ABiter_nonlinear} by removing the mask matrix $\sigma'(\widetilde{\bm X}\bm W_t)$, a smoothing technique from \cite{kalai2009isotron,kakade2011efficient,wu2023finite}, i.e.,
\begin{align*}
    \bm J_{\bm W_t}^{\tt GLM} := \frac{1}{N}\widetilde{\bm X}^{\!\top}\bigg(\sigma\left(\widetilde{\bm X}\widetilde{\bm W}^\natural\right)-\sigma\left(\widetilde{\bm X}\bm W_t\right)\bigg)\,.
\end{align*}
In this case, we reformulate \cref{eq:ABiter_nonlinear} as
\begin{equation}\label{ABiter_GLM}
    \begin{split}
        \bm A_{t+1} & = \bm A_t + \eta \bm J_{\bm W_t}^{\tt GLM}\bm B_t^{\!\top}\left(\bm B_t\bm B_t^{\!\top}\right)^{-1}\,,\\
        \bm B_{t+1} & = \bm B_t + \eta \left(\bm A_t^{\!\top}\bm A_t\right)^{-1}\bm A_t^{\!\top}\bm J_{\bm W_t}^{\tt GLM}\,.
    \end{split}
\end{equation}
And we propose to use $\bm G^\natural := \bm J_{\bm W^\natural}^{\tt GLM}$ for \eqref{eq:spectral-init-linear} to initialize $\bm A_0$ and $\bm B_0$. The global linear convergence results are given as below.
\begin{theorem}[Simplified version of \cref{smo-LC}]\label{main:smo-LC}
    Under assumptions in \cref{sec:assumptions} for the nonlinear setting, with training conducted by \cref{ABiter_GLM} and initialization via \eqref{eq:spectral-init-linear} by taking $\bm G^\natural := \bm J_{\bm W^\natural}^{\tt GLM}$, suppose $\epsilon = \mathcal{O}\left(\frac{1}{r^* \kappa \sqrt{d}}\right)$ and $\rho\leq\frac{1}{20}$, we take 
    \begin{equation*}
       \gamma\in\left[2-\frac{2\rho}{3\kappa\sqrt{2r^*}}\,,2+\frac{2\rho}{3\kappa\sqrt{2r^*}}\right]\,,
    \end{equation*}
   and set $\eta \in \left(c^{\tt GLM}_{\eta}\,,1\right)$ where $c^{\tt GLM}_{\eta}>0$ is a small constant, then with probability at least $1-2Cdk\operatorname{exp}\left(-\epsilon^2 N\right)$ for a universal constant $C>0$, we have
    \begin{align*}
            \left\|\bm A_{t}\bm B_{t} - \Delta\right\|_{\rm F} & \leq \left(1-\frac{\eta}{4}\right)^t \rho\lambda^*_{r^*}\,.
        \end{align*}
\end{theorem}

\noindent
{\bf Remark:} Removing the mask matrix $\sigma'(\widetilde{\bm X}\bm W_t)$ in \cref{eq:ABiter_nonlinear} allows for better linear convergence performance with $(1-\eta/4)^t$ than that in \cref{eq:atbtnon}, albeit without \cref{assum:nonlinear-orth}, \ref{assum:nonlinear-delta}.\\

\noindent
{\bf Proof Sketch:} The proof strategy is similar to \cref{main:LC}. The key difference is that the corresponding $\bm \Xi_t$ under \cref{ABiter_GLM} does not have the residual terms from Hermite decomposition. We deliver the complete proof to \cref{prec-smooth-gd}.