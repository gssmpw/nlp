\section{Analysis of LoRA under Linear Model}
\label{sec:linear}

In this section, we establish the alignment between LoRA and one gradient of full fine-tuning. This result guides us to design new strategies for speeding up practical LoRA-based algorithms, which achieve this alignment at initialization.

We formally define the negative gradient of full fine-tuning in \cref{eq:fulllinear} for the linear setting after the first step as
\begin{equation}\label{eq:G}
  {\bm G}^{\natural} := -\nabla_{\bm W} {L}(\bm W^\natural) = \frac{1}{N}\widetilde{\bm X}^{\!\top} (\widetilde{\bm Y} - \widetilde{\bm X}\bm W^\natural) \,.
\end{equation} 
Note that $\widetilde{\bm X}^{\!\top}\widetilde{\bm X}$ is a non-singular square matrix by \citet[Lemma 6]{zeng2023expressive}. Since left multiplication by a non-singular square matrix does not change the rank by \citet[0.4.6 (b)]{horn2012matrix}, we have $\operatorname{Rank}({\bm G}^{\natural}) = \operatorname{Rank}(\Delta) = r^*$. Denote by $\{ \lambda_i({\bm G}^{\natural}) \}_{i=1}^{r^*}$ the singular values of $\bm G^{\natural}$ in non-increasing order.

\subsection{Alignment under LoRA Initialization}
\label{sec:align_linear}

We first present the results for the alignment of $\bm B_t$ by recalling the notations $\bm V_{r^*}(\cdot)$ and $\bm V_{r^*,\perp}(\cdot)$.
\begin{theorem}[Alignment between ${\bm G}^{\natural}$ and $\bm B_t$]
\label{thm:alignlinearB}
    Under assumptions in \cref{sec:assumptions} for the linear setting, consider the LoRA updates \eqref{eq:ABiter} with \eqref{eq:lorainit}. We have
    \begin{align*}
        \left\|\bm V^{\!\top}_{r^*,\perp}\left({\bm G}^{\natural}\right)\bm V_{r^*}\left(\bm B_t\right)\right\|_{op} = 0\,, \quad \forall t \in \mathbb{N}_+\, .
    \end{align*}
\end{theorem}
One can see that, due to the zero initialization of $\bm B_0$ in \eqref{eq:lorainit}, after the first GD step, it holds that $\bm B_1 = \eta_1 \bm A_0^{\!\top} \bm G^\natural$,  which has rank $\leq r^*$ and lies in the right top-$r^*$ singular subspace of $\bm G^\natural$. The subsequent GD dynamics of $\bm B_t$ is always restricted to this invariant subspace.

Next we build the alignment for $\bm A_t$ with the notations $\bm U_{r^*}(\cdot)$, $\bm U_{r^*,\perp}(\cdot)$ and $\kappa^{\natural}$ as the condition number of ${\bm G}^{\natural}$.
\begin{theorem}[Alignment between ${\bm G}^{\natural}$ and $\bm A_t$. Simplified version of \cref{thm:alignlinearA:full}]\label{thm:alignlinearA}
    For the $r \geq 2r^*$ case, under assumptions in \cref{sec:assumptions} for the linear setting, we consider the LoRA updates \eqref{eq:ABiter} with $[\bm A_0]_{ij} \sim \mathcal{N}(0, \alpha^2)$ in \eqref{eq:lorainit}.
    Then for any constant $\theta \in (0,1)$, by taking $\alpha = \mathcal{O}\Big(\theta^{\frac{3}{2}\kappa^\natural} d^{-\frac{3}{4}\kappa^\natural-\frac{1}{2}}\| {\bm G}^{\natural}\|^{\frac{1}{2}}_{op}\Big)$, 
and running gradient descent for $t^*$ steps with 
\begin{equation}\label{eq:t-aligned}
  t^* \lesssim \frac{\ln\left(\frac{\sqrt{d}}{\theta}\right)}{\ln\left(1+\sqrt{\eta_1 \eta_2}\lambda_{r^*}\left({\bm G}^{\natural}\right)\right)} \,,
\end{equation}
we achieve the following the alignment on the left singular subspace between $\bm G^{\natural}$ and $\bm A_{t^*}$ as below
    \begin{align}\label{eq:align-t-main}
        &\left\|\bm U^{\!\top}_{r^*,\perp}( \bm G^{\natural})~\bm U_{r^*}\left(\bm A_{t^*}\right)\right\|_{op} \lesssim \theta\,,
    \end{align}
    with probability at least $1 - C_1\exp(- d) -C_2\exp(- r) -C_3\exp(- N)$ for some constants $C_1, C_2, C_3$.
\end{theorem}
The result under the $r^* \leq r < 2r^*$ case is more complex and we defer this result to \cref{thm:alignlinearA:full}.\\


\noindent
{\bf Remark:} 
We make the following remarks:
\begin{itemize}
    \item The choice of $\alpha$ in \cref{thm:alignlinearA} shows that  after $t^* = \Theta\left(\frac{\ln d}{\lambda_{r^*}(\bm G^{\natural})}\right)$ in \cref{eq:t-aligned}, the alignment can be achieved. Our results can cover the standard He-initialization \citep{he2015delving} if $\| \bm G^{\natural}\|_{op} \geq \Omega\left(d^{\frac{3}{4}\kappa^\natural}\right)$. Requirement on $\| \bm G^{\natural}\|_{op}$ can be relaxed under smaller initialization, illustrated by \cref{fig:small-init}.
    \item Using imbalanced step-size, e.g., increasing $\eta_2$ while fixing $\eta_1$, can reduce the time steps needed for alignment. In particular, if we fix $\eta_1$, increasing the step-size ratio $\sqrt{\eta_2/\eta_1}$ in \cref{eq:t-aligned} reduces the alignment time, theoretically support for LoRA+ \citep{hayou2024lora+}.
    \item Note that we can select any pair of stepsizes $(\eta_1\,,\eta_2)$ that satisfies the conditions $t^*>1$, $\eta_2 \geq \eta_1$, and $\zeta(\eta_1, \eta_2) = \Theta(1)$.
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{arxiv_template/pdf_figs/risk-vs-angle-corrected.pdf}
    \caption{Under \eqref{eq:lorainit}, the log-risk and the angle of alignment to full one-step GD of LoRA with different $\alpha^2$ and $d$, trained via GD on task \eqref{eq:fulllinear}. \textit{Left}: the log risk under different initialization variance $\alpha^2$. The risk is defined as $\frac{1}{2}\left\|\bm A_t \bm B_t - \Delta\right\|^2_{\rm F}$. \textit{Right}: the best principal angle between the top-$r$ singular subspace of $\bm G^\natural$ and $\bm A_t$ during training. Smaller is closer. The principal angle is defined as $\min_{t}\|\bm U^{\!\top}_{r^*,\perp}( \bm G^{\natural})~\bm U_{r^*}\left(\bm A_{t}\right)\|_{op}$. More experimental details can be found in \cref{exp:toy-setting}.}
    \label{fig:small-init}\vspace{-0.2cm}
\end{figure}

The above two theorems characterize the alignment between $\bm G^{\natural}$ and $(\bm A_t, \bm B_t)$.
\cref{fig:small-init} empirically validates \cref{thm:alignlinearA} in two folds:
\begin{enumerate}[i)]
    \item Smaller initialization ($\alpha^2$ in the x-axis) encourages better alignment (evaluated by the principal angle), and then better generalization performance of fine-tuning (evaluated by the risk). But in practice, a smaller initialization would increase the training time for convergence, as a double-edge sword.
    \item Increasing $d$ leads to longer alignment time, illustrated by \cref{eq:t-aligned}, and worse alignment performance, illustrated by the formulation of $\alpha$. 
\end{enumerate}

We remark that previous work on matrix sensing \citep{stoger2021small,soltanolkotabi2023implicit} via a symmetrization technique cannot be directly applied to our setting. Such symmetrization technique prevents the alignment results decoupling into two factorized matrices. We extend their technique to decouple the alignment for $\bm A_t$ and $\bm B_t$ individually via Schur decomposition of $\bm H$.

\subsection{Spectral Initialization for Global Convergence}
\label{sec:linear-spectral}

\cref{thm:alignlinearA} has demonstrated the alignment on the rank-$r^*$ singular space of $\bm G^{\natural}$ and $(\bm A_t, \bm B_t)$.
In other words, if we take the SVD of $\bm G^{\natural}$ and choose the certain singular subspace for initialization in \eqref{eq:spectral-init-linear}, we can directly achieve the alignment at the initialization (without training) and recover $\Delta$ to some extent, which is the main target of this work.

By the following standard concentration result for (sub)-Gaussian data:
with probability at least $1-2C\exp(-\epsilon^2 N)$ for some constants $C>0$, we have
\begin{align}
\label{concentration-N-main}
    \left\|\widehat{\bm \Sigma} - \bm I_d\right\|_{op} \leq \epsilon:= \min\left\{\frac{1}{2\kappa}\,,\frac{c}{\kappa^3}\right\} \leq \frac{1}{2} \,.
\end{align}
Recall $\kappa$ is the condition number of $\Delta$ and $\lambda_{r^*}^*$ is the $r^*$-th singular value of $\Delta$, we have the following result at the spectral initialization.
\begin{proposition}
\label{main:linear-initial-risk}[One-step gradient suffices]
    Under assumptions in \cref{sec:assumptions} for the linear setting via \eqref{eq:spectral-init-linear}, taking $\epsilon$ in \cref{concentration-N-main}, then with probability at least $1-2C\exp(-\epsilon^2 N )$ for constant $C>0$, we have
    \begin{equation*}
        \| \bm A_0 \bm B_0 - \Delta \|_{op} \leq \epsilon \| \Delta \|_{op} \leq \frac{\lambda_{r^*}^*}{2} \,.
    \end{equation*}
\end{proposition}
\cref{main:linear-initial-risk} demonstrates that, after one-step full gradient, i.e., using spectral initialization \eqref{eq:spectral-init-linear}, $\bm A_0 \bm B_0$ is able to recover $\Delta$ with small error.
Besides, under \eqref{eq:spectral-init-linear}, the alignment between $\Delta$ and $\bm B_t$ in \cref{thm:alignlinearB} via \eqref{eq:lorainit} can be simplified as below.
\begin{lemma}
\label{main:linear-invariant-B2}
   Under assumptions in \cref{sec:assumptions} for the linear setting, and spectral initialization \eqref{eq:spectral-init-linear}, we always have $\bm B_t \bm V_\perp = \bm 0_{d\times (d-r^*)}$ for any $t\in\mathbb{N}^{+}$, where $\bm V_\perp$ comes from the complete SVD of $\Delta$ in \cref{Delta-SVD}.
\end{lemma}
\cref{main:linear-invariant-B2} shows that $\bm B_t$'s dynamics always stays in the low-dimensional target (right singular) subspace under the spectral initialization, which contributes to track the behavior of $\left\|\bm A_t \bm B_t - \Delta\right\|_{op}$. In this regime, there is no significant difference on setting different step-size $\eta_1$ and $\eta_2$. For ease of description, we set $\eta_1=\eta_2 := \eta$ for the later analysis.
\begin{theorem}[Global convergence. Simplified version of \cref{risk-conv-linear-vanilla-gd}]\label{thm:gc-linear-spec}
     Under assumptions in \cref{sec:assumptions} for the linear setting, suppose we use the initialization scheme \eqref{eq:spectral-init-linear}, and take $\epsilon$ in \cref{concentration-N-main} and $r\geq r^*$, $\eta = \mathcal{O}(1/\kappa\lambda_1^*)$. Then with probability at least $1-2C\exp(-\epsilon^2 N)$ for a universal constant $C>0$, we have
     \begin{align*}
    \left\|\bm A_t \bm B_t - \Delta\right\|_{\rm F} \leq \sqrt{2 r^*} \left(1 - \eta \frac{\lambda_{r^*}^*}{64 \kappa}\right)^{t}\lambda_{r^*}^*(\Delta)\,, \quad \forall t \geq 1\,.
\end{align*}

\end{theorem}
\noindent
{\bf Remark:} The above convergence rate is independent of the choice of LoRA rank $r$ if $r\geq r^*$.
It achieves an $\varepsilon$-risk in $\mathcal{O}\left(\kappa^3\ln\left(1/\varepsilon\right)\right)$ iterations.


\subsection{Preconditioned GD under Spectral Initialization}
\label{sec:scaledgd}

By \cref{thm:gc-linear-spec}, the linear convergence rate heavily depends on $\kappa$. The convergence will be slow if the downstream feature shift $\Delta$ is ill-conditioned (i.e., $\kappa$ is extremely large). This motivates us to add preconditioners, which is a key technique to accelerate convergence in matrix factorization/sensing \citep{tong2021accelerating,zhang2021preconditioned,zhang2023preconditioned,jia2024preconditioning}. We apply to analysis of LoRA as well as algorithm design. 
In the over-ranked setting ($r>r^*$), $\bm B_t \bm B_t^{\!\top}$ and $\bm A_t^{\!\top} \bm A_t$ are not necessarily invertible. Hence we add the following preconditioners to vanilla GD \eqref{eq:ABiter}
\begin{equation}\label{eq:pgd}
    \begin{split}
\bm A_{t+1} & = \bm A_t - \eta\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\left(\bm B_t\right)^{\!\top}\left(\bm B_t \bm B_t^{\!\top}\right)^\dagger\,,\\
    \bm B_{t+1} & = \bm B_t - \eta\left(\bm A_t^{\!\top} \bm A_t\right)^\dagger\bm A_t^{\!\top}\widehat{\bm \Sigma}\left(\bm A_t \bm B_t-\Delta\right)\,,
    \end{split}
\end{equation}
where $\bm M^\dagger$ denotes the pseudo-inverse of a matrix $\bm M$. Such modified preconditioners are also considered in \cite{li2024crucialroleinitializationmatrix}. Under \eqref{eq:spectral-init-linear}, similar to \cref{main:linear-invariant-B2}, the dynamics of $\bm B_t$ under precondition GD are still limited to the $r^*$-dimensional singular subspace $\bm V$ of $\Delta$, i.e., $ \bm B_t \bm V_\perp = \bm 0_{r\times(k-r^*)}$; see the proof in \cref{BV-perp} in the appendix.
We also have the following linear convergence under preconditioners.
\begin{theorem}
    \label{main:prec-gd-linear-conv}
    Under assumptions in \cref{sec:assumptions} for the linear setting, using precondition GD in \cref{eq:pgd} under spectral initialization \eqref{eq:spectral-init-linear}, we choose $\epsilon\leq\min\left\{\frac{1}{2\sqrt{r^*}\kappa}\,,\frac{1}{4}\right\}$ and set $ \eta \in \left(0, \frac{0.5-2\epsilon}{(1+\epsilon)^2}\right) $, then with probability at least $1- 2C\exp(-\epsilon^2 N)$ for a universal constant $C>0$, we have
    \begin{align*}
        \left\|\bm A_t \bm B_t - \Delta \right\|_{\rm F} & \leq \frac{1}{2}\left(1-\frac{\eta}{2}\right)^t\lambda_{r^*}^*(\Delta)\,, \quad \forall t \geq 0\,.
    \end{align*}
\end{theorem}
The convergence rate is independent of the condition number of $\kappa$. The choice of stepsize $\eta$ is upper bounded by $\frac{0.5-2\epsilon}{(1+\epsilon)^2} \in (0,0.5)$, which is a decreasing function of $\epsilon$. 
Therefore, if the condition number $\kappa$ is very large and thus $\epsilon$ is chosen as sufficiently small, then $\eta$ can reach $0.5$ and we still have a fast convergence rate independent of $\kappa$.
This is particularly useful in practical fine-tuning tasks, where the adapted matrix can be highly ill-conditioned when its rank increases. We can empirically observe the ill-conditioned issues in real-world benchmarks, as shown in \cref{fig:SV}. For the difference matrix between pre-trained weight and fine-tuned weight, the singular values decrease drastically as the index increases, indicating an ill-conditioned behavior during fine-tuning.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.37\linewidth]{arxiv_template/pdf_figs/SV_p.pdf}
    \includegraphics[width=0.37\linewidth]{arxiv_template/pdf_figs/SV_f.pdf}
    \caption{Fine-tuning T5 base model \citep{raffel2020exploring} on SST2 from GLUE dataset \citep{wang2018glue}. The experimental details can be found in \cref{SV-figs}. \textit{Left}: top-$32$ singular values for each pre-trained weight matrices $\mathbf{W}^\natural$. \textit{Right}: top-$32$ singular values for each difference matrices $\Delta \mathbf{W} = \mathbf{W}_\text{fine-tuned} - \mathbf{W}^\natural$ after full fine-tuning. The Index is ranked from the largest to the smallest singular values.}
    \label{fig:SV}
\end{figure}