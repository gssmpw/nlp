\section{Problem Settings}
\label{sec:problsemsetting}
In this section, we introduce the problem setting of fine-tuning pre-trained linear and nonlinear models with the following assumptions. 


\subsection{Basic Assumptions}
\label{sec:assumptions}

We consider both linear and nonlinear pre-trained models with multiple outputs and thus matrix parameters (instead of vectors), which is consistent with LoRA in practice.
\begin{assumption}[Pre-trained model]
\label{assum:pre-trained-model}
    For the input $\bm x \in \mathbb{R}^d$, we denote by $\bm W^\natural \in \mathbb{R}^{d \times k}$ the {\bf known} pre-trained parameter matrix. We assume that the pre-trained model can be linear or nonlinear with $\sigma(\cdot) = \max\{0, \,\cdot\,\}$ is the (entry-wise) ReLU activation function.
    \[
f_\text{pre}\left(\bm x\right) := 
\begin{cases}
( \bm x^{\!\top} \bm W^\natural)^{\!\top} \in \mathbb{R}^k & \text{linear} \\ 
\sigma [(\bm x^{\!\top} \bm W^\natural)^{\!\top} ] \in \mathbb{R}^k & \text{nonlinear}
\end{cases}\,.
\]
\end{assumption}
Note that our results can handle large dimension $d$ and $k$.
Next, we assume there exists an {\bf unknown} low-rank feature shift $\Delta$ on ${\bm W}^\natural$ that we aim to estimate.
\begin{assumption}
\label{assum:downstream-delta}
The downstream feature matrix $\widetilde{\bm W}^\natural:={\bm W}^\natural+\Delta$ admits an {\bf unknown} low-rank feature shift  $\Delta\in\mathbb{R}^{d\times k}$, where  $\operatorname{Rank}\left(\Delta\right)=r^* < \min \{d\,,k\}$.  
\end{assumption}
This assumption is widely used in the literature on LoRA analysis and matrix factorization \citep{zhang2021preconditioned,stoger2021small,soltanolkotabi2023implicit,xiong2023over}. Besides, we also assume that the data is well-behaved, e.g., (Gaussian/sub-Gaussian) concentration. 
\begin{assumption}[Downstream data for fine-tuning]\label{assum:data}
    We consider the label-noiseless setting for fine-tuning  linear and nonlinear models. Given the unknown $\widetilde{\bm W}^\natural$, the $N$ downstream data points $\{ (\widetilde{\bm x}_i,\widetilde{\bm y}_i) \}_{i=1}^N$  are i.i.d.\ and satisfy the following data generation process:
       \[
\widetilde{\bm y} := 
\begin{cases}
(\widetilde{\bm x} ^{\!\top}\widetilde{\bm W}^\natural )^{\!\top} \in \mathbb{R}^k, \quad  \{ \widetilde{\bm x}_i\}_{i=1}^N \overset{i.i.d.}{\sim} SG, & \text{linear} \\ 
\sigma[(\widetilde{\bm x} ^{\!\top}\widetilde{\bm W}^\natural )^{\!\top}] ,~ \{ \widetilde{\bm x}_i\}_{i=1}^N \overset{i.i.d.}{\sim} \mathcal{N}(\bm 0, \bm I_d) & \text{nonlinear}
\end{cases}\,,
\]
where $SG$ denotes the probability distribution for isotropic centered sub-Gaussian random vectors.
\end{assumption}
Note that the nonlinear model can be regarded as a special case of multi-index model \citep{damian2022neural,abbe2022merged,bietti2023learning} and 
Gaussian data is a common assumption in the analysis of single/multi-index models \citep{damian2022neural,lee2024neural,oko2024pretrained}.
We additionally assume that $d < N$, which coincides with practical settings of LoRA for Llama 2-7b \citep{touvron2023llama} on real-world datasets, e.g., MetaMathQA \citep{yu2023metamath} and Code-Feedback \citep{zheng2024opencodeinterpreter}, where $d=1024$ and $N$ is on the order of $10^5$.


\subsection{Full Fine-tuning and LoRA}
Our goal is to efficiently recover $\Delta$ by fine-tuning on the downstream data. 
Let the complete SVD of $\Delta \in \mathbb{R}^{d \times k}$ be
\begin{align}
\Delta = \widetilde{\bm U} \widetilde{\bm S}^* \widetilde{\bm V}^{\!\top}:=
    \begin{bmatrix}
        \bm U & \bm U_\perp
    \end{bmatrix}\begin{bmatrix}
       \bm S^* & \bm 0 \\
        \bm 0 & \bm 0
    \end{bmatrix}  \begin{bmatrix}
        \bm V^{\!\top} \\ \bm V_\perp^{\!\top}
    \end{bmatrix}\,,\label{Delta-SVD}
\end{align}
where $\widetilde{\bm U} \in \mathbb{R}^{d \times d}$ and $\widetilde{\bm V} \in \mathbb{R}^{k \times k}$ are the left and right singular matrices, and $\widetilde{\bm S}^* \in \mathbb{R}^{d \times k}$ is a rank-$r^*$ diagonal matrix with nonzero singular values $\{ \lambda^*_i \}_{i=1}^{r^*}$. It admits the compact SVD $\Delta = \bm U \bm S^* \bm V^{\!\top}$ with $\bm U \in \mathbb{R}^{d \times r^*}$, $\bm V^{\!\top} \in \mathbb{R}^{r^* \times k}$, and $\bm S^* \in \mathbb{R}^{r^* \times r^*}$.
The left/right singular subspaces spanned by $\bm U$ and $\bm V$ play an important role in our analysis.

We write the downstream data in a compact form $\widetilde{\bm X} = [\widetilde{\bm x}_1, \cdots, \widetilde{\bm x}_N]^{\!\top} \in \mathbb{R}^{N \times d}$ and the label matrix $\widetilde{\bm Y} =[
    \widetilde{\bm y}_1 \cdots \widetilde{\bm y}_N]^{\!\top} \in \mathbb{R}^{N \times k}$ is generated by either linear or nonlinear target functions in \cref{assum:data}.
We introduce the training based on full fine-tuning and LoRA below.\\

\noindent
{\bf Full Fine-tuning:}  We consider the following empirical risk minimization with a squared loss
\begin{equation}\label{eq:fulllinear}
 L(\bm W) := \frac{1}{2N}
\begin{cases}
\left\| \widetilde{\bm X} \bm W -\widetilde{\bm Y}\right\|_{\rm F}^2 & \text{linear}, \\ 
\left\| \sigma(\widetilde{\bm X} \bm W) -\widetilde{\bm Y}\right\|_{\rm F}^2 & \text{nonlinear}
\end{cases}\,,      
\end{equation}
where the parameter $\bm W$ can be learned by gradient descent (GD) initialized at $\bm W^{\natural}$, i.e., $\bm W_0 := \bm W^{\natural}$.\\

\noindent
{\bf LoRA:} LoRA updates two low-rank matrices $\bm A \in \mathbb{R}^{d\times r}$, $\bm B \in \mathbb{R}^{r\times k}$ for efficiency with the following empirical risk
\begin{equation*}\label{eq:lab_linear}
    \begin{split}
        \widetilde{L}\left(\bm A\,,\bm B\right) \!:=\! \frac{1}{2N}
\begin{cases} 
\!\left\| \widetilde{\bm X}(\bm W^{\natural} \!+\! \bm A \bm B) \!-\!\widetilde{\bm Y}\right\|_{\rm F}^2, & \!\!\!\text{linear}, \\ 
\! \left\| \sigma\! \left( \widetilde{\bm X}(\bm W^{\natural} \!+\! \bm A \bm B) \right) \!-\! \widetilde{\bm Y}\right\|_{\rm F}^2, & \!\!\! \text{nonlinear}
\end{cases}
    \end{split}
\end{equation*}
which can be minimized using GD
\begin{equation}\label{eq:ABiter}
\begin{split}
     \bm A_{t+1} & = \bm A_t - \eta_1 \nabla_{\bm A} \widetilde{\mathcal{L}}\left(\bm A_t\,,\bm B_t\right)\,, \\
     \bm B_{t+1}  & = \bm B_t - \eta_2 \nabla_{\bm B} \widetilde{\mathcal{L}}\left(\bm A_t\,,\bm B_t\right)\,,
\end{split}
\end{equation}
with stepsizes $\eta_1\,,\eta_2>0$. Notice that our results are able to handle imbalanced step-sizes, i.e., $\eta_1 \neq \eta_2$ in \cite{hayou2024lora+}.

Since the true rank $r^*$ of $\Delta$ is unknown in LoRA, our results will cover two cases: {\em over-ranked} ($r \ge r^*$) and {\em exact-ranked} ($r=r^*$).\footnote{In the matrix sensing/completion literature, they are often called {\em over-} and {\em exact-parameterized}, respectively.}
Our results allow for large $d, k$ while $r, r^* = \Theta(1)$, which coincides with common practice.\\

\noindent
{\bf Optimization and Generalization:} We are interested in the error $\left\|\bm A_t \bm B_t - \Delta\right\|^2_{\rm F}$ under the LoRA training dynamics.
Bounds on this error also imply generalization performance, because the generalization error for a new data $(\widetilde{\bm x}, \widetilde{\bm y})$ satisfies $\mathbb{E}_{\widetilde{\bm x}} \left\| \widetilde{\bm y} - \sigma(\bm W^\natural+\bm A_t\bm B_t)^{\!\top} \widetilde{\bm x} \right\|_2^2 \leq \left\|\bm A_t \bm B_t - \Delta\right\|^2_{\rm F} $ in the nonlinear setting, with equality in the linear setting.




