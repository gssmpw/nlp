\section{Algorithm and Experiments}
\label{sec:algoexp}




\newcommand{\algorithmicinitialize}{\textbf{Initialize:}}
\newcommand{\Initialize}{\item[\algorithmicinitialize]}
\newcommand{\algorithmicinputy}{\textbf{Input:}}
\newcommand{\Input}{\item[\algorithmicinputy]}
\newcommand{\algorithmictrain}{\textbf{Train:}}
\newcommand{\Train}{\item[\algorithmictrain]}
\newcommand{\algorithmicre}{\textbf{Return:}}
\newcommand{\Return}{\item[\algorithmicre]}
\begin{algorithm}[t]
\caption{LoRA-One training for a specific layer}
\label{alg:lora_one_training}
\begin{algorithmic}[1]
\Input Pre-trained weight $\bm W^\natural$, batched data $\{\mathcal{D}_t\}_{t=1}^{T}$, LoRA rank $r$, LoRA alpha $\alpha$, loss function $L$, stable parameter $s$, damping parameter $\lambda$
\Initialize
\STATE Compute $\nabla_{\bm W} L(\bm W^\natural)$
\STATE $d_{\text{in}}, d_{\text{out}} \gets \text{size}(\nabla_{\bm W} L(\bm W^\natural))$
\STATE $\gamma \gets \frac{\sqrt{d_\text{out}}}{s}$
\STATE $\bm U, \bm S, \bm V \gets \text{SVD}\left(\nabla_{\bm W} L(\bm W^\natural)\right)$
\STATE $\bm A_0 \gets \sqrt{\gamma}\cdot\bm U_{[:,1:r]}$
\STATE {\color{red}$\bm B_0 \gets \sqrt{\gamma}\cdot \bm V^{\!\top}_{[:,1:r]}$}
\STATE $\bm W^\natural \gets \bm W^\natural - \frac{\alpha}{\sqrt{r}} \bm A_0 \bm B_0$
\STATE Clear $\nabla_{\bm W} L(\bm W^\natural)$
\Train
\FOR{$t=1\,,...\,,T$}
\STATE Compute preconditioned gradients given $\mathcal{D}_t$:\\
$\bm G^{\bm A}_t \gets \nabla_{\bm A}\widetilde{L}\left(\bm A_{t-1},\bm B_{t-1}\right){\color{blue}\left(\bm B_{t-1}\bm B^{\!\top}_{t-1}+\lambda \bm I_r\right)^{-1}}$

$
\bm G^{\bm B}_t \gets {\color{blue}\left(\bm A^{\!\top}_{t-1}\bm A_{t-1}+\lambda \bm I_r\right)^{-1}}\nabla_{\bm B}\widetilde{L}\left(\bm A_{t-1},\bm B_{t-1}\right)$

\STATE Update $\bm A_t\,,\bm B_t \gets \operatorname{AdamW}\left(\bm G^{\bm A}_t\,,\bm G^{\bm B}_t\right)$
\ENDFOR
\Return $\bm W^\natural + \frac{\alpha}{\sqrt{r}} \bm A_{T} \bm B_{T}$
\end{algorithmic}
\end{algorithm}

In this section, we firstly present our  algorithm, \emph{LoRA-One},  and compare with previous gradient alignment based algorithm for fine-tuning.
Then we evaluate LoRA-One against typical LoRA based algorithms across multiple NLP benchmarks and conduct an ablation study.\medskip


\noindent
{\bf Algorithm and Discussion:}
We formally present our LoRA-One algorithm in \cref{alg:lora_one_training}, which is driven by spectral initialization \eqref{eq:spectral-init-linear} (shown in line 4-6) and the pre-condition strategy (shown in line 10-11).
To ensure numerical stability, we slightly modify \eqref{eq:spectral-init-linear} in our \cref{alg:lora_one_training} (shown in line 5-6) inspired by the trick in \cite{wang2024lora}. This is because $\widetilde{\bm S}_{\bm G^\natural}$ is highly ill-conditioned and numerically unstable, and has some difficulty for hyperparameter search in practice, see the Ablation Study part. 

We remark that our initialization strategy in \cref{alg:lora_one_training} (line 4-6) shares some similarity with gradient alignment work, e.g., LoRA-GA \citep{wang2024lora}, LoRA-pro \citep{wang2024lorapro}, but the mechanisms for gradient alignment differ significantly. 

More importantly, the spirit of LoRA-GA might not help recover $\Delta$, as verified by our theory as well as the empirical results in \cref{figs:GA-vs-Ours}, where LoRA-GA fails to generalize and remain at a high-risk level throughout training. We provide more discussion in \cref{app:disGA}. Furthermore, we notice that \cite{zhang2024riemannian} propose to add preconditioners to AdamW \citep{loshchilov2017decoupled} in the view of stability. In contrast, our focus is on addressing the potential issue of ill-conditioning in the downstream tasks, which is theoretically proven to accelerate convergence, as demonstrated in \cref{main:prec-gd-linear-conv,main:LC,main:smo-LC}.\medskip

\noindent
{\bf Experiments on NLU tasks:} We evaluate \cref{alg:lora_one_training} on multiple natural language understanding (NLU) benchmarks, e.g., MNLI, SST2, CoLA, QNLI, and MRPC via a comprehensive comparison with full fine-tuning, LoRA \citep{hu2022lora}, LoRA+ \citep{hayou2024lora+}, Preconditioned LoRA (denoted as P-LoRA) \citep{zhang2024riemannian}, LoRA-GA \citep{wang2024lora}. More experimental details can be found in \cref{app:expNLP}. 
We use these algorithms to fine-tune T5-base model \citep{raffel2020exploring}. \cref{tab:nlu-performance} demonstrates the superior performance of our theoretically grounded algorithm LoRA-One.\medskip

\begin{table*}[t]
\centering
\caption{Accuracy comparison on various NLP tasks from GLUE across several typical LoRA based algorithms. Results are reported as accuracy (\%) with standard deviations with 3 runs (best in {\bf bold}). The subscript indicates the used rank. ``-'' on MNLI indicates that the test accuracy remains zero after one-step update, see \cref{app:expNLP} for illustration.}
\label{tab:nlu-performance}
\begin{tabular}{lccccccc}
\toprule
\textbf{Dataset} & \textbf{MNLI} & \textbf{SST-2} & \textbf{CoLA} & \textbf{QNLI} & \textbf{MRPC}  \\
\textbf{Size}              & 393k          & 67k            & 8.5k          & 105k          & 3.7k                \\
\midrule
Full          & 86.33$_{\pm0.00}$ & 94.75$_{\pm0.21}$ & 80.70$_{\pm0.24}$ & 93.19$_{\pm0.22}$ & 84.56$_{\pm0.73}$ \\
\midrule
Pre-trained & - & 89.79 & 59.03 & 49.28 & 63.48 \\
One-step GD & - & 90.48 & 73.00 & 69.13 & 68.38\\
\midrule
$\text{LoRA}_{8}$         & 85.30$_{\pm0.04}$ & 94.04$_{\pm0.09}$ & 72.84$_{\pm 1.25}$ & 93.02$_{\pm0.07}$ & 68.38$_{\pm0.01}$ \\
$\text{LoRA}_{32}$        &        85.23$_{\pm 0.11}$           &   94.08$_{\pm 0.05}$       &  70.66$_{\pm 0.41}$         &    92.87$_{\pm 0.05}$       &   67.24$_{\pm 0.58}$   \\
$\text{LoRA}_{128}$        &    85.53$_{\pm 0.13}$      &     93.96$_{\pm 0.05}$        &     69.45$_{\pm 0.25}$       &     92.91$_{\pm 0.13}$        &    65.36$_{\pm 0.31}$    \\
\midrule
$\text{LoRA+}_{8}$      & 85.81$_{\pm0.09}$ & 93.85$_{\pm0.24}$ & 77.53$_{\pm0.20}$ & 93.14$_{\pm0.03}$ & 74.43$_{\pm1.39}$  \\
$\text{LoRA+}_{32}$        &   85.88$_{\pm 0.16}$      &      94.15$_{\pm 0.25}$     &      79.29$_{\pm 0.96}$      &      \textbf{93.25}$_{\pm 0.08}$     &   79.49$_{\pm 0.64}$  \\
$\text{LoRA+}_{128}$        &    86.07$_{\pm 0.15}$       &    94.08$_{\pm 0.30}$      &     78.59$_{\pm 0.73}$     &   93.06$_{\pm 0.23}$       &    78.76$_{\pm 0.12}$    \\
\midrule
$\text{P-LoRA}_{8}$        &    85.28$_{\pm 0.15}$   &      93.88$_{\pm 0.11}$      &    79.58$_{\pm 0.67}$      &     93.00$_{\pm 0.07}$     &    83.91$_{\pm 1.16}$    \\
$\text{P-LoRA}_{32}$        &    85.07$_{\pm 0.11}$   &     94.08$_{\pm 0.14}$      &    76.54$_{\pm 1.29}$       &      93.00$_{\pm 0.08}$     &   79.49$_{\pm 0.50}$   \\
$\text{P-LoRA}_{128}$        &    85.38$_{\pm 0.11}$      &   93.96$_{\pm 0.24}$       &    72.04$_{\pm 1.89}$      &    92.98$_{\pm 0.06}$       &  79.66$_{\pm 1.44}$  \\
\midrule
$\text{LoRA-GA}_{8}$        & 85.70$_{\pm0.09}$ & 94.11$_{\pm0.18}$ & 80.57$_{\pm0.20}$ & 93.18$_{\pm0.06}$ & 85.29$_{\pm0.24}$ \\
$\text{LoRA-GA}_{32}$        &    83.32$_{\pm 0.10}$       &    94.49$_{\pm 0.32}$               &     80.86$_{\pm 0.23}$     &      93.06$_{\pm 0.14}$             &  86.36$_{\pm 0.42}$       \\
$\text{LoRA-GA}_{128}$        &    84.75$_{\pm 0.06}$    &    94.19$_{\pm 0.14}$               &   80.95$_{\pm 0.35}$       &          93.12$_{\pm 0.11}$         &  85.46$_{\pm 0.23}$      \\
\midrule
$\text{LoRA-One}_{8}$ (Ours)        &     \textbf{85.81}$_{\pm 0.03}$      &   \textbf{94.69}$_{\pm 0.05}$           &    \textbf{81.08}$_{\pm 0.36}$     &    \textbf{93.22}$_{\pm 0.12}$     &     \textbf{86.77}$_{\pm 0.53}$    \\
$\text{LoRA-One}_{32}$        &     \textbf{86.08}$_{\pm 0.01}$    &    \textbf{94.73}$_{\pm 0.37}$     &   \textbf{81.34}$_{\pm 0.51}$      &    93.19$_{\pm 0.02}$     &   \textbf{87.34}$_{\pm 0.31}$  \\
$\text{LoRA-One}_{128}$        &     \textbf{86.22}$_{\pm 0.08}$    &   \textbf{94.65}$_{\pm 0.19}$        &     \textbf{81.53}$_{\pm 0.36}$     &   \textbf{93.34}$_{\pm 0.11}$    &   \textbf{88.40}$_{\pm 0.70}$ \\
\bottomrule
\end{tabular}
\end{table*}

%\begin{wraptable}{r}{10cm}
\begin{table}[t]
%\vspace{-0.45cm}
\centering
\caption{Performance comparison across different methods on GSM8K, and Human-eval benchmarks. Results are reported as accuracy (\%) with standard deviations with 3 runs (higher is better). The subscript indicates the rank of LoRA.}
\label{tab:performance}
\begin{tabular}{lcc}
\toprule
\textbf{}& \textbf{GSM8K} & \textbf{Human-eval} \\
\midrule
Full & {59.36}$_{\pm 0.85}$ & {35.31}$_{\pm 2.13}$ \\
\midrule
$\text{LoRA}_{8}$ & {46.89}$_{\pm 0.05}$ & {15.67}$_{\pm 0.60}$ \\
\midrule
$\text{LoRA-GA}_{8}$ & {53.60}$_{\pm 0.13}$ & {20.45}$_{\pm 0.92}$ \\
$\text{LoRA-GA}_{32}$  & {55.12}$_{\pm 0.30}$ & {20.18}$_{\pm 0.19}$ \\
$\text{LoRA-GA}_{128}$ & {55.07}$_{\pm 0.18}$ & {23.05}$_{\pm 0.37}$\\
\midrule
$\text{LoRA-One}_{8}$ & \textbf{53.80}$_{\pm 0.44}$ & \textbf{21.02}$_{\pm 0.01}$ \\
$\text{LoRA-One}_{32}$ & \textbf{56.61}$_{\pm 0.29}$ & \textbf{23.86}$_{\pm 0.01}$ \\
$\text{LoRA-One}_{128}$ & \textbf{58.10}$_{\pm 0.10}$ & \textbf{26.79}$_{\pm 0.21}$ \\
\bottomrule
\end{tabular}
\end{table}
%\end{wraptable}

\noindent
{\bf Experiments on LLM:} Apart from NLU tasks, we also fine-tune the Llama 2-7B \citep{touvron2023llama} on two tasks: math and code generation, to evaluate the performance of \cref{alg:lora_one_training}. The training and evaluation details are as follows: 1) \textit{Math task}: we fine-tune the model using a 100k subset from MetaMathQA \citep{yu2023metamath} and test on GSM8K \citep{cobbe2021training} evaluation dataset. We take accuracy as the performance metric. 2) \textit{Code Task}: we fine-tune the model using a 100k subset from Code-Feedback \citep{zheng2024opencodeinterpreter} and evaluate on HumanEval \citep{chen2021evaluating}. We take the PASS@1 metric. More experimental details can be found in \cref{sec:exp:llm}.
\cref{tab:performance} shows that our method LoRA-One can achieve the best performance over both datasets. For rank $8$, we achieve significant improvements over vanilla LoRA, with a margin of $6.91$ on GSM8K and $5.35$ on Humaneval. LoRA-One outperforms LoRA-GA over all three ranks with notable improvements, which demonstrates the power of using  top-$r^*$ singular subspace as suggested by our theory and the good scalability in high ranks from preconditioning.\medskip

\noindent
{\bf Ablation study:} We perform two types of ablation study.
First, in \cref{tab:nlu-performance}, comparing ``One-step GD'' and ``Pre-trained'', we see that one-step full gradient descent significantly improves on pre-training and even performs better than LoRA on CoLA and MRPC. This supports our claim on one-step full gradient.

Second, \cref{fig:enter-label} compares the accuracy of LoRA-One and LoRA-GA, with and without preconditioners. 
Our choice of top-$r^*$ singular subspace can be seen to be empirically better than LoRA-GA's choice.  Moreover, LoRA-One and LoRA-One (-) exhibit comparable performance when the rank is  \( r = 8 \) or \( r = 32 \). Notably, LoRA-One (-) surpasses LoRA-One in the MRPC task at \( r = 8 \). These findings suggest that incorporating preconditioners may not be necessary for lower-rank settings. In contrast, for larger ranks (\( r = 128 \)), LoRA-One consistently outperforms LoRA-One (-). Therefore, we recommend using LoRA-One (-) for small-rank cases and LoRA-One for high-rank cases.

Moreover, from \cref{main:tableablation}, the performance of LoRA-One (-) is close to Spectral (-), even better for MRPC dataset. This demonstrates the validity of using lines 4--6 in \cref{alg:lora_one_training} for initialization instead of using $\bm G^\natural$ directly for a practical consideration. More details are provided in \cref{appx:ablation}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{arxiv_template/pdf_figs/ablation_v9.pdf}
    \caption{Ablation study of LoRA-One (\cref{alg:lora_one_training}), LoRA-One (-) (without preconditioners), LoRA-GA (-) (original LoRA-GA from \citealt{wang2024lora}), and LoRA-GA (+) (with preconditioners) on CoLA and MRPC from GLUE \citep{wang2018glue} under ranks $r=8\,,32\,,128$.}
    \label{fig:enter-label}
\end{figure}


\begin{table}[!htb]
    \setlength{\tabcolsep}{4pt}
    \caption{Accuracy comparison across different methods on MRPC and CoLA from GLUE \citep{wang2018glue} under ranks $r=8\,,32\,,128$. LoRA-One (-) stands for training with AdamW without preconditioning under initialization by lines 1--8 in \cref{alg:lora_one_training}.}
    \label{main:tableablation}
    \begin{minipage}{.5\linewidth}
      \captionsetup{labelformat=empty}
      \caption{CoLA}
      \centering
      \begin{tabular}{cccc}
\toprule
$r$ & LoRA-One & LoRA-One (-) & Spectral (-)\\
\midrule
8 & {81.08}$_{\pm 0.36}$ & 80.83$_{\pm 0.54}$ &\textbf{81.40}$_{\pm 0.31}$\\
32 & \textbf{81.34}$_{\pm 0.51}$ & 81.30$_{\pm 0.16}$ &81.18$_{\pm 0.30}$\\
128 & {81.53}$_{\pm 0.36}$ & 81.34$_{\pm 0.12}$ &\textbf{81.62}$_{\pm 0.48}$\\
\bottomrule
\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
      \captionsetup{labelformat=empty}
        \caption{MRPC}
        \begin{tabular}{cccc}
\toprule
$r$ & LoRA-One & LoRA-One (-) & Spectral (-)\\
\midrule
8 & 86.77$_{\pm 0.53}$ & \textbf{87.50}$_{\pm 0.60}$ &86.19$_{\pm 0.42}$\\
32 & \textbf{87.34}$_{\pm 0.31}$ & 87.34$_{\pm 0.42}$ &86.02$_{\pm 0.20}$\\
128 & \textbf{88.40}$_{\pm 0.70}$ & 87.26$_{\pm 0.20}$ &86.03$_{\pm 0.20}$\\
\bottomrule
\end{tabular}
    \end{minipage} 
\end{table}