\documentclass[11pt]{article}
\pdfoutput=1
\usepackage{microtype}
\usepackage{graphicx,wrapfig,lipsum}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[shortlabels]{enumitem}
\usepackage{siunitx}  
\usepackage{hyperref}
\hypersetup{colorlinks, linkcolor=red, anchorcolor=blue, citecolor=blue}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\renewcommand{\baselinestretch}{1.05}
\usepackage{mylatexstyle}
\usepackage{setspace}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb,bm}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption,color}
\usepackage{indentfirst,threeparttable,multirow}
\usepackage{amsfonts,tcolorbox}
\usepackage{float,url}
\usepackage{bbm,cancel}
\usepackage{lipsum}
\usepackage{nicefrac} 


\newcommand{\disableaddcontentsline}{%
  \let\savedaddcontentsline\addcontentsline 
  \renewcommand{\addcontentsline}[3]{}
}
\newcommand{\enableaddcontentsline}{%
  \let\addcontentsline\savedaddcontentsline
}

\usepackage[textsize=tiny]{todonotes}
\definecolor{myblue}{RGB}{0 114 199}
\definecolor{mylightblue}{RGB}{77 191 241}
\definecolor{darkgray}{HTML}{878787}
\definecolor{myorange}{RGB}{217 83 25}
\newtcolorbox{myorangebox}{colframe = myorange}
\newtcolorbox{mygraybox}{colframe = gray}
\newtcolorbox{mybluebox}{colframe = myblue}

\usepackage{xcolor}
\definecolor{fhcolor}{rgb}{0.523, 0.235, 0.625}
\newcommand{\fh}[1]{\textcolor{fhcolor}{(SgrA: #1)}}
\newcommand{\yccomment}[1]{\textcolor{red}{(Yudong: #1)}}
\let\yc\yccomment
\let\yudong\yccomment
\newcommand{\yh}[1]{\textcolor{blue}{(Yuanhe: #1)}}

\title{\huge One-step full gradient suffices for low-rank fine-tuning, provably and efficiently}

\author
{
     Yuanhe Zhang\thanks{Department of Statistics, University of Warwick, United Kingdom; Email: {\tt yuanhe.zhang@warwick.ac.uk}} 
     ~~~
     Fanghui Liu\thanks{Department of Computer Science, also Centre for Discrete Mathematics and its Applications (DIMAP), University of Warwick, United Kingdom; Email: {\tt fanghui.liu@warwick.ac.uk} (Corresponding author)} 
     ~~~
     Yudong Chen\thanks{Department of Computer Sciences, University of Wisconsin-Madison, USA; e-mail: {\tt yudong.chen@wisc.edu}}
}


\begin{document}
\disableaddcontentsline
\maketitle

\begin{abstract}
This paper studies how to improve the performance of Low-Rank Adaption (LoRA) \citep{hu2022lora} as guided by our theoretical analysis. 
Our first set of theoretical results show that for random initialization and linear models, \textit{i)} LoRA will align to the certain singular subspace of one-step gradient of full fine-tuning; \textit{ii)} preconditioners improve convergence in the high-rank case.
These insights motivate us to focus on preconditioned LoRA using a specific spectral initialization strategy for aligning with certain subspaces.
For both linear and nonlinear models, we prove that alignment and generalization guarantees can be directly achieved at initialization, and the subsequent linear convergence can be also built.
Our analysis leads to the \emph{LoRA-One} algorithm (using \emph{One}-step gradient and preconditioning), a theoretically grounded algorithm that achieves significant empirical improvement over vanilla LoRA and its variants on several benchmarks.
Our theoretical analysis, based on decoupling the learning dynamics and characterizing how spectral initialization contributes to feature learning, may be of independent interest for understanding matrix sensing and deep learning theory.
The source code can be found in the \url{https://github.com/YuanheZ/LoRA-One}.

\end{abstract}

\input{arxiv_template/main/intro}

\input{arxiv_template/main/related_works}

\input{arxiv_template/main/setting}

\input{arxiv_template/main/Linear}

\input{arxiv_template/main/nonlinear}

\input{arxiv_template/main/exp}

%\vspace{-0.2cm}
\section{Conclusion}
%\vspace{-0.1cm}
This paper theoretically demonstrates how LoRA can be improved from our theoretical analysis in both linear and nonlinear models: the alignment between LoRA's gradient update $(\bm A_t, \bm B_t)$ and the singular subspace of $\bm G^{\natural}$, and adding preconditioners.
Our theory clarifies some potential issues behind gradient alignment work and the theory-grounded algorithm, LoRA-One, obtains promising performance in practical fine-tuning benchmarks.

\section*{Acknowledgment}
F. Liu was supported by Royal Soceity KTP R1 241011 Kan Tong Po Visiting Fellowships. Y. Chen was supported in part by National Science Foundation grants CCF-2233152.

\bibliography{sample}
\bibliographystyle{ims}

\newpage
\appendix
\onecolumn
\enableaddcontentsline
\tableofcontents
\newpage
\section{Symbols and Notations}
\label{app:notation}
\begin{table}[h!]
\fontsize{8}{10}\selectfont
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|l}
\hline
\textbf{Symbol} & \textbf{Dimension(s)} & \textbf{Definition} \\
\hline
$\mathcal{N}(\bm \mu, \bm \sigma)$ & - & Multivariate normal distribution with mean vector $\bm \mu$ and covariance matrix $\bm \sigma$ \\
$\mathcal{O}, o, \Omega, \Theta$ & - & Bachmannâ€“Landau asymptotic notation \\
$\|\bm w\|_2$ & - & Euclidean norm of vector $\bm w$ \\
$\|\mathbf{M}\|_{op}$ & - & Operator norm of matrix $\mathbf{M}$ \\
$\|\mathbf{M}\|_{\rm F}$ & - & Frobenius norm of matrix $\mathbf{M}$ \\
$\langle \bm u, \bm v \rangle$ & - & Dot product of vectors $\bm u$ and $\bm v$ \\
$\mathbf{M}\odot \mathbf{N}$ & - & Hadamard product of matrix $\mathbf{M}$ and $\mathbf{N}$\\
\hline
$\bm W^\natural$ & $\mathbb{R}^{d\times k}$ & Pre-trained weight matrix\\
$\Delta$ & $\mathbb{R}^{d\times k}$ & Downstream feature shift matrix\\
$\widetilde{\bm W}^\natural$ & $\mathbb{R}^{d\times k}$ & Downstream weight matrix $\widetilde{\bm W}^\natural=\bm W^\natural+\Delta$\\
$\bm G^\natural$ & $\mathbb{R}^{d\times k}$ & The initial gradient matrix under full fine-tuning\\
$\bm A_t\,,\bm B_t$ & $\mathbb{R}^{d\times r}\,,\mathbb{R}^{r\times k}$ & Learnable low-rank adapters at step $t$\\
$\bm w^\natural_i$ & $\mathbb{R}^d$ & $i^\text{th}$ column of pre-trained weight matrix $\bm W^\natural$ \\
$\widetilde{\bm w}^\natural_i$ & $\mathbb{R}^d$ & $i^\text{th}$ column of downstream weight matrix $\widetilde{\bm W}^\natural$ \\
$\bm w_{t,i}$ & $\mathbb{R}^d$ & $i^\text{th}$ column of adapted weight matrix $\left(\bm W^\natural+\bm A_t \bm B_t\right)$ at step $t$ \\
$\Delta_{i}$ & $\mathbb{R}^d$ & $i^\text{th}$ column of downstream feature matrix $\Delta$\\
$[\bm A_t \bm B_t]_i$ & $\mathbb{R}^d$ & $i^\text{th}$ column of the product of adapters $\bm A_t \bm B_t$\\
$\widetilde{\bm X}$ & $\mathbb{R}^{N\times d}$ & Downstream data matrix\\
$\widetilde{\bm Y}$ & $\mathbb{R}^{N\times d}$ & Downstream label matrix\\
$\widetilde{\bm x}_n$ & $\mathbb{R}^d$ & $n^\text{th}$ downstream data point\\
\hline
$\mathbf{M}^{-1}$ & - & Inverse of matrix $\mathbf{M}$ \\
$\mathbf{M}^\dagger$ & - & Pseudo-inverse of matrix $\mathbf{M}$ \\
$\lambda_i\left(\mathbf{M}\right)$ & $\mathbb{R}$ & $i^\text{th}$ singular value of matrix $\mathbf{M}$ \\
$\lambda_i^*$ & $\mathbb{R}$ & $i^\text{th}$ singular value of downstream feature shift matrix $\Delta$ \\
$\kappa\left(\mathbf{M}\right)$ & $\mathbb{R}$ & The condition number of matrix $\mathbf{M}$ \\
$\kappa$ & $\mathbb{R}$ & The condition number of $\Delta$: $\kappa=\lambda_{\max}^*/\lambda^*_{\min}$ \\
$\kappa^{\natural}$ & $\mathbb{R}$ & The condition number of $\mathbf{G}^\natural$: $ \kappa^{\natural} =\lambda_{\max}\left(\mathbf{G}^\natural\right)/\lambda_{\min}\left(\mathbf{G}^\natural\right)$ \\
$\bm U_{m}\left(\mathbf{M}\right)$ & - & The left singular subspace spanned by the $m$ largest singular values of the input matrix $\mathbf{M}$\\
$\bm U_{m,\perp}\left(\mathbf{M}\right)$ & - & The left singular subspace orthogonal to $\bm U_{m}\left(\mathbf{M}\right)$\\
$\bm V_{m}\left(\mathbf{M}\right)$ & - & The right singular subspace spanned by the $m$ largest singular values of the input matrix $\mathbf{M}$\\
$\bm V_{m,\perp}\left(\mathbf{M}\right)$ & - & The right singular subspace orthogonal to $\bm V_{m}\left(\mathbf{M}\right)$\\
$\bm U_{\bm A}$ & - & The left singular matrix of the compact SVD of $\bm A$\\
$\bm U_{\bm A, \perp}$ & - & The corresponding orthogonal complement of $\bm U_{\bm A}$\\
$\bm V_{\bm A}$ & - & The right singular matrix of the compact SVD of $\bm A$\\
$\bm V_{\bm A, \perp}$ & - & The corresponding orthogonal complement of $\bm V_{\bm A}$\\
\hline
$\sigma(\,\cdot\,)$ & - & ReLU activation function \\
$\sigma'(\,\cdot\,)$ & - & The derivative of ReLU activation function \\
$c_j$ & $\mathbb{R}$ & $j^\text{th}$ Hermite coefficient of ReLU activation function \\
$\operatorname{He}_j(\,\cdot\,)$ & - & $j^\text{th}$ Hermite polynomial\\
\hline
$\nabla_{\mathbf{W}}f\left(\mathbf{W}\right)$ & - & The gradient matrix of function $f$ w.r.t. input matrix $\mathbf{W}$\\
$\widetilde{L}\left(\bm A\,,\bm B\right)$ & - & Loss function under LoRA fine-tuning\\
$L(\bm W)$ & - & Loss function under full fine-tuning \\
\hline
$N$ & - & Number of downstream data \\
$d$ & - & Input dimension of the data \\
$k$ & - & Output dimension of the label \\
$\eta\,,\eta_1\,,\eta_2$ & - & Learning rates \\
$\alpha$ & - & Random initialization scale of low-rank adapter $\bm A_0$ \\
\hline
\end{tabular}
\caption{Essential symbols and notations in this paper.}
\label{tab:notation}
\end{table}

\input{arxiv_template/appendix/linear}

\input{arxiv_template/appendix/nonlinear}

\input{arxiv_template/appendix/auxiliary}

\input{arxiv_template/main/discussionGA}

\input{arxiv_template/appendix/Experimental_Settings}
\end{document}

