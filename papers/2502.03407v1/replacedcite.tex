\section{Related Work}
\label{related-work}

\paragraph{Deception in language models.}
Several studies have documented language models behaving deceptively
in realistic settings.
____ find that, when put under pressure, an LLM acting as a trading assistant will sometimes conceal its use of insider information to make trades.
____ find that when audited, an LLM AI company assistant will frequently deny policy violations which occurred earlier in the transcript.
____ find that (strongly prompted) models will attempt to disable oversight mechanisms and then deny having done so.
____ find that Claude 3 Opus will \enquote{alignment fake}, professing different beliefs to avoid reinforcement learning updating its behaviour in a way that conflicts with its current values.

\paragraph{Model organisms of deception.}
Researchers have also deliberately trained deceptive models.
____ train \enquote{sleeper agents} that act misaligned in response to a trigger.
____ show simple probes are sufficient to detect misaligned outputs from these sleeper agents. This may be because the model organism is too simplistic, however, as ____ find these probes ineffective for a more realistic model organism created by training with an imperfect reward signal.

\paragraph{White-box detectors for falsehoods and deception.} 
Much prior work has focused on detecting factual falsehoods and common misconceptions on TruthfulQA ____.
____ use an unsupervised method that finds a direction that satisfies logical consistency with negated statements. ____ train a multi layer network to separate truths from falsehoods.
____, however, discuss generalization failures of these methods.
____ find simple difference-in-means probes work well, which ____ extends with a variety of different probing techniques and datasets.

More similar to our goals, others have tried to monitor for strategic deception.
____ train a \emph{black-box} truthfulness classifier using the model's
response to multiple choice follow-up questions.
They evaluate the classifier on goal directed lies, sycophantic lies, and models that have been fine-tuned to
conditionally lie.
____ present a few examples of using a probe to detect strategic deception, but only
systematically evaluate the probe on TruthfulQA.
We use their training dataset (although with a different probe fitting method) for one of the primary probes we evaluate.
We are unaware of any prior systematic evaluation of white-box probes for detecting strategic deception.

\paragraph{Probing for other safety relevant qualities.} ____ trains probes which detect when the model outputs advice for
committing theft. Their motivation is similar to ours (using probes to monitor for catastrophic actions).
They train their probes on off-policy completions written by a different model than the one being probed.
The target behaviour (advice for theft) is also narrower than ours (strategic deception).


____ train probes to detect harmful outputs.
Their probes perform well against \enquote{jailbreak} prompts that bypass the model's safety training but fail against \enquote{obfuscated} prompts that adversarially attack the probes themselves.

\paragraph{SAEs for building classifiers.} ____ train probes on Sparse Autoencoder (SAE) activations.
They find probing the raw activations generally performs better, although SAE-based probes are more interpretable and can be competitive if the SAE is trained on domain-relevant data.
____ find similar results, showing that probes using SAE latents are usually worse but competitive in some scenarios such as low amounts of data or corrupted labels.
____ use manual selection of SAE features to control probe generalization.