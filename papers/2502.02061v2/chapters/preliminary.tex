\section{TASK FORMULATION}

\noindent$\bullet$ \textbf{Deliberative Recommendation.} 
We formulate the task of deliberative recommendation, which performs thoughtful reasoning regarding user preference and item features before predicting user feedback as illustrated in Figure~\ref{fig:teaser}. 
% one paragraph introducing the main paradigm: input, output, difference with traditional recommenders. 
To guide the reasoning process, we propose incorporating verbalized user feedback for LLM optimization. 
In this work, we adopt user reviews due to fine-grained user preferences described in reviews~\cite{ReviewRecSurvey}. 
Besides, we investigate deliberative recommendation on the classic rating prediction task, following prior work~\cite{ACL_findings,expert}. 



% We first formulate the task of RecLLMs with explicit reasoning. 
% Following prior work~\cite{ACL_findings,expert}, we focus on the user rating prediction task. 
% Following prior work~\cite{ACL_findings,expert}, we investigate explicit reasoning for RecLLMs on the user rating prediction task. 
Formally, let $\mathcal{U}$ denote a set of users and $\mathcal{I}$ a set of items. 
% The historical interactions of users are represented as $(r_{ui}, c_{ui})$, where $r_{ui}$ is the rating given by user $u$ for item $i$, and $c_{ui}$ is the corresponding review. 
The interaction between user $u$ and item $i$ can be represented as $(r_{ui}, c_{ui})$, where $r_{ui}$ is the rating of user $u$ for item $i$, and $c_{ui}$ is the corresponding review, which exists in many datasets~\cite{movieLen,KaggleMovie}. 
% For an unseen user-item pair $(u, i)$, the historical interactions of the user and item are defined as:  
For a user-item pair $(u, i)$, the historical interactions of user $u$ and item $i$ are defined as:  
% \[
% H_u = \{(i_k, r_{ui_k}, c_{ui_k}) \mid i_k \in \mathcal{I}\}, \quad H_i = \{(u_j, r_{u_ji}, c_{u_ji}) \mid u_j \in \mathcal{U}\},
% \]  
\[
\mathcal{H}_u = \{(i, r_{ui}, c_{ui}) \mid i \in \mathcal{I}_u\}, \quad \mathcal{H}_i = \{(u, r_{ui}, c_{ui}) \mid u \in \mathcal{U}_i\},
\]  
% where $i_k$ represents items previously rated by user $u$, and $u_j$ represents users who rated item $i$. Given $H_u$ and $H_i$, the LLMs is tasked to predict the rating $\hat{r}_{ui}$ after performing explicit reasoning:
where $\mathcal{I}_u$ represents items previously rated by user $u$, and $\mathcal{U}_i$ denotes users who rated item $i$. 
Given $\mathcal{H}_u$ and $\mathcal{H}_i$, the LLM is tasked to predict the rating $\hat{r}_{ui}$ after performing explicit reasoning:
\[
\text{reason}_{ui}, \hat{r}_{ui} = \text{LLM}(\mathcal{H}_u, \mathcal{H}_i),
\]
where $\text{reason}_{ui}$ denotes the reasoning process and $\hat{r}_{ui}$ is the predicted rating. 

Considering the reasoning process is highly personalized and diverse across users, we decompose the complex reasoning into multiple steps, as shown in Figure~\ref{fig:framework}, to achieve different objectives: Preference Distillation, Preference Matching, and Feedback Prediction. 
% Beyond prediction accuracy, the task emphasizes alignment between the generated reasoning and the user's true preferences.
% Beyond prediction accuracy, the task emphasizes alignment between the generated reasoning process and the user's true preferences.
% To achieve this, we incorporate the user's review of the target item, referred to as the target item review $c_{ui}$, as additional supervision during training. However, it is excluded during testing to prevent data leakage.
To emphasize the alignment between the reasoning process and the user's true preference, we incorporate the target item review $c_{ui}$ of user $u$ for item $i$ as additional supervision during training. 
Notably, it is excluded in testing to prevent data leakage. 


\vspace{3pt}
\noindent$\bullet\quad$\textbf{Justification for explicit reasoning in RecLLMs.} 
% Incorporating explicit reasoning into RecLLMs holds significant potential, despite its potential impact on prediction time and cost. 
Incorporating explicit reasoning into RecLLMs holds significant potential, despite it potentially increases the prediction time and computational cost. 
% Specifically, it enables RecL'L'M to: 1) Fully leverage the reasoning capabilities of LLMs to improve prediction accuracy and reliability; 
Specifically, it enables RecLLMs to: 1) fully leverage the reasoning capabilities of LLMs to improve prediction accuracy and reliability; 
2) enhance interpretability through step-by-step reasoning as explanations, thereby enhancing user trust; 
% 3) possibly facilitate human-AI collaboration~\cite{yang-etal-2024-human}, making recommendations more flexible and controllable. 
3) possibly facilitate user-controllable recommendation for human-AI collaboration~\cite{yang-etal-2024-human}, since users can adjust the reasoning process to generate new recommendations. 
% Nota that these benefits make explicit reasoning especially promising for RecLLMs in applications where accuracy and interpretability are prioritized over timeliness, such as medication recommendation~\cite{drugRec}. 
% Note that explicit reasoning is more useful for the scenarios where accuracy is prioritized over timeliness, such as E-commerce product, movie, medication recommendation~\cite{drugRec}.
Notably, explicit reasoning is particularly beneficial in scenarios where real-time recommendation requirements are less critical, but accuracy is paramount, such as E-commerce product, movie, and medication recommendations~\cite{drugRec}. 
These items typically involve long usage periods and require careful deliberation before purchase or viewing. 
Furthermore, as LLMs efficiency continually improves~\cite{data_efficient}, the computational costs will decrease accordingly, enhancing the feasibility of wide applications. 
% , expanding its feasibility across a broader range of application scenarios.