\section{EXPERIMENT}

% Our research entails extensive experiments conducted on three real-world datasets to address the following research questions:
In this section, we conduct a series of experiments to answer the following research questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1:} 
    How does Reason4Rec perform compared to other baseline methods in terms of accuracy in predicting user feedback (i.e., ratings) and the quality of generated reasons?
    % How does Reason4Rec perform compared to other baseline methods, in terms of the accuracy of user feedback (i.e., rating) prediction and the quality of generated reasons?
    
    \item \textbf{RQ2:} Why is the multi-step reasoning strategy in Reason4Rec essential, and how does it compare in effectiveness to one-step reasoning strategies?
    
    \item \textbf{RQ3:} 
    How effective is our approach (the reason generation method and proposed reward model) in incorporating user-verbalized preference feedback?
\end{itemize}



\subsection{Experimental Settings}
\subsubsection{\textbf{Datasets.}}
We conduct experiments on three widely-used real-world datasets:
\begin{itemize}[leftmargin=*]
\item \textbf{Amazon Music (Music)}: This refers to the ``Digital Music'' subset of the well-known Amazon Product dataset\footnote{\url{https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html}.}, which records rich user reviews, ratings, and textual information about items, such as titles, across a broad range of product categories, on the Amazon platform. We refer to this dataset as ``Music'' for short.

\item \textbf{Amazon Book (Book)}: This refers to the ``Book'' subset of the Amazon Product dataset, shortened ``Book''.

\item \textbf{Yelp:}
This refers to the Yelp Open dataset\footnote{\url{https://business.yelp.com/data/resources/open-dataset/}.}, which includes user reviews, ratings for businesses such as restaurants and retail shops, as well as textual information about the businesses. It is widely used in recommendation tasks~\cite{rating_predict1}.
\end{itemize}



% We conduct experiments on three widely-used datasets from the following two sources:

% \begin{itemize}[leftmargin=*]
% \item \textbf{Amazon Product:}
% The Amazon Product dataset\footnote{\url{https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html}.} contains user reviews, ratings, and textual information about items, such as titles, across a wide range of product categories. This dataset has been widely adopted in prior work~\cite{deepconn}. For our experiments, we focus on two specific domains: "Digital Music" and "Books," which we refer to as "Music" and "Book" for simplicity.

% \item \textbf{Yelp:}
% The Yelp dataset\footnote{\url{https://business.yelp.com/data/resources/open-dataset/}.} includes user reviews, ratings, and textual information about various businesses, such as restaurants and retail shops. This dataset is also widely used in recommendation tasks~\cite{rating_predict1}.
% \end{itemize}

We use the entire Music dataset for experiments, while for the Book and Yelp datasets, we utilize only a subset due to their large size. For the Book dataset, we use data from the last two months, and for the Yelp dataset, we use data from the last six months.
For each dataset, we split it into training, validation, and test sets based on the timestamps of interactions, ensuring that test interactions occur after all training and validation interactions to prevent information leakage~\cite{DBLP:journals/tois/JiS0L23}.
Regarding data filtering, following prior work~\cite{liu2019nrpa}, we adopt a 5-core setting to filter the data and exclude cold-start users and items—those not appearing in the training set—from the validation and test sets. The statistical details of the processed dataset are provided in Table~\ref{tab:dataset_stats}.


% For Music, we use the entire dataset in our experiments. For Book and Yelp, due to their large size, we limit the data to the last two months for Book and the last six months for Yelp. For all three datasets, we split them into training, validation, and test sets based on interaction timestamps to prevent data leakage~\cite{DBLP:journals/tois/JiS0L23}. Following prior work~\cite{liu2019nrpa}, we adopt a 5-core setting and exclude cold-start items—those not appearing in the training set—from the validation and test sets. The statistical information of the processed dataset is available in Table~\ref{tab:dataset_stats}.

\begin{table}[t]
\centering
\caption{ Statistical details of the evaluation datasets.}
\label{tab:dataset_stats}
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset} & \textbf{\#Train} & \textbf{\#Valid} & \textbf{\#Test} & \textbf{\#User} & \textbf{\#Item} \\ \midrule
Music            & 43,071           & 3,271            & 1,296           & 4,183           & 2,660           \\
Book             & 71,972           & 6,144            & 5,541           & 13,863          & 13,515          \\
Yelp             & 51,497           & 4,757            & 4,328           & 8,453           & 13,426          \\ \bottomrule
\end{tabular}
\end{table}


\subsubsection{\textbf{Evaluation Metrics.}} 
% For rating prediction, we use the widely adopted Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to evaluate prediction accuracy, as these are standard metrics for the task~\cite{expert,chatgpt_CIKM}.
To evaluate the accuracy of rating prediction, we use two standard metrics~\cite{expert,chatgpt_CIKM}: Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).



To evaluate reasoning quality, we employ BLEURT~\cite{Bleurt} and GPTScore~\cite{GPTScore} to measure the semantic alignment between the generated reasoning and the target item review. BLEURT\footnote{\url{https://github.com/google-research/bleurt}.} is an evaluation metric that leverages contextual embeddings to assess the semantic similarity between text pairs, providing a fine-grained measure of alignment. 
GPTScore, computed based on GPT-4o\footnote{\url{https://chatgpt.com/?model=gpt-4o}.}, analyzes the semantic alignment of the given text pairs and assigns a score between 0 and 100, with higher scores indicating better alignment. These metrics are commonly adopted in explainable recommendation tasks~\cite{XRec}.

% To evaluate reasoning quality, we employ BLEURT~\cite{Bleurt} and GPTScore~\cite{GPTScore} to measure the semantic alignment between the generated reasoning and the target item review. BLEURT\footnote{\url{https://github.com/google-research/bleurt}.} is a pretrained evaluation metric that leverages contextual embeddings to assess the semantic similarity between text pairs, providing a fine-grained measure of alignment. GPTScore, based on GPT-4o\footnote{\url{https://chatgpt.com/?model=gpt-4o}.}, analyzes the semantic alignment of the given text pairs and assigns a score between 0 and 100, with higher scores indicating better alignment. These metrics are commonly adopted in explainable recommendation tasks~\cite{XRec}.



% \begin{table}[t]
% \setlength\abovecaptionskip{-0.3px}
% \setlength\belowcaptionskip{-0.3px}
% \setlength{\tabcolsep}{0.15mm}
% \small
% \centering
% \caption{
% Comparison of user feedback prediction performance between our Reason4Rec method and the baselines across the three evaluation datasets. 
% The best results are highlighted in bold and sub-optimal results are underlined.
% }
% \begin{tabular}{llcccccc}
% \toprule
% \multirow{2}{*}{\bf Type} & \multirow{2}{*}{\textbf{Method}} 
% & \multicolumn{2}{c}{\textbf{Music}} 
% & \multicolumn{2}{c}{\textbf{Book}} 
% & \multicolumn{2}{c}{\textbf{Yelp}} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
% & & \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$
% & \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$
% & \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$  \\
% \midrule
% \multirow{1}{*}{CF-based} 
% & MF 
% & 0.6188 & 0.8142 
% & 0.6277 & 0.8565
% & 0.7980 & 1.0711 \\
% \midrule
% \multirow{3}{*}{\shortstack[l]{Review-based}} 
% & DeepCoNN 
% & 0.6034 & 0.8057
% & 0.6221 & 0.8403
% & 0.8312 & 1.0665 \\
% & NARRE 
% & 0.5799 & 0.7881 
% & 0.6242 & 0.8435
% & 0.8177 & 1.0785 \\
% & DAML 
% & 0.5703 & \underline{0.7848} 
% & 0.6214 & \underline{0.8371}
% & \underline{0.7964} & \textbf{1.0405} \\
% \midrule
% \multirow{4}{*}{LLM-based} 
% & GPT-4o
% & 0.7438 & 1.1069 
% & 0.7591 & 1.1558
% & 0.8766 & 1.3005 \\
% & Rec-SAVER 
% & 0.6463 & 0.9262 
% & 0.6645 & 0.9356
% & 0.8295 & 1.1282 \\
% & EXP3RT 
% & \underline{0.5608} & 0.8385 
% & \underline{0.6135} & 0.9370
% & 0.8306 & 1.2311 \\
% \midrule
% \multirow{1}{*}{\bf Ours} 
% & \textbf{Reason4Rec}
% & \textbf{0.5442} & \textbf{0.7722} 
% & \textbf{0.6029} & \textbf{0.8345}
% & \textbf{0.7586} & \underline{1.0418} \\
% \bottomrule
% \end{tabular}
% \label{tab:feedback_acc}
% \end{table}




\subsubsection{\textbf{Baselines.}}

% To comprehensively evaluate the proposed paradigm, Reason4Rec, we compare it against two categories of methods: review-base recommendation methods and LLM-based methods. Specifically, we select the following methods as baselines:
To comprehensively evaluate performance, we compare Reason4Rec against three categories of recommendation methods: traditional methods (MF), review-based recommendation methods (DeepCoNN, NARRE, DAML), and LLM-based methods (GPT-4o, Rec-SAVER, EXP3TR).

\begin{itemize}[leftmargin=*]
    \item \textbf{MF}~\cite{MF}: Matrix Factorization is a classical collaborative filtering method that predicts user ratings based solely on the historical rating matrix.

    \item \textbf{DeepCoNN}~\cite{deepconn}: %Deep Cooperative Neural Networks extract user and item properties jointly from review documents (\ie concatenations of their reviews) using a CNN network to assist in rating prediction.
    % Deep Cooperative Neural Networks jointly extract user and item properties from review documents using Convolutional Neural Networks (CNN) to assist in rating prediction.
    This method employs a Convolutional Neural Network (CNN) model to jointly learn item properties and user behaviors from review text to assist in rating prediction.

    \item \textbf{NARRE}~\cite{NARRE}: 
    % Neural Attentional Regression model with Review-level Explanations employs an attention mechanism to prioritize reviews that are most useful, enabling better extraction of user and item features.
    This method employs an attention mechanism to prioritize the most useful reviews, enabling better extraction of user and item features to enhance recommendations.

    \item \textbf{DAML}~\cite{DAML}: This method models the interaction between user and item review documents to improve the representations of both users and items.
    

    \item \textbf{GPT-4o}~\cite{chatgpt_CIKM}: This approach uses GPT-4o to directly predict ratings based on the user's rating history. We adopted the same few-shot prompting template as \cite{chatgpt_CIKM}.

    \item \textbf{Rec-SAVER}~\cite{ACL_findings}: This method supplies LLMs with the target item's metadata (e.g., title, categories, description) and the user's historical interactions (e.g., metadata of previously interacted items, user ratings, and raw reviews), and then instructs the LLMs to generate their reasoning process before providing the prediction.
    
    % Since the authors did not give a specific name for this method, we use the name of their evaluation framework, Rec-SAVER, to refer to it.

    \item \textbf{EXP3RT}~\cite{expert}: 
    This method first constructs user and item profiles from historical reviews and ratings using LLMs. These profiles are then leveraged to reason about user preferences and predict potential ratings in a single step.
    % Explainable Personalized Preference Reasoner first constructs user and item profiles based on their historical reviews and ratings using LLMs. These profiles are then used to reason about user preferences and predict possible ratings in the same single step.
    
\end{itemize}

\subsubsection{\textbf{Implementation Details.}}
For review-based methods, we adopt the implementation\footnote{\url{https://github.com/ShomyLiu/Neu-Review-Rec}.} from previous work~\cite{liu2019nrpa}.
For all reasoning-enhanced LLMRec methods (\ie Rec-SAVER, EXP3RT and our Reason4Rec), we use GPT-3.5-turbo\footnote{\url{https://platform.openai.com/docs/models\#gpt-3-5-turbo}.} as the teacher model and fine-tune the LLama3-8B model\footnote{\url{https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/}.} using the QLoRA technique. To accelerate the training and inference process, we leverage the Unsloth\footnote{https://github.com/unslothai/unsloth.} framework.


To reduce computational costs, we randomly sample 12,000 user-item pairs from the training data to construct the instruct data for LLMs learning, while conventional methods use the full train dataset. This sample size is sufficient to achieve strong performance, as demonstrated in our main results. All reasoning-enhanced LLMRec methods are trained on the same 12,000 user-item pairs. For training the reward model, we use an additional 8,000 user-item pairs, ensuring no overlap with the instruct data. The hyper-parameter reward threshold (
$\tau$) is set to 0.1, 0.2, and 0.04 for Music, Book, and Yelp, respectively. More details can be found in our code.

% We set different reward threhold $\tau$ for Music, Book, Yelp 分别为 0.1，0.2，0.04. Other details can be found in our code.




\subsection{Main Results (RQ1)}
We compare our Reason4Rec method with baseline methods on the accuracy of user feedback (rating) prediction (Predictor output) and the quality of generated reasons (Reasoner output).





\vspace{+5pt}
\header{Accuracy of User Feedback Prediction}
The comparison of user feedback prediction performance is summarized in \tabref{tab:feedback_acc}. As shown in the table, for the MAE metric, our method outperforms all datasets, achieving an average MAE reduction of approximately 0.02 compared to the best baseline results. For the RMSE metric, our method achieves the best performance on the Amazon Music and Amazon Book datasets, while delivering comparable results to the best baseline on the Yelp dataset. 
These superior performances of Reason4Rec confirm that incorporating reasoning about user preference and item features before making the final prediction, i.e., applying deliberation, is effective in enhancing RecLLM performance, thereby validating the rationality of our formulation of deliberative recommendation. 
Notably, although Rec-SAVER and EXP3RT also incorporate reasoning, their reasoning is performed in a single step, unlike our approach, which involves multiple steps—preference distillation, matching, and prediction. This lack of true ``deliberation'' in a single step may lead to their relatively poorer performance.  





% First, we compare the performance of our Reason4Rec method with all baseline methods on user feedback prediction accuracy (i.e., rating prediction). The experimental results, shown in \tabref{tab:feedback_acc}, reveal that Reason4Rec significantly outperforms all compared methods across all datasets. Specifically, Reason4Rec achieves MAE scores of $0.5442$ and $0.6029$ on the Music and Book datasets, respectively, yielding error reductions of $0.0166$ and $0.0106$ compared to the second-best method, EXP3RT, another LLM-based approach. Regarding the RMSE metric, Reason4Rec achieves scores of $0.7722$ and $0.8345$ on the Music and Book datasets, respectively, surpassing the second-best method, DAML (a review-based method), by margins of $0.0126$ and $0.0026$. These results demonstrate the effectiveness of our proposed method for user feedback prediction.


% First, we compare the performance of our Reason4Rec method with all baseline methods on the accuracy of the user feedback prediction (\textit{i.e.}, rating prediction). 
% The experimental results are presented in 
% \tabref{tab:feedback_acc}, from which we can observe that:  our Reason4Rec method significantly outperforms all the compared methods on each dataset.
% In particular, Reason4Rec achieves MAE scores of $0.5442$ and $0.6029$ \fb{on the Music and Book datasets}, respectively, reaching an error reduction of $0.0166$ and $0.0106$ compared to the second-best method, EXP3RT, which is also a LLM-based method.
% In terms of the RMSE metric, our Reason4Rec method achieves a score of $0.7722$ and $0.8345$ on Music and Book datasets, surpassing the second-best method, DAML (a Review-based method), by a margin of $0.0126$ and $0.0026$, respectively.
% The results indicate the effectiveness of our proposed method in the task of user feedback prediction.




% \begin{table}[h]
% \centering
% \caption{Performance comparison on the accuracy of user feedback (i.e., rating) prediction.}
% \begin{tabular}{llcccc}
% \toprule
% \multirow{2}{*}{} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Music}} & \multicolumn{2}{c}{\textbf{Book}} \\
% \cmidrule(lr){3-4} \cmidrule(lr){5-6}
% & & \textbf{MAE} & \textbf{RMSE} & \textbf{MAE} & \textbf{RMSE} \\
% \midrule
% \multirow{1}{*}{} & MF & 0.6188 & 0.8142 & 0.6277 & 0.8565 \\
% \midrule
% \multirow{3}{*}{\shortstack[l]{Review-based}} 
% & DeepCoNN & 0.6034 & 0.8057 & 0.6221 & 0.8403 \\
% & NARRE & 0.5799 & 0.7881 & 0.6242 & 0.8435 \\
% & DAML & 0.5703 & \underline{0.7848} & 0.6214 & \underline{0.8371} \\
% \midrule
% \multirow{4}{*}{LLM-based} 
% & gpt-4o (few-shot) & 0.7438 & 1.1069 & 0.7591 & 1.1558 \\
% & Rec-SAVER & 0.6463 & 0.9262 & 0.6645 & 0.9356 \\
% & EXP3RT & \underline{0.5608} & 0.8385 & \underline{0.6135} & 0.9370 \\
% \midrule
% \multirow{1}{*}{Ours} 
% & Reason4Rec & \textbf{0.5442} & \textbf{0.7722} & \textbf{0.6029} & \textbf{0.8345} \\
% \bottomrule
% \end{tabular}
% \label{tab:feedback_acc}
% \end{table}


\vspace{+5pt}
\header{Quality of Generated Reasons}
We next analyze the quality of the reasons generated by the Reasoner in  Reason4Rec, compared to two reasoning-aware baselines (i.e., Rec-SAVER and EXP3RT). \tabref{tab:quality_eval} summarizes the results. 
% As shown by the results of the two metrics, whether compared with ground-truth reviews (corresponding to BLEURT metric) or evaluated using the more powerful GPT model (corresponding to GPTScore metric),
As shown by the results of the two metrics, whether compared with target item reviews using the BLEURT metric or evaluated using the more powerful GPT model (corresponding to GPTScore metric),
the reasons generated by our method significantly outperform those of the two reasoning-aware baselines. 
Our method achieves an average relative improvement of 7.9\% on the GPTScore metric and 7.0\% on the BLEURT metric across all datasets. The superiority of our method’s reasoning can be attributed to the fact that we teach LLMs with recognized reasoning capabilities, guided by verbalized user feedback (e.g., user reviews~\cite{ReviewRecSurvey}), while the baselines fail to leverage this guidance.




% Next, we analyze the quality of the reasons generated by our Reason4Rec method compared to the baseline methods, \ie Rec-SAVER and EXP3RT.
% To evaluate the quality of the reasons, we compare them with the original user reviews using two evaluation metrics: BLEURT and GPT Score.
% We present the evaluation results on Music and Book datasets in \tabref{tab:quality_eval}.
% From the table, we make the following observations:
% The reasons generated by our proposed Reason4Rec method demonstrate significantly higher quality compared to those produced by the two baseline methods across all datasets, indicating its superior performance in generating high-quality reasons.
% Specifically, Reason4Rec achieves the highest GPT Score($80.53$) and BLEURT score($0.4067$), followed by EXP3RT ($76.22$ and $0.3840$) and Rec-SAVER ($75.60$ and $0.3652$) on the Music dataset, showing a notable improvement of by our Reason4Rec.
% On the Book dataset, Reason4Rec again leads with a GPT Score of $77.31$ and a BLEURT score of $0.4731$, significantly outperforming EXP3RT ($73.60$ and $0.4373$) and Rec-SAVER ($72.45$ and $0.4233$).

% \header{Case Study} 
% To more clearly demonstrate that the reasoning generated by Reason4Rec better aligns with the user's true preferences, we provide a case comparing the reasoning outputs generated by Reason4Rec with those from baseline methods.

\begin{table}[t]
\setlength\abovecaptionskip{-0.3px}
\setlength\belowcaptionskip{-0.3px}
\setlength{\tabcolsep}{0.15mm}
\small
\centering
\caption{
Comparison of user feedback prediction performance between our Reason4Rec method and the baselines across the three evaluation datasets. 
The best results are highlighted in bold and sub-optimal results are underlined.
}
\begin{tabular}{llcccccc}
\toprule
\multirow{2}{*}{\bf Type} & \multirow{2}{*}{\textbf{Method}} 
& \multicolumn{2}{c}{\textbf{Music}} 
& \multicolumn{2}{c}{\textbf{Book}} 
& \multicolumn{2}{c}{\textbf{Yelp}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
& & \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$
& \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$
& \textbf{MAE} $\downarrow$ & \textbf{RMSE} $\downarrow$  \\
\midrule
\multirow{1}{*}{CF-based} 
& MF 
& 0.6188 & 0.8142 
& 0.6277 & 0.8565
& 0.7980 & 1.0711 \\
\midrule
\multirow{3}{*}{\shortstack[l]{Review-based}} 
& DeepCoNN 
& 0.6034 & 0.8057
& 0.6221 & 0.8403
& 0.8312 & 1.0665 \\
& NARRE 
& 0.5799 & 0.7881 
& 0.6242 & 0.8435
& 0.8177 & 1.0785 \\
& DAML 
& 0.5703 & \underline{0.7848} 
& 0.6214 & \underline{0.8371}
& \underline{0.7964} & \textbf{1.0405} \\
\midrule
\multirow{4}{*}{LLM-based} 
& GPT-4o
& 0.7438 & 1.1069 
& 0.7591 & 1.1558
& 0.8766 & 1.3005 \\
& Rec-SAVER 
& 0.6463 & 0.9262 
& 0.6645 & 0.9356
& 0.8295 & 1.1282 \\
& EXP3RT 
& \underline{0.5608} & 0.8385 
& \underline{0.6135} & 0.9370
& 0.8306 & 1.2311 \\
\midrule
\multirow{1}{*}{\bf Ours} 
& \textbf{Reason4Rec}
& \textbf{0.5442} & \textbf{0.7722} 
& \textbf{0.6029} & \textbf{0.8345}
& \textbf{0.7586} & \underline{1.0418} \\
\bottomrule
\end{tabular}
\label{tab:feedback_acc}
\end{table}

\begin{table}[t!]
\setlength\abovecaptionskip{-0.3px}
\setlength\belowcaptionskip{-0.3px}
\setlength{\tabcolsep}{0.6mm}
% \small
\centering
\caption{Quality evaluation of generated reasons. ``GPT'' refers to ``GPTScore''. The best results are highlighted
in bold and sub-optimal results are underlined.}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Music}} & \multicolumn{2}{c}{\textbf{Book}} & \multicolumn{2}{c}{\textbf{Yelp}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& \textbf{GPT} & \textbf{BLEURT} & \textbf{GPT} & \textbf{BLEURT} & \textbf{GPT} & \textbf{BLEURT} \\
\midrule
Rec-SAVER & 75.60 & 0.3652 & 72.45 & 0.4233 & \underline{66.43} & 0.4102 \\
EXP3RT & \underline{76.22} & \underline{0.3840} & \underline{73.60} & \underline{0.4373} & 64.28 & \underline{0.4275} \\
\textbf{Reason4Rec} & \textbf{80.53} & \textbf{0.4067} & \textbf{77.31} & \textbf{0.4731} & \textbf{72.70} & \textbf{0.4565} \\
\bottomrule
\end{tabular}
\label{tab:quality_eval}
\end{table}





% \begin{figure}[h]
  
%   \centering
%   \includegraphics[width=\linewidth]{figures/quality.pdf}
%   \caption{Quality evaluation of generated reasons (Step 2).}
%   \label{fig:quality}
% \end{figure}


\subsection{In-depth Analysis}
In the following sections, we present an analysis of the impact of multi-step reasoning and verbalized preference feedback in Reason4Rec,  followed by the evaluation of Reason4Rec's inference cost.
Lastly, we provide detailed cases comparing the reasoning generated by Reason4Rec with those from baseline methods to demonstrate its superior alignment with the user's true preferences.

\subsubsection{\textbf{Impact of Multi-step Reasoning  (RQ2)}} \label{sec:each_step_important}
To evaluate the impact of multi-step reasoning in Reason4Rec, we first conduct an ablation study to analyze the necessity of the component in each step (\ie Summarizer, Reasoner and Predictor) and then assess the performance changes by replacing the multi-step reasoning mechanism with two alternative single-step reasoning strategies.

\header{Necessity of Each Step}
An ablation study on Reason4Rec is conducted by removing each step to assess its contribution. 
% \fb{It's better to add some brief desc about how to remove each step. }
Specifically, we make the following changes in each experiment: 1) w/o Step 1 (\ie Summarizer): Directly using user historical reviews to replace the aspect-preference summaries as input for Step 2 and Step 3;  2) w/o Step 2 (\ie Reasoner): Remove the Reason Placeholder section from the Step 3 Prompt and instruct the LLMs to directly predict the rating based solely on the aspect-preference summaries and historical ratings; 3) w/o Step 3 (\ie Predictor): Combine Step 2 and Step 3 into a single step, asking the LLMs to simultaneously generate the match analysis and predicted ratings based on the aspect-preference summaries and the user's historical ratings.


We conduct this study on Amazon Music and Book datasets and present the results in \figref{fig:ablation}, from which we make the following observations:
\begin{itemize}[leftmargin=*]
    \item The removal of each step results in an increase of the rating prediction errors on both Music and Book datasets, as reflected by higher MAE and RMSE values in the figure. 

    \item The MAE and RMSE increase significantly on both datasets after removing Step 3, underscoring the importance of separately training match analysis and rating prediction. One potential reason is that simultaneously training the LLM to complete the reasoning and rating tasks imposes significant challenges, resulting in degraded performance.

    \item The removal of Step 2 results in a relatively slight increase in RMSE but a significant increase in MAE on both datasets. 
    This discrepancy can be attributed to the fact that RMSE is overly sensitive to extreme errors (e.g., predicting 1 for a true rating of 5). Without match analysis, LLMs can still predict a roughly correct rating based on the user's aspect-preference summaries and historical ratings, which helps avoid extreme errors and results in only a slight increase in RMSE. In contrast, MAE assigns equal weight to all errors and is better at capturing finer-grained inaccuracies. The significant increase in MAE highlights the critical role of Step 2 in enabling the LLMs to predict user feedback more effectively on a fine-grained scale.

\end{itemize}



\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/ablation.pdf}
  \caption{Necessity of each Step. An ablation study on each step in Reason4Rec.}
  \label{fig:ablation}
\end{figure}


\header{One-step v.s. Multi-step} 
To verify the effectiveness of the multi-step reasoning strategy in Reason4Rec, we compare it with two alternative strategies: (1) One-step reasoning, which generates both an entangled reasoning process and the ratings in a single step, and (2) Chain-of-Thought (CoT) reasoning, which generates the outputs of our three reasoning steps in one single prompt.
We prepare the datasets separately based on the two above strategies and train the LLMs accordingly. 
In \figref{fig:one_step},  we summarize the results and make the following observations: 
1) Both one-step reasoning and CoT reasoning strategies lead to a notable increase in prediction errors on both Music and Book datasets, as reflected by the higher MAE and RMSE values.
2) In contrast, the performance of the CoT reasoning strategy is inferior to that of the simpler one-step reasoning strategy. This may be due to the complexity of multiple tasks involved in the CoT reasoning strategy, which poses significant challenges for LLMs to effectively process simultaneously.   



\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/ons_vs_multi.pdf}
  \caption{One-Step v.s. Multi-Step. Performance comparison between Reason4Rec's multi-step reasoning strategy and two alternative one-step reasoning strategies.}
  \label{fig:one_step}
\end{figure}

\begin{table}[t]
\centering
\caption{Effect of different reason generation approaches.}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\bf Method} & \multicolumn{2}{c}{\bf Music} & \multicolumn{2}{c}{\bf Book} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                        & \bf MAE             & \bf RMSE           & \bf MAE             & \bf RMSE           \\ \midrule

Without Reason          & \underline{0.5529}          & \underline{0.7742}          & \underline{0.6225}          & 0.8394          \\
Review-guide Reason              & 0.5825          & 0.7812          & 0.6376          & 0.8390          \\
Review-inferred Reason   & 0.5723          & 0.7816          & 0.6306          & 0.8420          \\
History-inferred Reason        & 0.5640          & 0.7744          & 0.6305          & \underline{0.8376}         \\
\textbf{Reason4Rec}                    & \textbf{0.5442} & \textbf{0.7722} & \textbf{0.6029} & \textbf{0.8345} \\
\bottomrule
\end{tabular}
\label{tab:reason-gen}
\end{table}

% \subsubsection{\textbf{Impact of Verbalized Preference Feedback (RQ3)}} 
   
\subsubsection{\textbf{Effect of Reason Generation Approaches (RQ3)}} \label{sec:impact_of_verbalized}
To validate the effectiveness of our reason generation approach in Step 2 of Reason4Rec, we compare it against four alternative approaches:
1) \textbf{Without reason}: Remove Step 2 from Reason4Rec entirely and instruct the LLMs to directly predict the rating based solely on the aspect-preference summaries and historical ratings, as done in the ablation study;
2) \textbf{Review-guided reasons}: Use the target item reviews as direct supervision for the LLMs' learning process in Step 2;
3) \textbf{Review-inferred reasons}: Use ChatGPT to generate reasons inferred from the target item reviews and employ these generated reasons as supervision for Step 2;
4) \textbf{History-inferred reason}: Use ChatGPT to generate reasons inferred from the aspect-preference summaries using the Step 2 Prompt and directly use them as supervision for Step 2 without applying filtering through our reward model.

In \tabref{tab:reason-gen}, we present the results and make the following observations:
1) All four alternative approaches perform worse than our proposed method on both Music and Book datasets, highlighting the advantages of our proposed method in collecting high-quality reasons for training LLMs.
2) The performance of Review-inferred Reason is worse than Without Reason on both datasets in MAE and RMSE. This indicates that directly using reviews to generate reasons can have a negative impact. A possible explanation is that our objective is to train LLMs to perform reasoning based on historical interactions without relying on target item reviews. Using reasons derived directly from these reviews introduces the risk that the reasoning logic may rely on preferences not reflected in the historical data.
Such inconsistencies disrupt the training process and ultimately result in suboptimal performance.
3) History-inferred Reason also performs worse than ``Without Reason'' on most metrics. This indicates that directly supervising with GPT-generated reasons without refinement is ineffective, underscoring the importance of using verbalized preference feedback as supervision.



\subsubsection{\textbf{Effectiveness of Reward Model (RQ3)}} \label{sec:reasonable_of_reward}



% The reward model is designed to capture the user's true preferences from the target item's review for rating predictions. 
The reward model should be capable of assessing whether the generated reasons align with the ground-truth preferences (as reflected in the target item reviews) by utilizing these reasons for rating prediction.
We conduct experiments to verify whether the reward model successfully acquires this capability. Specifically, we construct different types of reasons with known similarity levels to target item reviews to compare the prediction accuracy. We include the four types of reviews defined in Section~\ref{sec:impact_of_verbalized}, along with an additional type: Hint-Inferred Reason. This type generates reasons using ChatGPT, based on aspect-preference summaries and our reasoner's prompt, with the user's actual rating included as a hint. Based on their similarity to ground-truth reviews, the types of reasons are ranked from highest to lowest as follows: Review-Guided, Review-Inferred, Hint-Inferred, and History-Inferred Reason. Table~\ref{tab:reward_effect} summarizes the reward model's prediction accuracy using each type of reason. The results show that the prediction accuracy ranking aligns with the known similarity ranking, verifying that the reward model effectively evaluates the quality of the generated reviews.



% The reward model is built on the premise of its ability to capture the user's true preferences from the target item's review for rating predictions. 
% To validate this premise, we compare the reward model's prediction accuracy when using reasons with varying degrees of alignment to the target item review. Specifically, we randomly selected 3,000 instances of untrained data from both the Music and Book datasets and evaluated the performance using different types of reasons.
% We used the four types of reasons described in Section~\ref{sec:impact_of_verbalized} and introduced a new type of reason: Hint-Inferred Reason. This type involves generating reasons using ChatGPT, based on the aspect-preference summaries and the Step 2 Prompt, with the user's actual rating included as a hint.
% The results are shown in Table~\ref{tab:reward_effect}, from which we can observe that: 
% (1) Using the target item review achieves the highest prediction accuracy, significantly outperforming the results of "Without Reason." This suggests that the reward model successfully learns to capture the user's preferences from the target review to enhance rating predictions;
% (2) The prediction accuracy of Review-inferred, Hint-inferred, and History-inferred reasons decreases progressively. Since Review-inferred reasons directly align with fine-grained user preferences using reviews, while Hint-inferred reasons align with binary preferences using ratings, the alignment of these reasons with the target item review also decreases accordingly. This demonstrates that better alignment with the target review leads to improved performance, which supports our hypothesis.










\begin{table}[t]
\centering
\caption{Rating prediction accuracy of reward model using different types of reasons.}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{\textbf{Music}} & \multicolumn{2}{c}{\textbf{Book}} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                                & MAE             & RMSE            & MAE             & RMSE            \\ \midrule
Without Reason               & 0.6798          & 0.9359          & 0.6186          & 0.8298          \\
Review-guided Reason      & \textbf{0.294}   & \textbf{0.4527}  & \textbf{0.3329}  & \textbf{0.4597}  \\
Review-inferred Reason      & \underline{0.4238}           & \underline{0.568}           & \underline{0.4214}           & \underline{0.5394}           \\
Hint-inferred Reason        & 0.4879           & 0.6876          & 0.5174           & 0.7137           \\
History-inferred Reason     & 0.7257           & 1.071           & 0.6842           & 0.9686           \\ \midrule
\end{tabular}
\label{tab:reward_effect}
\end{table}

\begin{table}[b]
\centering
\caption{
Cost analysis of the average inference time and the average number of tokens generated per prediction. Experiments were conducted on an NVIDIA A800 GPU using 100 data points from the Book dataset. For Reason4Rec, only the last two steps were measured, and for EXP3RT, profile construction costs were excluded.
% \fb{Cost Analysis. Experiments were conducted on an NVIDIA A800 GPU using a random sample of 100 data points from the Book dataset. For Reason4Rec, only the last two steps were measured. For EXP3RT, profile construction costs were excluded}.
}
\begin{tabular}{lcc}
\toprule
Method & Avg. Inference Time (s) & Avg. Tokens Generated \\ \midrule
Reason4Rec   & 5.86           & 147.78           \\
Rec-SAVER    & 6.43           & 175.59           \\
EXP3RT & 5.62           & 150.74           \\ \bottomrule
\end{tabular}
\label{tab:overhead-analysis}
\end{table}

\begin{figure}[b]
  \centering
\includegraphics[width=\linewidth]{figures/case_study.pdf}
  \caption{Case study on whether the reasons generated by our Reason4Rec and the baselines align with user preferences. The ``User Review''  includes the ground-truth user preferences.}
  \label{fig:case}
\end{figure}

\subsubsection{\textbf{Cost Analysis}}
To evaluate the efficiency of Reason4Rec, we compared its inference time and generation cost with those of reasoning-aware baselines (Rec-SAVER and EXP3RT). As shown in Table~\ref{tab:overhead-analysis}, despite dividing the reasoning process into three steps, Reason4Rec achieves comparable average inference time and generates a similar number of tokens as the baselines. Specifically, it requires approximately 6 seconds and generates an average of 148 tokens per prediction. This efficiency is primarily attributed to two factors: 1) the preference distillation step in Reason4Rec is preprocessed in advance, as discussed in Section~\ref{sec:step1}, leaving only the last two steps to be executed during inference; and 2) Reason4Rec only decodes a single token during the final step, as explained in Section~\ref{sec:feedback_prediction}. These designs effectively minimize computation costs.


\subsubsection{\textbf{Case study}}
We conduct a case study to examine whether the reasons generated by Reason4Rec better align with true user preferences, as reflected in the target item reviews. We compare the reasoning results of Reason4Rec with those of EXP3RT and Rec-SAVER. Figure~\ref{fig:case} illustrates an example. In this case, the target item review indicates that the user values the book for its practicality and helpfulness. Reason4Rec accurately identifies these features and aligns them with the user's preferences, as highlighted in green. In contrast, EXP3RT incorrectly attributes the user's preference to the storyline and characters. Rec-SAVER also fails to recognize both the item's key features and the user's actual preferences, despite predicting that the user would like the book.

% We present a case to more specifically demonstrate that the reasoning generated by Reason4Rec better aligns with the user's true preferences. As shown in Figure~\ref{fig:case}, we compare the reasons generated by different methods with the target item review, highlighting aligned portions in green and misaligned portions in red.
% In this example, the user prefers the book for its practicality and helpfulness. 
% Reason4Rec correctly identifies these features and aligns them with the user's preferences. In contrast, EXP3RT misattributes the user's preference to the storyline and characters, and Rec-SAVER fails to recognize both the item's features and the user's true preferences, despite predicting the user would like the book.


