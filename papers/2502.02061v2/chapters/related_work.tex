\section{RELATED WORK}
In this work, we focus on Deliberative Recommendation, empowering RecLLMs with explicit reasoning abilities and optimizing LLMs through verbalized user feedback. This is highly related to LLM-based recommendation, deliberative alignment, and review-based recommendation. 

\subsection{LLM-based Recommendation}
Recent research has shown growing interest in RecLLMs~\cite{DBLP:journals/corr/abs-2410-19744}, with many studies exploring the use of LLMs to predict user feedback through in-context learning~\cite{chatgpt_CIKM,DBLP:conf/recsys/DaiSZYSXS0X23,DBLP:conf/recsys/SannerBRWD23} and fine-tuning~\cite{TallRec,Binllm,collm}. 
However, these approaches often instruct LLMs to generate predictions directly without disclosing their intermediate reasoning steps, resulting in a limited utilization of the amazing reasoning capabilities of LLMs.
To address this limitation, some recent studies explored leveraging LLMs' reasoning capabilities for recommendation tasks through various prompting strategies, such as chain-of-thought (CoT)~\cite{CoT} and self-reflection~\cite{DRDT}.
For instance, DRDT~\cite{DRDT} encourages LLMs to perform sequential recommendation in a divergent thinking manner by prompting them to analyze user preferences from multiple aspects, while simultaneously reflecting on their analysis using a critic prompt~\cite{DRDT}. 
GOT4Rec~\cite{GOT4Rec} applies the graph of thought strategy to prompt LLMs to reason through three different directions.
However, these in-context learning methods are inherently constrained by the models' existing capabilities, making LLMs often struggle to handle recommendation tasks effectively without task-specific fine-tuning~\cite{TallRec}.


Some studies have explored fine-tuning LLMs to improve their reasoning ability specifically for RecLLMs~\cite{expert,ACL_findings,ReasoingRec,RecReasonor}. 
For example, RecSAVER utilizes reasoning generated by a larger LLM as ground truth to fine-tune a smaller LLM, enabling the latter to learn reasoning patterns tailored for recommendation tasks~\cite{ACL_findings}.
EXP3RT~\cite{expert} employs a similar fine-tuning strategy but introduces an additional preparatory step: to construct user and item profiles based on their review histories first. 
Leveraging these constructed profiles, EXP3RT reasons about user preferences and predicts possible ratings in the same single step.
However, these fine-tuning methods face two main limitations. First, they compress the complex reasoning process into a single step and train it jointly, which makes it challenging for LLMs to effectively learn. 
In contrast, Reason4Rec decomposes the reasoning process into multiple steps, training each step independently for better performance.
Second, they lack fine-grained supervision in training data selection, leading models to learn the reasoning patterns misaligned with users' real preferences. 
Reason4Rec addresses this limitation by using verbalized user feedback as supervision, resulting in reasons that more accurately reflect users’ preferences and enhance the multi-step reasoning abilities of RecLLMs.

\subsection{Deliberative Alignment}
% guan2024deliberative openai
% openai-o1
% wang2024q

Existing work has demonstrated the superiority of LLMs combined with multi-step reasoning~\cite{wang2024q}, with OpenAI's O1 showcasing remarkable reasoning capabilities~\cite{openai-o1}. 
Meanwhile, Deliberative Alignment~\cite{guan2024deliberative}, a training paradigm that directly teaches LLMs human-written safety specifications and trains them to explicitly reason about these specifications before responding, has garnered widespread attention in the LLM safety domain. 


However, there are significant differences between this and the Deliberative Recommendation task: 
1) in the safety domain, alignment specifications are relatively well-defined and consistent, whereas the reasoning process behind user feedback is highly complex, with significant personalization and diverse preferences. 
Achieving deliberative user preference alignment at the individual level is thus a far more challenging task. 
2) Given the complexity of the Deliberative Recommendation task, relying solely on numerical user feedback (\eg ratings) or pairwise preference data for LLM learning, is insufficient. We propose leveraging verbalized user feedback to supervise reasoning for superior alignment, setting it apart from previous approaches to Deliberative Alignment. 


\subsection{Review-based Recommendation}
Review-based recommendation systems have been widely studied recently to address tasks such as rating prediction~\cite{rating_predict1, rating_predict2,rating_predict3,rating_prediction4,rating_prediction5}. 
These methods typically adopt a dual tower architecture that employs a user encoder and an item encoder to capture the semantics of reviews as user and item embeddings. 
Then, the user and item embeddings are combined through a fusion layer to derive joint representations, which are passed to a rating prediction layer to produce the final output~\cite{ReviewRecSurvey}.
To obtain better embeddings of users and items from reviews, many advanced techniques have been explored. 
For instance, DeepCoNN~\cite{deepconn} employs convolutional neural networks (CNNs) to extract semantic information from historical reviews, achieving excellent results. Furthermore, attention mechanisms~\cite{attention} are also introduced at both the word and review levels to identify critical words and reviews that are most significant~\cite{liu2019nrpa,attentionRec2,attention3}. 
For example, NARRE~\cite{NARRE} employs attention mechanisms to prioritize reviews that are most useful for improving embedding learning.
While these methods leverage reviews to represent user preferences implicitly with embeddings, resulting in the lack of interoperability. 
In contrast, Reason4Rec generates the reasoning process for each step in explicit textual form, offering clearer and more interpretable explanations of user preferences. 
Additionally, most review-based recommendation methods primarily leverage reviews as input features for generating embeddings. However, Reason4Rec also utilizes reviews as supervisory signals on the output side, aligning the LLM’s reasoning with the user's true preferences.

