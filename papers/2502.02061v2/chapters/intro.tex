\section{INTRODUCTION}

% Large Language Models (LLMs) exhibit substantial potential in recommendation tasks, leveraging extensive world knowledge and powerful reasoning~\cite{DBLP:journals/www/WuZQWGSQZZLXC24, DBLP:conf/www/WangTHYLZZPW24, DBLP:journals/corr/abs-2410-19744}. 
% Recent studies have leveraged LLMs to predict user feedback on the item (\eg numerical ratings or binary feedback) based on user history and item features~\cite{TallRec,Binllm,chatgpt_CIKM}. 
% However, existing approaches typically require LLMs to generate predictions directly, bypassing explicit reasoning steps and limiting their capacity for in-depth reasoning~\cite{ACL_findings}.
% Extensive studies, such as chain-of-thought prompting~\cite{CoT} and OpenAI's o1\footnote{\url{https://openai.com/o1/}.}, demonstrates that using explicit reasoning steps can significantly enhance the reliability of LLMs in various tasks~\cite{cheng2025think,math1,math2,math3,logic1}. 
% % Furthermore, directly predicting user feedback without revealing the underlying reasoning reduces transparency, making recommendations less interpretable, which can in turn diminish user trust and negatively affect user experience~\cite{RecExplainable}. 
% As such, incorporating explicit reasoning into LLM-based recommenders (LLMRec) holds significant potential~\cite{CTRReasoning}.
%
%
%
%
%
%%% FFL 0124 %%%
Large Language Models are increasingly utilized and aligned with recommendation task through supervised fine-tuning~\cite{TallRec,Binllm,bigRec} or direct preference optimization~\cite{SDPO} for predicting the user feedback. Although these recommendation LLMs (RecLLMs) generally achieve performance gains in different scenarios~\cite{DBLP:journals/www/WuZQWGSQZZLXC24, DBLP:conf/www/WangTHYLZZPW24, DBLP:journals/corr/abs-2410-19744}, they can still fail in some complex cases, resulting in valueless even disappointing recommendations. For instance, a RecLLM would present a video on vanilla attention mechanisms to a user who has watched several videos on self-attention and its enhanced variants. 

Recent insights from OpenAI regarding the limitations of general LLMs~\cite{deliberative_alignment} suggest that failures of current RecLLMs come from their alignment objective, \ie optimizing the LLM to directly generate user feedback given the user history and candidate item. This pattern-based learning makes the model susceptible to data biases~\cite{DecodingMatters} and spurious correlations~\cite{yangzhang23,Xiangnan23}, hindering their ability to generalize to low-frequency or cold-start items. Moreover, this label-oriented optimization objective pushes the LLM to make prediction instantly, discouraging thorough reasoning about user preferences. Consequently, it is crucial to investigate alternative alignment objectives for RecLLMs.

\begin{figure}[tbp]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.3cm}
  \centering
  \includegraphics[width=\linewidth]{figures/teaser.pdf}
  %\caption{Comparison between existing one-step reasoning methods and our proposed HoTRec. HoTRec utilizes multi-step reasoning, which is supervised by tailored user verbalized preference feedback.}
  \caption{Comparison between the alignment objective of existing research, which optimizes LLMs to directly predict user feedback; and the objective of \textit{Deliberative Recommendation}, which optimizes LLMs to conduct explicit reasoning about user preferences before generating the prediction.}
  \label{fig:teaser}
\end{figure}

Inspired by the recent advancements in enhancing the reliability of general LLMs through explicit reasoning~\cite{cheng2025think,math1,math2,math3,logic1}, we introduce a new \textit{Deliberative Recommendation} task. This task is designed to train LLMs to engage in reasoning about user preferences and item features prior to predicting user feedback. As illustrated in Figure~\ref{fig:teaser}, the RecLLM will explicitly generate a reasoning process, using this process to augment its predictions. To achieve this goal while adhering to commercial restrictions, the key of this formulation is seeking user generated and freely available rationale behind the user feedback as guidance for training the LLM in reasoning generation. Consequently, we incorporate verbalized user feedback such as reviews or conversations, into our task formulation as supervisory signals for reasoning, setting our work apart from previous studies~\cite{ACL_findings,expert,GOT4Rec}. 

% Several preliminary studies have investigated prompting LLMs to generate reasoning steps before predicting user feedback~\cite{ACL_findings,expert,GOT4Rec}. 
% % They fine-tune smaller LLMs to produce reasoning and predictions in one step with reasoning generated by larger LLMs as training data.
% They typically fine-tune LLMs to generate the reasoning process and predictions in one step, where the reasoning process for fine-tining is usually generated by larger-size LLMs.
% However, as illustrated in Figure~\ref{fig:teaser}, these approaches face two key limitations:
% % 1) \textbf{Overcompressed and Entangled Reasoning Process.}
% 1) \textbf{Overcompressed Single-step Reasoning.}
% % Unlike other reasoning tasks such as mathematical reasoning, which often follow strict logical chains, reasoning in recommendation tasks is highly personalized and diverse~\cite{Wang0WWF022}; 
% Unlike other reasoning tasks such as mathematical reasoning, which often follow unified logic rules, 
% % reasoning in recommendation tasks is highly personalized and diverse~\cite{Wang0WWF022}; 
% reasoning in recommendation tasks is highly complex due to users' personalized and diverse preference patterns~\cite{Wang0WWF022}; 
% for example, two users may assign the same rating to an item for entirely different reasons. 
% Forcing LLMs to capture such diverse preference patterns through one-step reasoning is highly challenging since LLMs may ignore some essential user differences (see evidence in Section~\ref{sec:each_step_important}).
% % requires it to process multiple aspects of reasoning simultaneously, overwhelming the model with dispersed reasoning focus and hindering effective learning.
% 2) \textbf{Lack of Reliable Supervision for Reasoning.}
% Existing approaches generate and then select the reasoning process consistent with the user's binary feedback (\eg like or not) for LLM fine-tuning, 
% % without verifying whether the reasoning truly aligns with the user's nuanced preferences~\cite{expert}. 
% without verifying whether the reasoning truly aligns with the user's nuanced preferences and reasons behind the binary feedback~\cite{expert}. 
% % For instance, a user might rate an item highly for its affordability, while the reasoning attributes the preference to product quality.
% Fine-tuning LLMs on misaligned reasoning processes will lead LLMs to learn incorrect reasoning patterns, reducing the reliability of LLMRec.


% To overcome these limitations, we identify two key objectives for integrating explicit reasoning into the LLMRec task:
% % 1) the reasoning process should be decomposed into multiple steps, with each step trained independently to handle the complexity and diversity inherent in preference reasoning; 
% 1) a complex reasoning process should be decomposed into multiple steps, progressively analyzing user nuanced preferences from user behaviors and item features, and then inferring the reasons why users like or dislike an item. This allows for a deeper reasoning of the difference and personalization in user preferences; 
% % the reasoning process should be decomposed into multiple steps, with each step trained independently to handle the complexity and diversity inherent in preference reasoning; 
% and 2) 
% users' verbalized preference feedback, \ie preferences described in natural language such as user reviews and conversations, often explains the specific reasons why users like or dislike certain items. 
% We thus propose incorporating users' verbalized preference feedback as 
% % supervisory signals, 
% supervision signals for reasoning, 
% % guiding LLMs' learning to ensure that the generated reasoning process aligns with users' true preferences.
% guiding LLMs to generate the reasoning process well aligned with users' true preferences. 


% Toward these goals, we propose a multi-\textbf{step} reasoning paradigm for LLM\textbf{Rec} (named StepRec), optimizing LLMs by verbalized preference feedback, as illustrated in Figure~\ref{fig:teaser}. 
% Specifically, given historical user and item features (\eg historical reviews, ratings, and attributes), StepRec decomposes the complex reasoning process into three steps:
% 1) \textbf{Preference and Feature Summary:} 
% % StepRec analyzes historical reviews and ratings to extract the user's preference elements and the item's positive and negative features, summarizing them into representative keywords. 
% StepRec analyzes historical reviews and ratings to summarize the user's preferences in various aspects and the item's positive and negative features, organizing them into aspect-preference summaries. 
% % This step captures the user's preferences and the item's features in a concise textual format, providing a reasoning foundation for subsequent match analysis;
% These summaries record users' preferences and items' salient features in concise keywords, providing a reasoning foundation for subsequent match analysis; 
% 2) \textbf{User-Item Match Reasoning:}
% % Using the aggregated user preferences and item features from the first step, StepRec analysis the degree of alignment between the user and the item, generating reasons for why the user might like or dislike the item based on their preferences and the item's features;
% Using the summaries of user preferences and item features, StepRec evaluates the match between the user and the item, generating reasons why the user might like or dislike the item; 
% 3) \textbf{User Feedback Prediction:} StepRec finally predicts the user's feedback (\eg ratings) for the given item by jointly considering the rating history and the reasoning from the previous two steps, 
% To supervise the multi-step reasoning, we consider adopting user reviews, which encapsulate users' true preferences in natural language form~\cite{ReviewRecSurvey}, as an informative source of verbalized preference feedback. 
% % extracting meaningful signals from them to guide and supervise the multi-step reasoning process, ensuring that the reasoning aligns more closely with the user's true preferences.


% Nevertheless, it is non-trivial to effectively utilize user reviews as supervision. Review data often contains various types of noise~\cite{ NARRE}, such as low-quality comments or preference-irrelevant details. Directly using these reviews may result in suboptimal outcomes (see empirical evidence in Section~\ref{sec:impact_of_verbalized}).
% To address this issue, we propose a supervision strategy to extract step-specific signals from reviews and ratings. 
% % Leveraging these signals, we independently train task-specific QLoRA~\cite{Qlora} adapters for each step on an LLM:
% % First, for preference and feature extraction, we employ larger LLMs as teacher models. Key preference elements summarized from historical reviews by the teacher LLMs are used as supervision signals to guide the training process.
% First, for preference and feature summary, we employ ChatGPT~\cite{chatgpt} as a teacher model, which is powerful in summarizing key elements of preferences from historical reviews~\cite{textSummary}. 
% % Next, for the user-item match reasoning, to generate reasons that align with user preferences as supervision, we first provide the teacher LLMs with aggregated historical user preferences and item features to sample potential reasons. We then train a reward model to assess the quality of these reasons based on the user’s authentic review, selecting a set of high-quality reasons for match reasoning training.
% Next, for the user-item match reasoning, we first provide ChatGPT with summarized historical user preferences and item features to sample potential reasons, which are then selected by a reward model based on their alignment with the user’s authentic review. 
% % Finally, for user feedback prediction, the user's actual rating is used as a natural supervision signal.
% Finally, the user's actual rating can guide the user feedback prediction step. 
% These signals are then used to independently train three step-specific QLoRA~\cite{Qlora} adapters on an LLM. 
% % as a natural supervision signal.
% % Extensive studies demonstrate the superiority of StepRec in both prediction accuracy and reasoning quality. Furthermore, we validate the importance of multi-step reasoning and verbalized preference feedback supervision for recommendation reasoning through a series of ablation studies and analytical experiments. The code and datasets are available at xxx.
% % We conduct extensive experiments on rating prediction tasks across three real-world datasets to demonstrate the effectiveness of StepRec in both prediction accuracy and reasoning quality across two real-world datasets. 
% We conduct extensive experiments on rating prediction tasks across three real-world datasets, validating the prediction accuracy and reasoning quality of StepRec. 
% % To further validate our approach, 
% In addition, substantial ablation studies and comparisons with diverse variants demonstrate the superiority of StepRec's supervision strategy. 
% % we perform ablation studies and analytical experiments, including evaluations of the effectiveness of the supervision strategy. 
% % These analyses highlight the significance of multi-step reasoning and verbalized preference feedback supervision for recommendation tasks. 
% The code and datasets are available at \url{https://anonymous.4open.science/r/StepRec-3188}.





%%% FFL 0124 %%%

To effectively address the Deliberative Recommendation task, it is crucial to decompose the highly personalized and diverse reasoning process~\cite{Wang0WWF022} into distinct steps, and equipping the LLM with specialized reasoning capabilities for each step. In this way, we highlight three fundamental capabilities: 
1) \textit{Preference Distillation}, which analyzes the user history to identify aspect-level user preferences~\cite{guan2019attentive} and examines existing verbalized user feedback on the candidate item to recognize positive and negative features; 
2) \textit{Preference Matching}, which matches distilled user preferences with item features and generates rationales for why the user might like or dislike the candidate item; 
and 3) \textit{Feedback Prediction}, which predicts the user feedback with consideration of the generated raionales.

%To supervise the multi-step reasoning, we consider adopting user reviews, which encapsulate users' true preferences in natural language form~\cite{ReviewRecSurvey}, as an informative source of verbalized preference feedback. 
 


In this light, we propose a \textit{Reasoning-powered Recommender} (Reason4Rec) framework to teach LLMs with the recognized reasoning capabilities with the guidance of verbalized user feedback (\eg user reviews~\cite{ReviewRecSurvey}). As illustrated in Figure~\ref{fig:framework}, Reason4Rec employs 
% a mixture-of-expert architecture~\cite{bian2023multi} with three experts separately working for each reasoning steps to focus on learning the step-specific capabilities. 
three collaborative experts, each dedicated to a specific reasoning step, to hone step-specific reasoning capabilities. 
To reduce the computational and memory cost, these experts are implemented using separate QLoRA~\cite{Qlora} adapters on the same LLM. 
Recognizing that each reasoning step necessitates extracting distinct signals from the verbalized user feedback, each expert is equipped with a uniquely tailored training strategy. Taking rating prediction~\cite{DAML} as an example, we conduct extensive experiments across three real-world datasets, validating the effectiveness of Reason4Rec regarding prediction accuracy and reasoning quality, and the rationality of the framework design. The code and datasets are available at \url{https://github.com/Peter-Fy/Reason4Rec}.
% \url{https://anonymous.4open.science/r/DeliRec-F4B5}.


\begin{figure}[tbp]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.3cm}
  \centering
  \includegraphics[width=\linewidth]{figures/framework.pdf}
  %\caption{Comparison between existing one-step reasoning methods and our proposed HoTRec. HoTRec utilizes multi-step reasoning, which is supervised by tailored user verbalized preference feedback.}
  \caption{Illustration of the Reasoning-powered Recommender framework.}
  \label{fig:framework}
\end{figure}

To summarize, our contributions are threefold:
% \begin{itemize}[leftmargin=*]
% % \item We propose a multi-step reasoning paradigm optimized by user verbalized preference feedback for LLMRec, highlighting the significance of adopting slow thinking approaches for recommendation tasks. 
% \item We propose a multi-step reasoning paradigm for LLMRec, optimized through users' verbalized preference feedback, emphasizing the significance and potential of leveraging LLMs' slow thinking for complex recommendation tasks. 

% % \item To effectively utilize user reviews as verbalized preference feedback, we propose a supervision strategy and a reward model method to extract meaningful supervision signals from reviews for fine-tuning.
% \item To effectively utilize user reviews as verbalized preference feedback, we propose a supervision strategy and a reward model to extract meaningful supervision signals for multi-step reasoning. 

% \item We conduct extensive experiments to demonstrate the effectiveness of StepRec in prediction accuracy and reasoning quality, validating the superiority of multi-step reasoning and verbalized preference feedback.

% \end{itemize}
%
%
%
%
%
%%% FFL 0124 %%%
\begin{itemize}[leftmargin=*]
\item We formulate the \textit{Deliberative Recommendation} task, which pursues LLMs conducting reasoning before making prediction by learning from verbalized user feedback. 

\item We propose a \textit{Reasoning-powered Recommender} framework for deliberative user preference alignment, which achieves the reasoning process with step-wise experts associated with specifically designed training strategies. 

\item We conduct extensive experiments on three datasets, validating the effectiveness and rationality of the proposed Reason4Rec framework, showing the potential of slow thinking in recommendation.

\end{itemize}





