\section{METHOD}
% \section{DELIBERATIVE PREFERENCE ALIGNMENT}


% We present StepRec, a multi-step reasoning paradigm for LLMRec that optimizes LLMs using users' verbalized preference feedback. 
% Specifically, StepRec decomposes the recommendation reasoning process into three steps: 1) Preference and Feature Summary, 2) User-Item Match Reasoning, and 3) User Feedback Prediction. 
% For each step, we independently train a step-specific QLoRA adapter on an LLM using tailored supervision signals extracted from reviews and ratings. 
% An overview of StepRec is illustrated in Figure~\ref{fig:teaser}. 

%#################################

% We present a \textbf{Deli}berative user preference alignment framework for \textbf{Rec}LLMs, named Reason4Rec. 
In this section, we present Reason4Rec with three experts to collaboratively perform multi-step reasoning before recommendation. 
To supervise the reasoning process, Reason4Rec incorporates user reviews, directly aligning the reasoning with user preferences rather than only inferring preferences through users' numerical or binary feedback (\eg ratings and clicks). 
We then detail the three experts, \ie summarizer, reasoner, and predictor, with their fine-tuning strategies. An overview of Reason4Rec is illustrated in Figure~\ref{fig:framework}. 

% , a multi-step reasoning paradigm for LLMRec that optimizes LLMs using users' verbalized preference feedback. 
% Specifically, StepRec decomposes the recommendation reasoning process into three steps: 1) Preference and Feature Summary, 2) User-Item Match Reasoning, and 3) User Feedback Prediction. 
% For each step, we independently train a step-specific QLoRA adapter on an LLM using tailored supervision signals extracted from reviews and ratings. 
% An overview of StepRec is illustrated in Figure~\ref{fig:teaser}. 

%#################################


% We extract step-specific signals from reviews and ratings to used for indepen
% Given a user-item pair, StepRec first analyzes user preferences and item features from historical reviews and ratings, summarizing them into an aspect-preference summary. Next, it analyzes the degree of match between the user and the item based on these summaries. Finally, StepRec combines the generated reasons from the first two steps with historical ratings to predict user feedback. 
% For each step, we independently train a step-specific QLoRA adapter allowing the LLMs to address the unique requirements of each reasoning step effectively.

% To effective optimize LLMs learning through users' verbalized preference feedback , we propose a supervision strategy to use user's reviews and ratings as supersion to supervisie the training of StepRec. An overview of StepRec is illustrated in Figure~\ref{fig:teaser}.

% \subsection{Preference and Feature Summary} \label{sec:step1}
\subsection{Summarizer for Preference Distillation} \label{sec:step1}

% The first step of StepRec is to analyze the user's preference elements and the item's positive and negative features from their historical reviews. 
% The analysis is integrated into an aspect-preference summary, 
% \ie concise, representative keywords that encapsulate user preferences and item features. 
% This step filters out noise from the reviews, captures nuanced user preferences, and enriches item features with the LLM's inherent knowledge, providing a reasoning foundation for the subsequent reasoning process.

Reason4Rec leverages a summarizer to distill the user's preferences and the item's positive and negative features from their historical reviews. 
It filters out noise from the reviews, captures nuanced user preferences, and enriches item features with the LLM's inherent knowledge. 
The distillation output is formatted as an aspect-preference summary, 
\ie concise and representative keywords that encapsulate essential user preferences and item features, providing a reasoning foundation for the subsequent reasoning process. 

% \vspace{3pt}
% \noindent$\bullet\quad$\textbf{Design Motivation.} This step is designed for three purposes: 1) \textbf{Extract nuanced user preference and supplementary item features.} User reviews provide nuanced insights into user preferences that explicit ratings alone cannot capture. They may also offer additional item-specific information beyond the inherent knowledge of LLMs. Extracting these insights enrich the context for subsequent reasoning; 2) \textbf{Filtering and Condensing User Reviews for Better Reasoning.} By extracting user preference and item features into aspect-preference summary, we mitigate noisy or excessively long reviews, thereby improving the subsequent reasoning process; 3) \textbf{Efficient Historical Review Reasoning.} The extracted aspect-preference summary from a review can be stored for reuse in future reasoning tasks involving the same user or item, eliminating the need to repeatedly model user preferences and item features in a one-step reasoning process, thereby conserving computational resources.

\vspace{3pt}
% \noindent$\bullet\quad$\textbf{Prompt Construction.} 
\noindent$\bullet\quad$\textbf{Summarizer Instantiation.} 
% For a specific historical interaction $(r_{ui}, c_{ui})$, we summary the user's preferences and the item's features from the historical review using a one-shot prompt similar to the following (the complete prompt for this and subsequent steps can be found in our code):
% Given a specific historical interaction $(r_{ui}, c_{ui})$, we summarize the user's preferences and the item's features from the historical review using a one-shot prompt similar to the following prompt, which uses the Music dataset as an example. 
Formally, given a historical interaction $(r_{ui}, c_{ui})$ between user $u$ and item $i$, the summarizer will generate an aspect-preference summary. 
We instantiate the summarizer by fine-tuning a QLoRA adapter~\cite{Qlora} on an LLM and the summarizer's prompt is illustrated in the following, which uses the Music dataset as an example. 
% similar to the following (the complete prompt for this and subsequent steps can be found in our code):

\begin{center}
\fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{\centering % 添加居中命令
\textbf{Summarizer Prompt} \\[2pt]
\raggedright % 恢复左对齐
\textbf{Task:} Summarize the reasons behind the given rating of a Music based on the customer review. \\[2pt]
\textbf{Music:} $i$  \\
\textbf{Rating:} $r_{ui}$  \\
\textbf{Review:} $c_{ui}$  \\[2pt]
Analyze the above customer review for the Music $i$ and summarize the reasons behind the given rating of $r_{ui}$. Please consider the positive and negative aspects mentioned in the review and provide the keywords of reasons and user preference elements. 
}} 
\end{center}

The user's preferences and the item's features are outputted as an aspect-preference summary. A template example is as follows: 

\begin{center}
\fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{
\textbf{Positive Aspects:} Catchy Melody, Unique Instrumentation, ... \\
\textbf{Negative Aspects:} Repetitive Lyrics, Overuse of Autotune, ...  \\
\textbf{User Preference Elements:} Harmony, Emotional Resonance, ... 
}} 
\end{center}


\vspace{3pt}
\noindent$\bullet\quad$\textbf{Summarizer Fine-tuning.} 
% Considering that this step is analogous to a text summarization task---a domain where ChatGPT already shown strong performance~\cite{textSummary}---we employ ChatGPT as teacher models to generate ground-truth aspect-preference summaries to guide the LLMs' learning.
Considering ChatGPT~\cite{chatgpt} has shown strong performance in various summarization tasks~\cite{textSummary}, we employ closed-source ChatGPT as a teacher model to generate aspect-preference summaries, guiding the learning of open-source LLMs. 
With these summaries, we optimize the QLoRA adapter of the summarizer by SFT~\cite{SFT}. 


\vspace{3pt}
% \noindent$\bullet\quad$\textbf{Preprocessing and Reuse.} 
\noindent$\bullet\quad$\textbf{Offline Summarization.} 
% By preprocessing each historical interaction, we can store the generated aspect-preference summaries for reuse in future reasoning tasks involving the same user or item. This preprocessing step eliminates the need to repeatedly model user preferences and item features, as required in one-step reasoning processes, thereby reducing computational overhead and enhancing efficiency.
Given a summarizer, we can store the generated aspect-preference summaries offline for all the historical interactions between users and items.  
This step eliminates the need for repeated preference distillation, thereby reducing computational overhead and enhancing efficiency.


% \subsection{User-Item Match Reasoning}
\subsection{Reasoner for Preference Matching}

% The second step of StepRec involves analyzing the match degree between the user and the item based on the previously generated aspect-preference summary, generating reasons for why the user might like or dislike the item.
Given all the aspect-preference summaries of a user and an item, Reason4Rec adopts a reasoner to evaluate their matching degree, generating reasons for why the user might like or dislike the item. 

\vspace{3pt}
% \noindent$\bullet\quad$\textbf{Prompt Construction.} 
\noindent$\bullet\quad$\textbf{Reasoner Instantiation.} 
% For a specific user-item pair $(u, i)$, we aggregate the aspect-preference summaries from their historical interactions $\mathcal{H}_u$ and $\mathcal{H}_i$ in chronological order into a single prompt, tasking the LLMs to reason about the match between $u$ and $i$. An example of the match reasoning prompt is shown below: 
Formally, given a user-item pair $(u, i)$, we aggregate the aspect-preference summaries from their historical interactions $\mathcal{H}_u$ and $\mathcal{H}_i$ in chronological order into a single prompt, tasking the reasoner to measure the preference matching between $u$ and $i$. 
The reasoner is also implemented by another QLoRA adapter on the same LLM. 
An example of the reasoning prompt is shown below: 
\begin{center}
\fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{
\centering % 添加居中命令
\textbf{Reasoner Prompt} \\[2pt]
\raggedright % 恢复左对齐
\textbf{\#\#\# User Review History \#\#\#} 
$\langle$\textit{$\mathcal{H}_u$ organized as below}$\rangle$ \\
1. Title of Item 1\\
Positive Aspects: [Aspect 1], [Aspect 2], ... \\
Negative Aspects: [Aspect 1], [Aspect 2], ... \\
User Preference Elements: [Preference 1], [Preference 2], ... \\
2. Title of Item 2 \\
Positive Aspects: [Aspect 1], [Aspect 2], ... \\
Negative Aspects: [Aspect 1], [Aspect 2], ... \\
User Preference Elements: [Preference 1], [Preference 2], ... \\
... \\[3pt]
\textbf{\#\#\# Item Review History by Other Users \#\#\#} \\
$\langle$\textit{$\mathcal{H}_i$ organized in the same format as above}$\rangle$ \\[3pt]
\textbf{Task:} Analyze whether the user will like the new Music $i$ based on the user's preferences and the item's features. Provide your rationale in one concise paragraph.
}}
\end{center}

% \vspace{3pt}
% \noindent$\bullet\quad$\textbf{Supervision  Strategy.} 
% Considering that users often explicitly express their preferences in their reviews of the target item, we use these target item reviews as supervision for this step to better align the reasoning process with the users' true preferences.
\vspace{3pt}
\noindent$\bullet\quad$\textbf{Reasoner Fine-tuning.} 
Given a historical interaction between user $u$ and item $i$, we propose using target item review $c_{ui}$ to guide the reasoner learning, because users would explicitly explain their preferences in verbalized reviews. 
% , we use these target item reviews as supervision for this step to better align the reasoning process with the users' true preferences. 
However, directly using reviews for supervision suffers from noise issues, since user reviews often contain low-quality comments or preference-irrelevant details (see evidence in Section~\ref{sec:impact_of_verbalized}). 
Thus, we employ a generation-then-filter strategy: using ChatGPT first generates diverse reasons why user $u$ might like or dislike item $i$, and a reward model then selects the high-quality reasons based on target item review $c_{ui}$. The selected high-quality reasons are used for reasoner learning through SFT. 

% \vspace{3pt}
\noindent\textbf{Reason Generation.}
As outlined in Algorithm~\ref{alg:reason_sampling}, given a user-item pair, we repeatedly sample candidate reasons from ChatGPT based on the reasoner prompt and evaluate reasons using the reward model until either a high-quality reason is obtained or the maximum iteration count is reached. 
Particularly, to avoid repeatedly generating reasons that contradict the user's binary preference, we incorporate the user's actual rating as a hint, except for the initial generations. 
This is implemented by appending the following hint statement to the reasoner prompt:
\begin{center}
\fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{
\textbf{Hint:} The user actually rated the item \textbf{$r_{ui}$} stars. The star ranges from 1 to 5, with 5 being the best. Use the hint but don't mention the user's rating in your response.
}}
\end{center}

% \begin{algorithm}[t]
%     \caption{Generation-then-filter strategy}
%     \label{alg:reason_sampling}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \begin{algorithmic}[1]
%     \Require A teacher model LLM, a reward model $R$, the reasoner prompt $P$, an iteration count $T$, a Hint with user's actual rating.

%     \State Initialize a high-quality reason set $\mathcal{S}_{\text{high}} \leftarrow \emptyset$;
%     \State Initialize a reason $s \leftarrow \text{LLM}(P)$;

%     \ForAll{$t \in \{1, \dots, T\}$}
%         \State Get the reason quality $\text{Score} \leftarrow R(s)$;
%         \If{$\text{Score} = 1$}
%             \State Save $\mathcal{S}_{\text{high}} \leftarrow s$;
%             \State \textbf{break};
%         \Else
%             \State Regenerate a new reason with hint: $s \leftarrow \text{LLM}(P, \text{Hint})$;
%         \EndIf
%     \EndFor

%     \Ensure the high-quality reason in $\mathcal{S}_{\text{high}}$.
%     \end{algorithmic}
% \end{algorithm}

% We propose using target item reviews as a filter to select aligned reasons for LLMs' learning, rather than directly using reasons inferred from target item reviews as supervision, since the latter approach may suffer from inconsistent reasoning logic issues, as discussed in Section~\ref{sec:impact_of_verbalized}.
\noindent\textbf{Reason Filter.} 
% To achieve the filter process, we introduce a reward model (details in Section~\ref{sec:reward}) to evaluate the quality of candidate reasons based on their alignment with target item review. This model assigns a score of 1 to high-quality reasons and 0 to low-quality resons. Using this reward model, we design an iterative algorithm to generate high-quality reasons using ChatGPT for Step 2 learning process.
We introduce a reward model to filter candidate reasons based on their alignment with the target item review $c_{ui}$. 
Considering the alignment, the reward model assigns a score of 1 to high-quality reasons and 0 to low-quality reasons. 
Using this reward model, we can iteratively generate high-quality reasons using ChatGPT for the reasoner fine-tuning.
The detailed training and inference of the reward model can be found in Section~\ref{sec:reward}. 

% \vspace{3pt}
% \noindent$\bullet\quad$\textbf{Algorithm for Generating High-quality Reasons.}
% As outlined in Algorithm~\ref{alg:reason_sampling}, we repeatedly sample candidate reasons from ChatGPT and evaluate them using the reward model until either a high-quality reason is obtained or the maximum iteration count is reached. Particularly, to avoid repeatedly generating reasons that contradict the user's binary preference, we incorporate the user's actual rating as a hint, except for the initial generations. This is implemented by appending the following hint statement to Step 2 Prompt:







% A straightforward approach to leveraging target item reviews for supervision is to use reasons inferred from these reviews as ground truth for fine-tuning LLMs. However, this method encounters challenges with inconsistent reasoning logic. Since our objective is to train LLMs to reason based on historical interactions without involving target item reviews, using reasons derived from these reviews risks introducing confusing reasoning logic that depends on preferences not reflected in the historical data. Such inconsistencies disrupt the training process and ultimately result in suboptimal performance, as demonstrated in Section X. Therefore, we propose training LLMs using reasons derived exclusively from historical interactions, as outlined in the prompt above. Additionally, we utilize target item reviews as a filter to exclude misaligned training reasons, ensuring that the training process aligns with users' true preferences. 



% \subsection{User Feedback Prediction} \label{sec:feedback_prediction}
\subsection{Predictor for Feedback Prediction} \label{sec:feedback_prediction}

% Our final step involves predicting user feedback, such as ratings, based on the reasons generated in the prior two steps. Given that the user's historical ratings serve as a valuable reference for rating prediction~\cite{MF}, we include these ratings as input to the LLMs. In this step, the user's actual ratings are used as supervision, ensuring that the LLMs learn the inherent reasoning process that connects the generated reasons to the user's ratings. 
Reason4Rec finally introduces a predictor for user feedback prediction.  
In this work, we investigate the classic rating prediction task~\cite{deepconn}.
The input for the predictor is the reasoning process generated by the summarizer and reasoner. 
In addition, considering that the user's historical ratings serve as a valuable reference for rating prediction~\cite{MF}, we also include ratings as input. 
The user's actual rating for the target item $r_{ui}$ is used for supervision. 
% , forcing the predictor to align with users' ratings. 
% ensuring that the LLMs learn the inherent reasoning process that connects the generated reasons to the user's ratings. 


\vspace{3pt}
% \noindent$\bullet\quad$\textbf{Prompt Construction.} 
\noindent$\bullet\quad$\textbf{Predictor Instantiation.} 
% Given a user-item pair $(u, i)$ with historical interaction $(r_{ui}, c_{ui})$, we first aggregate the aspect-preference summaries from their historical interactions $\mathcal{H}_u$ and $\mathcal{H}_i$ in the same format as in Step 2, supplemented with historical ratings. 
Given a user-item pair $(u, i)$ with historical interactions $(\mathcal{H}_u, \mathcal{H}_i)$, we first aggregate the aspect-preference summaries from $\mathcal{H}_u$ and $\mathcal{H}_i$ in the same format as in the reasoner prompt, supplemented with historical ratings (see the following predictor prompt). 
Next, following previous work~\cite{expert}, we include the average ratings from all historical interactions in the prompt. 
Finally, the reason generated by the reasoner is appended to the \textit{Reason Placeholder}. 
The final prompt is shown below to predict the user ratings. To implement the predictor, we apply another QLoRA on the LLM and utilize the rating $r_{ui}$ for SFT. 
% with the newly introduced elements highlighted in red for emphasis:

\begin{center}
\fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{
\centering % 添加居中命令
\textbf{Predictor Prompt} \\[2pt]
\raggedright % 恢复左对齐
\textbf{\#\#\# User Review History \#\#\#} \\
$\langle$\textit{$\mathcal{H}_u$ is organized in the same format as in the reasoner prompt, except that historical ratings are additionally included.}$\rangle$ \\[3pt]
1. Title of Item 1, {\textbf{Rating:} 5.0} \\
Positive Aspects: [Aspect 1], [Aspect 2], ... \\
Negative Aspects: [Aspect 1], [Aspect 2], ... \\
User Preference Elements: [Preference 1], [Preference 2], ... \\
... \\[3pt]
\textbf{\#\#\# Item Review History by Other Users \#\#\#} \\
$\langle$\textit{$\mathcal{H}_i$ organized in the same format as above}$\rangle$ \\[3pt]
{\textbf{\#\#\# Average Past Ratings \#\#\#}} \\
User's Average Rating (all previous ratings): 4.5 \\
Item's Average Rating (all ratings by other users): 3.7 \\[3pt]
% \textcolor{red}
{\textbf{\#\#\# Personalized Recommendation Analysis \#\#\#}} \\
% $\langle$\textit{Reason derived from step 2 here}$\rangle$ \\[3pt]
$\langle$\textit{This section is referred to as \textbf{Reason Placeholder}, where we place the generated reason here.}$\rangle$ \\[3pt]
\textbf{Task:} Based on the above information, please predict the user's rating for $i$, (1 being the lowest and 5 being highest, directly give the rating without other content.) \\
\textbf{[Output Format]} Predicted Rating:\texttt{[Rating between 1 and 5]}
}}
\end{center}

\begin{algorithm}[t]
    \caption{Generation-then-filter strategy}
    \label{alg:reason_sampling}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \begin{algorithmic}[1]
    \Require A teacher model LLM, a reward model $R$, the reasoner prompt $P$, an iteration count $T$, a Hint with user's actual rating.

    \State Initialize a high-quality reason set $\mathcal{S}_{\text{high}} \leftarrow \emptyset$;
    \State Initialize a reason $s \leftarrow \text{LLM}(P)$;

    \ForAll{$t \in \{1, \dots, T\}$}
        \State Get the reason quality $\text{Score} \leftarrow R(s)$;
        \If{$\text{Score} = 1$}
            \State Save $\mathcal{S}_{\text{high}} \leftarrow s$;
            \State \textbf{break};
        \Else
            \State Regenerate a new reason with hint: $s \leftarrow \text{LLM}(P, \text{Hint})$;
        \EndIf
    \EndFor

    \Ensure the high-quality reason in $\mathcal{S}_{\text{high}}$.
    \end{algorithmic}
\end{algorithm}

\vspace{3pt}
\noindent$\bullet\quad$\textbf{Logit-weighted Decoding.} We adopt logit-weighted decoding for the final prediction, which transforms the predicted rating scores from discrete integer values to continuous scores---a method proven to offer greater precision and adopted in previous work~\cite{expert}.

Specifically, when decoding the rating token, the LLM first obtains the logit values for each possible rating token (\eg the logits for tokens ``1'', ``2'', ``3'', ``4'', and ``5''), denoted as \( l_1, l_2, \dots, l_5 \) (assuming the rating range is from one to five). 
These logits are then normalized into probabilities \( p_k \) for each rating \( k \) using the softmax function: $p_k = \frac{\exp(l_k)}{\sum_{j=1}^5 \exp(l_j)}, k \in \{1, 2, \dots, 5\}$. 
The final prediction is calculated as the expected value: $r_{ui} = \sum_{k=1}^5 k \cdot p_k$, where each possible rating is weighted by its corresponding probability.

\vspace{3pt}
\noindent$\bullet\quad$\textbf{Efficient Rating Prediction.} Since the LLM is instructed to output ratings in a fixed format, starting with the phrase ``Predicted Rating:'' as specified in the predictor prompt, we can skip decoding these predictable and invariant words. 
We directly decode the final rating using logit-weighted decoding. Only decoding the single rating token significantly reduces inference time and costs.

\subsection{Reward Model for Reason Judgment} \label{sec:reward}

We use a reward model to select high-quality reasons for the reasoner fine-tuning. 
Given candidate reasons and the corresponding target item reviews, the reward model assigns a score of 1 if the reason is high-quality and aligns with the target item review; otherwise, it assigns 0. 
% It learns the user’s preferences from the target item review and then predicts a rating. If using the candidate reason in place of the raw review preserves the prediction accuracy, we conclude that this reason captures the user’s true preferences, so it earns a score of 1. Otherwise, it is scored 0.

\vspace{3pt}
\noindent$\bullet\quad$\textbf{Reward Model Training.} 
% We employ an LLM as our reward model, training it to perform the rating prediction task using the prompt template from Step 3 of StepRec. 
% We fine-tune a llama3-8B-instruct model\footnote{\url{https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/}.} as the reward model
We fine-tune another QLoRA on the LLM as the reward model, which is trained to predict the user ratings based on the user review. 
The input is the predictor prompt template, where the \textit{Reason Placeholder} is filled by the target item review rather than the generated reasons. The reward model is trained to extract the user's true preferences from the target item review to enhance prediction accuracy. 

\textbf{Separate Training Data.} To prevent the reward model from predicting ratings solely by memorizing training data rather than capturing the user's true preferences from target item reviews, we separate the training data for the reward model and the reasoner. 
This separation ensures that the reward model is evaluated on unseen data, enabling a more reliable assessment of reason quality.
% \begin{center}
% \fcolorbox{black}{gray!6}{\parbox{0.98\linewidth}{
% \#\#\# User Review History \#\#\# \\
% \#\#\# Item Review History by Other Users \#\#\# \\
% \#\#\# Average Past Ratings \#\#\# \\
% \#\#\# Personalized Recommendation Analysis \#\#\# \\
% \textbf{\textcolor{red}{$\langle$\textit{Target item review here}$\rangle$}} \\[3pt]
% \textbf{Task:} Based on the above information, please predict the user's rating for $i$, (1 being the lowest and 5 being highest, directly give the rating without other content.)
% \textbf{[Output Format]} Predicted Rating:\texttt{[Rating between 1 and 5]}
% }}
% \end{center}

\vspace{3pt}
\noindent$\bullet\quad$\textbf{Reason Judgment.} 
% Since the reward model is trained to capture the user's true preferences from the target item's review to improve rating predictions, a reason that better aligns with the user's review on these preferences can achieve comparable prediction accuracy to directly using the raw review for prediction, as evidenced in Section X. 
Since the reward model is trained to capture the user's true preferences from the target item's review, a reason that better aligns with the user's review can achieve superior prediction accuracy, as evidenced in Section~\ref{sec:reasonable_of_reward}. 
% In some cases, it may even outperform the raw review because it contains less noise. 
Based on this insight, we define an evaluation score as:
\[
s_{\text{eval}} = \big| r_{ui} - r_{\text{reason}} \big| - \big| r_{ui} - r_{\text{review}} \big|
\]
where $r_{ui}$ represents the ground-truth rating for user $u$ and item $i$. $r_{\text{review}}$ and $r_{\text{reason}}$ denote the predicted ratings by the reward model based on the target item review and a generated reason. And $s_{\text{eval}}$ measures the prediction difference: 
\begin{itemize}[leftmargin=*]
    \item If $s_{\text{eval}} < 0$, it suggests that the reason surpasses the raw review in rating prediction performance. 
    \item If $s_{\text{eval}} > 0$, a smaller $s_{\text{eval}}$ indicates a closer performance between the reason and review, suggesting superior alignment. 
\end{itemize}
% \vspace{3pt}
% \noindent$\bullet\quad$\textbf{Trade-off between quantity and quality.}
By introducing a threshold $\tau$, we identify all reasons of $s_{\text{eval}} < \tau$ as high-quality ones with the evaluation score as 1. 
Treating $\tau$ as a hyperparameter can balance the trade-off between the quantity and quality of the reasons. 


% \vspace{3pt}
% \noindent$\bullet\quad$\textbf{Separate Training for Reward Model and StepRec.} 
% To prevent the reward model from predicting ratings solely by memorizing training data rather than capturing the user's true preferences from their target item reviews, we separate the training data for the reward model and Step 2. This separation ensures that the reward model is evaluated on unseen data, enabling a more reliable assessment of reason quality.