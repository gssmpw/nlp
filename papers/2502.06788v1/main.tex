% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{mlVecMat}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{algpseudocode}
\usepackage{pifont}
\usepackage{multirow,makecell}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{diagbox}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{color}
\usepackage{colortbl}
\usepackage{arydshln}
\usepackage{alltt}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{\aka}{\textit{a}.\textit{k}.\textit{a}}
\newcommand{\lc}[1]{{\color{purple}{$^\textbf{\emph{Long:}}$[#1]}}}
\newcommand{\gray}[1]{{\textcolor[RGB]{180,180,180}{#1}}}

\tcbuselibrary{most}
\tcbset{
  aibox/.style={
    width=\textwidth,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{EVEv2: Improved Baselines for Encoder-Free Vision-Language Models}
\newcommand*{\affaddr}[1]{#1} 
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}

\author{
Haiwen Diao\textsuperscript{1,2}\thanks{Equal contribution. $\dag$ Correspondence to \textit{wangxinlong@baai.ac.cn}.}\hspace{2.7mm}
Xiaotong Li\affmark[3,2]$^*$\hspace{1.5mm}
Yufeng Cui\affmark[2]$^*$\hspace{1.5mm}
Yueze Wang\affmark[2]$^*$\hspace{1.5mm}\\
Haoge Deng\affmark[4,2]\hspace{1.5mm}
Ting Pan\affmark[5,2]\hspace{1.5mm}
Wenxuan Wang\affmark[6,5,2]\hspace{1.5mm}
Huchuan Lu\affmark[1\dag]\hspace{1.5mm} 
Xinlong Wang\affmark[2\dag]\hspace{1.5mm}\\
\affaddr{
\affmark[1]DLUT\hspace{1.5mm}
\affmark[2]BAAI\hspace{1.5mm}
\affmark[3]PKU\hspace{1.5mm}
\affmark[4]BUPT\hspace{1.5mm}
\affmark[5]UCAS\hspace{1.5mm}
\affmark[6]CASIA}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%     Abstract    %%%%%%%%%%%%%%%%%%%%
\begin{abstract}
    Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. 
    We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones.
    After an in-depth investigation, we launch \textbf{EVEv2.0}, a new and improved family of encoder-free VLMs.
    We show that: 
    (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities.
    (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs.
    Through extensive evaluation, our \textbf{EVEv2.0} represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability.   
    Code is publicly available at: \href{https://github.com/baaivision/EVE}{\textcolor{blue}{https://github.com/baaivision/EVE}}.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%     Introduction    %%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Introduction}

With the recent rapid advancements in both large language models (LLMs)~\cite{VLM:GPT-4,TransF:LLaMA2,yang2024qwen2,TransF:InternLM2,TransF:Deepseekllm} and large vision models (LVMs)~\cite{TransF:ViT,CL:DINO,TransF:EVA,TransF:EVA-CLIP-18B,TransF:ViT-22B}, vision-language models (VLMs)~\cite{VLM:InternVL-1.5,VLM:qwen-vl,VLM:DeepSeek-VL,VLM:GPT-4v,VLM:Gemini,VLM:Claude3} have made remarkable strides, demonstrating impressive capabilities in multi-modal understanding and reasoning applications.
%
As illustrated in~\Cref{fig:motivation}(1), typical practice adopts pre-trained vision encoders to extract visual semantics, which are then translated into the text embedding space as Foreign language inputs for subsequent LLMs (\eg, BLIP~\cite{VLP:BLIPv2} and LLaVA~\cite{VLM:LLaVA}). 
%
In contrast, another representative branch transforms visual features from vision encoders' last layer across each layer of LLMs through cross-attention modules, like Flamingo~\cite{VLP:Flamingo} and LLaMA-3.2V~\cite{VLMs:LLama3.2}.
%
Thanks to well-aligned representations across modalities through large-scale contrastive learning~\cite{VLP:CLIP,TransF:Siglip,TransF:EVA-CLIP}, these studies can achieve promising performance and strong multi-modality capability after joint training. 
%
However, the inductive biases during visual pre-training, \eg, image resolution, aspect ratio, and semantic priors, limit the flexibility and applicability of the visual learning in diverse real-world scenarios~\cite{VLM:LLaVA-UHD,tong2024eyes,tong2024cambrian}. 

Unlike compositional VLMs, Fuyu~\cite{VLM:Fuyu-8b} takes the early step to explore monolithic VLMs at different scales, eliminating the requirements for pre-trained visual encoders, while EVE~\cite{VLM:EVE} first pioneers a transparent, efficient, and practical path for advancing encoder-free VLM direction.
%
Among them, PaliGemma~\cite{beyer2024paligemma} constructs an encoder-free VLM that demonstrates strong scaling efficiency while progressively approaching its encoder-based counterpart.
% 
With extensive training data and computing resources, Mono-InternVL~\cite{luo2024mono} narrows the gap and matches the performance of Intern-VL1.5~\cite{VLM:InternVL-1.5} starting with the same LLM capabilities.

\begin{figure*}[t]
    \vspace{-2mm}
    \centering 
    \includegraphics[width=0.98\linewidth,trim= 0 0 0 0,clip]
    {figures/motivation.pdf} 
    \caption{Overview of (1) diverse vision construction inside existing VLMs and (2) potential architecture variants of Encoder-Free VLMs.}
    \label{fig:motivation}
\end{figure*}

However, constructing encoder-free VLMs remains challenging, particularly in learning vision perception from scratch and reducing vision-language interference within a unified model.
To date, three solutions have been put forward:
(i) Visual feature supervision~\cite{VLM:EVE}; 
(ii) Incremental training recipes~\cite{VLM:EVE,chen2024solo,luo2024mono}; 
(iii) Mixture-of-Expert (MoE) detachment~\cite{lin2024moma,luo2024mono} in~\Cref{fig:motivation}(2c).
%
Nevertheless, we empirically discover that visual supervision can be substituted by large-scale, high-quality image-text datasets annotated by a powerful captioning engine.
%
During training, properly merging language and multimodal data helps mitigate knowledge-forgetting issues, while pressuring the development of multimodal capabilities.
%
From the view of VLM structure, decoupling partial visual functions from the unified model through a MoE design~\cite{bao2022vlmo,VLP:BEiTv3,li2024aria} aids in relieving vision-language interference to some extent. 
However, we discover significant weight shifts across various network layers between the VLMs and the original LLMs, revealing the insufficiency of the current decoupling degree. Notably, we further exploit a re-parameterize architecture in~\Cref{fig:motivation}(2b) for seamless LLM adaptation. Although yielding better gains over prototype EVE in~\Cref{fig:preliminary}(2a), it does not completely resolve the representational conflicts across modalities.

From the above observations, we launch \textbf{EVEv2.0}, a new and improved baseline for encoder-free VLMs.
\textbf{EVEv2.0} completely disentangles overall components and introduces modality-wise sparsity into one unified decoder-only backbone in~\Cref{fig:motivation}(2d). Such a Divide-and-Conquer architecture maximizes scaling efficiency in building visual perception while minimizing the negative influence on the LLM itself.
%
Besides, using an enhanced caption engine, we construct an effective and practical route for monolithic VLM research that facilitates data-scaling efficiency and stably transfers increasingly stronger LLMs into the encoder-free VLMs.
%
With 100M publicly available data, \textbf{EVEv2.0} outperforms encoder-free counterparts, and continually approaches encoder-based competitors of similar capacity across diverse vision-language benchmarks. 
%
Our \textbf{EVEv2.0} unveils valuable insights for developing scalable, native, and next-gen VLMs, paving a transparent roadmap for future research supported by larger training data and computational resources.


\begin{figure*}[t]
    \centering 
    \includegraphics[width=0.96\linewidth,trim= 0 0 0 0,clip]
    {figures/preliminary.pdf} 
    \caption{Preliminary analyses across various VLMs' scaling efficiency during pre-training or fine-tuning (More details in~\Cref{sec:exp-details}). Notably, VE / DT / EVE apply varying image downsampling rates (14$^2$ / 8$^2$ / 32$^2$). For fairness, we choose slightly different resolutions that yield relatively balanced token counts of 576 / 1024 / 625 tokens per image. Besides, we quantify weight changes between LLMs and VLMs by averaging absolute value variation within specific layer number or type. We report accuracy on GQA~\cite{Datasets:GQA}, SEED~\cite{VLM:SEED}, TextVQA~\cite{Datasets:TextVQA}, and SQA~\cite{Datasets:ScienceQA} to examine VLMs’ capabilities across general in-domain, open-domain, OCR-related, and text-related knowledge tasks.
    }
    \label{fig:preliminary}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%     Related work    %%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:Related-work}

\textbf{Encoder-based VLMs.} 
Encoder-based methods have become the dominant approach in vision-language models, widely adopted in commercial products, \eg. GPT-4V~\cite{VLM:GPT-4}, Claude 3.5~\cite{VLM:Claude3}, and Gemini~\cite{VLM:Gemini}, as well as in the open-source projects like LLaVA series~\cite{VLM:LLaVA, VLM:LLaVA-1.5, VLM:LLaVA-1.6, Llava-onevision}, Qwen-VL series~\cite{VLM:qwen-vl,wang2024qwen2-vl}, InternVL series~\cite{VLM:InternVL, VLM:InternVL-1.5}, BLIP series~\cite{VLP:BLIP,VLP:BLIPv2,xue2024blip-3}, and EMU series~\cite{VLM:EMU,VLM:EMUv2}. They benefit from the pre-trained knowledge from visual encoders~\cite{VLP:CLIP,TransF:Siglip,TransF:EVA-CLIP,CL:DINO} and LLMs~\cite{TransF:LLaMA,TransF:LLaMA2,TransF:Vicuna,TransF:InternLM,TransF:InternLM2,TransF:Qwen,yang2024qwen2,qwen2.5}, successfully building modular VLMs for various real-world applications. Among them, most studies~\cite{VLM:mPLUG-Owl,VLM:mPLUG-Owl2,VLM:Monkey,VLM:LLaVA-UHD,VLM:Sharegpt4v,VLM:Densefusion} directly translate vision representations into the input space of LLMs.
%
In contrast, another type of research~\cite{VLM:Llama-Adapter,VLM:Llama-Adapterv2,VLM:CogAgent,VLP:Flamingo,VLMs:LLama3.2} introduces the cross-attention module for integrating visual and language information layer-by-layer.
%
Despite achieving strong performance gains, it may be insufficient to simply map visual information into the input space of LLMs~\cite{VLM:LLaVA-1.5,VLM:mPLUG-Owl2} or connect the same visual features across different representational levels of the LLM~\cite{VLM:CogAgent,VLM:IDEFICS}, given the heterogeneous characteristics between vision and language. 
%
Besides, these modular VLMs face challenges in further development due to the strong inductive biases in pre-training visual encoding patterns, complex infrastructure requirements, and scaling laws necessary to balance various separate components.



\noindent\textbf{Encoder-free VLMs.} 
Another visual processing strategy is discrete visual tokenization~\cite{van2017VQVAE,VQGAN,LLamagen}, which is widely used in various multi-modal understanding and generation approaches~\cite{team2024chameleon,VLM:AnyGPT,wang2024mio,VLM:EMU3}.
However, the discretization inevitably results in lossy visual information and weakens in extracting semantic contents, which in turn hinders fine-grained visual understanding and reasoning~\cite{xie2024show-o,wu2024janus}.
%
Therefore, recent studies~\cite{wu2024vila-u,li2024imagefolder,qu2024tokenflow,xie2024muse} introduce semantic constraints in the visual tokenizer for both high-level semantic and fine-grained visual representations.
%
Compared to their highly-compressed features in the discrete-value space, encoder-free VLMs~\cite{VLM:Fuyu-8b,VLM:EVE,chen2024solo} have emerged as a promising architecture for lossless visual encoding, efficient data scaling, and end-to-end multimodal processing. 
%
Specially, EVE~\cite{VLM:EVE} pioneers an efficient and transparent path for monolithic VLMs, with its data-scaling efficiency preliminarily validated by PaliGemma~\cite{beyer2024paligemma}. Impressively, Mono-InternVL~\cite{luo2024mono} bridges the performance gap with its modular counterpart~\cite{VLM:InternVL-1.5} of the same LLM capacity, using adequate data. We emphasize that limited by current training data and device resources, EVEv2.0 does not aim for state-of-the-art performance but instead focuses on revealing the most efficient route for encoder-free VLMs from scratch.

%%%%%%%%%%%%%%%%%%%%     Methodology    %%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:Methodology}

\subsection{Preliminary}
\label{subsec:preliminary}

\textbf{Pioneer Experiments.} Building on~\cite{VLM:LLaVA-1.5,VLM:EMU3,VLM:EVE}, we conduct two pilot experiments in~\Cref{fig:preliminary}. 
%
\textbf{Exp.(i):} we adopt Vicuna-7B~\cite{TransF:Vicuna} via the vision encoder (CLIP-ViT-L-336px~\cite{VLP:CLIP}), discrete tokenizer (Mo-VQGAN~\cite{VLM:movq}), or one single transformer block~\cite{TransF:Transformer} from scratch, dubbed \textit{VE}, \textit{DT}, or EVEv1.0 (w/o visual supervision). 
%
We first train the projector, vision vocabulary, or vision block using EVE-cap-16M~\cite{VLM:EVE} caption data, followed by EVE-cap-33M~\cite{VLM:EVE} to update only vision encoder for \textit{VE} (work best), or extra LLM weights for \textit{DT} and EVEv1.0, respectively. 
%
Finally, we update the overall backbone except Mo-VQGAN during instruction-tuning. 
%
\textbf{Exp.(ii):} we employ stronger Qwen2-7B~\cite{TransF:Qwen} as the LLM for \textit{VE} and EVEv1.0, compared with EVEv1.2 in~\Cref{fig:motivation}(2b) with patch embedding layer.
%
Using 29M image-text annotations, we initially train the projector for \textit{VE} or visual layers for EVE, with further updates to EVE’s LLM layers via extra 48M data. Subsequently, we import varying-scale instruction data~\cite{VLM:LLaVA-1.5,VLM:EVE,Llava-onevision} to fine-tune the entire network.

\textbf{Finding (1):} \textit{Performance gap between various vision encoding modes.} Exp.(i) shows that initially, \textit{VE} performs better than \textit{DT} and EVEv1.0 in visual understanding due to much larger image-text pretraining datasets (400M) and already alignment space between vision-language embeddings.
%
Despite building visual recognition from scratch, EVEv1.0 demonstrates strong scaling properties,
progressively closing the performance gap with \textit{VE} as the data scale increases. Subsequent studies~\cite{beyer2024paligemma,luo2024mono} have proved that, with sufficient data, they can achieve comparable performance.
%
Notably, \textit{DT} maps visual information into a discrete space through quantization, hampering effective visual perception and weakening vision-language association via an image reconstruction objective. This ultimately results in subpar performance, even at larger data scales, leaving \textit{DT} less competitive overall.

\begin{figure*}[t]
    \centering 
    \includegraphics[width=0.98\linewidth,trim= 0 0 0 0,clip]
    {figures/framework.pdf} 
    \caption{Overview of our proposed EVEv2.0 framework. We first adopt a patch embedding layer to encode images losslessly, and then concatenate visual and textual tokens into a unified decoder-only vision-language model. Here, it extends the standard autoregressive transformer by incorporating modality-specific weights for each multi-head self-attention layer, feed-forward layer, and layer normalization.}
    \label{fig:framework}
\end{figure*}

\textbf{Finding (2):} \textit{Challenges towards multimodal interference and smooth transition.}
%
Exp.(ii) shows that compared with EVEv1.2 using the same data and stage, EVEv1.0 struggles to approach \textit{VE} in text-related knowledge (SQA-IMG) due to catastrophic linguistic forgetting in LLMs. 
%
While mixing text-only and multi-modal data slightly alleviates this issue, it slows down the development of multi-modal understanding  in~\Cref{fig:mixed_data_ratio}.
%
Hence, we explore potential architectures and training strategies targeting LLM adaptation and multimodal interference. EVEv1.2 with re-parameterization design for each feed-forward weight helps smooth the transition from LLMs to VLMs, while EVEv1.5 with MoE design helps decouple vision-language encoding heterogeneity.
%
However, after comparing LLMs and VLMs, we observe that \textit{VE} requires only minor LLM adjustments to achieve robust results, whereas EVE necessitates extensive updates for similar capabilities in~\Cref{fig:preliminary}. 
%
Besides, Layer normalization stands out as the most impacted module, indicating that EVEv1.2 and EVEv1.5 face challenges in efficiently constructing visual representations and achieving multimodal alignment when LLM weights are fixed in~\Cref{fig:different_version}. This dependency on the pre-training paradigm of LLM limits the VLMs' full potential for learning visual perception from scratch.

\subsection{Model Architecture}
\label{subsec:model_architecture}

Preliminary studies indicate that earlier EVE variants struggle to fully harness visual potential due to cross-modal interference within the pre-trained LLM distribution. To overcome this, we transform a dense transformer into a fully sparse, decoder-only architecture, guided by modality-aware routers in~\Cref{fig:framework}. This approach yields a heterogeneous, modality-mixing model that retains the computational structure and FLOP count of its dense transformer counterpart.

\textbf{Visual and Textual Encoding}.
For visual embeddings, we construct a minimalist patch embedding layer from scratch, eliminating strong inductive bias from pre-trained vision encoders in abstracting visual content.
%
Given an image input $I\in \mathbb{R}^{H\times W \times 3}$, we first employ a Convolution layer (Conv1), followed by a Gaussian Error Linear Unit (GELU) activation function. After obtaining the resulting 2-D feature map, we then adopt another Convolution layer (Conv2) to flexibly control computational complexity as follows: 
\begin{equation}
\label{eq:patch_embedding_layer}
x_{v}=\text{Conv2}(\text{GELU}(\text{Conv1}(I))),
\end{equation}
where $\text{Conv1}$ and $\text{Conv2}$ denote two convolutional layers with strides of 16 and 2, and output dimensions of 1024 and 3584, respectively. 
Besides, two learnable special tokens serve as the prompts for image start and line feed. The class token $<\!\text{CLS}\!>$ is appended at the beginning of the image sequence, while the split tokens $<\!\text{SPL}\!>$ are inserted after each row of image tokens for indicators.
Such a patch embedding layer supports arbitrary-ratio images with up to about 2.5M pixels, \ie, 2.5K patch tokens. Afterward, we adopt the text tokenizer from Qwen2.5~\cite{qwen2.5} to encode text $T$ into token embeddings $x_{t}$ with a dimension of 3584.

\begin{figure*}[t]
    \centering 
    \includegraphics[width=0.98\linewidth,trim= 0 0 0 0,clip]
    {figures/train_stage.pdf} 
    \caption{Overview of training procedure. PEL/WEL denotes patch/word embedding layer. We begin by training the patch embedding layer to establish initial alignment across modalities. Afterward, we only update vision layers within the LLM to enhance visual perception progressively. Notably, we gradually increase the image resolutions from 800$\times$800 to 1600$\times$1600 and keep the original image aspect ratio. Finally, we train the entire model via QA and instruction data to strengthen cross-modality correspondence and complex understanding.}
    \label{fig:train_stage}
    \vspace{-2mm}
\end{figure*}

\textbf{Divide-and-Conquer Design.} 
Building on prior analyses, we propose explicitly decoupling key modules by introducing modality-aware components, including separate attention matrices (query, key, and value), normalization layers, and feed-forward modules, each with distinct parameters. 
%
Given the token sequence $x=(x_1, ..., x_n)$ where $x_i$ belongs to one specific modality $u_{i}\in \{v, t\}$, we perform a multi-head self-attention (ATTN) across all modalities, modeling cross-modal relationships in a unified feature space:
\begin{equation}
\begin{split}
\label{eq:divide_and_conquer}
\text{ATTN}(x;\{\theta_\text{attn}^u\}) &= \left(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\right)W_O^{u_{i}}, \\
Q_{i} = x_{i} W_Q^{u_{i}}&, K_{i} = x_{i} W_K^{u_{i}}, V_{i} = x_{i} W_V^{u_{i}},
\end{split}
\end{equation}
where modality-specific query, key, and value are derived from their respective attention weight matrices $W^{u_{i}}, u_{i}\in \{v, t\}$. 
%
The interaction process is performed across modalities, \ie, visual $x_v$ and textual $x_t$ sets. 
%
Inspired by~\Cref{fig:preliminary}, the significant quantified weight changes highlight the importance of decoupling the LayerNorm (LN) and Feed-Forward (FFN) layers; otherwise, they cause representational interference, limiting mutual capacities and capabilities. After fully decoupling the architecture, the overall operations within the Transformer block are defined as follows:
\begin{equation}
\begin{split}
    h&=x+\text{ATTN}(
    \text{LN1}(x;{\theta^u_{\text{ln1}}});
    \{\theta_\text{attn}^u\}),
    \\
    x^{\prime}&=h+\text{FFN}(
    \text{LN2}(x;{\theta^u_{\text{ln2}}});
    \theta_{\text{ffn}}^u).
\end{split}
\end{equation}
Compared with earlier EVE variants, EVEv2.0 employs a comprehensive decomposition with modality-specific components.
%
This minimizes interference in the representation space by fully unbinding each layer and processing token sets separately for each modality. The structural decomposition supports efficient vision-perception training from scratch while retaining pre-trained knowledge by freezing off-the-shelf LLMs during pretraining. This also allows independent single-modality encoding and cross-modality correspondence across different layers simultaneously, enabling flexible modeling patterns for understanding and reasoning.

\subsection{Training Procedure}
\label{subsec:training_procedure}

We divide the training process into four sequential stages in~\Cref{fig:train_stage}. The training data consists of publicly available image datasets, along with diverse question-answering (QA) datasets and multimodal dialogue data in~\Cref{tab:training_dataset}.

\textbf{DenseFusion++.} 
Developing strong visual perception requires high-quality, highly-detailed image-text data. Hence, we propose an efficient and low-budget engine based on LLaVA-1.6 (7B)~\cite{VLM:LLaVA-1.6} for scaling up synthetic data.
%
Specifically, it leverages multiple vision experts (Tag, Detection, OCR, ...) and learns GPT-4V’s fusion strategy, greatly facilitating data-scaling efficiency for native VLMs with high-quality annotations, not just distilling LLaVA.
%
In~\Cref{fig:data_source}, we empirically demonstrate the superior training efficiency achieved by using intensive annotations from DenseFusion++ during pre-training. It consistently outperforms raw web-sourced captions or those generated by lower-quality caption engines. We will release the \href{https://github.com/baaivision/EVE/EVEv2/docs/Data.md}{\textcolor{blue}{caption engine}} code and weight
for better interpretation and further exploration.

\textbf{LLM-guided Pre-aligning.}
Following~\cite{VLM:EVE}, we freeze the LLM weights and train only the patch embedding layer to prevent model collapse and accelerate convergence in subsequent stages.
%
Using publicly available web data, we filter 44M image samples from Datacomp~\cite{Datasets:datacomp} recaptioned with our captioning engine. 
%
For training, we utilize a subset of 10M image-text pairs, dubbed \textit{EVE-recap-10M}, and optimize with cross-entropy (CE) loss. 
%
Our experiments suggest that more extensive training at this stage is beneficial for training stability, especially considering stronger LLMs.

\begin{table}[!t]
    \centering
    \caption{Details of training datasets across all stages. Note that we construct DenseFusion++ to re-caption web-scale image-text data.}
    \label{tab:training_dataset}
    \scalebox{1}{
    \begin{tabular}{c|c|c|c}
        \toprule
        Stage &Dataset &\#Num &Total\\
        \midrule
        \multirow{4}{*}{1 / 2.1} &Datacomp~\cite{Datasets:datacomp} &44M &\multirow{4}{*}{77M} \\
        &LAION~\cite{Datasets:Laion-5b} &15M &\\
        &SA-1B~\cite{TransF:SAM} &11M &\\
        &OpenImages~\cite{Datasets:OpenImages} &7M &\\
        \midrule
        2.2 &Infinity-MM-GeneralQA~\cite{gu2024infinity} &15M &15M\\
        \midrule
        \multirow{2}{*}{3} &LLaVA-onevision~\cite{Llava-onevision} &3.5M &\multirow{2}{*}{7.3M}\\
        &Infinity-MM-instruct~\cite{gu2024infinity} &3.8M &\\
        \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{Vision Perception Learning.}
In this stage, we initialize the vision layers inside the LLM by loading LLM weights, where only the patch embedding and vision layers are trainable, while the Qwen2.5~\cite{qwen2.5} model remains frozen during training.
%
This strategy enables efficient learning of visual representations through pre-training on large-scale synthetic data, without compromising the knowledge encoded in the pre-trained LLM. Besides, we carefully partition the training data into a progressive, coarse-to-fine visual learning process. 
%
For training, we first introduce 29M re-captioning data from Datacomp~\cite{Datasets:datacomp} supervised by CE loss with a maximum of 640K image pixels (\ie, 625 patch tokens). Afterward, we increase the maximum image resolution to 2.5M pixels (\ie, 2.5K patch tokens) on an expanded dataset comprising 15M Datacomp~\cite{Datasets:datacomp}, 15M LAION~\cite{Datasets:Laion-5b}, 11M SA-1B~\cite{TransF:SAM}, and 7M OpenImages~\cite{Datasets:OpenImages}, dubbed \textit{EVE-recap-48M}.

\textbf{Vision-Text Fully-aligning.}
After establishing an initial alignment across modalities, we update the entire architecture to further improve image-text associations via the same loss functions. 
%
To facilitate this, we curate a diverse dataset of 15M samples from Infinity-MM general visual instruction~\cite{gu2024infinity}, including chart comprehension, OCR recognition, mathematical reasoning, and \etc. This dataset, named \textit{EVE-multi-task-15M}, enhances visual perception and vision-language alignment, equipping EVE with the foundational capabilities to handle various multimodal tasks.


\begin{table*}[th]
    \vspace{-2mm}
    \caption{Comparison with existing vision-language models on various vision-language benchmarks, including MMMU~\cite{Datasets:MMMU};
    MMB$^\text{en}$: MMBench-EN~\cite{Datasets:MMBench};
    SEED$^\text{I}$: SEEDBench-Img~\cite{Datasets:Seed-bench};
    MMV: MMVet~\cite{Datasets:MM-vet};
    MME~\cite{Datasets:MME};
    POPE~\cite{Datasets:POPE};
    GQA~\cite{Datasets:GQA};  
    SQA$^\text{I}$: ScienceQA-Img~\cite{Datasets:ScienceQA}; 
    TVQA: TextVQA~\cite{Datasets:TextVQA};   
    CQA: ChartQA~\cite{Datasets:ChartQA};
    AI2D~\cite{Datasets:AI2D};
    RWQA: RealWorldQA~\cite{Datasets:Realworldqa};
    OCRB: OCRBench~\cite{Datasets:OCRBench}.
    Note that \#A-Param denotes the number of activated parameters;
    \#Data represents the pre-training / fine-tuning data volume;
    \#Vtoken indicates the maximum image patch tokens.
    For MME, we sum up the perception and cognition scores.
    The best two results are marked in \textbf{bold} and \underline{underline}.
    }
    \label{tab:multimodal_benchmark}
    \vspace{-2mm}
    \centering
    \setlength{\tabcolsep}{0.075cm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l ccc| cc ccccc cccc cc}
        \toprule
        Method & \#A-Param &\#Data &\#Vtoken
        & MMMU & MMB$^\text{en}$ 
        & SEED$^\text{I}$ & MMV & MME & POPE
        & GQA & SQA$^\text{I}$ & TQA & CQA 
        & AI2D & RWQA & OCRB \\
        \midrule
        \rowcolor{gray!17}
        \multicolumn{4}{l|}{\emph{Encoder-based Vision-Language Models:}} &\multicolumn{13}{l}{}\\
        InternVL-1.5
        & 2.2B &-- / -- &3328
        & 34.6 &70.9
        & 69.8 & 39.3 &\underline{1902} &\textbf{88.3}
        & 61.6 & \underline{84.9} &\underline{70.5} &\underline{74.8}
        &69.8 &-- & \textbf{654}
        \\
        QwenVL-Chat
        & 7B &7.2B / 50M &256
        & 35.9 & 60.6 
        & 58.2 & -- &1848 &--
        & 57.5 & 68.2 & 61.5 & 49.8 
        & 45.9 & 49.3 & 488
        \\
        LLaVA-1.5
        & 7B &0.4B+ / 665K &576
        & 35.3 & 64.3 
        & 64.3 & 30.5 &1859 &85.9
        & 62.0 & 66.8 & 46.1 & 18.2 
        & 54.8 & 54.8 & 318
        \\
        LLaVA-1.6
        &7B &0.4B+ / 760K &2880
        & 35.1 & 67.4 
        & 64.7 & \underline{43.9} &1842 &\underline{86.4}
        & \underline{64.2} & 70.2 & 64.9 & 54.8 
        & 66.6 & 57.8 & \underline{532} 
        \\
        Cambrian 
        &7B &10B+ / 7M &576
        &\underline{42.7} &\underline{75.9} 
        &\underline{74.7} &--  &-- &--
        &\textbf{64.6} &80.4 &\textbf{71.7} &73.3
        &\underline{73.0} &\underline{64.2} &--
        \\
        LLaVA-OV 
        &7B &10B+ / 3.2M &7290
        &\textbf{47.3} &\textbf{81.7}  
        &\textbf{74.8} &\textbf{58.8} &\textbf{1998} &--
        &-- &\textbf{96.6} &-- &\textbf{78.8}
        &\textbf{81.6} &\textbf{65.5} &--
        \\
        \midrule
        \rowcolor{gray!17}
        \multicolumn{4}{l|}{\emph{Encoder-free Vision-Language Models:}}  &\multicolumn{13}{l}{}\\
        Fuyu 
        & 8B &-- / -- &--
        & 27.9  & 10.7 
        & --   & 21.4 &-- &--
        & --   & -- & -- & --  
        & 64.5 & -- & --
        \\
        Chameleon
        &7B &1.4B+ / 1.8M &1024
        &25.4 &31.1   
        &30.6 & 8.3 &170 &--
        & --  & 47.2 &4.8 &2.9
        &46.0 & -- & 7.0 
        \\
        EVE
        &7B &33M / 1.8M &2304
        & 32.6 & 52.3
        & 64.6 & 25.7 & 1628 & 85.0  
        & \underline{62.6} &64.9 & 56.8 & 59.1  
        & 61.0 & -- & 398
        \\
        SOLO
        & 8B &43.7M / 2M &1024
        & --  &  --
        & 64.4 & -- & 1260 &--
        & --   &73.3   & -- & --  
        & 61.4 & -- & --  
        \\
        Mono-InternVL
        & 1.8B &1.3B / 7M &6400
        & \underline{33.7}  & \underline{65.5}
        & 67.4  & \underline{40.1} & \textbf{1875} &--
        & 59.5 &\underline{93.6}  &\textbf{72.6}  & \underline{73.7}  
        & 68.6 & -- & \textbf{767}
        \\
        Emu3
        & 8B &-- / -- &16K
        & 31.6 & 58.5
        & \underline{68.2} & 37.2 & -- & \underline{85.2}
        & 60.3 & 89.2 & 64.7 & 68.6 
        & \underline{70.0} & \underline{57.4} & 687 
        \\
        \textbf{EVEv2.0}
        & 7B &92M / 7.3M &2500
        &\textbf{39.3} &\textbf{66.3} 
        &\textbf{71.4} &\textbf{45.0} & \underline{1709} & \textbf{87.6}
        &\textbf{62.9} &\textbf{96.2} &\underline{71.1} &\textbf{73.9}
        &\textbf{74.8} &\textbf{62.4} &\underline{702}
        \\
        \bottomrule
        \\
        \end{tabular}
        }
    \vspace{-2mm}
\end{table*}

\textbf{Supervised Fine-tuning.}
During the SFT stage, we further enhance EVE's ability to understand complex linguistic instructions and multifarious dialogue patterns, which are crucial for real-world applications. 
%
Here, we optimize the overall network architecture on a diverse set of high-quality, multi-source instruction datasets, namely \textit{EVE-sft-7M}, including LLaVA-onevision~\cite{Llava-onevision} and partial Infinity-MM-instruct~\cite{gu2024infinity}. 
%
Notably, Stages 2.2 and 3 can be merged if large, balanced, and high-quality SFT data is available. We separate them to
handle diverse but uneven (Stage 2.2) and balanced (Stage 3) data to achieve consistent performance.


%%%%%%%%%%%%%%%%%%%%     Experiment    %%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:Experiments}

\subsection{Training Settings}

\textbf{Data Preparation.} 
All the training data is collected from publicly accessible sources to ensure reproducibility.
%
\textit{(1) Image-Text Datasets.} We follow the pre-processing pipeline outlined in~\cite{VLM:EVE} to process SA-1B~\cite{TransF:SAM}, OpenImages~\cite{Datasets:OpenImages}, and LAION~\cite{Datasets:Laion-5b}, resulting in a total of about 33M samples. For Datacomp~\cite{Datasets:datacomp}, we curate the images with resolutions greater than $512 \times 512$, using DenseFusion++ to obtain 44M high-quality image descriptions and abandon samples with repetitive text or incomplete sentences.
%
\textit{(2) Question-answering and Instruction-following Datasets.} We clean out 15M QA data from Infinity-MM-GeneralQA~\cite{gu2024infinity} in its Stage-2. Meanwhile, we collect a blended set of the LLaVA-onevision~\cite{Llava-onevision} and partial Infinity-MM-instruct~\cite{gu2024infinity} from its original Stage-3/4 for complicated conversation patterns.

\textbf{Implementation Details.}
We use sixteen 8-A100 (40G) nodes to train EVEv2.0 using AdamW optimizer~\cite{Training:Adam}. For Stage 1, 2.1, 2.2, and 3, the batch sizes are 1024, 1024, 512, and 512, while the maximum learning rates are set to $2 \times 10^{-4}$, $1 \times 10^{-4}$, $2 \times 10^{-5}$, and $1 \times 10^{-5}$. We adopt warm-up strategy with the ratio of 0.03 and cosine decay scheduler across all stages. Unless otherwise stated, we set image resolutions as 800$^2$ and report fine-tuned results by LLaVA-mix-665K~\cite{VLM:LLaVA-1.5} for Stage 1/2.1/2.2 in~\Cref{sec:ablation_studies}.

\subsection{Main Results}
\label{subsec:main_results}

We conduct standard evaluations using the LMMs-Eval~\cite{zhang2024lmms-eval} across various vision-language benchmarks, including (1) Chart, Diagram, and Document Understanding tasks: OCRBench~\cite{Datasets:OCRBench}, ChartQA~\cite{Datasets:ChartQA}, and AI2D~\cite{Datasets:AI2D}; 
%
(2) Visual Perception and Challenging Reasoning tasks:
MMMU~\cite{Datasets:MMMU}, MMBench-EN~\cite{Datasets:MMBench}, SEEDBench-Img~\cite{Datasets:Seed-bench}, MMVet~\cite{Datasets:MM-vet}, MME~\cite{Datasets:MME}, POPE~\cite{Datasets:POPE}, GQA~\cite{Datasets:GQA}, ScienceQA-Img~\cite{Datasets:ScienceQA}, and TextVQA~\cite{Datasets:TextVQA}.
%
All the results are reported with greedy decoding and zero-shot settings, unless otherwise stated.

From~\Cref{tab:multimodal_benchmark}, EVEv2.0 surpasses the encoder-free counterparts, \eg Fuyu~\cite{VLM:Fuyu-8b}, EVE~\cite{VLM:EVE}, SOLO~\cite{chen2024solo}, \etc across various vision-language benchmarks. 
%
Note that, due to training data and device resource limitations, we have not yet trained smaller models under 2B parameters in parallel and, as such, cannot provide a more direct comparison with Mono-InternVL~\cite{luo2024mono}. However, we emphasize that our Divide-and-Conquer architecture demonstrates superior data-scaling efficiency compared to conventional Mixture-of-Experts designs, as shown in~\Cref{fig:different_version}. Preliminary experiments in~\Cref{fig:preliminary} reveal that simply decoupling the feed-forward module is insufficient to resolve vision-language conflicts and module compatibility inside one single unified network. 

\begin{figure*}[t]
    \begin{minipage}[t]{0.44\textwidth}
    \centering 
    \includegraphics[width=\linewidth,trim= 0 0 0 0,clip]
    {figures/eve_loss.pdf} 
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.54\textwidth}
    \centering 
    \includegraphics[width=\linewidth,trim= 0 0 0 20,clip]
    {figures/different_version.pdf} 
    \end{minipage}
    \caption{Training loss curve and evaluation results in Stage 2. We adopt various EVE variants based on Qwen-2.5~\cite{qwen2.5} as the baseline. We first train the patch embedding layer using \textit{EVE-recap-10M} in Stage 1, and further unfreeze vision layers except LLM layers in Stage 2.}
    \label{fig:different_version}
\end{figure*}

\begin{figure*}[t]
    \centering 
    \includegraphics[width=\linewidth,trim= 0 0 0 20,clip]
    {figures/data_source.pdf} 
    \caption{Evaluation results of different data sources and caption engines. We utilize EVEv1.0 based on Vicuna-7B~\cite{TransF:Vicuna} as the baseline.
    Here ''*-raw``, ''*-cap``, or ''*-recap`` denote noisy web image captions, the samples annotated by both LLaVA-1.5 (13B) and Emu2 (17B), or modified DenseFusion++ (7B), respectively. Note that ''L.O.S.`` represents the mixture of LAION~\cite{Datasets:Laion-5b}, OpenImages~\cite{Datasets:OpenImages}, and SAM~\cite{TransF:SAM}.}
    \label{fig:data_source}
\end{figure*}

Besides, EVEv2.0 displays superior performance against the VLMs using discrete tokenizers, \ie Chameleon~\cite{team2024chameleon} and Emu3~\cite{VLM:EMU3}, despite being trained on significantly fewer data or utilizing fewer visual tokens. This further validates the efficiency and effectiveness of encoder-free VLMs with lossless visual encoding mode, even using a lightweight patch embedding layer from scratch. 
%
Notably, EVEv2.0 competes with popular and mainstream encoder-based VLMs, \eg LLaVA-1.6~\cite{VLM:LLaVA-1.6} and Cambrian~\cite{tong2024cambrian}. We argue that the performance gap between EVEv2.0 and advanced modular VLMs primarily arises from the significant discrepancy in data magnitude, restricting encoder-free VLMs from constructing a generalizable vision perception capability and handling more complicated visual perception scenarios.

\subsection{Ablation Studies}
\label{sec:ablation_studies}

\textbf{Divide-and-Conquer (DaC) design outperforms the re-parameterization (ReP) and mixture-of-experts (MoE).} 
As shown in~\Cref{fig:different_version}, (1) the training process of EVEv1.0 is the slowest, and training only the patch embedding layer proves insufficient, resulting in minimal performance gains.
%
(2) EVEv1.2 (Rep) shows a rapid loss decrease, which can be attributed to the gradual transfer of LLMs into the initial VLMs by updating the pre-trained LLM weights. However, this approach leads to a noticeable performance drop on the SQA-IMG task requiring abundant text-related knowledge.
%
(3) In contrast, EVEv1.5 (MoE) only updates visual parameters inside frozen LLMs to effectively mitigate catastrophic forgetting issues during pre-training.
However, solely decoupling FFN modules restricts distinct feature distributions across modalities, resulting in less-efficient improvements in visual perception and multi-modality alignment.
%
(4) With prior validation support, EVEv2.0 (DaC) achieves optimal improvements across all multi-modal benchmarks, highlighting its superior data-scaling efficiency during large-scale pre-training. This success can be attributed to its modality-wise sparsity, which effectively preserves linguistic knowledge while providing greater flexibility for visual learning.
%
This philosophy is further evidenced by the loss curve in~\Cref{fig:different_version} with faster convergence and better training stability than EVEv1.5 during pre-training. 
%
Notably, their Avg. accuracy gap rises from 0.8\% to 1.4\% as the training data grows from 8M to 24M, a trend likely to hold for other model sizes and data sources. Besides, only decoupling LayerNorm yields the Avg. accuracy of 48.8\% vs. 51.6\% for EVEv2.0 using 8M data, necessitating the complete decomposition.


\textbf{Fully-upgraded captioning engine facilitates training efficiency and model capabilities than prior competitors.} 
As illustrated in~\Cref{fig:data_source}, (1) web-scale image-text data often suffers from excessive noise and overly brief descriptions, which results in slow progress in visual content understanding and significantly pollutes LLM's pre-training knowledge. In contrast, using a powerful captioning engine to build high-quality, hyper-detailed image annotations proves essential for efficiently developing visual perception from scratch. Our modified DenseFusion++ (7B) outperforms previously adopted models like LLaVA-1.5 (13B) and Emu2 (17B) in this regard. Moreover, our model offers an additional advantage: its efficient and low-budget nature, capable of generating 700K descriptions per day with just a single 8-A100 (40G) node, accelerated by SGlang~\cite{VLM:SGlang}.
(2) A multi-source data mixture can significantly facilitate the visual training process. Our filtered LAION~\cite{Datasets:Laion-5b}, OpenImages~\cite{Datasets:OpenImages}, SAM~\cite{TransF:SAM} provide OCR-related images, real-world scenarios, and abundant image content, respectively. Together, these data mixtures can enhance the capability of VLMs to handle diverse image inputs, promoting the development of a more robust and versatile visual perception.

\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth,trim= 0 0 0 20,clip]
    {figures/mixed_data_ratio.pdf} 
    \caption{Evaluation results of mixed data ratio. We adopt EVEv1.0 with Vicuna-7B~\cite{TransF:Vicuna} for validation. Note that x:y:z denote the proportion of synthesized data : language-only data : web data.}
    \label{fig:mixed_data_ratio}
\end{figure}

\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth,trim= 0 0 0 20,clip]
    {figures/image_encoding.pdf} 
    \caption{Evaluation results of image settings. 
    We use EVEv1.0 with Vicuna-7B~\cite{TransF:Vicuna}.
    ``AnyRatio\_maxL'': longest image edge as 800, 
    ``AnyRatio\_LD'': fixed image area as 800$^2$,
    ``AnyRatio\_HD'': fixed image area as 1600$^2$,
    ``AnyResolution'': arbitrary resolution. }
    \label{fig:image_encoding}
\end{figure}

\textbf{Meticulously adjusting data proportional distribution facilitates the balanced improvements across modalities.} 
We explore maintaining pre-trained LLM knowledge from a data mixture perspective in~\Cref{fig:mixed_data_ratio}. Striking the right balance is crucial for achieving robust multimodal capabilities without significantly sacrificing language performance. However, this balance is delicate and often influenced by various factors, \eg image resolution, dataset composition, text sources, and the type of language model. In this paper, we address multi-modality compatibility from the model structure perspective, leveraging overall multi-modal synthesized data during pre-training. Nonetheless, we believe that combining these two could yield even greater benefits.

\textbf{Flexible image processing mode of encoder-free VLMs.}
In~\Cref{fig:image_encoding}, we explore four different input formats. Among them, ``AnyRatio\_HD'' (standard) provides the best performance gains, while ``AnyResolution'' performs poorly in the early stages but shows improved data-scaling efficiency over data scales. This early underperformance is likely due to limited pre-training data and the imbalance in image resolutions. We believe that with sufficient and well-balanced data, arbitrary-resolution inputs can offer better computational efficiency and greater flexibility for real-world images.

\textbf{Incremental benefits across different training stages.} 
To thoroughly investigate progressive training recipes, we present the training dynamics with detailed recipes in~\Cref{tab:training_dataset}.
From~\Cref{fig:training_process}, we observe continual improvements with increasing pre-training data scales, which are further enhanced by carefully organized question-answering datasets.
After meticulously curating instruction-tuning datasets, our EVEv2.0 ultimately achieves superior multi-modality capabilities to handle various real-world application scenarios.

\begin{figure}[t]
    \centering 
    \includegraphics[width=0.98\linewidth,trim= 0 0 20 20,clip]
    {figures/training_process.pdf} 
    \caption{Evaluation results across progressive training procedure in~\Cref{fig:training_process}. We adopt standard EVEv2.0 based on Qwen-2.5~\cite{qwen2.5}.}
    \label{fig:training_process}
\end{figure}


%%%%%%%%%%%%%%%%%%%%     Conclusion    %%%%%%%%%%%%%%%%%%%%
\section{Limitation and Discussion}
EVEv2.0 has systematically explored network architectures and training strategies for efficiently constructing encoder-free VLMs. Due to limitations in extensive high-quality data and computational devices, its full potential remains unrealized, thereby restricting performance on specific tasks, \eg knowledge- and document-oriented benchmarks. Besides, several promising directions remain for further exploration and improvement: Model Scaling, Data Scaling, and Modalities Expanding (\eg audio and video). We hope EVEv2.0 inspires further research on scaling laws for encoder-free VLMs with much more computational resources.

\section{Conclusion}
\label{sec:Conclusion}

In this paper, we present EVEv2.0, a carefully designed and transparent encoder-free architecture designed for vision-and-language understanding and reasoning. Rather than focusing solely on state-of-the-art performance, we systematically analyze and identify the most efficient approach for building visual perception from scratch.
%
To address interference and compatibility challenges between vision and language, we fully disentangle the model components and introduce modality-wise sparsity within a unified decoder-only backbone.
%
Besides, we establish an efficient pathway for optimizing data-scaling efficiency in monolithic VLM research through a modified caption engine and a carefully designed training recipe. Using only 100M publicly available data, EVEv2.0 outperforms existing encoder-free models and steadily approaches the performance of encoder-based counterparts of similar capacity across a range of vision-language benchmarks. This provides valuable insights for developing scalable, native VLMs for the next generation.

{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\clearpage





\begin{appendix}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle 
\vspace{-8mm}
\begin{center} 
\centering 
\captionof{table}{Experiment Details in the main body. Note that T.M. denotes a trainable module in each stage. PEL and VLayers represent patch embedding layers and newly added vision layers in the large language model. EVE-recap-8/29M indicates a subset 8M of 29M training data.}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cc|cc|cccc|cc}
\toprule
\multirow{2}{*}{\textbf{Exp.}} 
&\multirow{2}{*}{\textbf{Model}}
&\multirow{2}{*}{\textbf{LLM}}
&\multicolumn{2}{c|}{\textbf{Stage 1}} 
&\multicolumn{4}{c|}{\textbf{Stage 2}}
&\multicolumn{2}{c}{\textbf{Stage 3}}\\
& &
&\textbf{Data} &\textbf{T.M.}
&\multicolumn{2}{c}{\textbf{Training Data}} 
&\multicolumn{2}{c|}{\textbf{Trainable Module}}
&\textbf{Data} &\textbf{T.M.} \\
\midrule
Fig.2 (i)
& EVEv1.0 & Vicuna-7B
& EVE-cap-16M & PEL
& \multicolumn{2}{c}{EVE-cap-33M} 
& \multicolumn{2}{c|}{PEL, LLM}
& LLaVA-mix-665k & PEL, LLM
\\
\hdashline
\multirow{4}{*}{Fig.5}
&EVEv1.0 & Qwen2.5-7B
& EVE-recap-10M & PEL
& \multicolumn{2}{c}{EVE-recap-8/29M} 
& \multicolumn{2}{c|}{PEL}
& LLaVA-mix-665k & PEL, LLM
\\
&EVEv1.2 & Qwen2.5-7B
& EVE-recap-10M & PEL
& \multicolumn{2}{c}{EVE-recap-8/29M} 
& \multicolumn{2}{c|}{PEL, VLayers}
& LLaVA-mix-665k & PEL, LLM\\
&EVEv1.5 & Qwen2.5-7B
& EVE-recap-10M & PEL
& \multicolumn{2}{c}{EVE-recap-8/29M} 
& \multicolumn{2}{c|}{PEL, VLayers}
& LLaVA-mix-665k & PEL, LLM\\
&EVEv2.0 & Qwen2.5-7B
& EVE-recap-10M & PEL
& \multicolumn{2}{c}{EVE-recap-8/29M} 
& \multicolumn{2}{c|}{PEL, VLayers}
& LLaVA-mix-665k & PEL, LLM
\\
\hdashline
Fig.6 
& EVEv1.0 & Vicuna-7B
& 10M varied data & PEL
& \multicolumn{2}{c}{8M same data from Stage 1}
& \multicolumn{2}{c|}{PEL, LLM}
& LLaVA-mix-665k & PEL, LLM
\\
\hdashline
Fig.7
& EVEv1.0 & Vicuna-7B
& 10M varied data & PEL
& \multicolumn{2}{c}{8M same data from Stage 1}
& \multicolumn{2}{c|}{PEL, LLM}
& LLaVA-mix-665k & PEL, LLM
\\
\hdashline
Fig.8
& EVEv1.0 & Vicuna-7B
& EVE-recap-10M & PEL
& \multicolumn{2}{c}{EVE-recap-8/29M}
& \multicolumn{2}{c|}{PEL, LLM}
& LLaVA-mix-665k & PEL, LLM
\\
\bottomrule
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\multirow{2}{*}{\textbf{Exp.}} 
&\multirow{2}{*}{\textbf{Model}}
&\multirow{2}{*}{\textbf{LLM}}
&\multicolumn{2}{c|}{\textbf{Stage 1}} 
&\multicolumn{2}{c}{\textbf{Stage 2.1}}
&\multicolumn{2}{c|}{\textbf{Stage 2.2}}
&\multicolumn{2}{c}{\textbf{Stage 3}}\\
& &
&\textbf{Data} &\textbf{T.M.}
&\textbf{Data} &\textbf{T.M.}
&\textbf{Data} &\textbf{T.M.}
&\textbf{Data} &\textbf{T.M.} \\
\midrule
\multirow{2}{*}{Fig.2 (ii)}
& EVEv1.0 & Qwen2-7B
& EVE-recap-10M & PEL
& EVE-recap-29M & PEL
& EVE-recap-48M & PEL, LLM
& Various SFT data & PEL, LLM
\\
& EVEv1.2 & Qwen2-7B
& EVE-recap-10M & PEL
& EVE-recap-29M & PEL, VLayers
& EVE-recap-48M & PEL, LLM
& Various SFT data & PEL, LLM 
\\
\hdashline
Tab.2 
& EVEv2.0 &Qwen2.5-7B
& EVE-recap-10M & PEL
& EVE-recap-77M & PEL, VLayers
& EVE-multi-task-15M & PEL, LLM
& EVE-sft-7M & PEL, LLM  
\\
\hdashline
Fig.9
& EVEv2.0 &Qwen2.5-7B
& EVE-recap-10M & PEL
& EVE-recap-77M & PEL, VLayers
& EVE-multi-task-15M & PEL, LLM
& EVE-sft-7M & PEL, LLM  
\\
\bottomrule
\end{tabular}}
\label{tab:exp_setting}
\end{center}
}]


\begin{table}[t]
\caption{Dataset details in Stage 2.2, and 3 for fine-tuning EVEv2.0. Note that ***-FL denotes the filtered training dataset.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c}
\toprule
\textbf{Stage} & \textbf{Dataset} & \#Data \\
\midrule
\multirow{5}{*}{2.2} 
&Cambrian-FL~\cite{tong2024cambrian},
Infinity-Instruct-FL~\cite{gu2024infinity},
&  \multirow{5}{*}{15M}\\
& LVIS-Instruct-FL~\cite{VLM:LVIS-4v},
Sharegpt4v-FL~\cite{VLM:Sharegpt4v},
& \\
&ALLaVA-laion-FL~\cite{chen2024allava},
ALLaVA-vflan-FL~\cite{chen2024allava},
&  \\
&LLaVA-Pretrain-FL~\cite{VLM:LLaVA-1.5},
DocReason-FL~\cite{hu2024mplug-docowl2},
&  \\
&DocDownstream-FL~\cite{hu2024mplug-docowl2},
DocStruct4M-FL~\cite{hu2024mplug-docowl2}.
&  \\
\midrule
\multirow{4}{*}{3} 
& LLaVA-onevision~\cite{Llava-onevision}, 
Infinity-MM-Synthesis~\cite{gu2024infinity},
& \multirow{4}{*}{7.3M}  \\
&Infinity-MM-Preference~\cite{gu2024infinity},
Infinity-Instruct-FL~\cite{gu2024infinity},
& \\
&DenseFusion~\cite{VLM:Densefusion},
Cambrian-FL~\cite{tong2024cambrian},
Docmatix-FL~\cite{laurenccon2024docmatix},
& \\
&
LVIS-Instruct-FL~\cite{VLM:LVIS-4v},
BLIP-OCR\cite{VLM:InstructBLIP},
LLaVA-mix~\cite{VLM:LLaVA-1.5}.
&  \\
\bottomrule
\end{tabular}}
\label{tab:datasets_details}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Details}
\label{sec:exp-details}

All experiment details in the main body are listed in~\Cref{tab:exp_setting}. ''EVE-cap-16M`` denotes the data mixture of LAION~\cite{Datasets:Laion-5b}, OpenImages~\cite{Datasets:OpenImages}, and SAM~\cite{TransF:SAM} annotated by LLaVA-1.5 (13B) and Emu2 (17B). ''EVE-recap-16M`` denotes the data mixture of Datacomp~\cite{Datasets:datacomp}, LAION~\cite{Datasets:Laion-5b}, OpenImages~\cite{Datasets:OpenImages}, and SAM~\cite{TransF:SAM} annotated by DenseFusion++ (7B).

For Exp.(i) in~\Cref{fig:preliminary}, we first use EVE-cap-16M in Stage 1 to train the projector, vision vocabulary embeddings, and lightweight vision block for the vision encoder (\textit{VE}), the discrete tokenizer (\textit{DT}), and EVEv1.0. In Stage 2, we use EVE-cap-33M to train only the vision encoder and projector for \textit{VE}, as unfreezing LLM weights at this stage leads to performance collapse~\cite{VLM:EVE}. For \textit{DT} and EVEv1.0, we unfreeze all model parameters.
In Stage 3, we train all model weights across all models. Finally, we quantify weight changes between Vicuna-7B~\cite{TransF:Vicuna} and \textit{VE}/EVEv1.0 to analyze the architectural differences. To ensure fairness, we remove the original vision encoder supervision in EVEv1.0.

\begin{table}[t]
    \centering
    \caption{Hyper-parameter configurations in Stage 1-3 for training EVEv2.0. Note that we set the training epoch in each stage as 1.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
         \toprule
         Configuration & Stage 1 & Stage 2.1 & Stage 2.2 & Stage 3 \\
         \midrule
         Maximum Patch Token   & $625$ & $625-2500$ & $2500$ & $2500$ \\
         Optimizer & \multicolumn{4}{c}{AdamW} \\
         Hyperparameters & \multicolumn{4}{c}{$\beta_{1}=0.9, \beta_{2}=0.999, eps=1e^{-8}$} \\
         Peak learning rate       & $2e^{-4}$ & $1e^{-4}$ & $2e^{-5}$ & $1e^{-5}$\\
         LR schedule   & \multicolumn{4}{c}{cosine decay with warm-up}\\
         Warm-up steps  & \multicolumn{4}{c}{$0.03$}\\
         Weight decay             & \multicolumn{4}{c}{$0.0$} \\
         Global batch size        & $1024$ & $1024$ & $512$ & $512$\\
         Numerical precision      & \multicolumn{4}{c}{$\mathtt{bfloat16}$} \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:hyperparam}
\end{table}

For Exp.(ii) in~\Cref{fig:preliminary}, all VLMs use stronger Qwen2-7B~\cite{yang2024qwen2} and high-quality EVE-recap. In Stage 1, we train the projector for \textit{VE}, patch embedding layer for EVEv1.0, patch embedding and extra vision layer inside the LLM for EVEv1.2. In Stage 2 and 3, we train all model weights for EVEv1.0-1.2. Note that we skip Stage 2 for \textit{VE} as the baseline for comparison. Here, we compare the weights between Qwen2 and \textit{VE}/EVEv1.2 trained by LLaVA-onevision~\cite{Llava-onevision}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset Details}
The dataset details in Stages 2.2 and 3 are listed in~\Cref{tab:datasets_details}.
In Stage 2.2, we adopt Cambrian-FL~\cite{tong2024cambrian}, Infinity-Instruct-FL~\cite{gu2024infinity},
LVIS-Instruct-FL~\cite{VLM:LVIS-4v},
Sharegpt4v-FL~\cite{VLM:Sharegpt4v},
ALLaVA-laion-FL~\cite{chen2024allava},
ALLaVA-vflan-FL~\cite{chen2024allava},
LLaVA-Pretrain-FL~\cite{VLM:LLaVA-1.5},
DocReason-FL~\cite{hu2024mplug-docowl2},
DocDownstream-FL~\cite{hu2024mplug-docowl2},
and DocStruct4M-FL~\cite{hu2024mplug-docowl2}.
In Stage 3, we adopt LLaVA-onevision~\cite{Llava-onevision}, 
Infinity-MM-Synthesis~\cite{gu2024infinity},
Infinity-MM-Preference~\cite{gu2024infinity},
Infinity-Instruct-FL~\cite{gu2024infinity},
DenseFusion~\cite{VLM:Densefusion},
Cambrian-FL~\cite{tong2024cambrian},
Docmatix-FL~\cite{laurenccon2024docmatix},
LVIS-Instruct-FL~\cite{VLM:LVIS-4v},
BLIP-OCR\cite{VLM:InstructBLIP},
and LLaVA-mix~\cite{VLM:LLaVA-1.5}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyper-parameter Configurations}
The detailed implementation configurations in Stages 1, 2.1, 2.2, and 3 are summarized in~\Cref{tab:hyperparam}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Visual Understanding Demonstration}
We investigate several vision perception and reasoning capabilities of EVEv2.0, including OCR capability in~\Cref{tab:ocr_in_web,tab:ocr_in_ppt}, mixed information in~\Cref{tab:poster}, real-world scenes in~\Cref{tab:real1,tab:real2}, and set-of-mark prompting task in~\Cref{tab:refer_to_marks}.


\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 1: Understanding OCR Information in Webpage}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/MAR.jpg} \\
\footnotesize
\textbf{User} & Please output all the text information. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & Cornell University We gratefully acknowledge support member. arXiv:2406.11838 Computer Science $>$ Computer Vision and Pattern Recognition Submitted on 17 Jun 2024 (v1), last revised 1 Nov 2024 (this version, v3)) 

Autoregressive Image Generation without Vector Quantization 

Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, Kaiming He

Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: this https URL 

Comments: Neurips 2024 (Spotlight); Code: this https URL Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2406.11838 [cs.CV] (or arXiv:2406.11838v3 [cs.CV] for this version) Submission history From: Tianhong Li [view email] [v1] Mon, 17 Jun 2024 17:59:58 UTC (5,459 KB) [v2] Sun, 28 Jul 2024 05:30:08 UTC (5,447 KB) [v3] Fri, 1 Nov 2024 14:45:36 UTC (8,971 KB) \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent OCR recognition capabilities in understanding Webpage.}
\vspace{-1mm}
\label{tab:ocr_in_web}
\end{minipage}
\end{table*}


\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 2: Understanding OCR Information in PPT}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/eve_motivation1.png} \\
\footnotesize
\textbf{User} & Please output all the text information. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & (A) Modular Large Vision-Language Model (B) Originated Large Vision-Language Model Language Model Interaction Following (i) Aligning Reasoning (ii) Differentiating Vision Encoder Perceiver Merging Projector Abstractor Vision Encoder Vision Encoder Existing Issues (1) Resolution/Aspect Ratio (2) Deployment Overhead (3) Capacity Proportion Motivation: Can We Train an Encoder-free LVLM? How can We Efficiently Build Originated LVLMs that can Rival Modular LVLMs? Property: (1) Decoder-only Architecture (2) Simpler LVLM to Scale \& Deploy EVE. \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent OCR recognition capabilities in understanding PowerPoint.}
\vspace{-1mm}
\label{tab:ocr_in_ppt}
\end{minipage}
\end{table*}


\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 3: Understanding Poster Content}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/ocr_beijing.jpg} \\
\footnotesize
\textbf{User} & Please describe the image in detail. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & The image is a promotional poster for "Neanderthal," which appears to be a documentary or film about Neanderthals. The title "Neanderthal" is prominently displayed in large, bold yellow letters across the center of the image. Below the title, there is additional text that reads "Years 7 and 8 Welcome of the Day," suggesting that this event might be targeted towards students in those grade levels.

The background of the poster features a close-up photograph of a man with a Neanderthal-like appearance. He has a long beard, a prominent forehead, and a strong jawline, which are characteristic features often associated with Neanderthals. The man is holding a wooden stick or pole, which adds to the prehistoric theme of the image. The background behind him is blurred, drawing focus to his face and upper body.

At the bottom right corner of the poster, there is more text indicating the date of the event: "Friday 23 January 2015." This provides a specific time frame for when the event or release of the documentary/film would have taken place.

Overall, the poster uses visual elements such as the Neanderthal figure and the title to convey information about the subject matter, while also providing essential details like the target audience and the date of the event. The design effectively captures attention and communicates key information to viewers. \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent visual recognition capabilities in understanding Poster.}
\vspace{-1mm}
\label{tab:poster}
\end{minipage}
\end{table*}



\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 4: Understanding Real-world Scene}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/background.jpg} \\
\footnotesize
\textbf{User} & Please describe the image in detail. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & In the image, there is a man standing in front of a bathroom mirror, taking a selfie with his cell phone. He is wearing a purple shirt and leopard print pants. The man appears to be smiling as he captures the photo.

In the background, there is a potted plant with white flowers placed on the counter near the sink. The sink is white and has a modern design. There is also a poster hanging on the wall next to the mirror. The poster features a silhouette of a person's head with several circles on it. Below the silhouette, there is text that reads "COME SETS," followed by some additional information about Friday night events at Codame.com.

The bathroom appears to be well-lit, likely due to natural light coming from a window or another source not visible in the image. The overall atmosphere seems casual and relaxed, with the man enjoying his time in the bathroom while capturing a moment through his phone camera. \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.}
\vspace{-1mm}
\label{tab:real1}
\end{minipage}
\end{table*}


\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 5: Understanding Real-world Scene}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/mac.jpg} \\
\footnotesize
\textbf{User} & Please describe the image in detail. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & The image depicts a well-organized workspace featuring a wooden desk with various items on it. At the center of the desk is a closed silver laptop computer, which has an Apple logo on its lid. This laptop is accompanied by a white keyboard placed directly in front of it, suggesting that the user might prefer using an external keyboard for typing.

To the right of the laptop, there is a white wireless mouse, indicating that the user also utilizes a mouse for navigation and other tasks. On the left side of the desk, there is a glass containing a beverage, possibly water or another drink, which is positioned near the edge of the desk.

In addition to these items, there are two cylindrical objects resembling speakers, one on each side of the desk. These speakers likely provide audio output for the user while working or listening to music.

Behind the desk, there is a bookshelf filled with numerous books, suggesting that the user may have a keen interest in reading or research. The presence of multiple books indicates that this workspace could be used for studying, work, or leisure activities involving reading.

Overall, the image portrays a functional and organized workspace designed for productivity and comfort, with essential computing equipment and additional accessories such as speakers and a beverage. \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent recognition capabilities in the real-world scene.}
\vspace{-1mm}
\label{tab:real2}
\end{minipage}
\end{table*}


\begin{table*}
\begin{minipage}{0.99\textwidth}
\begin{AIbox}{Example 6: Set-of-mark Prompting}
\centering
\scalebox{0.80}{
\begin{tabular}{l p{18cm}}
&  \includegraphics[width=16cm]{figures/examples/som.png} \\
\footnotesize
\textbf{User} & Here is an image with objects labeled in numbers. Please describe the objects that are marked as 9, 12 and 15. \\
\midrule
\footnotesize
\textbf{EVEv2.0} & In the image, there are three objects marked with numbers: 9, 12, and 15. 

Object 9: A laptop computer is placed on the desk. It appears to be open and ready for use.

Object 12: A lamp is situated near the desk, providing light for the workspace. It has a sleek design and complements the overall aesthetic of the room.

Object 15: A potted plant is located on the desk, adding a touch of greenery and life to the space. It contributes to creating a more pleasant and inviting atmosphere in the room. \\
\end{tabular}
}
\end{AIbox}
\captionof{table}{EVEv2.0 exhibits excellent set-of-mark prompting capabilities, ie, referring to marks when answering questions.}
\vspace{-1mm}
\label{tab:refer_to_marks}
\end{minipage}
\end{table*}


\end{appendix}

\end{document}
