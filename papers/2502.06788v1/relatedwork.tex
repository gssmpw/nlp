\section{Related Work}
\label{sec:Related-work}

\textbf{Encoder-based VLMs.} 
Encoder-based methods have become the dominant approach in vision-language models, widely adopted in commercial products, \eg. GPT-4V~\cite{VLM:GPT-4}, Claude 3.5~\cite{VLM:Claude3}, and Gemini~\cite{VLM:Gemini}, as well as in the open-source projects like LLaVA series~\cite{VLM:LLaVA, VLM:LLaVA-1.5, VLM:LLaVA-1.6, Llava-onevision}, Qwen-VL series~\cite{VLM:qwen-vl,wang2024qwen2-vl}, InternVL series~\cite{VLM:InternVL, VLM:InternVL-1.5}, BLIP series~\cite{VLP:BLIP,VLP:BLIPv2,xue2024blip-3}, and EMU series~\cite{VLM:EMU,VLM:EMUv2}. They benefit from the pre-trained knowledge from visual encoders~\cite{VLP:CLIP,TransF:Siglip,TransF:EVA-CLIP,CL:DINO} and LLMs~\cite{TransF:LLaMA,TransF:LLaMA2,TransF:Vicuna,TransF:InternLM,TransF:InternLM2,TransF:Qwen,yang2024qwen2,qwen2.5}, successfully building modular VLMs for various real-world applications. Among them, most studies~\cite{VLM:mPLUG-Owl,VLM:mPLUG-Owl2,VLM:Monkey,VLM:LLaVA-UHD,VLM:Sharegpt4v,VLM:Densefusion} directly translate vision representations into the input space of LLMs.
%
In contrast, another type of research~\cite{VLM:Llama-Adapter,VLM:Llama-Adapterv2,VLM:CogAgent,VLP:Flamingo,VLMs:LLama3.2} introduces the cross-attention module for integrating visual and language information layer-by-layer.
%
Despite achieving strong performance gains, it may be insufficient to simply map visual information into the input space of LLMs~\cite{VLM:LLaVA-1.5,VLM:mPLUG-Owl2} or connect the same visual features across different representational levels of the LLM~\cite{VLM:CogAgent,VLM:IDEFICS}, given the heterogeneous characteristics between vision and language. 
%
Besides, these modular VLMs face challenges in further development due to the strong inductive biases in pre-training visual encoding patterns, complex infrastructure requirements, and scaling laws necessary to balance various separate components.



\noindent\textbf{Encoder-free VLMs.} 
Another visual processing strategy is discrete visual tokenization~\cite{van2017VQVAE,VQGAN,LLamagen}, which is widely used in various multi-modal understanding and generation approaches~\cite{team2024chameleon,VLM:AnyGPT,wang2024mio,VLM:EMU3}.
However, the discretization inevitably results in lossy visual information and weakens in extracting semantic contents, which in turn hinders fine-grained visual understanding and reasoning~\cite{xie2024show-o,wu2024janus}.
%
Therefore, recent studies~\cite{wu2024vila-u,li2024imagefolder,qu2024tokenflow,xie2024muse} introduce semantic constraints in the visual tokenizer for both high-level semantic and fine-grained visual representations.
%
Compared to their highly-compressed features in the discrete-value space, encoder-free VLMs~\cite{VLM:Fuyu-8b,VLM:EVE,chen2024solo} have emerged as a promising architecture for lossless visual encoding, efficient data scaling, and end-to-end multimodal processing. 
%
Specially, EVE~\cite{VLM:EVE} pioneers an efficient and transparent path for monolithic VLMs, with its data-scaling efficiency preliminarily validated by PaliGemma~\cite{beyer2024paligemma}. Impressively, Mono-InternVL~\cite{luo2024mono} bridges the performance gap with its modular counterpart~\cite{VLM:InternVL-1.5} of the same LLM capacity, using adequate data. We emphasize that limited by current training data and device resources, EVEv2.0 does not aim for state-of-the-art performance but instead focuses on revealing the most efficient route for encoder-free VLMs from scratch.

%%%%%%%%%%%%%%%%%%%%     Methodology    %%%%%%%%%%%%%%%%%%%%