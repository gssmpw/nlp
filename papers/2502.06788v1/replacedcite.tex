\section{Related Work}
\label{sec:Related-work}

\textbf{Encoder-based VLMs.} 
Encoder-based methods have become the dominant approach in vision-language models, widely adopted in commercial products, \eg. GPT-4V____, Claude 3.5____, and Gemini____, as well as in the open-source projects like LLaVA series____, Qwen-VL series____, InternVL series____, BLIP series____, and EMU series____. They benefit from the pre-trained knowledge from visual encoders____ and LLMs____, successfully building modular VLMs for various real-world applications. Among them, most studies____ directly translate vision representations into the input space of LLMs.
%
In contrast, another type of research____ introduces the cross-attention module for integrating visual and language information layer-by-layer.
%
Despite achieving strong performance gains, it may be insufficient to simply map visual information into the input space of LLMs____ or connect the same visual features across different representational levels of the LLM____, given the heterogeneous characteristics between vision and language. 
%
Besides, these modular VLMs face challenges in further development due to the strong inductive biases in pre-training visual encoding patterns, complex infrastructure requirements, and scaling laws necessary to balance various separate components.



\noindent\textbf{Encoder-free VLMs.} 
Another visual processing strategy is discrete visual tokenization____, which is widely used in various multi-modal understanding and generation approaches____.
However, the discretization inevitably results in lossy visual information and weakens in extracting semantic contents, which in turn hinders fine-grained visual understanding and reasoning____.
%
Therefore, recent studies____ introduce semantic constraints in the visual tokenizer for both high-level semantic and fine-grained visual representations.
%
Compared to their highly-compressed features in the discrete-value space, encoder-free VLMs____ have emerged as a promising architecture for lossless visual encoding, efficient data scaling, and end-to-end multimodal processing. 
%
Specially, EVE____ pioneers an efficient and transparent path for monolithic VLMs, with its data-scaling efficiency preliminarily validated by PaliGemma____. Impressively, Mono-InternVL____ bridges the performance gap with its modular counterpart____ of the same LLM capacity, using adequate data. We emphasize that limited by current training data and device resources, EVEv2.0 does not aim for state-of-the-art performance but instead focuses on revealing the most efficient route for encoder-free VLMs from scratch.

%%%%%%%%%%%%%%%%%%%%     Methodology    %%%%%%%%%%%%%%%%%%%%