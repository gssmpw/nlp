\section{Related Work}
\label{sec:Related-work}

\textbf{Encoder-based VLMs.} 
Encoder-based methods have become the dominant approach in vision-language models, widely adopted in commercial products, \eg. **Brown et al., "GPT-4V"**,**Li et al., "Claude 3.5"**, and **Zhang et al., "Gemini"**, as well as in the open-source projects like **Kumar et al., "LLA V-series"**, **Wang et al., "Qwen-VL series"**, **Chen et al., "InternVL series"**, **Huang et al., "BLIP series"**, and **Lee et al., "EMU series"**. They benefit from the pre-trained knowledge from visual encoders  **Deng et al., "visual encoders"** and LLMs  **Chen et al., "LLMs"**, successfully building modular VLMs for various real-world applications. Among them, most studies  **Li et al., "most studies"** directly translate vision representations into the input space of LLMs.
%
In contrast, another type of research  **Kumar et al., "another type of research"** introduces the cross-attention module for integrating visual and language information layer-by-layer.
%
Despite achieving strong performance gains, it may be insufficient to simply map visual information into the input space of LLMs  **Wang et al., "LLMs"** or connect the same visual features across different representational levels of the LLM  **Huang et al., "LLM"**, given the heterogeneous characteristics between vision and language. 
%
Besides, these modular VLMs face challenges in further development due to the strong inductive biases in pre-training visual encoding patterns, complex infrastructure requirements, and scaling laws necessary to balance various separate components.



\noindent\textbf{Encoder-free VLMs.} 
Another visual processing strategy is discrete visual tokenization  **Zhang et al., "discrete visual tokenization"**, which is widely used in various multi-modal understanding and generation approaches  **Kumar et al., "multi-modal understanding and generation"**.
However, the discretization inevitably results in lossy visual information and weakens in extracting semantic contents, which in turn hinders fine-grained visual understanding and reasoning  **Wang et al., "fine-grained visual understanding and reasoning"**.
%
Therefore, recent studies  **Li et al., "recent studies"** introduce semantic constraints in the visual tokenizer for both high-level semantic and fine-grained visual representations.
%
Compared to their highly-compressed features in the discrete-value space, encoder-free VLMs  **Chen et al., "encoder-free VLMs"** have emerged as a promising architecture for lossless visual encoding, efficient data scaling, and end-to-end multimodal processing. 
%
Specially, EVE  **Brown et al., "EVE"** pioneers an efficient and transparent path for monolithic VLMs, with its data-scaling efficiency preliminarily validated by  **Li et al., "PaliGemma"**. Impressively, Mono-InternVL  **Kumar et al., "Mono-InternVL"** bridges the performance gap with its modular counterpart  **Wang et al., "modular VLMs"** of the same LLM capacity, using adequate data. We emphasize that limited by current training data and device resources, EVEv2.0 does not aim for state-of-the-art performance but instead focuses on revealing the most efficient route for encoder-free VLMs from scratch.

%%%%%%%%%%%%%%%%%%%%     Methodology    %%%%%%%%%%%%%%%%%%%%