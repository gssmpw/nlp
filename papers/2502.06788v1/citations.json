[
  {
    "index": 0,
    "papers": [
      {
        "key": "VLM:GPT-4",
        "author": "OpenAI",
        "title": "{GPT-4} Technical Report"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "VLM:Claude3",
        "author": "Anthropic, AI",
        "title": "The claude 3 model family: Opus, sonnet, haiku"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "VLM:Gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "VLM:LLaVA",
        "author": "Haotian Liu and\nChunyuan Li and\nQingyang Wu and\nYong Jae Lee",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "VLM:LLaVA-1.5",
        "author": "Haotian Liu and\nChunyuan Li and\nYuheng Li and\nYong Jae Lee",
        "title": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "key": "VLM:LLaVA-1.6",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "Llava-onevision",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "Llava-onevision: Easy visual task transfer"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "VLM:qwen-vl",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond"
      },
      {
        "key": "wang2024qwen2-vl",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "VLM:InternVL",
        "author": "Zhe Chen and\nJiannan Wu and\nWenhai Wang and\nWeijie Su and\nGuo Chen and\nSen Xing and\nMuyan Zhong and\nQinglong Zhang and\nXizhou Zhu and\nLewei Lu and\nBin Li and\nPing Luo and\nTong Lu and\nYu Qiao and\nJifeng Dai",
        "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic\nVisual-Linguistic Tasks"
      },
      {
        "key": "VLM:InternVL-1.5",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "VLP:BLIP",
        "author": "Junnan Li and\nDongxu Li and\nCaiming Xiong and\nSteven C. H. Hoi",
        "title": "{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language\nUnderstanding and Generation"
      },
      {
        "key": "VLP:BLIPv2",
        "author": "Junnan Li and\nDongxu Li and\nSilvio Savarese and\nSteven C. H. Hoi",
        "title": "{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image\nEncoders and Large Language Models"
      },
      {
        "key": "xue2024blip-3",
        "author": "Xue, Le and Shu, Manli and Awadalla, Anas and Wang, Jun and Yan, An and Purushwalkam, Senthil and Zhou, Honglu and Prabhu, Viraj and Dai, Yutong and Ryoo, Michael S and others",
        "title": "xgen-mm (blip-3): A family of open large multimodal models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "VLM:EMU",
        "author": "Quan Sun and\nQiying Yu and\nYufeng Cui and\nFan Zhang and\nXiaosong Zhang and\nYueze Wang and\nHongcheng Gao and\nJingjing Liu and\nTiejun Huang and\nXinlong Wang",
        "title": "Generative Pretraining in Multimodality"
      },
      {
        "key": "VLM:EMUv2",
        "author": "Quan Sun and\nYufeng Cui and\nXiaosong Zhang and\nFan Zhang and\nQiying Yu and\nZhengxiong Luo and\nYueze Wang and\nYongming Rao and\nJingjing Liu and\nTiejun Huang and\nXinlong Wang",
        "title": "Generative Multimodal Models are In-Context Learners"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "VLP:CLIP",
        "author": "Alec Radford and\nJong Wook Kim and\nChris Hallacy and\nAditya Ramesh and\nGabriel Goh and\nSandhini Agarwal and\nGirish Sastry and\nAmanda Askell and\nPamela Mishkin and\nJack Clark and\nGretchen Krueger and\nIlya Sutskever",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "key": "TransF:Siglip",
        "author": "Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas",
        "title": "Sigmoid loss for language image pre-training"
      },
      {
        "key": "TransF:EVA-CLIP",
        "author": "Quan Sun and\nYuxin Fang and\nLedell Wu and\nXinlong Wang and\nYue Cao",
        "title": "{EVA-CLIP:} Improved Training Techniques for {CLIP} at Scale"
      },
      {
        "key": "CL:DINO",
        "author": "Mathilde Caron and\nHugo Touvron and\nIshan Misra and\nHerv{\\'{e}} J{\\'{e}}gou and\nJulien Mairal and\nPiotr Bojanowski and\nArmand Joulin",
        "title": "Emerging Properties in Self-Supervised Vision Transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "TransF:LLaMA",
        "author": "Hugo Touvron and\nThibaut Lavril and\nGautier Izacard and\nXavier Martinet and\nMarie{-}Anne Lachaux and\nTimoth{\\'{e}}e Lacroix and\nBaptiste Rozi{\\`{e}}re and\nNaman Goyal and\nEric Hambro and\nFaisal Azhar and\nAur{\\'{e}}lien Rodriguez and\nArmand Joulin and\nEdouard Grave and\nGuillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "key": "TransF:LLaMA2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "TransF:Vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.",
        "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality"
      },
      {
        "key": "TransF:InternLM",
        "author": "InternLM Team",
        "title": "InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities"
      },
      {
        "key": "TransF:InternLM2",
        "author": "Zheng Cai and\nMaosong Cao and\nHaojiong Chen and\nKai Chen and\nKeyu Chen and\nXin Chen and\nXun Chen and\nZehui Chen and\nZhi Chen and\nPei Chu and\nXiaoyi Dong and\nHaodong Duan and\nQi Fan and\nZhaoye Fei and\nYang Gao and\nJiaye Ge and\nChenya Gu and\nYuzhe Gu and\nTao Gui and\nAijia Guo and\nQipeng Guo and\nConghui He and\nYingfan Hu and\nTing Huang and\nTao Jiang and\nPenglong Jiao and\nZhenjiang Jin and\nZhikai Lei and\nJiaxing Li and\nJingwen Li and\nLinyang Li and\nShuaibin Li and\nWei Li and\nYining Li and\nHongwei Liu and\nJiangning Liu and\nJiawei Hong and\nKaiwen Liu and\nKuikun Liu and\nXiaoran Liu and\nChengqi Lv and\nHaijun Lv and\nKai Lv and\nLi Ma and\nRunyuan Ma and\nZerun Ma and\nWenchang Ning and\nLinke Ouyang and\nJiantao Qiu and\nYuan Qu and\nFukai Shang and\nYunfan Shao and\nDemin Song and\nZifan Song and\nZhihao Sui and\nPeng Sun and\nYu Sun and\nHuanze Tang and\nBin Wang and\nGuoteng Wang and\nJiaqi Wang and\nJiayu Wang and\nRui Wang and\nYudong Wang and\nZiyi Wang and\nXingjian Wei and\nQizhen Weng and\nFan Wu and\nYingtong Xiong and\net al.",
        "title": "InternLM2 Technical Report"
      },
      {
        "key": "TransF:Qwen",
        "author": "Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu",
        "title": "Qwen Technical Report"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others",
        "title": "Qwen2 technical report"
      },
      {
        "key": "qwen2.5",
        "author": "Qwen Team",
        "title": "Qwen2.5: A Party of Foundation Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "VLM:mPLUG-Owl",
        "author": "Qinghao Ye and\nHaiyang Xu and\nGuohai Xu and\nJiabo Ye and\nMing Yan and\nYiyang Zhou and\nJunyang Wang and\nAnwen Hu and\nPengcheng Shi and\nYaya Shi and\nChenliang Li and\nYuanhong Xu and\nHehong Chen and\nJunfeng Tian and\nQian Qi and\nJi Zhang and\nFei Huang",
        "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"
      },
      {
        "key": "VLM:mPLUG-Owl2",
        "author": "Qinghao Ye and\nHaiyang Xu and\nJiabo Ye and\nMing Yan and\nAnwen Hu and\nHaowei Liu and\nQi Qian and\nJi Zhang and\nFei Huang and\nJingren Zhou",
        "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with\nModality Collaboration"
      },
      {
        "key": "VLM:Monkey",
        "author": "Zhang Li and\nBiao Yang and\nQiang Liu and\nZhiyin Ma and\nShuo Zhang and\nJingxu Yang and\nYabo Sun and\nYuliang Liu and\nXiang Bai",
        "title": "Monkey: Image Resolution and Text Label Are Important Things for Large\nMulti-modal Models"
      },
      {
        "key": "VLM:LLaVA-UHD",
        "author": "Ruyi Xu and\nYuan Yao and\nZonghao Guo and\nJunbo Cui and\nZanlin Ni and\nChunjiang Ge and\nTat{-}Seng Chua and\nZhiyuan Liu and\nMaosong Sun and\nGao Huang",
        "title": "LLaVA-UHD: an {LMM} Perceiving Any Aspect Ratio and High-Resolution\nImages"
      },
      {
        "key": "VLM:Sharegpt4v",
        "author": "Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua",
        "title": "Sharegpt4v: Improving large multi-modal models with better captions"
      },
      {
        "key": "VLM:Densefusion",
        "author": "Li, Xiaotong and Zhang, Fan and Diao, Haiwen and Wang, Yueze and Wang, Xinlong and Duan, Ling-Yu",
        "title": "Densefusion-1m: Merging vision experts for comprehensive multimodal perception"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "VLM:Llama-Adapter",
        "author": "Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention"
      },
      {
        "key": "VLM:Llama-Adapterv2",
        "author": "Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others",
        "title": "Llama-adapter v2: Parameter-efficient visual instruction model"
      },
      {
        "key": "VLM:CogAgent",
        "author": "Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others",
        "title": "Cogagent: A visual language model for gui agents"
      },
      {
        "key": "VLP:Flamingo",
        "author": "Jean{-}Baptiste Alayrac and\nJeff Donahue and\nPauline Luc and\nAntoine Miech and\nIain Barr and\nYana Hasson and\nKarel Lenc and\nArthur Mensch and\nKatherine Millican and\nMalcolm Reynolds and\nRoman Ring and\nEliza Rutherford and\nSerkan Cabi and\nTengda Han and\nZhitao Gong and\nSina Samangooei and\nMarianne Monteiro and\nJacob L. Menick and\nSebastian Borgeaud and\nAndy Brock and\nAida Nematzadeh and\nSahand Sharifzadeh and\nMikolaj Binkowski and\nRicardo Barreira and\nOriol Vinyals and\nAndrew Zisserman and\nKar{\\'{e}}n Simonyan",
        "title": "Flamingo: a Visual Language Model for Few-Shot Learning"
      },
      {
        "key": "VLMs:LLama3.2",
        "author": "Meta Team",
        "title": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "VLM:LLaVA-1.5",
        "author": "Haotian Liu and\nChunyuan Li and\nYuheng Li and\nYong Jae Lee",
        "title": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "key": "VLM:mPLUG-Owl2",
        "author": "Qinghao Ye and\nHaiyang Xu and\nJiabo Ye and\nMing Yan and\nAnwen Hu and\nHaowei Liu and\nQi Qian and\nJi Zhang and\nFei Huang and\nJingren Zhou",
        "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with\nModality Collaboration"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "VLM:CogAgent",
        "author": "Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others",
        "title": "Cogagent: A visual language model for gui agents"
      },
      {
        "key": "VLM:IDEFICS",
        "author": "{IDEFICS Research Team}",
        "title": "Introducing idefics: An open reproduction of state-of-the-art visual language model"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "van2017VQVAE",
        "author": "Van Den Oord, Aaron and Vinyals, Oriol and others",
        "title": "Neural discrete representation learning"
      },
      {
        "key": "VQGAN",
        "author": "Esser, Patrick and Rombach, Robin and Ommer, Bjorn",
        "title": "Taming transformers for high-resolution image synthesis"
      },
      {
        "key": "LLamagen",
        "author": "Sun, Peize and Jiang, Yi and Chen, Shoufa and Zhang, Shilong and Peng, Bingyue and Luo, Ping and Yuan, Zehuan",
        "title": "Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "team2024chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      },
      {
        "key": "VLM:AnyGPT",
        "author": "Jun Zhan and\nJunqi Dai and\nJiasheng Ye and\nYunhua Zhou and\nDong Zhang and\nZhigeng Liu and\nXin Zhang and\nRuibin Yuan and\nGe Zhang and\nLinyang Li and\nHang Yan and\nJie Fu and\nTao Gui and\nTianxiang Sun and\nYugang Jiang and\nXipeng Qiu",
        "title": "AnyGPT: Unified Multimodal {LLM} with Discrete Sequence Modeling"
      },
      {
        "key": "wang2024mio",
        "author": "Wang, Zekun and Zhu, King and Xu, Chunpu and Zhou, Wangchunshu and Liu, Jiaheng and Zhang, Yibo and Wang, Jiashuo and Shi, Ning and Li, Siyu and Li, Yizhi and others",
        "title": "Mio: A foundation model on multimodal tokens"
      },
      {
        "key": "VLM:EMU3",
        "author": "Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others",
        "title": "Emu3: Next-Token Prediction is All You Need"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xie2024show-o",
        "author": "Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng",
        "title": "Show-o: One single transformer to unify multimodal understanding and generation"
      },
      {
        "key": "wu2024janus",
        "author": "Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others",
        "title": "Janus: Decoupling visual encoding for unified multimodal understanding and generation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wu2024vila-u",
        "author": "Wu, Yecheng and Zhang, Zhuoyang and Chen, Junyu and Tang, Haotian and Li, Dacheng and Fang, Yunhao and Zhu, Ligeng and Xie, Enze and Yin, Hongxu and Yi, Li and others",
        "title": "Vila-u: a unified foundation model integrating visual understanding and generation"
      },
      {
        "key": "li2024imagefolder",
        "author": "Li, Xiang and Qiu, Kai and Chen, Hao and Kuen, Jason and Gu, Jiuxiang and Raj, Bhiksha and Lin, Zhe",
        "title": "Imagefolder: Autoregressive image generation with folded tokens"
      },
      {
        "key": "qu2024tokenflow",
        "author": "Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong",
        "title": "Tokenflow: Unified image tokenizer for multimodal understanding and generation"
      },
      {
        "key": "xie2024muse",
        "author": "Xie, Rongchang and Du, Chen and Song, Ping and Liu, Chang",
        "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "VLM:Fuyu-8b",
        "author": "Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\\c{s}\\i{}rlar, Sa\\u{g}nak",
        "title": "Introducing our Multimodal Models"
      },
      {
        "key": "VLM:EVE",
        "author": "Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong",
        "title": "Unveiling Encoder-Free Vision-Language Models"
      },
      {
        "key": "chen2024solo",
        "author": "Chen, Yangyi and Wang, Xingyao and Peng, Hao and Ji, Heng",
        "title": "A Single Transformer for Scalable Vision-Language Modeling"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "VLM:EVE",
        "author": "Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong",
        "title": "Unveiling Encoder-Free Vision-Language Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "beyer2024paligemma",
        "author": "Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others",
        "title": "Paligemma: A versatile 3b vlm for transfer"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "luo2024mono",
        "author": "Luo, Gen and Yang, Xue and Dou, Wenhan and Wang, Zhaokai and Dai, Jifeng and Qiao, Yu and Zhu, Xizhou",
        "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "VLM:InternVL-1.5",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      }
    ]
  }
]