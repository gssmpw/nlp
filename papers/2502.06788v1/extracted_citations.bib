@inproceedings{CL:DINO,
  author    = {Mathilde Caron and
               Hugo Touvron and
               Ishan Misra and
               Herv{\'{e}} J{\'{e}}gou and
               Julien Mairal and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  booktitle = {ICCV},
  pages     = {9630--9640},
  year      = {2021}
}

@article{LLamagen,
  title={Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation},
  author={Sun, Peize and Jiang, Yi and Chen, Shoufa and Zhang, Shilong and Peng, Bingyue and Luo, Ping and Yuan, Zehuan},
  journal={arXiv preprint arXiv:2406.06525},
  year={2024}
}

@article{Llava-onevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{TransF:EVA-CLIP,
  author       = {Quan Sun and
                  Yuxin Fang and
                  Ledell Wu and
                  Xinlong Wang and
                  Yue Cao},
  title        = {{EVA-CLIP:} Improved Training Techniques for {CLIP} at Scale},
  journal      = {arXiv: 2303.15389},
  year         = {2023}
}

@misc{TransF:InternLM,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

@article{TransF:InternLM2,
  author       = {Zheng Cai and
                  Maosong Cao and
                  Haojiong Chen and
                  Kai Chen and
                  Keyu Chen and
                  Xin Chen and
                  Xun Chen and
                  Zehui Chen and
                  Zhi Chen and
                  Pei Chu and
                  Xiaoyi Dong and
                  Haodong Duan and
                  Qi Fan and
                  Zhaoye Fei and
                  Yang Gao and
                  Jiaye Ge and
                  Chenya Gu and
                  Yuzhe Gu and
                  Tao Gui and
                  Aijia Guo and
                  Qipeng Guo and
                  Conghui He and
                  Yingfan Hu and
                  Ting Huang and
                  Tao Jiang and
                  Penglong Jiao and
                  Zhenjiang Jin and
                  Zhikai Lei and
                  Jiaxing Li and
                  Jingwen Li and
                  Linyang Li and
                  Shuaibin Li and
                  Wei Li and
                  Yining Li and
                  Hongwei Liu and
                  Jiangning Liu and
                  Jiawei Hong and
                  Kaiwen Liu and
                  Kuikun Liu and
                  Xiaoran Liu and
                  Chengqi Lv and
                  Haijun Lv and
                  Kai Lv and
                  Li Ma and
                  Runyuan Ma and
                  Zerun Ma and
                  Wenchang Ning and
                  Linke Ouyang and
                  Jiantao Qiu and
                  Yuan Qu and
                  Fukai Shang and
                  Yunfan Shao and
                  Demin Song and
                  Zifan Song and
                  Zhihao Sui and
                  Peng Sun and
                  Yu Sun and
                  Huanze Tang and
                  Bin Wang and
                  Guoteng Wang and
                  Jiaqi Wang and
                  Jiayu Wang and
                  Rui Wang and
                  Yudong Wang and
                  Ziyi Wang and
                  Xingjian Wei and
                  Qizhen Weng and
                  Fan Wu and
                  Yingtong Xiong and
                  et al.},
  title        = {InternLM2 Technical Report},
  journal      = {arXiv: 2403.17297},
  year         = {2024}
}

@article{TransF:LLaMA,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {arXiv: 2302.13971},
  year         = {2023}
}

@article{TransF:LLaMA2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv: 2307.09288},
  year={2023}
}

@article{TransF:Qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv: 2309.16609},
  year={2023}
}

@inproceedings{TransF:Siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  pages={11975--11986},
  year={2023}
}

@misc{TransF:Vicuna,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{VLM:AnyGPT,
  author       = {Jun Zhan and
                  Junqi Dai and
                  Jiasheng Ye and
                  Yunhua Zhou and
                  Dong Zhang and
                  Zhigeng Liu and
                  Xin Zhang and
                  Ruibin Yuan and
                  Ge Zhang and
                  Linyang Li and
                  Hang Yan and
                  Jie Fu and
                  Tao Gui and
                  Tianxiang Sun and
                  Yugang Jiang and
                  Xipeng Qiu},
  title        = {AnyGPT: Unified Multimodal {LLM} with Discrete Sequence Modeling},
  journal      = {arXiv: 2402.12226},
  year         = {2024}
}

@article{VLM:Claude3,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}

@article{VLM:CogAgent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv:2312.08914},
  year={2023}
}

@article{VLM:Densefusion,
  title={Densefusion-1m: Merging vision experts for comprehensive multimodal perception},
  author={Li, Xiaotong and Zhang, Fan and Diao, Haiwen and Wang, Yueze and Wang, Xinlong and Duan, Ling-Yu},
  journal={arXiv preprint arXiv:2407.08303},
  year={2024}
}

@article{VLM:EMU,
  author       = {Quan Sun and
                  Qiying Yu and
                  Yufeng Cui and
                  Fan Zhang and
                  Xiaosong Zhang and
                  Yueze Wang and
                  Hongcheng Gao and
                  Jingjing Liu and
                  Tiejun Huang and
                  Xinlong Wang},
  title        = {Generative Pretraining in Multimodality},
  journal      = {arXiv: 2307.05222},
  year         = {2023},
}

@article{VLM:EMU3,
  title={Emu3: Next-Token Prediction is All You Need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}

@article{VLM:EMUv2,
  author       = {Quan Sun and
                  Yufeng Cui and
                  Xiaosong Zhang and
                  Fan Zhang and
                  Qiying Yu and
                  Zhengxiong Luo and
                  Yueze Wang and
                  Yongming Rao and
                  Jingjing Liu and
                  Tiejun Huang and
                  Xinlong Wang},
  title        = {Generative Multimodal Models are In-Context Learners},
  journal      = {arXiv: 2312.13286},
  year         = {2023}
}

@article{VLM:EVE,
  title={Unveiling Encoder-Free Vision-Language Models},
  author={Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
  journal={arXiv preprint arXiv:2406.11832},
  year={2024}
}

@misc{VLM:Fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title = {Introducing our Multimodal Models},
  url = {https://www.adept.ai/blog/fuyu-8b},
  year = {2023}
}

@article{VLM:GPT-4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {arXiv: 2303.08774},
  year         = {2023},
}

@article{VLM:Gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv: 2312.11805},
  year={2023}
}

@misc{VLM:IDEFICS,
  author = {{IDEFICS Research Team}},
  title = {Introducing idefics: An open reproduction of state-of-the-art visual language model},
  howpublished = {\url{https://huggingface.co/blog/idefics}},
  year = {2023}
}

@article{VLM:InternVL,
  author       = {Zhe Chen and
                  Jiannan Wu and
                  Wenhai Wang and
                  Weijie Su and
                  Guo Chen and
                  Sen Xing and
                  Muyan Zhong and
                  Qinglong Zhang and
                  Xizhou Zhu and
                  Lewei Lu and
                  Bin Li and
                  Ping Luo and
                  Tong Lu and
                  Yu Qiao and
                  Jifeng Dai},
  title        = {InternVL: Scaling up Vision Foundation Models and Aligning for Generic
                  Visual-Linguistic Tasks},
  journal      = {arXiv: 2312.14238},
  year         = {2023}
}

@article{VLM:InternVL-1.5,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv:2404.16821},
  year={2024}
}

@inproceedings{VLM:LLaVA,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
  title        = {Visual Instruction Tuning},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{VLM:LLaVA-1.5,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Yuheng Li and
                  Yong Jae Lee},
  title        = {Improved Baselines with Visual Instruction Tuning},
  journal      = {arXiv: 2310.03744},
  year         = {2023}
}

@misc{VLM:LLaVA-1.6,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    year={2024}
}

@article{VLM:LLaVA-UHD,
  author       = {Ruyi Xu and
                  Yuan Yao and
                  Zonghao Guo and
                  Junbo Cui and
                  Zanlin Ni and
                  Chunjiang Ge and
                  Tat{-}Seng Chua and
                  Zhiyuan Liu and
                  Maosong Sun and
                  Gao Huang},
  title        = {LLaVA-UHD: an {LMM} Perceiving Any Aspect Ratio and High-Resolution
                  Images},
  journal      = {arXiv: 2403.11703},
  year         = {2024}
}

@inproceedings{VLM:Llama-Adapter,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  booktitle={ICLR},
  year={2024}
}

@article{VLM:Llama-Adapterv2,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv: 2304.15010},
  year={2023}
}

@article{VLM:Monkey,
  author       = {Zhang Li and
                  Biao Yang and
                  Qiang Liu and
                  Zhiyin Ma and
                  Shuo Zhang and
                  Jingxu Yang and
                  Yabo Sun and
                  Yuliang Liu and
                  Xiang Bai},
  title        = {Monkey: Image Resolution and Text Label Are Important Things for Large
                  Multi-modal Models},
  journal      = {arXiv: 2311.06607},
  year         = {2023}
}

@article{VLM:Sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv: 2311.12793},
  year={2023}
}

@article{VLM:mPLUG-Owl,
  author       = {Qinghao Ye and
                  Haiyang Xu and
                  Guohai Xu and
                  Jiabo Ye and
                  Ming Yan and
                  Yiyang Zhou and
                  Junyang Wang and
                  Anwen Hu and
                  Pengcheng Shi and
                  Yaya Shi and
                  Chenliang Li and
                  Yuanhong Xu and
                  Hehong Chen and
                  Junfeng Tian and
                  Qian Qi and
                  Ji Zhang and
                  Fei Huang},
  title        = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
  journal      = {arXiv: 2304.14178},
  year         = {2023}
}

@article{VLM:mPLUG-Owl2,
  author       = {Qinghao Ye and
                  Haiyang Xu and
                  Jiabo Ye and
                  Ming Yan and
                  Anwen Hu and
                  Haowei Liu and
                  Qi Qian and
                  Ji Zhang and
                  Fei Huang and
                  Jingren Zhou},
  title        = {mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with
                  Modality Collaboration},
  journal      = {arXiv: 2311.04257},
  year         = {2023}
}

@article{VLM:qwen-vl,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={1},
  number={2},
  pages={3},
  year={2023}
}

@misc{VLMs:LLama3.2,
  author = {Meta Team},
  title = {Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
  url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
  year = {2024}
}

@inproceedings{VLP:BLIP,
  author       = {Junnan Li and
                  Dongxu Li and
                  Caiming Xiong and
                  Steven C. H. Hoi},
  title        = {{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language
                  Understanding and Generation},
  booktitle    = {ICLR},
  volume       = {162},
  pages        = {12888--12900},
  year         = {2022}
}

@inproceedings{VLP:BLIPv2,
  author       = {Junnan Li and
                  Dongxu Li and
                  Silvio Savarese and
                  Steven C. H. Hoi},
  title        = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image
                  Encoders and Large Language Models},
  booktitle    = {ICML},
  volume       = {202},
  pages        = {19730--19742},
  year         = {2023}
}

@inproceedings{VLP:CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {ICML},
  volume    = {139},
  pages     = {8748--8763},
  year      = {2021}
}

@inproceedings{VLP:Flamingo,
  author       = {Jean{-}Baptiste Alayrac and
                  Jeff Donahue and
                  Pauline Luc and
                  Antoine Miech and
                  Iain Barr and
                  Yana Hasson and
                  Karel Lenc and
                  Arthur Mensch and
                  Katherine Millican and
                  Malcolm Reynolds and
                  Roman Ring and
                  Eliza Rutherford and
                  Serkan Cabi and
                  Tengda Han and
                  Zhitao Gong and
                  Sina Samangooei and
                  Marianne Monteiro and
                  Jacob L. Menick and
                  Sebastian Borgeaud and
                  Andy Brock and
                  Aida Nematzadeh and
                  Sahand Sharifzadeh and
                  Mikolaj Binkowski and
                  Ricardo Barreira and
                  Oriol Vinyals and
                  Andrew Zisserman and
                  Kar{\'{e}}n Simonyan},
  title        = {Flamingo: a Visual Language Model for Few-Shot Learning},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@inproceedings{VQGAN,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={CVPR},
  pages={12873--12883},
  year={2021}
}

@article{beyer2024paligemma,
  title={Paligemma: A versatile 3b vlm for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv preprint arXiv:2407.07726},
  year={2024}
}

@article{chen2024solo,
  title={A Single Transformer for Scalable Vision-Language Modeling},
  author={Chen, Yangyi and Wang, Xingyao and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2407.06438},
  year={2024}
}

@article{li2024imagefolder,
  title={Imagefolder: Autoregressive image generation with folded tokens},
  author={Li, Xiang and Qiu, Kai and Chen, Hao and Kuen, Jason and Gu, Jiuxiang and Raj, Bhiksha and Lin, Zhe},
  journal={arXiv preprint arXiv:2410.01756},
  year={2024}
}

@article{luo2024mono,
  title={Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training},
  author={Luo, Gen and Yang, Xue and Dou, Wenhan and Wang, Zhaokai and Dai, Jifeng and Qiao, Yu and Zhu, Xizhou},
  journal={arXiv preprint arXiv:2410.08202},
  year={2024}
}

@article{qu2024tokenflow,
  title={Tokenflow: Unified image tokenizer for multimodal understanding and generation},
  author={Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong},
  journal={arXiv preprint arXiv:2412.03069},
  year={2024}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{van2017VQVAE,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@article{wang2024mio,
  title={Mio: A foundation model on multimodal tokens},
  author={Wang, Zekun and Zhu, King and Xu, Chunpu and Zhou, Wangchunshu and Liu, Jiaheng and Zhang, Yibo and Wang, Jiashuo and Shi, Ning and Li, Siyu and Li, Yizhi and others},
  journal={arXiv preprint arXiv:2409.17692},
  year={2024}
}

@article{wang2024qwen2-vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{wu2024janus,
  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},
  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2410.13848},
  year={2024}
}

@article{wu2024vila-u,
  title={Vila-u: a unified foundation model integrating visual understanding and generation},
  author={Wu, Yecheng and Zhang, Zhuoyang and Chen, Junyu and Tang, Haotian and Li, Dacheng and Fang, Yunhao and Zhu, Ligeng and Xie, Enze and Yin, Hongxu and Yi, Li and others},
  journal={arXiv preprint arXiv:2409.04429},
  year={2024}
}

@article{xie2024muse,
  title={MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding},
  author={Xie, Rongchang and Du, Chen and Song, Ping and Liu, Chang},
  journal={arXiv preprint arXiv:2411.17762},
  year={2024}
}

@article{xie2024show-o,
  title={Show-o: One single transformer to unify multimodal understanding and generation},
  author={Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2408.12528},
  year={2024}
}

@article{xue2024blip-3,
  title={xgen-mm (blip-3): A family of open large multimodal models},
  author={Xue, Le and Shu, Manli and Awadalla, Anas and Wang, Jun and Yan, An and Purushwalkam, Senthil and Zhou, Honglu and Prabhu, Viraj and Dai, Yutong and Ryoo, Michael S and others},
  journal={arXiv preprint arXiv:2408.08872},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

