@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
@String(L4DC = {L4DC})


@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


%----------------------------------------------%
%------------------ Datasets ------------------%
%----------------------------------------------%

@inproceedings{Datasets:MRPC,
  author       = {William B. Dolan and
                  Chris Brockett},
  title        = {Automatically Constructing a Corpus of Sentential Paraphrases},
  booktitle    = {IWP},
  year         = {2005}
}

@inproceedings{Datasets:RTE,
  author       = {Luisa Bentivogli and
                  Bernardo Magnini and
                  Ido Dagan and
                  Hoa Trang Dang and
                  Danilo Giampiccolo},
  title        = {The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
  booktitle    = {TAC},
  year         = {2009}
}

@inproceedings{Datasets:MSVD,
  author       = {David L. Chen and
                  William B. Dolan},
  title        = {Collecting Highly Parallel Data for Paraphrase Evaluation},
  booktitle    = {ACL},
  pages        = {190--200},
  year         = {2011}
}

@inproceedings{Datasets:SST-2,
  author       = {Richard Socher and
                  Alex Perelygin and
                  Jean Wu and
                  Jason Chuang and
                  Christopher D. Manning and
                  Andrew Y. Ng and
                  Christopher Potts},
  title        = {Recursive Deep Models for Semantic Compositionality Over a Sentiment
                  Treebank},
  booktitle    = {EMNLP},
  pages        = {1631--1642},
  year         = {2013}
}

@inproceedings{Datasets:MSCOCO,
	author    = {Tsung{-}Yi Lin and
	Michael Maire and
	Serge J. Belongie and
	James Hays and
	Pietro Perona and
	Deva Ramanan and
	Piotr Doll{\'{a}}r and
	C. Lawrence Zitnick},
	title     = {Microsoft {COCO:} Common Objects in Context},
	booktitle = {ECCV},
	volume    = {8693},
	pages     = {740--755},
	year      = {2014}
}

@article{Datasets:Flickr30k,
	author    = {Peter Young and
	Alice Lai and
	Micah Hodosh and
	Julia Hockenmaier},
	title     = {From image descriptions to visual denotations: New similarity metrics
	for semantic inference over event descriptions},
	journal   = {{TACL}},
	volume    = {2},
	pages     = {67--78},
	year      = {2014}
}

@article{Datasets:Imagenet,
	author    = {Olga Russakovsky and
	Jia Deng and
	Hao Su and
	Jonathan Krause and
	Sanjeev Satheesh and
	Sean Ma and
	Zhiheng Huang and
	Andrej Karpathy and
	Aditya Khosla and
	Michael S. Bernstein and
	Alexander C. Berg and
	Fei{-}Fei Li},
	title     = {ImageNet Large Scale Visual Recognition Challenge},
	journal   = {IJCV},
	volume    = {115},
	number    = {3},
	pages     = {211--252},
	year      = {2015}
}

@inproceedings{Datasets:Flickr30kEntities,
  author       = {Bryan A. Plummer and
                  Liwei Wang and
                  Chris M. Cervantes and
                  Juan C. Caicedo and
                  Julia Hockenmaier and
                  Svetlana Lazebnik},
  title        = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
                  Richer Image-to-Sentence Models},
  booktitle    = {ICCV},
  pages        = {2641--2649},
  year         = {2015}
}

@inproceedings{Datasets:REFCOCO,
  author       = {Licheng Yu and
                  Patrick Poirson and
                  Shan Yang and
                  Alexander C. Berg and
                  Tamara L. Berg},
  title        = {Modeling Context in Referring Expressions},
  booktitle    = {ECCV},
  volume       = {9906},
  pages        = {69--85},
  year         = {2016}
}

@inproceedings{Datasets:REFCOCOG,
  author       = {Junhua Mao and
                  Jonathan Huang and
                  Alexander Toshev and
                  Oana Camburu and
                  Alan L. Yuille and
                  Kevin Murphy},
  title        = {Generation and Comprehension of Unambiguous Object Descriptions},
  booktitle    = {CVPR},
  pages        = {11--20},
  year         = {2016}
}

@inproceedings{Datasets:MSRVTT,
  author       = {Jun Xu and
                  Tao Mei and
                  Ting Yao and
                  Yong Rui},
  title        = {{MSR-VTT:} {A} Large Video Description Dataset for Bridging Video
                  and Language},
  booktitle    = {CVPR},
  pages        = {5288--5296},
  year         = {2016}
}

@inproceedings{Datasets:QNLI,
  author       = {Pranav Rajpurkar and
                  Jian Zhang and
                  Konstantin Lopyrev and
                  Percy Liang},
  title        = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  booktitle    = {EMNLP},
  pages        = {2383--2392},
  year         = {2016}
}

@article{Datasets:STS-B,
  author       = {Daniel M. Cer and
                  Mona T. Diab and
                  Eneko Agirre and
                  I{\~{n}}igo Lopez{-}Gazpio and
                  Lucia Specia},
  title        = {SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and
                  Cross-lingual Focused Evaluation},
  journal      = {arXiv: 1708.00055},
  year         = {2017}
}

@article{Datasets:QQP,
  title={First quora dataset release: Question pairs},
  author={Iyer, Shankar and Dandekar, Nikhil and Csernai, Korn{\'e}l and others},
  journal={data. quora. com},
  year={2017}
}

@inproceedings{Datasets:VQAv2,
  author       = {Yash Goyal and
                  Tejas Khot and
                  Douglas Summers{-}Stay and
                  Dhruv Batra and
                  Devi Parikh},
  title        = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding
                  in Visual Question Answering},
  booktitle    = {CVPR},
  pages        = {6325--6334},
  year         = {2017}
}

@article{Datasets:VisualGenome,
	author    = {Ranjay Krishna and
	Yuke Zhu and
	Oliver Groth and
	Justin Johnson and
	Kenji Hata and
	Joshua Kravitz and
	Stephanie Chen and
	Yannis Kalantidis and
	Li{-}Jia Li and
	David A. Shamma and
	Michael S. Bernstein and
	Li Fei{-}Fei},
	title     = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
	Image Annotations},
	journal   = {IJCV},
	volume    = {123},
	number    = {1},
	pages     = {32--73},
	year      = {2017}
}

@inproceedings{Datasets:MNLI,
  author       = {Adina Williams and
                  Nikita Nangia and
                  Samuel R. Bowman},
  title        = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
                  Inference},
  booktitle    = {NAACL},
  pages        = {1112--1122},
  year         = {2018}
}

@inproceedings{Datasets:VLN,
  author       = {Peter Anderson and
                  Qi Wu and
                  Damien Teney and
                  Jake Bruce and
                  Mark Johnson and
                  Niko S{\"{u}}nderhauf and
                  Ian D. Reid and
                  Stephen Gould and
                  Anton van den Hengel},
  title        = {Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation
                  Instructions in Real Environments},
  booktitle    = {CVPR},
  pages        = {3674--3683},
  year         = {2018}
}

@inproceedings{Datasets:WSL,
  author       = {Dhruv Mahajan and
                  Ross B. Girshick and
                  Vignesh Ramanathan and
                  Kaiming He and
                  Manohar Paluri and
                  Yixuan Li and
                  Ashwin Bharambe and
                  Laurens van der Maaten},
  title        = {Exploring the Limits of Weakly Supervised Pretraining},
  booktitle    = {ECCV},
  volume       = {11206},
  pages        = {185--201},
  year         = {2018}
}

@article{Datasets:CoLA,
  author       = {Alex Warstadt and
                  Amanpreet Singh and
                  Samuel R. Bowman},
  title        = {Neural Network Acceptability Judgments},
  journal      = {TACL},
  volume       = {7},
  pages        = {625--641},
  year         = {2019}
}

@inproceedings{Datasets:GQA,
  author       = {Drew A. Hudson and
                  Christopher D. Manning},
  title        = {{GQA:} {A} New Dataset for Real-World Visual Reasoning and Compositional
                  Question Answering},
  booktitle    = {CVPR},
  pages        = {6700--6709},
  year         = {2019}
}

@inproceedings{Datasets:NLVR,
  author       = {Alane Suhr and
                  Stephanie Zhou and
                  Ally Zhang and
                  Iris Zhang and
                  Huajun Bai and
                  Yoav Artzi},
  title        = {A Corpus for Reasoning about Natural Language Grounded in Photographs},
  booktitle    = {ACL},
  pages        = {6418--6428},
  year         = {2019}
}

@inproceedings{Datasets:GLUE,
  author       = {Alex Wang and
                  Amanpreet Singh and
                  Julian Michael and
                  Felix Hill and
                  Omer Levy and
                  Samuel R. Bowman},
  title        = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  booktitle    = {ICLR},
  year         = {2019}
}

@inproceedings{Datasets:PhraseCut,
  author       = {Chenyun Wu and
                  Zhe Lin and
                  Scott Cohen and
                  Trung Bui and
                  Subhransu Maji},
  title        = {PhraseCut: Language-Based Image Segmentation in the Wild},
  booktitle    = {CVPR},
  pages        = {10213--10222},
  year         = {2020}
}

@article{Datasets:Laion-5b,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={NeurIPS},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@inproceedings{Datasets:ScienceQA,
  author       = {Pan Lu and
                  Swaroop Mishra and
                  Tony Xia and
                  Liang Qiu and
                  Kai{-}Wei Chang and
                  Song{-}Chun Zhu and
                  Oyvind Tafjord and
                  Peter Clark and
                  Ashwin Kalyan},
  title        = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science
                  Question Answering},
  booktitle      = {NeurIPS},
  year         = {2022}
}

@article{Datasets:MM-vet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv: 2308.02490},
  year={2023}
}

@article{Datasets:Hallusionbench,
  title={Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  journal={arXiv: 2310.14566},
  year={2023}
}

@article{Datasets:Mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv: 2310.02255},
  year={2023}
}

@article{Datasets:MMMU,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv: 2311.16502},
  year={2023}
}


@article{Datasets:Seed-bench,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv: 2307.16125},
  year={2023}
}

@article{Datasets:OCRBench,
  title={On the hidden mystery of ocr in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Yang, Biao and Li, Chunyuan and Yin, Xucheng and Liu, Cheng-lin and Jin, Lianwen and Bai, Xiang},
  journal={arXiv preprint arXiv:2305.07895},
  year={2023}
}

@inproceedings{Datasets:InfoVQA,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={WACV},
  pages={1697--1706},
  year={2022}
}

@article{Datasets:MMT-Bench,
  title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and Lei, Jiayi and Lu, Quanfeng and Chen, Runjian and Xu, Peng and Zhang, Renrui and Zhang, Haozhe and Gao, Peng and Wang, Yali and Qiao, Yu and Luo, Ping and Zhang, Kaipeng and Shao, Wenqi},
  journal={arXiv: 2404.16006},
  year={2024}
}

@article{Datasets:LAION-COCO,
  title={Laion coco: 600m synthetic captions from laion2b-en.},
  author={Schuhmann, Christoph and Köpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
  journal = {https://laion.ai/blog/laion-coco/},
  year={2022}
}


@inproceedings{Datasets:Ok-vqa,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={CVPR},
  pages={3195--3204},
  year={2019}
}

@inproceedings{Datasets:A-okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={ECCV},
  pages={146--162},
  year={2022}
}

@article{Datasets:Iconqa,
  title={Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning},
  author={Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv: 2110.13214},
  year={2021}
}

@inproceedings{Datasets:AI2D,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={ECCV},
  pages={235--251},
  year={2016}
}

@inproceedings{Datasets:VisDialog,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle={CVPR},
  pages={326--335},
  year={2017}
}


@inproceedings{Datasets:ChartQA,
  title={ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author={Masry, Ahmed and Do, Xuan Long and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  booktitle={ACL},
  pages={2263--2279},
  year={2022}
}


@inproceedings{Datasets:STVQA,
  title={Scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  booktitle={ICCV},
  pages={4291--4301},
  year={2019}
}

@inproceedings{Datasets:DocVQA,
  title={Simple and Effective Multi-Paragraph Reading Comprehension},
  author={Clark, Christopher and Gardner, Matt},
  booktitle={ACL},
  pages={845--855},
  year={2018}
}

@inproceedings{Datasets:OCRVQA,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={ICDAR},
  pages={947--952},
  year={2019}
}

@inproceedings{Datasets:BSTVQA,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle={CVPR},
  pages={10126--10135},
  year={2020}
}

@article{Datasets:OpenImages,
  author       = {Alina Kuznetsova and
                  Hassan Rom and
                  Neil Alldrin and
                  Jasper R. R. Uijlings and
                  Ivan Krasin and
                  Jordi Pont{-}Tuset and
                  Shahab Kamali and
                  Stefan Popov and
                  Matteo Malloci and
                  Tom Duerig and
                  Vittorio Ferrari},
  title        = {The Open Images Dataset {V4:} Unified image classification, object
                  detection, and visual relationship detection at scale},
  journal      = {arXiv: 1811.00982},
  year         = {2018}
}

@inproceedings{Datasets:SynthDog,
  title     = {OCR-Free Document Understanding Transformer},
  author    = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle = {ECCV},
  year      = {2022}
}

@misc{Datasets:Vision-Flan,
        title = {Vision-Flan:Scaling Visual Instruction Tuning},
        url = {https://vision-flan.github.io/},
        author = {Zhiyang Xu and Trevor Ashby and Chao Feng and Rulin Shao and Ying Shen and Di Jin and Qifan Wang and Lifu Huang},
        year = {2023}
}

@inproceedings{Datasets:DVQA,
  author       = {Kushal Kafle and
                  Brian L. Price and
                  Scott Cohen and
                  Christopher Kanan},
  title        = {{DVQA:} Understanding Data Visualizations via Question Answering},
  booktitle    = {CVPR},
  pages        = {5648--5656},
  year         = {2018}
}

@inproceedings{Datasets:VizWiz,
  author       = {Danna Gurari and
                  Qing Li and
                  Abigale J. Stangl and
                  Anhong Guo and
                  Chi Lin and
                  Kristen Grauman and
                  Jiebo Luo and
                  Jeffrey P. Bigham},
  title        = {VizWiz Grand Challenge: Answering Visual Questions From Blind People},
  booktitle    = {CVPR},
  pages        = {3608--3617},
  year         = {2018}
}

@inproceedings{Datasets:TextVQA,
  author       = {Amanpreet Singh and
                  Vivek Natarajan and
                  Meet Shah and
                  Yu Jiang and
                  Xinlei Chen and
                  Dhruv Batra and
                  Devi Parikh and
                  Marcus Rohrbach},
  title        = {Towards {VQA} Models That Can Read},
  booktitle    = {CVPR},
  year         = {2019}
}

@inproceedings{Datasets:POPE,
  author       = {Yifan Li and
                  Yifan Du and
                  Kun Zhou and
                  Jinpeng Wang and
                  Wayne Xin Zhao and
                  Ji{-}Rong Wen},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Evaluating Object Hallucination in Large Vision-Language Models},
  booktitle    = {EMNLP},
  pages        = {292--305},
  year         = {2023}
}

@article{Datasets:MMBench,
  author       = {Yuan Liu and
                  Haodong Duan and
                  Yuanhan Zhang and
                  Bo Li and
                  Songyang Zhang and
                  Wangbo Zhao and
                  Yike Yuan and
                  Jiaqi Wang and
                  Conghui He and
                  Ziwei Liu and
                  Kai Chen and
                  Dahua Lin},
  title        = {MMBench: Is Your Multi-modal Model an All-around Player?},
  journal      = {arXiv: 2307.06281},
  year         = {2023}
}

@article{Datasets:MME,
  author       = {Chaoyou Fu and
                  Peixian Chen and
                  Yunhang Shen and
                  Yulei Qin and
                  Mengdan Zhang and
                  Xu Lin and
                  Zhenyu Qiu and
                  Wei Lin and
                  Jinrui Yang and
                  Xiawu Zheng and
                  Ke Li and
                  Xing Sun and
                  Rongrong Ji},
  title        = {{MME:} {A} Comprehensive Evaluation Benchmark for Multimodal Large
                  Language Models},
  journal      = {arXiv: 2306.13394},
  year    = {2023}
}

@article{Datasets:LAION-Dedump,
  title={On the de-duplication of laion-2b},
  author={Webster, Ryan and Rabin, Julien and Simon, Loic and Jurie, Frederic},
  journal={arXiv preprint arXiv:2303.12733},
  year={2023}
}

%----------------------------------------------%
%------------------ Training ------------------%
%----------------------------------------------%

@article{Training:Dropout,
    author    = {Nitish Srivastava and
               Geoffrey E. Hinton and
               Alex Krizhevsky and
               Ilya Sutskever and
               Ruslan Salakhutdinov},
    title     = {Dropout: a simple way to prevent neural networks from overfitting},
    journal   = {J. Mach. Learn. Res.},
    volume    = {15},
    number    = {1},
    pages     = {1929--1958},
    year      = {2014}
}

@inproceedings{Training:Adam,
	author    = {Diederik P. Kingma and
	Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	booktitle = {ICLR},
	year      = {2015}
}

@article{Training:MPT,
  author       = {Paulius Micikevicius and
                  Sharan Narang and
                  Jonah Alben and
                  Gregory F. Diamos and
                  Erich Elsen and
                  David Garc{\'{\i}}a and
                  Boris Ginsburg and
                  Michael Houston and
                  Oleksii Kuchaiev and
                  Ganesh Venkatesh and
                  Hao Wu},
  title        = {Mixed Precision Training},
  journal      = {arXiv: 1710.03740},
  year         = {2017},
}

@article{Training:Sublinear,
  author       = {Tianqi Chen and
                  Bing Xu and
                  Chiyuan Zhang and
                  Carlos Guestrin},
  title        = {Training Deep Nets with Sublinear Memory Cost},
  journal      = {arXiv: 1604.06174},
  year         = {2016}
}

@inproceedings{Training:8-bit,
  author       = {Naigang Wang and
                  Jungwook Choi and
                  Daniel Brand and
                  Chia{-}Yu Chen and
                  Kailash Gopalakrishnan},
  title        = {Training Deep Neural Networks with 8-bit Floating Point Numbers},
  booktitle    = {NeurIPS},
  pages        = {7686--7695},
  year         = {2018}
}

@inproceedings{Training:ZeRO,
  author       = {Samyam Rajbhandari and
                  Jeff Rasley and
                  Olatunji Ruwase and
                  Yuxiong He},
  title        = {ZeRO: memory optimizations toward training trillion parameter models},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
                  Georgia, USA, November 9-19, 2020},
  pages        = {20},
  year         = {2020}
}


%----------------------------------------------%
%------------------- CNN ----------------------%
%----------------------------------------------%

@inproceedings{CNN:Resnet16,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	booktitle = {CVPR},
	pages     = {770--778},
	year      = {2016}
}

@inproceedings{CNN:Resnext17,
  author       = {Saining Xie and
                  Ross B. Girshick and
                  Piotr Doll{\'{a}}r and
                  Zhuowen Tu and
                  Kaiming He},
  title        = {Aggregated Residual Transformations for Deep Neural Networks},
  booktitle    = {CVPR},
  pages        = {5987--5995},
  year         = {2017}
}

@inproceedings{CNN:RevNet,
  author       = {Aidan N. Gomez and
                  Mengye Ren and
                  Raquel Urtasun and
                  Roger B. Grosse},
  title        = {The Reversible Residual Network: Backpropagation Without Storing Activations},
  booktitle    = {NeurIPS},
  pages        = {2214--2224},
  year         = {2017}
}

%----------------------------------------------%
%------------------- RNN ----------------------%
%----------------------------------------------%

@article{RNN:Bi-GRU,
	author    = {Mike Schuster and
	Kuldip K. Paliwal},
	title     = {Bidirectional recurrent neural networks},
	journal   = {TSP},
	volume    = {45},
	number    = {11},
	pages     = {2673--2681},
	year      = {1997}
}

@inproceedings{RNN:FV,
	author    = {Florent Perronnin and
	Christopher R. Dance},
	title     = {Fisher Kernels on Visual Vocabularies for Image Categorization},
	booktitle = {CVPR},
	year      = {2007},
}

@inproceedings{RNN:Skip-Gram,
	author    = {Tomas Mikolov and
	Kai Chen and
	Greg Corrado and
	Jeffrey Dean},
	title     = {Efficient Estimation of Word Representations in Vector Space},
	booktitle = {ICLR},
	year      = {2013}
}

@article{RNN:GRU,
	author    = {Ryan Kiros and
	Ruslan Salakhutdinov and
	Richard S. Zemel},
	title     = {Unifying Visual-Semantic Embeddings with Multimodal Neural Language
	Models},
	journal   = {arXiv: 1411.2539},
	year      = {2014}
}

%----------------------------------------------%
%---------------- Transformer -----------------%
%----------------------------------------------%

@inproceedings{TransF:Transformer,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention is All you Need},
	booktitle = {NIPS},
	pages     = {5998--6008},
	year      = {2017},
}

@inproceedings{TransF:BERT,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {NAACL},
  pages        = {4171--4186},
  year         = {2019},
}

@article{TransF:GPT-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{TransF:RoBERTa,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {arXiv: 1907.11692},
  year         = {2019}
}

@article{TransF:T5,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {JMLR},
  volume       = {21},
  pages        = {140:1--140:67},
  year         = {2020}
}

@inproceedings{TransF:GPT,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS},
  year         = {2020},
}

@inproceedings{TransF:Reformer,
  author       = {Nikita Kitaev and
                  Lukasz Kaiser and
                  Anselm Levskaya},
  title        = {Reformer: The Efficient Transformer},
  booktitle    = {ICLR},
  year         = {2020}
}

@inproceedings{TransF:ViT,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {ICLR},
  year         = {2021},
}

@inproceedings{TransF:SwinTransformer,
  author       = {Ze Liu and
                  Yutong Lin and
                  Yue Cao and
                  Han Hu and
                  Yixuan Wei and
                  Zheng Zhang and
                  Stephen Lin and
                  Baining Guo},
  title        = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  booktitle    = {ICCV},
  pages        = {9992--10002},
  publisher    = {{IEEE}},
  year         = {2021}
}

@inproceedings{TransF:BEiT,
  author       = {Hangbo Bao and
                  Li Dong and
                  Songhao Piao and
                  Furu Wei},
  title        = {BEiT: {BERT} Pre-Training of Image Transformers},
  booktitle    = {ICLR},
  year         = {2022}
}

@inproceedings{TransF:MAE,
  author       = {Kaiming He and
                  Xinlei Chen and
                  Saining Xie and
                  Yanghao Li and
                  Piotr Doll{\'{a}}r and
                  Ross B. Girshick},
  title        = {Masked Autoencoders Are Scalable Vision Learners},
  booktitle    = {CVPR},
  pages        = {15979--15988},
  year         = {2022}
}

@inproceedings{TransF:Rev-ViT,
  author       = {Karttikeya Mangalam and
                  Haoqi Fan and
                  Yanghao Li and
                  Chao{-}Yuan Wu and
                  Bo Xiong and
                  Christoph Feichtenhofer and
                  Jitendra Malik},
  title        = {Reversible Vision Transformers},
  booktitle    = {CVPR},
  pages        = {10820--10830},
  year         = {2022}
}

@article{TransF:SAM,
  author       = {Alexander Kirillov and
                  Eric Mintun and
                  Nikhila Ravi and
                  Hanzi Mao and
                  Chlo{\'{e}} Rolland and
                  Laura Gustafson and
                  Tete Xiao and
                  Spencer Whitehead and
                  Alexander C. Berg and
                  Wan{-}Yen Lo and
                  Piotr Doll{\'{a}}r and
                  Ross B. Girshick},
  title        = {Segment Anything},
  journal      = {arXiv: 2304.02643},
  year         = {2023}
}

@inproceedings{TransF:EVA,
  author       = {Yuxin Fang and
                  Wen Wang and
                  Binhui Xie and
                  Quan Sun and
                  Ledell Wu and
                  Xinggang Wang and
                  Tiejun Huang and
                  Xinlong Wang and
                  Yue Cao},
  title        = {{EVA:} Exploring the Limits of Masked Visual Representation Learning
                  at Scale},
  booktitle    = {CVPR},
  pages        = {19358--19369},
  year         = {2023}
}

@article{VLM:EVE,
  title={Unveiling Encoder-Free Vision-Language Models},
  author={Diao, Haiwen and Cui, Yufeng and Li, Xiaotong and Wang, Yueze and Lu, Huchuan and Wang, Xinlong},
  journal={arXiv preprint arXiv:2406.11832},
  year={2024}
}

@article{TransF:LLaMA,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {arXiv: 2302.13971},
  year         = {2023}
}

@article{TransF:LLaMA2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv: 2307.09288},
  year={2023}
}

@article{TransF:phi-1-5,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@misc{TransF:Vicuna,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{TransF:Alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{TransF:InternLM,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

@article{TransF:InternLM2,
  author       = {Zheng Cai and
                  Maosong Cao and
                  Haojiong Chen and
                  Kai Chen and
                  Keyu Chen and
                  Xin Chen and
                  Xun Chen and
                  Zehui Chen and
                  Zhi Chen and
                  Pei Chu and
                  Xiaoyi Dong and
                  Haodong Duan and
                  Qi Fan and
                  Zhaoye Fei and
                  Yang Gao and
                  Jiaye Ge and
                  Chenya Gu and
                  Yuzhe Gu and
                  Tao Gui and
                  Aijia Guo and
                  Qipeng Guo and
                  Conghui He and
                  Yingfan Hu and
                  Ting Huang and
                  Tao Jiang and
                  Penglong Jiao and
                  Zhenjiang Jin and
                  Zhikai Lei and
                  Jiaxing Li and
                  Jingwen Li and
                  Linyang Li and
                  Shuaibin Li and
                  Wei Li and
                  Yining Li and
                  Hongwei Liu and
                  Jiangning Liu and
                  Jiawei Hong and
                  Kaiwen Liu and
                  Kuikun Liu and
                  Xiaoran Liu and
                  Chengqi Lv and
                  Haijun Lv and
                  Kai Lv and
                  Li Ma and
                  Runyuan Ma and
                  Zerun Ma and
                  Wenchang Ning and
                  Linke Ouyang and
                  Jiantao Qiu and
                  Yuan Qu and
                  Fukai Shang and
                  Yunfan Shao and
                  Demin Song and
                  Zifan Song and
                  Zhihao Sui and
                  Peng Sun and
                  Yu Sun and
                  Huanze Tang and
                  Bin Wang and
                  Guoteng Wang and
                  Jiaqi Wang and
                  Jiayu Wang and
                  Rui Wang and
                  Yudong Wang and
                  Ziyi Wang and
                  Xingjian Wei and
                  Qizhen Weng and
                  Fan Wu and
                  Yingtong Xiong and
                  et al.},
  title        = {InternLM2 Technical Report},
  journal      = {arXiv: 2403.17297},
  year         = {2024}
}

@inproceedings{TransF:GLM,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={ACL},
  pages={320--335},
  year={2022}
}

@inproceedings{TransF:PaLI,
  title={PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  booktitle={ICLR},
  year={2022}
}

@article{TransF:Qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv: 2309.16609},
  year={2023}
}


@misc{TransF:Bard,
  title        = {Google Bard},
  author       = {Google},
  year         = {2023},
  howpublished = {\url{https://bard.google.com/}},
}

@article{TransF:Baichuan2,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv: 2309.10305},
  url={https://arxiv.org/abs/2309.10305},
  year={2023}
}

@article{TransF:EVA-CLIP-18B,
  author       = {Quan Sun and
                  Jinsheng Wang and
                  Qiying Yu and
                  Yufeng Cui and
                  Fan Zhang and
                  Xiaosong Zhang and
                  Xinlong Wang},
  title        = {{EVA-CLIP-18B:} Scaling {CLIP} to 18 Billion Parameters},
  journal      = {arXiv: 2402.04252},
  year         = {2024}
}

@article{VLM:movq,
  title={Movq: Modulating quantized vectors for high-fidelity image generation},
  author={Zheng, Chuanxia and Vuong, Tung-Long and Cai, Jianfei and Phung, Dinh},
  journal={NeurIPS},
  volume={35},
  pages={23412--23425},
  year={2022}
}

@article{luo2024mono,
  title={Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training},
  author={Luo, Gen and Yang, Xue and Dou, Wenhan and Wang, Zhaokai and Dai, Jifeng and Qiao, Yu and Zhu, Xizhou},
  journal={arXiv preprint arXiv:2410.08202},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{VLM:EMU3,
  title={Emu3: Next-Token Prediction is All You Need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}

@inproceedings{TransF:Siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  pages={11975--11986},
  year={2023}
}

@misc{TransF:Persimmon-8b,
  author = {Elsen, Erich and Odena, Augustus and Nye, Maxwell and Ta\c{s}\i{}rlar, Sa\u{g}nak and Dao, Tri and Hawthorne, Curtis and Moparthi, Deepak and Somani, Arushi},
  title = {Releasing {Persimmon-8B}},
  url = {https://www.adept.ai/blog/persimmon-8b},
  year = {2023}
}

@article{TransF:Deepseekllm,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv: 2401.02954},
  year={2024}
}

@article{TransF:EVA-CLIP,
  author       = {Quan Sun and
                  Yuxin Fang and
                  Ledell Wu and
                  Xinlong Wang and
                  Yue Cao},
  title        = {{EVA-CLIP:} Improved Training Techniques for {CLIP} at Scale},
  journal      = {arXiv: 2303.15389},
  year         = {2023}
}

%-------------------------------------------%
%------------------ Graph ------------------%
%-------------------------------------------%

@inproceedings{Graph:CNG,
	author    = {David Duvenaud and
	Dougal Maclaurin and
	Jorge Aguilera{-}Iparraguirre and
	Rafael G{\'{o}}mez{-}Bombarelli and
	Timothy Hirzel and
	Al{\'{a}}n Aspuru{-}Guzik and
	Ryan P. Adams},
	title     = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
	booktitle = {NIPS},
	pages     = {2224--2232},
	year      = {2015},
}

@inproceedings{Graph:GNN,
	author    = {Yujia Li and
	Daniel Tarlow and
	Marc Brockschmidt and
	Richard S. Zemel},
	title     = {Gated Graph Sequence Neural Networks},
	booktitle = {ICLR},
	year      = {2016}
}

@inproceedings{Graph:GCN,
	author    = {Thomas N. Kipf and
	Max Welling},
	title     = {Semi-Supervised Classification with Graph Convolutional Networks},
	booktitle = {ICLR},
	year      = {2017}
}

%-----------------------------------------------%
%------------------ Regulator ------------------%
%-----------------------------------------------%

@article{Regulator:AdaFR,
    author    = {Kirtankumar Thakkar and
               Victor Paredes and
               Ayonga Hereid},
    title     = {Adaptive Feedback Regulator for Powered Lower-Limb Exoskeleton under Model Uncertainty},
    journal   = {arXiv: 2104.11775},
    year      = {2021}
}

@inproceedings{Regulator:Feedpixel,
    author    = {Murad Abu{-}Khalaf and
               Sertac Karaman and
               Daniela Rus},
    title     = {Feedback from Pixels: Output Regulation via Learning-based Scene View
               Synthesis},
    booktitle = {L4DC},
    volume    = {144},
    pages     = {828--841},
    year      = {2021}
}

%-----------------------------------------------%
%------------------ Detection ------------------%
%-----------------------------------------------%

@article{Detection:DTPBM,
    author    = {Pedro F. Felzenszwalb and
               Ross B. Girshick and
               David A. McAllester and
               Deva Ramanan},
    title     = {Object Detection with Discriminatively Trained Part-Based Models},
    journal   = {TPAMI},
    volume    = {32},
    number    = {9},
    pages     = {1627--1645},
    year      = {2010}
}

@inproceedings{Detection:exemplarSVMs,
    author    = {Tomasz Malisiewicz and
               Abhinav Gupta and
               Alexei A. Efros},
    title     = {Ensemble of exemplar-SVMs for object detection and beyond},
    booktitle = {ICCV},
    pages     = {89--96},
    year      = {2011}
}

@inproceedings{Detection:FasterR-CNN,
	author    = {Shaoqing Ren and
	Kaiming He and
	Ross B. Girshick and
	Jian Sun},
	title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal Networks},
	booktitle = {NIPS},
	pages     = {91--99},
	year      = {2015}
}

%----------------------------------------------------%
%-------------- Knowledge Distillation --------------%
%----------------------------------------------------%

@inproceedings{KD:MC,
  author    = {Cristian Bucila and
               Rich Caruana and
               Alexandru Niculescu{-}Mizil},
  title     = {Model compression},
  booktitle = {SIGKDD},
  pages     = {535--541},
  year      = {2006}
}

@inproceedings{KD:DDNRN,
  author    = {Jimmy Ba and
               Rich Caruana},
  title     = {Do Deep Nets Really Need to be Deep?},
  booktitle = {NIPS},
  pages     = {2654--2662},
  year      = {2014}
}

@article{KD:KD,
  author    = {Geoffrey E. Hinton and
               Oriol Vinyals and
               Jeffrey Dean},
  title     = {Distilling the Knowledge in a Neural Network},
  journal   = {arXiv: 1503.02531},
  year      = {2015},
}

@inproceedings{KD:Fitnet,
    author    = {Adriana Romero and
               Nicolas Ballas and
               Samira Ebrahimi Kahou and
               Antoine Chassang and
               Carlo Gatta and
               Yoshua Bengio},
    title     = {FitNets: Hints for Thin Deep Nets},
    booktitle = {ICLR},
    year      = {2015}
}

@book{KD:DL,
    author    = {Ian J. Goodfellow and
               Yoshua Bengio and
               Aaron C. Courville},
    title     = {Deep Learning},
    series    = {Adaptive computation and machine learning},
    publisher = {{MIT} Press},
    year      = {2016}
}

@article{KD:NST,
    author    = {Zehao Huang and
               Naiyan Wang},
    title     = {Like What You Like: Knowledge Distill via Neuron Selectivity Transfer},
    journal   = {arXiv: 1707.01219},
    year      = {2017},
}

@inproceedings{KD:FSP,
    author    = {Junho Yim and
               Donggyu Joo and
               Ji{-}Hoon Bae and
               Junmo Kim},
    title     = {A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning},
    booktitle = {CVPR},
    pages     = {7130--7138},
    year      = {2017}
}

@inproceedings{KD:AT,
    author    = {Sergey Zagoruyko and
               Nikos Komodakis},
    title     = {Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer},
    booktitle = {ICLR},
    year      = {2017},
}

@inproceedings{KD:KDSVD,
    author    = {Seung Hyun Lee and
               Dae Ha Kim and
               Byung Cheol Song},
    title     = {Self-supervised Knowledge Distillation Using Singular Value Decomposition},
    booktitle = {ECCV},
    volume    = {11210},
    pages     = {339--354},
    year      = {2018},
}

@inproceedings{KD:Darkrank,
    author    = {Yuntao Chen and
               Naiyan Wang and
               Zhaoxiang Zhang},
    title     = {DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer},
    booktitle = {AAAI},
    pages     = {2852--2859},
    year      = {2018}
}

@inproceedings{KD:FT,
    author    = {Jangho Kim and
               Seonguk Park and
               Nojun Kwak},
    title     = {Paraphrasing Complex Network: Network Compression via Factor Transfer},
    booktitle = {NeurIPS},
    pages     = {2765--2774},
    year      = {2018},
}

@inproceedings{KD:DML,
    author    = {Ying Zhang and
               Tao Xiang and
               Timothy M. Hospedales and
               Huchuan Lu},
    title     = {Deep Mutual Learning},
    booktitle = {CVPR},
    pages     = {4320--4328},
    year      = {2018}
}

@inproceedings{KD:LMFT,
    author    = {Lu Yu and
               Vacit Oguz Yazici and
               Xialei Liu and
               Joost van de Weijer and
               Yongmei Cheng and
               Arnau Ramisa},
    title     = {Learning Metrics From Teachers: Compact Networks for Image Embedding},
    booktitle = {CVPR},
    pages     = {2907--2916},
    year      = {2019}
}

@inproceedings{KD:VID,
    author    = {Sungsoo Ahn and
               Shell Xu Hu and
               Andreas C. Damianou and
               Neil D. Lawrence and
               Zhenwen Dai},
    title     = {Variational Information Distillation for Knowledge Transfer},
    booktitle = {CVPR},
    pages     = {9163--9171},
    year      = {2019}
}

@inproceedings{KD:AB,
    author    = {Byeongho Heo and
               Minsik Lee and
               Sangdoo Yun and
               Jin Young Choi},
    title     = {Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons},
    booktitle = {AAAI},
    pages     = {3779--3787},
    year      = {2019}
}

@inproceedings{KD:CC,
    author    = {Baoyun Peng and
               Xiao Jin and
               Dongsheng Li and
               Shunfeng Zhou and
               Yichao Wu and
               Jiaheng Liu and
               Zhaoning Zhang and
               Yu Liu},
    title     = {Correlation Congruence for Knowledge Distillation},
    booktitle = {ICCV},
    pages     = {5006--5015},
    year      = {2019}
}

@inproceedings{KD:SP,
    author    = {Frederick Tung and
               Greg Mori},
    title     = {Similarity-Preserving Knowledge Distillation},
    booktitle = {ICCV},
    pages     = {1365--1374},
    year      = {2019}
}

@inproceedings{KD:RKD,
    author    = {Wonpyo Park and
               Dongju Kim and
               Yan Lu and
               Minsu Cho},
    title     = {Relational Knowledge Distillation},
    booktitle = {CVPR},
    pages     = {3967--3976},
    year      = {2019}
}

@inproceedings{KD:BYOT,
    author    = {Linfeng Zhang and
    Jiebo Song and
    Anni Gao and
    Jingwei Chen and
    Chenglong Bao and
    Kaisheng Ma},
    title     = {Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},
    booktitle = {ICCV},
    pages     = {3712--3721},
    year      = {2019}
}
  
@inproceedings{KD:MetaDistiller,
    author    = {Benlin Liu and
    Yongming Rao and
    Jiwen Lu and
    Jie Zhou and
    Cho{-}jui Hsieh},
    title     = {MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation},
    booktitle = {ECCV},
    volume    = {12359},
    pages     = {694--709},
    year      = {2020}
}

@article{KD:PKT,
  author    = {Nikolaos Passalis and
               Maria Tzelepi and
               Anastasios Tefas},
  title     = {Probabilistic Knowledge Transfer for Lightweight Deep Representation Learning},
  journal   = {TNNLS},
  volume    = {32},
  number    = {5},
  pages     = {2030--2039},
  year      = {2021}
}

%---------------------------------------------------%
%-------------- Contrastive Learning ---------------%
%---------------------------------------------------%

@article{CL:Polyak,
    author  =   {Polyak, Boris T and Juditsky, Anatoli B},
    title   =   {Acceleration of stochastic approximation by averaging},
    journal =   {SIAM journal on control and optimization},
    volume  =   {30},
    number  =   {4},
    pages   =   {838--855},
    year    =   {1992}
}

@techreport{CL:Robbins-Monro,
    author  =   {Ruppert, David},
    title   =   {Efficient estimations from a slowly convergent Robbins-Monro process},
    year    =   {1988}
}

@inproceedings{CL:NCELoss,
    author    = {Zhirong Wu and
               Yuanjun Xiong and
               Stella X. Yu and
               Dahua Lin},
    title     = {Unsupervised Feature Learning via Non-Parametric Instance Discrimination},
    booktitle = {CVPR},
    pages     = {3733--3742},
    year      = {2018}
}

@inproceedings{CL:DeepCluster,
    author    = {Mathilde Caron and
               Piotr Bojanowski and
               Armand Joulin and
               Matthijs Douze},
    title     = {Deep Clustering for Unsupervised Learning of Visual Features},
    booktitle = {ECCV},
    volume    = {11218},
    pages     = {139--156},
    year      = {2018},
}

@inproceedings{CL:SimCLR,
  author    = {Ting Chen and
               Simon Kornblith and
               Mohammad Norouzi and
               Geoffrey E. Hinton},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  booktitle = {ICML},
  volume    = {119},
  pages     = {1597--1607},
  year      = {2020}
}

@inproceedings{CL:swAV,
    author    = {Mathilde Caron and
               Ishan Misra and
               Julien Mairal and
               Priya Goyal and
               Piotr Bojanowski and
               Armand Joulin},
    title     = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
    booktitle = {NeurIPS},
    year      = {2020}
}

@inproceedings{CL:BYOL,
    author    = {Jean{-}Bastien Grill and
               Florian Strub and
               Florent Altch{\'{e}} and
               Corentin Tallec and
               Pierre H. Richemond and
               Elena Buchatskaya and
               Carl Doersch and
               Bernardo {\'{A}}vila Pires and
               Zhaohan Guo and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Koray Kavukcuoglu and
               R{\'{e}}mi Munos and
               Michal Valko},
    title     = {Bootstrap Your Own Latent - {A} New Approach to Self-Supervised Learning},
    booktitle = {NeurIPS},
    year      = {2020}
}

@article{CL:BYOLv2,
  author    = {Pierre H. Richemond and
               Jean{-}Bastien Grill and
               Florent Altch{\'{e}} and
               Corentin Tallec and
               Florian Strub and
               Andrew Brock and
               Samuel L. Smith and
               Soham De and
               Razvan Pascanu and
               Bilal Piot and
               Michal Valko},
  title     = {{BYOL} works even without batch statistics},
  journal   = {arXiv: 2010.10241},
  year      = {2020}
}

@inproceedings{CL:MoCo,
    author    = {Kaiming He and
               Haoqi Fan and
               Yuxin Wu and
               Saining Xie and
               Ross B. Girshick},
    title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
    booktitle = {CVPR},
    pages     = {9726--9735},
    year      = {2020}
}

@inproceedings{CL:DINO,
  author    = {Mathilde Caron and
               Hugo Touvron and
               Ishan Misra and
               Herv{\'{e}} J{\'{e}}gou and
               Julien Mairal and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  booktitle = {ICCV},
  pages     = {9630--9640},
  year      = {2021}
}

@inproceedings{CL:SimSiam,
    author    = {Xinlei Chen and
               Kaiming He},
    title     = {Exploring Simple Siamese Representation Learning},
    booktitle = {CVPR},
    pages     = {15750--15758},
    year      = {2021}
}

@inproceedings{CL:BarlowTwins,
    author    = {Jure Zbontar and
               Li Jing and
               Ishan Misra and
               Yann LeCun and
               St{\'{e}}phane Deny},
    title     = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
    booktitle = {ICML},
    volume    = {139},
    pages     = {12310--12320},
    year      = {2021}
}

%-------------------------------------------------%
%--------------- Metric Learning -----------------%
%-------------------------------------------------%

@inproceedings{ML:Contrast,
  author    = {Raia Hadsell and
               Sumit Chopra and
               Yann LeCun},
  title     = {Dimensionality Reduction by Learning an Invariant Mapping},
  booktitle = {CVPR},
  pages     = {1735--1742},
  year      = {2006}
}

@inproceedings{ML:Triplet,
    author    = {Florian Schroff and
               Dmitry Kalenichenko and
               James Philbin},
    title     = {FaceNet: {A} unified embedding for face recognition and clustering},
    booktitle = {CVPR},
    pages     = {815--823},
    year      = {2015}
}

@inproceedings{ML:Lifted,
    author    = {Hyun Oh Song and
               Yu Xiang and
               Stefanie Jegelka and
               Silvio Savarese},
    title     = {Deep Metric Learning via Lifted Structured Feature Embedding},
    booktitle = {CVPR},
    pages     = {4004--4012},
    year      = {2016}
}

@inproceedings{ML:N-pair,
    author    = {Kihyuk Sohn},
    title     = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
    booktitle = {NeurIPS},
    pages     = {1849--1857},
    year      = {2016}
}

@inproceedings{ML:Histogram,
  author    = {Evgeniya Ustinova and
               Victor S. Lempitsky},
  title     = {Learning Deep Embeddings with Histogram Loss},
  booktitle = {NeurIPS},
  pages     = {4170--4178},
  year      = {2016}
}

@article{ML:HardTriplet,
    author    = {Alexander Hermans and
               Lucas Beyer and
               Bastian Leibe},
    title     = {In Defense of the Triplet Loss for Person Re-Identification},
    journal   = {arXiv: 1703.07737},
    year      = {2017}
}

@inproceedings{ML:Quadruplet,
    author    = {Weihua Chen and
               Xiaotang Chen and
               Jianguo Zhang and
               Kaiqi Huang},
    title     = {Beyond Triplet Loss: {A} Deep Quadruplet Network for Person Re-identification},
    booktitle = {CVPR},
    pages     = {1320--1329},
    year      = {2017}
}

@inproceedings{ML:angular,
    author    = {Jian Wang and
               Feng Zhou and
               Shilei Wen and
               Xiao Liu and
               Yuanqing Lin},
    title     = {Deep Metric Learning with Angular Loss},
    booktitle = {ICCV},
    pages     = {2612--2620},
    year      = {2017}
}

@inproceedings{ML:Proxy-NCA,
    author    = {Yair Movshovitz{-}Attias and
               Alexander Toshev and
               Thomas K. Leung and
               Sergey Ioffe and
               Saurabh Singh},
    title     = {No Fuss Distance Metric Learning Using Proxies},
    booktitle = {ICCV},
    pages     = {360--368},
    year      = {2017}
}

@inproceedings{ML:SoftTriple,
    author    = {Qi Qian and
               Lei Shang and
               Baigui Sun and
               Juhua Hu and
               Tacoma Tacoma and
               Hao Li and
               Rong Jin},
    title     = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
    booktitle = {ICCV},
    pages     = {6449--6457},
    year      = {2019}
}

@inproceedings{ML:ATL,
    author    = {Xiaonan Zhao and
               Huan Qi and
               Rui Luo and
               Larry Davis},
    title     = {A Weakly Supervised Adaptive Triplet Loss for Deep Metric Learning},
    booktitle = {ICCV},
    pages     = {3177--3180},
    year      = {2019}
}

@article{ML:AMTL,
    author    = {Mai Lan Ha and
               Volker Blanz},
    title     = {Deep Ranking with Adaptive Margin Triplet Loss},
    journal   = {arXiv: 2107.06187},
    year      = {2021}
}

%------------------------------------------------------%
%------------ Visual Question Answering ---------------%
%------------------------------------------------------%

@inproceedings{VQA:VQA-A,
	author    = {Xiao Lin and
	Devi Parikh},
	title     = {Leveraging Visual Question Answering for Image-Caption Ranking},
	booktitle = {ECCV},
	volume    = {9906},
	pages     = {261--277},
	year      = {2016}
}
@inproceedings{VQA:DAN,
	author    = {Hyeonseob Nam and
	Jung{-}Woo Ha and
	Jeonghee Kim},
	title     = {Dual Attention Networks for Multimodal Reasoning and Matching},
	booktitle = {CVPR},
	pages     = {2156--2164},
	year      = {2017}
}

@inproceedings{VQA:GSR,
	author    = {Damien Teney and
	Lingqiao Liu and
	Anton van den Hengel},
	title     = {Graph-Structured Representations for Visual Question Answering},
	booktitle = {CVPR},
	pages     = {3233--3241},
	year      = {2017}
}

@inproceedings{VQA:MCAN,
    author    = {Zhou Yu and
               Jun Yu and
               Yuhao Cui and
               Dacheng Tao and
               Qi Tian},
    title     = {Deep Modular Co-Attention Networks for Visual Question Answering},
    booktitle = {CVPR},
    pages     = {6281--6290},
    year      = {2019}
}

%-----------------------------------------------------%
%-------------  Text-to-Image Generation -------------%
%-----------------------------------------------------%

@inproceedings{T2IG:attngan18,
	author    = {Tao Xu and
	Pengchuan Zhang and
	Qiuyuan Huang and
	Han Zhang and
	Zhe Gan and
	Xiaolei Huang and
	Xiaodong He},
	title     = {AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative
	Adversarial Networks},
	booktitle = {CVPR},
	pages     = {1316--1324},
	year      = {2018}}

%--------------------------------------------------------%
%---------  Referring Expression Comprehension ----------%
%--------------------------------------------------------%

@inproceedings{REC:LGGAN,
	author    = {Peng Wang and
	Qi Wu and
	Jiewei Cao and
	Chunhua Shen and
	Lianli Gao and
	Anton van den Hengel},
	title     = {Neighbourhood Watch: Referring Expression Comprehension via Language-Guided
	Graph Attention Networks},
	booktitle = {CVPR},
	pages     = {1960--1968},
	year      = {2019}
}

@inproceedings{REC:CKR,
    author    = {Chen Gao and
               Jinyu Chen and
               Si Liu and
               Luting Wang and
               Qiong Zhang and
               Qi Wu},
    title     = {Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring
               Expression},
    booktitle = {CVPR},
    pages     = {3064--3073},
    year      = {2021},
}

%-----------------------------------------------------%
%------------ Visual Commonsense Reasoning -----------%
%-----------------------------------------------------%

@inproceedings{VCR:VCR,
    author    = {Rowan Zellers and 
    Yonatan Bisk and 
    Ali Farhadi and 
    Yejin Choi},
    title     = {From Recognition to Cognition: Visual Commonsense Reasoning},
    booktitle = {CVPR},
    pages     = {6720--6731},
    year      = {2019}
}

%---------------------------------------------------%
%----------------- Image Captioning ----------------%
%---------------------------------------------------%

@inproceedings{IC:BU_TDA,
	author    = {Peter Anderson and
	Xiaodong He and
	Chris Buehler and
	Damien Teney and
	Mark Johnson and
	Stephen Gould and
	Lei Zhang},
	title     = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
	Answering},
	booktitle = {CVPR},
	pages     = {6077--6086},
	year      = {2018}
}

@inproceedings{IC:AESG,
	author    = {Xu Yang and
	Kaihua Tang and
	Hanwang Zhang and
	Jianfei Cai},
	title     = {Auto-Encoding Scene Graphs for Image Captioning},
	booktitle = {CVPR},
	pages     = {10685--10694},
	year      = {2019}
}

@inproceedings{IC:AoANet,
    author    = {Lun Huang and
               Wenmin Wang and
               Jie Chen and
               Xiaoyong Wei},
    title     = {Attention on Attention for Image Captioning},
    booktitle = {ICCV},
    pages     = {4633--4642},
    year      = {2019}
}

%-----------------------------------------------------%
%--------------- Text Person Search ------------------%
%-----------------------------------------------------%

@inproceedings{TPS:GNA-RNN,
    author    = {Shuang Li and
               Tong Xiao and
               Hongsheng Li and
               Bolei Zhou and
               Dayu Yue and
               Xiaogang Wang},
    title     = {Person Search with Natural Language Description},
    booktitle = {CVPR},
    pages     = {5187--5196},
    year      = {2017}
}

%-----------------------------------------------------%
%------------ Localizing Video Moments ---------------%
%-----------------------------------------------------%

@inproceedings{LVM:MCN,
    author    = {Lisa Anne Hendricks and
               Oliver Wang and
               Eli Shechtman and
               Josef Sivic and
               Trevor Darrell and
               Bryan C. Russell},
    title     = {Localizing Moments in Video with Natural Language},
    booktitle = {ICCV},
    pages     = {5804--5813},
    year      = {2017}
}

%----------------------------------------------------%
%-------------- Image-Text Matching -----------------%
%----------------------------------------------------%

@inproceedings{ITM:DeViSE,
	author    = {Andrea Frome and
	Gregory S. Corrado and
	Jonathon Shlens and
	Samy Bengio and
	Jeffrey Dean and
	Marc'Aurelio Ranzato and
	Tomas Mikolov},
	title     = {DeViSE: {A} Deep Visual-Semantic Embedding Model},
	booktitle = {NIPS},
	pages     = {2121--2129},
	year      = {2013}
}

@inproceedings{ITM:DeFrag,
    author    = {Andrej Karpathy and
               Armand Joulin and
               Li Fei{-}Fei},
    title     = {Deep Fragment Embeddings for Bidirectional Image Sentence Mapping},
    booktitle = {NIPS},
    pages     = {1889--1897},
    year      = {2014}
}

@inproceedings{ITM:GMM-FV,
	author    = {Benjamin Klein and
	Guy Lev and
	Gil Sadeh and
	Lior Wolf},
	title     = {Associating neural word embeddings with deep image representations
	using Fisher Vectors},
	booktitle = {CVPR},
	pages     = {4437--4446},
	year      = {2015}
}

@inproceedings{ITM:DVSA,
	author    = {Andrej Karpathy and
	Fei{-}Fei Li},
	title     = {Deep visual-semantic alignments for generating image descriptions},
	booktitle = {CVPR},
	pages     = {3128--3137},
	year      = {2015}
}

@inproceedings{ITM:M-RNN,
	author    = {Junhua Mao and
	Wei Xu and
	Yi Yang and
	Jiang Wang and
	Alan L. Yuille},
	title     = {Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},
	booktitle = {ICLR},
	year      = {2015}
}

@inproceedings{ITM:M-CNN,
	author    = {Lin Ma and
	Zhengdong Lu and
	Lifeng Shang and
	Hang Li},
	title     = {Multimodal Convolutional Neural Networks for Matching Image and Sentence},
	booktitle = {ICCV},
	pages     = {2623--2631},
	year      = {2015}
}

@inproceedings{ITM:Order,
	author    = {Ivan Vendrov and
	Ryan Kiros and
	Sanja Fidler and
	Raquel Urtasun},
	title     = {Order-Embeddings of Images and Language},
	booktitle = {ICLR},
	year      = {2016}
}

@inproceedings{ITM:DSPE,
	author    = {Liwei Wang and
	Yin Li and
	Svetlana Lazebnik},
	title     = {Learning Deep Structure-Preserving Image-Text Embeddings},
	booktitle = {CVPR},
	pages     = {5005--5013},
	year      = {2016}
}

@inproceedings{ITM:RRF,
	author    = {Yu Liu and
	Yanming Guo and
	Erwin M. Bakker and
	Michael S. Lew},
	title     = {Learning a Recurrent Residual Fusion Network for Multimodal Matching},
	booktitle = {ICCV},
	pages     = {4127--4136},
	year      = {2017}
}

@inproceedings{ITM:NMT,
	author    = {Joji Toyama and
	Masanori Misono and
	Masahiro Suzuki and
	Kotaro Nakayama and
	Yutaka Matsuo},
	title     = {Neural Machine Translation with Latent Semantic of Image and Text},
	booktitle = {ICLR},
	year      = {2017}
}

@inproceedings{ITM:HMLSTM,
	author    = {Zhenxing Niu and
	Mo Zhou and
	Le Wang and
	Xinbo Gao and
	Gang Hua},
	title     = {Hierarchical Multimodal {LSTM} for Dense Visual-Semantic Embedding},
	booktitle = {ICCV},
	pages     = {1899--1907},
	year      = {2017}
}

@inproceedings{ITM:sm-LSTM,
	author    = {Yan Huang and
	Wei Wang and
	Liang Wang},
	title     = {Instance-Aware Image and Sentence Matching with Selective Multimodal {LSTM}},
	booktitle = {CVPR},
	pages     = {7254--7262},
	year      = {2017}
}

@inproceedings{ITM:CMPL,
	author    = {Ying Zhang and
	Huchuan Lu},
	title     = {Deep Cross-Modal Projection Learning for Image-Text Matching},
	booktitle = {ECCV},
	volume    = {11205},
	pages     = {707--723},
	year      = {2018}
}

@inproceedings{ITM:SCAN,
	author    = {Kuang{-}Huei Lee and
	Xi Chen and
	Gang Hua and
	Houdong Hu and
	Xiaodong He},
	title     = {Stacked Cross Attention for Image-Text Matching},
	booktitle = {ECCV},
	volume    = {11208},
	pages     = {212--228},
	year      = {2018}
}

@inproceedings{ITM:VSE++,
    author    = {Fartash Faghri and
    David J. Fleet and
    Jamie Ryan Kiros and
    Sanja Fidler},
    title     = {{VSE++:} Improving Visual-Semantic Embeddings with Hard Negatives},
    booktitle = {BMVC},
    pages     = {12},
    year      = {2018}
}

@inproceedings{ITM:SCO,
	author    = {Yan Huang and
	Qi Wu and
	Chunfeng Song and
	Liang Wang},
	title     = {Learning Semantic Concepts and Order for Image and Sentence Matching},
	booktitle = {CVPR},
	pages     = {6163--6171},
	year      = {2018},
}

@inproceedings{ITM:GXN,
	author    = {Jiuxiang Gu and
	Jianfei Cai and
	Shafiq R. Joty and
	Li Niu and
	Gang Wang},
	title     = {Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval
	With Generative Models},
	booktitle = {CVPR},
	pages     = {7181--7189},
	year      = {2018}
}

@inproceedings{ITM:MTFN,
  author    = {Tan Wang and
               Xing Xu and
               Yang Yang and
               Alan Hanjalic and
               Heng Tao Shen and
               Jingkuan Song},
  title     = {Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking},
  booktitle = {ACMMM},
  pages     = {12--20},
  year      = {2019}
}

@inproceedings{ITM:UVSE,
	author    = {Hao Wu and
	Jiayuan Mao and
	Yufeng Zhang and
	Yuning Jiang and
	Lei Li and
	Weiwei Sun and
	Wei{-}Ying Ma},
	title     = {Unified Visual-Semantic Embeddings: Bridging Vision and Language With
	Structured Meaning Representations},
	booktitle = {CVPR},
	pages     = {6609--6618},
	year      = {2019}}

@inproceedings{ITM:GRN,
	author    = {Zhanghui Kuang and
	Yiming Gao and
	Guanbin Li and
	Ping Luo and
	Yimin Chen and
	Liang Lin and
	Wayne Zhang},
	title     = {Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid},
	booktitle = {ICCV},
	year      = {2019}}

@inproceedings{ITM:PFAN,
	author    = {Yaxiong Wang and
	Hao Yang and
	Xueming Qian and
	Lin Ma and
	Jing Lu and
	Biao Li and
	Xin Fan},
	title     = {Position Focused Attention Network for Image-Text Matching},
	booktitle = {IJCAI},
	pages     = {3792--3798},
	year      = {2019},
}

@inproceedings{ITM:RDAN,
	author    = {Zhibin Hu and
	Yongsheng Luo and
	Jiong Lin and
	Yan Yan and
	Jian Chen},
	title     = {Multi-Level Visual-Semantic Alignments with Relation-Wise Dual Attention
	Network for Image and Text Matching},
	booktitle = {IJCAI},
	pages     = {789--795},
	year      = {2019},
}

@inproceedings{ITM:BFAN,
	author    = {Chunxiao Liu and
	Zhendong Mao and
	An{-}An Liu and
	Tianzhu Zhang and
	Bin Wang and
	Yongdong Zhang},
	title     = {Focus Your Attention: {A} Bidirectional Focal Attention Network for
	Image-Text Matching},
	booktitle = {ACMMM},
	pages     = {3--11},
	year      = {2019},
}

@inproceedings{ITM:PVSE,
	author    = {Yale Song and
	Mohammad Soleymani},
	title     = {Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval},
	booktitle = {CVPR},
	pages     = {1979--1988},
	year      = {2019}
}

@inproceedings{ITM:SAEM,
    author    = {Yiling Wu and
    Shuhui Wang and
    Guoli Song and
    Qingming Huang},
    title     = {Learning Fragment Self-Attention Embeddings for Image-Text Matching},
    booktitle = {ACMMM},
    pages     = {2088--2096},
    year      = {2019}
}

@inproceedings{ITM:VSRN,
	author    = {Kunpeng Li and
	Yulun Zhang and
	Kai Li and
	Yuanyuan Li and
	Yun Fu},
	title     = {Visual Semantic Reasoning for Image-Text Matching},
	booktitle = {ICCV},
	pages     = {4653--4661},
	year      = {2019},
}

@inproceedings{ITM:ACMM,
	author    = {Yan Huang and
	Liang Wang},
	title     = {{ACMM:} Aligned Cross-Modal Memory for Few-Shot Image and Sentence
	Matching},
	booktitle = {ICCV},
	pages     = {5773--5782},
	year      = {2019},
}

@article{ITM:R-SCAN,
    author    = {Kuang{-}Huei Lee and
    Hamid Palangi and
    Xi Chen and
    Houdong Hu and
    Jianfeng Gao},
    title     = {Learning Visual Relation Priors for Image-Text Matching and Image Captioning with Neural Scene Graph Generators},
    journal   = {arXiv: 1909.09953},
    year      = {2019}
}

@inproceedings{ITM:CAMP,
	author    = {Zihao Wang and
	Xihui Liu and
	Hongsheng Li and
	Lu Sheng and
	Junjie Yan and
	Xiaogang Wang and
	Jing Shao},
	title     = {{CAMP:} Cross-Modal Adaptive Message Passing for Text-Image Retrieval},
	booktitle = {ICCV},
	pages     = {5763--5772},
	year      = {2019}
}

@inproceedings{ITM:SAN,
	author    = {Zhong Ji and
	Haoran Wang and
	Jungong Han and
	Yanwei Pang},
	title     = {Saliency-Guided Attention Network for Image-Sentence Matching},
	booktitle = {ICCV},
	pages     = {5753--5762},
	year      = {2019}
}

@inproceedings{ITM:SCG,
	author    = {Botian Shi and
	Lei Ji and
	Pan Lu and
	Zhendong Niu and
	Nan Duan},
	title     = {Knowledge Aware Semantic Concept Expansion for Image-Text Matching},
	booktitle = {IJCAI},
	pages     = {5182--5189},
	year      = {2019}
}

@inproceedings{ITM:CVSE,
    author    = {Haoran Wang and
    Ying Zhang and
    Zhong Ji and
    Yanwei Pang and
    Lin Ma},
    title     = {Consensus-Aware Visual-Semantic Embedding for Image-Text Matching},
    booktitle = {ECCV},
    volume    = {12369},
    pages     = {18--34},
    year      = {2020},
}

@inproceedings{ITM:SGM,
	author    = {Sijin Wang and
	Ruiping Wang and
	Ziwei Yao and
	Shiguang Shan and
	Xilin Chen},
	title     = {Cross-modal Scene Graph Matching for Relationship-aware Image-Text
	Retrieval},
	booktitle = {WACV},
	pages     = {1497--1506},
	year      = {2020},
}

@article{ITM:Dualpath,
    author    = {Zhedong Zheng and
    Liang Zheng and
    Michael Garrett and
    Yi Yang and
    Mingliang Xu and
    Yi{-}Dong Shen},
    title     = {Dual-path Convolutional Image-Text Embeddings with Instance Loss},
    journal   = {{ACM} Trans. Multim. Comput. Commun. Appl.},
    volume    = {16},
    number    = {2},
    pages     = {51:1--51:23},
    year      = {2020},
}

@inproceedings{ITM:Ladder,
    author    = {Mo Zhou and
               Zhenxing Niu and
               Le Wang and
               Zhanning Gao and
               Qilin Zhang and
               Gang Hua},
    title     = {Ladder Loss for Coherent Visual-Semantic Embedding},
    booktitle = {AAAI},
    pages     = {13050--13057},
    year      = {2020}
}

@inproceedings{ITM:ACME,
	author    = {Jonatas Wehrmann and
	Camila Kolling and
	Rodrigo C. Barros},
	title     = {Adaptive Cross-Modal Embeddings for Image-Text Alignment},
	booktitle = {AAAI},
	pages     = {12313--12320},
	year      = {2020},
}

@inproceedings{ITM:MPL,
    author    = {Jiwei Wei and
    Xing Xu and
    Yang Yang and
    Yanli Ji and
    Zheng Wang and
    Heng Tao Shen},
    title     = {Universal Weighting Metric Learning for Cross-Modal Matching},
    booktitle = {CVPR},
    pages     = {13002--13011},
    year      = {2020}
}

@inproceedings{ITM:AOQ,
    author    = {Tianlang Chen and
    Jiajun Deng and
    Jiebo Luo},
    title     = {Adaptive Offline Quintuplet Loss for Image-Text Matching},
    booktitle = {ECCV},
    volume    = {12358},
    pages     = {549--565},
    year      = {2020}
}

@inproceedings{ITM:DP-RNN,
	author    = {Tianlang Chen and
	Jiebo Luo},
	title     = {Expressing Objects Just Like Words: Recurrent Visual Embedding for
	Image-Text Matching},
	booktitle = {AAAI},
	pages     = {10583--10590},
	year      = {2020},
}

@inproceedings{ITM:GSMN,
    author    = {Chunxiao Liu and
    Zhendong Mao and
    Tianzhu Zhang and
    Hongtao Xie and
    Bin Wang and
    Yongdong Zhang},
    title     = {Graph Structured Network for Image-Text Matching},
    booktitle = {CVPR},
    pages     = {10918--10927},
    year      = {2020}
}

@inproceedings{ITM:IMRAM,
    author    = {Hui Chen and
    Guiguang Ding and
    Xudong Liu and
    Zijia Lin and
    Ji Liu and
    Jungong Han},
    title     = {{IMRAM:} Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval},
    booktitle = {CVPR},
    pages     = {12652--12660},
    year      = {2020}
}

@inproceedings{ITM:CAAN,
    author    = {Qi Zhang and
    Zhen Lei and
    Zhaoxiang Zhang and
    Stan Z. Li},
    title     = {Context-Aware Attention Network for Image-Text Retrieval},
    booktitle = {CVPR},
    pages     = {3533--3542},
    year      = {2020}
}

@inproceedings{ITM:MMCA,
    author    = {Xi Wei and
    Tianzhu Zhang and
    Yan Li and
    Yongdong Zhang and
    Feng Wu},
    title     = {Multi-Modality Cross Attention Network for Image and Sentence Matching},
    booktitle = {CVPR},
    pages     = {10938--10947},
    year      = {2020}
}

@inproceedings{ITM:HOAD,
    author    = {Yongzhi Li and
               Duo Zhang and
               Yadong Mu},
    title     = {Visual-Semantic Matching by Exploring High-Order Attention and Distraction},
    booktitle = {CVPR},
    pages     = {12783--12792},
    year      = {2020}
}

@inproceedings{ITM:HAL,
    author    = {Fangyu Liu and
               Rongtian Ye and
               Xun Wang and
               Shuaipeng Li},
    title     = {{HAL:} Improved Text-Image Matching by Mitigating Visual Semantic Hubs},
    booktitle = {AAAI},
    pages     = {11563--11571},
    year      = {2020}
}

@article{ITM:TERAN,
  author    = {Nicola Messina and
               Giuseppe Amato and
               Andrea Esuli and
               Fabrizio Falchi and
               Claudio Gennaro and
               St{\'{e}}phane Marchand{-}Maillet},
  title     = {Fine-Grained Visual Textual Alignment for Cross-Modal Retrieval Using
               Transformer Encoders},
  journal   = {{ACM} Trans. Multim. Comput. Commun. Appl.},
  volume    = {17},
  number    = {4},
  pages     = {128:1--128:23},
  year      = {2021}
}

@inproceedings{ITM:SHAN,
  author    = {Zhong Ji and
               Kexin Chen and
               Haoran Wang},
  title     = {Step-Wise Hierarchical Alignment Network for Image-Text Matching},
  booktitle = {IJCAI},
  pages     = {765--771},
  year      = {2021}
}

@inproceedings{ITM:GPO,
    author    = {Jiacheng Chen and
               Hexiang Hu and
               Hao Wu and
               Yuning Jiang and
               Changhu Wang},
    title     = {Learning the Best Pooling Strategy for Visual Semantic Embedding},
    booktitle = {CVPR},
    pages     = {15789--15798},
    year      = {2021}
}

@inproceedings{ITM:SGRAF,
    author    = {Haiwen Diao and
               Ying Zhang and
               Lin Ma and
               Huchuan Lu},
    title     = {Similarity Reasoning and Filtration for Image-Text Matching},
    booktitle = {AAAI},
    pages     = {1218--1226},
    year      = {2021}
}

@inproceedings{ITM:WCGL,
    author    = {Wang, Yun and Zhang, Tong and Zhang, Xueya and Cui, Zhen and Huang, Yuge and Shen, Pengcheng and Li, Shaoxin and Yang, Jian},
    title     = {Wasserstein Coupled Graph Learning for Cross-Modal Retrieval},
    booktitle = {ICCV},
    pages     = {1813-1822},
    year      = {2021}
}

@inproceedings{ITM:NCR,
  author    = {Zhenyu Huang and
               Guocheng Niu and
               Xiao Liu and
               Wenbiao Ding and
               Xinyan Xiao and
               Hua Wu and
               Xi Peng},
  title     = {Learning with Noisy Correspondence for Cross-modal Matching},
  booktitle = {NeurIPS},
  pages     = {29406--29419},
  year      = {2021}
}

@inproceedings{ITM:DIME,
  author    = {Leigang Qu and
               Meng Liu and
               Jianlong Wu and
               Zan Gao and
               Liqiang Nie},
  title     = {Dynamic Modality Interaction Modeling for Image-Text Retrieval},
  booktitle = {SIGIR},
  pages     = {1104--1113},
  year      = {2021}
}

@inproceedings{ITM:SAM,
  author    = {Ali Furkan Biten and
               Andr{\'{e}}s Mafla and
               Llu{\'{\i}}s G{\'{o}}mez and
               Dimosthenis Karatzas},
  title     = {Is An Image Worth Five Sentences? {A} New Look into Semantics for Image-Text Matching},
  booktitle = {WACV},
  pages     = {2483--2492},
  year      = {2022}
}

@inproceedings{ITM:CMCAN,
    author    = {Huatian Zhang and
               Zhendong Mao and
               Kun Zhang and
               Yongdong Zhang},
    title     = {Show Your Faith: Cross-Modal Confidence-Aware Network for Image-Text
               Matching},
    booktitle = {AAAI},
    pages     = {3262--3270},
    year      = {2022},
}

@inproceedings{ITM:AME,
    author    = {Jiangtong Li and
               Li Niu and
               Liqing Zhang},
    title     = {Action-Aware Embedding Enhancement for Image-Text Retrieval},
    booktitle = {AAAI},
    pages     = {1323--1331},
    year      = {2022},
}

@inproceedings{ITM:NAAF,
    title={Negative-Aware Attention Framework for Image-Text Matching},
    author={Zhang, Kun and Mao, Zhendong and Wang, Quan and Zhang, Yongdong},
    booktitle={CVPR},
    pages={15661--15670},
    year={2022}
}

@inproceedings{ITM:CODER,
    author    = {Haoran Wang and
               Dongliang He and
               Wenhao Wu and
               Boyang Xia and
               Min Yang and
               Fu Li and
               Yunlong Yu and
               Zhong Ji and
               Errui Ding and
               Jingdong Wang},
    title     = {{CODER:} Coupled Diversity-Sensitive Momentum Contrastive Learning
               for Image-Text Retrieval},
    booktitle={ECCV},
    year={2022}
}

@article{ITM:RCAR,
  author       = {Haiwen Diao and
                  Ying Zhang and
                  Wei Liu and
                  Xiang Ruan and
                  Huchuan Lu},
  title        = {Plug-and-Play Regulators for Image-Text Matching},
  journal      = {TIP},
  volume       = {32},
  pages        = {2322--2334},
  year         = {2023}
}

@article{ITM:DBL,
  title={Deep Boosting Learning: A Brand-new Cooperative Approach for Image-Text Matching},
  author={Diao, Haiwen and Zhang, Ying and Gao, Shang and Ruan, Xiang and Lu, Huchuan},
  journal={TIP},
  year={2024}
}

@article{ITM:GSSF,
  title={GSSF: Generalized Structural Sparse Function for Deep Cross-modal Metric Learning},
  author={Diao, Haiwen and Zhang, Ying and Gao, Shang and Zhu, Jiawen and Chen, Long and Lu, Huchuan},
  journal={arXiv preprint arXiv:2410.15266},
  year={2024}
}

%------------------------------------------------------%
%----------- Vision-Language Pretraining  -------------%
%------------------------------------------------------%

@inproceedings{VLP:ViLBERT,
    author    = {Jiasen Lu and
    Dhruv Batra and
    Devi Parikh and
    Stefan Lee},
    title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
    booktitle = {NeurIPS},
    pages     = {13--23},
    year      = {2019}
}

@inproceedings{VLP:UNITER,
    author    = {Yen{-}Chun Chen and
    Linjie Li and
    Licheng Yu and
    Ahmed El Kholy and
    Faisal Ahmed and
    Zhe Gan and
    Yu Cheng and
    Jingjing Liu},
    title     = {{UNITER:} Learning UNiversal Image-TExt Representations},
    journal   = {ECCV},
    year      = {2020},
}

@inproceedings{VLP:Unicoder-VL,
    author    = {Gen Li and
    Nan Duan and
    Yuejian Fang and
    Ming Gong and
    Daxin Jiang},
    title     = {Unicoder-VL: {A} Universal Encoder for Vision and Language by Cross-Modal
    Pre-Training},
    booktitle = {AAAI},
    pages     = {11336--11344},
    year      = {2020}
}

@article{VLP:ImageBERT,
    author    = {Di Qi and
    Lin Su and
    Jia Song and
    Edward Cui and
    Taroon Bharti and
    Arun Sacheti},
    title     = {ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data},
    journal   = {arXiv: 2001.07966},
    year      = {2020}
}

@inproceedings{VLP:VILLA,
  author    = {Zhe Gan and
               Yen{-}Chun Chen and
               Linjie Li and
               Chen Zhu and
               Yu Cheng and
               Jingjing Liu},
  title     = {Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  booktitle = {NIPS},
  year      = {2020}
 }

@inproceedings{VLP:MMT,
  author       = {Valentin Gabeur and
                  Chen Sun and
                  Karteek Alahari and
                  Cordelia Schmid},
  title        = {Multi-modal Transformer for Video Retrieval},
  booktitle    = {ECCV},
  volume       = {12349},
  pages        = {214--229},
  year         = {2020}
}

@article{VLP:CLIP4Clip,
  author       = {Huaishao Luo and
                  Lei Ji and
                  Ming Zhong and
                  Yang Chen and
                  Wen Lei and
                  Nan Duan and
                  Tianrui Li},
  title        = {CLIP4Clip: An Empirical Study of {CLIP} for End to End Video Clip
                  Retrieval},
  journal      = {arXiv: 2104.08860},
  year         = {2021}
}

@inproceedings{VLP:ALIGN,
    author    = {Chao Jia and
               Yinfei Yang and
               Ye Xia and
               Yi{-}Ting Chen and
               Zarana Parekh and
               Hieu Pham and
               Quoc V. Le and
               Yun{-}Hsuan Sung and
               Zhen Li and
               Tom Duerig},
    title     = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
    booktitle = {ICML},
    volume    = {139},
    pages     = {4904--4916},
    year      = {2021}
}

@inproceedings{VLP:CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {ICML},
  volume    = {139},
  pages     = {8748--8763},
  year      = {2021}
}

@inproceedings{VLP:ALBEF,
  author       = {Junnan Li and
                  Ramprasaath R. Selvaraju and
                  Akhilesh Gotmare and
                  Shafiq R. Joty and
                  Caiming Xiong and
                  Steven Chu{-}Hong Hoi},
  title        = {Align before Fuse: Vision and Language Representation Learning with
                  Momentum Distillation},
  booktitle    = {NeurIPS},
  pages        = {9694--9705},
  year         = {2021}
}

@inproceedings{VLP:MDETR,
  author       = {Aishwarya Kamath and
                  Mannat Singh and
                  Yann LeCun and
                  Gabriel Synnaeve and
                  Ishan Misra and
                  Nicolas Carion},
  title        = {{MDETR} - Modulated Detection for End-to-End Multi-Modal Understanding},
  booktitle    = {ICCV},
  pages        = {1760--1770},
  year         = {2021}
}

@inproceedings{VLP:DeCLIP,
  author       = {Yangguang Li and
                  Feng Liang and
                  Lichen Zhao and
                  Yufeng Cui and
                  Wanli Ouyang and
                  Jing Shao and
                  Fengwei Yu and
                  Junjie Yan},
  title        = {Supervision Exists Everywhere: {A} Data Efficient Contrastive Language-Image
                  Pre-training Paradigm},
  booktitle    = {ICLR},
  year         = {2022}
}

@inproceedings{VLP:FILIP,
  author    = {Lewei Yao and
               Runhui Huang and
               Lu Hou and
               Guansong Lu and
               Minzhe Niu and
               Hang Xu and
               Xiaodan Liang and
               Zhenguo Li and
               Xin Jiang and
               Chunjing Xu},
  title     = {{FILIP:} Fine-grained Interactive Language-Image Pre-Training},
  booktitle = {ICLR},
  year      = {2022}
}

@inproceedings{VLP:BLIP,
  author       = {Junnan Li and
                  Dongxu Li and
                  Caiming Xiong and
                  Steven C. H. Hoi},
  title        = {{BLIP:} Bootstrapping Language-Image Pre-training for Unified Vision-Language
                  Understanding and Generation},
  booktitle    = {ICLR},
  volume       = {162},
  pages        = {12888--12900},
  year         = {2022}
}

@article{VLP:WTI,
  author    = {Qiang Wang and
               Yanhao Zhang and
               Yun Zheng and
               Pan Pan and
               Xian{-}Sheng Hua},
  title     = {Disentangled Representation Learning for Text-Video Retrieval},
  journal   = {arXiv: 2203.07111},
  year      = {2022}
}

@inproceedings{VLP:CLIP-ViL,
  author       = {Sheng Shen and
                  Liunian Harold Li and
                  Hao Tan and
                  Mohit Bansal and
                  Anna Rohrbach and
                  Kai{-}Wei Chang and
                  Zhewei Yao and
                  Kurt Keutzer},
  title        = {How Much Can {CLIP} Benefit Vision-and-Language Tasks?},
  booktitle    = {ICLR},
  year         = {2022}
}

@article{VLP:CoCa,
  author       = {Jiahui Yu and
                  Zirui Wang and
                  Vijay Vasudevan and
                  Legg Yeung and
                  Mojtaba Seyedhosseini and
                  Yonghui Wu},
  title        = {CoCa: Contrastive Captioners are Image-Text Foundation Models},
  journal      = {TMLR},
  year         = {2022}
}

@inproceedings{VLP:Flamingo,
  author       = {Jean{-}Baptiste Alayrac and
                  Jeff Donahue and
                  Pauline Luc and
                  Antoine Miech and
                  Iain Barr and
                  Yana Hasson and
                  Karel Lenc and
                  Arthur Mensch and
                  Katherine Millican and
                  Malcolm Reynolds and
                  Roman Ring and
                  Eliza Rutherford and
                  Serkan Cabi and
                  Tengda Han and
                  Zhitao Gong and
                  Sina Samangooei and
                  Marianne Monteiro and
                  Jacob L. Menick and
                  Sebastian Borgeaud and
                  Andy Brock and
                  Aida Nematzadeh and
                  Sahand Sharifzadeh and
                  Mikolaj Binkowski and
                  Ricardo Barreira and
                  Oriol Vinyals and
                  Andrew Zisserman and
                  Kar{\'{e}}n Simonyan},
  title        = {Flamingo: a Visual Language Model for Few-Shot Learning},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@inproceedings{VLP:Imagebind,
  author       = {Rohit Girdhar and
                  Alaaeldin El{-}Nouby and
                  Zhuang Liu and
                  Mannat Singh and
                  Kalyan Vasudev Alwala and
                  Armand Joulin and
                  Ishan Misra},
  title        = {ImageBind One Embedding Space to Bind Them All},
  booktitle    = {CVPR},
  pages        = {15180--15190},
  year         = {2023}
}

@article{VLP:meta-transformer,
  author       = {Yiyuan Zhang and
                  Kaixiong Gong and
                  Kaipeng Zhang and
                  Hongsheng Li and
                  Yu Qiao and
                  Wanli Ouyang and
                  Xiangyu Yue},
  title        = {Meta-Transformer: {A} Unified Framework for Multimodal Learning},
  journal      = {arXiv: 2307.10802},
  year         = {2023}
}

@inproceedings{VLP:BLIPv2,
  author       = {Junnan Li and
                  Dongxu Li and
                  Silvio Savarese and
                  Steven C. H. Hoi},
  title        = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image
                  Encoders and Large Language Models},
  booktitle    = {ICML},
  volume       = {202},
  pages        = {19730--19742},
  year         = {2023}
}

@article{VLP:BEiTv3,
  author       = {Wenhui Wang and
                  Hangbo Bao and
                  Li Dong and
                  Johan Bjorck and
                  Zhiliang Peng and
                  Qiang Liu and
                  Kriti Aggarwal and
                  Owais Khan Mohammed and
                  Saksham Singhal and
                  Subhojit Som and
                  Furu Wei},
  title        = {Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language
                  Tasks},
  journal      = {arXiv: 2208.10442},
  year         = {2022}
}

%-----------------------------------------=---------%
%------------------ Math Analysis  -----------------%
%---------------------------------------------------%

@book{MA:NLA,
  title={Numerical linear algebra},
  author={Trefethen, Lloyd N and Bau III, David},
  volume={50},
  year={1997},
}

@article{MA:MAT,
  title={A Tutorial on Matrix Perturbation Theory (using compact matrix notation)},
  author={Bamieh, Bassam},
  journal={arXiv:2002.05001},
  year={2020}
}

%-----------------------------------------------%
%----------------- CNN Pruning  ----------------%
%-----------------------------------------------%

@inproceedings{CP:OrthoReg,
  author    = {Pau Rodr{\'{\i}}guez and
               Jordi Gonz{\`{a}}lez and
               Guillem Cucurull and
               Josep M. Gonfaus and
               F. Xavier Roca},
  title     = {Regularizing CNNs with Locally Constrained Decorrelations},
  booktitle = {ICLR},
  year      = {2017}
}

@inproceedings{CP:SRR-GR,
  author    = {Zi Wang and
               Chengcheng Li and
               Xiangyang Wang},
  title     = {Convolutional Neural Network Pruning With Structural Redundancy Reduction},
  booktitle = {CVPR},
  pages     = {14913--14922},
  year      = {2021}
}

@inproceedings{CP:CW-NNK,
  author    = {David Bonet and
               Antonio Ortega and
               Javier Ruiz Hidalgo and
               Sarath Shekkizhar},
  title     = {Channel Redundancy and Overlap in Convolutional Neural Networks with
               Channel-Wise {NNK} Graphs},
  booktitle = {ICASSP},
  pages     = {4328--4332},
  year      = {2022}
}

%---------------------------------------------------%
%--------------- Transfer Learning  ----------------%
%---------------------------------------------------%

@inproceedings{TL:ULMFiT,
  author       = {Jeremy Howard and
                  Sebastian Ruder},
  title        = {Universal Language Model Fine-tuning for Text Classification},
  booktitle    = {ACL},
  pages        = {328--339},
  year         = {2018}
}

@inproceedings{TL:Adapter-BERT,
  author       = {Neil Houlsby and
                  Andrei Giurgiu and
                  Stanislaw Jastrzebski and
                  Bruna Morrone and
                  Quentin de Laroussilhe and
                  Andrea Gesmundo and
                  Mona Attariyan and
                  Sylvain Gelly},
  title        = {Parameter-Efficient Transfer Learning for {NLP}},
  booktitle    = {ICML},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {2790--2799},
  year         = {2019}
}

@inproceedings{TL:Adapter-NMT,
  author       = {Ankur Bapna and
                  Orhan Firat},
  title        = {Simple, Scalable Adaptation for Neural Machine Translation},
  booktitle    = {EMNLP},
  pages        = {1538--1548},
  year         = {2019}
}

@inproceedings{TL:AdapterHub,
    title={AdapterHub: A Framework for Adapting Transformers},
    author={Pfeiffer, Jonas and
            R{\"u}ckl{\'e}, Andreas and
            Poth, Clifton and
            Kamath, Aishwarya and
            Vuli{\'c}, Ivan and
            Ruder, Sebastian and
            Cho, Kyunghyun and
            Gurevych, Iryna},
    booktitle={EMNLP},
    pages={46--54},
    year={2020}
}

@inproceedings{TL:Side-Tuning,
  author       = {Jeffrey O. Zhang and
                  Alexander Sax and
                  Amir Zamir and
                  Leonidas J. Guibas and
                  Jitendra Malik},
  title        = {Side-Tuning: {A} Baseline for Network Adaptation via Additive Side Networks},
  booktitle    = {ECCV},
  volume       = {12348},
  pages        = {698--714},
  year         = {2020}
}

@inproceedings{TL:Tiny-TL,
  author       = {Han Cai and
                  Chuang Gan and
                  Ligeng Zhu and
                  Song Han},
  title        = {TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@inproceedings{TL:MAD-X,
  author       = {Jonas Pfeiffer and
                  Ivan Vulic and
                  Iryna Gurevych and
                  Sebastian Ruder},
  title        = {{MAD-X:} An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {7654--7673},
  year         = {2020}
}

@inproceedings{TL:AdapterFusion,
  author       = {Jonas Pfeiffer and
                  Aishwarya Kamath and
                  Andreas R{\"{u}}ckl{\'{e}} and
                  Kyunghyun Cho and
                  Iryna Gurevych},
  title        = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  booktitle    = {EACL},
  pages        = {487--503},
  year         = {2021}
}

@inproceedings{TL:DiffPruning,
  author       = {Demi Guo and
                  Alexander M. Rush and
                  Yoon Kim},
  title        = {Parameter-Efficient Transfer Learning with Diff Pruning},
  booktitle    = {ACL},
  pages        = {4884--4896},
  year         = {2021}
}

@inproceedings{TL:Prefix-Tuning,
  author       = {Xiang Lisa Li and
                  Percy Liang},
  title        = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  booktitle    = {ACL},
  pages        = {4582--4597},
  year         = {2021}
}

@inproceedings{TL:PHC-GNN,
  author       = {Tuan Le and
                  Marco Bertolini and
                  Frank No{\'{e}} and
                  Djork{-}Arn{\'{e}} Clevert},
  title        = {Parameterized Hypercomplex Graph Neural Networks for Graph Classification},
  booktitle    = {ICANN},
  volume       = {12893},
  pages        = {204--216},
  year         = {2021}
}

@inproceedings{TL:PEPT,
  author       = {Brian Lester and
                  Rami Al{-}Rfou and
                  Noah Constant},
  title        = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle    = {EMNLP},
  pages        = {3045--3059},
  year         = {2021}
}

@inproceedings{TL:Compacter,
  author       = {Rabeeh Karimi Mahabadi and
                  James Henderson and
                  Sebastian Ruder},
  title        = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  booktitle    = {NeurIPS},
  pages        = {1022--1035},
  year         = {2021}
}

@inproceedings{TL:Frozen,
  author       = {Maria Tsimpoukelli and
                  Jacob Menick and
                  Serkan Cabi and
                  S. M. Ali Eslami and
                  Oriol Vinyals and
                  Felix Hill},
  title        = {Multimodal Few-Shot Learning with Frozen Language Models},
  booktitle    = {NeurIPS},
  pages        = {200--212},
  year         = {2021}
}

@inproceedings{TL:Hypernetwork,
  author       = {Rabeeh Karimi Mahabadi and
                  Sebastian Ruder and
                  Mostafa Dehghani and
                  James Henderson},
  title        = {Parameter-efficient Multi-task Fine-tuning for Transformers via Shared
                  Hypernetworks},
  booktitle    = {ACL},
  pages        = {565--576},
  year         = {2021}
}

@article{TL:CPT,
  author       = {Yuan Yao and
                  Ao Zhang and
                  Zhengyan Zhang and
                  Zhiyuan Liu and
                  Tat{-}Seng Chua and
                  Maosong Sun},
  title        = {{CPT:} Colorful Prompt Tuning for Pre-trained Vision-Language Models},
  journal      = {arXiv: 2109.11797},
  year         = {2021}
}

@article{TL:CLIP-Adapter,
  author       = {Peng Gao and
                  Shijie Geng and
                  Renrui Zhang and
                  Teli Ma and
                  Rongyao Fang and
                  Yongfeng Zhang and
                  Hongsheng Li and
                  Yu Qiao},
  title        = {CLIP-Adapter: Better Vision-Language Models with Feature Adapters},
  journal      = {arXiv: 2110.04544},
  year         = {2021}
}

@article{TL:Layernorm-Tuning,
  title={How to Adapt Your Large-Scale Vision-and-Language Model},
  author={Kim, Konwoo and Laskin, Michael and Mordatch, Igor and Pathak, Deepak},
  journal = {openreview},
  year={2021}
}

@inproceedings{TL:FISH-Mask,
  author       = {Yi{-}Lin Sung and
                  Varun Nair and
                  Colin Raffel},
  title        = {Training Neural Networks with Fixed Sparse Masks},
  booktitle    = {NeurIPS},
  pages        = {24193--24205},
  year         = {2021}
}

@article{TL:CoOp,
  author       = {Kaiyang Zhou and
                  Jingkang Yang and
                  Chen Change Loy and
                  Ziwei Liu},
  title        = {Learning to Prompt for Vision-Language Models},
  journal      = {IJCV},
  volume       = {130},
  number       = {9},
  pages        = {2337--2348},
  year         = {2022}
}

@inproceedings{TL:Head2Toe,
  author       = {Utku Evci and
                  Vincent Dumoulin and
                  Hugo Larochelle and
                  Michael C. Mozer},
  title        = {Head2Toe: Utilizing Intermediate Representations for Better Transfer
                  Learning},
  booktitle    = {ICML},
  volume       = {162},
  pages        = {6009--6033},
  year         = {2022}
}

@inproceedings{TL:PPT,
  author       = {Yuxian Gu and
                  Xu Han and
                  Zhiyuan Liu and
                  Minlie Huang},
  title        = {{PPT:} Pre-trained Prompt Tuning for Few-shot Learning},
  booktitle    = {ACL},
  pages        = {8410--8423},
  year         = {2022}
}

@inproceedings{TL:LoRA,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {ICLR},
  year         = {2022}
}

@inproceedings{TL:BitFit,
  author       = {Elad Ben Zaken and
                  Yoav Goldberg and
                  Shauli Ravfogel},
  title        = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based
                  Masked Language-models},
  booktitle    = {ACL},
  pages        = {1--9},
  year         = {2022}
}

@inproceedings{TL:UnifiedPET,
  author       = {Junxian He and
                  Chunting Zhou and
                  Xuezhe Ma and
                  Taylor Berg{-}Kirkpatrick and
                  Graham Neubig},
  title        = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  booktitle    = {ICLR},
  year         = {2022}
}

@inproceedings{TL:UniPELT,
  author       = {Yuning Mao and
                  Lambert Mathias and
                  Rui Hou and
                  Amjad Almahairi and
                  Hao Ma and
                  Jiawei Han and
                  Scott Yih and
                  Madian Khabsa},
  title        = {UniPELT: {A} Unified Framework for Parameter-Efficient Language Model
                  Tuning},
  booktitle    = {ACL},
  pages        = {6253--6264},
  year         = {2022}
}

@inproceedings{TL:Tip-adapter,
  title={Tip-adapter: Training-free clip-adapter for better vision-language modeling},
  author={Zhang, Renrui and Fang, Rongyao and Zhang, Wei and Gao, Peng and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={ECCV},
  year={2022}
}

@inproceedings{TL:DenseCLIP,
  author       = {Yongming Rao and
                  Wenliang Zhao and
                  Guangyi Chen and
                  Yansong Tang and
                  Zheng Zhu and
                  Guan Huang and
                  Jie Zhou and
                  Jiwen Lu},
  title        = {DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting},
  booktitle    = {CVPR},
  pages        = {18061--18070},
  year         = {2022}
}

@inproceedings{TL:Efficient-Prompt,
  author       = {Chen Ju and
                  Tengda Han and
                  Kunhao Zheng and
                  Ya Zhang and
                  Weidi Xie},
  title        = {Prompting Visual-Language Models for Efficient Video Understanding},
  booktitle    = {ECCV},
  volume       = {13695},
  pages        = {105--124},
  year         = {2022}
}

@inproceedings{TL:VL-ADAPTER,
  author       = {Yi{-}Lin Sung and
                  Jaemin Cho and
                  Mohit Bansal},
  title        = {{VL-ADAPTER:} Parameter-Efficient Transfer Learning for Vision-and-Language
                  Tasks},
  booktitle    = {CVPR},
  pages        = {5217--5227},
  year         = {2022}
}

@inproceedings{TL:Language-Planners,
  author       = {Wenlong Huang and
                  Pieter Abbeel and
                  Deepak Pathak and
                  Igor Mordatch},
  title        = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge
                  for Embodied Agents},
  booktitle    = {ICML},
  volume       = {162},
  pages        = {9118--9147},
  year         = {2022}
}

@inproceedings{TL:CoT,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={NeurIPS},
  year={2022}
}

@article{TL:Y-Tuning,
  author       = {Yitao Liu and
                  Chenxin An and
                  Xipeng Qiu},
  title        = {Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained
                  Models via Label Representation Learning},
  journal      = {arXiv: 2202.09817},
  year         = {2022}
}

@article{TL:MaPLe,
  author       = {Muhammad Uzair Khattak and
                  Hanoona Abdul Rasheed and
                  Muhammad Maaz and
                  Salman Khan and
                  Fahad Shahbaz Khan},
  title        = {MaPLe: Multi-modal Prompt Learning},
  journal      = {arXiv: 2210.03117},
  year         = {2022}
}

@inproceedings{TL:SSF,
  author       = {Dongze Lian and
                  Daquan Zhou and
                  Jiashi Feng and
                  Xinchao Wang},
  title        = {Scaling {\&} Shifting Your Features: {A} New Baseline for Efficient Model Tuning},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@inproceedings{TL:CoCoOp,
  author       = {Kaiyang Zhou and
                  Jingkang Yang and
                  Chen Change Loy and
                  Ziwei Liu},
  title        = {Conditional Prompt Learning for Vision-Language Models},
  booktitle    = {CVPR},
  pages        = {16795--16804},
  year         = {2022}
}

@inproceedings{TL:VPT,
  author       = {Menglin Jia and
                  Luming Tang and
                  Bor{-}Chun Chen and
                  Claire Cardie and
                  Serge J. Belongie and
                  Bharath Hariharan and
                  Ser{-}Nam Lim},
  title        = {Visual Prompt Tuning},
  booktitle    = {ECCV},
  volume       = {13693},
  pages        = {709--727},
  year         = {2022}
}

@article{TL:IA3,
  author       = {Haokun Liu and
                  Derek Tam and
                  Mohammed Muqeeth and
                  Jay Mohta and
                  Tenghao Huang and
                  Mohit Bansal and
                  Colin Raffel},
  title        = {Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than
                  In-Context Learning},
  journal      = {arXiv: 2205.05638},
  year         = {2022}
}

@inproceedings{TL:AdaptFormer,
  author       = {Shoufa Chen and
                  Chongjian Ge and
                  Zhan Tong and
                  Jiangliu Wang and
                  Yibing Song and
                  Jue Wang and
                  Ping Luo},
  title        = {AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
  booktitle      = {NeurIPS},
  year         = {2022}
}

@article{TL:NOAH,
  author       = {Yuanhan Zhang and
                  Kaiyang Zhou and
                  Ziwei Liu},
  title        = {Neural Prompt Search},
  journal      = {arXiv: 2206.04673},
  year         = {2022}
}

@inproceedings{TL:LST,
  author       = {Yi{-}Lin Sung and
                  Jaemin Cho and
                  Mohit Bansal},
  title        = {{LST:} Ladder Side-Tuning for Parameter and Memory Efficient Transfer
                  Learning},
  booktitle      = {NeurIPS},
  year         = {2022}
}

@article{TL:Convpass,
  author       = {Shibo Jie and
                  Zhi{-}Hong Deng},
  title        = {Convolutional Bypasses Are Better Vision Transformer Adapters},
  journal      = {arXiv: 2207.07039},
  year         = {2022}
}

@inproceedings{TL:PromptPG,
  author       = {Pan Lu and
                  Liang Qiu and
                  Kai{-}Wei Chang and
                  Ying Nian Wu and
                  Song{-}Chun Zhu and
                  Tanmay Rajpurohit and
                  Peter Clark and
                  Ashwin Kalyan},
  title        = {Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical
                  Reasoning},
  journal      = {ICLR},
  year         = {2023}
}

@inproceedings{TL:PLOT,
  title={PLOT: Prompt Learning with Optimal Transport for Vision-Language Models},
  author={Chen, Guangyi and Yao, Weiran and Song, Xiangchen and Li, Xinyue and Rao, Yongming and Zhang, Kun},
  booktitle={ICLR},
  year= {2023}
}

@inproceedings{TL:ViT-Adapter,
  author       = {Zhe Chen and
                  Yuchen Duan and
                  Wenhai Wang and
                  Junjun He and
                  Tong Lu and
                  Jifeng Dai and
                  Yu Qiao},
  title        = {Vision Transformer Adapter for Dense Predictions},
  booktitle      = {ICLR},
  year         = {2023}
}

@inproceedings{TL:VQT,
  author       = {Cheng{-}Hao Tu and
                  Zheda Mai and
                  Wei{-}Lun Chao},
  title        = {Visual Query Tuning: Towards Effective Usage of Intermediate Representations
                  for Parameter and Memory Efficient Transfer Learning},
  booktitle    = {CVPR},
  pages        = {7725--7735},
  year         = {2023},
}

@inproceedings{TL:Description,
  author       = {Sachit Menon and
                  Carl Vondrick},
  title        = {Visual Classification via Description from Large Language Models},
  journal      = {ICLR},
  year         = {2023}
}

@inproceedings{TL:SoLa,
  author       = {Hyolim Kang and
                  Hanjung Kim and
                  Joungbin An and
                  Minsu Cho and
                  Seon Joo Kim},
  title        = {Soft-Landing Strategy for Alleviating the Task Discrepancy Problem
                  in Temporal Action Localization Tasks},
  journal      = {CVPR},
  year         = {2023}
}

@inproceedings{TL:CLIP-ReID,
  author       = {Siyuan Li and
                  Li Sun and
                  Qingli Li},
  title        = {CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification
                  without Concrete Text Labels},
  journal      = {AAAI},
  year         = {2023}
}

@inproceedings{TL:FacT,
  author       = {Shibo Jie and
                  Zhi{-}Hong Deng},
  title        = {FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer},
  booktitle      = {AAAI},
  year         = {2023}
}

@article{TL:MM-CoT,
  author       = {Zhuosheng Zhang and
                  Aston Zhang and
                  Mu Li and
                  Hai Zhao and
                  George Karypis and
                  Alex Smola},
  title        = {Multimodal Chain-of-Thought Reasoning in Language Models},
  journal      = {arXiv: 2302.00923},
  year         = {2023}
}

@article{TL:UniAdapter,
  author       = {Haoyu Lu and
                  Mingyu Ding and
                  Yuqi Huo and
                  Guoxing Yang and
                  Zhiwu Lu and
                  Masayoshi Tomizuka and
                  Wei Zhan},
  title        = {UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal
                  Modeling},
  journal      = {arXiv: 2302.06605},
  year         = {2023}
}

@article{TL:PTUnifier,
  author       = {Zhihong Chen and
                  Shizhe Diao and
                  Benyou Wang and
                  Guanbin Li and
                  Xiang Wan},
  title        = {Towards Unifying Medical Vision-and-Language Pre-training via Soft
                  Prompts},
  journal      = {arXiv: 2302.08958},
  year         = {2023}
}

@inproceedings{TL:CaFo,
  author       = {Renrui Zhang and
                  Xiangfei Hu and
                  Bohao Li and
                  Siyuan Huang and
                  Hanqiu Deng and
                  Hongsheng Li and
                  Yu Qiao and
                  Peng Gao},
  title        = {Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong
                  Few-shot Learners},
  journal      = {CVPR},
  year         = {2023}
}

@inproceedings{TL:VoP,
  author       = {Siteng Huang and
                  Biao Gong and
                  Yulin Pan and
                  Jianwen Jiang and
                  Yiliang Lv and
                  Yuyuan Li and
                  Donglin Wang},
  title        = {VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval},
  booktitle    = {CVPR},
  pages        = {6565--6574},
  year         = {2023}
}

@inproceedings{TL:Tem-adapter,
  author       = {Guangyi Chen and
                  Xiao Liu and
                  Guangrun Wang and
                  Kun Zhang and
                  Philip H. S. Torr and
                  Xiao{-}Ping Zhang and
                  Yansong Tang},
  title        = {Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer},
  booktitle    = {ICCV},
  year         = {2023}
}

@inproceedings{TL:VL-PET,
  author       = {Zi{-}Yuan Hu and
                  Yanyang Li and
                  Michael R. Lyu and
                  Liwei Wang},
  title        = {{VL-PET:} Vision-and-Language Parameter-Efficient Tuning via Granularity Control},
  booktitle    = {ICCV},
  year         = {2023}
}

@inproceedings{TL:VLN-PETL,
  author       = {Yanyuan Qiao and
                  Zheng Yu and
                  Qi Wu},
  title        = {{VLN-PETL:} Parameter-Efficient Transfer Learning for Vision-and-Language Navigation},
  booktitle    = {ICCV},
  year         = {2023}
}

@inproceedings{TL:AdaLoRA,
  author       = {Qingru Zhang and
                  Minshuo Chen and
                  Alexander Bukharin and
                  Pengcheng He and
                  Yu Cheng and
                  Weizhu Chen and
                  Tuo Zhao},
  title        = {Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  booktitle    = {ICLR},
  year         = {2023}
}

@article{TL:QLoRA,
  author       = {Tim Dettmers and
                  Artidoro Pagnoni and
                  Ari Holtzman and
                  Luke Zettlemoyer},
  title        = {QLoRA: Efficient Finetuning of Quantized LLMs},
  journal      = {arXiv: 2305.14314},
  year         = {2023}
}

@article{TL:ReLoRA,
  author       = {Vladislav Lialin and
                  Namrata Shivagunde and
                  Sherin Muckatira and
                  Anna Rumshisky},
  title        = {Stack More Layers Differently: High-Rank Training Through Low-Rank
                  Updates},
  journal      = {arXiv: 2307.05695},
  year         = {2023}
}

@inproceedings{TL:SAN,
  author       = {Mengde Xu and
                  Zheng Zhang and
                  Fangyun Wei and
                  Han Hu and
                  Xiang Bai},
  title        = {Side Adapter Network for Open-Vocabulary Semantic Segmentation},
  booktitle    = {CVPR},
  pages        = {2945--2954},
  year         = {2023}
}

@inproceedings{TL:ViPT,
  title={Visual prompt multi-modal tracking},
  author={Zhu, Jiawen and Lai, Simiao and Chen, Xin and Wang, Dong and Lu, Huchuan},
  booktitle={CVPR},
  pages={9516--9526},
  year={2023}
}

@inproceedings{TL:Res-Tuning,
  author       = {Zeyinzi Jiang and
                  Chaojie Mao and
                  Ziyuan Huang and
                  Ao Ma and
                  Yiliang Lv and
                  Yujun Shen and
                  Deli Zhao and
                  Jingren Zhou},
  title        = {Res-Tuning: {A} Flexible and Efficient Tuning Paradigm via Unbinding
                  Tuner from Backbone},
  booktitle    = {NeurIPS},
  year         = {2023},
}

@inproceedings{TL:InCA,
  author       = {Yonatan Dukler and
                  Alessandro Achille and
                  Hao Yang and
                  Varsha Vivek and
                  Luca Zancato and
                  Benjamin Bowman and
                  Avinash Ravichandran and
                  Charless C. Fowlkes and
                  Ashwin Swaminathan and
                  Stefano Soatto},
  title        = {Your representations are in the network: composable and parallel adaptation
                  for large scale models},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@inproceedings{TL:UniPT,
  author       = {Haiwen Diao and
                  Bo Wan and
                  Ying Zhang and
                  Xu Jia and
                  Huchuan Lu and
                  Long Chen},
  title        = {UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory},
  booktitle    = {CVPR},
  year         = {2024},
}

@inproceedings{TL:SHERL,
  title={SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning},
  author={Diao, Haiwen and Wan, Bo and Jia, Xu and Zhuge, Yunzhi and Zhang, Ying and Lu, Huchuan and Chen, Long},
  booktitle={ECCV},
  pages={75--95},
  year={2025},
}

%---------------------------------------------------%
%---------------- Ethics Statement  ----------------%
%---------------------------------------------------%

@inproceedings{ES:IRGB,
  author       = {Shikha Bordia and
                  Samuel R. Bowman},
  title        = {Identifying and Reducing Gender Bias in Word-Level Language Models},
  booktitle    = {NAACL-HLT},
  pages        = {7--15},
  year         = {2019}
}

@inproceedings{ES:EPC_NLP,
  author       = {Emma Strubell and
                  Ananya Ganesh and
                  Andrew McCallum},
  title        = {Energy and Policy Considerations for Deep Learning in {NLP}},
  booktitle    = {ACL},
  pages        = {3645--3650},
  year         = {2019}
}

@inproceedings{ES:ETD_LLM,
  author       = {Nicholas Carlini and
                  Florian Tram{\`{e}}r and
                  Eric Wallace and
                  Matthew Jagielski and
                  Ariel Herbert{-}Voss and
                  Katherine Lee and
                  Adam Roberts and
                  Tom B. Brown and
                  Dawn Song and
                  {\'{U}}lfar Erlingsson and
                  Alina Oprea and
                  Colin Raffel},
  title        = {Extracting Training Data from Large Language Models},
  booktitle    = {USENIX Security Symposium},
  pages        = {2633--2650},
  year         = {2021}
}

%---------------------------------------------------%
%--------  Large Vision-Language Model  ------------%
%---------------------------------------------------%

@inproceedings{VLM:InstructBLIP,
  author       = {Wenliang Dai and
                  Junnan Li and
                  Dongxu Li and
                  Anthony Meng Huat Tiong and
                  Junqi Zhao and
                  Weisheng Wang and
                  Boyang Li and
                  Pascale Fung and
                  Steven C. H. Hoi},
  title        = {InstructBLIP: Towards General-purpose Vision-Language Models with
                  Instruction Tuning},
  booktitle    = {NeurIPS},
  year         = {2023},
}

@inproceedings{VLM:LLaVA,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
  title        = {Visual Instruction Tuning},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{VLM:LLaVA-1.5,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Yuheng Li and
                  Yong Jae Lee},
  title        = {Improved Baselines with Visual Instruction Tuning},
  journal      = {arXiv: 2310.03744},
  year         = {2023}
}

@misc{VLM:LLaVA-1.6,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    year={2024}
}

@misc{VLM:LLaVA-NeXT,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    year={2024}
}

@article{VLM:EMU,
  author       = {Quan Sun and
                  Qiying Yu and
                  Yufeng Cui and
                  Fan Zhang and
                  Xiaosong Zhang and
                  Yueze Wang and
                  Hongcheng Gao and
                  Jingjing Liu and
                  Tiejun Huang and
                  Xinlong Wang},
  title        = {Generative Pretraining in Multimodality},
  journal      = {arXiv: 2307.05222},
  year         = {2023},
}

@article{VLM:EMUv2,
  author       = {Quan Sun and
                  Yufeng Cui and
                  Xiaosong Zhang and
                  Fan Zhang and
                  Qiying Yu and
                  Zhengxiong Luo and
                  Yueze Wang and
                  Yongming Rao and
                  Jingjing Liu and
                  Tiejun Huang and
                  Xinlong Wang},
  title        = {Generative Multimodal Models are In-Context Learners},
  journal      = {arXiv: 2312.13286},
  year         = {2023}
}

@article{VLM:Xcomposer2-4KHD,
  author       = {Xiaoyi Dong and
                  Pan Zhang and
                  Yuhang Zang and
                  Yuhang Cao and
                  Bin Wang and
                  Linke Ouyang and
                  Songyang Zhang and
                  Haodong Duan and
                  Wenwei Zhang and
                  Yining Li and
                  Hang Yan and
                  Yang Gao and
                  Zhe Chen and
                  Xinyue Zhang and
                  Wei Li and
                  Jingwen Li and
                  Wenhai Wang and
                  Kai Chen and
                  Conghui He and
                  Xingcheng Zhang and
                  Jifeng Dai and
                  Yu Qiao and
                  Dahua Lin and
                  Jiaqi Wang},
  title        = {InternLM-XComposer2-4KHD: {A} Pioneering Large Vision-Language Model
                  Handling Resolutions from 336 Pixels to 4K {HD}},
  journal      = {arXiv: 2404.06512},
  year         = {2024}
}

@misc{VLM:Fuyu-8b,
  author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
  title = {Introducing our Multimodal Models},
  url = {https://www.adept.ai/blog/fuyu-8b},
  year = {2023}
}

@article{VLM:OtterHD,
  author       = {Bo Li and
                  Peiyuan Zhang and
                  Jingkang Yang and
                  Yuanhan Zhang and
                  Fanyi Pu and
                  Ziwei Liu},
  title        = {OtterHD: {A} High-Resolution Multi-modality Model},
  journal      = {arXiv: 2311.04219},
  year         = {2023}
}



@article{VLM:SGlang,
  author       = {Lianmin Zheng and
                  Liangsheng Yin and
                  Zhiqiang Xie and
                  Jeff Huang and
                  Chuyue Sun and
                  Cody Hao Yu and
                  Shiyi Cao and
                  Christos Kozyrakis and
                  Ion Stoica and
                  Joseph E. Gonzalez and
                  Clark W. Barrett and
                  Ying Sheng},
  title        = {Efficiently Programming Large Language Models using SGLang},
  journal      = {arXiv: 2312.07104},
  year         = {2023}
}

@inproceedings{VLM:vLLM-efficient,
  author       = {Woosuk Kwon and
                  Zhuohan Li and
                  Siyuan Zhuang and
                  Ying Sheng and
                  Lianmin Zheng and
                  Cody Hao Yu and
                  Joseph Gonzalez and
                  Hao Zhang and
                  Ion Stoica},
  title        = {Efficient Memory Management for Large Language Model Serving with
                  PagedAttention},
  booktitle    = {SOSP},
  pages        = {611--626},
  year         = {2023}
}

@article{VLM:S2-LVLM,
  author       = {Baifeng Shi and
                  Ziyang Wu and
                  Maolin Mao and
                  Xin Wang and
                  Trevor Darrell},
  title        = {When Do We Not Need Larger Vision Models?},
  journal      = {arXiv: 2403.13043},
  year         = {2024}
}

@article{VLM:InternVL-1.5,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv:2404.16821},
  year={2024}
}

@article{VLM:LLaVA-UHD,
  author       = {Ruyi Xu and
                  Yuan Yao and
                  Zonghao Guo and
                  Junbo Cui and
                  Zanlin Ni and
                  Chunjiang Ge and
                  Tat{-}Seng Chua and
                  Zhiyuan Liu and
                  Maosong Sun and
                  Gao Huang},
  title        = {LLaVA-UHD: an {LMM} Perceiving Any Aspect Ratio and High-Resolution
                  Images},
  journal      = {arXiv: 2403.11703},
  year         = {2024}
}

@article{VLM:Monkey,
  author       = {Zhang Li and
                  Biao Yang and
                  Qiang Liu and
                  Zhiyin Ma and
                  Shuo Zhang and
                  Jingxu Yang and
                  Yabo Sun and
                  Yuliang Liu and
                  Xiang Bai},
  title        = {Monkey: Image Resolution and Text Label Are Important Things for Large
                  Multi-modal Models},
  journal      = {arXiv: 2311.06607},
  year         = {2023}
}

@article{VLM:AnyGPT,
  author       = {Jun Zhan and
                  Junqi Dai and
                  Jiasheng Ye and
                  Yunhua Zhou and
                  Dong Zhang and
                  Zhigeng Liu and
                  Xin Zhang and
                  Ruibin Yuan and
                  Ge Zhang and
                  Linyang Li and
                  Hang Yan and
                  Jie Fu and
                  Tao Gui and
                  Tianxiang Sun and
                  Yugang Jiang and
                  Xipeng Qiu},
  title        = {AnyGPT: Unified Multimodal {LLM} with Discrete Sequence Modeling},
  journal      = {arXiv: 2402.12226},
  year         = {2024}
}

@article{VLM:GPT-4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {arXiv: 2303.08774},
  year         = {2023},
}

@article{VLM:Claude3,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}

@inproceedings{VLM:Minigpt-4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={ICLR},
  year={2024}
}

@article{VLM:Skywork,
  title={Skywork: A More Open Bilingual Foundation Model},
  author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and others},
  journal={arXiv: 2310.19341},
  year={2023}
}

@article{VLM:Sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv: 2311.12793},
  year={2023}
}

@misc{VLM::PpenCompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@misc{VLM:Step-1V,
  author = {{StepFun Research Team}},
  title = {Step-1V: A Hundred Billion Parameter Multimodal Large Model},
  howpublished = {\url{https://platform.stepfun.com}},
  year = {2024}
}

@misc{VLM:IDEFICS,
  author = {{IDEFICS Research Team}},
  title = {Introducing idefics: An open reproduction of state-of-the-art visual language model},
  howpublished = {\url{https://huggingface.co/blog/idefics}},
  year = {2023}
}

@article{VLM:MiniGemini,
  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv: 2403.18814},
  year={2024}
}

@misc{VLM:Grok-1.5,
  author = {X.ai},
  title = {Grok-1.5 Vision Preview},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@article{VLM:UREADER,
  title={Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Xu, Guohai and Li, Chenliang and Tian, Junfeng and Qian, Qi and Zhang, Ji and others},
  journal={arXiv: 2310.05126},
  year={2023}
}

@article{VLM:CogAgent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv:2312.08914},
  year={2023}
}

@article{VLM:MM-interleaved,
  title={Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer},
  author={Tian, Changyao and Zhu, Xizhou and Xiong, Yuwen and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Chen, Yuntao and Lu, Lewei and Lu, Tong and Zhou, Jie and others},
  journal={arXiv:2401.10208},
  year={2024}
}

@inproceedings{VLM:Llama-Adapter,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  booktitle={ICLR},
  year={2024}
}

@article{VLM:Llama-Adapterv2,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv: 2304.15010},
  year={2023}
}

@article{VLM:GPT4ROI,
  title={Gpt4roi: Instruction tuning large language model on region-of-interest},
  author={Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
  journal={arXiv: 2307.03601},
  year={2023}
}

@article{VLM:NeXTGPT,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv: 2309.05519},
  year={2023}
}

@article{VLM:GPT-4v,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv: 2309.17421},
  volume={9},
  year={2023}
}

@article{VLM:Bunny,
  author       = {Muyang He and
                  Yexin Liu and
                  Boya Wu and
                  Jianhao Yuan and
                  Yueze Wang and
                  Tiejun Huang and
                  Bo Zhao},
  title        = {Efficient Multimodal Learning from Data-centric Perspective},
  journal      = {arXiv: 2402.11530},
  year         = {2024}
}

@article{VLM:Gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv: 2312.11805},
  year={2023}
}

@article{VLM:DeepSeek-VL,
  author       = {Haoyu Lu and
                  Wen Liu and
                  Bo Zhang and
                  Bingxuan Wang and
                  Kai Dong and
                  Bo Liu and
                  Jingxiang Sun and
                  Tongzheng Ren and
                  Zhuoshu Li and
                  Hao Yang and
                  Yaofeng Sun and
                  Chengqi Deng and
                  Hanwei Xu and
                  Zhenda Xie and
                  Chong Ruan},
  title        = {DeepSeek-VL: Towards Real-World Vision-Language Understanding},
  journal      = {arXiv: 2403.05525},
  year         = {2024}
}

@article{VLM:InternVL,
  author       = {Zhe Chen and
                  Jiannan Wu and
                  Wenhai Wang and
                  Weijie Su and
                  Guo Chen and
                  Sen Xing and
                  Muyan Zhong and
                  Qinglong Zhang and
                  Xizhou Zhu and
                  Lewei Lu and
                  Bin Li and
                  Ping Luo and
                  Tong Lu and
                  Yu Qiao and
                  Jifeng Dai},
  title        = {InternVL: Scaling up Vision Foundation Models and Aligning for Generic
                  Visual-Linguistic Tasks},
  journal      = {arXiv: 2312.14238},
  year         = {2023}
}

@article{VLM:mPLUG-Owl,
  author       = {Qinghao Ye and
                  Haiyang Xu and
                  Guohai Xu and
                  Jiabo Ye and
                  Ming Yan and
                  Yiyang Zhou and
                  Junyang Wang and
                  Anwen Hu and
                  Pengcheng Shi and
                  Yaya Shi and
                  Chenliang Li and
                  Yuanhong Xu and
                  Hehong Chen and
                  Junfeng Tian and
                  Qian Qi and
                  Ji Zhang and
                  Fei Huang},
  title        = {mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality},
  journal      = {arXiv: 2304.14178},
  year         = {2023}
}

@article{VLM:mPLUG-Owl2,
  author       = {Qinghao Ye and
                  Haiyang Xu and
                  Jiabo Ye and
                  Ming Yan and
                  Anwen Hu and
                  Haowei Liu and
                  Qi Qian and
                  Ji Zhang and
                  Fei Huang and
                  Jingren Zhou},
  title        = {mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with
                  Modality Collaboration},
  journal      = {arXiv: 2311.04257},
  year         = {2023}
}

@article{Datasets:datacomp,
  title={Datacomp: In search of the next generation of multimodal datasets},
  author={Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@misc{Datasets:Realworldqa,
  author = {x.ai},
  title = {Grok-1.5 vision preview},
  url = {https://x.ai/blog/grok-1.5v},
  year = {2024}
}

@misc{VLMs:LLama3.2,
  author = {Meta Team},
  title = {Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
  url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
  year = {2024}
}

@inproceedings{tong2024eyes,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  booktitle={CVPR},
  pages={9568--9578},
  year={2024}
}

@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@article{dai2024nvlm,
  title={Nvlm: Open frontier-class multimodal llms},
  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuoling and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2409.11402},
  year={2024}
}

@article{xie2024show-o,
  title={Show-o: One single transformer to unify multimodal understanding and generation},
  author={Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2408.12528},
  year={2024}
}

@article{wu2024vila-u,
  title={Vila-u: a unified foundation model integrating visual understanding and generation},
  author={Wu, Yecheng and Zhang, Zhuoyang and Chen, Junyu and Tang, Haotian and Li, Dacheng and Fang, Yunhao and Zhu, Ligeng and Xie, Enze and Yin, Hongxu and Yi, Li and others},
  journal={arXiv preprint arXiv:2409.04429},
  year={2024}
}

@article{wang2024mio,
  title={Mio: A foundation model on multimodal tokens},
  author={Wang, Zekun and Zhu, King and Xu, Chunpu and Zhou, Wangchunshu and Liu, Jiaheng and Zhang, Yibo and Wang, Jiashuo and Shi, Ning and Li, Siyu and Li, Yizhi and others},
  journal={arXiv preprint arXiv:2409.17692},
  year={2024}
}

@article{zhou2024transfusion,
  title={Transfusion: Predict the next token and diffuse images with one multi-modal model},
  author={Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
  journal={arXiv preprint arXiv:2408.11039},
  year={2024}
}

@article{van2017VQVAE,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@inproceedings{VQGAN,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={CVPR},
  pages={12873--12883},
  year={2021}
}


@article{LLamagen,
  title={Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation},
  author={Sun, Peize and Jiang, Yi and Chen, Shoufa and Zhang, Shilong and Peng, Bingyue and Luo, Ping and Yuan, Zehuan},
  journal={arXiv preprint arXiv:2406.06525},
  year={2024}
}

@article{beyer2024paligemma,
  title={Paligemma: A versatile 3b vlm for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv preprint arXiv:2407.07726},
  year={2024}
}

@article{wu2024janus,
  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},
  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2410.13848},
  year={2024}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{lin2024moma,
  title={Moma: Efficient early-fusion pre-training with mixture of modality-aware experts},
  author={Lin, Xi Victoria and Shrivastava, Akshat and Luo, Liang and Iyer, Srinivasan and Lewis, Mike and Gosh, Gargi and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={arXiv preprint arXiv:2407.21770},
  year={2024}
}

@article{VLM:qwen-vl,
  title={Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  volume={1},
  number={2},
  pages={3},
  year={2023}
}

@article{Llava-onevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{gu2024infinity,
  title={Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data},
  author={Gu, Shuhao and Zhang, Jialing and Zhou, Siyuan and Yu, Kevin and Xing, Zhaohu and Wang, Liangdong and Cao, Zhou and Jia, Jintao and Zhang, Zhuoyi and Wang, Yixuan and others},
  journal={arXiv preprint arXiv:2410.18558},
  year={2024}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{VLM:LVIS-4v,
  author       = {Junke Wang and
                  Lingchen Meng and
                  Zejia Weng and
                  Bo He and
                  Zuxuan Wu and
                  Yu{-}Gang Jiang},
  title        = {To See is to Believe: Prompting {GPT-4V} for Better Visual Instruction
                  Tuning},
  journal      = {arXiv: 2311.07574},
  year         = {2023}
}

@inproceedings{TransF:ViT-22B,
  author       = {Mostafa Dehghani and
                  Josip Djolonga and
                  Basil Mustafa and
                  Piotr Padlewski and
                  Jonathan Heek and
                  Justin Gilmer and
                  Andreas Peter Steiner and
                  Mathilde Caron and
                  Robert Geirhos and
                  Ibrahim Alabdulmohsin and
                  Rodolphe Jenatton and
                  Lucas Beyer and
                  Michael Tschannen and
                  Anurag Arnab and
                  Xiao Wang and
                  Carlos Riquelme Ruiz and
                  Matthias Minderer and
                  Joan Puigcerver and
                  Utku Evci and
                  Manoj Kumar and
                  Sjoerd van Steenkiste and
                  Gamaleldin Fathy Elsayed and
                  Aravindh Mahendran and
                  Fisher Yu and
                  Avital Oliver and
                  Fantine Huot and
                  Jasmijn Bastings and
                  Mark Collier and
                  Alexey A. Gritsenko and
                  Vighnesh Birodkar and
                  Cristina Nader Vasconcelos and
                  Yi Tay and
                  Thomas Mensink and
                  Alexander Kolesnikov and
                  Filip Pavetic and
                  Dustin Tran and
                  Thomas Kipf and
                  Mario Lucic and
                  Xiaohua Zhai and
                  Daniel Keysers and
                  Jeremiah J. Harmsen and
                  Neil Houlsby},
  title        = {Scaling Vision Transformers to 22 Billion Parameters},
  booktitle    = {ICML},
  volume       = {202},
  pages        = {7480--7512},
  year         = {2023}
}

@article{TransF:AIM,
  title={Scalable pre-training of large autoregressive image models},
  author={El-Nouby, Alaaeldin and Klein, Michal and Zhai, Shuangfei and Bautista, Miguel Angel and Toshev, Alexander and Shankar, Vaishaal and Susskind, Joshua M and Joulin, Armand},
  journal={arXiv preprint arXiv:2401.08541},
  year={2024}
}


@article{Datasets:WizardLM,
  author       = {Can Xu and
                  Qingfeng Sun and
                  Kai Zheng and
                  Xiubo Geng and
                  Pu Zhao and
                  Jiazhan Feng and
                  Chongyang Tao and
                  Daxin Jiang},
  title        = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  journal      = {arXiv: 2304.12244},
  year         = {2023},
}

@article{Datasets:SVIT,
  author       = {Bo Zhao and
                  Boya Wu and
                  Tiejun Huang},
  title        = {{SVIT:} Scaling up Visual Instruction Tuning},
  journal      = {arXiv: 2307.04087},
  year         = {2023}
}

@inproceedings{Datasets:TextCaps,
  author       = {Oleksii Sidorov and
                  Ronghang Hu and
                  Marcus Rohrbach and
                  Amanpreet Singh},
  title        = {TextCaps: {A} Dataset for Image Captioning with Reading Comprehension},
  booktitle    = {ECCV},
  volume       = {12347},
  pages        = {742--758},
  year         = {2020}
}

@article{VLM:MANTIS,
  title={MANTIS: Interleaved Multi-Image Instruction Tuning},
  author={Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Con and Ku, Max and Liu, Qian and Chen, Wenhu},
  journal={arXiv:2405.01483},
  year={2024}
}

@article{VLM:MM1,
  author       = {Brandon McKinzie and
                  Zhe Gan and
                  Jean{-}Philippe Fauconnier and
                  Sam Dodge and
                  Bowen Zhang and
                  Philipp Dufter and
                  Dhruti Shah and
                  Xianzhi Du and
                  Futang Peng and
                  Floris Weers and
                  Anton Belyi and
                  Haotian Zhang and
                  Karanjeet Singh and
                  Doug Kang and
                  Ankur Jain and
                  Hongyu H{\`{e}} and
                  Max Schwarzer and
                  Tom Gunter and
                  Xiang Kong and
                  Aonan Zhang and
                  Jianyu Wang and
                  Chong Wang and
                  Nan Du and
                  Tao Lei and
                  Sam Wiseman and
                  Guoli Yin and
                  Mark Lee and
                  Zirui Wang and
                  Ruoming Pang and
                  Peter Grasch and
                  Alexander Toshev and
                  Yinfei Yang},
  title        = {{MM1:} Methods, Analysis {\&} Insights from Multimodal {LLM} Pre-training},
  journal      = {arXiv: 2403.09611},
  year         = {2024}
}

@article{VLM:Densefusion,
  title={Densefusion-1m: Merging vision experts for comprehensive multimodal perception},
  author={Li, Xiaotong and Zhang, Fan and Diao, Haiwen and Wang, Yueze and Wang, Xinlong and Duan, Ling-Yu},
  journal={arXiv preprint arXiv:2407.08303},
  year={2024}
}

@misc{VLM:surveyMMB,
      title={A Survey on Multimodal Benchmarks: In the Era of Large AI Models}, 
      author={Lin Li and Guikun Chen and Hanrong Shi and Jun Xiao and Long Chen},
      year={2024},
      eprint={2409.18142},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.18142}, 
}

@inproceedings{VLM:MIMDC,
  title={Masked Image Modeling with Denoising Contrast},
  author={Yi, Kun and Ge, Yixiao and Li, Xiaotong and Yang, Shusheng and Li, Dian and Wu, Jianping and Shan, Ying and Qie, Xiaohu},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{VLM:mc-beit,
  title={mc-beit: Multi-choice discretization for image bert pre-training},
  author={Li, Xiaotong and Ge, Yixiao and Yi, Kun and Hu, Zixuan and Shan, Ying and Duan, Ling-Yu},
  booktitle={ECCV},
  pages={231--246},
  year={2022}
}

@inproceedings{
VLM:SEED,
title={Making {LL}a{MA} {SEE} and Draw with {SEED} Tokenizer},
author={Yuying Ge and Sijie Zhao and Ziyun Zeng and Yixiao Ge and Chen Li and Xintao Wang and Ying Shan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=0Nui91LBQS}
}

@article{chen2024allava,
  title={Allava: Harnessing gpt4v-synthesized data for a lite vision-language model},
  author={Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2402.11684},
  year={2024}
}

@article{laurenccon2024docmatix,
  title={Building and better understanding vision-language models: insights and future directions},
  author={Lauren{\c{c}}on, Hugo and Marafioti, Andr{\'e}s and Sanh, Victor and Tronchon, L{\'e}o},
  journal={arXiv preprint arXiv:2408.12637},
  year={2024}
}

@article{hu2024mplug-docowl2,
  title={mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding},
  author={Hu, Anwen and Xu, Haiyang and Zhang, Liang and Ye, Jiabo and Yan, Ming and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2409.03420},
  year={2024}
}

@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32897--32912},
  year={2022}
}

@article{zhang2024lmms-eval,
  title={Lmms-eval: Reality check on the evaluation of large multimodal models},
  author={Zhang, Kaichen and Li, Bo and Zhang, Peiyuan and Pu, Fanyi and Cahyono, Joshua Adrian and Hu, Kairui and Liu, Shuai and Zhang, Yuanhan and Yang, Jingkang and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2407.12772},
  year={2024}
}

@article{xue2024blip-3,
  title={xgen-mm (blip-3): A family of open large multimodal models},
  author={Xue, Le and Shu, Manli and Awadalla, Anas and Wang, Jun and Yan, An and Purushwalkam, Senthil and Zhou, Honglu and Prabhu, Viraj and Dai, Yutong and Ryoo, Michael S and others},
  journal={arXiv preprint arXiv:2408.08872},
  year={2024}
}

@article{wang2024qwen2-vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{VLM:Capsfusion,
title={Capsfusion: Rethinking image-text data at scale},
author={Yu, Qiying and Sun, Quan and Zhang, Xiaosong and Cui, Yufeng and Zhang, Fan and Cao, Yue and Wang, Xinlong and Liu, Jingjing},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={14022--14032},
year={2024}
}

@article{chen2024solo,
  title={A Single Transformer for Scalable Vision-Language Modeling},
  author={Chen, Yangyi and Wang, Xingyao and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2407.06438},
  year={2024}
}

@article{li2024imagefolder,
  title={Imagefolder: Autoregressive image generation with folded tokens},
  author={Li, Xiang and Qiu, Kai and Chen, Hao and Kuen, Jason and Gu, Jiuxiang and Raj, Bhiksha and Lin, Zhe},
  journal={arXiv preprint arXiv:2410.01756},
  year={2024}
}

@article{qu2024tokenflow,
  title={Tokenflow: Unified image tokenizer for multimodal understanding and generation},
  author={Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong},
  journal={arXiv preprint arXiv:2412.03069},
  year={2024}
}

@article{xie2024muse,
  title={MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding},
  author={Xie, Rongchang and Du, Chen and Song, Ping and Liu, Chang},
  journal={arXiv preprint arXiv:2411.17762},
  year={2024}
}

@article{li2024aria,
  title={Aria: An open multimodal native mixture-of-experts model},
  author={Li, Dongxu and Liu, Yudong and Wu, Haoning and Wang, Yue and Shen, Zhiqi and Qu, Bowen and Niu, Xinyao and Wang, Guoyin and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2410.05993},
  year={2024}
}