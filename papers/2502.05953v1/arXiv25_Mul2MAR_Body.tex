\section{Introduction}
% no \IEEEPARstart
The use of Augmented Reality (AR) technology has increased considerably in recent years with development of devices such as smart phones, tablets, and smart glasses. The term AR can be defined in many ways, but general meaning of AR is that integrating 3D virtual objects into a 3D physical environment in a real time. AR allows us to  enhance the richness of the real world by using computers~\cite{azuma1997survey,Huma2015}. AR is different from the Virtual Reality (VR). The main difference is that the real world objects are enhanced with the help of virtually in AR, whereas VR offers a digital recreation of a real life and it replaces the real world with a digitally simulated one. AR has begun to be used in many fields with technological developments in recent years. Some of usage fields are military, health sector, industry, manufacturing, museums, entertainment and gaming. AR provides many convenience to user in all used fields and in the upcoming days, use of AR will increase much~\cite{Huma2015}. 

Nowadays, 3D televisions create illusion of three dimensions with using polarized glasses. There are two type of polarized glasses that are active and passive glasses. Before using of these polarized glasses, anaglyph 3D glasses were used. One of this kind of glasses is a red-cyan glass. The red-cyan glass is the most common, inexpensive and widely used than others. Polarized glasses create more effective 3D images than anaglyph 3D glasses. However, they are not suitable for using with mobile devices. They are also very expensive. Unlike them, anaglyph 3D glasses are suitable for use with mobile devices and it is also inexpensive~\cite{Gungor2014SIU}. Many companies produce special AR glasses for different purpose. Some of these are Vuzix M100~\cite{Vuzix}, Meta~\cite{Meta}, Google Glass~\cite{Google} and Apple Vision Pro~\cite{AppleVisionPro}. However, prices of these AR glasses are very high and it is difficult to afford the charge of these glasses for all consumers. 

In this paper, we present an inexpensive AR application which can be used by red-cyan glasses. We prefer to use red-cyan glasses, because it is inexpensive and widely used. We validate our mobile AR application on several objects, scenes, and views by using inexpensive red-cyan glasses. We show that our mobile AR application provides satisfactory 3D AR perception without causing any uncomfortable situation for human eyes.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.



\section{Related Works}
AR intends to place virtual objects into physical world visuals. To achieve this effect, a software that uses virtual reality combines real world elements with virtual objects on real-time images~\cite{Cawood2008}. Azuma~\cite{azuma1997survey} conducted a poll on his work, and defined three attributes that an ideal AR system should have: combination of reality and virtuality, real-time interactivity and 3D registration.

AR is used on many different fields; to create models and visualize historical buildings in architecture, to visualize human body in medicine, to enhance education materials in engineering, and also in entertainment and games~\cite{azuma1997survey,Broschart2014}. Although this technology seems recent, the first prototype accepted as a VR and AR system was developed by Sutherland~\cite{Sutherland1968}. However, this system was quite expensive and even heavy, needed to be hung on ceiling. Nowadays, AR is developing rapidly thanks to the widespread use of personal computers. In 2009, Christian Doppler Laboratory listed the most important milestones in AR technologies. Even though high-quality applications are being developed, there are not many differences between them and the ones listed~\cite{Doppler}.
%%%%
%%%%Figure 1
%%%%
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{artoolkitdiagram}
  \caption{ \label{fig:arsystem}
          A general overview of a marker-based AR system~\cite{ARToolKit}.
           }
%\vspace{-0.5cm}
\end{figure*}

\section{Method}
In this project we used ARToolKit for our augmented reality technique and OpenGL libraries for the graphics. In Augmented Reality, there are different methods to recognize the environment and place the object. The easiest and cheapest one is to use markers. A general overview of a marker-based AR system can be found in Figure~\ref{fig:arsystem}. Some of the recent works does not even use markers; instead they use Dense Tracking and Mapping (DTAM), but those are more expensive and computationally heavy. In this work, we prefer to use markers.
Markers are important elements in AR. They define where the object will be placed. Usually they are black-and-white geometric patterns, but coloured ones can be used as well. They should be rotationally asymmetric and unique.
Markers represent coordination system for the associated virtual objects in real-time images. When the camera captures an image, it is converted to a black and white image. In this image potential markers are searched by the AR software. If a marker is found, and it matches with the known patterns, software places coordination axes and origin in the image. Lastly, object is placed in the image accordingly~\cite{ARToolKit}. Best results are observed by using contrast and abstract black-and-white markers, since the software we use converts the captured image to black and white image. Coloured markers  may look fuzzy and blend into background. To make the distinction between background and markers, we preferred to use QR codes. As a result, the markers does not get mistaken for background objects, and also does not get mistaken for one another. Compared to traditional markers (see Figure~\ref{fig:arsystem}) using QR codes gives a better result on that case. 

People see with both eyes, but they will not see two different images. Brain form an overlap with images taken from two different angles and it creates the perception of depth. Images taken from two different channels with slightly different angles (like our eyes) can be combined to create 3D effect. Using red-cyan glasses (see Figure~\ref{fig:redcyangl}), right eye sees through cyan filter and only cyan shades (green and blue) passes through it. Similar rule applies for left eye and red filter, which allows only red, orange and yellow pass through. Red and cyan parts of the picture are placed slightly separated from each other, and our brain combines those two different images to achieve the desired 3D effect.

AR application inserts 3D objects into the center of the marker pattern after calculating its distance and angle to the camera. In our AR method, images are obtained for the left eye and the right eye by OpenGL camera movements without moving device camera, which can be seen in Figure~\ref{fig:leftrightview}.
%%%%
%%%%Figure 2
%%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{ar_rc_figure}
  \caption{ \label{fig:leftrightview}
          Taking images of 3D objects for the left and the right eyes.
           }
%\vspace{-0.5cm}
\end{figure}
%%%%
%%%%Figure 3
%%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{redcyanglasses}
  \caption{ \label{fig:redcyangl}
          A red-cyan glasses.
           }
%\vspace{-0.5cm}
\end{figure}

The following method is used for detection of the marker pattern with device camera in ARToolKit library:
\begin{lstlisting}
arGetTransMat(&marker_info[k], patt_center, patt_width1, patt_trans);
\end{lstlisting}
Here, $\small\texttt{patt\_trans}$ gives the location and angle of the pattern and it's a matrix of $3 \times 4$ dimensions. The following method is used for using of this matrix in OpenGL:
\begin{lstlisting}
argConvGlpara(patt_trans, gl_para);
\end{lstlisting}
Here, $\small\texttt{gl\_para}$ is a vector which has 16 elements. When this vector is transferred to OpenGL modelview matrix with the following method:
\begin{lstlisting}
glLoadMatrixd(gl_para);
\end{lstlisting}
the angle and the distance of the object are set. When an object is placed to the center of the marker pattern, an AR display is shown. However, left and right eyes see different perspectives of the object in a stereo view. To be able to create the stereo view effect, camera is shifted to the left and right eye points by modifying  $\small\texttt{gl\_para}$ vector before it's called by  $\small\texttt{glLoadMatrixd}$ function, as it's seen in Figure~\ref{fig:leftrightview}. 

In order to avoid eyes to see the images of each other, the OpenGL filter methods are applied. For the left eye:
\begin{lstlisting}
glColorMask(true, false, false, false);
\end{lstlisting}
and for the right eye:
\begin{lstlisting}
glColorMask(false, true, true, false);
\end{lstlisting}
In our AR application, when the scene is viewed without red-cyan glasses (see Figure~\ref{fig:redcyangl}), the color sliding appears. When the scene is viewed with red-cyan glasses (see Figure~\ref{fig:redcyangl}), the visual perception of three-dimensional AR is provided~\cite{Gungor2014SIU}.


\section{Results}
AR images of the objects are rendered by using QR-based markers. We tried to introduce many markers for each objects. The system could not fully recognized some markers and objects were not displayed correctly by using these markers. So we chose our markers which are similar to QR codes. These type of markers provide us more healthy displays than others. The markers which we used are shown in Figure~\ref{fig:arobjects}(a, d, g). To be able to get realistic rendering results, we also assigned some materials properties and made texture mappings to the objects by using OpenGL library functions (see Figure~\ref{fig:arobjects} and Figure~\ref{fig:resulttriple}).

Firstly, only one object image is rendered each time, by adjusting marker angle and its distance to camera, as it can be seen in Figure~\ref{fig:arobjects}. Later, other objects are added and different images are rendered; but in this case we had some problems with object positions. To solve this problem we made changes on the code, translating the objects on their positions of markers and scaling them. Thus, we managed to made several combinations of the objects and were able to render realistic 3D images, which can be seen in Figure~\ref{fig:arobjects} and Figure~\ref{fig:resulttriple}.

As a result of all our studies, we could render multiple objects with our mobile AR technique, as it's shown in Figure~\ref{fig:resulttriple}. When we look at the screen without the red-cyan glass, we see some colorimetric shift on the display. However, when we look at the screen with the red-cyan glasses, 3D image perception is provided and colorimetric shift are eliminated on the display.

%%%%
%%%%Figure 4
%%%%
\begin{figure*}[t]
  \centering
  \setlength{\tabcolsep}{1pt}
  \setlength{\fboxsep}{0pt}
  \begin{tabular}{cccc}
  \includegraphics[width=.224\linewidth]{mmasa}
  &\includegraphics[width=.376\linewidth]{masa1}
  &\includegraphics[width=.375\linewidth]{masa2}\\
  \mbox{(a)}
  &\mbox{(b)}
  &\mbox{(c)}\\
  \includegraphics[width=.224\linewidth]{mtekliKoltuk}
  &\includegraphics[width=.375\linewidth]{tekliKoltuk1}
  &\includegraphics[width=.375\linewidth]{tekliKoltuk2}\\
  \mbox{(d)}
  &\mbox{(e)}
  &\mbox{(f)}\\
  \includegraphics[width=.224\linewidth]{mikiliKoltuk}
  &\includegraphics[width=.375\linewidth]{ikiliKoltuk1}
  &\includegraphics[width=.375\linewidth]{ikiliKoltuk2}\\
  \mbox{(g)}
  &\mbox{(h)}
  &\mbox{(k)}
  \end{tabular}
  \caption{\label{fig:arobjects}%
           (b), (c), (e), (f), (h), and (k) were rendered by using our marker-based AR application. (a) were used to render (b) and (c). (d) were used to render (e) and (f). (g) were used to render (h) and (k). (b) and (c) are from a table object. (e) and (f) are from a single seat object. (h) and (k) are from a double seat object. We also assigned some material properties and textures to the objects by using OpenGL library. 
           }
% \vspace{-0.5cm}
\end{figure*} 
%%%%
%%%%Figure 5
%%%%
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{finalResultTriple}
  \caption{ \label{fig:resulttriple}
          Multiple objects can be rendered by our mobile AR technique.
           }
%\vspace{-0.5cm}
\end{figure*}

\section{Conclusions and Future Works}

In this study, we presented Mul2MAR, which is a mobile marker-based AR technique that works with red-cyan glasses. We validated our AR technique by using three different furniture objects. We assigned some material properties and textures to the objects by using OpenGL library functions. We showed that our AR system can render multiple 3D objects visually plausibly. Three different markers and the red-cyan glasses were used for this system and the results obtained are shown in Figure~\ref{fig:arobjects} and Figure~\ref{fig:resulttriple}.

Red-cyan glasses were preferred because they provide inexpensive solution. Even though this type of glasses can not create the most effective 3D images, they are preferred because of wide availability for end users and they are suitable to use with mobile devices. A simple red-cyan masking technique that was used to view objects was implemented, and with a slight adjustment in the application it can be even turned on and off at will.

There are two downsides using red-cyan glasses. First one is, prolonged use of glasses may cause temporary colour impairment on each eye. Second one, people who use prescribed glasses may not be able to use 3D glasses easily.

In the future, we would like to improve our mobile AR technique by investigating fast and accurate QR code algorithms. We believe that a fast and accurate QR code algorithm will make our mobile AR technique be more stable, faster and more accurate. Additionally, there is potential for upgrading and optimizing our Mul2MAR by incorporating state of the art deep learning methods and
techniques~\cite{Gok2023SIU, Azadvatan2024arXiv, Akdogan2024arXiv, Jabbarli2024arXiv}.

To improve visual perception of our Mul2MAR, we are also interested in implementing realistic Bidirectional Reflectance Distribution Function (BRDF)~\cite{Ozturk2006EGUK, Kurt2007MScThesis, Ozturk2008CG, Kurt2008SIGGRAPHCG, Kurt2009SIGGRAPHCG, Kurt2010SIGGRAPHCG, Ozturk2010GraphiCon, Szecsi2010SCCG, Ozturk2010CGF, Bigili2011CGF, Bilgili2012SCCG, Ergun2012SCCG, Toral2014SIU, Tongbuasirilai2017ICCVW, Kurt2019DEU, Tongbuasirilai2020TVC, Akleman2024arXiv}, Bidirectional Scattering Distribution Function (BSDF)~\cite{WKB12, Ward2014MAM, Kurt2014WLRS, Kurt2016SIGGRAPH, Kurt2017MAM, Kurt2018DEU}, Bidirectional Surface Scattering Reflectance Distribution Function (BSSRDF)~\cite{Kurt2013TPCG, Kurt2013EGSR, Kurt2014PhDThesis, Onel2019PL, Kurt2020MAM, Kurt2021TVC, Yildirim2024arXiv} and multi-layered material~\cite{WKB12, Kurt2016SIGGRAPH, Mir2022DEU} models into our multi-marker mobile AR application.
% conference papers do not normally have an appendix

% use section* for acknowledgment
\section*{Acknowledgment}

The author would like to thank Pelin Karag\"{o}zo\u{g}lu, B\"{u}\c{s}ra Temu\c{c}in, Burcu Daylar and Cengiz G\"{u}ng\"{o}r for their valuable comments and proofreading the paper.

% DO NOT INCLUDE ACKNOWLEDGMENTS IN AN ANONYMOUS SUBMISSION TO SIGGRAPH 2019
%\begin{acks}
%
%The authors would like to thank Dr. Maura Turolla of Telecom
%Italia for providing specifications about the application scenario.
%
%The work is supported by the \grantsponsor{GS501100001809}{National
%  Natural Science Foundation of
%  China}{http://dx.doi.org/10.13039/501100001809} under Grant
%No.:~\grantnum{GS501100001809}{61273304\_a}
%and~\grantnum[http://www.nnsf.cn/youngscientists]{GS501100001809}{Young
%  Scientists' Support Program}.
%
%
%\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{arXiv25_Mul2MAR_References}

