\section{Model Learning}
\label{sec:5}
In this section, we first focus on the trajectory-road representation alignment module in Sec~\ref{sec:5.1}. Next, we describe the reward evaluation in Sec.~\ref{sec:5.2}. Finally, in Sec.~\ref{sec:5.3}, we provide the details of the entire training process of the model and complexity analysis.
\subsection{Trajectory-Road Representation Alignment}
\label{sec:5.1}
The core challenge of map matching lies in accurately aligning trajectories to the corresponding road network. To address this, we design a trajectory-road representation alignment module that fundamentally enhances the ability of our model to discern and align trajectory data with road segments.

% Based on the effective trajectory and road representations detailed in Sec.~\ref{sec:4.1}, we first obtain representations for trajectory requiring matching and its corresponding candidate road segments. Specifically, we use the trajectory representation $z_{\mathcal{T}_i}^t \in \mathbb{R}^{k \times d_t}$  of trajectory $\mathcal{T}_i$ generated by the trajectory encoding module as anchor $\mathcal{A}_{i} = z_{\mathcal{T}_i}^t$, and the embeddings for candidates $E_i^C \in \mathbb{R}^{k \times n_c \times d_r}$ as sample set. Among the sample set, we select the representation for ground truth as positive samples $\mathcal{P}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \in y_{i}\} \in \mathbb{R}^{k \times d_r}$ and the representation for remaining parts as negative samples $\mathcal{N}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \notin y_{i}\} \in \mathbb{R}^{k \times (n_c-1) \times d_r}$, where $z_{u_j}^r$ denotes the representation of candidate road segment $u_j$ generated by the road encoding module, and $y_{i}$ denotes the ground truth road segments for trajectory $\mathcal{T}_i$.

Based on the effective trajectory and road representations detailed in Sec.~\ref{sec:4.1}, we first obtain representations for trajectory requiring matching and its corresponding candidate road segments. Specifically, we use the trajectory representation $z_{\mathcal{T}_i}^t$ for trajectory $\mathcal{T}_i$ as anchor $\mathcal{A}_{i} \in \mathbb{R}^{k \times d_t}$, and the embeddings for candidates $E_i^C \in \mathbb{R}^{k \times n_c \times d_r}$ as sample set. Among the sample set, we select the representation for ground truth as positive samples $\mathcal{P}_{i} \in \mathbb{R}^{k \times d_r}$ and the representation for remaining parts as negative samples $\mathcal{N}_{i} \in \mathbb{R}^{k \times (n_c-1) \times d_r}$. The process can be formulated as follows:
% \begin{equation}
%     \mathcal{A}_{i} = z_{\mathcal{T}_i}^t
% \end{equation}
% \begin{equation}
%     \mathcal{P}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \in y_{i}\}
% \end{equation}
% \begin{equation}
%     \mathcal{N}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \notin y_{i}\}
% \end{equation}
\begin{equation}
    \{\mathcal{A}_{i}, \mathcal{P}_{i}, \mathcal{N}_{i}\} = \left\{
    \begin{aligned}
        &\mathcal{A}_{i} = z_{\mathcal{T}_i}^t \\
        &\mathcal{P}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \in y_{i}\} \\
        &\mathcal{N}_{i} = \{z_{u_j}^r \in E_i^C \mid u_j \notin y_{i}\}
    \end{aligned}
    \right.
\end{equation}
where $z_{\mathcal{T}_i}^t \in \mathbb{R}^{k \times d_t}$ is the representation for trajectory $\mathcal{T}_i$ generated by the trajectory encoding module, $z_{u_j}^r \in \mathbb{R}^{d_r}$ denotes the representation of candidate road segment $u_j$ generated by the road encoding module, and $y_{i}$ denotes the ground truth road segments for trajectory $\mathcal{T}_i$.

Then, we introduce the InfoNCE loss~\cite{infonce} as an unsupervised alignment loss $\mathcal{L}_a$ to bring the anchor and positive samples closer together while distancing them from negative samples.
\begin{equation}
\mathcal{L}_a = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp\left(\frac{\mathcal{A}_i^\top \mathcal{P}_i}{\tau}\right)}{\exp\left(\frac{\mathcal{A}_i^\top \mathcal{P}_i}{\tau}\right) + \sum_{\mathbf{z}_{u_j}^r \in \mathcal{N}_i} \exp\left(\frac{\mathcal{A}_i^\top \mathbf{z}_{u_j}^r}{\tau}\right)}
\end{equation}
where $N$ is the number of anchor-sample pairs, and $\tau$ scales the softmax function. By doing so, the robustness of the representations for trajectories and roads is enhanced, enabling effective integration to achieve better matching performance.

\subsection{Reward Evaluation}
\label{sec:5.2}
After the score generation for each candidate road segment in Sec.~\ref{sec:4.3}, we get the scores $O_i \in \mathbb{R}^{k \times n_c}$ for $k$ time step of state $s_i$. Then, we select the maximum value in $O_i$ as the matching result for the current state $s_i$, which also serves as the action $a_i$:
\begin{equation}
    a_i^{(n)} = u_{k^*}, \text{ where } k^* = \arg \max_{j \in \{1, 2, \ldots, n_c\}} O_j^{(n)}
\end{equation}
where $a_i^{(n)}$ denotes the time step $n$ of action for state $i$, $\arg \max$ denotes the argument of the maximum, which is used to find the index $j$ that maximizes the value of $O_j^{(n)}$.

Upon obtaining the action $a_i$ corresponding to the current state $s_i$, in alignment with the OMDP modeling, we devise a reward evaluation mechanism to ascertain the reward. Specifically, we consider the reward from four aspects: \textit{Accuracy}, \textit{Consecutive Success}, \textit{Detour Penalty}, and \textit{Road Connectivity}.

\textbf{Accuracy.} The primary goal of map matching is to align the observed trajectory accurately with the corresponding road segments. The matching accuracy reward is designed to directly incentivize the model to select road segments that best fit the GPS points, which is formulated as follows:
\begin{equation}
   r_{ac} = \begin{cases} 
    1 & \text{if } a_i^{(n)} = y_{s_i}^{(n)} \\
    -1 & \text{otherwise}
\end{cases}
\end{equation}
where $a_i^{(n)}$ denotes the action of time step $n$, and $y_{s_i}^{(n)}$ is the ground truth road segments for state $s_i$ at time step $n$.

\textbf{Consecutive Success.} In real-world scenarios, trajectory points are collected sequentially as a vehicle moves. Consecutive success reward is given for consecutive successful matches along a trajectory, which motivates the model to not only focus on individual, isolated matchings but also to ensure that these matchings are logically consistent over time. This reward helps model to maintain continuity in the sequence of matched road segments, thereby improving overall accuracy from a future-oriented perspective.
\begin{equation}
    r_{cs} = \alpha \cdot \mathbb{I}(a_i^{(n)}= y_{s_i}^{(n)} \text{ and } m \geq \theta)
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function, which takes a value of 1 if the condition is satisfied and 0 otherwise. $m$ denotes the current number of consecutive success, $\theta$ is the predefined threshold, and $\alpha$ is the reward value.

\textbf{Detour Penalty.} Unnecessary detours are a significant issue in map matching, often caused by GPS errors where consecutive trajectory points erroneously match upstream of the previous point~\cite{mmsurvey,LHMM,DMM,AMM}. This issue severely affects the accuracy of matches, leading to abnormal and circuitous results in the matched routes. Motivated by this, the detour penalty is implemented to discourage the model from choosing circuitous or uncharacteristically long paths that deviate significantly from the most direct route between consecutive GPS points.
\begin{equation}
    r_{dp} = -\beta \cdot \mathbb{I}(a_i^{(n)} \neq y_{s_i}^{(n)} \text{ and } a_i^{(n)} \in H)
\end{equation}

where $H$ is a historical matching queue that stores recent matching results, and $\beta$ is the penalty value.

\textbf{Road Connectivity.} Ensuring that the selected road segments are interconnected is crucial for generating viable routes in map matching. A road connectivity reward is designed to promote the selection of navigable paths and help the model understand that accurate matching is constrained by the connectivity between roads. We specifically consider the shortest path distance between the two road segments within the link connection graph $G_R$ as the degree of connectivity. The rationale behind this is that a shorter shortest path distance signifies a stronger connection between the segments.
\begin{equation}
    r_{rc} = \gamma \cdot \frac{1}{\delta(a_i^{(n)}, a_i^{(n-1)})}
\end{equation}
where $\delta(\cdot)$ represents the degree of connectivity of current road and previous road, and $\gamma$ is the reward value. Note that in this paper, we use a fixed road network, meaning we do not consider dynamic changes such as temporary road closures, which is a common practice in existing works~\cite{mtrajrec,l2mm,graphmm, AMM}. However, we can address this issue to some extent by introducing a dynamic connectivity discrimination function $\delta^d(\cdot)$. This is difficult for other existing methods, as they do not explicitly account for the impact of road connectivity on matching results, making it hard for them to extend this capability.

Finally, we can evaluate the overall reward as follows:
\begin{equation}
    r = r_{ac} + r_{cs} + r_{dp} + r_{rc}
\end{equation}

\begin{figure}[!t]
\begin{algorithm}[H]
    \small % 设置字体大小
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{training inputs $\mathbf{T}, \mathbf{C}, \mathbf{y}$, network module $\mathcal{M}_t$, $\mathcal{M}_r$, $\mathcal{M}_s$, trajectory transition graph $G_T$, link connection graph $G_R$, experience buffer $\mathcal{B}$, matching steps $k$, learning rate $lr$, training epochs $ep$, batch size $bs$, loss weight $\lambda$, update interval $t$, discount factor $\gamma$.}
    \Output{parameters $\theta_t$, $\theta_r$, $\theta_s$, for the three parts $\mathcal{M}_t$, $\mathcal{M}_r$, $\mathcal{M}_s$.}
    \caption{Model Learning for \textbf{\modelName}}
    \label{alg:ml}
    initialize $\theta_t, \theta_r, \theta_s$ with normal distribution\;
    \For{$m \gets 1 \ldots ep$}{
        iterations $I = \lfloor \frac{|\mathbf{T}|}{bs} \rfloor$\;
        \For{$n \gets 1 \ldots I$}{
            fetch the batch $\{\mathcal{T}, C, y\}$ from \{$\mathbf{T}, \mathbf{C}, \mathbf{y}$\}\;
            inference times $IT = \lfloor \frac{|\mathcal{T}|}{k} \rfloor$\;
            \For{$i \gets 1 \ldots IT$}{
                $s_i \gets \{\mathcal{T}_i, U_i, H_i^t, H_i^r, C_i\}$\;
                $a_i, r_i \gets ExperienceInference(s_i, G_T, G_R)$\;
                $s_{i+1} \gets \{\mathcal{T}_{i+1}, U_{i+1}, H_{i+1}^t, H_{i+1}^r, C_{i+1}\}$\;
                construct transition $T_i = \{s_i, a_i, r_i, s_{i+1}\}$\;
                store $T_i$ into $\mathcal{B}$ based on First In First Out\;
            }
            $\theta_{t}, \theta_{r}, \theta_{s} \gets ModelTrain(\mathcal{B}, G_T, G_R, lr, \lambda, \gamma)$\;
            update the main network with $\theta_{t}, \theta_{r}, \theta_{s}$\;
            \If{$n\mod t = 0$}{
                using parameters of the main network to update the target network\;
            } % 完整的条件表达式
        }
    }
\end{algorithm}

\begin{algorithm}[H]
    \small % 设置字体大小
    \SetAlgorithmName{Function}{function}{List of Functions}
    \renewcommand{\thealgocf}{}  % 添加这行来取消编号
    \SetKwInOut{Input}{Input}
    % \SetKwInOut{Output}{Output}
    \Input{current state $s_i$, trajectory transition graph $G_T$, link connection graph $G_R$.}
    % \Output{action $a_i$, reward $r_i$.}
    \caption{ExperienceInference}
    \label{alg:ei}
    % Function steps...
    $\mathcal{T}_i, U_i, H_i^t, H_i^r, C_i \gets s_i$\;
    $H_{i+1}^t, E_i^t \gets \mathcal{M}_t(\mathcal{T}_i, H_i^t, G_T)$\;
    $H_{i+1}^r, E_i^r \gets \mathcal{M}_r(U_i, H_i^r, G_R)$\;
    $E_i^C \gets \mathcal{M}_r(C_i, G_R)$\;
    $O_i \gets \mathcal{M}_s(E_i^t, E_i^r, E_i^C)$\;
    $a_i \gets$ using Equation 12 with $O_i$\;
    $r_i \gets$ using Equations 13-17 with $a_i$ and $s_i$\;
    return $a_i, r_i$\;
\end{algorithm}

\begin{algorithm}[H]
    \small % 设置字体大小
    \SetAlgorithmName{Function}{function}{List of Functions}
    \renewcommand{\thealgocf}{}  % 添加这行来取消编号
    \SetKwInOut{Input}{Input}
    % \SetKwInOut{Output}{Output}
    \Input{experience buffer $\mathcal{B}$, trajectory transition graph $G_T$, link connection graph $G_R$, learning rate $lr$, loss weight $\lambda$, discount factor $\gamma$.}
    % \Output{parameters $\theta_t, \theta_r, \theta_s$.}
    \caption{ModelTrain}
    random sample transitions $T$ from $\mathcal{B}$\;
    training iterations $TI = |T|$\;
    % Function steps...
    \For{$i \gets 1 \ldots TI$}{
        % Function logic...
        $s_i, a_i, r_i, s_{i+1} \gets T_i$\;
        $q \gets Q(s_i, a_i; \theta_{main})$\;
        $a_{i+1}^* \gets \arg\max_{a_{i+1}} Q(s_{i+1}, a_{i+1}; \theta_{main})$\;
        $q_{target} \gets r_i + \gamma \cdot Q(s_{i+1}, a_{i+1}^*; \theta_{target})$\;
        $\mathcal{A}_i, \mathcal{P}_i, \mathcal{N}_i \gets $ using Equation 10\;
        $\mathcal{L} \gets$ using Equations 11, 18-19 and $\lambda$\;
    }
    return $\theta_{t}, \theta_{r}, \theta_{s}$;
\end{algorithm}  
\vspace{-0.2in}
\end{figure}

\subsection{Model Training}
\label{sec:5.3}
To leverage the effective sequential decision-making capabilities of reinforcement learning, as shown in Algorithm~\ref{alg:ml}, we develop a novel model learning algorithm that enhances the model's adaptability and robustness across diverse scenarios. For each epoch, we first fetch a batch of trajectories, candidates and ground truths $\{\mathcal{T}, C, y\}$ from whole set of trajectories, candidates and ground truths $\{\mathbf{T}, \mathbf{C}, \mathbf{y}\}$ (line 5 of Algorithm~\ref{alg:ml}). Then, the systematic accumulation of transitions through \textit{Experience Inference} is conducted, which are then stored in an experience buffer (lines 7-12 of Algorithm~\ref{alg:ml}). Next, the model continuously samples from these stored transitions, enabling robust \textit{Model Train} that incorporates a wide range of experiences (lines 13-16 of Algorithm~\ref{alg:ml}). Through this alternating process, this learning approach enhances the model’s generalization ability while reducing the risk of overfitting, achieving high adaptability and robustness, making it suitable for real-world applications.
% This approach enhances generalization and reduces the risk of overfitting, offering an adaptive strategy for tackling the complexity of map matching with reinforcement learning.

\noindent \textbf{Experience Inference.} The process begins by using the model to encode features and generate the scores (lines 1-5 of Function: ExperienceInference), which are detailed in Sec~\ref{sec:4}. Then, we select the action $a_i$ and evaluate the reward for action $a_i$ under state $s_i$ to obtain the reward $r_i$ that encompasses multiple aspects (lines 6-7 of Function: ExperienceInference). Together with the next state $s_{i+1}$, we construct a transition $T_i = \{s_i, a_i, r_i, s_{i+1}\}$ as a piece of experience and store it in the experience buffer $\mathcal{B}$. We refer to this process as the \textit{experience inference} process, and execute it multiple times to accumulate a sufficient amount of transitions.

\noindent \textbf{Model Train.} After a certain number of experience inferences, the model randomly samples a batch of transitions from the experience buffer $\mathcal{B}$ for experience replay. Specifically, we employ a deep Q-learning method, which learns the value of an action in a particular state, intending to maximize the total reward received~\cite{dqn}. Moreover, to reduce the overestimation of action values that often occurs in standard Q-learning, we apply Double DQN~\cite{double_dqn} to increase the stability of our training. In particular, we use two networks with identical structure that are composed of $\mathcal{M}_t$, $\mathcal{M}_r$, and $\mathcal{M}_s$, namely the main network and the target network. Note that we only train the main network, and use its parameters to update the target network periodically (lines 15-16 of Algorithm~\ref{alg:ml}). We first use the main network to estimate the q-value $q$ (line 5 of Function: ModelTrain). Then, we use the main network to select the action $a_{i+1}^*$ with the max q-value (line 6 of Function: ModelTrain). Next, we use the target network to estimate the Q-value of taking action $a_{i+1}^*$ under next state $s_{i+1}$, multiply it by the discount factor $\gamma$, and add it with reward $r_i$ to compute the target q-value $q_{target}$ (line 7 of Function: ModelTrain). Finally, we apply the Huber Loss~\cite{huber} as the Temporal Difference (TD) loss for Q-learning, which is formulated as follows:
\begin{equation}
    \mathcal{L}_{td} = 
    \begin{cases} 
    0.5 \times (q - q_{\text{target}})^2, & \text{if } |q - q_{\text{target}}| < 1 \\
    |q - q_{\text{target}}| - 0.5, & \text{otherwise}
    \end{cases}
\end{equation}
where $q$ is the estimated q-value and $q_{target}$ is the target q-value. Combined with the alignment loss $\mathcal{L}_a$ and corresponding weights $\lambda$, we can compute the overall learning objective as follows:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{td} + \lambda \cdot \mathcal{L}_a
\end{equation}

\input{tables/complexity}
\noindent \textbf{Complexity Analysis.} We compare \textbf{\modelName}~with \textbf{MDP-based}~\cite{MDPMM}, \textbf{DNN-based}~\cite{mtrajrec, l2mm, graphmm} offline methods, and \textbf{HMM-based}~\cite{HMM, FMM, AMM} online method in terms of complexity. We analyze the complexity of parameters and computation for the four types of methods, with the results shown in Table~\ref{tab:comp}. Specifically, we model the problem input as follows. The road network contains $n_R$ road segments. The number of trajectories is $n_T$, the longest trajectory length is $l_T$, and each time interval has $a$ new trajectory points. For each trajectory point, the maximum candidates road segments is $c$. The dimension of all hidden representations is $d$.

For \textbf{MDP} methods, the parameters mainly consist of transition probabilities for each state-action pair, which has the complexity of $O(n_R \times c)$. The computational complexity primarily arises from calculating the value function for each state-action pair during the iterative process (e.g., value iteration). Assuming the number of iterations required for convergence is $K$, the time complexity for a single computation is $O(K \times n_R \times c)$. In the online scenario, the method needs to recompute the MDP multiple times, with the trajectory length increasing each time. The cumulative computational complexity becomes $O( n_T\times \frac{l_T}{a} \times K \times n_R \times c)$, where $\frac{l_T}{a}$ represents the number of calculations for each trajectory.
For \textbf{DNN} methods, the FC layer and the road segment embedding layer have $O(n_R \times d)$ parameters, where $n_R >> d$, making these parameters significantly greater than the $O(d^2)$ parameters of other layers. This results in an overall parameter complexity of $O(n_R \times d)$. In the online scenario, the model needs to recompute over increasingly longer trajectories, leading to an accumulated computational complexity of $O(n_T\times \frac{l_T^2}{2a}\times n_R \times d)$, where the term $\frac{l_T^2}{2a}$ comes from summing the lengths of all steps for a trajectory, which forms an arithmetic series.
For \textbf{HMM} methods, the parameter complexity is $O(n_R \times c)$, which comes from emission probabilities and transition probabilities between states. Assuming the maximum number of transitions per candidate is $M_{\max}$, at each time step, HMM methods can perform incremental online matching, with a total time complexity of $O(n_T\times l_T \times c \times M_{\max})$. 
For our \textbf{\modelName}~method, it does not require embedding encoding of $n_R$ road segments, therefore the parameter complexity is $O(d^2)$. Since our method efficiently retains historical information and can match $a$ trajectory points at a time, the time complexity is reduced to $O(n_T \times \frac{l_T}{a} \times d^2)$.

\textbf{\textit{Remark:}} The complexity of \textbf{MDP} and \textbf{DNN} method has the large term $n_R$, $K$ and the quadratic term $\frac{l_T^2}{2a}$, making them inefficient because they redundantly process the entire trajectory. In contrast, in our method, $d$ is much smaller than $n_R$, and $\frac{l_T}{a}$ achieves optimal complexity with respect to the trajectory length term. Additionally, since $M_{\max}$ is relatively small compared to $n_R$, the complexity of \textbf{HMM} methods is relatively acceptable. However, it's term $l_T$ still exceeds our method’s  $\frac{l_T}{a}$.
Therefore, our \textbf{\modelName} achieves superior efficiency by eliminating redundant computations, making it well-suited for real-world applications.