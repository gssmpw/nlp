\section{Framework}
\label{sec:3}
\begin{figure}
  \centering  
  \includegraphics[width=0.7\linewidth]{figures/overview.pdf}
  \vspace{-0.15in}
  \caption{The architecture of \modelName, which consists of two parts: Online Inference and Offline Training. }
  \vspace{-0.1in}
  \label{fig:overview}
\end{figure}

In this section, we outline the framework of our proposed \textbf{\modelName}. As shown in Fig.~\ref{fig:overview}, \textbf{\modelName} contains two stages: the online inference and the offline training.

\noindent \underline{\textit{\textbf{Online Inference.}}} At this stage, we first extract real-time information and historical information to form \textit{State}. Next, we design feature encoding modules to generate effective representations for trajectories and roads, and generate score for each candidate road segment to guide the matching decisions.

\textbf{1. Extracting State:} For a given time step $i$, we extract real-time information from the online map matching scenario, and preserve historical information from previous matches to construct state $s_i$. Specifically, the \textit{previously matched road segments} $U_i$, \textit{current trajectory points} $\mathcal{T}_i$, and \textit{candidate road segments} $C_i$ are incorporate as real-time information since they are basic elements of map matching. Moreover, to maximize the efficiency of information transfer during the online matching process and to prevent redundant computations, we retain historical information by preserving the hidden states from the encoding module of the preceding step, which contains \textit{historical information} of road $H_i^r$ and trajectory $H_i^t$. 

\textbf{2. Encoding Feature:} After the construction of the state $s_i$, we design the trajectory and road encoding module to obtain effective encodings of the corresponding information within the state. In the trajectory encoding module, we first construct the trajectory transition graph $G_T$ based on the set of trajectories, aiming to capture the inter-trajectory correlations to obtain informative trajectory representations. Subsequently, to bridge the interaction between representations of road segments and trajectory, we incorporate the representations of the corresponding road segment for the \textit{current trajectory point} $\mathcal{T}_i$ and apply a graph neural network to derive the trajectory representation. This is then combined with the historical information of trajectory $H_i^t$ to generate the trajectory encoding $E_i^t$ for time step $i$. In the design of the road encoding module, we initially employ the road network to construct the link connection graph $G_R$. Subsequently, analogous to the trajectory encoding module, we deploy a graph neural network to generate representations for road segments, and in conjunction with the historical information of road $H_i^r$, we obtain the road encoding $E_i^r$. Moreover, as mentioned before, the corresponding road segment representation for the \textit{current trajectory point} $\mathcal{T}_i$ is transmitted to the trajectory encoding module. Finally, the hidden states $H_{i+1}^r$ and $H_{i+1}^t$ produced by the encoding module are retained as the new historical information for the next time step $i+1$.

\textbf{3. Score Generation:} In this step, we first employ the graph neural network within the road encoding module to encode the candidate road segments $C_i$, generating candidates encoding $E_i^C$. Then, we concatenate the trajectory encoding $E_i^t$ and the road encoding $E_i^r$ to obtain fusion encoding $E_i^F$, and together with the candidate encoding $E_i^C$, we generate attention scores and select the maximum value as the matching decision action $a_i$.


\noindent \underline{\textit{\textbf{Offline Training.}}} At this stage, we first prepare the training data through multiple inferences and reward evaluations, saving it to the experience buffer. Next, we replay the experience through the feature encoding module to prepare for loss calculation. Finally, we compute different losses from the perspectives of temporal difference and contrastive learning.

 % We perform multiple inferences for various online map matching scenarios during the \textit{Online Inference} stage.
\textbf{1. Training Data Preparation:} Given the action $a_i$ from the online inference matching decision and state $s_i$, we devise a meticulously reward evaluation function to generate reward $R(a_i,s_i)$. 
\textcolor{black}{Notably, the reward evaluation function thoroughly considers the complexity of online matching scenarios, performing a comprehensive assessment from a future-oriented perspective, which helps to minimize incorrect matches right from the start.}
After taking action $a_i$, we can obtain the next state $s_{i+1}$, which along with the state $s_i$, action $a_i$, and reward $R(a_i,s_i)$, are stored as a transition $T_i$ in the experience buffer. We repeat the aforementioned steps multiple times to prepare a collection of training data. This process will be conducted repeatedly to facilitate the model's continuous enhancement and refinement across various scenarios.

\textbf{2. Experience Replay:} After a certain number of online inferences and transition storages, we randomly select a batch of transitions from the experience buffer and replay them for training purposes. Specifically, these transitions are processed through our feature encoding module and score generation component, supplemented by double DQN techniques to prepare for loss calculation while reducing score overestimation.

\textbf{3. Computing Losses:} After the experience replay, we employ reinforcement learning strategies grounded in Temporal Difference (TD) learning principles. By leveraging the Temporal Difference (TD) Loss, the model can enhance the capability to estimate the q-value associated with action $a_i$ given the state $s_i$. Moreover, considering that the map matching problem necessitates effective encoding of both trajectories and road segments to integrate their information for accurate matching, we design a trajectory-road representation alignment module to align the representations in latent space. In particular, we select the representations of trajectory points as anchors, and take the representations of candidate road segments as positive and negative samples. Within the candidates, the ground truth and the remaining parts are considered as positive and negative, respectively. Finally, the contrastive loss is used together with the TD loss to train our model.



\iffalse
In $\mathcal{M}_t$, we first construct the trajectory transition graph $G_T$ based on the set of trajectories, aiming to capture the inter-trajectory correlations to obtain effective trajectory representations. Then, for any $k$ new trajectory points $\mathcal{T}_{i:i+k-1}$, we apply Graph Convolutional Network (GCN) to derive trajectory representations $Z_{i:i+k-1}^t$. Next, by integrating the hidden state of the historical trajectory $h_{i-1}^t$, a RNN module is employed to capture the sequence correlation of trajectory representations, obtaining the trajectory embeddings $E_{i:i+k-1}^t$.

When designing the road encoding module $\mathcal{M}_r$, we first utilize road network to construct link connection graph $G_R$. Then, similar to the trajectory encoding module $\mathcal{M}_t$, we apply Graph Isomorphism Network (GIN) to generate representations $Z_{i-k:i-1}^r$ for road segments $U_{i-k:i-1}$. Subsequently, a RNN module is also used to derive road embeddings $E_{i-k:i-1}^r$ with the hidden state of the historical road $h_{i-k-1}^r$.

To evaluate the Q-value of agent actions for reinforcement learning, we design the module $\mathcal{M}_q$. For each trajectory point of $\mathcal{T}_{i:i+k-1}$, we select $N$ candidate road segments based on the spatial distance, and utilize GIN in $\mathcal{M}_r$ to derive their representations, denoted as $C_{i:i+k-1}$. Then, we concatenate the trajectory embeddings $E_{i:i+k-1}^t$ and road embeddings $E_{i-k:i-1}^r$ to fusion them as $E_{i:i+k-1}^F$. To acquire the Q-values $Q_{i:i+k-1}$ of each candidate road segment, we compute the similarity between fusion embedding $E_{i:i+k-1}^F$ and candidate road segments $C_{i:i+k-1}$ by utilizing the attention mechanism.

The map matching problem necessitates effective encoding of both trajectories and road segments to integrate their information for accurate matching. Therefore, aligning the representations of trajectories and road segments in latent space is crucial. With this consideration, we design the trajectory-road representation alignment module $\mathcal{M}_a$. Specifically, we select the representations of trajectory points in $\mathcal{M}_t$ as anchors $Z_{i:i+k-1}^a$, and take the representations of candidate road segments in $\mathcal{M}_r$ as positive samples $C_{i:i+k-1}^p$ and negative samples $C_{i:i+k-1}^n$. Within the candidates, the ground truth and the remaining parts are considered as positive and negative, respectively. Finally, a contrastive loss is used to align the representations.

We model the online map matching problem as a Markov Decision Process, upon which we base the design of the reinforcement learning module $\mathcal{M}_l$. First, the agent select the maximum value of the Q-values $Q_{i:i+k-1}$ generated in $\mathcal{M}_q$ as actions $a_{i:i+k-1}$. Then, a meticulously designed reward evaluation module assesses the actions chosen by the agent by comprehensively considering factors such as accuracy, success streak, detours, and connectivity. Next, we store the agent's experiences in memory, and randomly samples from memory to replay later for training. Specifically, a double DQN structure of main network and target network is used to address the overestimation bias of action values. This process fosters a more stable learning environment by mitigating the risk of overfitting, which is particularly crucial in dynamic environments like map matching where the context and optimal decisions can vary significantly over time and space.
\fi
