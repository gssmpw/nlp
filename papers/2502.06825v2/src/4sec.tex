\section{Feature Encoding}
\label{sec:4}
As shown in Fig.~\ref{fig:sec4}, we design road encoding module $\mathcal{M}_r$, trajectory encoding module $\mathcal{M}_t$ and score generation module $\mathcal{M}_s$. In particular, we first elaborate on the details of deriving informative representations for trajectories and roads in Sec.~\ref{sec:4.1}, which is based on graph structures tailored to trajectories and roads. Then, we focus on efficiently capturing the sequence correlation in online map matching scenarios in Sec.~\ref{sec:4.2}. At last, we explain how to generate the score of each candidate in Sec.~\ref{sec:4.3}.

\begin{figure}
  \centering  
  \includegraphics[width=0.7\linewidth]{figures/sec4.pdf}
  \vspace{-0.1in}
  \caption{The framework of feature encoding.}
  \vspace{-0.15in}
  \label{fig:sec4}
\end{figure}

\subsection{Trajectory \& Road Representation}
\label{sec:4.1}
\noindent \textbf{Link Connection Graph Convolution.} The representation learning of road segments is crucial in the map matching problem. Considering that it can be naturally presented as a graph structure (i.e., the link connection graph $G_R$), we obtain its representation through a graph neural network. Specifically, considering the complex topological structure of road networks, motivated by~\cite{graphmm}, we adopt the Graph Isomorphism Network (GIN)~\cite{GIN}. GIN has the optimal ability to distinguish various graph structures, thus accurately representing the nuances of road networks. Its aggregation mechanism, which merges a node's features with those of its neighbors, is enhanced by a learnable parameter that precisely adjusts the balance between a node's own characteristics and its neighbors' influences. In our implementation, we adopt a lightweight version with a limited number of layers and a simple MLP, ensuring that this component is efficient for practical use. The update rule for feature $z_{u_j}^{(n)} \in \mathbb{R}^{d_r}$ of node $u_j \in V_R$ at the $n$-th layer can be formulated as follows:
\begin{equation}
    z_{u_j}^{(n)} = \text{MLP}^{(n)} ( (1 + \epsilon^{(n)}) \cdot z_{u_j}^{(n-1)} + \sum_{v \in \mathcal{N}(u_j)} z_v^{(n-1)})
\end{equation}
where $\text{MLP}^{(n)}$ denotes the multi-layer perceptron at the $n$-th layer, $\epsilon^{(n)}$ is a learnable parameter that adjusts the weight of a node's own features relative to its neighbors' features, $\mathcal{N}(u_j)$ represents the set of neighbor nodes of $u_j$, and $z_{u_j}^{(n-1)}$ and $z_{v}^{(n-1)}$ are the feature vectors of node $u_j$ and its neighbors, respectively. 
% For each road segment $u$, $z_{u}^{(0)}$ represents its initial features, which are the latitude and longitude coordinates of the segment's start and end points and their corresponding grid coordinates. 
For previously matched road segments $U_i=U_{j:j+k}=\langle u_j, u_{j+1}, \ldots, u_{j+k-1} \rangle$, we use the feature $\langle z_{u_j}^{(n)}, z_{u_{j+1}}^{(n)}, \ldots, z_{u_{j+k-1}}^{(n)} \rangle$ after $n$-th GIN layer as the representations $\langle z_{u_j}^{r}, z_{u_{j+1}}^{r}, \ldots, z_{u_{j+k-1}}^{r} \rangle$ of road segments.

\noindent \textbf{Trajectory Transition Graph Construction.} The map matching problem is inherently graph-natured because the road network is represented as a graph composed of nodes (links) and edges (connections). Therefore, representing trajectories in a graphical form can also unify the representation of the two types of data, thereby alleviating the impact of their heterogeneity. Motivated by this, we further design trajectory transition graph $G_T = (V_T, E_T)$ to model the trajectories as a graph to address the heterogeneity between roads and trajectories.  Specifically, for trajectory set $\{\mathcal{T}_1, \mathcal{T}_2, \ldots, \mathcal{T}_n\}$ and trajectory point $g$ mapped to the grid, we construct the node set $V_T$ and $E_T$, which is defined as follows:
\begin{equation}
    V_T = \cup_{i=1}^{n} \{ g \mid g \in \mathcal{T}_i \}
\end{equation}
% \vspace{-0.15in}
\begin{equation}
    E_T = \cup_{i=1}^{n} \{(g_j, g_{j+1}, w_{(g_j, g_{j+1})}) \mid g_j, g_{j+1} \in \mathcal{T}_i, g_j \neq g_{j+1} \}
\end{equation}
% \vspace{-0.1in}
\begin{equation}
    w_{(g_j, g_{j+1})} = \sum_{i=1}^{n} \sum_{g_j, g_{j+1} \in \mathcal{T}_i} \mathbb{I}(g_j \neq g_{j+1})
\end{equation}
where $w$ is the weight of each edge, $\mathbb{I}(\cdot)$ is the indicator function, which takes a value of 1 if $g_j$ is different from $g_{j+1}$, and 0 otherwise. By doing so, the trajectory transition graph $G_T$ is able to model the relationships between trajectories from a global perspective of the trajectory set, where the edges and weights in the graph explicitly indicate the degree of correlation between the grids.

\noindent \textit{\textbf{Remark.}} In terms of complexity, the trajectory transition graph can be constructed with time and space complexity linear to the number of points in the trajectory set, both of which can be represented as $O(\sum_{i=1}^{n}|\mathcal{T}_i|)$, where $|\mathcal{T}_i|$ denotes the number of points in $\mathcal{T}_i$. This endows the trajectory transition graph with good scalability, enabling our model to be rapidly deployed in practical application scenarios with large amounts of trajectory data, and to incrementally update the graph structure during service provision.

\noindent \textbf{Trajectory Transition Graph Convolution.}
To enhance the consistency between trajectory and road representations and bridge their interaction, benefiting from our adoption of the same grid representation method for both trajectories and roads, we select the representations of road segments that intersect with the corresponding grid of the trajectory point as its initial features. Specifically, we construct a road-to-trajectory mapping matrix $M$, which distributes the road representations $z_r$ to various nodes of the trajectory transition graph $G_T$ based on whether the roads intersect with the nodes. It can be formulated as follows:
\begin{equation}
    M_{ij} = 
  \begin{cases} 
   1 & \text{if } g_i \cap u_j \neq \emptyset, \\
   0 & \text{otherwise}.
  \end{cases}
\end{equation}
where $\cap$ denotes the intersection of grid node and road segment. Then, for gird $g_i \in V_T$, its initial representation $z^{(0)}_{g_i} \in \mathbb{R}^{d_t}$ is:
\begin{equation}
    z^{(0)}_{g_i} = \frac{\sum_{j=1}^{N_g} M_{ij} \cdot z^r_{u_j}}{\sum_{j=1}^{N_g} M_{ij}}
\end{equation}
where $N_g$ is the number of nodes in $G_T$, $z^r_{u_j}$ denotes the representation of road segments $u_j$ of the global graph representation $z_G^r$. By doing this, the initial representations of the nodes in the trajectory transition graph $G_T$ sufficiently incorporate the associated road information. This strategy for linking road and trajectory representations lays a robust groundwork for the subsequent derivation of effective trajectory representations.

By definition, all transitions from grid $g_i$ to grid $g_j$ are aggregated into a single edge $e_{ij}$, resulting in a relatively sparse Trajectory Transition Graph. Given this sparsity, Graph Convolutional Networks (GCNs) are a suitable choice for encoding the graph, as they are computationally efficient on sparse graphs~\cite{GCN}. Using GCNs in this context allows for efficient encoding of the trajectory while aligning well with the graphâ€™s structural characteristics. With the initial representations, we apply $n$ layers of Graph Convolutional Networks (GCN) to get the final representations. The update rule for feature $z_{g_i}^{(n)} \in \mathbb{R}^{d_t}$ of node $g_i \in V_T$ at the $n$-th layer can be formulated as follows:
\begin{equation}
    z_{g_i}^{(n)} = \sigma (\text{BN}^{(n)} (z_{g_i}^{(n-1)} \times {W}_a^{(n)} + \sum_{v \in \mathcal{N}(g_i)} \frac{1}{\sqrt{\hat{d}_{g_i} \hat{d}_v}} z_{v}^{(n-1)} \times {W}_b^{(n)} ))
\end{equation}
where $\sigma$ denotes the activation function ReLU, $\text{BN}^{(n)}$ denotes the batch normalization at the $n$-th layer, ${W}_a^{(n)} \in \mathbb{R}^{d_t \times d_t}$ and ${W}_b^{(n)} \in \mathbb{R}^{d_t \times d_t}$ are learnable weights, $\mathcal{N}(g_i)$ denotes the set of neighboring nodes of $g_i$, $\hat{d}_{g_i}$ and $\hat{d}_v$ are normalized degree terms for $g_i$ and $v$. For current
trajectory points $\mathcal{T}_i = \mathcal{T}_{j:j+k} = \langle g_j, g_{j+1}, \ldots, g_{j+k-1} \rangle$, we use the feature after $n$-th GCN layer as the representations $\langle z_{g_j}^{t}, z_{g_{j+1}}^{t}, \ldots, z_{g_{j+k-1}}^{t} \rangle$ of trajectory.

\subsection{Sequence Correlation Capture}
\label{sec:4.2}

In online map matching scenarios, we need to match newly generated trajectory points within a certain time interval. It's critical to take into account not only the historical information but also the intrinsic correlations present in the newly emerging sequence of trajectory points. In light of integrating them, a RNN model~\cite{RNN} is employed. Specifically, we leverage the sequential dependencies inherent in trajectory data by initializing the RNN with the hidden state derived from the previous trajectory points. This methodology facilitates a dynamic learning process, where the network progressively refines its understanding of the trajectory patterns, thereby improving the precision and efficiency of map matching process. 

For road representations $\langle z_{u_j}^{r}, z_{u_{j+1}}^{r}, \ldots, z_{u_{j+k-1}}^{r} \rangle$ and trajectory representations $\langle z_{g_j}^{t}, z_{g_{j+1}}^{t}, \ldots, z_{g_{j+k-1}}^{t} \rangle$, the update of hidden states and generation of output is denoted as follows:
\begin{equation}
h_{x_j} = \tanh(h_{x_{j-1}} \times {W}_{h} + b_{h} + z_{x_j} \times {W}_{z} + b_{z})
\end{equation}
\begin{equation}
e_{x_j} = h_{x_j} \times {W}_{e} + b_{e}
\end{equation}
% \begin{equation}
%     h_{u_j}^r = \tanh(h_{u_{j-1}}^r \times {W}_{h}^r + b_{h}^r + z_{u_{j}}^{r} \times {W}_{z}^r + b_{z}^r)
% \end{equation}
% \begin{equation}
%     h_{g_j}^t = \tanh(h_{g_{j-1}}^t \times {W}_{h}^t + b_{h}^t + z_{g_{j}}^{t} \times {W}_{z}^t + b_{z}^t)
% \end{equation}
% \begin{equation}
%     e_{u_j}^r = h_{u_j}^r \times {W}_{e}^r + b_e^r
% \end{equation}
% \begin{equation}
%     e_{g_j}^t = h_{g_j}^t \times {W}_{e}^t + b_e^t
% \end{equation}
where $x_j$ denotes road segment $u_j$ or current trajectory point $g_j$, $h_{x_j}$ and $e_{x_j}$ are hidden states and outputs, respectively. ${W}_h, {W}_z, {W}_e$ are the learnable weights, $b_h, b_z, b_e$ are the learnable bias. In particular, within our \modelName~framework's operation, $h_{u_{j-1}}^r$ and $h_{g_{j-1}}^t$ are included in the state $s_i$ of the online MDP we model, represented as historical information $H_i^r$ and $H_i^t$, respectively. Moreover, $h_{u_{j+k-1}}^r$ and $h_{g_{j+k-1}}^t$ are treated as the historical information $H_{i+1}^r$ and $H_{i+1}^t$ and saved for the next state $s_{i+1}$. After the sequence correlation capture of both trajectory and road segments, we get the trajectory embeddings $E_i^t = \langle e_{g_j}^t, e_{g_{j+1}}^t, \ldots, e_{g_{j+k-1}}^t \rangle$ and the road embeddings $E_i^r = \langle e_{u_j}^r, e_{u_{j+1}}^r, \ldots, e_{u_{j+k-1}}^r \rangle$ of state $i$.

\noindent \textit{\textbf{Remark.}} In online map matching, where immediate processing is essential, RNN~\cite{RNN} demonstrates advantages over GRU~\cite{GRU} and LSTM~\cite{LSTM} due to its simpler structure and faster computational capabilities. Additionally, compared to Transformer~\cite{transformer}, which lacks sequential hidden state transfer, RNN maintains a continuous context flow that aligns well with MDP modeling, where the current state encapsulates all necessary past information, reflecting the Markov property essential for sequential data processing. In this scenario, the relevance of long-term historical data to current matching is minimal, as distant trajectory information has little impact on immediate decisions. This context is congruent with RNN's operational strengths and mitigates their limitation of handling long-term dependencies~\cite{RNNsurvey}, making their efficient processing of short-term, relevant sequences particularly well-suited for the quick-paced, real-time requirements of online map matching.

\subsection{Score Generation}
\label{sec:4.3}
Based on considerations of consistency, for the candidate road segments $C_i \in \mathbb{R}^{k \times n_c}$ within state $i$, we use the GIN in the road encoding module to obtain their embeddings, where $k$ is the matching interval and $n_c$ is the number of candidates. Specifically, we select the representations of the corresponding road segments from the global graph representation $z_G^r$ as the candidates embedding $E_i^C \in \mathbb{R}^{k \times n_c \times d_r}$. Subsequently, we concatenate the trajectory embedding $E_i^t \in \mathbb{R}^{k \times d_t}$ and road embedding $E_i^r \in \mathbb{R}^{k \times d_r}$ of state $i$ to obtain a fused embedding $E_i^F \in \mathbb{R}^{k \times (d_r+d_t)}$.
% formulated as follows:
% \begin{equation}
%     E_i^F = \text{concat}(E_i^t, E_i^r)
% \end{equation}
Next, we apply an attention mechanism to calculate attention scores $O_i \in \mathbb{R}^{k \times n_c}$ based on the fused embedding $E_i^F$ and candidate road segments embedding $E_i^C$, serving as the scores for each candidate road segment. In particular, we transform $E_i^F$ and $E_i^C$ respectively and use them as the query $Q = \tanh(E_i^F \times W_F+ b_F)$ and key $K = \tanh(E_i^C \times W_C + b_C)$ in the attention mechanism to generate the score $O_i = Q \cdot K^\top$, where $W_F \in \mathbb{R}^{(d_r+d_t) \times d_a}$ and $W_C \in \mathbb{R}^{d_r \times d_a}$ are the learnable weights, $b_F \in \mathbb{R}^{d_a}$ and $b_C \in \mathbb{R}^{d_a}$ are the learnable bias.

  
