\section{RELATED WORK}
\label{sec:related work}
Motivated by integrating the power of deep networks with interpretable and theoretically grounded kernel methods, it is encouraging to see some contributions on such combinations across a range of contexts.

\paragraph{Variations of DKL.} 
Several recent studies have explored variations of DKL models. To handle large datasets and diverse tasks, \citet{wilson2016stochastic} extended the vanilla DKL \citep{wilson2016deep} to Stochastic Variational DKL (SV-DKL) by leveraging Stochastic Variational GP (SVGP) \citep{hensman2015scalable} and KISS-GP \citep{wilson2015kernel}. However, the kernel interpolation on a Cartesian grid does not scale in the high-dimensional space. \citet{xue2019deep} and \citet{xie2019deep} integrated deep kernel models with Random Fourier Features (RFF) \citep{rahimi2007random}, an efficient method for approximating GP kernel feature mappings. Some other work has focused on developing models with specialized kernel structures, such as recurrent kernels \citep{al2017learning} and compositional kernels \citep{sun2018differentiable}. More recently, %\citet{liu2021deep} introduced a latent-variable framework that integrates a stochastic encoding of inputs to enable regularized representation learning, while \citet{achituve2023guided} proposed a novel approach to training DKL by using an infinite-width NN to guide DKL optimization. 
\citet{ober2021promises} investigated overfitting in DKL models based on marginal likelihood maximization and proposed a fully Bayesian approach to mitigate this issue. \citet{matias2024amortized} introduced amortized variational DKL, which uses DNNs to learn variational distributions over inducing points in an amortized manner, thereby reducing overfitting by locally smoothing predictions. 
% \citet{harrison2024variational} introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks.

\paragraph{GPs and NNs.}
The connection between GPs and NNs was first established by \citet{neal1994bayesian}, who showed that the function defined by a single-layer NN with infinite width, random independent zero-mean weights, and biases is equivalent to a GP. This equivalence was later generalized to arbitrary NNs with infinite-width or infinite-depth layers in \citep{lee2017deep, cutajar2017random, matthews2018gaussian, yang2019wide, dutordoir2021deep, gao2023wide}. However, these studies focus primarily on fully connected NNs, which are not suitable for all practical applications. \citet{garriga2018deep} and \citet{novak2018bayesian} extended the equivalence to Convolutional Neural Networks (CNNs) \citep{lecun1989handwritten}, which are widely used in image recognition. Hybrid models combining GPs and NNs have also been investigated. \citet{bradshaw2017adversarial} proposed a hybrid GPDNN that feeds CNN features into a GP, while \citet{zhang2024gaussian} introduced GP Neural Additive Models (GP-NAM), a class of GAMs that use GPs approximated by RFF and a single-layer NN. However, GPDNN uses the standard GP, while we approximate GP via induced prior approximation. GP-NAM applies the additive model with RFF approximation but lacks Bayesian inference. \citet{harrison2024variational} introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. %In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks. The resulting GP layer has the form of a last-layer BNN which is easy to extend to various tasks and efficient in computing and inference.
% \textcolor{brown}{Tian: The last sentence may not be necessary, since we also highlight it in the remark.}