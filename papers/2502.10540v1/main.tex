\documentclass[twoside]{article}

\usepackage[accepted]{aistats2025}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add new packages here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{enumitem}
\usepackage{bm,dsfont}
\usepackage[T1]{fontenc}
\usepackage{xcolor,pifont}
\usepackage{graphicx}
\usepackage{caption, subcaption, float}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage[title]{appendix}
\usepackage{algorithm,algorithmic,setspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{color, colortbl, tcolorbox}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{ifthen}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add new commends here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\hatf}{\hat{f}}
\definecolor{Gray}{gray}{0.9}
% Define a custom variable: set to 1 to show, 0 to hide
\newcommand{\showcontent}{1}

% Remove the copyright box horizontal line
\makeatletter
\def\@copyrightspace{
\@float{copyrightbox}[b]
\begin{center}
\vspace{-1.3em}
\setlength{\unitlength}{1pc}
\begin{picture}(20,2.5)
\put(0,0){\parbox[b]{19.75pc}{\small \Notice@String}}
\end{picture}
\end{center}
\end@float}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% calligraphic letter for set and class
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}

% special set and measures
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\Ib}{\mathbb{I}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Zb}{\mathbb{Z}}

% vectors and matrices
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\fv}{\mathbf{f}}
\newcommand{\hv}{\mathbf{h}}
\newcommand{\gv}{\mathbf{g}}
\newcommand{\mv}{\mathbf{m}}

\newcommand{\Dv}{\mathbf{D}}
\newcommand{\Hv}{\mathbf{H}}
\newcommand{\Iv}{\mathbf{I}}
\newcommand{\Rv}{\mathbf{R}}
\newcommand{\Sv}{\mathbf{S}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Xv}{\mathbf{X}}
\newcommand{\Uv}{\mathbf{U}}
\newcommand{\Zv}{\mathbf{Z}}
\newcommand{\Lv}{\mathbf{L}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\Kv}{\mathbf{K}}
\newcommand{\Yv}{\mathbf{Y}}

% Random variables
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
% rm is already a command, just don't name any random variables m
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

% Random vectors
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

% Elements of random vectors
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

% Random matrices
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

% Elements of random matrices
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

% Vectors
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\newcommand{\argmax}{\arg\max}
\newcommand{\argmin}{\arg\min}


\newcommand{\dm}{\mathrm{d}}
\newcommand{\TV}{\text{TV}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\ERM}{\mathrm{ERM}}
\newcommand{\gen}{\mathrm{gen}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\SNR}{\mathrm{SNR}}
\newcommand{\numpy}{{\tt numpy}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}


\runningtitle{Deep Additive Kernel}
\runningauthor{Wenyuan Zhao, Haoyuan Chen, Tie Liu, Rui Tuo, Chao Tian}

\twocolumn[

\aistatstitle{From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation
}

\aistatsauthor{ Wenyuan Zhao$^*$ \And Haoyuan Chen$^*$ \And Tie Liu}
\aistatsaddress{Texas A\&M University \\ wyzhao@tamu.edu \And Texas A\&M University \\ chenhaoyuan2018@tamu.edu \And Texas A\&M University \\ tieliu@tamu.edu}

\aistatsauthor{Rui Tuo \And Chao Tian}
\aistatsaddress{Texas A\&M University \\ ruituo@tamu.edu \And Texas A\&M University \\ chao.tian@tamu.edu}

% \aistatsaddress{ $^1$Department of Electrical \& Computer Engineering, Texas A\&M University\\
% $^2$Department of Industrial \& Systems Engineering, Texas A\&M University}
]

\def\thefootnote{*}\footnotetext{The first two authors contributed equally.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
With the strengths of both deep learning and kernel methods like Gaussian Processes (GPs), Deep Kernel Learning (DKL) has gained considerable attention in recent years. From the computational perspective, however, DKL becomes challenging when the input dimension of the GP layer is high. To address this challenge, we propose the Deep Additive Kernel (DAK) model, which incorporates i) an additive structure for the last-layer GP; and ii) induced prior approximation for each GP unit. 
% \textcolor{red}{The ``last-layer GP'' comprises multiple additive GP units but in one GP layer, which is applied behind the feature extractor.} 
This naturally leads to a last-layer Bayesian neural network (BNN) architecture. The proposed method enjoys the interpretability of DKL as well as the computational advantages of BNN. Empirical results show that the proposed approach outperforms state-of-the-art DKL methods in both regression and classification tasks.
\end{abstract}


\section{INTRODUCTION}
%paragraph 1. introduce NN, GP, state the restrictions of GP, NN, and Deep GP, benefits of combining NN and GP, which is DKL: UQ and feature extractor

Deep Neural Networks (DNNs) \citep{lecun2015deep} are powerful tools capable of capturing intricate patterns in large datasets, and have demonstrated remarkable performance across a wide range of tasks. However, DNNs are prone to overfitting on small datasets, offer limited interpretability and transparency, and lack the ability to provide uncertainty estimation. On the other hand, as a traditional kernel-based method, Gaussian processes (GPs) \citep{williams2006gaussian} are robust against overfitting. In addition, they naturally incorporate uncertainty quantification and offer enhanced interpretability and adaptability for integrating prior knowledge. %through kernels. {\color{red}Tuo: But we choose the kernel not because of prior knowledge.}
% However, their practical applicability can be limited due to the need to specify a kernel function, which controls how inputs are related in the feature space. This specification often demands prior knowledge or extensive manual tuning, limiting the scalability and flexibility of GPs in handling complex, high-dimensional data. 
\citet{damianou2013deep} proposed Deep Gaussian Processes (DGPs) by stacking multiple layers of GPs, which introduces the hierarchical structure of deep learning with the probabilistic, non-parametric nature of GPs. Although the deep structure of DGPs allows them to learn features at different levels of abstraction, the tuning and optimization of DGPs, particularly for large datasets, can be difficult due to the layered structure, requiring careful consideration of hyperparameters and approximation techniques.

To learn rich hierarchical representations from the data with proper interpretability and uncertainty estimation, \citet{wilson2016deep} introduced Deep Kernel Learning (DKL) by incorporating DNN into the last layer kernel methods. This hybrid model enhances both flexibility and scalability compared to pure GPs, making it well-suited for a variety of real-world tasks, including regression, classification, and active learning, especially in scenarios where uncertainty quantification is critical.

%paragraph 2. limitations of DKL: computational complexity in GP layer, training complexity for ELBO? current literature to solve these issues? what's the limitation of current SOTA methods? Training with parameters of GP takes lots of time

One of the main challenges in DKL arises from the GP layer, which requires $\Oc(N^3)$ training time for $N$ data points, limiting its scalability for large datasets. Although DKL scales better than pure GPs as the feature dimension is reduced by the NN encoder, computing DKL exactly is still expensive, when the extracted features are high-dimensional, particularly in image (or video) datasets. To address this issue, several methods have been developed for GP approximation, including Random Fourier Features (RFF) \citep{rahimi2007random}, Stochastic Variational GP (SVGP) \citep{titsias2009variational,hensman2015scalable}, and Kernel Interpolation for Scalable Structured GP (KISS-GP) \citep{wilson2015kernel}, and these have been incorporated into DKL models \citep{wilson2016stochastic,xue2019deep,xie2019deep}. However, these sparse GP approximations via inducing points require $\mathcal{O}(M^3)$ time to compute the Evidence Lower Bound (ELBO), where $M$ is the number of inducing points. Therefore, DKL remains inefficient when a large number of inducing points are necessary for complex ML tasks.

Another challenge in DKL, as identified by \citet{ober2021promises}, is the tendency to overcorrelate features to minimize the complexity penalty term in the marginal likelihood. A fully Bayesian method incorporating Markov Chain Monte Carlo (MCMC) can effectively resolve this problem, but exhibit poor scalability with high-dimensional posterior distributions. In response, \citet{matias2024amortized} proposed Amortized Variational DKL (AV-DKL), which uses NN-based amortization networks to determine the inducing locations and variational parameters through input-dependent sparse GPs \citep{jafrasteh2021input}, thereby attenuating the overcorrelation of NN output features. 

%paragraph 3. To solve xxx issues, we consider expressing GP as a sparse BNN, advantages: efficient for training and inference, 

In this work, we propose the \textbf{D}eep \textbf{A}dditive \textbf{K}ernel (DAK) model, which embeds hierarchical features learned from NNs into additive GPs, and interpret the last-layer GP as a Bayesian neural network (BNN) layer \citep{mackay1992practical,harrison2024variational} with a sparse kernel activation via induced prior approximation on designed grids \citep{ding2024sparse}. The proposed methodology jointly trains the variational parameters of the last layer and the deterministic parameters of the feature extractor by maximizing the variational lower bound. This hybrid architecture enjoys the mathematical interpretability of DKL as well as the computational advantages of BNN, and 
% consisting of deterministic NNs followed by a sparse BNN, significantly enhances training and inference efficiency by treating the variational parameters as independent Gaussian weights and biases, allowing for a mean-field assumption. This 
also leads to a closed-form ELBO and predictive distribution for regression tasks, bypassing Monte Carlo (MC) sampling during inference and training. Our contributions are as follows:
\vspace{-0.3cm}
\begin{itemize}[itemsep=1.5pt,,parsep=1.5pt]
    \item We introduce a DAK model that reinterprets the deep additive kernel learning as a last-layer BNN via induced prior approximation. The proposed DAK enjoys the mathematical interpretability of DKL as well as the computational advantages of BNN, and can be adapted to a variety of DL applications.
    % combining an NN feature extractor with additive GP layers that have sparse kernel activations, utilizing the sparse structure of the Cholesky factor of the Laplace kernel on a designed induced grid. The proposed DAK model has a general form of BNN, which is flexible to generalize to different tasks.
    \item We derive closed-form expressions of both the predictive distribution in inference and the ELBO during training for regression tasks, eliminating the need for sampling. The reduced computational complexity is linear to the size of the induced grid.
    \item In experimental studies, we demonstrate that the proposed DAK model outperforms state-of-the-art DKL methods in both regression and classification tasks, while also mitigating the overfitting issue. Source code of DAK is available at the following link \url{https://github.com/warrenzha/dak2bnn}.
\end{itemize}
\vspace{-0.2cm}
The remainder of this paper is organized as follows: \Cref{sec:prelim} introduces the background on GPs, additive GPs, and DKL. In \Cref{sec:dak}, we present the proposed DAK model and discuss its computational complexity compared to other state-of-the-art DKL models, followed by related work in \Cref{sec:related work}. We present the experimental results in \Cref{sec:exp}, and conclude the paper in \Cref{sec:conc}.

\section{PRELIMINARIES}
\label{sec:prelim}
\paragraph{GPs.}
A GP $f(\cdot) \sim \mathcal{GP}(\mu(\cdot) ,k(\cdot,\cdot) )$ is completely specified by its mean function $\mu(\cdot)$ and the covariance (kernel) function $k(\cdot,\cdot)$. Given a dataset $\mathcal{D}=\{ \Xv, \yv \}$, where $\Xv=\{ \xv_{i} \in \mathbb{R}^D \}_{i=1}^{N}$ are training points and $\yv= ( y_1,\ldots,y_N )^{\top}$ is the corresponding observation where $y_i\in \Rb$, the standard procedure of GPs assumes $\mu(\cdot)$ is a constant function assigned to zero and considers $y_i=f(\xv_i) + \epsilon_i$ with Gaussian noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2_f)$ for $i=1,\ldots,N$. The predictive posterior $\fv_{\ast}:=f(\Xv^{\ast})$ 
% with zero-mean GP prior $f(\cdot) \sim \mathcal{GP}\left(0, k(\cdot,\cdot) \right)$ 
at $N_{\ast}$ test points $\Xv^{\ast}:= \{ \xv_{i}^{\ast} \in \mathbb{R}^D \}_{i=1}^{N_{\ast}}$ can also be expressed in closed form as a Gaussian distribution:
\begin{align}
    & \fv_{\ast} \vert \Xv,\yv,\Xv^{\ast} \sim \mathcal{N} \left( \bm{\mu}_{\ast} , \bm{\Sigma}_{\ast} \right), \\
    \bm{\mu}_{\ast} &= \mathbf{K}_{\Xv^*,\Xv} \left( \mathbf{K}_{\Xv,\Xv} + \sigma_f^2 \Iv \right) ^{-1} \yv,\label{eq:GPR mean}\\
    \bm{\Sigma}_{\ast} &= \mathbf{K}_{\Xv^*,\Xv^*}
    - \mathbf{K}_{\Xv^*,\Xv} \left( \mathbf{K}_{\Xv,\Xv} + \sigma_f^2 \Iv \right)^{-1} \mathbf{K}_{\Xv,\Xv^*},
\end{align}
where $\mathbf{K}_{\Xv,\Xv'}$ denotes the kernel matrix with $[\mathbf{K}_{\Xv,\Xv'}]_{ij}:=k(\xv_{i}, \xv'_{j})$. 
The common challenges of GPs are the $\mathcal{O}(N^3)$ computational complexity of inference and the ``curse of dimensionality'' with high-dimensional data. We refer the readers to \citet{williams2006gaussian} for more details. 

\paragraph{Additive GPs.}
\citet{duvenaud2011additive} introduced the additive GP model, which allows additive interactions of all orders, ranging from first-order interactions all the way to $D$-th order interactions. In this work, we consider the simplest case where the additive GP is restricted to the first order with the same base kernel for each unit $d \in \{1,\ldots,D\}$:
\begin{align}
\label{eq:additiveGP}
    f(\xv)=\sigma\sum_{d=1}^{D} g_d (x_d)+\mu(\xv),
\end{align}
where $x_d$ is the $d$-th feature of the point $\xv\in \Rb^D$, $g_d(x_d) \sim \mathcal{GP}(0,k_d(x_d,x_d^{\prime}))$ is the \emph{centered} base GP unit. The resulting $f(\xv): \Rb^D\rightarrow \Rb$ is a GP specified by mean function $\mu(\xv)$ and additive kernel $k^{[D]}_{\text{add}}(\xv, \xv^{\prime}) = \sigma^2 \sum_{d=1}^{D}k_d(x_d, x_d^{\prime})$, where $\sigma^2$ is the variance assigned to all first-order interactions. \citet{delbridge2020randomly} showed that additive kernels projecting to a low dimensional setting can match or even surpass the performance of kernels operating in the original space. However, an additive kernel alone offers no computational advantage, as the cubic complexity remains. As we will show shortly, the additive GP perspective allows us to apply an induced prior approximation, which reduces computational costs and leads to a last-layer Bayesian representation.
% Another advantage of additive GPs is that it retains interpretability. Each function $f_d(x_{d})$ can be interpreted as the contribution of the corresponding predictor $x_d$ to the model, allowing analysts to understand the effect of each variable on the response.

\paragraph{Deep Kernel Learning.} 
The performance of GPs is limited by the choice of kernel. DKL \citep{wilson2016deep} attempts to solve this problem by DNNs, of which the structural properties can model high-dimensional data and large datasets well. DKL first learns feature transformations $h_\psi(\cdot)$ of DNNs with the parameters $\psi$. The outputs of DNNs are then used as inputs to a GP layer $\mathcal{GP}\left( \mu_{\theta}(\cdot ),k_{\theta}(\cdot ,\cdot ) \right)$ with the parameters $\theta$ resulting in the effective kernel $k_{\text{DKL}}\left( \xv,\xv^{\prime} \right) =k_{\theta}\left( h_{\psi}(\xv), h_{\psi}(\xv^{\prime}) \right)$ to learn uncertainty representation provided by GPs. The complete set of parameters $\left\{ \psi ,\theta \right\}$ in DNNs and GPs are trained jointly by maximizing the marginal log-likelihood (MLL).
% \begin{align}
%     \log p\left( \yv|\Xv;\psi, \theta \right) 
%     \propto & 
%     -(\yv - \bm{\mu}_{\Xv})^{\top}\Kv_{\Xv,\Xv}^{-1} (\yv - \bm{\mu}_{\Xv}) \nonumber\\
%     &- \log \left| \Kv_{\Xv,\Xv} \right|, 
%     \quad \bm{\mu}_{\Xv} := \mu(\Xv).
% \end{align}


\section{DAK: Deep Additive Kernel}
\label{sec:dak}
In GPs, the kernel function defines the similarity between data points, playing a crucial role in the model's predictions. However, choosing the right kernel for complex, high-dimensional data can be challenging. DKL addresses this by learning the kernel function directly from data using a DNN. In practice, the outputs of DNNs can still be too complex to scale GPs, e.g. multi-task regression, or image recognition. Therefore, we present the Deep Additive Kernel as a last-layer BNN that can efficiently reduce GPs to single-dimensional ones without loss in performance.

% \paragraph{Kernel.} 
% In this work, we consider the Laplace kernel 
% \begin{align}
% \label{eq:laplace base kernel}
%     k_p(x_p, x_p^{\prime})= \exp\left( - \vert x_p - x_p^{\prime} \vert / \theta_p \right)
% \end{align}
% as the base kernel for $p$-th base GP, $p \in \{1,\ldots,P\}$. Laplace kernels retain the sparse Cholesky decomposition and consequently reduce the computational complexity, see details in \Cref{sec:sparse chol decompose}. Let $\hv := h_{\psi}(\xv)\in \Rb^P$, which could be a DNN, denote the output of a feature transformation $h_{\psi}:\Rb^D\rightarrow \Rb^P$ of the input $\xv \in \Rb^D$. The resulting deep additive Laplace kernel is given by:
% \begin{align}
%     k_{\text{DAK}}\left( \xv,\xv^{\prime} \right) 
%     &= k^{[P]}_{\text{add}}\left( h_{\psi}(\xv),h_{\psi}(\xv^{\prime}) \right) \nonumber \\
%     &= \sum_{p=1}^{P} \sigma_{p}^{2} k_{p}\Big(
%     \begingroup
%         \color{black}
%         \underbracket{
%             \color{black} h_{\psi}^{[p]}(\xv)
%         }_{ \color{black} h_{p}\in \Rb
%         }
%     \endgroup
%     , 
%     \begingroup
%         \color{black}
%         \underbracket{
%             \color{black} h_{\psi}^{[p]}(\xv^{\prime})
%         }_{ \color{black} h_{p}^{\prime}\in \Rb
%         }
%     \endgroup
%     \Big),
%     % &= \sum_{p=1}^{P} \sigma_{p}^{2} \exp \left( -\left| h_{p}-h_{p}^{\prime} \right| / \theta_{p} \right),
% \end{align}
% where $\xv, \xv^{\prime} \in \Rb^D$ are the inputs, $h_p:=h_{\psi}^{[p]}(\xv)$ and $h_p^{\prime}:=h_{\psi}^{[p]}(\xv^{\prime})$ are the $p$-th features of $h_{\psi}(\xv)$ and $h_{\psi}(\xv^{\prime})$ respectively, and $k_p(\cdot,\cdot)$ is the $p$-th kernel with $\theta_p$ and $\sigma_p^2$ being its lengthscale and variance. The number of base kernels $P$ can be selected by applying a linear projection to the last layer of the DNN.
% {\color{red}Tuo: I suggest we merging this section with the next one: first introduce model (3), then state that we use the Laplace kernel for the 1D GPs. No need to mention the kernel (3).}
% \textcolor{magenta}{Haoyuan: I removed ``Kernel'' part and add description of Laplace kernel in ``Model''}
% \textcolor{brown}{Tian: Since we have space, we can consider adding a connection to the kernel representation to clarify things, though we don't really need to use it. Also if we have space, we can reintroduce the exact GP formula to emphasize the computation bottleneck in GP. We should not assume the audience of this conference knows everything about it.}

\paragraph{Model.} 
We present the construction of the DAK using the proposed additive GPs. %Laplace kernel. 
Let $h_{\psi}: \Rb^D \rightarrow \Rb^{P}$ be a neural network, and we consider a total of $P$ \emph{centered} one-dimensional base GPs $g_p \sim \mathcal{GP}(0, k_p(\cdot, \cdot))$ with Laplace kernel $k_p(x_p, x_p^{\prime})= \exp\left( - \vert x_p - x_p^{\prime} \vert / \theta_p \right)$ %$k_p(\cdot, \cdot)$ defined in \cref{eq:laplace base kernel}, 
for $p=1,\ldots,P$. The forward pass of deep additive model $f(\cdot): \Rb^D \rightarrow \Rb$ with Laplace kernel for each base GP %deep additive kernel $k_{\text{DAK}}(\cdot,\cdot)$ 
at $N$ data points $\Xv$ is described as follows:
\begin{align}
    f (\Xv) &=\sum_{p=1}^{P} \sigma_{p}g_{p} \Big( 
    \begingroup
    \color{black}
        \underbrace{ 
            \color{black} 
            h^{[p]}_{\psi}\left( \Xv \right) 
        }_{ \color{black} := \Hv_p \in \Rb^{N} }
    \endgroup
    \Big) + \mu \label{eq:deepadditive},
    % &=\sum_{p=1}^{P} \sigma_{p} \left( \Kv_{\Hv_{p}^{\ast},\Hv_{p}}\Kv_{\Hv_{p},\Hv_{p}}^{-1} g_{\theta_p}( \Hv_p )+ \bm{\mu_{p}} \right),
\end{align}
where $\Hv_p:=h^{[p]}_{\psi}\left( \Xv \right) \in \Rb^{ N}$ is the $p$-th feature vector of neural network representations $h_{\psi}\left( \Xv \right) \in \Rb^{N \times P}$, and $\mu \in \mathbb{R}$ is a constant prior mean placed on $f(\cdot)$. The resulting deep additive kernel can be written as:
\begin{align}
    k_{\text{DAK}}\left( \xv,\xv^{\prime} \right) 
    &= k^{[P]}_{\text{add}}\left( h_{\psi}(\xv),h_{\psi}(\xv^{\prime}) \right) \nonumber \\
    &= \sum_{p=1}^{P} \sigma_{p}^{2} k_{p}\Big(
    \begingroup
        \color{black}
        \underbracket{
            \color{black} h_{\psi}^{[p]}(\xv)
        }_{ \color{black} h_{p}\in \Rb
        }
    \endgroup
    , 
    \begingroup
        \color{black}
        \underbracket{
            \color{black} h_{\psi}^{[p]}(\xv^{\prime})
        }_{ \color{black} h_{p}^{\prime}\in \Rb
        }
    \endgroup
    \Big).
    % &= \sum_{p=1}^{P} \sigma_{p}^{2} \exp \left( -\left| h_{p}-h_{p}^{\prime} \right| / \theta_{p} \right),
\end{align}
The corresponding coefficients $\{\sigma_{p} \}_{p=1}^{P}$ help learn a correlated contribution of each base kernel, and high-dimensional features of neural network $h_{\psi}\left( \Xv \right)$ can be solved in a single-dimensional space $\Rb$ to achieve scalability. The GP hyperparameters can be inherently optimized with DNN parameters $\psi$ without additional computational cost in our method. The details of reparameterization and optimization tricks are deferred when we discuss \textbf{Inference} and \textbf{Training} shortly.

\paragraph{Induced Prior Approximation.}
%In addition to additive method for high-dimensional data, we also place sparse approximation on each \emph{base GP} component to address the $\mathcal{O}(N^3)$ complexity of inference with large datasets of size $N$. Rather than approximating the covariance matrix $\Kv_{\Xv,\Xv}$, we directly approximate the prior of each base GP $g_{p}$ by reduced-rank induced prior approximation $\tilde{g}_{p}$ given as follows 
An essential ingredient of the proposed method is to apply a reduced-rank approximation to the prior, i.e., the base GPs. This approximation is referred to as the induced prior approximation \citep{ding2024sparse}, given by
\begin{align}
    \tilde{g}_{p} (\cdot ) &:=\Kv_{(\cdot),\Uv} \Kv_{\Uv,\Uv}^{-1} \, g_{p}(\Uv) \label{eq:inducedAppro}\\
    &=
    \begingroup
    \color{black}
        \underbrace{ 
            \color{black}
            \Kv_{(\cdot),\Uv} \left[ \Lv_{\Uv}^{\top} \right]^{-1}
        }_{\color{black}
            := \phi(\cdot) \in \Rb^{1\times M}
        }
    \endgroup
    \begingroup
    \color{black}
        \underbrace{ 
            \color{black}
            \Lv_{\Uv}^{-1} g_{p} ( \Uv ) 
        }_{\color{black}
        :=\zv_p \,\sim\, \mathcal{N}(\bm{0}, \bm{I}_{M})
        }
    \endgroup  \nonumber\\
    &=\phi \left( \cdot \right) \zv_p, \label{eq:GPlayer}
\end{align}
where $\Uv=\{ u_{i}\in \mathbb{R} \}_{i=1}^{M}$ denotes the induced grids (see details in \Cref{sec:sparse chol decompose}), $\Lv_\Uv \in \Rb^{M\times M}$ is a lower triangular matrix derived by the Cholesky decomposition of $\Kv_{\Uv,\Uv}=\Lv_{\Uv} \Lv^{\top}_{\Uv}$, and $\zv_p=\Lv_{\Uv}^{-1} g_{p} \left( \Uv \right)\sim \mathcal{N}(\mathbf{0},\bm{I}_M)$ are i.i.d. standard Normal random variables. The main idea of induced prior approximation (\cref{eq:inducedAppro}) is to use the GP regression (see \cref{eq:GPR mean}) to reconstruct the prior GP. According to the theory of GP regression \citep{yakowitz1985comparison,wang2020prediction}, $\tilde{g}_{p}$ converges to the original prior as $\Uv$ becomes dense in its domain.

%The induced approximation reduces the complexity of inference from $\Oc(N^3)$ to $\Oc(M^2N)$. 
The standard Cholesky decomposition of a dense matrix requires $O(M^3)$ time. However, by leveraging the Markov property of the Laplace kernel, the decomposed matrix $\Lv_{\Uv}^{-1}$ becomes sparse if $\Uv$ is designed by a one-dimensional dyadic point set with increasing order \citep{ding2024sparse}. As a consequence, the complexity of Cholesky decomposition can be reduced to $\Oc(M)$. Details of obtaining the sparse Cholesky factors are presented in \Cref{sec:sparse chol decompose}. 

Applying the sparsely induced GP approximation in \cref{eq:GPlayer} together with the deep additive model in \cref{eq:deepadditive}, we obtain the final DAK:
\begin{align}
    \tilde{f}(\Xv) &=\sum_{p=1}^{P} \sigma_p \tilde{g}_{p} \left( h^{[p]}_{\psi}(\Xv) \right) + \mu \notag \\
    % &=\sum_{p=1}^{P} \sigma_{p} \Big(
    % \phi \big(\Hv_p \big) \zv_{p} + \mu_p
    % \begingroup
    % \color{blue}
    %     \underbracket{
    %         \color{black}
    %         \phi \big(\Hv^*_p \big)
    %     }_{\color{blue}
    %     \text{activation}
    %     %\substack{\text{sparse} \\ \text{activation}}
    %     }
    % \endgroup
    % \begingroup
    % \color{blue}
    %     \underbracket{
    %         \color{black}
    %         \zv_{p}
    %     }_{\color{blue}
    %     \text{weights}
    %     }
    % \endgroup
    % + \hspace{0.5em}
    % \begingroup
    % \color{blue}
    %     \underbracket{
    %         \color{black}
    %         \mu_p(\Hv^*_p)
    %     }_{
    %         \color{blue}
    %         \text{bias}
    %     }
    % \endgroup
    % \Big) \\
    &=\sum_{p=1}^{P} \sigma_{p} \Big(
    \phi \big(\Hv_p \big) \zv_{p}\Big) + \mu,  \label{eq:additiveDKL} 
\end{align}
where $\phi(\Hv_p):=\Kv_{\Hv_p,\Uv} \left[ \Lv_{\Uv}^{\top} \right]^{-1}\in \Rb^{1\times M}$ denotes the kernel activation of $p$-th base GP, $\mu$ is the mean of additive GP, and $\zv_p$'s are random weights with  i.i.d. normal prior distribution $\zv_p \sim \Nc(\bm{0},\bm{I}_M)$, for all $p=1,\ldots,P$.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{DAK.pdf}
    \caption{\small{Model architecture of Deep Additive Kernel (DAK). DAK consists of a feature extractor $\text{NN}(\cdot)$ with a linear embedding layer $\Wv$, an additive kernel with base GP $\tilde{g}_{p}(\cdot)$ for $p=1,\ldots,P$, and a weighted sum layer. The embedded features learned by DNN are decomposed as first-order components and fed to base GPs, each consisting of a kernel activation and a GP forward layer. Each kernel activation is designed by a one-dimensional dyadic point set on an induced grid with sparse non-zero activated neurons.}}
    % \vspace{-0.4cm}
    \label{fig:model}
\end{figure*}

The proposed DAK in \cref{eq:additiveDKL} results in deep additive kernel learning which mathematically possesses the form of a last-layer BNN \citep{mackay1992practical} but with a kernel activation $\phi(\cdot):=\Kv_{(\cdot),\Uv} \left[ \Lv_{\Uv}^{\top} \right]^{-1}$ followed by a Gaussian forward layer with i.i.d. prior weights under standard Normal distribution. 

\Cref{fig:model} illustrates the module architecture of DAK. Thanks to the mathematical equivalence to BNN and reparameterization tricks on learnable parameters, all modules of our model can be implemented either as a deterministic or a Bayesian NN layer. This results in a single NN with hybrid layers, which allows us 
to straightforwardly take advantage of parallelized GPU computing using the readily available PyTorch package \citep{paszke2019pytorch}.


\paragraph{Inference.}
To obtain the predictive distribution, we perform Variational Inference (VI) to estimate the posterior using DAK in \cref{eq:additiveDKL}. With a motivation to naturally interpret the deep additive kernel learning as a last-layer BNN, and taking the mean field assumptions, we select a variational family of independent but not identical Gaussian weights $\Zv:=[ \zv_1,\ldots,\zv_P ]$ and additive Gaussian bias $\mu$, denoted by $\Theta_{\text{var}}:=\left\{ \{ \zv_{p}\}_{p=1}^{P}, \mu \right\}$. The variational Gaussian weights are parameterized as $\zv_{p} \sim \mathcal{N} (\bm{m}_{\zv_p} ,\Sv_{\zv_p})$ for $p=1,\ldots,P$, and the variational bias is parameterized as $\mu \sim \mathcal{N} ( m_{\mu},\sigma^2_{\mu} )$. We denote the reparameterization of $\Theta_{\text{var}}$ as $\bm{\eta}:=\left\{ \{ \mv_{\zv_{p}},\Sv_{\zv_{p}}\}_{p=1}^{P} , \{m_{\mu},\sigma_{\mu}\} \right\}$. 

Note that $\Sv_{\zv_p}\in\Rb^{M \times M}$ is a diagonal covariance matrix due to the mean field assumption of $\zv_p$, where $M$ is decided by the size of induced interpolation grids defined in \cref{eq:GPlayer}. The variational distribution is given by $q_{\bm{\eta}}(\Theta_{\text{var}}) = q(\mu) \prod_{p=1}^{P} q(\zv_{p}) = \Nc ( m_{\mu} ,\sigma_{\mu}^2 )\prod_{p=1}^{P} 
\Nc ( \bm{m}_{\zv_p} ,\Sv_{\zv_p} )$, and the prior of $\Theta_{\text{var}}$ is denoted by $p(\Theta_{\text{var}})$.

The other deterministic parameters consist of DNN parameters $\psi$ and additive GP hyperparameters $\left\{ \bm{\sigma} \right\}:=[\sigma_1,\ldots,\sigma_P]^{\top}$, denoted as $\bm{\theta}:=\{\psi, \bm{\sigma} \}$. We treat GP lengthscales as fixed parameters specified during initialization and apply an additional linearly embedding layer to DNN: $h_{\psi}(\cdot ):=\text{NN} (\cdot )\Wv: \Rb^D \rightarrow \Rb^P$, where $\text{NN} (\cdot ): \Rb^D \rightarrow \Rb^{D_w}$ represents any DNN, and $\Wv\in \Rb^{D_{w}\times P}$ is the linear embedding. The lengthscales can be inherently optimized by learning the NN weights $\Wv$ without loss in performance, which is encoded in the DNN parameters $\psi$. Further details are provided in \Cref{sec:theo}. 

Given a data point $\xv\in \Rb^D$, the forward pass of predictive distribution is given by
\begin{align}
\label{eq:DAK prediction}
    \tilde{f}_{\xv}:= \tilde{f}(\xv; \bm{\theta}, \bm{\eta}) =\sum_{p=1}^{P} \sigma_{p} \Big(
    \phi \big(h^{[p]}_{\psi}(\xv) \big) \zv_{p}\Big) + \mu,
\end{align}
where the complete set of parameters is $\Theta:= \left\{ \bm{\theta}, \bm{\eta} \right\} = \left\{ \psi ,\bm{\sigma}, \{ \mv_{\zv_{p}},\Sv_{\zv_{p}}\}_{p=1}^{P} , \{m_{\mu},\sigma_{\mu}\} \right\}$, consisting of the deterministic parameters $\bm{\theta} := \left\{ \psi ,\bm{\sigma} \right\}$ and the variational parameters $\bm{\eta}:= \left\{ \{ \mv_{\zv_{p}},\Sv_{\zv_{p}}\}_{p=1}^{P} , \{m_{\mu},\sigma_{\mu}\} \right\}$. In \Cref{sec:uq of inference}, we derive an analytical form for the predictive distribution $\tilde{f}$.

%is deferred to \Cref{sec:uq of inference}. 

%\textcolor{red}{In \Cref{sec:uq of inference}, we derive an analytical expression for the distribution of $\tilde{f}$ as
% \begin{align}       
%     \Nc\left(
%     \sum_{p=1}^{P}
%     \sigma_p ( \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p}) + m_{\mu} ,\hspace{0.2em}
%     \sum_{p=1}^{P}
%     \sigma_p^2( \bm{\phi}_{p}^{\top} \Sv_{\zv_p} \bm{\phi}_{p} ) + \sigma_{\mu}^2
%     \right).
% \end{align}}




\paragraph{Training.} 
Vanilla DKL optimizes the marginal log-likelihood $\log \text{Pr} \left( \yv \vert \Xv, \bm{\theta} \right)$ which involves intractable integral of non-conjugate likelihoods in some tasks such as classification. We apply the framework of stochastic variational inference to fit GPs via the variational distribution $q_{\bm{\eta}}(\Theta_\text{var})$. Consequently, during training, we optimize the variational lower bound, as formulated in \cite{hensman2015scalable}, to achieve efficient and scalable inference:
\begin{align}
\label{eq:VI lower bound}
\log \text{Pr}(\yv | \Xv, \bm{\theta}) \geq &\sum_{\xv, y \in \Xv, \yv}\Eb_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ \log \text{Pr} \big(y | \tilde{f}_{\xv} \big) \right] \nonumber \\
& - \text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}} ) \| p(\Theta_{\text{var}}) \right].
\end{align}
The details of the training are derived in \Cref{sec:training}. The resulting objective is known as Evidence Lower Bound (ELBO):
\begin{align}\label{eq:elbo}
    \mathcal{L} (\bm{\theta}, \bm{\eta}) :=
    & \  {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}} )} \left[ \log \text{Pr} (\yv \vert \tilde{f}_{\Xv}) \right] \nonumber \\
    & - \text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}} ) \| p(\Theta_{\text{var}}) \right].
\end{align}
The first term of ELBO, the expected log-likelihood, can be estimated by MC methods. To avoid the potential computing cost of sampling, we also derive a sampling-free analytical ELBO with a closed form for regression tasks. The details of the derivation are provided in \Cref{sec:elbo}.

\paragraph{Computational Complexity.}
We summarize the computational complexity of our proposed DAK model compared to other state-of-the-art GP and DKL methods in \Cref{tab:complexity}. On the one hand, the number of inducing points $\hat{M}$ in SVGP and KISS-GP may need to be chosen quite large in more complex or multitask GPs since it is associated with the GP input dimensionality, while at competitive performances the size of induced grids $M$ in DAK can be small due to the first-order additive structure. On the other hand, the dimension of the embedding layer $P$ is usually smaller than the dimension of NN outputs $D_w$. For tasks that require MC sampling, a small number of samples is usually sufficient due to the sampling efficiency of the BNNs. Further discussion is deferred to \Cref{sec:complexity}.

\begin{table}[tb!]
    \caption{\small{Computational complexity of DKL models for $N$ training points in one iteration. $\hat{M}$ is the number of inducing points in SVGP and KISS-GP, while $M$ is the size of induced grids in DAK, $M < \hat{M}$. $S$ is the number of MC samples, $B$ is the size of mini-batch, $D_w$ is the dimension of the NN outputs in DKL, $P$ is the dimension after the linear embedding of NN features. DAK-MC refers to DAK using MC approximation, while DAK-CF refers to DAK using closed-form inference and ELBO.}}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcc}
    \toprule[1pt]
                  & \textbf{Inference}       & \textbf{Training} (per iteration) \\
    \midrule[0.5pt]
    NN + SVGP     & $\Oc(\hat{M}^2 N)$    & $\Oc( S D_w \hat{M}B + \hat{M}^3)$ \\
    NN + KISS-GP  & $\Oc(D_w \hat{M}^{1+\frac{1}{D_w}})$  & $\Oc(S D_w \hat{M}B + D_w \hat{M}^{\frac{3}{D_w}})$ \\
    DAK-MC (ours) & $\Oc(SM)$       & $\Oc(SPMB + PM)$   \\
    DAK-CF (ours) & $\Oc(M)$        & $\Oc(PMB + PM)$    \\
    \bottomrule[1pt]
    \end{tabular}
    }
    % \vspace{-0.3cm}
    \label{tab:complexity}
\end{table}

\paragraph{Remarks.}
We highlight several key aspects of the proposed model: 
\textbf{1)} The proposed model adds base GP components directly, rather than adding kernels as described in \citep{duvenaud2011additive}. While they are mathematically equivalent, using an additive kernel alone does not lead to any computational advantage over any standard kernel, where the cubic computational complexity persists. In contrast, the additive GP led us to apply the induced approximation technique, which naturally leads to efficient computation and the last-layer Bayesian interpretation. %\tedoes not result in an explicit additive form for the prior $f(\cdot)$ when applying induced prior approximation, which leads to higher computational complexity.}
\textbf{2)} The induced prior approximation in our model differs from the standard inducing points approximation \citep{titsias2009variational} often used for GPs. In our approach, we approximate the prior using a fixed set of induced grids, resulting in a BNN representation. In contrast, the standard inducing point methods treat the inducing points as variational parameters for optimization, which does not lead to a BNN representation. Our method is much easier to implement and has a theoretical guarantee that as the inducing locations become dense in the input region, the approximation will become exact.
\textbf{3)} In the forward pass $f(\cdot)$ as defined in \cref{eq:DAK prediction}, we chose to place a constant prior to the mean $\mu$ on $f(\cdot)$, which also naturally facilitates the construction of the last-layer BNN. This provides a bridge between canonical GPs and BNNs, making them more flexible and extendable to diverse applications.


\begin{figure*}[ht]
    \centering
    \subfloat[\small{Exact GP.} \label{fig:gp1d}]{\includegraphics[width=.2\textwidth]{toy_gp.pdf}}
    \subfloat[\small{DGP.} \label{fig:dgp1d}]{\includegraphics[width=.2\textwidth]{toy_dgp.pdf}}
    \subfloat[\small{Exact DKL.} \label{fig:dkl1d}]{\includegraphics[width=.2\textwidth]{toy_dkl_2.pdf}}
    \subfloat[\small{DAK.} \label{fig:dak1d}]{\includegraphics[width=.2\textwidth]{toy_dak_new.pdf}}
    \subfloat[\small{DNN.} \label{fig:nn1d}]{\includegraphics[width=.2\textwidth]{toy_dnn.pdf}}
    % \subfloat[\small{Training curves} \label{fig:toyloss}]{\includegraphics[width=.23\textwidth]{figs/toy/toy_loss.pdf}}
    % \vspace{-0.2cm}
    \caption{\small{Results on toy dataset. (a)--(d) show the predictive posterior of the exact GP, DGP, exact DKL and proposed DAK model, respectively, on the noisy data generated by 1D GP with zero-mean and covariance function $k(x,x')=\exp( -(x-x')^2 )$. We set the number of MC samples $S=4$ for estimating the expected log-likelihood in ELBO during training.} The predictive mean and $\pm$2 standard deviations are plotted together with the observed data. (e) shows the NN fit with the same training data.}
    % \vspace{-0.3cm}
    \label{fig:toyGP}
\end{figure*}

\section{RELATED WORK}
\label{sec:related work}
Motivated by integrating the power of deep networks with interpretable and theoretically grounded kernel methods, it is encouraging to see some contributions on such combinations across a range of contexts.

\paragraph{Variations of DKL.} 
Several recent studies have explored variations of DKL models. To handle large datasets and diverse tasks, \citet{wilson2016stochastic} extended the vanilla DKL \citep{wilson2016deep} to Stochastic Variational DKL (SV-DKL) by leveraging Stochastic Variational GP (SVGP) \citep{hensman2015scalable} and KISS-GP \citep{wilson2015kernel}. However, the kernel interpolation on a Cartesian grid does not scale in the high-dimensional space. \citet{xue2019deep} and \citet{xie2019deep} integrated deep kernel models with Random Fourier Features (RFF) \citep{rahimi2007random}, an efficient method for approximating GP kernel feature mappings. Some other work has focused on developing models with specialized kernel structures, such as recurrent kernels \citep{al2017learning} and compositional kernels \citep{sun2018differentiable}. More recently, %\citet{liu2021deep} introduced a latent-variable framework that integrates a stochastic encoding of inputs to enable regularized representation learning, while \citet{achituve2023guided} proposed a novel approach to training DKL by using an infinite-width NN to guide DKL optimization. 
\citet{ober2021promises} investigated overfitting in DKL models based on marginal likelihood maximization and proposed a fully Bayesian approach to mitigate this issue. \citet{matias2024amortized} introduced amortized variational DKL, which uses DNNs to learn variational distributions over inducing points in an amortized manner, thereby reducing overfitting by locally smoothing predictions. 
% \citet{harrison2024variational} introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks.

\paragraph{GPs and NNs.}
The connection between GPs and NNs was first established by \citet{neal1994bayesian}, who showed that the function defined by a single-layer NN with infinite width, random independent zero-mean weights, and biases is equivalent to a GP. This equivalence was later generalized to arbitrary NNs with infinite-width or infinite-depth layers in \citep{lee2017deep, cutajar2017random, matthews2018gaussian, yang2019wide, dutordoir2021deep, gao2023wide}. However, these studies focus primarily on fully connected NNs, which are not suitable for all practical applications. \citet{garriga2018deep} and \citet{novak2018bayesian} extended the equivalence to Convolutional Neural Networks (CNNs) \citep{lecun1989handwritten}, which are widely used in image recognition. Hybrid models combining GPs and NNs have also been investigated. \citet{bradshaw2017adversarial} proposed a hybrid GPDNN that feeds CNN features into a GP, while \citet{zhang2024gaussian} introduced GP Neural Additive Models (GP-NAM), a class of GAMs that use GPs approximated by RFF and a single-layer NN. However, GPDNN uses the standard GP, while we approximate GP via induced prior approximation. GP-NAM applies the additive model with RFF approximation but lacks Bayesian inference. \citet{harrison2024variational} introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. %In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks. The resulting GP layer has the form of a last-layer BNN which is easy to extend to various tasks and efficient in computing and inference.
% \textcolor{brown}{Tian: The last sentence may not be necessary, since we also highlight it in the remark.}


\section{EXPERIMENTS}
\label{sec:exp}

In this section, we evaluate the proposed DAK model on multiple real datasets for both regression tasks in \Cref{subsec:uci reg} and classification tasks in \Cref{subsec:image class}. We compare its performance with several baselines, including a neural network without GP integration (NN), DKL with SVGP \citep{titsias2009variational,hensman2015scalable} as the GP approximation (NN+SVGP), and SV-DKL \citep{wilson2016stochastic}. Furthermore, in \Cref{subsec:toy}, we present a toy example that demonstrates how the proposed model mitigates the issue of out-of-sample overfitting highlighted by \citet{ober2021promises}, in contrast to the NN model. 
% Source code is available at the following link \footnote{\url{https://github.com/warrenzha/dak2bnn}}.


\subsection{Toy Example: GP}
\label{subsec:toy}

\begin{table*}[ht]
\centering
\caption{\small{Comparison of regression performance on UCI datasets using 5-fold cross-validation with a batch size of 512 and a fully connected neural network architecture of $D \rightarrow 64 \rightarrow 32 \rightarrow D_{w}$, where the output features $D_{w}$ are 16, 64, and 256 respectively. The best results are highlighted in \textbf{bold}. Our models, DAK-MC (using MC approximation) and DAK-CF (using closed-form inference and ELBO), are highlighted with  \protect\colorbox{Gray}{gray background}.}}
\label{tab:uci metrics}
\vspace{-0.2cm}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|l|ccc|ccc|ccc}
\toprule[1pt]
% \multirow{3}{*}{\makecell[tl]{\textbf{Dataset} \\ $(N, D)$ }} & \multirow{3}{*}{\textbf{Model}} & \multicolumn{9}{c}{\textbf{Batch size = 512}} \\ \cline{3-11} &
\multirow{2}{*}{\makecell[tl]{\textbf{Dataset} \\ $(N, D)$ }}  & \multirow{2}{*}{\textbf{Model}}

& \multicolumn{3}{c|}{\textbf{NN out features = 16}} & \multicolumn{3}{c|}{\textbf{NN out features = 64}} & \multicolumn{3}{c}{\textbf{NN out features = 256}} \\
\cline{3-11}
&  & \textbf{RMSE} $\downarrow$ & \textbf{NLPD} $\downarrow$ & \textbf{Time (s)} $\downarrow$  & \textbf{RMSE} $\downarrow$ & \textbf{NLPD} $\downarrow$ & \textbf{Time (s)} $\downarrow$  & \textbf{RMSE} $\downarrow$ & \textbf{NLPD} $\downarrow$ & \textbf{Time (s)} $\downarrow$ \\
\hline
\multirow{5}{*}{ \makecell[tl]{ \textbf{Gas} \\ $(2565, 128)$} } & NN & 2.377 $\pm$ 2.399 & 4.749 $\pm$ 6.594 & \textbf{2.345 $\pm$ 0.003} & 2.107 $\pm$ 1.212 & 3.092 $\pm$ 2.229 & \textbf{2.362 $\pm$ 0.029} & 1.196 $\pm$ 0.712 & 1.730 $\pm$ 0.711 & \textbf{2.346 $\pm$ 0.004} \\
& NN+SVGP & 0.502 $\pm$ 0.171 & 1.121 $\pm$ 0.106 & 4.727 $\pm$ 0.009 & 0.625 $\pm$ 0.148 & 1.206 $\pm$ 0.119 & 4.724 $\pm$ 0.004 & 0.743 $\pm$ 0.183 & 1.322 $\pm$ 0.143 & 4.718 $\pm$ 0.009 \\
& SV-DKL & 0.589 $\pm$ 0.161 & 1.303 $\pm$ 0.227 & 28.189 $\pm$ 0.490 & 0.499 $\pm$ 0.183 & 1.235 $\pm$ 0.256 & 27.956 $\pm$ 0.078 & 0.534 $\pm$ 0.189 & 1.207 $\pm$ 0.209 & 28.400 $\pm$ 0.185 \\
& AV-DKL &  0.538 $\pm$ 0.129  &  1.250 $\pm$ 0.222  &  25.315 $\pm$ 0.254  & 0.604 $\pm$ 0.016  &  1.250 $\pm$ 0.010  &  27.070 $\pm$ 1.620 &  0.760 $\pm$ 0.367  &  1.276 $\pm$ 0.220  &  28.860 $\pm$ 1.903  \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} \textbf{0.405 $\pm$ 0.061} & 
\cellcolor{Gray} \textbf{0.886 $\pm$ 0.048} & \cellcolor{Gray} 8.887 $\pm$ 0.007 & \cellcolor{Gray} 0.353 $\pm$ 0.046 & \cellcolor{Gray} \textbf{0.881 $\pm$ 0.053} & 
\cellcolor{Gray} 8.844 $\pm$ 0.005 & \cellcolor{Gray} 0.351 $\pm$ 0.019 & \cellcolor{Gray} \textbf{0.871 $\pm$ 0.027} & 
\cellcolor{Gray} 8.831 $\pm$ 0.017 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray} 0.412 $\pm$ 0.134 & \cellcolor{Gray} 0.928 $\pm$ 0.100 & \cellcolor{Gray} 7.400 $\pm$ 0.004 & \cellcolor{Gray} \textbf{0.350 $\pm$ 0.020} & 
\cellcolor{Gray} 0.898 $\pm$ 0.040 & \cellcolor{Gray} 7.398 $\pm$ 0.009 & \cellcolor{Gray} \textbf{0.342 $\pm$ 0.033} & 
\cellcolor{Gray} 0.895 $\pm$ 0.046 & \cellcolor{Gray} 7.410 $\pm$ 0.009 \\
\hline
\multirow{5}{*}{ \makecell[tl]{ \textbf{Parkinsons} \\ $(5875, 20)$ } } & NN & 2.692 $\pm$ 1.302 & 3.503 $\pm$ 2.630 & \textbf{2.693 $\pm$ 0.027} & 2.288 $\pm$ 0.712 & 2.724 $\pm$ 1.158 & \textbf{2.589 $\pm$ 0.050} & 2.252 $\pm$ 0.757 & 2.720 $\pm$ 1.188 & \textbf{2.885 $\pm$ 0.026} \\
& NN+SVGP & 3.481 $\pm$ 1.906 & 4.606 $\pm$ 4.606 & 5.516 $\pm$ 0.117 & 3.238 $\pm$ 2.419 & 4.940 $\pm$ 5.494 & 5.467 $\pm$ 0.097 & 3.676 $\pm$ 3.287 & 6.791 $\pm$ 8.612 & 5.945 $\pm$ 0.090 \\
& SV-DKL & 2.608 $\pm$ 1.023 & 2.745 $\pm$ 1.097 & 35.804 $\pm$ 1.590 & 2.817 $\pm$ 1.670 & 3.193 $\pm$ 2.109 & 33.042 $\pm$ 0.306 & 2.896 $\pm$ 2.055 & 3.206 $\pm$ 1.906 & 32.882 $\pm$ 0.060 \\
& AV-DKL &  1.942 $\pm$ 0.758  &  \textbf{2.223 $\pm$ 0.655} &  30.337 $\pm$ 1.086  &  2.267 $\pm$ 0.584  &  \textbf{2.397 $\pm$ 0.628} & 31.006 $\pm$ 1.162 & 3.096 $\pm$ 0.472 & 3.105 $\pm$ 0.793 &  31.859 $\pm$ 1.197 \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} 1.983 $\pm$ 1.154 & \cellcolor{Gray} 2.699 $\pm$ 1.819 & \cellcolor{Gray} 11.596 $\pm$ 0.260 & \cellcolor{Gray} 1.949 $\pm$ 0.912 & \cellcolor{Gray} 2.575 $\pm$ 1.121 & \cellcolor{Gray} 13.085 $\pm$ 0.055 & \cellcolor{Gray} 1.846 $\pm$ 0.974 & \cellcolor{Gray} 2.420 $\pm$ 1.212 & \cellcolor{Gray} 11.820 $\pm$ 0.296 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray} \textbf{1.801 $\pm$ 1.013} & \cellcolor{Gray} 2.848 $\pm$ 1.810 & \cellcolor{Gray} 9.071 $\pm$ 0.083 & \cellcolor{Gray} \textbf{1.788 $\pm$ 0.997} & \cellcolor{Gray} 2.801 $\pm$ 1.853 & \cellcolor{Gray} 9.073 $\pm$ 0.048 & \cellcolor{Gray} \textbf{1.466 $\pm$ 1.093} & \cellcolor{Gray} \textbf{2.308 $\pm$ 1.892} & \cellcolor{Gray} 8.166 $\pm$ 0.071 \\
\hline
\multirow{5}{*}{ \makecell[tl]{\textbf{Wine} \\ $(1599, 11)$} } & NN & 0.728 $\pm$ 0.055 & 1.233 $\pm$ 0.057 & \textbf{2.350 $\pm$ 0.011} & 0.725 $\pm$ 0.057 & 1.231 $\pm$ 0.054 & \textbf{2.343 $\pm$ 0.007} & 0.712 $\pm$ 0.062 & 1.214 $\pm$ 0.062 & \textbf{2.367 $\pm$ 0.021} \\
& NN+SVGP & 0.739 $\pm$ 0.069 & 1.243 $\pm$ 0.067 & 4.713 $\pm$ 0.010 & 0.801 $\pm$ 0.130 & 1.325 $\pm$ 0.130 & 4.725 $\pm$ 0.008 & 0.893 $\pm$ 0.180 & 1.418 $\pm$ 0.147 & 4.718 $\pm$ 0.016 \\
& SV-DKL & 0.899 $\pm$ 0.110 & 1.443 $\pm$ 0.135 & 27.224 $\pm$ 0.170 & 0.947 $\pm$ 0.109 & 1.460 $\pm$ 0.112 & 27.136 $\pm$ 0.023 & 0.879 $\pm$ 0.130 & 1.434 $\pm$ 0.158 & 27.332 $\pm$ 0.101 \\
& AV-DKL & \textbf{0.699 $\pm$ 0.060} & 1.207 $\pm$ 0.068 & 24.419 $\pm$ 0.316 & \textbf{0.711 $\pm$ 0.055} & 1.221 $\pm$ 0.060 & 24.904 $\pm$ 0.424 & \textbf{0.711 $\pm$ 0.055} & 1.221 $\pm$ 0.060 & 24.904 $\pm$ 0.424 \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} 0.756 $\pm$ 0.068 & \cellcolor{Gray} 1.164 $\pm$ 0.075 & \cellcolor{Gray} 8.820 $\pm$ 0.038 & \cellcolor{Gray} 0.751 $\pm$ 0.055 & \cellcolor{Gray} 1.163 $\pm$ 0.060 & \cellcolor{Gray} 8.821 $\pm$ 0.045 & \cellcolor{Gray} 0.727 $\pm$ 0.065 & \cellcolor{Gray} \textbf{1.140 $\pm$ 0.070} & \cellcolor{Gray} 8.765 $\pm$ 0.016 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray}  0.736 $\pm$ 0.042 & \cellcolor{Gray} \textbf{1.162 $\pm$ 0.049} & \cellcolor{Gray} 7.376 $\pm$ 0.018 & \cellcolor{Gray} 0.728 $\pm$ 0.064 & \cellcolor{Gray} \textbf{1.153 $\pm$ 0.067} & 
\cellcolor{Gray} 7.352 $\pm$ 0.010 & 
\cellcolor{Gray} 0.720 $\pm$ 0.057 & \cellcolor{Gray} 1.147 $\pm$ 0.061 & 
\cellcolor{Gray} 7.415 $\pm$ 0.029 \\
\hline
\multirow{5}{*}{ \makecell[tl]{ \textbf{Kin40K} \\ $(40000, 8)$ }} & NN & 0.109 $\pm$ 0.016 & 0.752 $\pm$ 0.004 & \textbf{16.017 $\pm$ 0.114} & 0.100 $\pm$ 0.012 & 0.749 $\pm$ 0.003 & \textbf{16.912 $\pm$ 0.112} & 0.087 $\pm$ 0.016 & 0.746 $\pm$ 0.004 & \textbf{18.048 $\pm$ 0.025} \\
& NN+SVGP & 0.104 $\pm$ 0.013 & 0.751 $\pm$ 0.004 & 40.749 $\pm$ 0.508 & 0.142 $\pm$ 0.017 & 0.763 $\pm$ 0.006 & 41.239 $\pm$ 0.304 & 0.123 $\pm$ 0.016 & 0.757 $\pm$ 0.006 & 43.111 $\pm$ 1.482 \\
& SV-DKL & 0.096 $\pm$ 0.024 & 0.750 $\pm$ 0.010 & 230.284 $\pm$ 7.049 & 0.092 $\pm$ 0.018 & 0.748 $\pm$ 0.006 & 228.137 $\pm$ 7.406 & 0.097 $\pm$ 0.025 & 0.750 $\pm$ 0.009 & 226.606 $\pm$ 4.231 \\
& AV-DKL & 0.084 $\pm$ 0.011 & 0.746 $\pm$ 0.003 & 75.427 $\pm$ 0.965 &  0.081 $\pm$ 0.014 & 0.745 $\pm$ 0.003 & 87.654 $\pm$ 1.664 & 0.071 $\pm$ 0.015 & 0.743 $\pm$ 0.029 & 158.860 $\pm$ 3.621 \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} 0.096 $\pm$ 0.042 & \cellcolor{Gray} 0.746 $\pm$ 0.010 & \cellcolor{Gray} 74.787 $\pm$ 0.383 & \cellcolor{Gray} 0.090 $\pm$ 0.028 & \cellcolor{Gray} 0.744 $\pm$ 0.006 & \cellcolor{Gray} 77.349 $\pm$ 5.409 & \cellcolor{Gray} 0.090 $\pm$ 0.031 & \cellcolor{Gray} 0.744 $\pm$ 0.007 & \cellcolor{Gray} 75.586 $\pm$ 0.313 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray} \textbf{0.073 $\pm$ 0.015} & \cellcolor{Gray} \textbf{0.742 $\pm$ 0.005} & \cellcolor{Gray} 54.174 $\pm$ 0.322 & \cellcolor{Gray} \textbf{0.069 $\pm$ 0.018} & \cellcolor{Gray} \textbf{0.741 $\pm$ 0.005} & \cellcolor{Gray} 60.339 $\pm$ 0.161 & \cellcolor{Gray} \textbf{0.068 $\pm$ 0.015} & \cellcolor{Gray} \textbf{0.741 $\pm$ 0.004} & \cellcolor{Gray} 55.148 $\pm$ 0.165 \\
\hline
\multirow{5}{*}{ \makecell[tl]{ \textbf{Protein} \\ $(45730, 9)$} } & NN & 4.678 $\pm$ 7.804 & 25.091 $\pm$ 52.135 & \textbf{24.910 $\pm$ 0.221} & 0.906 $\pm$ 0.304 & 1.421 $\pm$ 0.232 & \textbf{19.433 $\pm$ 0.093} & 1.403 $\pm$ 1.408 & 2.358 $\pm$ 2.349 & \textbf{28.447 $\pm$ 0.133} \\
& NN+SVGP & 0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 47.081 $\pm$ 0.643 & 0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 38.986 $\pm$ 0.485 & 0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 52.924 $\pm$ 1.144 \\
& SV-DKL & 0.769 $\pm$ 0.003 & 1.275 $\pm$ 0.001 & 262.274 $\pm$ 8.216 & 0.770 $\pm$ 0.003 & 1.276 $\pm$ 0.002 & 265.182 $\pm$ 3.680 & 0.769 $\pm$ 0.003 & 1.275 $\pm$ 0.001 & 264.678 $\pm$ 4.450 \\
& AV-DKL & 0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 87.939 $\pm$ 1.303 &  0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 92.591 $\pm$ 1.934 & 0.773 $\pm$ 0.003 & 1.278 $\pm$ 0.002 & 164.997 $\pm$ 4.108 \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} \textbf{0.622 $\pm$ 0.047} & \cellcolor{Gray} \textbf{1.017 $\pm$ 0.043} & \cellcolor{Gray} 85.729 $\pm$ 1.035 & \cellcolor{Gray} \textbf{0.612 $\pm$ 0.038} & \cellcolor{Gray} \textbf{1.008 $\pm$ 0.035} & \cellcolor{Gray} 86.594 $\pm$ 0.827 & \cellcolor{Gray} \textbf{0.608 $\pm$ 0.042} & \cellcolor{Gray} \textbf{1.004 $\pm$ 0.039} & \cellcolor{Gray} 86.245 $\pm$ 0.088 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray} 0.631 $\pm$ 0.021 & \cellcolor{Gray} 1.024 $\pm$ 0.019 & \cellcolor{Gray} 68.762 $\pm$ 0.379 & \cellcolor{Gray} 0.625 $\pm$ 0.027 & \cellcolor{Gray} 1.018 $\pm$ 0.026 & \cellcolor{Gray} 69.766 $\pm$ 0.190 & \cellcolor{Gray} 0.618 $\pm$ 0.029 & \cellcolor{Gray} 1.012 $\pm$ 0.026 & \cellcolor{Gray} 68.783 $\pm$ 0.222 \\
\hline
\multirow{5}{*}{ \makecell[tl]{ \textbf{KEGG} \\ $(48827, 20)$} } & NN & 0.132 $\pm$ 0.021 & 0.759 $\pm$ 0.006 & \textbf{21.856 $\pm$ 0.168} & 0.124 $\pm$ 0.009 & 0.758 $\pm$ 0.004 & \textbf{20.989 $\pm$ 0.132} & 0.127 $\pm$ 0.008 & 0.766 $\pm$ 0.019 & \textbf{23.143 $\pm$ 0.710} \\
& NN+SVGP & 0.128 $\pm$ 0.005 & 0.758 $\pm$ 0.001 & 43.788 $\pm$ 1.377 & 0.129 $\pm$ 0.013 & 0.759 $\pm$ 0.002 & 40.996 $\pm$ 0.280 & 0.127 $\pm$ 0.009 & 0.758 $\pm$ 0.002 & 46.882 $\pm$ 1.290 \\
& SV-DKL & 0.134 $\pm$ 0.011 & 0.766 $\pm$ 0.013 & 269.598 $\pm$ 5.733 & 0.139 $\pm$ 0.031 & 0.769 $\pm$ 0.025 & 271.083 $\pm$ 7.502 & 0.152 $\pm$ 0.024 & 0.780 $\pm$ 0.025 & 275.013 $\pm$ 2.443 \\
& AV-DKL & 0.127 $\pm$ 0.008 & 0.758 $\pm$ 0.002 & 92.000 $\pm$ 0.227 & 0.130 $\pm$ 0.009 & 0.759 $\pm$ 0.002 & 129.314 $\pm$ 1.607 & 0.130 $\pm$ 0.008 & 0.759 $\pm$ 0.002 & 254.537 $\pm$ 4.020 \\
& \cellcolor{Gray}\raggedright DAK-MC & \cellcolor{Gray} 0.126 $\pm$ 0.010 & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 90.473 $\pm$ 0.144 & \cellcolor{Gray} 0.124 $\pm$ 0.010 & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 92.507 $\pm$ 0.964 & \cellcolor{Gray} 0.123 $\pm$ 0.009 & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 113.738 $\pm$ 1.251 \\
& \cellcolor{Gray}\raggedright DAK-CF & \cellcolor{Gray} \textbf{0.124 $\pm$ 0.007} & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 65.070 $\pm$ 0.140 & \cellcolor{Gray} \textbf{0.122 $\pm$ 0.007} & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 79.732 $\pm$ 2.189 & \cellcolor{Gray} \textbf{0.121 $\pm$ 0.005} & \cellcolor{Gray} \textbf{0.748 $\pm$ 0.003} & \cellcolor{Gray} 78.702 $\pm$ 0.631 \\
\bottomrule[1pt]
\end{tabular}%
}
% \vspace{-0.2cm}
\end{table*}


We first evaluate a toy example of 1-Dimensional (1D) GP regression on synthetic data with zero mean and SE kernel $k(x,x')=\exp\left( -(x-x')^2 \right)$. The training set consists of $20$ noisy data points in the range of $[-7,7]$, while the test set consists of $100$ data points in $[-12,12]$. We consider a two-layer MLP with layer width $[64,32]$ as the feature extractor, letting $P=2$ be the number of \emph{base} GPs. To maintain a fair comparison, we use the same training recipe: full-batch training, a learning rate 0.01, and a number of optimization steps 1000. The only differences are the choice of the last layer and the loss function. We set the number of MC samples $S=4$ during training.

\Cref{fig:gp1d} -- \ref{fig:dak1d} shows the predictive posterior of the exact GP, two-layer DGP, exact DKL, and proposed DAK. We observe that DAK has good both in-sample and out-of-sample predictions, which is close to exact GP posterior, while DGP is unbiased but over-confident outside the training data. Exact DKL is biased and overconfident in out-of-sample predictions, which were also observed and investigated by \citet{ober2021promises}. We also compare DAK with the DNN that shares the same feature extractor and model depth. It is evident that the DNN fit in \Cref{fig:nn1d} suffers from ``overfitting'': the fit shows significant extrapolation beyond the training data. 


\subsection{UCI Regression}
\label{subsec:uci reg}

We benchmark the regression task on six datasets from the UCI repository \citep{dua2017uci}: three smaller datasets with fewer than 10K samples (Wine, Gas, Parkinsons) and three larger datasets with 40K to 50K samples (Kin40K, Protein, KEGG). All models use the same neural network architecture: a fully connected network with two hidden layers of 64 and 32 neurons, respectively. Training is performed using the Adam optimizer over 100 epochs with a learning rate of $0.001$, weight decay of $0.0005$, and batch size of 512. The noise variance is set to $\sigma_{f}^2 = 0.01$.

For NN+SVGP, SV-DKL, and AV-DKL, we use SE kernels and 64 inducing points initialized with uniformly distributed random variables of a fixed seed. In the proposed DAK models, the induced grid $\Uv$ from \cref{eq:GPlayer} consists of $M = 7$ equally spaced points over the interval $(0,1)$ for each base GP, i.e., $\Uv = \{1/8, 2/8, \ldots, 7/8\}$. The NN outputs of both SV-DKL and our DAK model, which utilize an additive GP structure, are embedded and normalized into 16 base GPs over the interval $[0,1]$. That is, the number of projections $P$ in \cref{eq:deepadditive} is set to 16. For the DAK model, we evaluate two methods: DAK-CF, using the closed-form inference from \Cref{sec:uq of inference} and the closed-form ELBO from \Cref{sec:elbo}, and DAK-MC, using MC sampling to approximate the mean and variance during inference and approximate the expected log likelihood term in ELBO during training. The number of MC samples is set to 20 for inference and 8 for training, the same for NN+SVGP, SV-DKL, and AV-DKL. Further details can be found in \Cref{subsec:regression supp}.

We evaluate the performance of each model using training time (in seconds), Root Mean Squared Error (RMSE), and Negative Log Predictive Density (NLPD) with varying neural network output feature sizes of 16, 64, and 256. The results are averaged over 5-fold cross-validation for each dataset. As shown in \Cref{tab:uci metrics}, except for the smallest dataset, Wine, our models, DAK-MC and DAK-CF (highlighted in gray), consistently achieve the best RMSE and NLPD performance compared to other models. 

\begin{table*}[ht]
\caption{\small{ELBO, Accuracy, NLL, ECE for DAK, SV-DKL, NN+SVGP, NN on image classification tasks averaged over 3 runs. MNIST uses a simple CNN with 16 output features; CIFAR-10 uses ResNet-18 with 64 output features; CIFAR-100 uses ResNet-34 with 512 output features. The best results are highlighted in \textbf{bold}.}}
\centering
\vspace{-0.1cm}
\resizebox{\linewidth}{!}{%
\begin{tabular}{rcccclccc}
\toprule[1pt]
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Batch size: 128}  &  & \multicolumn{3}{c}{Batch size: 1024} \\ \cline{2-5} \cline{7-9} \vspace{-8pt} \\
\multicolumn{1}{l}{} & NN   & NN+SVGP & SV-DKL & \cellcolor{Gray} DAK-MC &   & NN+SVGP  & SV-DKL & \cellcolor{Gray} DAK-MC \\ 
\midrule[1pt]
MNIST - ELBO $\uparrow$        & \rule{0.5cm}{0.4pt}  &  -0.121 $\pm$ 0.000    & -0.054 $\pm$ 0.000  & \cellcolor{Gray} \textbf{-0.030 $\pm$ 0.001}   &     & -1.997 $\pm$ 0.001        &  -0.305 $\pm$ 0.001      &  \cellcolor{Gray} \textbf{-0.048 $\pm$ 0.001}  \\
Top-1 Acc. (\%) $\uparrow$       & 99.14 $\pm$ 0.00     & 98.56 $\pm$ 0.02       & 99.16 $\pm$ 0.00  & \cellcolor{Gray} \textbf{99.26 $\pm$ 0.01}      &      & 14.79 $\pm$ 0.08         & 96.23 $\pm$ 0.02       &  \cellcolor{Gray} \textbf{99.1 $\pm$ 0.00}    \\
NLL $\downarrow$             & 0.026 $\pm$ 0.000    & 0.064 $\pm$ 0.001     & 0.030 $\pm$ 0.000      & \cellcolor{Gray} \textbf{0.024 $\pm$ 0.000}   &    & 1.985 $\pm$ 0.003        &  0.288 $\pm$ 0.001      &  \cellcolor{Gray} \textbf{0.028 $\pm$ 0.004}      \\
ECE $\downarrow$          & 0.005 $\pm$ 0.000     & 0.012 $\pm$ 0.000    & 0.005 $\pm$ 0.000       & \cellcolor{Gray} \textbf{0.004 $\pm$ 0.000}           &      & 0.062 $\pm$ 0.002         & 0.020 $\pm$ 0.000       & \cellcolor{Gray} \textbf{0.006 $\pm$ 0.000} \\
\midrule[1pt]
CIFAR-10 - ELBO $\uparrow$      & \rule{0.5cm}{0.4pt}     & -1.038 $\pm$ 0.004        & -0.017 $\pm$  0.001      &  \cellcolor{Gray} \textbf{-0.002 $\pm$ 0.000}     &     & -1.039 $\pm$ 0.004         & -0.034 $\pm$ 0.021       & \cellcolor{Gray} \textbf{-0.002 $\pm$ 0.000} \\
Top-1 Acc. (\%) $\uparrow$    & 94.72 $\pm$ 0.13   & 77.35 $\pm$ 0.13  & 93.44 $\pm$ 0.28    &  \cellcolor{Gray} \textbf{94.81 $\pm$ 0.13}   &     &  16.90 $\pm$ 0.29        & 90.22 $\pm$ 1.42       & \cellcolor{Gray} \textbf{93.02 $\pm$ 0.18}        \\
NLL $\downarrow$     & \textbf{0.252 $\pm$ 0.025}      & 1.790 $\pm$ 0.001    & 0.312 $\pm$ 0.033       &  \cellcolor{Gray} 0.256 $\pm$ 0.014     &      & 2.270 $\pm$ 0.000         & 0.485 $\pm$ 0.061       & \cellcolor{Gray} \textbf{0.345 $\pm$ 0.001}    \\
ECE $\downarrow$      & 0.040 $\pm$ 0.002     & 0.061 $\pm$ 0.003    & 0.046 $\pm$ 0.003       &  \cellcolor{Gray} \textbf{0.039 $\pm$ 0.002}          &     & \textbf{0.027 $\pm$ 0.002}       & 0.060 $\pm$ 0.004       & \cellcolor{Gray} 0.052 $\pm$ 0.001           \\
\midrule[1pt]
CIFAR-100 - ELBO $\uparrow$      & \rule{0.5cm}{0.4pt}     & -4.605 $\pm$ 0.000        & -0.059 $\pm$ 0.000       &  \cellcolor{Gray} \textbf{-0.003 $\pm$ 0.000}      &     & -4.605 $\pm$ 0.000         & -0.103 $\pm$ 0.009       & \cellcolor{Gray} \textbf{-0.005 $\pm$ 0.001}   \\
Top-1 Acc. (\%) $\uparrow$    & 75.88 $\pm$ 0.54   & 1.04 $\pm$ 0.01  & 74.52 $\pm$ 0.13       & \cellcolor{Gray}  \textbf{76.75 $\pm$ 0.18}     &     &  1.10 $\pm$ 0.09        & 66.54 $\pm$ 0.74       & \cellcolor{Gray} \textbf{70.38 $\pm$ 1.25}        \\
NLL $\downarrow$     & 1.018 $\pm$ 0.021      & 4.605 $\pm$ 0.000    & 1.041 $\pm$ 0.007       & \cellcolor{Gray}  \textbf{1.001 $\pm$ 0.027}     &      & 4.605 $\pm$ 0.000    & 1.738 $\pm$  0.058      & \cellcolor{Gray} \textbf{1.203 $\pm$ 0.040}        \\
ECE $\downarrow$      & 0.086 $\pm$ 0.002     & \textbf{0.003 $\pm$ 0.001}    & 0.049 $\pm$ 0.002       & \cellcolor{Gray}  0.041 $\pm$ 0.004        &     & \textbf{0.003 $\pm$ 0.001}         & 0.148 $\pm$ 0.007       &\cellcolor{Gray}  0.056 $\pm$ 0.006           \\
\bottomrule[1pt]
\end{tabular}
}
% \vspace{-0.2cm}
\label{tab:img metrics}
\end{table*}

Although NN+SVGP takes less time than DAK models, its performance often degrades, with higher RMSE and NLPD as the size of the NN output features increases. In contrast, the DAK models show improved performance with larger NN output features. This is because SVGP has a fixed number of 64 inducing points, which limits its ability to approximate GPs effectively in high-dimensional spaces, making it less suitable for complex tasks requiring high-dimensional NNs, such as multitask regression or meta-learning.

Additionally, SV-DKL, which also uses an additive GP layer and KISS-GP to accelerate GP computations, takes significantly more time than our DAK models. This is because SV-DKL treats dependent inducing variables as parameters in VI, which requires more training time, while our DAK models treat the GP layers as sparse BNNs with independent Gaussian weights and biases under the mean-field assumption, leading to more efficient training. 

While AV-DKL occasionally achieves higher accuracy, it also requires substantially more training time than our DAK methods. By using inducing locations dependent on NN outputs to mitigate overcorrelation in DKL, AV-DKL significantly increases the ELBO’s computational complexity. Similar to NN+SVGP, its performance degrades when the size of the NN output features grows, because GP approximation in higher-dimensional spaces would naturally require a corresponding increase in the number of inducing points.


\subsection{Image Classification}
\label{subsec:image class}

\begin{table*}[ht]
\caption{\small{The number of total trainable parameters and average training time per epoch across different tasks for each model. NN has smaller set of parameters and less training time as is expected. DAK is more scalable than SV-DKL in terms of total trainable parameters and training time per epoch.}}
\centering
\vspace{-0.1cm}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lllllccccccccc}
\toprule[1pt]
\multirow{2}{*}{Datasets} & \multirow{2}{*}{$N$} & \multirow{2}{*}{$D$} & \multirow{2}{*}{$C$} & \multirow{2}{*}{$D_w$} & \multicolumn{4}{c}{\# parameters} &  & \multicolumn{4}{c}{Epoch time (sec.)} \\ \cline{6-9} \cline{11-14} \vspace{-8pt} \\
                          &                    &                    &                    &                    & NN  & NN+SVGP  & SV-DKL  & \cellcolor{Gray} DAK-MC &  & NN & NN+SVGP & SV-DKL & \cellcolor{Gray} DAK-MC \\
\midrule[1pt]
MNIST                     & 60K                & 28$\times$28                 & 10                 & 16                 & 1.19M    &  $+$2.72M        &  $+$0.08M       & \cellcolor{Gray}  \textbf{$+$0.03M}      &  & 6.18   &  $+$16.57       &  \textbf{$+$4.35}      & \cellcolor{Gray}  $+$4.57      \\
CIFAR-10                  & 50K                & 3$\times$32$\times$32                 & 10                 & 64                 & 11.17M    &  $+$29.58M        & $+$0.30M        & \cellcolor{Gray}  \textbf{$+$0.11M}      &  & 34.41   &  $+$22.92       &  $+$7.88      & \cellcolor{Gray}  \textbf{$+$5.02}      \\
CIFAR-100                 & 50K                & 3$\times$32$\times$32                 & 100                & 512                & 21.32M    &  $+$52.53M        &  $+$2.19M       & \cellcolor{Gray}  \textbf{$+$1.68M}     &  & 42.82   &  $+$101.77       & $+$16.61       & \cellcolor{Gray}  \textbf{$+$6.24}     \\
\bottomrule[1pt]
\end{tabular}
}
% \vspace{-0.2cm}
\label{tab:runtime}
\end{table*}

We next benchmark the classification task on high-dimensional and highly structured image data, including MNIST \citep{lecun1998mnist}, CIFAR-10, and CIFAR-100 \citep{krizhevsky2009learning}. All models share the same neural network head as feature extractors, to which we add either a linear output layer in NN model or corresponding GP output layers in NN+SVGP, SV-DKL, and DAK. In classification, last layer outputs are followed by a Softmax layer to normalize the output to a probability distribution, and we perform MC sampling to approximate the Softmax likelihood term in ELBO. 

We use the same setting of training for all models (refer to \Cref{tab:optimizer classification} in \Cref{subsec:classification supp} for details). For MNIST, we consider a simple CNN as the feature extractor, 20 epochs of training using Adadelta optimizer with initial learning rate 1.0, weight decay 0.0001, and annealing learning rate scheduler at each step with a factor of 0.7. For CIFAR-10, we perform full-training on a ResNet-18 \citep{he2016deep} with GP layers for 200 epochs, while for CIFAR-100, we use a pre-trained ResNet-34 as the feature extractor and fine-tune GP layers for 200 epochs since NN+SVGP and SV-DKL struggled to fit. In both CIFAR10/100, we use SGD optimizer with an initial learning rate of 0.1, weight decay of 0.0001, momentum 0.9, and cosine annealing learning rate scheduler. 

For NN+SVGP, we use SE kernel and $512$ inducing points, while for SV-DKL, we use $64$ inducing points initialized with uniformly distributed random variable within the interval $[-1,1]^{D_w}$. For the proposed DAK-MC, we use $M=63$ equally spaced points over the interval $[-1,1]$ for the induced grid $\Uv$ in each base GP. More details can be found in \Cref{subsec:classification supp}.

We evaluate all models in terms of Top-1 accuracy, NLL, ELBO, and expected calibration error (ECE) over three runs. As shown in \Cref{tab:img metrics}, our DAK-MC (highlighted in gray) achieves the best accuracy, ELBO, and NLL performance compared to other DKL models. Although some ECEs of NN+SVGP and SV-DKL are lower than those of DAK-MC, their accuracy degrades more with increasing dimensionality. The failure of SVGP in CIFAR-10/100 demonstrated the necessity of additive structure in high-dimensional multitask DKL. Additionally, we observe that SVDKL struggles more to fit in CIFAR-100, indicating the importance of pre-training in SVDKL, while our proposed DAK does not hurt by the curse of dimensionality because of the computational advantages of the last-layer BNN feature. We also repeated the experiments with a larger batch size of 1024. Our proposed DAK model is more robust than other DKL methods when the batch size and the number of features increases. Further experimental results are provided in \Cref{sec:additional exp results}. In \Cref{tab:runtime}, we measure the total number of trainable parameters and the average training time. NN has the smallest set of parameters and the least training time as expected. Among DKL methods, DAK is more efficient than SV-DKL when large-scale neural networks and high-dimensional tasks are applied.


% 1 to show, 0 to hide
% \ifthenelse{\equal{\showcontent}{0}}{
% \section{Discussions of Weekly Meetings}
% {\color{red}{
% \subsection{Sep 2.}
% \begin{enumerate}
%     \item Motivation: DKL -> GP -> Approximation -> Hybrid NN
%     \item add a table of comparison of time complexity: DKL with KISS-GP, DKL with additive sparse approximation (ours) (Will be done by Haoyuan)
%     \item add "related work" section, find the latest papers about Deep kernel learning on how to optimize the hyperparameters, whether write  GP layer as a Bayesian Neural Network (if there exist some papers that represent NN+GP as an entire NN). If not, it can be a motivation 
%     \item do we need to do Monte Carlo sampling when computing the predictive distribution computation through \Cref{eq:additiveDKL} and log-likelihood in ELBO in \Cref{eq:elbo}? 
% \end{enumerate}


% \subsection{Sep 9.}
% \begin{enumerate}
%     \item Table 1, we didn't show that KISS-GP scales up with the dimension $D$, should we quantify the performance via some simulations? If we fix the number of inducing points $M$, the performance of KISS-GP will be worse. Remove Table 1, discuss both the complexity and the performance. (will be done by Haoyuan)
%     \item Should we implement closed-form of ELBO in the experiment. Will using closed-form improve largely the computational time than Monte Carlo samplings? Make a comparison between with Monte Carlo sampling and without Monte Carlo sampling in the experiment. 
%     \item Experiments better before Sep 16
%     \begin{enumerate}
%         \item comparison of runtimes of for computing the ELBO between with Monte Carlo sampling and without Monte Carlo sampling  (Wenyuan)
%         \item add baselines: NN, SV-DKL(SVI+KISS-GP) (Wenyuan), NN+SVGP (Haoyuan).
%         \item UCI regression (Haoyuan), Image classification (Wenyuan)
%     \end{enumerate}
    
% \end{enumerate}

% \subsection{Sep 16}
% \begin{enumerate}
%     \item \st{retest DAK-MC}
%     \item Regression
%     \begin{enumerate}
%         \item Different batch size for the regression
%         \item Compare the results of different dimensions of NN outputs for regression, look into why Parkinsons dat performance (input has binary feature)
%         \item add Coverage metrics to regression
%     \end{enumerate}
%     \item Classification
%     \begin{enumerate}
%         \item fix NN+SVGP for classification
%         \item \st{fix ECE} 
%     \end{enumerate}
    
% \end{enumerate}


% \subsection{Sep 21}
% \begin{enumerate}
%     \item num NN out features = [16, 64, 256]
%     \item fix SV-DKL regression: NN+additive KISS-GP
%     \item synthetic data (similar to parkinsons has binary features and target is continuous): uci data may also work if we choose a continuous feature as output.
%     \item remove coverage rate, may come from the effects of the choices of the fixed noise variance $\sigma_{f}^2$
%     \item motivation? 1.When input is largely heterogenous (or has binary feature)? 2.When NN output is high dimensional. However, In what conditions we may need large NN out features? (Image set) 3. Multi-task regression?
%     \item why our model perform better than SV-DKL? Any theoretical explanations for it? (Reason better than NN+SVGP: additive structure, suitable for high-dimensional NN outputs)
% \end{enumerate}

% \subsection{Sep 23}
% \begin{enumerate}
%     \item last layer GP equivalent to BNN refer to the literature ``drop out as variational ''
% \end{enumerate}

%\subsection{Oct 7}
% \begin{enumerate}
%     \item stress that computation of additive kernel is not equal to computation of additive f
%     \item remove \mu
%     \item Add a remark, By approximation, we can take full advanrage of additive GPs, different from standard inducining points, add remark. If additive GP, no simplification for computation
% \item add last-layer BNN reference to related work
%  \item move related work to the last
% add more explanation to Figure in toy example
%    \item revise 'DAK-UQ' to DAK-CF
% \end{enumerate}
%\subsection{Oct 9}
% 
% replace Figure 2 (d)DNN with true GP posterior ground truth
% }}
% }{}


\section{CONCLUSION}
\label{sec:conc}
In this work, we introduced the DAK model, which reinterprets DKL as a hybrid NN, representing the last-layer GP as
% integrating NNs with 
a sparse BNN layer to address the scalability and training inefficiencies of DKL. 
% While DKL integrates NNs and GPs for UQ and feature extraction, it faces computational challenges, particularly in the GP layer and optimizing the ELBO. 
The DAK model overcomes limitations of GPs by embedding high-dimenional features from NNs into additive GP layers and leveraging the sparse Cholesky factor of the Laplace kernel on the induced grids, significantly enhancing training and inference efficiency. The proposed model also provides closed-form solutions for both the predictive distribution and ELBO in regression tasks, eliminating the need for costly MC sampling. Empirical results show that DAK outperforms state-of-the-art DKL models in both regression and classification tasks. This work also opens new possibilities for improving DKL by establishing the connection between BNNs and GPs. 

Possible directions for future work include considering more general GP layers other than the Laplace kernels and the additive structure, and exploring other variational families for training the BNNs.
%as finite-width BNNs, and further calibrating the uncertainty bounds to reduce overconfidence when neural network output features are highly correlated.

% 1 to show, 0 to hide
\ifthenelse{\equal{\showcontent}{1}}{
\subsubsection*{Acknowledgements}
The authors gratefully acknowledge the support provided by NSF grant DMS-2312173, the computing infrastructure provided by the Department of Electrical \& Computer Engineering at Texas A\&M University, and the reviewers' constructive feedback.}

\bibliographystyle{apalike}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}
 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [\textbf{Yes}/No/Not Applicable]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [\textbf{Yes}/No/Not Applicable]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [\textbf{Yes}/No/Not Applicable]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [\textbf{Yes}/No/Not Applicable]
   \item Complete proofs of all theoretical results. [\textbf{Yes}/No/Not Applicable]
   \item Clear explanations of any assumptions. [\textbf{Yes}/No/Not Applicable]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [\textbf{Yes}/No/Not Applicable] 
   
   
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [\textbf{Yes}/No/Not Applicable]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [\textbf{Yes}/No/Not Applicable]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [\textbf{Yes}/No/Not Applicable]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [\textbf{Yes}/No/Not Applicable]
   \item The license information of the assets, if applicable. [Yes/No/\textbf{Not Applicable}]
   \item New assets either in the supplemental material or as a URL, if applicable. [Yes/No/\textbf{Not Applicable}]
   \item Information about consent from data providers/curators. [Yes/No/\textbf{Not Applicable}]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Yes/No/\textbf{Not Applicable}]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Yes/No/\textbf{Not Applicable}]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Yes/No/\textbf{Not Applicable}]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Yes/No/\textbf{Not Applicable}]
 \end{enumerate}

 \end{enumerate}

\include{supplement}
\end{document}
