% \documentclass[twoside]{article}

% \usepackage{aistats2025}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% \begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\appendix
\aistatstitle{From Deep Additive Kernel Learning to Last-Layer \\ Bayesian Neural Networks via Induced Prior Approximation: \\
Supplementary Materials}

\section{SPARSE CHOLESKY DECOMPOSITION}
\label{sec:sparse chol decompose}
In this section, we present the algorithm for constructing the induced grids $\mathbf{U}$ as defined in \cref{eq:GPlayer} by using sorted dyadic points, and obtaining the sparse Choleksy decomposition of the Laplace kernel in one dimension, as proposed in \citep{ding2024sparse}.

A set of one-dimensional level-$L$ dyadic points $\Xv_L$ in increasing order over the interval $[0,1]$ is defined as:
\begin{align}
    \Xv_{L}:= \left\{ \frac{1}{2^{L}}, \frac{2}{2^{L}}, \frac{3}{2^{L}}, \ldots, \frac{2^{L}-1}{2^{L}} \right\}.
\end{align}
However, this increasing order does not yield a sparse representation of the Markov kernel $k(\cdot,\cdot)$ on the points $\Xv_L$, i.e., Cholesky decomposition of the covariance matrix $k(\Xv_L, \Xv_L)$ is not sparse. To achieve a sparse hierarchical expansion, we first sort the dyadic points $\Xv_L$ according to their levels.

\paragraph{Sorted Dyadic Points}
For level-$\ell$ dyadic points $\Xv_{\ell}$ where $ \ell=1,\ldots,L$, we first define the set $\rho(\ell)$ consisting of odd numbers as follows:
\begin{align}
    \rho(\ell) = \left\{ 1,3,5,\ldots,2^{\ell}-1 \right\}.
\end{align}
Next, we define the sorted incremental set $\Dv_{\ell}$ (with $\Xv_{0}:= \varnothing$) as:
\begin{align}
    \Dv_{\ell} = 
    \left\{ \frac{i}{2^{\ell}}: i\in \rho(\ell) \right\} = \Xv_{\ell} - \Xv_{\ell-1}, \quad  \ell=1,\ldots L.
\end{align}
Thus, the level-$L$ dyadic points $\Xv_L$ can be decomposed into disjoint incremental sets $\{ \Dv_{\ell} \}_{\ell=1}^{L}$:
\begin{align}
    \Xv_{L} = \cup_{\ell=1}^{L} \Dv_{\ell}, \quad \Dv_{i} \cap \Dv_{j} = \varnothing \text{ for $i\neq j$}.
\end{align}
Therefore, we can define the sorted level-$L$ dyadic points using these incremental sets as:
\begin{align}\label{eq:sorted dyadic}
    \Xv_{L}^{\text{sort}}:= \left\{ \Dv_1,\Dv_2, \ldots, \Dv_{L} \right\} 
    = \left\{ \frac{i \in \rho(\ell) }{2^{\ell}}, \ell=1,\ldots,L \right\}.
\end{align}
For example, the sorted level-3 dyadic points are given by:
\begin{align}
    \Xv_{3}^{\text{sort}} 
    = \bigg\{ 
    \begingroup
        \color{blue}
        \underbracket{
            \color{black}
            \frac{1}{2^1}
        }_{\color{blue}
            \Dv_1
        }
    \endgroup
    , 
    \begingroup
        \color{blue}
        \underbracket{
            \color{black}
            \frac{1}{2^2}, \frac{3}{2^2}
        }_{\color{blue}
            \Dv_2
        }
    \endgroup
    ,
    \begingroup
        \color{blue}
        \underbracket{
            \color{black}
            \frac{1}{2^3}, \frac{3}{2^3}, \frac{5}{2^3}, \frac{7}{2^3}
        }_{\color{blue}
            \Dv_3
        }
    \endgroup
     \bigg\}.
\end{align}

\paragraph{Algorithm}
We now present the algorithm for computing the inverse of the upper triangular Cholesky factor $[ \Lv_{\Xv_{L}^{\text{sort}}}^{\top} ]^{-1}$ of the covariance matrix $k(\Xv_{L}^{\text{sort}}, \Xv_{L}^{\text{sort}})$ in \Cref{alg:cholesky}, where $\Lv_{\Xv_{L}^{\text{sort}}} \Lv_{\Xv_{L}^{\text{sort}}}^{\top} = k(\Xv_{L}^{\text{sort}}, \Xv_{L}^{\text{sort}})$.. The corresponding proof can be found in \citep{ding2024sparse}. The output of \Cref{alg:cholesky} is a sparse matrix with $\Oc(3 \cdot (2^{L}-1))$ nonzero entries. Since each iteration of the for-loop only requires solving a $3 \times 3$ linear system, which costs $\Oc(3^3)$ time, the total computational complexity of \Cref{alg:cholesky} is $\Oc(2^L-1)$. This implies that the complexity of computing $\left[ \Lv_{\Uv}^{\top} \right]^{-1}$ in \cref{eq:GPlayer} is $\Oc(M)$ when $\Uv$, the induced grid of size $M$, consists of sorted dyadic points as defined in \cref{eq:sorted dyadic}.

\begin{algorithm}[hbt!]
\caption{Computation of the inverse Cholesky factor for the Markov kernel $k(\cdot, \cdot)$ on sorted one-dimensional level-$L$ dyadic points $\Xv_L^{\text{sort}}$.}
\label{alg:cholesky}
\setstretch{0.99} % set the line spacing to 0.99
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} Markov kernel $k(\cdot,\cdot)$, sorted level-$L$ dyadic points $\Xv_{L}^{\text{sort}}$
    \STATE {\bfseries Output:} inverse of the upper triangular Cholesky factor $\Rv:= [ \Lv_{\Xv_{L}^{\text{sort}}}^{\top} ]^{-1}$, s.t. $\Lv_{\Xv_{L}^{\text{sort}}} \Lv_{\Xv_{L}^{\text{sort}}}^{\top} = k(\Xv_{L}^{\text{sort}}, \Xv_{L}^{\text{sort}})$
    \STATE Initialize $\Rv \leftarrow \text{zeros($2^L-1$,$2^L-1$)}$;
    \STATE Define $k(\pm \infty, \cdot) = k(\cdot, \pm \infty) = 0$;
    \FOR{$\ell=1$ {\bfseries to} $L$}
        \FOR{$i \in \rho(\ell)=\{1,3,\ldots,2^{\ell}-1\}$}
            \STATE $x_{\text{mid}} := \frac{i}{2^{\ell}}$;\quad
            $x_{\text{left}}:=\frac{i-1}{2^{\ell}}$ {\bfseries if} $i>1$ {\bfseries else} $-\infty$;\quad
            $x_{\text{right}}:=\frac{i+1}{2^{\ell}}$ {\bfseries if} $i<2^{\ell}-1$ {\bfseries else} $+\infty$;
            \STATE Get $i_{\text{mid}}$, $i_{\text{left}}$, $i_{\text{right}}$, the indices of the points $x_{\text{mid}}$, $x_{\text{left}}$, $x_{\text{right}}$ in the sorted set $\Xv_{L}^{\text{sort}}$ respectively;
            \STATE Get the coefficients $c_1$, $c_2$, $c_3$ by solving the following linear system:
            \begin{align}
                \begin{bmatrix}
                     & k(x_{\text{left}}, x_{\text{left}})
                     & k(x_{\text{left}}, x_{\text{mid}})
                     & k(x_{\text{left}}, x_{\text{right}}) \\
                     & k(x_{\text{mid}}, x_{\text{left}})
                     & k(x_{\text{mid}}, x_{\text{mid}})
                     & k(x_{\text{mid}}, x_{\text{right}}) \\
                     & k(x_{\text{right}}, x_{\text{left}})
                     & k(x_{\text{right}}, x_{\text{mid}})
                    &k(x_{\text{right}}, x_{\text{right}})
                \end{bmatrix}
                \begin{bmatrix}
                    c1\\
                    c2\\
                    c3
                \end{bmatrix}=
                \begin{bmatrix}
                    0\\
                    1\\
                    0
                \end{bmatrix}.
            \end{align}
            \STATE $[c_1,c_2,c_3] := [c_1,c_2,c_3] / \sqrt{c_2}$;
            \STATE {\bfseries if} $x_{\text{left}} \neq - \infty$, 
            {\bfseries then} $\Rv[i_{\text{left}} ,i_{\text{mid}}] = c_1$; \quad
            {\bfseries if} $x_{\text{right}} \neq + \infty$, 
            {\bfseries then} $\Rv[i_{\text{right}} ,i_{\text{mid}}] = c_3$;
            \STATE $\Rv[i_{\text{mid}} ,i_{\text{mid}}] = c_2$;
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\section{REPARAMETERIZATION OF KERNEL LENGTHSCALES}
\label{sec:theo}
Considering the additive Laplace kernel with fixed lengthscale $\tilde{\theta}$ for all base kernels, applying linear projections $\left\{ \wv_{p}^{\top}\xv \right\}_{p=1}^{P}$ on inputs $\xv\in \Rb^D$ will give:
\begin{align}
    &\sum_{p=1}^{P}\sigma^2_p k_p\left( \wv^{\top}_{p}\xv,\wv^{\top}_{p}\xv^{\prime} \right)\nonumber \\
    = & \sum_{p=1}^{P} \sigma^2_p\exp \left( -  \frac{\sum_{d=1}^{D} \left| w_{p,d}\left( x_{d}-x_{d}^{\prime} \right) \right|}{\tilde{\theta}} \right)\nonumber \\
    = & \sum_{p=1}^{P} \prod_{d=1}^{D} \sigma^2_p\exp \left( - \frac{\left| x_{d}-x_{d}^{\prime} \right|}{\tilde{\theta} / \left| w_{p,d}\right| } \right)\nonumber \\
    = & \sum_{p=1}^{P} \prod_{d=1}^{D} \sigma^2_p\exp \left( - \frac{\left| x_{d}-x_{d}^{\prime} \right|}{\theta_{p,d}} \right),
\end{align}
This still leads to an additive Laplace kernel but with adaptive lengthscale $\theta_{p,d}$ for base kernels. The resulting kernel also retains \emph{sparse} Cholesky decomposition by the properties of Markov kernels so that the complexity of inference is $\Oc(M)$.

\section{INFERENCE OF PREDICTIVE DISTRIBUTION}
\label{sec:uq of inference}
Given an input $\xv \in \Rb^D$, the prediction of the DAK model can be written in the following equation according to \cref{eq:DAK prediction}: 
\begin{align}
    \tilde{f}_{\xv}
    &= \sum_{p=1}^{P}
    \sigma_p \Big(
        \phi(h_{\psi}^{[p]}(\xv)) \zv_p
    \Big) + \mu \nonumber\\
    &= \sum_{p=1}^{P}
    \sigma_p \Big(
        \bm{\phi}_{p}^{\top} \zv_p
    \Big) + \mu,
\end{align}
where $\bm{\phi}_{p}^{\top}:=\phi(h_{\psi}^{[p]}(\xv)) \in \Rb^{1 \times M}$
% , $\mu_p:=\mu_p(h_{\psi}^{[p]}(\xv)) \in \Rb$
. We assume the variational distribution over the independent Gaussian weights $\zv_p \sim \Nc(\bm{m}_{\zv_p}, \Sv_{\zv_p})$ and the bias $\mu \sim \Nc(m_{\mu}, \sigma_{\mu}^2)$. Then it's straighforward to deduce that
\begin{align}
    \bm{\phi}_{p}^{\top} \zv_p + \mu 
    &\sim
    \Nc\left(
    \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p} + m_{\mu},\hspace{0.2em}
    \bm{\phi}_{p}^{\top} \Sv_{\zv_p} \bm{\phi}_{p} + \sigma_{\mu}^2
    \right), \\
    \sigma_p \left(
    \bm{\phi}_{p}^{\top} \zv_p 
    \right) + \mu
    & \sim
    \Nc\left(
    \sigma_p ( \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p} )+ m_{\mu} ,\hspace{0.2em}
    \sigma_p^2( \bm{\phi}_{p}^{\top} \Sv_{\zv_p} \bm{\phi}_{p}) + \sigma_{\mu}^2
    \right), \\
    \tilde{f}_{\xv} = 
    \sum_{p=1}^{P}
    \sigma_p \left(
    \bm{\phi}_{p}^{\top} \zv_p
    \right) + \mu
    & \sim
    \Nc\left(
    \sum_{p=1}^{P}
    \sigma_p ( \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p}) + m_{\mu} ,\hspace{0.2em}
    \sum_{p=1}^{P}
    \sigma_p^2( \bm{\phi}_{p}^{\top} \Sv_{\zv_p} \bm{\phi}_{p} ) + \sigma_{\mu}^2
    \right).
\end{align}
Therefore, we obtain the predictive distribution of the $\tilde{f}(\xv)$ at the point $\xv \in \Rb^D$ and its mean and variance are given by:
\begin{subequations}
\label{eq:dak inference closed form}
\begin{align}
    \Eb\left[ \tilde{f}_{\xv} \right]
        = \sum_{p=1}^{P}
        \sigma_p ( \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p}) + m_{\mu},
\end{align}
\begin{align}
    \text{Var}\left[ \tilde{f}_{\xv} \right]
        =\sum_{p=1}^{P}
        \sigma_p^2( \bm{\phi}_{p}^{\top} \Sv_{\zv_p} \bm{\phi}_{p}) + \sigma_{\mu}^2.
\end{align}
\end{subequations}
% \begin{subequations}
% \label{eq:dak inference closed form}
%     \begin{align}
%         \Eb\left[ \tilde{f}(\xv) \right]
%         = \sum_{p=1}^{P}
%         \sigma_p ( \bm{\phi}_{p}^{\top} \bm{m}_{\zv_p} + m_{\mu_p} ),
%     \end{align}
%     \begin{align}
%         \text{Var}\left[ \tilde{f}(\xv) \right]
%         =\sum_{p=1}^{P}
%         \sigma_p^2( \bm{\phi}_{p}^{\top} \Sigma_{\zv_p} \bm{\phi}_{p} + \sigma_{\mu_p}^2).
%     \end{align}
% \end{subequations}


\section{TRAINING OF VARIATIONAL INFERENCE}
\label{sec:training}
Given the dataset $\mathcal{D}=\{ \Xv, \yv \}$ where $\Xv:=\{ \xv_i \}_{i=1}^N$, $\yv=(y_1,\ldots,y_N)^{\top}$, $\xv_i \in \Rb^D$, $y_i\in\Rb$, the prediction $\tilde{f}_{\Xv}\in \Rb^N$ of DAK is given by all the parameters $\bm{\theta}=\left\{ \psi, \bm{\sigma} \right\}$, $\bm{\eta}=\left\{ \{ \mv_{\zv_{p}},\Sv_{\zv_{p}}\}_{p=1}^{P} , \{m_{\mu},\sigma_{\mu} \} \right\}$ according to \cref{eq:DAK prediction}:
\begin{align}
    \tilde{f}_{\Xv}:= \tilde{f}(\Xv; \bm{\theta}, \bm{\eta})
    = \sum_{p=1}^{P}
    \sigma_p \Big(
        \phi(h_{\psi}^{[p]}(\Xv)) \zv_p
    \Big) + \mu,
\end{align}
where $\zv_{p} \sim \mathcal{N} (\bm{m}_{\zv_p} ,\Sv_{\zv_p})$, $p=1,\ldots,P$, and $\mu \sim \mathcal{N} ( m_{\mu},\sigma^2_{\mu} )$ are variational variables $\Theta_{\text{var}}$ parameterized by $\bm{\eta}$. The variational distribution is denoted by $q_{\bm{\eta}}(\Theta_{\text{var}})= q(\mu)\prod_{p=1}^{P} q(\zv_{p}) = \Nc ( m_{\mu} ,\sigma_{\mu}^2 )\prod_{p=1}^{P} 
\Nc ( \bm{m}_{\zv_p} ,\Sv_{\zv_p} )$, and the variational prior is denoted by $p(\Theta_{\text{var}})$.

We consider the KL divergence between $q_{\bm{\eta}}(\Theta_{\text{var}})$ and the true posterior $p(\Theta_{\text{var}}\vert \yv, \Xv, \bm{\theta})$:
\begin{align}
& \qquad \text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}}) \| p(\Theta_{\text{var}} \vert \yv,\Xv, \bm{\theta} ) \right] \nonumber \\
= & \int q_{\bm{\eta}}(\Theta_{\text{var}} )\log \frac{q_{\bm{\eta}}(\Theta_{\text{var}} )}{p(\Theta_{\text{var}} \vert \yv,\Xv,\bm{\theta} )} d\Theta_{\text{var}} \nonumber \\
= & \int q_{\bm{\eta}}(\Theta_{\text{var}} )\log \frac{q_{\bm{\eta}}(\Theta_{\text{var}} )p(\yv \vert \Xv,\bm{\theta})}{p(\yv \vert \Xv,\bm{\theta} ,\Theta_{\text{var}} )p(\Theta_{\text{var}} )} d\Theta_{\text{var}} \nonumber \\
= & \int q_{\bm{\eta}}(\Theta_{\text{var}} )\log \frac{q_{\bm{\eta}}(\Theta_{\text{var}} )}{p(\Theta_{\text{var}} )} d\Theta_{\text{var}} -\int q_{\bm{\eta}}(\Theta_{\text{var}} )\log p(\yv \vert \tilde{f}_{\Xv} )d\Theta_{\text{var}} +\log p(\yv\vert \Xv,\bm{\theta}).
\end{align}
Using the fact that $\text{KL}[\cdot \| \cdot] \geq 0$, we have
\begin{align}
\label{eq:variational lower bound}
    \log p(\yv\vert \Xv,\bm{\theta}) & \geq \int q_{\bm{\eta}}(\Theta_{\text{var}} )\log p(\yv \vert \tilde{f}_{\Xv} )d\Theta_{\text{var}} - \text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}} ) \| p(\Theta_{\text{var}}) \right] \nonumber \\
    & = \Eb_{q_{\bm{\eta}}(\Theta_{\text{var}} )} \left[ \log p(\yv \vert \tilde{f}_{\Xv} ) \right] - \text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}} ) \| p(\Theta_{\text{var}}) \right].
\end{align}

\paragraph{Full-training.}
Firstly, we present the joint training of $\bm{\theta}$ and $\bm{\eta}$. The most common approach optimizes the marginal log-likelihood (the left-hand side of \cref{eq:variational lower bound}):
\begin{align}
    \bm{\theta}^{\ast} &=\argmax_{\bm{\theta}} \log p(\yv\vert \Xv,\bm{\theta} ) \\
    &= \argmax_{\bm{\theta}} \log \int p\left( y\vert X,\bm{\theta},\Theta_{\text{var}} \right) p(\Theta_{\text{var}})d\Theta_{\text{var}},
\end{align}
which involves intractable integral in some tasks such as classification. Instead, we optimize the variational lower bound (the right-hand side of \cref{eq:variational lower bound}):
\begin{align}
    \Theta^{\ast} := \argmax_{\bm{\theta},\bm{\eta}} \mathcal{L}(\bm{\theta},\bm{\eta}) =\argmax_{\bm{\theta},\bm{\eta}}\left\{ E_{q_{\bm{\eta}}(\Theta_{\text{var}} )}\left[ \log p(\yv|\tilde{f}_{\Xv} ) \right] -\text{KL} \left[ q_{\bm{\eta}}(\Theta_{\text{var}} )\| p(\Theta_{\text{var}} ) \right] \right\}.
\end{align}

\paragraph{Fine-tuning.}
An alternative training approach is to firstly pre-train the deterministic parameters of feature extractor by standard neural network training, with mean squared error for regression or cross-entropy for classification as the loss function, and then fine-tune the last layer additive GP with fixed features. The objective function is identical to \cref{eq:elbo}, but $\bm{\theta}$ is learned during the pre-training step and is no longer optimized during fine-tuning.


\section{ELBO}%{DERIVATION OF ELBO}
\label{sec:elbo}
\subsection{Assumptions}
Consider the model $y_i = \tilde{f}(\xv_i) + \epsilon_i$ with the i.i.d. noise $\epsilon_i \overset{\text{i.i.d.}}{\sim} \Nc(0, \sigma_{f}^2)$ and $\tilde{f} : \Rb^D \rightarrow \Rb$ is defined in \cref{eq:DAK prediction}. The training dataset is $\mathcal{D} = \{ \Xv, \yv \}$ where $\Xv:=\{ \xv_i \}_{i=1}^N$, $\yv=(y_1,\ldots,y_N)^{\top}$, $\xv_i \in \Rb^D$, $y_i\in\Rb$. $\Theta_{\text{var}}:= \{ \mu ,\{ \zv_{p}\}_{p=1}^{P} \}$ are the variational random variables consisting of Gaussian weights and bias of $P$ units, $\psi$ are the parameters of the NN, $\bm{\sigma}:=(\sigma_1, \ldots, \sigma_p)^{\top}$ are the scale parameters of base GP layers. The variational distributions are $q(\mu)=\Nc(m_{\mu}, \sigma_{\mu}^2)$, $q(\zv_p)=\Nc(\bm{m}_{\zv_p}, \Sv_{\zv_p})$ and the variational priors are $p(\mu)=\Nc(\check{m}_{\mu} ,\check{\sigma}^2_{\mu})$, $p(\zv_p)=\Nc(\check{\bm{m}}_{\zv_p} ,\check{\Sv}_{\zv_p})$. Note that $\Sv_{\zv_p}\in\Rb^{M \times M}$ is a diagonal covariance matrix due to the independence of $\zv_p$, $M$ is the number of inducing points $\Uv$ defined in \cref{eq:GPlayer}, and $\bm{m}_{\zv_p} \in \Rb^M$, $m_{\mu} \in \Rb$, $\sigma_{\mu}^2 \in \Rb$. We derive the ELBO in VI to learn the preditive posterior over the variational variables $\Theta_{\text{var}}:= \{ \mu ,\{ \zv_{p}\}_{p=1}^{P} \}$ parameterized by $\bm{\eta}:=\left\{ \{ \mv_{\zv_{p}},\Sv_{\zv_{p}}\}_{p=1}^{P} , \{m_{\mu},\sigma_{\mu} \} \right\}$, and optimize the deterministic parameters $\bm{\theta}:=\{\psi, \bm{\sigma}\}$.

\subsection{Expected Log Likelihood}
\paragraph{Closed Form}
The \emph{expected log likelihood}, which is the first term in ELBO defined in \cref{eq:elbo}, is given by 
\begin{align}
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ \log \text{Pr} (\yv \vert \tilde{f}_{\Xv} ) \right]
    &= {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ 
    \log \prod_{i=1}^{N} 
    p (y_i \vert \tilde{f}_{\xv_i} )
    \right] \nonumber\\
    &= \sum_{i=1}^{N} 
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ 
    \log
    p (y_i \vert \tilde{f}_{\xv_i} )
    \right] \nonumber\\
    &= \sum_{i=1}^{N} 
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ 
    \log
    \Nc( \tilde{f}_i,\hspace{0.2em} \sigma_{f}^2 )
    \right] \nonumber\\
    &= \sum_{i=1}^{N} 
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ 
    \log \left(
    (2\pi \sigma_{f}^2)^{-\frac{1}{2}}
    \exp\left\{  
        -\frac{ (y_i - \tilde{f}_i)^2 }{2 \sigma_{f}^2}
    \right\}
    \right)
    \right] \nonumber\\
    &= \sum_{i=1}^{N} 
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[
    -\frac{1}{2} \log(2\pi) 
    - \frac{1}{2}\log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    (y_i - \tilde{f}_i)^2
    \right] \nonumber\\
    &= - \frac{N}{2} \log(2\pi)
    - \frac{N}{2} \log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    \sum_{i=1}^{N}
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[
    (y_i - \tilde{f}_i)^2
    \right] \nonumber\\
    &= - \frac{N}{2} \log(2\pi)
    - \frac{N}{2} \log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    \sum_{i=1}^{N} \left(
    \left({\Eb}_{q(\Theta_{\text{var}})} \left[
    (y_i - \tilde{f}_i)
    \right] \right)^2
    + \text{Var}_{q(\Theta_{\text{var}})} \left[
    (y_i - \tilde{f}_i)
    \right]
    \right) \label{eq:evidence halfway},
\end{align}
where
\begin{align}
    \tilde{f}_i
    % \mu_{\tilde{f}_i} &:= \tilde{f}(\xv_i;\Theta_{\text{var}}, \Theta_{\text{det}} ) \nonumber\\
    &= \sum_{p=1}^{P} \sigma_p \Big(
    \begingroup
        \color{blue}
        \underbracket{
            \color{black}
            \phi(h_{\psi}^{[p]}(\xv_i))
        }_{\color{blue}
            :=\bm{\phi}_{i,p}^{\top} \in \Rb^{1 \times M}
        }
    \endgroup
    \zv_p
    \Big)
    + \mu
    % \begingroup
    %     \color{blue}
    %     \underbracket{
    %         \color{black}
    %         \mu_{p}(h_{\psi}^{[p]}(\xv_i))
    %     }_{\color{blue}
    %         :=\mu_{i,p} \in \Rb
    %     }
    % \endgroup 
    \nonumber\\
    &= \sum_{p=1}^{P} \sigma_p \left(
    \bm{\phi}_{i,p}^{\top} \zv_p 
    \right) + \mu.
\end{align}
Recall that the variational assumptions $q(\zv_p)=\Nc(\bm{m}_{\zv_p}, \Sv_{\zv_p})$ and $q(\mu)=\Nc(m_{\mu}, \sigma_{\mu}^2)$, we can infer that
\begin{align}
    \bm{\phi}_{i,p}^{\top} \zv_p + \mu 
    &\sim
    \Nc\left(
    \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} + m_{\mu},\hspace{0.2em}
    \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} + \sigma_{\mu}^2
    \right), \\
    \sigma_p \left(
    \bm{\phi}_{i,p}^{\top} \zv_p 
    \right) + \mu
    & \sim
    \Nc\left(
    \sigma_p ( \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} ) + m_{\mu},\hspace{0.2em}
    \sigma_p^2( \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} ) + \sigma_{\mu}^2
    \right), \\
    \tilde{f}_i = 
    \sum_{p=1}^{P}
    \sigma_p \left(
    \bm{\phi}_{i,p}^{\top} \zv_p 
    \right)+ \mu
    & \sim
    \Nc\left(
    \sum_{p=1}^{P}
    \sigma_p ( \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} )+ m_{\mu},\hspace{0.2em}
    \sum_{p=1}^{P}
    \sigma_p^2( \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} ) + \sigma_{\mu}^2
    \right), \\
    y_i - \tilde{f}_i
    & \sim 
    \Nc\left(
    y_i - 
    \sum_{p=1}^{P}
    \sigma_p ( \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} ) -m_{\mu},\hspace{0.2em}
    \sum_{p=1}^{P}
    \sigma_p^2( \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} ) + \sigma_{\mu}^2
    \right).
\end{align}
Therefore, 
\begin{subequations}\label{eq:exp and var in evidence}
    \begin{align}
        \left({\Eb}_{q(\Theta_{\text{var}})} \left[
        (y_i - \tilde{f}_i)
        \right] \right)^2
        = \left(
         y_i - 
        \sum_{p=1}^{P}
        \sigma_p ( \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} ) -m_{\mu}
        \right)^2,
    \end{align}
    \begin{align}
        \text{Var}_{q(\Theta_{\text{var}})}
        \left[
        (y_i - \tilde{f}_i)
        \right]
        = \sum_{p=1}^{P}
        \sigma_p^2( \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} ) + \sigma_{\mu}^2.
    \end{align}
\end{subequations}
By applying \cref{eq:exp and var in evidence} to \cref{eq:evidence halfway}, we derive the analytical formula for the expected evidence, expressed as
\begin{align}
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ \log \text{Pr} (\yv \vert \tilde{f}_{\Xv} ) \right]
    &= - \frac{N}{2} \log(2\pi)
    - \frac{N}{2} \log(\sigma_{f}^2) \nonumber\\
    &- \frac{1}{2 \sigma_{f}^2}
    \sum_{i=1}^{N} \left(
        \Big(
         y_i - 
        \sum_{p=1}^{P}
        \sigma_p ( \bm{\phi}_{i,p}^{\top} \bm{m}_{\zv_p} ) -m_{\mu}
        \Big)^2
        + \sum_{p=1}^{P}
        \sigma_p^2( \bm{\phi}_{i,p}^{\top} \Sv_{\zv_p} \bm{\phi}_{i,p} )+ \sigma_{\mu}^2
    \right). \label{eq:evidence final}
\end{align}

\paragraph{Monte Carlo Approximation}
For comparison, we provide the equation for computing the Monte Carlo estimate of the ELBO in the paragraph that follows.
\begin{align}
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ \log \text{Pr} (\yv \vert \tilde{f}_{\Xv} ) \right]
    % &= {\Eb}_{q(\Theta)} \left[ 
    % \log \prod_{i=1}^{N} 
    % p (y_i \vert \xv_i,\Theta, \psi, \bm{\sigma})
    % \right] \nonumber\\
    &= \sum_{i=1}^{N} 
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}} )} \left[ 
    \log
    p (y_i \vert \tilde{f}_{\xv_i} )
    \right] \nonumber\\
    & \approx \sum_{i=1}^{N}
    \frac{1}{S}
     \sum_{s=1}^{S}
    \log
    p (y_i \vert \xv_i,\tilde{\Theta}^{(s)}_{\text{var}}, \bm{\theta} ) \nonumber\\
    &= \frac{1}{S} \sum_{i=1}^{N} 
    \sum_{s=1}^{S} 
    \log
    \Nc(y_i \left\vert\right. \tilde{f}_{i}^{(s)},\hspace{0.2em} \sigma_{f}^2 )
    \nonumber\\
    &= \frac{1}{S} \sum_{i=1}^{N} 
    \sum_{s=1}^{S} 
    \log \left(
    (2\pi \sigma_{f}^2)^{-\frac{1}{2}}
    \exp\left\{  
        -\frac{ (y_i - \tilde{f}_{i}^{(s)})^2 }{2 \sigma_{f}^2}
    \right\}
    \right)
    \nonumber\\
    &= \frac{1}{S} \sum_{i=1}^{N} 
    \sum_{s=1}^{S} \left(
    -\frac{1}{2} \log(2\pi) 
    - \frac{1}{2}\log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    (y_i - \tilde{f}_{i}^{(s)})^2
    \right) \nonumber\\
    &= - \frac{N}{2} \log(2\pi)
    - \frac{N}{2} \log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    \sum_{i=1}^{N}
    \frac{1}{S} \sum_{s=1}^{S}
    (y_i - \tilde{f}_{i}^{(s)})^2, \label{eq:evidence halfway mc approx}
\end{align}
where $S$ is the number of Monte Carlo samples, $\{  \tilde{\mu}^{(s)} ,\{ \tilde{\zv}_{p}^{(s)} \}_{p=1}^{P} \} := \tilde{\Theta}^{(s)}_{\text{var}}$ are the $s$-th Monte Carlo samplings over the variational parameters $\Theta_{\text{var}}$ and $\tilde{\Theta}^{(s)}_{\text{var}} \sim q_{\bm{\eta}}(\Theta_{\text{var}})$, $\tilde{f}_{i}^{(s)}$ is given as follows:
\begin{align}
    \tilde{f}_{i}^{(s)} &:= \tilde{f}(\xv_i;\tilde{\Theta}^{(s)}_{\text{var}},\bm{\theta} ) \nonumber\\
    &= \sum_{p=1}^{P} \sigma_p \Big(
    \begingroup
        \color{blue}
        \underbracket{
            \color{black}
            \phi(h_{\psi}^{[p]}(\xv_i))
        }_{\color{blue}
            :=\bm{\phi}_{i,p}^{\top} \in \Rb^{1 \times M}
        }
    \endgroup
    \tilde{\zv}_p^{(s)} 
    \Big) + \tilde{\mu}^{(s)} \nonumber\\
    &= \sum_{p=1}^{P} \sigma_p \left(
    \bm{\phi}_{i,p}^{\top} \tilde{\zv}_p^{(s)} 
    \right)+ \tilde{\mu}^{(s)}. \label{eq:mc approx mean}
\end{align}
Therefore, we plug \cref{eq:mc approx mean} into \cref{eq:evidence halfway mc approx} and get the the Monte Carlo estimate of the ELBO written in the following formula:
\begin{align}
    {\Eb}_{q_{\bm{\eta}}(\Theta_{\text{var}})} \left[ \log \text{Pr} (\yv \vert \tilde{f}_{\Xv} ) \right]
    &\approx
    - \frac{N}{2} \log(2\pi)
    - \frac{N}{2} \log(\sigma_{f}^2)
    - \frac{1}{2 \sigma_{f}^2}
    \sum_{i=1}^{N}
    \frac{1}{S} \sum_{s=1}^{S}
    \Big(y_i - 
    \sum_{p=1}^{P} \sigma_p \left(
    \bm{\phi}_{i,p}^{\top} \tilde{\zv}_p^{(s)} 
    \Big)- \tilde{\mu}^{(s)}
    \right)^2, \label{eq:evidence final mc approx} \\
    \tilde{\zv}_p^{(s)} &\sim \Nc(\bm{m}_{\zv_p}, \Sv_{\zv_p}),\qquad
    \tilde{\mu}^{(s)} \sim \Nc(m_{\mu}, \sigma_{\mu}^2).
\end{align}


\subsection{KL Divergence}
Since we place Gaussian assumptions over the variational parameters $\Theta_{\text{var}}$,  the \emph{KL divergence}, which is the second term in ELBO defined in \cref{eq:elbo}, is then given by
\begin{align}
    \text{KL} \left[ q(\Theta_{\text{var}} ) \| p(\Theta_{\text{var}}) \right]
    &= \text{KL} \left[ q( \mu ,\{ \zv_{p}\}_{p=1}^{P} ) \Vert p( \mu ,\{ \zv_{p}\}_{p=1}^{P}) \right] \nonumber\\
    & =  
    \text{KL} \left[ q(\mu) \Vert p(\mu) \right] 
    + \sum_{p=1}^{P} 
    \text{KL} \left[ q(\zv_{p}) \Vert p(\zv_{p}) \right],
\end{align}

\begin{align}
     \text{KL} \left[ q(\mu) \Vert p(\mu) \right]
     = \frac{1}{2} \left(
     \frac{\sigma_{\mu}^2}{\check{\sigma}_{\mu}^2} 
     + \frac{(m_{\mu} - \check{m}_{\mu})^2}{\check{\sigma}_{\mu}^2} 
     -\log\left( \frac{\sigma_{\mu}^2}{\check{\sigma}_{\mu}^2} \right)
     -1
     \right),
\end{align}

\begin{align}
    \text{KL} \left[ q(\zv_{p}) \Vert p(\zv_{p}) \right]
    = \frac{1}{2} \sum_{i=1}^{M} \left(
     \frac{[\Sv_{\zv_p}]_{ii}}{[\check{\Sv}_{\zv_p}]_{ii}} 
     + \frac{([\bm{m}_{\zv_p}]_{i} - [\check{\bm{m}}_{\zv_p}]_i)^2}{[\check{\Sv}_{\zv_p}]_{ii}}
     -\log\left( 
     \frac{[\Sv_{\zv_p}]_{ii}}{[\check{\Sv}_{\zv_p}]_{ii}}  
     \right)
     -1
     \right),
\end{align}
where $[\Sv_{\zv_p}]_{ii}$ is the $(i,i)$-th element of the diagonal covariance matrix $\Sv_{\zv_p} \in \Rb^{M \times M}$, $[\bm{m}_{\zv_p}]_{i}$ is the $i$-th element of the mean vector $\bm{m}_{\zv_p} \in \Rb^M$, the approximated posteriors are $q(\mu)=\Nc(m_{\mu}, \sigma_{\mu}^2)$, $q(\zv_p)=\Nc(\bm{m}_{\zv_p}, \Sv_{\zv_p})$ and the priors are $p(\mu)=\Nc(\check{m}_{\mu} ,\check{\sigma}^2_{\mu})$, $p(\zv_p)=\Nc(\check{\bm{m}}_{\zv_p} ,\check{\Sv}_{\zv_p})$.

% \subsection{Performance Comparison}
% \label{sec:toy exp compare}
% We compare the perforamce of computing the ELBO in \cref{eq:elbo} by using closed form in \cref{eq:evidence final} and using Monte Carlo approximation in \cref{eq:evidence final mc approx} in a toy example.
% \textcolor{red}{Table or Figure to add if time available}


\subsection{Limitations of the Closed-Form ELBO}

The closed-form ELBO is only applicable to regression problems. In classification, applying the softmax function to $\tilde{f}(\xv;\bm{\theta}, \bm{\eta})$ results in a non-analytic predictive distribution, meaning the ELBO must still be computed via Monte Carlo sampling during training. Similarly, the closed-form expressions for the predictive mean and variance, as provided in \cref{eq:dak inference closed form} in \Cref{sec:uq of inference}, are not applicable to classification but only apply to regression problems.


\section{COMPUTATIONAL COMPLEXITY}
\label{sec:complexity}
In this section, we discuss the computational complexity of various DKL models compared to the proposed DAK method, focusing on the GP layer as the most computationally demanding component. \Cref{tab:complexity supp} shows the computational complexity of our model compared to other state-of-the-art GP and DKL methods.

\begin{table}[ht]
    \caption{Computational complexity of the DKL models for $N$ training points. The reported training complexity is for one iteration. $\hat{M}$ is the number of inducing points in SVGP and KISS-GP, while $M$ is the size of induced grids in DAK, $M < \hat{M}$. $S$ is the number of Monte Carlo samples, $B$ is the size of mini-batch, $D_w$ is the dimension of the NN outputs in DKL, $P$ is the dimension of the outputs after applying linear transformations to the NN outputs in the proposed DAK model. DAK-MC refers to the DAK model using Monte Carlo approximation, while DAK-CF refers to the DAK model using closed-form inference and ELBO.}
    \centering
    \begin{tabular}{lcc}
    \toprule[1pt]
                  & \textbf{Inference}       & \textbf{Training} (per iteration) \\
    \midrule[0.5pt]
    NN + SVGP     & $\Oc(\hat{M}^2 N)$    & $\Oc( S D_w MB + \hat{M}^3)$ \\
    NN + KISS-GP  & $\Oc(D_w \hat{M}^{1+\frac{1}{D_w}})$  & $\Oc(S D_w MB + D_w \hat{M}^{\frac{3}{D_w}})$ \\
    DAK-MC (ours) & $\Oc(SM)$       & $\Oc(SPMB + PM)$   \\
    DAK-CF (ours) & $\Oc(M)$        & $\Oc(PMB + PM)$    \\
    \bottomrule[1pt]
    \end{tabular}
    \label{tab:complexity supp}
\end{table}

\paragraph{Inference Complexity.}
In inference based on induced approximation, computing the multiplication of the inverse of the covariance matrix $k(\Uv, \Uv)$ and a vector takes $\Oc(\hat{M}^2N)$ time for $\hat{M}$ inducing points $\Uv$ and $N$ training points when using SVGP. This cost is reduced by KISS-GP to $\Oc(D \hat{M}^{1+\frac{1}{D}})$ by decomposing the covariance matrix into a Kronecker product of $D$ one-dimensional covariance matrices of the inducing points: $k(\Uv, \Uv) = \bigotimes_{d=1}^{D} k(\Uv^{[d]}, \Uv^{[d]})$. Despite the significant reduction on complexity, it requires inducing points $\Uv$ arranged on a Cartesian grid of size $\hat{M} = \prod_{d=1}^{D} \hat{M}_d$, where $\hat{M}_d$ is the number of inducing points in the $d$-th dimension. In high-dimensional spaces, fixing $\hat{M}$ leads to very small $\hat{M}_d$ per dimension, which can degrade model performance. To address this, we propose the DAK model via sparse finite-rank approximation, which employs an additive Laplace kernel for GPs. The inverse Cholesky factor $\Lv_{\Uv}^{\top}$ for one-dimensional induced grids $\Uv$ of size $M$, where $M < \hat{M}$, as defined in \cref{eq:GPlayer}, is sparse and can be computed in $\Oc(M)$ time.

\paragraph{Training Complexity.}
In training, VI requires computing the ELBO as described in \cref{eq:elbo}, which consists of two terms: the \emph{expected log likelihood} and the \emph{KL divergence} between the variational distributions and priors. 

1) The \emph{expected log likelihood} is usually approximated via Monte Carlo sampling at a cost of $\Oc(S N_{\Theta} N)$, where $S$ is the number of Monte Carlo samples, $N_{\Theta}$ is the total number of variational parameters $\Theta_{\text{var}}$, and $N$ is the number of training points. This complexity can be reduced to $\Oc(S N_{\Theta} B)$ by applying stochastic variational inference with a mini-batch of size $B \ll N$. For DKL models using SVGP and KISS-GP, $\Theta_{\text{var}}$ are inducing variables, and the expectation does not have a closed form, requiring Monte Carlo sampling. In contrast, in the proposed DAK model, $\Theta_{\text{var}}= \{ \{ \zv_{p}\}_{p=1}^{P}, \mu \}$ consists of independent Gaussian weights $\zv_p\in \Rb^M$ and bias $\mu$. This allows us to derive an analytical form for this term, as shown in \cref{eq:evidence final} in \Cref{sec:elbo}, reducing the computational cost to $\Oc(N_{\Theta} B) = \Oc(PM B)$ when using a mini-batch of size $B$.

2) The \emph{KL divergence} between two Gaussian distributions can be computed in closed form. This leads to a linear time complexity of $\Oc(N_{\Theta})$ if the parameters $\Theta_{\text{var}}$ are independent, or cubic time $\Oc(N_{\Theta}^3)$ if they are fully correlated. In SVGP and KISS-GP, $\Theta_{\text{var}}$ represents fully correlated Gaussian distributed inducing variables, so computing the KL divergence takes $\Oc(\hat{M}^3)$ for SVGP. In KISS-GP, this can be reduced to $\Oc(D \hat{M}^{\frac{3}{D}})$ using fast eigendecomposition of Kronecker matrices. In the DAK model, the weights $\{\zv_p\}_{p=1}^{P}$ as defined in \cref{eq:GPlayer} are independent Gaussian random variables, allowing the KL divergence to be computed in $\Oc(N_{\Theta}) = \Oc(PM)$ time, where $P$ is the number of base GP layers.


\section{ADDITIONAL DISCUSSIONS}

Although interpretability is one advantage of additive models, the main motivation for replacing a GP layer with an additive GP layer in our work is to handle high-dimensional data. When the input dimension is low, it is reasonable that GPs are superior to additive GPs since the additive kernel is an approximated and restrictive kernel. However, when the input dimension increases, the computational complexity grows considerably even in GPs with sparse approximation. For example, in DKL, the output dimension of NN encoder is usually chosen as small as 2, while in pixel data experiments, DKL cannot handle the computation associated with the dimensionality when the output dimension of ResNet is 512 or more. Although DKL is superior in low-dimensional and simple cases, we view additive structure as a necessary component to achieve scalability and good performance with high-dimensional data.

\subsection{Why choosing the induced grids instead of learning the inducing points?}

From an approximation accuracy point of view, there are two separate strategies to increase the accuracy. The first one is to learn the inducing point locations. The second one, however, is to simply increase the number of inducing points on a pre-specified finer grid. The second method is much easier to implement and has a theoretical guarantee by the GP regression theory: as the inducing points become dense in the input region, the approximation will become exact. In contrast, the first approach does not have such a favorable theoretical guarantee. 

The second approach would become difficult to use for many existing methodologies as in general the computational cost would scale as $\mathcal{O}(M^3)$ with $M$ inducing points, which is particularly problematic in high dimensions. 
% The first approach can be viewed as a compromise in those situations, and that is why many existing methods chose to learn the locations of the inducing points instead.
This difficulty is resolved by additive GPs, since approximating an additive GP boils down to approximating one dimensional GPs, which can be accomplished by using a set of pre-specified inducing points on a fine grid in 1-D. One major benefit of the proposed methodology is that the computation now scales at $\mathcal{O}(M)$, enabled by the Markov kernel and the additive kernel. Therefore, a large number of inducing points can be used in an efficient way. 

The proposed method also has several additional benefits: 1) It can decouple to some extent the neural network component and GP component by avoiding learning the inducing points, which may help reduce overfitting/overconfidence; 2) The equivalence to BNN holds exactly with the fixed inducing points, whereas for learned inducing points, this BNN equivalence breaks down, and the proposed computation/training framework would not be possible to carry through; 3) It can simplify the overall optimization since there is no need to learn the inducing points.

\subsection{Limitations and future directions}

Generally, a finer grid will lead to better approximations, but the number of parameters to be trained will also increase. Therefore, there is a trade-off between the accuracy and the computational cost that we can afford. This current work is using a specific Laplace kernel, which can utilize sparse Cholesky decomposition. More general kernels may result in more computational complexity but better representation power of the model. In addition, the current variational family is restricted under mean-field assumptions. A more general variational family, e.g. full/low-rank covariance, may lead to superior performance in some applications. 


\section{EXPERIMENTAL DETAILS}
\label{sec:expdetail}
In this section, we provide additional details regarding the experiments.

\subsection{Benchmarks for Regression}
\label{subsec:regression supp}
\paragraph{Experiment Setup}
For all models, the NN architecture is a fully connected NN with rectified linear unit (ReLU) activation function \citep{nair2010rectified} and two hidden layers containing 64 and 32 neurons, respectively, structured as $D \rightarrow 64 \rightarrow 32 \rightarrow D_w$, where $D$ is the input feature size (also the size of input $\Xv$) and $D_w$ is the output feature size. The models are evaluated with $D_w=16$, 64, and 256, respectively. The number of Monte Carlo samples is set to 8 during training and 20 during inference.

The NN is a deterministic model, and we use the negative Gaussian log-likelihood as the loss function to quantify the uncertainty of the NN outputs and compute the NLPD.

For NN+SVGP, the inducing points are set to the size of 64 in $D_w$ dimension. We implement the \texttt{ApproximateGP} model in GPyTorch \citep{gardner2018gpytorch}, defining the inducing variables as variational parameters, and use \texttt{VariationalELBO} in GPyTorch to perform variational inference and compute the loss.

SV-DKL is originally designed for classification, so for a fair comparison in regression tasks, we modify it by first applying a linear embedding layer $\Wv: \Rb^{D_w} \rightarrow \Rb^P$ with $P=16$ and normalizing the outputs to the interval $[0,1]$ for each base GP, similar to the DAK model. To adapt the additive GP layer for regression, we remove the softmax function from the model in eq. (1) of \citep{wilson2016stochastic}. Given training data $\{ \xv_i, \yv_i \}_{i=1}^{N}$, the model is modified as follows:
\begin{align}
    p(\yv_i \vert \fv_i, A) = \mathcal{A}(\fv_i)^{\top} \yv_i
\end{align}
where $\fv_i \in \Rb^P$ is a vector of independent GPs followed by a linear mixing layer $\mathcal{A}(\fv_i) = A \fv_i$, with $A \in \Rb^{C \times P}$ as the transformation matrix. Here, $C=1$ for single-task regression. For each $p$-th GP ($1 \leq p \leq P$) in the additive GP layer, the corresponding inducing variables $\uv_p$ are set to the size of 64 and treated as variational parameters for training. We use the \texttt{GridInterpolationVariationalStrategy} model with \texttt{LMCVariationalStrategy} in GPyTorch to perform KISS-GP with variational inducing variables, augmented by a linear mixing layer.

For AV-DKL, the inducing points are set to size of 64 in $D_{w}$ dimension. We implement the AV-DKL model based on the source code~\cite{matias2024amortized}.

Both DAK-MC and DAK-CF use the same additive GP layer size as SV-DKL, with $P=16$, and employ fixed induced grids $\Uv = \{1/8, 2/8, \ldots, 7/8\}$ of size 7 for each base GP, which is much smaller than that of SV-DKL.

\paragraph{Metrics}
Let $\{\xv_t, y_t\}_{t=1}^{T}$ represent a test dataset of size $T$, where $\mu_t$ and $\sigma_t^2$ are the predictive mean and variance. We evaluate model performance using two common metrics: Root Mean Squared Error (RMSE) and Negative Log Predictive Density (NLPD).

RMSE is widely used to assess the accuracy of predictions, measuring how far predictions deviate from the true target values. It is calculated as:
\begin{align}
    \text{RMSE} = \sqrt{ \frac{1}{T} \sum_{t=1}^{T}(y_t - \mu_t)^2 }.
\end{align}

NLPD is a standard probabilistic metric for evaluating the quality of a model's uncertainty quantification. It represents the negative log likelihood of the test data given the predictive distribution. For GPs, NLPD is calculated as:
\begin{align}
    \text{NLPD}
    &= - \sum_{t=1}^{T} \log p(y_t = \mu_t \vert \xv_t) \\
    &= \frac{1}{T}
    \sum_{t=1}^{T} \Big[
    \frac{(y_t - \mu_t)^2}{2\sigma_t^2} + \frac{1}{2} \log(2\pi \sigma_t^2)
    \Big].
\end{align}
Both RMSE and NLPD are widely used in the GP regression literature, where smaller values indicate better model performance.

\paragraph{Computing Infrastructure}
The experiments for regression were run on Macbook Pro M1 with 8 cores and 16GB RAM.

\subsection{Benchmarks for Classification}
\label{subsec:classification supp}
We use PyTorch \citep{paszke2019pytorch} baseline of NN models, GPyTorch \citep{gardner2018gpytorch} baseline of SVGP and SV-DKL models. In classification tasks, we apply a softmax likelihood to normalize the output digits to probability distributions. The NN is a deterministic model trained via negative log-likelihood loss, while DKL and DAK models are trained via ELBO loss. The setting of all training tasks are described in \Cref{tab:model classification} and \Cref{tab:optimizer classification}.

SVGP is originally designed for single-output regression. To make it fit for multi-output classification, we used \texttt{IndependentMultitaskVariationalStrategy} in GPyTorch to implement the multi-task \texttt{ApproximateGP} model, and use \texttt{VariationalELBO} with \texttt{SoftmaxLikelihood} in GPyTorch to perform variational inference and compute the loss. 

For SV-DKL, we employed the same \texttt{VariationalELBO} with \texttt{SoftmaxLikelihood} as the variational loss objective. \texttt{GridInterpolationVariationalStrategy} is applied within \texttt{IndependentMultitaskVariationalStrategy} to perform additive KISS-GP approximation. For each KISS-GP unit, we used $64$ variational inducing points initialized on a grid of size $[-1,1]$. 

For DAK, we implemented DAK-MC using Monte Carlo estimation given the intractable softmax likelihood. We employed fixed induced grids $\Uv=\{ -31/32, -30/32, \ldots, 30/32, 31/32 \}$ of size 63 for each base GP component.

\begin{table}[ht]
\caption{Model architectures for image classification on MNIST, CIFAR-10 and CIFAR-100.}
\centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{l|l|ccc}
\toprule[1pt]
Model                   & Hyper-parameter          & MNIST       & CIFAR-10    & CIFAR-100   \\
\midrule[0.5pt]
\multirow{4}{*}{NN+SVGP}   & Feature extractor        & CNN         & ResNet-18   & ResNet-34   \\
                        & NN out features $D_w$         & 128         & 512         & 512         \\
                        & Embedding features $P$               & 16          & 64          & 128         \\
                        & \# inducing points $\hat{M}$      & 512         & 512         & 512         \\
                        & \# epochs       & 20         & 200         & 200         \\
                        & Training strategy      & Full-training         & Full-training         & Fine-tuning         \\
\midrule[0.5pt]
\multirow{5}{*}{SV-DKL} & Feature extractor        & CNN         & ResNet-18   & ResNet-34   \\
                        & NN out features $D_w$         & 128         & 512         & 512         \\
                        & Embedding features $P$               & 16          & 64          & 128         \\
                        & \# inducing points $\hat{M}$      & 64          & 64          & 64          \\
                        & Grid bounds              & {[}-1,1{]} & {[}-1,1{]} & {[}-1,1{]} \\
                        & \# epochs       & 20         & 200         & 200         \\
                        & Training strategy       & Full-training         & Full-training         & Fine-tuning         \\
\midrule[0.5pt]
\multirow{4}{*}{DAK}    & Feature extractor        & CNN         & ResNet-18   & ResNet-34   \\
                        & NN out features $D_w$         & 128         & 512         & 512         \\
                        & Embedding features $P$               & 16          & 64          & 128         \\
                        & \# induced interpolation $M$ & 63          & 63          & 63         \\
                        & \# epochs       & 20         & 200         & 200         \\
                        & Training strategy      & Full-training         & Full-training         & Full-training         \\
\bottomrule[1pt]
\end{tabular}

}
\label{tab:model classification}
\end{table}

\paragraph{MNIST} We used a CNN implemented in PyTorch as the feature extractor: \texttt{Conv2d}(1,32,3) $\rightarrow$ \texttt{Conv2d}(32,64,3) $\rightarrow$ \texttt{MaxPool2d}(2) $\rightarrow$ \texttt{Dropout}(0.25) $\rightarrow$ \texttt{Linear}(9216,128) $\rightarrow$ \texttt{Dropout}(0.5). To make a fair comparison, for both SV-DKL and DAK, we applied an embedding module through a linear layer that transform $128$ output features into $P=16$ base GP channels. 

\paragraph{CIFAR-10} We used a ResNet-18 as the feature extractor followed by a linear embedding layer that compressed the $512$ output features into $P=64$ base GP channels. 

\paragraph{CIFAR-100} We used a pretrained ResNet-34 as the feature extractor for SV-DKL and fine-tuned GP output layers since SV-DKL struggled to fit using full-training. For proposed DAK, we used full-training. The number of base GP channels is selected as $P=128$. 

\begin{table}[ht]
\caption{Details of training optimizer for image classification on MNIST, CIFAR-10 and CIFAR-100.}
\centering
\resizebox{0.7\linewidth}{!}{

\begin{tabular}{l|ccc}
\toprule[1pt]
Optimization      & MNIST                                                             & CIFAR-10                                                                                                  & CIFAR-100                                                                                                 \\
\midrule[0.5pt]
Optimizer         & Adadelta                                                          & SGD                                                                                                       & SGD                                                                                                       \\
Initial lr.       & 1.0                                                               & 0.1                                                                                                       & 0.1                                                                                                       \\
Weight decay      & 0.0001                                                            & 0.0001                                                                                                    & 0.0001                                                                                                    \\
Scheduler         & StepLR                                                            & CosineAnnealingLR                                                                                         & CosineAnnealingLR                                                                                         \\
\midrule[0.5pt]
Data Augmentation & MNIST                                                             & CIFAR-10                                                                                                  & CIFAR-100                                                                                                 \\
\midrule[0.5pt]
RandomCrop        & -                                                                 & size=32, padding=4                                                                                        & size=32, padding=4                                                                                        \\
HorizontalFlip    & -                                                                 & p=0.5                                                                                                     & p=0.5                                                                                                     \\
% Normalization     & \begin{tabular}[c]{@{}l@{}}mean=0.1307,\\ std=0.3081\end{tabular} & \begin{tabular}[c]{@{}l@{}}mean={[}0.4914,0.4822,0.4465{]},\\ std={[}0.2023,0.1994,0.2010{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}mean={[}0.5071,0.4867,0.4408{]},\\ std={[}0.2675,0.2565,0.2761{]}\end{tabular} \\
\bottomrule[1pt]
\end{tabular}
}
\label{tab:optimizer classification}
\end{table}

\paragraph{Additional Benchmark.}  \citet{matias2024amortized} proposed Amortized Variational DKL (AV-DKL), which is a variant SV-DKL using amortization network to compute the inducing locations and variational parameters, thus attenuating the overcorrelation of NN extracted features. AV-DKL is included as the additional benchmark for classification tasks in \Cref{tab:img avdkl}. The training recipe is the same with SV-DKL. 


\begin{table*}[ht]
\caption{\small{Accuracy, NLL, ECE for AV-DKL, SV-DKL, DAK-MC on CIFAR-10/100 averaged over 3 runs. CIFAR-10 uses ResNet-18 with 64 features extracted; CIFAR-100 uses ResNet-34 with 512 features. The best results are highlighted in \textbf{bold}; the second best results are highlighted by \underline{underline}.}}
\centering
\vspace{-0.1cm}
\resizebox{\linewidth}{!}{%
\begin{tabular}{rccclccc}
\toprule[1pt]
\multicolumn{1}{l}{} & \multicolumn{3}{c}{Batch size: 128}  &  & \multicolumn{3}{c}{Batch size: 1024} \\ \cline{2-4} \cline{6-8} \vspace{-8pt} \\
\multicolumn{1}{l}{} & AV-DKL & SV-DKL & \cellcolor{Gray} DAK-MC &   & AV-DKL  & SV-DKL & \cellcolor{Gray} DAK-MC \\ 
\midrule[1pt]
CIFAR-10 - Acc. (\%) $\uparrow$    & \underline{94.23 $\pm$ 0.65}  & 93.44 $\pm$ 0.28    &  \cellcolor{Gray} \textbf{94.81 $\pm$ 0.13}   &     &  \textbf{93.32} $\pm$ \textbf{0.13}        & 90.22 $\pm$ 1.42       & \cellcolor{Gray} \underline{93.02 $\pm$ 0.18}        \\
NLL $\downarrow$     & 0.352 $\pm$ 0.084    & \underline{0.312 $\pm$ 0.033}       &  \cellcolor{Gray} \textbf{0.256} $\pm$ \textbf{0.014}     &      & \underline{0.439 $\pm$ 0.022}         & 0.485 $\pm$ 0.061       & \cellcolor{Gray} \textbf{0.345 $\pm$ 0.001}    \\
ECE $\downarrow$      & 0.048 $\pm$ 0.006    & \underline{0.046 $\pm$ 0.003}       &  \cellcolor{Gray} \textbf{0.039 $\pm$ 0.002}          &     & \underline{0.054 $\pm$ 0.001}       & 0.060 $\pm$ 0.004       & \cellcolor{Gray} \textbf{0.052 $\pm$ 0.001}           \\
\midrule[1pt]
CIFAR-100 -  Acc. (\%) $\uparrow$    & \textbf{77.47 $\pm$ 0.19}  & 74.52 $\pm$ 0.13       & \cellcolor{Gray}  \underline{76.75 $\pm$ 0.18}     &     &  \textbf{77.07 $\pm$ 0.10}        & 66.54 $\pm$ 0.74       & \cellcolor{Gray} \underline{70.38 $\pm$ 1.25}        \\
NLL $\downarrow$     & 1.787 $\pm$ 0.011    & \underline{1.041 $\pm$ 0.007}       & \cellcolor{Gray}  \textbf{1.001 $\pm$ 0.027}     &      & 2.326 $\pm$ 0.030    & \underline{1.738 $\pm$  0.058}      & \cellcolor{Gray} \textbf{1.203 $\pm$ 0.040}        \\
ECE $\downarrow$      & 0.166 $\pm$ 0.002    & \underline{0.049 $\pm$ 0.002}       & \cellcolor{Gray}  \textbf{0.041 $\pm$ 0.004}        &     & 0.175 $\pm$ 0.001         & \underline{0.148 $\pm$ 0.007}       &\cellcolor{Gray}  \textbf{0.056 $\pm$ 0.006}           \\
\bottomrule[1pt]
\end{tabular}
}
\vspace{-0.2cm}
\label{tab:img avdkl}
\end{table*}

\paragraph{Metrics} 
We evaluate model performance using four common metrics: Top-1 accuracy, ELBO, Negative Log Likelihood (NLL), and Expected Calibration Error (ECE). 

ECE is a metric used to quantify the degree of ``calibration'' of a probabilistic model in UQ, specifically for classification problems. It is defined as the weighted average of the absolute difference between the model's predicted probability (confidence) and the actual outcome (accuracy) over several bins of predicted probability. Mathematically, ECE is given by:
\begin{align}
    \text{ECE} =\sum_{m=1}^{M} \frac{\left| B_{m} \right|}{n} \left| \text{acc} (B_{m})-\text{conf} (B_{m}) \right|,
\end{align}
where $M$ is the number of bins into which the confidence values are partitioned, $B_m$ is the set of indices of samples whose predicted confidence falls into the $m$-th bin, $n$ is the total number of samples.

\paragraph{Computing Infrastructure}
The experiments for classification were run on a Linux machine with NVIDIA RTX4080 GPU, and 32GB of RAM.




\subsection{Additional Tables and Figures}
\label{sec:additional exp results}

\paragraph{Choices of learning rates.}
We evaluate the choices of learning rates on 1D regression examples. DKL requires a separate tuning of the learning rate of the GP covariance parameters, which differs from the learning rate of the NN feature extractor. In \Cref{fig:dkl lr}, we choose the learning rate of the NN feature extractor as $0.01$, while the learning rate of the GP covariance is set to different values. (a)-(c) show that different learning rates of covariance in DKL result in different predictive posterior. In particular, although the training losses for DKL in both (a) and (b) are minimal, the regressions do not fit well. On the other hand, DAK does not need a distinct recipe for tuning GP covariances because of the BNN interpretation. Furthermore, the poor posterior is indicated by the higher training loss, as illustrated in (d)-(f).

\begin{figure}[ht]
\centering
\subfloat[$\begin{gathered}\text{DKL: last-layer lr} =0.01.\\ \text{Training loss:} -0.21.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dkl_lr_01.pdf}}
\subfloat[$\begin{gathered}\text{DKL: last-layer lr} =0.001.\\ \text{Training loss: } -0.07.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dkl_lr_001.pdf}}
\subfloat[$\begin{gathered}\text{DKL: last-layer lr} =0.0001.\\ \text{Training loss: } 0.22.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dkl_lr_0001.pdf}}

\subfloat[$\begin{gathered}\text{DAK: last-layer lr} =0.1.\\ \text{Training loss: } 0.10.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dak_lr_1.pdf}}
\subfloat[$\begin{gathered}\text{DAK: last-layer lr} =0.01.\\ \text{Training loss: } 0.10.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dak_lr_01.pdf}}
\subfloat[$\begin{gathered}\text{DAK: last-layer lr} =0.001.\\ \text{Training loss: } 0.22.\end{gathered}$]{\includegraphics[width=.3\textwidth]{toy_dak_lr_001.pdf}}

\caption{Results on 1D regression with different last-layer learning rates. The learning rate of NN feature extractor is set as $0.01$. (a)--(f) shows the regression fits and corresponding training losses. DAK fits for the same learning rate strategy with NN feature extractor (lr=0.01), while DKL requires a separate tuning for last-layer learning rate of GPs. Additionally, a better training loss does not necessarily prevent overfitting for DKL.}
\label{fig:dkl lr}
\end{figure}


\paragraph{Learning curves.} We plot the learning curves of CIFAR-10/100 in \Cref{fig:cifar10 curves} and \ref{fig:cifar100 curves}. The learning curves of SVDKL in \Cref{fig:cifar10 curves} is more unstable, with many significant spikes, and the convergence is slower than DAK. Futhermore, SVDKL struggles to fit with full-training in CIFAR-100, and a pretrained feature extractor is used in CIFAR-100. Therefore, the learning curves of SVDKL look smoothing, but DAK fits well with full-training in CIFAR-100.


\begin{figure}[ht]
\centering
\subfloat[Test Error (\%).]{\includegraphics[width=.3\textwidth]{CIFAR_10_test_error.pdf}}
\subfloat[Test NLL.]{\includegraphics[width=.3\textwidth]{CIFAR_10_nll.pdf}}
\subfloat[ELBO.]{\includegraphics[width=.3\textwidth]{CIFAR_10_elbo.pdf}}
\caption{Test errors, test NLLs, ELBOs of NN, SVDKL, and DAK curves with batch size of 128/1024 for CIFAR-10 averaged on 3 runs. DAK outperforms SVDKL on both test error and NLL along the training epochs. Additionally, SVDKL degrades more and struggles to fit when the batch size becomes larger.}
\label{fig:cifar10 curves}
\end{figure}

\begin{figure}[ht]
\centering
\subfloat[Test Error (\%).]{\includegraphics[width=.3\textwidth]{CIFAR_100_test_error.pdf}}
\subfloat[Test NLL.]{\includegraphics[width=.3\textwidth]{CIFAR_100_nll.pdf}}
\subfloat[ELBO.]{\includegraphics[width=.3\textwidth]{CIFAR_100_elbo.pdf}}
\caption{Test errors, test NLLs, ELBOs of NN, SVDKL, and DAK curves with batch size of 128/1024 for CIFAR-100 averaged on 3 runs. DAK trained NN and last-layer additive GPs jointly, while SVDKL used the pre-trained NN and fine-tuned the last-layer GP since SVDKL struggles to fit using full-training. DAK outperforms SVDKL on both test error and NLL along the training epochs. SVDKL struggled to fit in high-dimensional multitask cases, indicating the necessity of pre-training in SVDKL. However, DAK fitted well with high dimensionality and large batch sizes.}
\label{fig:cifar100 curves}
\end{figure}







% \end{document}
