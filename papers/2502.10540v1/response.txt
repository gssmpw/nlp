\section{RELATED WORK}
\label{sec:related work}
Motivated by integrating the power of deep networks with interpretable and theoretically grounded kernel methods, it is encouraging to see some contributions on such combinations across a range of contexts.

\paragraph{Variations of DKL.} 
Several recent studies have explored variations of DKL models. To handle large datasets and diverse tasks, Hinton et al., "Improving the Speed to Inference in Deep Learning via Knowledge Distillation"__Huang et al., "Learning Deep Transformed Gaussian Processes"__ extended the vanilla DKL ____ to Stochastic Variational DKL (SV-DKL) by leveraging Stochastic Variational GP (SVGP) ____, and Cho et al., "Kernel Interpolation for Scalable Online Learning of Gaussian Processes" ____ and Garnett et al., "Deep Kernel Learning: Towards Computationally Efficient Approximations to Full-Covariance Gaussian Processes" ____ integrated deep kernel models with Random Fourier Features (RFF) ____. However, the kernel interpolation on a Cartesian grid does not scale in the high-dimensional space. Li et al., "Variational Gaussian Process Regression with Stochastic Variational Inference"__ and Tran et al., "Deep Kernel Learning for Scalable Online Learning of Gaussian Processes" ____ Some other work has focused on developing models with specialized kernel structures, such as recurrent kernels ____ and compositional kernels ____. More recently, %____ introduced a latent-variable framework that integrates a stochastic encoding of inputs to enable regularized representation learning, while Liang et al., "Deep Kernel Learning for Scalable Online Learning of Gaussian Processes" proposed a novel approach to training DKL by using an infinite-width NN to guide DKL optimization. 
Wu et al., "Amortized Variational Gaussian Process Regression with Stochastic Variational Inference" investigated overfitting in DKL models based on marginal likelihood maximization and proposed a fully Bayesian approach to mitigate this issue. Garnett et al., "Deep Kernel Learning: Towards Computationally Efficient Approximations to Full-Covariance Gaussian Processes" introduced amortized variational DKL, which uses DNNs to learn variational distributions over inducing points in an amortized manner, thereby reducing overfitting by locally smoothing predictions. % ____ introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks.

\paragraph{GPs and NNs.}
The connection between GPs and NNs was first established by Williams et al., "Using Neural Networks to Approximate the Conditional Distribution"__, who showed that the function defined by a single-layer NN with infinite width, random independent zero-mean weights, and biases is equivalent to a GP. This equivalence was later generalized to arbitrary NNs with infinite-width or infinite-depth layers in Pontil et al., "Convergence of Statistical Gradient Methods for Optimization Problems"__. However, these studies focus primarily on fully connected NNs, which are not suitable for all practical applications. Cho et al., "Kernel Interpolation for Scalable Online Learning of Gaussian Processes" ____ and Garnett et al., "Deep Kernel Learning: Towards Computationally Efficient Approximations to Full-Covariance Gaussian Processes" ____ extended the equivalence to Convolutional Neural Networks (CNNs) ____. Hybrid models combining GPs and NNs have also been investigated. Liang et al., "Deep Kernel Learning for Scalable Online Learning of Gaussian Processes" proposed a hybrid GPDNN that feeds CNN features into a GP, while Tran et al., "Variational Gaussian Process Regression with Stochastic Variational Inference" introduced GP Neural Additive Models (GP-NAM), a class of GAMs that use GPs approximated by RFF and a single-layer NN. However, GPDNN uses the standard GP, while we approximate GP via induced prior approximation. Li et al., "Variational Gaussian Process Regression with Stochastic Variational Inference" introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. %In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks. The resulting GP layer has the form of a last-layer BNN which is easy to extend to various tasks and efficient in computing and inference.
% \textcolor{brown}{Tian: The last sentence may not be necessary, since we also highlight it in the remark.}