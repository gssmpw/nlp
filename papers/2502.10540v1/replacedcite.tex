\section{RELATED WORK}
\label{sec:related work}
Motivated by integrating the power of deep networks with interpretable and theoretically grounded kernel methods, it is encouraging to see some contributions on such combinations across a range of contexts.

\paragraph{Variations of DKL.} 
Several recent studies have explored variations of DKL models. To handle large datasets and diverse tasks, ____ extended the vanilla DKL ____ to Stochastic Variational DKL (SV-DKL) by leveraging Stochastic Variational GP (SVGP) ____ and KISS-GP ____. However, the kernel interpolation on a Cartesian grid does not scale in the high-dimensional space. ____ and ____ integrated deep kernel models with Random Fourier Features (RFF) ____, an efficient method for approximating GP kernel feature mappings. Some other work has focused on developing models with specialized kernel structures, such as recurrent kernels ____ and compositional kernels ____. More recently, %____ introduced a latent-variable framework that integrates a stochastic encoding of inputs to enable regularized representation learning, while ____ proposed a novel approach to training DKL by using an infinite-width NN to guide DKL optimization. 
____ investigated overfitting in DKL models based on marginal likelihood maximization and proposed a fully Bayesian approach to mitigate this issue. ____ introduced amortized variational DKL, which uses DNNs to learn variational distributions over inducing points in an amortized manner, thereby reducing overfitting by locally smoothing predictions. 
% ____ introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks.

\paragraph{GPs and NNs.}
The connection between GPs and NNs was first established by ____, who showed that the function defined by a single-layer NN with infinite width, random independent zero-mean weights, and biases is equivalent to a GP. This equivalence was later generalized to arbitrary NNs with infinite-width or infinite-depth layers in ____. However, these studies focus primarily on fully connected NNs, which are not suitable for all practical applications. ____ and ____ extended the equivalence to Convolutional Neural Networks (CNNs) ____, which are widely used in image recognition. Hybrid models combining GPs and NNs have also been investigated. ____ proposed a hybrid GPDNN that feeds CNN features into a GP, while ____ introduced GP Neural Additive Models (GP-NAM), a class of GAMs that use GPs approximated by RFF and a single-layer NN. However, GPDNN uses the standard GP, while we approximate GP via induced prior approximation. GP-NAM applies the additive model with RFF approximation but lacks Bayesian inference. ____ introduced a sampling-free Bayesian last-layer architecture but did not establish a connection between this architecture and DKL. %In our work, we approximate GPs using additive structure and sparsely induced priors to enhance the computational efficiency of DKL, which results in a last-layer BNN that is both efficient and easily adaptable to various tasks. The resulting GP layer has the form of a last-layer BNN which is easy to extend to various tasks and efficient in computing and inference.
% \textcolor{brown}{Tian: The last sentence may not be necessary, since we also highlight it in the remark.}