\section{A Comparative Study of Initial and Curated Code Review Datasets}
\label{sec:analysis}

\subsection{Impact of Curated Reviews on Comment Generation}
\label{subsec:model_data}
In this section, we investigate the impact of curated reviews on automating the comment generation process. By comparing models trained on both original and curated comments, we aim to assess whether the reformulated reviews lead to more efficient automation of the comment generation task.

\paragraph{\textbf{Model and data selection}}
To ensure a fair comparison, we selected a subset of $20,000$ comments from both the original and curated datasets, such that each original review comment \( r_i \) from the original dataset is paired with its reformulated counterpart \( r'_i \) in the curated dataset. This selection strategy guarantees that any observed differences in model performance can be attributed to the quality of the data (\ie curation process) rather than differences in review content or model hyperparameters. We further split each subset into $75\%$ for training and $25\%$ for evaluation.

We selected \textit{DeepSeek-6.7B-Instruct} \cite{deepseek-coder}, an LLM tailored for code-related tasks. 
Given a code change, the model was tasked with generating either the original or the curated comment. To ensure consistency, we trained two separate models, one for each dataset version, using identical configurations.


\paragraph{\textbf{Experimental setup}}

This experiment aims to determine whether curated review comments improve the ability of LLMs to generate accurate review comments. For each dataset version (original and curated), we provided the model with code changes as input and tasked it with generating the corresponding review comment.

Each model was trained independently using the same configuration to ensure that observed performance differences could be attributed solely to the dataset quality, not to model or hyperparameter variations. The training was conducted on four \emph{NVIDIA RTX A5000 GPUs}, each with \emph{24GB} of memory. We used a batch size of $4$ and trained each model for $5$ epochs. To enable efficient, low-resource fine-tuning, we employed Low-Rank Adaptation (LoRA) \cite{hu2021lora}, a parameter-efficient fine-tuning technique, configured with settings of $r = 16$, $\alpha = 32$, and $dropout = 0.05$. LoRA operates by decomposing the weight updates of a neural network into low-rank matrices, significantly reducing the number of parameters that require updating during fine-tuning \cite{hu2021lora}, thus enhancing the overall efficiency of the training process.
LoRA has been widely used in prior work to fine-tune LLMs for software engineering tasks~\cite{lu2023llama, weyssow2023exploring, hou2023large, silva2023repairllama}


To evaluate the two produced models’ performance, we used the BLEU score \cite{papineni2002bleu}, a standard metric, widely used in the literature, that measures the precision of n-grams in the generated text relative to the ground truth. BLEU is well-suited for assessing the correctness of generated comments, with higher scores indicating greater accuracy with real output.


\paragraph{\textbf{Results}}
The results are presented in \Table{tab:com_results}. For the model trained on the original dataset, we obtained a BLEU score of $7.71$, whereas the model trained on the curated dataset achieved a BLEU score of $11.26$. This improvement suggests that the reformulated, curated comments are likely easier for the model to learn, potentially due to their enhanced clarity and structure.

These findings suggest that curated review comments provide clearer, more direct guidance, enabling the model to better capture the intended message and improving the quality of generated comments. The higher BLEU score with the curated dataset indicates that the curation process enhances the ability of models to generalize and learn producing more accurate review comments, thus facilitating a more efficient automation of the comment generation process.

% \begin{table}[!t]
% \centering
% \caption{Comparison of BLEU scores for DeepSeek-Coder-6.7B-Instruct trained on original and curated comments.}
% \label{tab:com_results}
% \begin{tabular}{@{}lcc@{}}
% \toprule
% \textbf{Dataset Version}       & \textbf{BLEU} \\ 
% \midrule
% Original Comments               & $7.71$ \\
% Curated Comments                & \textbf{$11.26$} \\ 
% \bottomrule
% \end{tabular}
% \vspace{-1.5em}
% \end{table}

\begin{table}[!t]
\centering
\caption{Comparison of BLEU scores for DeepSeek-Coder-6.7B-Instruct trained on original and curated comments.}
\label{tab:com_results}
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{Original Comments} & \textbf{Curated Comments} \\ 
\midrule
\textbf{BLEU} & $7.71$ & $\textbf{11.26}$ \\ 
\bottomrule
\end{tabular}
\vspace{-1em}
\end{table}



The code change, shown in \Table{tab:comm_gen_examples}, is an example picked from our test set. The real comment in the original dataset lacks clarity, as it does not provide the reason behind the question. The model trained on the original dataset generates an incorrect review comment. In the curated dataset, the reformulated version of the real review comment is clearer, though less concise. The model trained on the curated dataset generates a comment that is accurate, closely matching the real and reformulated comments, and is even more concise than the reformulated version.



\begin{table*}[!htbp]
\centering
\caption{Example of real, reformulated, and generated review comments from the initial and curated datasets.}
\label{tab:comm_gen_examples}
\begin{tabularx}{1\linewidth}{X}
\toprule

\multicolumn{1}{c}{\textbf{Code change}} \\\midrule
\makebox[\linewidth]{\includegraphics[width=0.6\linewidth]{figures/code_snippet3.png}} 
\\\midrule
\multicolumn{1}{c}{\textbf{Original dataset}}\\\midrule

\underline{\textbf{Real review comment}} \texttt{why this import is required ?} \\

\underline{\textbf{Generated review comment}} \texttt{I think we should use `javax.servlet.http.Part`instead of \newline`org.apache.commons.fileupload.FileItem`.} \\ \midrule

\multicolumn{1}{c}{\textbf{Curated dataset}} \\\midrule

\underline{\textbf{Reformulated review comment}} \texttt{This import is not immediately clear in its necessity. Could you provide context\newline or explain why `javax.servlet.http.Part` is required for this functionality?} \\

\underline{\textbf{Generated review comment}} \texttt{Consider removing the import statement for `javax.servlet.http.Part` as it's not\newline used in the code.} \\ \bottomrule

\end{tabularx}
\vspace{-.5em}
\end{table*}





\begin{center}
\begin{tcolorbox}[colframe=orange!75!black, colback=orange!5!white, title=\textbf{Answer to RQ3}]
Curated review comments are more effective for training models on the comment generation task, as evidenced by higher BLEU ($7.71 \rightarrow 11.26$).
\end{tcolorbox}
\end{center}


\subsection{A Comparative Analysis on the Usefulness of Curated Comments for Code Refinement}

In this section, we aim to evaluate the usefulness of the curated comments compared to the original comments for code refinement.
We conduct a comparative study to assess which version of the comment—original or curated—guides the code refinement model to generate more accurate code changes.


\paragraph{\textbf{Model and data selection}}
% We select \textit{DeepSeek-Coder-6.7B-Instruct} \cite{deepseek-coder} to serve as our code refinement model. For evaluation, we randomly select a subset of our large datasets, choosing 20,000 samples from the full set of 176,613 for each version of the dataset (\ie original and curated datasets). To ensure a fair comparison, we maintain consistency in the selection by pairing each original review comment \( r_i \) from the original dataset with its corresponding reformulated comment \( r'_i \) in the cated dataset, where \( i \in [1, 20,000] \). Thus, each review pair \( (r_i, r'_i) \) represents the same code review context, allowing us to directly evaluate the impact of the reformulated comments compared to the original. For consistency, we use the same \textit{DeepSeek-Coder-6.7B-Instruct} model configuration on both datasets to ensure that any observed differences are attributable to the quality of the input data (\ie review comments) rather than model variations. 

We use \textit{DeepSeek-Coder-6.7B-Instruct} \cite{deepseek-coder} as a code refinement model, applying it to the same selected subset of $20,000$ samples from each dataset version (original and curated), as in the previous experiment on comment generation, as explained in \Sect{subsec:model_data}. To ensure a fair comparison, each original review comment \( r_i \) is paired with its reformulated counterpart \( r'_i \), preserving the same review context across both datasets. The model configurations remain identical for both datasets, to ensure that any observed differences are attributable to the quality of the input data (\ie review comments) rather than model variations. 




\paragraph{\textbf{Experimental setup}}
For each dataset version, we provided the code refinement model with the original code diff, the old file, and the review comment (either original or curated) as context and prompted it to generate a code diff that accurately implements the specified changes.

We used the LLM directly for inference, as its extensive training on diverse code-related tasks equips it with the capabilities needed to effectively automate the code refinement task. The experiment was run twice, once with the original comments and once with the curated comments.

To evaluate the accuracy of the generated code diffs, we employed two evaluation metrics: 
\begin{itemize} 
    \item \textbf{CodeBLEU}: This metric measures the similarity of the generated code diff to the expected code diff, combining n-gram match, weighted n-gram match, AST match, and data-flow match scores \cite{ren2020codebleu}. 
    \item \textbf{Exact Match (EM)}: This metric calculates the number of generated code diffs that exactly match the expected code diff. 
\end{itemize}

Each experiment was conducted using identical model configurations for both dataset versions to ensure that any observed performance differences could be attributed solely to the quality of the review comments rather than model parameter variations.



\paragraph{\textbf{Results}}
The results, presented in \Table{tab:ref_results}, reveal substantial differences between the two dataset versions. Using the \textbf{original comments}, the model achieved a \emph{CodeBLEU} score of $0.36$ and an \emph{EM} of $408$. When utilizing the \textbf{curated comments}, the model performance improved significantly, reaching a \emph{CodeBLEU} score of $0.44$ and an \emph{EM} of $445$. These findings suggest that the curated comments offer more precise guidance, enabling the model to generate more accurate code changes that are closer to the ground truth.

This result indicates that the curated comments not only clarify the intended modifications but also reduce ambiguities in the model interpretation of the review instructions. The curated comments likely contain enhanced phrasing and structure that aid the model in better understanding and implementing the required code changes, thus improving the overall quality of the generated code diff.

\begin{table}[h!]
\centering
\caption{Comparison of DeepSeek-Coder-6.7B-Instruct's effectiveness for code refinement using the original and curated review comments.}
\label{tab:ref_results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Dataset Version}       & \textbf{CodeBLEU} & \textbf{Exact Match} \\ \midrule
Original Comments & $0.36$ & $408$\\
Curated Comments  & $\textbf{0.44}$ & $\textbf{445}$\\ 
\bottomrule
\end{tabular}
\vspace{-.5em}
\end{table}



This experiment showed that curated review comments are more useful for the next task, as they provide better guidance for the code refinement task, leading to more accurate code changes. This is reflected by significant improvements in both \emph{CodeBLEU} and \emph{EM}.




\begin{center}
\begin{tcolorbox}[colframe=black!75!black, colback=black!5!white, title=\textbf{Answer to RQ4}]
    Curated comments demonstrate superior utility for guiding code refinement models, leading to notably higher CodeBLEU ($0.36 \rightarrow 0.44$) and Exact Match scores ($408 \rightarrow 445$).
\end{tcolorbox}
\end{center}







\begin{comment}
    ================================================== 1 ==================================================


 INIT - Original comment:  This and the one below. Please, rename to `data range` also.
 INIT - Generated comment  I think this should be "dynamic range"

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   Consider renaming "value range" to "data range" for consistency.
 CUR - Generated comment  Consider renaming the parameter to `value_range` for clarity and consistency.



 ================================================== 27 ==================================================

@@ -2791,81 +2791,80 @@ static void Main()
                 Diagnostic(ErrorCode.WRN_UnreferencedVarAssg, "message").WithArguments("message").WithLocation(8, 22));
         \}

-        [WorkItem(863401, "DevDiv/Personal")]
         [Fact]
-        public void CS1101ERR_BadRefWithThis()
+        public void BadParameterModifeirs_ThisWithRef()


+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=


 INIT - Original comment:  Typo: `Modifiers` #Resolved
 INIT - Generated comment  ```suggestion public void BadParameterModifiers_ThisWithRef() ```

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   The method name `BadParameterModifeirs_ThisWithRef` contains a typo. Consider renaming it to `BadParameterModifiers_ThisWithRef` for consistency.
 CUR - Generated comment  Consider renaming the test method to `BadParameterModifiers_ThisWithRef` for consistency and clarity.



 ================================================== 6 ==================================================


 INIT - Original comment:  It should be without underscore, I think
 INIT - Generated comment  I think we should rename this to `IndexOf`.

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   Consider renaming the method to remove the underscore for consistency.
 CUR - Generated comment  Consider renaming the method to `IndexOfNonVectorized` for consistency and clarity.



================================================== 4845 ==================================================

@@ -75,6 +75,8 @@

 import com.google.common.annotations.VisibleForTesting;

+import javax.servlet.http.Part;


+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=


 INIT - Original comment:  why this import is required ?
 INIT - Generated comment  I think we should use `javax.servlet.http.Part` instead of `org.apache.commons.fileupload.FileItem`.

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   "This import is not immediately clear in its necessity. Could you provide context or explain why `javax.servlet.http.Part` is required for this functionality?"
 CUR - Generated comment  Consider removing the import statement for `javax.servlet.http.Part` as it's not used in the code.


================================================== 27 ==================================================

@@ -2791,81 +2791,80 @@ static void Main()
                 Diagnostic(ErrorCode.WRN_UnreferencedVarAssg, "message").WithArguments("message").WithLocation(8, 22));
         \}

-        [WorkItem(863401, "DevDiv/Personal")]
         [Fact]
-        public void CS1101ERR_BadRefWithThis()
+        public void BadParameterModifeirs_ThisWithRef()


+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=


 INIT - Original comment:  Typo: `Modifiers` #Resolved
 INIT - Generated comment  ```suggestion public void BadParameterModifiers_ThisWithRef() ```

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   The method name `BadParameterModifeirs_ThisWithRef` contains a typo. Consider renaming it to `BadParameterModifiers_ThisWithRef` for consistency.
 CUR - Generated comment  Consider renaming the test method to `BadParameterModifiers_ThisWithRef` for consistency and clarity.

================================================== 59 ==================================================

@@ -7,6 +7,7 @@
 to leave out days on which there is no data, i.e. weekends.  The example
 below shows how to use an 'index formatter' to achieve the desired plot
 """
+from __future__ import print_function


+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=


 INIT - Original comment:  This should not be added now since we're Python 3 only on master.
 INIT - Generated comment  ```suggestion from __future__ import print_function, division ```

+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
 CUR - Reformulated comment   "This import is unnecessary since we're only supporting Python 3 on master. Consider removing it."
 CUR - Generated comment  Consider using `print` instead of `print_function` for compatibility with Python 2.



 
\end{comment}






