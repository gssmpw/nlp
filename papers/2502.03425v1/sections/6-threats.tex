\section{Threats to Validity}
\label{sec:threats}

The evaluation results have shown that our proposed methodology is effective in curating review comments and improving the performance of downstream tasks (\ie comment generation and code refinement). However, certain threats may limit the validity of these evaluation results.


A primary threat pertains to the nature of the data, specifically the review comments, which may contain noise and potentially non-English or misspelled words. We have mitigated this by employing \emph{Llama-3.1-70B}, an LLM that utilizes Byte-Pair Encoding \cite{sennrich2015neural}, a subword-based tokenization algorithm. This algorithm breaks unseen words into several frequently seen sub-words that can be effectively processed by the model.

Another concern relates to the reliability of using an LLM as a judge, as LLM-generated judgments may not always match the accuracy of human evaluations. However, employing a bigger LLM, specifically \emph{Llama-3.1-70B}, helps mitigate this issue due to its advanced capabilities in producing accurate judgments. Prior research supports this hypothesis, showing that highly capable LLMs align closely with human assessments, often achieving agreement rates comparable to those between human evaluators \cite{zheng2024judging, li2023alpacaeval}. To further validate the reliability of LLM judgments in our study, we conducted a thorough sanity check on $100$ manually assessed samples. The obtained Cohen's kappa scores indicated strong agreement rates between human and LLM evaluations, reinforcing the reliability of \emph{Llama-3.1-70B} judgments.






