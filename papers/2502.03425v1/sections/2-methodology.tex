\section{Methodology}
\label{sec:methodology}

In this section, we provide an overview of our methodology. Our goal is to answer the following research questions (RQs):
\begin{itemize}
    \item \textbf{RQ1:} \emph{What are the main characteristics and concerns in the code reviews dataset?}
    \item \textbf{RQ2:} \emph{How can our data curation pipeline improve the overall quality of the dataset, specifically in terms of reducing noise and irrelevant reviews?}
    \item \textbf{RQ3:} \emph{What is the impact of dataset curation on the performance of LLMs for automated comment generation?}
    \item \textbf{RQ4:} \emph{What is the impact of dataset curation on the comments' usefulness and LLMs performance for automated code refinement?}
\end{itemize}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/methodology.pdf}
    \caption{Overview of our methodology. We use a large code review dataset of samples comprising pre-commit and post-commit codes along with review comments. For each sample, we use LLM-as-a-Judge with Llama-3.1-70B to generate a reformulated review comment, a categorization of the review, and a score for the original review comment. Next, we use the reformulated review comments to create our curated dataset, while filtering out irrelevant samples. Finally, we compare the effectiveness of LLMs fine-tuned on the original and curated datasets on two downstream tasks: comment generation and code refinement.}
    \label{fig:methodology}
    \vspace{-.5em}
\end{figure*}

\begin{comment}
\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/methodology.png}
    \caption{Overview of the proposed methodology.}
    \label{fig:methodology}
\end{figure*}
\end{comment}

\Fig{fig:methodology} illustrates the workflow of the proposed methodology. 
The process begins with selecting a large code review dataset \cite{li2022automating}, which forms the basis for subsequent analyses.
Then, we define an evaluation framework (see \Fig{fig:eval_framework}) to classify review comments according to multiple categories (\ie type, nature, civility) and to score them based on various criteria (\ie relevance, clarity, conciseness). We conduct a quality assessment and characterization of the dataset, utilizing an LLM, \emph{Llama-3.1-70B}, as an automated judge using the defined evaluation framework (\emph{RQ1}).

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/eval_framework.pdf}
    \caption{Overview of our evaluation framework.}
    % \vspace{-2em}
    \label{fig:eval_framework}
\end{figure}

Subsequently, we propose a curation pipeline aimed at refining the review comments. This step involves employing \emph{Llama-3.1-70B} to reformulate the comments based on some criteria to enhance their quality. A comparative analysis of both the original and curated datasets is then performed using the defined evaluation framework, focusing on classification categories and scoring criteria (\emph{RQ2}).


The next phase consists of fine-tuning two separate language models, each trained on either the original or curated dataset, to automate the task of comment generation. The performance of these models is evaluated and compared to determine which dataset better facilitates the learning process for generating high-quality review comments (\emph{RQ3}).


Finally, we assess the usefulness of the review comments for the subsequent task (\ie code refinement). We use two versions of the same language model, each provided with one version of the review comments (\ie original or curated), and compare the accuracy of the generated code. This comparison allows us to evaluate the impact of the curated versus original review comments on the correctness and effectiveness of automated code refinement (\emph{RQ4}).




