\section{Conclusion}
\label{sec:conclusion}

In this work, we present a methodology for enhancing the quality of code review datasets. We first propose an evaluation schema for review comments and conduct an empirical study to assess the dataset quality, identifying key issues and areas for improvement. Building on these findings, we introduce a curation process aimed at reformulating review comments to improve their quality, as measured by several criteria. We then perform a comparative analysis between the original and curated datasets. We evaluate the impact of these datasets on automating downstream code review tasks, specifically comment generation and code refinement.

Our findings open up numerous research opportunities. The insights from our empirical study could guide work on predicting the type of issue in a code change, balancing training datasets across different categories, and filtering datasets based on scoring criteria to produce more robust and unbiased models. Additionally, our curated dataset offers a valuable baseline for future research in code review, particularly in comment generation, where its improved quality will enable the development of models that generate relevant, clear, concise, and civil feedback.

In future work, we plan to incorporate multiple LLMs as juries to enhance dataset curation, fostering a diverse set of review comments. We also aim to explore prioritization strategies for review comments, helping reviewers efficiently prioritize pull requests based on urgency and relevance.


\section*{Data Availability}
We publicly release all the code \cite{github_replication}, models, data, and results \cite{zenodo_data} of our experiments.