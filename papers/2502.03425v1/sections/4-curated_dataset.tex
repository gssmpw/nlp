\section{CuRev: A Curated Dataset for Code Review}
\label{sec:curdata}

\subsection{Experimental Setup}
Based on our previous experiment findings, we developed a curation pipeline aimed at enhancing the overall quality of the code review dataset. This pipeline comprises two primary steps: filtering out irrelevant comments and reformulating comments to improve clarity, conciseness, and civility.


\paragraph*{\textbf{Filtering irrelevant review comments}}
The first step involves identifying and removing irrelevant review comments. While certain criteria, such as civility, clarity, and conciseness, can be improved through rephrasing, relevance is inherently static and tied to the core idea of the comment. A relevant or irrelevant comment retains this classification regardless of its formulation, as it addresses the same issue or improvement.

To implement this step, we carefully examined the relevance scores from the initial evaluation and set a threshold of $4$. Any comment with a relevance score below this threshold was considered irrelevant and subsequently filtered out. This filtering process resulted in the elimination of $5,895$ samples, leaving a curated dataset of size $170,718$.

These are three examples of review comments that were filtered out due to their low relevance scores ($<4$):
\begin{quote}
\textbf{Example 1}: \texttt{Need some edit here?}

\textbf{Example 2}: \texttt{Same here etc :)}

\textbf{Example 3}: \texttt{This is gross}
\end{quote}
While human reviewers might understand these comments in real-life contexts—where much communication occurs verbally and directly—they remain irrelevant and unhelpful for a model to learn from, as they lack useful information.


\paragraph*{\textbf{Review comments reformulation}}
The second step focuses on reformulating review comments to enhance the criteria of civility, clarity, and conciseness. For this task, we utilized an LLM, \emph{Llama-3.1-70B}, providing specific instructions to guide the reformulation process. The reformulation task was designed to maintain the original intent and content of each comment while improving its form and presentation. \Table{tab:reform_prompt} shows an excerpt of the prompt used for this task.


\begin{table}[!t]
\centering
\caption{Excerpt of the prompt for comment reformulation.}
% \vspace{-.5em}
\label{tab:reform_prompt}
\begin{tabularx}{\linewidth}{X}
\toprule
\textbf{\#\#\# Review comment reformulation}

Your task is to reformulate and improve the given review comment by making it civil, more clear, and more concise without changing its core message or intent. 
The reformulated comment should respect the following guidelines:\\
\\
\textsc{\textbf{1. Conciseness:}} The comment should convey its message in the fewest words necessary while still being informative. Eliminate redundancy and irrelevant details.
\vspace{.5em}

\textsc{\textbf{2. Clarity:}} Ensure the comment is straightforward, well-structured, and grammatically correct, making the feedback easy to understand without any ambiguity.
\vspace{.5em}

\textsc{\textbf{3. Civility:}} Keep the comment respectful, professional, and constructive, avoiding any harsh or inappropriate language.\\

\hspace{0.5\linewidth}\vdots

\textbf{\#\#\# Given review comment}\\
\{\textit{review\_comment\}}\\

\textbf{\#\#\# Code changes}\\
\{\textit{code\_diff\}}\\
\bottomrule
\end{tabularx}
\vspace{-1em}
\end{table}


The reformulation guidelines, employed in the prompt, emphasize three key criteria: conciseness, clarity, and civility. The reformulated comment should convey its message in the fewest words necessary while remaining informative, removing any redundancy or irrelevant details (\emph{conciseness}). It should also be well-structured, straightforward, and free of ambiguity to ensure ease of understanding (\emph{clarity}). Additionally, the comment must maintain a respectful and professional tone, providing constructive feedback without using harsh or inappropriate language (\emph{civility}).


% Handling non-english words
The code review dataset used is exclusively composed of English comments, though some comments may include non-English words depending on the context. A powerful and multilingual LLM, such as Llama-3.1-70B, can appropriately handle these cases effectively. This was supported by findings in the literature, which indicate that LLMs, despite being predominantly trained on large amounts of English data, can still manage non-English languages to a reasonable extent \cite{terryn2024exploratory}. 


In the final step, we re-evaluated the curated review comments using the same evaluation schema as was applied to the original dataset. Relevance was omitted from this re-evaluation, as it does not change with reformulation; it is dependent on the content of the comment rather than its form. This re-evaluation allowed for a direct comparison between the original and curated review comments, assessing variations in clarity, conciseness, and civility.




\subsection{Results}

The analysis of the scoring criteria across the curated dataset, as shown in \Table{tab:cur_scoring_dist}, reveals improvements in both clarity and conciseness. The average clarity score increased to $8.96$, representing a substantial improvement of $2.07$. Conciseness also saw an increase, reaching an average score of $8.05$, which marks a smaller improvement of $0.34$.

The important improvement in clarity indicates that curated comments are more structured, eliminating noise and unnecessary words while using grammatically correct vocabulary and well-formed sentences, making them clearer and easier to understand. However, the modest improvement in conciseness can be attributed to the inherently verbose nature of LLMs, which tend to produce more elaborate text by default.

Overall, the enhancements in clarity and conciseness suggest that the curation pipeline successfully refined the dataset by producing review comments that are more succinct and easier to understand. These improvements likely contribute to better data quality, facilitating more effective learning by language models.


\begin{table}[!t]
  \centering
  \caption{Evolution of the scoring criteria per category across the curated dataset.}
  \label{tab:cur_scoring_dist}
  \begin{tabular}{>{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1.5cm}*{2}{c}}
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Clarity} & \textbf{Conciseness} \\
    \midrule
    \multirow{6}{*}{\textbf{Type}} & Refactoring & $8.95$ (\up $0.63$) & $8.06$ (\up $1.07$) \\
    & Bugfix & $8.98$ (\up $0.45$) & $8.03$ (\up $1.19$) \\
    & Testing & $8.98$ (\up $0.56$) & $8.03$ (\up $1.06$) \\
    & Logging & $8.97$ (\up $0.54$) & $8.02$ (\up $1.17$) \\
    & Documentation & $8.98$ (\up $0.65$) & $8.02$ (\up $1.30$) \\
    & Other & $8.95$ (\up $1.93$) & $8.05$ (\up $2.15$) \\
    \midrule
    \multirow{4}{*}{\textbf{Nature}} & Descriptive & $8.76$ (\up $1.62$) &  $8.02$ (\up $2.41$)\\
    & Prescriptive & $8.96$ (\up $0.44$) & $8.05$ (\up $0.86$) \\
    & Clarification & $8.96$ (\up $0.67$) & $8.03$ (\up $1.37$) \\
    & Other & $9.00$ (\up $4.76$) & $8.00$ (\up $3.88$) \\
    \midrule
    \multirow{2}{*}{\textbf{Civility}} & Civil & $8.96$ (\up $0.70$) & $8.05$ (\up $1.12$) \\
    & Uncivil & -- & -- \\
    \midrule
    \multirow{1}{*}{\textbf{Average}} & -- & $8.96$ (\up $2.07$)  & $8.05$ (\up $0.34$) \\
    \bottomrule
  \end{tabular}
  \vspace{-1em}
\end{table}


\Table{tab:cur_categories_dist} presents the evolution of comment categorizations in the curated dataset compared to the original one. In the \emph{nature} category, the percentage of \emph{prescriptive} comments increased to $90.20\%$ (vs. $62.6\%$ in the original dataset) and the percentage of descriptive comments decreased significantly to $0.95\%$, demonstrating a strong shift towards more directive, explicit, and actionable guidance to developers. This could be explained by the fact that prescriptive comments might be clearer compared to descriptive comments.

In the \emph{civility} category, all comments were marked as \emph{civil}, achieving a perfect score ($100\%$) and eliminating any \emph{Uncivil} comments. This indicates that the curation process effectively addressed concerns related to inappropriate or harsh language, creating a more professional and constructive dataset.



\begin{table}[!htbp]
  \centering
  \caption{Categories statistics in the curated dataset.}
  \label{tab:cur_categories_dist}
  \begin{tabular}{>{\centering\arraybackslash}p{1.2cm} >{\centering\arraybackslash}p{2cm}*{2}{r}}
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Count} & \textbf{Percentage}\\
    \midrule
    \multirow{4}{*}{\textbf{Nature}} & Descriptive & $1,674$ & $0.95$\down \\
    & Prescriptive & $159,306$ & $90.20$\up \\
    & Clarification & $29,586$ & $16.75$\down \\
    & Other & $17,491$ & $9.90$\up \\
    \midrule
    \multirow{2}{*}{\textbf{Civility}} & Civil & $176,613$ & $100$\up \\
    & Uncivil & $0$ & $0$\down \\
    \bottomrule
  \end{tabular}
  \vspace{-1.5em}
\end{table}



\Table{tab:reform_examples} presents examples from the dataset illustrating the original and reformulated review comments. The original comments contain inappropriate language and lack professionalism, which could impede effective communication and contribute to an unconstructive code review environment. The reformulated versions demonstrate significant improvements in clarity and civility. For instance, the original comment, “\texttt{you need to use list\_delete here, list\_free doesn't do shit...}” was revised to a more professional and instructive tone: “\texttt{Consider using list\_delete instead of list\_free to properly clean up the nh\_list ...}”. Similarly, the second example shows a shift from an uncivil and vague comment to one that is respectful and precise. These examples reflect the impact of the curation pipeline in producing comments that are clearer, more actionable, and maintain a civil tone, making them more suitable for use in automated code review tools.



\begin{table*}[!htbp]
\centering
\caption{Examples of original and reformulated review comments from \oapp.}
\label{tab:reform_examples}
\begin{tabularx}{\textwidth}{XX}
\toprule
\multicolumn{1}{c}{\textbf{Original Review Comment}} & \multicolumn{1}{c}{\textbf{Reformulated Review Comment}} \\
\midrule

\texttt{you need to use `list\_delete` here, `list\_free` doesn't do shit... (honestly `list\_free` shouldn't be in the API to begin with)} &
\texttt{Consider using `list\_delete` instead of `list\_free` to properly clean up the `nh\_list` in `ospf6\_route\_delete`. The `list\_free` function does not handle the deletion of the list's elements, which is necessary in this case.} \\
\midrule

% \texttt{wtf? `r\_buf\_buffer` is now returning a new buffer with all the bytes? this is gonna be a huge pain for big files} &
% \texttt{Consider the performance implications of `r\_buf\_buffer` now returning a new buffer with all bytes, especially for large files. This change may introduce significant memory overhead. Could you provide insight into the reasoning behind this change and potential optimizations for handling big files?} \\
% \midrule

\texttt{mmmm welcome to the fucking world of TS ids... -1 or 0 for invalid?} &
\texttt{Consider returning 0 instead of -1 for invalid object IDs to maintain consistency with standard error handling conventions.} \\
\bottomrule

\end{tabularx}
\vspace{-1em}
\end{table*}




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/bar_plots_scores_curated.pdf}
    \caption{Distribution of the clarity and conciseness scoring criteria on the curated dataset.}
    \label{fig:curated_scoring_dist}
    \vspace{-1.5em}
\end{figure}




\begin{center}
\begin{tcolorbox}[colframe=purple!75!black, colback=purple!5!white, title=\textbf{Answer to RQ2}]
The results of the curation pipeline indicate that the curated dataset shifted towards clearer ($6.89 \rightarrow 8.96$), more actionable ($62.60\% \rightarrow 90.20\%$), more concise ($7.71 \rightarrow 8.05$), and more civil ($98.80\% \rightarrow 100\%$) review comments.
\end{tcolorbox}
\end{center}