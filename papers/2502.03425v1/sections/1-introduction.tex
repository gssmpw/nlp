\section{Introdution}
\label{sec:intro}

% General context of code review process
Code review is a critical component of the software development life cycle, aimed at identifying issues, detecting suboptimal code, and uncovering bugs \cite{mcintosh2014impact, mcintosh2016empirical}, while ensuring the overall quality and maintainability of the source code \cite{ackerman1989software, ackerman1984software, morales2015code}. This process typically involves a manual inspection of code by one or more developers, reviewing code written by their peers \cite{fagan2002design, bavota2015four}. The code review process consists of several key tasks, with the most essential being the identification and documentation of potential issues through review comments, the subsequent code refinement to resolve these concerns, and the quality assessment of the submitted code to decide if it should be accepted or needs further review.

% Put emphasis on the importance of 'comment generation' task 
Issue identification and description (\ie review comment generation) constitutes a foundational task in code review, focusing on the detection of defects or problems within the code. This phase is pivotal, as it involves not only identifying specific issues but also offers potential solutions for resolving them \cite{mcintosh2014impact, mcintosh2016empirical}. The significance of this task cannot be overstated, as subsequent stages of the review process heavily depend on its accuracy and thoroughness. Code refinement, for example, is an execution phase directly tied to insights gained from issue identification; developers revise and improve the code based on comments provided during this stage \cite{bacchelli2013expectations}. Thus, comment generation is essential to the entire code review process. Without precise execution of this task, the integrity and quality of later stages is compromised. As such, this task must be carried out meticulously, ensuring that issues and improvements are thoroughly examined and well-documented.






% Automation of comment generation
Automating comment generation has gained increasing attention, with various techniques proposed to improve accuracy and efficiency \cite{tufan2021towards, tufano2022using, li2022automating, li2022auger, ben2024improving}.
These methods typically use machine learning and natural language processing to analyze code and generate comments that identify defects and suggest fixes.
However, current systems still fall short of matching human reviewers in detecting nuanced issues and providing context-aware suggestions.
There is significant room for improvement across multiple facets of this task, from enhancing the quality of the existing datasets to developing more sophisticated generative language models.
Current models often struggle with producing comments that are both accurate and actionable, indicating a need for more refined approaches that can bridge the gap between human expertise and automated solutions.

% Challenges of automating comment generation
One key element to the success of deep learning models is the quality of training data. Language models cannot perform effectively if the training data is flawed or insufficient. Thus, high-quality data is foundational to the learning process, as models are only as good as the data they are trained on. In the case of code review, the datasets currently available \cite{li2022automating, sghaier2023multi}, while valuable, often suffer from several limitations. These datasets are frequently extracted in a raw form from repositories and are not curated or pre-processed to ensure their quality. As a result, they may contain various forms of noise, such as uncivil comments, irrelevant or poorly structured reviews, grammatical errors, or irrelevant comments. These issues not only hinder the learning process but can also lead to models learning undesirable patterns, such as generating irrelevant or incoherent feedback, thereby undermining the effectiveness of the automated task. Consequently, improving the quality of code review datasets is crucial for enhanced automation of comment generation and code refinement tasks.

%\begin{figure}[!t]
%    \centering
%    \includegraphics[width=\linewidth]{figures/eval_framework.pdf}
%    \caption{Overview of our evaluation framework.}
%    \vspace{-2em}
%    \label{fig:eval_framework}
%\end{figure}

In this paper, we propose an evaluation framework to categorize and assess the quality of review comments.
Fig.~\ref{fig:eval_framework} illustrates our evaluation framework consisting of (1) a categorization scheme to classify the type, nature, and civility of review comments and (2) scoring criteria to assess the overall quality of review comments based on relevance, clarity, and conciseness.
We apply our evaluation framework to the largest existing dataset of code reviews~\cite{li2022automating}.
Given the scale of the dataset, we employ a large language model (LLM) as a judge (i.e., LLM-as-a-Judge~\cite{zheng2023judging}) to automatically annotate samples with thoroughly designed prompts to ensure reliable and consistent annotations.
We show that $85.9\%$ of the samples are related to refactoring and bugfix, with a high proportion of prescriptive comments ($62.6\%$). 
Our analysis also reveals the presence of uncivil, lengthy, unclear, and irrelevant comments, highlighting the need to improve the dataset quality.

Subsequently, we introduce \oapp, a curated dataset for code review featuring a curation pipeline to improve the clarity, conciseness, and civility of review comments. 
Specifically, we filter out irrelevant comments from the original dataset and use \emph{Llama-3.1-70B} as LLM to reformulate review comments. 
This reformulation aims at maintaining the semantics of the original review comment while improving its form concerning the three scoring criteria. 
Our curated dataset features clearer, more concise, and 100\% civil review comments. 
To illustrate the actionability of \oapp, we conduct a comparative study with the original dataset by investigating their impact on the automation of comment generation and code refinement tasks.
We fine-tune \emph{DeepSeek-6.7B-Instruct}~\cite{deepseek-coder} using \oapp and the original dataset. 
Our results show a 46\% improvement in BLEU score for comment generation and a 22\% improvement in CodeBLEU for code refinement when fine-tuning the model using \oapp compared to the original dataset.

The main contributions of this paper are threefold: (1) the design of an evaluation framework to categorize and assess the quality of code review datasets, (2) the development of an automated curation pipeline to enhance dataset quality, and (3) a comparative study between our curated dataset, \oapp, and the original dataset, demonstrating the significant benefits of our curation pipeline in improving the effectiveness of LLMs for code review tasks.


% In this paper, we conduct an empirical study on the largest existing dataset of code reviews  to analyze and evaluate its quality. 
% Our study aims to identify specific weaknesses and improvement points, which will form the foundation for proposing a data curation pipeline to enhance the dataset quality.
% We conduct a comparative study of the original and curated datasets to compare their quality.
% Additionally, we explore the practical improvements brought by the curated dataset in terms of performance on the downstream tasks.

% The main contributions of this paper are threefold: (1) an empirical study of an existing dataset of code reviews, (2) the development of a curation pipeline to improve the dataset quality, and (3) an evaluation of the curated dataset, demonstrating its benefits over the original dataset in terms of performance of automated downstream tasks. 
% Through these contributions, we aim to significantly enhance the quality of training data used in automated code review systems, ultimately improving their ability to generate more accurate and useful review comments.


% The remainder of this paper is organized as follows. 
% In \Sect{sec:methodology}, we give an overview of our methodology and research questions.
% In \Sect{sec:initdata}, we present our evaluation framework in more details

% the empirical study conducted on an existing dataset of code reviews. 
% In \Sect{sec:curdata}, we describe the proposed data curation pipeline, and we present and evaluate the curated dataset. 
% In \Sect{sec:analysis}, we conduct a more profound comparison of both datasets in terms of training improvement and usefulness. 
% In \Sect{sec:threats}, we present the threats to the validity of our work.
% In \Sect{sec:related}, we summarize related work. In \Sect{sec:conclusion}, we discuss the future research opportunities brought by this work and we conclude.





















