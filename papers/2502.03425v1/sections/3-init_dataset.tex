\section{Quality Assessment and Characterization of Existing Code Review Dataset}
\label{sec:initdata}

In this section, we present the evaluation framework, detail its application in assessing the existing code review dataset, and then present the results.

\subsection{Dataset Selection}
We employ the code review dataset introduced in \cite{li2022automating}, which is the largest publicly available dataset for code reviews. This dataset has been widely adopted in several works \cite{li2022automating, ben2024improving, sghaier2023unity, lu2023llama} for automating code review tasks.
The dataset is multilingual, covering nine programming languages, and consists of $176,613$ samples. \Table{tab:dataset_stats} shows the distribution of the dataset across the different programming languages.
The dataset includes features such as the code changes submitted for review and the review comment provided by the reviewer assigned to the pull request. 

% The dataset includes the following key features:
% \begin{itemize} 
% \item Code changes submitted for review 
% \item The full original code file associated with the review 
% \item The review comment provided by the reviewer assigned to the pull request 
% \item Code edits that implement the changes requested in the review comment 
% \item GitHub repository information 
% \item Programming language 
% \end{itemize}


\begin{table}[!t]
  \centering
  \caption{Dataset distribution over programming languages.}
  \label{tab:dataset_stats}
  \begin{tabular}{{>{\centering\arraybackslash}p{1.5cm}}*{1}{r}}
    \toprule
    \textbf{Programming Language} & \textbf{\# Samples} \\
    \midrule
    PHP & $9,984$ \\
    Ruby & $6,713$ \\
    C\# & $17,085$ \\
    C & $4,108$ \\
    Java & $35,671$ \\
    Python & $36,382$ \\
    C++ & $15,944$ \\
    Go & $36,123$ \\
    JS & $14,603$ \\
    \midrule
    \textbf{Total} & $176,613$\\
    \bottomrule
  \end{tabular}
  \vspace{-1em}
\end{table}



\subsection{Evaluation Framework}

In this section, we introduce the evaluation framework used to assess the quality and characteristics of the code reviews in existing datasets. 
As shown in \Fig{fig:eval_framework}, this framework consists of three key categorizations, along with a scoring system to evaluate the clarity, relevance, and conciseness of each review comment. These categories and criteria aim to provide a comprehensive analysis of the reviews and enable us to identify areas for improvement in the dataset.

\Table{tab:categories} presents the categorization framework for code reviews, which is used to classify each review comment based on three key aspects: \emph{Type}, \emph{Nature}, and \emph{Civility}. The \emph{Type} category identifies the primary focus of the review, such as refactoring, bugfixes, or documentation \cite{tufano2024code}. The \emph{Nature} category assesses the intent behind the comment, categorizing it as prescriptive, descriptive, or clarifying. Finally, the \emph{Civility} category evaluates the tone of the comment; whether it is civil or uncivil \cite{rahman2024words}. 
Note that \emph{Type} and \emph{Nature} categories are multi-labeled. That is, a single review comment may address multiple types of issues (\eg documentation and testing) or have a mixed nature (\eg descriptive and requests clarifications).
This categorization provides a structured approach for systematically evaluating the dataset, offering a detailed breakdown of the content and intent of review comments.



\begin{table}[!t]
  \centering
  \caption{Categorization framework for reviews comments.}
  % \vspace{-.5em}
  \label{tab:categories}
  \begin{tabularx}{\linewidth}{llX}
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Description} \\
    \midrule
    \textbf{Type} & Refactoring & Suggestions to improve code structure \\
                  & Bugfix & Identifies and suggests fixes for bugs \\
                  & Testing & Comments related to test cases \\
                  & Logging & Suggestions for logging practices \\
                  & Documentation & Recommendations for documentation changes \\
                  & Other & Any other type of comment \\
    \midrule
    \textbf{Nature} & Prescriptive & Provides specific actions or recommendations \\
                    & Descriptive & Describes a situation without suggesting changes \\
                    & Clarification & Requests or provides clarification \\
                    & Other & Any other nature of comment \\
    \midrule
    \textbf{Civility} & Civil & Respectful and professional tone \\
                      & Uncivil & Disrespectful or inappropriate tone \\
    \bottomrule
  \end{tabularx}
  \vspace{-1em}
\end{table}


\Table{tab:criteria} outlines the scoring criteria used to assess the quality of reviews in three dimensions: \emph{clarity}, \emph{relevance}, and \emph{conciseness} \cite{rani2023decade, haouari2011good}. Each criterion is scored on a scale from 1 to 10. \emph{Clarity} measures how effectively the comment communicates its message, with higher scores reflecting clearer communication. \emph{Relevance} evaluates how pertinent the comment is to the code change, and \emph{Conciseness} assesses whether the comment is brief and to the point without unnecessary details. These criteria provide a structured way to evaluate the quality and effectiveness of each comment, ensuring a comprehensive assessment of the dataset. 




\begin{table}[!t]
  \centering
  \caption{Scoring criteria for review comments.}
  % \vspace{-.5em}
  \label{tab:criteria}
  \begin{tabularx}{\linewidth}{lX}
    \toprule
    \textbf{Criterion} & \textbf{Description} \\
    \midrule
    \textbf{Clarity (1--10)} & Assesses how clearly the review comment communicates its message. A score of 1 indicates very unclear, and 10 indicates very clear communication. \\
    \textbf{Relevance (1--10)} & Evaluates the extent to which the review comment is pertinent to the code change. A score of 1 means the comment is completely irrelevant, while a score of 10 means it is highly relevant. \\
    \textbf{Conciseness (1--10)} & Measures the brevity and efficiency of the review comment, ensuring it conveys the necessary information without unnecessary elaboration. A score of 1 indicates too verbose, and 10 indicates concise and to the point. \\
    \bottomrule
  \end{tabularx}
  \vspace{-1em}
\end{table}


This framework enables a structured analysis of the reviews, allowing us to assess their quality in a granular manner and providing the foundation for identifying trends, patterns, and areas for improvement within the dataset.


\subsection{LLM-as-a-Judge}

To implement the evaluation framework on the code review dataset, we leverage an LLM, specifically \emph{Llama-3.1-70B-Instruct}, to act as annotator for the various categories and criteria. Due to the substantial size of the dataset (176,613 samples), manual evaluation is infeasible. Drawing from findings in the literature~\cite{zheng2023judging, zhuo2023ice, chang2024survey}, we assume that a highly capable LLM can serve as a reliable substitute for human evaluators, accurately assessing review comments across the defined framework.
This claim is supported by previous research demonstrating that highly capable LLMs closely align with human judgments, achieving agreement rates comparable to human-to-human agreement~\cite{zheng2024judging, li2023alpacaeval}. Furthermore, LLMs have been recently exploited as judges for different software engineering tasks~\cite{zhuo2023ice, weyssow2024codeultrafeedback}, thereby justifying their use as annotators in this work.

For conciseness, we show an excerpt of the prompt \footnote{The full prompt is available in the \href{https://github.com/AI4CodeReview/CuREV}{replication package}.} used to evaluate review comments in \Table{tab:eval_promp}.
Initially, the LLM is instructed to generate what it considers an ideal review comment based on the provided code changes. 
Following this, the LLM is asked to evaluate the given review comment according to the different categories and criteria. 
The generated review serves as an implicit reference during this evaluation. 
This process is repeated across the entire code review dataset.

% Prompt design
The prompt was chosen through multiple refinement iterations, following a trial-and-error approach based on observations. Initially, we designed a preliminary prompt and tested it on a curated set of manually selected examples. Based on the identified issues, we refined the prompt to enhance its effectiveness. This iterative process was informed by techniques from the literature that demonstrated effectiveness in similar contexts \cite{weyssow2024codeultrafeedback}. 
For instance, one challenge encountered was the tendency of LLM to generate unstructured and verbose outputs in inconsistent formats, which hindered automated parsing and extraction of relevant information. To mitigate this issue, the prompt was augmented with explicit examples of the desired output format, thereby encouraging the LLM to produce more structured and consistent results.
Another observed limitation was the propensity of LLM to assign high scores (\eg 9 or 10) across all evaluation criteria, resulting in a lack of meaningful differentiation. To address this, detailed descriptions of each criterion were incorporated into the prompt, and the LLM was instructed to generate an exemplary review as a reference. This adjustment facilitated a more balanced and discriminative grading distribution. Additionally, we asked the LLM to provide a rationale for each evaluation, effectively introducing an implicit chain-of-thought process. This approach not only improved the explainability and justification of the assigned grades but also enhanced the overall reliability and quality of the evaluations.

\begin{table}[!t]
\centering
\caption{Excerpt of the prompt for comments evaluation.}
\vspace{-.5em}
\label{tab:eval_promp}
\begin{tabularx}{\linewidth}{X}
\toprule
\textbf{\#\#\# Code review comment generation}

 Generate a review comment that you consider perfect for the code change without considering the given input comment. A review comment should highlight the main issues, improvements, or suggestions for the code changes. The generated review comment should be concise, relevant, clear, useful, and complete. \\
\vspace{.3em}
\textbf{\#\#\# Code review comment assessment}

Then, evaluate and categorize only the given review comment, written by a reviewer, based on the below criteria.
You can use the generated review comment as a reference to evaluate the given review comment.
Note that multiple labels are allowed for the categories "Type" and "Nature". \\
\\
\textsc{\textbf{1. Type:}} Categorize the review according to the type of issue it addresses: Refactoring, Bugfix, Testing, Logging, Documentation, Other.
\vspace{.3em}

\textsc{\textbf{2. Nature:}} Specify the nature of the review according to these categories:\\
- \textit{Descriptive:} describe what the reviewer observes without explicitly suggesting specific actions.\\
- \textit{Prescriptive:} suggest or request specific actions on the code.\\
- \textit{Clarification:} request explanation or further information to better understand the code changes.\\
- \textit{Other:} for comments that do not fit the previous categories.

\hspace{0.5\linewidth}\vdots

\textsc{\textbf{4. Conciseness:}} Assess how effectively the comment conveys its message using the fewest necessary words while remaining fully informative. A concise comment should be completely brief but informative, avoiding unnecessary details, repetition, or verbosity. Use a 1-to-10 rating scale.

\hspace{0.5\linewidth}\vdots

\textbf{\#\#\# Given review comment}\\
\{\textit{review\_comment\}}\\

\textbf{\#\#\# Code changes}\\
\{\textit{code\_diff\}}\\
\bottomrule
\end{tabularx}
\vspace{-1em}
\end{table}



\subsection{Sanity Check}
To ensure the reliability of the judgments made by \emph{Llama-3.1-70B-Instruct}, we conducted a sanity check involving human assessments. A random sample of $100$ review comments was manually evaluated by two authors according to the evaluation schema defined in \Fig{fig:eval_framework}. This dual-author assessment aimed to mitigate individual biases and provide a robust baseline for comparison. Conflicts between the annotators were carefully resolved through discussion to reach a consensus on each sample's evaluation.

% This manual assessment provides a human baseline against which we can compare the LLM judging performance. To measure the agreement between LLM and human judgments, we used \emph{Cohen's kappa}, a statistical measure that accounts for the degree of agreement beyond chance \cite{mchugh2012interrater}. Our analysis resulted in a Cohen's kappa value of \obs{XX}, indicating substantial agreement between the LLM's assessments and human judgments.

This manual assessment provides a human baseline against which we can compare the LLM judging performance. To measure the agreement between LLM and human judgments, we used \emph{Cohen's kappa}, a statistical measure that accounts for the degree of agreement beyond chance \cite{mchugh2012interrater}. Our analysis showed \emph{perfect agreement} for the \emph{civility} category ($1$), \emph{near-perfect agreement} for \emph{type} ($0.88$) and \emph{nature} ($0.82$) categories. For criteria, we observed \emph{near-perfect agreement} for \emph{relevance} ($0.85$), \emph{substantial agreement} for \emph{Conciseness} ($0.76$) and \emph{Clarity} ($0.64$). These values provide strong evidence of the LLM capacity to make reliable judgments.


This outcome supports existing literature suggesting that LLMs possess the capability to evaluate review comments with accuracy comparable to human reviewers \cite{zheng2024judging, li2023alpacaeval}. Furthermore, it underscores the reliability of LLMs in the specific context of judging review comments.

\subsection{Categorization Results}

\Fig{fig:initinal_categ_dist} presents the results of the different categorizations according to the \emph{type}, \emph{nature}, and \emph{civility} of the comments. The findings provide valuable insights into the original dataset characteristics and highlight potential areas of improvements.

\begin{figure*}
    \centering
    \includegraphics[width=.9\linewidth]{figures/categories_distribution.pdf}
    % \vspace{-1em}
    \caption{Distribution of the different categories across the original dataset.}
    \label{fig:initinal_categ_dist}
    \vspace{-1em}
\end{figure*}


\paragraph*{\textbf{Type of review comments}}

The majority of the review comments in the dataset fall under the \emph{Refactoring} category, comprising $80.07\%$ of the total. This indicates that much of the feedback is centered on restructuring the code to enhance its quality and performance. The next most significant category is \emph{Bugfix} at $18.60\%$, underscoring the importance of addressing functional issues within the code to avoid bugs.

Other subcategories, such as \emph{Documentation} ($4.21\%$), \emph{Testing} ($2.42\%$), and \emph{Logging} ($0.65\%$), are relatively less frequent. This suggests that these areas receive less attention in code review feedback, potentially due to implicit understanding among developers or prioritization of other aspects of the code. The \emph{Other} category, which makes up $8.97\%$ of the comments, encompasses miscellaneous feedback types that do not fall into the primary classifications, \eg security.

The focus on \emph{Refactoring} and \emph{Bugfix} comments align with the primary objectives of code review, \ie preserving code quality and preventing bugs. However, the lower representation of \emph{Documentation}, \emph{Testing}, and \emph{Logging} comments suggests potential gaps in the comprehensiveness of code review practices. Addressing this imbalance presents an opportunity for enhancing training data to support more well-rounded feedback generation. This finding can guide researchers in balancing their datasets according to the \emph{type} of review comment, ultimately leading to more varied and comprehensive model-generated feedback.


\paragraph*{\textbf{Nature of review comments}}

Review comments categorized as \emph{Prescriptive}, which provide direct guidance or specific instructions, dominate the dataset at $62.6\%$. This reflects a strong focus on actionable feedback, aiding developers in making precise code changes. \emph{Clarification} comments make up $24.6\%$, indicating a notable effort to ensure code understanding among team members. \emph{Descriptive} comments, which explain aspects of the code without providing explicit recommendations, account for $12.8\%$.
The \emph{Other} category is negligible at $0.01\%$, showing that most comments fit well into defined subcategories.

The prevalence of \emph{prescreptive} comments highlights a code review culture geared towards providing direct solutions and explicit feedback. The significant presence of \emph{Clarification} comments suggests that maintaining clear communication and understanding is a priority during the review process.


\paragraph*{\textbf{Civility of review comments}}

Most comments in the dataset are categorized as \emph{Civil}, making up $98.77\%$ of the total. This suggests that the code review process is generally conducted professionally and constructively. However, there is still a portion ($1.23\%$) of \emph{Uncivil} comments present in the dataset. These comments often contain harsh or inappropriate language that could negatively influence a model learning process.
Training on such comments risks introducing undesirable patterns, potentially leading to the model generating inappropriate language in downstream tasks.

To mitigate this, it is crucial to either curate these reviews to remove harsh language or exclude them altogether to prevent models from learning from these undesirable examples and reinforcing negative behavior.


\subsection{Scoring Criteria Results}

\Fig{fig:init_scoring_dist} depicts scoring criteria distribution across the original dataset. \Table{tab:init_categories_distribution} provides a summary of the average values for each scoring criterion within the different categories.


\begin{table}[!t]
  \centering
  \caption{Average values of the scoring criteria per category across the original dataset.}
  \label{tab:init_categories_distribution}
  \begin{tabular}{>{\centering\arraybackslash}p{1cm} >{\centering\arraybackslash}p{1.5cm}*{3}{c}}
    \toprule
    \textbf{Category} & \textbf{Subcategory} & \textbf{Relevance} & \textbf{Clarity} & \textbf{Conciseness} \\
    \midrule
    \multirow{6}{*}{\textbf{Type}} & Refactoring & $8.32$ & $7.79$ & $6.99$ \\
    & Bugfix & $8.53$ & $7.74$ & $6.84$ \\
    & Testing & $8.42$ & $7.92$ & $6.97$ \\
    & Logging & $8.43$ & $7.84$ & $6.85$ \\
    & Documentation & $8.33$ & $7.62$ & $6.72$ \\
    & Other & $7.02$ & $6.86$ & $5.90$ \\
    \midrule
    \multirow{4}{*}{\textbf{Nature}} & Descriptive & $7.14$ & $6.63$ & $5.61$ \\
    & Prescriptive & $8.52$ & $7.95$ & $7.19$ \\
    & Clarification & $8.29$ & $7.57$ & $6.66$ \\
    & Other & $4.24$ & $4.40$ & $4.12$ \\
    \midrule
    \multirow{2}{*}{\textbf{Civility}} & Civil & $8.26$ & $7.75$ & $6.93$ \\
    & Uncivil & $5.60$ & $4.34$ & $4.34$ \\
    \midrule
    \multirow{1}{*}{\textbf{Average}} & -- & $8.23$ & $6.89$ & $7.71$ \\
    \bottomrule
  \end{tabular}
  \vspace{-1em}
\end{table}





\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{figures/violin_plots_scores.pdf}
    % \vspace{-.5em}
    \caption{Distribution of scoring criteria on the original dataset.}
    \label{fig:init_scoring_dist}
    \vspace{-1em}
\end{figure}



\paragraph*{\textbf{Relevance of review comments}}
The highest relevance scores are found in the \emph{Bugfix} ($8.53$) and \emph{Prescriptive} ($8.52$) subcategories. The lowest relevance scores are observed in the \emph{Other} subcategories of both the \emph{Type} ($7.02$) and \emph{Nature} ($4.24$) categories, as well as in \emph{Uncivil} comments ($5.60$).
The low relevance of these subcategories indicates that such comments may often lack the focus or constructive value necessary for high-quality code reviews. 

The relevance distribution, illustrated in \Fig{fig:init_scoring_dist}, indicates that while the majority of comments are relevant, a portion of irrelevant comments persists, diminishing the overall quality of the dataset. These irrelevant comments can negatively influence language models' performance by introducing unhelpful patterns. 
This study can support the filtering of irrelevant comments to ensure a high-quality dataset, as maintaining data relevance is crucial for effective model training and downstream task performance.


\paragraph*{\textbf{Clarity of review comments}}

Clarity scores across the dataset are varied, with the highest scores found in the \emph{Prescriptive} $(7.95$), \emph{Testing} ($7.92$), and \emph{Refactoring} ($7.79$) subcategories, while the lowest clarity scores appear in the \emph{Other} subcategories ($4.40$) and \emph{Uncivil} comments ($4.34$).

The data indicates that \emph{Prescriptive} comments, which provide direct and specific guidance, are not only relevant but also clear, making them highly effective in guiding developers. High clarity scores in the \emph{Testing} and \emph{Refactoring} categories further suggest that comments in these areas are typically well-understood. In contrast, comments categorized as \emph{Other} and \emph{Uncivil} exhibit significantly lower clarity, which can lead to misunderstandings and inefficiencies during the code review process. The low clarity score (average=$4.89$) underscores the importance of enhancing the clarity of review comments to improve dataset quality, ultimately supporting the training of language models to produce clear and precise feedback that developers can easily comprehend and act upon.


\paragraph*{\textbf{Conciseness of review comments}}

The analysis of conciseness scores reveals that the highest average scores are found in the \emph{Prescriptive} ($7.19$), \emph{Refactoring} ($6.99$), and \emph{Testing} ($6.97$) subcategories. The lowest conciseness scores are observed in \emph{Other} subcategories for both \emph{Type} ($5.90$) and \emph{Nature} ($4.12$), as well as in \emph{Uncivil} comments ($4.34$).

The high conciseness scores in \emph{Prescriptive} comments indicate that these types of feedback tend to be not only clear and relevant but also succinct, contributing to more efficient communication. 
The lower conciseness scores suggest that these comments may be more verbose or include unnecessary language, reducing their effectiveness.

As shown in \Fig{fig:init_scoring_dist}, a significant portion of the dataset consists of comments that lack conciseness. This suggests an opportunity for enhancing the quality of the dataset by refining review comments to be more succinct, removing unnecessary elements, and conveying the message more efficiently. This improvement would enable language models to focus on the core content of the comments, emphasizing essential information over extraneous details.




\begin{center}
\begin{tcolorbox}[colframe=teal!75!black, colback=teal!5!white, title=\textbf{Answer to RQ1}]
The code reviews dataset is characterized by a strong focus on \emph{refactoring} ($80.07\%$) and \emph{bugfix} ($18.60\%$). Most of the comments are \emph{prescriptive} ($62.6\%$) providing direct and actionable suggestions to developers.
However, the presence of uncivil, lengthy, unclear, and irrelevant comments highlights areas for improvement to enhance dataset quality.
\end{tcolorbox}
\end{center}