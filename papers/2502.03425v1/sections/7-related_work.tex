\section{Related Work}
\label{sec:related}

To assist developers in code review, various techniques and tools have been proposed to automate review comment generation.
Early approaches relied on information retrieval methods. For instance, Hong \etal \cite{hong2022commentfinder} introduced CommentFinder, an approach that retrieves relevant past review comments for new code changes. Similarly, Gupta et al. \cite{gupta2018intelligent} developed DeepCodeReviewer (DCR), an LSTM-based model that predicts relevant reviews by ranking them based on code similarity. Although these methods effectively recommend existing comments, their limitation lies in the inability to generate new comments for unseen code.


More advanced solutions leverage language models. Tufano \etal \cite{tufan2021towards, tufano2022using} used the T5 transformer, pre-trained with a masked language modeling task, and fine-tuned to generate review comments for Java code. Building on this, Li \etal \cite{li2022automating} introduced a CodeT5 model pre-trained on tasks specifically designed for code review, such as quality estimation and comment generation. This demonstrated significant advancements in handling multilingual datasets and downstream tasks, including comment generation and code refinement.


Recent work aimed to enhance the performance of the finetuning-based approaches. Sghaier \etal \cite{sghaier2024improving} proposed DISCOREV, an approach that incorporates cross-task knowledge distillation, connecting quality estimation, comment generation, and code refinement. This technique uses a cascade of models where each task informs the fine-tuning of the next, showing improvements over previous models.


Although these approaches have shown promising results, none of the existing works in the literature examined the quality of code review datasets or implemented preprocessing techniques to curate the data. Instead, most efforts focused on the fine-tuning phase of pre-trained language models. This leaves a significant gap in addressing the limitations of existing raw datasets. Our work proposes curating a code review dataset to improve the automation of code review tasks, overcoming the challenges posed by noisy and unrefined data.









