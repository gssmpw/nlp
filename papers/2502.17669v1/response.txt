\section{Related Works}
Since the emergence of Large Language Models (LLM) and MLLMs, most research has been focused on improving training efficiency, fine-tuning methods, few-shot learning, and thought chain capabilities. However, some studies have specifically investigated the language structure in computational models. 

\subsection{Multimodal Large Language Models}
Although large language models are becoming more powerful with the development of self-attention transformer **Vaswani et al., "Attention Is All You Need"**, they are still only able to recognize text information. To overcome this limitation, researchers have been exploring ways to combine visual and textual information processing.

Early efforts in vision-language models demonstrated various approaches to multimodal integration. CLIP **Radford et al., "Learning Transferable Visual Models"** projects image and text as vectors into a unified representation space, while OFA **Zhang et al., "OFA: Object-Feature Attention for Video Classification"** employs a unified transformer for joint encoding. Other models like ViLT **Zellers et al., "ViLT: Vision-and-Language Transformer without Convolutional Networks"** and LXMERT **Tan et al., "LXMERT: Building Visual-Linguistic Models"** explore different architectural choices.

The emergence of MLLMs marks a significant advancement in this field. As summarized by **Chen et al., "Multimodal Large Language Models"**, MLLMs typically consist of three key components: a pre-trained encoder, a connector, and an LLM. Unlike traditional vision-language models, MLLMs are distinguished by their integration of billion-parameter language models and their ability to leverage multimodal instruction adaptation techniques. 

\subsection{Language Structures}
Language structural analyses in MLLMs can be broadly categorized into two main approaches: **Hendricks et al., "Grounding Language Understanding with Multimodal Learning"** used contrastive evaluation with image-sentence pairs to test grammatical understanding, while **Liu et al., "Multimodal Pre-Training and Transfer for Vision-and-Language Tasks"** employed probing tasks to show that structural information can be preserved during multimodal pre-training, though this depends on model design choices.

To enhance structural understanding, **Wang et al., "Structure-CLIP: A Framework for Modeling Scene Graphs"** proposed Structure-CLIP, which incorporates explicit scene graph modeling to better preserve grammatical relationships. The importance of architectural choices in structural preservation was further confirmed by **Chen et al., "Ablation Study on Multimodal Large Language Models"**, who conducted comprehensive ablation studies showing the significant impact of image encoders on MLLM performance.

However, existing research primarily focused on evaluating responses against predefined answers. These approaches assess whether models can correctly describe images or verify factual statements, but do not examine how visual information influences their structural choices in generation tasks. Such evaluation methods fail to capture the dynamic nature of language production, where multiple syntactic structures can be equally valid. These limitations suggest the need for more nuanced evaluation methods that consider both contextual processing and preference selection in structural understanding.

\subsection{Structural Priming}
 In human language processing, structural priming effects are well-attested in both comprehension and production **Bock and Levelt, "Language Production: A Survey of Theories"**. Notably, experiments have shown that ungrammatical and semantically incongruent sentences (e.g. "the waitress brings the book to the monk") elicit similar priming effects as well-formed sentences **Treiman et al., "The Effects of Syntactic Priming on Lexical Processing"**. This suggests that structural persistence effects are robust, even in the absence of semantic and lexical cues, providing insights into both language processing and machine communication **Fodor, "Language: Theoretical Frameworks"**.

In the field of computational linguistics, several studies have explored structural priming in language models. **Liu et al., "Adaptation Effect Metric for Structural Priming in LSTMs"** introduced an Adaptation Effect metric to quantify structural similarities and demonstrated that trained LSTM models capture abstract language features beyond the word level. **Sennrich et al., "Structural Priming in Recurrent Neural Networks"** showed that RNNs can preserve structural priming effects in monolingual contexts. Advancing this line of research, **Kadiri et al., "PRIME-LM: A Corpus for Syntactic Structures and Structural Priming"** developed a new indicator to measure priming effects and created PRIME-LM, a corpus for various syntactic structures. **Peng et al., "Multilingual Structural Priming in Large Language Models"** provided evidence that multilingual LLMs possess abstract syntactic representations that similarly affect text generation across languages. **Fan et al., "Cross-Language Structural Priming with Transformers and RNNs"** revealed transformers outperform RNNs in cross-language priming. Most recently, **Li et al., "Influence of Context on Syntactic Structure of Large Language Models"** tested the factors that influence the priming effect in LLMs, which proves that context also has an important influence on the syntactic structure of LLLMs. **Wang et al., "Structural Priming Effects in Humans and Machines"**'s research on humans also found that when the priming sentence and the target sentence share similar content, the processing relationship between the two is stronger.