@article{DBLP:journals/corr/abs-1908-07490,
  author       = {Hao Tan and
                  Mohit Bansal},
  title        = {{LXMERT:} Learning Cross-Modality Encoder Representations from Transformers},
  journal      = {CoRR},
  volume       = {abs/1908.07490},
  year         = {2019},
  url          = {http://arxiv.org/abs/1908.07490},
  eprinttype    = {arXiv},
  eprint       = {1908.07490},
  timestamp    = {Fri, 17 Nov 2023 16:26:52 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1908-07490.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-11115,
  author       = {Adam Dahlgren Lindstr{\"{o}}m and
                  Suna Bensch and
                  Johanna Bj{\"{o}}rklund and
                  Frank Drewes},
  title        = {Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic
                  Case},
  journal      = {CoRR},
  volume       = {abs/2102.11115},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.11115},
  eprinttype    = {arXiv},
  eprint       = {2102.11115},
  timestamp    = {Wed, 24 Feb 2021 15:42:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-11115.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={ICML},
  year={2021}
}

@incollection{frank2019neural,
  title={Neural network models of language acquisition and processing},
  author={Frank, Stefan L and Monaghan, Padraic and Tsoukala, Chara},
  booktitle={Human language: From genes and brain to behavior},
  pages={277--293},
  year={2019},
  publisher={MIT Press}
}

@misc{huang2023structureclipscenegraphknowledge,
      title={Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations}, 
      author={Yufeng Huang and Jiji Tang and Zhuo Chen and Rongsheng Zhang and Xinfeng Zhang and Weijie Chen and Zeng Zhao and Zhou Zhao and Tangjie Lv and Zhipeng Hu and Wen Zhang},
      year={2023},
      eprint={2305.06152},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.06152}, 
}

@article{ivanova2017you,
  title={Do you what I say? People reconstruct the syntax of anomalous utterances},
  author={Ivanova, Iva and Branigan, Holly P and McLean, Janet F and Costa, Albert and Pickering, Martin J},
  journal={Language, Cognition and Neuroscience},
  volume={32},
  number={2},
  pages={175--189},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{jumelet2024language,
  title={Do Language Models Exhibit Human-like Structural Priming Effects?},
  author={Jumelet, Jaap and Zuidema, Willem and Sinclair, Arabella},
  journal={arXiv preprint arXiv:2406.04847},
  year={2024}
}

@misc{kim2021viltvisionandlanguagetransformerconvolution,
      title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision}, 
      author={Wonjae Kim and Bokyung Son and Ildoo Kim},
      year={2021},
      eprint={2102.03334},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2102.03334}, 
}

@article{linzen2021syntactic,
  title={Syntactic structure from deep learning},
  author={Linzen, Tal and Baroni, Marco},
  journal={Annual Review of Linguistics},
  volume={7},
  number={1},
  pages={195--212},
  year={2021},
  publisher={Annual Reviews}
}

@misc{mckinzie2024mm1methodsanalysis,
      title={MM1: Methods, Analysis \& Insights from Multimodal LLM Pre-training},
      author={Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu HÃ¨ and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Guoli Yin and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang},
      year={2024},
      eprint={2403.09611},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.09611}, 
}

@article{michaelov_structural_2023,
	title = {Structural {Priming} {Demonstrates} {Abstract} {Grammatical} {Representations} in {Multilingual} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2311.09194},
	doi = {10.48550/ARXIV.2311.09194},
	abstract = {Abstract grammatical knowledge - of parts of speech and grammatical patterns - is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.},
	urldate = {2024-04-05},
	author = {Michaelov, James A. and Arnett, Catherine and Chang, Tyler A. and Bergen, Benjamin K.},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
    journal = {arXiv preprint arXiv:2311.09194},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
Accepted at EMNLP 2023},
}

@inproceedings{nikolaus2022vision,
  title={Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?},
  author={Nikolaus, Mitja and Salin, Emmanuelle and Ayache, Stephane and Fourtassi, Abdellah and Favre, Benoit},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1538--1555},
  year={2022}
}

@InProceedings{pmlr-v162-wang22al,
  title = 	 {{OFA}: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  author =       {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {23318--23340},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wang22al/wang22al.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wang22al.html},
  abstract = 	 {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision &amp; language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.}
}

@article{prasad2019using,
  title={Using priming to uncover the organization of syntactic representations in neural language models},
  author={Prasad, G},
  journal={arXiv preprint arXiv:1909.10579},
  year={2019}
}

@article{sinclair-etal-2022-structural,
    title = "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
    author = "Sinclair, Arabella  and
      Jumelet, Jaap  and
      Zuidema, Willem  and
      Fern{\'a}ndez, Raquel",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.60",
    doi = "10.1162/tacl_a_00504",
    pages = "1031--1050",
    abstract = "We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model{'}s internal states.1",
}

@article{tooley2014parity,
  title={On the parity of structural persistence in language production and comprehension},
  author={Tooley, Kristen M and Bock, Kathryn},
  journal={Cognition},
  volume={132},
  number={2},
  pages={101--136},
  year={2014},
  publisher={Elsevier}
}

@article{tooley2025putting,
  title={Putting the prime in priming: Using prime processing behavior to predict target structural processing},
  author={Tooley, Kristen M and Brehm, Laurel},
  journal={Psychonomic Bulletin \& Review},
  pages={1--12},
  year={2025},
  publisher={Springer}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@misc{zhang2024modelingbilingualsentenceprocessing,
      title={Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming}, 
      author={Demi Zhang and Bushi Xiao and Chao Gao and Sangpil Youm and Bonnie J Dorr},
      year={2024},
      eprint={2405.09508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.09508}, 
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

