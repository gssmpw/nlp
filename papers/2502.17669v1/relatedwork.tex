\section{Related Works}
Since the emergence of Large Language Models (LLM) and MLLMs, most research has been focused on improving training efficiency, fine-tuning methods, few-shot learning, and thought chain capabilities. However, some studies have specifically investigated the language structure in computational models. 

\subsection{Multimodal Large Language Models}
Although large language models are becoming more powerful with the development of self-attention transformer \citep{zhao2023survey}, they are still only able to recognize text information. To overcome this limitation, researchers have been exploring ways to combine visual and textual information processing.

Early efforts in vision-language models demonstrated various approaches to multimodal integration. CLIP \citep{Radford2021LearningTV} projects image and text as vectors into a unified representation space, while OFA \citep{pmlr-v162-wang22al} employs a unified transformer for joint encoding. Other models like ViLT \citep{kim2021viltvisionandlanguagetransformerconvolution} and LXMERT \citep{DBLP:journals/corr/abs-1908-07490} explore different architectural choices.

The emergence of MLLMs marks a significant advancement in this field. As summarized by \citet{yin2023survey}, MLLMs typically consist of three key components: a pre-trained encoder, a connector, and an LLM. Unlike traditional vision-language models, MLLMs are distinguished by their integration of billion-parameter language models and their ability to leverage multimodal instruction adaptation techniques. 

\subsection{Language Structures}
Language structural analyses in MLLMs can be broadly categorized into two main approaches: \citet{nikolaus2022vision} used contrastive evaluation with image-sentence pairs to test grammatical understanding, while \citet{DBLP:journals/corr/abs-2102-11115} employed probing tasks to show that structural information can be preserved during multimodal pre-training, though this depends on model design choices.

To enhance structural understanding, \citet{huang2023structureclipscenegraphknowledge} proposed Structure-CLIP, which incorporates explicit scene graph modeling to better preserve grammatical relationships. The importance of architectural choices in structural preservation was further confirmed by \citet{mckinzie2024mm1methodsanalysis}, who conducted comprehensive ablation studies showing the significant impact of image encoders on MLLM performance.

However, existing research primarily focused on evaluating responses against predefined answers. These approaches assess whether models can correctly describe images or verify factual statements, but do not examine how visual information influences their structural choices in generation tasks. Such evaluation methods fail to capture the dynamic nature of language production, where multiple syntactic structures can be equally valid. These limitations suggest the need for more nuanced evaluation methods that consider both contextual processing and preference selection in structural understanding.

\subsection{Structural Priming}
 In human language processing, structural priming effects are well-attested in both comprehension and production \citep{tooley2014parity}. Notably, experiments have shown that ungrammatical and semantically incongruent sentences (e.g. "the waitress brings the book to the monk") elicit similar priming effects as well-formed sentences \citep{ivanova2017you}. This suggests that structural persistence effects are robust, even in the absence of semantic and lexical cues, providing insights into both language processing and machine communication \citep{linzen2021syntactic}.

In the field of computational linguistics, several studies have explored structural priming in language models. \citet{prasad2019using} introduced an Adaptation Effect metric to quantify structural similarities and demonstrated that trained LSTM models capture abstract language features beyond the word level. \citet{frank2019neural} showed that RNNs can preserve structural priming effects in monolingual contexts. Advancing this line of research, \citet{sinclair-etal-2022-structural} developed a new indicator to measure priming effects and created PRIME-LM, a corpus for various syntactic structures. \citet{michaelov_structural_2023} provided evidence that multilingual LLMs possess abstract syntactic representations that similarly affect text generation across languages. \citet{zhang2024modelingbilingualsentenceprocessing} revealed transformers outperform RNNs in cross-language priming. Most recently, \citet{jumelet2024language} tested the factors that influence the priming effect in LLMs, which proves that context also has an important influence on the syntactic structure of LLMs. \citet{tooley2025putting}'s research on humans also found that when the priming sentence and the target sentence share similar content, the processing relationship between the two is stronger.

%