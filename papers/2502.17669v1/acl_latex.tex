% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
%\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{afterpage}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models}


\author[1]{Bushi Xiao}
\author[1]{Michael Bennie}
\author[1]{Jayetri Bardhan}
\author[1]{Daisy Zhe Wang}
\affil[1]{%
  University of Florida\\
  \texttt{\{xiaobushi, michaelbennie, jayetri.bardhan\}@ufl.edu}, \texttt{daisyw@cise.ufl.edu}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
We introduced PRISMATIC, the first multimodal structural priming dataset, and proposed a reference-free evaluation metric that assesses priming effects without predefined target sentences. Using this metric, we constructed and tested models with different multimodal encoding architectures (dual encoder and fusion encoder) to investigate their structural preservation capabilities. Our findings show that models with both encoding methods demonstrate comparable syntactic priming effects. However, only fusion-encoded models exhibit robust positive correlations between priming effects and visual similarity, suggesting a cognitive process more aligned with human psycholinguistic patterns. This work provides new insights into evaluating and understanding how syntactic information is processed in multimodal language models.
\end{abstract}
\section{Introduction}
Structural priming refers to the tendency to reuse previously encountered linguistic structures in psycholinguistics \citep{pickering2008structural}. To systematically study this phenomenon in multimodal contexts, we present PRISMATIC (PRIming through Syntactic Manipulation And Text-Image Coupling), a syntactic priming dataset derived from Flickr30k \citep{young-etal-2014-image}. The dataset comprises 16 distinct syntactic structures paired with aligned images. Figure \ref{fig:dataset} shows one of the examples in our dataset. The dataset is constructed using a template-based methodology and validated by professional linguists to ensure quality. This resource is specifically designed to evaluate structural preservation capabilities in multimodal language models through various syntactic representations.

\begin{figure}[h]
    \centering  
    \includegraphics[width=0.9\linewidth]{figure/dataset.png}
    \caption{Each image in the PRISMATIC dataset has multiple descriptions with the same semantics but different syntax.
    }
    \label{fig:dataset}
\end{figure}

Previous studies show that language models exhibit priming effects, but their evaluations have largely focused on probability prediction tasks with fixed input as the target sentences, limiting the understanding of models' true generative capabilities. To address this research gap, we developed a new metric based on the tree kernel algorithm to evaluate structural priming effects without requiring reference answers.

We conducted controlled experiments comparing two distinct architectural approaches: a dual-encoder model that processes visual and semantic information separately through a multilayer perceptron embedding, and a fusion-encoder model based on pretrained OFA-Sys/ofa-large \citep{pmlr-v162-wang22al} that directly integrates visual and semantic information\footnote{Code: 

\url{https://github.com/michaelbennieUFL/2025MLLM}
Data:

\url{https://github.com/kitayamachingtak/PRISMATIC}}.

 This paper also evaluated open-source multimodal language models, including LLaVA-v1.5-7B (fusion encoding) \citep{liu2023visualinstructiontuning}  and BLIP-2-OPT-2.7b (dual encoding)\citep{li2023blip2bootstrappinglanguageimagepretraining}. Using our PE metric, we performed controlled experiments to evaluate structural priming effects across all models.

Results revealed that multimodal language models exhibit human-like structural priming effects when priming elements are introduced, despite their different encoding mechanisms. This is contrary to the common inference that the dual encoding model has a weak effect on syntactic structure. Further correlation analysis between priming information similarity and PE scores yielded a significant finding: fusion-encoded models demonstrate strong correlations between syntactic priming effects and visual content similarity. This aligns with human cognitive patterns, which suggests that fusion encoding may better approximate human information processing mechanisms, offering potential scientific insights into cognition mechanisms.
\\[5pt]
Our primary contributions are:
\begin{enumerate}
\item We introduced PRISMATIC, the first multimodal structural priming dataset with aligned image-sentence pairs. This dataset enables systematic evaluation of multimodal language models' (MLLMs') sensitivity to syntactic structures.

\item We invented a new evaluation metric based on tree kernel algorithm \citep{moschitti2006making}. This methodology quantifies structural priming effects by analyzing syntactic similarities between positive-negative prime pairs, without requiring predefined correct answers.

\item We conducted controlled experiments examining dual and fusion encoding methods in MLLMs, revealing new insights into how visual information affects syntactic choices. 
\end{enumerate}

\section{Related Works}
Since the emergence of Large Language Models (LLM) and MLLMs, most research has been focused on improving training efficiency, fine-tuning methods, few-shot learning, and thought chain capabilities. However, some studies have specifically investigated the language structure in computational models. 

\subsection{Multimodal Large Language Models}
Although large language models are becoming more powerful with the development of self-attention transformer \citep{zhao2023survey}, they are still only able to recognize text information. To overcome this limitation, researchers have been exploring ways to combine visual and textual information processing.

Early efforts in vision-language models demonstrated various approaches to multimodal integration. CLIP \citep{Radford2021LearningTV} projects image and text as vectors into a unified representation space, while OFA \citep{pmlr-v162-wang22al} employs a unified transformer for joint encoding. Other models like ViLT \citep{kim2021viltvisionandlanguagetransformerconvolution} and LXMERT \citep{DBLP:journals/corr/abs-1908-07490} explore different architectural choices.

The emergence of MLLMs marks a significant advancement in this field. As summarized by \citet{yin2023survey}, MLLMs typically consist of three key components: a pre-trained encoder, a connector, and an LLM. Unlike traditional vision-language models, MLLMs are distinguished by their integration of billion-parameter language models and their ability to leverage multimodal instruction adaptation techniques. 

\subsection{Language Structures}
Language structural analyses in MLLMs can be broadly categorized into two main approaches: \citet{nikolaus2022vision} used contrastive evaluation with image-sentence pairs to test grammatical understanding, while \citet{DBLP:journals/corr/abs-2102-11115} employed probing tasks to show that structural information can be preserved during multimodal pre-training, though this depends on model design choices.

To enhance structural understanding, \citet{huang2023structureclipscenegraphknowledge} proposed Structure-CLIP, which incorporates explicit scene graph modeling to better preserve grammatical relationships. The importance of architectural choices in structural preservation was further confirmed by \citet{mckinzie2024mm1methodsanalysis}, who conducted comprehensive ablation studies showing the significant impact of image encoders on MLLM performance.

However, existing research primarily focused on evaluating responses against predefined answers. These approaches assess whether models can correctly describe images or verify factual statements, but do not examine how visual information influences their structural choices in generation tasks. Such evaluation methods fail to capture the dynamic nature of language production, where multiple syntactic structures can be equally valid. These limitations suggest the need for more nuanced evaluation methods that consider both contextual processing and preference selection in structural understanding.

\subsection{Structural Priming}
 In human language processing, structural priming effects are well-attested in both comprehension and production \citep{tooley2014parity}. Notably, experiments have shown that ungrammatical and semantically incongruent sentences (e.g. "the waitress brings the book to the monk") elicit similar priming effects as well-formed sentences \citep{ivanova2017you}. This suggests that structural persistence effects are robust, even in the absence of semantic and lexical cues, providing insights into both language processing and machine communication \citep{linzen2021syntactic}.

In the field of computational linguistics, several studies have explored structural priming in language models. \citet{prasad2019using} introduced an Adaptation Effect metric to quantify structural similarities and demonstrated that trained LSTM models capture abstract language features beyond the word level. \citet{frank2019neural} showed that RNNs can preserve structural priming effects in monolingual contexts. Advancing this line of research, \citet{sinclair-etal-2022-structural} developed a new indicator to measure priming effects and created PRIME-LM, a corpus for various syntactic structures. \citet{michaelov_structural_2023} provided evidence that multilingual LLMs possess abstract syntactic representations that similarly affect text generation across languages. \citet{zhang2024modelingbilingualsentenceprocessing} revealed transformers outperform RNNs in cross-language priming. Most recently, \citet{jumelet2024language} tested the factors that influence the priming effect in LLMs, which proves that context also has an important influence on the syntactic structure of LLMs. \citet{tooley2025putting}'s research on humans also found that when the priming sentence and the target sentence share similar content, the processing relationship between the two is stronger.

%\section{Research Gap and Contributions}

%Existing computational models have merely focused on text. There remains a gap in multimodal contexts. Our primary contributions are:


\section{PRISMATIC Dataset}
PRISMATIC (PRIming through Syntactic Manipulation And Text-Image Coupling) comprises 4,208 sentences paired with 1,710 aligned images. Each image is annotated with multiple descriptive sentences with each sentence labeled with specific syntactic structures. This dataset serves as a benchmark for evaluating visual-language models' ability to integrate visual perception with syntactic comprehension.

\subsection{Dataset Construction}

The PRISMATIC dataset is built based on images and captions from Flickr30k \citep{young-etal-2014-image}, which contains 31,000 images with 5 caption sentences for each image.

\textbf{Reconstruct Syntax Trees:} The syntactic structure of each caption was converted to a syntax tree using the Natural Language Toolkit (NLTK) \citep{bird-loper-2004-nltk}. This identifies grammatical dependencies between words. Subsequently, each word of each syntax tree was assigned a label that describes its syntactic role.

\textbf{Fit into Templates:} The processed words with labels were fit into a set of predefined templates corresponding to various priming structures (see Appendix \ref{tab:syntax-templates} for details).
\\[5pt]
For example:

\begin{verbatim}
The(DET_det) talented(ADJ_mod) 
artist(WORD_NOUN) performs(WORD_VERB) 
art(WORD_NOUN) to(PREP) 
the(DET_det) audience(WORD_NOUN)
\end{verbatim}

These templates served to restructure the syntax patterns while preserving the core semantic content.

\textbf{Sentence Filtering:} GPT-2 \citep{radford2019language} is used to calculate the perplexity \citep{jelinek1977perplexity}, which removes illogical sentences\footnote{Perplexity Threshold=300}.

\textbf{Grammar Correction:} A fine-tuned version of the flan-t5-large model \citep{raheja2023coedittexteditingtaskspecific} was used to correct grammar mistakes in the generated outputs.

\textbf{Human Annotation:} A human annotator made further corrections. See Section 3.3 for details.

\subsection{Auto Evaluation}
To evaluate the quality of our generated dataset, we conducted a comprehensive assessment. As outlined in Table \ref{tab:data-stats}, the dataset contains 4,896 sentences with a total of 49,470 words.
%\footnote{Statistics before annotation in Appendix \ref{app:overall-stats}}.

\begin{table}[htbp]
\centering
\caption{Analysis of the Generated Dataset before Correction}
\begin{tabular}{lr}
\hline
Feature & Value \\
\hline
Total Sentences & 4,896 \\
Total Words & 49,470 \\
Word Error Rate & 0.0106 \\
\hline
Total Tokens & 56,294 \\
Token Types & 6293 \\
TTR & 0.1118 \\
Avg Tokens per Sentence & 11.49 \\
Token Range per Sentence & (4, 45) \\
\hline
Avg Perplexity & 116.77 \\
Perplexity Range & (10.14, 298.76) \\
\hline
\end{tabular}
\label{tab:data-stats}
\end{table}

With an average of 11.49 tokens per sentence, the sentence lengths range from 4 to 45 tokens. The dataset comprises 56,294 total tokens with 6,293 unique token types. To measure lexical diversity, we calculated the Type-Token Ratio (TTR) \citep{richards1987type}, which indicates the vocabulary richness of our dataset.

To assess sentence fluency and naturalness, we measured perplexity \citep{jelinek1977perplexity}, obtaining an average score of 116.77. Lower perplexity scores indicate more natural-sounding sentences. Appendix \ref{app:Perplexity_table} shows more detailed features for each syntax type.

We leveraged the LanguageTool Python library 2.8.2\footnote{https://pypi.org/project/language-tool-python/} to evaluate grammatical accuracy.


%Furthermore, we evaluate the alignment between images and their corresponding sentences using CLIPScore \citep{hessel2021clipscore} which measure the semantic similarity in the joint embedding space. This ensures that the generated captions maintain a strong visual-linguistic correspondence.

\subsection{Human Annotation and Evaluation}
Relying solely on automatic evaluation is insufficient to assess the quality of the multimodal dataset. To address this, one of our authors who is a native English speaker majoring in Linguistics evaluated and refined the dataset across three dimensions:

1. Semantic alignment between images and sentences (Error rate: 13.97\%)

2. Structural alignment of sentences and labels (Error rate: 4.41\%)

3. Grammatical accuracy (Error Rate: 10.29\%)

Based on this evaluation, the annotator was requested to remove sentences and images with severe errors from the dataset, while sentences with minor issues were corrected to maintain overall data quality.

\section{Metrics}

\begin{figure}[h]
    \centering  
    \includegraphics[width=\linewidth]{figure/demo1.png}
    \caption{Previous method to test structural priming effect on LLMs proposed by \citet{prasad2019using}.}
    \label{fig:previous}

    \vspace{0.5 cm} 

    \includegraphics[width=\linewidth]{figure/demo2.png}
    \caption{Our leveraged MLLM model to produce target sentence directly.}
    \label{fig:ours}
\end{figure}

Traditional evaluation methods for syntactic priming typically involve simultaneous input of both prime and target sentences into language models. Figure \ref{fig:previous} illustrates previous studies that evaluate the priming effect based on token probability \cite{prasad2019using, sinclair-etal-2022-structural, michaelov_structural_2023, jumelet2024language}. In that scenario, both the Prime Sentence and the Target Sentence are predetermined inputs, while LLM outputs the surprisal (token probability) to indicate the priming level. Although this framework is concrete and simple, it does not examine the model's ability to generate complete sentences. 

Our fairer evaluation method is to input the semantic information of the priming sentence and the visual information that aligns with the target sentence at the same time. In Figure \ref{fig:ours}, the MLLM is required to predict the target sentence. Therefore, we propose a new metric based on the tree kernel algorithm \citep{moschitti2006making}. Any machine-predicted sentence will be directly compared with the prime sentence to get the Priming Effect Score.

\subsection{Tree Kernel}
The tree kernel method \citep{moschitti2006making} calculates the structural similarity between two sentences by comparing their syntax trees \citep{DBLP:journals/corr/TaiSM15}. Given two trees $T_1$ and $T_2$, their Tree Kernel is defined as:
\begin{equation}
   K(T_1,T_2) = \sum_{n_1 \in N_1} \sum_{n_2 \in N_2} \Delta(n_1,n_2)
\end{equation}
where $N_1$ and $N_2$ are the sets of nodes in trees $T_1$ and $T_2$ respectively, and $\Delta(n_1,n_2)$ represents the number of common substructures between subtrees rooted at nodes $n_1$ and $n_2$ (see Appendix~\ref{app:delta} for $\Delta(n_1,n_2)$ calculation).

To obtain a normalized similarity score, we use:
\begin{equation}
   K_{norm}(T_1,T_2) = \frac{K(T_1,T_2)}{\sqrt{K(T_1,T_1) \cdot K(T_2,T_2)}}
\end{equation}
Finally, the tree distance can be derived from the kernel function:
\begin{equation}
   d(T_1,T_2) = \sqrt{2 - 2K_{norm}(T_1,T_2)}
\end{equation}
This method effectively captures structural relationships in sentences and provides interpretable similarity measures without considering semantic difference.

\subsection{New PE Metric}
Let us consider a priming pair that describes the same picture:

1. Prepositional Object (PO): The talented artist performs street art for the audience.

2. Double Object (DO): The talented artist performs the audience street art.

For our experiment, we select the PO sentence as the Positive Prime Sentence and the DO sentence as the Negative Prime Sentence for comparison. Only the Positive Prime Sentence is inputted along with a randomly selected Target Image from our dataset, then a predicted sentence is generated to describe the Target Image. The syntax trees illustrating their structural differences, are provided in Appendix~\ref{appendix:syntax_tree}.
\\[5pt]
\textbf{Notation and Definitions:}

- PP (Positive Prime): The syntax tree of the input sentence, which is the PO sentence

- NP (Negative Prime): The syntax tree of the corresponding sentence, which is the DO sentence here

- PS (Predicted Sentence): The syntax tree of the output sentence generated for the Target Image
\\[5pt]
\textbf{Algorithm Steps:}

The tree kernel between PP and PS:
\begin{equation}
D_p = K(T_{pp}, T_{ps})
\end{equation}

The tree kernel between NP and PS:
\begin{equation}
D_n = K(T_{np}, T_{ps})
\end{equation}
Here, $K(\cdot,\cdot)$ represents the tree kernel function that measures structural similarity between two syntax trees.

We employed a normalized exponential amplification method to map the relative difference between these kernel values to [-1, 1]. When the predicted sentence structure is more similar to the positive priming, the value approaches 1; when it is more similar to the negative priming, the value approaches -1. This relationship is expressed as:
\begin{equation}
PE= \frac{e^{\gamma(D_p-D_n)} - 1}{e^{\gamma(D_n-D_p)} + 1}, \quad 0.1 \leq \gamma \leq 10.0
\end{equation}
where $PE$ represents the Priming Effect and $\gamma$ is a scaling factor that controls the sensitivity of the transformation. Illustrated in Figure~\ref{fig:gamma}, We examined different $\gamma$ values that result in different sensitivities, and they eventually converge to 1 or -1.

\begin{figure}[htbp]
    \centering  
    \includegraphics[width=\linewidth]{figure/gamma.png}
    \caption{Relations of $\gamma$ value, PE value and the kernel difference.
    }
    \label{fig:gamma}
\end{figure}

%\footnote{Figure \ref{fig:gamma} in Appendix \ref{app:gamma_value}}.

\section{Models}

We evaluated two open-source MLLMs, LLaVA \citep{liu2023visualinstructiontuning} and BLIP-2 \citep{li2023blip2bootstrappinglanguageimagepretraining}, to examine their structural priming capabilities. Since they only support single-image input, the prime sentence and target image were inputted for a fair comparison. To enable more comprehensive analysis, we implemented our own architectures capable of processing a prime image, a prime sentence, and a target image simultaneously. 

\begin{figure}[t]
    \centering  
    \includegraphics[width=0.85\linewidth]{figure/structure4.png}
    \caption{Structure comparison of Model 1 using Dual Encoding and Model 2 using Fusion Encoding.}
    \label{fig:structure}
\end{figure}

For controlled experiments, we developed two MLLMs shown in Figure \ref{fig:structure}: a dual model integrating an MLP with a Llama decoder and a fusion model based on an OFA encoder.

\subsection{Model 1: Dual Encoding}
The dual encoding model utilizes BERT \citep{DBLP:journals/corr/abs-1810-04805} for text processing and CLIP \citep{Radford2021LearningTV} for image feature extraction. Multi-threading is employed for efficient data processing. The MLP module combines image and text embeddings, with a GELU for activation. They are then passed to a TinyLlama-1.1B decoder for natural language generation.

\subsection{Model 2: Fusion Encoding}
We trained the OFA encoder and used the same hyperparameters and dataset for fair comparison. Our model jointly encodes both images and fuses their representations with the prime sentence embedding. The encoder follows a transformer-based architecture, incorporating GELU activation for improved non-linearity. ResNet-101 is used for visual feature extraction, ensuring robust image representations. These fused embeddings are then passed to the OFA decoder to generate the description of the target image.


\section{Experiments}
We selected 1,006 annotated sentences across 16 syntactic types from PRISMATIC as the test set to avoid image duplication. Given a prime sentence and prime image, the program randomly selects a target image from the dataset, and the model generates a predicted sentence. Given the stochastic nature of target image selection, we employ our reference-free PE metric rather than comparing against predetermined ground truth. Each sentence serves as a priming probe, and the process is repeated for 10 iterations, with a new target image randomly drawn in each iteration to assess the priming effect\footnote{Temperature set to 0.7 for all groups}.

Both models were trained on COCO \citep{lin2014microsoft} using identical hyperparameters, including a batch size of 13, a tokenizer max length of 256, and same number of embedding dimensions\footnote{Computational budget in Appendix \ref{app:computing}}. 

The following experimental conditions were evaluated:
\begin{enumerate}
\item LLaVA-1.5: A pre-trained fusion-encoded model.
\item BLIP-2: A pre-trained dual model.
\item Model 1: Our transparent dual encoding architecture.
\item Model 2: Our transparent fusion encoding architecture.
\end{enumerate}

Each experimental group was paired with a control group where no priming sentence or priming image was provided as input.

\section{Results}
\subsection{Image-Sentence Alignment}
We leveraged CLIP \citep{Radford2021LearningTV} to compute the semantic alignment between the images and their corresponding generated descriptions by computing cosine similarity scores. Sentence quality remains stable across different priming conditions. We observed that complex priming sentences slightly increase the similarity scores, suggesting they encourage more detailed image descriptions.


\begin{table}[h]
    \centering
    \resizebox{1\linewidth}{!}{ 
    \begin{tabular}{lcccc}
        \toprule
        & LLaVA & BLIP-2 & Model 1 & Model 2 \\
        \midrule
        With Prime & 29.70 & 30.72 & 22.94 & 21.91 \\
        Without Prime & 32.82 & 30.08 & 22.37 & 22.76 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Average CLIP cosine similarity scores for image-text matching across different models. Values represent percentage of matching scores.}
    \label{tab:clip-scores}
\end{table}

As depicted in table \ref{tab:clip-scores}, the CLIP similarity scores remain relatively consistent between primed and non-primed conditions. However, we observe different patterns across architectures. While dual models (BLIP-2 and Model 1) maintain stable performance, fusion-encoded models (LLaVA and Model 2) show decreased similarity scores when prime information is added. This suggests that fusion models are more sensitive to priming input, sometimes leading to descriptions that deviate from the image content or exhibit hallucination effects. Since LLaVA and BLIP-2 have been adjusted and optimized for diverse data sets, their overall score is higher.

\subsection{Priming Effect Score}

We quantified performance using two metrics: PE scores\footnote{$\gamma$=3} and structural preservation rate (the proportion of generated sentences that maintain the prime sentence's syntactic structure). Intra-model comparisons reveal significant improvements in PE scores when priming sentences are provided as input compared to non-primed conditions. 

\begin{figure}[h]
    \centering  
    \includegraphics[width=\linewidth]{figure/blip_avg.png}
    \caption{Comparative analysis of BLIP-2's PE scores under primed and non-primed conditions.}
    \label{fig:blip_prime}
\end{figure}
As shown in Figure \ref{fig:blip_prime}, contrary to theoretical predictions suggesting dual-encoding architectures would have no priming effect due to their independent encoding, we discover that priming sentences can also enhance the structural preservation in dual encoded models. 

\begin{figure*}[t]
    \centering  
    \includegraphics[width=\linewidth]{figure/comparison.png}
    \caption{Comparison of the Correct (Positive) Rate of all models under different priming types.}
    \label{fig:all_models}
\end{figure*}

Figure \ref{fig:all_models} illustrates that while all architectures demonstrate similar priming characteristics when prime context is provided, Model 2 with fusion encoding performs more robustly in complex syntactic contexts. 

\subsection{Correlation with Context Similarity}
We use OpenCLIP \citep{Radford2021LearningTV} to calculate the visual similarity between Prime images and Target images and use Sentence Transformers \citep{reimers-2019-sentence-bert} to calculate the semantic similarity between Prime sentences and Target. We then performed a correlation analysis between the two arrays using the Pearson correlation coefficient:

\begin{equation}
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
\end{equation}

Our correlation analysis aligns with and extends previous findings in both computational and psycholinguistic research \citep{jumelet2024language, tooley2025putting}, revealing that fusion-encoded models show strong correlations between visual similarity and priming effects (r = 0.7018, p = 8.89e-150 for Model 2). Specifically, Table \ref{tab:correlation} demonstrates that fusion-encoded architectures (LLaVA and Model 2) have significant correlations between both semantic and visual similarities and priming effects. This pattern mirrors human cognitive processing, where similar visual contexts enhance structural priming. The particularly strong correlation between Model 2 and LLaVA suggests that the fusion encoder may better mimic the integrated nature of human multimodal processing.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcc}
        \toprule
        & LLaVA & BLIP-2 \\
        \midrule
        Sentence Correlation coefficient & 0.3212 & 0.0358 \\
        P-value & 1.58e-25 & 0.2595 \\
        \midrule
        & Model 1 & Model 2 \\
        \midrule
        Sentence Correlation coefficient & 0.0295 & 0.5745 \\
        P-value & 0.3500 & 2.99e-89 \\
        \hline
        Image Correlation coefficient & 0.0729 & 0.7018 \\
        P-value & 0.0207 & 8.89e-150 \\ 
        \bottomrule
    \end{tabular}
    }
    \caption{Correlation coefficient across all models.}
    \label{tab:correlation}
\end{table}

To make sure that the priming effect is not driven only by prime sentences that are inputted simultaneously, we also calculated the correlation between sentence semantic similarity and PE score. The result shows that the correlation between PE and prime sentence similarity is smaller than the correlation between PE and image similarity. Detailed plots are provided in Appendix~\ref{appendix:correlation}.

\section{Conclusion}

We developed the first multimodal structural priming dataset named PRISMATIC, together with a new metric that can evaluate models' priming effects without requiring standard answers. The PE Score serves as a new standard for assessing how well machine predictions preserve syntactic structures from previous contexts. With controlling experiments, we constructed and tested MLLMs with two different encoding methods. Contrary to traditional beliefs, we found no significant statistical difference in syntactic priming ability between dual and fusion encoders, suggesting that different encoding methods have similar capabilities in preserving syntax.

However, correlation analysis revealed an interesting pattern: only fusion-encoded models showed a strong positive correlation between syntactic priming effects and image similarity, while in dual-encoding models, syntactic priming effects were unrelated to image similarity. This indicates that fusion encoding more closely resembles human psycholinguistic cognitive processes. However, the mechanism that enhances syntactic priming capabilities in dual encoding still requires further investigation; its priming performance is similar, but the underlying mechanism warrants further investigation.

\section*{Future Works}
Several important directions remain for future research. Our findings revealed that dual encoding models achieve comparable priming effects despite the minimal correlation between PE scores and image-text similarities. Understanding the reason for this priming effect in dual encoder architectures remains an important direction for future research. 

Additionally, future work could explore which visual features most strongly influence syntactic choices in multimodal language models, potentially leading to more effective architectures for visual-linguistic integration. Finally, developing training strategies that better simulate human structural priming effects observed in psycholinguistic experiments will bridge the gap between model behavior and human cognitive patterns.

\section*{Limitations}
Although we proposed a new dataset, a new metric, built models for controlled experiments, and observed a significant Pearson correlation coefficient, our research still has limitations. 

Due to the reliance on manual verification, our current dataset size is limited and insufficient for training. Despite Flickr30k's size of over 30,000 images and 150,000 captions, the dataset construction remains challenging due to the complexity of certain syntactic structures. In particular, finding images that can appropriately elicit specific complex types, such as mediopassive syntax,  limits our potential sample size.

While our PE metric can effectively measure both positive and negative priming effects, it lacks the ability to assess situations where the model output completely deviates from the expected syntactic structure. Although we verify the image-text alignment via cosine similarity score, the PE metric itself cannot directly assess the semantic consistency between the generated description and the visual content.

During the experiment, manual observation found that hallucinations occur in fusion encoding models more frequently, but the current evaluation method cannot quantify the level of the hallucinations, nor can we prove whether the hallucinations are caused by prime information.

%Lastly, our analysis would benefit from systematic ablation studies examining how individual model components influence syntactic processing. A more detailed investigation of each element could reveal which components are crucial for structural preservation and why different encoding mechanisms contribute differently to priming effects.


\bibliography{acl_latex}

\appendix

%\section{Example Appendix}
%\label{sec:appendix}

\begin{appendix}

%\section{Dataset before Annotation}
%\begin{table}[ht]
%\centering
%\caption{Overall Statistics before Annotation}
%\begin{tabular}{lr}
%\hline
%Feature & Value \\
%\hline
%Total Sentences & 13,961 \\
%Total Words & 138,014 \\
%Total Errors & 1,458 \\
%Error Rate & 0.0106 \\
%\hline
%Total Tokens & 148,934 \\
%Token Types & 7,293 \\
%TTR & 0.0490 \\
%Avg Tokens per Sentence & 11.29 \\
%Token Range per Sentence & (4, 71) \\
%\hline
%Avg Perplexity & 116.77 \\
%Perplexity Range & (10.14, 297.96) \\
%\hline
%\end{tabular}
%\small
%Analysis of the Dataset before Annotation
%\label{app:overall-stats}
%\end{table}
\section{Computing Requirements}
\label{app:computing}
Model 1 was trained on a NVIDIA RTX 4060 GPU, and Model 2 was trained on a NVIDIA A100 GPU. The training process consisted of 9 epochs with the following computational requirements:

\begin{itemize}
   \item Model 1 : Average training time of 57 minutes per epoch
   \item Model 2 : Average training time of 50 minutes per epoch
\end{itemize}


\section{Perplexity Definition}
\label{app:Perplexity_cal}
Perplexity is calculated as:
\begin{equation}
\text{PPL}(W) = \sqrt[n]{\prod_{i=1}^{n} \frac{1}{P(w_i|w_1, \ldots, w_{i-1})}}
\end{equation}
\noindent where $W = (w_1, \ldots, w_n)$ is the sequence of words in a sentence, and $P(w_i|w_1, \ldots, w_{i-1})$ is the probability of word $w_i$ given the preceding words.



\section{Type-Token Ratio}
\label{app:ttr}
The type-token ratio (TTR) is calculated as:
\begin{equation}
\text{TTR} = \frac{\text{number of unique word types}}{\text{number of tokens}}
\end{equation}
\noindent where unique word types represent distinct words and tokens represent the total word count in a text.

\section{Calculation of Common Substructures}
\label{app:delta}
The function $\Delta(n_1,n_2)$ is calculated as:
\begin{equation}
   \Delta(n_1,n_2) = 
   \begin{cases}
       0 & \text{if } prod(n_1) \neq prod(n_2) \\[2ex]
       \lambda & \text{if } pre-terminal(n_1) \\[2ex]
       \lambda \prod\limits_{j=1}^{nc(n_1)} & (1 + \Delta(ch(n_1,j), \\
       & ch(n_2,j))) \text{ otherwise}
   \end{cases}
\end{equation}
where:
\begin{itemize}
   \item $prod(n)$ is the production rule at node $n$
   \item $pre-terminal(n)$ determines if node $n$ is pre-terminal
   \item $nc(n)$ is the number of children of node $n$
   \item $ch(n,j)$ is the $j$-th child of node $n$
   \item $\lambda$ is a decay factor $(0 < \lambda \leq 1)$
\end{itemize}

\clearpage
\section{Syntax Types}
\begin{table}[h]
\caption{Syntactic Structure Types and Examples}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{6cm}>{\raggedright\arraybackslash}p{7cm}}
\hline
Type & Structure & Example Sentence \\
\hline
Simple Active & \{subject\} \{verb\} \{obj\} & A boy carries a ball. \\
Simple Passive & \{subject\} \{auxiliary\_verb\} \{verb (past\_participle)\} \{by + agent\} & A ball is carried by a boy. \\
\hline
PO Passive & \{direct\_obj\} \{auxiliary\} \{past\_participle\} \{preposition\} by \{subject\} & The colors were painted on paper by a girl with the brush. \\
PO Active & \{subject\} \{active\_verb\} \{direct\_obj\} \{preposition\} \{prepositional\_object\} & A girl painted the colors on paper with the brush. \\
\hline
Embedded Passive & \{subject\} \{auxiliary\_verb\} \{verb (past participle)\} \{by + agent\} \{subordinate\_clause\} & The sidewalk that was washed by the women is green and purple. \\
Embedded Active & \{subject\} \{verb\} \{object\} \{subordinate\_clause\} & A woman washed the green and purple sidewalk. \\
\hline
Mediopassive & \{subject\} \{verb\} \{adverbial\_clause\} & The music plays loudly as the singer performs in front of the audience. \\
Mediopassive-like Active & \{subject\} \{verb\} \{adverbial\_modifier\} \{additional\_clause\} & The audience listens intently as the band plays their music. \\
\hline
Simple PO & \{subject\} \{verb\} \{direct\_object\} \{prep\} \{indirect\_object\} & A man tells stories to people. \\
Simple DO & \{subject\} \{verb\} \{indirect\_object\} \{direct\_object\} & A man tells people stories. \\
\hline
Complex PO & \{subject\_phrase\} \{verb\_phrase\} \{object\_phrase\}\{prep\_phrase\_text\} & A woman wearing black glasses share sweets with a toddler girl wearing a princess hat. \\
Complex DO & \{subject\_phrase\} \{verb\_phrase\} \{indirect\_object\_phrase\} \{direct\_object\_phrase\} & A woman wearing black glasses share a toddler girl wearing a princess hat sweets. \\
\hline
PO Clause & \{subject\} \{verb\}\{indirect\_object\_clause\} \{prep\} \{direct\_object\_clause\} & The teacher that carrys books give assignments to the student that studys in the library. \\
DO Clause & \{subject\} \{verb\} \{direct\_object\_clause\} \{indirect\_object\_clause\} & The teacher that carries books give the student that studys in the library assignments. \\
\hline
S-Genitive & \{possessor\} \{possessive 's\} \{possessed object\} & Reflections from the firefighters' uniforms. \\
Of-Genitive & \{possessed object\} \{of\} \{possessor\} & Reflections from the uniforms of the firefighters. \\
\hline
\end{tabular}
\label{tab:syntax-templates}
\end{table}




\clearpage
\section{Perplexity by Syntax Types}
\label{app:Perplexity_table}
\begin{table}[h]
\caption{Token Count and Perplexity Value before Annotation by Priming Types}
\begin{tabular}{lcccc}
\hline
Type & Avg Tokens & Token Range & Avg Perplexity & Perplexity Range \\
\hline
Total & 11.29 & $(4, 45)$ & 116.77 & $(10.14, 298.76)$ \\
Simple Active & 6.18 & $(4, 9)$ & 136.45 & $(24.62, 281.47)$ \\
Simple Passive & 10.00 & $(5, 11)$ & 140.22 & $(19.05, 287.67)$ \\
PO Passive & 10.82 & $(7, 30)$ & 149.37 & $(19.05, 297.97)$ \\
PO Active & 12.94 & $(4, 33)$ & 110.82 & $(10.14, 294.37)$ \\
Embedded Passive & 15.48 & $(7, 45)$ & 123.29 & $(13.60, 295.37)$ \\
Embedded Active & 12.11 & $(7, 42)$ & 81.54 & $(23.13, 282.88)$ \\
Mediopassive & 9.72 & $(4, 21)$ & 162.43 & $(15.29, 296.51)$ \\
Mediopassive-like Active & 8.63 & $(5, 36)$ & 155.18 & $(20.08, 298.76)$ \\
Simple PO & 8.07 & $(5, 12)$ & 128.40 & $(16.68, 294.88)$ \\
Simple DO & 7.28 & $(4, 12)$ & 149.45 & $(18.29, 296.12)$ \\
Complex PO & 18.09 & $(10, 39)$ & 117.54 & $(16.18, 291.22)$ \\
Complex DO & 18.55 & $(7, 32)$ & 137.15 & $(19.18, 297.06)$ \\
PO Clause & 14.40 & $(10, 31)$ & 112.96 & $(16.18, 297.93)$ \\
DO Clause & 15.24 & $(8, 40)$ & 139.20 & $(29.72, 288.52)$ \\
S-Genitive & 17.90 & $(6, 44)$ & 75.50 & $(11.76, 279.36)$ \\
Of-Genitive & 19.92 & $(5, 45)$ & 89.43 & $(12.15, 298.76)$ \\
\hline
\end{tabular}
\label{tab:sentence_analysis}
\end{table}

\clearpage
\section{Syntax Tree Representation}
\label{appendix:syntax_tree}
Syntax trees generated in Figure~\ref{fig:parse_appendix} show a comparison of the structure of Positive Prime and Negative Prime Sentences.
\begin{figure}[ht]
    \centering  
    \includegraphics[width=1.95\linewidth]{figure/tree.png}
    \caption{Syntax trees of a PO sentence and a DO sentence, illustrating structural differences.}
    \label{fig:parse_appendix}
\end{figure}

\clearpage

\section{Correlation Analysis}
\label{appendix:correlation}

Figure~\ref{fig:llava_correlation_appendix} illustrates the correlation between semantic similarity, image similarity, and syntactic priming.

\begin{figure}[htbp]
    \centering  
    \includegraphics[width=1.9\linewidth]{figure/correlations.png}
    \caption{Semantic similarity, image similarity, and syntactic priming correlation.}
    \label{fig:llava_correlation_appendix}
\end{figure}

\end{appendix}

\end{document}
