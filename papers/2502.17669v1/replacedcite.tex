\section{Related Works}
Since the emergence of Large Language Models (LLM) and MLLMs, most research has been focused on improving training efficiency, fine-tuning methods, few-shot learning, and thought chain capabilities. However, some studies have specifically investigated the language structure in computational models. 

\subsection{Multimodal Large Language Models}
Although large language models are becoming more powerful with the development of self-attention transformer ____, they are still only able to recognize text information. To overcome this limitation, researchers have been exploring ways to combine visual and textual information processing.

Early efforts in vision-language models demonstrated various approaches to multimodal integration. CLIP ____ projects image and text as vectors into a unified representation space, while OFA ____ employs a unified transformer for joint encoding. Other models like ViLT ____ and LXMERT ____ explore different architectural choices.

The emergence of MLLMs marks a significant advancement in this field. As summarized by ____, MLLMs typically consist of three key components: a pre-trained encoder, a connector, and an LLM. Unlike traditional vision-language models, MLLMs are distinguished by their integration of billion-parameter language models and their ability to leverage multimodal instruction adaptation techniques. 

\subsection{Language Structures}
Language structural analyses in MLLMs can be broadly categorized into two main approaches: ____ used contrastive evaluation with image-sentence pairs to test grammatical understanding, while ____ employed probing tasks to show that structural information can be preserved during multimodal pre-training, though this depends on model design choices.

To enhance structural understanding, ____ proposed Structure-CLIP, which incorporates explicit scene graph modeling to better preserve grammatical relationships. The importance of architectural choices in structural preservation was further confirmed by ____, who conducted comprehensive ablation studies showing the significant impact of image encoders on MLLM performance.

However, existing research primarily focused on evaluating responses against predefined answers. These approaches assess whether models can correctly describe images or verify factual statements, but do not examine how visual information influences their structural choices in generation tasks. Such evaluation methods fail to capture the dynamic nature of language production, where multiple syntactic structures can be equally valid. These limitations suggest the need for more nuanced evaluation methods that consider both contextual processing and preference selection in structural understanding.

\subsection{Structural Priming}
 In human language processing, structural priming effects are well-attested in both comprehension and production ____. Notably, experiments have shown that ungrammatical and semantically incongruent sentences (e.g. "the waitress brings the book to the monk") elicit similar priming effects as well-formed sentences ____. This suggests that structural persistence effects are robust, even in the absence of semantic and lexical cues, providing insights into both language processing and machine communication ____.

In the field of computational linguistics, several studies have explored structural priming in language models. ____ introduced an Adaptation Effect metric to quantify structural similarities and demonstrated that trained LSTM models capture abstract language features beyond the word level. ____ showed that RNNs can preserve structural priming effects in monolingual contexts. Advancing this line of research, ____ developed a new indicator to measure priming effects and created PRIME-LM, a corpus for various syntactic structures. ____ provided evidence that multilingual LLMs possess abstract syntactic representations that similarly affect text generation across languages. ____ revealed transformers outperform RNNs in cross-language priming. Most recently, ____ tested the factors that influence the priming effect in LLMs, which proves that context also has an important influence on the syntactic structure of LLMs. ____'s research on humans also found that when the priming sentence and the target sentence share similar content, the processing relationship between the two is stronger.

%