\section{Additional Results on MUSE}
\label{appendix: add_result_muse}

\begin{table}[htb]
% \vspace*{-6mm}
% \begin{table*}[htb]
\begin{center}
\caption{\footnotesize{
Performance comparison of NPO and NPO+SAM on MUSE before and after the relearning attack, evaluated under two unlearning settings: LLaMA2-7B on News and ICLM-7B on Books.
} 
}
\vspace*{2mm}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c|c|cc|cc}
\toprule[1pt]
\midrule
\multirow{4}{*}{\textbf{Method}} & {\textbf{UT}} & \multicolumn{4}{c}{\textbf{UE}} \\ 
\cmidrule{2-6}
& \multirow{3}{*}{\begin{tabular}{c}
     KnowMem  \\
    $\mathcal{D}_r$ ($\uparrow$)
\end{tabular}} & \multicolumn{2}{c|}{\textbf{W/o Relearning
 Attacks}} & \multicolumn{2}{c}{\textbf{W/ Relearning Attacks}} \\
\cline{3-6}

&

& \begin{tabular}{c}
     VerbMem   \\
     $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

& \begin{tabular}{c}
   KnowMem \\
      $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}  

& \begin{tabular}{c}
     VerbMem   \\
     $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

& \begin{tabular}{c}
   KnowMem \\
      $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

\\


\midrule
\rowcolor{Gray}
\multicolumn{6}{c}{\textbf{MUSE News}} \\\midrule

\textbf{Origin} & 54.31 & 58.29 & 62.93 & N/A & N/A  \\
\textbf{NPO} & 41.58 & 0.00  & 43.93 & 56.57 & 57.58 \\
\textbf{NPO+SAM} & 42.58 & 0.00  & 42.26 & 51.47 & 54.74 \\

\midrule
\rowcolor{Gray}
\multicolumn{6}{c}{\textbf{MUSE Books}} \\\midrule

\textbf{Origin} & 67.01 & 99.56 & 58.32 & N/A & N/A   \\
\textbf{NPO} & 34.71 & 0.00  & 0.00  & 67.52 & 45.33 \\
\textbf{NPO+SAM} & 35.48 & 0.00  & 0.00  & 58.38 & 38.33\\

\midrule
\bottomrule
\end{tabular}
}
\label{tab: relearn_muse}
% \vspace*{-4mm}
\end{center}
\end{table}

\textbf{Unlearning performance and robustness on MUSE.} \textbf{Table\,\ref{tab: relearn_muse}} demonstrates the unlearning robustness of NPO and NPO+SAM on MUSE datasets (News and Books). The unlearning performance, as measured by metrics such as KnowMem on $\mathcal{D}_r$ and VerbMem and KnowMem on $\mathcal{D}_f$ before the attack, remains almost identical. However, SAM substantially improves the robustness of the unlearned model against relearning attacks. This is reflected in the smaller discrepancies between no attack and after attack VerbMem and KnowMem on $\mathcal{D}_f$. For instance, on MUSE News, the VerbMem difference on $\mathcal{D}_f$ for NPO+SAM is significantly lower (51.47) compared to NPO (56.57). These findings underscore SAM’s effectiveness in enhancing the model’s resilience to relearning attacks.



% \begin{table}[htb]
% % \vspace*{-6mm}
% % \begin{table*}[htb]
% \begin{center}
% \caption{\small{
% Performance comparison of NPO and NPO w/ SAM on MUSE before and after the relearning attack, evaluated under two unlearning settings: LLaMA2-7B on News and ICLM-7B on Books.
% } 
% }
% \vspace*{2mm}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{c|c|cc|ccc}
% \toprule[1pt]
% \midrule
% \multicolumn{1}{c|}{\multirow{4}{*}{\textbf{Method}}} & 
% \multicolumn{1}{c|}{\multirow{1}{*}{\begin{tabular}{c}
% UT
% \end{tabular}}} & \multicolumn{4}{c|}{UE}\\
% & \cmidrule{3-7}\multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}{c}
%      KnowMem  \\
%     $\mathcal{D}_r$ ($\uparrow$)
% \end{tabular}}} & \multicolumn{2}{c|}{\textbf{Before attack}} & \multicolumn{3}{c}{\textbf{After attack}} \\
% \cmidrule{3-7}

% &

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & Avg  \\


% \midrule
% \rowcolor{Gray}
% \multicolumn{7}{c}{\textbf{MUSE News}} \\\midrule

% Origin & 54.31 & 58.29 & 62.93 & N/A                             & N/A                             & N/A   \\
% NPO & 41.58 & 0.00  & 43.93 & 56.57 (\textcolor{blue}{56.57}) & 57.58 (\textcolor{blue}{13.65}) & 35.11 \\
% NPO w/ SAM & 42.58 & 0.00  & 42.26 & 51.47 (\textcolor{blue}{51.47}) & 54.74 (\textcolor{blue}{12.48}) & 31.97 \\

% \midrule
% \rowcolor{Gray}
% \multicolumn{7}{c}{\textbf{MUSE Books}} \\\midrule

% Origin & 67.01 & 99.56 & 58.32 & N/A                             & N/A                             & N/A   \\
% NPO & 34.71 & 0.00  & 0.00  & 67.52 & 45.33 & 56.42 \\
% NPO w/ SAM & 35.48 & 0.00  & 0.00  & 58.38 & 38.33 & 48.36 \\

% \midrule
% \bottomrule
% \end{tabular}
% }
% \label{tab: relearn_muse}
% % \vspace*{-4mm}
% \end{center}
% \end{table}


\textbf{Ablation study on SAM's hyperparameter $\rho$.}
\textbf{Table\,\ref{tab: muse_rho_ablation}} presents the impact of $\rho$ on unlearning robustness. $\rho$ is a critical hyperparameter that controls the magnitude of weight perturbations in SAM, where larger values lead to stronger perturbation to the model's parameters. To understand its impact, we conduct an ablation study on $\rho$ using the MUSE Books dataset. The findings indicate that when $\rho$ is too small (\textit{e.g.}, $0.001$), the perturbations are minimal, resulting in limited improvement in mitigating relearning attacks. On the other hand, setting $\rho$ too large (\textit{e.g.}, $0.1$) introduces excessive perturbations, which disrupt the unlearning process and prevent the model from effectively forgetting. At an intermediate value of $\rho = 0.01$, the model achieves an optimal balance between effective unlearning and enhanced robustness. This balance is evident in the smaller changes observed in KnowMem and VerbMem on $\mathcal{D}_\mathrm{f}$ after the relearning attack.



\begin{table}[htb]
\begin{center}
\caption{\small{Performance comparison of NPO and NPO+SAM with different $\rho$ on MUSE Books before and after the relearning attack. The table format follows Table\,\ref{tab: 
 relearn_muse}.} 
}
\vspace*{2mm}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c|c|cc|cc}
\toprule[1pt]
\midrule
\multirow{4}{*}{\textbf{Method}} & {\textbf{UT}} & \multicolumn{4}{c}{\textbf{UE}} \\ 
\cmidrule{2-6}
& \multirow{3}{*}{\begin{tabular}{c}
     KnowMem  \\
    $\mathcal{D}_r$ ($\uparrow$)
\end{tabular}} & \multicolumn{2}{c|}{\textbf{W/o Relearning
 Attacks}} & \multicolumn{2}{c}{\textbf{W/ Relearning Attacks}} \\
\cline{3-6}

&

& \begin{tabular}{c}
     VerbMem   \\
     $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

& \begin{tabular}{c}
   KnowMem \\
      $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}  

& \begin{tabular}{c}
     VerbMem   \\
     $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

& \begin{tabular}{c}
   KnowMem \\
      $\mathcal{D}_f$ ($\downarrow$)
\end{tabular}

\\

\midrule
\rowcolor{Gray}
NPO & 34.71 & 0.00 & 0.00 & 67.52 & 45.33 \\
$\rho = 0.001$ & 37.41 & 0.00 & 0.00 & 70.9 & 42.5 \\
\rowcolor{Gray}
$\rho = 0.01$ & 35.48 & 0.00 & 0.00 & 58.38 & 38.33 \\
$\rho = 0.1$ & 23.91 & 0.00 & 0.00 & 52.96 & 40.52 \\
\midrule
\bottomrule
\end{tabular}
}
\label{tab: muse_rho_ablation}
% \vspace*{-4mm}
\end{center}
\end{table}