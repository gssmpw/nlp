\section{Detailed Experiment Setups}
\label{appendix: exp_setup}

For WMDP \citep{li2024wmdp}, we utilize Zephyr-7B-beta as the original model specified in the benchmark. The dataset includes a forget set composed of plain texts related to biosecurity knowledge and a retain set of unrelated general content from Wikitext \citep{merity2016pointer}. We perform 125 unlearning steps for both NPO and GradDiff, using grid searches over the learning rate in [$2.5 \times 10^{-6}$, $10^{-5}$] and $\lambda$ in [1, 2.5]. For NPO, we additionally tune $\beta$ in [0.01, 0.05]. For RMU, following \citep{li2024wmdp}, we conduct 150 unlearning steps with a grid search for $\lambda$ in the range [800, 1600]. Regarding smoothing methods, we run grid searches for $\rho$ within the range [$10^{-3}$, $10^{-1}$] under NPO + SAM/RS, and $\gamma$ in the range [1, 10] under NPO + CR/GP. In NPO + SWA, we apply model averaging starting at 100 steps and repeating every five steps thereafter. We set the number of perturbation samples for RS to 3. For RMU+SAM, we unlearn in layers 5 to 7 and apply perturbations to layers 1 to 7.


For MUSE \citep{shi2024muse}, we adopt LLaMA-2 7B, fine-tuned on BBC news articles, as the original model. For the Books dataset, we utilize ICLM 7B, fine-tuned on the Harry Potter books. Both original models are readily accessible from the benchmark. NPO is trained for 10 epochs with a learning rate of $10^{-5}$, and we set $\beta = 0.1$. Hyperparameter tuning involves a grid search for $\lambda$ before $\ell_\mathrm{r}$ in [0.25, 1.0], and $\rho$ in SAM within the range $[10^{-3}, 10^{-1}]$ across both datasets.