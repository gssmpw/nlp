\section{Related Work}
\label{sec: related_work}
%\paragraph{Large language model unlearning.} 
% \paragraph{{Machine unlearning and its applications to LLMs.}}
%\SL{[a few sentences to introduce MU and its broad applications from discriminative models to generative models, from vision to language tasks. Thene focusing on LLM unlearning.]}
% \noindent \textbf{Machine unlearning and its applications to LLMs.}
\paragraph{Machine unlearning and its applications to LLMs.}
Machine unlearning modifies models to remove the influence of undesirable data, originally developed to mitigate post-training privacy risks~\citep{cao2015towards,ginart2019making,ullah2021machine,fan2023salun}. While retraining from scratch guarantees exact unlearning, it is computationally prohibitive, leading to research on approximate unlearning methods that balance efficiency and effectiveness~\citep{kurmanji2024towards,fan2024challenging,chen2023boundary,zhang2024unlearncanvas,zhang2024defensive}.
A rapidly growing subfield is LLM unlearning \citep{jang2022knowledge,meng2022locating,yao2024large,eldan2023whos,jia2024soul,zhang2024negative,maini2024tofu,jia2024wagle,liu2024revisiting,fan2024simplicity,thaker2024guardrail}, which has been shown promise in mitigating the generation of harmful content~\citep{yao2024large,li2024wmdp,jia2024soul} and protecting sensitive, copyrighted, or private information~\citep{eldan2023whos,wu2023depn,jang2022knowledge}.
Existing LLM unlearning approaches include model-based optimization    \citep{maini2024tofu, yao2024large, jia2024wagle, fan2024simplicity, zhang2024negative, li2024wmdp, jia2024wagle,wu2023depn,fan2024simplicity} and input-based strategies (via prompting or in-context learning)  to facilitate unlearning without extensive parameter adjustments \citep{liu2024large, thaker2024guardrail, pawelczyk2023context}. Furthermore, recent benchmarking efforts provide valuable frameworks for evaluating the effectiveness of LLM unlearning approaches. These include TOFU \citep{maini2024tofu}, which focuses on fictitious unlearning using synthetic data, WMDP \citep{li2024wmdp}, which aims to mitigate sociotechnical harms in model generation, and MUSE \citep{shi2024muse}, which focuses on erasing copyrighted information from LLMs.

 




%\paragraph{Robustness challenges LLM unlearning faces.} 
\paragraph{{`Adversaries' in LLM unlearning.}}
Recent studies have also exposed critical robustness vulnerabilities in existing LLM unlearning approaches \citep{lynch2024eight, lucki2024adversarial, hu2024jogging, zhang2024does, shumailov2024ununlearning, barez2025open, patil2023can,deeb2024unlearning,lo2024large}. These vulnerabilities primarily fall into two categories: relearning attacks \citep{hu2024jogging, lynch2024eight,deeb2024unlearning,lo2024large}, where fine-tuning with even a small subset of forget samples can restore unlearned knowledge \citep{lynch2024eight}, and jailbreaking attacks \citep{lucki2024adversarial, lynch2024eight, patil2023can}, where adversarial prompts successfully recover forgotten information at inference time \citep{lucki2024adversarial}. 
%Additionally, \citept{zhang2024does} highlighted that even unrelated operations, such as model quantization, can inadvertently revive targeted information.
To enhance the robustness of LLM unlearning, 
%recent research has focused on strengthening the robustness of unlearning methods \citep{tamirisa2024tamper, sheshadri2024latent}. For instance, 
\citep{tamirisa2024tamper} utilized a model-agnostic meta-learning (MAML) framework \citep{nichol2018first} to counter tampering attacks, while \citep{sheshadri2024latent} employed adversarial training in the latent space of LLMs. Unlike existing work, we investigate unlearning robustness against relearning attacks through the lens of smoothness optimization, establishing a seamless connection to SAM, a direct yet underexplored optimization foundation for robust LLM unlearning.

% Recent advancements in LLM unlearning has made significant progress, but studies have revealed critical robustness vulnerabilities in these approaches \citep{lynch2024eight, lucki2024adversarial, hu2024jogging, zhang2024does, shumailov2024ununlearning, barez2025open, patil2023can}. These vulnerabilities are broadly categorized into relearning attacks \citep{hu2024jogging, lynch2024eight} and jailbreaking attac 
% \citep{lucki2024adversarial, lynch2024eight, patil2023can}. For instance, \citept{lynch2024eight} demonstrate that fine-tuning an unlearned model with a small subset of forget samples can easily restore the removed knowledge. Similarly, \citept{lucki2024adversarial} show that jailbreaking attack can effectively recover unlearned information with minimal effort. Additionally, \citept{zhang2024does} reveal that even unrelated operations, such as model quantization, can inadvertently revive targeted information. To address these challenges, recent research has focused on strengthening the robustness of unlearning methods \citep{tamirisa2024tamper, sheshadri2024latent}. For example, \citept{tamirisa2024tamper} leverage a first-order MAML framework \citep{nichol2018first} to counter relearning attacks, while \citept{sheshadri2024latent} employ adversarial training in the latent space of LLMs. In this work, we propose smoothing the loss landscape as a unified defense mechanism to combat both relearning and jailbreaking attacks.

\paragraph{{SAM and smoothness optimization.}} 
%Smoothness optimization enhances a model's smoothness while achieving the optimization objective. Since deep learning models are often inherently non-smooth (\textit{e.g.}, the gradient of the objective function is not necessarily Lipschitz continuous) \citep{gorbunov2024methods,qi2024extended,chen2023generalized}, various techniques have been developed to approximate and improve smoothness during optimization. 
Sharpness-aware minimization (SAM) is a representative smoothness optimization technique that minimizes both the loss value and its sharpness, effectively promoting a flatter loss landscape, originally introduced to improve model generalization \citep{foret2021sharpnessaware,andriushchenko2022towards,liu2022towards,du2022sharpness,zhang2023what,jiang2023adaptive}. SAM has also been applied in traditional adversarial training to defend against input-level adversarial attacks \citep{wei2023sharpness,zhang2024duality}.
Beyond SAM, other smoothness optimization approaches include gradient penalty (GP) and curvature regularization (CR), which impose penalties based on loss gradients or Hessian-gradient products to encourage smoothness \citep{dauphin2024neglected, zhao2024will}. Randomized smoothing (RS) improves smoothness by convolving a non-smooth objective function with a Gaussian distribution  \citep{duchi2012randomized, cohen2019certified, ji2024advancing}. Meanwhile, weight averaging (WA) enhances smoothness by averaging model weights across training iterations, leading to a smoother optimization trajectory \citep{izmailov2018averaging}. Further, activation smoothing has been used to mitigate the security risk of users fine-tuning for LLMs \citep{huang2024vaccine}. These smoothness optimization approaches will serve as a key foundation for enhancing the robustness of LLM unlearning in this work.

% Other common approaches, such as gradient penalization (GP) and curvature regularization (CR), penalize the loss using metrics derived from the loss gradient or the product of the Hessian and gradient \citep{dauphin2024neglected,zhao2024will}. Notably, SAM has been shown to implicitly implement GP via a first-order Taylor expansion \citep{zhao2024will,dauphin2024neglected}. Randomized smoothing (RS), on the other hand, smooths a non-smooth objective function by convolving it with a smooth Gaussian distribution, effectively transforming it into a smooth distribution function \citep{duchi2012randomized,cohen2019certified,ji2024advancing}. Finally, weight averaging (WA) improves smoothness by averaging model weights during training, which helps smooth the optimization trajectory \citep{izmailov2018averaging}.

% Despite the development of previous LLM unlearning methods, lots of recent work have reveals that current LLM unlearning methods are facing severe robustness challenges \citep{lynch2024eight, lucki2024adversarial, hu2024jogging, zhang2024does, shumailov2024ununlearning, barez2025open, patil2023can}. There are generally two typical category of vulnerabilities of current LLM unlearning methods, relearning attacks \citep{hu2024jogging, lynch2024eight}, such as \citept{lynch2024eight} have found that by simpling finetune the unlearned model even using a small number of forget samples can recover the unlearned knowledge. Another category of vulnerabilities is from jailbreaking attacks and extraction attacks\citep{lucki2024adversarial, lynch2024eight, patil2023can}, such as \citept{lucki2024adversarial} has found that by using they proposed jailbreaking attack can easily recover the unlearned knowledge. In addition to those mainstream attacks, \citept{zhang2024does} also find that even only conducting quatitization on the unlearned LLMs can recover the unlearned knowledge back. Recently, there are several works which aims to robustify the unlearning algorithms \citep{tamirisa2024tamper, sheshadri2024latent}, \citept{tamirisa2024tamper} which leverage the first-order MAML \citep{nichol2018first} to improve the robustness against relearning attacks. \citept{sheshadri2024latent} proposes intergrating adversarial training in the latent space of LLMs to defense the jailbreaking attacks on the LLMs unleanring. Both previous defense method suffered with high runinng time cost, compared with previous work, our work focus on improve the robustness against relearning attacks and jailbreaking attacks from smoothing loss landscape perspective, which avoids the heavy burden on the LLMs training. 


% Recently, LLM unlearning has been proposed and advanced to resolve undesired retention data during LLM development \citep{jang2022knowledge,meng2022locating,yao2024large,eldan2023whos, liu2024rethinking, jia2024soul, zhang2024negative, maini2024tofu, jia2024wagle, liu2024large, liu2024revisiting, fan2024simplicity, thaker2024guardrail}. Bunch of real life applications are considered from the LLM unlearning, such as avoiding toxic content generation \citep{yao2024large, li2024wmdp, jia2024soul,jia2024wagle}, protecting copyrighted and privated information from LLMs \citep{eldan2023whos, wu2023depn, jang2022knowledge}. The current LLM unlearning methods can be categratied into two dimensions, the first one often named by model optimization based methods \citep{maini2024tofu,yao2024large, jia2024soul, jia2024wagle, fan2024simplicity, zhang2024negative, li2024wmdp}. Another category is by using the in-context learning ability of LLM to fulfill unlearning \citep{liu2024large, thaker2024guardrail}. Besides the advancement of the LLM unlearning methods, bunch of LLM unlearning benchmark also proposed for help develop and understand LLM unlearning \citep{shi2024muse,maini2024tofu, li2024wmdp}, such as WMDP \citep{li2024wmdp} hazardous knowledge related to biosecurity, chemsecurity and cybersecurity unlearning for  and MUSE \citep{shi2024muse}
% for private or copyrighted information removal


% \begin{itemize}
%     \item Overview of related work on LLM unlearning.
%     \item Discussion of attacks and robustness in LLM unlearning, including relearning attacks, adversarial prompts, etc.
%     \item Introduction to Sharpness-Aware Minimization (SAM) and other smoothing methods.
% \end{itemize}