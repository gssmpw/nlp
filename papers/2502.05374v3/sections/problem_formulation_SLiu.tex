\section{LLM Unlearning and Relearning Attacks}
\label{sec: preliminary}

% In this section, we present the problem formulation of LLM unlearning and present its robustness challenge against relearning attacks. We then introduce a robust optimization perspective to capture the interplay between unlearning and relearning, in order to strengthen the unlearning robustness.



\paragraph{Preliminaries on unlearning and relearning attacks.}
% The problem of LLM unlearning arises as a post-pretraining strategy designed to \textit{mitigate} the influence of undesirable data (\textit{e.g.}, sensitive, biased, unsafe, or illegal information) and to suppress the corresponding model capabilities, thereby preventing LLMs from generating such harmful content. 
% To achieve effective and efficient LLM unlearning while maintaining a balance with the model's utility post-unlearning, the unlearning problem is often framed as a carefully designed optimization task to update the model parameters from their pretrained values 
To achieve efficient LLM unlearning while preserving model utility, the unlearning problem is formulated as an optimization task to update parameters from their pretrained values \citep{eldan2023whos,yao2024large,maini2024tofu,zhang2024negative,li2024wmdp}.
%
% The problem setup for LLM unlearning is detailed below, following the general formulation presented in \citep{liu2024rethinking}. 
To be specific, let $\mathcal{D}_{\mathrm{f}}$ and $\mathcal{D}_{\mathrm{r}}$ represent the `forget' and `retain' sets, respectively. Here the forget set $\mathcal{D}_{\mathrm{f}}$ defines the scope of unlearning, specifying the data samples whose influences are to be removed. Conversely, the retain set $\mathcal{D}_{\mathrm{r}}$ ensures the preservation of the model's utility post-unlearning. Built upon $\mathcal{D}_{\mathrm{f}}$ and $\mathcal{D}_{\mathrm{r}}$, a forget loss ($\ell_\mathrm{f}$) and a retain loss ($\ell_\mathrm{r}$) are defined to balance unlearning effectiveness and utility retention. The leads to the following regularized optimization problem \citep{liu2024rethinking}:
\vspace*{-2mm}
\begin{align}
\begin{array}{ll}
 \displaystyle \min_{\boldsymbol{\theta}}    &  \underbrace{ \ell_{\mathrm{f}}(\boldsymbol{\theta} | \mathcal{D}_\mathrm{f}) }_\text{Forget} + \lambda \underbrace{   \ell_\mathrm{r}(  \boldsymbol{\theta} |\mathcal{D}_{\mathrm{r}} )  }_\text{Retain}, 
 \vspace*{-2mm}
\end{array}
%\hspace*{-2mm}
\label{eq: prob_LLM_MU}
%\hspace*{-3mm}
%\nonumber
\end{align}
where $\btheta$ denotes the model parameters, 
$\ell( \boldsymbol{\theta} | \cdot )$ is the forget or retain loss associated with the model $\boldsymbol{\theta}$ under a forget or retain dataset, and $\lambda \geq 0$ is a regularization parameter to balance `forget'   and `retainâ€™. One popular approach for designing the forget loss is negative preference optimization (NPO) \citep{zhang2024negative}, which formulates $\ell_{\mathrm{f}}$ as a preference optimization objective \citep{rafailov2024direct} but exclusively treats the forget data as negative samples. The retain loss $\ell_{\mathrm{r}}$ can be set as the standard training loss, ensuring the model preserves its utility on the retain set.

%%% relearning attacks
Despite the growing demand for LLM unlearning, concerns also arise about its robustness against \textit{relearning attacks} \citep{hu2024jogging}. These attacks aim to recover unlearned knowledge by fine-tuning the unlearned model, even using a very small number of forget samples.
We present the relearning attack formulation below:
\vspace*{-2mm}
\begin{align}
\begin{array}{ll}
    \displaystyle   \min_{\boldsymbol{\delta}} 
  &   \ell_{\mathrm{relearn}} (\btheta_{\mathrm{u}} + \boldsymbol{\delta} |   \mathcal{D}_{\mathrm{f}}^\prime ),
  \vspace*{-2mm}
\end{array}
\label{eq: relearn_atk}
\end{align}
where $\btheta_{\mathrm{u}}$ represents the unlearned model obtained as a solution to \eqref{eq: prob_LLM_MU},  $\boldsymbol{\delta}$ denotes the optimization variable corresponding to the model update introduced during the relearning process,   the relearn set $\mathcal{D}_{\mathrm{f}}^\prime$ is given by a much smaller subset of $\mathcal{D}_{\mathrm{f}}$, and
the relearn objective, $ \ell_{\mathrm{relearn}} $, is defined to counteract the forget objective, \textit{e.g.}, the negative forget loss, or the standard finetuning loss on $\mathcal{D}_{\mathrm{f}}^\prime$. 
%similarly to the retain objective for enhanced utility on $\mathcal{D}_{\mathrm{f}}^\prime$.



% \begin{figure}[htb]
%     \centering
%     \begin{tabular}{c}
%         % \begin{minipage}{0.95\linewidth}
%         %     \centering
%           \hspace*{-2mm}  \includegraphics[width=0.9\linewidth]{figs/npo_example.pdf}  \vspace*{-1mm}\\
%            %
%            \hspace*{-2mm}  \footnotesize{(a) Unlearning effectiveness (UE) of NPO on WMDP Bio} \\
%            % \label{fig:first}
%       %  \end{minipage} \\
%         %\vspace{-4mm} \\ % Adds vertical space between minipages
%       %  \begin{minipage}{1.0\linewidth}
%            % \centering
%            \hspace*{-2mm}  \includegraphics[width=1\linewidth]{figs/exp_relearn.pdf} \vspace*{-1mm} \\
%             \hspace*{-2mm} \footnotesize{(b) Response of original/unlearned/relearned model}
%            % \label{fig:second}
%       %  \end{minipage} \\
%     \end{tabular}
%     \vspace*{-3mm}
%     \caption{
%     \small{
%     Unlearning example on the WMDP Bio dataset before and after relearning attacks: (a) {Unlearning effectiveness} is shown for the original LLM Zephyr-7B-beta (`Origin'), the NPO-unlearned model w/o relearning (`Unlearn'), and the relearned model from the unlearned one (`Relearn$\mathrm{N}$'), where $\mathrm{N}$ represents the number of forget data samples used for relearning. Note that `Unlearn' corresponds to `Relearn0'.
% (b) A sample response from the WMDP Bio forget set is provided for each model in (a), illustrating the impact of unlearning and relearning.
%     %\SL{[Talk to me. Unlearn -> NPO? Y-axis, smaller font. color. UE. Small]} 
%     }
%     }
%     \label{fig: NPO_example}
% \end{figure}


% \begin{figure}[htb]
% % \vspace*{-5mm}
% % \center
% % \hspace*{6mm}
% % \includegraphics[width=0.35\textwidth]{figs/adv_bar.pdf}\\
% % \vspace{-5mm}

% \begin{tabular}{cc}
% \hspace*{-9mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/npo_example_2.pdf} 
% &
% \hspace*{-6mm}
% % \vspace*{-3mm}
% \includegraphics[width=0.26\textwidth,height=!]{figs/exp_relearn_2.pdf}
% \vspace*{-1mm}
% \\
%  \small{(a) UE of NPO on WMDP Bio} &  \hspace*{-3mm}  
%  \small{(b) Response from model}\\
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{
% Unlearning example on the WMDP Bio dataset before and after relearning attacks: (a) UE (unlearning effectiveness) of the original LLM Zephyr-7B-beta (`Origin'), the NPO-unlearned model w/o relearning (`Unlearn'), and the relearned model from the unlearned one (`Relearn$\mathrm{N}$'), where $\mathrm{N}$ represents the number of forget data samples used for relearning. Note that `Unlearn' corresponds to `Relearn0'. (b) Response example of different models in (a) evaluated on WMDP.}
% }
% \label{fig: NPO_example}
% % \vspace*{-11mm}
% \end{figure}

%\begin{wrapfigure}{r}{0.5\textwidth} 
\begin{figure}[htb]
%
%\vspace*{-6mm}
\center
% \hspace*{6mm}
% \includegraphics[width=0.35\textwidth]{figs/adv_bar.pdf}\\
% \vspace{-5mm}

\begin{tabular}{cc}
\hspace*{4mm}
\includegraphics[width=0.32\textwidth,height=!]{figs/npo_example_2.pdf} 
&
% \hspace*{8mm}
% \vspace*{-3mm}
\includegraphics[width=0.37\textwidth,height=!]{figs/exp_relearn_2.pdf}
\vspace*{-1mm}
\\
\hspace*{9mm}
 \small{(a) UE of NPO on WMDP Bio} &  \hspace*{-3mm}  
 \small{(b) Response from model}\\
\end{tabular}
\vspace{-1mm}
% \caption{\small{
% Unlearning example on the WMDP Bio dataset before and after relearning attacks: (a) UE (unlearning effectiveness) of the original LLM Zephyr-7B-beta (`Origin'), the NPO-unlearned model w/o relearning (`Unlearn'), and the relearned model from the unlearned one (`Relearn$\mathrm{N}$'), where $\mathrm{N}$ represents the number of forget data samples used for relearning. Note that `Unlearn' corresponds to `Relearn0'. (b) Response example of different models in (a) evaluated on WMDP.}
% }
\caption{\small{
Unlearning example on the WMDP Bio dataset before and after relearning attacks: (a) UE (unlearning effectiveness) of Zephyr-7B-beta (`Origin'), the NPO-unlearned model w/o relearning (`Unlearn'), and the relearned model from the unlearned one (`Relearn$\mathrm{N}$'), where $\mathrm{N}$ represents the number of forget data samples used for relearning. (b) Response example of different models in (a) evaluated on WMDP.}
}
\label{fig: NPO_example}
\vspace*{-3mm}
%\end{wrapfigure}
\end{figure}

\paragraph{A motivating example.}
\textbf{Fig.\,\ref{fig: NPO_example}} presents the performance of the NPO-based unlearning approach to solve \eqref{eq: prob_LLM_MU} in mitigating the malicious use of the LLM Zephyr-7B-beta on the {WMDP (Weapons of Mass Destruction Proxy) Bio} dataset \citep{li2024wmdp}. In this context, a lower accuracy of the model on the WMDP (Bio) evaluation set corresponds to better unlearning. Thus, we define \textit{unlearning effectiveness} (\textbf{UE}) as \textit{1-Accuracy on WMDP evaluation set}, where a higher value indicates better unlearning performance.

As shown in {Fig.\,\ref{fig: NPO_example}-(a)}, the NPO-unlearned model  (termed `Unlearn') achieves a much higher UE compared to the original model prior to unlearning (referred to as `Origin'). And it effectively mitigates hazardous knowledge, as evidenced by the generation example in {Fig.\,\ref{fig: NPO_example}-(b)}.
%
However, when a relearning attack is introduced by fine-tuning the unlearned model for a single epoch using only a few forget samples--specifically, 20, 40, or 60 samples (referred to as `Relearn20', `Relearn40', and `Relearn60', respectively)--the unlearned model can be reverted,  resuming the generation of harmful responses similar to `Origin'. %The susceptibility to such a simple relearning attack is further evidenced by the generation response under Relearn60 in Fig.\,\ref{fig: NPO_example}-(b).







% we also compared the unlearning effectiveness of an NPO-unlearned model with the performance of the model after relearning using \CF{20, 40, 60} \SL{\textit{10, 20...??}} forget samples, respectively. As we can see, after just \textit{one} fine-tuning epoch, the unlearned model can be reverted, resuming the generation of harmful responses as the pre-unlearned model.



The above example underscores the need to re-examine current LLM unlearning approaches, as formulated in \eqref{eq: prob_LLM_MU}, and inspires us to identify and leverage overlooked unlearning optimization principles to strengthen its robustness.

\paragraph{Sharpness-aware minimization (SAM): A robust optimization perspective on unlearning against relearning.}
Building on \eqref{eq: prob_LLM_MU} and \eqref{eq: relearn_atk}, enhancing unlearning resistance to relearning attacks can be framed as an adversary-defense game. This framework, similar to adversarial training \citep{madry2018towards}, can be expressed using min-max optimization, where the objective is to jointly optimize the unlearning process to counteract the adversarial relearning attempts effectively.
However, unlike adversarial training, which defends against input-level adversarial examples, relearning attacks directly modify the weights of the unlearned model to counteract the forget objective. 
If the relearning objective $\ell_{\mathrm{relearn}}$ is defined to counteract the forget objective, such that $\ell_{\mathrm{relearn}} = -\ell_{\mathrm{f}}$,  then integrating the relearning adversary  \eqref{eq: relearn_atk} into LLM unlearning \eqref{eq: prob_LLM_MU} leads to the following min-max robust optimization problem:
% \vspace*{-2mm}
\begin{align}
% \hspace*{-8mm}
\begin{array}{l}
 \displaystyle \min_{\boldsymbol{\theta}}     \underbrace{ 
 \max_{\| \boldsymbol{\delta} \|_p \leq \rho } 
 \ell_{\mathrm{f}}(\boldsymbol{\theta} + \boldsymbol{\delta} | \mathcal{D}_\mathrm{f})
 }_\text{$\Def \ell^{\mathrm{SAM}}_{\mathrm{f}} (\btheta)$} 
 + \lambda     \ell_\mathrm{r}(  \boldsymbol{\theta} |\mathcal{D}_{\mathrm{r}} )   , 
 \vspace*{-2mm}
\end{array}
%\hspace*{-2mm}
\label{eq: prob_LLM_MU_SAM}
%\hspace*{-3mm}
%\nonumber
\end{align}
where $\| \cdot \|_p$ denotes the $\ell_p$ norm ($p \geq 1$), with $p = 2$ as the default setting. And similar to adversarial training \citep{madry2018towards}, we limit the ability of the adversary (\textit{i.e.}, `follower') to disrupt the unlearned model (\textit{i.e.}, ``leader''), given by the constraint  $\| \boldsymbol{\delta} \|_p \leq \rho$ with a small $\rho > 0$.


Interestingly, the formulation in \eqref{eq: prob_LLM_MU_SAM} aligns closely with the principles of SAM \citep{foret2021sharpnessaware}, with the SAM loss $\ell^{\mathrm{SAM}}_{\mathrm{f}} (\btheta)$ applied to forget objective.
%
%
%
Conventionally, SAM aims to enhance model generalization by explicitly considering the sensitivity of the loss landscape to weight perturbations, thereby encouraging \textit{smoothness} optimization. Yet,  SAM also resonates with the robust optimization for LLM unlearning in \eqref{eq: prob_LLM_MU_SAM}.
Inspired by the synergy between SAM and robust unlearning, we aim to explore in the rest of the work:
\textit{How does SAM enhance the resilience of LLM unlearning to relearning attacks?
And what are the broader implications of smoothness optimization techniques, beyond SAM, on the robustness of LLM unlearning?}
% {\color{red}[has any past works discussing similar approach that uses SAM for adversarial learning? e.g., https://arxiv.org/abs/2305.05392? Maybe we need to citep.]}




 


