\section{Introduction}
\label{sec: intro}



With the rapid advancement of large language models (LLMs), concerns about their privacy, safety, and trustworthiness, have become increasingly prominent \citep{liu2024towards,barez2025open}. However, retraining these models to eliminate the undesired data-model influence is often infeasible due to the significant computational and time costs involved. To address this challenge, LLM unlearning \citep{yao2024large,eldan2023whos,maini2024tofu,liu2024rethinking} has emerged as a post-pretraining strategy, which aims to \textit{mitigate} the impact of undesirable data (\textit{e.g.}, sensitive, biased, unsafe, or illegal information) and suppress associated model capabilities, thereby preventing LLMs from generating harmful content while simultaneously preserving the model's utility post-unlearning.




Despite the increasing importance of LLM unlearning, several recent studies \citep{lucki2024adversarial,zhang2024does,lynch2024eight,hu2024jogging,deeb2024unlearning} 
%
have identified a critical issue: \textit{LLM unlearning often lacks robustness}. Specifically, the susceptibility to quickly recovering `already-unlearned' knowledge post-unlearning is evident through so-called \textit{relearning attacks} \citep{lynch2024eight,hu2024jogging}. These attacks can effectively reverse the unlearning process by leveraging lightweight fine-tuning on the unlearned model using only a small number of data from the forget dataset.
%We refer readers to Sec.\,\ref{sec: preliminary}  for a detailed discussion on  the vulnerability of current LLM unlearning methods.


%\SL{[I stop here.]}

Although numerous LLM unlearning methods have been proposed in the literature \citep{yao2024large,maini2024tofu,ji2024reversing,zhang2024negative,liu2024large,ji2024reversing,li2024wmdp,jia2024wagle,jia2024soul}, few studies have explored the \textit{robust optimization} foundation for LLM unlearning. For example, negative preference optimization (NPO) \citep{zhang2024negative}, one of the state-of-the-art (SOTA) LLM unlearning methods, has demonstrated superior unlearning effectiveness compared to other approaches  \citep{shi2024muse}. However, as we will motivate in Sec.\,\ref{sec: preliminary}, NPO still remains vulnerable to relearning attacks. This highlights the need to develop a robust optimization foundation to strengthen LLM unlearning against such attacks.
Tracing back to defenses against classic (input-level) prediction-evasion adversarial attacks, \textit{adversarial training} \citep{madry2018towards}, built upon min-max optimization, has proven to be a generic and effective robust optimization framework. In a similar vein, we ask:
\begin{tcolorbox}[before skip=2mm, after skip=0.0cm, boxsep=0.0cm, middle=0.0cm, top=0.05cm, bottom=0.05cm, boxrule=0.6pt]
\begin{center}
     \textit{\textbf{(Q)} What is the robust optimization foundation for LLM unlearning against relearning attacks?}
\end{center}
\end{tcolorbox} 
\vspace*{2mm}

%\SL{[I stop here]}
Drawing inspiration from adversarial training \citep{madry2018towards}, we address \textit{(Q)} through the lens of min-max optimization.  Here the minimization step focuses on LLM unlearning, coupled with a maximization step that simulates relearning attacks. The maximization step identifies the worst-case \textit{weight perturbations}  (rather than input perturbations in adversarial training) to the unlearned model, aiming to reverse the unlearning effects.
We demonstrate that the robust optimization framework for LLM unlearning naturally aligns with sharpness-aware minimization (\textbf{SAM}) \citep{foret2021sharpnessaware}. SAM was originally developed to enhance model generalization by encouraging a uniformly low loss across the neighborhood of a given model, thereby promoting a \textit{smooth} loss landscape.
We will show that \textit{smoothness optimization}, such as SAM, is a critical yet underexplored factor for enhancing unlearning robustness against relearning attacks. 
%This work offers an in-depth exploration into robust optimization for LLM unlearning and its connection to SAM and other smooth optimization techniques beyond SAM.
We summarize our \textbf{contributions} below. 

% take inspiration from Sharpness-Aware Minimization (SAM) to enhance the robustness of LLM unlearning \citep{foret2021sharpnessaware}. SAM is a min-max optimization method that introduces weight-space perturbations during training. We provide theoretical insights demonstrating the connection between SAM and curvature regularization \citep{dauphin2024neglected}, and show that the smoothness introduced by SAM is a critical factor in improving robustness. Furthermore, we explore the benefits of combining smooth optimization techniques with LLM unlearning, showcasing their potential to strengthen unlearning against attacks.

% \paragraph{Contributions.} We summarize our contributions below.

$\bullet$ To our best knowledge, this is the first work to reveal that SAM naturally yields a robust optimization framework for LLM unlearning in defending against relearning attacks.

$\bullet$ We conduct an in-depth exploration of SAM-integrated LLM unlearning for enhanced robustness and establish its connection to curvature regularization and broader smoothness optimization techniques beyond SAM.

$\bullet$ We conduct extensive experiments to demonstrate the critical role of smoothness optimization, particularly SAM, in improving LLM unlearning robustness against various relearning attacks and jailbreaking attacks (that evades unlearned LLMs using input-level adversarial prompts).


 



% \begin{itemize}
%     \item (\textbf{Methodology-wise}): We develop a robust unlearning method against relearning attacks by deeply understanding the SAM formulation. The key innovation is introducing weight-level perturbations during the unlearning process.

%     \item (\textbf{Formulation-wise}) We establish that SAM enhances model smoothness by linking it to curvature regularization. Within this theoretical framework, we further show the relationship between SAM and other smooth optimization techniques, highlighting smoothness as a crucial factor in improving the robustness of LLM unlearning.

%     \item (\textbf{Experiment-wise}): We conduct extensive experiments across different datasets and models to demonstrate the robustness improvements brought by SAM and other smooth optimization techniques to LLM unlearning. Beyond relearning attacks, we show that smooth optimization also provides robustness against input-level attacks, such as adversarial prompts.
% \end{itemize}