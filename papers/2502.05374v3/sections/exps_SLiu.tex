%\newpage


\section{Experiments}
\label{sec: exp}
\subsection{Experiment setups}

% \noindent \textbf{Datasets and models.} 
\paragraph{Datasets and models.}To showcase the robustness improvements brought by SAM and other smoothing techniques, we perform experiments on two representative benchmarks:
 {(1) WMDP} \citep{li2024wmdp}, as used in Fig.\,\ref{fig: NPO_example}, which evaluates the unlearning capability in hazardous domains, such as biosecurity, cybersecurity, and chemical security. Our experiments primarily focus on the biosecurity aspect of WMDP; 
 % Our experiments primarily focus on the biosecurity aspect of WMDP; 
 {(2) MUSE} \citep{shi2024muse}, which features two distinct unlearning scenarios: forgetting text segments from the Harry Potter book series (labeled `Books') and forgetting news articles from BBC News (labeled `News'). Following the literature, we use Zephyr-7B-beta as the original model for WMDP, LLaMA-2 7B fine-tuned on BBC news for News, and ICLM 7B fine-tuned on Harry Potter books for Books. These models, prior to unlearning, are referred to as `Origin', consistent with the terminology in Fig.\,\ref{fig: NPO_example}.


 
%\SL{We use xxx as the original pre-unlearning model (referred to as `Origin' in Fig.\,\ref{fig: NPO_example}).} 
%For the MUSE benchmark, the original model was fine-tuned on the Books and News datasets for knowledge memorization.
% \SL{[missing model information.]}
  

% \noindent \textbf{LLM unlearning methods and evaluation.} 
\paragraph{LLM unlearning methods and evaluation.} For the WMDP benchmark, we use NPO \citep{zhang2024negative} with retain regularization as the primary unlearning baseline, as formulated by  \eqref{eq: prob_LLM_MU}. Additionally, we include representation misdirection for unlearning (RMU) \citep{li2024wmdp} and gradient difference (GradDiff) \citep{maini2024tofu,liu2022continual} as supplementary baselines. For   MUSE, we adopt NPO as the baseline due to its state-of-the-art (SOTA) performance on this benchmark \citep{shi2024muse}. More implementation details are provided in \textbf{Appendix\,\ref{appendix: exp_setup}}.

Following the used benchmarks, the performance of LLM unlearning is evaluated by \textbf{UE} (unlearning effectiveness) and post-unlearning utility retention (\textbf{UT}). For WMDP, UE is measured as 1-Accuracy on the WMDP Bio evaluation set, consistent with Fig.\,\ref{fig: NPO_example}. UT is assessed using zero-shot accuracy on the MMLU dataset \citep{hendrycks2020measuring}.
For MUSE, UE is evaluated based on knowledge memorization ({KnowMem}) and  verbatim memorization ({VerbMem}) on the forget set,  where lower values indicate better unlearning performance. {UT is calculated using KnowMem on the retain set.}
%\SL{[UT in MUSE is missing?]}
In addition to UE and UT, we assess the \textbf{robustness} of LLM unlearning in two adversarial settings: \textbf{relearning attacks} \citep{hu2024jogging}, which is our primary focus; And \textbf{jailbreaking attacks} \citep{lucki2024adversarial, thompson2024flrt}.
%, given by input-level adversarial prompts to extract unlearned information from an unlearned model without modifying its parameters.
To implement relearning attacks, we sample relearning data from either the forget set (the default setting) or a forget-unrelated dataset, such as AGNews \citep{zhang2015character}, GSM8K \citep{cobbe2021gsm8k}, and SST2 \citep{socher2013recursive}. The relearning data are randomly selected from any of the relearning sets, and the attack performance is averaged over 5 independent random trials.
For jailbreaking attacks, we use the enhanced-GCG algorithm \citep{lucki2024adversarial,zou2023universal,thompson2024flrt} to generate adversarial prefixes.

\noindent \textbf{Smoothness optimization implementation.} 
We integrate SAM, RS, CR, GP, and WA with LLM unlearning. For SAM, we set the perturbation parameter $\rho = 0.01$ in \eqref{eq: prob_LLM_MU_SAM}. \textbf{Table\,\ref{tab: muse_rho_ablation}} in \textbf{Appendix\,\ref{appendix: add_result_muse}} presents a sensitivity study on $\rho$. We find that when $\rho$ is too small (\textit{e.g.}, 0.001), SAM provides limited improvement against relearning attacks. Conversely, when $\rho$ is too large (\textit{e.g.}, 0.1), the perturbations hinder unlearning effectiveness. Additional smoothness optimization details can be found in Appendix\,\ref{appendix: exp_setup}.


% \ding{182} \textbf{Unlearning Effectiveness} measures the extent to which undesired data influences or model capabilities are removed. For WMDP, we use \textbf{1 - WMDP Bio Accuracy} as the metric, where a higher value indicates better unlearning. For MUSE, we evaluate using knowledge memorization (\textbf{KnowMem}) and  verbatim memorization (\textbf{VerbMem}) on the forget set $\mathcal{D}_f$; lower values are better. 
% \ding{183} \textbf{Utility Retention} assesses the preserved capabilities of the unlearned model. This is evaluated by the zero-shot accuracy on the \textbf{MMLU} dataset \citep{hendrycks2020measuring} within the WMDP benchmark. 

%\ding{184} \textbf{Robustness} evaluates the resilience of the unlearned model against relearning attacks \citep{hu2024jogging} and jailbreaking attacks\citep{lucki2024adversarial, thompson2024flrt}. The attack settings and detailed evaluation protocols are described in the following paragraph.

% \textbf{Attacks to LLM unlearning.}
% For the \textbf{relearning attacks}, we sample a small number of examples from a given dataset. Specifically, we consider two attack settings: \textit{(S1) the fixed number of samples which is randomly sampled from the given set, and conduct relearning on these samples with different numbers of relearning epochs}, and \textit{(S2) different numbers of relearning samples randomly sampled from given dataset and relearning on these samples for one epoch}. For WMDP, we use different relearning sets by sampling from the WMDP Bio forget set, AGNews, GSM8K, and SST2. In the case of (S1), we sample 20 examples and perform relearning for 1, 2, and 3 epochs, respectively. For (S2), we sample 20, 40, and 60 examples and perform relearning for 1 epoch, respectively. For MUSE, We sample 100, 125, and 150 examples from the forget set as the relearning set and conduct relearning for one epoch. \textbf{For the adversarial prompt}, we follow the setup in \citep{lucki2024adversarial,thompson2024flrt}, using Enhanced-GCG to generate adversarial prefixes, which are then combined with the WMDP Bio evaluation set for evaluation.


\subsection{Experiment results}

% \textbf{Robustness against relearning attacks with different epochs.} Following \textit{(S1)}, we fixed the number of relearning samples at 20 and progressively increased the number of relearning epochs. The results are shown in \textbf{Fig.\,\ref{fig: relearn_wmdp_npo}-(a)}. It showcases that SAM significantly reduces the rate of decline in 1 - WMDP Bio Acc compared with NPO, which represents that SAM can significant slow down the speed of relearning knowledge back on WMDP datasets compared with NPO. Specifically, unlearning effectiveness of NPO+SAM at epoch 3 have 0.2 improvement compared with NPO. In contrast, for NPO, relearning with just 20 samples for a single epoch already causes a substantial drop in 1 - WMDP Bio Acc. After 3 epochs of relearning, the model's performance nearly converges to that of the original model, effectively invalidating the unlearning process. Besides, we can also observe that other smoothing techiques also can improve the robustness in this relearning attacks settings but not competitive as SAM, which is reasonable, because from the \eqref{eq: prob_LLM_MU_SAM}, SAM is directly solving the robustness against relearning attacks problem. 


%\textbf{Robustness against relearning attacks with different epochs.}

% % \begin{wrapfigure}{r}{0.5\textwidth}
% \begin{figure}[htb]
% % \vspace*{-4mm}
% \center
% \hspace*{6mm}
% \includegraphics[width=0.4\textwidth]{figs/legend_method_1.pdf}\\
% \vspace{-1mm}
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.24\textwidth,height=!]{figs/method_epoch_wmdp_var_1.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.24\textwidth,height=!]{figs/method_step_wmdp_var_1.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a) UE vs. relearning epoch \#} &  \hspace*{-3mm}  
%  \small{(b) UE vs. relearning data \#}\\
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{Unlearning robustness comparison for different methods (NPO, GradDiff, and RMU) with and without SAM on WMDP under various relearning attacks settings. The UE of the original model (`Origin') is also included for comparison.
% (a) UE vs. the number of relearning epochs using 20 forget samples.
% (b) UE vs. the number of forget data points with 1 relearning epoch.}
% }
% \label{fig: relearn_wmdp_method}
% \vspace*{-2mm}
% \end{figure}
% % \end{wrapfigure}

\paragraph{Evaluation on SAM-integrated unlearning methods beyond NPO.} In \textbf{Fig.\,\ref{fig: relearn_wmdp_method}},
%\textbf{Fig.\,\ref{fig: relearn_wmdp_method}}, 
we show the applicability and effectiveness of SAM when integrated with multiple unlearning methods, including NPO, GradDiff \citep{maini2024tofu}, and RMU \citep{li2024wmdp}. 
% The unlearning and relearning settings are consistent with those in Fig.\,\ref{fig: relearn_wmdp_npo}; See \textbf{Fig.\,\ref{fig: relearn_wmdp_method}} in \textbf{Appendix\,\ref{appendix: add_result_wmdp}} for Fig.\,\ref{fig: relearn_wmdp_npo}-like presentation.
%The Fig.,\ref{fig: relearn_wmdp_npo}-like plotting is provided in \textbf{Fig.\,\ref{fig: relearn_wmdp_method}}.}
%
As we can see, all SAM-based variants enhance the robustness of their non-SAM counterparts against relearning attacks. Notably, this improvement does not compromise UT or UE in the absence of relearning attacks. The detailed UE and UT are provided in \textbf{Table\,\ref{tab: relearn_wmdp_method}} of \textbf{Appendix\,\ref{appendix: add_result_wmdp}}. RMU-type methods achieve better UT (0.57) compared to NPO or GradDiff-type methods (around 0.45). However, they exhibit weaker robustness against relearning attacks compared to NPO+SAM. This discrepancy arises because RMU achieves unlearning by updating only a subset of the model parameters (layers 5, 6, and 7) to balance unlearning with utility preservation. By contrast, relearning attacks can target the entire model, leading to a mismatch in parameter updates that may compromise RMU's robustness. In \textbf{Fig.\,\ref{fig: loss_lanscape_rmu}} of Appendix\,\ref{appendix: add_result_wmdp} , we further analyze the relationship between the number of parameters involved in smoothness optimization and unlearning robustness by examining RMU.

\begin{figure}[htbp]
  % \vspace{4mm}
  \centering
  \includegraphics[width=0.5\textwidth]{figs/legend_method_1.pdf}\\
  \vspace{-1mm}
  \begin{tabular}{cc}
    \hspace*{-3mm}
    \includegraphics[width=0.28\textwidth]{figs/method_epoch_wmdp_var_1.pdf} 
    &
    % \hspace*{-6mm}
    \includegraphics[width=0.28\textwidth]{figs/method_step_wmdp_var_1.pdf}
    \vspace*{-1mm}
    \\
    \hspace*{3mm} \small{(a) UE vs. relearning epoch \#} 
    &  
    \hspace*{6mm}
    \small{(b) UE vs. relearning data \#}
  \end{tabular}
  \vspace{-2mm}
  \caption{\small{Unlearning robustness comparison for different methods (NPO, GradDiff, and RMU) with and without SAM on WMDP under various relearning attacks settings. The UE of the original model (`Origin') is also included for comparison.
(a) UE vs. the number of relearning epochs using 20 forget samples.
(b) UE vs. the number of forget data points with 1 relearning epoch.}}
  \label{fig: relearn_wmdp_method}
  %\vspace{-2mm}
%\end{wrapfigure}
\end{figure}



\paragraph{Unlearning robustness vs. relearning attacks with different relearning epoch counts and data amounts.}
In \textbf{Table.\,\ref{tab: relearn_wmdp_npo}}, we showcase the UE of NPO and its smoothness optimization-based variants (integrated with SAM, RS, GP, CR, and WA) on WMDP, against the varying number of epochs ($M$) and the forget data amount ($N$) used in relearning attacks. 
% Here relearning is conducted using a small number of forget data points randomly sampled from the WMDP Bio forget set.
As we can see, UE decreases as either $M$ or $N$ increases. However, compared to the vanilla NPO approach, which nearly reverts to pre-unlearning performance (\textit{i.e.}, `Origin' in Fig.\,\ref{fig: relearn_wmdp_method}) under relearning attacks with $M \geq 2$ and $N \geq 40$, all proposed smooth variants of NPO exhibit much better robustness. Among these, NPO+SAM consistently outperforms the others, demonstrating the strongest resilience against relearning attacks.
%\SL{Additionally, NPO + WA and NPO + GP appear to deliver the second-best performance.}
Additionally, compared to increasing the number of relearning epochs, using a larger number of forget data samples for relearning leads to a more rapid decline in unlearning effectiveness. 


%\begin{wraptable}{r}{0.5\textwidth}
%
\begin{table}[htb]
%\vspace{-4mm}
\caption{\small{Unlearning robustness comparison of NPO and its smoothness optimization-based variants on WMDP under different relearning attacks settings. UT is evaluated using MMLU accuracy, while UE is measured as $1 - \text{WMDP accuracy}$ on  evaluation set. An upward arrow (\textuparrow) indicates that higher values represent better performance. $N$ represents the number of forget samples used for relearning with 1 epoch, and $M$ denotes the number of relearning epochs using 20 forget  samples. The best robustness in each relearning setting is highlighted in \colorbox{mr}{\parbox[c][0.2cm][c]{0.4cm}{\centering{red}}}.
%The results are averaged over 5 independent trials. 
%\SL{[redundant]}
}
}
% \vspace{-2mm}
\label{tab: relearn_wmdp_npo}
\begin{center}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{c|c|c|ccc|ccc}
\toprule[1pt]
\midrule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{UT} (\textuparrow)}} & \multicolumn{6}{c}{\textbf{UE} (\textuparrow)} \\                             
\cline{3-9}
& & W/o atk & $N$ = 20 & $N$ = 40 & $N$ = 60 & $M$ = 1  & $M$ = 2 & $M$ = 3 \\
\midrule
NPO     & 0.44 & 0.74 & 0.57 & 0.39 & 0.37 & 0.57 & 0.40 & 0.37 \\
\midrule
\rowcolor{Gray}
NPO+SAM & 0.42 & 0.74 & \cb{mr}{0.70} & \cb{mr}{0.50} & \cb{mr}{0.45} & \cb{mr}{0.70} & \cb{mr}{0.63} & \cb{mr}{0.59} \\
NPO+RS  & 0.41 & 0.74 & 0.65 & \cb{mr}{0.50} & 0.41 & 0.65 & 0.46 & 0.42 \\
\rowcolor{Gray}
NPO+CR  & 0.43 & 0.75 & 0.62 & 0.44 & 0.43 & 0.62 & 0.59 & 0.52 \\
NPO+GP  & 0.45 & 0.73 & 0.61 & 0.44 & 0.43 & 0.61 & 0.58 & 0.43 \\
\rowcolor{Gray}
NPO+WA  & 0.46 & 0.74 & 0.69 & 0.45 & 0.40 & 0.69 & 0.61 & 0.43 \\
\midrule
\bottomrule[1pt]
\end{tabular}
}
% \vspace{-4mm}
\end{center}
% 
\end{table}
%\end{wraptable}

%
\paragraph{{Unlearning robustness over diverse relearn sets.}} \textbf{Fig.\,\ref{fig: relearn_other_set}} illustrates the robustness of unlearning against relearning attacks using datasets 
(AGNews, GSM8K, and SST2) as motivated by \citep{lucki2024adversarial}. As shown, the UE of NPO+SAM after the relearning attacks consistently outperforms that of vanilla NPO. This suggests that, beyond the 
relearning attacks on the forget set, the robustness of the unlearned model using NPO+SAM generalizes to various types of 
relearning attacks, even when the relearn sets are derived from datasets different from the forget set.

%\begin{wrapfigure}{r}{0.25\textwidth} % Adjust width as needed
\begin{figure}[htb]
    \centering
   % \vspace{-6mm}
    \hspace*{2mm}
    \includegraphics[width=0.3\textwidth]{figs/legend_set_1.pdf}\\
    % \vspace{-5mm}
    \includegraphics[width=0.27\textwidth]{figs/multi_set_data_2.pdf}
    \vspace{-2mm}
    %
    \caption{\small{Unlearning robustness of NPO and NPO+SAM on WMDP under relearning attacks with different sets (AGNews, GSM8K, SST2), using 60 samples for 1 epoch.
    }}
    \label{fig: relearn_other_set}
    \vspace{-5mm}
%\end{wrapfigure}
\end{figure}


% \begin{figure}[htb]
% % \vspace*{-5mm}
% \center
% \hspace*{6mm}
% \includegraphics[width=0.35\textwidth]{figs/legend_set.pdf}\\
% \vspace{-5mm}

% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/multi_set_epoch_2.pdf} 
% &
% \hspace*{-3mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/multi_set_data_2.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a) UE vs. relearning epoch 3} &  \hspace*{-3mm}  
%  \small{(b) UE vs. relearning data 60}\\
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{
% Unlearning robustness comparison of NPO and NPO+SAM on WMDP under relearning attacks using different relearn sets (AGNews, GSM8K, and SST2). \SL{Here the relearn set size and the epoch number are set to xx and xxx, respectively.} 
% \SL{[wrapfigure to only show (b)?]}
% }
% }
% \label{fig: relearn_other_set}
% %\vspace*{-5mm}
% \end{figure}



% \begin{figure}[htb]
% % \vspace*{-5mm}

% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/multi_set_data.pdf} 
% &
% \hspace*{-3mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/multi_set_epoch.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a) Relearning setting (s1)} & 
%  \small{(b) Relearning setting (s2)}\\
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{Performance of relearning attacks on WMDP Bio for NPO, GradDiff, and RMU w/o or w/ SAM. The figure format follows Fig.\,\ref{fig: relearn_wmdp_npo}.
% \SL{[talk to me about marker and line type selection. remove origin.]}
% }}
% \label{fig: relearn_wmdp_method}
% %\vspace*{-5mm}
% \end{figure}


% \textbf{Generalization to smoothing method.} {\JH{not sure whether keeping this paragraph or not, we have discussed following in previous subsections}}In Fig.\,\ref{fig: relearn_wmdp_npo}-(a) and (b), we demonstrate the generalizability of NPO to various smoothing methods. We combine NPO with SAM, GNR, RS, and SWA, and evaluate their robustness under two relearning settings, following (S1) and (S2). The results show that incorporating smoothing methods consistently improves the robustness of NPO against relearning attacks. In Table\,\ref{tab: relearn_wmdp}, we present the final results for both (S1) and (S2). It can be observed that SAM and GNR are the most effective among the four smoothing methods, significantly enhancing NPO's robustness against relearning attacks.

%\textbf{Robustness Enhancement with SAM Across Different Unlearning Methods.} 



% \textbf{Visualization of loss landscape over forgetting data.} We visualize the prediction loss landscape of the unlearned model over the forgetting data in \textbf{Fig.\,\ref{fig: loss_landscape_df_diff_smooth}}. The loss landscape of NPO shows a sharp spike at the origin, with rapid variations occurring within a very small region. However, when NPO is combined with the smoothing method, the sharp spike disappears, and the variations at the origin become relatively smooth. This smoothness ensures that the unlearned model's loss does not fluctuate dramatically under a relearning attacks, making it more robust to such attacks. Additionally, in \textbf{Fig.\,\ref{fig: loss_landscape_dr_diff_smooth} of Appendix\,\ref{appendix: add_exp}}, we show the prediction loss landscape of the unlearned model over the retaining data. These observations demonstrate how the smoothing method enhances the robustness of the unlearning method.

% \begin{itemize}
%     \item Evaluate robustness under two different relearning settings (\textbf{Table\,\ref{tab: relearn_wmdp}} + \textbf{Fig.\,\ref{fig: relearn_wmdp_npo}} + \textbf{Fig.\,\ref{fig: relearn_wmdp_method}} + loss landscape)
%         \begin{itemize}
%             \item Same number of relearning samples but with different relearning epochs.
%             \item Different numbers of relearning samples with the same relearning epoch.
%         \end{itemize} 
%     \item Combining different smoothing methods with unlearning methods improves model robustness.
%     \item Robustness of unlearning method NPO $>$ GradDiff $>$ RMU
% \end{itemize}


\paragraph{{Evaluation on MUSE dataset.}} 
% \textbf{Fig.\,\ref{fig: relearn_muse_npo}} compares the unlearning robustness of NPO with the proposed SAM-enhanced variant (NPO + SAM) on the MUSE Books and MUSE News datasets.
\textbf{Fig.\,\ref{fig: relearn_muse_npo}} compares the unlearning robustness of NPO with NPO + SAM on the MUSE Books and MUSE News datasets.
Recall that unlearning effectiveness on MUSE is evaluated using knowledge memorization (KnowMem) and verbatim memorization (VerbMem) on the forget set $\Df$, with lower values indicating better unlearning effectiveness.  
As we can see, under relearning attacks with varying numbers of relearn samples (75, 100, 125), NPO+SAM consistently improves the robustness of NPO, as evidenced by lower KnowMem and VerbMem values. Furthermore, changes in VerbMem on $\mathcal{D}_\mathrm{f}$ after the relearning attacks are more pronounced compared to those in KnowMem on $\mathcal{D}_\mathrm{f}$. This indicates that unlearning precise tokens (VerbMem) is more vulnerable to relearning attacks than unlearning general knowledge encoded in the tokens (KnowMem).
In addition to UE, utility performance results are provided in \textbf{Table\,\ref{tab: relearn_muse}} in \textbf{Appendix\,\ref{appendix: add_result_muse}}.

\begin{figure}[htb]
\vspace{-3.5mm}
\center
% \hspace*{6mm}
\hspace*{2mm}
\includegraphics[width=0.48\textwidth]{figs/muse_legend.pdf}\\
\vspace{-1.3mm}
\begin{tabular}{cc}
\hspace*{-3mm}
\includegraphics[width=0.25\textwidth,height=!]{figs/muse_books_3.pdf} 
&
% \hspace*{-6mm}
\includegraphics[width=0.25\textwidth,height=!]{figs/muse_news_3.pdf}
\vspace*{-1mm}
\\
\hspace*{3mm} \small{(a) MUSE Books} & \small{(b) MUSE News}\\
\end{tabular}
\vspace{-2mm}
\caption{\small{
Unlearning robustness of NPO and NPO+SAM on MUSE Books and News under relearning attacks with varying data amounts ($\bullet$, $\blacksquare$, and $\blacktriangle$ denote 200, 300, and 400 samples for Books, and 400, 500, and 600 samples for News.). UE is measured via KnowMem and VerbMem on $\mathcal{D}_\mathrm{f}$ (lower is better). The original model’s performance is included for reference; results closer to `origin' indicate weaker unlearning robustness.
}}
\label{fig: relearn_muse_npo}
\vspace*{-1mm}
\end{figure}

% \begin{figure}[htb]
% \centering
% % 左图
% \begin{minipage}[t]{0.47\textwidth}
%     \centering
%     % 可以直接把第一张图的代码拷贝过来
%     % ------- BEGIN 第一张图代码 -------
%     \vspace{-3.5mm}
%     \includegraphics[width=\textwidth]{figs/muse_legend.pdf}\\
%     \vspace{-1.3mm}
%     \begin{tabular}{cc}
%     \hspace*{-3mm}
%     \includegraphics[width=0.45\textwidth,height=!]{figs/muse_books_3.pdf} 
%     &
%     \includegraphics[width=0.45\textwidth,height=!]{figs/muse_news_3.pdf}
%     \vspace*{-1mm}
%     \\
%     \hspace*{3mm} \small{(a) MUSE Books} & \small{(b) MUSE News}\\
%     \end{tabular}
%     \vspace{-2mm}
%     \caption{\small{
%     Unlearning robustness of NPO and NPO+SAM on MUSE Books and News under relearning attacks with varying data amounts 
%     (75, 100, 125 samples). UE is measured via KnowMem and VerbMem on $\mathcal{D}_\mathrm{f}$. 
%     The original model’s performance is included for reference.
%     }}
%     \label{fig: relearn_muse_npo}
%     % ------- END 第一张图代码 -------
% \end{minipage}
% \hfill
% % 右图
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     % 可以直接把第二张图的代码拷贝过来
%     % ------- BEGIN 第二张图代码 -------
%     \vspace{-1mm}
%     \begin{tabular}{cc}
%     % \hspace*{-6mm}
%     \includegraphics[width=0.45\textwidth,height=!]{figs/adv_bar.pdf} 
%     &
%     \hspace*{-6mm}
%     \includegraphics[width=0.45\textwidth,height=!]{figs/adv_kl_divergence.pdf}
%     \vspace*{-1mm}
%     \\
%     \hspace*{-4mm}
%     \small{(a) UE vs. adversarial prompt} & \hspace*{-3mm} \small{(b) KL divergence vs. token index}\\
%     \end{tabular}
%     \vspace{-2mm}
%     \caption{\small{
%     (a) Unlearning robustness comparison of NPO and its smooth enhancements on WMDP against jailbreaking attacks. UE is measured as $1 - \text{WMDP accuracy}$ on  evaluation set.
%     (b) KL divergence for each output token between the unlearned model and the original model.
%     }}
%     \label{fig: adv_prompt}
%     % ------- END 第二张图代码 -------
% \end{minipage}
% \end{figure}


\paragraph{{Unlearning robustness against jailbreaking attacks.}}
In \textbf{Fig.\,\ref{fig: adv_prompt}-(a)}, we present the unlearning effectiveness of NPO and its
smooth enhancements on WMDP under (input-level) adversarial prompts generated by the  enhanced GCG  \citep{lucki2024adversarial}.
As we can see, 
%smoothness optimization improves NPO's robustness against jailbreaking attacks. Specifically, 
NPO+SAM and NPO+RS yield lossless UE under jailbreaking attacks, while NPO suffers a significant drop in UE. This is because NPO+SAM and NPO+RS introduce weight smoothing through worst-case and randomized perturbations, respectively. 
These smoothing effects are known to be helpful in defending against input-level adversarial attacks \citep{xu2022weight,wei2023sharpness,zhang2024duality,cohen2019certified}. 
We also provide generation examples under jailbreaking attacks for NPO and NPO+SAM in \textbf{Table\,\ref{tab: adv_examples}} of \textbf{Appendix\,\ref{appendix: adv_examples}}.
Thus, our proposal improves resistance to not only relearning attacks (which perturb model weights) but also jailbreaking attacks (which perturb input prompts).

\begin{figure}[htb]
% \center
% \hspace*{6mm}
% \hspace*{-40mm}
% \includegraphics[width=0.23\textwidth]{figs/adv_legend.pdf}\\
\vspace{-2mm}
\centering
\begin{tabular}{cc}
\hspace*{-6mm}
\includegraphics[width=0.275\textwidth,height=!]{figs/adv_bar.pdf} 
&
\hspace*{-6mm}
\includegraphics[width=0.28\textwidth,height=!]{figs/adv_kl_divergence.pdf}
\vspace*{-1mm}
\\
 \small{(a) UE vs. adversarial prompt} &  \hspace*{-3mm}  
 \small{(b) KL divergence vs. token index}\\
\end{tabular}
\vspace{-3mm}
\caption{\small{
(a) Unlearning robustness comparison of NPO and its smooth enhancements on WMDP against jailbreaking attacks. (b) KL divergence for each output token between the unlearned model and the original model when facing jailbreaking attacks.}
}
\label{fig: adv_prompt}
%\vspace*{-5mm}
\end{figure}

In \textbf{Fig.\,\ref{fig: adv_prompt}-(b)}, we further investigate why smoothness optimization improves robustness against jailbreaking attacks by plotting the KL divergence between the unlearned model and the original model for each output token. A higher KL divergence indicates more effective unlearning.
%
As we can see, the KL divergence for NPO at the first few tokens is notably small, suggesting insufficient unlearning for these `shallow' tokens. This aligns with the known `shallow alignment issue', a limitation in model alignment against jailbreaking attacks \citep{qi2025safety}. In contrast, the use of smoothness optimization alleviates this issue, as the first few tokens are effectively unlearned. This improvement explains the enhanced robustness of smoothness optimization against jailbreaking attacks.






% \begin{table}[htb]
% \caption{\small{Performance overview of NPO variants with or without variant smoothing techniques against jailbreaking attacks. The table format follows Table\,\ref{tab: relearn_wmdp}.} \CF{Change this figure to table.}}
% \label{tab: adv_prompt_wmdp}
% \begin{center}
% \resizebox{0.8\columnwidth}{!}{
% \begin{tabular}{c|c|cc}
% \toprule[1pt]
% \midrule
% \multirow{3}{*}{Method} & \multirow{3}{*}{MMLU (\textuparrow)} & \multicolumn{2}{c}{1 - WMDP Bio Acc (\textuparrow)} \\                        
% \cmidrule{3-4}
% & & Before Attack & After Attack \\
% \midrule
% NPO       & 0.44 & 0.74 & 0.45 (\textcolor{blue}{0.29}) \\
% \midrule
% NPO + SAM & 0.42 & 0.74 & 0.73 (\textcolor{blue}{0.01}) \\
% NPO + RS  & 0.41 & 0.74 & 0.74 (\textcolor{blue}{0.00}) \\
% NPO + CR  & 0.43 & 0.75 & 0.63 (\textcolor{blue}{0.12}) \\
% NPO + GNR & 0.45 & 0.73 & 0.53 (\textcolor{blue}{0.20}) \\
% NPO + SWA & 0.46 & 0.74 & 0.47 (\textcolor{blue}{0.27}) \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{center}
% \end{table}




% \paragraph{Other ablation studies.}
% %\textbf{Ablation Study on SAM's Hyperparameter $\rho$.}
% \SL{[one sentence summary.]}
% In \textbf{Table\,\ref{tab: muse_rho_ablation}} in \textbf{Appendix\,\ref{appendix: add_result_muse}}, we present the impact of \(\rho\) on unlearning. $\rho$ is a hyperparameter in SAM that controls the magnitude of weight perturbation, with larger $\rho$ values corresponding to stronger perturbations. We conduct an ablation study on $\rho$ using the MUSE Books benchmark, and the results. It can be observed that when $\rho$ is too small (e.g., $0.001$), SAM provides limited improvement against relearning attacks. Conversely, when $\rho$ is too large (e.g., $0.1$), the perturbations become excessive, preventing the model from successfully unlearning. When $\rho = 0.01$, the model achieves a balance: it successfully unlearns while also demonstrating improved robustness, with smaller changes of KnowMem on $\mathcal{D}_\mathrm{f}$ and VerbMem on $\mathcal{D}_\mathrm{f}$ observed before and after the relearning attacks.


% \begin{table}[htb]
% \caption{Unlearning and Relearning Performance on WMDP Bio. relearning attacks are conducted using WMDP Bio, AGNews, GSM8K, and SST2. \textuparrow indicates that the higher the metric, the better. The performance gap in WMDP Bio Acc before and after the relearning attacks is indicated by (\textcolor{blue}{$\bullet$}). Note that a smaller performance gap compared to the \textit{"Before Attack"} accuracy reflects better robustness of the MU method. The metric \textit{Avg.} is calculated as the average 1 - WMDP Bio Acc, including WMDP Bio, AGNews, GSM8K, and SST2.}
% \label{tab: relearn_wmdp}
% \begin{center}
% \resizebox{0.8\columnwidth}{!}{
% \begin{tabular}{c|c|cc}
% \toprule[1pt]
% \midrule
% \multirow{3}{*}{Method} & \multirow{3}{*}{MMLU (\textuparrow)} & \multicolumn{2}{c}{1 - WMDP Bio Acc (\textuparrow)} \\                        
% \cmidrule{3-4}
% & & Before Attack & After Attack \\
% \midrule
% NPO       & 0.44 & 0.74 & 0.45 (\textcolor{blue}{0.29}) \\
% \midrule
% NPO + SAM & 0.42 & 0.74 & 0.73 (\textcolor{blue}{0.01}) \\
% NPO + RS  & 0.41 & 0.74 & 0.74 (\textcolor{blue}{0.00}) \\
% NPO + GNR & 0.45 & 0.73 & 0.53 (\textcolor{blue}{0.20}) \\
% NPO + SWA & 0.46 & 0.74 & 0.47 (\textcolor{blue}{0.27}) \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}
% }
% \end{center}
% \end{table}



% \begin{table*}[t]
% \caption{Unlearning and Relearning Performance on WMDP Bio. relearning attacks are conducted using WMDP Bio, AGNews, GSM8K, and SST2. \textuparrow indicates that the higher the metric, the better. The performance gap in WMDP Bio Acc before and after the relearning attacks is indicated by (\textcolor{blue}{$\bullet$}). Note that a smaller performance gap compared to the \textit{"Before Attack"} accuracy reflects better robustness of the MU method. The metric \textit{Avg.} is calculated as the average 1 - WMDP Bio Acc, including WMDP Bio, AGNews, GSM8K, and SST2.}
% \label{tab: relearn_wmdp}
% \begin{center}
% \begin{tabular}{c|c|c|cccc|c}
% \toprule[1pt]
% \midrule
% \multirow{3}{*}{Method} & \multirow{3}{*}{MMLU (\textuparrow)} & \multicolumn{6}{c}{1 - WMDP Bio Acc (\textuparrow)} \\                             
% \cmidrule{3-8}
% & & Before Attack & WMDP Bio & AGNews & GSM8K & SST2 & Avg. \\
% \midrule
% \rowcolor{Gray}
% \multicolumn{8}{c}{\textbf{(S1) 20 samples for 3 epochs}} \\
% \midrule
% NPO       & 0.44 & 0.74 & 0.37 (\textcolor{blue}{0.37}) & 0.39 (\textcolor{blue}{0.35}) & 0.38 (\textcolor{blue}{0.36}) & 0.38 (\textcolor{blue}{0.36}) & 0.38 (\textcolor{blue}{0.36}) \\
% \midrule
% NPO + SAM & 0.42 & 0.74 & 0.59 (\textcolor{blue}{0.15}) & 0.41 (\textcolor{blue}{0.33}) & 0.42 (\textcolor{blue}{0.32}) & 0.41 (\textcolor{blue}{0.32}) & 0.46 (\textcolor{blue}{0.28}) \\
% NPO + RS  & 0.41 & 0.74 & 0.42 (\textcolor{blue}{0.32}) & 0.42 (\textcolor{blue}{0.32}) & 0.42 (\textcolor{blue}{0.32}) & 0.42 (\textcolor{blue}{0.33}) & 0.42 (\textcolor{blue}{0.32}) \\
% NPO + GNR & 0.45 & 0.73 & 0.43 (\textcolor{blue}{0.30}) & 0.40 (\textcolor{blue}{0.33}) & 0.45 (\textcolor{blue}{0.28}) & 0.49 (\textcolor{blue}{0.24}) & 0.44 (\textcolor{blue}{0.29}) \\
% NPO + SWA & 0.46 & 0.74 & 0.43 (\textcolor{blue}{0.30}) & 0.39 (\textcolor{blue}{0.34}) & 0.39 (\textcolor{blue}{0.35}) & 0.39 (\textcolor{blue}{0.34}) & 0.40 (\textcolor{blue}{0.33}) \\
% \midrule
% \rowcolor{Gray}
% \multicolumn{8}{c}{\textbf{(S2) 60 samples for 1 epoch}} \\
% \midrule
% NPO       & 0.44 & 0.74 & 0.37 (\textcolor{blue}{0.37}) & 0.39 (\textcolor{blue}{0.35}) & 0.37 (\textcolor{blue}{0.37}) & 0.38 (\textcolor{blue}{0.36}) & 0.38 (\textcolor{blue}{0.36}) \\
% \midrule
% NPO + SAM & 0.42 & 0.74 & 0.45 (\textcolor{blue}{0.29}) & 0.42 (\textcolor{blue}{0.32}) & 0.42 (\textcolor{blue}{0.32}) & 0.43 (\textcolor{blue}{0.31}) & 0.43 (\textcolor{blue}{0.31}) \\
% NPO + RS  & 0.41 & 0.74 & 0.41 (\textcolor{blue}{0.33}) & 0.41 (\textcolor{blue}{0.33}) & 0.41 (\textcolor{blue}{0.33}) & 0.40 (\textcolor{blue}{0.34}) & 0.41 (\textcolor{blue}{0.33}) \\
% NPO + GNR & 0.45 & 0.73 & 0.43 (\textcolor{blue}{0.30}) & 0.48 (\textcolor{blue}{0.25}) & 0.50 (\textcolor{blue}{0.23}) & 0.49 (\textcolor{blue}{0.24}) & 0.47 (\textcolor{blue}{0.25}) \\
% NPO + SWA & 0.46 & 0.74 & 0.40 (\textcolor{blue}{0.34}) & 0.42 (\textcolor{blue}{0.32}) & 0.39 (\textcolor{blue}{0.34}) & 0.40 (\textcolor{blue}{0.34}) & 0.40 (\textcolor{blue}{0.34}) \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}
% \end{center}
% \end{table*}

% % \begin{figure}[htb]
% % % \vspace*{-5mm}
% % \resizebox{\columnwidth}{!}{
% %     \includegraphics[width=0.28\columnwidth]{figs/npo_step_wmdp_var.pdf}
% %     \includegraphics[width=0.28\columnwidth]{figs/npo_epoch_wmdp_var.pdf}

% % }
% % \vspace*{-6.5mm}
% % \caption{Performance of relearning attacks on NPO w/o or w/ Smoothing. The evaluation is conducted under two relearning settings: (a) varying the number of relearning samples (20, 40, 60) with the same number of relearning epochs (1), and (b) fixing the number of relearning samples (20) while varying the number of relearning epochs (1, 2, 3). The results are averaged over five independent random trials.
% % }
% % % \vspace*{-5mm}
% % \label{fig: relearn_wmdp_npo}
% % \end{figure}

% \begin{figure}[htb]
% % \vspace*{-5mm}
% % \resizebox{\columnwidth}{!}{
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.25\textwidth,height=!]{figs/npo_epoch_wmdp_var.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.25\textwidth,height=!]{figs/npo_step_wmdp_var.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a)} & \hspace*{6mm} \small{(b)}\\
% % \hspace*{2mm} {(a) Forget quality} & {(b) Model utility}
% \end{tabular}
% % }
% \vspace{-2mm}
% \caption{\small{Performance of relearning attacks on WMDP Bio for NPO w/o or w/ Smoothing. The evaluation is conducted under two relearning settings: (a) fixing the number of relearning samples (20) while varying the number of relearning epochs (1, 2, 3), and (b) varying the number of relearning samples (20, 40, 60) with the same number of relearning epochs (1). The results are averaged over five independent random trials.}}
% \label{fig: relearn_wmdp_npo}
% \vspace*{-5mm}
% \end{figure}


% \begin{figure}[htb]
% % \vspace*{-5mm}
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.25\textwidth,height=!]{figs/method_epoch_wmdp_var.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.25\textwidth,height=!]{figs/method_step_wmdp_var.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a)} & \hspace*{3mm} \small{(b)} \\
% % \hspace*{2mm} {(a) Forget quality} & {(b) Model utility}
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{Performance of relearning attacks on WMDP Bio for NPO, GradDiff, and RMU w/o or w/ SAM. The figure format follows Fig.\,\ref{fig: relearn_wmdp_method}.
% }}
% \label{fig: relearn_wmdp_method}
% \vspace*{-5mm}
% \end{figure}


% \begin{figure}[htb]
% % \vspace*{-5mm}
% % \resizebox{\columnwidth}{!}{
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/muse_books_2d.pdf} 
% &
% % \hspace*{-6mm}
% \includegraphics[width=0.23\textwidth,height=!]{figs/muse_news_2d.pdf}
% \vspace*{-1mm}
% \\
% \hspace*{3mm} \small{(a) MUSE Books} & \small{(b) MUSE News}\\
% \end{tabular}
% \vspace{-2mm}
% \caption{\small{Performance of relearning attacks on MUSE Books/News for NPO and NPO w/ SAM. Robustness is evaluated using KnowMem on $\mathcal{D}_\mathrm{f}$ and VerbMem on $\mathcal{D}_\mathrm{f}$. Different Relearned models are obtained by varying the number of relearning samples while keeping the number of relearning epochs constant. The closer the Relearned model is to the Original model, the less robust it is.}}
% \label{fig: relearn_muse_npo}
% \vspace*{-5mm}
% \end{figure}


% \begin{figure}[htb]
% % \vspace*{-5mm}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.24\columnwidth,height=!]{figs/npo_df.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.24\columnwidth,height=!]{figs/npo_sam_df.pdf}
% \vspace*{-1mm}
% \\
% % \hspace*{2mm} {(a) Forget quality} & {(b) Model utility}
% \end{tabular}
% }
% \hspace*{21mm} \small{(a)} \hspace{31mm} \small{(b)} \\
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{cc}
% \hspace*{-3mm}
% \includegraphics[width=0.24\columnwidth,height=!]{figs/npo_gnr_df.pdf} 
% &
% \hspace*{-6mm}
% \includegraphics[width=0.24\columnwidth,height=!]{figs/npo_rs_df.pdf}
% \vspace*{-1mm}
% \\
% % \hspace*{2mm} {(a) Forget quality} & {(b) Model utility}
% \end{tabular}
% }
% \hspace*{21mm} \small{(c)} \hspace{31mm} \small{(d)}
% \vspace{-2mm}
% \caption{\small{Prediction loss landscape of the unlearned model obtained by NPO w/ or w/o smoothing over the WMDP Bio forget data. Z axis denotes the loss value.
% }}
% \label{fig: loss_landscape_df_diff_smooth}
% \vspace*{-5mm}
% \end{figure}


% % \begin{figure}[htb]
% % % \vspace*{-5mm}
% % \resizebox{\columnwidth}{!}{
% %     \includegraphics[width=0.28\columnwidth]{figs/method_step_wmdp_var.pdf}
% %     \includegraphics[width=0.28\columnwidth]{figs/method_epoch_wmdp_var.pdf}

% % }
% % % \vspace*{-6.5mm}
% % \caption{Performance of relearning attacks on NPO, GradDiff, and RMU w/o or w/ SAM. The figure format follows Fig.\,\ref{fig: relearn_wmdp_npo}.
% % }
% % \vspace*{-5mm}
% % \label{fig: relearn_wmdp_method}
% % \end{figure}