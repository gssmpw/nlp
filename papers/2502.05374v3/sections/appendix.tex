\clearpage
\onecolumn
\section*{\Large{Appendix}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\makeatletter 
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\theHsection}{\Alph{section}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\theHfigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\theHtable}{A\arabic{table}}
\makeatother

\renewcommand{\thetable}{A\arabic{table}}
\setcounter{mylemma}{0}
\renewcommand{\themylemma}{A\arabic{mylemma}}
\setcounter{algorithm}{0}
\renewcommand{\thealgorithm}{A\arabic{algorithm}}
\setcounter{equation}{0}
\renewcommand{\theequation}{A\arabic{equation}}

\input{sections/appendix/algorithm}
\input{sections/appendix/dr_landscape}
\input{sections/appendix/exp_setup}
\input{sections/appendix/add_result_wmdp}
\input{sections/appendix/add_result_muse}
\input{sections/appendix/adv_example}

% \begin{figure}[htb]
% \vspace{3mm}
% \centerline{
% \hspace*{0mm}
% \includegraphics[width=0.65\textwidth,height=!]{figs/adv_example.pdf}
% }
% \vspace*{-10mm}
% \caption{
% \small{The response of NPO and NPO+SAM under jailbreaking attack on WMDP.}
% % \SL{[talk to me; considering putting in appendix.]}
% }
%   \label{fig: illustration_length_normalization}
%   % \vspace*{-6mm}
% \end{figure}

% \section{Additional Experiment Details and Results}





% \noindent \textbf{Relearning Attack} For WMDP, we measure this using MMLU Accuracy, and for MUSE, we rely on KnowMem on $\mathcal{D}_r$. Robustness evaluates the model's resilience against relearning attacks and adversarial prompts. It is quantified by the change in unlearning effectiveness before and after the attack on the unlearned model.

% \noindent \textbf{Adversarial Prompt} For WMDP, we measure this using MMLU Accuracy, and for MUSE, we rely on KnowMem on $\mathcal{D}_r$. Robustness evaluates the model's resilience against relearning attacks and adversarial prompts. It is quantified by the change in unlearning effectiveness before and after the attack on the unlearned model.

% \subsection{Additional Experiment Results}
% \label{appendix: add_exp}

% In \textbf{Fig.\,\ref{fig: loss_lanscape_dr}}, we further illustrate the loss landscapes of the origin model, the unlearned model obtained using NPO, and the smooth variants of NPO on the retain set. It is evident that the loss landscapes of the origin model and the unlearned model are quite similar, indicating that the unlearning process primarily affects the model's performance on the forget data while having minimal impact on its performance on the retain set. Furthermore, it is worth noting that the loss landscapes of the unlearned models from NPO and its smooth variants show little difference on the retain data but exhibit significant differences on the forget data (as shown in Fig.\,\ref{fig: loss_lanscape}). This observation further suggests that the robustness of the unlearned model is closely related to the smoothness of the forget loss.


% \begin{figure*}[htb] % 或者 [htbp], 视排版需求
% \centering

% %============= 第二行：5 幅图 =============
% \begin{tabular}{ccccccc}
% \hspace*{-3mm}
% \raisebox{-0.02\height}{\rotatebox{90}{\small{Loss landscape on $\mathcal{D}_\mathrm{r
% }$}}} \hspace*{-5mm} % 提高位置，使其与图片垂直居中
% &
% \hspace*{-3mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_dr.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_sam_dr.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_rs_dr.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_gnr_dr.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_cr_dr.pdf}
% &
% \hspace*{-6mm}
% \includegraphics[width=0.16\textwidth,height=!]{figs/npo_swa_dr.pdf}
% \\

% &
% \hspace*{-6mm}
% \small{(a) NPO}
% &
% \hspace*{-6mm}
% \small{(b) NPO + SAM}
% &
% \hspace*{-6mm}
% \small{(c) NPO + RS}
% &
% \hspace*{-6mm}
% \small{(d) NPO + GP}
% &
% \hspace*{-6mm}
% \small{(e) NPO + CR}
% &
% \hspace*{-6mm}
% \small{(f) NPO + WA}
% \\
% \end{tabular}

% \vspace*{-2mm}
% \caption{\small{The prediction loss landscape of the NPO-unlearned and smooth variants of NPO-unlearned model on the retain set.
% }}
% \label{fig: loss_lanscape_dr}
% \end{figure*}

% \begin{figure}[htb]
% % \begin{wrapfigure}{r}{0.8\textwidth}
% \centering
% \vspace*{-3mm}
% \begin{tabular}{ccccc}
% \hspace*{-5mm}
% \includegraphics[width=0.19\textwidth]{figs/npo_dr.pdf} &
% \hspace*{-5mm}
% \includegraphics[width=0.19\textwidth]{figs/npo_sam_dr.pdf} &
% \hspace*{-5mm}
% \includegraphics[width=0.19\textwidth]{figs/npo_gnr_dr.pdf} &
% \hspace*{-5mm}
% \includegraphics[width=0.19\textwidth]{figs/npo_RS_dr.pdf}
% &
% \hspace*{-5mm}
% \includegraphics[width=0.19\textwidth]{figs/npo_swa_dr.pdf}\\
% \scriptsize{(a) NPO} & \scriptsize{(b) NPO w/ SAM} &
%  \scriptsize{(c) NPO w/ GNR} &
%  \scriptsize{(d) NPO w/ RS} &
%  \scriptsize{(e) NPO w/ SWA}
% \end{tabular}
% \vspace*{-2.5mm}
% \caption{
% \small{Prediction loss landscape of the unlearned model obtained by NPO w/ or w/o smoothing over the WMDP Bio forget data. The figure format follows Fig.\,\ref{fig: loss_landscape_df_diff_smooth}.}
% }
% \vspace*{-3mm}
% \label{fig: loss_landscape_dr_diff_smooth}
% \end{figure}


% \begin{table}[htb]
% % \vspace*{-6mm}
% % \begin{table*}[htb]
% \begin{center}
% \caption{\small{
% Performance comparison of NPO and NPO w/ SAM on MUSE before and after the relearning attack, evaluated under two unlearning settings: LLaMA2-7B on News and ICLM-7B on Books.
% } 
% }
% % \vspace*{-2mm}
% \resizebox{0.9\textwidth}{!}{
% \begin{tabular}{c|c|ccc|ccc}
% \toprule[1pt]
% \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}{c}
%      KnowMem  \\
%     $\mathcal{D}_r$ ($\uparrow$)
% \end{tabular}}} & \multicolumn{3}{c|}{\textbf{Before attack}} & \multicolumn{3}{c}{\textbf{After attack}} \\
% \cmidrule{3-8}

% &

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%     PrivLeak \\
%     ($\rightarrow$ 0)
% \end{tabular} 

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%     PrivLeak \\
%     ($\rightarrow$ 0)
% \end{tabular} \\

% \begin{table}[htb]
% % \vspace*{-6mm}
% % \begin{table*}[htb]
% \begin{center}
% \caption{\small{
% Performance comparison of NPO and NPO w/ SAM on MUSE before and after the relearning attack, evaluated under two unlearning settings: LLaMA2-7B on News and ICLM-7B on Books.
% } 
% }
% % \vspace*{-2mm}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{c|c|cc|ccc}
% \toprule[1pt]
% \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}{c}
%      KnowMem  \\
%     $\mathcal{D}_r$ ($\uparrow$)
% \end{tabular}}} & \multicolumn{2}{c|}{\textbf{Before attack}} & \multicolumn{3}{c}{\textbf{After attack}} \\
% \cmidrule{3-7}

% &

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & Avg  \\


% \midrule
% \rowcolor{Gray}
% \multicolumn{7}{c}{\textbf{MUSE News}} \\\midrule

% Origin & 54.31 & 58.29 & 62.93 & N/A                             & N/A                             & N/A   \\
% NPO & 41.58 & 0.00  & 43.93 & 56.57 (\textcolor{blue}{56.57}) & 57.58 (\textcolor{blue}{13.65}) & 35.11 \\
% NPO w/ SAM & 42.58 & 0.00  & 42.26 & 51.47 (\textcolor{blue}{51.47}) & 54.74 (\textcolor{blue}{12.48}) & 31.97 \\

% \midrule
% \rowcolor{Gray}
% \multicolumn{7}{c}{\textbf{MUSE Books}} \\\midrule

% Origin & 67.01 & 99.56 & 58.32 & N/A                             & N/A                             & N/A   \\
% NPO & 34.71 & 0.00  & 0.00  & 67.52 (\textcolor{blue}{67.52}) & 45.33 (\textcolor{blue}{45.33}) & 56.42 \\
% NPO w/ SAM & 35.48 & 0.00  & 0.00  & 58.38 (\textcolor{blue}{58.38}) & 38.33 (\textcolor{blue}{38.33}) & 48.36 \\

% \midrule
% \bottomrule
% \end{tabular}
% }
% \label{tab: relearn_muse}
% % \vspace*{-4mm}
% \end{center}
% \end{table}

% \textbf{Ablation Study on SAM's Hyperparameter $\rho$.}
% $\rho$ is a hyperparameter in SAM that controls the magnitude of weight perturbation, with larger $\rho$ values corresponding to stronger perturbations. We conduct an ablation study on $\rho$ using the MUSE Books benchmark, and the results are shown in \textbf{Table\,\ref{tab: muse_rh0_ablation}}. It can be observed that when $\rho$ is too small (e.g., $0.001$), SAM provides limited improvement against relearning attacks. Conversely, when $\rho$ is too large (e.g., $0.1$), the perturbations become excessive, preventing the model from successfully unlearning. When $\rho = 0.01$, the model achieves a balance: it successfully unlearns while also demonstrating improved robustness, with smaller changes of KnowMem on $\mathcal{D}_\mathrm{f}$ and VerbMem on $\mathcal{D}_\mathrm{f}$ observed before and after the relearning attack.


% \begin{table}[htb]
% \begin{center}
% \caption{\small{Performance comparison of NPO w/ SAM with different $\rho$ on MUSE Books before and after the relearning attack. The table format follows Table\,\ref{tab: relearn_wmdp}.
% } 
% }
% % \vspace*{-2mm}
% \resizebox{0.8\textwidth}{!}{
% \begin{tabular}{c|c|cc|ccc}
% \toprule[1pt]
% \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}{c}
%      KnowMem  \\
%     $\mathcal{D}_r$ ($\uparrow$)
% \end{tabular}}} & \multicolumn{2}{c|}{\textbf{Before attack}} & \multicolumn{3}{c}{\textbf{After attack}} \\
% \cmidrule{3-7}

% &

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & Avg  \\

% \midrule
% \rowcolor{Gray}
% w/o SAM & 34.71 & 0.00 & 0.00 & 67.52 (\textcolor{blue}{67.52}) & 45.33 (\textcolor{blue}{45.33}) & 56.92 \\
% $\rho = 0.001$ & 37.41 & 0.00 & 0.00 & 70.9 (\textcolor{blue}{70.90})   & 42.5 (\textcolor{blue}{42.50})   & 56.70 \\
% \rowcolor{Gray}
% $\rho = 0.01$ & 35.48 & 0.00 & 0.00 & 58.38 (\textcolor{blue}{58.38}) & 38.33 (\textcolor{blue}{38.33}) & 48.36 \\
% $\rho = 0.1$ & 23.91 & 0.00 & 0.00 & 52.96 (\textcolor{blue}{52.96}) & 40.52 (\textcolor{blue}{40.52}) & 46.74 \\
% \midrule
% \bottomrule
% \end{tabular}
% }
% \label{tab: muse_rh0_ablation}
% % \vspace*{-4mm}
% \end{center}
% \end{table}


% \begin{table}[htb]
% % \vspace*{-6mm}
% % \begin{table*}[htb]
% \begin{center}
% \caption{\small{Performance comparison of NPO w/ SAM with different $\rho$ on MUSE before and after the relearning attack, evaluated under one unlearning settings: ICLM-7B on Books.
% } 
% }
% % \vspace*{-2mm}
% \resizebox{0.9\textwidth}{!}{
% \begin{tabular}{c|c|ccc|ccc}
% \toprule[1pt]
% \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}{c}
%      KnowMem  \\
%     $\mathcal{D}_r$ ($\uparrow$)
% \end{tabular}}} & \multicolumn{3}{c|}{\textbf{Before attack}} & \multicolumn{3}{c}{\textbf{After attack}} \\
% \cmidrule{3-8}

% &

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%     PrivLeak \\
%     ($\rightarrow$ 0)
% \end{tabular} 

% & \begin{tabular}{c}
%      VerbMem   \\
%      $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}

% & \begin{tabular}{c}
%    KnowMem \\
%       $\mathcal{D}_f$ ($\downarrow$)
% \end{tabular}  

% & \begin{tabular}{c}
%     PrivLeak \\
%     ($\rightarrow$ 0)
% \end{tabular} \\

% \midrule

% \begin{tabular}{c}
%    w/ SAM \\
%    $\rho = 0.001$
% \end{tabular} & & & & & & & \\


% \midrule
% \bottomrule
% \end{tabular}
% }
% \label{tab: muse}
% % \vspace*{-4mm}
% \end{center}
% \end{table}




