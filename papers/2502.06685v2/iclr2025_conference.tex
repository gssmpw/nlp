
\documentclass{article} %
\usepackage{iclr2025_conference,times}

\input{package}
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{soul}
\usepackage{afterpage}


\hypersetup{
   breaklinks=true,   %
   colorlinks=true,   %
}
\definecolor{red}{HTML}{ca0020}
\definecolor{lightred}{HTML}{f4a582}
\definecolor{lightblue}{HTML}{92c5de}
\definecolor{green}{HTML}{008837}
\definecolor{blue}{HTML}{2c7bb6}


\def\ptarget{p_\text{target}}
\def\ptilde{\tilde{p}_\text{target}}

\title{No Trick, No Treat: \\Pursuits and Challenges Towards \\Simulation-free Training of Neural Samplers}


\author{Jiajun He$^{*, 1}$\thanks{Equal Contribution. Corresponding to \texttt{jh2383@cam.ac.uk}, \texttt{yuanqidu@cs.cornell.edu}},
Yuanqi Du$^{*, 2}$,
Francisco Vargas$^{1,3}$,
Dinghuai Zhang$^{4}$, \\
\AND
Shreyas Padhy$^{1}$,
RuiKang OuYang$^{1}$,
Carla Gomes$^{2}$,
José Miguel Hernández-Lobato$^{1}$
\Note $^1$University of Cambridge, $^2$Cornell University, $^3$Xaira Therapeutics, $^4$Microsoft Research
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\JJ}[1]{\textcolor{orange}{JH: #1}}
\newcommand{\SP}[1]{\textcolor{red}{(SP: #1)}}
 \iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
We consider the \textit{sampling} problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. 
Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. 
However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. 
This motivates the pursuit of \textit{simulation-free} training procedures of neural samplers. 
In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. 
However, it ultimately suffers from severe mode collapse. 
On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. 
We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of  Langevin preconditioning, most of them fail to adequately cover even a simple target. 
Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers.
\end{abstract}

\input{sections/introduction}

\input{sections/method}

\input{sections/results}

\input{sections/conclusion}

\newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix



\section{Taxonomy of Objective Functions}

In this section, we briefly describe different objectives that we reviewed and used in the main text.

\subsection{Path measure alignment objectives}


The \textit{path measure alignment} framework aims to align the 
sampling process starting from $p_\text{prior}$ to a      ``target"  process starting from $p_\text{target}$. 
In the following, we denote  $\mathbb{Q}$ as the sampling process  and $\mathbb{P}$ as the ``target" process.
However, we should note that this notation does not necessarily imply that $\mathbb{Q}$ is the process parameterized by the model. 
In fact, this is only true for samplers like PIS or DDS. 
For escorted transport samplers like CMCD, both  $\mathbb{Q}$ and  $\mathbb{P}$ involve the model, and for annealed variance reduction sampler like MCD,  $\mathbb{Q}$ is fixed, and the model only appears in $\mathbb{P}$.
We now describe five commonly used objectives:

\textbf{Reverse KL divergence.} 
Reverse KL divergence is defined as
\begin{align}
    D_\mathrm{KL}[\mathbb{Q}||\mathbb{P}] = \E_{\mathbb{Q}} \left[
  \log   \frac{\mathrm{d} \mathbb{Q}}{\mathrm{d} \mathbb{P}}
    \right].
\end{align}
In practice, we approximate the expectation with Monte Carlo estimators, and calculate the log Radon–Nikodym derivative $\log   \frac{\mathrm{d} \mathbb{Q}}{\mathrm{d} \mathbb{P}}$, either through Gaussian approximation via Euler–Maruyama discretization or by applying Girsanov's theorem.

\textbf{Log-variance divergence.} Log-variance divergence optimizes the second moment of the log ratio
\begin{align}
    D_\mathrm{logvar}[\mathbb{Q}||\mathbb{P}] = \text{Var}_{\tilde{\mathbb{Q}}} \left(
  \log   \frac{\mathrm{d} \mathbb{Q}}{\mathrm{d} \mathbb{P}}
    \right).
\end{align}
Unlike KL divergence, which requires the expectation to be taken with respect to \(\mathbb{Q}\), log-variance allows the variance to be computed under a different measure \(\tilde{\mathbb{Q}}\). 
This flexibility suggests that we can detach the gradient of the trajectory or utilize a buffer to stabilize training.
On the other hand, when the variance is taken under  \(\mathbb{Q}\), the gradient of log-variance divergence w.r.t parameters in  \(\mathbb{Q}\) is the same as that of reverse KL divergence~\citep{richter2020vargrad}:
\begin{align}
    \frac{\mathrm{d}}{\mathrm{d} \mathbb{\theta}} \text{Var}_{\tilde{\mathbb{Q}}} \left(
  \log   \frac{\mathrm{d} \mathbb{Q}_{\theta}}{\mathrm{d} \mathbb{P}}
    \right)\Bigg|_{\tilde{\mathbb{Q}} = {\mathbb{Q}_{\theta}}} =    \frac{\mathrm{d}}{\mathrm{d} {\theta}}  D_\mathrm{KL}[\mathbb{Q}_{\theta}||\mathbb{P}].
\end{align}
However, we note that this conclusion holds only \emph{in expectation}.
In practice, when the objective is calculated with Monte Carlo estimators, they will exhibit different behavior.

\textbf{Trajectory balance.} Trajectory balance optimizes the squared log ratio
\begin{align}
    D_\mathrm{TB}[\mathbb{Q}||\mathbb{P}] = \E_{\tilde{\mathbb{Q}}} \left[
  \left(\log   \frac{\mathrm{d} \mathbb{Q}}{\mathrm{d} \mathbb{P}} - k\right)^2
    \right],
\end{align}
which is equivalent to the log-variance divergence with a learned baseline $k$.

\textbf{Sub-trajectory balance.} 
TB loss matches the entire $\mathbb{Q}$ and $\mathbb{P}$ as a whole. 
Alternatively, we can match segments of each trajectory individually to ensure consistency across the entire trajectory. This approach leads to the sub-trajectory balance objective. 
For simplicity, though it is possible to define sub-trajectory balance in continuous time, we define it with time discretization.
\par
With Euler–Maruyama discretization, we discretize $\mathbb{Q}$ and $\mathbb{P}$ into sequential produce of measure, with density given by:
\begin{align}
 p_0(X_0) \prod_{n=0}^{N-1} p_F(X_{n+1}|X_n) \quad\text{and}\quad   \ptilde(X_N) \prod_{t=0}^{N-1} p_B(X_n|X_{n+1}).
\end{align}
Note that the density for discretized $\mathbb{P}$ can be unnormalized.


Then, we introduce a sequence of intermediate densities \(\{\pi_n\}_{n=0}^{N}\), where the boundary conditions are given by \(\pi_0 = p_{\text{prior}}\) and \(\pi_N = \tilde{p}_{\text{target}}\). 
These intermediate distributions can either be prescribed as a fixed interpolation between the target and prior distributions or be learned adaptively through a parameterized neural network.

Finally, we define the sub-trajectory balance objective as
\begin{align}
      D_\mathrm{STB}[\mathbb{Q}||\mathbb{P}] = \E_{\tilde{\mathbb{Q}}} \left[ \sum_{0 \leq i<j\leq N}
  \left(\log   \frac{\pi_i(x_i) \prod_{n=i}^{j-1} p_F(x_{n+1}|x_n) }{\pi_j(x_j) \prod_{n'=i}^{j-1} p_B(x_{n'}|x_{n'+1})} + k_i - k_j\right)^2
    \right].
\end{align}

\textbf{Detailed balance.} 
Detailed balance can be viewed as an extreme case of sub-trajectory balance, where instead of summing over sub-trajectories of all lengths, we only calculate the sub-trajectory balance over each discretization step:

\begin{align}
      D_\mathrm{DB}[\mathbb{Q}||\mathbb{P}] = \E_{\tilde{\mathbb{Q}}} \left[ \sum_{0 \leq i\leq N-1}
  \left(\log   \frac{\pi_i(x_i) p_F(x_{i+1}|x_i) }{\pi_j(x_{i+1})  p_B(x_{i}|x_{i+1})} + k_i - k_{i+1}\right)^2
    \right].
\end{align}












\subsection{Marginal alignment objectives}
Unlike path measure alignment, \emph{marginal alignment} objectives directly enforce the sampling process at each time step $t$ to match with some marginal $\pi_t$.
$\pi_t$ can be either prescribed as an interpolation between the target and prior, with boundary conditions $\pi_0 = p_\text{prior}$ and $\pi_T = p_\text{target}$, or be learned through a network under the constraint of the boundary conditions. 
Commonly used objectives in this framework include PINN and action matching:

\textbf{PINN.} For the sampling process defined by $ dX_t =   \left(f_\theta(X_t, t) +\sigma_t^2  \nabla \log \pi_t (X_t)\right) dt + \sigma_t\sqrt{2} dW_t$, the PINN loss is given by
\begin{align}
    \mathcal{L}_{\text{PINN}} = \int_0^T \mathbb{E}_{\tilde{q}_t(X_t)}  ||
  \nabla \cdot f_\theta(X_t, t) + \nabla \log\pi_t(X_t) \cdot f_\theta(X_t, t) + (\partial_t \log\pi_t)(X_t) + \partial_t F(t) 
    ||^2 dt,
\end{align}
where $F(t) $ is parameterized by a neural network. 
Note that the expectation can be taken over an arbitrary $\tilde{q}_t$, as long as the marginal of $\mathbb{Q}$ at time $t$ is absolute continuous to $\tilde{q}_t$.
We also note PINN does not depend on the specific value of $\sigma_t$ in the sampling process.

\textbf{Action matching.} Similar to PINN, an action matching-based~\citep{neklyudov2023action} objective is derived by~\citep{albergo2024nets} for the PDE-constrained optimization problem
\begin{multline}
\mathcal{L}_{\text{AM}} =  \int_0^T  \mathbb{E}_{q_t(X_t)} \left[ \frac{1}{2} ||\nabla \phi_t(X_t)||^2 + \partial_t \phi_t(X_t)\right] dt \\+ \mathbb{E}_{p_{\text{prior}}(X_0)} \left[ \phi_0(X_0)\right] - \mathbb{E}_{p_{\text{target}}(X_T)} \left[ \phi_T(X_T) \right] ,
\end{multline}
where the vector field $b_t = \nabla \phi_t$, induced by a scalar potential, and $\phi_t$ is called the ``action''.


\section{Detailed Summary of Samplers}\label{appendix:review}
In this section, we provide a more detailed review of diffusion and control-based neural samplers.
We also discuss how these neural samplers rely on the Langevin preconditioning in the end.
\subsection{Sampling process and Objectives}
We write the sampling process as follows:
    \begin{align}\label{eq:x_appendix}
        d X_t = \left [\mu_t(X_t) + \sigma_t^2 b_t(X_t) \right] dt + \sigma_t \sqrt{2} dW_t, \quad X_0 \sim p_\text{prior},
    \end{align}

 \begin{enumerate}[label=({{\arabic*}}), leftmargin=*]
        \item Path Integral Sampler \citep[PIS,][]{zhangpath} and concurrently \citep[NSFS, ][]{vargas2021bayesian}: 
        PIS fixes $p_\text{prior} = \delta_0,  \sigma_t=1/\sqrt{2}$ and learns a network $f_\theta(\cdot) =\mu_t(\cdot) + \sigma_t^2 b_t(\cdot)$ so that \Cref{eq:x_appendix} approximate the time-reversal of the following SDE (Pinned Brownian Motion):
        \begin{align}\label{eq:pis_y_appendix}
             d Y_t = -\frac{Y_t}{T-t} dt + dW_t, \quad Y_0 \sim \ptarget.
        \end{align}
        We define \Cref{eq:pis_y_appendix} as the time-reversal of \Cref{eq:x_appendix} when $Y_t \sim X_{T-t}$.
        The network is learned by matching the reverse KL \citep{zhangpath,vargas2021bayesian} or log-variance divergence \citep{richterimproved} between the path measures of the sampling and the target process.
       
       \item Diffusion generative flow samplers~\citep[DFGS,][]{zhangdiffusion} learns to sample from  the same process as PIS,  but with a new introduction of local objectives including detailed balance and (sub-)trajectory balance.
        In fact, trajectory balance can been shown to be equivalent to the log-variance objective with a learned baseline rather than a Monte Carlo (MC) estimator for the first moment~\citep{nusken2021solving}.  
        \item Denoising Diffusion Sampler \citep[DDS,][]{vargasdenoising} and time-reversed Diffusion Sampler \citep[DIS,][]{berneroptimal}: 
        both DDS and DIS fix $\mu_t(X_t, t) = \beta_{T-t} X_t, \sigma_t = v \sqrt{\beta_{T-t}}, p_\text{prior} = \mathcal{N}(0, v^2I)$, and learn a network $f_\theta(\cdot, t) = b_t(\cdot, t)/2 $ so that \Cref{eq:x_appendix} approximates the time-reversal of the VP-SDE:
        \begin{align}\label{eq:dds_y_appendix}
            dY_t = -\beta_t Y_t dt + v \sqrt{2\beta_{t}} dW_t, \quad  Y_0 \sim \ptarget.
        \end{align}
        Similar to PIS, the network can be trained either with reverse KL divergence or log-variance divergence.
        In an optimal solution, $f_\theta$ will approximate the score $f_\theta(\cdot, t) \approx \nabla\log p_{T-t}(\cdot)$, where $ p_{t}(X) = \int  \mathcal{N}(X|\sqrt{1-\lambda_t}Y, v^2\lambda_t I) \ptarget(Y)dY$ and $\lambda_t = 1-\exp(-2\int_0^t  \beta_s ds)$.
        
       \item Iterated Denoising Energy Matching \citep[iDEM,][]{akhounditerated}: iDEM fixes $\mu_t(X_t, t) = 0, p_\text{prior} = \mathcal{N}(0, T^2 I)$, and learns a network $f_\theta(\cdot, t) = b_t(\cdot, t) / 2$ to approximate the score  $f_\theta(X_t, t) \approx \nabla \log p_{T-t} (X_t)$.
        This is achieved by writing the score with target score identity \citep[TSI, ][]{de2024target}, and estimating it with a self-normalized importance sampler:
        \begin{align}
            \nabla \log p_{T-t} (X_t) &\stackrel{\text{TSI}}{=} \int p_{T|T}(X_T | X_t)  \nabla \log \ptilde (X_T) dX_T\\
            &\stackrel{\text{Bayes' Rule}}{=}\int \frac{\ptilde (X_T)p_{t|T}(X_t|X_T)}{\int \ptilde (X_T)p_{t|T}(X_t|X_T) dX_T}  \nabla \log \ptilde (X_T) dX_T\\
            &=\int q_{T|t}(X_T | X_t)\frac{\ptilde (X_T)p_{t|T}(X_t|X_T) \nabla \log \ptilde (X_T) }{q_{T|t}(X_T | X_t) \int q_{T|t}(X_T | X_t) \frac{\ptilde (X_T)p_{t|T}(X_t|X_T) }{q_{T|t}(X_T | X_t)}dX_T} dX_T.
        \end{align}
        By choosing $q_{T|t}(X_T | X_t)\propto p_{t|T}(X_t|X_T)$, we obtain
        \begin{align}
            \nabla \log p_{T-t} (X_t) &=\int q(X_T | X_t)\frac{\ptilde (X_T)\nabla \log \ptilde (X_T)}{ \int q(X_T | X_t) \ptilde (X_T)dX_T}   dX_T\\
            &\approx\sum_{n} \frac{\ptilde (X_T^{(n)})}{ \sum_{n}  \ptilde (X_T^{(n)})}\nabla \log \ptilde (X_T^{(n)}), \quad X_T^{(n)}\sim  q_{T|t}(X_T | X_t)\\
            & =: \widehat{\nabla \log p_{T-t} (X_t)}.
        \end{align}
        Then, iDEM matches $f_\theta(X_t, t)$ with $\widehat{\nabla \log p_{T-t} (X_t)}$ by $L^2$ loss.
        In optimal, the sampling process will approximate the time-reversal of a VE-SDE:
         \begin{align}\label{eq:idem_y_appendix}
            dY_t =\sqrt{2t} dW_t, \quad  Y_0 \sim p_\text{target}.
        \end{align}
Several extensions have been developed based on iDEM:
Bootstrapped Noised Energy Matching \citep[BNEM, ][]{ouyang2024bnemboltzmannsamplerbased} generalizes the self-normalized importance sampling estimator of the score to the energy function, enabling the training of energy-parameterized diffusion models.
They also proposed a bootstrapping approach to reduce the training variance.
Diffusive KL \citep[DiKL, ][]{he2024training} integrates this estimator with variational score distillation techniques \citep{poole2022dreamfusion, luo2024diff} to train a one-step generator as the neural sampler.
Also, DiKL proposes using MCMC to draw samples from $p_{T|t}(X_T|X_t)$ to estimate the score,  instead of relying on the self-normalized importance sampling estimator with the proposal $q_{T|t}(X_T | X_t)\propto p_{t|T}(X_t|X_T)$, leading to lower variance during training.
\par
~ We also note that iDEM's score estimator is closely related to stochastic control problems.
   One can re-express the estimator regressed in iDEM in terms of the optimal drift of a stochastic control problem~\citep{huang2021schr}, the optimal control $f^*$ can be expressed in terms of the score (e.g. See Remark 3.5 in \cite{reusmooth}) :
        \begin{align}\label{eq:optimal_drift}
             f^*_{t}(X_t) = - \nabla \log \phi_{T-t}(X_t) = - \nabla \ln \nu_{T-t}^{\mathrm{ref}}(X_T) +\nabla \log {p_{T-t}(X_t)},
        \end{align} where $\phi_t(X_t) $ is the 
     value function, which can be expressed as a conditional expectation via the Feynman-Kac formula followed by the Hopf-Cole transform \citep{hopf1950partial,cole1951quasi,fleming1989logarithmic}:
\begin{align}\label{eq:opt_val_func_appendix}
\phi_t(x) = \mathbb{E}_{X_T \sim q_{T|t}(X_T|x)}\left [\frac{\ptilde}{\nu^{\mathrm{ref}}_T}(X_T)\right]. 
\end{align}
Where in the case for VE-SDE (i.e. iDEM) and 
$\nu^{\mathrm{ref}}_t(x) = \mathcal{N}(x| 0, t+ \sigma_{\mathrm{prior}}^2)$ and thus $\phi_{T-t}(X_t) = \frac{X_t}{T-t +\sigma_{\mathrm{init}}^2} +\nabla \log {p_{T-t}(X_t)}$.

Note the MC Estimator of $\nabla \log \phi_{T-t}(X_t)$ (e.g. Equation \ref{eq:opt_val_func}) was used in Schr\"odinger-F\"ollmer Sampler \citep[SFS, ][]{huang2021schr} to sample from time-reversal of pinned Brownian Motion, yielding an akin estimator to the one used in iDEM, in particular they carry out an MC estimator of the following quantity:

\begin{align}
   \nabla \phi_{T-t}(x) = \frac{\mathbb{E}_{Z \sim \mathcal{N}(0,I)}\left [\nabla\frac{\ptilde}{\nu^{\mathrm{ref}}_T}( \mu_{T|T-t}x + \sigma_{T|T-t}Z)\right]}{\mathbb{E}_{Z \sim \mathcal{N}(0,I)}\left [\frac{\ptilde}{\nu^{\mathrm{ref}}_T}( \mu_{T|T-t}x + \sigma_{T|T-t}Z)\right]}.
\end{align}

Where, we have assumed that $q_{T|t}(x_T|x_t) =\mathcal{N}(x_T| \mu_{T|t}x_t, \sigma_{T|t})$ as is the case with most time reversal based samplers and generative models.





        \item Monte Carlo Diffusion \citep[MCD,][]{doucet2022score}: unlike other neural samplers, MCD's sampling process is fixed as $\mu_t = 0, \sigma_t = 1, b_t(X_t, t) = \nabla \log \pi_t(X_t)$, where $\pi_t$ is the geometric interpolation between target and prior, i.e., $\pi_t(X_t) = p_\text{target}^{\beta_t}(X_t)p_\text{prior}^{1-\beta_t}(X_t)$.
    It can be viewed as sampling with AIS using ULA as the kernel.
    Note, that this transport is non-equilibrium, as the density of $X_t$ is not necessary $\pi_t(X_t)$.
    Therefore, MCD trains a network to approximate the time-reversal of the forward process and perform importance sampling (more precisely, AIS) to correct the bias of the non-equilibrium forward process.
        \item Controlled Monte Carlo Diffusion \citep[CMCD,][]{vargas2024transport} and Non-Equilibrium Transport Sampler \citep[NETS,][]{mate2023learning,albergo2024nets}: 
        Similar to MCD, CMCD and NETS also set $b_t(X_t, t) = \nabla \log \pi_t(X_t)$ and $\pi_t$ is the interpolation between target and prior.
Different from MCD where the sampling process is fixed, CMCD and NETS learn $f_\theta(\cdot, t) = \mu_t(\cdot, t)$ so that the marginal density of samples $X_t$ simulated by \Cref{eq:x} will approximate $\pi_t$.
As a special case, Liouville Flow Importance Sampler \citep[LFIS,][]{tian2024liouville} fixes $\sigma_t=0$ and learns an ODE to transport between $\pi_t$.

    
\end{enumerate}

\subsection{Lanvegin Preconditioning in Diffusion/Control-based Neural Samplers}
\label{appendix:langevin_precond}

\textbf{Explicit Langevin preconditioning. }
In samplers including PIS, DDS, DFGS, DIS, etc., 
The network is parameterized with a skip connection using Lanvegin preconditioning:
\begin{align}
  f_\theta(\cdot, t) = \text{NN}_{1,\theta}(\cdot, t) + \text{NN}_{2, \theta}(t) \circ  \nabla\log \ptarget(\cdot).
\end{align}
In samplers like MCD, the forward process is a sequence of Lanvegin dynamics with invariant density $\pi_t$ as the interpolation between prior and target.
In CMCD and NETS, the drift of forward process is given by the network output plus a score term: 
\begin{align}
    f_\theta(X_t, t) +\sigma_t^2  \nabla \log \pi_t (X_t).
\end{align}
\textbf{Implicit Langevin preconditioning. } In iDEM, we regress the network with 
\begin{align}
     \nabla \log p_{T-t} (X_t) 
            &\approx\sum_{n} \frac{\ptilde (X_T^{(n)})}{ \sum_{n}  \ptilde (X_T^{(n)})}\nabla \log \ptilde (X_T^{(n)}), \quad X_T^{(n)}\sim  q_{T|t}(X_T | X_t).
\end{align}
Note that while the network does not explicitly depend on the score of the target density, the objective compels it to learn gradient information.
This gradient information is utilized during simulation when collecting the buffer every few iterations, effectively inducing an implicit Langevin preconditioning.

\textbf{No Langevin preconditioning. } 
LFIS does not rely on Langevin preconditioning during simulation.
Like NETS, it employs the PINN loss, but its sampling process is governed by an ODE.
Thus, similar to our discussion in the main text on eliminating Langevin preconditioning for PINN-based CMCD, LFIS inherently removes this dependency in its design.

LFIS adopts several tricks to stabilize the training and ensure mode covering:
it learns the ODE drift progressively, starting from the prior and gradually transitioning to the target.
Additionally, it employs separate networks for different time steps to prevent forgetting.
But even without these tricks,  our results in \Cref{tab:pinn} confirm the robustness of the PINN loss to Langevin preconditioning when the interpolation and prior are carefully tuned.





\section{NF-DDS}\label{appendix:nf-dds}

Here we present a derivation of the NF-DDS objective.
\begin{align}
   \nonumber & D_\text{KL}[\mathbb{Q}||\mathbb{P}]\\  \nonumber = &\E_{\mathbb{Q}} \left[
    \int_0^T \frac{1}{4v^2\beta_t}\|
   \tilde{F}_\theta(Y_t, T-t) - v^2\beta_{t} \nabla\log q_\theta(Y_t, T-t) - \beta_t Y_t
    \|^2 dt
    \right] + D_\text{KL}[q_\theta(\cdot, T)|| p_\text{target}]\\ \nonumber 
   = & 
    \int_0^T \E_{\mathbb{Q}} \left[\frac{1}{4v^2\beta_t}\|
   \tilde{F}_\theta(Y_t, T-t) - v^2\beta_{t} \nabla\log q_\theta(Y_t, T-t) - \beta_t Y_t
    \|^2\right] dt+ D_\text{KL}[q_\theta(\cdot, T)|| p_\text{target}]\\ \nonumber 
   = & 
    \int_0^T \frac{1}{4v^2\beta_t}\E_{{q_\theta(Y, T-t)}}\|
   \tilde{F}_\theta(Y, T-t) - v^2\beta_{t} \nabla\log q_\theta(Y, T-t) - \beta_t Y
    \|^2 dt+ D_\text{KL}[q_\theta(\cdot, T)|| p_\text{target}]\\
  = &
    \int_0^T \frac{1}{4v^2\beta_{T-t}} \E_{{q_\theta(Y, t)}} \|
   \tilde{F}_\theta(Y, t) - v^2\beta_{T-t} \nabla\log q_\theta(Y, t) - \beta_{T-t} Y
    \|^2 dt+ D_\text{KL}[q_\theta(\cdot, T)|| p_\text{target}].\label{eq:flowDDS-obj_appendix}
\end{align}

\section{NF-CMCD}\label{appendix:nf-cmcd}
In this section, we proposed a CMCD variation with normalizing flow for simulation-free training.
\par
In CMCD, we match the forward sampling process:
\begin{align}\label{eq:nf-sde-cmcd}
        dX_t = \left(\tilde{F}_\theta(X_t, t) + \sigma_t^2 \nabla\log q_\theta(X_t, t) \right)dt + \sigma_t\sqrt{2} dW_t, X_0 \sim q_\theta(X_0, 0),
    \end{align}
 with a target backward process, calculated by Nelson's condition assuming the marginal of the SDE at each time step matches with a prescribed marginal density  e.g. $\pi_t(\cdot) = p_\text{target}^{\beta_t}(\cdot)p_\text{prior}^{1-\beta_t}(\cdot)$:
 \begin{multline}\label{eq:nf-sde-cmcd-target}
        dY_t = -\Big(\tilde{F}_\theta(Y_t, T-t) + \sigma_{T-t}^2 \nabla\log q_\theta(Y_t, T-t)\\- 2 \sigma_{T-t}^2 \nabla\log \pi_{T-t}(Y_t, T-t) \Big)dt + \sigma_{T-t}\sqrt{2} dW_t, Y_0 \sim p_\text{target}.
 \end{multline}
Again, similar to NF-DDS, the time-reversal of \Cref{eq:nf-sde-cmcd} can be calculated by Nelson's condition:
\begin{align}\label{eq:nf-sde-cmcd-backward}
     dY_t = -\left(\tilde{F}_\theta(Y_t, T-t) - \sigma_{T-t}^2 \nabla\log q_\theta(Y_t, T-t) \right)dt + \sigma_{T-t}\sqrt{2} dW_t, Y_0 \sim q_\theta(Y_0, T).
\end{align}
By Girsanov theorem, the KL divergence between the path measure by \Cref{eq:nf-sde-cmcd-backward} (denoted as $\mathbb{Q}$) and \Cref{eq:nf-sde-cmcd-target} (as $\mathbb{P}$) is:
\begin{align}
  D_\text{KL}[\mathbb{Q}||\mathbb{P}] = 
    \int_0^T \frac{1}{\sigma_{t}} \E_{{q_\theta(Y, t)}} \| \sigma_{t}^2 \nabla\log q_\theta(Y, t) - \sigma_{t}^2 \nabla\log \pi_{t}(Y, t) 
    \|^2 dt+ D_\text{KL}[q_\theta(\cdot, T)|| p_\text{target}].
\end{align}
This coincides with the Fisher divergence between each marginal.

\par
After training, we can sample from 
\begin{align}\label{eq:nf-sde-cmcd_forward}
        dX_t = \left(\tilde{F}_\theta(X_t, t) + \sigma_t^2 \nabla\log \pi_t(X_t) \right)dt + \sigma_t\sqrt{2} dW_t, X_0 \sim q_\theta(X_0, 0),
\end{align}
with approximate reversal
\begin{align}\label{eq:nf-sde-cmcd-backward_sample}
     dY_t = -\left(\tilde{F}_\theta(Y_t, T-t) - \sigma_{T-t}^2 \nabla\log \pi_{T-1}(Y_t) \right)dt + \sigma_{T-t}\sqrt{2} dW_t, Y_0 \sim \ptilde.
\end{align}


\section{Langevin Preconditioning as an Approximation to the Optimal Control}
\label{appendix:langevin_approx}

As we discussed for iDEM (similarly for PIS and DDS), the optimal drift can be expressed as the gradient of the value function in \Cref{appendix:langevin_precond}
$$f^*_{t}(X_t) = - \nabla \log \phi_{T-t}(X_t)$$ 
where the value function itself can be expressed as
\begin{align}
   \log \phi_{t}(x) = \log \mathbb{E}_{X_T \sim q_{T|t}(X_T|x)}\left [\frac{\ptilde}{\nu^{\mathrm{ref}}_T}(X_T)\right].
\end{align}
where we define the reference process to be the uncontrolled process starting from $p_\text{prior}$, $\nu^{\mathrm{ref}}_T$ is the marginal density for the last time step of the reference process, and $q_{T|t}(X_T|x)$ is the conditional density for the reference process.
Specifically, for PIS, the reference is 
\begin{align}
    d X_t = dW_t, X_0 = 0;
\end{align}
and for DDS, the reference is 
\begin{align}
     dX_t = -\beta_{T-t} X_t dt + v \sqrt{2\beta_{T-t}} dW_t, \quad  X_0 \sim p_\text{prior} = \mathcal{N}(0, v^2).
\end{align}
We hence have
\begin{align}
   & \nu^{\mathrm{ref}}_T = \mathcal{N}(0, TI) \text{ for PIS}, \quad  \nu^{\mathrm{ref}}_T = \mathcal{N}(0, v^2I)\text{ for DDS}\\
    &  \mathbb{E}[X_T |X_t=x]  = x \text{ for PIS}, \quad  \mathbb{E}[X_T |X_t=x] = \sqrt{1-\lambda_t} \text{ for DDS}
\end{align}
Now consider approximating the above expectation with a point estimate using the posterior mean (e.g. reversing Jensens Inequality):
\begin{align}
    \log \phi_{t}(x) &\approx   \log  \frac{\ptilde}{\nu^{\mathrm{ref}}_T}(\mathbb{E}[X_T |X_t=x]) \\
    &\approx  \log \ptilde (\mathbb{E}[X_T |X_t=x]) -\log \nu^{\mathrm{ref}}_T (\mathbb{E}[X_T |X_t=x])
\end{align}
Taking gradients on both sides:
\begin{align}
    \nabla_x     \log \phi_{t}(x) &\approx \nabla_x   \log \ptilde (\mathbb{E}[X_T |X_t=x]) - \nabla_x    \log\nu^{\mathrm{ref}}_T (\mathbb{E}[X_T |X_t=x])
\end{align}

where for PIS this reduces to :
\begin{align}
    \nabla_x  \log \phi_{t}(x) &\approx \nabla_x\log \ptilde (x) + \frac{x}{T}
\end{align}
and for DDS
\begin{align}
    \nabla_x  \log \phi_{t}(x) &\approx \nabla_x\log \ptilde (\sqrt{1-\lambda_t} x) + \frac{\sqrt{1-\lambda_t}x}{v^2}
\end{align}

In both cases we can see the general form  $ \nabla_x  \log  \phi_{t}(x) =\nabla_x \log  \ptilde (a_t x) + c_t x$ for some time varying coefficients $a_t,b_t$, and thus it seems like a reasonable inductive bias to employ the Langevin preconditioning,
\begin{align}
  f_\theta(\cdot, t) = \text{NN}_{1,\theta}(\cdot, t) + \text{NN}_{2, \theta}(t) \circ ( \nabla\log \ptarget(a_t\cdot) + b_t \cdot),
\end{align}
or more simply $f_\theta(\cdot, t) = \text{NN}_{1,\theta}(\cdot, t) + \text{NN}_{2, \theta}(t) \circ  \nabla\log \ptarget(\cdot)$ as typically done in many SDE based neural samplers.





\section{Additional Experiment Details}




\subsection{Evaluation Metrics}
In this paper, we evaluate the samples quality by ELBO, EUBO and MMD. 
The ELBO (Evidence Lower Bound) is a lower bound of the (log) normalization factor, reflecting how well the model is concentrated within each mode;
on the other hand, EUBO \citep[Evidence Upper Bound, ][]{blessingbeyond}
provides an upper bound, representing if the model successfully covers all modes.
\par
The MMD (Maximum Mean Discrepancy) measures the distributional discrepancy between the generated samples and the target distribution.
We base our MMD implementation on the code by \citet{chen2024diffusivegibbssampling} at \url{https://github.com/Wenlin-Chen/DiGS/blob/master/mmd.py}, using 10 kernels and fixing the \texttt{sigma = 100}.
\par
For all experiments, we evaluate ELBO, EUBO and MMD with 10000 samples.



\subsection{Hyperparameters}

\begin{table}[H]
\centering
   \captionof{table}{Hyperparameters used for experiments.\vspace{-6pt}}\label{tab:hyperparam}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    method & objective & prior & lr & precond & network size \\ \midrule
    \multirow{6}{*}{DDS} & rKL & $\mathcal{N}(0, 30^2I)$ &  5e-4 & LG & [64, 64] \\
    & LV & $\mathcal{N}(0, 30^2I)$ &  5e-4  & LG & [64, 64] \\
    & TB & $\mathcal{N}(0, 30^2I)$ &   5e-4 & LG & [64, 64]\\
   & rKL & $\mathcal{N}(0, 30^2I)$ &  5e-4 & - / $\log p_\text{target}$& [256, 256, 256, 256, 256] \\
    & LV & $\mathcal{N}(0, 30^2I)$ &  5e-4  & - / $\log p_\text{target}$& [256, 256, 256, 256, 256] \\
    & TB & $\mathcal{N}(0, 30^2I)$ &   5e-4 & - / $\log p_\text{target}$  & [256, 256, 256, 256, 256]\\\midrule
    \multirow{4}{*}{CMCD} & rKL & $\mathcal{N}(0, 30^2I)$ & 5e-4 & LG & [64, 64] \\
    & LV & $\mathcal{N}(0, 30^2I)$ & 5e-3 & LG & [64, 64]\\ 
    & TB & $\mathcal{N}(0, 30^2I)$ & 5e-4 & LG & [64, 64] \\ 
    & rKL & $\mathcal{N}(0, 30^2I)$ & 5e-4 & - & [256, 256, 256, 256, 256]  \\
    \bottomrule
    \end{tabular}
\end{table}

We summarize the hyperparameters for DDS and CMCD in \Cref{tab:hyperparam}.
These hyperparameters are chosen according to \citet{blessingbeyond}.
For PINN-based experiments shown in \Cref{tab:pinn}, we follow the hyperparameter used in NETS \citep{albergo2024nets}, including network size, learning rate and its schedule. etc.



\section{Additional Experimental Results}\label{appendix:visualize}
\newpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth, trim={100, 0, 150, 0}, clip]{figures/dds_vis.pdf}
    \caption{Sampled obtained by DDS with different settings.
    The first line shows the initialization. }
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth, trim={100, 0, 150, 0}, clip]{figures/cmcd_vis.pdf}
    \caption{Sampled obtained by CMCD with different settings.
    The first line shows the initialization and N/A indicates diverging. 
    We can see when trained with Langevin preconditioning, we can see that CMCD already captures modes after initialization.}
\end{figure}


\end{document}
