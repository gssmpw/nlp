\section{Discussions and Conclusions}
Motivated by the pursuit of simulation-free training, we reviewed neural samplers from the perspective of sampling processes and training objectives, as well as revisiting their dependence on Langevin preconditioning.
Our findings reveal that most training methods for diffusion and control-based neural samplers heavily rely on Langevin preconditioning.
While PINN appears to be an exception, it still requires evaluating both the target density and the modelâ€™s divergence at every time step along the trajectory, making it no more efficient in practice.
This highlighted critical caveats in scaling neural samplers to high-dimensional and real-world problems.
In fact, while significant advances have been made in learning neural samplers directly from unnormalized densities, the most efficient and practical approach remains running MCMC first and fitting a generative model post hoc.
\par
Our results reveal several open questions and future directions worth exploring.
First, talking about neural samplers, many works focus on learning models directly from the unnormalized density, avoiding the use of any data from the target density. 
However, given that the Langevin preconditioning plays a crucial role in most approaches, we may equivalently interpret the training process as running several steps of MCMC to obtain approximate samples.
This interpretation, blurring the distinction between data-driven and data-free approaches,  challenges the definition of these ``data-free" neural samplers.
 Furthermore, as our results demonstrate, a straightforward two-step approach---first running Parallel Tempering (PT) to obtain samples, followed by fitting a diffusion model---yields significantly higher efficiency compared to nearly all neural samplers.
This observation further questions the practical justification and motivation of ``data-free" neural samplers.
Therefore, rather than attempting to completely avoid the use of data, \textbf{a more promising and practical direction may involve developing objective functions or training pipelines that rely on a limited amount of data for a more efficient acquisition of information from each target density evaluation.}
\par
However, we emphasize that while we advocate for the explicit utilization of data, we acknowledge that \textbf{it may not always be feasible, or even reasonable, for newly developed approaches to surpass these well-established baselines from the outset.}
The methods developed within ``data-free" training pipelines remain valuable and can provide inspiration for approaches that more effectively leverage data, potentially leading to improved efficiency and performance in neural samplers.
\par
Based on our observations, PINN loss appears to be an example with such potential. 
It demonstrates greater robustness in the absence of Langevin preconditioning and naturally supports simulation-free training by its design. 
However, it still requires extensive target evaluations along the entire trajectory and tends to be more sensitive to hyperparameters, making it challenging to apply as an off-the-shelf method.
Therefore, \textbf{future research could focus on learning better priors or interpolations.} 
A straightforward approach may involve first obtaining approximate samples from the target distribution using methods such as MCMC, then learning priors or interpolants from these samples, and using the learned hyperparameters to refine the sample quality in an iterative manner. 





\section*{Acknowledgment}
We thank Julius Berner, Brian Lee, Ezra Erives and Bowen Jing for helpful suggestions regarding implementations of several neural samplers and objective functions. We thank Michael Albergo for aiding us in running the NETS and PINN based experiments. 
JH and SP acknowledge support from the University of Cambridge Harding Distinguished Postgraduate Scholars Programme.  
JMHL and RKOY acknowledge support from a Turing AI Fellowship under grant EP/V023756/1. 
CPG and YD acknowledge the support of Schmidt Sciences programs, an AI2050 Senior Fellowship and Eric and Wendy Schmidt AI in Science Postdoctoral Fellowships; the National Science Foundation (NSF); the National Institute of Food and Agriculture (USDA/NIFA); the Air Force Office of Scientific Research (AFOSR).
