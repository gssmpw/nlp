
\section{Ablation on Langevin Preconditioning and Its Implications}



In this section, we ablate the effectiveness of the Langevin preconditioning on examples of different neural samplers.  
For the time-reversal sampler, we take DDS as an example, while for the escorted transport sampler, we take CMCD as an example.
We will explore objectives including reverse KL, Log-var divergence, trajectory balance, and PINN.

First, we discuss {how to remove Langevin preconditioning in different samplers}:
\begin{itemize}[leftmargin=*]
    \item \emph{DDS without Langevin Preconditioning}. DDS's Langevin preconditioning occurs in its network parameterization. 
Therefore, to eliminate the help of Langevin during simulation in the training process, we can simply replace the network in \Cref{eq:DDS_network} by a standard MLP. 
To ensure the model capacity, we increase the MLP size to 5 layers with 256 hidden units.
\item \emph{CMCD without Langevin Preconditioning}. Unlike DDS, Langevin preconditioning in CMCD naturally emerges from its formulation.
Specifically, CMCD defines the drift terms for the sampling and ``target" processes as $
    f_\theta(X_t, t) +\sigma_t^2  \nabla \log \pi_t (X_t)$ and $-(f_\theta(Y_{t}, T-t) - \sigma_t^2 \nabla \log \pi_{T-t} (Y_{t}))$ respectively. By aligning their path measures, the marginal density of the sampling process at time 
$t$ is ensured to match 
$\pi_t$ in accordance with Nelson's condition \citep{nelson}.
In order to eliminate the Langevin preconditioning $\nabla \log \pi_t (X_t)$ during simulation in training, we redefine the sampling and "target" processes as $
    f_\theta(X_t, t)$ and $-(f_\theta(Y_{t}, T-t) - 2\sigma_t^2  \nabla \log \pi_{T-t} (Y_{t}))$.
Aligning their path measures still ensures that the marginal density of the sampling process at time
$t$ matches
$\pi_t$, while the training simulation does not rely on the help of Langevin preconditioning.
\item \emph{PINN without Langevin Preconditioning}. In CMCD/NETS, sampling process is defined as $
  dX_t =   \left(f_\theta(X_t, t) +\sigma_t^2  \nabla \log \pi_t (X_t)\right) dt + \sigma_t\sqrt{2} dW_t$, and the objective is independent of the value of $\sigma_t$.
Therefore, we simply set $\sigma_t = 0$ during training to eliminate the Langevin preconditioning. 
\end{itemize}
 Additionally, we investigate the performance of DDS and CMCD when the initialization is close to optimal.
To achieve this, we first train DDS and CMCD with Langevin preconditioning until convergence. 
Then, we use a new network without Langevin preconditioning to distill the teacher output with Langevin preconditioning at each time step using an $L_2$ loss.
After distillation, we fine-tune the student network using different objectives. 
This allows us to examine whether Langevin preconditioning primarily aids in localizing the model in the early training stage or also contributes to stabilizing the results in the end of training.

For DDS, we also test the results using a network conditioned on the target density instead of the target score: $f_\theta(X, t) = \text{NN}_{\theta}(X, \log \tilde{p}_\text{target}(X), t)$. 
This allows us to verify whether neural samplers require an explicit score term to ensure that the simulation behaves similarly to running Langevin dynamics, or if they only need some information about the target density.





\begin{table}[t]
\centering
\caption{Sample quality of time-reversal sampler and escorted transport sampler trained with different objectives.
We compare their performances both with and without the Langevin preconditioning. 
We measure MMD, EUBO and ELBO.
MMD can have a comprehensive reflection on the sample quality, and the difference between EUBO and ELBO measures the mode coverage: large EUBO indicates mode collapsing. 
As some methods diverge in the end, we report the results with early stopping, according to ELBO.  N/A denotes unstable training, and no reasonable result is obtained.
}
\label{tab:main_result}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Obj.}} & \multicolumn{5}{c}{\textbf{DDS}} & \multicolumn{4}{c}{\textbf{CMCD}} \\
\cmidrule(lr){2-6}\cmidrule(lr){7-10}  
 & \multicolumn{1}{c}{\small w. LG} & \multicolumn{1}{c}{\small w/o LG} &  
 \multicolumn{1}{c}{\small w. $\log p_\text{target}$} & \multicolumn{1}{c}{\small distil init.}  & \multicolumn{1}{c}{\small w/o LG + init.}  & \multicolumn{1}{c}{\small w. LG} & \multicolumn{1}{c}{\small w/o LG} & \multicolumn{1}{c}{\small distil init.} & \multicolumn{1}{c}{\small w/o LG + init.} \\ \midrule
\multicolumn{10}{c}{MMD ($\downarrow$)} \\ \midrule \small
rKL & 0.074 & 1.497 & 4.260 & \multirow{3}{*}{0.121} & 0.333 & 0.075 & 4.011 &   & 1.827\\
LV & 0.064 & 1.938 & 1.995 & &  0.014 &  0.017  & N/A & 0.079 & 0.036 \\
TB & 0.054 & 4.413 & 4.550 & &  0.015 &  0.035 & N/A &  & 0.130  \\
\midrule 
\multicolumn{10}{c}{\normalsize ELBO ($\uparrow$) /EUBO ($\downarrow$)} \\ \midrule \small
rKL & -0.45/0.49 & -1.93/28.52 &  -2.36/35.02 & \multirow{3}{*}{-0.88/0.64} & 
 -1.14/3.03 & -0.40/0.45  & -4.45/193.06 &  & -3.28/3$\times 10^5$  \\
LV & -0.90/0.77 & -2.07/16.26 & -1.96/17.19 & &  -0.53/0.44 & -0.28/0.33   & N/A  & -0.89/0.82  & -0.53/0.77 \\
TB & -1.73/1.36 & -2.62/23.00 & -2.61/28.75 & &  -0.46/0.45 &  -0.52/0.77   & N/A  &   & -0.77/1.20 \\
\bottomrule\normalsize
\end{tabular}%
}\vspace{-10pt}
\end{table}












\begin{figure}[t]
    \centering
    \begin{minipage}{0.43\textwidth}
        \centering
           \captionof{table}{Sample quality (MMD) by NETS trained with PINN loss \citep[][Alg 1]{albergo2024nets}, both with and without LG in the simulation process during training. As NETS used a different prior and interpolation ($\mathcal{N}(0, 2I)$, mode interpolation) compared to CMCD ($\mathcal{N}(0, 30^2I)$, geometric interpolation), we present the results by both settings for a fair investigation. N/A suggests diverging.\vspace{-6pt}}\label{tab:pinn}
            \begin{adjustbox}{width=\linewidth}
            \begin{tabular}{@{}lccc@{}}
            \toprule
            interpolant & prior & train w. LG & train w/o LG \\ \midrule
            \multirow{2}{*}{geom} & $\mathcal{N}(0, 2I)$ & 6.9529 & 7.0091 \\
            & $\mathcal{N}(0, 30^2I)$ & 0.3368 & 0.1721 \\
            \multirow{2}{*}{mode} & $\mathcal{N}(0, 2I)$ & 0.0034  & 0.0040 \\
            & $\mathcal{N}(0, 30^2I)$ & N/A & N/A  \\ \bottomrule
            \end{tabular}
            \end{adjustbox}\vspace{0pt}
    \end{minipage}\hfill
    \begin{minipage}{0.55\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/call_mmd.png}
        \caption{Sample quality vs target evaluation times for different approaches with different objectives on GMM-40 target.  *NETS uses mode interpolation, which is distinct from that employed in others. }\label{fig:energy_call_mmd}\vspace{0pt}
    \end{minipage}\\
     \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=\linewidth, trim={80 150 100 100}, clip]{figures/pinn_visual.pdf}
        \caption{Sampled obtained by PINN with different settings.
   We can see PINN seems to be highly robust to Langevin preconditioning. 
   However, it is highly sensitive to the prior and interpolation. }\label{fig:pinn_sample}\vspace{0pt}
    \end{minipage}%
\end{figure}

We present results for DDS and CMCD using reverse KL (rKL), log-variance divergence (LV), and trajectory balance (TB) on a 40-mixture Gaussian target proposed by \citet{midgleyflow} in \Cref{tab:main_result}, with more visualization in \Cref{appendix:visualize}.
Our findings reveal the following key observations:
\begin{itemize}[leftmargin=*]
    \item \textbf{Most objectives significantly collapse without Langevin preconditioning.}
We note that, at initialization, the samples from the neural samplers already cover all modes, meaning there is no inherent exploration issue. 
However, even with this favorable initialization, the absence of Langevin preconditioning leads to severe collapse in most objectives.
\item \textbf{Langevin preconditioning cannot be replaced by alternative target information, such as} $\log p_\text{target}$.
This suggests that neural samplers require an explicit score term to ensure that the simulation behaves similarly to Langevin dynamics.
\item \textbf{If the initialization is close to optimal, TB and LV refine the solution more stably, while rKL remains prone to mode collapse.}
This suggests that future work could explore a training pipeline where the sampler is first warmed up using Langevin dynamics, followed by fine-tuning with these objectives to reduce the number of target energy evaluations during sampling.
\end{itemize}
We also include results obtained by NETS with the PINN loss in \Cref{tab:pinn}  with more visualization in \Cref{fig:pinn_sample}.
Since NETS employs a different prior and interpolation scheme compared to CMCD in \Cref{tab:main_result}, we present results for both settings to ensure a fair comparison.
Surprisingly, we observe that \textbf{the PINN loss is relatively robust to Langevin preconditioning during simulation}.
Additionally, by design, the PINN loss naturally supports simulation-free training.
However, its performance is highly sensitive to the choice of prior and interpolation:
a large prior leads to diverging in mode interpolation, while a smaller one also fails under geometric interpolation.
Furthermore, the PINN loss requires computing an expensive divergence term, making it challenging to apply to simulation-free approaches with normalizing flows proposed in \Cref{sec:sim_free}.


Finally, the critical role of Langevin preconditioning naturally raises an important question:
Is simulation during training with Langevin preconditioning more efficient than directly generating data with Langevin dynamics and fitting a model post hoc?
Unfortunately, the answer is no.
In \Cref{fig:energy_call_mmd}, we compare several neural samplers against an alternative approach where Parallel Tempering (PT) \citep[PT, ][]{PhysRevLett.57.2607,earl2005parallel} is first used to generate samples, followed by fitting a diffusion model. 
We assess both sample quality and the number of target energy evaluations required.
The results clearly show that \textbf{almost all neural samplers require several orders of magnitude more target evaluations compared to PT.}
