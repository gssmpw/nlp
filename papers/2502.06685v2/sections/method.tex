\vspace{-5pt}
\section{Review of diffusion and controlled samplers}\label{sec:review}
\vspace{-5pt}
Before discussing the potential design of a simulation-free training approach, we first present a systematic review of current diffusion and controlled-based samplers in this section.
Despite the abundance of existing approaches, most samplers can be broadly categorized based on their sampling processes and training objectives:\vspace{-3pt}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Sampling processes}. We can write the sampling process as follows:
    \begin{align}\label{eq:x}
        d X_t = \left [\mu_t(X_t) + \sigma_t^2 b_t(X_t) \right] dt + \sigma_t \sqrt{2} dW_t, \quad X_0 \sim p_\text{prior},
    \end{align}
    fixing or learning $\{\mu_t, \sigma_t, b_t, p_\text{prior}\}$ results in different sampling strategies. 
    Broadly, there are three main types of processes:
    \begin{itemize}[leftmargin=*]
        \item  \emph{time-reversal sampler}: the first involves training \Cref{eq:x} to approximate the time-reversal of a target process that begins with the target  $p_\text{target}$ and evolves toward a tractable distribution such as $p_\text{prior}$. 
        The target process is typically designed with a tractable drift term to ensure that its terminal density (approximately) converges to $p_\text{prior}$, with common choices including variance-preserving (VP) and variance-exploding (VE) SDEs and pinned Brownian motion (PBM).
        This category includes methods like PIS \citep{zhangpath,vargas2021bayesian}, DDS \citep{vargasdenoising}, DIS \citep{berneroptimal}, and iDEM \citep{akhounditerated}.
        \item \emph{escorted transport sampler}: the second trains \Cref{eq:x} to transport between a sequence of prescribed marginal densities $\pi_t$, typically defined by interpolation between $p_{\text{prior}}$ and $p_{\text{target}}$, with $\pi_0 = p_{\text{prior}}$ and $\pi_T = p_{\text{target}}$.
    Representative methods include Escorted Jarzynski \citep{vaikuntanathan2011escorted},  CMCD \citep{vargas2024transport}, NETS / PINN-based transport \citep{mate2023learning,albergo2024nets}, LFTS \citep{tian2024liouville}, etc.
    \item \emph{annealed variance reduction sampler}: Similar to the escorted transport sampler, these approaches prespecify an annealed target $\pi_t$  and set $b_t=0$ and  $\mu_t = \nabla \ln \pi_t$ just like the proposal in AIS \citep{neal2001annealed,jarzynski1997nonequilibrium}, the forward process remains fixed so no guidance/escorting is learned. 
    However, one approximates the reversal of this forward proposal so that the Radon-Nikodym derivative (RND) between the time-reversal and the forward proposal has a low variance, allowing a more efficient importance sampling.
    This category includes methods like AIS \citep{neal2001annealed,jarzynski1997nonequilibrium}, MCD \citep{doucet2022score,zhang2021some,hartmann2019jarzynski}, LDVI \citep{geffner2023langevin}, among others. 
    \end{itemize}
    \input{tables/sampling_process}
  We compare the properties of different underlying processes in \Cref{tab:process_property}, including ergodicity (i.e., whether the sampler can mix within a finite number of steps \citep{albergo2023stochastic,huang2021schr,zhangpath,vargas2021bayesian,grenioux2024stochastic}), flexibility on the choice of prior, and the ``smoothness" \citep{chemseddine2024neural,woodard2009sufficient,tawn2020weight,syed2022non,phillips2024particle} of the induced flow
  (i.e. the mass teleportation problem, also known as mode switching).
    \item \textbf{Training objectives}.
    There are mainly two families of objectives:
    \begin{itemize}[leftmargin=*]
        \item \emph{path measure alignment}: the first one aligns the path measure induced by the sampling process, i.e., the SDE starting from $p_\text{prior}$, with another process starting from the target distribution $p_\text{target}$ and traversing in reverse. Common objectives include KL divergence \citep{zhangpath,vargas2021bayesian,vargasdenoising,doucet2022score,lahlou2023theorycontinuousgenerativeflow,berneroptimal,vargas2024transport}, log-variance divergence \citep{richterimproved}, the (sub-)trajectory balance objective~\citep{zhangdiffusion}, and detailed balance objective~\citep{bengio2021flow}. 
        \item \emph{marginal alignment}:  this approach aims to align the drift term or vector field of the sampling process with a prescribed target, ensuring that the marginal distributions of the generated samples closely follow the desired trajectory at each time step.
        Common objectives in this category include the physics-informed neural network (PINN) loss \citep{sun2024dynamical,albergo2024nets}, action matching loss \citep{albergo2024nets}, and score matching with importance sampling \citep{akhounditerated}.
    \end{itemize}
   We compare the properties of different objectives in \Cref{tab:obj_property}. 
   Specifically, we assess whether they support off-policy training, can be computed without simulation, require the costly calculation of network divergence, and ensure unbiasedness. 
   
   We note that the \emph{simulation-free training} can relate to several concepts in the neural sampler literature: (1) training without using MCMC, (2) detaching gradients on samples when evaluating trajectory-based objectives, and (3) evaluating objectives at any time step without simulating the trajectory.
  In this paper, we formally define simulation-free training as training with an objective that can be evaluated without simulating any ODE or SDE, aligning with the principles of diffusion and flow matching methods.
\end{enumerate}

\begin{table}[t]
\centering
\caption{Properties of different objectives. 
*KL divergence does not support simulation-free training in general.
However, it can be calculated without simulation for some special cases. 
We will provide an example later in \Cref{sec:sim_free}.
**KL divergence and log-variance divergence typically do not require computing the divergence. However, \citet{richterimproved} proposed objectives for neural samplers based on the general Schr√∂dinger Bridge that requires computing this divergence.
}\label{tab:obj_property}
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Objective}} & \multicolumn{4}{c}{\textbf{Properties}} \\
 & off-policy & sim-free & div-free & unbiased \\ \midrule
\textbf{KL} & \xmark & \xmark(\cmark*) & \cmark(\xmark**) & \cmark \\
\textbf{LV} & \cmark & \xmark & \cmark(\xmark**) & \cmark \\
\textbf{TB/STB} & \cmark & \xmark & \cmark & \cmark \\
\textbf{DB} & \cmark & \xmark & \cmark & \cmark \\
\textbf{PINN} & \cmark & \cmark & \xmark & \cmark \\
\textbf{AM} & \cmark & \xmark & \cmark & \cmark \\
\textbf{SM w. IS} & \cmark & \cmark & \cmark & \xmark \\
\bottomrule
\end{tabular}\vspace{-7pt}
\end{table}


    
Combining different underlying processes and objectives, we will recover many common neural samplers.
In the following, we briefly explain their design and categorize them in \Cref{tab:neural_samplers}. 
We include more details in \Cref{appendix:review}.
    \begin{enumerate}[label=({{\arabic*}}), leftmargin=*]
        \item Path Integral Sampler \citep[PIS,][]{zhangpath} and concurrently \citep[NSFS, ][]{vargas2021bayesian}: 
        PIS fixes $p_\text{prior} = \delta_0,  \sigma_t=1/\sqrt{2}$ and learns a network $f_\theta(\cdot) =\mu_t(\cdot) + \sigma_t^2 b_t(\cdot)$ so that \Cref{eq:x} approximate the time-reversal of the following SDE (Pinned Brownian Motion):
        \begin{align}\label{eq:pis_y}
             d Y_t = -\frac{Y_t}{T-t} dt + dW_t, \quad Y_0 \sim \ptarget.
        \end{align}
        We define \Cref{eq:pis_y} as the time-reversal of \Cref{eq:x} when $Y_t \sim X_{T-t}$.
        The network is learned by matching the reverse KL \citep{zhangpath,vargas2021bayesian} or log-variance divergence \citep{richterimproved} between the sampling and the target process.
        \par
        Diffusion generative flow samplers~\citep[DFGS,][]{zhangdiffusion} time-reversal the same pinned Brownian motion but with a new introduction of local objectives including detailed balance and (sub-)trajectory balance which has been shown equivalent to the log-variance objective with a learned baseline rather than a Monte Carlo (MC) estimator~\citep{nusken2021solving}.  
        \item Denoising Diffusion Sampler \citep[DDS,][]{vargasdenoising} and time-reversed Diffusion Sampler \citep[DIS,][]{berneroptimal}: 
        both DDS and DIS fix $\mu_t(X_t, t) = \beta_{T-t} X_t, \sigma_t = v \sqrt{\beta_{T-t}}, p_\text{prior} = \mathcal{N}(0, v^2I)$, and learn a network $f_\theta(\cdot, t) = b_t(\cdot, t)/2 $ so that \Cref{eq:x} approximates the time-reversal of the VP-SDE:
        \begin{align}\label{eq:dds_y}
            dY_t = -\beta_t Y_t dt + v \sqrt{2\beta_{t}} dW_t, \quad  Y_0 \sim \ptarget.
        \end{align}
        In an optimal solution, $f_\theta$ will approximate the score $f_\theta(\cdot, t) \approx \nabla\log p_{T-t}(\cdot)$, where $ p_{t}(X) = \int  \mathcal{N}(X|\sqrt{1-\lambda_t}Y, v^2\lambda_t I) \ptarget(Y)dY$ and $\lambda_t = 1-\exp(-2\int_0^t  \beta_s ds)$.
        Similar to PIS, the network can be trained either with reverse KL divergence or log-variance divergence.
        \item Iterated Denoising Energy Matching \citep[iDEM,][]{akhounditerated}: iDEM fixes $\mu_t(X_t, t) = 0, p_\text{prior} = \mathcal{N}(0, T^2 I)$, and learns a network $f_\theta(\cdot, t) = b_t(\cdot, t) / 2$ to approximate the score  $f_\theta(\cdot, t) \approx \nabla \log p_{T-t} (\cdot)$, where $\log p_{T-t}$ is estimated by target score identity \citep[TSI, ][]{de2024target} with a self-normalized importance sampler:
        \begin{align}
           {\nabla \log p_{T-t} (X_t)}& \approx \sum_{n} \frac{\ptilde (X_T^{(n)})}{ \sum_{m}  \ptilde(X_T^{(m)})}\nabla \log \ptilde (X_T^{(n)}), \quad X_T^{(n)}\sim  q_{T|t}(X_T | X_t),
        \end{align}
        $q_{T|t}(X_T | X_t)$ is the importance sampling proposal chosen as $q_{T|t}(X_T | X_t)\propto p_{t|T}(X_t | X_T)$.
        In an optimal solution, the sampling process approximates the time-reversal of a VE-SDE:
         \begin{align}\label{eq:idem_y}
            dY_t = \sqrt{2 t} dW_t, \quad  Y_0 \sim {\ptarget}.
        \end{align}
        One can re-interpret the estimator regressed in iDEM in terms of the optimal drift solving a stochastic control problem~\citep{huang2021schr}. The optimal control $f^*_{\sigma_{\mathrm{init}}}$ can be expressed in terms of the score (e.g. See Remark 3.5 in \cite{reusmooth}), for any $\sigma_{\mathrm{init}}>0$ :
        \begin{align}
             f^*_{\sigma_{\mathrm{init}}}(X_t,t) = - \nabla \log \phi_{T-t}(X_t) = -\frac{X_t}{T-t +\sigma_{\mathrm{init}}^2} +\nabla \log {p_{T-t}(X_t)},
        \end{align}
        where $\phi_t(X_t) $ is the 
     value function. 
     It can be expressed as a conditional expectation via the Feynman-Kac formula with Hopf-Cole transform \citep{hopf1950partial,cole1951quasi,fleming1989logarithmic}: \begin{align}\label{eq:opt_val_func}
            \phi_t(X_t) = \mathbb{E}_{X_T \sim q_{T|t}(X_T|X_t)}\left [\frac{\ptilde}{\mathcal{N}(0, T+ \sigma_{\mathrm{init}}^2)}(X_T)\right]. 
        \end{align}
       Note the MC Estimator of $\nabla \log \phi_{T-t}(X_t)$ (e.g. Equation \ref{eq:opt_val_func}) was used in Schr\"odinger-F\"ollmer Sampler \citep[SFS, ][]{huang2021schr} to sample from time-reversal of pinned Brownian Motion, yielding an estimator akin to the one used in iDEM.
        \item Monte Carlo Diffusion \citep[MCD,][]{doucet2022score}: unlike other neural samplers, MCD's sampling process is fixed as $\mu_t = 0, \sigma_t = 1, b_t(X_t, t) = \nabla \log \pi_t(X_t)$, where $\pi_t$ is the geometric interpolation between target and prior, i.e., $\pi_t(X_t) = p_\text{target}^{\beta_t}(X_t)p_\text{prior}^{1-\beta_t}(X_t)$.
    It can be viewed as sampling with AIS using ULA as the kernel.
    Note, that this transport is non-equilibrium, as the density of $X_t$ is not necessary $\pi_t(X_t)$.
    Therefore, MCD trains a network to approximate the time-reversal of the forward process and perform importance sampling (more precisely, AIS) to correct the bias of the non-equilibrium forward process.
        \item Controlled Monte Carlo Diffusion \citep[CMCD,][]{vargas2024transport} and Non-Equilibrium Transport Sampler \citep[NETS,][]{mate2023learning,albergo2024nets}: 
        Similar to MCD, CMCD and NETS also set $b_t(X_t, t) = \nabla \log \pi_t(X_t)$ and $\pi_t$ is the interpolation between target and prior\footnote{CMCD defines $\pi_t$ with geometric interpolation between target and prior  $\pi_t(X_t) = p_\text{target}^{\beta_t}(X_t)p_\text{prior}^{1-\beta_t}(X_t)$.
         In contrast, NETS defines 
 $\pi_t$ differently depending on the target distribution. 
 For example, with a GMM target, 
 $\pi_t$ is constructed as a GMM whose components' means and variances are linearly interpolated between the target mixture components and a Gaussian around 0. We will denote this as mode interpolation.
}.
Different from MCD where the sampling process is fixed, CMCD and NETS learn $f_\theta(\cdot, t) = \mu_t(\cdot, t)$ so that the marginal density of samples $X_t$ simulated by \Cref{eq:x} will approximate $\pi_t$.
As a special case, Liouville Flow Importance Sampler \citep[LFIS,][]{tian2024liouville} fixes $\sigma_t=0$ and learns an ODE to transport between $\pi_t$.

    
\end{enumerate}


\input{tables/compare}

\vspace{-5pt}
\section{Simulation-free Training with Normalizing Flow Induced SDEs}\label{sec:sim_free}\vspace{-4pt}
In this section, we propose a potential design for simulation-free training of DDS and CMCD using normalizing flows (NF)\footnote{We use NF to refer to an invertible network rather than continuous normalizing flows \citep{chen2018neural}.}.
Consider a time-dependent normalizing flow defined as $F_\theta: \mathcal{X} \times [0, T] \rightarrow \mathcal{X}.$
We denote the density of the samples drawn from the normalizing flow as $q_\theta(X_t, t)$.
A key property of NFs that enables simulation-free training is their ability to generate samples $X_t \sim q_\theta(X_t, t)$ through two distinct approaches \citep{bartosh2024neural}:
\begin{enumerate}[leftmargin=*]
    \item Drawing from the base distribution \( X_\text{base} \sim p_\text{base} \) and transforming it via \( F_\theta(X_\text{base}, t) \);
    \item Drawing an initial sample \( X_\text{base} \sim p_\text{base} \), $X_0 = F_\theta(X_\text{base}, 0)$ and evolving through an ODE:
   $
        dX_t = \partial_t F_\theta(X_\text{base}, t) dt = (\partial_t F_\theta(X_\text{base}, t)|_{X_\text{base} = F_\theta^{-1}(X_t, t)} )dt
 $. 
    For simplicity, we write $\tilde{F}_\theta(X_t, t) = \partial_t F_\theta(X_\text{base}, t)|_{X_\text{base} = F_\theta^{-1}(X_t, t)}$.
    Additionally, the following SDE will have the same marginal density as the ODE for any $\sigma\geq 0$:
    \begin{align}\label{eq:nf-sde}
        dX_t = \left(\tilde{F}_\theta(X_t, t) + \sigma_t^2 \nabla\log q_\theta(X_t, t) \right)dt + \sigma_t\sqrt{2} dW_t.
    \end{align}   
\end{enumerate}
The first approach allows us to directly generate samples along the trajectory without simulation, while the second approach allows the use of the same objective as previously described in control-based samplers.
In the following, we introduce NF-DDS, leveraging normalizing flows to achieve a simulation-free training objective.
In \Cref{appendix:nf-cmcd}, we present an alternative approach, NF-CMCD, which coincides with matching the reverse Fisher divergence between marginals in all time steps.
\par

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.44\textwidth}
      \includegraphics[height=60pt, trim={0 0 80 0}, clip]{figures/init.png}
    \caption{Initialization of NF-DDS, samples generated at different time steps $0, 0.8, 1.0$. As we can see, the initialization already covers all modes.}
    \label{fig:init_nf_dds}  
    \end{subfigure}\hfill
    \begin{subfigure}{0.55\textwidth}
      \includegraphics[height=60pt]{figures/trained.png}
    \caption{NF-DDS after training with \Cref{eq:flowDDS-obj}, samples generated at different time steps $0, 0.8, 1.0$.
   Unlike DDS, NF-DDS fails to capture all modes.}
    \label{fig:nf_dds}  
    \end{subfigure}\vspace{-7pt}
\end{figure}

\textbf{NF-DDS:} 
Recall that in DDS, we match the sampling process in \Cref{eq:nf-sde} with the time-reversal of a VP-SDE starting from the target density: \vspace{-2pt}
\begin{align}\label{eq:target}
   dY_t = -\beta_t Y_t dt + v \sqrt{2\beta_{t}} dW_t, \quad  Y_0 \sim \ptarget. 
\end{align}
To have a bounded RND between the target path measure and \Cref{eq:nf-sde}, we set $\sigma_t= v\sqrt{\beta_{T-t}}$. 
We rewrite the sampling process for easy reference: \vspace{-2pt}
\begin{align}\label{eq:nf-sde-dds}
        dX_t = \left(\tilde{F}_\theta(X_t, t) + v^2\beta_{T-t} \nabla\log q_\theta(X_t, t) \right)dt + v\sqrt{2\beta_{T-t}} dW_t.
    \end{align}  
By Nelson's condition \citep{nelson,ANDERSON1982313}, we can write its time-reversal as  \vspace{-2pt} 
\begin{align}\label{eq:nf-sde-dds-backward}
      dY_t = -\left(\tilde{F}_\theta(Y_t, T-t) - v^2\beta_{t} \nabla\log q_\theta(Y_t, T-t) \right)dt + v\sqrt{2\beta_{t}} dW_t, Y_0 \sim q_\theta(Y_0, T).
    \end{align} 
By Girsanov theorem, the KL divergence $D_\text{KL}[\mathbb{Q}||\mathbb{P}]$ between the path measure induced by \Cref{eq:nf-sde-dds-backward} (denoted as $\mathbb{Q}$) and \Cref{eq:target} (as $\mathbb{P}$) is tractable (derivation details in \Cref{appendix:nf-dds}): \vspace{-6pt}
\begin{align}\label{eq:flowDDS-obj}
  \!
   \! \int_0^T\!\!\! \!\frac{1}{\text{\footnotesize$4v^2\beta_{T-t}$}} \E_{{q_\theta(Y, t)}} \|
\tilde{F}_\theta(Y, t) \!- \!v^2\beta_{T-t} \nabla\log q_\theta(Y, t) \!- \!\beta_{T-t} Y
    \|^2 dt\!+ \!D_\text{KL}[q_\theta(\cdot, T)|| \ptarget].
\end{align}
\textbf{Failure of NF-DDS: } Although NF-DDS enables simulation-free training for DDS, it struggles to perform well even on simple tasks. 
We evaluate NF-DDS by training it on a 2D 3-mode Gaussian Mixture target distribution. \Cref{fig:init_nf_dds,fig:nf_dds} illustrate the initialization and the outcomes after training. 
Despite starting with an initialization that covers all modes, and being optimized using the same objective as DDS, NF-DDS fails to achieve satisfactory results.
\par
\emph{What is the difference between DDS and NF-DDS leading to this performance discrepancy}?
Excluding the influence of objectives, the only difference left is the model.
Specifically,  DDS adopts the network proposed by PIS \citep{zhangpath}:
\begin{align}\label{eq:DDS_network}
  f_\theta(\cdot, t) = \text{NN}_{1,\theta}(\cdot, t) + \text{NN}_{2, \theta}(t) \circ  \nabla\log \ptarget(\cdot),
\end{align}
and initializes $\text{NN}_{1,\theta} \approx 0$ and $\text{NN}_{2, \theta}=1$.
In the early stages of training, DDS simulation closely resembles running MCMC with Langevin dynamics and an approximation to the optimal solution (detailed in \Cref{appendix:langevin_approx}). 
In fact, nearly all algorithms discussed in \Cref{sec:review} incorporate a similar term, either explicitly or implicitly. 
If simulating these Langevin terms plays a crucial role, then modifying current algorithms to achieve simulation-free training may not be straightforward or even infeasible.
Therefore, in the next section, we provide ablation studies on the influence of the Langevin term, which we denote as \emph{Langevin preconditioning}, in both time-reversal sampler and escorted transport sampler, trained with different objectives.





