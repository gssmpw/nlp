

\section{Introduction}
Sampling is a fundamental task in statistics, with broad applications in Bayesian inference, rare event sampling, and molecular simulation~\citep{box2011bayesian,tuckerman2023statistical,dellago2002transition,dudoob}.
Consider a target distribution with the following density function:
\begin{equation}
\ptarget(x) = \frac{\ptilde(x)}{Z}, \quad Z = \int_{\Omega} \ptilde(x) dx,
\end{equation}
where $\ptilde(x)$ is the unnormalized density which we can evaluate for a given $x$, and $Z$ is an unknown normalization factor.
We aim to generate samples following $\ptarget$.
These samples can be used to estimate the normalization factor or the expectation over some test functions.
\par
A ``standard" solution to this problem is Markov chain Monte Carlo (MCMC), which runs a Markov chain whose invariant density is $\ptarget$. 
Building on top of MCMC, various advanced sampling techniques have been developed, with the most efficient methods including Parallel Tempering (PT)~\citep{swendsen1986replica,earl2005parallel}, Annealed Importance Sampling (AIS)~\citep{neal2001annealed}, and Sequential Monte Carlo (SMC)~\citep{doucet2001introduction}.
However, MCMC-based approaches typically suffer from slow mixing time and dependency between samples. 
\par
A growing trend of research directions therefore focus on the learned neural sampler, e.g.,~\citep{noe2019boltzmann}, where we train a neural network to amortize the sampling procedure. 
Initial attempts studied normalizing flows (NFs) and used them as proposals for importance sampling (IS)~\citep{noe2019boltzmann,midgleyflow}. 
Later, diffusion and control-based samplers gained notable attention ~\citep{zhangpath,doucet2022score,vargasdenoising,berneroptimal,vargas2024transport,albergo2024nets} due to their success in generative modeling \citep{ho2020denoising,songscore,karras2022elucidating}.
These methods start with an easy-to-sample distribution (e.g., Gaussian) and evolve them through a stochastic differential equation (SDE) or ordinary differential equation (ODE).
However, despite significant progress, these approaches typically require \emph{simulating} the entire trajectory to evaluate the training objective. 
For instance, the most common objective - the reverse KL divergence between the model path measure and the target path measure - generally necessitates simulating the full trajectory for every sample and backpropagating through it. 
This leads to substantial memory consumption and slows down the training process.
\par
To this end, various objectives have been proposed to reduce computational costs. 
Some off-policy objectives enable detaching the gradient from the simulation~\citep{richterimproved}, while others involve simulating only a partial path~\citep{zhangdiffusion}.
The ultimate goal, however, is to design a sampler and training objective that can be optimized without any trajectory simulation following lessons learned from diffusion and flow matching models~\citep{ho2020denoising,songscore,lipmanflow}.
While appealing, we argue that most current approaches are not well-suited for such a design. 
This obstacle stems not only from how to modify the objective formulation for simulation-free evaluation but also from these approaches' reliance on tricks in network parameterization and sampling procedures that are not compatible with simulation-free training - most notably, the Langevin preconditioning, first proposed by \citet{zhangpath}. 
Through a simple example, we demonstrate that even with the same objective and a mode covering initialization, simulation-free training leads to significant mode collapse. 
We attribute this failure to the absence of the Langevin preconditioning in the simulation-free training pipeline.
To further support this claim, we provide ablation studies, showing that most current approaches struggle without the Langevin preconditioning. 
This observation highlights critical caveats and considerations that must be addressed in future work aimed at developing training-free objectives and pipelines.
\par
Running simulations with the Langevin preconditioning also poses a new challenge: 
simulation during training greatly increases the number of evaluations of the target density, which can be prohibitively expensive in some applications.
Consequently, it remains unclear whether these approaches are efficient compared to directly running MCMC and fitting a diffusion sampler post-hoc.
To investigate this, we compare the samplers with a state-of-the-art MCMC method, Parallel Tempering \citep[PT, ][]{PhysRevLett.57.2607,earl2005parallel}.
We find that PT serves as a remarkably strong baseline that should not be overlooked. 
\par
In summary, our main contributions are as follows:
(1) We provide a systematic review of current samplers, focusing on classifying different approaches by their underlying process and objectives. 
(2) We propose a simple direction for achieving it using Normalizing Flows. 
    Unfortunately, this attempt does not perform as desired, which we attribute to the absence of Langevin preconditioning widely applied in other neural samplers.
(3)  We investigate the influence of Langevin preconditioning. 
    Our findings reveal that most approaches fail when the sampler is not parameterized with the gradient of the target density. 
    This indicates critical caveats and considerations that should be addressed in developing simulation-free approaches.
(4) Finally, we compare several diffusion neural samplers with PT, and find that they lag significantly behind the results obtained from running traditional MCMC methods and fitting a diffusion model post hoc. 
    This highlights key challenges and critical considerations for enhancing the practicality of neural samplers in future work.

\par

