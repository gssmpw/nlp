\begin{figure*}
  \centering
    \includegraphics[width=\linewidth]{figures/src/overview.pdf}
  \caption{\textbf{\ours system overview}. 
  Our getting-up policy (\cref{sec:policy}) is trained in simulation using two-stage RL training, after which it is directly deployed in the real world. 
  (a) \sone (\cref{sec:stageI}) learns a discovery policy $f$ that figures out a getting-up trajectory with minimal deployment constraints.
  (b) \stwo (\cref{sec:stageII}) converts the trajectory discovered by \sone into a policy $\pi$ that is deployable, robust, and generalizable. This policy $\pi$ is trained by learning to track a slowed down version of the discovered trajectory under strong control regularization on varied terrains and from varied initial poses.
  (c) The two-stage training induces a curriculum (\cref{sec:curriculum}). \sone targets motion discovery in easier settings (simpler collision geometry, same starting poses, weak regularization, no variations in terrain), while \stwo solves the task of making the learned motion deployable and generalizable. 
  }\label{fig:overview}
  \vspace{-3pt}
\end{figure*}