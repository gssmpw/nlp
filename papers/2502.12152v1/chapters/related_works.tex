\section{Related Work}
We review related works on humanoid control, learning for humanoid control, and work specifically targeted toward fall recovery for legged robots.

\subsection{Humanoid Control}
Controlling a high degree of freedom humanoid robots have fascinated researchers for the last several decades. 
Model-based techniques, such as those based on the Zero Moment Point (ZMP) principle~\cite{ZeroMoment04,HondaHumanoid98,ASIMO02,WalkMan17}, optimization~\cite{OptimizationBasedLocomotion16,bookwalkingrunning2023,BipedalRunning23}, and Model Predictive Control (MPC)~\cite{MITHumanoid21,galliker2022planar,DynamicLocomotionMITConvexMPC18,FullOrderSamplingBasedMPC24}, have demonstrated remarkable success in fundamental locomotion tasks like walking, running and jumping.
However, these approaches often struggle to generalize or adapt to novel environments.
In contrast, learning-based approaches have recently made significant strides, continuously expanding the generalization capabilities of humanoid locomotion controllers.
\input{figures/overview}

\subsubsection{Learning for humanoid control}
Learning in simulation via reinforcement followed by a sim-to-real transfer has led to many successful locomotion results for quadrupeds~\cite{AgileDynamicMotorSkills19, RMA21} and humanoids~\cite{RealWorldHumanoidLocomotionScienceRobotics24, HumanoidLocomotionNextTokenPrediction24, HumanoidLocomotionChallengingTerrain24, advancinglocomotion2024, LCP24, KinodynamicFabrics23}. This has enabled locomotion on challenging in-the-wild terrain~\cite{HumanoidLocomotionChallengingTerrain24,DenoisingWorldModel24}, agile motions like jumping~\cite{BipedalJumpingControl23,WoCoCo24}, and even locomotion driven by visual inputs~\cite{HumanoidParkour24,long2024learning}. Researchers have also expanded the repertoire of humanoid motions to skillful movements like dancing and naturalistic walking gaits through use of human mocap or video data~\cite{Exbody2_24, Exbody24, UH1_24, Hover24}. Some works address locomotion and manipulation problems for humanoids simultaneously to enable loco-manipulation controllers in an end-to-end fashion facilitated by teleportation~\cite{OmniH2O24,HumanPlus24,MobileTelevision24}. 
Notably, these tasks mostly involve contact between the feet and the environment, thus requiring only limited contact reasoning. How to effectively develop controllers for more \textit{contact-rich} tasks like crawling, tumbling, and getting up that require numerous, dynamic, and unpredictable contacts between the whole body and the environment remains under-explored.

\subsection{Legged robots fall recovery}
Humanoid robots are vulnerable to falls due to under-actuated control dynamics, high-dimensional states, and unstructured environments~\cite{WABOT1_73,HondaHumanoid98,MABEL09,krotkov2018darpa,Humanoid35DoF96,gu2025humanoid}, making the ability to recover from falling of great significance. 
Over the years, this problem has been tackled in the following ways.

\subsubsection{Getting up via motion planning}
Early work from \citet{Learning2StandUp98} solved the getting-up problem for a two-joint, three-link walking robot in 2D, and several discrete states are used as subgoals to transit via hierarchical RL. 
This line of work can be viewed as an application of motion planning by \textit{configuration graph transition} learning~\cite{StateTransitionGraph96}, where stored robot states between lying and standing are used as graph nodes to transit~\cite{UKEMI02,FirstHumanoidGetUp03,GettingUpMotionPlanning07,HumanoidBalancing16}.
More recently, some progress has been made to enable toy-sized humanoid robots to get up.
For example, \citet{HumanoidStandingUpLfDMultimodalReward13} explores getting up from a canonical sitting posture with motion planning by imitating human demonstration with ZMP ceriterion.
To address the high-dimensionality of humanoid configurations, \citet{StandUpSymmetry16} leverage bilateral symmetry to reduce the control DoFs by half and a clustering technique is used for further reducing the complexity of configuration space, thereby improving getting-up learning efficiency.
However, such state machine learning using predefined configuration graphs may not be sufficient for generalizing to unpredictable initial and intermediate states, which happens when the robot operates on challenging terrains.

\subsubsection{Hand-designed getting-up trajectories} 
Another solution, often adopted by commercial products, is to replay a manually designed motion trajectory. For example, Unitree~\cite{UnitreeG124} has a getting-up controller built into G1's default controllers. Booster Robotics~\cite{Booster} designed a specific recovery controller for their robots that can help the robot recover from fallen states. The main drawback of such pre-defined trajectory getting-up controllers is that they can only handle a limited number of fallen states and lack generalization, as our experimental comparisons will show.

\subsubsection{Learned getting-up policies for real robots} 
RL followed by sim-to-real has also been successfully applied for quadruped~\cite{AgileDynamicMotorSkills19,dribblebot2023,guardiansasyoufall2024,ma2023learning} fall recovery. 
For example, \citet{AgileDynamicMotorSkills19} explore sim2real RL to achieve real-world quadruped fall recovery from complex configurations.
\citet{dribblebot2023} train a recovery policy that enables the quadruped to dribble in snowy and rough terrains continuously.
\citet{guardiansasyoufall2024} develop a quadruped recovery policy in highly dynamic scenarios..

\subsubsection{Learned getting-up policies for character animation} A parallel research effort in character animation, also explores the design of RL-based motion imitation algorithms: DeepMimic~\cite{DeepMimic18}, AMP~\cite{AMP2021}, PHC~\cite{PHC23}, among others~\cite{PhysicsBasedMocapImitationDRL18,PULSE24,MaskedMimic24,HierachicalWorldModel25,CooHOI24,HOIHumanLevelInstruction24}. These have also demonstrated successful getting-up controllers in simulation. 
By tracking user-specified getting-up curves, \citet{frezzato2022synthesizing} enable humanoid characters to get up by synthesizing physically plausible motion.
Without recourse to mocap data, such naturalistic getting-up controllers for simulated humanoid characters can also be developed with careful curriculum designs~\cite{Learning2GetUp22}.
Some works explore sampling-based methods for addressing contact-rich character locomotion including getting up~\cite{SAMCONSamplingBasedContactRichMotion10,OnlineMotionSynthesis14,SampleEfficientCE21}, while some works have demonstrated success in humanoid getting up with online model-predictive control~\cite{OnlineTrajectoryOptimization12}.
It is worth noticing, however, that these works use humanoid characters with larger DoFs compared to humanoid robots (\eg, 69 DoFs in SMPL~\cite{SMPL15}) and use simplified dynamics.
As a result, learned policies operate body parts at high velocities and in infeasible ways, leading to behavior that cannot be transferred into the real world directly.
Hence, developing generalizable recovery controllers for humanoid robots remains an open problem. 


