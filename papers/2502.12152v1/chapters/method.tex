\section{\ours: Sim-to-Real Humanoid Getting Up}
Our goal is to learn a getting-up policy $\pi$ that enables a humanoid to get up from arbitrary initial postures. We consider getting up from two families of lying postures: a) supine poses (\ie lying facing up) and b) prone poses (\ie lying face down).
Getting up from these two groups of postures may require different behaviors, which makes it challenging to learn a single policy that handles both scenarios.
To tackle this issue, we decompose the getting-up task from a prone pose to first rolling over and then standing up from the resulting supine posture. Therefore, our objective is to learn policies for rolling over from a prone pose and getting up from a supine pose separately.

To solve these two tasks, we propose \ours, a general learning framework for training getting-up and rolling over policies, which is illustrated in \cref{fig:overview}.
In stage I, a discovery policy $f$ is trained to figure out standing-up or rolling-over motions.
$f$ is trained without deployment constraints, and only our task and symmetry rewards are used.
In stage II, a deployable policy $\pi$ imitates the rolling-over / getting-up behaviors obtained from stage I under strong control regularization.
This deployable policy $\pi$ is transferred from simulation to the real world as the final policy.
To overcome the difficulty of contact-rich locomotion learning involved in these tasks, we design a curricula for learning. We detail the policy model and two-stage training in \cref{sec:two-stage-policy}, and the curriculum design is introduced in \cref{sec:curriculum}.


\subsection{Policy Architecture}\label{sec:policy}
\ours trains two policy models $f$ and $\pi$ with RL.
Both policy models take observation $\mathbf{o}_t = [\mathbf{z}_t, \mathbf{s}_t, \mathbf{s}_{t-10:t-1}] \in \mathbb{R}^{868}$ as input and output action $\mathbf{a}_t \in \mathbb{R}^{23}$, where $\mathbf{s}_t \in \mathbb{R}^{74}$ is the robot's proprioceptive information, $\mathbf{s}_{t-10:t-1}$ is the $10$ steps history states, and $\mathbf{z}_t\in\mathbb{R}^{54}$ are the encoded environment extrinsic latents learned using regularized online adaptation~\cite{DeepWholeBodyControl22}.
We use the proprioceptive information $\mathbf{s}_t$ consisting of the robot's roll and pitch, angular velocity, DoF velocities, and DoF positions.
Such proprioceptive information can be accurately obtained in the real world, and we find that these are sufficient for the robot to infer the overall posture.
We do not use any linear velocity and \texttt{yaw} information as it is difficult to reliably estimate them in the real world~\cite{OmniH2O24}. 

The policy models are implemented as MLPs and trained via PPO~\cite{PPO17}.
The optimization targets maximizing the expected $\gamma$-discounted policy return within $T$ episode length: 
\(\mathbb{E}\left[\sum_{t=1}^T \gamma^{t-1}r_t\right]\), where $r_t$ is the reward at timestamp $t$.

\subsection{Two-Stage Policy Learning}\label{sec:two-stage-policy}

\subsubsection{\sone: Discovery Policy}\label{sec:stageI}
This stage discovers getting-up / rolling-over behavior efficiently without deployment constraints. We use the following task rewards with very weak regularization to train this discovery policy $f$. We describe the rewards used for optimizing the two policies. Timestep $t$ and reward weight terms are omitted for simplicity. The precise expressions for each reward term and their weights are provided in supplementary.


\vspace{5pt}
\noindent\textbf{Rewards for Getting Up:} $r_{\text{up}} = r_{\text{height}} + r_{\Delta \text{height}} + r_{\text{uprightness}} + r_{\text{stand\_on\_feet}} + r_{\Delta\text{feet\_contact\_forces}} + r_\text{symmetry}$, 
    where \\
    \textbullet~\(r_{\text{height}}\) encourages the robot's height to be close to a target height when standing; \\
    \textbullet~\(r_{\Delta\text{height}}\) encourages the robot to continuously increasing its height; \\
    \textbullet~\(r_{\text{uprightness}}\) encourages the robot to increase the projected gravity on $z$ axis so that the robot will stand upright; \\
    \textbullet~\(r_{\text{stand\_on\_feet}}\) encourages the robot to stand on both feet; \\
    \textbullet~\(r_{\Delta\text{feet\_contact\_forces}}\) encourages the robot to increase contact forces applied to the feet continuously;\\
    \textbullet~\(r_{\text{symmetry}}\) reduces the search space by {\it encouraging (but not requiring)} the robot to output bilaterally symmetric actions. Past work~\cite{SymmetricLeggedLocomotion24,StandUpSymmetry16} employed hard symmetry which improves RL sample efficiency at the cost of limiting robots' DoFs and generalization. Our {\it soft symmetry reward} inherits the benefit but mitigates the limitation.

\vspace{5pt}
\noindent\textbf{Rewards for Rolling Over:} ~\(r_{\text{roll}} = r_{\text{gravity}}\), where \(r_{\text{gravity}}\) encourages the robot to change its body orientation so that its projected gravity is close to the target projected gravity when lying facing up.



\subsubsection{\stwo: Deployable Policy}\label{sec:stageII}
This stage trains policy $\pi$ that will be directly deployed in the real world. Policy $\pi$ is trained to imitate an 8$\times$ slowed-down version of the state trajectories discovered in \sone, while also respecting strong regularization to ensure Sim2Real transferability. We use the typical control regularization rewards and describe them in supplementary. Below, we describe the tracking reward.

\vspace{5pt}
\noindent\textbf{Tracking Rewards:}
The tracking reward, $r_{\text{tracking}}$, encourages the humanoid to act close to the given motion trajectory derived from the discovered motion. \(r_{\text{tracking}} = r_{\text{tracking\_DoF}} + r_{\text{tracking\_body}}\), where \\
\textbullet~\(r_{\text{tracking\_DoF}}\) encourages the robot to move to the same DoF position as the reference motion, and \\
\textbullet~\(r_{\text{tracking\_body}}\) encourages the robot to move the body to the same position as the reference.
Specifically, \(r_{\text{tracking\_body}}\) becomes \(r_{\text{head\_height}}\) and \(r_{\text{head\_gravity}}\) which encourage the robot to track the height and the projected gravity of the robot's head for getting-up and rolling-over tasks, respectively.


\subsection{\sone to \stwo Curriculum}\label{sec:curriculum}
The design of two-stage policy learning intrinsically constructs a
\textit{hard-to-easy} curriculum~\cite{CurriculumLearning09}. \sone targets
motion discovery in easier settings (weak regularization, no variations
in terrain, same starting poses, simpler collision geometry). Once motions have
been discovered, \stwo solves the task of making the learned motion deployable
and generalizable. As our experiments will show, splitting the work into two
phases is crucial for successful learning. Specifically, complexity increases
from \sone to \stwo in the following ways: 



\subsubsection{Collision mesh}
As shown in \cref{fig:overview}, \sone uses a simplified collision mesh for faster motion discovery, while \stwo uses the full mesh for improved Sim2Real performance. 

\subsubsection{Posture randomization}

\sone learns to get up (and roll over) from a canonical pose, accelerating learning, while \stwo starts from arbitrary initial poses, enhancing generalization. To further speed up \sone, we mix in standing poses.
For \stwo, we generate a dataset \(\mathcal{P}\) of 20K supine poses \(\mathcal{P}_{\text{supine}}\) and 20K prone poses \(\mathcal{P}_{\text{prone}}\) by randomizing initial DoFs from canonical lying poses, dropping the humanoid from 0.5m, and simulating for 10s to resolve self-collisions. We use 10K poses from each set for training and the rest for evaluation.

\subsubsection{Control Regularization and Terrain Randomization}
For Sim2Real transfer, we use the following control regularization terms and environment randomization in \stwo: \\
% \begin{itemize}
\textbullet~\textbf{Weak $\rightarrow$ strong control regularization.} Weak control regularization in \sone enables discovery of getting-up / rolling-over motion, while strong control regularization (via smoothness rewards, DoF velocity penalties, \etc, see the full list in supplementary) in \stwo encourages more deployable action.
\\
\textbullet~\textbf{Fast $\rightarrow$ slow motion speed. }
    Without strong control regularization, \sone discovers a fast but unsafe getting-up motion ($<$1s), infeasible for real-world deployment. 
    To address this, we slow it to 8s via interpolation, providing stable tracking targets for \stwo, which better aligns with its control regularization.
\\  
\textbullet~\textbf{Fixed $\rightarrow$ random dynamics and domain parameters.}
    \stwo also employs domain and dynamics randomization via terrain randomization and noise injection. Such randomization has been shown to play a vital role in successful Sim2Real~\cite{DomainRandomizationSim2Real17}.

\input{tables/simulation_benchmark}