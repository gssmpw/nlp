\section{Introduction}
This paper develops learned controllers that enable a humanoid robot to get up
from varied fall configurations on varied terrains.  Humanoid robots are
susceptible to falls, and their reliance on humans for fall recovery hinders their
deployment. Furthermore, as humanoid robots are expected to work in
environments involving complex terrains and tight workspaces (\ie challenging
scenarios that are too difficult for wheeled robots), a humanoid robot may end up
in an unpredictable configuration upon a fall, or may be on an unknown terrain.
26 of the 46 trials at
the DARPA Robotics Challenge (DRC) had a fall and 25 of these falls required
human intervention for recovery~\cite{krotkov2018darpa}. The DRC identified
fall prevention and recovery as a major topic needing more research.  This
paper pursues this research and proposes a learning-based framework for
learning fall recovery policies for humanoid robots under varying
conditions.


The need for recovering from varied initial conditions makes it hard to design a fall recovery controller by hand and motivates the need for learning via trial
and error in simulation. Such learning has produced exciting results
in recent years for locomotion problems involving quadrupeds and
humanoids, \eg~\cite{AgileDynamicMotorSkills19,RealWorldHumanoidLocomotionScienceRobotics24}.
Motivated by these exciting results, we started with simply applying the Sim-to-Real (Sim2Real) paradigm
for the getting-up problem. However, we quickly realized that the getting-up problem is
different from typical locomotion problems in the following three significant
ways that made a naive adaptation of previous work inadequate: 

\begin{itemize}
    \item[\textbf{a)}] \textbf{Non-periodic behavior.} 
    In locomotion, contacts with the environment happen in structured ways:
cyclic left-right stepping pattern. The getting-up problem doesn't have such a periodic behavior. The contact sequence necessary for getting up
itself needs to be figured out. This makes optimization harder and may render
phase coupling of left and right feet commonly used in locomotion
ineffective. 
\item[\textbf{b)}] \textbf{Richness in contact.} Different from locomotion, 
contacts necessary for getting up are not
limited to just the feet. Many other parts of the robot are likely already in
touch with the terrain. But more importantly, the robot may find it useful to
employ its body, outside of the feet, to exert forces upon the environment, in
order to get up. Freezing / decoupling the upper body, only coarsely modeling
the upper body for collisions, and using a larger simulation step size: the
typical design choices made in locomotion, are no longer applicable for the
getting up task.
\item[\textbf{c)}] 
\textbf{Reward sparsity.} Designing rewards for getting up is harder than other locomotion tasks. 
Velocity tracking offers a dense reward and feedback on
whether the robot is meaningfully walking forward is available within a few tens
of simulation steps. In contrast, many parts of the body make negative
progress, \eg, the torso first needs to tilt down for seconds before
tilting up to finally get up. 
\end{itemize}


We present \ours, which circumvents these problems via a two-stage reinforcement learning (RL) based training.
\sone targets \textit{task solving} in easier settings (sparse task rewards with weak regularization), while \stwo solves the task of making the learned motion \textit{deployable} (\ie, control should be smooth; velocities and executed torques should be small; \etc).
To be specific, Stage I is designed for \textit{motion discovery} without being 
limited by smoothness in motion or speed / torque limits, which tackles a hard task under sparse and under-specified task rewards while being less constrained for ensuring task solving.
Stage II is optimized to track the state trajectory discovered in the first stage to tackle easier motion tracking with dense tracking rewards, which is under strict Sim2Real control regularization for ensuring Sim2Real transfer.
From \sone to \stwo, we employ a Sim2Real learning curriculum that progresses from
simplified $\rightarrow$ full collision mesh,
canonical $\rightarrow$ random initial lying posture, and weak to strong control regularization and domain randomization.
This two-stage approach integrates a \textit{hard-to-easy} task-solving curriculum with an \textit{easy-to-hard} Sim2Real curriculum, both of which are crucial for successful learning, as demonstrated in our experiments.

We conduct experiments in simulation and the real world with the G1 platform
from Unitree. In the real world, we find our framework enables the G1 robot to
get up from three situations that we considered: a) supine (lying face up) poses and b) prone (lying face down) poses.
This expands the capability that the G1 comes with: the default hand-crafted getting-up controller only successfully gets up from supine poses 
on a flat surface without bumps. In simulated experiments, we find that our framework can
successfully learn getting-up policies that work on varied terrains, varied
starting poses.
