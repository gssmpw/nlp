\section{Implementation Details}\label{sec:exp_setup}

\subsection{Platform Configurations}
We use the Unitree G1 platform~\cite{UnitreeG124} in all real-world and simulation experiments. G1 is a medium-sized humanoid robot with 29 actuatable degrees of freedom (DoF) in total. Specifically, the upper body has 14 DoFs, the lower body has 12 DoFs, and the waist has 3 DoFs. As getting up does not involve object manipulation, we disable the 3 DOFs in the wrists, resulting in 23 DoFs in total.
Unlike previous robots, G1 has waist \texttt{yaw} and \texttt{roll} DoFs, and we find them useful for our getting-up task.
The robot has an IMU sensor for \texttt{roll} and \texttt{pitch} states, and the joint states can be obtained from the motor encoders. We use position control where the torque is derived by a PD controller operating at 50 Hz.

\subsection{Simulation Configurations}
We use Isaac Gym~\cite{IsaacGym21} for simulated training and evaluation.
We use a URDF with simplified collision for stage-I training and the official whole-body URDF from Unitree~\cite{UnitreeG124} when training stage-II.
To accurately model the numerous contacts between the humanoid and the ground, we use a high simulation frequency of 1000 Hz, while the low-level PD controller frequency operates at 50 Hz.

\section{Simulation Results}\label{sec:sim_exp}


\subsection{Tasks}
We evaluate three tasks involved in the humanoid getting-up process:
\ding{182} \textit{getting up from supine poses}, \ding{183} \textit{rolling over from prone to supine poses}, and \ding{184} \textit{getting up from prone poses} which can be addressed by solving task \ding{183} and task \ding{182} consecutively.  Simulation tests are conducted with the full URDF.


\subsection{Baselines} We compare to the following baselines,
\begin{itemize}
    \item[a)] \textbf{RL with Simple Task Rewards (\citet{Learning2GetUp22}):} 
    This policy is trained with RL using rewards from~\citet{Learning2GetUp22} originally designed for physically animated characters instead of humanoid robots.
    Similar to our method, this baseline applies a three-stage strong-to-weak torque limit and motion speed curriculum for getting-up policy learning.
    Because \cite{Learning2GetUp22} does not consider sim2real deployment regularization and requirements (\eg, smoothness and collision mesh usage), policies learned through their scheme aren't appropriate for real-world humanoid deployment.
    \item[b)] \textbf{\ours w/o Stage II:} 
    Our policy trained with only stage I, where no deployment constraints are applied.
    \item[c)] \textbf{\ours w/o Full URDF:} 
    Our policy trained with two stages, but stage II uses the simplified collision mesh.
    \item[d)] \textbf{\ours w/o Posture Randomization:} 
    Our policy trained on a single canonical lying posture without any randomization of initialization postures.
    \item[e)] \textbf{\ours w/ Hard Symmetry:}
    Our policy trained using a humanoid with a symmetric controller.
    This symmetric controller follows the symmetry control principle of the G1 controller baseline described in real-world experiments, which leads to bilaterally symmetric control.
    We set all \texttt{pitch} DoFs actions to be the same between the left and the right DoFs, while flipping the directions of all the \texttt{roll} and \texttt{yaw} actions.
    \item[f)] \textbf{\ours w/o Two-Stage Learning:}
    Our policy trained in a single stage with the full collision mesh, posture randomization, and all rewards and regularization terms applied at the same time.
\end{itemize}

\subsection{Metrics}\label{sec:metrics}
\noindent \textbullet~\textbf{Task Success.} We report task success rate \textit{Success (\%)} and \textit{Task Metrics}: the head height (m) for the getting-up task, and cosine of the angle between the robot's X-axis (sticking out to the front from the torso) and the gravity vector, for the rolling over task.

\noindent \textbullet~\textbf{Smoothness.} We measure the \textit{Action Jitter (rad/$\text{s}^3$)}, \textit{DoF Pos Jitter (rad/$\text{s}^3$)}, and mean \textit{Energy ($N\cdot$ rad/s)} for action smoothness evaluation. The jitter metrics are computed as the third derivative values~\cite{JitterMeasurement85}, which indicates the coordination stability of body movements.

\noindent \textbullet~\textbf{Safety.} We introduce safety scores $\mathcal{S}^{\text{Torque}}_{\delta,\alpha} \in [0, 1]$ and $\mathcal{S}^{\text{DoF}}_{\delta,\beta}\in[0,1]$ that measure the relative magnitude of commanded torque / DoF compared to the torque and DoF limits, where $\delta$ is a safety threshold. This is essential for robotic safety during execution, as large torques or DoFs will lead to overheating issues and cause mechanical and motor damage. Formally, these scores are defined as:
\[
\begin{aligned}
\mathcal{S}^{\text{Torque}}_{\delta, \alpha} &= 1 - \Bigg( \frac{\alpha}{T J} \sum_{t,j} \frac{| \tau_{t, j} |}{\tau_j^{\max}} + \frac{1-\alpha}{TJ} \sum_{t,j} \mathbbm{1} \Big( \frac{| \tau_{t, j} |}{\tau_j^{\max}} > \delta \Big) \Bigg),
\end{aligned}
\]
\[
% \begin{aligned}
\mathcal{S}^{\text{DoF}}_{\delta,\beta} = 1 - \Bigg( \frac{\beta}{T J} \sum_{t,j} \frac{| q_{t, j} |}{q_j^{\max}} + \frac{1 - \beta}{TJ} \sum_{t,j} \mathbbm{1} \Big( \frac{| q_{t, j} |}{q_j^{\max}} > \delta \Big) \Bigg),
% \end{aligned}
\]
where $\tau_{t, j}$ and $q_{t, j}$ denote the applied torque and joint displacement at time step $t$ for joint $j$, respectively. $\tau_j^{\max}$ and $q_j^{\max}$ represent their respective limits, $T$ is the total number of time steps, and $J$ is the total number of joints. The threshold $\delta$ determines when a torque or displacement is considered excessive. The indicator function $\mathbbm{1}(\cdot)$ returns 1 if the condition is met and 0 otherwise. The parameters $\alpha, \beta \in [0,1]$ control the trade-off between peak and prolonged violations, ensuring a balanced assessment of safety risks.
In this paper, we use $\delta=0.8$, $\alpha=0.5$, $\beta=0.5$ as default during evaluation.

\subsection{Results and Analysis}\label{sec:sim_res}
\cref{tab:simulation_results} presents evaluation results based on policies tested on \texttt{held-out} subsets of our curated initial posture set \(\mathcal{P}\)—10K samples each from \(\mathcal{P}_{\text{supine}}\) and \(\mathcal{P}_{\text{prone}}\). \cref{fig:learning_curve} shows the learning curve for the getting-up task, where the termination base height reflects the robot’s ability to lift its body, and body uprightness indicates whether it achieves a stable standing posture.

\input{figures/real_results}

\subsubsection{Ignoring Torque / Control Limits Leads to Undeployable Policies}
While \cite{Learning2GetUp22} and \ours achieve similar success rates, the smoothness and safety metrics for \cite{Learning2GetUp22} are significantly worse than \ours.
For example, the average action jitter metric is nearly 18$\times$ higher than \ours. Actions from \cite{Learning2GetUp22} are highly unstable and unsafe and thus cannot be safely deployed to the real robot. Furthermore, \cite{Learning2GetUp22} learns a very fast getting-up motion that keeps jumping after getting up. See visualization \cite{Learning2GetUp22}'s getting up motion in the supplementary material. A similar trend can be seen when comparing \ours to \ours w/o \stwo. While \ours w/o \stwo also solves the task to some extent, it achieves unsatisfying smoothness and safety metrics similar making it inappropriate for real-world deployment. Thus, the regularization imposed in \stwo is essential to making policies more amenable to Sim2Real transfer.

\subsubsection{Importance of Learning via a Curriculum}
So, while it is clear that we need to incorporate strong control regularization for good safety metrics and sim2real transfer, our 2 stage process is better than doing it in a single stage. In fact, as plotted in \cref{fig:learning_curve}, \ours w/o Two Stage Learning where the policy is trained in a single stage using all sim2real regularization fails to solve the task. This is because the strict Sim2Real regularization makes task learning extremely challenging. Our two-stage curriculum successfully incorporates both aspects: it learns to solve the task, but the policy also operates safely.

\subsubsection{Full URDF vs. Simplified URDF}
Somewhat surprisingly, even though \ours w/o Full URDF was trained without the full URDF mesh, it generalizes fine when tested with the full URDF in simulation, as reported in \cref{tab:simulation_results}. However, we found poor transfer of this policy to the real world. 
It failed on all 5 trials on a simple flat terrain. We believe the poor real-world performance was because of the mismatch between the contact it was expecting and the contact that actually happened.
 

\subsubsection{Posture randomization helps}
\ours w/o posture randomization (PR) works much worse than \ours suggesting that PR is necessary for generalizable control.

\subsubsection{Soft symmetry vs. hard symmetry}
Compared to \ours w/ Hard Symmetry, \ours achieves better task success in \cref{tab:simulation_results}, particularly for the rolling-over task which is very difficult with symmetric commands.


\input{figures/learning_curve}
\input{figures/compare_baseline_motion}
\input{figures/failure_examples}
\section{Real World Results}\label{sec:real_exp}
We also tested \ours policies in the real world on G1 robot. 
Our real-world test bed consists of 6 different terrains shown in \cref{fig:realworld_res_main}: concrete, brick, stone tiles, muddy grass, grassy slope, and a snow field. 
These terrains span diverse surface properties, including both man-made and natural surfaces, and cover a wide range of roughness (rough concrete to slippery snow), bumpiness (flat concrete to tiles), ground compliance (completely firm concrete to being swampy muddy grasp), and slope (flat to $\sim 10^\circ$). 
We tested two tasks: a) getting up from supine poses, and b) rolling over from prone to supine poses. 


We compare our final \ours policy with
1) \textbf{G1 controller} and 2) a high-performing ablation of \ours (\textbf{\ours w/o posture randomization}). 
The G1 controller, that comes with the robot, tracks a hand-crafted trajectory in three-phase shown in \cref{fig:compare_g1_controller}:
Phase 0 brings the robot to a canonical lying pose; Phase 1 first props up and then slides the torso forward using hands, followed by bending legs to squat; Phase 2 uses its waist to tilt up the torso to stand up from squatting.
The motions in phase 1 and phase 2 are \textit{symmetric}, and this default G1 controller only works for supine poses.

\subsection{Results}\label{sec:real_res}
\cref{fig:realworld_res_main} presents the experimental results. 
Overall we find that \ours policies perform better than the G1 controller and the version of \ours without posture randomization (PR). We discuss the results and observed behavior further.

\subsubsection{Getting up from supine poses} 
The default G1 controller works under nominal conditions, \ie, firm, flat concrete ground with a reasonable friction value. 
However, it starts to fail on more challenging terrains. 
For the bumpier and rougher terrains (brick surface and stone tiles), the arms may get stuck between bumps causing failures. 
On slopes, the robot fails to squat or hoist itself up due to both the resistance of the grassland and the unstable squatting pose prone to falling caused by slopes.
On the compliant ground, the robot gets destabilized due to the compliance of the ground. 
On slippery snow, the robot slips. 


Both versions of \ours outperform the default G1 controller. Trained with terrain and domain randomization, they are robust to real-world variations such as slipperiness, bumps, and slopes. Dynamics randomization further enhances resilience to minor perturbations like slippage or ground compliance. The full method, incorporating posture randomization, performs better than the variant without it, as it is specifically trained to handle diverse initial configurations. Overall, \ours achieves a 78.3\% success rate in getting up.


\subsubsection{Rolling over from prone to supine poses} Findings are similar for the rolling over task. As noted, the default controller can't handle this situation. The full model exhibits more robust performance than the model trained without posture randomization. Rolling over seems to be easier than getting up, \ours achieves a 98.3\% success rate for rolling over. 


\subsection{Motion analysis}
\cref{fig:compare_g1_controller} show the motion and recorded motor temperatures for executions of the G1 controller and \ours policy.

\subsubsection{Motor temperature}
Even when the G1 controller works, it uses the arms during Phase 1 of getting up.
\cref{fig:compare_g1_controller}\,(a) shows that the default controller's execution causes the arm motors to heat up significantly more when compared to \ours execution. 
In contrast, our policy makes more use of the leg motors that are larger (higher torque limit of $83N$ as opposed to $25N$ for the arm motors) and thus able to take more load. 

\subsubsection{Efficiency}
In addition, we find that \ours gets the robot to stand successfully within about 6 seconds through a smooth and continuous motion, which is over 2$\times$ more efficient compared to the G1 controller which takes nearly 11 seconds.

\subsection{Failure Mode Analysis} 
\cref{fig:failure_examples} shows example failure modes for the G1 controller and \ours on challenging terrains.
\cref{fig:failure_examples}\,(a) shows that the G1 controller tries to utilize the robot's hands to squat, while the sloping ground prevents it from getting to the full squatting pose due to high friction and weak waist torques to move against the dumping tendency.
In contrast, \ours manages to lift the body, while the sloping ground sometimes causes an unstable foot orientation.
\cref{fig:failure_examples}\,(b) shows that on even more challenging terrains like snow fields, both G1 and \ours controllers may fail due to the slippery and deformable ground.