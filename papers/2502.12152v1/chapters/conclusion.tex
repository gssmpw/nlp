\section{Discussion}
In this paper, we tackle the problem of {\it learning} getting-up controllers for real-world humanoid robots. Different from locomotion tasks, getting up involves complex contact patterns that are not known apriori. 
We develop a two-stage solution for this problem based on reinforcement learning and sim-to-real. Stage I finds a solution under minimal constraints, while \stwo learns to track the trajectory discovered in \sone under regularization on control and from varied starting poses and on varied terrains. We found this two stage strategy to be effective both in simulation and the real world. Specifically, it enabled us to get a real-world G1 humanoid to stand up from a supine pose and roll over from a supine pose to a prone pose on different terrains and from different starting poses. \ours achieves a higher success rate than the default G1 controller and expands the capabilities of the G1 robot.

We hope our learned policies for automatic fall recovery will be useful to researchers and practitioners, while our two stage learning framework may be helpful for other problems that require figuring out complex contact patterns.
