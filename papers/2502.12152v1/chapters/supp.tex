\clearpage
\input{figures/tao_case}
\appendix
\section{Material Statements}
In our supplementary material, we provide a video demo presenting \ours, and this PDF for supplementary information including our reward designs, training details, and additional qualitative analysis.

\section*{Rewards}

\subsubsection{Rewards components in Stage I} 

Detailed reward components used in Stage I are summarized in \cref{tab:reward_stage1}.

\input{tables/rewards}

\subsubsection{Rewards components in Stage II} 
Detailed reward components used in Stage II are summarized in \cref{tab:reward_stage2}.

\input{tables/reward_stage2}


\section*{Training Details}
\ours contains two-stage learning, which are both trained in simulation.
In Stage I, we train the discovery policy $f$ for overall 5B simulation steps in total, and 20K simulation steps for Stage II deployable policy $\pi$.
All training is conducted on IsaacGym~\cite{IsaacGym21}, and we train our policies using 4,096 paralleled environments on a single NVIDIA RTX 4090 or L40S GPU.
For the getting-up task, we slow down the discovered trajectory to 8 seconds.
For the rolling-over task, the trajectory is slowed down to 4 seconds.
We use flat terrains in Stage I and varied terrains during Stage II training, involving flat, rough, and slopes.
We follow previous works~\cite{OmniH2O24,ExtremeParkour24,HumanoidParkour24} to apply varied dynamics randomization such as friction, base CoM offset, and control delay.



\section*{Additional Qualitative Simulation Baseline Results}
\cref{fig:tao_case} showcases a visualization of getting up from a prone pose generated by baseline method~\cite{Learning2GetUp22}.
It can be observed that this method generates motion that is highly unstable and unsafe to deploy in the real world.
For example, its joints continuously jitter, the feet are stumbling and the body is keep jumping up.
This indicates that this baseline~\cite{Learning2GetUp22} cannot be Sim2Real.
