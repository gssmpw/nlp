\section{Related work}
\textbf{Diffusion bridge simulation:} 
This topic has received considerable attention over the past two decades and it is hard to give a short complete overview. Early contributions are \citet{clark1990simulation, chib2004likelihood, delyon2006simulation, beskos2006exact, lin2010generating}, and \citet{golightly2010learning}. The approach of guided proposals that we use here was introduced in \citet{schauer2017guided} for fully observed uniformly elliptic diffusions and later extend to partially observed hypo-elliptic diffusions in \citet{bierkens2020simulation}.

Another class of methods approximate the intractable transition density using machine learning or kernel-based techniques.  \citet{heng2022simulating} applied score-matching to define a variational objective for learning the additional drift in the reversed diffusion bridge. \citet{baker2024score} proposed learning the additional drift directly in the forward bridge via sampling from an adjoint process. \citet{chau2024efficient} leveraged Gaussian kernel approximations for drift estimation. 

The method we propose is a combination of existing ideas. It used the guided proposals from \citet{schauer2017guided} to construct a conditioned process, but learns an additional drift term parametrized by a neural net using variational inference. 

\textbf{Diffusion Schrödinger bridge:} The diffusion bridge problem addressed in this paper may appear similar to the diffusion Schrödinger bridge (DSB) problem due to their names, but they are fundamentally different. A diffusion bridge refers to a diffusion process conditioned to start at one fixed point and reach another fixed point, while the DSB involves connecting two fixed marginal distributions and is often framed as an entropy-regularized optimal transport problem. Although DSB has gained attention for applications in generative modelling \citep{thornton2022riemannian, de2021diffusion, shi2024diffusion, tang2024simplified}, it is important to recognize the distinctions between these problems.

\textbf{Neural SDE:} Neural SDEs extend neural ODEs \citep{chen2018neural} by incorporating inherent stochasticity, making them suitable for modelling data with stochastic dynamics. Research on neural SDEs can be broadly categorized into two areas: (1) modelling terminal state data \citep{tzen2019neural, tzen2019theoretical}, and (2) modelling entire data trajectories \citep{li2020scalable, kidger2021neural}. Our method aligns with the second category, as it employs trainable drift terms and incorporates end-point constraints to model the full data trajectory.