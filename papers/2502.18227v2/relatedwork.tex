\section{Related Work}
\label{sec:RelatedWork}
Our research aligns closely with studies in the following areas.
\subsection{Primitive algorithms}
\subsubsection{Local differential privacy by adding random noise}
Primitive algorithms refer to those that protect data privacy by randomly adding random noise to individual data. They ensure privacy independently and do not rely on any external mechanisms. They include the Gaussian mechanism~\cite{dwork2014algorithmic}, Laplace mechanism~\cite{dwork2006calibrating}, Matrix Variate Gaussian (MVG)~\cite{chanyaswad2018mvg}, and Independent Directional Noise (IDN)~\cite{direction}.

Although the mechanisms mentioned above do not directly apply to tensor-valued queries, our research is nevertheless grounded in the fundamental principle of preserving privacy through the injection of noise.
The Gaussian mechanism introduces independent and identically distributed (i.i.d.) Gaussian noise, calibrated according to the $l_2$-sensitivity of the query, guarantees $(\epsilon, \delta)$-differential privacy. Similarly, by incorporating noise derived from the Laplace distribution, scaled according to the query function's $l_1$-sensitivity, the Laplace mechanism enforces strong $\epsilon$-differential privacy. 
The MVG mechanism is tailored for matrix-valued queries and incorporates matrix-valued noise to ensure $(\epsilon, \delta)$-differential privacy. The $l_2$-sensitivity of the MVG mechanism is measured using the Frobenius norm, which quantifies the discrepancy between two neighboring matrices. 
The IDN mechanism achieves differential privacy by appending a noise tensor of equal magnitude to the data tensor. However, LDP demands a higher level of privacy than traditional differential privacy, necessitating more noise to achieve comparable privacy guarantees, which can significantly degrade data utility due to extensive perturbation.
Our mechanism is specifically tailored to address the complexities of tensor data. We demonstrate that our mechanism attains $\epsilon$-local differential privacy for tensors, as detailed in Section \ref{sec:Preliminaries}. This innovation bridges the gap between traditional local differential privacy mechanisms and the sophisticated requirements of high-dimensional data, preserving privacy while maintaining utility. 

Beyond these fundamental mechanisms, several sophisticated approaches have also been introduced.
Mechanisms that derive their privacy guarantees from the above mechanisms are well-established, as exemplified by the composition schemes referenced in~\cite{abadi2016deep,lee2018concentrated,lecuyer2019privacy,yu2019differentially,individual,qiu2023fast,10726610}. Among these,~\cite{abadi2016deep} presents an innovative accounting approach for the Gaussian mechanism that minimizes the total additive noise while preserving the same level of privacy protection. In contrast, Kairouz et al.~\cite{kairouz2014extremal} propose an optimal composition scheme that is applicable to a broader range of noise distributions. The studies in~\cite{lee2018concentrated,yu2019differentially} provide dynamic accounting methods that adjust based on the algorithm's runtime convergence. Moreover,~\cite{lecuyer2019privacy} ensures a global differential privacy guarantee in environments where datasets are continuously expanding.
\cite{individual} applies differential privacy to high-order, high-dimensional sparse tensors within the IoT transmission context through individual randomized responses. However, when the central server is required to receive tensor information directly from users, this approach risks disclosing the user's original information with a certain probability, thereby undermining the purpose of privacy protection.
\cite{qiu2023fast} introduced a sparse weight matrix that computes the non-zero elements of the matrix, thereby reducing the computational load of LDP when applied to large-scale datasets.
Furthermore, \cite{10726610} proposed a new privacy budget recovery mechanism, called ChainDP, which adds noise sequentially, so that the noise added by the previous user can be used by subsequent users, thereby reducing the overall noise level and estimation error.
% allows users to reclaim privacy budgets through sequential data aggregation. In the traditional Local Differential Privacy (LDP) mechanism, each user adds noise independently, while ChainDP adds noise sequentially, so that the noise added by the previous user can be used by subsequent users, thereby reducing the overall noise level and estimation error.
%  Zero- concentrated differential privacy [9] imposes a bound on the moment generating function of the privacy loss, and enjoys a nice composition property than conventional differential privacy. Balle and Wang [1] improves the conventional Gaussian mechanism by directly using the Gaussian cumulative density function instead of a tail bound approximation.
% Our work is also aligned with works addressing the utility of additive noise such as [5]–[7]. The optimal noise distribution is found by Geng and Viswanath [6] in terms of the magnitude of the noise, but has restriction on data dimensions. Similar to [7], we formulate the problem of seeking the optimal noise distribution as a constrained optimization problem, and such a distribution in fact indicates directional noise as introduced in [5].
\subsubsection{Local differential privacy by randomized response to a query}
Apart from adding noise to individual data, random response is another popular approach to achieve local differential privacy. Stanley L. Warner et al.~\cite{warner1965randomized} introduced the randomized response mechanism in 1965, which is a technique specifically designed to preserve respondent privacy, commonly referred to as $w$-RR. The key idea is to introduce randomness into the response process, allowing respondents to answer sensitive questions without fear of exposure, while still enabling researchers to estimate the true distribution of responses.
The $w$-RR technique is specifically designed for discrete datasets with exactly two distinct values. 
To extend its applicability, two primary paths for improvement can be explored. The first involves encoding and transforming the variable's multiple values, as demonstrated in methods such as Randomized Aggregatable Privacy-Preserving Ordinal Response (RAPPOR)~\cite{rappor} and S-Hist~\cite{bassily2015local}, to ensure compatibility with the binary nature expected by $w$-RR.
% numerous random response methods have been proposed for protecting individual data's privacy such as Randomized Aggregatable Privacy-Preserving Ordinal Response (RAPPOR)~\cite{rappor}, S-Hist~\cite{bassily2015local}, $k$-RR~\cite{kairouz2014extremal}, and $O$-RR~\cite{kairouz2016discrete}. 
The RAPPOR employs Bloom filters and hash functions to encode a single element into a vector, then perturb the original value through both permanent and instantaneous randomized responses. It represents a method for collecting statistical data from end-user applications while preserving anonymity and ensuring robust privacy protections. 
However, RAPPOR has two main drawbacks: (1) the transmission overhead between the user and the data aggregator is relatively high, as each user is required to send a vector whose length is determined by the size of its bloom filter; (2) the data collector must pre-collect a list of candidate strings for frequency counting. 

To address the first issue, the S-Hist method~\cite{bassily2015local} offers a solution in which each user encodes the string, randomly selects a bit, applies random response techniques to perturb it, and then sends it to the data collector. This approach significantly reduces the transmission cost by minimizing the amount of data each user needs to send. 
For the second problem, building upon the RAPPOR-based encoding-decoding framework, Kairouz et al.~\cite{kairouz2016discrete} further proposed the $O$-RR method by introducing hash mapping and grouping operations. Hash mapping allows the method to focus on the encoded values rather than the original strings, eliminating the need to pre-collect a list of candidate strings. Additionally, the use of grouping operations reduces the probability of hash mapping value collisions. This combination of techniques enhances the robustness and flexibility of differential privacy in various data collection scenarios.%they may not be suitable for distributed systems due to the requirement of the pre-allocated list of candidate strings or the high communication overhead caused by the enlarged data size (e.g., one scalar is encoded into a vector in RAPPOR, the data size could be extremely high considering the high dimensionality of a tensor). Moreover, these approaches are primarily designed for scalars instead of tensor-valued data.

To overcome the second limitation of $w$-RR, one approach is to improve the distribution of the $w$-RR technique, making it applicable to variables with more than two values. This is achieved through mechanisms such as $k$-RR~\cite{kairouz2014extremal}. In the $k$-RR method, the original data is preserved with probability $p$ and is flipped to any other value with a probability of $\frac{1-p}{n-1}$,  where $n$ is the number of possible values. This mechanism ensures that the perturbation is uniformly distributed across all possible values, thereby preserving the privacy of the data. 

The techniques discussed above are designed for discrete values. For continuous values, however, they should first be discretized to meet the fundamental requirement of the $w$-RR technique before privacy protection can be applied. This concept is primarily applied in the context of local differential privacy for mean statistics methods like MeanEst~\cite{duchi2013local,duchi2014privacy} and Harmony-mean~\cite{nguyen2016collecting}.

% \subsection{Randomized Response}
% In some research, different questions may elicit different degrees of cooperation from respondents. People are generally more willing to share their genuine opinions on innocent topics, such as “What is your favorite book/movie?”, but are more likely to avoid answering or provide misleading answers when confronted with revealing questions, such as “Do you have a criminal record?”. To mitigate this evasive response bias, Stanley L. Warner et al.~\cite{warner1965randomized} introduced the randomized response, a survey technique specifically designed to preserve respondent privacy, in 1965, commonly referred to as $w$-RR. In this method, respondents are randomly assigned to one of two groups with probability $p$ and $1-p$ and are then instructed to answer “yes” or “no” to a question based on their group assignment. The key innovation of this technique is its ability to protect individual privacy while still enabling the collection of meaningful data. Since interviewers only receive the aggregate count of "yes" and "no" responses, they cannot ascertain the specific question to which each answer corresponds, thus maintaining anonymity and promoting truthful answers. This approach has been widely adopted in research involving sensitive topics, ensuring the confidentiality of respondents while providing valuable insights into the population's attitudes and behaviors.

% The $w$-RR technique is specifically designed for discrete datasets with exactly two distinct values. To extend its applicability, two primary paths for improvement can be explored. The first involves encoding and transforming the variable's multiple values, as demonstrated in methods such as Randomized Aggregatable Privacy-Preserving Ordinal Response (RAPPOR)~\cite{rappor} and S-Hist~\cite{bassily2015local}, to ensure compatibility with the binary nature expected by $w$-RR.

% The RAPPOR employs Bloom filter and hash functions to encode a single element into a vector, then perturb the origin value through both permanent and instantaneous randomized responses. It represents a method for collecting statistical data from end-user applications while preserving anonymity and ensuring robust privacy protections. 
% However, RAPPOR has two main drawbacks: (1) the transmission overhead between the user and the data aggregator is relatively high, as each user is required to send a vector whose length is determined by the size of its bloom filter; (2) the data collector must pre-collect a list of candidate strings for frequency counting. 

% To address the first issue, the S-Hist method~\cite{bassily2015local} offers a solution in which each user encodes the string, randomly selects a bit, applies random response techniques to perturb it, and then sends it to the data collector. This approach significantly reduces the transmission cost by minimizing the amount of data each user needs to send. 
% To overcome the second limitation, one approach is to improve the distribution of the $w$-RR technique, making it applicable to variables with more than two values. This is achieved through mechanisms such as $k$-RR~\cite{kairouz2014extremal} and $O$-RR~\cite{kairouz2016discrete}. In the $k$-RR method, the original data is preserved with probability $p$ and is flipped to any other value with a probability of $\frac{1-p}{n-1}$,  where $n$ is the number of possible values. This mechanism ensures that the perturbation is uniformly distributed across all possible values, thereby preserving the privacy of the data. Building on this principle, Kairouz et al.~\cite{kairouz2016discrete} further proposed the $O$-RR method, which handles cases where the possible values of a variable are unknown by introducing hash mapping and grouping operations. Hash mapping allows the method to focus on the encoded values rather than the original strings, eliminating the need to pre-collect a list of candidate strings. Additionally, the use of grouping operations reduces the probability of hash mapping value collisions. This combination of techniques enhances the robustness and flexibility of differential privacy in various data collection scenarios.

% The techniques discussed above are designed for discrete values. For continuous values, however, they should first be discretized to meet the fundamental requirement of the $w$-RR technique before privacy protection can be applied. This concept is primarily applied in the context of local differential privacy for mean statistics methods like MeanEst~\cite{duchi2013local,duchi2014privacy} and Harmony-mean~\cite{nguyen2016collecting}.

% \subsection{Noise Mechanisms}
% In conventional differential privacy approaches, data perturbation is achieved through various noise mechanisms, some of which are independent of each other. These include the Gaussian mechanism~\cite{dwork2014algorithmic}, Laplace mechanism~\cite{dwork2006calibrating}, 
% and Matrix Variate Gaussian (MVG)~\cite{chanyaswad2018mvg}.

% Although the mechanisms mentioned above do not directly apply to tensor-valued queries, our research is nevertheless grounded in the fundamental principle of preserving privacy through the injection of Gaussian or Laplace noise. The Gaussian mechanism introduces independent and identically distributed (i.i.d.) Gaussian noise, calibrated according to the $l_2$-sensitivity of the query, guarantees $(\epsilon, \delta)$-differential privacy. Similarly, by incorporating noise derived from the Laplace distribution, scaled according to the query function's $l_1$-sensitivity, the Laplace mechanism enforces strong $\epsilon$-differential privacy. Building upon these concepts, our mechanism is specifically tailored to address the complexities of tensor data. We demonstrate that our mechanism attains $\epsilon$-differential privacy for tensors, as detailed in Section III. This innovation bridges the gap between traditional differential privacy mechanisms and the sophisticated requirements of high-dimensional data, preserving privacy while maintaining utility.
% The Multivariate Gaussian (MVG) mechanism is tailored for matrix-valued queries and incorporates matrix-valued noise to ensure $(\epsilon, \delta)$-differential privacy. The $l_2$-sensitivity of the MVG mechanism is measured using the Frobenius norm, which quantifies the discrepancy between two neighboring matrices. 

% Mechanisms that derive their privacy guarantees from the above mechanisms are well-established, as exemplified by the composition schemes referenced in~\cite{dwork2014algorithmic,abadi2016deep,kairouz2015composition,lee2018concentrated,lecuyer2019privacy,yu2019differentially}, as well as privacy amplification through sampling as outlined in~\cite{beimel2014bounds}. Among these,~\cite{abadi2016deep} presents an innovative accounting approach for the Gaussian mechanism that minimizes the total additive noise while preserving the same level of privacy protection. In contrast, Kairouz et al.~\cite{kairouz2014extremal} propose an optimal composition scheme that is applicable to a broader range of noise distributions. The studies in~\cite{lee2018concentrated,yu2019differentially} provide dynamic accounting methods that adjust based on the algorithm's runtime convergence. Furthermore,~\cite{lecuyer2019privacy} ensures a global differential privacy guarantee in environments where datasets are continuously expanding.

\subsection{Learning With Differential Privacy}
A range of studies explored the integration of differential privacy techniques into machine learning models, addressing various privacy-preserving objectives. These efforts encompass differentially private individual data~\cite{chanyaswad2018mvg,wang2018not}, model outputs~\cite{papernot2018scalable}, model parameters~\cite{lee2018concentrated,lecuyer2019privacy,yu2019differentially,song2013stochastic,shokri2015privacy,bassily2014private,wang2017differentially}, and even objective functions~\cite{phan2016differential,zhang2017efficient}. Our focus is especially on machine learning applications in distributed systems, which often involve frequent transmission of high-dimensional or complex tensor-valued data like raw private data, extracted features, or model parameters. 
% The TLDP mechanism proposed in this paper is designed to protect private individual data, such as, outputs, and model parameters, especially when the data is structured in tensor format, as evidenced by the result of the final simulation experiments.

% \subsection{Summaries}
In summary, we consider our paper introduces a primitive local differential privacy scheme, which fits well in distributed computing systems but certainly can be applied in any other machine learning systems, where individual users' tensor-valued data is private and requires to be preserved.