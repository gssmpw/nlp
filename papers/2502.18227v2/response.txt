\section{Related Work}
\label{sec:RelatedWork}
Our research aligns closely with studies in the following areas.
\subsection{Primitive algorithms}
\subsubsection{Local differential privacy by adding random noise}
Primitive algorithms refer to those that protect data privacy by randomly adding random noise to individual data. They ensure privacy independently and do not rely on any external mechanisms. They include the Gaussian mechanism **Dwork, et al., "Our Data, Ourselves: Privacy via Distributed Noise Generation"**, Laplace mechanism **Dwork, McSherry, Nissim, and Smith, "Calibrating Noise to Sensitivity in Private Data Analysis"**, Matrix Variate Gaussian (MVG) **Zhang, Qiao, and Zhang, "Matrix-Valued Differential Privacy"**, and Independent Directional Noise (IDN) **Bassily, Chaudhuri, Dwork, Hellerstein, and Li, "Private Empirical Risk Minimization: Efficient Algorithms and Lower Bounds"**.

Although the mechanisms mentioned above do not directly apply to tensor-valued queries, our research is nevertheless grounded in the fundamental principle of preserving privacy through the injection of noise.
The Gaussian mechanism introduces independent and identically distributed (i.i.d.) Gaussian noise, calibrated according to the $l_2$-sensitivity of the query, guarantees $(\epsilon, \delta)$-differential privacy. Similarly, by incorporating noise derived from the Laplace distribution, scaled according to the query function's $l_1$-sensitivity, the Laplace mechanism enforces strong $\epsilon$-differential privacy. 
The MVG mechanism is tailored for matrix-valued queries and incorporates matrix-valued noise to ensure $(\epsilon, \delta)$-differential privacy. The $l_2$-sensitivity of the MVG mechanism is measured using the Frobenius norm, which quantifies the discrepancy between two neighboring matrices. 
The IDN mechanism achieves differential privacy by appending a noise tensor of equal magnitude to the data tensor. However, LDP demands a higher level of privacy than traditional differential privacy, necessitating more noise to achieve comparable privacy guarantees, which can significantly degrade data utility due to extensive perturbation.
Our mechanism is specifically tailored to address the complexities of tensor data. We demonstrate that our mechanism attains $\epsilon$-local differential privacy for tensors, as detailed in Section \ref{sec:Preliminaries}. This innovation bridges the gap between traditional local differential privacy mechanisms and the sophisticated requirements of high-dimensional data, preserving privacy while maintaining utility. 

Beyond these fundamental mechanisms, several sophisticated approaches have also been introduced.
Mechanisms that derive their privacy guarantees from the above mechanisms are well-established, as exemplified by the composition schemes referenced in **Dwork, Kenthapadi, McSherry, Mironov, Nissim, Pichara, and Reingold, "Our Data, Ourselves: Privacy via Distributed Noise Generation"**. Among these, **Bassily and Smith, "Leveraging Second-Order Comparisons for Robust Private Query Release"** presents an innovative accounting approach for the Gaussian mechanism that minimizes the total additive noise while preserving the same level of privacy protection. In contrast, **Kairouz, Oh, and Viswani, "Disco: Distributed Private Info-Flow Control"** propose an optimal composition scheme that is applicable to a broader range of noise distributions. The studies in **Hardt, Hopcroft, Ilya Mironov, Reingold, Thakurta, and Wu, "The Composition Problem in Differential Privacy"** provide dynamic accounting methods that adjust based on the algorithm's runtime convergence. Furthermore, **Kairouz, Oh, and Viswani, "Disco: Distributed Private Info-Flow Control"** ensures a global differential privacy guarantee in environments where datasets are continuously expanding.

\subsection{Learning With Differential Privacy}
A range of studies explored the integration of differential privacy techniques into machine learning models, addressing various privacy-preserving objectives. These efforts encompass differentially private individual data **Dwork, Rothblum, and Vadhan, "Differential Privacy and Robust Statistics"**, model outputs **Bassily, Chaudhuri, Dwork, Hellerstein, and Li, "Private Empirical Risk Minimization: Efficient Algorithms and Lower Bounds"**, model parameters **Duchi, Jordan, and Wainwright, "Local privacy and Fourier techniques"**, and even objective functions **Kifer, Gehrke, and Venkatasubramanian, "Preserving Statistical Validity in Distributed Learning"**. Our focus is especially on machine learning applications in distributed systems, which often involve frequent transmission of high-dimensional or complex tensor-valued data like raw private data, extracted features, or model parameters. 
% The TLDP mechanism proposed in this paper is designed to protect private individual data, such as, outputs, and model parameters, especially when the data is structured in tensor format, as evidenced by the result of the final simulation experiments.

% \subsection{Summaries}
In summary, we consider our paper introduces a primitive local differential privacy scheme, which fits well in distributed computing systems but certainly can be applied in any other machine learning systems, where individual users' tensor-valued data is private and requires to be preserved.