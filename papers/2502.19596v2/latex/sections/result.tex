This section presents the performance of individual and overall RAG components, with scalability tests assessing performance and latency trade-offs discussed in \ref{subsec:latency}.


\subsection{Reference Matching}
Table \ref{tab:reference_matching_result} provides an overview of the operation of the reference matching algorithm. On average, the answers are divided into about 3.2 segments, with the reference matching achieving an accuracy of around 88\%. The accuracy was evaluated using GPT-4-Turbo\footnote{\url{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo}}, and the prompt used for the evaluation is depicted in Figure \ref{fig:prompt_eval_ref} (\ref{subsubsec:rag_eval}).
% manual evaluation 들어가야함 (=> 이걸 gold testset으로 쓰기)
% -> 지금 100개 sampling 한거임 (sampling 방법도 고민해보기)
% LLM (Qwen vanilla, fine-tuned)과 우리방식의 performance 차이 비교
% -> retrieved docs 개수가 커질수록 우리가 잘하면 좋음 (n = 5, 10)
% -> 속도 비교
% -> Stress test (LLM은 n 늘리면 비용, 속도 문제가 있음)

\begin{table}[h!]
\centering
%\small
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
              & \textbf{Segments}                    & \textbf{Accuracy}                    \\ \midrule
\textbf{Avg.} & \multicolumn{1}{r}{3.2245 ($\pm$ 3.34)} & \multicolumn{1}{r}{0.8835 ($\pm$ 0.22)} \\ \bottomrule
\end{tabular}%
%}
\caption{Summary of the reference matching algorithm's performance, including the average number of segments per answer and the reference accuracy.}
\label{tab:reference_matching_result}
\end{table}
