\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/evaluation_pipeline_edit_v2.pdf}
    \caption{Final answer evaluation pipeline for the QA system, illustrating the scoring and ranking of model-generated responses using the given question, gold answer, and reference documents.}    
    \label{fig:evaluation_pipeline}
%\vspace{-1em}
\end{figure*}

\subsection{Models}
%% 3.3 섹션 아래 내용 정리해서 옮기기 
We use Qwen-2.5 \cite{qwen} as our backbone LLMs, one of the few multilingual models that officially supports Korean. Notably, there are currently no Korean-centric open-source LLMs with sufficient context length to address our requirements. We fine-tune Qwen 2.5 (14B, 72B) on our training dataset, performing full fine-tuning on the 14B model and applying Low-Rank Adaptation (LoRA) \citep{hu2021loralowrankadaptationlarge} for the 72B model. Experimental details including hyperparameters and GPU configurations, are in \ref{subsubsec:model_training}.
%, mentioned in Section \ref{sec:data_processing}, which comprises pairs $(q, a)$

\subsection{Evaluation}
\label{subsec:rag_evaluation}
\subsubsection{LLM-as-a-Judge}
As illustrated in Figure \ref{fig:evaluation_pipeline}, we employ the LLM-as-a-judge approach \citep{zheng2023judgingllmasajudgemtbenchchatbot} to evaluate the performance of both LLM-only and RAG models. For the LLM evaluation, we compare four model variations: the vanilla and fine-tuned versions of 14B and 72B models. GPT-4o\footnote{\url{https://platform.openai.com/docs/models/gpt-4o}} is used to rank anonymized models (A, B, C, and D) based on factual correctness, helpfulness, and informativeness. These metrics are chosen to evaluate distinct, non-overlapping dimensions:
\vspace{-0.5em}
\begin{itemize}
\setlength\itemsep{-0.1em}
    \item \textbf{Correctness} measures alignment with the gold answer, rewarding correct responses without hallucinations.
    \item \textbf{Helpfulness} assesses clarity and relevance in addressing key points, while avoiding unnecessary content.
    \item \textbf{Informativeness} evaluates the inclusion of relevant details or additional context that enhances completeness.
\end{itemize}

\noindent For RAG evaluation, we use the best-performing one among four models and compare its performance with and without RAG integration. Instead of pairwise comparisons, we employ single-answer evaluations, scoring responses (1–5) on three aspects. This approach offers a more detailed assessment, emphasizing how RAG impacts specific aspects of response quality. All the evaluation prompts used are detailed in \ref{subsubsec:eval_prompt}.

\subsubsection{Human Evaluation} \label{sec:result_humaneval}
%alignment between the human eval and the gpt eval 
To assess the reliability of the RAG evaluation using the LLM-as-a-judge method, we conducted a human evaluation to compare its alignment with the LLM evaluation. An expert evaluator, a native Korean automotive engineer, assessed 100 randomly selected and anonymized responses. These responses were generated by the 72B fine-tuned model, both with and without RAG integration. A screenshot of the human evaluation interface used is in shown Figure \ref{fig:humaneval_screen} (\ref{sec:appendix_humaneval}).

\subsection{Result}
\subsubsection{LLM-only Comparison}
\label{sec:rag_result}
Figure \ref{fig:4models_rank} illustrates the rank distribution and win-lose-tie comparison across four Qwen models. Among them, the 72B fine-tuned one shows the best performance, followed by the 72B vanilla, 14B fine-tuned, and 14B vanilla models. These results emphasize the critical impact of model size and demonstrate the significant effectiveness of fine-tuning, as fine-tuned models consistently outperform their vanilla counterparts.
% 모델 사이즈 이야기하기 72B > 14B
%%%정성적으로 어떤 부분/이유에서 이런 결과가 나온걸지 분석필요 .. vanilla의 경우 한자 포함 다수.. ft의 경우 거의 없음(구체적인 숫자 필요)

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/final_rank_distribution_and_win_lose_tie.png}
    \caption{LLM-only performance comparison: \textbf{(a)} Proportion of responses from the four models ranked 1st, 2nd, 3rd, or 4th, \textbf{(b)} Win, tie, and loss rates for selected model pairs, where a "win" indicates the first model in the pair outperformed the second model.}
    \label{fig:4models_rank}
\vspace{-1em}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=1.0\linewidth]{latex/figures/winlosetie.png}
%     \caption{Win-rate comparison between selected model pairs. The chart shows the win-rate of the latter against the former. Fine-tuned models consistently outperform their vanilla counterparts. The result also shows that the larger models generally outperform compared to the smaller models.}
%     \label{fig:winlosetie}
% \end{figure}

\subsubsection{LLM-only vs. RAG}
We compare the performance of the 72B fine-tuned model in its LLM-only (non-RAG) and RAG-integrated versions. Table \ref{tab:rag-nonrag-avg} compares the average scores for correctness, helpfulness, and informativeness between the two versions, while Figure \ref{fig:rag_ft_model_winrate} shows the win rates. The results clearly demonstrate that integrating RAG with the LLM improves factual correctness by reducing hallucinations and provides more helpful and informative answers.

\begin{table}[h!]
\centering
\small
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{non-RAG} & \textbf{RAG} \\
\midrule
\textbf{Correctness} & 2.18 & \textbf{4.12} \textcolor{blue}{(+1.94)} \\
\textbf{Helpfulness} & 2.52 & \textbf{4.19} \textcolor{blue}{(+1.67)} \\
\textbf{Informativeness} & 2.61 & \textbf{3.77} \textcolor{blue}{(+1.16)} \\
\bottomrule
\end{tabular}%
%}
\caption{Average scores (1 to 5 scale) for the model with and without RAG across three metrics. Detailed counts and score distributions are in Table \ref{tab:rag-nonrag-appendix} (\ref{subsubsec:rag_eval}).}
\label{tab:rag-nonrag-avg}
\vspace{-1.5em}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/comparison_rag_non_rag_edit_v2.pdf}
    \caption{Win-Tie-Lose comparison of the model with and without RAG across three evaluation metrics.}
    \label{fig:rag_ft_model_winrate}
\vspace{-1em}
\end{figure}

\subsubsection{Comparison with Human Evaluation}
Table \ref{tab:human_eval_rag_vs_no_rag} shows that human evaluation aligns with GPT evaluation, both assigning higher scores to the RAG model across all three aspects. Figure \ref{fig:heatmap} visualizes the differences in scoring tendencies between human and GPT evaluations. Notably, correctness and informativeness show an inverted pattern: human favors more polarized correctness scores (1 and 5), while GPT prefers mid-range (2 and 3). Additionally, human assigns higher helpfulness scores more frequently, as seen in the positive values for 4 and 5. While differences in score distributions exist between evaluators, these differences are independent of the specific models. Both evaluations consistently show that RAG outperforms non-RAG by achieving higher scores.
% Positive values (\textcolor{orange}{orange}) indicate that humans assigned a higher proportion of a given score, while negative values (\textcolor{blue}{blue}) indicate that GPT assigned that score more frequently.

\begin{table}[h!]
\centering
%\scriptsize
% \setlength{\tabcolsep}{3pt} 
% \renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|c|r|rrrrr}
\toprule
\textbf{Metric} & \textbf{Model} & \textbf{\begin{tabular}[c]{@{}c@{}}Avg. \\ Score\end{tabular}} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5}  \\
\midrule
\multirow{2}{*}{Correctness} & A & 2.54 & 34 & 19 & 20 & 13 & 14  \\
                          & B & \textbf{4.33} & 10 & 4  & 4  & 7  & 75  \\
\midrule
\multirow{2}{*}{Helpfulness} & A & 2.89 & 2  & 38 & 36 & 17 & 7  \\
                             & B & \textbf{4.22} & 3  & 9  & 6  & 27 & 55  \\
\midrule
\multirow{2}{*}{Informativeness} & A & 2.60 & 3  & 44 & 44 & 8  & 1   \\
                                 & B & \textbf{3.68} & 2  & 8  & 31 & 38 & 21  \\
\bottomrule
\end{tabular}%
}
\caption{Human evaluation result comparing non-RAG (anonymized as A) and RAG (anonymized as B).}
\label{tab:human_eval_rag_vs_no_rag}
\vspace{-1em}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/heatmap_edit_v2.png}
    \caption{Heatmap illustrating the differences in score distributions between human and GPT evaluations, calculated as the percentage of scores (ranging from 1 to 5) assigned by humans, subtracted by the corresponding percentages assigned by GPT. \textcolor{orange}{Positive values} indicate scores more frequently given by humans, while \textcolor{blue}{negative values} indicate scores more frequently given by GPT.}
    \label{fig:heatmap}
\vspace{-1em}
\end{figure}

