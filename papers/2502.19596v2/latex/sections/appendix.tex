\label{sec:appendix}

\subsection{Experimental Details}
\label{subsec:exp_details}

\subsubsection{Dataset}
\label{subsubsec:appendix_dataset}
We split the generated dataset at the chunk level, slide level for slides, and chapter level for the textbook, using an 8:1:1 ratio for training, validation, and testing. Table \ref{tab:dataset_split} summarizes the dataset splits, while token length statistics for the training data are presented in Table \ref{tab:data_token_stat}.

\begin{table}[h!]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l|rrr|r}
\toprule
\multicolumn{1}{c|}{\textbf{Source}} & \multicolumn{1}{c|}{\textbf{Data}} & \multicolumn{1}{c}{\textbf{Train}} & \multicolumn{1}{c}{\textbf{Val}} & \multicolumn{1}{c|}{\textbf{Test}} & \multicolumn{1}{l}{\textbf{Total}} \\ \midrule
\multirow{2}{*}{Test Report}         & Chunk                              & 3,729                              & 466                              & 467                                & 4,662                              \\
                                     & Q\&A Pair                          & 47,660                             & 5,823                            & 5,919                              & 59,402                             \\ \midrule
\multirow{2}{*}{Meeting Report}      & Chunk                              & 705                                & 88                               & 89                                 & 882                                \\
                                     & Q\&A Pair                          & 6,144                              & 752                              & 800                                & 7,696                              \\ \midrule
\multirow{2}{*}{Textbook}            & Chunk                              & 64                                 & 8                                & 9                                  & 81                                 \\
                                     & Q\&A Pair                          & 1,182                              & 162                              & 161                                & 1,505                              \\ \bottomrule
\end{tabular}%
}
\caption{Dataset split statistics by source, detailing the distribution of chunks and Q\&A pairs.}
\label{tab:dataset_split}
\end{table}

\begin{table}[h!]
\centering
\tiny
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrr}
\toprule
                                             & \multicolumn{1}{c}{\textbf{Min}} & \multicolumn{1}{c}{\textbf{Mean}} & \multicolumn{1}{c}{\textbf{Max}} \\ \midrule
\multicolumn{1}{l|}{Question}       & 9                                & 35                                & 82                               \\
\multicolumn{1}{l|}{Answer}         & 6                                & 56                                & 271                              \\
\multicolumn{1}{l|}{Chunk (\texttt{Ver.0})} & 66                              & 843                               & 9,528                            \\
\multicolumn{1}{l|}{Chunk (\texttt{Ver.1})} & 106                              & 884                               & 9,528                            \\ \bottomrule
\end{tabular}%
}
\caption{Token length statistics in the training data, processed with the BGE M3 tokenizer. \texttt{Ver.0} refers to the raw chunk without the pre-pended header, while \texttt{Ver.1} includes header.}
\label{tab:data_token_stat}
\end{table}

\begin{figure}[h!]
    \centering
    \setlength{\fboxrule}{0.2pt}
    \fbox{\includegraphics[width=1.0\linewidth]{latex/figures/original_slide.png}}
    \caption{A screenshot of an original PPT slide from an internal report on vehicle crash collision tests by an automotive company in Korea.}
    \label{fig:original_slide}
\end{figure}

\subsubsection{Model Training}
\label{subsubsec:model_training}
The retriever was trained for 10 epochs, determined to be optimal, using 4 $\times$ 48GB RTX A6000 GPUs. The hyperparameters were set to the default configuration provided in the open-source implementation of BGE-M3. For the ranker, early stopping was employed by validating the model every 1,000 steps. Training was stopped if performance did not improve after 3 consecutive validations, and the best-performing model was selected. This training used 1 $\times$ 48GB RTX A6000 GPU with a learning rate of 1e-5, the AdamW optimizer \cite{DBLP:journals/corr/abs-1711-05101}, and a batch size of 4. For both components, validation was based on the MAP@10 metric evaluated on the validation set. To train our answer generation model, full fine-tuning was conducted for the 14B model with a batch size of 2, gradient accumulation steps of 64, a learning rate of 2e-5, and 3 training epochs using 3 $\times$ 80GB H100 GPUs. For the 72B model, LoRA was applied with a batch size of 1, gradient accumulation steps of 64, a learning rate of 2e-5, and 3 training epochs using 2 × 141GB H200 GPUs.
%For our answer generation task, we used Qwen 2.5 models, 14B and 72B. We selected Qwen because it is one of the few multilingual models that officially supports Korean, and there are currently no Korean-centric open-source LLMs with sufficient context length. For the 14B model, full fine-tuning was applied with a batch size of 2, gradient accumulation steps of 64, a learning rate of 2e-5, and trained for 3 epochs using 3 * 80GB * H100 GPUs. For 72B model. For 72B model, Low-Rank Adaptation (LoRA) \citep{hu2021loralowrankadaptationlarge} was applied with batch size of 1, gradient accumulation of 64, learning rate of 2e-5, and training epochs of 3, using 2 * 141GB * H200 GPUs.  

\subsection{Prompt Engineering}
\label{subsec:prompt}

\subsubsection{Prompts for Data Generation}
\label{subsubsec:data_prompt}
The prompt used to convert slides into Markdown text is shown in Figure \ref{fig:prompt_md}, and the prompt for Q\&A generation is depicted in Figure \ref{fig:prompt_qa}. An example of a generated Q\&A pair from a chunk is shown in Figure \ref{fig:generated_data}. An average of twelve Q\&A pairs were generated from a single chunk.

\begin{figure*}[h!]
    \small
    \begin{tcolorbox}[width=\textwidth, colback=white, colframe=black, title=Prompt]

    1. Examine the provided image of a PowerPoint slide from a car collision safety report.

    2. Convert all content into markdown format, preserving the original structure and information.

    3. Use a single hashtag \( \# \) for section headings only. Do not use any variation of hashtags for anything else. Only use one hashtag for each section heading. All heading MUST have section content underneath it. There should never be two headings in a row without content in between.

    4. Each graphic element type should have a section heading. This means tables, graphs, and images should each have their own section heading. Text should be included in the appropriate section or if different enough, have its own section.

    5. Structure your output as follows:

    \hspace{1em}$<$ markdown\_conversion $>$

    \hspace{2em} \# Heading appropriate for section content underneath
    
    \hspace{2em}[section content goes here]
    
    \hspace{1em}$<$ markdown\_conversion $>$

    6. For each element type:

    \hspace{1em}Text:

    \hspace{2em}- Convert to markdown, maintaining structure, bullet points, and numbering.
    
    \hspace{2em}- Include all text: titles, subtitles, and body.

    \hspace{1em}Tables:
    
    \hspace{2em}- Convert to markdown table format.
    
    \hspace{2em}- Maintain original content and structure.
    
    \hspace{2em}- Provide a detailed interpretation after each table.
    
    \hspace{2em}- For images in cells, describe them within the appropriate cell in markdown.

    \hspace{1em}Graphs and Images:
    
    \hspace{2em}- Describe in detail: type, labels, data representation, trends.
    
    \hspace{2em}- Explain significance in the context of car collision safety under it.

    \hspace{1em}Remember:
    
    \hspace{2em}- Do not use unnecessary affirmations or filler phrases.
    
    \hspace{2em}- Do not include personal opinions or anecdotes.
    
    \hspace{2em}- Use markdown for code snippets if applicable.

    \end{tcolorbox}
\caption{Prompt used to convert a presentation slide into markdown text.}    
\label{fig:prompt_md}
\end{figure*}

\begin{figure*}[h!]
    \small
    \begin{tcolorbox}[width=\textwidth, colback=white, colframe=black, title=Prompt]
        You are an AI assistant tasked with creating Q\&A sets based on a car crash safety test report. Your goal is to generate question-answer pairs that can be used for training a language model. Follow these instructions carefully: 
        
        1. Read the following crash test report: <crash\_test\_report> \{crash\_test\_report\} </crash\_test\_report>
        
        2. Extract the test information from the head of the report. It should contain the following details: Test ID, PRJ ID, Region, Test Name, Stage, Purpose
        
        3. Create Q\&A pairs based on the information provided in the report and the test information. The number of pairs should be sufficient to cover the report's content adequately. Follow these guidelines:
        
        \hspace{1em} a. Focus on creating questions that a human would naturally ask about the crash test results, vehicle performance, or safety implications.
        
        \hspace{1em} b. Create a diverse set of questions covering different aspects of the crash test, such as test conditions, vehicle performance, safety features, and results.           
        
        \hspace{1em} c. Ensure that all questions can be answered solely based on the information provided in the report and test information.
        
        \hspace{1em} d. Include a mix of factual questions and more analytical questions.
        
        \hspace{1em} e. Avoid creating questions that require information not present in the report or test information.
        
        \hspace{1em} f. Create meaningful and insightful questions that provide valuable information about the crash test and its results.
        
        \hspace{1em} g. Write all questions and answers in Korean.
        
        \hspace{1em} h. Incorporate the test information (Test ID, PRJ ID, Region, Test Name, Stage, Purpose) into your questions where relevant, but don't create questions that simply ask for this information directly.
        
        \hspace{1em} j. When referring to the test in questions, use the following format: "[PRJ ID] [Region] [Stage] [Test Name]" (e.g., "HDC Domestic Proto 64kph Offset 40\% (LH) Test"). Do not use phrases like "The 64kph Offset 40\% (LH) test of the HDC prototype conducted in Korea."
        
        \hspace{1em} k. When referring to the region in questions, use "Domestic" instead of "Korea" and "North America (USA)" instead of "USA."
            
        4. Format your output as follows:
        
        \hspace{2em} <qa\_pairN>
        
        \hspace{3em} <question> Write your N th question here</question>
        
        \hspace{3em} <answer>Write the corresponding answer here</answer>
        
        \hspace{2em} </qa\_pairN>
                
        5. Additional tips for creating meaningful questions:
        
        \hspace{1em} a. Focus on questions that relate to vehicle safety, performance, and test outcomes rather than technical details of the test setup.
        
        \hspace{1em} b. Consider creating questions about specific safety features mentioned in the report and their effectiveness.
        
        \hspace{1em} c. Include questions about the overall safety rating or performance of the vehicle, if such information is provided.
        
        \hspace{1em} d. Ask about any notable findings or unusual results mentioned in the report.
        
        \hspace{1em} e. Create questions that compare the results to safety standards or expectations, if such information is available.
        
        6. Create questions that can be definitively answered based solely on the information provided in the crash test report and test information. Do not include any information or assumptions that are not explicitly stated. Ensure that your questions and answers are meaningful, insightful, and provide valuable information about the crash test and its results.
        
        7. Before finalizing your Q\&A pairs, review them to ensure they are diverse, meaningful, and directly relevant to the crash test report and test information provided. Make sure that each question includes the Region and Test Name along with the PRJ ID and Stage where appropriate, using the format specified in guideline 3j.
        
        8. Use the following explanations for the test information components:
        
        \hspace{1em} - Test ID: A unique test number used to distinguish tests (this number is rarely included directly in questions).
        
        \hspace{1em} - PRJ ID: The name of the vehicle model (project name) under development.
        
        \hspace{1em} - Region: The country where the crash test is conducted.
        
        \hspace{1em} - Test Name: The type of crash test, such as frontal test or side test.
        
        \hspace{1em} - Stage: The development stage.
        
        \hspace{1em} - Purpose: The purpose of conducting the test.
        
        9. The number of Q\&A pairs you create should be flexible based on the content of the report. If the report contains a lot of information, you may create up to 30 pairs. If the report has limited content, create fewer pairs, but ensure that all relevant information from the report is covered in the Q\&A sets. The goal is to adequately represent the report's content without creating redundant or overly similar questions.
        
        10. In addition to the standard question-answer format, create some Q\&A pairs using the following alternative format:
        
        \hspace{1em} a. Ask about specific phenomena, results, or improvements mentioned in the report.
        
        \hspace{1em} b. In the answer, specify which test produced these results or phenomena.
        
        \hspace{1em} c. Use the following structure for these alternative Q\&A pairs:
        
        \hspace{2em} <question> Ask about a specific phenomenon, result, or improvement.</question>
        
        \hspace{2em} <answer>[PRJ ID] [Region] [Stage] [Test Name] The phenomenon/result/improvement was observed during the test. (Additional explanation) </answer>
        
        \hspace{1em} Example:
        
        \hspace{2em} <question> Has there been a case where moving the weld point location increased the survival space?</question>
        
        \hspace{2em} <answer>In the HDC North America (USA) P2 60kph Side 90 (LH) IIHS test, a measure was implemented to increase survival space by moving the weld point location. As a result, the survival space rating is expected to improve from 'A' to 'G'.</answer>
        
        Generate your Q\&A pairs now, following all the guidelines and instructions provided above. Include a mix of standard and alternative format Q\&A pairs to provide a comprehensive representation of the crash test report's content.
    \end{tcolorbox}
\caption{Prompt used to generate Q\&A pairs from a given chunk}    
\label{fig:prompt_qa}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/generated_data_censored_v2.png}
    \caption{An example of a generated QA pair from the chunk, originally in Korean and translated into English.}
    \label{fig:generated_data}
\end{figure*}

\subsubsection{Prompts for Final Answer Evaluation}
\label{subsubsec:eval_prompt}
Our prompts for evaluating final answers from LLM-only models and comparing LLM-only and RAG models are in Figure \ref{fig:prompt_eval_llm} and \ref{fig:prompt_eval_rag}, respectively.

\begin{figure*}[h!]
    \small
    \begin{tcolorbox}[width=\textwidth, colback=white, colframe=black, title=Prompt]
You are an evaluator tasked with ranking Korean generative model outputs based on three criteria: accuracy, fluency, and helpfulness.

\hspace{1em}1. **Accuracy**: Evaluate how closely each response aligns with the given gold answer. Responses that are factually correct and contain relevant information rank higher. 

\hspace{1em}2. **Fluency**: Evaluate the quality of the language. Responses written in natural and grammatically correct Korean rank higher. Responses with awkward phrasing, grammar mistakes, or written in other languages rank lower.

\hspace{1em}3. **Helpfulness**: Evaluate how effectively the response answers the question. Clear and informative answers rank higher, while incomplete or overly verbose responses rank lower.


\textbf{Instructions:}


- Assign **ranks** (1 to 4) to the four responses (A, B, C, D), where 1 is the best and 4 is the worst.

- If two or more responses are equally good, assign them the same rank.

- Provide ranks in the following format:

\hspace{1em}A: [rank]  

\hspace{1em}B: [rank]  

\hspace{1em}C: [rank]  

\hspace{1em}D: [rank]  

\textbf{Examples:}

1. If all responses are equally good:  

   \hspace{1em}A: 1  
   
   \hspace{1em}B: 1  
   
   \hspace{1em}C: 1  
   
   \hspace{1em}D: 1  

2. If A is the best, B and C are tied for second place, and D is the worst:  

   \hspace{1em}A: 1  
   
   \hspace{1em}B: 2  
   
   \hspace{1em}C: 2  
   
   \hspace{1em}D: 4  

3. If all responses have distinct ranks: 

   \hspace{1em}A: 1  
   
   \hspace{1em}B: 2  
   
   \hspace{1em}C: 3  
   
   \hspace{1em}D: 4  

\textbf{Input:}

Question: \{question\}

Gold Answer: \{correct\_answer\}

A: \{response\_a\}

B: \{response\_b\}

C: \{response\_c\}

D: \{response\_d\}

    \end{tcolorbox}
\caption{Prompt used to compare answers from LLM-only models}
\label{fig:prompt_eval_llm}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h!]
    \small
    \begin{tcolorbox}[width=\textwidth, colback=white, colframe=black, title=Prompt]
You are tasked with evaluating two responses (A and B) generated by generative models for a given question and its correct gold answer. Evaluate each response based on the following three criteria, and assign a score between 1 and 5 for each criterion:

\hspace{1em}1. **Accuracy**: How well does the response align with the gold answer without hallucination? Factual correctness is essential.  

\hspace{1em}2. **Helpfulness**: How helpful is the response in answering the question? A helpful response addresses the key points clearly and avoids unnecessary content.  

% \hspace{1em}3. **Fluency**: How natural and grammatically correct is the response in Korean? Awkward phrasing, grammar mistakes, or foreign languages lower the score.

\hspace{1em}3. **Informativeness**: How informative is the response in addressing the question? An informative response includes relevant details or additional context derived from the provided sources, enhancing the overall completeness and clarity of the answer.


\textbf{Instructions:}


- Assign a score between 1 and 5 for each criterion (1: Poor, 5: Excellent).  

- Provide only the scores for each aspect without any explanations.

- Use the following exact format to provide scores:

\hspace{1em}A\_Accuracy: [ ]  

\hspace{1em}B\_Accuracy: [ ]  

\hspace{1em}A\_Helpfulness: [ ]  

\hspace{1em}B\_Helpfulness: [ ]  

\hspace{1em}A\_Informativeness: [ ]  

\hspace{1em}B\_Informativeness: [ ]  

\textbf{Input:}

Question: \{question\}  

Gold Answer: \{correct\_answer\}  

Response A: \{response\_a\}  

Response B: \{response\_b\}  
    \end{tcolorbox}
\caption{Prompt used to compare answers from LLM-only and RAG models}
\label{fig:prompt_eval_rag}
\end{figure*}

%\subsection{Latency Test} 
%\label{subsec:latency}
%To assess the scalability of each component in our RAG system relative to its performance, we compared the response latency and performance across different component combinations, as shown in Figure \textcolor{red}{TBU}. The experiments were conducted in the following configurations:
%\vspace{-0.5em}
%\begin{itemize}
%\setlength\itemsep{-0.3em}
%    \item Retrieval-only vs. Re-ranking after Retrieval
%    \item LLM-only vs. LLM with RAG
%\end{itemize}

\subsubsection{Prompts for Reference Matching}
\label{subsubsec:ref_prompt}
The prompt used for LLM reference matching is shown in Figure \ref{fig:prompt_ref}.

\begin{figure*}[h!]
    \small
    \begin{tcolorbox}[width=\textwidth, colback=white, colframe=black, title=Prompt]
    
    You are an expert tasked with analyzing user questions, an answer split into sentences, and a list of 5 related chunks. Your job is to identify the single most relevant chunk for each sentence based on sentence indices provided. Map the segments to relevant chunks as follows: \\
    
    Example:
    
    \hspace{1em}sentence\_idx [0,1,2]: chunk 0
    
    \hspace{1em}sentence\_idx [3,4]: chunk 3
    
    \hspace{1em}sentence\_idx [5]: chunk 4 \\
    
    Follow the format above and do not provide additional explanations.
    
    \hspace{1em}Question: \{question\}
    
    \hspace{1em}Answer: \{Answer with sentence\_idxs\}
    
    \hspace{1em}Documents: \{chunks\}
    
    \end{tcolorbox}
\caption{Prompt used for LLM reference matching}
\label{fig:prompt_ref}
\end{figure*}

\subsection{Further Analysis}
\label{subsec:further_analysis}

\subsubsection{TS+TCL Effect on Ranker Training }
\label{subsubsec:ts_tcl}
Table \ref{tab:ranker_tcl_disabled} shows the re-ranking performance without TS+TCL training. When compared to the performance with TS+TCL training (Table \ref{tab:ranker_performance}), there is a noticeable decline across all metrics.

\begin{table}[h!]
\centering
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|rrrr}
\toprule
\textbf{$k$} & \textbf{MAP@1}                              & \textbf{MAP@5}                              & \textbf{MAP@10}                             & \textbf{Success@5}                           \\ \midrule
10 & \begin{tabular}[c]{@{}r@{}}0.6349\\ \footnotesize(± 0.013)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7341\\ \footnotesize(± 0.009)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7394\\ \footnotesize(± 0.009)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.8801\\ \footnotesize(± 0.002)\end{tabular} \\
20 & \begin{tabular}[c]{@{}r@{}}0.6271\\ \footnotesize(± 0.017)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7276\\ \footnotesize(± 0.013)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7344\\ \footnotesize(± 0.012)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.8769\\ \footnotesize(± 0.006)\end{tabular} \\
30 & \begin{tabular}[c]{@{}r@{}}0.6237\\ \footnotesize(± 0.019)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7243\\ \footnotesize(± 0.014)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7314\\ \footnotesize(± 0.014)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.8748\\ \footnotesize(± 0.008)\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{Re-ranker performance with TS+TCL training disabled.}
\vspace{-1em}
\label{tab:ranker_tcl_disabled}
\end{table}


\subsubsection{Analysis of Retrieval \& Ranking Phase} \label{subsubsec:analysis_retrieval_ranking} 
Figure \ref{fig:error_analysis_retrieval} and \ref{fig:error_analysis_ranking} present real-world examples of two error types: (a) cases where the top-retrieval process failed, and (b) cases where retrieval succeeded but the ranker's top-1 result did not match the gold context.

\begin{figure*}[h!]
    \centering
    \small
    \resizebox{\textwidth}{!}{% 
        \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\textbf{Question:} Were there cases where the body rating significantly decreased?} \\ \hline
        \textbf{Model Top-1 Chunk} & \textbf{Gold Chunk} \\ \hline
        \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
        ---\\
        Region: USA\\
        Test Name: 64kph Small Overlap 25\% (RH)\\
        Stage: T-Car\\
        Purpose: Evaluation of HDC Small Overlap Improvements\\
        ---\\ \\
        \textbf{Test Conditions}\\
        The table below compares the test conditions for the two trials of HDC. In the second trial, the speed slightly increased, the weight decreased, and the impact location shifted.\\
        ...\\ \\
        \textbf{Evaluation Results}\\
        The evaluation results indicate a decline in ratings between the first and second tests.\\
        Target: Overall rating - Good (G), Body rating - Good (G)\\
        \textcolor{blue}{1st Test: Overall rating - G+0, Body rating - Acceptable (A)}\\
        \textcolor{blue}{2nd Test: Overall rating - A+1, Body rating - Marginal (M)}\\
        All other ratings, including restraint/dummy behavior and passenger injuries, remained at "Good" (G) for both tests.\\
        ...
        \end{tabular}} & 
        \begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
        ---\\
        Region: USA\\
        Test Name: 64kph Small Overlap 25\% (RH)\\
        Stage: Proto\\
        Purpose: Development of GTC Proto Performance.\\
        ---\\ \\
        \textbf{Body Deformation Analysis}\\
        The following table shows the deformation measurements at various vehicle sections during a crash test. The red line represents the evaluation model, while the blue line represents the baseline model (P2).Measurements are shown in centimeters and are categorized into 'GOOD', 'ACCEPTABLE', 'MARGINAL', and 'POOR' ratings.\\
        ... \\ \\
        \textcolor{blue}{\textbf{Body Deformation Table}}\\
        \{Table\} \\
        The table provides a detailed breakdown of deformation measurements for each section of the vehicle body.\\
        \textcolor{blue}{The baseline model (P2) achieved an overall rating of 'G' (Good), while the evaluation model received an overall rating of 'A' (Acceptable).}\\
        ...
        \end{tabular} \\ \hline
        \end{tabular}
    }
    \caption{Comparison of Retriever Top-1 Result and Gold Chunk for the Question \textit{"Were there cases where the body rating significantly decreased?"}. \textcolor{blue}{The blue text} highlights the parts relevant to the question, demonstrating that the top-1 retrieved chunk is as acceptable as the gold chunk.}
    \label{fig:error_analysis_retrieval}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \small
    \resizebox{\textwidth}{!}{% 
        \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Question:} What differences are observed in the head acceleration graphs for the driver and passenger seats \\ during the HDC P9 60kph side 90° (LH) AEMDB test?\end{tabular}} \\ \hline
        \textbf{Model Top-1 Chunk} & \textbf{Gold Chunk} \\ \hline
        \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
        ---\\
        Region: Europe\\
        Test Name: 60kph Side 90° (LH) AEMDB\\
        Stage: P9\\
        Purpose: Performance evaluation of HDC P9\\
        ---\\ \\
        \textbf{\textcolor{blue}{Driver Seat Head Acceleration Graph}} \\
        \{Table\} \\
        The graph illustrates the head acceleration of the driver seat over time. \textcolor{blue}{The maximum acceleration reaches approximately 40G at 0.05 seconds, followed by a sharp decline and subsequent fluctuations.} \\
        ... \\ \\
        \textbf{\textcolor{blue}{Passenger Seat Head Acceleration Graph}} \\
        \{Table\} \\
        The passenger seat head acceleration graph shows a delayed response compared to the driver seat. \textcolor{blue}{The maximum acceleration is lower, at approximately 30G. While the overall impact duration is similar, the acceleration pattern differs slightly.} \\
        ...
        \end{tabular}} & 
        \begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
        ---\\
        Region: Europe \\
        Test Name: 60kph Side 90° (LH) AEMDB \\
        Stage: P9\\
        Purpose: Performance evaluation of HDC P9\\
        ---\\ \\
        \textbf{\textcolor{blue}{Driver Seat Head Acceleration Graph}}\\
        \{Table\}\\
        The graph illustrates the head acceleration of the driver seat over time. The maximum acceleration reaches approximately 40G at 0.05 seconds, followed by a sharp decrease and subsequent fluctuations.
        ... \\ \\
        \textbf{\textcolor{blue}{Passenger Seat Head Acceleration Graph}}\\
        \{Table\}\\
        \textcolor{blue}{The passenger seat head acceleration graph shows a delayed response compared to the driver seat. The maximum acceleration is lower, at approximately 30G. While the overall impact duration is similar, the acceleration patterns differ slightly.}
        ...
        \end{tabular} \\ \hline
        \end{tabular}
    }
    \caption{Comparison of Re-ranker Top-1 Result and Gold Chunk for the Question \textit{"What differences are observed in the head acceleration graphs for the driver and passenger seats during the HDC P9 60kph side 90° (LH) AEMDB test?"}. \textcolor{blue}{The blue text} highlights the parts relevant to the question, demonstrating that the top-1 ranked chunk is as acceptable as the gold chunk.}
    \label{fig:error_analysis_ranking}
\end{figure*}


%\subsubsection{Efficacy of Fine-tuning}\label{sec:appendix_efficacyofft}
%Figure \ref{fig:winlosetie} presents the win-rate comparison among selected model pairs. Notably, the first two charts on the left highlight the impact of fine-tuning, with fine-tuned models outperforming their base counterparts in over 60\% of cases.

%\begin{figure}[h!]
%    \centering
    %\includegraphics[width=1.0\linewidth]{latex/figures/winlosetie_edit.png}
    %\caption{Win-rate comparison between selected model pairs. The chart shows the win-rate of the latter against the former. Fine-tuned models consistently outperform their vanilla counterparts. The result also shows that the larger models generally outperform compared to the smaller models.}
    %\label{fig:winlosetie}
%\end{figure}


\subsubsection{LLM-only vs. RAG}
\label{subsubsec:rag_eval}
Table \ref{tab:rag-nonrag-appendix} presents detailed counts and score distributions from GPT-4o, comparing RAG and non-RAG. 

\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llll}
\toprule
\textbf{Metric} & \textbf{Score} &\textbf{Non-RAG} & \textbf{RAG} \\
\midrule
\multirow{6}{*}{\textbf{Correctness}} 
 & 1 & 1,640 & 388 \\
 & 2 & 3,166 & 579 \\
 & 3 & 1,457 & 705 \\
 & 4 & 430 & 1,359 \\
 & 5 & 187 & 3,849 \\
 \midrule
& Avg. Score & 2.1799 & \textbf{4.1195} \\
 \midrule
 \multirow{6}{*}{\textbf{Helpfulness}} 
 & 1 & 758 & 154 \\
 & 2 & 2,819 & 522 \\
 & 3 & 2,422 & 868 \\
 & 4 & 722 & 1,649 \\
 & 5 & 159 & 3,687 \\ \midrule
 & Avg. Score & 2.5211 & \textbf{4.1908} \\ \midrule
  \multirow{6}{*}{\textbf{Informativeness}} 
 & 1 & 461 & 142 \\
 & 2 & 2,764 & 564 \\
 & 3 & 2,789 & 1,609 \\
 & 4 & 756 & 2,974 \\
 & 5 & 110 & 1,591 \\ \midrule
 & Avg. Score & 2.6061 & \textbf{3.7715} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of the Non-RAG and RAG models, reporting the distribution of scores (1--5) assigned to each model's outputs.}
\vspace{-1em}
\label{tab:rag-nonrag-appendix}
\end{table}


% \begin{table*}[hb!]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{p{1.5cm}p{2cm}p{2cm}p{3cm}}
% \toprule
% \textbf{Metric} & \textbf{Score} & \textbf{Non-RAG} & \textbf{RAG} \\
% \midrule
% \multirow{6}{*}{\textbf{Accuracy}} 
%  & 1 (Count)  & 1,289  & 326 \\
%  & 2   & 3,266  & 669 \\
%  & 3   & 1,689  & 748 \\
%  & 4   & 496   & 1,687 \\
%  & 5   & 140   & 3,450 \\
%  \hline
%  & \textbf{Avg. Score} & 2.2634 & \textbf{4.0561} \textcolor{blue}{(+1.7927)} \\
% \midrule
% \multirow{6}{*}{\textbf{Helpfulness}} 
%  & 1 (Count) & 454   & 163 \\
%  & 2   & 2,995  & 514 \\
%  & 3   & 2,607  & 931 \\
%  & 4   & 715   & 1,829 \\
%  & 5   & 109   & 3,443 \\
%  \hline
%  & \textbf{Avg. Score} & 2.5683 & \textbf{4.1446} \textcolor{blue}{(+1.5763)} \\
% \midrule
% \multirow{6}{*}{\textbf{Fluency}} 
%  & 1 (Count)  & 0     & 17 \\
%  & 2   & 8     & 76 \\
%  & 3   & 41    & 208 \\
%  & 4   & 2,335  & 1,664 \\
%  & 5   & 4,496  & 4,915 \\
%  \hline
%  & \textbf{Avg. Score} & 4.6452 & \textbf{4.6547} \textcolor{blue}{(+0.0095)} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Comparison of the Non-RAG and RAG models for Accuracy, Helpfulness, and Fluency metrics. The table reports the distribution of scores (1–5) assigned to each model's outputs, with the average scores provided for each metric. The results are based on evaluations over a test set of 6,880 samples.}
% \label{tab:rag-nonrag-appendix}
% \end{table*}



% Why did we set the top document number to 5/6/7?
% Qwen : The current config.json is set for context length up to 32,768 tokens. (without yarn)

% 1. Top 5 

% Mean Token Count: 5352.3755813953485
% Median Token Count: 4664.5
% Min Token Count: 1248
% Max Token Count: 41327

% Total (Test set) rows: 6880
% Rows exceeding 28,000 tokens: 33
% Percentage exceeding 28,000 tokens: 0.48%



% 2. Top 6

% Mean Token Count: 6416.370930232558
% Median Token Count: 5604.0
% Min Token Count: 1685
% Max Token Count: 46822

% Total rows: 6880
% Rows exceeding 28,000 tokens: 57
% Percentage exceeding 28,000 tokens: 0.83%
% Rows exceeding 30,000 tokens: 47
% Percentage exceeding 30,000 tokens: 0.68%


% 3. Top 7
% Mean Token Count: 7471.698837209302
% Median Token Count: 6584.5
% Min Token Count: 2099
% Max Token Count: 52706

% Total rows: 6880
% Rows exceeding 28,000 tokens: 80
% Percentage exceeding 28,000 tokens: 1.16%
% Rows exceeding 30,000 tokens: 71
% Percentage exceeding 30,000 tokens: 1.03%

% 4. Top 8
% Mean Token Count: 8534.886046511629
% Median Token Count: 7545.0
% Min Token Count: 2689
% Max Token Count: 57817
% Total rows: 6880
% Rows exceeding 28,000 tokens: 93
% Percentage exceeding 28,000 tokens: 1.35%
% Rows exceeding 30,000 tokens: 85
% Percentage exceeding 30,000 tokens: 1.24%

% 5. Top 10
% Mean Token Count: 10628.991424418604
% Median Token Count: 9465.0
% Min Token Count: 3991
% Max Token Count: 66573

% Total rows: 6880
% Rows exceeding 28,000 tokens: 120
% Percentage exceeding 28,000 tokens: 1.74%
% Rows exceeding 30,000 tokens: 112
% Percentage exceeding 30,000 tokens: 1.63%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis of Reference Matching}
\label{sec:appendix_ref}
An example where only a single chunk needs to be referred to answer the question is in Figure \ref{fig:reference_case_1}.

\begin{figure*}[h!]
\centering
\small
\renewcommand{\arraystretch}{1.5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccclc}
\toprule
\multicolumn{5}{c}{\textbf{Question:} What was the material of the back beam used in the HDC P5 40kph slope 30 (RH) test?}                                                                                                                                                                                                                   \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Answer}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Segment 1}}} & \textbf{Sentence 1} & \multicolumn{1}{l|}{The material of the back beam used in the HDC P5 40kph slope 30 (RH) test was aluminum.} & \multirow{2}{*}{\textbf{Chunk 1}} \\ \cline{3-4}
\multicolumn{1}{c|}{}                                 & \multicolumn{1}{c|}{}                                    & \textbf{Sentence 2} & \multicolumn{1}{l|}{This information can be found in the test conditions section.}                                               &                                                                             \\ \bottomrule
\end{tabular}%
}
\caption{An example where the question can be answered by referring to only a single chunk.}
\label{fig:reference_case_1}
\end{figure*}

\subsection{Human Evaluation Interface}
\label{sec:appendix_humaneval}
The human evaluation interfaces for assessing the retrieval and re-ranking phase, as well as the reliability of the RAG evaluation with LLM, are shown in Figures \ref{fig:humaneval_screen_retrieval_ranking} and \ref{fig:humaneval_screen}, respectively. The human evaluation interface used for annotating the evaluation dataset to aseess the reference matching task is shown in Figure \ref{fig:ref_annotation_tool}.

%%영어 번역 필요!!
\begin{figure*}[h!]
    \centering
    \setlength{\fboxrule}{0.5pt}
    \fbox{\includegraphics[width=1.0\linewidth]{latex/figures/screenshot_error_analysis_retrieval_ranker_censored.png}}
    \caption{A screenshot of the user interface used for human evaluation to assess the retrieval \& re-ranking phase.}
    \label{fig:humaneval_screen_retrieval_ranking}
\end{figure*}

%%영어 번역 필요!! 
\begin{figure*}[h!]
    \centering
    \setlength{\fboxrule}{0.5pt}
    \fbox{\includegraphics[width=1.0\linewidth]{latex/figures/humaneval_screenshot_edit.png}}
    \caption{A screenshot of the user interface used for human evaluation to assess the reliability of the RAG evaluation with LLM.}
    \label{fig:humaneval_screen}
\end{figure*}

%%영어 번역 필요!! 
\begin{figure*}[h!]
    \centering
    \setlength{\fboxrule}{0.5pt}
    \fbox{\includegraphics[width=1.0\linewidth]{latex/figures/ref_annotation_tool_censored.png}}
    \caption{A screenshot of the user interface used for annotating the data to evaluate the reference matching task.}
    \label{fig:ref_annotation_tool}
\end{figure*}
