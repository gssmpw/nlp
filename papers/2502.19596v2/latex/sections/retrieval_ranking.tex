\subsection{Models}
For retriever training, we use BGE-M3 \cite{chen-etal-2024-m3} as the backbone, a multilingual Encoder capable of processing Korean, and fine-tune it on our training dataset with the publicly available code\footnote{\url{https://github.com/FlagOpen/FlagEmbedding}}. For re-ranker training, we initialize the weights using the fine-tuned retriever. Further experimental details are in \ref{subsubsec:model_training}.

\subsection{Evaluation}
We evaluate the retriever and re-ranker using Mean Average Precision (MAP@k) and Success@k \cite{Manning2008IR}. MAP@k measures the ranking quality by averaging precision over relevant results up to rank $k$, while Success@k indicates the proportion of queries with at least one relevant result in the top $k$. The high Success@k score indicates that relevant chunks are retrieved, while the improved MAP@k highlights better ranking of those retrieved chunks. The evaluation is conducted using test set questions, but the chunk pool for retrieval included all splits (training, validation, and test) to ensure sufficient data and avoid biases from the limited test set size.

\subsection{Result}
\label{sec:retriever_ranker_result}
Table \ref{tab:retriever_performance} shows that prepending a header to each chunk—adding only a small number of tokens (see Table \ref{tab:data_token_stat})—significantly enhances retrieval performance. Fine-tuning the BGE-M3 model on \texttt{Ver.1} (BGE-FT) further improves results notably, demonstrating the importance of task-specific model adaptation to optimize performance. Table \ref{tab:ranker_performance} shows that the re-ranking phase notably enhances the precision of retrieved information. The results indicate that ranking performance is optimal when the number of retrieved chunks is limited to 10. Despite the constraints of retriever failure, this improves retrieval results by about $+$4\% in terms of MAP@1. In addition, incorporating TS+TCL during training boosts ranking performance (See \ref{subsubsec:ts_tcl}).

%\begin{figure}[t!]
%    \centering
%    \includegraphics[width=1.0\linewidth]{latex/figures/retriever_ranker.png}
%    \caption{Ranking performance across retrieval depths for the top 10, 20, and 30 results.}
%    \label{fig:retriever_ranker}
%\end{figure}

\begin{table}[h!]
\centering
%\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cllrrr}
\toprule
\textbf{Model}                                                                              & \multicolumn{1}{c}{\textbf{Train}}           & \multicolumn{1}{c}{\textbf{Test}} & \multicolumn{1}{c}{\textbf{MAP@1}} & \multicolumn{1}{c}{\textbf{MAP@5}} & \multicolumn{1}{c}{\textbf{MAP@10}} \\ \midrule
\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BGE-\\ Vanilla\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{N/A}}    & \multicolumn{1}{l|}{\texttt{Ver.0}}       & 0.1978                             & 0.2672                             & 0.2766                              \\
\multicolumn{1}{c|}{}                                                                       & \multicolumn{1}{l|}{}                        & \multicolumn{1}{l|}{\texttt{Ver.1}}       & 0.2991                             & 0.3978                             & 0.4091                              \\
\midrule
\multicolumn{1}{c|}{\multirow{1}{*}{BGE-FT}} & \multicolumn{1}{l|}{\multirow{1}{*}{\texttt{Ver.1}}} & \multicolumn{1}{l|}{\texttt{Ver.1}}       & \textbf{0.6048}                             & \textbf{0.7111}                             & \textbf{0.7175}                              \\ \bottomrule
\end{tabular}%
}
\caption{Retrieval performance comparison between models across different data configurations. \texttt{Ver.0} refers to the raw chunk without the prepended header, while \texttt{Ver.1} includes header.}
\label{tab:retriever_performance}
\vspace{-1em}
\end{table}

\begin{table}[h!]
\centering
\small
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|rrrr} % Unified column alignment
\toprule
\textbf{$k$} & \textbf{MAP@1}                              & \textbf{MAP@5}                              & \textbf{MAP@10}                             & \textbf{Success@5}                           \\ \midrule
10 & \textbf{\begin{tabular}[c]{@{}r@{}}0.6444\\ \scriptsize(± 0.014)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}r@{}}0.7416\\ \scriptsize(± 0.009)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}r@{}}0.7464\\ \scriptsize(± 0.009)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}r@{}}0.8839\\ \scriptsize(± 0.004)\end{tabular}} \\
20 & \begin{tabular}[c]{@{}r@{}}0.6362\\ \scriptsize(± 0.017)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7353\\ \scriptsize(± 0.013)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7413\\ \scriptsize(± 0.013)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.8822\\ \scriptsize(± 0.008)\end{tabular} \\
30 & \begin{tabular}[c]{@{}r@{}}0.6328\\ \scriptsize(± 0.019)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7316\\ \scriptsize(± 0.015)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.7378\\ \scriptsize(± 0.015)\end{tabular} & \begin{tabular}[c]{@{}r@{}}0.8796\\ \scriptsize(± 0.011)\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{Ranking performance based on the number of retrieved chunks $k$. Instances that failed during the retrieval phase were scored as 0.}
\vspace{-1em}
\label{tab:ranker_performance}
\end{table}

\subsubsection{Multi-modal Specific Questions}
\label{sec:retriever_ranker_result_table_specific}
Table \ref{tab:ranker_performance_mutimodal_specific} summarizes the performance on questions requiring information from tables, graphs, and images. Compared to general questions (Section \ref{sec:retriever_ranker_result}), the results are notably lower, with the retriever facing significant challenges. Even at $k$ = 30, Success@k only reaches 83.61\%, underscoring a major bottleneck in the retrieval phase. Despite these challenges, the re-ranker proved effective, delivering a substantial performance improvement of approximately $+$15\% in MAP@1 at $k$ = 20. These results underscore the need for further advancements to better handle multi-modal questions.

\begin{table}[h!]
\centering
\small
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|c|rrr}
\toprule
\multicolumn{1}{c|}{\textbf{Phase}}                                    & \textbf{$k$} & \multicolumn{1}{c}{\textbf{MAP@1}} & \multicolumn{1}{c}{\textbf{MAP@5}} & \multicolumn{1}{c}{\textbf{MAP@10}} \\ \midrule
\begin{tabular}[c]{@{}l@{}}\textbf{Retrieval}\\ \scriptsize{w/ BGE-FT}\end{tabular} & N/A        & 0.3482                             & 0.4561                             & 0.4684                              \\ \midrule
\multirow{3}{*}{\textbf{Reranking}}                                    & 10         & 0.4723                             & 0.5587                             & 0.5630                              \\
                                                                       & 20         & \textbf{0.4949}                    & 0.5923                             & 0.5995                              \\
                                                                       & 30         & 0.4933                             & \textbf{0.5965}                    & \textbf{0.6041}                     \\ \bottomrule
\end{tabular}%
}
\caption{Performance of retrieval and ranking on multi-modal specific questions. The same models and evaluation methods described in Section \ref{sec:retriever_ranker_result} were used.}
\label{tab:ranker_performance_mutimodal_specific}
\vspace{-1em}
\end{table}

\subsection{Analysis}
\label{subsec:retriever_ranker_analysis}
Figure \ref{fig:retriever_ranker} illustrates the trade-off between retrieval success and re-ranking performance as the number of retrieved chunks $n$ increases. While increasing $n$ improves the retrieval success rate (e.g., from 92\% at $n$ = 10 to 96\% at $n$ = 30), it can lead to diminishing improvements in re-ranking performance (e.g., MAP@5 decreases from 0.74 at $n$ = 10 to 0.73 at $n$ = 30). This result highlights the importance of carefully selecting $n$ to balance retrieval success and re-ranking quality, as overly increasing $n$ may not yield proportional benefits for re-ranking.

\vspace{-0.5em}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/retriever_ranker.png}
    \caption{The impact of the number of retrieved chunks $n$ on retrieval success rates and ranking performance.}
    \label{fig:retriever_ranker}
\vspace{-0.5em}
\end{figure}

\noindent An analysis of the failed cases during the retrieval and ranking phase identified two error types: (a) Top-10 retrieval failure (23.3\%) and (b) Retrieval success but incorrect re-ranker top-1 (76.7\%). Type (a) were mainly caused by the open-ended nature of the questions, which led to multiple relevant documents beyond the gold context (Figure \ref{fig:error_analysis_retrieval} in \ref{subsubsec:analysis_retrieval_ranking}). In contrast, type (b) occurred with more specific and detailed questions, where there were several relevant chunks from the same vehicle crash collision test, in addition to the gold chunk, making it difficult to identify a single correct one (Figure \ref{fig:error_analysis_ranking} in \ref{subsubsec:analysis_retrieval_ranking}). Failures in both types were often due to the presence of multiple valid chunks, rather than the ranking of irrelevant chunks. Table \ref{fig:retrieval_ranking_error_type} presents a human evaluation of 100 sampled cases for each error type, assessing whether the re-ranker top-1 was relevant to the given question, even though it did not match the gold chunk. A screenshot of the human evaluation interface is in Figure \ref{fig:humaneval_screen_retrieval_ranking} (\ref{sec:appendix_humaneval}).

\vspace{-0.5em}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/retrieval_ranking_error_type.png}
    \caption{Human evaluation of the re-ranker top-1 for each error type, based on 100 sampled cases, assessing whether it is relevant to the given question.}
    \label{fig:retrieval_ranking_error_type}
\vspace{-0.5em}
\end{figure}

