% 1/31 까지 끝내기

% Change tone
% Domain-specific -> Domain expert (Avoid using "specific")
% -> Low-Resource Domain expert model
% -> Domain specialized

% (o) Change the title
% -> Scratch-RAG
% -> Low-resource, PPT Slide를 강조하는 제목으로 바꾸기

% (o) RAG 아웃풋 예시 하나 위로 올려도 괜찮을듯

% (o) Past -> Present 하고, discussion 할때만 pask로

% ( ) 전반적인 revision을 우리의 Noble 한것들 눈에 잘들어오게 하기 
% -> low-resource로 emphasize하기


Retrieval-Augmented Generation (RAG) has shown potential in reducing hallucinations and providing up-to-date knowledge in Large Language Models (LLMs). This success has grown interest in domain expert RAG-Question Answering (QA) systems to meet specialized knowledge needs. While previous studies (\citealt{han-etal-2024-rag}; \citealt{siriwardhana-etal-2023-improving}; \citealt{mao-etal-2024-rag}) have proposed general methods for adapting RAG models to domain knowledge bases—such as syntactic QA pair generation or model fine-tuning—they face several challenges in low-resource domains.

In practical settings, available data sources in low-resource domains are often presented in heterogeneous formats and exhibit an unstructured nature, making their direct integration into RAG system development challenging \cite{hong-etal-2024-intelligent}. General models may not have enough inherent knowledge in low-resource domains \cite{zhao2024optimizing}, making fine-tuning essential to adapt the model to specific knowledge requirements. However, the lack of structured data for training further complicates this process. In addition, data privacy and security concerns restrict the full utilization of API-based LLMs (\citealt{achiam2023gpt}; \citealt{claude2024}) within RAG systems, necessitating the use of open-source LLMs (\citealt{qwen}; \citealt{touvron2023llama}).

The retrieval phase is another key aspect of domain expert RAG systems, as referencing accurate documents is essential for generating reliable answers. However, research on improving ranking in the retrieval phase or tracing the documents referenced to generate the answer remains limited. Most domain expert RAG frameworks rely on a single-stage retrieval process, with few exploring multi-stage approaches (\citealt{DBLP:journals/corr/abs-1910-14424}; \citealt{nogueira-etal-2020-document}; \citealt{karpukhin-etal-2020-dense}), such as retrieval followed by re-ranking—a method widely used in Information Retrieval (IR)—which can help ensure the use of the most relevant references.

The evaluation of RAG systems is also an area that has not been fully addressed. Many studies continue to rely on gold answer similarity metrics, such as overlapping words between the generated answer and the ground truth, which inadequately capture critical dimensions like faithfulness, coherence, and contextual relevance. Recently, the LLM-as-a-judge framework \cite{zheng2023judgingllmasajudgemtbenchchatbot} has gained attention as a qualitative alternative. However, these methods often overlook diverse evaluation aspects, and a standardized framework for assessing RAG systems has yet to emerge.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/RAG_v3.png}
    \caption{When a user question is given, our RAG system retrieves and re-ranks the text chunks, generates answers using the top $k$ relevant chunks, and ensures each part of the answer is backed by clear references. The example shown was originally in Korean and translated into English.}
    \label{fig:RAG}
    \vspace{-0.3cm}
\end{figure*}

In this work, we address three key challenges and present the RAG development pipeline, demonstrating its application in the automotive engineering domain, with a specific focus on QA for vehicle crash collision tests. First, we present a data generation pipeline (Section \ref{sec:data_processing}), leveraging diverse formats of internal documents from an automobile company. Second, we incorporate an advanced re-ranking phase (Section \ref{sec:approach_ranking}) and a reference-matching algorithm (Section \ref{sec:reference_matching_algorithm}), not only enhancing the accuracy of the final answers but also improving traceability by tagging sources for each segment of the answer. Third, we evaluate the answers obtained from our generation model (Section \ref{sec:apporach_generation}) from multiple perspectives, emphasizing their qualitative aspects. Our ultimate goal is a fully local system, ensuring data privacy and independence from external servers. The overall workflow of our RAG system is shown in Figure \ref{fig:RAG}, and our key contributions are summarized as follows:
\vspace{-0.5em} 
\begin{itemize} 
\setlength\itemsep{-0.3em} 
\item We propose a data generation pipeline that transforms multi-modal raw data into a structured corpus and high-quality Q\&A pairs. 
\item We integrate re-ranking and reference matching to enhance retrieval precision and answer traceability, ensuring a reliable RAG system.\footnote{The full RAG pipeline code will be publicly available: \url{https://github.com/emorynlp/MultimodalRAG}}
\item We assess the final answer from diverse qualitative perspectives, evaluating each along distinct, non-overlapping dimensions.
\end{itemize}

% LLM Fine-tuning 할때 보통 (Q,A) (Q, Retrieved_chunks, A) 둘 중 어떻게하나? 
% -> 보통 (Q, Retrieved_chunks, A) 이렇게 하는데 우리처럼 (Q,A) 만 넣으면 좀 더 light-weight 한 방식 아닌가? Efficiency 어필?!

%On the other hand, other studies focus on designing RAG models tailored specifically to a single domain, such as E-commerce \cite{guan-etal-2024-bsharedrag}, IT Operations \cite{zhang-etal-2024-rag4itops}, Power Plants \cite{hong-etal-2024-intelligent} and Automobile Insurance \cite{beauchemin-etal-2024-quebec}.

%While general adaptation methods offer reasonable approaches, their effectiveness in addressing the highly specialized demands of expert fields remains uncertain. The requirements for QA models—such as the format of available resources to train models and the patterns of user queries—can vary greatly across systems, particularly in low-resource, niche domains. These domains often face the additional challenge of limited access to open-source models or datasets, making it difficult to develop effective solutions without significant domain-specific customization and data augmentation efforts. In this work, we underscore the importance of specificity by collaborating with domain experts to develop \textbf{CARAG}, a domain-specific Korean RAG system tailored for car collision test queries, specifically designed to meet the unique requirements of automotive engineers.

%We built a data generation pipeline for our RAG system from scratch, using car crash reports provided as presentation slides and educational textbooks on car crash tests. The data was converted into a text corpus, segmented into chunks, and used to generate possible Q\&A pairs. A domain expert played a key role in designing the prompts and ensuring the quality of both the prompts and outputs during the text conversion and Q\&A generation process. The expert also meticulously reviewed the intermediate and final outputs of the RAG system, validating its practical utility for real users.
