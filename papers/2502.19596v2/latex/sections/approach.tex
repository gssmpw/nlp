\subsection{Data Generation \& Processing}
\label{sec:data_processing}
We build our dataset using two distinct sources: (1) internal reports on vehicle crash collision tests in presentation slide format from an automotive company in Korea, and (2) the textbook \textit{Crash Safety of Passenger Vehicles} \cite{mizuno2016crash}. Vehicle crash collision tests are exceptionally valuable, as each test incurs substantial costs and produces detailed reports that are typically confidential and difficult to access publicly. Figure \ref{fig:data_pipeline} illustrates the process of extracting, analyzing, and converting slides and textbook PDFs into structured Markdown text and Q\&A sets, leveraging Python scripts and the Claude LLM\footnote{We restrict the use of API-based LLMs for initial data processing; this limitation is discussed in Section \ref{sec:limitation}.} \cite{claude2024}. Table \ref{tab:dataset_raw} summarizes the data statistics by source. 

\begin{figure}[htp!]
    \centering
    \includegraphics[width=1.0\linewidth]{latex/figures/data_pipeline_edit_small_v2.pdf}
    \caption{Overview of the data generation pipeline, converting each PPT slides and Textbook pages into text chunks and generating Q\&A pairs. An example of the original PPT slide is in Figure \ref{fig:original_slide} (\ref{subsubsec:appendix_dataset}).}
    \label{fig:data_pipeline}
%\vspace{-1em}
\end{figure}

\noindent The original slides often contain tables, graphs, and images, which are simplified into plain text descriptions during data processing. To evaluate performance on questions referencing these elements, we sample 143 multi-modal chunks and generated 1,861 targeted questions. Evaluation results are provided in Section \ref{sec:retriever_ranker_result_table_specific}.

\begin{table}[h!]
\centering
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrrr}
\toprule
\multicolumn{1}{c|}{\textbf{Type}}        & \multicolumn{1}{c|}{\textbf{Source}} & \multicolumn{1}{c}{\textbf{File}}   & \multicolumn{1}{c}{\textbf{Slide}} & \multicolumn{1}{l}{\textbf{Q\&A Pairs}} \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{PPT}} & \multicolumn{1}{l|}{Test Report}     & 1,463                               & 4,662                              & 59,402                                  \\
\multicolumn{1}{l|}{}                     & \multicolumn{1}{l|}{Meeting Report}  & 249                                 & 882                                & 7,696                                   \\ \midrule
\multicolumn{1}{c|}{}                     & \multicolumn{1}{c|}{}                & \multicolumn{1}{c}{\textbf{Page}} & \multicolumn{1}{c}{\textbf{Chapter}}  & \multicolumn{1}{c}{\textbf{Q\&A Pairs}} \\ \midrule
\multicolumn{1}{l|}{PDF}                  & \multicolumn{1}{l|}{Textbook}        & 404                                   & 81                                 & 1,505                                   \\ \midrule
\multicolumn{2}{r}{\textbf{Total}}                                               & \multicolumn{1}{l}{}                & 5,625                              & 68,603                                  \\ \bottomrule
\end{tabular}%
}
\caption{Raw data statistics by source, along with the corresponding number of generated Q\&A pairs. Dataset splits and token length statistics are described in \ref{subsubsec:appendix_dataset}.}
\vspace{-1em}
\label{tab:dataset_raw}
\end{table}

\noindent Additionally, we extract and prepend headers summarizing each report, including the \textit{Test Name}, \textit{Region}, \textit{State}, and \textit{Purpose}, which significantly improve the retrieval phase (See Section \ref{sec:retriever_ranker_result}). Domain experts are responsible for the entire process, including prompt engineering and reviewing intermediate and final outputs. All prompts and an example of a generated Q\&A pair are shown in \ref{subsubsec:data_prompt}.


%%기간도 넣기 
% 파워포인트
% 1. 시험 후에 나오는 엔지니어 시험 보고서 5,471개 슬라이드
% 2. Additional : 주간 업무 회의 (내부에서 일주일에 한 번씩 보고하는) (개수 몇 갠지 문의) 
%textbook 
% 3. 교과서 : (페이지 수)  

\subsection{Retrieval \& Ranking}
\label{sec:approach_ranking}
We employ a Dual-Encoder as our retriever, fine-tuning it on our training data. For a question $q$, the model retrieves the top $n$ chunks from $m$ chunks $\mathbf{D}$, based on the [CLS] token embedding similarity between $q$ and each chunk $d_i \in \mathbf{D}$, as follows:
\[
\begin{split}
q_{\text{[CLS]}} \in \mathbb{R}^{1 \times d}; \; \mathbf{D}_{\text{[CLS]}} \in \mathbb{R}^{m \times d}; \\
\text{Similarity}(q, \mathbf{D}) = q_{\text{[CLS]}} \cdot \mathbf{D}_{\text{[CLS]}}^\top \in \mathbb{R}^{1 \times m} \\
\mathbf{D}_{\text{top}_n} = \text{Sort}\left(\mathbf{D}, \; \text{key}=\text{Similarity}(q, \mathbf{D})\right)[:n]
\end{split}
%\label{eq:retrieval}
\]
Our re-ranker is a point-wise Cross-Encoder, trained on a classification task \cite{DBLP:journals/corr/abs-1910-14424} that takes a single $(q, d_i)$ pair as input and returns a scalar relevance score. It re-ranks $D_{top_n}$, obtained from the retrieval stage, to extract $D_{top_k}$, which will be passed to the generation model. The process is formalized as follows:
\[
\begin{split}
\mathbf{D}_{\text{top}_n} = [d_1, d_2, \dots, d_n] \\
x_i = \text{"\{$q$\} \{sep\_token\} \{$d_i$\}"} \quad \forall d_i \in \mathbf{D}_{\text{top}_n} \\
\text{Rel}_{x_i} = \text{Ranker}(x_i) \quad \forall i \in \{1, 2, \dots, n\} \\
\mathbf{D}_{\text{top}_k} = \text{Sort}(\mathbf{D}_{\text{top}_n}, \; \text{key}=\text{Rel}_{x_i})[:k]
\end{split}
%\label{eq:ranker}
\]
To optimize $\text{Rel}_{x_i}$, we apply Token Selection (TS) + Term Control Layer (TCL) training method from RRADistill \cite{choi2024rradistilldistillingllmspassage}. It effectively integrates the importance of the general semantic and specific query terms during the training process.
\[
\begin{split}
\resizebox{\columnwidth}{!}{
$\begin{aligned}
d_i^{\text{TS}} = \text{RRA}_\text{TS}(q, d_i) \\
x_i^{\text{TS}} = \text{"\{$q$\} \{$sep\_token$\} \{$d_i^{\text{TS}}$\}"} \\
\text{Rel}_{x_i}^{\text{TS}} = \text{Ranker}(x_i^{\text{TS}}) \quad \forall i \in \{1, 2, \dots, n\} \\
\mathcal{L} = \frac{1}{n} \sum_{i=1}^{n} \left( \alpha \cdot \mathcal{L}(\text{Rel}_{x_i}, y_i) + \beta \cdot \mathcal{L}(\text{Rel}_{x_i}^{\text{TS}}, y_i) \right)
\end{aligned}$
}
\end{split}
%\label{eq:ranker_rra}
\]
TS+TCL are used during training only, while inference uses the standard Cross-Encoder approach only with $\text{Rel}_{x_i}$. The target label $y$ is binary: $y = 1$ for relevant pairs and $y = 0$ for irrelevant pairs. Negative sampling retrieves the top 10 chunks from the train data for each query $q$, excluding the gold chunk, and randomly selects three from the rest.

\subsection{Answer Generation} 
\label{sec:apporach_generation}
Our answer generation model takes the user question $q$ and the top $k$ chunks ($\mathbf{D}_{\text{top}_n}$) from the retrieval and ranking phase as input to generate the final answer $a'$, as follow:
\[
\begin{split}
a' = \text{LLM}(q, \mathbf{D}_{\text{top}_n})
\end{split}
%\label{eq:generation_model}
\]
We fine-tune open-source LLMs on our training dataset, which consists of $(q, a)$ pairs derived from our data generation pipeline (Section \ref{sec:data_processing}). During fine-tuning, the question $q$ and the answer $a$ are concatenated into a single sequence $S = [q; a]$, and the model is trained in an auto-regressive manner to predict the next token. %The fine-tuning objective minimizes the cross-entropy loss over the concatenated sequence:
%\begin{equation}
%\begin{split}
%\mathcal{L} = -\sum_{t=1}^{|S|} \log P(S_{t}|S_{<t}),
%\end{split}
%\label{eq:generation_loss}
%\end{equation}
%where $|S|$ is the length of the concatenated sequence $S$, and $S_{t}$ denotes the $t$-th token in the sequence \( S \). We performed full fine-tuning on the 14B model and applied Low-Rank Adaptation (LoRA) \citep{hu2021loralowrankadaptationlarge} for the 72B model.


\subsection{Reference Matching Algorithm}
\label{sec:reference_matching_algorithm}
We propose a reference matching algorithm that segments generated answers and links them to relevant references using a Dynamic Programming (DP) approach \cite{bellman1954dynamic}. Algorithm \ref{algo:reference_matching} outlines the detailed procedure.
% enhancing the transparency and accuracy of RAG systems and supporting decision-making in specialized fields. 

\begin{algorithm}
%\small
\fontsize{9.5pt}{10pt}\selectfont
\caption{Reference Matching Algorithm}
\label{algo:reference_matching}
\begin{algorithmic}
\State \textbf{Input:} $a' = [s_1, \dots, s_n]$, $\mathbf{D}_{\text{top}_k} = [d_1, \dots, d_k]$
\State \textbf{Output:} $(s_i\text{:}s_j, d_t, \text{Score}_{(i, j, d_t)})$

\vskip 0.2cm
\State \textbf{Step 1: Compute Segment-Chunks Scores}
\State Let $a'_{\text{segments}} = \{ \text{concat}(s_i, \dots, s_j) \mid 1 \leq i < j \leq n \}$ be the set of all concatenated sentence subsequences from $s_i$ to $s_j$ of $a'$.
\For{each segment ${\mathbf{seg}}_u$ in $a'_{\text{segments}}$}
    \For{each chunk $d_t$ in $\mathbf{D}_{\text{top}_k}$}
        \State Calculate $\text{Score}_{(i, j, d_t)} = \text{Re-ranker}(\mathbf{seg}_u, d_t)$
    \EndFor
\EndFor
\vskip 0.2cm

\State \textbf{Step 2: Optimal Score Selection}
\State \texttt{Initialize} the array $dp$ to track optimal scores.
\State \texttt{Initialize} the array $choice$ to store optimal choices.
\For{each possible ending sentence $s_j$}
    \For{each possible starting sentence $s_i$ ($1 \leq i < j$)}
        \For{each chunk $d_t$ in $\mathbf{D}_{\text{top}_k}$}
            \If{$\text{Score}_{(i, j, d_t)} > dp[j+1]$}
                \State Update $dp[j+1] = \text{Score}_{(i, j, d_t)}$
                \State Record the choice $(i, j, d_t)$ in $choices$
            \EndIf
        \EndFor
    \EndFor
\EndFor
\vskip 0.2cm

\State \textbf{Step 3: Backtracking}
\State \texttt{Initialize} $current = n$ to start from the last sentence.
\While{$current > 0$}
    \State Retrieve the best $(i, j, d_t)$ from $choices$ for $current$.
    \State Add the tuple $(s_i\text{:}s_j, d_t, \text{Score}_{(i, j, d_t)})$ to the result.
    \State Set $current = i$ to move to the previous segment.
\EndWhile
\end{algorithmic}
\end{algorithm}
% , with $dp[0] = 0$ and $dp[i] = -\infty$ for all $i > 0$.

\noindent In Step 1, the algorithm computes the scores for all possible sentence subsequences $a'_{\text{segments}}$ against the top-k chunks $D_{\text{top}_k}$, using our re-ranker (Section \ref{sec:approach_ranking}). In Step 2, it selects the optimal segment-chunk combinations, updating scores and recording the best choices for each ending sentence (e.g., if the answer consists of five sentences like $a = [s_1, \dots, s_5]$, the choices might be $(s_1\text{:}s_1, d_1), (s_2\text{:}s_2, d_3), (s_2\text{:}s_3, d_3), (s_1\text{:}s_4, d_1),$ $(s_5\text{:}s_5, d_2)$). Finally, in Step 3, backtracking is performed to retrieve and output the best matches from the choices (e.g., $(s_1\text{:}s_4, d_1), (s_5\text{:}s_5, d_2)$).
