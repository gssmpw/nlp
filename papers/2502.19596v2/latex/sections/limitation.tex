\label{sec:limitation}
\textbf{Constraints in Backbone Model Selection} $\;$ The selection of a backbone model was constrained by the need for an open-source model supporting Korean with sufficient context length, limiting the available options across retrieval and LLM components. BGE-M3 for retriever \& re-ranker, along with Qwen2.5 for the LLM, were the options available to us. However, it is worth noting that the newly released Qwen2.5-1M \cite{yang2025qwen251mtechnicalreport} offers a longer context length, and our preliminary experiments demonstrated its potential for better handling long inputs. As the availability of open-source LLMs continues to grow, we anticipate that this limitation will gradually be resolved over time.

\vspace{1em}
\noindent \textbf{Restricted Use of API-based LLM for Initial Data Processing} $\;$
While our ultimate goal is to develop a fully local system, we initially relied on the API-based LLM, specifically the Claude API, for limited data processing. Among the available open-source LLMs, none met the requirements due to context limits and certain performance considerations, making them unsuitable for our needs. We determined that leveraging an API-based LLM with expert-driven prompting would be more effective in terms of both quality and cost compared to full human annotation. However, this reliance was limited to the initial data required for fine-tuning in the open-source model, and any future updates to the document pool will completely eliminate the need for external dependencies.

\vspace{1em}
\noindent \textbf{Weakness in Multi-modal Questions} $\;$ Our RAG system exhibited relatively lower performance when retrieving contexts for multi-modal specific questions that require information from tables, graphs, and images to answer the given question (Section \ref{sec:retriever_ranker_result_table_specific}). We hypothesize that this is due to the loss of rich content when multi-modal elements are converted into plain text during the data processing stage. To address this, we are experimenting with Llama 3.2 Vision\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-11B-Vision}} to improve the conversion of multi-modal elements into text, aiming for better representation of these elements. % 지금 llama vision 가지고 실험하고 있다는 예시와 함께 (이 데이터 다 쓸때 600불들어서 하고있는 중이다.)

\vspace{1em}
\noindent \textbf{Experiments Limited by Computational Constraints} $\;$ The number of experiments we could conduct on the generation model was constrained by computational limitations. Fine-tuning the model on $(q, a, D_{\text{top}k})$ instead of $(q, a)$ (Section \ref{sec:apporach_generation}) proved impractical due to the extensive context length, which exponentially increased GPU memory requirements, surpassing our available computational resources. Although we observed that Qwen models inherently possess summarization capabilities for the given $D_{\text{top}_k}$, and fine-tuning on $(q, a)$ yielded effective results, we were unable to explore a variety of learning strategies for the generation model within the RAG framework.

%\textbf{Use of API-based LLM in Data Processing} $\;$ While our ultimate goal is to develop a fully local system, we initially relied on the API-based LLM, specifically the Claude API, for limited data processing. This dependency was restricted to the initial data required for fine-tuning in the open-source mode, and any future updates to the document pool will eliminate the need for external reliance entirely.
% It is worth mentioning that~~ 우리가 새 Qwen으로 실험해봤는데 되더라. 이게 commercial api 쓴 최소한의 부분이다. 맨 뒤에 이 연구 시작했을때 비전 모델 오픈소스가 없었는데 라마도 있고 qwen도 나와서 미래에 해볼거다. 그러면 오픈소스 문제는 해결이 된다. 새 qwen이 우리꺼랑 comparable 하더라.
% (future work) 오픈소스로 fully open source화 하겟다.
% o1이랑 qwen 새모델 언급해주기

%\noindent \textbf{Weakness in multi-modal specific questions} $\;$ TBU
% Multi-model Specific Question 한계 극복
% -> 지금 llama vision 가지고 실험하고 있다는 예시와 함께 (이 데이터 다 쓸때 600불들어서 하고있는 중이다.)

%Second, the absence of public datasets in the automobile domain forced us to rely on GPT and human evaluations using our dataset. While these methods provide qualitative insights, they lack robust baseline comparisons and standardized metrics for evaluating performance across datasets.

%Third, the selection of the backbone model was limited by the availability of Korean-focused LLMs capable of handling sufficient context lengths. We utilized the Qwen-2.5 model, one of the latest models officially supporting Korean.

%Fourth, computational constraints limited the number of experiments we could perform on the generation model. Fine-tuning the model on $(q, a, D_{\text{top}_k})$ instead of $(q, a)$ was impractical due to the extensive context length, which demands exponential GPU memory and exceeded our computational capacity.

%% + Summarization training 해야 하는데 너무 max length 때문에 .. gpu 리소스 문제...  왜 못했는지를 써야 함 -> limitation 에 쓰기 -> 완료
