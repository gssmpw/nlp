\textbf{Data Processing} $\;$ RAG-Studio (\citealt{mao-etal-2024-rag}) employs synthetic data generation for in-domain adaptation, reducing reliance on costly human-labeled datasets. However, it assumes access to well-structured data, limiting its applicability in scenarios with unstructured raw data. To bridge this gap, \citet{hong-etal-2024-intelligent} tackle challenges with real-world formats (e.g., DOC, HWP), proposing a chunking method that converts documents to HTML, retaining structural information in low-resource settings. Meanwhile, \citet{guan-etal-2024-bsharedrag} address the issue of short and noisy e-commerce datasets in RAG system development by building a new dataset from a raw corpus crawled from an e-commerce website, providing a richer resource. %Together, these studies highlight the importance of tailored data processing pipelines to manage unstructured and noisy data, enabling RAG systems to excel in practical applications.
% Effective data processing is essential for adapting RAG systems to real-world domains. 

\vspace{0.5em}
\noindent \textbf{Retrieval in RAG} $\;$ \citet{wang-etal-2024-searching} highlight the importance of re-ranking modules in RAG systems to enhance the relevance of the retrieved documents. Similarly, \citet{zhao2024optimizing} demonstrate that the ranking position of the gold document in the retrieval phase plays a significant role in determining the quality of the final answer. Given these insights, optimizing the retrieval phase is crucial for obtaining accurate context, particularly in specialized, low-resource domains where LLMs lack sufficient inherent knowledge \cite{beauchemin-etal-2024-quebec}. Despite these findings, the analysis of the effectiveness and applicability of re-rankers in domain expert RAG systems remains underexplored.

\vspace{0.5em}
\noindent \textbf{RAG Evaluation} $\;$  The evaluation of RAG systems \cite{yu2024evaluationretrievalaugmentedgenerationsurvey} has relied on text similarity-based metrics such as BLEU \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge}, and EM \cite{rajpurkar2016squad}. These metrics provide a baseline but overlook valid answer diversity and qualitative aspects like factual consistency and relevance. Recent advancements are made to utilize LLM-as-a-judge \cite{zheng2023judgingllmasajudgemtbenchchatbot} to evaluate qualitative dimensions. \citet{han-etal-2024-rag} propose a pairwise preference comparison framework considering helpfulness and truthfulness, while \citet{saad-falcon-etal-2024-ares} assess context relevance, answer faithfulness, and query relevance, addressing hallucination issues. However, a standardized framework for RAG evaluation remains a challenge.

% https://aclanthology.org/2024.emnlp-main.249.pdf
% -> Helpfulness, Truthfulness 고려하자만 지표마다 점수를 내는게 아니고, Pairwise로 Model A,B 중에 더 선호하는걸 return하는 방식

% https://aclanthology.org/2024.naacl-long.20.pdf
% - 1. Context Relevance: Is the passage returned relevant for answering the given query?
% - 2. Answer Faithfulness: Is the answer generated faithful to the retrieved passage, or does it contain hallucinated or extrapolated statements beyond the passage?
% - 3. Answer Relevance: Is the answer generated relevant given the query and retrieved passage?
% -> hallucination에 초점


