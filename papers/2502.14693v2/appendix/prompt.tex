\newpage
\section{Prompts}
\label{app:prompt}

\subsection{Introspection Node Expansion Prompt}
\begin{lstlisting}[style=pythonstyle]
INTROSPECTION_PROMPT = """
You are an AI assistant tasked with analyzing a machine learning solution and proposing new insights to IMPORVE its performance. Given the current plan, solution code, Evaluation Metrics and performance on development score, suggest innovative approaches to ENHANCE and IMPORVE the performance.

# Current Plan:
{current_plan}

# Current Solution Code:
{solution_code}

# Current Evaluation Metrics:
{current_metrics}

# Current Performance on Development Set: 
{dev_score}

There may be some drawbacks in the Current Plan or Solution that leads to a lower development score.
Based on this information, please propose ONE new insight w.r.t. the following specific stage of the machine learning pipeline to IMPORVE the overall performance. Your insight should be specific, actionable, and have the potential to IMPORVE the model's performance.
{task_type}

NOTE that the following insights have been proposed before. Kindly AVOID suggesting an insight that is similar to them.
{cur_expansion_list}

Please strictly format your response as a JSON with the following structure:
```json
{{
    "task_type": "Data Preprocessing",
    "critic_feedback": "feedback and explanation on potential drawbacks in the Current Plan or Solution that leads to a lower development score"
    "insight": "ONE insight that specific, actionable, and have the potential to IMPORVE the model's performance."
}}
```
"""
\end{lstlisting}

\subsection{Node Evaluation Prompt}
Each Node will be evaluated by the LLM based on the following prompt.
\label{app:llm_evaluation_schema}

\begin{lstlisting}[style=pythonstyle]
EVALUATOR_SYSTEM_MSG = "As a Kaggle Grandmaster competing in a challenge, Your role is to evaluate the candidate solution for the given data science problem."

EVALUATION_CRETERIA = """
### Evaluation Criteria for AI Engineer's Execution Plan in Kaggle Data Science Competition  
Below is a scoring framework to evaluate the quality and feasibility of an execution plan. The total score is **100 points**, divided into **7 criteria**, each with defined scoring ranges and descriptions.  
---
#### **1. Problem Understanding (15 points)**  
- **15**: Clearly defines the competition goal, success metrics (e.g., AUC, RMSE), and constraints (e.g., runtime limits). Includes domain-specific nuances (e.g., business impact for sales prediction).  
- **10 to 14**: Adequate understanding but misses minor details (e.g., unclear evaluation metric implications).  
- **5 to 9**: Superficial analysis; overlooks key competition rules or data constraints.  
- **0 to 4**: Incorrect problem framing or missing critical objectives.  
---
#### **2. Data Preprocessing & Feature Engineering (20 points)**  
- **20**: Comprehensive plan addressing missing values, outliers, categorical encoding, and normalization. Proposes novel features (e.g., interaction terms, domain-specific transformations) and validates their impact.  
- **15 to 19**: Solid strategy but lacks innovation (e.g., standard scaling but no feature interactions).  
- **10 to 14**: Basic techniques (e.g., mean imputation, one-hot encoding) with gaps (e.g., no outlier handling).  
- **5 to 9**: Incomplete or naive methods (e.g., dropping all missing values without analysis).  
- **0 to 4**: No preprocessing/feature engineering or harmful approaches.  
---
#### **3. Model Selection & Validation Strategy (20 points)**  
- **20**: Justifies model choices (e.g., LightGBM for tabular data, NN for high dimensionality) and advanced techniques (e.g., stacking, automated hyperparameter tuning). Designs robust cross-validation aligned with competition rules (e.g., time-series splits for temporal data).  
- **15 to 19**: Reasonable models (e.g., XGBoost) and CV but lacks optimization (e.g., no Bayesian hyperparameter search).  
- **10 to 14**: Basic models (e.g., random forest) with weak validation (e.g., simple holdout).  
- **5 to 9**: Inappropriate models (e.g., CNNs for small tabular data) or validation leakage.  
- **0 to 4**: No clear model selection or validation plan.  
---
#### **4. Training & Optimization (15 points)**  
- **15**: Efficient resource use (e.g., GPU acceleration, parallel trials), advanced optimization (e.g., class imbalance handling, quantile loss for skewed targets), and time management.  
- **10 to 14**: Logical training workflow but suboptimal resource allocation (e.g., no early stopping).  
- **5 to 9**: Basic training loop with critical inefficiencies (e.g., no hyperparameter tuning).  
- **0 to 4**: Unworkable training strategy (e.g., no batch processing for large data).  
---
#### **5. Post-Processing & Interpretation (10 points)**  
- **10**: Ensemble methods (e.g., weighted blending), thorough error analysis, and model interpretation (e.g., SHAP values, feature importance).  
- **7 to 9**: Simple ensembling (e.g., averaging) and basic interpretation (e.g., feature importance plots).  
- **4 to 6**: Minimal post-processing (e.g., no calibration) or superficial analysis.  
- **0 to 3**: No post-processing or interpretation.  
---
#### **6. Documentation & Reproducibility (10 points)**  
- **10**: Clean, modular code structure with detailed documentation (e.g., Docker setup, dependency lists). Includes version control and experiment tracking (e.g., MLflow).  
- **7 to 9**: Readable code but limited documentation (e.g., no README).  
- **4 to 6**: Disorganized code with critical reproducibility gaps.  
- **0 to 3**: Undocumented or non-reproducible code.  
---
#### **7. Innovation & Practicality (10 points)**  
- **10**: Novel yet practical ideas (e.g., synthetic data for imbalance, custom loss functions). Balances creativity with competition constraints.  
- **7 to 9**: Minor innovations (e.g., new feature engineering) but overly complex in parts.  
- **4 to 6**: Generic approach with no novel elements.  
- **0 to 3**: Impractical or gimmicky methods (e.g., unnecessary deep learning).  
---
### **Scoring Scale**  
| Total Score | Grade          | Description  |  
| **90 to 100**  | Exceptional    | Well-structured, innovative, and executable plan. High chance of top ranks. |  
| **70 to 89**   | Strong         | Minor gaps but logically sound. Competitive but needs refinement.           |  
| **50 to 69**   | Average        | Basic approach with significant oversights (e.g., weak validation).         |  
| **<50**     | Insufficient   | Flawed or incomplete plan; unlikely to succeed.                             |  
---
### **Evaluation Guidance**  
- Prioritize **justification of choices** (e.g., why CatBoost over XGBoost?).  
- Reward **pragmatism** (e.g., focusing on feature engineering over overly complex models).  
- Penalize **ignoring competition constraints** (e.g., submission runtime limits).  
- Highlight **risks** (e.g., overfitting due to improper CV) and mitigation strategies.  
This framework ensures a balance between technical rigor, innovation, and practicality in tabular data competitions.
"""

EVALUATE_NODE_BY_LLM = """
# Evaluation Criteria
{evaluation_Criteria}
# Instruction:
Based on the above evaluation Criteria, Please give feedback and rating for the following solution. 
# Candidate Case
## User Requirement
{user_requirement}
## Candidate Solution
{candidate_plan}
# Format:
Please ensure your output strictly adheres to the following structure, ensure the "Total_Score" is a INT number":
```json
{{
    "evaluation_feedback": "Your evaluation feedback for the candidate solution based on the evaluation criteria. Please provide a detailed analysis of each criterion and explain why you gave the specific rating",
    "total_score": x
}}
\end{lstlisting}
