\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow}
\usepackage[table,xcdraw,dvipsnames,svgnames,x11names]{xcolor}
\usepackage{booktabs}
\usepackage{amsfonts,amssymb} 
\usepackage{enumerate}
\usepackage{color}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\definecolor{AliceBlue}{RGB}{240, 248, 255}  % Alice Blue
\definecolor{LightCyan}{RGB}{224, 255, 255}  % Light Cyan
\definecolor{LavenderBlue}{RGB}{195, 191, 255} % Lavender Blue
\begin{document}

% \title{JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework for Enhanced Performance}
\title{JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework}

\author{Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu,~\IEEEmembership{Senior Member,~IEEE}
        % <-this % stops a space
\thanks{Ziyuan Liu and Yuantao Gu are with the Department of Electronic Engineering, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China (e-mail: liuziyua22@mails.tsinghua.edu.cn; gyt@tsinghua.edu.cn). 
Ruifei Zhu, Long Gao, Yuanxiu Zhou, and Jingyu Ma are with Chang Guang Satellite Technology Co., Ltd. (CGSTL)
Changchun 130102, China (e-mail: zhuruifei@jl1.cn; gaolong1056@jl1.cn; zhouyuanxiu@jl1.cn; majingyu@jl1.cn). 
% This work was supported by NSAF (Grant No. U2230201) and a Grant from the Guoqiang Institute, Tsinghua University. 
(\textit{Corresponding author: Yuantao Gu.})}% <-this % stops a space
}

% The paper headers
\markboth{IEEE Transactions on Geoscience and Remote Sensing}
{Liu \MakeLowercase{\textit{et al.}}: JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. 
To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 × 512 pixel images with a resolution of 0.5 to 0.75 meters. 
Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. 
Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results.
The code is available at https://github.com/circleLZY/MTKD-CD.
\end{abstract}

\begin{IEEEkeywords}
Knowledge distillation, change detection, remote sensing.
\end{IEEEkeywords}

\section{INTRODUCTION} \label{A}
Remote sensing image change detection (CD) is a technique used to detect and analyze surface changes by leveraging multi-temporal data \cite{review2024}. 
Over the past few decades, it has been extensively studied and has become a crucial tool for Earth surface observation. 
CD plays a significant role in various fields, including land-use change updates, natural disaster assessment, environmental monitoring, and urban planning.

Early traditional CD methods primarily relied on image processing techniques, detecting changes by directly comparing pixel values or spectral features between multi-temporal images. 
Examples include Image Differencing\cite{imagediff}, Change Vector Analysis (CVA)\cite{cva}, Principal Component Analysis (PCA)\cite{pca}, Kauth–Thomas (KT) transforms\cite{kt}, and Multivariate Alteration Detection (MAD)\cite{mad}. 
While these methods are simple and intuitive, they exhibit limited performance when handling complex change patterns, such as those affected by significant noise, lighting variations, or seasonal differences.  

With the rise of machine learning (ML), CD methods began to incorporate feature extraction and classifiers. Common techniques include Support Vector Machines (SVM)\cite{svm}, Random Forest (RF)\cite{rf}, K-means clustering\cite{kmeans} and so on. 
These machine learning approaches significantly improved detection accuracy but heavily relied on high-quality labeled data and manually designed features.

In recent years, the rapid advancement of deep learning (DL) has revolutionized remote sensing CD, delivering substantial performance breakthroughs. DL-based CD methods generally involve three steps: (1) extracting change features from image pairs, (2) generating change maps based on the extracted features, and (3) predicting labels based on the feature maps. 
Convolutional Neural Networks (CNNs), which achieved remarkable success in image processing, were the first neural network architecture applied to remote sensing CD and remain widely optimized and utilized today \cite{fcsn, cgnet, two}. 
With the introduction of Transformers, some studies have explored their application in CD tasks \cite{changeformer,changer}. 
More recently, Foundational Model (FM) has emerged as a novel paradigm, aiming to achieve multi-task and multi-domain generalization through large-scale pretraining \cite{optimizing}. 
There have been works utilizing remote sensing data to fine-tune pretrained models such as Vision Transformers (ViT)\cite{vit}, Segment Anything Model (SAM)\cite{sam}, and Contrastive Language-Image Pretraining (CLIP)\cite{clip}, achieving higher performance in CD tasks \cite{ban,changeclip,ttp,anychange}.
Several semi-supervised methods have also been proposed, further improving CD performance under limited labeled data. 
Zhang et al.\cite{zhang2024semi} propose a novel multilevel consistency-regularization-based semi-supervised CD approach, incorporating Fourier-based frequency transformation and a dynamic pseudolabel selection scheme to mitigate background noise and improve unlabeled data utilization.
Xu et al. \cite{xu2024semi} introduce a semi-supervised label and embedding consistency network (SS-LEC) for ORSI scene classification, which strategically enforces consistency across augmentations and stages of training.
Li et al. \cite{li2024semicd} propose SemiCD-VL, a VLM-guided semi-supervised change detection method that synthesizes pseudo labels via a mixed change event generation strategy, achieving significant performance gains over FixMatch and SOTA unsupervised methods.

However, DL-based CD methods generally face two major challenges: the scarcity of high-quality, high-resolution, all-inclusive CD datasets and limitations in handling highly dynamic change areas. 
Although numerous CD datasets have been constructed and proposed, they are often tailored to specific scenarios, which restricts the generalization capabilities of the algorithms. 
For instance, models trained on datasets focused on human-induced changes often fail to perform effectively when confronted with natural change scenarios. 
On the other hand, the learning capacity of these models is inherently limited. 
Most existing algorithms rely on a single-phase training approach, typically end-to-end training, supplemented by techniques such as learning rate decay. 
While such training strategies can achieve satisfactory results within a constrained range of changes, the models' performance significantly degrades when addressing scenarios with wide variations in change areas, ranging from no change to complete change.

% Figure: Survey
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/survey.png}
    \caption{Timeline of the development of mainstream DL-based CD methods.}
    \label{fig:survey}
\end{figure*}

To address the aforementioned challenges, we construct a new large-scale, high-resolution, all-inclusive open-source CD dataset, named ``JL1-CD'' (named after the Jilin-1 satellite). 
This dataset comprises 5,000 pairs of 512 × 512 images captured in China, with a resolution of 0.5–0.75 meters, along with binary change labels at the pixel level. 
The JL1-CD dataset not only includes common human-induced changes such as buildings and roads but also encompasses various natural changes, such as forests, water bodies, and grasslands. 
Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for change detection optimization. 
First, we introduce the O-P strategy. 
To address the difficulty of handling highly dynamic change areas in existing algorithms, we propose the concept of the Change Area Ratio (CAR) and partition the dataset based on different CAR levels. 
CD models are then trained on each partition, reducing the learning burden on individual models, thereby achieving better training outcomes and higher detection accuracy. 
Next, to lower the computational and time complexity during inference, we extend the O-P strategy by training a student model under the MTKD framework. The student model learns the strengths of teacher models optimized for various CAR scenarios, achieving superior detection accuracy without increasing resource consumption during inference. 

Our main contributions are as follows: 
\begin{enumerate}[1)]
    \item We introduce JL1-CD, a new sub-meter, all-inclusive open-source CD dataset comprising 5,000 pairs of remote sensing image patches with a resolution of 0.5–0.75 meters. 
    \item We propose the O-P strategy, which partitions the training of CD models based on CAR levels, significantly improving performance across diverse CAR scenarios.
    \item We further develop the MTKD framework, where models trained under the O-P strategy serve as teacher models. The student model trained under the supervision of multiple teachers achieves superior detection accuracy without additional computational or time costs during inference. 
    \item Extensive benchmarking experiments on existing algorithms demonstrate that O-P and MTKD significantly enhance performance across various architectures and parameter sizes, achieving new state-of-the-art (SOTA) results. 
\end{enumerate}


\section{RELATED WORKS} \label{B}
\subsection{Traditional and ML-Based CD}  

Traditional CD methods have been extensively studied in remote sensing, with early approaches relying on simple algebraic operations such as image differencing \cite{imagediff} and image ratioing \cite{imageratio}. 
These techniques compute pixel-level differences or ratios between two images and apply a threshold to identify change regions. 
Subsequent advancements introduced improved thresholding strategies, such as the Otsu method \cite{ostu} and the normalized difference vegetation index (NDVI) algorithm \cite{ndvi}, to enhance detection accuracy. 
Transform-based methods, such as PCA \cite{pca} and MAD \cite{mad}, were later adopted, leveraging statistical properties of images. 
However, these methods are heavily dependent on image statistics, which limits their scalability and precision in large-scale, high-resolution CD applications.  
The advent of machine learning has significantly enhanced the ability to extract useful change features. 
For instance, Bovolo et al. \cite{svm2} proposed an unsupervised change detection framework that leverages a semi-supervised SVM initialized with pseudo-training data, effectively addressing the complexity of multi-temporal spectral feature analysis. 
Wessels et al. \cite{randomforest} developed an automated system for land cover update mapping, integrating iteratively reweighted MAD (IRMAD) for change mask generation and RF classifiers for robust land cover classification, achieving notable accuracy in operational settings.  
Despite these advancements, traditional and early ML-based approaches often rely on manually designed features, which perform well in straightforward scenarios but exhibit limited generalization capability for complex and diverse change types. 

\subsection{DL-Based CD}
In recent years, deep learning has experienced rapid advancements, achieving remarkable success in remote sensing image CD. 
As illustrated in Fig. \ref{fig:survey}, we present a timeline of the development of mainstream DL-based CD algorithms. 
Based on the differences in neural network architectures and training paradigms, DL-based CD methods can be classified into three main categories

\paragraph{CNN-Based CD}  
CNNs serve as the foundation of many early DL-based CD methods and remain widely utilized today.
Daudt et al. \cite{fcsn} proposed three fully convolutional neural network architectures, including two Siamese network extensions, which achieved significant improvements in accuracy and efficiency for CD tasks on multiple datasets.
Zhang et al. \cite{ifn} introduced the Image Fusion Network (IFN), which employs a deeply supervised two-stream architecture for high-resolution remote sensing CD, achieving SOTA performance with superior boundary completeness and compactness in change maps.
Chen et al. \cite{stanet} proposed the Siamese-based Spatial-Temporal Attention Network (STANet), incorporating a novel CD self-attention module to model spatial-temporal dependencies at various scales, significantly improving F1-scores on benchmark datasets.
Fang et al. \cite{snunet} designed SNUNet-CD, a densely connected Siamese network that preserves localization information and employs an Ensemble Channel Attention Module (ECAM) for deep supervision, achieving better trade-offs between accuracy and computational cost.
Zheng et al. \cite{changestar} proposed ChangeStar, a model leveraging single-temporal supervised learning with ChangeMixin modules to train CD models using unpaired images.
Han et al. \cite{hanet} introduced HANet, a hierarchical attention network with progressive foreground-balanced sampling and a lightweight self-attention mechanism, effectively addressing class imbalance in CD tasks and achieving superior results on highly imbalanced datasets.
The Change Guiding Network (CGNet) introduced by Han et al. \cite{cgnet} utilizes a self-attention mechanism to improve edge detection and internal consistency in change maps, demonstrating robust performance across multiple CD datasets.
Some studies have focused on designing lightweight and fast CD models.
Codegoni et al. \cite{tinycd} presented TinyCD, a lightweight and efficient CD model using a Siamese U-Net architecture and the Mix and Attention Mask Block (MAMB), outperforming SOTA models while being significantly smaller and faster.
Xing et al. \cite{lightcdnet} proposed LightCDNet, a lightweight CD model with an early fusion backbone and pyramid decoder.

% Figure: Dataset
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/dataset-class.png}
    \caption{Sample images from the JL1-CD dataset. Each row, from top to bottom, represents: the image at time 1, the image at time 2, and the ground truth label. Each column corresponds to different change types: (a) Decrease in woodland; (b) Building changes; (c) Conversion of cropland to greenhouses; (d) Road changes; (e) Waterbody changes; and (f) Surface hardening (central region).}
    \label{fig:dataset-class}
\end{figure*}

\paragraph{Transformer-Based CD}  
Transformer-based methods have emerged as a promising approach for CD. 
Chen et al. \cite{bit} introduced the bitemporal image transformer (BIT), combining a transformer encoder with a ResNet backbone to model spatial-temporal contexts efficiently. 
Bandara et al. \cite{changeformer} proposed ChangeFormer, a fully transformer-based Siamese network for CD, which unifies a hierarchical transformer encoder with a multi-layer perceptron (MLP) decoder.
Fang et al. \cite{changer} introduced the Changer series framework, a novel architecture for CD that incorporates alternative interaction layers between bi-temporal features. 
This framework is applicable to both CNN-based and Transformer-based models, significantly enhancing the performance of the original models.

\paragraph{FM-Based CD}  
Recently, foundation models have become a new training paradigm, and some works have applied them to CD tasks.
Li et al. \cite{ban} proposed the Bi-Temporal Adapter Network (BAN), a universal FM-based framework for CD, which enhances existing models with minimal additional parameters and achieves significant performance improvements.
Chen et al. \cite{ttp} introduced Time Travelling Pixels (TTP), a method that integrates latent knowledge from the SAM model into CD, overcoming domain shifts and spatio-temporal complexities, demonstrating SOTA results on the LEVIR-CD\cite{stanet} dataset.
Zheng et al. \cite{anychange} developed AnyChange, a zero-shot CD model built on the SAM that utilizes bitemporal latent matching for training-free adaptation, setting a new SOTA on the SECOND\cite{second} benchmark and achieving significant improvements in both unsupervised and supervised CD tasks.

\subsection{Knowledge Distillation in CD}

Knowledge distillation (KD), introduced by Hinton et al. \cite{hinton}, aims to transfer the representational knowledge of a teacher network to a smaller student network. 
In recent years, as the complexity of DL models in remote sensing tasks has increased, researchers have explored how to transfer knowledge from large, complex teacher models to smaller, more efficient student models through KD, thereby improving performance\cite{dsrkd, sar}.

Yan et al. \cite{distill1} proposed a novel self-supervised learning approach for unsupervised CD by fusing domain knowledge of remote sensing indices during both training and inference. By calculating cosine similarity, they selected high-similarity feature vectors from both the teacher and student networks to implement a hard negative sampling strategy, effectively improving CD performance.
Wang et al. \cite{distill2} addressed remote sensing semantic CD (SCD), which focuses on detecting changes in land cover and land use over time. The authors introduced a dual-dimension feature interaction network (DFINet) that enhances intraclass and interclass feature differentiation by incorporating a temporal difference feature enhancement (TDFE) module, which captures temporal features comprehensively. 
Wang et al. \cite{distill3} proposed a KD-based method for CD (CDKD), designed to overcome the challenges of deploying large deep learning models with high computational and storage requirements on resource-constrained spaceborne edge devices. 
Although these methods have successfully utilized KD to enhance the performance of various student models, they are tailored to specific models and do not provide a generalized distillation framework applicable to various CD models. 
Furthermore, there is a lack of open-source KD-based code for remote sensing image CD tasks.

In contrast, the proposed MTKD framework significantly improves the performance of CD models with various architectures and parameter sizes, and we commit to open-sourcing all the code and models.


\begin{table}[!t]
\centering
\caption{Information of Open-Source CD Datasets and the Proposed JL1-CD Dataset \label{table:dataset_information}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{lcccc}
\hline
Dataset & Class & Image Pairs & Image Size & Resolution \\ \hline
SZTAKI\cite{sztaki} & 1  & 13  & \begin{tabular}[c]{@{}c@{}}$952\times640$\\ $1,048\times724$\end{tabular} & 1.5 \\
DSIFN\cite{ifn}     & 1     & 394         & 512 × 512  & 2  \\
SECOND\cite{second}    & 6     & 4,662   & 512 × 512 & 0.5-3 \\
WHU-CD\cite{whucd}    & 1     & 1    & 32,20 × 15,354 & 0.2    \\
LEVIR-CD\cite{stanet}  & 1     & 637  & 1,024 × 1,024   & 0.3   \\
S2Looking\cite{s2looking} & 1  & 5,000 & 1,024 × 1,024 & 0.5-0.8 \\
CDD\cite{cdd}       & 1     & 16,000      & 256 × 256  & 0.03-1 \\
SYSU-CD\cite{sysucd}   & 1     & 20,000  & 256 × 256 & 0.5  \\
JL1-CD  & 1  & 5,000   & 512 × 512  & 0.5-0.75     \\ \hline
\end{tabular}
\end{table}

% Figure: Diagram
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/pipeline.png}
    \caption{Overview of the training (green boxes) and testing (pink boxes) pipelines of the proposed Origin-Partition (O-P) strategy and Multi-Teacher Knowledge Distillation (MTKD) framework.}
    \label{fig:pipeline}
\end{figure*}

\section{JL1-CD Dataset} \label{C}
High-resolution, all-inclusive CD datasets are crucial for remote sensing applications. High-resolution images provide richer spatial information, which is more conducive to visual interpretation compared to medium- and low-resolution images. 
Datasets with comprehensive change features enable the development of algorithms with greater generalization and transferability.  
Despite the numerous open-source change detection datasets proposed over the past decades, many still lack sub-meter-level resolution, and the variety of change types remains limited. 
These limitations hinder progress in CD research, particularly in the development of DL-based algorithms.

We collect the number of types of changes, number of image pairs, image size, and resolution information of mainstream CD datasets in Table~\ref{table:dataset_information}.  
The SZTAKI AirChange Benchmark \cite{sztaki} contains 12 pairs of 952 × 640 and one pair of 1,048 × 724 optical aerial images. It is one of the earliest and most commonly used CD datasets in early research. 
The DSIFN dataset \cite{ifn} consists of 6 large bi-temporal image pairs from 6 cities in China, which are cropped into 394 sub-image pairs, each sized 512 × 512. 
The SECOND dataset \cite{second} includes 4,662 pairs of aerial images collected from multiple platforms and sensors, covering cities such as Hangzhou, Chengdu, and Shanghai. Unlike other datasets that classify changes into only two categories (change and no change), SECOND provides detailed annotations for change types, including non-vegetated surfaces, trees, low vegetation, water, buildings, and playgrounds. 
However, the resolution of all or part of the images in these datasets does not reach the sub-meter level.  
WHU-CD \cite{whucd}, LEVIR-CD \cite{stanet}, and S2Looking \cite{s2looking} are very popular datasets that are specifically designed for monitoring building changes. 
These datasets predominantly include human-induced changes and lack natural change types. 
The CDD dataset is derived from 7 pairs of 4,725 × 2,700 real-world seasonal change remote sensing images. 
The SYSU-CD contains 20,000 pairs of 0.5-meter aerial images captured in Hong Kong between 2007 and 2014. 
These datasets feature high resolution and diverse change types.  

To provide a better benchmark for evaluating CD algorithms, we propose the JL1-CD dataset, a high-resolution, all-inclusive change detection dataset. 
JL1-CD includes 5,000 pairs of satellite images captured in China from early 2022 to the end of 2023, including Shandong, Ningxia, Anhui, Hebei, Hunan, and other regions.
The images have sub-meter resolutions ranging from 0.5 to 0.75 meters and are sized 512 × 512 pixels. 
As shown in Fig. \ref{fig:dataset-class}, the dataset covers various common human-induced and natural surface features, such as piled earth, buildings, roads, hardened surfaces, woodlands, grasslands, croplands, and water bodies.
The original 5,000 image pairs are divided into 4,000 pairs for training and 1,000 pairs for testing, following an 80:20 split.  
The JL1-CD dataset will be made openly available for all research needs.

% Figure: Dataset
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/dataset.png}
    \caption{Sample images with different change area ratios (CAR). Each column represents a specific CAR: (a) 0.00\%; (b) 19.98\%; (c) 39.93\%; (d) 59.96\%; (e) 80.25\%; and (f) 100.00\%.}
    \label{fig:dataset}
\end{figure*}

% Figure: CAR JL1-CD
\begin{figure}[!t]
    \centering
    \includegraphics[width=3.2in]{pic/CAP.png}
    \caption{CAR distribution of the training, validation and test sets in JL1-CD.}
    \label{fig:CAR-cg-cd}
\end{figure}

% Figure: CAR SYSUCD
\begin{figure}[!t]
    \centering
    \includegraphics[width=3.2in]{pic/CAP-SYSUCD.png}
    \caption{CAR distribution of the training and test sets in SYSU-CD.}
    \label{fig:CAR-sysucd}
\end{figure}

\section{Methodology} \label{D}
In this section, we provide a comprehensive overview of the proposed methods. 
In Section \ref{D.1}, we first introduce the Origin-Partition (O-P) strategy designed for the challenging all-inclusive CD dataset. 
Building upon the O-P strategy, we further present our Multi-Teacher Knowledge Distillation (MTKD) framework in Section \ref{D.2}. 
Finally, in Section \ref{D.3}, we describe the overall loss function used for training the teacher and student models.

\subsection{O-P Strategy} \label{D.1}

The traditional training and testing approach for CD models is illustrated in the upper-left corner of Fig. \ref{fig:pipeline} (green box) and the upper-right corner (pink box). 
For a given change detection model $\mathcal{M}$, the input consists of a pair of images $(X_1, X_2)$, and the output is a change map (CM). 
If the number of channels $c = 1$, the predicted label $\hat{Y}$ can be directly obtained using a thresholding method as follows:
\begin{equation}
\hat{Y}(i) = 
\begin{cases} 
1, & \text{if } CM(i) > th \\
0, & \text{if } CM(i) \leq th
\end{cases}
\end{equation}
where $(i)$ denotes the pixel location in the image, "1" represents a change, and "0" represents no change (for visualization purpose, "1" will be mapped to a grayscale value of 255). The threshold $th$ is a predefined value. 
If $c = 2$, the predicted label $\hat{Y}$ is typically determined using an element-wise comparison of the two channels, such as:
\begin{equation}
\hat{Y}(i) = 
\begin{cases} 
1, & \text{if } CM_0(i) \geq CM_1(i) \\
0, & \text{if } CM_0(i) < CM_1(i)
\end{cases}
\end{equation}
where $CM_0$ and $CM_1$ represent the first and second layers of the change map, respectively. 
If $c > 2$, it indicates that the CD model $\mathcal{M}$ not only predicts the locations of changes but also detects the types of changes, which is beyond the scope of this paper.

However, as shown in Fig. \ref{fig:dataset}, for a dataset like JL1-CD, where the Change Area Ratio (CAR) can range from 0\% to 100\%, training a single model using traditional methods may not be optimal. 
The model would struggle to learn the full range of change patterns in an all-inclusive manner. 
To address this issue, we propose the Origin-Partition (O-P) strategy to enhance the model's detection performance. 
As illustrated in the green boxes of Fig. \ref{fig:pipeline} (Step 1 and Step 2), for a given CD algorithm, we train the corresponding models in the following sequence:

1) The original model $\mathcal{M}_O$ is trained on the complete training set using the algorithm's default configuration.

2) As shown in Fig. \ref{fig:CAR-cg-cd}, the CAR range for the training, validation, and test sets is very large. 
However, the majority of images have a CAR of less than 5\%. Therefore, we set appropriate thresholds $th_1$ and $th_2$ and divide the original training set into three categories: small, medium, and large.

3) The models are then trained from scratch using the partitioned training sets, yielding models $\mathcal{M}_{T_S}$, $\mathcal{M}_{T_M}$, and $\mathcal{M}_{T_L}$. The training process can be formalized as:
\begin{equation}
\hat{Y} = 
\begin{cases}
f_{\mathcal{M}_{T_S}}(X_1, X_2), & \text{if } CAR_{GT} \leq th_1 \\
f_{\mathcal{M}_{T_M}}(X_1, X_2), & \text{if } th_1 < CAR_{GT} \leq th_2 \\
f_{\mathcal{M}_{T_L}}(X_1, X_2), & \text{if } CAR_{GT} > th_2
\end{cases}
\end{equation}
where $CAR_{GT}$ denotes the CAR calculated based on the ground truth label for the image pair $(X_1, X_2)$.

As shown in the middle pink box in Fig. \ref{fig:pipeline}, during testing, since we do not know the CAR of the test images, we first use $\mathcal{M}_O$ to estimate the CAR roughly. 
Based on this estimated CAR, we then classify the image into one of the three categories: small, medium, or large, and send it to the corresponding model $\mathcal{M}_{T_S}$, $\mathcal{M}_{T_M}$, or $\mathcal{M}_{T_L}$ to obtain the final detection result:
\begin{equation}
\hat{Y} = 
\begin{cases}
f_{\mathcal{M}_{T_S}}(X_1, X_2), & \text{if } CAR_{\mathcal{M}_O} \leq th_1 \\
f_{\mathcal{M}_{T_M}}(X_1, X_2), & \text{if } th_1 < CAR_{\mathcal{M}_O} \leq th_2 \\
f_{\mathcal{M}_{T_L}}(X_1, X_2), & \text{if } CAR_{\mathcal{M}_O} > th_2
\end{cases}
\end{equation}
where $CAR_{\mathcal{M}_O}$ denotes the CAR calculated based on the predicted label from the original model $\mathcal{M}_O$.

\subsection{MTKD Framework} \label{D.2}

By partitioning the training set, we effectively reduce the learning burden for each model. 
As a result, the Origin-Partition (O-P) strategy significantly enhances the performance of the CD algorithm. 
However, a clear issue arises: during inference, we are required to load 4 different models, and even disregarding data throughput, the time spent is at least twice that of the original algorithm (often much more). 
This substantially increases both memory and time complexity. 
We are thus prompted to consider: is there a way to combine the capabilities of these four models into a single model?

To address this, we propose the Multi-Teacher Knowledge Distillation (MTKD) framework. 
In the O-P strategy, we have already trained the models $\mathcal{M}_O$, $\mathcal{M}_{T_S}$, $\mathcal{M}_{T_M}$, and $\mathcal{M}_{T_L}$. 
Building on this, we further train a student model $\mathcal{M}_S$. 
First, we initialize the student model $\mathcal{M}_S$ using the parameters from $\mathcal{M}_O$, and then use $\mathcal{M}_{T_S}$, $\mathcal{M}_{T_M}$, and $\mathcal{M}_{T_L}$ as teacher models to perform KD. 
For each input image pair, we select the appropriate teacher model based on the image's CAR to guide the student model. 
The process is shown in the green box at the bottom of Fig. \ref{fig:pipeline}. 
In this framework, when the student model $\mathcal{M}_S$ is supervised by the ground truth labels, it also receives the CM information corresponding to different CAR ranges, provided by the teacher models. 

During the testing phase, only the student model $\mathcal{M}_S$ is used for inference, thereby significantly improving the model’s CD performance across different CAR ranges without introducing any additional computational cost.

\subsection{Loss Function} \label{D.3}
When training the original model $\mathcal{M}_O$ and the teacher models $\mathcal{M}_{T_S}$, $\mathcal{M}_{T_M}$, and $\mathcal{M}_{T_L}$, we employ the standard binary cross-entropy loss:
\begin{equation}
L_{\text{CE}} = \frac{1}{N} \sum_{i=1}^{N} ( -Y(i) \log(\hat{Y}(i)) - (1 - Y(i)) \log(1 - \hat{Y}(i)) )
\end{equation}
where $Y(i)$ denotes the ground truth label of pixel $i$.

When training the student model $\mathcal{M}_S$, we select the appropriate teacher model based on the CAR range. 
Then the mean squared error (MSE) is computed at the CM layer as the distillation loss:
\begin{equation}
L_{\text{KD}} = \frac{1}{N} \sum_{i=1}^{N} \left( CM(i) - CM_{\mathcal{T}}(i) \right)^2, \quad \mathcal{T} \in \{ T_S, T_M, T_L \}.
\end{equation}

Thus, the complete loss function for training $\mathcal{M}_S$ is given by:
\begin{equation}
L = L_{\text{CE}} + \lambda L_{\text{KD}}
\end{equation}
where the parameter $\lambda$ is used to balance the contributions of the cross-entropy loss and the distillation loss.


% Table: models
% 挑选的模型按照时间顺序排列
% Alice Blue: CNN
% Light Cyan: Transformer
% Lavender Blue: foundation model
\begin{table*}[!t]
\centering
\caption{Benchmark Methods and the Corresponding Implementation Details \label{table:benchmark}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{lcccccccc}
\hline
Method                         & Backbone                                    & Param (M) & Flops (G) & Initial LR & $\lambda$ & Scheduler         & Batch Size & GPU  \\ \hline
FC-EF\cite{fcsn}                          & \cellcolor[HTML]{ECF4FF}CNN                 & 1.353     & 12.976    & 1e-3          & -         & LinearLR          & 8          & 3090 \\
FC-Siam-Conc\cite{fcsn}                   & \cellcolor[HTML]{ECF4FF}CNN                 & 1.548     & 19.956    & 1e-3          & -         & LinearLR          & 8          & 3090 \\
FC-Siam-Diff\cite{fcsn}                   & \cellcolor[HTML]{ECF4FF}CNN                 & 1.352     & 17.540    & 1e-3          & -         & LinearLR          & 8          & 3090 \\ \hline
STANet-Base\cite{stanet}                    & \cellcolor[HTML]{ECF4FF}ResNet-18           & 12.764    & 70.311    & 1e-3          & 5e-3      & LinearLR          & 8          & 3090 \\ \hline
IFN\cite{ifn}                            & \cellcolor[HTML]{ECF4FF}VGG-16              & 35.995    & 323.584   & 1e-3          & 1e-4      & LinearLR          & 8          & 3090 \\ \hline
SNUNet-c16\cite{snunet}                     & \cellcolor[HTML]{ECF4FF}CNN                 & 3.012     & 46.921    & 1e-3          & 1e-4      & LinearLR          & 8          & 3090 \\ \hline
BIT\cite{bit}                            & \cellcolor[HTML]{ECF4FF}ResNet-18           & 2.990     & 34.996    & 1e-3          & 1e-4      & LinearLR          & 8          & 3090 \\ \hline
                               & \cellcolor[HTML]{ECF4FF}FarSeg (ResNet-18)\cite{farseg}  & 16.965    & 76.845    & 1e-3          & 1e-3      & LinearLR          & 16         & 3090 \\
\multirow{-2}{*}{ChangeStar\cite{changestar}}   & \cellcolor[HTML]{ECF4FF}UPerNet (ResNet-18)\cite{upernet} & 13.952    & 55.634    & 1e-3          & 1e-4      & LinearLR          & 8          & 3090 \\ \hline
                               & \cellcolor[HTML]{96FFFB}MiT-b0              & 3.847     & 11.380    & 6e-5          & 1e-3      & LinearLR          & 8          & 3090 \\
\multirow{-2}{*}{ChangeFormer\cite{changeformer}} & \cellcolor[HTML]{96FFFB}MiT-b1              & 13.941    & 26.422    & 6e-5          & 5e-4      & LinearLR          & 8          & 3090 \\ \hline
TinyCD\cite{tinycd}                         & \cellcolor[HTML]{ECF4FF}CNN                 & 0.285     & 5.791     & 3.57e-3       & 1e-5      & LinearLR          & 8          & 3090 \\ \hline
HANet\cite{hanet}                          & \cellcolor[HTML]{ECF4FF}CNN                 & 3.028     & 97.548    & 1e-3          & 1e-3      & LinearLR          & 8          & A800 \\ \hline
                               & \cellcolor[HTML]{96FFFB}MiT-b0              & 3.457     & 8.523     & 1e-4          & 1e-4      & LinearLR          & 8          & 3090 \\
                               & \cellcolor[HTML]{96FFFB}MiT-b1              & 13.355    & 23.306    & 1e-4          & 1e-3      & LinearLR          & 8          & 3090 \\
                               & \cellcolor[HTML]{ECF4FF}ResNet-18           & 11.391    & 23.820    & 5e-3          & 1e-3      & LinearLR          & 8          & 3090 \\
\multirow{-4}{*}{Changer\cite{changer}}      & \cellcolor[HTML]{ECF4FF}ResNeSt-50          & 26.693    & 67.241    & 5e-3          & 1e-5      & LinearLR          & 8          & 3090 \\ \hline
LightCDNet-s\cite{lightcdnet}                   & \cellcolor[HTML]{ECF4FF}CNN                 & 0.342     & 6.995     & 3e-3          & 5e-3      & LinearLR          & 8          & 3090 \\ \hline
CGNet\cite{cgnet}                          & \cellcolor[HTML]{ECF4FF}VGG-16              & 38.989    & 425.984   & 5e-4          & 1e-3      & LinearLR          & 8          & A800 \\ \hline
                               & \cellcolor[HTML]{CBCEFB}ViT-B               & 91.346    & 74.409    & 1e-4          & -         & LinearLR          & 8          & 3090 \\
                               & \cellcolor[HTML]{CBCEFB}ViT-B (IN21K)       & 115.712   & 83.142    & 1e-4          & -         & LinearLR          & 8          & 3090 \\
\multirow{-3}{*}{BAN\cite{ban}}          & \cellcolor[HTML]{CBCEFB}ViT-L               & 261.120   & 346.112   & 1e-4          & 1e-3      & LinearLR          & 8          & A800 \\ \hline
TTP\cite{ttp}                            & \cellcolor[HTML]{CBCEFB}SAM\cite{sam}                 & 361.472   & 929.792   & 4e-4          & 5e-3      & CosineAnnealingLR & 8          & A800 \\ \hline
\end{tabular}
\end{table*}


% Table: 
\begin{table*}[!t]
\centering
\caption{Experimental Results on JL1-CD Test Set \label{table:metrics}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{ccccccccccccc}
\hline
Method                                                                           & Strategy & mIoU           & mAcc           & mPrecision     & mFscore        &  & Method                                                                           & Strategy & mIoU           & mAcc           & mPrecision     & mFscore        \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}STANet\\ (Base)\end{tabular}}         & -        & 66.76          & 81.71          & 74.73          & 74.73          &  & \multirow{3}{*}{IFN}                                                             & -        & 71.25          & 78.91          & 84.53          & 77.33          \\
                                                                                 & O-P      & 64.56          & 78.47          & \textbf{78.47} & 71.25          &  &                                                                                  & O-P      & 71.06          & 78.37          & 84.28          & 77.21          \\
                                                                                 & MTKD     & \textbf{67.92} & \textbf{82.07} & 76.24          & \textbf{75.10} &  &                                                                                  & MTKD     & \textbf{72.72} & \textbf{80.28} & \textbf{84.66} & \textbf{78.80} \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SNUNet\\ (c16)\end{tabular}}          & -        & 68.97          & 74.87          & \textbf{85.06} & 75.25          &  & \multirow{3}{*}{BIT}                                                             & -        & 67.22          & 74.47          & 83.71          & 73.37          \\
                                                                                 & O-P      & \textbf{71.39} & \textbf{78.60} & 83.36          & \textbf{77.98} &  &                                                                                  & O-P      & \textbf{69.41} & \textbf{76.29} & 84.02          & \textbf{75.77} \\
                                                                                 & MTKD     & 71.12          & 78.27          & 84.96          & 77.56          &  &                                                                                  & MTKD     & 68.86          & 75.49          & \textbf{84.71} & 74.88          \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ChangeStar\\ (FarSeg)\end{tabular}}   & -        & \textbf{69.47} & 75.58          & 84.46          & \textbf{75.57} &  & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ChangeStar\\ (UPerNet)\end{tabular}}  & -        & 64.85          & 69.18          & \textbf{88.26} & 70.19          \\
                                                                                 & O-P      & 68.87          & 74.74          & \textbf{84.90} & 74.86          &  &                                                                                  & O-P      & 64.68          & 69.05          & 87.23          & 70.08          \\
                                                                                 & MTKD     & 69.14          & \textbf{76.49} & 82.09          & 75.41          &  &                                                                                  & MTKD     & \textbf{65.10} & \textbf{70.26} & 87.69          & \textbf{70.58} \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ChangeFormer\\ (MiT-b0)\end{tabular}} & -        & \textbf{73.51} & \textbf{80.46} & 86.33          & \textbf{79.70} &  & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ChangeFormer\\ (MiT-b1)\end{tabular}} & -        & 73.05          & 79.70          & 86.95          & 79.22          \\
                                                                                 & O-P      & 72.58          & 79.16          & 86.33          & 78.79          &  &                                                                                  & O-P      & 73.45          & 79.19          & \textbf{87.45} & 79.41          \\
                                                                                 & MTKD     & 73.25          & 79.20          & \textbf{87.15} & 79.30          &  &                                                                                  & MTKD     & \textbf{73.92} & \textbf{80.43} & 86.89          & \textbf{80.18} \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{TinyCD}                                                          & -        & 71.04          & 78.77          & 83.05          & 77.74          &  & \multirow{3}{*}{HANet}                                                           & -        & 63.64          & 69.77          & 83.43          & 69.39          \\
                                                                                 & O-P      & 72.22          & 79.93          & \textbf{83.49} & 78.76          &  &                                                                                  & O-P      & \textbf{69.05} & \textbf{76.53} & 83.05          & \textbf{75.66} \\
                                                                                 & MTKD     & \textbf{72.55} & \textbf{80.98} & 83.17          & \textbf{79.26} &  &                                                                                  & MTKD     & 67.67          & 74.39          & \textbf{84.38} & 73.92          \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b0)\end{tabular}}      & -        & 74.85          & \textbf{81.84} & 86.09          & 80.98          &  & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b1)\end{tabular}}      & -        & 75.94          & 81.99          & \textbf{87.74} & 81.93          \\
                                                                                 & O-P      & 75.29          & 81.40          & 87.06          & \textbf{81.32} &  &                                                                                  & O-P      & 75.42          & 81.67          & 87.13          & 81.43          \\
                                                                                 & MTKD     & \textbf{75.35} & 81.76          & \textbf{87.18} & 81.28          &  &                                                                                  & MTKD     & \textbf{76.15} & \textbf{82.85} & 86.98          & \textbf{82.13} \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (r18)\end{tabular}}         & -        & 68.37          & 75.15          & 83.43          & 74.54          &  & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (s50)\end{tabular}}         & -        & 62.31          & 69.23          & 80.91          & 67.83          \\
                                                                                 & O-P      & \textbf{70.76} & \textbf{77.42} & \textbf{83.86} & \textbf{77.01} &  &                                                                                  & O-P      & \textbf{71.80} & \textbf{79.76} & \textbf{83.15} & \textbf{78.23} \\
                                                                                 & MTKD     & 69.45          & 77.26          & 81.50          & 75.86          &  &                                                                                  & MTKD     & 62.96          & 69.65          & 81.76          & 68.52          \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}LightCDNet\\ (s)\end{tabular}}        & -        & 66.70          & 73.21          & 83.45          & 72.46          &  & \multirow{3}{*}{CGNet}                                                           & -        & 73.37          & 80.31          & 85.33          & 79.65          \\
                                                                                 & O-P      & \textbf{70.19} & \textbf{77.43} & \textbf{83.99} & \textbf{76.16} &  &                                                                                  & O-P      & 72.95          & 79.71          & 85.50          & 79.12          \\
                                                                                 & MTKD     & 65.99          & 72.44          & 83.86          & 71.48          &  &                                                                                  & MTKD     & \textbf{73.82} & \textbf{80.32} & \textbf{86.33} & \textbf{79.91} \\ \cline{1-6} \cline{8-13} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}BAN\\ (ViT-L)\end{tabular}}           & -        & 73.54          & 79.54          & 87.89          & 79.47          &  & \multirow{3}{*}{TTP}                                                             & -        & 75.05          & 80.24          & \textbf{89.82} & 80.76          \\
                                                                                 & O-P      & 73.61          & 79.17          & \textbf{88.10} & 79.45          &  &                                                                                  & O-P      & 76.69          & \textbf{83.48} & 87.27          & 82.52          \\
                                                                                 & MTKD     & \textbf{73.95} & \textbf{80.26} & 87.12          & \textbf{79.92} &  &                                                                                  & MTKD     & \textbf{76.85} & 82.99          & 88.05          & \textbf{82.56} \\ \cline{1-6} \cline{8-13} 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BAN\\ (ViT-B)\end{tabular}}           & -        & \textbf{73.30} & \textbf{80.36} & 85.91          & \textbf{79.47} &  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BAN\\ (ViT-B-IN21K)\end{tabular}}      & -        & \textbf{74.69} & \textbf{81.09} & \textbf{87.14} & \textbf{80.75} \\
                                                                                 & O-P      & 72.47          & 78.78          & \textbf{86.31} & 78.58          &  &                                                                                  & O-P      & 73.50          & 79.98          & 86.25          & 79.50          \\ \cline{1-6} \cline{8-13} 
\multirow{2}{*}{FC-EF}                                                           & -        & \textbf{57.08} & \textbf{61.90} & 86.40          & \textbf{61.28} &  & \multirow{2}{*}{FC-Siam-Conc}                                                    & -        & \textbf{63.79} & \textbf{69.54} & 84.77          & \textbf{69.19} \\
                                                                                 & O-P      & 49.59          & 53.30          & \textbf{95.54} & 51.47          &  &                                                                                  & O-P      & 60.25          & 63.84          & \textbf{91.19} & 64.72          \\ \cline{1-6} \cline{8-13} 
\multirow{2}{*}{FC-Siam-Diff}                                                    & -        & \textbf{61.30} & \textbf{66.03} & 86.45          & \textbf{66.34} &  &                                                                                  &          &                &                &                &                \\
                                                                                 & O-P      & 56.49          & 60.05          & \textbf{91.64} & 60.57          &  &                                                                                  &          &                &                &                &                \\ \hline
\end{tabular}
\end{table*}

\section{EXPERIMENT} \label{E}

\subsection{Dataset Description}
We first conduct experiments on our JL1-CD dataset. 
Additionally, to validate the robustness of the proposed O-P strategy and MTKD framework, we further perform experiments on the SYSU-CD dataset \cite{sysucd}. The detailed information for these two datasets is as follows:

1) JL1-CD Dataset: As described in Section \ref{C}, the JL1-CD dataset consists of 5,000 pairs of high-resolution images, with a resolution of 0.5-0.75 meters and image size of 512 × 512. In the competition, the first 4,000 image pairs are used for training, and the remaining 1,000 image pairs are used for testing (with the ground truth labels being unavailable during the competition). 
To ensure sufficient data for training, we use the first 100 image pairs as the validation set, the next 3,900 image pairs as the training set, and the remaining 1,000 image pairs for testing. 
As shown in Fig. \ref{fig:CAR-cg-cd}, our data split is reasonable, as the CAR distributions of the three sets are very similar.

2) SYSU-CD Dataset: As illustrated in Fig. \ref{fig:CAR-sysucd}, the SYSU-CD dataset is also a challenging dataset with a very large CAR range, so we choose this dataset as the second benchmark to validate the robustness of our proposed methods. 
The SYSU-CD dataset contains 12,000 pairs for training, 4,000 pairs for validation, and 4,000 pairs for testing, with each image having a resolution of 0.5 meters and a size of 256 × 256.


% Figure: visual comparison on JL1-CD
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/visual.png}
    \caption{Visual comparison on the JL1-CD dataset. Each row, from top to bottom, represents the following: image at time 1, image at time 2, ground truth, output from the original model, output from the O-P strategy, and output from the MTKD framework. Red denotes missed detections (FN), while blue indicates false alarms (FP). The selected algorithms are: (a) BAN-ViT-L, (b) BIT, (c) TTP, (d) SNUNet, (e) IFN, (f) Changer-MiT-b1, (g) ChangeFormer-MiT-b1, (h) TinyCD, and (i) CGNet.}
    \label{fig:visual}
\end{figure*}

\subsection{Benchmark Methods}
To comprehensively verify the validity of our JL1-CD dataset and the superiority of the proposed O-P strategy and MTKD framework, we conduct extensive experiments on existing benchmark algorithms. 
The selected models, along with their corresponding backbones, parameter sizes, and computational complexities, are summarized in Table \ref{table:benchmark}. 
Based on the backbone architecture, the models are categorized into three groups: 
Alice Blue for CNN-based models, Light Cyan for Transformer-based models, and Lavender Blue for FM-based models, encompassing almost all mainstream architectures. 
FLOPs is calculated with an input image of size 512 × 512. As shown in the table, the selected models span a wide range of sizes, from lightweight models such as TinyCD and LightCDNet with less than 1MB to the latest SOTA model TTP, which exceeds 360MB. 
This wide range allows us to verify the universality of the O-P and MTKD methods across models with different backbones and scales.

\subsection{Evaluation Metrics}
The common evaluation metrics for CD models include Intersection over Union (IoU), accuracy, precision, recall, and F1-score. 
IoU measures the overlap between the detected change region and the ground truth. 
Accuracy reflects the overall correctness of the model. Precision indicates the false positive rate of the model, recall reflects the false negative rate, and F1-score balances both precision and recall. 
A higher F1-score indicates better detection performance.

However, given the large CAR range in the JL1-CD dataset, both change and non-change regions are equally important. Therefore, we choose the averaged versions of the aforementioned metrics, which are calculated as follows:
\begin{equation}
\begin{aligned}
\text{mIoU} &= \frac{1}{2} \left( \text{IoU}_0 + \text{IoU}_1 \right) \\
&= \frac{1}{2} \left( \frac{\text{TN}}{\text{TN} + \text{FP} + \text{FN}} + \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}} \right) 
\\
\text{mPrecision} &= \frac{1}{2} \left( \text{Precision}_0 + \text{Precision}_1 \right) \\
&= \frac{1}{2} \left( \frac{\text{TN}}{\text{TN} + \text{FN}} + \frac{\text{TP}}{\text{TP} + \text{FP}} \right) 
\\
\text{mAcc} &= \frac{1}{2} \left( \text{Recall}_0 + \text{Recall}_1 \right) \\
&= \frac{1}{2} \left( \frac{\text{TN}}{\text{TN} + \text{FP}} + \frac{\text{TP}}{\text{TP} + \text{FN}} \right) 
\\
\text{mFscore} &= \frac{1}{2} \left( \text{Fscore}_0 + \text{Fscore}_1 \right) \\
&= \frac{1}{2}\sum_{i=0}^1\frac{2 \times \text{Precision}_i \times \text{Recall}_i}{\text{Precision}_i + \text{Recall}_i}
\end{aligned}
\end{equation}
where TP, FP, TN, and FN represent true positives, false positives, true negatives, and false negatives, respectively. 
The averaged accuracy and recall are equivalent, so we only use mAcc for consistency in subsequent experiments.

It is important to note that in the toolboxes like MMSegmentation\cite{mmseg} and MMRotate \cite{zhou2022mmrotate},  these metrics are computed based on the total number of pixels across all predicted label images. 
In this paper, however, to align with the competition requirements, we first calculate these metrics for each individual image and then compute the average of the results across all images to obtain the final outcome.

% Figure: CAR per Partition
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/CAR_per_Partition.png}
    \caption{mIoU of HANet, ChangeFormer-MiT-b1, and TTP across different CAR ranges. The first and second rows show results on the validation and test sets, respectively. In each plot, the left y-axis represents CAR size, and the right y-axis represents mIoU.}
    \label{fig:car_per_partition}
\end{figure*}


\subsection{Implementation Details}
All algorithms are trained and tested using the PyTorch-based OpenCD Toolbox \cite{opencd}. 
To ensure a fair comparison and clearly assess the contributions of the Origin-Partition (O-P) strategy and the Multi-Teacher Knowledge Distillation (MTKD) framework to the performance improvement of the CD models, we adopt consistent settings for all models ($\mathcal{M}_O$, $\mathcal{M}_{T_L}$, $\mathcal{M}_{T_M}$, $\mathcal{M}_{T_S}$, and $\mathcal{M}_S$) across the various algorithms, as described below:

\begin{enumerate}[1)]
    \item The patch size of the input images is 512 × 512, which matches the original image dimensions. 
    \item Data augmentation methods, including RandomRotate, RandomFlip, and PhotoMetricDistortion, are applied.
    \item The AdamW optimizer is used with $\beta_1 = 0.9$ and $\beta_2 = 0.99$, and the default batch size is set to 8 image pairs (with a batch size of 16 for ChangeStar-FarSeg).
    \item Models $\mathcal{M}_O$, $\mathcal{M}_{T_L}$, $\mathcal{M}_{T_M}$, and $\mathcal{M}_{T_S}$ are trained for 200k iterations on the original and the corresponding partitioned datasets (300 epochs for TTP), while the student model $\mathcal{M}_S$ is trained for an additional 100k iterations (100 epochs for TTP) on the original dataset.
    \item Training begins with a warm-up phase of 1k iterations (5 epochs for TTP), during which the learning rate (LR) is linearly increased from 1e-6 to the initial LR value, as specified in Table \ref{table:benchmark}. 
Afterward, the LR is linearly decayed to 0 as training progresses (TTP employs a CosineAnnealing decay schedule).
    \item The HANet, CGNet, BAN (ViT-L), and TTP models are trained on the NVIDIA A800 server, while other models are trained on the NVIDIA RTX 3090 server. All models are tested on the A800 server.
\end{enumerate}

When partitioning the dataset, we set the thresholds $th_1 = 0.05$ and $th_2 = 0.2$, which ensures a balanced distribution of samples across the partitions. 
The model is saved every 1k iterations (5 epochs for TTP), and the checkpoint with the highest mIoU value on the validation set is selected for testing.

For the training of $\mathcal{M}_{T_S}$, 
due to the varying distributions of the change maps and the different magnitudes of $L_{CE}$ across modelss, a grid search is performed over the set $\{1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2\}$ to determine the optimal distillation loss weight $\lambda$ for each model. 
More configuration details are summarized in Table \ref{table:benchmark}.

\begin{table}[!t]
\centering
\caption{Comparison of Detection Results on Change and No-Change Classes \label{table:Metrics on Different Classes}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{cccccc}
\hline
Method                                                                            & Class     & IoU                          & Acc                          & Precision                    & Fscore                       \\ \hline
                                                                                  & unchanged & +0.24                        & +0.29                        & +0.04                        & +0.12                        \\
\multirow{-2}{*}{IFN}                                                             & changed   & {\textbf{+2.71}} & {\textbf{+2.44}} & {\textbf{+0.22}} & {\textbf{+2.82}} \\ \hline
                                                                                  & unchanged & +0.10                        & -0.60                        & {\textbf{ +0.65}} & +0.06                        \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}SNUNet\\ (c16)\end{tabular}}          & changed   & {\textbf{+4.21}} & {\textbf{+7.38}} & -0.86                        & {\textbf{+4.54}} \\ \hline
                                                                                  & unchanged & {\textbf{+0.21}} & {\textbf{-0.02}} & +0.24                        & {\textbf{+0.18}} \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}ChangeFormer\\ (MiT-b0)\end{tabular}} & changed   & -0.74                        & -2.51                        & {\textbf{+1.41}} & -0.99                        \\ \hline
                                                                                  & unchanged & +0.07                        & +0.12                        & {\textbf{ -0.01}} & +0.05                        \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}ChangeFormer\\ (MiT-b1)\end{tabular}} & changed   & {\textbf{ +1.68}} & {\textbf{+1.35}} & -0.11                        & {\textbf{+1.86}} \\ \hline
                                                                                  & unchanged & +0.30                        & +0.29                        & +0.08                        & +0.20                        \\
\multirow{-2}{*}{TinyCD}                                                          & changed   & {\textbf{+2.72}} & {\textbf{+4.13}} & {\textbf{+0.16}} & {\textbf{+2.85}} \\ \hline
                                                                                  & unchanged & +0.21                        & {\textbf{+0.32}} & -0.14                        & +0.19                        \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b0)\end{tabular}}      & changed   & {\textbf{+0.80}} & -0.48                        & {\textbf{+2.32}} & {\textbf{+0.41}} \\ \hline
                                                                                  & unchanged & +0.02                        & +0.09                        & {\textbf{-0.04}} & -0.01                        \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b1)\end{tabular}}      & changed   & {\textbf{+0.41}} & {\textbf{+1.63}} & -1.47                        & {\textbf{+0.42}} \\ \hline
                                                                                  & unchanged & +0.02                        & -0.03                        & -0.04                        & -0.06                        \\
\multirow{-2}{*}{CGNet}                                                           & changed   & {\textbf{+0.88}} & {\textbf{ +0.03}} & {\textbf{+2.04}} & {\textbf{+0.59}} \\ \hline
                                                                                  & unchanged & +0.12                        & +0.23                        & {\textbf{-0.09}} & +0.07                        \\
\multirow{-2}{*}{\begin{tabular}[c]{@{}c@{}}BAN\\ (ViT-L)\end{tabular}}           & changed   & {\textbf{+0.70}} & {\textbf{+1.21}} & -1.46                        & {\textbf{+0.82}} \\ \hline
                                                                                  & unchanged & +0.23                        & -0.19                        & {\textbf{ +0.45}} & +0.20                        \\
\multirow{-2}{*}{TTP}                                                             & changed   & {\textbf{+3.36}} & {\textbf{+5.69}} & -3.99                        & {\textbf{+3.39}} \\ \hline
\end{tabular}
\end{table}

\subsection{Experimental Results}
\paragraph{Quantitative Comparison}  
Table \ref{table:metrics} summarizes the numerical results of mIoU, mAcc, mPrecision, and mFscore for all methods on the JL1-CD test set, trained under the original, O-P, and MTKD strategies. 
As shown in the table, we observe that the O-P strategy does not lead to performance improvements for BAN-ViT-B, BAN-ViT-B-IN21k, FC-EF, FC-Siam-Conc, and FC-Siam-Diff. 
This suggests that the partition-based training method is not suitable for these algorithms, and thus cannot obtain sufficiently strong teacher models. 
Therefore, we do not continue with MTKD experiments for these methods. 
However, the O-P strategy or MTKD framework can improve the performance of all other algorithms to a certain extent. 
The following conclusions can be drawn from these results:
\begin{enumerate}[1)]
    \item Surprisingly, unlike their performance on the validation set, many algorithms under O-P perform worse than under MTKD, and in some cases, even worse than the original models (e.g., STANet, IFN, Changer-MiT-b1, etc.). However, the student models obtained through MTKD training can achieve significantly better performance than models trained under the original and O-P methods. This is mainly due to the inability of the original models to achieve accurate CAR on the test set during the coarse detection stage. This finding also indicates that our MTKD method can endow student models with superior capabilities compared to the teacher models, without increasing computation and time cost during inference.
    \item After MTKD optimization, the single Changer-MiT-b0 and Changer-MiT-b1 models can outperform the original TTP in terms of mIoU. Additionally, the TTP model, after MTKD optimization, shows improvements in mIoU and mFscore by 1.30\% and 1.80\%, respectively, setting a new SOTA.
    \item The O-P strategy shows the most significant improvement for Changer-MiT-s50 (with a 9.49\% increase in mIoU and a 10.4\% increase in mFscore), while the MTKD framework yields the most significant performance boost for HANet (with a 4.03\% increase in mIoU and a 4.53\% increase in mFscore).
\end{enumerate}


\paragraph{Visual Comparison}  
In remote sensing change detection, four main challenges are typically encountered: false alarms, missed detections, internal density, and boundary completeness. 
To visually assess the effectiveness of the O-P and MTKD strategies in addressing these challenges, we present the visual results of several algorithms in Fig. \ref{fig:visual}, where red indicates missed detections and blue indicates false alarms. 
The first five columns show performance in terms of missed detection. 
As depicted, the original algorithms struggle to detect small changes accurately, particularly for objects with subtle variations. 
However, after optimization with O-P and MTKD, the missed detection rate for small-scale changes is significantly reduced. 
Of course, the improvement is limited; for example, in the first and fourth columns, although MTKD detects more road changes, it still cannot fully capture the narrow variations. 
The sixth column displays the performance in terms of false alarms, where it is evident that the false alarm rate decreases progressively across the original, O-P, and MTKD methods. 
The last three columns illustrate the improvements in internal density and boundary completeness. 
We effectively reduce the occurrence of internal holes within the change areas, and the boundaries become more complete.

\paragraph{Results on Different CAR Partitions}  
We select one model from each of the CNN, Transformer, and FM architectures and evaluate their performance across different CAR ranges. 
Fig. \ref{fig:car_per_partition} summarizes the mIoU results of HANet, ChangeFormer-MiT-b1, and TTP on both the validation set (first row) and the test set (second row). 
The images are sorted by CAR in ascending order and divided into five equal partitions. 
Each bar in the figure represents the lower and upper bounds of CAR for each partition, while the line graph indicates the mIoU across the different partitions. 
From the figure,  it can be observed that the detection performance of O-P and MTKD may actually decrease for images with high CAR (e.g., ChangeFormer-MiT-b1 and TTP). 
However, O-P and MTKD significantly enhance model performance on images with low CAR. 
In the first partition of the test set (CAR range 0.02\% to 0.88\%), O-P improves the mIoU for the three algorithms by 8.15\%, 3.21\%, and 6.55\%, respectively, while MTKD improves it by 4.42\%, 3.36\%, and 2.85\%. 
These results demonstrate a significant improvement in the accuracy of detecting tiny changes, which is consistent with the findings in Fig. \ref{fig:visual}.

\paragraph{Comparison of Results on Change and No-Change Classes}  
We further select all algorithms with an mIoU greater than 70\% under the MTKD framework and compared their performance in detecting change and no-change regions. 
The experimental results are summarized in Table \ref{table:Metrics on Different Classes}. 
Using the original models as a baseline, we analyze the contribution of MTKD to different metrics. 
Except for the ChangeFormer-MiT-b0 model, all MTKD-optimized models demonstrated significant improvements in both IoU and Fscore for the change regions. 
This indicates that the MTKD framework is more sensitive to the detection of change areas.


% Figure: visual comparison on JL1-CD
\begin{figure*}[!t]
    \centering
    \includegraphics[width=7in]{pic/visual_sysucd.png}
    \caption{Visual comparison on the SYSU-CD dataset. Red denotes missed detections (FN). Blue indicates false alarms (FP). (a) Image at Time 1. (b) Image at Time 2. (c) Ground Truth. (d) Changer-MiT-b1 (Original). (e) Changer-MiT-b1 (MTKD). (f) CGNet (Original). (g) CGNet (MTKD). (h) TTP (Original). (i) TTP (MTKD).}
    \label{fig:visual-sysucd}
\end{figure*}

\begin{table*}[!t]
\centering
\caption{Impact of Different Numbers of Teacher Models on O-P and MTKD Performance \label{table:Metrics_with_two_teachers}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{cccllll}
\hline
Method                                                                       & Strategy               & No. of $\mathcal{M}_T$ & \multicolumn{1}{c}{mIOU}                                 & \multicolumn{1}{c}{mAcc}                                 & \multicolumn{1}{c}{mPrecision}                           & \multicolumn{1}{c}{mFscore}                              \\ \hline
                                                                             &                        & 3                      & \multicolumn{1}{c}{75.29 (+0.44)}                        & \multicolumn{1}{c}{81.40 (-0.44)}                        & \multicolumn{1}{c}{\textbf{87.06 (+0.97)}} & \multicolumn{1}{c}{81.32 (+0.34)}                        \\
                                                                             & \multirow{-2}{*}{O-P}  & 2                      & \multicolumn{1}{c}{\textbf{75.44 (+0.59)}} & \multicolumn{1}{c}{\textbf{81.96 (+0.12)}} & \multicolumn{1}{c}{85.85 (-0.24)}                        & \multicolumn{1}{c}{\textbf{81.51 (+0.53)}} \\ \cline{2-7} 
                                                                             &                        & 3                      & \multicolumn{1}{c}{75.35 (+0.50)}                        & \multicolumn{1}{c}{81.76 (-0.08)}                        & \multicolumn{1}{c}{\textbf{87.18 (+1.09)}} & \multicolumn{1}{c}{81.28 (+0.30)}                        \\
\multirow{-4}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b0)\end{tabular}} & \multirow{-2}{*}{MTKD} & 2                      & \multicolumn{1}{c}{\textbf{75.72 (+0.87)}} & \multicolumn{1}{c}{\textbf{82.30 (+0.46)}} & \multicolumn{1}{c}{86.80 (+0.71)}                        & \multicolumn{1}{c}{\textbf{81.66 (+0.68)}} \\ \hline
                                                                             &                        & 3                      & \multicolumn{1}{c}{75.42 (-0.52)}                        & \multicolumn{1}{c}{81.67 (-0.32)}                        & \multicolumn{1}{c}{{\color[HTML]{333333} 87.13 (-0.61)}} & \multicolumn{1}{c}{81.43 (-0.50)}                        \\
                                                                             & \multirow{-2}{*}{O-P}  & 2                      & \multicolumn{1}{c}{\textbf{75.91 (-0.03)}} & \multicolumn{1}{c}{\textbf{82.11 (+0.12)}} & \multicolumn{1}{c}{\textbf{87.87 (+0.13)}} & \multicolumn{1}{c}{\textbf{81.97 (+0.04)}} \\ \cline{2-7} 
                                                                             &                        & 3                      & 76.15 (+0.21)                                            & 82.85 (+0.86)                                            & 86.98 (-0.76)                                            & 82.13 (+0.20)                                            \\
\multirow{-4}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b1)\end{tabular}} & \multirow{-2}{*}{MTKD} & 2                      & \textbf{76.77 (+0.83)}                     & \textbf{83.38 (+1.39)}                     & \textbf{87.30 (-0.44)}                     & \textbf{82.66 (+0.73)}                     \\ \hline
                                                                             &                        & 3                      & 72.95 (-0.42)                                            & 79.71 (-0.60)                                            & \textbf{85.50 (+0.17)}                     & 79.12 (-0.53)                                            \\
                                                                             & \multirow{-2}{*}{O-P}  & 2                      & \textbf{73.56 (+0.19)}                     & \textbf{80.76 (+0.45)}                     & 85.07 (-0.26)                                            & \textbf{79.92 (+0.27)}                     \\ \cline{2-7} 
                                                                             &                        & 3                      & \textbf{73.82 (+0.45)}                     & 80.32 (+0.01)                                            & \textbf{86.33 (+1.00)}                     & \textbf{79.91 (+0.26)}                     \\
\multirow{-4}{*}{CGNet}                                                      & \multirow{-2}{*}{MTKD} & 2                      & 73.78 (+0.41)                                            & \textbf{80.61 (+0.29)}                     & 85.67 (+0.34)                                            & 79.89 (+0.24)                                            \\ \hline
                                                                             &                        & 3                      & \textbf{76.69 (+1.64)}                     & \textbf{83.48 (+3.24)}                     & 87.27 (-2.55)                                            & \textbf{82.52 (+1.76)}                     \\
                                                                             & \multirow{-2}{*}{O-P}  & 2                      & 76.65 (+1.60)                                            & 82.98 (+2.74)                                            & \textbf{87.39 (-2.43)}                     & 82.49 (+1.73)                                            \\ \cline{2-7} 
                                                                             &                        & 3                      & \textbf{76.85 (+1.80)}                     & 82.99 (+2.75)                                            & \textbf{88.05 (-1.77)}                     & \textbf{82.56 (+1.80)}                     \\
\multirow{-4}{*}{TTP}                                                        & \multirow{-2}{*}{MTKD} & 2                      & 76.31 (+1.26)                                            & \textbf{83.24 (+3.00)}                     & 86.81 (-3.01)                                            & 82.22 (+1.46)                                            \\ \hline
\end{tabular}
\end{table*}

\subsection{Robustness of MTKD}  
To verify the robustness of the MTKD framework, we conduct the following two sets of experiments:  

\paragraph{Impact of Different Numbers of Teacher Models on MTKD Performance}  
In the previous subsection, we have demonstrated that the MTKD framework based on three teacher models can significantly improve the performance of various CD models. 
Therefore, we further explore the performance of MTKD with two teacher models. 
We set a threshold of $th = 0.10$ to divide the dataset into small and large partitions and train the models $\mathcal{M}_{T_S}$ and $\mathcal{M}_{T_L}$ on these partitions, respectively. 
Subsequently, the student model $\mathcal{M}_S$ is trained using the MTKD framework. 
Except for the reduced number of teacher models, all other experimental settings remain unchanged.  
Using the original models as a baseline, Table~\ref{table:Metrics_with_two_teachers} compares the metrics of models trained with the O-P strategy and the MTKD framework under different numbers of teacher models. The results indicate that two teacher models can still achieve significant performance improvements over the baseline. 
Notably, for Changer-MiT-b0 and Changer-MiT-b1, the two-teacher model setup yields even greater improvements compared to the three-teacher setup. 
For CGNet, the O-P strategy with two teachers achieves higher mIoU and mFscore, although the MTKD performance is lower in this case. 
For TTP, the performance of O-P and MTKD under the three teacher model still remains the strongest.
These experiments validate the robustness of our methods in terms of the number of teachers. 
They also suggest that for some models, reducing the number of teachers can lead to comparable or even superior results while consuming fewer training resources.  

\paragraph{Performance of MTKD on the SYSU-CD Dataset}  
We further validated the effectiveness of the MTKD framework on the SYSU-CD dataset. 
Transformer-based Changer-MiT-b1, CNN-based CGNet, and FM-based TTP models are selected. 
As shown in Table~\ref{table:metrics_sysucd}, after MTKD optimization, all the models achieve higher mIoU and mFscore values, with the most significant improvement on CGNet (mIoU increased by 0.46\% and mFscore increased by 0.60\%). 
However, the O-P strategy does not yield better results compared to the original models.  
The visual comparison is presented in Fig.~\ref{fig:visual-sysucd}, illustrating the detection results of these algorithms on four image pairs. 
Comparing the 4th and 5th columns, it is evident that MTKD significantly reduces the false alarm rate for Changer-MiT-b1. 
For example, in the 1st row, large areas of false detection are eliminated, and in the 3rd row, a small false-positive patch is removed. 
However, the 4th row exhibits an increase in false alarms. For CGNet, MTKD's most notable contribution is the enhancement of internal compactness in detected regions.  
The original TTP model already produces satisfactory detection results, but MTKD further reduces false alarms. For instance, a large patch in the 1st row and a small patch in the 3rd row are effectively eliminated. 
However, the 1st row also shows an increase in missed detections.  
Overall, MTKD demonstrated substantial effectiveness on the SYSU-CD dataset, further validating its robustness.  

\begin{table}[!t]
\centering
\caption{Experimental Results on SYSU-CD Test Set \label{table:metrics_sysucd}}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{cccccc}
\hline
Method                                                                      & Strategy & mIoU           & mAcc           & mPrecision     & mFscore        \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Changer\\ (MiT-b1)\end{tabular}} & -        & 75.49          & \textbf{84.25} & 86.38          & 82.38          \\
                                                                            & O-P      & 75.32          & 83.88          & 85.86          & 82.19          \\
                                                                            & MTKD     & \textbf{75.56} & 83.97          & \textbf{87.33} & \textbf{82.40} \\ \hline
\multirow{3}{*}{CGNet}                                                      & -        & 71.41          & 80.32          & \textbf{84.72} & 78.82          \\
                                                                            & O-P      & 69.85          & 80.05          & 82.39          & 77.40          \\
                                                                            & MTKD     & \textbf{71.87} & \textbf{81.74} & 84.05          & \textbf{79.42} \\ \hline
\multirow{3}{*}{TTP}                                                        & -        & 76.09          & \textbf{84.59} & 87.08          & 82.72          \\
                                                                            & O-P      & 75.97          & 84.54          & 86.20          & 82.66          \\
                                                                            & MTKD     & \textbf{76.11} & 83.93          & \textbf{87.77} & \textbf{82.77} \\ \hline
\end{tabular}
\end{table}

\section{Conclusion} \label{F}
In this work, we introduce a new benchmark dataset, JL1-CD, which significantly complements existing CD datasets by offering sub-meter resolution, a wide range of change types, and a large dataset scale. 
We also propose the MTKD framework, which significantly enhances the performance of CD models on dual-temporal remote sensing images with different change areas, without increasing time and computational complexity during inference. 
Our approach effectively improves the generalization and robustness of the model across different cD scenarios. Future work will focus on developing a more universal knowledge distillation framework that not only further enhances model performance in diverse scenarios but also reduces model size and increases inference speed.
% \section*{Acknowledgments}

\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}

