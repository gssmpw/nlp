\section{Related Work}
\label{sec:related work}

This paper investigates online statistical learning and optimization in inventory control, specifically in a finite-horizon repeated newsvendor problem where the demand distribution parameters are initially unknown and must be learned over time. We focus on a perishable product with unobserved lost-sales, where sales data are censored by inventory levels, and any excess inventory does not carry over to the next period. The decision-maker must determine the order quantity before observing demand realization in that period. To address this challenge, we apply TS, a Bayesian approach that iteratively updates demand beliefs based on censored observations. This framework effectively balances exploration and exploitation, leading to improved inventory decisions over time and reducing long-term regret associated with demand uncertainty.
\subsubsection{Bayeisan Dynamic Programming Literature}
The first stream of research has formulated this problem using an offline dynamic programming (DP) approach, typically solved via backward induction. However, backward induction often suffers from the curse of dimensionality, making it computationally intractable for large-scale problems. %This is because the state space that describes the dynamic program grows exponentially, increasing the complexity of computing the optimal policy. 
Consequently, much of the existing literature in this area has focused on heuristic solutions as approximations to the optimal policy. \cite{chen2010bounds} propose heuristics based on the bounds of Bayesian DP-optimal decisions and value functions, which provide practical yet computationally feasible alternatives.

Another policy that Bayesian DP literature adopts is a myopic policy, where the decision-maker optimizes inventory decisions one period at a time, solving a single-period problem without considering how the chosen order quantity impacts future learning of demand parameters. This myopic approach has been widely studied in inventory management (see \cite{kamath2002bayesian}, \cite{dehoratius2008retail}, \cite{bisi2011censored}, \cite{besbes2022exploration}, \cite{chuang2023bayesian}). While myopic policies offer computational advantages, they often lead to suboptimal long-term inventory strategies, as they fail to fully account for the value of exploration in learning-based settings.


Specifically, we would like to compare our work with Theorem 3 in \cite{besbes2022exploration}. Our approach differs by benchmarking against the ground truth policy, whereas Bayesian DP-based approaches in prior work compare against Bayesian DP-optimal policies. In the frequentist setting, the ground truth policy corresponds to the true demand parameter \( \theta^* \). In contrast, in the Bayesian setting, the policy evolves dynamically, selecting the optimal decision distribution in each round rather than following the dynamic programming approach, which sums the policy over \(T\) rounds and minimizes it (as in traditional backward induction approaches). This distinction in benchmarking leads to a fundamentally different regret characterization. Unlike Bayesian DP policies, which rely on backward induction to compute the best policy in expectation, our method ensures that the regret bound scales as \( \sqrt{T} \). This result highlights how our approach inherently differs in how the policies are constructed, updated, and evaluated over time. Moreover, since our benchmark does not rely on the dynamic programming framework, it avoids the computational overhead associated with backward induction, making it more scalable and efficient.

Compared to offline Bayesian DP methods, our work employs TS to learn the unknown demand parameter, providing a simpler and more computationally efficient alternative. Instead of requiring a full-state space formulation and solving for an optimal policy via backward induction, our approach dynamically learns the demand distribution while simultaneously making inventory decisions. TS offers a practical solution for real-time decision-making, as it balances exploration and exploitation without requiring predefined state transitions or explicit value function approximations. %Unlike traditional methods, which rely on complex DP formulations, TS continuously refines posterior beliefs through observed censored demand feedback and updates ordering decisions accordingly. This makes TS particularly well-suited for large-scale inventory problems, where real-time adaptability is crucial for managing demand uncertainty. Because our framework does not depend on backward induction or dynamic programming techniques, it is significantly more scalable and applicable to real-world settings, where demand distributions evolve over time. By leveraging a Bayesian learning-based approach, our method ensures that the decision-maker optimally adjusts ordering policies, leading to low regret and computational efficiency.


%%%%new 

%%%%%%new
% 
\subsubsection{Non-Parametric and Other Related Newsvendor Literature}
Next, we discuss another line of research that focuses on nonparametric methods for solving joint demand estimation and inventory optimization problems. Unlike the Bayesian approach, which relies on a specific parametric demand distribution, this approach does not impose any predefined distributional assumptions on demand. Instead, it estimates demand directly from observed data. Researchers in this area develop models and algorithms that adjust inventory decisions based on demand observations without assuming a fixed functional form. For instance, \cite{huh2009nonparametric} proposes non-parametric adaptive policies that generate ordering decisions over time, allowing for flexibility in adapting to various demand patterns. Similarly, \cite{agrawal2019learning} proposes an updating confidence interval method that employs a phase-based UCB approach for learning and decision-making, which iteratively refines order quantities as more data becomes available. In our experiments detailed in Section \ref{sec:numerical}, we demonstrate that TS outperforms these algorithms %when provided with informative priors, highlighting the advantages of incorporating prior knowledge into the learning process.

Additionally, recent studies have explored the integration of feature-based learning into inventory systems with censored demand, introducing approaches that leverage contextual information to improve decision-making. For instance, \cite{ding2024feature} proposes the feature-based adaptive inventory algorithm and the dynamic shrinkage algorithm, which utilize observed demand patterns and additional features to dynamically adjust inventory policies. These algorithms aim to enhance the responsiveness of inventory systems to changing demand conditions by incorporating relevant external information. Meanwhile, \cite{tang2025offline} extends this idea to a pricing problem under censored demand, demonstrating how contextual features can inform pricing strategies in uncertain demand environments, thereby improving revenue management.

%While these approaches rely on feature-driven models to refine inventory or pricing decisions, our work does not require additional contextual information to implement our algorithm. Instead, we focus on learning from censored observations alone, making our approach more data-efficient and adaptable in settings where feature information may be unavailable or costly to obtain. By solely relying on the available sales data, our method remains broadly applicable across various industries and scenarios, ensuring robustness even when external contextual information is limited or unreliable.


\subsubsection{Thompson Sampling regret analysis}
In this section, we highlight how our TS regret analysis differs from previous approaches, such as those in \cite{russo2014learning} and \cite{russo2016information}. Specifically, we leverage the problem structure to reformulate regret analysis in terms of the convergence of the posterior parameter, providing a more structured and interpretable framework for regret analysis. This perspective allows for a clearer understanding of how the learning process influences decision-making over time and offers insights into the dynamics of regret reduction.

A key distinction between our work and \cite{russo2014learning} lies in how exploration and exploitation are handled. Unlike UCB-based methods, which construct deterministic confidence intervals to manage the exploration-exploitation trade-off, TS operates in a Bayesian framework, dynamically updating the posterior distribution based on observed data. This posterior-driven approach allows for more adaptive decision-making, where uncertainty is reduced naturally over time without the need for explicit confidence interval constructions. By sampling from the posterior distribution, TS inherently balances the need to explore suboptimal actions to gather information and the desire to exploit actions that currently appear optimal, leading to more efficient learning and improved performance in practice.

Additionally, our analysis differs from the information-theoretic regret framework of \cite{russo2016information}, which relies on the concept of the information ratio to bound regret. While this approach has been successfully applied to fully observed bandit problems, it is not directly applicable to our setting, where demand is censored. In censored demand environments, the information ratio is difficult to compute due to missing observations on lost-sales, making the standard information-theoretic regret bounds less effective. Instead, our analysis is tailored to the specific structural properties of the newsvendor problem with censored demand, ensuring that regret is properly quantified under partial observation constraints. By focusing on the convergence properties of the posterior distribution, we provide a regret analysis that is both practical and theoretically sound in the context of censored data.

Unlike existing methods that focus on confidence-based or information-theoretic approaches, we introduce a novel regret analysis that directly links regret minimization to the convergence of the posterior distribution. This formulation offers new insights into how uncertainty reduction in the posterior translates to improved decision-making, setting the foundation for future Bayesian regret analysis in inventory and learning-based optimization problems. By establishing a direct connection between the learning dynamics of the posterior distribution and the resulting regret, our analysis provides a deeper understanding of the mechanisms driving performance in Bayesian adaptive algorithms and opens avenues for further research in this area.

%%%%%
The rest of the paper is organized as follows: In Section \ref{sec:Preliminaries and Model Setup}, we present the preliminaries and the newsvendor setup, establishing the foundation for our study. Section \ref{sec: ts alg newsvendor} details the dynamics of the TS algorithm as applied to the newsvendor problem, explaining its operation and relevance to inventory decision-making under uncertainty. In Section \ref{sec:regret analysis}, we provide a regret analysis along with a sketch of the proof, quantifying the performance of our approach compared to the optimal benchmark. Section \ref{sec:numerical} showcases numerical experiments where we evaluate our algorithm against existing methods, highlighting its practical effectiveness. In Section \ref{sec:extension}, we discuss the broader applicability of our framework, outlining how TS with censored feedback can be implemented in other contexts. Finally, Section \ref{sec: conclusion} concludes our work, summarizing findings and suggesting potential future research directions. All proofs supporting our theoretical claims are provided in the Appendix.