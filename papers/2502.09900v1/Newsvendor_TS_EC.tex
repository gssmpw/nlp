\documentclass[format=acmsmall, review=false]{acmart}
\usepackage{acm-ec-25}
\usepackage{graphicx} % Required for inserting images
\usepackage{graphicx,graphics}
\usepackage{subcaption} 
\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\usepackage{xcolor}
\newtheorem{assumption}{Assumption}
%\usepackage{algorithm,algorithmic,color}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\newcommand{\E}{\mathbb{E}}
% Choose a citation style by commenting/uncommenting the appropriate line:
%\setcitestyle{acmnumeric}
\setcitestyle{authoryear}

% Title. Note the optional short title for running heads. In the interest of anonymization, please do not include any acknowledgements.
\title{Thompson Sampling for Repeated Newsvendor}

% Anonymized submission.
\author{Weizhou Zhang, Chen Li, Hanzhang Qin, Yunbei Xu, Ruihao Zhu}

% Abstract. Note that this must come before \maketitle.
\begin{abstract}
In this paper, we investigate the performance of Thompson Sampling (TS) for online learning with censored feedback, focusing primarily on the classic repeated newsvendor model--a foundational framework in inventory management--and demonstrating how our techniques can be naturally extended to a broader class of problems. We model demand using a Weibull distribution and initialize TS with a Gamma prior to dynamically adjust order quantities. Our analysis establishes optimal (up to logarithmic factors) frequentist regret bounds for TS without imposing restrictive prior assumptions. More importantly, it yields novel and highly interpretable insights on how TS addresses the exploration-exploitation trade-off in the repeated newsvendor setting. Specifically, our results show that when past order quantities are sufficiently large to overcome censoring, TS accurately estimates the unknown demand parameters, leading to near-optimal ordering decisions. Conversely, when past orders are relatively small, TS automatically increases future order quantities to gather additional demand information. Extensive numerical simulations further demonstrate that TS outperforms more conservative and widely-used approaches such as online convex optimization, upper confidence bounds, and myopic Bayesian dynamic programming. 
This study also lays the foundation for exploring general online learning problems with censored feedback.
\end{abstract}

\begin{document}

% Title page for title and abstract only.
\begin{titlepage}

\maketitle

% Optionally include a table of contents
\vspace{1cm}
\setcounter{tocdepth}{2} % adjust to 1 if desired
\tableofcontents

\end{titlepage}

% Paper body
\section{Introduction}
%{\color{orange} 1. TS has advantage over OCO (e.g. numerical) 2. More systematical and iterpretable examination of exploration-exploitation trade-off (what is confidence bound, how TS automatically balance large exploration and right exploitation) 3. Extension to more general setting of online learning with censored feedback}

The repeated newsvendor problem is a classic framework in the operations management literature \citep{huh2009nonparametric,besbes2022exploration}. In this problem, a decision-maker must repeatedly chooses how much quantity to stock in each period without knowing the true demand distribution. After each period, the decision-maker observes only censored feedback. That is, the decision-maker only sees how many units were sold (up to the stocking level) but do not learn whether additional demand went unmet once the inventory ran out. %This setup naturally creates a trade-off between exploration (ordering more to learn about the higher end of demand) and exploitation (ordering to minimize immediate costs or lost-sales). %Also, demand pattern change rapidly, the decision maker has to adapt their inventory decisions  to balance learning demand trends and optimizing inventory levels in real time. 
There is a trade-off inherent from this problem between exploration and exploitation: 
\begin{enumerate}
    \item Exploration: stocking more inventory than necessary to gather more information about tail distribution of demand. However doing so may cause the problem of overstocking and incur more holding cost at warehouse. 
    \item Exploitation: order the quantity based on the current estimation of demand so as to minimize the holding cost but doing so may incur lost-sales penalty and fail to gather valuable information of demand distribution, which can cause suboptimal inventory decision in the future. 
\end{enumerate}
More broadly, the repeated newsvendor problem serves as a key representative of a broader class of problems referred to as {\bf ``online learning with censored feedback.''} In this setting, the observation is always the minimum of an unknown random variable and the chosen action. For instance, in the newsvendor problem, the censored feedback corresponds to the minimum of the demand and the stock order. Similarly, in an auction, it is given by the minimum of the buyer's willingness to pay and the seller's set price. These problems inherently exhibit a trade-off between {\bf ``large exploration''}—choosing a sufficiently large action to better observe demand or willingness to pay for more accurate estimation—and {\bf ``optimal exploitation''}—making the right decision to minimize regret. While this paper primarily focuses on the repeated newsvendor problem, we also take an initial step toward systematically exploring the broader class of online learning problems with censored feedback.

Existing studies on the repeated newsvendor problem have established a 
$\sqrt{T}$-regret bound under fairly general unknown demand distributions and censored feedback, often leveraging the online convex optimization (OCO) framework \cite{huh2009nonparametric}. However, as widely recognized in the bandit and online learning literature \cite{chapelle2011empirical, seldin2014one, xu2023bayesian}, TS often outperforms OCO-based approaches (which were originally developed for adversarial online learning) as well as other methods such as the Upper Confidence Bound (UCB). These advantages are supported by extensive numerical experiments and theoretical analyses in the aforementioned studies, as well as in our own work. This motivates us to adopt TS as the preferred approach for the repeated newsvendor problem and beyond.

\subsection{Main Contributions}
In this study, we provide a systematic analysis by establishing a $\sqrt{T}$-frequentist regret bound for TS in the repeated newsvendor problem. Our contributions are four-fold.

First, this study presents {\it the first regret analysis of TS}, one of the best-performing and most widely adopted algorithms in the bandit literature, in the repeated newsvendor model with censored feedback. This sets our work apart from prior studies that primarily focus on more conservative approaches such as OCO and UCB. Additionally, our frequentist regret analysis provides guarantees that hold for arbitrary underlying Weibull demand parameters or priors, further distinguishing it from Bayesian dynamic programming approaches that rely on restrictive assumptions, such as a well-specified Gamma prior. See Section \ref{sec:related work} for details.



Second, our study provides a highly interpretable framework for analyzing the exploration-exploitation trade-off. Specifically, it offers insights into estimation error of unknow demand and enables a closed-form understanding of how TS naturally balances large-scale exploration with optimal exploitation in an intuitive and automatic manner. This novel perspective, to the best of our knowledge, provides the most interpretable explanation of the exploration-exploitation trade-off in the repeated newsvendor problem. A brief explanation is provided in the ``Main Messages'' subsection (Section \ref{subsec:main messages}), followed by a more detailed theoretical analysis in Section \ref{sec:regret analysis}.

Third, through extensive numerical experiments, we demonstrate that TS outperforms existing approaches, as shown in Section \ref{sec:numerical}. This finding aligns with the widely recognized effectiveness of TS in prior studies.

Fourth, TS and our analytical framework naturally extend to the broader settings of online learning with censored feedback, making it a versatile and effective approach for a wide range of decision-making problems under uncertainty. This extension requires only that the regret is Lipschitz continuous with respect to actions (Assumption \ref{assumption:regret}) and that the relationship between the optimal action and the underlying parameter is continuous and monotone (Assumption \ref{assumption:action}). Further details and discussion of these assumptions are provided in Section \ref{sec:extension}.









%Then, we provide some real-world examples:
%\begin{example}[Retail and E-commerce]
 %  An online retailer must decide how many items to stock each day without knowing the exact demand distribution.  The retailer only sees the number of units sold but does not learn how many additional customers were turned away after a stockout. This censored demand feedback creates uncertainty in estimating the true demand and complicates inventory decision-making.\end{example}

%\begin{example}[Humanitarian Food Distribution]
%A humanitarian organization distributing food aid must decide how much to allocate to different regions each day without knowing the exact demand. If all available food is distributed, the organization only observes how much was given out but does not learn how many additional people might have needed assistance after supplies ran out. This censored demand feedback makes it challenging to estimate the true food requirements and optimize future allocations.
%\end{example}

%\begin{example}[Fashion-wear Industry]
%\cite{fisher1997right} Consumer demand for fashionwear is inherently volatile, shaped by factors like brand image, retailing price, customer taste, and social impact. With frequent product launches for every season and limited-time seasonal sales (such as Black Friday), inventory planning becomes particularly intense. To optimize decision-making, businesses must make inventory decision in real time.
%\end{example}
%\begin{example}[Electronic]
 % For products like personal computers or smartphones, consumer demand is driven by brand perception and personal taste, making demand patterns unpredictable and historical data unreliable \cite{raz2013design}. This creates a trade-off between stockouts and excess inventory—ordering too little risks lost-sales, while ordering too much increases holding costs, especially when new models launch and older versions lose value.

%\end{example}

%\begin{example}[Cloud Computing Resource Allocation]
  %  In cloud computing, providers allocate CPU/GPU resources to handle variable client workloads. Future demand is uncertain: Under-allocation causes performance issues or failed tasks; over-allocation wastes capacity. Usage beyond allocated resources is censored, making it difficult to learn true demand. 

%In cloud computing, providers must dynamically allocate CPU/GPU resources to support fluctuating client workloads. The challenge arises since future demand is unstable, and resource allocation decisions must be made before demand is fully realized. A trade-off exists between under-allocation and over-allocation:

%Under-allocation results in performance degradation, task failures, or service shutdown, leading to dissatisfied customers and revenue loss.
%Over-allocation wastes valuable computing capacity, increasing additional operational costs.
%\end{example}


\subsection{Main Messages}\label{subsec:main messages}
In this paper, we investigate an online learning problem with censored feedback using the classic newsvendor model--one of the most fundamental frameworks in inventory management--as our pivotal example. Specifically, we consider a setting where the true demand $D_t$	
  is unknown, the action $y_t$
  is the order quantity, and the observation $Y_t$	
  is censored feedback given by
  \begin{align}\label{eq:censored demand}
      Y_t=\min\{D_t, y_t\}.
  \end{align}
Where demand is exactly observed when sales are less than the order quantity, that is, when $D_t<y_t$; and the demand is censored at the order quantity when sales equal $y_t$, that is, when $D_t \geq y_t$. The newsvendor setting experiences censored feedback--the decision-maker never observes lost-sales if demand exceeds the order quantity. This makes it difficult to accurately estimate demand, as it requires finding the right balance between not ordering too much to prevent excess inventory and placing larger orders to better understand how much demand is actually being missed (i.e., the afore-mentioned exploration-exploitation trade-off).%This structure gives rise to a natural exploration–exploitation trade-off:
%\begin{itemize}
 %     \item On the one hand, choosing large actions $y_t$ helps gather information about the tail of the demand distribution, improving parameter estimation—analogous to uniform exploration in bandit problems.  where sampling different action is helpful to estimating expected payoffs. 
  %\item On the other hand, choosing the right action to minimize immediate regret remains crucial, since overshooting the demand unnecessarily could be costly.
%\end{itemize}

To address this trade-off, we model the demand distribution by a Weibull distribution--a flexible and widely used parametric family--and propose using TS to dynamically select order quantities. Our key insights include:

\paragraph{\textbf{Estimation under Censored Feedback}}
Regardless of the algorithm used, we derive the confidence interval for demand estimation under censored feedback \eqref{eq:censored demand}. The estimation error at round $t$ scales inversely with $\sum_{i=1}^{t-1}(1 - e^{\theta^* y_i^k})$, where $\theta^*$ and $k$ are the scale and shape parameters of the Weibull distribution. This provides a rigorous quantification of how smaller past actions lead to larger errors and highlights the critical trade-off between large exploration and optimal exploitation.

\paragraph{\textbf{Automatic Compensation via TS}}  
From the closed-form expression of TS under Weibull demand, we derive a key insight:
\begin{itemize}
\item When past actions (order quantities) are sufficiently {\it large}, the observed data is more likely to be uncensored and can provide accurate information about the demand’s upper tail. This enables precise estimation of the Weibull parameters and near-optimal ordering decisions.

\item When past actions are relatively {\it small}, TS naturally pushes future actions higher, preventing the algorithm from being stuck with poor estimates. This ensures systematic exploration of larger actions to refine demand knowledge and improve future decisions.
\end{itemize}
In essence, {\it large actions enhance estimation accuracy, while small actions drive future TS-selected actions higher}. This adaptive mechanism allows TS to balance learning and cost minimization, avoiding suboptimal ordering.

\paragraph{\textbf{Balancing Exploration and Exploitation in a Frequentist Setting}}
Despite the Bayesian flavor of TS, we show a frequentist regret bound. In particular, with an  initialization of Gamma prior on the Weibull parameters, TS implicitly achieves the balance between exploration and exploitation, even when we have no prior knowledge of the actual demand parameters. This balance arises because TS automatically change its exploration strategy according to its level of uncertainty in its posterior estimates. As more data is observed, TS naturally puts more weight toward exploitation, which improves estimation accuracy while still allows for occasional exploration to occurs.

\paragraph{\textbf{Empirical Effectiveness}}
We conduct extensive numerical experiments demonstrating that TS yields competitive performance in terms of cumulative regret, outperforming existing widely-used approaches such as OCO, UCB, and myopic Bayesian dynamic programming. These experiments confirm the widely recognized belief on the effectiveness of TS in online learning and bandit literature and practical applications \cite{chapelle2011empirical, seldin2014one, xu2023bayesian}.


\paragraph{\textbf{Extensions to Online Learning with Censored Feedback}}
We illustrate how our analytical framework naturally extends to broader settings of online learning with censored feedback, making it applicable to a wide range of problems where the feedback is censored. As validated by Assumptions \ref{assumption:regret} and \ref{assumption:action} in Section \ref{sec:extension}, this extension requires only that the regret is Lipschitz continuous with respect to actions and that the relationship between the optimal action and the underlying parameter is continuous and monotone. We also discuss the technical limitation and possible refinement of these assumptions.


\subsection{Related Work}\label{sec:related work}

This paper investigates online statistical learning and optimization in inventory control, specifically in a finite-horizon repeated newsvendor problem where the demand distribution parameters are initially unknown and must be learned over time. We focus on a perishable product with unobserved lost-sales, where sales data are censored by inventory levels, and any excess inventory does not carry over to the next period. The decision-maker must determine the order quantity before observing demand realization in that period. To address this challenge, we apply TS, a Bayesian approach that iteratively updates demand beliefs based on censored observations. This framework effectively balances exploration and exploitation, leading to improved inventory decisions over time and reducing long-term regret associated with demand uncertainty.
\subsubsection{Bayeisan Dynamic Programming Literature}
The first stream of research has formulated this problem using an offline dynamic programming (DP) approach, typically solved via backward induction. However, backward induction often suffers from the curse of dimensionality, making it computationally intractable for large-scale problems. %This is because the state space that describes the dynamic program grows exponentially, increasing the complexity of computing the optimal policy. 
Consequently, much of the existing literature in this area has focused on heuristic solutions as approximations to the optimal policy. \cite{chen2010bounds} propose heuristics based on the bounds of Bayesian DP-optimal decisions and value functions, which provide practical yet computationally feasible alternatives.

Another policy that Bayesian DP literature adopts is a myopic policy, where the decision-maker optimizes inventory decisions one period at a time, solving a single-period problem without considering how the chosen order quantity impacts future learning of demand parameters. This myopic approach has been widely studied in inventory management (see \cite{kamath2002bayesian}, \cite{dehoratius2008retail}, \cite{bisi2011censored}, \cite{besbes2022exploration}, \cite{chuang2023bayesian}). While myopic policies offer computational advantages, they often lead to suboptimal long-term inventory strategies, as they fail to fully account for the value of exploration in learning-based settings.


Specifically, we would like to compare our work with Theorem 3 in \cite{besbes2022exploration}. Our approach differs by benchmarking against the ground truth policy, whereas Bayesian DP-based approaches in prior work compare against Bayesian DP-optimal policies. In the frequentist setting, the ground truth policy corresponds to the true demand parameter \( \theta^* \). In contrast, in the Bayesian setting, the policy evolves dynamically, selecting the optimal decision distribution in each round rather than following the dynamic programming approach, which sums the policy over \(T\) rounds and minimizes it (as in traditional backward induction approaches). This distinction in benchmarking leads to a fundamentally different regret characterization. Unlike Bayesian DP policies, which rely on backward induction to compute the best policy in expectation, our method ensures that the regret bound scales as \( \sqrt{T} \). This result highlights how our approach inherently differs in how the policies are constructed, updated, and evaluated over time. Moreover, since our benchmark does not rely on the dynamic programming framework, it avoids the computational overhead associated with backward induction, making it more scalable and efficient.

Compared to offline Bayesian DP methods, our work employs TS to learn the unknown demand parameter, providing a simpler and more computationally efficient alternative. Instead of requiring a full-state space formulation and solving for an optimal policy via backward induction, our approach dynamically learns the demand distribution while simultaneously making inventory decisions. TS offers a practical solution for real-time decision-making, as it balances exploration and exploitation without requiring predefined state transitions or explicit value function approximations. %Unlike traditional methods, which rely on complex DP formulations, TS continuously refines posterior beliefs through observed censored demand feedback and updates ordering decisions accordingly. This makes TS particularly well-suited for large-scale inventory problems, where real-time adaptability is crucial for managing demand uncertainty. Because our framework does not depend on backward induction or dynamic programming techniques, it is significantly more scalable and applicable to real-world settings, where demand distributions evolve over time. By leveraging a Bayesian learning-based approach, our method ensures that the decision-maker optimally adjusts ordering policies, leading to low regret and computational efficiency.


%%%%new 

%%%%%%new
% 
\subsubsection{Non-Parametric and Other Related Newsvendor Literature}
Next, we discuss another line of research that focuses on nonparametric methods for solving joint demand estimation and inventory optimization problems. Unlike the Bayesian approach, which relies on a specific parametric demand distribution, this approach does not impose any predefined distributional assumptions on demand. Instead, it estimates demand directly from observed data. Researchers in this area develop models and algorithms that adjust inventory decisions based on demand observations without assuming a fixed functional form. For instance, \cite{huh2009nonparametric} proposes non-parametric adaptive policies that generate ordering decisions over time, allowing for flexibility in adapting to various demand patterns. Similarly, \cite{agrawal2019learning} proposes an updating confidence interval method that employs a phase-based UCB approach for learning and decision-making, which iteratively refines order quantities as more data becomes available. In our experiments detailed in Section \ref{sec:numerical}, we demonstrate that TS outperforms these algorithms %when provided with informative priors, highlighting the advantages of incorporating prior knowledge into the learning process.

Additionally, recent studies have explored the integration of feature-based learning into inventory systems with censored demand, introducing approaches that leverage contextual information to improve decision-making. For instance, \cite{ding2024feature} proposes the feature-based adaptive inventory algorithm and the dynamic shrinkage algorithm, which utilize observed demand patterns and additional features to dynamically adjust inventory policies. These algorithms aim to enhance the responsiveness of inventory systems to changing demand conditions by incorporating relevant external information. Meanwhile, \cite{tang2025offline} extends this idea to a pricing problem under censored demand, demonstrating how contextual features can inform pricing strategies in uncertain demand environments, thereby improving revenue management.

%While these approaches rely on feature-driven models to refine inventory or pricing decisions, our work does not require additional contextual information to implement our algorithm. Instead, we focus on learning from censored observations alone, making our approach more data-efficient and adaptable in settings where feature information may be unavailable or costly to obtain. By solely relying on the available sales data, our method remains broadly applicable across various industries and scenarios, ensuring robustness even when external contextual information is limited or unreliable.


\subsubsection{Thompson Sampling regret analysis}
In this section, we highlight how our TS regret analysis differs from previous approaches, such as those in \cite{russo2014learning} and \cite{russo2016information}. Specifically, we leverage the problem structure to reformulate regret analysis in terms of the convergence of the posterior parameter, providing a more structured and interpretable framework for regret analysis. This perspective allows for a clearer understanding of how the learning process influences decision-making over time and offers insights into the dynamics of regret reduction.

A key distinction between our work and \cite{russo2014learning} lies in how exploration and exploitation are handled. Unlike UCB-based methods, which construct deterministic confidence intervals to manage the exploration-exploitation trade-off, TS operates in a Bayesian framework, dynamically updating the posterior distribution based on observed data. This posterior-driven approach allows for more adaptive decision-making, where uncertainty is reduced naturally over time without the need for explicit confidence interval constructions. By sampling from the posterior distribution, TS inherently balances the need to explore suboptimal actions to gather information and the desire to exploit actions that currently appear optimal, leading to more efficient learning and improved performance in practice.

Additionally, our analysis differs from the information-theoretic regret framework of \cite{russo2016information}, which relies on the concept of the information ratio to bound regret. While this approach has been successfully applied to fully observed bandit problems, it is not directly applicable to our setting, where demand is censored. In censored demand environments, the information ratio is difficult to compute due to missing observations on lost-sales, making the standard information-theoretic regret bounds less effective. Instead, our analysis is tailored to the specific structural properties of the newsvendor problem with censored demand, ensuring that regret is properly quantified under partial observation constraints. By focusing on the convergence properties of the posterior distribution, we provide a regret analysis that is both practical and theoretically sound in the context of censored data.

Unlike existing methods that focus on confidence-based or information-theoretic approaches, we introduce a novel regret analysis that directly links regret minimization to the convergence of the posterior distribution. This formulation offers new insights into how uncertainty reduction in the posterior translates to improved decision-making, setting the foundation for future Bayesian regret analysis in inventory and learning-based optimization problems. By establishing a direct connection between the learning dynamics of the posterior distribution and the resulting regret, our analysis provides a deeper understanding of the mechanisms driving performance in Bayesian adaptive algorithms and opens avenues for further research in this area.

%%%%%
The rest of the paper is organized as follows: In Section \ref{sec:Preliminaries and Model Setup}, we present the preliminaries and the newsvendor setup, establishing the foundation for our study. Section \ref{sec: ts alg newsvendor} details the dynamics of the TS algorithm as applied to the newsvendor problem, explaining its operation and relevance to inventory decision-making under uncertainty. In Section \ref{sec:regret analysis}, we provide a regret analysis along with a sketch of the proof, quantifying the performance of our approach compared to the optimal benchmark. Section \ref{sec:numerical} showcases numerical experiments where we evaluate our algorithm against existing methods, highlighting its practical effectiveness. In Section \ref{sec:extension}, we discuss the broader applicability of our framework, outlining how TS with censored feedback can be implemented in other contexts. Finally, Section \ref{sec: conclusion} concludes our work, summarizing findings and suggesting potential future research directions. All proofs supporting our theoretical claims are provided in the Appendix.













\section{Model Setup and the Thompson Sampling Algorithm}
\label{sec:Preliminaries and Model Setup}
In this section, we discuss the repeated newsvendor model setup and the associated TS algorithm in detail. 
\subsection{Repeated Newsvendor Model}
Following the setup by \cite{bisi2011censored} and \cite{chuang2023bayesian},
we consider a Repeated Newsvendor Model in which a retailer sells a single perishable product over a discrete and finite decision horizon. A Bayesian Repeated Newsvendor Model can be defined as a tuple $(T,f_{\theta_{\star}}(\cdot),\rho_0(\cdot),h,p)$, where $T \in \mathbb{R}^+$ is the known length of decision horizon, $f_{\theta_{\star}}(\cdot)$ is the known class of demand distributions, parameterized by an unknown parameter $\theta_{\star}$. We define the expression of $f_{\theta_{\star}}(\cdot)$ and $\rho_0(\cdot)$ in the next subsection. $h>0$ is the unit overage cost, and $p>0$ is the unit stock-out penalty. $h$ occurs if there is any leftover. $p$ occurs if there is any unmet demand. 
%which is sampled from a known prior distribution $\rho_0(\cdot)$.
%\textcolor{magenta}{LX: Could you better describe what is the exact definition of $h$ and $p$ in a inventory story-telling? If I miss some environment parameter, just describe it here...}

The dynamic is defined as follows. Before the decision-making process, the parameter $\theta_*$ is unknown.
At time $t \in [T]$, three events happens sequentially:
% Consider a retailer selling a single perishable product sold for consecutive $T$ periods. For each time step $t \in [T]$ of the inventory control problem, the following sequence of events happens:
\begin{enumerate}
    \item The retailer determines an order quantity $y_t \geq 0$.
    % At the beginning of each period $t \in [T]$, DM determines an order quantity $y_t \geq 0$.
    \item The demand $D_t$ is i.i.d generated from demand distribution $f_{\theta_*}(\cdot)$.
    % Demands $D_t$ s are i.i.d generated from a demand distribution $f_{\theta}(\cdot)$ parameterized by an \emph{unknown} demand parameter $\theta_{\star}$. We assume that $\theta$ is sampled from a known prior $\rho_0(\theta_{\star})$.
    % \item A unit overage cost $h$ and stock-out penalty $p$ are incurred if there is any leftover or unmet demand, respectively.
    \item Lost-sales are not observed, demand $D_t$ are censored on the right by the inventory levels $y_t$. The retailer only observes the data pairs $\left(Y_t, \delta_t\right)$, where $Y_t =D_t \wedge y_t$ and $\delta_t =1 \left[D_t <  y_t \right]$. interpreted as the number of exact observations of demand. Where demand is exactly observed when sales are less than the order quantity, that is, when $D_t<y_t$; and the demand is censored at the order quantity when sales equal $y_t$, that is, when $D_t \geq y_t$.
    
    The expected cost incurred at time step $t$ is   
    \begin{equation}
     \label{cost_function}
        g(y_t,D_t)=\E\left[h\left(y_t-D_t\right)^{+}+p\left(D_t-y_t\right)^{+}\right].
    \end{equation}
\end{enumerate}
The retailer knows the length of horizon $T$, the class of demand distributions $f_{\theta}(\cdot)$, the prior distribution $\rho_0$, $h$ and $p$, but does not know the exact value of $\theta_*$.

According to \cite{chuang2023bayesian}.
We denote $H = \left\{H_t \right\}$ the natural filtration generated by the right-censored sales data, i.e $H_t=\sigma  \left\{(Y_i,\delta_i) : i \leq t\right\} $, where $Y_t =D_t \wedge y_t$ and $\delta_t =1 \left[D_t <  y_t \right]$. DM chooses an action $y_t$. The DM aims to minimize the total expected cost in the $T$- period
online phase. We quantify the performance guarantee of the DM's non-anticipatory policy $\pi$ by its regret. We define regret as $\operatorname{Regret(T, \pi,\theta_{\star})}$ be the regret with respect to a fixed $\theta_{\star}$.
{\color{orange} }
\begin{definition}
\label{def:bayesian regret}
\begin{align}
\label{eq: freq regret}
 & \operatorname{Regret(T, \pi,\theta_{\star})}=\mathbb{E}\left[\sum_{t=1}^Tg\left(y_t, D_t\right) - \sum_{t=1}^T g\left(y_{\star}, D_{t}\right)\mid 
 \theta_{\star}\right].  \\
\label{eq: TS regret}
% &  %\operatorname{BayesianRegret(T,\pi)}=\mathbb{E}[\operatorname{Regret}(T,\pi,\theta_{*})]
\end{align}
\end{definition}
For simplicity, throughout the paper we abbreviate $\operatorname{Regret(T, \pi,\theta_{\star})}$  as $\operatorname{Regret(T,\theta_{\star})}$. 
%and $\operatorname{BayesianRegret(T,\pi)}$  as $\operatorname{Regret(T,\theta_{\star})}$ and $\operatorname{BayesianRegret(T)}$.


%\textcolor{magenta}{LX: Put the definition of Bayesian Regret (Currently in Section 3.2.1) and the definition of $y_*$ (Currently in Section 3.2.2, in English words and mathematical to describe $y_{*}(\theta_{\star})=F^{-1}_{\theta_{\star}}(\frac{p}{p+h})$) in the following:
\begin{equation*}
    y_* = \mathop{\arg\max}_y \  \E\left[h\left(y-D_t\right)^{+}+p\left(D_t-y\right)^{+}\right] = F^{-1}_{\theta_{\star}}\left(\frac{p}{p+h} \right).
\end{equation*}

We note by definition, the regret here is essentially a frequentist (non-Bayesian) regret. In this definition, $\theta_\star$ should be viewed as a fixed parameter.  Even though our work focuses on the development of TS, which is a Bayesian online learning algorithm, our regret analysis holds for the more general case in which the prior demand can be drawn from arbitrary probability distributions.


\subsection{Preliminaries}
In this section, we introduce the necessary tools to implement TS.

\vspace{2mm}
\noindent\textbf{Demand Distribution: Newsvendor Family.} The newsvendor (newsboy) family, introduced by \cite{braden1991informational}, is known to be the only family whose posterior distributions with censored demand information have conjugate priors. Formally put, a random variable is a member of the newsvendor distributions if its density is given by
$$
f_{\theta}(x )=\theta d^{\prime}(x) e^{-\theta d(x)}, \quad F_{\theta}(x)=1-e^{-\theta d(x)},
$$
where $d^{\prime}(x)>0, \ \forall x>0$, so $f_{\theta}(x )$ is positive on $(0, \infty)$
 $\lim _{x \rightarrow 0} d(x)=0$ and $\lim _{x \rightarrow \infty} d(x)=\infty$. So $F_\theta(x)$ is a valid probability distribution,
where $d(x)$ is a positive, differentiable, and increasing function and $\theta \in R_{+}$. 

\cite{lariviere1999stalking} show that when the demand distribution is Weibull with a gamma prior, the optimal solution for repeated newsvendor problem admits a closed form. Namely,
by letting $d(x)=x^k$ with a known constant $k>0$, we get the Weibull distribution. If $k=1$, we get the exponential distribution. In such cases, the underlying density function of demand is

$$
f_\theta(x)=\theta k x^{k-1} e^{-\theta x^k}.
$$

\vspace{2mm}
\noindent\textbf{Prior Distribution and Parametric Demand.} With the true value of $\theta_{\star}$ being unknown, the decision maker initiates TS with a prior distribution $\rho_0$ at the outset. Throughout the paper, we adopt the prior family and parametric demand introduced by \cite{braden1991informational}. Namely, the prior follows $\rho_0 \sim \operatorname{Gamma}(\alpha_0, \beta_0)$ ($\rho_0(\theta) = \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \theta^{\alpha_0 - 1} e^{-\theta \beta_0})$. When demand is described by a member of the newsvendor family, the gamma distribution remains a conjugate prior. Under the Weibull distribution of demand, we have
\begin{equation*}
    F^{-1}_{\theta_{\star}}(\frac{p}{p+h})=\frac{1}{\theta_{\star}}\left(-\ln (\frac{h}{p+h})\right)^{1 / k},
\end{equation*}
 where $F^{-1}_{\theta_{\star}}$ is the inverse cumulative distribution function. Here, we emphasize that this prior is only used to initiate TS and we do not impose any prior distribution on $\theta_*$.

\vspace{2mm}
\noindent\textbf{Likelihood Function.} The likelihood function can be formulated for a set of observed data pairs, including both censored and uncensored data. Let's start by considering the first censored data pair denoted as $\left(Y_0, \delta_0\right)$. We use  $\theta \mapsto \mathcal{L}\left(\theta \mid Y_0, \delta_0\right)$ to denote the likelihood function:
$$
\mathcal{L}\left(\theta \mid Y_0, \delta_0\right)= \begin{cases}f_\theta\left(Y_0\right), & \text { if } \delta_0=1 ; \\ 1-F_\theta\left(Y_0\right), & \text { if } \delta_0=0 .\end{cases}
$$  
 Consider we have $t \geq 2$ observations of data pairs denoted as $Y=(Y_0,Y_1,\cdots,Y_t),\delta=(\delta_0,\cdots,\delta_t)$ and we use $C$ denote the set of all observations of censored data pairs and $\Bar{C}$ denote the set of all observations of uncensored data pairs , where $|C|=m$, $|\Bar{C}|=n$, and $m+n=t$
 Then the likelihood function is
\begin{align*}
     \mathcal{L}\left(\theta \mid Y, \delta \right) &=\Pi_{i=1}^{n} \left(f_\theta\left(Y_i\right)\right)^{i}  \Pi_{j=1}^{m} \left((1-F_\theta\left(Y_j\right)\right)^{j} \\
     &=\left(\theta k\right)^n \left(\Pi_{i=1}^{n} Y_i\right)^{k-1} e^{-\theta\sum_{l=1}^{t} Y_l^{k}}.
\end{align*}

\vspace{2mm}
\noindent\textbf{Posterior Update.} The posterior demand distribution $\rho_t$ at the beginning of period $t$ can be derived as follows: 
\begin{align*}
    \rho_t(\theta) & \propto {\rho_0} \times \mathcal{L}\left(\theta \mid Y, \delta \right) \\
    & \propto \beta_0^{\alpha_0} \theta^{\alpha_0-1} e^{-\theta \beta_0} \times \left(\theta k\right)^n \left(\Pi_{i=1}^{n} Y_i\right)^{k-1} e^{-\theta\sum_{l=1}^{t} Y_l^{k}}\\
    & \propto  \theta^{\alpha_0+n-1} e^{-\theta \left(\beta_0+\sum_{i=1}^{t} Y_l^{k}\right)}\\  
      & \propto \operatorname{Gamma}(\alpha_0+\sum_{i=1}^{t} \delta_i,\beta_0+\sum_{i=1}^{t} Y_i^{k}).
\end{align*} 
Thus, the posterior at the beginning of period $t$ is given by $\rho_t = \operatorname{Gamma}\left(\alpha_t, \beta_t\right)$, where $\alpha_t=\alpha_0+\sum_{i=1}^{t} \delta_i$ and $\beta_t=\beta_0+\sum_{i=1}^{t} Y_i^k$.
\subsection{Algorithm: Thompson Sampling for Repeated Newsvendor Problem}
\label{sec: ts alg newsvendor}
TS is a Bayesian approach used to balance exploration and exploitation in sequential decision-making problems. In the context of the newsvendor problem, TS can be implemented to decide on the optimal order quantity under demand uncertainty. The specific TS procedure involves the following steps:

\begin{algorithm}[H]
\caption{TS for Repeated Newsvendor}
\label{alg:ts for newsvendor}
\KwIn{Prior distribution $\rho_0=$$\operatorname{Gamma}(\alpha_{0},\beta_{0})$, where $\alpha_0 \ge \max\left\{ \frac{\ln{\frac{T}{\delta}}}{\ln{\frac{e}{2}}}, 2 \right\},$ $\delta \in \left(0, \frac{1}{6}\right)$, Time Horizon $T.$ }
\For{$t=1$ to $T$}{
 Place order quantity
 \begin{equation*}
     y_{t}=\frac{1}{\theta_{t}}\left(-\ln \left(\frac{h}{p+h}\right)\right)^{1 / k},
 \end{equation*}
 where $\theta_t \sim \operatorname{Gamma}(\alpha_{t},\beta_{t})$\;
 Observe sales $Y_t=\min\{D_t,y_t\}$ and indicator of whether demand is censored $\delta_t = \mathbf{1}[D_{t} <y_{t}]$ \;
 Update the posterior $\rho_t \sim \operatorname{Gamma}\left(\alpha_{t}, \beta_{t}\right)$, where
 \begin{equation*}
     \alpha_{t}=\alpha_{0}+\sum_{i=0}^{t-1} \delta_{i}, \quad \beta_{t}=\beta_{0}+\sum_{i=0}^{t-1} Y_{i}^k.
 \end{equation*}

 % which is given by $\rho_t =\operatorname{Gamma}\left(\alpha_{t}, \beta_{t}\right)$, where $\alpha_{t}=\alpha_{0}+\sum_{i=0}^{t-1} \delta_{i}$ and $\beta_{t}=$ $\beta_{0}+\sum_{i=0}^{t-1} Y_{i}^k$. $Y_{t}=D_{t} \wedge y_{t} $ is the observed value and $\delta_{t}=\mathbf{1}[D_{t} <y_{t}]$, an indicator of whether demand is censored\;
}
\end{algorithm}
%\subsection{Bayesian Regret}
Initially, the environment draws a sample of $\theta_{\star}$ from prior $\rho_{0}=\operatorname{Gamma}(\alpha_{0},\beta_{0})$, which is unknown to DM.  and a known time horizon $T$. Then, for each $t \in [T]$, DM place the order quantity $y_t$ and then observes the sales $Y_t$, which is the minimum of demand and order quantity. Then the posterior is updated accordingly.  $y_t$ iteratively updates the posterior and samples from it. Specifically, 
$ y_t(\theta_{t})=F^{-1}_{\theta_t}(\frac{p}{p+h})
    =\frac{1}{\theta_{t}}\left(-\ln \left(\frac{h}{p+h}\right)\right)^{1 / k}$ and the property of $\theta_t$ (e.g. $\mathbb{E}[\theta_t]$ and $\mathbb{E}[1/\theta_t]$).
$\theta_t$ is sampled from Gamma distribution with $\alpha_t$, $\beta_t$. This is motivated from the posterior update. $\theta_t$ satisfies $\operatorname{Gamma}(\alpha_t,\beta_t)$ $\mathbb{E}[1 / \theta_t] = \frac{\beta_t}{\alpha_t-1}$. TS efficiently balances exploration (learning about the true demand distribution) and exploitation (placing optimal orders based on current knowledge). This approach is particularly useful in multi-period inventory problems, where demand is uncertain and needs to be learned over time.



\section{Regret Analysis}
\label{sec:regret analysis}
In this section, we provide the analysis for the regret upper bound on our Algorithm \ref{alg:ts for newsvendor}, which is equal to 
\begin{align*}
     \tilde{O}\left(\max\{h,p\} \cdot \left(-\ln (\frac{h}{p+h})\right)^{\frac{1}{k}}\cdot \frac{1}{\theta_{\star}^2} \cdot \sqrt{T}\right).
\end{align*} 
Section \ref{sec:Main Result} we provide the main theorem that state the upper bound. In Section \ref{sec:sketch proof} we provide a sketch proof for proving the Theorem. The proof consists of three key steps:
\begin{itemize}
    \item Lipchitz Continuity of Regret (Section \ref{subsubsec:Lipchitz regret})
    \item Confidence Analysis of Estimation (Section \ref{subsubsec:confidence})
    \item Lower bounding the Actions
 (Section \ref{subsubsec:lower bound action})
\end{itemize}

We also discuss how the these steps can be generalized to broader models of online learning with censored feedback in Section \ref{sec:extension}.

%{\color{orange} What term shall I include in the $\tilde{O}$?}

\subsection{Main Result: Regret}
\label{sec:Main Result}
\begin{theorem}
$T$-period regret of a given $\theta_{\star}$ for repeated newsvendor problem is 
  \label{thm:bay-regret-ts}
\begin{align*}
    \operatorname{Regret(T,\theta_{\star})}\leq \tilde{O}\left(\max\{h,p\} \cdot \left(-\ln (\frac{h}{p+h})\right)^{1 / k} \cdot\frac{1}{\theta_{\star}^2}\cdot \sqrt{T}\right).
\end{align*}  
\end{theorem}

% Theorem \ref{thm:regret for ts} is proved in Appendix \ref{Appendix: proof-thm-ts-regret}. And we provide the sketch proof in the following sections. 

\subsection{Proof for Theorem \ref{thm:bay-regret-ts}}
\label{sec:sketch proof}
The entire proof consists of several main steps. Firstly, we focus on $\operatorname{Regret(T, \theta_{\star})}$ for a fixed $\theta_*$. We decompose it by Lipchitz Continuity.

\subsubsection{\textbf{Key Step 1: Regret Decomposition: Lipchitz Continuity}}\label{subsubsec:Lipchitz regret}

We decompose the $\operatorname{Regret(T,\theta_{\star})}$ as follows: By the Lipchitz continuity of $\min$, 
\begin{subequations}
    \begin{align}
\operatorname{Regret(T,\theta_{\star})} & =\mathbb{E}\left[\left(\sum_{t=1}^Tg\left(y_t, D_t\right)-\sum_{t=1}^T pD_t\right) - \left(\sum_{t=1}^T g\left(y_{\star}, D_{t}\right)-\sum_{t=1}^T pD_t\right)\right] \nonumber\\ 
&= \E\left[ \sum_{t=1}^T\left[hy_t-(h+p)\min\{y_t,D_t\}\right]-\sum_{t=1}^T\left[hy_{\star}-(h+p)\min\{y_{\star},D_t\}\right]\right] \nonumber\\
&=\E\left[ \sum_{t=1}^T\left[h\left(y_t-y_{\star}\right)\right]-\sum_{t=1}^T(h+p) \left(\min\{y_t,D_t\}-\min\{y_{\star},D_t\}\right)\right] \nonumber\\
&\leq  \max\{h,p\} \cdot  \E \left[\sum_{t=1}^T\left|\E\left[y_t\right]-y_{\star}\right|\right] \label{pf-regret-a-sketch} \\
& = \max\{h,p\} \cdot \sum_{t=1}^T \E \left[\left|\E\left[y_t\right]-y_{\star}\right|\right] . \nonumber
    \end{align}
    \label{pf:regret-decompose}
\end{subequations}
Inequality (\ref{pf-regret-a-sketch}) comes from the following case discussion on $\min\{y_t,D_t\}-\min \left\{y_*, D_t\right\}$: 
% of the following cases further decompose the difference of $\min\{y_t,D_t\}-\min \left\{y_*, D_t\right\}$:

\textbf{Case 1: }$D_t>y_t$: In this case, $\min \left\{y_t, D_t\right\}-\min \left\{y_*, D_t\right\} = y_t - \min \left\{y_*, D_t\right\} \ge y_t - y_*$. Then we have
\begin{equation*}
    \begin{aligned}
    \mathbb{E} [ h(y_t -y_*) - (h+p)(\min \left\{y_t, D_t\right\}-\min \left\{y_*, D_t\right\})] & \le -p \mathbb{E}[y_t - y^* ] \\
    & = -p \mathbb{E}[\mathbb{E}[y_t] - y^* ] \\
    & \le p \mathbb{E}[\left|\mathbb{E}[y_t] - y^*\right|].
    \end{aligned}
\end{equation*}

\textbf{Case 2: }$D_t\le y_t$: In this case $\min \left\{y_t, D_t\right\}-\min \left\{y_*, D_t\right\} = D_t - \min \left\{y_*, D_t\right\} \ge 0$. Similarly,
\begin{equation*}
    \begin{aligned}
    \mathbb{E} [ h(y_t -y_*) - (h+p)(\min \left\{y_t, D_t\right\}-\min \left\{y_*, D_t\right\})] &\le h \mathbb{E}[y_t - y^* ] \\
    &= h \mathbb{E}[\mathbb{E}[y_t] - y^* ] \\
    & \le h \mathbb{E}[\left|\mathbb{E}[y_t] - y^*\right|].
    \end{aligned}
\end{equation*}

Altogether, we show that regret analysis can be transformed into the convergence analysis of the posterior parameter.

\subsubsection{\textbf{Key Step 2: Confidence Analysis of}
$\left|\E\left[y_t\right]-y_{\star}\right|$}
\label{subsubsec:confidence}
Before we proceed, we give the definition for $y_t$ and $y_{\star}$ as follows:
\begin{lemma}
\label{lem:y_t-and-y_star}
The order quantity $y_t$ at $t$ and the optimal myopic order quantity $y_t$ satisfies
% Denote the order quantity placed for each period $t \in [T]$ as $y_t$  at period $t \in [T]$ which is obtained by taking the quantile function of the demand distribution, which yields,
\begin{align}
\label{eq: ystar}
    y_{*}(\theta_{\star})&=F^{-1}_{\theta_{\star}}(\frac{p}{p+h})=\frac{1}{\theta_{\star}}\left(-\ln (\frac{h}{p+h})\right)^{1 / k}\\
\label{eq: yt}
   y_t(\theta_{t})&=F^{-1}_{\theta_t}(\frac{p}{p+h})
   =\frac{1}{\theta_{t}}\left(-\ln (\frac{h}{p+h})\right)^{1 / k}
\end{align}
% Where $y_{*}$ is the optimal myopic order quantity had we known the true value of demand parameter $\theta_{\star}$, 
where $F^{-1}$ is the inverse cumulative distribution function of the demand distribution. Moreover, 
  $\E\left[\frac{1}{\theta_t}\right]=\frac{\beta_t}{\alpha_t-1}$.
 \end{lemma}


By examining the expressions for $y_t$ and $y_{\star}$ in equations (\ref{eq: yt}) and (\ref{eq: ystar}), we can directly derive that:
\begin{align}\label{eq:lipchitz action 2}
    \begin{aligned}
        \left|\E \left[y_t\right]-y_{\star}\right| & = \left(-\ln (\frac{h}{p+h})\right)^{1 / k} \left|\frac{\beta_t}{\alpha_t-1}- \frac{1}{\theta_{\star}}\right|
    \end{aligned}
\end{align}
To proceed further, we establish a range for the demand $D_t$ at each time $t$. The following lemma provides this range with high probability:
\begin{lemma}
\label{lem:demand-range}
    For each $t \in [T]$, with probability $\ge 1-\delta/T$, the realization of demand $D_t \sim \operatorname{Weibull}(\theta_{\star}
    )$ will be in the range $[\underline{D},\overline{D}]$ such that, 
    \begin{align*}
     \underline{D}=\left(\frac{\ln{\left(\frac{2T}{2T-\delta}\right)}}{\theta_{\star}}\right)^{\frac{1}{k}}, \qquad \overline{D}=\left(\frac{\ln{\left(\frac{2T}{\delta}\right)}}{\theta_{\star}}\right)^{\frac{1}{k}}.
 \end{align*}
\end{lemma}
Lemma \ref{lem:demand-range} is proved in Appendix \ref{appendix: lemma demand range}. 
This lemma ensures that, with high probability, the demand realizations are confined within the specified range, which is crucial for later analysis.

Next, we provide confidence bound for how close the $\frac{1}{\theta_t}$ and its mean $\frac{\beta_t}{\alpha_t-1}$ is. Ideally, as $t$ increases, $\theta_t$ will converge to $\theta_*$ and $\frac{1}{\theta_t}$ will converge to $\frac{\beta_t}{\alpha_t-1}$. The following lemma shows the rate of convergence as follows: 
\begin{lemma}
\label{beta/alpha-1}
For any $t \in [T]$ and for any realization of  $\theta_{\star}$, 
\begin{align*}
    \mathbb{P}\left( \left|\frac{\beta_t}{\alpha_t-1}- \frac{1}{\theta_{\star}}\right|  \geq \sqrt{\ln{\left(\frac{2t^2}{\delta}\right)}}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}} \right) \leq \frac{\delta}{t^2}.
 \end{align*}    
\end{lemma}

Lemma \ref{beta/alpha-1} is proved in Appendix \ref{sec:appendix-lemma beta/alpha-1}. This lemma provides a probabilistic bound on the estimation error of $\frac{1}{\theta_t}$, which is key in assessing the accuracy of the order quantity decisions over time.
%alpha_t 
%mark this  from this estimation defintion y_t lower bound so that estimation can have a upper bound 

Combining these results, we can bound $\left|\mathbb{E}\left[y_t\right] - y_{\star}\right|$ as follows:
\begin{equation*}
    \begin{aligned}
     \left|\E \left[y_t\right]-y_{\star}\right| 
     & \leq \left(-\ln (\frac{h}{p+h})\right)^{1 / k}\sqrt{\ln{\left(\frac{2t^2}{\delta}\right)}}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}}\\
& \leq \left(-\ln (\frac{h}{p+h})\right)^{1 / k}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{2\ln{\left(\frac{T}{\delta}\right)}}\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}}.
\end{aligned}
\label{eq:sket-pf-bound-yt-decompose}
\end{equation*}
\begin{lemma}[\cite{chuang2023bayesian}]
\label{lemma:alpha_t,beta_t}
    The stochastic processes $\left\{\alpha_t\right\}$ and $\left\{\beta_t\right\}$ can be represented by
\begin{align*}
\alpha_t & =\alpha_0+\sum_{i=0}^{t-1} \delta_i \\
& =\alpha_0+\sum_{i=0}^{t-1} \mathbb{E}_{\theta_{\star}}^\pi\left[\delta_i \mid H_{i-1}\right]+\sum_{i=0}^{n-1}\left(\delta_i-\mathbb{E}_{\theta_\star}^\pi\left[\delta_i \mid H_{i-1}\right]\right)\\
&=\alpha_0+\sum_{i=0}^{t-1}\left(1-e^{-\theta_{\star} y_i^k}\right)+M_t
\end{align*}
Where,
\begin{align*}
      M_t =\sum_{i=0}^{t-1}\left(\delta_i-\mathbb{E}\left[\delta_i \mid \mathcal{H}_{i-1}\right]\right)-1.
\end{align*}

$\mathbb{E}_{\theta_\star}^\pi$ denotes the expectation operator under admissible Bayesian policy $\pi \in \Pi$ given that the true unknown parameter is $\theta_\star \in \mathbb{R}_{+}$.

\end{lemma}
From the above lemma \ref{lemma:alpha_t,beta_t}, we can see that as long as $y_t$ has a lower bound, we are able to derive the upper bound for regret. 
\subsubsection{\textbf{Key Step 3:  Uniform Lower Bound of} $y_t$}\label{subsubsec:lower bound action}

In order to establish the regret bound, it is essential to establish a uniform lower bound for $y_t$, as this will play a crucial role in our subsequent derivations. 

According to Lemma \ref{lower bound of y_t}, we have:

\begin{lemma}
\label{lower bound of y_t}

$$\mathbb{P}\left(\frac{1}{\theta_t} > \frac{\beta_t}{2\alpha_t}\right) \ge 1-\left(\frac{2}{e}\right)^{\alpha_t},\qquad \forall t \in [T].$$
\end{lemma}

The proof of Lemma \ref{lower bound of y_t} is provided in Appendix \ref{appendix: lower bound of y_t}.

Building upon this lemma, we proceed by conditioning on the event that $\frac{1}{\theta_t} > \frac{\beta_t}{2\alpha_t}$ and that the demand $D_t$ satisfies $D_t \ge \underline{D}$. Under these conditions, we can derive a lower bound for $y_t$ as follows:
%Then, conditioning on the event that $\frac{1}{\theta_t} > \frac{\beta_t}{2\alpha_t}$ and $D_t \ge \underline{D}$, we have
\begin{subequations}
    \begin{align}
    y_t &= \frac{1}{\theta_{t}}\left(-\ln (\frac{h}{p+h})\right)^{1 / k} \label{lb-yt-a}\\
    & \ge \frac{\beta_t}{2\alpha_t} \cdot \left(-\ln (\frac{h}{p+h})\right)^{1 / k} \label{lb-yt-b}\\
    & = \frac{1}{2}\left(-\ln (\frac{h}{p+h})\right)^{1 / k} \cdot \frac{\beta_0+\sum_{i=1}^{t} \min\{y_i,D_i\}^k}{\alpha_0+\sum_{i=1}^{t} \delta_i}\label{lb-yt-c} \\
    & \ge \frac{1}{2}\left(-\ln (\frac{h}{p+h})\right)^{1 / k} \cdot \min \left \{\frac{\beta_0}{\alpha_0},\underline{D}^k \right\}  = L.\label{lb-yt-d}
    \end{align}
\end{subequations}
Here, equation (\ref{lb-yt-a}) follows directly from the definition of $y_t$ as given in equation (\ref{eq: yt}). Inequality (\ref{lb-yt-b}) utilizes the result from Lemma \ref{lower bound of y_t}, indicating that with high probability, $\frac{1}{\theta_t}$ is bounded below by $\frac{\beta_t}{2\alpha_t}$. The equality in (\ref{lb-yt-c}) comes from the update rules for $\alpha_t$ and $\beta_t$ as defined in Algorithm \ref{alg:ts for newsvendor}. Finally, inequality (\ref{lb-yt-d}) is justified by applying Lemma \ref{lemma:sequence a and b}, which is an auxiliary result crucial to our analysis. 


\begin{lemma}
\label{lemma:sequence a and b}
    for two sequence $\{a_i\}_{i=1}^n$, $\{b_i\}_{i=1}^n$ satisfies $a_i \ge 0$ and $b_i \ge 0$ for any $i \in [n]$, and for at least one $i \in [n]$, $b_i > 0$. Then we have
    \begin{equation*}
        \frac{\sum_{i=1}^n a_i}{ \sum_{i=1}^n b_i} \ge \min_{i \in [n]: b_i > 0} \left \{ \frac{a_i}{b_i}\right \}.
    \end{equation*}
\end{lemma}


%%% talk about lemma 3.7 when delta_0=1 observed the d 
%when delta_1=0, one of the most important ascept of TS algorithm !!!
% fill in insight 
%delete related work at least one page (one and half) 

From the closed-form expression \eqref{lb-yt-c} and Lemma \ref{lemma:sequence a and b}, we reveal the most important ascept of TS algorithm as follows: 
\begin{enumerate}
    \item when $\delta_i=1$ (i.e. $D_t < y_t$), we obtain the full observation demand. the increment is $\frac{D_t}{1}$ As a result, the observed data is uncensored and can provide accurate information about the demand’s upper tail. 
    \item when $\delta_i=0$ (i.e. $D_t \ge y_t$). we get the censored demand, which indicates the past action is relatively small. Interestingly, since $\delta_i$ appears in the denominator in the closed-form expression \eqref{lb-yt-c}, TS naturally pushes future actions higher in subsequent periods, preventing the algorithm from getting stuck with poor estimates.
\end{enumerate}
This key observation illustrates how TS automatically balances the exploration-exploitation trade-off in the repeated newsvendor problem.
 
Applying Lemma \ref{lemma:sequence a and b} ( proved in Appendix \ref{appendix: proof sequence a and b} ) in our context, and considering that $D_t \ge \underline{D}$, we conclude that $y_t$ is uniformly bounded below by $L$ for all $t$. 


This uniform lower bound on $y_t$ is a critical to establish the regret bound. we plug back the lower bound to the definition of $\alpha_t $ in Lemma \ref{lemma:alpha_t,beta_t} to analyze the term $\left|\alpha_t-1\right|$, which analysis is referred in Appendix \ref{appendix: proof for lemma: alpha_t-1}.

To establish the regret, we use the technique of truncating $T$ and define a constant  $C_0$  to encapsulate the terms independent of $t$ as follows:

Denote 
$$T_0 = 64\left(1-\exp\{-\theta_{\star} L^k\}\right)^{-2}\ln{\left(\frac{T}{\delta}\right)},$$ 
 define,
\begin{equation*}
    C_0 = \max\{h,p\}\left(-\ln (\frac{h}{p+h})\right)^{1 / k}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{2\ln{\left(\frac{T}{\delta}\right)}}.
\end{equation*}
This allows us to express the regret bound more succinctly. Combining equation (\ref{pf:regret-decompose}), section (\ref{eq:sket-pf-bound-yt-decompose}) and the discussions above, we can now bound the cumulative regret by truncating $T$ as follows: 


\begin{equation*}
    \begin{aligned}
        \operatorname{Regret(T,\theta_{\star})} & \le C_0 \sum_{t=1}^T \frac{\sqrt{t}}{\alpha_t - 1} \\
        &\le \begin{cases}
       C_0 \cdot \left(T_0^{\frac{3}{2}} \cdot \frac{1}{\alpha_0 - 1} \right)  & t \le T_0,\\
       C_0 \cdot \left(4\left(1-\exp\{-\theta_{\star} L^k\}\right)^{-1}\sqrt{T}\right)   & t > T_0.
    \end{cases}
    \end{aligned}
\end{equation*}



\subsubsection{\textbf{Putting All together}} \label{subsec: everything}
In this section, we synthesize our previous findings to derive a comprehensive regret bound for the TS algorithm for the newsvendor problem.


\begin{equation*}
    \begin{aligned}
   &  \operatorname{Regret(T,\theta_{\star})}\\
     & \le \max\{h,p\} \cdot \sum_{t=1}^T \E \left[\left|\E\left[y_t\right]-y_{\star}\right|\right] \qquad \text{(Lipchitz Continuity)} \\
     & \le \max\{h,p\} \cdot \left(-\ln (\frac{h}{p+h})\right)^{1 / k} \sum_{t=1}^T \left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{2\ln{\left(\frac{T}{\delta}\right)}}\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}} \qquad \text{(Analysis of
$\left|\E\left[y_t\right]-y_{\star}\right|$)} \\
& \le C_0 \cdot \left(512 \left(1-\exp\{-\theta_{\star} L^k\}\right)^{-3} \cdot \ln\left( \frac{T}{\delta}\right)^{\frac{3}{2}} + 4\left(1-\exp\{-\theta_{\star} L^k\}\right)^{-1}\sqrt{T}\right) \qquad \text{($y_t$ lower bound)}.
      \end{aligned}
\end{equation*}
As shown in the display, the three inequalities precisely correspond to the three key steps outlined in Section \ref{subsubsec:Lipchitz regret}, Section \ref{subsubsec:confidence}, and Section \ref{subsubsec:lower bound action}.
%{\color{red} more description in the display}
%key step 1 (y_t-ystar), step 2 (\alpha_t/beta_t-1, step 3 final step 
%Finally, in order to get Bayesian regret, we integrate  $\operatorname{Regret(T,\theta_{\star})}$ over the prior of $\theta_{\star} \sim \text{Gamma}(\alpha_0, \beta_0)$, 
%\begin{subequations}
 %   \begin{align*}
 %  & \operatorname{BayesianRegret(T)}  \nonumber\\
%    &\leq \max\{h,p\}  \left(-\ln (\frac{h}{p+h})\right)^{1 / k}\sqrt{16\ln{\left(\frac{T}{\delta}\right)}} \sqrt{T}\int_{\theta_{\star}=0}^{\infty}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \theta^{\alpha_0 - 2} e^{-\theta \beta_0}d\theta_{\star} \nonumber\\
%    &\leq \max\{h,p\}  \left(-\ln (\frac{h}{p+h})\right)^{1 / k}\sqrt{16\ln{\left(\frac{T}{\delta}\right)}} \frac{\sqrt{T}\beta_0\Gamma(\alpha_0-2)}{\Gamma{(\alpha_0)}}. \nonumber\\
%\end{align*}
%\end{subequations}


\subsection{Insights}
 In our analysis of the newsvendor problem with censored demand data, we employ Thompson Sampling (TS) to balance exploration and exploitation effectively. By modeling demand using a Weibull distribution with parameters estimated from prior data, TS updates these estimates as new sales data becomes available. In (\ref{lb-yt-c}), we come up with the uniform lower bound of action $y_t$, showing that large actions can enhance estimation accuracy, while small actions drive future TS-selected actions higher. In section \ref{subsec: everything}, we conclude that our proof into three key steps. and we replace them by Assumptions \ref{assumption:regret} and \ref{assumption:action} in Section \ref{sec:extension}, which enlightens us on how to extend the existing model to broader broader class of online learning. 
\section{Numerical Experiments}
\label{sec:numerical}
% \subsection{Benchmark}
We conduct numerical experiments to evaluate the performance of TS in the repeated newsvendor problem and compare it against three benchmark policies. The first benchmark is the phased-UCB algorithm \citep{agrawal2019learning}, which updates the confidence interval of the base-stock level at the beginning of each epoch, further subdividing each epoch into consecutive time steps. We denote this policy as \texttt{UCB}. The second benchmark is the non-parametric adaptive policy proposed by \cite{huh2009nonparametric}, which generates ordering decisions dynamically over time. We denote this policy as \texttt{OCO}. Finally, we compare \texttt{TS} with the myopic policy from \cite{besbes2022exploration}.  which is a deterministic policy where the decision-maker optimizes inventory decisions one period at a time, solving a single-period problem without considering how the chosen order quantity impacts future learning of demand parameters.
% \subsection{Experiments}


To systematically analyze the impact of different service levels, we define the service level as $\gamma = \frac{p}{p+h}$,
where we fix \( p = 1 \) and vary \( h \) to achieve service levels of \( 50\% \), \( 90\% \), and \( 98\% \). For each experiment, we simulate the TS algorithm and the three benchmark policies on a common problem instance. Each algorithm is run for \( 100 \) independent trials to mitigate randomness, and we report the average cumulative regret. We set the prior parameters of the Weibull distribution to \( \alpha_0 = \beta_0 = 4 \) and consider a time horizon of \( T = 600 \).

We present our results in two sets of plots:
\begin{enumerate}
    \item Comparison of TS, UCB, and OCO. We plot the average cumulative regret of \texttt{TS}, \texttt{UCB}, and \texttt{OCO} to assess their relative learning performance in Figure \ref{fig:1}.
    \item Comparison of TS and Myopic. We compare the average cumulative regret of \texttt{TS} and the myopic policy against the optimal cost in Figure \ref{fig:2}.
\end{enumerate}


Our results demonstrate that \texttt{TS} consistently outperforms \texttt{UCB} and \texttt{OCO} across all service levels. Additionally, when comparing \texttt{TS} to the myopic policy, we observe that \texttt{TS} converge faster than \texttt{Myopic}, further reinforcing its effectiveness in balancing exploration and exploitation in the newsvendor setting.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{online.pdf}
    \caption{Compare TS with OCO and UCB}
    \label{fig:1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{myopic.pdf}
    \caption{Compare TS with Myopic Policy}
    \label{fig:2}
\end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Paper Template/figure/regret0.5.pdf}
%         \caption{}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Paper Template/figure/dp_ts_0.5.pdf}
%         \caption{}
%     \end{subfigure}
%     \caption{}
% \end{figure}

% \begin{figure}[htb]
%     \centering
%     \begin{subfigure}[b]{0.9\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plot}
%         \caption{}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.9\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{plotwith3}
%         \caption{}
%     \end{subfigure}
%     \caption{}
% \end{figure}
\section{Extensions to Online Learning with Censored Feedback}
\label{sec:extension}

In this section, we extend the regret analysis of TS for the repeated newsvendor problem to a broader class of online learning algorithms. We consider a setting where the demand $D_t$ in period $t$ is drawn from Weibull distribution parameterized by $\theta_{\star}$. The decision-maker selects an action $A_t$ in each period, resulting in an observed feedback of $\min\{D_t, A_t\}$. The loss incurred in each period is defined as $l(A_t) = \min\{D_t, A_t\}$.
%A_t >0 
The cumulative regret over $T$ periods is defined as:

\[
\operatorname{Regret}(T, \theta_{\star}) = \sum_{t=1}^T \left( l(A_t) - l(A_{\star}) \right),
\]

where $A_{\star}$ denotes the optimal action that minimizes the expected loss, given by:

\[
A_{\star} = \arg\min_{A} \mathbb{E}_{D \sim \theta_{\star}}[l(A)].
\]

\subsection{Key Assumptions and Results}
We make the following assumptions to facilitate the analysis:

\begin{assumption}[Lipschitz Continuity of Regret]\label{assumption:regret}
    The regret function is Lipschitz continuous with respect to the action, allowing it to be decomposed as:

\[
\operatorname{Regret}(T, \theta_{\star}) \leq C_1 \sum_{t=1}^T \mathbb{E} \left[ \left| \mathbb{E}[A_t] - A_{\star} \right| \right],
\]

where $C_1$ is a positive constant.
\end{assumption}
 Clearly, Assumption \ref{assumption:regret} is precisely the conclusion of key step 1 (see Section \ref{subsubsec:Lipchitz regret}) in our earlier analysis of the repeated newsvendor problem. Consequently, once this assumption is satisfied, no further model requirements are needed to validate the conclusions drawn in key step 1.
\begin{assumption}[Lipschitz Continuity and Monotonicity of $A_t$]\label{assumption:action}
    $A_t$ is non-decreasing with respect to $\frac{1}{\theta_t}$, and the deviation of the expected action from the optimal action is proportional to the estimation error of the parameter $\theta_{\star}$, such that:
\begin{align}\label{eq:lipchitz action}
    \left| \mathbb{E}[A_t] - A_{\star} \right| \le C_2 \left| \mathbb{E}[\frac{1}{\theta_t}] - \frac{1}{\theta_{\star}} \right|,
\end{align}
where $\theta_t$ is the parameter estimate at time $t$, and $C_2$ is a positive constant.
\end{assumption}
Assumption \ref{assumption:action} is satisfied in the repeated newsvendor model. Specifically, Lemma \ref{lem:y_t-and-y_star} shows that the optimal action in the newsvendor problem is given by $y_t(\theta_t)=\frac{1}{\theta_t}\left(-\ln(\frac{h}{p+h}\right)^{1/k}$. Let us show how the the conclusions in  key step 2 and key step 3 hold under this assumption.

For the key step 2, the Lipschitz continuity assumption in \eqref{eq:lipchitz action} directly leads to \eqref{eq:lipchitz action 2}. Additionally, Lemma \ref{beta/alpha-1} provides a generic estimation result for censored feedback under the Weibull distribution that is independent of the loss function or algorithm in use. Consequently, the conclusion of key step 2 (Section \ref{subsubsec:confidence}) holds under Assumption \ref{assumption:action}.

For key step 3, we observe that the lower bounds for  $\frac{1}{\theta_t}$ (as shown in inequalities \eqref{lb-yt-b} to \eqref{lb-yt-d}) are general results for censored feedback under the Weibull distribution and do not depend on the loss function or the specific algorithm used. Therefore, as long as the positive function $y_t$ is a monotone in $\frac{1}{\theta_t}$, a uniform lower bound for
$y_t$  is guaranteed.

%\paragraph{Assumption 3 (Uniform Lower Bound on Actions)}
%There exists a constant $K > 0$ such that, with high probability, $A_t \geq K$ for all $t$.


By synthesizing the above analysis on how the conclusions of all three key steps hold, we establish the following theorem on cumulative regret:
\begin{theorem}[Regret of TS for general online learining with censored feedback]Under Assumption \ref{assumption:regret} and Assumption \ref{assumption:action}, we have that 
    \[
\operatorname{Regret}(T, \theta_{\star}) \leq O\left( C_3 \ln(T) \sqrt{T} \right),
\]
where $C_3$ is a positive constant that depends on $C_1$, $C_2$, and the distribution parameters.
\end{theorem}

This establishes the $\sqrt{T}-$regret for the general online learning model we considered in this section.

\subsection{Technical limitation, possible refinement, and open questions} We highlight a technical limitation in Assumptions \ref{assumption:regret} and \ref{assumption:action}, which we believe can be addressed with additional complexity, as well as a more challenging open question.

First, the Lipschitz continuity assumptions currently involve expectations inside the absolute value. A more natural formulation would be to remove the expectation from these assumptions (e.g., by moving it outside the absolute value). This adjustment can be justified if the distribution of $A_t$ exhibits concentration properties (e.g., light-tailed, sub-Gaussian, etc.), though it would introduce additional complexity through high-probability arguments and tail assumptions.

Second, a more significant challenge lies in relaxing the Weibull distribution assumption. If successful, this would represent a substantial step forward from the current analysis. In particular, it would allow Assumption \ref{assumption:action} to be stated directly in terms of $\theta$ rather than 
$\frac{1}{\theta}$, making it conceptually more natural. We conclude that relaxing the Weibull assumption remains a central open question in developing a more general theory for online learning with censored feedback, especially in higher-dimensional action spaces. We leave this extension for future work.

\section{Conclusions}
\label{sec: conclusion}
We present the first systematic study on applying Thompson Sampling (TS) to the repeated newsvendor problem and provide an initial exploration of how our analytical framework can be extended to broader online learning problems with censored feedback. We establish frequentist regret bounds and offer insights into how TS automatically balances the trade-off between ``large exploration'' and ``optimal exploitation.'' Our analysis follows three key steps, which naturally generalize to broader settings.

This work opens up a range of compelling research directions. A key avenue for future exploration is extending regret analysis to broader online learning environments with censored feedback, particularly by relaxing the Weibull demand assumption to develop a more flexible and general framework. Additionally, applying TS to broader economic settings—such as auctions, dynamic pricing, and real-time resource allocation—presents exciting opportunities, as willingness-to-pay observations are often censored in these contexts. Advancing research in these areas has the potential to enhance decision-making under uncertainty, fostering more robust, efficient, and adaptive learning mechanisms for complex real-world problems.
%extension section 
%lipschiz assumption (6a)--invenotry
%from action to the model parameter (7)--ts close form 
% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

% Appendix
\appendix
\section{Appendix}

\subsection{Proof for Lemma \ref{lem:demand-range}}
\label{appendix: lemma demand range}
\begin{proof}
Since $D_t \sim \operatorname{Weibull}(\theta_{\star}
    )$, the cumulative distribution function for demand $D_t$ is indicated as $F_{D_t}(x)=1-e^{-\theta_{\star}x^k}$. Then we have
    \begin{align*}
          \mathbb{P}\left(D_t < \underline{D}\right)=1 - e^{-\theta_{\star}\underline{D}^k}  \le  \frac{\delta}{2T}, \qquad \mathbb{P}\left(D_t > \Bar{D}\right)=e^{-\theta_{\star}\Bar{D}^k} \le \frac{\delta}{2T}. 
    \end{align*}
    Choose appropriate $\underline{D},\overline{D}$ that satisfy above two inequalities and we obtain the lemma.
\end{proof}
\subsection{Proof for Lemma \ref{beta/alpha-1}}
\label{sec:appendix-lemma beta/alpha-1}
\begin{proof}

The proof largely follows Lemma B2, B3 in \cite{chuang2023bayesian}. We denote $H = \left\{H_t \right\}$ the natural filtration generated by the right-censored sales data, i.e $H_t=\sigma  \left\{(Y_i,\delta_i) : i \leq t\right\} $, where $Y_t =D_t \wedge y_t$ and $\delta_t =1 \left[D_t <  y_t \right]$.

According to the proof of Lemma B2 and B3 in \cite{chuang2023bayesian}, we have 


\begin{align*}
     N_t =\sum_{i=0}^{t-1}\left(Y_i^k-\mathbb{E}\left[Y_i^k \mid \mathcal{H}_{i-1}\right]\right),\qquad  M_t =\sum_{i=0}^{t-1}\left(\delta_i-\mathbb{E}\left[\delta_i \mid \mathcal{H}_{i-1}\right]\right)-1.
\end{align*}
$\left\{M_t\right\}$ and $\left\{N_t\right\}$ are zero-mean martingales given that the true unknown parameter is $\theta_{\star} \in \mathbb{R}_{+}$. We define $A_t=\sum_{i=0}^{t-1}\left(1-e^{-\theta_{\star} y_i^k}\right)$ then

Then we have, 
\begin{align*}
     \frac{\beta_t}{\alpha_t-1} -\frac{1}{\theta_{\star}}  &=\frac{1}{\theta_{\star}}\left(\frac{A_t+\theta_{\star} N_t}{A_t+ M_t-1}-1\right)\\
    &=\frac{1}{\theta_{\star}}\left(\frac{A_t+\theta_{\star} N_t-A_t-M_t+1}{A_t+ M_t-1}\right)\\
    &=\frac{1}{\theta_{\star}}\left(\frac{\theta_{\star} N_t-M_t+1}{A_t+ M_t-1}\right)\\
    &=\frac{N_t-\frac{1}{\theta_{\star}}\left(M_t-1\right)}{\alpha_t-1}.
\end{align*}

From \cite{chuang2023bayesian}, we have 
\begin{align*}
    N_t =\sum_{i=0}^{t-1}\left(Y_i^k-\mathbb{E}\left[Y_i^k \mid \mathcal{H}_{i-1}\right]\right),\qquad M_t-1 =\sum_{i=0}^{t-1}\left(\delta_i-\mathbb{E}\left[\delta_i \mid \mathcal{H}_{i-1}\right]\right)-1
\end{align*}
Therefore,
\begin{align*}
   N_t-\frac{1}{\theta_{\star}}\left(M_t-1\right)=\sum_{i=0}^{t-1}\left(Y_i^k-\frac{\left(\delta_i-1\right)}{\theta_{\star}}-\mathbb{E}_{\theta_{\star}}\left[\left(Y_i^k-\frac{\delta_i}{\theta_{\star}}\right) \mid \mathcal{H}_{i-1}\right]\right)
\end{align*} is a martingale values and satisfy 
\begin{align*}
    & Y_i^k- \frac{\left(\delta_i-1\right)}{\theta_{\star}} \leq \min\{D_i,y_i\}^k+\frac{2}{\theta_{\star}} \leq \overline{D}^k+\frac{2}{\theta_{\star}}
\end{align*}
  
Applying the Azuma–Hoeffding inequality, for $t \in [T]$,  with probability $1-\frac{1}{t^2}$
\begin{align*}
    \mathbb{P}\left( \left|\frac{\beta_t}{\alpha_t-1}- \frac{1}{\theta_{\star}}\right| =\frac{N_t-\frac{1}{\theta_{\star}}\left(M_t-1\right)}{\alpha_t-1} \geq \epsilon_t \right)&=\mathbb{P}\left( \left|  \frac{\beta_t}{\alpha_t-1}- \frac{1}{\theta_{\star}}\right| =N_t-\frac{1}{\theta_{\star}}\left(M_t -1\right)\geq \left(\alpha_t -1 \right)\epsilon_t \right)\\
 & \leq 2\exp{\left(\frac{- \epsilon_t^2 \cdot  \left(\alpha_t-1\right)^2 }{t \cdot\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)^2}  \right)}
 \end{align*}
 Plug in $\epsilon_t= \sqrt{\ln{\left(\frac{2t^2}{\delta}\right)}}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}}$ then we obtain the lemma.
\end{proof} 

\subsection{Proof for Lemma \ref{lower bound of y_t}}
\label{appendix: lower bound of y_t}
\begin{proof}
    Since $\theta_t \sim \operatorname{Gamma}(\alpha_t,\beta_t)$, we have $\frac{1}{\theta_t} \sim \operatorname{InverseGamma}(\alpha_t,\beta_t)$. According to \cite{chen2014concentration} Theorem 20, we have  
    
A random variable $X$ is said to have an inverse gamma distribution if it possesses a probability density function

$$
f(x)=\frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha-1} \exp \left(-\frac{\beta}{x}\right), \quad x>0, \quad \alpha>0, \quad \beta>0
$$


Let $X_1, \cdots, X_t$ be i.i.d. samples of random variable $X$. By virtue of the LR method, we have obtained the following results.

$$
\begin{aligned}
 \mathbb{P}\left\{\bar{X}_n \leq z\right\} \leq\left[\left(\frac{\beta}{\alpha z}\right)^\alpha \exp \left(\frac{\alpha z-\beta}{z}\right)\right]^n \quad \text { for } 0<z \leq \frac{\beta}{\alpha}
\end{aligned}
$$
 $\forall \ t \in [T]$, We plug in $n=1, z=\frac{\beta_t}{2\alpha_t}$ and $\bar{X}_t=\frac{1}{\theta_t}$, then get 
\begin{align*}
    \mathbb{P}\left(\frac{1}{\theta_t} \leq \frac{\beta_t}{2\alpha_t}\right) \leq \left(\frac{2}{e}\right)^{\alpha_t}
\end{align*}
Then, $\forall \ t \in [T], \mathbb{P}\left(\frac{1}{\theta_t} > \frac{\beta_t}{2\alpha_t}\right) \ge 1-\left(\frac{2}{e}\right)^{\alpha_t}$
\end{proof}
\subsection{Proof for Lemma \ref{lemma:sequence a and b}}
\label{appendix: proof sequence a and b}
\begin{proof}
    The proof is straightforward. Denote
    \begin{equation*}
        \min_{i \in [n]: b_i > 0} \left \{ \frac{a_i}{b_i}\right \} = \kappa,
    \end{equation*}
    then $a_i \ge \kappa b_i$ for any $i$ such that $b_i > 0$. Hence,
    \begin{equation*}
        \frac{\sum_{i=1}^n a_i}{ \sum_{i=1}^n b_i} \ge \frac{\sum_{i=1}^n 
 \kappa b_i}{ \sum_{i=1}^n b_i} = \kappa = \min_{i \in [n]: b_i > 0} \left \{ \frac{a_i}{b_i}\right \}.
    \end{equation*}
    This completes the proof.
\end{proof}
\subsection{Proof for Lemma \ref{lem:Mt-confidence}}
\label{appendix:Mt}
\begin{proof}
    Recall that $M_t=\sum_{i=0}^{t-1}\left(\delta_i-\mathbb{E}\left[\delta_i \mid \mathcal{H}_{i-1}\right]\right)$ defined in Lemma \ref{lemma:alpha_t,beta_t} and
$M_t$ is a martingale with bounded increments (specifically, bounded by 2 ), by Azuma's inequality we have,
\begin{align*}
    \mathbb{P}\left(\left|M_t\right| \geq \epsilon\right) \leq 2 \exp \left(-\frac{\epsilon_t^2}{8 t}\right).
\end{align*}
Therefore $\mathbb{P}\left(\left|M_t\right| \geq \sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)} \right) \leq \frac{\delta}{ t^2}$. 
\end{proof}
\subsection{Auxiliary Lemmas}
\label{appendix: proof for lemma: alpha_t-1}
\begin{lemma}
    \label{lemma: alpha_t-1 }
    Denote 
$$T_0 = 64\left(1-\exp\{-\theta_{\star} L^k\}\right)^{-2}\ln{\frac{T}{\delta}},$$ Therefore,
    \begin{equation*}
    \alpha_t - 1 \ge \begin{cases}
        \alpha_0 - 1 & t \le T_0,\\
        \frac{1}{2} t\left(1-\exp\{-\theta_{\star} L^k\}\right) & t > T_0.
    \end{cases}
\end{equation*}
\end{lemma}
\begin{proof}
    Given above $\alpha_t$ is defined as $\alpha_t = \alpha_0 + \sum_{i=1}^{t} \delta_i$. Given that $\alpha_0 \ge 2$, it follows that $\alpha_t \ge \alpha_0 \ge 2$, and thus $\alpha_t - 1 > 0$ for all $t$.

To facilitate our analysis, we define the following high-probability events:

\begin{equation*}
    \begin{aligned}
    & \xi^{(1)}_{t} = \{\underline{D}\le D_t\le \overline{D} \},\quad \xi^{(2)}_{t} = \left\{\left|\frac{\beta_t}{\alpha_t-1}- \frac{1}{\theta_{\star}}\right| \leq \sqrt{\ln{\left(\frac{2t^2}{\delta}\right)}}\left(\overline{D}^k+\frac{2}{\theta_{\star}}\right)\sqrt{\frac{t}{\left(\alpha_t-1\right)^2}}\right \} \\
    & \xi^{(3)}_{t}= \left \{\left|M_t\right| \leq \sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)} \right \},\quad \xi^{(4)}_{t}= \left\{\frac{1}{\theta_t} > \frac{\beta_t}{2\alpha_t} \right \},
    \end{aligned}
\end{equation*}
and $\xi^{(1)} = \cap_{t=1}^T \xi^{(1)}_{t}$, $\xi^{(2)} = \cap_{t=1}^T \xi^{(2)}_{t}$, $\xi^{(3)} = \cap_{t=1}^T \xi^{(3)}_{t}$, $\xi^{(4)} = \cap_{t=1}^T \xi^{(4)}_{t}$, $\xi = \cap_{i=1}^4 \xi^{(i)}$. Condition on event $\xi$, we have for all $t\in[T]$, 
\begin{subequations}
\begin{align}
     \alpha_t-1&=\alpha_0+\sum_{i=0}^{t-1}\left(1-e^{-\theta_{\star} y_i^k}\right)+M_t-1 \label{eq:sket-pf-bound-yt-alphat-a}\\
     &\ge \alpha_0-1+\left(t-1\right)\left(1-\exp\{-\theta_{\star} L^k\right)+M_t \label{eq:sket-pf-bound-yt-alphat-b}\\
    & \ge t\left(1-\exp\{-\theta_{\star} L^k\}\right)+\alpha_0-1-\left(1-\exp\{-\theta_{\star} L\}^k\right)-\sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)} \label{eq:sket-pf-bound-yt-alphat-c} \\
    & \ge t\left(1-\exp\{-\theta_{\star} L^k\}\right)-\sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)} \nonumber . 
\end{align}
\label{eq:sket-pf-bound-yt-alphat}
\end{subequations}
 (\ref{eq:sket-pf-bound-yt-alphat-a}) is derived from Lemma \ref{lemma:alpha_t,beta_t}. (\ref{eq:sket-pf-bound-yt-alphat-b}) comes from the fact that $y_t \ge L$ for all $t$ when the event $\xi$ holds. (\ref{eq:sket-pf-bound-yt-alphat-c}) comes from the following Lemma \ref{lem:Mt-confidence}, which is proved in Appendix \ref{appendix:Mt}. 
\begin{lemma}
    \label{lem:Mt-confidence}
    For $t \in [T]$,
    \begin{align*}
        \mathbb{P} \left(M_t \geq \sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)}\right) \leq \frac{\delta}{ t^2}.
    \end{align*}
\end{lemma}


To further analyze $\alpha_t - 1$, we use the technique of truncating $T$ as follows:


Denote 
$$T_0 = 64\left(1-\exp\{-\theta_{\star} L^k\}\right)^{-2}\ln{\frac{T}{\delta}},$$
When $t > T_0$, we have
\begin{equation*}
    \alpha_t-1 \ge t\left(1-\exp\{-\theta_{\star} L^k\}\right)-\sqrt{8t}\ln{\left(
    \frac{2t^2}{\delta}\right)} > \frac{1}{2} t\left(1-\exp\{-\theta_{\star} L^k\}\right).
\end{equation*}
Therfore we have,
\begin{equation*}
    \alpha_t - 1 \ge \begin{cases}
        \alpha_0 - 1 & t \le T_0,\\
        \frac{1}{2} t\left(1-\exp\{-\theta_{\star} L^k\}\right) & t > T_0.
    \end{cases}
\end{equation*}

Finally we discuss the probability of event $\xi$.
\begin{subequations}
    \begin{align}
    \mathbb{P}(\xi) & = 1 - \sum_{i=1}^4 \mathbb{P}(\neg\xi^{(i)}) \nonumber  \\
    & = 1 - \sum_{i=1}^4 \sum_{t=1}^T \mathbb{P}(\neg\xi^{(i)}_t) \nonumber \\
    & \ge 1 - \sum_{t=1}^T \frac{\delta}{T} - \sum_{t=1}^T \frac{\delta}{t^2} - \sum_{t=1}^T \frac{\delta}{t^2}- \sum_{t=1}^T \left(\frac{2}{e} \right)^{\alpha_t} \label{eq:pf-yt-xi-bound-a} \\
    & \ge 1 - \delta - \frac{\pi^2}{6} \delta - \frac{\pi^2}{6} \delta - \delta \label{eq:pf-yt-xi-bound-b} \\
    & \ge 1 - 6 \delta.\nonumber
    \end{align}
    \label{eq:pf-yt-xi-bound}
\end{subequations}
For (\ref{eq:pf-yt-xi-bound-a}), the first term comes from Lemma \ref{lem:demand-range}, the second term comes from Lemma \ref{beta/alpha-1}. The third term comes from Lemma  \ref{lem:Mt-confidence}. The fourth term comes from Lemma \ref{lower bound of y_t}. (\ref{eq:pf-yt-xi-bound-b}) comes from the following, recall $\alpha_0 \ge \frac{\ln{\frac{T}{\delta}}}{\ln{\frac{e}{2}}}$.
    \begin{equation*}
        \sum_{t=1}^T\left(\frac{2}{e}\right)^{\alpha_t} \le \sum_{t=1}^T  \left(\frac{2}{e}\right)^{\alpha_0} = T \cdot \left(\frac{2}{e}\right)^{\alpha_0} = T \cdot e^{-\ln(e/2) \cdot \alpha_0} = T \cdot \left(\frac{\delta}{T} \right) \le \delta. \nonumber
    \end{equation*}

Consequently, with probability $\ge 1 - 6 \delta$,
\begin{equation*}
    \alpha_t - 1 \ge \begin{cases}
        \alpha_0 - 1 & t \le T_0,\\
        \frac{1}{2} t\left(1-\exp\{-\theta_{\star} L^k\}\right) & t > T_0.
    \end{cases}
\end{equation*}
\end{proof}
\end{document}
