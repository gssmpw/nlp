%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\renewcommand{\baselinestretch}{0.965}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% custom package
\usepackage{multirow}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization}

\begin{document}

\twocolumn[
\icmltitle{Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chanhui Lee}{ku-ai}
\icmlauthor{Yuheon Song}{ku-ai}
\icmlauthor{YongJun Jeong}{ku-ai}
\icmlauthor{Hanbum Ko}{ku-ai}
\icmlauthor{Rodrigo Hormazabal}{lg-ai}
\icmlauthor{Sehui Han}{lg-ai}
\icmlauthor{Kyunghoon Bae}{lg-ai}
\icmlauthor{Sungbin Lim}{ku-stat}
\icmlauthor{Sungwoon Kim}{ku-ai}

\end{icmlauthorlist}

\icmlaffiliation{ku-ai}{Department of Artificial Intelligence, Korea University, Seoul, Korea}
\icmlaffiliation{ku-stat}{Department of Statistics, Korea University, Seoul, Korea}
\icmlaffiliation{lg-ai}{LG AI Research, Seoul, Korea}

\icmlcorrespondingauthor{Sungwoon Kim}{swkim01@korea.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Multimodal LLM, Molecular Structure Learning, Multi-task Learning, Multimodal Preference Optimization}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks.
While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure.
Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs.
To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multi-modal training method based on a thorough multi-modal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs.
On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs.
Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective.
\end{abstract}

\section{Introduction}
\label{sec:intro}

% LLM으로 molecular task 수행하는 이유
Thanks to their impressive capabilities in complex reasoning and task generalization, Large Language Models (LLMs) \cite{GPT3,GPT4,Gemini,llama2} have been widely utilized as adaptable tools for performing a broad array of tasks across multiple domains.
This achievement has recently generated interest in applying LLMs on molecular data to solve diverse molecular tasks such as molecule property prediction, molecule description generation, and chemical reaction analysis, which are essential in drug discovery and materials science \cite{Yu2024LlaSMolAL,Pei2024BioT5TG,Fang2023MolInstructionsAL,Liu2023MolCAMG,Cao2023InstructMolMI,Liu_2024,zhang2024unimotunifiedmoleculetextlanguage,li20243dmoleculetextinterpretationlanguage}. 

% LLM으로 molecular task 수행할 때 키포인트 뭔지
Notably, recent molecular LLMs aim to leverage two important components for better molecular language modeling: molecular graph utilization and multi-task instruction tuning.
For example, a number of studies \cite{Liu2023MolCAMG,zhang2024unimotunifiedmoleculetextlanguage,Liu_2024} have moved away from conventional molecular language modeling based on 1D textual representations like SMILES \cite{10.1021/ci00057a005} and SELFIES \cite{Krenn_2020}.
Instead, they have developed multi-modal LLMs by incorporating 2D molecular graphs \cite{liu2024multimodalmoleculestructuretextmodel,Yuyang_2022,su2022molecularmultimodalfoundationmodel} as an additional input modality, which better represents molecular structures and topologies, leading to improved performances on various molecular tasks.
Meanwhile, other researches \cite{Fang2023MolInstructionsAL,Cao2023InstructMolMI,Yu2024LlaSMolAL,Pei2024BioT5TG} have built instruction tuning datasets for multiple molecular tasks and fine-tuned LLMs on these datasets, enabling them to acquire transferable and generalizable knowledge and also to differentiate, understand, and execute a variety of tasks using only natural language instructions or prompts. 


% naive string+graph 는 grpah util을 안하고, 그래서 효과적인 graph utilization 방법이 필요
\begin{figure}\centering
    \includegraphics[width=.38\textwidth]{figures/fig4_random_input_result.png}
    \caption{Investigation on which modality multimodal molecular LLM trained via SFT training focus on molecular tasks (QM9).}
    \label{fig:random_result}
    \vspace{-0.7cm}
\end{figure}

%To evaluate whether SELFIES and graph representations are effectively utilized, we conducted experiments by substituting each with different graphs or SELFIES  representations within the same task. Figure \ref{fig:random_result} illustrates the limitations of the naive SELFIES and graph approach, which fails to leverage the structural information of the graph. The results on random graphs were indistinguishable from those on the original graph, demonstrating the failure to exploit the graph's potential information. In contrast, using random SELFIES led to a significant performance drop, highlighting the need for a method that efficiently integrates both SELFIES and graph information.
% 그러나, 1D molecular sequence와 2D molecular graph structure를 사용하는 모델들이 graph utilization을 잘 하고 있는지는 불분명하다. 이를 위해, 우리는 1D molecular sequence보다 2D graph 사용이 유리할 것으로 일반적으로 믿어지는 QM9 task에서 1D molecular sequence와 2D graph를 모두 사용하는 모델들이 graph utilization을 실제호 하고 있는지 검증하고자 했다. 이를 위해 1D molecular sequence와 2D molecular graph를 사용하여 SFT training한 molecular LLM을 활용, molecular task를 수행할 때 주어진 1D molecular sequencedhk 2D graph 각각 을 random molecule의 것으로 치환하여 task 를 수행했고, 이를 통해 molecular llm이 해당 task를 수행할 떄 어떤 modality를 주로 사용하는지를 성능 하락하는 정도로 파악할 수 있다. 드러난 결과는 naive SFT training으로는 graph modality를 downstream task에 거의 활용하고 있지 않다는 것으로, molecular graph utilization을 위해 별도의 방법이 필요함을 할 수 있다.
However, it remains unclear whether models using both 1D molecular sequences and 2D molecular graph structures effectively utilize the graph information. To investigate this, using a molecular LLM trained with SFT on both 1D molecular sequences and 2D molecular graphs, we conducted experiments where we substituted either the given 1D molecular sequence or 2D molecular graph with those from random molecules during task execution Figure \Cref{fig:random_result}. The extent of performance degradation indicates which modality the molecular LLM primarily relies on for task completion. The results revealed that naive SFT training barely utilizes the graph modality for downstream tasks, largely depend on 1D molecular sequences, suggesting that additional methods are necessary for effective molecular graph utilization.


% 그 키포인트 관련해서 현재 모델들이 못하는 점
Further, despite the potential for synergistic performance improvements by these two components, few studies have fully harnessed both of their benefits, especially for a universal molecular LLM.
Specifically, some recent studies \cite{Cao2023InstructMolMI,li20243dmoleculetextinterpretationlanguage,liang2023drugchatenablingchatgptlikecapabilities,zhang2024unimotunifiedmoleculetextlanguage,pei20243dmolt5unified3dmoleculetext} have attempted to combine molecule graph representations with instruction tuning, however, their instruction tunings have been focused on task-specific fine-tuning or text-oriented tasks.
As a result, these models failed to perform as true generalist molecular LLMs, particularly in tasks like molecule generation and chemical reaction prediction.

% 이 논문에서 그걸 극복하려고 뭘 했는지
In this paper, we propose a unified and generalist molecular LLM that can reap the merits of using the graph representation in a multi-modal and multi-task instruction tuning way.
In particular, while maintaining the structure of the multi-modal LLM based on Q-Former \cite{li2023blip2bootstrappinglanguageimagepretraining}, we introduce a novel multi-modal instruction tuning to force the model to produce responses based on the conditional graph modality. 
More specifically, we corrupt the SELFIES~\citep{Krenn_2020} input tokens by replacing some tokens with random samples during the instruction tuning over many molecular tasks and datasets, which mitigates the overlooking of the graph condition and effectively improves the graph utilization.

%In addition, during multi-task instruction tuning, we discovered that varying the data sampling ratio according to the difficulty of each task leads to performance improvements. Consequently, we implemented and applied a training framework that allows for the adjustment of the data sampling ratio for each task.
Experimental results on various molecule benchmark tasks including molecule property prediction, chemical reaction prediction, molecule description generation, and description-guided molecule generation show that the proposed generalist model, which we call Mol-LLM, can learn vast molecular tasks with improved multi-modal and multi-task training with better utilization of the graph modality.
To the best of our knowledge, Mol-LLM is the first multi-modal molecular LLM that deals with various existing molecular tasks by a single generalist model, demonstrating the effectiveness of using the graph modality in a multi-task instruction-tuning scheme.

To summarize, our main contributions are:
(1) a generalist molecular LLM as a unified multi-modal model that makes use of both 1D textual and 2D graph representations as inputs;
(2) an extensive instruction tuning on almost all existing molecular tasks addressed by molecular LLMs by preference-based cross-modal training; and
(3) demonstration of the generalization ability to various tasks through the improvement of structural understanding in Mol-LLM.
% empirical strategy to pre-train cross alignment projector in causal language modeling, when there is no sufficient pre-training data for molecule, text description pairs.


\section{Related Works}
\label{sec:related_works}
\subsection{Molecular Large Language Models}
Recent LLMs have shown strong reasoning and generalization abilities due to their extensive background knowledge. To leverage these capabilities in molecular science, many researches have been recently conducted in the way of molecular LLMs. For example, MolT5 \cite{Edwards2022TranslationBM} employs a T5-based \cite{raffel2023exploringlimitstransferlearning} framework when trained on a large number of molecule SMILES and texts to translate between them, while MolXPT \cite{molxpt} is a unified language model of text and SMILES based on GPT \cite{radford2019language} for text-molecule translation as well as molecular property prediction. MolCA \cite{Liu2023MolCAMG} and GIT-Mol \cite{Liu_2024} utilize a 2D molecular graph as an additional input modality for better capturing molecular structures and exploit multi-modal LLMs using a Q-Former \cite{li2023blip2bootstrappinglanguageimagepretraining} to align graph and text representations. MolLM \cite{mollm} is designed to further encode 3D molecular structures for explicitly utilizing geometric information. In order to seamlessly perform generative modeling on molecular structures in LLMs, UniMoT \cite{zhang2024unimotunifiedmoleculetextlanguage} obtains discrete graph tokens from output representations of the Q-Former projector while 3D-MolT5 \cite{pei20243dmolt5unified3dmoleculetext} employs 3D structure tokens.
However, these models cannot handle various modalities and different tasks simultaneously.
% have limitations in becoming generalist molecular models due to their lack of molecular structure understanding.
To address this, we developed a generalist model using a structural preference method that enhances molecular structure comprehension.

\subsection{Instruction Tuning on Molecular Tasks}
In order to perform diverse tasks including new tasks by a single model without task-specific fine-tuning, instruction tuning has become a popular technique for LLMs. In particular, instruction tuning over multiple tasks allows LLMs to acquire transferable and generalizable knowledge while maintaining the capability to understand task instructions. Therefore, recently, there has been a growing number of studies dealing with instruction tuning for molecular LLMs across various molecular tasks. For instance, Mol-Instructions \cite{Fang2023MolInstructionsAL} develops the first comprehensive instruction dataset for a wide range of molecular tasks with instructional formats and demonstrates that instruction tuning with this dataset can improve performances of molecular LLMs on both understanding and generation tasks. InstructMol \cite{Cao2023InstructMolMI} applies task-specific instruction tuning when fine-tuning a multi-modal molecular LLM for each task, while BioT5+ \cite{Pei2024BioT5TG} conducts multi-task instruction tuning for each group of tasks as a semi-generalist model using the Mol-Instructions dataset and shows remarkable performances in most tasks without the use of molecular graphs. On the other hand, LlaSMol \cite{Yu2024LlaSMolAL} collects a much larger and more diverse instruction tuning dataset that consists of 3.3M instances from 14 tasks and trains a generalist molecular LLM, demonstrating the merits of the dataset, especially in terms of generalist model's performances. 
%Moreover, 3D-MoLM \cite{li20243dmoleculetextinterpretationlanguage} develops a 3D molecule-centric instruction tuning dataset and performs instruction tuning for a molecular LLM that incorporates a 3D molecular encoder.

Here, it is noted that some recent studies such as InstructMol \cite{Cao2023InstructMolMI}, UniMoT \cite{zhang2024unimotunifiedmoleculetextlanguage}, 3D-MolT5 \cite{pei20243dmolt5unified3dmoleculetext} attempt to integrate molecular structure representations with instruction tuning. However, their instruction tunings are restricted to task-specific fine-tuning for specialist models or multi-task fine-tuning on text-oriented tasks excluding molecule generation and chemical reaction prediction. While our work also lies in the combination of the multi-modal molecular LLM using the molecular graph modality with multi-task instruction tuning, different from the previous models, our model performs as a generalist multi-modal model that can cover more diverse tasks including understanding and generation of both molecules and texts, resulting from our robust multi-modal instruction tuning.

% \subsection{Preference Optimization}

% Preference optimization is the process of fine-tuning a LLM to generate preferred outputs based on user preferences from a range of possible outputs.
% DPO~\citep{rafailov2023dpo} achieves this by increasing the likelihood of preferred outputs while decreasing the likelihood of non-preferred outputs, thereby guiding the model toward generating desirable results.
% To enhance conditioning on images in multi-modal tasks, mDPO~\citep{Wang2024mDPOCP} was proposed to deal with the image modality.
% However, these models require the use of a reference model, which incurs computational costs.
% To address this issue, SimPO~\citep{Meng2024SimPOSP} was introduced, which eliminates the need for a reference model in the reward objective function.
% Although preference optimization techniques have been extensively validated in tasks involving images and text, they have not been thoroughly tested on molecular tasks. Mol-LLM aims to evaluate the effectiveness of preference optimization in tasks involving molecular structures.

\section{Method}
\label{sec:method}

\begin{figure*}\centering
    \includegraphics[width=1.\textwidth]{figures/fig1_overview.pdf}
    \caption{Structure of Mol-LLM and examples of downstream tasks. Task instructions, Molecular 1D SELFIES, and Molecular 2D graph are given as inputs, and the graph is converted into a fixed number of tokens through Q-former. Examples of special tokens used in each downstream task are shown in the answers to each task.}
    \label{fig:overview}
    \vspace{-0.5cm}
\end{figure*}
We introduce our multi-modal architecture using both 1D SELFIES and 2D graphs and propose a multi-modal training method to enhance molecular structure understanding of molecular LLM.

\subsection{Model Architecture}

\paragraph{Graph Encoder}
For the LLM versatile on diverse molecular tasks, it is beneficial in leveraging molecular structure representations, in addition to 1D textual representations such as SMILES \cite{10.1021/ci00057a005} or SELFIES \cite{Krenn_2020}.
Given that, \citet{Liu2023MolCAMG, Cao2023InstructMolMI} choose molecular graph representation, where a graph encoder encodes a molecular graph and then cross-modal projector is used to align graph embedding and text embedding to feed LLMs. 
We take MoleculeSTM \cite{liu2024multimodalmoleculestructuretextmodel} as a graph encoder that is composed of a 5-layer graph isomorphism network (GIN) \cite{gin}. For the cross-modal alignment between graph and text, MoleculeSTM is pre-trained via contrastive learning on the molecular caption and molecular graph on 280K molecule-text pairs from PubChem \cite{pubchem}.
Given a 2D molecular graph $G=(V, E)$, the graph encoder $f_g$ extracts node-level embeddings $\mathbf{Z}_{node}=f_g(V, E)\in \mathbb{R}^{|V| \times d_g}$, where $|V|, d_g$ indicate the number of nodes in the molecule and its embedding dimension, respectively.
Then, we concatenate mean pooled global graph embeddings as $Z_{global}=\frac{1}{|V|}\sum_i^{|V|}Z_{node, i} \in \mathbb{R}^{d_g}$ to $\mathbf{Z}_{node}$ getting $\mathbf{Z} = [Z_{global}, Z_1, \dots, Z_{|V|}] \in \mathbb{R}^{(|V|+1) \times {d_g}}$.

\paragraph{Cross-modal Projector}
There are two well-known methods to bridge different modalities for multi-modal LLMs: using shallow linear layers as used in \citet{llava} or using Q-Former, which is a bi-directional transformer encoder as \citet{li2023blip2bootstrappinglanguageimagepretraining}.
The key distinction between the two is whether they compress the information from graph embedding. 
While the linear projector inputs graph embeddings equal to the number of nodes, Q-Former compresses graph embeddings to a fixed number using Q-Former's attention mechanisms. 
% Although the use of graph tokens as inputs showed minor improvements in regression tasks, we found that inputting many graph tokens into LLM adversely affected the task recognition ability across various foundation modeling applications.
% In addition, representing molecules of an arbitrary size by a fixed number of queries helps LLM training by facilitating batch processing.
% Given these findings, we selected Q-Former since its compressed and fixed number of graph token approach provides advantages in task recognition. 
We select Q-Former since its compressed and fixed number of graph token approach provides advantages in task recognition and efficient batch processing. 
With $N_q$ queries $\mathbf{Q} \in \mathbb{R}^{N_q \times d_q}$ representing an input molecule, Q-Former conducts cross-attention between queries $\mathbf{Q}$ and dimension-adapted molecular graph embedding $\mathbf{Z'} \in \mathbb{R}^{(|V|+1) \times d_q}$, by linear dimension projection.
Through the cross-modal cross-attention, Q-Former learns to feature queries to represent impactful molecular structural information, such as the functional group in the molecule and the molecule's backbone structure, etc.
In general, the greater $N_q$, the queries have more capacity to express molecular information.
Here, we use 32 Q-Former queries for molecular foundation modeling, unlike 8 queries of MolCA, a task-specific model. 

\paragraph{LLM}
We choose universal molecular language modeling as depicted in Figure \ref{fig:overview}, where LLMs address a variety of molecular tasks through next token prediction without the need for task-specific adapters. 
Since addressing diverse molecular tasks requires large model capacity, we choose the Mistral-7B-Instruct-v0.3 \citep{jiang2023mistral7b} with LoRA rank 64 as a backbone LLM for text and 1D SELFIES input, similar to \citet{Yu2024LlaSMolAL}. Besides obtaining from the graph, we also use 1D SELFIES as molecular representations for LLM inputs.
%The molecular tasks encompass molecular property classification and regression, chemical reaction prediction, molecule captioning, and molecule generation guided by text descriptions.
%For tasks involving molecular inputs, the initial step involves feeding the molecule graph $G$ into a graph encoder to obtain the graph embeddings $\mathbf{Z}$. Subsequently, learnable queries are inputs into the Q-Former, where they undergo information extraction and compression from graph embeddings through cross-attention where the graph embeddings $\mathbf{Z}$ serve as keys.
%Besides obtaining from the graph, we also use 1D SELFIES as molecular representations for LLM inputs.
% SELFIES are recognized for their greater robustness compared to SMILES, which is crucial for employing molecular textual representations in LLM-based molecule generation.

\paragraph{Tokenization} 
As we optimize for molecular representation not only from the 2D graph but also 1D SELFIES, we added 3K SELFIES tokens into LLM's vocabulary.
Moreover, empirical evidence indicates that in regression tasks, utilizing existing numerical tokens in a pre-trained LLM is not ideal.
This is due to the difference in the context of number generation between next-token prediction during LLM's pre-training and the regression tasks. To bridge this context gap, we have introduced several specialized tokens to represent numerical values, $`` |<i>| ", i \in [0, 9]$.
Additionally, we have introduced tokens to denote answer types for floats, booleans, text descriptions (as shown in Figure \ref{fig:overview}), molecular representation types, and the direction of chemical reactions, represented by $``|>>|"$.
%, whose motivations are also to clarify the formulation in each molecular tasks.

%By solely relying on natural language descriptions to solve diverse tasks and adding special tokens without the need for task-specific adapters or complex pipelines, we can fully leverage the benefits of instruction tuning and enhance the generalization capabilities of LLMs.


\subsection{Training Framework}

% Instruction dataset table
\begin{table}[t]
% \footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Details of Mol-LLM instruction tuning training data and its sources.
}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}llcc@{}}
\toprule 
& TASKS & \# SAMPLES & DATA SOURCES  \\
\midrule
& Property Prediction (Regression) & 1.4M & MoleculeNet \\
& Property Prediction (Classification) & 58K & MoleculeNet\\
& Molecule Description Generation & 50K & ChEBI-20 \\
& Description Guided Molecule Generation & 50K & ChEBI-20 \\
& Name Conversion & 1.2M & PubChem \\
& Forward Prediction & 1.1M & USPTO \\
& Retrosynthesis & 1M & USPTO\_500MT \\
& Reagent Prediction & 120K & USPTO\_500K \\
\midrule

\end{tabular}
}
\label{tab:instruction_stat}
\vspace{-0.5cm}
\end{table}

\paragraph{Diverse Instruction Tuning Datasets}
To construct thorough instruction tuning on diverse molecular tasks, we aimed to collect as diverse dataset as possible, with high-quality standards. In details, leveraged Mol-Instruction dataset \cite{Fang2023MolInstructionsAL}, SMolInstruct dataset \cite{Yu2024LlaSMolAL}, MoleculeNet dataset \cite{Wu2017MoleculeNetAB}, and ChEBI-20 dataset \cite{Edwards2022TranslationBM}, whose dataset sizes are specified in Table \ref{tab:instruction_stat}.

For molecular property classification problems, we utilized the MoleculeNet datasets, which are famously adopted, including various problems for classification for molecular toxicity (Tox21, SIDER, Toxcast, Clintox), HIV inhibition, blood-brain barrier penetration (BBBP), Alzheimer's inhibition (BACE).
The Mol-Instruction dataset for molecular learning comprises three tasks: property regression, chemical reaction prediction, molecule captioning, and description-guided molecule generation.
To increase the amount of training data and incorporate high-quality datasets, we utilized not only Mol-Instruction but also SMolInstruct. We deduplicated molecules and texts between Mol-Instruction and SMolInstruct in each task, removing Mol-Instruction instances for duplicated cases.
The property regression problems come from the QM9 dataset, which is used to predict 12 atomic level molecular properties. We used 11 labels including HOMO, LUMO, and HOMO-LUMO gap for training to enable multifaceted learning of atomic-level property knowledge.
Chemical reaction prediction tasks are classified into forward reaction prediction, which is to predict a product molecule given a reactant molecule, retrosynthesis, which is the inverse version of forward prediction, and reagent prediction, which is to predict a reagent molecule given a reactant and product molecule.
The remaining tasks are description-guided molecule generation and molecule captioning, where Mol-Instruction used PubChem as data source.
However, as specified in \ref{sec:method}, we found that the majority portions of the PubChem~\cite{pubchem} data consist of very short and repetitive sentences, which is the reason we omitted it from our multi-task instruction tuning datasets.
Instead, we chose the ChEBI-20~\cite{Edwards2022TranslationBM} dataset, which provides more detailed descriptions of molecular structure and properties, as our translation task data.


\paragraph{Two-staged training: Supervised Fine-Tuning (SFT)}
In causal language modeling tasks involving multiple modalities, pre-training a cross-modal projector is typically considered essential for the optimal utilization of multi-modal information, thereby which enable to achieve optimal performance.
%If the cross-modal projector, modality encoder and LLM are trained simultaneously without prior alignment, the embedding of the cross-modal projector may not be aligned with causal language modeling, which could limit LLM to achieve optimal performance.
Although \citet{Liu2023MolCAMG} pre-trained the Q-Former on a dataset of molecule and molecule caption pairs collected from PubChem, we observed that these captions do not result in meaningful performance improvement in downstream tasks.
This lack of improvement may be attributed to the limited information content in the dataset, where 290K out of 324K captions contained fewer than 30 tokens.
This suggests that obtaining large-scale molecule-text pairs and performing Q-Former pretraining might potentially improve downstream task performance, only after obtaining large-scale molecule-text pair, which is very challenging. 
To our knowledge, no dataset currently exists that contains more molecule-caption pairs than PubChem. While using LLMs to generate captions for molecules could potentially scale up the data, this approach is known to be vulnerable to critical hallucinations in the scientific domain.

Given this challenge of multi-modal utilization due to lack of high-quality pre-training data, instead, we propose a training framework that enhances the performance of existing Molecular LLMs without requiring pre-training on large-scale molecule-caption pair data. 
Instead, our approach combines supervised fine-tuning on downstream task data and subsequent preference optimization for effective utilization of multi-modal information.
For this purpose, we first conduct SFT training using a graph encoder and Q-Former. During this process, we input query embeddings extracted from Q-Former using the frozen graph encoder's graph embeddings into the LLM. The Q-Former and LLM LoRA parameters are trained using SFT loss from the LLM's next token prediction task for downstream tasks. This enables the model to learn molecular task performance capabilities achievable without sophisticated multimodal utilization, while aligning the Q-Former's cross-modal representations.


\begin{figure}\centering
    \includegraphics[width=.36\textwidth]{figures/fig2_molpo.pdf}
    \caption{An overview of Molecular Structure Preference Optimization (MolPO). When original data is entered (chosen), the likelihood of the output is increased, and when data containing rejected molecules is entered (Rejected), the likelihood of the output is lowered.}
    \label{fig:molpo}
    \vspace{-0.6cm}
\end{figure}

\paragraph{Two-staged training: Molecular Structure Preference Optimization}
\label{subsec:corruption}
% diverse molecular task에 대한 SFT training 수행한 다음, molecular graph를 활용도를 높여 기존 SFT training보다 더욱 최적의 성능을 얻기 위한 molecular structure preference optimization (MolPO)을 수행한다.
After performing SFT training on various molecular tasks, \textit{Molecular Structure Preference Optimization} (MolPO) is conducted to enhance the utilization of molecular graphs and achieve optimal performance beyond conventional SFT training.
% 이를 위해, \Cref{fig:molpo}에 묘사된 것과 같이 각 Task의 original molecular graph를 chosen로 활용하여, positive molecular graph로부터 각 task를 수행하기 위해 필요한 informative한 feature를 훼손하는 molecular graph structure modification을 수행하여 rejected molecular graph를 만든다. 
As depicted in \Cref{fig:molpo}, the original data triplet of $(g, s, q, y)$ is used as chosen pair $(g_w, s, q, y)$ w.r.t. the molecular graph, where $g, s$ each represents molecular graph and molecular selfies, and $q, y$ are task instruction and corresponding ground truth label  molecular graphs from each task are used as the chosen examples. 
Rejected molecular graphs $g_l$ are then created by performing molecular graph structure modifications that degrade the informative features necessary for each task from $q_w$.
% 예를 들어, molecule captioning Task에서 카르복실기가 포함된 분자와 이에 대해 설명 텍스트가 label도 주어진 경우, LLM은 분자의 구조 및 기능적인 특징을 다루기 위해 functional group을 설명하여야 한다. 이때 chosen molecular graph에 포함된 카르복실기를 제거하는 structure modification을 가하여 rejected molecular graph를 만들면, rejected molecular graph를 입력받은 LLM은 ground truth label에 대해 높은 Likelihood를 주지 않도록 training한다.
Then, by leveraging molecular structure preference pair $(g_w, s, q, y), (g_l, s, q, y)$, we formulate following preference optimization objective motivated from \citet{Wang2024mDPOCP} to enhance molecular graph utilization of molecular LLM $\pi_\theta$:
\begin{align}
\label{eq:molpo}
    \mathcal{L}_\text{MolPO} = -\log\sigma(\beta\log\frac{\pi_\theta(y|g_w, s, q, y)}{\pi_\text{ref}(y|g_w, s, q, y)} \nonumber \\
    - \beta\log\frac{\pi_\theta(y|g_l, s, q, y)}{\pi_\text{ref}(y|g_l, s, q, y)} - \gamma),
\end{align}
where $\beta$ is a hyperparameter preventing too much deviation of $\pi_\theta$ from reference model $\pi_\text{ref}$, and $\gamma$ a hyperameter to guarantee proper likelihood gap from chosen and rejected pairs.
For example, in a molecule captioning task where a molecule containing a carboxyl group is provided along with its descriptive text label, the LLM must describe the functional group to address the molecule's structural and functional characteristics (details in \Cref{appx:gen_rejectged}. By creating a rejected molecular graph through removing the carboxyl group from the chosen molecular graph, the LLM is trained to assign low likelihood scores to the ground truth label when presented with the rejected molecular graph as input.

% 그러나, diverse molecular task에 대한 foundation modeling은 대규모 데이터에 데이터에 대한 Training을 요구하므로, reference model을 training objective에 포함시키는 것은 burdensome하다. 우리는 reference model의 computational cost를 줄임으로써 scalable training이 가능하도록 하기 위해 \citet{simpo}의 방법을 따라 preference objective를 다음과 같이 대체하였다.
However, foundation modeling for diverse molecular tasks requires training on large-scale datasets, making it burdensome to include a reference model in the training objective. To enable scalable training by reducing the computational cost of the reference model, we followed the approach of \citet{Meng2024SimPOSP} to use reference-free preference objective as follows:
\begin{align}
\label{eq:molpo_ref_free}
    \mathcal{L}_\text{MolPO}^\prime = -\log\sigma(\frac{\beta} {|y|}(\log\pi_\theta(y|g_w, s, q, y) \nonumber \\
    - \log\pi_\theta(y|g_l, s, q, y) - \gamma)).
\end{align}

Following \citet{Meng2024SimPOSP}, we employ the length-normalized sum of token log probabilities from $\pi_\theta$ as the reference-free reward.  Here, length normalization is to address length bias, because longer sequences typically yield lower log probabilities, creating challenges for molecular modeling across different tasks. 
While classification and regression tasks require only brief token sequences to represent boolean or numeric values, tasks such as reaction prediction, molecule generation, and molecular captioning require hundreds of tokens to generate SELFIES and text descriptions. 
Without length normalization of the reward, preference optimization across tasks with varying token lengths becomes significantly challenging.

With the reference-free preference objective, the total training objective in MolPO phase is as follows:
\begin{align}
\label{eq:tot_loss}
    \mathcal{L}_\text{tot} = \mathcal{L}_\text{MolPO}^\prime + \mathcal{L}_\text{SFT}.
\end{align}

% Property Prediction Table
\begin{table*}[ht!]
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on molecule property tasks on MoleculeNet~\cite{Wu2017MoleculeNetAB} dataset. RMSE for Lipo and Esol, MAE for Homo, Lumo, Gap, and ROC-AUC for molecule classification tasks. * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ll|cccccccccc@{}}
\toprule 
& MODEL & Lipo ($\downarrow$) & Esol ($\downarrow$) & Homo ($\downarrow$) & Lumo ($\downarrow$) & $\Delta \epsilon$($\downarrow$) & BACE ($\uparrow$) & BBBP ($\uparrow$) & Clintox ($\uparrow$) & HIV ($\uparrow$) & SIDER ($\uparrow$) \\
\midrule

& \underline{\textit{Specialist Models}} \\ [.2em]
& InstructMol & - & - & 0.0048 & 0.0050 & 0.0061 & 82.3 & 70.0 & - & 68.9 & -\\
& MolCA & - & - & - & - & - & 79.8 & 70.0 & \textbf{89.5} & - & 63.0 \\
\midrule

& \underline{\textit{Semi-Generalist Models}} \\[.2em]
& Mol-Instruction & - & - & 0.0210 & 0.0210 & 0.0203 & - & - & - & - & - \\
% Re-checked Performance by BioT5+ checkpoint, especially difference performance with paper performance comes from classification tasks (BACE, BBBP, Clintox, HIV)
& $\text{BioT5+}^*$ & - & - & \textbf{0.0022} & \textbf{0.0024} & \textbf{0.0028} & 81.1 & 68.9 & 83.7 & 67.0 & -  \\
\midrule

& \underline{\textit{Generalist Models}} \\[.2em]
& $\text{LlaSMol}^*$ & 1.01 & 1.21 & - & - & - & - & 82.4 & 77.5 & 70.3 & \underline{\textbf{78.4}} \\
& Mol-LLM (SELFIES) & 1.18 & 0.93 & 0.0039 & 0.0044 & 0.0039 & \underline{\textbf{87.1}} & 83.5 & \underline{86.9} & 77.4 & 76.1 \\
& Mol-LLM & \underline{\textbf{0.80}} & \underline{\textbf{0.90}} & \underline{0.0037} & \underline{0.0042} & \underline{0.0038} & 82.0 & \underline{\textbf{88.5}} & 81.1 & \underline{\textbf{80.0}} & 76.3 \\
\bottomrule
\end{tabular} }
\label{tab:property_prediction_main}
\end{table*}



% Reaction Prediction Table
\begin{table*}[h!]
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on reaction prediction task on Mol-Instruction~\cite{Fang2023MolInstructionsAL} and SMolInstruct \cite{Yu2024LlaSMolAL} datasets.
FS, RS, RP each represent Forward synthesis, Retrosynthesis, and Reagent prediction.
Because SMolInstruct dataset does not include reagent prediction data, we only evaluated FS and RS in SMolInstruct dataset. * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\resizebox{\textwidth}{!} {
\begin{tabular}{@{}l|l|l|cccccccc@{}}
\toprule 
& & MODEL & EXACT ($\uparrow$) & BLEU ($\uparrow$) & RDK FTS ($\uparrow$) & MACCS FTS ($\uparrow$) & MORGAN FTS ($\uparrow$) & VALIDITY ($\uparrow$) \\
\midrule

\multirow{15}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{FS}} }
& \multirow{8}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{Mol-Instruction}} }
& \underline{\textit{Specialist Models}} \\ [.2em]
& & InstructMol & 0.536 & 0.967 & 0.776 & 0.878 & 0.741 & 1.000 \\
& & \underline{\textit{Semi-Generalist Models}} \\
& & Mol-Instruction & 0.045 & 0.654 & 0.313 & 0.509 & 0.262 & 1.000 \\
& & $\text{BioT5+}^*$ & 0.864 & \textbf{0.993} & 0.949 & 0.975 & 0.935 & 1.000 \\
& & \underline{\textit{Generalist Models}} \\[.2em]
& & $\text{LlaSMol}^*$ & 0.743 & 0.835 & 0.920 & 0.955 & 0.910 & 0.953 \\
& & Mol-LLM (SELFIES) & 0.928 & 0.975 & 0.981 & \underline{\textbf{0.990}} & 0.974 & 1.000 \\
& & Mol-LLM & \underline{\textbf{0.936}} & \underline{0.978} & \underline{\textbf{0.983}} & \underline{\textbf{0.990}} & \underline{\textbf{0.977}} & 1.000 \\
\cmidrule{2-9}


& \multirow{5}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{SMol.}} }
& \underline{\textit{Semi-Generalist Models}} \\[.2em]
& & $\text{BioT5+}^*$ & 0.081 & 0.455 & 0.418 & 0.537 & 0.376 & 1.000\\
& & \underline{\textit{Generalist Models}} \\[.2em]
& & $\text{LlaSMol}^*$ & \underline{\textbf{0.629}} & \underline{\textbf{0.883}} & \underline{\textbf{0.871}} & \underline{\textbf{0.919}} & \underline{\textbf{0.848}} & 0.998 \\
& & Mol-LLM & 0.607 & 0.874 & 0.858 & 0.911 & 0.827 & 1.000 \\
\midrule



\multirow{15}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{RS}} }
& \multirow{8}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{Mol-Instruction}} }
& \underline{\textit{Specialist Models}} \\ [.2em]
& & InstructMol & 0.407 & 0.941 & 0.753 & 0.852 & 0.714 & 1.000 \\
& & \underline{\textit{Semi-Generalist Models}} \\
& & Mol-Instruction & 0.009 & 0.705 & 0.283 & 0.487 & 0.230 & 1.000 \\
& & $\text{BioT5+}^*$ & \textbf{0.642} & \textbf{0.969} & \textbf{0.897} & \textbf{0.930} & \textbf{0.866} & 1.000 \\
& & \underline{\textit{Generalist Models}} \\[.2em]
& & $\text{LlaSMol}^*$ & 0.453 & 0.722 & 0.826 & 0.885 & 0.788 & 0.954 \\
& & Mol-LLM (SELFIES) & 0.558 & \underline{0.862} & \underline{0.863} & \underline{0.905} & \underline{0.828} & 1.000 \\
& & Mol-LLM & \underline{0.564} & 0.857 & 0.855 & 0.898 & 0.823 & 1.000 \\
\cmidrule{2-9}

& \multirow{5}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{SMol.}} }
& \underline{\textit{Semi-Generalist Models}} \\[.2em]
& & $\text{BioT5+}^*$ & 0.152 & 0.662 & 0.623 & 0.751 & 0.567 & 1.000 \\
& & \underline{\textit{Generalist Models}} \\[.2em]
& & $\text{LlaSMol}^*$ & 0.323 & 0.759 & 0.749 & 0.827 & 0.699 & 0.997 \\
& &  Mol-LLM & \underline{\textbf{0.377}} & \underline{\textbf{0.779}} & \underline{\textbf{0.756}} & \underline{\textbf{0.832}} & \underline{\textbf{0.707}} & 1.000 \\

\midrule

\multirow{8}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{RP}} }
& \multirow{8}{*}{\centering \raisebox{-0.5ex} {\rotatebox{90}{Mol-Instruction}} }
& \underline{\textit{Specialist Models}} \\ [.2em]
& & InstructMol & 0.129 & 0.610 & 0.444 & 0.539 & 0.400 & 1.000 \\
& & \underline{\textit{Semi-Generalist Models}} \\
& & Mol-Instruction & 0.044 & 0.224 & 0.237 & 0.364 & 0.213 & 1.000 \\
& & $\text{BioT5+}^*$ & \textbf{0.257} & \textbf{0.695} & \textbf{0.539} & \textbf{0.621} & \textbf{0.512} & 1.000 \\
& & \underline{\textit{Generalist Models}} \\[.2em]
& & Mol-LLM (SELFIES) & 0.223 & \underline{0.589} & \underline{0.515} & 0.600 & 0.479 & 1.000 \\
& & Mol-LLM & \underline{0.232} & 0.573 & \underline{0.515} & \underline{0.606} & \underline{0.486} & 1.000 \\


\bottomrule


\end{tabular} }
\label{tab:reaction_prediction_main}
\end{table*}


% Text2Mol Table
\begin{table*}[h!]
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on description guided molecule generation task on ChEBI-20~\cite{Edwards2022TranslationBM} datasets. * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}ll|ccccccc@{}}
\toprule 
& MODEL & EXACT ($\uparrow$) & BLEU ($\uparrow$) & RDK FTS ($\uparrow$) & MACCS FTS ($\uparrow$) & MORGAN FTS ($\uparrow$) & VALIDITY ($\uparrow$) \\
\midrule

& \underline{\textit{Semi-Generalist Models}} \\[.2em]
& $\text{BioT5+}^*$ & \textbf{0.557} & \textbf{0.931} & 0.835 & 0.907 & \textbf{0.779} & 1.000 \\
& \underline{\textit{Generalist Models}} \\[.2em]
& $\text{LlaSMol}^*$ & 0.274 & 0.644 & 0.755 & 0.871 & 0.679 & 0.948 \\
& Mol-LLM (SELFIES) & \underline{0.460} & \underline{0.815} & 0.835 & 0.911 & 0.773 & 1.000 \\
& Mol-LLM & \underline{0.460} & 0.800 & \underline{\textbf{0.841}} & \underline{\textbf{0.912}} & \underline{0.775} & 1.000 \\
\bottomrule

\end{tabular} }
\label{tab:text2mol_main}
\end{table*}


% Mol2Text Table
\begin{table*}[h!]
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on molecule description generation task on ChEBI-20~\cite{Edwards2022TranslationBM} datasets.  * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\begin{tabular}{@{}ll|cccccc@{}}
\toprule 
& MODEL & BLEU-2 ($\uparrow$) & BLEU-4 ($\uparrow$) & ROUGE-1 ($\uparrow$) & ROGUE-2 ($\uparrow$) & ROUGE-L ($\uparrow$) & METEOR ($\uparrow$) \\
\midrule

& \underline{\textit{Specialist Models}} \\ [.2em]
& InstructMol & 0.475 & 0.371 & 0.566 & 0.394 & 0.502 & 0.509 \\
& MolCA & 0.620 & 0.531 & 0.681 & 0.537 & 0.618 & 0.651 \\
\midrule

& \underline{\textit{Semi-Generalist Models}} \\[.2em]
%& Mol-Instruction & - & - & - & - & - & - \\
& $\text{BioT5+}^*$ & \textbf{0.666} & \textbf{0.591} & \textbf{0.710} & \textbf{0.584} & \textbf{0.650} & \textbf{0.681} \\
\midrule

& \underline{\textit{Generalist Models}} \\[.2em]
& $\text{LlaSMol}^*$ & 0.432 & 0.333 & 0.522 & 0.356 & 0.464 & 0.466 \\
& Mol-LLM (SELFIES) & \underline{0.587} & \underline{0.515} & \underline{0.627} & \underline{0.487} & \underline{0.571} & \underline{0.617} \\
& Mol-LLM & 0.560 & 0.490 & 0.524 & 0.370 & 0.467 & 0.593 \\
\bottomrule
\end{tabular}
\label{tab:mol2text_main}
\end{table*}




\section{Experiments}
\label{sec:experiments}

We constructed an instruction dataset comprising 7 tasks and 16 sub-tasks. 
To examine the impact of molecule SELFIES and molecule graphs on model training, we conducted experiments under three conditions: learning molecular representation from 1D SELFIES, 2D graph each, and from both of them.
To prevent alterations in graph information, we kept the graph encoder frozen. To align the LLM with the q-former embeddings, we kept the LLM frozen during the first half of the training process and then trained the LLM using a LoRA for the second half. 
We employed beam search with a beam width of 5 as the sampling method to generate high-quality texts, without applying repetition penalty. 
Additional details and ablation studies for generation can be found in the Appendix.
All experiments are run with 8$\times$ NVIDIA A100 (80GB) GPUs.


\subsection{Evaluation Metrics}
We used the following metrics that are commonly employed in previous studies: Exact Match, the proportion of tokens where the model's predicted results exactly match the ground truth labels, BLEU, ROUGE, LEVENSHTEIN, and METEOR, which measures the quality of predicted results based on ground truth labels, RDK/MACCS/MORGAN FTS, measures the structural similarity of molecule, Validity, the ratio of valid molecules follows SELFIES rules.

\subsection{Baselines}

The baseline models were divided into three categories for performance comparison. The first category consists of specialist models, where each task requires its own trained model. The second category includes semi-generalist models, where similar tasks are combined and trained as groups. The third category contains generalist models, where a single model handles all tasks. Among the generalist models, LlaSMol differs from our proposed model by excluding two challenging components: the QM9 dataset tasks, which represent a significant portion of the data and complicate multi-task learning, and the reagent prediction task, which is considered the most complex among reaction predictions due to its requirement to process three molecules simultaneously. This section analyzes and compares the performance across these three baseline model categories.

\subsection{Property Prediction}
% molecular property prediction은 materials discovery, drug discovery 등 실세계에서 impact 가 큰 application들과 연걸되어 있다. \citet{llasmol}, \citet{biot+}를 따라, 우리는 Moleculenet의 classification, regression task들에서 property prediction 성능을 평가했다.
% Classification은 BACE, BBBP, Clintox, HIV, SIDER dataset에 대해 각 dataset이 다루는 물성에 대한 "True", "False"를 예측을 Mol-LLM을 통해 수행하며, 이때 해당 token의 probabilty를 사용하여 ROC-AUC를 측정한다.
% 실험 결과는 generalist model인 LlaSMol, Mol-LLM이 specialist, semi-specialist model들보다 property predictgion에 뛰어난 것으로 확인되었으며, 이는 다양한 molecular task에서의 학습을 통해 얻는 chemical knowledge가 toxicity 예측 등에도 transfer되는 효과가 있음을 나타낸다. Mol-LLM안의 비교에서는 5개 중 3개 task에서 molecular graph를 활용함으로써 성능이 개선되는 것을 확인했다.
Molecular property prediction is connected to real-world applications with significant impact, such as materials discovery and drug discovery. Following \citet{Yu2024LlaSMolAL} and \citet{Pei2024BioT5TG}, we evaluated property prediction performance on classification and regression tasks from MoleculeNet \cite{Wu2017MoleculeNetAB}.

For classification tasks, we used Mol-LLM to predict $``\text{True}"$ or $``\text{False}"$ for properties across BACE, BBBP, Clintox, HIV, and SIDER datasets, measuring ROC-AUC using the probability of these tokens.
The experimental results in \Cref{tab:property_prediction_main} showed that generalist models like LlaSMol and Mol-LLM outperform specialist and semi-specialist models in property prediction, indicating that chemical knowledge acquired through learning various molecular tasks transfers effectively to tasks like toxicity prediction. In comparisons within Mol-LLM variants, we observed performance improvements in three out of five tasks when utilizing molecular graphs.

In regression tasks, we evaluated Mol-LLM with other models in Lipophilicity, Esol, QM9 datasets.
In these tasks, Mol-LLM predicts numerical value via next token prediction with additionally added number tokens $`` |<i>| ", i \in [0, 9]$.
% Regression task에서는 모든 데이터셋에 걸쳐 molecular graph utilization을 통해 Mol-LLM의 성능이 향상되는 것을 확인할 수 있다. 그중 특히 Lipophilicity dataset에서는 graph utilization을 통해 MAE가 33.3% 가량 개선되며, 나머지 데이터셋들에서는 비교적 소폭 감소하여 예측하는 물성마다 graph utilization의 효과가 상이할 수 있지만, 전반적으로는 도움이 된다는 것을 알 수 있다. Esol dataset에서는 graph utilization 없이도 이미 llasmol과 큰 성능 격차가 있어, graph utilization과 별개로 diverse task를 학습하는 것이 큰 도움이 된다는 것으로 보인다. QM9에서는 MolPO 학습 적용한 모델과 적용하지 않은 모델 모두 BioT5+ 다음으로 molecular LLM들 중 좋은 성능을 기록하며, 그 와중에 MolPO를 적용하여 추가적인 MAE 성능 개선을 이끌어냈다.
For regression tasks, we observed performance improvements in Mol-LLM across all datasets through molecular graph utilization. Notably, the Lipophilicity dataset showed a 33.3\% reduction in MAE by applying MolPO, while other datasets showed relatively smaller improvements, indicating that the effect of graph utilization may vary depending on the predicted property, but is generally beneficial. 
In the Esol dataset, Mol-LLM showed significant performance advantages over LLaSMol even without graph utilization, suggesting that learning diverse tasks might be largely beneficial independently of graph utilization. For QM9, both models with and without MolPO training achieved the second-best performance among molecular LLMs after BioT5+, with MolPO application leading to additional MAE performance improvements.
All results are shown in Table~\ref{tab:property_prediction_main}.

\subsection{Reaction Prediction}

% 

Unlike Mol-Instruction, which covers a broad scope of the biomolecular domain, SMolInstruct focuses exclusively on small molecules while incorporating more complex and diverse molecular structures. We compared the models' performance in reaction prediction tasks on both Mol-Instruction and SMolInstrut to evaluate each model in various molecule distribution. Since SMolInstruct does not include the reagent prediction task, we evaluated the model's performance on the reagent prediction task solely using Mol-Instruction.
BioT5+ performs comparably to Mol-LLM on the Mol-Instruction dataset but shows decreased performance on the SMol dataset.
Conversely, LlaSMol achieves state-of-the-art (SOTA) performance on the SMolInstruct dataset but exhibits a performance gap compared to BioT5+ and Mol-LLM on the Mol-Instruction dataset. Mol-LLM achieves SOTA performance on the Mol-Instruction dataset and performs comparably to SOTA on the SMolInstruct dataset.
All results are shown in Table~\ref{tab:reaction_prediction_main}.

\paragraph{Molecular graph structure understanding}
% bio5가 Mol-instruction에서는 mol-llm과 comparable하게 성능이 좋지만 smol에서는 성능이 안좋아졌고, 반대로 llasmol은 smol에서는 sota이지만 mol-instruction에서는 biot5, mol-llm과 성능 차이가 있다. mol-llm은 mol-instruction에서는 sota, smol에서는 sota comparable을 달성했고, 이 차이점을 알기 위새 MolPO 학습을 통한 molecular structure understanding의 양상을 reaction prediciton task에서 분석했다. 이를 위해 positive와 negative 에 대해 모델이 각각 부여하는 probability를 계산하여 어느 쪽을 더 선호하는지 확인했고, 그 결과 MolPO 학습을 하지 않은 경우 positive와 negative에 대해 구분을 거의 하지 못하는 반면 MolPO학습을 하는 경우 positive를 주로 선호하는 것을 확인했다.

To understand this difference, we analyzed the molecular structure understanding patterns learned through MolPO training in the reaction prediction task in Mol-Instruction and SMolInstruct datasets.
Specifically, we calculated the probabilities assigned by the model to chosen and rejected cases to determine its preference.
The results showed that, without MolPO training, the model could hardly distinguish between chosen and rejected cases.
However, with MolPO objective, the model predominantly preferred chosen cases, which demonstrated the effectiveness of MolPO in molecular structure understanding, which enhance task generalization on diverse molecular structure. The results are shown in Figure~\ref{fig:preference_accuracy}.

\begin{figure}[t!]\centering
    \includegraphics[width=.35\textwidth]{figures/fig3_preference_accuracy.png}
    \caption{Preference accuracy between chosen and rejected pairs on reaction prediction tasks (FS, RS, RP), with and without MolPO training.}
    \label{fig:preference_accuracy}
    \vspace{-1.0cm}
\end{figure}
%\paragraph{Molecular graph structure understanding}
% add figure



\subsection{Description Guided Molecule Generation}
Table~\ref{tab:text2mol_main} presents the results of description-guided molecule generation.
The performance is lower compared to the semi-generalist model, BioT5+, which is likely due to BioT5+'s ability to process IUPAC names, allowing it to better understand the information embedded in the molecular descriptions. Nevertheless, it outperforms LlaSMol, which is also a generalist model, by a significant margin.


\subsection{Molecule Captioning}
Table~\ref{tab:mol2text_main} presents the results of Molecule Captioning.
The performance of Mol-LLM is lower than that of the specialist model (MolCA) and the semi-generalist model (BioT5+), and slightly lower than Mol-LLM, which solely uses SELFIES.
This is likely because molecule descriptions are composed of text that does not particularly require graph structural information.
As a result, 1D SELFIES alone appears to be sufficient for text generation.
Nevertheless, Mol-LLM outperforms the generalist model, LlaSMol, across all metrics.

 
\section{Conclusion}
\label{sec:conclusion}
In this study, we proposed a generalist molecular LLM, Mol-LLM, and introduced MolPO, a training objective designed to enhance the model's understanding of molecular structures.
MolPO enables the model to better distinguish between correct and incorrect molecular structures.
As a result, Mol-LLM achieved state-of-the-art performance among generalist molecular LLMs in most molecule-centric tasks and demonstrated performance comparable to or surpassing specialized molecular LLMs.
Furthermore, Mol-LLM exhibited strong generalization capabilities in reaction prediction tasks, which is attributed to excellent molecular structure understanding identified by preference pair accuracy.
We believe that Mol-LLM can be utilized in real-world applications such as drug discovery and new material discovery in the future.


\section*{Impact Statement}
This paper aims to advance the field of AI for Science; however, there is a possibility that increased training and usage of larger LLMs in the future could lead to higher carbon emissions.
Nevertheless, this is not a concern at present, nor does it raise any ethical issues.



\bibliography{ICML2025/reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
%\onecolumn

%\section{Effect of MolPO in Validation Data}

% About how to make negative case molecule graphs
\section{Generation Methods of Rejected molecule for MolPO}
\label{appx:gen_rejectged}
To enhance the model's understanding of molecule graphs, we generated negative cases by augmenting the original molecule graphs included in our training dataset, ensuring that the model assigns them lower preferences. Our augmentation methods can be categorized into two approaches: the Random Atom Exchange and the MACCS Key-based Property Exchange.  

The Random Atom Exchange method augments molecular structures by modifying specific atoms at random positions within a molecule. This is achieved by replacing an existing atom with a different atom, removing the atom, or connecting a new arbitrary atom at that position. Through this approach, molecules with different properties can be generated while maintaining the overall number of constituent atoms. Additionally, by adding or removing multiple atoms, the overall size of the molecule can be increased or decreased. This method enables the generation of virtual molecules with altered properties without significantly disrupting the overall bond connectivity in the molecular graph. Consequently, it aids LLMs in developing a deeper understanding of how the connectivity between specific atoms influences molecular properties.

The MACCS Key-based Property Exchange method directly modifies molecular substructures associated with specific properties by selectively removing and adding them. This approach first identifies the substructures of the molecule corresponding to MACCS keys, generating two lists: one containing the MACCS keys representing properties present in the molecule, and the other containing the keys for properties absent in the molecule.
For augmentation, a random key is selected from the list of present MACCS keys, and the corresponding substructure is removed from the original molecular graph. Subsequently, a random key is chosen from the list of absent MACCS keys, and a substructure corresponding to this property is generated and attached at a random position in the molecule. This method creates molecules with different properties through substructure-level enhancement, facilitating the understanding of molecular properties by strengthening its ability to recognize and interpret substructure patterns.

\section{Training Hyperparameter}

Table~\ref{tab:training_hyperparamter} represents hyperparameters of Mol-LLM during the training phase of SFT and MolPO. We use 8$\times$ NVIDIA A100 (80GB) GPUs. We use greedy decoding for token generation.

% Training Hyperparameter table
\begin{table}[h!]
% \footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Details of Mol-LLM training hyperparamter.
}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}llcc@{}}
\toprule 
& Hyperparameter & SFT & MolPO  \\
\midrule
& Base LLM & Mistral 7B & Mistral 7B \\
& Training Param & LoRA, Q-Former & LoRA, Q-Former, GNN \\
& CodeBook Size & 35K & 35K \\
& LR & 1e-4 & 2e-5\\
& Epoch & 12 & 5 \\
& LoRA $$r$$ & 64 & 64\\
& LoRA Layers & QKV & QKV \\
\midrule

\end{tabular}
}
\label{tab:training_hyperparamter}
\end{table}

\section{Experiments in SMolInstruct Dataset}

Table~\ref{tab:text2mol_smol} and table~\ref{tab:mol2text_smol} present the results of description-guided molecule generation and Molecule Captioning in SMolInstruct test set.

% Text2Mol Table
\begin{table*}
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on description-guided molecule generation task on SMolInstruct dataset \cite{Yu2024LlaSMolAL}. * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|cccccccc@{}}
\toprule 
MODEL & EXACT ($\uparrow$) & BLEU ($\uparrow$) & RDK FTS ($\uparrow$) & MACCS FTS ($\uparrow$) & MORGAN FTS ($\uparrow$) & VALIDITY ($\uparrow$) \\
\midrule

\underline{\textit{Semi-Generalist Models}} \\[.2em]
$\text{BioT5+}^*$ & \textbf{0.519} & \textbf{0.918} & \textbf{0.822} & \textbf{0.897} & \textbf{0.757} & 1.000 \\
\underline{\textit{Generalist Models}} \\[.2em]
$\text{LlaSMol}^*$ & 0.180 & 0.718 & 0.712 & 0.845 & 0.623 & 0.929 \\
Mol-LLM & \underline{0.383} & \underline{0.766} & \underline{0.810} & \underline{0.895} & \underline{0.734} & 0.999 \\
\bottomrule

\end{tabular} }
\label{tab:text2mol_smol}
\end{table*}


% Mol2Text Table
\begin{table*}[t]
\footnotesize\centering\setlength{\tabcolsep}{4.4pt}
\caption{
Performance comparison on molecule description generation task on SMolInstruct dataset \cite{Yu2024LlaSMolAL}. * denotes performance evaluated based on the official checkpoint, bold denotes the best performance overall, and underline denotes the best performance of the generalist.
}
\begin{tabular}{@{}ll|cccccc@{}}
\toprule 
& MODEL & BLEU-2 ($\uparrow$) & BLEU-4 ($\uparrow$) & ROUGE-1 ($\uparrow$) & ROGUE-2 ($\uparrow$) & ROUGE-L ($\uparrow$) & METEOR ($\uparrow$) \\
\midrule

& \underline{\textit{Semi-Generalist Models}} \\[.2em]
%& Mol-Instruction & - & - & - & - & - & - \\
& $\text{BioT5+}^*$ & \textbf{0.651} & \textbf{0.577} & \textbf{0.686} & \textbf{0.562} & \textbf{0.629} & \textbf{0.662} \\
\midrule

& \underline{\textit{Generalist Models}} \\[.2em]
& $\text{LlaSMol}^*$ & 0.427 & 0.328 & \underline{0.525} & \underline{0.359} & \underline{0.465} & 0.470 \\
& Mol-LLM & \underline{0.556} & \underline{0.483} & 0.500 & 0.342 & 0.445 & \underline{0.588} \\
\bottomrule
\end{tabular}
\label{tab:mol2text_smol}
\end{table*}

\newpage
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
