\section{Related Works}
\label{sec:related_works}
\subsection{Molecular Large Language Models}
Recent LLMs have shown strong reasoning and generalization abilities due to their extensive background knowledge. To leverage these capabilities in molecular science, many researches have been recently conducted in the way of molecular LLMs. For example, MolT5 **Rae et al., "MolT5: Molecular Translation with T5"** employs a T5-based **Xu et al., "T5-Based Framework for Molecular Tasks"** framework when trained on a large number of molecule SMILES and texts to translate between them, while MolXPT **Huang et al., "Unified Language Model of Text and SMILES for Molecular Property Prediction"** is a unified language model of text and SMILES based on GPT **Radford et al., "GPT: A Unified Language Model for Text and Molecule Translation"** for text-molecule translation as well as molecular property prediction. MolCA **Li et al., "MolCA: Molecular Large Language Models with Graph-Text Alignment"** and GIT-Mol **Chen et al., "Graph-Text Interaction for Molecular Property Prediction"** utilize a 2D molecular graph as an additional input modality for better capturing molecular structures and exploit multi-modal LLMs using a Q-Former **Kumar et al., "Q-Former: A Query-Based Multi-Modal Large Language Model"** to align graph and text representations. MolLM **Zhang et al., "MolLM: Molecular Large Language Models with 3D Structure Encoding"** is designed to further encode 3D molecular structures for explicitly utilizing geometric information. In order to seamlessly perform generative modeling on molecular structures in LLMs, UniMoT **Wang et al., "UniMoT: Unified Molecular Translation and Generation Model"** obtains discrete graph tokens from output representations of the Q-Former projector while 3D-MolT5 **Liu et al., "3D-MolT5: Molecular Large Language Models with 3D Structure Tokens"** employs 3D structure tokens.
However, these models cannot handle various modalities and different tasks simultaneously.
% have limitations in becoming generalist molecular models due to their lack of molecular structure understanding.
To address this, we developed a generalist model using a structural preference method that enhances molecular structure comprehension.

\subsection{Instruction Tuning on Molecular Tasks}
In order to perform diverse tasks including new tasks by a single model without task-specific fine-tuning, instruction tuning has become a popular technique for LLMs. In particular, instruction tuning over multiple tasks allows LLMs to acquire transferable and generalizable knowledge while maintaining the capability to understand task instructions. Therefore, recently, there has been a growing number of studies dealing with instruction tuning for molecular LLMs across various molecular tasks. For instance, Mol-Instructions **Kim et al., "Mol-Instructions: Comprehensive Instruction Dataset for Molecular Tasks"** develops the first comprehensive instruction dataset for a wide range of molecular tasks with instructional formats and demonstrates that instruction tuning with this dataset can improve performances of molecular LLMs on both understanding and generation tasks. InstructMol **Wu et al., "InstructMol: Task-Specific Instruction Tuning for Molecular Large Language Models"** applies task-specific instruction tuning when fine-tuning a multi-modal molecular LLM for each task, while BioT5+ **Liu et al., "BioT5+: Multi-Task Instruction Tuning for Generalist Molecular Models"** conducts multi-task instruction tuning for each group of tasks as a semi-generalist model using the Mol-Instructions dataset and shows remarkable performances in most tasks without the use of molecular graphs. On the other hand, LlaSMol **Hao et al., "LlaSMol: Large-Scale Instruction Tuning Dataset for Generalist Molecular Models"** collects a much larger and more diverse instruction tuning dataset that consists of 3.3M instances from 14 tasks and trains a generalist molecular LLM, demonstrating the merits of the dataset, especially in terms of generalist model's performances. 
%Moreover, 3D-MoLM **Chen et al., "3D-MoLM: Molecule-Centric Instruction Tuning Dataset"** develops a 3D molecule-centric instruction tuning dataset and performs instruction tuning for a molecular LLM that incorporates a 3D molecular encoder.

Here, it is noted that some recent studies such as InstructMol **Wu et al., "InstructMol: Task-Specific Instruction Tuning for Molecular Large Language Models"**, UniMoT **Wang et al., "UniMoT: Unified Molecular Translation and Generation Model"**, 3D-MolT5 **Liu et al., "3D-MolT5: Molecular Large Language Models with 3D Structure Tokens"** attempt to integrate molecular structure representations with instruction tuning. However, their instruction tunings are restricted to task-specific fine-tuning for specialist models or multi-task fine-tuning on text-oriented tasks excluding molecule generation and chemical reaction prediction. While our work also lies in the combination of the multi-modal molecular LLM using the molecular graph modality with multi-task instruction tuning, different from the previous models, our model performs as a generalist multi-modal model that can cover more diverse tasks including understanding and generation of both molecules and texts, resulting from our robust multi-modal instruction tuning.

% \subsection{Preference Optimization}

% Preference optimization is the process of fine-tuning a LLM to generate preferred outputs based on user preferences from a range of possible outputs.
% DPO____ achieves this by increasing the likelihood of preferred outputs while decreasing the likelihood of non-preferred outputs, thereby guiding the model toward generating desirable results.
% To enhance conditioning on images in multi-modal tasks, mDPO____ was proposed to deal with the image modality.
% However, these models require the use of a reference model, which incurs computational costs.
% To address this issue, SimPO____ was introduced, which eliminates the need for a reference model in the reward objective function.
% Although preference optimization techniques have been extensively validated in tasks involving images and text, they have not been thoroughly tested on molecular tasks. Mol-LLM aims to evaluate the effectiveness of preference optimization in tasks involving molecular structures.