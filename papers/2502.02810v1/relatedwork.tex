\section{Related Works}
\label{sec:related_works}
\subsection{Molecular Large Language Models}
Recent LLMs have shown strong reasoning and generalization abilities due to their extensive background knowledge. To leverage these capabilities in molecular science, many researches have been recently conducted in the way of molecular LLMs. For example, MolT5 \cite{Edwards2022TranslationBM} employs a T5-based \cite{raffel2023exploringlimitstransferlearning} framework when trained on a large number of molecule SMILES and texts to translate between them, while MolXPT \cite{molxpt} is a unified language model of text and SMILES based on GPT \cite{radford2019language} for text-molecule translation as well as molecular property prediction. MolCA \cite{Liu2023MolCAMG} and GIT-Mol \cite{Liu_2024} utilize a 2D molecular graph as an additional input modality for better capturing molecular structures and exploit multi-modal LLMs using a Q-Former \cite{li2023blip2bootstrappinglanguageimagepretraining} to align graph and text representations. MolLM \cite{mollm} is designed to further encode 3D molecular structures for explicitly utilizing geometric information. In order to seamlessly perform generative modeling on molecular structures in LLMs, UniMoT \cite{zhang2024unimotunifiedmoleculetextlanguage} obtains discrete graph tokens from output representations of the Q-Former projector while 3D-MolT5 \cite{pei20243dmolt5unified3dmoleculetext} employs 3D structure tokens.
However, these models cannot handle various modalities and different tasks simultaneously.
% have limitations in becoming generalist molecular models due to their lack of molecular structure understanding.
To address this, we developed a generalist model using a structural preference method that enhances molecular structure comprehension.

\subsection{Instruction Tuning on Molecular Tasks}
In order to perform diverse tasks including new tasks by a single model without task-specific fine-tuning, instruction tuning has become a popular technique for LLMs. In particular, instruction tuning over multiple tasks allows LLMs to acquire transferable and generalizable knowledge while maintaining the capability to understand task instructions. Therefore, recently, there has been a growing number of studies dealing with instruction tuning for molecular LLMs across various molecular tasks. For instance, Mol-Instructions \cite{Fang2023MolInstructionsAL} develops the first comprehensive instruction dataset for a wide range of molecular tasks with instructional formats and demonstrates that instruction tuning with this dataset can improve performances of molecular LLMs on both understanding and generation tasks. InstructMol \cite{Cao2023InstructMolMI} applies task-specific instruction tuning when fine-tuning a multi-modal molecular LLM for each task, while BioT5+ \cite{Pei2024BioT5TG} conducts multi-task instruction tuning for each group of tasks as a semi-generalist model using the Mol-Instructions dataset and shows remarkable performances in most tasks without the use of molecular graphs. On the other hand, LlaSMol \cite{Yu2024LlaSMolAL} collects a much larger and more diverse instruction tuning dataset that consists of 3.3M instances from 14 tasks and trains a generalist molecular LLM, demonstrating the merits of the dataset, especially in terms of generalist model's performances. 
%Moreover, 3D-MoLM \cite{li20243dmoleculetextinterpretationlanguage} develops a 3D molecule-centric instruction tuning dataset and performs instruction tuning for a molecular LLM that incorporates a 3D molecular encoder.

Here, it is noted that some recent studies such as InstructMol \cite{Cao2023InstructMolMI}, UniMoT \cite{zhang2024unimotunifiedmoleculetextlanguage}, 3D-MolT5 \cite{pei20243dmolt5unified3dmoleculetext} attempt to integrate molecular structure representations with instruction tuning. However, their instruction tunings are restricted to task-specific fine-tuning for specialist models or multi-task fine-tuning on text-oriented tasks excluding molecule generation and chemical reaction prediction. While our work also lies in the combination of the multi-modal molecular LLM using the molecular graph modality with multi-task instruction tuning, different from the previous models, our model performs as a generalist multi-modal model that can cover more diverse tasks including understanding and generation of both molecules and texts, resulting from our robust multi-modal instruction tuning.

% \subsection{Preference Optimization}

% Preference optimization is the process of fine-tuning a LLM to generate preferred outputs based on user preferences from a range of possible outputs.
% DPO~\citep{rafailov2023dpo} achieves this by increasing the likelihood of preferred outputs while decreasing the likelihood of non-preferred outputs, thereby guiding the model toward generating desirable results.
% To enhance conditioning on images in multi-modal tasks, mDPO~\citep{Wang2024mDPOCP} was proposed to deal with the image modality.
% However, these models require the use of a reference model, which incurs computational costs.
% To address this issue, SimPO~\citep{Meng2024SimPOSP} was introduced, which eliminates the need for a reference model in the reward objective function.
% Although preference optimization techniques have been extensively validated in tasks involving images and text, they have not been thoroughly tested on molecular tasks. Mol-LLM aims to evaluate the effectiveness of preference optimization in tasks involving molecular structures.