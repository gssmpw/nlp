\section{Related Works}
\subsection{Towards Long Context Window}
With increasing demand for long-text processing, currently leading LLMs are trending towards longer context window lengths in order to enhance their capabilities for handling long texts. For example, **Vaswani et al., "Attention Is All You Need"** and **Tang et al., "Deep Contextualized Word Representations"** can handle up to 128K, while **Li et al., "ChatGLM: A General-Purpose Large Language Model"** and **Gao et al., "InternLM2: A Highly Scalable Large Language Model"** can process up to 1M tokens. These examples reflect the importance of extending context windows. The above foundation LLMs are primarily continued and phased training by adjusting the base frequency of the RoPE, which requires efficient design and substantial computational resources to support additional training on long texts.

Considering the substantial training resources, **Tang et al., "LongLoRA: Efficient Fine-Tuning for Long Context"** fine-tuned LLMs to handle long texts through the efficient LoRA method. SelfExtend constructed bi-level attention information by implementing the position encoding calculation without finetuning to extend context window. However, such training-free methods have limited improvement effects. TransformerFAM innovatively proposed a feedback attention mechanism as working memory to manage the past context in cache. Its operational principle is similar to that of KV Cache Compression methods.

\subsection{KV Cache Compression}
Considering both costs and improvement effects, researchers found that efficient manage of the KV Cache is crucial method to improve the ability of handling long texts, such as **Li et al., "H2O: Efficient In-Memory Caching for Long-Context"**, **Tang et al., "StreamingLLM: Streaming-Based Long-Context Processing"**, **Gao et al., "SnapKV: Snapshotted Key-Value Cache Compression"**, **Wu et al., "InfiniPot: Infinite Potential of KV Cache Compression"**, **Zhang et al., "DuoAttention: Dual Attention for Efficient Long Context"**, **Li et al., "PyramidKV: Pyramid-Based Key-Value Cache Compression"**, **Sun et al., "DynamicKV: Dynamic Key-Value Cache Management"**, and **Wang et al., "CAKE: Compact and Adaptive Key-Value Embeddings"**. Overall, the fundamental framework of these methods involves designing distinct strategies to retrain a subset of tokens in the cache, thereby reducing computational costs during inference.

Although these methods, including H2O, SnapKV, InfiniPot, PyramidKV and DynamicKV have been incrementally optimized and improved on prior work, their reliance on caching past key-value states fundamentally constrains their performance. No matter how these methods are designed, they cannot match the performance of full key-value (Full KV) caching at a low cost. Compared to Full KV, these methods can only achieve reduce memory and computational costs. In contrast, our method not only reduces costs but also outperforms Full KV in terms of effectiveness. The essence of this result is that the aforementioned methods have not sufficiently leveraged the inherent capabilities of LLMs to design strategies for caching tokens.

Specifically, methods such as H2O, InfiniPot, PyramidKV, and SnapKV all considered the allocation of attention scores as a feature in their method design. Starting from SnapKV, through PyramidKV to DynamicKV, there has been in-depth research proposing that the distribution of LLMâ€™s attention scores follows a specific pattern. SnapKV also emphasizes a viewpoint similar to ours, suggesting that "LLMs know what you are looking for before generation". However, since these methods all cache tokens at the granularity of individual token, they have not fully leveraged this phenomenon to achieve the desired improvements in long-context processing.