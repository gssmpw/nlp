[
  {
    "index": 0,
    "papers": [
      {
        "key": "command",
        "author": "Cohere",
        "title": "Command r: Retrieval-augmented generation at production scale"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cai2024internlm2",
        "author": "Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others",
        "title": "Internlm2 technical report"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "su2024roformer",
        "author": "Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng",
        "title": "Roformer: Enhanced transformer with rotary position embedding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiong2023effective",
        "author": "Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others",
        "title": "Effective long-context scaling of foundation models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2023longlora",
        "author": "Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya",
        "title": "Longlora: Efficient fine-tuning of long-context large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jin2024llm",
        "author": "Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia",
        "title": "Llm maybe longlm: Self-extend llm context window without tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hwang2024transformerfam",
        "author": "Hwang, Dongseong and Wang, Weiran and Huo, Zhuoyuan and Sim, Khe Chai and Mengibar, Pedro Moreno",
        "title": "TransformerFAM: Feedback attention is working memory"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2024snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snapkv: Llm knows what you are looking for before generation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kim2024infinipot",
        "author": "Kim, Minsoo and Shim, Kyuhong and Choi, Jungwook and Chang, Simyung",
        "title": "Infinipot: Infinite context processing on memory-constrained llms"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xiao2024duoattention",
        "author": "Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song",
        "title": "Duoattention: Efficient long-context llm inference with retrieval and streaming heads"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "cai2024pyramidkv",
        "author": "Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others",
        "title": "Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhou2024dynamickv",
        "author": "Zhou, Xiabin and Wang, Wenbin and Zeng, Minyan and Guo, Jiaxian and Liu, Xuebo and Shen, Li and Zhang, Min and Ding, Liang",
        "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "qin2025cake",
        "author": "Ziran Qin and Yuchen Cao and Mingbao Lin and Wen Hu and Shixuan Fan and Ke Cheng and Weiyao Lin and Jianguo Li",
        "title": "{CAKE}: Cascading and Adaptive {KV} Cache Eviction with Layer Preferences"
      }
    ]
  }
]