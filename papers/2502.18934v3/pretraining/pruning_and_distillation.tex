\subsubsection{Pruning and Distillation}
\label{subsec:pretrain_pd}


In opposition to efficiently up-scaling the model size, knowledge distillation is an effective method to efficiently down-scale the model size \citep{hinton2015knowledge-distillation, gunter2024apple, llama3.2}.
Leveraging the 8B model from Section \ref{subsec:pretrain_staged_pretrain}, we efficiently produce smaller models by improving the pruning and distillation of Minitron \citep{muralidharan2024compact, sreenivas2024llm}.
This process allows us to produce models with better performance at one-tenth of the data size compared to training from scratch, as shown in \autoref{tab:pd-vs-fs}.
We further show that iteratively extending the process beyond two iterations remains effective, preserving 87-99\% of KMMLU score at only 50\% of the model size, as shown in \autoref{tab:pd-iterative}.
Our models achieve competitive performance to recent open-source models \citep{allal2025smollm2smolgoesbig, llama3, gemma2024gemma2, qwen25techreport}, as presented in \autoref{tab:pd-base-all}.


\begin{table}[ht]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|c|cccccc|c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{\makecell{Training \\ Tokens}}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAERAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
    \midrule
    2.1B PD & 0.3T & 54.83 & 44.80 & 77.09 & 31.10 & 46.20 & 46.32 & 50.06 \\
    2.1B & 3T & 50.66 & 36.61 & 68.74 & 24.45 & 41.60 & 36.69 & 43.13 \\
    \bottomrule
    \end{tabular}%
    }
    \caption{
    Token consumption and performance of pruning \& distillation (PD) from preceding models and training from scratch. We use the same 2.1B architecture.
    }
    \label{tab:pd-vs-fs}
\end{table}


\begin{table}[ht]
    \centering
    \begin{tabular}{l|cccccc|c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAERAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} \\
    \midrule
    \tikzmark{t} 8B$^\dag$ & 64.22 & 48.30 & 83.41 & 40.24 & 51.40 & 57.09 & 57.44 \\
    \tikzmark{a} 4.5B & 59.74 & 48.09 & 82.58 & 34.76 & 48.60 & 57.01 & 55.13 \\
    \tikzmark{b} 2.1B & 54.83 & 44.80 & 77.09 & 31.10 & 46.20 & 46.32 & 50.06 \\
    \tikzmark{c} 1.3B & 53.55 & 39.91 & 72.59 & 28.05 & 39.60 & 36.01 & 44.95 \\
    \tikzmark{d} 635M & 46.28 & 34.60 & 62.69 & 23.17 & 31.40 & 19.26 & 36.23 \\
    \tikzmark{e} 385M & 41.16 & 31.70 & 47.94 & 18.90 & 24.00 & 10.83 & 29.08 \\
    \tikzmark{f} 192M & 26.11 & 30.16 & 19.71 & 12.80 & 12.40 & 2.43  & 17.27 \\
    \bottomrule
    \end{tabular}%
    \caption{Performance through iterative compression beyond two iterations. Each model is pruned from the preceding model. $^\dag$ Each model is distilled using the 8B model as the teacher.}
    \label{tab:pd-iterative}
    \begin{tikzpicture}[overlay, remember picture, shorten >=1pt, shorten <=1pt, transform canvas={yshift=.25\baselineskip}]
        \draw [-stealth] ({pic cs:t}) [bend right=50] to ({pic cs:a});
        \draw [-stealth] ({pic cs:a}) [bend right=50] to ({pic cs:b});
        \draw [-stealth] ({pic cs:b}) [bend right=50] to ({pic cs:c});
        \draw [-stealth] ({pic cs:c}) [bend right=50] to ({pic cs:d});
        \draw [-stealth] ({pic cs:d}) [bend right=50] to ({pic cs:e});
        \draw [-stealth] ({pic cs:e}) [bend right=50] to ({pic cs:f});
    \end{tikzpicture}
\end{table}


In order to improve the pruning and distillation process, we refine Minitron's width importance scoring while preserving its simplicity and efficiency.
Its scoring process begins by measuring the importance of embedding channels, feed-forward neurons, and attention heads using activations from a small calibration dataset.
Next, we show that summing layer-wise scores plays a crucial role in performance, whereas the prior work performed ablations along batch and sequence axes.
Moreover, for Grouped-Query Attention (GQA) \citep{ainslie2023gqa}, we improve performance by ensuring query-key-value alignment. Specifically, we remove an equal number of query heads within each group, as shown in \autoref{fig:pd-gqa}.
Additionally, since Kanana employs SwiGLU \citep{swiglu}, we choose between averaging gate and up states or using intermediate states, whereas the original formulation relies on pre-activation values.
All ablation results for importance scoring are in \autoref{tab:pd-score-detail}.


We further enhance the pruning strategies with a focus on intermediate model structures.
Consistent with the findings from Minitron, we observe that excessive single-step compression leads to significant degradation.
Although maintaining attention heads is generally beneficial, our experiments reveal that pruning them for smaller models is effective when done earlier at larger scales as presented in \autoref{tab:pd-attn-heads}.
Additionally, we find that input and output embeddings can be tied by averaging without causing noticeable degradation, which we apply when pruning from 4.5B to 2.1B as shown in \autoref{tab:pd-tie}.


Lastly, we observe that the composition of distillation data directly influences the performance, while pruning data is less important.
For models larger than 2B, we use high-quality 300 billion tokens of stage 2 described in Section \ref{subsec:pretrain_staged_pretrain}.
However, for smaller models, increasing the proportion of general-domain English data increases both English performance and other benchmark scores, as shown in \autoref{tab:pd-distill-data}.
