\subsubsection{Staged Pre-training from Scratch}
\label{subsec:pretrain_staged_pretrain}

\begin{figure}[h]
    \centering
    \subfloat[Stage 1 data]{
        \includegraphics[width=0.45\textwidth]{figures/pretraining/data-stage1.pdf}
        \label{fig:pre_stage1_data}
    }
    \hfill
    \subfloat[Stage 2 data]{
        \includegraphics[width=0.45\textwidth]{figures/pretraining/data-stage2.pdf}
        \label{fig:pre_stage2_data}
    }
    \caption{Kanana's staged pre-training data mixture.}
    \label{fig:pre_data_stats}
\end{figure}


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lc|cccccc|c}
\toprule
    \multirow{2}{*}{Models} & \multirow{2}{*}{Stage} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
\midrule
\multirow{2}{*}{26.8B} & Stage 1 & 73.38 & 54.26 & 84.97 & 32.32 & 47.20 & 57.77 & 58.32 \\
& Stage 2 & 74.27 & 59.04 & 88.45 & 51.22 & 61.60 & 67.48 & 67.01 \\
\midrule
\multirow{2}{*}{8B} & Stage 1 & 63.48 & 45.51 & 77.27 & 23.78 & 35.80 & 35.03 & 46.81 \\
& Stage 2 & 64.22 & 48.30 & 83.41 & 40.24 & 51.40 & 57.09 & 57.44 \\
\bottomrule
\end{tabular}
}
\caption{
Performance of from-scratch Kanana models at the end of each training stage.
} \label{table:from-scratch}
\end{table}

To maximize performance under fixed compute budget, we adopt the staged pre-training strategy \citep{minicpm, gemma, opencoder, yi-lightning, granite} with two stages.
Staged pre-training divides the pre-training process into multiple stages, starting with training LLMs on a large amount of moderate-quality data in the initial stages, and gradually increasing the proportion of high quality data in the subsequent stages. 

We begin by training 8B from scratch using the diverse 2.7 trillion in stage 1 as shown in Figure \autoref{fig:pre_stage1_data}.
In stage 2, we further train the model using 300 billion tokens shown in Figure \ref{fig:pre_stage2_data}. 
Specifically, we set aside high quality data for each category using the available high quality classifiers.
Then, we perform lightweight annealing experiments to select candidate datasets to search for the data mixture following \citet{llama3}.
Then, the optimal data mixture is selected through ablation study.
The final model of stage 2 results in a 2.79 point increase in KMMLU and a 10.63 point increase in average performance, demonstrating the effectiveness and efficiency of staged pre-training. 
We apply the same data mixture that was used during the training of 8B to 26.8B model. 
Direct application of the recipe consistently yields remarkable performance and stable training as shown in \autoref{table:from-scratch}, demonstrating the scalability of our recipe. See Appendix \ref{appendix:pretrain-details} for our pre-training configurations.
