\subsection{Training Process}
\label{subsec:pretrain_train_process}


To enhance computational efficiency in pre-training LLMs, we employ three key techniques: staged pre-training from scratch, depth up-scaling, and pruning and distillation.
In Section \ref{subsec:pretrain_staged_pretrain}, we first train 26.8B and 8B models using a staged pre-training approach, which serves as the foundation for obtaining LLMs at various scales.
In Section \ref{subsec:pretrain_dus}, we describe the process to obtaining \textit{Kanana Flag 32.5B} and \textit{Kanana Essence 9.8B} models by depth up-scaling from 26.8B and 8B models, respectively.
In Section \ref{subsec:pretrain_pd}, we derive \textit{Kanana Nano 2.1B} model through pruning and distillation from the 8B model, reducing training costs while achieving superior performance compared to training a model from scratch.


\input{pretraining/from_scratch_training}

\input{pretraining/dus}

\input{pretraining/pruning_and_distillation}

In conclusion, our comprehensive pre-training process, which includes staged pre-training, depth up-scaling, and iterative pruning and distillation, offers a compute-efficient strategy for developing high-performing language models. 
This combined approach not only enhances performance across diverse benchmarks, but also ensures computational efficiency, demonstrating the effectiveness of our strategy in producing a robust family of models spanning the range from 2.1B to 32.5B. See Appendix \ref{appendix:pd-details} for our pruning and distillation configurations.