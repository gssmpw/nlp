\subsection{Data}
\label{subsec:pretrain_data}

We train Kanana models on 3 trillion tokens, primarily focusing on English and Korean bilingual capabilities.
We collect our corpora from various sources and categorize them as English web, Korean web, academic, code, encyclopedic documents, and instruction data. 
All our data come from publicly available sources and do not include data from Kakao's products or services.

We begin by collecting various open-source datasets from multiple high-quality sources such as arXiv and Wikipedia.
However, we observe that these datasets often suffer quality issues due to suboptimal extraction pipelines, resulting in omissions or incoherent paragraph ordering (see Appendix \ref{appendix:suboptimal-arxiv-wiki} for details).
Inherently, we improve source-specific extraction processes for these sources and re-extract documents with more valuable information and higher coherence.
For code datasets, we utilize open-source datasets from \citet{starcoder} and \citet{starcoder2}. We use only permissively licensed code and exclude any with non-permissive or missing licenses.
Following \citet{inf-llm}'s observation that adding instruction data at the end of pre-training enhances performance after SFT, we also incorporate instruction data with decontamination.

Utilizing the high potential of web as a source of valuable and diverse documents \citep{dclm, nemotron-cc, deepseekmath}, we apply series of filtering methods to extract high quality data.
The first filtering process is cascaded filtering pipeline \citep{yi, llama3, dclm, gemma, fineweb-edu} consisting of deduplication, heuristic filtering, and personally identifiable information (PII) anonymization. 
After the cascaded filtering, we further apply language-specific model-based filtering on high quality documents \citep{nemotron-cc, deepseekmath, dclm, fineweb-edu} separately on English and Korean. 
For English web documents, we utilize a DCLM \citep{dclm} classifier.
For Korean web documents, due to the lack of publicly available high quality classifiers, we iteratively train edu filter as high quality classifier using \texttt{FastText} \citep{fasttext} based on the FineWeb-Edu pipeline \citep{fineweb-edu}.
When applying the FineWeb-Edu pipeline, we observe that most of the documents are classified as uneducational, leading to a distribution imbalance.
To address this issue, we iteratively retrain the classifier by augmenting educational documents from the previous iteration.

To assess the quality of our edu filter and Korean web corpus, we perform experiments by continual pre-training Llama 3 8B with 25B tokens.
As shown in \autoref{tab:kor-edu}, the quality of our Korean web corpus is comparable to that of FineWeb 2 \citep{penedo2024fineweb-2}, which is the largest open-source Korean corpus.
Furthermore, when using our edu filter to extract high quality data from Korean web corpus, we observe a significant performance improvement in the experimental results through training.
Interestingly, we observe that using high quality English data, regardless of the quality of Korean data, can improve the scores on Korean benchmarks such as KMMLU and HAE-RAE, as well as the English benchmark MMLU. 
The results from this experiment make a foundation of our intuition for data mixture strategy in the staged pre-training in the following section.

\begin{table}[ht]
    \centering
        \begin{tabular}{ll|ccc}
        \toprule
        \multirow{2}{*}{\textbf{English Corpus}} & \multirow{2}{*}{\textbf{Korean Corpus}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} \\
        & & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} \\
        \midrule
        - & - & 65.14 & 40.29 & 61.23 \\ % Before Training
        \midrule
        DCLM random & FineWeb2 Korean & 64.16 & 41.02 & 70.39 \\
        DCLM random & Our Korean web & 63.59 & 41.41 & 71.31 \\
        DCLM random & Our Korean web w/ edu filter & 63.47 & 43.60 & 74.89\\
        DCLM high & FineWeb2 Korean & 65.36 & 41.78 & 71.22 \\
        DCLM high  & Our Korean web & 64.80 & 41.96 & 72.59 \\
        DCLM high & Our Korean web w/ edu filter & \textbf{65.40} & \textbf{44.19} & \textbf{75.99} \\
        
        \bottomrule
        \end{tabular}
    \caption{Performance of Llama 3 8B before and after continual pre-training with only 25B tokens, using different combinations of English and Korean corpora at a 1:1 ratio.}
    \label{tab:kor-edu}
\end{table}

In summary, we share two insights to consider when building bilingual corpora with underrepresented language for enhanced computational efficiency.
(1) Prioritize quality over quantity.
For languages that do not have vast tokens available, such as Korean, prioritizing quality over quantity is an effective solution.
(2) Knowledge from English data transfers to Korean.
Even with quality filtering on Korean dataset, English data remains a primary source of diverse and high-quality knowledge.
We observe that, under the same conditions for the quality of Korean data, improving the quality of English data leads to higher scores on Korean-related benchmarks.