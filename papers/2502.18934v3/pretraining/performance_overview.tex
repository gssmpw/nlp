\subsection{Performance}
\label{subsec:pretrain_overview}
\input{pretraining/bench}




We evaluate our pre-trained models using a series of standard benchmarks designed to assess English/Korean general knowledge, code, and mathematical reasoning.
For general knowledge, we employ multiple choice tasks of MMLU \citep{hendryckstest2021} for English knowledge, and KMMLU \citep{son2024kmmlu} and HAE-RAE \citep{son-etal-2024-hae} for Korean-specific knowledge.
To evaluate domain-specific abilities, we use HumanEval \citep{humaneval} and MBPP \citep{mbpp} for code and GSM8K \citep{cobbe2021gsm8k} for mathematical reasoning.
We use log-likelihood for multiple choice tasks, and greedy generation for generative tasks.

To demonstrate the effectiveness of our training strategy, we compare our models with representative open-source models in various model sizes \citep{llama3, gemma2024gemma2, qwen25techreport, research2024exaone, dang2024ayaexpansecombiningresearch}.
For EXAONE and Aya Expanse models \citep{research2024exaone, dang2024ayaexpansecombiningresearch}, we report only the performances on multiple-choice tasks using the same evaluation protocol.
This decision is based on the observation that multiple-choice performances largely remain unchanged between the base and instruct models, whereas generative tasks exhibit notable divergences (see Appendix \ref{appendix:pre-vs-post} for a detailed discussion).

As shown in \autoref{table:base-eval-1} and \autoref{fig:flops-vs-mmlu}, our models demonstrate strong performance in various domains and exhibit impressive Korean language capabilities, while requiring significantly less training compute. 
Kanana Flag 32.5B outperforms Llama 3.1 70B, Gemma 2 27B, and EXAONE-3.5-32B on knowledge-intensive natural language understanding benchmarks, such as MMLU and KMMLU, while consuming substantially fewer computational resources. 
In particular, the computational cost is even lower than that of Llama 3.1 8B, and is similar to Gemma 2 9B and EXAONE-3.5-7.8B. 
On the HAE-RAE benchmark, all Kanana LLMs demonstrate superior performance compared to other LLMs of similar sizes.