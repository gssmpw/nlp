\subsubsection{Depth Up-scaling}
\label{subsec:pretrain_dus}

To further enhance the model performance within limited resources after pre-training, we adopt the depth up-scaling (DUS) which increases model capacity by stacking additional layers \citep{kim2023solar}.
We apply DUS to expand Kanana 8B into Kanana Essence 9.8B and Kanana 26.8B into Kanana Flag 32.5B.
After the up-scaling process, each model variant is further trained on the same data mixtures used in pre-training, with 100 billion tokens dedicated to stage 1 and another 100 billion to stage 2.
Results of the up-scaling strategy demonstrates that the additional layers consistently contribute to performance enhancements as summarized in \autoref{tab:performance}.
\begin{table}[ht]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|cccccc|c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
    \midrule
    26.8B + DUS (32.5B) & 77.68 & 62.10 & 90.47 & 51.22 & 63.40 & 70.05 & 69.15 \\
    26.8B & 74.27 & 59.04 & 88.45 & 51.22 & 61.60 & 67.48 & 67.01 \\
    \midrule
    8B + DUS (9.8B)     & 67.61 & 50.67 & 84.98 & 40.24 & 53.60 & 63.61 & 60.10 \\
    8B & 64.22 & 48.30 & 83.41 & 40.24 & 51.40 & 57.09 & 57.44 \\
    \bottomrule
    \end{tabular}%
    }
    \caption{Performance comparison of Kanana models before and after depth up-scaling.}
    \label{tab:performance}
\end{table}

\autoref{tab:performance} illustrates the performance improvements achieved through depth up-scaling.
Kanana Essence 9.8B consistently outperforms its non-upscaled version, Kanana 8B with the average score rising from 57.52 to 60.12. 
This improvement is evident in MMLU, KMMLU, HAE-RAE, MBPP, and GSM8K, except for HumanEval. 
Similarly, Kanana Flag 32.5B achieves average score of 69.15, notably surpassing the non-upscaled Kanana 26.8B model. 
These results emphasize the effectiveness of depth up-scaling in improving various benchmark scores.



Notably, our strategy saves 11.06\% of total computational cost compared to the training of 9.8B and 32.5B LLMs from scratch.
This strategy of increasing model capacity through depth up-scaling only occupies about 6.67\% of the total computing resources across the entire training procedure. 
In combination with pre-training, depth up-scaling offers a strategic approach to significantly enhance model performance without introducing heavy computational demands of building new models from scratch.
