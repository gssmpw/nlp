
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccc|c}
\toprule
    \multirow{2}{*}{\textbf{Models}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
\midrule
\rowcolor{yellow} Kanana Flag 32.5B & 77.68 & 62.10 & \textbf{90.47} & \textbf{51.22} & 63.40 & 70.05 & 69.15 \\
Qwen2.5 32B & \textbf{83.10} & \textbf{63.15} & 75.16 & 50.00	& \textbf{73.40} & \textbf{82.41} & 71.20 \\
Gemma 2 27B & 75.45 & 51.16 & 69.11 & \textbf{51.22} & 64.60 & 74.37 & 64.32 \\
EXAONE-3.5-32B$^\dag$ & 72.68 & 46.36 & 82.22 & - & - & - & - \\
Aya Expanse 32B$^\dag$ & 74.52 & 49.57 & 80.66 & - & - & - & - \\
\midrule
\rowcolor{yellow} Kanana Essence 9.8B & 67.61 & 50.57 & \textbf{84.97} & 40.24 & 53.60 & 63.61 & 60.10 \\
Llama 3.1 8B & 65.18 & 41.02 & 61.78 & 35.37 & 48.60 & 50.87 & 50.47 \\
Qwen2.5 7B & \textbf{74.19} & \textbf{51.68} & 67.46 & \textbf{56.71} & \textbf{63.20} & \textbf{83.85} & 66.18 \\
Gemma 2 9B & 70.34 & 48.18 & 66.18 & 37.20 & 53.60 & 68.16 & 57.28 \\
EXAONE-3.5-7.8B$^\dag$ & 65.36 & 45.30 & 77.54 & - & - & - & - \\
Aya Expanse 8B$^\dag$ & 62.52 & 40.11 & 71.95 & - & - & - & - \\
\midrule
\rowcolor{yellow} Kanana Nano 2.1B & 54.83 & 44.80 & \textbf{77.09} & 31.10 & 46.20 & 46.32 & 50.06 \\
Llama 3.2 3B & 56.40 & 35.57 & 47.66 & 25.61 & 39.00 & 27.37 & 38.60 \\
Qwen2.5 3B & \textbf{65.57} & \textbf{45.28} & 61.32 & \textbf{37.80} & \textbf{55.60} & \textbf{69.07} & 55.77 \\
Gemma 2 2B & 52.89 & 30.67 & 45.55 & 20.12 & 28.20 & 24.72 & 33.69 \\
EXAONE-3.5-2.4B$^\dag$ & 59.27 & 43.58 & 68.65 & - & - & - & - \\
\midrule\midrule
Llama 3.1 70B & 78.93 & 53.00 &	76.35 & 57.32 &	66.60 &	81.73 & 68.99 \\
Qwen2.5 72B & 86.12 & 68.57 & 80.84 & 55.49 & 76.40 & 92.04 & 76.58 \\ 
\bottomrule
\end{tabular}
}
\caption{
Performance of Kanana base models on a set of standard benchmarks. 
The best scores are denoted in bold. 
70B sized Models have been included for reference purposes.
$^\dag$ For these models, results are obtained using instruct models because base model checkpoints are not released. 
}\label{table:base-eval-1}
\end{table}