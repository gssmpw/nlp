\subsection{Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) methods \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} enable large language models to access the latest external or proprietary information without altering model parameters \citep{liu2024chatqasurpassinggpt4conversational}.
% With access to reliable data sources, such as search results, databases, and tool call, RAG greatly reduces the hallucination of LLMs \citep{Shuster2021RetrievalAR}.
% To take advantages of RAG, model can ensure factual consistency given context.
% So, additional training process and mixture of datasets is required to enhance grounding performance \citep{lin2024flamefactualityawarealignmentlarge}.
In order to ensure factual consistency during retrieval, the grounding ability of the model needs to be trained through additional data mixture \citep{lin2024flamefactualityawarealignmentlarge}. 
In this section, we describe a process for developing reliable RAG models with enhanced grounding ability from Kanana LLMs. 

% For evaluation, we collect and assess our model on diverse RAG scenario benchmarks.
For evaluation, we collect RAG scenario benchmarks and evaluate our model on them.
% ContextualBench \citep{nguyen2024sfrragcontextuallyfaithfulllms} is set of multi-hop QA that golden answers are highly concise rather than chat-style. 
% We utilized it for measure conciseness of response and intuitive evaluation method based on exact match.
ContextualBench \citep{nguyen2024sfrragcontextuallyfaithfulllms} is set of multi-hop QA, which we specifically include to consider the conciseness in evaluation. 
FACTs \citep{jacovi2025factsgroundingleaderboardbenchmarking} consists of various tasks with contexts such as reasoning, QA, summarization, rewriting, and extraction. 
\footnote{We filtered with character length of 20k since our base model was trained with token length limit of 8k. This dataset is not labeled golden answer, so we only measure grounding score with it.}
IFEval \citep{zhou2023instructionfollowingevaluationlargelanguage} measures maintenance of helpfulness of our instruct model. 
However, these benchmarks are all English-based, making them insufficient to judge the RAG abilities in Korean. 
To this end, we develop an internal FACTs-like Korean RAG benchmark called RAG-General-Bench that focuses on measuring factual consistency in Korean. 
During the development, human annotators manually constructed the dataset with context, instruction, and reference answer, to evaluate helpfulness as well. 
The benchmark consists of a total of 115 samples with 4 main tasks, categorized into 27 subcategories, providing a diverse set of scenarios for evaluation. There are 2 samples of QA task in \autoref{appendix:rag-bench-example}.

% There are 4 samples of each task in \autoref{appendix:rag-bench-example}.


\begin{figure}[h]
    \small
    \centering
    \includegraphics[width=0.6\textwidth]{figures/rag/rag-main-performance-iter4.pdf}
    \caption{Performance Comparison of Various Models Based on averaged helpfulness and grounding in RAG-General-Bench.}
    % \caption{Performance Overview}
    \label{fig:rag-main-performance}
\end{figure}

\begin{figure}[h]
\centering
\resizebox{1.0\textwidth}{!}{%
    \includegraphics[width=\textwidth]{figures/rag/data-generation-pipeline-iter3.pdf}
}
\caption{QA Generation Pipeline}
\label{fig:rag-data-generation-pipeline}
\end{figure}

% To encourage responses that leverage contextual information, 
To increase grounding ability, we synthetically generate question-answer pairs using high-quality bilingual documents as seed documents, following the pipeline in \autoref{fig:rag-data-generation-pipeline}.
% To construct mixture of datasets for grounding enhancement, we synthetically generate question-answer pairs using high-quality bilingual documents as seed documents, following the pipeline in \autoref{fig:rag-data-generation-pipeline}.
% We construct a dataset that requires specialized knowledge to respond, to 
% After generation of large amount of QA pair, we filter low-grounded responses with LLM-Judge and then execute response refinement with reflection, ensuring that only high-grounded responses remained.
Then, we filter out instances with low grounding scores and use LLM-judge to reflect and refine the low grounding instances.
We call the dataset at this point as SynQA-SFT. 
% Additionally, we argumented responses of low grounding score for preference tuning using SynQA-SFT as a seed (SynQA-DPO). 
With SynQA-SFT, we augment responses with low grounding score to produce preference dataset that we call SynQA-DPO.
% These synthetic datasets allow model more focused on given context rather than intrinsic knowledge. 
Along with SynQA datasets, we utilize StructLM \citep{zhuang2024structlmbuildinggeneralistmodels} and FollowRAG \citep{dong2024generalinstructionfollowingalignmentretrievalaugmented} to adapt diverse context format and instructions in RAG scenarios and replay SFT dataset from Section \ref{subsec:post-training-data} to prevent general capability of the instruction model from degrading during training.
% to adapting diverse context format and instructions in RAG scenarios.

However, we observe a decline in the helpfulness score as the model is trained through SFT and DPO in \autoref{tab:rag-performance-comparison}.
In order to address this issue, we merge the DPO model with the instruction model to preserve helpfulness \citep{kim2024prometheus2opensource}.
% When merging models with different concepts, task transfer could be possible in addition to the ensemble effect \citep{kim2024prometheus2opensource}.
% So, we can get also higher helpfulness score while maintaining higher grounding score against the base model. 
As a result, Kanana Essence 9.8B RAG achieves 91.4\% of GPT-4o's grounding performance while maintaining our instruct model's helpfulness in our benchmark as presented in \autoref{fig:rag-main-performance}.
% Through this experiment, we show that Kanana model can sufficiently achieve grounding performance against oracle model. 
% The evaluation of diverse models of Korean RAG scenarios is presented in \autoref{fig:rag-main-performance}.

% During training phase, we focus on improving grounding capability.
% In SFT phase, we utilize a mixture of StructLM \citep{zhuang2024structlmbuildinggeneralistmodels}, FollowRAG \citep{dong2024generalinstructionfollowingalignmentretrievalaugmented} and synthetic QA dataset for training to adapting diverse context format and instructions in RAG scenarios. Additionally, we replay SFT dataset from Section \ref{subsec:post-training-data} to maintain helpfulness and instruction-following capabilities.
% During DPO phase, we inject preference for better grounding responses by augmenting rejected responses.
% Our comprehensive training process demonstrates improvements in grounding performance as shown in \autoref{tab:rag-performance-comparison}.
    
% However, it also declines helpfulenss compared to base model.  
% To address this issue, we apply weight averaging at the end of process. To enhance overall performance, grounding-focused model was merged with helpfulness-focused model.
% When merging models with different concepts, task transfer could be possible in addition to the ensemble effect \citep{kim2024prometheus2opensource}.
% Through this approach, we can get also higher score of helpfulness compared to base model.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|c|cc|c|c}
    \toprule
    \textbf{Models} & \multicolumn{1}{c|}{\textbf{FACTs}} & \multicolumn{2}{c|}{\textbf{RAG-General-Bench}} & \multicolumn{1}{c|}{\textbf{ContextualBench}} & \multicolumn{1}{c}{\textbf{IFEval}} \\
    & \small{Grounding} & \small{Grounding} & \small{Helpfulness} & \small{Exact-match} &  \\
    \midrule
    Kanana Essence 9.8B & 40.66 & 32.63 & 55.86 & 20.22 & \textbf{79.93}\\
    \midrule
    + SFT & 62.40& 59.29& 51.60& 48.08& 72.99\\
    + DPO & \textbf{63.09}& \textbf{65.33}& 52.67& \textbf{48.76}& 75.00\\
    + Merge (Kanana Essence 9.8B RAG) & 53.09 & 57.38& \textbf{57.32}& 48.31& 78.44\\
    \bottomrule
    \end{tabular}
}
    \caption{Performance change of each phase of recipe. Grounding score is average of two metric RAGAS \citep{es2023ragasautomatedevaluationretrieval} Faithfulness and rubric based LLM-judge. Helpfulness score is average of two metric RAGAS Answer Relevancy and rubric based LLM-judge. EM means exact matching normalized answer with golden label. IFEval scoring is as same as Section \ref{subsec:post_train_performance}.}
    \label{tab:rag-performance-comparison}
\end{table}
    

% \begin{wrapfigure}[14]{r}{0.45\textwidth}
%     \centering
%     \resizebox{0.44\textwidth}{!}{%
%         \includegraphics{figures/rag/eval-ko-data-portion.pdf}
%     }
%     \caption{Task distribution of RAG-General-Bench, which was developed by Kanana team.}
%     \label{rag-ko-eval-data}
% \end{wrapfigure}