\subsection{Function Calling}
Function calling is the ability to reliably connect LLMs to external tools to enable effective tool usage and interaction with external APIs. We constructed training data based on the Korean language. As a result, we achieved a model with outstanding performance in Korean.\par

The evaluation utilized FunctionChat-Bench \citep{lee2024functionchat}, which is constructed with Korean evaluation data. Single-call evaluates how accurately the model can select and call the necessary function among several options. Dialogue evaluates how accurately the model responds in multi-turn situations.
\begin{table}[hbt!]
\centering
\resizebox{0.6\columnwidth}{!}{
\begin{tabular}{lrr}
\toprule
Models & \begin{tabular}[c]{@{}l@{}}\textbf{Single-call} \end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Dialogue}\end{tabular}  \\
\midrule
gpt-4o-2024-05-13     & 0.93    & \textbf{0.95}    \\
gpt-4-0125-prevew & \textbf{0.94}   & 0.94  \\
gpt-3.5-turbo-0125 & 0.92 & 0.89 \\
\midrule
Llama3-cpt-8b-instruct &  &  \\
\hspace{1em}+ single  & 0.89    & 0.74    \\
\hspace{1em}+ multi   & \textbf{0.94}     & 0.76    \\
\hspace{1em}+ multi + sgd  & 0.82  & 0.84    \\
Kanana Essence 8B Instruct &  &  \\
\hspace{1em}+ multi + sgd & 0.81   & 0.83  \\
\hspace{1em}+ multi + sgd + synthetic\_single & 0.88  & \textbf{0.89} \\
\bottomrule
\end{tabular}
}
\caption{Accuracy of FunctionChat-Bench}\label{tab-fc-overall}
\end{table}
\par
The experiment utilizes two models. One is a model that performs continual pre-training on 500 billion Korean and English datasets using the LLama3-8B\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}} model, followed by instruct tuning. This model is referred to as llama3-cpt-8b-instruct. Another one is an 8B-sized model trained from-scratch and then instruct tuned. We refer to this model as Kanana Essence 8B Instruct. This model has high performance in the Korean language because it has extensively learned from Korean data. Our GPT results utilized zero-shot. Our model achieved more than 90\% performance compared to the GPT model in both Single-call and Dialogue.


\subsubsection{Data}
The datasets are comprised of public datasets \citep{rastogi2020towards, liu2024apigen} and private dataset (synthetic\_single). The internally developed dataset is called the private dataset.  Figure \ref{fig:fc_dataset} outlines the process for generating training data.\par
We used different combinations of datasets for each training stage. Specifically, stage-1 primarily consists of English data, while stage-2 is composed solely of Korean data. We uniformly divided the final constructed dataset into a 9:1 ratio to form the training and validation sets.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.37]{figures/function_calling/fc_dataset.pdf}
    \caption{Construction process of function calling dataset}
    \label{fig:fc_dataset}
\end{figure}

\subsubsection{Multi-staged Training}
First, post-training with function call training data is performed to strengthen the understanding of contextual meaning and domain terminology\citep{whang2019effective}. This is defined as stage-1. In particular, at this stage, English data is incorporated to address the issue of insufficient data.\par

Stage-2 is defined as the step where supervised fine-tuning is performed to execute actual function calling. In this stage, only Korean data is used. Figure \ref{fig:fc_multi_stage} illustrates the overall training process.\par

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/function_calling/fc_multi_stage.pdf}
    \caption{Multi-stage training process}
    \label{fig:fc_multi_stage}
\end{figure}

\textbf{Stage-1} The base model has never been exposed to the function calling dataset before, so it needs to adapt to this dataset. In addition, chat template associated with function calls can use various special tokens (e.g., \big|function\big|). If this special token is one that has never been encountered in the base model or pre-trained model, then it cannot adequately learn the information about the token through the supervised fine-tune stage alone. This is because the fine-tune dataset often contains only a small amount of data. This deficiency can be overcome through the post-training process. \par

There are various examples of functions in different forms and their call results included in the publicly available English datasets.  By utilizing this during the post-training phase, one can effectively use rich information while minimizing the impact on Korean language performance. We use the NTP-loss and apply masking to special tokens in the input labels. Masking is used to exclude certain tokens from loss calculation.\par

\textbf{Stage-2} Once post-training is completed, further training is conducted. In stage-1, special tokens were masked. In this stage, only the output necessary for actual function calling is produced by masking user input and function call results. For user queries, the function calling model outputs the function to be called along with its parameters, and it is trained to generate an appropriate natural language response based on the function call results.\par
Through this, one can learn the ability to distinguish between cases where a function call is necessary and those where it is not. Additionally, it develops the ability to fill in missing arguments for function calls through re-quring. This stage is identical to stage 1, except for the difference in the region of masking on the input data.\par


