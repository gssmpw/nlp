\subsection{Retrieval-Augmented Generation}
In this section, we describe a process of developing reliable RAG models with enhanced fluency, coherence, and factual accuracy from Kanana LLMs and introduce internal Korean RAG benchmark developed to more accurately measure the factual consistency.

% The evaluation of diverse models of korean RAG scenarios is presented in \autoref{fig:rag-main-performance}.
% After our training process, Kanana Essence 9.8B RAG achieved helpfulness while maintaining 91.4\% of GPT-4o's grounding performance. So we comfirmed that Kanana model can sufficiently achieve grounding performance against oracle model.

\begin{figure}[h]
    \small
    \centering
    \includegraphics[width=0.6\textwidth]{figures/rag/rag-main-performance-iter4.pdf}
    \caption{Performance Comparison of Various Models Based on averaged helpfulness and grounding in RAG-General-Bench.}
    % \caption{Performance Overview}
    \label{fig:rag-main-performance}
\end{figure}
Retrieval-Augmented Generation (RAG) methods \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} enable large language models to access the latest external or proprietary information without altering model parameters \citep{liu2024chatqasurpassinggpt4conversational}.
% Retrieval-Augmented Generation (RAG) is a hybrid generation model that combines Large Language Models (LLMs) with access to non-parametric memory \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}.
With access to reliable data sources, such as search results, data bases, and tool call, RAG greatly reduces the hallucination of LLMs \citep{Shuster2021RetrievalAR}.
% In other words, it enhances reliability and contextual accuracy by ensuring that generated responses are conditioned on the given information from credible sources, such as search, database, and tool call.
% To ensure advantages of RAG, additional training process is required to enhance factuality \citep{lin2024flamefactualityawarealignmentlarge}.
However, in order to endow RAG ability, models require further training using appropriate RAG data \citep{lin2024flamefactualityawarealignmentlarge}.
% Additionally, robust evaluation methods are crucial to accurately measure factual consistency for model ablation.

\begin{figure}[h]
\centering
\resizebox{0.9\textwidth}{!}{%
    \includegraphics[width=\textwidth]{figures/rag/data-generation-pipeline.pdf}
}
\caption{QA Generation Pipelines}
\label{fig:rag-data-generation-pipeline}
\end{figure}

% To encourage responses that leverage contextual information, 
To construct RAG dataset, we synthetically generate question-answer pairs  using high-quality bilingual documents as seed documents, following the pipeline in \autoref{fig:rag-data-generation-pipeline}.
% We build dataset around specialized knowledge to ensure that model relies on contextual information rather than its intrinsic knowledge.
% We construct a dataset that requires specialized knowledge to respond, to 
% After generation of large amount of QA pair, we filter low-grounded responses with LLM-Judge and then execute response refinement with reflection, ensuring that only high-grounded responses remained.
From the synthetically generated pairs, we filter out instances with low grounding scores and use LLM-judge to reflect and refine the low grounding instances.
At this point, we have obtained dataset for SFT, which we call SynQA-SFT. 
Along with SynQA-SFT, we augment StructLM \citep{zhuang2024structlmbuildinggeneralistmodels}, FollowRAG \citep{dong2024generalinstructionfollowingalignmentretrievalaugmented}, and SFT dataset from Section \ref{subsec:post-training-data} to train the instruction model to adapt to the RAG scenarios.
% while maintaining its instruction following capabilities.
Then, we augment low grounding responses to high grounding instances in SynQA-SFT to construct preference dataset for DPO.
Because the SFT and DPO process decays instruction following capability, we merge the DPO model with the instruction model to preserve both abilities.
% To execute the pipline cost-effectively, we utilized strong model for generation and LLM-Judge locally served.
Our comprehensive training process demonstrates improvements in grounding performance while maintaining the instruction following scores, as shown in \autoref{tab:rag-performance-comparison}.

% During training phase, we focus on improving grounding capability.
% In SFT phase, we utilize a mixture of StructLM \citep{zhuang2024structlmbuildinggeneralistmodels}, FollowRAG \citep{dong2024generalinstructionfollowingalignmentretrievalaugmented} and synthetic QA dataset for training to adapting diverse context format and instructions in RAG scenarios. Additionally, we replay SFT dataset from Section \ref{subsec:post-training-data} to maintain helpfulness and instruction-following capabilities.
% During DPO phase, we inject preference for better grounding responses by augmenting rejected responses.
% Our comprehensive training process demonstrates improvements in grounding performance as shown in \autoref{tab:rag-performance-comparison}.
    
% However, it also declines helpfulenss compared to base model.  
% To address this issue, we apply weight averaging at the end of process. To enhance overall performance, grounding-focused model was merged with helpfulness-focused model.
% When merging models with different concepts, task transfer could be possible in addition to the ensemble effect \citep{kim2024prometheus2opensource}.
% Through this approach, we can get also higher score of helpfulness compared to base model.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|c|cc|c|c}
    \toprule
    \textbf{Models} & \multicolumn{1}{c|}{\textbf{FACTs}} & \multicolumn{2}{c|}{\textbf{RAG-General-Bench}} & \multicolumn{1}{c|}{\textbf{ContextualBench}} & \multicolumn{1}{c}{\textbf{IFEval}} \\
    & \small{grounding} & \small{grounding} & \small{helpfulness} & \small{em} &  \\
    \midrule
    Kanana Essence 9.8B & 40.66 & 32.63 & 55.86 & 20.22 & \textbf{79.93}\\
    \midrule
    % Phase1 (SFT)& 62.40& 59.29& 51.60& 48.08& 72.99\\
    % Phase2 (DPO)& \textbf{63.09}& \textbf{65.33}& 52.67& \textbf{48.76}& 75.00\\
    % Phase3 (Merge)& 53.09 & 57.38& \textbf{57.32}& 48.31& 78.44\\
    + SFT & 62.40& 59.29& 51.60& 48.08& 72.99\\
    + DPO & \textbf{63.09}& \textbf{65.33}& 52.67& \textbf{48.76}& 75.00\\
    + Merge (Kanana Essence 9.8B RAG) & 53.09 & 57.38& \textbf{57.32}& 48.31& 78.44\\
    \bottomrule
    \end{tabular}
}
    \caption{Performance change of each phase of recipe. Grounding score is average of RAGAS \citep{es2023ragasautomatedevaluationretrieval} Faithfulness and rubric based LLM-judge. Helpfulness score is average of RAGAS Answer Relevancy and rubric based LLM-judge.}
    \label{tab:rag-performance-comparison}
\end{table}
    

% \begin{wrapfigure}[14]{r}{0.45\textwidth}
%     \centering
%     \resizebox{0.44\textwidth}{!}{%
%         \includegraphics{figures/rag/eval-ko-data-portion.pdf}
%     }
%     \caption{Task distribution of RAG-General-Bench, which was developed by Kanana team.}
%     \label{rag-ko-eval-data}
% \end{wrapfigure}

For completeness of the evaluation, we collect and assess our model on diverse RAG scenario benchmarks of ContextualBench \citep{nguyen2024sfrragcontextuallyfaithfulllms}, FACTs \citep{jacovi2025factsgroundingleaderboardbenchmarking}, and IFEval \citep{zhou2023instructionfollowingevaluationlargelanguage}.
% ContextualBench \citep{nguyen2024sfrragcontextuallyfaithfulllms} is set of multi-hop QA testset that golden answers are highly concise rather than chat-style. 
% we utilized it for measure conciseness of response and intuitive evaluation method based on exact match.
% FACTs \citep{jacovi2025factsgroundingleaderboardbenchmarking} is consist of diverse task given context like Reasoning, QA, Summarization, Rewriting and Extraction. 
% We filtered within char-level length 20k since our base model was trained with token length limit of 8k. This dataset is not labeled golden answer, so we only measure grounding score with it.
% IFEval \citep{zhou2023instructionfollowingevaluationlargelanguage} measures the maintenance of instruction following performance. 
% While utilizing english public benchmarks, we developed internal RAG benchmark named RAG-General-Bench to address insufficient of korean evaluation set.
However, these benchmarks are all English-based, making them insufficient to judge the RAG abilities in Korean.
To this end, we have developed an internal FACTs-like Korean benchmark for evaluating RAG abilities, called RAG-General-Bench.
In RAG-General-Bench, human annotators manually constructed the dataset with context, instruction, and reference answer, allowing the evaluation of helpfulness as well. 
% Since reference answer exists, helpfulness can also be evaluated.
% It focued on RAG-specific tasks similar with FACTs.
% In \autoref{rag-ko-eval-data}, benchmark consists of a total of 115 samples, categorized into 27 subcategories, providing a diverse set of scenarios for evaluation. There are 4 samples of each category in \autoref{appendix:rag-bench-example}.
The benchmark consists of a total of 115 samples, categorized into 27 subcategories, providing a diverse set of scenarios for evaluation. There are 4 samples of each category in \autoref{appendix:rag-bench-example}.

% The evaluation of diverse models of korean RAG scenarios is presented in \autoref{fig:rag-main-performance}.
% After our training process, Kanana Essence 9.8B RAG achieved helpfulness while maintaining 91.4\% of GPT-4o's grounding performance. So we comfirmed that Kanana model can sufficiently achieve grounding performance against oracle model.

% The evaluation of diverse models of korean RAG scenarios is presented in \autoref{fig:rag-main-performance}.
% After our training process, Phase 2 model outperformed GPT-4o in grounding performance but its helpfulness fell short of both the base model and other open models.
% Meanwhile, Phase 3 improved helpfulness while maintaining 91.4\% of GPT-4o's grounding performance. So we comfirmed that Kanana model can sufficiently achieve grounding performance against oracle model.

    % We train Kanana Essence model following training process in \autoref{fig:model-recipe}.
    % We conduct ablations while fixing SFT-DPO flow and model merge is utilized after DPO to maintain helpfulness and instruction-following performance.

    % In phase1, we use 2.1M unique samples for training, including SynQA-SFT Collection.
    % To maintain the general capabilities of the base model, around 40\% of the total samples were replayed using Kanana-SFT dataset.
    % Sequence packing was employed to ensure contamination-free training, allowing the attention mechanism to function solely within individual samples.
    % We set maximum learning rate to $5\times10^{-6}$, the warmup steps to 200, minimum learning rate to 10\% of peak lr and trained with 3 epochs.
    % In phase2, We further train supervised fine-tuned model using 250K unique samples from SynQA-DPO Collection.
    % We set the maximum learning rate to $1\times10^{-7}$, the warmup steps to 200, and the minimum learning rate to 0.1 and trained with 3 epochs. In phase3, we merged Kanana-Inst and DPO model in a 1:1 ratio.
    % It enables an ensemble effect without increasing inference time \citep{wortsman2022modelsoupsaveragingweights}. 
    
  
    
% \begin{figure}[h]
%     \centering
%     \resizebox{0.8\textwidth}{!}{%
%         \includegraphics[width=\textwidth]{figures/rag/model-recipe.pdf}
%     }
%     \caption{Training Process}
%     \label{fig:model-recipe}
% \end{figure}