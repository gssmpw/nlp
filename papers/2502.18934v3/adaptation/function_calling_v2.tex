\subsection{Function Calling}
Function calling is an essential ability for large language models (LLMs) to interact with external tools and databases, granting them access to up-to-date information stored in dynamic or structured formats \citep{schick2023toolformer}.
This capability helps 
% mitigate hallucination by 
integrating real-time data with the static knowledge inherent in LLMs, which is particularly vital in enterprises.
% Furthermore, function calling empowers LLMs to delegate specialized tasks to external systems, thereby enhancing their role as autonomous agents capable of making informed decisions in real-world scenarios.

Previous works highlight the increasing importance of function calling, which has led to various efforts in data generation for fine-tuning and model evaluation \citep{basu2024api, guo2024api, qin2023toolllm, tang2023toolalpaca, li2023api, rastogi2020towards, liu2024apigen}.
However, these efforts predominantly focused on English, making it necessary to create a function calling dataset for low-resource languages.
To address this gap within Korean contexts, we create a fine-tuning dataset, referred to as \texttt{korean-fc-corpus}.
The corpus is constructed by: (1) translating two key English function calling corpora, glaive-function-calling-v1 (\texttt{gfc-v1}) \citep{glaiveai2023fcv1} and the Schema-Guided Dialogue Dataset (\texttt{sgd}) \citep{rastogi2020towards}, into their Korean equivalents, \texttt{ko-gfc-v1} and \texttt{ko-sgd}; and (2) creating an in-house function calling dataset (\texttt{inhouse-fc}) specifically tailored for corporate applications.

% Fine-tuning solely on \texttt{korean-fc-corpus}, however, proved insufficient due to their limited size.
% We hypothesize that our instruct-tuned models are not yet familiar with the specialized tokens and domain-specific terminology in function calling tasks.
% Therefore, we adopted a two-stage training process comprising domain pre-training and supervised fine-tuning.
We further adopt two-staged training process comprising domain specific pre-training and supervised fine-tuning to adapt instruct-tuned models to function calling specific tokens and terminologies.
In the domain pre-training phase, we leveraged multiple English-based function calling datasets, including \texttt{gfc-v1}, glaive-function-calling-v2 \citep{glaiveai2024fcv2}, xlam-function-calling-60k \citep{liu2024apigen}, as well as \texttt{sgd}, supplemented by our \texttt{inhouse-fc}.
This foundation enabled us to perform supervised fine-tuning exclusively on \texttt{korean-fc-corpus}.
This two-stage strategy ensures that models become adequately versed in function calling conventions and domain terminologies before focusing on Korean-specific nuances, thereby enhancing their performance in Korean function calling tasks.

\begin{table}[hbt!]
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Models} & \textbf{Single-call} & \textbf{Dialogue} \\
\midrule
Kanana 8B FC & 0.88 & 0.89 \\
% gpt-3.5-turbo-0125 & 0.92 & 0.89 \\
gpt-4-0125-preview & 0.94 & 0.94 \\
gpt-4o-2024-05-13 & 0.93 & 0.95 \\
\bottomrule
\end{tabular}
}
\caption{Evaluation on FunctionChat-Bench: Single-call and Dialogue Accuracy}
\label{tab-fc-overall}
\end{table}

To evaluate function calling capabilities in corporate environments, we introduce FunctionChat-Bench \citep{lee2024functionchat}, a benchmark designed for Korean conversational settings.
This benchmark measures performance on two metrics: Single-call accuracy, which evaluates how well a model selects and invokes the necessary function from several options, and Dialogue accuracy, which examines the model’s capability in multi-turn interactions. 
% For comprehensive comparisons, we evaluated OpenAI’s proprietary models (gpt-3.5-turbo-0125, gpt-4-0125-preview, and gpt-4o-2024-05-13) and our Kanana 8B model. \autoref{tab-fc-overall} presents the average scores for Single-call and Dialogue accuracy.
For comparative analysis, we evaluate OpenAI’s proprietary models (
% gpt-3.5-turbo-0125, 
gpt-4-0125-preview, and gpt-4o-2024-05-13) and Kanana 8B FC model as shown in \autoref{tab-fc-overall}.

% The results indicate that Kanana 8B FC model slightly lags behind the OpenAI models in Single-call accuracy.
% % , with a score of 0.88 compared to 0.92 for gpt-3.5-turbo-0125.
% % , it matches gpt-3.5-turbo-0125 in Dialogue accuracy, both achieving a score of 0.89. 
% % This performance highlights our model's proficiency in managing multi-turn interactions, demonstrating its effectiveness in conversational contexts. 
% While gpt-4 models show superior performance, the comparable scores showcase the potential emergence of adaptive abilities of Kanana models. 
% % within its design constraints.

% 25/02/24 mat's ver
This result indicates that leveraging task specific fine-tuning on moderately sized LLMs, which are trained at a lower cost, may offer a more cost effective and efficient approach for addressing certain tasks.