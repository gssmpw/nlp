\section{Pre-training}

Since pre-training constitutes the majority of computational costs, we focus on reducing the expenses of this stage and show our results in Section \ref{subsec:pretrain_overview}.
To enhance efficiency in pre-training LLMs, we employ two key strategies: data efficiency and training efficiency.
In Section \ref{subsec:pretrain_data}, we discuss our data curation method to maximize the data efficiency under fixed token budget.
In Section \ref{subsec:pretrain_train_process}, we adopt cost-effective training techniques to minimize the computational overhead associated with model scaling.



\input{pretraining/performance_overview}
\input{pretraining/data}
\input{pretraining/training_process}
