\subsection{Performance}
\label{subsec:post_train_performance}



\input{posttraining/bench2}
\input{posttraining/bench1}


We evaluate our instruction-tuned models across various tasks: chat, instruction following, general knowledge, coding, and mathematics and compare their performance to previous instruction-tuned models.
For general chat ability, we use MT-Bench \citep{zheng2023judging}, LogicKor \citep{park2024logickor}, KoMT-Bench \citep{KoMT-Bench}, and WildBench \citep{lin2024wildbench}.
To test instruction following ability, we use IFEval\footnote{We report the average of Prompt-level strict-accuracy and Instruct-level strict-accuracy.}\citep{zhou2023instructionfollowingevaluationlargelanguage}.
For general knowledge tasks, we use MMLU \citep{hendryckstest2021}, KMMLU \citep{son2024kmmlu}, and HAE-RAE\footnote{We report general knowledge category scores in this section.} \citep{son-etal-2024-hae}, with zero-shot chain-of-thought (CoT) \citep{wei2022chainofthought} setting along with the chat template.
Employing zero-shot CoT with the chat template, rather than multi-shot prompts, allows us to evaluate the inherent capabilities of the instruction model, without residual traces from the pre-trained model.
For coding ability, we use HumanEval+ \citep{evalplus} and MBPP+ \citep{evalplus}.
For Mathematical ability, we use GSM8K \citep{cobbe2021gsm8k} and MATH \citep{hendrycksmath2021}.
See Appendix \ref{subsec:evaluation-prompts-for-post-trained-models} for detailed prompts of benchmarks.

\autoref{table:chat-eval-2} and \autoref{table:chat-eval-1} show that our models excel similar sized models on Korean tasks.
The 32.5B model achieves the highest performance in Korean chat tasks (LogicKor, KoMT-Bench) and Korean knowledge tasks (KMMLU, HAE-RAE). 
The 9.8B and 2.1B models rank second in Korean chat tasks and either best or second-best in Korean knowledge tasks.
Additionally, our models exhibit competitive performance across other tasks except in math.

