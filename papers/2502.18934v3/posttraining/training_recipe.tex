\subsection{Training Process}
\label{subsec:post_train_process}

We adopt the widely used multi-stage post-training procedure comprising SFT and a series of preference optimization processes \citep{ouyang2022instruct, llama3, qwen25techreport, gemma2024gemma2}.
In Section \ref{subsec:post_train_sft}, we provide details on the SFT process.
In Section \ref{subsec:reward-model-training}, we share information on training our reward model from the SFT model for the subsequent preference optimization process.
In Section \ref{subsec:preference-optimization}, we perform preference optimization on the SFT model, which is a sequential process consisting of offline and online preference optimization. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/posttraining/stage_score_wide.pdf}
    \caption{
    Kanana model performance for each stage of training across different model sizes.
    The y-axis is the average of normalized scores of all benchmarks in \autoref{table:chat-eval-2} and \autoref{table:chat-eval-1}.
    The normalization process is done by dividing each score with the maximum possible score.
    }
    \label{fig:stage_score}
\end{figure}

As shown in \autoref{fig:stage_score}, each step of this process quantitatively enhances the instruction-tuned model across different model sizes.
Qualitatively, we observe that during the SFT stage, the model learns to generate structured chat responses while integrating relevant knowledge, and this ability persists through subsequent stages.
Building on the SFT model, the preference optimization stages further enhance performance by refining the model’s tone and manner.
\autoref{sec:qualititive_results} presents qualitative results and illustrates the evolution of model completions throughout each phase of post-training.



\subsubsection{Supervised Fine-Tuning}
\label{subsec:post_train_sft}
\input{posttraining/sft_abl}

During the SFT stage, the model develops the ability to generate structured chat responses while integrating relevant knowledge.
% train data
In this stage, we train the model using 1.2M data instances, as described in Section~\ref{subsec:post-training-data}.
While optimizing the proportion of domain-specific data, we observed that such data is crucial for achieving high performance in its respective domain and does not negatively impact other domains.
\autoref{table:sft_abl} demonstrates that excluding domain-specific data from total dataset only reduces performance on the corresponding domain's benchmark, while performance in other domains remains unaffected.
Consequently, we incorporate the full extent of each domain-specific dataset while ensuring balanced performance across all domains.




\subsubsection{Reward Model Training} \label{subsec:reward-model-training}

We train a reward model for subsequent online preference optimization process, assuming a Bradley-Terry model \citep{bradley1952rank}.
The reward model is trained using the offline preference data along with additional public preference data.
Among various reward models trained with different data proportions and settings, we select the one that demonstrates the strongest best-of-N policy~\citep{gao2023reward-model-overoptimization} performance.
The best-of-N policy performance is evaluated by generating N responses from the policy model, scoring them with the reward model, selecting the highest-scoring response, and then assessing the final response's quality using a benchmark judge.
This approach is based on the intuition that the chosen reward model should effectively evaluate the response distribution of the online preference optimization stage in accordance with the benchmark evaluation criteria.




\subsubsection{Preference Optimization} \label{subsec:preference-optimization}

To further improve the SFT model’s performance on LLM benchmarks, we conduct a preference optimization stage. 
The process begins with offline preference optimization \citep{meng2024simpo, jung2024bco}, where we apply direct preference optimization (DPO)~\citep{rafailov2023direct} using the offline preference data.




We then conduct online preference optimization, initializing from the offline DPO model.
During training, policy-generated responses are evaluated by the reward model from Section \ref{subsec:reward-model-training}, providing training data for online DPO \citep{guo2024online-dpo} with asynchronous response sampling \citep{noukhovitch2024asynchronous-rlhf}.
This approach can be considered as a form of iterative DPO~\citep{xiong2024iterative-preference}.
However, unlike prior work~\citep{tran2023iterative-dpo-snorkel}, we maintain a fixed reference model, specifically the offline DPO model, throughout all iterations.
This decision is based on our observation that updating the reference model led to undesirable increases in response length.
