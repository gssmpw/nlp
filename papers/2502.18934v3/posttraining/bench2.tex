
\begin{table}[h]
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{l|cccc|c}
\toprule
\multirow{2}{4em}{\textbf{Models}} & \multicolumn{4}{c|}{\textit{Chat}} & \textit{Instruction Following} \\
      & \textbf{MT-Bench} & \textbf{LogicKor} & \textbf{KoMT-Bench} & \textbf{WildBench} & \textbf{IFEval} \\
\midrule
\rowcolor{yellow} Kanana Flag 32.5B & 8.356 & \textbf{9.524} & \textbf{8.058} & 54.14 & \textbf{0.856} \\
Qwen2.5 32B & 8.331 & 8.988 & 7.847 & 51.13 & 0.822 \\
Gemma 2 27B & 8.088 & 8.869 & 7.373 & 46.46 & 0.817 \\
EXAONE-3.5-32B & \textbf{8.375} & 9.202 & 7.907 & \textbf{54.30} & 0.845 \\
Aya Expanse 32B & 7.788 & 8.941 & 7.626 & 48.36 & 0.735 \\
\midrule
\rowcolor{yellow} Kanana Essence 9.8B & 7.769 & 8.964 & 7.706 & 47.27 & 0.799 \\
Llama 3.1 8B & 7.500 & 6.512 & 5.336 & 33.20 & 0.772 \\
Qwen2.5 7B & 7.625 & 7.952 & 6.808 & 41.31 & 0.760 \\
Gemma 2 9B & 7.633 & 8.643 & 7.029 & 40.92 & 0.750 \\
EXAONE-3.5-7.8B & \textbf{8.213} & \textbf{9.357} & \textbf{8.013} & \textbf{50.98} & \textbf{0.826} \\
Aya Expanse 8B & 7.131 & 8.357 & 7.006 & 38.50 & 0.645\\
\midrule
\rowcolor{yellow} Kanana Nano 2.1B &  6.400 & 7.964 & 5.857 & 25.41 & 0.720 \\
Llama 3.2 3B & 7.050 & 4.452 & 3.967 & 21.91 & 0.767 \\
Qwen2.5 3B & 6.969 & 6.488 & 5.274 & 25.76 & 0.355 \\
Gemma 2 2B & 7.225 & 5.917 & 4.835 & 28.71 & 0.428 \\
EXAONE-3.5-2.4B & \textbf{7.919} & \textbf{8.941} & \textbf{7.223} & \textbf{41.68} & \textbf{0.790} \\
\midrule\midrule
Llama 3.1 70B & 8.275 & 8.250 & 6.970 & 46.50 & 0.875 \\
Qwen2.5 72B & 8.619 & 9.214 & 8.281 & 55.25 & 0.861 \\
\bottomrule
\end{tabular}
}
\caption{
Performance of Kanana and previous instruction-tuned models in general chat and instruction following benchmarks.
Across all \textit{Chat} benchmarks, we use \texttt{gpt-4o-2024-08-06} as a judge model.
The best scores are denoted in \textbf{bold}.
70B sized models have been included for reference purposes.
}\label{table:chat-eval-2}
\end{table}

