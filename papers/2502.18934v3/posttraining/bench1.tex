% Benchmark Table
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccc|cc|cc}
\toprule
\multirow{2}{4em}{\textbf{Models}} & \multicolumn{3}{c|}{\textit{General}} & \multicolumn{2}{c|}{\textit{Coding}} & \multicolumn{2}{c}{\textit{Mathematics}} \\
      & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} &  \textbf{HumanEval+} & \textbf{MBPP+} & \textbf{GSM8K} & \textbf{MATH} \\
\midrule
\rowcolor{yellow} Kanana Flag 32.5B & 81.08 & \textbf{64.19} & \textbf{68.18} & 77.44 & 69.84 & 90.83 & 57.82 \\
Qwen2.5 32B & \textbf{84.40} & 59.37 & 48.30 & \textbf{82.32} & \textbf{71.96} & \textbf{95.30} & \textbf{81.90} \\
Gemma 2 27B & 78.01 & 49.98 & 46.02 & 70.12 & 70.90 & 91.05 & 53.80 \\
EXAONE-3.5-32B & 78.30 & 55.44 & 52.27 & 78.66 & 70.90 & 93.56 & 76.80 \\
Aya Expanse 32B & 74.49 & 42.35 & 51.14 & 64.63 & 65.61 & 75.06 & 42.82 \\
\midrule
\rowcolor{yellow} Kanana Essence 9.8B & 70.64 & 50.76 & \textbf{47.16} & 72.56 & 69.05 & 84.91 & 42.24 \\
Llama 3.1 8B & 71.18 & 39.24 & 40.91 & 60.98 & 57.67 & 82.71 & 49.86 \\
Qwen2.5 7B & \textbf{77.23} & 46.87 & 37.50 & 73.78 & \textbf{70.63} & \textbf{91.58} & \textbf{75.22} \\
Gemma 2 9B & 73.47 & 44.47 & 39.77 & 59.76 & 64.55 & 87.72 & 48.10 \\
EXAONE-3.5-7.8B & 72.62 & \textbf{52.09} & 46.02 & \textbf{79.27} & 66.67 & 89.99 & 73.50 \\
Aya Expanse 8B & 61.23 & 35.78 & 39.20 & 42.68 & 56.88 & 78.85 & 30.80 \\
\midrule
\rowcolor{yellow} Kanana Nano 2.1B & 52.48 & \textbf{38.51} & \textbf{33.52} & 63.41 & 62.43 & 72.32 & 29.26 \\
Llama 3.2 3B & 56.09 & 3.07 & 17.05 & 56.71 & 50.26 & 66.57 & 38.18 \\
Qwen2.5 3B & \textbf{69.18} & 38.33 & 32.39 & 67.68 & \textbf{64.02} & \textbf{84.00} & \textbf{65.72} \\
Gemma 2 2B & 57.69 & 6.99 & 7.95 & 35.37 & 45.24 & 49.81 & 21.68 \\
EXAONE-3.5-2.4B & 63.19 & 14.27 & 14.20 & \textbf{70.73} & 59.79 & 83.78 & 64.04 \\
\midrule\midrule
Llama 3.1 70B & 83.48 & 39.08 & 53.41 & 75.61 & 66.40 & 91.66 & 63.98 \\
Qwen2.5 72B & 87.14 & 65.78 & 60.80 & 81.10 & 75.66 & 95.45 & 82.60 \\
% HyperCLOVA X\footnotemark[1] & 66.50 & 49.09 & 59.65 & - & 62.17 & 63.46 & -  \\
\bottomrule
\end{tabular}
}
\caption{
Performance of Kanana post-trained models on a set of standard benchmarks. 
All benchmarks under General category are measured using 0-shot CoT with respective chat-template of each model.
The best scores are denoted in \textbf{bold}.
70B sized models have been included for reference purposes.
}\label{table:chat-eval-1}
\end{table}