\clearpage
\subsection{Details of pre-training from scratch}\label{appendix:pretrain-details}

To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 \citep{llama3}.
Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana.
Based on the observations of \citet{small-scale-proxy}, we adopt independent weight decay, which follows the original proposal of \citet{adamw} and differs from the PyTorch implementation, and a z-loss \citep{palm} to obtain effective and stable training across various model scales.
We set an independent weight decay of $1\times10^{-4}$ and a z-loss coefficient of $5 \times 10^{-6}$, regardless of model size.
For peak learning rates, learning rate schedulers, and batch sizes, the hyperparameter scaling law and multi-step scheduler from \citet{deepseek-llm} are employed.