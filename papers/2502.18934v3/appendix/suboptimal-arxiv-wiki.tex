\subsection{Suboptimal extraction of open-source datasets}\label{appendix:suboptimal-arxiv-wiki}

\begin{figure}[h]
\centering
\begin{tcolorbox}[colframe=black!80!white, colback=black!2!white, boxrule=0.5mm, width=\textwidth, arc=2mm, auto outer arc, title=Example of suboptimal extraction from arXiv, fonttitle=\color{white}\bfseries]
\begin{lstlisting}[
  basicstyle=\ttfamily,
  breaklines=true,
  breakatwhitespace=false,
  columns=fullflexible,
  keepspaces=true,
  breakautoindent=false,
  breakindent=0ex
]
(...)
In this work, we use sine and cosine functions of different frequencies:

\begin{align*}
    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}}) \\
    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})
\end{align*}

where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.

We also experimented with using learned positional embeddings \citep{JonasFaceNet2017} instead, and found that the two versions produced nearly identical results (see Table~\ref{tab:variations} row (E)).  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.

\section{Introduction}

\input{introduction}

\section{Background}

\input{background}

\section{Model Architecture}
\input{model_architecture}

\section{Why Self-Attention}
\input{why_self_attention}

\section{Training}
\input{training}

\section{Results} \label{sec:results}
\input{results}

\section{Conclusion}
In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.
(...)
\end{lstlisting}
\end{tcolorbox}
\caption{
    Example of suboptimal extraction from arXiv subset of \citet{redpajama}. 
    The original content is from \citet{Vaswani+2017}.
}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Open-source]{
\begin{tcolorbox}[colframe=black!80!white, colback=black!2!white, boxrule=0.5mm, width=\textwidth, arc=2mm, auto outer arc, title=Example of suboptimal extraction from Wikipedia, fonttitle=\color{white}\bfseries]
\begin{CJK}{UTF8}{mj}
발생원인\\
\\
회전 좌표계\\
\\
회전좌표계
좌표계 x, y, z와 좌표계 x', y', z'을 보자 두 좌표계의 원점은 같다. 각각의 경우에 대해 벡터 .은 두 좌표계에서 다음과 같이 표시된다.\\
\\
. (x, y, z 좌표)\\
\\
. (x', y', z' 좌표계)\\
\\
벡터의 내적을 이용해 x, y, z를 (.), (.), (.)으로 표현할 수 있다. 내적의 방법은 다음과 같다.\\
\\
.\\
\\
.\\
\\
.
\\
으로 표현되는 것을 확인할 수 있다.
\end{CJK}
\end{tcolorbox}
}

\subfloat[Improved]{
\begin{tcolorbox}[colframe=black!80!white, colback=black!2!white, boxrule=0.5mm, width=\textwidth, arc=2mm, auto outer arc, title=Example of improved extraction from Wikipedia, fonttitle=\color{white}\bfseries]
\begin{CJK}{UTF8}{mj}
\#\# 발생원인\\
\\
\#\#\# 회전 좌표계\\
\\
\#\#\#\# 회전좌표계\\
좌표계 x, y, z와 좌표계 x', y', z'을 보자 두 좌표계의 원점은 같다. 각각의 경우에 대해 벡터 $\mathbf{r}$.은 두 좌표계에서 다음과 같이 표시된다.\\
\\
$\mathbf{r} = x\hat x  + y\hat y  + z\hat z  $. (x, y, z 좌표)\\
\\
$\mathbf{r} = x'\hat x'  + y'\hat y'  + z'\hat z'  $. (x', y', z' 좌표계)\\
\\
벡터의 내적을 이용해 x, y, z를 ($\ x', \hat x', \hat x $.), ($\ y', \hat y', \hat y $.), ($\ z', \hat z', \hat z $.)으로 표현할 수 있다. 내적의 방법은 다음과 같다.\\
\\
$  \mathbf{r}{\hat x} =x = (x'\hat x'  + y'\hat y'  + z'\hat z')(\hat x) = x'(\hat x' \hat x)  + y'(\hat y' \hat x)  + z'(\hat z' \hat x) $.\\
\\
$  \mathbf{r} {\hat y} =y = (x'\hat x'  + y'\hat y'  + z'\hat z')(\hat y) = x'(\hat x' \hat y)  + y'(\hat y' \hat y)  + z'(\hat z' \hat y) $.\\
\\
$  \mathbf{r} {\hat z} =z = (x'\hat x'  + y'\hat y'  + z'\hat z')(\hat z) = x'(\hat x' \hat z)  + y'(\hat y' \hat z)  + z'(\hat z' \hat z) $.\\
\\
으로 표현되는 것을 확인할 수 있다.
\end{CJK}
\end{tcolorbox}
}

\caption{
    Example of suboptimal and our improved extraction from open-source Wikipedia dataset (\url{https://huggingface.co/datasets/wikimedia/wikipedia}).
    The original content is from the Korean Wikipedia article on the Coriolis effect.
}
\end{figure}