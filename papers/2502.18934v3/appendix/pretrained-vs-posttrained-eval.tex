\subsection{Comparison between pre-trained models and post-trained models}\label{appendix:pre-vs-post}


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcc|cccccc}
\toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Tokens}} & \multirow{2}{*}{\textbf{Category}}  & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} \\
    & & & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} \\
\midrule
\midrule
\multirow{2}{*}{Kanana Flag 32.5B}   & \multirow{2}{*}{3.2T} & base     & 77.68 & 62.10 & 90.47 & 51.22 & 63.40 & 70.05 \\
                                     & & instruct & 77.84 & 62.08 & 89.37 & 64.63 & 73.00 & 84.08 \\
\midrule
\multirow{2}{*}{Llama 3.1 70B}       & \multirow{2}{*}{15T} & base     & 78.93 & 53.00 & 76.35 & 57.32 & 66.60 & 81.73 \\
                                     & & instruct & 82.42 & 52.80 & 76.08 & 78.05 & 70.40 & 86.66 \\
\midrule
\multirow{2}{*}{Qwen2.5 32B}         & \multirow{2}{*}{18T} & base     & 83.10 & 63.15 & 75.16 & 50.00 & 73.40 & 82.41 \\
                                     & & instruct & 83.41 & 61.20 & 74.61 & 54.88 & 73.00 & 76.27 \\
\midrule
\multirow{2}{*}{Gemma 2 27B}         & \multirow{2}{*}{13T} & base     & 75.45 & 51.16 & 69.11 & 51.22 & 64.60 & 74.37 \\
                                     & & instruct & 76.39 & 51.49 & 68.84 & 71.34 & 66.20 & 84.46 \\
\midrule
EXAONE-3.5-32B                       & 6.5T & instruct & 72.68 & 46.36 & 82.22 & 74.39 & 67.80 & 55.50  \\
\midrule
Aya Expanse 32B                      & - & instruct & 74.52 & 49.57 & 80.66 & 12.20 & 60.40 & 85.97 \\
\midrule
\midrule
\multirow{2}{*}{Kanana Essence 9.8B} & \multirow{2}{*}{3.2T} & base     & 67.61 & 50.57 & 84.97 & 40.24 & 53.60 & 63.61 \\
                                     & & instruct & 66.45 & 49.95 & 82.95 & 61.59 & 51.60 & 76.04 \\
\midrule
\multirow{2}{*}{Llama 3.1 8B}        & \multirow{2}{*}{15T} & base     & 65.18 & 41.02 & 61.78 & 35.37 & 48.60 & 50.87 \\
                                     & & instruct & 68.17 & 41.22 & 64.44 & 59.76 & 58.00 & 69.52 \\
\midrule
\multirow{2}{*}{Qwen2.5 7B}          & \multirow{2}{*}{18T} & base     & 74.19 & 51.68 & 67.46 & 56.71 & 63.20 & 83.85 \\
                                     & & instruct & 74.23 & 50.13 & 65.72 & 65.85 & 31.60 & 77.56 \\
\midrule
\multirow{2}{*}{Gemma 2 9B}          & \multirow{2}{*}{8T$^\dag$} & base     & 70.34 & 48.18 & 66.18 & 37.20 & 53.60 & 68.16 \\
                                     & & instruct & 72.30 & 46.56 & 66.73 & 56.10 & 57.60 & 80.12 \\
\midrule
EXAONE-3.5-7.8B                      & 9T & instruct & 65.36 & 45.30 & 77.54 & 70.73 & 61.60 & 64.67 \\
\midrule
Aya Expanse 8B                       & - & instruct & 62.52 & 40.11 & 71.95 &  7.93 & 47.40 & 75.97 \\
\midrule
\midrule
\multirow{2}{*}{Kanana Nano 2.1B}    & \multirow{2}{*}{300B$^\dag$} & base     & 54.83 & 44.80 & 77.09 & 31.10 & 46.20 & 46.32 \\
                                     & & instruct & 53.67 & 42.92 & 77.17 & 54.88 & 55.00 & 64.37 \\
\midrule
\multirow{2}{*}{Llama 3.2 3B}        & \multirow{2}{*}{9T$^{\dag\ddag}$} & base     & 56.40 & 35.57 & 47.66 & 25.61 & 39.00 & 27.37 \\
                                     & & instruct & 60.60 & 35.44 & 48.21 & 49.39 & 49.00 & 58.76 \\
\midrule
\multirow{2}{*}{Qwen2.5 3B}          & \multirow{2}{*}{18T} & base     & 65.57 & 45.28 & 61.32 & 37.80 & 55.60 & 69.07 \\
                                     & & instruct & 66.47 & 44.51 & 60.77 & 50.61 & 54.60 & 11.37 \\
\midrule
\multirow{2}{*}{Gemma 2 2B}          & \multirow{2}{*}{2T$^\dag$} & base     & 52.89 & 30.67 & 45.55 & 20.12 & 28.20 & 24.72 \\
                                     & & instruct & 57.04 & 33.48 & 49.77 & 23.78 & 37.80 & 44.05 \\
\midrule
EXAONE-3.5-2.4B                      & 6.5T & instruct & 59.27 & 43.58 & 68.65 & 63.41 & 58.40 & 53.07 \\
\bottomrule
\end{tabular}
}
\caption{
$^\dag$ For distilled models, distillation tokens are only counted
$^\ddag$ Information from \url{https://huggingface.co/meta-llama/Llama-3.2-3B}
}\label{table:appendix-pre-vs-post}
\end{table}

