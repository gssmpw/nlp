\subsection{Details of pruning and distillation}\label{appendix:pd-details}
 
The hyperparameters differ from those used in pre-training from scratch. We apply a cosine learning rate schedule \citep{loshchilov2017sgdr} with an initial learning rate of $1.2 \times 10^{-4}$, batch size of 512, sequence length of 8192, and a warmup phase of 100 steps.
Following the recommendation of Minitron \citep{muralidharan2024compact, sreenivas2024llm}, we employ KL divergence \citep{Kullback1951OnIA} on final logits as the sole loss function.
Additionally, we conclude training early during ablation studies, as pruned models quickly regain performance and the ranking of ablation options rapidly stabilizes.


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccccc|c}
\toprule
    \multirow{2}{*}{\textbf{Models}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAE-RAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}} \\
    & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
\midrule
Kanana 4.5B &  59.74 &  48.09 &  82.58 &  34.76 &  48.60 &  57.01 &  55.13 \\ 
\midrule
Kanana 3B &  58.21 &  \bfseries 47.55 &  \bfseries79.19 &  34.15 &  45.90 &  53.75 &  53.13 \\
Llama 3.2 3B & 56.40 & 35.57 & 47.66 & 25.61 & 39 & 27.37 & 38.60 \\
Qwen2.5 3B & \bfseries 65.57 & 45.28 & 61.32 & \bfseries 37.80 & \bfseries 55.60 & \bfseries 69.07 & \bfseries 55.77 \\
\midrule
Kanana 2.1B &  54.83 &  \bfseries 44.80 &  \bfseries 77.09 &  31.10 &  \bfseries 46.20 &  46.32 &  \bfseries 50.06 \\
Kanana 1.3B &  53.55 &  39.91 &  72.59 &  28.05 &  39.60 &  36.01 &  44.95 \\
Gemma 2 2B & 52.89 & 30.67 & 45.55 & 20.12 & 28.20 & 24.72 & 33.69 \\
SmolLM2-1.7B & 50.08 & 24.36 & 30.52 & 0.61 & 34.00 & 32.00 & 28.60 \\
Qwen2.5 1.5B & \bfseries 60.86 & 36.63 & 49.68 & \bfseries 37.20 & 44.00 & 62.09 & 48.41 \\
Llama 3.2 1B & 31.51 & 26.46 & 23.10 & 18.90 & 27.60 & 6.14 & 22.29 \\
\midrule
Kanana 635M &  46.28 &  \bfseries 34.60 &  \bfseries 62.69 &  23.17 & \bfseries 31.40 & 19.26 & \bfseries 36.23 \\
Kanana 385M &  41.16 &  31.70 &  47.94 &  18.90 &  24.00 &  10.83 &  29.09 \\
Kanana 192M &  26.11 &  30.16 &  19.71 &  12.80 &  12.40 &  2.43 &  17.27 \\ 
Qwen2.5 0.5B & \bfseries 47.59 & 31.79 & 31.44 & \bfseries 28.66 & 31.00 & \bfseries 35.10 & 34.26 \\ 
SmolLM2-360M & 24.84 & 15.14 & 21.26 & 0.00 & 19.00 & 3.94 & 14.03 \\
SmolLM2-135M & 25.28 & 25.73 & 20.71 & 0.00 & 3.40 & 1.29 & 12.74 \\
\bottomrule
\end{tabular}
}
\caption{
Performance of our models obtained with iterative pruning \& distillation, compared to similar-sized open-source base models.
}\label{tab:pd-base-all}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/pretraining/gqa-pruning.pdf}
    \caption{Illustration of ensuring query-key-value alignment in GQA pruning.}
    \label{fig:pd-gqa}
\end{figure}


\begin{table}[h]
    \centering
    \resizebox{0.85\textwidth}{!}{
        \begin{tabular}{cc|ccc|c}
        \toprule
        \multirow{2}{*}{\textbf{GQA alignment}} & \multirow{2}{*}{\textbf{Swiglu importance}} & \multicolumn{3}{c|}{\textbf{Aggregation}} & \multirow{2}{*}{\textbf{Avg}} \\ 
        & & \small{Layer} & \small{Batch} & \small{Sequence} & \\
        \midrule
        \textbf{\cmark} & \textbf{intermediate states} & \textbf{sum} & \textbf{l2norm} & \textbf{avg} & \textbf{36.41} \\
        \textcolor{red}\xmark & intermediate states & sum & l2norm & avg & 20.13 \\
        \cmark & avg of gate, up states & sum & l2norm & avg & 36.04 \\
        \cmark & intermediate states & \textcolor{red}\xmark & l2norm & avg & 13.81 \\
        \cmark & intermediate states & sum & avg & avg & 35.65\\
        \cmark & intermediate states & sum & l2norm & l2norm & 34.25 \\
        \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on importance scoring details, followed by training the same 1.3B architecture with 25B tokens.}
    \label{tab:pd-score-detail}
\end{table}


\begin{table}[h]
    \centering
    \resizebox{0.75\textwidth}{!}{
        \begin{tabular}{cccc|c}
        \toprule
        \multirow{2}{*}{\textbf{Hidden}} & \multirow{2}{*}{\textbf{Intermediate}} & \multirow{2}{*}{\textbf{Query heads}} & \textbf{Non-embedding} & \multirow{2}{*}{\textbf{Avg}} \\
         & & & \textbf{parameters} & \\
        \midrule
        1280 & 5120 & \textbf{24} & 0.96B & \textbf{32.81} \\
        1280 & 5760 & 16 & 0.96B & 28.71 \\
        1280 & 5760 & \textbf{24} & 1.04B & \textbf{34.27}\\
        1536 & 4608 & 16 & 0.98B & 31.99 \\
        1536 & 4608 & \textbf{24} & 1.08B & \textbf{35.10} \\
        1536 & 5376 & 8  & 0.99B & 24.39 \\
        1536 & 5376 & 16 & 1.09B & 32.39 \\
        1536 & 6144 & 8  & 1.11B & 25.91 \\
        \midrule
        1024 & 3072 & 24$\xrightarrow{}$16 & 1.08B$\xrightarrow{}$504M & 20.85 \\
        1024 & 3072 & \textbf{16$\xrightarrow{}$16} & 1.09B$\xrightarrow{}$504M & \textbf{21.78} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on model architectures, using 25B training tokens.}
    \label{tab:pd-attn-heads}
\end{table}


\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cccccc|c}
    \toprule
    \multirow{2}{*}{\textbf{Embedding}} & \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAERAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}}\\
    & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
\midrule
    tied & 49.07 & 40.41 & 70.49 & 30.49 & 40.60 & 38.21 & 44.88 \\
    untied & 49.88 & 39.61 & 70.21 & 29.88 & 40.20 & 36.92 & 44.45 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on tying input and output embeddings by averaging, using 63B training tokens. The rest of the architecture remains unchanged, with 1.86B non-embedding parameters.}
    \label{tab:pd-tie}
\end{table}


\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ll|cccccc|c}
    \toprule
    \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Data}}& \textbf{MMLU} & \textbf{KMMLU} & \textbf{HAERAE} & \textbf{HumanEval} & \textbf{MBPP} & \textbf{GSM8K} & \multirow{2}{*}{\textbf{Avg}}\\
    & & \textit{5-shot} & \textit{5-shot} & \textit{5-shot} & \textit{0-shot} & \textit{3-shot} & \textit{5-shot} & \\
\midrule
    1.3B & stage2 & 39.52 & 26.65 & 49.77 & \textbf{25.00} & 32.40 & \textbf{21.00} & 32.39\\
    1.3B & \textbf{stage2 en++} & \textbf{44.00} & \textbf{33.90} & \textbf{62.42} & 23.78 & \textbf{33.80} & 20.55 & 
    \textbf{36.41}\\
    \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on distillation data, using 25B training tokens.}
    \label{tab:pd-distill-data}
\end{table}

\clearpage
