\section{Introduction}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/introduction/flops-vs-mmlu.pdf}
    \caption{
    Performance to pre-training computational cost for Kanana and comparable models.
    We measure computational cost in FLOPs (Floating Point Operations), which is approximately calculated as 6 $\times$ training tokens $\times$ model size \citep{kaplan2020scalinglaws}.
    We only calculate student training FLOPs for distillation models.
    Obviously, Kanana models achieves decent performance given their limited computational cost.
    }
    \label{fig:flops-vs-mmlu}
\end{figure}


Recent breakthroughs in large language models (LLMs) have been driven by increasing training data~\citep{chinchilla} and model parameters~\citep{gpt3, kaplan2020scalinglaws, palm}.
However, advances have also introduced substantial computational costs that reach millions of dollars \citep{llama3}, which poses a challenge to the community on developing LLMs \textit{from scratch}.
As a result, reducing computational cost has emerged as a crucial problem in order to popularize the development of LLMs for both academia and industry \citep{zhao2024galore, fishman2025scaling-fp8, wang2025fp4-training}. 
To this end, recent works have presented various solutions to the computation problem in model architectures and scaling \citep{deepseek-v2, kim2023solar, muralidharan2024compact}, through data \citep{fineweb-edu, ask-llm}, and through training strategies \citep{deepseek-llm, minicpm}.

As the product of our endeavor to address the computational challenges, we introduce \textit{Kanana} model family, developed using only a fraction of computational cost while maintaining performance compared to those of the state-of-the-art (SOTA) open LLMs.
The family of models includes pre-trained base model and post-trained instruction models in sizes of \{2.1B, 9.8B, 32.5B\}. 
We show in \autoref{fig:flops-vs-mmlu} that Kanana models establish a new Pareto frontier in the computational cost of the train time versus the performance.

In the pre-training phase, as it accounts for the majority of the training costs for LLMs, we focus on reducing its computational demands while maintaining performance.
Since the cost of the pre-training phase primarily arises from the large dataset size and model scale, we reduce it by improving both data efficiency and training efficiency.
To improve data efficiency, we carefully curate a training dataset of 3 trillion tokens, enabling our models to achieve competitive performance despite using a smaller dataset than SOTA pre-trained models.
For training efficiency, we employ cost-effective techniques such as staged pre-training \citep{minicpm, ibrahim2024simple-and-scalable} and depth up-scaling \citep{kim2023solar} to reduce computational costs associated with model size.
From the models obtained, we extend pruning and distillation technique \citep{muralidharan2024compact} to train smaller models using only a handful subset of the pre-training data.



Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models.
To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization.
As a result, our instruction models achieve competitive performance to that of SOTA models on various tasks, including English/Korean chat, general knowledge reasoning, instruction following, code generation, and mathematical problem-solving.
In addition, we adapt instruction models to develop embedding models, retrieval-augmented generation models, and function-calling models.
