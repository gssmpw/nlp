\section{Conclusion}


In this report, we present Kanana, a family of large language models available in sizes of \{2.1B, 9.8B, 32.5B\}, with a focus on the cost-effective training procedure compared to other prominent open models.
We emphasize the strong bilingual capability of Kanana models, showcasing state-of-the-art performance on Korean benchmarks of KMMLU, HAE-RAE, and KoMT-Bench and competitive results on various English benchmarks.
However, we also acknowledge the limitations of Kanana models in overall performance on small scale models sizes, particularly in math domains.
To address the limitations, we plan to improve small models and the math ability of all models through data quality and mixture.
To further our commitment in cost-effective training, we intend to explore strategical approaches such as formulating scaling laws and other training methodologies as possible future directions.
% Furthermore, in order to continue our endeavor towards cost-efficient training of LLMs, we view strategical approaches such as formulating scaling laws as possible future directions.
Additionally, we aim to expand the linguistic ability from bilingual to multilingual prioritizing the intuition of treating the underrepresented languages covered in this report.
% We will also continue to fortify and produce smaller scale models. 
By continuing to build on these efforts, we aspire to make advancements in the field of large language models, balancing performance with efficiency and broadening the linguistic scope of our models.