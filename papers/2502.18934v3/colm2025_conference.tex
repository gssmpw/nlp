
\documentclass{article} % For LaTeX2e
% \usepackage[submission]{colm2025_conference}
\usepackage[table]{xcolor}
\usepackage[preprint]{colm2025_conference}
\usepackage{multirow}
\usepackage{microtype}
\usepackage{url}
\usepackage{booktabs}
\usepackage{makecell}
% \usepackage{lineno}
\usepackage{graphicx}
\usepackage[font=small]{caption} % 캡션 폰트 크기 호조절
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{footnote}
\usepackage{CJKutf8}
\usepackage{kotex}
\usepackage{spverbatim}

\usepackage{hyperref}

\usepackage{tikz}
\usetikzlibrary{tikzmark,positioning}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\definecolor{yellow}{RGB}{255, 230, 100}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}



\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\title{Kanana: Compute-efficient Bilingual Language Models}


\author{\textbf{Kanana LLM Team} \thanks{A detailed contributor list can be found in the last section of the main paper.} \\
\texttt{kanana-llm@kakaocorp.com} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle


\begin{abstract}
We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. 
The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size.
The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.
Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users.
Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling.
The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.
\end{abstract}


\input{introduction}

\input{pretraining}

\input{posttraining}

\input{adaptation}

\input{conclusion}

\input{contributors_and_acknowledgements}

% \input{colm2025_legacy}


\newpage
\begin{CJK}{UTF8}{mj}
\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}
\end{CJK}

\newpage
\appendix
\input{appendix}


\end{document}
