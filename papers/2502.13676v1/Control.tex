\section{Data-enabled policy optimization for autonomous bicycle  control}
\label{sec:3}
This section first describes a brief overview of the linear quadratic regulator (LQR). Then, we propose a data-enabled policy optimization with a forgetting factor for adaptive learning of the LQR based on~\cite{zhao2023data, zhao2024data}. 

\subsection{The linear quadratic regulator}
	Consider a linear time-invariant system
	\begin{equation}\label{equ:sys}
	\left\{\begin{aligned}
	x\dt{t+1} & =A x\dt{t}+B u\dt{t}+w\dt{t} \\
	h\dt{t} & =\begin{bmatrix}
	Q^{1 / 2} & 0 \\
	0 & R^{1 / 2}
	\end{bmatrix}
	\begin{bmatrix}
	x\dt{t}  \\
	u\dt{t} 
	\end{bmatrix}
	\end{aligned}\right. .
	\end{equation}
	Here, $x\dt{t}$ is the state, $u\dt{t}$ is the control input, $w\dt{t}$ represents the noise, and $h\dt{t}$ is the performance signal of interest, $(A,B)$ are controllable, and the weighting matrices $(Q, R)$ are positive definite. 
	
    The LQR problem aims to find an optimal state-feedback gain $K\in \mathbb{R}^{m\times n}$ that minimizes the $\mathcal{H}_2$-norm of the transfer function $\mathscr{T}(K):w \rightarrow h$ of the closed-loop system
	\begin{equation}
	\begin{bmatrix}
	x\dt{t+1}  \\
	h\dt{t} 
	\end{bmatrix}=\begin{bmatrix}
	A+BK & I_n \\
	\hline \begin{bmatrix}
	Q^{1 / 2} \\
	R^{1 / 2} K
	\end{bmatrix} & 0
	\end{bmatrix}\begin{bmatrix}
	x\dt{t}  \\
	w\dt{t} 
	\end{bmatrix}.
	\end{equation}
	When $A+BK$ is stable, it holds that \cite{anderson2007optimal}
	\begin{equation}\label{equ:transfer}
	\|\mathscr{T}(K)\|_2^2  = \text{Tr}((Q+K^{\top}RK)\Sigma_K)=:C(K),
	\end{equation}
	where $\Sigma_K$ is the closed-loop state covariance matrix obtained as the positive definite solution to the Lyapunov equation
	\begin{equation}\label{equ:Sigma}
	\Sigma_K = I_n + (A+BK)\Sigma_K (A+BK)^{\top}.
	\end{equation}
	We refer to $C(K)$ as the LQR cost and to (\ref{equ:transfer})-(\ref{equ:Sigma}) as a \textit{policy parameterization} of the LQR.

The optimal LQR gain $K^*$ is unique and can be found by, e.g., solving an algebraic Riccati equation with $(A,B)$~\cite{anderson2007optimal}. When $(A,B)$ is unknown, data-driven methods can be used to learn the LQR gain from input-state data. In the sequel, we propose a covariance parameterization method for direct data-driven learning of the LQR.

\subsection{Data-driven covariance parametrization of the LQR with exponential weighted data}\label{sec:3-B}
Consider the $t$-long time series of states, inputs, noises, and successor states
\begin{align}
X_{0,t} &:= \begin{bmatrix}
x\dt{0} & x\dt{1} & \dots& x\dt{t-1} 
\end{bmatrix}\in \mathbb{R}^{n\times t},\nonumber\\
U_{0,t} &:= \begin{bmatrix}
u\dt{0} & u\dt{1} & \dots& u\dt{t-1} 
\end{bmatrix}\in \mathbb{R}^{m\times t}, \label{equ:dataMat}\\
W_{0,t} &:= \begin{bmatrix}
w\dt{0} & w\dt{1} & \dots& w\dt{t-1} 
\end{bmatrix}\in \mathbb{R}^{n\times t}, \nonumber\\
X_{1,t} &:= \begin{bmatrix}
x\dt{1} & x\dt{2} & \dots& x\dt{t} 
\end{bmatrix}\in \mathbb{R}^{n\times t}, \nonumber
\end{align}
which satisfy the system dynamics
\begin{equation}\label{equ:dynamics}
X_{1,t} = AX_{0,t}+ BU_{0,t} + W_{0,t}.
\end{equation}


Assume that the data is {\em persistently exciting (PE)} \cite{willems2005note}, i.e., the block matrix of input and state data
\begin{equation}
D_{0,t} := 
\begin{bmatrix}
    U_{0,t} \\
    X_{0,t}
\end{bmatrix}
\end{equation}
has full row rank
\begin{equation}\label{equ:rank}
\text{rank}(D_{0,t}) = m+n.
\end{equation}
Define the covariance of exponentially weighted data as
\begin{equation}
    \Phi_{t} := \frac{1}{t}D_{0,t} S_{\lambda} D_{0,t}^{\top},
\end{equation}
where $\lambda \in (0,1)$ is a forgetting factor and $S_{\lambda} := \text{diag}\{ \lambda^{t-1}, \lambda^{t-2},\dots, 1\}$. Compared with \cite{zhao2024data}, the forgetting factor here makes the weight of past data decay exponentially, such that the sample covariance can also reflect and adapt to the behavior of time-varying or nonlinear systems. 


Since $D_{0,t}$ has full row rank and $S_{\lambda}\succ 0$, the covariance matrix is positive definite, i.e., $\Phi_{t} \succ 0$. Then, for any gain $K$, there exist a matrix $V$ such that
\begin{equation}\label{equ:forget}
\begin{bmatrix}
K \\
I_n
\end{bmatrix}=  \Phi_{t} V.
\end{equation}
We refer to \eqref{equ:forget} as the \textit{covariance parameterization} with exponentially weighted data and to $V\in \mathbb{R}^{(n+m)\times n}$ as the \textit{parameterized policy}.

With \eqref{equ:forget}, the LQR problem \eqref{equ:transfer}-\eqref{equ:Sigma} can be expressed by raw data matrices $(X_{0,t}, U_{0,t}, X_{1,t})$ and the optimization matrix $V$. For brevity, let $\overline{X}_{0,t}= X_{0,t}S_{\lambda}D_{0,t}^{\top}/t$ and $\overline{U}_{0,t}=  U_{0,t}S_{\lambda}D_{0,t}^{\top}/t$ be a partition of $\Phi_t$, and let
$\overline{W}_{0,t}=  W_{0,t}S_{\lambda}D_{0,t}^{\top}/t$ be the noise-state-input covariance, and finally define the covariance with respect to the successor state as $\overline{X}_{1,t}=  X_{1,t}S_{\lambda}D_{0,t}^{\top}/t$.
Then, the closed-loop matrix can be written as
\begin{equation}
A+BK=[B,A]\begin{bmatrix}
K \\
I_n
\end{bmatrix}\overset{\eqref{equ:forget}}{=}[B,A]\Phi_t V\overset{\eqref{equ:dynamics}}{=}(\overline{X}_{1,t} - \overline{W}_{0,t})V.
\end{equation}
Following the certainty-equivalence principle~\cite{dorfler2021certainty}, we disregard the
unmeasurable $\overline{W}_{0,t}$ for the design and use $\overline{X}_{1,t}V$ as the closed-loop matrix. After substituting $A+BK$  with $\overline{X}_{1,t}V$ in (\ref{equ:transfer})-(\ref{equ:Sigma}) and leveraging \eqref{equ:forget}, the LQR problem becomes 
\begin{equation}\label{prob:equiV}
\begin{aligned}
&\mathop{\text {minimize}}\limits_{V}~J_t(V) :=\text{Tr}\left((Q+V^{\top}\overline{U}_{0,t}^{\top}R\overline{U}_{0,t}V)\Sigma_t(V)\right),\\
&\text{subject to}~ ~\overline{X}_{0,t}V= I_n,
\end{aligned}
\end{equation}
where $\Sigma_t(V) = I_n + \overline{X}_{1,t}V\Sigma_t(V) V^{\top}\overline{X}_{1,t}^{\top}$ is a covariance parameterization of \eqref{equ:Sigma},
and the original gain matrix can be recovered as $K = \overline{U}_{0,t}V$. We refer to (\ref{prob:equiV}) as the covariance-parameterized LQR problem, which is direct data-driven and does not involve any explicit SysID.



\subsection{Data-enabled policy optimization for adaptive LQR control with exponentially weighted data}
In previous work \cite{zhao2023data,zhao2024data}, a data-enabled policy optimization (DeePO) method for direct adaptive learning of the LQR was proposed, where the control policy is parameterized by sample covariance and updated recursively using gradient methods. In this subsection, we propose a DeePO algorithm based on our covariance parameterization with exponentially weighted data \eqref{equ:forget}, detailed in Algorithm \ref{alg:deepo}.

Algorithm \ref{alg:deepo} alternates between control (line 2) and policy update (lines 3-6).
The DeePO algorithm uses online gradient descent of \eqref{prob:equiV} to recursively update $V$. 
At time $t$, we apply the linear state feedback policy $u\dt{t}=K_tx\dt{t}  + e\dt{t} $ for control and observe the new state $x\dt{t+1} $, where $e\dt{t}$ is a probing noise used to ensure the PE rank condition \eqref{equ:rank}. To update the policy, we first use $(X_{0,t+1}, U_{0,t+1}, X_{1,t+1})$ to formulate the covariance-parameterized LQR problem \eqref{prob:equiV}. Then, instead of solving this optimization problem optimality, we only take a single step of projected gradient descent towards its solution in \eqref{equ:pro_gd}. Here, the projection  
\begin{equation}
\Pi_{\overline{X}_{0,t+1}}: = I_{n+m}-\overline{X}_{0,t+1}^{\dagger}\overline{X}_{0,t+1}
\end{equation}
onto the nullspace of $\overline{X}_{0,t+1}$ is to ensure the subspace constraint in \eqref{prob:equiV}.
 Define the feasible set of \eqref{prob:equiV} (i.e., the set of stable closed-loop matrices) as $\mathcal{S}_t:= \{V\mid \overline{X}_{0,t}V =I_n,  \rho (\overline{X}_{1,t}V)<1\}$. Then, the gradient can be computed as follows.
\begin{lemma}[\cite{zhao2024data}]\label{lem:gradient}
For $V\in \mathcal{S}_t$, the gradient of $J_t(V)$ with respect to $V$ is given by
	\begin{equation}\label{equ:pg}
	\nabla J_t(V) = 2 \left(\overline{U}_{0,t}^{\top}R\overline{U}_{0,t}+\overline{X}_{1,t}^{\top}P_t\overline{X}_{1,t}\right)V \Sigma_t(V),
	\end{equation}
	where $P_t$ satisfies the Lyapunov equation 
	\begin{equation}
	P_t = Q + V^{\top}\overline{U}_{0,t}^{\top}R\overline{U}_{0,t}V + V^{\top}\overline{X}_{1,t}^{\top}P_t\overline{X}_{1,t}V.
	\end{equation}
\end{lemma}
 


Algorithm \ref{alg:deepo} is \textit{direct and adaptive} in the sense that it directly uses online closed-loop data to update the policy. Thanks to the forgetting factor, it can rapidly adapt to changes in system behavior reflected in the data. 
As in \cite{zhao2024data}, Algorithm \ref{alg:deepo} can also be implemented recursively. We write the sample covariance  recursively as
\begin{equation}\label{equ:recur}
    \Phi_{t+1} = \frac{\lambda t}{t+1} \Phi_{t} + \frac{1}{t+1} \phi_t\phi_t^{\top},
\end{equation}
where $\phi_t = [u_t^\top, x_t^\top]^\top$. By the Sherman-Morrison formula~\cite{sherman1950adjustment}, its inverse $\Phi_{t+1}^{-1}$ satisfies
\begin{equation}
    \Phi_{t+1}^{-1} = \frac{t+1}{\lambda t}\left(\Phi_{t}^{-1} - \frac{\Phi_{t}^{-1}\phi_t\phi_t^{\top}\Phi_{t}^{-1}}{\lambda t+\phi_t^{\top}\Phi_{t}^{-1}\phi_t}\right).
\end{equation}
Furthermore, the rank-one update of the parameterized policy is given by
\begin{align}
V_{t+1}&= \frac{t+1}{t}\left(\Phi_{t}^{-1} - \frac{\Phi_{t}^{-1}\phi_t\phi_t^{\top}\Phi_{t}^{-1}}{t+\phi_t^{\top}\Phi_{t}^{-1}\phi_t}\right) \Phi_{t} V_{t}' \nonumber \\ 
&= \frac{t+1}{\lambda t} \left(V_{t}' - \frac{\Phi_{t}^{-1}\phi_t\phi_t^{\top}V_{t}'}{\lambda t+\phi_t^{\top}\Phi_{t}^{-1}\phi_t}\right),
\end{align}
where $\Phi_{t}^{-1}$ and $V_{t}'$ are given from the last iteration.

\begin{remark}
Using the forgetting factor $\lambda$ may asymptotically lead to failure of the rank-one update as time tends to infinity. To see this, we notice that the covariance update in
\eqref{equ:recur} satisfies stable linear dynamics. This implies a loss of persistency of excitation and $\Phi_t$ will tend to zero, and hence $\Phi^{-1}_t$ will grow to infinity. A simple remedy is to reset the covariance $\Lambda_{t}$  occasionally, i.e., set $\Lambda_{t}=I_{n+m}, \forall t\in \{T,2T,\dots\}$. Since the autonomous bicycle operates only for a finite time, we do not reset the covariance in our subsequent experiments in Section \ref{sec:4}. Another approach is to use sliding window data rather than exponentially weighted data for the covariance parameterization \eqref{equ:forget}, where a key is to select an optimal window size to balance data informativity and adaptation efficiency. We leave this exploration to future work.
\qed
\end{remark}

\begin{remark}
    The stepsize $\eta_t$ should be set according to the signal-to-noise ratio (SNR) of online data. For example, when the SNR is large, we are confident with the gradient direction, and the stepsize can be chosen more aggressively; on the contrary, when the SNR is small, the stepsize should be small to prevent the policy from moving out of the stability region. To this end, we set the stepsize as 
    \begin{equation}    
    \eta_t = \frac{\eta_0}{\left\|\overline{U}_{0,t}\Pi_{\overline{X}_{0,t}}\overline{U}_{0,t}^{\top}\right\|}, ~t\geq t_0,
    \end{equation}
    where $\eta_0$ is a constant, and the denominator is used to quantify the SNR. Another motivation of the denominator is from \cite[Lemma 3]{kang2024linear}, which reveals the equivalence between data-enabled and model-based policy gradients up to the data matrix $\overline{U}_{0,t}\Pi_{\overline{X}_{0,t}}\overline{U}_{0,t}^{\top}$. \qed
\end{remark}
 

\begin{algorithm}[t]
	\caption{DeePO for direct adaptive LQR control}
	\label{alg:deepo}
	\begin{algorithmic}[1]
		\Require Offline data $(X_{0,t_0}, U_{0,t_0}, X_{1,t_0})$, an initial policy $K_{t_0}$, and a stepsize $\eta$.
		\For{$t=t_0,t_0+1,\dots$}
		\State Apply $u\dt{t} =K_tx\dt{t}  + e\dt{t} $ and observe $x\dt{t+1} $.
        \State Update covariance matrices $\Phi_{t+1}$ and $\overline{X}_{1,t+1}$.
		\State \textbf{Policy parameterization:} given $K_{t}$, solve $V_{t+1}$ via 
		$$
		V_{t+1} =\Phi_{t+1}^{-1} \begin{bmatrix}
		K_{t} \\
		I_n
		\end{bmatrix}.
		$$
		\State \textbf{Update of the parameterized policy:} perform one-step projected gradient descent
		\begin{equation}\label{equ:pro_gd}
		V_{t+1}' = V_{t+1} - \eta_t  \Pi_{\overline{X}_{0,t+1}} \nabla J_{t+1}(V_{t+1}),
		\end{equation} 
		where the gradient $\nabla J_{t+1}(V_{t+1})$ is given by Lemma \ref{lem:gradient}.
		\State \textbf{Gain update:} update the control gain by 
		$$
		K_{t+1} = \overline{U}_{0,t+1}V_{t+1}'.
		$$
		\EndFor	
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:deepo} requires the initial policy to be stabilizing. A potential approach is to solve the  covariance-parameterized LQR \eqref{prob:equiV} with the offline data $(X_{0,t_0}, U_{0,t_0}, X_{1,t_0})$. However, due to the nonlinearity in the system dynamics of the autonomous bicycle, the solution of covariance-parameterized LQR may be destabilizing. Next, we propose a robustness-promoting regularizer for the covariance parameterization to obtain a stabilizing initial policy.
 
\subsection{Learning an initial stabilizing policy using robustness promoting regularization}


	The feasibility of the covariance-parameterized LQR problem \eqref{prob:equiV} depends on that of the Lyapunov equation
	\begin{equation}\label{equ:lyap}
		\Sigma = I_n + \overline{X}_1V\Sigma V^{\top}\overline{X}_1^{\top},
	\end{equation}
	where $\overline{X}_1V$ is regarded as the closed-loop matrix. However, having assumed certainty-equivalence by the covariance parameterization \eqref{equ:forget} and the relation $A+BK = (\overline{X}_1 - \overline{W}_0)V$, the Lyapunov equation that should be met is
	\begin{equation}\label{equ:true_lyap}
		\Sigma = I_n + (\overline{X}_1 - \overline{W}_0)V\Sigma V^{\top}(\overline{X}_1 - \overline{W}_0)^{\top}.
	\end{equation}
	The gap between the right-hand side of \eqref{equ:lyap} and \eqref{equ:true_lyap} is
	\begin{equation}\label{equ:diff}
		\begin{aligned}
		&\overline{W}_0 V\Sigma V^{\top}  \overline{W}_0^{\top} - \overline{W}_0 V\Sigma V^{\top} \overline{X}_1^{\top} -   \overline{X}_1V\Sigma V^{\top} \overline{W}_0^{\top} \\
		&= \frac{1}{t^2}  W_0D_0^{\top}V\Sigma V^{\top}D_0W_0^{\top} \\
		&-\frac{1}{t^2}(  W_0D_0^{\top}V\Sigma V^{\top}D_0X_1^{\top} +   X_1D_0^{\top}V\Sigma V^{\top}D_0W_0^{\top}).
		\end{aligned}
	\end{equation}
	To reduce the gap, it suffices to make $\text{Tr}(D_0^{\top}V\Sigma V^{\top}D_0/t)$ small. To this end, we introduce the regularizer $\text{Tr}(V\Sigma V^{\top}\Phi)$ to the covariance-parameterized LQR problem \eqref{prob:equiV}, leading to
	\begin{equation}\label{prob:regu}
	\begin{aligned}
	&\mathop{\text {minimize}}\limits_{V, \Sigma\succeq 0}~ J_t(V) + \gamma\text{Tr}(V\Sigma V^{\top}\Phi),\\
	&\text{subject to}~ ~\Sigma = I_n + \overline{X}_1V\Sigma V^{\top}\overline{X}_1^{\top},\overline{X}_0V= I_n
	\end{aligned}
	\end{equation}
	with gain matrix $K = \overline{U}_0V$, where $\gamma>0$ is the regularization coefficient. We refer to \eqref{prob:regu} as the regularized covariance parameterization of the LQR problem.

    To obtain an initial stabilizing policy for Algorithm \ref{alg:deepo}, we solve \eqref{prob:regu} with offline data $(X_{0,t_0}, U_{0,t_0}, X_{1,t_0})$.

\subsection{Control gain update rate}
Rapid changes in an adaptive control policy, $K_t$ can potentially induce oscillations and, in the worst case, render the system unstable~\cite{landau2011adaptive}. Moreover, the control policy at certain time intervals may be significantly influenced by measurement noise, meaning that updates could be driven more by noise than by the actual system dynamics. 

To address these potential issues, we propose updating the DeePO control gain less frequently than the sampling frequency. To regulate the update frequency, we introduce the parameter $\xi$, which determines the intervals at which the controller in line 6 of Algorithm~\ref{alg:deepo} is updated. For instance, if $\xi = 1$, the control gain is updated at every iteration, whereas if $\xi = 100$, the gain is updated every $100$ iterations.








