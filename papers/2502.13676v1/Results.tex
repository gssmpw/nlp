\section{Simulations and experiments}
\label{sec:4}
\definecolor{c1}{RGB}{27,158,119}
\definecolor{c2}{RGB}{217,95,2}
\definecolor{c5}{RGB}{117,112,179}
\definecolor{c4}{RGB}{231,41,138}
\definecolor{c3}{RGB}{102,166,30}
\definecolor{c6}{RGB}{230,171,2}

\colorlet{c3a}{c3!100}
\colorlet{c3b}{c3!80}
\colorlet{c3c}{c3!60}
\colorlet{c3d}{c3!40}

\colorlet{c4a}{c4!100}
\colorlet{c4b}{c4!80}
\colorlet{c4c}{c4!60}
\colorlet{c4d}{c4!40}

\colorlet{c32a}{c3!100}
\colorlet{c32b}{c3!88}
\colorlet{c32c}{c3!76}
\colorlet{c32d}{c3!64}
\colorlet{c32e}{c3!52}
\colorlet{c32f}{c3!40}

\colorlet{c42a}{c4!100}
\colorlet{c42b}{c4!88}
\colorlet{c42c}{c4!76}
\colorlet{c42d}{c4!64}
\colorlet{c42e}{c4!52}
\colorlet{c42f}{c4!40}



In this section, we first provide details of the instrumented bicycle used in the experiments. Next, we describe the simulation setup, followed by the results obtained from the simulations. Finally, we present the details of the experiments and the corresponding results.  


\subsection{Instrumented Bicycle}
The bicycle we consider in experiments is a men's model of an electric bicycle, as shown in Fig.~\ref{fig:expPlatform}. The factory-installed rear wheel motor and the battery mounted on the main frame are used to drive the bicycle forward. The rear wheel is controlled through a Phoenix Edge HV 60 AMP Electronic Speed Controller (ESC), and the rear wheel velocity is estimated using 12 evenly distributed magnets on the rear wheel and a Hall sensor. A PI controller is manually tuned to maintain an approximately constant forward velocity. An Xsens MTi-7 GNSS/INS at the bottom bracket shell measures the lean angle and rate.

Furthermore, a Dynamixel XH540-W270-T servo with a cog belt is mounted on the bicycle's main frame to control the handlebar. The servo is controlled through a velocity command, $u\dt{t}=\dot{\delta}\dt{t}$ rad/s, using an integer between $-167$ and $167$ with a resolution of approximately $0.024$ rad/s per unit. The control signal is saturated at $\pm4$ rad/s. The control algorithms are implemented, and the data is processed using ROS2 Humble running on a Raspberry Pi 4b with Ubuntu 20.04. Finally, a radio controller (RC) lets the operator send wireless commands to the RC receiver mounted on the bicycle. 

\begin{figure}[t]
    \centering
    \includegraphics{expPlatform.pdf}
    \begin{tabular}{@{}llll@{}}
        \toprule
        \multicolumn{4}{c}{\textbf{Hardware}} \\ \midrule
        \includegraphics[height=0.5cm]{main-figure0.pdf}   & RC receiver    
        & \includegraphics[height=0.5cm]{main-figure1.pdf}     & Bafang RM G040.250.DC    \\
        \includegraphics[height=0.5cm]{main-figure2.pdf}      & Raspberry Pi 4b     & 
        \includegraphics[height=0.5cm]{main-figure3.pdf} &   Xsens MTi-7  \\
        \includegraphics[height=0.5cm]{main-figure4.pdf}   & ESC    
        & \includegraphics[height=0.5cm]{main-figure5.pdf}     & Batteries    \\
        \includegraphics[height=0.5cm]{main-figure6.pdf}      & Hall sensor     & 
        \includegraphics[height=0.5cm]{main-figure7.pdf}      & Dynamixel XH540-W270-T \\ \bottomrule
        \end{tabular}
    \caption{Instrumented bicycle used in the experiments.}
    \label{fig:expPlatform}
\end{figure}

\subsection{Simulation setup}
A CAD model of the instrumented bicycle was designed using SolidWorks. The CAD model is imported into MathWorks Simscape and controlled through Simulink. The rear and front wheels are connected to the mainframe through revolute joints, where the rear joint is actuated and given a constant speed corresponding to a forward velocity of $8$ km/h. A third revolute joint connects the steering axis to the bicycle's mainframe and is actuated through the control signal $u(t) = \dot\delta(t)$. The steering dynamics are modeled using an identified steering step response matching procedure~\cite{PerssonECC}, from the control signal $u(t)$ to the steering rate $\dot\delta(t)$ the resulting transfer function is: 
\begin{equation}
    H(s) = \frac{100+s}{100}.
\end{equation}
The transfer function is placed in series with the bicycle model, as shown in Fig.~\ref{fig:ControlStructure}. The control signal is saturated at $\pm 4$ rad/s. The figure also highlights that the input signal, $u\dt{t}$, is composed of two parts. First, an inner loop control signal $u\dti{t}{i}$ from the FL controller with the parameters given in Table \ref{tab:modParam}, and second, an outer control loop signal $u\dti{t}{o}$ which originates from a persistently excited input $u\dti{t}{PE} = \mathcal{N}(0,\sigma_{PE})$ with $\sigma_{PE}=0.2$ rad/s when the switch is in position $\alpha$, and from DeePO when the switch is in position $\beta$. The lean angle, lean rate, and steering angle measurements are induced with zero-mean, normally distributed noise using $\sigma_{\varphi} = \sigma_{\delta}=0.5$ deg and $\sigma_{\dot{\varphi}} =0.5$ deg/s. The forward velocity is set to a constant value of $8$ km/h.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{controlSimulation.pdf}
    \caption{Control setup where the gray box represents the system controlled using DeePO.}
    \label{fig:ControlStructure}
\end{figure}

The tracking error is used as the state vector, i.e., $x\dti{t}{e} = [\varphi\dti{t}{r} - \varphi\dt{t}, \dot\varphi\dti{t}{r} -\dot\varphi\dt{t}]^\top$. The state variables and the persistently exciting input signal are sampled at $100$ Hz. The same execution rate is used for DeePO and the FL controller, generating a new control signal for the steering motor every $0.01$ s.

An initial simulation is performed with the switch in Fig.~\ref{fig:ControlStructure} in position $\alpha$, and the bicycle tracks a lean angle reference $\varphi\dti{t}{r}=0$ and its derivative $\dot\varphi\dti{t}{r}=0$ for 10 seconds.  We collect $T=200$ samples of the acquired data to construct $U_{0,T}, X_{0,T}, X_{1,T}$ using \eqref{equ:dataMat}. An initial policy is obtained by solving the regularized covariance-parameterized LQR problem~\eqref{prob:regu} using CVX~\cite{cvx} with $Q=\diag{[1,0.01]}$ and $R=10^{-4}$. Higher values of $\gamma$ promote robustness against uncertainties in the system, while lower values prioritize performance. In our simulations, we set it $\gamma = 1$, which balances robustness and performance in our initial control policy. 
 
 Next, the switch in Fig.~\ref{fig:ControlStructure} is set to position $\beta$, and Algorithm~\ref{alg:deepo} is used to update the control policy at every time sample, i.e., $\xi=1$ and using a forgetting factor $\lambda = 1-10^{-4}$, and learning rate $\eta=10^{-3}$. To ensure a persistently exciting input, the probing noise $e\dt{t}$ is a zero-mean normally distributed random number, which is added to the DeePO output and constructs the input to the system as:
\begin{equation}
    u\dti{t}{DeePO} = u\dti{t}{DeePO} + e\dt{t},
\end{equation}
where $e\dt{t}=\mathcal{N}(0,0.2u\dti{t}{DeePO})$. Moreover, the bicycle tracks a time-varying reference, as shown in Fig.~\ref{fig:simResults}.

To evaluate the update rate for the gain update in Algorithm~\ref{alg:deepo} (line 6), multiple simulations are conducted where $\xi$ varies while the rest of the parameters are kept fixed. Four different update rates for the gain update are considered: $\xi=1$, $10$, $50$, and $100$. Moreover, one simulation is conducted with only the FL controller as a baseline. The forgetting factor is evaluated in a similar fashion using $\lambda = 1-10^{-\zeta}$ with $\zeta=\infty$, $2$, $3$, $4$, $5$, $6$, where $\zeta=\infty$ corresponds to a controller without a forgetting factor. In these simulations, the control update rate is set to $\xi=1$, and the rest of the parameters are kept at their initial values. The performance of the controllers is evaluated using the integrated squared error of the lean angle and the lean rate with respect to their respective reference values: 
\begin{equation}
    \text{ISE}_\varphi = \sum_{i=0}^t (\varphi\dt{i}-\varphi\dti{i}{r})^2,\quad \text{ISE}_{\dot{\varphi}} = \sum_{i=0}^t (\dot{\varphi}\dt{i}-\dot{\varphi}\dti{i}{r})^2.
\end{equation}

\subsection{Simulation results}
The results for tracking a reference lean angle and lean rate are given in the top and middle plots of Fig.~\ref{fig:simResults}. The bottom plot represents the contribution of the DeePO algorithm to the system's total control signal. The update rate of the control gain is $\xi=1$, i.e., the control gain is updated with every sample, in Fig.~\ref{fig:simResults}. The results highlight the effectiveness of our unified control framework using FL and DeePO for tracking the reference lean angle, $\varphi\dti{t}{r}$, and lean rate, $\dot{\varphi}\dti{t}{r}$. The tracking is smoother, and the error is reduced compared to using only FL, as shown in the top and middle plots of Fig.~\ref{fig:simResults}. This improvement demonstrates DeePO's ability to adapt and refine the control policy iteratively, even under simulated nonlinear dynamics and sensor noise. The bottom plot of Fig.~\ref{fig:simResults} shows the contribution of DeePO to the total control signal and highlights that DeePO complements the FL controller and enables fine-tuning of the control signal online.

\begin{figure}[t]
    \centering
    \includegraphics{simResultsRows.pdf}
    \caption{Tracking performance of the DeePO algorithm in simulation with $\xi = 1$. The top and middle plots illustrate the lean angle and rate tracking results for the DeePO+FL setup and the FL-only approach. The bottom plot presents the total control signal along with DeePOâ€™s contribution.}
    \label{fig:simResults}
\end{figure}

In Fig.~\ref{fig:simControlPolicy}, the control policy for different update rates with respect to time is presented, and the corresponding ISE values are reported in Fig.~\ref{fig:simISE}, together with the ISE values when using only FL. These results clearly show that updating the control policy applied to the bicycle at every iteration of the algorithm is unnecessary. In fact, the performance improves for some of the lower update rates, i.e., higher values of $\xi$. However, it is not obvious how this parameter should be chosen, as for $\xi=10$, the performance is slightly worse than $\xi=1$. On the other hand, the performance is improved for $\xi=50$ and $\xi=100$. However, a low update rate may reduce DeePO's adaptation to fast changes in the system's behavior, reducing performance in systems with rapidly changing dynamics. Moreover, from Fig.~\ref{fig:simControlPolicy}, the control policy does not seem to converge, which may be due short simulation horizon. 

\begin{figure}[t]
    \centering
    \includegraphics{simControlPolicy.pdf}
    \caption{Evaluation of the control policy in simulation over time with different update rates of the control gain.}
    \label{fig:simControlPolicy}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics{simISE.pdf}
    \caption{Integrated squared error of the lean angle and lean rate for different values of $\xi$ in simulation. The performance of only using FL is also included as a baseline.}
    \label{fig:simISE}
\end{figure}

Finally, Fig.~\ref{fig:simVarylambda} presents the ISE values when the forgetting factor varies while the remaining control parameters are kept fixed. The results show that introducing a forgetting factor in DeePO may further enhance control performance. An intermediate forgetting factor of $\lambda=1-10^{-5}$ yields the best performance, while $\lambda = 1-10^{-2}$ results in the largest ISE value. Though varying $\xi$ and $\lambda$ may offer performance enhancements, finding the optimal and likely state-dependent values, for these parameters remains an area for future research. Moreover, the consistency of the simulation results highlights the efficiency of our unified control framework, which integrates FL and DeePO under varying circumstances.

\begin{figure}[t]
    \centering
    \includegraphics{simVarylambda.pdf}
    \caption{Integrated squared error of tracking the lean angle and lean rate references for different values of the forgetting factor in DeePO. The forgetting factor is defined as $\lambda = 1 - 10^{-\zeta}$.}
    \label{fig:simVarylambda}
\end{figure}





\subsection{Experimental setup} 
An initial experiment is conducted in which a dataset is collected from the sensors and the control signal. Similar to the initial simulation, the FL controller with the parameters from Table~\ref{tab:modParam} together with a persistently exciting input as $u_{PE} = \mathcal{N}(0,\sigma_{PE})$ are used as the input to the system, with $\sigma_PE=0.2$ rad/s. Furthermore, we consider tracking of a lean angle and lean rate reference of zero, and the tracking errors make up the states as in the simulation. The experiments are conducted in an indoor warehouse building with a flat concrete floor; see Fig.~\ref{fig:expBike}. The instrumented bicycle starts from a standstill, and the operator starts the bicycle using a switch on the RC controller and assists in balancing the bicycle until it reaches its constant goal velocity of $8$ km/h. Using a second switch on the RC controller, the experiment is initiated, and the persistently exciting input, together with the FL controller, regulates the steering velocity of the handlebar and, by extension, the bicycle's balance. The Xsens MTi-7 and the Dynamixel data are collected at a sampling frequency of $100$ Hz on the Raspberry Pi. Algorithm~\ref{alg:deepo} and the FL controller, implemented on the Raspberry Pi, compute a new control signal every $0.01$ s and transmit it to the Dynamixel servo. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{expBike.png}
    \caption{The indoor environment where the experiments were conducted on a flat concrete floor.}
    \label{fig:expBike}
\end{figure}

The sampled input and state data are post-processed in Matlab, where they are used to construct $U_{0,T}, X_{0,T}, X_{1,T}$, with $T=300$. Next, the initial policy is obtained by solving~\eqref{prob:regu} in Matlab, with $Q=I_2$, $R=0.01$, and $\gamma=1$. In the subsequent experiments, we set the forgetting factor and learning rate as $\lambda = 1-10^{-4}$ and $\eta=10^{-3}$, respectively. While $\gamma$, $\lambda$, and $\eta$ are kept the same in the experiments as in the simulation, the $Q$, $R$, and $T$ are changed in experiments. The increase in the number of data samples can be explained by the idealized dynamics in simulation compared to experiments where, for instance, mechanical imperfections, delays, and external disturbances are present. Thus, a larger dataset ensures a more reliable estimation of the system's behavior. Moreover, the increase in the weight of $\dot\varphi$ in $Q$ is justified by the sensor noise and the need for robustness in experiments. Finally, in simulations, the resolution of the  steering motor is neglected, and the complex dynamics of the actuating steering system are not fully captured. Thus, we can allow for more aggressive control actions using a lower value of $R$. On the other hand, in experiments, the resolution of the motor limits the possible steering commands. A larger $R$ in experiments ensures a smoother, more physically feasible control input. The same time-varying reference lean angle and lean rate used in simulations are also utilized in the experiments. Furthermore, we conduct several experiments where $\xi$ varies as $\xi=1, 10, 50,$ and $100$. Additionally, one experiment is conducted with only the FL controller as a baseline.


\subsection{Experimental results}
The lean angle and lean rate tracking performance of the DeePO algorithm is illustrated in the top and middle plots of Fig.~\ref{fig:expResults} using $\xi = 1$. The results of using only FL are also included in the figure. The control signal of DeePO and the total control signal of the system are highlighted in the bottom plot of Fig.~\ref{fig:expResults}. The results demonstrate the effectiveness of DeePO and its robust tracking of the reference lean angle and lean rate, with noticeable improvements compared to the FL controller, as evident from the first two plots of Fig.~\ref{fig:expResults}. The results also show how DeePO adapts over time, even in the presence of nonlinearities, sensor noise, and external disturbances (e.g., floor imperfections or variations in tire grip). However, the oscillations in lean angle and lean rate have a much higher amplitude in experiments compared to simulations, which the steering motor's resolution limitation could partly explain. In experiments, the resolution is limited to $0.024$ rad/s per unit, a factor not considered in simulations. Moreover, simulations have unmodelled dynamics and environmental details compared to experiments, such as joint friction, uneven terrain, and approximations made in the model. 

\begin{figure}[t]
    \centering
    \includegraphics{expResultsRows.pdf}
    \caption{Experimental results of DeePO where the control policy is updated at every time step, i.e., $\xi=1$.}
    \label{fig:expResults}
\end{figure}

The evolution of the control gain values for the lean angle and lean rate are reported in Fig.~\ref{fig:expControlPolicy} with varying values for gain update frequency $\xi$.  As observed in simulations, the evolution of the control gains in experiments follows the same trend,  even though the update frequencies vary. However, the difference between the evolution of the control gains in experiments and the control gains of the simulation, as presented in Fig.~\ref{fig:simControlPolicy}, is quite different, which indicates a gap between the simulations and experiments. It also highlights the usefulness of adaptive control methods, which can refine the feedback gain based on online experiment data. The gap between simulations and experiments is also evident when comparing the ISE values in Fig.~\ref{fig:simISE} and Fig.~\ref{fig:expISE}. In experiments, update at every time step or intermediate rates of $\xi=50$ produces significantly better results than update at lower rates or using only FL. The considerably higher ISE for $\xi=100$ and using only FL further highlights the limitations of static or infrequent control gain updates. In simulations where we have control over the initial conditions, noise, and external disturbances, the results are more aligned, and the impact of $\xi$ is not as evident as in experiments. One particular challenge in the experiments was finding a suitable pre-stablizing control form initial data and control of the initial conditions. A video demonstration of the simulations and experiments is available online~\footnote{\href{https://youtu.be/5RKnr6tPiuw}{https://youtu.be/5RKnr6tPiuwonline}}.

\begin{figure}[t]
    \centering
    \includegraphics{expControlPolicy.pdf}
    \caption{Evolution of control gains in experiments for different values of $\xi$.}
    \label{fig:expControlPolicy}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics{expISE.pdf}
    \caption{Integrated squared error for different values of $\xi$ and when using the FL controller alone in experiments.}
    \label{fig:expISE}
\end{figure}



