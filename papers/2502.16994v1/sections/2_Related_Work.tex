\section{Related Work}


Evaluating the alignment between features and their descriptions has become increasingly important with the rise of automated interpretability approaches. While manually inspecting highly activating examples remains a common method to validate interpretations and demonstrate automated interpretability techniques \cite{bills2023language, templeton2024scaling}, more scalable tools are needed for thorough quantitative evaluation. Many automated or semi-automated approaches have been proposed, generally falling into activation-centric and output-centric methods.

Activation-centric methods focus on measuring how well a feature’s activations correspond to its assigned description.

One prominent approach is a simulation-based scoring, where an LLM predicts feature activations based on the description and input data, and the correlation between predicted and real activations of a feature is measured \cite{bills2023language, bricken2023monosemanticity, choi2024automatic}. While elegant, this approach can be computationally expensive and tends to favor broad, high-level explanations.

A related and conceptually more straightforward way to measure how well the description explains a feature's behavior is to try to directly generate synthetic samples using the description and compare the resulting activations between concept and non-concept samples \cite{huang-etal-2023-rigorously, DBLP:conf/nips/KopfBHLHB24, gurarieh2025enhancingautomatedinterpretabilityoutputcentric, shaham2025multimodalautomatedinterpretabilityagent}. However, generated datasets are typically small (on the order of 5–20 samples \cite{huang-etal-2023-rigorously, gurarieh2025enhancingautomatedinterpretabilityoutputcentric}) and often constrained to rigid syntactic structures or focus only on the occurence of particular tokens, making them less effective for evaluating abstract or open-ended language concepts \cite{huang-etal-2023-rigorously, foote2023n2gscalableapproachquantifying}.

Another strategy is rating individual samples from a natural dataset for how strongly they express a concept and compare those ratings to the feature's activations \cite{huang-etal-2023-rigorously, paulo2024automaticallyinterpretingmillionsfeatures, templeton2024scaling}. 

A common limitation of activation-centric methods is that they primarily evaluate positively correlated activations while ignoring negatively encoded neurons, effectively ignoring negatively encoded features \cite{huang-etal-2023-rigorously, DBLP:conf/nips/KopfBHLHB24}.

Output-centric methods instead assess how feature activations influence model behavior. Some approaches measure the general decrease in performance of the model after ablating the feature \cite{bills2023language, makelov2024principledevaluationssparseautoencoders}, while others use steering-based interventions, where an increase in generated outputs containing the concept is used as a proxy for feature alignment \cite{paulo2024automaticallyinterpretingmillionsfeatures, gurarieh2025enhancingautomatedinterpretabilityoutputcentric}.

There is a growing need for frameworks that integrate multiple perspectives to provide a comprehensive assessment of feature-to-description alignment across different interpretability methods.

For instance, prior work \cite{bills2023language, menon2025analyzinginabilitiessaesformal, gurarieh2025enhancingautomatedinterpretabilityoutputcentric} has shown that while activation-centric and output-centric measures often correlate, they do not necessarily imply a causal relationship.
 
Some studies focus exclusively on SAEs \cite{goodfire, paulo2024automaticallyinterpretingmillionsfeatures}, while others analyze MLP neurons \cite{bills2023language, choi2024automatic}. Developing an architecture-agnostic framework for feature-to-description evaluation is essential for enabling robust quantitative comparisons across interpretability approaches.

Although efforts have been made to integrate multiple evaluation perspectives \cite{paulo2024automaticallyinterpretingmillionsfeatures, gurarieh2025enhancingautomatedinterpretabilityoutputcentric}, these remain fragmented and are often too narrowly scoped to handle open-ended language descriptions.

Our work addresses these limitations by introducing a more comprehensive evaluation framework that combines activation- and output-centric metrics while explicitly considering interpretability for open-ended language descriptions.