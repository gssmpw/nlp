\section{Introduction}

Understanding the latent features of machine learning models and aligning their descriptions with human-comprehensible concepts remains a crucial challenge in AI interpretability research. Recent advances have made significant strides in this direction, by introducing automated interpretability methods \cite{bills2023language, bykov2024labeling, choi2024automatic}, that leverage larger language-capable models to describe the latent representations of smaller models \cite{DBLP:journals/tmlr/BykovDGMH23, templeton2024scaling, dreyer2025mechanistic}. This facilitates inspection of ML models, enabling a deeper understanding of models' behaviour. Consequently, this enhances our ability to identify or mitigate harmful responses and biases, thus improving model transparency and interpretability \cite{lee2024mechanistic, gandikota2024erasing}.
%
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{attachments/1_Introduction/evaluation_pipeline.png} 
    \caption{Visualization of the \ours \ pipeline for three features and their corresponding feature descriptions.
    }
    \label{fig:evaluation-pipeline-graphic}
\end{figure*}
%
A key insight from these investigations is the highly polysemantic nature of individual neurons --- they rarely correspond to single, clear concepts. This discovery has led to the development and adoption of sparse autoencoders (SAEs) \cite{ goodfire, bricken2023monosemanticity, rajamanoharan2024jumpingaheadimprovingreconstruction}, which are intended to decompose polysemantic representations by separating neuron activations into more interpretable components. While SAEs offer a promising approach for feature decomposition, their reliability remains an open question. Recent research reveals significant variability in the way SAEs capture the underlying learned features \cite{heap2025sparse, paulo2025sparse}, thus highlighting the need for a holistic framework for the evaluation of feature-description alignment.
%
 To the best of our knowledge, there is an absence of widely accepted quantitative metrics for evaluating the quality and effectiveness of open-vocabulary feature descriptions.
 Different methodologies rely on custom evaluation criteria which makes it challenging to conduct meaningful, generalizable comparisons across techniques. Additionally, existing evaluation approaches typically optimize for a single metric \cite{bills2023language, choi2024automatic} which may not capture the full complexity of a feature's behavior and leaves open questions about whether the model truly encodes the hypothesized concept rather than simply correlating with the measured feature. With our work, we contribute as follows:\newline

\textbf{[1]} We present a robust automated evaluation framework designed for broad applicability across model architectures and their SAE implementations.\ours \ combines four metrics that allow quantitative analysis of different aspects of alignment between features and their generated descriptions.\footnote{We release \ours \ as an open-source Python package (available at \repolink) that includes notebooks demonstrating example neurons featured in this work.} \newline

\textbf{[2]} Through systematic empirical analysis, we provide insights into how various components of the autointerpretability pipeline --- such as the number of layers, sample sizes, and architectural choices --- affect the quality of feature descriptions.\newline

\textbf{[3]} We release a selected subset of feature descriptions, presented as a part of this work for \texttt{Gemma-2-2b} and \texttt{Gemma Scope} SAEs along with their evaluations.