\begin{abstract}
Recent advances in mechanistic interpretability have highlighted the potential of automating interpretability pipelines in analyzing the latent representations within LLMs. While they may enhance our understanding of internal mechanisms, the field lacks standardized evaluation methods for assessing the validity of discovered features. We attempt to bridge this gap by introducing \textbf{\ours: Feature Alignment to Description Evaluation}, a scalable model-agnostic framework for evaluating feature-description alignment. FADE evaluates alignment across four key metrics --- \textit{Clarity, Responsiveness, Purity, and Faithfulness} --- and systematically quantifies the causes for the misalignment of feature and their description. We apply \ours \ to analyze existing open-source feature descriptions, and assess key components of automated interpretability pipelines, aiming to enhance the quality of descriptions. Our findings highlight fundamental challenges in generating feature descriptions, particularly for SAEs as compared to MLP neurons, providing insights into the limitations and future directions of automated interpretability. We release \ours\ as an open-source package at: \url{https://github.com/brunibrun/FADE}. 
\end{abstract}