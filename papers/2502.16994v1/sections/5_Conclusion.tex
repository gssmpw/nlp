\section{Conclusion}

In this work, we presented \ours, a new automated evaluation framework designed to rigorously evaluate the alignment between features and their open-vocabulary feature descriptions. By combining four complementary metrics \textit{Clarity}, \textit{Responsiveness}, \textit{Purity}, and \textit{Faithfulness}, our approach gives a comprehensive assessment of how a feature reacts to instances of the described concept, an evaluation of the description itself as well as the feature's causal role in the modelâ€™s outputs. Through extensive experiments across different feature types, layers, and description generation mechanisms, we demonstrated that methods relying on a single metric (e.g., simulation-based approaches) often give incomplete or misleading feature descriptions. Our framework can be used to highlight both the strengths and weaknesses of existing methods, while it also helps in debugging and improving these methods. We highlighted multiple results for improving the quality of feature explanations, such as using larger, more capable LLMs for the explainer and including more examples in the prompt. We hope that the open-source implementation of \ours\ will drive further research in automated interpretability and help make language models more transparent and safe to use.