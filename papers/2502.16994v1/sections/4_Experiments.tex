

\section{Experiments}

In this section, we apply \ours\ to assess the quality of descriptions generated for various state-of-the-art feature descriptions \cite{choi2024automatic, lieberum-etal-2024-gemma}. Our goal is to demonstrate that the proposed framework provides a robust, multidimensional measure of feature to feature description alignment.

\paragraph{Experimental Setup}
As a natural dataset for the evaluations we use samples drawn from the test partition of the Pile dataset \cite{gao2020pile}, preprocessed as shown in 
Appendix~\ref{appendix-dataset}.
As evaluating LLM we use the OpenAI model \texttt{gpt-4o-mini-2024-07-18} unless stated otherwise. Prompts for the evaluating LLM as well as details on the hyperparameters can be found in Appendix~\ref{appendix-evaluation-experiment-setup}. We run the experiments on $10^3$ randomly chosen features from a single layer of a model: layer 20 for \texttt{Gemma-2-2b} \cite{gemmateam2024gemma2improvingopen}, layer 20 of \texttt{Gemma Scope} SAEs \cite{lieberum-etal-2024-gemma}, and layer 19 of \texttt{Llama-3.1-8B-Instruct} \cite{grattafiori2024llama3herdmodels} (see Appendix~\ref{appendix-autointerpretability-pipeline} for details). 
The evaluation results have a high variance, which is caused by both the inherent difficulty of interpreting some features as well as the quality of the ascribed feature descriptions. 
Therefore the mean values are demonstrated only if they aid the analysis of metrics distributions. For all of the presented tables we demonstrate the full distributions as kernel-density estimations with bandwidth adjustment factor 0.3 in Appendix~\ref{appendix-extended-results}. 

\paragraph{Automated interpretability approach}
Feature descriptions, which we refer to as MaxAct*, are generated based on samples of the train partition of the Pile dataset, that demonstrate maximum activation on the feature, similarly to methods utilized in \cite{bills2023language, paulo2024automaticallyinterpretingmillionsfeatures, rajamanoharan2024jumpingaheadimprovingreconstruction}, that we refer to as MaxAct. The minor differences between MaxAct and MaxAct* are prompts, optimized on qualitative analysis provided via \ours, and preprocessing steps of the dataset. The automated interpretability pipeline is described in Appendix~\ref{appendix-autointerpretability-pipeline}.


\subsection{Depth and Reliability of Evaluations}\label{sec:Results}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\linewidth]{attachments/4_Experiments/metrics_ours_vs_transluce.png}
    \caption{Distribution of metrics in \ours\ framework and a simulation-based metric on a uniformly subsampled set of features and the top 10\% of subsampled features, selected based on the simulated activation metric generated in \cite{choi2024automatic} (right), and comparison to the proposed descriptions for these features, generated in this work (left).}
    \label{fig:metrics-comparison}
\end{figure*}


\paragraph{Limitations of single-metric approaches}
We compare \ours\ with simulated-activation-based metrics \cite{bills2023language, templeton2024scaling, choi2024automatic}, that, while computationally efficient, 
fail to fully capture the feature-to-description alignment, potentially overlooking critical issues like polysemanticity.
To illustrate this, we analyze feature descriptions of \texttt{Llama-3.1-8B-Instruct} generated in \cite{choi2024automatic}. As shown in Figure~\ref{fig:metrics-comparison}, despite high simulated-activation scores, \ours\ identifies many features with low purity. Moreover, comparing the average results across all subsampled features with the top 10\% of features based on the simulated-activation metric, we find only a marginal gain in clarity and responsiveness, while the purity worsens.
Via MaxAct*, we generate descriptions with slightly lower clarity but significantly higher responsiveness and purity. We attribute this to the explainer model in \cite{choi2024automatic} being fine-tuned on descriptions optimized for the simulated-activation metric, which aligns more closely with clarity but neglects responsiveness and purity.

\paragraph{Better models provide better evaluations}
As the evaluating LLM is one of the most computationally expensive components of our framework, selecting a model that balances performance and cost is critical. Larger models generally achieve better performance, but at significantly higher computational costs. To determine a minimal feasible model size and capability required for effective evaluation, we conduct a quantitative analysis of concept-expression ratings across various open-weight and proprietary models, using \texttt{GPT-4o} as a baseline due to its superior benchmark performance \cite{openai2024gpt4o}. 
We evaluate models on Neuronpedia \cite{neuronpedia} feature descriptions for the \texttt{Gemma Scope} SAEs, generated via MaxAct method \cite{rajamanoharan2024jumpingaheadimprovingreconstruction}. By comparing deviations in concept strength ratings between \texttt{GPT-4o} and other models, we assess their relative performance (see Table~\ref{tab:evaluation-models-comparison}).

\begin{table}
    \scriptsize
    \centering
    \begin{tabular}{lcccc}
        Model & Class 0 & Class 1 & Class 2 & Valid \\
        \hline
        \hline
        \noalign{\vskip .5mm}  
        GPT-4o & 243,233 & 24,766 & 19,716 & 100 \\
        \hline
                \noalign{\vskip .5mm}  
        Llama-3.2-1B & 63.8 & 22.1 & 20.9 & 8.8\\
        Llama-3.2-3B & 77.4 & 9.9 & 70.3 & 72.2 \\
        Llama-3.1-8B & 82.0 & 14.6 & 85.6 & 82.8\\
        Llama-3.3-70B 4q & 88.7 & 31.5 & \textbf{92.6} & 88.3\\
        GPT-4o mini & \textbf{93.4} & \textbf{44.8} & 79.3 & \textbf{88.6} \\
        \hline
    \end{tabular}
    \caption{Concept rating procedure for different evaluating LLMs. The \texttt{GPT-4o} baseline shows the number of occurrences per class. The other models show their alignment with the \texttt{GPT-4o} rating in percent. The ``Valid'' column shows the percentage of samples that were correctly classified. Class 0 represents no alignment with the concept, class 1 a partial alignment and class 2 means the samples clearly exhibits the concept.}
    \label{tab:evaluation-models-comparison}
\end{table}
%
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{attachments/4_Experiments/neuron_vs_sae.png}
    \caption{Feature description fit for neuron-based features (\texttt{Gemma-2}) and SAE-based features (\texttt{Gemma Scope}).}
    \label{fig:neurons-vs-saes}
\end{figure*}
%
Our findings reveal a clear trend: larger, more capable models consistently yield better evaluations. The open-weight \texttt{Llama-3.3-70B-Instruct} (AWQ 4-bit quantized) performs comparably to the proprietary \texttt{GPT-4o mini}, a widely used model in autointerpretability research \cite{choi2024automatic, neuronpedia}. While Class 1 (partial alignment) is the most error-prone, smaller models, such as \texttt{Llama-3.2-3B-Instruct}, remain viable for the more critical Class 0 (no alignment) and Class 2 (strong alignment). However, for optimal performance, models smaller than \texttt{Llama-3.1-8B-Instruct} are likely insufficient.

\paragraph{Generating feature descriptions for SAEs is more challenging than for MLP neurons} 
To compare MLP neurons and SAE features, we analyze \texttt{Gemma-2-2b} and \texttt{Gemma Scope} SAEs. While \texttt{Gemma-2} outperforms \texttt{Gemma Scope} SAEs in average clarity (see Table~\ref{tab:numberical-vs-delimeters}), the clarity score distribution reveals a left-skewed peak for SAEs, as depicted on Figure~\ref{fig:neurons-vs-saes}. Further analysis identifies a cluster of features with low clarity but moderate to high responsiveness and purity. These features have descriptions that approximate the encoded concept but lack the precision to strongly activate the SAE feature (see (d) in Figure~\ref{fig:qualitative-examples}). This suggests that despite greater monosemanticity, interpreting SAE features remains challenging due to the difficulty of generating precise descriptions. In contrast, responsiveness and purity are higher on average for SAEs, as these metrics are less sensitive to imprecise descriptions and still align with the underlying concept. The higher purity in SAEs aligns with their increased monosemanticity.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.8cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}} 
SAE Size        &   Clarity     & Respon-siveness & Purity         & Faithfulness     \\ \hline \hline 
MaxAct* 16K     & \textbf{0.57} &   \textbf{0.78} &  \textbf{0.69} &  \textbf{0.17}   \\
MaxAct* 65K     &    0.46       &   0.71          &  0.66          &  0.15            \\ \hline
Neuronpedia 16K & \textbf{0.43} &   \textbf{0.67} &  \textbf{0.60} &  \textbf{0.17}   \\
Neuronpedia 65K &    0.29       &   0.64          &  0.56          &  0.13            \\\hline 
\end{tabular}
\caption{Comparison results for SAEs of different sizes, see metrics distributions on Figure~\ref{fig:sae-sizes}. }
\label{tab:saes-sizes-comparison}
\end{table}
%
\paragraph{Interpreting larger SAEs is more difficult} 
We investigate whether SAEs with a higher number of features inherently exhibit a better alignment with the feature descriptions. To quantify this, we compare feature descriptions from \texttt{Gemma Scope 16K}  and   \texttt{Gemma Scope 65K}. We compare it based on Neuronpedia feature descriptions, as well as the ones obtained in this work via MaxAct*.
Consistent with our previous finding, our results indicate that increasing the number of SAE features does not inherently improve the alignment of features with their descriptions, as shown in Table~\ref{tab:saes-sizes-comparison}. We hypothesize that this stems from a finer-grained decomposition of concepts, making it more challenging for the explainer LLM to capture and articulate the precise concept. 

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.2cm}p{1cm}p{1cm}p{1cm}p{1cm}} 
Layer &   Clarity     & Respon-siveness & Purity        & Faithfulness \\ \hline \hline 
\noalign{\vskip .5mm}  
3              &    0.60       &   0.71          &  0.55         &  0.009   \\
12             &    0.44       &   0.54          &  0.43         &  0.016   \\
20             &    \textbf{0.67}       &   \textbf{0.74}          &  0.61         &  0.011   \\
25             &    0.59       &   0.65          &  0.54         &  \textbf{0.025}  \\ \hline 
\end{tabular}
\caption{Evaluation results for different layers in \texttt{Gemma-2}, see metrics distributions on Figure~\ref{fig:gemma2-layers}. }
\label{tab:gemma2-layers-comparison}
\end{table}

\paragraph{Interpretability varies across layers}
Table~\ref{tab:gemma2-layers-comparison} presents an evaluation of feature descriptions from different layers of \texttt{Gemma-2-2b}. 
Our analysis identifies layer 12 as the most challenging to interpret. A manual inspection of 50 randomly sampled features confirms these results: features in layer 12 exhibit high polysemanticity. 
The highest scores are observed in layer 20, with the exception of the faithfulness metric. However, this may be due to the fine-tuning of the MaxAct* approach on this layer, which introduces a bias that specifically affects faithfulness.
The highest faithfulness score is observed in layer 25, while the lowest is found in layer 3. 


\subsection{Evaluating Autointerpretability: Prompts, Examples, and Model Size}

Despite the growing number of methods proposed in automated interpretability research, there has been surprisingly little comprehensive evaluation of different approaches. In this section, we present a series of experiments that assess different components of feature generation pipelines and demonstrate how \ours\ can help in fine-tuning interpretability pipelines. 

\paragraph{Prompting with numerical- or delimiter-based input}
Prompt construction can significantly influence the quality of the generated descriptions. We investigate two primary approaches: passing (word, activation) pairs and using \{\{delimiters\}\} to highlight the most activated tokens (see Appendix~\ref{appendix-prompts} for more details). 
Our experiments indicate that the numerical input performs slightly better than the delimiter-based prompt, which contradicts previous research \cite{choi2024automatic}.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{0.7cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{1.2cm}} 
Model                     & Input type          & Clarity          & Respon-siveness   & Purity        & Faithfulness \\ \hline \hline 
\multirow{2}{1cm}{Gemma-2} &  delimiters         &    \textbf{0.67}     &   0.74      &  0.61     & 0.01 \\ 
                          &  numeric            &    \textbf{0.67}     &   \textbf{0.76}      &  \textbf{0.64}     & 0.01  \\ \hline
\multirow{2}{1cm}{Gemma Scope}&  delimiters     &    0.57     &   0.78      &  0.69     & 0.16 \\  
                          &  numeric            &    \textbf{0.59}     &   \textbf{0.80}      &  \textbf{0.72}     & 0.17 \\ \hline 
\end{tabular}
\caption{Comparison results of activations input types via delimiters vs numerical input, see metrics distributions on Figure~\ref{fig:prompts_analysis} (a). }
\label{tab:numberical-vs-delimeters}
\end{table}

\paragraph{Few-shot prompting improves description quality}
Next we test how many examples should be passed to the explainer model in the prompt. We compare 0-shot (without examples), 1-shot, 2-shot, 5-shot, 10-shot and 20-shot prompts on \texttt{Gemma Scope}. In these variations, we use the delimiter-based prompts. The results, provided in Table~\ref{tab:number-of-shots-in-prompt}, demonstrate, that a larger number of examples brings steady improvement in clarity, responsiveness and purity. Faithfulness shows no clear trend.  

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.2cm}p{1cm}p{1cm}p{1cm}p{1cm}} 
Number of shots &   Clarity     & Respon-siveness & Purity        & Faithfulness \\ \hline \hline 
0-shot             &    0.53       &   0.76          &  0.70         &  0.17   \\
1-shot             &    0.55       &   0.76          &  0.68         &  \textbf{0.19}  \\ 
2-shot             &    0.57       &   0.78          &  0.69         &  0.17  \\ 
5-shot             &    0.60       & 0.79            & 0.72           &  0.16  \\
10-shot             &   0.60       & 0.79            & 0.72           &  0.16  \\
20-shot             & \textbf{0.61} & \textbf{0.81}   & \textbf{0.73} &  0.16  \\ \hline 
\end{tabular}
\caption{Comparison results for different number of shots provided to the explainer model with the prompt based on Gemma Scope, see metrics distributions on Figure~\ref{fig:prompts_analysis} (c). }
\label{tab:number-of-shots-in-prompt}
\end{table}


\paragraph{Providing more samples increases evaluation scores}
We test 5, 15 and 50 samples, using the same delimiter-based prompts (see Table~\ref{tab:comparison-samples-number}). The results indicate, that increasing the number of samples improves description quality, though the gains are not substantial for any of the tested number of samples.

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{0.7cm}p{0.8cm}p{0.8cm}p{0.8cm}p{0.8cm}p{1.2cm}}  
Model & \# samples & Clarity      & Respon-siveness & Purity   & Faithfulness \\ \hline \hline 
\multirow{3}{1cm}{Gemma2} & 5      &    0.63 &  0.72   & 0.59   &  \textbf{0.01}    \\
                          & 15     &    0.67 &  0.74   & 0.61   &  \textbf{0.01}  \\ 
                          & 50     &    \textbf{0.69} &  \textbf{0.77}   & \textbf{0.62}   &  \textbf{0.01} \\  \hline 
\multirow{3}{1cm}{Gemma Scope} & 5 & 0.56          & 0.77          & 0.67          & 0.17 \\
                          & 15     & 0.57          & 0.78          & 0.69          & \textbf{0.18} \\ 
                          & 50     & \textbf{0.60} & \textbf{0.79} & \textbf{0.71} & 0.17 \\  \hline 
\end{tabular}
\caption{Comparison results for different number of activating samples provided to the explainer model, see metrics distributions on Figure~\ref{fig:prompts_analysis} (b). }
\label{tab:comparison-samples-number}
\end{table}


\paragraph{Better models produce better feature descriptions} 
Similarly to the experiment, comparing different evaluation models presented in Table~\ref{tab:evaluation-models-comparison}, we compare different explainer models and demonstrate the results in Table~\ref{tab:comparison-explainer-models}. 
\texttt{GPT-4o} achieves the highest scores, with \texttt{Llama-3.3-70B-Instruct} (AWQ 4-bit quantized) and \texttt{GPT-4o mini} as close alternatives. The smaller models struggle with assigning reasonable feature descriptions, in particularly \texttt{Llama-3.2-1B}, which frequently fails to maintain a consistent response structure (see Appendix~\ref{appendix-extended-results}). 

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.6cm}p{1cm}p{1cm}p{1cm}p{1cm}} 
Model                 &   Clarity     & Respon-siveness & Purity        & Faithfulness \\ \hline \hline 
Llama-3.2-1B  &    0.39       &   0.56          &  0.35         &  0.10  \\  
Llama-3.2-3B  &    0.51       &   0.73          &  0.61         &  0.15  \\ 
Llama-3.1-8B  &    0.50       &   0.75          &  0.66         &  0.17   \\ 
Llama-3.3-70B &    0.54       &   0.78          &  0.70         &  \textbf{0.19}  \\ 
GPT-4o mini           &    0.58       &   0.78          &  0.70         &  0.17  \\ 
GPT-4o                &    \textbf{0.61}       &   \textbf{0.80}          &  \textbf{0.73 }        &  0.17  \\ \hline

\end{tabular}
\caption{Explainer models comparison, see metrics distributions on Figure~\ref{fig:explainer-models}. }
\label{tab:comparison-explainer-models}
\end{table}


\paragraph{Baselines fail in predictable ways}
To assess the effectiveness of MaxAct* approach, we compare it against established baseline methods, including the Neuronpedia feature descriptions, a TF-IDF \cite{ramos2003using, salton1988term} based description approach, and an unembedding-based method \cite{bloom2024understandingfeatureslogitlens} (see Appendix~\ref{appendix-autointerpretability-pipeline} for methodological details). The results are presented in Table~\ref{tab:baselines-gemmascope}.
MaxAct* consistently outperforms baselines in clarity, responsiveness, and purity. Notably, the unembedding method achieves the highest faithfulness score, a result that aligns with our expectations and related work \cite{gurarieh2025enhancingautomatedinterpretabilityoutputcentric}. Since this method explicitly considers the output that a given feature promotes, it naturally excels at capturing causal influence of the feature. However, this focus on output consistency often comes at the expense of clarity, responsiveness, and purity, as raw unembedding-based descriptions do not incorporate any information about what activates the feature.
These findings again highlight the necessity of a holistic evaluation framework, as different methods optimize for different aspects of interpretability. 


\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.4cm}p{1cm}p{1cm}p{1cm}p{1cm}} 
Approach &   Clarity     & Respon-siveness & Purity        & Faithfulness \\ \hline \hline 
Neuronpedia  &    0.43       &   0.67          &  0.60         &  0.21   \\% 
TF-IDF       &    0.42       &   0.72          &  0.53         &  0.21   \\% 
Unembedding  &    0.38       &   0.65          &  0.61         &  \textbf{0.29}   \\% 
MaxAct* &    \textbf{0.57}       &   \textbf{0.78 }         &  \textbf{0.69 }        &  0.21  \\ \hline 
\end{tabular}
\caption{Comparison of the quality of our feature descriptions to baselines, see metrics distributions on Figure~\ref{fig:baselines}. }
\label{tab:baselines-gemmascope}
\end{table}