\section{Evaluating Feature Explanations} \label{sec:methodology}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{attachments/3_Evaluations/qualitative_examples_new.png}
    \caption{ \ours \ can highlight different problems that arise with description generation. (a)
Feature 128 contains concept of the expression ``under someone's belt''. However, the derived concept ``belt'', is not clear/ specific enough to be useful for generating synthetic data, that would activate the feature. However, the description is still very close to the concept, therefore Responsiveness and Purity are high. (b) Feature 1776 strongly reacts to the word ``each'', but also to many other general, unrelated words, resulting in lower Clarity and Purity. (c) The description for feature 7657 catches its main concept, low Purity indicates that this feature is clearly polysemantic. (d)
Description for feature 10647 is expressed too broad, resulting in low Responsiveness and Purity. 
}
    \label{fig:qualitative-examples}
\end{figure*}

Our primary objective is to establish a comprehensive framework that automatically evaluates feature descriptions across a variety of feature types without human intervention. Our framework encompasses four distinct metrics: \textbf{Clarity}, \textbf{Responsiveness}, \textbf{Purity}, and \textbf{Faithfulness}, which we consider necessary and sufficient for assessing the alignment between a feature and its description. In our opinion, such a comprehensive evaluation framework is necessary to ensure that features encode the ascribed concept in a robust way. As feature descriptions are often generated by optimizing for a single metric, such as maximizing the activations of specific neurons, they do not necessarily generalize well to other quantifiable aspects, such as faithfulness \cite{bills2023language, choi2024automatic}.

We base our approach on four key assumptions. First, we adopt a \raisebox{.5pt}{\textcircled{\footnotesize 1}} \textit{Binary Concept Expression} model, whereby a concept is either present in a text sequence or absent. Second, we assume \raisebox{.5pt}{\textcircled{\footnotesize 2}} \textit{Concept Sparsity}, i.e.\ that a given concept appears only rarely in natural datasets, though a sufficiently large dataset will contain some representative examples.
Third, we assume \raisebox{.5pt}{\textcircled{\footnotesize 3}} \textit{Feature Reactivity}, meaning, when a feature encodes a concept, its activations are significantly stronger on samples that express the concept.
This will be valid especially for SAEs, since by construction, for most samples, their activations are zero.
This is a strong assumption, as it also implies that a feature should activate strongly only for a single concept.
Note, however, that this does not require strict monosemanticity \cite{bricken2023monosemanticity}.
In our framework a feature might encode multiple, even entirely unrelated topics, as long as its feature description fully describes all of them. Unlike traditional monosemanticity, which assumes features should directly align with a single human-interpretable category, our framework evaluates interpretability based on whether the feature description accurately reflects the feature’s truly encoded concept, rather than enforcing human-aligned conceptual boundaries.
Assumption \raisebox{.5pt}{\textcircled{\footnotesize 1}} and \raisebox{.5pt}{\textcircled{\footnotesize 3}} allow us to interpret the activations of a feature as output of a ``classifier'' of the encoded concept, which can then be easily evaluated. For our metrics, we expect a feature to encode the concept linearly in its activations. Finally, we assume \raisebox{.5pt}{\textcircled{\footnotesize 4}} \textit{Causality} - a feature is expected to causally influence the model’s output so that modifying its activation will lead to predictable changes in the generation of concept-related content. These four assumptions will not always hold but are necessary simplifications for now.


\subsection{Evaluation Framework Components}

Our evaluation framework consists of three main components: A \textit{subject LLM}, that contains the features we want to evaluate, a \textit{natural dataset}, that ideally should be close to the LLM training data distribution and is sufficiently large to contain all the concepts, of which the descriptions we want to evaluate, and an \textit{evaluating LLM}, an open- or closed-source LLM that is used for automating the evaluation process. The evaluating LLM is used for ``human-like'' inference tasks, such as rating the strength of concept expression in samples and creating synthetic concept data.


\subsection{Evaluation Metrics}


\paragraph{Clarity} evaluates whether a feature’s description is precise enough to generate strongly activating samples. We assess this by prompting the evaluating LLM to generate synthetic samples based on the feature description (see prompts in Appendix~\ref{appendix-evaluation-experiment-setup-prompts}). Unlike \citet{gurarieh2025enhancingautomatedinterpretabilityoutputcentric}, which generates non-concept sequences artificially, we sample them uniformly from the natural dataset to avoid unnatural biases (i.e. by asking the evaluating LLM not to think about pink elephants). If a feature is well explained by its description, the synthetic concept samples should elicit significantly stronger activations than non-concept samples. We quantify this separability using the absolute Gini coefficient

{\scriptsize
    \begin{align}
    \mathrm{G_{abs}}(A_{c}, A_{n}) = \left| 2 \cdot \left( \frac{\sum\limits_{a_c \in A_c} \sum\limits_{a_n \in A_n} \mathbf{1}_{[a_c > a_n]}}{\| A_c \|_0 \cdot \| A_n \|_0} \right) -1 \right|
   \end{align}
}

where $A_c$ and $A_n$ are the sets of concept and non-concept activations, respectively. Since this metric focuses on linear separability rather than precision, it remains robust even when concept samples occasionally appear within the natural dataset. A low clarity score indicates that either the description is not precise enough to be useful, or might simply be unfitting for the feature, resulting in similar activations for both concept and non-concept samples. For example, in Figure~\ref{fig:qualitative-examples}, feature (d) responds to ``having something under one’s belt,'' yet is inaccurately described as ``belt''. Conversely, a high clarity score confirms that we can effectively generate samples that elicit strong activations in the feature, although it does not guarantee that the feature is monosemantic or causally involved.

\paragraph{Responsiveness} evaluates the difference in activations between concept and non-concept samples. We select samples from the natural dataset based on their activation levels, drawing both from the highest activations and from lower percentiles (details in Appendix~\ref{appendix-evaluation-implementation}). Following an approach similar to \citet{templeton2024scaling}, we prompt the evaluating LLM to rate each sample on a three-point scale to indicate how strongly the concept is present (0 = not expressed, 1 = partially expressed, 2 = clearly expressed). By discarding the ambiguous (partially expressed) cases, we effectively binarize samples into concept and non-concept categories. We compute the responsiveness score again using the absolute Gini coefficient. A low responsiveness score indicates that activations of concept-samples are similarly strong as non-concept samples, while a high score indicates that, in natural data, samples with strong activations reliably contain the concept.

\paragraph{Purity} is computed using the same set of rated natural samples as responsiveness, but with a different focus: it evaluates whether the strong activations are exclusive to the target concept. In contrast to \cite{huang-etal-2023-rigorously}, who measure recall and precision for a single threshold, we measure the purity using the Average Precision (AP)

{\scriptsize
    \begin{align}
    \mathrm{AP}(A_{c}, A_{n}) = \sum_{j} \left( r_{j} - r_{j-1} \right) \cdot p_{j}
    \end{align}
}
where $r_j$ is the recall and $p_j$ is the precision computed at threshold $j$, for each possible threshold, based on $A_c$ and $A_n$. The AP penalizes instances where non-concept samples also trigger high activations. A purity score near one thus indicates that the feature’s activations are highly specific to the concept, whereas a score near zero suggests that top activations occur for other unrelated concepts as well. This is, for example, the case in polysemanticity, where a feature responds to multiple unrelated concepts. 

\paragraph{Faithfulness} addresses the causal relationship between a feature and the model’s output. In other words, it tests whether direct manipulation of the feature’s activations can steer the model’s output toward generating more concept-related content. To evaluate faithfulness, we take random samples from the natural dataset and have the subject LLM generate continuations while applying different modifications to the feature’s activation.
For neurons, we multiply the raw activations by a range of factors, including negative values, so that we do not impose a directional bias on how the concept is encoded. For SAE features, of which the activations are more sparse, we first determine the maximum activation observed in the natural dataset \cite{templeton2024scaling} and then scale this value by the different modification factors. After generating the modified continuations, the evaluating LLM rates how strongly the concept appears in each output.
%
We quantify the strength of this causal influence by measuring the largest increase we were able to steer the model in producing concept-related outputs

{\scriptsize
    \begin{align}
    \mathrm{Faithfulness}(\mathbf{R}) = \dfrac{\max(\max(\mathbf{R}) - R_0, 0)}{1-R_0}
    \end{align} 
}
where $\mathbf{R}$ is a vector capturing the proportion of concept-related outputs for each modification factor, and $R_0$ denotes the base case in which the feature is “zeroed out” (i.e., multiplied by zero). A faithfulness score of zero implies that manipulating the feature does not increase the occurrence of concept-related outputs, while a score of one indicates that for some modification factor the concept is produced in every continuation. 



