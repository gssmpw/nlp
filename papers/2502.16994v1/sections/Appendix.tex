\label{sec:appendix}
\section{Extended related work}
\label{appendix-related-work}

A common approach to automatic interpretability includes selecting data samples that strongly activate a given neuron and using these samples, along with their activations as input to a larger LLM, that serves as an explainer model and generates feature descriptions \cite{bills2023language, bricken2023monosemanticity, choi2024automatic, paulo2024automaticallyinterpretingmillionsfeatures, rajamanoharan2024jumpingaheadimprovingreconstruction}. Previous research has investigated various factors influencing this method, including prompt engineering, the number of data samples used, and the size of the explainer model (see Appendix~\ref{appendix-related-work}).

Building on this approach, \cite{choi2024automatic} advanced the method by fine-tuning \texttt{Llama-3.1-8B-Instruct} on the most accurate feature descriptions, as determined by their  simulated-activation metric. This fine-tuning aimed to improve the performance and accuracy of description generation, ultimately outperforming \texttt{GPT-4o} mini.

An output-centric approach was introduced by
\cite{gurarieh2025enhancingautomatedinterpretabilityoutputcentric} in an  attempt to address another key challenge-the feature descriptions generated via the data samples that activate the feature the most, often fail to reflect its influence on the model’s output. The study demonstrates that a combined approach, integrating both activation-based and output-based data, results in more accurate feature descriptions and improves performance in causality evaluations.

Several studies perform their experiments exclusively on SAEs \cite{goodfire, paulo2024automaticallyinterpretingmillionsfeatures, rajamanoharan2024jumpingaheadimprovingreconstruction,templeton2024scaling}, while others focus on MLP neurons \cite{bills2023language, choi2024automatic}. Although \cite{templeton2024scaling} compares the interpretability of SAEs to that of neurons and concludes that features in SAEs are significantly more interpretable, these findings heavily rely on qualitative analyses. 

Previous research consistently shows that open-source models are effective for generating explanations, with advanced models producing better descriptions. For instance, \citet{bills2023language} report that GPT-4 achieves the highest scores, whereas Claude 3.5 Sonnet performs best for \citet{paulo2024automaticallyinterpretingmillionsfeatures}. The number of data samples used to generate feature description also varies across studies. \citet{bills2023language} use the top five most activating samples, while \citet{choi2024automatic} select 10-20 of the most activating samples to generate multiple descriptions for evaluation. In contrast, \citet{paulo2024automaticallyinterpretingmillionsfeatures} use 40 samples and suggests that randomly sampling from a broader set of activating leads to descriptions that cover a more diverse set of activating examples, whereas using only top activating examples often yields more concise descriptions which fails to capture the entire description.

The datasets used in these studies also differs. \citet{paulo2024automaticallyinterpretingmillionsfeatures} utilize the RedPajama 10M \cite{together2023redpajama} dataset, \citet{choi2024automatic} use the full LMSYSChat1M \cite{zheng2023lmsys} and 10B token subset of FineWeb \cite{penedo2024the}, and \citet{bills2023language} --- on WebText\cite{radford2019language} and the data used to train GPT-2 \cite{radford2019language}. Additionally, different delimiter conventions are observed: \citet{choi2024automatic} use {} delimiters, \citet{paulo2024automaticallyinterpretingmillionsfeatures} use << >>, and \citet{bills2023language} use numerical markers.


\section{Data Preprocessing}
\label{appendix-dataset}
For our work, we used an uncopyrighted version of the Pile dataset, with all copyrighted content removed, available on Hugging Face \cite{gao2020pile} (\url{https://huggingface.co/datasets/monology/pile-uncopyrighted}). This version contains over 345.7 GB of training data from various sources. From this dataset, we extracted approximately 6 GB while preserving the relative proportions of the original data sources. The extracted portion from the training partition was used to collect the most activated samples. For evaluations, we utilized the test partition from the same dataset, applying identical preprocessing steps as those used for the training data.
    
\begin{table}[h!]
\scriptsize
\centering % Reduce font size slightly
\setlength{\tabcolsep}{6pt}  % Adjust column separation
\begin{tabular}{@{}lrr@{}}  % @{} removes padding at edges, r aligns numbers
Component & Size (GB) & Proportion (\%) \\  \hline \hline
Pile-CC             & 1.93  & 32.17 \\
PubMed Central     & 1.16  & 19.38 \\
ArXiv             & 0.70  & 11.67 \\
FreeLaw          & 0.55  &  9.14 \\
PubMed Abstracts    & 0.32  &  5.39 \\
USPTO Backgrounds & 0.31  &  5.23 \\
Github         & 0.27  &  4.54 \\
Gutenberg (PG-19)  & 0.23  &  3.85 \\
Wikipedia (en)      & 0.15  &  2.57 \\
DM Mathematics      & 0.11  &  1.87 \\
HackerNews          & 0.07  &  1.17 \\
Ubuntu IRC          & 0.06  &  0.99 \\
EuroParl            & 0.06  &  0.95 \\
PhilPapers          & 0.03  &  0.58 \\
NIH ExPorter        & 0.03  &  0.50 \\
\hline
\textbf{Total}             & \textbf{5.99} & \textbf{100.00} \\ \hline
\end{tabular}
\caption{Extracted dataset and proportion of sub components}
\label{tab:dataset}
\end{table}

Our preprocessing involved several steps to ensure a balanced and informative dataset. First, we used the NLTK \cite{bird2009natural} sentence tokenizer to break large text chunks into individual sentences. We then filtered out sentences in the bottom and top fifth percentiles based on length, as these were typically out-of-distribution cases consisting of single words or characters or a few outliers. This step helped achieve a more balanced distribution. Additionally, we removed sentences containing only numbers or special characters with no meaningful content. Finally, duplicate sentences were deleted.

\begin{table}[h]
\scriptsize
\centering
\setlength{\tabcolsep}{6pt}  
\begin{tabular}{@{}lrr@{}}  % @{} removes padding at edges, r aligns numbers
\textbf{} & Training & Test \\
\hline \hline
Number of sentences & 88{,}689{,}425 & 5{,}443{,}427 \\
Number of tokens & 2{,}284{,}636{,}243 & 137{,}600{,}815 \\
Number of unique tokens & 21{,}707{,}092 & 2{,}336{,}552 \\
\hline
\end{tabular}
\caption{Dataset Statistics}
\label{tab:dataset-stats}
\end{table}

\section{Automated Interpretability Pipeline}

\subsection{Implementation of Automated Interpretability}
\label{appendix-autointerpretability-pipeline}

\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{p{2.2cm}p{1.2cm}p{4.6cm}p{6cm}} 
Model                 &   Layers       &       HuffingFace             & Descriptions   \\ \hline \hline 
Gemma-2-2b            & 3, 12, 20, 25 &   google/gemma-2-2b  &  --     \\
Gemma Scope 16K       &    20         & google/gemma-scope-2b-pt-res/tree/main/ layer\_20/width\_16k/average\_l0\_71  & neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k    \\
Gemma Scope 65K       &    20         & google/gemma-scope-2b-pt-res/tree/main/ layer\_20/width\_65k/average\_l0\_114  & neuronpedia.org/gemma-2-2b/20-gemmascope-res-65k    \\
Llama3.1-8B-Instruct  & 19   & meta-llama/Llama-3.1-8B-Instruct &  github.com/TransluceAI/observatory.git \\ \hline 
\end{tabular}
\caption{Sources for models, SAEs and feature descriptions, used in this work. }
\label{tab:models-sources}
\end{table*}

Our experiments are based on models and feature descriptions presented in Table~\ref{tab:models-sources}. We generate our feature descriptions as follows. 
%
In the implementation of the autointerpretability pipeline, we're closely following \cite{bills2023language, paulo2024automaticallyinterpretingmillionsfeatures, rajamanoharan2024jumpingaheadimprovingreconstruction} and others: we pass the natural dataset, used for generating descriptions (see Appendix~\ref{appendix-dataset}) through the model, and via forward hooks we access the activations of each feature and each layer. This part can also be easily parallelized. With \texttt{Gemma Scope} SAEs, a wrapper class is implemented, that is built into the model as another module. It can be extended easily to other SAEs. 

After passing the whole dataset through the model, for each feature we take top $10^3$ data samples based on the maximum activation of tokens. We only consider the maximum activating token of a data sample when we do the sorting. Later, we uniformly subsample the necessary number of data samples, i.e. 5, 15, ..., 50, that are further passed to an explainer LLM. 

The reason why we subsample from top $10^3$ is to avoid outliers in terms of activation. Top $10^3$ still represents top 0.001\% of the dataset. More complex sampling strategies can bring better performance, as described in Appendix~\ref{appendix-related-work}, but their implementation and evaluation is left out for the future work. 

Experiments are performed on the following models: \texttt{Gemma-2-2b} layer 20 \cite{gemmateam2024gemma2improvingopen}, \texttt{Llama-3.1-8B-Instruct} layer 19 \cite{grattafiori2024llama3herdmodels}, and SAEs \texttt{Gemma Scope} 16K and 65K layer 20 \cite{lieberum-etal-2024-gemma}. 
We also generate descriptions using baseline methods, namely TF-IDF and unembedding matrix projection. \textit{Term Frequency-Inverse Document Frequency (TF-IDF)} is a widely used technique in NLP for measuring the importance of a word in a document relative to a corpus. It balances word frequency with how uniquely the word appears across documents, assigning higher scores to informative words while down-weighting common ones. We generate these values using 15 maximally activating samples. On the other hand, the \textit{unembedding matrix} (\( W_U \)) \cite{bloom2024understandingfeatureslogitlens} in transformer models maps the residual stream activations to vocabulary logits, determining word probabilities in the output. By analyzing projections onto this unembedding matrix, we gain insight into how learned features influence token predictions. 
To generate SAE unembedding descriptions, we generate logit weight distribution across the vocabulary, and then use  the top ten words with the highest probabilities as feature description.
\[
\text{Logit weight distribution} = W_U*W_{dec}[feature]
\]
here, \( W_U \) is the unembedding matrix of a transformer model and \( W_{dec}\) are the decoder weights of sparse auto encoders.

This reveals which words are most associated with a given feature, enabling interpretability of sparse autoencoder (SAE) features, as well as MLP neurons. Both these methods are cheap baselines to compare with different auto-interpretability generated descriptions.


\subsection{Prompts Engineering}
\label{appendix-prompts}
Our feature description pipeline consists of several key components. The Subject Model for which the descriptions are getting generated, while the Explainer Model is a larger model used to generate descriptions. The System Prompt provides task-specific instructions to the LLM, detailing what to focus on in the provided samples and how to format the output. We append 5, 10, or 50 sentences along with a \texttt{user\_message\_ending} at the end, which helps reinforce the expected output structure. Before normalizing activations, we first average activations for tokens belonging to the same word. These values are then normalized between 0 and 10. For delimiters, we use single curly brackets if the activation intensity is below 4 and double curly brackets otherwise. For numerical input, we provide the LLM with dictionary of most activated token at the end of each sample. 
\begin{table}
    \scriptsize
    \centering
    \begin{tabular}{lcc}
        & Avg \# of tokens & Cost/$10^3$ feature (\$)\\
        \hline
        \hline
        \noalign{\vskip .5mm}  
 0-shot &  1,423 & 2.13\\
 1-shot &  1,498 & 2.25\\
        2-shot  & 1,564  & 2.35\\
        10-shot & 2,395  & 3.95\\
        20-shot & 3,393  & 5.09\\
        \hline
    \end{tabular}
    \caption{Token usage and cost comparison for different shot settings.}
    \label{tab:token-cost-comparison}
\end{table}

\begin{table}
    \scriptsize
    \centering
    \begin{tabular}{cc}
        \# of sentences & Avg \# of tokens \\
        \hline
        \hline
        \noalign{\vskip .5mm}  
        5  & 333  \\
        15 & 964  \\
        50 & 2,507 \\
        \hline
    \end{tabular}
    \caption{Average number of tokens as the number of sentences increases. These values are based on tokenizer used for \texttt{Gemma-Scope-16k} and are not the number of tokens per requests generated by openai.}
    \label{tab:avgtokens_per_numberofsamples}
\end{table}
\textbf{Main prompt: The main prompt for our we use to generate descriptions is given below:}
\begin{lstlisting}

You are a meticulous AI researcher conducting an important investigation into sparse autoencoders of a language model that activates in response to specific tokens within text excerpts. Your overall task is to identify and describe the common features of highlighted tokens, focusing exclusively on the tokens that activate and ignoring broader sentence context unless absolutely necessary.

You will receive a list of sentences in which specific tokens activate the neuron. Tokens causing activation will appear between delimiters like {{ }}. The activation values range from 0-10:

- If a token activates with an intensity of <4, it will be delimited like {{this}}.
- If a token activates with an intensity of >4, it will be delimited like {{{{this}}}}.

Guidelines:
1. Focus on the activated tokens: The description must primarily relate to the highlighted tokens, not the entire sentence.
2. Look for patterns in the tokens: If a specific token or a group of similar tokens repeatedly activates, center your analysis on them.
3. Sparse Autoencoder Dependency: The activations depend only on the words preceding the highlighted token. Descriptions should avoid relying on words that come after the activated token.
4. No coherent presence of concept: Return 'NO CONCEPT FOUND' if there is no coherent theme in the sentences provided, do not force a concept and stay grounded.
Output Format:
Concept: [Focus on the common concept tied to the highlighted tokens, described in a concise phrase.]

### Example:
Input example 1:
Sentence 1: The {{United States}} will not allow {{threats}} against its people.  
Sentence 2: The {{U.S.}}  emphasizes deterrence in foreign policy.

Example Output:  
Concept: U.S. deterrence policies and moral stance on global threats.

Input example 2:
Sentence 1:  Ash/Brock [Bouldershipping]\nI forget about this one {{all}} for one favours.
Sentence 2: I see this attitude brewing {{all}} at once.
Sentence 3: I get emails from people {{all}} over the world.

Example Output:  
Concept: Presence of the word 'all'.

user_message_ending: >
Analyze all these sentences as ONE corpus and provide your description in the following format:

Concept: Your devised concept and it's description in a concise manner, and very few words.

\end{lstlisting}

\textbf{Prompt variation 1: }using numbers, instead of delimiters for further experiments as mentioned in , we include small variations in the main prompt

\begin{lstlisting}

You are a meticulous AI researcher [...]
 
You analyze a list of most-activated sentences and a dictionary of relevant tokens, each with assigned activation values, to identify key themes and concepts. Tokens causing activations will be provided in the dictionary after each sentence. The activation values range from 0-10.
          
Guidelines:[..]
          
Output Format:
Concept: [Focus on the common concept tied to the highlighted tokens, described in a concise phrase.]
          
EXAMPLE 1
Sentence 1: "The choir's harmonies resounded throughout the church as the congregation stood in awe."
Most relevant tokens: {{"harmonies": 9, "resounded": 8, "church": 6, "congregation": 4}}[..]


\end{lstlisting}
\textbf{Prompt variation 2:} main prompt + no shotsc\newline
\textbf{Prompt variation 3:} main prompt + one shot example \newline
\textbf{Prompt variation 4:} main prompt + five shots \newline

\subsection{Computational Costs}
\label{appendix-autointerpretability-costs}

Obtaining maximum activating samples for features is performed locally on a cluster with 8 NVIDIA A100 GPUs with VRAM 40Gb. The process can be easily parallelized, but for cost calculation we are using the total consumed time. Such GPUs can be rented starting with \$0.67/hr, however, an average price on the market exceeds this value. For simplicity of calculations, we take a price of \$1/hr. For convenience, we consider a price per $10^3$ features, the results are presented in Table~\ref{tab:maxact-costs}. 

\begin{table}[t]
    \scriptsize
    \centering
    \begin{tabular}{p{1.9cm}p{1.2cm}p{0.85cm}p{2.15cm}}
                             & \# of features & Time (h) & Cost/$10^3$ features (\$) \\
        \hline
        \hline
        \noalign{\vskip .5mm}  
        Gemma-2-2b            & 239,616  & 185  & 0.77  \\
        Gemma Scope 16K       & 16,384  & 524  & 31.98  \\
        Gemma Scope 65K       & 65,536  & 622  & 9.49 \\
        Llama-3.1-8B          & 458,752  & 361  & 0.79  \\
        \hline
    \end{tabular}
    \caption{Cost comparison for discovering samples, that are maximally activating features. }
    \label{tab:maxact-costs}
\end{table}

Due to the differences in the implementation, calculating maximum activating samples is significantly cheaper for the complete models, since we are doing it at the same moment for the whole model, and the price is divided on the total number of features that we consider. For SAEs, in the current implementation we only do it per one layer, which is significantly reducing the number of considered features. In the future we are planning on optimizing this process in a way, that it is possible to calculate it at the same time for multiple chosen SAEs. The size of SAEs is also playing an important role: in terms of total costs it is more expensive, but if we consider the cost per $10^3$ features, the larger the SAE - the cheaper it is. 

The total computational cost for this part of the experiments is estimated at 4,920 GPU hours. This includes all experiments conducted on the models referenced in this paper. While not all experiments are explicitly discussed, they contributed to the final results presented here.


\section{\ours \ Evaluation Framework}\label{appendix-evaluation}

Our framework is designed to work with a wide variety of subject models, including most HuggingFace Transformers \citep{wolf-etal-2020-transformers} as well as any feature implemented as a named module in PyTorch. Moreover, our framework is extensible: interpretability tools such as SAEs or various supervised interpretability techniques can be integrated with minimal effort, provided they implement some basic steering functions. In addition, we offer an interface for a diverse set of evaluating LLMs, whether open-weight or proprietary, with support for platforms such as vllm, Ollama, OpenAI, and Azure APIs. Our implentation can be found at \url{https://github.com/brunibrun/FADE}. %\repolink.


\subsection{Implementation and Computational Efficiency}\label{appendix-evaluation-implementation}

To compute the purity and responsiveness measures, we base our sampling from the natural dataset on the activations of the subject LLM. For each sequence, we calculate the maximum absolute activation value across all tokens. Using these values, we sample sequences by selecting a user-configurable percentage of those with the strongest activations and for the remainder, drawing an equal number of samples from each of the following percentile ranges: 
\([0\%, 50\%[\), 
\([50\%, 75\%[\), 
\([75\%, 95\%[\), 
and \([95\%, 100\%]\).

Computational efficiency is a key consideration in our design, as evaluating every neuron in an LLM can be prohibitively expensive. The cost of evaluations is dynamically adjustable based on several factors, including the number of samples generated and rated, the evaluating LLM used, and the natural dataset selection. 

Our method allows users to control the cost by setting the number of synthetic samples (denoted $n$) relative to the full size of the natural dataset ($N$). By pre-computing activations from the natural dataset in parallel, we effectively reduce the per-run complexity from $\mathcal{O}(N*M)$ for $M$ neurons to $\mathcal{O}(n*M)$. Given that $n$ is typically in the hundreds while $N$ is in the millions, this strategy yields big efficiency gain.

Additionally, we only execute the computationally intensive faithfulness evaluation when both clarity and responsiveness exceed a user-configurable threshold. This conditional execution ensures that unnecessary computations are avoided for features that do not meet our interpretability criteria.


\subsection{Details on the Experiment Setup}\label{appendix-evaluation-experiment-setup}

In our experiments we send 15 requests to the evaluation LLM for generating synthetic samples. We remove duplicates and use these as concept-samples. We use the whole evaluation dataset as control samples. For rating we draw 500 samples from the natural dataset, where we take 50 from the top activated and 450 from the lower percentiles, according to the sampling strategy outlined above in Appendix~\ref{appendix-evaluation-implementation}. If we obtain fewer than 15 concept-samples in this first rating we again sample 500 new samples with the same sampling approach. We rate 15 samples at once, and if one of the calls fails, for example due to formatting errors of the evaluating LLM, we retry the failed samples once. 
For the Faithfulness experiments we use the modification factors $[-50, -10, -1, 0, 1, 10, 50]$. We draw 50 samples from the natural dataset and let the subject LLM continue them for 30 tokens. We then rate only these continuations for concept strength by the evaluating LLM, again retrying once, if the rating fails. We only execute the Faithfulness experiment, if both the Clarity and Responsiveness of a feature is larger or equal to 0.5. 

\textbf{Licenses} \texttt{Gemma-2-2b} is released under a custom Gemma Terms of Use. Gemma Scope SAEs are released under Creative Commons Attribution 4.0 International. \texttt{Llama3.1-8B-Instruct} is released under a custom Llama 3.1 Community License. Transluce feature descriptions, Pile Uncopyrighted dataset and LangChain are released under MIT License. vLLM is released under  Apache 2.0 License.


\subsubsection{Prompts for the Evaluating LLM}\label{appendix-evaluation-experiment-setup-prompts}

\textbf{Generating Synthetic Data Prompt}

\begin{lstlisting}
You are tasked with building a database of sequences that best represent a specific concept. 
To create this, you will generate sequences that vary in style, tone, context, length, and structure, while maintaining a clear connection to the concept. 
The concept does not need to be explicitly stated in each sequence, but each should relate meaningfully to it. Be creative and explore different ways to express the concept.

Here are examples of how different concepts might be expressed:

Concept: "German language" - Sequences might include German phrases, or sentences.
Concept: "Start of a Java Function" - Sequences might include Java code snippets defining a function.
Concept: "Irony" - Sequences might include ironic statements or expressions.

Provide your sequences as strings in a Python List format.

Example: ["This is a first example sequence.", "Second example sequence but it is much longer also there are somy typos in it. wjo told you that I can type?"]

Output only the Python List object, without any additional comments, symbols, or extraneous content.
\end{lstlisting}


\textbf{Rating Natural Data Prompt}

\begin{lstlisting}
You are tasked with building a database of sequences that best represent a specific concept. 
To create this, you will review a dataset of varying sequences and rate each one according to how much the concept is expressed.

For each sequence, assign a rating based on this scale:

0: The concept is not expressed.
1: The concept is vaguely or partially expressed.
2: The concept is clearly and unambiguously present.

Use conservative ratings. If uncertain, choose a lower rating to avoid including irrelevant sequences in your database. 
If no sequence expresses the concept, rate all sequences as 0.

Each sequence is identified by a unique ID. Provide your ratings as a Python dictionary with sequence IDs as keys and their ratings as values.

Example Output: {{"14": 0, "15": 2, "20": 1, "27": 0}}

Output only the dictionary - no additional text, comments, or symbols."    
\end{lstlisting}


\subsubsection{Associated Cost}

The activation generation for the subject models for the results in section \ref{sec:Results} was run locally on a cluster with NVIDIA A100 GPUs with 40GB of VRAM. Similar to section \ref{appendix-autointerpretability-costs} we assume a price of \$1/hr. Since activations can be cached in parallel for the whole model, only a single pass over the evaluation dataset was needed per model. We estimate a needed time of 24 hours on one GPU for the activation generation, resulting in a cost of 24\$ per model. During the evaluations, only the activations of synthetic samples need to be computed. Due to a suboptimal configuration of the evaluation and subject LLM in our experiments we assume an average time of one minute per neuron evaluation, leading to an average cost of 0.024\$ per evaluated feature. The estimated cost for the evaluating LLM consists of the cost for the generation of synthetic samples as well as the cost for rating natural data. In the configuration used for the experiments, unless stated otherwise, we estimate the evaluating LLM creates about 2,000 tokens per feature evaluation, which corresponds to a cost of about 0.0012\$ per feature at an output token cost of 0.6\$ per million tokens. For the rating part the evaluating LLM recieves about 20,000 tokens as input, which corresponds to a cost of 0.003\$ per feature at an input token cost of 0.15\$ per million tokens. Corresponding results are presented in Table~\ref{tab:eval-subject-costs}.


\begin{table}[t]
    \scriptsize
    \centering
    \begin{tabular}{p{1.9cm}p{1.2cm}p{0.85cm}p{2.15cm}}
                             & \# of features evaluated & GPU Time (h) & Cost/ $10^3$ features(\$) \\
        \hline
        \hline
        \noalign{\vskip .5mm}  
        Gemma-2-2b            & 9,000  & 216 + 24  & 26.67  \\
        Gemma Scope 16K       & 25,000  & 600 + 24 & 24.96   \\
        Gemma Scope 65K       & 2,000  & 48 + 24 & 36 \\
        Llama-3.1-8B          & 2,000  & 48 + 24 & 36  \\
        \hline
    \end{tabular}
    \caption{Computational cost comparison for running the evaluation experiments on GPU.}
    \label{tab:eval-subject-costs}
\end{table}



\section{Extended Results}
\subsection{Quantitative Analysis}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Experiments/sae_sizes.png}
    \caption{Feature descriptions fit for SAEs of different sizes.}
    \label{fig:sae-sizes}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Experiments/gemma2_layers.png}
    \caption{Feature descriptions fit for different layers of Gemma-2. }
    \label{fig:gemma2-layers}
\end{figure*}
\label{appendix-extended-results}


This section provides a more in-depth analysis of the experimental results presented in Section~\ref{sec:Results}.

\textbf{SAEs concept narrowness results in lower interpretability}. 
This might seem counterintuitive at first, but again it is important to stress that we are not evaluating the features by themselves, but instead the adequacy of the proposed feature description to these features. 
One potential explanation is that larger SAEs distribute concepts more finely across features. As a result, a slightly inaccurate description that might have still activated a feature in a smaller SAE may fail to activate the corresponding feature in a larger SAE, where concepts are encoded even more sparsely. The consistency of this result is demonstrated by using different feature descriptions --- MaxAct*, produced in this work, and the ones available on Neuronpedia. 

Interestingly, we observe a small left-skewed peak in the responsiveness distribution for 65K SAEs labelled via MaxAct*, a pattern not seen in any other experiment (see Figure \ref{fig:sae-sizes}). A qualitative analysis suggests that this is primarily caused by features representing out-of-distribution concepts relative to the dataset used for evaluation, such as, e.g., ``new line'' feature, which due to the preprocessing steps is not present in the dataset used for descriptions generation (see Figure~\ref{fig:new-line-example}). Due to the lower quality of feature descriptions, the purity distribution for Neuronpedia descriptions of \texttt{Gemma Scope 65K} exhibits a bimodal pattern. High-quality descriptions tend to have high purity, reflecting the greater monosemanticity of the features. However, a substantial number of inaccurate descriptions lead to very low purity values.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Experiments/prompts_optimization.png}
    \caption{Analysis of the main prompt's components: (a) numeric input vs delimeter-based; (b) number of activating samples, provided to the explainer LLM; (c) number of shots used in the prompt. }
    \label{fig:prompts_analysis}
\end{figure*}

\textbf{Some Gemma-2 layers look almost not interpretable}. 
The feature distribution in layer 12 differs significantly from other layers. Unlike other layers, layer 12 lacks the characteristic rightward elevation in clarity, responsiveness, and purity scores, as presented on Figure~\ref{fig:gemma2-layers}. Manual analysis of the heatmaps supported the theory, that layer 12 demonstrates a high level of polysemanticity. Interestingly, similar result was demonstrated on \texttt{Llama-3.1-8B-Instruct} by \cite{choi2024automatic}. Evaluating several complete models with \ours\ would provide more insights into interpretability of different models components. 

\textbf{Numeric input shows marginally better performance}. As illustrated in Figure~\ref{fig:prompts_analysis} (a), the performance difference between numeric input-based prompts and delimiter-based highlighting is not statistically significant for both MLP neurons and SAEs, though the mean score is slightly higher for numeric input. A more comprehensive evaluation across multiple layers and a larger feature set is needed to determine the optimal approach. Nonetheless, these findings challenge prior assertions that highlighting the most activating tokens is superior due to LLMs' assumed difficulty in processing numerical inputs \cite{choi2024automatic}.

\textbf{Increasing sample count improves performance}. A clear trend emerges: providing more samples to the explainer LLM enhances its performance. However, this also increases the computational cost of feature generation due to the higher token count. While 15 samples yield strong results, 50 samples perform even better, as shown in Figure~\ref{fig:prompts_analysis} (b).


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Experiments/comparison_explainer_models.png}
    \caption{Performance of feature descriptions generation via different explainer LLMs.}
    \label{fig:explainer-models}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Experiments/baselines.png}
    \caption{MaxAct* comparison to feature descriptions, generated via other methods on \texttt{Gemma Scope 16K} and \texttt{Llama3.1-8B-Instruct}.}
    \label{fig:baselines}
\end{figure*}

\textbf{Increasing examples improves performance but raises costs}. As shown in Figure~\ref{fig:prompts_analysis} (c), increasing the number of shots consistently enhances performance on activation-based metrics without significantly affecting faithfulness, contradicting findings in \cite{choi2024automatic}. This discrepancy may stem from differences in provided examples or feature types, as our analysis focuses on \texttt{Gemma Scope SAEs}, whereas \cite{choi2024automatic} examined \texttt{Llama-3.1-8B-Instruct} neurons. Additionally, a higher shot count increases computational costs due to the larger token input (see Table~\ref{tab:token-cost-comparison}). In this study, we primarily use 2-shot prompts, balancing performance and cost efficiency.

\textbf{Stronger explainer LLMs yield better feature descriptions}. More capable models consistently achieve higher performance across nearly all metrics, as presented in Figure \ref{fig:explainer-models}. \texttt{Llama-3.1-70B-Instruct} (quantized) performs comparably to \texttt{GPT-4o mini}, aligning with findings from the evaluation of LLMs (see Table~\ref{tab:evaluation-models-comparison}). Performance generally declines with model size, except for \texttt{Llama-3.2-1B-Instruct}, which frequently fails to adhere to the required output format, leading to significantly poorer results across all metrics.

\textbf{MaxAct* demonstrates superior performance}. Our findings highlight the importance of considering all four \ours\ metrics when optimizing automated interpretability approaches. On \texttt{Gemma Scope 16K}, MaxAct* outperforms all baselines across all metrics except faithfulness (see Figure~\ref{fig:baselines}). We show that our generated descriptions significantly surpass those currently available on Neuronpedia. Additionally, we compare MaxAct* to TF-IDF and unembedding-based baselines (see Appendix~\ref{appendix-autointerpretability-pipeline}). While the unembedding method underperforms in activation-based metrics such as clarity, responsiveness, and purity, it achieves notably higher faithfulness by explicitly considering the feature’s output behavior. This underscores that faithfulness depends on both the feature type (e.g., SAEs vs. MLP neurons) and the generated description.


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Qualitative_Examples/self_and_other_examples.png}
    \caption{Examples of feature descriptions, obtained via MaxAct*, demonstrating low clarity, but medium or high responsiveness and purity. Feature 475: ``Frequent presence of token 'self' indicating object oriented programming concept''; feature 653: ``Word 'make' in different forms, expressions and concepts''; feature 821: ``Expressions indicating lists or explanations''. }
    \label{fig:low-calrity-heatmaps}
\end{figure*}

Our results align with \cite{gurarieh2025enhancingautomatedinterpretabilityoutputcentric}, which demonstrates that combining MaxAct-like approaches with output-based methods enhances overall feature description quality. However, the input-centric metric used in that work does not fully capture failure modes that clarity, responsiveness, and purity account for.

This becomes particularly evident when comparing MaxAct* to feature descriptions generated for \texttt{Llama-3.1-8B-Instruct} in \cite{choi2024automatic}. While clarity scores are comparable—albeit slightly lower for MaxAct* --- responsiveness and purity show significant improvements. This difference may partially stem from dataset construction variations. Notably, the purity distributions of the two approaches are strikingly different, even opposing: MaxAct* exhibits a right-skewed peak, whereas Transluce’s feature descriptions perform poorly overall. Faithfulness differences are minor but still favor MaxAct*, likely due to the generation of higher-quality feature descriptions.

\subsection{Qualitative Analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Qualitative_Examples/llama_feature_5183.png}
    \caption{Heatmap and descriptions evaluation result for feature 5183 of \texttt{Llama3.1-8B-Instruct} layer 19.}
    \label{fig:feature-5183}
\end{figure}

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.5cm}p{5.5cm}}
Method & Label  \\ \hline \hline
Transluce & activation on names with specific formatting, including "Bryson," "Brioung," "Bryony," and "Brianna"  \\ 
MaxAct* & Presence and significance of the word "is" \\ \hline
\end{tabular}
\caption{Descriptions for \texttt{Llama3.1-8B-Instruct} feature 5183 layer 19.}
\label{tab:feature-5183}
\end{table}


 \begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Qualitative_Examples/out_of_distribution_example.png}
    \caption{Feature 3,286 of \texttt{Gemma Scope 65K} SAE. MaxAct* description: ``Mathematical expressions and significant numerical values''. Neuronpedia description: ``function definitions in a programming context''. }
    \label{fig:new-line-example}
\end{figure*}

\textbf{Fine-tuning automated interpretability requires consideration of all metrics in \ours}. The comparison of metric distributions against the simulated activation-based metric in Figure \ref{fig:metrics-comparison} highlights that relying solely on this metric is insufficient for accurately assessing the quality of feature descriptions. If an automated interpretability framework is fine-tuned exclusively on such a metric, it may generate suboptimal descriptions.

For instance, evaluations indicate that the description for feature 5183 in layer 19 of \texttt{Llama3.1-8B-Instruct}, as generated in \cite{choi2024automatic}, performs well in terms of clarity and responsiveness, yet achieves a near-zero purity score (see Figure \ref{fig:feature-5183}). Conversely, a description produced using MaxAct* (see Table \ref{tab:feature-5183}) exhibits lower clarity and responsiveness but significantly higher purity.

The heatmap of activations during description generation suggests that the description from \cite{choi2024automatic} strongly activates this feature. While the heatmap does not show all the names listed in Transluce’s description, this may be due to the sampling method, which selects random sentences from the top 1000 to mitigate outliers. However, activation is observed on similar names, such as “Bernstein” and “Brittney.” More importantly, this clearly polysemantic feature responds to multiple distinct concepts, including the word “is” in specific contexts (included into a description generated via MaxAct*), as well as certain coding patterns. 

As a result, despite the relatively high metric score of 0.77 in Transluce’s evaluation, the description has very low purity. This underscores the importance of considering not only how well a concept activates a feature but also other interpretability factors measurable with \ours. In this case, although the feature is inherently difficult to interpret, we argue that the MaxAct* description provides a more accurate representation, as it better captures the feature’s activating pattern, and \ours\ is clearly demonstrating the feature's polysemanticity.


\textbf{SAEs descriptions often fail to accurately capture the concept}. Figure~\ref{fig:low-calrity-heatmaps} presents the heatmap for feature 475, whose description emphasizes the occurrence of the token ``self.'' However, the feature does not activate on all instances of ``self'' (highlighted in yellow), indicating that a crucial aspect of the concept is missing or remains unclear. Additionally, the feature activates on other tokens, further suggesting that the description is incomplete.

This is reflected in the evaluation metrics: low clarity indicates that the concept is not expressed precisely enough for the evaluating LLM to generate synthetic data that reliably activates the feature. A responsiveness score of 0.93 suggests that the feature does activate on natural data aligned with the concept, while a purity score of 0.81 reveals that although the feature is primarily associated with the described concept, it also responds to other inputs.

Similar issues arise in features 653 and 821, where the underlying concepts appear highly specific -- activating on a particular token within a specific context. However, their descriptions are overly broad, making it difficult to generate synthetic data that reliably triggers the feature. These and many other similar features contribute to the left-skewed peak in the clarity distribution, which becomes more pronounced for narrower concepts represented by features in \texttt{Gemma Scope 65K}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments/Appendix_Qualitative_Examples/feature_9295.png}
    \caption{Heatmap and descriptions evaluation result for feature 9295 of \texttt{Gemma Scope} layer 20.}
    \label{fig:feature9295}
\end{figure}

\begin{table}[h]
\scriptsize
\centering
\begin{tabular}{p{1.5cm}p{5.5cm}}
Method & Label  \\ \hline \hline
Neuronpedia & The presence of JavaScript code segments or functions  \\
TF-IDF & asdfasleilse asdkhadsj easy file jpds just mean span think \\
Unembedding & f,  <eos>, fd, wer, sdf, df, b, jd, hs, ks\\
MaxAct* & Presence of nonsensical or random alphanumeric strings \\ \hline
\end{tabular}
\caption{Descriptions for \texttt{Gemma Scope} feature 9295 layer 20.}
\label{tab:feature9295}
\end{table}

\textbf{Out-of-Distribution Features in \texttt{Gemma Scope 65K} SAEs}. The dataset used for automatic interpretability omitted certain concepts, such as "new line," leading to gaps in feature descriptions. These omissions contribute to the small left-side peak in responsiveness distribution in Figure~\ref{fig:sae-sizes}. Several features, including 3315, 3858, and 4337, lack activation heatmaps under the MaxAct* approach, as the dataset does not represent their concepts. Consequently, the explainer model, relying on unrelated sentences, generates incorrect descriptions (see Figure~\ref{fig:new-line-example}). Heatmaps from Neuronpedia\footnote{https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-65k/3286} reveal what would activate these features, highlighting limitations of the dataset, used in this work, and broader issues in the automated interpretability pipeline. For example, despite obtaining and visualizing correct results, feature descriptions available on Neuronpedia are also not representing a correct concept. Similar results have been obtained for the <bos> token and indentation in text and code.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{attachments//Appendix_Qualitative_Examples/feature1139.png}
    \caption{Heatmap and descriptions evaluation result for feature 1139 of \texttt{Gemma Scope} layer 20.}
    \label{fig:feature 1139}
\end{figure}

\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{p{1.5cm}p{5.5cm}}
Method & Label  \\ \hline \hline
Neuronpedia & references to problematic situations or conflicts that cause trouble  \\
TF-IDF & trouble \\
Unembedding & troubles, difficulties, problems, troublesome, mischief, ...\\
MaxAct* & Activation of "into" and "trouble" indicating situations leading to problem \\ \hline
\end{tabular}
\caption{Descriptions for \texttt{Gemma Scope} feature 1139 layer 20.}
\label{tab:feature1139}
\end{table}

\textbf{Reliable Evaluation — \ours\ Identifies the Best Description}. Different automated interpretability methods prioritize either activation-based metrics or faithfulness-based measures, leading to descriptions that may be overly broad or inaccurate. 

In some cases, even manual inspection of heatmaps fails to fully capture the underlying concept represented by a feature. Therefore, a comprehensive evaluation must consider all four metrics. 

Table~\ref{tab:feature9295} presents feature descriptions generated by various methods. Based on the heatmap analysis, the MaxAct* description most accurately represents the concept. The unembedding method, while incorporating specific tokens promoted by the feature, also demonstrates strong alignment with the concept, as reflected in the corresponding metrics. However, it is not descriptive enough, which is resulting in lower responsiveness and purity.

Sometimes baseline methods may outperform more complex approaches, particularly on specific metrics. For instance, TF-IDF and unembedding baselines exhibit significantly higher faithfulness compared to Neuronpedia or MaxAct* for certain features (see Figure \ref{fig:feature 1139}).

Feature 1139, for example, influences the output of tokens related to the concept of ``trouble''. Descriptions that explicitly capture this aspect tend to achieve higher faithfulness (see Table \ref{tab:feature1139}). The MaxAct* description, in contrast, emphasizes the broader meaning and the most activating expression, ``into trouble'', leading to higher clarity.