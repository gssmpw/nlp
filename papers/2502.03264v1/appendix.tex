%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Appendix}
\subsection{Additional Related Work}
\label{sec:addRelatedWork}

\subsubsection{Deep Learning Models}

The attention mechanism \cite{Vaswani17} has proven 
to be highly effective, establishing transformer-based architectures as the dominant approach for MTS representation learning in the temporal domain \cite{Zhou21, Wu21, Li22a, Wang24, Xu22, Song23, Yang23, Du23}.   
These models outperform traditional RNN- and CNN-based networks \cite{Flunkert17, Lai18, Bai18, Cheng20}, particularly in capturing long-range dependencies, thereby delivering superior performance.  
However, time series data consists solely of scalar sequences indexed in time order, which constrains the ability to effectively learn the complex representations inherent in MTS when relying solely on temporal-domain information.  
%To address this limitation, 
Several studies have explored transforming time series data into the frequency domain using the Fast Fourier Transform (FFT) to extract additional insights from a different perspective. For instance, Fredformer \cite{Piao24} leverages frequency channel-wise attention to learn time series representations, while FreTS \cite{Yi23} employs MLPs to model both frequency-channel and frequency-temporal dependencies in MTS. Additionally, FITS \cite{Xu24} utilizes complex-valued linear layers to learn frequency-domain interpolation patterns. These models achieve results either on-par or better than their purely temporal analysis counterparts. However, the lack of temporal dependency analysis limits their ability to further improve performance.  
Recent studies have highlighted the advantages of combining both temporal and frequency domain information for enhanced performance. In this context, CDX-Net \cite{Li22b} designs hybrid networks that integrate CNN, RNN, and attention mechanisms to enhance feature extraction and fusion of multi-time series (MTS) data from both the temporal and frequency domains. FEDformer \cite{Zhou22} integrates seasonal-trend decomposition with Fourier analysis and a Transformer-based model to capture the global distribution and characteristics of MTS. TimeMixer++ \cite{Wangsy24} generates multi-scale time series through temporal down sampling, followed by FFT-based periodic component analysis, and applies inter- and intra-image attention mechanisms to learn robust representations of seasonal and trend components. 

However, most of these methods are designed for specific downstream tasks or require modifications to the input or output projection layers to adapt to different tasks. This focus limits their ability to generalize and extract broader knowledge from MTS data, confining them to task-specific characteristics and preventing them from functioning as general-purpose models.  
We propose a general pre-training strategy that simultaneously handles generative and reconstruction tasks, aiming to learn a universal representation of MTS datasets by combining both temporal and frequency domain features.  

\subsubsection{LLM Empowered MTS Foundation Models}
This line of works follow the paradigm that 
freeze LLM encoder backbones while simultaneously fine-tuning/adapting the input and distribution heads for forecasting, 
and notable ones include 
Time-LLM\cite{Jin2024}, LLM4TS\cite{Chang2024}, GTP4TS\cite{Zhou23}, UniTime\cite{Liu2024} and Tempo\cite{Cao2024}.  
This effectiveness of this paradigm is currently in debating 
in the sense that some works present promising results  
while the latest ablation studies show the counterpart \cite{Tan2024}. 
Nevertheless, this paper complement this 
research line by training time series model from scratch.  




 

\subsection{Details of Experiments}
\subsubsection{Datasets description}
\label{appendix:datasets}
We use the UTSD-12G dataset, released by \cite{Liu24}, for pre-training. The Unified Time Series Dataset (UTSD) includes seven domains: Energy, Environment, Health, IoT, Nature, Transportation, and Web, with varying sampling frequencies. It contains up to 1 billion time points and hierarchical structures, supporting large-scale time series model research. The overall statistics of UTSD-12G is shown in Table \ref{table:statistics of UTSD-12G}.


For downstream tasks like long-term forecasting and imputation, we conduct experiments on five widely used public datasets from \cite{Wu21}: ETTh, ETTm, Weather, Electricity, and Traffic. For anomaly detection, we use five popular datasets: SMD \cite{Su19}, MSL, SMAP \cite{Hundman18}, SWaT \cite{Mathur16}, and PSMAbdulaal \cite{Abdulaal21}. The statistics of the datasets for these tasks are listed in Table\ref{table:datasets for forecasting and Imputation} and \ref{table:datasets for AD}

\input{tables/stats_UTSD12G}
\input{tables/stats_data_forecast_imputation}
\input{tables/stats_data_Anomaly}

\subsubsection{Baseline model selection}
\label{appendix:benchmark}

We summarize the baseline models in Table\ref{table: classification of benchmark models}. We classify these models into four categories, including LLM-enhanced models for MTS analysis, MLP-based models, Transformer-based models, and MTS foundation models. The MTS foundation models are further divided into two sub-categories: task-specific foundation models and multi-task foundation models. Since each model has its own design goals and experimental settings, it is challenging to align them all for reproducing their best results presented in papers. Therefore, we follow established protocols from previous works and select typical models as benchmarks for each downstream task, ensuring a fair comparison of GTM with SOTA results.

\input{tables/benchmark_models}


%\textbf{Benchmarks for Long-term Forecasting}


%\textbf{Benchmarks for Imputation}

%\textbf{Benchmarks for Anomaly Detection}

\subsubsection{Experimental settings and implementation details}

\paragraph{Pre-training}
In the pre-training stage, we trained our GTM model on the UTSD-12G dataset \cite{Liu24}. During data preprocessing, we defined a lookback window of 1440 timestamps and split the raw data into overlapping samples with a stride $\tau = 192$. We then generated 15 patches with a patch size $L_p = 96$. For critical model hyperparameters, we set the batch size to 1024 and the learning rate to $1\times10^{-5}$, using Adam as the optimizer with a cosine annealing learning rate decay. We trained for 30 epochs with an early stopping mechanism, and the decay steps were proportional to the number of training epochs. In the model backbone, we set the number of layers (N-stack) to 12 and the feature dimension to 768. The Fourier Knowledge Attention layer consisted of 5 attention modules, each with a low-rank matrix parameterized by ${AB}$, where $A\in\mathbb{R}^{385\times 1} $, $B\in\mathbb{R}^{1\times 385}$. Finally, we implemented the GTM model in PyTorch \cite{Paszke19} and trained it on 6 NVIDIA A100 40GB GPUs.

\paragraph{Fine-tune}
\textbf{Long-term Forecasting}
For long-term forecasting, we directly reuse the pre-trained GTM model without any special adaptations, only removing the masking process. We dynamically choose look-back window in range $\left[ 96, 1440 \right]$ and forecast future time points $T\in \{96, 192, 336, 720\}$. The results are compared with the best-performing results SOTA models presented in papers or source codes.

\textbf{Imputation}
To align with benchmark settings, we follow the protocol proposed by \cite{Zhou23} for imputation tasks. We use point-wise missing ratios of $\{12.5\%, 25\%, 37.5\%, 50\%\}$ at the time-point level for interpolation, omitting the patching process. For all other aspects, we reuse the settings from the pre-training stage.

\textbf{Anomaly Detection}
We use a common adjustment strategy \cite{Xu18, Su19, Shen20} for anomaly detection: if an anomaly is detected at any time point in an abnormal segment, all anomalies in that segment are considered detected. This approach is based on the fact that detecting one abnormal point usually triggers an alert for the entire segment in real-world scenarios. We calculate F1-scores for each datasets to evaluate the results. the As we do in other generative tasks, we directly reuse the GTM model settings from the pre-training stage.

\subsection{Full Results}
Due to space limitations in the main body of the paper, we provide the full experimental results in this section, to complement the discussion in section \ref{sec:exp}.


\subsubsection{Imputation}
\label{appendix:imputation}
Table\ref{table:full-imputation} provides the full results of Imputation for various data missing ratios of $\{12.5\%, 25\%, 37.5\%, 50\%\}$ at the time-point level. Except for the Electricity dataset (where it achieved second-best performance), GTM outperforms all other methods in other experiments.

\input{tables/full_imputation_results}


\subsubsection{Effectiveness of pre-training}
\label{appendix:effect of pretrain}
\paragraph{Forecasting}
Table \ref{table: GTM Vs. GTM w/o pre-train in forecasting} presents a detailed comparison between the pre-trained GTM model and the GTM model without pre-training. We also conduct experiments for different length $T\in \{96, 192, 336, 720\}$. The results demonstrate that pre-trained GTM model outperforms the non-pre-trained version, highlighting the benefit of the pre-training stage in leveraging general knowledge from large-scale datasets.

\input{tables/full_pretrain_forecasting}


\paragraph{Imputation}
Table \ref{table:GTM Vs. GTM w/o pre-train in imputation} provides detailed results of comparison in Imputation tasks between the pre-trained GTM model and the GTM model without pre-training. As described in Sec\ref{exp:imputation}, we also conduct experiment for different data missing ratios of $\{12.5\%, 25\%, 37.5\%, 50\%\}$ at the time-point level. As expected, the pre-trained GTM model outperforms the non-pre-trained version in all tests, achieving significant improvements.

\input{tables/full_pretrain_imputation}



\subsubsection{Ablation test}
\label{appendix:ablation}
Table \ref{table: full ablation forecasting} presents the full ablation results for forecasting tasks with varying prediction lengths, includes $T\in \{96, 192, 336, 720\}$ time points. The comparison involves the complete GTM model, an advanced version of GTM without the frequency knowledge attention module, and a baseline version that includes only the temporal analysis module. The results demonstrate that the complete design of the GTM model effectively supports the learning of universal representations for MTS datasets with varying time granularities.

\input{tables/full_ablation}



\subsection{Visualization analysis}

\subsubsection{Distribution discrepancy of MTS datasets}
\label{measurement analysis}
We conduct measurement analysis on UTSD-12G datasets and 5 popular multi-domain datasets for downstream tasks as described in Table \ref{table:statistics of UTSD-12G} and \ref{table:datasets for forecasting and Imputation}.  To complement the limited information available in the temporal domain, we transform the datasets into the frequency domain using FFT. This allows us to analyze data distribution patterns from various perspectives, including amplitude, phase, periodicity, frequency resolution, etc.. 
Due to the complexity of the joint distribution, we apply a non-parametric estimation method, specifically 2-D Kernel Density Estimation (KDE) (Eq\ref{eq:2D-KDE}), to estimate the joint probability density distribution (PDF) of amplitude-frequency and phase-frequency for time series data with varying granularities. We use a 2-D Gaussian kernel function (Eq\ref{eq:2D-Gaussian Kernel}) and 2-D Scott’s rule (Eq\ref{eq:2D-Scott's rule}) as bandwidth fuction. Where $n$ denotes number of data samples, $h$ is the bandwidth, $\sigma$ and $\mu$ are standard deviation and mean of the samples. The results are presented in Figures \ref{fig:fre-dist-amp} and \ref{fig:freq-dist-phase}, respectively.
The figures reveal notable discrepancies in the joint distributions across MTS datasets with different time granularities. 
This observation highlights the importance of learning these distribution discrepancies as critical knowledge in the process of building a universal representation of MTS, which has often been overlooked in previous studies.

\begin{equation} 
\label{eq:2D-KDE}
    \hat{f}(x,y)=\frac{1}{n h_x h_y}\sum_{i = 1}^{n}K\left(\frac{x - x_i}{h_x},\frac{y - y_i}{h_y}\right)
\end{equation}

\begin{equation} 
\label{eq:2D-Gaussian Kernel}
    K(x,y)=\frac{1}{2\pi\sigma_x\sigma_y}\exp\left(-\frac{(x - \mu_x)^2}{2\sigma_x^2}-\frac{(y - \mu_y)^2}{2\sigma_y^2}\right)
\end{equation}

\begin{equation} 
\label{eq:2D-Scott's rule}
    h_x = h_y = n^{-\frac{1}{6}}(\sigma_x \sigma_y)^{\frac{1}{2}}
\end{equation}


\input{Figures/VG-3Dphase/3D_phase}


\subsubsection{Long-term Forecasting}
To clearly present the results, we select some representative samples for visualization analysis. 
Figure\ref{fig:visual-forecasting} shows the long-term forecasting results from 4 different datasets. We select 3 typical forecasting results from 3 different dimensions of each datasets.

\input{Figures/GTM_result/forecasting}
\input{Figures/GTM_result/imputation}
\input{Figures/GTM_result/anomaly}
\subsection{Imputation}
Figure\ref{fig:visual-imputation} illustrates the imputation results from three dimensions across four different datasets. Clearly, GTM can effectively reconstruct the missing data, adapting to varying data patterns.



\subsubsection{Anomaly Detection}
Figure \ref{fig:visual-anomaly} demonstrates four anomaly events detected by GTM in two datasets, along with their corresponding anomaly scores. The results align precisely with the labeled anomalies in the data.


\subsection{Limitations and future work}
So far, GTM has shown promising results in multi-task analysis of MTS, offering a novel approach to learning universal representations of MTS data. However, several challenges remain. Through a comprehensive survey of time series analysis models, we identified a significant issue beyond the scope of universal knowledge learning: the absence of unified benchmarks. Current models are often pre-trained on different datasets and employ varied hyperparameter settings, or involve complex processes to optimize performance. Consequently, researchers are left with two primary options for comparison: citing the best results from previous papers or attempting to reproduce these results under new conditions. However, due to time and resource constraints, build a framework for fair comparisons is challenging. Establishing a standardized, unified benchmark would be immensely beneficial for future research in this field.

Additionally, with adequate resources, further development of the GTM model, from employing a few low-rank knowledge attention modules to constructing extensive mixtures of experts (MoE) architectures, could significantly enhance its knowledge learning capabilities. This evolution could lead to more robust and versatile models, capable of addressing the diverse needs of MTS analysis.
