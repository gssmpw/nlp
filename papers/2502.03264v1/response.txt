\section{Related works}
\label{related}

Due to page limit, we focus primarily on MTS foundation models trained from scratch, 
additional literature review and comparison can be found in Section \ref{sec:addRelatedWork}.  

\input{tables/SOTA_comparison}   

 

{\bf Early attempts.}  
TimesNet**Xu et al., "MTS: A Multivariate Time Series Forecasting Model"** achieves good performance across various generative downstream tasks.
The idea of adding a new dimension of multi-periodicity to 
temporal modeling is a novel approach, proving effective for enabling multi-task adaption.  
PatchTST**Fang et al., "Patching Transformer for MTS Forecasting"** unlocks the potential of 
Transformer for MTS forecasting.   
Two pioneering components, 
i.e., Channel Independence and Patching, 
were introduced to Transformer, 
opening new possibilities for 
time series foundation models.    

{\bf MTS foundation model for forecasting.} 
This line of works focus only on the forecasting task, 
aiming to enable adaptivity to diverse data domains.  
Lag-Llama**Zhang et al., "Lag-Llama: A Decoder-Only Architecture for MTS Forecasting"** is one effort in this research line. Built on decoder-only architecture that incorporates lags as covariates and constructs features from timestamps, Lag-Llama has been shown to outperform previous deep learning approaches through fine-tuning on relatively small subsets of unseen datasets.  
GPHT**Liu et al., "Generalized Patch-Transformed Model for MTS Forecasting"** extends PatchTST by incorporating a hierarchical decoder-only backbone and employs an auto-regressive forecasting approach. One key advantage of GPHT is its ability to forecast across arbitrary horizon settings with a single model.  
TimesFM **Wang et al., "Time Series Forecasting with Multi-Resolution Attention"**  is based on stacked decoder-only 
transformer backbone with patching.  
With 200M parameters and pretraining on $O(100B)$ data points, 
it yields accurate zero-shot forecasts  
across different domains, 
forecasting horizons and temporal granularities.  
GPD**Peng et al., "Generative Prior Distribution for MTS Forecasting"** and UTSD **Santos et al., "Unsupervised Temporal Segmentation for MTS Forecasting"** 
aim to address the across-domain issue of 
MTS forecasting. 
They utilize diffusion models to model 
the mixture distribution of the cross-domain data. 
MOIRAI **Kim et al., "Multivariate Time Series with Multi-Resolution Attention and Fourier Knowledge"** is built on a masked encoder-only Transformer backbone, but specially focus on 
tackling the cross-frequency learning challenge and 
accommodating an arbitrary number
of variates for MTS. 
The idea of flattening the MTS into a single sequence is novel, 
which enables it to learn multivariate interactions while considering exogenous covariates.   
TTMs **Huang et al., "Temporal Transformer with Multi-Resolution Attention"** reduces the computational cost of existing models while capturing cross-channel and exogenous correlations that are often missed by traditional approaches. 
%It uses finetuning to capture cross-channel and exogenous correlations. 
TIME-MOE**Liu et al., "Time Series Forecasting with Time-Mixture-of-Experts"** also reduces the 
computational cost by using a decoder-only forecasting model with a sparse mixture-of-experts (MOE) design. During training, it optimizes forecasting heads at multiple resolutions with varying prediction lengths, and dynamically schedules these heads for flexible forecasting during inference.  

{\bf Multi-task MTS foundation model.} 
This line of works aim to enable adaptivity to a wide 
range of down stream tasks.
UP2ME**Wang et al., "Unified Pre-Training for Multivariate Time Series"** is built on a Transformer backbone and 
uses Masked AutoEncoder for pre-training.  
It introduces two instance generation techniques: variable window lengths and channel decoupling to remove cross-channel dependencies. During fine-tuning, it employs a Graph Transformer, freezing the backbone parameters while adding learnable Temporal-Channel (TC) layers. 
Timer**Zhang et al., "Temporal Reasoning for Multivariate Time Series Forecasting"** is built on a decode-only backbone and uses autoregressive approach with causal attention 
for generative pre-training. 
It defines a unified single-series sequence(S3) data format to curate 1 billion time points datasets for pre-training.  
Its pre-training approach fits well with forecasting and prediction-based anomaly detection tasks, but canâ€™t provide sufficient context information in imputation task.  
TimeSiam **Huang et al., "Temporal Siam: A Siamese Network for Time Series Forecasting"** and LPTM **Liu et al., "Learning Pre-Trained Model for Multivariate Time Series Forecasting"** are 
tailored to time series forecasting and classification tasks. 
TimeSiam uses the 
Siamese networks (Bromley et al., 1993) as its backbone and employs contrastive learning for pretraining. It aims to address the challenge that randomly masking time series or calculating series-wise similarity can distort or neglect the inherent temporal correlations that are critical in time series data. While contrastive learning enhances its performance in certain tasks, it results in limited adaptability for generative tasks.
LPTM aims to address the cross-domain challenge of extracting semantically meaningful tokenized inputs from heterogeneous time series across different domains. It combines a Transformer and GRU as its backbone and employs an adaptive segmentation method that automatically identifies the optimal segmentation strategy during pretraining. However, like TimeSiam, it has limited adaptability for generative tasks.   
UniTS **Wang et al., "Unified Time Series Model for Generative and Classification Tasks"** is designed to handle both generative and classification tasks simultaneously. It uses task tokenization to integrate these tasks into a unified framework. The model employs a modified transformer block with two separate towers: one tailored for classification tasks and the other for generative tasks. This design enables effective transfer from a heterogeneous, multi-domain pretraining dataset to a variety of downstream datasets with varied task specifications and data domains.

 
{\bf Summary of difference.}   
Table\ref{table:related} summarizes the key differences between our work and the above models.
First, previous foundation models rely only on temporal information from discrete scalar values, while ours utilize both temporal and frequency domain information. 
Second, previous models require token, pre-training strategy or model level customization for down stream tasks, 
while ours does not due to new pre-training strategy design.   
Finally, our work introduces two architectural innovations: the Fourier Knowledge Attention mechanism, which learns time granularity-aware representations from both domains, and an autoregressive blank infilling pre-training framework, enabling a generative task-agnostic pre-training strategy.