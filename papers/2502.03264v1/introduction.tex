\section{Introduction}
\label{intro}

Recently, there is a surge of interests 
in time series foundation models that 
can accommodate diverse data domains and 
support a wide range of downstream tasks \cite{Wu23,gao24}.  
There are two typical categories of MTS downstream tasks: 
(1) generative tasks including forecasting, imputation and anomaly detection; 
(2) predictive tasks including classification \cite{gao24}.  
 
One central question in building MTS foundation model 
is universal knowledge representation, 
and yet it still remains open \cite{gao24}.  
Formal definition of universal knowledge of MTS 
is still missing.  
The dominant paradigm is to encode MTS knowledge within a black-box model, 
with downstream task performance serving as the golden metric\cite{Zhou23,Rasul23,Ekambaram24,Shi24,gao24}. 
The development of methodologies is driven by understanding the multifaceted nature of MTS, where the time domain captures temporal variation, and the frequency domain depicts amplitude and phase variation. 
            
In the task-specific knowledge representation setting, 
both the time domain and frequency domain are extensively 
studied \cite{Piao24, Wang24,Zhou22,Wangsy24}. 
Deep learning models, especially Transformer-based models, have demonstrated strong representational capabilities, as evidenced by their effectiveness in capturing long-range dependencies\cite{Piao24, Wang24}. 
Recent studies have highlighted the advantages 
of combining both temporal and frequency domain information for enhanced performance \cite{Zhou22,Wangsy24}.  
However, this kind of knowledge is far from universal, as these models often struggle with adaptability across diverse domains and are typically tailored to specific tasks.   

A number of MTS foundation models 
made notable progress in mitigating 
the aforementioned adaptivity limitation 
through exploiting the temporal domain.  
One typical line of works freeze LLM encoder backbones while simultaneously fine-tuning/adapting the input and distribution heads for downstream tasks.  
Its effectiveness is currently under debating 
in the sense that positive progress was reported  
such as    
Time-LLM\cite{Jin2024}, LLM4TS\cite{Chang2024}, GPT4TS\cite{Zhou23}, UniTime\cite{Liu2024} and Tempo\cite{Cao2024}, 
while latest ablation studies showed the counterpart \cite{Tan2024}.  
Another line of works train MTS foundation models 
from scratch \cite{gao24,Liu24}. 
For the forecasting task,  
a number time-series foundation model were shown to 
have nice adaptivity to diverse data domains 
\cite{Rasul23,Ekambaram24,Shi24}. 
Furthermore, several recent time-series foundation models have shown the ability to adapt to a wide range of generative tasks, including forecasting, imputation, and anomaly detection, simultaneously\cite{Liu24,Zhang24}. Others are even capable of extending across both generative tasks (e.g., forecasting) and classification tasks\cite{Dong24,Kamarthi23}.
However, the adaptability of contemporary multi-task foundation models faces a 'last-mile' bottleneck, as they often require token-level, pre-training strategy-level, or model-level customizations for downstream tasks.  
For instance, Timer\cite{Liu24} offers three pre-training strategies to accommodate down stream tasks, while UP2ME\cite{Zhang24} introduces specialized TC layers for task-specific fine-tuning to achieve improved performance


This paper investigates universal knowledge representation in MTS data from the first principle. Specifically, we begin by examining what additional aspects or features contribute to a more complete universal representationâ€”a question often overlooked by contemporary MTS foundation models. While both temporal and frequency domain information have been shown to contribute to task-specific ones, we question their sufficiency in achieving a more comprehensive results. Our findings reveal new dimensions of universal knowledge representation, which inspire the design of the GTM model, enhancing its capabilities for both knowledge representation and adaptivity.   
Our contributions are:   
 
(1) \textit{New findings in the knowledge of MTS data.} 
We show that besides conventional temporal domain and frequency domain information, 
time granularity is a crucial factor for universal 
knowledge representation.  

We use FFT and statistical techniques to 
analyzed large-scale time series datasets like UTSD-12G \cite{Liu24} 
(details of the analysis is presented in Section \ref{measurement analysis}).  
Figure \ref{fig:fre-dist-amp} shows the joint distribution of frequency and amplitude extracted from time series with varying time granularities. It is evident that time series with different time granularities exhibit distinct joint density distributions over the amplitude-frequency pair, as well as phase-frequency pair. This finding highlights the importance of time granularities as intrinsic elements of time series knowledge, which, however, are overlooked by all contemporary time series foundation models.  
 
(2) \textit{Frequency and time granularity aware backbone design.} 
Inspired by the aforementioned findings, 
we propose an $N$-stack Decoder-Only Backbone with low-rank modules to implement a frequency domain knowledge attention mechanism, enhancing universal knowledge representation of MTS data with varying time granularities. To the best of our knowledge, this is the first MTS foundation model to integrate Fourier Knowledge Attention modules, enabling the learning of time granularity-aware, universal representations from both the temporal and frequency domains.  

(3) \textit{Unified pre-training strategy.} 
We introduce an autoregressive blank infilling pre-training framework from the LLM field, adapted for MTS analysis, with a unified linear projection header to generate output data autoregressively. This strategy solves the 'one task, one model' challenge, overcoming the limitations of MTS foundation models, which often require customizations at the token, model, or pre-training level for downstream tasks.    

(4) \textit{Extensive evaluation.}  
We conducted extensive experiments comparing GTM with state-of-the-art (SOTA) models across typical generative downstream tasks, like forecasting, anomaly detection, and imputation. The results show that GTM outperforms baseline methods in nearly all aspects, further validating our findings and design principles.

 

\input{Figures/VG-3DAmp/3D_amplitude}
 
 

