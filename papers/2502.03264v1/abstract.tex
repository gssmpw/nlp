\begin{abstract} 
Universal knowledge representation 
is a central problem for multivariate time series(MTS)
foundation models and yet remains open.    
This paper investigates this problem from the first principle and it makes four folds of contributions.     
\textbf{First}, 
a new empirical finding is revealed: time series with different  
time granularities (or corresponding frequency resolutions)   
exhibit distinct joint distributions in the frequency domain. 
This implies a crucial aspect of learning universal knowledge, 
one that has been overlooked by previous studies. 
\textbf{Second}, a novel Fourier knowledge 
attention mechanism is proposed to enable learning 
time granularity-aware representations from both the temporal and frequency domains. \textbf{Third}, an autoregressive blank infilling pre-training framework is incorporated to time series analysis for the first time,  
leading to a generative tasks agnostic pre-training strategy.  
To this end, we develop the General Time-series Model (GTM), a unified MTS foundation model that addresses the limitation of 
contemporary time series models, which often require token, pre-training, 
or model-level customizations for downstream tasks adaption.   
\textbf{Fourth}, extensive experiments show that 
GTM outperforms state-of-the-art (SOTA) methods across all generative tasks, including long-term forecasting, anomaly detection, and imputation. 
\end{abstract}