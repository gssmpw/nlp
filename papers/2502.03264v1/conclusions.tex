\section{Conclusion}
Large-scale MTS analysis presents distinct challenges compared to LLMs, particularly in learning effective universal representations and building models for multi-task scenarios. In this paper, based on new insights observed from multi-granularity MTS data analysis, we propose GTM, a general time series analysis model with a decoder-only backbone that incorporates both temporal and frequency domain granularity aware attention mechanisms to enhance MTS representations. Additionally, we introduce a blank infilling pre-training strategy tailored to MTS analysis, unifying all generative downstream tasks. Experimental results demonstrate that GTM performs on par with or surpasses SOTA methods across all generative MTS analysis tasks.