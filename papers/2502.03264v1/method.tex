\section{Method}
\label{sec:method}
\subsection{Design Overview}

We denote An MTS by 
$ 
\bm{X} 
\triangleq 
[X_{c,t}: c \in [C], t \in [T]] 
,
$ 
where $T$ and $C$ denote the number of timestamps 
and variates respectively.  
We consider an MTS dataset, UTSD-12G, which comprises of a large number of 
MTS from diverse application domains.   

GTM is pre-trained on this dataset from scratch, 
aiming to support generative tasks, such as forecasting, imputation and anomaly detection, simultaneously.  
Fig.\ref{fig:method} illustrates the architecture of GTM.  
 
\textbf{Input data embedding}. 
Reversible Instance Normalization\cite{Kim22}, 
Channel Independence (CI), Patching\cite{Nie23} and Masking\cite{Du22} techniques are applied 
to transform raw MTS data into univariate masked token sequences.  
We also incorporate linear embedding and positional embedding before feeding these tokens into the model backbone.  

\textbf{N-stack Decoder-only backbone}. 
 
We adopt a decoder-only framework as the backbone architecture for generating output autoregressively. 
To enhance the representation and knowledge learning of MTS data in both the temporal and frequency domains, we preserve the temporal self-attention module while re-design the Fourier attention module, which will be elaborated in the subsection~\ref{subsec:FA}.

\textbf{Output projection}. 
Mainstream models typically use a flatten layer with a linear head for one-step generation. This design requires modifying the output layer for different downstream tasks, hindering the reuse of pre-trained parameters and knowledge. To overcome this limitation, we unify the output layer for both pretraining and downstream tasks by using a direct linear projection and instance denormalization to generate output autoregressively.

% method figure
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./Figures/method.pdf} 
\vspace{-0.6cm} 
\caption{GTM model architecture for pre-training. \textbf{Left}: MTS data pass through three key components—input embedding, N-stack Transformer backbone, and output projection—to generate reconstruction results autoregressively. \textbf{Lower right}: Patching and masking processes using both full attention and causal attention mechanisms, adapted from the NLP field and optimized for MTS pre-training. \textbf{Upper right}: A novel knowledge attention module designed to learn representations of MTS data with varying time granularities.}
\vspace{-0.10in}
\label{fig:method}
\end{figure*}

\subsection{The $N$-stack Decoder-only Backbone}
\label{subsec:FA}

We propose an $N$-stack decoder-only backbone that enhances the representation of MTS data by integrating temporal and frequency attention. 
GTM combines a standard temporal self-attention layer with a modified Fourier attention block to capture frequency domain knowledge using data processed via Fast Fourier Transform (FFT).
Unlike the MoE\cite{Lepikhin21} architecture, our design is closer to a knowledge attention mechanism, incorporating various frequency domain knowledge modules to learn distinct joint distributions of MTS with vary time granularities.   
We propose a time-granularity aware representation that captures all temporal granularity information in a quintuple format, where each element represents day, hour, minute, second, and millisecond. 
For instance, the time granularity of the ETTm dataset\cite{Wu21} is represented as [0, 0, 15, 0, 0], which is then transformed into a query vector through linear embedding. 
We also initiate five learnable vectors as key vectors for each granularity, and compute attention scores with the softmax function to weigh the importance of the corresponding knowledge matrices. Additionally, a global frequency knowledge module runs in parallel, representing overall frequency domain knowledge without frequency resolution and is always activated with a probability of 1.

\textbf{Temporal \& Fourier Attention}.

The temporal self-attention module takes $\bm{H}_{in}$ as input, which is obtained by performing the input embedding on $\bm{X}$.
The output is:
\begin{equation} 
\label{eq:temporal_attention}
    \bm{H}_{\text{TemAttOut}} = \texttt{SelfAttention}(\bm{Q}_h, \bm{K}_h, \bm{V}_h), 
\end{equation}
where  
$ 
\bm{Q}_h {=} \bm{H}_{in}^T \bm{W}_h^Q,  
\bm{K}_h {=} \bm{H}_{in}^T  \bm{W}_h^K,  
\bm{V}_h {=} \bm{H}_{in}^T  \bm{W}_h^V,  
$
and $\bm{W}_h^Q, \bm{W}_h^K,\bm{W}_h^V$ 
denote weight matrices.  
Each column of $\bm{H}_{\text{TemAttOut}}$ is a temporal patch, 
and FFT is applied to transform 
each path into frequency domain signals:  
\begin{equation} 
\label{eq:fft}
    \bm{H}_{\text{Fft}} = \texttt{FFT} (\bm{H}_{\text{TemAttOut}})  
    [\text{column-wise transform}]
\end{equation}
Six frequency domain knowledge modules are designed afterwards, 
including five modules with low ranking parameters, 
denoted as $\{\bm{A}_{1}, \bm{B}_{1}\}, \ldots, 
\{\bm{A}_{5}, \bm{B}_{5}\}$,   
and one full connection layer with weight matrix 
$\bm{W}_{\text{full}}$. 
We define key matrix with five learnable vectors represents five different time granularity as $\bm{K}_{\text{f}}$.  
Query vector $\bm{q}_{\text{f}}$ represents specific time granularity, 
yielding 
$ 
\bm{q}_{\text{f}}=\bm{q} \cdot \bm{W}_{\text{f}}^Q, 
$
where $\bm{W}_{\text{f}}^Q$ denotes the weight matrix for query vector.
We can then obtain the Fourier attention results using Eq. \ref{eq:fourier_attention}, along with the representation of one layer after the iFFT transformation, $\bm{H}_{out}\in\mathbb{R}^{D\times N_p}$:
\begin{align}  
& 
\bm{H}_{\text{FourierAtt}} 
{=}  \sum_{n=1}^{5}
    \texttt{SoftMax} 
    \left(
    \frac{\bm{q}_{\text{f}}*\bm{K}_{\text{f}}}{\sqrt{d_{fk}}}
    \right)
    \nonumber
    \\
 &   \hspace{0.7in} 
  \times 
    (
    \bm{A}_{i}\bm{B}_{i}
    ) 
\times \bm{H}_{\text{Fft}}  
+ \bm{W}_{\text{full}}\cdot \bm{H}_{\text{Fft}},
 \label{eq:fourier_attention}
\\ 
&    \bm{H}_{\text{out}} = \texttt{iFFT}(\bm{H}_{\text{FourierAtt}}).
    \label{eq:ifft}
\end{align}
The $N$ stack decoder-only backbone yields:
\begin{align} 
& 
\bm{H}_{out}^{(n)} 
= 
\texttt{GTM\_Decoder}(\bm{H}_{in}^{(n)}),
&
\bm{H}_{in}^{(n)} = \bm{H}_{out}^{(n-1)},   
\label{eq:block}
\end{align}
where $n \in [N]$ denote the index of layer and 
the the first layer takes $\bm{H}_{in}$ as input, 
i.e., $\bm{H}_{in}^{(1)} = \bm{H}_{in}$.   

\textbf{Output Projection}. We use unified linear projection to generate output patches autoregressively 
$
    \bm{X}_{out} = \bm{W}_{\text{LinPoj}}\cdot \bm{H}_{out}^{(N)}, 
$
where $\bm{W}_{\text{LinPoj}}$ denotes linear projection weight. In this manner, except for special cases, the GTM model can adapt to various downstream tasks without requiring any modifications to the network architecture.

\subsection{Pre-training Framework}
\label{sec:pre-tain}

The MTS $\bm{X}$ is devided into patches following the ideas of 
CI and Patching\cite{Nie23}.  
Each row of $\bm{X}$ is processed independently.
The $c$-th row of $\bm{X}$ is divided 
into overlapping windows of data points with a stride 
$\tau \in \mathbb{N}_+$ and length $L$. 
Formally, the $i$-th windows of data points denoted by $\bm{x}_i$ 
can be expressed as  
$ 
\bm{x}_{i} 
= 
[X_{c,i\times \tau}, \ldots, 
X_{c,i\times \tau + L -1}
].   
$   
The $\bm{x}_{i}$ is divided into patches forming a patch matrix 
$\bm{P}$:
\[
\bm{P} 
= 
\texttt{Paching} 
(\bm{x}_{i}). 
\,\, \text{ \cite{Nie23} }
\] 

GLM \cite{Du22} proposes an autoregressive blank infilling pre-training framework for various NLP tasks. Inspired by this, we adapt and develop a general masking process for generative task-agnostic MTS analysis, as shown in Fig.\ref{fig:method}.  
Each patch span consists of one or more consecutive patches, 
and $\ell \in \mathbb{N}_+$ patch spans are randomly sampled 
from $\bm{P}$ denoted by: 
\[
\{ \bm{S}_1, \ldots, \bm{S}_{\ell} \} 
=\texttt{RandPatchSpanSample} (\bm{P})
\]
 
Fig.\ref{fig:method} illustrates an example, where $\ell=2$, 
$\bm{S}_1 = [\bm{p}_2 \,\, \bm{p}_3 \,\, \bm{p}_4]$ and 
$\bm{S}_2 = [\bm{p}_8]$.  
Each sampled patch span is replaced by a single \textbf{[MASK]} token 
to form a corrupted patch denoted by $\bm{P}_{\text{crpt}}$, formally  
\[
\bm{P}_{\text{crpt}} 
= 
\texttt{MaskSampledSpan} 
(\bm{P}).  \text{\cite{Du22}}
\] 
In this example,   
$\bm{P}_{\text{crpt}} = [\bm{p}_1  \, [M] \, \bm{p}_5 \, \bm{p}_6 \, \bm{p}_7 \, [M] \, ]$. 

\input{tables/full_forecasting_resutls}

The sampled patch spans are randomly permuted.  
Special tokens \textbf{[START]} and \textbf{[END]} are padded 
to each of sampled span forming input and label data: 
\begin{align}
& 
\bm{S}_{\text{in}}  
= 
[ \,\, [S] \,\, \bm{S}_{\sigma(1)} \,\,\cdots\,\, 
[S] \,\, \bm{S}_{\sigma(\ell)} \,\,
],  
\\
&
\bm{Y} 
= 
[ \, \, \bm{S}_{\sigma(1)} \,\,  [E] \,\, \cdots \,\,
\bm{S}_{\sigma(\ell)} \,\, [E] \,\,
],   
\end{align}
where $\sigma(\cdot)$ denotes a random permutation 
over $\{1,\ldots,\ell\}$.  
The goal of the pre-training is to autoregressively reconstruct all masked patches (Eq.\ref{eq:autoregression}), minimizing the discrepancy(MSE) compared with ground-truth Eq.(\ref{eq:loss}).
\begingroup
\allowdisplaybreaks
\begin{align} 
&
\mathbb{P} 
(\widehat{\bm{Y}}) 
=
\prod\nolimits_{i} 
\mathbb{P} ( \widehat{\bm{y}}_{i} 
| 
\bm{P}_{\text{crpt}}, \bm{s}_{\text{in}, \leq i}),
\label{eq:autoregression}
\\
& 
    Loss (\bm{x}_i)
    = \frac{1}{\text{\# of rows of } \bm{Y}} 
    \sum\nolimits_{i} 
\| \widehat{\bm{y}}_i - \bm{y}_i \|^2,
\label{eq:loss}
\end{align}
\endgroup
where $\bm{y}_{i}$ and $\bm{s}_{\text{in}, \leq i}$ denote the 
$i$-th row of $\bm{Y}$ and $\bm{S}_{\text{in}}$.  

Since \textbf{[START]} and \textbf{[END]} do not exist in the time-series analysis domain, 
we employ learnable vectors to represent them.   
To better adapt to various generative tasks, we set the proportion to apply all the \textbf{[MASK]} tokens to the consecutive patches at the tail of the input data. 
This strategy makes the generation of the masked patches more akin to a forecasting task.
By combining these techniques, we overcome the limitations of SOTA models that rely on either mask reconstruction or the autoregressive method for pre-training.
After trainable linear embedding $\bm{W}_{emb}\in\mathbb{R}^{D\times L_p}$, We also leverage the 2D learnable positional encoding method \cite{Du22} to ensure that the backbone model is aware of the length of the masked span when generating output patches. 
Based on masking process, the input data $\bm{X}_{in}$ 
is $
\bm{X}_{in} 
= 
[\bm{P}_{\text{crpt}} \,\, \bm{S}_{\text{in}} ].    
$ 
and it is fed into the backbone network for attention-based processing: 
\begin{equation} 
\label{eq:2D-PE}
\bm{H}_{in} = 
\bm{W}_{emb} \bm{X}_{in} + 
\bm{W}_{1D\_pos} + \bm{W}_{2D\_pos}
\end{equation}
where $\bm{W}_{1D\_pos}$ and $\bm{W}_{2D\_pos}$ 
denotes 1D and 2D position coding matrix.  
In this manner, the GTM\_Decoder backbone module can apply full-attention mask for $\bm{P}_{\text{crpt}}$, while causal attention mask for $\bm{Y}$. 

\subsection{Fine-tuning for Downstream Tasks}
Benefit from the model design and pre-training strategy, GTM can adapt to various generative downstream tasks without changes to the network architecture, apart from minor pre-processing adjustments, such as removing the masking process and 2D positional encoding. This makes GTM a versatile, pre-trained time series model capable of delivering high-precision results, as demonstrated in Sec. \ref{sec:exp}.


