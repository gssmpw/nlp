\section{Experiments}
\label{sec:exp}
We present extensive experiments evaluating our proposed GTM model, comparing it with different kinds of SOTA models (details are listed in Appendix \ref{appendix:benchmark}) to highlight the performance improvements achieved by our design. We also provide results from directly training the model for downstream tasks, demonstrating that pre-training on large-scale datasets yields additional performance gains. Finally, we conduct ablation studies to show the effectiveness of the key network modules.

\input{tables/avg_results_imputation}

\input{tables/results_anomaly_detection}

\subsection{Datasets description}
We use the large-scale public time series dataset UTSD-12G for pre-training, ensuring no downstream task-related data is included to prevent leakage. 
For forecasting and imputation tasks, we use five widely used public datasets from \cite{Wu21}, and for anomaly detection tasks, we utilize five popular labeled datasets from \cite{Su19, Hundman18, Mathur16, Abdulaal21}.
The detailed statistics of all these public datasets are provided in Appendix \ref{appendix:datasets}




\subsection{Long-term Forecasting}
\label{sec:exp-forecasting}
For long-term forecasting, we select representative baselines and cite their results respectively. These SOTA models include the LLM-enhanced model GPT4TS\cite{Zhou23}, the multi-task time series foundation model UniTS-PMT\cite{gao24}, the task-specific time series foundation model $TTM_E$, TimesNet\cite{Ekambaram24, Wu23}, the Transformer-based models PatchTST, FEDformer, Autoformer, Informer\cite{Nie23, Zhou22, Wu21, Zhou21}, and the MLP-based model Dlinear\cite{Zeng23}.
Note that we choose baselines that match our experimental settings the most and exclude models that involve pre-training and fine-tuning on the same datasets for downstream tasks. The long-term forecasting lengths includes $T\in \{96, 192, 336, 720\}$ time points. We utilize MSE and MAE as evaluating metrics for long-term forecasting. Notable, GTM directly utilizes pre-trained model without any modifications. As shown in Table \ref{table:full-forecasting}, GTM outperforms all the SOTA models, achieving the best result in 21 and second best in 22 out of total 50 tests. The second best model $PatchTST$, achieves the best in 14 and second best in 15.  

\input{tables/pretrain_forecasting}

\subsection{Imputation}
\label{exp:imputation}
We use the same publicly available datasets in forecasting tasks and follow the protocol proposed by \cite{Zhou23} for imputation tasks. To align with benchmark settings, we apply point-wise missing ratios for interpolation, and directly use pre-trained model for fine-tuning, only omitting the patching process. The point-wise imputation baselines include GPT4TS, TimesNet, PatchTST, FEDformer, Informer and Dlinear.
We conduct the task with varying missing data ratios of $\{12.5\%, 25\%, 37.5\%, 50\%\}$ at the time-point level. Table \ref{table:avg. results of Imputation} demonstrates that, even without patch preprocessing, GTM achieves significant performance improvements. Compared to the second best model, GTM gets a 23.1\% reduction in MSE, 12.1\% in MAE for ETTh1 data, and 25.0\% reduction in MSE, 8.6\% in MAE for ETTm1 data. More details are in Appendix \ref{appendix:imputation}




\input{tables/pretrain_imputation}




\input{tables/ablation_avg}
 
\subsection{Anomaly Detection}



In the anomaly detection tasks, we fine-tune the pre-trained GTM model in a self-supervised manner through data reconstruction without any additional adaptions. We use a common adjustment strategy\cite{Xu18} where data points with reconstruction errors exceeding a threshold are considered anomalies. The baselines include the multi-task foundation model UP2ME, TimesNet, the LLM-enhanced model GPT4TS,  the transformer-based models PatchTST, FEDformer, Autoformer, Informer, and the MLP-based model Dlinear. As shown in Table \ref{table:F1 of anomaly detection}, GTM achieves the highest F1 score improvement compared to all baselines, with gains ranging from 0.33\% (GPT4TS) to 10.38\% (Informer).








\subsection{Effectiveness of Pre-training}
By pre-training on large-scale, multi-scenario, and multi-time granular MTS data, GTM learns richer and more universal representations. We compare the performance of two models: the baseline GTM, trained directly on task-specific datasets with random initialization, and the fine-tuned GTM, which benefits from pre-training. This comparison highlights the effectiveness of the pre-training approach.

Tables \ref{table:forecasting GTM vs. GTM w/o} and \ref{table:Imputation results GTM vs. GTM w/o} summarize the average experimental results of both models across all datasets used for long-term forecasting of varying lengths and for imputation with different data missing ratios. The results indicate that, for long-term forecasting tasks, fine-tuned GTM consistently outperforms the baseline GTM in every comparison. It achieves a reduction in MSE ranging from 0.5\% to 7.8\% and a reduction in MAE ranging from 0.8\% to 8.0\%. Similarly, for imputation tasks, fine-tuned GTM also outperforms the baseline GTM, achieving an MSE reduction of 1.2\% to 11.7\% and an MAE reduction of 0.5\% to 14.2\%. More details of the experiments are provided in Appendix \ref{appendix:effect of pretrain}


For anomaly detection, Table \ref{table:Anomaly detection GTM vs. GTM w/o} shows that with pre-training, the fine-tuned GTM model achieves performance improvements across all test datasets, with an average increase of 1.2\% in F1-score compared to the baseline GTM model.

\input{tables/pretrain_anomaly} 


\subsection{Ablation tests}



We conduct a series of ablation experiments on long-term forecasting tasks for different prediction lengths to evaluate the effectiveness of key components in the GTM model.
We use a baseline version of the GTM model without the frequency domain analysis module and compare it with an advanced version that lacks the knowledge attention modules. By also comparing both with the complete GTM model, we gain insights into the impact of these key design elements.



Table \ref{table:forecasting ablation} shows the average long-term forecasting results for each dataset. The complete GTM model outperforms all other models in every test. The advanced GTM model ranks second. This demonstrates that the combination of temporal and frequency domain analysis, especially, the knowledge attention modules helps the GTM model effectively learn distribution representations from MTS datasets with varying time granularities. More details of ablation tests are listed in Appendix \ref{appendix:ablation}




