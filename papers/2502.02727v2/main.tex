\documentclass{article}
\usepackage{titletoc}

\RequirePackage{algorithm}
\usepackage{algorithmic}
\newcommand\mycommfont[1]{\footnotesize{#1}}
\let\oldnl\nl%
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}%

\usepackage{geometry}
\geometry{
left=1.4in,
right=1.4in,
top=1in,
bottom=1in,
}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\widowpenalty10000
\clubpenalty10000

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{times}

\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor,graphicx}      %

\usepackage{multirow}
\usepackage{makecell}

\usepackage{subfigure}
\usepackage{subcaption} 


\usepackage{amsmath,amssymb,amsfonts}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{float}
\usepackage[numbers,sort]{natbib}


\definecolor{lightred}{rgb}{1, 0.7, 0.7}
\definecolor{lightgreen}{rgb}{0.7, 1, 0.7}


\usepackage{enumitem}

\usepackage{wrapfig}
\usepackage{xfrac}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{multicol}
\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1pt\hskip\tabcolsep}}
\makeatother

\usepackage[none]{hyphenat}  %


\newcommand{\footnoteremember}[2]{%
\footnote{#2}
\newcounter{#1}%
\setcounter{#1}{\value{footnote}}%
}
\newcommand{\footnoterecall}[1]{%
\footnotemark[\value{#1}]
}

\setlength\emergencystretch{.5\textwidth}
\addtolength{\skip\footins}{12pt plus 0pt minus 2pt}

\renewcommand\topfraction{0.95}
\renewcommand\bottomfraction{0.95}
\renewcommand\textfraction{0.05}
\renewcommand\floatpagefraction{0.95}
\newcommand{\com}[1]{\tiny$\pm$#1}



\usepackage{titlesec}
\titlespacing*{\section}
{0pt}{1.1ex plus .2ex minus .2ex}{0.8ex plus .2ex minus .2ex}
\titlespacing*{\subsection}
{0pt}{0.8ex plus .2ex minus .2ex}{0.5ex plus .2ex minus .2ex}

\allowdisplaybreaks

\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}


\newcommand{\Identity}{{\rm I\kern-.2em l}}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Expectbracket}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Expectsubbracket}[2]{\mathbb{E}_{#1}\left[ #2 \right]}
\newcommand{\Expectcond}[2]{\mathbb{E}\left[\left. #1 \right| #2 \right]}
\newcommand{\Variancebracket}[1]{\mathrm{Var}\left[ #1 \right]}


\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}

\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normsq}[1]{\left\Vert #1 \right\Vert^2}
\newcommand{\innerprod}[1]{\left\langle #1 \right\rangle}


\begin{document}

\title{Parameter Tracking in Federated Learning with Adaptive Optimization}



\author{
Evan Chen\,\footnoteremember{purdue}{Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN}~~~~~~~~~
Jianing Zhang\,\footnoterecall{purdue}~~~~~~~~~
Shiqiang Wang\,\footnoteremember{ibm}{IBM T. J. Watson Research Center, Yorktown Heights, NY, USA}\\
Chaoyue Liu
\,\footnoterecall{purdue}~~~~~~~~~
Christopher Brinton
\,\footnoterecall{purdue}
\date{}
}

\maketitle

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\blfootnote{\!\!\!\emph{Email:}~ 
\{chen4388, zhan4670, cyliu, cgb\}@purdue.edu,~
\href{mailto:wangshiq@us.ibm.com}{wangshiq@us.ibm.com}
}

\blfootnote{\!\!\!Preprint. Work in progress.\vspace{0.1in}}





\begin{abstract}
\noindent In Federated Learning (FL), model training performance is strongly impacted by data heterogeneity across clients. Gradient Tracking (GT) has recently emerged as a solution which mitigates this issue by introducing correction terms to local model updates. To date, GT has only been considered under Stochastic Gradient Descent (SGD)-based model training, while modern FL frameworks increasingly employ adaptive optimizers for improved convergence. In this work, we generalize the GT framework to a more flexible Parameter Tracking (PT) paradigm and propose two novel adaptive optimization algorithms, {\tt FAdamET} and {\tt FAdamGT}, that integrate PT into Adam-based FL. We provide a rigorous convergence analysis of these algorithms under non-convex settings. Our experimental results demonstrate that both proposed algorithms consistently outperform existing methods when evaluating total communication cost and total computation cost across varying levels of data heterogeneity, showing the effectiveness of correcting first-order information in federated adaptive optimization.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Federated Learning (FL) has emerged as a promising paradigm for training Machine Learning (ML) models across distributed devices~\cite{li2020federated,kairouz2021advances}. 
Migitating data heterogeneity remains one of the most critical challenges in FL. This non-i.i.d. characteristic of client data often results in slower convergence and suboptimal model performance. While traditional Stochastic Gradient Descent (SGD)-based optimizers are widely employed in FL frameworks, they exhibit limited effectiveness in addressing the adverse effects of heterogeneous data~\cite{ruder2016overview}. To overcome these limitations, the concept of Gradient Tracking (GT) has been introduced~\cite{di2016next}. GT incorporates correction terms during communication, enabling clients to locally estimate and compensate for the gradient contributions from other clients in the network. This approach helps mitigate the negative impacts of data heterogeneity, thereby improving the convergence behavior and stability of the global model. 

Beyond the widely used SGD optimizer, moment-based optimizers have emerged as prominent local update schemes for ML training~\cite{duchi2011adaptive,graves2013generating,kingma2014adam}. Numerous algorithms have been proposed, and these adaptive optimizers have demonstrated superior empirical performance compared to SGD. 
Efforts have been made to integrate adaptive optimizers into FL frameworks; however, these approaches often suffer from performance degradation in the presence of data heterogeneity, especially when communication cost is high and cannot be performed frequently. Existing research leveraging GT to address data heterogeneity in FL primarily focuses on SGD optimizers. Consequently, since most adaptive optimization algorithms involves multiplying and dividing gradient-based estimates, how to effectively track first-order gradient information remains a challenging task.

Motivated by this, we investigate the following questions:
\begin{enumerate}
\vspace{-1mm}
\item \textit{Will adaptive optimization algorithms designed using GT obtain performance advantages across FL systems as their SGD counterparts? }
\item \textit{What is the best way to incorporate the concept of GT into adaptive federated optimization?}
\vspace{-2mm}
\end{enumerate}

In answering these questions, we extend the concept of GT to a more generalized framework termed Parameter Tracking (PT). Building upon this foundation, we propose two novel algorithms leveraging the Adaptive Moment Estimation ({\tt Adam}) optimizer. Through rigorous convergence analysis and comprehensive experimental evaluations, we demonstrate that both algorithms effectively stabilize the global learning process. As a result, increasing levels of non-i.i.d. data distributions across clients do not readily lead to a decline in performance, addressing a critical limitation in existing federated optimization techniques.

Our main contributions are as follows:
\vspace{-0.1in}
\begin{itemize}[itemsep=1pt]
\item We introduce a generalized concept of GT termed Parameter Tracking (PT), where local clients track the discrepancy between their locally collected first-order information and the server's aggregated information (Sec.~\ref{ssec:intuition}). This extension proves beneficial when integrating the {\tt Adam} optimizer into FL.

\item Base on the concept of PT, we propose two novel {\tt Adam}-based algorithms, {\tt FAdamET} and {\tt FAdamGT}, by leveraging different interpretations of PT. Both approaches effectively track global information by using control variables that does not require any additional fine-tuning, mitigating model biases caused by various levels of non-i.i.d. data efficiently (Sec.~\ref{ssec:et_alg} and \ref{ssec:gt_alg}).

\item We provide a rigorous theoretical analysis of the convergence rates for both algorithms. This offers insights into their stability and efficiency under heterogeneous data conditions, and better understanding of how PT works differently when applied to different steps of the local update process (Sec.~\ref{ssec:theories}).

\item We perform extensive experiments across diverse datasets and multiple FL settings, including image classification tasks using convolution neural networks (CNN) and sequence classification tasks using large language  models (LLMs), demonstrating the superior performance of our proposed methods compared to existing baselines (Sec.~\ref{sec:exp}).
\end{itemize}

\section{Related Works}
\textbf{Gradient Tracking:} Gradient Tracking (GT) methods \cite{di2016next,nedic2017achieving,tian2018asy,koloskova2021improved,carnevale2022gtadam,takezawa2022momentum,wang2024momentum} have emerged as a powerful solution to address data heterogeneity challenges in decentralized optimization algorithms. The core principle of GT lies in tracking gradient information from neighboring nodes during each communication round, ensuring more accurate gradient estimates across the network. Centralized FL algorithms such as {\tt SCAFFOLD}~\cite{karimireddy2020scaffold} and {\tt Proxskip}~\cite{mishchenko2022proxskip} are both designed base on this concept, and multiple works on serverless FL settings demonstrated superior improvement \cite{liu2023decentralized,ge2023gradient,berahas2023balancing,alghunaim2024local}, where communication efficiency is a primary concern. By effectively reducing the synchronization frequency while still guaranteeing convergence to the optimal point, GT has proven to be highly effective in mitigating the adverse effects of heterogeneous data distributions. Furthermore, existing studies have shown that under proper initialization of gradient tracking variables, many standard assumptions on data heterogeneity can be relaxed.

Recent advancements have also extended GT methods to address hierarchical network structures. {\tt SDGT} was introduced as the first GT algorithm tailored for semi-decentralized networks~\cite{chen2024taming}, bridging the gap between fully decentralized and centralized topologies. Meanwhile,~\cite{fang2024hierarchical} proposed {\tt MTGC}, a multi-timescale GT algorithm capable of operating efficiently in multi-tier networks. 
Despite these advancements, existing works predominantly focus on designing SGD based algorithms, leaving the combination of GT and adaptive optimizers largely unexplored and an open challenge.

\textbf{Adaptive Optimizer:} SGD optimizers rely on fixed or decaying learning rates, which often require careful tuning and may struggle with scenarios involving sparse gradients or noisy updates. To address these limitations, adaptive optimizers dynamically adjust learning rates based on the gradient history of individual parameters, enabling more effective navigation of complex optimization landscapes. Among the most prominent adaptive optimizers are {\tt AdaGrad}~\cite{duchi2011adaptive}, and {\tt Adam}~\cite{kingma2014adam}. {\tt AdaGrad} introduces per-parameter learning rate scaling to handle sparse features effectively. Building on this foundation, {\tt Adam} combines the benefits of momentum with adaptive learning rates, achieving robust convergence across various learning tasks. Recent advancements have further explored the decoupling of weight decay~\cite{loshchilov2017decoupled} and the time-varying effects of regularization terms~\cite{xie2024overlooked}, pushing the boundaries of adaptive optimization.

Several approaches have been proposed to integrate adaptive optimizers into FL. \cite{reddi2020adaptive} introduced {\tt FedAdam}, where the central server employs an adaptive optimizer to update the global model using aggregated client gradients. Additionally, \cite{xie2019local} incorporates adaptive optimization directly on local clients. More recently, \cite{sun2023efficient} presented {\tt FedLADA}, an FL algorithm in which clients utilize the Adam optimizer for local updates. In {\tt FedLADA}, the update gradient is computed as a weighted average of local gradients and global gradients. However, besides {\tt FedLADA}, all of these algorithms aren't designed to deal with data heterogeneity, and hence requires frequent global aggregation for good results. {\tt FedLADA} although maintained a global gradient estimation, requires a weighted sum operation where an additional hyperparameter has to be fine-tuned based on different data. This causes the performance to vary dramatically base on the chosen weights, which is not required in our algorithms.

\section{System Model and Motivation}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/GTvisualize2.pdf}
    \caption{Visualization of the local update process of multiple federated learning algorithms. Whereas {\tt SCAFFOLD} adds correction terms into the SGD update of {\tt FedAvg}, our methods, {\tt FAdamET} and {\tt FAdamGT}, incorporate PT correction terms into local updates based on adaptive optimization updates. {\tt FAdamET} injects correction terms after computing adaptive estimated moments, and {\tt FAdamGT} injects correction terms before computing adaptive estimated moments.}
    \label{fig:GT_visualize}
\end{figure*}
\subsection{System Model}
The problem we aim to solve follows the form:
\begin{align}
    \textstyle \min_{x\in \mathbb{R}^d}f(x) &\textstyle= \frac{1}{n}\sum_{i=1}^n f_i(x),
\end{align}
where $f_i(x) = \mathbb{E}_{\xi_i \sim \mathcal{D}_i}f_i(x;\xi_i)$ is the expectation of the stochastic local function, and $n$ is the total number of clients (typically edge devices) in the system, indexed $i = 1, \ldots, n$. $f_i(x)$ is the local ML loss function computed at client $i$ for model parameters $x \in \mathbb{R}^d$, $\mathcal{D}_i$ is the local data distribution at client $i$, and $\xi_i$ is an unbiased random sample from $\mathcal{D}_i$. The server is connected
to each device over a star topology, hence allowing the server to have direct communication between any device in the network.

The training process operates on two distinct timescales: an outer timescale and an inner timescale. The outer timescale, denoted as $t = 1, 2, \ldots, T$, represents global aggregation rounds where the central server updates the global model. The inner timescale, denoted as $k = 1, \ldots, K$, represents local training steps performed by each client between global aggregations. We assume a fixed number of $K$ local updates occur between two consecutive global aggregation rounds.

For each global iteration $t$, the training procedure can be described in three iterative steps:
(i) \textit{Client Selection and Initialization}: At each global round $t$, the server selects a subset of clients $\mathcal{S}^t \subseteq \{1, \ldots, n\}$, where $|\mathcal{S}^t| = S \leq n$. The global model is broadcasted to the selected clients to initialize local training.
(ii) \textit{Local Model Updates}: Each selected client performs $K$ local updates using a local optimizer, independently updating their local models based on their respective datasets.
(iii) \textit{Global Model Aggregation}: After completing $K$ local updates, the selected clients send their updated model parameters to the server. The server then aggregates these updates to refine the global model.


\subsection{Limitation of Existing Works}
Federated adaptive algorithms such as {\tt LocalAdam} exhibit instability under data heterogeneity, similar to {\tt FedAvg}. Specifically, the local update does not remain stable at the optimal solution. Let $m_i = m^* = \nabla f(x^*) = 0$, $v_i = v^* = \nabla f(x^*) \odot \nabla f(x^*)$, and $x_i = x^*$. A single local update on client $i$ at the optimal point can be expressed as:
\begin{align}
\textstyle m_i^+ &\textstyle = \beta_1 m^* + (1-\beta_1) \nabla f_i(x^*),\notag\\
\textstyle v_i^+ &\textstyle = \beta_2 v^* + (1-\beta_2)\nabla f_i(x^*) \odot \nabla f_i(x^*),\notag\\
x_i^{+} &\textstyle = x_i - \eta\frac{m_i^+}{\sqrt{v_i^+} + \epsilon} \neq x^*. \tag{\texttt{LocalAdam}}
\label{eq:unstable_fixpoint_1}
\end{align}

This discrepancy between $x_i^{+}$ and $x^*$ implies that {\tt LocalAdam} only converges to a region around the stationary point, where the radius depends on the degree of data heterogeneity.

To aim to address this issue, {\tt FedLADA} introduces a weighted-average update, where a global averaged term $g_\alpha$ is calculated by the server and broadcasted to all sampled clients. The ideal value of $g_\alpha$ is $g_\alpha^* = \frac{m^*}{\sqrt{v^*} + \epsilon}$. However, even with the additional weighted-average operation, the ideal update of {\tt FedLADA} still remains unstable:
\begin{align}
\textstyle m_i^+ &\textstyle = \beta_1 m^* + (1-\beta_1) \nabla f_i(x^*),\notag\\
\textstyle v_i^+ &\textstyle = \beta_2 v^* + (1-\beta_2)\nabla f_i(x^*) \odot \nabla f_i(x^*),\notag\\
x_i^{+} &\textstyle = x_i - \eta \left(\alpha \frac{m_i^+}{\sqrt{v_i^+} + \epsilon} + (1-\alpha)g_\alpha^*\right). \tag{\texttt{FedLADA}}
\label{eq:unstable_fixpoint_2}
\end{align}

The iterates are still unstable, since for any $\alpha \in (0,1]$, $x_i^{+} \neq x^*$. The equality only holds when $\alpha = 0$, indicating that no local gradients are utilized during local updates, which is a trivial and impractical solution for mitigating data heterogeneity effectively.

\section{Proposed Algorithm}
The usage of PT correction is shown in Figure~\ref{fig:GT_visualize}, where we show two possible locations when correction terms can participate into the local update: before the moment estimation and after the moment estimation.
\subsection{Model Correction with First-order Information}
\label{ssec:intuition}


GT has significantly improved the performance of SGD methods. We generalize GT to Parameter Tracking, where the core principle remains to locally track first-order update information, but it is not strictly limited to gradient information. This generalization aims to make FL with all kinds of local optimizer more resilient to data heterogeneity.

Adam optimizer employs multiple variables to track first-order information. With parameter tracking, these variables can be adapted at various stages of the algorithm, enabling the derivation of novel FL algorithms. In this paper, we propose two such algorithms, building upon {\tt LocalAdam}~\cite{sun2023efficient} as a baseline, which replaces the local SGD optimizer in FedAvg with an Adam optimizer.

\textbf{Case 1: Estimate Correction with Parameter Tracking.} 
For a given local iteration, the local Adam optimizer performs updates as follows:
\begin{align}
m_i^+ & \textstyle= \beta_1 m_i + (1-\beta_1)g_i, \notag\\
v_i^+ &\textstyle= \beta_2 v_i + (1-\beta_2)g_i \odot g_i,\notag\\
x_i^{+} &\textstyle= x_i - \eta\frac{m_i^+}{\sqrt{v_i^+} + \epsilon},
\label{eq:localadam}
\end{align}
where $m_i$ and $v_i$ represent first and second moment estimates, and $\epsilon$ ensures numerical stability. In an ideal communication scenario where server synchronization occurs at every local step, the server update can be expressed as:
\begin{equation*}
\textstyle x^{+} = x - \eta\sum_{i=1}^n \frac{m_i^+}{\sqrt{v_i^+} + \epsilon}.
\end{equation*}

The objective of parameter tracking is to correct the discrepancy between the server and local updates via an ideal correction term:
\begin{equation*}
\textstyle z_{\textrm{ideal}} = \sum_{i=1}^n \frac{m_i^+}{\sqrt{v_i^+} + \epsilon} - \frac{m_i^+}{\sqrt{v_i^+} + \epsilon}.
\end{equation*}

As shown in Figure~\ref{fig:GT_visualize}, the corrected local update now becomes:
\begin{equation*}
\textstyle x_i^{+} = x_i - \eta\left(\frac{m_i^+}{\sqrt{v_i^+} + \epsilon} + z_{\textrm{ideal}}\right) = x^+.
\end{equation*}
This adjustment ensures that local updates better approximate globally synchronized updates.


\textbf{Case 2: Gradient Correction with Parameter Tracking.} 

Instead of emulating {\tt LocalAdam} with per-iteration communication, we now start from the setting where the server applies {\tt Adam} updates onto the global model using aggregated gradients from local clients. Based on \eqref{eq:localadam}, the ideal server-side update can be given as:
\begin{align}
\label{eq:ideal_severupdate}
m^+ &= \textstyle\beta_1 m + (1-\beta_1)\sum_{i=1}^n g_i, \notag\\
v^+ &= \textstyle\beta_2 v + (1-\beta_2)\sum_{i=1}^n g_i \odot \sum_{i=1}^n g_i,\notag\\
x^{+} &= \textstyle x - \eta \frac{m^+}{\sqrt{v^+} + \epsilon}.
\end{align}

The correction term here thus accounts for the difference between aggregated gradients and local gradients:
\begin{equation*}
\textstyle z_{\textrm{ideal}} = \sum_{i=1}^n g_i - g_i.
\end{equation*}

As shown in Figure~\ref{fig:GT_visualize}, by incorporating this correction term, the local updates are refined as:
\begin{align*}
m_i^+ &\textstyle= \beta_1 m_i + (1-\beta_1)(g_i + z_{\textrm{ideal}}),\\
v_i^+  &\textstyle= \beta_2 v_i + (1-\beta_2)(g_i + z_{\textrm{ideal}}) \odot (g_i + z_{\textrm{ideal}}),\\
x_i^{+} &\textstyle= x_i - \eta\left(\frac{m_i^+}{\sqrt{v_i^+} + \epsilon}\right) \approx x^{+}.
\end{align*}

This formulation ensures that local updates align closely with the server updates in \eqref{eq:ideal_severupdate}, and if $m_i = m, \forall i$ and $v_i = v, \forall i$, we get $x_i^+ = x^+$.


\subsection{Estimate Tracking}
\label{ssec:et_alg}
Based on Case 1 of Sec.~\ref{ssec:intuition}, we propose Federated Adaptive Moment Estimation with Estimate Tracking ({\tt FAdamET}), where we incorporate the concept of PT into {\tt LocalAdam}. As shown in Algorithm~\ref{alg:FAdam}, during each global iteration $t$, the server samples a set of clients $\mathcal{S}^t$ with size $S$ for training, and a smaller subset of clients $\mathcal{Y}^t \subseteq \mathcal{S}^t$ with size $Y$ that will perform update on the tracking terms. Further experimental results shows that choosing $Y \leq S$ can still obtain comparable results while saving total communication. 

During the start of each local training interval, the server broadcasts the global model $x^{(t)}$ and the global correction term $y^{(t)}$ to all sampled clients $\mathcal{S}^t$. Then, for each sampled client $i$ at local iteration $k$, stochastic gradient $g_i^{(t,k)} = \nabla f_i(x_i^{(t,k)}, \xi_i^{(t,k)})$ is computed locally using the local model $x_i^{(t,k)}$. The adaptive local update direction $\Delta_i^{(t,k)}$ then calculated using $g_i^{(t,k)}$. In this work, we use the Adam optimizer as shown in line 13-16 of Algorithm~\ref{alg:FAdam}, but it is possible for a more general framework where other adaptive optimizers are considered. Then, the local model performs one update using the PT correction terms and the local update direction:
\begin{equation}
    x_i^{(t,k+1)} = x_i^{(t,k)} - \eta_l(\Delta_i^{(t,k)}+ y^{(t)} - y_i^{(t)}). \notag
\end{equation}

After $K$ local updates, all clients in $\mathcal{S}^t$ aggregated its local model to the server to update the global model $x^{(t)}$, and all clients in $\mathcal{Y}^t$ updates locally its PT correction terms and aggregate them to the server to update the server's correction term $y^{(t)}$.

\begin{algorithm}[tb]
   \caption{\colorbox{lightred}{FAdamET} and \colorbox{lightgreen}{FAdamGT}  }
   \label{alg:FAdam}
\begin{algorithmic}[1]
\small
   \STATE {\bfseries Input:} Global aggregations $T$, minibatch size, $|\xi_i^{(t,k)}|$, initial global model $x^{(1)}$
   \STATE {\bfseries Output:} Global model $x^{(T+1)}$
   \FOR{$t=1$ {\bfseries to} $T$}
        \STATE randomly sample clients $\mathcal{S}^t\subseteq \{1, \ldots, n\}$.
        \STATE randomly sample clients for update tracking terms $\mathcal{Y}^t \subseteq \mathcal{S}^t$
        \STATE server broadcasts $(x^{(t)}, y^{(t)})$ to all clients $i\in \mathcal{S}^t$
        \FOR{clients $i\in \mathcal{S}^t$ in parallel}

        \STATE initialize: $x_i^{(t,1)} = x^{(t)}$, $m_i^{(t,1)} = 0$, $v_i^{(t,1)} = v_i^{(t)}$
        \FOR{$k=1$ {\bfseries to} $K$}
        \STATE compute mini-batch gradient $g_i^{(t,k)}$
        \STATE \colorbox{lightred}{$\hat{g}_i^{(t,k)} = g_i^{(t,k)}$ (\textbf{FAdamET})}
        \STATE \colorbox{lightgreen}{$\hat{g}_i^{(t,k)} = g_i^{(t,k)} + y^{(t)} - y_i^{(t)}$ (\textbf{FAdamGT})}
        \STATE $m_i^{(t,k+1)} =  \beta_1m_i^{(t,k)} + (1-\beta_1) \hat{g}_i^{(t,k)}$\label{algeq:adam_start}
        \STATE $v_i^{(t,k+1)} = \beta_2v_i^{(t,k)} + (1-\beta_2) \hat{g}_i^{(t,k)} \odot \hat{g}_i^{(t,k)}$
        \STATE $\hat{v}_i^{(t,k+1)} = \max (\hat{v}_i^{(t,k)}, v_i^{(t,k+1)})$
        \STATE $\Delta_i^{(t,k)} = m_i^{(t,k+1)}/(\sqrt{\hat{v}_i^{(t,k+1)}} + \epsilon)$ \label{algeq:adam_end}
        \STATE\colorbox{lightred}{$x_i^{(t,k+1)} = x_i^{(t,k)} - \eta_l(\Delta_i^{(t,k)}+y^{(t)} - y_i^{(t)})$ (\textbf{FAdamET})}
        \STATE\colorbox{lightgreen}{$x_i^{(t,k+1)} = x_i^{(t,k)} - \eta_l\Delta_i^{(t,k)}$ (\textbf{FAdamGT})}
        
        \ENDFOR
        \IF{ $i\in \mathcal{Y}^t$}
          \STATE \colorbox{lightred}{$y_i^{(t+1)} = y_i^{(t)} - y^{(t)} + \frac{1}{K\eta_l} (x^{(t)}-x_i^{(t,K+1)})$ (\textbf{FAdamET})}
          \STATE \colorbox{lightgreen}{$y_i^{(t+1)} = \frac{1}{K}\sum_{k=1}^K g_i^{(t,k)}$ (\textbf{FAdamGT})}
          \ENDIF
          \STATE $v_i^{(t+1)} = v_i^{(t,K+1)}$
        \ENDFOR
        \STATE Server aggregates $x_i^{(t,K+1)} - x^{(t)}$ from clients $i \in \mathcal{S}^{t}$.
        \STATE Server aggregates $y_i^{(t+1)} - y_i^{(t)}$ from clients $i \in \mathcal{Y}^{t}$.
        \STATE $x^{(t+1)} = x^{(t)} + \eta_g \frac{1}{S}\sum_{i\in \mathcal{S}^{t}} (x_i^{(t,K+1)} - x^{(t)})$.
        \STATE $y^{(t+1)} = y^{(t)} + \frac{1}{n}\sum_{i\in \mathcal{Y}^{t}} (y_i^{(t+1)} - y_i^{(t)})$
   \ENDFOR

\end{algorithmic}
\end{algorithm}

\subsection{Gradient Tracking}
\label{ssec:gt_alg}
Based on Case 2 of Sec.~\ref{ssec:intuition}, we propose Federated Adaptive Moment Estimation with Gradient Tracking ({\tt FAdamGT}), where the PT correction is injected before moment estimation. As shown in Algorithm~\ref{alg:FAdam}, during each global iteration $t$, the server samples a set of clients $\mathcal{S}^t$ with size $S$ for training, and a smaller subset of clients $\mathcal{Y}^t \subseteq \mathcal{S}^t$ with size $Y$ that updates on the tracking terms. 

The broadcast and aggregation mostly aligns with {\tt FAdamET}, the main difference being where the PT correction occurs. After each client $i$ computes the stochastic gradient $g_i^{(t,k)}$, the PT correction is added to the gradient:
\begin{equation}
    \hat{g}_i^{(t,k)} = g_i^{(t,k)} + y^{(t)} - y_i^{(t)}. \notag
\end{equation}
The corrected gradient direction $\hat{g}_i^{(t,k)}$ is then used to compute the adaptive local update direction $\Delta_i^{(t,k)}$.  Same as {\tt FAdamET}, it is possible to replace the Adam optimizer used in line 13-16 of Algorithm~\ref{alg:FAdam} with other adaptive optimizers for a more general framework.


        
        


\section{Convergence Analysis}
\subsection{Analysis Assumptions}
We first establish a few general and commonly employed assumptions that we will consider throughout our analysis.
\begin{assumption}[General Characteristics of Loss Functions]\label{assump:genLoss} 
Assumptions applied to loss functions include:
1) The stochastic gradient norm of the loss function $\ell(\cdot)$ is bounded by a constant $G$, i.e.,  
$\Vert{ g}_{i}^{(t)}\Vert \leq G, ~\forall i, t$.\footnote{ The bounded gradient assumption is a necessary condition for \texttt{Adam}-based methods, as controlling the behavior of the second moment relies on a universal bound on the gradient's magnitude. This assumption is widely adopted in numerous analysis of \texttt{Adam}-based algorithms~\cite{kingma2014adam,zou2019sufficient,reddi2020adaptive,sun2023efficient}.}
      2) Each local loss $f_i$ is $L$-smooth $\forall i\in \{1,\ldots,n\}$, i.e., 
    $
        \Vert \nabla f_i(x_1)-\nabla f_i(x_2)\Vert \leq L\Vert x_1-x_2 \Vert, ~\forall x_1, x_2 \in \mathbb{R}^d.
        $

\end{assumption}
\begin{assumption}[Characteristics of Gradient Noise] \label{assump:SGD_noise}
    Consider ${ n}_{i}^{(t,k)}={ g}_{i}^{(t,k)}-\nabla f_i( x_{i}^{(t,k)})$ as the noise of the gradient estimate through the SGD process for device $i$ at time $t,k$. The noise variance is upper bounded by $\sigma^2 > 0$, i.e., $\mathbb{E}[\Vert{ n}_{i}^{(t,k)}\Vert^2]\leq \sigma^2~\forall i,t,k$.
\end{assumption}

\subsection{Non-Convex Convergence Behavior}
\label{ssec:theories}
We now present our main theoretical result, the cumulative average of the global loss gradient can attain sublinear convergence to a stationary point under non-convex problems. The detailed proofs, including of the intermediate lemmas, can be found in Appendix~\ref{appen:fadamet} and \ref{appen:fadamgt}.

\begin{theorem}
\label{thm:fadamet}
Under Assumptions \ref{assump:genLoss}, \ref{assump:SGD_noise}, and the global and local step size satisfies the following conditions:
\begin{align*}
\eta_g\eta_l &= \min(\frac{(1-\beta_1)\beta_1}{8KL(G+\epsilon)}, \frac{1}{8KL}, \frac{1}{12TL}, \sqrt{\frac{S}{T}}). \tag{C.1}\label{eq:condition_common}
\end{align*}
Consider the following conditions for local step size $\eta_l$:
\begin{align}
    \eta_l &\leq \frac{1}{12T^{3/2}L}, \tag{C.2}\label{eq:condition_gt}\\
    \eta_l &\leq \min\left(\frac{\sqrt{G+\epsilon+(1-\beta_1)\beta_1}\sqrt{G+\epsilon}}{12\sqrt{2}(1-\beta_1)\beta_1KL},\frac{1}{12T^{3/2}L}\right). \tag{C.3}\label{eq:condition_et}
\end{align}
When satisfying Conditions~\eqref{eq:condition_common} and \eqref{eq:condition_et}, for any given global iteration $T > 1$, the iterates of {\tt FAdamET} can be bounded as:
\begin{align}
&\frac{1}{T}\sum_{t=1}^T\mathbb{E}\|\nabla f(x^{(t)})\|^2 \notag\\
&= \mathcal{O}\left(\frac{\mathbb{E} f(x^{(1)}) - f^*}{K\sqrt{ST}} +\frac{K}{T}+\frac{YK^2}{nT}+\frac{K^2}{T^3}\right).
\end{align}
When satisfying Conditions~\eqref{eq:condition_common} and \eqref{eq:condition_gt}, for any given global iteration $T > 1$, the iterates of {\tt FAdamGT} can be bounded as:
\begin{align}
&\frac{1}{T}\sum_{t=1}^T\mathbb{E}\|\nabla f(x^{(t)})\|^2 = \mathcal{O}\left(\frac{\mathbb{E} f(x^{(1)}) - f^*}{K\sqrt{ST}} +\frac{K}{T}+\frac{K^2}{T^3}\right).
\end{align}
\end{theorem}

The theorems demonstrate that for both algorithms, the global model \( x^{(t)} \) converges sublinearly to a stationary point as \( T \to \infty \). Both algorithms exhibit a primary convergence term of \(\mathcal{O}(\frac{1}{K\sqrt{ST}})\), which is influenced by the model's initialization. However, while {\tt FAdamGT} has only two terms that grow with \( K \), {\tt FAdamET} includes three such terms. 

Notably, {\tt FAdamGT} achieves a superior convergence rate compared to {\tt FAdamET}. This is attributed to fewer constraints on the upper bound of the local step size \( \eta_l \) and the absence of the \(\mathcal{O}(\frac{YK^2}{nT})\) term present in {\tt FAdamET}, resulting in a tighter convergence guarantee. Detailed proofs in Appendix~\ref{appen:fadamet} and \ref{appen:fadamgt} show that the correction terms in {\tt FAdamET} introduce additional growing terms, leading to the \(\mathcal{O}(\frac{YK^2}{nT})\) component. In contrast, the negative effects of correction terms in {\tt FAdamGT} cancel out during global aggregation. The \(\mathcal{O}(\frac{YK^2}{nT})\) term also implies that the performance of estimate tracking in {\tt FAdamET} can vary depending on the subset size $Y$ of sampled clients used to update the tracking variables.


\section{Experiments}
\label{sec:exp}


\subsection{Experimental Setup}
In the baseline comparisons, we consider three widely used datasets: CIFAR-10, CIFAR-100~\cite{krizhevsky2009learning} and TinyImageNet~\cite{le2015tiny}. For all three datasets, we adopt the ResNet-18 model. We set the total number of clients as $n = 100$, the client sampling rate $\frac{S}{n}$ to $10\%$, and set the number of local iterations $K = 3$. To generate non-i.i.d. data distribution, each dataset is distributed among all clients through a Dirichlet distribution, and the Dirichlet parameter is set to $\alpha = 0.1$.

We compared our algorithm with several FL methods: 1) {\tt FedAvg}, where local updates are performed using SGD optimizer, 2) {\tt SCAFFOLD}, where local updates are performed using SGD optimizer with gradient correction, 3) {\tt LocalAdam}, where the local updates are performed using Adam optimizer and 4) {\tt FedLADA}, where the local updates are performed using a weighted average of Adam optimizer and gradient estimation. For SGD-based baselines, we set $\eta_l = 0.1$ and $\eta_g = 1$. For Adam-based baselines and our methods, we set $\eta_l = 0.001$ and $\eta_g = 1$. We set $(\beta_1, \beta_2) = (0.9, 0.99)$ and $\epsilon = 10^{-8}$ for all Adam optimizers, and set weight decay to $10^{-8}$. All mean and standard deviation is based on four random trials.

Furthermore, we conducted experiments on Large Language Models (LLMs). We tested on a Parameter-Efficient Fine-Tuning (PEFT) algorithm named {\tt FedIT}~\cite{zhang2024towards}, where only a limited amount of the LLM's parameters are trained using Low Rank Adaptation (LoRA) modules. The baseline we consider is {\tt FedIT}, where clients use {\tt Adam} to perform local updates, we name it {\tt FedIT-Adam}. We then compare this baseline with our method, where we add estimate tracking and gradient tracking to the Adam optimizer, named {\tt FedIT-AdamET} and {\tt FedIT-AdamGT}. We use the GPT-2 model~\cite{radford2019language}, and set the total number of clients as $n = 100$ and the client sampling rate to $10\%$. We set $(\beta_1, \beta_2) = (0.9, 0.99)$ and $\epsilon = 10^{-8}$ for all Adam optimizers, and set weight decay to $10^{-8}$. We tested on two datasets, 20NewsGroups and the GLUE benchmark~\cite{lang1995newsweeder,wang2018glue}. All datasets are distributed among all clients through Dirichlet distribution with parameter set to $\alpha = 0.1$. The mean and standard deviation is based on four random trials. All learning rates and the target accuracy for each dataset are listed in Appendix~\ref{appen:lr_and_target_acc}.

\subsection{Experiments on CIFAR and TinyImageNet}
Table~\ref{table:baseline} presents a comparison of the total cost to attain certain accuracy between our proposed methods and existing algorithms. We evaluate two types of cost: 1) The total global iterations, where the system priorities computation efficiency, 2) the total communication operations each client performs, where the system priorities communication efficiency. The sample set size for tracking term aggregation is half the size of the sample set used for model aggregation, denoted as $Y=\frac{S}{2}$. 
Our method, {\tt FAdamGT}, consistently outperforms all baseline algorithms both when we evaluate performance using total global iterations and using total communication per client, demonstrating superior convergence performance and robustness to data heterogeneity through the integration of adaptive optimization and parameter tracking.
For \texttt{FAdamET}, while it continues to outperform existing methods when evaluating total global iterations, the performance gap between \texttt{FAdamET} and other baselines diminishes when evaluating total communication per client.
Figure~\ref{fig:local_iters} demonstrates that both {\tt FAdamET} and {\tt FAdamGT} outperforms the baselines in different number of local iterations, showing the consistent performance of our algorithm across multiple communication settings.

Figure~\ref{fig:non-iid} illustrates the performance improvement of our algorithm compared to {\tt LocalAdam} under varying levels of non-iid data. We vary the Dirichlet parameter $\alpha$ from 0.1 to 1 to represent different levels of non-i.i.d.
When evaluating the total global iterations, the performance gap between {\tt LocalAdam} and our proposed methods is more pronounced under high data heterogeneity. 
In contrast, for the more i.i.d. setting, the performance gap between {\tt FAdamET} and {\tt LocalAdam} becomes negligible, and the gap between {\tt LocalAdam} and {\tt FAdamGT} also narrows. 
When evaluating the total communication per client, we can see that though {\tt FAdamET} cannot match the performance of {\tt LocalAdam}, {\tt FAdamGT} still outperforms {\tt LocalAdam} under high data heterogeneity.
These observations demonstrate that \textit{parameter tracking significantly enhances performance in scenarios with high data heterogeneity, while maintaining comparable performance to baseline methods in low heterogeneity settings.}

\begin{table*}[t]
\caption{The comparison of our methods against multiple baselines on multiple datasets. For all CIFAR-10 experiments, the target accuracy is 75\%, for CIFAR-100, the target accuracy is set at 50\%, while for TinyImageNet, it is set at 30\%. The mean and standard deviation is based on four random trials. We see that {\tt FAdamET} and {\tt FAdamGT} steadily outperforms all baselines when counting the total global iterations, while {\tt FAdamGT} outperforms all baseline when counting the total communication for each client. }
\label{table:baseline}
\vskip 0.15in
\begin{center}
\resizebox{0.98\textwidth}{!}{
\begin{small}
\begin{sc}
\begin{tabular}{cccccccc}
\toprule
Settings & Dataset & FedAvg & SCAFFOLD & LocalAdam & FedLADA & FAdamET (ours) & FAdamGT (ours)\\
\midrule
\multirow{3}{*}{\makecell{Total \\
Global \\Iterations}}

& CIFAR-10 & 1388.5\com{98.6}& 561.8\com{41.3}& 589.5\com{74.0}& 790.3\com{19.1}& 394.8\com{31.3} & \textbf{310.0}\com{16.8}\\
\cmidrule(lr){2-8}
& CIFAR-100 & $>$3000 & 662.5\com{29.0} & 678.3\com{40.6}&$>$3000 & 530.3\com{17.6} & \textbf{323.8}\com{16.3}\\
\cmidrule(lr){2-8}
& TinyImageNet & 1994.5\com{198.9} & 209.8\com{7.2} & 177.3\com{8.3} & 198.8\com{8.7} &157.0\com{6.4} & \textbf{66.3}\com{4.4} \\
\midrule
\multirow{3}{*}{\makecell{Total\\ Communication\\ per Client}}

& CIFAR-10 & 2777.0\com{197.1}& 2247.0\com{165.2}& 1179.0\com{148.0} & 3951.3\com{95.6}& 1381.6\com{109.4} & \textbf{1085.0}\com{58.9} \\
\cmidrule(lr){2-8}
& CIFAR-100 &$>$6000 &2650.0\com{116.0} & 1356.5\com{81.3}&$>$15000 &1855.9\com{61.6} & \textbf{1133.1}\com{57.0}\\
\cmidrule(lr){2-8}
& TinyImageNet & 3989.0\com{397.8} & 839.0\com{28.9} & 354.5\com{16.6} & 988.8\com{43.6} & 549.5\com{22.4} & \textbf{231.9}\com{15.3}\\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
}
\end{center}
\vspace{-0.2in}
\end{table*}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/iter_comp.jpg}
    \vspace{-0.1in}
    \caption{Comparison of achieved accuracy for a given number of global iterations or total communication on CIFAR-100. We can show that our method steadily outperform baselines under different evaluation methods.
    }
    \label{fig:local_iters}
    \vspace{-0.1in}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/alpha_comp.jpg}
    \vspace{-0.1in}
    \caption{Comparison of the total cost of Adam-based methods under varying Dirichlet parameters on CIFAR-100 to attain $50\%$ accuracy. The results show that,
    in terms of communication efficiency, the proposed methods are more advantageous in non-i.i.d. settings, where data heterogeneity poses greater challenges. }
    \label{fig:non-iid}
    \vspace{-0.1in}
\end{figure}


\begin{figure}[t]
    \centering
    \subfigure[CIFAR-100]{ \includegraphics[width=0.98\linewidth]{images/ys_cifar.jpg}}
    \vspace{-0.1in}
    \subfigure[TinyImageNet]{ \includegraphics[width=0.98\linewidth]{images/ys_tiny.jpg}}
    \vspace{-0.1in}
    \caption{Comparison of total cost to attain certain accuracy between different tracking sampling rate on CIFAR-100 and TinyImageNet with $S = 50$, where the target accuracy set for CIFAR-100 is $50\%$ and TinyImageNet is 30\%. 
    Both algorithms outperform the baseline when comparing the total global iterations, while {\tt FAdamGT} steadily outperforms the baseline when comparing the total communication per client, showing the superior performance of our method while being communication efficient. 
    }
    \label{fig:tracking_rate}
\end{figure}

Figure~\ref{fig:tracking_rate} evaluates the performance of our proposed methods under different subset sizes \( Y \) used for updating tracking terms. We set the total clients to be $n = 100$. In these set of experiments, we increase the client sample size from $S = 10$ to $S = 50$. This allows a wider range of $Y$ value to compare the difference in terms of communication efficiency. We then compare the communication and computation cost for both algorithms across various $Y$ values ranging from 1 to 50.
The results reveal that, for both {\tt FAdamET} and {\tt FAdamGT}, the total iterations to achieve certain accuracy under increases slowly as the $Y$ value decreases. \textit{This finding demonstrates the possibility to significantly reduce the total number of communications required by the parameter tracking process without compromising training performance}. The implication is particularly valuable when communication costs are a major bottleneck.

These findings are further substantiated by the communication per client plots in Figure~\ref{fig:tracking_rate}. The plots highlight that \textit{the appropriate $Y$ values not only reduces communication overhead but also maintains superior performance compared to all other configurations and baseline methods}. This advantage underscores the robustness of parameter tracking in {\tt FAdamGT}, which effectively balances communication efficiency and convergence. By leveraging a reduced set size \( Y \), {\tt FAdamGT} achieves steady improvements over baselines while preserving its performance.

\subsection{Experiments on PEFT tasks using LLMs}
In these PEFT tasks, the model weights transmitted between the server and each client account for only 1.9\% of the total weights stored locally on the clients. This indicates that the primary bottleneck lies in the computational cost rather than the communication cost. Consequently, we focus on evaluating the performance of each method based on the total number of required global iterations.

Table~\ref{table:LLM} presents the performance of parameter tracking when integrated into the {\tt FedIT} framework. In these experiments, the size of the sample set used for model aggregation is equal to the sample set for tracking term aggregation, i.e., \( Y = S \). The local epochs between two consecutive global aggregations is set to one, and the target accuracy for each experiment is detailed in Appendix~\ref{appen:lr_and_target_acc}.

The results demonstrate that while the improvement introduced by ET is less pronounced compared to its impact in CIFAR-100 and TinyImageNet experiments, GT consistently yields significant enhancements over the vanilla version of {\tt FedIT}. This observation highlights the robustness and adaptability of parameter tracking mechanisms. 
The substantial improvements achieved by GT \textit{emphasize its ability to capture and leverage first-order information effectively during adaptive optimization}. This underscores the generalizability of parameter tracking techniques, where a broad family of optimizers can benefit from enhanced convergence rates, 
showing its potential for advancing large-scale federated learning applications.


\begin{table}[t]
\caption{\textit{Required total global iterations} for {\tt FedIT} to attain certain accuracy. The mean and standard deviation is based on four random trials. Though incorporating ET doesn't bring visible improvement, GT improves the performance of Adam optimizer and results in faster convergence. }
\label{table:LLM}
\vskip 0.15in
\begin{center}
\resizebox{0.48\textwidth}{!}{
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Dataset & \makecell{FedIT-\\Adam } & \makecell{FedIT-\\AdamET} & \makecell{FedIT-\\AdamGT }\\
\midrule
20NewsGroups & 156.8\com{7.8} & 155.0\com{7.0}& \textbf{143.3}\com{4.1}\\
\midrule  
SST-2  &  47.0\com{5.8}& 48.8\com{8.4} & \textbf{30.3}\com{7.3}\\
\midrule
QQP & 213.0\com{53.8} & 196.3\com{5.1}  & \textbf{63.0}\com{4.6}\\
\midrule
QNLI &117.0\com{10.4} & 99.8\com{11.2} & \textbf{55.5}\com{16.3}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
}
\end{center}
\vspace{-0.3in} 
\end{table}

\section{Conclusion}
In this paper, we introduce Parameter Tracking, a general framework for developing FL algorithms that are robust to data heterogeneity. By incorporating parameter tracking into local adaptive optimizers, we propose two novel algorithms: {\tt FAdamET} and {\tt FAdamGT}. Through rigorous theoretical analysis, we demonstrate that both algorithms achieve sublinear convergence to a stationary point and reveal the impact of parameter tracking when applied at different stages of the local update process. Comprehensive numerical evaluations confirm that both methods outperform all baselines, delivering superior training performance in heterogeneous data settings. These results pave the way for future advancements in federated optimization algorithms.



\bibliography{main}
\bibliographystyle{abbrvnat}


\clearpage


\begin{center}
\LARGE \textbf{Appendix}
\end{center}


\startcontents[sections]
\printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}

\appendix

\numberwithin{equation}{subsection}
\counterwithin{figure}{subsection}
\counterwithin{theorem}{subsection}
\counterwithin{definition}{subsection}

\counterwithin{table}{section}

\clearpage
\newgeometry{left=0.6in, right=0.6in, top=1in, bottom=1in}


\section{Theoretical Analysis for FAdamET (Theorem \ref{thm:fadamet})}
\label{appen:fadamet}

We first define the following auxilary definitions that will be helpful throughout the proof.

We define $c^k$ as the sum of all moving average coefficients to compute the first order moment $m_i^{(t,k)}$:
\begin{align}
    c^{(k,k')} &= (1-\beta_1)\beta_1^{k-k'}\\
    c^k &=  \sum_{k'=1}^k c^{(k,k')} < 1
\end{align}
We first define the unbiased version of $m_i^{(t,k)}$ taking expectation on all stochastic gradients $g_i^{(t,k)}$. We define $\Tilde{m}_i^{(t,k)}$ as the following:
\begin{align}
    \Tilde{m}_i^{(t,k)} &\overset{\Delta}{=} \sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k)}) 
\end{align}
We define an auxilary variable $\alpha_i^{t,k}$:
\begin{equation}
    \alpha_i^{t,k} = \begin{cases}
        m_i^{t-1,k}/(\sqrt{v_i^{t-1,k}} +\epsilon), & i \in \mathcal{Y}^{t-1}\\
        \alpha_i^{t-1,k},& i \notin \mathcal{Y}^{t-1}
    \end{cases}
\end{equation}
We define the tracking variable drift term as:
\begin{equation}
    \Gamma^{(t)} = \frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\mathbb{E}\left\|\alpha_i^{t,k} - \nabla f_i(x^{(t)})\right\|^2
\end{equation}
We define the local update deviation term as:
\begin{equation}
    \mathcal{E}^{(t)} = \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2
\end{equation}

\begin{proof}
Given global iteration $t$, the update of the model at the server can be written as:
\begin{align}
    x^{(t+1)} &= x^{(t)} + \eta_g \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} (x_i^{(t,K+1)} - x^{(t)})\\
    &=x^{(t)} - \eta_g\eta_l \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}
\end{align}
By injecting Assumption~\ref{assump:genLoss}, we can get the following inequality:

\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) - \underbrace{\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{|\mathcal{S}^{(t)}|}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle}_\text{Term I}\\
    &+\underbrace{\eta_g^2\eta_l^2\frac{L}{2}\mathbb{E}\left\|\frac{1}{|\mathcal{S}^{(t)}|}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\|^2}_\text{Term II}
\end{align}
For term I, we first define the average of all square root second moment:
\begin{equation}
    \Bar{v}^{(t)} = \frac{1}{n}\sum_{i=1}^n \sqrt{v_i^{(t)}}
\end{equation}
Then we can upper bound it by Assumption \ref{assump:genLoss}:
\begin{align}
    &-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    &-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    &= -\eta_g\eta_l  \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \left(\frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\Tilde{m}_i^{(t,k)}}{{\Bar{v}^{(t)}}+\epsilon} + \frac{\Tilde{m}_i^{(t,k)}}{{\Bar{v}^{(t)}}+\epsilon} - \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon} + \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon}\right)\right\rangle\\
    &\overset{(a)}{\leq} -\eta_g\eta_l K\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 \\
    &- \eta_g\eta_l K\mathbb{E}\left\langle\nabla f (x^{(t+1)}), \frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\left(\frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\Tilde{m}_i^{(t,k)}}{{\Bar{v}^{(t)}}+\epsilon} + \frac{\Tilde{m}_i^{(t,k)}}{{\Bar{v}^{(t)}}+\epsilon} - \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon} \right)\right\rangle \\
    &\leq -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G+\epsilon}{(1-\beta_1)\beta_1} \mathbb{E}\|\frac{1}{nK}\sum_{i=1}^n\sum_{k=1}^K\frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\Tilde{m}_i^{(t,k)}}{{\Bar{v}^{(t)}}+\epsilon}\|^2 \\
    &\leq -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1} \mathbb{E}\|\frac{1}{nK}\sum_{i=1}^n\sum_{k=1}^K\frac{\sqrt{\hat{v}_i^{(t,k)}} - \Bar{v}^{(t)}}{(\sqrt{\hat{v}_i^{(t,k)}}+\epsilon)(\Bar{v}^{(t)}+\epsilon)}\|^2 \\
    &\overset{(b)}{\leq} -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 
\end{align}

Where (a) the fact that $(1-\beta_1)\beta_1\leq c^k \leq \beta_1$, and (b) uses the fact that $\hat{v}_i^{(t,1)} \leq \hat{v}_i^{(t,2)} \leq \ldots \leq \hat{v}_i^{(t,K+1)}$.

For term II, we can bound it as:
\begin{align}
    &\frac{\eta_g^2\eta_l^2L}{2}\mathbb{E}\left\|\frac{1}{|\mathcal{S}^{(t)}|}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\|^2 = \eta_g^2\eta_l^2L\mathbb{E}\left\|\frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\|^2 + \eta_g^2\eta_l^2KL\sigma^2\\
    &= \eta_g^2\eta_l^2L\mathbb{E}\left\|\frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \nabla f_i(x^{(t)}) + \nabla f_i(x^{(t)})\right\|^2 + \eta_g^2\eta_l^2KL\sigma^2\\
    &\leq 2\eta_g^2\eta_l^2K^2L\mathbb{E}\|\nabla f(x^{(t)}\|^2 + 2\eta_g^2\eta_l^2L \frac{K}{n}\sum_{i=1}^n \sum_{k=1}^K \|\frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{c^k\nabla f_i(x^{(t)})}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} + \frac{c^k\nabla f_i(x^{(t)})}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \nabla f_i(x^{(t)})\|^2 \\
    &+ \eta_g^2\eta_l^2KL\sigma^2\\
    &\leq 2\eta_g^2\eta_l^2K^2L\mathbb{E}\|\nabla f(x^{(t)}\|^2 + \frac{4\eta_g^2\eta_l^2KL}{\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 + \frac{4\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2\\
    &+ \eta_g^2\eta_l^2KL\sigma^2
\end{align}
If we choose $\eta_g\eta_l \leq \min(\frac{(1-\beta_1)\beta_1}{8KL(G+\epsilon)}, \frac{1}{4KL})$, we can combine Term I and II and get:
\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) -\frac{\eta_g\eta_l K}{4}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 +  \frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) \mathcal{E}^{(t)} \\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 + \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2+ \eta_g^2\eta_l^2KL\sigma^2
\end{align}

By using Lemma \ref{lem:deviation}, we can formulate the following:
\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) \left(-\frac{\eta_g\eta_l K}{4}\frac{(1-\beta_1)\beta_1}{G+\epsilon} + \frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)48\eta_l^2K^3L^2\right)  \mathbb{E}\|\nabla f(x^{(t)}\|^2 \\
    &+\frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)96 K^3L^2\eta_l^2 \Gamma^{(t)}\\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 \\
    &+ \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 +\frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) 144K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
    &+ \eta_g^2\eta_l^2KL\sigma^2
\end{align}
By choosing the local step size as $\eta_l \leq \frac{\sqrt{G+\epsilon+(1-\beta_1)\beta_1}\sqrt{G+\epsilon}}{12\sqrt{2}(1-\beta_1)\beta_1KL}$, we can get:
\begin{align}
    \frac{\eta_g\eta_l K}{12}\frac{(1-\beta_1)\beta_1}{G+\epsilon} \mathbb{E}\|\nabla f(x^{(t)}\|^2 &\leq \mathbb{E} f(x^{(t)}) - \mathbb{E} f(x^{(t+1)}) \\
    &+\frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)96 K^3L^2\eta_l^2 \Gamma^{(t)}\\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 \\
    &+ \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 +\frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) 144K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
    &+ \eta_g^2\eta_l^2KL\sigma^2
\end{align}
By moving constants across the inequality and taking average over all iterations, we can get:
\begin{align}   
\frac{1}{T}\sum_{t=1}^T\mathbb{E}\|\nabla f(x^{(t)}\|^2 &\leq \frac{12(G+\epsilon)(\mathbb{E} f(x^{(1)}) - \mathbb{E} f(x^{(T+1)}))}{\eta_g\eta_lK(1-\beta_1)\beta_1 T}\\
&+\frac{12}{K\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2 96 K^3L^2\eta_l^2 \sum_{t=1}^T\Gamma^{(t)}\\
&+ \frac{12G^2(G+\epsilon)^2}{(1-\beta_1)^2\beta_1^2 \epsilon^2T} \mathbb{E}\|\Bar{v}^{(T+1)} - \Bar{v}^{(1)}\|^2 \\
&+\frac{24\eta_g\eta_l (1-\epsilon)^2 KL G^2(G+\epsilon)}{(1-\beta_1)\beta_1\epsilon^2} \\
&+\frac{12}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2 144K^2L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
&+ \frac{12\eta_g\eta_lL(G+\epsilon)}{(1-\beta_1)\beta_1}\sigma^2
\end{align}
By using Lemma \ref{lem:variable_drift}, we can bound $\sum_{t=1}^T\Gamma^{(t)}$ with:
\begin{align}
    \sum_{t=1}^T\Gamma^{(t)} &\leq \sum_{t=1}^T (1-\frac{Y}{2n})^{T-t}\Gamma^{(1)} + T^2\frac{Y}{n}\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}+6n^2G^2\right)\\
    &= T^2\frac{Y}{n}\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}+6n^2G^2\right)
\end{align}
Finally, by bounding $\eta_l \leq \frac{1}{12T^{3/2}L}$, $\eta_g\eta_l \leq \frac{1}{12TL}$, and a specific step size
\begin{equation}
    \eta_g\eta_l = \min(\frac{(1-\beta_1)\beta_1}{8KL(G+\epsilon)}, \frac{1}{4KL}, \frac{1}{12TL}, \sqrt{\frac{S}{T}})
\end{equation}

we can get the convergence rate:
\begin{align}
\frac{1}{T}\sum_{t=1}^T\mathbb{E}\|\nabla f(x^{(t)}\|^2 &\leq \frac{12(G+\epsilon)(\mathbb{E} f(x^{(1)}) - \mathbb{E} f(x^{(T+1)}))}{K(1-\beta_1)\beta_1 \sqrt{ST}}\\
&+ \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}+6n^2G^2\right)\frac{12YK^2}{\epsilon^2nT}\\
&+ \frac{24G^4(G+\epsilon)^2}{(1-\beta_1)^2\beta_1^2 \epsilon^2T} \\
&+\frac{2(1-\epsilon)^2 K G^2(G+\epsilon)}{(1-\beta_1)\beta_1\epsilon^2T} \\
&+\left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2  \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\frac{12K^2}{\epsilon^2T^3}\\
&+ \frac{12(G+\epsilon)}{(1-\beta_1)\beta_1T}\sigma^2\\
&= \mathcal{O}\left(\frac{\mathbb{E} f(x^{(1)}) - f^*}{K\sqrt{ST}}+ \frac{1}{T} + \frac{K}{T} + \frac{YK^2}{T}+\frac{K^2}{T^3}\right)
\end{align}

\end{proof}
\begin{lemma}
\label{lem:deviation}   
Under Assumption~\ref{assump:genLoss}, the local devation term $\mathcal{E}^{(t)} = \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2$ can be bounded as the following:
\begin{align}
        \mathcal{E}^{(t)}\leq 48K^3L^2\eta_l^2\|\nabla f(x^{(t)})\|^2 + 96 K^3L^2\eta_l^2 \Gamma^{(t)} +144K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{align}
\end{lemma}
\begin{proof}

    \begin{align}
&\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\Tilde{m}_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 \\
        &= \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \left(\nabla f_i(x_i^{(t,k')}) - \nabla f_i(x^{(t)})\right)\|^2\\
        &\leq  \frac{L^2}{n}\sum_{i=1}^n\sum_{k=1}^K \sum_{k'=1}^k c^{(k,k')}\mathbb{E}\| x_i^{(t,k')}- x^{(t)}\|^2 
    \end{align}

We can simplify the formulation by first unfolding each local step $x_i^{(t,k')}$:
\begin{align}
    &\frac{1}{n}\sum_{i=1}^n\mathbb{E}\| x_i^{(t,k')}- x^{(t)}\|^2\\
    &\leq  (1+\frac{1}{K-1}) \frac{1}{n}\sum_{i=1}^n\mathbb{E}\|x_i^{(t,k'-1)}- x^{(t)}\|^2 \\
    &+ K\frac{1}{n}\sum_{i=1}^n \mathbb{E}\left\|\eta_l \left(\frac{m_i^{(t,k'-1)}}{\sqrt{\hat{v}_i^{(t,k'-1)}}+\epsilon} - \nabla f_i(x^{(t)}) + y^{(t)} - y_i^{(t)} + \nabla f_i(x^{(t)}) - \nabla f(x^{(t)}) + \nabla f(x^{(t)})\right) \right\|^2\\
    &\leq (1+\frac{1}{K-1}) \frac{1}{n}\sum_{i=1}^n\mathbb{E}\|x_i^{(t,k'-1)}- x^{(t)}\|^2 + 3K\eta_l^2\mathbb{E}\|\nabla f(x^{(t)})\|^2 + 3K\eta_l^2\Gamma^{(t)} + 12K\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
    &\leq \sum_{r=1}^{k'} (1+\frac{1}{K-1})^r\left(4K\eta_l^2\mathbb{E}\|\nabla f(x^{(t)})\|^2 + 8K\eta_l^2\frac{1}{nK}\sum_{i=1}^n \sum_{k''=1}^K\|\alpha_i^{t,k''} - \nabla f_i(x^{(t)})\|^2\right)\\
    &+ \sum_{r=1}^{k'} (1+\frac{1}{K-1})^r 12K\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{align}
Using the fact that $(1+\frac{1}{K-1})^r \leq 2e \leq 6$, we can get that: 
\begin{align}
    \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|m_i^{(t,k)} - c^k\nabla f_i(x^{(t)})\|^2 \leq 48K^3L^2\eta_l^2\|\nabla f(x^{(t)})\|^2 + 96 K^3L^2\eta_l^2 \Gamma^{(t)} +144K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{align}
\end{proof}

\begin{lemma}
\label{lem:variable_drift}
Under Assumption~\ref{assump:SGD_noise}, the tracking variable drift term $\Gamma^{(t)} = \frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\mathbb{E}\left\|\alpha_i^{t,k} - \nabla f_i(x^{(t)})\right\|^2$ can be bounded as:
\begin{equation}
    \Gamma^{(t)} \leq (1-\frac{Y}{n})\Gamma^{(t-1)} + \frac{Y}{n}\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{equation}
\end{lemma}
\begin{proof}
By using the definition of $\alpha_i^{t,k}$, we can get the following relation:
   \begin{align}
        \Gamma^{(t)} &= \frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\mathbb{E}\left\|\alpha_i^{t,k} - \nabla f_i(x^{(t)})\right\|^2\\
        &\leq (1-\frac{Y}{n})(1 + \frac{Y}{2n})\frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\mathbb{E}\left\|\alpha_i^{t-1,k} - \nabla f_i(x^{(t-1)})\right\|^2 + 2(1 + \frac{2n}{Y})G^2\\
        &+ \frac{Y}{n}\frac{1}{nK}\sum_{i=1}^n \sum_{k=1}^K\mathbb{E}\left\|\frac{ m_i^{t-1,k}  }{\sqrt{v_i^{t-1,k}}+\epsilon} - \nabla f_i(x^{(t)})\right\|^2\\
        &\leq (1-\frac{Y}{2n})\Gamma^{(t-1)} + \frac{Y}{n}\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon} +6n^2G^2\right)
   \end{align}
\end{proof}
\section{Theoretical Analysis for FAdamGT (Theorem \ref{thm:fadamet})}
\label{appen:fadamgt}
We first define the expected first order moment $\Tilde{m}_i^{(t,k)}$ as the following:
\begin{align}
    \Tilde{m}_i^{(t,k)} &\overset{\Delta}{=} \sum_{k'=1}^k c^{(k,k')} \left(\nabla f_i(x_i^{(t,k)}) - \nabla f_i(\gamma_i^{(t,k)}) + \frac{1}{n}\sum_{i=1}^n\nabla f_i(\gamma_i^{(t,k)})\right)
\end{align}
Where $\gamma_i^{(t,k)}$ is an auxilary variable that tracks the GT terms:
\begin{equation}
    \gamma_i^{(t,k)} = \begin{cases}
        x_i^{(t-1,k)} \quad i \in \mathcal{Y}^{t-1}\\
        \gamma_i^{(t-1,k)} \quad i \notin \mathcal{Y}^{t-1}
    \end{cases}
\end{equation}
We further define the local deviation term $\Xi^{(t)}$ as:
\begin{equation}
    \Xi^{(t)} = \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2
\end{equation}
\begin{proof}
    Given global iteration $t$, the update of the model at the server can be written as:
\begin{align}
    x^{(t+1)} &= x^{(t)} + \eta_g \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} (x_i^{(t,K+1)} - x^{(t)})\\
    &=x^{(t)} - \eta_g\eta_l \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}
\end{align}
By injecting Assumption~\ref{assump:genLoss}, we can get the following inequality:

\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) - \underbrace{\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle}_\text{Term I}\\
    &+\underbrace{\eta_g^2\eta_l^2\frac{L}{2}\mathbb{E}\left\|\frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\|^2}_\text{Term II}
\end{align}
For term I, we first define the average of all square root second moment:
\begin{equation}
    \Bar{v}^{(t)} = \frac{1}{n}\sum_{i=1}^n \sqrt{v_i^{(t)}}
\end{equation}
Term I can be upper bounded as:
\begin{align}
    &-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    &=-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{\Tilde{m}_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    &=-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \frac{\sum_{k'=1}^k c^{(k,k')} \left(\nabla f_i(x_i^{(t,k')})  + \frac{1}{K}\sum_{k''=1}^K \left(\frac{1}{n}\sum_{i'=1}^n\nabla f_i'(\gamma_i'^{(t,k'')})- \nabla f_i(\gamma_i^{(t,k'')})\right)\right)}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    \end{align}
    Using the fact that $\sum_{i=1}^n \left(\nabla f_i(\gamma_i^{(t,k)}) - \frac{1}{n}\sum_{i'=1}^n\nabla f_i'(\gamma_i'^{(t,k)})\right) = 0$, we can show that:
    \begin{align}
    &-\eta_g\eta_l \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\rangle\\
    &= -\eta_g\eta_l  \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \left(\frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) }{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})  }{{\Bar{v}^{(t)}}+\epsilon}\right)\right\rangle\\
    &-\eta_g\eta_l  \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \left( \frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})}{{\Bar{v}^{(t)}}+\epsilon} - \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon} + \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon}\right)\right\rangle\\
    &\leq -\eta_g\eta_l K\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 \\
    & -\eta_g\eta_l  \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \left(\frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) }{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})  }{{\Bar{v}^{(t)}}+\epsilon}\right)\right\rangle\\
    &-\eta_g\eta_l  \mathbb{E}\left\langle\nabla f (x^{(t)}), \frac{1}{n}\sum_{i=1}^n \sum_{k=1}^K \left( \frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})}{{\Bar{v}^{(t)}}+\epsilon} - \frac{c^k\nabla f_i(x^{(t)})}{{\Bar{v}^{(t)}}+\epsilon} \right)\right\rangle\\
    &\leq -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G+\epsilon}{(1-\beta_1)\beta_1} \mathbb{E}\|\frac{1}{nK}\sum_{i=1}^n\sum_{k=1}^K\frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon} - \frac{\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')})}{{\Bar{v}^{(t)}}+\epsilon}\|^2 \\
    &\leq -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1} \mathbb{E}\|\frac{1}{nK}\sum_{i=1}^n\sum_{k=1}^K\frac{\sqrt{\hat{v}_i^{(t,k)}} - \Bar{v}^{(t)}}{(\sqrt{\hat{v}_i^{(t,k)}}+\epsilon)(\Bar{v}^{(t)}+\epsilon)}\|^2 \\
    &\leq -\frac{\eta_g\eta_l K}{2}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 + \eta_g\eta_l \frac{G+\epsilon}{(1-\beta_1)\beta_1\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K\mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \\
    & + \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 
\end{align}
Similar to Appendix \ref{appen:fadamet}, term II can also be upper bounded as:
\begin{align}
    &\frac{\eta_g^2\eta_l^2L}{2}\mathbb{E}\left\|\frac{1}{S}\sum_{i\in \mathcal{S}^{(t)}} \sum_{k=1}^K \frac{m_i^{(t,k)}}{\sqrt{\hat{v}_i^{(t,k)}}+\epsilon}\right\|^2 \\
    &\leq 2\eta_g^2\eta_l^2K^2L\mathbb{E}\|\nabla f(x^{(t)}\|^2 + \frac{4\eta_g^2\eta_l^2KL}{\epsilon^2} \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \\
    &+ \frac{4\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 + \eta_g^2\eta_l^2KL\sigma^2
\end{align}
If we choose $\eta_g\eta_l \leq \min(\frac{(1-\beta_1)\beta_1}{8KL(G+\epsilon)}, \frac{1}{4KL})$, we can combine Term I and II and get:
\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) -\frac{\eta_g\eta_l K}{4}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 +  \frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) \Xi^{(t)} \\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 + \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 + \eta_g^2\eta_l^2KL\sigma^2
\end{align}
By using Lemma \ref{lem:deviation2}, we can bound $\Xi^{(t)}$ and get:
\begin{align}
    \mathbb{E} f(x^{(t+1)}) \leq& \mathbb{E} f(x^{(t)}) -\frac{\eta_g\eta_l K}{4}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 +  \frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) 4K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right) \\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 + \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 + \eta_g^2\eta_l^2KL\sigma^2
\end{align}
We can reorganize the inequality and get:
\begin{align}
\frac{\eta_g\eta_l K}{4}\frac{(1-\beta_1)\beta_1}{G+\epsilon}  \mathbb{E}\|\nabla f(x^{(t)}\|^2 \leq& \mathbb{E} f(x^{(t)}) - \mathbb{E} f(x^{(t+1)})\\
&+  \frac{\eta_g\eta_l}{\epsilon^2} \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right) 4K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right) \\
    &+ \eta_g\eta_lK \frac{G^2(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2} \mathbb{E}\|\Bar{v}^{(t+1)} - \Bar{v}^{(t)}\|^2 \\
    &+ \frac{2\eta_g^2\eta_l^2 (1-\epsilon)^2}{\epsilon^2} K^2L G^2 + \eta_g^2\eta_l^2KL\sigma^2
\end{align}
By summing up all global iterations $T$ and dividing both sides with constants, we get:
\begin{align}
    \frac{1}{T} \sum_{i=1}^T\mathbb{E}\|\nabla f(x^{(t)}\|^2 \leq & \frac{4(G+\epsilon)(\mathbb{E} f(x^{(1)}) - \mathbb{E} f(x^{(T+1)}))}{\eta_l\eta_g K (1-\beta_1)\beta_1 T}\\
    &+ 16K^2L^2\eta_l^2 \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
    &+ \frac{G^4(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2 T} \\
    &+ \frac{8\eta_g\eta_l (1-\epsilon)^2 (G+\epsilon)}{(1-\beta_1)\beta_1\epsilon^2} KL G^2\\
    &+\frac{4\eta_g\eta_lL(G+\epsilon)}{(1-\beta_1)\beta_1}\sigma^2
\end{align}

Finally, by bounding $\eta_l \leq \frac{1}{12T^{3/2}L}$, $\eta_g\eta_l \leq \frac{1}{12TL}$, and a specific step size
\begin{equation}
    \eta_g\eta_l = \min(\frac{(1-\beta_1)\beta_1}{8KL(G+\epsilon)}, \frac{1}{4KL}, \frac{1}{12TL}, \sqrt{\frac{S}{T}})
\end{equation}
we can get:
\begin{align}
\frac{1}{T} \sum_{i=1}^T\mathbb{E}\|\nabla f(x^{(t)}\|^2 \leq & \frac{4(G+\epsilon)(\mathbb{E} f(x^{(1)}) - \mathbb{E} f(x^{(T+1)}))}{ (1-\beta_1)\beta_1 K \sqrt{ST}}\\
    &+ \left(\frac{G+\epsilon}{(1-\beta_1)\beta_1} + \frac{1}{2}\right)^2\left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\frac{16K^2}{T^3}\\
    &+ \frac{G^4(G+\epsilon)}{(1-\beta_1)\beta_1 \epsilon^2 T} \\
    &+ \frac{8(1-\epsilon)^2 G^2(G+\epsilon)}{(1-\beta_1)\beta_1\epsilon^2} \frac{K}{T} \\
    &+\frac{4(G+\epsilon)}{(1-\beta_1)\beta_1}\sigma^2\frac{1}{T}\\
    &= \mathcal{O}\left(\frac{\mathbb{E} f(x^{(1)}) - f^*}{K\sqrt{ST}}+ \frac{1}{T} +\frac{K}{T}+\frac{K^2}{T^3}\right)
\end{align}

\end{proof}



\begin{lemma}
\label{lem:deviation2}   
Under Assumption~\ref{assump:genLoss} and \ref{assump:SGD_noise}, the local deviation term $\Xi^{(t)}$ can be bounded as the following:
\begin{equation}
    \Xi^{(t)} \leq 4K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{equation}
\end{lemma}
\begin{proof}
We first define the unbiased version of $m_i^{(t,k)}$ taking expectation on all stochastic gradients $g_i^{(t,k)}$. 
Then, we can bound the deviation term as:
\begin{align}
    \Xi^{(t)} = &\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \\
    &\leq  \frac{L^2}{n}\sum_{i=1}^n\sum_{k=1}^K \sum_{k'=1}^k c^{(k,k')}\mathbb{E}\| x_i^{(t,k')}- x^{(t)}\|^2
\end{align}

We can simplify the formulation by first unfolding each local step $x_i^{(t,k')}$:
\begin{align}
    &\frac{1}{n}\sum_{i=1}^n\mathbb{E}\| x_i^{(t,k')}- x^{(t)}\|^2\\
    &\leq  (1+\frac{1}{K-1}) \frac{1}{n}\sum_{i=1}^n\mathbb{E}\|x_i^{(t,k'-1)}- x^{(t)}\|^2 + K\frac{1}{n}\sum_{i=1}^n \mathbb{E}\left\|\eta_l \frac{m_i^{(t,k'-1)}}{\sqrt{\hat{v}_i^{(t,k'-1)}}+\epsilon}\right\|^2\\
    &\leq (1+\frac{1}{K-1}) \frac{1}{n}\sum_{i=1}^n\mathbb{E}\|x_i^{(t,k'-1)}- x^{(t)}\|^2 + 4K\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\\
    &\leq \sum_{r=1}^{k'} (1+\frac{1}{K-1})^r\left(4K\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)\right)
\end{align}
Using the fact that $(1+\frac{1}{K-1})^r \leq 2e \leq 6$, we can get that: 
\begin{align}
    \frac{1}{n}\sum_{i=1}^n\sum_{k=1}^K \mathbb{E}\|\sum_{k'=1}^k c^{(k,k')} \nabla f_i(x_i^{(t,k')}) - c^k\nabla f_i(x^{(t)})\|^2 \leq 4K^3L^2\eta_l^2 \left(\frac{G^2(1+\epsilon) + \sigma^2}{\epsilon}\right)
\end{align}

\end{proof}
\newpage
\section{Learning Rate and Target Accuracy for PEFT tasks}
\label{appen:lr_and_target_acc}
\begin{table}[ht]
\caption{The hyperparameters and target accuracy for PEFT tasks.}
\label{table:lr_and_target}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Dataset & \makecell{$\eta_l$} & \makecell{$\eta_g$} & \makecell{Target Accuracy}\\
\midrule
20NewsGroups  & $5\times 10^{-3}$ & $1\times 10^{-1}$& 75\%\\
\midrule  
SST-2 & $1\times 10^{-3}$ & $3\times 10^{-1}$& 85\%\\
\midrule  
QQP   & $1\times 10^{-4}$& $3\times 10^{-1}$& 75\%\\
\midrule  
QNLI  & $3\times 10^{-5}$& $3\times 10^{-1}$& 75\%\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\end{document}
