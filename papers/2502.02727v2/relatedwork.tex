\section{Related Works}
\textbf{Gradient Tracking:} Gradient Tracking (GT) methods \cite{di2016next,nedic2017achieving,tian2018asy,koloskova2021improved,carnevale2022gtadam,takezawa2022momentum,wang2024momentum} have emerged as a powerful solution to address data heterogeneity challenges in decentralized optimization algorithms. The core principle of GT lies in tracking gradient information from neighboring nodes during each communication round, ensuring more accurate gradient estimates across the network. Centralized FL algorithms such as {\tt SCAFFOLD}~\cite{karimireddy2020scaffold} and {\tt Proxskip}~\cite{mishchenko2022proxskip} are both designed base on this concept, and multiple works on serverless FL settings demonstrated superior improvement \cite{liu2023decentralized,ge2023gradient,berahas2023balancing,alghunaim2024local}, where communication efficiency is a primary concern. By effectively reducing the synchronization frequency while still guaranteeing convergence to the optimal point, GT has proven to be highly effective in mitigating the adverse effects of heterogeneous data distributions. Furthermore, existing studies have shown that under proper initialization of gradient tracking variables, many standard assumptions on data heterogeneity can be relaxed.

Recent advancements have also extended GT methods to address hierarchical network structures. {\tt SDGT} was introduced as the first GT algorithm tailored for semi-decentralized networks~\cite{chen2024taming}, bridging the gap between fully decentralized and centralized topologies. Meanwhile,~\cite{fang2024hierarchical} proposed {\tt MTGC}, a multi-timescale GT algorithm capable of operating efficiently in multi-tier networks. 
Despite these advancements, existing works predominantly focus on designing SGD based algorithms, leaving the combination of GT and adaptive optimizers largely unexplored and an open challenge.

\textbf{Adaptive Optimizer:} SGD optimizers rely on fixed or decaying learning rates, which often require careful tuning and may struggle with scenarios involving sparse gradients or noisy updates. To address these limitations, adaptive optimizers dynamically adjust learning rates based on the gradient history of individual parameters, enabling more effective navigation of complex optimization landscapes. Among the most prominent adaptive optimizers are {\tt AdaGrad}~\cite{duchi2011adaptive}, and {\tt Adam}~\cite{kingma2014adam}. {\tt AdaGrad} introduces per-parameter learning rate scaling to handle sparse features effectively. Building on this foundation, {\tt Adam} combines the benefits of momentum with adaptive learning rates, achieving robust convergence across various learning tasks. Recent advancements have further explored the decoupling of weight decay~\cite{loshchilov2017decoupled} and the time-varying effects of regularization terms~\cite{xie2024overlooked}, pushing the boundaries of adaptive optimization.

Several approaches have been proposed to integrate adaptive optimizers into FL. \cite{reddi2020adaptive} introduced {\tt FedAdam}, where the central server employs an adaptive optimizer to update the global model using aggregated client gradients. Additionally, \cite{xie2019local} incorporates adaptive optimization directly on local clients. More recently, \cite{sun2023efficient} presented {\tt FedLADA}, an FL algorithm in which clients utilize the Adam optimizer for local updates. In {\tt FedLADA}, the update gradient is computed as a weighted average of local gradients and global gradients. However, besides {\tt FedLADA}, all of these algorithms aren't designed to deal with data heterogeneity, and hence requires frequent global aggregation for good results. {\tt FedLADA} although maintained a global gradient estimation, requires a weighted sum operation where an additional hyperparameter has to be fine-tuned based on different data. This causes the performance to vary dramatically base on the chosen weights, which is not required in our algorithms.