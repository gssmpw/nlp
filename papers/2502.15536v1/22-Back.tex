%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}\label{sec:back}
Since Rust is a relatively new programming language, Section \ref{sec:rust_back} offers a brief overview of its key concepts and innovations. 
%while Section \ref{sec:rayon_back} introduces Rayon. 
Section \ref{sec:par_back} give a background on parallel patterns. 
Section \ref{sec:npb_back} presents the NPB suite's characteristics and details each benchmark.
    
    \subsection{Rust Programming Language}\label{sec:rust_back}
    \textbf{Rust fundamentals.} Rust is a safe and performant low-level language launched by Mozilla as an open-source project \cite{rustbook}. Its safety features derive from an ownership system, where every value has exactly one owner at a time, which is the variable or structure responsible for managing its memory. Once the owner goes out of scope, the value is dropped, avoiding memory leaks and double frees. When a value is assigned to another variable, the ownership is transferred, making the original variable invalid. Rust also allows borrowing, a system where references to data can be passed temporarily without transferring ownership. The borrowing rules permit multiple immutable references to exist simultaneously, allowing shared read-only access, but only one mutable reference can exist at a time to ensure exclusive write access. Rust’s compiler also tracks lifetimes, which ensures that references never outlive the data they point to, further preventing dangling pointers and undefined behavior \cite{rustbook}.
    
    \textbf{Unsafe Rust.}  A program that violates Rust's safety guarantees will not compile. Conversely, a benign program may also fail to compile if the compiler lacks sufficient information to be confident. This conservative approach makes Rust sound but incomplete \cite{rivera2021keeping}. In order to deal with this, programmers can use unsafe Rust. With this extension, developers can create unsafe blocks to bypass some safety checks enforced by Rust's ownership and borrowing system in specific code regions. When using unsafe Rust, the programmer must manually guarantee that the code respects Rust's rules. Misusing it can lead to memory problems, as happens in other unsafe languages like C/C++ \cite{rustbook}.

    %\subsection{Rayon Library}\label{sec:rayon_back}
    \textbf{Rayon library.} Rayon is a high-level framework for data parallelism that extends Rust's iterator-based traits to support regular parallel operations. For instance, a sequential iterator \texttt{into\_iter()} can be conveniently transformed to parallel by changing it to \texttt{into\_par\_iter()}. Rayon adheres to Rust's safety standards by limiting write access to shared data within parallel iterators. It uses a global thread pool to manage concurrency, maintains multiple threads waiting for tasks to be allocated, and uses a work-stealing system \cite{rayon}. The NPB suite mainly contains data-parallelism computations. Analogous to the OpenMP for Fortran and C++, Rayon can express operations such as \texttt{Map} and \texttt{MapReduce} \cite{map}, which are the main parallel patterns in the NPB. This paper uses Rayon to create a parallel version of the NPB-Rust.

        \begin{figure}[t]
        %\vspace{-0.5cm}
        \centering
        \includegraphics[width=0.36\textwidth]{Images/WS3.pdf}
        \vspace{-0.4cm}
        \caption{Rayon's work stealing methodology.}
        \vspace{-0.4cm}
        \label{fig:ws}
        \end{figure}

         \textbf{Work-stealing.} Rayon implements a work-stealing load-balancing strategy to manage threads' dynamically. A thread that receives an iterative task splits it into two parts: one to process immediately and another to defer for later execution. Idle threads in a thread pool continuously search for a job, and eventually, they check the to-do list of other threads. Figure \ref{fig:ws} illustrates this technique, where the idle thread \textit{B} steals a task from the to-do list of thread \textit{A}.


    \begin{figure}[!b!]
        \vspace{-0.8cm}
         \centering
         \subfloat[Sequential \texttt{Map}]{\includegraphics[width=0.16\textwidth]{Images/map_serial.pdf}\label{fig:map_ser}}
         \subfloat[Parallel \texttt{Map}]{\includegraphics[width=0.16\textwidth]{Images/map_parallel.pdf}\label{fig:map_par}}
         \subfloat[Parallel \texttt{MapReduce}]{\includegraphics[width=0.16\textwidth]{Images/mapreduce_parallel.pdf}\label{fig:map_reduce}}
         \caption{Illustration of sequential \texttt{Map}, parallel \texttt{Map}, and parallel \texttt{MapReduce}.}
         \vspace{0.8cm}
        %\end{figure}

   % \begin{figure}[h]
    \lstset{aboveskip=0pt, belowskip=7pt}
    \begin{lstlisting}[language=C++, style=boxed,caption={\texttt{Map} and \texttt{MapReduce} with OpenMP in C++.}, label={lst:map_c}]
std::vector<int> results(n);
#pragma omp parallel for
for(i=0; i<n; i++){
  results[i] = /* computation */
}

int sum = 0;
#pragma omp parallel for reduction (+:sum)
for(i=0; i<n; i++){
  sum += /* computation */
}
\end{lstlisting}
    %\end{figure}
\vspace{0.4cm}
    %\begin{figure}[t]
    \lstset{aboveskip=0pt, belowskip=0pt}
    \begin{lstlisting}[language=Rust, style=boxed,caption={\texttt{Map} and \texttt{MapReduce} with Rayon in Rust.}, label={lst:map_r}]
let results: Vec<_> = (0..n).into_par_iter()
  .map(|i| {
    /* computation */
  })
.collect();

let sum: i32 = (0..n).into_par_iter()
  .map(|i| {
    /* computation */
  })
.reduce(|| 0, |acc, x| acc + x);
\end{lstlisting}
    \end{figure}


    \subsection{Parallel Patterns in NPB}\label{sec:par_back}
    Parallel patterns can be understood in two parts: their high-level semantics and their low-level implementation details. High-level semantics refers to when and where a pattern can be applied in sequential code and any limitations to its use. The low-level implementation deals with the technical details hidden from the programmer, like communication methods, synchronization, and task scheduling. By focusing on the high-level purpose, programmers can add parallelism to their code without dealing with the complex details of how it works. Each parallel programming framework provides its own version of these patterns with unique interfaces and internal designs. As a result, patterns may differ in usability and efficiency based on their design and implementation \cite{NPB-CPP-2021}. Rayon and OpenMP provide different APIs to express data-parallelism, but both can be described using the abstract \texttt{Map} and \texttt{MapReduce}.

    \textbf{Map.} The \texttt{Map} pattern \cite{map} involves applying a set of identical operations to all elements in a collection. This can be used to parallelize loops without data dependencies between iterations. Figures \ref{fig:map_ser} and \ref{fig:map_par} illustrate how the sequential and parallel \texttt{Map} pattern works. The white squares represent the initial values, the blue squares represent the final values, and the green square represents the operation. In OpenMP, programmers annotate parallel \texttt{for} loops using compiler preprocessor directives. Lines 1 to 5 in Listing \ref{lst:map_c} showcase how to implement a parallel \texttt{Map} in C++ with OpenMP. Rayon expresses parallelism by extending the iterator traits to parallel versions, lines 1 to 5 in Listing \ref{lst:map_r} demonstrates how to express the parallel \texttt{Map} pattern in Rust.

    \textbf{MapReduce.} The \texttt{MapReduce} pattern \cite{map} is the union of the \texttt{Map} with the \texttt{Reduce} operation. In the reduction process, the elements from a collection are combined into a single output. Figure \ref{fig:map_reduce} illustrates how the parallel \texttt{MapReduce} works, where the purple squares represent the reduction operation. This pattern can be used to parallelize \texttt{for} loops that exhibit specific data dependencies, and synchronization is necessary. In Listing \ref{lst:map_c}, lines 7 to 11 indicate how to apply this pattern in C++ using the OpenMP annotation. Lines 7 to 11 in Listing \ref{lst:map_r} show a manual implementation of a reduction function applied after a \texttt{Map} operation, leveraging iterator-based traits.

    \textbf{Barrier.} By default, in the end of parallel \texttt{Map} and \texttt{MapReduce} regions, there are an implicit barrier. It is a synchronization primitive for a group of threads \cite{map}. The barrier is a point where any thread must stop and can not proceed until all other threads reach it. With the \texttt{nowait} directive is possible to avoid this synchronization in OpenMP, on the other hand, the Rayon library does not provide a similar approach. It is also possible to add explicit barriers in the code, this feature is available for Fortran, C++, and Rust.
    

    \subsection{NAS Parallel Benchmarks}\label{sec:npb_back}
    Originally stemming from the field of computational fluid dynamics (CFD), the NPB approximates real-world workloads and mathematical methods. The suite's applications encompass various computational characteristics, including irregular memory access patterns, complex data dependencies, and intensive data communication requirements \cite{NPBOriginal1}. 
    
    %Applications offer different problem sizes — called NPB classes — that are configurable at compile time.
    Each application offers different levels of problem sizes, which are configurable through the selection of a specific class at compile time. Classes S and W represent the smallest scale and are intended for conducting small tests. The standard test classes are A, B, and C, where the complexity increases fourfold with each ascending class. Finally, Classes D, E, and F are specifically designed for highly intensive tests, with the complexity escalating sixteen-fold with each step up in class.
    
    The eight benchmarks starts with an initialization phase and concludes with a thoroughly built-in verification mechanism to check the results' correctness. The applications feature automated measurement of execution time and performance estimation in terms of mega floating-point operations per second (MFLOPS). The five kernels and three pseudo-applications are described as follows, and Figure \ref{fig:npb-flowchart} presents a detailed flowchart of the NPB programs.

            \begin{figure*}[h!t!]
        \centering
        \includegraphics[width=1\textwidth]{Images/npb_flow.pdf}
        \caption{NPB kernels and pseudo-applications flowchart. Adapted from \cite{beyond}.}
        \label{fig:npb-flowchart}
        \vspace{-0.4cm}
        \end{figure*}
    
        \textbf{Embarrassingly Parallel (EP).}
        This kernel primarily consists of two key processes. It generates a large set of Gaussian pseudo-random numbers and then accumulate two-dimensional statistics from those numbers. While this problem can be readily divided into concurrent components, its low communication overhead makes it suitable for estimating the floating-point operational capacity of the target hardware \cite{npb-results-11-96}. 

        \textbf{Conjugate Gradient (CG).}
        Employs an iterative conjugate gradient algorithm to approximate the smallest eigenvalue of a large and unstructured matrix. Induces long-distance communication and performs sparse matrix-vector multiplications. Due to the irregular nature of the computations, this kernel also imposes demands on memory locality \cite{npb-results-11-96}.
        
        \textbf{Discrete 3D Fast Fourier Transform (FT).}
        Performs a Fast Fourier Transform in three-dimensional matrices of complex numbers to solve a three-dimensional partial differential equation. Apply both spectral and inverse methods within nested loops. The kernel is specifically designed to test exhaustive long-distance communication performance.\cite{npb-results-11-96, NPB-2.0}.

        \textbf{Integer Sort (IS).}
        A bucket-sorting algorithm implementation that operates over a sparse set of integer numbers. This kernel focuses on testing integer computation speed and communication performance without involving any intensive floating-point operations. It simulates a computational task analogous to particle-in-cell applications in physics. \cite{npb-results-11-96, nasomp}.

        \textbf{Multi Grid (MG).}
        A simplified multi-grid V-cycle operation and residual calculation using constant coefficients instead of variable ones. It is designed to solve a three-dimensional scalar Poisson equation. The benchmark is intended for performing high-volume structured short- and long-distance communications \cite{npb-results-11-96, NPB-2.0}.

        \textbf{Block Tri-diagonal Solver (BT).}
        Simulates a CFD application that solves three-dimensional compressible Navier-Stokes equations. The finite differences solution is based on an Alternating Direction Implicit (ADI) approximate factorization of the dimensions. Produces block-tridiagonal systems that solve the unknown vectors using back substitution \cite{npb-results-11-96, NPB-2.0}.

        \textbf{Scalar Penta-diagonal Solver (SP).}
        This pseudo-application is similar to BT. It is a simulated CFD program that solves multiple independent systems of scalar penta-diagonal equations, which are non-diagonally dominant. The finite differences solution is based on a Beam-Warming approximate factorization of the dimensions \cite{npb-results-11-96, NPB-2.0}.

        \textbf{Lower-Upper Gauss-Seidel Solver (LU).}
        Represents the computations associated with a class of CFD algorithms. This pseudo-application employs a symmetric successive over-relaxation (SSOR) numerical scheme to solve a regular-sparse, lower and upper triangular system. It is a variation of the Gauss-Seidel method for linear system equations \cite{npb-results-11-96, NPB-2.0}.


        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
