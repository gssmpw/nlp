@article{berahas2022theoretical,
  title={A theoretical and empirical comparison of gradient approximations in derivative-free optimization},
  author={Berahas, Albert S and Cao, Liyuan and Choromanski, Krzysztof and Scheinberg, Katya},
  journal={Foundations of Computational Mathematics},
  volume={22},
  number={2},
  pages={507--560},
  year={2022},
  publisher={Springer}
}

@article{biderman2024lora,
  title={Lora learns less and forgets less},
  author={Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{chen2024enhancing,
  title={Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures},
  author={Chen, Yiming and Zhang, Yuan and Cao, Liyuan and Yuan, Kun and Wen, Zaiwen},
  journal={arXiv preprint arXiv:2410.07698},
  year={2024}
}

@article{chen2024fira,
  title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?},
  author={Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren},
  journal={arXiv preprint arXiv:2410.01623},
  year={2024}
}

@article{das2024natural,
  title={Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning},
  author={Das, Arijit},
  journal={arXiv preprint arXiv:2410.16029},
  year={2024}
}

@article{dettmers2023qlora,
  title={QLoRA: efficient finetuning of quantized LLMs (2023)},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  volume={52},
  pages={3982--3992},
  year={2023}
}

@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@article{gautam2024variance,
  title={Variance-reduced zeroth-order methods for fine-tuning language models},
  author={Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok},
  journal={arXiv preprint arXiv:2404.08080},
  year={2024}
}

@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}

@article{he2024subspace,
  title={Subspace optimization for large language models with convergence guarantees},
  author={He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun},
  journal={arXiv preprint arXiv:2410.11289},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{lai2024lisa,
  title={Lisa: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9579--9589},
  year={2024}
}

@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{liang2024memory,
  title={Memory-efficient llm training with online subspace descent},
  author={Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang},
  journal={arXiv preprint arXiv:2408.12857},
  year={2024}
}

@article{liu2024mobilellm,
  title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}

@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017},
  publisher={Springer}
}

@inproceedings{ren2021zero,
  title={$\{$Zero-offload$\}$: Democratizing $\{$billion-scale$\}$ model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}

@article{robert2024ldadam,
  title={Ldadam: Adaptive optimization from low-dimensional gradient statistics},
  author={Robert, Thomas and Safaryan, Mher and Modoranu, Ionut-Vlad and Alistarh, Dan},
  journal={arXiv preprint arXiv:2410.16103},
  year={2024}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{tang2024rethinking,
  title={Rethinking optimization and architecture for tiny language models},
  author={Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.02791},
  year={2024}
}

@inproceedings{thangarasa2023spdf,
  title={SPDF: Sparse pre-training and dense fine-tuning for large language models},
  author={Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2134--2146},
  year={2023},
  organization={PMLR}
}

@article{wen2025breaking,
  title={Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training},
  author={Wen, Ziqing and Luo, Ping and Wang, Jiahuan and Deng, Xiaoge and Zou, Jinping and Yuan, Kun and Sun, Tao and Li, Dongsheng},
  journal={arXiv preprint arXiv:2501.07237},
  year={2025}
}

@article{xia2024chain,
  title={Chain of lora: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@inproceedings{zhang2023g10,
  title={G10: Enabling an efficient unified gpu memory and storage architecture with smart tensor migrations},
  author={Zhang, Haoyang and Zhou, Yirui and Xue, Yuqi and Liu, Yiqi and Huang, Jian},
  booktitle={Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={395--410},
  year={2023}
}

@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{zhao2024second,
  title={Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer},
  author={Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W},
  journal={arXiv preprint arXiv:2402.15173},
  year={2024}
}

@article{zhu2024apollo,
  title={APOLLO: SGD-like Memory, AdamW-level Performance},
  author={Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon},
  journal={arXiv preprint arXiv:2412.05270},
  year={2024}
}

