\documentclass{article}

\input{math_commands.tex}

\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{authblk} % For author affiliation formatting

\title{A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

% 定义作者及其上标
\author[1,*]{Yiming Chen}
\author[2,*]{Yuan Zhang}
\author[1]{Yin Liu}
\author[3,\dag]{Kun Yuan}
\author[1]{Zaiwen Wen}

% 定义作者的单位
\affil[1]{Beijing International Center for Mathematical Research, Peking University, Beijing, China}
\affil[2]{Center for Data Science, Peking University, Beijing, China}
\affil[3]{Center for Machine Learning Research, Peking University, Beijing, China}

% 添加 Equal contribution 的脚注
\renewcommand\Authands{ and }  % 修改默认的 "and" 连接方式
\renewcommand\footnotemark{}  % 避免在最后出现多余符号
\renewcommand{\thefootnote}{\fnsymbol{footnote}}  % 使用符号编号脚注

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
% \renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf

\begin{document}
\maketitle

\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding author (email: kunyuan@pku.edu.cn).}
% \footnotetext[3]{Our code is available at \url{https://github.com/optsuite/LOZO}.}

\begin{abstract}
The memory challenges associated with training Large Language Models (LLMs) have become a critical concern, particularly when using the Adam optimizer. To address this issue, numerous memory-efficient techniques have been proposed, with GaLore standing out as a notable example designed to reduce the memory footprint of optimizer states. However, these approaches do not alleviate the memory burden imposed by activations, rendering them unsuitable for scenarios involving long context sequences or large mini-batches. Moreover, their convergence properties are still not well-understood in the literature. In this work, we introduce a Randomized Subspace Optimization framework for pre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional training problem into a series of lower-dimensional subproblems. At each iteration, a random subspace is selected, and the parameters within that subspace are optimized. This structured reduction in dimensionality allows our method to simultaneously reduce memory usage for both activations and optimizer states. We establish comprehensive convergence guarantees and derive rates for various scenarios, accommodating different optimization strategies to solve the subproblems. Extensive experiments validate the superior memory and communication efficiency of our method, achieving performance comparable to GaLore and Adam.
\end{abstract}

% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}

\section{Introduction}\label{sec-introduction}

Large Language Models (LLMs) have achieved remarkable success across various domains \citep{brown2020language, achiam2023gpt, dubey2024llama}, primarily driven by the increasing scale of datasets and model parameters. The Adam optimizer \citep{kingma2014adam, loshchilov2017decoupled} is widely recognized as the default choice for training these models, owing to its operation efficiency and robust performance. 

However, as the scale of LLMs continues to grow, the associated memory demands have emerged as a significant bottleneck. This challenge stems from the need to store optimizer states, such as first-order and second-order moments, alongside the activations required for gradient computations. For instance, training a LLaMA-7B model necessitates 28GB of memory to store optimizer states in FP16 precision \citep{zhao2024galore}, while a GPT-3 model with 175B parameters requires an extraordinary 1.4TB memory in FP32 precision. Additionally, in scenarios involving long sequence lengths or large mini-batches, activation memory dominates as the primary constraint \citep{zhang2024revisiting}. These substantial memory requirements necessitate either deploying additional GPUs or reducing batch sizes. However, increasing the number of GPUs introduces additional communication overhead, potentially limiting training scalability \citep{malladi2023fine}, while smaller batch sizes prolong training time due to reduced throughput.

\textbf{Memory-efficient training algorithms.} Significant efforts have been made to address the memory overhead in LLMs training. One line of research  focuses on parameter-efficient methods, such as Low-Rank Adaptation (LoRA) and its variants \citep{hu2021lora,lialin2023relora,xia2024chain}, which constrain trainable parameters to low-rank subspaces for each weight matrix. Similarly, sparsity-based techniques \citep{thangarasa2023spdf} reduce memory usage by training only a subset of weights. These strategies decrease the number of trainable parameters, thereby reducing the memory requirements for storing gradients and optimizer states. Another research direction aims to achieve memory savings through the compression of optimizer states. For instance, GaLore and its variants \citep{zhao2024galore,he2024subspace,hao2024flora,chen2024enhancing} project gradients onto low-rank subspaces, leveraging the compressed gradients to compute the first- and second-order moments, which significantly reduces their memory footprint. {Alternatively, Adam-mini \citep{zhang2024adam} uses block-wise second-order moments for learning rate adjustments to reduce memory redundancy.} A recent study, Apollo \citep{zhu2024apollo}, reinterprets Adam as an adaptive learning rate algorithm applied to the gradient. Instead of the coordinate-wise approach used in Adam, it employs a column-wise adaptive learning rate, thereby effectively reducing the memory overhead associated with optimizer states.

\textbf{Limitations in existing approaches.} Despite the progress in  memory-efficient algorithms for training LLMs, two critical limitations persist in the aforementioned approaches:
\begin{itemize}
    \item[\textbf{L1.}] \textbf{Inability to reduce activations.} While the aforementioned approaches effectively reduce memory associated with optimizer states, they fail to address the memory burden posed by activations. This limitation stems from their reliance on computing full-rank gradients, which necessitates storing the complete activations. As a result, these methods are unsuitable for scenarios involving long context sequences or large mini-batches. 

    \item[\textbf{L2.}]  \textbf{Insufficient convergence guarantees.} While the aforementioned approaches demonstrate strong empirical performance, their theoretical convergence properties remain less understood. For instance, GaLore \citep{zhao2024galore} provides convergence analysis only for fixed projection matrices, rather than for the periodically updated projection matrices used in practical implementations. This lack of comprehensive theoretical guarantees raises concerns about whether these methods reliably converge to the desired solution and the rates at which such convergence occurs.

    % \item[\textbf{L2.}]  \textbf{Absence of convergence guarantees.} While the aforementioned approaches demonstrate strong empirical performance, their theoretical convergence properties remain less understood. Due to the absence of rigorous convergence guarantees, it is unclear whether these methods consistently converge to the desired solution or what their convergence rates might be. This gap raises concerns about the reliability and generalizability of these approaches. For instance, \cite{he2024subspace} identifies counterexamples in which GaLore \citep{zhao2024galore} converges to biased solutions. 

\end{itemize}

\textbf{Main results and contributions.}
In this work, we propose a method that concurrently reduces memory consumption for both the optimizer states and activations. The central idea behind our approach is to decompose the original high-dimensional training problem into a series of lower-dimensional subproblems. Specifically, at each iteration, we randomly select a subspace and optimize the parameters within this subspace. After completing the optimization in one subspace, we switch to a different subspace and continue the process. Since each subproblem operates in a lower-dimensional space, it requires smaller gradients and optimizer states. As we will demonstrate, the reduced dimensionality of the subproblems also leads to a significant reduction in the memory required for storing activations. Furthermore, the smaller scale of the subproblems results in reduced communication overhead when training across multiple workers. Our main contributions are as follows:
\begin{itemize}
    \item[\textbf{C1.}] \textbf{Subspace method for LLM training.} We introduce a \underline{\textbf{R}}andomized \underline{\textbf{S}}ubspace \underline{\textbf{O}}ptimization (\textbf{RSO}) framework for LLM training, which decomposes the original training problem into a series of lower-dimensional subproblems. This decomposition simultaneously reduces the memory required for optimizer states and activations, effectively addressing \textbf{Limitation L1}. Furthermore, the framework can reduce communication overhead in distributed training scenarios.
    
    \item[\textbf{C2.}] \textbf{Theoretical convergence guarantees.} We provide a comprehensive convergence analysis for the RSO framework. The established guarantees and rates apply across various scenarios. These include subproblems solved using zeroth-order, first-order, or second-order algorithms, as well as optimization methods like gradient descent, momentum gradient descent, adaptive gradient descent, and their stochastic variants. This addresses \textbf{Limitation L2}. Notably, we present refined convergence guarantees for scenarios where subproblems are solved using the Adam optimizer.
    % \blue{\sout{Prior to our work, no convergence rates had been established in the literature for Adam-based methods using compressed optimizer states. Our contribution thus fills a significant gap in memory-efficient LLMs training.}}
    
    \item[\textbf{C3.}] \textbf{Improved experimental performances.} We conduct extensive experiments to evaluate the proposed RSO framework. The experimental results demonstrate that our approach significantly enhances memory efficiency compared to state-of-the-art methods, such as GaLore and LoRA. Additionally, our method achieves faster training speeds by reducing communication overhead, outperforming both GaLore and Adam while maintaining comparable performance levels. These findings highlight the practical values of our approach.

\end{itemize}

\section{Related Works}
% \subsection{Algorithm-based Memory-efficient Methods}

\textbf{Parameter-efficient methods.} 
% A direct approach to memory savings is to reduce the number of trainable parameters. This can be achieved by designing smaller models \citep{liu2024mobilellm, tang2024rethinking}, or by training only a low-rank adaptation of the original models \citep{hu2021lora}.
A promising approach to memory-efficient training involves parameter-efficient methods, which reduce the number of trainable parameters and consequently lower the memory required for storing optimizer states. For example, \cite{hu2021lora} propose Low-Rank Adaptation (LoRA), which restricts trainable parameters to a low-rank subspace for each weight matrix. Similarly, \cite{thangarasa2023spdf} incorporate sparsity by training only a subset of weights. While these methods effectively reduce memory consumption, the reduction in trainable parameters can sometimes lead to suboptimal model performance \citep{biderman2024lora}. To address this limitation, recent advancements suggest using multiple LoRA updates to enable high-rank weight updates \citep{lialin2023relora, xia2024chain}. However, in pre-training settings, this approach still relies on a full-rank weight training phase as a warm-up before transitioning to low-rank training \citep{lialin2023relora}, thereby limiting its memory efficiency.

\textbf{{Optimizer-efficient methods.}} An alternative approach to memory savings focuses on compressing optimizer states while maintaining the number of trainable parameters. GaLore \citep{zhao2024galore} achieves this by compressing the gradient matrix through a projection onto a subspace and leveraging the compressed gradient to compute first- and second-order moments. This projection reduces the gradient size and is typically derived via the Singular Value Decomposition (SVD) of the true gradient \citep{zhao2024galore}. To mitigate the computational cost of SVD, alternative methods have been proposed, such as using random matrices \citep{hao2024flora,he2024subspace} or generating the projection matrix through online Principal Component Analysis (PCA) \citep{liang2024memory}. {Fira \citep{chen2024fira} and LDAdam \citep{robert2024ldadam} employ an error-feedback mechanism. The former combines the true gradient with the GaLore update to improve performance, while the latter explicitly accounts for both gradient and optimizer state compression.} Apollo \citep{zhu2024apollo} interprets Adam as an adaptive learning rate algorithm and uses compressed optimizer states directly as scaling factors for the true gradient. Additionally, Adafactor \citep{shazeer2018adafactor} discards the first-order moment and approximates the second-order moment with two low-rank matrices, while Adam-mini \citep{zhang2024adam} proposes that block-wise second-order moments are sufficient for adjusting learning rates. \citep{das2024natural} integrates the GaLore method with a natural gradient optimizer to enhance performance. Meanwhile, \citet{wen2025breaking} applies wavelet transforms to compress gradients beyond the low-rank structures.

\textbf{Activation-efficient methods.} 
Although the aforementioned methods effectively reduce memory consumption for optimizer states, they do not address the memory costs associated with activations. To reduce activations, zeroth-order (ZO) algorithms have been introduced in LLM training \citep{malladi2023fine}. These methods can be further improved through variance reduction techniques \citep{gautam2024variance}, while \citet{zhao2024second} utilizes ZO approaches to approximate a natural gradient algorithm. Moreover, \citet{chen2024enhancing} proposes a novel ZO framework to enhance performance. Unlike first-order (FO) methods, ZO algorithms approximate gradients by finite differences in function values, eliminating the need for explicit gradient computation. This approach bypasses backpropagation and activation storage, significantly reducing memory demands. However, due to their slower convergence rates \citep{duchi2015optimal, nesterov2017random, berahas2022theoretical}, ZO methods are primarily suitable for fine-tuning applications. Similarly, FO methods can achieve activation savings by layer-wise training \citep{lai2024lisa}, but their use also predominantly targets fine-tuning phases.

\textbf{System-based methods.} 
Several system-level techniques have been proposed to improve memory efficiency. Activation checkpointing \citep{chen2016training} reduces memory usage by recomputing activations on demand rather than storing them throughout the entire iteration, though this comes at the cost of increased computational complexity. Quantization \citep{dettmers2023qlora} lowers memory consumption by using lower-bit data representations, but this may introduce a trade-off between memory efficiency and training precision. Additionally, methods such as those introduced by \citet{ren2021zero, zhang2023g10} reduce GPU memory usage by offloading data to non-GPU resources, which can lead to additional communication overhead.

\section{Preliminaries}
This section introduces the optimization framework for LLM pre-training and fine-tuning, followed by a review of several memory-efficient methods. 

\subsection{LLM Optimization}
When addressing the pre-training or fine-tuning of LLMs, the problem can be formulated as follows: 
\begin{equation}\label{prob-opt}
    \min_{\bW} f(\bW) := \mathbb{E}_{\xi} \left[F(\bW; \xi)\right],
\end{equation}  
where \(\bW = \{W_\ell\}_{\ell=1}^\cL\) represents the set of trainable parameters with a total dimension of \(d\). Here, \(W_\ell \in \mathbb{R}^{m_\ell \times n_\ell}\) denotes the weight matrix for the \(\ell\)-th layer, and \(\cL\) is the total number of layers. The function \(F(\bW; \xi)\) is the loss function, which depends on the random variable \(\xi\) representing individual data samples.

To address the optimization problem defined in (\ref{prob-opt}), commonly used approaches include  SGD \citep{bottou2010large}, Momentum SGD \citep{sutskever2013importance}, and Adam \citep{kingma2014adam}. The iterative update rule for Adam is as follows:
\begin{subequations}\label{eqa:adam}
\begin{align}
 \mathbf{M}^t & = \beta_1 \cdot \mathbf{M}^{t-1} + (1 - \beta_1) \cdot \nabla F(\bW^t; \xi^t), \label{eqa:adam-1}\\
 \mathbf{V}^t &= \beta_2 \cdot \mathbf{V}^{t-1} + (1 - \beta_2) \cdot (\nabla F(\bW^t; \xi^t))^2, \label{eqa:adam-2}\\
 \hat{\mathbf{M}}^t &= {\mathbf{M}^t}/{(1 - \beta_1^t)}, \quad \hat{\mathbf{V}}_t = {\mathbf{V}_t}/{(1 - \beta_2^t)},\\
 \bW^{t+1} &= \bW^t - \alpha \cdot {\hat{\mathbf{M}}^t}/{(\sqrt{\hat{\mathbf{V}}^t} + \epsilon)}.
\end{align}
\end{subequations}
Here, $\mathbf{M}$ and $\mathbf{V}$ represent the first-order and second-order moments, respectively, and $\epsilon > 0$ is a small constant.

\subsection{Memory Consumption in LLM Training}
The key memory components involved in the training process include four primary elements: model parameters, optimizer states, gradients, and activations. The model component stores parameters required for training. In the case of the Adam optimizer, the optimizer states are represented by the first and second moment estimates, denoted as \(\mathbf{M}\) and \(\mathbf{V}\). The gradient corresponds to the memory cost associated with \(\nabla F(\bW; \xi)\). With the Adam optimizer, both the optimizer state and the gradient are determined by the number of trainable parameters, see recursions (\ref{eqa:adam-1})--(\ref{eqa:adam-2}). 

Another significant memory cost arises from the activations, which represent the intermediate values computed during forward propagation. Unlike the optimizer state and gradients, the memory  for activations depends on multiple factors, including model size, batch size, and sequence length.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/1b-memory.pdf}
    \caption{\small Memory components for training LLaMA-1B model with Adam optimizer.}
    \label{fig:memory-component-llama1b}
\end{figure}

Figure \ref{fig:memory-component-llama1b} illustrates the memory consumption during the training of the LLaMA-1B model. For small batch sizes, the optimizer state constitutes a substantial portion of the memory usage. In contrast, for large batch sizes, activations dominate and account for nearly the entire memory cost.

\subsection{Memory-efficient Method}
As previously discussed, the optimizer state imposes a substantial memory overhead. To address this challenge, GaLore \citep{zhang2023fine} introduces a projection technique that generates a compressed representation of the optimizer state, eliminating the need to store its full version. Consequently, the update rule of GaLore is as follows:
\begin{subequations}\label{eqa:galore}
\begin{align}
 \tilde{\mathbf{M}}^t &= \beta_1 \cdot \mathbf{\tilde M}^{t-1} \hspace{-0.5mm}+\hspace{-0.5mm} (1 \hspace{-0.5mm}-\hspace{-0.5mm} \beta_1) \hspace{-0.5mm}\cdot\hspace{-0.5mm} \bP^\top \nabla F(\bW^t; \xi^t), \label{eqa:galore-1} \\
 \tilde{\mathbf{V}}^t &= \beta_2 \cdot \mathbf{\tilde V}^{t-1} \hspace{-0.5mm}+\hspace{-0.5mm} (1 \hspace{-0.5mm}-\hspace{-0.5mm} \beta_2) \hspace{-0.5mm}\cdot\hspace{-0.5mm}(\bP^\top \nabla F(\bW^t; \xi^t))^2, \label{eqa:galore-2} \\
 \hat{\mathbf{M}}_t &= {\mathbf{\tilde M}_t}/{(1 - \beta_1^t)}, \quad \hat{\mathbf{V}}_t = {\tilde{\mathbf{V}}_t}/{(1 - \beta_2^t)},\\
 \bW^{t+1} &= \bW^t - \alpha \cdot \bP \ {\hat{\mathbf{M}}_t}/{(\sqrt{\hat{\mathbf{V}}}_t + \epsilon)}.
\end{align}
\end{subequations}
Here, \( \bP \) represents the projection matrix, which maps the gradient matrix onto a lower-dimensional subspace. Specifically, GaLore selects \( \bP \) as the top left singular vectors of the gradient matrix, capturing its most important components.

Since the projected gradient \( \bP^\top \nabla F(\bW^t; \xi^t) \) lies within a low-rank subspace, the associated optimizer states \( \tilde{\mathbf{M}} \) and \( \tilde{\mathbf{V}} \) in GaLore are also substantially reduced in size. This leads to notable memory savings compared to the Adam optimizer. However, as shown in Figure \ref{fig:memory-component-llama1b}, the optimizer state contributes significantly to memory costs primarily when using a small batch size. Conversely, with larger batch sizes—more practical in many scenarios—the memory efficiency advantages of GaLore diminish.

\section{Randomized Subspace Optimization}
In this section, we present the randomized subspace optimization (RSO) method, tailored explicitly for the pre-training and fine-tuning of LLMs.

\subsection{Algorithm Framework}  
As previously discussed, the memory overhead in LLM training primarily stems from the large scale of the models. In other words, the primary source of memory consumption arises from the high dimensionality of the LLM training problem (\ref{prob-opt}). This observation motivates us to decompose the original problem into a series of lower-dimensional subproblems. By partitioning the problem into smaller components, we can effectively reduce memory usage, as each subproblem requires less memory to process.

Similar to the random coordinate descent \citep{wright2015coordinate}, which optimizes the objective function one coordinate at a time, we address problem (\ref{prob-opt}) incrementally, subspace by subspace. The proposed update rules are as follows:
\begin{subequations}\label{eqa-SO}
\begin{align}
    & \tilde{\bB}^k \approx \argmin_{\bB} \left\{ f(\bW^{k} + \bP^{k} \bB) + \frac{1}{2\eta^{k}} \|\bB\|^2 \right\}, \label{prob-subopt-prox} \\
    & \bW^{k+1} = \bW^{k} + \bP^{k} \tilde{\bB}^k, \label{eqa-outer-step}
\end{align}
\end{subequations}
Here, \( f(\bW^{k} + \bP^{k} \bB) = \mathbb{E}_\xi[F(\bW^{k} + \bP^{k} \bB; \xi)] \) in which \( \bP^{k} = \{P^{k}_{\ell}\}_{\ell=1}^\cL \) denotes the subspace projection matrices. Each \( P^{k}_\ell \in \mathbb{R}^{m_\ell \times r_\ell} \) is a randomly selected matrix with \( r_\ell \ll m_\ell \). The parameters \( \bB = \{B_\ell\}_{\ell=1}^\cL \) consist of variables with significantly smaller dimensions compared to \( \bW \). Specifically, in the \( \ell \)-th layer, \( B_\ell \) has dimensions \( r_\ell \times n_\ell \), whereas \( W_\ell \) has dimensions \( m_\ell \times n_\ell \). A proximal term $\|\bB\|^2:=\sum_\ell\|B_\ell\|_F^2$ is introduced to (\ref{prob-subopt-prox}) to ensure convergence, with coefficient \( \eta^{k} \)  to regulate its influence.

In the \( k \)-th iteration, a subspace projection matrix \( \bP^k \) is randomly selected, and the subproblem in (\ref{prob-subopt-prox}) is solved. {This process approximately minimizes the objective function within the chosen subspace. Upon solving the subproblem, the current parameters are updated, and a new subspace projection matrix, \( \bP^{k+1} \), is selected for the subsequent iteration.} When addressing the subproblem in (\ref{prob-subopt-prox}), standard optimizers such as GD, SGD, momentum SGD or Adam can be employed. Notably, obtaining an exact solution in (\ref{prob-subopt-prox}) is not required; an inexact solution suffices for the proposed approach. The RSO algorithm is presented in Algorithm \ref{alg-rso}.

\begin{algorithm}[!htbp]
\caption{Randomized Subspace Optimization}\label{alg-rso}
\KwIn{Initialization $\bW^0$}
\KwOut{Solution $\bW^K$}
\For{$k = 0, 1, \dots, K-1$}{
    Sample $\bP^k$ according to a given distribution\;
    Solve subproblem (\ref{prob-subopt-prox}) and obtain the approximate solution $\tilde{\bB}^k$ using a given optimizer such as Adam\;
    Update the weights by $\bW^{k+1} = \bW^{k} + \bP^{k} \tilde{\bB}^k$\;
}
\end{algorithm}

\subsection{Memory Efficiency}

We now demonstrate that the proposed RSO approach offers superior memory efficiency. Unlike other memory-efficient methods (e.g., GaLore, Adam-mini, Apollo, etc.) that primarily focus on reducing the memory usage of optimizer states, the RSO method additionally achieves substantial savings in gradient and activation memory requirements.

\textbf{Memory for optimizer states.} When solving (\ref{prob-subopt-prox}), the reduced dimensionality of the subproblem significantly decreases the memory requirements for optimizer states. For instance, the memory required for both the first-order and second-order moment estimates in each subproblem is \( r_\ell n_\ell \) parameters per \(\ell\)-th layer, which is substantially lower than the \( m_\ell n_\ell \) memory overhead in the standard Adam optimizer.

\textbf{Memory for gradients.} Specifically, for the subproblems in (\ref{prob-subopt-prox}), it is sufficient to compute the gradient with respect to \( \bB \), i.e., \( \nabla_\bB F(\bW^{k} + \bP^{k} \bB; \xi) \), rather than calculating the full-dimensional gradient {\(\nabla_\bW F(\bW^k; \xi)\)} with respect to the original weight matrix \( \bW \), as outlined in the GaLore recursion in (\ref{eqa:galore-1})--(\ref{eqa:galore-2}). This results in considerable memory savings associated with the gradient computation.

\textbf{Memory for activations.} {The RSO method not only reduces memory usage for gradients but also significantly minimizes the memory required to store activations. For example, consider a neural network where the \( \ell \)-th layer is defined as follows:
\begin{align}
\mbox{(Adam)}:& \quad Z_\ell  = Y_\ell \cdot W_\ell , \hspace{14.5mm} \quad y = L(Z_\ell). \label{eqa-one-layer-Adam} \\
\mbox{(RSO)}:& \quad Z_\ell  = Y_\ell \cdot (W_\ell + P_\ell B_\ell), \quad y = L(Z_\ell). \label{eqa-one-layer-rso}
\end{align}  
Expression (\ref{eqa-one-layer-Adam}) represents the forward process of Adam, where the \( \ell \)-th layer is associated with the weight matrix \( W_\ell \), while (\ref{eqa-one-layer-rso}) corresponds to the RSO method associated with the weight matrix \( B_\ell \). Here, \( Y_\ell \in \mathbb{R}^{s_\ell \times m_\ell} \) denotes the output of the previous layer (i.e., the activation), and \( Z_\ell \) serves as the input to the next layer. The function \( L(\cdot) \), encompassing all subsequent layers and the loss function, depends only on \( Z_\ell \) and not on \( Y_\ell \). Thus, once \( Z_\ell \) is computed, \( Y_\ell \) is no longer required for calculating the loss \( y \).

In the backward-propagation process, Adam and RSO computes the weight gradient as follows: 
\begin{align}
\mbox{(Adam)}:& \quad \frac{\partial y}{\partial W_\ell} = Y_\ell^\top \frac{\partial y}{\partial Z_\ell},\label{eqa-one-layer-Adam-grad} \vspace{1mm} \\
\mbox{(RSO)}:& \quad \ \frac{\partial y}{\partial B_\ell} = (Y_\ell P_\ell)^\top \frac{\partial y}{\partial Z_\ell}. \label{eqa-one-layer-rso-grad}
\end{align}
Adam requires storing the activation \( Y_\ell \in \mathbb{R}^{s_\ell \times m_\ell} \) in (\ref{eqa-one-layer-Adam-grad}) to compute gradients with respect to \( W_\ell \). In contrast, RSO only needs to store \( Y_\ell P_\ell \in \mathbb{R}^{s_\ell \times r_\ell} \) to compute gradients with respect to \( B_\ell \). Since \( r_\ell \ll m_\ell \), this approach achieves significant memory savings. As a result, the RSO method substantially reduces the memory overhead associated with activations in layers of the form (\ref{eqa-one-layer-rso}).

We analyze the memory overhead of the proposed RSO method for a typical transformer block. Table \ref{tab:memory-analysis} summarizes the results, comparing memory usage for optimizer states and activations across various algorithms. Details  of this memory analysis is presented in Appendix \ref{app-memory}.

\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{Memory} \\  % Specify unit for clarity
    \cmidrule(lr){2-3} % Underline the Memory header
               & Optimizer States & Activations \\
    \midrule
   \textbf{RSO}        & $24nr$ & $8bsn+4bsr+2bs^2$ \\ 
    GaLore     & $24nr$ & $15bsn+2bs^2$ \\  % "-" indicates no activations stored
    LoRA       & $48nr$ & $15bsn+2bs^2$ \\ 
    Adam       & $24n^2$ & $15bsn+2bs^2$ \\ 
    \bottomrule
\end{tabular}
\vspace{0.2cm}  % Add vertical space for better layout
\caption{\small Memory analysis of different algorithms in terms of optimizer states and activations for one typical transformer block. Here, $s$, $b$, and $n$ represent the sequence length, batch size, and embedding dimension, respectively. The intermediate dimension of the feed-forward network is assumed to be $4n$.}
\label{tab:memory-analysis}
\end{table}

\subsection{Communication Efficiency}
As previously mentioned, the RSO algorithm solves smaller subproblems at each iteration, resulting in gradients with reduced dimensionality compared to methods like Adam and GaLore, which rely on full-dimensional gradients. This reduction in gradient size enables RSO to achieve improved communication efficiency.

Specifically, in a data-parallel framework such as Distributed Data Parallel (DDP), the model is replicated across multiple devices, with each device computing gradients on its local data batch. These gradients are then aggregated across devices, necessitating gradient communication. By operating with lower-dimensional gradients, the RSO method effectively reduces communication overhead compared to existing approaches.

\begin{table*}[!htbp]
    \renewcommand{\arraystretch}{1.2} % 调整行高
    \centering
    \small % 调整字体大小
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} >{\centering\arraybackslash}p{6.5cm} >{\centering\arraybackslash}p{4.8cm} >{\centering\arraybackslash}p{4.8cm} }
    \toprule\toprule
        \textbf{Subproblem Solver} & \textbf{Subproblem Complexity} & \textbf{Total Complexity} \\
    \midrule
        \multicolumn{3}{c}{\textbf{Zero-Order (ZO) Methods}} \\
    \midrule
        Stochastic ZO Method \cite{shamir2013complexity} 
        & \(O\big((\sum_{\ell=1}^\cL n_\ell r_\ell)^2 \epsilon^{-2}\big)\)  
        & \(O((\sum_{\ell=1}^\cL n_\ell r_\ell)^2 \epsilon^{-3}) \) \\
    \midrule
        \multicolumn{3}{c}{\textbf{First-Order (FO) Methods}} \\
    \midrule
        GD & \(O(\log \epsilon^{-1})\) & \(\tilde{O}(\epsilon^{-1})\)\\
        Accelerated GD \cite{nesterov2018lectures} & \(O(\log \epsilon^{-1})\) & \(\tilde{O}(\epsilon^{-1})\)   \\
        SGD \cite{bottou2018optimization} & \(O(\epsilon^{-1})\)  & \(O(\epsilon^{-2})\) \\
        Momentum SGD \cite{yuan2016influence} & \(O(\epsilon^{-1})\)  & \(O(\epsilon^{-2})\) \\
        Adam-family \cite{guo2024unifiedconvergenceanalysisadaptive} & \(O(\epsilon^{-1})\)  & \(O(\epsilon^{-2})\) \\
    \midrule
        \multicolumn{3}{c}{\textbf{Second-Order (SO) Methods}} \\
    \midrule
        Newton's method \cite{boyd2004convex} & \(O(\log (\log \epsilon^{-1}))\)  & \(\tilde{O}(\epsilon^{-1})\)  \\
        Stochastic Quasi-Newton method \cite{byrd2016stochastic} & \(O(\epsilon^{-1})\) & \(O(\epsilon^{-2})\) \\
    \bottomrule\bottomrule
    \end{tabular*}
    \caption{\small The sample complexities of the RSO method with various subproblem solvers. For the ZO solver, it refers to the number of {stochastic} function value evaluations; for the FO solver, it refers to the number of {deterministic/stochastic} gradient computations; and for the SO solver, it refers to the number of {deterministic/stochastic} Hessian or estimated Hessian computations.  $\tilde{O}(\cdot)$ hides logarithm terms.}
    \label{tab:convergence-rate}
\end{table*}

\section{Convergence Analysis}
In this section, we present the convergence guarantees for the RSO method. To account for the use of various optimizers in solving the subproblem (\ref{prob-subopt-prox}), we assume that, at each iteration \( k \), the chosen optimizer produces an expected \( \epsilon \)-inexact solution. Such an expected \( \epsilon \)-inexact solution is defined below:

\begin{definition}[Expected \(\epsilon\)-inexact solution]  
A solution \(\tilde{\bB}^k\) is said to be an expected \(\epsilon\)-inexact solution if it satisfies:  
\begin{equation}
    \EE[g^k(\tilde{\bB}^k)] - g^k(\bB^k_\star) \leq \epsilon,
\end{equation}  
where \( g^k(\bB) := f(\bW^{k} + \bP^{k} \bB) + \frac{1}{2\eta^{k}} \|\bB\|^2 \), and $\bB^k_\star$ is the optimal solution define as \(\bB^k_\star := \argmin_{\bB} g^k(\bB) \).
\end{definition}
When $\eta^k$ is properly chosen, it can be guaranteed that \(g^k(\bB)\) is a strongly convex function hence \(\bB^k_\star\) is unique.

To establish convergence guarantees for the RSO algorithm, we require the following assumptions:

\begin{assumption}\label{ass-smooth}
The objective function \(f(\bW)\) is \(L\)-smooth, i.e., it holds for any $\bW^1$ and $\bW^2$ that
\begin{align*}
\|\nabla f(\bW^1) - \nabla f(\bW^2)\| \leq L \|\bW^1 - \bW^2\|,
\end{align*}
where $\|\bW\| := \sqrt{\sum_{\ell=1}^\cL \|W_\ell\|_F^2}\ $ for any $\bW = \{W_\ell\}_{\ell=1}^\cL$.
% \( , \ \forall \ \bW^1, \bW^2\)
\end{assumption} 

\begin{assumption}\label{ass-orthogonal}
The random matrix \( \bP = \{P_\ell\}_{\ell=1}^\cL \) is sampled from a distribution such that \( P_\ell^\top P_\ell = (m_\ell/r_\ell) I_{r_\ell} \) and \( \mathbb{E}[P_\ell P_\ell^\top] = I_{m_\ell} \) for each \(\ell\). 
\end{assumption}

\begin{remark}\label{rem-orthogonal}
In practice, when \( m_\ell \gg r_\ell \), sampling each \( P_\ell \) from a normal distribution \(\mathcal{N}(0, \frac{1}{r_\ell})\) yields an approximation \( P_\ell^\top P_\ell \approx (m_\ell/r_\ell) I_{r_\ell} \). This approach provides computational efficiency. However, to rigorously satisfy Assumption \ref{ass-orthogonal}, \( P_\ell \) should be drawn from a Haar distribution or constructed as a random coordinate matrix (see \citep{kozak2023zeroth}, Examples 1 and 2 for further details).
\end{remark}

The following theorem establish the convergence rate of the RSO algorithm. Detailed proofs are provided in Appendix \ref{app-proof}.

\begin{theorem}\label{thm-rso}
Under Assumptions \ref{ass-smooth} and \ref{ass-orthogonal}, let each subproblem in (\ref{eqa-outer-step}) be solved starting from the initial point \(\bB^0 = \vzero\) to an expected \(\epsilon\)-inexact solution $\tilde \bB^k$ with suitable choice of $\eta^k$. The sequence \(\{ \bW^k \}\) generated by the RSO method satisfies the following bound:
\begin{equation}
\frac{1}{K}\sum_{k=0}^{K-1}\EE\| \nabla f(\bW^k)\|^2 \leq \frac{18\hat L\Delta_0}{K} + 18\hat L\epsilon,
\end{equation}
where \( \Delta_0 := f(\bW^0) - f^* \) and \( \hat L := \max_{\ell}\{m_\ell / r_\ell\}L \).
\end{theorem}

\textbf{Sample complexity with different optimizers.} When all subproblems are solved to expected \(\epsilon\)-inexact solutions, the RSO method achieves an \(\epsilon\)-stationary point within \(O(\epsilon^{-1})\) iterations. As each iteration requires solving the subproblem (\ref{prob-subopt-prox}), the total sample complexity of the RSO method depends on the solver employed for this subproblem. For instance, since gradient descent solves (\ref{prob-subopt-prox}) in \(O(\log \epsilon^{-1})\) inner iterations, the RSO method using gradient descent attains a total sample complexity of \(O(\epsilon^{-1} \log \epsilon^{-1})\). Table \ref{tab:convergence-rate} summarizes the sample complexities for the RSO method when equipped with various solvers, including zeroth-order, first-order, and second-order scenarios with optimizers such as gradient descent, momentum gradient descent, adaptive gradient descent, and their stochastic variants.

\textbf{Comparable complexity with vanilla Adam.} It is observed in Table~\ref{tab:convergence-rate} that RSO with Adam to solve subproblem has sample complexity ${O}(\epsilon^{-2})$, which is on the same order as vanilla Adam \citep{kingma2014adam} without subspace projection.

\begin{table*}[!htbp]\label{tab:complexities}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{60M} & \textbf{130M} & \textbf{350M} & \textbf{1B} \\ 
\midrule
Adam*  & 34.06 (0.22G)   & 25.08 (0.50G)  & 18.80 (1.37G) & 15.56 (4.99G) \\ 
\midrule
GaLore*      & 34.88 (0.14G)   & 25.36 (0.27G)  & 18.95 (0.49G) & \textbf{15.64} (1.46G) \\ 
LoRA*       & 34.99 (0.16G)   & 33.92 (0.35G)  & 25.58 (0.69G) & 19.21 (2.27G)  \\ 
ReLoRA*      & 37.04 (0.16G)   & 29.37 (0.35G)  & 29.08 (0.69G) & 18.33 (2.27G) \\ 
\textbf{RSO}         & \textbf{34.55}(0.14G)  & \textbf{25.34} (0.27G) & \textbf{18.86} (0.49G)            & 15.86 (1.46G) \\ 
\midrule
\textbf{$r / d_{model}$} & 128 / 256  & 256 / 768  & 256 / 1024 & 512 / 2048 \\ 
Training Tokens (B) & 1.1  & 2.2 & 6.4  & 13.1 \\ 
\bottomrule
\end{tabular}
\caption{\small Comparison of validation perplexity and estimated memory usage for optimizer states across different algorithms during the pre-training of LLaMA models of various sizes on the C4 dataset. The optimizer states are stored in BF16 format. Results marked with * are sourced from \citep{zhao2024galore}.}
\label{tab:llama-comparison}
\end{table*}

\begin{table*}[!htbp]
    \centering
    \renewcommand{\arraystretch}{1} % 调整行高，使表格更易读
    \setlength{\tabcolsep}{6pt} % 调整列间距，避免数据太挤
    \begin{tabular}{l c c c c c c c c | c} 
    \toprule
    & \textbf{CoLA} & \textbf{STS-B} & \textbf{MRPC} & \textbf{RTE} & \textbf{SST2} & \textbf{MNLI} & \textbf{QNLI} & \textbf{QQP} & \textbf{Avg} \\
    \midrule
    Adam  & 62.24 & 90.92 & 91.30 & 79.42 & 94.57 & 87.18 & 92.33 & 92.28 & 86.28 \\
    \midrule
    GaLore (rank=4)  & 60.35 & \textbf{90.73} & \textbf{92.25} & \textbf{79.42} & 94.04 & \textbf{87.00} & 92.24 & 91.06 & 85.89 \\
    LoRA (rank=4)  & 61.38 & 90.57 & 91.07 & 78.70  & 92.89 & 86.82 & 92.18 & \textbf{91.29} & 85.61 \\
    \textbf{RSO} (rank=4)  & \textbf{62.47} & 90.62 & \textbf{92.25} & 78.70 & \textbf{94.84} & 86.67 & \textbf{92.29} & 90.94 & \textbf{86.10} \\
    \midrule
    GaLore (rank=8)  & 60.06 & \textbf{90.82} & 92.01 & \textbf{79.78} & 94.38 & \textbf{87.17} & 92.20 & 91.11 & 85.94 \\
    LoRA (rank=8) & 61.83 & 90.80 & 91.90 & 79.06  & 93.46 & 86.94 & 92.25 & 91.22 & 85.93 \\
    \textbf{RSO} (rank=8) & \textbf{64.62} & 90.71 & \textbf{93.56} & 79.42 & \textbf{95.18} & 86.96 & \textbf{92.44} & \textbf{91.26} & \textbf{86.77} \\
    \bottomrule
    \end{tabular}
    \caption{\small Evaluation of various fine-tuning methods on the GLUE benchmark using the pre-trained RoBERTa-Base model. The average score across all tasks is provided.}
    \label{tab:glue-fine_tuning}
\end{table*}

\section{Experiments}
\label{sec-exp}
In this section, we present numerical experiments to evaluate the effectiveness of our RSO method. We assess its performance on both pre-training and fine-tuning tasks across models of varying scales. In all tests, the RSO method uses the Adam optimizer to solve the subproblems. Additionally, we compare the memory usage and time cost of our RSO method with existing approaches to highlight its advantages in memory and communication efficiency.

\subsection{Pre-training with RSO}

\textbf{Experimental setup.} We evaluate the performance of our RSO method on LLaMA models with sizes ranging from 60M to 7B parameters. The experiments are conducted using the C4 dataset, a large-scale, cleaned version of Common Crawl's web corpus, which is primarily intended for pre-training language models and word representations \citep{raffel2020exploring}. We compare our method against LoRA \citep{hu2021lora}, GaLore \citep{zhao2024galore}, ReLoRA \citep{lialin2023relora}, and Adam \citep{kingma2014adam} as baseline methods. We adopt the same configurations as those reported in \citep{zhao2024galore}, and the detailed settings for pre-training are provided in Appendix \ref{app-setting-pretrain}.

\textbf{Main results.} As shown in Table \ref{tab:llama-comparison}, under the same rank constraints, RSO outperforms other memory-efficient methods in most cases. We also report the estimated memory overhead associated with optimizer states. From Table \ref{tab:llama-comparison}, we observe that for LLaMA-350M, RSO achieves nearly the same performance as Adam while reducing the memory required for optimizer states by 64.2$\%$. For LLaMA-1B, this reduction increases to 70.7$\%$. The results for LLaMA-7B are provided in Appendix \ref{app-pretrain-llama7b}.

\subsection{Fine-tuning with RSO}  
\textbf{Experimental setup.}  
We extend the application of our RSO algorithm to fine-tuning tasks. Specifically, we fine-tune pre-trained RoBERTa models \citep{liu2019roberta} on the GLUE benchmark \citep{wang2018glue}, which encompasses a diverse range of tasks, including question answering, sentiment analysis, and semantic textual similarity. Detailed settings can be found in Appendix \ref{app-finetuning}.

\textbf{Main results.}  
As shown in Table \ref{tab:glue-fine_tuning}, our RSO method surpasses other memory-efficient approaches and delivers performance comparable to Adam across most datasets in the GLUE benchmark. Notably, RSO significantly outperforms Adam on the CoLA and MRPC datasets when the rank is set to 8. Additional fine-tuning experiments on LLaMA and OPT models are provided in Appendix \ref{app-finetune-llamaopt}.

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=0.3\linewidth]{figures/LLaMA-350M-Memory.pdf}
    \hspace{0.5cm}
    \includegraphics[width=0.3\linewidth]{figures/LLaMA-1B-Memory.pdf}
    \hspace{0.5cm}
    \includegraphics[width=0.3\linewidth]{figures/LLaMA-7B-Memory.pdf}
    \caption{\small Comparison of peak memory usage (in GB) per device for RSO and GaLore during LLaMA training with varying ranks. All hyperparameters, except rank, are consistent with \citep{zhao2024galore}. Adam's memory usage is reported for LLaMA-350M and LLaMA-1B but excluded for LLaMA-7B due to an out-of-memory (OOM) error.}
    \label{fig:memory}
\end{figure*}

\subsection{Memory and Communication Efficiency}

To evaluate the memory and communication efficiency of our proposed method, we measure the peak memory usage and iteration time during the pre-training of LLaMA models of various sizes.

\textbf{RSO method requires less memory overhead.} Figure \ref{fig:memory} illustrates the actual memory usage during the training of LLaMA models. As shown, the RSO method incurs significantly lower memory overhead compared to GaLore and Adam. This reduction is attributed to RSO's ability to save memory for activations and use low-dimensional gradients. For instance, in the case of LLaMA-7B with a rank of 64, the RSO method achieves over a 40\% reduction in memory overhead compared to GaLore.

Additionally, in Figure \ref{fig:memory}, the memory gap between RSO and GaLore widens as the rank decreases. This is because, as indicated in Table \ref{tab:memory-analysis}, the RSO method further reduces memory consumption for activations with lower ranks, whereas GaLore does not benefit in this regard. For LLaMA-350M and LLaMA-1B, GaLore's memory usage is observed to be comparable to that of Adam, as activation memory dominates in these cases. However, RSO still achieves superior memory efficiency due to its reduced activation cost.

\begin{table*}[!htbp]
    \centering
    % \renewcommand{\arraystretch}{1.3} % 增加行高
    \setlength{\tabcolsep}{8pt} % 适当减少列间距
    \begin{tabular}{l ccc c ccc}
    \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{LLaMA-1B (Seconds)}} & \phantom{-} & \multicolumn{3}{c}{\textbf{LLaMA-7B (Seconds)}} \\
        \cmidrule{2-4} \cmidrule{6-8} 
        & \textbf{Seq 64} & \textbf{Seq 128} & \textbf{Seq 256} & & \textbf{Seq 64} & \textbf{Seq 128} & \textbf{Seq 256} \\
    \midrule
        RSO    & 0.94  & 1.70  & 3.29  & & 2.40  & 2.94  & 4.60  \\
        GaLore & 1.12  & 1.84  & 3.35  & & 7.86  & 8.26  & 9.12  \\
        Adam   & 1.11  & 1.81  & 3.32  & & 7.84  & 8.23  & OOM  \\
    \bottomrule
    \end{tabular}
    \caption{\small Comparison of iteration time (in seconds) for different methods in LLaMA training across various sequence lengths. All hyperparameters, except sequence length, follow \citep{zhao2024galore}. LLaMA-1B runs on $4\times$ A800 GPUs, while LLaMA-7B uses $8\times$ A800 GPUs. SVD decomposition time in GaLore is excluded. Additionally, for LLaMA-7B with a sequence length of 256, the Adam optimizer encounters an out-of-memory (OOM) error.}
    \label{tab:time}
\end{table*}

\textbf{RSO method requires less time per iteration.} Table \ref{tab:time} presents a comparison of the time required for one iteration across different methods when training LLaMA models. As shown, the RSO method requires significantly less time compared to GaLore or Adam due to its improved communication efficiency, achieved by reducing the dimensionality of gradients. For example, when training LLaMA-7B with a sequence length of 64, the time required by RSO is only one-third that of GaLore or Adam. Notably, while GaLore involves SVD decomposition (which is excluded from this measurement), RSO demonstrates even greater efficiency in training time.

As shown in Table \ref{tab:time}, the difference in iteration time between RSO and other approaches becomes more pronounced as model size increases or sequence length decreases. This phenomenon can be attributed to the communication overhead, which primarily stems from the synchronization of gradients across devices. Such overhead is more closely tied to model size while being less affected by sequence length. In contrast, the computational overhead is highly sensitive to sequence length. Therefore, RSO exhibits a more substantial advantage when the sequence length is considerably smaller than the model size, as communication overhead constitutes a larger fraction of the total iteration time under these conditions.

\section{Conclusion}

We propose a Randomized Subspace Optimization (RSO) method, aiming for Large Language Model (LLM) pre-training and fine-tuning. By decomposing the origanal training problem into small subproblems, our method achieves both memory and communication efficiency, while reach same level performance compared with GaLore and Adam.

We outline two directions for future work on the RSO method. First, further reduction of memory overhead for activations could be explored. While part of the activation memory has already been reduced, the remaining portion might be further optimized through alternative strategies for partitioning the original problem. Second, it is worth investigating the performance of various methods for solving the subproblems. Given the low-dimensional nature of the subproblems, exploring the application of second-order methods could be particularly promising.

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix

\section{Memory Complexity Analysis}\label{app-memory}
In this section, we analyze the memory overhead of our proposed RSO algorithm for one typical transformer block.

\subsection{Transformer Structure}
Transformers \cite{vaswani2017attention} have become a foundational component of LLMs. Here, we focus on the forward and backward propagation processes within a single transformer block using our RSO algorithm.

\textbf{Forward Propagation.} Consider the input \( X \in \mathbb{R}^{s \times n} \) to a transformer block, where \( s \) is the sequence length and \( n \) is the embedding dimension. The attention mechanism within the transformer block performs the following linear operations:
\begin{equation}\label{eqa-forward-qkv}
    Q = X(W_q + P_q B_q), \quad K = X(W_k + P_k B_k), \quad V = X(W_v + P_v B_v),
\end{equation}
where \( W_q, W_k, W_v \in \mathbb{R}^{n \times n} \) are the original weight matrices, \( P_q, P_k, P_v \in \mathbb{R}^{n \times r} \) are the projection matrices, and \( B_q, B_k, B_v \in \mathbb{R}^{r \times n} \) are the low-rank weight matrices used in the RSO method for each subproblem. These intermediate values are then combined as follows:
\begin{equation}\label{eqa-forward-attention}
    \tilde A_s = QK^\top, \quad A_s = \sigma_s\bigg(\frac{\tilde A_s}{\sqrt{n}}\bigg), \quad A_h = A_s V, \quad A_o = A_h (W_o + P_o B_o),
\end{equation}
where \( \sigma_s \) represents the softmax activation function, and \( W_o \in \mathbb{R}^{n \times n} \) is the output projection matrix.

Next, the feed-forward network consists of two fully-connected layers, which are computed as:
\begin{equation}\label{eqa-forward-ffn}
       \tilde Z_1 = A_o (W_1 + P_1 B_1), \quad Z_1 = \sigma(\tilde Z_1), \quad Z_2 = Z_1 (W_2 + P_2 B_2), 
\end{equation}
where \( W_1 \in \mathbb{R}^{n \times 4n} \) and \( W_2 \in \mathbb{R}^{4n \times n} \) are the weights of the feed-forward layers. We assume that the intermediate dimension of the feed-forward network is four times the embedding dimension. Similarly, \( P_1 \in \mathbb{R}^{n \times r}, P_2 \in \mathbb{R}^{4n \times r} \) are the projection matrices, and \( B_1 \in \mathbb{R}^{r \times 4n}, B_2 \in \mathbb{R}^{r \times n} \) are the low-rank trainable parameters for RSO. The function \( \sigma \) represents the activation function.

For the Adam and GaLore methods, there are no \( P \) and \( B \) matrices, as they directly work with the original weight matrices. In the case of the LoRA method, each projection matrix \( P \) is replaced by a trainable parameter \( A \).

\textbf{Backward Propagation.} To calculate the gradients of all the weight matrices, the backward propagation begins with the partial gradient of the loss function \( F \) with respect to the output of this block, denoted as \(\mathcal{D} Z_2 := \frac{\partial F}{\partial Z_2}\). Here, we use \(\mathcal{D}\) to represent the derivative of \( F \) with respect to any matrix. Note that in our RSO algorithm, we compute the gradient with respect to \( B \), the low-rank trainable parameters, instead of the original weight matrix \( W \). The gradients for the weights in the feed-forward network are computed as follows:
\begin{equation}\label{eqa-backward-ffn}
    \mathcal{D} B_2 = (Z_1P_2)^\top \mathcal{D} Z_2, \
    \mathcal{D} \tilde{Z}_1 = \mathcal{D} Z_2 (W_2 + P_2 B_2)^\top \odot \sigma'(\tilde{Z}_1), \
    \mathcal{D} B_1 = (A_oP_1)^\top \mathcal{D} \tilde{Z}_1, \
    \mathcal{D} A_o = \mathcal{D} \tilde{Z}_1 (W_1 + P_1 B_1)^\top.
\end{equation}

For the attention mechanism, the gradients for the corresponding matrices are calculated as:
\begin{equation}\label{eqa-backward-attention}
\begin{aligned}
    \mathcal{D} B_o = (A_h P_o)^\top \mathcal{D} A_o, \quad 
    \mathcal{D} A_h = \mathcal{D} A_o (W_o + P_o B_o)^\top, \quad
    \mathcal{D} A_s = \mathcal{D} A_h V^\top.
\end{aligned}
\end{equation}

To compute the gradients for the matrices \( Q, K, V \), the following equations are used:
\begin{equation}\label{eqa-backward-qkvmat}
    \mathcal{D} V = A_s^\top \mathcal{D} A_h, \quad 
    \mathcal{D} Q = \left[\mathcal{D} A_s \odot \frac{1}{\sqrt{n}}\sigma_s'\left(\frac{\tilde{A}_s}{\sqrt{n}}\right)\right] K, \quad 
    \mathcal{D} K = \left[\mathcal{D} A_s \odot \frac{1}{\sqrt{n}}\sigma_s'\left(\frac{\tilde{A}_s}{\sqrt{n}}\right)\right]^\top Q.
\end{equation}

The gradients for the low-rank weight matrices \( B_q, B_k, B_v \) are computed as follows:
\begin{equation}\label{eqa-backward-qkv}
    \mathcal{D} B_v = (X P_v)^\top \mathcal{D} V, \quad 
    \mathcal{D} B_q = (X P_q)^\top \mathcal{D} Q, \quad 
    \mathcal{D} B_k = (X P_k)^\top \mathcal{D} K.
\end{equation}

Finally, to ensure that the backward propagation process can continue, the derivative with respect to the input \( X \) must also be calculated. This is given by:
\begin{equation}\label{eqa-backward-x}
    \mathcal{D} X = \mathcal{D} Q (W_q + P_q B_q)^\top + \mathcal{D} K (W_k + P_k B_k)^\top + \mathcal{D} V (W_v + P_v B_v)^\top.
\end{equation}

When using the Adam or GaLore algorithms, the derivatives must be computed with respect to the original weight matrix \( W \) instead of the low-rank matrix \( B \). As a result, all occurrences of \( \mathcal{D} B \) need to be replaced with \( \mathcal{D} W \). For example, the derivatives with respect to \( W_q, W_k, \) and \( W_v \) are computed as follows:
\[
    \mathcal{D} W_v = X^\top \mathcal{D} V, \quad 
    \mathcal{D} W_q = X^\top \mathcal{D} Q, \quad 
    \mathcal{D} W_k = X^\top \mathcal{D} K.
\]

\subsection{Memory for Optimizer States Analysis}
For Adam algorithm, the trainable parameters include \( W_q, W_k, W_v, W_o, W_1, W_2 \). It is straightforward to compute the total number of parameters as \( 12n^2 \). Consequently, the optimizer states, considering both the first-order and second-order moments, require \( 24n^2 \) storage.

For RSO method, the trainable weights for each subproblem are \( B_q, B_k, B_v, B_o, B_1, B_2 \), with a total of \( 12nr \) parameters per subproblem, leading to an optimizer state storage requirement of \( 24nr \). LoRA trains an additional matrix \( A \) (corresponding to the matrix \( P \) used above), resulting in twice the optimizer state memory required compared to RSO. GaLore projects each gradient matrix from \( \mathbb{R}^{n \times n} \) to \( \mathbb{R}^{r \times n} \), resulting in the same optimizer state memory requirement of \( 24nr \).

\subsection{Memory for Activations Analysis}

From the backward propagation process, it is evident that the activations generated during forward propagation are required. Specifically, in (\ref{eqa-forward-qkv}), the matrices \( X, Q, K, V \) need to be stored, resulting in the following memory requirement: \( M_1 = 4sn \).

However, in our RSO algorithm, \( X \) is only needed to compute \( \cD B_q, \cD B_k, \cD B_v \), where only the projections \( XP_v, XP_q, XP_k \) are required. By setting \( P_q = P_k = P_v = P \), we only need to store \( XP \), reducing the memory requirement to \( \tilde{M}_1 = 3sn + sr \).

Additionally, in (\ref{eqa-forward-attention}), the matrices \( \tilde{A}_s, A_s, A_h, A_o \) must be stored, requiring \( M_2 = 2s^2 + 2sn \). It is worth noting that \( \frac{\tilde{A}_s}{\sqrt{n}} \) does not need to be stored, as \( A_s \) can be used to recover it due to the properties of the softmax function. For the RSO algorithm, storing \( A_h \) and \( A_o \) is unnecessary, as \( A_h P_o \) and \( A_o P_1 \) suffice. Consequently, the memory requirement is reduced to \( \tilde{M}_2 = 2s^2 + 2sr \).

For the feed-forward network in (\ref{eqa-forward-ffn}), the matrices \( \tilde{Z}_1, Z_1, Z_2 \) need to be stored, resulting in a memory requirement of \( M_3 = 9sn \). In the RSO algorithm, \( Z_1 \) can be replaced with \( Z_1 P_2 \), reducing the memory requirement to \( \tilde{M}_3 = 5sn + sr \).

Combining these results, the total memory cost for activations in the RSO algorithm is 
\[ \tilde{M}_{\text{total}} = 8sn + 2s^2 + 4sr, \] 
compared to the memory cost in Adam or GaLore: 
\[ M_{\text{total}} = 15sn + 2s^2. \]
As the LoRA method trains parameters \( A \) (corresponding to \( P \) in our method), it requires the same activations as Adam, resulting in the same memory overhead as Adam or GaLore.

\section{Convergence Analysis}\label{app-proof}
In this section, we present the convergence analysis of the RSO algorithm and provide a detailed proof of Theorem~\ref{thm-rso}. 

Under Assumption \ref{ass-orthogonal}, the following properties hold and can be straightforwardly derived:  
\begin{align*}
    \|\bP \bW\| &= \sqrt{\sum\limits_{\ell}\mathrm{tr}(W_\ell^\top P_\ell^T P_\ell W_\ell)} \leq \sqrt{\max\limits_\ell\{m_\ell/r_\ell\}} \|\bW\|,
\end{align*}  
where \( \bW = \{W_\ell\}_{\ell=1}^\cL \) denotes any family of matrices with \( W_\ell \in \mathbb{R}^{m_\ell \times r_\ell} \). For simplicity, we will not explicitly reference these properties when they are used.

\begin{lemma} Under Assumptions \ref{ass-smooth} and \ref{ass-orthogonal},  $g^k(\bB)$ is \((\frac{1}{\eta^k} - \hat{L} )\)-strongly convex and \((\frac{1}{\eta^k}+ \hat{L})\)-smooth, with \(0 < \eta < 1/\hat{L} \) and \(\hat{L}=\max\limits_{\ell} \{m_\ell/r_\ell\}L\).
\end{lemma}
\begin{proof}
 We denote the gradient of \( f \) with respect to the parameters in the \(\ell\)-th layer by \(\nabla_\ell f(\bW)\). Let \( h^k(\bB) := f(\bW^k + \bP^k \bB) \). It can be shown that \( h^k \) is \(\hat{L}\)-Lipschitz smooth, as follows:
\begin{align*}    
    \|\nabla h^k(\bB^1) - \nabla h^k(\bB^2) \|^2 & = \sum_{\ell=1}^\cL \left\|\left(\frac{\partial f }{\partial B^1_l}(\bW^k + \bP^k \bB^1_\ell) - \frac{\partial f}{\partial B_\ell^2}(\bW^k+\bP^k\bB^2) \right)  \right \|_F^2 \\
    &  = \sum_{\ell=1}^\cL \left \| (P_\ell^k)^\top ( \nabla_\ell f(\bW^k + \bP^k \bB^1) - \nabla_\ell f(\bW^k + \bP^k \bB^2))  \right\|_F^2 \\ 
    & \leq \sum_{\ell=1}^\cL \| P_\ell^k\|^2_F \|\nabla_\ell f(\bW^k + \bP^k \bB^1) - \nabla_\ell f(\bW^k + \bP^k \bB^2)) \|_F^2\\
    & \leq \max\limits_\ell\left(\frac{m_\ell}{r_\ell}\right)\ \sum_{\ell=1}^\cL \|\nabla_\ell f(\bW^k + \bP^k \bB^1) - \nabla_\ell f(\bW^k + \bP^k \bB^2)) \|_F^2\\
    & = \max\limits_\ell\left(\frac{m_\ell}{r_\ell}\right)\|\nabla f(\bW^k + \bP^k \bB^1) - \nabla f(\bW^k + \bP^k \bB^2)) \|^2\\
    & \leq L^2\max\limits_\ell\left(\frac{m_\ell}{r_\ell}\right)\ \|\bP^k\bB^1 - \bP^k\bB^2\|^2 \leq \hat L^2\|\bB^1-\bB^2\|^2.
\end{align*}
As \(h^k\) is $\hat L$-Lipschitz smooth, we can conclude that \(g^k(\bB) = h^k(\bB) + \frac{1}{2\eta^k}\|\bB\|^2\) is \((\frac{1}{\eta^k}+ \hat{L})\)-smooth. Furthermore, 
we have the inequality
\[
| h^k(\bB^2) - h^k(\bB^1) - \langle \nabla h^k(\bB^1),\bB^2-\bB^1 \rangle | \leq \frac{\hat{L}}{2}\|\bB^1 - \bB^2\|^2.
\]
Based on this inequality, with the definition of \(g^k\), we have
\begin{align*}
    g^k(\bB^2) & \geq g^k(\bB^1)  - \frac{1}{2\eta^k}\|\bB^1\|^2 +  \frac{1}{2\eta^k}\| \bB^2\|^2 + \langle \nabla h^k(\bB^1),\bB^2-\bB^1 \rangle - \frac{\hat{L}}{2}\|\bB^1 - \bB^2\|^2 \\
    & = g^k(\bB_1) + \left\langle \nabla h^k(\bB_1)+ \frac{1}{\eta^k}\bB_1,\bB_2 - \bB_1 \right\rangle + \left(  \frac{1}{2\eta^k} - \frac{\hat{L}}{2} \right)\|\bB_1 - \bB_2\|^2 \\
    & = g^k(\bB_1) + \langle \nabla g^k(\bB_1), \bB_2 - \bB_1\rangle + \left(  \frac{1}{2\eta^k} - \frac{\hat{L}}{2} \right)\|\bB_1 - \bB_2\|^2,
\end{align*}
which shows that \(g^k(\bB)\) is \((\frac{1}{\eta^k} - \hat{L})\)-strongly convex when \(0 < \eta < 1/\hat{L}\).
\end{proof}

\begin{theorem}[Theorem \ref{thm-rso}]Under Assumptions \ref{ass-smooth} and \ref{ass-orthogonal}, let the subproblem (\ref{eqa-outer-step}) is solved from the initial point \(\bB^0 = \vzero\) to an expected \(\epsilon\)-inexact solution $\tilde \bB^k$ with \(\eta^k = \frac{1}{2\hat{L}}\), the sequence \(\{ \bW^k \}\) generated by the RSO  method satisfies
\begin{equation}
    \frac{1}{K}\sum_{k=0}^{K-1}\EE[\| \nabla f(\bW^k)\|^2] \leq \frac{18\hat{L} (f(\bW^0) - f^\star)}{ K} + 18\hat{L} \epsilon.
\end{equation}
\end{theorem}

\begin{proof}
For simplicity, we denote \(\mu := \frac{1}{\eta^k} - \hat{L} \). As $g^k(\bB)$ is $\mu-$strongly convex, \(\forall\ \bB\), we have
\[
g^k(\bB^k_\star) \leq g^k(\bB) - \frac{\mu}{2}\|\bB^k_\star - \bB \|^2.
\]
Let \(\bB = \vzero\) and using the definition of \(\tilde{\bB}^k\), we can obtain a descent condition as
\[
\EE[g^k(\tilde{\bB}^k)] \leq g^k(\vzero) - \frac{\mu}{2}\|\bB^k_\star\|^2 + \epsilon.
\]
Taking the expectation with respect to the initial condition \(\bW^k\) and random matrix \(\bP^k\), by the tower rule, it can be derived
\[
\EE[g^k(\tilde{\bB}^k)] \leq \EE[g^k(\vzero)] - \frac{\mu}{2}\EE[\|\bB^k_\star\|^2] + \epsilon.
\]
Telescoping the above inequality from $k=0$ to $K-1$, we have
\[
\sum_{k=0}^{K-1}\frac{\mu}{2}\EE[\|\bB^k_\star \|^2] \leq \sum_{k=0}^{K-1}\EE[g^k(\vzero)] - \sum_{k=0}^{K-1} \EE[g^k (\tilde{\bB}^k)] + \epsilon.
\]
Notice that, by the update rule of (\ref{eqa-outer-step}), \(g^{k+1}(\vzero) =f(\bW^{k+1}) = f(\bW^k +\bP^k \tilde{\bB}^k) = g^k(\tilde{\bB}^k) - \frac{1}{2\eta^k}\| \tilde{\bB}^k\|^2\), the terms in the RHS of the above inequality can be canceled with each other and resulting in the following inequality: 
\[
\sum_{k=0}^{K-1}\frac{\mu}{2}\EE[\|\bB^k_\star\|^2] \leq g^0(\vzero) - \EE[g^{K-1}(\tilde{\bB}^{K-1})] - \sum_{k=0}^{K-2} \frac{1}{2\eta^k}\EE[\|\tilde{\bB}^k \|^2] + \epsilon.
\]
Dividing $K$ on both sides and using the fact 
\(g^{K-1}(\tilde{\bB}^{K-1}) \geq f(\bW^{K-1}+\bP^{K-1}\tilde{\bB}^{K-1}) \geq f^\star \), we can derive
\[
\frac{1}{K}\sum_{k=0}^{K-1}\frac{\mu}{2}\EE[\|\bB^k_\star\|^2] + \frac{1}{K}\sum_{k=0}^{K-2} \frac{1}{2\eta^k}\EE[\|\tilde{\bB}^k \|^2] \leq \frac{g^0(\vzero) - f^\star}{K}  + \epsilon.
\]
Next, we need to establish the connection between \(\|\bB^k_\star\|^2\) and the final stationary measure \(\|\nabla f(\bW^k)\|^2\). As $g^k(\bB)$ is \((1/\eta^k+ \hat{L})\)-smooth, \(\forall \bB\) it holds that
\[
\left(\hat{L}+ \frac{1}{\eta^k}\right)^2\| \bB - \bB^k_\star \|^2 \geq \| \nabla g^k(\bB)  - \nabla g^k(\bB^k_\star)\|^2 = \| \nabla g^k(\bB) \|^2.
\]
With \(\bB = \vzero\), it holds 
\begin{equation}\label{eq:bound-of-B}
 \frac{1}{K} \sum_{k=0}^{K-1} \frac{\mu}{2} \left(\hat{L}+ \frac{1}{\eta^k}\right)^{-2} \EE[\|  \nabla g^k(\vzero)\|^2] \leq \frac{f(\bW_0)- f^\star}{K}  + \epsilon.
\end{equation}
Furthermore, notice that 
\(\nabla g^k(\vzero) = (\bP^k)^\top \nabla f(\bW^{k}) \) and the random matrix \(\bP^k\) is sampled independently to \(\bW^k\), for any fixed \(\bW^k\), we have
\begin{align*}
    \EE_{\bP^k}[\| \nabla g^k(\vzero)\|^2] & = \sum\limits_\ell\EE_{P_\ell^k}[\Tr(\nabla_\ell f(\bW^k)^\top P_\ell^k (P_\ell^k)^\top \nabla_\ell f(\bW^k))] \\
    & = \sum\limits_\ell\EE_{P_\ell^k}[\Tr( P_\ell^k (P_\ell^k)^\top \nabla_\ell f(\bW^k)\nabla_\ell f(\bW^k)^\top )] \\
    & = \sum\limits_\ell\Tr(\EE_{P_\ell^k}[ P_\ell^k (P_\ell^k)^\top] \nabla_\ell f(\bW^k)\nabla_\ell f(\bW^k)^\top ) \\
    & = \sum\limits_\ell \| \nabla_\ell f(\bW^k)\|_F^2 = \| \nabla f(\bW^k)\|^2.
\end{align*}
Taking expectation with the respect to the randomness in \(\bW^k\), we can claim \(\EE[\| \nabla g^k(\vzero)\|^2 = \EE[\|\nabla f(\bW^k)\|^2]\). Inserting this result back to (\ref{eq:bound-of-B}) with \(\eta^k = \frac{1}{2\hat{L}}\), it can be written as 
\[
\frac{1}{K}\sum_{k=0}^{K-1}\EE[\| \nabla f(\bW^k)\|^2] \leq \frac{18\hat{L} (f(\bW^0) - f^\star)}{ K} +18\hat{L} \epsilon.
\]
Thus we complete the proof.
\end{proof}

\begin{table}[!htbp]
\centering
\begin{tabular}{l|c|c|c}
\toprule
Method  & Memory (GB)  &  Training Time  (h) &  Perplexity \\
\midrule
Adam  & 78.92  & 216 &15.43 \\
GaLore & 75.33  & 134 &15.59 \\
\textbf{RSO} & 54.81  & 64 & 15.99 \\
% \midrule
% Tokens (B) & & & 6.5 \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{\small Comparison of various pre-training methods for the LLaMA-7B model on the C4 dataset. Perplexity is reported at 50K steps. The training is conducted on \( 8\times \) A800 GPUs. The actual memory cost per device and the total training time are also reported. RSO and GaLore are configured with a batch size of 16, while Adam uses a batch size of 8.}
\label{tab:pretrain-llama-7b}
\end{table}

\section{More Experimental Results}
\subsection{Pre-training on LLaMA-7B Model}\label{app-pretrain-llama7b}
Table \ref{tab:pretrain-llama-7b} compares the performance of our RSO method with GaLore and Adam on the LLaMA-7B model, where evaluations are conducted for 50K steps due to limited computational resources. We also report the memory overhead and total training time for each method. As shown in the table, RSO exhibits performance comparable to that of GaLore and Adam. Notably, RSO requires less than half the training time of the other methods.

\subsection{Fine-tuning on LLaMA and OPT Models}\label{app-finetune-llamaopt}

Table \ref{tab:finetune-wincopa} compares RSO with other fine-tuning methods on LLaMA and OPT models across two datasets. As shown in the table, RSO outperforms other memory-efficient methods in terms of accuracy on most tasks.

\begin{table}[!htbp]
\centering
\begin{tabular}{l cccc cccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{WinoGrande}} & \multicolumn{4}{c}{\textbf{Copa}} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
 & Adam & LoRA & GaLore & \textbf{RSO} & Adam & LoRA & GaLore & \textbf{RSO} \\
\midrule
LLaMA-7B    & 64.4 & 70.9 & 70.9 & \textbf{71.0} & 84.0    & 84.0    & 85.0    & \textbf{86.0}    \\
LLaMA-13B   & 73.3 & \textbf{76.6} & 74.6 & 74.7 & 90.0    & \textbf{92.0}    & \textbf{92.0}    & \textbf{92.0}    \\
\midrule
OPT-1.3B    & 60.4    & 57.3    & 58.3    & \textbf{58.9}    & 76.0 & 73.0 & 72.0 & \textbf{74.0} \\
OPT-6.7B    & 62.2    & 64.7    & 66.8    & \textbf{69.2}    & 78.0 & 80.0 & 80.0 & \textbf{82.0} \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{\small Comparison of various methods for fine-tuning LLaMA and OPT models on the WinoGrande and COPA datasets. The test accuracy for each method is reported.}
\label{tab:finetune-wincopa}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{l c c c c c c c}
\toprule
\textbf{Params} & \textbf{Hidden} & \textbf{Intermediate} & \textbf{Heads} & \textbf{Layers} & \textbf{Steps} & \textbf{Data amount} \\
\midrule
60M   & 512   & 1376  & 8  & 8  & 10K   & 1.3 B \\
130M  & 768   & 2048  & 12 & 12 & 20K   & 2.6 B \\
350M  & 1024  & 2736  & 16 & 24 & 60K   & 7.8 B \\
1 B   & 2048  & 5461  & 24 & 32 & 100K  & 13.1 B \\
7 B   & 4096  & 11008 & 32 & 32 & 150K  & 19.7 B \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{\small Hyperparameter configurations for LLaMA models of different scales, along with the corresponding number of training steps. Due to limited computational resources, only the first 50K steps are completed for LLaMA-7B.}
\label{tab-pretrain-setup-llama}
\end{table}

\section{Experimental Details}

\subsection{Pre-training Experimental Setup}\label{app-setting-pretrain}
For the pre-training of the LLaMA models across all sizes, we employ a configuration aligned with \citep{zhao2024galore}. Specifically, the maximum sequence length is set to 256, and the total training batch size is set to 512, corresponding to 131K tokens per batch. A learning rate warm-up is applied during the first $10\%$ of the training steps, followed by a cosine annealing schedule to reduce the learning rate to $10\%$ of its initial value. The model configuration and training step details are summarized in Table \ref{tab-pretrain-setup-llama}. 

For the RSO method, we use a learning rate and subspace update interval that closely match those of GaLore. Similar to GaLore, a learning rate scaling factor is applied to the weights optimized by the RSO method, which is set to $0.35$.
% The detailed hyperparameter settings are presented in \textcolor{red}{Table}.

\begin{table}[!htbp]
\centering
\begin{tabular}{lcccccccc}
\toprule
 & MNLI & SST-2 & MRPC & CoLA & QNLI & QQP & RTE & STS-B \\
\midrule
Batch Size & 16 & 16 & 16 & 32 & 16 & 16 & 16 & 16 \\
Epochs & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 \\
Learning Rate & 1E-05 & 3E-05 & 3E-05 & 3E-05 & 1E-05 & 1E-05 & 1E-05 & 1E-05 \\
Rank & \multicolumn{8}{c}{4} \\
Scaling Factor & \multicolumn{8}{c}{\{8, 16, 32\}} \\
RSO interval & \multicolumn{8}{c}{\{300, 500\}} \\
Max Seq Length & \multicolumn{8}{c}{512} \\
\bottomrule
\toprule
 & MNLI & SST-2 & MRPC & CoLA & QNLI & QQP & RTE & STS-B \\
\midrule
Batch Size & 16 & 16 & 16 & 32 & 16 & 16 & 16 & 16 \\
 Epochs & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 \\
Learning Rate & 1E-05 & 2E-05 & 2E-05 & 1E-05 & 1E-05 & 2E-05 & 2E-05 & 3E-05 \\
Rank & \multicolumn{8}{c}{8} \\
Scaling Factor & \multicolumn{8}{c}{\{8, 16, 32\}} \\
RSO interval & \multicolumn{8}{c}{\{300, 500\}} \\
Max Seq Length & \multicolumn{8}{c}{512} \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{\small Hyperparameter settings for fine-tuning RoBERTa-Base model on the GLUE benchmark using the RSO method.}
\label{app-glue-hyper}
\end{table}

\begin{table}[!htpb]
\centering
\begin{tabular}{ccc}
\toprule
\textbf{Experiment} & \textbf{Hyperparameters} & \textbf{Values} \\
\midrule
\multirow{3}{*}{FT} & Batch size & 16 \\
    & Learning rate & \{1E-07, 1E-06, 1E-05\} \\
    & Weight Decay & 0 \\
\midrule
\multirow{4}{*}{LoRA} & Batch size & 16 \\
    & Learning rate & \{1E-07, 1E-06, 5E-06\} \\
    & Rank & 8 \\
    & Weight Decay & 0 \\
\midrule
\multirow{5}{*}{GaLore} & Batch size & 16 \\
    & Learning rate & \{1E-07, 1E-06, 5E-06\} \\
    & Rank & 8 \\
    & Update interval & \{300, 500\} \\
    & $\alpha$ & \{4, 8\} \\
    & Weight Decay & 0 \\
\midrule
\multirow{6}{*}{RSO} & Batch size & 16 \\
    & Learning rate & \{1E-07, 1E-06, 5E-06\} \\
    & Rank & 8 \\
    & Update interval & \{300, 500\} \\
    & Scaling Factor & \{4, 8\} \\
    & Weight Decay & 0 \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{\small Hyperparameter settings for fine-tuning LLaMA and OPT models on the WinoGrande and COPA datasets.}
\label{ft-copa-wg}
\end{table}

\subsection{Fine-tuning Experimental Setup}\label{app-finetuning}

\textbf{Details of fine-tuning on GLUE benchmark.} For fine-tuning the pre-trained RoBERTa-Base model on the GLUE benchmark, the model is trained for 30 epochs with a batch size of 16 for all tasks, except for CoLA, which uses a batch size of 32. Similar to GaLore, a scaling factor is applied to the learning rate for the weights which applies the RSO method. The detailed hyperparameter settings are provided in Table \ref{app-glue-hyper}.

\textbf{Details of fine-tuning on WinoGrande and Copa datasets.}
For fine-tuning LLaMA and OPT models on the WinoGrande and Copa datasets, we randomly sample 1,000 examples from each dataset for training, 500 examples for validation, and 1,000 examples for testing. All experiments are conducted over 1,000 steps. The detailed hyperparameter settings are provided in Table \ref{ft-copa-wg}.

\end{document}