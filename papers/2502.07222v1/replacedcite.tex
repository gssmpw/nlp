\section{Related Works}
% \subsection{Algorithm-based Memory-efficient Methods}

\textbf{Parameter-efficient methods.} 
% A direct approach to memory savings is to reduce the number of trainable parameters. This can be achieved by designing smaller models ____, or by training only a low-rank adaptation of the original models ____.
A promising approach to memory-efficient training involves parameter-efficient methods, which reduce the number of trainable parameters and consequently lower the memory required for storing optimizer states. For example, ____ propose Low-Rank Adaptation (LoRA), which restricts trainable parameters to a low-rank subspace for each weight matrix. Similarly, ____ incorporate sparsity by training only a subset of weights. While these methods effectively reduce memory consumption, the reduction in trainable parameters can sometimes lead to suboptimal model performance ____. To address this limitation, recent advancements suggest using multiple LoRA updates to enable high-rank weight updates ____. However, in pre-training settings, this approach still relies on a full-rank weight training phase as a warm-up before transitioning to low-rank training ____, thereby limiting its memory efficiency.

\textbf{{Optimizer-efficient methods.}} An alternative approach to memory savings focuses on compressing optimizer states while maintaining the number of trainable parameters. GaLore ____ achieves this by compressing the gradient matrix through a projection onto a subspace and leveraging the compressed gradient to compute first- and second-order moments. This projection reduces the gradient size and is typically derived via the Singular Value Decomposition (SVD) of the true gradient ____. To mitigate the computational cost of SVD, alternative methods have been proposed, such as using random matrices ____ or generating the projection matrix through online Principal Component Analysis (PCA) ____. {Fira ____ and LDAdam ____ employ an error-feedback mechanism. The former combines the true gradient with the GaLore update to improve performance, while the latter explicitly accounts for both gradient and optimizer state compression.} Apollo ____ interprets Adam as an adaptive learning rate algorithm and uses compressed optimizer states directly as scaling factors for the true gradient. Additionally, Adafactor ____ discards the first-order moment and approximates the second-order moment with two low-rank matrices, while Adam-mini ____ proposes that block-wise second-order moments are sufficient for adjusting learning rates. ____ integrates the GaLore method with a natural gradient optimizer to enhance performance. Meanwhile, ____ applies wavelet transforms to compress gradients beyond the low-rank structures.

\textbf{Activation-efficient methods.} 
Although the aforementioned methods effectively reduce memory consumption for optimizer states, they do not address the memory costs associated with activations. To reduce activations, zeroth-order (ZO) algorithms have been introduced in LLM training ____. These methods can be further improved through variance reduction techniques ____, while ____ utilizes ZO approaches to approximate a natural gradient algorithm. Moreover, ____ proposes a novel ZO framework to enhance performance. Unlike first-order (FO) methods, ZO algorithms approximate gradients by finite differences in function values, eliminating the need for explicit gradient computation. This approach bypasses backpropagation and activation storage, significantly reducing memory demands. However, due to their slower convergence rates ____, ZO methods are primarily suitable for fine-tuning applications. Similarly, FO methods can achieve activation savings by layer-wise training ____, but their use also predominantly targets fine-tuning phases.

\textbf{System-based methods.} 
Several system-level techniques have been proposed to improve memory efficiency. Activation checkpointing ____ reduces memory usage by recomputing activations on demand rather than storing them throughout the entire iteration, though this comes at the cost of increased computational complexity. Quantization ____ lowers memory consumption by using lower-bit data representations, but this may introduce a trade-off between memory efficiency and training precision. Additionally, methods such as those introduced by ____ reduce GPU memory usage by offloading data to non-GPU resources, which can lead to additional communication overhead.