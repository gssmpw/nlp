[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2024mobilellm",
        "author": "Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others",
        "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases"
      },
      {
        "key": "tang2024rethinking",
        "author": "Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe",
        "title": "Rethinking optimization and architecture for tiny language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "thangarasa2023spdf",
        "author": "Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas",
        "title": "SPDF: Sparse pre-training and dense fine-tuning for large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "biderman2024lora",
        "author": "Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others",
        "title": "Lora learns less and forgets less"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lialin2023relora",
        "author": "Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna",
        "title": "Relora: High-rank training through low-rank updates"
      },
      {
        "key": "xia2024chain",
        "author": "Xia, Wenhan and Qin, Chengwei and Hazan, Elad",
        "title": "Chain of lora: Efficient fine-tuning of language models via residual learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lialin2023relora",
        "author": "Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna",
        "title": "Relora: High-rank training through low-rank updates"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hao2024flora",
        "author": "Hao, Yongchang and Cao, Yanshuai and Mou, Lili",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      },
      {
        "key": "he2024subspace",
        "author": "He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun",
        "title": "Subspace optimization for large language models with convergence guarantees"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liang2024memory",
        "author": "Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang",
        "title": "Memory-efficient llm training with online subspace descent"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024fira",
        "author": "Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren",
        "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "robert2024ldadam",
        "author": "Robert, Thomas and Safaryan, Mher and Modoranu, Ionut-Vlad and Alistarh, Dan",
        "title": "Ldadam: Adaptive optimization from low-dimensional gradient statistics"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhu2024apollo",
        "author": "Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon",
        "title": "APOLLO: SGD-like Memory, AdamW-level Performance"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "shazeer2018adafactor",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive learning rates with sublinear memory cost"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024adam",
        "author": "Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu",
        "title": "Adam-mini: Use fewer learning rates to gain more"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "das2024natural",
        "author": "Das, Arijit",
        "title": "Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wen2025breaking",
        "author": "Wen, Ziqing and Luo, Ping and Wang, Jiahuan and Deng, Xiaoge and Zou, Jinping and Yuan, Kun and Sun, Tao and Li, Dongsheng",
        "title": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "malladi2023fine",
        "author": "Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev",
        "title": "Fine-tuning language models with just forward passes"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "gautam2024variance",
        "author": "Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok",
        "title": "Variance-reduced zeroth-order methods for fine-tuning language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhao2024second",
        "author": "Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W",
        "title": "Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chen2024enhancing",
        "author": "Chen, Yiming and Zhang, Yuan and Cao, Liyuan and Yuan, Kun and Wen, Zaiwen",
        "title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "duchi2015optimal",
        "author": "Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre",
        "title": "Optimal rates for zero-order convex optimization: The power of two function evaluations"
      },
      {
        "key": "nesterov2017random",
        "author": "Nesterov, Yurii and Spokoiny, Vladimir",
        "title": "Random gradient-free minimization of convex functions"
      },
      {
        "key": "berahas2022theoretical",
        "author": "Berahas, Albert S and Cao, Liyuan and Choromanski, Krzysztof and Scheinberg, Katya",
        "title": "A theoretical and empirical comparison of gradient approximations in derivative-free optimization"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "lai2024lisa",
        "author": "Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya",
        "title": "Lisa: Reasoning segmentation via large language model"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "chen2016training",
        "author": "Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos",
        "title": "Training deep nets with sublinear memory cost"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "dettmers2023qlora",
        "author": "Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke",
        "title": "QLoRA: efficient finetuning of quantized LLMs (2023)"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "ren2021zero",
        "author": "Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong",
        "title": "$\\{$Zero-offload$\\}$: Democratizing $\\{$billion-scale$\\}$ model training"
      },
      {
        "key": "zhang2023g10",
        "author": "Zhang, Haoyang and Zhou, Yirui and Xue, Yuqi and Liu, Yiqi and Huang, Jian",
        "title": "G10: Enabling an efficient unified gpu memory and storage architecture with smart tensor migrations"
      }
    ]
  }
]