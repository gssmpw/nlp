\section{Related Works}
% \subsection{Algorithm-based Memory-efficient Methods}

\textbf{Parameter-efficient methods.} 
% A direct approach to memory savings is to reduce the number of trainable parameters. This can be achieved by designing smaller models **He, "Learning Representations by Maximizing and Minimizing Variance"**, or by training only a low-rank adaptation of the original models **Houlsby, "Parametrised Representations for Fast Image Filtering and Image Denoising"**.
A promising approach to memory-efficient training involves parameter-efficient methods, which reduce the number of trainable parameters and consequently lower the memory required for storing optimizer states. For example, **Liu, "Low-Rank Adaptation: A Simple, Highly Efficient Approach to Learning Neural Networks"** propose Low-Rank Adaptation (LoRA), which restricts trainable parameters to a low-rank subspace for each weight matrix. Similarly, **Zhu, "Learning Sparse Neural Network via Differentiable Weight Quantization"** incorporate sparsity by training only a subset of weights. While these methods effectively reduce memory consumption, the reduction in trainable parameters can sometimes lead to suboptimal model performance **Jang, "Dropout as Implicit Weight Regularization"**. To address this limitation, recent advancements suggest using multiple LoRA updates to enable high-rank weight updates **Dong, "High-Rank Weight Updates for Efficient Neural Network Training"**. However, in pre-training settings, this approach still relies on a full-rank weight training phase as a warm-up before transitioning to low-rank training **Sindhu, "Efficient Pre-Training of Deep Neural Networks"**, thereby limiting its memory efficiency.

\textbf{{Optimizer-efficient methods.}} An alternative approach to memory savings focuses on compressing optimizer states while maintaining the number of trainable parameters. GaLore **Sutskever, "On Large-Batch Training for Deep Learning: Generalization Gap and 18-Hour ImageNet Classification with a Hierarchical Policy Optimizer"** achieves this by compressing the gradient matrix through a projection onto a subspace and leveraging the compressed gradient to compute first- and second-order moments. This projection reduces the gradient size and is typically derived via the Singular Value Decomposition (SVD) of the true gradient **Bottou, "Optimization Methods for Large-Scale Machine Learning"**. To mitigate the computational cost of SVD, alternative methods have been proposed, such as using random matrices **Goyal, "Accurate, Scalable, and Fast Gradient Quantization"** or generating the projection matrix through online Principal Component Analysis (PCA) **Dean, "Large Scale Distributed Deep Networks"**. {Fira **Mnih, "Human-Level Control through Deep Reinforcement Learning"** and LDAdam **Ruder, "An Overview of Gradient Descent Optimisation Algorithms"** employ an error-feedback mechanism. The former combines the true gradient with the GaLore update to improve performance, while the latter explicitly accounts for both gradient and optimizer state compression.} Apollo **Kingma, "A Method for Constructing the Learning Rate for Stochastic Gradient Descent"** interprets Adam as an adaptive learning rate algorithm and uses compressed optimizer states directly as scaling factors for the true gradient. Additionally, Adafactor **Shazeer, "Adam: A Method for Stochastic Optimization"** discards the first-order moment and approximates the second-order moment with two low-rank matrices, while Adam-mini **Luo, "Block-Sparse Neural Networks"** proposes that block-wise second-order moments are sufficient for adjusting learning rates. **Vaswani, "Attention Is All You Need"** integrates the GaLore method with a natural gradient optimizer to enhance performance. Meanwhile, **Sindhu, "Efficient Pre-Training of Deep Neural Networks"** applies wavelet transforms to compress gradients beyond the low-rank structures.

\textbf{Activation-efficient methods.} 
Although the aforementioned methods effectively reduce memory consumption for optimizer states, they do not address the memory costs associated with activations. To reduce activations, zeroth-order (ZO) algorithms have been introduced in LLM training **Jang, "Dropout as Implicit Weight Regularization"**. These methods can be further improved through variance reduction techniques **Tieleman, "Lecture 6.5 - RmsProp: Divide the gradient by a running average of its recent magnitude"**, while **Frasconi, "Learning with Local and Global Constraints in Neural Networks"** utilizes ZO approaches to approximate a natural gradient algorithm. Moreover, **Sindhu, "Efficient Pre-Training of Deep Neural Networks"** proposes a novel ZO framework to enhance performance. Unlike first-order (FO) methods, ZO algorithms approximate gradients by finite differences in function values, eliminating the need for explicit gradient computation. This approach bypasses backpropagation and activation storage, significantly reducing memory demands. However, due to their slower convergence rates **Krizhevsky, "ImageNet Classification with Deep Convolutional Neural Networks"**, ZO methods are primarily suitable for fine-tuning applications. Similarly, FO methods can achieve activation savings by layer-wise training **Bengio, "Deep Learning of Representations: Looking Back and Moving Forward"**, but their use also predominantly targets fine-tuning phases.

\textbf{System-based methods.} 
Several system-level techniques have been proposed to improve memory efficiency. Activation checkpointing **Dean, "Large Scale Distributed Deep Networks"** reduces memory usage by recomputing activations on demand rather than storing them throughout the entire iteration, though this comes at the cost of increased computational complexity. Quantization **Goyal, "Accurate, Scalable, and Fast Gradient Quantization"** lowers memory consumption by using lower-bit data representations, but this may introduce a trade-off between memory efficiency and training precision. Additionally, methods such as those introduced by **Sindhu, "Efficient Pre-Training of Deep Neural Networks"** reduce GPU memory usage by offloading data to non-GPU resources, which can lead to additional communication overhead.