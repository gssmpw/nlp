\section{Related Works}
% \subsection{Algorithm-based Memory-efficient Methods}

\textbf{Parameter-efficient methods.} 
% A direct approach to memory savings is to reduce the number of trainable parameters. This can be achieved by designing smaller models \citep{liu2024mobilellm, tang2024rethinking}, or by training only a low-rank adaptation of the original models \citep{hu2021lora}.
A promising approach to memory-efficient training involves parameter-efficient methods, which reduce the number of trainable parameters and consequently lower the memory required for storing optimizer states. For example, \cite{hu2021lora} propose Low-Rank Adaptation (LoRA), which restricts trainable parameters to a low-rank subspace for each weight matrix. Similarly, \cite{thangarasa2023spdf} incorporate sparsity by training only a subset of weights. While these methods effectively reduce memory consumption, the reduction in trainable parameters can sometimes lead to suboptimal model performance \citep{biderman2024lora}. To address this limitation, recent advancements suggest using multiple LoRA updates to enable high-rank weight updates \citep{lialin2023relora, xia2024chain}. However, in pre-training settings, this approach still relies on a full-rank weight training phase as a warm-up before transitioning to low-rank training \citep{lialin2023relora}, thereby limiting its memory efficiency.

\textbf{{Optimizer-efficient methods.}} An alternative approach to memory savings focuses on compressing optimizer states while maintaining the number of trainable parameters. GaLore \citep{zhao2024galore} achieves this by compressing the gradient matrix through a projection onto a subspace and leveraging the compressed gradient to compute first- and second-order moments. This projection reduces the gradient size and is typically derived via the Singular Value Decomposition (SVD) of the true gradient \citep{zhao2024galore}. To mitigate the computational cost of SVD, alternative methods have been proposed, such as using random matrices \citep{hao2024flora,he2024subspace} or generating the projection matrix through online Principal Component Analysis (PCA) \citep{liang2024memory}. {Fira \citep{chen2024fira} and LDAdam \citep{robert2024ldadam} employ an error-feedback mechanism. The former combines the true gradient with the GaLore update to improve performance, while the latter explicitly accounts for both gradient and optimizer state compression.} Apollo \citep{zhu2024apollo} interprets Adam as an adaptive learning rate algorithm and uses compressed optimizer states directly as scaling factors for the true gradient. Additionally, Adafactor \citep{shazeer2018adafactor} discards the first-order moment and approximates the second-order moment with two low-rank matrices, while Adam-mini \citep{zhang2024adam} proposes that block-wise second-order moments are sufficient for adjusting learning rates. \citep{das2024natural} integrates the GaLore method with a natural gradient optimizer to enhance performance. Meanwhile, \citet{wen2025breaking} applies wavelet transforms to compress gradients beyond the low-rank structures.

\textbf{Activation-efficient methods.} 
Although the aforementioned methods effectively reduce memory consumption for optimizer states, they do not address the memory costs associated with activations. To reduce activations, zeroth-order (ZO) algorithms have been introduced in LLM training \citep{malladi2023fine}. These methods can be further improved through variance reduction techniques \citep{gautam2024variance}, while \citet{zhao2024second} utilizes ZO approaches to approximate a natural gradient algorithm. Moreover, \citet{chen2024enhancing} proposes a novel ZO framework to enhance performance. Unlike first-order (FO) methods, ZO algorithms approximate gradients by finite differences in function values, eliminating the need for explicit gradient computation. This approach bypasses backpropagation and activation storage, significantly reducing memory demands. However, due to their slower convergence rates \citep{duchi2015optimal, nesterov2017random, berahas2022theoretical}, ZO methods are primarily suitable for fine-tuning applications. Similarly, FO methods can achieve activation savings by layer-wise training \citep{lai2024lisa}, but their use also predominantly targets fine-tuning phases.

\textbf{System-based methods.} 
Several system-level techniques have been proposed to improve memory efficiency. Activation checkpointing \citep{chen2016training} reduces memory usage by recomputing activations on demand rather than storing them throughout the entire iteration, though this comes at the cost of increased computational complexity. Quantization \citep{dettmers2023qlora} lowers memory consumption by using lower-bit data representations, but this may introduce a trade-off between memory efficiency and training precision. Additionally, methods such as those introduced by \citet{ren2021zero, zhang2023g10} reduce GPU memory usage by offloading data to non-GPU resources, which can lead to additional communication overhead.