@article{solaiman2019release,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint ArXiv:2005.14165},
  year={2020}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{amari1993backpropagation,
  title={Backpropagation and stochastic gradient descent method},
  author={Amari, Shun-ichi},
  journal={Neurocomputing},
  volume={5},
  number={4-5},
  pages={185--196},
  year={1993},
  publisher={Elsevier}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{malladi2023fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@article{spall1992multivariate,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992},
  publisher={IEEE}
}

@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@article{berahas2022theoretical,
  title={A theoretical and empirical comparison of gradient approximations in derivative-free optimization},
  author={Berahas, Albert S and Cao, Liyuan and Choromanski, Krzysztof and Scheinberg, Katya},
  journal={Foundations of Computational Mathematics},
  volume={22},
  number={2},
  pages={507--560},
  year={2022},
  publisher={Springer}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM journal on optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{ding2023sparse,
  title={Sparse low-rank adaptation of pre-trained language models},
  author={Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2311.11696},
  year={2023}
}

@inproceedings{tu2019autozoom,
  title={Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks},
  author={Tu, Chun-Chen and Ting, Paishun and Chen, Pin-Yu and Liu, Sijia and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui and Cheng, Shin-Ming},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={742--749},
  year={2019}
}

@inproceedings{ilyas2018black,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle={International conference on machine learning},
  pages={2137--2146},
  year={2018},
  organization={PMLR}
}

@article{zhang2022robustify,
  title={How to robustify black-box ml models? a zeroth-order optimization perspective},
  author={Zhang, Yimeng and Yao, Yuguang and Jia, Jinghan and Yi, Jinfeng and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
  journal={arXiv preprint arXiv:2203.14195},
  year={2022}
}

@inproceedings{zhao2019design,
  title={On the design of black-box adversarial examples by leveraging gradient-free optimization and operator splitting method},
  author={Zhao, Pu and Liu, Sijia and Chen, Pin-Yu and Hoang, Nghia and Xu, Kaidi and Kailkhura, Bhavya and Lin, Xue},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={121--130},
  year={2019}
}

@article{dhurandhar2019model,
  title={Model agnostic contrastive explanations for structured data},
  author={Dhurandhar, Amit and Pedapati, Tejaswini and Balakrishnan, Avinash and Chen, Pin-Yu and Shanmugam, Karthikeyan and Puri, Ruchir},
  journal={arXiv preprint arXiv:1906.00117},
  year={2019}
}

@article{wang2022zarts,
  title={Zarts: On zero-order optimization for neural architecture search},
  author={Wang, Xiaoxing and Guo, Wenxuan and Su, Jianlin and Yang, Xiaokang and Yan, Junchi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12868--12880},
  year={2022}
}

@article{chen2019zo,
  title={Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization},
  author={Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{liu2018zeroth,
  title={Zeroth-order stochastic variance reduction for nonconvex optimization},
  author={Liu, Sijia and Kailkhura, Bhavya and Chen, Pin-Yu and Ting, Paishun and Chang, Shiyu and Amini, Lisa},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{ji2019improved,
  title={Improved zeroth-order variance reduced algorithms and analysis for nonconvex optimization},
  author={Ji, Kaiyi and Wang, Zhe and Zhou, Yi and Liang, Yingbin},
  booktitle={International conference on machine learning},
  pages={3100--3109},
  year={2019},
  organization={PMLR}
}

@article{lian2016comprehensive,
  title={A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order},
  author={Lian, Xiangru and Zhang, Huan and Hsieh, Cho-Jui and Huang, Yijun and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{liu2019signsgd,
  title={signSGD via zeroth-order oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{liu2020primer,
  title={A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}

@article{balasubramanian2018zeroth,
  title={Zeroth-order (non)-convex stochastic optimization via conditional gradient and gradient updates},
  author={Balasubramanian, Krishnakumar and Ghadimi, Saeed},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{cai2022zeroth,
  title={Zeroth-order regularized optimization (zoro): Approximately sparse gradients and adaptive sampling},
  author={Cai, HanQin and McKenzie, Daniel and Yin, Wotao and Zhang, Zhenliang},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={687--714},
  year={2022},
  publisher={SIAM}
}

@article{chen2023deepzero,
  title={Deepzero: Scaling up zeroth-order optimization for deep model training},
  author={Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.02025},
  year={2023}
}

@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}

@article{muhamed2024grass,
  title={GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients},
  author={Muhamed, Aashiq and Li, Oscar and Woodruff, David and Diab, Mona and Smith, Virginia},
  journal={arXiv preprint arXiv:2406.17660},
  year={2024}
}

@article{gautam2024variance,
  title={Variance-reduced zeroth-order methods for fine-tuning language models},
  author={Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok},
  journal={arXiv preprint arXiv:2404.08080},
  year={2024}
}

@article{zhao2024second,
  title={Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer},
  author={Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W},
  journal={arXiv preprint arXiv:2402.15173},
  year={2024}
}

@article{liu2024sparse,
  title={Sparse mezo: Less parameters for better performance in zeroth-order llm fine-tuning},
  author={Liu, Yong and Zhu, Zirui and Gong, Chaoyu and Cheng, Minhao and Hsieh, Cho-Jui and You, Yang},
  journal={arXiv preprint arXiv:2402.15751},
  year={2024}
}

@article{zhang2024revisiting,
  title={Revisiting zeroth-order optimization for memory-efficient llm fine-tuning: A benchmark},
  author={Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D and Yin, Wotao and Hong, Mingyi and others},
  journal={arXiv preprint arXiv:2402.11592},
  year={2024}
}

@inproceedings{liaddax,
  title={Addax: Memory-Efficient Fine-Tuning of Language Models with a Combination of Forward-Backward and Forward-Only Passes},
  author={Li, Zeman and Zhang, Xinwei and Razaviyayn, Meisam},
  booktitle={5th Workshop on practical ML for limited/low resource settings},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017},
  publisher={Springer}
}

@article{kozak2023zeroth,
  title={Zeroth-order optimization with orthogonal random directions},
  author={Kozak, David and Molinari, Cesare and Rosasco, Lorenzo and Tenorio, Luis and Villa, Silvia},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={1179--1219},
  year={2023},
  publisher={Springer}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}
@inproceedings{yu2017compressing,
  title={On compressing deep models by low rank and sparse decomposition},
  author={Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7370--7379},
  year={2017}
}
@article{kamalakara2022exploring,
  title={Exploring low rank training of deep neural networks},
  author={Kamalakara, Siddhartha Rao and Locatelli, Acyr and Venkitesh, Bharat and Ba, Jimmy and Gal, Yarin and Gomez, Aidan N},
  journal={arXiv preprint arXiv:2209.13569},
  year={2022}
}
@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}
@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}
@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}
@inproceedings{bar2006second,
  title={The second pascal recognising textual entailment challenge},
  author={Bar-Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the second PASCAL challenges workshop on recognising textual entailment},
  volume={1},
  year={2006},
  organization={Citeseer}
}
@inproceedings{giampiccolo2007third,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}
@article{bentivogli2009fifth,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  journal={TAC},
  volume={7},
  number={8},
  pages={1},
  year={2009},
  publisher={Citeseer}
}
@inproceedings{voorhees2000building,
  title={Building a question answering test collection},
  author={Voorhees, Ellen M and Tice, Dawn M},
  booktitle={Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={200--207},
  year={2000}
}
@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}
@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}
@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}
@article{pilehvar2018wic,
  title={WiC: the word-in-context dataset for evaluating context-sensitive meaning representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  journal={arXiv preprint arXiv:1808.09121},
  year={2018}
}
@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}
@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI spring symposium series},
  year={2011}
}
@article{zhang2018record,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@article{dua2019drop,
  title={DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1903.00161},
  year={2019}
}
@article{li2018measuring,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1804.08838},
  year={2018}
}
@article{larsen2021many,
  title={How many degrees of freedom do we need to train deep networks: a loss landscape perspective},
  author={Larsen, Brett W and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
  journal={arXiv preprint arXiv:2107.05802},
  year={2021}
}
@article{gur2018gradient,
  title={Gradient descent happens in a tiny subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  journal={arXiv preprint arXiv:1812.04754},
  year={2018}
}
@article{sagun2017empirical,
  title={Empirical analysis of the hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@inproceedings{zhang2023fine,
  title={Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models},
  author={Zhang, Zhong and Liu, Bang and Shao, Junming},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}

@article{hong2017iteration,
  title={Iteration complexity analysis of block coordinate descent methods},
  author={Hong, Mingyi and Wang, Xiangfeng and Razaviyayn, Meisam and Luo, Zhi-Quan},
  journal={Mathematical Programming},
  volume={163},
  pages={85--114},
  year={2017},
  publisher={Springer}
}

@article{wright2015coordinate,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer}
}

@article{tseng2001convergence,
  title={Convergence of a block coordinate descent method for nondifferentiable minimization},
  author={Tseng, Paul},
  journal={Journal of optimization theory and applications},
  volume={109},
  pages={475--494},
  year={2001},
  publisher={Springer}
}

@book{boyd2004convex, 
  title={Convex optimization}, 
  author={Boyd, Stephen and Vandenberghe, Lieven}, 
  year={2004}, 
  publisher={Cambridge university press} 
}

@article{guo2024unifiedconvergenceanalysisadaptive,
      title={Unified Convergence Analysis for Adaptive Optimization with Moving Average Estimator}, 
      author={Zhishuai Guo and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
      year={2024},
      journal={arXiv preprint arXiv:2104.14840},
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{byrd2016stochastic,
  title={A stochastic quasi-Newton method for large-scale optimization},
  author={Byrd, Richard H and Hansen, Samantha L and Nocedal, Jorge and Singer, Yoram},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={2},
  pages={1008--1031},
  year={2016},
  publisher={SIAM}
}

@InProceedings{shamir2013complexity,
  title = 	 {On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization},
  author = 	 {Shamir, Ohad},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {3--24},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR}
}

@article{agarwal2017second,
  title={Second-order stochastic optimization for machine learning in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={116},
  pages={1--40},
  year={2017}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{yuan2016influence,
  title={On the influence of momentum acceleration on online learning},
  author={Yuan, Kun and Ying, Bicheng and Sayed, Ali H},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={192},
  pages={1--66},
  year={2016}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@article{wen2025breaking,
  title={Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training},
  author={Wen, Ziqing and Luo, Ping and Wang, Jiahuan and Deng, Xiaoge and Zou, Jinping and Yuan, Kun and Sun, Tao and Li, Dongsheng},
  journal={arXiv preprint arXiv:2501.07237},
  year={2025}
}

@article{ding2023sparse,
  title={Sparse low-rank adaptation of pre-trained language models},
  author={Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2311.11696},
  year={2023}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}
@inproceedings{yu2017compressing,
  title={On compressing deep models by low rank and sparse decomposition},
  author={Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7370--7379},
  year={2017}
}
@article{kamalakara2022exploring,
  title={Exploring low rank training of deep neural networks},
  author={Kamalakara, Siddhartha Rao and Locatelli, Acyr and Venkitesh, Bharat and Ba, Jimmy and Gal, Yarin and Gomez, Aidan N},
  journal={arXiv preprint arXiv:2209.13569},
  year={2022}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{thangarasa2023spdf,
  title={SPDF: Sparse pre-training and dense fine-tuning for large language models},
  author={Thangarasa, Vithursan and Gupta, Abhay and Marshall, William and Li, Tianda and Leong, Kevin and DeCoste, Dennis and Lie, Sean and Saxena, Shreyas},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={2134--2146},
  year={2023},
  organization={PMLR}
}

@article{biderman2024lora,
  title={Lora learns less and forgets less},
  author={Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}

@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{xia2024chain,
  title={Chain of lora: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@article{he2024subspace,
  title={Subspace optimization for large language models with convergence guarantees},
  author={He, Yutong and Li, Pengrui and Hu, Yipeng and Chen, Chuyan and Yuan, Kun},
  journal={arXiv preprint arXiv:2410.11289},
  year={2024}
}

@article{liang2024memory,
  title={Memory-efficient llm training with online subspace descent},
  author={Liang, Kaizhao and Liu, Bo and Chen, Lizhang and Liu, Qiang},
  journal={arXiv preprint arXiv:2408.12857},
  year={2024}
}

@article{chen2024fira,
  title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?},
  author={Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren},
  journal={arXiv preprint arXiv:2410.01623},
  year={2024}
}

@article{zhu2024apollo,
  title={APOLLO: SGD-like Memory, AdamW-level Performance},
  author={Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon},
  journal={arXiv preprint arXiv:2412.05270},
  year={2024}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{zhang2024adam,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}

@inproceedings{lai2024lisa,
  title={Lisa: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9579--9589},
  year={2024}
}

@article{liu2024mobilellm,
  title={Mobilellm: Optimizing sub-billion parameter language models for on-device use cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
  journal={arXiv preprint arXiv:2402.14905},
  year={2024}
}

@article{tang2024rethinking,
  title={Rethinking optimization and architecture for tiny language models},
  author={Tang, Yehui and Liu, Fangcheng and Ni, Yunsheng and Tian, Yuchuan and Bai, Zheyuan and Hu, Yi-Qi and Liu, Sichao and Jui, Shangling and Han, Kai and Wang, Yunhe},
  journal={arXiv preprint arXiv:2402.02791},
  year={2024}
}

@article{robert2024ldadam,
  title={Ldadam: Adaptive optimization from low-dimensional gradient statistics},
  author={Robert, Thomas and Safaryan, Mher and Modoranu, Ionut-Vlad and Alistarh, Dan},
  journal={arXiv preprint arXiv:2410.16103},
  year={2024}
}

@article{das2024natural,
  title={Natural GaLore: Accelerating GaLore for memory-efficient LLM Training and Fine-tuning},
  author={Das, Arijit},
  journal={arXiv preprint arXiv:2410.16029},
  year={2024}
}

@article{chen2024enhancing,
  title={Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures},
  author={Chen, Yiming and Zhang, Yuan and Cao, Liyuan and Yuan, Kun and Wen, Zaiwen},
  journal={arXiv preprint arXiv:2410.07698},
  year={2024}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{ren2021zero,
  title={$\{$Zero-offload$\}$: Democratizing $\{$billion-scale$\}$ model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}

@article{dettmers2023qlora,
  title={QLoRA: efficient finetuning of quantized LLMs (2023)},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  volume={52},
  pages={3982--3992},
  year={2023}
}

@inproceedings{zhang2023g10,
  title={G10: Enabling an efficient unified gpu memory and storage architecture with smart tensor migrations},
  author={Zhang, Haoyang and Zhou, Yirui and Xue, Yuqi and Liu, Yiqi and Huang, Jian},
  booktitle={Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={395--410},
  year={2023}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}