\section{Related Work}
\paragraph{Adversarial Attack.}
Adversarial attacks manipulate clean images with imperceptible modifications to fool Deep Neural Networks (DNNs) into making incorrect predictions. These attacks are broadly divided into black-box and white-box attacks. White-box attacks leverage detailed information about the target model, including training data and gradients, to craft adversarial images. In contrast, black-box attacks, which do not rely on any internal information about the victim model, are divided into transfer-based and query-based strategies. Transfer-based attacks create adversarial images using a surrogate model, aiming for these examples to also be effective against the target model. Query-based attacks iteratively modify clean images and query the victim model, using the resulting confidence scores to refine the attack. Typically, attack strategies are evaluated using the $L_p$ norm to restrict the perturbation to remain imperceptible to humans____.


\paragraph{Black-box Pixel Attack in Image Classification.}
Unlike other metrics, the $L_0$ norm, also known as pixel norm, targets only a small subset of pixels in a clean image rather than attacking all of them. The pioneering pixel attack method, OnePixel____, employed Differential Evolution (DE) to generate adversarial images. An advanced approach, ScratchThat____, used DE to create curves and applied a parametric model to perturbations, reducing parameters and improving performance. A more recent study, PIXLE____, enhanced query efficiency and attack success rate by using a simple algorithm instead of DE. Briefly, PIXLE generates adversarial images by selecting arbitrary patches in a clean image and applying the brightness of these pixels to others. Although this method improved performance, it ignored pixel independence due to its reliance on patches and exhibited inefficiencies stemming from randomness in brightness mapping. The previous study, PatchAttack____, utilized RL to embed textures in specific regions of the clean image, discovering vulnerable patches and reducing randomness, which significantly decreased the number of queries and improved attack success rates. Unfortunately, this method still depended on patches, requiring at least 3\% of the image area to be attacked. Our research focuses on eliminating patch dependency by attacking individual pixels and reducing randomness through RL. Extensive experiments demonstrate that our proposed attack outperforms the state-of-the-art methods in both query efficiency and attack success rate.

\paragraph{Query-based Adversarial Attack in Object Detection.} 
Adversarial attacks in object detection are more challenging than those in image classification. The first query-based attack in object detection, PRFA____, generates adversarial images using a parallel rectangle flipping strategy. Recent research, GARSDC____, employs a genetic algorithm to create adversarial images, improving optimization efficiency by using adversarial examples generated from transfer-based attacks as the initial population. We extend our proposed attack from image classification to object detection. Experiments show that our method achieves a comparable mAP reduction on YOLO____ to state-of-the-art methods while significantly reducing the number of queries, demonstrating its effectiveness in object detection.