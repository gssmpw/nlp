\section{Proofs}
\label{app:proofs}

\subsection{Certificate Proofs}

\subsubsection{Proof of Proposition \ref{cert:reach} -- Reachability Certificate}
Fix $\delta > -\sup_{x \in X_I} V(x) \geq 0$, and recall that $k_G = \min \{k \in \{0,\dots,T\} \colon V(x(k)) \leq -\delta\}$. Consider then the difference condition in \eqref{eq:reach_deriv}, namely,
    \begin{equation} \label{eq:proof_dec_reach}
        \begin{aligned}
		&V(x(k+1)) - V(x(k)) \\ 
        & < - \frac{1}{T}\Big (\sup_{x \in X_I} V(x)+\delta \Big),~ k=0,\dots,k_G-1,
        \end{aligned}
    \end{equation}
By recursive application of this inequality $k \leq k_G$ times, 
\begin{align}
&V(x(k)) < V(x(0)) -\frac{k}{T} \Big ( \sup_{x \in X_I} V(x) +\delta\Big ) \nonumber \\
&\leq \frac{T-k}{T} \sup_{x \in X_I} V(x) -\frac{k}{T} \delta  \leq -\frac{k}{T} \delta \leq 0, \label{eq:proof_dec_reach1}
\end{align}
where the second inequality is since $V(x(0)) \leq \sup_{x \in X_I} V(x)$, as $x(0) \in X_I$. The third
one is since $\sup_{x \in X_I} V(x) \leq 0$ as by \eqref{eq:reach_init}, $V(x) \leq 0$, for all $x \in X_I$, and  $k \leq k_G \leq T$, while the last inequality is since $\delta>0$.


By \eqref{eq:proof_dec_reach1} we then have that for all $k\leq k_G$, $V(x(k)) <0$, which implies that $x(k)$ does not leave $X$ for all $k\leq k_G$ (see \eqref{eq:reach_else}), while by the definition of $k_G$, $x(k_G) \in X_G$. Notice that if $k_G = T$, then \eqref{eq:proof_dec_reach1} (besides implying that $x(k) \in X$ for all $k\leq T$), also leads to $x(T) \leq -\delta$, which means that $x(T) \in X_G$ after $T$ time steps (see \eqref{eq:reach_goal}), which captures the latest time the goal set is reached.

Therefore, all trajectories that start within $X_I$ reach the goal set $X_G$ in at most $T$ steps, without escaping $X$ till then, thus concluding the proof. \qed

\subsubsection{Proof of Proposition \ref{cert:barr} -- Safety Certificate}

Consider the condition in \eqref{eq:barr_deriv}, namely,
    \begin{equation}
     \begin{aligned}
        &V(x(k+1))-V(x(k)) \\ &< \frac{1}{T} \Big( \inf_{x \in X_U}V(x)-\sup_{x \in X_I}V(x) \Big ),~ k=0,\dots,T-1.
     \end{aligned}
     \end{equation}
     By recursive application of this inequality for $k\leq T$ times, we obtain 
     \begin{align}
         &V(x(k)) < V(x(0)) + \frac{k}{T} \Big( \inf_{x \in X_U}V(x)-\sup_{x \in X_I}V(x) \Big ) \nonumber \\
         &\leq \frac{T-k}{T} \sup_{x \in X_I}V(x) + \frac{k}{T} \inf_{x \in X_U}V(x) \nonumber \\
         &\leq \frac{k}{T} \inf_{x \in X_U}V(x) \leq \inf_{x \in X_U}V(x). \label{eq:proof_safety}
     \end{align}
     where the second inequality is is since $V(x(0)) \leq \sup_{x \in X_I}V(x)$, as $x(0) \in X_I$. The third inequality is since $\sup_{x \in X_I}V(x) \leq 0$ as by \eqref{eq:barr_states1}, $V(x) \leq 0$ for all $x \in X_I$ and $k\leq T$. The last inequality is since $\inf_{x \in X_U}V(x)\geq 0$, as by \eqref{eq:barr_states2} $V(x) >0$ for all $x \in X_U$, and $k\leq T$.
     We thus have
           \begin{align}
       V(x(k)) &< \inf_{x \in X_U}V(x),~ k=1,\dots,T.
       \end{align}
       and hence $x(k) \notin X_U, k=0,\dots,T$ (notice that $x(0) \notin X_U$ holds since $X_I \cap X_U = \emptyset$). The latter implies that all trajectories that start in $X_I$ avoid entering the unsafe set $X_U$, thus concluding the proof. \qed

\subsubsection{Proof of Proposition \ref{cert:RWA} -- RWA Certificaate}

Fix $\delta > -\sup_{x \in X_I} V(x) \geq 0 $, and recall that $k_G = \min \{k \in \{0,\dots,T\} \colon V(x(k)) \leq -\delta\}$.
Consider then the difference condition in \eqref{eq:RWA_deriv}, namely,
    \begin{equation} \label{eq:proof_RWA}
     \begin{aligned}
        &V(x(k+1))-V(x(k)) \\ &<-\frac{1}{T} \Big ( \sup_{x \in X_I}V(x)+\delta \Big ),~ k=0,\dots,k_G-1,
     \end{aligned}
     \end{equation}
    Note that this is identical to the difference condition for our reachability property, and hence following the same arguments with the proof of Proposition \ref{cert:reach}, we can infer that state trajectories emanating from $X_I$ will reach the goal set $X_G$ in at most $T$ time steps.
    
    By \eqref{eq:RWA_else} we have that $V(x) > 0$, for all $x \in _U$ while by \eqref{eq:RWA_init} we have that $V(x) \leq 0$, for all $x \in X_I$. Therefore, $\sup_{x\in X_I} V(x) \leq 0 \leq \inf_{x \in X_U} V(x)$. At the same time by our choice for $\delta$ we have that $\delta>-\sup_{x \in X_I}V(x)$. Combining these, we infer that $\delta >-\inf_{x \in X_U} V(x)$.
    Thus, \eqref{eq:proof_RWA} implies that for all $k=0,\ldots,k_G-1$,
    \begin{align}
        -\frac{1}{T} \Big ( \sup_{x \in X_I}&V(x) +\delta \Big )  \nonumber \\
        &< \frac{1}{T} \Big (\inf_{x \in X_U}V(x)-\sup_{x \in X_I}V(x)\Big ).
    \end{align}
    Therefore,
     \begin{align}
        &V(x(k+1))-V(x(k)) \\ &<\frac{1}{T} \Big (\inf_{x \in X_U}V(x)-\sup_{x \in X_I}V(x)\Big ), k=0,\dots,k_G-1. \nonumber
     \end{align}
     Note that this is identical to the difference condition for our safety property, and hence following the same arguments with the proof of Proposition \ref{cert:barr}, we can infer that state trajectories emanating from $X_I$ will never pass through the unsafe set $X_U$ until time $k=k_G$.

     Moreover, by \eqref{eq:RWA_deriv2}, we have that 
      \begin{equation}
     \begin{aligned}
        &V(x(k+1))-V(x(k)) \\ &<\frac{1}{T} \Big (\inf_{x \in X_U}V(x)+\delta\Big ),~ k=k_G,\dots,T-1.
     \end{aligned}
     \end{equation}
     Note that this is also a difference condition identical to that for our safety property, but with $\delta$ in place of $\sup_{x \in X_I} V(x)$ (since we know that $V(x(k_G)) \leq -\delta$ by definition of $k_G$).
     Hence, we have a safety condition for all trajectories emanating from this sublevel set.
     We know from \eqref{eq:proof_RWA} that trajectories reach this sublevel set, and hence remain safe for $k=k_G,\ldots,T$

    Therefore, we have shown that starting at $X_I$ trajectories reach $X_G$ in at most $T$ time steps, while they never pass through $X_U$ and do not leave the domain $X$, thus concluding the proof. \qed

\subsection{Proof of Proposition \ref{prop:converge} -- Properties of Algorithm 1}
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
\item By construction, Algorithm \ref{algo:sub} creates a non-increasing sequence of iterates $\{L_k\}_{k\geq 0}$ that is bounded below by the global minimum of $\max_{\xi \in D} L(\theta,\xi)$ which exists and is finite due to Assumption \ref{ass:exist}.
As such, the sequence $\{L_k\}_{k\geq 0}$ is convergent, which in turn implies that Algorithm \ref{algo:sub} terminates.\\
\item We need to show that the set $\mathcal{C}_N$ is a compression set in the sense of Definition \ref{def:compress} with $\mathcal{A}$ being Algorithm \ref{algo:sub} with $D = \{\xi_i\}_{i=1}^N$. 
To see this, we ``re-run'' Algorithm \ref{algo:sub} from the same initial choice of the parameter vector $\theta$ but with $\mathcal{C}_N$ in place of $D$. 
Notice that exactly the same iterates will be generated, as $\mathcal{C}_N$ contains all samples that have a misaligned subgradient and value greater than the loss evaluated on the running compression set. %all samples that have led to a worst case loss across iterations (step 6). 
As a result, the same output will be returned, which by Definition \ref{def:compress} establishes that $\mathcal{C}_N$ is a compression set.\\
\item We show that all properties of Assumption \ref{ass:alg_prop} are satisfied by Algorithm \ref{algo:sub}. \\

\emph{Preference:} Consider a fixed (sample independent) initialization of Algorithm \ref{algo:sub} in terms of the parameter $\theta$. 
Consider also any subsets $\mathcal{C}_1,\mathcal{C}_2$ of 
$\{\xi^i\}_{i=1}^N$ with $\mathcal{C}_1\subseteq \mathcal{C}_2$. 

Suppose that the compression set returned by Algorithm \ref{algo:sub} when fed with $\mathcal{C}_2$ is different from 
$\mathcal{C}_1$. 
          Fix any $\xi \in \Xi$ and consider the set $\mathcal{C}_2 \cup \{\xi\}$. We will show that the compression set returned by Algorithm \ref{algo:sub} when fed with $\mathcal{C}_2 \cup \{\xi\}$ is different from $\mathcal{C}_1$ as well.\\
          \emph{Case 1}: The new sample $\xi$ does not appear as a maximizing sample in step~\ref{line:max_D} of Algorithm \ref{algo:sub}, or its subgradient is such that the quantity in step~\ref{line:inner} is positive. This implies that step~\ref{line:update_C} is not performed and the algorithm proceeds directly to step~\ref{line:approx_step}. As such, $\xi$ is not added to the compression set returned by Algorithm \ref{algo:sub}, which remains the same with that returned when the algorithm is fed only by $\{\xi^i\}_{i=1}^N$. However, the latter is not equal to $\mathcal{C}_1$, thus establishing the claim.\\  
	       \emph{Case 2}: The new sample $\xi$ appears as a maximizing sample in step~\ref{line:max_D} of Algorithm \ref{algo:sub}, and has a subgradient such that the quantity in step~\ref{line:inner} is non-positive. As such, step~\ref{line:update_C} is performed and $\xi$ is added to the compression returned by Algorithm \ref{algo:sub}.
          If $\xi \notin \mathcal{C}_1$ then the resulting compression set will be different from $\mathcal{C}_1$ as it would contain at least one element that is not $\mathcal{C}_1$, namely, $\xi$.
          
	       If $\xi \in \mathcal{C}_1$ then it must also be in $\mathcal{C}_2$ as $\mathcal{C}_1 \subseteq \mathcal{C}_2$. In that case $\xi$ would appear twice in $\mathcal{C}_2 \cup \{\xi\}$, i.e., the set of samples with which Algorithm \ref{algo:sub} is fed has $\xi$ as a repeated sample (notice that this can happen with zero probability due to Assumption \ref{ass:non-conc_mass}). 
                           
          Once one of these repeated samples is added to the compression set returned by Algorithm \ref{algo:sub}, then the other will never be added. This is since when this other sample appears as a maximizing one in step~\ref{line:max_D} then its duplicate will already be in the compression set, and hence the exact and approximate subgradients in steps~\ref{line:subgrad_D} and~\ref{line:subgrad_C} would be identical. 
          As such, the quantity in step~\ref{line:inner} would be non-negative (and, by positive-definiteness of the inner product, only zero when both vectors are zero-vectors) and hence step \ref{line:update_C} will not be performed, with the duplicate not added to the compression set. 
          As such, one of the repeated $\xi$'s is redundant, which implies that the compression set returned by Algorithm \ref{algo:sub} when fed with $\mathcal{C}_2 \cup \{\xi\}$ is the same with the one that would be returned when it is fed with $\mathcal{C}_2$. 
          However, this would imply that if $\mathcal{C}_1$ is the compression returned by Algorithm \ref{algo:sub} when fed with of $\mathcal{C}_2 \cup \{\xi\}$, it will also be the compression set for $\mathcal{C}_2$ (as the duplicate $\xi$ would be redundant). 
          However, the starting hypothesis has been that $\mathcal{C}_1$ is not a compression of $\mathcal{C}_2$. As such, it is not possible for $\mathcal{C}_1$ to be a compression set of $\mathcal{C}_2 \cup \{\xi\}$ as well, establishing the claim.\\
          
\emph{Non-associativity:} Consider a fixed (sample independent) initialization of Algorithm \ref{algo:sub} in terms of the parameter $\theta$. Let $\{\xi^i\}_{i=1}^{N+\bar{N}}$ for some $\bar{N} \geq 1$.
        Suppose that $\mathcal{C}$ is returned by Algorithm \ref{algo:sub} a compression set of $\{\xi^i\}_{i=1}^{N} \cup \{\xi\}$, for all $\xi \in \{\xi^i\}_{i=N+1}^{N+\bar{N}}$. 
        Therefore, up to a measure zero set we must have that
        \begin{align}
        \mathcal{C} \subset \bigcap_{j=N+1}^{\bar{N}}
       \Big ( \{\xi^i\}_{i=1}^N \cup \{\xi^j\} \Big ) = \{\xi^i\}_{i=1}^N, \label{eq:proof_nonassoc}
        \end{align}
where the inclusion is since $\mathcal{C}$ is assumed to be returned as a compression set by Algorithm \ref{algo:sub} when this is fed with any set within the intersection, while the equality is since by Assumption \ref{ass:non-conc_mass} all samples in $\{\xi^i\}_{i=1}^{N+\bar{N}}$ are distinct up to a measure zero set. This implies that up to a measure zero set $\mathcal{C}$ should be a compression set returned by Algorithm \ref{algo:sub} whenever this is fed with $\{\xi^i\}_{i=1}^N$ as any additional sample would be redundant.

Fix now any $\xi \in \{\xi^i\}_{i=N+1}^{N+\bar{N}}$, and consider 
Algorithm \ref{algo:sub} with $D = \{\xi^i\}_{i=1}^{N} \cup \{\xi\}$. The fact that $\mathcal{C}$ is returned as a compression set for $\{\xi^i\}_{i=1}^{N} \cup \{\xi\}$ implies that whenever $\xi$ is a maximizing sample in step~\ref{line:max_D} of Algorithm \ref{algo:sub}, it should give rise to a subgradient such that the quantity in step $10$ of the algorithm is positive. This implies that step~\ref{line:approx_step} is performed and hence $\xi$ is not added to $\mathcal{C}$. 

Considering Algorithm \ref{algo:sub} this time with $D =\{\xi^i\}_{i=1}^{N+\bar{N}}$, i.e., fed with all samples at once, due to the aforementioned arguments, whenever a $\xi \in \{\xi^i\}_{i=N+1}^{N+\bar{N}}$ is a maximizing sample in step~\ref{line:max_D}, then the algorithm would proceed to step~\ref{line:approx_step}, and steps~\ref{line:true_step}--\ref{line:update_C} will not be executed. As such, no such $\xi$ will be added to $\mathcal{C}$. 

Hence, the compression set returned by Algorithm \ref{algo:sub} when fed with $\{\xi^i\}_{i=1}^{N+\bar{N}}$ would be the same with the one that would be returned if the algorithm was fed with $\{\xi^i\}_{i=1}^{N}$. By \eqref{eq:proof_nonassoc} this then implies that the returned set should be $\mathcal{C}$ up to a measure zero set. \qed

\end{enumerate}