\section{Data-Driven Certificates}
\label{sec:learn_certs}

In some cases, one may have access to the underlying dynamics $f$, in which case it is possible to directly find a certificate that satisfies the relevant criteria $\psi^s$, $\psi^{\Delta}$.
However, in many real-world scenarios, access to the true dynamics requires a complete model of the physics of the system, and may not always be possible.
Instead, we take a data-driven approach to synthesize a certificate based only on available trajectories/signatures of that system.

We denote by $(X_I,\mathcal{F},\mathbb{P})$ a probability space, where $\mathcal{F}$ is a $\sigma$-algebra and $\mathbb{P}\colon \mathcal{F}\rightarrow[0,1]$ is a probability measure on our set of initial states $X_I$.
Then, the initial state of our system is randomly distributed according to $\mathbb{P}$.

To obtain our sample set, we consider $N$ initial conditions, according to probability distribution $\mathbb{P}$, namely 
$
    \{x^i(0)\}_{i=1}^N \sim \mathbb{P}^N,$
where we assume that all samples are independent and identically distributed (i.i.d.).
Initializing the dynamics from each of these initial states, we unravel a set of trajectories $\{\xi^i\}_{i=1}^N$.  %$\{ \{x^i(k)\}_{k=0}^T\}_{i=1}^N$. 
Since there is no stochasticity in the dynamics, we can equivalently say that trajectories (generated from the random initial conditions) are distributed according to the same probabilistic law; hence, with a slight abuse of notation, we write $\xi\sim \mathbb{P}$.
In the case of a stochastic dynamical system, the vector field would depend on some additional disturbance vector; our subsequent analysis will remain valid with $\mathbb{P}$ being replaced by the probability distribution that captures both the randomness of the initial state and the distribution of the disturbance. 
 We impose the following assumption.
\begin{assum}[Non-concentrated Mass]\label{ass:non-conc_mass}
	Assume that $\mathbb{P}\{\xi \}=0$, for any $\xi \in \Xi$.
\end{assum}

\subsection{Problem Statement}
\label{sec:learn_certs:data}

Since we are now dealing with a sample-based problem, we will be constructing probabilistic certificates and hence probabilistic guarantees on the satisfaction of a given property. We will present our results for a generic property $\phi \in \{\phi_{\mathrm{reach}}, \phi_{\mathrm{safe}}, \phi_{\mathrm{RWA}}\}$ and associated certificate conditions $\psi^s, \psi^{\Delta}$. 

Denote by $V_N$ a certificate of property $\phi$, we introduce the subscript $N$ to emphasize that this certificate is constructed on the basis of sampled trajectories $\{\xi^i\}_{i=1}^N$.

\begin{prob}[Probabilistic Property Guarantee]\label{prob:guarantees}
   Consider $N$ sampled trajectories, and fix a confidence level $\beta \in (0,1)$. We seek a property violation level $\epsilon \in (0,1)$ such that 
    \begin{align}
       \mathbb{P}^N &\big\{ \{\xi^i\}_{i=1}^N \in \Xi^N:~ \nonumber \\
       &\mathbb{P}\{\xi \in \Xi \colon V_N \not\models \psi^s \wedge \psi^\Delta(\xi)\} \leq \epsilon \big \} \geq 1-\beta. \label{eq:prop_prob}
    \end{align}
\end{prob}

Addressing this problem allows us to provide guarantees even if part of the initial set does not satisfy our specification.
Our statement is in the realm of probably approximately correct (PAC) learning: the probability of sampling a new trajectory $\xi \sim \mathbb{P}$ failing to satisfy our certificate condition is itself a random quantity depending on the samples $\{\xi^i\}_{i=1}^N$, and encompasses the generalization properties of a learned certificate $V_N$. It is thus distributed according to the joint probability measure $\mathbb{P}^N$, hence our results hold with some confidence $(1-\beta)$.

Providing a solution to Problem \ref{prob:guarantees} is equivalent to determining an $\epsilon \in (0,1)$, such that with confidence at least $1-\beta$, the probability that $V_N$ does not satisfy the condition $\psi^s \wedge \psi^\Delta(\xi)$ for another sampled trajectory $\xi \in \Xi$ is at most equal to that $\epsilon$. As such, with a certain confidence, a certificate $V_N$ \emph{trained} on the basis of $N$ sampled trajectories, will remain a valid certificate with probability at least $1-\epsilon$. Therefore, we can argue that  $V_N$ is a \emph{probabilistic} certificate.

\subsection{Probabilistic Guarantees}

We now provide a solution to Problem \ref{prob:guarantees}. 
To this end, we refer to a mapping $\mathcal{A}$ such that $V_N = \mathcal{A}(\{\xi^i\}_{i=1}^{N})$ as an algorithm that, based on $N$ samples, returns a certificate $V_N$. Our main result will apply to a generic algorithm as long this exhibits certain properties that will be outlined as assumptions below. In Section \ref{sec:training} we provide a specific synthesis procedure through which $\mathcal{A}$ (and hence the certificate $V_N$) can be constructed, and show that this algorithm satisfies the considered properties.

The following definition constitutes the backbone of our analysis. The notion introduced below appears with different terms in the literature; we adopt the terminology introduced in \cite{DBLP:journals/jmlr/CampiG23,DBLP:journals/tac/MargellosPL15} (adapted to our purposes) to align with the statistical learning literature. 



\begin{defn}[Compression Set]\label{def:compress}
Fix any $\{\xi^i\}_{i=1}^{N}$, and let $\mathcal{C}_N \subseteq \{\xi^i\}_{i=1}^{N}$ be a subset of the samples with cardinality $C_N = |\mathcal{C}_N| \leq N$.
Define $V_{C_N} = \mathcal{A}(\mathcal{C}_N)$.
We say that $\mathcal{C}_N$ is a compression of $\{\xi^i\}_{i=1}^{N}$ for algorithm $\mathcal{A}$, if
    \begin{equation}
    \begin{aligned}
		V_{C_N} = \mathcal{A}(\mathcal{C}_N) = \mathcal{A}(\{\xi^i\}_{i=1}^{N}) = V_N.
    \end{aligned}
    \end{equation}
\end{defn}
Notice the slight abuse of notation, as the argument of $\mathcal{A}$ might be a set of different cardinality; in the following, its domain will always be clear from the context. 

Figure \ref{fig:Compression} illustrates Definition \ref{def:compress} pictorially. 
It should be noted that compression set cardinalities may be bounded \emph{a priori}~\cite{DBLP:journals/siamjo/CampiG08}, that is, without knowledge of the sample-set, or obtained \emph{a posteriori}, and hence depending on the given set $\{\xi^i\}_{i=1}^{N}$~\cite{DBLP:journals/mp/CampiG18}. 
Trivially, we could take a compression set as the entire sample set, resulting in a trivial risk upper bound of 1. 
    However, it is of benefit to determine a compression set with small (ideally minimal) cardinality, as the smaller $C_N$ is, the smaller risk we can guarantee. 
In this paper we are particularly interested in a posteriori results, since we solve a non-convex problem we cannot in general provide a non-trivial bound to the cardinality of the compression set a priori~\cite{DBLP:journals/tac/CampiGR18}.
Therefore, we introduce the subscript $N$ in our notation for $\mathcal{C}_N$ (set) and $C_N$ (corr. cardinality), respectively.

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=0.35cm ]
        \node (start) {$\xi^1$};
        \node (blank1) [below of=start] {\phantom{$\xi^N$}};
        \node (next) [below of= blank1] {$\vdots$\phantom{$\xi^i$}};
        \node (blank2) [below  of= next] {\phantom{$\xi^N$}};
        \node (final) [below of =blank2] {$\xi^N$};
        
        \node (alg1) [startstop, right=0.7cm of next] {$\mathcal{A}$};
        
        \node (out1) [right=0.7cm of alg1] {$V_N$};
        
        \node (equals) [right = 0.1cm of out1] { $=$};

        \node (out2) [right = 0.1cm of equals] {$V_{C_N}$};

        \node (alg2) [startstop, right=0.7cm of out2] {$\mathcal{A}$};
        
        \node (next2) [right=0.7cm of alg2] {$\mathcal{C}_N$\phantom{$\xi^i$}};
        \node (blank3) [above of=next2] {\phantom{$\xi^N$}};

        \node (blank4) [below of=next2] {\phantom{$\xi^N$}};

        \draw [arrow] (alg1) -- (out1);
        \draw [arrow] (alg2) -- (out2);
        
        \draw [arrow] (start.east) -- (alg1.west|-start.east);
        \draw [arrow] (blank1.east) -- (alg1.west|-blank1.east);
        \draw [arrow] (next.east) -- (alg1.west|-next.east);
        \draw [arrow] (blank2.east) -- (alg1.west|-blank2.east);
        \draw [arrow] (final.east) -- (alg1.west|-final.east);
        \draw [arrow] (next2.west) -- (alg2.east|-next2.west);
        
        \end{tikzpicture}
    \caption{Pictorial illustration of the compression set notion of Definition \ref{def:compress}.}
    \label{fig:Compression}
\end{figure}

The theorem below provides probabilistic guarantees that are valid irrespective of the cardinality of the underlying compression set. 
However, the quality of these bounds depends significantly on this cardinality, resulting in progressively sharper probabilistic bounds as the compression set cardinality decreases. 
In Algorithm \ref{algo:main} we propose a mechanism to obtain non-trivial compression sets, while avoiding computationally expensive procedures. This can be thought of as a by-product of the certificate synthesis procedure of Section \ref{sec:training}, and constitutes per se a novel  contribution of this work. 

For the guarantees we introduce in the sequel to hold, that the algorithm $\mathcal{A}$ must satisfy the following properties (adapted from \cite{DBLP:journals/jmlr/CampiG23}; note that in \cite{DBLP:journals/jmlr/CampiG23} a non-concentrated mass property is also imposed, which here appears separately as Assumption \ref{ass:non-conc_mass}). 
We later demonstrate that our proposed algorithm satisfies these.

\begin{assum}[Properties of $\mathcal{A}$]\label{ass:alg_prop}
Assume that algorithm $\mathcal{A}$ exhibits the following properties:
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
	\item \emph{Preference:} For any pair of multisets $\mathcal{C}_1$ and $\mathcal{C}_2$ of elements of $\{\xi^i\}_{i=1}^N$, with $\mathcal{C}_1 \subseteq \mathcal{C}_2$, if  $\mathcal{C}_1$ does not constitute a compression set of $\mathcal{C}_2$ for algorithm $\mathcal{A}$, then $\mathcal{C}_1$ will not constitute a compression set of $\mathcal{C}_2 \cup\{\xi\}$ for any $\xi \in \Xi$.
	\item \emph{Non-associativity:} Let $\{\xi^i\}_{i=1}^{N+\bar{N}}$ for some $\bar{N} \geq 1$. If $\mathcal{C}$ constitutes a compression set of $\{\xi_i\}_{i=1}^{N} \cup \{\xi\}$ for all $\xi \in \{\xi^i\}_{i=N+1}^{N+\bar{N}}$ for algorithm $\mathcal{A}$, then $\mathcal{C}$ constitutes a compression set of $\{\xi_i\}_{i=1}^{N+\bar{N}}$ (up to a measure-zero set).
\end{enumerate}
\end{assum}

We are now able to state the main result of this section.

\begin{thm}[Probabilistic Guarantees]
\label{thm:Guarantees}
Consider any algorithm $\mathcal{A}$ satisfying Assumption \ref{ass:alg_prop} such that $V_N = \mathcal{A}(\{\xi^i\}_{i=1}^{N})$, with trajectories $\{\xi^i\}_{i=1}^{N}$ generated in an i.i.d. manner from a distribution satisfying Assumption~\ref{ass:non-conc_mass}. 
Fix $\beta \in (0,1)$, and for $k<N$, let
let $\varepsilon(k,\beta,N)$ be the (unique) solution to the polynomial equation in the interval $[k/N,1]$
    \begin{align}
               \frac{\beta}{2N} \sum_{m=k}^{N-1}&\frac{\binom{m}{k}}{\binom{N}{k}}(1-\varepsilon)^{m-N} \nonumber \\
               &+\frac{\beta}{6N}\sum_{m=N+1}^{4N}\frac{\binom{m}{k}}{\binom{N}{k}}(1-\varepsilon)^{m-N} = 1,
     \end{align}
    while for $k=N$ let $\varepsilon(N,\beta,N) =1$. We then have that
    \begin{align}
	    \label{eq:cert_bound}
        &\mathbb{P}^N\big\{ \{\xi^i\}_{i=1}^N \in \Xi^N:~  \\
        &\mathbb{P}\{\xi \in \Xi\colon V_N \not\models \psi^s \wedge \psi^\Delta(\xi)) \} \leq \varepsilon(C_N,\beta,N)\big\} \nonumber \geq 1-\beta.
    \end{align}
\end{thm}
\textbf{Proof}
Fix $\beta \in (0,1)$, and for each $\{\xi^i\}_{i=1}^N$ let $\mathcal{C}_N$ be a compression set for algorithm $\mathcal{A}$. Moreover, note
that letting $V_N = \mathcal{A}(\{\xi^i\}_{i=1}^{N})$ we construct a mapping from samples $\{\xi\}_{i=1}^N$ to a decision, namely, $V_N$, while we impose as an assumption that this mapping satisfies the conditions of Assumption \ref{ass:alg_prop}. 

This framework directly fits into the setting of \cite[Theorem~7]{DBLP:journals/jmlr/CampiG23}, which implies that with confidence at least $1-\beta$, the probability that for a new $\xi \in \Xi$ the compression set changes, is at most $\epsilon(\mathcal{C}_N,\beta,N)$, i.e., 
\begin{align}
\mathbb{P}\{\xi \in \Xi\colon  \mathcal{C}_N^{+} \neq \mathcal{C}_N\} \leq \varepsilon(C_N,\beta,N), \label{eq:proof_thm}
\end{align}
where $\mathcal{C}_N^{+}$ denotes a compression set for algorithm $\mathcal{A}$ when fed with $\{\xi\}_{i=1}^N \cup \{\xi\}$.
However, we then have that
\begin{align}
\{\xi \in \Xi &\colon V_N \not\models \psi^s \wedge \psi^\Delta(\xi)) \} \nonumber \\
&\subseteq \{\xi \in \Xi \colon V_N \neq \mathcal{A}(\{\xi\}_{i=1}^N \cup \{\xi\}) \} \nonumber \\
&= \{\xi \in \Xi \colon \mathcal{A}(\mathcal{C}_N) \neq \mathcal{A}(\mathcal{C}_N^{+})\} \nonumber \\
&\subseteq \{\xi \in \Xi \colon \mathcal{C}_N^{+} \neq \mathcal{C}_N \}, \label{eq:proof_thm1}
\end{align}
where the first inclusion is since for any $\xi \in \Xi$ for which $V_N$ no longer satisfies the certificate condition 
$(\psi^s \wedge \psi^\Delta(\xi))$, we must have that the certificate changes, i.e., $\mathcal{A}(\{\xi^i\}_{i=1}^N \cup \{\xi\})$ (the output/certificate of our algorithm when fed with one more sample) is different from $V_N$. 
Notice that the opposite statement does not always hold, having a different certificate does not necessarily mean the old one violates an existing condition for a new $\xi \in \Xi$. 

The following equality holds as $V_N = \mathcal{A}(\mathcal{C}_N)$, $\mathcal{A}(\{\xi\}_{i=1}^N \cup \{\xi\}) = A(\mathcal{C}_N^{+})$, by definition of a compression set.

Finally, the last inclusion stands since any $\xi \in \Xi$ for which $\mathcal{A}(\mathcal{C}_N) \neq \mathcal{A}(\mathcal{C}_N^{+})$, should be such that $\mathcal{C}_N^{+} \neq \mathcal{C}_N$. 
The opposite direction does not always hold, as if 
$\mathcal{C}_N^{+} \supset \mathcal{C}_N$ then we get another compression set of higher cardinality, and hence we may still have $\mathcal{A}(\mathcal{C}_N) = \mathcal{A}(\mathcal{C}_N^{+})$. By \eqref{eq:proof_thm} and \eqref{eq:proof_thm1}, \eqref{eq:cert_bound} follows, concluding the proof.
 \qed

By the implication in Definition \ref{prob:property_cert}, and under the same conditions as those in Theorem~\ref{thm:Guarantees}, we have the following corollary, which relates correctness of the certificate to bounds on the property under study. 
\begin{cor}[Bounds on Property Satisfaction]\label{corr:prop}
Suppose \eqref{eq:cert_bound} is satisfied for the calculated risk $\epsilon(C_N,\beta,N)$, we can guarantee that 
\begin{equation}
    \begin{aligned}
	\mathbb{P}^N&\big\{\{\xi^i\}_{i=1}^N\in \Xi^N\colon\\&\mathbb{P}\{\xi \in \Xi \colon \neg \phi(\xi)\} \leq \epsilon(C_N,\beta,N)\} \geq 1-\beta. \label{eq:prop_viol}
\end{aligned}
\end{equation}
\end{cor}
That is, if there exists a certificate $V_N$ that satisfies \eqref{eq:cert_bound} up to a level $\epsilon$, then the property $\phi(\xi)$ is satisfied up to the same level (with some confidence). This confidence refers to the selection of sampled trajectories we observed.

The following remarks are in order.
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
	\item Notice that Theorem \eqref{thm:Guarantees} involves evaluating $\varepsilon(k,\beta,N)$ at $k=C_N$, i.e., at the cardinality of the compression set. As such, with confidence at least $1-\beta$ with respect to the choice of the trajectories $\{\xi_i\}_{i=1}^N$, the probability that $V_N$ is not a valid certificate when it comes to a new trajectory $\xi$, is at most $\varepsilon(C_N,\beta,N)$. Due to the dependency of $\varepsilon$ on the samples (via $C_N$), the proposed  probabilistic bound is \emph{a posteriori} as it is adapted to the samples we ``see''. As a result, this is often less conservative compared to \emph{a priori} counterparts.
    \item  
    For cases where algorithm $\mathcal{A}$ takes the form of an optimization program that is convex with respect to the parameter vector, determining non-trivial bounds on the cardinality of compression sets is possible \cite{DBLP:journals/siamjo/CampiG08,DBLP:journals/tac/MargellosPL15}, as this is related to the notion of support constraints in convex analysis.
    However, determining compression sets of low cardinality (necessary for small risk bounds) becomes a non-trivial task if $\mathcal{A}$ involves a non-convex optimization program and/or is iterative (as Algorithm \ref{algo:sub}). 
    In such cases, samples that give rise to inactive constraints may still belong to a compression set, as they may affect the optimal parameter implicitly. 
    A direct way of determining the minimal compression set is to resolve the problem with every subset of the samples \cite{DBLP:journals/mp/CampiG18,DBLP:journals/tac/CampiGR18}. 
    Computationally, however, this would be an intense procedure, often prohibitive due to its combinatorial nature \cite{DBLP:journals/tac/FeleM21}. 
\item An alternative procedure could be to use sampled trajectories and check directly whether a property is satisfied for them (by checking the property definition, rather than using the associated certificate's conditions). 
	This is a valid alternative but has the drawback of not providing a certificate $V_N$, but simply provides an answer as far as the property satisfaction is concerned.
    This direction is pursued in \cite{DBLP:journals/sttt/BadingsCJJKT22}; we review this result and compare with our approach in Section \ref{sec:related:direct_prop}. Note that
	having a certificate is interesting per se, and opens the road for control synthesis; which we aim to pursue in future work. 
\end{enumerate}