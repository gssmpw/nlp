\section{Evaluation}\label{sec:eval}

\subsection{Experimental Setup}\label{sec:eval:setup}

\textbf{Implementation.}
\xxx is implemented using PyTorch~\cite{paszke2019pytorch}.
We skip overlapping sparse collective communication with expert execution, as there exists a few attempts~\cite{he2022fastermoe,hwang2022tutel} to overlap it with \collatoa communication, and it is orthogonal to our design.
As a prototype system, the two sparse collectives in \xxx are implemented with NCCL~\cite{nccl2020docs} by leveraging group calls to simultaneously schedule a series of \collbc and \collrd operations.
While more efficient algorithms for sparse collectives could theoretically exploit the sparsity of data distribution and network topology, our straightforward implementation is sufficiently efficient to meet the upper bound analysis discussed in \refsec{sec:design:spcoll}, and we leave an optimized implementation for future work.
\newline
\textbf{Testbeds.}
We conducted experiments on two cloud clusters:
Cluster A with 4 AWS p3dn.24xlarge nodes, each having 8 NVIDIA V100-32G GPUs connected via 300\,GB/s NVLink~\cite{nvlink2022docs}, and nodes linked by a 100\,Gbps network; and
Cluster B with 4 AWS p4d.24xlarge nodes, each containing 8 NVIDIA A100-40G GPUs interconnected using 600\,GB/s NVSwitch~\cite{nvswitch2022docs}, and nodes connected through a 400\,Gbps network.


\textbf{Models and Metrics.}
We evaluate the training workloads of the sparse counterparts of two popular transformer-based language models, GPT-3~\cite{brown2020language} and BERT~\cite{Devlin2019BERTPO}, with 4 representative model sizes and architectures to showcase the effectiveness of \xxx, as detailed in Table \ref{tab:models}.
To sparsify the original models, we replace the feed-forward networks (FFNs)~\cite{vaswani2017attention} in both models with MoE layers, where experts are still FFNs with the same model dimension $d_{model}$ and the FFN hidden dimension $d_{ffn}$ set to twice $d_{model}$.
We select the widely used GShard~\cite{lepikhin2020gshard} Top-2 gating mechanism for assigning tokens to experts.
Experiments on varying sequence lengths $SeqLen$ showcase the performance of \xxx under different opportunities for overlapping parameter materialization communication and Attention~\cite{vaswani2017attention} computation.

\input{tab/tab.tex}


\textbf{Baselines.}
We compare \xxx with several baseline systems.
FasterMoE~\cite{he2022fastermoe} is an early effort to mitigate straggler effects in MoE training by replicating overloaded experts to every device.
SmartMoE~\cite{zhai2023smartmoe} exchanges experts between devices to balance device load, with its strategy relying on the presence of multiple experts on each device.
FlexMoE~\cite{nie2023flexmoe} supports the most comprehensive expert rearrangement by allowing both replication and relocation of experts.
Since FlexMoE does not have an open-source implementation, we implemented its proposed rearrangement strategy based on the description in the paper.
To ensure fairness in comparison, we manually tune hyper-parameters (e.g., rearrangement frequencies and reserved memory) of the baseline systems to achieve good performance.
Megatron-LM~\cite{shoeybi2019megatron} is used as the training framework, and the baseline systems are employed solely to optimize the training of MoE layers.
In each set of comparative experiments, we used the largest batch size that did not cause an out-of-memory (OOM) error in any system.
\xxx's re-sharding is triggered at a low frequency of every 100 iterations, executing only when shards change, leveraging its insensitivity to frequency.
Unless otherwise specified, \xxx's re-materialization feature is not switched on by default.


\subsection{End-to-End Performance}\label{sec:eval:end2end}

To assess the performance of \xxx, we evaluate the overall training speedup of four MoE models on both clusters.
Expert parallelism (EP) is used as a baseline for calculating the relative performance improvement.

\reffig{fig:end2end_a} illustrates the end-to-end performance of training four MoE models on Cluster A.
The experiments are conducted in a weak scaling manner, with the number of experts set to 32 for the 16 GPU experiments.
Across all cases, \xxx consistently achieves the highest speedup compared to the baseline systems.
The speedup exhibits an increasing trend with the number of GPUs.
At the smaller scale of 16 GPUs, \xxx achieves a 1.40 - 1.58$\times$ speedup, while scaling to 32 GPUs yields a 1.34 - 1.78$\times$ speedup.
The higher speedup at the larger scale can be attributed to the significantly more expensive \collatoa, which leads to performance degradation in EP.
In contrast, \xxx effectively mitigates this cost through efficient placement.
Compared to the best performance of all baseline systems, \xxx achieves a geo-mean speedup of 1.645$\times$ with 16 GPUs and 2.05$\times$ with 32 GPUs.

To further investigate the performance characteristics, we conduct experiments on Cluster B, which offers more powerful computational capabilities and higher communication bandwidth compared to Cluster A.
In \reffig{fig:end2end_b}, \xxx obtains a 1.70 - 1.26$\times$ speedup relative to EP.
The lower communication bandwidth of Cluster A exacerbates the straggler effect of \collatoa, resulting in more pronounced performance gains for \xxx.
\xxx achieves a substantial geo-mean speedup of 2.945$\times$ on Cluster B compared to the baseline systems, surpassing the speedup observed on Cluster A.
This reveals the consistent superiority of \xxx over the baseline systems across various model architectures and cluster configurations, attributing to the \yyy paradigm which maximizes load balancing opportunities with minimized system overhead.
Systems with restricted rearrangement strategies (\eg, SmartMoE can only exchange experts between devices) are unable to fully unlocking the potential of expert placements to mitigate straggler effects, resulting in suboptimal performance.
The load balancing returns of these systems sometimes cannot offset the rearrangement overhead, resulting in slower training than EP.




\input{figs/eval/end2end_b_fig.tex}

\subsection{Fine-Grained Performance Breakdown}

In this section, we conduct an in-depth analysis to examine how \xxx optimizes the training process and identify the critical performance costs.

\input{figs/eval/layerwise_fig.tex}

\reffig{fig:layerwise-speedup} illustrates the layer-wise speedup of \xxx when training GPT-MoE-S on Cluster B.
\xxx consistently outperforms EP across all layers, yielding a 2.8 - 18.8$\times$ speedup, with a geo-mean of 11.87$\times$.
The figure reveals the significant variations in degrees of load imbalance across layers, resulting in varying execution time of different MoE layers under EP.
Under this situation, systems that allocate identical memory resources for load balancing in each MoE layer (e.g., FlexMoE) may lead to inefficient resource allocation for expert placement across layers, impacting overall training performance.
\xxx's heterogeneous sharding effectively utilizes memory resources across MoE layers, enabling heterogeneous memory allocation for expert placement in each layer without incurring additional memory overhead.

\input{figs/eval/perf_breakdown_fig.tex}

\reffig{fig:perf_breakdown} breaks down the performance critical path of baseline systems and \xxx of training BERT-MoE-Deep on Cluster B.
FasterMoE fuses its computation, \collatoa communication, and rearrangement communication into a single kernel, labeled as FusedKernel (Comp+A2A+Rearr) in the figure.
As illustrated in the figure, \collatoa communication (A2A) dominates the MoE training latency of all systems.
\xxx attains the lowest \collatoa communication time with its topology-aware algorithm designs, scheduling \collsag (SpAG) and \collsrs (SpRS) efficiently to maximally mitigate communication stragglers, resulting in a 12.3X reduction in A2A time compared to EP.
Compared to FlexMoE's rearrangement overhead (Rearr), \xxx demonstrates a smaller overhead for its sparse collectives due to its reduced communication volume (from communicating optimizer states to only parameters) and overlapping with previous Attention computation.
\xxx-RM represents \xxx with releasing and re-materialization of parameters enabled.
\xxx-RM incurs additional overhead due to re-materialization, resulting in a 3.6$\times$ increase in the sparse collective communication overhead, while still outperforming baseline systems by 1.4$\times$.








\input{figs/eval/mem_breakdown_fig.tex}

We further investigated the peak memory usage of different systems, focusing on the memory consumption of optimizer states, gradients, and parameters, as shown in \reffig{fig:mem_breakdown}.
We omitted the memory footprint of activations due to the dynamic batch sizes in MoE training.
SmartMoE consumes the least memory, comparable to EP, but fails to achieve satisfactory performance improvements, as a result of underperforming expert placement.
FlexMoE exhibits the highest memory consumption, requiring 83\% more memory than \xxx to accommodate experts on each device, indicating memory-inefficiency in employing an expert placement.
With sufficient memory, \xxx utilizes the most memory for parameters (5.73$\times$ compared to EP) to materialize the most load-balancing expert placement, resulting in a 64\% increase in total memory usage compared to EP.
\xxx-RM significantly reduces the additional memory footprint for materialized parameters (by 90.2\% compared to \xxx) by releasing the materialized parameters after use, leading to consuming only 11.6\% more total memory than EP.








\input{figs/eval/mbs_fig.tex}

\subsection{Effectiveness of Components}

We evaluate the effectiveness of \xxx's re-materialization for reducing the memory footprint of materializing expert placement.
Through re-materialization, \xxx only need to reserve memory for one MoE layer's placement materialization during training, significantly reducing the additional memory overhead introduced by expert placement.
As shown in \reffig{fig:rematerialization}, \xxx-RM exhibits a 7.5\% to 16.9\% slowdown compared to \xxx, indicating that re-materialization introduces additional overhead.
However, \xxx-RM is the only strategy that can scale the batch size to 6 while still maintaining performance advantages over the baselines.



\input{figs/eval/component_fig.tex}

We evaluate the effectiveness of \xxx's components (\reffig{fig:component}).
\xxx's heterogeneous sharding achieves consistent speedups (1.36, 1.41, 1.34 and 1.42$\times$) across different re-sharding intervals ranging from 10 to 100 iterations.
This demonstrates the insensitivity of heterogeneous sharding to the trigger frequency, allowing \xxx to maintain the benefits of heterogeneous sharding at a lower re-sharding frequency, thereby minimizing the overhead of re-sharding and freeing \xxx from fine-tuning this interval for optimal performance.
The combination of materialization and heterogeneous sharding is crucial for efficient load balancing, achieving 3.32$\times$ and 1.27$\times$ speedups over \xxx with only heterogeneous sharding and materialization enabled, respectively.
