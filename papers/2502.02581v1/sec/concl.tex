\section{Conclusion}\label{sec:concl}


We put forward \yyy, a MoE training paradigm for efficient parallelization under imbalanced expert loads, and realize it with \xxx.
\yyy addresses the inefficiency in MoE training caused by load imbalance by sparsely materializes MoE parameters on demand to construct timely expert placements with sparse collective communication with minimal memory overhead.
We demonstrate that \xxx improves the performance of MoE parallel training by up to 3.54$\times$.
Exploration of efficient sparse collective communication is an interesting direction for future work.




