\input{figs/arch/arch_fig.tex}

\section{\xxx System}\label{sec:arch}


\subsection{Architecture Overview}\label{sec:arch:overview}

\reffig{fig:arch} illustrates the \xxx architecture, where each device launches a runtime consisting of an executor, scheduler, dispatcher, and communicator.
The executor, the main process for each device, controls the \yyy workflow and interacts with all components.
In the sharding phase, it queries the scheduler for an MoE layer sharding plan and accommodates the MoE shards containing relevant expert parameters and optimizer states on the device.
In the materialization phase, it invokes the scheduler for a sparse materialization plan, launched through the communicator as sparse collectives.
Additionally, when an MoE gate makes a token assignment decision in each layer, the executor queries the dispatcher for each token's destination device, as the same expert can be available on multiple devices.

The scheduler generates expert placement plans based on the expert load distribution to mitigate straggler effects.
It implements two topology-aware algorithms: heterogeneous sharding (\refsec{sec:arch:o3}) and sparse materializing (\refsec{sec:arch:o2}) for the two phases of \yyy, respectively.
The dispatcher determines each token's destination device based on token assignment of MoE gates and the current materialized MoE layer parameter placement (\refsec{sec:arch:dispatch}).
The communicator handles assigned communication tasks by maintaining a queue and scheduling them to the runtime communication library (\eg, NCCL~\cite{nccl2020docs}), executing the dispatching plan as an \collatoa collective.






\subsection{Sparse materialization}\label{sec:arch:o2}

In the materialization phase of each MoE layer, \yyy requires a sparse materialization plan $\set{P}'$ as the target placement for \collsagop{\set{P}}{\set{P}'} to perform sparse communication of MoE layer parameters.
Here, the chunk placement is $\set{P} = \set{E} \times \set{D}$, where $\setE = \{E_1, E_2, \ldots\}$ represents the parameters for all experts in an MoE layer as the collective logical buffer, with each expert serving as a chunk.

\refalgo{algo:o2} presents the topology-aware \textit{sparse materialization} algorithm, used by \xxx's scheduler to heuristically search for a near-optimal parameter placement under two system constraints: overlap degree $t$ and memory capacity $m$.
The overlap degree $t$ represents the maximum number of experts that can be materialized on other devices with the communication overhead completely hidden in attention layers. 
According to \refeq{eq:vol_spcoll}, it can be calculated by $t = T_\text{non-MoE} \cdot \frac{\bw}{\text{expert\_size}}$, where $T_\text{non-MoE}$ is computation latency of previous non-MoE layers (e.g., the attention layer) and \texttt{expert\_size} is an expert's parameter byte size.
Critically, \texttt{\bw} reflects the cluster's interconnect topology.
When the cluster features heterogeneous interconnects with significant bandwidth differences between inter-node and intra-node communication, \texttt{\bw} represents the inter-node bandwidth, as the algorithm prioritizes minimizing cross-node communication.
If the interconnect is homogeneous, \texttt{\bw} reflects the uniform inter-device bandwidth.
The memory capacity denotes the maximum number of experts that can be materialized on each device's available memory. 
These two integers are profiled by the scheduler and passed to the algorithm as input. 
Expert load $F$ is estimated using a sliding window average over the latest $w$ iterations (\xxx uses $w=5$).

The two outermost branches in \refalgo{algo:o2} represent different conditions of system constraints. 
When the overlap degree is less than or equal to the memory capacity (lines~\ref{algo:o2:br1start} to \ref{algo:o2:br1end}), the algorithm materializes as many overloaded experts as possible on all devices within the overlappable time. 
Otherwise
(lines~\ref{algo:o2:br2start} to \ref{algo:o2:br2end}), the algorithm sparsely materializes experts on devices according to their load distribution. Experts with higher loads are materialized on more devices (line~\ref{algo:o2:slot_assignment}), prioritizing nodes that do not already have the expert parameters materialized (line~\ref{algo:o2:topo_awareness}).
This topology-aware design, which considers the potential bandwidth disparities between inter-node and intra-node links, helps mitigate \collatoa straggler effects due to inter-node communication congestion.

The sparse materialization can include a \textit{calibration} stage additionally, occurring immediately after the MoE gate generates the token assignment decision. Since the overlapped sparse materialization is based on an estimated expert load distribution, the current distribution (\ie the real-time token assignment decision) can still vary due to the stochastic nature of training. The \textit{calibration} re-runs \refalgo{algo:o2} with the latest expert loads and remaining memory capacity to determine if an additional \collsag can be executed to further reduce load imbalance. If the calibrated placement results in a lower latency, considering the additional communication overhead on the training critical path, the scheduler will accept and return the placement plan for the communicator to execute before initiating token dispatching.

\input{algo/o2.tex}

\subsection{Heterogenous Sharding}\label{sec:arch:o3}

The design of \xxx's sparse materialization primarily benefits the overloaded experts, as their placements are more likely to be materialized on multiple devices. However, the placement of underloaded experts can also be optimized to further reduce straggler effects, particularly when training with multiple nodes. 
For instance, if a node contains MoE shards with only underloaded experts, the inbound bandwidth of this node may be oversubscribed by \collatoa for these crowded underloaded experts to receive their tokens, as the node is likely the sole destination for these tokens.

\textit{Heterogeneous sharding} algorithm is introduced in \xxx for sharding MoE layers across devices in the sharding phase, determining better placements for the underloaded experts.
The algorithm is \textit{heterogeneous} since it allows an MoE shard to have an arbitrary number of experts (ranging from $0$ to $|\set{E}|$) while maintaining memory balance across devices, as depicted in \reffig{fig:hetero}.
In \xxx, MoE layers are initialized using homogeneous sharding (\ie even sharding), and periodically re-sharded in heterogenous manners during training.

\input{figs/o3/hetero_fig.tex}

\refalgo{algo:o3} presents the sharding algorithm. It schedules all MoE layers in the PTM collectively to ensure even memory demand for sharding all layers across devices. As the algorithm involves cross-layer scheduling, some variables in the algorithm pseudocode may have the superscript ``g'' to indicate that they cover all MoE layers (\eg $\gset{E}$), while the index ``$l$'' is used to denote variables specific to a particular MoE layer $l$ (\eg $\set{E}_l$).
It returns the sharding plan for all MoE layers in the form of $\gset{P} = \{\set{P}_0, \set{P}_1, \cdots, \set{P}_{L}\}$, where each element is an expert placement for the parameters and optimizer states of the corresponding MoE layer.

Experts first partitioned into two disjoint sets layer-wisely (from line~\ref{algo:o3:partition-start} to line~\ref{algo:o3:partition-end}): $\set{J}$ are overloaded experts that can be selected by the sparse materialization, and $\set{J}'$ contains the remaining experts that are not ``overlappable''. 
The algorithm initializes same number of slots per device (line~\ref{algo:o3:slots}) for plugging in experts while ensuring consistent memory demand across devices. 
Experts in $\set{J}'$ are scheduled firstly, layer by layer 
(from line~\ref{algo:o3:underloaded-start} to line~\ref{algo:o3:underloaded-end})
, prioritizing layers with the most overloaded expert.
For each expert, the algorithm first attempts to find the least-loaded node. If multiple nodes have the same lowest load, the node with fewer available slots is prioritized. 
The algorithm then tries to find the least-loaded device on the selected node, using the same priority rule. 
The expert is then assigned to the device, and the available slots on the device are decreased. 
Finally, the algorithm fills the remaining slots with experts from $\set{J}$ (line~\ref{algo:o3:overlappable}). 



It is important to note that unlike sparse materialization, the heterogeneous sharding of \xxx introduces re-sharding latency to the training critical path, which may seem to result in the timeliness challenge. However, we argue that re-sharding can be performed at a low frequency, amortizing the overhead over iterations. Since it focuses on the placement of underloaded experts, which are trained with fewer tokens per iteration, the MoE gate will have gradients with smaller magnitudes corresponding to these experts. This implies that the loads of underloaded experts change slowly (confirmed by \reffig{fig:trace}). Consequently, re-sharding can be triggered less frequently, extracting the last bit of performance improvement from the \yyy sharding design.

\input{algo/o3.tex}

\subsection{Token Dispatching}\label{sec:arch:dispatch}

\input{figs/eval/end2end_a_fig.tex}

With sparse materialization, an expert's parameters may exist on multiple devices. Tokens assigned to this expert from all devices must select one of the devices where the expert is materialized to be dispatched to. \xxx employs a topology-aware algorithm in its dispatcher to generate a token dispatching plan.
The algorithm aims to minimize inter-node communication, as inter-node bandwidths (\eg NICs~\cite{nvbandwidth2023}) are typically much lower than intra-node high-speed bandwidths (\eg NVLinks~\cite{nvlink2022docs}). 
If an expert is materialized on a device, all tokens assigned to that expert on the device are dispatched locally. Otherwise, the algorithm prioritizes devices within the same node as the token's destination device, only dispatching a token across nodes when no devices in the source node have the expert materialized. When performing inter-device dispatching, the algorithm evenly distributes the tokens among the selected devices.








