\section{Related Work}\label{sec:related}

\textbf{Load balancing in MoE training.}
The gating algorithms of MoE frequently leverage an auxiliary loss~\cite{lepikhin2020gshard}
during training to balance tokens across experts. Despite the adoption of
auxiliary loss, the issue of load imbalance still persists. Systems like GShard~\cite{lepikhin2020gshard}
and Switch Transformer~\cite{fedus2022switch} further limit the expert capacity (i.e., the number of
tokens an expert can process) and drop excess tokens for each expert. However,
MegaBlocks~\cite{gale2022megablocks} has revealed that token dropping degrades model quality and should be
avoided.
\newline
\textbf{MoE training systems.}
Tutel~\cite{hwang2022tutel} incorporates data and tensor model parallelism to scale up MoE training.
Lita~\cite{li2022lita} accelerates MoE training through prioritizing all-to-all communication.
Lazarus~\cite{wu2024lazarus} supports fault-tolerant MoE training via adaptive expert placement.
\xxx is orthogonal to these systems and can integrate their optimizations.
\newline
\textbf{Sparse Collective Communication.}
In distributed deep learning, sparse collectives are often combined with Top-k sparsification to reduce gradient communication~\cite{peng2024sparse, zhao2024spardl, li2022near, renggli2019sparcml, shi2019distributed}.
\xxx explores the sparse communication pattern in MoE load balancing for the first time and establishes the FSSDP paradigm to alleviate straggler effects in MoE training.

Recent works also explore the synthesis of collective algorithms (\ie, data routing plans) tailored to specific collectives and network topologies~\cite{cai2021msccl,shah2023taccl,kim2024tccl,wang2020blink,cowan2022gc3}.
These synthesizers can potentially be integrated into \xxx to dynamically generate efficient sparse collectives across iterations.
