\section{Introduction}\label{sec:intro}

Large pre-trained models (PTMs) have driven numerous breakthroughs in recent machine learning tasks~\cite{floridi2020gpt,alexey2020vit,liu2021swin}.
Larger PTMs possess superior modeling capacity for vast data~\cite{kaplan2020scaling,hoffmann2022chinchilla}.
However, training such extensive PTMs becomes extremely expensive, as the computational cost per input increases rapidly with the growing model size.

Mixture-of-Experts (MoE)~\cite{jacobs1991adaptive, shazeer2017outrageously} presents a sparse paradigm enhancing the cost-effectiveness of training PTMs.
Each MoE layer comprises multiple expert networks (\ie, experts), each tailored to learn a subset of inputs. 
A gate network serves as a router, conditionally assigning inputs to the most suitable experts.
MoE allows a model to scale to an outrageous size with the same computational cost per input, while achieving higher sample efficiency compared to its dense counterparts
~\cite{du2022glam, riquelme2021vmoe, fedus2022switch}.

Training MoE PTMs involves additional complexities due to their large scale and dynamic nature.
As MoE layers can easily exceed the memory capacity of a single accelerating device (\eg, a GPU), they are commonly trained with expert parallelism (EP)~\cite{shazeer2017outrageously},
where experts are evenly distributed across devices, and their inputs need to be dispatched across devices via \collatoa communication.
Moreover, the MoE gate is trained along with rest part of the model for effective data assignment among experts. 
As illustrated in \reffig{fig:trace}, the gating frequently evolves during training,
resulting in rapid fluctuations and imbalances in expert loads (\ie, the numbers of inputs assigned to different experts).

\input{figs/overlap/overlap_fig.tex}

EP brings \textit{straggler effects} to MoE training, significantly constraining the training throughput. As experts are trained in a distributed manner, imbalanced expert loads lead to varying computation and communication times across devices.
The most overloaded device (\ie, the one with the most expert inputs) determines the overall computation time of the MoE layer, while other devices will remain idle when waiting for it.
Similarly, the overloaded device tends to experience more inbound communication 
to receive expert inputs, resulting in communication bottlenecks.
Our evaluation of EP on an AWS V100 cluster reveals that compared to a balanced load distribution, imbalanced expert loads can significantly slow down the training performance by up to \imbalanceSlowdown.

To mitigate straggler effects in EP, state-of-the-art MoE training systems~\cite{nie2023flexmoe,zhai2023smartmoe,he2022fastermoe} mainly resort to \textit{expert rearrangement} strategies, which dynamically modify \textit{expert placement} (i.e., the presence of each expert on each device) to adapt to imbalanced and fluctuated expert loads. 
Initialized with EP, these systems trigger the rearrangement to transition between expert placements by relocating or replicating experts across devices to reduce the peak device load throughout the training, as depicted in \reffig[d]{fig:overlap}.
At the end of
each iteration, the devices with expert replicas use \collar to synchronize the gradients of these experts.%

Unfortunately, despite these efforts to mitigate straggler effects, expert rearrangement also come with certain costs, leading these systems to the following two challenges:

\textbf{Memory challenge (C1)}: A more load-balancing expert placement can be more memory-hungry. A system needs to reserve device memory in advance to accommodate newly received parameters and optimizer states of experts during rearrangement. When modifying expert placement, smaller reserved memory may hinder the system from considering candidate placements with larger memory footprints but balancing loads better.
Our experiments on \cite{nie2023flexmoe} shows that for \memorySpeedup speed up, 4 times more memory is required to be reserved for rearrangement. %

\textbf{Timeliness challenge (C2)}: For optimal training throughput, a trade-off exists between the timeliness of expert placement and the communication overhead incurred by frequent rearrangements.
To adapt to fluctuating expert loads over iterations (see \reffig{fig:trace}), a system needs to perform rearrangement more frequently to work under a placement timely for the current load distribution.
However, a higher rearrangement frequency can also lead to a higher amortized communication overhead for replicating and relocating experts between devices, potentially resulting in a higher overall latency (see \reffig[b]{fig:overlap}). The optimal rearrangement frequency may vary across different training scenarios, making it impossible to determine an optimal one universally.
Our experiments on \cite{zhai2023smartmoe} achieve the minimum MoE execution time at a moderate rearrangement frequency (\eg, every 25 steps). Further increasing the frequency (\eg, to every 10 steps) can improve non-rearrangement iteration time by \timelinessSpeedup but results in \timelinessSlowdown higher overall latency due to the associated overhead. %














In this paper, we propose \textit{\YYY (\yyy)}, a novel MoE training paradigm addressing the two aforementioned challenges, inspired by FSDP~\cite{zhao2023pytorchfsdp} for dense PTM training.
Unlike existing systems that transition from one placement to another over iterations, \yyy fully shards the parameters and optimizer states of MoE layers across devices, and sparsely materializes MoE parameters for an ephemeral expert placement from scratch in each iteration.
By maintaining only one complete copy of the optimizer states of MoE layers globally, \yyy achieves minimal and balanced memory footprint between devices, freeing up valuable memory for materializing more load-balancing expert placements (\textbf{C1}).
Furthermore, \yyy decomposes the \collar communication of gradient synchronization introduced by rearrangement into two sparse collective communications: \textit{\collsag} and \textit{\collsrs}. 
\newline These collectives enable materializing any placement from MoE parameter shards and synchronizing gradients afterward, while incurring the same overall communication volume as {\collar} supporting that placement.
This design removes explicit rearrangement overhead from the performance critical path (\textbf{C2}), as depicted in \reffig[c]{fig:overlap}.



\input{figs/moe_arch/moe_layer_fig.tex}

We build \xxx, an MoE training system fully incorporating \yyy to achieve superior training throughput and efficient memory utilization.
\xxx introduces heterogeneous sharding, which utilizes the unified memory space across MoE layers to fully shard them at once, 
forming flexible expert placements with heterogeneous MoE shards between different devices without additional memory overhead.
\xxx further utilizes sparse materialization to support expert placement, including constructing candidate expert placements in a topology-aware manner and scheduling the two sparse collectives efficiently to be overlapped with computation, thus achieving superior training throughput with low memory overhead.
To further reduce the memory footprint of materializing expert placements, \xxx supports re-materialization, which promptly releases materialized MoE parameters after computation for memory reuse of new materializations.
As PTMs continue to grow in size, \yyy can serve as an unified and scalable approach to optimize MoE training with manageable memory overhead and remarkable performance improvements.

Our contributions can be summarized as follows:
\begin{niceritemize}
    \item We propose \textit{\YYY (\yyy)}, an innovative MoE training paradigm that tackles the parallelization of MoE layers and potential straggler effects caused by imbalanced expert loads from a new angle.
    \item We build \xxx, a high-performance MoE training system atop \yyy to fully unlock its potential.
    \xxx implements heterogeneous sharding and sparse materialization in a topology-aware manner, schedules the two novel sparse collectives to overlap with preceding computation, enabling more efficient placement with lower memory and communication overhead.
    \item We evaluate \xxx on training workloads of typical MoE models across various baseline systems~\cite{he2022fastermoe,nie2023flexmoe,zhai2023smartmoe} in two clusters.
    Compared to the state-of-the-art systems, \xxx achieves a significant speedup of up to 3.54$\times$ and is robust for consistently outperforming these systems under different configurations.
    With re-materialization, \xxx achieves up to 1.52$\times$ speedup, while reduces parameter memory footprint of \xxx by 90.2\%.
\end{niceritemize}







