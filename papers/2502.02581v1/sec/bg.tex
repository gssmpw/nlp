\section{Background and Motivation}

\subsection{Mixture of Experts in PTMs}

A MoE layer comprises a gate and multiple expert networks, as illustrated in \reffig[a]{fig:moe_layer}.
Given an input, the MoE gate assigns a score to each expert to indicate its affinity with that expert.
Based on these gating scores, the input is routed to experts with the top-$k$ scores, where $k$ is a tunable hyperparameter.
These experts process the input independently, and their outputs are then weighted by the corresponding gating scores and aggregated to produce the final output.

MoE layers are typically integrated into Transformer-based PTMs by replacing the feed-forward networks (FFNs) with MoE layers of experts of the same size~\cite{vaswani2017attention}. 
This hybrid architecture is composed of stacked Transformer-MoE blocks, each containing an Attention layer followed by an MoE layer, as shown in \reffig[c]{fig:moe_layer}.
The inputs to the MoE layers are referred to as tokens, 
which represent embeddings of different granularities depending on the task domain, 
such as word or sub-word in natural language processing~\cite{Devlin2019BERTPO} and pixel or image patch in computer vision~\cite{alexey2020vit}.





\subsection{Distributed Training} 


MoE has a strong capability to expand model with significantly more parameters.
As the scale of MoE PTM continues to grow, distributed training has become crucial.

\textbf{Data parallelism (DP)}~\cite{li2020pytorchddp} 
replicates model parameters and optimizer states across devices, and training data is split among them.
Each device computes gradients of the model on the split data, synchronizes the gradients with the other devices via \collar which is typically overlapped with backward computation~\cite{lamy2023breadth,hashemi2019tictac}, and then updates the model parameters with the optimizer.
However, as the size of an MoE layer grows along with the number of experts or the FFN size in a larger PTM, training MoE layers with solely DP is infeasible, as the working-set memory of experts can easily exceed the capacity of a single device.

\textbf{Expert parallelism (EP)}~\cite{shazeer2017outrageously}
enables larger MoE PTM training by evenly distributing experts of MoE layers across multiple devices, as illustrated in \reffig[b]{fig:moe_layer}.
Typically, EP is only applied to experts, while leaving the rest of the model to be parallelized with DP.
Tokens are dispatched to their experts potentially residing on other devices, and results are gathered back to their original devices to ensure the arithmetic consistency.
This exchanges of tokens between devices is facilitated by \collatoa communication.
Unfortunately, the skewness of token assignment in MoE gates can lead to workload imbalances among devices, ultimately causing straggler effects.
As expert loads fluctuate over time (\reffig{fig:trace}), efficient MoE training necessitates adaptive strategies to handle this dynamicity.





\input{figs/trace/trace_fig.tex}

\subsection{Expert Rearrangement}

Recent systems~\cite{zhai2023smartmoe,nie2023flexmoe,he2022fastermoe} address the straggler effect in MoE by rearranging expert placement during training.
SmartMoE~\cite{zhai2023smartmoe} periodically exchanges the positions of experts between devices, aiming to balance device loads with combinations of expert loads, \eg, placing the experts with the most and least tokens on the same device and vice versa.
FlexMoE~\cite{nie2023flexmoe} employs heuristic algorithms to create or remove expert replicas across devices, featuring flexibility in expert placements.
FasterMoE~\cite{he2022fastermoe} selectively replicates experts to every device after obtaining MoE gating decisions.



However, maintaining efficient memory utilization (C1) and manageable communication overhead (C2) when rearranging experts remains challenging.
SmartMoE and FlexMoE rearrange experts along with their optimizer states, whose size are significantly larger than parameters (\eg, when using Adam optimizer~\cite{kingma2014adam} under mixed precision training~\cite{micikevicius2017mixed,rajbhandari2020zero}, the size of optimizer states is at least 6$\times$ larger than parameters), incurring high memory and communication overheads.
FlexMoE's replication strategy rapidly exhausts memory, while SmartMoE's permutation-based approach is infeasible when a device cannot hold multiple experts and their optimizer states for every MoE layer.
As these systems place rearrangement communication on the performance critical path, to avoid unacceptable rearrangement overheads, they either impose strict rearrangement conditions~\cite{he2022fastermoe}, reducing sensitivity to load imbalance, or leave the issue to users by offering hyper-parameters to control rearrangement frequency~\cite{nie2023flexmoe,zhai2023smartmoe}.


\subsection{FSDP to \yyy}\label{sec:bg:fsdp}


Fully Sharded Data Parallelism (FSDP)~\cite{rajbhandari2020zero,zhao2023pytorchfsdp} is a DP variant that fully shards model parameters and optimizer states across devices to reduce memory requirements in pre-trained model (PTM) training.
Parameters are materialized on-demand using \collag and released after use, while gradients are reduced via \collrs after backward computation.
For models with multiple layers, these communications can be overlapped with computation in preceding layers.

However, FSDP becomes substantially inefficient when applied to MoE layers.
When using FSDP, a MoE layer with $|\mathcal{E}|$ experts incurs $|\mathcal{E}|$ times more communication overhead than its dense counterpart, which is hardly overlapped with the computation time.

Inspired by FSDP, we propose \yyy, a novel MoE parallel training paradigm that (1) fully shards MoE layers at a different granularity from FSDP, eliminating redundant memory footprint for expert placements, and (2) proposes sparse collectives to replace \collag and \collrs to allow a new placement to be materialized in each iteration with manageable communication overhead.


\input{figs/fsdp/comparison_fig.tex}


