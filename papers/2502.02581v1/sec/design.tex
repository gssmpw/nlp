\input{figs/workflow/workflow_fig.tex}

\section{\yyy Design}\label{sec:design}

\YYY (\yyy) can be divided into two phases, as depicted in Figure~\ref{fig:workflow}: (1) the \textit{sharding phase}, where the parameters and optimizer states of an MoE layer are partitioned into multiple MoE shards and distributed across different devices; and (2) the \textit{materialization phase}, where a timely expert placement is materialized using two novel communication collectives, \collsag and \collsrs. During both forward and backward pass, \collsag partially materializes the MoE layer parameters to employ a low-latency expert placement. The gradients produced in backward pass of replicated experts are reduced by \collsrs back to the device where the corresponding MoE shards are located. At the end of each iteration, the MoE shards use the synchronized gradients to update their optimizer states and model parameters. In this sections, we first introduce the two sparse collectives powering \yyy, and then provide a detailed explanation of \yyy's parallelization strategies.

\subsection{Sparse Collectives}\label{sec:design:spcoll}

The two novel sparse communication collectives both operate on a logical input buffer, which is split into a set of equal-sized chunks $\set{C} = \{C_0, C_1, \ldots\}$.
Denote all devices in the communication group as $\set{D} = \{D_0, D_1, \ldots\}$. 
A chunk placement $\set{P}$ is defined as $\set{P} \subseteq \set{C} \times \set{D}$, where each element $(c, d) \in \set{P}$ indicates that chunk $c \in \set{C}$ is available on device $d \in \set{D}$.
A collective is defined by a pair of chunk placements, pre-condition $\set{P}_0$ and post-condition $\set{P}_1$, representing the data layout before and after the collective operation.

\input{figs/spcoll/spcoll_fig.tex}

\textbf{\collsag} is designed for materializing a placement of expert parameters of an MoE layer at every iteration in \yyy, where each chunk corresponds to parameters of an expert.
The pre-condition $\set{P}_0$ of \collsag partitions all blocks into disjoint subsets, each of which is assigned to a unique device. \collsag optionally materializes chunks that devices do not have in the pre-condition. Data layout ends up in a post-condition $\set{P}_1$ which is a superset of the pre-condition. Thus, a specific \collsag can be formulated as:
\begin{align*}
    & \operatorname{\collsag}(\set{P}_0, \set{P}_1) \\
    & \text{s.t. } \set{P}_0 \text{ is surjective and } \set{P}_0 \subseteq \set{P}_1
\end{align*}
, which we denote concisely as \collsagop{\set{P}_0}{\set{P}_1}.
An example of \collsag is illustrated in \reffig[a]{fig:spcoll}, which is used to perform the sparse materialization for the case of \reffig[b]{fig:fsdp_vs_fssdp}.

\textbf{\collsrs} is designed for reducing gradients of the ephemerally materialized experts to specified devices in \yyy, where each chunk corresponds to gradients of an expert. For \collsrs, each chunk $c$ in the post-condition $\set{P}_1$ has a value summing up all chunks $c$ in the pre-condition $\set{P}_0$. 
A specific \collsrs can be formulated as:
\begin{align*}
    & \operatorname{\collsrs}(\set{P}_0, \set{P}_1) \\
    & \text{s.t. } \set{P}_1 \text{ is surjective and } \set{P}_1 \subseteq \set{P}_0
\end{align*}
, denoted as \collsrsop{\set{P}_0}{\set{P}_1}.
At each iteration of \yyy, each \collsagop{\set{P}}{\set{P}'} is paired with a symmetric \collsrsop{\set{P}'}{\set{P}} to reduce the gradients back to source devices where corresponding MoE shard reside.
An example of \collsrs is illustrated in \reffig[b]{fig:spcoll}, which is used to perform the gradient reduction for the case of \reffig[b]{fig:fsdp_vs_fssdp}.

\textbf{Comparison with FSDP.}
A sparse collective practically has lower communication volumes than its counterpart in FSDP, since FSDDP only materializes a subset of MoE layer parameters. 
An \collag can be simulated by a collection of {\collbc}s, each of which is dedicated to one chunk to be broadcasted to all devices. In this context, for an input buffer of size $S$, the communication volume of \collsag is $O(S)$.
\footnote{
    An \collag may run a ring algorithm~\cite{chan2006collective} with a sightly lower volume of $O(\frac{|\set{D}|-1}{|\set{D}|}\cdot S)$, but the value will still approach $O(S)$ when $|\set{D}|$ scale up.
}
On the other hand, a \collsag can also be regarded as a collection of ``broadcasts'', each of which is dedicated to one chunk that may be replicated to only a subset of devices. 
Denote input chunks involved in inter-device communication in \collsag as $\set{\hat{C}}$, then the size of the inter-device data is $\lambda S$, where $\lambda = |\set{\hat{C}}| / |\set{C}|$ indicates the sparsity of the collective.
The worst-case communication latency occurs when there is one (or more) device that needs to receive all these inter-device chunks and becomes the bottleneck with a communication volume of $O(\lambda S)$.
Therefore, with sparsity, the upper bound of the communication volume of a \collsag in \yyy is lower than the \collag in FSDP, \ie $O(\lambda S) \ll O(S)$ when $\lambda \ll 1$.
Similarly, the communication volume of \collsrs in \yyy can be formulated in the same way as \refeq{eq:vol_spcoll}, and it is also practically lower than that of \collrs in FSDP.
\begin{equation}\label{eq:vol_spcoll}
    \text{Vol}(\collsagop{\set{P}}{\set{P}'}) 
    = \text{Vol}(\collsrsop{\set{P}'}{\set{P}}) 
    = O(\lambda S) \\
\end{equation}
This loosely upper-bounded latency is introduced by the communication sparsity of the two new collectives, and is the key to enabling short enough communication to be effectively overlapped.

\textbf{Comparison with Rearrangement.} 
For the same expert placement, the pair of sparse collective in \yyy has the same latency upper bound as the \collar communication in existing rearrangement systems.
In rearrangement systems, for each expert (\ie, a chunk of size $S/|\set{C}|$) replicated on more than one device (\ie, a DP group) in a placement $\set{P}'$, an \collar is required at the end of each iteration to synchronize gradients of the expert across the DP group.
Denote the $i$-th DP group as $\set{D}_i$, the overall communication volume of \collar operations of all DP groups is
\begin{equation}\label{eq:val_ars}
    \text{Vol}(\texttt{{\collar}s}) = 
    \sum_{i}^{|\set{\hat{C}}|} \frac{2(|\set{D}_i| - 1)}{|\set{D}_i|} \cdot \frac{S}{|\set{C}|}
\end{equation}
When the number of devices in each DP group scale up, \refeq{eq:val_ars} approaches $O(2\lambda S)$, which is the same as the overall volume upper bound of a \collsrsop{\set{P}'}{\set{P}} and a \collsagop{\set{P}}{\set{P}'} used by \yyy for the same placement. This shows that \yyy achieves the same expert placement for load balancing with only the same communication overhead as the \collar communication in existing systems, without the need for additional rearrangement overhead.









\subsection{Paralleling Strategies}

During the sharding phase, each MoE layer in the PTM is partitioned into $|\set{D}|$ disjoint \textit{MoE shards}. %
\yyy considers an expert as the atomic unit for sharding the MoE layer. Namely, each MoE shard contains of the \textit{model parameters} of a subset of experts along with their \textit{optimizer states}, and is uniquely assigned to a distinct device. 
A trivial sharding choice is to evenly split each MoE layer.

In the materialization phase, \yyy performs \collsagop{\set{P}}{\set{P}'} to sparsely materialized parameters of an MoE layer and \collsrsop{\set{P}'}{\set{P}} to synchronize gradients. This essentially requires a new placement $\set{P}'$ of the MoE layer parameters.
To determine ideal collectives for expert rearrangement in \yyy, two factors must be considered: (1) the expert load distribution, which causes the straggler effects to be mitigated by $\set{P}'$; and (2) the latency of attention layer, where communication of the sparse collectives can be hidden.
Since computations of the attention layer are all dense, the attention layer latency is contingent on the fixed mini-batch size used during training. Thus attention latency can be either profiled before the training or captured in real-time during the training process.
As for expert loads,The temporal locality in the MoE layer's architectural learning leads to smooth changes in expert load distribution over iterations~\cite{nie2023flexmoe}. This allows predicting the next iteration's load distribution based on previous iterations. By using this estimated distribution, the optimal collectives to mitigate load imbalance can be scheduled before the next MoE gate.

It is worth noting that \collsag is launched twice for an MoE layer in each iteration, since the sparsely materialized parameters are discarded immediately after being used for memory reuse across MoE layers. Thus, there are two collective instances to be overlapped with the attention backward computation, \ie \collsrs for gradient reduction of the current layer and \collsag for re-materializing the following layer. Typically, the backward computation takes twice as long as the forward computation~\cite{lecun1988theoretical}.
Thanks to this characteristic, if judiciously scheduling the \collsag to take time no more than attention forward, there will be enough time during the attention backward for both sparse collectives to be hidden, as shown in \reffig[c]{fig:overlap}.

Having understood how \yyy parallelizes MoE training, the next question is what algorithms can be used in the two phases for better expert placements and handling token dispatching. For these tasks, we propose a series of algorithms implemented in our system \xxx.
