\section{Related Work}
\label{sec:related_work}

While many \gls{tinyml} surveys are not specifically focused on \gls{eai} tools, they often include these tools as part of their review **Sze, "Hardware for Deep Learning: Challenges and Opportunities"**. These surveys typically provide a brief overview of available tools, covering many important ones but lacking completeness. Given the rapid evolution of the field, some information happens to be missing or becomes outdated.

MLPerf Tiny **Shi, "MLPerf Inference Benchmark"** is a benchmark suite for \gls{tinyml} models, designed to provide a standardized method for evaluating the performance of \gls{tinyml} models across different platforms. It includes four benchmarks in widely used \gls{tinyml} applications. Although it has played a significant role in improving \gls{tinyml} tools, we identify two main limitations in its use for comparing \gls{eai} tools. First, it includes only four models, all of which are considered large for many embedded devices. As a result, it cannot provide a comprehensive view of the tools' performance across a wide range of models, especially given that many industrial applications of \gls{eai} involve small models requiring less than 100 kB of memory. Second, MLPerf Tiny does not impose fixed hardware or configuration settings, which makes it challenging to compare the performance of different tools, as each may be evaluated under varying test conditions.

Several studies have compared popular \gls{eai} tools. For instance, **Cheng, "Comparison of STM32Cube.AI and TFLM"** examines the performance of \gls{tflm} and STM32Cube.AI on a few of models, concluding that STM32Cube.AI outperforms \gls{tflm} but is limited to STM32 devices, whereas \gls{tflm} offers broader flexibility. In **Mendez, "Comparison of AIfES and TFLM"**, the creators of AIfES compare its performance against \gls{tflm}, showing that AIfES outperforms \gls{tflm}, particularly on \gls{fc} models. Similarly, **Chen, "TVM vs Interpreter-based TinyML Tools"** contrasts the interpreter and non-interpreter versions of \gls{tflm} with microTVM. This work also introduces a third backend for TVM that optimizes RAM usage through \gls{usmp} and runtime improvements. Although these studies provide valuable insights, they generally focus on comparing two tools within constrained test environments, falling short of offering a comprehensive evaluation.

Few works address the automation of benchmarking systems for \gls{tinyml}. In **Kim, "Benchmarking System Automation"**, the authors implement automation to a certain extent, allowing them to conduct numerous comparisons within a short timeframe. **Zhou, "Automation and Evaluation of TinyML Tools"** adopts this principle at its core but focuses more on traditional \gls{ml} models and tools. Nevertheless, the need for a comprehensive automation system capable of handling the entire workflow for deploying \gls{ml} models on embedded devices remains evident.