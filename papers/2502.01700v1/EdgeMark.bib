@article{David2020,
   abstract = {Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100 -- 1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.},
   author = {Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
   journal = {Proceedings of Machine Learning and Systems 3 (MLSys 2021)},
   month = {10},
   title = {TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems},
   year = {2020},
}
@inproceedings{MLonMCU,
   author = {Philipp van Kempen and Rafael Stahl and Daniel Mueller-Gritschneder and Ulf Schlichtmann},
   city = {New York, NY, USA},
   doi = {10.1145/3615338.3618128},
   isbn = {9798400703379},
   booktitle = {Proceedings of the 2023 Workshop on Compilers, Deployment, and Tooling for Edge AI},
   month = {9},
   pages = {32-36},
   publisher = {ACM},
   title = {MLonMCU: TinyML Benchmarking with Fast Retargeting},
   year = {2023},
}
@misc{EdgeImpulseEON,
   author = {Edgeimpulse},
   title = {Introducing EON: Neural Networks in Up to 55\% Less RAM and 35\% Less ROM},
   url = {https://edgeimpulse.com/blog/introducing-eon},
   note = {accessed: 16 January, 2025},
}
@misc{MicroTVM,
   author = {{Apache TVM}},
   title = {microTVM: TVM on bare-metal},
   url = {https://tvm.apache.org/docs/topic/microtvm},
   note = {accessed: 16 January, 2025},
}
@misc{Weber2020,
   author = {Logan Weber and Andrew Reusch},
   month = {6},
   title = {TinyML - How TVM is Taming Tiny},
   url = {https://tvm.apache.org/2020/06/04/tinyml-how-tvm-is-taming-tiny},
   year = {2020},
   note = {accessed: 16 January, 2025},
}
@misc{STM32CubeAI,
   author = {STMicroelectronics},
   title = {STM32Cube.AI - STMicroelectronics - STM32 AI},
   url = {https://stm32ai.st.com/stm32-cube-ai},
   note = {accessed: 16 January, 2025},
}
@misc{STM32CubeAISolution,
   author = {STMicroelectronics},
   title = {Microcontrollers STM32CubeAI Solution},
   url = {https://www.st.com/content/ccc/resource/sales_and_marketing/presentation/product_presentation/group0/69/82/bf/ae/5a/8b/40/91/STM32CubeAI_press_pres/files/STM32CubeAI_press_pres.pdf/jcr:content/translations/en.STM32CubeAI_press_pres.pdf},
   note = {accessed: 16 January, 2025},
}
@misc{eAITranslator,
   author = {{Renesas Electronics Corporation}},
   title = {e-AI Development Environment for Microcontrollers},
   url = {https://www.renesas.com/en/e-ai-development-environment-microcontrollers},
   note = {accessed: 16 January, 2025},
}
@misc{Ekkono,
   author = {Ekkono},
   title = {Edge Machine Learning and Virtual Sensors - Ekkono Solutiona},
   url = {https://www.ekkono.ai},
   note = {accessed: 16 January, 2025},
}
@misc{ArmNN,
   author = {Arm},
   title = {Arm NN SDK | Efficient ML for Arm CPUs, GPUs, \& NPUs - Arm},
   url = {https://www.arm.com/products/silicon-ip-cpu/ethos/arm-nn},
   note = {accessed: 16 January, 2025},
}
@misc{ELL,
   author = {{Microsoft Corporation}},
   title = {The Embedded Learning Library - Embedded Learning Library (ELL)},
   url = {https://microsoft.github.io/ELL},
   note = {accessed: 16 January, 2025},
}
@misc{NanoEdgeAIStudio,
   author = {STMicroelectronics},
   title = {NanoEdgeAIStudio - Automated Machine Learning (ML) tool for STM32 developers - STMicroelectronics},
   url = {https://www.st.com/en/development-tools/nanoedgeaistudio.html},
   note = {accessed: 16 January, 2025},
}
@misc{NanoEdgeAINews,
   author = {STMicroelectronics},
   title = {STMicroelectronics breaks down barriers to edge AI adoption with free NanoEdge AI deployment - ST News},
   url = {https://newsroom.st.com/media-center/press-item.html/n4592.html},
   note = {accessed: 16 January, 2025},
}
@misc{uTensor,
   author = {uTensor},
   title = {microTensor},
   url = {https://utensor.github.io/website},
   note = {accessed: 16 January, 2025},
}
@misc{Shelby2019,
   author = {Zach Shelby},
   month = {5},
   title = {uTensor and Tensor Flow Announcement | Mbed},
   url = {https://os.mbed.com/blog/entry/uTensor-and-Tensor-Flow-Announcement},
   year = {2019},
   note = {accessed: 16 January, 2025},
}
@misc{MbedUpdate,
   author = {Will Lord},
   title = {Important Update on Mbed | Mbed},
   url = {https://os.mbed.com/blog/entry/Important-Update-on-Mbed},
   note = {accessed: 16 January, 2025},
}
@misc{MicromindToolkit,
   author = {Francesco Paissan},
   title = {micromind: A toolkit for tinyML research and deployment},
   url = {https://github.com/micromind-toolkit/micromind},
   note = {accessed: 16 January, 2025},
}
@misc{ExecuTorch,
   author = {{The PyTorch Foundation}},
   title = {PyTorch ExecuTorch | PyTorch},
   url = {https://pytorch.org/executorch-overview},
   note = {accessed: 16 January, 2025},
}
@misc{Imagimob,
   author = {{Imagimob AB}},
   title = {Imagimob - Edge AI | tinyML | Deep learning},
   url = {https://www.imagimob.com},
   note = {accessed: 16 January, 2025},
}
@misc{OmniML,
   author = {Arm},
   title = {OmniML Inc. - Arm},
   url = {https://www.arm.com/partners/catalog/omnimlinc},
   note = {accessed: 16 January, 2025},
}
@inproceedings{Jaiswal2023,
   author = {Shikhar Jaiswal and Rahul Kranti Kiran Goli and Aayan Kumar and Vivek Seshadri and Rahul Sharma},
   city = {New York, NY, USA},
   doi = {10.1145/3589610.3596278},
   isbn = {9798400701740},
   booktitle = {Proceedings of the 24th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
   month = {6},
   pages = {26-39},
   publisher = {ACM},
   title = {MinUn: Accurate ML Inference on Microcontrollers},
   year = {2023},
}
@article{Wang2019,
   abstract = {The growing number of low-power smart devices in the Internet of Things is coupled with the concept of "Edge Computing", that is moving some of the intelligence, especially machine learning, towards the edge of the network. Enabling machine learning algorithms to run on resource-constrained hardware, typically on low-power smart devices, is challenging in terms of hardware (optimized and energy-efficient integrated circuits), algorithmic and firmware implementations. This paper presents FANN-on-MCU, an open-source toolkit built upon the Fast Artificial Neural Network (FANN) library to run lightweight and energy-efficient neural networks on microcontrollers based on both the ARM Cortex-M series and the novel RISC-V-based Parallel Ultra-Low-Power (PULP) platform. The toolkit takes multi-layer perceptrons trained with FANN and generates code targeted at execution on low-power microcontrollers either with a floating-point unit (i.e., ARM Cortex-M4F and M7F) or without (i.e., ARM Cortex M0-M3 or PULP-based processors). This paper also provides an architectural performance evaluation of neural networks on the most popular ARM Cortex-M family and the parallel RISC-V processor called Mr. Wolf. The evaluation includes experimental results for three different applications using a self-sustainable wearable multi-sensor bracelet. Experimental results show a measured latency in the order of only a few microseconds and a power consumption of few milliwatts while keeping the memory requirements below the limitations of the targeted microcontrollers. In particular, the parallel implementation on the octa-core RISC-V platform reaches a speedup of 22x and a 69% reduction in energy consumption with respect to a single-core implementation on Cortex-M4 for continuous real-time classification.},
   author = {Xiaying Wang and Michele Magno and Lukas Cavigelli and Luca Benini},
   journal = {IEEE Internet of Things Journal},
   month = {11},
   title = {FANN-on-MCU: An Open-Source Toolkit for Energy-Efficient Neural Network Inference at the Edge of the Internet of Things},
   year = {2019},
}
@misc{FraunhoferAIfES,
   author = {{Fraunhofer Institute for Microelectronic Circuits and Systems}},
   title = {Artificial Intelligence for Embedded Systems - Fraunhofer IMS},
   url = {https://www.ims.fraunhofer.de/en/Business-Unit/Industry/Industrial-AI/Artificial-Intelligence-for-Embedded-Systems-AIfES.html},
   note = {accessed: 16 January, 2025},
}
@misc{Salerno2020,
   author = {Simone Salerno},
   title = {tinymlgen: Generate C code for microcontrollers from Tensorflow models},
   url = {https://github.com/eloquentarduino/tinymlgen},
   year = {2020},
   note = {accessed: 16 January, 2025},
}
@misc{Moretti2016,
   author = {Caio B. Moretti},
   title = {Neurona: Artificial Neural Networks for Arduino},
   url = {https://github.com/moretticb/Neurona},
   year = {2016},
   note = {accessed: 16 January, 2025},
}
@misc{HLS4ML,
   author = {{Fast Machine Learning Lab}},
   title = {hls4ml 1.0.0 documentation},
   url = {https://fastmachinelearning.org/hls4ml/},
   note = {accessed: 16 January, 2025},
}
@misc{MLTK,
   author = {{Silicon Labs}},
   title = {Silicon Labs Machine Learning Toolkit (MLTK) — MLTK 0.20.0 documentation},
   url = {https://siliconlabs.github.io/mltk},
   note = {accessed: 16 January, 2025},
}
@misc{CoreML,
   author = {{Apple Inc.}},
   title = {Core ML | Apple Developer Documentation},
   url = {https://developer.apple.com/documentation/coreml},
   note = {accessed: 16 January, 2025},
}
@misc{Glow,
   author = {Meta},
   title = {Glow},
   url = {https://ai.meta.com/tools/glow},
   note = {accessed: 16 January, 2025},
}
@misc{EIQ,
   author = {{NXP Semiconductors}},
   title = {eIQ ML Software Development Environment | NXP Semiconductors},
   url = {https://www.nxp.com/design/design-center/software/eiq-ml-development-environment:EIQ},
   note = {accessed: 16 January, 2025},
}
@misc{ONNXRuntime,
   author = {Microsoft},
   title = {ONNX Runtime},
   url = {https://onnxruntime.ai},
   note = {accessed: 16 January, 2025},
}
@misc{Larq,
   author = {Plumerai},
   title = {Larq},
   url = {https://larq.dev},
   note = {accessed: 16 January, 2025},
}
@misc{ONNC,
   author = {{Skymizer Taiwan Inc.}},
   title = {ONNC},
   url = {https://onnc.ai},
   note = {accessed: 16 January, 2025},
}
@misc{LatentAI,
   author = {{Latent AI}},
   title = {Latent AI - Find your best model faster},
   url = {https://latentai.com},
   note = {accessed: 16 January, 2025},
}
@misc{Plumerai,
   author = {Plumerai},
   title = {Plumerai},
   url = {https://plumerai.com/},
   note = {accessed: 16 January, 2025},
}
@misc{NNTool,
   author = {{GreenWaves Technologies}},
   title = {NNTool — GAP SDK documentation},
   url = {https://greenwaves-technologies.com/manuals_gap9/gap9_sdk_doc/html/source/tools/nntool},
   note = {accessed: 16 January, 2025},
}
@misc{Morawiec2017,
   author = {Darius Morawiec},
   title = {weka-porter: Transpile trained decision trees from Weka to C, Java or JavaScript.},
   url = {https://github.com/nok/weka-porter},
   year = {2017},
   note = {accessed: 16 January, 2025},
}
@misc{M2CGen,
   author = {Nikita Titov and Iaroslav Zeigerman and Viktor Yershov},
   title = {m2cgen: Transform ML models into a native code (Java, C, Python, Go, JavaScript, Visual Basic, C\#, R, PowerShell, PHP, Dart, Haskell, Ruby, F\#, Rust) with zero dependencies},
   url = {https://github.com/BayesWitnesses/m2cgen},
   note = {accessed: 16 January, 2025},
}
@misc{MicromlgenSklearn,
   author = {Simone Salerno},
   title = {micromlgen: Generate C code for microcontrollers from Python's sklearn classifiers},
   url = {https://github.com/eloquentarduino/micromlgen},
   note = {accessed: 16 January, 2025},
}
@article{Tsutsui2022,
   author = {Lucas Tsutsui da Silva and Vinicius M. A. Souza and Gustavo E. A. P. A. Batista},
   doi = {10.1109/JSEN.2021.3128130},
   issn = {1530-437X},
   issue = {1},
   journal = {IEEE Sensors Journal},
   month = {1},
   pages = {544-554},
   title = {An Open-Source Tool for Classification Models in Resource-Constrained Hardware},
   volume = {22},
   year = {2022},
}
@misc{QeexoAutoML,
   author = {{Qeexo AutoML}},
   title = {AutoML by TDK SensEI Help Center},
   url = {https://docs.qeexo.com/guides/userguides},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleDynamicQuant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training dynamic range quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_quant},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleIntegerQuant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training integer quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_quant},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleInt16Quant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training integer quantization with int16 activations},
   url = {https://ai.google.dev/edge/litert/models/post_training_integer_quant_16x8},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleFloat16Quant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training float16 quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_float16_quant},
   note = {accessed: 16 January, 2025},
}
@article{Rusci2019,
   abstract = {This paper presents a novel end-to-end methodology for enabling the deployment of low-error deep networks on microcontrollers. To fit the memory and computational limitations of resource-constrained edge-devices, we exploit mixed low-bitwidth compression, featuring 8, 4 or 2-bit uniform quantization, and we model the inference graph with integer-only operations. Our approach aims at determining the minimum bit precision of every activation and weight tensor given the memory constraints of a device. This is achieved through a rule-based iterative procedure, which cuts the number of bits of the most memory-demanding layers, aiming at meeting the memory constraints. After a quantization-aware retraining step, the fake-quantized graph is converted into an inference integer-only model by inserting the Integer Channel-Normalization (ICN) layers, which introduce a negligible loss as demonstrated on INT4 MobilenetV1 models. We report the latency-accuracy evaluation of mixed-precision MobilenetV1 family networks on a STM32H7 microcontroller. Our experimental results demonstrate an end-to-end deployment of an integer-only Mobilenet network with Top1 accuracy of 68% on a device with only 2MB of FLASH memory and 512kB of RAM, improving by 8% the Top1 accuracy with respect to previously published 8 bit implementations for microcontrollers.},
   author = {Manuele Rusci and Alessandro Capotondi and Luca Benini},
   journal = {Proceedings of Machine Learning and Systems},
   month = {5},
   title = {Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers},
   year = {2019},
}
@article{Lin2020,
   author = {Ji Lin and Wei-Ming Chen and Yujun Lin and Chuang Gan and Song Han},
   journal = {Advances in Neural Information Processing Systems},
   title = {Mcunet: Tiny deep learning on iot devices},
   volume = {33},
   year = {2020},
}
@misc{Microsoft2021,
   author = {Microsoft},
   month = {12},
   title = {Neural Network Intelligence},
   url = {https://github.com/microsoft/nni},
   year = {2021},
   note = {accessed: 16 January, 2025},
}
@inproceedings{Lv2022,
   author = {Chengfei Lv and Chaoyue Niu and Renjie Gu and Xiaotang Jiang and Zhaode Wang and Bin Liu and Ziqi Wu and Qiulin Yao and Congyu Huang and Panos Huang and Tao Huang and Hui Shu and Jinde Song and Bin Zou and Peng Lan and Guohuan Xu and Fei Wu and Shaojie Tang and Fan Wu and Guihai Chen},
   city = {Carlsbad, CA},
   isbn = {978-1-939133-28-1},
   booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
   month = {7},
   pages = {249-265},
   publisher = {USENIX Association},
   title = {Walle: An End-to-End, General-Purpose, and Large-Scale Production System for Device-Cloud Collaborative Machine Learning},
   url = {https://www.usenix.org/conference/osdi22/presentation/lv},
   year = {2022},
}
@article{Coelho2020,
   author = {Claudionor N Coelho and Aki Kuusela and Hao Zhuang and Thea Aarrestad and Vladimir Loncar and Jennifer Ngadiuba and Maurizio Pierini and Sioni Summers},
   journal = {arXiv preprint arXiv:2006.10159},
   pages = {108},
   title = {Ultra low-latency, low-area inference accelerators using heterogeneous deep quantization with QKeras and hls4ml},
   year = {2020},
}
@article{Burrello2021,
   author = {A Burrello and A Garofalo and N Bruschi and G Tagliavini and D Rossi and F Conti},
   doi = {10.1109/TC.2021.3066883},
   journal = {IEEE Transactions on Computers},
   pages = {1},
   title = {DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT MCUs},
   year = {2021},
}
@unpublished{SklearnPorter,
   author = {Darius Morawiec},
   note = {Transpile trained scikit-learn estimators to C, Java, JavaScript and others},
   title = {sklearn-porter},
   url = {https://github.com/nok/sklearn-porter},
}
@misc{Karpathy2015,
   author = {Andrej Karpathy},
   title = {char-rnn},
   year = {2015},
   howpublished={\url{https://github.com/karpathy/char-rnn} },
   note = {accessed: 16 January, 2025},
}
@misc{EmLearn,
   author = {Jon Nordby},
   doi = {10.5281/zenodo.2589394},
   month = {3},
   title = {emlearn: Machine Learning inference engine for Microcontrollers and Embedded Devices},
   url = {https://doi.org/10.5281/zenodo.2589394},
   year = {2019},
}
@misc{NanoEdgeAIReq,
   author = {STMicroelectronics},
   title = {AI:NanoEdge AI Studio - stm32mcu},
   url = {https://wiki.st.com/stm32mcu/wiki/AI:NanoEdge_AI_Studio},
   month = {6},
   year = {2024},
   note = {accessed: 16 January, 2025},
}
@article{Njor2024,
   author = {Emil Njor and Mohammad Amin Hasanpour and Jan Madsen and Xenofon Fafoutis},
   doi = {10.1109/ACCESS.2024.3512860},
   issn = {2169-3536},
   journal = {IEEE Access},
   month = {12},
   pages = {1-1},
   title = {A Holistic Review of the TinyML Stack for Predictive Maintenance},
   year = {2024},
}
@article{Sanchez2020,
   author = {Ramon Sanchez-Iborra and Antonio F. Skarmeta},
   doi = {10.1109/MCAS.2020.3005467},
   issn = {1531-636X},
   issue = {3},
   journal = {IEEE Circuits and Systems Magazine},
   month = {8},
   pages = {4-18},
   title = {TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities},
   volume = {20},
   year = {2020},
}
@article{Tsoukas2024,
   abstract = {<p>Tiny Machine Learning (TinyML) is an emerging technology proposed by the scientific community for developing autonomous and secure devices that can gather, process, and provide results without transferring data to external entities. The technology aims to democratize AI by making it available to more sectors and contribute to the digital revolution of intelligent devices. In this work, a classification of the most common optimization techniques for Neural Network compression is conducted. Additionally, a review of the development boards and TinyML software is presented. Furthermore, the work provides educational resources, a classification of the technology applications, and future directions and concludes with the challenges and considerations.</p>},
   author = {Vasileios Tsoukas and Anargyros Gkogkidis and Eleni Boumpa and Athanasios Kakarountas},
   doi = {10.1145/3661820},
   issn = {0360-0300},
   issue = {10},
   journal = {ACM Computing Surveys},
   month = {10},
   pages = {1-37},
   title = {A Review on the emerging technology of TinyML},
   volume = {56},
   year = {2024},
}
@article{Ray2022,
   author = {Partha Pratim Ray},
   doi = {10.1016/j.jksuci.2021.11.019},
   issn = {13191578},
   issue = {4},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   month = {4},
   pages = {1595-1623},
   title = {A review on TinyML: State-of-the-art and prospects},
   volume = {34},
   year = {2022},
}
@article{Immonen2022,
   abstract = {<p>We use 250 billion microcontrollers daily in electronic devices that are capable of running machine learning models inside them. Unfortunately, most of these microcontrollers are highly constrained in terms of computational resources, such as memory usage or clock speed. These are exactly the same resources that play a key role in teaching and running a machine learning model with a basic computer. However, in a microcontroller environment, constrained resources make a critical difference. Therefore, a new paradigm known as tiny machine learning had to be created to meet the constrained requirements of the embedded devices. In this review, we discuss the resource optimization challenges of tiny machine learning and different methods, such as quantization, pruning, and clustering, that can be used to overcome these resource difficulties. Furthermore, we summarize the present state of tiny machine learning frameworks, libraries, development environments, and tools. The benchmarking of tiny machine learning devices is another thing to be concerned about; these same constraints of the microcontrollers and diversity of hardware and software turn to benchmark challenges that must be resolved before it is possible to measure performance differences reliably between embedded devices. We also discuss emerging techniques and approaches to boost and expand the tiny machine learning process and improve data privacy and security. In the end, we form a conclusion about tiny machine learning and its future development.</p>},
   author = {Riku Immonen and Timo Hämäläinen},
   doi = {10.1155/2022/7437023},
   issn = {1687-7268},
   journal = {Journal of Sensors},
   month = {11},
   pages = {1-11},
   title = {Tiny Machine Learning for Resource-Constrained Microcontrollers},
   volume = {2022},
   year = {2022},
}
@article{Saha2022,
   author = {Swapnil Sayan Saha and Sandeep Singh Sandha and Mani Srivastava},
   doi = {10.1109/JSEN.2022.3210773},
   issn = {1530-437X},
   issue = {22},
   journal = {IEEE Sensors Journal},
   month = {11},
   pages = {21362-21390},
   title = {Machine Learning for Microcontroller-Class Hardware: A Review},
   volume = {22},
   year = {2022},
}
@article{Capogrosso2024,
   author = {Luigi Capogrosso and Federico Cunico and Dong Seon Cheng and Franco Fummi and Marco Cristani},
   doi = {10.1109/ACCESS.2024.3365349},
   issn = {2169-3536},
   journal = {IEEE Access},
   pages = {23406-23426},
   title = {A Machine Learning-Oriented Survey on Tiny Machine Learning},
   volume = {12},
   year = {2024},
}
@article{Abadade2023,
   author = {Youssef Abadade and Anas Temouden and Hatim Bamoumen and Nabil Benamar and Yousra Chtouki and Abdelhakim Senhaji Hafid},
   doi = {10.1109/ACCESS.2023.3294111},
   issn = {2169-3536},
   journal = {IEEE Access},
   pages = {96892-96922},
   title = {A Comprehensive Survey on TinyML},
   volume = {11},
   year = {2023},
}
@article{Banbury2020,
   abstract = {Recent advancements in ultra-low-power machine learning (TinyML) hardware promises to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted benchmark for these systems. Benchmarking allows us to measure and thereby systematically compare, evaluate, and improve the performance of systems and is therefore fundamental to a field reaching maturity. In this position paper, we present the current landscape of TinyML and discuss the challenges and direction towards developing a fair and useful hardware benchmark for TinyML workloads. Furthermore, we present our four benchmarks and discuss our selection methodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf working group that is comprised of over 30 organizations.},
   author = {Colby R. Banbury and Vijay Janapa Reddi and Max Lam and William Fu and Amin Fazel and Jeremy Holleman and Xinyuan Huang and Robert Hurtado and David Kanter and Anton Lokhmotov and David Patterson and Danilo Pau and Jae-sun Seo and Jeff Sieracki and Urmish Thakker and Marian Verhelst and Poonam Yadav},
   journal = {ArXiv},
   month = {3},
   title = {Benchmarking TinyML Systems: Challenges and Direction},
   year = {2020},
}
@article{Baciu2024,
   author = {Vlad-Eusebiu Baciu and Johan Stiens and Bruno da Silva},
   doi = {10.1016/j.sysarc.2024.103262},
   issn = {13837621},
   journal = {Journal of Systems Architecture},
   month = {10},
   pages = {103262},
   title = {MLino bench: A comprehensive benchmarking tool for evaluating ML models on edge devices},
   volume = {155},
   year = {2024},
}
@article{Wulfert2024,
   author = {Lars Wulfert and Johannes Kühnel and Lukas Krupp and Justus Viga and Christian Wiede and Pierre Gembaczka and Anton Grabmaier},
   doi = {10.1109/TPAMI.2024.3355495},
   issn = {0162-8828},
   issue = {6},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   month = {6},
   pages = {4519-4533},
   title = {AIfES: A Next-Generation Edge AI Framework},
   volume = {46},
   year = {2024},
}
@InProceedings{Osman2022,
author="Osman, Anas
and Abid, Usman
and Gemma, Luca
and Perotto, Matteo
and Brunelli, Davide",
editor="Saponara, Sergio
and De Gloria, Alessandro",
title="TinyML Platforms Benchmarking",
booktitle="Applications in Electronics Pervading Industry, Environment and Society",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="139--148",
abstract="Recent advances in state-of-the-art ultra-low power embedded devices for machine learning (ML) have permitted a new class of products whose key features enable ML capabilities on microcontrollers with less than 1 mW power consumption (TinyML). TinyML provides a unique solution by aggregating and analyzing data at the edge on low-power embedded devices. However, we have only recently been able to run ML on microcontrollers, and the field is still in its infancy, which means that hardware, software, and research are changing extremely rapidly. Consequently, many TinyML frameworks have been developed for different platforms to facilitate the deployment of ML models and standardize the process. Therefore, in this paper, we focus on benchmarking two popular frameworks: Tensorflow Lite Micro (TFLM) on the Arduino Nano BLE and CUBE AI on the STM32-NucleoF401RE to provide a standardized framework selection criterion for specific applications.",
isbn="978-3-030-95498-7"
}
@inproceedings{liu2021deep,
  title={Deep Architecture Compression with Automatic Clustering of Similar Neurons},
  author={Liu, Xiang and Liu, Wenxue and Wang, Li-Na and Zhong, Guoqiang},
  booktitle={Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29--November 1, 2021, Proceedings, Part IV 4},
  pages={361--373},
  year={2021},
  organization={Springer}
}
@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal = {arXiv preprint arXiv:1503.02531},
  volume  = {2},
  number  = {7},
  year    = {2015}
}
@inproceedings{ghanathe2022t,
  title={T-recx: Tiny-resource efficient convolutional neural networks with early-exit},
  author={Ghanathe, Nikhil P and Wilton, Steve},
  booktitle={Proceedings of the 20th ACM International Conference on Computing Frontiers},
  pages={123--133},
  year={2023}
}
@inproceedings{song2022,
 author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22941--22954},
 publisher = {Curran Associates, Inc.},
 title = {On-Device Training Under 256KB Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/90c56c77c6df45fc8e556a096b7a2b2e-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}
@misc{hymel2023edgeimpulsemlopsplatform,
   title={Edge Impulse: An MLOps Platform for Tiny Machine Learning}, 
   author={Shawn Hymel and Colby Banbury and Daniel Situnayake and Alex Elium and Carl Ward and Mat Kelcey and Mathijs Baaijens and Mateusz Majchrzycki and Jenny Plunkett and David Tischler and Alessandro Grande and Louis Moreau and Dmitry Maslov and Artie Beavis and Jan Jongboom and Vijay Janapa Reddi},
   year={2023},
   eprint={2212.03332},
   archivePrefix={arXiv},
   primaryClass={cs.DC},
   url={https://arxiv.org/abs/2212.03332}, 
}
@INPROCEEDINGS{Hasanpour2024,
  author={Hasanpour, Mohammad Amin and Engholm, Rasmus and Fafoutis, Xenofon},
  booktitle={2024 IEEE Annual Congress on Artificial Intelligence of Things (AIoT)}, 
  title={Pump Cavitation Detection with Machine Learning: A Comparative Study of SVM and Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={219-225},
  keywords={Support vector machines;Deep learning;Adaptation models;Analytical models;Pumps;Predictive models;Hardware;cavitation;support vector machine;deep learning;machine learning;centrifugal pump},
  doi={10.1109/AIoT63253.2024.00050}
}
@article{pavan2024tybox,
  title={TyBox: An Automatic Design and Code Generation Toolbox for TinyML Incremental On-Device Learning},
  author={Pavan, Massimo and Ostrovan, Eugeniu and Caltabiano, Armando and Roveri, Manuel},
  journal={ACM Transactions on Embedded Computing Systems},
  volume={23},
  number={3},
  pages={1--27},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{ren2021tinyol,
  title={Tinyol: Tinyml with online-learning on microcontrollers},
  author={Ren, Haoyu and Anicic, Darko and Runkler, Thomas A},
  booktitle={2021 international joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}
@article{cai2019once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}
@inproceedings{gambella2022cnas,
  title={CNAS: Constrained Neural Architecture Search},
  author={Gambella, Matteo and Falcetta, Alessandro and Roveri, Manuel},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={2918--2923},
  year={2022},
  organization={IEEE}
}
@article{lomurno2024pomonag,
  title={POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator},
  author={Lomurno, Eugenio and Mariani, Samuele and Monti, Matteo and Matteucci, Matteo},
  journal={arXiv preprint arXiv:2409.20447},
  year={2024}
}
