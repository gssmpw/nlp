[
  {
    "index": 0,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, T.  and Mann, B.  and Ryder, N.  and Subbiah, M.  and Kaplan, J. D. and Dhariwal, P.  and Neelakantan, A.  and Shyam, P.  and Sastry, G.  and Askell, A.  and others.  ",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, H.  and Lavril, T.  and Izacard, G.  and Martinet, X.  and Lachaux, M.  and Lacroix, T.  and Rozi{\\`e}re, B.  and Goyal, N.  and Hambro, E.  and Azhar, F.  and others.  ",
        "title": "{LLaMA}: Open and efficient foundation language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ramesh2021zero",
        "author": "Ramesh, A.  and Pavlov, M.  and Goh, G.  and Gray, S.  and Voss, C.  and Radford, A.  and Chen, M.  and Sutskever, I. ",
        "title": "Zero-shot text-to-image generation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "firoozi2023foundation",
        "author": "Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and others",
        "title": "Foundation models in robotics: Applications, challenges, and the future"
      },
      {
        "key": "zhang2024scientific",
        "author": "Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and others",
        "title": "Scientific large language models: A survey on biological \\& chemical domains"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2024recent",
        "author": "Wang, H.  and Cao, Y.  and Huang, Z.  and Liu, Y.  and Hu, P.  and Luo, X.  and Song, Z.  and Zhao, W.  and Liu, J.  and Sun, J.  and others.  ",
        "title": "Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "schaeffer2017learning",
        "author": "Schaeffer, H. ",
        "title": "Learning partial differential equations via data discovery and sparse optimization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "tan2024language",
        "author": "Tan, Mingtian and Merrill, Mike A and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas",
        "title": "Are language models actually useful for time series forecasting?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2024data",
        "author": "Chen, W.  and Song, J.  and Ren, P.  and Subramanian, S.  and Morozov, D.  and Mahoney, M. W.",
        "title": "Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning"
      },
      {
        "key": "herde2024poseidon",
        "author": "Herde, M.  and Raoni{\\'c}, B.  and Rohner, T.  and K{\\\"a}ppeli, R.  and Molinaro, R.  and B{\\'e}zenac, E.  and Mishra, S. ",
        "title": "Poseidon: Efficient Foundation Models for PDEs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yang2023context",
        "author": "Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J",
        "title": "In-context operator learning with data prompts for differential equation problems"
      },
      {
        "key": "yang2023fine",
        "author": "Yang, Liu and Liu, Siting and Osher, Stanley J",
        "title": "Fine-Tune Language Models as Multi-Modal Differential Equation Solvers"
      },
      {
        "key": "yang2024pde",
        "author": "Yang, Liu and Osher, Stanley J",
        "title": "Pde generalization of in-context operator networks: A study on 1d scalar nonlinear conservation laws"
      },
      {
        "key": "cao2024vicon",
        "author": "Cao, Yadi and Liu, Yuxuan and Yang, Liu and Yu, Rose and Schaeffer, Hayden and Osher, Stanley",
        "title": "VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yang2024pde",
        "author": "Yang, Liu and Osher, Stanley J",
        "title": "Pde generalization of in-context operator networks: A study on 1d scalar nonlinear conservation laws"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yang2023fine",
        "author": "Yang, Liu and Liu, Siting and Osher, Stanley J",
        "title": "Fine-Tune Language Models as Multi-Modal Differential Equation Solvers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024prose",
        "author": "Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "PROSE: Predicting Multiple Operators and Symbolic Expressions using multimodal transformers"
      },
      {
        "key": "sun2024towards",
        "author": "Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation"
      },
      {
        "key": "liu2024prosefd",
        "author": "Liu, Yuxuan and Sun, Jingmin and He, Xinjie and Pinney, Griffin and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics"
      },
      {
        "key": "jollie2024time",
        "author": "Jollie, D.  and Sun, J.  and Zhang, Z.  and Schaeffer, H. ",
        "title": "Time-Series Forecasting, Knowledge Distillation, and Refinement within a Multimodal PDE Foundation Model"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sun2024towards",
        "author": "Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "sun2024lemon",
        "author": "Sun, J.  and Zhang, Z.  and Schaeffer, H. ",
        "title": "Lemon: Learning to learn multi-operator networks"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lorsung2024physics",
        "author": "Lorsung, C.  and Li, Z.  and Farimani, A. B.",
        "title": "Physics informed token transformer for solving partial differential equations"
      },
      {
        "key": "ye2024pdeformer",
        "author": "Ye, Z.  and Huang, X.  and Chen, L.  and Liu, Z.  and Wu, B.  and Liu, H.  and Wang, Z.  and Dong, B. ",
        "title": "PDEformer-1: A Foundation Model for One-Dimensional Partial Differential Equations"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lu2019vilbert",
        "author": "Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan",
        "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks"
      },
      {
        "key": "sun2019videobert",
        "author": "Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia",
        "title": "Videobert: A joint model for video and language representation learning"
      },
      {
        "key": "tan2019lxmert",
        "author": "Tan, Hao and Bansal, Mohit",
        "title": "Lxmert: Learning cross-modality encoder representations from transformers"
      },
      {
        "key": "xu2023multimodal",
        "author": "Xu, Peng and Zhu, Xiatian and Clifton, David A",
        "title": "Multimodal learning with transformers: A survey"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "tan2019lxmert",
        "author": "Tan, Hao and Bansal, Mohit",
        "title": "Lxmert: Learning cross-modality encoder representations from transformers"
      },
      {
        "key": "sun2019videobert",
        "author": "Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia",
        "title": "Videobert: A joint model for video and language representation learning"
      },
      {
        "key": "li2019visualbert",
        "author": "Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei",
        "title": "Visualbert: A simple and performant baseline for vision and language"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "tan2019lxmert",
        "author": "Tan, Hao and Bansal, Mohit",
        "title": "Lxmert: Learning cross-modality encoder representations from transformers"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2019visualbert",
        "author": "Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei",
        "title": "Visualbert: A simple and performant baseline for vision and language"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "feng2020deep",
        "author": "Feng, Di and Haase-Sch{\\\"u}tz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus",
        "title": "Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges"
      },
      {
        "key": "liu2021multimodal",
        "author": "Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei",
        "title": "Multimodal motion prediction with stacked transformers"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "driess2023palm",
        "author": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
        "title": "Palm-e: An embodied multimodal language model"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "pan2009survey",
        "author": "Pan, Sinno Jialin and Yang, Qiang",
        "title": "A survey on transfer learning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "kenton2019bert",
        "author": "Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, T.  and Mann, B.  and Ryder, N.  and Subbiah, M.  and Kaplan, J. D. and Dhariwal, P.  and Neelakantan, A.  and Shyam, P.  and Sastry, G.  and Askell, A.  and others.  ",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "han2021pre",
        "author": "Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others",
        "title": "Pre-trained models: Past, present and future"
      },
      {
        "key": "liu2024prose",
        "author": "Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "PROSE: Predicting Multiple Operators and Symbolic Expressions using multimodal transformers"
      },
      {
        "key": "sun2024towards",
        "author": "Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden",
        "title": "Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation"
      }
    ]
  }
]