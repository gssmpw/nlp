@article{liu2024prose,
  title={PROSE: Predicting Multiple Operators and Symbolic Expressions using multimodal transformers},
  author={Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={Neural Networks},
  volume={180},
  pages={106707},
  year={2024},
  publisher={Elsevier}
}

@article{sun2024towards,
  title={Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation},
  author={Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2404.12355},
  year={2024}
}

@article{liu2024prosefd,
  title={PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics},
  author={Liu, Yuxuan and Sun, Jingmin and He, Xinjie and Pinney, Griffin and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2409.09811},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{vaswani2017attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title = {Attention is all you need},
  year = {2017},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{yang2023fine,
  title={Fine-Tune Language Models as Multi-Modal Differential Equation Solvers},
  author={Yang, Liu and Liu, Siting and Osher, Stanley J},
  journal={arXiv preprint arXiv:2308.05061},
  year={2023}
}

@article{cao2024vicon,
  title={VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction},
  author={Cao, Yadi and Liu, Yuxuan and Yang, Liu and Yu, Rose and Schaeffer, Hayden and Osher, Stanley},
  journal={arXiv preprint arXiv:2411.16063},
  year={2024}
}

@article{yang2023context,
  title={In-context operator learning with data prompts for differential equation problems},
  author={Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={39},
  pages={e2310142120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{yang2024pde,
  title={Pde generalization of in-context operator networks: A study on 1d scalar nonlinear conservation laws},
  author={Yang, Liu and Osher, Stanley J},
  journal={arXiv preprint arXiv:2401.07364},
  year={2024}
}

@article{golkar2023xval,
  title={xval: A continuous number encoding for large language models},
  author={Golkar, Siavash and Pettee, Mariel and Eickenberg, Michael and Bietti, Alberto and Cranmer, Miles and Krawezik, Geraud and Lanusse, Francois and McCabe, Michael and Ohana, Ruben and Parker, Liam and others},
  journal={arXiv preprint arXiv:2310.02989},
  year={2023}
}

@article{lu2019deeponet,
  title={Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1910.03193},
  year={2019}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}
@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

@article{li2019visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}

@article{xu2023multimodal,
  title={Multimodal learning with transformers: A survey},
  author={Xu, Peng and Zhu, Xiatian and Clifton, David A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{feng2020deep,
  title={Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges},
  author={Feng, Di and Haase-Sch{\"u}tz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={3},
  pages={1341--1360},
  year={2020},
  publisher={IEEE}
}

@inproceedings{liu2021multimodal,
  title={Multimodal motion prediction with stacked transformers},
  author={Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7577--7586},
  year={2021}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{brown2020language,
  author  = {Brown, T.  and Mann, B.  and Ryder, N.  and Subbiah, M.  and Kaplan, J. D. and Dhariwal, P.  and Neelakantan, A.  and Shyam, P.  and Sastry, G.  and Askell, A.  and others.  },
  journal = {Advances in neural information processing systems},
  pages   = {1877--1901},
  title   = {Language models are few-shot learners},
  volume  = {33},
  year    = {2020}
}

@article{touvron2023llama,
  author  = {Touvron, H.  and Lavril, T.  and Izacard, G.  and Martinet, X.  and Lachaux, M.  and Lacroix, T.  and Rozi{\`e}re, B.  and Goyal, N.  and Hambro, E.  and Azhar, F.  and others.  },
  journal = {arXiv preprint arXiv:2302.13971},
  title   = {{LLaMA}: Open and efficient foundation language models},
  year    = {2023}
}

@inproceedings{ramesh2021zero,
  author       = {Ramesh, A.  and Pavlov, M.  and Goh, G.  and Gray, S.  and Voss, C.  and Radford, A.  and Chen, M.  and Sutskever, I. },
  booktitle    = {International conference on machine learning},
  organization = {Pmlr},
  pages        = {8821--8831},
  title        = {Zero-shot text-to-image generation},
  year         = {2021}
}

@article{zhang2024scientific,
  title={Scientific large language models: A survey on biological \& chemical domains},
  author={Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and others},
  journal={arXiv preprint arXiv:2401.14656},
  year={2024}
}

@article{firoozi2023foundation,
  title={Foundation models in robotics: Applications, challenges, and the future},
  author={Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and others},
  journal={The International Journal of Robotics Research},
  pages={02783649241281508},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{wang2024recent,
  author  = {Wang, H.  and Cao, Y.  and Huang, Z.  and Liu, Y.  and Hu, P.  and Luo, X.  and Song, Z.  and Zhao, W.  and Liu, J.  and Sun, J.  and others.  },
  journal = {arXiv preprint arXiv:2408.12171},
  title   = {Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey},
  year    = {2024}
}

@inproceedings{tan2024language,
  title={Are language models actually useful for time series forecasting?},
  author={Tan, Mingtian and Merrill, Mike A and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{schaeffer2017learning,
  author    = {Schaeffer, H. },
  journal   = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number    = {2197},
  pages     = {20160446},
  publisher = {The Royal Society Publishing},
  title     = {Learning partial differential equations via data discovery and sparse optimization},
  volume    = {473},
  year      = {2017}
}

@article{chen2024data,
  author  = {Chen, W.  and Song, J.  and Ren, P.  and Subramanian, S.  and Morozov, D.  and Mahoney, M. W.},
  journal = {arXiv preprint arXiv:2402.15734},
  title   = {Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning},
  year    = {2024}
}

@article{herde2024poseidon,
  author  = {Herde, M.  and Raoni{\'c}, B.  and Rohner, T.  and K{\"a}ppeli, R.  and Molinaro, R.  and B{\'e}zenac, E.  and Mishra, S. },
  journal = {arXiv preprint arXiv:2405.19101},
  title   = {Poseidon: Efficient Foundation Models for PDEs},
  year    = {2024}
}


@article{jollie2024time,
  author  = {Jollie, D.  and Sun, J.  and Zhang, Z.  and Schaeffer, H. },
  journal = {arXiv preprint arXiv:2409.11609},
  title   = {Time-Series Forecasting, Knowledge Distillation, and Refinement within a Multimodal PDE Foundation Model},
  year    = {2024}
}

@article{sun2024lemon,
  author  = {Sun, J.  and Zhang, Z.  and Schaeffer, H. },
  journal = {arXiv preprint arXiv:2408.16168},
  title   = {Lemon: Learning to learn multi-operator networks},
  year    = {2024}
}

@article{lorsung2024physics,
  author    = {Lorsung, C.  and Li, Z.  and Farimani, A. B.},
  journal   = {Machine Learning: Science and Technology},
  number    = {1},
  pages     = {015032},
  publisher = {IOP Publishing},
  title     = {Physics informed token transformer for solving partial differential equations},
  volume    = {5},
  year      = {2024}
}

@article{ye2024pdeformer,
  author  = {Ye, Z.  and Huang, X.  and Chen, L.  and Liu, Z.  and Wu, B.  and Liu, H.  and Wang, Z.  and Dong, B. },
  journal = {arXiv preprint arXiv:2407.06664},
  title   = {PDEformer-1: A Foundation Model for One-Dimensional Partial Differential Equations},
  year    = {2024}
}

%%% introduction references
@article{chen1995universal,
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
  author={Chen, Tianping and Chen, Hong},
  journal={IEEE transactions on neural networks},
  volume={6},
  number={4},
  pages={911--917},
  year={1995},
  publisher={IEEE}
}

@article{li2020fourier,
  title={Fourier neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2010.08895},
  year={2020}
}

@article{de2023machine,
  title={Machine learning for numerical weather and climate modelling: a review},
  author={de Burgh-Day, Catherine O and Leeuwenburg, Tennessee},
  journal={Geoscientific Model Development},
  volume={16},
  number={22},
  pages={6433--6477},
  year={2023},
  publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@article{sezer2020financial,
  title={Financial time series forecasting with deep learning: A systematic literature review: 2005--2019},
  author={Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  journal={Applied soft computing},
  volume={90},
  pages={106181},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{ott2018analyzing,
  title={Analyzing uncertainty in neural machine translation},
  author={Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marcâ€™Aurelio},
  booktitle={International Conference on Machine Learning},
  pages={3956--3965},
  year={2018},
  organization={PMLR}
}

@article{kulikov2018importance,
  title={Importance of search and evaluation strategies in neural dialogue modeling},
  author={Kulikov, Ilia and Miller, Alexander H and Cho, Kyunghyun and Weston, Jason},
  journal={arXiv preprint arXiv:1811.00907},
  year={2018}
}

@article{lu2021learning,
  title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Nature machine intelligence},
  volume={3},
  number={3},
  pages={218--229},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{jadon2024comprehensive,
  title={A comprehensive survey of regression-based loss functions for time series forecasting},
  author={Jadon, Aryan and Patil, Avinash and Jadon, Shruti},
  booktitle={International Conference on Data Management, Analytics \& Innovation},
  pages={117--147},
  year={2024},
  organization={Springer}
}


@article{poznyak2019survey,
  title={A survey on artificial neural networks application for identification and control in environmental engineering: Biological and chemical systems with uncertain models},
  author={Poznyak, Alexander and Chairez, Isaac and Poznyak, Tatyana},
  journal={Annual Reviews in Control},
  volume={48},
  pages={250--272},
  year={2019},
  publisher={Elsevier}
}

@article{lin2021accelerated,
  title={Accelerated replica exchange stochastic gradient Langevin diffusion enhanced Bayesian DeepONet for solving noisy parametric PDEs},
  author={Lin, Guang and Moya, Christian and Zhang, Zecheng},
  journal={arXiv preprint arXiv:2111.02484},
  year={2021}
}


@article{zhang2024bayesian,
  title={Bayesian deep operator learning for homogenized to fine-scale maps for multiscale PDE},
  author={Zhang, Zecheng and Moya, Christian and Leung, Wing Tat and Lin, Guang and Schaeffer, Hayden},
  journal={Multiscale Modeling \& Simulation},
  volume={22},
  number={3},
  pages={956--972},
  year={2024},
  publisher={SIAM}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{cao2021choose,
  title={Choose a transformer: Fourier or galerkin},
  author={Cao, Shuhao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24924--24940},
  year={2021}
}

%%%transfer learning 
@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}
@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
  organization={Minneapolis, Minnesota}
}
@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}