\section{Related Works}
\subsection{Foundation Models}
Foundation models are large-scale, pre-trained models that can be fine-tuned for various downstream tasks in natural language processing~\cite{brown2020language,touvron2023llama}, computer vision~\cite{ramesh2021zero}, and other domains including robotics and biology \cite{firoozi2023foundation, zhang2024scientific}. Despite their versatility, these models are not inherently suitable for number-sensitive tasks, such as scientific computing \cite{wang2024recent}, PDE discovery~\cite{schaeffer2017learning}, and time series forecasting \cite{tan2024language}, where high-precision solutions are critical.
Early attempts have been made to adapt foundation models or similar transformer-based structures for scientific computing tasks. Some approaches utilize pre-training and fine-tuning techniques~\cite{chen2024data,herde2024poseidon}, though these methods often require additional computational cost for downstream tasks.
The In-Context Operator Network (ICON) \cite{yang2023context, yang2023fine, yang2024pde,cao2024vicon} uses in-context learning to learn operators through example input-output pairs, which are few-shot learners and have demonstrated generalization capabilities for various differential equations \cite{yang2024pde}. Zero-shot PDE foundation models integrate additional information to aid the prediction process. ICON-LM \cite{yang2023fine} enables in-context learning with both text descriptions of governing physics and numerical data in the model inputs, but outputs are numerical only. In this way, text is used as a label to signal which PDE or task is needed. The PROSE framework~\cite{liu2024prose,sun2024towards,liu2024prosefd,jollie2024time} is a multimodal approach that encodes numerical data along with symbolic representations of the PDE. In recent work, PROSE has been shown capable of extrapolation \cite{sun2024towards} and improved generalization through fine-tuning \cite{sun2024lemon}.
Other methods achieve zero- or few-shot generalization by embedding PDE structures within the network architecture~\cite{lorsung2024physics,ye2024pdeformer}.


\subsection{Multimodal Machine Learning}
Multimodal machine learning focuses on models capable of integrating data from various modalities \cite{lu2019vilbert, sun2019videobert, tan2019lxmert,xu2023multimodal}. A key area of interest in this field is developing methods for information fusion across modalities, analyzing their interactions, and designing appropriate models and algorithms. For instance, in visual-language reasoning \cite{tan2019lxmert, sun2019videobert, li2019visualbert}, combining visual data like images or videos and linguistic information such as captions or text descriptions \cite{tan2019lxmert} facilitates the development of models with enhanced multimodal understanding \cite{li2019visualbert}. Similarly, AI robots utilize multimodal sensors, including cameras, radar, and ultrasound, to interpret their environments and make well-informed decisions \cite{feng2020deep, liu2021multimodal}. Notably, the concept of a multimodal sentence was introduced in PaLM-E \cite{driess2023palm}, where image and text can appear anywhere in the sentence and be processed in a flexible manner. 


\subsection{Transfer Learning}
Transfer learning has become a cornerstone of modern machine learning, enabling the adaptation of pre-trained models to specialized tasks with relatively limited data \cite{pan2009survey}. In the context of large language models (LLMs), such as GPT and BERT, fine-tuning on domain-specific data allows the model to leverage its extensive pre-trained knowledge while focusing on the nuances of a target domain \cite{radford2019language, kenton2019bert}. For instance, fine-tuning GPT-2 or GPT-3 models on scientific texts has shown improved performance in tasks such as equation generation, technical summarization, and scientific question answering \cite{brown2020language}. In our work, which follows the operator learning paradigm, we fine-tune an LLM with examples of numerical equations, symbolic representations, and descriptive text to enhance its ability to generate meaningful and contextually relevant outputs. The inclusion of this step in our framework, aligns well with recent advancements in multimodal tasks \cite{han2021pre,liu2024prose,sun2024towards}, demonstrating the importance of adapting pre-trained LLMs to specialized contexts.