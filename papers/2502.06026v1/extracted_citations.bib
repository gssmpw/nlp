@article{brown2020language,
  author  = {Brown, T.  and Mann, B.  and Ryder, N.  and Subbiah, M.  and Kaplan, J. D. and Dhariwal, P.  and Neelakantan, A.  and Shyam, P.  and Sastry, G.  and Askell, A.  and others.  },
  journal = {Advances in neural information processing systems},
  pages   = {1877--1901},
  title   = {Language models are few-shot learners},
  volume  = {33},
  year    = {2020}
}

@article{cao2024vicon,
  title={VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction},
  author={Cao, Yadi and Liu, Yuxuan and Yang, Liu and Yu, Rose and Schaeffer, Hayden and Osher, Stanley},
  journal={arXiv preprint arXiv:2411.16063},
  year={2024}
}

@article{chen2024data,
  author  = {Chen, W.  and Song, J.  and Ren, P.  and Subramanian, S.  and Morozov, D.  and Mahoney, M. W.},
  journal = {arXiv preprint arXiv:2402.15734},
  title   = {Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning},
  year    = {2024}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{feng2020deep,
  title={Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges},
  author={Feng, Di and Haase-Sch{\"u}tz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glaeser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={3},
  pages={1341--1360},
  year={2020},
  publisher={IEEE}
}

@article{firoozi2023foundation,
  title={Foundation models in robotics: Applications, challenges, and the future},
  author={Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and others},
  journal={The International Journal of Robotics Research},
  pages={02783649241281508},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}

@article{herde2024poseidon,
  author  = {Herde, M.  and Raoni{\'c}, B.  and Rohner, T.  and K{\"a}ppeli, R.  and Molinaro, R.  and B{\'e}zenac, E.  and Mishra, S. },
  journal = {arXiv preprint arXiv:2405.19101},
  title   = {Poseidon: Efficient Foundation Models for PDEs},
  year    = {2024}
}

@article{jollie2024time,
  author  = {Jollie, D.  and Sun, J.  and Zhang, Z.  and Schaeffer, H. },
  journal = {arXiv preprint arXiv:2409.11609},
  title   = {Time-Series Forecasting, Knowledge Distillation, and Refinement within a Multimodal PDE Foundation Model},
  year    = {2024}
}

@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
  organization={Minneapolis, Minnesota}
}

@article{li2019visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}

@inproceedings{liu2021multimodal,
  title={Multimodal motion prediction with stacked transformers},
  author={Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7577--7586},
  year={2021}
}

@article{liu2024prose,
  title={PROSE: Predicting Multiple Operators and Symbolic Expressions using multimodal transformers},
  author={Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={Neural Networks},
  volume={180},
  pages={106707},
  year={2024},
  publisher={Elsevier}
}

@article{liu2024prosefd,
  title={PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics},
  author={Liu, Yuxuan and Sun, Jingmin and He, Xinjie and Pinney, Griffin and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2409.09811},
  year={2024}
}

@article{lorsung2024physics,
  author    = {Lorsung, C.  and Li, Z.  and Farimani, A. B.},
  journal   = {Machine Learning: Science and Technology},
  number    = {1},
  pages     = {015032},
  publisher = {IOP Publishing},
  title     = {Physics informed token transformer for solving partial differential equations},
  volume    = {5},
  year      = {2024}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{ramesh2021zero,
  author       = {Ramesh, A.  and Pavlov, M.  and Goh, G.  and Gray, S.  and Voss, C.  and Radford, A.  and Chen, M.  and Sutskever, I. },
  booktitle    = {International conference on machine learning},
  organization = {Pmlr},
  pages        = {8821--8831},
  title        = {Zero-shot text-to-image generation},
  year         = {2021}
}

@article{schaeffer2017learning,
  author    = {Schaeffer, H. },
  journal   = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number    = {2197},
  pages     = {20160446},
  publisher = {The Royal Society Publishing},
  title     = {Learning partial differential equations via data discovery and sparse optimization},
  volume    = {473},
  year      = {2017}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}

@article{sun2024lemon,
  author  = {Sun, J.  and Zhang, Z.  and Schaeffer, H. },
  journal = {arXiv preprint arXiv:2408.16168},
  title   = {Lemon: Learning to learn multi-operator networks},
  year    = {2024}
}

@article{sun2024towards,
  title={Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation},
  author={Sun, Jingmin and Liu, Yuxuan and Zhang, Zecheng and Schaeffer, Hayden},
  journal={arXiv preprint arXiv:2404.12355},
  year={2024}
}

@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

@inproceedings{tan2024language,
  title={Are language models actually useful for time series forecasting?},
  author={Tan, Mingtian and Merrill, Mike A and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{touvron2023llama,
  author  = {Touvron, H.  and Lavril, T.  and Izacard, G.  and Martinet, X.  and Lachaux, M.  and Lacroix, T.  and Rozi{\`e}re, B.  and Goyal, N.  and Hambro, E.  and Azhar, F.  and others.  },
  journal = {arXiv preprint arXiv:2302.13971},
  title   = {{LLaMA}: Open and efficient foundation language models},
  year    = {2023}
}

@article{wang2024recent,
  author  = {Wang, H.  and Cao, Y.  and Huang, Z.  and Liu, Y.  and Hu, P.  and Luo, X.  and Song, Z.  and Zhao, W.  and Liu, J.  and Sun, J.  and others.  },
  journal = {arXiv preprint arXiv:2408.12171},
  title   = {Recent Advances on Machine Learning for Computational Fluid Dynamics: A Survey},
  year    = {2024}
}

@article{xu2023multimodal,
  title={Multimodal learning with transformers: A survey},
  author={Xu, Peng and Zhu, Xiatian and Clifton, David A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@article{yang2023context,
  title={In-context operator learning with data prompts for differential equation problems},
  author={Yang, Liu and Liu, Siting and Meng, Tingwei and Osher, Stanley J},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={39},
  pages={e2310142120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{yang2023fine,
  title={Fine-Tune Language Models as Multi-Modal Differential Equation Solvers},
  author={Yang, Liu and Liu, Siting and Osher, Stanley J},
  journal={arXiv preprint arXiv:2308.05061},
  year={2023}
}

@article{yang2024pde,
  title={Pde generalization of in-context operator networks: A study on 1d scalar nonlinear conservation laws},
  author={Yang, Liu and Osher, Stanley J},
  journal={arXiv preprint arXiv:2401.07364},
  year={2024}
}

@article{ye2024pdeformer,
  author  = {Ye, Z.  and Huang, X.  and Chen, L.  and Liu, Z.  and Wu, B.  and Liu, H.  and Wang, Z.  and Dong, B. },
  journal = {arXiv preprint arXiv:2407.06664},
  title   = {PDEformer-1: A Foundation Model for One-Dimensional Partial Differential Equations},
  year    = {2024}
}

@article{zhang2024scientific,
  title={Scientific large language models: A survey on biological \& chemical domains},
  author={Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and others},
  journal={arXiv preprint arXiv:2401.14656},
  year={2024}
}

