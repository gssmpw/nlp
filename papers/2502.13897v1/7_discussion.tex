\section{Conclusion}
\vspace{-0.2cm}
\label{sec: discussion}
This paper introduces \benchmark, a novel framework tailored to assess the capabilities of LLMs in data science tasks. By meticulously curating challenging prompts and leveraging robust LLMs alongside a self-consistency strategy, we generate ground truth for all prompts. To efficiently evaluate LLMs' performance, we aggregate evaluation metrics and synthesize the TFC list programmatically. Subsequently, we assess \textbf{23} API-based and open-source models, offer valuable research and engineering insights, and present error analyses of the assessed LLMs.

\section{Limitations}
In certain visualization tasks, our initial metrics and evaluation methods (e.g., VLM-as-a-judge) may lack precision. Further refinement of metrics is required to evaluate data visualization tasks effectively. One potential approach could involve employing VLMs to train critic models, enhancing the capability for fine-grained evaluations of visualizations.