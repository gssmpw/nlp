
\section{DataSciBench}
\label{sec: DataSciBench}

\benchmark$\space$ consists of three important components as outlined in Figure~\ref{fig: framework}. 

\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{Prompt Definition and Collection} which defines \textbf{6} task types and collects \textbf{222} real, challenging, and high-quality prompts through question filtering and expert review.

    \item \textbf{Response Integration and Validation} which proposes a novel Task - Function - Code (TFC) that produces \textbf{519} test cases to effectively assess the key tasks of each prompt through defined aggregate functions and programmatic rules.

    \item \textbf{LLM Evaluation} which assesses \textbf{6} API-based models, \textbf{8} open-sourced general models, and \textbf{9} open-sourced code generation models from coarse-grained (i.e. success rate, completion rate) and fine-grained (e.g., Vision-Language Model (VLM)-as-a-judge, \textbf{25} aggregate functions) perspectives.
\end{itemize}


\subsection{Prompt Definition for Data Science}

\vpara{Task Type.} We define six typical data science tasks in Appendix~\ref{ssec: task_des} that include Data cleaning and preprocessing, Data exploration and statistics understanding, Data visualization, Predictive modeling, Data mining and Pattern recognition, and Interpretability and Report generation.

\vpara{Task Integration.} To increase the difficulty of prompts, we chose more complex prompts that included multiple defined task types. These sequential tasks can be any combination of six task types.

\subsection{Dataset Collection}
\vpara{Question Collection.}
We collect questions from four sources:
\textbf{1) Extensive collection from a real-world online platform.} We collect natural prompts from one online code-generation platform, CodeGeeX~\citep{zheng2023codegeex}.
\textbf{2) Extracted and rewritten from a public code benchmark.} We utilize \textbf{167} high-quality data science prompts from BigCodeBench (BCB) and then refine them to our specified format encompassing \textit{input data or file, prompt, and expected output file} with TFC for standardized evaluation.
\textbf{3) Hand-written by humans.} We also write elaborated prompts to increase the difficulty and robustness of the evaluated benchmark by referring to relative websites\footnote{https://ds100.org/course-notes/eda/eda.html}. 
\textbf{4) Synthesized from LLMs.} We use a few shot examples drawn from human-written prompts to ask LLM to generate prompts.

\vpara{Question Filtering.}
We filter low-quality questions via the following principles:
\textbf{1) Choose questions} that include keywords, but are not limited to, ``machine learning'', ``deep learning'', ``data preprocessing'', and ``data visualization'';
\textbf{2) Filter questions} that require rewriting code, finding errors, and explaining basic concepts.


\vpara{Expert Review.}
We review the prompts we collect with experts in computer science and data analysis to ensure their quality.
The review process includes three stages:
\textbf{1) In stage 1,} experts verify the correctness and adjust the suitability of prompts. In addition, experts ensure that the responses to the questions are clear and structured in a way that facilitates assessment. For example, handling missing values in a data frame.
\textbf{2) In stage 2}, experts format all verified prompts into a unified instruction that encompasses \textit{input data or file, prompt, and expected output file}.
\textbf{3) In stage 3}, experts ensure the availability of input prompt datasets by collecting public datasets or generating random datasets.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/function.pdf}
    \caption{Statistics of \textbf{6} task types and \textbf{25} aggregate functions in Task-Function-Code (TFC) list. DM \& PR denotes Data Mining \& Pattern Recognition. Interpre. \& RG denotes Interpretability \& Report Generation.}
    \label{fig: function}
    \vspace{-0.4cm}
\end{figure}

\subsection{Response Integration and Validation}

\vpara{Ground Truth Generation and Verification.} To obtain the response of the collected questions, we propose the following strategy to generate test cases for each question. 
Firstly, we generate the outputs of each prompt by sampling LLMs several times and then execute the generated code to obtain the final output.
Then, we use two different validation methods to determine whether the LLM-generated answer aligns with the meaning specified in the aggregate functions to ensure the reliability of the answer. 
For questions originating from BCB, where reliable test cases are provided, we validate the generated answer by performing all test cases. Answers that pass all test cases are re-checked by humans and finally considered ground truth. 
As for other prompts, we initially adopt a self-consistency strategy~\citep{wang2022self} to obtain outputs and then ensure their reliability and precision by having six authors of the paper verify the default assigned prompts and corresponding ground truth elements, including task type, evaluation function, programmatic code, and final outputs. In cases where uncertainties arise in the generated outputs, we cross-validate them among three authors.


\vpara{Evaluation Selection.} We introduce a structured approach to identify and evaluate key tasks across six established types.
We first use GPT-4o-mini to select several valuable task types, return the corresponding evaluation functions, and generate the evaluation codes for each prompt to effectively evaluate the capabilities of LLM and reduce the evaluation cost.
Each group data is simplified as a tuple (T, F, C) in generated $\mathbf{R}$ as follows:
\begin{equation}
    \vspace{-0.1cm}
    \mathbf{R} = \{(\text{T}_i, \text{F}_i, \text{C}_i)|_i^N\},
    \vspace{-0.1cm}
\end{equation}
where $N$ is the number of valuable task types per prompt, and this value is different for each question.
Then we use the data interpreter (DI)~\citep{hong2024datainterpreter} to generate a hierarchical directed acyclic graph (DAG) for each prompt, in which each task type is defined as a node at one level in a DAG, as illustrated in Figure~\ref{fig: framework}.
Based on the generated graphs, we take a powerful LLM as a backbone and run all evaluation functions to obtain the ground truth of each task type.
To some extent, this way of verification can avoid the commonly used LLM-as-a-Judge black-box assessment.

\vpara{Function Aggregation.} To unify the key functions and improve the scalability of the evaluation, we aggregate all generated functions to the top-$K$ function category, select top-$K$ functions for each task type, and finally obtain 25 functions, as shown in Figure~\ref{fig: function}.
Generally, $K$ is set as 5. For example, the function category for data cleaning and preprocessing includes Data Cleaning Completeness, DataFrame Shape Validation, Data Completeness, Normalization Range Check, and Data Quality Score.


\vpara{Programmatic Rules.} Regarding aggregate functions with corresponding codes, we define unified rules to verify generated code.
Specifically, we unify all initial outputs as boolean or decimal types ranging between $0$ and $1$. Then, we obtain the final boolean value by comparing ground truth with prediction output depending on the specific task description of aggregate functions.
For example, regarding Data Cleaning Completeness, which calculates the final number of rows/columns after preprocessing, the final output is $1$ if the number is the same as the number of ground truths; otherwise, it is $0$.
For some specific tasks whose output type is decimal, we also set a corresponding threshold to transform the output to boolean for simplicity. For example, the threshold is set to $0.5$ if the aggregate function is silhouette score for data mining and pattern recognition.

\vpara{Summary.} Based on the above three submodules, we obtain \textbf{222} effective prompts and \textbf{519} corresponding test cases for each prompt, \textbf{25} aggregated functions, which help evaluations of \textbf{6} API-based and \textbf{17} open-sourced models.