\section{Related Works}
\label{sec: related_works}
% \vspace{-0.2cm}
\subsubsection{LLMs for Data Science} 
With the popularity of large-scale language models, researchers have developed a series of LLM-based agents~\cite{yao2022react, wang2024battleagentbench, xu2024androidlab, xia2024scenegenagent}. 
For data science, SheetCopilot~\citep{li2024sheetcopilot} designs a tabular agent, which directly processes natural language-described tasks, and generates and executes a series of operation plans on datasheets to produce the desired results. 
Data Copilot~\citep{zhang2024data} is an intelligent agent that serves as a bridge between users and data, which automatically executes data processing, prediction, and visualization tasks based on users' data needs.
InsightPilot~\citep{ma2023insightpilot} focuses on exploratory data analysis and can automatically discover data insights related to fuzzy questions raised by users.
Data interpreter~\citep{hong2024datainterpreter} augments problem-solving in data science with dynamic planning with hierarchical graph structures, tool integration, and logical inconsistency identification in feedback.
Furthermore, ReAct~\cite{yao2022react} is quite similar to our backbone agentic framework, DataInterpreter. Both frameworks offer foundational pipelines rather than specific evaluation metrics tailored for diverse data science tasks.
However, the correctness of data analysis in data science has a significant impact on decision-making.
Therefore, with the continuous increase of data science agents, it is urgent to conduct a comprehensive and in-depth evaluation of data science agents.


\subsubsection{LLM Agent Evaluation Benchmarks for Data Science}

Assessing the effectiveness of LLMs in handling diverse and challenging data science prompts is essential to push the boundaries of benchmarking practices in the field of data science and LLM research.
Data science agents often solve problems by generating code, so the capabilities of data science agents are closely related to the code generation capabilities of large models. 
There are already many benchmarks for evaluating the code capability of large models.
MLAgentBench~\citep{huang2023mlbenchmarking} benchmarks the LLMs' abilities on traditional machine learning tasks.
NaturalCodeBench~\citep{zhang2024naturalcodebench} evaluates the capabilities of code generation models on the real prompts from the CodeGeeX~\citep{zheng2023codegeex} platform.
However, the general code evaluation benchmark ignores the characteristics of data science tasks and cannot comprehensively and effectively evaluate the capabilities of large models in data science. 

Recently, some evaluation benchmarks for large language models in data science have been proposed. 
Text2Analysis~\citep{he2024text2analysis} constructs the evaluation benchmark to evaluate the model's ability to handle data analysis functions and fuzzy questions on tabular data. Their prompts are obtained through manual annotation and large model generation.
Furthermore, DAEval~\citep{huinfiagent} is developed as another evaluation benchmark and it contains 257 data analysis questions on CSV data and questions, which are generated by LLMs.
However, the prompts in these two works often only involve one task, and these prompts involve relatively simple data analysis operations.
In practical data science analysis tasks, user questions often involve multiple tasks and involve performing complex data analysis operations.
Therefore, we aim to provide a data science evaluation benchmark that is more in line with practical scenarios, especially for problems involving multiple subtasks and complex data analysis operations.