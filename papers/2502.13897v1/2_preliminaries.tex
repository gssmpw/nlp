\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/framework.pdf}
    \vspace{-0.2cm}
    \caption{ 
    The overall framework of \benchmark$\space$ encompasses three key components aligned with Section~\ref{sec: DataSciBench}:
    1) Prompt definition and collection, which covers \textbf{6} task types, prompt collection, question filtering, and expert review.
    2) Response integration and validation, incorporating the TFC framework (\textbf{25} aggregated functions and programmatic rules) and \textbf{519} test cases with ground truth.
    3) LLM evaluation involving \textbf{23} LLMs.
    }
    \label{fig: framework}
\end{figure*}

\section{Background on using LLMs for Data Science}
This section discusses the key aspects that underlie our benchmarking approach.

\noindent \textbf{Ground Truth Generation.} Ground truth serves as the cornerstone for evaluating the performance of LLMs in data science tasks. For diverse and challenging data science prompts, we aim to propose a semi-automated pipeline that leverages a robust LLM to generate GTs and employs self-consistency and human validation strategies to ensure the accuracy and reliability of generated GTs.

\noindent \textbf{Evaluation Metric Definition.} Defining appropriate and meaningful evaluation metrics is essential for effectively comparing and analyzing the effectiveness of different LLMs in data science tasks. Our study meticulously defines evaluation metrics tailored to the specific tasks and challenges the collected prompts pose. These metrics are designed to capture the diverse nuances of data analysis and visualization tasks, enabling a comprehensive assessment of LLMs' capabilities.

\noindent \textbf{Limitation of Previous Studies.} Prior research in benchmarking LLMs for data science has often been limited by focusing on single tasks, simplistic evaluation metrics, and readily available ground truth. These shortcomings hinder the thorough evaluation of LLMs and may not fully capture their strengths and weaknesses. By addressing these limitations, our study seeks to provide a more comprehensive and nuanced assessment of LLMs in data science. Through the development of \benchmark$\space$ and the implementation of a rigorous evaluation framework, we aim to push the boundaries of benchmarking practices in the field of data science and LLM research.
