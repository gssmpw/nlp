% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
% \usepackage{listings}
\usepackage{enumitem} % itemize
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
% Appendix TOC
\usepackage{minitoc}
\newif\ifshowcomment
\showcommenttrue % To show  comments
% \showcommentfalse % To hide comments

\newcommand{\vpara}[1]{\vspace{0.05in}\noindent \textbf{#1 }}
\newcommand{\todo}[1]{\ifshowcomment \textbf{\color{red}[(TODO: #1 )]}\fi}
% \newcommand{\benchmark}{\texttt{DataSciBench}\space}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% \newcommand{\model}{\texttt{DataSciBench}\space}
\newcommand{\benchmark}{\texttt{DataSciBench}}
% \title{SciAgentBench: LLM Agent Benchmark for Data Science}
\title{DataSciBench: An LLM Agent Benchmark for Data Science}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Dan Zhang$^{1,2,\dagger}$,\ Sining Zhoubian$^{1,2,\dagger}$,\ Min Cai$^{2,\dagger}$,\ Fengzu Li$^{1}$, \ Lekang Yang$^{1}$\\
\textbf{Wei Wang$^{1}$, Tianjiao Dong$^{3, \dagger}$,\ Ziniu Hu$^{4}$,\ Jie Tang$^{1}$, Yisong Yue$^{4}$}\\
$^1$Tsinghua University; $^2$Zhipu AI;\\
$^3$University of California, Berkeley; $^4$California Institute of Technology\\
\normalsize\rule{0pt}{1em}\url{https://datascibench.github.io/}\\
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\begin{document}


\maketitle

\begin{abstract}
This paper presents \benchmark, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science.
Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated.
In contrast, \benchmark$\space$ is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. 
We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules.
Our experimental framework involves testing \textbf{6} API-based models, \textbf{8} open-source general models, and \textbf{9} open-source code generation models using the diverse set of prompts we have gathered.
This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses.
Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models.
We release all code and data at~\url{https://github.com/THUDM/DataSciBench/}.

\end{abstract}

{\let\thefootnote\relax\footnotetext{$^\dagger$ work done while these authors interned at Zhipu AI.}}

\input{1_introduction}

\input{2_preliminaries}

\input{3_DataSciBench}

\input{4_experiments}

\input{5_results}

% \input{6_related_works}

\input{7_discussion}

% \subsubsection*{Author Contributions}


% \subsubsection*{Acknowledgments}
% CodeGeeX's log

% \newpage

\bibliography{ref}
\bibliographystyle{acl_natbib}


\clearpage\newpage
\appendix
% \part{Appendix}
% \parttoc

\input{8_appendix}

\end{document}
