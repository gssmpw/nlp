\section{Experiments}
\label{sec: experiments}

\subsection{Settings}
We construct a comprehensive benchmark on our collected prompts to assess the performance of \textbf{23} different models (e.g., API-based models and open-sourced general/code generation models). 

\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{Six API-based models} include o1-mini~\citep{jaech2024openai}, GPT-4o-mini~\cite{hurst2024gpt4o}, GPT-4o-2024-05-13, GPT-4-Turbo~\citep{achiam2023gpt}, Claude-3.5-Sonnet\footnote{\textcolor{teal}{https://www.anthropic.com/news/claude-3-5-sonnet}}, and GLM-4-Flash~\citep{glm2024chatglm}. 
    \item \textbf{Eight open-sourced general models} include Llama3.1-8B-Instruct, Llama3-8B-Instruct, Qwen2.5-7B-Instruct, Qwen2-1.5/7B-Instruct~\citep{yang2024qwen2}, Gemma2-9B-it~\citep{team2024gemma}, GLM-4-9B-chat, and Yi-1.5-9B-chat-16k~\citep{young2024yi}.
    \item \textbf{Nine open-sourced code models} include Deepseek-Coder-1.3/6.7/33B-Instruct~\citep{guo2024deepseek}, CodeLlama-7/13/34B-Instruct~\citep{roziere2023code}, Qwen2.5-Coder-1.5/7B-Instruct~\citep{hui2024qwen2}, and StarCoder2-15B~\citep{lozhkov2024StarCoder}.
\end{itemize}


\input{tables/overall_results}


\subsection{Evaluation Metrics}
\vpara{Coarse-Grained Metrics.} We define the coarse-grained metrics (CR and SR) for evaluating LLMs.

{\noindent \bf $\bullet$ Completion Rate (CR).} Following the CR in Data Interpreter \citep{hong2024datainterpreter}, we calculate the CR  for each TFC in $\mathbf{R}$.
Specifically, we give it a completion score, with a minimum score of $0$ and a maximum score of $2$. The step completion scores were given as follows: missing (score of $0$), fail (score of $0$), success-non-compliant (score of $1$), and success-compliant (score of $2$). The completion rate is then calculated as follows:
\begin{equation}
    \vspace{-0.1cm}
    \text{Completion Rate (CR)} = \frac{\sum_{t=1}^{T} s_t}{T \times s_{\text{max}}},
    \vspace{-0.1cm}
    \label{eq:completion_rate}
\end{equation}
where the numerator was the sum of the completion scores for each step, and the denominator was the sum of the maximum possible scores for all steps ($2 \times T$ and $T$ is the number of TFCs).

{\noindent \bf $\bullet$ Success Rate (SR).} Similar to Codex~\citep{chen2021evaluating}, our success rate is defined as the rate of complete success on a single prompt estimated under 10 runs. Specifically, if all the TFC in $\mathbf{R}$ have passed within a run of a single prompt, it will count as a success. Otherwise, it will count as a failure. Note that for prompts acquired from BigCodeBench, we compare the completion function's outputs with the ground truth completion function's outputs to determine whether a single run passes, since $\mathbf{R}$ is derived based on demanded function outputs in this case. The formula for calculating SR is as follows:
\begin{equation}
    \vspace{-0.1cm}
    \text{Success Rate (SR)} \coloneqq  \underset{\text{Prompts}}{\mathbb{E}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right],
    \vspace{-0.1cm}
\end{equation}
where $n=10$ and $k=1$ in our case, $c$ denotes the number of runs that have passed all TFCs in $\mathbf{R}$.


\vpara{Fine-grained Aggregate Metrics.} We also define the fine-grained aggregate metrics for detail evaluating all LLMs, as shown in Figure~\ref{fig: function} and Table~\ref{tab: program_rules}. 
It's worth noting that data visualization is one of the more challenging tasks. It confirms the existence of outputs but also validates their accuracy and completeness.
Specifically, for tasks like data visualization, we focus on metrics such as visualization quality and use VLM-as-a-judge to assess the quality of images. Finally, we select six representative aggregated metrics as follows:

\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item VLM-as-a-judge assesses the overall score of two visual inputs based on predefined criteria for progressively adding scores (see Appendix~\ref{sec:vlm_prompt}) to write the total score and provide a brief step-by-step reasoning explanation for its evaluation, with GPT-4o-mini serving as the VLM.
    \item Data Quality Score (F1) in Data cleaning and preprocessing aims to assess the cleanliness of data post-processing. It yields a boolean output of $1$ if it matches the ground truth or $0$ otherwise.
    \item Plot Validity (F2) in Data visualization pertains to the accuracy of visual representations, such as checking whether the shape of an association matrix is consistent with the ground truth. If consistent, the final value is $1$, otherwise $0$.
    \item Data Accuracy (F3) in Data Exploration and Statistics Understand focuses on understanding data quality and can be quantified using Mean Squared Error (MSE). The final value is derived by comparing it against the ground truth with a predefined threshold.
    \item Visualization Completeness (F4) in Data visualization evaluates the comprehensiveness of generated images (e.g., PNG, jpeg, PDF) by checking their existence compared to the ground truth. A score of $1$ is assigned if the files exist, and $0$ otherwise. Note that we use VLM-as-a-judge and F2 to assess the quality of generated images. 
    \item Model Accuracy (F5) in Predictive modeling is utilized to gauge the predictive performance of models, providing a boolean accuracy value or decimal ranging between $0$ and $1$.
\end{itemize}

\vpara{Final Score.} We calculate the final score through the following definition:
\small
\begin{equation}
    \vspace{-0.1cm}
    \text{Score}=0.05 \times \sum_{i=1}^{5}\text{F}_i + 0.05 \times \text{S}_\text{VLM} + 0.05 \times \text{SR} + 0.65 \times \text{CR},
    \vspace{-0.05cm}
\end{equation}
\normalsize
here, the final Score is computed by combining the weighted sum of five metrics $\text{F}_{i}$ (where i ranges from $1$ to $5$), a value VLM score, a value SR, and a value CR. Each factor is multiplied by its respective weight (0.05 or 0.65).