\section{Appendix}
\label{sec: appendix}

\subsection{Task Definition}
\label{ssec: task_des}
We define six typical data science tasks as follows:

1) \textbf{Data cleaning and preprocessing.} This task detects and processes missing values, outliers, and duplicate data; and standardizes data formats, such as a uniform format for dates and times.
    
2) \textbf{Data exploration and statistics understanding.} This task calculates basic statistical indicators of data (mean, median, standard deviation, etc.), generates data distribution charts (histograms, box plots, etc.), calculates correlations between variables, and draws correlation matrices or maps. 
    
3) \textbf{Data visualization.} The goal of this task is to visualize and analyze data and create interactive charts so users can freely explore the data.
    
4) \textbf{Predictive modeling.} The task aims to select the appropriate machine learning algorithm, such as linear regression, decision tree, random forest, etc.; carry out feature engineering, such as feature selection, feature transformation, feature combination, etc.; the data set is divided into the training set and test set, and the model is trained and evaluated; and select the corresponding evaluation indicators for different prediction problems, such as classification, regression or clustering. 
    
5) \textbf{Data mining and Pattern recognition.} This task uses association rule mining, frequent item set mining, and other methods to find interesting patterns in the data; Text mining technology is used to extract keywords, topics, and other information from text data; and apply cluster analysis, classification algorithms, etc. to identify underlying patterns and structures. 
Pattern recognition tasks can conduct these functions: image recognition, text clustering, and time series detection.
    
6) \textbf{Interpretability and Report generation}. This task aims to provide explanations of model results, such as feature importance, model parameters, etc., and automatically generate reports and summaries that present the results of the analysis in a way that is easy to understand and share. 


\subsection{Comparison with Existing Benchmarks}
\label{app_ssec: comparison}
We perform correlation analysis to evaluate the alignment between \benchmark\space and coding evaluations like BCB and LCB.
To achieve correlation analysis, we calculate both Pearson’s $r$ and Spearman’s $p$ correlation coefficients, which provide insights into the strength and direction of relationships between our benchmark and these established metrics. 
This analysis not only validates our results but also ensures robustness across different evaluation dimensions. Our findings indicate strong positive correlations, suggesting that our benchmark aligns well with these established coding evaluation metrics.

% \vspace{-0.2cm}
\begin{table}[t!]
    \centering
    \caption{Correlation with LCB and BCB.}
    \vspace{-0.3cm}
    \label{tab: correlation}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{lcc} 
    \specialrule{.16em}{0pt}{.65ex}
    & \multicolumn{2}{c}{\benchmark} \\
    \cmidrule(lr){2-3} 
    \cmidrule(lr){2-3} & $r$ & $p$ \\ 
    \specialrule{.10em}{.4ex}{.65ex}
    LiveCodeBench (LCB) & 0.853 & 0.673  \\ 
    BigCodeBench (BCB) & 0.823 & 0.808 \\
    \specialrule{.16em}{.4ex}{0pt}
    \end{tabular}
    }
\end{table}

While \benchmark\space does show a correlation with LCB or BCB in Table~\ref{tab: correlation}, our benchmark offers several unique and important contributions:

\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{Domain-Specific Focus:} \benchmark\space specifically targets data science and analytics tasks. However, existing benchmarks primarily focus on general programming problems. This specialization helps evaluate models' capabilities in handling real-world data analysis scenarios.

    \item \textbf{Task Diversity:} Our \benchmark\space includes unique task types like data preprocessing, visualization, and statistical analysis. These tasks are underrepresented in current benchmarks. This provides deeper insights into models' data science-specific capabilities.

    \item \textbf{Complementary Insights:} While overall correlations exist, we observe meaningful differences in model rankings. For example, models like Meta-Llama-3-8B-Instruct and CodeLlama-34B-Instruct show distinct performance patterns. These differences highlight capabilities specific to data science tasks that other benchmarks may not capture.
\end{itemize}
\textcolor{black}{The correlation with existing benchmarks validates our evaluation methodology, while our domain-specific focus provides valuable new insights for assessing AI models in data science applications.}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/TFC.pdf}
    \caption{An example of TFC tuples.}
    \label{fig: tfc}
\end{figure}


\subsection{Motivation and Example of Task-Function-Code (TFC)}
\textcolor{black}{
The TFC framework was developed to address several critical challenges in automated evaluation of data science tasks:}
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{Systematic Task Selection:} TFC provides a structured approach to identify and categorize key tasks across six established types. This systematic organization ensures comprehensive coverage of essential data science operations and helps maintain evaluation consistency and completeness.
    \item \textbf{Standardized Evaluation Metrics:} Data science tasks often lack standardized evaluation criteria. TFC addresses this by explicitly defining appropriate evaluation functions for each task. For example, data preprocessing tasks require specific metrics that differ from visualization tasks. This standardization ensures fair and consistent assessment.
    \item \textbf{Automated Execution Framework:} TFC includes executable code components for both tasks and evaluation metrics. This automation significantly improves evaluation efficiency, result reproducibility, and testing scalability.
    \item \textbf{Ground Truth Generation:} TFC serves as a crucial foundation for establishing ground truth, particularly valuable for complex tasks where ground truth is not readily available, and enables systematic verification and validation of model outputs.
\end{itemize}
Overall, the TFC structure in Figure~\ref{fig: tfc} represents a novel contribution by providing a comprehensive framework that bridges the gap between task definition, evaluation criteria, and automated assessment in data science contexts.


\subsection{Programmatic Rules}
We list all programmatic rules in Table~\ref{tab: program_rules}.
\input{tables/program_rules}


\subsection{Caveats when Using LLMs for Data Science}
Here we list the issues that occurred during testcase generation, most of which have been addressed by modifying the prompts. We notice that some of the issues may be disruptive to the system and some may be subtle but important.
\begin{enumerate}
    \item Be careful when using LLMs on well-known open source datasets, especially with customized tasks and data split. LLMs may memorize some open-source datasets. For example, if we want to use part of the penguin dataset that does not contain certain columns, the model (GPT-4o) will still explicitly process those columns in the code.
    \item Hallucination during data pre-processing. For example, when the model is required to merge two CSV, it may hallucinate on a common column and not go through all the columns in the files to find the actual ones.
    \item On multilingual tasks. LLMs may not be able to select the correct encoding. For instance, when they are required to open a CSV file that has content in Chinese, they will struggle to choose the correct encoding to open the file. Even if they are hinted that the file is in Chinese, they may choose encodings other than ``gbk'', e.g., ``latin''.
\end{enumerate}


\input{6_related_works}


\subsection{Related Work in Data Science}
Recently, some evaluation benchmarks for large language models in data science have been proposed. 
Text2Analysis~\citep{he2024text2analysis} constructs the evaluation benchmark to evaluate the model's ability to handle data analysis functions and fuzzy questions on tabular data. Their prompts are obtained through manual annotation and large model generation.
Furthermore, DAEval~\citep{huinfiagent} is developed as another evaluation benchmark and it contains 257 data analysis questions on CSV data and questions, which are generated by LLMs.
However, the prompts in these two works often only involve one task, and these prompts involve relatively simple data analysis operations.
In practical data science analysis tasks, user questions often involve multiple tasks and involve performing complex data analysis operations.
Therefore, we aim to provide a data science evaluation benchmark that is more in line with practical scenarios, especially for problems involving multiple subtasks and complex data analysis operations.



\subsection{Results}
We present the detailed results for our collected prompt and BigCodeBench source in Table~\ref{tab:our_results} and Table~\ref{tab:bcb_results}.

\input{tables/our_prompt_results}

\input{tables/bcb_results}


\subsection{AI Assistants In Writing}
While writing this paper, we employed AI to rectify grammatical errors and revise unreasonable descriptions.



\subsection{VLM-as-a-judge Prompt and Examples}
\label{sec:vlm_prompt}
In this section, we define the criteria to measure the quality of LLM-generated figures/charts.


\input{appendix/0_prompt_vlm}
\clearpage
\input{appendix/1_vlm_1}

\input{appendix/1_vlm_2}

\input{appendix/1_vlm_3}



\subsection{Qualified Prompts}
\begin{itemize}
    \item \textbf{Original Prompt 1:}    

There is a dataset with missing values in a CSV file, which records the region, height, weight, age, and salary of 36 individuals. Please address the following issues: \\(a) Calculate the proportion of missing values in each column and select the rows with at least two non-missing values in the last three columns. \\(b) Please fill in the weight column reasonably by combining the data from the height and region columns.
    
    \item \textbf{Qualified Prompt 1:}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Qualified Prompt 1:]
Given a dataset with missing values in a file named \texttt{'data.csv'}which records the region, height, weight, age, and salary of 36 individuals, please address the following issues:\\
(a) Calculate the proportion of missing values in each column and select the rows with at least two non-missing values in the last three columns. Save your output in a CSV file named \texttt{'missing\_values\_proportion\\.csv'}.\\
(b) Fill in the weight column reasonably by combining the data from the height and region columns. Save this updated dataset in a CSV file named \texttt{'updated\_data.csv'}.
\end{tcolorbox}

\item \textbf{Original Prompt 2:}
You are required to analyze and visualize the "Global Terrorism Database" from Kaggle. Please load the dataset and perform data cleaning by handling missing values, removing duplicates, and correcting any anomalies. Conduct an exploratory data analysis (EDA) to understand the distribution and relationships within the dataset. Calculate basic statistical indicators such as mean, median, standard deviation, and provide summary statistics for key features like attack type, target type, and region. Generate visualizations to uncover patterns and insights. Create histograms and box plots to display the distribution of numerical features, and bar plots to show the frequency of categorical variables. Use scatter plots and heatmaps to visualize relationships and correlations between features. Identify patterns in the data related to terrorist activities. For instance, determine trends over time, geographical hotspots, and common attack methods. Use clustering techniques ( K-means clustering) to identify patterns and group similar incidents together.
  

    \item \textbf{Qualified Prompt 2:}


\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Qualified Prompt 2:, text width=\textwidth]
You are required to analyze and visualize the \textit{Global Terrorism Database} from Kaggle. Please follow the steps below:
\begin{enumerate}
    \item Load the dataset \\
    Input: \texttt{gtd.csv} \\
    Output: \texttt{loaded\_data.csv} (This should contain the original data loaded without any modifications.)
    
    \item Data Cleaning 
    \begin{itemize}
        \item Handle missing values
        \item Remove duplicates
        \item Correct anomalies
    \end{itemize}
    Input: \texttt{loaded\_data.csv} \\
    Output: \texttt{cleaned\_data.csv} (This should reflect the cleaned dataset, ready for analysis.)
    
    \item Exploratory Data Analysis (EDA) 
    \begin{itemize}
        \item Calculate basic statistical indicators such as mean, median, and standard deviation
        \item Provide summary statistics for key features (attack type, target type, region)
    \end{itemize}
    Input: \texttt{cleaned\_data.csv} \\
    Output: \texttt{eda\_summary\_statistics.csv} (This should include all calculated statistics for key features.)
    
    \item Generate Visualizations 
    \begin{itemize}
        \item Create histograms and box plots for numerical features
        \item Generate bar plots for categorical variables
        \item Use scatter plots and heatmaps to visualize relationships and correlations
    \end{itemize}
    Input: \texttt{cleaned\_data.csv} \\
    Output: \texttt{visualizations.pdf} (This should include all visualizations generated in a single PDF file.)
    
    \item Identify Patterns in Data Related to Terrorist Activities 
    \begin{itemize}
        \item Determine trends over time
        \item Identify geographical hotspots
        \item Analyze common attack methods
    \end{itemize}
    Input: \texttt{cleaned\_data.csv} \\
    Output: \texttt{patterns\_analysis.csv} (This should summarize the identified patterns, trends, and hotspots.)
    
    \item Clustering Techniques 
    \begin{itemize}
        \item Use K-means clustering to identify patterns and group similar incidents
    \end{itemize}
    Input: \texttt{cleaned\_data.csv} \\
    Output: \texttt{clustering\_results.csv} (This should include the results of the clustering analysis, showing which group each incident belongs to.)
\end{enumerate}

Ensure that each output file reflects the quality of the completion of the respective subtask for further evaluation.
\end{tcolorbox}
    
\end{itemize}


\clearpage\newpage


\subsection{Modified Prompts}

\begin{itemize}
    \item \textbf{Original Prompt 1:}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Modified Prompt 1:, text width=\textwidth]
Searches a directory for CSV files matching a given regular expression pattern, reads sales data from these files, and plots the sales data with month on the x-axis and sales on the y-axis.\\
Note that: Each CSV file contains two columns: \texttt{Month} and \texttt{Sales}.\\
The function should output with: \\
A list of \texttt{matplotlib.axes.\_axes.Axes} objects, each representing a plot of sales data from a matched CSV file.\\
You should write self-contained code starting with:
\begin{verbatim}
import os
import pandas as pd
import re
import matplotlib.pyplot as plt
def task_func(directory: str, pattern: str) -> list:
\end{verbatim}
\end{tcolorbox}



    \item \textbf{Modified Prompt 1:}

\onecolumn
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Modified Prompt 1:, text width=\textwidth]
Search a directory for CSV files matching a given regular expression pattern, read sales data from these files, and plot the sales data with month on the x-axis and sales on the y-axis.\\

Input Requirements:
\begin{itemize}
    \item Input Directory: \texttt{data}.
    \item Input Pattern: \texttt{"csv\_\textbackslash d+\textbackslash.csv"}.
\end{itemize}

Output Requirements:
\begin{enumerate}
    \item A list of \texttt{matplotlib.axes.\_axes.Axes} objects representing the plot of sales data from each matched CSV file.
    \item Save each plot as a separate image file:
    \begin{itemize}
        \item File format: PNG
        \item Output filenames: \texttt{"sales\_plot\_<filename>.png"} where \texttt{<filename>} is the name of the CSV file without the extension.
    \end{itemize}
\end{enumerate}
Input File Specification:
\begin{itemize}
    \item Each CSV file should contain two columns: \texttt{'Month'} and \texttt{'Sales'}. The input files will be located in the specified directory.
\end{itemize}

You should write self-contained code starting with:
\begin{verbatim}
import os
import pandas as pd
import re
import matplotlib.pyplot as plt

def task_func(directory: str, pattern: str) -> list:
\end{verbatim}

\end{tcolorbox}

\clearpage
\newpage

    
    \item \textbf{Original Prompt 2:}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Modified Prompt 1:, text width=\textwidth]
Plot a scatter graph of tuples and highlight the tuple with the maximum value at index 1.\\
The function should output with: \\
\texttt{matplotlib.axes.Axes}: The Axes object of the plot for further manipulation and testing, with the title 'Max Tuple Highlighted', x-axis labeled 'x', y-axis labeled 'y', and a legend.\\
You should write self-contained code starting with:
\begin{verbatim}
import numpy as np
from operator import itemgetter
import matplotlib.pyplot as plt
def task_func(data):
\end{verbatim}
\end{tcolorbox}



    \item \textbf{Modified Prompt 2:}
    
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Modified Prompt 2:, text width=\textwidth]
Plot a scatter graph of tuples and highlight the tuple with the maximum value at index 1 using the input data from \texttt{"data.csv"}. The function should output the following: \\

A scatter plot saved as \texttt{"scatter\_plot.png"} with the title \texttt{'Max Tuple Highlighted'}, x-axis labeled \texttt{'x'}, y-axis labeled \texttt{'y'}, and a legend. The highlighted point should signify the tuple with the maximum value at index 1. \\

Please write self-contained code starting with:
\begin{verbatim}
import numpy as np
from operator import itemgetter
import matplotlib.pyplot as plt
def task_func(data):
\end{verbatim}
\end{tcolorbox}

\end{itemize}



\subsection{Prompt examples of different difficulty levels}
\input{appendix/3_easy_1}


\input{appendix/3_easy_2}


\input{appendix/3_medium_1}


\input{appendix/3_hard_1}


\clearpage\newpage
\section{Error Analysis and Case Study}
We have analyzed the common errors during problem-solving, summarized them here, and then studied several cases, including a successful case and a failure case.
The common errors include:
\begin{enumerate}
    \item Coding errors when solving data science problems using codes. And based on our observation, the main kind of these is execution error. It may be due to different reasons. For example, hallucination on the column name of a csv file.
    \item Json format errors. These errors come from the agent framework side, where they use json format to wrap up actions, e.g. \texttt{WriteAnalysis}.
\end{enumerate}



\subsection{Successful cases}

\input{appendix/4_success_1}

\input{appendix/4_success_2}

\input{appendix/4_success_3}

\input{appendix/4_success_4}



\subsection{Failure Case}
\label{appendix: worse_example}
\input{appendix/4_bad_o1_1}
\input{appendix/4_bad_o1_2}
\input{appendix/4_bad_o1_3}
\input{appendix/4_bad_o1_4}


\input{appendix/4_fail_1}

\input{appendix/4_fail_2}
\input{appendix/4_fail_3}


\begin{itemize}
\item \textbf{The expected format should at least begin and end with three backticks and must comply with JSON format. Here's an example:}


\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Expected JSON format:, text width=\textwidth]
\begin{verbatim}
"
[
    {
        "task_id": "1",
        "dependent_task_ids": [],
        "instruction": "Preprocess and clean the data.",
        "task_type": "preprocessing and cleaning"
    },
    {
        "task_id": "2",
        "dependent_task_ids": ["1"],
        "instruction": "Explore the data to find 
        patterns and insights.",
        "task_type": "data exploration"
    },
    {
        "task_id": "3",
        "dependent_task_ids": ["2"],
        "instruction": "Visualize the data to communicate insights.",
        "task_type": "data visualization"
    },
    {
        "task_id": "4",
        "dependent_task_ids": ["3"],
        "instruction": "Generate an interpretability report.",
        "task_type": "interpretability report"
    }
]
"
\end{verbatim}
\end{tcolorbox}

\end{itemize}