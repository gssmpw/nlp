\section{Introduction}
\label{sec: introduction}
Large language models (LLMs)~\citep{achiam2023gpt, team2023gemini, glm2024chatglm} are increasingly used in data science and scientific domains, e.g., data analysis~\citep{hong2024datainterpreter}, protein generation~\citep{jumper2021highly, chen2024xtrimopglm}, and scientific discovery~\citep{lu2024ai} and reasoning~\citep{zhang2024sciglm, zhang2024rest}.
For data science tasks, given a publicly known problem, LLMs offer the potential to (semi-)autonomously conduct data analysis~\citep{huang2023mlbenchmarking} and data visualization~\citep{hong2024datainterpreter} by invoking code interpreters with corresponding Python libraries.
These works are benchmarked on relatively straightforward tasks where ground truth (GT) labels can be precisely obtained.
However, much of real-world data analysis requires reasoning over more complex scenarios~\citep{chen2024viseval} as shown in Figure~\ref{fig: motivation}, such as calculating expenditure, and evaluating the quality of the images generated by the data visualization task.
Properly evaluating these more complex data science tasks remains an open research direction.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.93\linewidth]{figures/motivation.pdf}
    \vspace{-0.2cm}
    \caption{This example compares the vanilla response and the \benchmark\space response for a given prompt. The vanilla response provides only code, lacking evaluation metrics. In contrast, \benchmark\space identifies evaluation tasks, provides evaluation functions, and generates programmatic code to form a TFC list.}
    \label{fig: motivation}
\end{figure}

\input{tables/comparison}


While some existing benchmarks are used to evaluate LLMs for related challenges (see Table~\ref{tab: comparison}), those benchmarks (e.g., MLAgentBench, Text2Analysis) typically focus on evaluating narrower tasks with easy-to-obtain ground truth and straightforward evaluation metrics (e.g., Acc.).
For example, MLAgentBench~\citep{huang2023mlbenchmarking} presents a machine learning research benchmark by building an LLM Agent pipeline. 
As shown in Figure~\ref{fig: motivation}, the frontier of LLM evaluation is towards more complex real-world tasks that consist of multiple subtasks. For these challenging prompts, how to generate GT and define specific evaluation metrics for each subtask in a comprehensive perspective is a question worth exploring.


To address the gap between task selection, evaluation function definition, and automated code execution in data science contexts, we introduce a new benchmark, called \benchmark, which evaluates the data science abilities of LLMs and provides insights to help LLMs improve their data analysis and data visualization abilities.
Regarding collected prompts, their corresponding responses, and evaluation metrics, we hope that prompts meet the following characteristics:
1) Require more natural, challenging, and high-quality prompts to promote the development of LLMs' improvement;
2) Identify \textbf{6} defined data science tasks, e.g., data preprocessing, statistics, visualization, mining, and interpretability;
3) Require multiple types of results to perform comprehensive evaluations and distinguish models well.


In this paper, to efficiently generate GT and the evaluation metric for key tasks, we propose a novel semi-automated framework called \underline{\textbf{T}}ask - \underline{\textbf{F}}unction - \underline{\textbf{C}}ode (TFC) to address critical challenges in the automated evaluation of data science tasks.
Specifically, from a coarse-grained perspective, we first aggregate the scope of task types, evaluation functions, and corresponding codes.
Then, from a fine-grained perspective, we define programmatic rules for each function's output based on the specific tasks and compare generation results with ground truth to ensure a fair and consistent assessment. 

To validate the effectiveness of LLMs on collected \textbf{222} comprehensive prompts with proposed \textbf{519} GTs, we experiment with \textbf{6} API-based models, \textbf{8} open-sourced general models, and \textbf{9} open-sourced code generation models.
We observe that API-based models greatly outperform open-sourced models on average.
Specifically, GPT-4o surpasses all other models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. 
However, all models have significant room for improvement in following fine-grained instructions, calling the appropriate tools, executing accurate plans, and exporting the required execution outputs.

Overall, our key contributions are as follows:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item We introduce \benchmark, a comprehensive benchmark designed to assess the performance of LLMs in data science tasks in Figure~\ref{fig: framework}. We develop a semi-automated pipeline to generate ground truth and evaluate aggregated metrics using carefully crafted complex questions.
    \item We propose an innovative Task-Function-Code (TFC) evaluation framework based on predefined aggregated metrics and programmatic rules. We then assess \textbf{23} large language models from both coarse-grained and fine-grained perspectives, presenting the results in Table~\ref{tab:overall_results}.
    \item Various analyses of comparisons and correlations between ours and existing benchmarks are performed in Figure~\ref{fig: ours_vs_humaneval} and Table~\ref{tab: correlation}. Furthermore, we provide research insights derived from experimental outcomes of the evaluated LLMs that point to interesting directions for future work.
\end{itemize}

