\begin{table*}[t!]
    \centering
    \caption{Evaluation results for \benchmark$\space$ (our collected prompts).}
    \vspace{-0.2cm}
    \label{tab:our_results}
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
    \specialrule{.16em}{0pt}{.65ex}
        \multirow{2}{*}{Models} & \multirow{2}{*}{Size} & \multicolumn{2}{c|}{Coarse-grained Metrics} & \multicolumn{6}{c|}{Fine-grained Metrics} & \multirow{2}{*}{Score}
        \\
        \cmidrule{3-10}
        & & Success (\%) & CR (\%) & VLM & F1 & F2 & F3 & F4 & F5 & \\
        \specialrule{.10em}{.4ex}{.65ex}
        o1-mini                      & N/A                      & 13.45  & 15.43      & 2.87 & 53.75 & \text{0.00}  & 45.50 & 46.25 & 25.79 & 19.41 \\
        GPT-4o-2024-05-13            & N/A                      & \textbf{19.82}  & \textbf{17.89}      & \textbf{3.91} & \textbf{71.79} & \text{0.00}  & \textbf{54.50} & \underline{70.00} & 33.95 & \textbf{24.32} \\
        GPT-4o-mini                  & N/A                      & 12.73  & 17.35      & 3.05 & 62.86 & \textbf{15.00} & \underline{50.50} & 69.38 & \textbf{43.68} & 24.13 \\
        GPT-4-Turbo                  & N/A                      & \underline{17.27}  & \underline{17.36}      & \underline{3.09} & \underline{65.18} & 10.00 & 44.50 & \textbf{78.12} & \underline{41.05} & \underline{24.24} \\
        Claude-3-5-Sonnet-20240620   & N/A                      & 8.00      & 11.12      & 2.14 & 32.32 & \text{0.00}  & 36.00 & 63.12 & 17.63 & 15.19 \\
        GLM-4-Flash                  & N/A                      & 9.82   & 7.43       & 1.33 & 47.14 & \text{0.00}  & 22.00 & 24.38 & 3.95  & 10.26 \\
        \specialrule{.10em}{.4ex}{.65ex}
        Meta-Llama-3.1-8B-Instruct   & 8B                       & 10.00     & 7.72       & 1.29 & 45.18 & \text{0.00}  & 16.00 & 23.12 & 8.95  & 10.24 \\
        Meta-Llama-3-8B-Instruct     & 8B                       & 1.64   & 1.43       & 0.52 & 7.86  & \text{0.00}  & 4.50  & 6.88  & \text{0.00}  & 2.00  \\
        Gemma-2-9B-it                & 9B                       & 5.64   & 5.51       & 1.06 & 26.79 & \text{0.00}  & 13.00 & 22.50 & 2.89  & 7.18  \\
        GLM-4-9B-Chat                & 9B                       & 10.55  & 9.96       & 1.69 & 55.36 & \text{0.00}  & 31.00 & 28.75 & 21.32 & 13.91 \\
        Qwen2.5-7B-Instruct          & 7B                       & 11.64  & 10.11      & 1.43 & 55.36 & \text{0.00}  & 36.50 & 33.12 & 18.42 & 14.40 \\
        Qwen2-7B-Instruct            & 7B                       & 6.91   & 5.90       & 1.16 & 32.50 & \text{0.00}  & 18.00 & 21.88 & 2.37  & 7.97  \\
        Qwen2-1.5B-Instruct          & 1.5B                     & 1.82   & 1.60       & 0.40 & 3.57  & \text{0.00}  & 2.00  & 13.12 & 0.79  & 2.13  \\
        Yi-1.5-9B-Chat-16K           & 9B                       & 6.18   & 4.25       & 0.73 & 30.36 & \text{0.00}  & 16.00 & 8.75  & 3.95  & 6.06  \\
        \specialrule{.10em}{.4ex}{.65ex}
        CodeLlama-34B-Instruct    & 34B                      & \text{0.00}      & 0.03       & \text{0.00} & \text{0.00}  & \text{0.00}  & \text{0.00}  & \text{0.00}  & \text{0.00}  & 0.02  \\
        CodeLlama-13B-Instruct    & 13B                      & 0.73   & 0.50       & 0.04 & 4.46  & \text{0.00}  & \text{0.00}  & 3.75  & \text{0.00}  & 0.77  \\
        CodeLlama-7B-Instruct     & 7B                       & 0.55   & 0.27       & \text{0.00} & 1.96  & \text{0.00}  & \text{0.00}  & \text{0.00}  & \text{0.00}  & 0.30  \\
        StarCoder2-15B               & 15B                      & 0.18   & 0.20       & 0.07 & 0.54  & \text{0.00}  & \text{0.00}  & 0.62  & \text{0.00}  & 0.20 \\
        Deepseek-Coder-33B-instruct  & 33B                      & 12.55  & 13.53      & 2.29 & 62.86 & \text{0.00}  & 43.00 & 51.88 & 21.32 & 18.49 \\
        Deepseek-Coder-6.7B-instruct & 6.7B                     & 12.55  & 13.56      & 1.93 & 63.21 & \text{0.00}  & 39.00 & 53.75 & 21.05 & 18.39 \\
        Deepseek-Coder-1.3B-instruct & 1.3B                     & 0.73   & 0.61       & 0.10 & 3.39  & \text{0.00}  & \text{0.00}  & 1.25  & \text{0.00}  & 0.67  \\
        Qwen2.5-Coder-7B-Instruct    & 7B                       & 6.18   & 7.87       & 1.48 & 40.18 & \text{0.00}  & 27.50 & 33.75 & 4.47  & 10.79 \\
        Qwen2.5-Coder-1.5B-Instruct  & 1.5B                     & 6.18   & 7.52       & 0.81 & 38.57 & \text{0.00}  & 15.50 & 40.00 & 10.53 & 10.46 \\
        \specialrule{.16em}{.4ex}{0pt}
    \end{tabular}
    }
\end{table*}