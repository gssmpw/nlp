\begin{table*}[t!]
    \centering
    \vspace{-0.2cm}
    \caption{Overall evaluation results for \benchmark$\space$ on all our curated prompts.}
    \vspace{-0.2cm}
    \label{tab:overall_results}
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
    \specialrule{.16em}{0pt}{.65ex}
        \multirow{2}{*}{Models} & \multirow{2}{*}{Size} & \multicolumn{2}{c|}{Coarse-grained Metrics} & \multicolumn{6}{c|}{Fine-grained Metrics} & \multirow{2}{*}{Score}
        \\
        \cmidrule{3-10}
        & & SR (\%) & CR (\%) & VLM & F1 & F2 & F3 & F4 & F5 & \\
        \specialrule{.10em}{.4ex}{.65ex}
        o1-mini                      & N/A  & 29.77 & 45.26 & 2.87 & 44.63 & 19.27 & 36.01 & 30.94 & 23.81 & 38.78\\
        GPT-4o-2024-05-13            & N/A  & \textbf{66.31} & \textbf{68.44} & \textbf{3.91} & \textbf{75.93} & \textbf{56.14} & \textbf{69.33} & \textbf{71.35} & \textbf{57.67} & \textbf{64.51}\\
        GPT-4o-mini                  & N/A  & 50.63 & 57.78 & 3.05 & 60.30 & \underline{48.02} & 57.84 & 59.24 & \underline{53.54} & 54.18\\
        GPT-4-Turbo                  & N/A  & 51.93 & 58.87 & \underline{3.09} & 62.30 & 41.62 & 57.75 & 60.25 & 50.75 & 54.65\\
        Claude-3-5-Sonnet-20240620   & N/A  & 47.48 & 58.11 & 2.14 & 49.07 & 36.94 & 55.84 & 52.87 & 46.04 & 52.29\\
        GLM-4-Flash                  & N/A  & 30.32 & 34.04 & 1.33 & 36.53 & 29.42 & 32.57 & 27.64 & 14.44 & 30.74\\
        \specialrule{.10em}{.4ex}{.65ex}
        Meta-Llama-3.1-8B-Instruct   & 8B   & 24.73 & 33.89 & 1.29 & 38.24 & 18.25 & 21.98 & 22.89 & 25.85 & 29.69\\
        Meta-Llama-3-8B-Instruct     & 8B   & 2.88  & 3.92  & 0.52 & 4.18  & 1.26  & 2.70  & 2.67  & 1.47  & 3.33\\
        Gemma-2-9B-it                & 9B   & 7.07  & 11.00 & 1.06 & 26.16 & 16.90 & 23.81 & 18.11 & 17.15 & 12.66\\
        GLM-4-9B-Chat                & 9B   & 25.72 & 30.38 & 1.69 & 31.51 & 23.15 & 28.07 & 27.19 & 19.14 & 27.57\\
        Qwen2.5-7B-Instruct          & 7B   & 43.83 & 50.74 & 1.43 & 51.18 & 36.41 & 47.25 & 45.24 & 34.77 & 45.99\\
        Qwen2-7B-Instruct            & 7B   & 22.84 & 25.58 & 1.16 & 30.93 & 20.78 & 28.73 & 25.87 & 7.52  & 23.52\\
        Qwen2-1.5B-Instruct          & 1.5B & 3.96  & 5.46  & 0.40 & 4.54  & 1.98  & 3.26  & 5.76  & 4.71  & 4.78\\
        Yi-1.5-9B-Chat-16K           & 9B   & 38.20 & 42.35 & 0.73 & 38.14 & 36.36 & 35.64 & 37.08 & 27.79 & 38.22\\
        \specialrule{.10em}{.4ex}{.65ex}
        CodeLlama-34B-Instruct    & 34B  & 0.90  & 1.47  & \text{0.00} & 1.02  & 0.84  & 1.98  & 1.54  & 1.19  & 1.33\\
        CodeLlama-13B-Instruct    & 13B  & 10.49 & 14.64 & 0.04 & 11.67 & 11.34 & 9.43  & 14.43 & 5.15  & 12.64\\
        CodeLlama-7B-Instruct     & 7B   & 2.88  & 3.97  & \text{0.00} & 3.53  & 2.37  & 2.57  & 1.74  & 1.59  & 3.31\\
        StarCoder2-15B               & 15B  & 2.07  & 2.61  & 0.07 & 2.57  & 1.81  & 1.59  & 3.43  & 1.19  & 2.33\\
        Deepseek-Coder-33B-instruct  & 33B  & \underline{55.86} & \underline{61.23} & 2.29 & \underline{65.66} & 47.11 & \underline{58.17} & \underline{61.65} & 48.60 & \underline{56.76}\\
        Deepseek-Coder-6.7B-instruct & 6.7B & 37.03 & 41.62 & 1.93 & 43.49 & 34.57 & 46.36 & 46.49 & 18.09 & 38.45\\
        Deepseek-Coder-1.3B-instruct & 1.3B & 15.50 & 19.00 & 0.10 & 13.04 & 14.62 & 13.26 & 16.32 & 7.92  & 16.39\\
        Qwen2.5-Coder-7B-Instruct    & 7B   & 45.18 & 53.11 & 1.48 & 51.58 & 43.21 & 43.87 & 42.50 & 35.23 & 47.67\\
        Qwen2.5-Coder-1.5B-Instruct  & 1.5B & 22.74 & 28.64 & 0.81 & 29.82 & 21.79 & 23.96 & 29.58 & 16.39 & 25.87\\
        \specialrule{.16em}{.4ex}{0pt}
    \end{tabular}
    }
\end{table*}