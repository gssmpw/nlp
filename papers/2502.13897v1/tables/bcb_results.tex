\begin{table*}[t!]
    \centering
    \caption{Evaluation results for \benchmark$\space$ (BigCodeBench source).}
    \vspace{-0.2cm}
    \label{tab:bcb_results}
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \specialrule{.16em}{0pt}{.65ex}
    \multirow{2}{*}{Models} & \multirow{2}{*}{Size} & \multicolumn{2}{c|}{Coarse-grained Metrics} & \multicolumn{5}{c|}{Fine-grained Metrics} & \multirow{2}{*}{Score} \\
    \cmidrule{3-9}
    & & Success (\%) & CR (\%) & F1 & F2 & F3 & F4 & F5 & \\
    \specialrule{.10em}{.4ex}{.65ex}
    o1-mini                      & N/A                      & 35.15  & 55.08      & 41.62 & 25.62 & 32.89 & 25.90 & 23.16 & 47.77 \\
    GPT-4o-2024-05-13            & N/A                      & \textbf{81.62}  & \textbf{85.09}      & \textbf{77.30} & \textbf{74.63} & \textbf{74.21} & \textbf{71.79} & \textbf{65.48} & \textbf{81.81} \\
    GPT-4o-mini                  & N/A                      & 63.11  & 71.10       & 59.46 & 58.89 & 60.26 & 55.90 & 56.79 & 67.49 \\
    GPT-4-Turbo                  & N/A                      & 63.35  & 72.54      & 61.35 & 52.04 & 62.11 & 54.36 & 53.95 & 68.14 \\
    Claude-3-5-Sonnet-20240620   & N/A                      & 60.48  & 73.59      & 54.59 & 49.11 & 62.37 & 49.49 & 55.39 & 68.08 \\
    GLM-4-Flash                  & N/A                      & 37.07  & 42.8       & 33.04 & 39.11 & 36.05 & 28.72 & 17.89 & 39.55 \\
    \specialrule{.10em}{.4ex}{.65ex}
    Meta-Llama-3.1-8B-Instruct   & 8B                       & 29.58  & 42.51      & 35.95 & 24.26 & 23.95 & 22.82 & 31.41 & 38.16 \\
    Meta-Llama-3-8B-Instruct     & 8B                       & 3.29   & 4.74       & 2.97  & 1.67  & 2.11  & 1.28  & 1.96  & 3.98  \\
    Gemma-2-9B-it                & 9B                       & 7.54   & 12.81      & 25.95 & 22.46 & 27.37 & 16.67 & 21.84 & 15.06 \\
    GLM-4-9B-Chat                & 9B                       & 30.72  & 37.11      & 23.65 & 30.78 & 27.11 & 26.67 & 18.42 & 33.84 \\
    Qwen2.5-7B-Instruct          & 7B                       & 54.43  & 64.12      & 49.80 & 48.40 & 50.79 & 49.23 & 40.15 & 59.52 \\
    Qwen2-7B-Instruct            & 7B                       & 28.08  & 32.06      & 30.41 & 27.63 & 32.26 & 27.18 & 9.21  & 30.18 \\
    Qwen2-1.5B-Instruct          & 1.5B                     & 4.67   & 6.73       & 4.86  & 2.63  & 3.68  & 3.33  & 6.00  & 5.97  \\
    Yi-1.5-9B-Chat-16K           & 9B                       & 48.74  & 54.9       & 40.70 & 48.34 & 42.11 & 46.41 & 35.64 & 51.53 \\
    \specialrule{.10em}{.4ex}{.65ex}
    CodeLlama-34B-Instruct    & 34B                      & 1.20    & 1.94       & 1.35  & 1.11  & 2.63  & 2.05  & 1.58  & 1.85  \\
    CodeLlama-13B-Instruct    & 13B                      & 13.71  & 19.3       & 14.05 & 15.07 & 12.53 & 17.95 & 6.84  & 17.52 \\
    CodeLlama-7B-Instruct     & 7B                       & 3.65   & 5.19       & 4.05  & 3.15  & 3.42  & 2.31  & 2.11  & 4.57  \\
    StarCoder2-15B               & 15B                      & 2.69   & 3.41       & 3.24  & 2.41  & 2.11  & 4.36  & 1.58  & 3.21  \\
    Deepseek-Coder-33B-instruct  & 33B                      & \underline{70.12}  & \underline{76.94}      & \underline{66.58} & \underline{62.63} & \underline{63.16} & \underline{64.87} & \underline{57.59} & \underline{73.11} \\
    Deepseek-Coder-6.7B-instruct & 6.7B                     & 45.09  & 50.86      & 37.00 & 45.96 & 48.79 & 44.10 & 17.11 & 47.50 \\
    Deepseek-Coder-1.3B-instruct & 1.3B                     & 20.36  & 25.05      & 16.22 & 19.44 & 17.63 & 21.28 & 10.53 & 22.81 \\
    Qwen2.5-Coder-7B-Instruct    & 7B                       & 58.02  & 68.01      & 55.34 & 57.44 & 49.26 & 45.38 & 45.36 & 63.15 \\
    Qwen2.5-Coder-1.5B-Instruct  & 1.5B                     & 28.20   & 35.60       & 26.94 & 28.96 & 26.74 & 26.15 & 18.32 & 32.69 \\
    \specialrule{.16em}{.4ex}{0pt}
    \end{tabular}
    }
\end{table*}