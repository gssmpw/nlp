\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/all_models.pdf}
    \vspace{-0.3cm}
    \caption{Overall score results of all tested LLMs.}
    \vspace{-0.3cm}
    \label{fig: final_evaluation}
\end{figure}

\section{Results and Analysis}

\subsection{Overall Performance}

We demonstrate overall experiment results in Table~\ref{tab:overall_results} and Figure~\ref{fig: final_evaluation}. The key findings are: \textbf{1) Concerning average performance, API-based models outperform open-sourced models.} Among API-based models, GPT-4o achieves the highest total score of $64.51\%$, attaining a significant $9.86\%$ advantage over GPT-4-Turbo, which achieves $54.65\%$ total score. GPT-4o also surpasses all other models on all metrics, indicating its comprehensive capacity over various aspects. 
In comparison, the performance variance between API-based models is smaller than that of open-sourced models.
\textbf{2) As for open-sourced models, the performance gap between general models and code generation models is insignificant.} Among those, Deepseek-Coder-33B-Instruct achieves the highest score of $56.76\%$, even outperforming various API-based models like o1-mini and GPT-4-Turbo. Other models like Qwen2.5-Coder-7B-Instruct and Qwen2.5-7B-Instruct also show fair good capability, attaining total scores of $47.67\%$ and $45.99\%$, respectively. In contrast, a few models only pass very few tasks, achieving total scores even lower than $5.0\%$. Of these, CodeLlama-34B-Instruct unexpectedly achieves a score of $1.33\%$, even lagging behind its small-scale version CodeLlama-7B-Instruct. In this regard, we also present an analysis of the anomaly in Section~\ref{sec: insights}. 


\subsection{Study with Different Difficulty Levels}
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/task_difficulty.pdf}
    \vspace{-0.3cm}
    \caption{Average Completion Rate results regarding different difficulty levels.}
    \vspace{-0.3cm}
    \label{fig: task_level}
\end{figure}
To evaluate multiple LLMs on their ability to complete prompts of varying difficulty, we categorized tasks using BCB and data formatted in CSV, human handwritten prompts, and data science-related DL tasks as easy - 167, medium - 30, and hard levels - 25, respectively. 
We assessed multiple LLMs by combining different difficulty levels, overall average CR, and the average CR for each difficulty level. From the Figure~\ref{fig: task_level}, it can be observed that: 
\textbf{1)} Some LLMs, like GPT-4o, GPT-4o-mini, GPT-4-Turbo, and Deepseek-Coder-33B-Instruct, exhibit consistent performance across all difficulty levels, indicating robustness.
\textbf{2)} Models such as GPT-4 series and Deepseek-Coder-Instruct series are among the top performers, scoring high average CRs, particularly excelling in complex, data-driven tasks defined as hard.
\textbf{3)} There are noticeable performance disparities among general models and small-scale models in terms of average metrics, such CR, with some achieving lower scores overall, suggesting that general models are less efficient or accurate in data science tasks.


\begin{figure}[t!]
    \centering
\includegraphics[width=0.98\linewidth]{figures/DSB_vs_HumanEval.pdf}
    \vspace{-0.1cm}
    \caption{Pass@1 comparison of all tested LLMs between \benchmark$\space$ and HumanEval. Circle markers denote the API-based models while others denote various open-sourced LLMs. The green dashed areas indicate the LLMs perform well on the two benchmarks and the orange solid areas indicate performances of the two datasets are relatively mismatched. 
    }
    \label{fig: ours_vs_humaneval}
    \vspace{-0.1cm}
\end{figure}

\subsection{Comparison with HumanEval}
We compare our proposed \benchmark$\space$ with HumanEval.
As shown in Figure~\ref{fig: ours_vs_humaneval}, we observe that most LLMs are located in the upper triangular region of the graph and all tested models are divided into two groups, 
in which the green-dashed-line areas where LLMs perform well on the two benchmarks and the orange-solid-line area where performances on the two datasets with the same model indicate significant discrepancies.

\subsection{Deeper Insights into LLMs' Ability}
\label{sec: insights}
% \vspace{-0.2cm}
With curated metrics, we can obtain deeper research insights into LLMs' ability to plan and execute complex data science tasks. The experiment results also raise questions that are worth exploring since some results do not conform to conventional perceptions.


\vpara{Models demonstrate proficiency in reasoning tasks but may not consistently excel in complex data science tasks that necessitate precise adherence to detailed instructions, utilization of existing tools, and strategic planning.}
Although, indeed, data science coding tasks often involve scheduling and step-by-step execution similar to reasoning scenarios, results show that even the LLMs proficient in reasoning tasks can still fail to complete complex data science tasks. For instance, the o1-mini model, which is commonly regarded as one of the best reasoning models, unexpectedly failed on many of \benchmark's tasks. The model only achieves a 29.77\% overall success rate, significantly lagging behind the company's previously introduced models like GPT-4o and GPT-4-Turbo. After examining the completions generated by o1-mini, we discovered that the failures are primarily caused by non-compliance with instructions, incorrect calls, and forgetfulness. While successfully splitting the task into multiple subtasks, the model often forgets to export required execution outcomes or just outputs undesired data. In other cases, the model may falsely call a library function or method that sometimes does not even exist. 
The main kind of coding error is an execution error. An example is a hallucination about the column name of a CSV file.
These facts remind us that real-life data science coding tasks often comprehensively challenge the model's ability to follow fine-grained instructions, utilize existing tools (libraries, APIs$\ldots$), and do planning. To excel and align effectively with these tasks, a model must demonstrate competitiveness across all relevant aspects.
We included four bad examples generated by o1-mini for further clarification in Appendix~\ref{appendix: worse_example}.

\vpara{Larger-scale models may encounter challenges in following simple instructions due to the extensive data utilized during training, potentially leading to difficulties in generating outputs in formats different from those within the training data.}
StarCoder2-15B performs worse than some smaller models, and CodeLlama-34B-Instruct even performs worse than 13B and 7B versions. The main reason is that the larger-scale version lacks some other ability like generating formatted text according to prompts. Perhaps a large amount of data in a certain format is being used to train a larger version that fails to follow the prompt to generate another format different from that. We present some examples in Appendix~\ref{appendix: worse_example}. Indeed, the larger-scale model of CodeLlama also fails to outperform the smaller-scale version in LCB.

