%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{float}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{xcolor}

\definecolor{lightblue}{rgb}{0.5, 0.7, 0.90}
\definecolor{lavender}{rgb}{0.6, 0.5, 0.9}
% \definecolor{lavender}{HTML}{C8C8FA} 
\definecolor{softgreen}{rgb}{0.6, 0.8, 0.6}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\flushbottom


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{DMWM: Dual-Mind World Model with Long-Term Imagination}

\begin{document}

\twocolumn[
\icmltitle{DMWM: Dual-Mind World Model with Long-Term Imagination}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lingyi Wang}{yyy}
\icmlauthor{Rashed Shelim}{yyy}
\icmlauthor{Walid Saad}{yyy}
\icmlauthor{Naren Ramakrishnan}{comp}

%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical and Computer Engineering, Virginia Tech, USA}

\icmlaffiliation{comp}{Department of Computer Science, Virginia Tech, USA}

% \icmlcorrespondingauthor{Lingyi Wang}{lingyiwang@vt.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{World model, logical reasoning, dual-process system}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors.
    Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. 
    DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment.
    The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite.
    Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models. 
\end{abstract}

\vspace{-0.3cm}
\section{Introduction}
Imagination is a core capability of world models that allows agents to predict and plan effectively within internal virtual environments by using real-world knowledge \cite{lin2020improving,zhu2020bridging,mattes2023hieros,cohen2024improving}. By predicting future scenarios in latent spaces, agents can evaluate potential outcomes without frequent real-world interactions thus significantly improving data efficiency and minimizing trial-and-error costs. For complex tasks requiring long-term planning, imagination capabilities allow world models to evaluate the long-term consequences of diverse strategies and identify the optimal action plans. Consequently, the effectiveness of model-based decision-making approaches, such as model-based reinforcement learning (RL) \cite{hafner2019dream,hafner2020mastering,hafner2023mastering,wang2024ad3} and model predictive control (MPC) \cite{hafner2019learning,hansen2022temporal,sv2023gradient}, can heavily depend on the quality of their imagination abilities.

One of the most widely used frameworks for world models is the so-called recurrent state-space model (RSSM) \cite{hafner2019learning,zhu2020bridging,du2024learning} and its variants \cite{hansen2023td,sun2024learning} that combine deterministic recurrent structures with stochastic latent variables to model environmental dynamics in a compact latent space. By doing so, RSSM models can capture the sequential dependencies and uncertainty of their target environment.
However, RSSM faces challenges in providing reliable, long-term predictions over extended imagination horizons due to the accumulation of prediction errors and the limitations of statistical inference \cite{ke2018modeling,simeonov2021long,clinton2024planning}. In particular, although existing RSSM-based solutions can generate accurate short-term predictions in a single-step manner, small errors inevitably propagate over longer time horizons \cite{lambert2021learning}, gradually amplifying over each step and resulting in significant deviations between the imagined and actual states. Moreover, RSSM schemes often optimize state-space representations through reconstruction loss or regression. This approach can lead to overfitting to observed patterns and cannot properly capture latent dynamics \cite{lambert2021learning,simeonov2021long}, particularly in complex, dynamic environments.

Several models have been proposed to enhance long-term planning. For instance, trajectory-based models \cite{lambert2021learning} learn long-term dynamics by directly predicting future states to reduce compounding errors compared to single-step prediction models. However, the absence of intermediate states impedes accurate action decisions, particularly in complex, high-precision tasks. Latent space-enhanced models \cite{ke2018modeling,simeonov2021long} incorporate abstracted long-term information, such as high-level goals and global knowledge, to mitigate error propagation in each single-step prediction. However, these long-term planning approaches \cite{lambert2021learning,ke2018modeling,simeonov2021long} still rely on statistical inference and lack logical consistency along with interpretability over extended horizons.

Motivated by the challenges of long-term imagination and the limitations of existing long-term planning approaches, we propose a novel dual-mind world model (DMWM) framework based on the dual-process theory of human cognition \cite{frankish2010dual,evans2003two}. The proposed DMWM framework can achieve reliable and efficient imagination by combining the complementary strengths of System 1 and System 2 learning processes \cite{hua2022system}. By designing this framework, we make the following key contributions:
\vspace{-0.3cm}
\begin{itemize}
    \item We propose DMWM, a novel world model framework consisting of RSSM-based System 1 (RSSM-S1) component and logic-integrated neural network-based System 2 (LINN-S2) component inspired by the dual-process theory of human cognition. In DMWM, the RSSM-S1 component predicts the state transitions in a fast, intuitive-driven manner, while the LINN-S2 component guides the imagination process through logical reasoning at a higher level.  
    \vspace{-0.3cm}

    \item In LINN-S2, we introduce logical regularization rules to conduct logical reasoning within the state and action spaces by using operations such as $\land$, $\lor$, $\neg$ and $\rightarrow$. The logical rules allows the logical consistency and interpretability of the world model. Additionally, we propose a recursive logic reasoning framework that extends local reasoning into globally consistent long-term planning, enabling the modeling of logical sequence dependencies in complex tasks.
    
    \item We design an inter-system feedback mechanism. In particular, LINN-S2 provides logical constraints to guide RSSM-S1 so as to ensure that predicted sequences are consistent with domain-specific logical rules. For the feedback based on real-world data from RSSM-S1, it updates the domain-specific logics of LINN-S2, thereby allowing dynamic refinement and adaptation.
    
    \item We evaluate our proposed DMWM with actor-critic based RL and MPC in extensive experiments. The simulation results demonstrate that DMWM is able to respectively provide 14.3\%, 5.5-fold, 32\% and 120\% improvement in logic consistency, trial efficiency, data efficiency and reliable imagination over extended horizon size compared to baselines in complex tasks.
\end{itemize}

\vspace{-0.5cm}
\section{Proposed DMWM Framework}
In this section, we introduce the proposed DMWM framework with long-term imagination capabilities inspired by the dual-process theory of human cognition. DMWM consists of RSSM-S1 and LINN-S2. First, we introduce RSSM-S1, which builds upon the RSSM architecture to learn the environment dynamics in a latent space and perform fast, intuitive state representations and predictions. Next, based on the LINN, we introduce a novel LINN-S2 to capture the intricate logical relationships between the state space and the action space.
By employing a hierarchical deep reasoning framework, LINN-S2 facilitates structured reasoning and enforces logical consistency over extended horizons. Finally, we provide an inter-system feedback mechanism. The pipeline of the proposed DMWM framework is shown in Figure 1.
\begin{figure}[t]
    \label{fg:1}
    \centering
    \includegraphics[width=1\linewidth]{Fig/Pipline.png}
    \vspace{-0.7cm}
    \caption{The pipeline of the proposed framework with DMWM.}
    \vspace{-0.3cm}
\end{figure}

\vspace{-0.2cm}
\subsection{RSSM-based System 1}
The RSSM-S1 component is based on DreamerV3 \cite{hafner2023mastering}.
\begin{align*}
    \begin{array}{ll}
    \text{Deterministic State: }  &h_t = f_\varphi\left(h_{t-1}, z_{t-1}, a_{t-1}\right),\\
    \text{Encoder:}  &z_t \sim q_\varphi\left(z_t \mid h_t, o_t\right), \\
    \text{Stochastic State: }  &\hat{z}_t \sim p_\varphi\left(\hat{z}_t \mid h_t\right), \\
    \text{Reward Predictor: }  &\hat{r}_t \sim p_\varphi\left(\hat{r}_t \mid h_t, z_t\right), \\
    \text{Decoder: }  &\hat{o}_t \sim p_\varphi\left(\hat{o}_t \mid h_t, z_t\right),
    \end{array}  
\end{align*} 
with the deterministic state $h_{t}$, observation $o_{t}$, predicted observation $\hat{o}_{t}$, stochastic state $z_{t}$, predicted stochastic state $\hat{z}_{t}$, action $a_{t}$ and the predicted reward $\hat{r}_t $ at time step $t$. RSSM-S1 achieves an effective balance between deterministic and stochastic states, enabling efficient data-driven prediction similar to the intuitive and automatic processes of System 1. The deterministic state captures data patterns, and the stochastic state models inherent uncertainty and variability for dynamic and complex environments. 

\textbf{System 1 loss. } RSSM-S1 is optimized by using the loss functions of DreamerV3.

\begin{equation}
    \begin{aligned}
    &\mathcal{L}_{\text {S1 }}(\varphi) = \mathcal{L}_{\text {pred }}(\varphi) + \varpi_{_{\text {dyn }}} \mathcal{L}_{\text {dyn }}(\varphi) + \varpi_{_{\text {rep }}}  \mathcal{L}_{\text {rep }}(\varphi), \\
    &\mathcal{L}_{\text {pred }}(\varphi) = -\ln p_\varphi\left(o_t \mid z_t, h_t\right)-\ln p_\varphi\left(r_t \mid z_t, h_t\right)\\
    &\quad \quad \quad \quad \quad -\ln p_\varphi\left(c_t \mid z_t, h_t\right), \\
    &\mathcal{L}_{\text {dyn }}(\varphi) = \operatorname{KL}\left[\operatorname{sg}\left(q_\varphi\left(z_t \mid h_t, o_t\right)\right) \| p_\varphi\left(z_t \mid h_t\right)\right], \\
    &\mathcal{L}_{\text {rep }}(\varphi) = \operatorname{KL}\left[q_\varphi\left(z_t \mid h_t, o_t\right) \| \operatorname{sg}\left(p_\varphi\left(z_t \mid h_t\right)\right)\right],
    \end{aligned}
\end{equation}
where $\varpi_{_{\text {dyn}}}$ and $\varpi_{_{\text {rep}}}$
are respectively the weight factors of the dynamic loss $\mathcal{L}_{\text {dyn }}$ and the representation loss $\mathcal{L}_{\text {rep }}$, and $\operatorname{sg}(\cdot)$ represents the stop-gradient operator.

\textbf{System 1 limitations. } The RSSM can be regarded as a computational counterpart of System 1 within dual-process theories of cognition, doing well in fast, intuitive-driven reasoning. However, it is inherently constrained by two critical limitations: \textit{the absence of explicit logical reasoning} and \textit{the inability to maintain coherence over extended temporal horizons}. While RSSM-S1 enables efficient and immediate response through pattern recognition and intuitive prediction, these strengths come at the cost of enforcing logical consistency or inferring causal relationships in complex scenarios. Moreover, RSSM-S1's focus on short-term processing limits its ability to integrate information across long periods, resulting in fragmented or inconsistent predictions in tasks that demand global planning or long-term foresight. These inherent limitations demonstrate the need to complement the capabilities of RSSM-S1 with mechanisms similar to System 2, which does well in structured reasoning and sustained temporal coherence, to construct a more robust and comprehensive cognitive world model framework.

\subsection{Logic-integrated neural network-based System 2}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Fig/Logics.png}
    \vspace{-0.2cm}
    \caption{Logic reasoning with LINN-S2 for DMWM}
    \vspace{-0.8cm}
\end{figure}
Next, we introduce the LINN-S2 component whose deep logical reasoning is illustrated in Figure 2. In LINN-S2, the states and actions are encoded as logic vector inputs, enabling logical deduction through operations such as negation (\(\neg\)), conjunction (\(\land\)), disjunction (\(\lor\)), and implication (\(\rightarrow\)). To establish these logical operations, the LINN framework \cite{shi2020neural} serves as the foundational module for capturing and reasoning about structural logical relationships between the state embedding space and the action embedding space within the world model.
To further enhance the logical reasoning capabilities, we incorporate a set of regularization rules specifically designed for implication-based operations.

\textbf{Neural modules for basic logic operations. } In logical reasoning for world models, it is essential to uncover structural information and semantic relationships across different embedding spaces, specifically the action space and the state space. \cite{shi2020neural} proposed a straightforward concatenation-based method for LINN but it limits that input vectors are from the same source space. Moreover, the straightforward concatenation-based method overlooks the semantic disparities between states and actions, risking the loss of critical information and failing to capture complex cross-space logical relationships.

To address the need for cross-space logical reasoning in world models, we propose to explore the action embeddings and apply the Kronecker product for cross-space feature alignment, which preserves logic integrity and captures second-order relationships. 
The state (imagination) logical embedding $v$ and the state logical embedding $m$ are obtained with multilayer perception (MLP) by
\begin{equation}
    v = \boldsymbol{W}^s_{2}f\left(\boldsymbol{W}^s_{1}(z)+\boldsymbol{b}^s_w\right), m = \boldsymbol{W}^a_{2}f\left(\boldsymbol{W}^a_{1}(z \oplus a)+\boldsymbol{b}^a_w\right),
\end{equation}
where $\oplus$ is the operation of vector concatenation, and $z \oplus a$ aims to capture the action logic within the state context.
The formulations of operations $(\mathrm{AND}, \mathrm{OR}, \mathrm{NOT})$ are given as follows
\begin{equation}
\begin{split}
    \mathrm{AND}(v,m)&=\boldsymbol{W}^a_{2}f\left(\boldsymbol{W}^d_{1}(v\oplus m)+\boldsymbol{b}^d_w \right)\\
    &\quad +\mathrm{Conv}2\mathrm{D}(v\otimes m,\boldsymbol{K}^d)+\boldsymbol{b}^d_k,
\end{split}
\end{equation}
\begin{equation}
    \begin{split}    
\mathrm{OR}(v,m)&=\boldsymbol{W}^o_{2}f\left(\boldsymbol{W}^o_{1}
    (v\oplus m)+\boldsymbol{b}^o_w \right)\\
    & \quad +\mathrm{Conv}2\mathrm{D}(v\otimes m,\boldsymbol{K}^o)+\boldsymbol{b}^o_k,
\end{split}
\end{equation}
\begin{equation}
\operatorname{NOT}(v)=v + \boldsymbol{W}^n_2 f\left(\boldsymbol{W}^n_1 v+\boldsymbol{b}^n_w\right),
\end{equation}
where $v \in \mathbb{R}^{d}$, $m\in \mathbb{R}^{d}$, $\boldsymbol{W}^l_{1} \in \mathbb{R}^{d \times 2 d}, \boldsymbol{W}^l_{2} \in \mathbb{R}^{d \times d}, \boldsymbol{b}^l_{w} \in \mathbb{R}^d, \boldsymbol{b}^l_{k} \in \mathbb{R}^d$ are the parameters of the logical neural networks, $l\in \{d,o,n\}$, $\mathrm{Conv}2\mathrm{D}(\cdot)$ represents the convolution neural network, $\boldsymbol{K}$ is the convolution core, $\otimes$ is the operation of Kronecker product,  and $f(\cdot)$ is the activation function. 

It is clear that logical operations capture intricate logical correlations that cannot be fully encapsulated by the geometric properties of the embedding space. For instance, the logical independence in $\mathrm{NOT}(v)$, reflected in the relationship between $v$ and $\neg v$, does not correspond precisely to vector orthogonality. Logical operations are governed by logical axioms (e.g., identity, annihilator, and complementation), which impose algebraic constraints that transcend purely geometric interpretations, which are introduced in the logic regularization part. 

\textbf{Implication relationship for state reasoning. } 
Based on the basic logical operations ($\operatorname{AND}$, $\operatorname{OR}$, $\operatorname{NOT}$),  we implement the implication operation (\( p_\varphi \rightarrow q_\varphi \)) to enable logical reasoning within the state and action logical embedding spaces. The implication operation is critical for assessing the rationality of a predicted state given the current imagination and action embeddings, which is derived through the equivalence relationship:
\begin{equation}
p \rightarrow q \Longleftrightarrow \neg p \vee q.
\end{equation}
Hence, the operation IMPLY for $v \rightarrow m$ can be realized based on the fundamental operations of negation ($\neg$) and conjunction ($\land$), which is represented by
\begin{equation}
\operatorname{IMPLY}(v,m)= \operatorname{OR}(\operatorname{NOT}(v),m).
\end{equation}
\vspace{-0.2cm}

\begin{table*}[h!]
    \centering
    \vspace{-0.2cm}
    \caption{Partital Logical Regularizers and Rules for Implication $\rightarrow$}
    \label{tab:logical_regularizers}
    \begin{tabular}{l l l l l}
    \hline
    \hline
    &\textbf{Logical Rule} & \textbf{Equation}  & \textbf{Logic Regularizer $r_i$} \\
    \hline
    \multirow{4}*{\textbf{IMPLY}}&\textbf{Identity} & $w \rightarrow \mathbf{T} = \mathbf{T}$  & $r_{11} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w),\mathbf{T}), \mathbf{T})$ \\
    &\textbf{Annihilator} & $w \rightarrow \mathbf{F} = \neg w$  & $r_{12} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), \mathbf{F}), \mathrm{NOT}(w))$ \\
    &\textbf{Idempotence} & $w \rightarrow w = \mathbf{T}$  & $r_{13} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), w), \mathbf{T})$ \\
    &\textbf{Complementation} & $w \rightarrow \neg w \equiv \neg w$ & $r_{14} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), \mathrm{NOT}(w)), \mathrm{NOT}(w))$ \\
    \hline
    \hline
    \end{tabular}
    \vspace{-0.2cm}
\end{table*}
\vspace{-0.3cm}

\textbf{Logical regularizations. }
To ensure that neural modules accurately perform logical operations, 
\cite{shi2020neural} introduced logic regularizers $\{r_i\}$ that enforce adherence to fundamental logical rules, thereby constraining module behavior. While neural networks can implicitly learn logical operations from data, the explicit incorporation of logical constraints improves model consistency, interpretability, and robustness while maintaining efficient neural computation. Derived from principles of $\mathrm{AND}$, $\mathrm{OR}$, and $\mathrm{NOT}$, logic regularizers establish a unified inference framework with fundamental and advanced operations. This approach aligns model outputs with human-understandable reasoning, bridging neural networks and formal logic, and enabling effective learning, representation, and inference of logical formulas for interpretable reasoning in world models.

We extend the foundational framework of logical regularization in \cite{shi2020neural} by introducing implication regularizers to address advanced logical rules. As summarized in Table \ref{tab:logical_regularizers}, the implication rules refine basic logical structures to handle intricate constructs. By explicitly encoding principles such as \textit{contraposition} ($w \rightarrow v \equiv \neg v \rightarrow \neg w$) and \textit{complementation} ($w \rightarrow \neg w \equiv \neg w$), LINN-S2 improves its capacity to manage compound logical expressions while ensuring consistency across implication operations.

The logical regularizers for $\mathrm{IMPLY}$ ($r_{11}$-$r_{14}$), partial depicted in Table \ref{tab:logical_regularizers}, combined with the basic logical regularizers for $\mathrm{AND}$, $\mathrm{OR}$, and $\mathrm{NOT}$ ($r_{1}$-$r_{10}$), collectively define the regularization loss function $L_r$, which is expressed as
\begin{equation}
    \mathcal{L}_{\text{reg}}=\frac{1}{N_{r}} \sum_i r_i,
\end{equation}
where $r_i$ represents individual logical regularizers and $N_{r}$ presents the total number of the regularizers. The complete table of the logical regularizers is given in Appendix \ref{lgr}.

\textbf{Symbolic representation of deep logical reasoning. }
In this part, we present the symbolic representation of LINN-S2 for deep logical reasoning. Specifically, the logical reasoning process is formalized by using symbolic logic to represent logical relationships in sequences. By leveraging logical operations ($\land, \rightarrow$), LINN-S2 constructs a hierarchical logical reasoning framework, facilitating systematic and interpretable reasoning over sequential dependencies. The framework of the hierarchical logical reasoning is illustrated in Figure 2, and three detailed process procedures are given as follows.

\textit{1) Local logical composition}: Each state (imagination) logical embedding $v_t$ and action logical embedding $m_t$ are combined by using the logical conjunction ($\land$) to capture the intrinsic logic relationship
\vspace{-0.1cm}
\begin{equation}
    c_t : v_t \land m_t, \quad \forall t < T.
\end{equation}
The composition $c_t$ establishes localized logical features based on the existing $v_t$ and $m_t$.

\textit{2) Recursive implication reasoning}: To ensure logical consistency across states (imagination), each composition ($c_t$) undergoes an implication operation ($\rightarrow$) that aligns the logical information with the subsequent state (imagination)
\vspace{-0.1cm}
\begin{equation}
    \phi_t : c_t \rightarrow z_{t+1}, \quad \forall t< T.
\end{equation}
The recursive formulation (9) encodes sequential dependencies and enforces consistency in predictive state transitions across the hierarchy. However, $\phi_t$ provides only single-step logical inference, and it lacks reasoning depth and comprehensiveness. To address this limitation, we propose incorporating deterministic historical information into the logical reasoning framework. The deep recursive implication reasoning is represented by
\vspace{-0.1cm}
\begin{equation}
    \phi^{\alpha}_t : c_{t-\alpha} \cdots \land c_{t-1} \land c_t \rightarrow z_{t+1}, \forall \alpha < t < T,
\end{equation}
where $\alpha$ is the inference depth. 
In complex scenarios with long-term dependencies, the model utilizes logical relationships from past states to ensure coherent reasoning. Capturing logic across time steps retains historical information, strengthens sequential dependencies and enhances global consistency. This prevents inference bias from information loss during multi-step reasoning and ensures the reliability of long-term imagination.

\textit{3) Global logical chain}: The global reasoning process integrates local consistency and recursive reasoning into a unified global logical chain $L_{T}$ within the time period $T$
\begin{equation}
    L^{\alpha}_{T} = \phi^{\alpha}_1 \land \phi^{\alpha}_2 \land \phi^{\alpha}_3 \cdots \phi^{\alpha}_{T-1} \rightarrow \mathbf{T},
\end{equation}
where $\mathbf{T}$ represents the terminal consistency condition, ensuring that the state (imagination) sequences align with the reasoning objectives. This formulation consolidates local information into a globally consistent reasoning framework.

In this way, LINN-S2 enhances the formal interpretation and mathematical rigor of world models along with several key features. First, the hierarchical structure encodes deep logical reasoning as a layered process, which allows logical consistency between state and action representations. Second, logical composition leverages logical operations for binding states and actions in a logical manner. This composition facilitates formal logic principles and enables modular, interpretable reasoning. Finally, the recursive implication structure ensures robustness and interpretability for long-term imagination.

\textbf{System 2 loss. } The loss function of the logic reasoning with the inference depth $\alpha$ is represented by
\begin{equation}
    \begin{split} 
    \mathcal{L}^{\alpha}_{\text{log}} & = \frac{1}{T-1} \sum_t   \mathcal{S}(\phi^{\alpha}_t, \mathbf{T}) - \mathcal{S}(\phi^{\alpha}_t, \mathbf{F}) \\
    & = \frac{1}{T-1} \sum_t  \mathcal{S}(\phi^{\alpha}_t, \mathbf{T}) - \mathcal{S}(\phi^{\alpha}_t, \mathrm{NOT}(\mathbf{T})),
    \end{split}
\end{equation}
where the $\mathcal{S}$ is the logic similarity metric that takes value between 0 and 1. Here, we use the vector cosine similarity since the logical information of action and state spaces has been aligned in the logical vector space, given by
\begin{equation}
\mathcal{S}(v,m) = \sigma \left( \kappa \frac{v \cdot m}{\| v \| \| m \|} \right).
\end{equation}
To ensure the logical consistency of basic operations of AND and OR by order-independence,
i.e., $v \land m = m \land v$ and $v \lor m = m \lor v$, the inputs of AND and OR are randomly disrupted.
Moreover, the \( \ell_2 \)-regularization term \( \mathcal{L}_v \) \cite{shi2020neural} is employed to prevent the vector lengths from exploding, which could otherwise lead to trivial solutions (e.g., logical rules becoming ineffective) during optimization, and constraint on the model parameters to mitigate the risk of overfitting, represented by
\begin{equation}
    \mathcal{L}_{\ell_2} = \sum_{v \in \mathcal{V}}\|v\|_F^2 + \sum_{m \in \mathcal{M}}\|m\|_F^2 + \sum_{w \in \mathcal{W}} \|w\|_F^2,
\end{equation}
where \( \mathcal{W} \) represents the model parameters of LINN-S2. The loss function of System 2 is expressed as
\vspace{-0.2cm}
\begin{equation}
    \mathcal{L}_{\text{S2}}(w) = \sum_{\alpha = 0}^{\Lambda} \mathcal{L}^{\alpha}_{\text{log}} + \beta_{\text{reg}} \mathcal{L}_{\text{reg}} + \beta_{\ell_2} \mathcal{L}_{\ell_2},
\end{equation}
where $\beta_{\text{reg}}$ and $\beta_{\ell_2}$ represents the weight factors, and $\Lambda$ denotes the maximum reasoning depth.

\subsection{Inter-System Feedback Mechanisms}
\textbf{Feedback from S1 to S2. }
During the real-environment interactions, LINN-S2 updates the domain-specific logical relationships by using the actual state transitions from RSSM-S1. In particular, the state-action sequence $\{z_t,a_t,z_{t+1}\}_{t=0}^{T}$ generated by RSSM-S1 based on the observations $\{o_t\}_{t=0}^{T+1}$ serves as the labeled data, which is fed into LINN-S2 with the objective of minimizing the inference loss (12).

\textbf{Feedback from S2 to S1. }
To embed LINN-S2-inspired logical reasoning into RSSM-S1, we propose a logic feedback that utilizes LINN-S2's logical consistency mechanisms to guide RSSM-S1. By unifying high-level logical structures with low-level representations, the approach fosters a more robust and coherent model for imagination. We particularly introduce the logical rule and rederive the variational evidence lower bound (ELBO) \cite{hafner2019dream} with a logic inference term.
\begin{equation}
p_\varphi(o_{1:T}, z_{1:T} \mid a_{1:T}) = \prod_{t=1}^T p_\varphi(o_t \mid z_t)\tilde{p}_{\phi}(z_t \mid z_{t-1}, a_{t-1}),
\end{equation}
where $\tilde{p}_{\phi}(z_t \mid z_{t-1}, a_{t-1}) \propto p_\varphi(z_t \mid z_{t-1}, a_{t-1}) \cdot \mathcal{C}(\phi(z_t, z_{t-1}, a_{t-1}))$ and $\mathcal{C}(\phi)$ is the logical consistency function that measures whether the logical rule $\phi$ is satisfied by imagination. The ELBO of the observation loss can be expressed as (Derivation in Appendix \ref{elbo})
\vspace{-0.2cm}
\begin{align*}
& \ln p_\varphi\left(o_{1: T} \mid a_{1: T}\right) = \int p_\varphi(o_{1:T}, z_{1:T} \mid a_{1:T}) \, dz_{1:T} \\
& \ge \sum_{t=1}^T\left( \underbrace{\mathbb{E}_{q_1}\left[\ln p_\varphi\left(o_t \mid z_t\right)\right]}_{\mathrm{Decoding}} \right. + \underbrace{\mathbb{E}_{q_1}\left[\ln \mathcal{C}(\phi(z_t, z_{t-1}, a_{t-1}))\right]}_{\mathrm{Logic \ Inference}}  \\
& \left. -\underbrace{\mathbb{E}_{q_2}\left[\operatorname{KL}\left[q_\varphi\left(z_t \mid o_{\leq t}, a_{<t}\right) \| p_\varphi\left(z_t \mid z_{t-1}, a_{t-1}\right)\right]\right]}_{\mathrm{Prediction}}\right).
\end{align*}
where $q_1=q_\varphi\left(z_t \mid o_{\leq t}, a_{<t}\right)$ and $q2=q_\varphi\left(z_{t-1} \mid o_{\leq t-1}, a_{<t-1}\right)$.
The approximate state priors can be obtained by past observations and actions with the encoder
\begin{equation}
q_\varphi(z_{1:T} \mid o_{1:T}, a_{1:T}) = \prod_{t=1}^{T} q_\varphi(z_t \mid h_t, o_t).
\end{equation}
\vspace{-0.6cm}

% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.
\begin{table*}[h!]
    \label{tb:loc}
    \centering
    \vspace{-0.4cm}
    \caption{Performance comparison of our approach with various baselines on DMC tasks in terms of logic consistency. The mean and variance of logical consistency are reported over 100 test episodes with the horizon size $H=30$. Complete results of logical consistency over 20 tasks with varying horizon size are concluded in Appendix \ref{lct}.}
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    Env & Dreamer & Hieros & HRSSM & DMWM (\textbf{Proposed}) \\ 
    \hline
    Cartpole Balance & $0.683 \pm 0.057$ & $0.711 \pm 0.032$ & $0.713 \pm 0.041$ & $\boldsymbol{0.727 \pm 0.023}$ \\
    Pendulum Swingup & $0.611 \pm 0.137$ & $0.709 \pm 0.054$ & $0.699 \pm 0.079$ & $\boldsymbol{0.730 \pm 0.037}$\\
    Reacher Hard & $0.608 \pm 0.121$ & $0.702 \pm 0.062$ & $0.703 \pm 0.072$ & $\boldsymbol{0.730 \pm 0.042 }$\\
    Finger Turn Hard & $0.627 \pm 0.131$ & $0.698 \pm 0.061$ & $0.703 \pm 0.073$ & $\boldsymbol{0.725 \pm 0.029}$\\
    Cheetah Run & $0.643 \pm 0.131$ & $0.689 \pm 0.113$ & $0.695 \pm 0.087$ & $\boldsymbol{0.725 \pm 0.049 }$\\
    Cup Catch & $0.652 \pm 0.087$ & $0.701 \pm 0.072$ & $0.714 \pm 0.061$ & $\boldsymbol{0.728 \pm 0.021 }$\\
    Walker Walk & $0.612 \pm 0.140$ & $0.696 \pm 0.063$ & $0.701 \pm 0.073$ & $\boldsymbol{0.730 \pm 0.034 }$\\
    Quadruped Walk & $0.656 \pm 0.092$ & $0.701 \pm 0.067$ & $0.703 \pm 0.072$ & $\boldsymbol{0.723 \pm 0.039 }$ \\ 
    Hopper Hop & $0.633 \pm 0.127$ & $0.704 \pm 0.087$ & $0.701 \pm 0.092$ & $\boldsymbol{0.722 \pm 0.038 }$\\     
    \hline
    \end{tabular} 
    \vspace{-0.3cm}
\end{table*}

\section{Experimental Results and Analysis}
\vspace{-0.1cm}
In this section, we conduct extensive experiments to address the following key questions: (a) Can our model effectively capture logical relationships in dynamic environments?, (b) Does enhanced logical consistency enable our model to achieve higher task rewards under limited environment trials and data?, and (c) Over an extended horizon, can our model generate reliable long-term imagination?

\textbf{Experimental setup. }
The training environments consist of 20 continuous control tasks from DeepMind control (DMC) suite. We evaluate DMWM using two model-based decision-making approaches: An actor-critic reinforcement learning method and a gradient-based model predictive control (Grad-MPC) approach \cite{sv2023gradient}, referred to as DMWM-AC and DMWM-GD, respectively. Training details for DMWM-AC and DMWM-GD are provided in Appendix \ref{algs}. For comparison purposes and benchmarking, we include DreamerV3 \cite{hafner2023mastering} and Dreamer-enabled Grad-MPC \cite{sv2023gradient} as baselines. To highlight the limitations of using a single RSSM for world cognition, we compare our method against two state-of-the-art RSSM variants: Hieros \cite{mattes2023hieros} and HRSSM \cite{sun2024learning}. HRSSM improves representation robustness via masking and bisimulation to mitigate visual noise interference, while Hieros enhances long-term modeling and exploration efficiency through S5WM and hierarchical strategies. Details on environment and model settings are provided in Appendix \ref{hyp}.

\textbf{Logical consistency. }
Figure \ref{hm} shows the logical correlations between individual state-action pairs ($s_i \wedge$ $a_j$) and the target state ($s_{30}$). The diagonal shows strong correlations for one-step pairs $s_i \wedge a_i \rightarrow s_{30}$, which is the key path of localized reasoning. The off-diagonal elements show interdependency across different state-action pairs ($s_i \wedge a_j, i \neq j$) that propagate global logical information. The observed patterns highlight the need for deep logical reasoning to capture both short-term and long-term logical dependencies.

\begin{figure}
    \label{hm}
    \centering
    \includegraphics[width=0.85\linewidth]{Fig/DLHM.png}
    \vspace{-0.4cm}
    \caption{Heatmap of deep logic correlations for sequential imagination $s_0 \land a_0 \land ... s_{29} \land a_{29} \rightarrow s_{30}$ with reasoning depth $\alpha = 30$. The horizontal axis indicates the past states $s_i$ and the vertical axis indicates past actions $a_j$. The color of points represents the logical strength of state-action pairs to the long-term reasoning process.}
    \vspace{-0.7cm}
\end{figure}

Table 2 compares the logical consistency of our proposed DMWM against various RSSM baselines on DMC tasks. Logical consistency data for 20 tasks with different horizon sizes is provided in Appendix \ref{lct}. The proposed DMWM achieves state-of-the-art logical consistency in both mean logic loss and stability across all DMC environments. DMWM respectively achieves 14.3\%, 2.6\% and 3.3\% improvement of logic consistency compared to Dreamer, Hieros, and HRSSM. For instance, while the masking and hierarchical strategies in Hieros and HRSSM can reduce single-step propagation errors by mitigating environment noise, they still have difficulty in addressing long-term imagination due to predictive deviation in statistical inference and error accumulation. This highlights the limitations of relying solely on System 1 and the need for logical consistency from System 2 for robust long-term imagination.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9\linewidth]{Fig/EPS8.png}
    \vspace{-0.5cm}
    \caption{Performance comparison of results on 8 DMC tasks under limited environment trials, where the standard error is shaded in the distraction setting. The horizontal axis indicates the number of times that models explore the environments. The vertical axis indicates the average return over 100 test episodes. Complete results on 20 DMC tasks under limited environment interactions are concluded in Appendix \ref{eps}.}
    \vspace{-0.5cm}
\end{figure*}

\textbf{Trial efficiency. }
Figure 4 presents test returns under limited environment trials.
The number of environment trials serves as the x-axis to quantify exploration efficiency by measuring performance under the same environment interaction opportunities, which is a crucial factor for real-world applications and high-cost simulations.
The complete results for 20 DMC tasks under limited environment trials are provided in Appendix \ref{eps}.
The results show that DMWM-AC and DMWM-MPC significantly outperform baseline methods across most tasks with limited environment trials, particularly in complex environments such as Cheetah Run, Finger Turn Hard, and Quadruped Run. In contrast, Dreamer and GD-MPC exhibit limited performance in high-dimensional control tasks with lower learning efficiency and stability. As shown in Figure 4, DMWM approaches achieve an average 5.5-fold improvement in test return under limited environment trials under limited environment trials compared to baseline methods. These observations highlight that leveraging logical information from the environment enhances exploration efficiency, especially in complex tasks, enhancing DMWM's capability in long-term imagination.

\textbf{Data efficiency. }
Figure 5 presents test results under limited environment steps for the purpose of quantifying the data efficiency. The complete results for 20 tasks under limited environment steps are provided in Appendix \ref{es}.
DMWM approaches consistently outperform Dreamer and GD-MPC across most tasks in terms of convergence speed and final returns, particularly in high-dimensional dynamics such as Cheetah Run and Quadruped Run. These results highlight DMWM's superior long-term planning and dynamic modeling capabilities, enabling more efficient utilization of environment data. In tasks that demand simple dynamic modeling like Cartpole Balance and Cup Catch, all methods converge rapidly. In contrast, for complex tasks like Reacher Hard and Hopper Hop, DMWM approaches achieve stable performance and an average improvement of $32\%$ in test return under limited data compared to Dreamer and GD-MPC.

\begin{figure*}[!ht]
    \centering
    \label{de}
    \includegraphics[width=0.9\linewidth]{Fig/ES8.png}
    \vspace{-0.4cm}
    \caption{Performance comparison on 8 DMC tasks under limited environment steps, with standard error shaded in the distraction setting. The horizontal axis represents the number of environment interactions, while the vertical axis denotes the average test return over 100 episodes. Complete test results on 20 DMC tasks under limited environment steps are provided in Appendix \ref{es}.}
    \vspace{-0.2cm}
\end{figure*}

\textbf{Imagination ability over extended horizon size.}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Fig/HS8.png}
    \vspace{-0.2cm}
    \caption{Performance comparison on 8 DMC tasks across different horizon size. The horizontal axis indicates the horizon size. The vertical axis indicates the average test return over 100 episodes. Complete results on 20 DMC tasks across varying horizon size are provided in Appendix \ref{hs}.}
    \vspace{-0.4cm}
\end{figure*}
As shown in Figure 6, although a long-term prediction horizon introduces cumulative errors, DMWM-AC and DMWM-MPC consistently achieve high returns across most tasks and remain stable even over extended horizons. In stability-critical tasks such as Cartpole Balance and Pendulum Swingup, DMWM maintains strong performance across a wide range of prediction horizons, whereas Dreamer and GD-MPC are more susceptible to degradation due to prediction errors. For extended horizon size of $H>30$ in complex tasks,
DMWM approaches achieve an average 120\% improvement in test return compared to Dreamer and GD-MPC. These results emphasize the crucial role of logical reasoning in long-term imagination. Across most tasks, DMWM demonstrates superior performance over long-term horizons.

\vspace{-0.3cm}
\section{Conclusion and Limitations}
\textbf{Conclusion. }
In this paper, we have proposed DMWM, a novel world model framework for reliable long-term imagination that combines the fast, intuitive-driven RSSM-S1 with the structured, logic-driven inference of LINN. The feedback mechanisms between the two systems enhanced the logic coherence and adaptability. Extensive evaluations across diverse benchmarks demonstrated significant improvements in logical consistency, data-efficiency and reliable imagination over extended horizon size.

\textbf{Limitations and future work. }
A key limitation of our approach lies in its reliance on predefined simple domain-specific logical rules, which restricts its adaptability to environments where such rules are ambiguous or constantly evolving. This dependency limits the framework's ability to generalize to novel tasks. Future work could focus on enabling the model to autonomously learn and adapt logical rules from data by exploring causal relationships.
This, in turn, can reduce the need for explicit manual definitions and enhance flexibility in dynamic and complex environments.
\newpage
\section*{Impact Statement}
Inspired by the human cognitive model, this paper proposes a novel DMWM architecture that integrates intuitive RSSM-S1 with logic-driven LINN-S2 for the first time, which addresses long-horizon imagination for model-based RL and MPC. By enhancing logical consistency of the architecture, DMWM improves both efficiency and robustness in complex tasks. Because of these benefits, DMWM provides a practical and interpretable solution for real-world applications such as autonomous driving and robotic planning. 

Moreover, this work presents a generalized dual-mind framework for world models that can serve as a solid foundation for future research on general world models. We believe this approach marks a meaningful step toward artificial general intelligence (AGI) by bridging intuitive processes with logical reasoning. Hence, DWMW paves the way for more robust, logical and human-like decision-making systems.

\nocite{garrido2024learning,du2023video,hu2022model,lecun2022path,ye2021mastering,schrittwieser2020mastering,ma2024transformer,feldstein2024mapping}

\bibliography{example_paper}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Background}
\subsection{Actor-Critic}
The actor-critic model is a widely used algorithmic framework in Reinforcement Learning (RL), which combines the advantages of policy gradient methods and value function approximation. It addresses RL tasks through the collaborative learning of two primary components: the Actor (policy network) and the Critic (value network), given as follows:
\begin{equation}
    \begin{split}  
        &\text{Action model:}  \quad \quad a_\tau \sim q_{\vartheta}(a_\tau \mid s_\tau) \\
        &\text{Value model:}  \quad \quad v_\psi(s_\tau) \approx \mathbb{E}_{q(\cdot \mid s_\tau)} \left( \sum_{t=\tau}^H \gamma^{t-\tau} r_{\tau} \right).
    \end{split}
\end{equation}
In the world Model, the target of the actor-critic model is to maximize the reward over the imagined trajectories with the horizon size $H$. Specifically, the action model seeks to maximize an estimated value, while the value model strives to accurately predict the value estimate, which evolves as the action model updates. The training target of the Actor and the Value are given as follows \cite{sutton2018reinforcement, hafner2019dream,sv2023gradient}.
\begin{equation}
    \vartheta^* = \max_\vartheta \mathbb{E}_{q_\phi, q_\vartheta} \left[ \sum_{\tau=t}^{t+H} V_\lambda(s_\tau) \right]
\end{equation}
\begin{equation}
    \psi^* = \min_\psi \mathbb{E}_{q_\phi, q_\vartheta} \left[ \sum_{\tau=t}^{t+H} \frac{1}{2} \left( v_\psi(s_\tau) - V_\lambda(s_\tau) \right)^2 \right]
\end{equation}
\begin{equation}
    V_\lambda(s_\tau) = (1 - \lambda) \left( \sum_{n=1}^{H-1} \lambda^{n-1} V_n^N(s_\tau) \right) + \lambda^{H-1} V_H^N(s_\tau)
\end{equation}
\begin{equation}
    V_k^N(s_\tau) = \mathbb{E}_{q_\phi, q_\vartheta} \left[ \sum_{n=\tau}^{h-1} \gamma^{n-\tau} r_n + \gamma^{h-\tau} v_\psi(s_h) \right],
\end{equation}
where $h = \min(\tau + k, t + H)$, $\vartheta$ represents the parameters of the Actor $psi$ represents the parameters of the Critic, and $\lambda$ is the discount factor.

\subsection{Model Predictive Control}
Model predictive control (MPC) is an optimization-based control method extensively applied in engineering and industrial control systems \cite{muske1993model,kamthe2018data,wu2021statistical,hansen2023td}. By leveraging the system's dynamic model, MPC predicts future behavior through rolling optimization, generating optimal control inputs at each time step to achieve the desired system objectives. However, due to the fact that MPC strongly relies on the system model and the requirement for online optimization, it has difficulty in effective decision-making performance if the world model fails to provide stable and reliable imagined trajectories \cite{hafner2019learning, sv2023gradient}. The gradient-based MPC framework \cite{sv2023gradient} for world model is presented as 
\begin{equation}
    a_{t:t+H}^{*} = \max_{a_{t:t+H}^{(j)}} R^{(j)}, \quad R^{(j)} = \sum_{\tau=t+1}^{t+H+1} \mathbb{E} \big[ q_\phi(r_\tau \mid s_\tau^{(j)}) \big]
\end{equation}
\begin{equation}
    \left\{a_{t:t+H}^{(j)}\right\}_{j=1}^J \sim \mathcal{N}(\mu_t, \operatorname{diag}(\sigma_t^2))
    \end{equation}
\begin{equation}
    s_{t:t+H+1}^{(j)} \sim q_{\phi}(s_t \mid o^{(j)}_{1:t}, a^{(j)}_{1:t-1}) \prod_{\tau=t+1}^{t+H+1} p_{\phi}(s_\tau \mid s^{(j)}_{\tau-1}, a_{\tau-1}^{(j)})
\end{equation}
        
\begin{equation}
    a_{t:t+H}^{(j)} = a_{t:t+H}^{(j)} - \nabla R^{(j)},
\end{equation}
where $a_{t:t+H}^{(j)}$ and $s_{t:t+H}^{(j)}$ are sequences of states and actions of the candidate $j$ from the timestep $t$ to the timestep $t+H$.

\section{Algorithms}\label{algs}
\subsection{DMWM With Actor-Critic}
The training process of DMWM with actor-critic-based decision module is shown in Algorithm \ref{lb:a1}.
\begin{algorithm}[H]
    \caption{DMWM With Actor-Critic}
    \label{lb:a1}
 \begin{algorithmic}
    \STATE Hyper Parameters: seed episode $S$, training episodes $N$, batch size $B$, collect interval $C$, \\ $\quad \quad \quad \quad \quad \quad \quad \quad$ sequence length $L$, imagination horizon $H$, learning rate $\eta$.
    \STATE Initialize dataset $\mathcal{D}$ with $S$ seed episodes. 
    \STATE Initialize DMWM parameters $\phi, w$.
    \STATE Initialize Actor-Critic parameters $\vartheta, \psi$.
    \FOR{Training step $n \rightarrow N$}
    \FOR{Collect interval $c \rightarrow C$}
    \STATE \textcolor{softgreen}{// System 1 Training}
    \STATE Sample $B$ Sequences $\{(o_t, a_t, r_t)\}_{t=k}^{k+L} \sim \mathcal{D}$.
    \STATE Compute $h_t = f_\varphi\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$.
    \STATE Predict $\hat{z}_t \sim p_\varphi\left(\hat{z}_t \mid h_t\right), \hat{o}_t \sim p_\varphi\left(\hat{o}_t \mid h_t, z_t\right)$.
    \STATE Update S1 $\psi \gets \psi - \eta_\psi \nabla_\psi \mathcal{L}_{\text{S1}}(\psi)$.
    \STATE \textcolor{softgreen}{// System 2 Training}
    \STATE Learn Logic Regularizations $\mathcal{L}_{\text{reg}}$.
    \STATE \textcolor{lightblue}{// S1's Guidance on S2 Based on Truth $\{(h_t, a_t, h_{t+1})\}$}
    \STATE Compute $L^{\lambda}_{T} = \phi^{\lambda}_1 \land \phi^{\lambda}_2 \land \phi^{\lambda}_3 \cdots \phi^{\lambda}_{T-1}$.
    \STATE Update S2 $w \gets w - \eta_w \nabla_w \mathcal{L}_{\text{S1}}(w)$.
    \STATE \textcolor{softgreen}{// Actor-Critic Training}
    \STATE Imagine $\{(z_\tau, a_\tau)\}_{\tau=t}^{t+H}$ from each $z_t$.
    \STATE Predict rewards $\mathbb{E}(q_\varphi(r_\tau \mid h_\tau, z_\tau))$ and values $v_\psi(s_\tau)$.
    \STATE Compute value estimates $V_\lambda(s_\tau)$.
    \STATE Update $\vartheta \gets \vartheta + \eta_{\vartheta} \nabla_\vartheta \sum_{\tau=t}^{t+H} V_\lambda(s_\tau)$.
    \STATE Update $\psi \gets \psi - \eta_{\psi} \nabla_\psi \sum_{\tau=t}^{t+H} \frac{1}{2} \| v_\psi(s_\tau) - V_\lambda(s_\tau) \|^2$.
    \STATE \textcolor{lightblue}{// S2's Guidance on S1 Based on Imagination $\{(z_\tau, a_\tau)\}$}
    \STATE Compute Logic Consistency of $\{(z_\tau, a_\tau)\}$ With $L^{\lambda}_{T}$
    \STATE Update S1 $\psi \gets \psi - \eta_\psi \nabla_\psi \mathcal{L}_{\text{S2}}(\psi)$.
    \ENDFOR
    \STATE \textcolor{lavender}{// Real Environment Interaction \& Data Collection}
    \STATE Start a environment $env.reset()$
    \FOR{Time step $t \rightarrow T$}
    \STATE Compute $h_t = f_\varphi\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$.
    \STATE Compute $z_t \sim q_\varphi\left(z_t \mid h_t, o_t\right)$
    \STATE Obtain $a_t \sim q_\vartheta(a_t \mid z_t)$ from decision-making model.
    \STATE Interact $r_t, o_{t+1} \gets env.step(a_t)$.
    \ENDFOR
    \STATE Add experience to dataset $\mathcal{D} \gets \mathcal{D} \cup \{(o_t, a_t, r_t)\}_{t=1}^T$.
    \ENDFOR
 \end{algorithmic}
 \end{algorithm}
\newpage

 \subsection{DMWM With MPC}
 The training process of DMWM with Grad-MPC-based decision-making module \cite{sv2023gradient} is shown in Algorithm \ref{lb:a2}.
 \begin{algorithm}[H]
    \caption{DMWM With Grad-MPC}
    \label{lb:a2}
 \begin{algorithmic}
    \STATE Hyper Parameters: iterations $I$, candidate Size $J$, learning rate $\eta_{R}$.
    \STATE Initialize dataset $\mathcal{D}$ with $S$ seed episodes. 
    \STATE Initialize DMWM parameters $\phi, w$.
    \FOR{Training step $n \rightarrow N$}
    \STATE $\cdots$
    \STATE \textcolor{lavender}{// Real Environment Interaction \& Data Collection}
    \STATE Start a environment $env.reset()$
    \FOR{Time step $t \rightarrow T$}
    \STATE Compute $h_t = f_\varphi\left(h_{t-1}, z_{t-1}, a_{t-1}\right)$.
    \STATE Compute $z_t \sim q_\varphi\left(z_t \mid h_t, o_t\right)$
    \STATE \textcolor{softgreen}{// MPC Decision Making}
    \STATE Sample Actions $\left\{a_{t:t+H}^{(j)}\right\}_{j=1}^J \sim \mathcal{N}(\mu_t, \operatorname{diag}(\sigma_t^2))$.
    \FOR{Iteration $i \rightarrow I$}
    \FOR{Candidate sequence $j \rightarrow J$}
    \STATE $s_{t:t+H+1}^{(j)} \sim q_{\phi}(s_t \mid o^{(j)}_{1:t}, a^{(j)}_{1:t-1}) \prod_{\tau=t+1}^{t+H+1} p_{\phi}(s_\tau \mid s^{(j)}_{\tau-1}, a_{\tau-1}^{(j)})$.
    \STATE $R^{(j)} = \sum_{\tau=t+1}^{t+H+1} \mathbb{E} \big[ p(r_\tau \mid s_\tau^{(j)}) \big]$.
    \STATE Update Action $a_{t:t+H}^{(j)} = a_{t:t+H}^{(j)} - \eta_{R} \nabla R^{(j)}.$
    \ENDFOR
    \ENDFOR
    \STATE $a_{t:t+H}^{*} = \max_{a_{t:t+H}^{(j)}} R^{(j)}$
    \STATE Interact $r_t, o_{t+1} \gets env.step(a^*_t)$.
    \ENDFOR
    \STATE Add experience to dataset $\mathcal{D} \gets \mathcal{D} \cup \{(o_t, a_t, r_t)\}_{t=1}^T$.
    \STATE $\cdots$
    \ENDFOR
 \end{algorithmic}
 \end{algorithm}

 \section{Hyperparameters}\label{hyp}
 \subsection{Environment Setting}
 The action repeat setting \cite{hafner2019dream, sv2023gradient} of different environments is presented in TABLE 2.
 \begin{table}[H]
    \label{tb:ar}
    \centering
    \caption{Action Repeat Setting}
    \begin{tabular}{l|c|c}
    \hline
    \textbf{Env} & \textbf{Action Dim} & \textbf{Action Repeat} \\ \hline
    Cartpole Swingup & 1 & $8$ \\
    Pendulum Swingup & 1 & $6$ \\
    Reacher Easy & 2 & $4$ \\
    Finger Spin & 2 & $2$ \\
    Cheetah Run & 6 & $4$ \\
    Cup Catch & 2 & $6$ \\
    Walker Walk & 6 & $2$ \\
    Quadruped Walk & 12 & $2$ \\ 
    Hopper Hop & 4 & $2$ \\ \hline
    \end{tabular} 
\end{table}
\newpage

\subsection{Model Setting}
The hyperparameters of models are presented in TABLE 3.
\begin{table}[H]
    \centering
    \caption{Hyperparameter Setting}
    \begin{tabular}{l|c|c}
    \hline
    \textbf{Parameter} & \textbf{Symbol} & \textbf{Value} \\ \hline
    \multicolumn{3}{l}{\textbf{Dual-Mind World Model (General)}} \\ \hline
    Replay memory size & --- & 1e6 \\ 
    Batch size & $B$ & 50 \\ 
    Sequence length & $L$ & 64 \\ 
    Seed episode & $S$ & 5 \\ 
    Training episodes & $N$ & 1e3 \\
    Collect Interval & $C$ & 100 \\
    Max episode length & --- & 500 \\
    Exploration noise & --- & 0.3 \\
    Imagination horizon & $H$ & 30 \\ 
    Gradient clipping & --- & 100 \\ 
    \hline
    \multicolumn{3}{l}{\textbf{RSSM-S1}} \\ \hline
    Activation function & --- & Relu \\ 
    Embedding size & --- & 1024 \\ 
    Hidden size & --- & 200 \\ 
    Belief size & --- & 200 \\ 
    State size & --- & 30 \\ 
    Overshooting distance & --- & 50 \\
    Overshooting KL-beta & --- & 0 \\
    Global KL-beta & --- & 0 \\
    overshooting reward scale & --- & 0 \\
    Free nats & --- & 3 \\
    Bit-depth & --- & 5 \\ 
    Weights & $\varpi_{_{\text {dyn}}}$, $\varpi_{_{\text {rep}}}$ & 1 \\ 
    Optimizer & --- & Adam \\ 
    Adam epsilon & --- & 1e-4 \\ 
    Learning rate & $\eta_\psi$ & 1e-3 \\
    \hline
    \multicolumn{3}{l}{\textbf{LINN-S2}} \\ \hline
    Reasoning depth & $\alpha$ & 30 \\ 
    Logic vector size & $|v|$, $|m|$ & 64 \\
    L2 weight & $\beta_{\ell_2}$ & 1e-5 \\ 
    Regularization weight & $\beta_{\text{reg}}$ & 1 \\
    Logic MLP number & --- & 3 \\
    Optimizer & --- & SGD \\ 
    Learning rate & $\eta_w$ & 1e-2 \\
    \hline
    \multicolumn{3}{l}{\textbf{Actor-Critic} \cite{hafner2019dream}} \\ \hline
    Return lambda & $\lambda$ & 0.95 \\ 
    Planning horizon discount & --- & 0.99 \\ 
    Optimizer & --- & Adam \\ 
    Adam epsilon & --- & 1e-4 \\ 
    Learning rate & $\eta_{\vartheta}$, $\eta_{\psi}$ & 1e-4 \\ 
    \hline
    \multicolumn{3}{l}{\textbf{Grad-MPC} \cite{sv2023gradient}} \\ \hline
    Iterations & $I$ & 40 \\ 
    Candidate Size & $J$ & 1000 \\ 
    Learning Rate & $\eta_{R}$ & 0.1-0.01-0.005-0.0001\\ \hline
    \end{tabular} 
\end{table}
\newpage

\section{Further Related Work}
\subsection{World Model}
World models are fundamental building blocks for model-based intelligent systems to make decisions, learn, and reason in complex environments. It enables prediction, efficient behavior planning through environment simulation, and strategy optimization using virtual simulations, reducing reliance on real-world trial and error \cite{hafner2019dream,hafner2020mastering,hafner2023mastering,micheli2024efficient,zhou2024robodreamer}.
Existing world modeling frameworks can be categorized as Recurrent State-Space Model (RSSM) \cite{hafner2019dream,hafner2020mastering,zhu2020bridging,hansen2022temporal,hafner2023mastering,wu2023daydreamer,lin2023learning,hansen2023td, feng2023finetuning, mattes2023hieros,sun2024learning,georgiev2024pwm}, 
Generative Mdoel \cite{chen2022transdreamer,wang2023drivedreamer,hu2023gaia,zhang2023learning,rigter2023world,alonso2024diffusion,ding2024diffusion,bruce2024genie,du2024learning,zhen20243d}, 
and Large Language Model (LLM) \cite{nottingham2023embodied,wong2023word,hao2023reasoning,wu2024ivideogpt,xiang2024language,xiang2024pandora}.

The Dreamer series \cite{hafner2019dream,hafner2020mastering,hafner2023mastering} lays an important foundation for general model-based reinforcement learning (MBRL) by continuously improving sample efficiency and task generalization ability through dynamic modeling and planning in the latent space. To adapt the RSSM-based world model for real-world environments, \cite{zhu2020bridging} optimized confidence and entropy regularization for the gradient to mitigate discrepancies between virtual simulations and real-world environments. \cite{lin2020improving} studied an object-centric world model, and introduced an object-state recurrent neural network (OS-RNN) to follow the object states. \cite{du2024learning} extended multimodal RSSM to support joint text and visual inputs.  \cite{hafner2023mastering,hansen2023td,georgiev2024pwm} explored the multiple tasks with RSSM-based world models. \cite{hansen2023td} utilized SimNorm to sparse and normalize the potential states, which projected the latent representations to simplices of fixed dimensions, thus mitigating the gradient explosion and improving the training stability. \cite{sun2024learning} introduced masking-based latent reconstruction with a dual branch structure to handle the exogenous noise in the complex environment.
\cite{mattes2023hieros} introduced the hierarchical imagination with parallel processing and proposed efficient time-balanced sampling.

The diffusion model-based world model offers high-quality and diverse generations with temporal smoothness and consistency, achieved through denoising and time inversion processes \cite{chen2022transdreamer,wang2023drivedreamer,hu2023gaia,zhang2023learning,rigter2023world,alonso2024diffusion,du2024learning}. However, its reliance on multi-step denoising significantly slows downsampling, inference, and computation compared to RSSM, making it less suitable for real-time tasks and challenging to maintain long-term consistency \cite{hu2023gaia,zhang2023learning,rigter2023world,du2024learning}. Additionally, unlike RSSM's latent space modeling, diffusion models cannot extract task-relevant features and accurately capture environment dynamics, particularly in complex environments with long-tailed distributions \cite{chen2022transdreamer,hu2023gaia,rigter2023world,ding2024diffusion,du2024learning}. The LLM-based world model serves as a powerful tool for complex task planning and execution through its logical reasoning capability and dynamic controllability \cite{nottingham2023embodied,wong2023word,hao2023reasoning}, and is able to realize task decomposition and cross-domain knowledge transfer through natural language instructions \cite{xiang2024pandora}. However, the model cannot perform in dynamic modeling, and the generated states and actions often cannot accurately reflect the dynamic changes in the environment. In addition, its high reliance on linguistic representation may lead to insufficient quality of modal alignment, posing the risk of information loss or misinterpretation, while the significant consumption of computational resources during training and inference limits the application of LLM-based world models in resource-constrained environments \cite{nottingham2023embodied,wong2023word,hao2023reasoning,wu2024ivideogpt}.

Unlike the aforementioned work, this work focuses on the long-term imagination of world models and proposes to enable the logical consistency of state representations and predictions over an extended horizon for the first time. We retain the efficient sampling and representation capabilities of RSSM as System 1 and integrate a logic-integrated neural network (LINN) as System 2 to imbue the world model with logic reasoning capabilities. The proposed DMWM thus delivers reliable and efficient long-term imagination with logical consistency, addressing a critical research gap in existing approaches.

\subsection{Logic Neural Reasoning}
Logic neural reasoning enhances the logical consistency, long-term imagination, and generalization ability of world models by embedding logical rules and reasoning capabilities \cite{ferreira2022looking}. By combining the interpretability of symbolic logic with the expressive power of neural networks, it provides robust and rapid reasoning for complex task planning \cite{li2019augmenting, qu2019probabilistic, shi2020neural,riegel2020logical,chen2021neural,badreddine2022logic,sen2022neuro}.
However, explicitly modeling the logical structure of the world presents significant challenges due to the inherent ambiguity of logical rules, the prevalence of noisy data, and the dynamic and complex nature of logical interactions. \cite{li2019augmenting} converted first-order logic rules into computational graphs for neural networks, enhancing their learning capabilities in low-data environments while providing limited support for complex temporal logic. \cite{qu2019probabilistic} integrated Markov logic networks with knowledge graph embeddings to address uncertainty in logical reasoning. The logical neural network (LNN) \cite{riegel2020logical} mapped neurons to logical formulas, enabling each neuron to function as a logical operator. However, LNNs rely on predefined and static logical rules, facing limitations in generalization when confronted with uncertainty or evolving logical variables. \cite{sen2022neuro} introduced inductive logic programming and automatically learned logic rules. However, the aforementioned approaches make it hard to capture the environment dynamics and extend the long-term imagination capability of the world model. In contrast, the LINN \cite{shi2020neural,chen2021neural} introduces a paradigm shift by dynamically constructing computational graphs and employing neural modules to learn logical operations. Thus, LINN can automatically infer implicit logical rules, circumventing the need for explicit specification. By integrating neural flexibility with logical rigor \cite{shi2020neural,chen2021neural}, LINN achieves superior generalization capacity and enhanced robustness to noise, making it well-suited for reasoning tasks in dynamic and complex environments. 

This work enhances the long-term imagination capabilities of world models through logical reasoning, a critical yet unexplored area in world model research, with significant implications for advancing Artificial General Intelligence (AGI).


\section{Derivation}\label{elbo}
The ELBO with logic rules for the observation loss can be derived by
\begin{equation}
\begin{split}
\ln p_\varphi\left(o_{1: T} \mid a_{1: T}\right) 
& = \int p_\varphi(o_{1:T}, z_{1:T} \mid a_{1:T}) \, dz_{1:T} \\
& = \mathbb{E}_{q_\varphi(z_{1:T} \mid o_{1:T}, a_{1:T})
} \left[ \frac{p_\varphi(o_{1:T}, z_{1:T} \mid a_{1:T})}{q_\varphi(z_{1:T} \mid o_{1:T}, a_{1:T})} \right]\\
& = \ln \mathbb{E}_{q_\varphi(z_{1:T} \mid o_{1:T}, a_{1:T})} \left[
\prod_{t=1}^T \frac{p_\varphi(o_t \mid z_t)p_\varphi(z_t \mid z_{t-1}, a_{t-1})\mathcal{C}(\phi(z_t, z_{t-1}, a_{t-1}))}{q_\varphi(z_t \mid o_{\leq t}, a_{\leq t})}\right]\\
& \ge \mathbb{E}_{q_\varphi(z_{1:T} \mid o_{1:T}, a_{1:T})} \left[
\sum_{t=1}^T \ln p_\varphi(o_t \mid z_t) + \ln p_\varphi(z_t \mid z_{t-1}, a_{t-1}) \right. \\
& \left. \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad + \ln \mathcal{C}(\phi(z_t, z_{t-1}, a_{t-1})- \ln q_\varphi(z_t \mid o_{\leq t}, a_{\leq t}))
\right] \\
& = \sum_{t=1}^T\left( \underbrace{\mathbb{E}_{q_\varphi\left(z_t \mid o_{\leq t}, a_{<t}\right)}\left[\ln p_\varphi\left(o_t \mid z_t\right)\right]}_{\mathrm{Decoding}} + \underbrace{\mathbb{E}_{q_\varphi\left(z_t \mid o_{\leq t}, a_{<t}\right)}\left[\ln \mathcal{C}(\phi(z_t, z_{t-1}, a_{t-1}))\right]}_{\mathrm{Logic \ Inference}} \right.\\
& \left. \quad \quad \quad \quad -\underbrace{\mathbb{E}_{q_\varphi\left(z_{t-1} \mid o_{\leq t-1}, a_{<t-1}\right)}\left[\operatorname{KL}\left[q_\varphi\left(z_t \mid o_{\leq t}, a_{<t}\right) \| p_\varphi\left(z_t \mid z_{t-1}, a_{t-1}\right)\right]\right]}_{\mathrm{Prediction}}\right)\\
\end{split}
\end{equation}
\newpage
 
\section{Complete Logic Regularization and Rules Table}\label{lgr}
\vspace{-0.1cm}
The complete logic rules and the corresponding regularizers are given in TABLE \ref{tb:2} \cite{shi2020neural}.
\begin{table}[H]
    \centering
    \vspace{-0.6cm}
    \caption{Complete Logical Regularizers and Rules for System 2}
    \label{tb:2}
    \begin{tabular}{l l l l l}
    \hline
    \hline
    &\textbf{Logical Rule} & \textbf{Equation}  & \textbf{Logic Regularizer $r_i$} \\
    \hline
    \multirow{2}*{\textbf{NOT}}&\textbf{Negation} & $w \rightarrow \mathbf{T} = \mathbf{T}$  & $r_1 = \sum_{w \in W \cup \{\mathbf{T}\}} \text{Sim}(\text{NOT}(w), w)$\\
    &\textbf{Double Negation} & $\neg (\neg w) = w$ & $r_2 = \sum_{w \in W} 1 - \text{Sim}(\text{NOT}(\text{NOT}(w)), w)$\\
    \hline
    \multirow{4}*{\textbf{AND}}&\textbf{Identity} & $w \land \mathbf{T} = w$ & $r_3 = \sum_{w \in W} 1 - \text{Sim}(\text{AND}(w, \mathbf{T}), w)$\\
    &\textbf{Annihilator} & $w \land \mathbf{T} = w$ & $r_4 = \sum_{w \in W} 1 - \text{Sim}(\text{AND}(w, \mathbf{F}), \mathbf{F})$\\
    &\textbf{Idempotence} & $w \land \mathbf{F} = \mathbf{F}$ & $r_5 = \sum_{w \in W} 1 - \text{Sim}(\text{AND}(w, w), w)$ \\
    &\textbf{Complementation} & $w \land w = w$ & $r_6 = \sum_{w \in W} 1 - \text{Sim}(\text{AND}(w, \text{NOT}(w)), \mathbf{F})$\\
    \hline
    \multirow{4}*{\textbf{OR}}&\textbf{Identity} & $w \lor \mathbf{F} = w$ &$r_7 = \sum_{w \in W} 1 - \text{Sim}(\text{OR}(w, \mathbf{F}), w)$\\
    &\textbf{Annihilator} & $w \lor \mathbf{T} = \mathbf{T}$ & $r_8 = \sum_{w \in W} 1 - \text{Sim}(\text{OR}(w, \mathbf{T}), \mathbf{T})$\\
    &\textbf{Idempotence} & $w \lor w = w$ & $r_9 = \sum_{w \in W} 1 - \text{Sim}(\text{OR}(w, w), w)$ \\
    &\textbf{Complementation} & $w \lor \neg w = \mathbf{T}$ & $r_{10} = \sum_{w \in W} 1 - \text{Sim}(\text{OR}(w, \text{NOT}(w)), \mathbf{T})$\\
    \hline
    \multirow{5}*{\textbf{IMPLY}}&\textbf{Identity} & $w \rightarrow \mathbf{T} = \mathbf{T}$  & $r_{11} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w),\mathbf{T}), \mathbf{T})$ \\
    &\textbf{Annihilator} & $w \rightarrow \mathbf{F} = \neg w$  & $r_{12} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), \mathbf{F}), \mathrm{NOT}(w))$ \\
    &\textbf{Idempotence} & $w \rightarrow w = \mathbf{T}$  & $r_{13} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), w), \mathbf{T})$ \\
    &\textbf{Complementation} & $w \rightarrow \neg w \equiv \neg w$ & $r_{14} = \sum_{w \in W} 1 - \mathrm{Sim}(\mathrm{OR}(\mathrm{NOT}(w), \mathrm{NOT}(w)), \mathrm{NOT}(w))$ \\
    \hline
    \hline
    \end{tabular}
    \vspace{-0.5cm}
\end{table}

\section{Additional Experiments}\label{experiment}
\vspace{-0.2cm}
\subsection{Logical Consistency}\label{lct}
\vspace{-0.2cm}
\begin{table}[!htbp]
    \label{tb:loc2}
    \centering
    \vspace{-0.2cm}
    \caption{Logical consistency comparison between our proposed DMWM and various RSSM baselines across 20 DMC tasks. We report the mean and variance of logical consistency from imagination over 100 test episodes with the horizon size of $H=10, 30, 50, 100$.}
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    Env & Horizon Size & Dreamer & Hieros & HRSSM & DMWM (\textbf{Ours}) \\ \hline
    \multirow{4}{*}{Acrobot Swingup} 
    & 10  & $0.713 \pm 0.031$ & $0.730 \pm 0.012$ & $0.722 \pm 0.017$ & $0.733 \pm 0.007$ \\
    & 30  & $0.667 \pm 0.063$ & $0.704 \pm 0.032$ & $0.712 \pm 0.044$ & $\boldsymbol{0.731 \pm 0.017}$ \\
    & 50  & $0.568 \pm 0.129$ & $0.692 \pm 0.058$ & $0.687 \pm 0.083$ & $0.715 \pm 0.032$ \\
    & 100  & $0.485 \pm 0.167$ & $0.672 \pm 0.112$ & $0.651 \pm 0.132$ & $0.699 \pm 0.078$ \\
    \hline
    \multirow{4}{*}{Cartpole Balance} 
    & 10  & $0.721 \pm 0.023$ & $0.730 \pm 0.009$ & $0.729 \pm 0.011$ & $0.730 \pm 0.008$ \\
    & 30  & $0.683 \pm 0.057$ & $0.711 \pm 0.032$ & $0.713 \pm 0.041$ & $\boldsymbol{0.727 \pm 0.023}$ \\
    & 50  & $0.574 \pm 0.112$ & $0.687 \pm 0.084$ & $0.695 \pm 0.072$ & $0.717 \pm 0.045$ \\
    & 100  & $0.491 \pm 0.171$ & $0.663 \pm 0.142$ & $0.655 \pm 0.121$ & $0.701 \pm 0.092$ \\
    \hline
    \multirow{4}{*}{Cartpole Balance Sparse} 
    & 10  & $0.705 \pm 0.132$ & $0.719 \pm 0.032$ & $0.722 \pm 0.034$ & $0.722 \pm 0.026$ \\
    & 30  & $0.602 \pm 0.167$ & $0.682 \pm 0.101$ & $0.693 \pm \boldsymbol{0.081}$ & $0.695 \pm 0.093$ \\
    & 50  & $0.432 \pm 0.203$ & $0.632 \pm 0.178$ & $0.643 \pm 0.162$ & $0.652 \pm 0.135$ \\
    & 100  & $0.321 \pm 0.286$ & $0.571 \pm 0.232$ & $0.589 \pm 0.213$ & $0.603 \pm 0.182$ \\
    \hline
    \multirow{4}{*}{Cartpole Swingup} 
    & 10  & $0.719 \pm 0.031$ & $0.729 \pm 0.011$ & $0.723 \pm 0.023$ & $0.730 \pm 0.011$ \\
    & 30  & $0.678 \pm 0.062$ & $0.698 \pm 0.043$ & $0.703 \pm 0.082$ & $0.723 \pm 0.032$ \\
    & 50  & $0.563 \pm 0.091$ & $0.672 \pm 0.124$ & $0.662 \pm 0.145$ & $0.702 \pm 0.078$ \\
    & 100  & $0.474 \pm 0.158$ & $0.621 \pm 0.191$ & $0.602 \pm 0.191$ & $0.672 \pm 0.152$ \\
    \hline
    \multirow{4}{*}{Cartpole Swingup Sparse} 
    & 10  & $0.703 \pm 0.142$ & $0.717 \pm 0.036$ & $0.715 \pm 0.052$ & $0.720 \pm 0.030$ \\
    & 30  & $0.613 \pm 0.187$ & $0.669 \pm 0.121$ & $0.671 \pm 0.098$ & $0.699 \pm 0.087$ \\
    & 50  & $0.443 \pm 0.213$ & $0.621 \pm 0.185$ & $0.621 \pm 0.173$ & $0.669 \pm 0.121$ \\
    & 100  & $0.403 \pm 0.252$ & $0.532 \pm 0.241$ & $0.559 \pm 0.231$ & $0.627 \pm 0.162$ \\
    \hline
    \multirow{4}{*}{Cheetah Run} 
    & 10  & $0.709 \pm 0.085$ & $0.723 \pm 0.031$ & $0.721 \pm 0.023$ & $0.730 \pm 0.016$ \\
    & 30  & $0.643 \pm 0.131$ & $0.689 \pm 0.113$ & $0.695 \pm 0.087$ & $0.725 \pm 0.049 $ \\
    & 50  & $0.527 \pm 0.165$ & $0.651 \pm 0.147$ & $0.667 \pm 0.131$ & $0.703 \pm 0.092$ \\
    & 100  & $0.428 \pm 0.221$ & $0.606 \pm 0.186$ & $0.627 \pm 0.183$ & $0.676 \pm 0.142$ \\
    \hline
    \multirow{4}{*}{Cup Catch} 
    & 10  & $0.711 \pm 0.049$ & $0.728 \pm 0.010$ & $0.726 \pm 0.014$ & $0.732 \pm 0.008$ \\
    & 30  & $0.652 \pm 0.087$ & $0.701 \pm 0.072$ & $0.714 \pm 0.061$ & $\boldsymbol{0.728 \pm 0.021}$ \\
    & 50  & $0.534 \pm 0.113$ & $0.681 \pm 0.098$ & $0.683 \pm 0.112$ & $0.712 \pm 0.053$ \\
    & 100  & $0.465 \pm 0.181$ & $0.647 \pm 0.182$ & $0.633 \pm 0.172$ & $0.692 \pm 0.112$ \\
    \hline
\end{tabular} 
\end{table}

\begin{table}[!htbp]
    \centering
    \begin{tabular}{|p{3.5cm}|c|c|c|c|c|}
    \hline
    \multirow{4}{*}{Finger Spin} 
    & \makebox[2cm]{10}  & $0.712 \pm 0.075$ & $0.727 \pm 0.011$ & $0.728 \pm 0.017$ & $0.733 \pm 0.009$ \\
    & 30  & $0.647 \pm 0.102$ & $0.705 \pm 0.052$ & $0.712 \pm 0.057$ & $\boldsymbol{0.729 \pm 0.017}$ \\
    & 50  & $0.517 \pm 0.131$ & $0.687 \pm 0.091$ & $0.679 \pm 0.136$ & $0.710 \pm 0.062$ \\
    & 100  & $0.435 \pm 0.211$ & $0.636 \pm 0.165$ & $0.645 \pm 0.185$ & $0.684 \pm 0.127$ \\
    \hline
    \multirow{4}{*}{Finger Turn Easy} 
    & 10  & $0.708 \pm 0.098$ & $0.726 \pm 0.013$ & $0.724 \pm 0.018$ & $0.732 \pm 0.015$ \\
    & 30  & $0.634 \pm 0.113$ & $0.702 \pm 0.051$ & $0.705 \pm 0.067$ & $0.728 \pm 0.023 $ \\
    & 50  & $0.512 \pm 0.157$ & $0.681 \pm 0.117$ & $0.682 \pm 0.132$ & $0.712 \pm 0.067$ \\
    & 100  & $0.412 \pm 0.191$ & $0.644 \pm 0.194$ & $0.617 \pm 0.197$ & $0.683 \pm 0.148$ \\
    \hline
    \multirow{4}{*}{Finger Turn Hard} 
    & 10  & $0.704 \pm 0.118$ & $0.723 \pm 0.014$ & $0.724 \pm 0.021$ & $0.731 \pm 0.011$ \\
    & 30  & $0.627 \pm 0.131$ & $0.698 \pm 0.061$ & $0.703 \pm 0.073$ & $0.725 \pm 0.029 $ \\
    & 50  & $0.487 \pm 0.162$ & $0.673 \pm 0.112$ & $0.673 \pm 0.152$ & $0.705 \pm 0.073$ \\
    & 100  & $0.385 \pm 0.231$ & $0.623 \pm 0.201$ & $0.601 \pm 0.205$ & $0.675 \pm 0.142$ \\
    \hline
    \multirow{4}{*}{Hopper Hop} 
    & 10  & $0.703 \pm 0.092$ & $0.729 \pm 0.023$ & $0.726 \pm 0.023$ & $0.731 \pm 0.017$ \\
    & 30  & $0.633 \pm 0.127$ & $0.704 \pm 0.087$ & $0.701 \pm 0.092$ & $0.722 \pm 0.038 $ \\
    & 50  & $0.506 \pm 0.191$ & $0.664 \pm 0.132$ & $0.673 \pm 0.132$ & $0.698 \pm 0.092$ \\
    & 100  & $0.407 \pm 0.237$ & $0.612 \pm 0.237$ & $0.623 \pm 0.201$ & $0.689 \pm 0.143$ \\
    \hline
    \multirow{4}{*}{Hopper Stand} 
    & 10  & $0.709 \pm 0.056$ & $0.728 \pm 0.021$ & $0.725 \pm 0.019$ & $0.732 \pm 0.013$ \\
    & 30  & $0.645 \pm 0.112$ & $0.699 \pm 0.057$ & $0.704 \pm 0.081$ & $0.724 \pm 0.039 $ \\
    & 50  & $0.523 \pm 0.168$ & $0.671 \pm 0.121$ & $0.689 \pm 0.137$ & $0.703 \pm 0.080$ \\
    & 100  & $0.421 \pm 0.197$ & $0.632 \pm 0.211$ & $0.642 \pm 0.189$ & $0.692 \pm 0.139$ \\
    \hline
    \multirow{4}{*}{Pendulum Swingup} 
    & 10  & $0.715 \pm 0.054$ & $0.728 \pm 0.017$ & $0.719 \pm 0.023$ & $0.732 \pm 0.015$ \\
    & 30  & $0.611 \pm 0.137$ & $0.709 \pm 0.054$ & $0.699 \pm 0.079$ & $\boldsymbol{0.730 \pm 0.037}$ \\
    & 50  & $0.526 \pm 0.161$ & $0.684 \pm 0.112$ & $0.664 \pm 0.146$ & $0.721 \pm 0.088$ \\
    & 100  & $0.359 \pm 0.224$ & $0.641 \pm 0.198$ & $0.612 \pm 0.204$ & $0.686 \pm 0.131$ \\
    \hline
    \multirow{4}{*}{Quadruped Run} 
    & 10  & $0.718 \pm 0.042$ & $0.727 \pm 0.023$ & $0.725 \pm 0.019$ & $0.731 \pm 0.013$ \\
    & 30  & $0.642 \pm 0.107$ & $0.701 \pm 0.072$ & $0.702 \pm 0.074$ & $0.726 \pm 0.042 $ \\
    & 50  & $0.556 \pm 0.143$ & $0.679 \pm 0.135$ & $0.683 \pm 0.137$ & $0.705 \pm 0.078$ \\
    & 100  & $0.398 \pm 0.217$ & $0.644 \pm 0.193$ & $0.629 \pm 0.184$ & $0.682 \pm 0.129$ \\
    \hline
    \multirow{4}{*}{Quadruped Walk} 
    & 10  & $0.719 \pm 0.045$ & $0.728 \pm 0.027$ & $0.724 \pm 0.017$ & $0.732 \pm 0.011$ \\
    & 30  & $0.656 \pm 0.092$ & $0.701 \pm 0.067$ & $0.703 \pm 0.072$ & $0.723 \pm 0.039 $ \\
    & 50  & $0.534 \pm 0.129$ & $0.682 \pm 0.114$ & $0.685 \pm 0.132$ & $0.701 \pm 0.082$ \\
    & 100  & $0.413 \pm 0.212$ & $0.654 \pm 0.183$ & $0.634 \pm 0.179$ & $0.687 \pm 0.137$ \\
    \hline
    \multirow{4}{*}{Reacher Easy} 
    & 10  & $0.711 \pm 0.065$ & $0.729 \pm 0.012$ & $0.724 \pm 0.013$ & $0.732 \pm 0.008$ \\
    & 30  & $0.634 \pm 0.102$ & $0.706 \pm 0.061$ & $0.705 \pm 0.067$ & $\boldsymbol{0.731 \pm 0.031} $ \\
    & 50  & $0.464 \pm 0.167$ & $0.675 \pm 0.104$ & $0.678 \pm 0.121$ & $0.720 \pm 0.075$ \\
    & 100  & $0.373 \pm 0.221$ & $0.631 \pm 0.175$ & $0.614 \pm 0.163$ & $0.705 \pm 0.125$ \\
    \hline
    \multirow{4}{*}{Reacher Hard} 
    & 10  & $0.712 \pm 0.072$ & $0.729 \pm 0.013$ & $0.724 \pm 0.014$ & $0.732 \pm 0.010$ \\
    & 30  & $0.608 \pm 0.121$ & $0.702 \pm 0.062$ & $0.703 \pm 0.072$ & $\boldsymbol{0.730 \pm 0.042 }$ \\
    & 50  & $0.501 \pm 0.189$ & $0.663 \pm 0.114$ & $0.674 \pm 0.132$ & $0.717 \pm 0.082$ \\
    & 100  & $0.349 \pm 0.233$ & $0.602 \pm 0.183$ & $0.607 \pm 0.172$ & $0.701 \pm 0.165$ \\
    \hline
    \multirow{4}{*}{Walker Run} 
    & 10  & $0.702 \pm 0.137$ & $0.720 \pm 0.023$ & $0.713 \pm 0.027$ & $0.729 \pm 0.013$ \\
    & 30  & $0.554 \pm 0.158$ & $0.676 \pm 0.067$ & $0.682 \pm 0.092$ & $0.711 \pm 0.072 $ \\
    & 50  & $0.424 \pm 0.213$ & $0.613 \pm 0.132$ & $0.658 \pm 0.188$ & $0.672 \pm 0.128$ \\
    & 100  & $0.323 \pm 0.261$ & $0.532 \pm 0.191$ & $0.572 \pm 0.243$ & $0.645 \pm 0.175$ \\
    \hline
    \multirow{4}{*}{Walker Stand} 
    & 10  & $0.706 \pm 0.087$ & $0.728 \pm 0.012$ & $0.721 \pm 0.012$ & $0.734 \pm 0.008$ \\
    & 30  & $0.598 \pm 0.128$ & $0.690 \pm 0.051$ & $0.692 \pm 0.113$ & $0.722 \pm 0.067 $ \\
    & 50  & $0.494 \pm 0.173$ & $0.648 \pm 0.108$ & $0.658 \pm 0.162$ & $0.701 \pm 0.138$ \\
    & 100  & $0.369 \pm 0.241$ & $0.568 \pm 0.172$ & $0.572 \pm 0.221$ & $0.662 \pm 0.185$ \\
    \hline
    \multirow{4}{*}{Walker Walk} 
    & 10  & $0.701 \pm 0.118$ & $0.727 \pm \boldsymbol{0.015}$ & $0.723 \pm \boldsymbol{0.022}$ & $0.731 \pm 0.028$ \\
    & 30  & $0.612 \pm 0.140$ & $0.696 \pm 0.063$ & $0.701 \pm 0.073$ & $\boldsymbol{0.730 \pm 0.034 }$ \\
    & 50  & $0.474 \pm 0.181$ & $0.664 \pm 0.123$ & $0.667 \pm 0.112$ & $0.718 \pm 0.079$ \\
    & 100  & $0.371 \pm 0.251$ & $0.598 \pm 0.194$ & $0.601 \pm 0.171$ & $0.675 \pm 0.142$ \\
    \hline
    \end{tabular} 
\end{table}
\clearpage

\subsection{Data Efficiency}\label{eps}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Fig/ES.png}
    \vspace{-0.3cm}
    \caption{Performance comparison of test results on 20 DMC tasks under limited environment interactions, where the standard error is shaded in the distraction setting. The horizontal axis indicates the number of times that the models explore the environments. The vertical axis represents the average test return over 100 test episodes.}
\end{figure}
\clearpage

\subsection{Trial Efficiency}\label{es}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Fig/EPS.png}
    \vspace{-0.2cm}
    \caption{Performance comparison of test results on 20 DMC tasks under limited environment steps, where the standard error is shaded in the distraction setting. The horizontal axis indicates the number of environment data that is used to train the models. The vertical axis represents the average test return over 100 test episodes.}
\end{figure}

\subsection{Long-term Imaginations Over Extended Horizon Size}\label{hs}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{Fig/HS.png}
    \vspace{-0.2cm}
    \caption{Performance comparison of test results on 20 DMC tasks over different horizon size of imagination. The horizontal axis indicates the horizon size of each imagination. The vertical axis represents the average test return over 100 test episodes.}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
