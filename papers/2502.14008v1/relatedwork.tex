\section{Related Work}
\paragraph{Importance metric-based Methods}SparseGPT \cite{frantar2023sparsegpt} evaluates the importance of weights using second-order Hessian information and compensates for other weights during the pruning process, thereby achieving unstructured pruning. Wanda \cite{sun2023wanda} simplifies this approach by relying solely on the magnitude of the weights and the activation values on a calibration set to determine the importance of the weight, accelerating the pruning process. Additionally, its methods can be extended to semi-structured pruning. Pruner-Zero \cite{dong2024pruner} employs genetic programming to efficiently search for optimal symbolic pruning metrics, avoiding heuristic weight importance searches. LLM-Pruner \cite{ma2023llm} was the first to utilize structured pruning methods to compress large language models (LLMs), assessing the importance of weight groups through approximate first-order Hessian information. Bonsai \cite{dery2024everybody} samples the correlation between sub-modules and model performance, using linear regression to determine the importance of weight groups. LoRAPrune \cite{zhang2023loraprune} estimates the original gradients of weights through the gradients of LoRA matrices, thereby reducing memory consumption during backpropagation.

\paragraph{Optimization-based Methods}However, metric-based methods like LLM-Pruner \cite{ma2023llm} often fail to fully capture the importance of weights, leading to suboptimal generalization performance. To address this, many optimization-based methods have focused on learning masks for pruned weights. $L_0$ regularization \cite{louizos2018learning} offers a general paradigm for mask learning, enabling the optimization of non-differentiable masks. CoFi \cite{xia2022structured} integrates a hierarchical distillation loss into the training loss function, while SheardLlama \cite{xia2023sheared} specifies target structures to achieve a unified model architecture and employs dynamic batch loading to enhance generalization performance. Compresso \cite{guo2023compresso} introduces specific prompts during training and incorporates LoRA modules into the optimization process, combining fine-tuning with mask training. NutePrune \cite{li2024nuteprune} leverages progressive distillation to enhance the transfer of knowledge from teacher to student models.