\section{Related Work}
\paragraph{Importance metric-based Methods}SparseGPT ____ evaluates the importance of weights using second-order Hessian information and compensates for other weights during the pruning process, thereby achieving unstructured pruning. Wanda ____ simplifies this approach by relying solely on the magnitude of the weights and the activation values on a calibration set to determine the importance of the weight, accelerating the pruning process. Additionally, its methods can be extended to semi-structured pruning. Pruner-Zero ____ employs genetic programming to efficiently search for optimal symbolic pruning metrics, avoiding heuristic weight importance searches. LLM-Pruner ____ was the first to utilize structured pruning methods to compress large language models (LLMs), assessing the importance of weight groups through approximate first-order Hessian information. Bonsai ____ samples the correlation between sub-modules and model performance, using linear regression to determine the importance of weight groups. LoRAPrune ____ estimates the original gradients of weights through the gradients of LoRA matrices, thereby reducing memory consumption during backpropagation.

\paragraph{Optimization-based Methods}However, metric-based methods like LLM-Pruner ____ often fail to fully capture the importance of weights, leading to suboptimal generalization performance. To address this, many optimization-based methods have focused on learning masks for pruned weights. $L_0$ regularization ____ offers a general paradigm for mask learning, enabling the optimization of non-differentiable masks. CoFi ____ integrates a hierarchical distillation loss into the training loss function, while SheardLlama ____ specifies target structures to achieve a unified model architecture and employs dynamic batch loading to enhance generalization performance. Compresso ____ introduces specific prompts during training and incorporates LoRA modules into the optimization process, combining fine-tuning with mask training. NutePrune ____ leverages progressive distillation to enhance the transfer of knowledge from teacher to student models.