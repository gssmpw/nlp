% \renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[1]{Equal Contribution.}
% \renewcommand{\thefootnote}{\arabic{footnote}}


\section{Introduction}

Large Language Models (LLMs), such as OpenAI's GPT series \citep{achiam2023gpt} and Meta's LLaMA \citep{touvron2023llama, touvron2023llama2}, have made substantial advancements in the domain of Natural Language Processing (NLP). These models exhibit robust capabilities in language understanding and generation, facilitated by extensive pre-training and fine-tuning. However, as the size of these models continues to expand, their computational and storage demands increase sharply, presenting significant challenges for practical applications. Model compression, a vital approach to reducing memory footprint and computational load during model deployment, offers unique benefits across various domains. Techniques such as pruning \citep{frantar2023sparsegpt, ma2023llm, sun2023wanda}, quantization \citep{frantar2023gptq, xiao2023smoothquant, lin2024awq}, knowledge distillation \citep{gu2024minillm, agarwal2023gkd}, and low-rank factorization \citep{yuan2023asvd, wang2024svd} can significantly decrease the number of model parameters and computational complexity, thereby enabling large-scale language models to function efficiently in resource-constrained environments.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{maskPrune1.png}
    \caption{Compresso/NutePrune results in heterogeneous inter-layer structures, whereas MaskPrune achieves uniform inter-layer structures, which is friendly to inference deployment and continual training.}
    \label{fig:MaskPrune1}
\end{figure}

The pruning technique reduces the size and computational complexity of the models by eliminating redundant parameters, which can generally be categorized into unstructured pruning \citep{frantar2023sparsegpt, sun2023wanda, dong2024pruner}, semi-structured pruning \citep{mishra2021accelerating}, and structured pruning \citep{ma2023llm, xia2023sheared, an2023fluctuationbased}. Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning. Currently, most pruning techniques employ metric-based methods, which determine the modules to be pruned by introducing specific pruning metrics. These metrics are usually designed heuristically and often perform poorly at high pruning rates. Moreover, a single metric cannot fully capture the importance of model weights, making it difficult to identify superior local optimal solutions. In contrast, optimization-based pruning methods determine which weights to prune by learning a pruning mask, thereby avoiding the performance degradation associated with manually designed metrics. This paper primarily focuses on optimization-based pruning methods.

Given the large scale of Large Language Models (LLMs), existing optimization-based pruning methods employ structured pruning, wherein a single mask prunes entire modules of the model. Methods such as CoFi \citep{xia2022structured}, Compresso \citep{guo2023compresso} and NutePrune \citep{li2024nuteprune} follow the $L_0$ regularization \citep{louizos2018learning} training paradigm during the training of pruning masks, learning masks by setting a total sparsity without additional constraints. This approach results in a lack of uniformity between layers during training, causing each layer to have a different number of attention heads and FFN intermediate dimensions, as illustrated in Figure \ref{fig:MaskPrune1}, which leads to suboptimal inference speed. Moreover, this irregular structure necessitates adaptations during model deployment. Additionally, to achieve higher performance post-compression, existing model compression techniques typically involve continued training and fine-tuning after compression. However, the irregular structure hinders models from fully utilizing existing model parallelism techniques, resulting in diminished performance for the same continued training cost\citep{xia2023sheared}.

To address these issues, this paper proposes a method called MaskPrune for jointly training pruning masks and target structures across various dimensions. This approach optimizes the target dimension parameters simultaneously during training to maintain uniformity of dimensions across the layers of the pruned model while achieving the preset model sparsity. The key idea is to frame the sparsity constraint of model pruning as a minimax problem. Since the introduced sparsity loss is non-differentiable, it cannot be directly optimized using gradient descent. By employing proximal operators and straight-through estimators to optimize masks and target dimensions respectively, the pruning optimization problem is effectively solved. The contributions of this paper can be summarized as follows:



\begin{itemize} 
    \item We propose a mask training method based on minimax optimization, enabling end-to-end optimization of mask values during the pruning and automatically maintaining the layerwise uniform structure throughout training. 
    \item Mask parameters are optimized by proximal operators, maintaining the original model's capabilities to the greatest extent while adhering to target sparsity constraints and minimizing performance degradation during pruning.
    \item Extensive experiments were conducted across various sparsity levels on models from the LLaMA family, demonstrating the effectiveness of our method by maintaining high performance on diverse tasks while preserving the model's uniform structure.
\end{itemize}
