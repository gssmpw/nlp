\section{Methodology}
In this chapter, we explain how MaskPrune employs an optimization-based approach to generate structured pruning masks while maintaining consistency across inter-layer structures throughout the process. Specifically, Section \ref{Problem Definition} defines the optimization problem and Section \ref{Parameter Update Strategy} introduces the methods to solve this problem. Figure \ref{fig:MaskPrune2} illustrates the overall framework of our proposed method.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{MaskPrune2.png}
    \caption{Overall framework of MaskPrune. We optimize mask values through proximal gradient updates to identify the optimal pruning structure while simultaneously fine-tuning other parameters.}
    \label{fig:MaskPrune2}
\end{figure}

\subsection{Problem Definition} \label{Problem Definition}

For the transformer models such as Llama families, the Multi-Head Attention (MHA) layer and the Feed-Forward Network (FFN) comprise the primary components. During the pruning, the main structural elements, specifically, the attention heads and the intermediate dimensions of the FFN, are typically processed. To facilitate pruning, the corresponding masks, $m_{head}$ and $m_{inter}$ are introduced at their respective positions within the model. Taking the Llama architecture as an example:

\begin{equation*}
MHA^{l}(X) = \sum_{j=1}^{N_{h}} m_{head}^{(l, j)} \cdot \text{Attn}^{(l, j)}(X)
\end{equation*}

where \( l \) denotes the \( l \)-th layer and \( N_h \) denotes the number of attention heads in the Multi-Head Attention layer.

{\small
\begin{equation*}
FFN^{l}(X) = m^l_{inter} \cdot \left( W^l_{gate}(X) \odot W^l_{up}(X) \right) \cdot W^l_{down}(X)
\end{equation*}
}


Here, $m_{head}$ and $m_{inter}$ are restricted within the range $[0,1]$. When a mask value is 0, the corresponding module is pruned. The actual sparsity of the model is calculated as follows:

\begin{equation*}
\begin{aligned} 
\hat{s} &= \frac{1}{M} \cdot 4 \cdot d_{h} \cdot d_{hidden} \cdot \sum_{l=1}^{L} \sum_{j=1}^{N_{h}} \mathbb{I}\left(m_{head}^{(l, j)}=0\right) \\
&\quad + \frac{1}{M} \cdot 3 \cdot d_{hidden} \cdot \sum_{l=1}^{L} \sum_{j=1}^{d_{f}} \mathbb{I}\left(m_{inter}^{(l, j)}=0 \right)
\end{aligned}
\end{equation*}

Here, \( M \) denotes the original size of the model, \( L \) is the total number of layers in the model, \( d_{hidden} \) represents the hidden dimension, and \( d_f \) and \( d_h \) signify the head dimension and the intermediate dimension within the FFN, respectively. The indicator functions are used to identify the pruned components of the model, these components constitute the pruned model. The objective of our method is to implement structured pruning using a regularized approach. Therefore, the optimization target encompasses not only the pruning mask $m$ but also the sparsity parameters $s=\{s_{head},s_{inter}\}$ across various dimensions, where $s_{head}$ and $s_{inter}$ represent the sparsity ratios of the MHA layers and FFN layers, respectively. For clarity, $s$ below denotes the number of pruned dimensions, which is the product of the original dimensions and the sparsity.

The sparse constraint problem for the model can be reformulated as follows \cite{tono2017efficientdcalgorithmconstrained}, where \( m \in \{ m_{head}, m_{inter} \} \) and \( s \in \{ s_{head}, s_{inter} \} \):

\begin{equation*}
\sum_j \mathbb{I} \left(  m^{(l,j)} = 0 \right) \geq s \iff \left\| m^{l} \right\|_{s,2} = 0
\end{equation*}


where \( \left\| m^{l} \right\|_{s,2} \) represents the 	$\ell_2$ norm of a subvector of \( m^{l} \), consisting of the \( s \) elements with the smallest norms. This constraint ensures that, for a given dimension's mask, the $s$ smallest elements in each layer's mask are zero, meaning $s$ dimensions are pruned, thereby maintaining the uniformity of the mask structure across layers.

Our method aims to compress the model under resource constraints. Given a resource target, the algorithm will continuously compress the model until the target is met. In the context of model compression, the resource constraint typically refers to the model's memory footprint. When the target size of the model after pruning is ${M}_{prune}$, it satisfies

\begin{equation*}
M(s) \leq M_{prune}
\end{equation*}


where \( {M}(s) \) quantifies the memory footprint of the model after pruning based on sparsity \( s \):
{
\small
\begin{equation*}
M(s) = M - L \cdot (4 \cdot d_{hidden} \cdot d_{h} \cdot s_{head} + 3 \cdot d_{hidden} \cdot s_{inter})
\end{equation*}
}


In summary, our final optimization goal can be described as a minimax problem:

% \begin{align*}
% \small
% \min_{\{m,s\}}\max_{\{y,z\geq0\}}\mathcal{L}_{\mathrm{pruning}} &= \min_{\{m,s\}}\max_{\{y,z\geq0\}}\mathcal{L}(m) \\
% &\hspace{1em} + \underbrace{y\left\|m\right\|_{\lceil s \rceil,2}^2}_{\mathrm{sparsity~loss}}  \\
% &\hspace{1em}+ \underbrace{z\left(M(s)-M_{prune}\right)}_{\mathrm{resource~loss}}
% \end{align*}



\begin{align*} 
% \small 
&\min_{\{m,s\}}\max_{\{y,z\geq0\}}\mathcal{L}_{\mathrm{pruning}} = \min_{\{m,s\}}\max_{\{y,z\geq0\}}\mathcal{L}(m) \\ & + \underbrace{y \sum_{l=1}^{L} \left( \left\|m_{head}^{l}\right\|_{\lceil s_{head} \rceil,2}^2 + \left\|m_{inter}^{l}\right\|_{\lceil s_{inter} \rceil,2}^2 \right)}_{\mathrm{sparsity~loss}} \\ & + \underbrace{z\left(M(s)-M_{prune}\right)}_{\mathrm{resource~loss}} 
\end{align*}



Ultimately, we utilize the solution $s$ to the aforementioned minimax problem to determine the compression ratio for each layer. In this process, we directly sort the norms of the elements, selecting and removing those with the smallest norms to perform pruning.

\subsection{Parameter Update Strategy} \label{Parameter Update Strategy}

We iteratively solve the aforementioned optimization problem. Following the approach of \cite{tono2017efficientdcalgorithmconstrained, Chen2023ResourceCM, yu2022unified}, we first update the mask to optimize the training loss. Subsequently, we update \( s \), which balances the sparsity loss and resource loss, ultimately achieving a convergence state. Finally, we update the variables \( y \) and \( z \) to increase the penalties associated with the sparsity and resource losses, thereby promoting the convergence of \( s \). The specific update strategy is as follows:

In this optimization step, we fix the model parameters, the sparsity variable \( s \), and the Lagrange multipliers \( y \) and \( z \) while updating \( m \). Following the methodology of \cite{yang2019ecc}, we minimize the loss proxy of \( m \) in the original loss function \( \mathcal{L}(m) \) at \( m_t \):

\begin{equation*}
\mathcal{L}(m^t) + \langle \hat{\nabla} \mathcal{L}(m^t), m - m^t \rangle + \frac{1}{2\eta_1} \| m - m^t \|^2
\end{equation*}


where ${\eta_1}$ is the learning rate for $m$. Therefore, the original problem can be simplified to the following proximal optimization update:

\begin{equation*}
\arg\min_m \frac{1}{2} \left\| m - \bar{m} \right\|^2 + \eta_1 y \left\| m \right\|_{\lceil s \rceil,2}^2
\end{equation*}

where $\bar{m}=m^t-\eta_1\hat{\nabla}_m\mathcal{L}(m^t).$
We set $S(y^t,s^t,m) = {y^t\left\|m\right\|_{\lceil s^t \rceil,2}^2}$ and the solution to the proximal operator $\mathrm{Prox}_{\eta_1S(y^t,s^t,m)}(\bar{m})$ can be defined as follows:

\begin{equation*}
m_i^* = 
\begin{cases}
\bar{m}_i, & \mathrm{if}\,\bar{m}_i \geq \bar{m}_{\text{least-}\lceil s\rceil} \\
\frac{1}{1+2\eta_1y} \bar{m}_i, & \text{otherwise}
\end{cases}
\end{equation*}


Here, $m_i$ represents the $i$-th element of the mask and least-$j$ denotes the index of the element in $m$ with the $j$-th smallest norm.

Unlike previous optimization-based methods, we do not use reparameterization to force $m$ to polarize to 
0 and 1. Instead, we adopt a uniformly distributed normal mask, allowing $m$ to update freely within the range of 0 to 1. This approach continually decays $m$ during the proximal optimization update to achieve pruning. Meanwhile, $\eta_1$ as the decay rate, can be freely adjusted as a hyperparameter and does not need to match the learning rate of the mask during actual optimization. In this process, the mask itself acts as a scaling factor for the weights, and the masks corresponding to unpruned weights can also attain intermediate values between 0 and 1 during optimization, thereby scaling the entire weight group and fine-tuning the weights. Since the mask can be freely optimized and is not limited to 0 and 1, in the actual pruning process, it is necessary to fuse the masks that are not equal to 1 into the weights. For the Llama model, the MHA and FFN components respectively scale the $W_V$ weights and the $W_{gate}$, $W_{up}$ weights with the weight group to maintain weight uniformity before and after pruning:

\begin{equation*}
\hat{W}_{V}^{(l, j)} = W_{V}^{(l, j)} \cdot m_{head}^{(l, j)}
\end{equation*}
\begin{equation*}
\hat{W}_{gate}^{l} = W_{gate}^{l} \cdot m_{inter}^{l}
\end{equation*}
\begin{equation*}
\hat{W}_{up}^{l} = W_{up}^{l} \cdot m_{inter}^{l}
\end{equation*}


In the optimization objective, the gradient of $s$ is related to two terms. The second term's resource constraint is typically a differentiable function of $s$, allowing for direct computation of its gradient $\tilde{\nabla}_{\boldsymbol{s}} z\left(M(s)-M_{prune}\right)$. However, the first term's sparsity constraint is non-differentiable. Specifically, $s$ is a floating-point number representing the model's dimension value, in practice, the ceiling function should be used to determine the integer number of dimensions to be pruned for each layer. However, the ceiling function is non-differentiable. To address this issue, apply the Straight-through Estimator \cite{bengio2013estimatingpropagatinggradientsstochastic} to provide an approximate gradient during backpropagation, i.e.,$\frac{\tilde{\partial}\lceil s\rceil}{\tilde{\partial}s}=1.$

For the sparsity loss involving $\left\|m\right\|_{s,2}^2$, using $\left\|m\right\|_{s+1,2}^2-\left\|m\right\|_{s,2}^2$ as the approximate proxy value for the partial derivative of $\left\|m\right\|_{s,2}^2$ :

\begin{equation*}
\frac{\tilde{\partial}\left\| m \right\|_{s,2}^2}{\tilde{\partial}s} = m_{\text{least-min}\{\mathrm{Dim}(m),s+1\}}^2
\end{equation*}

where $\mathrm{Dim}(m)$ is the number of elements in $m$.The variables $y$ and $z$ are coefficients of the Lagrangian penalty terms, which need to be continuously increased during the optimization process to minimize their corresponding penalty terms, thereby ensuring that the optimization objectives for $s$ and $m$ meet the desired sparsity goals. Specifically, we use gradient ascent to update them as follows:

\begin{equation*}
y^{t+1} = y^t + \eta_3 \| m^{t+1} \|_{\left\lceil s^{t+1} \right\rceil,2}^2
\end{equation*}
\begin{equation*}
z^{t+1} = \max(0, z^t + \eta_4 (M(s^{t+1}) - M_{prune}))
\end{equation*}






\subsection{Optimization with LoRA and Distillation}
LoRA \cite{hu2021lora} has been widely demonstrated to be efficient in fine-tuning LLMs. To effectively update weights during the optimization of the mask and achieve enhanced performance, similar to Compresso \cite{guo2023compresso} and NutePrune \cite{li2024nuteprune}, we introduce the LoRA module during optimization:
$$
W^{\prime}={W}+\Delta W=W+BA
$$

where $B \in  {\mathbb{R}}^{d \times  r},A \in  {\mathbb{R}}^{r \times  k}\;$ and $\;r \ll  \min \left( {d,k}\right)$. During the training process, the model's original weights $W$ are frozen, and only the parameters in the low-rank matrices $A$ and $B$ are trained.

Regarding loss functions, we introduce a distillation loss similar to those in CoFi \cite{xia2022structured} and NutePrune \cite{li2024nuteprune}. Specifically, $\mathcal{L}_{KL}$
denotes the Kullback-Leibler (KL) divergence between the probability distributions ${\mathbf{p}}_{t}$ and ${\mathbf{p}}_{s}$ of the output from the teacher model before pruning and the student model after pruning, respectively. Additionally, $\mathcal{L}_{layer}$ represents the sum of mean squared errors (MSE) of the hidden representations $h_s^l$ and $h_t^l$ cross the intermediate layers of the teacher and student models:

$$
{\mathcal{L}}_{\text{KL}} = {D}_{\mathrm{{KL}}}\left( {{\mathbf{p}}_{s}\parallel {\mathbf{p}}_{t}}\right)
$$
$$
{\mathcal{L}}_{\text{layer }} = \mathop{\sum }\limits_{l=1}^{L}\operatorname{MSE}(h_s^l,h_t^l)
$$
The coefficient of the two losses is controlled by the hyperparameter $\alpha$, and the final loss is formulated as:
$$
\mathcal{L}_{distill}=\mathcal{L}_{KL}+\alpha * \mathcal{L}_{layer}
$$






