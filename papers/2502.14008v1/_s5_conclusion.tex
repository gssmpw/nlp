% \vspace{-1em} 
\section{Conclusion}
% \vspace{-0.5em}
This paper introduces a structured pruning method for Large Language Models (LLMs) by formulating model pruning as a minimax optimization problem. During optimization, the pruning mask and the model's target dimensions are trained simultaneously, resulting in a pruned model with uniform inter-layer structures. We evaluated models with varying sparsity levels and sizes across multiple benchmarks, and the results consistently demonstrate the superiority of our method. Our approach also investigates different strategies for performing optimization-based structured pruning on LLMs, providing new insights into the compression of these models.

\section{Limitations}
The efficacy and effectiveness of mask learning-based pruning is highly dependent on the quality of the training or calibration data. Determining whether the chosen data is optimal and developing various strategies for selecting superior datasets remain significant challenges that warrant further investigation. 

% The process of learning pruning masks often requires adapting different models at the code level by inserting masks into various components of the model's architecture. This approach is mechanical and tedious, underscoring the need for more flexible and scalable methods to achieve end-to-end pruning. Additionally, the efficacy of mask learning is highly dependent on the quality of the selected data. Determining whether the chosen data is optimal and developing strategies for selecting superior datasets remain significant challenges that warrant further investigation.
