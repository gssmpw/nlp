\section{Experiments}

\subsection{Setup}

\paragraph{Model}

To validate the effectiveness and generalizability of our method, we conducted experiments on several models, including the Llama-1 \cite{touvron2023llama} and Llama-2 \cite{touvron2023llama2} families, encompassing 7B and 13B configurations.

\paragraph{Implementation Details}

We sampled 20,000 data instances, each consisting of 512 tokens, from the C4 dataset \cite{raffel2020exploring} to serve as training data for mask optimization using the AdamW optimizer. The learning rate was set to 1e-2 for the mask parameters and 1e-3 for the LoRA parameters, with a batch size of 16. The pruning process was conducted over 7 epochs, during which the sparsity of each dimension incrementally increased until the target total sparsity was achieved. For the masks corresponding to the intermediate dimensions of FFN, proximal gradient updates were not performed at every step. Instead, updates occurred every $t$ iteration, while regular gradient descent was conducted at other times. The parameter $t$ decreased linearly from 10 to 1 throughout the optimization process. The target model was obtained directly after pruning without post-fine-tuning. All experiments were executed on a single NVIDIA A100 GPU.

\paragraph{Evaluation}

We initially assessed the pruned model's language modeling capability by measuring its perplexity on the Wikitext \cite{merity2016pointer} dataset. Furthermore, to ensure consistency with previous approaches, we conducted a comprehensive evaluation of the model's zero-shot capabilities using the lm-evaluation-harness \cite{eval-harness}. This evaluation encompassed zero-shot tasks on common sense reasoning datasets, including BoolQ \cite{clark2019boolq}, PIQA \cite{bisk2020piqa}, HellaSwag \cite{zellers2019hellaswag}, WinoGrande \cite{sakaguchi2020winogrande}, ARC-easy \cite{clark2018think}, ARC-challenge \cite{clark2018think} and OpenbookQA \cite{mihaylov2018can}.

\paragraph{Baseline}

We compared our method with the widely used LLM-Pruner \cite{ma2023llm}. Since our approach is based on optimization for structured pruning, we also evaluated it against methods with similar objectives, such as Compresso \cite{guo2023compresso} and NutePruner \cite{li2024nuteprune}. Although we adopted the same experimental settings as these methods, MaskPrune is disadvantaged due to the inherent layer alignment characteristic of our approach, especially when compared to methods like NutePrune \cite{li2024nuteprune}, which utilize different inter-layer structures. The NutePrune codebase provides a layer-uniform loss function similar to SheardLlama \cite{xia2023sheared}. We employed this configuration to reproduce NutePrune-uniform, thereby ensuring a more equitable comparison. However, due to the intrinsic properties of this loss function, complete alignment could not be achieved within the same 7 epochs; while achieving full alignment required up to 36 epochs. For optimization-based methods, this extended time cost is impractical. Therefore, we increased the coefficient of the sparsity loss term in NutePrune's loss function to attain a balanced state as effectively as possible within 7 epochs.

\subsection{Main Results}


\begin{table*}
  \centering
  % \small
  \resizebox{\linewidth}{!}{
  \begin{tabular}{@{}lccccccccccl@{}}
    \hline
    \textbf{Ratio} & \textbf{Method} & \textbf{WikiText2$\downarrow$} & \textbf{BoolQ} & \textbf{PIQA} & \textbf{HellaSwag} & \textbf{Winogrande} & \textbf{ARC-e} & \textbf{ARC-c} & \textbf{OBQA} & \textbf{Avg.$\uparrow$} \\ 
    \hline
    0\%  & LLaMA-7B & 5.68 & 73.18 & 78.35 & 72.99 & 67.01 & 67.45 & 41.38 & 42.40 & 66.39 \\ 
    \hline
    20\% 
    & LLM-Pruner & 9.96 & 59.39 & 75.57 & 65.34 & 61.33 & 59.18 & 37.12 & 39.80 & 59.01 \\ 
    & Compresso & 10.38 & \textbf{73.64} & 75.08 & 64.77 & \textbf{67.72} & 66.12 & 37.54 & \textbf{40.40} & 60.75 \\
    & NutePrune-uniform & 8.74 & 71.01 & \textbf{76.55} & 67.97 & 65.75 & 68.10 & 36.60 & 38.20 & 60.59 \\
    & \textbf{MaskPrune} & \textbf{7.77}  & 68.50 & 76.17 & \textbf{69.84} & 66.30 & \textbf{68.56} & \textbf{39.68} & 39.20 & \textbf{61.17} \\
    \hline
    25\%
    & NutePrune-uniform & 9.48 & 66.85 & \textbf{75.52} & 65.92 & 61.33 & \textbf{66.29} & 34.64 & 36.00 & 58.07 \\
    & \textbf{MaskPrune} & \textbf{8.70} & \textbf{67.77} & 75.41 & \textbf{66.54} & \textbf{61.40} & 64.90 & \textbf{35.75} & \textbf{37.40} & \textbf{58.45} \\
    \hline
    50\% 
    & LLM-Pruner & 98.10 & 52.32 & 59.63 & 35.64 & 53.20 & 33.50 & 27.22 & 33.40 & 40.94 \\ 
    & Compresso & 59.73 & 60.09 & 66.70 & 39.31 & 51.93 & 48.82 & \textbf{27.82} & 33.40 & 46.87 \\
    & NutePrune-uniform & 20.69 & 62.84 & 68.50 & \textbf{50.76} & 54.22 & 48.99 & 26.96 & \textbf{33.60} & 49.41 \\
    & \textbf{MaskPrune} & \textbf{19.33} & \textbf{63.39} & \textbf{70.73} & 49.96 & \textbf{55.56} & \textbf{50.76} & 25.68 & 33.40  & \textbf{49.92} \\
    \hline
  \end{tabular}
  }
  \caption{\label{Llama-7B}
    Performance on zero-shot tasks and perplexity on the Wikitext2 dataset for the compressed Llama-7B model. NutePrune-uniform denotes the version of NutePrune with uniform inter-layer structures.
  }
\end{table*}

\paragraph{Zero-Shot Tasks}

For the Llama-7B baseline model, we applied pruning to generate models with three sparsity levels: 50\%, 25\%, and 20\%. As shown in Table \ref{Llama-7B}, our method outperforms Compresso \cite{guo2023compresso}, which employs uneven inter-layer sparsity, in terms of both perplexity and common sense reasoning tasks, demonstrating superior performance under challenging conditions. Compared to the layer-uniform version of NutePrune \cite{li2024nuteprune}, our method maintains higher model capabilities at each sparsity level, showing improvements of 0.58\%, 0.38\%, and 0.51\%, respectively. These results indicate that our method can consistently identify appropriate modules for pruning during optimization and effectively scale the remaining modules to preserve model performance.

Our method also demonstrates superior capabilities for larger models, such as Llama-13B and Llama-2-13B. Specifically, for Llama-13B and Llama-2-13B at a 20\% sparsity level, our method maintains 94\% and 95\% of the original model's zero-shot task capabilities, with no significant decrease in perplexity. 

\paragraph{Uniformity}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{uniform.png}
    \caption{The number of heads and FFN intermediate dimensions retained after pruning Llama-7B to a sparsity of 50\% }
    \label{fig:uniform}
\end{figure}

As illustrated in Figure~\ref{fig:uniform}, our method continuously regularizes the sparsity across the model's layers to maintain uniformity throughout the training process, ultimately achieving a completely uniform structure. In contrast, Compresso and NutePrune permit unrestricted learning of masks during training, resulting in significant disparities in sparsity between layers, especially at higher sparsity levels. For instance, at 50\% sparsity of Llama-7B, some layers retain only the 2 attention heads while others retain the 27 attention heads. This irregular sparsity distribution poses greater challenges for the model during training and post-processing. The layer-uniform version of NutePrune essentially maintains uniformity across layers but struggles to achieve complete uniformity, ultimately having to compromise by reducing sparsity or performance during the actual pruning stage. In contrast, our method inherently maintains a uniform inter-layer structure, consistently preserving uniformity after the training process concludes.

% \paragraph{Larger Models}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{decay_rate.png}
    \caption{Zero-shot performance and actual sparsity of the Llama-7B model at 50\% sparsity under different decay rates.}
    \label{fig:Decay Rate}
\end{figure}




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{interval.png}
    \caption{Training loss under different optimization intervals}
    \label{fig:Optimization Intervals Loss}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{intermediate_dims_var_comparison.png}
    \caption{Variance changes in the intermediate dimensions of each FFN layer during the pruning process under different optimization intervals.}
    \label{fig:Optimization Intervals Variance}
\end{figure}

\begin{table*}
  \centering
  % \small
  \resizebox{\linewidth}{!}{
  \begin{tabular}{@{}lccccccccccl@{}}
    \hline
    \textbf{Ratio} & \textbf{Method} & \textbf{WikiText2$\downarrow$} & \textbf{BoolQ} & \textbf{PIQA} & \textbf{HellaSwag} & \textbf{Winogrande} & \textbf{ARC-e} & \textbf{ARC-c}
    & \textbf{OBQA} & \textbf{Avg.$\uparrow$} \\ 
    \hline
    0\%  & LLaMA-13B & 5.62 & 77.68 & 79.49 & 77.01 & 72.69 & 76.68 & 45.99 & 44.00 & 67.64 \\
    \hline
    20\% 
        & NutePrune-uniform & \textbf{7.59} & \textbf{73.91} & \textbf{77.75} & 71.68 & \textbf{68.75} & 71.51 & 39.76 & 40.40 & 63.39 \\
        & \textbf{MaskPrune} & 8.25 & 73.00 & 77.15 & \textbf{72.21} & 68.51 & \textbf{72.14} & \textbf{41.21} & \textbf{41.00} & \textbf{63.60} \\
    \hline
    0\%  & LLaMA-2-13B & 5.30 & 82.05 & 77.86 & 77.22 & 72.77 & 78.37 & 47.18 & 44.80 & 68.83 \\
    \hline
    20 \%
        & NutePrune-uniform & 7.40 & 76.24 & 76.55 & 71.53 & 67.48 & 71.76 & 39.93 & 41.00 & 63.52 \\
        & \textbf{MaskPrune} & \textbf{6.64} & \textbf{76.70} & \textbf{77.69} & \textbf{73.06} & \textbf{70.80} & \textbf{74.16} & \textbf{43.26} & \textbf{41.60} & \textbf{65.32} \\
    \hline
  \end{tabular}
  }
  \caption{\label{Llama-13B}
    Performance on zero-shot tasks and perplexity on the Wikitext2 dataset for the compressed Llama-13B and Llama-2-13B model.
  }
\end{table*}


% For larger models, such as Llama-13B and Llama-2-13B, our method also demonstrates superior capabilities. Specifically, for Llama-13B and Llama-2-13B at 20\% sparsity level, our method maintains 94\% and 95\% of the original model's zero-shot task capabilities, with no significant decrease in perplexity (PPL). Additionally, for these larger models, the Massive Multitask Language Understanding (MMLU) benchmark, which evaluates context learning capabilities, serves as an important evaluation metric. We assessed the performance of the pruned models on these benchmarks, and the pruned Llama-13B model achieved \textbf{\textit{[percentage]}} MMLU capabilities, while Llama-2-13B achieved \textbf{\textit{[percentage]}} MMLU capabilities, both outperforming the layer-uniform versions of LLM-Pruner and NutePrune.

% \begin{table*}[h!]
% % \small
% \centering
% \caption{In-Context Learning Capabilities of Llama-13B and Llama-2-13B on the 5-shot MMLU Benchmark}
% \label{incontextLlama-13B}
% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{@{}lccccccc@{}}
% \toprule
% \textbf{Ratio} & \textbf{Method}  & \textbf{STEM} & \textbf{Humans} & \textbf{Social} & \textbf{Other} & \textbf{MMLU (5-shot)$\uparrow$} \\ 
% \midrule
% 0\%  & LLaMA-13B  & - & - & - & - & -  \\
%  \midrule
% 20\% 
%     & NutePrune-uniform  & - & - & - & - & - \\
%     & \textbf{MaskPrune}  & - & - & - & - & - \\
% \midrule
% 0\%  & LLaMA-2-13B  & - & - & - & - & - \\
% \midrule
% 20 \%
%     & NutePrune-uniform  & - & - & - & - & - \\
%     & \textbf{MaskPrune}  & - & - & - & - & - \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table*}

\subsection{Analysis}

\paragraph{Type of Mask}

\begin{table}
  \small
  \centering
  \begin{tabular}{@{}lcl@{}}
    \hline
    \textbf{Model} & \textbf{Type of mask} & \textbf{Avg.$\uparrow$} \\ 
    \hline
    & \textbf{Uniform (Ours)} & \textbf{49.92} \\
    LLaMA-7B & $L_0$ \cite{louizos2018learning} & 45.36 \\
    & Polarization \cite{guo2021gdp} & 48.29 \\
\hline
  \end{tabular}
  \caption{\label{Type of Mask}
    Impact of different types of masks on model compression performance.
  }
\end{table}

We explored the impact of different mask types on our method by selecting three representative mask distributions: (1) a standard uniform distribution mask, which can be freely optimized within the range of [0,1]; (2) a standard $L_0$ regularization mask \cite{louizos2018learning}, which drives the mask values to 0 and 1 through reparameterization; and (3) a differentiable polarizing mask \cite{guo2021gdp}. As shown in Table \ref{Type of Mask}, the uniform distribution mask performs the best. We speculate that this is because decaying the mask essentially involves a smooth transition from 1 to 0. During intermediate values, it is equivalent to scaling the model's weights, allowing the model to transition more effectively from dense to sparse. The steep distribution of the $L_0$ regularization mask conceals this advantage, preventing the mask from correctly attaining intermediate states. The differentiable polarizing mask gradually becomes steeper during training, and this changing distribution also alters the scaled weights. Additionally, our method naturally contracts to 0, eliminating the need to approximate the mask distribution using other distributions.

\paragraph{Decay Rate}

We also investigated the impact of different mask decay rates on the final model performance. As shown in Figure \ref{fig:Decay Rate}, experiments indicate that a decay rate that is too high causes masks with initially small values to quickly decay to 0, preventing the adjustment of incorrectly pruned weights. Conversely, a decay rate that is too low fails to counteract the mask's inherent optimization trend, preventing the mask from decaying to minimal values and achieving the desired sparsity and layer uniformity.

\paragraph{Optimization Interval}




In our method, we do not perform proximal gradient updates on the mask values at every iteration step. This approach can cause some masks to erroneously decay to zero rapidly and become irrecoverable in subsequent optimization processes, leading to the unintended pruning of weights and a significant decline in model performance. This phenomenon is particularly evident when pruning the intermediate dimensions of the FFN. To prevent this, during regular iteration steps, we only perform standard gradient descent on the masks and conduct proximal gradient updates at specified intervals to ensure that the masks achieve the target sparsity. The optimization interval linearly decreases from 10 to 1 throughout the optimization process, allowing the model to eventually converge to the desired sparsity level. We investigate the impact of the hyperparameter optimization interval on model pruning performance. As shown in Figure \ref{fig:Optimization Intervals Loss} and Figure \ref{fig:Optimization Intervals Variance}, we present the final training loss and the variance of the FFN dimensions across layers under different optimization intervals, respectively. The latter reflects the uniformity across model layers and typically exhibits a trend of initially increasing and then decreasing. It can be observed that when the optimization interval is set to 10, the training loss reaches its minimum. When the interval is smaller than this value, the model converges rapidly in the early stages of training, causing some masks to erroneously decay to zero and remain irrecoverable. Conversely, when the interval is larger than 10, the masks only begin to decay to zero as the optimization interval gradually decreases in the later stages, thereby wasting the early pruning process.

% \paragraph{Mask Distribution}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{head_z_heatmap.png}
%     \caption{Heatmap of the value distribution of attention head pruning masks}
%     \label{fig:head_z_heatmap}
% \end{figure}

% To investigate the effects of mask values across different layers, we plotted their distribution. As shown in Figure \ref{fig:head_z_heatmap}, this figure represents the pruning mask corresponding to multi-head attention. It can be observed that, aside from attention heads with values of 0 that have been pruned, the remaining mask values exhibit varying distributions across different layers. In the first half of the model's layers, mask values are predominantly distributed around 0.8, scaling the parameters of the attention heads to compensate for performance degradation caused by pruning. In contrast, in the second half of the model's layers, mask values are generally maintained around 1. We speculate that these layers are less sensitive to pruning and that direct pruning has a minimal impact on the model's performance, thus negating the need for compensation by other weights.
