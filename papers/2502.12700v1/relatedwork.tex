\section{Related Work}
Creative writing \cite{Kobak2024, lee2024} is on the rise; however, some studies suggest that content generated by human users tends to be more creative \cite{Kefford2023}. This study show that ChatGPT's ideas are more purchased from Wharton MBA students. There is an ongoing debate about whether LLMs can enhance creativity. To explore this, \cite{lee2024} demonstrates that when participants were tasked with generating creative ideas for everyday purposes, their creativity improved. However, \cite{Begus2023} finds that AI-generated narratives often lack imagination and typically include plot twists in a more casual manner. Additionally, \cite{Chakrabarty2024} invited expert writers to evaluate stories generated by LLMs versus those created by professional writers using a standard creativity test. Their findings suggest that LLM-generated stories are less creative compared to those written by professionals.
Empirical studies have underscored this issue. For example, \cite{si2024can} conducted qualitative analyses involving human judgment and found that after generating 500 samples, 50\% were non-repetitive ideas. However, in the following 1,500 generations, only an additional 50\% of non-repetitive ideas were produced. Alarmingly, in the final 2,000 rounds, just 12.5\% of the generated ideas were non-repetitive. This suggests that while an individual LLM output may appear novel, when generating multiple outputs, the LLM tends to become repetitive, lacking the diversity necessary to effectively enhance collective creativity.
This decline underscores the resource inefficiency and diminishing returns in prolonged LLM-generated content. \\
\cite{McCoy2023} suggests that novelty in LLM outputs can be detected by ensuring "the text must not have been copied from the training data." However, a more recent study by \cite{xu2024echoes} argues that this definition is superficial. In their experiment on story continuation, they demonstrate that while GPT-4ogenerated samples may meet this standard, the generated continuations are still quite conventional and lack diversity.
\cite{Shaib2024} analyze different existing scores that can help measure diversity in LLM outputs, but these metrics all focus on surface-level features such as n-gram overlaps. \cite{Ghosal2022} indicate that "identifying novel text is not straightforward because the text many have less lexical overlap yet convey the same information." and to the best of our knowledge there is no study to evaluate the diversity, novelty, and correctness of the generated outputs at the same time.