\documentclass{article} % For LaTeX2e
\usepackage{times}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{booktabs} % for professional tables
\usepackage{ragged2e}
\usepackage{soul}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{Styles/icml2025}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{Styles/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
% simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\newcommand{\ky}[1]{\textcolor{violet}{[Kayo: #1]}}
\newcommand{\js}[1]{\textcolor{blue}{[Jacob: #1]}}
\newcommand{\red}[1]{\textcolor{red}{#1}}



% \newcommand{\ky}[1]{}
% \newcommand{\js}[1]{}
% \newcommand{\red}[1]{}

\sethlcolor{yellow}

% \title{Induction Heads and Function Vectors\\ for In-Context Learning}


\begin{document}

\twocolumn[
\icmltitle{Which Attention Heads Matter for  In-Context Learning?}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kayo Yin}{yyy}
\icmlauthor{Jacob Steinhardt}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{UC Berkeley}

\icmlcorrespondingauthor{Kayo Yin}{kayoyin@berkeley.edu}
\icmlcorrespondingauthor{Jacob Steinhardt}{jsteinhardt@berkeley.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{interpretability, in-context learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution



\begin{abstract}
Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. 
Two different mechanisms have been proposed to explain ICL:
induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task.
To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models. 

Through detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads
start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL\footnotemark[2]. %Finally, through reconciling our findings with previous work, we identify several important methodological considerations for 
%Finally, the prevalence of FV and induction heads varies with architecture, which questions strong versions of the
%``universality'' hypothesis: findings from interpretability research are not always generalizable across models\footnote{Code and data will be released upon publication.}. 
\end{abstract}
\footnotetext[2]{Code and data: \url{https://github.com/kayoyin/icl-heads}.}

\section{Introduction}

One of the most remarkable features of large language models (LLM) is their ability to perform in-context learning (ICL), where they can adapt to various new tasks using only a few demonstrations at inference time. This capability has become a crucial tool for adapting pre-trained LLMs to specific tasks, sparking significant research interest in understanding its underlying mechanisms \citep{ih, akyurek2022learning, von2023transformers}. 


To date, two key mechanisms have been primarily associated with ICL, substantiated by different lines of evidence. First, \textit{induction circuits} \citep{elhage2021mathematical} were hypothesized to be the primary mechanism behind ICL in LLMs \citep{ih, singh2024needs, crosbie2024induction, dong2022survey}. Induction circuits operate by identifying previous occurrences of the current token in the prompt and copying the subsequent token. More recently, \citet{fv} and \citet{tv} propose the existence of \textit{function vectors} (FV). FVs are a compact representation of a task extracted from specific attention heads, and they can be added to a model's computation to recover ICL behavior without in-context demonstrations.


\begin{figure}[tbp]
\captionsetup[subfigure]{justification=Centering}
\begin{subfigure}{\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/abl_1.pdf}
    \caption{ICL accuracy of Pythia 6.9B across different percentages of heads ablated.}
    \label{fig:abl_6}
\end{subfigure} 
\begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figures/fig1.pdf}
    \caption{Induction score (blue) and FV score (pink) of an FV head during training.}
    \label{fig:indv_ckpt_6}
\end{subfigure}
\caption{(a) Ablating function vector (FV) heads significantly degrades few-shot in-context learning (ICL) accuracy, while ablating induction heads has minimal impact beyond ablating random heads. (b) Evolution of an FV head during training, demonstrating high induction scores earlier in training that decrease as FV score emerges. This pattern suggests induction may serve as a precursor for FV mechanism.}
\label{fig:fig1}
\end{figure}

\begin{table*}[htp]
    \centering
    \caption{Summary of findings in this work, where $\checkmark$ represents findings with evidence directly shown by our experiments and $\sim$ represents conjectures that our results suggest.}
      \resizebox{\linewidth}{!}{
    \begin{tabular}{lccl}
    \toprule
     \textbf{Findings} & \textbf{Evidence}  & \textbf{Section} & \textbf{Contribution}\\
     \midrule
Induction heads and FV heads are distinct. & $\checkmark$ & \ref{sec:overlap} & Context-setting\\
Induction scores and FV scores are correlated. & $\checkmark$ & \ref{sec:overlap} & Context-setting\\
Ablating FV heads hurts few-shot ICL accuracy more than ablating induction heads. & $\checkmark$ & \ref{sec:abl} & Main finding \\
Some FV heads evolve from induction heads during training. & $\checkmark$ & \ref{sec:ckpt} & Main finding \\
FV heads implement more complex or abstract computations than induction heads. & $\sim$ &  \ref{sec:ckpt} & Speculation \\
% High head dimensionality is a predictor of FV strength & \sim & \ref{sec:ckpt}\\
\bottomrule
    \end{tabular}}
    \label{tab:hypotheses}
\end{table*}

% \ky{Rewrite intro: make it clear what key claims are (induction heads are responsible for a narrowly defined ICL, function vectors more important for few-shot ICL, field should clarify this in subsequent works)}


To resolve whether one or both mechanisms drive ICL in transformer LLMs, we conduct a comprehensive study of the attention heads implementing these mechanisms (termed \textit{induction heads} and \textit{FV heads}) across 12 decoder-only transformer models ranging from 70M to 7B parameters (Table \ref{tab:models}) and 45 natural language ICL tasks (listed in Appendix \ref{app:tasks}). Our analysis reveals several key findings.

First, we verify that there is a difference to explain, i.e. that induction and FV heads are indeed distinct (\S\ref{sec:overlap}). Across models, there is a low or zero overlap between induction and FV heads. These heads also have distinct characteristics: induction heads generally appear in slightly earlier layers than FV heads, and emerge significantly earlier during training. On the other hand, there are correlations in behavior: FV heads behave more similarly to induction heads than a random head from the same network, and vice versa.

Second, through ablation studies (\S\ref{sec:abl}), we demonstrate that FV heads are the primary drivers of ICL performance. Removing FV heads substantially degrades ICL task accuracy, while removing induction heads has a limited effect (Figure \ref{fig:abl_6}). This effect is consistent in all 12 models we studied, and becomes more pronounced in larger models (\S\ref{sec:abl}). Interestingly, this challenges the prevailing view of induction heads as the key mechanism for few-shot ICL \citep{ih, crosbie2024induction, dong2022survey}.


Third, we reconcile our findings with previous work by identifying three key methodological differences (\S\ref{sec:bg}): earlier studies used a different metric for ICL that does not strongly track few-shot performance, did not account for correlations between FV and induction heads, and sometimes focused on small models. We find the choice of metric is the most significant factor, and discuss this in detail in \S\ref{sec:bg} with detailed experiments in \S\ref{sec:abl}.

%which we call \textit{token-loss difference}. In our work, we measure ICL performance by computing accuracy on few-shot learning tasks, since this definition of ICL is more commonly adopted in the literature \citep{brown2020language, dong2022survey}. We find that token-loss difference and few-shot accuracy behave differently, and ablating induction heads do strongly affect token-loss difference without similarly affecting few-shot accuracy. Second, studies on induction heads did not consider possible overlap between induction and FV heads. Once we ablate only induction heads that are not also FV heads, the effect on ICL performance becomes much weaker. Previous studies were also often done on smaller models, since they required mechanistic analysis, but we find that FV heads become progressively more important for ICL relative to induction heads as models get larger.

% We start by examining where and when induction and FV heads occur in models. We define \textbf{induction score} and \textbf{FV score} to measure the induction and FV strengths of attention heads (\S\ref{sec:rel_work}). Using these scores, we find that the prevalence of induction heads and FV heads are mostly consistent across model size with a few exceptions: models with high attention head dimensionality have weaker induction heads and stronger FV heads (\S\ref{sec:ihfv}). Although we confirm that the set of induction heads and FV heads are indeed distinct, we also find that induction and FV scores are correlated (\S\ref{sec:overlap}). Most importantly, we find that ICL is primarily driven by FV heads---ablating FV heads reduces ICL performance much more than induction heads and this gap increases with model scale. When we ablate induction heads but preserve FV heads, models are able to preserve most of their ICL performance (\S\ref{sec:abl}; Figure \ref{fig:abl_6}). This calls into question the widely held belief that induction heads implement ICL \citep{ih, zhou2023mystery, bansal-etal-2023-rethinking}. 
% ) reveal that to the extent induction and FV heads are different, most of ICL in LLMs are driven by FV heads, contrary to the widely held belief that induction heads implement ICL. Not only does ablating FV heads have a bigger effect on ICL performance than ablating induction heads, ablating only induction heads with low FV strength reveals no significant impact on ICL accuracy. This indicates that FV heads account for ICL performance (

% We further examine induction and FV heads in relation to each other, and identify several phenomena of interest:

% \begin{itemize}
% \item The set of induction heads and FV heads are distinct, but there is some correlation between them (\S\ref{sec:overlap}).
% \item FV heads appear deeper in models than induction heads (\S\ref{sec:overlap}).
% \item FV heads appear later during training than induction heads (\S\ref{sec:evolution}).
% \item There are many instances of induction heads that \textit{transition to} FV heads during training, but the reverse does not occur (\S\ref{sec:evolution}).
% \end{itemize}

% We further explore how the two types of attention heads evolve during training (\S\ref{sec:ckpt}). In all models, induction heads emerge before FV heads during training, which may imply FV heads are harder to learn. Induction strength quickly plateaus or decreases after its initial emergence, whereas FV strength steadily increases throughout training. Interestingly, many FV heads start as induction heads earlier in training before gaining FV head behavior (Figure \ref{fig:indv_ckpt_6}), whereas the reverse does not happen; this suggests that induction heads might be a precursor to FV heads.

% In \S\ref{sec:disc}, we propose working conjectures to unify the above findings. In one conjecture, we speculate that induction heads facilitate learning the more complex FV heads for ICL -- the FV mechanism is more accurate at performing ICL, and therefore eventually replaces the simpler induction mechanism. Our findings also question universality -- we find that despite induction and FV strengths seemingly being consistent across scale, they exhibit a marked change for certain architectures, and importance of induction heads for ICL also varies with model scale. Our study underscores the variability of neural models and the importance of understanding the interplay between different mechanisms. % as well as how generalizable interpretability findings are.
% \ky{TODO also elaborate on polysemantic }

Finally, by analyzing training dynamics (\S\ref{sec:ckpt}), we uncover a surprising developmental relationship: many induction heads \textit{evolve} into FV heads during training, but the reverse never occurs (Figure \ref{fig:indv_ckpt_6}). 
This leads us to speculate that induction heads facilitate learning the more complex FV heads for ICL -- the FV mechanism is more effective at performing ICL, and therefore eventually replaces the simpler induction mechanism \S\ref{sec:disc}. 

Aside from clarifying the drivers of few-shot ICL, our findings offer broader lessons for model interpretability research. They highlight how correlations between related mechanisms can lead to illusory explanations (e.g. the confounding effect of the correlation between induction and FV heads), and the choice of definitions may lead to different conclusions (e.g. different metrics to measure ICL).
Additionally, our results challenge strong versions of universality -- the difference between the importance of FV heads and induction heads shifts significantly with model scale (\S\ref{sec:abl}). %Our study underscores the variability of neural models and the importance of understanding the interplay between different mechanisms.



\section{Background \& related work}\label{sec:bg}
% \ky{Summarize all works on mechanism behind ICL. Also mention how FV paper mentions that some FV heads have induction like attention patterns too.}

We present a comparative analysis of two mechanisms proposed to explain ICL: induction heads \citep{elhage2021mathematical, ih} and FV heads \citep{fv, tv}.

\subsection{Induction heads}

Induction heads were first identified by \citet{elhage2021mathematical} and extensively studied by \citet{ih} as the mechanism behind ICL. They are attention heads that operate by identifying repeated patterns in the input: when processing a token, they attend to the token that followed a previous occurrence of the same token, predicting it will appear next.
% Given an input string that contains the pattern ``...AB...AB...A'', we call \textbf{induction score} the sum of attention mass on all tokens B that immediately comes after earlier copies of the current token A.

The initial evidence for induction heads' role in ICL came from \citet{ih}, who studied small attention-only models (1-3 layers). They observed that the emergence of induction heads during training coincided with improvements in ICL ability -- measured as the difference between the loss at the 500th versus 50th token in the context. Their ablation studies showed that removing induction heads impaired this metric.
% We also study training dynamics and ablations in our analysis, however we focus on LLMs and natural language ICL tasks.

To identify and analyze induction heads, we measure their \textbf{induction scores} using the TransformerLens framework \citep{tflens}. For each attention head $a$, we compute its induction score on a synthetic sequence constructed by repeating a uniformly sampled random token sequence: $r = r_1r_2...r_{50}r'_1r'_2...r'_{50}$. The induction score is defined as:
$$
S_I(a, r) = \sum_{i=1}^{50} a_{r'_i \rightarrow r_{i+1}}
$$
where $a_{r'_i \rightarrow r_{i+1}}$ represents the attention weight that head $a$ places on token $r_{i+1}$ when processing token $r'_i$. For each attention head in each model, we take the mean induction score over 1000 samples of random sequences $r$, normalized by total attention mass to obtain a value between 0 and 1. 

\subsection{FV heads}

Function vectors (FV) were concurrently discovered by \citet{fv} and \citet{tv}. They represent a different mechanism for ICL: FVs are compact vector representations of ICL tasks that can be extracted from specific attention heads and added back into the language model's computations to reproduce ICL behavior.
We refer to the attention heads that encode and transport these function vectors as \textbf{FV heads}.

% We adopt the definition from \citep{fv} in our study as it allows us to study the attention heads that transport FVs. 

% \citep{fv} similarly compute the induction score of each head in GPT-J, and finds that 3 out of 10 FV heads are also induction heads. They suggest that FVs are formed by a combination of induction heads and other attention heads that also contribute relevant information.

To identify FV heads, we employ the casual mediation analysis framework from \citet{fv}. For each ICL task $t$ in our task set $\mathcal{T}$, where $t$ is defined by a dataset $P_t$ of in-context prompts $p_i^t \in P_t$ consisting of input-output pairs $(x_i,y_i)$, we:
\begin{enumerate}
    \item Compute the mean activation of an attention head $a$ over prompts in $P_t$:  $\bar{a}^t = \frac{1}{P_t}\sum_{p_i^t \in P_t}a(p_i^t)$
    \item Create corrupted ICL prompts $\tilde{p}_i^t \in \tilde{P}_t$ by randomly shuffling the output labels $\tilde{y_i}$ while maintaining the same inputs $x_i$
    \item Measure each head's \textbf{function vector score} (FV score) as its causal contribution to recovering the correct output $y$ for the input $x$ given corrupted examples $(x_i, \tilde{y_i})$ when its activation pattern is replaced with the mean task-conditioned activation $\bar{a}^t$:
    $$
S_{FV}(a | \tilde{p}_i^t) = f(\tilde{p}_i^t | a := \bar{a}^t)[y] - f(\tilde{p}_i^t)[y].
$$
\end{enumerate} 

% While running the model, we replace the activation of an attention head $a$ with the mean task-conditioned activation $\bar{a}^t$, and measure its \textbf{function vector score} (FV score) $S_{FV}$ as its causal indirect effect towards recovering the correct answer $y$ for the input $x$ given corrupted examples $(x_i, \tilde{y_i})$:
% $$
% S_{FV}(a | \tilde{p}_i^t) = f(\tilde{p}_i^t | a := \bar{a}^t)[y] - f(\tilde{p}_i^t)[y].
% $$

For each attention head, we take the mean FV score across 37 natural language ICL tasks from \citep{fv} (Appendix \ref{app:tasks}), using 100 prompts per task. Each prompt contains 10 input-output demonstration pairs followed by a single test instance.

% \citep{ih} also performs ablations on small models \ky{TODO say which}, by either performing a full ablation that replaces an attention head's result vector with a zero vector, or by performing a pattern-preserving ablation that removes the ablated head's contribution to output vectors but preserves its contribution to attention patterns. We, on the other hand, perform mean ablation which is better (\ky{TODO citep work that mean is better than zero ablate and briefly say why}). We also perform ablations on a pre-selected group of heads, rather than ablating all heads one at a time, and do not use pattern-preserving ablation, which greatly reduces computational cost and enables ablation analysis on large models.

\subsection{Reconciling divergent findings}

While both mechanisms have been proposed by their respective works as the mechanism behind ICL, our side-by-side analysis of induction and FV heads reveals that FV heads seem to primarily contribute to ICL performance. 
We believe that the main reason for the divergence between our result and previous work lies in several intuitively related concepts in the literature that are assumed to be the same. 

First, there is an important distinction between two different conceptualizations of ICL:
\begin{itemize}
    \item On one hand, ICL is often used synonymously with few-shot learning from the prompt without parameter updates \citep{brown2020language, dong2022survey, wei2023larger}. We adopt this conceptualization of ICL in this paper since it is the most standard in the literature, and for clarity, we will use ``in-context learning'' in this paper with this definition. 
    \item On the other hand, ICL performance is measured in \citet{ih} by computing the difference between the model loss of the 500th token in the context and the loss of the 50th token. This difference was previously called ``ICL score'' but we recommend adopting distinct terminology to avoid confusion, and we call this \textbf{token-loss difference}. 
\end{itemize}

Our experiments reveal that these metrics capture different phenomena: FV heads strongly influence few-shot ICL accuracy but not token-loss difference, while induction heads show the opposite pattern (\S\ref{sec:abl}). This divergence of token-loss difference from few-shot ICL performance accounts for much of the apparent contradiction with previous findings.

Second, we find that induction heads and FV heads are correlated (\S\ref{sec:overlap}), a confound not previously controlled for. Initial ablation studies, including ours (\S\ref{sec:abl}), showed that removing induction heads significantly degrades few-shot ICL accuracy \citep{crosbie2024induction, bansal-etal-2023-rethinking}. However, when we control for this correlation by only ablating induction heads with low FV scores, their impact becomes comparable to random ablation. In contrast, ablating FV heads with low induction scores still significantly degrades ICL performance. This suggests that previous studies may have attributed to induction heads effects that actually stemmed from FV-like behavior in a subset of induction heads.

Third, scale matters. Previous work establishing induction heads as the key ICL mechanism \citep{ih, singh2024needs} focused on small models to enable detailed mechanistic analysis. However, we find that the relative importance of FV heads increases with model scale. In our smallest model (70M parameters), induction and FV heads have similar causal effects on few-shot ICL (\S\ref{app:abl}), but this does not hold true in larger models, which highlights the importance of studying these phenomena across different model scales.

% We find in Figure \ref{fig:abl} that their definition of ICL based on token-loss difference gives very different results than when we measure ICL as few-shot task performance. This suggests that induction heads may play an important role in decreasing loss at later tokens of the context, but our results show that this metric does not measure the same behavior as few-shot ICL task accuracy.

% First, \citep{ih} base most of their analysis on small Transformer models with 1 to 6 layers, with limited experiments on full-scale models with parameter count comparable to the models we study. Our analysis has shown that in smaller models, induction heads and FV heads do have comparable influence on ICL performance when we ablate them (Figure \ref{fig:abl_excl_all}), however, in all models larger than 345M parameters, ablating FV heads has a significantly larger drop to ICL performance than ablating induction heads. 


% Finally, when studying induction heads and FV heads in isolation to one another, this does not take into account their overlap. We find that once we only ablate induction heads that are not FV heads, their influence on ICL task accuracy is close to random. Therefore, certain induction heads may play an important role in ICL task performance to the extent that they are also FV heads.



\begin{table*}[htp]
    \centering
    \caption{Models studied in this work. We use huggingface implementations \citep{huggingface} for all models. We report the number of parameters, number of layers $|L|$, total number of attention heads $|a|$, and the dimension of each head dim$_a$ for each model.}
    \vspace{10pt}
      \resizebox{\linewidth}{!}{
    \begin{tabular}{llcccc}
    \toprule
     \textbf{Model} & \textbf{Huggingface ID}  & \textbf{Parameters} & $|L|$ & $|a|$ & dim$_a$\\
     \midrule
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-70m-deduped}& 70M& 6&  48& 64\\
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-160m-deduped}& 160M& 12&  144& 64\\
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-410m-deduped}& 410M& 24&  384& 64\\
Pythia \citep{pythia}&\texttt{EleutherAI/pythia-1b-deduped}& 1B& 16&  128& 256\\   
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-1.4b-deduped}& 1.4B& 24&  384& 128\\
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-2.8b-deduped}& 2.8B& 32&  1024& 80\\
Pythia \citep{pythia}& \texttt{EleutherAI/pythia-6.9b-deduped}& 6.9B& 32&  1024& 128\\
\midrule
GPT-2 \citep{gpt2}& \texttt{openai-community/gpt2}& 117M& 12&  144& 64\\
GPT-2 \citep{gpt2}& \texttt{openai-community/gpt2-medium}& 345M& 24&  384& 64\\
GPT-2 \citep{gpt2}& \texttt{openai-community/gpt2-large}& 774M& 36&  720& 64\\
GPT-2 \citep{gpt2}& \texttt{openai-community/gpt2-xl}& 1.6B& 48&  1200& 64\\
\midrule
Llama 2 \citep{llama}& \texttt{meta-llama/Llama-2-7b-hf}& 7B& 32&  1024& 128\\
\bottomrule
    \end{tabular}}
    \label{tab:models}
\end{table*}



\section{Induction heads and function vector heads are distinct but correlated}\label{sec:overlap}


Before analyzing the relative contributions of induction and FV heads to ICL performance, we first establish that these represent distinct mechanisms, while noting important correlations between them.


\subsection{Head locations}
We begin by examining the location of the top induction and FV heads within the models. Figure \ref{fig:head_layers} shows the layers where the top 2\%\footnote{In certain cases, we need to differentiate between meaningful induction / FV heads and the long tail of attention heads that perform neither induction nor the FV mechanism. We choose the top 2\% induction and FV heads as the representative set of induction and FV heads, following \citet{fv}.} induction heads and FV heads appear in three representative Pythia models (see Appendix \ref{app:layers} for all 12 models).

In general, induction heads appear in early-middle layers and FV heads appear in slightly deeper layers than induction heads. This suggests that induction and FV heads do not fully overlap and are indeed distinct mechanisms. Moreover, the deeper locations of FV heads may indicate they implement more abstract computations than induction heads, though this interpretation remains speculative.



\subsection{Overlap between induction and FV heads}\label{sec:overlap2}

To further examine how distinct induction and FV heads are, we analyze the extent of the overlap between the two types of heads in two ways.

First, we measure direct overlap - the percentage of heads that rank in the top 2\% for both mechanisms: $100 \times \frac{|IH \cap FV|}{|IH|}$ where $IH$ and $FV$ represent the sets of top induction and FV heads respectively. The results show minimal overlap: seven of our twelve models show zero overlap, with the remaining models showing only 5-15\% overlap (Figure \ref{fig:head_percentiles} left). This leads us to conclude that \textbf{induction heads and FV heads are mostly distinct} and justifies studying them as two different mechanisms.

However, a more nuanced pattern emerges when we compute the percentile of the induction score of the top 2\% FV heads (Figure \ref{fig:head_percentiles} center) and the percentile of the FV score of the top 2\% induction heads (Figure \ref{fig:head_percentiles} right). In most models, FV heads are at around the 90-95th percentile of induction scores, and vice versa. Therefore, although there is little overlap between the sets of induction and FV heads, \textbf{induction and FV scores are correlated}: FV heads have high induction scores relative to other attention heads, and induction heads have relatively high FV scores
\footnote{In our main analysis, we do not rely on the correlation between the distribution of induction scores and FV scores across the full set of attention heads because there is a long tail of attention heads with low scores on both induction and FV. For completeness, we plot the induction and FV scores of all heads in Appendix \ref{app:corr}.}.

\section{Function vector heads drive in-context learning}\label{sec:abl}

Having established that induction and FV heads represent distinct mechanisms, we now investigate their relative causal importance for ICL through systematic ablation studies. Our analysis focuses primarily on few-shot ICL accuracy while also examining effects on token-loss difference for comparison with previous work.

\subsection{Method}

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/head_layers-crop.pdf}
  \caption{Location of induction heads (blue) and FV heads (pink) in model layers.
The average layer of induction and FV heads are shown in blue and pink dotted lines respectively. Most induction heads appear in early-middle layers, FV heads appear at layers slightly deeper than induction heads.}
  \label{fig:head_layers}
\end{figure*}
\begin{figure*}
\centering
    \includegraphics[width=0.95\linewidth]{figures/head_percentiles-crop.pdf}
  \caption{Percentage of head overlap between induction and FV heads (left) in green, and between induction and randomly sampled heads in gray. Percentile of induction score of FV heads (center). Percentile of FV score of induction heads (right). There is little overlap between induction and FV heads, but FV heads have relatively high induction scores and vice versa.}
  \label{fig:head_percentiles}
\end{figure*}


% \subsubsection*{Ablation}
\textbf{Ablation.} To assess the causal contribution of different attention heads, we measure how ICL performance changes when specific heads are disabled. We use mean ablation, where we replace each target head's output with its average output across our task dataset (described in later sections). This approach avoids the out-of-distribution effects associated with zero ablation \citep{hase2021out,wang2023interpretability,zhang2024towards}, though our findings remain robust across different ablation methods (Appendix \ref{app:zero_ablate}).

To control for the correlation between induction and FV heads identified in Section \ref{sec:overlap}, we introduce ablation with ``exclusion'': when ablating $n$ FV heads, we select the top $n$ heads by FV score that are not in the top 2\% by induction score, and vice versa. This helps isolate the unique contributions of each mechanism.



% Since FV and induction scores are correlated (\S\ref{sec:overlap}), there may be some overlap between the heads we ablate for induction and FV mechanisms, especially in larger ablation quantities. This may obfuscate the contributions of the two types of heads. We therefore also perform ablation experiments with ``exclusion'': when ablating $n$ FV heads, we take the top $n$ heads by FV score that do not appear in the top $2\%$ heads by induction score. Similarly, when ablating $n$ induction heads, we take the top $n$ heads by induction score that do not appear in the top $2\%$ heads by FV score.


\textbf{Few-shot ICL accuracy.}
Our primary metric evaluates ICL performance on a series of few-shot ICL tasks. Each ICL task is defined by a set of input-output pairs $(x_i, y_i)$. The model is prompted with 10 input-output exemplar pairs that demonstrate this task, and one query input $x_q$ that corresponds to a target output $y_q$ that is not part of the model's prompt. We compute the model's accuracy in predicting the correct output $y_q$. We summarize the full set of ICL tasks we study in Appendix \ref{app:tasks}.
% While different definitions and setups for ICL exist in the literature, we focus on the setup described in \citep{brown2020language} that conceptualizes ICL as few-shot learning with no parameter updates, where the model is prompted with several demonstration examples of a task. We focus on this setup since this definition is the most widely adopted in current literature \citep{dong2022survey, wei2023larger, pmlr-v202-von-oswald23a, min-etal-2022-metaicl} and it more directly reflects how users leverage ICL in LLMs in practice. To achieve more generalized insights into ICL, we include a range of different ICL tasks in our study (Appendix \ref{app:tasks}). 

To avoid leakage between ICL tasks used to identify FV heads and those used to evaluate FV head ablations, we randomly split the 37 ICL tasks from \citet{fv} into 26 tasks used to measure FV scores of heads, and 11 tasks to evaluate ICL performance. We also add 8 new tasks for ICL evaluation: 4 tasks are variations of tasks in \citet{fv}, and 4 are binding tasks from \citet{binding}. In total, we evaluate ICL accuracy on 19 natural language tasks, with 100 prompts per task.

\textbf{Token-loss difference.}
To compare with previous work, we also study the effect of ablations on token-loss difference used in \citet{ih}. We measure token-loss difference by taking the loss of the 50th token in the context minus the loss of the 500th token in the context\footnote{We invert the difference used in \citet{ih} so higher scores indicate better ICL performance.}, averaged over 10,000 examples from the Pile dataset \citep{pile}.  

% \footnote{The number of examples taken for each task is limited by the computational cost of running multiple ablations. However, the small data size is mitigated by repeating the experiment on 37 tasks and 12 models, }.

% \begin{figure}
% \captionsetup[subfigure]{justification=Centering}
% \begin{subfigure}[t]{\textwidth}
%     \includegraphics[width=\linewidth]{figures/abl.pdf}
%     % \caption{}
%     \label{fig:for-asl-freq}
% \end{subfigure}
% \medskip % more vertical separation
% \begin{subfigure}[t]{\textwidth}
%     \includegraphics[width=\linewidth]{figures/abl_excl.pdf}
%     % \caption{}    
%     \label{fig:conf}
% \end{subfigure}
% \caption{}
% \label{fig:ablations}
% \end{figure}



% \begin{figure}
% \centering
%     \includegraphics[width=\linewidth]{figures/abl_excl.pdf}
%   \caption{TODO}
%   \label{fig:abl_excl}
% \end{figure}





\subsection{Results}

We evaluate the impact of ablating different proportions (1-20\%) of the top attention heads based on induction or FV score, across all models. We compare against two baselines: model performance with no ablation (``clean'') and with ablations of randomly sampled heads (``random'').  Figure \ref{fig:abl} shows results for three representative models, where few-shot ICL accuracy is averaged over the 19 evaluation tasks. We provide comprehensive results across all models and ICL accuracy broken down by task in Appendix \ref{app:abl_task}.

Our initial ablation experiments, shown in the top row of Figure \ref{fig:abl}, removed heads based on their scores without considering the potential overlap between induction and FV heads. These results revealed that ablating FV heads caused greater degradation in few-shot ICL performance compared to ablating induction heads, with this disparity becoming more pronounced in larger models. We also find that ablating induction heads has more effect on ICL performance than random. The effect of ablating induction heads converges to the effect of ablating FV heads as we increase the number of heads ablated.


\begin{figure*}
\centering
\vspace{-30pt}
    \includegraphics[width=\linewidth]{figures/abl_6.pdf}
    \vspace{-20pt}
  \caption{Top: Few-shot ICL accuracy after ablating induction and FV heads. Center: Few-shot ICL accuracy after ablating non-FV induction and non-induction FV heads. Bottom: Token-loss difference after ablating non-FV induction and non-induction FV heads. Ablating FV heads lead to a bigger drop in ICL accuracy, especially in larger models. Ablating induction heads with low FV scores does not significantly affect ICL accuracy. ICL accuracy and token-loss difference behave differently.}
  \label{fig:abl}
\end{figure*}

% To address the possibility that this convergence stemmed from increasing overlap between ablated head sets (\S\ref{app:overlap}), we conducted a second set of experiments using exclusion-based ablation, shown in the center row of Figure \ref{fig:abl}. When ablating induction heads while preserving the top 2\% FV heads, we observed minimal impact on few-shot ICL performance - comparable to random ablations in models exceeding 1B parameters. Conversely, ablating FV heads while preserving induction heads continued to significantly impair ICL performance. This performance gap between FV and induction head ablations widened with model scale, suggesting that the earlier observed effects were primarily driven by heads exhibiting both induction and FV properties.


However, the convergence noted above may be due to an increasing overlap in the set of heads ablated in the induction head and FV head ablations (Appendix \ref{app:overlap}). To address this, we conduct a second set of experiments using ablation with \textit{exclusion}, shown in the center row of Figure \ref{fig:abl}. When ablating induction heads while preserving the top 2\% FV heads, we observe minimal impact on few-shot ICL performance -- comparable to random ablations in models exceeding 1B parameters. Conversely, ablating FV heads while preserving induction heads continues to significantly impair ICL performance. The performance gap between FV and induction head ablations widens with model scale, suggesting that the earlier observed effects on induction head ablations were primarily driven by heads exhibiting both induction and FV properties.

These ablations suggest that the contributions of induction heads to ICL in the top row of Figure \ref{fig:abl} mostly come from heads that are both induction and FV heads, and that \textbf{FV heads matter the most for few-shot ICL}: as long as the model preserves its top 2\% FV heads, it can perform ICL with reasonable accuracy even if we ablate induction heads. 

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{figures/icl_scores.pdf}
%   \caption{Token-loss difference after ablating non-FV induction and non-induction FV heads. \ky{TODO merge with above fig}}
%   \label{fig:icl_scores}
% \end{figure}


The bottom row of Figure \ref{fig:abl} presents the effects of ablations with exclusion on token-loss difference. In smaller models (below 160M parameters), neither ablating induction nor FV heads shows significant impact to token-loss difference compared to random ablations. However, in models with over 345M parameters, induction head ablations affect token-loss difference more than FV head ablations, though this disparity decreases with model scale. This experiment primarily demonstrates that few-shot ICL accuracy and token-loss difference measure two very different things. These contrasting results between the two metrics help reconcile apparently contradictory findings in existing literature.


% \ky{Add something like we find that the results using the two definitions for ICL are not the same, but also using few-shot ICL def gives a clearer trend that FV gives more influence than induction heads}
% We interpret that as long as the model has the top 2% FV heads, it can perform ICL even if we knock out induction heads. But even if the model keeps the top 2% induction heads, it cannot perform ICL without FV heads (ICL score degrades once we ablate FV heads even if the model still has induction heads).




% To do so, we use the set of 40 ICL tasks from \citep{FV} $\mathcal{T}$, where each ICL task $t\in\mathcal{T}$ defined by a dataset $P_t$ of in-context prompts $p_i^t \in P_t$, and each prompt $p_i^t$ is composed of 10 input-output demonstration pairs $(x_i, y_i)$ as well as one query input $x_{iq}$ to test whether the model predicts the correct output for the query $y_{iq}$. 
% Then, for each task $t$, we compute the mean output of an attention head $a$ for $t$: $$\bar{a}^t = \frac{1}{P_t}\sum_{p_i^t \in P_t}a(p_i^t)$$




\section{FV heads evolve from induction heads}\label{sec:ckpt}

Finally, to further understand how these two families of attention heads develop, we analyze their evolution during model training. We examine attention heads across 8 intermediate training checkpoints in 7 Pythia models.

\subsection{Induction and FV strength during training}\label{sec:evolution}
\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/mean_ckpt.pdf}
  \caption{Evolution of induction and FV score averaged over top 2\% heads across training. Induction score rises sharply, then plateaus. FV score rises slightly later and gradually increases. ICL accuracy rises around the same time as induction and gradually increases.}
  \label{fig:mean_ckpt}
\end{figure*}


To measure the general strength of induction and FV mechanisms during training, we plot the mean induction and FV scores of the top 2\% induction and FV heads at each model checkpoint, along with few-shot ICL accuracy (Figure \ref{fig:mean_ckpt}). We include plots for all Pythia models in Appendix \ref{app:train_scores}. 


Our analysis reveals a consistent pattern across all Pythia models: induction heads emerge early in training, at around step 1,000 out of 143,000, while FV heads appear substantially later at around step 16,000. The development of these heads shows distinct characteristics as well -- induction scores exhibit a sharp initial rise followed by a plateau or slight decline, whereas FV scores demonstrate a gradual but sustained increase from step 16,000 through the end of training. This temporal asymmetry suggests that induction heads represent a simpler mechanism that models can acquire earlier, while FV heads embody a more complex mechanism that requires extended training. In addition, we observe that in all models, few-shot ICL accuracy begins to improve around the same time as when induction heads appear, and continues to gradually increase throughout training. 



\subsection{Evolution of individual heads during training}
\begin{figure*}
\centering
    \includegraphics[width=0.8\linewidth]{figures/indv_ckpt.pdf}
  \caption{Evolution of induction scores (top) and FV scores (bottom) of individual induction and FV heads across training. Certain FV heads have a high induction score earlier in training; the reverse is not true for induction heads.}
  \label{fig:indv_ckpt}
\end{figure*}

To gain more granular insights into head development, we investigate the evolution of individual attention heads throughout training. Figure \ref{fig:indv_ckpt} the induction scores (top row) and FV scores (bottom row) of the top 2\% induction and FV heads across training steps. Individual heads are represented by continuous lines, with line opacity corresponding to their final induction or FV scores.

A striking pattern emerges across all models: many heads that ultimately become strong FV heads initially exhibit high induction scores, emerging around the same time as dedicated induction heads. These proto-FV heads initially achieve induction scores comparable to those of specialized induction heads. However, as training progresses, their induction scores gradually decline while their FV scores increase. Importantly, this pattern is unidirectional; we found no instances of induction heads that develop significant FV capabilities during training, as evidenced by their consistently low FV scores throughout training. This suggests \textbf{many FV heads evolve from induction heads during training}, but not vice versa.


% In each model, we find that certain FV heads have a high induction score earlier in training, at around the same time as when induction heads form. These FV heads often match the induction score of induction heads earlier in training. The FV heads then observe a decay in induction score while increasing in FV score later in training. However, the reverse does not occur: all induction heads have low FV scores throughout training. This suggests \textbf{many FV heads evolve from induction heads during training}, but not vice versa.
% We also remark that across all models, around 1-3 FV heads have FV scores considerably greater than other FV heads, which may suggest that models learn only a few strong FV heads.



\section{Interpretation and discussion}\label{sec:disc}

% Our experiments reveal interesting, and sometimes surprising, results on two distinct mechanisms for ICL and the connection between them. To summarize, induction and FV strengths are relatively stable across model size, except in models with low/high attention head dimensionality relative to parameter count, where large head dimension may predict FV strength (\S\ref{sec:ihfv}; H6). There is little overlap between the top induction heads and FV heads, however we find that induction and FV scores are correlated (\S\ref{sec:overlap}; H1-2). Ablations reveal that, especially in larger models, FV heads drive most of ICL performance and induction heads with low FV scores contribute little to ICL (\S\ref{sec:abl}; H3). We also observe that FV heads emerge later than induction heads during training, and deeper in the model architecture, suggesting that FV heads are more complex than induction heads (\S\ref{sec:ckpt}; H5). Curiously, many FV heads are among the highest in induction score earlier during training, but their induction score drops while their FV score increases later in training (H4). 

Our investigation revealed several key insights about the relationship between induction and FV heads in transformer models and their effect on ICL. While these mechanisms are distinct, they show notable correlation (\S\ref{sec:overlap}). FV heads consistently appear in deeper layers and emerge later in training compared to induction heads (\S\ref{sec:overlap},\ref{sec:evolution}). Our ablation studies demonstrated that FV heads are crucial for few-shot ICL performance, particularly in larger models, while induction heads have comparatively minimal impact (\S\ref{sec:abl}). Furthermore, we observed multiple instances of heads transitioning from induction to FV functionality during training, but never the reverse (\S\ref{sec:evolution}). We propose two working conjectures to explain these empirical findings more broadly, and consider arguments for and against them.


% To summarize our key findings, we first verified that induction heads and FV heads are distinct, but correlated (\S\ref{sec:overlap}). FV heads appear in slightly deeper layers than induction heads, and emerge later during training (\S\ref{sec:overlap},\ref{sec:evolution}). To the extent that these two types of heads are different, FV heads play a large role in few-shot ICL, especially in bigger models, whereas ablating induction heads does not decrease ICL accuracy to the same extent (\S\ref{sec:abl}). We also find several examples of induction heads that transition to FV heads during training, whereas the reverse does not occur (\S\ref{sec:evolution}).
% We propose two working conjectures to explain these findings more broadly, and consider arguments for and against them.

% \ky{TODO think more about actual mechanism for induction-turning-into-FV}


Our first conjecture (C1) posits that \textbf{induction heads are an early version of FV heads}. Under this interpretation, induction heads serve as a stepping stone for models to develop the more sophisticated FV mechanism. As FV heads emerge and prove more effective at ICL tasks, they gradually supersede the simpler induction mechanism.

Several lines of evidence support this conjecture. First, we observed multiple FV heads that initially displayed strong induction behavior before transitioning to FV functionality. The subsequent decline in induction scores suggests these heads abandon the simpler mechanism once the more effective FV capability develops. The unidirectional nature of this transition - we never observe induction heads with initially high FV scores - further supports this interpretation. Additionally, the minimal effect of ablating pure induction heads (those with low FV scores) on few-shot ICL performance suggests their role becomes less critical once FV heads develop. To further verify this, future work could explore how removing induction heads during training could impact the development of FV heads. However, C1 does not fully explain the existence of FV heads with low induction scores throughout training.

C1 also aligns with our observations about architectural and training dynamics. FV heads consistently emerge later in training and appear in deeper layers, consistent with them implementing a more complex computation. In addition, ICL performance begins to increase at the same time as the emergence of induction heads, but continues to improve after induction heads form, similarly to how FV heads gradually increase. This may indicate that the sharp emergence of induction heads contributes to an initial rise in ICL performance, but the emergence of the FV mechanisms contributes to further improvements in ICL.

% The most compelling argument for C1 is the existence of FV heads that are formerly induction heads during training. The decrease in induction score late in training suggests that induction is not needed by the model in its final form. This is reinforced by the fact that the reverse never happens (FV heads never become induction heads), and that induction heads with low FV scores contribute little to ICL under ablations. To further verify this, future work could explore how removing induction heads during training could impact the development of FV heads. However, C1 does not fully explain the existence of FV heads with low induction score throughout training.
% why some FV heads have low induction score throughout the entire training process.

% C1 also correctly predicts that FV heads are more complex and harder to learn than induction heads (they appear later in training and deeper in model architectures). In addition, ICL performance begins to increase at the same time as the emergence of induction heads, but continues to improve after induction heads form, similarly to how FV heads gradually increase. This may indicate that the sharp emergence of induction heads contributes to an initial rise in ICL performance, but the emergence of the FV mechanisms contributes to further improvements in ICL.
% This can also explain why FV heads have a stronger effect on ICL in larger models, relative to induction heads.
% However, C1 does not explain why FV strength is similar in small and large models.


An alternative conjecture (C2) suggests that \textbf{FV heads are a combination of induction and another mechanism}. This conjecture proposes that heads appearing to ``transition'' from induction to FV functionality are actually polysemantic heads that implement both, and possibly other mechanisms. Their measured induction scores decline as their attention patterns diversify to support multiple mechanisms.

% Under this conjecture, the induction heads that ``become'' FV heads are polysemantic heads that implement both induction and FVs, and possibly other mechanisms. We speculate that the induction score of these polysemantic heads drops when the attention patterns become split between the different mechanisms of the attention head. 
% This conjecture is supported by how ICL performance gradually improves after the sharp emergence of induction heads, before   -- there may be an additional mechanism between 

While C2 explains the correlation between induction and FV mechanisms as arising from shared underlying mechanisms, it faces a significant challenge: our ablation studies show that removing monosemantic FV heads (those without significant induction scores) substantially hurts ICL performance. This is difficult to reconcile with C2's prediction that pure FV heads should be less critical if the key functionality depends on combined induction-FV mechanisms.


% Under C2, induction and FV heads are correlated because they share an underlying mechanism that the model learns to re-use for both tasks during training. However, C2 would predict that ablating monosemantic FV heads would not hurt ICL performance, whereas we observe that ablating monosemantic FV heads while preserving polysemantic induction-FV heads lowers ICL accuracy.

% Our second hypothesis (H2) is that \textbf{FV heads are a combination of induction and another mechanism}. FV heads may be polysemantic heads that exhibits induction-like behavior on data with repetition patterns, and also implements a different mechanism to transport FVs. H2 would explain why induction and FV scores are correlated, and why heads with larger dimensions have stronger FV mechanism if larger heads are more able to encode both induction and FV-specific mechanisms. Under this hypothesis, prior evidence of induction heads implementing ICL may come from polysemantic induction-FV heads. This is further evidenced in \S\ref{sec:abl} where knocking out induction heads with low FV score does not impact ICL accuracy much. However, this does not explain why some FV heads have high induction scores while others do not.



\section{Conclusion}

Our research challenges the prevailing understanding of in-context learning mechanisms in transformer models. While induction heads have been widely considered the primary driver of ICL, our evidence demonstrates that function vector (FV) heads play a more crucial causal role in few-shot ICL performance. We attribute previous misconceptions to two key factors: the conflation of few-shot ICL with token-loss difference metrics, as well as not accounting for the overlap between induction and FV heads.


Remarkably, although induction and FV mechanisms appear to implement two distinct processes, we also observe an interesting interplay between the two types of heads: induction and FV scores are correlated, and many FV heads are ``former'' induction heads with high induction scores earlier in training. This observation supports an early conjecture where induction heads serve as precursors to FV heads: the simpler induction mechanism provides an initial foundation for ICL capability, from which the more sophisticated FV mechanism eventually emerges.

Our investigation also yields important methodological insights for the broader field of model interpretability. First, seemingly equivalent definitions of model capabilities (such as few-shot accuracy versus token-loss difference) can lead to substantially different conclusions. Second, studying mechanistic components in isolation may produce misleading results when these components share overlapping behaviors, as demonstrated by the confounding effects of ablating heads that exhibit both high induction and FV scores.

Furthermore, our results challenge strong versions of the universality hypothesis in interpretability. While both induction and FV heads contribute meaningfully to few-shot ICL in smaller models, their relative importance diverges with scale -- FV heads become increasingly crucial while induction heads' impact approaches that of random ablations. This scale-dependent behavior suggests that mechanisms may vary across model architectures.

These findings prompt several important questions for future research. If induction heads indeed serve as precursors to FV heads, what makes this necessary? What role do the remaining induction heads serve in fully trained models? 
% Why do FV heads strongly influence few-shot ICL while having minimal impact on token-loss difference? 
Finally, are there additional mechanisms that could provide an even more complete explanation of ICL capabilities? 
% Answering these questions could further advance our understanding of how language models develop sophisticated capabilities like ICL.

% Our results open up several avenues for future research. If our first proposed conjecture is true, why do FV heads need induction as a precursor? What are the remaining induction heads used for? Why do FV heads have a strong effect on few-shot ICL but a relatively weak effect on token-loss difference? Could there be an additional mechanism that provides an even more precise explanation for ICL?
% If model mechanisms are not universal across size or architecture, what is the best approach to generalizing findings from interpretability research?

% \section*{Reproducibility statement}
% We recognize the importance of reproducibility and have made the following efforts to ensure reproducibility of our findings. First, we detail the huggingface IDs of all the models we use, as long as the machines we conducted all experiments on, in Table \ref{tab:models}. Second, we define how we computed induction and FV scores of attention heads, including the public packages we used, in \S\ref{sec:bg}. Third, we also describe the datasets we used in \S\ref{sec:abl}. Lastly, we will release all code and data needed to reproduce all findings and figures in our work upon publication.

% \section*{Impact statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% \section*{Limitations}
% \ky{TODO write this up}

% - Study limited to models under 7b parameters, but this is already quite big in scale compared to previous analysis of ICL mechanisms

% - Our study does not really address polysemantic heads / superposition, so the way we quantify induction and FV strength in section 2 could be misleading? We don't use these results as direct evidence of our hypotheses and don't use them for the main claims of this paper

% - In many experiments we picked the top 2\% heads as the set of induction or FV heads. Thresholds are necessary to separate meaningful FV / induction heads vs. the long tail of attention heads that dont perform FV or induction. Our choice of 2\% comes from 1) the original function vectors paper from Todd et al., 2024 where they take roughly the top 2\% FV heads across different model sizes to compute function vectors; 2) studying full scatter plots of FV scores of all heads, where we confirmed that 2\% is a good representative of heads with significantly high FV scores than most heads; 3) we computed the number of attention heads with FV score or induction score above a fixed threshold across model sizes and found this number is proportional to the models head count, which justifies using a fixed percentage across model sizes, and include raw data in the appendix

\section*{Acknowledgements}
We thank Jiahai Feng, Neel Nanda, Robert Kirk, and Anish Kachinthaya for their helpful feedback. KY is supported by the Vitalik Buterin Ph.D. Fellowship in AI Existential Safety.
% Thank Neel Nanda, Jiahai, Anish, Robert Kirk, 

\bibliographystyle{Styles/icml2025}
\bibliography{main}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section*{NeurIPS Paper Checklist}

% % %%% BEGIN INSTRUCTIONS %%%
% % The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and precede the (optional) supplemental material.  The checklist does NOT count towards the page
% % limit. 

% % Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
% % \begin{itemize}
% %     \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
% %     \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
% %     \item Please provide a short (12 sentence) justification right after your answer (even for NA). 
% %    % \item {\bf The papers not including the checklist will be desk rejected.}
% % \end{itemize}

% % {\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

% % The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

% % IMPORTANT, please:
% % \begin{itemize}
% %     \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
% %     \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
% %     \item {\bf Do not modify the questions and only use the provided macros for your answers}.
% % \end{itemize} 
 

% % %%% END INSTRUCTIONS %%%


% \begin{enumerate}

% \item {\bf Claims}
%     \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The abstract and introduction make the same claims as the main body of the paper and accurately reflect the paper.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
%         \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
%         \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
%         \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
%     \end{itemize}

% \item {\bf Limitations}
%     \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper takes care in also presenting counter-arguments for the hypotheses put forward in the paper.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
%         \item The authors are encouraged to create a separate "Limitations" section in their paper.
%         \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
%         \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
%         \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
%         \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
%         \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
%         \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
%     \end{itemize}

% \item {\bf Theory Assumptions and Proofs}
%     \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper does not include theoretical results. 
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include theoretical results. 
%         \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
%         \item All assumptions should be clearly stated or referenced in the statement of any theorems.
%         \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
%         \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
%         \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
%     \end{itemize}

%     \item {\bf Experimental Result Reproducibility}
%     \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper includes sufficient information on models, data, and experimental setup for reproducibility. Code to reproduce experiments will be made available upon publication.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
%         \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
%         \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
%         \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
%         \begin{enumerate}
%             \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
%             \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
%             \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
%             \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
%         \end{enumerate}
%     \end{itemize}


% \item {\bf Open access to data and code}
%     \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: We will release code and data with sufficient instructions for reproduction after submission.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that paper does not include experiments requiring code.
%         \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
%         \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
%         \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
%         \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
%         \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
%         \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
%     \end{itemize}


% \item {\bf Experimental Setting/Details}
%     \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper includes full details on interpretability experiments and data splits.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
%         \item The full details can be provided either with the code, in appendix, or as supplemental material.
%     \end{itemize}

% \item {\bf Experiment Statistical Significance}
%     \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
%     \item[] Answer: \answerNo{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: Error bars are not reported due to the computational burden of re-running the full experiments, and we have high confidence on the main claims of the paper due to the number of tasks, examples, and models considered. 
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
%         \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
%         \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
%         \item The assumptions made should be given (e.g., Normally distributed errors).
%         \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
%         \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
%         \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
%         \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
%     \end{itemize}

% \item {\bf Experiments Compute Resources}
%     \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: We specify the GPUs we use to load models in this study and perform experiments
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not include experiments.
%         \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
%         \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
%         \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
%     \end{itemize}
    
% \item {\bf Code Of Ethics}
%     \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: This research confrom with the NeurIPS Code of Ethics.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
%         \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
%         \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
%     \end{itemize}


% \item {\bf Broader Impacts}
%     \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: Our paper provides an analysis of previously discovered mechanisms and does not contribute any new technological tools that potentially have negative societal impact. We discuss the broader impact of our findings for best practices of interpretability research in general.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that there is no societal impact of the work performed.
%         \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
%         \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
%         \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
%         \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
%         \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
%     \end{itemize}
    
% \item {\bf Safeguards}
%     \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper does not train or release new model, and new ICL data pose no risk for misuse.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper poses no such risks.
%         \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
%         \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
%         \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
%     \end{itemize}

% \item {\bf Licenses for existing assets}
%     \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
%     \item[] Answer: \answerYes{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: We citep all sources of software, data and models used in our experiments.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not use existing assets.
%         \item The authors should citep the original paper that produced the code package or dataset.
%         \item The authors should state which version of the asset is used and, if possible, include a URL.
%         \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
%         \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
%         \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
%         \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
%         \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
%     \end{itemize}

% \item {\bf New Assets}
%     \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: No new assets are introduced.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not release new assets.
%         \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
%         \item The paper should discuss whether and how consent was obtained from people whose asset is used.
%         \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
%     \end{itemize}

% \item {\bf Crowdsourcing and Research with Human Subjects}
%     \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
%         \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
%     \end{itemize}

% \item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
%     \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
%     \item[] Answer: \answerNA{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
%     \item[] Justification: The paper does not involve crowdsourcing nor research with human subjects.
%     \item[] Guidelines:
%     \begin{itemize}
%         \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
%         \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
%         \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
%         \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
%     \end{itemize}

% \end{enumerate}

% \newpage

\appendix

\section{Appendix}
\subsection{Induction scores vs. FV scores}\label{app:corr}
In Figure \ref{fig:corr}, we plot the induction score and FV score of each attention head. 

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/corr.pdf}
  \caption{Induction and FV scores of attention heads.}
  \label{fig:corr}
\end{figure*}

\subsection{Ablations}\label{app:abl}
In Figure \ref{fig:abl_all}, we plot model accuracy averaged over ICL tasks across different quantities of heads ablated in each head type. In Figure \ref{fig:icl_scores_all}, we plot the token-loss difference of models across different quantities of heads ablated.

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/abl_all.pdf}
  \caption{ICL accuracy after ablating induction and FV heads.}
  \label{fig:abl_all}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/icl_scores_all.pdf}
  \caption{Token-loss difference after ablating induction heads with low FV scores and FV heads with low induction scores.}
  \label{fig:icl_scores_all}
\end{figure*}

\subsection{Random and zero ablations}\label{app:zero_ablate}
In Figure \ref{fig:random_zero_abl}, we plot model accuracy averaged over ICL tasks across different quantities of heads ablated with random ablation or zero ablation. For random ablations, we replace the head's output vector with the output vector of a randomly sampled different head. For zero ablations. we replace the head's output vector with a zero vector.

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/random_zero_abl.pdf}
  \caption{ICL accuracy after ablating induction heads and FV heads with random or zero ablation.}
  \label{fig:random_zero_abl}
\end{figure*}

\subsection{Ablating random heads at specific layers}
In Figure \ref{fig:abl_rand}, we ablate heads randomly sampled from specific layers of the model. Let $L$ be the number of heads in each layer, $A$ be the number of heads we're ablating, and $\ell$ be the layer we're targeting. Then, if $A < L$, we sample $A$ heads from layer $\ell$. If $A \geq L$, we ablate all $L$ heads in layer $\ell$ and we sample $A - L$ heads from other layers to ablate.

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/abl_rand.pdf}
  \caption{ICL accuracy after ablating randomly sampled heads from specific layers. The clean ICL accuracy, induction ablations and FV ablations are also plotted for comparison but only the random ablations (green curve) are affected by the choice of target layer.}
  \label{fig:abl_rand}
\end{figure*}

\subsection{Induction and function vector scores across models}\label{sec:scores}



\begin{figure*}
\centering
    \includegraphics[width=0.9\linewidth]{figures/head_strength.pdf}
  \caption{Induction score (left) and FV score (right) of attention heads across model size. We plot the maximum score of all heads, mean of the top 2\% scores, and mean score of all heads. Overall, induction scores are similar across models. Pythia 70M and Llama 2 have relatively low FV scores, Pythia 1B and 1.4B have relatively high FV scores.}
  \label{fig:head_strength}
\end{figure*}

\begin{figure}[h]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ih_boxplot.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm} % Add space between subfigures
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fv_boxplot.pdf} % Replace with your image path
    \end{subfigure}
    \caption{Distribution of induction scores (top) and FV scores (bottom) across model size.}
    \label{fig:box}
\end{figure}

Our ablation studies reveal a consistent trend where FV heads are increasingly important relative to induction heads for ICL performance as model scale increases. To further explore this trend, we examine how induction scores and FV scores vary with model scale, and whether these scores follow similar trends to our ablation experiments.
% We now examine the amount of induction and FV heads in 12 transformer decoder-only autoregressive language models, listed in Table \ref{tab:models}. 
% For instance, we hypothesize that models with higher attention head dimensionality will have stronger FV heads, since the activations of these heads will have more parameters that can encode the ICL task.

In Figure \ref{fig:head_strength}, we plot the maximum and mean induction and FV scores across all heads, and mean scores of top 2\% heads, for each model. The left plot in Figure \ref{fig:head_strength} shows that induction scores are relatively similar across model size, with a small increase in maximum induction score and a decrease in the top 2\% mean induction score with model scale.

In the right plot of Figure \ref{fig:head_strength}, there is no clear trend between FV score and model scale, however, Pythia 1B and 1.4B models have markedly higher maximum FV scores. One possible explanation is that models with high head dimensionality relative to total parameter count have stronger FV heads: Pythia 1B and 1.4B have head dimensionality of 256 and 128 respectively (Table \ref{tab:models}) whereas other models with similar parameter count have only 64-80 attention head dimensions. 

We also find very low FV scores in Pythia 70M and Llama 2 models. FV scores may be low in Pythia 70M because it is too small in parameter size for FV heads to emerge. Low scores in Llama 2 compared to other models may be due to differences in architecture, and additional experiments can help confirm this. Overall, we do not recover the same trend in induction/FV scores as the trend in our ablation studies. 

For reference, we also provide box plots of the full distribution of induction and FV scores in Figure \ref{fig:box}. 

\subsection{Evaluating function vectors on task execution}\label{sec:fvtask}

To further inspect the prevalence of the FV mechanism in different models, we evaluate the efficacy of FVs for ICL task execution. A successful FV triggers the model to execute the particular task the FV encodes, even when the model sees no useful in-context demonstrations of the task. 
First, to extract FVs, for each model we gather the top 2\% attention heads with highest FV scores as the set $\mathcal{A}$. Then, for each ICL task $t\in \mathcal{T}$, we sum the average outputs of heads in $\mathcal{A}$ over prompts from $t$ and obtain the FV for the task $t$:
$
FV_t = \sum_{a\in\mathcal{A}} \bar{a}^t.
$

In Figure \ref{fig:fv_intervention}, we report model accuracy averaged over 40 ICL tasks where the model performs inference on uncorrupted prompts (clean), prompts with shuffled labels (shuffled), shuffled prompts with $FV_t$ added to hidden states at layer $|L|/3$, and shuffled prompts with FV extracted from random heads added to hidden states at layer $|L|/3$. We take 1000 examples per task that are previously unseen during FV score computation.

In most models, adding the FV recovers model performance on uncorrupted prompts, with the exception of Pythia 2.8B. One possible explanation for this is again due to head dimensionality: Pythia 2.8B has head dimension 80, which is significantly smaller than other models with similar parameter size that have head dimensions of 128. Together with our experiments in \S\ref{sec:scores}, results provide preliminary evidence that \textbf{high head dimensionality relative to model size is a predictor of FV strength (H6)}.

\begin{figure}
\centering
    \includegraphics[width=\linewidth]{figures/fv_intervention.pdf}
  \caption{Model ICL accuracy on prompts with 10 in-context examples (clean), on uninformative shuffled prompts, on shuffled prompts with FV, and on shuffled prompts with random head outputs. Adding FV recovers most of the model accuracy on a clean run, with the exception of Pythia 2.8B.}
  \label{fig:fv_intervention}
\end{figure}


\subsection{ICL tasks}\label{app:tasks}
In Table \ref{tab:task_list}, we list the ICL tasks used in this study. We refer to \citet{fv} and \citet{binding} for a detailed description of each task.
\begin{table*}[ht]
    \centering
        \caption{Summary of ICL tasks used in our study. Tasks in \textbf{bold} are new tasks that were not used in \citet{fv}.}
    \resizebox{0.6\linewidth}{!}{\begin{tabular}{ll}
        \toprule
        \textbf{Task Name} & \textbf{Task Source} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Abstractive Tasks}} \\
        \midrule
        \textbf{Abstract clf }&  \\
        Antonym & \citet{nguyen-etal-2017-distinguishing} \\
        \textbf{Binding capital} & \citet{binding}\\
        \textbf{Binding capital parallel} & \citet{binding}\\
        \textbf{Binding fruit} & \citet{binding}\\
        \textbf{Binding shape} & \citet{binding}\\
        Capitalize first letter & \citet{nguyen-etal-2017-distinguishing} \\
        \textbf{Capitalize index} & \\
        \textbf{Capitalize second letter} & \\
        Capitalize & \citet{nguyen-etal-2017-distinguishing} \\
        Country-capital & \citet{fv}\\
        Country-currency & \citet{fv}\\
        English-French & \citet{conneau2017word} \\
        English-German & \citet{conneau2017word} \\
        English-Spanish & \citet{conneau2017word} \\
        \textbf{French-English} & \citet{conneau2017word} \\
        Landmark-Country & \citet{hernandez2024linearity} \\
        Lowercase first letter & \citet{fv}\\
        National parks & \citet{fv}\\
        Next-item & \citet{fv}\\
        Previous-item & \citet{fv}\\
        Park-country &\citet{fv} \\
        Person-instrument & \citet{hernandez2024linearity} \\
        Person-occupation & \citet{hernandez2024linearity} \\
        Person-sport & \citet{hernandez2024linearity} \\
        Present-past & \citet{fv}\\
        Product-company & \citet{hernandez2024linearity} \\
        Singular-plural & \citet{fv}\\
        Synonym & \citet{nguyen-etal-2017-distinguishing} \\
        \midrule
        CommonsenseQA (MC-QA) & \citet{talmor-etal-2019-commonsenseqa} \\
        Sentiment analysis (SST-2) & \citet{socher-etal-2013-recursive} \\
        AG News & \citet{zhang2015character} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Extractive Tasks}} \\
        \midrule
        Adjective vs. verb & \citet{fv}\\
        Animal vs. object & \citet{fv}\\
        Choose first of list & \citet{fv}\\
        Choose middle of list & \citet{fv}\\
        Choose last of list & \citet{fv}\\
        Color vs. animal & \citet{fv}\\
        Concept vs. object & \citet{fv}\\
        Fruit vs. animal & \citet{fv}\\
        Object vs. concept & \citet{fv}\\
        Verb vs. adjective & \citet{fv}\\
        \midrule
        CoNLL-2003, NER-person & \citet{tjong-kim-sang-de-meulder-2003-introduction} \\
        CoNLL-2003, NER-location & \citet{tjong-kim-sang-de-meulder-2003-introduction} \\
        CoNLL-2003, NER-organization & \citet{tjong-kim-sang-de-meulder-2003-introduction} \\
        \bottomrule
    \end{tabular}}
    \label{tab:task_list}
\end{table*}

\subsection{Ablations by task}\label{app:abl_task}

In Figures \ref{fig:abl_task1}-\ref{fig:abl_task4}, we plot the ICL accuracy after ablating induction heads and FV heads for each task in the evaluation set. We also compute the random baseline for each task, where we randomly sample outputs seen during training and compare these random outputs to the ground truth. The random baselines are shown in red horizontal lines.

\begin{figure*}[h]
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-70m.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_gpt2.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-160m.pdf} % Replace with your image path
    \end{subfigure}
        \caption{ICL accuracy after ablations by task. The red horizontal line represents the random baseline.}
    \label{fig:abl_task1}
\end{figure*}
\begin{figure*}[h]
    \centering
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_gpt2-medium.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-410m.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_gpt2-large.pdf} % Replace with your image path
    \end{subfigure}
        \caption{ICL accuracy after ablations by task. The red horizontal line represents the random baseline.}
    \label{fig:abl_task2}
\end{figure*}
\begin{figure*}[h]
    \centering
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-1b.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-1.4b.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_gpt2-xl.pdf} % Replace with your image path
    \end{subfigure}
        \caption{ICL accuracy after ablations by task. The red horizontal line represents the random baseline.}
    \label{fig:abl_task3}
\end{figure*}
\begin{figure*}[h]
    \centering\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-2.8b.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_pythia-6.9b_task.pdf} % Replace with your image path
    \end{subfigure}
    \vspace{0.5cm}
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/abl_Llama-2-7b-hf.pdf} % Replace with your image path
    \end{subfigure}
    \caption{ICL accuracy after ablations by task. The red horizontal line represents the random baseline.}
    \label{fig:abl_task4}
\end{figure*}

\subsection{Head locations}\label{app:layers}
In Figure \ref{fig:head_layers_all}, we plot the locations of induction heads and FV heads across model layers.

\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/head_layers_all.pdf}
  \caption{Location of induction heads (blue) and FV heads (pink) in model layers}
  \label{fig:head_layers_all}
\end{figure*}


\subsection{Overlap between ablated induction and FV heads}\label{app:overlap}
In Figure \ref{fig:abl_overlap}, we plot the percentage of attention heads that overlap between the set of induction heads and FV heads we ablate. We find that as the number of ablated heads increases, the overlap between the two sets of ablated heads also increases. This demonstrates the importance of performing ablations with exclusion to control for overlap.
\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/abl_overlap.pdf}
  \caption{Overlap between set of induction heads and FV heads ablated.}
  \label{fig:abl_overlap}
\end{figure*}






\subsection{Scores across training}\label{app:train_scores}
In Figure \ref{fig:mean_ckpt_all}, we plot the evolution of induction and FV scores averaged over top 2\% heads across model training, along with the few-shot accuracy of the model checkpoints averaged over the evaluation tasks.
In Figure \ref{fig:indv_ckpt_all}, we plot the evolution of induction and FV scores of individual heads across training.
\begin{figure*}
\centering
    \includegraphics[width=\linewidth]{figures/mean_ckpt_all.pdf}
  \caption{Evolution of induction score and FV score averaged over top 2\% heads across training.}
  \label{fig:mean_ckpt_all}
\end{figure*}

\begin{figure*}
\centering
    \includegraphics[width=0.75\linewidth]{figures/indv_ckpt_all.pdf}
  \caption{Evolution of induction scores (left) and FV scores (right) of individual induction and FV
heads across training}
  \label{fig:indv_ckpt_all}
\end{figure*}



\end{document}