% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsfonts,amssymb}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{booktabs,siunitx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{0307}{.} % Define the unknown character
\usepackage{microtype}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{siunitx}
\PassOptionsToPackage{table}{xcolor}
\usepackage{xcolor}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{FLanTS-EHR: Enhancing Fusion of Language and Time-Series in EHRs via Self-Caption of Time-Series Anomalies}

\title{
% Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion
% Multimodal Fusion of Language and Time Series in EHRs via Self-Supervised Prompt Learning
Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion
}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Shuai Niu$^1$, Jing Ma$^1$, Hongzhan Lin$^1$, Liang Bai$^2$, Zhihua Wang$^3$, Wei Bi$^5$, Yida Xu$^1$, Guo Li$^4$, and Xian Yang$^6$\thanks{* This is the corresponding author.}
    \\
    Hong Kong Baptist University$^1$, Shanxi University$^2$, \\
    Shanghai Institute for Advanced Study of Zhejiang University$^3$,
    Manchester Metropolitan University$^4$,\\
    Tencent AI Lab$^5$,
    The University of Manchester$^6$\\
    \texttt{\{cssniu,majing, cshzlin\}@comp.hkbu.edu.hk,  sxbailiang@126.com}, zhihua.wang@zju.edu.cn,\\
    \texttt{victoriabi@tencent.com, xuyida@hkbu.edu.hk, l.guo@mmu.ac.uk, xian.yang@manchester.ac.uk} \\
    % \thanks{* This is the corresponding author.}\\
}

\begin{document}
\maketitle
\begin{abstract} Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches. \end{abstract}
% ====
% \begin{abstract}
% Multimodal large language models (LLMs) have shown impressive capabilities in vision-language tasks but remain underexplored in integrating time series data. This gap arises from the inherent differences between discrete textual data and continuous time series data, which challenge alignment in a shared representation space. To address this, we propose a novel self-supervised multimodal learning  framework that bridges modality gap by leveraging anomaly captions—textual description generated from time series data through a lightweight anomaly detection process—to guide the learning of time series prompts. These prompts encode time series data into a shared latent space with textual representations, enabling seamless modality alignment and integration.
% Importantly, while anomaly captions provide semantic guidance, the time series prompts also retain the rich and detailed temporal information, capturing patterns and dynamics that extend beyond the scope of simple anomaly descriptions. Our framework incorporates tailored self-supervised loss functions to align and fuse intra- and inter-modalities, ensuring the preservation of fine-grained time series details. Extensive evaluations on the MIMIC-III and MIMIC-IV datasets for disease diagnosis demonstrate ProMedTS outperforms state-of-the-art baseline models, establishing a new benchmark for multimodal alignment with LLMs and broadening their applications in time series–rich domains.



% \end{abstract}


\begin{figure}[t] \centering \includegraphics[scale=0.23]{Figs/Intro_new.pdf} \caption{(a) LLMs struggle to process continuous time series data due to modality gaps with discrete textual representations. (b) ProMedTS bridges this gap by leveraging anomaly descriptions and time series prompts, aligning structured EHR data with clinical notes for improved multimodal understanding.} \label{Intro} \end{figure}

\section{Introduction}

Recent advancements in natural language processing (NLP) have revolutionized healthcare by enabling deeper insights into electronic health records (EHRs). EHRs combine structured data, such as time series laboratory (lab) test results, with unstructured data, including clinical notes and medical images. While large language models (LLMs) excel at processing unstructured text \cite{nori2023capabilities,singhal2023large} and vision transformers have driven progress in medical image analysis \cite{wang2022medclip,chen2021transunet}, integrating continuous time series data with text remains a challenge. Unlike text, which is composed of discrete tokens, time series data contain continuous signals with temporal dependencies \cite{jintime} as illustrated in Figure~\ref{Intro}(a).

Current multimodal learning approaches, especially contrastive learning methods \cite{radford2021learning,li2023blip2}, have been effective in aligning vision and language. However, they are less suited to bridge the gap between time series and text. Time series data require fine-grained temporal representations in a high-dimensional space and are often irregularly sampled, exhibit diverse frequencies, and include missing values \cite{harutyunyan2019multitask}. In addition, the lack of large-scale paired datasets that link raw time series with textual descriptions further hampers LLMs from incorporating structured information into clinical decision-making \cite{niu2024ehr}. Without an effective fusion mechanism, LLMs cannot fully exploit the rich temporal patterns  in structured EHR data.

To address these challenges, we propose ProMedTS, a self-supervised and prompt-guided framework designed to unify medical notes and time series lab test for naturally understood by LLMs. As shown in Figure~\ref{Intro}(b), instead of feeding raw time series directly into LLMs, our framework uses \emph{anomaly descriptions}, capturing key patterns in lab test results for helping multimodal fusion between text and time series data. These descriptions are generated using lightweight anomaly detection techniques \cite{vinutha2018detection} and convert continuous signals into human-readable summaries. The process involves two steps. First, anomaly descriptions establish a direct connection between time series EHRs and medical notes. Second, time series prompt embeddings are generated and added as prefix tokens to the LLM input. This method integrates structured time series information into the language modeling process without altering the LLM architecture, unifying both modalities within a same encoding space and enhancing clinical decision-making.

We optimize ProMedTS with three self-supervised learning objectives. A contrastive loss maps textual and time series modalities into a shared latent space. An anomaly-time series matching loss links lab test with their corresponding anomaly descriptions to reinforce consistency. {Finally, an anomaly caption generation loss improves the fine-grained alignment between numeric time series lab test and time series prompt embeddings.} Together, these objectives enable LLMs to process both structured and unstructured EHR data more effectively, addressing the gap between language and time series representations in healthcare applications.

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt] \item We propose ProMedTS, a self-supervised framework that integrates structured time series and unstructured textual EHR data into LLMs without changing their architectures. \item We introduce anomaly descriptions as a textual bridge to align time series data with clinical notes, supported by three self-supervised objectives. \item We demonstrate that ProMedTS significantly improves disease diagnosis on MIMIC-III and MIMIC-IV, setting a new benchmark for multimodal EHR learning. \end{itemize}




% ======

% The rapid advancement of natural language processing (NLP) techniques has significantly transformed healthcare, particularly in the analysis of Electronic Health Records (EHRs), which comprise structured data (e.g., time series lab test results) and unstructured data (e.g., medical notes, medical images). \textcolor{black}{Large language models (LLMs) have demonstrated exceptional capabilities in understanding unstructured textual data \cite{nori2023capabilities,singhal2023large}. Vision transformers (ViTs) are widely employed for medical image analysis \cite{wang2022medclip,chen2021transunet}, ushering in the era of large vision-language models \cite{li2023blip2,liu2023visual}. However, for time series data, LLMs are still in their early stages \cite{jintime}, and integrating time series data with textual data remains a largely underexplored domain.}


% Multimodal learning, leveraging contrastive learning paradigms \cite{radford2021learning,li2023blip2}, has achieved significant success in vision-language tasks by aligning visual features with textual representations using large-scale paired datasets. However, the continuous and dynamic nature of time series data introduces unique challenges. \textcolor{black}{Unlike discrete tokens used by LLMs, time series data require capturing intricate temporal variations in a continuous encoding space, posing significant challenges for LLMs understanding and alignment within a shared latent space.} Additionally, time series data often exhibit diversity, irregularity, and redundancy, further complicating their representation. Combined with the absence of large-scale paired datasets for time series and textual data, these challenges highlight the gap in time series-language fusion methods compared to the advances in vision-language alignment achieved by models like CLIP \cite{radford2021learning} and BLIP \cite{li2022blip}.

% To address these challenges, we propose ProMedTS, a novel framework that integrates language and time series EHRs through self-supervised prompt learning. As illustrated in Figure~\ref{Intro}, \textcolor{black}{ProMedTS leverages discrete anomaly descriptions (textual summaries generated via lightweight anomaly detection on lab test results) as a modality bridge, providing semantic guidance to assist in generating time series prompts and aligning time series and textual data within a shared latent space.} Importantly, anomaly descriptions are not replacements for time series data but rather serve as auxiliary context to guide the alignment and fusion process of modality semantic information while preserving the rich and detailed temporal information inherent in lab test. This ensures that ProMedTS captures intricate patterns and dynamics beyond what is encapsulated in anomaly summaries.

% \begin{figure}[t]
% \centering
% \centerline{\includegraphics[scale=0.22 ]{Figs/Intro_new.pdf}}
% \caption{ Challenges and solutions for language–time series multimodal learning with LLMs in EHRs. 
% }
% \label{Intro}
% \end{figure}
% Our framework employs three self-supervised loss functions to ensure effective modality alignment and time series prompt embedding learning. First, a language–time series contrastive learning loss aligns textual modalities (i.e., anomaly descriptions and medical notes) with time series lab test results, bridging the modality gap. Second, an anomaly-time series matching loss enhances intra-modality alignment by linking lab test results with their corresponding anomaly descriptions, preserving the temporal nuances of time series. Finally, an anomaly caption generation loss facilitates the learning of time series prompts, ensuring seamless integration with textual data for straightforward understanding by LLMs.
% Through these components, ProMedTS effectively bridges the modality gap between language and time series data while preserving essential temporal information, endowing LLMs with the capability to comprehend language-time series EHRs for enhanced performance in downstream healthcare tasks. 

% The primary contributions of this study are as follows:

% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]

% \item We propose ProMedTS, a novel framework that integrates language and time series EHRs through self-supervised prompt learning, enabling seamless modality alignment without altering LLM architectures to enhance their downstream healthcare performance.

% \item We introduce anomaly descriptions derived from lightweight anomaly detection and design three self-supervised loss functions to ensure effective modality alignment, enabling robust time series prompt embedding learning while preserving temporal details.


% \item We demonstrate that ProMedTS significantly outperforms state-of-the-art models in disease diagnosis tasks on the MIMIC-III \cite{johnson2016mimic} and MIMIC-IV \cite{johnson2023mimic}  datasets, establishing a new benchmark for multimodal alignment in LLMs for healthcare applications.

% \end{itemize}


\section{Related Work}

The increasing diversity of EHR data has led to significant advancements in multimodal learning for healthcare applications. MedCLIP \cite{wang2022medclip} employs semantic contrastive learning to align medical images with textual reports, while RAIM \cite{qiao2019mnn} and GLoRIA \cite{huang2021gloria} integrate numerical and image data with text using attention mechanisms. LDAM \cite{niu2021label} further extends these approaches by leveraging cross-attention with disease labels to fuse features from lab tests and clinical notes. EHR-KnowGen \cite{niu2024ehr} transforms structured lab data into text and incorporates external knowledge for improved modality fusion. Despite these advancements, achieving a unified latent embedding that effectively captures interactions across diverse modalities remains a key challenge in multimodal EHR processing.

Beyond multimodal learning, recent research has explored generative approaches to healthcare modeling. Conventional methods have primarily relied on discriminative models for disease risk assessment and diagnosis \cite{choi2016retain,niu2021lerp,qiao2019mnn}. However, generative models are increasingly being adopted, as demonstrated by Clinical CoT \cite{kwon2024large}, applying LLMs for disease diagnosis generation. Reinforcement learning from human feedback (RLHF) \cite{ouyang2022training} and Chain-of-Thought (CoT) prompting \cite{wei2022chain} have further enhanced medical reasoning capabilities in models such as GatorTron \cite{yang2022large}, MedPalm \cite{singhal2023large}, and GPT4-Med \cite{nori2023capabilities}. While these models excel in medical question-answering, they remain limited in real-world direct disease diagnosis and multimodal EHR processing. EHR-KnowGen \cite{niu2024ehr} reframes disease diagnosis as a text-to-text generation problem but overlooks the crucial temporal details embedded in time series lab tests, underscoring the need for more effective and dedicated multimodal fusion strategies.

% \section{Related Work}


% \textbf{Multimodal Learning in Healthcare.} Multimodal learning has become increasingly prevalent in healthcare due to the diverse EHR data. Models like MedCLIP \cite{wang2022medclip} use semantic contrastive learning to align medical images with textual reports, while RAIM \cite{qiao2019mnn} and GLoRIA \cite{huang2021gloria} employ attention mechanisms for integrating numerical and image data with texts. LDAM \cite{niu2021label} utilizes cross-attention with disease labels to fuse features from lab tests and medical notes. EHR-KnowGen \cite{niu2024ehr} introduces a novel approach by transforming lab data into text and applying external knowledge for modality fusion. However, achieving a unified latent embedding for effective information utilization across modalities remains a challenge.

% \textbf{Generative Models in Healthcare.} In the realm of deep learning methods for healthcare, most conventional approaches \cite{choi2016retain,niu2021lerp,qiao2019mnn} focused on employing discriminatory methods for disease risk diagnosis or prediction.  The current trend in healthcare modelling is shifting towards generative methods, as exemplified by Clinical-CoT \cite{kwon2024large}, which uses LLM for disease diagnosis generation. Supported by RLHF \cite{ouyang2022training} and Chain-of-Thought \cite{wei2022chain} techniques, LLMs like GatorTron \cite{yang2022large}, MedPalm \cite{singhal2023large}, and GPT4-Med \cite{nori2023capabilities} excel in medical question-answering but fall short in direct disease diagnosis and multimodal EHR processing. EHR-KnowGen \cite{niu2024ehr} uniquely approaches disease diagnosis as a text-to-text generation problem yet struggles with multimodal data integration.




\begin{figure*}[t]
\centering
\centerline{\includegraphics[scale=0.235]{Figs/model_stg1.pdf}}
\caption{
The ProMedTS model comprises three modules: the Time Series Prompt Embedding (TSPE) module, the Multimodal Textual Information Fusion (MTIF) module, and the Self-supervised Learning (SSL) module. The MTIF module utilizes Clinical-BERT to encode medical notes $\bm{M}$, lab test data $\bm{X}$, and anomaly descriptions $\bm{C}$ to generate time series prompt embeddings $\bm{\mathcal{T}}$. 
}
\label{model1}
\end{figure*}

\section{Methodology}
In this section, we present the ProMedTS framework for unifying heterogeneous EHR data through prompt-guided learning. We begin by defining the problem and describing the model inputs, then provide a high-level overview of the architecture. In subsequent sections, we detail each module and discuss how these components are applied to downstream tasks such as disease diagnosis.

\subsection{Problem Definition}
We introduce ProMedTS, aiming to reduce discrepancies between language and time series data in EHRs. Specifically, it leverages anomaly captions and generates time series prompt embeddings to unify both modalities in a shared latent space. The inputs to ProMedTS, denoted by $\{\bm{M}, \bm{X}\}$, include medical notes $\bm{M} \in \mathbb{R}^{B \times N_m}$ (where $B$ is the batch size and $N_m$ is the number of tokens) and numeric lab test data $\bm{X} \in \mathbb{R}^{B \times L \times N_x}$ (where $L$ is the sequence length and $N_x$ is the number of lab test variants). Additionally, a lightweight anomaly detection \cite{vinutha2018detection} is employed to generate textual descriptions of anomalies $\bm{C} \in \mathbb{R}^{B \times N_c}$ (details in Appendix A.2). ProMedTS also uses learnable time series query embeddings $\bm{P} \in \mathbb{R}^{B \times N_p \times D}$, which are transformed into time series prompt embeddings $\bm{\mathcal{T}} \in \mathbb{R}^{B \times N_p \times D}$, where $N_p$ is the query length and $D$ is the hidden dimension.

\subsection{Model Overview}
Figure~\ref{model1} illustrates the overview of ProMedTS, which comprises three main modules.  {Three modules share the same Clinical-BERT\cite{alsentzer2019publicly} structured model and are extended to support cross-attention, self-attention, and prompt generation.} The Time Series Prompt Embedding (TSPE) module applies a cross-attention mechanism to convert raw lab test data into prompt embeddings, preserving key temporal features. The Multimodal Textual Information Fusion (MTIF) module encodes and merges medical notes with anomaly captions in a unified latent space, facilitating the extraction of complementary semantic information. Finally, the Self-supervised Learning (SSL) module employs tailored loss functions to bridge the modality gap and maintain fine-grained temporal details in the learned representations. These modules work in tandem to achieve robust alignment and fusion of heterogeneous EHRs, and the following sections provide in-depth explanations of each component and their applications.

% ======
% \subsection{Problem Formulation}
% We introduce ProMedTS, a model designed to minimize discrepancies between language and time series EHRs, by introducing anomaly captions and creating time series prompt embeddings to unify latent embedding space. The input to ProMedTS, denoted as $\{\bm{M}, \bm{X}\}$, consists of medical notes $\bm{M} \in \mathbb{R}^{B \times N_m}$, where $B$ is the batch size and $N_m$ is number of tokens; and numeric lab test data $\bm{X} \in \mathbb{R}^{B \times L \times N_x}$, where $L$ is the sequence length and $N_x$ the number of test result variants. \textcolor{black}{In addition, we generate anomaly captioning through anomaly detection \cite{vinutha2018detection} and designed templates to describe time series anomalies $\bm{C} \in \mathbb{R}^{B \times N_c}$ as additional inputs to the model, where $N_c$ is a number of tokens (details see Appendix A.2).} Moreover, ProMedTS  incorporates learnable time series query embeddings $\bm{P} \in \mathbb{R}^{B \times N_p \times D}$, aiming to generate prompt embeddings $\bm{\mathcal{T}} \in \mathbb{R}^{B \times N_p \times D}$, with $N_p$ as the query length and $D$ as the hidden dimension.

% Figure~\ref{model1} outlines the framework's components for modality alignment and fusion: the Time Series Prompt Embedding (TSPE) module, the Multimodal Textual Information Fusion (MTIF) module, and the Self-supervised Learning (SSL) module. These modules, along with their application in downstream tasks, are detailed in subsequent sections.


\subsection{ Time Series Prompt Embedding}

The objective of the TSPE module is to extract and encapsulate the inherent information from time series lab test data into a time series prompt embedding. Let $\{\bm{X}, \bm{P}\}$ represent the module inputs. The numeric lab test data $\bm{X}$ is first processed by a time series encoder (TSE) using PatchTST \cite{nie2022time}. {In parallel, the learnable query embeddings $\bm{P}$,initialized using vectors extracted from the Clinical-BERT word embedding layer, serve as query tokens in the cross-attention mechanism, guiding the selection of relevant temporal features by attending to time series lab test results encoded by TSE. To generate the final prompt embedding $\bm{\mathcal{T}}$, we extend the multi-head self-attention encoder of Clinical-BERT to support a multi-head cross-attention mechanism, following a strategy similar to that adopted in \cite{li2023blip2}. We designate $\bm{X}$ as both key and value while $\bm{P}$ serves as the query:}
\begin{equation}
\bm{\mathcal{T}} = \text{Clinical-BERT}\bigl(\bm{P}, \text{TSE}(\bm{X}), \text{TSE}(\bm{X})\bigr).
\label{eq1}
\end{equation}
This design ensures that the rich temporal patterns in $\bm{X}$ are captured within $\bm{\mathcal{T}}$, enabling subsequent modules to leverage these features effectively.

% The objective of the TSPE Module is to extract and encapsulate the inherent information from time series lab test data from EHRs into a time series prompt embedding. Let $\{\bm{X}, \bm{P}\}$ represent the module inputs.  $\bm{X}$ is processed through a time series encoder  (TS Encoder) employing PatchTST \cite{nie2022time}. Concurrently, \textit{the learnable query embeddings} $\bm{P}$ are initialized using the word embedding of Clinical-BERT  \cite{alsentzer2019publicly}. To generate the time series prompt embedding $\bm{\mathcal{T}}$, similar to the application of the multi-head cross-attention mechanism in BERT \cite{li2023blip2}, $\bm{X}$ is designated as both the key and value inputs, while $\bm{P}$ serves as the query input:
% \begin{equation}
% \bm{\mathcal{T}} = \text{Clinical-BERT}(\bm{P}, \text{TSE}(\bm{X}), \text{TSE}(\bm{X})).
% \label{eq1}
% \end{equation}
% The cross-attention mechanism ensures that the inherent time series features can be preserved in the time series prompt embeddings.



\subsection{Multimodal Textual Information Fusion }
 The MTIF module is designed to fuse medical notes and anomaly descriptions effectively. \textcolor{black}{We use the anomaly captioning method to generate anomaly descriptions, as illustrated in Figure~\ref{model1}.} The inputs to the MTIF module are medical notes $\bm{M}$ and lab test anomaly descriptions $\bm{C}$, which are encoded separately by Clinical-BERT via the multi-head self-attention mechanism:
\begin{equation}
\begin{aligned}
&\bm{E}_m = \text{Clinical-BERT}(\bm{M},\bm{M},\bm{M}), \\
&\bm{E}_c = \text{Clinical-BERT}(\bm{C},\bm{C},\bm{C}), 
\label{eq2}
\end{aligned}
\end{equation}
where $\bm{E}_m \in \mathbb{R}^{B \times N_m \times D}$ and $\bm{E}_c \in \mathbb{R}^{B \times N_c \times D}$. The repeated inputs indicate that the key, query, and value matrices are identical for the self-attention mechanism.
This structure enables the model to encode each type of textual information independently while capturing the inherent characteristics and context of each input. The combined textual representation is then derived from these encoded inputs:
\begin{equation}
\bm{E}_f = AVG([\bm{E}_m \oplus \bm{E}_c]),
\label{eq3}
\end{equation}
where $\bm{E}_f \in \mathbb{R}^{B \times D}$, with $\oplus$ indicating concatenation, and $AVG$ representing average pooling.



\subsection{Self-Supervised Learning}

This module addresses the modality gap between textual and time series EHR data using three specialized loss functions. By simultaneously aligning cross-modal representations and preserving fine-grained temporal details, the model learns to capture both semantic and temporal nuances.

% \subsubsection{Language-Time Series Contrastive Learning}
\subsubsection{Cross-Modal Contrastive Alignment}

To promote cross-modal alignment, we design a contrastive loss that brings language and time series embeddings closer when they originate from the same patient and pushes them apart otherwise. We first compute similarity matrices by multiplying the fused text representation $\bm{E}_f$ with the time series prompt embeddings $\bm{\mathcal{T}}$:
\begin{equation}
\begin{aligned}
\bm{g}_{(\bm{E}_f,\bm{X})} &= \max\Bigl(\bigl[\bm{E}_f\,\bm{\mathcal{T}}^{(1)T},\ldots,\bm{E}_f\,\bm{\mathcal{T}}^{(N_p)T}\bigr]\Bigr),\\
\bm{g}_{(\bm{X},\bm{E}_f)} &= \max\Bigl(\bigl[\bm{\mathcal{T}}^{(1)}\,\bm{E}_f^T,\ldots,\bm{\mathcal{T}}^{(N_p)}\,\bm{E}_f^T\bigr]\Bigr),
\end{aligned}
\label{eq4}
\end{equation}
where the $\max$ operator performs max-pooling across $N_p$ dimensions, yielding $\bm{g}_{(\bm{E}_f,\bm{X})}$ and $\bm{g}_{(\bm{X},\bm{E}_f)} \in \mathbb{R}^{B \times B}$. Note that $\bm{g}_{(\bm{E}_f,\bm{X})}$ measures text-to-time series similarity (by fixing $\bm{E}_f$ and iterating over $\bm{\mathcal{T}}$), while $\bm{g}_{(\bm{X},\bm{E}_f)}$ captures time-series-to-text similarity (by fixing $\bm{\mathcal{T}}$ and iterating over $\bm{E}_f$). We then apply the SoftMax function to generate two distinct sets of logits:
\begin{equation}
\begin{aligned}
\hat{\bm{y}}_c^{f2x} &= \text{SoftMax}\bigl(\bm{g}_{(\bm{E}_f,\bm{X})}\bigr),\\
\hat{\bm{y}}_c^{x2f} &= \text{SoftMax}\bigl(\bm{g}_{(\bm{X},\bm{E}_f)}\bigr).
\end{aligned}
\label{eq5}
\end{equation}
{ Let $\bm{y}_c^{f2x}$ and $\bm{y}_c^{x2f}$denote the ground truth labels indicating whether the pairs correspond to the same patient in a training batch (1 if matched, 0 otherwise). We use cross-entropy $H(\cdot)$ to define the contrastive loss:}
\begin{equation}
\begin{aligned}
\mathcal{L}_{contrast} \;&=\; \tfrac{1}{2}\,\mathbb{E}\Bigl[
  H\bigl(\bm{y}_c^{f2x},\,\hat{\bm{y}}_c^{f2x}\bigr)\;\\
  &+\;
  H\bigl(\bm{y}_c^{x2f},\,\hat{\bm{y}}_c^{x2f}\bigr)
\Bigr].
\end{aligned}
\label{eq6}
\end{equation}


% \subsubsection{Anomaly Time Series Matching}
\subsubsection{Intra-Modal Matching}
To further capture intra-modality consistency, we align lab tests with corresponding anomaly descriptions. This alignment is modeled as a binary classification task, distinguishing matched from unmatched pairs of lab tests and anomaly captions. { Following \citet{li2021align}, we employ a negative mining strategy to generate labels $\bm{y}_m$ by selecting the most similar pairs in a training batch as negative samples, where the top 1-ranked pair is labeled as 1 and the others as 0, based on the similarity computed in Equation \ref{eq4}.} We employ Clinical-BERT’s cross-attention, where the concatenation of $\bm{C}$ and $\bm{P}$ serves as the query, and the encoded time series $\bm{X}$ is used as both key and value. A Multilayer Perceptron (MLP) classifier with softmax activation, denoted $f_{match}$, predicts the probability $\hat{\bm{y}}_m$:
\begin{equation}
\begin{aligned}
\hat{\bm{y}}_m =
f_{match}\Bigl(\text{Clinical-BERT}\bigl(
  f_\mathcal{W}(\bm{C}) \oplus \bm{P}, \\
  \text{TSE}(\bm{X}),
  \text{TSE}(\bm{X})
\bigr)\Bigr),
\end{aligned}
\label{eq7}
\end{equation}
where $f_\mathcal{W}$ is the word embedding layer in Clinical-BERT. We define the matching loss as:
\begin{equation}
\mathcal{L}_{match} \;=\;
\mathbb{E}\bigl[H\bigl(\bm{y}_m,\,\hat{\bm{y}}_m\bigr)\bigr],
\label{eq8}
\end{equation}
where $\bm{y}_m$ is the one-hot ground truth label.

% \subsubsection{Anomaly Description Generation}
\subsubsection{Anomaly Description Reconstruction}
To ensure the time series prompt embeddings encode both coarse anomaly descriptions and fine-grained temporal details, we reconstruct anomaly captions from the learned embeddings. This step helps unify language tokens and time series representations in a shared space. Specifically, we use Clinical-BERT with a language model head $f_{head}$, setting $\bm{E}_c$ as the query and $\bm{\mathcal{T}}$ as key and value:
\begin{equation}
\begin{small}
\mathcal{L}_{gen} =
\mathbb{E}\Bigl[
  H\bigl(\bm{C},
    f_{head}\bigl(\text{Clinical-BERT}(\bm{E}_c,
      \bm{\mathcal{T}},\bm{\mathcal{T}})\bigr)
  \bigr)
\Bigr].
\end{small}
\label{eq9}
\end{equation}
This objective encourages the model to generate accurate textual descriptions, thereby reinforcing alignment between time series prompts and language tokens.

\textbf{Overall Loss:}
We combine these objectives into a single training loss:
\begin{equation}
\mathcal{L}_{total}=
\alpha\,\mathcal{L}_{contrast}
\;+\;\beta\,\mathcal{L}_{match}
\;+\;\gamma\,\mathcal{L}_{gen},
\label{eq10}
\end{equation}
where $\alpha$, $\beta$, and $\gamma$ are hyperparameters balancing the three losses (see Appendix A.6). Our training algorithm aims to minimize $\mathcal{L}_{total}$ across all samples (details in Appendix A.1).



% ======


% \subsection{Self-supervised Learning }

% \textcolor{black}{The SSL module aims to migrate the modality gap between medical notes and lab test results through three self-supervised loss functions, while simultaneously further preserving the unique characteristics of the time series in lab test data.}

% \textbf{Language-Time Series Contrastive Learning} focuses on cross/inter-modality alignment,  improving the model's capability to identify and distinguish between the language and time series modalities (temporal information preserved in the prompt embeddings), effectively pulling related language-time series pairs closer and pushing unrelated ones further apart. To establish the degree of similarity between the language and time series representations, we compute similarity score matrices by performing matrix multiplication between $\bm{E}_f$ and $\bm{\mathcal{T}}$:
% \begin{equation}
% \begin{aligned}
% &\bm{g}_{(\bm{E}_f,\bm{X})} = \max([\bm{E}_f \bm{\mathcal{T}}^{(1)T},\ldots, \bm{E}_f \bm{\mathcal{T}}^{(N_p)T}]), \\
% &\bm{g}_{(\bm{X},\bm{E}_f)} = \max([\bm{\mathcal{T}}^{(1)}\bm{E}_f^T,\ldots, \bm{\mathcal{T}}^{(N_p)}\bm{E}_f^T]),
% \end{aligned}
% \label{eq4}
% \end{equation}
% where the $\max$ operation performs max-pooling across $N_p$ dimensions, generating the similarity matrices $\bm{g}_{(\bm{E}_f,\bm{X})}$ and $\bm{g}_{(\bm{X},\bm{E}_f)}$, each in $\mathbb{R}^{B \times B}$. Following this, we normalize the similarity scores using the SoftMax function:
% \begin{equation}
% \begin{aligned}
% &\hat{\bm{y}}_c^{f2x} = \text{SoftMax}(\bm{g}_{(\bm{E}_f,\bm{X})}),\\
% &\hat{\bm{y}}_c^{x2f} = \text{SoftMax}(\bm{g}_{(\bm{X},\bm{E}_f)),
% \end{aligned}
% \label{eq5}
% \end{equation}
% where $\hat{\bm{y}}_c^{f2x}$ and $\hat{\bm{y}}_c^{x2f} $ in $\mathbb{R}^{B \times B}$. The contrastive determination is based on whether the embeddings $\bm{E}_f$ and $\bm{\mathcal{T}}$ correspond to the same patient within a batch. This is evaluated using cross-entropy, with the true labels designated as $\bm{y}^{f2x}_c$ and $\bm{y}^{x2f}_c$. The loss is then calculated as follows:
% \begin{equation}
% \mathcal{L}_{contrast} = \frac{1}{2} \mathbb{E}[H(\bm{y}^{f2x}_c,\hat{\bm{y}}_c^{f2x}) + H(\bm{y}^{x2f}_c, \hat{\bm{y}}_c^{x2f} )],
% \label{eq6}
% \end{equation}
% \textbf{Anomaly Time Series Matching} focuses on intra-modality alignment, achieving fine-grained matching between lab tests and the corresponding anomaly descriptions to further preserve the unique temporal characteristics into time series prompt embeddings. This matching is modelled as a binary classification task, predicting whether a pair of lab tests, anomaly descriptions, and time series query embedding is positive (matched) or negative ( not matched). We follow the negative mining strategy employed in \cite{li2021align} to generate matching ground truth labels $\bm{y}_m$ (positive pairs and negative pairs). 
 
% We leverage the cross-attention layer provided by the Clinical-BERT, the concatenation of anomaly description $\bm{C}$ and time series query embedding $\bm{P}$ will be set as the query vector and time series data $\bm{X}$ will be set as the key and value vectors. A Multilayer Perceptron (MLP) classifier  with a softmax activation function $f_{match}$ is used to predict the two-class (positive and negative) probability  $\hat{\bm{y}}_m$ for a pair of input combinations:
% \begin{equation}
% \begin{aligned}
% \hat{\bm{y}}_m = &f_{match}(\text{Clinical-BERT}(\mathcal{W}(\bm{C})\oplus\bm{P},\\
% &TSE(\bm{X}),TSE(\bm{X})))
% \end{aligned}
% \label{eq7}
% \end{equation}
% where $\mathcal{W}$ is the word embedding layer embedded in Clinical-BERT. 

% Subsequently, a cross-entropy loss function is utilized to optimize the matching performance:
% \begin{equation}
% \mathcal{L}_{match} = \mathbb{E}[H( \bm{y}_m,\hat{\bm{y}}_m)],
% \label{eq8}
% \end{equation}
% where $\bm{y}_m $ refers one-hot ground truth label.

% \textbf{Anomaly Description Generation Grounded in Time Series} \textcolor{black}{aims to reconstruct anomaly descriptions from the learned time series prompt embeddings, facilitating their alignment with a unified encoding space shared by language tokens. This process ensures that the time series prompt embeddings effectively encapsulate fine-grained temporal details from lab test results and their coarse-grained anomaly descriptions within a unified encoding space, enhancing LLMs' capability to effectively interpret language-time series EHRs.  In this process, we leverage Clinical-BERT with a language model (LM) head $f_{head}$ to produce logits for generating time series prompts, where $\bm{E}_c$ is set to the query input, and $\bm{\mathcal{T}}$ is set to the key input and value input:}
% \begin{equation}
% \mathcal{L}_{gen} = \mathbb{E}[H(\bm{C}, f_{head}(\text{Clinical-BERT}(\bm{E}_c, \bm{\mathcal{T}},\bm{\mathcal{T}}))],
% \label{eq9}
% \end{equation}

% \textbf{Overall Loss Function} consolidates the individual objectives into a single training loss:
% \begin{equation}
% \mathcal{L}_{total} = \alpha \mathcal{L}_{contrast} + \beta\mathcal{L}_{match} + 
% \gamma\mathcal{L}_{gen}
% \label{eq10}
% \end{equation}
% Where $\alpha$, $\beta$, and $\gamma$ are ratios balancing the three loss functions (results in Appendix A.6), the training objective is to minimize $\mathcal{L}_{total}$ across all samples. The training algorithm is detailed in Appendix A.1.

\subsection{LLM-based Disease Diagnosis with ProMedTS}
To illustrate the practical effectiveness of ProMedTS in unifying textual and time series data, we employ a pre-trained, frozen LLM model for disease diagnosis. As depicted in Figure~\ref{model2}, ProMedTS first converts numeric lab test results into time series prompt embeddings, which are then aligned via a fully connected layer to match the LLM’s input dimensions. These embeddings serve as prefix soft prompts, concatenated with the medical notes so that the model can ingest structured signals from time series alongside unstructured clinical text. By bridging language and time series modalities, the LLM can process both inputs concurrently, leveraging complementary information for enhanced diagnostic accuracy. 




% \subsection{ProMedTS: Empowering LLMs in Disease Diagnosis}

% To evaluate the practical effectiveness of our ProMedTS model in generating time series prompt embeddings for LLM on healthcare downstream tasks, we implemented it for disease diagnosis using a pre-trained and frozen Flan-T5 \cite{chung2024scaling}. This task involves generating disease diagnoses from $\{\bm{M}, \bm{X}\}$. As illustrated in Figure~\ref{model2}, the process begins by using ProMedTS to convert numeric lab test results into time series prompt embeddings. A fully connected network then aligns the variations of these embeddings shape $\bm{\mathcal{T}}$ across different LLMs. The adapted embeddings are integrated as prefix soft prompts alongside medical notes. Due to the pre-training of ProMedTS to bridge the modality gap between textual and time series data, it enables Flan-T5 to effectively interpret and analyze both modalities concurrently, leveraging the distinctive characteristics of time series prompts for improved diagnostic performance. 



\begin{figure}[t]
\centering
\centerline{\includegraphics[scale=0.24]{Figs/model_stg2.pdf}}
\caption{ProMedTS for empowering LLMs to in disease diagnosis.}
\label{model2}
\end{figure}

% This process is summarized as follows:
% \begin{equation}
% \bm{\mathcal{T}}^*} = f_{adp}(\text{ProMedTS }(\bm{X}})),
% \label{eq11}
% \end{equation}
% where  $\bm{\mathcal{T}}^{*} \in \mathbb{R}^{B \times N_p \times D_l}$, with $D_l$ being Flan-T5's hidden dimension. The Time Series Prompt Embedding embeddings $\bm{\mathcal{T}}^{(b)*}$ are input alongside medical notes, preparing them for the disease diagnosis. The disease diagnosis is generated through the LM by minimizing the cross-entropy loss:
% \begin{equation}
%  \begin{aligned}
%  & \bm{E}_w =  f_{te}(\bm{M}), \\
% &\mathcal{L}_{diag} = \mathbb{E}[H(\bm{y}_d, PLM( [\bm{\mathcal{T}}*} \oplus \bm{E}_w))],
%  \end{aligned}
%  \label{eq12}
% \end{equation}
% where  $f_{te}$ is the token embedding function, $\bm{E}_w \in \mathbb{R}^{b \times N_m \times D_l}$, $PLM$ is the pre-trained  Flan-T5-Small. The training process is focused on minimizing the loss defined above using the AdamW optimizer to optimize the disease diagnosis generation. 
\begin{table*}[t]
\small
% \hspace*{-19em}
\setlength\tabcolsep{1.6pt}
\renewcommand\arraystretch{1}
\centering

\begin{tabular}{l|r|cc|cc|ccc|ccc}
\toprule[1pt]

\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Size}} &\multicolumn{2}{c|}{\textbf{Type}} &\multicolumn{2}{c|}{\textbf{Modality}} &\multicolumn{3}{c|}{\textbf{Micro}} &\multicolumn{3}{c}{\textbf{Macro}}  \\
                            \cline{3-12}
                          && \multicolumn{1}{c|}{\textbf{CLS}} & \textbf{GEN} & \multicolumn{1}{c|}{\textbf{Lab}} &\multicolumn{1}{c|}{\textbf{Note}} & \multicolumn{1}{c}{\textbf{Precision}} & \multicolumn{1}{c}{\textbf{Recall}}   & \multicolumn{1}{c|}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Precision}} & \multicolumn{1}{c}{\textbf{Recall}}   & \multicolumn{1}{c}{\textbf{F1}}   \\ 
                         \cline{1-12}
 \multicolumn{12}{c}{\textbf{ MIMIC-III}}  \\ 
                         \cline{1-12}
\textbf{GRU}  &  7.9M  & \checkmark &  & \checkmark &  & 46.41$_{(3.48)}$ & 21.88$_{(3.59)}$ & 29.43$_{(1.89)}$ & 30.47$_{(4.23)}$ & 13.00$_{(1.14)}$ & 14.59$_{(1.48)}$  \\
\textbf{PatchTST}  &  19.2M  & \checkmark &  & \checkmark & & 32.64$_{(3.59)}$ & 42.72$_{(5.01)}$ & 36.02$_{(1.09)}$ & 26.86$_{(3.51)}$ & 29.71$_{(4.78)}$ & 19.25$_{(3.50)}$    \\
\textbf{TimeLLM}  & 78M & \checkmark &  & \checkmark & & 37.43$_{(1.17)}$ & 54.93$_{(6.56)}$ & 36.59$_{(1.17)}$ & 10.18$_{(2.30)}$ & $35.21_{(6.47)}$ & 15.16$_{(2.17)}$    \\
\textbf{CAML} & 36.1M& \checkmark &  & &   \checkmark & 69.04$_{(0.18)}$ & 55.87$_{(2.72)}$ & 61.54$_{(0.30)}$ &  65.08$_{(2.56)}$ & 50.12$_{(3.05)}$ &  54.42$_{(0.94)}$ \\
\textbf{DIPOLE}  & 39M & \checkmark &  &  & \checkmark & 64.38$_{(0.89)}$ & 57.94$_{(1.15)}$ & 60.98$_{(0.27)}$ & 61.63$_{(1.03)}$ & 53.02$_{(1.18)}$ & 55.68$_{(0.49)}$   \\
\textbf{PROMPTEHR} &75.2M &  & \checkmark &  & \checkmark & 59.29$_{(0.97)}$  & 65.53$_{(0.69)}$ & 62.24$_{(0.23)}$ & 57.44$_{(0.97)}$ & 62.87$_{(0.61)}$   & 59.10$_{(0.24)}$  \\
\textbf{LLaMA}  &7B& & \checkmark &  & \checkmark & 61.32$_{(2.10)}$  & 65.88$_{(1.54)}$  & 63.44$_{(0.37)}$  & 60.98$_{(1.54)}$  &  61.59$_{(1.36)}$  & 60.34$_{(0.38)}$    \\
\textbf{LDAM}   &41.3M& \checkmark &  & \checkmark & \checkmark & 68.00$_{(1.23)}$ & 57.12$_{(0.47)}$ & 62.18$_{(0.40)}$  & 67.38$_{(0.35)}$ & 51.50$_{(0.95)}$ & 57.44$_{(0.60)}$  \\

\textbf{FROZEN}  &265M& &  \checkmark & \checkmark & \checkmark & 61.09$_{(1.81)}$ & 64.07$_{(1.58)}$ & 62.51$_{(0.34)}$  &  59.96$_{(1.55)}$ & 59.99$_{(1.66)}$ & 
59.15$_{(0.30)}$ \\
\textbf{EHR-KnowGen} &76.9M &  & \checkmark & \checkmark & \checkmark & 60.01$_{(0.29)}$ & 65.51$_{(0.18)}$ & 62.62$_{(0.06)}$ & 58.34$_{(0.38)}$ &  61.81$_{(0.28)}$ & 59.44$_{(0.06)}$  \\
\rowcolor{gray!20} \textbf{ProMedTS} &267.5M& &  \checkmark & \checkmark & \checkmark & 61.32$_{(0.54)}$ & 66.65$_{(0.51)}$ & \underline{63.67}$_{(0.08)}$  & 60.35$_{(0.61)}$ & 61.62$_{(0.71)}$ & \underline{60.42}$_{(0.18)}$  \\
\rowcolor{gray!20} \textbf{ProMedTS}* & 1B & &  \checkmark & \checkmark & \checkmark & 60.62$_{(0.22)}$ & 67.83$_{(0.18)}$ &  \textbf{64.02}$_{(0.11)}$ & 59.43$_{(0.37)}$ & 63.65$_{(0.54)}$ &  \textbf{60.78}$_{(0.13)}$  \\
\hline
 \multicolumn{12}{c}{\textbf{ MIMIC-IV}}  \\ 
                         \cline{1-12}

\textbf{GRU}  & 7.9M & \checkmark &  & \checkmark &  & 56.23$_{(1.13)}$ & 25.77$_{(1.58)}$ & 35.21$_{(1.36)}$ & 38.37$_{(1.90)}$ & 16.97$_{(1.22)}$ & 20.65$_{(1.32)}$  \\    
\textbf{PatchTST}  & 19.2M & \checkmark &  & \checkmark & & 27.26$_{(0.03)}$ & 57.42$_{(0.41)}$ & 36.97$_{(0.10)}$ & 20.59$_{(2.76)}$ & 43.72$_{(0.25)}$ & 21.78$_{(2.83)}$   \\
\textbf{TimeLLM}   & 78M & \checkmark &  & \checkmark & & 30.30$_{(1.78)}$ & 60.46$_{(1.98)}$ & 40.31$_{(1.20)}$ & 24.61$_{(2.21)}$ & 47.26$_{(2.37)}$ & 25.56$_{(1.60)}$    \\

\textbf{CAML}  &36.1M& \checkmark &  & &  \checkmark & 72.82$_{(0.54)}$  & 59.48$_{(0.82)}$  & 65.40$_{(0.36)}$  & 67.25$_{(0.99)}$  & 50.73$_{(1.49)}$  & 54.71$_{(1.42)}$   \\
\textbf{DIPOLE} & 39M & \checkmark &  & &  \checkmark & 72.39$_{(0.51)}$ & 61.38$_{(0.83)}$ & 66.43$_{(0.33)}$ & 70.45$_{(0.37)}$ & 55.65$_{(0.79)}$ & 60.37$_{(0.62)}$  \\

\textbf{PROMPTEHR} & 75.2M &  & \checkmark &   & \checkmark & 65.24$_{(0.68)}$ & 70.31$_{(0.56)}$ & 68.02$_{(0.17)}$ & 63.53$_{(0.47)}$ & 67.02$_{(0.65)}$ & 65.01$_{(0.28)}$  \\
\textbf{LLaMA}  &7B& &  \checkmark & &   \checkmark &  68.44$_{(1.03)}$ & 69.49$_{(0.70)}$ & 69.13$_{(0.31)}$ &   67.45$_{(0.93)}$ & 66.18$_{(1.12)}$ & 66.15$_{(0.56)}$   \\
\textbf{LDAM}   & 41.3M& \checkmark &  & \checkmark &  \checkmark & 72.01$_{(0.85)}$ & 62.74$_{(0.62)}$ & 66.91$_{(0.20)}$ & 69.77$_{(0.18)}$ & 56.72$_{(0.69)}$ & 60.77$_{(0.48)}$   \\

\textbf{FROZEN} & 265M &  & \checkmark & \checkmark &  \checkmark &  67.81$_{(0.78)}$ & 69.08$_{(0.94)}$ & 68.42$_{(0.08)}$ & 66.27$_{(1.00)}$ & 65.21$_{(0.97)}$ & 65.30$_{(0.05)}$  \\
\textbf{EHR-KnowGen} &76.9M& &  \checkmark & \checkmark &  \checkmark & 65.80$_{(0.64)}$  & 70.85$_{(0.45)}$ &  68.16$_{(0.11)}$ & 63.82$_{(0.53)}$ & 67.24$_{(0.55)}$ & 65.11$_{(0.13)}$   \\
\rowcolor{gray!20}  \textbf{ProMedTS } &267.5M& &  \checkmark & \checkmark & \checkmark & 71.63$_{(0.46)}$ & 67.81$_{(0.85)}$ & \underline{69.69}$_{(0.18)}$ &  70.12$_{(0.47)}$ & 63.58$_{(0.79)}$ &  \underline{66.21}$_{(0.17)}$ \\
\rowcolor{gray!20} \textbf{ProMedTS}* & 1B & &  \checkmark & \checkmark & \checkmark & 71.12$_{(0.31)}$ & 69.33$_{(0.42)}$ & \textbf{70.21}$_{(0.05)}$  & 70.97$_{(0.42)}$ & 65.51$_{(0.64)}$ &  \textbf{67.56}$_{(0.09)}$  \\
\toprule[1pt]
\end{tabular}


\caption{ The performance of comparative methods in the disease diagnosis tasks on MIMIC-III and MIMIC-IV. Please note CLS - classification model, GEN -generative model, Lab - lab test result, and Note - medical notes. }
\label{table1}
\end{table*}


% % Table adjustments
% \begin{table*}[t]
% \small
% \setlength\tabcolsep{4pt} % Adjusted column spacing for better readability
% \renewcommand\arraystretch{1.2} % Increased row height
% \centering
% \caption{Performance of comparative methods in disease diagnosis tasks on MIMIC-III and MIMIC-IV. Best results are in \textbf{bold}.}
% \begin{tabular}{
%     l
%     |r
%     |c@{\hskip 6pt}c
%     |c@{\hskip 6pt}c
%     |S[table-format=2.2]S[table-format=2.2]S[table-format=2.2]
%     |S[table-format=2.2]S[table-format=2.2]S[table-format=2.2]}
% \toprule[1.2pt]

% \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Size}} 
% & \multicolumn{2}{c|}{\textbf{Type}} 
% & \multicolumn{2}{c|}{\textbf{Modality}} 
% & \multicolumn{3}{c|}{\textbf{Micro}} 
% & \multicolumn{3}{c}{\textbf{Macro}}  \\

% \cline{3-12}
% && \textbf{CLS} & \textbf{GEN} 
% & \textbf{Lab} & \textbf{Note} 
% & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
% & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\

% \midrule
% \multicolumn{12}{c}{\textbf{MIMIC-III}} \\
% \midrule

% \textbf{GRU} & 7.9M & \checkmark &  & \checkmark & 
% & 46.41$_{(3.48)}$ & 21.88$_{(3.59)}$ & 29.43$_{(1.89)}$ 
% & 30.47$_{(4.23)}$ & 13.00$_{(1.14)}$ & 14.59$_{(1.48)}$  \\

% \textbf{PatchTST} & 19.2M & \checkmark &  & \checkmark & 
% & 32.64$_{(3.59)}$ & 42.72$_{(5.01)}$ & 36.02$_{(1.09)}$ 
% & 26.86$_{(3.51)}$ & 29.71$_{(4.78)}$ & 19.25$_{(3.50)}$  \\

% \textbf{TimeLLM} & 78M & \checkmark &  & \checkmark & 
% & 37.43$_{(1.17)}$ & 54.93$_{(6.56)}$ & 36.59$_{(1.17)}$ 
% & 10.18$_{(2.30)}$ & 35.21$_{(6.47)}$ & 15.16$_{(2.17)}$  \\

% \textbf{CAML} & 36.1M & \checkmark &  &  & \checkmark 
% & \textbf{69.04}$_{(0.18)}$ & 55.87$_{(2.72)}$ & \textbf{61.54}$_{(0.30)}$ 
% & \textbf{65.08}$_{(2.56)}$ & 50.12$_{(3.05)}$ & \textbf{54.42}$_{(0.94)}$ \\

% \textbf{DIPOLE} & 39M & \checkmark &  &  & \checkmark 
% & 64.38$_{(0.89)}$ & 57.94$_{(1.15)}$ & 60.98$_{(0.27)}$ 
% & 61.63$_{(1.03)}$ & 53.02$_{(1.18)}$ & 55.68$_{(0.49)}$ \\

% \rowcolor{gray!20} 
% \textbf{ProMedTS (small)} & 267.5M &  & \checkmark & \checkmark & \checkmark 
% & 61.32$_{(0.54)}$ & \textbf{66.65}$_{(0.51)}$ & 63.67$_{(0.08)}$ 
% & 60.35$_{(0.61)}$ & \textbf{61.62}$_{(0.71)}$ & 60.42$_{(0.18)}$ \\

% \rowcolor{gray!20} 
% \textbf{ProMedTS (large)} & 1B &  & \checkmark & \checkmark & \checkmark 
% & \textbf{---} & \textbf{---} & \textbf{---} 
% & \textbf{---} & \textbf{---} & \textbf{---} \\

% \midrule
% \multicolumn{12}{c}{\textbf{MIMIC-IV}} \\
% \midrule

% % Add the remaining data following the same pattern...

% \toprule[1.2pt]
% \end{tabular}
% \end{table*}
\section{Experiments}
\subsection{Datasets and Preprocessing}
The MIMIC-III dataset \cite{johnson2016mimic} is a publicly available EHR dataset containing de-identified  patients who were admitted to ICUs between 2001 and 2012. It includes medical discharge summaries, lab test results, chest x-ray images and more. Our analysis focuses on EHR data from approximately 27,000 patients including complete medical discharge summaries and lab test results. The MIMIC-IV dataset \cite{johnson2023mimic} comprises EHR data from 2008 to 2019. We utilize approximately 29,000 EHR records from MIMIC-IV, which include complete medical discharge summaries and lab test results. 
% Given the computational limitations associated with Flan-T5, 
Our study targets 25 disease phenotypes as defined in the MIMIC-III benchmark \cite{harutyunyan2019multitask}.

\paragraph{Data Pre-processing.} 
For medical notes, we extract the brief course from medical discharge summaries, removing numbers, noise, and stopwords. Numerical lab test results are converted into time series data using the benchmark tools \cite{Harutyunyan2019}, with missing values filled using the nearest available numbers. Time series anomaly descriptions are used the method defined in Appendix A.2. Data splitting follows the guidelines in \cite{Harutyunyan2019}, using a 4:1 ratio for training and testing.


% \subsection{Baseline Methods}

% For our evaluation, we benchmarked against various methods: GRU \cite{cho2014learning}, PatchTST \cite{nie2022time}, TimeLLM \cite{jintime}, CAML \cite{mullenbach2018explainable}, DIPOLE \cite{ma2017dipole}, PROMPTEHR \cite{wang2022promptehr}, LLaMA--7B \cite{touvron2023llama}, LDAM \cite{niu2021label}, FROZEN \cite{tsimpoukelli2021multimodal}, and EHR-KnowGen \cite{niu2024ehr}. Detailed configurations of these baselines are provided in Appendix A.3. We use Flan-T5-small \cite{chung2024scaling} as the encoder-decoder model for diagnosis. this choice is primarily driven by resource constraints and ease of experimentation. In practice, any encoder–decoder LLM with sufficient capacity can be substituted, offering greater flexibility and potentially better results.
% To ensure a fair comparison, all baselines utilize Flan-T5-small as the text encoder. Results are averaged over five runs with different random seeds. Implementation details for all models are described in Appendix A.4, and training instructions are outlined in Appendix A.5. Our code is public available at \href{https://anonymous.4open.science/r/ProMedTS-V1-5F51/}{https://anonymous.4open.science/r/ProMedTS-V1-5F51/}.

\subsection{Baseline Methods}
We benchmark our approach against a range of methods: GRU \cite{cho2014learning}, PatchTST \cite{nie2022time}, TimeLLM \cite{jintime}, CAML \cite{mullenbach2018explainable}, DIPOLE \cite{ma2017dipole}, PROMPTEHR \cite{wang2022promptehr}, LLaMA--7B \cite{touvron2023llama}, LDAM \cite{niu2021label}, FROZEN \cite{tsimpoukelli2021multimodal}, and EHR-KnowGen \cite{niu2024ehr}. Detailed configurations of these baselines are provided in Appendix~A.3. {For the disease diagnosis task, we adopt two scales of Flan-T5 \cite{chung2024scaling} as the inference LLM to validate our model's effectiveness in understanding multimodal EHRs, primarily driven by resource considerations and ease of experimentation. The Flan-T5-Small-based model is denoted as PromMedTS, while the Flan-T5-Large-based model is denoted as ProMedTS*.} In principle, any sufficiently LLM could be substituted to potentially achieve even stronger results.

To ensure a fair comparison, all baselines also employ Flan-T5 as their backbone. Reported results are averaged over five runs with different random seeds. The statistical significance determined at p < 0.05 by t-test. Implementation details for every model are described in Appendix~A.4, and training instructions appear in Appendix~A.5. Our code is publicly available at \href{https://anonymous.4open.science/r/ProMedTS-V1-5F51/}{https://anonymous.4open.science/r/ProMedTS-V1-5F51/}.

% \begin{table*}[htbp]
% \small

% \setlength\tabcolsep{2.8pt}
% \renewcommand\arraystretch{1}
% \centering



% \begin{tabular}{l|cc|cc|ccc|ccc}
% \toprule[1pt]
% \multirow{2}{*}{\textbf{Models}}  & \multicolumn{2}{c|}{\textbf{Input}}  & \multicolumn{2}{c|}{\textbf{Alignment}} & \multicolumn{3}{c|}{\textbf{Micro}} & \multicolumn{3}{c}{\textbf{Macro}}\\

%             \cline{2-11}
%                           & \multicolumn{1}{c}{\textbf{Lab}} & \textbf{Note} & \textbf{Anomaly}  &  \textbf{Note} & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} \\ 
%                           \toprule[1pt]
% \multicolumn{11}{c}{\textbf{ MIMIC-III}}  \\ 
% \hline

% \rowcolor{gray!20} \textbf{ProMedTS }&\checkmark &  \checkmark &  \checkmark&  & 61.32$_{(0.54)}$ & 66.65$_{(0.51)}$ & \textbf{63.67}$_{(0.08)}$  & 60.35$_{(0.61)}$ & 61.62$_{(0.71)}$ & \textbf{60.42}$_{(0.18)}$  \\
% \hline
% \textbf{\textit{w/o} LAB }   &  & \checkmark & & & 58.91$_{(0.83)}$ & 66.59$_{(0.57)}$ & 62.34$_{(0.26)}$ & 57.32$_{(0.88)}$ & 62.56$_{(0.61)}$ & 59.05$_{(0.22)}$  \\
% \textbf{\textit{w/o} ANOMALY }   &\checkmark & \checkmark &  & \checkmark & 60.09$_{(0.32)}$ &   65.03$_{(0.98)}$ & 62.44$_{(0.22)}$ & 59.13$_{(0.43)}$ &  60.46$_{(1.15)}$ & 59.11$_{(0.25)}$ \\
% \hline

% \multicolumn{11}{c}{\textbf{ MIMIC-IV}}  \\ 
% \hline
% \rowcolor{gray!20}  \textbf{ProMedTS }&\checkmark &  \checkmark & \checkmark &  & 71.63$_{(0.46)}$ & 67.81$_{(0.85)}$ & \textbf{69.69}$_{(0.18)}$ &  70.12$_{(0.47)}$ & 63.58$_{(0.79)}$ &  \textbf{66.21}$_{(0.17)}$ \\
% \hline
% \textbf{\textit{w/o} LAB }   &  & \checkmark & & & 67.16$_{(0.55)}$ & 69.42$_{(0.59)}$ & 68.22$_{(0.31)}$ &  65.74$_{(0.62)}$ & 64.69$_{(0.48)}$ & 64.33$_{(0.18)}$   \\
% \textbf{\textit{w/o} ANOMALY}  &\checkmark & \checkmark &  & \checkmark & 70.94$_{(0.37)}$ &  66.44$_{(1.37)}$ &   68.47$_{(0.12)}$ & 68.95$_{(0.79)}$    & 62.45$_{(0.96)}$ &  65.13$_{(0.12)}$  \\

% \toprule[1pt]
% \end{tabular}
% \caption{Ablation studies on different modality input and alignment designs for disease diagnosis. Please note Anomaly indicated lab test anomaly captions. }
% \label{table2}
% \end{table*}

\begin{table*}[htbp]
\small

\setlength\tabcolsep{10.5pt}
\renewcommand\arraystretch{1}
\centering
\begin{tabular}{l|ccc|ccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Models}} & \multicolumn{3}{c|}{\textbf{Micro}} & \multicolumn{3}{c}{\textbf{Macro}}\\

            \cline{2-7}
                         & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} \\ 
                          \toprule[1pt]
\multicolumn{7}{c}{\textbf{ MIMIC-III}}  \\ 
\hline

\rowcolor{gray!20} \textbf{ProMedTS } & 61.32$_{(0.54)}$ & 66.65$_{(0.51)}$ & \textbf{63.67}$_{(0.08)}$  & 60.35$_{(0.61)}$ & 61.62$_{(0.71)}$ & \textbf{60.42}$_{(0.18)}$  \\
\hline
\textbf{\textit{w/o} LAB }  & 58.91$_{(0.83)}$ & 66.59$_{(0.57)}$ & 62.34$_{(0.26)}$ & 57.32$_{(0.88)}$ & 62.56$_{(0.61)}$ & 59.05$_{(0.22)}$  \\
\textbf{\textit{w/o} ANOMALY }  & 60.09$_{(0.32)}$ &   65.03$_{(0.98)}$ & 62.44$_{(0.22)}$ & 59.13$_{(0.43)}$ &  60.46$_{(1.15)}$ & 59.11$_{(0.25)}$ \\
\hline

\multicolumn{7}{c}{\textbf{ MIMIC-IV}}  \\ 
\hline
\rowcolor{gray!20}  \textbf{ProMedTS }& 71.63$_{(0.46)}$ & 67.81$_{(0.85)}$ & \textbf{69.69}$_{(0.18)}$ &  70.12$_{(0.47)}$ & 63.58$_{(0.79)}$ &  \textbf{66.21}$_{(0.17)}$ \\
\hline
\textbf{\textit{w/o} LAB }& 67.16$_{(0.55)}$ & 69.42$_{(0.59)}$ & 68.22$_{(0.31)}$ &  65.74$_{(0.62)}$ & 64.69$_{(0.48)}$ & 64.33$_{(0.18)}$   \\
\textbf{\textit{w/o} ANOMALY} & 70.94$_{(0.37)}$ &  66.44$_{(1.37)}$ &   68.47$_{(0.12)}$ & 68.95$_{(0.79)}$    & 62.45$_{(0.96)}$ &  65.13$_{(0.12)}$  \\

\toprule[1pt]
\end{tabular}

\caption{Ablation studies on different modality input and alignment designs for disease diagnosis.}
% Please note Anomaly indicated lab test anomaly captions. 
\label{table2}
\end{table*}

% \begin{table*}[t]
% \small
% \setlength\tabcolsep{5.2pt}
% \renewcommand\arraystretch{1}
% \centering

% \begin{tabular}{l|ccc|ccc|ccc}
% \toprule[1pt]
% \multirow{2}{*}{\textbf{Models}} &\multicolumn{3}{c|}{\textbf{Loss}} &\multicolumn{3}{c|}{\textbf{Micro}}  &\multicolumn{3}{c}{\textbf{Macro}} \\

%          \cline{2-10}
%                           & \multicolumn{1}{c}{$\mathcal{L}_{c}$} & $\mathcal{L}_{m}$ & $\mathcal{L}_{g}$ & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} \\ 
% \toprule[1pt]

% \multicolumn{10}{c}{\textbf{ MIMIC-III}}  \\ 
% \hline
% \rowcolor{gray!20}  \textbf{ProMedTS}  &  \checkmark & \checkmark & \checkmark & 61.32$_{(0.54)}$ & 66.65$_{(0.51)}$ & \textbf{63.67}$_{(0.08)}$  & 60.35$_{(0.61)}$ & 61.62$_{(0.71)}$ & \textbf{60.42}$_{(0.18)}$  \\
% \hline
% \textbf{\textit{w/o} CONTRAST} & & \checkmark & \checkmark & 60.24$_{(0.25)}$ & 66.00$_{(0.39)}$ & 62.99$_{(0.07)}$  & 59.92$_{(0.60)}$ & 61.41$_{(0.51)}$ & 59.73$_{(0.07)}$  \\
% \textbf{\textit{w/o} MATCH} & \checkmark&  & \checkmark &  60.12$_{(0.58)}$ &  66.14$_{(1.50)}$ & 62.96$_{(0.02)}$  & 59.70$_{(1.18)}$ & 61.37$_{(1.34)}$ & 59.65$_{(0.11)}$  \\
% \textbf{\textit{w/o} GEN} & \checkmark & \checkmark &  & 59.95$_{(0.38)}$ &  66.15$_{(0.27)}$ & 62.89$_{(0.19)}$  &   59.57$_{(0.55)}$ &  61.32$_{(0.30)}$ &  59.61$_{(0.20)}$ \\
% \hline

% \multicolumn{10}{c}{\textbf{ MIMIC-IV}}  \\ 
% \hline
% \rowcolor{gray!20}  \textbf{ProMedTS }& \checkmark &  \checkmark & \checkmark  & 71.63$_{(0.46)}$ & 67.81$_{(0.85)}$ & \textbf{69.69}$_{(0.18)}$ &  70.12$_{(0.47)}$ & 63.58$_{(0.79)}$ &  \textbf{66.21}$_{(0.17)}$ \\
% \hline
% \textbf{\textit{w/o} CONTRAST} & & \checkmark & \checkmark &  70.19$_{(0.25)}$ &  66.22$_{(0.39)}$ & 68.61$_{(0.09)}$ &  69.05$_{(0.24)}$ & 62.40$_{(0.35}$ & 65.21$_{(0.12)}$\\
% \textbf{\textit{w/o} MATCH} & \checkmark&  & \checkmark & 70.79$_{(0.34)}$ & 66.49$_{(0.38)}$ & 68.67$_{(0.15)}$ & 68.91$_{(0.73)}$ & 62.25$_{(0.48)}$ & 65.47$_{(0.15)}$\\
% \textbf{\textit{w/o} GEN} & \checkmark & \checkmark &  &  71.30$_{(0.29)}$ &  65.79$_{(0.57)}$ &  68.44$_{(0.13)}$ & 69.14$_{(0.48)}$ & 62.05$_{(0.54)}$ &  65.03$_{(0.13)}$\\

% \toprule[1pt]
% \end{tabular}

% \caption{Ablation studies on the effectiveness of different loss functions of our model for disease diagnosis. Please note $\mathcal{L}_{c}$ denotes $\mathcal{L}_{contrast}$, $\mathcal{L}_{m}$ denotes $\mathcal{L}_{match}$, and $\mathcal{L}_{g}$ denotes $\mathcal{L}_{gen}$. }
% \label{table3}
% \end{table*}

\begin{table*}[t]
\small
\setlength\tabcolsep{10.5pt}
\renewcommand\arraystretch{1}
\centering

\begin{tabular}{l|ccc|ccc}
\toprule[1pt]
\multirow{2}{*}{\textbf{Models}} &\multicolumn{3}{c|}{\textbf{Micro}}  &\multicolumn{3}{c}{\textbf{Macro}} \\

         \cline{2-7}
                         & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} & \textbf{Precision}  & \textbf{Recall}  & \textbf{F1} \\ 
\toprule[1pt]

\multicolumn{7}{c}{\textbf{ MIMIC-III}}  \\ 
\hline
\rowcolor{gray!20}  \textbf{ProMedTS} & 61.32$_{(0.54)}$ & 66.65$_{(0.51)}$ & \textbf{63.67}$_{(0.08)}$  & 60.35$_{(0.61)}$ & 61.62$_{(0.71)}$ & \textbf{60.42}$_{(0.18)}$  \\
\hline
\textbf{\textit{w/o} CONTRAST}  & 60.24$_{(0.25)}$ & 66.00$_{(0.39)}$ & 62.99$_{(0.07)}$  & 59.92$_{(0.60)}$ & 61.41$_{(0.51)}$ & 59.73$_{(0.07)}$  \\
\textbf{\textit{w/o} MATCH}  &  60.12$_{(0.58)}$ &  66.14$_{(1.50)}$ & 62.96$_{(0.02)}$  & 59.70$_{(1.18)}$ & 61.37$_{(1.34)}$ & 59.65$_{(0.11)}$  \\
\textbf{\textit{w/o} GEN}& 59.95$_{(0.38)}$ &  66.15$_{(0.27)}$ & 62.89$_{(0.19)}$  &   59.57$_{(0.55)}$ &  61.32$_{(0.30)}$ &  59.61$_{(0.20)}$ \\
\hline

\multicolumn{7}{c}{\textbf{ MIMIC-IV}}  \\ 
\hline
\rowcolor{gray!20}  \textbf{ProMedTS }& 71.63$_{(0.46)}$ & 67.81$_{(0.85)}$ & \textbf{69.69}$_{(0.18)}$ &  70.12$_{(0.47)}$ & 63.58$_{(0.79)}$ &  \textbf{66.21}$_{(0.17)}$ \\
\hline
\textbf{\textit{w/o} CONTRAST} &  70.19$_{(0.25)}$ &  66.22$_{(0.39)}$ & 68.61$_{(0.09)}$ &  69.05$_{(0.24)}$ & 62.40$_{(0.35}$ & 65.21$_{(0.12)}$\\
\textbf{\textit{w/o} MATCH}& 70.79$_{(0.34)}$ & 66.49$_{(0.38)}$ & 68.67$_{(0.15)}$ & 68.91$_{(0.73)}$ & 62.25$_{(0.48)}$ & 65.47$_{(0.15)}$\\
\textbf{\textit{w/o} GEN}  &  71.30$_{(0.29)}$ &  65.79$_{(0.57)}$ &  68.44$_{(0.13)}$ & 69.14$_{(0.48)}$ & 62.05$_{(0.54)}$ &  65.03$_{(0.13)}$\\

\toprule[1pt]
\end{tabular}

\caption{Ablation studies on the effectiveness of different loss functions of our model for disease diagnosis. }
% Please note $\mathcal{L}_{c}$ denotes $\mathcal{L}_{contrast}$, $\mathcal{L}_{m}$ denotes $\mathcal{L}_{match}$, and $\mathcal{L}_{g}$ denotes $\mathcal{L}_{gen}$.
\label{table3}
\end{table*}
\subsection{Disease Diagnosis Performance}
Table~\ref{table1} shows that ProMedTS achieves the highest overall performance, particularly in F1 scores on MIMIC-IV. {In addition, replacing the LLM with a larger model improves F1 scores on both datasets, indicating our model's scalability and robustness across different LLMs.} Furthermore, TimeLLM performs strongly with lab test, highlighting the value of time-series inputs for LLMs in disease diagnosis. Text-based methods (e.g., LLaMA) generally outperform time-series approaches, suggesting that medical notes capture richer disease-related information. Multimodal models (LDAM, FROZEN, EHR-KnowGen) exceed single-modality baselines (TimeLLM, CAML, DIPOLE, PROMPTEHR), confirming the benefits of integrating text and time series. Generative approaches (TimeLLM, LLaMA, FROZEN, EHR-KnowGen) also outperform classification-based methods. Although LLaMA performs well, its higher variance and parameter requirements reduce practicality. Notably, the Flan-T5-based ProMedTS and ProMedTS* surpass all baselines in F1 on both datasets, highlighting its efficiency and effectiveness.


% \subsection{Disease Diagnosis Performance}
% This study compares ProMedTS with state-of-the-arts baseline models on the MIMIC-III and MIMIC-IV datasets for disease diagnosis, using micro and macro precision, recall, and F1 scores, as summarized in Table~\ref{table1}.

% \textit{Performance of Single-Modal Models:} TimeLLM excels in disease diagnosis with lab test results, demonstrating the value of time series descriptions for LLMs. Text-based models outperform time series methods, suggesting that medical notes capture more comprehensive disease-related information and effectively represent the complexity of time series data.  LLaMA achieves the best results overall, highlighting the strength of large-scale parameters tuning with scaling laws \cite{kaplan2020scaling}.

% \textit{Advantages of Multimodal Learning:} Multimodal models, such as LDAM, FROZEN, and EHR-KnowGen, outperform single-modality models like TimeLLM, CAML, DIPOLE, and PROMPTEHR, demonstrating the effectiveness of multimodal language-time series learning for disease diagnosis.

% \textit{Superiority of Generative Models:} Single modality generative models, such as, TimeLLM and LLaMA outperform classification-based models like PatchTST and CAML. Multimodal generative models like Frozen and EHR-KnowGen surpass classification-based models such as LDAM, emphasizing the strength of generative methods in uncovering semantic relationships between inputs and outputs for enhancing prediction performance, especially in scenarios with extensive output spaces.

% \textit{Overall Performance of ProMedTS:} ProMedTS achieves the best overall performance, especially in F1 scores on MIMIC-IV.  Although LLaMA achieves comparable performance on MIMIC-III, its standard deviation is four times higher, and it requires seven times more parameters, making it less practical for downstream tasks (see Section~\ref{complexity analysis}).  Notably, Flan-T5-small-based ProMedTS surpasses all baseline models in F1 scores across both datasets, highlighting its efficacy and effectiveness in integrating multimodal EHRs for disease diagnosis through self-supervised prompt learning with pretrained LLMs.

\subsection{Ablation Studies}
% \subsubsection{Ablation Study with ProMedTS  Modules}

\subsubsection{Effect of Modality Alignment in ProMedTS}

This section presents ablation studies to evaluate each module in ProMedTS. ProMedTS \textit{w/o} LAB excludes lab test, removing modality alignment with anomaly descriptions and medical notes. ProMedTS \textit{w/o} ANOMALY removes alignment with anomaly descriptions while keeping alignment between lab test and medical notes to assess the impact of self-supervision. Table~\ref{table2} summarizes the results, showing that ProMedTS \textit{w/o} LAB  suffers a significant drop in F1 scores, highlighting the importance of lab test. ProMedTS \textit{w/o} ANOMALY also shows reduced performance, highlighting the challenges of aligning modalities from discrete and continuous encoding spaces and the adverse effects of misalignment on multimodal understanding.




% \subsubsection{Ablation Study with Loss Functions}
\subsubsection{Impact of Self-Supervised Loss Functions}

Table~\ref{table3} summarizes an ablation study on the loss functions in ProMedTS. Both ProMedTS \textit{w/o} CONTRAST and ProMedTS \textit{w/o} MATCH show slight declines in F1 scores, emphasizing the importance of $\mathcal{L}_{contrast}$ for aligning and unifying time series and textual inputs within a shared latent space. The results also underscore the role of $\mathcal{L}_{match}$ in intra-modal alignment, ensuring the distinctiveness of time series data by aligning lab test with time series prompt embeddings. Notably, ProMedTS \textit{w/o} GEN exhibits a significant drop in F1 scores, highlighting the critical role of $\mathcal{L}_{gen}$ in refining prompt embeddings and integrating temporal information from time series data and anomaly descriptions.

\begin{figure}[t]
\hspace{-0.5cm}
% \centering
\centerline{\includegraphics[scale=0.4]{Figs/computation_time.pdf}}
\caption{ The model parameters and computation time of all baselines.}
\label{complexity}
\end{figure}


% \subsection{Model Complexity Analysis} \label{complexity analysis}
\subsection{Model Efficiency and Complexity} \label{complexity analysis}
Figure~\ref{complexity} illustrates the parameter counts and computation times of baseline models on the two datasets. Our model, ProMedTS, matches the parameter counts and computation times of multimodal baselines such as LDAM and FROZEN, while using 25× fewer parameters and requiring one-third less training time than LLaMA, all while achieving superior diagnostic performance, highlighting its efficiency and effectiveness in language-time series multimodal alignment and fusion.

\begin{table}[t]
\small
\setlength\tabcolsep{18.5pt}
\renewcommand\arraystretch{1}
\centering

\begin{tabular}{c|c|c}
\toprule[1pt]
 \multicolumn{1}{c|}{$N_p$} & \multicolumn{1}{c}{\textbf{Micro F1}} & \multicolumn{1}{|c}{\textbf{Macro F1}}\\
                          \toprule[1pt]
\multicolumn{3}{c}{\textbf{ MIMIC-III}}  \\ 
\hline
 12 & 63.09$_{(0.06)}$ & 59.69$_{(0.12)}$  \\
 \rowcolor{gray!20} 24 & 63.67$_{(0.08)}$ & 60.42$_{(0.18)}$ \\
 36 & 63.32$_{(0.09)}$ &  59.96$_{(0.15)}$ \\
\hline
\multicolumn{3}{c}{\textbf{ MIMIC-IV}}  \\ 
\hline
 12 & 68.98$_{(0.15)}$ & 65.43$_{(0.18)}$  \\
\rowcolor{gray!20} 24 & 69.69$_{(0.18)}$ & 66.21$_{(0.17)}$ \\
 36 & 69.41$_{(0.19)}$ & 65.91$_{(0.20)}$ \\
\toprule[1pt]
\end{tabular}
\caption{Sensitivity analysis on different length of time series prompt embedding. }
\label{sensitivity}
\end{table}

% \subsection{Sensitivity Analysis of Varying Ratios in Loss Function Components}

% To examine the impact of different combinations of the three loss functions, $\mathcal{L}_{contrast}$, $\mathcal{L}_{match}$, and $\mathcal{L}_{gen}$, on the downstream performance, we perform a sensitivity analysis using three sets of loss ratios: 1:1:1, 1:2:2, and 1:2:1 on MIMIC-III and MIMIC-IV datasets. Since the value of $\mathcal{L}_{contrast}$ is typically larger than those of $\mathcal{L}_{match}$ and $\mathcal{L}_{gen}$, we assign greater weights to $\mathcal{L}_{match}$ and $\mathcal{L}_{gen}$. Figure~\ref{loss_sensivity} presents the results, where lines indicate the variation in the sum of the three loss functions on the testing dataset  and bars represent the Micro and Macro F1 scores. The figure reveals that varying the weight ratios of the three loss functions has minimal impact on model convergence and the performance of downstream disease diagnosis tasks.

% \subsection{Sensitivity on Time Series Prompt Embedding Length}
 \subsection{Sensitivity Analysis of Time Series Prompt Length}
We performed a sensitivity analysis to examine the impact of the time series prompt embedding length ($N_p$) on the performance of ProMedTS in disease diagnosis. Table~\ref{sensitivity} shows the F1 scores for embedding lengths of 12, 24, and 36. Slight fluctuations are observed in both micro and macro F1 scores across the MIMIC-III and MIMIC-IV datasets. The optimal embedding length is 24 for both datasets, consistent with the configuration used in our experiments.

\begin{table}[htbp]
\small
\setlength\tabcolsep{8pt}
\renewcommand\arraystretch{1}
\centering


\begin{tabular}{c|c|c}
\toprule[1pt]
 \multicolumn{1}{c|}{Lab Test Input} & \multicolumn{1}{c}{\textbf{Micro F1}} & \multicolumn{1}{|c}{\textbf{Macro F1}}\\
                          \toprule[1pt]
\multicolumn{3}{c}{\textbf{ MIMIC-III}}  \\ 
\hline
Numerical Values & 32.21$_{(1.33)}$ & 23.53$_{(1.21)}$  \\
Anomaly captions & 35.19$_{(0.92)}$ & 24.75$_{(0.76)}$ \\
Time series prompts & 36.11$_{(1.14)}$ & 25.47$_{(1.02)}$ \\
 \hline
\multicolumn{3}{c}{\textbf{ MIMIC-IV}}  \\ 
\hline
Numerical Values & 37.75$_{(1.46)}$ & 26.10$_{(1.09)}$  \\
Anomaly captions & 39.56$_{(1.14)}$ & 27.22$_{(0.77)}$ \\
Time series prompts & 40.14$_{(1.05)}$ & 28.43$_{(0.91)}$ \\
\toprule[1pt]
\end{tabular}
\caption{Micro and Macro F1 Scores Across Various Lab Test Input Types on Flan-T5 for Disease Diagnosis}
\label{tsvsanomaly}
\end{table}

% \subsection{Why Use Anomalies Captions?}
\subsection{Evaluating the Role of Anomaly Descriptions}
To highlight the advantages of using lab test anomaly captions over raw numerical time series values in LLMs, we evaluate Flan-T5-small with both input types. Table~\ref{tsvsanomaly} presents the evaluation results on the MIMIC-III and MIMIC-IV datasets for disease diagnosis. The results show that Flan-T5 achieves over a 2\% improvement in Micro F1 score when using anomaly captions, demonstrating that LLMs interpret anomaly captions more effectively than raw numerical values in time series lab test data. Additionally, the inclusion of time series prompts underscores the effectiveness of our model, ProMedTS, in capturing both fine-grained and coarse-grained temporal information from lab test results for disease diagnosis.

% \subsection{Evaluate Applicability Across LLMs of Varying Scales} 


\section{Conclusion and Future Work}

In this paper, we introduce ProMedTS, a lightweight and effective modality fusion framework that leverages self-supervised prompt learning for multimodal EHR integration. By bridging the modality gap between medical notes and lab test results, ProMedTS enables LLMs to process structured and unstructured medical data more effectively. Its three key modules and self-supervised loss functions advance language–time series integration in healthcare, providing a scalable and adaptable approach for real-world clinical applications. 
Evaluation on two EHR datasets demonstrates that ProMedTS significantly outperforms existing models in disease diagnosis, underscoring its potential to enhance clinical decision-making and improve patient care.  In future work, we plan to extend our approach to larger and more diverse datasets, explore additional LLM architectures, and investigate further improvements in modality alignment techniques.


\section*{Limitations}

While this study focuses on modality alignments and their application in downstream tasks, enhancing the explainability of disease diagnosis remains an area for future work, where we plan to incorporate the Chain-of-Thought rationale \cite{wei2022chain}. Additionally, computational constraints required the use of a relatively compact LLM, limiting the amount of clinical text processed at once, which may impact the model’s ability to leverage full medical histories. Expanding to more capable models will help address this challenge. Furthermore, our study primarily targets higher-level disease phenotypes in the International Classification of Diseases (ICD) codes \cite{slee1978international}, which could be expended to more downstream tasks. Future work will explore larger models, such as LLaMA \cite{touvron2023llama} and Mistral \cite{jiang2023mistral}, to improve diagnostic granularity and broaden coverage.


% \section*{Limitations}
% There are several ways to further improve this work:


% \begin{itemize}

% \item While this study predominantly explores modality alignments and their utilization in downstream tasks, further enhancement in explainability is an area for future development. We intend to integrate the Chain-of-Thought rationale \cite{wei2022chain} to improve the explainability of disease diagnosis within our model.

% \item Our computational resources constrained us to use the Flan-T5 small model as our language model for disease diagnosis, which supports an input limit of 512 medical note tokens. This restriction meant that longer medical notes, such as those found in databases like MIMIC-III or MIMIC-IV, were not fully accommodated, potentially affecting the model's diagnostic accuracy. We plan to address this limitation by exploring more robust models in future research.

% \item The study currently targets the higher levels of the international classification of diseases (ICD) codes \cite{slee1978international}---disease phenotypes due to the model's maximum token limitation of 512 tokens. Looking forward, we aim to employ larger language models, such as LLaMA \cite{touvron2023llama} and Mistral \cite{jiang2023mistral}, to enhance our model's diagnostic capabilities and scope.


% \item Finally, our work is currently evaluated only on the MIMIC series datasets. There are many other multimodal healthcare datasets for disease diagnosis, such as the UK Biobank \cite{sudlow2015uk} and ANDI \cite{mueller2005alzheimer}, which could be utilized for further validation of our model's effectiveness.



% \end{itemize}

\section*{Ethics Statement}

\paragraph{Data Privacy:} While the datasets utilized in our research, such as MIMIC-III and MIMIC-IV, are publicly accessible and feature de-identified patient data, accessing these datasets still requires passing the \href{https://about.citiprogram.org/}{CITI examination} and applying for the data through \href{https://physionet.org/}{PhysioNet}.



% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section*{Ethics Statement}
% Scientific work published at EMNLP 2023 must comply with the \href{https://www.aclweb.org/portal/content/acl-code-ethics}{ACL Ethics Policy}. We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
% \bibliographystyle{acl_natbib}

\appendix
\newpage
\section{Appendix}
\label{sec:appendix}



\subsection{Lab Test Anomaly Caption}
Time series anomaly descriptions are generated using the IQR method \cite{vinutha2018detection} to identify anomalies, capturing their timing and polarity (above or below standard values) and describing them with handcrafted templates. To caption the lab test anomaly into textual format, we design several text templates to describe the lab test anomalies. All templates are illustrated in Table~\ref{anomaly}.

\subsection{Algorithm}
The training procedure to optimize ProMedTS by minimizing the loss defined in Equation \eqref{eq10} is shown in Algorithm 1.
\begin{algorithm}[t]
    \caption{The ProMedTS Model}
    \label{alg:algorithm}
    \begin{algorithmic}[1]
        \STATE {\textbf{Input}: Given lab test $\bm{X}$ and medical note $\bm{M}$ denote the EHRs input. $\bm{P}$ consists of a set of learnable prompt embeddings.}
        \WHILE{not converge}
            \FOR{mini-batch $B$}
                \STATE {Obtain the time series anomaly caption $\bm{\mathcal{T}}$ using equation \eqref{eq1}.}
                \STATE {Obtain multimodal textual embedding $\bm{E}_f$ using equations \eqref{eq2} and \eqref{eq3}.}
                \STATE {Calculate the contrastive loss $\mathcal{L}_{contrast}$ between lab test, anomalies, and medical notes using equations \eqref{eq4}, \eqref{eq5}, and \eqref{eq6}.}
                \STATE {Calculate the matching loss $\mathcal{L}_{match}$ between lab test and anomalies using equations \eqref{eq7} and \eqref{eq8}.}
                \STATE {Calculate the generation loss $\mathcal{L}_{gen}$ between lab test and anomalies using equation \eqref{eq9}.}
            \ENDFOR
            \STATE{Update parameters by minimizing the total loss $\mathcal{L}_{total}$ defined in Equation \eqref{eq10} by  using the AdamW optimizer \cite{loshchilov2018decoupled} for patients in each batch.}
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}
\subsection{Baseline Models}

\begin{table*}[htbp]
\setlength\tabcolsep{2.2pt}
\renewcommand\arraystretch{1}
\centering
\begin{tabular}{l}
\toprule[1pt]
If lab test value is not an abnormal value  \\
\{Lab features\} is normal all the time.\\
\hline
If the lab test value is an abnormal value higher than the standard. \\
\{Lab features\} is higher than normal \{number of times\} times.\\
\hline
If the lab test value is an abnormal value lower than the standard. \\
\{Lab features\} is lower than normal \{number of times\} times.\\
\hline
If the lab test value is an abnormal that include both higher and lower than the standard value. \\
\{Lab features\} is higher than normal \{number of times\} times and lower than normal \{number of \\ times\} times.\\
\hline

\toprule[1pt]
\end{tabular}
\caption{Lab test anomaly caption template. }

\label{anomaly}
\end{table*}
\begin{itemize}



\item \textbf{GRU}: The Gated Recurrent Unit (GRU) \cite{cho2014learning}, a variant of recurrent neural networks (RNNs), employs two gates to capture both long-term and short-term temporal features effectively.



\item \textbf{PatchTST}: PatchTST \cite{nie2022time} is a transformer-based time series encoder designed for long-term forecasting. It segments time series into subseries-level patches, treating each as a token within the transformer architecture.

\item \textbf{TimeLLM}: TimeLLM \cite{jintime} is an LLM-based time series prediction model that reprograms input time series into text prototypes before processing them with a frozen LLM. It achieves state-of-the-art performance in mainstream forecasting tasks, particularly in few-shot and zero-shot scenarios.

\item \textbf{CAML}: The Convolutional Attention for Multi-Label classification (CAML) \cite{mullenbach2018explainable} is a classical model for classifying medical notes, incorporating a cross-attention mechanism and label embeddings to enhance interpretability. For a fair comparison with more recent language models, its original embedding layer is replaced with one from T5.

\item \textbf{DIPOLE}: DIPOLE \cite{ma2017dipole} is a classic disease prediction model that utilizes two Bi-directional RNNs. It incorporates an attention mechanism to integrate information from both past and future hospital visits. For a fair comparison with more recent language models, its original embedding layer has been replaced with one from T5.


\begin{table*}[htbp]
\setlength\tabcolsep{1.9pt}
\renewcommand\arraystretch{1}
\centering
\begin{tabular}{|l|}
\toprule[1pt]
\textbf{Diagnose disease from the following medical notes and lab test:}\\
\textbf{Medical Notes:} ms woman signficant pmh atrial fibrillation lung cancer resection congestive heart \\ 
hypertension worsening diarrhea dysuria hypotensive admitted unit presumed active issues  hypotensive \\
pronounced leukocytosis multiple potential sources ct scan peritoneal fluids free fluid started surgery ... \\
\textbf{Lab Test:} $<prefix_1>,<prefix_2>,...,<prefix_n>$\\ 
\hline
\textbf{Diagnosis:} \\
 Acute and unspecified renal failure, Fluid and electrolyte disorders, Septicemia (except in labor), Shock,\\
 Chronic obstructive pulmonary disease and bronchiectasis, Disorders of lipid metabolism, \\
 Cardiac dysrhythmias, Congestive heart failure; nonhypertensive, Diabetes mellitus with complications, \\
 Other liver diseases.\\
\toprule[1pt]
\end{tabular}
\caption{Training Instruction Template}

\label{intrusction}
\end{table*}

\item \textbf{PROMPTEHR}: PROMPTEHR \cite{wang2022promptehr} introduces a novel approach in generative models for electronic health records (EHRs), implementing conditional prompt learning. In this study, the model is specifically geared towards disease diagnosis.

\item \textbf{LLaMA}: LLaMA-7B \cite{touvron2023llama}, one of the leading large language models, is enhanced by Reinforcement Learning with Human Feedback (RLHF) and instructive tuning. It is fine-tuned for disease diagnosis in this study, demonstrating its versatility in various NLP tasks.

\item \textbf{LDAM}: LDAM \cite{niu2021label} leverages multimodal inputs, combining laboratory testing results and medical notes for disease risk prediction. It utilizes label embedding to effectively integrate these two modalities.

\item \textbf{FROZEN}: FROZEN \cite{tsimpoukelli2021multimodal} represents the cutting-edge multimodal vision-language models for few-shot learning. In our study, it is adapted to the disease diagnosis task using inputs from lab test results and medical notes.

\item \textbf{EHR-KnowGen}: EHR-KnowGen \cite{niu2024ehr}, touted as the state-of-the-art in EHR multimodal learning models, focuses on disease diagnosis generation. For this study, external domain knowledge is excluded to ensure a fair comparison.
\end{itemize}


\begin{figure}[t]
\hspace{-0.5cm}
% \centering
\centerline{\includegraphics[scale=0.5]{Figs/loss_sensivity.pdf}}
\caption{Sensitivity analysis of varying ratios in loss function components.}
\label{loss_sensivity}
\end{figure}



\subsection{Implementation Details}
In experiments, we utilized PyTorch framework version 2.0.1, operating on a CUDA 11.7 environment. We employed the AdamW optimizer with a starting learning rate of $1e^{-5}$ and a weight decay parameter of 0.05. Additionally, we implemented a warm-up strategy covering 10\% of the training duration. Our experiments were conducted on high-performance NVIDIA Tesla V100 GPUs. Within the ProMedTS model, we used 24 time series prompt embeddings, each with a dimensionality of 768. The model's hidden layer size was maintained at 768 for modality alignment and adjusted to 512 for downstream tasks. To standardize the time series data input, we padded all lab test results to a uniform length of 1000 time steps, allowing us to divide the data into 125 patches, with each patch containing 8 time steps. The Flan-T5 is fine-tuned on two MIMIC datasets \cite{johnson2016mimic,johnson2023mimic} and then frozen for downstream tasks. 

\subsection{Training Instruction Template}
Table~\ref{intrusction} illustrates the training instruction template for our model ProMedTS for disease diagnosis on MIMIC-III and MIMIC-IV datasets.

\subsection{Sensitivity Analysis of Varying Ratios in Loss Function Components}

To examine the impact of different combinations of the three loss functions, $\mathcal{L}_{contrast}$, $\mathcal{L}_{match}$, and $\mathcal{L}_{gen}$, on the downstream performance, we perform a sensitivity analysis using three sets of loss ratios: 1:1:1, 1:2:2, and 1:2:1 on MIMIC-III and MIMIC-IV datasets. Since the value of $\mathcal{L}_{contrast}$ is typically larger than those of $\mathcal{L}_{match}$ and $\mathcal{L}_{gen}$, we assign greater weights to $\mathcal{L}_{match}$ and $\mathcal{L}_{gen}$. Figure~\ref{loss_sensivity} presents the results, where lines indicate the variation in the sum of the three loss functions on the testing dataset  and bars represent the Micro and Macro F1 scores. The figure reveals that varying the weight ratios of the three loss functions has minimal impact on model convergence and the performance of downstream disease diagnosis tasks.




% \subsection{Qualitative Result on Modality Visualization}

% To illustrate the bridging role of time series prompts between lab test results and textual medical notes, we analyzed the t-SNE visualizations for MuTA and its ablation model, MuTA-II, depicted in Figures~\ref{tsne}(a) and (b). Figure~\ref{tsne} (a) shows that the embeddings of medical notes and lab test results overlap in the lower left, while a significant portion of the lab test embeddings remains distant on the right, supporting our hypothesis that information from lab tests and medical notes can be consistent or complementary. While, time series prompts, situated between these embeddings, emphasizing their bridging function. Figure~\ref{tsne}(b) shows merged embeddings due to contrastive learning between medical notes and lab test result. Furthermore, time series prompts are closer to lab test, suggesting that the prompts primarily learn from lab tests, also indicating the challenges involved in enabling time series prompts to assimilate multimodal information.


% \begin{figure}[t]
% \centering
% \centerline{\includegraphics[scale=0.31]{Figs/computation_time.pdf}}
% \caption{ The model parameters and computation time of all baselines on MIMIC-III and MIMIC-IV datasets for disease diagnosis.}
% \label{complexity}
% \end{figure}

% \subsection{Model Complexity Analysis}
% Figure~\ref{complexity} displays the model parameter counts and computation times of baseline models on the MIMIC-III and MIMIC-IV datasets. Notably, LLaMA has the highest number of parameters, approximately 7 billion, necessitating the longest training duration. Moreover, almost all baseline models require more computation time on the MIMIC-IV dataset due to its larger data volume. In comparison, our model, MuTA, exhibits similar parameter counts and computation times as multimodal baselines like LDAM and FROZEN on both datasets, yet demonstrates superior diagnostic performance, underscoring its efficiency and effectiveness.


\end{document}
