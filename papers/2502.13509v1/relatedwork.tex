\section{Related Work}
The increasing diversity of EHR data has led to significant advancements in multimodal learning for healthcare applications. MedCLIP \cite{wang2022medclip} employs semantic contrastive learning to align medical images with textual reports, while RAIM \cite{qiao2019mnn} and GLoRIA \cite{huang2021gloria} integrate numerical and image data with text using attention mechanisms. LDAM \cite{niu2021label} further extends these approaches by leveraging cross-attention with disease labels to fuse features from lab tests and clinical notes. EHR-KnowGen \cite{niu2024ehr} transforms structured lab data into text and incorporates external knowledge for improved modality fusion. Despite these advancements, achieving a unified latent embedding that effectively captures interactions across diverse modalities remains a key challenge in multimodal EHR processing.

Beyond multimodal learning, recent research has explored generative approaches to healthcare modeling. Conventional methods have primarily relied on discriminative models for disease risk assessment and diagnosis \cite{choi2016retain,niu2021lerp,qiao2019mnn}. However, generative models are increasingly being adopted, as demonstrated by Clinical CoT \cite{kwon2024large}, applying LLMs for disease diagnosis generation. Reinforcement learning from human feedback (RLHF) \cite{ouyang2022training} and Chain-of-Thought (CoT) prompting \cite{wei2022chain} have further enhanced medical reasoning capabilities in models such as GatorTron \cite{yang2022large}, MedPalm \cite{singhal2023large}, and GPT4-Med \cite{nori2023capabilities}. While these models excel in medical question-answering, they remain limited in real-world direct disease diagnosis and multimodal EHR processing. EHR-KnowGen \cite{niu2024ehr} reframes disease diagnosis as a text-to-text generation problem but overlooks the crucial temporal details embedded in time series lab tests, underscoring the need for more effective and dedicated multimodal fusion strategies.

%