\section{Related Work}
This section positions our work in the literature on stowage planning, stochastic programming and ML in optimization.  

\textbf{Stowage Planning.} To address the complexity of stowage planning, various solution approaches have been applied to different problem formulations, including exact methods \cite{roberti_decomposition_2018}, linearly relaxed MIP \cite{pacino_fast_2011}, matheuristics \cite{parreno-torres_solving_2021}, population-based metaheuristics \cite{chang_solving_2022}, neighbourhood-based metaheuristics \cite{pacino_crane_2018}, hybrid frameworks \cite{bilican_mathematical_2020}. 
Despite these techniques, a recent survey highlights that scalable solutions for representative stowage and MPP problems remain unsolved \cite{van_twiller_literature_2024}. Research has largely focused on deterministic problems, overlooking real-world needs for profit maximization in the face of demand uncertainty. 

\textbf{Stochastic Programming.} Decision-making under uncertainty is traditionally approached through stochastic programming, where uncertainty is explicitly represented as a set of discrete scenarios evolving over multiple stages in time, approximating the underlying probability distribution \cite{birge_introduction_2011}. This process results in a scenario tree, whose computational complexity grows exponentially as $\mathcal{O}(b^T)$, where $b$ is the branching factor and $T$ is the number of stages. Hence, multi-stage problems with $T > 2$ and a reasonably large $b$ are often intractable. Common techniques to overcome this complication are scenario reduction to reduce problem size \cite{watanabe_scenario_2009}, decomposition techniques like Benders decomposition or Lagrangian relaxation to divide the problem \cite{rahmaniani_benders_2017}, progressive hedging to enforce non-anticipativity iteratively \cite{boland_combining_2018}, or approximation methods, such as stochastic dual dynamic programming \cite{shapiro_analysis_2011} and sample average approximation \cite{chen_sample_2022}, to enhance tractability while preserving solution quality.

\textbf{Learning with Hard Constraints.} In learning solution heuristics, a challenge arises to ensure feasibility in complex action spaces. This challenge is exacerbated when dealing with the dependence on state variables to define the feasible region. Several deep learning approaches have dealt with learning constraints by backpropagation through (in)equality completion \cite{donti_dc3_2021}, or differentiable projection layers, such as mapping interior points to boundaries \cite{li_learning_2023},  convex programming layers \cite{agrawal_differentiable_2019}, or problem-specific repair layers \cite{chen_end--end_2024}. Moreover, safe reinforcement learning has dealt with constraints by, e.g., constrained MDPs with a primal-dual approach \cite{ding_natural_2020}, soft barriers function \cite{wang_enforcing_2023}, and safety shields \cite{alshiekh_safe_2018}.