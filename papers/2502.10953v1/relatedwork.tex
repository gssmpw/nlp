\section{Related work}
Several research efforts have explored ways to improve configuration challenges processes through automation and tool support.

The work of Duc et al. \cite{10.1145/3120459.3120471} examined the main failure patterns of smart home systems, this work focuses on hardware failure(wireless link loss, battery damage, power outage). Chen et al. \cite{Chen2017ApplicationOF} analyzed the fault symptoms and provided maintenance suggestions after modeling the IoT system with four layers: application, storage, communication, and data.

In the realm of natural language processing for code, \cite{10.1145/3212695} surveyed various techniques for learning from source code, including approaches for code summarization and bug detection. Their work provides a foundation for our use of NLP and machine learning to analyze code changes and generate human-readable comments. \cite{article}

V. J. HELLENDOORN ET AL\cite{hellendoorn2020global} explored the use of machine learning models(RNN, Transformer, GGRN etc.)\cite{bani2021deep,han2021transformer,tkachenko2020approach} for code completion and bug detection. Their findings on the effectiveness of transformer-based models for code understanding tasks inform our choice of model architectures for comment generation. However, they did not consider any large language model for analysis.

A few recent studies have focused on identifying and characterizing issues in IoT systems \cite{8835392}\cite{10.1145/3290605.3300782}. Makhshari and Mesbah \cite{9402092} conducted interviews and surveys with IoT developers, revealing that testing and debugging are the primary challenges faced. However, their work does not address configuration issues. Brackenbury et al. \cite{10.1145/3290605.3300782} focused on the trigger-action programming (TAP) model. They analyzed and systematized temporal paradigms within TAP systems. Their work identified that TAP systems express rules and categorized TAP programming bugs into three main types: control logic errors, timing issues, and bugs arising from inaccurate user expectations. Ahmad et al. \cite{10462177} designed and implemented a framework to quantitatively evaluate how effectively an LLM can fix specified bugs related to hardware security.

Focusing on root causes, fixes, triggers, and impacts Wang et al. \cite{10.1145/3533767.3534365} analyzed 330 device integration bugs from HAC. This work is similar to our work, rather than addressing device integration, our study concentrates on coding issues in automation configuration.

S. M. H. Anik et al. \cite{Anik2018} conducted a comprehensive study on the challenges and opportunities in programming automation configurations for smart home systems \cite{inproceedings}. Their work provides valuable insights into the complexities of setting up and maintaining smart home devices, which has informed our approach to developing AI-assisted configuration tools. Our study was conducted using the dataset extracted in their work.

In the realm of IoT security, the Bitdefender 2024 IoT Security Landscape Report\cite{Bitdefender} offers crucial insights into the evolving threats facing smart homes. This report highlights the need for robust security measures in smart home configurations, which our AI-assisted approach aims to address.
 
Although none of these previous works focused on the evaluation of LLMs in predicting
fixes of Configuration bugs in Smart Home System, these insights help us understand the problem fully and design our solution design considering it.