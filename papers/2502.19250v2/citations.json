[
  {
    "index": 0,
    "papers": [
      {
        "key": "o2023open-x",
        "author": "O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others",
        "title": "Open x-embodiment: Robotic learning datasets and rt-x models"
      },
      {
        "key": "khazatsky2024droid",
        "author": "Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and others",
        "title": "Droid: A large-scale in-the-wild robot manipulation dataset"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kim24openvla",
        "author": "{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      },
      {
        "key": "[pi0",
        "author": "Kevin Black and Noah Brown and Danny Driess and Adnan Esmail and Michael Equi and Chelsea Finn and Niccolo Fusai and Lachy Groom and Karol Hausman and Brian Ichter and Szymon Jakubczak and Tim Jones and Liyiming Ke and Sergey Levine and Adrian Li-Bell and Mohith Mothukuri and Suraj Nair and Karl Pertsch and Lucy Xiaoyang Shi and James Tanner and Quan Vuong and Anna Walling and Haohuan Wang and Ury Zhilinsky",
        "title": "$\\pi_0$: A Vision-Language-Action Flow Model for General Robot Control"
      },
      {
        "key": "ecot",
        "author": "Micha\u0142 Zawalski and William Chen and Karl Pertsch and Oier Mees and Chelsea Finn and Sergey Levine",
        "title": "Robotic Control via Embodied Chain-of-Thought Reasoning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "karamcheti2024prismatic",
        "author": "Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa",
        "title": "Prismatic vlms: Investigating the design space of visually-conditioned language models"
      },
      {
        "key": "llava1.5",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kim24openvla",
        "author": "{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      },
      {
        "key": "wen2024tinyvla",
        "author": "Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and others",
        "title": "Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "tobin",
        "author": "Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter",
        "title": "Domain randomization for transferring deep neural networks from simulation to the real world"
      },
      {
        "key": "james2019sim",
        "author": "James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos",
        "title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "finn2017model",
        "author": "Finn, Chelsea and Abbeel, Pieter and Levine, Sergey",
        "title": "Model-agnostic meta-learning for fast adaptation of deep networks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "laskin2020reinforcement",
        "author": "Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind",
        "title": "Reinforcement learning with augmented data"
      },
      {
        "key": "kostrikov2020image",
        "author": "Kostrikov, Ilya and Yarats, Denis and Fergus, Rob",
        "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "stone2023open",
        "author": "Stone, Austin and Xiao, Ted and Lu, Yao and Gopalakrishnan, Keerthana and Lee, Kuang-Huei and Vuong, Quan and Wohlhart, Paul and Kirmani, Sean and Zitkovich, Brianna and Xia, Fei and others",
        "title": "Open-world object manipulation using pre-trained vision-language models"
      },
      {
        "key": "zhu2024vision",
        "author": "Zhu, Yifeng and Lim, Arisrei and Stone, Peter and Zhu, Yuke",
        "title": "Vision-based Manipulation from Single Human Video with Open-World Object Graphs"
      },
      {
        "key": "codeaspolicy",
        "author": "Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy",
        "title": "Code as policies: Language model programs for embodied control"
      },
      {
        "key": "ahn2022saycan",
        "author": "Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others",
        "title": "Do as i can, not as i say: Grounding language in robotic affordances"
      }
    ]
  }
]