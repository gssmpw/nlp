\section{Introduction}

Vision-language-action (VLA) models have emerged as a transformative paradigm for teaching robots dexterous skills, enabling them to replicate human behavior and master complex tasks~\cite{brohan2022rt-1,rt-2, [pi0, pertsch2025fast,ecot}. However, a critical limitation persists: these models rely heavily on human demonstration data, which constrains their scalability and practicality in dynamic real-world environments~\cite{kim24openvla,wen2024tinyvla,wen2024diffusionvla}. For instance, a robot trained to execute “hand over the apple” often fails to generalize to analogous tasks like “hand over the peach,” despite conceptual similarity. This underscores the unresolved challenge of \textbf{object generalization} — adapting learned skills to novel, unseen objects — particularly when such objects lie \textbf{beyond the category of the teleoperated training data}. We name these objects as out-of-distribution (OOD) objects.

The core limitation stems from imitation learning’s tendency to learn fixed mappings from instruction and visual input to action. When encountering objects absent from teleoperation data, the model lacks mechanisms to associate the object’s name, visual features, and learned actions. To address this, we propose a framework that bridges visual-language semantics and robotic actions through localization-aware reasoning.

Our approach begins by curating a dataset of visual-textual pairs augmented with localization metadata (e.g., bounding boxes). This dataset is co-finetuned with teleoperated robot interaction data, while the robot data itself is enriched with localization-guided reasoning. By embedding localization as a bridging representation, we create a unified pathway between visual-language inputs and robotic actions. This enables zero-shot object generalization: the model can recognize and manipulate novel objects—even those absent from robot training data—without task-specific retraining.

We designed rigorous real-robot experiments to validate the generalization capabilities of our framework, ObjectVLA. In these trials, six objects are positioned at distinct locations (left or right side of a table), with configurations spanning combinations of objects seen in robot interaction data or vision-language data. The robot is tasked with the instruction “move to the {object}”, achieving a 100\% success rate for in-domain objects. To stress-test generalization, we evaluated 100 OOD objects, observing a 64\% success rate. These experiments demonstrate that our method adapts to diverse novel object types when trained with vision-language priors. 

The versatility of our approach is further demonstrated across diverse scenarios, including bin-picking and tasks requiring composite skills like pushing and rotating. Notably, our framework supports rapid adaptation to novel objects: by collecting smartphone-captured images and performing lightweight fine-tuning, the model generalizes to objects absent from the original dataset. An example is illustrated in Figure~\ref{fig:smartphone_exp}. These experiments underscore our method’s ability to reduce reliance on large-scale human demonstrations while achieving robust object generalization.

Our primary contribution is a unified pipeline for integrating vision-language datasets with robot interaction data, enabling end-to-end object generalization. Through systematic evaluation, we validate the framework’s performance on complex multi-stage tasks (e.g., bin-picking) and multi-skill manipulation (e.g., rotating, pushing), highlighting its universality. Despite some of the existing works, such as RT-2~\cite{rt-2} and ECoT~\cite{ecot} giving a glimpse of how co-finetuning can achieve simple object generalization, they neither elucidate the underlying mechanism of achieving such generalization nor address the boundary of their methodologies. In contrast, our approach — though simple and straightforward — demonstrates that training VLA models with a hybrid dataset of robot interaction data and visual-textual data significantly enhances generalization. This level of generalization goes significantly beyond previously demonstrated end-to-end approaches. Crucially, our framework enables practical deployment: even a small set of smartphone images and brief fine-tuning suffices to adapt the model to novel objects, significantly advancing real-world robotic flexibility.

