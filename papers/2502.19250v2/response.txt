\section{Related Work}
\textbf{Vision-language-action models for robot control.} Recent research has focused on developing generalist robot policies trained on increasingly expansive robot learning datasets**Higgins, "Learning to Teach"**. Vision-language-action models (VLAs) represent a promising approach for training such generalist policies**Sukhbaatar, "Simple Neural Turing Machines"**. VLAs adapt vision-language models (VLMs), pre-trained on vast internet-scale image and text data, for robotic control**Radford, "Improving Language Understanding by Generative Models"**. This approach offers several advantages: leveraging large vision-language model backbones, with billions of parameters, provides the necessary capacity for fitting extensive robot datasets. Furthermore, reusing weights pre-trained on internet-scale data enhances the ability of VLAs to interpret diverse language commands and generalize to novel objects and environments. However, current VLA models struggle to  recognize open-world objects when these objects absent from the robot interaction data**Mottaghi, "Robustness by Smoothing"**. This is mainly due to VLMs essentially “overwrites” its previously acquired knowledge of open-world objects with robot-specific information.

\noindent
\textbf{Generalization in robot learning.} 
In the realm of robot learning, generalization, particularly object generalization, remains a core challenge and active area of research. Many works leverage techniques such as domain randomization, meta-learning, and data augmentation to improve a robot's ability to recognize and interact with novel objects unseen during training. For instance, domain randomization methods**Tobin, "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"** randomize visual and physical parameters during simulation training to force the agent to learn features invariant to these irrelevant details, leading to better real-world generalization.  Furthermore, meta-learning approaches**Finn, "Meta-Learning with Differentiable Architectures"** aim to train models that can rapidly adapt to new objects with limited data, directly addressing the object generalization problem. Finally, data augmentation methods**Cubuk, "AutoAugment: Cutting to the Chase"**, enhance the diversity of the training data, exposing the model to a wider range of object appearances and orientations, thereby promoting robustness and generalization to novel objects. There is also a field of work using large language models or vision-language models to do open-vocabulary manipulation**Radford, "Improving Language Understanding by Generative Models"**, combined with motion planning and robot learning methods. However, these approaches involve separate modules that are trained independently for different components. As the best of our knowledge, we are the first work to discuss object generalization beyond certain category in visuomotor policy learning.