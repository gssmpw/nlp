\section{Experiments}
In this section, we examine the effectiveness of ObjectVLA for object generalization in embodied control. In section~\ref{sec:validate}, we verify the effectiveness of our method in object generalization. In section~\ref{sec:skills} and~\ref{sec:bin-picking}, we illustrate how our model transfers skills to objects not present in robot interaction data but included in the vision-language corpus. In section~\ref{sec:smart-phone}, we show that even a small set of smartphone images and brief fine-tuning can effectively adapt the pretrained model to novel objects.

\noindent
\textbf{Real robot setup.} All experiments are conducted on a Franka robot~\cite{haddadin2024franka} equipped with a 7-degree-of-freedom arm and a gripper. We use two external ZED cameras and a wrist Realsense 435i camera to obtain real-world visual information. Our real-world robot setup is illustrated in Figure~\ref{fig:robot_setup}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/objectvla_vl_example.pdf}
    \caption{\textbf{Example of constructed visual-text data.} \textit{Left:} Photo taken by the robot's camera. \textit{Right:} Object captured with a smartphone.}\label{fig:examples}
\end{figure*}






\subsection{Validating Object Generalization}
\label{sec:validate}
In this section, we conduct rigorous experiments to verify the object generalization capability of our method. We begin by describing the experimental setup and evaluation criteria. Next, we evaluate ObjectVLA on both in-distribution and out-of-distribution objects. Finally, we explore several interesting observations related to object generalization.
\subsubsection{Experimental Setup}
To verify the object generalization capability, we begin with a simple yet effective task, ``Move to the object.''. In this task, we position objects on both sides of the robot, ensuring that each side has at least three objects on the table. The model is required to move toward the target object based on the given instruction. These objects are randomly chosen from a diverse set. For in-distribution (ID) evaluation, objects are only selected from the robot's training data. And, for out-of-distribution evaluation, objects are randomly selected from either the robot's training data or the vision-language data. A complete list of objects from both datasets is provided in the Appendix.

\noindent
\textbf{Evaluation criterion.} We evaluate each object over 4 trials, with the target area's side switching every two trials. We consider the model to have successfully recognized a novel object if and only if it moved toward the target object in all four trials. This criterion ensures that the model cannot achieve success simply by chance. 



\noindent
\textbf{Experimental Results.} Figure~\ref{fig:move} presents the real-world experimental results for the "Move" task. Our ObjectVLA achieves a 100\% success rate in ID evaluation. In the stress-test evaluation, our model successfully recognizes 64\% of objects that are not present in the robot interaction data, confirming the effectiveness of co-training robot data with localization metadata.

\noindent
\textbf{Ablation study.} To further understand our method's effectiveness, we conducted an ablation study. We found that object generalization relies heavily on two key factors: first, explicitly linking vision and language to action through bounding boxes. This provides a direct connection between the visual object, its linguistic description, and the required manipulation. Second, designing a reasoning process in the robot data that mirrors the structure of vision-language pair data. This allows the model to leverage the rich information encoded in pre-trained vision-language models.

To analyze the impact of these factors, we removed the reasoning module for robot data and eliminated bounding boxes for vision-language data. The VLA model is then co-finetuned with vision-language data and evaluated using the same criteria and test settings as our full method.

As illustrated in Figure~\ref{fig:move}, the model without bounding boxes achieves only a 19\% success rate in OOD evaluation, representing a significant performance decline compared to our method, despite achieving a 100\% success rate in the ID test. This suggests that without explicit grounding and a structured reasoning process, the model struggles to differentiate objects in vision-language data, leading to confusion about object-instruction correspondence and appropriate action selection.

\subsubsection{More Observations}

\textbf{Can VLA recognize unseen objects if only trained with teleoperated data?} To further assess the importance of vision-language data, we evaluated a VLA model trained exclusively on robot data, without any vision-language co-finetuning. As shown in Figure~\ref{fig:move}, this model (DiVLA) achieved 8\% accuracy, which is almost equivalent to random guessing. This stark outcome highlights the critical role of vision-language data in multimodal understanding.

While the VLA model's backbone is pre-trained on internet-scale vision-language data, focusing solely on robot data during training leads to catastrophic forgetting. The model essentially ``overwrites'' its previously acquired knowledge of visual concepts with robot-specific information, hindering its ability to comprehend multimodal scenes. Consequently, even objects encountered during pre-training, such as Pikachu, remain unrecognizable to the VLA model without vision-language co-finetuning.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/objectvla_main_objects.pdf}
    \caption{\textbf{Example Objects Used in Experiments.} \textit{Left:} Objects present in the robot training data. \textit{Right:} Examples of novel objects, not present in the robot data, but included in the visual-text co-training dataset (see Appendix for a comprehensive list).}\label{fig:seen-objs}
\end{figure*}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/move_exp.pdf}
    \caption{\textbf{Validation experiments on object generalization.} 
Our method achieved the best performance in both the in-distribution test setup and under visual changes. Each object is evaluated across 4 trials. We report the number of objects that were correctly identified in all four trials.}
    \label{fig:move}
\end{figure}

\subsection{Combining with More Skills}
\label{sec:skills}
While the previous section employed a simple ``move to'' demonstration to validate the fundamental approach of our method, this section expands the evaluation to encompass more complex skills, specifically "push" and "rotate." This broader assessment aims to demonstrate the generalizability of our method and its applicability beyond the "move to" task.
\begin{table}[t]
\centering
\vspace{0.1cm}
\caption{\textbf{Experimental Results for rotate and push skills.} Our proposed ObjectVLA achieves high performance on both 5 in-distribution objects and 20 out-of-distribution objects. Each object is evaluated with three trials. We report the number of success trials.}
\label{tab:rotate}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{c|c|c}
\toprule
\textbf{Task}  & In-Distribution & Out-of-Distribution \\
\midrule
Rotate & 13/15  & 39/60    \\
Push   & 12/15  & 52/60    \\ 
\bottomrule
\end{tabular}}
\vspace{-0.5cm}
\end{table}

\noindent
\textbf{Experimental setup.} In this experimental setup, we placed three objects in front of the robot: one on the center, one on the right, and one on the left. The robot is instructed to either "rotate the object counterclockwise" or "push the object forward," as illustrated in Figure~\ref{fig:robot_setup}. Following previous setup, we evaluate the model's performance for both in-distribution (ID) and out-of-distribution (OOD) objects. Recognizing that some objects are inherently unsuitable for rotation or pushing actions (e.g., dishes), we conducted experiments on a curated set of 5 ID objects and 20 OOD objects. For each object in a skill, 40 demonstrations were collected, resulting in a total of 400 demonstrations.

\noindent
\textbf{Implementation details.} We train one model for each skill to ensure that the model focuses more on understanding the objects rather than multi-task learning. 
We use the same visual-textual data of ``move'' task. Following established protocols from our prior work, this visual-text dataset trained concurrently with the demonstration data for comprehensive evaluation. Each object was tested with 3 trials. In total, 150 trials were conducted. The training setting is provided in Appendix.

\noindent
\textbf{Results.} As shown in Table~\ref{tab:rotate}, our method achieved high success rates on the robot interaction objects for both rotate and push skill. Analysis of the failed ``rotate'' trials revealed that the primary cause is the model's inability to grasp the target object securely. When evaluating performance on out-of-distribution (OOD) objects, we observed a decrease in task completion rates compared to in-distribution objects, as expected. However, the model still successfully completed nearly two-thirds of the trials. Notably, in most failure cases, the model did not incorrectly identify the target object but rather failed to execute the skill completely. This was particularly evident in the ``rotate'' trials, where successful execution hinges on a secure grasp, a challenging requirement for unseen objects. Nevertheless, these experiments strongly support the claim that ObjectVLA can transfer learned skills, beyond basic pick and place, to novel objects within the framework we have developed. The results underscore the potential of ObjectVLA for generalized robotic manipulation, capable of adapting to new objects and tasks beyond its initial training.


\subsection{Instruction-Driven Bin Picking}
\label{sec:bin-picking}
\begin{table}[t]
\centering
\vspace{0.1cm}
\caption{\textbf{Experimental results for bin picking.} Our proposed ObjectVLA achieves high performance on both 11 in-distribution objects and 50 out-of-distribution objects, with each object evaluated across 3 trials. We report the number of successful trials over total trials.}
\label{tab:bin-picking}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{c|c|c}
\toprule
\textbf{Method}  & In-Distribution & Out-of-Distribution \\
\midrule
OpenVLA    & 14/33  &  17/150 \\
ObjectVLA &  \textbf{21/33}  &  \textbf{87/150}\\
\bottomrule
\end{tabular}}
\vspace{-0.5cm}
\end{table}
To further evaluate ObjectVLA, we conducted experiments in a more practical scenario: end-to-end instruction-driven bin-picking. Unlike prior works (e.g., GR-2~\cite{cheang2024gr2} and DiVLA~\cite{wen2024diffusionvla}) that execute bin-picking tasks without specific semantic instructions—typically limited to generic actions like transferring all objects from one container to another—we focus on a significantly more challenging setting~\cite{cheang2024gr2,wen2024diffusionvla}. In our experiments, the robot is required to identify and retrieve a specific target object based on natural language instructions (e.g., "Pick the hexagonal bolt from the bin"). This scenario elevates the complexity of conventional bin-picking tasks by integrating cross-modal understanding (vision-to-language alignment) and fine-grained object discrimination that have multiple objects in the scene. Notably, the objects are randomly placed on the panel, which is a large area. Not only the model need to figure out the object's position, but also need to aware of its pose. 

\noindent
\textbf{Implementation details.} We collected new data within this environment.  For robot interaction data, we collected 600 pick-and-place trajectories using the same "seen" objects as in previous experiments.  For visual-text data, we used half the number of objects from previous experiments, capturing 20 images of each. We compared our method against OpenVLA, a state-of-the-art VLA model, reporting success rates for both in-distribution and out-of-distribution objects. Evaluation consisted of three trials per object, totaling 183 trials per method. In each trial, at least two objects were randomly placed on the plate, and the model was instructed to pick and place a specific object according to the given instruction.

\noindent
\textbf{Results.} Table \ref{tab:bin-picking} presents our experimental results.  Bin picking, requiring object retrieval from random positions and poses, poses a significant challenge even for in-distribution objects.  OpenVLA achieves a success rate of only 42.4\% for in-distribution objects, significantly less than half. Surprisingly, it still completed roughly 10\% of trials with out-of-distribution objects. This is likely due to some test objects sharing attributes with training objects (e.g., bread resembling a muffin, a green mug differing only in color from a brown training mug).  In contrast, our method successfully completed 87 of 150 trials, including many completely novel objects, a 46.7\% improvement over OpenVLA. This further emphasizes the necessity of co-training with both robot interaction and visual-text data for effective object generalization.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/smart-phone-exp.pdf}
    \caption{\textbf{Experimental results for smart-phone captured objects and trained by continual learning.} We test two new objects. We take pictures of these two objects via smart phone, and continually trained the objects on pre-trained model. Each object evaluated across 10 trials. We report the success rate for each object.}\label{fig:smart-phone}
\end{figure}

\subsection{Cheap Object Generalization via Smart-Phone Pictures and Continual Learning}
\label{sec:smart-phone}


The previous section demonstrated that our proposed co-training strategy enables the imitation learning method to generalize to any object by constructing corresponding visual-text data for each object. This approach significantly enhances the model's ability to handle a broader range of objects, without requiring extensive retraining. However, there are two key limitations that need to be addressed.

First, when a new object is introduced, the model must be trained from scratch to incorporate the new object into its understanding. This process can be highly cost-inefficient, as it involves retraining the model every time a novel object is added. In real-world applications, where objects are frequently introduced or changed, this limitation could significantly slow down deployment and increase operational costs. Second, our current image data is collected using cameras mounted on the robot, which ensures that there is no visual gap between the images captured by the robot's cameras and the images input into the model. This setup works well in controlled environments but presents challenges in real-world scenarios. For instance, in order to capture the same images as the robot sees, you would need to replicate the exact camera positions and angles of the robot’s setup. This is not only cumbersome but also expensive, as it requires building an identical system with matching camera views. Moreover, in environments where the robot is mobile or the scene is dynamic, maintaining consistent camera alignment becomes even more difficult.

Therefore, in this section, we test a simpler and more cost-effective approach: using a smart-phone camera to collect images from various perspectives. The model is then continuously trained on the pre-trained weights using this more accessible data collection method. As shown in Figure~\ref{fig:smart-phone}, we test with two objects: Pikachu and a brown toy cat. For each object, we capture 21 images and follow the same data construction pipeline discussed earlier. We train these objects in bin-picking environment. Our results demonstrate that the model is able to recognize and successfully grasp the objects with a high success rate, 80\% for Pikachu and 90\% success rate for toy cat. More importantly, we only need to continue training the model for 1 epoch. Because the collected data size is small, the training process can be extremely fast that can be finished up to ten minutes. This validates the effectiveness of our approach, showing that simple smartphone image collection combined with continuous learning enables open-world object manipulation in an end-to-end model. This experiment demonstrates that our method is flexible and cost-effective, making it a plug-and-play solution for existing VLA models, enabling them to generalize to virtually any object.