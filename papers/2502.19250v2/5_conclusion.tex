\section{Conclusion}
In this work, we present ObjectVLA, a Vision-Language-Action framework that addresses object generalization in robotic manipulation. By integrating vision-language datasets with robot interaction data, our method establishes a unified pipeline that bridges semantic understanding and physical action execution. This enables zero-shot generalization to over 100 novel objects with a 64\% success rate, even when objects differ in category, appearance, or fine-grained attributes (e.g., color, shape). Our framework demonstrates that lightweight co-training with visual-textual priors and localization-aware reasoning can unlock robust cross-modal alignment. Key to our success is the ability to adapt rapidly to real-world scenarios: using just a few smartphone-captured images and quick continual finetuning, robots generalize to unseen objects without costly human demonstrations. We validate our approach across diverse tasks—including bin-picking, rotating and pushing—showcasing its versatility and practicality. Our results highlight a path toward scalable robotic learning systems that reduce dependence on large-scale teleoperation data while maintaining high performance.

