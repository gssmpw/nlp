\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/objectvla_setup.pdf}
    \caption{\textbf{Robot setup and examples for real-world manipulation tasks.} We evaluate ObjectVLA with 4 skills on a Franka robot arm equipped with two external Zed cameras and a Realsense 435i wrist camera.}\label{fig:robot_setup}
\end{figure*}




\section{Methodology}
\subsection{Notation and Motivation}
Given a set of expert demonstrations that contain complex robot skill trajectories, we want to learn a visuomotor policy $\pi : \{\mathcal{O}_{r}, \mathcal{I}_{r}\} \mapsto \mathcal{A}$ that maps the visual observations $o_{r} \in \mathcal{O}_{r}$ and the language instruction $i_{r} \in \mathcal{I_{r}}$ to actions $a \in \mathcal{A}$. The action changes accordingly when the language instruction and visual input change. The $r$ denote for data in human demonstration data. Typically, for each language instruction it contains robot skill such as "push" or "pick up" and the target object, which is denoted as $\{obj_{r}, skill_{r}\} \in i_{r}$. We then formally define the visual-text data, where $\varphi : \{\mathcal{O}_{v}, \mathcal{I}_{v}\} \mapsto \mathcal{L}_{v}$, where we input the image $o_{v} \in \mathcal{O}_{v}$ and give a language instruction $i_{v} \in \mathcal{I}_{v}$, the model is output with the corresponding answer $l_{r} \in \mathcal{L}_{v}$. The notation $v$ denotes visual-text data.

In this work, we explore the generalization of objects, focusing on those that are not part of the robot interaction data but are present in visual-textual data.

\subsection{Data Construction}


\textbf{Visual-textual data construction.} To explore the model's ability to generalize to novel object, we constructed a diverse visual-textual dataset. For the visual component, we collected 100 distinct objects that are not included in the robot interaction objects. Specifically, using three cameras mounted on the robot (see Figure~\ref{fig:robot_setup}), we captured 20 images per object, covering various poses and orientations to ensure diversity. For the textual component, we employ a fixed template, ``Detecting the bounding box of object.'', as the question, and the corresponding bounding box as the answer. In total, our vision-language dataset comprises 2,000 visual-textual pairs.

\noindent
\textbf{Reasoning data construction.}
We utilize localization metadata to bridge the gap between visual-text data and robot data, as previously mentioned. To establish this implicit link between visual-text and action, we incorporate localization metadata into the robot data. This section details how we construct reasoning with localization for robot data.

For each task, we first identify target objects based on the language instructions. We then employ DinoX~\cite{ren2024dino}, a cutting-edge open-vocabulary object detector, to annotate the bounding boxes of these objects. DinoX can generate a bounding box given an object's name. To ensure accuracy, we manually verify and correct any erroneous bounding boxes produced by DinoX. Since our workspace has two external camera views, which can result in different bounding boxes for the same object, we annotate only one (right camera in our experiments). Following Qwen2-VL~\cite{wang2024qwen2}, we use a fixed template, ``<|object\_ref\_start|>{object}<|object\_ref\_end|><|box\_start|
$(x_{1}, y_{1})$,$(x_{2}, y_{2})$<|box\_end|>.'', to represent the localization reasoning. 
This reasoning is generated before each action and injected into the policy model through a learnable module. For a detailed explanation of this injection module's architecture, we refer readers to DiVLA~\cite{wen2024diffusionvla}, the base model used in our experiments. An example of contructed visual-text data is at Figure~\ref{fig:examples}.


\subsection{Training Strategy and Implementation Details}
For our experiments, we adopt diffusion-based VLA, a widely used class of Vision-Language-Action (VLA) models exemplified by methods like $\pi_{0}$~\cite{[pi0} and TinyVLA~\cite{wen2024tinyvla}. We select diffusion-based VLA over auto-regressive alternatives due to its significantly faster inference speed, a critical advantage for real-time robotic applications (see FAST~\cite{pertsch2025fast} for a detailed comparison). Specifically, we utilize DiVLA~\cite{wen2024diffusionvla}, a representative VLA architecture, co-train on a hybrid dataset comprising robot interaction data and the vision-language corpus. To balance task-specific adaptation and semantic generalization, we maintained a 10:1 data ratio (robot-to-visual-textual data) across all tasks. This ratio empirically proved sufficient for robust object generalization, aligning with prior findings on the benefits of co-training for VLA capabilities. Notably, increasing the proportion of robot data beyond this ratio led to a decline in in-domain task success rates. We hypothesize this stems from the limited capacity of the 2B-parameter DiVLA model compared to larger architectures like ECoT (7B)~\cite{ecot} and RT-2 (55B)~\cite{rt-2}, which can better absorb domain-specific data without overfitting.