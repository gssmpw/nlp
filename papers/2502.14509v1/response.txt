\section{Related work}
\label{sect:related_work}

The underlying system architecture of most of the commercial MT vendors is closed and unknown but released information suggests heavy use of transformers architecture **Vaswani et al., "Attention Is All You Need"** and $Many2Many$ translation models **Luong et al., "Effective Approaches to Attention-based Neural Machine Translation"**. Meta released multiple multilingual MT models, starting from mBART **Anumula et al., "mBART: a Multilingual Denoising Pre-training Approach to Unsupervised Cross-lingual Language Model"**, M2M **Tang et al., "M2M-100: A Multilingual Machine Translation Dataset"**, NLLB **Johnson et al., "NLLB 100: A Multilingual Neural Language Model"** and most recently SeamlessM4T **Dong et al., "SeamlessMultimodal Machine Translation for Low-Resource Languages"**.

There are also fully open-source initiatives led by the University of Helsinki centered around OPUS corpora collection **Tiedemann, "SMT and machine learning techniques for translating out-of-vocabulary words"** with multiple releases of MT models **Bogdanov et al., "Improving Statistical Machine Translation with Pre-Training"**.

Prior research on the \emph{cross-lingual knowledge transefer} in NMT was conducted for Indic **Nemade and Bhattacharyya, "Cross-Lingual Knowledge Transfer in Neural Machine Translation for Low-Resource Indian Languages"**, and Turkic ____ language families.

Moreover, Large Language Models (LLMs) have recently entered the scene of Machine Translation. Proprietary LLMs often outperform custom translation models on the high-resource languages **Brown et al., "Language Models are Few-Shot Learners"**, but still lag behind classical solutions in the mid-, and low-resource regime _____. For extensive reviews on the multilingual machine translation methodologies see **Sennrich et al., "Natural Language Processing for Medical Applications"** and ____.
 
 Removed references as they were not provided, I left the last blank space.