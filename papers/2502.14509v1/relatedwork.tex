\section{Related work}
\label{sect:related_work}

The underlying system architecture of most of the commercial MT vendors is closed and unknown but released information suggests heavy use of transformers architecture \cite{NIPS2017_3f5ee243} and $Many2Many$ translation models \cite{johnson-etal-2017-googles}. Meta released multiple multilingual MT models, starting from mBART \cite{liu-etal-2020-multilingual-denoising}, M2M \cite{10.5555/3546258.3546365}, NLLB \cite{nllb2022} and most recently SeamlessM4T \cite{seamlessm4t2023}.

There are also fully open-source initiatives led by the University of Helsinki centered around OPUS corpora collection \cite{tiedemann-2012-parallel} with multiple releases of MT models \cite{tiedemann-thottingal-2020-opus, tiedemann-2020-tatoeba}.

Prior research on the \emph{cross-lingual knowledge transefer} in NMT was conducted for Indic \cite{10.1145/3587932}, and Turkic \cite{mirzakhalov-etal-2021-evaluating} language families.

Moreover, Large Language Models (LLMs) have recently entered the scene of Machine Translation. Proprietary LLMs often outperform custom translation models on the high-resource languages \cite{kocmi-etal-2023-findings}, but still lag behind classical solutions in the mid-, and low-resource regime \cite{hendy2023good, zhu2023multilingual}. For extensive reviews on the multilingual machine translation methodologies see \citet{kocmi-2021} and \citet{10.1145/3406095}.