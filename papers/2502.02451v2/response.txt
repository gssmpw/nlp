\section{Related Work}
The unique link between word usage and the expressed moral values provides a theoretical foundation for automated MF measurement from online texts **Zywica, "Automated Moral Foundation Measurement"**. Scholars have explored different computational approaches, including dictionaries **Wiegand, "Computational Moral Foundations"**__, word embeddings __**, machine learning __**, deep learning language models ____ and LLMs ____. These computational methods demonstrate great advantages on scalability and labor intensity compared to traditional human annotations. 

However, these approaches are primarily developed for English language, and are not directly applicable to non-English texts due to several major concerns: differences in cultural contexts, the lack of annotated datasets, and limitations in domain generalizability. To address the cross-language challenges and bridge knowledge gaps in MF measurements, various computational approaches have been proposed, which can be broadly categorized into two paths: machine translation to English and the development of cross-language measurement tools ____. 

\subsection{English Centric Machine Translation}
Translation is a widely used technique in cross-language MF measurement. For instance, the MFT survey has been translated into over 20 languages for cross-lingual studies **Zywica, "Cross-Lingual Moral Foundation Measurement"**. For large scale text analysis, the development of multilingual neural machine translation has significantly improved translation quality compared to earlier statistical methods, enhancing context understanding, ambiguity resolution, and fluency ____. This advancement enables a machine-translated approach to cross-language MF measurement by translating target languages into English and applying established English-based methods ____.

%The development of multilingual neural machine translation (MNMT) further improved the translation quality from statistical machine translation methods ____ on context understanding, ambiguity handling, fluency, multilingual training and other aspects. As a result, non-English deductive coding tasks can translate target languages into English and apply established English measurements. For example, **Meng, "Multilingual Neural Machine Translation"** showed that language models trained on English can also perform well on non-English documents translated to English. Therefore, it is a possible means to convert large non-English data to English first and then apply established English resources to measure MF values. 

%as well as the low rate of idiomatic phrase and metaphor recognition ____. This paper uses Google Translation ____ as a MNMT approach and compares it with other approaches. 

% Third, it may not be able to capture the cultural elements, which will be a major concern in MFT research, particularly for its comparative research agenda ____. MNMT is observed to be limitated in translating rare and culture-specific words as well as low rate of idiomatic phrase and metaphor recognition ____. This paper uses Google Translation ____ as a MNMT approach and compares it with other approaches. 

% \subsubsection{Established English MF Measurements}
Previous research has proposed various established methods for automatically measuring MFs in English, including dictionaries **Wiegand, "Computational Moral Foundations"**__, word embeddings __**, machine learning and deep learning models ____ trained on annotated English-language social media data ____. %, and LLMs (LLMs) with zero/few-shot learning approaches ____. 

\paragraph{Moral foundation dictionaries (MFDs)} 
Word-count methods with crafted English moral lexicons are common. Scholars have developed four English MFDs. The original MFD is an expert-crafted dictionary containing a list of 600 words across five foundation values **Zywica, "Automated Moral Foundation Measurement"**. **Wiegand, "MoralFoundation Dictionary"** then expanded this vocabulary to MFD2 with over 2,000 words by automatically querying similar words with word2vec word embeddings ____. Similarly, **Ribeiro, "Moral Strength Dictionary"** extended the original MFD to a MoralStrength dictionary with approximately 1,000 English lemmas based on WordNet synsets ____. Compared to MFD2, MoralStrength added a round of crowd-sourced ratings on expanded lemmas. **Wiegand, "Ethics Dictionary"**, however, curated a fully crowd-sourced dictionary named eMFD. It is different from previous expert-curated dictionaries for its layperson focus, contextual annotations, probability labeling and large vocabulary with 3,200 words. 

% were asked to annotate texts with MF values, and the probability of words being associated with specific MF values was calculated based on a large-scale English news corpus. Unlike MFD and MFD2, which rely on expert coding, eMFD construction is based on laypeople’s intuitive judgment of word associations with MF values with contextual information. **Ribeiro, "Layperson-Mediated Moral Foundations"**. Furthermore, **Zywica, "Automated Moral Foundation Measurement"** also used a similar approach to create another dictionary.

% It is also more data-efficient, achieving good performance with minimal local-language annotated data ____.

\subsection{Cross-Language MF Measurement}

\paragraph{Machine Translation}
To leverage the readily-available English resources in the transfer learning, current literature suggest two strategies: (1) machine translate English annotated data to local languages for monolingual language models training ____, or (2) train the multilingual language models with English annotated data first, then fine-tune with additional local language annotated data ____. The second approach has been proven to outperform the first in some deductive coding tasks, such as sentiment **Hovy, "Sentiment Analysis"** and hate speech detection ____. And it is also data-efficient which can result in great performance with little local-language annotated training data ____. This project evaluates the second transfer learning strategy on language models in the MF measurement tasks. 

%Furthermore, **Zywica, "Multilingual Neural Machine Translation"** suggest that with machine translation of English annotated data for extra data-augmentation, the second approach could improve on the model performance even better. 

\paragraph{Large Language Models (LLMs)}
The rise of LLMs provides an alternative for cross-language MF measurement. LLMs show exceptional zero/few-shot learning capability, enabling them to directly label human values out-of-the-box, which is particularly valuable for tasks with limited human annotated data ____. They also have demonstrated strong performance in measuring various human values, including emotions, stance, and political ideology **Hovy, "Sentiment Analysis"**. Notably, LLMs sometimes are not as good as specialized fine-tuned language models __**, which may be due to the lack of explicit, colloquial definitions of the target human values ____. MFT’s well-established conceptual framework may help address this limitation. Not only are LLMs pre-trained on rich MFT literature ____, but MFT also offers clear guidance for crafting clear and effective prompts. Additionally, LLMs trained on vast multilingual data exhibit promising capabilities to handle cross-language measurements ____.
% deductive coding tasks, and ``perform well enough to directly label text out-of-the-box'' in a zero/few-shot unsupervised manner ____. Though they sometimes are not as good as specialized fine-tuned language models __**, LLMs have demonstrated superior performance in many human-value measurements such as emotions, stance, and political ideology __**. A key factor influencing LLM performance is the presence of explicit, colloquial definitions of the target human values __**. Given MFT’s established conceptual foundation, LLMs have a strong basis for classifying MF values. Furthermore, recent LLMs trained on vast multilingual data exhibit promising multilingual capabilities across dozens of languages ____.

\subsection{Cross-Language Challenges}

Despite the strengths in accessibility, efficiency, multilingualism, and reasoning, there are also some concerns for LLMs' performance in cross-language MF measurement. First, LLMs exhibit a substantial degree of subordinate multilingualism, displaying proficiency in some languages but not others __**, which has a strong correlation with the proportion of those languages in the pre-training corpus __**. Second, there are potential language biases, particularly in human-value relevant coding tasks __**. For example, non-English prompts are more likely to generate malicious responses compared to English prompts __**. LLMs may also have moral bias across languages, as pre-trained multilingual language models often display distinct moral directions across languages __**. %Experiments on pre-trained multilingual language models found that models display different moral directions across cultures/languages %\footnote{While languages and cultures are closely related, important distinctions remain __**. This study focuses on language-based research and advises caution when interpreting cultural generalizations.}, but differences/biases are ``unnecessarily consistent with human value differences, which means the models are not always adequate'' __**.  %moral reasoning capabilities vary across different LLMs. Notably, 
Third, different LLMs exhibit varying baseline moral tendencies __**, such as GPT-3’s MF preferences aligning more closely with politically conservative individuals when minimal prompt engineering is used in zero-shot learning __**.

This study uses the **Llama3.1 model ____ to evaluate the LLMs approach for cross-language MF measurement.