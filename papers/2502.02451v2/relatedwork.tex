\section{Related Work}
The unique link between word usage and the expressed moral values provides a theoretical foundation for automated MF measurement from online texts \citep{brady2020mad, gantman2014moral, gantman2016see}. Scholars have explored different computational approaches, including dictionaries \citep{graham2009liberals, hopp2021extended}, word embeddings \citep{kwak2021frameaxis, araque2020moralstrength}, machine learning \citep{lan2022text}, deep learning language models \citep{preniqi2024moralbert, nguyen2024measuring} and LLMs \citep{rathje2024gpt}. These computational methods demonstrate great advantages on scalability and labor intensity compared to traditional human annotations. 

However, these approaches are primarily developed for English language, and are not directly applicable to non-English texts due to several major concerns: differences in cultural contexts, the lack of annotated datasets, and limitations in domain generalizability. To address the cross-language challenges and bridge knowledge gaps in MF measurements, various computational approaches have been proposed, which can be broadly categorized into two paths: machine translation to English and the development of cross-language measurement tools \citep{zhuang2020comprehensive}. 

\subsection{English Centric Machine Translation}
Translation is a widely used technique in cross-language MF measurement. For instance, the MFT survey has been translated into over 20 languages for cross-lingual studies \citep{yilmaz2016validation, nilsson2015moral}. For large scale text analysis, the development of multilingual neural machine translation has significantly improved translation quality compared to earlier statistical methods, enhancing context understanding, ambiguity resolution, and fluency \citep{stasimioti2020machine}. This advancement enables a machine-translated approach to cross-language MF measurement by translating target languages into English and applying established English-based methods \citep{artetxe2020translation}.

%The development of multilingual neural machine translation (MNMT) further improved the translation quality from statistical machine translation methods \citep{stasimioti2020machine} on context understanding, ambiguity handling, fluency, multilingual training and other aspects. As a result, non-English deductive coding tasks can translate target languages into English and apply established English measurements. For example, \citet{artetxe2020translation} showed that language models trained on English can also perform well on non-English documents translated to English. Therefore, it is a possible means to convert large non-English data to English first and then apply established English resources to measure MF values. 

%as well as the low rate of idiomatic phrase and metaphor recognition \citep{dorothy2019lost}. This paper uses Google Translation \citep{johnson2017google} as a MNMT approach and compares it with other approaches. 

% Third, it may not be able to capture the cultural elements, which will be a major concern in MFT research, particularly for its comparative research agenda \citep{haidt2012righteous}. MNMT is observed to be limitated in translating rare and culture-specific words as well as low rate of idiomatic phrase and metaphor recognition \citep{dorothy2019lost}. This paper uses Google Translation \citep{johnson2017google} as a MNMT approach and compares it with other approaches. 

% \subsubsection{Established English MF Measurements}
Previous research has proposed various established methods for automatically measuring MFs in English, including dictionaries \citep{graham2009liberals, frimer2019moral, hopp2021extended}, word embeddings \citep{kwak2021frameaxis, araque2020moralstrength}, machine learning and deep learning models \citep{preniqi2024moralbert, nguyen2024measuring} trained on annotated English-language social media data \citep{hoover2020moral, trager2022moral}. %, and LLMs (LLMs) with zero/few-shot learning approaches \citep{rathje2024gpt}. 

\paragraph{Moral foundation dictionaries (MFDs)} 
Word-count methods with crafted English moral lexicons are common. Scholars have developed four English MFDs. The original MFD is an expert-crafted dictionary containing a list of 600 words across five foundation values \citep{graham2009liberals}. \citet{frimer2019moral} then expanded this vocabulary to MFD2 with over 2,000 words by automatically querying similar words with word2vec word embeddings \citep{mikolov2013distributed}. Similarly, \citet{araque2020moralstrength} extended the original MFD to a MoralStrength dictionary with approximately 1,000 English lemmas based on WordNet synsets \citep{wordnet2010princeton}. Compared to MFD2, MoralStrength added a round of crowd-sourced ratings on expanded lemmas. \citet{hopp2021extended}, however, curated a fully crowd-sourced dictionary named eMFD. It is different from previous expert-curated dictionaries for its layperson focus, contextual annotations, probability labeling and large vocabulary with 3,200 words. 

% were asked to annotate texts with MF values, and the probability of words being associated with specific MF values was calculated based on a large-scale English news corpus. Unlike MFD and MFD2, which rely on expert coding, eMFD construction is based on laypeople’s intuitive judgment of word associations with MF values with contextual information. In addition, \citet{araque2020moralstrength} introduced another moral lexicon, MoralStrength, which includes approximately 1,000 lemmas.

\paragraph{Moral word embeddings} 
Semantic similarity methods using embeddings are another approach. To address the limitations of word-count methods, such as context insensitivity and vocabulary coverage \citep{nguyen2024measuring}, scholars have introduced semantic similarity methods. For example, \citet{kwak2021frameaxis} proposed an embedding framework FrameAxis. It predefines a vector space of micro moral frames with two sets of opposing seed words. Target documents are then converted into vectors using word embedding models, and their MF values are determined by comparing to the micro-frames. This method has been used to extract MF values from various online texts \citep[e.g.,][]{mokhberian2020moral,jing2021characterizing}. %Additionally, the MoralStrength lexicon's statistical properties also showed good performance as a feature of word embedding method based on semantic similarity. 

\paragraph{Moral language models} 
Supervised classification models with annotated English-language training data have also been used. With advancements in language models and efforts to create human-labeled MF training datasets \citep{hoover2020moral, trager2022moral}, recent studies have demonstrated the potential of fine-tuning language models. For example, \citet{preniqi2024moralbert} fine-tuned a BERT-based classifier MoralBert with large-scale annotated English data and achieved state-of-the-art performance. To address generalizability limitations in out-of-domain datasets \citep{liscio2022cross}, \citet{nguyen2024measuring} proposed another language model Mformer, and reported superior performance compared to other established English methods in evaluations. 

% such as  \citep{preniqi2024moralbert} and Mformer \citep{nguyen2024measuring}, to measure MF values from English. Leveraging deep learning architectures and extensive English-labeled training data, these models have shown superior performance compared to other established methods in evaluations \citep{preniqi2024moralbert, nguyen2024measuring}. 

Although machine translation offers several advantages in cross-language MF measurement, including interpretability, scalability, efficiency and accessibility, it also faces significant limitations. First, translation quality varies across languages and domains \citep{ranathunga2023neural}. In some low-resource languages like Tamil, machine translation often makes errors in translating domain terms, polysemous words, and contains repetitions for semantically similar terms \citep{ramesh2021comparing}. Second, it often fails to retain non-propositional information, such as emotion nuances. This can lead to emotion loss, toning down, or amplification across languages, introducing bias in subsequent analyses \citep{troiano2020lost}. Third, machine translation struggles to capture cultural elements, which is a major concern for cross-cultural and comparative research \citep{haidt2012righteous}. It often shows limited performance with rare or culture-specific words, idiomatic phrases, and metaphor recognition \citep{dorothy2019lost}. Thus, it remains unclear whether machine translation is a reliable method for measuring cross-language MFs. %This study evaluates the approach using Google Translation \citep{johnson2017google}, a multilingual neural machine translation tool, combined with the aforementioned English measurement methods. %The results are benchmarked against other tested approaches. 

\subsection{Cross-language Measurement Tools}
A second path is to develop cross-language tools, where scholars create computational MF resources tailored to local languages. Common cross-language measurements include local language dictionaries, task-specific multilingual language models, and LLMs.
% and can be generally grouped into three approaches based on the final product: local language dictionary construction, local language model training, and generative AI model fine-tuning. 

\paragraph{Local language dictionaries}
Due to the efficiency at scale and multilingual capabilities, local language dictionaries are widely used to estimate MF values from non-English texts \citep{hopp2021extended}. Developing local language MFDs generally involves three steps: (1) translating English MFDs to target languages; (2) adding culturally specific and non-translatable vocabulary; and (3) validating with native speakers and local language corpora.  Several extensive non-English MFDs have been developed and validated in Turkish \citep{alper2020changes}, Japanese \citep{matsuo2019development}, Portuguese \citep{carvalho2020brazilian}, and Chinese \citep{cheng2023c}. Despite the abovementioned advantages, the dictionary approach still faces the inherent limitations of general bag-of-words methods. In this paper, we use C-MFD2---a Chinese MFD---as a cross-language tool to evaluate a local language dictionary approach. We also test a semantic similarity approach using the FrameAxis architecture and cross-language word embedding models. 

% Compared to document-level machine translation, lexicon-level machine translation typically yields wider coverage due to its ability to collect synonyms---that is, a single English word can be mapped to multiple target language words with similar meanings across various contexts. Additionally, the involvement of human experts helps address the challenge of incorporating culturally specific vocabulary.

% However, local MF dictionaries still face abovementioned inherent limitations of bag-of-words methods in measuring MF values. Although unsupervised learning techniques like FrameAxis \citep{kwak2021frameaxis} improve performance by assessing semantic similarity in vector space rather than word frequency, this approach still struggles to fully capture moral values in complex, real-world contexts. Moreover, MF dictionaries are not generally applicable across domains and require customized validation for specific user contexts  \citep{cheng2023c}. 


\paragraph{Multilingual Language Models}
To overcome the limitations of bag-of-words methods, literature has suggested machine learning and deep learning approaches. A primary challenge with these methods is the scarcity of annotated data in local languages for model training \citep[e.g.,][]{ji2024moralbench, nguyen2024measuring}. Therefore, scholars have adopted transfer learning techniques that leverage English-annotated resources for cross-language classifier development. Two major transfer learning strategies are commonly proposed: (1) machine-translating annotated English-language data into local languages to train monolingual models \citep{schuster2019cross}, or (2) using annotated English-language data to train multilingual models \citep{barriere2020improving}. Given the strong performance of multilingual language models in deductive coding tasks such as sentiment analysis \citep[e.g.,][]{barriere2020improving} and hate speech detection \citep[e.g.,][]{rottger2022data}, this paper focuses on the second transfer learning strategy and evaluates multilingual language models for cross-language MF measurement. 
% evaluates the second transfer learning strategy for language models in MF measurement tasks.

% It is also more data-efficient, achieving good performance with minimal local-language annotated data \citep{rottger2022data}.

% To leverage the readily-available English resources in the transfer learning, current literature suggest two strategies: (1) machine translate English annotated data to local languages for monolingual language models training \citep{schuster2019cross}, or (2) train the multilingual language models with English annotated data first, then fine-tune with additional local language annotated data \citep{rottger2022data,barriere2020improving}. The second approach has been proven to outperform the first in some deductive coding tasks, such as sentiment \citep{barriere2020improving} and hate speech detection \citep{rottger2022data}. And it is also data-efficient which can result in great performance with little local-language annotated training data \citep{rottger2022data}. This project evaluates the second transfer learning strategy on language models in the MF measurement tasks. 

%Furthermore, \citet{barriere2020improving} suggest that with machine translation of English annotated data for extra data-augmentation, the second approach could improve on the model performance even better. 

%and machine translation data augmentation. So the experiment should be (1) train the multilingual language model on English labelled dataset (2) fine-tune it with local-language annotated dataset (as Paul) (3) data-augmentation it with MT translated English annotated datasets. 

\paragraph{Large Language Models (LLMs)}
The rise of LLMs provides an alternative for cross-language MF measurement. LLMs show exceptional zero/few-shot learning capability, enabling them to directly label human values out-of-the-box, which is particularly valuable for tasks with limited human annotated data \citep{ziems2024can}. They also have demonstrated strong performance in measuring various human values, including emotions, stance, and political ideology \citep{ziems2024can,gilardi2023chatgpt, tornberg2023chatgpt}. Notably, LLMs sometimes are not as good as specialized fine-tuned language models \citep{amin2023will,preniqi2024moralbert}, which may be due to the lack of explicit, colloquial definitions of the target human values \citep{ziems2024can}. MFT’s well-established conceptual framework may help address this limitation. Not only are LLMs pre-trained on rich MFT literature \citep[e.g.,][]{abdulhai2023moral}, but MFT also offers clear guidance for crafting clear and effective prompts. Additionally, LLMs trained on vast multilingual data exhibit promising capabilities to handle cross-language measurements \citep{ahuja2023mega}.
% deductive coding tasks, and ``perform well enough to directly label text out-of-the-box'' in a zero/few-shot unsupervised manner \citep[p.13]{ziems2024can}. Though they sometimes are not as good as specialized fine-tuned language models \citep{amin2023will,preniqi2024moralbert}, LLMs have demonstrated superior performance in many human-value measurements such as emotions, stance, and political ideology \citep{ziems2024can,gilardi2023chatgpt, tornberg2023chatgpt}. A key factor influencing LLM performance is the presence of explicit, colloquial definitions of the target human values \citep{ziems2024can}. Given MFT’s established conceptual foundation, LLMs have a strong basis for classifying MF values. Furthermore, recent LLMs trained on vast multilingual data exhibit promising multilingual capabilities across dozens of languages \citep{ahuja2023mega}.

Despite the strengths in accessibility, efficiency, multilingualism, and reasoning, there are also some concerns for LLMs' performance in cross-language MF measurement. First, LLMs exhibit a substantial degree of subordinate multilingualism, displaying proficiency in some languages but not others \citep{zhang2023don}, which has a strong correlation with the proportion of those languages in the pre-training corpus \citep{li2024quantifying}. Second, there are potential language biases, particularly in human-value relevant coding tasks \citep{kirk2024PRISM}. For example, non-English prompts are more likely to generate malicious responses compared to English prompts \citep{shen2024language}. LLMs may also have moral bias across languages, as pre-trained multilingual language models often display distinct moral directions across languages \citep{hammerl2022speaking}. %Experiments on pre-trained multilingual language models found that models display different moral directions across cultures/languages %\footnote{While languages and cultures are closely related, important distinctions remain \citep{hershcovich2022challenges}. This study focuses on language-based research and advises caution when interpreting cultural generalizations.}, but differences/biases are ``unnecessarily consistent with human value differences, which means the models are not always adequate'' \citep[p.9]{hammerl2022speaking}.  %moral reasoning capabilities vary across different LLMs. Notably, 
Third, different LLMs exhibit varying baseline moral tendencies \citep{ji2024moralbench}. For instance, GPT-3’s MF preferences align more closely with politically conservative individuals when minimal prompt engineering is used in zero-shot learning \citep{abdulhai2023moral}. These moral tendencies, however, are highly sensitive to prompting, with different prompting strategies significantly influencing classification outcomes in MF measurements \citep{abdulhai2023moral}.

Thus, it is unclear how LLMs performs in cross-language MF measurement tasks. This paper selects a cutting-edge, open-source model---Llama3.1 \citep{dubey2024llama} to evaluate the LLMs approach.