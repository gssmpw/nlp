%File: llm_mft_writing_sample.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS

%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{amssymb}

% Checklist macros
\usepackage{xcolor}
\newcommand{\answerYes}[1]{\textcolor{blue}{#1}} 
\newcommand{\answerNo}[1]{\textcolor{teal}{#1}} 
\newcommand{\answerNA}[1]{\textcolor{gray}{#1}} 
\newcommand{\answerTODO}[1]{\textcolor{red}{#1}} 

% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
\nocopyright
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title\


\begin{document}

\title{Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study}
\author{Calvin Yixiang Cheng\textsuperscript{\rm 1}, 
    Scott A. Hale\textsuperscript{\rm 1}\\
    \textsuperscript{\rm 1} Oxford Internet Institute, University of Oxford\\
    {calvin.cheng}@oii.ox.ac.uk, {scott.hale}@oii.ox.ac.uk
}

\maketitle

\begin{abstract}
% AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.
This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.
\end{abstract}

\section{Introduction} 

Moral intuitions have long fascinated social scientists, as they help explain a wide range of cognitive and behavioral phenomena across individuals and groups  \citep{effron2022moral}. Moral foundation theory (MFT) is among the most prominent psychology frameworks for understanding the origin and development of human morality \citep{graham2013moral}. Rooted in moral nativism, MFT argues there are five universal moral foundations---care/harm, fairness/cheating, authority/subversion, loyalty/betrayal, and sanctity/degradation---that transcend languages and cultures and underlie people’s moral judgments and decision-making processes \citep{graham2013moral} \footnote{Each MF includes both virtue and vice dimensions. This paper uses the virtue label to represent the entire foundation.}. While some scholars propose other foundations \citep{haidt2012righteous,atari2023morality}, %\footnote{There are several different models within the MFT framework to categorize the foundation values. For example, \citet{haidt2012righteous} suggests ``liberty/oppression'' as a sixth foundation; while \citet{atari2023morality} argues for replacing ``fairness/cheating'' foundation with ``equality'' and ``proportionality'', supporting a revised six-foundation model.} 
these five foundations have received the most empirical validation across domains, languages, and cultures \citep{iurino2020testing}. 

A growing body of literature employs MFT to investigate online social behaviors. For example, MFT provides a framework for understanding rising political polarization. Individuals who prioritize loyalty, authority, and sanctity foundations are more likely to endorse conservative views and engage in polarized political discourse, while those affiliated with liberal ideologies tend to value all MFs more evenly \citep{koleva2012tracing, haidt2007morality}. MFT also sheds light on online cultural clashes. \citet{atran2007religion} identified differing valuations of the sanctity foundation as a key factor in many religious and ideological conflicts. Beyond polarization, MFT has been applied to a range of social issues, including climate change \citep{markowitz2012climate}, vaccine hesitancy \citep{amin2017association}, anti-abortion views \citep{koleva2012tracing}, nationalism \citep{kertzer2014moral}, violent collectivism \citep{nussio2023moral}, and terrorism \citep{tamborini2020impact}. 

%%%% This par was added to argue the measurement important in misinformation research. it is not a necessity in the journal submission
%Measuring MFT is particularly meaningful in the context of misinformation and conspiracy theory research. Above all, misinformation online prevalently deploys moral framings. Latent MF values like authority influence individuals' sharing behaviors \citep{yang2024sharing}. MFT can also partially explain the virality of online misinformation diffusion. \citet{solovev2022moral} found that the differences in moral--emotional linguistic expressions between misinformation and truthful content distinguish their distinctive spreading patterns. \citet{brady2017emotion} theorized this phenomenon as the ``moral contagion'' process, where morally charged misinformation often goes viral when the embedded MF values resonate with individuals’ moral concerns. 

% Therefore, the ability to efficiently and accurately measure MF values in online discourse also has significant practical implications regarding content moderation.

% understanding the influence of morally charged information becomes paramount. Also, prevalent misinformation online is found widely embedded with moral-laden messages, which is effective in attracting more online engagements and challenging public discourse \citep{solovev2022moral, brady2020mad}. Therefore, the need to efficiently and accurately estimate people’s MF values in the online environment is imperative for social scientists. 
Given its broad relevance, measuring MF values in online discourse is essential; yet, automated extraction of MF values from large-scale texts remains challenging, particularly for non-English corpora. Like other latent human values, MFs are often conveyed through abstract narratives that vary across languages. Also, most computational resources for the measurement of MFs are designed for English content \citep{hoover2020moral, trager2022moral}. This reliance on English hinders cross-cultural comparative research on MFT and limits theoretical advancement from non-English data \citep{cheng2023c}. Although MFT is a universal theory intended to apply across languages and cultures, the limited availability of non-English resources severely restricts its research scope and further development of the theoretical framework \citep{graham2013moral}.

% As a universal theory, MFT is supposed to be applicable across languages and cultures, however the lack of limited non-English resources severely restricts the research scope of its theory-method co-development framework 

In this work, we investigate various computational approaches for cross-language measurement of MFs with a particular focus on data-efficiency. We use Chinese as an example and find that (1) MF local language lexicons yield suboptimal performance. They are worse than machine translation approaches that utilize established English measurements such as Mformer; (2) multilingual language models can achieve moderate success when trained on annotated English data along with some local language labeled data. However, this strategy is less data-efficient in measuring MFs than in other deductive coding tasks \citep[e.g.,][]{rottger2022data}. %substantial non-English labeled data is required for robust performance. (3) LLMs outperform other approaches in transfer-learning with greater data efficiency. Without requiring additional non-English annotated data, simple data augmentation techniques are sufficient to achieve strong performance measuring MFs in the non-English datasets; (4) human validation still plays an important role in LLMs-assisted deductive coding, especially in complex tasks like MF measurements.
(3) LLMs outperform other approaches in cross-language MF measurements in both accuracy and data efficiency. Simply fine-tuning and augmenting with English annotated data can achieve strong performance in non-English corpora; (4) nevertheless, this performance is in consistent on different MF values, as LLMs may overlook cultural nuances in cross-language measurements, particularly for culturally distinct values. \footnote{Code is available at GitHub \url{https://github.com/calvinchengyx/cross-lan-mft-measure}.}

% we emphasize the role of human validation in automated MF measurements, particularly in complex morality measurement tasks.


%Current computational measurements, although efficient, face major challenges in accuracy and validation across studies, and are restricted from domains and languages \citep{cheng2023c}. 

%The emergence of generative AI such as LLMs provides a promising approach to address these challenges. LLMs are trained on vast datasets that encompass diverse sources of texts, spanning various domains and languages. Their remarkable capacity to comprehend and interpret texts enables them to capture human-embedded values \citep{ziems2024can, tornberg2023chatgpt}. LLMs have demonstrated superior performance compared to human coders in certain language coding tasks, exhibiting higher levels of accuracy and reliability. As generative models, LLMs possess the ability to understand and code social science tasks across different domains without the need for extensive training. It also supports a wide range of languages, making them valuable tools for cross-language MF measurement tasks.

%This paper compares the performance of GPT-4 in coding MF values with existing human and computational measurements and particularly explores LLMs' capability in assisting cross-cultural and -language MF research. We demonstrated that GPT-4 could serve as a reliable classifier for MF value coding tasks with cautious prompting, and advocate a LLM-Assisted Content Analysis \citep{chew2023llm} approach for MF and other MF measurement tasks. Also, we highlighted LLMs' potential limitations in coding multilingual information. 

\section{Related Work} 
The unique link between word usage and the expressed moral values provides a theoretical foundation for automated MF measurement from online texts \citep{brady2020mad, gantman2014moral, gantman2016see}. Scholars have explored different computational approaches, including dictionaries \citep{graham2009liberals, hopp2021extended}, word embeddings \citep{kwak2021frameaxis, araque2020moralstrength}, machine learning \citep{lan2022text}, deep learning language models \citep{preniqi2024moralbert, nguyen2024measuring} and LLMs \citep{rathje2024gpt}. These computational methods demonstrate great advantages on scalability and labor intensity compared to traditional human annotations. 

However, these approaches are primarily developed for English language, and are not directly applicable to non-English texts due to several major concerns: differences in cultural contexts, the lack of annotated datasets, and limitations in domain generalizability. To address the cross-language challenges and bridge knowledge gaps in MF measurements, various computational approaches have been proposed, which can be broadly categorized into two paths: machine translation to English and the development of cross-language measurement tools \citep{zhuang2020comprehensive}. 

\subsection{English Centric Machine Translation}
Translation is a widely used technique in cross-language MF measurement. For instance, the MFT survey has been translated into over 20 languages for cross-lingual studies \citep{yilmaz2016validation, nilsson2015moral}. For large scale text analysis, the development of multilingual neural machine translation has significantly improved translation quality compared to earlier statistical methods, enhancing context understanding, ambiguity resolution, and fluency \citep{stasimioti2020machine}. This advancement enables a machine-translated approach to cross-language MF measurement by translating target languages into English and applying established English-based methods \citep{artetxe2020translation}.

%The development of multilingual neural machine translation (MNMT) further improved the translation quality from statistical machine translation methods \citep{stasimioti2020machine} on context understanding, ambiguity handling, fluency, multilingual training and other aspects. As a result, non-English deductive coding tasks can translate target languages into English and apply established English measurements. For example, \citet{artetxe2020translation} showed that language models trained on English can also perform well on non-English documents translated to English. Therefore, it is a possible means to convert large non-English data to English first and then apply established English resources to measure MF values. 

%as well as the low rate of idiomatic phrase and metaphor recognition \citep{dorothy2019lost}. This paper uses Google Translation \citep{johnson2017google} as a MNMT approach and compares it with other approaches. 

% Third, it may not be able to capture the cultural elements, which will be a major concern in MFT research, particularly for its comparative research agenda \citep{haidt2012righteous}. MNMT is observed to be limitated in translating rare and culture-specific words as well as low rate of idiomatic phrase and metaphor recognition \citep{dorothy2019lost}. This paper uses Google Translation \citep{johnson2017google} as a MNMT approach and compares it with other approaches. 

% \subsubsection{Established English MF Measurements}
Previous research has proposed various established methods for automatically measuring MFs in English, including dictionaries \citep{graham2009liberals, frimer2019moral, hopp2021extended}, word embeddings \citep{kwak2021frameaxis, araque2020moralstrength}, machine learning and deep learning models \citep{preniqi2024moralbert, nguyen2024measuring} trained on annotated English-language social media data \citep{hoover2020moral, trager2022moral}. %, and LLMs (LLMs) with zero/few-shot learning approaches \citep{rathje2024gpt}. 

\paragraph{Moral foundation dictionaries (MFDs)} 
Word-count methods with crafted English moral lexicons are common. Scholars have developed four English MFDs. The original MFD is an expert-crafted dictionary containing a list of 600 words across five foundation values \citep{graham2009liberals}. \citet{frimer2019moral} then expanded this vocabulary to MFD2 with over 2,000 words by automatically querying similar words with word2vec word embeddings \citep{mikolov2013distributed}. Similarly, \citet{araque2020moralstrength} extended the original MFD to a MoralStrength dictionary with approximately 1,000 English lemmas based on WordNet synsets \citep{wordnet2010princeton}. Compared to MFD2, MoralStrength added a round of crowd-sourced ratings on expanded lemmas. \citet{hopp2021extended}, however, curated a fully crowd-sourced dictionary named eMFD. It is different from previous expert-curated dictionaries for its layperson focus, contextual annotations, probability labeling and large vocabulary with 3,200 words. 

% were asked to annotate texts with MF values, and the probability of words being associated with specific MF values was calculated based on a large-scale English news corpus. Unlike MFD and MFD2, which rely on expert coding, eMFD construction is based on laypeople’s intuitive judgment of word associations with MF values with contextual information. In addition, \citet{araque2020moralstrength} introduced another moral lexicon, MoralStrength, which includes approximately 1,000 lemmas.

\paragraph{Moral word embeddings} 
Semantic similarity methods using embeddings are another approach. To address the limitations of word-count methods, such as context insensitivity and vocabulary coverage \citep{nguyen2024measuring}, scholars have introduced semantic similarity methods. For example, \citet{kwak2021frameaxis} proposed an embedding framework FrameAxis. It predefines a vector space of micro moral frames with two sets of opposing seed words. Target documents are then converted into vectors using word embedding models, and their MF values are determined by comparing to the micro-frames. This method has been used to extract MF values from various online texts \citep[e.g.,][]{mokhberian2020moral,jing2021characterizing}. %Additionally, the MoralStrength lexicon's statistical properties also showed good performance as a feature of word embedding method based on semantic similarity. 

\paragraph{Moral language models} 
Supervised classification models with annotated English-language training data have also been used. With advancements in language models and efforts to create human-labeled MF training datasets \citep{hoover2020moral, trager2022moral}, recent studies have demonstrated the potential of fine-tuning language models. For example, \citet{preniqi2024moralbert} fine-tuned a BERT-based classifier MoralBert with large-scale annotated English data and achieved state-of-the-art performance. To address generalizability limitations in out-of-domain datasets \citep{liscio2022cross}, \citet{nguyen2024measuring} proposed another language model Mformer, and reported superior performance compared to other established English methods in evaluations. 

% such as  \citep{preniqi2024moralbert} and Mformer \citep{nguyen2024measuring}, to measure MF values from English. Leveraging deep learning architectures and extensive English-labeled training data, these models have shown superior performance compared to other established methods in evaluations \citep{preniqi2024moralbert, nguyen2024measuring}. 

Although machine translation offers several advantages in cross-language MF measurement, including interpretability, scalability, efficiency and accessibility, it also faces significant limitations. First, translation quality varies across languages and domains \citep{ranathunga2023neural}. In some low-resource languages like Tamil, machine translation often makes errors in translating domain terms, polysemous words, and contains repetitions for semantically similar terms \citep{ramesh2021comparing}. Second, it often fails to retain non-propositional information, such as emotion nuances. This can lead to emotion loss, toning down, or amplification across languages, introducing bias in subsequent analyses \citep{troiano2020lost}. Third, machine translation struggles to capture cultural elements, which is a major concern for cross-cultural and comparative research \citep{haidt2012righteous}. It often shows limited performance with rare or culture-specific words, idiomatic phrases, and metaphor recognition \citep{dorothy2019lost}. Thus, it remains unclear whether machine translation is a reliable method for measuring cross-language MFs. %This study evaluates the approach using Google Translation \citep{johnson2017google}, a multilingual neural machine translation tool, combined with the aforementioned English measurement methods. %The results are benchmarked against other tested approaches. 

\subsection{Cross-language Measurement Tools}
A second path is to develop cross-language tools, where scholars create computational MF resources tailored to local languages. Common cross-language measurements include local language dictionaries, task-specific multilingual language models, and LLMs.
% and can be generally grouped into three approaches based on the final product: local language dictionary construction, local language model training, and generative AI model fine-tuning. 

\paragraph{Local language dictionaries}
Due to the efficiency at scale and multilingual capabilities, local language dictionaries are widely used to estimate MF values from non-English texts \citep{hopp2021extended}. Developing local language MFDs generally involves three steps: (1) translating English MFDs to target languages; (2) adding culturally specific and non-translatable vocabulary; and (3) validating with native speakers and local language corpora.  Several extensive non-English MFDs have been developed and validated in Turkish \citep{alper2020changes}, Japanese \citep{matsuo2019development}, Portuguese \citep{carvalho2020brazilian}, and Chinese \citep{cheng2023c}. Despite the abovementioned advantages, the dictionary approach still faces the inherent limitations of general bag-of-words methods. In this paper, we use C-MFD2---a Chinese MFD---as a cross-language tool to evaluate a local language dictionary approach. We also test a semantic similarity approach using the FrameAxis architecture and cross-language word embedding models. 

% Compared to document-level machine translation, lexicon-level machine translation typically yields wider coverage due to its ability to collect synonyms---that is, a single English word can be mapped to multiple target language words with similar meanings across various contexts. Additionally, the involvement of human experts helps address the challenge of incorporating culturally specific vocabulary.

% However, local MF dictionaries still face abovementioned inherent limitations of bag-of-words methods in measuring MF values. Although unsupervised learning techniques like FrameAxis \citep{kwak2021frameaxis} improve performance by assessing semantic similarity in vector space rather than word frequency, this approach still struggles to fully capture moral values in complex, real-world contexts. Moreover, MF dictionaries are not generally applicable across domains and require customized validation for specific user contexts  \citep{cheng2023c}. 


\paragraph{Multilingual Language Models}
To overcome the limitations of bag-of-words methods, literature has suggested machine learning and deep learning approaches. A primary challenge with these methods is the scarcity of annotated data in local languages for model training \citep[e.g.,][]{ji2024moralbench, nguyen2024measuring}. Therefore, scholars have adopted transfer learning techniques that leverage English-annotated resources for cross-language classifier development. Two major transfer learning strategies are commonly proposed: (1) machine-translating annotated English-language data into local languages to train monolingual models \citep{schuster2019cross}, or (2) using annotated English-language data to train multilingual models \citep{barriere2020improving}. Given the strong performance of multilingual language models in deductive coding tasks such as sentiment analysis \citep[e.g.,][]{barriere2020improving} and hate speech detection \citep[e.g.,][]{rottger2022data}, this paper focuses on the second transfer learning strategy and evaluates multilingual language models for cross-language MF measurement. 
% evaluates the second transfer learning strategy for language models in MF measurement tasks.

% It is also more data-efficient, achieving good performance with minimal local-language annotated data \citep{rottger2022data}.

% To leverage the readily-available English resources in the transfer learning, current literature suggest two strategies: (1) machine translate English annotated data to local languages for monolingual language models training \citep{schuster2019cross}, or (2) train the multilingual language models with English annotated data first, then fine-tune with additional local language annotated data \citep{rottger2022data,barriere2020improving}. The second approach has been proven to outperform the first in some deductive coding tasks, such as sentiment \citep{barriere2020improving} and hate speech detection \citep{rottger2022data}. And it is also data-efficient which can result in great performance with little local-language annotated training data \citep{rottger2022data}. This project evaluates the second transfer learning strategy on language models in the MF measurement tasks. 

%Furthermore, \citet{barriere2020improving} suggest that with machine translation of English annotated data for extra data-augmentation, the second approach could improve on the model performance even better. 

%and machine translation data augmentation. So the experiment should be (1) train the multilingual language model on English labelled dataset (2) fine-tune it with local-language annotated dataset (as Paul) (3) data-augmentation it with MT translated English annotated datasets. 

\paragraph{Large Language Models (LLMs)}
The rise of LLMs provides an alternative for cross-language MF measurement. LLMs show exceptional zero/few-shot learning capability, enabling them to directly label human values out-of-the-box, which is particularly valuable for tasks with limited human annotated data \citep{ziems2024can}. They also have demonstrated strong performance in measuring various human values, including emotions, stance, and political ideology \citep{ziems2024can,gilardi2023chatgpt, tornberg2023chatgpt}. Notably, LLMs sometimes are not as good as specialized fine-tuned language models \citep{amin2023will,preniqi2024moralbert}, which may be due to the lack of explicit, colloquial definitions of the target human values \citep{ziems2024can}. MFT’s well-established conceptual framework may help address this limitation. Not only are LLMs pre-trained on rich MFT literature \citep[e.g.,][]{abdulhai2023moral}, but MFT also offers clear guidance for crafting clear and effective prompts. Additionally, LLMs trained on vast multilingual data exhibit promising capabilities to handle cross-language measurements \citep{ahuja2023mega}.
% deductive coding tasks, and ``perform well enough to directly label text out-of-the-box'' in a zero/few-shot unsupervised manner \citep[p.13]{ziems2024can}. Though they sometimes are not as good as specialized fine-tuned language models \citep{amin2023will,preniqi2024moralbert}, LLMs have demonstrated superior performance in many human-value measurements such as emotions, stance, and political ideology \citep{ziems2024can,gilardi2023chatgpt, tornberg2023chatgpt}. A key factor influencing LLM performance is the presence of explicit, colloquial definitions of the target human values \citep{ziems2024can}. Given MFT’s established conceptual foundation, LLMs have a strong basis for classifying MF values. Furthermore, recent LLMs trained on vast multilingual data exhibit promising multilingual capabilities across dozens of languages \citep{ahuja2023mega}.

Despite the strengths in accessibility, efficiency, multilingualism, and reasoning, there are also some concerns for LLMs' performance in cross-language MF measurement. First, LLMs exhibit a substantial degree of subordinate multilingualism, displaying proficiency in some languages but not others \citep{zhang2023don}, which has a strong correlation with the proportion of those languages in the pre-training corpus \citep{li2024quantifying}. Second, there are potential language biases, particularly in human-value relevant coding tasks \citep{kirk2024PRISM}. For example, non-English prompts are more likely to generate malicious responses compared to English prompts \citep{shen2024language}. LLMs may also have moral bias across languages, as pre-trained multilingual language models often display distinct moral directions across languages \citep{hammerl2022speaking}. %Experiments on pre-trained multilingual language models found that models display different moral directions across cultures/languages %\footnote{While languages and cultures are closely related, important distinctions remain \citep{hershcovich2022challenges}. This study focuses on language-based research and advises caution when interpreting cultural generalizations.}, but differences/biases are ``unnecessarily consistent with human value differences, which means the models are not always adequate'' \citep[p.9]{hammerl2022speaking}.  %moral reasoning capabilities vary across different LLMs. Notably, 
Third, different LLMs exhibit varying baseline moral tendencies \citep{ji2024moralbench}. For instance, GPT-3’s MF preferences align more closely with politically conservative individuals when minimal prompt engineering is used in zero-shot learning \citep{abdulhai2023moral}. These moral tendencies, however, are highly sensitive to prompting, with different prompting strategies significantly influencing classification outcomes in MF measurements \citep{abdulhai2023moral}.

Thus, it is unclear how LLMs performs in cross-language MF measurement tasks. This paper selects a cutting-edge, open-source model---Llama3.1 \citep{dubey2024llama} to evaluate the LLMs approach. 

\section{Data}
Table \ref{tab:datasets} shows details of the benchmarking and training datasets used in this work. We used three human annotated datasets---moral foundation vignettes (MFV), Chinese moral scenarios (CCS) and Chinese core values (CCV), to benchmark the performance of different cross-language MF measurement approaches. And we used three English annotated MFT datasets---Reddit Corpus, Twitter Corpus and news corpus---for model training and fine-tuning, aligning with the work of \citet{nguyen2024measuring}.

\begin{table}[ht]
    {\small
    \centering
    \setlength{\tabcolsep}{1.6mm}
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    &\textbf{MFV} &\textbf{CCS} &\textbf{CCV} &\textbf{EN} &\textbf{Total} \\
    \midrule
    care/harm &27 &389 &3,030 &16,607 &20,053 \\
    loyalty/betrayal &16 &248 &1,712 &11,772 &13,748 \\
    authority/subversion &25 &331 &1,278 &13,176 &14,810 \\
    fairness/cheating &12 &259 &1,225 &16,292 &17,788 \\
    sanctity/degradation &10 &226 &247 &9,165 &9,648 \\
    non-moral & 0 & 0 & 0 &28,988 &28,988 \\
    \textbf{Total} &90 &1,453 &7,492 &71,242 &80,277 \\
    \hline
    \end{tabular}
    \caption{Moral foundation annotated datasets used in this paper. MFV, CCS and 20\% of CCV were used for benchmarking; EN and 80\% of CCV were used for fine-tuning XLM-T and Llama3.1-8b language models.}
    \label{tab:datasets}
    }
\end{table}

\paragraph{Moral Foundation Vignettes (MFV)} MFV is a list of social behaviors constructed by psychologists based on MFT, describing the violations of specific moral values from a third-party perspective \citep{clifford2015moral}. It has been widely validated and used to assess measures of moral judgment \citep[e.g.,][]{atari2023morality, curry2019mapping}. The original MFV is in English; we manually translate it to Chinese and ensure the cultural context is preserved. MFV is used as a universal stimulus to benchmark the performance of different cross-language MF measurement approaches on an expert-crafted sample.  
% a laboratory experiment setting.

%As MFV depicts simple and short moral relevant scenarios with little cultural-specific content, we are confident that the Chinese translation maintains all original information of the vignettes. We used this dataset, comprising 90 records, serves as a universal stimulus to evaluate various non-English MF measurement approaches in a lab experiment setting.

% This dataset will serve as universial stimulus to test the performance of different approaches of measuring non-English test in the lab experiment setting. This is small dataset with 90 records.  %(Translation is attached in Appendix \ref{tab:mfv_chinese}). 

\paragraph{Chinese Moral Scenarios (CCS)} CCS is list of moral scenarios written by laypeople to describe their intuitive understanding of MF values \citep{cheng2023c}. It is written by 202 native Chinese speakers describing situations that either violate or respect specific MF values, with minimal training in MFT literacy. This reverse-annotation method is commonly used for validating MF measurements \citep[e.g.,][]{cheng2023c, frimer2019moral, matsuo2019development}. It incorporates culturally specific content, reflecting native speakers’ natural and intuitive understanding of MFT in a real-word context. 
% a reverse-labeled Chinese datasets that describing moral scenarios by laypeople. A total of 202 native Chinese participants described situations that either violated or respected particular moral values based on their intuitive understanding of MF concepts. This reverse-labeling method is commonly used for MF measurement evaluation \citep[e.g.,][]{cheng2023c, frimer2019moral, matsuo2019development} and includes more culturally specific content, capturing native speakers’ naive understanding of MFT in a natural environment. 

\paragraph{Chinese Core Values (CCV)} CCV is a human-annotated real-world dataset, including 6,994 sentences collected from four local Chinese news websites.\footnote{CCV includes data from the CMOS corpus \citep{peng2021morality}, China Cultural and Ethical Website \url{wenming.cn}, Youth Patriotism News \url{agzy.youth.cn}, and Sohu News \url{news.sohu.com}.} The dataset is annotated by three native Chinese speakers based on the Chinese core socialist moral value coding scheme, which is highly correlate with the five universal moral foundation values \citep{liu2022corevalue}. The original CCV dataset includes eight labels,\footnote{The Chinese core socialist moral values include civility, justice, equality, rule of law, patriotism, dedication, integrity, and friendship. The curated mapping scheme to the five moral foundation values is in the Appendix} which are qualitatively evaluated and re-mapped to the universal five foundation values following an expert-curated mapping scheme. We sample 20\% of the CCV as the primary benchmarking dataset to evaluate the performance across approaches stratifying on the values. The remaining 80\% is used as local-language annotated data to test the data-efficiency  fine-tuning language models.

%Original Chinese core values include eight classes: civility, justice, equality, rule of law, patriotism, dedication, integrity, and friendship. We qualitatively evaluated each category and mapped labels to the five foundation values following a curated mapping scheme. %the mapping scheme in Appendix \ref{tab:cv_map}. 


All benchmarking documents are single-class labeled. For multi-class predicted documents, we decided the measurement performance by a lenient evaluation criterion: a prediction is considered correct if one of the predicted values matches the true label. 
%Some evaluation approaches may assign multiple MF values to a single document. In such cases, we used a lenient evaluation criterion: if one of the predicted values matches the true label, it is considered a correct prediction.

% All benchmarking documents are single-class labeled, meaning each document has only one corresponding MF value. However, it is possible for some evaluation methods to assign more than one MF value to a single document. In this approach, we assess model performance with lenient freedom of degree: if one of the predicted values matches the correct MF value, we consider it a correct prediction.

\paragraph{English annotated data (EN)} We use three English annotated MF datasets for transfer learning, including a Twitter corpus \citep{hoover2020moral}, a Reddit corpus \citep{trager2022moral} and a news corpus \citep{hopp2021extended}, which are widely used in training English MF classifiers and show reliable performance \citep{nguyen2024measuring, preniqi2024moralbert}. 

\section{Methods}
\subsection{Machine Translation}
We use Google Translate as an example of a machine translation approach due to its accessibility and consistent performance across domains. %\footnote{\url{https://cloud.google.com/translate/docs/basic/translating-text}}. No human intervention or validation was conducted on the translation. 
First, we machine translate benchmarking datasets except MFV to Chinese using the Google Cloud API---Basic Translation service. Then we estimate the MF values from translated documents with established English measurements, including lexicons MFD \citep{graham2009liberals}, MFD2 \citep{frimer2019moral}, eMFD \citep{hopp2021extended} and MoralStrength \citep{araque2020moralstrength}; word embeddings with FrameAxis \citep{kwak2021frameaxis}; and specialized-fine-tuned language models MoralBert \citep{preniqi2024moralbert} and Mformer \citep{nguyen2024measuring}. 

For MFD, MFD2, and eMFD, we calculate word frequencies using the \texttt{eMFDscore} Python package \citep{hopp2021extended}. %\footnote{eMFDscore: Extended Moral Foundation Dictionary Scoring for Python \url{https://github.com/medianeuroscience/emfdscore/tree/master}}.
For MFD and MFD 1.0, each document’s MF value is determined by the most frequent MF class in the respective dictionary. A document is mapped to multiple classes if there is an equal number of class matches. If there are no matching words, no class is assigned to the document. eMFD, however, assigns probabilities to its vocabulary, representing their likelihood of being associated with certain MF classes. We sum the probabilities and label the document with the class that has the highest sum. 

We use the \texttt{moralstrength} package \citep{araque2020moralstrength} for the MoralStrength dictionary. We first test its performance of the lexicon features alone with bag-of-word methods; then we train a Support Vector Machine (SVM) model and combine its lexicon features. Since English training sets contain many non-moral labels, while the benchmarking dataset contains only moral labels, we train two SVM models: one with the full training data and another with only moral-labeled training data.

For word embedding methods, we use \texttt{FrameAxis} Python package \citep{kwak2021frameaxis} to compute anchor micro-frames based on different MFDs with the word2vec embedding model \citep{mikolov2013distributed}. For each MFD, we generate the corresponding micro moral frames from the vocabulary in its class. We then compute and aggregate word contributions to each microframe in the document, and label the document's moral class by identifying significant microframes through comparison with a null model. %\footnote{FrameAxis: \url{https://github.com/negar-mokhberian/Moral_Foundation_FrameAxis}; Word2vec \url{https://radimrehurek.com/gensim/models/word2vec.html}; MoralStrength---moral foundations theory predictor and lexicon \url{https://github.com/oaraque/moral-foundations}; MoralBert \url{https://huggingface.co/spaces/vjosap/MoralBERTApp}; MFormer \url{https://huggingface.co/joshnguyen/mformer-authority}}

For language models, we apply pre-trained language models \texttt{MoralBert}\footnote{Accessed from \url{https://huggingface.co/vjosap}} and \texttt{Mformers}\footnote{Accessed from \url{https://huggingface.co/joshnguyen}} from HuggingFace with their default settings and no additional fine-tuning. 

\subsection{Local Language Lexicons}
We apply C-MFD2 \citep{cheng2023c} to evaluate the performance of the local language dictionary approach. We test two techniques for locally-developed MF lexicons: word counts and embeddings. For the word embedding method, we test two approaches with the fastText model \citep{grave2018learning}. One involves measure simple semantic similarity. Words in C-MFD2 are grouped into five pseudo-documents based on their MF labels, each serving as anchor frames. MF values are then determined by calculating the semantic distance between the text to be classified and these anchor frames.  %we tested it replace word count with word embeddings by aggregating words within each moral group in the dictionary into a single document and comparing the semantic similarity of documents to these embeddings. .\footnote{FastText is an open-sourced word embedding library developed by Meta and supports 157 languages: \url{https://fasttext.cc/docs/en/crawl-vectors.html}.} 
The other uses FrameAxis to construct anchor frames. As C-MFD2 does not have the virtue/vice dimension which is essential to calculate micro-frames in FrameAxis, we automatically assign this dimension using the RoBERTa-based Chinese sentiment model \texttt{c2-roberta-base-finetuned-dianping-chinese} from HuggingFace\footnote{Accessed from \url{https://huggingface.co/liam168/c2-roberta-base-finetuned-dianping-chinese}}. %Moreover, for computing the document embeddings of benchmarking data, in addition to averaging fastText word embeddings, we also test the multilingual sentence embedding model \texttt{Paraphrase-xlm-r-multilingual-v1}, which is  effective in capturing a holistic representation of sentences \citep{conneau2019unsupervised}. 

\subsection{Multilingual Language Models}
We select  XLM-T \footnote{Accessed from \url{https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base}} as the base model to test transfer learning on the multilingual language model approach \citep{barbieri2022xlm}. Fine-tuned on 198 million multilingual tweets on the XLM-RoBERTa architecture---originally trained on 2.5 TB of Common Crawl data \citep{conneau2019unsupervised}---this model is particularly well-suited for analyzing online content. Also, previous research indicates that it has strong performance in cross-language deductive coding tasks such as hate speech detection, compared to other multilingual language models \citep{rottger2022data}. 
%language models that can effectively and efficiently transfer English knowledge to other languages

We follow \citet{nguyen2024measuring}'s experience on fine-tuning Mformer with some tweaks. First, we replace the base architecture from RoBERTa-base \citep{liu2019roberta} with \texttt{twitter-xlm-roberta-base} and tokenization is handled by the model's built-in tokenizer \citep{barbieri2022xlm}, with token sequences truncated to a maximum of 512. Second, we set the learning rate, epochs, and batch size to 2$e$-5, 3, and 16 respectively following \citet{rottger2022data}. %For each MF value, we fine-tune XLM-T on a binary text classification task using the same training datasets as \citet{nguyen2024measuring}. 
Third, we opt to fine-tune five binary classifiers rather than a single multi-label classifier because binary models generally outperform multi-label models in English MF measurements \citep{nguyen2024measuring}.\footnote{We note that in hate speech detection, \citet{rottger2022data} found no significant performance difference between binary and multi-label models when using XLM-T. Given that multi-label models require less storage and training resources, they are a viable alternative for future applications.} Fourth, we adopt a conservative under-sampling strategy in the English annotated training dataset to address class imbalance, establishing a baseline for future improvements. 

After fine-tuning with English annotated data, we further fine-tune each base model with the 80\% of CCV dataset in order to test the data-efficiency as in \citet{rottger2022data}. We incrementally train models with batches of additional CCV data, with each batch containing 100 annotated records.

\subsection{Large Language Models}
For the LLMs approach, we select the Llama3.1-8b instruct model, an open-source multilingual LLM developed by Meta with a reasonable balance between model performance and computational cost. Compared to closed-source models like GPT-4, Llama3.1 offers greater control, flexibility, transparency, and reproducibility---all of which are important in human-value measurement tasks.

We first test LLMs with prompt-engineering and few-shot learning. Next, we apply the same fine-tuning process used for XLM-T to Llama3.1-8b. Notably, an additional round of data augmentation is performed afterwards, where all English annotations are machine-translated into Chinese using the Google Translate API. Fine-tuning is conducted with the \texttt{unsloth} package using 4-bit quantization on a single NVIDIA L40S GPU. Additionally, we test data efficiency with 20 batches of Chinese annotated items from the CCV dataset. A conservative under-sampling strategy is applied as well with each batch containing 50 records evenly distributed across the five classes.

\section{Results}
\subsection{Machine Translation}
As shown in Table \ref{tab:mt}, the performance of machine translation generally fall short in evaluation. With the MFV benchmarking dataset, the lexicon method MFD2 shows the best performance with a weighted F1 score of $0.60$. In the reverse-annotated CCS dataset, a simple SVM model with lexicon features outperforms other measurements ($F1 = 0.74$), but the model coverage is relatively low at only 23\%. In the real-word CCV dataset, the deep learning model Mformer exhibits the strongest performance ($F1 = 0.47$) and maintains comparable results across the other two benchmark datasets. Note that although some MF classes in Mformer displayed good performance (i.e., care/harm, $F1 = 0.72$), some fine-grained MF measurements are very poor. For example, Mformer's prediction on ``loyalty'' ($F1 = 0.21$) is worse than random guessing baseline ($F1 = 0.23$) in the cross-language evaluation setting. 

\begin{table}[!ht]
    {\small
    \centering  % Ensures the table is centered horizontally
     % Set text size to normal
    \setlength{\tabcolsep}{1mm}
    %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lccccccccc}
        \toprule
        & \textbf{Auth} & \textbf{Care} & \textbf{Fair} & \textbf{Loya} & \textbf{Sanc} & \textbf{Acc} & \textbf{Cov} & \textbf{Fw} & \textbf{Fm} \\
        
        \multicolumn{10}{l}{\textbf{MFV}} \\
        \midrule
        Baseline & 0.28&0.30&0.13&0.18&0.11&0.23&1.00&0.23&0.20 \\
        MFD & \textbf{0.77} & 0.29 & 0.00 & 0.55 & 0.00 & 0.56 & 0.28 & 0.55 & 0.32 \\
        MFD2 & 0.69 & 0.50 & \textbf{0.75} & \textbf{0.59} & 0.33 & \textbf{0.59} & 0.46 & \textbf{0.60} & \textbf{0.57} \\
        eMFD & 0.39 & 0.58 & 0.31 & 0.22 & \textbf{0.44} & 0.42 & \textbf{1.00} & 0.41 & 0.39 \\
        MS & 0.00 & 0.00 & 0.00 & 0.33 & 0.31 & 0.17 & 0.20 & 0.13 & 0.13 \\
        FA+MFD & 0.39 & 0.06 & 0.00 & 0.29 & 0.31 & 0.27 & \textbf{1.00} & 0.21 & 0.21 \\
        FA+MFD2 & 0.19 & 0.18 & 0.06 & 0.43 & 0.17 & 0.22 & \textbf{1.00} & 0.21 & 0.21 \\
        FA+eMFD & 0.07 & 0.47 & 0.19 & 0.40 & 0.17 & 0.31 & \textbf{1.00} & 0.28 & 0.26 \\
        svm+MS & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
        svm+MS* & 0.07 & 0.32 & 0.21 & 0.09 & 0.00 & 0.19 & \textbf{1.00} & 0.16 & 0.14 \\
        MoralBert & 0.27 & 0.55 & 0.50 & 0.10 & 0.30 & 0.38 & \textbf{1.00} & 0.36 & 0.34 \\
        MFormer & 0.65 & \textbf{0.65} & 0.72 & 0.31 & 0.40 & 0.58 & \textbf{1.00} & 0.57 & 0.55 \\
        \midrule
        
        \multicolumn{10}{l}{\textbf{CCS}} \\
        \midrule
        Baseline&0.23&0.27&0.18&0.17&0.16&	0.21&1.00&	0.21&0.22 \\
        MFD &0.47 &0.62 &0.67 &0.18 &\textbf{0.84}&0.54 &0.6 &0.54 &0.55 \\
        MFD2 &0.55 &\textbf{0.71} &0.71 &0.61 &0.71 &0.66 &0.75 & 0.66 &0.66 \\
        eMFD &0.48 &0.52 &0.48 &0.44 &0.25 &0.47 &0.98 &0.45 &0.43 \\
        MS &0.16 &0.11 &0.16 &0.18 &0.21 &0.16 &0.17 & 0.16 &0.16 \\
        FA+MFD &0.45 &0.38 &0.31 &0.26 &0.47 &0.38 &\textbf{1.00} &0.38 &0.37 \\
        FA+MFD2 &0.39 &0.57 &0.57 &0.39 &0.43 &0.47 &\textbf{1.00} &0.47 &0.47 \\
        FA+eMFD &0.33 &0.42 &0.28 &0.17 &0.22 &0.30 &\textbf{1.00} &0.30 &0.28 \\
       svm+MS &\textbf{0.68} &\textbf{0.71} &\textbf{0.77} &\textbf{0.85} &0.61 &\textbf{0.74} &0.23 & \textbf{0.74} &\textbf{0.72} \\
       svm+MS* &0.21 &0.39 &0.28 &0.15 &0.14 &0.26 &\textbf{1.00} &0.25 &0.23 \\
       MoralBert &0.41 &0.63 &0.58 &0.64 &0.49 &0.57 &\textbf{1.00} &0.55 &0.55 \\
       MFormer &0.65 &\textbf{0.71} &0.70 &0.71 &0.71 &0.69 &\textbf{1.00} & 0.69 &0.70 \\
        \midrule
        
        \multicolumn{10}{l}{\textbf{CCV}} \\
        \midrule
        Baseline &0.18&0.40&0.16&0.23&0.03&0.27	&1.00&	0.27&0.20 \\
        MFD &\textbf{0.33} &0.40 &0.43 &0.3 &0.00 &0.34 &0.47 &0.35 &0.29 \\
        MFD2 &0.23 &0.62 &0.43 &0.27 &0.06 &0.43 &0.72 & 0.43 &0.32 \\
        eMFD &0.11 &0.59 &0.36 &0.20 &0.02 &0.40 &\textbf{1.00} &0.36 &0.26 \\
        MS &0.17 &0.21 &0.23 &0.23 &0.09 &0.20 &0.36 & 0.21 &0.19 \\
        FA+MFD &0.29 &0.30 &0.27 &\textbf{0.31} &0.06 &0.28 &\textbf{1.00} &0.29 &0.25 \\
        FA+MFD2 &0.07 &0.49 &0.35 &0.30 &0.11 &0.34 &\textbf{1.00} &0.34 &0.27 \\
        FA+eMFD &0.04 &0.56 &0.31 &0.24 &0.08 &0.38 &\textbf{1.00} &0.34 &0.25 \\
       svm+MS &0.28 &0.59 &0.49 &0.27 &\textbf{0.18 }&0.47 &0.17 & 0.44 &\textbf{0.36 }\\
       svm+MS* &0.12 &0.35 &0.25 &0.12 &0.03 &0.23 &\textbf{1.00} &0.23 &0.17 \\
       MoralBert &0.15 &0.62 &0.47 &0.24 &0.10 &0.45 &\textbf{1.00} &0.41 &0.32 \\
       MFormer &0.23 &\textbf{0.72} &\textbf{0.52} &0.21 &0.14 &\textbf{0.50} &\textbf{1.00} & \textbf{0.47} &\textbf{0.36} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Established English moral foundation measurements applied to machine translated text. Acc, Cov, Fw and Fm refers to accuracy, coverage, F1 weighted and F1 macro respectively. Baseline, FA and MS in the first column represent random guessing, FrameAxis and MoralStrength. The best performing methods for each dataset are in \textbf{bold}. MoralStrength models trained with no non-moral labeled data are marked with a star (*). The Coverage column measures the percentage of all moral labels in the prediction.}
    \label{tab:mt}
\end{table}


\subsection{Local Language Lexicons}
Local language lexicon approaches demonstrate similarly moderate performance with word-count methods. Table \ref{tab:lld-dict} shows that across all benchmarking datasets, C-MFD2 consistently outperforms other methods although its performance is generally still poor. In the real-world CCV dataset, C-MFD2 achieves the best performance ($F1 = 0.43$), which is comparable to the machine translation approach with Mformer. We find multilingual word embedding model fastText with FrameAxis framework do not improve the cross-language measurement performance in this task. 

% Similar to the machine translation approach, the local language lexicon approach does not show reliable performance in the CCV dataset ($F1_{C-MFD2}=0.43$). 

% raising concerns about the effectiveness of embedding approaches with local language dictionaries

\begin{table}[!ht]
    {\small
    \centering  % Ensures the table is centered horizontally
    \setlength{\tabcolsep}{1mm}
    %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lccccccccc}
        \toprule
        & \textbf{Auth} & \textbf{Care} & \textbf{Fair} & \textbf{Loya} & \textbf{Sanc} & \textbf{Acc} & \textbf{Cov} & \textbf{Fw} & \textbf{Fm} \\
        
        \multicolumn{10}{l}{\textbf{MFV}} \\
        \midrule
        Baseline & 0.28&\textbf{0.30}&0.13&0.18&	0.11&0.23&1.00&0.23&0.20 \\
        cMFD2 &\textbf{0.48 }&0.18 &\textbf{0.67} &0.29 &\textbf{0.22} &\textbf{0.39} &0.40 & \textbf{0.42} &\textbf{0.37} \\
        FT &0.44 &0.00 &0.00 &0.12 &0.18 &0.30 &\textbf{1.00} &0.16 &0.15 \\
        FT+FA &0.08 &0.00 &0.00 &\textbf{0.30 }&0.00 &0.19 &\textbf{1.00} & 0.08 &0.08 \\
        % FA+XLM &0.00 &0.07 &0.24 &0.12 &0.00 &0.16 &\textbf{1.00} &  0.07 &0.09 \\

        \midrule
        \multicolumn{10}{l}{\textbf{CCS}} \\
        \midrule
        Baseline&0.23&0.27&0.18&0.17&0.16&	0.21&1.00&	0.21&0.22 \\
        cMFD2 &\textbf{0.74} &\textbf{0.76} &\textbf{0.73} &\textbf{0.68} &\textbf{0.74} &\textbf{0.74} & 0.76 & \textbf{0.73} &\textbf{0.73} \\
        FT&0.44 &0.32 &0.35 &0.39 &0.37 &0.39 &\textbf{0.97} &0.37 &0.37 \\
        FT+FA &0.14 &0.05 &0.15 &0.29 &0.02 &0.20 &\textbf{0.97} &0.12 &0.13 \\
        % FA+XLM &0.00 &0.14 &0.30 &0.00 &0.00 &0.19 &\textbf{1.00} & 0.09 &0.09 \\
        
        \midrule
        \multicolumn{10}{l}{\textbf{CCV}} \\
        \midrule
        Baseline &0.18&0.40&0.16&0.23&0.03&0.27	&1.00&	0.27&0.20 \\
        cMFD2 &0.26 &\textbf{0.63} &\textbf{0.45} &0.28 &\textbf{0.09} &\textbf{0.45} &0.64 & \textbf{0.43} &\textbf{0.34} \\
        FT &\textbf{0.28} &0.17 &0.09 &0.01 &0.03 &0.20 &\textbf{0.98} &0.13 &0.12 \\
        FT+FA &0.02 &0.03 &0.04 &\textbf{0.37} &0.00 &0.23 &\textbf{0.98} & 0.10&0.09 \\
        % FA+XLM &0.00 &0.18 &0.27 &0.02 &0.00 &0.19 &\textbf{1.00} &0.12 &0.09 \\
    \bottomrule
    \end{tabular}
    }
    \caption{The performance of C-MFD2 in cross-language MF measurement. FT and FA in the first column represent FastText and FrameAxis. The best performing methods for each dataset are in \textbf{bold}. Other abbreviations are the same as those in Table \ref{tab:mt}.}
    \label{tab:lld-dict}
\end{table}

\subsection{Multilingual Language Models}
The multilingual language model XLM-T, trained on the same English annotated data, shows a moderate but reduced performance compared to the monolingual English model Mformer. In Table \ref{tab:xlm}, XLM-T shows moderate performance across all benchmarking datasets ($F1_{MFV} = 0.71, F1_{CCS} = 0.71, F1_{CCV} = 0.63$) outperforming both machine translation and local lexicon approaches. In addition, its performance is generally consistent across the five foundation values, showing a better reliability in the fine-grained MF measurements. This consistency is likely due to training five separate classifiers. In the real-world CCV dataset, the fine-tuned XLM-T model has moderate performance in four out of five foundation values with an average F1 score of $0.63$. The ``authority'' model has lower performance with $F1 = 0.46$.


\begin{table}[!ht]
    {\small
    \centering  % Ensures the table is centered horizontally
    % \setlength{\tabcolsep}{1mm}
    %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccccc} 
    \toprule
    &\textbf{Auth} &\textbf{Care} &\textbf{Fair}  &\textbf{Loya} &\textbf{Sanc} & \textbf{XLM-T Avg} \\
    \multicolumn{6}{l}{\textbf{MFV}} \\
    \midrule
    0 &0.70 &0.81 &0.76 &0.88 &0.85 & 0.80\\
    1 &0.27 &0.41 &0.15 &0.54 &0.15 & 0.30\\
    Acc &0.58 &0.71 &0.62 &0.81 &0.74 & 0.69\\
    Fm &0.49 &0.61 &0.45 &0.71 &0.50 & 0.55\\
    Fw &0.58 &0.69 &0.68 &0.82 &0.77 & 0.71 \\

    \midrule
    \multicolumn{6}{l}{\textbf{CCS}}\\
    \midrule
    0 &0.66 &0.76 &0.79 &0.91 &0.89 & 0.80\\
    1 &0.19 &0.44 &0.02 &0.54 &0.51 & 0.34\\
    Acc &0.52 &0.66 &0.65 &0.85 &0.82 & 0.70 \\
    Fm &0.42 &0.60 &0.4 &0.73 &0.70 & 0.57\\
    Fw &0.55 &0.67 &0.65 &0.85 &0.83 & 0.71\\
    
    \midrule
    \multicolumn{6}{l}{\textbf{CCV}} \\ 
    \midrule
    0 &0.51 &0.63 &0.74 &0.78 &0.75 & 0.68\\
    1 &0.24 &0.58 &0.20 &0.41 &0.10 & 0.31\\
    Acc &0.40 &0.60 &0.61 &0.68 &0.61 &0.58 \\
    Fm &0.37 &0.60 &0.47 &0.60 &0.43 &0.49\\
    Fw &0.46 &0.61 &0.65 &0.70 &0.73 & 0.63\\
    \bottomrule
    \end{tabular}
    }
   %}
    \caption{Multilingual language models for moral foundation measurement. \textit{XLM-T Avg} refers to the average F1 score across five foundation models.}
    \label{tab:xlm}
\end{table}

In the follow-up evaluation of batch training with local language data, we observe improved model performance as the volume of local language training data increases. As shown in Figure \ref{fig:xlm-batch-cv}, the fine-tuned XLM-T models achieve higher F1 scores with more locally labeled data. For example, the F1 score of the ``loyalty'' model rose from $0.62$ to $0.80$ with 22 batches of local-language data. Similar increasing patterns are observed across all five models. 

However, the amount of annotated non-English data required to reach reliable classification thresholds for MFs is much more than for hate speech detection \citep{rottger2022data}. On average, 16 batches are needed to reach the F1 score of $0.70$ and 49 batches to reach $0.80$. The ``care'' model, which is the best performing model of the five, requires six additional batches to reach an F1 score of $0.70$ and 27 batches to reach $0.80$. There are 10 batches of data available for the ``sanctity'' model, and it shows no significant improvement even once all 10 batches are used for training.

It is worth noting that limited local language annotated data may also negatively impact the model’s performance. For example, the ``fairness'' model's F1 score initially drops from $0.50$ to $0.46$ when fine-tuned with local-language data. It only begins to stabilize and improve after 11 batches. In sum, while moderate performance can be achieved with local-language annotations, considerably more data is required to attain robust classification performance with the multilingual language model approach in cross-language MF measurements.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{icwsm_xlm_batch_cv4.pdf} 
\caption{Accumulated fine-tuning of the multilingual language model XLM-T with local language annotated data from the CCV dataset. Each batch includes 100 items.}
\label{fig:xlm-batch-cv}
\end{figure}

\subsection{Large Language Models}
Table \ref{tab:llama-8b} shows the results of using Llama3.1-8b. With only prompt engineering, Llama3.1-8b already performs better than the machine translation and local language lexicon approaches across all benchmarking datasets. After fine-tuning with annotated English-language data, it immediately achieves strong performance on the MFV ($F1 = 0.81$) and CCS ($F1 = 0.82$) datasets, and moderate performance on the CCV ($F1 = 0.65$) dataset. This performance exceeds that of XLM-T with the same English-language training data ($F1 = 0.63$). %It suggests that Llama3.1-8b might be a better base model than XLM-T. 

The model is fine-tuned with a data augmentation strategy where the English labeled data are machine translated into Chinese and fed into the LLM again. This step significantly improves the model performance and results in the best performance for all three benchmarking dataset (MFV $F1 = 0.86$; CCS $F1 = 0.82$; CCV $F1 = 0.74$). These F1 scores exceed all other approaches tested in this paper. 

Additionally, we notice that English prompts generally outperform non-English prompts on Llama3.1-8b, even when analyzing non-English documents. This difference is more pronounced in the base instruct model but becomes less significant with progressive fine-tuning using annotated data. After fine-tuning with English data, the difference in the F1 scores for English and Chinese prompts on the CCV dataset decreases from 0.11 to 0.08, further reducing to 0.03 with data augmentation and 0.02 when fine-tuned with both English and augmented data.

%This difference was more pronounced in the base instruct model, becoming less significant after increasingly fine-tuning with annotated data. After fine-tuning with English data, the F1 gap decreases from 0.11 to 0.08 and is further reduced to 0.03 with data augmentation and 0.02 when fine-tuned with both English data and augmented data.

Moreover, a better LLM base model may further improve the model performance. Table \ref{tab:llama-70b} shows that under the same condition, Llama3.1-70b outperforms Llama3.1-8b when prompting in local languages. In the CCV dataset, under a few-shot learning setup with Chinese prompting, the larger LLM ($F1 = 0.60$) is better than both its smaller counterpart ($F1 = 0.42$) and the English prompting counterpart ($F1 = 0.51$). 

\begin{table}[tb]
    {\small % Set text size to normal
    \centering  % Ensures the table is centered horizontally
    \setlength{\tabcolsep}{1mm}
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccccccccc}
    \toprule
    &\textbf{Auth} &\textbf{Care} &\textbf{Fair} &\textbf{Loya} &\textbf{Sanc} &\textbf{Acc} &\textbf{Cov} &\textbf{Fw} &\textbf{Fm} \\
    \textbf{MFV} & & & & & & & & & \\
    \midrule
    Baseline &0.28 &0.30 &0.13 &0.18 &0.11 &0.23 &1.00 &0.23 &0.20 \\
    en$\times\varnothing$ &0.41 &0.78 &0.40 &0.35 &\textbf{0.40 }&0.55 &\textbf{0.61 }&0.54 &0.47 \\
    zh$\times\varnothing$ &0.56 &0.40 &0.25 &0.00&0.00&0.42 &0.36 &0.34 &0.24 \\
    en$\times$en &0.89 &0.87 &0.94 &0.57 &0.00&0.84 &0.41 &0.81 &0.65 \\
    zh$\times$en &0.82 &0.88 &0.75 &0.67 &0.00&0.79 &0.32 &0.77 &0.62 \\
    en$\times$zh &0.89 &0.86 &0.90 &\textbf{0.80} &0.00 &0.86 &0.54 &0.85 &0.69 \\
    zh$\times$zh &0.91 &0.83 &0.91 &0.75 &0.50 &0.85 &0.58 &0.84 &\textbf{0.78} \\
    en$\times$(en+zh) &0.88 &\textbf{0.89 }&\textbf{0.95} &\textbf{0.80} &0.00&\textbf{0.87} &0.60 & \textbf{0.86} &0.70 \\
    zh$\times$(en+zh) &\textbf{0.93} &0.88 &0.91 &0.77 &0.00&\textbf{0.87 }&0.60 &0.85 &0.70 \\
    \midrule
    \textbf{CCS} & & & & & & & & & \\
    \midrule
    Baseline &0.23 &0.27 &0.18 &0.17 &0.16 &0.21 &1.00 &0.21 &0.22 \\
    en$\times\varnothing$ &0.58 &0.78 &0.71 &0.64 &0.71 &0.70 &\textbf{0.93} & 0.69 &0.68 \\
    zh$\times\varnothing$ &0.66 &0.80 &0.71 &0.63 &\textbf{0.72} &0.71 &0.82 &0.71 &0.71 \\
    en$\times$en &\textbf{0.78} &\textbf{0.85} &\textbf{0.88} &0.84 &0.68 &\textbf{0.82 }&0.52 & \textbf{0.82} &0.80 \\
    zh$\times$en &0.75 &0.84 &\textbf{0.88} &0.80 &0.69 &0.80 &0.45 &0.80 &0.79 \\
    en$\times$zh &0.73 &\textbf{0.85} &0.84 &0.85 &\textbf{0.72} &0.81 &0.56 &0.80 &0.80 \\
    zh$\times$zh &0.74 &0.82 &0.85 &0.86 &0.65 &0.80 &0.56 &0.80 &0.78 \\
    en$\times$(en+zh) &0.76 &\textbf{0.85} &0.86 &\textbf{0.87} &0.69 &\textbf{0.82} &0.55 & \textbf{0.82} &\textbf{0.81} \\
    zh$\times$(en+zh) &0.74 &0.84 &0.85 &\textbf{0.87} &0.70 &0.81 &0.57 &0.81 &0.80 \\
    \midrule
    \textbf{CCV} & & & & & & & & & \\
    \midrule
    Baseline &0.18 &0.40 &0.16 &0.23 &0.03 &0.27 &1.00 &0.27 &0.20 \\
    en$\times\varnothing$ &0.29 &0.69 &0.47 &0.50 &0.06 &0.54 &\textbf{0.80} &0.53 &0.40 \\
    zh$\times\varnothing$ &0.29 &0.70 &0.35 &0.16 &0.13 &0.48 &0.61 &0.42 &0.32 \\
    en$\times$en &0.28 &0.83 &0.68 &0.55 &0.00 &0.66 &0.62 &0.65 &0.47 \\
    zh$\times$en &0.19 &0.82 &0.59 &0.23 &0.00 &0.59 &0.41 &0.57 &0.37 \\
    en$\times$zh &0.39 &\textbf{0.85} &0.77 &0.69 &0.36 &0.73 &0.68 &0.72 &0.61 \\
    zh$\times$zh &0.32 &0.81 &0.73 &0.70 &0.36 &0.71 &0.69 &0.69 &0.59 \\
    en$\times$(en+zh) &\textbf{0.42} &\textbf{0.85} &\textbf{0.79} &\textbf{0.72 }&\textbf{0.43 }&\textbf{0.75} &0.72 &\textbf{0.74} &\textbf{0.64} \\
    zh$\times$(en+zh) &0.36 &0.84 &0.75 &\textbf{0.72} &0.31 &0.73 &0.72 &0.72 &0.60 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Llama3.1-8b model with few-shot learning prompts for cross-language moral foundation measurements. The first language refers to the language of prompt while the languages after $\times$ refer to the language(s) of fine-tuning datasets. $\varnothing$ denotes the empty set when no fine-tuning is done. For example, zh$\times$(en+zh) denotes a Chinese prompt on a model fine-tuned using English and Chinese data. The best performing method per dataset is in \textbf{bold}. Column labels are consistent with Table \ref{tab:mt}.}
    \label{tab:llama-8b}
\end{table}

% However, it should be noted that llama3.1 is not really multilingual. Officially it only support 6 languages, Chinese is not included though it can do a decent job on understanding Chinese. And according to the multilingual leading board, GPT-4o is way better than Llama3.1. 

% \textit{Notes} the reason to choose generative AI models: (1) accessibility --- conversation format; (2) reasoning ability, less training data; (3) multilingual. So, the purpose of testing LLMs here is to (a) use no or very few local language annotated data, (b) don't have to train/finetune the model with heavy coding.


\begin{table}[!ht]
    {\small % Set text size to normal
    \centering  % Ensures the table is centered horizontally
    \setlength{\tabcolsep}{1mm}
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccccccccc}
    \toprule
    &\textbf{Auth} &\textbf{Care} &\textbf{Fair} &\textbf{Loya} &\textbf{Sanc} &\textbf{Acc} &\textbf{Cov} &\textbf{Fw} &\textbf{Fm} \\
    \textbf{MFV} & & & & & & & & & \\\midrule
    Baseline &0.28 &0.30 &0.13 &0.18 &0.11 &0.23 &1.00 &0.23 &0.20 \\
    8b en  &0.41 &0.78 &0.40 &0.35 &0.40 &0.55 &0.61&0.54 &0.47 \\
    8b zh &0.56 &0.40 &0.25 &0.00 &0.00 &0.42 &0.36 & 0.34 &0.24 \\
    70b en  &0.67 &\textbf{0.82} &0.52 &0.48 &0.55 &\textbf{0.66} &\textbf{0.83 }&\textbf{0.66} &0.61 \\
    70b zh &\textbf{0.73} &0.67 &\textbf{0.53} &\textbf{0.56} &\textbf{0.60} &0.63 &0.64 &0.64 &\textbf{0.62} \\
    \midrule
    \textbf{CCS} & & & & & & & & & \\\midrule
    Baseline &0.23 &0.27 &0.18 &0.17 &0.16 &0.21 &1.00 &0.21 &0.22 \\
    8b en  &0.58 &0.78 &\textbf{0.71} &0.64 &0.71 &0.70 &0.93 &0.69 &0.68 \\
    8b zh  &\textbf{0.66} &0.80 &\textbf{0.71} &0.63 &\textbf{0.72} &\textbf{0.71} &0.82 &\textbf{0.71} &\textbf{0.71} \\
    70b en &0.54 &0.77 &0.63 &0.68 &0.65 &0.67 &\textbf{0.96} &0.66 &0.65 \\
    70b zh  &0.56 &\textbf{0.83 }&0.63 &\textbf{0.71 }&0.70 &0.70 &0.89 &0.69 &0.69 \\
    \midrule
    \textbf{CCV} & & & & & & & & & \\
    \midrule
    Baseline &0.18 &0.40 &0.16 &0.23 &0.03 &0.27 &1.00 &0.27 &0.20 \\
    8b en  &\textbf{0.29} &0.69 &0.47 &0.50 &0.06 &0.54 &0.80 &0.53 &0.40 \\
    8b zh  &\textbf{0.29} &0.70 &0.35 &0.16 &0.13 &0.48 &0.61 &0.42 &0.32 \\
    70b en  &0.18 &0.69 &0.52 &0.41 &0.15 &0.55 &\textbf{0.86} &0.51 &0.39 \\
    70b zh &0.18 &\textbf{0.76} &\textbf{0.54} &\textbf{0.59} &\textbf{0.18} &\textbf{0.62} &0.63 &\textbf{0.60} &\textbf{0.45} \\
    \bottomrule
    \end{tabular}
    
    }
    \caption{\textit{Llama3.1} 8b and 70b models with few-shot prompting for cross-language moral foundation measurement. The best performing method for each dataset is in \textbf{bold}. Column labels are consistent with Table \ref{tab:mt}.}
    \label{tab:llama-70b}
\end{table}

% Last but not least, fine-tuning with more local language annotated data does not contribute to better performance in Llama3.1-7b models as in XLM-T models. In the real-life core value benchmarking dataset, the model performance significantly drops with small amount of local-language annotated training data across all five classes. With more training data (maximum 20 batches), the model performance recovers but is not as good as previous ones which is not fine-tuned with local data at all (see Figure \ref{fig:llama-batch-cv})


\section{Discussion and Limitations} 

% \subsection{Moral Foundation Benchmarking Datasets}
% Expert-crafted texts (MFV) are often expected to perform better than crowd-sourced scenarios (CCS) in expressing MF values due to their explicit design. However, results indicate otherwise: CCS often outperforms MFV in evaluations, especially with local language lexicons and LLMs. This finding aligns with \citet{weberExtracting2018}, suggesting that expert-crafted scenarios may not capture nuanced moral expressions more effectively than crowd-sourced texts. For benchmarking computational MF measurements, relying solely on expert coding might not be ideal\footnote{\textit{Communication Monograph}, vol. 88, 2021, issue 3, discusses the strengths and limitations of crowd-sourced versus expert approaches to measuring MF.}. 

% Additionally, we observed a significant performance drop when shifting from lab-crafted datasets (MFV and CCS) to real-world data (CCV), raising concerns about overestimating performance using reverse-labeled datasets. When developing local language tools, some scholars validate only on crowd-sourced data, which potentially is an over-estimation of the model performance \citep[e.g.,][]{matsuo2019development, cheng2023c}. To create valid MF benchmarks, we recommend using real-world data annotated by crowd-sourced participants.

% \subsection{Machine Translation}
We find traditional approaches, such as machine translation and local language lexicons, may not be the most effective solutions for cross-language moral foundation measurements. Instead, fine-tuning multilingual language models and leveraging LLMs through transfer learning emerge as promising alternatives. Notably, LLMs demonstrate strong performance and data efficiency, making them particularly well-suited for addressing the challenges of cross-language MF analysis. In this section, we extend the discussion to the general human value elements measurements in computational social science research. 

Above all, the machine translation approach has proven unreliable in cross-language MF measurement. Even with state-of-the-art English tools, machine translation remains suboptimal in estimating MFs from real-world non-English corpora and, in some cases, performs worse than chance. This issue is particularly concerning for measuring culturally significant human values. For example, within MFT, previous studies highlight East--West cultural differences in emphasis on various foundation values \citep{graham2011mapping}. Participants from Eastern regions (South Asia, East Asia, and Southeast Asia) generally place a stronger emphasis on ``loyalty'' compared to their Western counterparts (United States, Canada, United Kingdom, and Western Europe) \citep{graham2013moral}. In our test, however, Mformer’s ``loyalty'' classification on translated data ($F1 = 0.21$) falls below random chance ($F1 = 0.23$), suggesting substantial loss of this cultural nuance. Our results caution against over reliance on machine translation in cross-language MF applications and raise broader methodology concerns for its reliability in measuring other cross-language human values.

% The machine translation approach, particularly with Mformer, achieved the best performance among English-based classifiers for translated data. However, even with state-of-the-art models, results are weak in real-world settings, and certain foundations perform worse than random guessing. Machine translation struggles with culturally distinct foundations. For instance, previous studies show that there are East-West cultural differences in people’s concerns about different foundation values \citep{graham2011mapping}. Eastern participants (South Asia, East Asia, and Southeast Asia) showed stronger concerns about loyalty compared to their Western counterparts (United States, Canada, United Kingdom, and other Western European countries) in surveys. However,results for MFormer’s loyalty classification on translated data ($F1 = 0.21$) were below random guessing ($F1 = 0.23$), indicating significant cultural nuance loss. This limitation complicates downstream MF measurements in cross-language settings.

% \subsection{Local Language Lexicons}
Researchers should also be cautious with local language lexicons---they perform worse than the machine translation for cross-language MF measurement in some cases. Given the inherent limitations of lexicon-based methods and the extensive resources required to develop them, they are inefficient for cross-language MF measurement, particularly at the document level where semantic complexity is higher. However, culturally specific moral lexicons may still provide valuable insights at the vocabulary level for cross-cultural research, though broader applications should be approached cautiously and with extra validations.

% \subsection{Language Models}
Our evaluation with multilingual language models and LLMs also reveals several key implications. Above all, fine-tuning LLMs is more data-efficient than fine-tuning multilingual language models for cross-language MF measurement. In the CCV benchmarking dataset, XLM-T requires on average more than 2,000 local-language annotated records (20 batches) to achieve strong performance (i.e., $F1 = 0.75$). In contrast, Llama3.1-8b can reach comparable performance with only English data machine-translated to Chinese and thus no local language labeling. 

In addition, data augmentation shows strong potential in enhancing LLMs’ performance on culturally distinct MF values. For example, Llama3.1-8b notably improves on ``loyalty'' when fine-tuned with English data machine-translated to Chinese ($F1 = 0.69$), compared to its performance with the original English training data ($F1 = 0.57$). Such data augmentation may serve as a practical and cost-effective way to improve model performance in cross-language measurements, especially in cases where mass-labeling of local-language data is infeasible due to the resource-intensive process of creating high-quality language-specific labeled datasets.

%LLMs show slightly better transfer learning outcomes than multilingual language models. For example in the CCV dataset, with the same fine-tuning dataset, the $F1$ scores of Llama3.1-8b is 0.65 while XLM-T is 0.63. 
% Though the performance of multilingual models improved with fine-tuning on local-language data, achieving strong performance (i.e., $F1 = 0.75$) required on average more than 2,000 annotated local language records (20 batches), which severely limited the data efficiency of this approach. In contrast, Llama3.1-8b reached comparable performance using only English data (including the data augmentation), requiring no additional local language annotations. %This suggests that LLMs may offer a more resource-effective and better alternative for cross-language MF measurement than the multilingual language model approach, especially in contexts with limited non-English labeled data. 

% Also, the data augmentation strategy notably enhanced LLMs' performance on culturally distinct MF values. For example, performance on ``loyalty'' — a culturally nuanced moral foundation that differs between Eastern and Western contexts — improved significantly when fine-tuned with machine-translated Chinese data ($F1 = 0.69$) compared to original English training data ($F1 = 0.57$). It suggests that augmenting English MF annotations through machine translation to local languages can effectively help LLMs in capturing cultural nuances more in MF measurements. Such a data augmentation approach may be more practical and cost-effective to measure MF from non-English corpora than gathering extensive local-language annotations, given the resource-intensive nature of developing high-quality, language-specific labeled datasets.

Our findings also suggest that larger or more advanced LLMs such as Llama3.1-70b may perform better for measuring MFs in non-English corpora. That is, updating the base LLM to a more power model with enhanced multilingual capabilities could  further improve the effectiveness of the LLM approach for cross-language MF measurement.
% expanding the base LLMs scale or leveraging LLMs that are specifically optimized for multilingual tasks csould provide substantial gains in accuracy and cultural nuance handling cross-language MF measurement. 

% Particularly, the data augmentation strategy can significantly improve LLMs' performance on cultural distinct MF values. LLMs' performance on ``loyalty'' --- a representative culturally distinct moral value between the East and the West --- effectively improved after fine-tuned with Chinese translated annotated data compared to English annotated data . In other words, data augmentation by machine translating English annotated MF data to non-English languages can significantly help LLMs capture cultural nuances in measuring MF in the local language. .

% Additionally, English prompts generally performed better than non-English ones on Llama3.1-8b for non-English classification, likely due to its smaller multilingual capacity. However, in Llama3.1-70b, Chinese prompts outperformed English in few-shot learning, indicating that larger models may yield better non-English baselines. Given that Llama3.1-8b is not the best multilingual model on the HuggingFace multilingual task leaderboard, larger or more specialized LLMs may further improve cross-language MF measurement.

Nevertheless, several limitations of using  LLMs  for cross-language MF measurement should be acknowledged. Firstly, their performance on fine-grained MF values can vary. For example, although the measurement on some culturally sensitive values like ``loyalty'' is relatively reliable, others like ``authority'' and ``sanctity'' still miss significant cultural nuances. Careful prompt crafting and fine-tuning LLMs for  binary instead of multi-label classification task may help mitigate this limitation. 

Secondly, the model coverage remains a challenge. Our tested four Llama models in Table \ref{tab:llama-70b} on average successfully detect 72.50\% of the MF values in the CCV dataset, with coverage varying with the prompting language and fine-tuning data. This issue can be partially attributed to the model safety policies as Llama refuses to respond to certain prompts. Some documents in the benchmarking dataset contain morally controversial content that may prevent Llama from generating outputs---a limitation that is less prominent in multilingual language models and other approaches. %Future research should further investigate the causes of this limitation. 

Thirdly, fine-tuning LLMs with limited local-language annotated data might backfire. While training with local-language annotated data is commonly used to incorporate cultural specificity and enhance transfer learning, this approach appears less effective for LLMs when data volume is small. %In this study, applying the same local-language fine-tuning strategy used in XLM-T to Llama3.1-8b results in a significant performance drop. 
We fine-tuned Llama3.1-8b using 20 batches of CCV data with the same strategy used in XLM-T to test data efficiency. Each batch containing 50 MF labels evenly distributed across five classes. As shown in Figure \ref{fig:llama-batch-cv}, %increasing the volume of local-language data leads to a decline in model performance, which 
The initial drop in model performance struggles to recover within the available data for care, fairness and sanctity classifiers. Researchers with limited local-language annotated data should be cautious about potential performance degradation when fine-tuning LLMs for cross-language MF measurements. 

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{icwsm_llama_batch_cv2.pdf} 
\caption{Accumulated fine-tuning multilingual language model Llama3.1-8b with local language annotated data from the Chinese CV dataset. Each batch includes 50 items evenly distributed across five classes.}
\label{fig:llama-batch-cv}
\end{figure}


% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{icwsm_llama_batch_cv.pdf} 
%     \caption{Accumulated fine-tuning Llama3.1-8b with local language annotated data from CCV dataset. Each batch included 50 items evenly distribute across five classes.}
%     \label{fig:llama-batch}
% \end{figure}

In addition, it is important to acknowledge potential limitations in our mapping scheme. The CCV dataset was originally labeled based on Chinese core values, and the mapping process to moral foundation framing may have led to some cultural information loss.

Finally, we would like to highlight the essential role of human validation in measuring cross-language MFs with  LLMs. Given the limitations discussed, human validation is strongly recommended to ensure the reliability of LLM-based cross-language MF measurements. LLMs can effectively assist in this process by providing rationales alongside their classification outputs, supporting human evaluators and facilitating a more efficient assessment of cross-language MF measurements.

In summary, our findings suggest LLMs trained on English, combined with prompt-engineering and data augmentation can achieve robust performance with strong reliability and data efficiency in cross-language MF measurements. Local-language annotations should focus on the culturally distinct moral values to further enhance the performance. 

%If a large-scale of local-language annotations are available (e.g., more than 2,000 labeled records), fine-tuning a multilingual model like XLM-T is a viable option.

\section{Conclusions}
This study examined computational approaches for automatically measuring moral foundation values in non-English texts. It uses Chinese as a case study and leverages established English resources for this cross-language deductive coding task. 

It first highlighted the limitations of dictionary and machine translation approaches: while local language dictionaries can support lexicon-level analysis, they often lack the depth needed for complex semantic assessments. Notably, advanced English-based tools applied to machine-translated data outperform local lexicon-based methods, which underscores the limitations of lexicon approaches in this task. The study then explores the potential of transfer learning in language models, showing that both multilingual language models and LLMs demonstrate strong performance, with LLMs performing better and being more data-efficient. It is recommended to select approaches based on the availability of local-language annotated data. When sufficient data is available, a smaller multilingual language model generally yields satisfactory results. Otherwise, LLMs  fine-tuned and augmented on English resources offer the best performance, though with higher computational demands. Finally, this study emphasizes the necessity of human validation in cross-language MF measurement. The reasoning capability of LLMs can significantly support this process by generating rationales for classification decisions, assisting human evaluators with more efficient review and reliable assessments. 

The findings in this paper provide valuable insights for cross-cultural MF research and shed light on future applications of LLM-assisted deductive coding in multilingual tasks. 

%% use \small (9pt) size for reference list to sav esome space
%% this is allowed in aaai25 latex submission \section{reference}
%% TODO 20241119 - prob need to reduce the reference list, now it takes more than 2 pages, ideally 1 and a half. 
% {\small
% \bibliography{reference}
% }
{
\bibliographystyle{aaai} 
\bibliography{reference}
}

%%%%%%% add appendix page
\onecolumn
\section{Appendix} 
The appendix presents the mapping scheme of Chinese core values to moral foundation values.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\pdfpagewidth]{cv_map.jpg} 
    % \caption{Mapping Scheme of Chinese Core Value Dataset}
    % \label{fig:cv_mapping}
\end{figure}

\end{document}
