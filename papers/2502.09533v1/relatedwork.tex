\section{Related Work}
\textbf{GAN-Based Methods.}  
GAN-based approaches~\cite{kim2018deep, zhou2020makelttalk, pumarola2018ganimation, vougioukas2020realistic, zhang2023sadtalker, wang2021one, hong2022depth, chan2022efficient, guo2024liveportrait} for TalkingFace generation extract motion features from audio or visual inputs and map them to intermediate representations such as facial landmarks~\cite{yang2023effective}, 3DMM~\cite{sun2023vividtalk}, or HeadNeRF~\cite{hong2022headnerf}. MakeItTalk~\cite{zhou2020makelttalk} employs LSTMs to predict landmarks from audio, followed by a warp-based GAN for video synthesis. GANimation~\cite{pumarola2018ganimation} models facial motion via continuous manifolds, enhancing expression dynamics. SadTalker~\cite{zhang2023sadtalker} integrates ExpNet and PoseVAE to refine motion representations within the FaceVid2Vid~\cite{wang2021one} framework. DaGAN~\cite{hong2022depth} introduces self-supervised geometric learning to capture dense 3D motion fields. While effective, GAN-based methods suffer from adversarial training instability and motion inaccuracies, often resulting in artifacts that degrade realism.

\textbf{Diffusion-Based Methods.}  
Diffusion models~\cite{rombach2022high} have gained traction in TalkingFace generation, producing high-quality, diverse outputs. AniPortrait~\cite{wei2024aniportrait} maps audio to 3D facial structures, generating temporally coherent videos with expressive detail. MegActor-$\Sigma$~\cite{wang2024v} synchronizes lip movements, expressions, and head poses using a reference UNet~\cite{hu2024animate} and facial loss functions to enhance fidelity. Hallo~\cite{xu2024hallo} and EchoMimic~\cite{chen2024echomimic} leverage limited motion reference frames to improve expression diversity and pose alignment. However, reliance on short-term frame histories ($2$-$4$ frames) compromises long-term motion consistency, while increased frame dependencies escalate computational costs. Additionally, static audio features and restricted references fail to capture natural motion variations, leading to artifacts such as motion distortion and rigid expressions in extended sequences.

Unlike prior work, our approach introduces motion priors from both archived and present clips to enhance long-term motion prediction and identity consistency. By leveraging historical frames and memory-efficient temporal attention, MCDM improves motion continuity while maintaining realism in TalkingFace generation.