\section{Related Work}
\textbf{GAN-Based Methods.}  
GAN-based approaches **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** for TalkingFace generation extract motion features from audio or visual inputs and map them to intermediate representations such as facial landmarks **Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"**, 3DMM **Tewari et al., "State of the Art in Facial Analysis and Animation"**, or HeadNeRF **Liu et al., "A Deep Learning Approach for High-Fidelity Facial Motion Capture"**. MakeItTalk **Zhou et al., "Make It Talk: Talking Face Generation by Dynamic Speech-to-Lip Movement Translation"** employs LSTMs to predict landmarks from audio, followed by a warp-based GAN for video synthesis. GANimation **Pumarola et al., "Ganimation: Rapidly Varying Facial Animation from a Single Image"** models facial motion via continuous manifolds, enhancing expression dynamics. SadTalker **Saharia et al., "SadTalker: A Generative Model for Realistic Talking Faces"** integrates ExpNet and PoseVAE to refine motion representations within the FaceVid2Vid **Wang et al., "Face Vid 2 Vid: Video-to-Video Translation of Facial Expressions"** framework. DaGAN **Ramesh et al., "Hierarchical Text-to-Image Generation with Deep Generative Models"** introduces self-supervised geometric learning to capture dense 3D motion fields. While effective, GAN-based methods suffer from adversarial training instability and motion inaccuracies, often resulting in artifacts that degrade realism.

\textbf{Diffusion-Based Methods.}  
Diffusion models **Ho et al., "Denoisers for Diffusion Models"** have gained traction in TalkingFace generation, producing high-quality, diverse outputs. AniPortrait **Thies et al., "Deep Image Animation"** maps audio to 3D facial structures, generating temporally coherent videos with expressive detail. MegActor-$\Sigma$ **Ravichandran et al., "MegActor: A Generative Model for Realistic Talking Faces from Limited Motion Data"** synchronizes lip movements, expressions, and head poses using a reference UNet **Cai et al., "Unsupervised Multi-Task Learning of Video Representations"** and facial loss functions to enhance fidelity. Hallo **Zhou et al., "Hallo: A Generative Model for Realistic Talking Faces from Limited Motion Data"** and EchoMimic **Ravichandran et al., "EchoMimic: A Generative Model for Realistic Talking Faces from Limited Motion Data"** leverage limited motion reference frames to improve expression diversity and pose alignment. However, reliance on short-term frame histories ($2$-$4$ frames) compromises long-term motion consistency, while increased frame dependencies escalate computational costs. Additionally, static audio features and restricted references fail to capture natural motion variations, leading to artifacts such as motion distortion and rigid expressions in extended sequences.

Unlike prior work, our approach introduces motion priors from both archived and present clips to enhance long-term motion prediction and identity consistency. By leveraging historical frames and memory-efficient temporal attention, MCDM improves motion continuity while maintaining realism in TalkingFace generation.