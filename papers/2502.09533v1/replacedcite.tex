\section{Related Work}
\textbf{GAN-Based Methods.}  
GAN-based approaches____ for TalkingFace generation extract motion features from audio or visual inputs and map them to intermediate representations such as facial landmarks____, 3DMM____, or HeadNeRF____. MakeItTalk____ employs LSTMs to predict landmarks from audio, followed by a warp-based GAN for video synthesis. GANimation____ models facial motion via continuous manifolds, enhancing expression dynamics. SadTalker____ integrates ExpNet and PoseVAE to refine motion representations within the FaceVid2Vid____ framework. DaGAN____ introduces self-supervised geometric learning to capture dense 3D motion fields. While effective, GAN-based methods suffer from adversarial training instability and motion inaccuracies, often resulting in artifacts that degrade realism.

\textbf{Diffusion-Based Methods.}  
Diffusion models____ have gained traction in TalkingFace generation, producing high-quality, diverse outputs. AniPortrait____ maps audio to 3D facial structures, generating temporally coherent videos with expressive detail. MegActor-$\Sigma$____ synchronizes lip movements, expressions, and head poses using a reference UNet____ and facial loss functions to enhance fidelity. Hallo____ and EchoMimic____ leverage limited motion reference frames to improve expression diversity and pose alignment. However, reliance on short-term frame histories ($2$-$4$ frames) compromises long-term motion consistency, while increased frame dependencies escalate computational costs. Additionally, static audio features and restricted references fail to capture natural motion variations, leading to artifacts such as motion distortion and rigid expressions in extended sequences.

Unlike prior work, our approach introduces motion priors from both archived and present clips to enhance long-term motion prediction and identity consistency. By leveraging historical frames and memory-efficient temporal attention, MCDM improves motion continuity while maintaining realism in TalkingFace generation.