\section{Related Work}
Our approach is in line with methodologies employing a two-stage architecture for semantic parsing tasks, akin to works such as **Li et al., "Coarse2Fine: A Two-Stage Framework for Semantic Parsing"**__**Zhang et al., "STaG-QA: Stack-Augmented Graph Question Answering"**__**Huang et al., "HGNet: Heterogeneous Graph Neural Networks for Knowledge Graph Question Answering"**, where questions are first mapped to an initial outline and then filled in details later. However, they overlook condition expressions or constraints in SPARQL queries, whereas OntoSCPrompt's SPARQL structure representation is more comprehensive, enhancing KGQA generalization.
Most existing KGQA systems lack generalization as they are either typically tailored to a specific KG or focus only on within-a-KG generalization. While some methods have demonstrated limited ability to generalize across KGs, particularly in handling assertion heterogeneity between datasets such as **Yao et al., "WebQSP: A Large-Scale Web-Question Answering Dataset"** (Freebase) and **Guo et al., "MetaQA: A Question Answering Benchmark for Metaworld"** (Wikimovies), they fail to generalize across different schemas or topologies. LLMs also suffer from issues like hallucinations and factual inaccuracy when answering questions__**Kang et al., "LLMs on KGQA: A Study on Hallucination and Factual Inaccuracy"**, specifically in the knowledge-intensive task KGQA__. Some studies__**Wang et al., "KG-Augmented Prompt for Question Answering over Knowledge Graphs"** resort to KG-augmented prompt, i.e., injecting question-related factual information (e.g., KG triples) into predefined templates. Hallucinations still remain in the context of KGQA generalization as they adapt to heterogeneous KGs. In this work, we integrate the ontology verbalized in a unified way into the prompt and guide LLMs to fulfil our task, facilitating reasoning over KG ontology.