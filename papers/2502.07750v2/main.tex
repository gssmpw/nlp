\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024
\usepackage[plain,noend,ruled,linesnumbered]{algorithm2e}
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{booktabs} % for better formatting of tables
\usepackage{array}    % for better column formatting
\usepackage[numbers,sort&compress]{natbib}
\usepackage{color}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{PFedDST: Personalized Federated Learning with Decentralized Selection Training}


\author{\IEEEauthorblockN{Mengchen Fan$^{a}$, Keren Li$^{b}$, Tianyun Zhang$^{c}$, Qing Tian$^{a}$ and Baocheng Geng$^{a}$}
\IEEEauthorblockA{$^a$\textit{Department of Computer Science, University of Alabama at Birmingham, Birmingham, US}\\
$^b$\textit{Department of Mathematics, University of Alabama at Birmingham, Birmingham, US}\\
$^c$\textit{Department of Computer Science, Cleveland State University, Cleveland, US}
}
}


\maketitle

\begin{abstract}
% Distributed learning (DL) provides a decentralized framework for machine learning, enabling models to
% train on data spread across multiple devices or clients. However, non-IID local data will
% lead to degradation of model accuracy and the heterogeneity
% of edge nodes inevitably slows down model training efficiency.
% Moreover, to avoid the potential communication bottleneck in
% the parameter-server-based FL, we concentrate on the Decentralized Federated Learning (DFL) that performs distributed
% model training in Peer-to-Peer manner. 

% To address these
% challenges, we propose a partial-freeze DFL system by incorporating communication selection, which means that each local model will score the neighbors and have the high priority to select the high scores client.  Our experimental
% results demonstrate that PFedDST achieves state-of-the-art accuracy in data heterogeneity scenarios, demonstrating the efficiency of the
% selection and partial-freeze training. Furthermore, we demonstrate that our method can easily adapt to heterogeneous local clients with
% varying computation complexities and achieves
% better convergence rate.

% Distributed learning (DL) offers a framework for machine learning, allowing models to train across data distributed over multiple devices or clients. However, non-IID local data can degrade model accuracy, and the heterogeneity of edge nodes tends to reduce the efficiency of model training. Furthermore, to solve potential communication bottlenecks in Federated Learning (FL), we focus on Decentralized Federated Learning (DFL), which facilitates distributed model communication in a Peer-to-Peer manner.
% To address these challenges, we propose a partial-freeze Decentralized Federated Learning (PFedDST) framework that incorporates selective communication. In PFedDST, each local model scores its neighbors and prioritizes connections with higher-scoring clients. The experimental results show that PFedDST achieves state-of-the-art accuracy in data heterogeneity distribution, highlighting the effectiveness of our selection strategy and partial-freeze training approach. Additionally, this paper demonstrates that PFedDST adapts to heterogeneous local clients with varying computational complexities and achieves a faster convergence rate.

% Distributed Learning (DL) allows training machine learning models on data spread across multiple devices. However, challenges arise when data is non-IID and the varied capabilities of devices can slow down the training process. Communication between devices can also become a bottleneck, especially in traditional Federated Learning (FL) setups.
% To tackle these issues, we propose a partial-freeze Decentralized Personalized Federated Learning (PFedDST) framework, which enables devices to strategically evaluate and select neighbors with higher scores for communication.
% Our experiments show that PFedDST achieves state-of-the-art accuracy in data heterogeneity distribution, highlighting the effectiveness of our selection strategy and partial-freeze training approach. It effectively manages the differing computational abilities of each device, ensuring that the overall network trains more efficiently. The results show that PFedDST obtains higher accuracy and faster convergence in training models across diverse and decentralized setups.

%Distributed Learning (DL) allows training machine learning models on data spread across multiple devices, but challenges such as non-IID data and device capability variations can slow down the process. Communication bottlenecks are also common in traditional Federated Learning (FL) setups. To address these issues, we introduce a partial-freeze Decentralized Personalized Federated Learning (PFedDST) framework. This framework allows devices to smartly evaluate and choose neighbors with higher communication scores. PFedDST employs a segmented training approach that accelerates training communication and enhances personalization. Our experiments show that PFedDST achieves state-of-the-art accuracy in data heterogeneity distribution, highlighting the effectiveness of our selection strategy and partial-freeze training approach. It efficiently handles the different computational powers of each device, ensuring faster and more effective training across diverse and decentralized systems. The results show that PFedDST enhances accuracy and speeds up model training convergence.

Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with
Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems.

\end{abstract}

 \begin{IEEEkeywords}
Personalized federated learning, distributed systems, heterogeneity, decentralized learning.
 \end{IEEEkeywords}

\section{Introduction}
% {\color{blue} I changed the abstract, check if it is correct. we still need to add some references in the introduction.}
% Interest in leveraging signal processing and machine learning (ML) for critical predictive tasks across various fields is on the rise. The strategy of integrating information from multiple sources, such as sensors, for
% information gathering promises enhanced outcomes by offering multiple viewpoints on a single phenomenon. Additionally, the global shift towards stricter data privacy laws, with an increasing number of entities implementing regulations against the sharing of sensitive data like health information, has spurred progress in distributed learning and decision-making processes that operate without sharing raw data.
% Interest in leveraging signal processing and machine learning (ML) for critical predictive tasks across various fields is increasing. 
There has been growing interest in applying signal processing and machine learning (ML) to critical predictive tasks across various disciplines \cite{zhang2019fusion,10706324,li2023nn,zhai2025machine,zhao2024deep,zhai2024impact}. The strategy of integrating data from multiple sources, such as sensors, enhances outcomes by providing multiple perspectives on a single phenomenon. Moreover, the global trend toward stricter data privacy laws and the growing implementation of regulations that restrict the sharing of sensitive data, such as health information, has accelerated advancements in distributed learning and decision-making processes that function without exchanging raw data \cite{quan2023distributed,quan2023efficient,quan2023ordered,geng2021collaborative,quan2022enhanced}.

% One notable approach, Distributed learning (DL), has
% gained traction for its ability to train ML models on
% decentralized devices. By co-ordinating learning processes among multiple nodes (clients)
% without centrally aggregating raw data, these systems address
% privacy and communication constraints. However, a persistent
% and critical challenge in these settings is heterogeneity, the
% variations in data, resources, tasks, or network characteristics
% across different nodes, that can significantly degrade both
% model accuracy and convergence performance. Therefore, Personalized Federated Learning (PFL) has
% emerged to find the best model for each client since one
% consensus model can not satisfy all clientsâ€™ needs in classical Federated Learning (FL). The existing PFL algorithm can be categorized into two branches in terms of the
% existence of the centralized server (i.e., Centralized Personalized Federated Learning (CPFL)  and Decentralized Personalized Federated Learning (DPFL) . The challenges of centralized communication bottleneck or central failure may incur low communication efficiency or system crash in the federated processing. Thus,
% we focus on the DPFL, which allows edge clients to communicate with each other in a peer-to-peer manner, aiming
% to reduce the communication column of the busiest server
% node and embrace peer-to-peer communication for faster
% convergence. In decentralized FL, clients usually follow
% an undirected and symmetric communication topology to
% reach a consensus model, which means if one
% client receives neighborsâ€™ models, it sends its model back.
Federated learning (FL) has gained prominence for its ability to train models on decentralized devices \cite{chen2021distributed}. FL systems facilitate multi-client learning without centralizing raw data, addressing both privacy and communication challenges. However, \textit{heterogeneity} in data distribution, resource allocation, task objectives, or network characteristics across nodes poses challenges to model accuracy and convergence \cite{fan2025measuringheterogeneitymachinelearning}. Personalized Federated Learning (PFL) \cite{tan2022towards} addresses these issues by tailoring models to specific client needs, thereby enhancing the effectiveness beyond the conventional single-model approach (e.g., \cite{arivazhagan2019federated}) in FL.

PFL is categorized into Centralized Personalized Federated Learning (CPFL) and Decentralized Personalized Federated Learning (DPFL) \cite{lalitha2018fully}. CPFL can suffer from communication bottlenecks and server failures, leading to increased communication traffic and potential system crashes. In contrast, DPFL emphasizes peer-to-peer interactions among edge clients, reducing communication loads on local nodes and promoting faster convergence. In this topology, clients maintain an undirected and symmetric communication structure, facilitating model exchanges with peers.
% In order to satisfy the unique needs of individual clients,
% most existing works in PFL carefully designed the relationships between the global model and personalized models to
% fit the local data distribution via different techniques, such
% as parameter decoupling , knowledge distillation, multi-task learning , model interpolation  and clustering. These techniques can
% also be adopted to improve the personalized performance in
% DPFL. However, the heterogeneity among clients
% exists not only in local data distribution but also in the communication power and computation resources. The
% power level of the wireless channel among clients may be
% different and time-varying in communication networks, and
% some clients may get offline occasionally without sending
% messages to their neighbors. These result in long-term waits
% or incidents of deadlock for their neighbors and also
% lead to poor convergence for the whole system. Besides,
% there is no reason to expect that the exchanged models are
% trained at the same convergence level due to the heterogeneous computation resources. Clients may receive excessive poor-performing models which can not help their training and degrade the personalized performance.

Most existing PFL approaches finely tune the interactions between global and personalized models to accommodate local data variations using methods such as regularization \cite{li2020federated}, knowledge distillation \cite{gou2021knowledge}, multi-task learning \cite{marfoq2021federated}, and clustering \cite{sattler2020clustered}. These techniques aim to enhance personalized performance in the heterogeneous setting. For example, approaches like FedPer \cite{arivazhagan2019federated} propose to capture personalization aspects in
federated learning by viewing deep learning models as base and personalization layers. And FedBABU \cite{oh2021fedbabu} utilize a single global feature representation coupled with multiple local classifiers, differing in how they manage the relationship between the shared representation and the individual linear components. FedFusion \cite{10706324} utilizes a representation method to fuse the batch information to solve the heterogeneity problem. Cho et al. \cite{cho2020client} provides theoretical convergence analysis for these algorithms under general non-convex conditions. DFedAvgM \cite{sun2022decentralized} employs multiple local iterations with SGD and quantization techniques to reduce communication overhead. Dis-PFL \cite{dai2022dispfl} designs personalized models and pruned masks for each client to personalized convergence. OSGP \cite{assran2019stochastic}, DfedPGP \cite{liu2024decentralized}, and AsyNG \cite{chen2023enhancing} utilize the push-sum method to enhance training efficiency.


Despite ongoing efforts, DPFL methodologies continue to face slow convergence rates during aggregations, a challenge compounded by heterogeneous data distributions among clients. Additionally, \textit{disparities in communication bandwidth and computational capabilities} complicate these issues further, leading to unstable communication channels between clients. As a result, clients are compelled to selectively engage with only a limited subset of peers for communication.%Furthermore, the power levels in client communication channels can fluctuate over time, complicating network stability. %Additionally, clients may occasionally go offline unexpectedly, failing to send updates to their neighbors. These issues can lead to prolonged waiting times or even deadlocks, affecting the convergence of the entire system. Moreover, due to varying computational resources, there is no guarantee that models exchanged between clients are trained to the same level of convergence. Consequently, clients may receive models that do not contribute positively to their training efforts and thus affect personalized performance.
% To tackle the data heterogeneity and client heterogeneity
% problems together, we propose PFedDST, a Decentralized
% selection training based Personalized Federated Learning
% approach, to solve the personalized FL problem using score strategy to find the benefit communication choices in the decentralized setting.
% Instead of deploying the consensus model for each user,
% PFedDST allows each client to own their personalized unique
% model's header, allowing them
% to better adapt to their local data. Specifically, the decentralized selection training technique mainly consists of three steps: First, the active client will grab the information in the client pool and use score strategy, which contains the information with the header and loss, and then combine the selection history to got a score for each avaliable client. Second, after the each client have final dicision, the local client need to directed collect the whole of feature extraction information from selected clients. Third, do the feature extraction training and push to the out neighbors and then do the header training and push to the selection pool and update the information. 

To address these challenges, we introduce Personalized Federated Learning with
Decentralized Selection Training (PFedDST), a decentralized selection training-based Personalized Federated Learning approach. This method ensures that each client maintains a model of the same dimensionality, facilitating efficient aggregation and strategic communication among clients. During each communication round, clients selectively engage with a subset of peers, chosen through a \textit{strategic scoring strategy} for their relevance to the current learning context. They then aggregate their own model with those selected from their peers. After local updates, clients share their newly trained model parameters with the required peers, thereby enhancing the collective learning process and ensuring continuous improvement and relevance of the shared data.
We employ an \textbf{innovative} scoring scheme that evaluates potential peer clients based on three key factors: feature extraction capability, task heterogeneity, and communication frequency. Simulations in heterogeneous settings demonstrate that PFedDST not only increases the average test accuracy on local test data but also reduces the number of communication rounds required to achieve the same performance targets.


%This method utilizes a scoring strategy to identify beneficial communication choices in a decentralized setting, aiming to solve personalized FL issues more effectively. Unlike conventional approaches that deploy a consensus model for each user, PFedDST enables each client to maintain a personalized model header, enhancing their ability to adapt to local tasks. This approach allows the active client first to collect and analyze information from the client pool, including frequency, headers, and losses. It employs a scoring strategy that integrates this information to assign scores to available clients. Once the local client makes a decision based on these scores, it directly gathers all necessary feature extraction information from the chosen peers. The client then proceeds with feature extraction training and push to the out-neighbors. Next, the client local updates its model header based on the newly trained parameters and sends this updated information to the selection pool, ensuring continuous improvement and relevance of the data shared among clients. The overall framework is shown in Figure (\ref{fig:overall}). In heterogeneity settings, the results demonstrate that PFedDSTincreases the averaged test accuracy on local test data, requires fewer communication rounds to reach the same target. 
We summarize our contributions as following: 
\begin{itemize} \item We propose the PFedDST framework, a personalized federated learning approach where each client continuously learns from selected peers to update its feature extraction capabilities while maintaining a personalized prediction header. This integration of peer selection and partial model personalization enhances robust communication and accelerates convergence. \item Strategic selection enables clients to enhance their feature extraction capabilities from the most informative and relevant neighbors. It also prioritizes communication with clients that have not recently interacted, thereby diversifying and refreshing the learning inputs. \item Experimental results demonstrate that PFedDST outperforms various state-of-the-art baselines. It proves particularly effective in environments characterized by data heterogeneity and limited computational resources. \end{itemize}
%\begin{itemize}
%    \item We introduce score selection optimization to DPFL, enabling clients to flexibly choose the communication peers and ensuring a larger feature search space in communication and computational heterogeneity.
%    \item We propose a Personalized Federated Learning framework, PFedDST, which integrates peer selection and partial model personalization freeze training to enhance robust communication and accelerate convergence.
%    \item The newly proposed partial aggregation freeze training technique optimizes information aggregation, saves local computing resources, speeds up the training process for decentralized FL.
 %   \item Experimental results demonstrate that PFedDST outperforms various state-of-the-art baselines and is well-suited to environments with data heterogeneity and limited computational resources.
%     \item We introduce the score selection optimization to DFL,
% which allows clients to choose their neighbors flexibly
% and guarantees a larger feature search space in a communication, and computation heterogeneity scenario.
%     \item We propose a FL framework PFedDST, incorporated
% with neighbor selection and partial model personalization training for robust communication and fast convergence.
%     \item The newly proposed partial-freeze training technique can better
% aggregate the information, save the local computing, speedup the whole training process and achieve better personalization
% for decentralized FL.
%     \item Experimental results indicate the superiority of the proposed
% PFedDST compared with various state-of-the-art baselines and it
% can be well adapted to the data heterogeneous and computation resources constrained settings.
%\end{itemize}

It should be noted that our strategy is different from traditional directed DFL methods such as Dis-PFL and AsyNG, which typically involve exchanging all parameters for a single consensus model or selecting communication targets randomly. Instead, our approach incorporates score-based neighbor selection, partial freeze \cite{brock2017freezeout} training, and alternating optimization to accelerate convergence. This method not only ensures model robustness and enhances personalization but also optimizes communication efficiency.



%\section{Related Work}
%The exploration of distributed learning and detection encompasses a broad spectrum of applications within signal processing and machine learning. The objective in distributed learning is to construct predictive models from historical data \cite{verbraeken2020survey,mcmahan2017communication,lian2017can,assran2019stochastic,liu2019communication,li2020federated}. Conversely, distributed detection and decision-making seek to amalgamate data from diverse sources to facilitate informed decision-making \cite{varshney2012distributed,veeravalli2012distributed,kailkhura2015distributed,verbraeken2020survey}. 


%In the context of deep learning, we consider a dataset \(D\) comprising \(n\) data points \(X = (X_1, ..., X_n)^T\), with each \(X_i\) being a vector in \(\mathbb{R}^d\), and associated outputs \(y = (y_1, ..., y_n)^T\). The task involves solving an optimization problem:

%\begin{equation}
%    \min_{\beta} f(\beta; y, X) = l(\beta; y, X) + r(\beta; \lambda)
%\end{equation}
%
%where \(f\) symbolizes the system model, incorporating a loss function \(l(\beta; y, X)\) and a regularization term \(r(\beta; \lambda)\), with \(\beta\) as the model parameters.


% The persistent and critical challenge in distributed learning is heterogeneity that can significantly degrade both
% model accuracy and convergence performance. Several forms of heterogeneity have been identified in
% the literature: task heterogeneity, network heterogeneity, resource heterogeneity, and data heterogeneity. In this paper, we mainly focus on data heterogeneity, which is arguably the most pervasive and,
% in many cases, the most detrimental to learning performance. It occurs when the data distributions across nodes differ. It includes
% cases where either the marginal distribution or the conditional distribution vary between nodes.
% Such variations complicate model aggregation since local models, trained on data with different
% distributions, optimize for different objectives.

%The persistent and critical challenge in distributed learning is heterogeneity, which can significantly reduce both model accuracy and convergence performance. The literature identifies several forms of heterogeneity, including task, network, resource, and data heterogeneity. This paper primarily focuses on data heterogeneity, arguably the most pervasive and detrimental to learning performance. Data heterogeneity occurs when the data distributions across nodes differ, encompassing cases where either the marginal or conditional distributions vary between nodes. These variations complicate model aggregation, as local models trained on data with different distributions optimize for different objectives. Therefore, Personalized Federated Learning (PFL) has been proposed as a method to address data heterogeneity, aimed at accommodating the distinct data distributions at each node. This approach allows each client to utilize unique model parameters, enabling better prediction outcomes tailored to their specific data distributions. Also in DPFL, clients connect only with their neighbors through peer-to-peer communication, aiming to create the most effective personalized models for each client to solve the heterogeneity problems. 
% Due to the
% computation and communication resources limitation
% among clients, Decentralized Personalized Federated Learning (DPFL) has been an encouraging field in recent
% years, where clients only connect with their
% neighbors through peer-to-peer communication and aims
% to produce the greatest personalized models for each client. FedPer, FedRep, and FedBABU each employ a single global feature representation paired with multiple local classifiers. They differ in how they configure the relationship between the shared representation and the individual linear components. Fed-RoD concurrently trains a comprehensive global model and numerous private classifiers, utilizing both class-balanced and empirical loss functions. Theoretically, FedSim and FedAlt offer convergence analysis for both algorithms under general non-convex conditions. Meanwhile, FedAvg-P and Scaffold-P  build upon and enhance the findings presented in FedAlt. DFedAvgM utilizes multiple local iterations with SGD and quantization techniques to minimize communication costs. Dis-PFL tailors personalized models and pruned masks for each client to accelerate personalized convergence. KD-PDFL employs knowledge distillation to enable devices to recognize statistical differences between local models. ARDM establishes lower bounds for communication and local computation costs in a peer-to-peer personalized federated learning (FL) setting. DfedPGP and AsyNG implement the push-sum method to expedite training convergence.
% Almost all DPFL methodologies are vulnerable to deadlock caused by unstable communication channels and the degraded quality due to disparate levels of convergence during aggregations. To tackle these issues, we propose a partial-freeze framework based on selective communication for DPFL. This strategy deviates from traditional directed DFL methods such as Dis-PFL and AsyNG, which generally involve exchanging all parameters for a single consensus model or selecting communication targets randomly. Our approach incorporates score-based neighbor selection, multi-step local iterations, and alternating optimization to improve convergence. This method ensures unbiased gradient estimation, enhances personalization, and optimizes communication efficiency. Consequently, both the algorithmic design and the theoretical analysis of our framework are innovative and demonstrate excellent performance.
%DPFL methodologies often struggle with slow convergence rates during aggregations and are vulnerable to deadlock due to unstable communication channels. To tackle these issues, we propose a partial freeze training framework based on selective communication for DPFL. This strategy deviates from traditional directed DFL methods such as Dis-PFL and AsyNG, which generally involve exchanging all parameters for a single consensus model or selecting communication targets randomly. Instead, our approach incorporates score-based neighbor selection, partial freeze training, and alternating optimization to accelerate the convergence of the model. This method ensures model robustness, enhances personalization, and optimizes communication efficiency. Consequently, both the algorithmic design and theoretical analysis of the framework are innovative and demonstrate superior performance.

% There are three primary categories we can use to address the challenges posed by heterogeneity in DPFL. The first
% category involves operations on the data. These methods are designed to smooth out the statistical
% heterogeneity of local data across clients. For example, data augmentation or resampling techniques can
% be applied to ensure that the local data distribution better represents the overall population. This helps to
% reduce discrepancies between clients. Next, we have operations on the model. These focus on improving
% model optimization, by either designing new model architectures that are more robust to the heterogeneity
% across clients or by optimizing the learning process to account for these variations. For example, adaptive
% learning rates can help models converge more effectively across diverse client environments. Finally, It is
% the operations on the server. This can include strategies such as selecting which clients will participate
% in each round of training, or grouping clients with similar data distributions into clusters to make the
% aggregation process more effective. By strategically managing which clients contribute to the global model
% and how their contributions are aggregated, we can improve the overall quality and fairness of the model.
% Each of these methods plays an important role in tackling heterogeneity and ensuring that the federated
% learning process remains efficient and effective. Next, we will explore more on each method.

\section{System Model and Methodology}

In centralized model training, consider a classification or multiclass detection task in which each data sample is a pair of \((x,y)\), where $x \in \mathbb{R}^d$ represents the input features, and $y \in \{0, 1, \dots, k-1\}$ signifies the corresponding labels, with $k$ being the number of possible classes. The goal is to classify the variable \(x\) into one of \(k\) categories. This classification is achieved using a model parameterized by $w: \mathbb{R}^d \rightarrow \mathbb{R}^k$. Each component of the output, \(\gamma_y(x)\) for \(y = 0, \dots, k-1\), represents the likelihood (or confidence score) that the instance \(x\) belongs to class \(y\). The primary objective is to minimize the expected loss, defined by the equation:
\begin{equation}\label{eq:classical}
    \mathcal{L}(w) := \mathbb{E}_{(x,y) \sim D}[L(w;x, y)],
\end{equation}
where  \(L(w; x,y)\) measures the loss of the decision margins \(\gamma_y(x; w) \in \mathbb{R}^k\) when the true label of \(x\) is \(y\), and the expectation is taken over the joint distribution of the dataset $D$.


Optimization of this expected loss commonly uses gradient-based algorithms like Stochastic Gradient Descent (SGD) or Adam. These methods iteratively adjust the parameters $w$ to minimize the empirical loss for data $(x,y)$, $w_{\text{new}} = w_{\text{old}} - \eta \nabla_w L(w; x, y)$,
where $\eta$ denotes the learning rate.


\subsection{Decentralized Personalized Federated Learning with Partial Freezing}

In decentralized personalized federated learning, where data distribution varies across clients, each client $i$ has a distinct data distribution $D_i$ and maintains a personalized model parameterized by $w_i$. The index $i$ ranges over a total of $M$ clients, $i \in \{1, \dots, M\}$. In this setting, a objective is to optimize the local models jointly \cite{hanzely2021personalized}:
\begin{equation}\label{eq:obj}
\min_{w_1, \cdots, w_M} \mathcal{L}(w_1, \cdots, w_M) = \frac{1}{M}\sum_{i=1}^M \mathcal{L}_i(w_i),
\end{equation}
where $\mathcal{L}_i(w_i) = \mathbb{E}_{(x,y) \sim D_i} \left[ L(w_i; x, y) \right]$ represents the empirical risk associated with the $i$-th client's local data and $L$ denotes the loss function.

% To improve personalized performance, we consider the selection of partial-freeze in decentralized personalized FL. The model structure can consist of a fully personalized header $h_i$ and an aggregated feature extraction $e_i$, aggregating from the optimized selected neighbors $\mathcal{M}_i$. In detail, we divide the model into two parts: the header and the feature extraction layers. The header refers to the model's final linear layers, which primarily perform the final linear classification of the output. The feature extraction component comprises the remaining layers of the model, which are principally responsible for extracting features from the input data. During local optimization, these two sections are trained separately. Once the training of the feature extraction component is complete, it can then proceed to the next round of communication with out-neighbors. 

To enhance personalized model performance and expedite convergence, we integrate the concept of partially frozen training. \textit{The model is structured into two distinct parts: the header and the feature extraction layers.} The header comprises the modelâ€™s final fully-connected layers, primarily responsible for classification tasks. This component is essential for customizing the model to meet each client's specific requirements, allowing personalized adjustments in the decision-making process. The feature extraction layers consist of the earlier stages of the model, which are tasked with processing and extracting pertinent features from the input data.

During each communication round, each client $i$ strategically selects a subset of peer clients, denoted by $\mathcal{M}_i$, and aggregates (e.g., simple average) its own feature extraction layers with those from its peers to obtain the aggregated feature extraction layer $e_i$. The header layers of client $i$, denoted by $h_i$, remain unchanged and do not participate in the model aggregation. Then, client $i$ undergoes local training to sequentially update $e_i$ (with frozen $h_i$) and $h_i$ (with frozen $e_i$). Specifically, $h_i$ is frozen first and the aggregated $e_i$ is updated using the local data distribution $D_i$.

\begin{equation}\label{eq:obj_e}
\min_{e_i}\ \ \ \  \mathcal{L}_i(e_i) = \mathbb{E}_{(x,y) \sim D_i} \left[ L((e_i, h_i^f); x, y) \right]
\end{equation}
where the subscript $f$ in $h_i^f$ indicates that the parameters are frozen.
Upon updating $e_i$, it is sent to the required peers and the parameters in $h_i$ are unfrozen and updated next:

\begin{equation}\label{eq:obj_h}
\min_{h_i}\ \ \ \  \mathcal{L}_i(h_i) = \mathbb{E}_{(x,y) \sim D_i} \left[ L((e_i^f, h_i); x, y) \right]
\end{equation}

Once the local update is complete, client $i$ shares its updated $h_i$ back to the network. This updated information helps other clients make informed decisions about which peers to communicate with in the next round. The entire workflow of our approach is depicted in Figure 1.



\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{FedDSST.drawio.png} % Replace 'example-image.jpg' with your file's name
\caption{Overview of PFedDST} % This sets the image's title/caption
\label{fig:overall} % This labels the image for cross-referencing
\end{figure}




\subsection{Strategic Peer Selection for Communication}
For each client, we quantify the degree of information contribution from others by assigning a score to each peer. A higher score indicates that a peer holds more valuable information for enhancing the feature extraction capabilities of the local client, thus facilitating more targeted updates. Essentially, our aim is for clients to augment their feature extraction abilities from peers that are better equipped to guide them, particularly those undertaking similar tasks.

The scoring system is based on a composite evaluation of three factors: the loss disparity score $s_l$, the header distance $s_d$, and the peer recency $s_p$. Specifically:
\begin{itemize}
    \item The \textit{loss disparity score} ($s_l$) measures the potential to enhance the generality of a clientâ€™s feature extraction capabilities. This score is calculated by assessing the loss when predictions are made using this clientâ€™s model on a peer's local dataset. A higher loss indicates a significant gap in the clientâ€™s ability to predict the peerâ€™s data, signaling a stronger need for adaptation.
    \item The \textit{header distance score} ($s_d$) identifies peers whose tasks are more closely related to the current client's tasks. It is measured by the weight distance between the header layers of the two clients. A smaller distance indicates that the label distributions, or tasks, of the two clients are more similar, making learning from such peers particularly beneficial.
    \item The \textit{peer recency score} ($s_p$) is designed to enhance learning generalization by avoiding repetitive communication with the same few peers and encouraging engagement with those not recently communicated with. This approach helps prevent overfitting and promotes a more diverse and robust learning process.
\end{itemize}

By employing this holistic scoring mechanism, we strategically select the most beneficial peers for communication, thereby optimizing the efficiency and effectiveness of the distributed learning environment.



\textbf{Loss Disparity Score.} 
The concept of selection skew, denoted by $\rho$, was defined in \cite{cho2020client} within the context of centralized FL. This skew quantifies the disparity in loss outcomes when evaluating the unified model on the data of a strategically selected subset of clients, as opposed to a random selection. The findings in \cite{cho2020client} suggest that a larger lower bound on $\rho$ leads to faster convergence during the training process, indicating the advantage of selecting clients whose data produce larger losses because the current model underperforms on these and requires further training.

Inspired by this, we define a \textit{decentralized version of $\rho$} for a specific local client $i$, representing the model loss difference between selecting a subset of peers $\mathcal{M}_i$ and a full random selection of peers in model aggregation:
\begin{equation}
    \rho_{i} = \frac{\sum_{j \in \mathcal{M}_i} n_j \left(\mathcal{L}_j(w_i) - \mathcal{L}_j(w_j^*)\right)/\sum_{j \in \mathcal{M}_i} n_j}{\mathcal{L}_i(w_i) - \sum_{j \in \mathcal{M}} n_j\mathcal{L}_j(w_j^*)/\sum_{j \in \mathcal{M}} n_j} \geq 0 
\label{eq:rho}
\end{equation}
where $\mathcal{M}$ represents the collection of available clients to the client $i$, $n_j$ is the fraction of data at the j-th client and $w_j^* = \arg \min_{w_j} \mathcal{L}_j$ is the optimized $w_j$ for client $j$. With purely random selection, $\rho = 1$ since the numerator and denominator in (\ref{eq:rho}) are equal.

Inspired by the findings in \cite{cho2020client}, we adopt a client selection strategy that seeks to maximize lower bound of $\rho$, thereby accelerating the convergence rate. However, evaluating $\rho$ for all potential subsets of peers is computationally impractical due to its NP-hard nature. Instead, we utilize the loss of applying the $i$th client's model on the $j$th peer's data, denoted by $l_j(w_{i,j})$, as a surrogate to measure the desirability of selecting peer $j$. Mathematically, the loss score between client $i$ and its peer $j$ is given by:
\begin{equation}
    s_l^{i,j} = \norm{l_j(w_{i,j})} = \norm{\mathbb{E}_{(x,y) \sim D_j} [ L(w_i;x,y) ]}
    \label{eq:sl}
\end{equation}
where $D_j$ is the data distribution at the $j$th peer. A higher $s_l^{i,j}$ indicates that the $i$th client's model struggles with handling the $j$th peer's data, suggesting a greater preference for selecting $j$ in the next communication round.




\textbf{Header Distance Score.} 
% \textit{Second, we also need to consider the influence of the data heterogeneity.} 
% Unlike the traditional centralized FL, which wants to obtain a highly generalized model to adapt the different type of data, the decentralized personalized FL mainly focus on obtain a highly personalized model to adapt to their local data. When we using the unbiased neighbor selection strategy, \textit{we noticed that} there are multiple selected neighbors have very low validation accuracy in the training process, show in Fig \ref{fig:validation}(a). The result show that most of selected neighbors have the low accuracy under the local validation data. Motivated by this result, we prefer selecting the neighbors have similar data distribution. Instead of validating the neighbors' parameters, which is very time-consuming, we want to use the distance of fully personalized headers to represent the data distance. 
Unlike traditional centralized FL, which aims to develop a unified model across diverse data types, decentralized personalized FL focuses on creating models tailored to specific local data. When two clients have similar tasks (e.g., comparable label distributions), they are likely to benefit from communication and model aggregation. Similar tasks imply compatible data or learning objectives, enhancing the learning process through effective information sharing. However, if two clients have significantly different tasks and data distributions, such as one client focusing on images of animals and another on images of plants, their feature extraction layers possess distinct properties. Aggregating their models might not only fail to improve but could potentially deteriorate each other's performance.

Based on this reasoning, it is preferable for clients to select peers with similar tasks. We propose using the element-wise cosine similarity between header layers' weights to measure this similarity. We prefer cosine similarity over Euclidean distance as a metric because it emphasizes the directional trends (patterns) of the weights rather than their absolute magnitudes. This approach is preferred as it focuses on the relative importance of input features, which reflects the true nature of the task. In addition, it is important to note that other distance metrics like Kullback-Leibler (KL) divergence are unsuitable for measuring distances between model parameters, as these weight parameters do not inherently possess probabilistic properties.

Let $H = (h_1, h_2, \dots, h_n)$ and $G = (g_1, g_2, \dots, g_n)$ represent the weight parameters corresponding to the header layers and the header distance score (coefficient) can be computed as follows:
\begin{equation}
  s_d = \frac{\sum_{i=1}^{n}h_i \cdot g_i}{\sqrt{\sum_{i=1}^{n}h_i^2} \cdot \sqrt{\sum_{i=1}^{n}g_i^2}} 
  \label{eq:sd}
\end{equation}
where $h_i$ and $g_i$ are the $i$-th elements of $H$ and $G$, and $n$ is the number of elements in $H$ and $G$.


%Unlike traditional centralized FL, which aims to develop a generalized model adaptable to various types of data, decentralized personalized FL focuses on creating personalized models tailored to local data. Communication between clients with the related task can enhance the efficiency and accuracy of local personalized training. Clients that have similar tasks often have compatible data or learning objectives, which can improve each other's learning processes through effective information sharing. Communication between clients with different tasks might be less beneficial or even detrimental to the training process. Based on this, selecting peers with similar tasks is more efficient. Instead of conducting time-consuming validations of peer parameters, we propose using the euclidean distance between fully personalized headers as a measurement for header distance. This method provides a more efficient way to validate the similarity of tasks between different clients, thereby enhancing the personalization and effectiveness of the model for each specific client.



\begin{figure}[ht]
    \centering
    \subfloat[Random selection]{ \includegraphics[width=0.4\textwidth]{validation_acc_diff_random_selection.png}}

    
    \vspace{0.1cm} 
    
    
   \subfloat[Score-based selection]{   \includegraphics[width=0.4\textwidth]{validation_acc_diff_score_selection.png}}

    
    \caption{The validation result of each selected peer in the local data}
    \label{fig:validation}
\end{figure}



% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.4\textwidth]{validation_acc_diff_random_selection.png} % Replace 'example-image.jpg' with your file's name
% \caption{Random Selection} % This sets the image's title/caption
% \label{fig:random} % This labels the image for cross-referencing
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.4\textwidth]{validation_acc_diff_score_selection.png} % Replace 'example-image.jpg' with your file's name
% \caption{Score Selection} % This sets the image's title/caption
% \label{fig:score} % This labels the image for cross-referencing
% \end{figure}

% To address this need, we leverage the energy distance, a versatile metric that quantifies distributional differences by comparing pairwise distances between neighbors. This measure highlights significant disparities between header distributions and forms the foundation for adaptive strategies to address heterogeneity-induced challenges in distributed systems. After using the score selection, each round selection validation results show in fig \ref{fig:validation}(b). The result shows that each neighbors validation results are improved after using the score selection, which demonstrate that our score selection can select more relative neighbors.


% % {\color{red} which one to use and why?}
% The Energy Coefficient $H$ extends the energy distance\cite{szekely2003statistics, szekely2005new} into a normalized measure of header distance score. We use the local header layers as $X$ and the neighbor layers as $Y$. { \color{red} For the $d$-dimensional, (remove?)} Let $X = (X_1, X_2, \dots, X_d)$ and $Y = (Y_1, Y_2, \dots, Y_d)$ represent the vectors corresponding to each header layer for dimensions $i = 1, \dots, d$. Using these representations, we can calculate the header distance scores as follows:
% \begin{equation}
%  s_d = 1 - \frac{D^2(X, Y)}{2 \mathbb{E} ||X - Y||}=\frac{\mathbb{E} ||X - X'||}{2 \mathbb{E} ||X - Y||}+\frac{\mathbb{E} ||Y - Y'||}{2 \mathbb{E} ||X - Y||}
%  \label{eq:sd}
% \end{equation}
% where $H \in [0, 1]$, with $H = 0$ if and only if $P_X = P_Y$. $X', Y'$ are independent copies of $X, Y$. Mean $\mu_{X_i} = \mathbb{E}[X_i]$ and $\mu_{Y_i} = \mathbb{E}[Y_i]$, variances $\sigma_{X_i}^2 = \text{Var}(X_i)$ and $\sigma_{Y_i}^2 = \text{Var}(Y_i)$, skewnesses $\gamma_{3X_i}$ and $\gamma_{3Y_i}$, and kurtoses $\gamma_{4X_i}$ and $\gamma_{4Y_i}$.
% As shown in our prior work \cite{fan2025measuringheterogeneitymachinelearning}, the expectation of the Euclidean distance between $X$ and $Y$ is approximated by:
% \begin{align}\label{eq:approx_xyd}
%  \mathbb{E} \left[ \norm{X - Y} \right] \approx \sqrt{\nu_{XY}} \left(\frac{3}{4} - \frac{C_{4XY} + 4 C_{3XY}  \delta_{\mu 1} - 2 (\delta_{\mu 2})^2}{8 \nu_{XY}^2}\right),
% \end{align}
% where $\nu_{XY} = \sum_{i=1}^d \left( \sigma_{X_i}^2 + \sigma_{Y_i}^2 + (\mu_{X_i} - \mu_{Y_i})^2 \right)$, 
%  $C_{4XY} = \sum_{i=1}^d \left( \sigma_{X_i}^4 \gamma_{4X_i} + \sigma_{Y_i}^4 \gamma_{4Y_i} \right)$, 
%  $C_{3XY} = \sum_{i=1}^d \left( \sigma_{X_i}^3 \gamma_{3X_i} - \sigma_{Y_i}^3 \gamma_{3Y_i} \right)$, 
%  $\delta_{\mu 1} = \sum_{i=1}^d (\mu_{X_i} - \mu_{Y_i})$, and 
%  $\delta_{\mu 2} = \sum_{i=1}^d (\mu_{X_i} - \mu_{Y_i})^2$. %This approximation method is shown in \cite{fan2025measuringheterogeneitymachinelearning} to be efficient and accurate.
 
% The computational complexity of the empirical formula is $O(n^2d)$, while this approximation reduces it to $O(nd)$, making it well-suited for distributed scenarios. 

Here, we validate the effectiveness of client selection based \textit{solely} on the header distance score. Figure 2 illustrates the model accuracy for a specific local client during decentralized training (for more details on the experimental setup, see the Experiments Section III). Each training round involves the client selecting 10 peers as candidates for communication. We evaluate the performance of the models from 10 selected peers on the local client's data, comparing random and strategic selections. Random selection is depicted in Figure \ref{fig:validation}(a), while strategic selection based on the header distance score is illustrated in Figure \ref{fig:validation}(b). The prediction accuracy of the models from the selected peers (represented by red bars) on the clientâ€™s data is plotted on the y-axis for each training round across both selection approaches.
While the local model performs best on its own data (denoted by green), models from strategically selected peers generally outperform those from peers selected randomly. This comparison demonstrates the utility of the header distance score in identifying the most relevant peers for communication.



%During the training process, we use local data to validate the effectiveness of each selected peer, represented by their accuracy. Figure \ref{fig:validation}(a) shows that neighbors selected through random selection generally have lower accuracy during the training process, which significantly reduces training efficiency. In contrast, after implementing our score-based selection method, each peer makes a useful contribution to the prediction of local data, demonstrating the utility of our approach in improving the training process, as shown in Figure %\ref{fig:validation}(b).

% {\color{red}give high level explaination of the meaning of this score}
\textbf{Peer Recency Score.} A critical factor in decentralized FL is the communication frequency. The peer recency score helps determine a local client's priority for selecting a peer based on how recently local client has communicated. This score is designed to prevent the local client from ``forgetting" the knowledge it might have acquired from peers that have not been engaged for several rounds. A higher peer recency score, indicating a longer interval since last communication, increases a peer's probability of being selected. This mechanism aims to enhance both the model's convergence rate and its generalization, thereby improving overall training effectiveness.

In a fully decentralized network, global training information and iteration counts are not accessible for each peer. Therefore, we calculate the peer recency score using only local iteration data. For a local client, let $n_{0,j}$ be the iteration number at which peer $j$ was last selected, and $n_t$ denote the current iteration number. The peer recency score of peer $j$, $s_{t,j}$ is designed to range from 0 to 1, where it approaches 0 if \( n_t - n_{0,j} \) is small, discouraging the repetitive selection of the same peer and promoting diversity in peer engagement. Conversely, as \( n_t - n_{0,j} \) exceeds a certain threshold \( c_0 \), $s_{t,j}$ increases to its maximum value of 1. To achieve this property, we use the cumulative distribution function (CDF) of the exponential distribution:

\begin{equation}
\label{eq:st}
    s_p = \phi(1 - e^{-\lambda (n_t - n_{0,j})})
\end{equation}
where $\phi$ represents the CDF and $\lambda$ is the rate (scaling) parameter of the exponential distribution.

%Since we focus on the fully decentralized network, the global training information and iteration is unavailable for each clients. Therefore, we propose this feasible solution to calculate the frequency score, which can be easily collected through the local client and can directly measure the peers priority. By adjusting the rate parameter $\lambda$ to be larger, we can make the $s_p \approx 1$ when the $n_i-n_0>c_0$, where $c_0$ is a threshold. %After $c_0$ times the neighbors are not be selected, we want the frequency score will not affect its selection priority.


\textbf{Holistic Determination of the Final Score.} For a specific client, the cumulative score for selecting a peer $j$ indicating its preference for selecting it incorporates three factors: the loss score $s_l$ from (\ref{eq:sl}), header distance score $s_d$ from (\ref{eq:sd}), and peer recency score $s_p$ from (\ref{eq:st}). For a specific local client, a peer's overall communication score is intelligently designed as follows,
\begin{equation}
\label{eq:score}
    \mathcal{S} = s_p(\alpha s_l - s_d + c)
\end{equation}
where $\alpha$ is a scaling parameter, and $c$ is a constant that represents the communication cost score between the corresponding peer.

This overall score increases under the following conditions: a. when $s_l$ increases, indicating a larger loss on the peerâ€™s data and a greater need for the client to learn from it; b. when $s_d$ decreases, reflecting a higher task similarity with the peer; and c.  when $s_p$ increases, suggesting that the client has not communicated with this peer recently. In addition, the peer recency score $s_p$, ranging from 0 to 1, converges quickly to 1 as $|n_t - n_{0,j}|$ grows large. The multiplication of $s_p$ and $\alpha s_l - s_d + c$ ensures that $s_p$ does not dominate the selection process. This design prevents the selection of peers that are significantly different from the local client solely based on infrequent prior communication, therefore we enhance the stability of personalized training.


%We can know that \( s_p \) is a number between 0 and 1. When \( s_p \) approaches 0, which occurs if $n_i-n_0$ is small, \( s_p \) significantly influences the final score. For example, if a peer node was recently selected in the previous round, it will have a very small \( s_p \), thereby reducing its overall score substantially. This dynamic discourages the repeated selection of the same node, promoting diversity in node participation.

%Conversely, when \( n_i - n_0 \) exceeds \( c_0 \), the terms \( s_d \) and \( s_l \) become the dominant factors in the score, facilitating faster convergence. For instance, if a peer has not been selected for several iterations, \( s_p \) will approach 1, making the score dominant by \( s_d + \alpha s_l \). This approach rewards nodes that have been less active recently, providing them a higher score and thus a greater possibility of being selected in the current round.

%This mechanism allows us to find a trade-off between the speed of convergence and the error rate, thereby balancing these two critical aspects effectively. By adjusting the influence of \( s_p \), \( s_d \), and \( s_l \) based on the conditions of selection frequency and data distribution, we can optimize the overall performance of the distributed learning network. This strategic balancing act ensures that each relative node has an equitable chance of contributing to the modelâ€™s learning process, enhancing the robustness and personalization of the model.



\subsection{Algorithm}

In this section, we propose the PFedDST algorithm, which facilitates peer selection under a fully decentralized setting. The algorithm is designed to operate on each client, allowing for local decision-making without centralized oversight.

Each client maintains two context information arrays to support decision-making processes, the loss array \(l\) and the peer recency array \(t\).  The loss array \(l\) stores the loss information calculated from aggregated parameters with each peer, and the peer recency array \(t\) records the iteration numbers that each peer was last selected by the local client. By leveraging data from loss, header distance, and selection frequency, each eligible client is assigned a score that reflects its priority as a potential peer. This score is then used to determine the selected communication peers $\mathcal{M}_i$. The selection process ensures that communication efforts are focused on the most relevant and beneficial peers, optimizing model performance and enhancing training efficiency.



%Inspired by multi-task learning, we have adopted a partial-freeze training method in our subsequent model training sessions. This approach acknowledges that a model's core components, combined with different headers, can predict different tasks. We consider heterogeneity as a multi-tasking environment where different data distributions represent different tasks. Thus, we propose the partial-freeze method to accelerate and enhance training effectiveness. %Particularly, we divide the model parameters into two parts: feature extraction and the header. The feature extraction part is more generalized, encompassing the model's convolutional layers and the linear layers that do not make the final output decisions. These parameters are aggregated with those from other neighbors. The header represents each local client model's last multiple linear layers, which is entirely personalized. It does not participate in any form of aggregation and aligns with local data characteristics.%

After completing peer selection, the feature extraction layers are aggregated from each selected peer. The training then proceeds in two phases. In the first phase, the header layers are frozen, and the feature extraction layers are trained. This targeted training helps to enhance the model's ability to accurately interpret and process input data. Once this training phase is complete, the trained parameters can immediately be dispatched to peers that have already made requests. In the second phase, the feature extraction layers are frozen, and the training focuses on the header layers. This phase is dedicated to fine-tuning the header, which is responsible for making the final decisions and classifications based on the processed features. The overall training approach allows each component of the model to be optimized for its specific role, enhancing the overall performance and efficiency of the distributed learning system. The details of our proposed framework are shown in Algorithm \ref{alg:1}.

\begin{algorithm}[htb]
\caption{Data Fusion Algorithm}
\label{alg:1}
\SetAlgoLined
\KwIn{Total number of clients $M$; Local input data $D$; Total communication rounds $T$; Number of local training iterations $K_e$ and $K_h$; Communication cost $C$}
\KwOut{Personalized local trained models $e_i$ and $h_i$}

Initialize each local client's header parameters $h_{i,0}$, feature extraction parameters $e_{i,0}$, peer recency array $t$, loss array \(l\), communication cost score $c$ based on $C$ and peers information collector $n_{i}$



\For{$ t = 1$ \KwTo $T$}{
    \For{client $i$ in parallel \KwTo $M$}{

        Calculate the $\mathcal{S}_{i,j} = s_p(\alpha s_l - s_d + c)$ by Eq. (\ref{eq:sl}), (\ref{eq:sd}), and (\ref{eq:st})

        Construct the selected peers set $\mathcal{M}_i \in \{\mathcal{S}_{i,j} > s^*\}$

        Receive selected peer's parameters and get the aggregated feature extraction  parameters $e_i = \sum_{j \in \mathcal{M}_i}e_j$

        Update the loss array $l$
        
        \For{k = 1 \KwTo $K_e$}{
            Sample a batch of data $(x,y)$ from local dataset.

            Update feature extraction parameters $e_i$:
            $e_{i}^{t,k+1} = e_i^{t,k} - \eta_e \nabla_e  L(({h_i^t}^{f},e_i^t);x,y)$
        }\textbf{end for}

        Broadcast the updated $e_i$

        \For{k = 1 \KwTo $K_h$}{
            Sample a batch of data $(x,y)$ from local dataset.

            Update header parameters $e_i$:
            $h_{i}^{t,k+1} = h_{i}^{t,k} - \eta_h \nabla_h  L((h_i^t,{e_{i}^t}^{f});x,y)$
        }\textbf{end for}

        Update the peer recency array $t$
        
    }\textbf{end for}
}\textbf{end for}
\end{algorithm}

\section{Experiments}
In this section, we conduct extensive experiments to verify the effectiveness of the proposed PFedDST algorithm in scenarios characterized by data heterogeneity and computation resources heterogeneity. These experiments are designed to evaluate how well PFedDST handles diverse datasets and varying computational resources across different nodes in a distributed system.

\subsection{Experimental Setup}
To evaluate the performance of the proposed algorithm, we use the CIFAR-10 and CIFAR-100 datasets, which are real-life image classification datasets containing images distributed across 10 and 100 classes, respectively. These datasets are commonly used in machine learning research to benchmark image classification algorithms and are particularly useful for assessing performance in heterogeneous data distribution scenarios. The data for each dataset is partitioned in a Pathological distribution manner, intended to simulate a realistic scenario where each client may have access to only a limited subset of the total classes. Specifically, for CIFAR-10, we sample 2 classes from the total of 10 for each client. Similarly, for CIFAR-100, each client is assigned 5 classes from the total of 100. This partitioning method ensures that each client's training and testing data are distributed according to the same class subset, which introduces challenges typical of federated learning environments where data may not be identically and independently distributed across clients.



To ensure a fair comparison across all methods, we maintain consistent experimental conditions for each baseline. The experiments are conducted over 500 communication rounds involving 100 clients. Each client in the federated learning setup communicates with 10 neighbors, and similarly, in the PFedDST method, 10 clients are also chosen at each communication round. The client sampling ratio is set at 0.1. The training involves using a batch size of 128. For the PFedDST method, the feature extraction part is trained for 5 epochs per round, matching the training duration of other baselines. The header part is only trained for 1 epoch per round to reduce computational overhead. All methods employ Stochastic Gradient Descent (SGD) as the optimizer, with a learning rate 0.1. Additionally, all methods implement a decay rate of 0.005 and a local momentum of 0.9 to optimize the convergence and stability of training. The communication cost is equal between each client.



\subsection{Experimental Evaluation}

We assess our proposed methods against current state-of-the-art baselines in PFL. The evaluation includes centralized federated learning methods such as FedAvg \cite{mcmahan2017communication}, FedPer\cite{arivazhagan2019federated} and FedBABU \cite{oh2021fedbabu}, and decentralized federated learning methods such as DFedAvgM \cite{sun2022decentralized}, Dis-PFL\cite{dai2022dispfl}, and DFedPGP \cite{liu2024decentralized} and reproduced the result of \cite{liu2024decentralized}. Each method is tested using a ResNet-18 architecture. In our setup for partial PFL methods, the header layers are personalized for complex pattern recognition, while the remaining layers are shared for feature extraction. Our primary evaluation metric is personalized test accuracy, which aligns with our goal of addressing the challenges in PFL.

As presented in Figure \ref{fig:cifar10} and \ref{fig:cifar100}, the proposed Personalized Federated Learning Decentralized Selection Training (PFedDST) shows \textit{superior stability and performance} over baseline methods across diverse datasets and scenarios of data heterogeneity. Specifically, on the CIFAR-10 dataset, PFedDST achieves a remarkable accuracy of \textbf{92.25\%}, outperforming the nearest baseline method, by \textbf{1.0\%}. On the CIFAR-100 dataset, DFedPGP leads with an accuracy of \textbf{79.41\%}, which is at least \textbf{0.7\%} higher than other baseline methods. The implementation of a communication protocol based on a directed graph allows clients to flexibly select their peers, thus facilitating the choice of pertinent information for their local training processes.

In Table 2, we present the learning curves illustrating the convergence speeds of the methods compared. PFedDST has the \textit{fastest convergence} among the methods tested, which benefited from the peers selection algorithm. Notably, DFedPGP demonstrates that a convergence rate is much better than other methods in both CIFAR-10 and CIFAR-100 scenarios. 

Compared to other methods, PFedDST further optimizes the clients' communication and aggregation. It enhances convergence speed and generalization capability by selecting peers based on their relevance scores, ensuring a more balanced choice of beneficial peers. Additionally, the use of a partially frozen training approach speeds up the training process and enhances transfer efficiency, which minimize the cost consumption while maximizing the information gain. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Cifar10_val.png}
    \caption{Test accuracy on CIFAR-10 }
    \label{fig:cifar10}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Cifar100_val.png}
    \caption{Test accuracy on CIFAR-100 }
    \label{fig:cifar100}
\end{figure}

\begin{table}[ht]
\centering
\caption{The required communication rounds when achieving the
target accuracy (\%).}
\label{exp_mnist_regular_cnn2}
\begin{tabular}{>{\centering\arraybackslash}m{2.3cm} 
                |>{\centering\arraybackslash}m{2.3cm} 
                |>{\centering\arraybackslash}m{2.3cm}}
\toprule
\textbf{Method} & \textbf{CIFAR-10 (target acc is 90)} & \textbf{CIFAR-100  (target acc is 75)} \\ 
\midrule
FedAvg\cite{mcmahan2017communication}          & - & -  \\
FedPer\cite{arivazhagan2019federated}          & 350 & 254  \\
%FedRep\cite{collins2021exploiting}          & 323 & 225 \\
FedBABU\cite{oh2021fedbabu}         & 321 & 306  \\
%Ditto\cite{li2021ditto}           & - & - \\
DFedAvgM\cite{sun2022decentralized}        & 462 & 399  \\
%OSGP\cite{assran2019stochastic}            & 442 & 342 \\
Dis-PFL\cite{dai2022dispfl}         & - & -  \\
DFedPGP\cite{liu2024decentralized}         & 238 & 178 \\
\textbf{PFedDST} & \textbf{184}  & \textbf{133} \\
\bottomrule
\end{tabular}
\end{table}
\section{Discussion}
 Our framework is designed to prioritize the selection of the most beneficial communication peers and utilize partial personalization, ensuring optimal performance and efficiency in distributed learning scenarios. The simulation demonstrates that our method outperforms the current state-of-the-art methods in terms of accuracy and convergence rate. This improvement is more distinct as the model complexity grows, data heterogeneity intensifies, and the number of clients increases.
 
 This enhancement is attributed to the combined training of a fully personalized header and a shared feature extraction layer, supplemented by an effective benefit selection strategy. Initially, we implement a partially frozen training method. During local optimization, the header is frozen while the feature extraction layers are actively trained. Upon completion, the trained component is shared with the required peers, and the previously frozen sections are then unfrozen for further training. This method diverges from traditional training approaches by reducing the number of model parameters trained and communicated, enabling faster training completion. Additionally, it promotes stable parameter optimization and minimizes gradient conflicts. Secondly, we employ a score selection strategy, evaluating potential communication partners across various dimensions, including loss, selection frequency, communication costs, and task similarity. This comprehensive scoring method facilitates the identification of the most suitable partners for exchange, consequently improving the overall training outcomes by increasing accuracy and speeding up convergence. A notable feature of PFedDST is robustness. This selection mechanism automatically filters out potential attackers and clients with noisy data by measuring header distances, improving the robustness of the local model aggregation.
\section{Conclusion}

In conclusion, we propose a unified decentralized federated learning selection framework PFedDST for personalization, fast convergence, privacy, robustness, and communication efficiency within distributed learning environments. By employing score selection score based on loss, peer recency, and task similarity on decentralized devices, we offer the PFedDST that enhances the ability to communicate with beneficial peer models while ensuring a fast convergence rate and privacy. Theoretical findings and experimental results show that our method achieved a faster convergence rate and higher model accuracy compared to other state-of-the-art methods.

% \begin{table}[ht]
% \caption{Comparisons between training methods on CNN model for MNIST dataset with non-IID data distribution.}
% \begin{tabular}{p{2.3cm}p{2.3cm}p{2.3cm}}
% \hline
% Method   & CIFAR-10 &  CIFAR100 \\ 
% \hline
%     Local & 85.16 $\pm$ .18 & 71.34 $\pm$ .46  \\
%     FedAvg & 85.04 $\pm$ .11 & 69.05 $\pm$ .43  \\
%     FedPer & 90.94 $\pm$ .24 & 78.48 $\pm$ .93  \\
%     FedRep & 91.09 $\pm$ .12 & 78.77 $\pm$ .19 \\
%     FedBABU & 91.28 $\pm$ .15 & 77.50 $\pm$ .33  \\
%     Ditto & 84.96 $\pm$ .40 & 69.48 $\pm$ .45 \\
%     DFedAvgM & 90.23 $\pm$ .97 & 75.89 $\pm$ .65  \\
%     OSGP & 90.72 $\pm$ .08 & 76.70 $\pm$ .59 \\
%     Dis-PFL & 88.19 $\pm$ .47 & 71.79 $\pm$ .42  \\
%     DFedPGP & 91.26 $\pm$ .05 & 78.78 $\pm$ .41 \\
%     \textbf{PFedDSST} & \textbf{92.25 $\pm$ .74}  & \textbf{79.41 $\pm$ .57} \\
%     \hline
% \end{tabular}
% \label{exp_mnist_regular_cnn}
% \end{table}




\bibliographystyle{IEEEtran}
\bibliography{refer.bib}


\end{document}
