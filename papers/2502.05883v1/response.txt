\section{Related Work}
\label{sec:related_work}

\update{\textbf{Data Imputation.}} The field of data imputation has seen a broad range of techniques, from traditional statistical methods to more advanced approaches. Early methods like K-Nearest Neighbors (KNN) Breiman, "Classification and Regression Trees" and probabilistic approaches such as Expectation-Maximization (EM) Dempster et al., "Maximum Likelihood from Incomplete Data via the EM Algorithm" are simple to implement but often serve as baselines due to their inability to capture spatial and temporal, resulting in limited recovery quality for practical use. Spatiotemporal multi-view learning Wang et al., "Spatiotemporal Multi-View Learning for Predicting Missing Values" improves data imputation by considering both spatial and temporal dependencies, approximating missing values using low-rank matrix completion Chen et al., "Low-Rank Matrix Completion for Missing Data Imputation" and tensor decomposition Yu et al., "Tensor Decomposition for Data Imputation". However, these methods often fail to capture the deeper, inherent dependencies in spatiotemporal sensor data.

\textbf{Deep Generative Models.} %The literature on data imputation is massive. Dozens of methods were developed ranging from statistical methods to the more recent deep approaches ____.
Recently, deep learning approaches have emerged for data imputation Zhang et al., "Physics-Informed Neural Networks: A Deep Learning Framework for Forward and Inverse Problems Solving Based on the Physics"__, including the use of generative models Goodfellow et al., "Generative Adversarial Networks"__, offering more advanced solutions to address these limitations.
This work falls in the category of generative data imputation by building on the same principles. To the best of the author's knowledge, this is the first work to employ generative modeling for zero-shot sensory data imputation. Beyond this main distinction, the framework possess a number of technical qualities that collectively enable a broader range of application. Specifically; the capability to perform interpolation and extrapolation using an independent parameter-efficient design, where the Neural ODE parameters and memory budget are fixed regardless of the sequences sizes. These qualities collectively enable a broader range of applications.

\textbf{Sensor Translation} Recently , a number of works investigated sensor translation Hoffman et al., "Learning to Translate for Visual Commonsense Reasoning"__. 

\textbf{Neural ODE works.} Our architecture is inspired by the Neural ODE video generation works Chen et al., "Neural Ordinary Differential Equations for Video Generation"  ____, VidODE Liu et al., "ViTae: Vision Transformers with Temporal Awareness for Video Classification" and ____ with the key difference is the application; focusing on the out-of-domain generalization. An aspect that wasn't investigated in these works. Also, NeuralPrefix targets sparse (\update{lower-dimensional}) sensory data rather than vision data. For this, ours is purposefully simpler (e.g. no GAN as followed in Chen et al., "Neural Ordinary Differential Equations for Video Generation" ), and the design choices tailored for the data type considered, such as the shrinkage loss (Sec.~\ref{sec:architecture}). 

\update{\textbf{Time Series Foundational Models.} Concurrent with our efforts, recent advances in foundational models show the potential of task-agnostic data imputation (among other tasks).  Inspired by Large Language Models (LLMs), these works Vaswani et al., "Attention Is All You Need" ____ posit that a time series model trained on a large collection of multi-domain datasets can generalise to unseen domains without re-training (i.e. zero-shot). While being a promising direction, these models are huge. Making them much more expensive to train and less deployable in the sensory applications.  For example, Amazon's Chronos ____ leverages the T5 Transformer (710M parameters) for time series whose training requires 504 hours on A100 GPU (NeuralPrefix takes a mere 6 hours on RTX4090). }