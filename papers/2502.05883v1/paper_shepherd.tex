%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf,authordraft]{acmart}
\documentclass[conference]{IEEEtran}

\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}

\usepackage{color, colortbl}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\usepackage{stmaryrd}


% \usepackage{colortbl}
% \usepackage{xcolor}
% \usepackage{booktabs}
% \usepackage{pgf}

\usepackage{wrapfig}
\usepackage[normalem]{ulem}
\usepackage{multicol}


\def\AK#1{{\bf [Abdo:} {\it\color{teal} {#1}}{\bf ]}}

\def\update#1{\textcolor{teal} {#1}}

\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks = false,
    linkbordercolor = red,
    citebordercolor = green,
    urlbordercolor = blue,
    pdfborder = {0.5 0.5 1.5}
}

\begin{document}
\thispagestyle{empty} 

\input{shp_preamble}
\pagestyle{plain}  
\setcounter{page}{0}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.


\title{NeuralPrefix: A Zero-shot Sensory Data Imputation Plugin}

\author{\IEEEauthorblockN{ Abdelwahed Khamis}
\IEEEauthorblockA{\textit{Data61}\\
\textit{CSIRO}\\
Brisbane, Australia \\
abdelwahed.khamis@data61.csiro.au}
\and
\IEEEauthorblockN{Sara Khalifa}
\IEEEauthorblockA{\textit{School of Information Systems} \\
\textit{Queensland University of Technology}\\
Brisbane, Australia \\
sara.khalifa@qut.edu.au}
}

\maketitle

\begin{abstract}
Real-world sensing challenges such as sensor failures, communication issues, and power constraints lead to data intermittency. An issue that is known to undermine the traditional classification task that assumes continuous data stream. Previous works addressed this issue by designing bespoke solutions (i.e. task-specific and/or modality-specific imputation). These approaches, while effective for their intended purposes, had limitations in their applicability across different tasks and sensor modalities. This raises an important question: \textit{Can we build a task-agnostic imputation pipeline that is transferable to new sensors without requiring additional training?} In this work, we formalize the concept of zero-shot imputation and propose a novel approach that enables the adaptation of pre-trained models to handle data intermittency. This framework, named NeuralPrefix, is a generative neural component that precedes a task model during inference filling in gaps caused by data intermittency. NeuralPrefix is built as a continuous dynamical system, where its internal state can be estimated at any point in time by solving an Ordinary Differential Equation (ODE). This approach allows for a more versatile and adaptable imputation method, overcoming limitations of task-specific and modality-specific solutions. We conduct a comprehensive evaluation of NeuralPrefix on multiple sensory datasets, demonstrating its effectiveness across various domains. When tested on intermittent data with a high 50\% missing data rate, NeuralPreifx accurately recovers all the missing samples, achieving SSIM score between 0.93-0.96.  Zero-shot evaluations show that NeuralPrefix generalizes well to unseen datasets, even when the measurements come from a different modality. 

% Presenting the first attempt of bringing Neural ODE to the sensing domain.

% by reconstructing missing samples. 
% The intermittency-free output is then forwarded to the unmodified host model that continues to function as norm.
% Ultimately, the host model acquires intermittence handling capability without re-training.

% our insights on the sparsity of the sensory data.  
% Interestingly, the design can also be considered for extrapolative intermittency. Enabling the host model to act as \textit{early} action \textit{predictor} rather than a classifier.

% We build NeuralPrefix on the recent advances of generative modeling while integrating design considertion to suit the sparse nature of sensory data.



\end{abstract}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{IEEEkeywords}
 Data Imputation, Modality Compensation, Neural ODE
\end{IEEEkeywords}
    
% \begin{teaserfigure}
% \centering
%   \includegraphics[width=0.85\textwidth]{images/NeuralPrefix_motivation.pdf}
%   \caption{
% % Depiction of NeuralPrefix concept in imagined hazard perception scenario. 
% (Left) A hazard perception suit encompasses several wearable sensors, such as on-chip radars, to sense the ambiance and inform/warn the wearer of important events, including quickly falling objects or approaching vehicles. Intermittency (e.g. transient sensor failure) can result in missing those events and hence dangerous consequences.  (Right) NeuralPrefix can preface existing model to reconstruct the complete signal \underline{in a zero-shot manner} (i.e. generalizing to unseen modalities).  It doesnâ€™t require modifying either the sensor's operational cycle (on/off schedule) or the underlying ML model.}
%   \Description{NeuralPrefix depiction}
%   \label{fig:signature}
% \end{teaserfigure}


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

% \textit{[intermittency in sensing applications, focus you definition of intermittency on missing samples]} typical applications in which data intermittency are present or could be an issue are . Critical application include automotive sensing. For example, a transient sensory failure in on-road object detector can lead to serious consequences.

% \textit{[talk about figure here ]}


Data intermittency is one of the most important problems encountered in the sensing and pervasive computing domains. Unavoidable issues such as transient failures, communication issues, and power constraints lead to intermittent observations that are known to negatively impact the data analysis pipelines and machine learning models acting on the observations. Such a critical problem has been addressed by many research works \cite{adhikari2022comprehensive}.  Most notable among the proposed techniques are the learned generative models \cite{gao2022generative} that made a remarkable progress in spatial-temporal data imputation. These models \cite{yuan2022stgan} are trained to fill the missing data points by conditioning on observed values.
% and the underlying spatiotemporal dependence of them.


A common assumption in learned data imputation is that the test domain is homogeneous to the training domain. Thus, the training and testing setups are required to represent the same task (e.g. human walking action) and/or the same sensory modality (e.g. Radio Frequency Sensing). Overcoming this limitation would enable exciting opportunities in the sensing domain.  For example, a generic data intermittence handling mechanism can be developed and ``pooled'' among numerous sensors (e.g. sensor 1 and 2 in Fig.~\ref{fig:signature}), allowing new sensors to use it without cumbersome re-training efforts.


\begin{figure}
    \centering
    \label{fig:signature}
    \includegraphics[width=0.95\linewidth]{images/neural_prefix_signature.pdf}
    \caption{\textbf{Zero-shot Imputation (ZSI)}. We frame the new problem of zero-shot imputation and highlight potential applications (Fig. \ref{fig:application}). Given \update{sensory data} in a specific domain (A), ZSI seeks to learn an imputation model that can be transferred to unseen domains (B) without re-training. The table above contrasts the idea to vanilla imputation. \textit{Vanilla imputation} handles intermittency in \update{the} same domain (e.g. same sensor) . 
    % \textit{Cross-Modality Translation} (e.g., WiFi2Radar \cite{nirmal2024wifi2radar}) learns the correlation with a co-located supporting (green) modality during training. 
    % During inference, the supporting modality is used as a proxy for constructing the data of the main modality.
    \textit{Zero-shot imputation}, on the other hand, generalizes imputation capabilities to domains (modalities) unseen during training.  
    }
    \label{fig:signature}
\end{figure}

\begin{figure*}[t!]
\centering
  \includegraphics[width=0.9\textwidth]{images/NeuralPrefix_motivation.pdf}
  \caption{\textbf{Example application of Zero-shot Impuation}. (Left) A hazard perception suit encompasses several wearable sensors, such as on-chip radars, to sense the ambiance and inform/warn the wearer of important events, including quickly falling objects or approaching vehicles. Intermittency (e.g. transient sensor failure) can result in missing those events and hence dangerous consequences.  (Right) NeuralPrefix can preface existing model to reconstruct the complete signal \underline{in a zero-shot manner} (i.e. generalizing to unseen modalities).  It doesnâ€™t require modifying either the sensor's operational cycle (on/off schedule) or the underlying ML model.}
  \label{fig:application}
\end{figure*}


\textbf{Zero-shot imputation (ZSI) (Figure. \ref{fig:signature}).} Inspired by the concept of zero-shot learning (ZSL) \cite {pourpanah2022review}, this paper introduces zero-shot sensory data imputation as a key enabler for handling missing data across diverse sensors without requiring re-training. ZSL is a paradigm where a model can make predictions on categories or tasks it has never seen during training by leveraging prior knowledge that relates to the unseen data. Given this, a question that emerges is \textit{Can we develop a learned data imputer that generalizes to unseen sensory modalities without re-training?} This paper answers this question affirmatively. 
% Projecting the problem of sensory data imputation int the zero-shot learning setting, an important
% \noindent\textit{[previous approaches and their limitations]} Previous works \cite{} have shown that partially observable sensory samples can be used to infer the target class.  \textit{[this has been used in tasks such as ]}. Common among all these models is that the algorithms are task specific  \text{[task specific]}. One has to design new algorithms. \textit{[other limitations regarding task flexibility, adapting multiple models ]}. 

To achieve this, we can build on the observation that intermediate missed frames can be thought of a product of the interaction between appearance (content) and dynamics (motion) features. While appearance features vary across sensor modalities, underlying dynamics (human motion) tend to be similar across dataset and can therefore be leveraged as the zero-shot prior. To illustrate, consider the example sensory data in Figure.~\ref{fig:common_dynamics} from RF and pressure mat sensors: the top RF frames show a hand gesture, while the bottom frames depict walking on a pressure mat. Despite the disparity in actions and modalities, common dynamics can be observed in the data space. Specifically, the brightest blob in each frame follows a semi-linear trajectory (motion) while being continually deformed (content) over time.
% Our intuition is that that underlying natural phenomenon captured by the sensors are change smoothly (rather than abruptly) on the spatial dimension and according to some sort of physical  dynamics ( not random).
% \noindent\textit{[our proposal intuitively]} In this paper, we focus on the data aspect (rather than the methods). We claim that partial observations from sensory stream provide enough hints for recovering the missing data samples.
This example reflects a broader trend in real datasets, where sensors capture natural phenomena that tend to change smoothly over space and time, following certain continuous physical dynamics.  By learning these dynamics, we can impute the missing signal at any time step. Therefore, we propose leveraging these smooth and predictable changes to design an effective \textit{generative model that weighs more on the dynamics side (rather than content)}. This approach enables sensory data imputation pipeline that can be \textit{transferred to novel sensory modalities in a zero-shot manner (without the need for re-training)}.

To build on this, we introduce a novel independent component, NeuralPrefix, which can preface the task model and recover the missing \update{spatiotemporal} data samples before they are consumed by the task model (see Fig. \ref{fig:signature}).  Prior to deployment, the prefix model can be calibrated to learn the continuous dynamics of the target data. The calibration is done through a \textit{commensal training}
% \footnote{In biological interactions, a commensal organism (i.e. a prefix model) may obtain benefits (i.e. data) from host species (i.e. task model) without substantially affecting the latter}
\footnote{In biological interactions, a commensal organism benefits from attachment to host species without affecting the latter. Analogously, the prefix model nourishes on the unlabeled task training data (or a similar dataset) without affecting the task model. }
process. That is; training on an unlabeled version of the task dataset with the objective of sample reconstruction.

% One major difference here is that the prefix training is done completely in unsupervised manner (i.e. without the task labels)

% \noindent\textit{[Challenges, and tackling them]} NeuralPrefix is inspired by the recent advances of generative modeling in computer vision \cite{}. Yet applying these methods in intermittent sensing directly is ineffective. First, traditional generative models are trained on uniformly sampled data. Intermittency isn't captured in the latent space.

% Second, the richness of the imagery signal makes the reconstruction task easier and  the focus of the reconstruction is on the perceptual quality of the output. Thus, small glitches in generated visual samples are typically forgiven. Contrary to images, the sensory streams can be very sparse. Minor distortions in critical spatial regions can be detrimental to the accuracy (of the task model). 

% To address the challenges of non-uniform data sampling and the sparisty of sensory datafirst challenge,




% \begin{figure}[t!]
%     \centering
%     \includegraphics[width = 1\linewidth]{images/RF_vs_Carpet.pdf}
%     \caption{\textbf{Common Dynamics.} Common (apparent) dynamics exist in (unpaired) datasets in the data space despite the modality disparity. The heatmap shows two representations of human walking by radar (top) and a pressure matt (bottom). Despite the modalities disparities, one can notice semi-linear progression of the hight energy blobs (highlighted) in each across the frames from left to right. }
%     \label{fig:common_dynamics}
% \end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width = 1\linewidth]{images/RF_vs_Carpet_2.pdf}
    \caption{\textbf{Common dynamics exist in different datasets in the data space despite the modality disparity.}}
    \label{fig:common_dynamics}
\end{figure}

In this direction, we suggest integrating Neural Ordinary Differential Equations (Neural ODE) \cite{chen2018neural} into the generative component. This concept is fundamentally different from traditional networks (e.g. RNNs) whose internal states are defined at discrete time points. In contrast, Neural ODE models the hidden state as a continuous trajectory represented by ordinary differential equations. 
% Enabling modeling time-dependent processes without the constraints imposed by fixed time step models
By using an ODE solver, the hidden state can be integrated from one observation to the next, regardless of the spacing between time points, making it practically suitable for intermittent observations. The solution is further improved by addressing the sparsity of sensory data frames. The shrinkage loss \cite{lu2020deep} is leveraged to focus the reconstruction budget only on the important and hard-to-construct parts (foreground pixels) while tolerating the background and noise.

% An important consideration in sensory data , we move from the observation that which background and noisy pixels, unlike the foreground, donâ€™t have consistent dynamics. Thus a greedy reconstruction of  every pixel is neither attainable nor useful. We focus the reconstruction budget only on the important and hard-to-construct parts.


% \noindent\textit{[contributions:]} In this work, we make the following contributions:
% \begin{itemize}
%     \item We present NeuralPrefix; a neural plugin that predicts sensory samples at any given timestamps and demonstrate its utility in intermittent sensing applications. To the best of our knowledge, this is the first task-agnostic and model-agnostic data intermittency handling solution. 
%     \item We conceive a novel and simple architectural  paradigm (\textit{prefix model}) that decouples the data intermittency handling mechanism from the target model. Thus, upgrading traditional models with intermittency robustness without re-training or fine-tuning. 
%     \item \textit{[could be dropped]} As the core of our work is NeuralODE, we demonstrate the potential of continuous-time neural networks in RF sensing applications. 
%     \item Through experimental evaluations on an extensive a spatio-temporal human action datasets \textit{[include data stats]}, we demonstrate the effectiveness of NerualPrefix\textit{[rephrase]}.
% \end{itemize}



This work makes the following contributions:

\begin{itemize}
    \item Formalizing the concept of zero-shot sensory data imputation as a key enabler for handling missing data
across diverse sensors without requiring re-training.
    % \item Presenting NeuralPrefix; a neural plugin that predicts sensory samples at any given timestamps and demonstrate its utility in intermittent sensing applications. To the best of the author's knowledge, this is the first task/sensor-agnostic data intermittency handling solution. 
    \item Introducing NeuralPrefix; a novel and simple architectural  paradigm (\textit{prefix model}) that decouples the data intermittency handling mechanism from the target model. Thus, upgrading traditional models with intermittency robustness without re-training or fine-tuning. To the best of the authors' knowledge, this is the first task/sensor-agnostic data intermittency handling solution. 
    
   \item Comprehensive experimental demonstration of NeuralPrefix by evaluating it against large-scale spatiotemporal human action datasets, which include two RF datasets and one tactile carpet dataset.
\end{itemize}

\section{Related Work}
\label{sec:related_work}

\update{\textbf{Data Imputation.}} The field of data imputation has seen a broad range of techniques, from traditional statistical methods to more advanced approaches. Early methods like K-Nearest Neighbors (KNN) \cite{knn} and probabilistic approaches such as Expectation-Maximization (EM) \cite{EM} are simple to implement but often serve as baselines due to their inability to capture spatial and temporal, resulting in limited recovery quality for practical use. Spatiotemporal multi-view learning \cite{ST-MVL} improves data imputation by considering both spatial and temporal dependencies, approximating missing values using low-rank matrix completion \cite{LRMC,ensemble} and tensor decomposition \cite{tensor,tensor2}. However, these methods often fail to capture the deeper, inherent dependencies in spatiotemporal sensor data.

\textbf{Deep Generative Models.} %The literature on data imputation is massive. Dozens of methods were developed ranging from statistical methods to the more recent deep approaches \cite{wang2024deep, sun2023deep}. 
Recently, deep learning approaches have emerged for data imputation \cite{wang2024deep, sun2023deep}, including the use of generative models \cite{adhikari2022comprehensive, shahbazian2023generative}, offering more advanced solutions to address these limitations.
This work falls in the category of generative data imputation by building on the same principles. To the best of the author's knowledge, this is the first work to employ generative modeling for zero-shot sensory data imputation. Beyond this main distinction, the framework possess a number of technical qualities that collectively enable a broader range of application. Specifically; the capability to perform interpolation and extrapolation using an independent parameter-efficient design, where the Neural ODE parameters and memory budget are fixed regardless of the sequences sizes. These qualities collectively enable a broader range of applications. 
% \begin{itemize}
%     \item \textit{\textbf{universality} (interpolation, extrapolation, etc ) , \underline{implication} : broader range of apps.}
%     \item \textit{\textbf{learned dynamics }. learning the signal ( latent space) dynamics rather than fixing a prior or being purely data-driven, \underline{implication}: adaptable to  modalities/datasets of different dynamics with reasonable data budget.}
    % \item \textit{\textbf{parameter efficiency} (Neural ODE uses a fixed number of parameters and consumes fixed memory regardless of the input and output sizes), \underline{implication} : useful on edge devices ? }

    % \item \textit{architecture designed for \textbf{spatial-temporal sensory data}, \underline{implication}: addresses the niche area falling between the low dimensional time-series and the high dimensional dense vision data.}
    
% \end{itemize}

% \textbf{Sensor Translation} Recently , a number of works investigated sensor translation \cite{nirmal2024wifi2radar}. \textit{[explain what's sensor translation and what are the different approaches]}


\textbf{Neural ODE works.} Our architecture is inspired by the Neural ODE video generation works MODE-GAN  \cite{kim2021continuous} , VidODE \cite{park2021vid} and \cite{kim2021continuous} with the key difference is the application; focusing on the out-of-domain generalization. An aspect that wasn't investigated in these works. Also, NeuralPrefix targets sparse (\update{lower-dimensional}) sensory data rather than vision data. For this, ours is purposefully simpler (e.g. no GAN as followed in \cite{park2021vid, kim2021continuous}  ), and the design choices tailored for the data type considered, such as the shrinkage loss (Sec.~\ref{sec:architecture}). 

\update{\textbf{Time Series Foundational Models.} Concurrent with our efforts, recent advances in foundational models show the potential of task-agnostic data imputation (among other tasks).  Inspired by Large Language Models (LLMs), these works \cite{ansari2024chronos}  posit that a time series model trained on a large collection of multi-domain datasets can generalise to unseen domains without re-training (i.e. zero-shot). While being a promising direction, these models are huge. Making them much more expensive to train and less deployable in the sensory applications.  For example, Amazon's Chronos \cite{ansari2024chronos} leverages the T5 Transformer (710M parameters) for time series whose training requires 504 hours on A100 GPU (NeuralPrefix takes a mere 6 hours on RTX4090). }


% \section{Prefix Model Background}

% \textbf{Data intermittency:}
% \textit{[ data intermittency impact on downstream tasks from literature]}

% \textbf{Prefix Model.} The prefix model has a \emph{commensal} relationship with the host model as they both \emph{eat at the same data table}. Specifically, the prefix model is  trained on the intermittence-free dataset similar to that used for training the target model. Note that this doesn't require exposing/witnessing the training  of the task model. In fact, training cab applied in  retrospective manner



% \textit{Benefits \textit{[this need to be revised and stressed]}:} This simple design given that the prefix model is detached from to the host model and  target task, it is very flexible. Thus one can use the same prefix for multiple sensors (task models) or even multiple modalities (as we will see in Sec.~\ref{sec:out_of_domain}) 

% \section{Zero-Shot Imputation : Why and How?}

% \subsection{Practical Motivation}
% \label{sec:background_apps}

% \AK{ talk about applications value here. Modality compensation (in unattended sensing)}


% \subsection{Physical Continuity in Sensory Data}



% \subsection{Neural Prefixing}





\section{Method}

% \AK{\sout{give a brief summary (intuitive and convincing) of the method here as section is lengthy}}

\textbf{Brief Notation}. Lowercase bold symbols ($\mathbf{v}$) denote vectors and  uppercase bold symbols ($\mathbf{V}$) for tensors. 
% The symbol ${\llbracket i \rrbracket}_1^N$ denotes the sequence
% $[1, \cdots, N]$ while $\big\{i\big\}_{1}^{N}$  denotes the set of the same values.  
The symbol $\big\{i\big\}_{1}^{N}$  denotes the sequence
$[1, \cdots, N]$.  

\textbf{Problem.} Our goal is developing the prefix  
$\mathcal{G}$
% (\mathbf{X}=\big\{\mathbf{x}(t_i)\big \}^N_{i=1}, {\llbracket m_i \rrbracket}_{i=1}^{K})$
% that takes an intermittent (incomplete) input sequence $\mathbf{X}$ and recover the data  points impacted by intermittency at the temporal coordinates $[m_1,\cdots,m_K]$ .
that takes an intermittent input sequence $\mathbf{X}$ whose data samples \update{are observed} at points $\{t_i\}$ and recovers the data points impacted by intermittency at the temporal coordinates $\{m_i\}$. 
% \The prefix model has a \emph{commensal} relationship with the host model as they both \emph{eat at the same data table}.
Specifically, the prefix model is trained on the intermittence-free dataset similar to that used for training the target model. 
% Note that this doesn't require exposing/witnessing the training  of the task model. 
This simple design, given that the prefix model is \update{detached from the host model} and target task, is very flexible. \update{Thus,} one can use the same prefix for multiple sensors (task models) or even multiple modalities (as we will see in Sec.~\ref{sec:out_of_domain}) 

Formally,  the problem is framed as sequence to sequence regression. Specifically, we can build a generative model to perform the mapping
% $\mathcal{G} : \mathbf{X}=\big\{\mathbf{x}(t_i)\big \}^N_{i=1} \mapsto \big\{\mathbf{x}(t_j)\big\}^K_{j=a}$ such that $[1, \cdots , N] \cap [a, \cdots , K] = \emptyset$.
$\mathcal{G} : \big\{\mathbf{x}(t_i)\big \}^N_{i=1} \mapsto \big\{\mathbf{x}^{\prime}(m_i)\big\}^K_{i=1}$. 
% where $t_i$ denotes the observed time steps and $m_i$ denotes the .
The input data frames $\mathbf{x}(t_i) \in \mathcal{R}^{W\times H \times C}$
% $\big\{\mathbf{x}(t_i)\big\}^N_{i = 1}$ 
% denote the set of non
, where  $W, H$ and $C$  denote width, height and number of channels; respectively, aren't uniformly spaced in time
% input data frames 
(i.e. $(t_i - t_{i-1}) \neq (t_{i-1} - t_{i-2}) $). 
% $\mathbf{x}(t_i) \in \mathcal{R}^{W\times H \times C}$ denotes a single frame at time $t_i$ 
Formulated this way, this mapping is interpolation if we set $m_1 > t_1$ and $m_K<t_N$ and extrapolation if  $m_K> m_1 > t_N$. 
% Note that  $\mathbf{x}(t_i)$ and $\mathbf{x}^{\prime}(m_i)$ don't necessarily come from the same distribution (i.e. sensory domain).

At \update{a high} level, we build $\mathcal{G}$ as Encoder-Decoder architecture (Sec. \ref{sec:architecture}) with a learning objective of minimizing data reconstruction error. During training, we simulate data intermittency by masking some samples from the input sequences and tasking the model with reconstructing them.  
% Yet, the modeling choice is very important. \textit{[relationship between data and internal state? {Very important question} ]}. 
% \textit{[impact of sporadic data on model's state]}
Yet, modeling the sporadic data is challenging, and the key idea is to treat the \update{latent state} as a continuous trajectory. \update{The next} section introduces this idea.




\subsection{Continuous Latent Dynamics Primer}
\label{sec:latent_dynamics}
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width= \linewidth]{images/hidden_state_transition.pdf}
%     % \caption{Caption}
%     % \label{fig:enter-label}
%     \vspace{-1em}
% \end{figure}

\begin{figure}[th!]
    \centering
    \includegraphics[width=1\linewidth]{images/continuous_latent_state.pdf}
    % \vspace{-0.75em}
    \caption{\textbf{Continuous vs Discrete Latent States in Neural Networks.} (Bottom Left) state estimation using a form of ODE. The state at time $t_i$, denoted as $\mathbf{h}(t_i)$, is updated to next state $\mathbf{h}(t_{i+1})$ through the equation above. Since intermediate steps ($\Delta t$) can be arbitrarily small, the continuous trajectory (curved path) can be obtained. }
    \label{fig:continuous_latent_state}
    % \vspace{-1.2 em}
\end{figure}


A typical data-driven approach for modeling sequence to sequence prediction is recurrent  modeling (e.g., RNN). The input data is processed sequentially with the help of internal state $\mathbf{h}(t_i)$ corresponding the temporal coordinate $t_i$ as follows:
\begin{equation}
 \mathbf{h}(t_{i}) = \text{RNN}(\mathbf{h}^{\prime}(t_i),  \mathbf{x}(t_i)) \ , \ \mathbf{h}(t_{i-1}) \xrightarrow{\mathcal{T}_{{}_\text{RNN}}}  \mathbf{h}^{\prime}(t_i) 
 \label{eg:rnn}
\end{equation}
where $\mathcal{T}_{{}_\text{RNN}}$ manages the state transition across temporal steps in a uniform manner regardless of the temporal gap between incoming samples. Since \eqref{eg:rnn} assumes uniformly spaced data samples, its internal state is kept unchanged until the next data point arrives
% . Ultimately, its evolution is discrete
as shown in Fig.~\ref{fig:continuous_latent_state} (\update{top} right). Consider the long temporal gap between the square and triangle in the figure. The internal state (and thus the output) won't evolve in that stretch due to the discrete nature of $\mathcal{T}_{{}_\text{RNN}}$. 
% the temporal gaps without observations will \textit{[complete]} .
A possible solution is to gradually deviate from the last state as time progresses using  approaches such as temporal decay \cite{rajkomar2018scalable}. A better alternative is reflecting the continuous nature of incoming signal into the model's latent space by requiring the internal state transitions to follow a continuous trajectory (identified by an ODE). In effect, combining the data with a ODE state transition prior $\mathcal{T}_{{}_\text{ODE}}$. Thus at any temporal point, even when the observation is missing,  the model will estimate the internal state trajectory by relying on the last known state 
% (corresponding to an observation )
and the expected state dynamics (i.e. rate of change) as depicted in Fig.~\ref{fig:continuous_latent_state} (bottom left). In other words, we frame the problem as state estimation in a continuous dynamical system. 


% Specifically, we are given  non uniformly spaced observations $\{\mathbf{x}(t_i)\}^n_{i = 1}$ at times $t_1,t_2,\cdots,t_n$ from which we seek to compute the state $\mathbf{h}(t)$ at arbitrary time $t$ (including unobserved time points).
Specifically, and given $\mathbf{X}$,  we seek to compute the state $\mathbf{h}(t) \in \mathcal{R}^{d}$ at arbitrary times $t_i$ (equivalently at intermittent points $m_i$).  We compensate for missing data by relying on a component $\mathbf{g}(\mathbf{h}(t), t) = \frac{d\mathbf{h}(t)}{dt}$ that captures the evolution of model's internal state. 
% known (later this will be relaxed)
% dynamics.
Initially, assume that $\mathbf{g}(\mathbf{h}(t), t)$ is given (we relax this later).
% by solving the ODE starting from a known state. 
Given the above, $ \mathbf{h}(t)$ can be determined from the last known state at $t_k$, where $t_k<t_i$,  as follows:
\begin{equation}
\mathbf{h}(t_i) = \mathbf{h}(t_k) + \int_{t_k}^{t_i}  \mathbf{g}(\mathbf{h}(t), t) dt
\label{eq:ivp}
\end{equation}



Intuitively, equation \eqref{eq:ivp} is an ODE initial value problem (IVP) that continually integrates the rate of change (of the system's state) to find the next state. One step of this process (in a \update{discrete-time} step) is depicted in Fig.~\ref{fig:continuous_latent_state} (top). This is fundamental in solving problems involving dynamic systems. 

Before continuing,  we note two things. First, the dynamics of the hidden state $\mathbf{g}(\mathbf{h}(t), t) $ aren't known in practice. We follow \cite{chen2018neural} and parameterize that component as a neural network $\mathbf{g}_{\theta}(\mathbf{h}(t), t)$, where $\theta$ denote the network's weights, to learn the dynamics in end-to-end manner. Second, there is no \update{closed form} solution for the integral in \eqref{eq:ivp}. However, off-the-shelf ODE solvers, such as Euler and Runge-Kutta, can be leveraged. Consequently and considering the changes, we can re-write \eqref{eq:ivp} as :
% \begin{equation}
%     \mathbf{h}(t_i) = \overbrace{\textbf{\text{ODESolve}}}^{\text{black box ode solver}}(\underbrace{\mathbf{g}_{\theta}}_{\text{dynamics}},\underbrace{\mathbf{h}(t_{k})}_\text{initial state},\underbrace{(t_{k}, t_i)}_{\text{time steps}})
% \end{equation}
\begin{equation}
    \mathbf{h}(t_{i+1}) = \overbrace{\textbf{\text{ODESolve}}}^{\text{black box ode solver}}(\underbrace{\mathbf{g}_{\theta}}_{\text{dynamics}},\underbrace{\mathbf{h}(t_{i})}_\text{initial state},\ \underbrace{(t_{i}, t_{i+1})}_{\text{time steps}}\ )
\label{eq:ode_solve}
\end{equation}
where \textbf{\text{ODESolve}} abstracts away the integral computation in a black box ODE solver. Including the solver as \update{a black box} component offers interesting benefits. The solver can be upgraded/adapted during the inference time without re-training. It allows for different trade-offs between computational efficiency and accuracy at various stages of the model's deployment. Note that the end-to-end network training can be done efficiently in the presence of black box solver using the Adjoint Sensitivity Method .\footnote{ Note that the end-to-end network training can be done efficiently in the presence of black box solver using the Adjoint Sensitivity Method. \update{Details were omitted due to space limitations.} The interested reader can consult \cite{chen2018neural}.}. 

A unique property of \update{continuous modelling is the flexibility} that allows catering for various intermittency modes and applications, \update{as shown in Figure~\ref{fig:forward_backward_mode}}. In addition to interpolation, the prefix can be configured on extrapolation. Thus, upgrading the task model with early action prediction \cite{wang2019progressive,kong2017deep} capabilities. Even more interesting, as ODE can be solved in the backward direction, retrospective discovery can be done. Thus, one can estimate the dynamics preceding the first incoming frame. 
\begin{figure}[t!]
    \centering
    \includegraphics[width= 1\linewidth]{images/ODE_modes_2.png}
    \caption{\update{\textbf{ODE data recovery modes include interpolative, predictive and retrospective. }}}
    \label{fig:forward_backward_mode}
\end{figure}


So far, we demonstrated the mechanism by which we can enable continuous state transition in a neural network. Next, we explain how this mechanism is used in NeuralPrefix encoder-decoder architecture.



\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{images/neuralprefix_architecture_large.pdf}
    \caption{\textbf{NeuralPrefix Plugin.}
    % \AK{I need to expand this a bit}
    NeuralPrefix is a continuous Encoder-Decoder architecture that imputes data in an autoregressive manner. Internal (latent) state transitions are governed by Ordinary  Differential Equations. (Left) simplified linear view \update{of the} architecture. (Right) unrolled view of the architecture showing the progression of data generation process. 
    % Dotted layers indicate the same layer involvement at different temporal coordinate (model parameters are fixed for all input sizes).
    Modular Frame Generation (purple box) uses Eq.~\ref{eq:composition} to compose the decoder's outputs into the target frame.}
    \label{fig:arhcitecture}
\end{figure*}

\update{Before presenting NeuralPrefix architecture, we note a key distinction between the dynamics in NeuralODE  ($\mathbf{g}_\theta$) and those in traditional (neural network-free) differential equations ($\mathbf{g}$). Such distinction makes each suitable for a different scenario depending on the application's requirements.}

% Applications requiring interpretability and theoretical grounding favor traditional ODEs, while tasks needing flexibility, data-driven adaptation, or complex feature learning benefit from NeuralODEs.

\update{\textbf{Interpretability and Data-Efficiency.} Indeed, \textit{standard differential equations} can be used for imputation applications.  
For example, in a wearable sensing context, one might model human arm movements as a damped harmonic motion.} 
% where the arm's return to a rest position (like hanging down by the side) acts similarly to a spring pulling it back, and the natural muscular and air resistance act as damping forces
\update{Thus, the dynamics $\mathbf{g}$ are explicitly captured (not learned) using the differential equation $\ddot{x} + 2\zeta\omega_0\dot{x} + \omega_0^2 x = 0
$ where $x$ is the displacement from the equilibrium (rest position), $\omega_0$ is the natural frequency of the system and $\zeta$ is the damping ratio. After calibration (i.e. estimating the parameters $\omega_0$ and $\zeta$), one can leverage the analytical formulation for imputation directly. A notable advantage here is data efficiency, as the parameters can be estimated using much fewer samples. However, such simplified modelling can fail to capture complexities such as individual variability,  fatigue, and impact of muscle dynamics. Standard differential equations are more suitable in low-data regimes and when the dynamics are well understood. }

 % by solving it numerically

% \update{Such approach is generally helpful in cases where the explicit dynamics are available. In NeuralPrefix we propose to learn these dynamics (rather than fixing a single model)  in the latent space ( i.e features processed by neural components rather than the raw noisy data space)} 

\update{\textbf{Non-rigid Dynamics.} \textit{Neural differential equations}, on the other hand,  allows for more flexibility as it learns the dynamics function ($\mathbf{g}_\theta$) in a data-driven way while incorporating neural networks for advanced feature extraction. This approach is especially useful when the true dynamics are unknown or too complex to describe analytically, enabling its application to a wider range of sensory problems. Additionally, it is more powerful when the data is noisy \cite{Goyal2023,goyallearning}. However, the effectiveness of data-driven methods depends on the quantity and diversity of the data. With the growing availability of large sensory datasetsâ€”and given that our approach leverages only \textbf{unlabeled} dataâ€”we see significant potential for its application.  }

% allows for more flexibility as it learns the dynamics function ($\mathbf{g}_\theta$) in a data-driven manner and further integrates neural networks for advanced feature extraction. This is particularly useful where the true dynamics are unknown or too complex to be described analytically. Thus, it can be applied to a much broader range of applications. A caveat here is that the generalization of data-driven approaches depends on the data quantity and diversity. Given that big sensory datasets are becoming increasingly available and we use only the \textbf{unlabeled} data, we see great potential for the proposed approach.








\subsection{NeuralPrefix Archiecture}
\label{sec:architecture}






% \begin{figure}[th!]
%     \centering
%     \includegraphics[width=1\linewidth]{images/neuralprefix_architecture.pdf}
%     \caption{\textbf{NeuralPrefix Plugin.} (Top) overview of the architecture. (Bottom) unrolled view of the architecture showing the progression of data generation process. Dotted layers indicate the same layer involvement at different temporal coordinate (model parameters are fixed for all input sizes). Eq.~\ref{eq:composition}  is used to compose the decoder outputs into the target frame. }
%     \label{fig:arhcitecture}
% \end{figure}



Our architecture is a continuous-time encoder-decoder pipeline similar to \cite{rubanova2019latent} trained to reconstruct the masked data frames. During training, frames of the complete samples are randomly masked to imitate intermittency. The model is then trained to recover the masked samples by conditioning on the observed ones.

We follow the standard blueprint of  encoder-decoder approaches. We design continuous-time \update{enconder} and decoder. In the encoding \update{stage,} we perform \textit{forward consolidation} in which all the available frames are encoded into one (condensed) latent state vector. Similar to recurrent \update{approaches,} we process the frames sequentially. Unlike them, we base the latent state transition on NeuralODE.
For generation, we apply \textit{ backward unrolling}; an autoregressive process that continually generates all the missing \update{frames} (in reverse order) from the latent state vector. Directly predicting the target frame in ``one go'' is typically employed when operating in in-domain (i.e, training and test sample are expected to be homogeneous).  However, this doesn't generalize to out-of-domain setting.  To resolve this, we integrate a \textit{modular frame generation} component. Roughly speaking, we think of the generated frame as mostly a perturbation of the last observed frame. Thus, the decoder is tasked with estimating the perturbations as dictated by the cross-frames dynamics rather than the appearance. 
% \begin{wrapfigure}{r}{0.35\linewidth}
%   \begin{center}
%     \includegraphics[width=1\linewidth]{images/simplified_view.png}
%   \end{center}
%   % \caption{Birds}
% \end{wrapfigure}
% As demonstrated in Fig.~\ref{fig:arhcitecture}, each frame of the unmasked input sequence $\mathbf{X}$ is first mapped into embedding using convolutions layer.
Fig.~\ref{fig:arhcitecture}, summarizes NeuralPrefix architecture. Details of the architecture components below. 

\textbf{Encoder \textit{(forward consolidation)}.} Convolutional gated recurrent unit \texttt{ConvGRU} is used in the encoder $\mathcal{G}_{E}$ to capture the spatio-temporal patterns of the sensory data. \update{ConvGRU is an extension of the standard Gated Recurrent Unit (GRU) designed specifically to process spatiotemporal data, such as video frames or sequences of images. It combines the recurrent nature of GRUs, which handle temporal dependencies, with convolutional operations, which handle spatial patterns. Check \cite{ballas2015delving} for more details.} At each \update{step,} the input is processed using convolutional embedding prior to applying  \texttt{ConvGRU}. Latent state is continually estimated at all times as an ODE solution using Eq.~\eqref{eq:ode_solve} where a fully connected network \texttt{MLP} approximates the dynamics component $\mathbf{g}_\theta$. As shown in the unrolled view in Fig.~\ref{fig:arhcitecture},  which is passed to the decoder to be used for frames imputation. 




% We follow \cite{rubanova2019latent} in performing the ODE solving in the backward direction. 

\textbf{Decoder \textit{(backward unrolling)}} The decoder $\mathcal{G}_D$
% \big(\llbracket m_i \rrbracket, \mathbf{h}(t_N) \big)$ 
generates the missing samples at the specified time steps. First, using ODESolve, we  estimate the latent codes $\big\{ \mathbf{h}(m_i) \big\}$ (green circles in Fig. \ref{fig:arhcitecture}) at the target temporal coordinates $\big\{ m_i \big\}$.   Then, the decoder iteratively maps the latent codes into the target frame $\mathbf{x}^\prime(m_i)$.  


\textbf{Modular Frame Generation.}
% While it is possible to predict the target frame directly, we set the decoder to predict a decomposed outputs that collectively recover the target frame. Specifically, we regard the generated data frames as a composition of appearance (content) and dynamics (motion). The decoder predicts content $C_{m_i}$ and a residual $R_{m_i}$ capturing frames difference between the steps $m_i$ and $m_{i-1}$. Together they can be blended \cite{shen2024ladder, kim2021continuous} into the final prediction as follows:
An important question that we address in NeuralPrefix is how to generate frames for unseen modalities? While it is possible to train the decoder to generate the whole frame directly, this approach creates issues in out-of-domain \update{settings}. Essentially, it will cause the appearance features to be ``baked'' into the generative model weights. Thus, the output will resemble the training data even when operating on \update{a new} modality. To address the \update{issue,} we predict the \textit{frame differences} components rather than the actual frames. 
% Specifically, we adopt a modular generation in which we generate a set of  elementary motion components that when combined with the last observed/generated frame recovers the new frame. 
We predict the motion flow information that represents cross-frames motion of pixels. \update{Then,} we compose the actual frame guided by the predicted motion flow and the last observed (i.e. seen or generated) frame.  To understand this, one can think of the generated frame as a weighted sum of a transformed (wrapped) version of the last frame and the new contents (appearance residuals) that will appear only in the new frame. Formally \cite{shen2024ladder, yu2022deep}, the generated frame is :

\begin{align}
    \label{eq:composition}
    \mathbf{x}^\prime(m_i) & = \Gamma_{m_i} \odot C_{m_i} + (1 - \Gamma_{m_i}) \odot R^\prime_{m_i} \\
    \label{eq:content}
    C_{m_i} & = \mathcal{W}(F_{m_i}, \mathbf{x}^\prime(m_{i-1}) )
\end{align}
where $\Gamma$ , $\mathcal{W}$  and $F$  denote a binary composition mask, the image warping operator and the forward motion flow; respectively. We task the decoder to learn the components of equation above. The mask is ensured to be bounded in the range [0,1] by applying sigmoid function. Notably, the motion flow is readily interpretable (akin to optical flow in the vision domain) and can offer insights into the system \udpate{behaviour} under various settings (Sec. \ref{sec:out_of_domain} ).   We can see that the content term is created as a warping of the frame generated by the model at the last timestep. during training, we use a residual loss $\mathcal{L}_{\text{residual}} = R^{\prime}_{m_i} - R_{m_i}$  where $R_{m_i}$ is calculated from the ground truth as the pixel-wise difference between consecutive frames. Additionally, we employ a content loss $\mathcal{L}_{\text{content}}$ as the MSE between all the frames generated at $m_i$ and the corresponding ground truth.

% For the first generated frame we condition on the last frame from input. For other   



% Leveraged the continuous capability of the model we can ensure that the 



% All are learned during training. 


% \newpage


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{images/shrinkage_small.pdf}
    \caption{Comparison of pixel-wise losses. 
    % The weighting considered by the proposed loss (2nd row, right) is contrasted to the  uniform weighting (2nd row, left) common in the literature.
    The proposed $\mathcal{L}_{\text{shrinkage}}$ focuses on foreground regions and less on the background and noise as opposed to the  uniform weighting by $\mathcal{L}_{\text{MSE}}$. }
    \label{fig:shrinkage}

\end{figure}


\textbf{Shrinkage Loss} The sparsity of sensory (e.g. RF frames) results in an imbalance between the foreground (e.g. RF reflections from hand and body) and background pixels (e.g. pixels with low energy). 
% This imbalance can be extreme in some cases \textit{[experiment to show the sparseness of different RF datasets (Soli, TMC cross domain). note that this holds after background subtraction]}.
Making the reconstruction tricky compared to natural images. For example, the  Mean Squared Error (MSE) 
% $\mathcal{L} = \frac{1}{NM} \sum_{u=1}^{N} \sum_{v=1}^{M} \phi(u,v)  $ ,
% % \begin{equation}
 %    \mathcal{L} = \frac{1}{NM} \sum_{u=1}^{N} \sum_{v=1}^{M} \phi(u,v) \ , \label{eq:mse}
 % \end{equation}
% where $\phi(u,v) = {[ \mathbf_g(u,v)- I_p(u,v) ]}^{2}$ 
 % quantifies the squared difference of pixels values between the ground truth and prediction. In this form, the loss
 penalizes the reconstruction error for the whole image pixels in a uniform way.
 % Implying equal importance of all pixels. 
 While this is desirable for natural images with rich content and texture, it can 
 % lead to under-fitting in sparse RF images \textit{[rephr]}. Specifically,
 % the dominance of
 sub-optimal for data where zero pixels and low energy noise dominate. 
 Without proper weight, the loss {\em unrealistically} small even for poor predictions.
 % For example, if $N=M=100$ and $I_g$ has a sparsity of 80\%, then a completely blank prediction (i.e. $I_p(u,v) =0 ,  \forall u,v $)  will have a mere MSE loss of $0.2$. A possible solution is to nudge the network to focus more on the non-sparse regions of the RF image.
 To mitigate the issue, we suggest attaching more importance to the few, but hard to reconstruct, informative pixels. Inspired by a similar issue in object detection \cite{lin2017focal} and object tracking \cite{lu2020deep}, we adopt the shrinkage loss $\mathcal{L}_{\text{shrinkage}}$ ( Eq.(6) in \cite{lu2020deep}) that down weights the easy pixels. Figure~\ref{fig:shrinkage} contrasts it to the MSE. 
 % \begin{equation}
 %    \mathcal{L}_{\text{shrinkage}} = \frac{1}{WH} \sum_{u=1}^{W} \sum_{v=1}^{H} \ \underbrace{{ |I_g(u,v)- I_p(u,v)| }^{\alpha}}_{ \text{focal weight} : w(u,v,\alpha)} . \ \phi(u,v) \label{eq:focal_mse}
 % \end{equation}
Putting it all together,  the model is trained in end-to-end manner with the following loss:
\begin{equation}
    \mathcal{L}(\mathcal{G}) = \lambda_{1} \mathcal{L}_{\text{shrinkage}} +\lambda_{2} \mathcal{L}_{\text{residual}} +\lambda_{3} \mathcal{L}_{\text{content}}
\end{equation}
where $\lambda_{1}$, $\lambda_{2}$  and $\lambda_{3}$ are hyper parameters controlling the contribution of each loss term.



\section{Experimental Evaluation}

% In this section, we evaluate NeuralPrefix out-of-domain data imputation performance and compare it to the state of the art imputation techniques. \AK{rephrase}.  

\subsection{Datasets}
\label{sec:eval_dataset}


We test NeuralPrefix \footnote {More implementation details and trained models can be found in \href{https://anonymous.4open.science/r/NeuralPrefix-1A94/readme.md}{https://anonymous.4open.science/r/NeuralPrefix-1A94/readme.md}.}on three public sensory datasets. The first two datasets contain radar sensor measurements of human gestures, while the third is a pressure mat dataset capturing daily human activities such as locomotion, exercises, and resting. For all datasets, NeuralPrefix treats the samples as 4D heatmaps with time, spatial (width \& height) and channel dimensions. Our motivation for considering the radar data is being well known in the RF domain \cite {wang2022placement} as well as deployment consideration significantly \update{impacts} the datasets \cite{wang2022placement}. Even simple changes in either the \update{receivers'} placements, the user location, the user's orientation or the furniture in the environment will alter the received signal considerably. While this \update{holds} for other sensing modalities such as IMUs for human activity recognition), the domain shift is often more challenging in RF \cite{nirmal2021deep}. This presents a nice playground for testing zero-shot imputation performance under deployment domain shift (\textit{domain-OOD}).  Specifically, by training the model in one configuration (user orientation and device placement) and testing it on the rest, we can assess the system's performance in unseen domains. Additionally, the radar and the pressure mat datasets enable us to evaluate the system's performance on unseen modalities (modality-OOD) by training on one dataset and testing on the others. Details of the datasets are provided below.

% \AK{more details about the datasets to be added}

\begin{itemize}
    \item \textit{Google's Soli Dataset} \cite{wang2016interacting}. This dataset contains 11 gestures from 10 subjects collected over multiple sessions. Each frame of the dataset is represented as a Range Doppler Image (RDI). In an RDI, one axis denotes the radial distance (or range) between the hand and the radar, while the other axis represents velocity. The pixel intensity corresponds to the energy reflected from objects (e.g. hands). The Soli sensor was employed in various applications, including gesture interaction with wearable \cite{wang2016interacting}, and objects \cite{vcopivc2019missing}, as well as material sensing \cite{vcopivc2022solids}. 
    
    \item \textit{MCD dataset} \cite{li2022towards}. This dataset contains RF measurements (using TI AWR1843 mmWave radar sensor) from 750 domains (6 environments $\times$ 25 subjects $\times$ 5 locations). The frames are Dynamic Range Angle Image (DRAI) measurements of subjects performing hand gestures.  In the RF datasets, we use half of the data for training and the other half for testing. 
    \item \textit{Intelligent Carpet dataset}\cite{luo2021intelligent}.  This dataset is used as an out-of-domain modality for the zero-shot experiment in Sec.~\ref{sec:out_of_domain}. A random subject data (\textit{24\_10\_TZ}) was used for testing.
% for training the model and a different subject (\textit{24\_10\_YL})

\end{itemize}


% We consider the following evaluation setups: 

% \begin{itemize}
%     \item \textit{OOD-domain}
%     \item \textit{OOD-modality}
% \end{itemize}




\subsection{Metrics}

% \end{itemize}
% \textit{Metrics.}
To report the performance, standard metrics used in spatial-temporal data imputation are employed. Namely, the Mean Squared Error (MSE), the Structural Similairy (SSIM) \cite{wang2004image}, Peak Signal-to-Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018unreasonable}.  These metrics capture \update{the} distance/similarity between the model's prediction and the ground truth. \update{SSIM is bounded in the range $[-1,1]$.} Additionally, we report the performance on a downstream task of hand tracking after applying the imputation.

\subsection{Baselines and Training}
\label{sec:eval_baseline_details}

% \textit{Baseline and Training.} 
In the evaluation, we compare against \update{several traditional approaches commonly used as efficient baselines in time series imputation works \cite{du2024tsi, ma2020transfer}. Additionally, we consider a theoretically-principled training-free approach based on optimization and a spatial-temporal interpolation deep model. Specifically, we consider the following traditional approaches:}

\begin{itemize}
    \item \update{\textbf{Mean.} Missing values are imputed as the mean of the observed values.}
    \item \update{\textbf{Last Observation Carried Forward (LOCF).} Missing values are replaced with the last observed value for that variable.}
    \item \update{\textbf{Expectation Maximization (EM) \cite{dempster1977maximum}}. We adopt the expectation-maximization algorithm to impute the missing frames in $\mathbf{x}$. We consider imputing the temporal trajectory of each pixel $\mathbf{x}_{h,w,c} = \{x_{h,w,c}(o_1), \cdots, x_{h,w,c}(o_q)\}$ where $h,w$ denotes the spatial coordinates, $c$ is the channel coordinate and $o_1, \cdots , o_q$ is the set observed temporal coordinates. We assume Gaussian Mixture Model (GMM) as the data distribution. The algorithm alternates between probabilities computation (E Step) and parameters update (M Step). After convergence, imputation is done by sampling from the fitted GMM $\sim \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \sigma_k^2)$ where $K = 3$ is the number of components. $\pi_k , \mu_k$ , and $\sigma_k^2$ are the weight, mean, and variance of the $k$-th Gaussian component; respectively.} 
    % Before applying the EM steps, the frames are normalized in the range [0-1] and after imputation, post-processing is applied to scale the values back to the range [0-255]
    \item \update{\textbf{Optical Flow (OF) \cite{horn1981determining}.} OF leverages the vector field describing the apparent motion of each pixel between two adjacent
frames for imputation. }
    % \item \textbf{Optimal Transport}

    
\end{itemize}



\update{Additionally, we consider  the training-free baseline; \textbf{Optimal Transport (OT)} \cite{khamis2023earth}.} OT is a mathematical framework that enables us to compute the most efficient way to ``move'' one set of data points (such as probability distributions, images, or signals) to another while minimizing some notion of cost. In the context of data interpolation, OT can be used to create smooth transitions between two frames by finding an optimal transportation plan between them. Specifically, we treat two frames as probability distributions $P$ and $Q$. Then, we compute the optimal transportation plan using Equation 2 in \cite{khamis2023earth}. \update{The imputed data is computed using the transportation plan as a series of intermediate distributions that gradually transition from $P$ to $Q$.} For the training-based baseline, we consider a generative model. For fair evaluation, we compare against a Neural ODE -based architecture \cite{kanaa2021simple}. 

\update{\textbf{Training and Implementation Details.} In all experiments, the model is trained for 500 epochs on a Nvidia RTXA6000 GPU. The training takes about 6 hours.  Adam optimizer is used, and the learning rate is set as ${10}^{-3}$, then exponentially decayed at a rate of 0.99 per epoch. We consider a batch size of 64. The parameters $\lambda_{1}$ , $\lambda_{2}$  and $\lambda_{3}$ are set to  0.05, 0.5 and 1; respectively.}




\begin{figure*}
    \centering
    \includegraphics[width = 1\linewidth] {images/qualitative_output.pdf}
    \caption{Qualitative comparison of intermittent data reconstruction using NeuralPrefix vs Neural ODE video generation baseline  \cite{kanaa2021simple} on MCD dataset. For visual convenience, we show only the dropped samples (i.e. w/o preceding and the following samples). It can be seen that NeuralPrefix significantly outperforms the baseline. }
    \label{fig:qualitative_comparison}
\end{figure*}





\subsection{Zero-shot Imputation Performance (domain-OOD)}
\label{sec:eval_imputation}

\begin{table}[h!]
    \centering
    \caption{Imputation Performance}
    \label{tab:imputation_performance}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Datasets} & \textbf{Model} & \multicolumn{4}{c}{\textbf{Interpolation}} \\ 
        \cmidrule(lr){3-6} 
        & & \textbf{SSIM}$\uparrow$ & \textbf{MSE}$\downarrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ \\ 
        \midrule
        \rowcolor{teal!20}\multirow{7}{*}{MCD} 
         & Mean & 0.7534  & 0.0017 & 0.0437 & 31.4275  \\
        \rowcolor{teal!20} & LOCF & 0.7512  & 0.0023 & 0.0506  &  30.5471 \\ 
        \rowcolor{teal!20}& EM & 0.6582  &  0.0035& 0.0952  & 28.9941  \\ 
        \rowcolor{teal!20} MCD& OF & 0.7831  & 0.0018 & 0.0370 &   34.4903\\ 
        
         & OT & 0.7143  &0.0041  & 0.1734 &  33.7536 \\ 

        & Baseline \cite{kanaa2021simple} & 0.8726  & 0.0015  & 0.1225 &  28.2929 \\ 
        & NeuralPrefix & 0.9399 & 0.0007 & 0.0752 & 31.8556  \\ 
        \midrule
        \rowcolor{teal!20}\multirow{7}{*}{Soli} 
        & Mean & 0.9561  & 0.0017 & 0.0441 &  32.2322 \\ 
        \rowcolor{teal!20}&  LOCF & 0.9646 & 0.0019  & 0.0331 & 32.1244 \\ 
        \rowcolor{teal!20}& EM & 0.9474  &  0.0026  & 0.0361 &  30.0411 \\ 
        \rowcolor{teal!20} Soli& OF & 0.9675  & 0.0017 & 0.0313 &  31.9493 \\ 
        
        & OT & 0.8937 & 0.0325 & 0.1815 & 24.0149  \\ 
        & Baseline \cite{kanaa2021simple} & 0.9497 & 0.0025 & 0.1524 & 26.1049  \\ 
        & NeuralPrefix & 0.9817 & 0.0006 & 0.0260 & 32.4366  \\ 
        \midrule
        \multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Extrapolation}} \\
        \cmidrule(lr){3-6} 
        & & \textbf{SSIM}$\uparrow$ & \textbf{MSE}$\downarrow$ & \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ \\ 
        \midrule
         MCD & NeuralPrefix & 0.8569 & 0.0017 &  0.1524 & 27.4833  \\
         \midrule
         Soli & NeuralPrefix &0.9202 & 0.0076 & 0.1930 & 30.4366 \\ 
         \bottomrule
    \end{tabular}
\end{table}



We first evaluate the model's imputation performance in two modes; interpolation and extrapolation. In each mode we drop 50\% of the data samples at the points ${m_i}$. In interpolation mode, the samples are taken at random position ${m_i}$  between $t_1$ and $t_N$. In extrapolation mode, the model observes the first half of the window and then reconstructs the remaining half. We set the window size to 10.


Table~ \ref{tab:imputation_performance} shows the imputation performance.  \update {Quantitatively, NeuralPrefix outperforms all the baselines on both Soli and MCD datasets for most metrics. The performance gap is bigger in the MCD dataset. One can notice that the traditional approaches Mean, LOCF and OF are slightly worse than the learned approaches on the Soli dataset.  However, the performance gap is much bigger in the MCD case. This is because MCD dynamics are more complex as the data contains reflections from multiple objects (subject's arm, subject's body and background reflections). On the other hand, Soli data is dominated by hand reflections with minimal background interference. Also, MCD has a much lower sampling rate than Soli (20 FPS in MCD vs 40 FPS in Soli). Thus, cross-frame transitions are much less smooth in MCD.} 
\update{The results reveal that Expectation Maximization is the least performing on the MCD dataset despite being much more computationally demanding than simpler approaches such as Mean and LOCF.  EM has several limitations, including data modelling assumptions (e.g., assuming Gaussian Mixture Models). Recent works \cite{ma2021emflow, richardson2020mcflow} address this by combining EM with deep models. Optimal Transport (OT) also underperforms NeuralPrefix as it relies only on the physical principles (i.e., the most efficient transportation of pixels between frames) without leveraging the knowledge in the training dataset. We exclude traditional approaches from further evaluations.}

Qualitatively, it can be seen from Fig.~\ref{fig:qualitative_comparison} that the learned baseline output is smeared out in regions with high energy. NeuralPrefix, on the other hand, preserves the sharp details as its frame composition (Eq. \eqref{eq:composition} and Eq.\eqref{eq:content}) explicitly learns the motion flow and the difference (residual) across frames. 


% \begin{table}[h!]
%     \centering
%     \caption{Imputation Performance}
%     \label{tab:imputation_performance}
%     \label{table:comparison}
%     \begin{tabular}{llcccc}
%         \toprule
%         \textbf{Datasets} & \textbf{Model} & \multicolumn{4}{c}{\textbf{Interpolation}} & 
%         \cmidrule(lr){3-6} 
%         & & \textbf{SSIM}$\uparrow$ &
%         \textbf{MSE}$\downarrow$ &
%         \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ & 
%         \midrule
%         \multirow{2}{*}{\textbf{MCD}} 
%         & Baseline &  &  &  &  \\
%         & NeuralPrefix & 0.9399 & 0.0007 & 0.0752 & 31.8556  \\
%         \midrule
%         \multirow{2}{*}{\textbf{Soli}} 
%         & Baseline& 0.9497 & 0.0025 &  0.1524 & 26.1049  \\
%         & NeuralPrefix & 0.9817 &0.0006 & 0.0260 & 32.4366  \\
%         \bottomrule
%          & & \multicolumn{4}{c}{\textbf{Extrapolation}} &
%         \cmidrule(lr){3-6} 
%         & & \textbf{SSIM}$\uparrow$ &
%         \textbf{MSE}$\downarrow$ &
%         \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ & 
%         \midrule
%     \end{tabular}
% \end{table}









\subsection{Impact on Downstream Task}
\label{sec:eval_downstream_task}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/ecdf.pdf}
    \caption{\textbf{Hand tracking through intermittent stream in MCD dataset.} CDF curves for the hand tracking error demonstrates the superiority of NeuralPrefix. Median error are denoted by triangular markers on x-axis. Inset plot shows the trivial baselines in which the hand position in the missed frames is set either randomly (`random') or as the median point of the range/angle bounds (`midpoints').  }
    \label{fig:CDF_tracking}
    % \vspace{-2em}
\end{figure}


While the results in Sec.~\ref{sec:eval_imputation} quantify the visual resemblance between the model's predictions and the ground truth, it is also informative to quantify the impact of this on a downstream task.  We perform hand tracking on the MCD frames generated by NeuralPrefix. Note that RDAI sequences encode the location and motion information of the hand and the body. We first locate the hand in each frame as the center $(r,\theta)$ of the blob with the highest magnitude (following Sec. 3.2.7 in \cite{li2022towards}), where $r$ and $\theta$ denote the range and angle polar coordinates; respectively. We assume that the location in the ground truth frames is perfect (i.e. error = $0cm$) and calculate the deviation with respect to it.  The error is reported as euclidean distance $d = \sqrt{r_{i}^2 + r_{g}^2 - 2r_{i} r_{g} \cos(\theta_{i} - \theta_{g})}$, where the subscripts $i$ and $g$ denote the coordinates in the imputed and ground truth frames; respectively. The results are shown in Fig.~\ref{fig:CDF_tracking}.
% For this problem, trivial baselines (inset plot in Fig.~\ref{fig:CDF_tracking}) fail.
NeuralPrefix median error is $14.85cm$ compared to $56.85cm$ by the baseline. This \update{is a} $3.8$x improvement in the tracking accuracy.  This is a direct consequence of the improved visual quality of NeuralPrefix's output. As the details of frames are sharper in NeuralPrefix case, the hand's blob can be located accurately.



\subsection{Zero-shot Imputation Performance (modality-OOD) }
\label{sec:out_of_domain}

This section evaluates the zero-shot performance of NeuralPrefix. The model is trained on one dataset (seen) and tested on another (unseen) dataset from a different sensory domain. This is done without re-training or adaptation. Thus, testing  the model's zero-shot generalization capacity. The results shown in  Table. ~\ref{tab:zero_shot_performance} are promising. It demonstrates good performance on the unseen dataset with a minor degradation compared to the in-domain reference (in which the target and source datasets are the same). Note that although MCD and Soli are RF datasets, they were collected using different sensors with different configurations. Also, the data frames capture semantically different measurements. The results are more interesting in the second part of Table. ~\ref{tab:zero_shot_performance}, in which we test on the Intelligent Carpet dataset. In this case, Soli is picked as the seen dataset as it is visually closer to it compared to the MCD. 


\begin{table}[h!]
    \centering
    \caption{Zero-shot Data Imputation Performance}
    % \vspace{-1em}
    \label{tab:zero_shot_performance}
    \label{table:comparison}
    \begin{tabular}{llcccc}
        \toprule
        \textbf{Seen} &  \textbf{Unseen} &  \multicolumn{4}{c}{\textbf{ performance}} \\
        \cmidrule(lr){3-6} 
        & &  \textbf{MSE$\downarrow$} & 
        \textbf{SSIM$\uparrow$} &
        \textbf{LPIPS$\downarrow$} & 
        \textbf{PSNR$\uparrow$ }  
        \\
        \midrule
        MCD 
        & Soli & 0.0012 & 0.9402 & 0.0843 & 29.1026 \\
        \multicolumn{2}{c}{in-domain reference} & 0.0006 & 0.9817& 0.0260 & 32.4366 \\
        \midrule
        Soli
        & MCD&0.0012 & 0.8888 & 0.1269 & 29.1438 \\
       \multicolumn{2}{c}{in-domain reference} & 0.0007 & 0.9399 & 0.0752 & 31.8556 \\
       \bottomrule
       Soli & Carpet & 0.0016 & 0.8820  & 0.1335 & 27.8321 \\
        \bottomrule
    \end{tabular}
\end{table}


Upon further analysis, we observed the model can still predict the dynamics on the unseen dataset correctly. In Fig.~\ref{fig:out_of_domain_carpet} (zoomed-in part), the ground truth reveals the subject motion transition from two feet on the mat to just one. Visually NeuralPrefix wasn't able to recover the frames exactly. Yet, the forward flow predicted by the model ($F_{m_{i}}$ in Eq.\eqref{eq:composition}) caused the active blob to gradually shrink closing the gap between the two shapes.  

Recall from Sec. \ref{sec:architecture} that the motion flow information ($F_{m_i}$ in Equation. \ref{eq:composition}) is one of the outputs of the modular frame generation component. Since the motion flow can be visualized as vector flow, we can understand NeuralPrefix's behaviour in unseen modalities by inspecting them. Figure \ref{fig:motion_flow} shows the motion flow in seen and unseen modalities. In this case, NeuralPrefix was trained on MCD dataset and tested on Soli then on Carpet without fine-tuning or re-training. First, we note that the motion flow correctly captures the pixels motion across frames in MCD dataset. Interestingly, we noticed that occasional inaccuracies in motion vectors positions can happen without impacting the final frame quality! This happens because the final frame is a composition of the motion flow $F_{m_i}$ and residual $R_{m_i}$. We notice that when the network fails to exactly recover the motion flow, the estimated residual $R_{m_i}$ compensates for it. 

Moving to the unseen modalities, the dynamics produced for the unseen modalities (Carpet and Soli) are significantly different from the reference modality (MCD) as shown in Figure \ref{fig:motion_flow}. For example, on Carpet,  the vectors magnitudes are much smaller than that of either Soli or MCD. Signifying the true fact that most pixels in the frames belong to the background. This suggests that NeuralPrefix learned transferable dynamics from MCD and didn't memorize to the seen dynamics. The dashed box superimposed on the motion flow frames indicates the position of the active object (human foot in Carpet, reflection from hand in Soli) in the original frames. Ideally, these regions should contain the highest magnitude of motion. While it is observable in Soli, it is less observable in Carpet due to the high noise in the background.



\begin{figure}
    \centering
    \includegraphics[width= 1\linewidth]
    {images/carpet_out_of_domain.pdf}
    % \vspace{-1em}
    \caption{\textbf{Zero-shot Imputation} on Intelligent \update{carpet} dataset using NeuralPrefix trained on Soli dataset. (Top) ground truth (bottom) data imputed by the model.}
    \label{fig:out_of_domain_carpet}
    % \vspace{-1em}
\end{figure}












% \begin{table*}[ht]
%     \centering
%     \caption{Imputation Performance}
%     \label{tab:imputation_performance}
%     \label{table:comparison}
%     \begin{tabular}{llcccccccc}
%         \toprule
%         \textbf{Datasets} & \textbf{Model} & \multicolumn{4}{c}{\textbf{Interpolation}} & \multicolumn{4}{c}{\textbf{Extrapolation}} \\
%         \cmidrule(lr){3-6} \cmidrule(lr){7-9}
%         & & \textbf{SSIM}$\uparrow$ &
%         \textbf{MSE}$\downarrow$ &
%         \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & 
%         \textbf{MSE}$\downarrow$ &
%         \textbf{LPIPS}$\downarrow$ & \textbf{PSNR}$\uparrow$ \\
%         \midrule
%         \multirow{2}{*}{\textbf{MCD}} 
%         & Baseline &  &  &  &  &  &  \\
%         & NeuralPrefix & 0.9399 & 0.0007 & 0.0752 & 31.8556  &  &  \\
%         \midrule
%         \multirow{2}{*}{\textbf{Soli}} 
%         & Baseline& 0.9497 & 0.0025 &  0.1524 & 26.1049 & & & & \\
%         & NeuralPrefix & 0.9817 &0.0006 & 0.0260 & 32.4366  &  &  &  &\\
%         \bottomrule
%     \end{tabular}
% \end{table*}


\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{images/dynamics_preview.png}
    \caption{\textbf{NeuralPrefix learned Motion Flow $F_m$ (Equation \ref{eq:composition})  generalizes to unseen modalities. }}
    \label{fig:motion_flow}
\end{figure}





% \section{Discussion}

% \begin{itemize}
%     \item Quantification of latency for ealry action prediction wasn't done in this work. The model as is can be optimized in straightforward manner by  optimizinng the forward evaluation of the ODE solver.
%     \item \textit{[consider dropping this after adding the relevant experiment]} Domain gap between the prefix and the host model wasn't studied in this work.  In our experiments, we trained the prefix and the task model on the same dataset. A better approach is to train the prefix on extensive dataset. This is similar to the standard practice of transfer learning in which a model is trained on .
% \end{itemize}


\subsection{Computationally Adaptive NeuralPrefix}
\label{sec:comp_adaptive}

\update{One interesting aspect of our architecture is its computational efficiency and elasticity. During the inference stage, NeuralPrefix takes an average of 124 milliseconds for processing a single sequence $\mathbf{X}$ when the window size $t$ is set to 10. For a window size of 20, it takes an average of 196 milliseconds. Additionally, NeuralPrefix offers a very natural way to trade off the accuracy and computational load without changing the current architecture. Thus, having the potential to adapt to the computational requirements of various ubiquitous devices.} Specifically, the ODESolve in NeuralPrefix can be expedited by limiting the number of iterations. This can be done very simply by changing the tolerance parameter which controls the accuracy of the ODE solution. It sets the acceptable error threshold that the numerical solver uses when approximating the solution at each time step. 


We experimented with the tolerance of the ODESolve in the Encoder and Decoder components. The default tolerance is set to $10^{-5}$. On the MCD dataset, when increasing it to $10^{-3}$  we gain a processing speedup by 20\% without impacting the quality of the frames (SSIM:0.9122 compared to 0.9399 in the default case). A major speedup of 50\% can be gained by setting the tolerance to 0.5. \update{However,} it comes at the cost of degraded quality of output (SSIM:0.8134). Note that this approach is more flexible than weight pruning or quantization \cite {cheng2024survey}, since the tolerance can be adjusted even after the model has been deployed.



\section{Discussion and Limitations}
\label{sec:dicussion}

% \AK{ Revisit this. Fix  references and broken parts}
% \textit{[talk about low performance with sensory data with non smooth dynamics. For example in Widar dataset we noticed lower performance. Note that the dataset is synthesized from distributed sensors using compressive sensing. Show some visual examples to support the claim.]}


In our effort to demonstrate the feasibility of \update{zero-shot} imputation through the development of NeuralPrefix, we learned a number of lessons. We highlight both limitations and opportunities that can inspire future research in this field. 

% \textit{Computationally adaptive NeuralPrefix.} One very interesting aspect of our architecture that we didnâ€™t explore here is computationally adaptation. NeuralPrefix offers a very natural way to trade off the accuracy and computational load without changing the current architecture. Making it very suitable for operating on ubiquitous devices. Specifically, the ODEsolver ( the bottleneck ) can be by limiting the number of iterations. 


\textit{Performance on Synthetic Measurements.} Our initial investigation on the Widar 3.0 dataset \cite{zheng2019zero} shows a decreased performance compared to the datasets considered in the evaluation. Visually, the cross-frame apparent dynamics are less smooth, with many abrupt changes. This is due to the fact that the frames are analytically synthesized (from multiple receivers) to approximate a body velocity profile (BVP) using compressive sensing techniques. This suggests that heavily pre-processed (or analytically synthesized) data are harder to learn from. 

\textit{Prior Integration.}  We noticed that the motion flow \update{vectors magnitudes} can be exaggerated for unseen modalities.  The current pipeline offers opportunities for integrating prior beliefs. One of which is regularizing the motion vector. For example, in the human sensing domain, \update{human} motion typically has a known range. This can be used analytically to estimate the maximum magnitude \update{of the} motion vectors and regularizing against it. Achieving this without re-training the whole model can be done by integrating adapter techniques \cite {kang2024sf} that are employed in unsupervised Test Time Adaptation (TTA). 


\update{\textit{Robusteness Evaluation}. While our method shows promising OOD results on the considered setup, further evaluation is needed to understand its behaviour and limitations in more challenging scenarios. For example, long missingness windows,  edge cases not represented in the training dataset, and novel missingness patterns \cite{qian2024unveiling}.} 










\section{Conclusion}

This work introduced NeuralPrefix; a first attempt to formalize and solve zero-shot sensory data imputation. The work demonstrated the practical feasibility of the idea and the utility by evaluating it on real datasets. We carefully designed NeuralPrefix to ensure flexibility of the imputation mode and generalization in unseen contexts.  Given this, the current system can enable many applications in the pervasive sensing domain such as modality compensation and early action prediction. The current work can be extended in a number of ways. For example, quantifying the latency of extrapolation for early action prediction and speeding up the model by  optimizing the forward evaluation of the ODE solver. 

% \appendix

% \subsection{Adjoint Sensitivity}
% \label{app:adjoint}


% Following \cite{chen2018neural}. 
% \textit{[details goes here]} 

% \subsection{Datasets and Implementation }
% \label{app:dataset}




\bibliographystyle{acm}
\bibliography{refs}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
