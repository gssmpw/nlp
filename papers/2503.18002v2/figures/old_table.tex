\begin{table*}[h]
\caption{Throughput and energy efficiency for two transformer-based language models running on the NVIDIA Jetson Orin Nano compared to our MatMul-free LM running on Intel's Loihi 2, across different sequence lengths for prefill and generation. Metrics for Loihi 2 are based on preliminary experiments and subject to further performance optimization, see Appendix \ref{app:l2-results-detailed}. \textbf{Gen}: autoregressive generation, \textbf{Prefill}: prefill mode.}
% new measurements: 71.2607 tokens/sec  @  4.23335W (0.2655 dyn + 0.1653 st)
% mJ/token: (265,5+165,3*24)/1000 * (1/71,26) * 1000 = 59.398
\centering
\begin{tabular}{cl|rrrr|rrrr}
\toprule
 && \multicolumn{4}{l}{Throughput ($\uparrow$ tokens/sec)} & \multicolumn{4}{l}{Efficiency ($\downarrow$ mJ/token)} \\
\midrule
 & Sequence length & 500 & 1000 & 2000 & 4000 & 500 & 1000 & 2000 & 4000 \\
\midrule
\multirow{3}{*}{\rotatebox{90}{Gen.}} &
% ./profiling/profiling_iclr-test-store-hicore-ap-24chip-tmp-ap01.html
\textbf{Ours (370M)} && \multicolumn{2}{c}{\textbf{41.5}} &&& \multicolumn{2}{c}{\textbf{405}} \\
% & \textbf{Ours (370M)} (est.) && \multicolumn{2}{c}{\textbf{41.5}} &&& \multicolumn{2}{c}{\textbf{405}} \\
% & \textbf{Ours (370M)} (1-chip) && \multicolumn{2}{c}{\textbf{71.3}} &&& \multicolumn{2}{c}{\textbf{59}} \\
% & \textbf{Ours (370M)} (24-chip) && \multicolumn{2}{c}{\textbf{42.2}} &&& \multicolumn{2}{c}{\textbf{468}} \\
& Ours (370M) on H100 & - & 13.3 & - & - & - & 10074 & - & - \\
& TF++ (370M) on H100 & - & 22.9 & - & - & - & 5624 & - & - \\
& Llama series (400M) & 14.3 & 14.9 & 15.0 & 14.7 & 723 & 719 & 751 & 853 \\
& Qwen2 (500M) & 13.4 & 14.0 & 14.1 & 14.1 & 791 & 785 & 816 & 912 \\
\midrule
% Prefill &&&&&&&& \\
% \midrule
\multirow{3}{*}{\rotatebox{90}{Prefill}} &
% \textbf{Ours (370M)} & \textbf{13965} & \textbf{13965} & \textbf{13965} & \textbf{13965} & \textbf{2.8} & \textbf{2.8} & \textbf{2.8} & \textbf{2.8} \\
\textbf{Ours (370M)} && \multicolumn{2}{c}{\textbf{6632}} &&& \multicolumn{2}{c}{\textbf{3.7}} \\
& Ours (370M) on H100 & - & 13066 & - & - & - & 5.3 & - & - \\
& TF++ (370M) on H100 & - & 32715 & - & - & - & 7.3 & - & - \\
% & \textbf{Ours (370M)} (est.) && \multicolumn{2}{c}{\textbf{6632}} &&& \multicolumn{2}{c}{\textbf{3.7}} \\
% & \textbf{Ours (370M)} (1-chip) && \multicolumn{2}{c}{\textbf{13965}} &&& \multicolumn{2}{c}{\textbf{2.8}} \\
% & \textbf{Ours (370M)} (24-chip) && \multicolumn{2}{c}{\textbf{6632}} &&& \multicolumn{2}{c}{\textbf{3.7}} \\
& Llama-series (400M) & 849.4 & 1620 & 2858 & 3153 & 11.7 & 7.8 & 5.8 & 6.8 \\
& Qwen2 (500M) & 627 & 909 & 1514 & 2639 & 17.9 & 13.9 & 9.5 & 6.7 \\
\bottomrule
\multicolumn{9}{p{12.5cm}}{\tiny$^*$ The MatMul-free LM on Loihi 2 was characterized on an Oheo Gulch single-chip Loihi 2 system (N3C1 silicon) running NxKernel v0.2.0 and NxCore v2.5.8 (accessible to Intel Neuromorphic Research Community members).
\par
$^\ddagger$ Transformer LMs were characterized on NVIDIA Jetson Orin Nano 8GB using the MAXN power mode running Jetpack 6.2, TensorRT 10.3.0, CUDA 12.4. Energy values include CPU\_GPU\_CV, SOC, and IO components as reported by jtop 4.3.0.
\par
Performance results are based on testing as of Jan 2025 and may not reflect all publicly available security updates. Results may vary.
}
\end{tabular}
\label{tab:jetson-loihi-comp}
\end{table*}