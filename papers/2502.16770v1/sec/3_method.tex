

\section{Methodology}
\paragraph{Preliminaries.}
We primarily focus on the homologous model merging, in which $\boldsymbol{\theta}_i$ all come from the same base model $\boldsymbol{\theta}_{\rm{base}}$. Given $K$ tasks $\{T_1,T_2,\cdots,T_K\}$ and $K$ corresponding fine-tuned models with parameters $\{\boldsymbol{\theta}_1,\boldsymbol{\theta}_2,\cdots,\boldsymbol{\theta}_K\}$, model merging aims to combine $K$ fine-tuned models into one single model simultaneously performing on $\{T_1,T_2,\cdots,T_K\}$ without post-training~\cite{method_p1_1,method_p1_2}.
Task vector~\cite{ilharco2023editing,yang2024adamerging} is a key element in merging method which could enhances the base modelâ€˜s ability or enable the model to handle other tasks. Specifically, for task $T_i$, the task vector $\boldsymbol\tau_i\in \mathbb{R}^D$ is defined as the vector obtained by subtracting the SFT weights $\boldsymbol{\theta}_i$ from the base model weight
$\boldsymbol{\theta}_{\rm{base}}$, \emph{i.e.}, $\boldsymbol\tau_i=\boldsymbol{\theta}_i-\boldsymbol{\theta}_{\rm{base}}$. The merged model could be denoted as $\boldsymbol{\theta}_m=\boldsymbol{\theta}_{\rm{base}}+\sum_i \lambda_i\boldsymbol{\tau}_i$, which $\lambda_i$ is the scaling factor measuring the importance of task vector. For clarification, we also denote the neuron set in $\boldsymbol{\theta}_i$ as $\mathcal{N}_i$, the neuron set in $\boldsymbol{\tau}_i$ as $\mathcal{T}_i$.


\input{algs/LED}
\input{figures/pipeline}

\paragraph{LED-Merging: Location, Election, and Disjoint Merging}
To address the neuron misidentification and interference issues in existing model merging methods, we propose LED-Merging (Location, Election, and Disjoint Merging). Specifically, previous studies \cite{modelstock, ilharco2023editing, tiesmerging} fail to accurately identify safety-related neurons in task vectors with a single magnitude score, namely \textit{neuron misidentification}. Meanwhile, there exists an interference between safety-related and utility-related task vector neurons during the merging process, namely \textit{neuron interference}. To address neuron misidentification, we first locate important neurons both in the base and fine-tuned models and then elect neurons from the task vector considering these two scores together. Subsequently, to mitigate the interference, we introduce a disjoint step, isolating these important neurons so that they influence different base neurons. The whole process is illustrated in Figure~\ref{fig:method}. 




In the location and election step, we consider the importance score from base and fine-tuned models simultaneously to locate task-specific neurons. In this way, it is more accurate than relying on the magnitude score alone because task-specific neurons with high importance score in the fine-tuned model may not necessarily score high in the base model, and vice versa.

{\textbf{Location}}.  We first calculate importance scores for each neuron in a base/fine-tuned model. Given a location dataset $\mathcal{X}_i=\{(x,y)_k\}$, where $x$ is the question and $y$ is the answer, we calculate the importance scores for the weight $\boldsymbol{\theta}_i\in\mathbb{R}^D$ in any  layer as follows~\cite{snip,spareseGPT,sun2024a}:
\begin{equation}
    I(\boldsymbol{\theta}_i)=\mathbb{E}_{x\sim \mathcal{X}_i}[\boldsymbol{\theta}_i\odot \nabla _{\boldsymbol{\theta}_i}\mathcal{L}(x)],
    \label{location}
\end{equation}
which $\mathcal{L}(x)=-\log p(y\mid x)$ is the conditional negative log-likelihood loss. We choose the SNIP score~\cite{snip} because it balances computational efficiency and performance~\cite{cq}. Please refer to Sec.~\ref{sec:ablation} for the comparison between different location methods. After computing importance scores, we choose top-$r_i$ neurons as the important neuron subset $\mathcal{N}_{i}^{r_i}$ from $I(\boldsymbol{\theta}_i)$.
 
 % After computing locating scores, we select the neurons scoring both high in base and fine-tuned models as important neurons in task vectors. Then in the disjoint step,  with preventing  polysemantic neurons  from receiving gradient updates towards different directions,
 % we use set difference to isolate the safety   and utility-related neurons  and construct corresponding masks for merging process,

{\textbf{Election}}. A natural question is how to select important neurons in the task vector $\boldsymbol{\tau}_i$ based on $I(\boldsymbol{\theta}_{\rm{base}})$ and $I(\boldsymbol{\theta}_{i})$. The important neurons in the base model may be different from neurons in the fine-tuned model. Therefore, we introduce the following election strategy to select neurons with high scores in both base and fine-tuned models:
\begin{equation}
    \mathcal{T}_i^{r_i}=\mathcal{N}_i^{r_i}\cap \mathcal{N}_{\rm{base}}^{r_i}.
    \label{vote}
\end{equation}
\emph{Remark}. We compare different choosing methods, including scoring low or high in base or fine-tuned model in Section~\ref{sec:ablation} and find that Equation \ref{vote} achieves the best performance.





{\textbf{Disjoint}}. As important neurons from different task vectors may conflict with each other at the same position, we use the set difference to disjoint the neurons from others to prevent interference:
\begin{equation}
    \text{Disjoint}(\mathcal{T}^{r_i}_{i})=\mathcal{T}^{r_i}_{i}-\mathop{\cup}\limits_{{J}\subsetneqq [K],|J|\geq 2}\mathop{\cap}\limits_{j\in {J}}\mathcal{T}^{r_j}_{j}.
    \label{disjoint_safety}
\end{equation}

Next, we construct a mask $\boldsymbol{m}_i\in\mathbb{R}^D$ to implement disjoint in the merging process. Specifically, this mask $\boldsymbol{m}_i$ is used to select neurons from $\mathcal{T}_i$. The mask ratio is $r_i$, where $r\in(0,1]$. The mask $\boldsymbol{m}_i$ can be derived from:
\begin{equation}
    \boldsymbol{m}_{i,d}=\begin{aligned} &\left\{ \begin{array}{ll} 1, & \text{if } d\in \text{Disjoint}(\mathcal{T}_{i}^{r_i}), \\ 0, & \text{otherwise}. \end{array} \right. \end{aligned}
    \label{mask_safety}
\end{equation}


% \subsection{Merging Models with Masks}
{\textbf{Merging}}. The final
merged task vector $\boldsymbol{\tau}_m$ is as follows:
\begin{equation}
    \boldsymbol{\tau}_m= \sum_i \lambda_i\boldsymbol{\tau}_{i}\odot\boldsymbol{m}_i.
    \label{merged_task_vector}
\end{equation}
We summarize the workflow in Algorithm \ref{alg1}.


