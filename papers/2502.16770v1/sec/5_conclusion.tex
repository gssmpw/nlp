%\section{Discussion}
% \paragraph{Visualization of Neruons.}
% As shown in Fig.~\ref{fig:}, we visualize the 


%\paragraph{Merging Impairs Instruction Following.}
%\label{sec:discussion_impair}
%Conventional merging methods possibly degrade instruction-following capabilities, particularly in safety-aligned models. For instance, merging WizardLM-13B (LM) with llama-2-13b-code-alpaca (Code) using Model Stock results in an abnormally low HarmBench ASR~(3.25), shown in Tab.~\ref{tab:llms_merging_llama2}, but this ostensibly "strong safety" stems from catastrophic over-suppression: the merged model fails to follow benign instructions entirely (e.g., MBPP Pass@1 drops to 6.20).

%\paragraph{Merging Results in Confused Outputs.}
%Existing methods frequently produce incoherent or repetitive outputs due to unmitigated neuron interference. As shown in Tab.~\ref{tab:llms_merging_llama3}, Breadcrumbs achieves high GSM8K accuracy(83.01\%) when merging LM, Math, and Code models but generates nonsensical repetitions, such as \textit{duplicating phrases like "the answer is 42" regardless of input}, rendering outputs practically unusable despite numerical correctness. Similarly, Ties-Merging on Llama-2-13B~(shown in Tab.~\ref{tab:llms_merging_llama2}) yields inconsistent code generation with erratic syntax, as conflicting neurons overwrite coherent programming patterns. 



\section{Conclusion}
In this paper, we propose LED-Merging, a training-free framework to address the critical safety-utility conflicts inherent in model merging for LLMs. 
By integrating gradient-based neuron localization, dynamic importance election, and parameter space isolation, our method achieves robust safety alignment, LED-Merging achieves robust safety alignment~(e.g., 75.9\% reduction in harmful responses for code-specialized models) while preserving task performance~(e.g. 52.39\% GSM8K accuracy). Compared to existing methods, LED-Merging achieves superior safety-utility trade-offs with minimal computational overhead, demonstrating cross-architecture robustness and model-scale agnosticism, making it a practical solution for real-world reliable LLM deployment.

% By introducing three stages, including Location, Election, and Disjoint Merging, LED-Merging dynamically identifies safety-critical neurons, resolves cross-task interference, and preserves task-specific utility without additional alignment training. Extensive experiments on Llama3-8B, Mistral-7B, and Llama2-13B demonstrate its effectiveness. Specifically, LED-Merging reduces safety vulnerabilities by 9.44\% on HarmBench while retaining over 95\% of original task performance on mathematical and code generation benchmarks. Compared to existing methods, LED-Merging achieves superior safety-utility trade-offs with minimal computational overhead, making it a practical solution for real-world reliable LLM deployment.


\section{Limitations and Future Work}
While our focus is on homologous model merging, extending this framework to heterogeneous architectures (e.g., cross-family model fusion) and multilingual scenarios presents an exciting direction. Additionally, exploring tokenization divergence effects in multilingual models could uncover new safety-utility dynamics. We advocate for community efforts to establish standardized benchmarks for merged model evaluation, ensuring transparency and reproducibility in this rapidly evolving field.

\section{Broader Impact and Ethics Statement}
This research tackles the pivotal challenge of balancing safety alignment and functional utility in large language models (LLM) merging techniques. Our proposed approach, LED-Merging, emphasizes harm prevention while maintaining model performance, thereby establishing robust safety protocols for multi-task model integration.
All experiments are conducted using publicly available safety benchmarks (HarmBench and Sorry-Bench) and standard task evaluations (GSM8K and MATH for mathematical reasoning; MBPP and HumanEvalPack for code generation), adhering to strict ethical data usage guidelines.
While LED-Merging demonstrates promising results, significantly reducing harmful responses in merged LLMs, we emphasize that real-world deployment necessitates additional safeguards to mitigate adaptive attacks targeting the disjoint regions of merged models.