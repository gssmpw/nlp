
\appendix

\newpage
\section{Experiment Details}

\subsection{Model Merging Baselines}
\label{sec:appendix_baselines}
\begin{itemize}
    \item \textbf{Model Stock}~\cite{modelstock} averages layer-wise weights from two fine-tuned models to enhance performance on both in-distribution and out-of-distribution tasks.
    \item \textbf{Model Breadcrumbs}~\cite{breadcrumbs} sparsifies the differences of task vectors and integrates them back into the pre-trained model to efficiently construct a multi-task model without the need for hyperparameter tuning for each new task.
    \item \textbf{Task Arithmetic}~\cite{ilharco2023editing} scales and then adds the task vectors to the initial model to produce the merged model.
    \item \textbf{Ties-Merging}~\cite{tiesmerging} trims redundant parameters from task vectors by keeping the top-$k\%$ values according to their magnitude and elects neurons that agree with their major sign direction.
\end{itemize}

\subsection{Datasets and Evaluation Metrics}
\label{sec:appendix_dataset}
We select safety benchmarks and utility tasks to evaluate the safety-utility trade-off comprehensively. For safety evaluation, we choose HarmBench~\cite{harmbench} and SORRY-Bench~\cite{xie2024sorrybench}, and employ attack success rate(\textit{ASR}$\downarrow$) as primary metrics based on expert annotations. For mathematical reasoning, we evaluate on GSM8K~\cite{gsm8k} and MATH~\cite{hendrycksmath2021} using \textit{Accuracy}$\uparrow$ with chain-of-thought reasoning verification. Code generation capabilities are measured through MBPP~\cite{mbpp} (Python programming tasks) and HumanEvalPack~\cite{humanevalpack} (extended to code repair and explanation), adopting \textit{Pass@1}$\uparrow$ evaluation with test-case verification.

\subsection{Details of SFT Models and Corresponding Pretrained Models}
\label{sec:sft-models-info}
For the Llama-3 series, we choose Llama-3-8B~\cite{llama3} as the base model, Llama-3-8B-Instruct~\cite{llama3-instruct} as safety model, MAmmoTH2-8B-Plus~\cite{yue2024mammoth2} as math model and Replete-Coder-Llama3-8B as code model. For Wizard-LM series, we choose WizardLM-13B~\cite{xu2024wizardlm}, WizardMath-13B~\cite{luo2023wizardmath} and llama-2-13b-code-alpaca~\cite{touvron2023llama2} to conduct the experiments. For the Mistral series, we choose Mistral-7B~\cite{jiang2023mistral7b} as the base model, Mistral-7B-Instruct as the safety model, and MetaMath-Mistral-7B as the math model. 
Tab.~\ref{tab:llms_SFT_backbone_correspondences} shows the versions of SFT LLMs and corresponding pre-trained backbones.


\subsection{Details of Hyperparameters' Ranges for Merging Baselines}
\label{sec:hyperparam-baselines}

Tab.~\ref{tab:hyperparameter_baselines} demonstrates the recommended ranges of hyperparameters of model merging approaches.




\section{Additional Experiment Results}
\subsection{Cases of LLMs’ Impaired Instruction Following Ability}
\label{sec:appendix_impair_Instruct_following}

As discussed in Sec.~\ref{sec:discussion_impair}, 
Tab.~\ref{tab:llms_merging_llama2} shows that Model Stock merges WizardLM-13B (LM) and LLama-2-13B-Code-Alpaca (Code) results in a LLM with extremely low instruction following ability. Specifically, the merged model fails to follow common instructions entirely and the performance on MBPP Pass@1 drops to 6.20. In this way, evaluating the safety ability of the merged LLM is unnecessary, because it refuses to answer anything queries and achieves a superficial safety performance. 
In this section, we present some cases in Fig.~\ref{fig:impair_inst} in which the merged model fails to follow human beings' instructions. As shown in Fig.~\ref{fig:impair_inst}, the merged model outputs the Instruction Template, followed by a confused answer or harmful responses. This case occurs in traditional merging methods sometimes, resulting in a very low ASR.
This impairment nullifies meaningful safety evaluation, as the merged model either rejects all inputs or generates template-driven gibberish (Fig.~\ref{fig:impair_inst}), artificially inflating safety metrics through non-responsiveness.

\input{figures/impair_instruction}

\subsection{Cases of LLMs’ Impaired Structured Response Ability}
\label{sec:appendix_impair_structed_response}
As discussed in Sec.~\ref{sec:discussion_impair}, existing model merging methods frequently produce incoherent or repetitive outputs due to unmitigated neuron interference. Tab.~\ref{tab:llms_merging_llama3} shows that Breadcrumbs merges Llama3-8B-Instruct (LM), MAmmoTH2-8B-Plus (Math), and Replete-Coder-
Llama3-8B (Code), but generates nonsensical repetitions, such as \textit{duplicating phrases like "$\text{\#\#\#\#\#\#\#}$ 2 weeks." regardless of input}, rendering outputs practically unusable despite numerical correctness.
Similarly, Ties-Merging on Llama-2-13B~(shown in Tab.~\ref{tab:llms_merging_llama2}) yields inconsistent code generation with erratic syntax, as conflicting neurons overwrite coherent programming patterns. 
In this section, we present some cases in Fig.~\ref{fig:impair_structure} in which the merged model fails to output a structured response. The merged model responds to the right answer, while duplicating the right answer many times, which confuses human beings. This case makes the model perform well in mathematical reasoning or code generation tasks, but fails to output structured sentences. 
\input{figures/impair_structure}



\input{tabs/tab-sft-model-versions}
\input{tabs/tab-hyperparams_baselines}