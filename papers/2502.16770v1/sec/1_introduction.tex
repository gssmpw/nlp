\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks~\cite{gpt3,gpt4,cai2024internlm2,touvron2023llama2, qwen}. Although post-training is widely used to improve LLMs' performances on downstream tasks, training task-specific models for different tasks leads to significant storage and training costs. To this end, \emph{model merging}~\cite{model_soup,evo_merge,xiao-etal-2024-lm}, a training-free technique that combines parameters from multiple fine-tuned models into a unified model, has emerged as a promising solution. 

Previous research has shown that merging methods can lead to \textit{safety-utility conflicts}, where improvements in general ability (\emph{e.g.}, mathematical reasoning) degrade safety safeguards~\cite{hammoud-etal-2024-model}. For instance, merging safety-aligned and math-specific fine-tuned models get an unsafe mathematical AI expert~(left conversation in Fig.~\ref{fig:intro-final}\textcolor{darkblue}{a}), reducing safety capabilities by over 30\%, as shown in Fig.~\ref{fig:intro-final}\textcolor{darkblue}{b}. To address this problem, additional alignment training has been employed to improve the safety capabilities of the merged model~\cite{safetymerge1,aakanksha2024mix}. However, such consequential safety-specific training requires labeled data and training costs, limiting their applicability in privacy-sensitive or resource-constrained scenarios. More critically, these methods address symptoms rather than root causes—they neither analyze neuron-level conflicts nor resolve interference mechanisms.

\input{figures/intro-fig1}

The \textit{safety-utility conflicts} stem from two fundamental limitations in existing methods: (i) \emph{Neuron misidentification:} Previous merging methods rely on simplistic metrics like parameter magnitude to select neurons, failing to distinguish safety-related regions from LLMs and impair safety capacity (ii) \emph{Neuron interference:} Neurons optimized for different tasks (\emph{e.g.}, safety and code generation) exhibit antagonistic updates during merging, causing destructive parameter collisions and severely reduced performance, as shown in Fig.~\ref{fig:intro-final}\textcolor{darkblue}{b}, and Fig.~\ref{fig:intro-final}\textcolor{darkblue}{c}.

%Existing solutions~\cite{safetymerge1,aakanksha2024mix}, require extensive labeled data and costly retraining, limiting their applicability in privacy-sensitive or resource-constrained scenarios. More critically, these methods address symptoms rather than root causes—they neither analyze neuron-level conflicts nor resolve interference mechanisms, relying instead on heuristic post-processing, \emph{e.g.,} hyper-parameter optimization. 

In this paper, we propose LED-Merging, a simple and effective merging method to address the above problems. Specifically, LED-Merging has three steps, including \textbf{L}ocation, \textbf{E}lection, and \textbf{D}isjoint Merging. For the \textbf{Location}, LED-Merging identifies critical neurons in both base and fine-tuned models using gradient-based attribution scores to avoid \textit{neuron misidentification}. For the \textbf{Election}, LED-Merging dynamically elects safety-critical neurons by fusing importance signals across different models, ensuring the balanced representation of safety and utility. For the \textbf{Disjoint Merging}, LED-Merging isolates conflicting weight updates via set difference operations, preventing interference between safety and task-specific neurons to avoid \textit{Cross-tasks interference}. The overall workflow is illustrated in Fig.~\ref{fig:method}.

% To address these safety deterioration resources, we propose LED-Merging, a simple and effective merging method by combining task vectors that has three steps, namely, \textbf{L}ocation-\textbf{E}lection-\textbf{D}isjoint Merging. For misidentification, we propose a location-election step to dynamically elect candidate neurons by fusing the general and unique scores from base and fine-tuned models, thereby potentially eliciting the critical neurons of the merged model beyond those of any merging method with single score. Specifically, we retain only the influential neurons across base and fine-tuned models. After this step, interference may still persist among these neurons. Our disjoint step therefore resolves interference between different neurons and our last step only average neurons which agrees with the corresponding fine-tuning domain. 


To empirically evaluate the effectiveness of LED-Merging, we conduct extensive experiments comparing it with existing model merging methods in distinct model sizes and families, such as Llama-3-8B~\cite{llama3}, Llama2-13b~\cite{touvron2023llama2}, and Mistral-7B~\cite{jiang2023mistral7b}. First, experimental results on two representative safety benchmarks, including HarmBench~\cite{harmbench} and SORRY-Bench~\cite{xie2024sorrybench} indicate that LED-Merging achieves a strong resilience in safety domain tasks while preserving
 fine-tuning performance on utility
domains, \emph{e.g.}, improving the Llama-3-8B-Instruct's safety score by 31.4\%  and WizardLM-13B's safety score by 70.8\% on HarmBench.  Second, LED-Merging could locate safety neurons in task vectors more accurately through a dynamic election strategy.

Our contributions are summarized as: (1) We design a fusion strategy to collaborate with base and fine-tuned models to identify safety neurons in task vectors, addressing the shortcomings of existing magnitude-based methods. (2) We propose a training-free merging method called LED Merging, which mitigates safety-utility conflicts without post-training on annotated alignment data.  (3) Extensive experiments on safety, math and code benchmarks demonstrate its effectiveness in various safety-utility merging scenarios. LED-Merging ensures the safety of the responses while preserving proprietary capabilities (mathematical, code) as much as possible. 

