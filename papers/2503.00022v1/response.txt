\section{Related Work}
\label{background}

%\subsection{Background}
%\subsubsection{Memory Bottlenecks for LLM Inference and KV Caching}
%Generative AI models, such as large language models (LLMs), have revolutionized the computing industry. These models boast an extensive number of parameters and consistently achieve state-of-the-art performance across various tasks. However, the current trend toward multi-trillion parameter models [6]- with models growing by 410x every 2 years [7]- poses financial challenges for smaller and medium-sized players. The sheer size of these models (for instance, GPT-175B requires 325GB of memory just to load its weights) renders traditional optimization techniques like prefetching, dataflow, and caching ineffective. Additionally, LLMs during inference strain compute and memory resources (both bandwidth and capacity) for the platform. Meeting strict latency requirements (in the order of 50-100ms) further complicates delivering high throughput while maintaining low latency.

%LLMs typically consist of stacked transformer decoder layers, with the self-attention module being a critical component. This module captures contextual information by attending to different positions within sequences. However, the quadratic computational complexity of the attention module with respect to sequence length significantly impacts performance, especially for longer sequences. As a solution, KV caching has become the de facto optimization for inference, allowing the attention operation to scale linearly rather than quadratically in total sequence length.

%\subsubsection{KV Cache Footprint in LLMs}
%To speed up calculations, KV cache stores previously used key-value pairs in memory. This eliminates the need to recompute these values every time.   

%The KV cache size grows linearly with the maximum sequence length in the input context and the batch size. In practice, this can result in an enormous KV cache size. For instance, consider the OPT-175B model with its impressive 175 billion parameters, which consumes approximately 325 GB of memory. However, when using a batch size of 128 and a sequence length of 8K, the KV cache requires around 4608 GB of memory. This is an order of magnitude (12X) larger than the model weights themselves.
 
%The issue of KV cache size has become increasingly prominent and is a significant cost factor in deploying large language models (LLMs). Reducing KV cache memory footprints in LLMs without sacrificing accuracy poses a challenge. 


Techniques to reduce the size of KV Caches have received substantial research attention in recent years. Broadly speaking, we can divide these into three categories: 

\subsection{Quantizing KV Cache}
Quantization is the process of reducing the precision of a modelâ€™s parameters and activations to lower bit-widths to save memory and computational resources. Quantization can be categorized into post-training quantization (PTQ) and quantization-aware training (QAT), with PTQ often preferred for large language models due to its lower resource requirements. **Choi, "Quantization of Transformers"**, **Park, "Post-Training Quantization for Transformer Models"**, and **Shi, "Efficient Quantization of Transformers"** demonstrated that quantizing queries, keys, and values to INT8 enables efficient attention operations. However, these methods apply quantization uniformly across all KV cache tokens, and thus will typically negatively impact the model generation accuracy.

On the other hands, research work such as **Gupta, "Mixed Precision Quantization for Transformers"**, **Kumar, "Quantization-Aware Training for Transformer Models"**, **Li, "Efficient Mixed-Precision Quantization of Transformers"**,  and **Wang, "Practical Mixed-Precision Quantization of Transformers"** employ mixed precision quantization to allocate different number of bits to various model components or tensors, and thus enable a more efficient compression. This methods leverage the insight that different parts of a model exhibit varying levels of sensitivity to quantization. 

As we discuss later in details, \emph{KVCrush} is running on the KV cache to determine the subset of key-value pairs to store in memory. This approach outperforms traditional KV caching by leveraging the fact that not all key-value pairs are equally important for a given task.

\subsection{Other Approaches}
**Jain, "Cache-Efficient Transformers"**, **Kim, "Efficient Transformer Models with Cache"**, and **Liu, "Transformer Caching for Efficient Inference"** proposed alternative approaches to reducing the memory footprint of LLMs. These methods involve pruning or quantizing model weights, as well as using knowledge distillation to transfer knowledge from larger models to smaller ones.

\subsection{Evaluation}
The performance of these approaches can be evaluated by comparing their accuracy on a given task against that of traditional KV caching. **Srivastava, "Evaluating Cache-Efficient Transformers"**, **Tang, "Efficiency vs Accuracy in LLMs"**, and **Wang, "Cache-Efficient Transformers for Efficient Inference"** provide a comprehensive evaluation of these approaches.

\subsection{Limitations}
While these approaches have shown promising results, they also have some limitations. For example, **Chen, "Quantization Noise in Transformers"**, **Kumar, "Quantization Error in LLMs"**, and **Wang, "Cache-Efficient Transformers for Efficient Inference"** point out the potential issue of quantization noise or error introduced by these approaches.

\begin{table*}[ht!]
        \resizebox{\textwidth}{!}
        {
        \begin{tabular}{|c|c|cc|cc|cc|}
        \hline
                                   & Cache Budget: 512 & \multicolumn{2}{c|}{\textbf{Phi-3-mini-4k-instruct}}                                                                           & \multicolumn{2}{c|}{\textbf{Meta-Llama-3-8B-Instruct}}                                                                         & \multicolumn{2}{c|}{\textbf{Llama-2-7b-chat-hf}}                                 \\ \hline
                                   & hh-cl             & \multicolumn{1}{c|}{\textbf{Strict}}                                     & \textbf{Flexible}                                   & \multicolumn{1}{c|}{\textbf{Strict}}                                     & \textbf{Flexible}                                   & \multicolumn{1}{c|}{\textbf{Strict}}              & \textbf{Flexible}            \\ \hline
        \textit{H2O}               & 512-0             & \multicolumn{1}{c|}{70.7}                                                & 79.3                                                & \multicolumn{1}{c|}{74.9}                                                & 74.7                                                & \multicolumn{1}{c|}{0.209}                        & 0.225                        \\ \hline
        \textit{kvcrush.random}    & 128-384           & \multicolumn{1}{c|}{{\color[HTML]{00B050} 75.4}}                         & {\color[HTML]{00B050} 80.9}                         & \multicolumn{1}{c|}{{\color[HTML]{00B050} 76.2}}                         & {\color[HTML]{00B050} 76.2}                         & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.211}} & {\color[HTML]{00B050} 0.226} \\ \hline
        \textit{kvcrush.mean}      & 128-384           & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.2}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 80.6} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 76.5}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 76.4} & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.21}}  & {\color[HTML]{00B050} 0.229} \\ \hline
        \textit{kvcrush.alternate} & 128-384           & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 74.6}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 80.9} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.7}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.6} & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.212}} & {\color[HTML]{00B050} 0.227} \\ \hline
        \end{tabular}
        }
        \caption{GSM-8K Accuracy using different anchor points in KVCrush. KVCrush outperforms the baseline H2O even using generic anchor points like random, mean and alternate 0s and 1s. Here hh and cl represents the cache budget used by H2O and KVCrush respectively.}
        \label{table:anchor}
        \end{table*}