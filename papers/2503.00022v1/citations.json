[
  {
    "index": 0,
    "papers": [
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2024llmqat",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sheng2023flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y. and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E. and Liang, Percy and R{\\'e}, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mq1",
        "author": "Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song",
        "title": "HAQ: Hardware-Aware Automated Quantization with Mixed Precision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mq2",
        "author": "Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W. and Keutzer, Kurt",
        "title": "HAWQ-V3: Dyadic Neural Network Quantization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mq3",
        "author": "Chauhan, Arun and Tiwari, Utsav and Vikram, N R",
        "title": "Post Training Mixed Precision Quantization of Neural Networks Using First-Order Information"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "mikv",
        "author": "Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo",
        "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mqa",
        "author": "Shazeer, Noam",
        "title": "Fast Transformer Decoding: One Write-Head is All You Need"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "gqa",
        "author": "Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr\u00f3n, Federico and Sanghai, Sumit",
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "streamingllm",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "{H2O}: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "scissorhands",
        "author": "Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali",
        "title": "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "fastgen",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snapkv: Llm knows what you are looking for before generation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pyramidkv",
        "author": "Zhang, Yichi and Gao, Bofei and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and Xiao, Wen and others",
        "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling"
      }
    ]
  }
]