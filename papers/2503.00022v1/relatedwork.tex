\section{Related Work}
\label{background}

%\subsection{Background}
%\subsubsection{Memory Bottlenecks for LLM Inference and KV Caching}
%Generative AI models, such as large language models (LLMs), have revolutionized the computing industry. These models boast an extensive number of parameters and consistently achieve state-of-the-art performance across various tasks. However, the current trend toward multi-trillion parameter models [6]- with models growing by 410x every 2 years [7]- poses financial challenges for smaller and medium-sized players. The sheer size of these models (for instance, GPT-175B requires 325GB of memory just to load its weights) renders traditional optimization techniques like prefetching, dataflow, and caching ineffective. Additionally, LLMs during inference strain compute and memory resources (both bandwidth and capacity) for the platform. Meeting strict latency requirements (in the order of 50-100ms) further complicates delivering high throughput while maintaining low latency.

%LLMs typically consist of stacked transformer decoder layers, with the self-attention module being a critical component. This module captures contextual information by attending to different positions within sequences. However, the quadratic computational complexity of the attention module with respect to sequence length significantly impacts performance, especially for longer sequences. As a solution, KV caching has become the de facto optimization for inference, allowing the attention operation to scale linearly rather than quadratically in total sequence length.

%\subsubsection{KV Cache Footprint in LLMs}
%To speed up calculations, KV cache stores previously used key-value pairs in memory. This eliminates the need to recompute these values every time.   

%The KV cache size grows linearly with the maximum sequence length in the input context and the batch size. In practice, this can result in an enormous KV cache size. For instance, consider the OPT-175B model with its impressive 175 billion parameters, which consumes approximately 325 GB of memory. However, when using a batch size of 128 and a sequence length of 8K, the KV cache requires around 4608 GB of memory. This is an order of magnitude (12X) larger than the model weights themselves.
 
%The issue of KV cache size has become increasingly prominent and is a significant cost factor in deploying large language models (LLMs). Reducing KV cache memory footprints in LLMs without sacrificing accuracy poses a challenge. 


Techniques to reduce the size of KV Caches have received substantial research attention in recent years. Broadly speaking, we can divide these into three categories: 

\subsection{Quantizing KV Cache}
Quantization is the process of reducing the precision of a modelâ€™s parameters and activations to lower bit-widths to save memory and computational resources. Quantization can be categorized into post-training quantization (PTQ) and quantization-aware training (QAT), with PTQ often preferred for large language models due to its lower resource requirements. \cite{xiao2023smoothquant}, \cite{liu2024llmqat}, and \cite{sheng2023flexgen} demonstrated that quantizing queries, keys, and values to INT8 enables efficient attention operations. However, these methods apply quantization uniformly across all KV cache tokens, and thus will typically negatively impact the model generation accuracy.

On the other hands, research work such as \cite{mq1}, \cite{mq2}, \cite{mq3}, \cite{mikv} employ mixed precision quantization to allocate different number of bits to various model components or tensors, and thus enable a more efficient compression. This methods leverage the insight that different parts of a model exhibit varying levels of sensitivity to quantization. 

As we discuss later in details, \emph{KVCrush} is running on the KV cache to determine the subset of tokens to retain or to evict. As a result, it is complementary to quantization technology used. The (mixed)-precision used to represent each token can be easily combined with KV-crush by assigning different precision bits to the set of retained tokens based on the "importance" of the token as we will explain. 

\subsection{Sharing KV Cache}
Multi-Query Attention (MQA)  and Grouped Query Attention (GQA)  are techniques developed to address the memory footprint issues LLMs allowing to share KV caches across heads. MQA, introduced by \citep{mqa}, reduces memory usage by sharing key and value representations across all attention heads, which enhances memory efficiency and inference speed, at the expense of generation quality. GQA, proposed by \citep{gqa}, extends this concept by grouping query heads to share KVs, balancing memory reduction with better performance retention. However, GQA involves higher training costs and complexity. Both methods offer trade-offs between memory efficiency and model performance, with MQA favoring memory savings and GQA providing a more balanced approach. 

Intuitively, \emph{KVCrush} is completely orthogonal to these sharing schemes of the KV cache. Our technology is applicable to the deployed KV cache irrespective of whether this cache is shared and grouped across attention heads or not. 

\subsection{Evicting inconsequential Keys and Values} \label{related:kv_eviction}
This category is the closest related work to \emph{KVCrush}. In this research category  different methods aim to prune key-value (KV) pairs from cache after input processing, aiming to enhance decoding efficiency. By evicting tokens out of KV cache, memory consumption is reduced, facilitating support for larger batch sizes and longer context windows. 

Different strategies for KV pruning and selectively dropping tokens from the KV cahce have been proposed in recent research work, For example, In StreamLLM~\cite{streamingllm}, only the most recent tokens and attention sinks (first few tokens) are retained. H2O \cite{h2o} and Scissorhands \cite{scissorhands} utilize attention-based metrics to determine eviction, with H2O summing attention weights and Scissorhands considering the frequency of attention surpassing a threshold. FastGen \cite{fastgen} combines these methods with heuristics for special tokens. SnapKV~\cite{snapkv} similarly depends on attention weights to prune the KV cache, but addresses the efficiency during prefill with long context, and due to the complexity of computing attention score for the whole context, it limits the observation window to the final tokens of the input prompt, thus reducing the complexity from $O(L^2)$ to $O(L)$, where $L$ is the context length, while it used max pooling to retain neighboring KVs. PyramidKV \cite{pyramidkv} builds on SnapKV by configuring variable eviction rates across layers, with more aggressive pruning in later layers where attention is less evenly distributed. 

The common drawback across these existing methods is that the model accuracy drops with the higher the KV compression ratio. These works concentrate solely on finding a set of pivotal tokens (the specifics of the algorithms to select these will vary) and evict the rest of the tokens which shrink the KV cache size but at the expense of a negative impact on the model generation quality. \emph{KVCrush} addresses this drawback by trying to represent the evicted tokens in the final KV cache. It ensures that different token groups are represented in the reduced KV memory footprint, and thus can sustain the accuracy. As a result, \emph{KVCrush} can be combined with many existing methods to remedy the accuracy drop at the same KV cache budget. We present results in Section~\ref{experiments}, combining \emph{KVCrush} with state-of-the-art research, showing the improved performance of our technology at the same compression ratio.

\begin{figure*}[t]
\centering
     \includegraphics[width=0.80\textwidth]{figures/kvcrush_grouping.png}
     \caption{ KVCrush KV Eviction}
    \label{fig:kvcrush_grouping}
\end{figure*}


        

        \begin{table*}[ht!]
        \resizebox{\textwidth}{!}
        {
            \begin{tabular}{@{}ccc@{}}
            \toprule
            \textbf{Benchmarks}                  & \textbf{LongBench}             & \textbf{lm-eval-harness}                  \\ \midrule
            \textbf{Experiments}                 & RQ3, RQ5                       & RQ1, RQ2, RQ4                             \\
            \textbf{Datasets} &
              \begin{tabular}[c]{@{}c@{}}narrativeqa, qasper,   multifieldqa\_en, hotpotqa, 2wikimqa, \\      musique, gov\_report, qmsum, multi\_news, trec, triviaqa, \\      samsum,  passage\_count,   passage\_retrieval\_en,   repobench-p\end{tabular} &
              GSM8K and XSUM \\
            \textbf{Models} &
              Mistral-7B-Instruct-v0.2, Meta-Llama-3-8B-Instruct &
              Phi-3-mini-4k-instruct, Meta-Llama-3-8B-Instruct,  Llama-2-7b-chat-hf \\
            \textbf{Baselines}                   & FullKV, H2O, SnapKV, PyramidKV & H2O                                       \\
            \textbf{Paging Mode}                 & Token Level, Chunk Level       & Page Level (Page Size = 32)                               \\
            \textbf{Total Cache Budget (tokens)} & 2048                           & 672 = 32(initial)+512(middle)+128(recent) \\
            \textbf{Cache Budget Partitioning}   & 25\% KVCrush + 75\% Baseline   & Best of (25\%+75\%) and (75\% and 25\%)   \\
            \textbf{Cuda Version}                & 12.2                           & 12.2                                      \\
            \textbf{Pytorch Version}             & '2.4.1+cu121'                  & '2.4.1+cu121'                             \\ \bottomrule
            \end{tabular}
        }
        \caption{Experimental settings used for evaluation}
        \label{table:settings}
        \end{table*}

        \begin{table*}[ht!]
        \resizebox{\textwidth}{!}
        {
        \begin{tabular}{|c|c|cc|cc|cc|}
        \hline
                                   & Cache Budget: 512 & \multicolumn{2}{c|}{\textbf{Phi-3-mini-4k-instruct}}                                                                           & \multicolumn{2}{c|}{\textbf{Meta-Llama-3-8B-Instruct}}                                                                         & \multicolumn{2}{c|}{\textbf{Llama-2-7b-chat-hf}}                                 \\ \hline
                                   & hh-cl             & \multicolumn{1}{c|}{\textbf{Strict}}                                     & \textbf{Flexible}                                   & \multicolumn{1}{c|}{\textbf{Strict}}                                     & \textbf{Flexible}                                   & \multicolumn{1}{c|}{\textbf{Strict}}              & \textbf{Flexible}            \\ \hline
        \textit{H2O}               & 512-0             & \multicolumn{1}{c|}{70.7}                                                & 79.3                                                & \multicolumn{1}{c|}{74.9}                                                & 74.7                                                & \multicolumn{1}{c|}{0.209}                        & 0.225                        \\ \hline
        \textit{kvcrush.random}    & 128-384           & \multicolumn{1}{c|}{{\color[HTML]{00B050} 75.4}}                         & {\color[HTML]{00B050} 80.9}                         & \multicolumn{1}{c|}{{\color[HTML]{00B050} 76.2}}                         & {\color[HTML]{00B050} 76.2}                         & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.211}} & {\color[HTML]{00B050} 0.226} \\ \hline
        \textit{kvcrush.mean}      & 128-384           & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.2}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 80.6} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 76.5}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 76.4} & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.21}}  & {\color[HTML]{00B050} 0.229} \\ \hline
        \textit{kvcrush.alternate} & 128-384           & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 74.6}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 80.9} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.7}} & \cellcolor[HTML]{FFFFFF}{\color[HTML]{00B050} 75.6} & \multicolumn{1}{c|}{{\color[HTML]{00B050} 0.212}} & {\color[HTML]{00B050} 0.227} \\ \hline
        \end{tabular}
        }
        \caption{GSM-8K Accuracy using different anchor points in KVCrush. KVCrush outperforms the baseline H2O even using generic anchor points like random, mean and alternate 0s and 1s. Here hh and cl represents the cache budget used by H2O and KVCrush respectively.}
        \label{table:anchor}
        \end{table*}