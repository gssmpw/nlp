@article{gpt3,
  title={Language Models are Few-Shot Learners},
  author={Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{decomp,
  title={Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference.},
  author={Harry Dong and Xinyu Yang and Zhenyu Zhang and Zhangyang Wang and Yuejie Chi and Beidi Chen},
  journal={arXiv preprint arXiv:2402.09398},
  year={2024}
}
@article{gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Petko Georgiev and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  journal={arXiv preprint arXiv:2309.06180},
  year={2023}
}

@article{mixed,
  title={Mixed-precision Quantization for Efficient LLM Deployment},
  author={Shiyao Li and Xuefei Ning  and Ke Hong and Tengxuan Liu and Luning Wang and Xiuhong Li and Kai Zhong and Guohao Dai and Huazhong Yang and Yu Wang},
  journal={International Conference on Neural Information Processing Systems, NeurIPS},
  year={2023}
}

@misc{llama,
  title = {{LLaMA-65B}: A 65-billion-parameter large language model},
  author = {Meta AI},
  year = {2023},
  url = {https://github.com/facebookresearch/llama},
}

@article{allyouneed,
  title={Attention is all you need},
  author={Ashish Vaswani and others},
  journal={International Conference on Neural Information Processing Systems, NeurIPS},
  year={2017}
}

@article{survey,
  title={Keep the Cost Down: A Review on Methods to Optimize {LLM' s KV-Cache} Consumption},
  author={Luohe Shi and Hongyi Zhang and Yao Yao and Zuchao Li and Hai Zhao},
  journal={arXiv preprint arXiv:2407.18003 },
  year={2024}
}

@article{h2o,
  title={{H2O}: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{fastgen,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{pyramidkv,
  title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
  author={Zhang, Yichi and Gao, Bofei and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and Xiao, Wen and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@article{mqa,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

@article{gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@inproceedings{liu2024llmqat,
  title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
  author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={467--484},
  year={2024},
  address={Bangkok, Thailand and virtual meeting},
  publisher={Association for Computational Linguistics}
}

@inproceedings{sheng2023flexgen,
  title={FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y. and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E. and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@inproceedings{mq1,
  title={HAQ: Hardware-Aware Automated Quantization with Mixed Precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={8612--8620},
  year={2019}
}

@inproceedings{mq2,
  title={HAWQ-V3: Dyadic Neural Network Quantization},
  author={Yao, Zhewei and Dong, Zhen and Zheng, Zhangcheng and Gholami, Amir and Yu, Jiali and Tan, Eric and Wang, Leyuan and Huang, Qijing and Wang, Yida and Mahoney, Michael W. and Keutzer, Kurt},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages={11875--11886},
  year={2021},
  organization={PMLR}
}

@inproceedings{mq3,
  title={Post Training Mixed Precision Quantization of Neural Networks Using First-Order Information},
  author={Chauhan, Arun and Tiwari, Utsav and Vikram, N R},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
  pages={1343--1352},
  year={2023}
}

@article{mikv,
  title={No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
  author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2402.18096},
  year={2024}
}

@inproceedings{memrec,
  title = {{Mem-Rec}: {M}emory Efficient Recommendation System using Alternative Representation},
  author = {Jha, Gopi Krishna and Thomas, Anthony and Jain, Nilesh and Gobriel, Sameh and Rosing, Tajana and Iyer, Ravi},
  booktitle = {Proceedings of the 15th Asian Conference on Machine Learning},
  pages = {518--533},
  year = {2024},
  editor = {Yanıkoğlu, Berrin and Buntine, Wray},
  volume = {222},
  series = {Proceedings of Machine Learning Research},
  month = {11--14 Nov},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v222/jha24a/jha24a.pdf},
  url = {https://proceedings.mlr.press/v222/jha24a.html}
}

@article{transformer1,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, M},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{transformer2,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{gholami2024ai,
  title={AI and Memory Wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W. and Keutzer, Kurt},
  journal={IEEE Micro},
  year={2024},
  volume={44},
  number={2},
  pages={56--67},
  doi={10.1109/MM.2024.1234567}
}

@inproceedings{sun2019bert4rec,
  title={BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer},
  author={Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  booktitle={Proceedings of the 28th ACM international conference on information and knowledge management},
  pages={1441--1450},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{isaev2023scaling,
  title={Scaling Infrastructure to Support Multi-Trillion Parameter LLM Training},
  author={Isaev, Mikhail and McDonald, Nic and Vuduc, Richard},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture (ISCA)},
  year={2023},
  organization={IEEE}
}
@article{streamingllm,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}
@misc{subgen,
      title={SubGen: Token Generation in Sublinear Time and Memory}, 
      author={Amir Zandieh and Insu Han and Vahab Mirrokni and Amin Karbasi},
      year={2024},
      eprint={2402.06082},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.06082}, 
}
@misc{keyformer,
      title={Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference}, 
      author={Muhammad Adnan and Akhil Arunkumar and Gaurav Jain and Prashant J. Nair and Ilya Soloveychik and Purushotham Kamath},
      year={2024},
      eprint={2403.09054},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.09054}, 
}

@misc{longbench,
      title={LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding}, 
      author={Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
      year={2024},
      eprint={2308.14508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.14508}, 
}

@misc{lmevalharness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}