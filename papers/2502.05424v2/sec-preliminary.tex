\section{Preliminaries}
In this section, we provide technical background, and outline the scope of our work.

\stitle{Graph encoder.}
A \emph{graph} is defined as \( G = (V, E, \vec{X}) \), where \( V \) is the set of nodes, \( E \) is the set of edges, and \( \vec{X} \in \mathbb{R}^{|V| \times d} \) is the node feature matrix with each row \( \vec{x}_i \) representing the feature vector of node \( v_i \in V \). A collection of graphs is denoted as \( \mathcal{G} \).

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figures/framework.pdf}%
\vspace{-1mm}%
\caption{Overall framework of \model.}
\label{fig.framework}
\end{figure*}

%\stitle{Graph encoder.}
Message-passing GNNs are a common choice for encoding graph representations \cite{wu2020comprehensive}. Specifically, each node updates its embedding by receiving and aggregating features or embeddings from its neighbors. By stacking such message-passing layers, information can propagate recursively throughout the graph. Therefore, the node embeddings are encoded based on both input features and graph structure. Let us denote the embedding of node \( v \) at the \( l \)-th layer as \( \vec{h}^l_v \), which is derived from the features or embeddings in the preceding layer as follows.
\begin{align}
    \vec{h}^l_v = \mathtt{Aggr}(\vec{h}^{l-1}_v, \{\vec{h}^{l-1}_u : u\in\bN_v\}; \theta^l),
\end{align}
where \( \mathcal{N}_v \) denotes the set of neighboring nodes of \( v \), \( \theta^l \) represents the learnable parameters in layer \( l \), and \( \mathtt{Aggr}(\cdot) \) stands for the neighborhood aggregation function. In the first layer, the node embedding \( \vec{h}^0_v \) is initialized as the input feature vector $\vec{x}_v$. We denote the output node embedding after the last layer as \( \vec{h}_v \), which is a row in the node embedding matrix \( \vec{H} \). Overall, the multi-layer message-passing process can be abstracted as a \emph{graph encoder}, as follows. 
\begin{align}
    \vec{H} = \mathtt{GE}(G,\vec{X};\Theta),
\end{align}
where $\mathtt{GE}$ denotes a graph encoder, \( \Theta = \{\theta^1, \theta^2, \ldots\} \) is the full set of trainable parameters for the graph encoder.

\stitle{Multi-domain pre-training with feature alignment.}\label{sec.pre.multi-domain}
Consider a set of unlabeled graphs \( \mathcal{G}_S = \{ G_1, G_2, \ldots, G_K \} \) for pre-training, where each graph \( G_i \) belongs to a specific \emph{source domain} \( D_{S_i} \in \mathcal{D}_S \). 
%Each domain \( D_{S_i} \) is characterized by its structure (node\& edge), and feature distributions, represented respectively as \( P^{(V)}_{S_i} \), \( P^{(E)}_{S_i} \), and \( P^{(\vec{X})}_{S_i} \). 
Thus, we have graph-domain pairs \( \{(G_i, D_{S_i}): i \in \{ 1, 2, \ldots, K\}\} \). 
%Note that some graphs may belong to the same domain, meaning there could be $D_{S_i} = D_{S_j}$ for some $i\ne j$.
%\( \exists i \neq j, D_{S_i} = D_{S_j} \).
%Subsequently, the input to the pre-training phase consists of the unlabeled source domain graphs $\mathcal{G}_S$. 

As different domains exhibit distinct feature distributions, previous works \cite{zhao2024all,yu2024text} have proposed solutions to align feature dimensions and semantics, which can be directly employed in our work. 
Given a graph \( G_{i} = (V_{i}, E_{i}, \vec{X}_{i}) \) from the source domain \( D_{S_i} \), we first align the dimensions of its feature matrix:
\begin{align}\label{eq.pre-train.dim-align}
    \tilde{\Vec{X}}_{i} = \mathtt{DAL}_{S_i}(\Vec{X}_{i}),
\end{align}
where \( \mathtt{DAL}_{S_i} \colon \mathbb{R}^{|V| \times d_{S_i}} \rightarrow \mathbb{R}^{|V| \times \tilde{d}} \) is the dimension alignment function for domain \( D_{S_i} \), transforming the original dimension $d_{S_i}$ to a common dimension $\tilde{d}$ across domains. We implement $\mathtt{DAL}$ as singular value decomposition \cite{stewart1993early} following prior art \cite{zhao2024all,yu2024text}.
Next, given the source-domain graphs $\mathcal{G}_S$ with their dimension-aligned features $\mathcal{\tilde{X}}_S=\{\tilde{\Vec{X}}_{i}: G_{i} \in \bG_{S}\}$, we further align the features to unify their semantic space across various domains. Letting $\mathtt{FAL}$ denote the feature alignment procedure, we pre-train a graph encoder with feature alignment:
\begin{align}\label{eq.pre-train.feature-align}
    \Vec{H}^{\mathtt{FAL}} = \mathtt{GE}(\mathtt{FAL}(\bG_S,\tilde{\mathcal{X}_S};\Psi);\Theta),
\end{align}
where $\Psi$ denotes the learnable parameters in $\mathtt{FAL}$, and $\Vec{H}^{\mathtt{FAL}}$ is the output node embedding matrix with feature alignment. While any feature alignment model can be employed  \cite{zhao2024all,yu2024text}, we follow the work of \citet{yu2024text} due to its superior performance.


\stitle{Cross-domain task with feature adaptation.}\label{sec:problem_definition}
For each downstream task, consider a set of graphs \( \mathcal{G}_T \) belonging to a target domain \( D_T \). The task is \emph{cross-domain} if the target domain is \emph{unseen} during pre-training,
\ie, \( \forall i \; D_T \ne D_{S_i} \). Again, since the target domain may exhibit different feature characteristics from the source domains, 
previous works \cite{zhao2024all,yu2024text} have proposed feature adaptation strategies to transfer prior multi-domain knowledge to the target domain, which can be directly integrated into our work.
Specifically, we first employ the same dimension alignment method used in the pre-training phase,
transforming the feature matrix of a downstream graph $G=(V,E,\Vec{X}) \in \bG_T$ to $\tilde{\Vec{X}}=\mathtt{DAL}_T(\Vec{X})$\label{eq.target-dimension}. We then employ a feature adaptation technique $\mathtt{FAD}$ to adapt the pre-trained model to the target domain, as follow.
\begin{align}\label{eq.feature-emb}
    \Vec{H}^\mathtt{FAD} = \mathtt{GE}(\mathtt{FAD}(G,\tilde{\Vec{X}};\Gamma);\Theta_\text{pre}),
\end{align}
where $\Gamma$ denotes the learnable parameters in $\mathtt{FAD}$, and $\Theta_\text{pre}$ is the pre-trained weights in graph encoder $\mathtt{GE}$. Here we implement $\mathtt{FAD}$ following  \citet{yu2024text}, which is paired with the feature alignment method in pre-training.

\stitle{Our scope: Few-shot classification}. For the downstream applications, we aim to solve \emph{few-shot} node and graph classification tasks. For node classification, given a graph \( G = (V, E, \vec{X}) \in \mathcal{G}_T \), each node \( v \in V \) is associated with a label \( y \in Y \), where $Y$ denotes the set of node classes. For graph classification over a set of graphs \( \mathcal{G}_T \), each graph \( G \in \mathcal{G}_T \) is associated with a label \( y \in Y \), where $Y$ denotes the set of graph classes. An \( m \)-shot classification task consists of only \( m \) labeled examples per class, along with an arbitrary number of unlabeled examples for testing. 

In particular, we focus on \emph{low-shot} settings, where \( m \) is a small number (\eg, \( m \leq 5 \)), reflecting real-world scenarios where labeled data are expensive or difficult to obtain. Due to the parameter-efficient nature of prompt learning, many previous methods for prompt learning on graphs  \cite{liu2023graphprompt,yu2023generalized,sun2023all,yu2024text,zhao2024all}  also emphasize this setting. It is expected that, as more task-specific labeled data become available, conventional fine-tuning or supervised approaches may become sufficient. 
%Therefore, our evaluation is tailored to low-resource scenarios, reflecting real-world situations where labeled data are expensive to obtain. 
%Moreover, in common graph benchmark datasets, \( m > 5 \) already constitutes a significant portion of the data (see Appendix~\ref{app.why-low-shot}).

