\section{Related Work}

We review related literature on pre-training, cross-domain transfer learning, and multi-domain pre-training for graph data.

\stitle{Graph pre-training.}
Graph pre-training methods aim to extract inherent properties of graphs, often utilizing self-supervised learning approaches, which can be either generative \cite{hu2020gpt,li2023s,hou2022graphmae,jiang2023incomplete} or contrastive \cite{velivckovic2018deep,xia2022simgrace,xu2021self,li2022mining}. The pre-trained model is then employed to address downstream tasks through fine-tuning \cite{you2020graph,velivckovic2018deep,qiu2020gcc} or parameter-efficient adaptation methods, notably prompt-based learning \cite{sun2022gppt,liu2023graphprompt,yu2023generalized,fang2022universal}. However, these methods typically assume that the pre-training and downstream graphs originate from the same domain, such as different subgraphs of a large graph \cite{you2020graph,yu2023hgprompt} or collections of similar graphs within the same dataset \cite{hu2020gpt,qiu2020gcc}, failing to account for multiple domains in either pre-training or downstream graphs.

\stitle{Graph cross-domain transfer.}
This line of work aims to transfer single-source domain knowledge to a different target domain by leveraging domain-invariant properties across domains~\cite{ding2021cross,hassani2022cross,wang2021pre,wang2023cross}. However, they rely exclusively on a single source domain, failing to harness the extensive knowledge available across multiple domains. Additionally, these approaches are often tailored to specific tasks or domains \cite{ding2021cross,hassani2022cross,wang2021pre,wang2023cross}, limiting their generalization.

\stitle{Multi-domain graph pre-training.}
In the context of graphs from multiple domains, recent works \cite{liu2023one,tang2024higpt,xia2024opengraph} utilize large language models to align node features from different domains through textual descriptions, thereby limiting their applicability to text-attributed graphs \cite{zhaolearning,wen2023prompt,zhang2024text}. For graphs without textual attributes, GraphControl \cite{zhu2024graphcontrol} applies ControlNet~\cite{zhang2023adding} to incorporate target domain node features with the pre-trained model, while neglecting the alignment among multiple source domains. Another recent study proposes GCOPE~\cite{zhao2024all}, which employs domain-specific virtual nodes that interconnect nodes across domains, facilitating the alignment of feature distribution and homophily patterns. Meanwhile, MDGPT~\cite{yu2024text} pre-trains domain-specific tokens to align feature semantics across various domains. However, these studies do not account for structural variance across different domains, hindering their effectiveness in integrating multi-domain knowledge. On a related front, multi-task pre-training techniques \cite{wang2022multi,yu2023multigprompt} employ pretext tokens for each pre-training task. It is important to note that they address a distinct problem, aiming to overcome potential interference among multiple tasks within a single domain, rather than interference across multiple domains. 
%In our work, we propose structure tokens and dual prompts to overcome the limitations of current multi-domain graph pre-training methods. 
%Note that multi-task pre-training aims to reduce interference among different pre-training tasks within a single domain, distinct from our objective of multi-domain pre-training.

