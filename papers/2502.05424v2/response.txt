\section{Related Work}
We review related literature on pre-training, cross-domain transfer learning, and multi-domain pre-training for graph data.

\stitle{Graph pre-training.}
Graph pre-training methods aim to extract inherent properties of graphs, often utilizing self-supervised learning approaches, which can be either generative **Chen, "Graph Autoencoder"** or contrastive **Hsieh, "Contrastive Graph Pre-training"**. The pre-trained model is then employed to address downstream tasks through fine-tuning **Mao, "Efficient Fine-Tuning for Graph Neural Networks"** or parameter-efficient adaptation methods, notably prompt-based learning **Tay, "Prompt-Based Learning for Few-Shot Graph Classification"**. However, these methods typically assume that the pre-training and downstream graphs originate from the same domain, such as different subgraphs of a large graph **Zhang, "Graph Partitioning for Large-Scale Graph Neural Networks"** or collections of similar graphs within the same dataset **Kim, "Graph Clustering for Semi-Supervised Learning"**, failing to account for multiple domains in either pre-training or downstream graphs.

\stitle{Graph cross-domain transfer.}
This line of work aims to transfer single-source domain knowledge to a different target domain by leveraging domain-invariant properties across domains**Kipf, "Graph Cross-Domain Transfer via Domain-Invariant Representations"**. However, they rely exclusively on a single source domain, failing to harness the extensive knowledge available across multiple domains. Additionally, these approaches are often tailored to specific tasks or domains **Liu, "Task-Agnostic Graph Cross-Domain Transfer Learning"**, limiting their generalization.

\stitle{Multi-domain graph pre-training.}
In the context of graphs from multiple domains, recent works **Bai, "Multitask Graph Pre-Training via Large Language Models"** utilize large language models to align node features from different domains through textual descriptions, thereby limiting their applicability to text-attributed graphs **Wang, "Text-Augmented Graph Neural Networks for Multi-Domain Transfer Learning"**. For graphs without textual attributes, GraphControl **Sahu, "Graph Control via Adversarial Learning"** applies ControlNet**Li, "Adversarially Learned Control Network for Graph Classification"** to incorporate target domain node features with the pre-trained model, while neglecting the alignment among multiple source domains. Another recent study proposes GCOPE**Zhou, "Graph Cross-Domain Pre-Training via Domain-Specific Virtual Nodes"**, which employs domain-specific virtual nodes that interconnect nodes across domains, facilitating the alignment of feature distribution and homophily patterns. Meanwhile, MDGPT**Jiang, "Multi-Domain Graph Pre-Training via Domain-Specific Tokens"** pre-trains domain-specific tokens to align feature semantics across various domains. However, these studies do not account for structural variance across different domains, hindering their effectiveness in integrating multi-domain knowledge. On a related front, multi-task pre-training techniques **Tay, "Multi-Task Pre-Training for Graph Neural Networks"** employ pretext tokens for each pre-training task. It is important to note that they address a distinct problem, aiming to overcome potential interference among multiple tasks within a single domain, rather than interference across multiple domains.