\section{Introduction}\label{sec.intro}
How to build foundation models has emerged as an important question, paving a plausible path toward artificial general intelligence. In natural language processing, recent works \cite{touvron2023llama,achiam2023gpt} have demonstrated the capabilities of universal foundation models. They are trained on a wide variety of data from multiple domains, and can be further adapted to solve a diverse range of tasks. Other than natural languages, the World Wide Web has become a vast knowledge repository, connecting an enormous amount of entities to form extensive and complex graphs. These graphs enable diverse Web applications, including social network analysis~\cite{miyake2024netevolve,jiang2024ragraph}, Web mining~\cite{fangjf2023exgc,agarwal2022graphnli}, and recommendation systems~\cite{ma2024temporal,ni2024graph}. 
Given the rich graph data on the Web, can we build a universal graph model based on multi-domain graphs, to address various downstream graph-centric applications \cite{liu2023towards}? 

Traditional supervised graph learning struggles to build universal models. 
These approaches require retraining a new graph neural network (GNN) \cite{kipf2016semi,fangjf2023eva,yu2023learning} or graph transformer \cite{yun2019graph,rampavsek2022recipe,ying2021transformers} for each new task, relying on abundant task-specific labeled data. 
In contrast, more recent graph pre-training methods \cite{hu2020gpt,qiu2020gcc,velivckovic2018deep} attempt to learn universal properties from unlabeled graphs in a self-supervised manner, which can be subsequently adapted to a downstream task with some task-specific labels through fine-tuning \cite{kipf2016variational,velivckovic2018deep,qiu2020gcc} or prompt learning  \cite{liu2023graphprompt,sun2023all,yu2025node}.
However, in most existing graph pre-training approaches, the pre-training and downstream graphs originate from the same dataset \cite{velivckovic2018deep,you2020graph,liu2023graphprompt,sun2023all}, a practice we refer to as \emph{single-domain} methods, which fall short of building a universal, \emph{multi-domain} graph model from multiple graph datasets.


\stitle{Research problem.}
Thus, it is crucial to pre-train a graph model on a wide range of multi-domain (\ie, multi-dataset) graphs and achieve cross-domain adaptation. However, graph structures from different datasets often exhibit markedly distinct characteristics. For instance, the structural patterns in a social network might not be directly applicable to a citation or e-commerce graph. 
Such diversity poses significant challenges in integrating graphs from multiple domains and adapting prior knowledge to different domains. Although some studies have explored cross-domain adaptation from a single source domain~\cite{ding2021cross,hassani2022cross,wang2021pre,wang2023cross,yang2019cross}, they do not exploit multiple  source domains. Another line of work~\cite{liu2023one,tang2024higpt,xia2024opengraph} employs large language models to extract and utilize multi-domain knowledge based on textual descriptions associated with graphs, using text as a universal medium to bridge different domains. However, this limits their applicability to text-attributed graphs~\cite{zhaolearning,wen2023augmenting} and cannot be extended to general graphs without textual descriptions. Few recent studies~\cite{zhao2024all,yu2024text} have explored multi-domain pre-training on text-free graphs, but they focus on aligning the divergent feature spaces and homophily patterns across multi-domain graphs, while overlooking the structural differences across  domains. 

\stitle{Challenges and insights.}
In this paper, we propose \model, a graph foundation model with \textbf{S}tructural \textbf{A}lignment for text-free \textbf{M}ulti-domain \textbf{G}raph \textbf{P}re-\textbf{T}raining, to facilitate cross-domain adaptation. This is non-trivial due to two key challenges.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/intro.pdf}%
\vspace{-1mm}%
\caption{Motivation of \model.}
\label{fig.intro-motivation}
\end{figure}

First, \textit{how do we harmonize structural variance across multiple domains during pre-training?} 
Graphs from different domains often exhibit distinct structural and topological characteristics, such as average node degree, shortest path length and clustering coefficient, as depicted in Table~\ref{table.datasets}. %~\ref{app.dataset}.
Thus, merging multi-domain graphs without structure alignment during pre-training may cause interference rather than synergy, leading to suboptimal performance.
In \model, we propose \textit{structure tokens} to align structural distributions across multiple domains, as shown in Fig.~\ref{fig.intro-motivation}(a). Each domain is equipped with a series of structure tokens, which modify the structure-based aggregation in each layer of the graph encoder. These tokens are learnable vectors that capture domain-specific structural patterns, enabling the model to accommodate the unique structural characteristics of each domain during pre-training.

Second, \textit{how do we adapt multi-domain prior  structural knowledge to cross-domain downstream tasks?} 
Multi-domain prior knowledge includes not only holistic knowledge across source domains, but also domain-specific knowledge from each domain. Therefore, in \model, we %extend beyond feature alignment by proposing 
propose dual \textit{structural prompts}, comprising a set of \textit{\op} and a set of \textit{\cp}, thus facilitating the adaptation of both holistic and domain-specific knowledge to downstream tasks, as illustrated in Fig.~\ref{fig.intro-motivation}(b). On one hand, the \op\ consist of learnable vectors that holistically align the target domain's structural characteristics with the unified pre-trained knowledge from all source domains. On the other hand, \cp\ integrate multi-domain structure tokens in a learnable mixture to align the target domain with  knowledge from each source domain, capturing domain-specific structural information for finer-grained adaptation.

\stitle{Contributions.}
In summary, we make the following contributions in this work.
(1) We propose \model, a text-free graph foundation model with structure alignment for multi-domain graph pre-training and cross-domain adaptation.
(2) For pre-training, we propose structure tokens to align structural distributions across domains, training a universal foundation model with multi-domain graphs.
(3) For downstream adaptation, we propose a dual-prompt strategy, using \op\ to leverage holistic prior structural knowledge and \cp\ to facilitate finer-grained, domain-specific structural adaptation.
(4) We conduct extensive experiments on seven benchmark datasets, demonstrating the superior performance of \model\ compared to state-of-the-art methods.
