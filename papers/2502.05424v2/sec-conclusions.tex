\section{Conclusions}
In this paper, we proposed \model, a graph foundation model with structure alignment for text-free graphs, supporting both multi-domain graph pre-training and cross-domain adaptation. In the pre-training phase, \model\ utilizes a series of structure tokens to harmonize the structural distributions across multiple source domains and to extract multi-domain structural knowledge. For downstream cross-domain adaptation, \model\ employs dual prompts to tailor pre-trained holistic and domain-specific structural knowledge to the target domain. We conducted extensive experiments on seven benchmark datasets, demonstrating that \model\ significantly outperforms various state-of-the-art baseline methods.

