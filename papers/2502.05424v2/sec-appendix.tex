


\subsection{Algorithm}\label{app.alg}
Our algorithm consists of two stages, multi-domain graph pre-training and downstream adaptation.

In the multi-domain pre-training phase,  we first apply the dimension alignment function, $\mathtt{DAL}$, to align feature dimensions from different source domains by Eq.~\eqref{eq.pre-train.dim-align}. Then, we use a feature alignment method to unify feature semantic spaces by Eq.~\eqref{eq.pre-train.feature-align}. For structure alignment, we inject source domain-specific structure tokens into each layer of the graph encoder by Eq.~\eqref{eq:structure_aggregation}. Finally, we fuse the feature aligned embedding and structure aligned embedding by Eq.~\eqref{eq.source-fusion}, and optimize the pre-training loss by Eq.~\eqref{eq:generalized_loss}.

We further present the key steps for cross-domain adaptation in Algorithm~\ref{alg.prompt}. In lines 3--4, we align target domain feature dimensions with source domains. In lines 6--7, we integrate the feature adaptation method to generate feature-level adapted embeddings. In lines 8--21, we employ dual prompts to adapt structural prior knowledge to the target domain. Specifically, we first inject \op\ to modify the structure-based aggregation in each layer of the graph encoder for holistic knowledge adaptation (lines 9--12). Then, we generate \cp\ by fusing the pre-trained structure tokens (lines 13--15), and utilize \cp\ for domain-specific knowledge adaptation (lines 17--19). We obtain structure-level adapted embeddings by fusing holistic and domain-specific embeddings (lines 20--21), and generate final embeddings by aggregating feature- and structure-level adapted embeddings (lines 22--23). Finally, we update the embeddings for the prototypical instances based on the labeled samples in the task and optimize \op, $\Lambda$ and $\Gamma$ (lines 24-28). %Note that updating prototypical is required exclusively for classification tasks.

\begin{algorithm}[b]
\small
\caption{\textsc{Cross-domain Adaptation for \model}}
\label{alg.prompt}
\begin{algorithmic}[1]
    \Require Pre-trained graph encoder $\mathtt{GE}$ with parameters $\Theta_\text{pre}$, pre-trained structure tokens $\bT_\text{pre}$, target domain dimension alignment function $\mathtt{DA}_T(\cdot)$, and feature adaptation function $\mathtt{FAD}(\cdot)$
    \Ensure Optimized holistic prompts $\bP_\text{hol}$, coefficients $\Lambda$, and feature adaptation parameters $\Gamma$
    \While{not converged} 
        \For{each graph $G_T=(V_T, E_T, \vec{X}_T)$ in target domain $D_T$}
            \State \slash* Target domain feature dimensions alignment by Eq.~(\ref{eq.target-dimension}) *\slash
            \State $\tilde{\Vec{X}}\leftarrow \mathtt{DAL}_T(\Vec{X})$
            \State $\bP_\text{hol}$, $\Lambda, \Gamma \leftarrow$ initialization
                \State \slash* Feature adaptation by Eq.~(\ref{eq.feature-emb}) *\slash
                \State $\Vec{H}^\mathtt{FAD} \leftarrow \mathtt{GE}(\mathtt{FAD}(\bG,\tilde{\Vec{X}};\Gamma);\Theta_\text{pre})$
                \State \slash* Structure alignment by dual prompts *\slash
                %\State \slash* Adaptation of holistic structural prior knowledge *\slash
                \State \slash* Modification to $\mathtt{GE}$ via \op\ by Eq.~(\ref{eq.open-prompt}) *\slash
                \For{each layer in $\mathtt{GE}$}
                    \State $\vec{h}^l_{v} \leftarrow \mathtt{Aggr}(\vec{h}^{l-1}_{v}, \{\Vec{p}^l_\text{hol} \odot \vec{h}^{l-1}_{u} : u\in\bN_v\}; \theta^l_\text{pre}),~\forall v\in G_T$
                \EndFor
                \State $\vec{H}^\text{hol}\leftarrow \mathtt{STACK}(\{\vec{h}_{v}:~\forall v\in G_T\})$
                \State \slash* Generation of \cp\ by Eq.~(\ref{eq.specific-prompt-generation}) *\slash
                \For{$\Vec{p}^l_\text{spe}~\text{in}~\bP_\text{spe}$}
                    \State $\Vec{p}^l_\text{spe} \leftarrow \sum_{i=1}^K \lambda^l_i \Vec{t}^l_{S_i}$
                \EndFor
                \State \slash* Modification to $\mathtt{GE}$ via \cp *\slash
                \For{Each layer in $\mathtt{GE}$}            
                    \State $\vec{\tilde{h}}^l_{v} \leftarrow \mathtt{Aggr}(\vec{\tilde{h}}^{l-1}_{v}, \{\Vec{p}^l_\text{spe} \odot \vec{\tilde{h}}^{l-1}_{u} : u\in\bN_v\}; \theta^l_\text{pre}),~\forall v\in G_T$
                \EndFor
                \State $\vec{H}^\text{spe}\leftarrow \mathtt{STACK}(\{\vec{\tilde{h}}_{v}:~\forall v\in G_T\})$
                \State \slash* Fusion of dual-prompt tuned embeddings by Eq.~(\ref{eq.structure-emb}) *\slash
                \State $\vec{H}^\text{SAD} \leftarrow \vec{H}^\text{hol}+\beta\vec{H}^\text{spe}$
                \State \slash* Fusion of adapted embeddings by Eq.~(\ref{eq.down-fusion}) *\slash
                \State $\vec{H}^\mathtt{AD} \leftarrow \vec{H}^\text{FAD}+\alpha\vec{H}^\text{SAD}$
                \State \slash* Update prototypical nodes *\slash
                    \For{each class $y$} 
                        \State ${\vec{h}}_{y} \leftarrow \textsc{Mean}(\{\vec{h}_{x}$: instance $x$ belongs to class $y$\})
                    \EndFor
                \State \slash* Optimizing $\bP_\text{hol}$, $\Lambda$, and $\Gamma$ *\slash
                \State Calculate $\bL_\text{down}(\Omega;\bP_\text{hol},\Lambda,\Gamma)$ by Eq.~(\ref{eq.downstream_loss})
        \EndFor
    \EndWhile 
    \State \Return $\bP_\text{hol}$, $\Lambda$, and $\Gamma$
\end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}\label{complexity}
For a downstream graph \( G_T = (V_T, E_T, \vec{X}_T) \) from the target domain \( D_T \), the computational process of structure adaptation involves injecting \op\ and \cp\ into the pre-trained GNN to modify the encoding of nodes. 

In a standard GNN, each node aggregates messages from its neighbors in each layer. Assuming that the aggregation involves at most $n$ neighbors, the complexity of calculating node embeddings over $L$ layers is $O(n^L \cdot |V_T|)$. One one hand, \op\ are directly injected into each layer of the GNN, increasing the complexity by $O(L \cdot |V_T|)$. On the other hand, \cp\ are first generated by the pre-trained structure tokens from $K$ source domains, with an additional complexity of $O(L \cdot K)$. Then, \cp\ modify the structure-base aggregation with a complexity of $O(L \cdot |V_T|)$. Since \op\ and \cp\ modifies the node encoding phase separately, the overall complexity would increase by $(L \cdot |V_T| + L \cdot K)$.

Thus, the encoding time by the pre-trained GNN still dominates the overall complexity, as $O(n^L \cdot |V_T|)$ typically exceeds $O(L \cdot |V_T|+ L\cdot K)$ given that $n^L > L$ and $|V_T| > K$ in general. In other words, our structural adaptation adds a marginal overhead to the pre-trained graph encoder.



\subsection{Further Descriptions of Datasets} \label{app.dataset}
In this section, we provide more comprehensive descriptions of the benchmark datasets used in our experiments, as summarized in Table~\ref{table.datasets},
in the following.
%Specifically, average node degree, average shortest path length \cite{borgwardt2005shortest}, and average clustering coefficient \cite{giatsidis2014corecluster} reflect the structural and topological properties of various datasets or domains, from which we observe that different domains exhibit unique structural characteristics, highlighting the importance of structural alignment in both multi-domain pre-training and cross-domain adaptation.




\begin{itemize}[leftmargin=*]
    \item \emph{Cora} \cite{mccallum2000automating} consists of 2,708 publications in the computing field, each categorized into one of seven classes. The citation network comprises 5,429 edges. Each publication is represented by a binary word vector indicating the presence or absence of words from a dictionary containing 1,433 unique words.
    \item \emph{Citeseer}\cite{sen2008collective} contains 3,312 computer science publications, each belonging to one of six categories, distinct from those in \textit{Cora}. The citation network consists of 4,732 edges. Each publication is represented by a binary word vector, reflecting the presence or absence of words from a dictionary of 3,703 unique words.
    \item \emph{PubMed} \cite{sen2008collective} consists of 19,717 biomedical publications related to diabetes, each classified into one of three categories. The citation network includes 44,338 edges. Each publication is represented by a TF/IDF-weighted word vector, indicating the presence of 500 unique words from the dictionary.
    \item \emph{Photo} \cite{shchur2018pitfalls} contains 7,487 products related to photography, each assigned to one of eight categories. The co-purchase network comprises 119,043 edges, representing products frequently bought together. Each product is described by a feature vector derived from its metadata and reviews, and is labeled according to its category.
    \item \emph{Computers} \cite{shchur2018pitfalls} includes 13,752 computer-related products, divided into ten categories. The co-purchase network consists of 245,861 edges, representing products that are frequently bought together. Each product is characterized by a feature vector generated from its metadata and reviews and is labeled according to its respective category.
    \item \emph{Facebook} \cite{rozemberczki2021multi} represents a page-to-page Web graph of verified Facebook pages. The nodes correspond to official Facebook pages, and the edges indicate mutual ``likes'' between these pages. Node features are derived from the descriptions provided by the page owners that outline the purpose of their pages.
    \item \emph{LastFM} \cite{rozemberczki2020characteristic} represents a social network of LastFM users, collected via the public API in March 2020. The nodes correspond to LastFM users from various Asian countries, and the edges represent mutual follower relationships. The node features are extracted based on the artists that users have liked. The associated task for this graph is multinomial node classification, where the objective is to predict each user's location, derived from the country field in their profile.
\end{itemize}



% \subsection{Why low-shot setting} \label{app.why-low-shot}
% In this work, we focus on low-shot scenarios, since in some common graph datasets, having ten labeled examples per class already constitutes a significant portion of the data. Moreover, traditional supervised and semi-supervised graph models like Planetoid~\cite{yang2016revisiting}, GCN~\cite{kipf2016semi}, and GAT~\cite{velivckovic2017graph}, along with fine-tuning methods such as DGI~\cite{velivckovic2018deep} and GraphCL~\cite{you2020graph}, utilize 20 nodes per class for training or tuning on the Cora and Citeseer datasets. %This accounts for merely 5.17\% and 4.21\% of the total nodes, respectively. 



% \begin{table}[tbp]
% \center
% \caption{Summary of datasets. 
% \label{table.datasets}}
% \resizebox{1\linewidth}{!}{%
% \begin{tabular}{@{}c|ccccccc@{}}
% \toprule
% 	& \makecell[c]{Nodes} & \makecell[c]{Edges} & \makecell[c]{Feature\\dimension} & \makecell[c]{Node\\classes} & \makecell[c]{Avg\\nd} &\makecell[c]{Avg\\spl} &\makecell[c]{Avg\\cc}\\
% \midrule
%      Cora & 2,708 & 10,556 & 1,433 & 7 & 3.89 & 6.30
%  & 0.24 \\
%      Citeseer & 3,327 & 9,104 & 3,703 & 6 & 2.73
% &9.31 & 0.14 \\ 
%      Pubmed & 19,717 & 88,648 & 500 & 3 & 4.49
% &6.33   & 0.06\\
%      Photo & 7,650 & 238,162 & 745 & 8 & 31.13
% &4.05   & 0.40 \\
%      Computers & 13,752 & 491,722 & 767 & 10 & 35.75
% &3.38   & 0.34\\
%      Facebook & 22,470 & 342,004 & 128 & 4 & 15.22
% &4.97   &  0.35 \\
%      LastFM & 7,624 & 55,612 & 128 & 18 & 7.29
% & 5.23 & 0.21

% \\
%  \bottomrule
% \end{tabular}}
%    \parbox{1\linewidth}{Avg is short for average. nd stands for node degree. spl denotes shortest path length. cc represents clustering coefficient.}
% \end{table}



\subsection{Further Descriptions of Baselines} \label{app.baselines}
In this section, we provide additional details about the baselines used in our experiments.

\vspace{1mm}
\noindent (1) End-to-end GNNs:
\begin{itemize}[leftmargin=*]
    \item \textbf{GCN} \cite{kipf2016semi}: GCN employs a mean-pooling approach for neighborhood aggregation, enabling the integration of information from adjacent nodes.
    \item \textbf{GAT} \cite{velivckovic2017graph}: GAT relies on neighborhood aggregation for node representation learning, but distinguishes itself by assigning varying attention weights to neighbors, thus adjusting their influence on the aggregation process.
\end{itemize}

\noindent (2) Graph pre-training models:
\begin{itemize}[leftmargin=*]
    \item \textbf{DGI} \cite{velivckovic2017graph}: DGI is a self-supervised pre-training approach. It is based maximizing mutual information (MI), with the goal of strengthening the MI between local node representations and their global context.
    \item \textbf{InfoGraph} \cite{sun2019infograph}: Building on DGI, InfoGraph focuses on graph-level tasks, aiming to align node and graph embeddings by maximizing the similarity between them.
    \item \textbf{GraphCL} \cite{you2020graph}: GraphCL applies various graph augmentations for self-supervised learning, leveraging structural patterns within graphs. Its main objective is to improve the similarity across different augmentations during pre-training.
    \item \textbf{GPPT} \cite{sun2022gppt}: GPPT pre-trains a GNN model via link prediction task. Its downstream prompt module is specifically designed for node classification, unifying it with the pre-training link prediction task.
    \item \textbf{GPF} \cite{fang2022universal}: GPF serves as a universal prompt-based tuning approach for pre-trained graph models. It adapts the input graph's feature space to simulate the behavior of various prompting functions.
    \item \textbf{GraphPrompt} \cite{liu2023graphprompt}: GraphPrompt utilizes subgraph similarity calculations as a unified framework to bridge the gap between pre-training and downstream tasks, supporting both node and graph classification. During downstream adaptation, a learnable prompt is tuned to incorporate task-specific knowledge.
\end{itemize}

\noindent (3) Graph cross-domain models:
\begin{itemize}[leftmargin=*]
    \item Hassani \cite{hassani2022cross}: Hassani proposes an attention-based graph encoder that leverages both contextual and topological views to capture task-specific information for quick adaptation, as well as task-independent knowledge for efficient transfer across domains.
\end{itemize}

% \noindent (4) \textbf{Graph Prompt Models}
% \begin{itemize}
%     \item \textbf{GPPT} \cite{sun2022gppt}: GPPT pre-trains a GNN model via link prediction task. Its downstream prompt module is specifically designed for node classification, unifying it with the pre-training link prediction task.
%     \item \textbf{GPF} \cite{fang2022universal}: GPF serves as a universal prompt-based tuning approach for pre-trained graph models. It adapts the input graph's feature space to simulate the behavior of various prompting functions.
%     \item \textbf{GraphPrompt} \cite{liu2023graphprompt}: GraphPrompt utilizes subgraph similarity calculations as a unified framework to bridge the gap between pre-training and downstream tasks, supporting both node and graph classification. During downstream adaptation, a learnable prompt is tuned to incorporate task-specific knowledge.
% \end{itemize}

\noindent (4) Multi-domain graph pre-training models:
\begin{itemize}[leftmargin=*]
    \item \textbf{GCOPE} \cite{zhao2024all}: GCOPE propose a multi-domain pre-training strategy that integrates graph datasets from various domains using domain-specific interconnecting virtual nodes, which link nodes within the same domain. The main objective is to enhance downstream performance by harnessing knowledge from multiple source domains.
\end{itemize}



\subsection{Implementation Details} \label{app.parameters}

% \stitle{Environment.}
% \label{app.general-setting}
% \textbf{Optimizer:} 
% The environment in which we ran all experiments is listed below.
% \begin{itemize}
%    \item Operating system: Ubuntu 22.04.2,
%    \item CPU information: AMD EPYC 7742 64-Core Processor,
%    \item GPU information: NVIDIA GeForce RTX 3090 (24 GB).
% \end{itemize}

We outline key settings for the baselines and \model. 

\stitle{Baseline settings.}
We utilized the official codes for all open-source baselines. Each model was tuned based on the settings recommended in their respective work to achieve optimal performance.

For the baseline GCN \cite{kipf2016semi}, we employ a 3-layer architecture, and set the hidden dimensions to 256. 
For GAT \cite{velivckovic2017graph}, we employ a 2-layer architecture and set the hidden dimension to 64. Additionally, we apply 8 attention heads in the first GAT layer.

For DGI \cite{velivckovic2017graph}, we utilize a 1-layer GCN as the backbone and set the hidden dimensions to 256. Additionally, we employ prelu as the activation function.
For InfoGraph \cite{sun2019infograph}, a 3-layer GCN is used as the backbone, with its hidden dimensions set to 256.
For GraphCL \cite{you2020graph}, a 1-layer GCN is also employed as its backbone, with the hidden dimensions set to 256. Specifically, we select edge dropping as the augmentations, with a default augmentation ratio of 0.2.
For GPPT \cite{sun2022gppt}, we utilize a 2-layer GraphSAGE as its backbone, setting the hidden dimensions to 256. We employ a mean aggregator for the aggregation in the backbone.
For GraphPrompt \cite{liu2023graphprompt}, a 3-layer GCN is used as the backbone for all datasets, with the hidden dimensions set to 256.
For GPF \cite{fang2022universal}, we employ a 5-layer GCN as the backbone for all datasets, following the recommended settings. The hidden dimensions are set to 256.


For Hassani \cite{hassani2022cross}, a 3-layer GCN is used as the backbone for all datasets, with the hidden dimensions set to 256.


For GCOPE \cite{zhao2024all}, we employ a 2-layer GCN as the backbone and set the hidden dimensions to 100. Downstream adaptation is achieved through fine-tuning, as it is reported to yield the best performance in their literature.

For all baselines except GCOPE, we set the unified feature dimensions to 50, matching our \model. For GCOPE, we adhere to the recommended settings and set it to 100.

\stitle{\model\ settings.}
For our proposed \model, we utilize a 3-layer GCN as the backbone for all datasets, with the hidden dimensions set to 256. We set the unified feature dimensions to 50.

\subsection{Details about Heterophilic Datasets}\label{app.hetero}
To evaluate the robustness of \model\ across graphs with varying homophily ratios, we conducted experiments on both homophilic and heterophilic datasets in Sect.~\ref{sec.hetero}. Details of the heterophilic datasets are introduced as follows.
(1) \emph{Chameleon} \cite{rozemberczki2021multi} is a Wikipedia-based network containing 2,277 pages, categorized into five groups based on their average monthly traffic. This dataset forms a network with 36,101 edges, and the node features are derived from key nouns extracted from the Wikipedia content. %The homophily ratio is 0.23.
(2) \emph{Cornell} \cite{pei2020geom} is another webpage network consisting of 183 nodes, where each node represents a webpage, and 295 edges denoting hyperlinks between them. The node features are derived from a bag-of-words representation of the webpages. These pages are manually classified into five categories: student, project, course, staff, and faculty. %The homophily ratio is 0.22.
(3) \emph{Squirrel} \cite{rozemberczki2021multi} consists of 5,201 Wikipedia pages discussing specific topics. The pages are divided into five categories based on their average monthly traffic. This dataset forms a page-to-page network with 217,073 edges, and the node features are derived from various informative nouns present in the Wikipedia content. %The homophily ratio is 0.30.

\begin{figure}[t]
\centering 
\includegraphics[width=1\linewidth]{figures/alpha.pdf}%
 \vspace{-4mm}%
\caption{Sensitivity study of $\alpha$ and $\beta$.}
\label{fig.hyperpara}
\end{figure}

\subsection{Hyperparameter Sensitivity}\label{app.hyperpara}
We investigate the impact of hyperparameters, $\alpha$ and $\beta$, in \model. $\alpha$ governs the fusion of feature and structure alignment, as well as their adaptation, in Eqs.~\eqref{eq.source-fusion} and~\eqref{eq.down-fusion}, whereas $\beta$ controls the aggregation of holistic and domain-specific adaptation in Eq.~\eqref{eq.structure-emb}. We vary $\alpha$ and $\beta$ and present 1-shot node and graph classification results on three target domain, \textit{Cora}, \textit{Photo} and \textit{Facebook}, in Fig.~\ref{fig.hyperpara}, with error bars denoting the standard deviation. 

We observe that increasing $\alpha$ from lower values initially enhances performance as structure alignment and adaptation are emphasized. However, after reaching a peak ($\alpha=1$), accuracy begins to decline as $\alpha$ grows further, implying that both feature and structure alignment are essential. 
%, further demonstrating the importance of our structural alignment design.
Moreover, $\beta$ exhibit a trend similar to that of $\alpha$, demonstrating that incorporating both holistic and domain-specific knowledge is vital for cross-domain adaptation.
Based on the above observations, we set $\alpha=1$ in our experiments, indicating a balance between the feature and structure counterparts, and $\beta=1$, indicating a balance between holistic and specific prompts, both of which show robust empirical performance.


\subsection{Data Ethics Statement}
To evaluate the efficacy of \model, we conducted experiments with only publicly available
datasets, including Cora\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/cora.npz}}, Citeseer\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/citeseer.npz}}, Pubmed\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/pubmed.npz}}, Photo\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_photo.npz}},  Computers\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz}}, 
Facebook\footnote{\url{https://graphmining.ai/datasets/ptg/facebook.npz}},
LastFM\footnote{\url{https://graphmining.ai/datasets/ptg/lastfm_asia.npz}},
Chameleon\footnote{\url{https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/chameleon}},
Cornell\footnote{\url{https://github.com/bingzhewei/geom-gcn/tree/master/new_data/cornell}}, and
Squirrel\footnote{\url{https://github.com/SitaoLuan/ACM-GNN/tree/main/new_data/squirrel}}
in accordance to their usage terms and conditions, if any.
We also confirm that no personally identifiable information was utilized, and this research did not involve any human or animal subjects.

