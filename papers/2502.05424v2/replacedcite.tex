\section{Related Work}
We review related literature on pre-training, cross-domain transfer learning, and multi-domain pre-training for graph data.

\stitle{Graph pre-training.}
Graph pre-training methods aim to extract inherent properties of graphs, often utilizing self-supervised learning approaches, which can be either generative ____ or contrastive ____. The pre-trained model is then employed to address downstream tasks through fine-tuning ____ or parameter-efficient adaptation methods, notably prompt-based learning ____. However, these methods typically assume that the pre-training and downstream graphs originate from the same domain, such as different subgraphs of a large graph ____ or collections of similar graphs within the same dataset ____, failing to account for multiple domains in either pre-training or downstream graphs.

\stitle{Graph cross-domain transfer.}
This line of work aims to transfer single-source domain knowledge to a different target domain by leveraging domain-invariant properties across domains____. However, they rely exclusively on a single source domain, failing to harness the extensive knowledge available across multiple domains. Additionally, these approaches are often tailored to specific tasks or domains ____, limiting their generalization.

\stitle{Multi-domain graph pre-training.}
In the context of graphs from multiple domains, recent works ____ utilize large language models to align node features from different domains through textual descriptions, thereby limiting their applicability to text-attributed graphs ____. For graphs without textual attributes, GraphControl ____ applies ControlNet____ to incorporate target domain node features with the pre-trained model, while neglecting the alignment among multiple source domains. Another recent study proposes GCOPE____, which employs domain-specific virtual nodes that interconnect nodes across domains, facilitating the alignment of feature distribution and homophily patterns. Meanwhile, MDGPT____ pre-trains domain-specific tokens to align feature semantics across various domains. However, these studies do not account for structural variance across different domains, hindering their effectiveness in integrating multi-domain knowledge. On a related front, multi-task pre-training techniques ____ employ pretext tokens for each pre-training task. It is important to note that they address a distinct problem, aiming to overcome potential interference among multiple tasks within a single domain, rather than interference across multiple domains. 
%In our work, we propose structure tokens and dual prompts to overcome the limitations of current multi-domain graph pre-training methods. 
%Note that multi-task pre-training aims to reduce interference among different pre-training tasks within a single domain, distinct from our objective of multi-domain pre-training.