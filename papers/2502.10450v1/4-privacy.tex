\section{Privacy}\label{sec:privacy}
In this section, we discuss the Membership Privacy Attack (MIA) in which an attacker tries to infer whether a sample belongs to the train set or not.
Common deep learning models are often vulnerable to such membership privacy attacks when they exhibit behavioral discrepancies between training and unseen data points. %These risks expose the training data information of the model. 
We discuss such privacy risks from two perspectives based on the current advancement. The first perspective is from the correlations between the capacity of the learning model and the complexity of training data points (and/or the set) regarding privacy. The other perspective is from privacy preservation and model generalizability.


\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figs/tmg_attack_stat.pdf}
         \caption{Privacy attack success rate}
         \label{fig:cmp_half_full_tmg}
     \end{subfigure}
     \hspace{0.1in}
     \begin{subfigure}[b]{0.21\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figs/tmg_ce_stat.pdf}
         \caption{Cross-entropy distribution}
         \label{fig:cmp_half_full_tmg_ce}
     \end{subfigure}
      \caption{Per-sample attack success rates and loss distribution in the original trainset and the half (MobileNetV3-S, 40 runs, TinyImageNet). Test accuracies parenthesized in the legends.}
      \vspace{-0.1cm}
  \label{fig:cmp_data_tmg}
\end{figure}



% model capacity vs. data complexity
\subsection{Privacy Correlation on Model Capacity and Data Complexity}

The membership privacy risks of machine learning models are mainly caused by the model's memorization of the training data points. This means that over-memorization is one of the sources of privacy risks \cite{yeom2020overfitting}. \cite{carlini2022onion} claimed some data points must be more privacy-risky after the removal of original privacy-risky data points and retraining from scratch. This is mainly due to the relative changes between the model capacity and the data complexity.
\cite{tan2022parameters} found that excess model capacity (\textit{a.k.a.}, overparameterization) is another factor of privacy risks. Additionally, \cite{tan2023dimensionblessing} showed the larger-capacity model not only memorizes more on training data points than smaller networks but also memorizes faster (\emph{i.e.}, within fewer iterations). In fact, changing data complexity can also change the model's memorization behavior.
As shown in Fig.~\ref{fig:cmp_data_tmg}, we empirically find that increasing data capacity can prevent privacy leakage as utilizing the entire dataset shows much better privacy preservation than utilizing only the half, which implies that the model may have well-concealed privacy under proper data complexity. 
Since a lower-capacity model (considering the data complexity) can protect privacy better, the sparsity of the model can also be beneficial to privacy. \cite{kaya2020effectivenessregularizationmembership} showed that regularization can mitigate some privacy risks while data augmentation techniques also help with privacy. The role of data augmentation was further studied and it was pointed out that only specific data augmentation techniques have such ability to mitigate privacy risks \cite{kaya2021whendataaug,yu2021howdoesdataaug}. In addition, \cite{yuan2022miapruning} found that traditional model pruning techniques do not work as well as the layer-wise architectural changes of the model in terms of reducing model capacities for privacy.
Besides classification models, such privacy risk led by improper memorization also widely exists in models that are trained in various forms, e.g., regression learning \cite{tarun2023regression_ua} and self-supervised learning \cite{wang2024localizing}. 





% \begin{figure}
% \vspace{-0.4cm}
%   \begin{center}
%   \subfloat[Privacy attack success rate]{
%     \label{fig:cmp_half_full_tmg}
%     \includegraphics[width=0.45\textwidth]{figs/tmg_attack_stat.pdf}
%   }
%   \subfloat[Cross-entropy distribution]{
%     \label{fig:cmp_half_full_tmg_ce}
%     \includegraphics[width=0.45\textwidth]{figs/tmg_ce_stat.pdf}
%   }
%   \end{center}
%   \caption{Per-sample attack success rates and loss distribution in the original trainset and the half (MobileNetV3-S, 40 runs).}
%   \label{fig:cmp_data_tmg}
%   \vspace{-0.1cm}
% \end{figure}






\begin{figure}[t]
\begin{center}
\begin{subfigure}[b]{0.19\textwidth}
   \centering
   \includegraphics[width=\textwidth]{figs/d2b_mia_train.png}
   \caption{Member (train)}
   \label{fig:d2db_mia_train}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.19\textwidth}
    \centering
   \includegraphics[width=\textwidth]{figs/d2b_mia_test.png}
   \caption{Non-Member (test)}
   \label{fig:d2db_mia_test}
\end{subfigure}
\\
\begin{subfigure}[b]{0.19\textwidth}
         \centering
   \includegraphics[width=\textwidth]{figs/d2o_d2b_train.png}
   \caption{Member (train)}
   \label{fig:d2o_d2db_train}
\end{subfigure}
\quad
\begin{subfigure}[b]{0.19\textwidth}
         \centering
   \includegraphics[width=\textwidth]{figs/d2o_d2b_test.png}
   \caption{Non-Member (test)}
   \label{fig:d2o_d2db_test}
\end{subfigure}
\end{center}
\caption{\textbf{[1st row]}: the distance to the decision boundary and MIAs accuracy; \textbf{[2nd row]}: the distance to the origin and the distance to the decision boundary. For a sample's distance to the decision boundary, we use the difference between 1st and 2nd maximum prediction probabilities. The results are obtained from dozens of independent experiments. The blue charts ((a) \& (c)) are from train set, and the green charts ((b) \& (d)) are from test set. (ResNet18, CIFAR-100). This figure is excerpted from \protect\cite{fang2024representation}.}
\vspace{-0.1cm}
\label{fig:d2o_d2db_mia}
\end{figure}




% privacy vs. generalizability
\subsection{Trade-Offs Between Privacy Preservation and Generalizability}

The behavioral inconsistency of deep learning models in training and testing time, i.e., bad generalizability, leads to privacy-leakage problems. The attacker can steal various information from highly valued samples used to train the model according to this inconsistency. 


To show the relationship between representation inconsistency and MIAs accuracy, we visualize the sample-level distribution of the training and testing sets. Fig.~\ref{fig:d2o_d2db_mia} displays the sample-level predictions of MIAs accuracy versus distance to the decision boundary, as well as the relationship between distance to the origin and distance to the decision boundary. The distance to the decision boundary and the distance to the origin are computed from the last and the penultimate layers, respectively. When trained with the standard cross-entropy loss, the model exhibits distinct prediction and attack distributions for members and non-members in both of the layers, indicating that there are multiple privacy-risky layers in the model due to disagreement of representation alignment.


Hence, a straightforward way to mitigate privacy vulnerability is to align the predictions (and representations) between training and testing sets. In the following paragraphs, the introduced approaches try to achieve this alignment goal from different aspects.
In this section, we categorize them into three categories: the model-level solutions, the external obfuscators, and the data-level solutions. The approaches are overviewed in Fig.~\ref{fig:overview_privacy_defence}.


\begin{figure}[t]
     \centering
         \includegraphics[width=0.92\linewidth]{figs/privacy_approaches.pdf}
         \vspace{-0.1cm}
      \caption{The overview of the privacy preservation approaches.}
      \vspace{-0.2cm}
  \label{fig:overview_privacy_defence}
\end{figure}


% model level
\paragraph{Model-Level Solutions}
The model-level solutions aim to develop a mechanism to make the prediction distributions aligned on the model's end. 
A classical model-level solution is differentially private stochastic gradient descent (\texttt{DP-SGD}) \cite{abadi2016dpsgd}. It adds the noise into the optimizer to prevent the model from taking the (undesirable) easiest way to fit on the training data points and also memorizing them.
\cite{nasr2018advreg} introduced an adversarial training framework (\texttt{AdvReg}) that mitigates membership inference attacks by aligning prediction distributions. It tries to develop a discriminator, similar to GAN, to identify the prediction inconsistency of the model while it makes the model try to deceive the discriminator to achieve prediction alignment.
\cite{chen2022relaxloss} (\texttt{RelaxLoss}) established a threshold to prevent improper fitting for the alignment between member and non-member distributions while preserving the model's generalizability through a technique similar to label smoothing.
\cite{tan2023ws} proposed weighted smoothing (\texttt{WS}) to mitigate memorization by adding normalized random noise to the weights.
\cite{liu2024ccloss} incorporated a concave term called Convex-Concave Loss (\texttt{CCLoss}) to lessen the convexity of loss functions, aiming to enhance privacy preservation.
Besides end-to-end solutions, there are also some studies exploring finer-grained solutions.
\cite{fang2024representation} introduced the Saturn Ring Classification Module (\texttt{SRCM}) to bound the representation magnitude to mitigate prediction disparity.
\cite{fang2024crl} tried to align representations in multiple layers by Center-based Relaxed Learning (\texttt{CRL}).
\cite{hu2024past} proposed Privacy-Aware Sparsity Tuning (\texttt{PAST}) to measure weight-level privacy sensitivity and deactivate privacy-risky weights via regularization. 



% Obfuscator
\paragraph{External Obfuscator}
The external obfuscator is a special kind of model-level solution. Instead of developing a privacy-safe model, it aims to build an obfuscator, which reproduces the prediction probabilities, to remedy the inconsistency in the prediction probabilities.
Similar to the idea of \texttt{DP-SGD}, \texttt{MemGuard} \cite{jia2019memguard} interferes with the prediction confidence distribution of the model by adding additional noise after the model has been trained. 
\cite{yang2023purifier} tried to develop a VAE-based external prediction obfuscator named \texttt{Purifier} to align the prediction probabilities' disparity. Different from \texttt{MemGuard}, it tries to reconstruct the prediction confidences to remove the prediction inconsistency instead of noise confusion.


% data level
\paragraph{Data-Level Solutions}
The data level approaches have two principles: training the privacy-safe model via \textbf{(i) privacy-safe data} or \textbf{(ii) privacy-safe labels}. It is straightforward that when all training data points are privacy-safe, there are no privacy-risky features included in the data, such as shortcut features \cite{geirhos2020shortcut}.


The most straightforward solution to produce privacy-safer data is \texttt{Data Augmentation}. There are some data augmentation techniques, such as random cropping and flipping, determined that are able to produce privacy-safer data \cite{kaya2021whendataaug,yu2021howdoesdataaug}. With augmented data, the model can usually achieve better privacy and generalizability. However, there are still no quantifiable metrics to measure how to further produce privacy-safe data through data augmentation. In other words, although the machine learning model can obtain privacy for free via data augmentation, it is unclear if the model achieves complete privacy safety yet.
\cite{stadler2022groundhogday} tried to analyze the effect of synthetic data on the model's privacy (\texttt{Data Synthesis}). 
Besides these two kinds of solutions, there is also an intuitive way to mitigate privacy risks. The first one is \texttt{Data Pruning}. With data pruning techniques, the model can use only a small amount of training data points to develop a well-generalized model. In other words, most membership privacy of the entire train set can still be protected. However, the privacy risk mitigation by data pruning is still not perfect, because some membership information of pruned data points could be leaked \cite{li2024datalineageinferenceuncovering}. The other one is \texttt{Data Distillation}. The data distillation aims to refine generalizability-critical features to produce some representative synthetic data. In this process, privacy-risky features can be removed \cite{dong2022privacy_dataset_condensation}. As this direction has not been extensively researched yet, it is foreseen that more studies will be contributed to this topic very soon.


Since the prediction disparity in training and testing is due to the improper fitting of the training data points, another way is to stop the model from further fitting into the data when it has learned enough information from the data. This means that if an ideal set of labels exists, the model can be trained perfectly privacy-safe on these labeled data. An intuitive idea is a distillation approach for membership privacy (\texttt{DMP}) \cite{shejwalkar2021dmp}. It trains a protected model via non-member data and produces labels from an unprotected model.
Another solution to better utilize limited data is self-ensemble architecture (\texttt{SELENA}) \cite{tang2022selena}. It developed an ensemble with an efficient sampling strategy to produce privacy-safe labels with better generalizability.

