%\clearpage
\section{Safety Alignment in LLMs}\label{sec:safety}

In Large Language Models, alignment aims to teach models human-desired behaviors and remove undesired behaviors. Safety alignment has often been treated as a subset of broader alignment challenges, with a primary focus on safety~\cite{li2024superficial}. In this context, the goal of safety alignment is preventing LLMs from generating toxic or harmful content and simultaneously considers security problems in adversarial scenarios, such as jailbreak attempts~\cite{qi2024ai}. 
% Since safety alignment has this specifically dedicated scope, unlike general alignment challenges, it has unique properties for the primary focus on safety.

\subsection{Why is Safety in LLMs Important?}

With the release of AI services such as ChatGPT, Claude, and Gemini, AI-powered applications have become increasingly integrated into our daily lives, spanning fields like healthcare, finance, education, transportation, and even military applications~\cite{openai2024chatgpt,team2023gemini}. However, this rapid adoption also raises serious concerns regarding AI misuse. For instance, in 2023, the first suspected case of AI-assisted suicide was reported~\cite{brusselstimes2023ai}, and AI-generated misinformation has been widely disseminated online, potentially manipulating public opinion~\cite{monteith2024artificial}. These incidents compel us to critically examine how to prevent AI from being misused in ways that harm society. 

This challenge has become even more pressing with the rise of open-sourced large language models such as Llama and Deepseek families~\cite{touvron2023llama,dubey2024llama,guo2025deepseek}, which allow individuals to control and finetune LLMs directly. As a result, the risks associated with AI misuse are expanding exponentially while government regulatory measures struggle to keep pace. Given these developments, AI safety has become a pressing need in the current research community.

\subsection{How to Implement Safety in LLMs?} %late 2022 to late 2023

% GPT-3 2020 powerful, 
% bias, safety ..... Discussione by news or researer, 
% OpenAI can't release it (GPT-3) to the public

% prevent direct attacks
Upon the release of GPT-3 in 2020, we witnessed LLM's remarkable language generation capabilities. However, concerns regarding bias, toxic content, and hallucinations also emerged, indicating that the model was still not ready for public release~\cite{brown2020language}. 
% However, even before LLMs were introduced to the public, concerns regarding bias, toxic content, and hallucinations also emerged, which highlighted the challenges.
Later, in 2021, Anthropic introduced the HHH principle—Helpful, Honest, and Harmless—as a key principle of a truly beneficial AI assistant~\cite{askell2021general}. This concept set a foundational standard for AI assistants, clearly indicating that safety is an important objective of LLMs.

The launch of ChatGPT in late 2022 marked a turning point, as it was the first large-scale exposure of LLMs to the public, allowing people to experience an AI assistant that felt genuinely helpful and capable of human-like reasoning. This breakthrough was largely built upon the techniques outlined in the InstructGPT, which introduced Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) as key methods for aligning LLMs with human values~\cite{ouyang2022training}. Around the same time, Anthropic also released its own research on RLHF, demonstrating its effectiveness in guiding model behavior~\cite{bai2022training}. These efforts treated safety (or harmlessness) as a subset of preference optimization, laying the foundation for future safety alignment research. From a high-level perspective, subsequent alignment techniques can be approximately categorized into the following approaches:  

\paragraph{In-Context Learning}  
This category of methods does not rely on model retraining; instead, inspired by Chain-of-Thought (CoT) reasoning and GPT-3, researchers have designed either hard prompts or soft prompts to guide the model toward producing helpful and harmless outputs~\cite{wei2022chain,brown2020language}. Examples include the official system prompt in Llama2 and soft prompts optimized via P-tuning, which embed safety reasoning signals that may not be readable by humans~\cite{touvron2023llama,xie2023defending}. 
However, in-context learning has several limitations: \textbf{(1)} It requires careful manual design and optimization, making automation and scalability difficult. \textbf{(2)} It has limited generalization ability, struggling to handle long-tail scenarios where prompts may not be well-defined. \textbf{(3)} It is highly sensitive to prompt design; small variations in the prompt can lead to drastically different outputs, making the method vulnerable to jailbreak attacks. \textbf{(4)} Since in-context learning does not modify the model's weights, it cannot permanently alter its underlying behavior.  

\paragraph{Imitation Learning}  
This approach primarily relies on supervised fine-tuning (SFT) to train models using carefully curated aligned datasets~\cite{ouyang2022training}.~\cite{zhou2024lima} introduced the Superficial Alignment Hypothesis, suggesting that alignment may merely adjust the model’s output distribution to be more interaction-friendly rather than fundamentally changing its reasoning capabilities. They demonstrated that with only about 1k high-quality training examples, a model could achieve performance comparable to GPT-4. 
% This result suggests that large language models already acquire substantial human interaction knowledge during pretraining, and the alignment process might serve more to refine the style of responses rather than deeply alter the model’s reasoning mechanisms. 
However, this paper focused predominantly on helpfulness, and its training data contained only 13 safety-related samples, which led to poor overall safety performance.

\paragraph{Reinforcement Learning}  
Reinforcement Learning with Human Feedback (RLHF) improves model alignment by optimizing the policy (parameters) using reinforcement learning. The core idea is to train the model to generate better responses by maximizing a reward function, which is defined by a reward model trained on human feedback. The reward model assigns scores to different responses, providing a structured signal to guide the optimization process. To prevent the model from diverging too far from its original behavior, RLHF typically employs proximal policy optimization (PPO), which introduces a KL divergence constraint between the logits of the original and updated policies~\cite{christiano2017deep}. This ensures that while the model learns to produce more aligned outputs, it does not lose fluency or develop unintended artifacts. RLHF has significantly improved both helpfulness and harmlessness, establishing itself as a foundational technique in alignment research~\cite{ouyang2022training,bai2022training}. Despite its effectiveness, RLHF comes with significant challenges. The dependence on human feedback makes it highly labor-intensive, as continuous human involvement is required to annotate responses and update the reward model. To reduce reliance on human labor and improve scalability, researchers have proposed Reinforcement Learning with AI Feedback (RLAIF) as an alternative. For example, Anthropic incorporates an AI-generated feedback mechanism, where the model itself evaluates responses, reducing the need for human intervention while still refining behavior and mitigating harmful outputs~\cite{bai2022constitutional}. However, both RLHF and RLAF are resources intensive, as their training typically involves the following components: (1) a reference model, (2) a policy model (an adapted version of the base model being optimized), and (3) a reward model trained to assess response quality. In some implementations, additional models may even be required, further increasing the memory and computational burden~\cite{yao2023deepspeed}. 

\paragraph{Contrastive Learning}  
Researchers have explored contrastive learning as a more efficient alternative to reduce the complexity and resource demands of RL-based approaches. Direct Preference Optimization (DPO) introduced the key insight that large language models inherently act as implicit reward models~\cite{rafailov2024direct}. By leveraging the preference dataset, a model can directly learn preferences without requiring an explicit reinforcement learning loop and a reward model. A similar approach is proposed in~\cite{liu2023training}, where contrastive signals are embedded in datasets to steer models toward desired behaviors. This category of methods has notable advantages: (1) it significantly reduces memory costs since optimization requires loading at most two models at a time, and with logit caching, only one model may be sufficient; (2) it eliminates the need for an explicit reward model, simplifying the alignment process. However, these methods also come with challenges: the performance is highly dependent on the quality of the preference dataset.
% —if the dataset is insufficient or biased, the model may fail to learn appropriate alignment signals.

\paragraph{Conditional Learning}  
This approach is conceptually similar to in-context learning but differs in that it explicitly optimizes the model to recognize specific triggers, ensuring that desired behaviors are always generated when these triggers are present. The key idea is to induce the model to produce desired behavior rather than removing undesired outputs. However, this approach has significant vulnerabilities when confronted with jailbreak attacks and thus is rarely used as a standalone alignment technique~\cite{korbak2023pretraining}. 


\subsection{Safety in Existing LLMs is Still Brittle} % late 2023 to middle 2024

% fine-tuning attack
% Jailbreak attack
% Decoding exploitation attack
% Superficiality

Although various general alignment methods have been proposed, and safety alignment has been improved to some extent, treating safety merely as a subset of human preference overlooks its unique challenges. As a result, current alignment techniques remain vulnerable to adversarial attacks. In literature, adversarial attacks on LLMs can generally be classified into three types: 
% \textbf{(1) Direct Attacks}: The model is explicitly prompted with malicious requests to induce harmful outputs. While aligned models can resist many such attacks, failure cases persist, especially in previously unseen scenarios. 
\textbf{(1) Jailbreak Attacks}: Attackers exploit techniques such as role-playing or suffix injections to bypass safety guardrails and manipulate the model into generating harmful content. Studies show that these methods can effectively evade existing alignment mechanisms~\cite{zouuniversal}. This vulnerability extends beyond open-source models—even state-of-the-art systems like the GPT-4 series struggle to block harmful outputs in complex, nested scenarios consistently~\cite{li2023deepinception}. \textbf{(2) Finetuning Attacks}: Even unintentional finetuning can weaken a model’s safety mechanisms. A model trained with safety alignment may gradually lose its safeguards when adapted to downstream tasks via domain-specific finetuning, even if the dataset itself is benign. This phenomenon has been observed in both open-source and proprietary models~\cite{qi2023fine}. \textbf{(3) Decoding Attacks}: Safety-aligned models may still produce harmful content under certain decoding settings, such as modifications to Top-P, Top-K, or Temperature~\cite{huang2023catastrophic}. These variations may break built-in safeguards, leading to outputs that would otherwise be restricted under default configurations. These attack vectors underscore a critical issue: existing safety alignment methods lack robustness and, in many cases, remain highly brittle, especially in novel or adversarial conditions.  

\subsection{How to Implement Robust Safety in LLMs?} % middle 2024 to now

Recent studies have highlighted that existing alignment methods often achieve safety at a superficial level. \cite{wei2024assessing} identified safety-critical parameters in LLMs and found that removing them catastrophically degrades safety performance while leaving utility performance unaffected. However, their findings also revealed that merely retaining these safety-critical parameters does not preserve safety under finetuning attacks. In contrast, \cite{li2024superficial} demonstrated that the atomic functional unit for safety in LLMs resides at the \emph{neuron level} and successfully mitigated finetuning attacks by freezing updates to these safety-critical components. Their study further showed that aligned models remain vulnerable to finetuning attacks because key attributes, such as utility, can be achieved by repurposing neurons originally responsible for other functions, such as safety. Additionally, this research examined how alignment influences model behavior in safety-critical contexts and observed that, at its core, this effect could be framed as an implicit safety-related binary classification task. To resolve the superficiality issue above, they further propose that alignment should enable models to choose the correct safety-aware reasoning direction (either to refuse or fulfill) at each generation step, ensuring safety throughout the entire response. 
However, their work did not propose specific methods for implementing this deeper safety mechanism in practice.

~\cite{qi2024ai} have also examined the shallow alignment in existing LLMs and found that this issue often stems from alignment disproportionately affecting early-generated token distribution. This creates optimization shortcuts where models rely on superficial decision patterns, leading them toward local optima that fail to generalize to more complex safety challenges. To mitigate this, they introduced a data augmentation strategy designed to expose models to more nuanced scenarios where an initially harmful response later transitions into a safe refusal. Similarly,~\cite{yuan2024refuse} have adopted more aggressive data construction rules, aiming to add more variety of training examples. However, while these methods increase the diversity of training examples, they do not fundamentally address the root problem. All of these highlight a critical issue: Existing alignment techniques lack effective and robust mechanisms to handle complex and nuanced harmful reasoning patterns. In this context, this survey paper acknowledges the hypothesis from~\cite{li2024superficial}, and believes that a robust safety alignment should teach the model to select and maintain the correct safety reasoning direction throughout the entire text generation process.