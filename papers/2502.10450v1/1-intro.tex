\section{Introduction}\label{sec:intro}



% Replacement Paragraph

The value of AI systems lies in their ability to generalize to unseen situations - if one already knows what their model is going to encounter, they must program it instead of training them to learn generalizable patterns. The standard method to assess how well a model is able to generalize is to measure how well it does on an unseen dataset drawn from the same distribution as the data the model is trained on. However, this provides an incomplete picture regarding the trustworthiness/reliability of these models in the real world. In this paper, we discuss three critical aspects of deep learning that allow for a greater understanding of the trustworthiness of AI systems: safety, bias, and privacy.

%Safety - Jianwei
% In particular, as for safety, we discuss safety alignment in LLMs, introduce the current status of the field, and convey our perspectives. Unlike general alignment challenges, safety alignment has its distinctive properties to prevent LLMs from generating toxic or harmful content against adversarial attacks. ............

With the rapid adoption of large language models (LLMs) in fields such as healthcare, finance, and cybersecurity, ensuring their safe deployment has become a pressing concern. While LLMs offer immense potential, their misuse—whether intentional or accidental—can lead to severe societal consequences, such as misinformation propagation, security vulnerabilities, and ethical risks. Safety alignment aims to mitigate these issues by preventing LLMs from generating harmful or unethical content. Over the recent years, many techniques have been developed to improve safety, including supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), etc. These methods have significantly enhanced LLM alignment, enabling models to better adhere to human-defined safety constraints. However, challenges remain in ensuring their robustness across adversarial scenarios, as existing approaches often struggle with various jailbreak attacks, fine-tuning exploits, and decoding manipulations. Addressing these limitations requires a deeper understanding of safety alignment strategies and the development of techniques that can establish safety guardrails throughout the text generation.


%Bias - Varun
As for issues concerning bias in deep learning, we focus on the problem of spurious correlations, where a network primarily relies on weakly predictive features in the training set that are causally unrelated to ground truth labels in classification tasks. Reliance on these features is undesirable as they may disappear or become associated with a different task during testing. To overcome the reliance on these features, many promising solutions have been recently proposed. In this paper, we study these techniques in detail and understand their limitations while introducing studies of new directions to overcome these limitations. We also discuss the intersection of spurious correlations with other fields of study within deep learning.


%Privacy - Xingli
For the last, we discuss privacy issues in deep learning models, especially for membership inference attacks where an attacker tries to infer whether a sample belongs to a train set or not - which is membership information.
Existing deep learning models are often vulnerable to such attacks when they exhibit behavioral discrepancies between training and unseen data points. Once a model is under such an attack, it is disclosed whether a data point has been involved in training the model. To avoid such privacy leakage, many solutions from various perspectives have been proposed. In this paper, we discuss the privacy vulnerabilities in terms of the correlations between model capacity and data complexity. We also comprehensively present the existing privacy preservation approaches and discuss their potential future directions.



This paper provides a comprehensive survey and discussions regarding trustworthy AI, especially safety, bias, and privacy, which will contribute to the research community for further actionable works and also provide insights to the fields outside of AI/deep learning.