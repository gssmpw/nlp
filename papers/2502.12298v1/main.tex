
\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}

\let\classAND\AND
\let\AND\relax

\usepackage{algorithmic}

\let\algoAND\AND
\let\AND\classAND


\AtBeginEnvironment{algorithmic}{\let\AND\algoAND}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\usepackage{adjustbox}
\usepackage{subfig}

% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}




\title{Symmetric Rank-One Quasi-Newton Methods for Deep Learning Using Cubic Regularization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{%
  Aditya Ranganath \\
  Center for Applied Scientific Computing\\
  Lawrence Livermore National Laboratory\\
  7000 East Avenue, 
  Livermore, CA 94550 \\
  \texttt{ranganath2@llnl.gov} 
  \AND
  Mukesh Singhal \\
  Electrical Engineering and Computer Science \\
  University of California, Merced\\
  5200 N Lake Road\\
  Merced, CA 95343 \\
  \texttt{msinghal@ucmerced.edu}
  \AND
  Roummel Marcia \\
  Applied Mathematics \\
  University of California, Merced\\
  5200 N Lake Road \\
  Merced, CA 95343 \\
  \texttt{rmarcia@ucmerced.edu}
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle



\begin{abstract}
Stochastic gradient descent and other first-order variants, such as Adam and AdaGrad, are commonly used in the field of deep learning due to their computational efficiency and low-storage memory requirements. However, these methods do not exploit curvature information. Consequently, iterates can converge to saddle points or poor local minima. On the other hand, Quasi-Newton methods compute Hessian approximations which exploit this information with a comparable computational budget. Quasi-Newton methods re-use previously computed iterates and gradients to compute a low-rank structured update. The most widely used quasi-Newton update is the L-BFGS, which guarantees a positive semi-definite Hessian approximation, making it suitable in a line search setting. However, the loss functions in DNNs are non-convex, where the Hessian is potentially non-positive definite. In this paper, we propose using a limited-memory symmetric rank-one quasi-Newton approach which allows for indefinite Hessian approximations, enabling directions of negative curvature to be exploited. Furthermore, we use a modified adaptive regularized cubics approach, which generates a sequence of cubic subproblems that have closed-form solutions with suitable  regularization choices. We investigate the performance of our proposed method on autoencoders and feed-forward neural network models and compare our approach to state-of-the-art first-order adaptive stochastic methods as well as other quasi-Newton methods.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{Sections/Intro}

\noindent\section{Proposed approach
%Adaptive Regularization using Cubics with L-SR1 Updates
}
\label{sec:ProposedApproach}
\input{Sections/LimitedSimRank1}

\section{Results}
\label{sec:Experiments}
\input{Sections/Experiments}
%
% \section{Results}
\label{sec:Results}
\input{Sections/Results}


\section{Conclusion}
\label{sec:Conclusion}
\input{Sections/Conclusion}

\section{Acknowledgments}
\label{sec:Ack}
\input{Sections/Ack.tex}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{refs}
\bibliographystyle{tmlr}


\end{document}
