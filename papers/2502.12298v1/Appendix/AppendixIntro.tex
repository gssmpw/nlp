Here, we present some additional results to demonstrate the performance of the proposed approach. We analyse the performance of the proposed approach using different batch-sizes, different iterations for each approach in SdLBFGS, LBFGS and the proposed approach. We do a deep dive on each method's efficacy using MNIST classification as our experiment.




\noindent \textbf{Optimization approaches.} 
We list the various optimization approaches to which we compared our proposed method.  For the numerical experiments, we empirically fine-tuned the hyperparameters and select the best for each update scheme.

\begin{enumerate}

\item \textbf{Stochastic Gradient Descent (SGD) with Momentum}  (see e.g., \cite{Qia99}).
%A gradient-descent algorithm that uses (i) an estimate of the gradient calculated from a randomly selected subset of the dataset, and (ii) a moving average of these gradient approximations .  
For the experiments, we used a momentum parameter of $0.9$ and a learning rate of $1.0 \times 10^{-1}$.

\item 
\textbf{Adaptive Gradient Algorithm (Adagrad)} 
%An algorithm similar to SGD but with an adaptive learning rate for each dimension at each iteration 
\cite{duchi2011adaptive}.  
In our experiments, the initial accumulator value is set to 0, the perturbation $\epsilon$ is set to $1.0 \times 10^{-10}$, and the learning rate is set to $1.0 \times 10^{-2}$.


\item \textbf{Root Mean Square Propagation (RMSProp)} 
%An algorithm similar to Adagrad but decays the contribution of older gradients at each iteration 
\cite{hinton2012neural}.  For our experiments, the perturbation $\epsilon$ is set  to $1.0 \times 10^{-8}$. We set $\alpha = 0.99$, and used a learning rate of $1.0 \times 10^{-2}$.

\item \textbf{Adam} 
%Related to RMSProp, this algorithm generates its parameter updates using a running average of first and second moment of the gradient 
\cite{kingma2014adam}. For  our experiments, we apply an $\epsilon$ perturbation of $1.0 \times 10^{-6}$. The momentum parameters $\beta_0$ and $\beta_1$ are chosen to be 0.9 and 0.999, respectively.  The learning rate is set to $1.0 \times 10^{-3}$. 


\item \textbf{Limited-memory BFGS (L-BFGS)}: We set the default learning rate to $1.0$. The tolerance on function value/parameter change is set to $1.0 \times 10^{-9}$ and the first-order optimality condition for termination is defined as $1.0 \times 10^{-9}$

\item  \textbf{ARCs-LSR1 (Proposed method)}: For the experiments, we choose the same parameters as those used in L-BFGS. We also provide the performance of this approach with different set of parameter for the MNIST classification in the appendix section.


\end{enumerate}