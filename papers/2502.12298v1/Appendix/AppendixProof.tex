\noindent 
Because full gradient computation is very expensive to perform, we impement a stochastic version 
of the proposed ARCs-LSR1 method.  In particular, we use the batch gradient approximation
$$
	\tilde{\mathbf{g}}_k \equiv \frac{1}{| \mathcal{B}_k |} \sum_{i \in \mathcal{B}_k} \nabla f_i (\Theta_k).
$$
In defining the SR1 matrix, we use the quasi-Newton pairs $(\mathbf{s}_k, \tilde{\mathbf{y}}_k)$,
where $\tilde{\mathbf{y}}_k = \tilde{\mathbf{g}}_{k+1} - \tilde{\mathbf{g}}_k$ (see e.g., \cite{Erway2020TrustregionAF}).
We make the following additional assumption (similar to Assumption 4 in  \cite{Erway2020TrustregionAF}) to guarantee that the loss function $f(\Theta)$ decreases over time:

\medskip

\noindent
\textbf{A4.} The loss function $f(\Theta)$ is fully evaluated at every $J > 1$ iterations (for example, 
at iterates $\Theta_{J_0}, \Theta_{J_1}, \Theta_{J_2}, \dots,$ where $0 \le J_0 < J$ and
$J = J_1 - J_0 = J_2 - J_1 = \cdots $) and nowhere else in the algorithm.  The batch size $d$ is increased 
monotonically if $f(\Theta_{J_{\ell}}) > f(\Theta_{J_{\ell - 1}}) - \tau$ for some $\tau > 0$.

\medskip

\noindent 
With this added assumption, we can show that the stochastic version of the proposed ARCs-LSR1 method converges.

\begin{theorem}\label{thm:sARCs}
	The stochastic version of ARCs-LSR1 converges with  
	$$\underset{k \to \infty}{\text{lim}} \|\mathbf{g}_k\| = 0.$$
\end{theorem}

\textit{Proof:} Let $\widehat{\Theta}_i = \Theta_{J_i}$.  By Assumption 4, $f(\Theta)$ must 
decrease monotonically over the subsequence $\{ \widehat{\Theta}_i \}$ or $d \rightarrow |\mathcal{D}|$,
where $|\mathcal{D}|$ is the size of the dataset.    If the objective function is decreased 
$\iota_k$ times over the subsequence $ \{ \widehat{\Theta}_i\}_{i=0}^k$, then
%\begin{eqnarray*}
%	f(\widehat{\Theta}_k) &=& f(\hat{\Theta}_0) + \sum_{i=1}^{\iota_k}
%	\left \{
%		f(\widehat{\Theta}_i) - f(\widehat{\Theta}_{i-1})
%	\right \} \\
%	&\le& f(\widehat{\Theta}_0) - \iota_k \tau.
%\end{eqnarray*}
\begin{eqnarray*}
	f(\widehat{\Theta}_k) = f(\hat{\Theta}_0)  + \sum_{i=1}^{\iota_k}
	\left \{
		f(\widehat{\Theta}_i)  -  f(\widehat{\Theta}_{i-1})
	\right \} \le  f(\widehat{\Theta}_0) - \iota_k \tau.
\end{eqnarray*}
If $d \rightarrow |\mathcal{D}|$, then $\iota_k \rightarrow \infty$ as $k \rightarrow \infty$.
By Assumption \textbf{A2}, $f(\Theta)$ is bounded below, which implies $\iota_k$ is finite.  
Thus, $d \rightarrow |\mathcal{D}|$, and the algorithm reduces to the full ARCs-LSR1 method,
whose convergence is guaranteed by Corollary \ref{cor:ARCs}.  $\square$

\medskip

\noindent 
We note that the proof to Theorem \ref{thm:sARCs} follows very closely the proof of Theorem 2.2 in 
\cite{Erway2020TrustregionAF}.


\medskip