In this paper, we proposed a novel quasi-Newton approach in an adaptive regularized cubics (ARCs) setting 
using the less frequently used limited-memory Symmetric Rank-1 (L-SR1) update and a shape-changing norm to define the regularizer. This shape-changing norm allowed us to solve for the minimizer exactly.
We provided convergence guarantees for the proposed ARCs-LSR1 method and 
analyzed its computational complexity.  Using a set of experiments in classification, image reconstruction, and language modeling, we demonstrated that ARCs-LSR1 achieves the highest accuracy in fewer epochs than a variety of existing state-of-the-art optimization methods. Striking a comfortable balance between the computational and space complexity, the competitive nature of the ARCs-LSR1 performance makes it a superior alternative to existing gradient and quasi-Newton based approaches.

%\section*{Acknowledgments}

%This research work was partially funded by NSF Grants IIS 1741490 and DMS 1840265.



%were able to empirically and theoretically show how an L-SR1 quasi-Newton approximation in an ARCs setting was able to perform either better or comparably to most of the state of the art optimization schemes. 
%Even though the approach has yielded exceptional results, we need to test the method's efficacy when the network size and dataset size is large and when availability of data is sparse. %We would also like to explore the proposed stochastic version with batch overlaps.