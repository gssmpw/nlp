Deep learning problems often involve training deep neural networks (DNN) by 
minimizing an empirical risk of estimation, given by 
\begin{equation}\label{eq:emp}
	\underset{\Theta \in \mathbb{R}^n}{\text{minimize}} 
	 \ f(\Theta)  = \frac{1}{N} \sum_{i=1}^N f_i (x_i, y_i; \Theta) 
\end{equation}
where $\Theta$ is the vector of weights and each $f_i$ is a scalar-valued loss function 
that depends on a vector of data inputs, $x_i \in \mathbb{R}^{n_1}$, and outputs, $y_i \in \mathbb{R}^{n_2}$.  
Here, $N$ corresponds to the cardinality of the data set $\mathcal{D} = \{ x_i, y_i \}$.
In this paper, we assume that $f$ is continuously differentiable.
%, and we 
%To solve (\ref{eq:emp}), various optimization approaches are used, which we describe below. Throughout this paper, 
%we write $f(\Theta)$ and $f(x; \Theta)$ interchangeably.

Gradient and adaptive gradient methods are the most widely used methods for 
solving (\ref{eq:emp}). In particular, stochastic gradient descent (SGD), despite its simplicity, performs well over a wide range of applications. However, in a sparse training data setting, SGD performs poorly due to limited training speed \citep{1902.09843}. To address this problem, \textit{adaptive} methods such as AdaGrad \citep{duchi2011adaptive}, AdaDelta \citep{zeiler2012adadelta}, RMSProp \citep{hinton2012neural} and Adam \citep{kingma2014adam} have been proposed. These methods take the root mean square of the past gradients to  influence the current step. %Amongst all of these adaptive methods, Adam is arguably the most widely used in a deep learning setting due to it rapid training speed.

In contrast, Newton's method has the potential to exploit curvature information from the second-order derivative (Hessian) matrix (see e.g., \citet{GouldLRMT00}).  Generally, the iterates are defined by $\Theta_{k+1} = \Theta_k - \alpha_k \nabla^2 f(\Theta_k)^{-1} \nabla f(\Theta_k)$, where $\alpha_k > 0$ is a steplength defined by a linesearch criterion \citep{NoceWrig06}.
%
%the step $s_k \overset{\text{def}}{=} \Theta_{k+1} - \Theta_k$ is given by $-H_k^{-1}g_k = s_k$, where $H_k \overset{\text{def}}{=} \nabla^2 f(\Theta_k)$ is the Hessian and $g_k\overset{\text{def}}{=}\nabla f(\Theta_k)$ is the gradient of the function.
In a DNN setting, the number of parameters ($n$) can be of the order of millions. Thus, full Hessians are rarely ever computed. Instead, Hessian-vector products and Hessian-free methods are used (see e.g., \citet{martens2010deep}, \citet{ranganath2021second}) which reduce the cost of storing the Hessian and inverting it.

Quasi-Newton methods  compute Hessian approximations, $\mathbf{B}_{k} \approx \nabla^2 f(\Theta_k)$, that satisfy the \emph{secant condition} given by $\mathbf{y}_{k-1} = \mathbf{B}_{k} \mathbf{s}_{k-1}$, where 
$$
\mathbf{s}_{k-1} = \Theta_{k} - \Theta_{k-1} \quad \text{and} \quad 
\mathbf{y}_{k-1} = \nabla f(\Theta_{k}) - \nabla f(\Theta_{k-1}).
$$
The most commonly used quasi-Newton method, including in the realm of deep learning, is the limited-memory BFGS update, or L-BFGS (see e.g., \citet{Liu1989}), where the Hessian approximation is given by
	\begin{equation}\label{eqn:LBFGS}
		\mathbf{B}_{k} = \mathbf{B}_{k-1} + \frac{\mathbf{y}_{k-1}\mathbf{y}_{k-1}^{\top}}{\mathbf{y}_{k-1}^{\top}\mathbf{s}_{k-1}} - \frac{\mathbf{B}_{k-1} \mathbf{s}_{k-1} \mathbf{s}_{k-1} \mathbf{B}_{k-1}^{\top}}{\mathbf{s}_{k-1}^{\top}\mathbf{B}_{k-1}\mathbf{s}_{k-1}}.	
	\end{equation} 
One advantage of using an L-BFGS update is that the Hessian approximation can be guaranteed to be positive definite. This is highly suitable in line-search settings because the update $\mathbf{s}_k$ is guaranteed to be a descent direction. This means there is some step length along this direction that results in a decrease in the objective function (see \citet{NoceWrig06}, Algorithm 6.1). Because the L-BFGS update is positive definite, it does not readily detect directions of negative curvature for avoiding saddle points.  In contrast, the Symmetric Rank-One (SR1) quasi-Newton update is not guarateed to be positive definite and can result in \emph{ascent} directions for line-search methods.  However, in trust-region settings where indefinite Hessian approximations are an advantage because they can capture directions of negative curvature, the limited-memory SR1 (L-SR1) has been shown to outperform L-BFGS in DNNs for classification (see \citet{Erway2020TrustregionAF}).  We discuss this in more detail in Section \ref{sec:ProposedApproach} but in the context of adaptive regularization using cubics (see e.g., \citet{NesP06}).  

\textbf{Contributions.}
The main contributions of this paper are as follows: 
(1) The use of the L-SR1 update to model potentially indefinite Hessians of the non-convex loss function;
(2) The use of adaptive regularization using cubics (ARCs) approach as an alternative to line-search and trust-region optimization methods;
(3) The use of a shape-changing norm to define the cubic regularization term, which allows us to compute the closed form solution to the cubic subproblem in the ARCs approach;
(4) Convergence proof of the proposed ARCs approach with L-SR1 Hessian approximations.
To the knowledge of the authors, \textbf{this is the first time} a quasi-Newton approach has been used in an adaptive regularized cubics setting. 
%\subsection{Contributions}
%The main contributions of this paper are as follows: 
%(1) The use of the L-SR1 update to model potentially indefinite Hessians of the non-convex loss function;
%(2) The use of adaptive regularization using cubics (ARCs) approach as an alternative to line-search and trust-region optimization methods;
%(3) The use of a shape-changing norm to define the cubic regularization term, which allows us to compute the closed form solution to the cubic subproblem in the ARCs approach;
%(4) Convergence proof of the proposed ARCs approach with L-SR1 Hessian approximations; 
%(5) Computational complexity analysis for the proposed method; and
%(6) Numerical experiments that compare our approach to state-of-the-art first-order adaptive stochastic methods as well as another quasi-Newton method.

%1.\ \text{L-SR1 quasi-Newton methods:} The most commonly used quasi-Newton approach is the L-BFGS method.  In this work, we use the L-SR1 update to better model potentially indefinite Hessians of the non-convex loss function. 
%2.\ \text{Adaptive Regularization using Cubics (ARCs):} Given that the quasi-Newton approximation is allowed to be indefinite, we use an Adaptive Regularized using Cubics approach to 
%safeguard each search direction.
%3.\ \text{Shape-changing regularizer:} We use a shape-changing norm to define the cubic regularization term, which allows us to compute the closed form solution to the cubic subproblem (\ref{eq:cr}).  
%4.\  \text{Computational complexity:} Let  $m$ be the number of previous iterates and gradients stored in memory. The proposed ARCs-LSR1 approach is comparable to L-BFGS in terms of storage and compute complexity 
%

%\subsection{Dedication}
%
%We dedicate this paper to Oleg P.\ Burdakov, whose work on shape-changing norms and quasi-Newton methods inspired this work.  






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%