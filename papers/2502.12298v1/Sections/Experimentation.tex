The following algorithm is used to run experiments on different datasets given in Section \ref{sec:Experiments}.

\begin{algorithm}
	\caption{Forward and backward pass of DNN for a single data-point}
	\begin{algorithmic}
		\State \textbf{Given} input ($x$,$y$), weights (and biases) $\Theta_{l}$ and activations $\sigma_l$ where $l \in [1,L]$ is the layer and $L$ is the number of layers
		\State  $\mathbf{a}_0 = x$; 
		\For {$l=1, \ldots, L$}{ $h_l = \Theta_l a_{l-1};$}{$a_l = \sigma_{l}(h_l)$}
		\EndFor
		\State {$\dot{a}_L \overset{\text{def}}{=}\nabla_z a_L \leftarrow \frac{\partial \mathcal{L}(z,y)}{\partial z}\rvert_{z=a_L}$}
		\For {$l=1, \ldots, L$} {$g_l = \dot{a}_l \bigodot \sigma'(h_l)$};{$ \dot{\Theta}_l = g_i \hat{a}^{\top}_{i-1};$}{$\dot{a}_{l-1} = \Theta_l^\top g_l$}
		\EndFor
	\end{algorithmic}\label{alg:DNN}
\end{algorithm}
We present the learning rates for the different algorithms used in Table (\ref{tbl:lr}):
\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\textbf{Method} & 
		Adam &  
		SGD & 
		Adagrad &
		RMSProp & 
		L-BFGS \\
		\hline
		\textbf{Learning rate} & 
		$1.0 \times 10^{-3}$ &
		$1.0 \times 10^{-1}$  & 
		$1.0 \times 10^{-2}$ & 
		$1.0 \times 10^{-2}$ & 
		$1.0 \times 10^0$ \\
		\hline
	\end{tabular}
	\caption{Learning rates for each update rule.}
	\label{tbl:lr}
\end{table}

For the MNIST dataset, the architecture is presented in detail in Table \ref{tbl:mnistnetwork}.
For Fashion-MNIST (FMNIST), the network is described in Table \ref{tbl:fmnist-network}.
For CIFAR10, the network architecture is described in Table \ref{tbl:CIFAR10-networks}.
For the autoencoder, we use the architecture presented in Table \ref{tbl:AutoEncMNIST}.
Finally, the same network architecture is used for reconstruction of FMNIST images.


\begin{table*}[!h]
	\centering
	\resizebox{0.6\textwidth}{!}{
		\begin{tabular}{c}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				\multicolumn{6}{|c|}{\small{MNIST network }}\\
				\hline\hline
				Layer& In & Out&Kernel& Stride&Padding\\
				\hline
				Linear & 784 & 500 & - & - & -\\
				ReLU & 500 & 500 & - & - & -\\
				Dropout & - & - & - & - &-\\
				Linear & 500 & 10 & - & - & -\\
				SoftMax & 10 & 10 & - & - & - \\
				\hline
			\end{tabular}\\
			\\[-.15cm]
	\end{tabular}}\caption{Network architecture for MNIST.}\label{tbl:mnistnetwork}
\end{table*}

\begin{table*}[!h]
	\centering
	\resizebox{0.6\textwidth}{!}{
		\begin{tabular}{c}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				\multicolumn{6}{|c|}{\small{F-MNIST network }}\\
				\hline\hline
				Layer& In & Out&Kernel& Stride&Padding\\
				\hline
				Conv2d & 1 & 10 & 5 $\times$ 5 & 1 & None\\
				MaxPool2d & - & - & 2 $\times$ 2 & - & -\\
				ReLU & - & - & - & - & -\\
				Conv2d & 10 & 20 & 5 $\times$ 5 & 1 & None\\
				MaxPool2d & 10 & 10 & 2 $\times$ 2 & - & -\\
				ReLU & - & - & - & - & -\\
				Dropout & - & - & - & - &-\\
				Linear & 320 & 50 & - & - & -\\
				Linear & 50 & 10 & - & - & - \\
				LogSoftmax & 50 & 10 & - & - & - \\
				\hline
			\end{tabular}\\
			\\[-.15cm]
	\end{tabular}}\caption{Network architecture for FMNIST.}\label{tbl:fmnist-network}
\end{table*}


\begin{table*}[!h]
	\centering
	\resizebox{0.6\textwidth}{!}{
		\begin{tabular}{c}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				\multicolumn{6}{|c|}{\small{CIFAR10 network}}\\
				\hline\hline
				Layer& In & Out&Kernel& Stride&Padding\\
				\hline
				Conv2d & 3 & 6 & 5 $\times$ 5 & 1 & None\\
				MaxPool2d & - & - & 2 $\times$ 2 & - & -\\
				ReLU & - & - & - & - & -\\
				Conv2d & 6 & 16 & 5 $\times$ 5 & 1 & None\\
				MaxPool2d & 10 & 10 & 2 $\times$ 2 & - & -\\
				ReLU & - & - & - & - & -\\
				Linear & 120 & 84 & - & - & -\\
				Linear & 84 & 10 & - & - & - \\
				LogSoftmax & 50 & 10 & - & - & - \\
				\hline
			\end{tabular}\\
			\\[-.15cm]
	\end{tabular}}\caption{Network architecture for CIFAR10.}\label{tbl:CIFAR10-networks}
\end{table*}

\begin{table*}[!h]
	\centering
	\resizebox{0.6\textwidth}{!}{
		\begin{tabular}{c}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				\multicolumn{6}{|c|}{\small{MNIST-Autoencoder}}\\
				\hline\hline
				Layer& In & Out&Kernel& Stride&Padding\\
				\hline
				Conv2d & 1 & 8 & 3 $\times$ 3 & 2 & 1\\
				ReLU & - & - & - & - & -\\
				Conv2d & 8 & 16 & 3 $\times$ 3 & 2 & 1\\
				BatchNorm & - & - & - & - & -\\
				ReLU & - & - & - & - & -\\
				Conv2d & 16 & 32 & 3 $\times$ 3 & 2 & 0\\
				Linear & 288 & 128 & - & - & -\\
				ReLU & - & - & - & - & -\\
				Linear & 128 & 4 & - & - & - \\
				Linear & 4 & 128 & - & - & - \\
				ReLU & - & - & - & - & -\\
				Linear & 128 & 288 & - & - & - \\
				ReLU & - & - & - & - & -\\
				ConvTrans2d & 32 & 16 & 3 $\times$ 3 & 2 & 0\\
				BatchNorm & - & - & - & - & -\\
				ReLU & - & - & - & - & -\\
				ConvTrans2d & 16 & 8 & 3 $\times$ 3 & 2 & 1\\
				BatchNorm & - & - & - & - & -\\
				ReLU & - & - & - & - & -\\
				ConvTrans2d & 8 & 1 & 3 $\times$ 3 & 2 & 1\\
				
				
				\hline
			\end{tabular}\\
			\\[-.15cm]
	\end{tabular}}\caption{Network architecture for Fashion-MNIST Autoencoder operation.}\label{tbl:AutoEncMNIST}
\end{table*}


\newpage

\null

\vfill

\newpage

