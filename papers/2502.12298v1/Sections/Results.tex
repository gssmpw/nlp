To empirically compare the efficiency of the proposed method with widely-used optimization methods, we focus on three broad deep learning problems: image classification, image reconstruction and language modeling. All experiments were conducted using open-source software PyTorch \citep{NEURIPS2019_9015}, SciPy \citep{2020SciPy-NMeth}, and NumPy \citep{harris2020array}. We use an Intel Core i7-8700 CPU with a clock rate of 3.20 GHz and an NVIDIA RTX 2080 Ti graphics card.


%\begin{figure}[t]
%	\centering
%	\begin{tabular}{c}
%%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/MNIST_train_paper.png} \\
%%		(a) Training accuracy for MNIST dataset
%%		\\
%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/MNIST_test_paper.png}\\
%		 
%%		\\   
%%		(b) Testing accuracy\\
%	\end{tabular}
%	\caption{The classification results for Experiment I.B: MNIST. 
%	%(a) Training accuracy of the network. The $y$-axis represents the percentage of samples predicted correctly. (b) 
%	The classification accuracy of the testing samples correctly predicted. Note that the proposed method (ARCs-LSR1) achieves the highest classification accuracy.\label{fig:fashionresults}}
%\end{figure}


%\begin{figure}[t]
%	\centering
%	%	\begin{tabular}{c}
%	%		\adjincludegraphics[scale=0.5,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/CIFAR10_train_paper.png} \\(a) Cross-entropy training loss for CIFAR10 dataset \\
%	%		\adjincludegraphics[scale=0.5,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/CIFAR_test_paper.png}\\
%	%		(b) Classification accuracy
%	%	\end{tabular}
%	\begin{tabular}{c}
%%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/CIFAR10_train_paper.png} 
%%		\\
%%		(a) Training loss for CIFAR10
%%		\\
%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/CIFAR_test_paper.png}
%%		\\
%%		(b) Classification accuracy
%	\end{tabular}
%	\caption{The classification results for Experiment I.C: CIFAR10. 
%	%(a) Training accuracy of the network. The $y$-axis represents the negative log-likelihood loss, and the $x$-axis represents the number of epochs. (b) 
%	The classification accuracy of the testing samples correctly predicted. The proposed method (ARCs-LSR1) achieves the lowest training loss and highest classification accuracy within the fewest number of epochs.\label{fig:CIFAR10}}
%\end{figure}
%

\subsection{Experiment I: Image classification}
We present the classification results for IRIS, MNIST,  and CIFAR. %Additional results on the MNIST dataset are presented in Appendix C. We also present the timing results for CIFAR10 dataset in Appendix D.% \ref{appnd:IRISResults}. 

  

%\begin{figure}
%	\centering
%	\begin{tabular}{cc}
%			\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{Figures/MNIST_train_paper.png} &
%			\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{Figures/MNIST_test_paper.png}\\
%			(a) & 
%			(b) 
%		\end{tabular}
%	\caption{The classification results on MNIST. The $y$-axis represents the classification accuracy on the MNIST dataset, and the $x$-axis represents the number of epochs. (a) Training response.  (b) Testing response.\label{fig:MNIST}}
%\end{figure}

\noindent 

\textbf{Experiment I.A: IRIS.}  %The IRIS dataset consists of 50 samples of three species of the iris flower.
%The features correspond to the length and width of the sepals and petals for each sample.
This dataset is relatively small; consequently, we only consider a shallow network with three fully connected layers and 2953 parameters. We set the history size and maximum iterations for the proposed approach and L-BFGS to 10. Figure \ref{fig:IRIS}(a) shows the comparative performance of all the methods. Note that our proposed method (ARCs-LSR1) achieves the highest classification accuracy in the fewest number of epochs. 

\noindent 
\textbf{Experiment I.B: MNIST.} The MNIST classifier is a shallow network with 3 fully connected layers and 397510 parameters. We train the network for 20 epochs with a batch size of 256 images, keeping the history size and maximum iterations same for the proposed approach and L-BFGS. Figure \ref{fig:CIFAR}(a) shows that the proposed ARCs-LSR1 outperforms the other methods.

\begin{figure}[!htbp]
    \centering
    %\subfloat[IRIS dataset]
{\adjincludegraphics[width=0.75\linewidth, trim={{.05\width}  {0.05\height} {.06\width} {0.15\height}},clip]{./Figures/iris_test_paper.png}}
    \caption{The classification accuracy results for \textbf{Experiment I.A: IRIS}. 
	%(a) Training loss of the network. The $y$-axis represents the negative log-likelihood loss and the $x$-axis represents the number of epochs. (b) 
	%The classification accuracy for each method, i.e., 
	The percentage of testing samples correctly predicted in the testing dataset for each method is presented. Note that the proposed method (ARCs-LSR1) achieves the highest classification accuracy within the fewest number of epochs.}
    \label{fig:IRIS}
\end{figure}
\noindent 
\textbf{Experiment I.C: CIFAR10.} Because the CIFAR10 dataset contains color images (unlike the MNIST grayscale images), the network used  has more layers compared to the previous  experiments.  The network has 6 convolutional layers and 3 fully connected layers with 62006 parameters. For ARCs-LSR1 and L-BFGS, we have a history size of 100 with a maximum number of iterations of 100 and a batch size of 1024. Figure \ref{fig:CIFAR}(b) represents the testing accuracy, i.e., the number of samples correctly predicted in the testing set.
 %Fig.\ \ref{fig:CIFAR10}(a) represents the training loss (cross-entropy loss). 




\begin{figure}[!htbp]
	\centering
 \begin{tabular}{cc}
      \subfloat[MNIST dataset]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  {0.05\height} {.06\width} {0.15\height}},clip]{./Figures/MNIST_test_paper.png}}&  \subfloat[CIFAR dataset]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  {0.05\height} {.06\width} {0.15\height}},clip]{./Figures/CIFAR_test_paper.png}}
 \end{tabular}
	\caption{The classification accuracy results for \textbf{Experiment I.B and I.C}. 
	%(a) Training loss of the network. The $y$-axis represents the negative log-likelihood loss and the $x$-axis represents the number of epochs. (b) 
	%The classification accuracy for each method, i.e., 
	The percentage of testing samples correctly predicted in the testing dataset for each method is presented. Note that the proposed method (ARCs-LSR1) achieves the highest classification accuracy within the fewest number of epochs.% compared to existing gradient-descent methods (Adam, SGD, RMSProp, and Adagrad) and to the quasi-Newton method L-BFGS.
\label{fig:CIFAR}}
\end{figure}

\textbf{Experiment I: Additional MNIST results.} We select the best history, max-iterations and batch-size hyperparameters by conducting a thorough parameter search described below.

\textbf{History.} The ARCs-LSR1 method requires some history from the past to form the Limited-memory SR1 approximation. The history stores a set of steps `$\mathbf{s}$' and their corresponding change in gradients `$\mathbf{y}$'. This is the most important parameter for the proposed approach - as the number of history pairs increase, the approximation begins converging to the true Hessian. However, we cannot have a full-rank approximation, so the number of history pairs are limited. In addition, in the context of deep learning, a high memory parameter might not be ideal owing to its large storage complexity.

To empirically show this, we ran a set of experiments by varying the batch-size and the the number of iterations for each batch and present results on the MNIST classification task. We selected a history-size of $5, 10, 15, 20, 50$ and $100$.

\begin{figure}[!htpb]
    \begin{tabular}{cc}
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.02\width}  0 {.01\width} {0.15\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-1-hist-all-epochs-early.png}} 
        & 
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.11\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-1-hist-all-epochs-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification.} We fix the maximum iterations to 1 and batch-size of 128. (a) presents the epochs [1-5] and (b) presents epochs [15-20].} \label{fig:batch-128}
\end{figure}

\begin{figure}[!htpb]
    \begin{tabular}{cc}
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.01\width}  0 {.01\width} {0.05\height}}, clip]{./Appendix-figures/Appendix-batch-size-256-max-iterations-1-early-epochs.png}} 
        & 
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-256-max-iters-1-hist-all-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification.} We fix the maximum iterations to 1 and batch-size of 256. (a) presents the epochs [1-5] and (b) presents epochs [15-20].}\label{fig:batch-256}
\end{figure}

\begin{figure}[!htpb]
    \begin{tabular}{cc}
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.01\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-512-max-iters-1-hist-all-early.png}} 
        & 
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.11\height}}, clip]{./Appendix-figures/Appendix-batch-size-512-max-iters-1-hist-all-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification.} We fix the maximum iterations to 1 and batch-size of 512. (a) presents the epochs [1-5] and (b) presents epochs [15-20].}\label{fig:batch-512}
\end{figure}
\begin{figure}[!hb]
    \begin{tabular}{cc}
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.005\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-1024-max-iters-1-hist-all-early.png}} 
        &
        \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.11\height}}, clip]{./Appendix-figures/Appendix-batch-size-1024-max-iters-1-hist-all-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification.} We fix the maximum iterations to 1 and batch-size of 1024. (a) presents the epochs [1-5] and (b) presents epochs [15-20].}\label{fig:batch-1024}
\end{figure}


\begin{figure}[!ht]
    \begin{tabular}{cc}
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.02\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-10-hist-all-early.png}} \hspace{5mm} 
         & 
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.11\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-10-hist-all-late.png}} 
    \end{tabular}
    \caption{\textbf{MNIST classification:} The figure shows the classification response for a upper bound max-iterations of $10$.  The batch-size is fixed to $128$ images. (a) presents the early epochs $[1-5]$ while the second column (b) presents the late epochs $[15-20]$.} \label{fig:max-iterations-10}
\end{figure}

\begin{figure}[!htpb]
    \begin{tabular}{cc}
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.01\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-15-hist-all-early.png}} 
         & 
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.05\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-15-hist-all-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification:} The figure shows the classification response for a upper bound max-iterations of $15$.  The batch-size is fixed to $128$ images. (a) presents the early epochs $[1-5]$ while the second column (b) presents the late epochs $[15-20]$.}\label{fig:max-iterations-15}
\end{figure}

\begin{figure}[!htpb]
    \begin{tabular}{cc}
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth, trim={{.01\width}  0 {.01\width} {0\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-20-hist-all-early.png}} 
         & 
         \subfloat[]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.01\width} {0.05\height}}, clip]{./Appendix-figures/Appendix-batch-size-128-max-iters-20-hist-all-late.png}}
    \end{tabular}
    \caption{\textbf{MNIST classification:} The figure shows the classification response for a upper bound max-iterations of $20$.  The batch-size is fixed to $128$ images. (a) presents the early epochs $[1-5]$ while the second column (b) presents the late epochs $[15-20]$.}\label{fig:max-iterations-20}
\end{figure}


\textbf{Different max-iterations.} The max-iterations determines how many times the proposed approach is applied to each individual batch for an optimization step and its corresponding history update ($\mathbf{s}, \mathbf{y}$). For the most ideal condition, we consider the trade-off between computational complexity and improvement of accuracy. This means the accuracy of prediction does not increase significantly with the increase in the upper bound of iterations. We fix the batch-size to $128$ and switch the max-iterations between $10, 15$ and $20$.

The results are presented in Figure(s) \ref{fig:max-iterations-10}, \ref{fig:max-iterations-15}, and \ref{fig:max-iterations-20}. From these results, it was certain that a maximum iteration of 10 was ideal. 

\textbf{Different batch-sizes.} For this experiment, we chose from batch-sizes of $128, 256, 512$ and $1024$. We fixed the maximum-iterations to $1$. The results are presented in Figure(s) \ref{fig:batch-128},\ref{fig:batch-256}, \ref{fig:batch-512}, and \ref{fig:batch-1024}.



\subsection{Experiment II: Image reconstruction}
The image reconstruction problem involves feeding a feedforward convolutional autoencoder model a batch of the dataset. The loss function is defined between the reconstructed image and the original image. We use the Mean-Squared Error (MSE) loss between the reconstructed image and the original image. For this experiment, we use the MNIST and FMNIST dataset.

\noindent \textbf{Experiment II.A: MNIST.} The network is shallow, with 53415 parameters, which are initialized randomly. We considered a batch size of 256 images and trained over 50 epochs. Each experiment has been conducted 5 times. The results for the image reconstruction can be seen in Figure \ref{fig:recon}, where the initial descent of the proposed approach yields a significant decrease in the training loss. We provide the training loss results for the early (Figure  \ref{fig:recon}(a)) and late epochs (Figure \ref{fig:recon}(b)).
%This is empirical evidence that the method converges to the minimizer in fewer steps in comparison to the adaptive methods. 
In Figure \ref{fig:recon}(b), all the methods eventually converge to the same training loss value (except for L-BFGS).  We 
see a similar trend during the early and late epochs for the testing loss (see Figure \ref{fig:recon2}).

%note that the network generalizes well on the testing dataset in comparison to all other adaptive and quasi-Newton methods. For more details, please refer Fig.\ \ref{fig:recon2}.

\noindent \textbf{Experiment II: FMNIST.} 
Figure(s) \ref{fig:recon4}(a) and \ref{fig:recon4}(b) show the testing response for the early and late epochs, respectively. The early iterates generated by the proposed approach significantly decreases the objective function. The proposed approach has maintained this trend in the later epochs as well (see Figure \ref{fig:recon4}(b)). This shows that the network is capable of generalizing on a testing dataset as well in comparison to all other adaptive and quasi-Newton methods.

\begin{figure}
	\centering
    \begin{tabular}{cc}
		\subfloat[FMNIST testing loss for early epochs]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_fmnist_256_initial_test.png}}
        &
        \subfloat[FMNIST testing loss for late epochs]{\adjincludegraphics[width=0.48\linewidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_fmnist_256_final_test.png}}
    \end{tabular}
	\caption{The image reconstruction results for \textbf{Experiment II}. (a) Initial  testing loss of the network. The $y$-axis represents the MSE loss in the first $6$ epochs. (b) The final MSE loss of the testing samples from epoch $41$ to $50$. The proposed method (ARCs-LSR1) achieves the lowest testing loss. \label{fig:recon4}}
\end{figure}

\begin{figure}[htp]
	\begin{tabular}{cc}
    \adjincludegraphics[width=0.48\textwidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_MNIST_256_initial.png}
        &
        \adjincludegraphics[width=.48\textwidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_MNIST_256_final.png}
		\\
        (a) MNIST testing loss for early epochs
        &
		(b) MNIST testing loss for late epochs
	\end{tabular}
	\caption{ The image reconstruction results for \textbf{Experiment II.A: MNIST}. (a) Initial training loss. The $y$-axis represents the Mean-Squared Error (MSE) loss from the first four epochs. (b) Final training loss from epochs $43$ to $50$. Note that the proposed method (ARCs-LSR1) achieves the lowest training loss.\label{fig:recon}}
\end{figure}

\begin{figure}[htp]
	\begin{tabular}{cc}
    \adjincludegraphics[width=0.48\textwidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_MNIST_256_initial_test.png}
        &
        \adjincludegraphics[width=.48\textwidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_MNIST_256_final_test.png}
		\\
        (a) MNIST training loss for early epochs
        &
		(b) MNIST training loss for late epochs
	\end{tabular}
	\caption{ The image reconstruction results for \textbf{Experiment II.A: MNIST}. (a) Initial testing loss. The $y$-axis represents the Mean-Squared Error (MSE) loss from the first four epochs. (b) Final testing loss from epochs $43$ to $50$. Note that the proposed method (ARCs-LSR1) achieves the lowest testing loss.\label{fig:recon2}}
\end{figure}
%
%
%\begin{figure}[ht]
%	\begin{tabular}{c}
%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_fmnist_256_initial.png} 
%		\\
%		(a) Early epoch FMNIST training loss
%		\\
%		\adjincludegraphics[width=6.75cm,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Autoencoder_fmnist_256_final.png}\\
%		 \\ 
%		(b) Late epoch FMNIST training loss 
%	\end{tabular}
%	\caption{The image reconstruction results for Experiment II.B: FMNIST (a) Initial training loss. The $y$-axis represents the Mean-Squared Error (MSE) loss in the first three epochs. (b) Final training loss from epochs $42$ to $50$.  Note that the proposed method (ARCs-LSR1) achieves the lowest training loss. \label{fig:recon3}}
%\end{figure}
%\hspace{-5cm}



\subsection{Experiment III: Natural language modeling}
We conducted word-level predictions on the Penn Tree Bank (PTB) dataset \citep{treebank}. We used a state-of-the-art Long-Short Term Memory (LSTM) network which has 650 units per layer and its parameters are uniformly regularized in the range [-0.05, 0.05]. For more details on implementation, please refer \citet{1409.2329}. For the ARCs-LSR1 method and the L-BFGS, we used a history size of 5 over 4 iterations. The prediction loss results are shown in Figure \ref{fig:pentreebank}.
In contrast to the previous experiments, here, both quasi-Newton methods (L-BFGS and ARCs-LSR1) outperform the adaptive methods, with the proposed method (ARCs-LSR1) achieving the lowest cross-entropy prediction loss.  


 
\begin{figure}[H]
	\centering
	\begin{tabular}{c}
		\adjincludegraphics[width=0.7\linewidth,trim={{.05\width}  0 {.06\width} {0.15\height}},clip]{./Figures/Pentreebank_prelim_results.png}
	\end{tabular}
	\caption{The prediction loss for \textbf{Experiment III: Penn Tree Bank}.  The $y$-axis represents the cross-entropy loss, and the $x$-axis represents the number of epochs. Note that the proposed method (ARCs-LSR1) achieves the lowest loss. \label{fig:pentreebank}}
\end{figure}


\subsection{Experiment IV: Comparison with Stochastically Damped L-BFGS} 
In the  previous experiments on image classification and reconstruction (Experiments I and II), the L-BFGS approach performs poorly, which can be attributed to noisy gradient estimates and non-convexity of the problems. To tackle this, a \textsl{stochastically damped} L-BFGS  (SdLBFGS) approach was proposed (see \citet{wang2017stochastic}) which adaptively generates a variance reduced, positive-definite approximation of the Hessian. We compare the proposed approach to L-BFGS and SdLBFGS on the MNIST classification problem. From Figure \ref{fig:QN}(a), the proposed approach achieves a comparable performance to the stochastic version and is able to achieve the best accuracy in later epochs (see Figure \ref{fig:QN}(b)).

\begin{figure}[!ht]
	\centering
	
%		\subfloat[Epochs 0-5]{\adjincludegraphics[width=0.45\linewidth,trim={{.01\width}  0 {.01\width} {0.13\height}},clip]{./Figures/final_mnist_classification_quasi.png}}\subfloat[Epochs 16-20]{\adjincludegraphics[width=0.45\linewidth,trim={{.01\width}  0 {.01\width} {0.3\height}},clip]{./Figures/final_mnist_epochs_classification_quasi.png}}
		\subfloat[Epochs 0-5]{\adjincludegraphics[width=0.38\linewidth,trim={{.01\width}  0 {.01\width} {0.13\height}},clip]{./Figures/final_mnist_classification_quasi.png}}\subfloat[Epochs 16-20]{\adjincludegraphics[width=0.55\linewidth,trim={{.01\width}  0 {.01\width} {0.3\height}},clip]{./Figures/final_mnist_epochs_classification_quasi.png}}
	\caption{ The prediction loss for \textbf{Experiment IV}: Comparison with stochastically damped 
	L-BFGS. The $x$-axis represents the number of epochs and the $y$-axis represents the accuracy of prediction. (a) Accuracy for epochs 0-5.  (b) Accuracy for epochs 16-20.}\label{fig:QN}
\end{figure}

%\noindent \textbf{Computational time analysis.} 
%We understand that the proposed approach performs competitively against all existing methods.
% We now analyze the time-constraints of each method. We choose to clock Experiment 3. We chose a maximum iterations of 100 with a history size of 100 for L-BFGS and ARCs-LSR1, with a batch size of 1024 images. Fig.\ \ref{fig:timings} shows the time required by each of the methods to reach a non-overtrained minima. Note that the proposed approach reaches the desired minima in much less time than the other algorithms. L-BFGS does not converge perhaps due to a very noisy loss function and a small batch size, thus causing the algorithm to break. 
%(see e.g., \cite{Dooptmethodsmatter}). argue that a large batch size is required for quasi-Newton methods to perform well. 
%However, the ARCs-LSR1 method performs well  even with a small batch size.



\subsection{Experiment V: Timing results}
\label{sec:Timing results}

\begin{figure}[!htb]
    \adjincludegraphics[width=\linewidth]{./Appendix-figures/timings.png}
    \caption{\textbf{Experiment V}: CIFAR-10 classification time complexity. The figure shows the time complexity of all the methods and the proposed approach. Even though the proposed approach takes $\approx$ 500 seconds longer, the best accuracy is achieved the fastest in comparison to all the other state-of-the-art approaches.}\label{appnd:CIFAR10-timings}
\end{figure}
We take the CIFAR10 experiment into consideration as its the most computationally expensive experiment for classification with a parameter count of ~62k and a memory parameter of 100, with a total of $\approx$ 6M memory allocations. Figure \ref{appnd:CIFAR10-timings} shows the time-budget for each of the adaptive techniques and the proposed approach. We observe that the proposed approach is able to achieve the highest accuracy the quickest, even with a higher computational budget.
