In this section, we describe our proposed approach by first discussing the L-SR1 update.

\noindent \textbf{Limited-memory symmetric rank-one updates.} 
Unlike the BFGS update (\ref{eqn:LBFGS}), which is a rank-two update, the SR1 update is a rank-one update, which is  given by
\begin{equation}\label{eq:SR1}
	\mathbf{B}_{k+1} = \mathbf{B}_{k} + \frac{(\mathbf{y}_k - \mathbf{B}_k\mathbf{s}_k)
	(\mathbf{y}_k - \mathbf{B}_k\mathbf{s}_k)^{\top}}{\mathbf{s}_k^{\top}(\mathbf{y}_k - \mathbf{B}_k\mathbf{s}_k)}
\end{equation}
(see \citet{KhaBS93}).  As previously mentioned, $\mathbf{B}_{k+1}$ in (\ref{eq:SR1}) is not guaranteed to be definite.  However,
it can be shown that the SR1 matrices can converge to the true Hessian (see \citet{Conn1991} for details).
We note that the pair $(\mathbf{s}_k, \mathbf{y}_k)$ is accepted only when 
\begin{equation}\label{eq:acceptance1}
|\mathbf{s}_k^{\top}(\mathbf{y}_k - \mathbf{B}_k\mathbf{s}_k)| > \varepsilon \| \mathbf{s}_k \|_2 \| \mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k \|_2,
\end{equation}
for some constant $\varepsilon > 0$ (see \citet{NoceWrig06}, Sec.\ 6.2, for details). The SR1 update can be defined recursively as
\begin{equation}\label{eq:SR1_B0}
	\mathbf{B}_{k+1} = \mathbf{B}_{0} + 
	\sum_{j = 0}^k \frac{(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)
	(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)^{\top}}{\mathbf{s}_j^{\top}(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)}.
\end{equation}
In limited-memory settings, only the last $m \ll n$ pairs of $(\mathbf{s}_j, \mathbf{y}_j)$ 
are stored and used.  
For ease of presentation, here we choose $k < m$.  We define
$$\mathbf{S}_{k} = [ \ \mathbf{s}_0 \ \  \mathbf{s}_1 \ \ \cdots  \ \ \mathbf{s}_{k-1} \ ] \quad \text{and} \quad 
\mathbf{Y}_{k} = [ \ \mathbf{y}_0 \ \ \mathbf{y}_1 \ \ \cdots \  \ \mathbf{y}_{k-1} \ ].
$$
Then 
$\mathbf{B}_{k}$ admits a compact representation of the form
\begin{equation}\label{eqn:compactSR1}
	\mathbf{B}_{k} \ = \ \mathbf{B}_0 + 
	\begin{bmatrix}
	\\
	\mathbf{\Psi}_{k}  \\
	\phantom{t}
	\end{bmatrix}
	\hspace{-.3cm}
	\begin{array}{c}
	\left  [ \  \mathbf{M}_{k}^{\phantom{h}}  \right ] \\
	\\
	\\
	\end{array}
	\hspace{-.3cm}
	\begin{array}{c}
	\left [  \ \quad \mathbf{\Psi}_{k}^{\top} \quad \ \right ] \\
	\\
	\\
	\end{array},
\end{equation}
where  $\mathbf{\Psi}_{k} = \mathbf{Y}_{k} -  \mathbf{B}_0 \mathbf{S}_{k}$ and 
%\begin{align}\label{eq:PsiM}
%	\nonumber
%	\mathbf{\Psi}_{k+1} &= \mathbf{Y}_{k+1}\!  -\! \mathbf{B}_0 \mathbf{S}_{k+1}  \ \\\nonumber \text{and}\\ 
$$
        \mathbf{M}_{k} = (\mathbf{D}_{k} \!+\! \mathbf{L}_{k} \!+\! \mathbf{L}_{k}^{\top} \!-\! \mathbf{S}_{k}^{\top}\!\mathbf{B}_0\mathbf{S}_{k})^{-1}\!,
$$
%\end{align}
where $\mathbf{L}_{k}$ is the strictly lower triangular part, $\mathbf{V}_{k}$ is the strictly
upper triangular part, and $\mathbf{D}_{k}$ is the diagonal part of 
$
	\mathbf{S}_{k}^{\top}\mathbf{Y}_{k} =   \mathbf{L}_{k} + \mathbf{D}_{k} + \mathbf{V}_{k}
$
(see \citet{ByrNS94} for further details).  


Because of the compact representation of $\mathbf{B}_{k}$, 
its partial eigendecomposition can be computed (see  \citet{ErwM15}).  
In particular, if we compute the QR decomposition of $\mathbf{\Psi}_{k} = \mathbf{QR}$
and the eigendecomposition $\mathbf{RMR}^\top= \mathbf{P} \hat{\mathbf{\Lambda}}_{k} \mathbf{P}^\top$,
then we can write 
$$
\mathbf{B}_{k} = \mathbf{B}_0 + \mathbf{U}_{\parallel} \hat{\mathbf{\Lambda}}_{k} 
\mathbf{U}_{\parallel}^{\top},
$$
where $\mathbf{U}_{\parallel}  = \mathbf{QP} \in \mathbb{R}^{n \times k}$ has
orthonormal columns and $\hat{\mathbf{\Lambda}}_{k} \in \mathbb{R}^{k \times k}$ 
is a  diagonal matrix.  
If $\mathbf{B}_0 =  \delta_k \mathbf{I}$ (see e.g., Lemma 2.4 in  \citet{Erway2020TrustregionAF}), 
where $0 < \delta_k < \delta_{\max}$ is some scalar and $\mathbf{I}$ is the identity matrix, 
then we obtain the eigendecomposition 
\begin{equation}
\mathbf{B}_{k} = \mathbf{U}_{k}\mathbf{\Lambda}_{k}\mathbf{U}_{k}^{\top}
=
\bigg [ \ 
\mathbf{U}_{\parallel}  \ \ \ \mathbf{U}_{\perp}
\bigg ]
\begin{bmatrix}
\hat{\mathbf{\Lambda}}_{k} + \delta_k \mathbf{I} & 0 \\
0 & \delta_k \mathbf{I} 
\end{bmatrix}
\begin{bmatrix}
\ \mathbf{U}_{\parallel}^{\top} \ 
\\[.2cm]
\mathbf{U}_{\perp}^{\top}
\end{bmatrix}
\end{equation}
where $\mathbf{U}_{k} = [  \ \mathbf{U}_{\parallel}  \ \ \mathbf{U}_{\perp} \ ]$ is an orthogonal 
matrix and
$\mathbf{U}_{\perp} \in \mathbb{R}^{n \times (n-k)}$ is a matrix
whose columns form an orthonormal basis orthogonal to the range space of $\mathbf{U}_{\parallel}$.
% and $\mathbf{U}_{k+1}^{\top} \mathbf{U}_{k+1}^{\phantom{\top}} = \mathbf{I}$.  
Here, 
\begin{equation}
	(\mathbf{\Lambda}_{k})_i =
	\begin{cases}  
	\delta_k + \hat{\lambda}_i & \text{ if $i \le k$} \\
	\delta_k & \text{ if $i > k$}
	\end{cases}.
\end{equation}
%$(\mathbf{\Lambda}_{k+1})_i = \delta_k + \hat{\lambda}_i$ for $i \le k+1$, where
%$\hat{\lambda}_i$ is the $i$th diagonal in $\hat{\mathbf{\Lambda}}_{k+1}$,
%and $(\mathbf{\Lambda}_{k+1})_i = \delta_k $ for $i > k+1$.  


%In particular, if $\mathbf{\Phi}_{k+1} = \mathbf{Q}_{k+1} \mathbf{R}_{k+1}$ is the
%QR decomposition of $\mathbf{\Phi}_{k+1}$ and $\mathbf{R_{k+1}M_{k+1}R_{k+1}^{\top} =
%\mathbf{P}_{k+1}\mathbf{\Lambda}_{k+1}\mathbf{P}_{k+1}^{\top}}$ is the eigendecomposition of the product 
%$\mathbf{R}_{k+1}\mathbf{M}_{k+1}\mathbf{R}_{k+1}^{\top}$, then
%$$
%	\mathbf{B}_{k+1} = 
%	\mathbf{B}_0 + \mathbf{Q}_{k+1}\mathbf{P}_{k+1}
%	\mathbf{\Lambda}_{k+1}\mathbf{P}_{k+1}^{\top}\mathbf{Q}_{k+1}^{\top}.
%$$

%\begin{figure}[t]
%	\centering
%		\begin{tabular}{cc}
%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic1.pdf} &
%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic2.pdf}  \\
%		(a) $\lambda > 0$ and $g > 0$ & (b) $\lambda > 0$ and $g < 0$ \\
%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic3.pdf} &
%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic4.pdf} \\
%		(c) $\lambda < 0$ and $g > 0$ & (d)  $\lambda < 0$ and $g < 0$
%		\end{tabular}
%%	\begin{tabular}{cccc}
%%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic1.pdf} &
%%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic2.pdf} &
%%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic3.pdf} &
%%		\includegraphics[width=3.175cm, trim={2cm 1cm 2cm 0},clip]{Figures/cubic4.pdf} 
%%		\\
%%		\footnotesize (a) $\lambda > 0$ and $\bar{g} > 0$ & 
%%		\footnotesize (b) $\lambda > 0$ and $\bar{g} < 0$ &
%%		\footnotesize (c) $\lambda < 0$ and $\bar{g} > 0$ &
%%		\footnotesize (d)  $\lambda < 0$ and $\bar{g} < 0$
%%	\end{tabular}
%	\caption{Illustration of the piece-wise cubic function $m(s)$. When $\lambda > 0$, 
%		$m(s)$ is a convex function and has a unique local minimum, which is also the global minimum 
%		((a) and (b)).
%		If $\lambda < 0$, then $m(s)$ has two local minima ((c) and (d)).
%		The scalar $\bar{g}$ corresponds to the slope of $m(s)$ at $s = 0$, i.e., $m'(0) = \bar{g}$.
%		If $\bar{g} > 0$, the minimum $s^*$ of $m(s)$
%		corresponds to the minimum of $m_-(s)$ (black circle in (a) and (c)), 	
%		and if $\bar{g} < 0$, then $s^*$ corresponds to the minimum of $m_+(s)$ (red circle in (b) and (d)).
%		\label{fig:cubic}}
%\end{figure}

\noindent \textbf{Adaptive regularization using cubics.} 
 Since the SR1 Hessian approximation can be indefinite, some safeguard must be implemented to ensure that the resulting search direction $\mathbf{s}_k$ is a descent direction.  One such safeguard is to use a ``regularization" term.
The Adaptive Regularization using Cubics (ARCs) method (see \citet{Griewank1981,NesP06,cartis2011adaptive}) can be viewed as an alternative to line-search and trust-region methods. At each iteration, an approximate global minimizer of a local (cubic) model,
\begin{equation}\label{eq:cr}
	\underset{\mathbf{s}\in \mathbb{R}^n}{\text{min}}  \ m_k(\mathbf{s}) 
	\equiv 
	%\underset{s\in \mathcal{R}^n}{\text{min}} 
	\mathbf{g}_k^{\top}\mathbf{s}
	+ \frac{1}{2} \mathbf{s}^{\top}\mathbf{B}_k \mathbf{s} + \frac{\mu_k}{3} (\Phi_k(\mathbf{s}))^3,
\end{equation}
is determined, where  $\mathbf{g}_k = \nabla f(\Theta_k)$, $\mu_k > 0$ is a regularization parameter, and
$\Phi_k$ is a function (norm) that regularizes $\mathbf{s}$.   Typically, the Euclidean norm is used.
In this work, we use an alternative ``shape-changing" norm that allows us to solve each subproblem 
(\ref{eq:cr}) exactly.  Proposed in \citet{Burdakov2017}, this shape-changing norm is
based on the partial eigendecomposition of $\mathbf{B}_{k}$.  Specifically, if 
$\mathbf{B}_{k} = \mathbf{U}_k \mathbf{\Lambda}_k \mathbf{U}_k^{\top}$ is the eigendecomposition
of $\mathbf{B}_k$, then we can define the norm 
$$
 \|\mathbf{s}\|_{\mathbf{U}_k}\overset{\text{def}}{=}\|\mathbf{U}_k^\top \mathbf{s}\|_3.
 $$
 It can be shown using H\"{o}lder's Inequality that 
 $$
 \frac{1}{\sqrt[\leftroot{1} 6]{n}}
 %n^{-1/6} 
 \| \mathbf{s} \|_2 
\le  \| \mathbf{s} \|_{\mathbf{U}_k}
\le  \| \mathbf{s} \|_2.
$$

As per the authors' literature review, this is the first time the adaptive regularized cubics has been used in conjunction with a shape changing norm in a deep learning setting. The main motivation of using this adaptive regularized cubics comes from better convergence properties when compared with a trust-region approach (see \citet{cartis2011adaptive}). Using the shape-changing norm allows us to solve the subproblem exactly.
 
 \noindent \textbf{Closed-form solution.} 
 Applying a change of basis with
$\bar{\mathbf{s}} = \mathbf{U}_k^{\top} \mathbf{s}$ and 
$\bar{\mathbf{g}}_k = \mathbf{U}_k^{\top}\mathbf{g}_k$, 
we can redefine the cubic subproblem as
%The ARC algorithm has claimed to achieve a 2-norm of the gradient $\norm{g} = \norm{\nabla f}$ below the desired accuracy $\epsilon$ in at most $\mathcal{O}(\epsilon^{-1.5})$ steps. Now we are ready to explain the regularization function $\Phi_k(s)$.
%\textbf{New basis:} If $\Phi(s)$ in (\ref{eq:cr}) is a two-norm operation on $s$, then our model function will, at most, have only two local minima, making it cumbersome to find the global minima. We propose a transformation of the parameter space $s$ to $\bar{s}$ such that $ \norm{s}_\mathbf{U}\overset{\text{def}}{=}\norm{\mathbf{U}^\top s}_3$ and redefine our objective function as
\begin{equation}\label{eq:modcr}
	\underset{\bar{\mathbf{s}} \in \mathbb{R}^n}{\text{min}}  \ \bar{{m}}_{k} (\bar{\mathbf{s}})
	= \bar{\mathbf{g}}_k^\top\bar{\mathbf{s}}
	+ \frac{1}{2}\bar{\mathbf{s}}^\top \mathbf{\Lambda}_k\bar{\mathbf{s}}
	+ \frac{\mu_k}{3}\|\bar{\mathbf{s}}\|_3^3.
\end{equation}
With this change of basis, we can easily find a closed-form solution of (\ref{eq:modcr}), which is generally not the case for other choices of norms.  
Note that $\bar{m}_k(\bar{\mathbf{s}})$ is a separable function,  
meaning we can write $\bar{m}_k(\bar{\mathbf{s}})$ as
$$
	\bar{m}_k(\bar{\mathbf{s}})
	=
	\sum_{i=1}^n
	\left \{
	(\bar{\mathbf{g}}_k)_i (\bar{\mathbf{s}})_i
	+
	\frac{1}{2}(\mathbf{\Lambda}_k)_i(\bar{\mathbf{s}})_i^2
	+
	\frac{\mu_k}{3} |(\bar{\mathbf{s}})_i |^3
	\right \}.
$$
Consequently, we can  solve (\ref{eq:modcr}) by solving one-dimensional problems 
of the form 
\begin{equation}\label{eq:modcr1}
	\underset{\bar{s} \in \mathbb{R}}{\text{min}}  \ \ \bar{m}(\bar{s})
	= \bar{g} \bar{s}   
	+ \frac{1}{2}\lambda \bar{s}^2
	+ \frac{\mu_k}{3}|\bar{s}|^3,
\end{equation}
where $\bar{g} \in \mathbb{R}$ corresponds to entries in $\bar{\mathbf{g}}_k$ and
$\lambda \in \mathbb{R}$ corresponds to diagonal entries in $\mathbf{\Lambda}_k$.  
To find the minimizer of (\ref{eq:modcr1}), we first write $\bar{m}(\bar{s})$ as follows:
\begin{equation*}
	\bar{m}(\bar{s}) = 
	\begin{cases}
		\bar{m}_+(s) = \bar{g} \bar{s}  
	+ \frac{1}{2}\lambda \bar{s}^2
	+ \frac{\mu_k}{3}\bar{s}^3 & \text{if $\bar{s} \ge 0$}, \\
		\bar{m}_-(\bar{s}) = \bar{g}\bar{s}  
	+ \frac{1}{2}\lambda \bar{s}^2
	- \frac{\mu_k}{3}\bar{s}^3 & \text{if $\bar{s} \le 0$}. 	
	\end{cases}
\end{equation*}
%The corresponding derivative is given by
%\begin{equation*}
%	m'(s) = 
%	\begin{cases}
%		m_+'(s) = g
%	+ \lambda s
%	+ \mu_ks^2 & \text{if $s \ge 0$},\\
%		m_-'(s) = g 
%	+ \lambda s
%	- \mu_ks^2 & \text{if $s \le 0$}.
%	\end{cases}
%\end{equation*}
The minimizer $\bar{s}^*$ of $\bar{m}(\bar{s})$ is obtained by setting $\bar{m}'(\bar{s})$ to zero and will depend on the sign of $\bar{g}$ because $\bar{g}$ is the slope of $\bar{m}(\bar{s})$ at $\bar{s} = 0$, i.e., $\bar{m}'(0) = \bar{g}$.  
In particular,
if $\bar{g} > 0$, then $\bar{s}^*$  is the minimizer of $\bar{m}_-(\bar{s})$,
%(see Figs.\ \ref{fig:cubic}(a) and (c)), 
namely
$\bar{s}^* = (-\lambda + \sqrt{\lambda^2 + 4\bar{g}\mu})/(-2\mu).$
If $\bar{g} < 0$, then $\bar{s}^*$ is the minimizer of $\bar{m}_+(\bar{s})$,
% (see Figs.\ \ref{fig:cubic}(b) and (d)), 
which is given by
$	\bar{s}^* = (-\lambda + \sqrt{\lambda^2 - 4\bar{g}\mu})/(2\mu).$
Note that these two expressions
for $\bar{s}^*$ are equivalent to the following formula:
$$
	\bar{s}^* = \frac{-2\bar{g}}{\lambda + \sqrt{\lambda^2 + 4|\bar{g}|\mu}},
$$
In the original space, $\mathbf{s}^* = \mathbf{U}_k \bar{\mathbf{s}}^*$ and 
$\mathbf{g}_k = \mathbf{U}_k \bar{\mathbf{g}}_k$.
Letting 
\begin{equation}\label{eq:Ck}
	\mathbf{C}_k = \text{diag} (\bar{c}_1, \dots, \bar{c}_n), \quad \text{where \ } \bar{c}_i =  \frac{2}{\lambda_i + \sqrt{\lambda_i^2 + 4|\bar{\mathbf{g}}_i|\mu}},
\end{equation}
then the solution $\mathbf{s}^*$ in the original space is  given by
\begin{equation}\label{eq:sstar}
	\mathbf{s}^* = \mathbf{U}_k \bar{\mathbf{s}}^* =  -\mathbf{U}_k  \mathbf{C}_k \mathbf{U}_k^{\top} \mathbf{g}_k.
\end{equation}
%For a more detailed description of the closed form solution, see  Appendix \ref{sec:Solution}. 




\noindent \textbf{Practical implementation.} While computing 
$\mathbf{U}_{\parallel} \in \mathbb{R}^{n \times k}$
in the matrix $\mathbf{U}_{k} = [  \ \mathbf{U}_{\parallel}  \ \ \mathbf{U}_{\perp} \ ]$
is feasible since 
$k \ll n$, computing $\mathbf{U}_{\perp}$ explicitly is not.  Thus, we must be able to compute 
$\mathbf{s}^*$ without needing $\mathbf{U}_{\perp}$.  
First, we define the following quantities
$$
\begin{array}{lllllll}
\bar{\mathbf{s}}_{\parallel} 
 = \mathbf{U}_{\parallel}^{\top} \mathbf{s} 
& \text{and}  
& \bar{\mathbf{s}}_{\perp} = \mathbf{U}_{\perp}^{\top} \mathbf{s},
\\[.2cm]
\bar{\mathbf{g}}_{\parallel} = \mathbf{U}_{\parallel}^{\top} \mathbf{g}_k
& \text{and} 
& \bar{\mathbf{g}}_{\perp} = \mathbf{U}_{\perp}^{\top} \mathbf{g}_k.
\end{array}
$$
Then the cubic subproblem (\ref{eq:modcr})  becomes
\begin{equation}
\underset{\bar{\mathbf{s}}\in \mathbb{R}^n
}{\text{minimize}}  \ \bar{{m}}_{k} (\bar{\mathbf{s}})
	\ = \ 
\underset{\bar{\mathbf{s}}_{\parallel} \in \mathbb{R}^k
}{\text{minimize}}  \ \bar{{m}}_{\parallel} (\bar{\mathbf{s}}_{\parallel}) + 
\underset{\bar{\mathbf{s}}_{\perp} \in \mathbb{R}^{n-k}
}{\text{minimize}}  \ \bar{{m}}_{\perp} (\bar{\mathbf{s}}_{\perp}),
\end{equation}
where
\begin{eqnarray} \label{eq:mparallel}
	\bar{m}_{\parallel}( \bar{\mathbf{s}}_{\parallel}) \!  \ \ &=& 
	\bar{\mathbf{g}}_{\parallel}^\top\bar{\mathbf{s}}_{\parallel}
	+ \frac{1}{2}\bar{\mathbf{s}}_{\parallel}^\top \hat{\mathbf{\Lambda}}_k\bar{\mathbf{s}}_{\parallel}
	+ \frac{\mu_k}{3}\|\bar{\mathbf{s}}_{\parallel} \|_3^3,
	\\
	\label{eq:mperp}
	\bar{m}_{\perp} ( \bar{\mathbf{s}}_{\perp}) &=& 
	\bar{\mathbf{g}}_{\perp}^\top\bar{\mathbf{s}}_{\perp}
	+ \frac{\delta_k}{2} \| \bar{\mathbf{s}}_{\perp} \|_2^2
	+ \frac{\mu_k}{3}\|\bar{\mathbf{s}}_{\perp}\|_3^3.
\end{eqnarray}
We minimize $\bar{m}_{\parallel}(\bar{s}_{\parallel})$ in (\ref{eq:mparallel}) similar to how we solved (\ref{eq:modcr1}).
In particular, if we let 
\begin{equation}\label{eq:Cparallel}
	\mathbf{C}_{\parallel} = \text{diag} (c_1, \dots, c_n), \ \text{where } c_i =  \frac{2}{\lambda_i + \sqrt{\lambda_i^2 + 4| (\bar{\mathbf{g}}_{\parallel})_i|\mu}},
\end{equation}
then the solution is given by 
\begin{equation}\label{eq:sparallelstar}
	\mathbf{s}_{\parallel}^* =
	-\mathbf{C}_{\parallel} \bar{\mathbf{g}}_{\parallel}.
\end{equation}



Minimizing $\bar{m}_{\perp}(\bar{s}_{\perp})$ in (\ref{eq:mperp}) is more challenging.
The only restriction on the matrix $\mathbf{U}_{\perp}$ is that its columns must form an orthonormal basis for the orthogonal complement of the range space of $\mathbf{U}_{\parallel}$.  
We are thus free to choose the columns of $\mathbf{U}_{\perp}$ as long as they satisfy this restriction.
In particular, we can choose the first column of $\mathbf{U}_{\perp}$ to be the normalized orthogonal projection of $\mathbf{g}_k$ onto the orthogonal complement of the range space of $\mathbf{U}_{\parallel}$, i.e.,
$$
	(\mathbf{U}_{\perp})_1 = ( \mathbf{I} - \mathbf{U}_{\parallel}\mathbf{U}_{\parallel}^{\top})\mathbf{g}_k
	/ \| ( \mathbf{I} - \mathbf{U}_{\parallel}\mathbf{U}_{\parallel}^{\top})\mathbf{g}_k \|_2.
$$
If $\mathbf{g}_k \in $ Range($\mathbf{U}_{\parallel}$), then 
$\bar{\mathbf{g}}_{\perp} = \mathbf{U}_{\perp}^{\top}\mathbf{g}_k = 0$  
% because g_k = U_parallel b for some b 
and the minimizer of (\ref{eq:mperp}) %$\bar{m}_{\perp}(\bar{\mathbf{s}}_{\perp})$ 
is $\bar{\mathbf{s}}_{\perp}^* = 0$ (since $\delta_k > 0$ and $\mu_k > 0$).
If $\mathbf{g}_k \notin $ Range($\mathbf{U}_{\parallel}$), then $(\mathbf{U}_{\perp})_1 \ne 0$ and 
we can choose vectors $ (\mathbf{U}_{\perp})_i \in \text{Range}(\mathbf{U}_{\parallel})^{\perp}$
such that $(\mathbf{U}_{\perp})_i^{\top} (\mathbf{U}_{\perp})_1 = 0$ for all $2 \le i \le n-k$.
Consequently,  $\mathbf{U}_{\perp}^{\top} (\mathbf{U}_{\perp})_1 = \kappa \mathbf{e}_1$,
where $\kappa$ is some constant and $\mathbf{e}_1$ is the first column of the identity matrix.  
Specifically,  
$$
	\kappa \mathbf{e}_1 
	=
	\mathbf{U}_{\perp}^{\top} (\mathbf{U}_{\perp})_1 
	= 
	\mathbf{U}_{\perp}^{\top}  \left ( \mathbf{U}_{\perp}  \mathbf{U}_{\perp}^{\top} \mathbf{g}_k \right )
	=
	\mathbf{U}_{\perp}^{\top} \mathbf{g}_k
	=
	\bar{\mathbf{g}}_{\perp},
$$
which implies $\kappa = \| \bar{\mathbf{g}}_{\perp} \|_2$.  Thus $ \bar{\mathbf{g}}_{\perp}$
has only one non-zero component (the first component) and therefore, the minimizer 
$\bar{\mathbf{s}}_{\perp}^*$ of 
$\bar{m}_{\perp} ( \bar{\mathbf{s}}_{\perp}) $ in (\ref{eq:mperp}) also has only one non-zero compoent (the first component as well).  In particular, 
\begin{align*}
	(\bar{\mathbf{s}}_{\perp}^*)_i
	&=
	\begin{cases}
		\displaystyle 
		-\alpha^* \| \bar{\mathbf{g}}_{\perp} \|_2
		& \text{if $i = 1$} 
		\\
		0 & \text{otherwise}
	\end{cases},
\end{align*}
where
\begin{equation}\label{eq:alphastar}
	\alpha=  \frac{2 }{ \delta_k 
		+ \sqrt{\delta_k^2 + 4 \mu \| \bar{\mathbf{g}}_{\perp} \|_2} }.
\end{equation}
Equivalently, $\bar{\mathbf{s}}_{\perp}^*=- \alpha^* \bar{\mathbf{g}}_{\perp}$.  
Note that the quantity $ \|  \bar{\mathbf{g}}_{\perp}\|_2$ can be computed without computing 
$ \bar{\mathbf{g}}_{\perp}$ from  the fact that $\| \mathbf{g} \|_2^2=
 \| \bar{\mathbf{g}}_{\parallel}\|_2^2 +  \| \bar{\mathbf{g}}_{\perp} \|_2^2$.  
 
 Combining the expressions for $\bar{s}_{\parallel}^*$ in (\ref{eq:sparallelstar}) and for 
 $\bar{\mathbf{s}}_{\perp}^*$, the solution in the original space is given by
 \begin{align*}
 	\mathbf{s}^* &=
	\mathbf{U}_{\parallel} \mathbf{s}_{\parallel}^* + 
	\mathbf{U}_{\perp}^{\phantom{^*}} \mathbf{s}_{\perp}^* \\
	&=
%	-\mathbf{U}_{\parallel}\mathbf{C}_{\parallel}\bar{\mathbf{g}}_{\parallel} - \alpha^* \bar{\mathbf{g}}_{\perp}
%= 
%-\mathbf{C}_{\parallel}\bar{\mathbf{g}}_{\parallel} 
- \mathbf{U}_{\parallel}\mathbf{C}_{\parallel}\mathbf{U}_{\parallel}^{\top} \mathbf{g} 
- \alpha^* (\mathbf{I}_n - \mathbf{U}_{\parallel}\mathbf{U}_{\parallel}^{\top}) \mathbf{g}\\
&= -\alpha^* \mathbf{g}  + \mathbf{U}_{\parallel}(\alpha^* \mathbf{I} - \mathbf{C}_{\parallel})\mathbf{U}_{\parallel}^{\top} \mathbf{g}.
 \end{align*}
 Note that computing $\mathbf{s}^*$ neither  involves forming $\mathbf{U}_{\perp}$ nor
 computing $\bar{\mathbf{g}}_{\perp}$ explicitly.
 
\bigskip




\noindent \textbf{Termination criteria.} 
With each cubic subproblem solved, the iterations are terminated when 
the change in iterates, $\mathbf{s}_k$, is sufficiently small, i.e., 
\begin{equation}\label{eq:acceptance2}
\| \mathbf{s}_k \|_2 < \tilde{\epsilon} \| \mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k\|_2,
\end{equation}
for some $\tilde{\epsilon}$, 
or when the maximum number of allowable iterations is achieved.
The proposed Adaptive Regularization using Cubics with L-SR1 (ARCs-LSR1) algorithm is given in Algorithm \ref{alg:LSR1ARC}.






\begin{algorithm}[!h]
	\caption{Adaptive Regularization using Cubics with Limited-Memory SR1 (ARCs-LSR1) }
	\begin{algorithmic}[1]
		\STATE $\textbf{Given: }\Theta_0, \gamma_2 \geq \gamma_1, 1 > \eta_2 \geq \eta_1 > 0,\  \sigma_0 > 0, \tilde{\epsilon} > 0, k = 0,$	 and $k_{\text{max}} > 0$
%		\Require $S_k = \{s_0, \ldots, s_k\}$, $Y_k = \{y_0, \ldots, y_k\}$
		\WHILE {$k < k_{\text{max}} \ \text{and} \  \| \mathbf{s}_k \|_2 \ge \tilde{\epsilon} \| \mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k\|_2$}
		\STATE {Obtain $\mathbf{S}_k = [ \ \mathbf{s}_0 \ \  \cdots \ \  \mathbf{s}_k \ ]$ and $\mathbf{Y}_k = [ \ \mathbf{y}_0 \ \  \cdots \ \ \mathbf{y	}_k \ ]$}
		\STATE {Solve the generalized eigenvalue problem $\mathbf{S}_k^{\top}\mathbf{Y}_k \mathbf{u} = \hat{\lambda}\mathbf{S}_k^{\top}\mathbf{S}_k \mathbf{u}$ 
		and let $\delta_k=\min\{ \hat{\lambda}_i\}$}
		\STATE {Compute $\mathbf{\Psi}_k = \mathbf{Y}_k - \delta_k \mathbf{S}_k$}
%		\If {Cholesky is available}
%		\State {$\Psi^{\top} \Psi = R^{\top}R$}
%		\State {$Q = \Psi R^{-1}$}
%		\Else  { Perform QR-decomposition of $\Psi$}
		\STATE {Perform QR decomposition of $\mathbf{\Psi}_k = \mathbf{Q}\mathbf{R}$}
%		\EndIf
		\STATE {Compute eigendecomposition 
		%\begin{equation}%\label{eqn:eigenvaluedecomposition}
		$	\mathbf{RMR}^\top= \mathbf{P\Lambda P}^\top$
		%\end{equation}
		
		}
		\STATE {Assign $\mathbf{U}_\parallel = \mathbf{QP}$ and $\mathbf{U}_{\parallel}^{\top} = \mathbf{P}^{\top} \mathbf{Q}^{\top}$}
		\STATE {%With $D = \text{diag}(\lambda_0,\ldots, \lambda_{m})$, 
		Define $\mathbf{C}_\parallel = \text{diag}(c_1,\ldots, c_k)$, where $c_i = \frac{2}{\lambda_i + \sqrt{\lambda_i^2 + 4\mu | (\bar{\mathbf{g}}_{\parallel})_{i}|}}$ 
		and  $\bar{\mathbf{g}}_\parallel = \mathbf{U}^\top_\parallel \mathbf{g}$}
		\STATE Compute {$\alpha^{*}$ in \eqref{eq:alphastar}} %= \frac{2}{\delta_k + \sqrt{\delta_k^2 + 4\mu\| \bar{\mathbf{g}}_{\perp}\|}}$} %where $\mathbf{g}_{\perp} = \mathbf{g} - \mathbf{U}_\parallel \bar{\mathbf{g}}_\parallel$
		\STATE {Compute step $\! \mathbf{s}^* = -\alpha^{*}\mathbf{g} + \mathbf{U}_{\parallel}(\alpha^{*}\mathbf{I} - \mathbf{C}_{\parallel})\mathbf{U}_{\parallel}^{\top}\mathbf{g}$}
		\STATE Compute $m_k(\mathbf{s}^*\!)$ \! and \!  $\rho_k \! \!=\!  (f(\Theta_k) 
		\!-\! f(\Theta_{k+1})\!)\!/m_k(\mathbf{s}^*\!)$% from (\ref{eq:ratio}) in Appendix A
		\STATE {Set 
		\begin{align*}
			\Theta_{k+1} &=
			\begin{cases}
				\Theta_k + \mathbf{s}^* \hspace{.85cm}  & \text{if }\rho_k\geq\eta_1\\
				\Theta_k, & \text{otherwise}		
			\end{cases}, \quad \text{and} 
			\\
%			 \left\{ 
%			\begin{array}{lr}
%				\Theta_k + s_k, & \text{if }\rho_k\geq\eta_1,\\
%				\Theta_k, & \text{otherwise}
%			\end{array}\right\}.
%		\end{align*}
%		\begin{align*}
			\mu_{k+1} &=
			\begin{cases}
				\tfrac{1}{2} \mu_k & \text{if }\rho_k > \eta_2,\\
				\tfrac{1}{2} \mu_k (1 + \gamma_1) & \text{if }\eta_1 \leq \rho_k \leq \eta_2,\\
				\tfrac{1}{2} \mu_k (\gamma_1 + \gamma_2) & \text{otherwise}			
			\end{cases}
%			\left\{\begin{array}{lr}
%				[0, \sigma_k] & \text{if }\rho_k > \eta_2,\\
%				\left[\sigma_k, \gamma_1\sigma_k\right] & \text{if }\eta_1 \leq \rho_k \leq \eta_2,\\
%				\left[\gamma_1\sigma_k, \gamma_2\sigma_k\right] & \text{otherwise}
%			\end{array}\right\}.
		\end{align*}
		}
		\STATE {$k \leftarrow k+1$}
			\ENDWHILE
	\end{algorithmic}\label{alg:LSR1ARC}
\end{algorithm}

%\begin{algorithm}[!h]
%	\caption{Adaptive Regularization using Cubics (ARC)}
%	\begin{algorithmic}
%		\State $\textbf{Given: }\Theta_0, \gamma_2 \geq \gamma_1, 1 > \eta_2 \geq \eta_1 > 0,\ \text{and}\ \sigma_0 > 0$
%		\While{$k \leq k_{\text{max}}$}
%		\State Compute a step $s_k$ using Algorithm \ref{alg:LSR1obs}
%		\State Compute $\rho_k$ using modified formula ratio of actual reduction to model reduction
%		\State Set 
%		\begin{equation*}
%			\Theta_{k+1} = \left\{ 
%			\begin{array}{lr}
%				\Theta_k + s_k, & \text{if }\rho_k\geq\eta_1,\\
%				\Theta_k, & \text{otherwise}
%			\end{array}\right\}.
%		\end{equation*}
%		\State Set 
%		\begin{equation*}
%			\sigma_{k+1} \in \left\{\begin{array}{lr}
%				[0, \sigma_k] & \text{if }\rho_k > \eta_2,\\
%				\left[\sigma_k, \gamma_1\sigma_k\right] & \text{if }\eta_1 \leq \rho_k \leq \eta_2,\\
%				\left[\gamma_1\sigma_k, \gamma_2\sigma_k\right] & \text{otherwise}
%			\end{array}\right\}.
%		\end{equation*}
%		\EndWhile
%	\end{algorithmic}\label{alg:adaptivereg}
%\end{algorithm}
%


%
%\subsection{Contributions}
%The main contributions of this paper are as follows:
%\begin{enumerate}[leftmargin=0.45cm]
%	\itemsep 0em
%	\item \textbf{L-SR1 quasi-Newton methods.} The most commonly used quasi-Newton approach is the L-BFGS method.  In this work, we use the L-SR1 update to better model potentially indefinite Hessians of the non-convex loss function. 
%	\item \textbf{Adaptive Regularization using Cubics (ARCs).} Given that the quasi-Newton approximation is allowed to be indefinite, we use an Adaptive Regularized using Cubics approach to 
%safeguard each search direction.
%	\item \textbf{Shape-changing regularizer.} 
%	We use a shape-changing norm to define the cubic regularization term, which allows us 
%	to compute the closed form solution to cubic subproblem (\ref{eq:cr}).  
%	\textbf{Computational complexity.} Let  $m$ be the number of previous iterates and gradients stored in memory. The proposed LSR1 ARC approach is comparable to L-BFGS in terms of storage and compute complexity (see Table \ref{tbl:storagecomplexity}).  
%	\begin{table*}[h]
%		\centering
%		\caption{Storage and compute complexity of the methods used in our experiments.}
%		\begin{tabular}{|c|c|c|}
%			\hline
%			\textbf{Algorithms} & \textbf{Storage complexity} & \textbf{Compute complexity}\\
%			\hline
%			SGD/Adaptive methods & $\mathcal{O}(n)$ & $\mathcal{O}(n)$ \\
%			L-BFGS & $\mathcal{O}(n + mn)$ &  $\mathcal{O}(mn)$\\
%			ARCs-LSR1 & $\mathcal{O}(n + mn)$ & $\mathcal{O}(m^3 + 2mn)$ \\ 
%			\hline
%		\end{tabular}\label{tbl:storagecomplexity}
%		\centering
%	\end{table*}
%\end{enumerate}
%%

\medskip

\noindent \textbf{Convergence.} 
Here, we prove convergence properties of the proposed method (ARCs-LSR1 in Algorithm \ref{alg:LSR1ARC}).
The following theoretical guarantees follow the ideas from \citet{Benson2018,cartis2011adaptive}.
First, we make the following mild assumptions:


\medskip

\noindent 
\textbf{A1.} The loss function $f(\Theta)$ is continuously differentiable, i.e., 
$f \in C^1(\mathbb{R}^n)$.

\noindent
\textbf{A2.} The loss function $f(\Theta)$ is bounded below.

\medskip

%
%It is reasonable to assume that the function $f$ in \eqref{eq:emp} is bounded below by some value $K$ and continuous.
%\begin{lemma}\label{con:lemma1}
%		$f \in C^1(\mathbb{R}^n)$
%\end{lemma}

\noindent 
Next, 
%under the assumption that the norm of the rank-1 matrix $(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)
%	(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)^{\top}$ 
%	in (\ref{eq:SR1_B0}) is bounded above
%	(see \cite{Benson2018}), 
%we obtain a upper bound on the norm of the Hessian approximation $\mathbf{B}_k$.
we prove that the matrix $\mathbf{B}_k$ in (\ref{eq:SR1_B0}) is bounded.  

\begin{lemma}\label{lemma:1}
%If $\| (\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)
%(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)^{\top} \|_F \le K$ for some constant $K > 0$, then
The SR1 matrix $\mathbf{B}_{k+1}$  in (\ref{eq:SR1_B0}) satsifies
$$
	\text{$\|\mathbf{B}_{k+1}\|_F  \leq \kappa_B$  \ \ \text{for all $k \geq$ 1}}
$$
for some $\kappa_B$ $>$ 0.
\end{lemma}

\textit{Proof:} 
Using the limited-memory SR1 update with memory parameter $m$ in (\ref{eq:SR1_B0}), we have
$$
	\| \mathbf{B}_{k+1} \|_F \le \| \mathbf{B}_0 \|_F + 
	\hspace{-.4cm}
	\sum_{j = k-m+1}^k 
	\hspace{-.4cm} 
	\frac{\| (\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j) (\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)^{\! \top} \! \|_F}
	{| \mathbf{s}_j^{\top} ( \mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j) |}.
%	\le \delta_{\max} + \frac{m K}{\varepsilon} \equiv \kappa_B.
$$
Because $\mathbf{B}_0 = \delta_k \mathbf{I}$ with $\delta_k < \delta_{\max}$ for some $\delta_{\max} > 0$,
we have that $\| \mathbf{B}_0 \|_F = \sqrt{n} \delta_{\max}$.  
Using a property of the Frobenius norm,
namely, for real matrices $\mathbf{A}$, $\| \mathbf{A} \|_F^2 = \text{trace}(\mathbf{AA}^{\top})$, we have that
$\| (\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j) (\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)^{\top} \|_F 
= \| \mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j \|_2^2$.
Since the pair $(\mathbf{s}_j, \mathbf{y}_j)$ is accepted only when $|\mathbf{s}_j^{\top}(\mathbf{y}_j - \mathbf{B}_j\mathbf{s}_j)| > \varepsilon \| \mathbf{s}_j \|_2 \| \mathbf{y}_j - \mathbf{B}_j \mathbf{s}_j \|_2$, for some constant $\varepsilon > 0$, and since $\| \mathbf{s}_k \|_2 \ge \tilde{\epsilon} \| \mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k\|_2$, we have
$$
	\| \mathbf{B}_{k+1} \|_F \le \sqrt{n} \delta_{\max} + \frac{m}{\varepsilon \tilde{\epsilon}} \equiv \kappa_B,
$$
which completes the proof.
$\square$

\medskip

\noindent 
Given the bound on $\| \mathbf{B}_{k+1} \|_F$, we obtain the following result, which is similar to Theorem 2.5 in \citet{cartis2011adaptive}.

\begin{theorem}\label{thm:liminf}
	Under Assumptions \textbf{A1}  and \textbf{A2}, if Lemma \ref{lemma:1} holds, then
	$$\underset{k \to \infty}{\text{lim inf}} \  \|\mathbf{g}_k\| = 0.$$
\end{theorem}

\noindent 
Finally, we consider the following assumption, which can be satisfied when the gradient, $\mathbf{g}(\Theta)$, is Lipschitz continuous on $\Theta$. 

\medskip

\noindent 
\textbf{A3.} If $\{ \Theta_{t_i} \}$ and $\{ \Theta_{l_i} \}$ are subsequences of $\{ \Theta_k \}$, then  $\| \mathbf{g}_{t_i} - \mathbf{g}_{l_i} \| \rightarrow 0$ whenever 
$\| \Theta_{t_i} - \Theta_{l_i} \| \rightarrow 0$ as $i \rightarrow \infty$.


\medskip

\noindent 
If we further make Assumption  \textbf{A3}, we have the following stronger result (which is based on Corollary 2.6 in \citet{cartis2011adaptive}):

\begin{corollary}\label{cor:ARCs}
Under Assumptions \textbf{A1},  \textbf{A2}, and \textbf{A3}, 
 if Lemma \ref{lemma:1} holds, then
	$$\underset{k \to \infty}{\text{lim}} \|\mathbf{g}_k\| = 0.$$
\end{corollary}

By Corollary 2.3, the proposed ARCs-LSR1 method converges to first-order critical points.   


\noindent \textbf{Stochastic implementation.} Because full gradient computation is very expensive to perform, we impement a stochastic version 
of the proposed ARCs-LSR1 method.  In particular, we use the batch gradient approximation
$$
	\tilde{\mathbf{g}}_k \equiv \frac{1}{| \mathcal{B}_k |} \sum_{i \in \mathcal{B}_k} \nabla f_i (\Theta_k).
$$
In defining the SR1 matrix, we use the quasi-Newton pairs $(\mathbf{s}_k, \tilde{\mathbf{y}}_k)$,
where $\tilde{\mathbf{y}}_k = \tilde{\mathbf{g}}_{k+1} - \tilde{\mathbf{g}}_k$ (see e.g., \citet{Erway2020TrustregionAF}).
We make the following additional assumption (similar to Assumption 4 in  \citet{Erway2020TrustregionAF}) to guarantee that the loss function $f(\Theta)$ decreases over time:

\medskip

\noindent
\textbf{A4.} The loss function $f(\Theta)$ is fully evaluated at every $J > 1$ iterations (for example, 
at iterates $\Theta_{J_0}, \Theta_{J_1}, \Theta_{J_2}, \dots,$ where $0 \le J_0 < J$ and
$J = J_1 - J_0 = J_2 - J_1 = \cdots $) and nowhere else in the algorithm.  The batch size $d$ is increased 
monotonically if $f(\Theta_{J_{\ell}}) > f(\Theta_{J_{\ell - 1}}) - \tau$ for some $\tau > 0$.

\medskip

\noindent 
With this added assumption, we can show that the stochastic version of the proposed ARCs-LSR1 method converges.

\begin{theorem}\label{thm:sARCs}
	The stochastic version of ARCs-LSR1 converges with  
	$$\underset{k \to \infty}{\text{lim}} \|\mathbf{g}_k\| = 0.$$
\end{theorem}

\textit{Proof:} Let $\widehat{\Theta}_i = \Theta_{J_i}$.  By Assumption 4, $f(\Theta)$ must 
decrease monotonically over the subsequence $\{ \widehat{\Theta}_i \}$ or $d \rightarrow |\mathcal{D}|$,
where $|\mathcal{D}|$ is the size of the dataset.    If the objective function is decreased 
$\iota_k$ times over the subsequence $ \{ \widehat{\Theta}_i\}_{i=0}^k$, then
%\begin{eqnarray*}
%	f(\widehat{\Theta}_k) &=& f(\hat{\Theta}_0) + \sum_{i=1}^{\iota_k}
%	\left \{
%		f(\widehat{\Theta}_i) - f(\widehat{\Theta}_{i-1})
%	\right \} \\
%	&\le& f(\widehat{\Theta}_0) - \iota_k \tau.
%\end{eqnarray*}
\begin{eqnarray*}
	f(\widehat{\Theta}_k) = f(\hat{\Theta}_0)  + \sum_{i=1}^{\iota_k}
	\left \{
		f(\widehat{\Theta}_i)  -  f(\widehat{\Theta}_{i-1})
	\right \} \le  f(\widehat{\Theta}_0) - \iota_k \tau.
\end{eqnarray*}
If $d \rightarrow |\mathcal{D}|$, then $\iota_k \rightarrow \infty$ as $k \rightarrow \infty$.
By Assumption \textbf{A2}, $f(\Theta)$ is bounded below, which implies $\iota_k$ is finite.  
Thus, $d \rightarrow |\mathcal{D}|$, and the algorithm reduces to the full ARCs-LSR1 method,
whose convergence is guaranteed by Corollary \ref{cor:ARCs}.  $\square$

\medskip

\noindent 
We note that the proof to Theorem \ref{thm:sARCs} follows very closely the proof of Theorem 2.2 in 
\citet{Erway2020TrustregionAF}.


%\ref{appnd:stochastic}.  

%\begin{equation*}
%	\mathbf{B}_{k+1} = \mathbf{B}_0 + \mathbf{Y}_k -\delta_k \mathbf{S}_k \mathbf{M}_k (\mathbf{Y}_k -\delta_k \mathbf{S}_k)^\top,
%\end{equation*}
%where $\mathbf{B}_0 = \delta_k I$. Using the triangle inequality, we have
%\begin{equation*}
%	\norm{\mathbf{B}_{k+1}} \leq \norm{\mathbf{B}_0}  + \norm{\mathbf{Y}_k -\delta_k \mathbf{S}_k \mathbf{M}_k (\mathbf{Y}_k -\delta_k \mathbf{S}_k)^\top}.
%\end{equation*}
%We rewrite the equation above as follows:
%\begin{equation*}
%	\norm{\mathbf{B}_{k+1}} \leq \norm{\mathbf{B}_0}  + \frac{m}{\epsilon} \max_{k - m + 1\leq i \leq k} \norm{\mathbf{y}_i - \mathbf{B}_i \mathbf{s}_i}_2^2
%\end{equation*}
%
%It is reasonable to assume that $\norm{\mathbf{y}_i - \mathbf{B}_i \mathbf{s}_i}_2^2 = (\mathbf{y}_i - \mathbf{B}_i \mathbf{s}_i)(\mathbf{y}_i - \mathbf{B}_i \mathbf{s}_i)^T$ is bounded above in norm (see \cite{Benson2018} Lemma 1). Thus, we prove $\mathbf{B}_k$ is bounded:
%
%If lemma \ref{con:lemma1} and lemma \ref{con:lemma2} hold, the following theorem is obtained.

%\begin{theorem}\label{thm:con}
%	$\underset{k \to \infty}{\text{lim inf}} \norm{g_k} = 0$ 
%\end{theorem}
%For proof, please refer \cite{Benson2018}, theorem 2.5.

%Additionally, we make the following assumption,
%\begin{assumption}\label{con}
%	$\norm{g_t - g_l} \to 0$ whenever $\norm{\Theta_t - \Theta_l} \to 0$, $i \to \infty$.
%\end{assumption}
%
%
%
%Since Lemma \ref{con:lemma1}, Lemma \ref{con:lemma2}, Theorem \ref{thm:con},\ref{con} hold and $f(\Theta_k)$is bounded below, we state the following corollary
%\begin{corollary}
%	$\underset{k \to \infty}{\text{lim}} \norm{g_k} = 0$.
%\end{corollary}

\noindent \textbf{Complexity analysis.} 
SGD methods and the related adaptive methods require $\mathcal{O}(n)$ memory storage to store
the gradient and $\mathcal{O}(n)$ computational complexity to update each iterate.
Such low memory and computational requirements make these methods easily implementable.  
Quasi-Newton methods store the previous $m$ gradients and use them to compute the update at each iteration.  Consequently,  L-BFGS methods require $\mathcal{O}(mn)$ memory storage to store
the gradients and $\mathcal{O}(mn)$ computational complexity to update each iterate 
(see \citet{Burdakov2017} for details).  Our proposed ARCs-LSR1 approach also uses 
$\mathcal{O}(mn)$ memory storage to store the gradients, but the computational 
complexity to update each iterate requires an additional eigendecomposition of the $m \times m$
matrix $\mathbf{RMR}^{\top}$, so that the overall computational complexity at each iteration is
$\mathcal{O}(m^3+ mn)$.  However, since $m \ll n$, this additional factorization does not significantly
increase the computational time.% (see Table \ref{tbl:storagecomplexity}).

%	\begin{table}[!h]
%		\centering
%		\caption{Storage and compute complexity of the methods used in our experiments.}
%		\begin{tabular}{|c|c|c|}
%			\hline
%			\textbf{Algorithms} & \textbf{Storage complexity} & \textbf{Compute complexity}\\
%			\hline
%			SGD/Adaptive methods & $\mathcal{O}(n)$ & $\mathcal{O}(n)$ \\
%			L-BFGS & $\mathcal{O}(n + mn)$ &  $\mathcal{O}(mn)$\\
%			ARCs-LSR1 & $\mathcal{O}(n + mn)$ & $\mathcal{O}(m^3 + 2mn)$ \\ 
%			\hline
%		\end{tabular}\label{tbl:storagecomplexity}
%		\centering
%	\end{table}