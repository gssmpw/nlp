
\noindent \textbf{Optimization approaches.} 
We list the various optimization approaches to which we compared our proposed method.  For the numerical experiments, we empirically fine-tuned the hyperparameters and selected the best for each update scheme.




\begin{enumerate}

\item \textbf{Stochastic Gradient Descent (SGD) with Momentum}  (see e.g., \citet{Qia99}).
%A gradient-descent algorithm that uses (i) an estimate of the gradient calculated from a randomly selected subset of the dataset, and (ii) a moving average of these gradient approximations .  
For the experiments, we used a momentum parameter of $0.9$ and a learning rate of $1.0 \times 10^{-1}$.

\item 
\textbf{Adaptive Gradient Algorithm (Adagrad)} 
%An algorithm similar to SGD but with an adaptive learning rate for each dimension at each iteration 
(see \citet{duchi2011adaptive}).  
In our experiments, the initial accumulator value is set to 0, the perturbation $\epsilon$ is set to $1.0 \times 10^{-10}$, and the learning rate is set to $1.0 \times 10^{-2}$.


\item \textbf{Root Mean Square Propagation (RMSProp)} 
%An algorithm similar to Adagrad but decays the contribution of older gradients at each iteration 
(see \citet{hinton2012neural}).  For our experiments, the perturbation $\epsilon$ is set  to $1.0 \times 10^{-8}$. We set $\alpha = 0.99$, and used a learning rate of $1.0 \times 10^{-2}$.

\item \textbf{Adam} 
%Related to RMSProp, this algorithm generates its parameter updates using a running average of first and second moment of the gradient 
(see \citet{kingma2014adam}). For  our experiments, we apply an $\epsilon$ perturbation of $1.0 \times 10^{-6}$. The momentum parameters $\beta_0$ and $\beta_1$ are chosen to be 0.9 and 0.999, respectively.  The learning rate is set to $1.0 \times 10^{-3}$. 


\item \textbf{Limited-memory BFGS (L-BFGS)}: We set the default learning rate to $1.0$. The tolerance on function value/parameter change is set to $1.0 \times 10^{-9}$ and the first-order optimality condition for termination is defined as $1.0 \times 10^{-9}$

\item  \textbf{ARCs-LSR1 (Proposed method)}: For the experiments, we choose the same parameters as those used in L-BFGS. %We also provide the performance of this approach with different set of parameter for the MNIST classification in the appendix section.
%like SGD, Adagrad, Adam, RMSProp and L-BFGS
\end{enumerate}
%For more information with different hyperparameters, please refer the appendix section.

\noindent \textbf{Dataset.}
We measure the performance of each optimization method on the following five commonly-used datasets for training and testing in machine learning: (1) MNIST \citep{lecun2010mnist}, (2)  CIFAR10 \citep{CIFAR10}, (3) Fashion-MNIST  \citep{xiao2017/online} and (4) IRIS \citep{fisher1936use,anderson1935irises} and (5) Penn Tree Bank \citep{treebank}.
%\begin{enumerate}
%\item \textbf{IRIS:}   A dataset consisting of 50 samples from each of three species of the iris flower (iris setosa, iris virginica, and iris versicolor).
%The features correspond to the length and width of the sepals and petals for each sample \cite{Dua:2019}.
%\item \textbf{MNIST:}  A database of $28 \times 28$ grayscale images of handwritten digits from $0$ to $9$, containing 60,000 training images and 10,000 testing images \cite{lecun2010mnist}.
%\item \textbf{CIFAR10:}  A dataset consisting of $32 \times 32$ color images of 10 mutually exclusive classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck) with 6,000 images per class, 50,000 training images, and 10,000 testing images \cite{CIFAR10}.
%\item \textbf{Fashion-MNIST:}  A database of $28 \times 28$ grayscale images of articles of clothing associated with a label from 10 classes, containing 60,000 training images and 10,000 testing images  \cite{xiao2017/online}.
%\end{enumerate}
%We have provided a comprehensive view of the experiments in Table \ref{tbl:experiments}

%\begin{table}[h]
%	\centering
%	\begin{tabular}{|c|c|c|}
%		\hline
%		\textbf{Dataset} & \textbf{Network} & \textbf{Type}\\[0.5ex]
%		\hline
%		IRIS & MLP & Classification\\
%		MNIST & MLP & Classification\\
%		FMNIST & Convolutional & Classification\\
%		CIFAR10 & Convolutional & Classification \\
%		FashionMNIST & Convolutional & Reconstruction\\
%		MNIST & Convolutional & Reconstruction\\
%		\hline
%	\end{tabular}
%	\caption{List of experiments}
%	\label{tbl:experiments}
%\end{table}

%\subsection{Network architecture}

%For each problem, we define the model architecture in Table \ref{tbl:networks}.
%
%\begin{table}[t]
%	\centering
%	\begin{tabular}{|c|c|c|c|c|}
%		\hline
%		\textbf{Dataset} & \textbf{Network} & \textbf{Convolution layers} & \textbf{Fully connected layers} & \textbf{Parameters}\\[0.5ex]
%		\hline
%		IRIS & Classifier & - & 3 & 2953\\
%		MNIST & Classifier & - & 3 & 397510\\
%		CIFAR10 & Classifier & 2 & 3 & 62006\\
%		MNIST & Autoencoder & 6 & 4 & 53415\\
%		FashionMNIST & Autoencoder & 6 & 4 & 53415\\
%		\hline
%	\end{tabular}
%	\caption{List of experiments}
%	\label{tbl:networks}
%\end{table}
%\subsection{Testbed and software}
%All experiments were conducted using open-source software PyTorch (\cite{NEURIPS2019_9015}), SciPy (\cite{2020SciPy-NMeth}) and NumPy (\cite{harris2020array}). We use an Intel Core i7-8700 CPU with a clock rate of 3.20 GHz and an NVIDIA RTX 2080 Ti graphics card.

