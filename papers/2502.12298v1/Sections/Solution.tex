%We define the optimization problem as follows:
%\begin{equation}
%	\underset{x \in \mathbb{R}^n}{\text{min}} f(\Theta)
%\end{equation}
%
%This can be decomposed to the following:
%\begin{equation*}
%\underset{\bar{\mathbf{s}}\in \mathbb{R}^n}{\text{min}} \bar{\mathbf{m}}(\bar{\mathbf{s}}) = \sum\limits_{i=1}^{n} \underset{\bar{\mathbf{s}}_i}{\text{min}} \left( \bar{\mathbf{g}}_i\bar{\mathbf{s}}_i  + \frac{\lambda_i}{2} \bar{\mathbf{s}}_i^2 + \frac{\mu}{3} |\bar{\mathbf{s}_i}|^3\right)
%\end{equation*}
%
%We re-write the solution from Algorithm \ref{alg:LSR1ARC} as $\bar{\mathbf{s}}^{*} = - \mathbf{C}\bar{\mathbf{g}}$, where $\mathbf{C} = diag(c_1, \ldots, c_n)$ and $c_i = \frac{2}{\lambda_i + \sqrt{\lambda_i^2 + 4\mu|\bar{\mathbf{g}}_{i}|}}$. We provide the complete formulation and solution to the subproblem in sections \ref{sec:B0}, \ref{sec:limsetting} and \ref{sec:closedformsolve}
%
%\subsection{Limited-memory}\label{sec:limsetting}
%We acknowledge that a $n \times n$ Hessian approximation cannot be stored. Hence we propose a limited-memory solution. We use the compact representation equation (\ref{eqn:compactSR1}) to define our approximation update. Now, $\Psi$ is an $m \times n$ matrix where $m \ll n$. We perform a `thin' QR-decomposition on $\Psi = \mathbf{Q}\mathbf{R}$. This yields
%
%
%\begin{equation*}	
%\mathbf{B}_{k+1} \ = \ \mathbf{B}_0 + 
%	\begin{bmatrix}
%	\\
%	\mathbf{Q}_{k+1}\mathbf{R}_{k+1}  \\
%	\phantom{t}
%	\end{bmatrix}
%	\hspace{-.3cm}
%	\begin{array}{c}
%	\left  [ \  \mathbf{M}_{k+1}^{\phantom{h}}  \right ] \\
%	\\
%	\\
%	\end{array}
%	\hspace{-.3cm}
%	\begin{array}{c}
%	\left [  \ \quad \mathbf{R}_{k+1}^{\top}\mathbf{Q}_{k+1}^{\top}\quad \ \right ] \\
%	\\
%	\\
%	\end{array},
%\end{equation*}
%where $\mathbf{R} \in \mathbb{R}^{m \times m}$, $\mathbf{Q} \in \mathbb{R}^{m \times n}$. Next, we compute the spectral decomposition on the matrix $\mathbf{RMR}^\top$ to yield $\mathbf{P}\Lambda \mathbf{P}^\top$. We define $\mathbf{U}_\parallel = \mathbf{QP}$ and form the following representation:
%\begin{equation*}
%\mathbf{B}_{k+1} \ = \ \mathbf{B}_0 + 
%	\begin{bmatrix}
%	\\
%	\mathbf{U_{\parallel}} \\
%	\phantom{t}
%	\end{bmatrix}
%	\hspace{-.3cm}
%	\begin{array}{c}
%	\left  [ \  \Lambda^{\phantom{h}}  \right ] \\
%	\\
%	\\
%	\end{array}
%	\hspace{-.3cm}
%	\begin{array}{c}
%	\left [  \ \quad \mathbf{U}_{\parallel}^{\top}\quad \ \right ] \\
%	\\
%	\\
%	\end{array}.
%\end{equation*}
%
%\subsection{Dynamic initialization of $\mathbf{B}_0$}\label{sec:B0} 
%
%We define $\mathbf{B}_0 = \delta \mathbf{I}$, where $\delta = 0 < \delta < \hat{\lambda}_i $.
%$\hat{\lambda}_i$ denotes the smallest eigenvalue of the generalized eigenvalue problem 
%\begin{equation*}
%	(\mathbf{D}_k + \mathbf{L}_k + \mathbf{L}_k^{\top})\mathbf{u} = \hat{\lambda}_i \mathbf{S}^{\top}_k \mathbf{S}_k \mathbf{u}
%\end{equation*}
%For further information, refer to \cite{Erway2020TrustregionAF} (Lemma 2.4)
%
%\subsection{Closed-form solution}\label{sec:closedformsolve}
%Now we are ready to discuss the closed-form solution of the cubic-regularized model. Suppose we approximate a $n \times n$ Hessian. The closed form solution in the new space of variables is given by
%\begin{equation*}
%	\bar{\mathbf{s}}^* = -\mathbf{\mathbf{C}}\bar{\mathbf{g}}.
%\end{equation*}
%Here, $\mathbf{C} = diag(c_1,\ldots, c_n)$ and $c_i \overset{\text{def}}{=} \frac{2}{\lambda_i^2 + \sqrt{\lambda_i^2 + 4 \mu |\bar{g}_i|}}$, where $\bar{\mathbf{g}} = \mathbf{U}^\top \mathbf{g}$. To get the closed form solution in the original space, we transform the new space back to the original space as
%\begin{equation*}
%	\mathbf{s}^* = \mathbf{U} \bar{\mathbf{s}}^*.
%\end{equation*}
%However, we only operate on a limited memory approximation of the Hessian. We define $\mathbf{U} = [ \mathbf{U}_\parallel, \mathbf{U}_\perp] \in \mathbb{R}^{n \times n}$, where we only operate on $\mathbf{U}_\parallel$. Thus the formulation is defined as 
%\begin{align*}
%	\mathbf{B}u &= \delta \mathbf{u},\\
%	\mathbf{B}u &= (\delta +\lambda_i) \mathbf{u},
%\end{align*}
%where $\delta$ is computed using the formulation described in \ref{sec:B0} and $\lambda_i$ is defined by the eigen value decomposition in . Thus the solution in the new space is given by,
%\begin{align*}
%	\bar{\mathbf{s}}_\parallel^{*} = \mathbf{U}_{\parallel}^{\top}\mathbf{s}^{*},\quad \bar{\mathbf{s}}^{*}_{\perp} = \mathbf{U}_{\perp}^{\top}\mathbf{s}^{*},\\
%	\bar{\mathbf{g}}_{\parallel} = \mathbf{U}_{\parallel}^{\top}\mathbf{g}, \quad \bar{\mathbf{g}}_{\perp} = \mathbf{U}_{\perp}^{\top}\mathbf{g}.
%\end{align*}
%We make the following observations:
%\begin{enumerate}
%	\item $\delta$ is a multiple eigen value. This means $\mathbf{U}_{\perp}$ is not uniquely defined.
%	\item $\mathbf{s}^*$ depends on the choice of $\mathbf{U}_{\perp}$.
%	\item $\bar{\mathbf{g}}_{\perp}$ is used for computing $\bar{\mathbf{s^{*}}}_{\perp}$ and it requires $\mathbf{U}_{\perp}^{\top}\mathbf{g}$.
%\end{enumerate} 
%
%$\mathbf{U}_{\perp}^{\top}\mathbf{g}$ may be prohibitively expensive to compute and store, unless $\mathbf{U}_{\perp}$ is chosen in a special way. We define $\mathbf{g}_{\perp} = (\mathbf{I}_n - \mathbf{U}_{\parallel} \mathbf{U}_{\parallel}^{\top})\mathbf{g}$ which can be re-written as $\mathbf{g}_{\perp} =\mathbf{g} - \mathbf{U}_{\parallel} \mathbf{g}_{\parallel}$.
%
%\subsection{Adaptive regularized cubics}
Once we compute the step in Algorithm \ref{alg:LSR1ARC}, we compute the ratio between the reduction in the actual objective function and the reduction in the model  defined by
		\begin{equation}\label{eq:ratio}
		\rho_k = (f(\Theta_k) - f(\Theta_{k+1}))/(m(\mathbf{s}^*)).
		\end{equation}
where
\begin{equation*}
m(\mathbf{s}^*) = q(\mathbf{s}^*) + \frac{\mu}{3} (\norm{\mathbf{C}_\parallel \bar{\mathbf{g}}_{\parallel}}_3^3 + (\alpha^*)^3\norm{\mathbf{g}_{\perp}}_2^3).
\end{equation*}
and
$$q(\mathbf{s}^*) = \bar{\mathbf{g}}_\parallel^{\top}(\mathbf{C}_{\parallel}^2 \Lambda_{\parallel} + \mathbf{C}_{\parallel}\bar{\mathbf{g}}_{\parallel} + \frac{\delta_k(\alpha^{\star})^2 - 2\alpha^*}{2} \norm{\mathbf{g}_{\perp}}.
$$

