\documentclass[11pt,a4paper]{article}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\input{header.en.tex}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Tackling the Zero-Shot RL Loss Directly}
\author{Yann Ollivier}
\date{}

\newcommand{\TODO}[1]{{\color{red} TODO: {#1}}}
\newcommand{\todo}[1]{\TODO{#1}}
\newcommand{\option}[1]{{\color[rgb]{.4,0,.8}[Optional:#1]}} %blue for working version: optional stuff
%%REMOVE COMMENTS FOR FINAL VERSIONS (though some spacing may be incorrect around comments)
% \renewcommand{\TODO}[1]{}
% \renewcommand{\todo}[1]{}
% \renewcommand{\option}[1]{}  %remove optional text

\newcommand{\tsum}{{\textstyle\sum}}
\newcommand{\betatest}{\beta_{\mathrm{test}}}
\newcommand{\losstest}{\ell_{\mathrm{test}}}
\newcommand{\Dir}{_\mathrm{Dir}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\orth}{_\mathrm{orth}}

\begin{document}

\maketitle

\begin{abstract}
Zero-shot reinforcement learning (RL) methods aim at instantly producing a behavior for an RL task in a given environment, from a description of the reward function.  These methods are usually tested by evaluating their average performance on a series of downstream tasks. Yet they cannot be trained directly for that objective, unless the distribution of downstream tasks is known.  Existing approaches either use other learning criteria \cite{borsa2018universal,zeroshot,allpolicies,visr}, or explicitly set a prior on downstream tasks, such as reward functions given by a random neural network \cite{frans2024unsupervised}.

Here we prove that the zero-shot RL loss can be optimized directly, for a range of non-informative priors such as white noise rewards, temporally smooth rewards, ``scattered'' sparse rewards, or a combination of those.

Thus, it is possible to learn the optimal zero-shot features algorithmically, for a wide mixture of priors.

Surprisingly, the white noise prior leads to an objective almost identical to the one in VISR \cite{visr}, via a different approach. This shows that some seemingly arbitrary choices in VISR, such as Von Mises--Fisher distributions, do maximize downstream performance. This also suggests more efficient ways to tackle the VISR objective.

Finally, we discuss some consequences and limitations of the zero-shot RL objective, such as its tendency to produce narrow optimal features if only using Gaussian dense reward priors.
\end{abstract}


\section{Introduction}

Zero-shot reinforcement learning (RL) methods aim at instantly producing
a behavior for an RL task in a given environment, from a description of
the reward function. This is done after an unsupervised training phase.
Such methods include, for instance, universal successor features (SFs,
\cite{borsa2018universal}) and the forward-backward framework (FB,
\cite{zeroshot, allpolicies}).

Zero-shot RL is usually tested by reporting average performance on
a series of downstream tasks: a reward function $r$ is sampled from a
distribution $\betatest$ of tasks, a reward representation $z=\Phi(r)$ is
computed, \footnote{A requirement of zero-shot RL is that this
computation should be scalable, with $z$ of reasonable size. Without a computational constraint, one
could just pre-compute all optimal policies of all possible downstream
tasks up to some degree of approximation.
} and a policy $\pi_z$ is applied, starting at some initial state
$s_0$. Thus, the reported performance is the expectation
\begin{equation}
\label{eq:testloss}
\E_{r\sim \betatest} \E_{s_0\sim \rho_0} V^{\pi_z}_r(s_0)
\end{equation}
where the value function $V^{\pi_z}_r(s_0)$ is the performance of policy
$\pi_z$ on the reward function $r$ when starting at $s_0$
(Section~\ref{sec:notation}).

Yet zero-shot RL methods are usually not trained by maximizing the
performance \eqref{eq:testloss}, because the distribution of downstream
tasks $\betatest$ is unknown. Other training criteria have to be
introduced, such as a finite-rank representation of long-term transition
probabilities in FB \cite{allpolicies}, or an information criterion on
policies $\pi_z$ in VISR \cite{visr}.

Alternatively, it is possible to explicitly set a prior
$\beta$ on downstream tasks, and optimize the criterion \eqref{eq:testloss} using that
prior instead of the true task distribution $\betatest$. 
This follows the machine learning
philosophy of ``follow the gradient of what you are actually doing'',
rather than made-up criteria.

For instance,
\cite{frans2024unsupervised} use random neural networks as a prior for
the downstream reward function $r$. This prior is parametric (it is
parameterized by the weights of a network), and it is unclear how
sensitive performance is to this choice.

Here:
\begin{itemize}
\item
We show that the zero-shot RL performance can be maximized directly
for a wide mixture of nonparametric, uninformative priors. This includes
dense reward priors such as white noise rewards, temporally smooth
rewards with a ``Dirichlet norm'' prior related to Laplacian
eigenfunctions, and sparse priors such as mixtures of a number of target
states with random weights.

Arguably, a mixture of such uninformative priors has the best chance of
covering the unknown test distribution $\betatest$. Note that learning
meaningful representations does not require informative priors on
downstream tasks:  environment dynamics lead to informative
representations even with non-informative priors.

This makes it possible to compute the best possible representations
for zero-shot RL by following the gradient of the criterion
\eqref{eq:testloss}. Notably, we can do this for dense reward priors
without
explicitly sampling a reward from the prior, which
would not be possible for infinite-dimensional priors such as white
noise.

\item We clarify the implicit priors on rewards in SFs: the SF strategy
implicitly relies on a \emph{white noise prior} on rewards
(Section~\ref{sec:SF}).

Doing so, we extend the SF framework to other priors, such as a prior
based on the Dirichlet norm, which introduces temporal smoothness related
to Laplacian eigenfunctions (Section~\ref{sec:priors}).

\item We show a surprising connection with VISR \cite{visr}: VISR
``almost'' computes the optimal zero-shot features for a white noise
prior (Section~\ref{sec:lossistractable}). 
(The ``almost'' comes from a minor change in the way the features are
normalized.)

This is unexpected, as VISR was not defined to maximize downstream
zero-shot performance. Instead, VISR was defined as a feedback loop
between a diversity method \cite{eysenbach2018diversity} and successor
features, by 
training a family of
policies $\pi_z$ that maximize the rewards $\transp{\phi}z$ for some
features $\phi$, and learning $\phi$ in turn by increasing $\transp{\phi}z$
at the places visited by $\pi_z$, thus creating specialization.

This newfound connection between VISR and downstream performance for a
white noise prior may justify some seemingly arbitrary choices in VISR,
such as its use of Von Mises--Fisher distributions.

The analysis also suggests more efficient ways to tackle the VISR
objective, notably, relying on occupation measures rather than Monte
Carlo sampling.

\item We derive further theoretical properties of the zero-shot RL loss.
Notably, the Bayesian viewpoint has no regularizing effect on the
policies learned: these policies are ``sharp'' in that they are
necessarily optimal policies for some particular task $r$
(Proposition~\ref{prop:posteriormean}).

This has some consequences for exploration in settings where the reward
is not exactly known: indeed, the zero-shot RL setting assumes that the
reward function is fully specified at test time (such as reaching a
particular goal or maximizing a particular quantity).

This can also produce unexpectedly narrow optimal features
(Section~\ref{sec:whatfeatures}) for some particular priors.

\item We discuss some limitations of the zero-shot RL setting, and
possible extensions.

\end{itemize}

% If we have a prior $\beta$ on rewards, we can formalize this by asking:
% Find the best reward representation $r\mapsto z=\Phi(r)$ among easy-to-compute
% ones,
% and the best policies $\pi_z$, so that return is maximal on average
% for rewards $r\sim \beta$. The loss on the representation $B$ and
% policies $\pi_z$ is thus
% \begin{equation*}
% \ell(\Phi,\pi)\deq - \E_{r\sim \beta}\, \E_{s_0\sim \rho_0}\,
% V^{\pi_{\Phi(r)}}_r(s_0)
% \end{equation*}
% where $\rho_0$ is some distribution of initial states.
% (though we should also care about variance...)
% 
% It turns out this loss is fully tractable for several classes of natural
% priors on $r$. This holds for linear representations $\Phi$ as used for
% instance in SFs and FB, but also for nonlinear representations, because
% the latter can be approximated by iterated hierarchical linear
% representations. This much extends the theoretical expressivity of
% SF-like methods.

%\TODO{cite entropy regul case}

\section{Setup, Notation, and Some Reward Priors}
\subsection{General Notation}
\label{sec:notation}

\paragraph{Markov decision process.}
We consider a reward-free Markov decision process (MDP) $\mathcal{M}=(S,A,P,\gamma)$
with state space $S$, action space $A$, transition probabilities
$P(s'|s,a)$ from state $s$ to $s'$ given action $a$,
and discount factor $0 < \gamma < 1$ \cite{sutton2018reinforcement}.
A policy $\pi$ is a function $\pi\from S\to \mathrm{Prob}(A)$ mapping a
state $s$ to the probabilities of actions in $A$.
Given
$(s_0,a_0)\in S\times A$ and a policy $\pi$,
we denote $\Pr(\cdot|s_0,a_0,\pi)$ and $\E[\cdot|s_0,a_0,\pi]$ the
probabilities and expectations under state-action sequences $(s_t,a_t)_{t
\geq 0}$ starting at $(s_0,a_0)$ and following policy $\pi$ in the
environment, defined by sampling $s_t\sim P( s_t|s_{t-1},a_{t-1})$ and
$a_t\sim \pi( a_t | s_t)$.
% We define $P_\pi(s', a' | s, a)
% \deq
% P( s'| s, a) \pi( a' | s')$,
% the state-action transition probabilities
% induced by $\pi$.
Given any reward
function $r\from S \to \R$, the $Q$-function of $\pi$ for $r$ is
$Q_r^\pi(s_0,a_0)\deq \sum_{t\geq 0} \gamma^t \E
[r(s_t)|s_0,a_0,\pi]$. The \emph{value function} of $\pi$ for $r$ is
$V_r^\pi(s)\deq \sum_{t\geq 0} \gamma^t \E
[r(s_t)|s_0,\pi]$.

We assume access to a dataset consisting of \emph{reward-free} observed
transitions $(s_t,a_t,s_{t+1})$ in the environment. We denote by $\rho$
the distribution of states $s_t$ in the training set.

\paragraph{Occupation measures.} We let $\rho_0$ be some distribution of initial states in the environment; if no such
distribution is available, we just take $\rho_0\deq \rho$.

\emph{Occupation measures} will pop up repeadtedly in our analysis. The
occupation measure $d_\pi$ of policy $\pi$ is a probability distribution over $S$, defined
for each $X\subset S$ as
\begin{equation}
\label{eq:dpi}
d_\pi(X)\deq (1-\gamma) \E_{s_0\sim \rho_0} \sum_{t\geq 0} \gamma^t \Pr(s_t\in
X|s_0,\pi).
\end{equation}
In particular, by construction,
\begin{equation}
\label{eq:Vsucc}
\E_{s\sim d_\pi} r(s)=(1-\gamma)
\E_{s_0\sim \rho_0} V^\pi_r(s_0).
\end{equation}

\subsection{The Zero-Shot RL Objective: Optimize Expected Downstream
Performance}

Existing zero-shot RL procedures proceed as follows: after an unsupervised,
reward-free pretraining phase, the agent is confronted with a reward $r$
(either via reward samples or via an explicit reward formula), computes a
task representation $z=\Phi(r)$ in a simple, fast way, then apply an
existing policy $\pi_z$. The map $\Phi$ from reward to task
representation, as well as the policies $\pi_z$, are learned during
pretraining.

Such methods are evaluated by running the policies $\pi_z$ on a number of
downstream tasks, and reporting the cumulated reward. Thus, if
$\betatest$ is the distribution of downstream tasks, the reported
loss is, in expectation,
\begin{equation}
%\label{eq:testloss}
\losstest(\Phi,\pi)= -\E_{r\sim \betatest} \E_{s_0\sim \rho_0}
V_r^{\pi_{\Phi(r)}}(s_0)
\end{equation}
where $\rho_0$ is the distribution of initial states used for testing.
This corresponds to
sampling a downstream task $r \sim \betatest$, computing $z=\Phi(r)$, and
running $\pi_z$ on reward $r$.

Usually the downstream task distribution $\betatest$ is unknown. Still, if we have a prior $\beta$ on rewards, a natural objective for
the pretraining phase is to minimize the loss
\begin{equation}
\label{eq:mainloss}
\ell_\beta(\Phi,\pi)\deq -\E_{r\sim \beta} \E_{s_0\sim \rho_0}
V_r^{\pi_{\Phi(r)}}(s_0)
\end{equation}
over $\Phi$ and $\pi$. The prior $\beta$ should ideally encompass the
unknown actual distribution $\betatest$ of downstream tasks.

Without computational constraints, this problem is theoretically ``easy'' to solve:
just precompute all optimal policies for all possible rewards. This
corresponds to $\Phi=\Id$, namely, a reward function $r$ is representated
by $z=r$ itself, and then $\pi_z$ should just be the optimal policy for
$r$. If the state space is continuous, $r$ and $z$ are
infinite-dimensional.

In practical methods, the task representation $z$ will be
finite-dimensional. This means some reward functions $r$ are necessarily
lumped together via $\Phi$, and determining the best way to do this
(e.g., for a fixed dimension of $z$)
becomes a nontrivial mathematical question. This is what we address in
the rest of the text.

\subsection{Some Uninformative Priors on Reward Functions}
\label{sec:priors}

We now introduce some priors on downstream tasks. Ideally, the prior
should cover the true distribution of tasks at test time.
Since this distribution is unknown, we try to consider the most
uninformative priors we could handle, in the hope this results in more
generic zero-shot performance.

We consider both dense and sparse reward priors. For dense rewards, we
include white noise, and a Gaussian process based on the Dirichlet norm,
which imposes more spatial smoothness on the rewards than white noise,
related to Laplacian eigenfunctions. For sparse rewards, we consider
random goal-reaching (reaching a target state specified at random), and
mixtures of several goals with random weights.

These are some of the most agnostic models we can find on an arbitrary
state equipped with an arbitrary probability distribution. All models are
built to have well-defined continuous-space limits, and still make sense
in an abstract state space equipped with a measure $\rho$. To avoid
excessive technicality, we restrict ourselves to the finite case in this
text.

Importantly, these priors rely on quantities that can be estimated from
the dataset (such as expectations under $\rho$). This is why we use
norms related to the dataset distribution $\rho$.

We will also use mixtures of these priors.

\subsubsection{Dense Reward Priors}

\paragraph{White noise prior.} This is defined as 
\begin{equation}
\beta(r)\propto
\exp(-\norm{r}^2_\rho/2)
\end{equation}
where $\norm{f}^2_\rho\deq \E_{s\sim \rho}
f(s)^2$.

This prior is very agnostic: the reward at every state is assumed to be
independent from every other state.

\paragraph{Dirichlet prior.} 
This is defined as 
\begin{equation}
\beta(r)\propto
\exp(-\norm{r}^2\Dir/2)
\end{equation}
where
\begin{equation}
\norm{f}\Dir^2\deq \E_{(s_t,a_t,s_{t+1})\sim \rho}\,
(f(s_t)-f(s_{t+1}))^2+\alpha \norm{f}^2_\rho
\end{equation}
where some $\alpha>0$ is used because the first term vanishes on constant $f$.

Contrary to white noise, this prior enforces some smoothness over
functions: the values at related states are closer.

The Dirichlet norm is directly related to Laplacian eigenfunctions.
Indeed, when $\rho$ is the invariant distribution of the policy in the dataset
\footnote{
More precisely, it is sufficient that the distributions of $s_t$ and $s_{t+1}$ under
the distribution $\rho$ in the dataset are the same. This does not
require the existence of a specific policy that produced the dataset. For
instance, if a dataset is a mixture of long trajectories from several
policies, then the laws of $s_t$ and of $s_{t+1}$ in the dataset will be
almost the same (up to neglecting the first and last state of each trajectory).}, one has
\begin{equation}
\norm{f}\Dir^2=2\langle f,\Delta f\rangle_\rho + \alpha \norm{f}^2_\rho
\end{equation}
where $\Delta \deq \Id - P_0$ is the Laplace operator of the
transition operator $P_0(s_{t+1}|s_t)$ of the policy implicitly defined
by the dataset.

\paragraph{General Gaussian priors.} To avoid proving the same results
separately for white noise and Dirichlet priors, we will more generally
use priors of the form
\begin{equation}
\beta(r)\propto \exp(-\norm{r}^2_K)
\end{equation}
where $\norm{f}^2_K$ denotes an arbitrary symmetric positive-definite
quadratic form on reward functions.

On a finite state space, this is equivalent
to $\exp(-\transp{r}Kr/2)$ for some p.s.d.\ matrix $K$ of size $\#S\times
\#S$. For instance, on
a finite state space, the white noise prior corresponds to
$K=\diag(\rho)$, and the Dirichlet prior is given by the matrix
$K=\E_{(s,s')\sim \rho}
[(\1_s-\1_{s'})\transp{(\1_s-\1_{s'})}]+\alpha \E_{s\sim \rho}[
\1_s\transp{\1_s}]$.

On infinite state spaces, this is an ``infinite-dimensional Gaussian''
whose formal definition involves having a consistent set of Gaussian
distributions in every finite-dimensional projection.% \TODO{REF}.

We will also use the associated dot
product $\langle{f,g}\rangle_K$. For instance,
\begin{equation}
\langle{f,g}\rangle\Dir=\E_{(s_t,a_t,s_{t+1})\sim
\rho}\,(f(s_t)-f(s_{t+1}))(g(s_t)-g(s_{t+1}))+\alpha f(s_t)g(s_t).
\end{equation}
Like $\norm{f}\Dir$, this can be estimated from the dataset.

\begin{rem}
In general, the optimal features are \emph{not} directly related to the
largest eigenvectors or singular vectors of $K$. For instance, the white
noise prior corresponds to $K=\diag(\rho)$, whose eigendecomposition is
independent of the dynamics of the environment, while optimal features
depend on the dynamics.
\end{rem}

\subsubsection{Sparse Reward Priors}
\label{sec:sparsepriors}

\paragraph{Random goal-reaching prior.} A \emph{goal-reaching} reward is
a reward that is nonzero only at a particular state, and $0$ everywhere
else.

If the prior $\beta$ on downstream tasks only includes goal-reaching
tasks (with some distribution of goals $g$), then arguably zero-shot RL
is not needed: it is better to just do goal-reaching, namely, directly
use $z=g$ as the task representation for goal $g$, and learn $Q(s,g)$ via
algorithms such as HER \cite{andrychowicz2017hindsight}.

But we want to mix goal-reaching with other priors, and find zero-shot RL
methods that can work in a mixture of different priors, hence the
interest of a general setup. So we formally define here a goal-reaching
prior.

In this model, we first
select a random state $s^\star\sim \rho$ in $S$. Then we put
a reward $1/\rho(s^\star)$ at $s^\star$, and $0$
everywhere else:
\begin{equation}
\label{eq:goalreaching}
r(s)=\frac{1}{\rho(s^\star)} \,\1_{s=s^\star}.
\end{equation}

The $1/\rho$ factor maintains $\int r\d \rho=1$. Without this scaling,
all $Q$-functions degenerate to $0$ in continuous spaces,
as discussed in
\cite{blier2021unbiased}. Indeed, if we omit this factor, and just set the reward to be $1$ at
a given goal state $s^\star\in S$ in a continuous space $S$, the probability of
exactly reaching that state with a stochastic policy is usually $0$, and all
$Q$-functions are $0$. Thanks to the $1/\rho$ factor, the
continuous limit is a \emph{Dirac function} reward, infinitely sparse,
corresponding to the limit of putting a reward $1$ in a small ball
$B(s^\star,\eps)$ of radius $\eps\to 0$ around $s^\star$, and rescaling by
$1/\rho(B(s^\star,\eps))$ to keep $\int r\d \rho=1$. This produces meaningful,
nonzero $Q$-functions in the continuous limit \cite{blier2021unbiased}.

This model combines well with successor features or the FB framework:
indeed, this model satisfies
\begin{equation}
\E_{s\sim \rho} [r(s)\phi(s)]=\phi(s^\star)
\end{equation}
(both in finite spaces and in the continuous-space limit). This is useful
in conjunction with the SF formulas such as \eqref{eq:SF} in Section~\ref{sec:SF}.

\paragraph{Scattered random reward prior.} We extend the random
goal-reaching prior to rewards comprising several goals with various
weights, where the weights may be random and may be positive or negative.

Generally speaking, we will call \emph{scattered random reward prior} any
prior which consists in first choosing an integer $k\geq 0$ according to
some probability distribution, then choosing $k$ goal states
$(s^\star_i)_{1\leq i \leq k}\sim \rho$ and $k$ random weights
$w_i,\ldots,w_k$ according to some fixed probability distribution on
$\R$, and setting
\begin{equation}
r(s)=c_k \sum_{i=1}^k 
\frac{w_i}{\rho(s^\star_i)} \,\1_{s=s^\star_i}
\end{equation}
namely, a sum of $k$ goal-reaching rewards \eqref{eq:goalreaching}.

A suitable scaling factor $c_k$ can sometimes produce more
meaningful behavior for large $k$. For instance, if we take $w_i\sim N(0,1)$ and
$c_k=1/\sqrt{k}$, and let $k\to \infty$, then this prior tends to the white noise
prior above.

Therefore, scattered random reward priors can be seen as interpolating
between the pure goal-reaching and white noise priors.

\section{Algorithmic Tractability of the Zero-Shot RL Loss}

\subsection{The Optimal Policies Given a Representation $\Phi$}

Here we work out half of the objective \eqref{eq:mainloss}: what are the
optimal policies
$\pi_z$ if the task representation $\Phi$ is
known?

\begin{prop}[ (Policies must be optimal for the mean posterior reward
knowing $z$)]
\label{prop:posteriormean}
For each $z$, define
\begin{equation}
\label{eq:postreward}
r_z\deq \E_{r|\Phi(r)=z} [r]
\end{equation}
the mean reward function knowing $\Phi(r)=z$ under the prior $\beta$. Let
also $\beta_z$ be the distribution of $z=\Phi(r)$ when sampling $r\sim
\beta$.

Then
\begin{equation}
\ell_\beta(\Phi,\pi)=-\E_{z\sim \beta_z}\,\E_{s_0\sim \rho_0} V^{\pi_z}_{r_z}(s_0).
\end{equation}

Consequently, given the representation $\Phi$, for every $z$, the best policy $\pi_z$ is the optimal
policy $\pi^\star_{r_z}$ for reward $r_z$.
\end{prop}

So, in this model, the optimal zero-shot policies have no induced stochasticity to account for
uncertainties. This holds even if there is noise in the computation of
$z$. The full point of zero-shot RL is to decide which rewards to lump together under the
same policy.

This does not hold if one includes variance over $r$ in the main loss
\eqref{eq:mainloss} (Section~\ref{sec:varianceregul}).

% It follows that the optimum above is attained for
% \begin{equation}
% \pi_z=\pi^\star_{r_z}
% \end{equation}
% the optimal policy for reward $r_z$, and the representation problem above
% is
% \begin{equation}
% \sup_B \E_{z\sim \beta_z} \,\E_{s_0\sim \rho}\, V^\star_{r_z}(s_0)
% \end{equation}

% \section{Generic Priors on Rewards: White Noise and the Gaussian Free Field}
% 
% We would like the prior on rewards to cover as many tasks as possible,
% and to yield tractable learning. TODO will never have to generate reward
% funcs.
% 
% Here I will consider two natural priors on rewards: white noise, and the
% \emph{Gaussian free field} (GFF), a canonical choice of Gaussian field
% which induces spatial correlations. Both allow for tractable updates.
% Arguably, white noise is the ``widest'' possible prior for random
% functions, while the GFF is still very general but induces more
% regularity.
% 
% TODO see Section TODO for sparse reward priors
% 
% TODO priors can be combined because the loss is a mixture
% 
% \bigskip
% 
% \emph{White noise} is defined by a Gaussian distribution on $r$,
% \begin{equation}
% \beta(r) \propto \exp \left(-\frac{\norm{r}_\rho^2}{2}\right)
% \end{equation}
% where $\rho$ is some reference measure that we will have to sample states
% from, so, the distribution of states in the training set.
% 
% TODO EXPLAIN MORE, eg, finite state
% On a discrete space, $r_s$ is a centered Gaussian of
% variance $1/\rho(s)$. This $1/\rho$ scaling also means
% there is more uncertainty on less visited states.
% This model has a meaningful limit on
% continuous spaces, white noise, where $r$ is a random distribution rater
% than a function: the value of $r$ at a given point is undefined (with
% infinite variance), but the \emph{integrals} of $r$ on any given set are
% well-defined, and the integrals on two disjoint sets are independent.
% 
% \bigskip
% 
% The \emph{Gaussian free field} (GFF) induces some spatial smoothness in a
% canonical way.
% Assume we have access to a trainset of
% transitions $(s_t,s_{t+1})$ (or actually, any set of pairs of ``related''
% states). The GFF is a Gaussian with a more complex variance:
% \begin{align}
% \label{eq:lapprior}
% \beta(r) &\propto \exp \left(-\tfrac12 \E_{(s_t,s_{t+1})\sim \rho}
% (r(s_t)-r(s_{t+1}))^2 -\tfrac\alpha2 \E_{s\sim \rho} \,r(s)^2\right)
% \\&=
% \exp(-\transp{r}Dr/2)
% \end{align}
% with $D$ the matrix
% \begin{equation}
% D=\E_{(s,s')\sim \rho}
% [(\1_s-\1_{s'})\transp{(\1_s-\1_{s'})}]+\alpha \E_{s\sim \rho}[
% \1_s\transp{\1_s}].
% \end{equation}
% This prior induces spatial continuity, since rewards have a higher
% prior probability if $r(s_t)$ and $r(s_{t+1})$ are close. It is necessary
% to include $\alpha>0$, because the first term vanishes for constant
% rewards. This also has an RKHS interpretation. I consider this prior to be more natural in general than white
% noise.
% 
% In general, the GFF above will be a true function (if the exploration
% policy has noise).
% 
% \bigskip
% 
% On continuous spaces, the \emph{continuous GFF} can be defined as
% \begin{equation}
% \beta(r) \propto \exp\left(-
% \E_{s\sim \rho} \norm{\nabla r(s)}^2/2 -\alpha \norm{r}^2_\rho/2
% \right).
% \end{equation}
% It is the limit of the discrete GFF on a grid.
% It depends less on the notion of neighborhood implied by transitions
% $(s_t,s_{t+1})$ in the exploration policy. In general, this is a
% distribution not a true function, except in dimension $1$. This is
% another canonical prior with important applications in probability,
% statistical physics and quantum physics.
% 
% \bigskip
% 
% There are many other, less canonical choices. Given any
% positive definite kernel $k(s,s')$ on states, we can use a Gaussian prior
% \begin{equation}
% \exp \left(- \tfrac12 \E_{s \sim \rho,s'\sim \rho}
% \,k(s,s')r(s)r(s')\right).
% \end{equation}
% This may suffer from high
% variance if most independent pairs $(s,s')\sim (\rho,\rho)$ have low
% kernel values. For instance, a Gaussian kernel
% $k(s,s')=\exp(-\norm{s-s'}^2/2\sigma^2)$ may be better replaced with a
% GFF on pairs $(s,s')$ with $s\sim \rho,\,s'\sim N(s,\sigma^2)$.
% 
% \bigskip
% 
% The important thing in all these priors is that the defining norms can be
% computed from expectations over samples $s\sim \rho$ from the trainset. The algorithms below will
% heavily use this, only relying on square norms $\norm{f}^2_D$ and dot
% products $\langle f,g\rangle_D$ for the quadratic forms
% $\norm{\cdot}^2_D$ in the Gaussians
% above.
% 
% TODO explain Dirac rewards
% 
% % The second prior is random goal-reaching, with rewards a Dirac measure at
% % a random goal $s\sim \rho$. (As a function, $r=\delta_{s=g}/\rho(s)$
% % represented as a density wrt $\rho$.) Meaningful continuous limit, etc.
% 
% The white noise and discrete GFF priors are independent of a state
% encoding. The continuous GFF prior is not: for complex states such as
% images, it might be better to first encode
% $s$ as pre-trained image features $s'$ and take the continuous GFF prior on $s'$.
% \footnote{Namely, it does not matter whether we
% define the WN and GFF prior on $s$ directly or on another encoding of $s$
% (such as pre-learned image features): these will describe the same
% rewards. This is because these models are defined over ``abstract''
% states $s$ without actually performing operations on $s$. On the other
% hand, the continuous GFF prior does depend on the encoding because we take
% gradients with respect to $s$.}
% 
% TODO: priors on $r$ depending on restricted variables. Is it obvious we
% can do the same on $\phi$? Also, Optimal solutions seem to be something
% like finding a subspace that keeping as much variance as possible from
% the prior in a metric like $\transp M M$. which is a bit circular since
% $M$ depends on the policies...

The value of $r_z$ can be derived explicitly for some priors (Gaussian
priors and linear $\Phi$), which we now turn to. Other priors
(goal-oriented or scattered
random rewards) require a slightly different approach
(Section~\ref{sec:phisparse}).

\subsection{Linear Task Representations $\Phi$}
\label{sec:SF}

We now emphasize the case of \emph{linear} task representations
$\Phi$, because it corresponds to successor features and to the
forward-backward framework, which are the most successful zero-shot RL
approaches to date. (See Section~\ref{sec:future} for nonlinear $\Phi$.)

The easiest-to-compute
task representations $z=\Phi(r)$ are linear functions of $r$. Any such
finite-dimensional function $\Phi$ is given by integrating
the reward against some features $\phi=(\phi_{i}(s))_{i=1,\ldots,k}$:
% \begin{equation}
% z=\E_{s\sim \rho} \,r(s)\Phi(s)
% \end{equation}
% as we do in FB. We will focus on the SF case with $B=(\Cov
% \phi)^{-1}\phi$, namely,
\begin{equation}
z=(\Cov \phi)^{-1} \E_{s\sim \rho} \,r(s)\phi(s)
\end{equation}
where we include a preconditioning by $(\Cov \phi)^{-1}$ as in SFs. \footnote{
Including $(\Cov \phi)^{-1}$ from the start (as opposed to
$z=\E_{s\sim \rho} \,r(s)\phi(s)$ as in FB) is more adapted to
distribution shifts. Indeed, for rewards in the span of
$\phi$, then the reward representation 
$z$ is independent of the distribution $\rho$ of states used for the
computation.
} 
Here all covariances are expressed with respect to the state
distribution $\rho$: $\Cov \phi\deq \E_{s\sim \rho} \,\phi(s)\transp{\phi(s)}$.

By Proposition~\ref{prop:posteriormean}, given the features $\phi$, the
best policies are the optimal policies for the rewards $r_z$. So we 
have to compute $r_z$. Then the policies can be learned, e.g.,\ via
$Q$-learning for each $z$.

The following result specifies the value of $r_z$ and hence the optimal
policies given the features, but does not yet say how to choose the
features $\phi$: this is covered in the next sections.

\begin{prop}[ (Linear task representations and white noise prior)]
\label{prop:linearpostmean}
Assume the reward representation $z=\Phi(r)$ is given by the successor
feature model
\begin{equation}
\label{eq:SF}
z=(\Cov \phi)^{-1} \E_{s\sim \rho} \,r(s)\phi(s)
\end{equation}
using some linearly independent features $\phi\from S \to \R^d$.

Then, for the white noise prior on rewards, the posterior mean 
reward $r_z$ \eqref{eq:postreward} is
% \begin{equation}
% r_z(s)=\transp{z}(\Cov B)^{-1}\Phi(s).
% \end{equation}
\begin{equation}
\label{eq:rz}
r_z(s)=\transp{z}\phi(s).
\end{equation}
Therefore, by Proposition~\ref{prop:posteriormean}, for a given $\phi$, the policies $\pi_z$ that optimize the
zero-shot RL loss \eqref{eq:mainloss} are the
optimal policies for reward $\transp{z}\phi$, for each $z$.

Moreover, under these assumptions, the distribution $\beta_z$ of $z$ is a centered Gaussian with
covariance matrix $(\Cov \phi)^{-1}$.
\end{prop}

This proposition gives a justification for part of the strategy behind
successor features, namely, projecting the reward onto the features and
applying the optimal policy for the projected reward. This is optimal on
average under
an implicit \emph{white noise prior} on rewards. 

% \TODO{also justifies the use of learning $Q$-functions for
% $\transp{\phi}z$ instead of full vector loss in SFs.  the best thing to
% do is to compute, for each $z$, the $Q$-function $Q_z$ for reward
% $\transp{z}\phi$. For FB, this strongly suggests to retrain a
% $Q$-function model on top the features $B$, rather than directly use $F$
% to estimate $Q$-functions.}

In the forward-backward framework (FB), the task representation $z$ is
computed as $z=\E_{s\sim \rho} r(s)B(s)$ with features $B$. This is the
same as \eqref{eq:SF} up to the change of variables by $(\Cov
\phi)^{-1}$. Therefore, this result strongly suggests to train policies
$\pi_z$ for the rewards $\transp{z}(\Cov B)^{-1}B$. This contrasts with
the FB framework, in which the policies $\pi_z$ are defined through the
forward function $F$. The two coincide only if the training of $F$ is
perfect. \footnote{because in that case, $F$ contains the successor
features of $(\Cov B)^{-1}B$, by one of the results in
\cite{allpolicies}.}

In general, the proposition is \emph{not} true for other reward priors, such as
random goal-reaching. \footnote{For instance, take any set of features
such that $\phi\from S\to \R^d$ is injective, such as $\phi=\Id$. Take
for $\beta$ the goal-reaching prior. Then for reaching a goal $g$, the
reward $r$ is a Dirac at $g$ so that
$\E [r\phi]=\phi(g)$ and
$z=C^{-1}g$ with $C$ the covariance matrix. Since the map $g\to z$ is
bijective, it conveys full knowledge of the task for this prior, and the
posterior mean $r_z$ is just the single reward for reaching $g$.}
Still, \eqref{eq:rz} also
holds for any Gaussian prior on rewards such that the
components of $r$ along $\phi$ and its $L^2(\rho)$-orthogonal are independent, namely,
$r(s)=\transp{\theta_1}\phi(s)+\transp{\theta_2}\xi(s)$ where $\xi$ are
any features such that $\E_{s\sim \rho}[\phi(s)\transp{\xi(s)}]=0$ and
$\theta_1$, $\theta_2$ are independent Gaussian vectors with any
covariance matrix. But this cannot be used to optimize the features
$\phi$, because this condition depends on $\phi$ itself so it does not
represent a fixed prior for the loss \eqref{eq:mainloss}.

\bigskip

This result extends to the more general case of arbitrary Gaussian priors
given by a metric $\norm{\cdot}_K$: we just have to compute $z$ by a formula
involving this norm, instead of the SF formula
\eqref{eq:SF}. 

This is especially relevant if $K$
can be computed from expectations over the dataset, as with the Dirichlet
prior: this results in an SF-like approach, but relying on a different
implicit prior instead of the white noise prior on rewards.

\begin{prop}[ (Linear representations with arbitrary Gaussian prior)]
\label{prop:gaussianprior}
Assume that the prior on rewards is
\begin{equation}
\beta(r)\propto \exp(-\tfrac12 \norm{r}^2_K)
\end{equation}
for some Euclidean norm $\norm{\cdot}_K$ on the space of rewards.

Assume the reward representation $z=\Phi(r)$ is computed as
\begin{equation}
\label{eq:genSF}
z=C^{-1}  \langle r,\phi\rangle_K
\end{equation}
using some linearly independent features $\phi\from S \to \R^d$,
where $C$ is the $k\times k$ matrix with entries $C_{ij}=\langle
\phi_i,\phi_j\rangle_K$. Namely, $z$ contains the weights of
the $L^2(\norm{\cdot}_K)$-orthogonal projection of $r$ onto the features
$\phi$.

Then the posterior mean 
reward $r_z$ given $z$ is
\begin{equation}
\label{eq:rzD}
r_z(s)=\transp{z}\phi(s)
\end{equation}
Therefore, by Proposition~\ref{prop:posteriormean}, for a given $\phi$,
the policies $\pi_z$ that optimize the zero-shot RL loss
\eqref{eq:mainloss} are the optimal policies for reward $\transp{z}\phi$,
for each $z$.

Moreover, the distribution $\beta_z$ of $z$ is Gaussian with
covariance matrix $(\langle \phi,\phi\rangle_K)^{-1}$.
\end{prop}

For instance, with the Dirichlet prior, we have
\begin{equation}
\langle\phi ,r\rangle\Dir = \E_{(s_t,s_{t+1})\sim \rho}\,
(r(s_t)-r(s_{t+1}))(\phi(s_t)-\phi(s_{t+1}))+ \alpha \E_{s\sim\rho}\,
r(s)\phi(s)
\end{equation}
and
\begin{equation}
\langle \phi , \phi\rangle\Dir= \E_{(s_t,s_{t+1})\sim
\rho}\,(\phi(s_t)-\phi(s_{t+1}))\transp{(\phi(s_t)-\phi(s_{t+1}))}
+\alpha\E_{s\sim \rho}\, \phi(s)\transp{\phi(s)}
\end{equation}
and so $z$ can be estimated from
samples. This gives rise to a Dirichlet-prior-based version of successor
features. \footnote{Depending on how the reward is specified for zero-shot
RL, in some situations, we might not have access to both
$r(s_t)$ and $r(s_{t+1})$. And for goal-oriented tasks, we usually don't have access
to $\phi(s_{t+1})$, the state visited one step after reaching the goal.

This contrasts with basic successor features, for which setting a goal
state
$s^\star$ just gives $z\propto (\Cov \phi)^{-1} \phi(s^\star)$.
}



\begin{rem}
It
is also possible to train a Gaussian $\exp(-\norm{r}^2_K)$ prior while
using features $z=(\E \phi\transp\phi)^{-1} \E r\phi$ that do not use the $K$-norm.
But in that case the expression for the posterior mean $r_z$
is much more complex
and requires inverting $K$. %Still, this may be useful for mixing several priors while using the same representation $z$.
\end{rem}

\subsection{The Zero-Shot Loss is Tractable for Linear Representations}
\label{sec:lossistractable}

These results for fixed $\phi$ pave the way to
computing the gradient of the zero-shot loss
\eqref{eq:mainloss} with respect to $\phi$: putting together all the
ingredients yields the following result.


\begin{thm}[ (Zero-shot RL loss for linear task representations)]
\label{thm:main}
Assume that the prior $\beta$ on reward functions $r$ is $\beta(r)\propto
\exp(-\tfrac12 \norm{r}^2_K)$ for some Euclidean norm $\norm{\cdot}_K$.
Assume that the reward representation $z=\Phi(r)$ is computed as in successor
features \eqref{eq:genSF} using the norm $\norm{\cdot}_K$, namely,
\begin{equation}
z=C^{-1} \langle r,\phi\rangle_K
\end{equation}
where $\phi\from S\to \R^d$ are linearly independent features, and where
$C$ is the matrix with entries $C_{ij}= \langle \phi_i,\phi_j\rangle_K$.

Then the zero-shot RL loss \eqref{eq:mainloss} is
\begin{equation}
\label{eq:tractableloss}
\ell_\beta(\Phi,\pi)=-\frac1{1-\gamma} \E_{z\sim N(0,C^{-1})} \,\E_{s\sim
d_{\pi_z}}\transp{\phi(s)}z
\end{equation}
where $d_{\pi_z}$ is the occupation measure \eqref{eq:dpi} of policy
$\pi_z$.

Moreover, the optimal $\pi_z$ given $\Phi$ is the optimal policy for
reward $r_z(s)\deq \transp{\phi(s)}z$.
\end{thm}

% Here $\pi'_z$ is a relabelling of $\pi_z$, $\pi'_z\deq \pi_{(\phi D
% \transp{\phi})^{-1/2} z}$.



\paragraph{Relationship with VISR \cite{visr}: VISR almost optimizes
expected downstream performance under a white noise prior.} Surprisingly, the loss
\eqref{eq:tractableloss} is very close to the loss optimized in VISR,
although VISR was built in a different way with no formal connection to
expected downstream task performance.

VISR is a criterion to build features $\phi$ for successor features. It
works with a set of features $\phi$ and policies $\pi_z$. Each policy
$\pi_z$ is the optimal policy for reward function $\transp{\phi}z$. The
features $\phi$ are chosen to maximize the mutual information between $z$
and the states $s$ visited by $\pi_z$; more exactly, the states $s$ are
assumed to be observed only through $\phi(s)$, and the distribution of
$z$ knowing $\phi(s)$ is assumed to follow a Von Mises--Fisher
distribution $\exp(\transp{\phi}z)$ (this is chosen for convenience so
that the log-likelihood $\transp{\phi}z$ matches with the reward). The
features $\phi$ attempt to maximize the mutual information under this
model of $z$ given $s$; this mutual information is estimated via a
variational lower bound. We refer to the VISR paper \cite{visr} for
further details.

Yet it turns out Algorithm 1 in \cite{visr} optimizes the loss
\eqref{eq:tractableloss} above, except for a difference in the way
$z$ and $\phi$ are normalized. More precisely, the VISR algorithm
consists in:
\begin{enumerate}
\item Sampling a hidden vector $z$ (denoted $w$ in \cite{visr}).
\item Training the policy $\pi_z$ to optimize the reward
$\transp{\phi}z$. This is done in VISR via the computation of the successor
features $\psi$ of $\phi$.
\item Running the policy $\pi_z$ to get a sequence of states $s_t$, whose
distribution is thus $d_{\pi_z}$.
\item Updating the features $\phi$ to minimize $-\transp{\phi(s_t)}z$.
\end{enumerate}
%This is one of the possible methods to optimize the loss
%\eqref{eq:tractableloss}. (We discuss other options below.)

VISR ``almost'' optimizes the loss \eqref{eq:tractableloss}:
the only difference between VISR and Theorem~\ref{thm:main} lies in the
normalization of $z$ and $\phi$. In Theorem~\ref{thm:main}, we sample $z$ from $N(0,C^{-1})$
where $C$ is the covariance matrix of $\phi$, and we have no constraint
on $\phi$. In VISR, $z$ is sampled from $N(0,\Id)$ then normalized to
unit length, and the features $\phi$ use a normalized output layer so
that $\norm{\phi(s)}=1$ for any state $s$.

Normalization is necessary in VISR: otherwise, the loss of $\phi$ can be
brought to $0$ by downscaling $\phi$. On the other hand, in
Theorem~\ref{thm:main}, if we downscale $\phi$, the distribution $z\sim
N(0,C^{-1})$ gets \emph{up}scaled by the same factor so $\transp{\phi}z$
is unchanged. This emphasizes the role of sampling $z$ with covariance
matrix
$C^{-1}$. Also note that the normalization $\norm{\phi(s)}=1$ in VISR
does \emph{not} imply that the covariance matrix of $\phi$ is $C=\Id$. So there is
a slight mismatch between the VISR objective and the zero-shot RL loss. 

Still, Theorem~\ref{thm:main} proves that \emph{VISR ``almost'' optimizes the
expected downstream performance of $\pi_z$ under a white noise prior on
reward functions}, where the ``almost'' accounts for the difference in
normalization and covariance of $z$. This is surprising, as expected downstream
performance was not explicitly used to derive VISR.

\subsection{Algorithms for Optimizing the Representation $\phi$}
\label{sec:phiopt}

A generic VISR-like algorithm to optimize the zero-shot RL loss
\eqref{eq:tractableloss} in Theorem~\ref{thm:main} may have the following
structure:
\begin{enumerate}
\item Sample a minibatch of $z$ values.
\item Do a policy optimization step to bring $\pi_z$ closer to the
optimal policy for reward $\transp{\phi(s)}z$.
\item Estimate the occupation measures $d_{\pi_z}$ of $\pi_z$.
\item Do a gradient step on $\phi$ using the loss
\eqref{eq:tractableloss}.
\item Iterate.
\end{enumerate}

We present one possible such algorithm in Algorithm~\ref{algo:main}. It
departs from VISR in three ways:
\begin{itemize}
\item Fixing normalization and influence of $C$: sampling  $z$ from
$N(0,C^{-1})$. An extra complication occurs: since $C$ depends on $\phi$,
it is necessary
to estimate the gradients coming from $C^{-1}=(\langle
\phi,\phi\rangle_K)^{-1}$ when taking gradients with respect to $\phi$.

This ensures we exactly optimize te zero-shot RL loss
\eqref{eq:tractableloss}.

\item Estimating a model of the occupation measures $d_{\pi_z}$.  VISR
obtains sample states $s\sim d_{\pi_z}$ by running trajectories of
$\pi_z$ and using a Monte Carlo estimate by averaging over these
trajectories. This both suffers from high variance and limits
applicability to the online RL setup, since interactions with the
environment are needed during training.

Instead, learning a model of $d_{\pi_z}$ allows Algorithm~\ref{algo:main}
to run in an offline RL setting. It should also result in larger bias but
smaller variance with respect to Monte Carlo sampling from $d_{\pi_z}$.

\item Simplifying the learning of $\pi_z$: this can be done using any
$Q$-learning algorithm with $z$-dependent $Q$-function $Q(s,a,z)$ for 
reward $\transp{\phi(s)}z$. It does not have to use the successor
features of $\phi$ as in VISR.
\end{itemize}

\newcommand{\algindent}{\STATE\hspace{\algorithmicindent}}

  \begin{algorithm}[tb]
    \small
    \caption{One possible algorithm to optimize the zero-shot RL loss
    \eqref{eq:tractableloss}}
    \label{algo:main}
 \begin{algorithmic}
    \STATE \textbf{Input:}
    \\Dataset of transitions $(s_t,a_t,s_{t+1})$
    with distribution $\rho$.
    \\Norm $\norm{\cdot}_K$ on features (default:
    $\norm{\phi}_K^2\deq \E_{s\sim \rho} \abs{\phi(s)}^2$), and
    associated dot product.
    \\Weights $\lambda_C\in \{0,1\}, \lambda\orth\geq0$ for auxiliary
    losses.
    \\Online EMA weights $\beta_t\in (0,1)$ to estimate $C$.
    \STATE \textbf{Output:}\\
    Trained features $\phi_1,\ldots,\phi_d$ with their covariance
    matrix $C$.
    \\Trained policies $\pi_z$.
    \WHILE{not done}
    \STATE Update covariance matrix $C$ via EMA:
    %\algindent
    $C_{ij}\gets \beta_t
    C_{ij}+(1-\beta_t)\langle \phi_i,\phi_j\rangle_K$
    \STATE Sample a minibatch of values of $z$: $z\sim N(0,C^{-1})$
    \STATE Update a $Q$-function $Q(s,a,z)$ and policy $\pi_z(a|s)$ for
    reward $\transp{\phi(s)}z$, using any RL algorithm
    \STATE Update the occupation measure model $d(s,z)$ via one step of
    Algorithm~\ref{algo:occupation}
    \STATE Sample a minibatch of states $s$ from the dataset, and
    update $\phi$ with the loss
    \algindent $\loss(\phi)=-d(s,z)\transp{\phi(s)}z+\lambda_C\,
    \loss_C(\phi,s,z)+\lambda\orth\,\loss\orth(\phi)$
    \\where $\loss_C$ and $\loss\orth$ are the auxiliary losses
    \eqref{eq:lossC} and \eqref{eq:lossorth} respectively
    \ENDWHILE
    \STATE \textbf{Deployment:}
    \\Once the reward function $r$ is known:
    \STATE Estimate $\langle r,\phi_1\rangle_K$, \ldots $\langle
    r,\phi_d\rangle_K$
    \STATE Set $z=C^{-1}\langle r,\phi\rangle_K$
    \STATE Apply policy $\pi_z$
 \end{algorithmic}
 \end{algorithm}


Let us further discuss two of these points (gradients coming from $C$, and
estimating $d_{\pi_z}$). The exact derivations are included in
Appendix~\ref{sec:proofs}.

\paragraph{Learning the occupation measures $d_{\pi_z}$.}
Instead of explicitly running the policy $\pi_z$ as in VISR, a number of techniques allow for direct estimation of the
density of $d_{\pi_z}$.

Indeed, $d_{\pi_z}(s)$ is the average over $s_0\sim \rho_0$ of the
\emph{successor measures} $M^{\pi_z}(s_0,a_0,s)$, multiplied by
$(1-\gamma)$. We refer to \cite{successorstates} or to
Appendix~\ref{sec:proofs}
for successor measures: intuitively, $M^{\pi_z}(s_0,a_0,s)$ encodes the
expected amount of time spent at $s$ if starting at $(s_0,a_0)$ and
running $\pi_z$.

Algorithm~\ref{algo:occupation} first learns a model $m(s_0,a_0,s,z)$ of
the successor measure, using one of the methods from
\cite{successorstates} (the measure-valued Bellman equation satisfied by
successor measures). Then it averages the result over $s_0$ and $a_0$ to
obtained the model $d(s,z)$ of the occupation measure $d_{\pi_z}$. The
mathematical derivations are given in Appendix~\ref{sec:proofs}. The
model $m(s_0,a_0,s,z)$ may take any form; a particular case is a
finite-rank approximation $m(s_0,a_0,s,z)=\transp{F(s_0,a_0,z)}B(s,z)$
similar to the forward-backward representation from \cite{allpolicies},
except that here $B$ can be $z$-dependent.
\footnote{
A model $m(s_0,a_0,s,z)=\transp{F(s_0,a_0,z)}B(s)$, with $B$ independent
of $z$, would be too
restrictive here: in this model, everything is projected onto the span of
$B$, and the optimal $\phi$ is just $B$.
}

  \begin{algorithm}[tb]
    \small
    \caption{One possible algorithm to estimate occupation measures
    $d(s,z)$}
    \label{algo:occupation}
 \begin{algorithmic}
    \STATE \textbf{Input:} Dataset of transitions $(s_t,a_t,s_{t+1})$
    with distribution $\rho$.
    \STATE Distribution of initial states $\rho_0$
    (default: $\rho_0=\rho$).
    \STATE Policies $\pi_z(a|s)$.
    \STATE Covariance matrix $C$ for sampling $z$.
    \STATE \textbf{Output:} Trained occupation model $d(s,z)$.
    \WHILE{not done}
    \STATE Sample a minibatch of values of $z$: $z\sim N(0,C^{-1})$
    \STATE Sample a minibatch of transitions $(s_t,a_t,s_{t+1})\sim \rho$
    \STATE Sample actions $a_{t+1}\sim \pi_z(a_{t+1}|s_{t+1})$
    \STATE Sample a minibatch of states $s'\sim \rho$
    \STATE Update the successor measure model $m(s_t,a_t,s',z)$ with the loss
    \algindent $\loss(m)=
\left(m(s_t,a_t,s',z)-\gamma
\bar m(s_{t+1},a_{t+1},s',z)\right)^2-2m(s_t,a_t,s_t,z)$ with $\bar m$ a
target network version of $m$ (using EMA of parameters of $m$ and a
stop-grad)
    \STATE Sample a minibatch of initial states $s_0\sim \rho_0$ and
    actions $a_0\sim \pi_z(a_0|s_0)$
    \STATE Update the occupation measure model $d(s,z)$ with the loss
    \algindent $\loss(d)=\left(d(s',z)-(1-\gamma)m(s_0,a_0,s',z)\right)^2$
    \ENDWHILE
 \end{algorithmic}
 \end{algorithm}

\paragraph{Dealing with the covariance matrix $C$.} In the loss
\eqref{eq:tractableloss}, the variable $z$ is sampled from $z\sim
N(0,C^{-1})$. Since $C$ depends on $\phi$, this produces extra terms when
attempting to optimize the loss over $\phi$.

Here a reparameterization trick $z\gets C^{1/2}z$ is inconvenient,
because it still requires computing the gradient of $C^{-1/2}$ with
respect to $C$, and this requires inverting a $d^2\times d^2$ matrix, not
just a $d\times d$ matrix.

Instead, two other strategies are possible:
\begin{enumerate}
\item Only work with orthonormal features, i.e., impose $C=\Id$ at all
times. This is possible without loss of generality, because zero-shot RL
with linear features only depends on the linear span of the features.

In practice, this can be done by imposing a Lagrange multiplier for the
constraint $C=\Id$. This means adding a loss term $\lambda\orth\,
\loss\orth(\phi)$ in the algorithm, where $\lambda\orth$ is a large
weight, and where
\begin{align}
\label{eq:lossorth}
\loss\orth(\phi)& \deq  \norm{\langle
\phi,\phi\rangle_K-\Id}^2_{\mathrm{Frobenius}}
\\&=-2\sum_i \norm{\phi_i}_K^2
+\sum_{ij} \left(\langle \phi_i,\phi_j\rangle_K\right)^2
+\mathrm{cst}
\end{align}
is the loss associated with violating the constraint $C=\Id$.
This option corresponds to $\lambda_C=0$ in Algorithm~\ref{algo:main}.

With $\norm{\cdot}_K=\norm{\cdot}_\rho$, this loss simplifies to
\begin{equation}
\loss\orth(\phi)=\E_{s\sim \rho,\,s'\sim \rho}\left[
\left(\transp{\phi(s)}\phi(s')\right)^2-\norm{\phi(s)}^2-\norm{\phi(s')}^2
\right]+\mathrm{cst}
\end{equation}
similarly to the orthonormalization loss for $B$ in \cite{zeroshot}.
%The general expression depends on how $\norm{\cdot}_K$ is estimated.

Even with a large weight $\lambda\orth$, the condition $C=\Id$ will be
satisfied only approximately. Thus we still include $C$ in the algorithm.

When using the orthonormalization loss $\loss\orth$ with a large weight,
it is better if $\phi$ is initialized so that $C$ is not too far from
$\Id$.

\item The second option provides an exact estimation of the gradient of
$C$. 
This is carried out in Appendix~\ref{sec:proofs}, and results in the
following loss $\loss_C$ included in Algorithm~\ref{algo:main}:
\begin{equation}
\label{eq:lossC}
\loss_C(\phi,s,z)\deq
\tfrac12
d(s,z)\left(\transp{\bar \phi(s)}z\right)
\sum_{ij} \left((\bar C^{-1})_{ij}-z_iz_j\right)\langle
\phi_i,\phi_j\rangle_K
\end{equation}
where $\bar \phi$ and $\bar C$ are stop-grad versions of $\phi$ and $C$,
respectively.

If $\norm{\cdot}_K=\norm{\cdot}_\rho$, this can be estimated
as
\begin{equation}
\loss_C(\phi,s,z)=\tfrac12 d(s,z)\left(\transp{\bar \phi(s)}z\right)
\,\E_{s'\sim \rho} \left[\transp{\phi(s')}\bar
C^{-1}\phi(s')-(\transp{\phi(s')}z)^2
\right]
\end{equation}

Even if using the loss $\loss_C$, we still recommend to include a loss
$\loss\orth$, for numerical reasons to keep $\phi$ within a reasonable
numerical range. \footnote{When $\loss_C$ is included, mathematically $\loss\orth$
has no effect since everything only depends on the span of $\phi$ and not
$\phi$ itself. But numerically it will be more convenient to keep $\phi$
well-conditioned.}
\end{enumerate}

These results make it possible to optimize the features $\phi$ for a
Gaussian prior on downstream tasks. We now turn to other priors.

% For the third step, the gradient of \eqref{eq:tractableloss} w.r.t.\ $\phi$ is readily computed if a
% model or samples for $d_{\pi_z}$ is accessible. However, there is an
% extra complication due to $C$ also depending on $\phi$. We give more
% details in Section~\ref{sec:phiopt}.
% 
% 
% We now discuss options to optimize the zero-shot RL loss
% \eqref{eq:tractableloss}. These options depart from VISR in several ways.
% We discuss in turn how to deal with $d_{\pi_z}$ instead of just running
% $\pi_z$, and how to handle the gradients with respect to the covariance
% matrix $C$ of $z$. These options result in full algorithms collected in
% Section\todo{}.
% 
% The gradient of $\E_{s_0\sim \rho_0}
% M^{\pi_z}(s_0)\transp{\phi}z$ with respect to $\Phi$ is readily computed,
% using
% \begin{equation}
% \E_{s_0\sim \rho_0}
% M^{\pi_z}(s_0) \transp{\phi}z
% =
% \E_{s_0\sim \rho_0} \E_{(s_t)}\left[
% {\tsum_{t\geq 0}}\, \gamma^t \transp{\phi(s_t)}z
% \mid s_0,\pi_z
% \right]
% \end{equation}
% namely, we have to sample $z$, sample an initial state $s_0$,
% launch policy $\pi_z$ starting at $s_0$, and take the discounted sum of
% $\nabla \transp{\phi(s_t)}z$ along the trajectory.
% 
% This is possible online but not offline. Even if the online case, this
% Monte Carlo sampling might suffer from high variance.
% 
% For the offline case, the best bet is to learn a model $b(s,a,z)$ of the
% time spent at $(s,a)$ by policy $\pi_z$. This is dual to learning
% $Q$-functions: namely,
% this occupation measure satisfies a backward Bellman equation for each $z$,
% $\E_{s_0}M^{\pi_z}=\rho_0 {\pi_z}+\gamma \E_{s_0} M^{\pi_z}P_{\pi_z}$. Plugging
% the parametric model 
% \begin{equation}
% \E_{s_0} M^{\pi_z}(s_0,\d s,a)=b(s,a,z)\rho(\d s,a)
% \end{equation}
% into the Bellman gap loss $\norm{(\E_{s_0}M^{\pi_z}-\rho_0
% {\pi_z}-\gamma \E_{s_0} \bar M^{\pi_z}P_{\pi_z})/\rho}^2_\rho$, with
% $\bar M$ a target version of $M$, we find that
% % TODO write loss on Bellman gap instead
% % \begin{equation}
% % b(s_{t+1},a_{t+1},z)=\frac{\rho_0(\d
% % s_{t+1})\pi_z(a_{t+1}|s_{t+1})}{\rho(\d s_{t+1}, a_{t+1})}+\gamma
% % b(s_t,a_t,z)\pi_z(a_{t+1}|s_{t+1})
% % \end{equation}
% % in expectation over transitions $(s_t,a_t,s_{t+1})$ from $\rho$. Thus
% the loss \footnote{We could also use a KL loss on the Bellman gap instead of a square loss,
% provided the parameterization maintains $b>0$. A KL loss makes sense
% since $b$ represents a 
% distribution.} for $b$ is
% \begin{multline}
% \ell(b)=\E_{sa\sim \rho}\,b(s,a,z)^2-2\E_{s_0\sim \rho_0,\,a\sim \pi_z(s_0)}
% b(s_0,a,z)\\-2\gamma \E_{s_ta_ts_{t+1}\sim \rho,\,a_{t+1}\sim
% \pi_z(s_{t+1})} \,b(s_{t+1},a_{t+1},z)\bar b(s_t,a_t,z)+ \mathrm{Cst}
% \end{multline}
% with $\bar b$ a non-trainable target version of $b$.
% 
% When this
% loss is minimized, one has 
% \begin{equation}
% \E_{s_0\sim \rho_0}\,
% M^{\pi_z}(s_0)\transp{\phi}z=\E_{sa\sim \rho} \,b(s,a,z)\transp{\phi(s)}z
% \end{equation}
% which can be used to estimate gradients with respect to $\phi$ in the
% loss $\ell$.
% % so we can estimate the gradient above as
% % \begin{equation} \nabla
% % \E_{s_0\sim \rho_0} M^{\pi_z}(s_0) 
% % \transp{\phi}z = \E_{sa\sim \rho}\, b(s,a,z)\nabla
% % \transp{\phi(s)}z.
% % \end{equation}
% 
% How worried should we be about using a learned model $b$? Learning
% occupation measures is formally dual to learning $Q$-functions.
% When learning $\pi$ by policy gradient,
% we first estimate a $V$ function by Bellman, then weigh $\nabla \ln\pi$
% by $V$. Here we first estimate $b$ by backward Bellman, then weigh
% $\nabla\phi$ by $b$. \footnote{Contrary to instances of backward Bellman
% on the full successor measure as in FB, here we only use backward Bellman to
% estimate a single function $b$, thus difficulty should be similar to
% ordinary $Q$-learning.}
% 

% \subsection{Gradient of the Loss from $z\sim N(0,(\phi
% D\transp\phi)^{-1})$}
% 
% The sampling of $z$ is $\phi$-dependent, so this produces gradients with
% respect to $\phi$. The meaning of $z$ depends on $\phi$. Ignoring these gradients would change the prior
% during optimization, and just blow $\phi$ up since otherwise the loss is just
% proportional to $\phi$. \footnote{An easy option would be to ignore this
% and use a strong auxiliary normalization loss on $\phi D\transp\phi$ to
% stay close to $\phi D\transp\phi\approx \Id$.}
% 
% Instead, we can use the plain old log-trick: \footnote{namely
% $\partial_\theta \E_{x\sim p_\theta(x)}\,\ell(x)=\E_{x\sim
% p_\theta(x)}[\ell(x)\,\partial_\theta \ln p_\theta(x)]$}
% \begin{equation}
% \partial_C \E_{z\sim N(0,C^{-1})}\, \ell_z = \tfrac12 \E_{z\sim
% N(0,C^{-1})}\left[\ell_z
% \left(-\transp z \delta C z + \Tr C^{-1}\delta C\right)\right]
% \end{equation}
% where $\ell_z=-\E_{s_0\sim \rho_0} M^{\pi_z}(s_0)\transp \phi
% z=-\E_{s_0\sim \rho_0} V^{\pi_z}_{\transp\phi z}(s_0)$ is the
% loss associated with each $z$.
% 
% This works in all cases, whether we estimate $M^{\pi_z}$ from trajectories
% or from a model $b(s,a,z)$.
% 
% We could also use the less noisy, gradient-based
% \begin{equation}
% \partial_C \E_{z\sim N(0,C^{-1})} \ell_z = -\tfrac12 \E_{z\sim
% N(0,C^{-1})} \left[\transp z \delta C C^{-1} \nabla_z \ell_z\right]
% \end{equation}
% but this only works if we have access to $\nabla_z \ell_z$. In general we
% don't have access to $\nabla_z M^{\pi_z}$, but if we use the model
% $b(s,a,z)$ we may use $\nabla_z b(s,a,z)$. This can be implemented as
% \begin{equation}
% \partial_C \E_{z\sim N(0,C^{-1})} \ell_z = -\tfrac12 \E_{z\sim
% N(0,\bar C^{-1})}\, \partial_C \ell_z(\bar C^{-1}Cz)
% \end{equation}
% using a stop-grad version $\bar C$ of $C$. Applicable if we use $b$,
% but not if we Monte Carlo sample form $M^{\pi_z}$ (unless we include the grad of log-policies along
% the whole trajectory, Reinforce-style).
% 
% \subsection{Conditioning $\phi$}
% 
% We can use an auxiliary loss to help with
% matrix conditioning and bring $\Cov \phi$ closer to $\Id$. Define
% \newcommand{\lorth}{\ell_{\mathrm{orth}}}
% \begin{equation}
% \lorth(\Phi)\deq \norm{\phi D
% \transp{\phi}-\Id}^2_{\mathrm{Frobenius}}=\Tr \left(
% (\phi D \transp{\phi}-\Id)\transp{(\phi D \transp{\phi}-\Id)}
% \right).
% \end{equation}
% 
% The orthonormalization loss $\lorth$ can be computed explicitly depending
% on the expression of $D$: for white noise, $D=\E_{s\sim \rho} [\1_s
% \transp{1_s}]$ and
% \begin{equation}
% \lorth(\Phi)=\E_{s\sim \rho,\,s'\sim \rho} \left[\left(
% \transp{\phi(s)}\phi(s')\right)^2-\norm{\phi(s)}^2-\norm{\phi(s')}^2
% \right] + \mathrm{Cst}.
% \end{equation}
% The expression for the GFF prior is longer but
% similar in structure.
% 
% It is better if $\phi$ is initialized so that $\phi D
% \transp{\Phi}\approx\Id$. This can be done by explicit projection $\phi
% \gets (\phi D
% \transp{\phi})^{-1/2} \phi$  (which might also be useful once in a
% while): if $\phi$ is given by a NN with linear last layer, this amounts
% to multiplying the last layer weight matrix by $(\phi D
% \transp{\phi})^{-1/2}$ for the same rewards. At the same time, the first layer of $\pi_z$
% should be multiplied by $(\phi D
% \transp{\phi})^{-1/2}$, namely, 
% $\pi_z\gets \pi_{(\phi D \transp{\phi})^{-1/2}z}$,
% so that we still apply the same policies for the
% same rewards and don't have to re-learn $\pi_z$ to match $\phi$.

\subsection{Learning the Optimal Features for Sparse Reward Priors}
\label{sec:phisparse}

We now turn to the sparse reward priors from
Section~\ref{sec:sparsepriors}. Since goal-reaching is a special case of
scattered random rewards (with $k=1$), we only deal with the latter.
Namely, we consider sparse rewards of the form
\begin{equation}
\label{eq:sparsereward}
r=c_k \sum_{i=1}^k w_i \,\delta_{s^\star_i}
\end{equation}
where $\delta_{s^\star}(s)\deq
\1_{s=s^\star}/\rho(s^\star)$
is the Dirac sparse reward \footnote{Dirac with respect to the measure $\rho$,
namely, $\E_\rho [f.\delta_{s^\star}]=f(s^\star)$. In
particular, $\E_\rho \delta_{s^\star}=1$.} at
$s^\star$ as defined in Section~\ref{sec:sparsepriors}, 
$k$ is an integer following some probability distribution,
$(s^\star_i)_{1\leq i \leq k}$ are goal states sampled
from the data distribution $\rho$, the $w_i$ are weights sampled from
some distribution on $\R$, and $c_k$ is a scaling factor. A typical example is $w_i\sim N(0,1)$ and
$c_k=1/\sqrt{k}$.

Arguably, with such a model, we could just send the full reward
description $(s^\star_i,w_i)_{1\leq i \leq k}$ to a $Q$-function
or policy model. However, our goal is
to be able to mix several types of priors on rewards
(Section~\ref{sec:mixing}): we want to find zero-shot RL methods that
work both for dense and sparse rewards. Therefore, we describe a method
whose structure is closer to that of the previous sections, by learning
optimal features $\phi$.


\begin{prop}
\label{prop:sparseloss}
Let $\beta$ be a prior on sparse rewards of the type
\eqref{eq:sparsereward}, for some distribution of $(k,(w_i),c_k)$ and
where each $(s^\star_i,a^\star_i)$ has distribution $\rho$.

Assume that the reward representation $z=\Phi(r)$ is computed as in successor
features \eqref{eq:genSF} using the norm $\norm{\cdot}_K$, namely,
\begin{equation}
z=C(\phi)^{-1} \langle r,\phi\rangle_K
\end{equation}
where $\phi\from S\to \R^d$ are linearly independent features, and where
$C(\phi)$ is the matrix with entries $C_{ij}= \langle \phi_i,\phi_j\rangle_K$.

Then the zero-shot RL loss $\ell_\beta(\Phi,\pi)$ satisfies
\begin{equation}
\ell_\beta(\Phi,\pi)=-\frac{1}{1-\gamma} \E_{k,\, s^\star_i,\,w_i}\,
\sum_{i=1}^k
c_k w_i\,
d\left(s^\ast_i,z(\phi)\right)
\end{equation}
where
\begin{equation}
z(\phi)=\sum_j c_kw_j C(\phi)^{-1} \langle \delta_{s^\star_j},\phi\rangle_K
%\phi(s^\star_j)
%+c_kw_j (\bar C^{-1}-\bar C^{-1} C(\phi) \bar C^{-1} )\bar \phi(s^\star_j)
\end{equation}
and where $d(s,z)$ is the density of $d_{\pi_z}(s)$ with respect to the data
distribution $\rho$.%, and $\bar C$ and $\bar \phi$ denote stop-grad
%versions of $C$ and $\phi$.
\end{prop}

The density $d(s,z)$ is the same as in Algorithm~\ref{algo:main}, and can
be learned via Algorithm~\ref{algo:occupation}.

Algorithm~\ref{algo:sparse} instantiates this result for the case where
$\norm{\cdot}_K=\norm{\cdot}_\rho$. In that case, we have
\begin{equation}
\langle \delta_{s^\star_j},\phi\rangle_\rho=\phi(s^\star_j)
\end{equation}
which simplifies the expression for $z$.

Two points in Algorithm~\ref{algo:sparse} are tricky. The first is how to compute the
gradient of $z(\phi)$ with respect to $\phi$, and in particular the
gradient of $C(\phi)^{-1}$. In Algorithm~\ref{algo:sparse}, we have used
that $C=\E_{s'} \phi(s')\transp{\phi(s')}$ when
$\norm{\cdot}_K=\norm{\cdot}_\rho$. We have included an extra term
\begin{equation}
\bar C^{-1}
    \left(\bar\phi(s')\transp{\bar\phi(s')}-\phi(s')\transp{\phi(s')}\right)\bar
    C^{-1}\bar \phi(s^\star_i)
\end{equation}
which evaluates to $0$ in the forward pass (since $\bar\phi=\phi$) but
provides the correct gradients with respect to $C(\phi)^{-1}$ in the backward pass.

The second tricky point is how to update the $Q$-function for the sparse
reward. Here we have directly applied the results from
\cite{blier2021unbiased} for $Q$-learning with Dirac rewards such as
\eqref{eq:sparsereward}. This point
is important when mixing different priors (Section~\ref{sec:mixing}): the
$Q$-functions for different priors should be updated in a consistent way,
(e.g., all updated using the Bellman loss for their respective rewards).

  \begin{algorithm}[h!]
    \small
    \caption{One possible algorithm to optimize the zero-shot RL loss
    with sparse rewards \eqref{eq:sparsereward}}
    \label{algo:sparse}
 \begin{algorithmic}
    \STATE \textbf{Input:}
    \\Dataset of transitions $(s_t,a_t,s_{t+1})$
    with distribution $\rho$.
    \\Online EMA weights $\beta_t\in (0,1)$ to estimate $C$.
    \\Probability distribution on $k\in \N$, the number of goals in the
    sparse rewards.
    \\Probability distribution on weights $w_i$ (default: $N(0,1)$),
    scaling factor $c_k$ (default: $1/\sqrt{k}$).
    \STATE \textbf{Output:}\\
    Trained features $\phi_1,\ldots,\phi_d$ with their covariance
    matrix $C$.
    \\Trained policies $\pi_z$.
    \WHILE{not done}
    \STATE Update covariance matrix $C$ via EMA:
    %\algindent
    $C_{ij}\gets \beta_t
    C_{ij}+(1-\beta_t)\E_{s\sim \rho} \phi(s)\transp{\phi(s)}$
    Sample a value of $k$. Sample $k$ goal state-actions
    $(s^\star_i,a^\star_i)$ from the dataset distribution $\rho$. Sample
    weights $w_i$.
    \STATE Sample a state $s'\sim \rho$
    \STATE Compute
    \algindent
    $z(\phi)=\sum_i c_k w_i \bar C^{-1}
    \phi(s^\star_i)+c_k w_i \bar C^{-1}
    \left(\bar\phi(s')\transp{\bar\phi(s')}-\phi(s')\transp{\phi(s')}\right)\bar
    C^{-1}\bar \phi(s^\star_i)$
    \\where $\bar C$ and $\bar \phi$ are stop-grad versions of $C$ and
    $\phi$
    \STATE Update a $Q$-function $Q(s,a,z)$ at $z=z(\phi)$ with the Bellman loss
    \algindent
    $\ell(Q)=Q(s_t,a_t,z)^2-2\sum_i c_kw_iQ(s^\star_i,a^\star_i,z)-2\gamma
    Q(s_t,a_t,z)\bar Q(s_{t+1},a_{t+1},z)$
    where $(s_t,a_t,s_{t+1})$ is sampled from $\rho$, where $a_{t+1}$ is
    sampled from $\pi_z(s_{t+1})$, and where $\bar Q$ is a target version
    of $Q$.
    \STATE Update a policy $\pi_z(a|s)$ based on $Q(s,a,z)$, using any RL
    policy algorithm
    \STATE Update the occupation measure model $d(s,z)$ via one step of
    Algorithm~\ref{algo:occupation}
    \STATE Update $\phi$ with the loss
    \algindent $\loss(\phi)=-\sum_i c_k w_i\,d(s^\star_i,z(\phi))$
    \\where the gradients w.r.t.\ $\phi$ are backpropagated through $d$ and
    $z$.
    \ENDWHILE
    \STATE \textbf{Deployment:}
    \\Once the reward function $r$ is known:
    \STATE Estimate $\langle r,\phi_1\rangle_K$, \ldots $\langle
    r,\phi_d\rangle_K$
    \STATE Set $z=C^{-1}\langle r,\phi\rangle_K$
    \STATE Apply policy $\pi_z$
 \end{algorithmic}
 \end{algorithm}

% Here we consider a model of sparse rewards of the form
% \begin{equation}
% r=\frac1{\sqrt{k}}\sum_{i=1}^k \alpha_i \delta_{s_i}
% \end{equation}
% where $s_i$ are random states sampled from some distribution $\mu$, 
% where $\delta(s_i)$ is a Dirac function
% \footnote{In continuous spaces where policies or the environment are
% stochastic, sparse rewards are best treated as Diracs (cf
% Blier-Ollivier), because otherwise the probability to exactly reach a
% goal state is $0$. With this formalism, the $Q$-function is the probability
% density to reach the target state (with probability densities expressed
% wrt $\rho$). In a discrete space, this Dirac
% function is just $\1_{s_i}/\rho(s_i)$.}
% wrt the reference measure $\rho$
% (namely $\E_{s\sim \rho} \delta_{s_i}(s) f(s)=f(s_i)$ by definition),
% and where $\alpha_i$ are, say, Gaussian with unit variance. The number of
% targets $k$ may be fixed or random. The scaling $1/\sqrt{k}$ ensures unit
% variance of $\E_\rho r$ for any $k$. When
% $k\to \infty$ and $\mu=\rho$, this tends to the white noise prior in weak
% topology. \footnote{So maybe we can just use this one?...}
% 
% Arguably, with such a model, this is a parametric family of tasks
% parameterized by the goal $g=(\alpha_i,s_i)_{i=1\ldots k}$, and one
% should just train a parametric $Q$-function $Q(s,a,g)$. Still, the
% network to handle this input $g$ will compute a representation $z$ of
% the $k$-tuple $(\alpha_i,s_i)$, then
% apply a policy $\pi_z$. Moreover, we may want to find a system that can
% handle \emph{both} continuous and sparse rewards. So we 
% keep the same formalism as above. \footnote{would mix qute well with the ideas
% from Section~\ref{sec:alt} of representing a reward by a tuple of states,
% though}
% 
% With such a model, one has
% \begin{equation}
% V^{\pi_z}_r(s_0)=\sum_i \alpha_i M^{\pi_z}(s_0,\d s_i)/\rho(\d s_i)
% \end{equation}
% and moreover, assuming we compute features by the unnormalized
% \footnote{Here normalization by $(\E_\rho \phi  \transp{\phi})^{-1}$ is
% inconvenient, producing gradients of this inverse matrix. Still,
% normalization might be re-introduced at test time to protect against
% distribution shift of $\rho$.} SF formula
% \begin{equation}
% \Phi(r)=\E_{s\sim \rho} \,r(s)\phi(s)=\sum_i
% \tfrac{\alpha_i}{\sqrt{k}} \phi(s_i)
% \end{equation}
% so
% the loss \eqref{eq:mainloss} is
% \begin{align}
% \ell(\Phi,\pi)&=-\E_r \E_{s_0} V^{\pi_{\Phi(r)}}_r (s_0)
% \\&=-\E_{(\alpha_i,s_i)} \sum_i \alpha_i\, b\left(s_i,
% \tsum_i \tfrac{\alpha_i}{\sqrt{k}}
% \phi(s_i)\right)
% \label{eq:sparseloss}
% \end{align}
% where as above, $b$ is a model of the occupation measure $M$:
% \begin{equation}
% \E_{s_0} M^{\pi_z}(s_0,\d s)=b(s,z)\rho(\d s)
% \end{equation}
% which was used previously.
% 
% Given $b$, computing the gradient wrt $\phi$ is trivial. The effect will
% push $\phi(s)$ towards the values of $z$ such that $\pi_z$ most visits
% $s$.
% 
% What is less trivial is training the policies $\pi$. By
% Proposition~\ref{prop:posteriormean}, $\pi_z$ must be the optimal policy
% for the posterior mean reward associated to $z$. This can be obtained by
% sampling a reward, computing $z$, and adjusting $Q(s,a,z)$ for that
% reward. \footnote{Using the gradient of $\ell(\Phi, \pi)$ with respect to
% $\pi$ leads to something similar. Indeed, $\ell(\Phi, \pi)$ depends on
% $\pi$ via $b$. Using that $\partial_\pi M^{\pi}=-M^\pi \partial \pi
% M^\pi$, this leads to a policy gradient of $\pi_z$ for the reward $r$,
% weighted by $b$ itself (namely, computed online for each $\pi_z$).}
% 
% $Q$-learning for
% infinitely sparse rewards can be done exactly (Blier--Ollivier), just by
% writing the Bellman gap loss and integrating the Dirac:
% \begin{align}
% \ell(Q)&=\E_{(s,a)\sim\rho} \left(Q(s,a)-\tsum_i
% \tfrac{\alpha_i}{\sqrt{k}}
% \delta_{s_i}(s)-\gamma\E_{s'|(s,a),\,a'\sim \pi(s')}\bar Q(s',a')\right)^2
% \\&=\E_\rho Q(s,a)^2-2\tsum_i \tfrac{\alpha_i}{\sqrt{k}} Q(s_i,??)-2\gamma Q(s,a)\bar
% Q(s',a')+\mathrm{cst}
% \end{align}
% TODO I need to put $(s_i,a_i)$ instead of just $s_i$ for the sparse
% reward... TODO if $\mu=\rho$ this gets simpler, one can take actions from
% the replay buffer
% 
% So the algo would be to sample $(s_i)$ and $(\alpha_i)$, compute $z$, and
% train $Q(s,a,z)$ with the loss above.
% 
% Problem: requires a lot of samples and evaluations $Q(s_i,a_i,z)$ for a
% single update. Even if using a minibatch, and for each $(s_i,a_i)$ in the
% minibatch, using a reward defined by coeffs $\alpha_{ij}$ on the other
% states $(s_j)_{j\neq i}$ of the minibatch,
% these would have different values for $z$, still requiring many
% evaluations of $Q$. Can probably be subsampled.
% 
% \bigskip
% 
% This time, there is no chance of the features $\phi$ being sparse: the
% features have to somehow extract the info about the location and values
% of the rewards.
% 
% Since this sparse prior can be combined with the white noise 
% prior or the continuous GFF prior, we can use the sum loss to train features $\phi$ which should work
% for both. \footnote{I don't know how to combine with the discrete GFF
% prior: one must compute $\langle \delta,\phi\rangle_D$ which requires
% backsampling states leading to $s_i$, and sampling states from $s_i$. For
% the continuous GFF this is simple, as $\int \nabla \delta_s\cdot \nabla
% \phi=-\int \delta_s\,\Delta \phi=-\Delta\phi(s)$. So in that case the
% formula \eqref{eq:sparseloss} can be adapted.}
% 
% Does this explain our mixture sampling method for $z$ values in FB??
% 
% \bigskip
% 
% Fundamentally, this is equivalent to a goal-conditioned DQN where the
% goal is $g=(k,(\alpha_i,s_i)_{i=1\ldots k})$ (a potentially
% variable-dimensional or infinite-dimensional goal). But learning as in
% goal-conditioned DQN would not obviously provide policies for continuous
% rewards (it would not provide the representation $\phi$).

\subsection{Mixing Priors}
\label{sec:mixing}

The zero-shot RL loss \eqref{eq:mainloss} is linear in the prior $\beta$.
Therefore, if two priors $\beta_1$ and $\beta_2$ are amenable to gradient
descent for this loss, one can deal with a mixture prior just by mixing
the losses for $\beta_1$ and $\beta_2$,
using a single set of features $\phi$, $Q$-functions, and
policies $\pi_z$.

In practice, this just means choosing at random, at each step, between
doing an optimization steps for one of the priors, e.g., alternating
between Algorithms~\ref{algo:main} and \ref{algo:sparse}.

Of course, this requires using consistent optimization methods for both
priors: the same optimizer, but also similar Bellman losses and policy
updates for $\beta_1$ and $\beta_2$. For instance, $\beta_1$ and
$\beta_2$ may both use the standard Bellman loss
$\left(Q(s_t,a_t,z)-r(s_t,z)-\gamma \bar Q(s_{t+1},a_{t+1},z)\right)^2$
where $r(s,z)$ is the mean reward knowing $z$ for a given prior. Then if
$\beta_1$ has posterior mean reward $r_1(s,z)$ knowing $z$ and likewise
for $\beta_2$, optimizing the $Q$-function alternatively between
$\beta_1$ and $\beta_2$ effectively optimizes for the mean posterior
reward of the mixture.


\section{Discussion}

\subsection{What Kind of Features are Learned? Skill Specialization and the Zero-Shot RL Loss}
\label{sec:whatfeatures}

The features
learned are influenced by the prior, and this is one reason why mixing
priors may be appealing.

For a pure goal-oriented prior, it is enough to learn a feature that
represents different goals by different values of $z$, so, it is enough
for $\phi$ to be injective (e.g., with $\dim \phi= \dim s$ and
$\phi=\Id$). On a discrete space, a one-dimensional $\phi$ may solve the
problem just by sending every state to a different value. Of course, this
will not work when mixed with other types of rewards.

For dense Gaussian priors, on the other hand, learning may produce
narrow features $\phi$, resulting in overspecialized skills. Indeed, 
conceptually, from Theorem~\ref{thm:main}, gradient descent of $\ell$ for $\pi_z$ and $\phi$ amounts
to:
\begin{itemize}
\item Learn $\pi_z$ to optimize reward $\transp{\phi}z$ for each $z$;
\item Learn $\phi$ by increasing $\transp{\phi}z$ at the states visited
by $\pi_z$.
\end{itemize}

The above is related to diversity methods \cite{eysenbach2018diversity}
and has a ``rich-get-richer'' dynamics: this is good for diversifying and
specializing, but might overspecialize. We illustrate this phenomenon
more precisely in the next paragraph.

\paragraph{Understanding overspecialization: Analysis with only one
feature, and influence of the prior.}
This is best understood on a ``bandit'' case (we can jump directly to any
state) and with only one feature.
In this case, a full analysis can be done, and the optimal
one-dimensional feature has only two non-zero values: a large positive
value at a state $s_1$ and a large negative value at a state $s_2$.
\footnote{Indeed, take
a finite state space $S=\{1,\ldots,n\}$ and assume that at any state,
there's an action directly leading to any other state: this makes the MDP
into a bandit problem.
Take $1$-dimensional $\phi$. Then from
Theorem~\ref{thm:main}, the gradient with respect to $\phi$ is
$d_{\pi_z}.z$ where $\pi_z$ is the policy to maximize reward $z.\phi$. If
$z>0$ then $\pi_z$ goes to the maximum of $\phi$, and
$d_{\pi_z}=(1-\gamma)U+\gamma \1_{\argmax \phi}$ where $U$ is the uniform
distribution. If $z<0$ then $d_{\pi_z}=(1-\gamma)U+\gamma \1_{\argmin
\phi}$. Since the distribution of $z$ is symmetric, on average the
gradient w.r.t.\ $\phi$ is proportional to $\1_{\argmax
\phi}-\1_{\argmin\phi}$. Gradient ascent on $\phi$ will converges to a
$\phi$ that has only two nonzero values, one positive and one negative.
(The cases where there are ties between the values of $\phi$ at several
states are numerically unstable.)
This applies to any Gaussian prior on rewards.
}

Is this specific to the ``bandit'' case? If the environment has full
reachability (the agent can reach any state and stay there), and if
the discount factor $\gamma$ is close to $1$, then the problem is
essentially a bandit problem. The transient dynamics before reaching a
target state will contribute $O(1-\gamma)$ to occupation measures
$d_\pi$, any the analysis done on the bandit case will hold up to
$O(1-\gamma)$.

Using a smoother prior on rewards (such as the Dirichlet prior, which
favors spatially smooth rewards) does not change this: this applies to
any Gaussian prior including the Dirichlet prior. The prior will
influence the location of the two states $s_1$ and $s_2$ at which the
feature is nonzero. \footnote{Intuitively, the feature $\phi$ only looks
at the reward at states $s_1$ and $s_2$ before choosing and applying a
policy.  With a Dirichlet prior, nearby states have correlated rewards,
so looking at the reward at $s_2$ does not bring much information if
$s_2$ is close to $s_1$: it brings more information to measure the reward at distant states
$s_1$ and $s_2$.
}

Yet such features \emph{are} optimal for the zero-shot RL loss with a
Gaussian prior. In an environment with full reachability and
$\gamma$ close to $1$, \emph{the optimal zero-shot behavior with one
feature consists in measuring the reward at two states and going to
whichever of those two states has the largest reward}. This applies to
any Gaussian prior on rewards, including priors whose covariance matrix
produces spatial smoothness on rewards.

So, if these features are considered undesirable, this
reflects a mismatch between the prior $\beta$ and the true distribution
of test tasks in the test loss \eqref{eq:testloss}. This pushes towards
mixing different types of priors, such as Gaussian and sparse reward priors.

Sparse reward priors such as goal-oriented (Dirac) rewards
correspond to smoother features such as $\phi=\Id$. This illustrates the
mathematical duality between $\phi$ and $r$ when estimating $z$ via
$\E[r.\phi]$: smoother priors on $r$ may lead to \emph{less} smooth
features $\phi$ (the dual of a space of smoother functions contains less
smooth functions). Intuitively, with sparse rewards $r$, the features
$\phi$ must be able to ``catch'' the location of the reward anywhere in
the space, and cannot be zero almost-everywhere.

\paragraph{Does the Bayesian viewpoint regularize the optimal features?} One might
have expected that the Bayesian flavor of the zero-shot RL objective
would result in regularized policies. But this is not the case: by
Proposition~\ref{prop:posteriormean}, every policy $\pi_z$ is a ``sharp''
policy, in the sense that it is optimal for some reward $r_z$.
Uncertainty on the reward does not induce noise on the policy: if the
maximum of $r_z$ is reachable, $\pi_z$ will go straight to it and stay
there. This contrasts with the effect of regularizations such as an
entropy regularization, which adds noise to the policy.

Yet this is the ``correct'' (optimal) answer given the zero-shot RL loss.

This overspecialization tendency has already been observed for diversity
methods: for instance, \cite{eysenbach2021information} also find that
skills learned must be optimal for some particular downstream task,
although they work from an information criterion and not from the
zero-shot RL loss. This seems to be an intrinsic property of this general
approach.

This illustrates the main assumption in the zero-shot RL framework: at test
time, the reward function is fully known and one can compute $z=\Phi(r)$.
This leaves no space for uncertainties on $r$, or any fine-tuning based
on further reward observations. The model estimates $z$, then applies a
policy that will maximize the mean posterior reward $r_z$, e.g., by going
to the maximum of $r_z$ and staying there if possible.

This optimal only if no uncertainty exists on $r$ and no fine-tuning of
the policies is possible.

\paragraph{Comparison with the Forward-Backward framework.} The results
in this text shows that forward-backward representations \cite{zeroshot,
allpolicies} have no reason
to be optimal: If the prior $\beta$ on tasks is known, then one shold
optimize the features for that prior.

However, the discussion above shows that the priors for which we can
compute optimal features may not necessarily reflect the kind of features
we expect to learn. Mixing different priors should mitigate that effect,
but to what extent is currently unclear.

Forward-backward representations aim at learning features that can
faithfully represent the long-term dynamics (successor measures) of many
policies. This is a different kind of implicit prior, closer in spirit to
a world model, and with no explicit distribution over downstream tasks.

\subsection{Future Directions}
\label{sec:future}

\paragraph{Avoiding overspecialized skills.} The zero-shot RL loss can
lead to very narrow optimal features with a Gaussian prior, as we have
seen. This is optimal for the loss \eqref{eq:mainloss}, but not what we
want in general, possibly reflecting a mismatch between a Gaussian prior
and ``interesting'' rewards.

One possible solution is to mix different priors.

Another possible solution is to account for variance over downstream
tasks in the zero-shot loss: we not only want the best expected
performance, but we don't want performance to be very bad for some tasks.
For the white noise prior, a full analysis is possible
(Appendix~\ref{sec:varianceregul}): incorporating variance is equivalent
to penalizing the $L^2$ norm of the occupation measures $d_\pi$ (this
will minimize spatial variance, thus ``spreading'' $d_\pi$). However,
it is not obvious how to exploit this algorithmically (since $d_\pi$ is
computed from $\pi$ and not the other way around, adding a penalty on
$d_\pi$ will just make the computation of $d_\pi$ wrong). Things are
actually simpler if we add a downstream task variance penalty to the FB
framework (Appendix~\ref{sec:varianceregul}).

Other solutions are to explicitly regularize the features (e.g., minimize
their spatial variance, or their Dirichlet norm to impose temporal
smoothness) or the policies (e.g., by entropy regularization). But since
the overspecialized features actually optimize the zero-shot RL loss (for
some priors), it is more principled to regularize the loss itself.

\paragraph{Nonlinear task representations.} In this text, we have covered
linear task representations, as these are the ones in the main zero-shot
frameworks available (successor features and forward-backward). However,
a linear task representation $r\mapsto z$ clearly limits the
expressivity of zero-shot RL.

One way to get nonlinear
reward representations, introduced in \cite{fb-aware}, is to iterate
linear reward representations in a hierarchical manner:
\begin{equation}
z_1=\E\, r(s)\phi_1(s),\qquad z_2=\E\,r(s) \phi_2(s,z_1),\qquad z_3=\ldots
\end{equation}
namely, $z_1$ provides a rough first reward representation, which can be
used to adjust features 
$\phi_2$ more precisely to the reward function. \cite{fb-aware} prove
that two such levels already provides full expressivity for the correspondence $r\mapsto z$.
This is amenable to a similar analysis as
the one performed in this text. The sparse reward case looks largely
unchanged, but the case of Gaussian priors is more complex: the
covariance matrix $C$ now depends on $z_1$, so it would have to be
represented via a learned model or estimated on a minibatch. We leave
this for future work.

Another way to bypass the linearity of the task representation would be
to kernelize the norm $\norm{r}_K$ used in the definition of Gaussian
reward priors.

\paragraph{Incorporating fine-tuning and reward uncertainty at test
time.} Finally, the analysis here relies the main hypothesis behind
the zero-shot RL framework: that at test time, the reward function is
instantly and exactly known. This is the case in some scenarios (eg,
goal-reaching, or letting a user specify a precise task), but not all. In
such situations, some fine-tuning of the policies will be necessary.
Which features provide the best initial guess for real-time fine-tuning is
out of the scope of this text. Zero-shot RL assumes the reward function is fully
specified at test time: if it is not, then meta-RL approaches
\cite{beck2023survey}
probably provide a better solution.

\section{Conclusions}

The zero-shot RL loss is the expected policy performance of a zero-shot
RL method on a distribution of downstream tasks. We have shown that this
loss is algorithmically tractable for a number of uninformative priors on
downward tasks, such as white noise, other Gaussian distributions
favoring spatial smoothness, and sparse reward priors such as
goal-reaching or random combinations of goals. We recover VISR as
a particular case for the white noise prior. We have also illustrated how
dense Gaussian reward priors can lead to very narrow optimal features,
which suggests that a mixture of different priors could work best.

% \TODO
% 
% 
% I have problems because this approach (at least with a white noise
% prior) tends to produce very specialized skills (go to a single state);
% for instance, I think it cannot solve goal-reaching on a grid unless
% using an exponential number of features, while FB can do it with a 
% number of features linear in the grid dimension.
% 
% Accounting for the variance over $r$ in the loss \eqref{eq:mainloss}
% should help; this is tractable in principle (Section~\ref{sec:varianceregul})
% and amounts to penalizing the spatial variance of the visitation measure
% of $\pi_z$.
% 
% 
% TODO more examples, explain more the surprises and they are maybe not too
% negative.
% 
% In particular, we can expect a prior containing sparse rewards to force
% the features to be nonzero everywhere. For instance, with goal-reaching
% rewards, the features must ``catch'' the goal $s^\star$ anywhere in the
% state space.
% 
% \paragraph{Do these policies overspecialize or make too narrow bets at
% test time?}
% Depending on the prior, these policies may actually be too narrow.
% In general, this loss will jump to the most-controllable skills in the
% environment, which can be quite narrow. Whatever the prior, by
% Proposition~\ref{prop:posteriormean}, we make a bet on the location of the
% best reward and go there.
% 
% Take the white noise prior.  Assuming we can reach arbitrary states and
% stay there, for $\gamma\approx 1$, and $d=1$, I think the fixed points
% are as follows: $\phi$ has one state $s_1$ with very high value and one
% state $s_2$ with very large negative value, all other states have values
% $O(1-\gamma)$.  What this policy does: it compares the rewards at $s_1$
% and $s_2$, and goes to the state with the highest reward; not an
% unreasonable behavior with a white noise prior and only one feature.
% This is stable: with such $\phi$, the optimal policy $\pi_z$ spends all
% its time on $s_1$ for positive $z$ and on $s_2$ for negative $z$, and so
% the values there get reinforced. $s_1$ and $s_2$ may be anything, due to
% initialization amplified by the rich-get-richer dynamics, but the optimal
% choice is when $s_1$ and $s_2$ are the states with smallest $\rho$,
% because the white noise has highest variance. This is a bit weird
% offline, but online this will induce exploration.  (In a continuous space
% with only imperfect control, the equivalent would be the smallest zone
% which we can reliably reach, because the variance of white noise in that
% zone will be high.)
% 
% Still, this does look like the most natural loss in the
% fully-known-reward-function scenario. Some further possibilities around
% this:
% 
% \begin{enumerate}
% 
% \item Admit the features above are the optimal choice given the white
% noise prior. Then work with different priors instead of white noise...
% Still, even with different priors, some of the same will occur.  With
% spatially correlated priors, different features will select maximally
% decorrelated states to maximize information, but in the end we will
% probably make very precise ``bets'' at test time. In particular the
% optimal policies are not stochastic.
% 
% \item Accounting for variance across rewards: instead of optimizing the
% expectation over rewards, optimize $(\E_r[\cdots])^2-\lambda \Var_r
% [\cdots]$. Not sure this stays tractable, because we lose the equivalence
% with optimizing $r_z$. (I think it's feasible to compute this variance
% for WN prior, using the model $b$ of $M^{\pi_z}$, get something like the
% norm of $b$ projected onto the orthogonal of $\phi$.)
% 
% \item Accounting for variance at test time from $z$ estimation: the
% narrower $\phi$, the harder it will be to reliably estimate $z$ at test
% time. Better, accounting for any possible fine-tuning at test time.
% Still, in the fully-known reward setting, this changes nothing.
% 
% \item Regularize the policies, e.g. by entropy regularization.(We might
% have to regularize policies anyway because for offline learning, things
% go off if policies start to take actions not well represented in the
% dataset.)
% 
% For entropy regularization in the large temperatures regime, my
% other text provides an exact description of the optimal features for the
% white noise prior: namely, the eigenfunctions of the symmetrized inverse
% Laplacian.  These can be learned via a successor-and-predecessor features
% algorithm; then we
% can use SFs on those features.
% 
% With the approach here combined with policy regularization, things stay
% tractable at whatever temperature and in non-deterministic environments
% (and with other priors, but I think the eigenfunctions argument can be
% extended to other priors too, probably get symmetrization wrt $D$ metric).
% 
% We might regularize and train $\phi$ with finite-temperature policies to
% get smoother features, then apply with $0$-temperature policies at test
% time. Still, even with finite temperature, we might get overspecialized
% skills, because the underlying loss is the same.
% 
% Alternatively, one may directly penalize the variance over $r$ in the
% loss (Section~\ref{sec:varianceregul}).
% 
% \item Regularize the features, eg by $-\norm{\phi}^4$ or by
% $\norm{\phi(s_t)-\phi(s_{t+1})}^2$ thus incorporating a bit of Laplacian
% feature loss. Already covered by the GFF prior? not sure if
% the priors actually regularize the features
% 
% \item Regularize the overall occupancy measure of the policies
% $\pi_z$ (as in entropy-based exploration). Given the gradient
% $\transp{\partial\phi}z$ on states visited by $\pi_z$, this is essentially
% VISR. TODO compare precisely with VISR, eg their normalizations, + Monte
% Carlo versus $b$ model of $M^{\pi_z}$.
% 
% \item Account for the observations made at test time. For instance, 
% let $\pi_z$ maximize not just $r_z$ but $r_z$ plus an exploration
% bonus corresponding to the Bayesian identification of the residual
% $r-r_z$. By one of my other texts, up to many approximations this bonus
% at $(s,a)$ is given by the matrix $\E_{z'}\E_{(s',a')\sim \pi_z}
% (A^{\pi_{z'}}(s',a')-A^{\pi_z}(s',a'))^2$ where $z'$ is the new value of
% $z$ after observing a new reward at a state. This can be estimated, I
% think. This would crudely account for effects of possible finetuning at
% test time: thus, we would not just be optimizing $r_z$ as if we were sure
% the reward is $r_z$.
% 
% Any test-time adaptation can be accounted for by MAML-like approach of
% backpropagating through mock adaptation phases.
% 
% But once more, this does not help in the ``known
% reward'' scenario.
% \end{enumerate}
% 
% Interestingly, the white noise prior is extremely agnostic (rewards on
% difference subspaces are uncorrelated, there is no prior on structure),
% ut we still get that some features are better than others thanks to the
% dynamics.
% 
% \paragraph{Does this depend too much on the prior?} Currently unclear.
% For reference, SFs rely on priors as well, as seen from
% Proposition~\ref{prop:posteriormean}: either a white noise prior, or a
% prior that rewards will be linear in the
% features with independent noise orthogonal to the features.
% 
% FB is less impacted by the prior. 
% Indeed, FB tries to
% minimize the $L^2$-error on the $Q$-functions of each $\pi_z$.
% Minimizing
% $\norm{\transp{F}_zB\rho-M^{\pi_z}}^2_\rho$ is equivalent to minimizing
% $\E_r \norm{\transp{F_z}B\rho r-Q^{\pi_z}_r}_{L^2(\rho)}$ with
% \emph{either} white
% noise $r$ or random goals (Dirac reward at a random target state).
% Indeed, in both cases the covariance matrix $\E[r\transp r]$ is the same,
% and this covariance matrix is the only way the prior on rewards
% influences $\E_r \norm{\transp{F_z}B\rho r-Q^{\pi_z}_r}_{L^2(\rho)}$. 
% 
% \paragraph{What is FB good for?} If the prior is known, the theory above does not particularly suggest
% to learn FB: overall it argues more for SFs with well-chosen features.
% 
% FB tries to minimize the $L^2$-error on the $Q$-functions of each $\pi_z$
% for random white noise rewards. This is very imperfect for several
% reasons: first, this optimizes every policy for every reward, not just
% the policy for the reward it's used for; second, the $L^2$ error on a
% $Q$-function does not directly translate into performance (there are
% bounds from the $L^\infty$ error on the $Q$ or advantage function, but
% these are only bounds). Using fb-FB will have the same fixed points as
% the above, while using ff-FB as we normally do will have different fixed
% points related to eigenfunctions of the averages of policies $\pi_z$, so
% probably corresponding to a Laplacian-style prior instead of white noise,
% which is probably good.  Still, the link to downstream performance is
% much less direct.
% 
% From this, it's not clear why the specific FB algorithm is interesting.
% Of course FB provides a one-in-all loss and the problem is solved if the
% loss is $0$. But one can say the same for SFs with the factorization of
% $P$. There are theoretical bounds for both, too (though I think all of
% them are trivial when taking a white noise prior or a sup over reward
% functions). Of course the FB approximation will usually be better than
% the P approximation, but that would require a prior on MDPs...
% 
% TODO check if the approach here works with reduced goal spaces. I think
% it does, but if not, this is a good argument for FB.
% 
% \section{Extensions}
% \subsection{Nonlinear Reward Representations}
% 
% All reward representations above were linear in $r$. This is limiting,
% and means that rewards are expanded in a basis of features that cannot
% depend on the task.
% 
% One way to get nonlinear
% reward representations is to iterate linear reward representations:
% \begin{equation}
% z_1=\E\, r(s)\phi_1(s),\qquad z_2=\E\,r(s) \phi_2(s,z_1),\qquad z_3=\ldots
% \end{equation}
% so that the features $\phi_2$ can be adjusted to the reward function via a rough
% reward summary $z_1$.
% 
% In general, assuming we have access to rewards by samples $(s_i,r_i)$, a
% reward representation
% should be any permutation-invariant function of those. All such
% permutation-invariant functions can be described by iterating parallel
% treatment of the samples, and taking averages (see our paper with
% Corentin, Thomas and Jakob on batch-gan a few years ago).
% 
% The $z_1,z_2\ldots$ example above is one particular case, because at each
% step is stays linear in $r$ conditioned to the previous steps.
% \footnote{Actually, $\phi_2(\E_s r(s)\phi_1(s))$ is already enough to
% represent any function of $r$. Proof for discrete spaces: let $\phi_1$ be
% the one-hot encoding, and let $\phi_2$ be a NN approximating the desired
% function of the reward as a list of reward values. For continuous spaces:
% Let $\phi_1$ be a partition of unity up to some approx, then apply the
% same argument, need some Lipschitz-type assumption to control errors.
% 
% With $z_2=\E r\phi_2$ I think we can represent all functions of $r$
% that are $0$ for $r=0$.
% 
% With $z_2=\phi_2(\E_s r(s)\phi_1(s))$ this just means we use $\phi_2$ as
% a preconditioning of $z_1$ for $Q$ and $\pi$, so this does not change the
% expressivity of the whole procedure, but allows $Q$ and $\pi$ to share
% a common nonlinear reward preprocessing.
% } I *think*
% the more
% general case is something like $z_1=\E\, \phi_1(r(s),s)$,
% $z_2=\E\,\phi_2(r(s),s,z_1)$ etc.
% For such nonlinear rewards, I don't know how to compute $r_z$ in general.
% 
% With the hierarchical linear model 
% \begin{equation}
% z_1=rD\phi_1,\qquad  z_2=r D\phi_2(\cdot,z_1),
% \end{equation}
% I think it works as above, by
% nesting the conditional expectations wrt $z_1$ and $z_2$. I think the
% posterior mean is
% \begin{equation}
% r_z=\transp{z}(\phi_{z_1}D \transp{\phi_{z_1}})^{-1}\phi_{z_1}
% \end{equation}
% (which certainly satisfies
% $\phi(r_z)=z$),
% where $\phi_{z_1}(s)\deq (\phi_1(s),\phi_2(s,z_1))$ is the concatenation of the
% two features. (Here it seems better to put the matrix inversion at the
% end, otherwise we'l get Schur residuals from any correlations between
% $\phi_1$ and $\phi_2$. TODO check distribution of $z$, looks like Schur residuals are
% coming anyway... Set $B'_2$ the projection of $B_2$ onto the orthogonal
% of $B_1$, $z'_2=...$ and things will be simpler)
% 
% Weirdly, it seems the entropy-regularized high-temperature regime cannot
% exploit such nonlinearities.  With the white noise prior, I find that if
% $\phi_1$ is aligned with the largest eigenfunctions, then $\phi_2$ should
% be the next largest eigenfunctions, whatever $z_1$, because this is the
% best bet for the residual... so in the large-regularization regime, there
% is no interest in having nonlinear features!?!?!? of course in this
% regime everything is based on a first-order expansion... try 2ndorder? or
% let go of this regime.
% 
% \subsection{Kernelizing}
% 
% TODO
% 
% \subsection{Alternatives: End-to-End NN Meta-RL on Reward Observations, and
% Learning to Reach States Like This}
% \label{sec:alt}
% 
% \paragraph{Baseline: classical meta-RL.}
% Start with a model of rewards given by a random NN. We can change the
% rewards eg by Langevin dynamics on all weights to explore the Glorot
% prior.
% 
% Train a reward representation: can be an RNN that processes observations
% $(s_i,r_i)$ on the fly, or a more structured permutation-invariant
% function as above, with layers $z_1=\E \phi_1(r_i,s_i)$, $z_2=\E
% \phi_2(r_i,s_i,z_1)$ etc where each $\phi_i$ can be a full NN or a single
% layer, and where $\E$ is a moving average or on \emph{very large} minibatches
% (must be large enough to provide a good idea of the reward).
% 
% This reward representation produces a vector $z$; train policies $\pi_z$
% based on currently observed rewards.
% 
% Slowly change the reward by changing the weights of the NN, and hope there
% is no catastrophic forgetting. Or work on large batches, setting a reward
% function then generating the weights $(r_i,s_i)$ then computing
% $z=\Phi(r_i,s_i)$ and $Q(s,z)$ from this batch, with gradients on
% everything.
% 
% This is arguably simpler than the above, and allows for any kind of
% nonlinear reward representation. May be too restricted to the particular
% reward class, though. The first approach above algebraically integrates
% over all possible reward functions.
% 
% But this will handle sparse rewards poorly. Especially in the online or
% RNN versions, the agents will try to identify the reward by exploring:
% this is good in other scenarios but not what we want in a known reward
% scenario. Goal-reaching will not work. On the other hand, the algebraic
% approach uses the whole dataset for reward representation.
% 
% \paragraph{Reach states like this: representing rewards by sampled
% states.}
% TODO clarify: reward represented by batch of values. Sample a reward,
% sample a batch, compute z, optimize policy z for that reward.
% 
% 
% Much better reward representation: instead of providing $(r_i,s_i)$,
% provide a set of states $(s_i)$ with frequency proportional to $r_i$
% (assuming positive rewards; this can be done by resampling from
% $(r_i,s_i)$ samples). Apply the same approach using
% minibatches; would probably work with much smaller batches. Goal-reaching
% becomes trivial: provide a batch made entirely of copies of the goal.
% Still need a prior on rewards, can be random NN or can be one of the
% priors above; MCMC or Langevin can be run in parallel to optimization to
% sample rewards over time.
%  \footnote{eg using Langevin on the trainset with GFF prior just means
%  maintaining a reward $r_i$ for each state in the dataset, and iterating:
%  sample a transition $(s_i,s'_i)$ from the dataset, add independent noise
%  $\sqrt{\eta} N(0,1)$ to $r_i$ and $r'_i$, and then push $r_i$ and $r'_i$
%  towards each other
%  by a factor $(1-\eta)$; $\eta$ is the learning rate. TODO CHECK, added
%  noise from sampling the states}
% 
% Effectively, such an agent learns to ``reach states like this''.
% 
% So: Sample a function $r$, sample states $s_i$, 
% re-sample the minibatch proportionally
% to $r_i$ (need to split positive and negative samples), compute
% representation $z$ from resampled batch, train $Q_z$ and $\pi_z$ on a
% non-resampled batch (we need to learn $Q$ everywhere, not only on the
% good points), backprop through the representation to maximize $V_z(s_0)$ for
% $s_0\sim \rho_0$ (probably use duelling networks to not overestimate
% upward fluctuations of $V_z$).
% 
% Loss is
% \begin{equation}
% -\E_{r\sim \beta} \E_{\{s_i\}\propto r(s_i)} \E_{s_0\sim \rho_0}
% V_r^{\pi_{\Phi(\{s_i\})}}(s_0)
% \end{equation}
% where $\E_{\{s_i\}\propto r(s_i)}$ means sampling a set of states with probability
% proportional to $r$ (more exactly $r(s)\rho(s)$), and $\Phi(\{s_i\})$ is
% the representation of these states. This can be differentiated with
% respect to $\phi$.
% 
% Might be problematic to have to synthesize full-universe states to
% specify a reward; see if we can work in a goal space.
% 
% TODO write loss, it's the same loss as the main loss above.
% 
% \bigskip
% 
% Overall, a fully zero-shot system should be able to specify rewards
% (including setting a goal space on the fly) in a user-friendly way.
% 
% 
% 
% \section{Discussion 2: Are we Just Learning a Finite Number of Policies?}
% 
% Let's consider the controllable case, and let's take a large $\gamma$.
% Then I think the optimal $\phi$ for white noise and GFF priors is: choose
% $d+1$ states $s_0,\ldots, s_d$. Set $\phi_i=\1_{s_i}-\1_{s_0}$.
% Namely, (up to the covariance matrix) $z$ is the set of reward
% differences $r(s_i)-r(s_0)$. Then $\pi_z$ is just going to the state
% $s_i$ with max reward among these.
% 
% Simulation of GFF on a 2d grid shows this is clearly superior to taking
% $\phi$ the main eigenfunctions (with the same $d$). (In 1d it's very
% close, and in higher dim I expect things get worse as local variance of
% the GFF increases even more.)
% 
% I don't think this is a trick with the $\gamma\to 1$ limit.  I think it's
% a real property of these priors: white noise or GFF describe functions
% with large reward variations everywhere (the pointwise reward value is
% undefined in continuous spaces), so a good policy exploits these
% variations by measuring various values of the reward. Even with the GFF,
% and even if the $s_i$ are close to each other (thus the $r(s_i)$ are
% correlated), the variations are much larger than the correlations and we
% can exploit those. With WN and GFF, the SNR is too small and policies
% just exploit the variance.
% 
% In general, the "fully controllable, $\gamma\to 1$" setting means we
% basicaly have a continuous bandit on the state space, which clearly shows
% why a prior is fundamental. 
% 
% But I don't think this depends much on controllability. In general, I
% expect the optimal policies to identify a finite number of "controllable
% trajectories" on which the agent can stay, and select the trajectory with
% the highest reward. Thus we would still learn only $d+1$ policies.
% 
% So even with smaller $\gamma$ and non-fully
% controllable, I think the same happens on a set of "minimally
% controllable trajectories". Those are basically the possible occupation
% measures of policies: so similarly to Eysenbach et al, we risk ending up
% on just a number of extremal points on the set of realisable occupation
% measures. This is very much due to the SNR of these priors.
% 
% \bigskip
% 
% The situation is very different for sparse rewards. Then $\phi$ must give
% a summary of where the states with good rewards lie, and $\phi$ has to be
% continuous.
% 
% If
% we take the random sparse reward model above, we just have to be able to
% represent $k$ random points in space, which can be done with $k\dim(S)$
% features. But then we're basically back to representing the
% reward as a $t$-uple $(r_i,s_i)$, so this is just multitask RL with a
% parametric family... Perhaps there's interest in mixing both sparse and
% white noise rewards (seeing that 
% white noise is the limit $k\to\infty$, this is natural: use a mixture of
% $k$ including $k=\infty$).
% 
% 
% But this prior still represents "structure-free" reward functions. For
% those, I'm not sure we can do much better than try to build some kind of
% $\eps$-covering of the space, which does not scale well. Perhaps the
% hirerarchical nonlinear representations do induce some kind of
% hierarchical prior on rewards...
% 
% \bigskip
% 
% Overall, this optimal $\phi$ approach seems to be very dependent on a
% choice of priors, perhaps more than FB which is more like a model of the
% world.
% 
% \bigskip
% 
% FB also has fixed points of the form $B_i=\1_{s_i}$ among $d$ fixed
% states $s_1,\ldots,s_d$, yielding $z=(r(s_1),\ldots,r(s_d))$ and then
% $\pi_z$ goes to the state $s_i$ with max reward among those. \footnote{We
% cannot subtract $s_0$ because $B$ still has to describe the sucessor
% measure, which does not have a $-\1_{s_0}$ component.} For
% $\gamma=1$ this is an exact fixed point; \footnote{It's not only a fixed
% point of FB training (local minimum of the loss) but an exact solution of
% the FB loss (loss is $0$). This
% contradicts our theorem so here there's clearly some fishy stuff with the
% $\gamma\to 1$ limit; actually for this limit we multiply loss by
% $(1-\gamma)$; real loss is $O(1-\gamma)$ so no contradiction.} for large $\gamma<1$, I think there's a
% corresponding fixed point close to that, but I don't know if it is
% attractive (but I think it is).
% 
% So \textbf{FB \emph{does} have a problem of a risk of collapse on just $d$
% distinct policies.}
% Possible solutions: 
% \begin{enumerate}
% \item 
% Relative TD,
% \footnote{For
% $Q$-learning, relative TD uses the Bellman equation $Q=R+\gamma PQ-\gamma
% Q(s_0,a_0)$ with a reference state $s_0$. This computes the $Q$-function up to
% a constant, thus preserves advantages and policies. It works when $\gamma=1$ if unichain. For FB this would be
% similar, removing $\transp{F(s_0,a_0,z)}B(s')$. Namely, in the FB loss,
% replace the target $\bar F(s,a,z)$ with $\bar F(s,a,z)-\bar F(s_0,a_0,z)$.
% 
% While relative TD on
% $Q$ is neutral in principle, relative TD on FB changes the notion of
% low-rank approximation: the same way that an FB model on advantage
% functions is not equivalent to an FB model on $Q$-functions, here we
% would do the finite-rank approximation on the transient behavior only.
% This transient behavior contains all the info needed in FB to compute the
% policies.
% 
% But with optimal-$\phi$-learning this is different, I don't
% really understand why. I suspect this is because FB might have an implicit step
% of policy improvement... in FB we can choose to model only advantages,
% why not with optimal $\phi$-learning?
% 
% I think FB models the way the estimated Q-funcs are used in policy
% learning (only advantages matter), while optimal $\phi$-learning  does
% not. But still the loss on $\phi$ and $\pi$ seems right...
% } which seems to completely remove these fixed points in an elegant way.
% 
% \item Hierarchical FB. The problem above is, we have a large rank-one term that
% depends on $z$, and we use $B$ to model the different values of this
% rank-1 term when $z$ changes.
% But $B_2$ can easily take care of a rank-one term that
% depends on $z$, even if $d_2=1$.
% 
% \item Regularize policies, but might just put some blurring around the
% fixd point above.
% \end{enumerate}
% 
% \bigskip
% 
% Since all of the above uses backward Bellman to learn $b$, I don't think
% it carries so easily to goal spaces: one would have to learn $b$ on the
% full state space, then learn its projection onto goals.
% 
% \bigskip
% 
% An algorithm in between FB, diversity,  and optimal $\phi$-learning: in FB,
% only learn $f(z)=\E_{s_0,a_0\sim \rho_0}F(s_0,a_0,z)$ instead of $F$. Indeed,
% $\transp{f(z)}B$ is the occupation measure of policy $\pi_z$ starting at
% $\rho_0$, and this occupation measure satisfies a backward Bellman
% equation. This backward Bellman equation can be used to train both $f$ and $B$. Then we can just
% train the $Q$-functions of $\transp{z}B$ (either by learning $Q(s,a,z)$
% or by learning SFs on $B$ with the $Q$-style loss). Not sure what this
% means: we are trying to learn some kind of a set of policies with
% mutually distinct occupation measures/trying to make a small-rank model
% of the occupation measures of the policies we have.
% 
% % \section{Worst-Case Loss and Adversarial Formulation}
% % 
% % Without priors, we can reformulate the problem as a worst-case loss
% % against reward functions. The class of reward functions considered plays
% % the role of prior in this case.
% % 
% % We are going to write things with rewards
% % $r$ such that $\norm{r}\leq 1$ for an arbitrary norm $r$, a typical case
% % being $\norm{r}_{L^2(\rho)}\leq 1$. (This puts more importance on
% % states not well represented in $\rho$. I don't know if this is a good or
% % bad thing: optimization could blow up, but it might help exploration.)
% % Other examples could be a Lipschitz norm for spatial continuity priors.
% % 
% % Let us assume we just work with SFs. So for each reward we just compute
% % $z_r=(\Cov \phi)^{-1}\E [r\phi]$ and apply $\pi_z$. For simplicitity I'm assuming we can
% % compute the SFs $\psi$ given $\phi$ (we have plenty of methods) so I'm
% % omitting explicit
% % optimization on $\psi$.
% % 
% % The problem can be written as
% % \begin{equation}
% % \argmin_\phi \sup_{r,\,\norm{r}\leq 1} \E_{s_0}
% % \abs{V^\star_r(s_0)-V^{\pi_{z_r}}_r(s_0)}
% % \end{equation}
% % which can be bounded (Cauchy--Schwartz) by
% % \begin{equation}
% % \argmin_\phi \sup_{r,\,\norm{r}\leq 1} \E_{s_0}
% % \left(V^\star_r(s_0)-V^{\pi_{z_r}}_r(s_0)\right)^2
% % \end{equation}
% % which is also a valid standalone objective that emphasizes variance of
% % the regret.
% % 
% % 
% % We could optimize this by learning both $V^\star_r$ and $V^{\pi_z}_r$:
% % \begin{equation}
% % \argmin_\phi \sup_{r,V^\star,z,V^z: \norm{r}= 1,
% % V^\star=V^\star_r, z=z_r,V^z=V^{\pi_z}_r}
% % \norm{V^\star-V^z}^2_{L^2(\rho_0)}
% % \end{equation}
% % which could be implemented with a bunch of auxiliary losses and Lagrange
% % relaxations:
% % \begin{multline}
% % \argmin_\phi \sup_r
% % \inf_{Q^\star,Q^z,z}
% % \norm{V^\star-V^z}^2
% % \\+\lambda\left(
% % \norm{Q^\star-r-\gamma P^{\pi^\star}Q^\star}^2_{BG}
% % -(\norm{r}^2-1)^2
% % +\norm{Q^z-\transp{\phi}z-\gamma P^{\pi_z} Q^z}^2_{BG}
% % +\norm{(\Cov \phi)z-\E_\rho r\phi}^2
% % \right)
% % \end{multline}
% % where $\pi$ and $V$ are defined via the corresponding $Q$.
% % To be optimized fast over $Q^z$, $Q^\star$ and $z$, slow over $r$, and
% % even slower over $\phi$.
% % 
% % I'm not sure we can make this work in a stable way. The main signal for
% % $r$ seems to be to increase Bellman gaps?
% % 
% % \TODO{} we can probably parameterize $r$ by its $Q$-function $Q^\star$, in
% % which case we should \emph{maximize} over $Q^\star$.
% % Then the Bellman gap of $Q^\star$ is $0$ by definition but the
% % constraint on $\norm{r}$ becomes a constraint on $\norm{Q^\star-\gamma
% % P^{\pi^\star}Q^\star}$, but there is a double sampling probleme here. We
% % can probably ignore this Bellman gap double sampling, which amounts to
% % overpenalize the rewards that have variance in the value of
% % $s_{t+1}$ (though we have less protection against such rewards).
% % 
% % Also there's going to be instability of $z$ because $r$ is move towards
% % the orthogonal of $\phi$, so $z$ is going to be close to $0$ and $\pi_z$
% % might be unstable. Probably use Boltzmann or entropy-regularized policies
% % for $\pi_z$.
% % 
% % \bigskip
% % 
% % Let's take a fresh start. What I can do is minimize the difference
% % between the true $Q$-function for $\pi_z$ and the estimated $Q$-function
% % for $\pi_z$ (which is $0$ on the orthogonal of $B$). This bounds the
% % optimality gap by one of the standard bounds.
% %  [TODO
% % but I think this bound is trivial in many cases]
% % 
% % So let's consider instead
% % \begin{equation}
% % \argmin_\phi \sup_{r,\,\norm{r}\leq 1} \E_{s_0} \left(
% % Q^{\pi_{z_r}}_r(s_0)-\hat Q^{\pi_{z_r}}_r(s_0)
% % \right)^2
% % \end{equation}
% % where $\hat Q$ is the estimated $Q$-function given by the SF model.
% % 
% % Each reward $r$ with $\norm{r}=1$ can be written as $r=cr_\phi+s\bar r$
% % where $r_\phi$ is in the span of $\phi$ and $\bar r$ is orthogonal to
% % $\phi$, both have unit norm, and $c^2+s^2=1$. Then the difference above is
% % $sQ^{\pi_{z_r}}_{\bar r}(s_0)$.
% % 
% % So
% % this is minimal when $r$ is almost orthogonal to $\phi$, with
% % a tiny dose of the worst possible $z$ thrown in. So we get
% % \begin{equation}
% % \argmin_\phi \sup_z \sup_{r,\,\norm{r}\leq 1,\,r\bot\phi} \E_{s_0}
% % Q^{\pi_z}_r(s_0)^2
% % \end{equation}
% % because the estimated $Q$-function is $0$ on the orthogonal of $B$, 
% % or
% % \begin{equation}
% % \argmin_\phi \sup_z \sup_{r,\,\norm{r}\leq 1,\,r\bot\phi} \norm{
% % M^{\pi_z}r}^2_{L^2(\rho_0)}
% % \end{equation}
% % which can be rewritten as an unconstrained sup over $r$ if we introduce
% % the orthogonal projector onto $\phi$,
% % \begin{equation}
% % \argmin_\phi \sup_z \sup_{r,\,\norm{r}\leq 1} \norm{
% % M^{\pi_z}(\Id-\Pi_\phi) r}^2_{L^2(\rho_0)}
% % \end{equation}
% % (this should be the orthogonal projector for the norm $\norm{\cdot}$ we are
% % using, not always $L^2(\rho)$ if we use other norms...). This is equal to
% % \begin{equation}
% % \argmin_\phi \sup_z \norm{
% % M^{\pi_z}(\Id-\Pi_\phi)}^2_{\norm{\cdot}\to L^2(\rho_0)}
% % \end{equation}
% % using the operator norm of $M^{\pi_z}(\Id-\Pi_\Phi)$.
% % 
% % On the span of $\phi$, the SF model is correct:
% % $M^{\pi_z}\Pi_\Phi=\transp{F}\phi\rho$ for the appropriate $F$. (This
% % also assumes $\norm{\cdot}$ is $L^2(\rho)$, so that the projector is obtained by
% % taking the expectation $\E[r\phi]$.) So in that case this rewrites as
% % \begin{equation}
% % \argmin_\phi \sup_z \norm{
% % M^{\pi_z}-\transp{F}\phi\rho}^2_{L^2(\rho)\to L^2(\rho_0)}
% % \end{equation}
% % 
% % \bigskip
% % 
% % The operator norm is unpractical. I see two ways to tackle it:
% % adversarial over $r$, or using a slightly different norm.
% % 
% % In FB, we use a slightly different norm: a Hilbert--Schmidt norm, $\norm{M}_{L^2(\rho_0)\otimes
% % L^2(\rho)^*}$: average over $s_0\sim \rho$ and dual over $L^2(\rho)$.
% % This is actually a bound on the operator norm
% % $\norm{M}_{L^2(\rho)\to L^2(\rho_0)}$. (TODO WRITE: same argument as in
% % the imitation doc).
% % 
% % Moreover, we take expectations over $z$.
% % 
% % \bigskip
% % 
% % Alternatively, we might take an adversarial viewpoint on $r$.
% 
% % \newpage
% % \appendix
% % \section{Early version}
% % 
% % 
% % What we want in zero-shot RL is: Given a reward $r$, compute a task
% % representation $z=B(r)$ in a simple, fast way, then apply an existing
% % policy $\pi_z$. This policy should have performance as high as
% % possible for $r$.
% % \footnote{
% % There is no meaningful notion of optimal features $B$ without specifying
% % a constraint or functional space. E.g., even with a 1-dim $B$, one can
% % get perfect features by using bijections from $\R^n$ to $\R$ eg defining a 1d Peano curve that is dense in the
% % space of reward functions, and mapping each reward to the closest point
% % on that curve, thus encompassing all reward info within a single real
% % number. But this misses the requirement of ``easy to compute from samples''.
% % }
% % 
% % If we have a prior $\beta$ on rewards, we can formalize this by asking:
% % Find the best reward representation $r\mapsto z=B(r)$ among easy-to-compute
% % ones, and the best policies $\pi_z$, so that return is maximal on average
% % for rewards $r\sim \beta$:
% % \begin{equation}
% % \sup_{B,\,(\pi_z)} \E_{r\sim \beta}\, \E_{s_0\sim \rho}\,
% % V^{\pi_{B(r)}}_r(s_0)
% % \end{equation}
% % (though we should also care about variance...)
% % 
% % Let $\beta_z$ be the pushforward of $\beta$ by the map $r\mapsto B(r)$.
% % (It depends on $B$.) The above rewrites as
% % \begin{align}
% % \cdots
% % &=\sup_{B,\,(\pi_z)} \E_{z\sim \beta_z} \,\E_{r|B(r)=z}\,\E_{s_0\sim \rho}\,
% % V^{\pi_z}_r(s_0)
% % \\&=\sup_{B,\,(\pi_z)} \E_{z\sim \beta_z}\, \E_{r|B(r)=z}\,\E_{s_0\sim \rho}\,
% % M^{\pi_z}(s_0) r
% % \\&=\sup_{B,\,(\pi_z)} \E_{z\sim \beta_z}\, \E_{s_0\sim \rho}\,
% % M^{\pi_z}(s_0)\E_{r|B(r)=z}\, r
% % \\&=\sup_{B,\,(\pi_z)} \E_{z\sim \beta_z}\, \E_{s_0\sim \rho}
% % \,V^{\pi_z}_{r_z}(s_0)
% % \end{align}
% % where
% % \begin{equation}
% % r_z\deq \E_{r|B(r)=z} \,r
% % \end{equation}
% % is the average reward function conditioned to have representation $z$. Here we
% % have used that $Q$-functions are linear in $r$ for a given policy. All
% % rewards represented by $z$ will be used with $\pi_z$, and so the
% % average return over rewards is the return of the average reward among
% % those represented by $z$.
% % 
% % It follows that the optimum above is attained for
% % \begin{equation}
% % \pi_z=\pi^\star_{r_z}
% % \end{equation}
% % the optimal policy for reward $r_z$, and the representation problem above
% % is
% % \begin{equation}
% % \sup_B \E_{z\sim \beta_z} \,\E_{s_0\sim \rho}\, V^\star_{r_z}(s_0)
% % \end{equation}
% % 
% % Remark: In fully controllable environments (where you can reach any state
% % and stay there), every optimal policy (for $\gamma=1$) is of the type
% % ``go to the state with max reward and stay there''. \footnote{Note that
% % Mujoco-style environments are not fully controllable, because the state
% % is (position,speed), and it is not possible to reach an arbitrary such
% % pair $(x,v)$ and stay there without moving, unless $v=0$.} From the above, the
% % optimal zero-shot behavior is to go to the max of the posterior mean
% % reward. These policies will be too sharp in practice. This is because our
% % setting does not allow for any kind of exploration, adaptation or fine-tuning at test
% % time, so the best we can do is make a bet on the location of the max
% % reward. This shows the limitations of our zero-shot setting.
% % 
% % Still, the above is fully justified in the case of known reward
% % functions, where computing $z$ and applying $\pi_z$ is done for
% % computational convenience rather than uncertainty.
% % 
% % \paragraph{Two priors on rewards.} Here I will consider two natural
% % priors on rewards. The first is a white noise prior with reference
% % measure $\rho$. On a discrete space, $r_s$ is a centered Gaussian of
% % variance $1/\rho(s)$. This scaling provides a meaningful limit on
% % continuous spaces: white noise, a natural and very agnostic random
% % function model on any measured space. (The $1/\rho$ scaling also means
% % there is more uncertainty on less visited states.)
% % 
% % The second prior is random goal-reaching, with rewards a Dirac measure at
% % a random goal $s\sim \rho$. (As a function, $r=\delta_{s=g}/\rho(s)$
% % represented as a density wrt $\rho$.) Meaningful continuous limit, etc.
% % 
% % \paragraph{Linear reward representations.} The easiest-to-compute
% % reward representations are the linear ones, given by features $B$,
% % \begin{equation}
% % z=\E_{s\sim \rho} \,r(s)B(s)
% % \end{equation}
% % as we do in FB, and in SFs with $B=(\Cov \phi)^{-1}\phi$.
% % 
% % By the above, the best we can do once we have features $B$, is
% % to compute the optimal policies for rewards $r_z$, e.g.\ via $Q$-learning
% % for each $z$ or via an SF-like loss.
% % 
% % 
% % \begin{prop}
% % In the white noise reward model above, one has
% % \begin{equation}
% % r_z(s)=\transp{z}(\Cov B)^{-1}B(s).
% % \end{equation}
% % For SFs this is $r_z(s)=\transp{z}\phi(s)$.
% % 
% % Therefore, once we know $B$, the best thing to do is to compute the
% % $Q$-functions and optimal policies of these rewards.
% % 
% % This also holds for any Gaussian model of rewards such that the
% % components along $B$ and its orthogonal are independent, namely,
% % $r(s)=\transp{\theta_1}B(s)+\transp{\theta_2}\xi(s)$ where $\xi$ are
% % any features such that $\E_{s\sim \rho}[B(s)\xi(s)]=0$ and
% % $\theta_1$, $\theta_2$ are independent Gaussian vectors with any
% % covariance matrix.
% % \end{prop}
% % 
% % Also, in the white noise model, the distribution $\beta_z$ of $z$ is
% % Gaussian with covariance $\Cov B$ (so $(\Cov \phi)^{-1}$ for SFs), and
% % $(\Cov B)(\Cov \theta_1)\Cov B$ for the second model.
% % 
% % So the SF loss with $\transp{\phi}z$ does
% % correspond to many priors.
% % This justifies the SF $Q$-type loss (either with $Q(s,a,z)$ or with
% % $\transp{F(s,a,z)}z$: choice unclear). For FB, it strongly suggests to
% % train a $Q$-func model on top of $FB$ rather than directly use $F$, even with the
% % auxiliary loss $\mathcal{L}'$.
% % 
% % Note: if we know that rewards are Gaussian with covariance matrix
% % $C_{ss'}$, eg, $C=\transp{\chi}\chi$ which means rewards are linear
% % combinations of $\chi$ with Gaussian coeffs, then the best is to do SFs
% % on the features $\chi$, which provides the exact solution. But this does
% % \emph{not} mean that given $C$, the best $d$-dim features for
% % $d<\mathrm{rank}(C)$ are the truncated SVD of $C$. (When $C=\rho^{-1}$
% % and $d=1$ this does not seem to be the case by the example below; in
% % general, the dynamics must play a role, but the SVD of $C$ does not
% % depend on $P$.)
% % 
% % The proposition is \emph{not} 
% % true in the random goal model of random
% % rewards. For random goals, $r_z$ is something else. For instance, in dim
% % $1$ with sine and cosine features, the map from a goal to the reward
% % representation is \emph{injective}: for each $z$, there is only one
% % reward function in this family such that $B(r)=z$. This will happen more
% % generally (typically, as soon as the representation dimension is larger
% % than the state space dimension). In such situations, we should just
% % compute the optimal policy for reaching the goal. This is not realistic
% % because first, we cannot assume the only tasks are goal-reaching, and,
% % second, it's not easy to reverse-engineer the goal. The ``right'' way to
% % work with the goal-reaching prior is just to learn goal-reaching, namely,
% % the reward representation is just the goal $g$, and we just train
% % goal-conditioned $Q$-functions.
% % 
% % \begin{proof}
% % With a prior where $r$ is Gaussian with covariance matrix
% % $C_{ss'}$, in matrix notation, with feature matrix $B_{is}$ one has
% % $r_z=\argmin\{\transp{r}C^{-1}r\mid\E_\rho rB=z\}=C\rho \transp{B}\left(B\rho C
% % \rho \transp{B}\right)^{-1}z$, or with SFs $\phi$, $r_z=C\rho
% % \transp{\phi} \left(\phi\rho C \rho \transp{\phi}\right)^{-1}
% % \Cov(\phi)z$. White noise corresponds to $C=\rho^{-1}$. The case of
% % independent components along $B$ and $\xi$ is 
% % $C=\transp{B}C_1B+\transp{\xi}C_2\xi$ with $\xi\rho \transp{B}=0$ which proves the theorem.
% % \end{proof}
% % 
% % \paragraph{The best linear features, and why the loss above is wrong.}
% % Surprisingly, for linear features, the loss above is fully tractable.
% % However, it leads to uninteresting, extremely specialized skills. We have
% % to include some form of regularization, e.g., account for the variance of
% % the performance over rewards or add entropy regularization.
% % 
% % Let $\phi$ be the features. WLOG we can assume that we optimize only
% % among features with $\Cov \phi=\Id$ (eg in practice, by adding a strong
% % auxiliary loss for $\Cov(\phi)-\Id$ \footnote{For the general case the
% % loss is $\E_{z\sim N(0,\Id)} \E_{s_0} M^{\pi_z}(s_0)\transp{\phi}(\Cov
% % \phi)^{-1/2}z$ and the matrix square root is a pain. I think the best
% % strategy is to assume that $\Cov(\phi)=\Id$, use that the gradient
% % restricted to $\Cov\phi=\Id$ is the projection of the unrestricted
% % gradient onto $\Cov\Phi=\Id$, add an auxiliary loss on $\Cov\Phi$ to
% % ensure we stay reasonably close, and explicitly reproject $\phi$ once in
% % a while by explicitly resetting the last layer's weights by $(\Cov
% % \phi)^{-1/2}$. This way we don't have to compute gradients of $(\Cov
% % \phi)^{-1/2}$.}).
% % The loss above is
% % \begin{equation}
% % \sup_{\phi, (\pi_z)} \E_{z\sim \beta_z} \,\E_{s_0\sim \rho}\,
% % V^{\pi_z}_{r_z}(s_0)
% % \end{equation}
% % and by the proposition above, $\beta_z=N(0,\Id)$ and
% % $r_z=\transp{\phi}z$. Therefore, the loss is
% % \begin{equation}
% % \label{eq:philoss}
% % \ell(\phi,(\pi_z)) =\E_{z\sim N(0,\Id)} \,\E_{s_0\sim \rho}\,
% % V^{\pi_z}_{\transp{\phi}z}(s_0).
% % \end{equation}
% % Optimizing $\pi_z$ is done as usual in RL, by learning each policy
% % $\pi_z$ for reward $\transp{\phi}z$ for each $z$.
% % Computing the gradient wrt $\phi$ is easy as well, using that
% % $V^{\pi_z}_{\transp{\phi}z}=M^{\pi_z} \transp{\phi}z$, and therefore
% % \begin{equation}
% % \label{eq:philoss}
% % \ell(\phi,(\pi_z)) =\E_{z\sim N(0,\Id)} \,\E_{s_0\sim \rho}\,
% % M^{\pi_z}(s_0) \transp{\phi}z
% % \end{equation}
% % and
% % the
% % gradient wrt $\phi$ is
% % \begin{equation}
% % \partial_\phi \ell(\phi,z)=\E_{z\sim N(0,\Id)} \,\E_{s_0\sim \rho}\,
% % M^{\pi_z}(s_0)\,
% % \partial \transp{\phi}z
% % \end{equation}
% % and on-policy, we can estimate $M^{\pi_z}(s_0)
% % \partial \transp{\phi}z$ by launching a trajectory $\pi_z$ and averaging
% % $\partial \transp{\phi}z$ along the way, or (off-policy or to reduce
% % variance) by training an auxiliary model of the average occupation
% % measure of $\pi_z$,
% % $\int_{s_0}\rho(\d s_0)M^{\pi_z}$, 
% % which satisfies a backward Bellman equation, as a function of $z$.
% % \footnote{If we train $\phi$ with the loss above on the set of normalized
% % $\phi$, based on a model
% % $M^{\pi_z}\approx \transp{F_z}B\rho$, we will get that the span of $\phi$
% % is the span of $B$, and so the results will be the same as taking
% % $\phi=B$. This is because with this approximation for $M^\pi$, the loss
% % for $\phi$ is invisible in the directions orthogonal to $B$, so given the
% % normalization, it's better to align $\phi$ with $B$. So if we approximate
% % $M^\pi$ this way, $\phi$ is useless and we should set $\phi=B$ and use
% % the loss on $\phi$ as another loss on $B$, $\ell_2(B)=\E_z \E_{s_0}
% % M^{\pi_z}\transp{B}z$ with $M^{\pi_z}=\transp{\bar F_z}\bar B\rho$ (using
% % target versions).
% % 
% % However, we have no reason to do things this way. Starting with the loss
% % of $\phi$, we might approximate $M^{\pi_z}$ better with a model
% % $\transp{F_z}B_z$, and then we don't find $\phi=B_z$. Actually we only
% % have to approximate $\E_z \E_{s_0} M^{\pi_z}$ so we only need to
% % approximate a single function $b'(s,a,z)$ which satisfies a backward
% % Bellman equation.
% % }
% % 
% % Conceptually, gradient descent of $\ell$ for $\pi_z$ and $\phi$ amounts
% % to:
% % \begin{itemize}
% % \item Learn $\pi_z$ to optimize reward $\transp{\phi}z$ for each $z$;
% % \item Learn $\phi$ by increasing $\transp{\phi}z$ at the states visited
% % by $\pi_z$.
% % \end{itemize}
% % 
% % Have we fully solved the problem? yes and no, because the fixed points
% % can be poor for zero-shot RL. In general, this loss will jump to the most-controllable skills in the
% % environment, which can be quite narrow.
% % 
% % The above has a ``rich-get-richer''
% % dynamics. Assuming we can reach arbitrary states and stay there, for $\gamma\approx 1$, and $d=1$, I think the fixed points are as follows:
% % $\phi$ has one state $s_1$ with very high value and one state $s_2$ with
% % very large negative
% % value, all other states have values $O(1-\gamma)$. What this policy does: it
% % compares the rewards at $s_1$ and $s_2$, and goes to the state with the
% % highest reward; not an unreasonable behavior with a white noise prior and
% % only one feature.
% % This is stable:
% % with such $\phi$, the optimal policy $\pi_z$ spends all its time on $s_1$
% % for positive $z$ and on $s_2$ for negative $z$, and so the values there get
% % reinforced. $s_1$ and $s_2$ may be anything, due to initialization
% % amplified by the rich-get-richer dynamics, but the optimal 
% % choice is when $s_1$ and $s_2$ are the states with smallest $\rho$,
% % because the white noise has highest variance. This is a bit weird
% % offline, but online this will induce exploration.
% % (In a continuous space with only imperfect control,
% % the equivalent would be the smallest zone which we can reliably reach,
% % because the variance of white noise in that zone will be high.)
% % 
% % Therefore, this loss might be a poor choice, although it does look like the most
% % natural loss... I see some ways out:
% % 
% % \begin{enumerate}
% % 
% % \item Admit the features above are the right choice given the white noise
% % prior. Then work with different priors instead of white
% % noise... If the reward at one point is independent from the rewards at any
% % other point, then observing the reward at one point is as good a feature
% % as any other, and by observing rewards at two points we can go to the one
% % with the highest reward. 
% % 
% % But if we think the white noise prior is wrong, there's no reason
% % to train SFs with the loss $\transp{\phi}z$...
% % 
% % \item Accounting for variance: instead of optimizing the expectation over
% % rewards, optimize $(\E_r[\cdots])^2-\lambda \Var_r [\cdots]$. Not sure
% % this stays tractable, because we lose the equivalence with optimizing
% % $r_z$. (We seem to get policy optimization under the constraint of
% % minimizing the norm of the $Q$-function and the successor measure...)
% % 
% % \item Accounting for variance at test time from $z$ estimation: the
% % narrower $\phi$, the harder it will be to reliably estimate $z$ at test
% % time. Better, accounting for any possible fine-tuning at test time.
% % Still, in the fully-known reward setting, this changes nothing.
% % 
% % \item Regularize the policies, e.g. by entropy regularization. In this
% % regime, for large temperatures I know exactly what the optimal features
% % are (for deterministic environments). Things stay tractable at whatever
% % temperature and even for stochastic environments. We might train $\phi$
% % with finite-temperature policies to get smoother features, then apply with $0$-temperature
% % policies at test
% % time. Still, even with finite
% % temperature, we might get overspecialized skills, because the underlying
% % loss is the same.
% % 
% % \item regularize the features, eg by $-\norm{\phi}^4$? may be related to
% % variance of $z_r$ estimator at test time. Or by
% % $\norm{\phi(s_t)-\phi(s_{t+1})}^2$ thus incorporating a bit of Laplacian
% % feature loss. Can we interpret that as a modified prior on $r$?
% % 
% % \item Somehow regularize the overall occupancy measure of the policies
% % $\pi_z$ (looks like entropy-based exploration). Given the gradient
% % $\transp{\partial\phi}z$ on states visited by $\pi_z$, this is essentially
% % VISR. TODO CHECK
% % 
% % \item Let $\pi_z$ maximize not just $r_z$ but $r_z$ plus an exploration
% % bonus corresponding to the Bayesian identification of the residual
% % $r-r_z$. By one of my other texts, up to many approximations this bonus
% % at $(s,a)$ is given by the matrix $\E_{z'}\E_{(s',a')\sim \pi_z}
% % (A^{\pi_{z'}}(s',a')-A^{\pi_z}(s',a'))^2$ where $z'$ is the new value of
% % $z$ after observing a new reward at a state. This can be estimated, I
% % think. This would crudely account for effects of possible finetuning at
% % test time: thus, we would not just be optimizing $r_z$ as if we were sure
% % the reward is $r_z$. Once more, this does not help in the ``known
% % reward'' scenario.
% % \end{enumerate}
% % 
% % With strong entropy regularization wrt
% % the exploration policy, my other result states that the optimal $B$ is
% % the eigenfunctions of the symmetrized inverse Laplacian, which we can
% % learn via an FB-style algorithm. Then we should use SFs on top of that.
% % This provides the optimal solution to the zero-shot problem with strong
% % entropy regularization and white noise prior on rewards.
% % 
% % Interestingly, the white noise prior is extremely agnostic (rewards on
% % difference subspaces are uncorrelated, there is no prior on structure),
% % ut we still get that some features are better than others.
% % 
% % \paragraph{What is FB good for?} The above does not particularly suggest
% % to learn FB: overall it argues more for SFs with well-chosen features.
% % 
% % We can use successor measures to learn the symmetrized inverse Laplacian,
% % but this is successor measures of a single policy (so without the $z$ in
% % $F$). Perhaps the $z$ in $F$ is needed to move away from the
% % strong-regularization regime (try to write the optimality argument at second
% % order instead of first order...).  Or perhaps $z$ should only be used for the
% % last step, when computing the SFs of the features $B$, but $B$ should be
% % computed from a $z$-free symmetric FB. (LSA-SR has $B$ from a $z$-free
% % non-symmetric FB.)
% % 
% % From this, it's not clear why the specific FB algorithm is interesting.
% % Of course FB provides a one-in-all loss and the problem is solved if the
% % loss is $0$. But one can say the same for SFs with the factorization of
% % $P$. There are theoretical bounds for both, too (though I think all of
% % them are trivial when taking a white noise prior or a sup over reward
% % functions). Of course the FB approximation will usually be better than
% % the P approximation, but that would require a prior on MDPs...
% % 
% % Arguments for FB and against symmetric eigenfunctions: sym eigenfunctions
% % don't work in the reduced-goal-space case (TODO try again...). Sym
% % eigenfunctions assume the environment is deterministic. And they are
% % optimal only close to the exploration policy, which is optimal for
% % nothing, while FB produces features adapted to policies $\pi_z$ that are
% % optimal for something (precisely, I think TODO CHECK that $B$ is an
% % eigenspace of the $\E_z M^{\pi_z}$, so characterizes the variations in
% % occupation measures for optimal policies for various $z$).
% % Symmetric features can only be done on a single policy at once
% % (can't do symmetric features with a $z$ parameter, because then features
% % depend on $z$ too: $M^{\pi_z}=\transp{\phi_z}\phi_z+...$), so can't state
% % an FB-like theorem which says we've got a solution if we solve the
% % equation. (Then again, such theorems are easy: for any $\pi$, if the
% % symmetrized $M^\pi$ is exactly equal to $\transp{\phi}\phi$, then $\phi$
% % is a complete basis and SFs on $\phi$ solve the problem...)
% % 
% % FB try to minimize the $L^2$-error on the $Q$-functions of each $\pi_z$
% % for random white noise rewards: indeed, minimizing
% % $\norm{\transp{F}_zB\rho-M^{\pi_z}}^2_\rho$ is equivalent to minimizing
% % $\E_r \norm{\transp{F_z}B\rho r-Q^{\pi_z}_r}_{L^2(\rho)}$ with white
% % noise $r$. However, this is very imperfect for several reasons: first,
% % this optimizes every policy for every reward, not just the policy for the
% % reward it's used for; second, the $L^2$ error on a $Q$-function does not
% % directly translate into performance (there are bounds from the $L^\infty$
% % error on the $Q$ or advantage function, but these are only bounds). Using
% % fb-FB will have the same fixed points as the above, while using ff-FB as
% % we normally do will have different fixed points related to eigenfunctions
% % of the averages of policies $\pi_z$, so probably corresponding to a
% % Laplacian-style prior instead of white noise, which is probably good.
% % Still, the link to downstream performance is much less direct.
% % 
% % \paragraph{Nonlinear reward representations.} One way to get nonlinear
% % reward representations is to iterate linear reward representations:
% % \begin{equation}
% % z_1=\E\, r(s)B_1(s),\qquad z_2=\E\,r(s) B_2(s,z_1),\qquad z_3=\ldots
% % \end{equation}
% % so that the features $B_2$ can be adjusted to the reward function via a rough
% % reward summary $z_1$.
% % 
% % In general, assuming we have access to rewards by samples $(s_i,r_i)$, a
% % reward representation
% % should be any permutation-invariant function of those. All such
% % permutation-invariant functions can be described by iterating parallel
% % treatment of the samples, and taking averages (see our paper with
% % Corentin, Thomas and Jakob on batch-gan a few years ago).
% % 
% % The $z_1,z_2\ldots$ example above is one particular case, because at each
% % step is stays linear in $r$ conditioned to the previous steps. I *think*
% % the more
% % general case is something like $z_1=\E\, B_1(r(s),s)$,
% % $z_2=\E\,B_2(r(s),s,z_1)$ etc.
% % 
% % The first case above is quite natural, though. In particular, if $B_1$ is
% % fixed, I think we can compute the optimal $B_2$ for each $z_1$ (with
% % entropy regularization), which is probably something like the
% % eigenfunctions of the symmetrized inverse Laplacian of the policy for the
% % reward $r_{z_1}=\transp{z_1}(\Cov B_1)^{-1}B_1$. \footnote{Actually with the white noise
% % model, I find that if $B_1$ is aligned with the largest eigenfunctions,
% % then $B_2$ should be the next largest eigenfunctions, whatever $z_1$,
% % because this is the best bet for the residual... so in the large-regul
% % regime, no interest in having nonlinear features!?!?!? of course in this
% % regime everything
% % is based on a first-order expansion... should really try 2ndorder,
% % probably some interactions between the eigenfunctions as the policies and
% % distributions get away from $\rho$...}
% % So at least we have a
% % good candidate for the last step of the process. But then I don't know
% % how to optimize $B_1$. Perhaps a symmetrized hierarchical FB scheme?
% % 
% % For nonlinear rewards, I don't know how to compute $r_z$ in general. With
% % $z_1=\E\, r(s)B_1(s), z_2=\E\,r(s) B_2(s,z_1)$ it is tempting to set
% % $r_z=\transp{z}(\Cov B_{z_1})^{-1}B(s,z_1)$ which certainly satisfies
% % $B(r_z)=z$, but I'm not sure this is the posterior mean. [TODO I think it
% % works, by nesting the conditional expectations wrt $z_1$ and $z_2$]
% % 
% % \paragraph{Accounting for test-time sampling.} Let us
% % use a more precise model of what happens at test time.
% % 
% % At test time, we do not observe $z=\E_\rho \,rB$.  We observe a set of
% % samples $\tilde z=\{(s_i,r_i)\}$. This $\tilde z$ is a non-parametric
% % representation of the reward.
% % 
% % Under the white noise model and on a finite space (otherwise, I don't
% % know what $r_i$ is because white noise takes infinite values at every
% % state), and assuming we observe $r_i=r(s_i)$ noise-free (otherwise the
% % numbers change slightly), we have
% % \begin{equation}
% % r_{\tilde z}(s)=\sum_i r_i \1_{s=s_i}
% % \end{equation}
% % (for simplicity I assumed states were
% % observed at most once). [With noise level $\sigma^2$ on $r_i$, this
% % becomes $r_i/(1+\rho(s_i)\sigma^2$).]
% % 
% % The reasoning on the first page above is still valid, so the best bet is
% % still to apply the optimal policy for $r_{\tilde z}$. This means having
% % pre-computed all optimal policies for all possible rewards $r_{\tilde
% % z}$.
% % 
% % In practice we don't do that: we only apply a policy $\pi_z$ where $z$ is
% % computed from $(s_i,r_i)$. In FB we use
% % \begin{equation}
% % z=\frac1N \sum r_i B(s_i).
% % \end{equation}
% % Given that we do this, we should let $\pi_z$ be the optimal policy for
% % $r_z=\E[r|z]$.
% % 
% % This $z$ is a random variable since the $s_i$ are random. I don't know if
% % we can compute $r_z=\E[r|z]$ in this model (the distribution of
% % $r$ knowing $z$ is quite involved), although asymptotically it
% % does coincide with the $r_z$ computed from exact expectations. And this
% % probably does not help with too sharp policies, since we are still
% % applying an exact optimal policy of something.
% % 
% % \paragraph{General reward priors given by NNs.} Assume that the prior on
% % rewards is given by randomly initialized NNs (this allows for some prior
% % information to be encoded in the NN structure), namely, a parameterized
% % function $r_\theta(s)$ with random $\theta$. \footnote{In that case, a
% % default option is to set $z=\theta$ and just train $\pi_\theta$ on
% % $r_\theta$, but using $\theta$ as a network input for $\pi$ might be too
% % big (also, when faced with a reward $r$, we must find the best $\theta$
% % for $r_\theta\approx r$, which requires test-time training). If we
% % downsize $\theta$ to be only the last layer on top of pre-trained
% % features, then this is exactly SFs.}
% % 
% % We can train (heavily) the posterior rewards $r_z$ by having another
% % network $\bar r(s,z)$ and proceeding as follows: Set $\theta$ at random;
% % compute $z=B(r_\theta)$ (this requires evaluating $r_\theta$ on many
% % states). Do teacher-student network on $\bar r$ so that $\bar
% % r(s,z)=r_\theta(s)$.
% % 
% % Then we have access to $r_z$, and we can both train $\pi_z$ to optimize
% % $r_z$, and optimize the features $B$ as above (for linear $B$).
% % 
% % Probably a bit heavy.
% % 
% % Alternatively, we could introduce some noise on $z$, representing
% % $z=\E_{\rho} rB+\eps N(0,\Id)$. This defines a joint distribution on
% % $(\theta,z)$. For learning we only need samples from this joint
% % distribution, then we can use $z$ and $r_{theta}$. This can be done via
% % any form on MCMC, eg Langevin dynamics. Indeed, assuming eg a standard
% % Bottou--Glorot prior on $\theta$, the joint distribution would be
% % $\exp(-\norm{\theta}^2_{BG}-\norm{z-\E rB}^2/2\eps^2$ which we can sample
% % from. Still requires many sample states $s$ to estimate $\E rB$ at each
% % step, so still heavy.
% % 
% % \paragraph{General Gaussian priors.} On the other hand, general Gaussian
% % priors are very easy to handle: we only need to change the formula used
% % for $z$ to reflect the norm used. Namely, consider a prior $\exp
% % (-\transp{r}Dr)$ for some symmetric positive definite $D$. Compute $z$ at test time
% % via
% % \begin{equation}
% % z=BDr
% % \end{equation}
% % which is feasible as soon as $D$ is samplable from the buffer. The prime
% % example is a Laplacian smoothness prior \footnote{actually we have to add
% % $\transp{r}\rho r$ otherwise this is only a seminorm as it's zero on
% % constants}
% % \begin{equation}
% % \transp{f}Df\deq \E_{(s_t,s_{t+1})} (f(s_t)-f(s_{t+1}))^2
% % \end{equation}
% % over transitions in the replay buffer, corresponding to
% % \begin{equation}
% % z=\E_{(s_t,s_{t+1})} (r(s_t)-r(s_{t+1}))(B(s_t)-B(s_{t+1}))
% % \end{equation}
% % which can be estimated. \footnote{At test time we might not have access to
% % both $r(s_t)$ and $r(s_{t+1})$... this requires the reward samples to be
% % provided on trajectories, or by explicit knowledge of $r$. Also the
% % goal-reaching case $r=$Dirac does not work directly, would have to set
% % $r$ to $1$ on a few nearest-neighbors of the goal in the replay buffer,
% % which requires a distance on states... probably use the states with
% % closest $B$-values or use smoothed reward such as $r(s)\approx \exp(-\lambda
% % \norm{B(s)-B(g)}$).} (Also corresponds to some RKHS TODO)
% % 
% % Then we have the same results as above, by substituting $D$ for $\rho$
% % everywhere (except sampling initial states $s_0\sim \rho$). Namely:
% % 
% % \begin{prop}
% % Consider a Gaussian prior $r\sim \exp
% % (-\transp{r}Dr)$ for some symmetric positive  definite $D$. Estimate $z$ at test
% % time via
% % \begin{equation}
% % z=BDr
% % \end{equation}
% % Then
% % \begin{equation}
% % r_z(s)=\transp{z}(BD\transp{B})^{-1}B(s).
% % \end{equation}
% % SF-wise, this is $z=(\phi D\transp{\phi})^{-1} (\phi Dr)$ and
% % $r_z(s)=\transp{z}\phi(s)$.
% % 
% % In this model the distribution of $z$ is Gaussian with covariance
% % $BD\transp{B}$ (or $(\phi D\transp{\phi})^{-1}$ for SFs).
% % 
% % This also holds for any Gaussian model of rewards such that the
% % components along $B$ and its $D$-orthogonal are independent, namely,
% % $r(s)=\transp{\theta_1}B(s)+\transp{\theta_2}\xi(s)$ where $\xi$ are
% % any features such that $B D\transp{\xi}=0$ and
% % $\theta_1$, $\theta_2$ are independent Gaussian vectors with any
% % covariance matrix.
% % \end{prop}
% % 
% % So we can apply all the results before, including the optimization of
% % $\phi$ by \eqref{eq:philoss}. In that case, $B$ or $\phi$ should be normalized in $D$-norm
% % which is a bit trickier (the penalty is not just the Laplacian norm
% % penalty on $B$, it's $(BD\transp{B})^2-2BD\transp{B}$ whose main term
% % requires sampling two independent transitions). Another interesting case
% % could be a Laplace perturbation around white noise, $D=\rho+\eps
% % (\rho\Delta+\transp{\Delta} \rho)$.
% % 
% % Generating $z$ with covariance matrix $BD\transp{B}$ for Laplacian $D$
% % can be done just by sampling a transition $(s_t,s_{t+1})$ and setting
% % $z=B(s_t)-B(s_{t+1})$. (This has the right covariance but is not quite
% % Gaussian, but adding a few such values with random coeffs will quickly
% % become Gaussian.)
% % 
% % \paragraph{Accounting for $\Cov \phi\neq \Id$ Version 1: directly using the square root.} This can be done
% % easily thanks to the following lemma. TODO triple-check carefully because
% % the change $\pi\to \pi'$ depends on the square root determination... but
% % I think it's ok to have a $\phi$-dependent change of policy
% % parameterization, we still optimize over the same set of policies and
% % $\phi$.
% % 
% % 
% % \begin{lem}
% % The gradient $\nabla ((\phi\transp{\phi})^{-1/2}\phi)$ can be treated as $(\phi
% % \transp{\phi})^{-1/2}\nabla \phi (\Id-\Pi_\phi)$ if the determination of
% % the matrix square root does not matter, where $\Pi_\phi\deq
% % \transp{\phi}(\phi \transp{\phi})^{-1}\phi$ is the orthogonal projector
% % onto the span of $\phi$.
% % 
% % Therefore, if a loss is a function of
% % $(\phi\transp{\phi})^{-1/2}\phi$ and does not depend on the determination of
% % the square root, then computing the gradients of this loss can be done by
% % replacing $(\phi\transp\phi)^{-1/2}\phi$ with
% % $(\bar\phi\transp{\bar\phi})^{-1/2}(\bar \phi+\phi (\Id-\Pi_{\bar \phi}))$ where
% % $\bar\phi$ denotes the stop-grad operator.
% % \end{lem}
% % 
% % TODO include $D$ back and write the loss...
% % 
% % 
% % \paragraph{Accounting for $\Cov \phi\neq \Id$ Version 2: trying to maintain $\Cov \phi\approx \Id$.}
% % Since the loss only depends on the linear span of the features $\phi$, we
% % can assume that $\phi D\transp{\phi}=\Id$ without loss of generality by a
% % change of variables. More precisely, let $C$ be the set of $\phi$ such
% % that $\phi D\transp{\phi}=\Id$: without loss of generality, we can
% % optimize $\ell$ on $C$ only. On $C$, the loss $\ell$ coincides with the
% % loss
% % \begin{equation}
% % \ell_C(\Phi,\pi)\deq -\E_{z\sim N(0,\Id)}
% % \E_{s_0\sim \rho_0}\,M^{\pi_z}(s_0) \transp{\phi}z
% % \end{equation}
% % so we can optimize $\ell$ by optimizing $\ell_C$ \emph{restricted to
% % $C$}.
% % 
% % By the general theory of constrained optimization, 
% % optimizing $\ell_2$ restricted to $C$ can be done by following a normal
% % gradient then projecting back onto $C$. The projection can be achieved in
% % several ways: either by explicitly resetting $\phi \gets (\phi D
% % \transp{\phi})^{-1/2} \phi$ (if $\phi$ is a neural network, this just
% % means multiplying the output layer weight matrix by $(\phi D
% % \transp{\phi})^{-1/2}$ TODO we should also relabel the policies $\pi_z$
% % accordingy, by preprocessing of $z$ by the covariance for $\pi$), or by using an auxiliary loss on $(\phi D
% % \transp{\phi})-\Id$ with a strong weight.
% % 
% % Thus, let us set
% % \begin{equation}
% % \lorth(\Phi)\deq \norm{\phi D
% % \transp{\phi}-\Id}^2_{\mathrm{Frobenius}}=\Tr \left(
% % (\phi D \transp{\phi}-\Id)\transp{(\phi D \transp{\phi}-\Id)}
% % \right).
% % \end{equation}
% % 
% % 
% % \begin{prop}
% % Optimizing $\ell(\Phi,\pi)$ by gradient descent is equivalent to optimizing
% % \begin{equation}
% % \ell_2(\Phi,\pi)\deq -\E_{z\sim N(0,\Id)}
% % \E_{s_0\sim \rho_0}\,M^{\pi_z}(s_0) \transp{\phi}z +\lambda \lorth(\Phi)
% % \end{equation}
% % by gradient descent
% % when $\lambda\to \infty$.
% % \end{prop}
% % 
% % Of course, $\phi$ must be initialized so that $\phi D
% % \transp{\Phi}\approx\Id$. This can be done by explicit projection $\phi
% % \gets (\phi D
% % \transp{\phi})^{-1/2} \phi$  (which might also be useful once in a
% % while).
% % 
% % The orthonormalization loss $\lorth$ can be computed explicitly depending
% % on the expression of $D$: for white noise, $D=\E_{s\sim \rho} [\1_s
% % \transp{1_s}]$ and
% % \begin{equation}
% % \lorth(\Phi)=\E_{s\sim \rho,\,s'\sim \rho} \left[\left(
% % \transp{\phi(s)}\phi(s')\right)^2-\norm{\phi(s)}^2-\norm{\phi(s')}^2
% % \right].
% % \end{equation}
% % The expression for the GFF prior is longer but
% % similar in structure.
% % 
% % 
% % \paragraph{Accounting for $\Cov \phi\neq \Id$ Version 3, valid for
% % $\ell(\phi, \pi^\star(\phi))$.}
% % We know that the loss only depends on the span of $\phi$, so the derivative of $\ell$ along
% % the span of $\phi$ is $0$ (TODO this holds if $\pi_z$ is optimal, or if we align the
% % $\pi_z$ for different $\phi$'s by a change of variable).
% % 
% % Set the loss 
% % \begin{equation}
% % \ell'(\Phi,\pi)\deq -\E_{z\sim N(0,\Id)}\E_{s_0\sim \rho_0} M^{\pi_z}
% % (\Id-\Pi_{\bar\phi}) \transp\phi (\bar \phi D
% % \transp{\bar \phi})^{-1/2}z
% % \end{equation}
% % where $\Pi_{\bar\phi}\deq
% % \transp{\bar\phi}(\bar\phi D\transp{\bar\phi})^{-1}\bar\phi D$
% % is the $D$-orthogonal projector
% % onto the span of $\bar \phi$, and $\bar\phi$ denotes stop-grad.
% % 
% % Let us prove that $\ell$ and $\ell'$ have the same differential. So we
% % can just optimize using the loss $\ell'$.
% % Thus, we can treat $\phi D
% % \transp\phi$ as constant up to introducing the projector.
% % 
% % Let us
% % decompose the tangent space into the span of $\phi$ and its orthogonal,
% % $\{\delta\phi \mid \Pi_\phi \transp{\delta \phi}=\transp{\delta \phi}\}$
% % and $\{\delta \phi\mid \Pi_\phi
% % \transp{\delta \phi}=0\}$ respectively.
% % By construction, $\ell$ and $\ell'$ have the same differential (namely $0$) in
% % the subspace given by the span of $\phi$. In the orthogonal subspace,
% % $\delta \phi$ is orthogonal to $\phi$, so $\phi D\transp\phi$ is
% % constant. Therefore, the derivative of $\phi D\transp\phi$ is $0$ in that
% % direction, and the differential of $(\phi D\transp\phi)^{-1/2}\phi$ is just $(\phi
% % D\transp\phi)^{-1/2}\d \phi$ in that subspace.
% % 
% % (Note: we cannot compute the orthogonal projector explicitly, but we can
% % sample from the resulting expression. Find a sampling such that variance is
% % small? eg similar to the orthonormalization loss)
% % 
% % TODO: invariance only holds when $\pi$ are defined as the optimal
% % policies, namely, for $\ell(\phi, \pi^\star(\phi))$. Given the $\pi_z$
% % it's not true; actually we might want to compute the gradient wrt to
% % $\phi$ given $\pi_z$: this might fit the $\phi$ to the existing
% % $\pi_z$. I don't know how to do that. Maybe just removing the projector
% % works, by the argument version 1 in the appendix... but it's the same???
% % 


\appendix

\section{Additional Proofs}
\label{sec:proofs}

\begin{dem}[ of Proposition~\ref{prop:posteriormean}]
This is because
$Q$-functions are linear in $r$ for a given policy. Intuitively, all
rewards represented by $z$ will share policy $\pi_z$, and so the
average return over rewards is the return of the average reward among
those represented by $z$. More precisely, by definition of $\beta_z$,
and by \eqref{eq:Vsucc},
the loss rewrites as
\begin{align}
\ell_\beta(\Phi,\pi)&=- \E_{z\sim \beta_z} \,\E_{r|\Phi(r)=z}\,\E_{s_0\sim \rho_0}\,
V^{\pi_z}_r(s_0)
% \\&=- \E_{z\sim \beta_z}\, \E_{r|\Phi(r)=z}\,\int
%  r(s) \,d_{\pi_z}(\d s)
% \\&=- \E_{z\sim \beta_z}\, \int 
% \left(\E_{r|\Phi(r)=z}\, r(s)\right) \,d_{\pi_z}(\d s)
% \\&=-\E_{z\sim \beta_z} \int r_z(s) \,d_{\pi_z}(\d s)
\\&=- \frac{1}{1-\gamma}\E_{z\sim \beta_z}\, \E_{r|\Phi(r)=z}\,\E_{s\sim d_{\pi_z}}\,
 r(s)
\\&=- \frac{1}{1-\gamma}\E_{z\sim \beta_z}\, \E_{s\sim d_{\pi_z}}
\left(\E_{r|\Phi(r)=z}\, r(s)\right)
\\&=- \frac{1}{1-\gamma}\E_{z\sim \beta_z}\, \E_{s\sim d_{\pi_z}}
\, r_z(s)
\\&=- \E_{z\sim \beta_z}\, \E_{s_0\sim \rho_0}
\,V^{\pi_z}_{r_z}(s_0)
\end{align}
as needed.
\end{dem}


\begin{dem}[ of Propositions~\ref{prop:linearpostmean} and
\ref{prop:gaussianprior}]
Since Proposition~\ref{prop:linearpostmean} is a particular case of
Proposition~\ref{prop:gaussianprior}, we only prove the latter.

By definition, the reward $r$ is a centered
Gaussian vector with probability density $\exp(-\norm{r}^2_K/2)$.

The posterior mean reward $r_z$ is the expectation of $r$ knowing
\begin{equation}
%z=(\Cov \phi)^{-1} \E_{s\sim \rho} [r(s)\phi(s)]= (\Cov \phi)^{-1}\langle r,\phi\rangle_\rho
z=C^{-1}\langle r,\phi\rangle_K
\end{equation}
where
$\langle
\cdot,\cdot\rangle_K$ is the dot product associated with the quadratic
form $\norm{\cdot}^2_K$, and
$C=\langle \phi,\phi\rangle_K$
is the $K$-covariance matrix of the features $\phi$,namely
$C_{ij}=\langle \phi_i,\phi_j\rangle_K$.

Without loss of generality, by the change of
variables $\phi\gets C^{-1/2}\phi$ (which yields $z\gets C^{1/2}z$), we
can assume that $C=\Id$, namely, the features $\phi$ are $K$-orthonormal. So we must compute
the mean of $r$ knowing $z=\langle r,\phi\rangle_K$.

Since $\phi$ is $k$-dimensional, this is a set of $k$ constraints
$\langle r,\phi_1\rangle_K=z_1, \ldots, \langle
r,\phi_d\rangle_K=z_d$.

These $k$ constraints define a codimension-$k$ affine hyperplane in the space of
reward functions. We have to compute the expectation of $r$ conditioned
to $r$ lying on this hyperplane.

For any Euclidean norm $\norm{\cdot}$, 
the restriction of a centered Gaussian distribution $\exp(-\norm{x}^2/2)$
to an affine subspace is again a Gaussian distribution, whose mean is
equal to the point of smallest norm in the subspace. (This can be proved
for instance by applying a rotation so the affine subspace aligns with
coordinate planes, at which point the result is immediate.)

Therefore, the posterior mean $r_z$ is the reward function that minimizes
$\norm{r_z}^2_K$ given the constraints $\langle
r,\phi_1\rangle_K=z_1, \ldots, \langle r,\phi_d\rangle_K=z_d$.
Since the $\phi_i$ are $K$-orthonormal, this is easily seen to be
$z_1\phi_1+\cdots+z_d \phi_d$. This proves the claim about the posterior
mean reward $r_z$.

For the claim about the distribution of $z$, assume again that the set of features
$\phi$ is $K$-orthonormal ($C=\Id$). Completing $\phi$ into a $K$-orthonormal
basis, the Gaussian prior $\exp(-\norm{r}^2_K)$ means that all components
of $r$ onto this basis are one-dimensional standard Gaussian variables.
So $z=\langle r,\phi\rangle_K$ is a $k$-dimensional standard Gaussian.
Undoing the change of variables with $C$, namely, $\phi \gets C^{1/2}\phi$ and
$z\gets C^{-1/2}z$, results in $z$ having covariance
$C^{-1}$.
\end{dem}

\begin{dem}[ of Theorem~\ref{thm:main}]
Theorem~\ref{thm:main} is a direct consequence of
Proposition~\ref{prop:gaussianprior},
Proposition~\ref{prop:posteriormean}, the definition of $\ell_\beta$, and
the expression \eqref{eq:Vsucc} of
$V$-functions using the occupation measures $d_\pi$.
\end{dem}



\paragraph{Derivation of Algorithm~\ref{algo:occupation}.}
The \emph{successor measure} \cite{successorstates} of a policy $\pi$ is
a measure over the state space $S$ depending on an initial state-action
pair $(s_0,a_0)$. It encodes the expected total time spent in any part
$X\subset S$, if starting at $(s_0,a_0)$ and following $\pi$. The formal
definition is
\begin{equation}
M^{\pi}(s_0,a_0,X)\deq \sum_{t\geq 0} \Pr(s_t\in X|s_0,a_0,\pi)
\end{equation}
for each $X\subset S$.

By definition, the occupation measure \eqref{eq:dpi} is the average of
the successor measure over the initial state and action:
\begin{equation}
\label{eq:dm}
d_\pi(X)=(1-\gamma) \E_{s_0\sim \rho_0, \, a_0\sim \pi(a_0|s_0)}
M^\pi(s_0,a_0,X).
\end{equation}

A parametric model of $M^\pi$ can be learned through various measure-valued Bellman
equations satisfied by $M$. For instance, TD learning for $M$ is
equivalent to the following.

Represent $M$ by its density with respect to the data distribution
$\rho$, namely
\begin{equation}
M^{\pi}(s_0,a_0,\d s)=m^\pi(s_0,a_0,s)\rho(\d s)
\end{equation}
where we want to learn $m^\pi(s_0,a_0,s)$.
This can be done using one of the methods from
\cite{successorstates}. For instance, $M^\pi$ satisfies a measure-valued
Bellman equation, which gives rise to a Bellman-style loss on $m^\pi$
with loss
\begin{multline}
\label{eq:lossm}
\loss_m\deq \E_{
(s_t,a_t,s_{t+1})\sim \rho,\,
a_{t+1}\sim \pi_z(s_{t+1}),\,
s'\sim \rho
}\\
\left[
\left(m^\pi(s_t,a_t,s')-\gamma
m^\pi(s_{t+1},a_{t+1},s')\right)^2-2m^\pi(s_t,a_t,s_t)
\right].
\end{multline}
This is the loss $\loss(M)$ used in
Algorithm~\ref{algo:occupation}, where an additional $z$ parameter
captures the dependency on $\pi_z$.

In turn, the relationship \eqref{eq:dm} between $d$ and $M$ can be used
to learn a parametric model of $d$ from a parametric model of $M$.
Let us parameterize  $d$ by its density with respect to $\rho$ as we did
for $M$, namely
\begin{equation}
d_{\pi_z}(\d s)=d(s,z)\rho(\d s)
\end{equation}
we find
\begin{equation}
d(s,z)=(1-\gamma) \E_{s_0\sim \rho_0, \, a_0\sim \pi(a_0|s_0)}
m(s_0,a_0,s,z)
\end{equation}
which provides the loss for $d$ in Algorithm~\ref{algo:occupation}.

\paragraph{Derivation of $\loss_C$.} This is essentially an application
of the log-trick $\partial_\theta \E_{z\sim p_\theta} f(z)=\E_{z\sim
p_\theta} [f(z)\partial_\theta \ln p_\theta(z)]$, as follows.

\begin{lem}
\label{lem:gradexpN}
Let $f\from \R^d\to \R$ be a bounded function and let $C$ be a $d\times
d$ matrix. Then the derivative with respect to $C$ of $\E_{z\sim
N(0,C^{-1})} f(z)$ satisfies
\begin{align}
\partial_C \E_{z\sim N(0,C^{-1})} f(z)=\tfrac12 \partial_C \E_{z\sim N(0,\bar C^{-1})}
\left[
f(z) \left(
-\transp{z}Cz+\Tr(\bar C^{-1}C)
\right)
\right]
\end{align}
taken at $\bar C=C$ (namely, $\bar C$ is a stop-grad version of $C$).
\end{lem}

Applying this to $C=\langle\phi,\phi\rangle_K$ yields
\begin{align}
\partial_\phi \E_{z\sim N(0,C^{-1})} f(z)&=
% \tfrac12
% \partial_\phi \E_{z\sim N(0,\bar C^{-1})}
% \left[
% f(z) \left(
% -\norm{\transp{z}\phi}^2_K+\sum_{ij} (\bar C^{-1})_{ij}\langle
% \phi_i,\phi_j\rangle_K
% \right)\right]
% \\&=
\tfrac12
\partial_\phi \E_{z\sim N(0,\bar C^{-1})}
\left[
f(z) 
\sum_{ij} \left((\bar C^{-1})_{ij}-z_iz_j\right)\langle
\phi_i,\phi_j\rangle_K
\right]
\end{align}
as needed for $\loss_C$.


\begin{dem}[ of Lemma~\ref{lem:gradexpN}]
For any smooth parametric probability distribution $p_\theta$ and any bounded
function $f$, one has the log-trick identity
\begin{equation}
\partial_\theta \E_{z\sim p_\theta} f(z)=\E_{z\sim p_\theta}[f(z)\partial_\theta \ln
p_\theta(z)].
\end{equation}
Here we have $\theta=C$ and
\begin{equation}
\ln p_\theta(z)=-\frac12 \transp{z}Cz+\frac12 \ln \det C+\mathrm{cst}
\end{equation}
and Jacobi's formula for the derivative of the determinant states that
\begin{equation}
\partial_C \det C=\partial_C \left(
(\det \bar C) \Tr(\bar C^{-1}C)\right)
\end{equation}
evaluated at $\bar C=C$, hence
\begin{equation}
\partial_C \ln \det C=\partial_C \Tr(\bar C^{-1}C)
\end{equation}
which implies the result.
\end{dem}


\begin{dem}[ of Proposition~\ref{prop:sparseloss}% and derivation of Algorithm~\ref{algo:sparse}
]
Given a reward function $r$ and policy $\pi$, we have
\begin{align}
V^\pi_r&=\frac{1}{1-\gamma} \E_{s\sim d_\pi}r(s)
\\&=\frac{1}{1-\gamma} \E_{s\sim \rho} [d(s,\pi)r(s)]
\end{align}
where $d(s,\pi)$ denotes the density of $d_\pi$ with respect to $\rho$.

Applying this to a Dirac reward at $s^\star$, namely,
$r(s)=\1_{s=s^\star}/\rho(s^\star)$, yields
\begin{equation}
V^\pi_r=\frac{1}{1-\gamma} d(s^\star,\pi).
\end{equation}
The computation is the same when the reward is a sum of Dirac masses,
$r=c_k \sum_k w_i \delta_{s^\star_i}$
yielding
\begin{equation}
V^\pi_r=\frac{1}{1-\gamma} \sum_i c_k w_i d(s^\star_i,\pi).
\end{equation}

Proposition~\ref{prop:sparseloss} then follows from the definition of the
zero-shot loss $\ell_\beta$.
\end{dem}

\section{Penalizing Variance over the Reward $r$ is Equivalent to Spatial
Regularization for the White Noise Prior}
\label{sec:varianceregul}

The loss \eqref{eq:mainloss} maximizes the expected performance over the
reward $r$, but does not account for variance. This is one of the reason
we might get overspecialized skills that take ``risks'' such as making
a bet on the location of the best reward and going there.

Instead, let us consider a variance-penalized version of this loss,
\begin{equation}
\label{eq:mainlosspen}
\ell(\Phi,\pi)\deq - \E_{r\sim \beta}\, \E_{s_0\sim \rho_0}\,
V^{\pi_{\Phi(r)}}_r(s_0) + \lambda \Var_{r\sim \beta} (\E_{s_0\sim \rho_0}\,
V^{\pi_{\Phi(r)}}_r(s_0))
\end{equation}
where $\lambda\geq 0$ is the regularization parameter.

This is tractable, as follows. We only reproduce the main part of the
computation, the second moment term in the variance. With notation as in
Proposition~\ref{prop:posteriormean}, we have
\begin{align}
\E_{r\sim\beta} (\E_{s_0\sim \rho_0}\,
V^{\pi_{\Phi(r)}}_r(s_0))^2
&=
\E_{z\sim \beta_z} \E_{r| \Phi(r)=z} (\E_{s\sim \rho_0}\,
V^{\pi_z}_r(s_0))^2
\\&=\frac{1}{1-\gamma}
\E_{z\sim \beta_z} \E_{r| \Phi(r)=z} (
\E_{s\sim d_{\pi_z}} r(s)
)^2
\\&=\frac{1}{1-\gamma}
\E_{z\sim \beta_z} \E_{r| \Phi(r)=z} \langle d(s,z),r(s)\rangle_{L^2(\rho)}^2
\end{align}
where as in Section~\ref{sec:lossistractable}, $d$ is defined by
\begin{equation}
d_{\pi_z}(\d s)=d(s,z)\rho(\d s)
\end{equation}
namely, $d(s,z)$ is the density of the occupation measure of policy $\pi_z$
wrt the data distribution $\rho$.

For a white noise prior on $r$,
we have 
\begin{equation}
\E_r \langle d(\cdot,z),r\rangle_{L^2(\rho)}^2= \norm{d(\cdot,z)}^2_{L^2(\rho)}
\end{equation}
by the definition of white noise in general measure spaces.

But here we should not use $\E_r$ but $\E_{r|\Phi(r)=z}$, namely, we now
what the rewar features are. With a white noise prior and with linear
task representation, the distribution of $r$ knowing $\Phi(r)=z$ is the
$L^2(\rho)$-orthogonal projection of the white noise onto the orthogonal of the span of the
features. Denoting $\Pi^\bot_\phi$ this projector, we have
$\E_{r|\Phi(r)=z} \langle
d(\cdot,z),r\rangle_{L^2(\rho)}^2=\E_r \langle
d(\cdot,z),\Pi^\bot_\phi r\rangle_{L^2(\rho)}^2=\E_r \langle
\Pi^\bot_\phi d(\cdot,z),\Pi r\rangle_{L^2(\rho)}^2$. So, we have proved:


\begin{prop}
With linear features, penalizing the variance over $r$ of the expected
return is equivalent to penalizing the spatial variance (in
$L^2(\rho)$-norm) of the projection
onto the features of the occupation measure density $d(\cdot,z)$.
\end{prop}

Actually it might be safer just not to use the projection onto $\phi$:
it will overestimate variance, but results in simpler algorithms. Anyway,
the estimation of $z=\E[\phi.r]$ is itself subject to noise because we
use a finite number of samples, so even
knowing the empirical estimate of $z$, there is still variance in the
direction of the span of $\phi$.

Algorithmically, the applicability of this depends on the method. If the
occupation measure $d$ is just computed from $\pi_z$ which is computed
from $\phi$, then adding a penalty on $d$ will just throw off the
computation of $d$ without affecting the features $\phi$. But in the
VISR-like algorithm from Section~\ref{sec:lossistractable}, $\phi$ is in
turn computed from $d$ (the features $\transp{z}\phi$ are increased on
the part of the stte visited by $\pi_z$), so penalized the variance of $d$ is more or
less equivalent to penalizing the variance of $\phi$.


A similar penalty over the variance of the policy performance can be
incorporated in the FB framework. Things are a bit simpler because $B$ is
both the features and the successor measure $d$: we have
$d(s,z)=\E_{s_0\sim \rho_0,a_0\sim \pi_z(s_0)} \transp{F(s_0,a_0,z)}B(s)$, so
we might directly penalize the spatial variance of $d$, with loss
\begin{equation}
\E_{s\sim \rho} ((\E_{s_0,a_0} \transp{F(s_0,a_0,z)})B(s))^2
-(
\E_{s\sim \rho} \E_{s_0,a_0} \transp{F(s_0,a_0,z)}B(s)
)^2
\end{equation}

This may be a sensible and principled way to avoid degenerate
features in FB.

\bibliographystyle{alpha}
\bibliography{zeroshotloss}

\end{document}
