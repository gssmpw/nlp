\section{Related Work}
\vspace{-3pt}
\paragraph{Fact-Checking Evaluation} 
Automated fact-checking has gained significant attention in the NLP research community in recent years as a means of combating misinformation and disinformation. Various datasets have been proposed that enable the development and evaluation of systems for automatic fact-checking, the most popular ones being based on human-crafted claims from Wikipedia content~\cite{thorne2018fever, sathe2020automated, schuster2021get}, claims in fake news published by a news outlet~\cite{buntain2017automatically, shu2020fakenewsnet, nakov2022clef}, rumorous claims on social media~\cite{ma2015detect, ma2017detect, lin2022detect}, complex claims that require multi-step reasoning~\cite{jiang2020hover, aly2021feverous}, naturally occurring claims in specific domains~\cite{gupta2021x, wadden2022scifact, lin2023zero}, and LLM-generated misinformation~\cite{chen2024can}, etc. To understand the factual knowledge of LLMs, \citet{hu2024large} curated a new fact-checking benchmark by organizing previous representative datasets, aiming to identify weaknesses in LLM fact verification. However, besides the inevitable issue of test set leakage, this static evaluation approach relied primarily on expert-designed, specialized tasks from existing datasets, overlooking emerging LLM-generated content and lacking adaptability to the complex, open-ended nature of real-world applications. Different from previous work on static accuracy evaluation, leveraging the derived justification~\cite{atanasova2020generating, guo2022survey} from LLMs, our work aims to explore the dynamic auditing beyond the veracity prediction, to dynamically elicit the limitations of fact-checking in LLMs.

\vspace{-3pt}
\paragraph{LLM Agent} 
The integration of LLMs as agents spans various domains, such as code generation and game-playing, demonstrating their robust planning and reasoning capabilities across diverse contexts~\cite{wang2023voyager, yao2022react, shen2023hugginggpt, mu2023embodiedgpt, hong2023metagpt, liu2023agentbench, sun2023adaplanner, qian2023communicative}. These advancements highlight the ability of LLMs to handle complex tasks with minimal supervision. In parallel, self-improvement methodologies~\cite{chen2022codet, chen2023teaching, shinn2023reflexion, madaan2023self} have emerged, utilizing feedback-driven processes to iteratively enhance output quality. Building on these insights, we develop a novel agentic framework for systematical LLM auditing in fact-checking complex claims, fake news or rumors.