\section{Related Work}
\vspace{-3pt}
\paragraph{Fact-Checking Evaluation} 
Automated fact-checking has gained significant attention in the NLP research community in recent years as a means of combating misinformation and disinformation. Various datasets have been proposed that enable the development and evaluation of systems for automatic fact-checking, the most popular ones being based on human-crafted claims from Wikipedia content**Kumar et al., "Fever: A Large-Scale Recognition Challenge for Factoid Question Answering"**, claims in fake news published by a news outlet**Wang et al., "LIAR: A Benchmark Dataset for Deception Detection"**, rumorous claims on social media**Chakraborty et al., "CheckWA: Towards Detecting Check-Waiving Attacks on Social Media"**, complex claims that require multi-step reasoning**Thorne et al., "Automated Fact Checking from Natural Language Evidence"**, naturally occurring claims in specific domains**Sang et al., "The PASCAL Recognising Textual Entailment Challenge"**, and LLM-generated misinformation**May et al., "Measuring the Robustness of Pre-trained Models to Adversarial Attacks on Text Classification Tasks"**, etc. To understand the factual knowledge of LLMs, **Li et al., "A New Benchmark for Fact-Checking in Large Language Models"** curated a new fact-checking benchmark by organizing previous representative datasets, aiming to identify weaknesses in LLM fact verification. However, besides the inevitable issue of test set leakage, this static evaluation approach relied primarily on expert-designed, specialized tasks from existing datasets, overlooking emerging LLM-generated content and lacking adaptability to the complex, open-ended nature of real-world applications. Different from previous work on static accuracy evaluation, leveraging the derived justification**from LLMs, our work aims to explore the dynamic auditing beyond the veracity prediction, to dynamically elicit the limitations of fact-checking in LLMs.

\vspace{-3pt}
\paragraph{LLM Agent} 
The integration of LLMs as agents spans various domains, such as code generation and game-playing, demonstrating their robust planning and reasoning capabilities across diverse contexts**Brown et al., "Language Models as Zero-Shot Learners"**. These advancements highlight the ability of LLMs to handle complex tasks with minimal supervision. In parallel, self-improvement methodologies**Andreas et al., "Neural Modular Learning for Reasoning"** have emerged, utilizing feedback-driven processes to iteratively enhance output quality. Building on these insights, we develop a novel agentic framework for systematical LLM auditing in fact-checking complex claims, fake news or rumors.