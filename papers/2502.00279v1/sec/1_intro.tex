\section{Introduction}
\label{sec:intro}

\noindent
Semi-supervised learning (SSL) aims to augment the small labeled set of data with a large unlabeled set of data \cite{chapelle2009semi}. 
This is of considerable practical significance since in many applications unlabeled data is easily available but the labeling effort is very costly. 
Many semi-supervised learning methods have proven successful, even given very small amounts of labeled data. 
However there is very limited information about the unlabeled data, since we only have access to their features and not the unlabeled class distribution.
We will write the labeled and unlabeled class distributions as $P(Y|A=1)$ and $P(Y|A=0)$, respectively.
In some situations $P(Y|A=0)$ is known a priori, but in many practical applications it is unknown and difficult to estimate from $P(Y|A=1)$.
In particular, the distribution $P(Y|A=0)$ is frequently long-tailed.
%, and can evolve over time as new data is collected.
This SSL variant, where the unlabeled class distribution $P(Y|A=0)$ is unknown and differs from $P(Y|A=1)$, is sometimes called \textit{realistic long-tailed semi-supervised learning} (RTSSL).
This topic has drawn considerable recent interest (see for example \cite{simpro,kim2020distribution,crest,oh2022daso,acr,cpe}) since it reflects realistic assumptions in many applications.


\begin{figure*}[th]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/distribution.pdf}
\end{center}
\caption{The labeled class distribution and 5 possible unlabeled class distributions studied in \cite{simpro}. SimPro significantly overestimates the head classes in consistent, reverse and head-tail settings. Our doubly-robust estimate is more accurate at the head classes as well as the overall distribution in all but the middle setting, as measured in total variation distance in \cref{tab:cifar10-tv}. Our proposed 2-stage SimPro+ outperforms SimPro in classification accuracy in the middle setting as well.}
\label{fig:distribution}
\end{figure*}
% In this paper, we explicitly estimate the unlabeled class distribution $P(Y|A=0)$ as an initial first step. We note that a good approximation of this distribution can lead to good classification accuracy and vice versa, but a good classifier is not necessary to obtain a good class distribution because the former is infinite dimensional while the latter is not. We apply non-ignorable missingness, where the probability of data being missing depends on its value; this is an important statistical problem formulation that appears to have been overlooked in the vision community. 
In this paper, we explicitly estimate the unlabeled class distribution $P(Y|A=0)$ as a separate first step. We note that existing methods that estimate and use this distribution during training produce biased estimate. In particular, SimPro \cite{simpro} tends to significantly overestimate the head classes as shown in \cref{fig:distribution} in 4 out of 5 unlabeled class distributions studied. In contrast, our proposed doubly-robust estimator are more accurate. Our technique derives from semi-parametric efficiency theory predominantly studied in causal inference and has well-understood and strong theoretical guarantee \cite{dml}. Leveraging this improvement, we plug this first-stage estimate into a second stage algorithm for training the final classifier.

We also adapt a maximum likelihood framework for semi-supervised learning with label shift. The framework uses and estimates a \textit{missingness mechanism} which encodes the tendency of a label to be in the labeled $(A=1)$ or unlabeled $(A=0)$ set, and allows learning from both sets from one missing-data likelihood, which we address with an Expectation-Maximization (EM) algorithm. 
The basic idea dates as far back as \cite{ibrahim1996parameter}, yet despite its simplicity, it is often overlooked in the label shift and vision community. We show that it naturally generalizes and extends FixMatch \cite{fixmatch}. The recent work of \cite{simpro} can be seen as the same algorithm but with different parameterization (see \cref{subsec:simpro}). 

In summary, we propose a 2-stage algorithm for RTSSL (see \cref{fig:algorithm}). The first stage uses maximum likelihood and EM to initially learn about the data. The first-stage estimates are used for a meta doubly-robust estimator which significantly improves on the initial estimate of $P(Y|A=0)$.
We then plug this estimate into existing pseudo-labeling-based techniques to learn the final classifier.
Experiments demonstrate that our method produces a significantly better estimate of $P(Y|A=0)$. 
We also show that we improve overall accuracy when our estimate is plugged into existing pseudo-labeling-based techniques.
Additional experimental results and some technical details are deferred to the supplemental.

% Label shift is anti-causal, label causes features (ML perspective). For example, diseases causes symptoms. Covid and common cold both cause runny nose. However, covid is more likely during the pandemic while common cold is more likely in the year's Fall and Winter. The causal mechanism is assumed to be constant.

% Label shift is closely related to non-ignorable missingness in statistics, where the outcome also causes the missingness. For example, a missing outcome in a survey where the outcome is sickness can be a result of sickness forcing the surveyee to skip the survey. Label shift has an additional assumption, that the missingness is independent of the outcome given the covariate. This assumption 
% \textit{identifies} the data: given infinite data, no 2 different distributions can represent it. (label shift estimator, are labels informative)

% \begin{align*}
% P_S(y)P_S(x|y) &\neq P_T(y)P_Y(x|y)\\
% P_S(x|y) &= P_T(x|y)
% \end{align*}
% see \href{https://vectorinstitute.ai/wp-content/uploads/2021/08/ds_project_report_final_august9.pdf}{here}.


% \paragraph{\textbf{Why is it important?}} Recent works in semi-supervised learning (often without stating) and test-time adaption use label-shift assumptions. We show that our semi-supervised method readily improves them, and is also grounded.

% We apply our method to 2 settings:

% Semi-supervised learning: we are given labeled and unlabeled data at training time. This situation emphasizes a lack of labeled data such that a large neural model will not learn well from labeled data alone. Self-training methods that usually do well in this setting suffer from low quality pseudo-labels due to the label shift between the 2 sets. Our semi-supervised method gives a statistically grounded Monte Carlo EM framework to solve this problem.

% Test-time adaptation: we are given labeled data at training and unlabeled data at test. There are more labeled data than than the first setting, however the emphasis is now on imbalance or spurious correlation in labeled data such that a model will not perform well on the worst group, and therefore not generalize well to balanced test data. Usually, a model is first trained on the labeled data, then it is adapted, perhaps by adjusting a few parameters, for the unlabeled data. Our method, which can be seen as generalization of the label shift EM (MLLS), use both calibration data and unlabeled test data to learn a small set of parameters (e.g. temperature scaling) to quickly adapt the model to shifted test data.

% \paragraph{\textbf{Method.}} Our method is a simple EM algorithm for non-ignorable missing data. At the E-step, we impute the missing data (label). This naturally connects to pseudo-labeling and therefore we can add data augmentation and confidence thresholding to this step, a heuristic technique that has worked very well for the image domain (fixmatch). At the M-step, we find the model parameter, which consists of the \textit{inclusive} class-conditional and the missingness mechanism. We note that these parameters are different from the usual parameters that label shift papers optimize. We can find the optimal solution, or update incrementally. All in all, we are just modifying the same EM framework (Neal and Hinton).

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}

% \ignore{
% There are 2 important realistic problems to consider. The first problem is that the data is long-tailed, which makes it difficulty to learn the tail classes, and the second problem which is unique to the semi-supervised setting is that the unlabeled data has a different class distribution. Recent works have tried to address this realistic setting. In these works, the underlying assumption that makes learning possible is label shift, where it is assumed that the class conditional $P(X|Y)$ holds constant among the 2 datasets, while the class distribution $P(Y)$ varies. 
% % RDZ: We should either add "for features $X$ and class $Y$" or drop the math, which is my preference
% With this assumption, the main technique for label shift adaptation is logit adjustment. When applied to the first problem of long-tailed recognition, logit adjustment can be seen as adapting the training data prior to the uniform test prior. Empirically, it has proven to be a great solution to the first problem. However, the lack of unlabeled class distribution remains a challenge to apply logit adjustment to the second problem.
% }

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.7\textwidth]{figures/drsimpro.drawio-2.pdf}
\end{center}
\caption{Overview of our 2-stage method (\cref{subsec:2-stage}). In stage 1, we use Expectation-Maximization (EM, \cref{subsec:em}) to estimate the missingness mechanism and classifier from observable data. These quantities are used as nuisance components for the doubly-robust estimator of the class distribution \cref{eq:dr}. In stage 2, we can use EM or other existing methods that also use logit-adjustment with the (unlabeled) class distribution to estimate the final classifier. We use SimPro as our implementation of EM (\cref{subsec:simpro}). The network in stage 1 can be of equal or smaller size than the network in stage 2 (\cref{subsec:label}).}
\label{fig:algorithm}
\end{figure*}