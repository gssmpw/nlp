
\clearpage
% \setcounter{page}{1}
% \maketitlesupplementary
\begin{center}
Supplementary Material
\end{center}

% {
%     \onecolumn
%     \centering
%     \Large
%     \textbf{\thetitle}\\
%     \vspace{0.5em}Supplementary Material \\
%     \vspace{1.0em}
% }

\section{Proof of \cref{theorem:dr}}
We require some additional regularity assumptions:
\begin{assumption} 1) The number of classes $C$ is bounded w.r.t the number of samples $N$, 2) the missingness mechanism $P(A=1|Y,\theta)$, as well as its estimated counterpart $P(A=1|Y,\theta)$, are bounded below by some constant $\epsilon > 0$, 3) the quantities $P(Y|X,\theta)$ and $P(A|Y,\theta)$ are estimated using auxiliary samples independent of samples used for the sample averaging.
\label{assumption:extra}
\end{assumption}
Assumptions 1 and 2 are natural. For the missingness mechanism, the ground truth being bounded means that there is a non-vanishing proportion of samples for every class. The boundedness of the estimate can be enforced by clipping the estimate. Assumption 3 is called sample splitting in \cite{kennedy-dr}.

For convenience we use operator $\E_N$ to denote the average of $N$ samples i.e. $\frac{1}{N}\sum_{i=1}^N$. Note that this is by itself a random variable, in contrast to $\E$ which is a fixed number.

\begin{proof}[Proof of \cref{theorem:dr}] Because $C$ is bounded (assumption \ref{assumption:extra}), we can fix a class $c$ and prove the theorem.
Let us define the influence function $\phi$, parameterized by $\theta$, as
\begin{equation}
\phi(O | \theta)(c) = P(Y=c|X,\theta) + \frac{\one(A=1)}{P(A=1|Y,\theta)} (\one(Y=c) - P(Y=c|X,\theta)) - P(Y=c)
\end{equation}
As we have done in the main text, we use $\phi(O)$ to denote the same function but all estimated quantities are replaced with their truths. In other words, we use $\phi(O)$ for $\phi(O|\theta_0)$ where $\theta_0$ is the truth, given that our model contains $\theta_0$ e.g. when the model is consistent.

Recall that:
\begin{equation}
\begin{aligned}
\Psi_{dr}(\theta)(c) &= \frac{1}{N}\sum_{i=1}^N \left\{P(Y=c|X,\theta) + \frac{\one(A=1)}{P(A=1|Y,\theta)} (\one(Y=c) - P(Y=c|X,\theta))\right\}\\
&= \E_N [\phi(O|\theta)(c)] + P(Y=c)
\end{aligned}
\end{equation}

We will show that:
\begin{equation}
\Psi_{dr}(\theta)(c) - P(Y=c) = (\E_N - \E)[\phi(O)(c)] + o_P(N^{-1/2})
\label{eq:proof-linearity}
\end{equation}
To do that, we use the following decomposition
\begin{equation}
\begin{aligned}
\Psi_{dr}(\theta)(c) - P(Y=c) &= \E_N [\phi(O|\theta)(c)] \\
&= (\E_N - \E)[\phi(O)(c)] + (\E_N - \E)[\phi(O|\theta)(c) - \phi(O)(c)] + \E[\phi(O|\theta)(c)]
% &+ (\E_n - \E)[\phi(O;\theta) - \phi(O)]\\
% &+ \E[P(Y=c|X,\theta)] - \E[P(Y=c|X)] + \E[\phi(O,\theta)]
\end{aligned}
\end{equation}
and analyze the second and third term. The third term is:
\begin{equation}
\begin{aligned}
\E[\phi(O|\theta)(c)] &= \E[P(Y=c|X,\theta)] + \E\left[\frac{\one(A=1)}{P(A=1|Y,\theta)}(\one(Y=c) - P(Y=c|X,\theta))\right]- P(Y=c) \\
&= \E\left[P(Y=c|X,\theta) + \frac{P(A=1|Y)}{P(A=1|Y,\theta)}(P(Y=c|X) - P(Y=c|X,\theta))\right] - \E[P(Y=c|X)]\\
&= \E\left[(P(Y=c|X,\theta) - P(Y=c|X)) (P(A=1|Y,\theta) -P(A=1|Y)) \frac{1}{P(A=1|Y,\theta)}\right]\\
\end{aligned}
\end{equation}
by Cauchy-Schwarz inequality:
\begin{equation}
\begin{aligned}
\E[\phi(O|\theta)(c)] &\le \frac{1}{\epsilon} \|P(A=1|Y,\theta) - P(A=1|Y)\|_2 \|P(Y=c|X,\theta) - P(Y=c|X)\|_{L_2(P)}\\
&= \frac{1}{\epsilon} o_P(N^{-1/4} N^{-1/4}) = o_P(N^{-1/2})
\end{aligned}
\end{equation}
by assumption \ref{assumption:4th-root-n} and that $P(A=1|Y,\theta) > \epsilon$ (assumption \ref{assumption:extra}). The second term can be bounded by Chebyshev inequality
% \begin{equation}
% \begin{aligned}
% \E[\E_N[\phi(O|\theta)(c) - \phi(O)(c)]] &= \E[\phi(O|\theta)(c) - \phi(O)(c)]\\
% \var[\E_N[\phi(O|\theta)(c) - \phi(O)(c)]] &= \frac{1}{N}\var[\phi(O|\theta)(c) - \phi(O)(c)] \le 
% \end{aligned}
% \end{equation}
\begin{equation}
P(|(\E_N - \E)[\phi(O|\theta)(c) - \phi(O)(c)]| \ge t) \le \frac{\var[\E_N[\phi(O|\theta)(c) - \phi(O)(c)]]}{t^2} = \frac{\var[\phi(O|\theta)(c) - \phi(O)(c)]}{Nt^2}
\end{equation}
note here that $\theta$ is independent of the samples used for $\E_N$ by assumption \ref{assumption:extra}. For any $\varepsilon > 0$, by picking $t = \frac{1}{\sqrt{N\varepsilon}}$ we get
\begin{equation}
P\left(\left|\frac{(\E_N - \E)[\phi(O|\theta)(c) - \phi(O)(c)]}{N^{-1/2}}\right| \ge \frac{1}{\sqrt{\varepsilon}}\right) \le \varepsilon \var[\phi(O|\theta)(c) - \phi(O)(c)]
\end{equation}
by the definition of $O_P$, we then get
\begin{equation}
(\E_N - \E)[\phi(O|\theta)(c) - \phi(O)(c)] = O_P(N^{-1/2}\var[\phi(O|\theta)(c) - \phi(O)(c)])
\end{equation}
Because $\phi$ is a continuous function of $P(Y|X,\theta)$ and $P(A|Y,\theta)$ (given $P(A|Y,\theta) > \epsilon$, assumption \ref{assumption:extra}), by the continuous mapping theorem and the fact that $P(Y|X,\theta)$ and $P(A|Y,\theta)$ are convergent in probability (assumption \ref{assumption:4th-root-n}), we get $\var[\phi(O|\theta)(c) - \phi(O)(c)] = o_P(1)$. This gives
\begin{equation}
(\E_N - \E)[\phi(O|\theta)(c) - \phi(O)(c)] = o_P(N^{-1/2})
\end{equation}
Therefore, we have shown that the second and third term are both $o_P(N^{-1/2})$, proving \cref{eq:proof-linearity}. As the final step, multiply both sides of this equation by $\sqrt{N}$ we get:
\begin{equation}
\sqrt{N}(\Psi_{dr}(\theta)(c) - P(Y=c)) = \sqrt{N} (\E_N - \E)[\phi(O)(c)] + o_P(1) \rightsquigarrow \mathcal{N}(0, \var[\phi(O)(c)])
\end{equation}
by the central limit theorem, and $\var[\phi(O)(c)] = \E[\phi(O)(c)^2]$ because $\E[\phi(O)(c)] = 0$.
\end{proof}

While we started with the definition of $\phi$, \cref{eq:proof-linearity} shows that $\phi$ is indeed an influence function. Now we show that $\phi$ is also the efficient influence function, by using the characterization of the model's tangent space \cite{tsiatis-missingdata}. Note that the joint probability factorizes as $P(X,A,Y) = P(X)P(Y|X)P(A|Y)$, therefore the tangent space $\mathcal{T}$ factorizes as $\mathcal{T} = \mathcal{T}_{X} \oplus \mathcal{T}_{Y|X} \oplus \mathcal{T}_{A|Y}$ where $\mathcal{T}_X = \{h(X): \E[h] = 0\}$, $\mathcal{T}_{Y|X} = \{h(X,Y): \E[h|X] = 0\}$, $\mathcal{T}_{A|Y} = \{h(A,Y): \E[h|Y] = 0\}$, and the 3 subspaces are pairwise orthogonal. All influence functions are orthogonal to the tangent space, but the influence function that is also in the tangent space has the smallest variance and is called the efficient influence function. As $\phi$ is already an influence function, we need only show that $\phi$ is in $\mathcal{T}$. We write $\phi$ as
\begin{equation}
\phi(O)(c) = (P(Y=c|X) - P(Y=c)) + \left[\frac{\one(A=1)}{P(A=1|Y)} - 1\right](\one(Y=c) - P(Y=c|X)) + (\one(Y=c) - P(Y=c|X))
\end{equation}
and note that the first, second and third term are in $\mathcal{T}_X$, $\mathcal{T}_{A|Y}$ and $\mathcal{T}_{Y|X}$ respectively. Therefore, $\phi$ is indeed in $\mathcal{T}$. The efficient influence function has the smallest variance of all influence function, and therefore our estimator being asymptotically linear in $\phi$ (\cref{eq:proof-linearity}) has the smallest mean squared error in a local asymptotic minimax sense \cite{kennedy-dr, asymptoticstatistics}

\section{Further background and related work}
\paragraph{Discussion on semi-supervised EM.}
It appears that semi-supervised EM was first used for parameter estimation when the missingness mechanism is non-ignorable in \cite{ibrahim1996parameter}, but has not been used for label shift estimation.
Perhaps this is because the semi-supervised situation where additional unlabeled data is available during training is rarer than the test-time adaptation case. EM is well suited to take advantage of the extra unlabeled data to improve the classifier under very scarce and long-tailed labeled data. While the connection between pseudo-labeling and EM has been explored before \cite{entropyminimization}, the situation with label shift has not until recently \cite{simpro}. Here the application of EM is much more interesting, because other than simply giving pseudo-labeling a rigorous formulation, EM also estimates the missingness mechanism (equivalently the label distribution shift), which is important for shift correction and thus high-quality pseudo-labels \cite{acr}. The application of confidence thresholding can be seen as a sparse variant of EM \cite{neal1998view}.

\paragraph{The doubly-robust risk.} 
\label{subsec:dr-risk}
A technique that also derives from the theory of semi-parametric efficiency is orthogonal statistical learning \citep{foster2023orthogonal}. The idea is to minimize the doubly-robust risk:
\label{subsec:method-dr-risk}
\begin{equation}
\label{eq:dr-risk}
\mathcal{R}(\theta_2) = \frac{1}{N} \sum_{i=1}^N \Bigg[ l(x_i, \hat y_i|\theta_2) + \frac{\one(a_i=1)}{P(A=a_i|Y=y_i, \theta_1)} (l(x_i, y_i | \theta_2) - l(x_i, \hat y_i | \theta_2))\Bigg]
\end{equation}
where $l(x,y|\theta) = -\sum_{c=1}^C [y]_c \log P(Y=c|X=x,\theta)$ is the negative cross-entropy. 
The notation $[y]_c$ means that we are using the $c$-entry in a C-dimension probability vector $y$. 
Thus, $y_i$ denotes the one-hot label of observation $i$, while $\hat y_i$ denotes the pseudo-label, which can be one-hot or all-zero. 
Finally, we use $\theta_1$ to denote that $P(a|y,\theta_1)$ is an estimation from a previous stage, but it can be estimated with $\theta_2$ as well. 
The risk $\mathcal{R}(\theta_2)$ can be used as a training loss in a straightforward fashion. 
Similar to the doubly robust estimation of $P(Y)$, the doubly robust risk provides approximately unbiased estimation of the risk. 
This property has been used in \citep{arelabelsinformative, onnonrandommissinglabels, drst} also in the semi-supervised learning setting.
More broadly, it is at the heart of one of the core techniques in heterogenous treatment effect estimation in causal estimation \cite{kennedy2023towards, foster2023orthogonal, wager2018estimation}. 
The focus here is not the estimation of $\mathcal{R}(\theta_2)$ per se, but the quality of the learned model \cite{foster2023orthogonal}.
By using the doubly-robust risk, we can achieve an optimality result similar in spirit to our theorem \cref{theorem:dr}, but for the generalization error.
While this is appealing, in practice there are 2 problems with this approach. First, the inverse probability weight $P(A=a_i|Y=y_i,\theta_1)$ can be very large if the class ratio is highly unlabeled, making training unstable \cite{kallus2020deepmatch, pham2023stable}. 
This problem exists for our estimation as well. However, it is much easier to control for estimation than for training because of the iterative nature of model update. Secondly, we can further write $\mathcal{R}$ as:
\begin{equation}
\mathcal{R}(\theta_2) = \frac{1}{N}\sum_{i=1}^N l\left(x_i, \hat y_i + \frac{\one(a_i=1)}{P(A=a_i|Y=y_i,\theta_1)} (y_i - \hat y_i)\Bigg\vert\theta_2\right)
\end{equation}
which is a cross-entropy loss with new meta-pseudo-labels. However, these labels are not meant to be learned exactly, and furthermore they can be negative. Thus, theoretical works have to put stringent assumptions on the models. In \cref{subsec:ablation-1}, we show that experimentally that the instability problem makes doubly-robust risk performance worse than our 2-stage approach.

\section{Training and hyperparameter settings.}
\label{subsec:training-setting}
For neural network training, we follow the implementation and hyperparameter settings of \cite{simpro}. In particular, we adapt the core code of SimPro for Supervised, MLE and EM. For MLE, we update $P(A|Y)$ using the Adam optimizer with learning rate 1e-3, while for EM we use a momentum update similar to SimPro's update of $P(Y|A)$ because it has a a closed-form solution at each mini-batch. We use Wide ResNet-28-2 on all methods and all datasets in this section, including Imagenet-127, because we are motivated by the fact that stage-1's goal is not classification accuracy but the estimation of a finite-dimensional parameter. When using Wide ResNet-28-2 for Imagenet-127, we use the hyperparameters of CIFAR-100, except we lower the batch size of unlabeled data to 2 times that of labeled data instead of 8 for memory reason. We do not perform additional hyperparameter tuning. All experiments can be performed on 1 A6000 RTX GPU, and are run 3 times. We report the total variation distance between the estimated and the ground truth unlabeled class distribution, similar to its usage in Theorem 3.1 of \cite{lsc}, and the top-1 classification accuracy.

In the second stage of our algorithm, we freeze our estimation and plug it in SimPro and BOAT.
We keep exactly the same hyperparameter settings that SimPro and BOAT use. In particular, for Imagenet-127, we now use ResNet-50 and run each experiment once.
In SimPro, we set the unlabeled class distribution $P(Y|A=0)$ at the E-step;  however, we still keep a running estimate of the class distribution $P(Y)$ in the logit adjustment loss \cref{eq:simpro-la-loss}. While it is possible to use the first stage estimate in the logit adjustment loss, we observe that doing so results in lower accuracy than using the the running average. This is conceptually consistent with the role of the running average - serving not as an accurate estimate of $P(Y)$ but to make the classifier's class distribution uniform through the logit adjustment loss, which is good for the test set. Similarly, in BOAT, we only replace $\Delta_c = \log P(Y|A=1) - \log P(Y|A=0)$ in equation (4) of \cite{boat}, which is adjusting a classifier's predictions from the labeled to the unlabeled class distribution, with our SimPro + DR estimate instead of their on-the-fly estimate. 


% \section{Additional experiments}
% % \input{tables/cifar10-tv}
% \input{tables/cifar100-tv}