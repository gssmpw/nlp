

\section{Experimental Results}
\input{tables/cifar10-tv}
\input{tables/cifar100-tv}
\input{tables/cifar10-acc}
\input{tables/cifar100-acc}

We perform experiments for each stage of our algorithm. In the first stage, we compare among various methods to estimate the unlabeled class distribution $P(Y|A=0)$, showing that SimPro + DR performs well. In the second stage, we freeze the unlabeled class distribution, using our best estimator  SimPro + DR, and plug it into 2 SOTA semi-supervised learning algorithms, SimPro and BOAT~\cite{boat}. We show that this simple procedure improves the existing methods, and is even capable of improving the original SimPro when used for both stages.


% \textbf{Datasets} We adopt 4 standard benchmarks used frequently in other semi-supervised learning work: CIFAR-10, CIFAR-100~\cite{cifar}, STL-10~\cite{stl10} and Imagenet-127~\cite{cossl}. To match our RTSSL setting, we create long-tailed labeled and unlabeled sets from CIFAR-10 and CIFAR-100. Specifically, we use $\gamma_l$ and $n_1$ to denote the imbalance ratio and the head class's number of samples of the labeled data, the remaining class's size is computed as $n_c = n_1 \times \gamma_l^{-\frac{c-1}{C-1}}$ and likewise, $\gamma_u$ and $m_1$ of the unlabeled data. For CIFAR-10, we fix $n_1=500$ and $m_1=4000$. We test 2 different configurations $\gamma_l=\gamma_c=150$ and $\gamma_l=\gamma_c=100$. We further permute classes the unlabeled sets in 5 ways: consistent, uniform, reversed, middle and headtail, similar to \cite{simpro} and visualized in figure~\ref{fig:distribution}, which results in 10 different datasets in total. Similarly for CIFAR-100, we fix $n_1=500$ and $m_1=4000$, use 2 configurations $\gamma_l=\gamma_c=20$ and $\gamma_l=\gamma_c=10$, and permute the classes in 5 ways, resulting in 10 datasets as well. For STL-10, the unlabeled set has no ground truth labels, therefore we use all samples in the head class and set the imbalance ratio $\gamma_l$ to $10$ or $20$. Imagenet-127 is a naturally long-tailed dataset with 127 classes. We train on 32x32 and 64x64 image resolutions following ~\cite{cossl}.


\textbf{Datasets} We evaluate our method on four standard semi-supervised learning benchmarks: CIFAR-10, CIFAR-100~\cite{cifar}, STL-10~\cite{stl10}, and Imagenet-127~\cite{cossl}. To simulate RTSSL, we construct long-tailed labeled and unlabeled sets for CIFAR-10 and CIFAR-100. The labeled data follows an imbalance ratio $\gamma_l$ with head class size $n_1$, while the remaining class sizes are computed as $n_c = n_1 \times \gamma_l^{-\frac{c-1}{C-1}}$. The unlabeled data follows a similar setup with $\gamma_u$ and $m_1$.  

For CIFAR-10, we set $n_1 = 500$, $m_1 = 4000$, and test two configurations: $\gamma_l = \gamma_u = 150$ and $\gamma_l = \gamma_u = 100$. We generate 10 datasets by permuting the unlabeled class distributions in five ways: \textit{consistent, uniform, reversed, middle}, and \textit{head-tail}, as in~\cite{simpro}. CIFAR-100 follows the same setup with $n_1 = 50$, $m_1 = 400$, and $\gamma_l, \gamma_u$ values of 20 and 10.  

For STL-10, where unlabeled data lacks ground-truth labels, we use all head-class samples and set $\gamma_l$ to 10 or 20. Imagenet-127 is naturally long-tailed with 127 classes, and we train on 32$\times$32 and 64$\times$64 resolutions as in~\cite{cossl}.


\paragraph{Training.} We follow the implementation and hyperparameter settings of \cite{simpro}. We defer these details in \cref{subsec:training-setting}. One important exception is that for Imagenet-127, we use the smaller Wide ResNet-28-2 in stage 1 and the larger ResNet-50 for stage 2, to demonstrate that a smaller model is sufficient for distribution estimation.


\input{tables/stl10-acc}
\input{tables/imagenet-127-acc}
\input{tables/imagenet-127-tv}

\subsection{Better results on label distribution} 
\label{subsec:label}
We have mentioned various ways throughout the papers to estimate the unlabeled class distribution. In what follows, method consists of a model, which is how the learning is done, and an estimator, which is how the final distribution is estimated using parameters learned from the model.

%\begin{enumerate}
%\item 
\noindent
\textbf{Supervised}. The model is trained on the labeled set only and used to estimate the unlabeled class distribution \cite{unifiedlabelshift}. 2 successful estimators for this setting are \textbf{RLLS} \cite{rlls} and \textbf{MLLS} \cite{mlls}. 

%\item 
\noindent\textbf{MLE}. The model is trained by directly maximizing the likelihood \cref{eq:likelihood}. We also use the decomposition $P(Y|X)$ and $P(A|Y)$, and write the unlabeled term as $P(A=0, X) = \sum_{c} P(Y=c|X) P(A=0|Y=c)$, which enables gradient descent training on these parameters. This is also the MLE method to estimate $P(A|Y)$ in \cite{arelabelsinformative}.

%\item 
\noindent\textbf{EM}. We further test the EM algorithm in \cref{subsec:em}. In particular we also use strong and weak augmentations similar to FixMatch, but not the pseudo labeling operator. Confidence thresholding removes the soft predictions of the non-dominant classes, which may be better to keep since our target of the first stage is the global class statistics. We also try 3 estimators on this model.

%\item 
\noindent\textbf{SimPro} \cite{simpro} can be seen as our previous EM but also with FixMatch's confidence thresholding and logit adjustment loss in \cref{subsec:simpro}. Confidence thresholding is a powerful regularization technique that encodes the assumption that classes are well separated \cite{entropyminimization}, but can introduce bias to the estimation, which justifies the use of DR.
%\end{enumerate}

% For semi-supervised methods MLE, EM and SimPro, as we now have additional information on the missingness mechanism, we can use 3 estimators OR, IPW and DR presented in \cref{subsec:2-stage}


Results on \cref{tab:cifar10-tv} presents the performance of various models and estimators on CIFAR-10. We can see that SimPro + DR performs best. In contrast, SimPro + OR, SimPro's original way of estimating $P(Y|A=0)$, and SimPro + IPW tend to underperform EM on the consistent and uniform datasets. The consistent setting is worth noting, since it arises when data is sampled uniformly at random for labeling,  representative of a large number of real world situations. EM is competitive to SimPro as well even without pseudo labeling, but overall we found this regularization to offer significant gains in the reversed, middle and head-tail settings. Finally, Supervised with either MLLS or RLLS estimators performs much worse than the semi-supervise methods.

\cref{tab:imagenet-127-tv} aligns with the observations  made in \cref{tab:cifar10-tv}. In particular, SimPro + DR is the best method for class distribution estimation of the much larger Imagenet-127. We also found that a small neural network and a small image resolution is sufficient for the distribution estimation of the much larger dataset Imagenet-127. This matches our intuition that the finite-dimensional parameter is easier to learn.

\cref{tab:cifar100-tv} shows that most methods understandably struggle to estimate the class distributions in CIFAR-100. This is because there are few samples in each class, the head class has 10 times less samples while the number of classes multiplies 10 times compared to CIFAR-10. We see here that SimPro + DR is not the best method, but the relative gap between estimators are small.

% Among the models, the supervised baseline do not perform well even in the consistent setting, showing that when unlabeled data is available during training, learning from them can be valuable for class distribution estimation, especially in the cases with little labeled data like ours. Both the MLE and supervised models perform badly on the reversed, middle and head-tail settings

% Among the estimators, we see that DR boosts the performance of SimPro and EM in CIFAR-10, and of all semi-supervised models in Imagenet-127. It does not improve MLE on CIFAR-10, and it does not improve on CIFAR-100. However, for most of the time, the decrease is not much. In constrast, IPW estimators can be significantly worse, for example in the reversed setting of CIFAR-10, where the distance is $0.254$ for $\gamma_l=150$ and $0.233$ for $\gamma_l=100$, compared to OR's 0.040 and 0.059. 

% Both the MLE and supervised models perform badly on the reversed, middle and head-tail settings. EM does a decent job, though not as well as SimPro, on all 5 distribution settings of CIFAR-10. However, on Imagenet-127, EM without DR performs worse than MLE.

% We note that the performance on DR is similar to OR in these cases, showing that DR has a double robustness property. While IPW only relies on the finite-dimensional $P(A|Y)$, which intuitively is easy to estimate, we found that the inverse probability weight can nevertheless be unstable when some probabilities are small, and this is where DR shows its strength by combining both IPW and OR.



\subsection{Two-stage algorithm improves accuracy}

In the second stage of our algorithm, we freeze our estimation and plug it in SimPro and BOAT. We denote SimPro+ and BOAT+ for algorithms that use our first stage estimate.



\cref{tab:cifar10-acc} shows that for CIFAR-10 SimPro+ and BOAT+ improve over their original versions across most settings, leading to large improvements in both the consistent and middle class distribution settings. In particular, our two-stage approach improves SimPro in 9 / 10 settings and BOAT in 8 / 10 settings.
We also observe consistent improvements ove both base algorithms, SimPro and BOAT, for several other datasets. \cref{tab:stl10-acc} demonstrates improvements for 2 / 2 class imbalance ratios in STL-10 and \cref{tab:imagenet-127-acc} for 2 / 2  different resolutions of ImageNet-127. 


We also evaluate on CIFAR-100 for multiple unlabeled  class distribution settings and with mediocre class label distribution estimates in stage 1, demonstrate no degradation in accuracy in stage 2. As shown in \cref{tab:cifar100-acc}, the two stage algorithm with a mediocre stage 1 estimation leads to parity with the baseline. Stage 2 provides small improvements in 5 / 10 settings for SimPro and in 4 / 10 (with 2 ties) for BOAT.


\subsection{Ablation Study: Alternative implementations.}
\label{subsec:ablation-1}
In this section, we ablate on our 2-stage choice. Specifically, we consider 2 alternative implementations:
\paragraph{\textbf{Doubly-robust risk}}  
This approach is \cite{arelabelsinformative, onnonrandommissinglabels}, as discussed in \cref{sec:background}. we consider the doubly-robust risk as our training loss. We use the missingness mechanism estimation from stage-1 of SimPro+ for fair comparison. \cref{eq:dr-risk} is used for training in which the pseudo-labeling operators can be applied straightforwardly. More detail in \cref{subsec:dr-risk}
\paragraph{\textbf{Batch-update doubly-robust $P(Y|A)$}} Different from SimPro+, here we remove the first stage and instead update our doubly robust estimation of the unlabeled class distribution using a moving average of the batch statistics.

\cref{tab:cifar10-ablation-1} shows that the batch-update version of SimPro+ is significantly worse on the consistent and uniform settings, while the doubly-robust risk is worst overall, especially in the reversed setting where $P(A|Y)$ is very small for the labeled tail classes, causing instability issues during training. In conclusion, our 2-stage approach is the best.

\input{tables/cifar10-ablation-1}