\section{Our approach}
\label{sec:method}
\subsection{Label shift Expectation Maximization}
\label{subsec:em}
When pseudo-labeling is applied naively, a classifier trained on the labeled set with class distribution $P(Y|A=1)$ may not do well on the unlabeled set that has a different class distribution $P(Y|A=0)$ thus resulting in incorrect pseudo labels for training and consequently confirmation bias \cite{confirmationbias}. We can not straightforwardly adapt to the unlabeled class distribution because it is unknown. In the following, we detail a likelihood maximization framework that eventually is shown to generalize pseudo-labeling to the label shift case. Using the indicator $A$, we can write the observed (or missing) data log-likelihood as
\begin{equation}
\begin{aligned}
L(\theta) &= \sum_{i=1}^{N_l} \log P(X=x_i,Y=y_i, A=1 | \theta)\\
&+ \sum_{i=N_l+1}^N \log P(X=x_i, A=0 | \theta),
\label{eq:likelihood}
\end{aligned}
\end{equation}
%\vspace*{-.05in}
where $\theta$ represents the parameter of the joint distribution $P(X,A,Y)$. This likelihood consists of the labeled term and an unlabeled term with a missing $Y$. Immediately, we can maximize $L(\theta)$ by writing the unlabeled term as a $Y$-marginalization of the joint as in \cite{arelabelsinformative}. As we will use EM to maximize $L(\theta)$, we apply Jensen inequality to the each term in the second sum using the posterior weight $\omega^t(x,y) = P(Y=y|X=x,A=0,\theta^t)$ where $\theta^t$ is value of $\theta$ in previous EM iteration, to get the lower bound
\vspace*{-.15in}
\begin{equation}
\begin{aligned}
&Q(\theta|\theta^t) = \sum_{i=1}^{N_l} \log P(X=x_i, Y=y_i, A=1 | \theta)\\
&+ \sum_{i=N_l+1}^N \sum_{c=1}^C \omega^t(x_i,c) \log P(X=x_i, Y=c, A=0 | \theta)
\label{eq:e-step}
\end{aligned}
\end{equation}
This is the E-step of EM and we have found the "pseudo-label" $\omega^t(x,c)$ for our unlabeled data, reducing the problem to a supervised learning one for the moment. Now we need to decide how to decompose the joint $P(X,Y,A|\theta)$ which decides what the parameter specification will be. It is natural that we use the invariance $P(X|Y,A) = P(X|Y)$ in \cref{eq:label-shift} to decompose $P(X|Y)P(Y|A)P(A)$, but this requires generative modeling for $P(X|Y)$. Instead, we use $P(A|Y)P(Y|X)P(X)$, which means we only need to learn a classifier $P(Y|X)$ and a finite-dimensional $P(A|Y)$, which are recipes for the posterior weight $\omega^t(x,y)$. With this, we get
\vspace*{-.2in}
\begin{equation}
\begin{aligned}
% Q(\theta|\theta^t) &= \sum_{i=1}^{N_l} \left[\log P(y_i | x_i, \theta) + \log P(a_i=1|y_i, \theta)\right] \\
% &+ \sum_{i=N_l+1}^{N} \sum_{c=1}^C P(y_i=c|x_i,a_i=0,\theta^t)\left[\log P(y_i=c|x_i, \theta) + \log P(a_i=0|y_i=c, \theta)\right]\\
Q(\theta|\theta^t) &= \sum_{i=1}^N \sum_{c=1}^C \gamma_i(c) \log P(Y=c|X=x_i,\theta)\\ 
&+ \sum_{c=1}^C \sum_{a=0}^1 \zeta_c(a) \log P(A=a|Y=c,\theta)
\label{eq:m-step}
\end{aligned}
\end{equation}
where $\gamma_i(c) = \one(y_i=c)$ for $i \le N_l$ and $P(Y=c|X=x_i,A=0,\theta^t)$ for $i > N_l$. $\zeta_c(1) = \sum_{i=1}^{N_l} \one(y_i=c)$ and $\zeta_c(0) = \sum_{i=N_l+1}^N P(Y=c|X=x_i,A=0,\theta^t)$. This means that maximizing $L(\theta|\theta^t)$ is equivalent to minimizing a sum of cross entropy losses. To compute the posterior weight $\omega^t(x,c)$, we use Bayes law:
\vspace*{-.1in}
\begin{equation}
\omega^t(x,c) \propto P(Y=c|X=x, \theta^t)P(A=0|Y=c, \theta^t)
\label{eq:e-step-posterior}
\end{equation}

\noindent In summary, the 2 steps of the EM are:

\medskip
\noindent
\textbf{E-step:} Given $P(Y=y|X=x,\theta^t)$ and $P(A=0|Y=y,\theta^t)$, set $\omega^t(x,c)$ according to \cref{eq:e-step-posterior}

\noindent
\textbf{M-step:} Given $\omega^t(x,c)$, find the new $P(Y|X,\theta)$ and $P(A|Y,\theta)$ by maximizing $Q(\theta|\theta^t)$. 


\subsection{Label-shift Fixmatch and SimPro}
\label{subsec:simpro}
Pseudo labeling methods such as Fixmatch has a deep connection with Expectation-Maximization. Indeed, \cref{eq:fixmatch-unlabeled-loss} without the 3 operators is just the unlabeled term in \cref{eq:e-step} and $P(Y|A=1) = P(Y|A=0) = P(Y|\textit{uniform})$. SimPro is a recent work which also derives an almost equivalent EM formula to ours. They used a similar E-step but also applied Fixmatch's confidence thresholding and augmentation. Their M-step parameterizes the distribution as 2 parameters $\frac{P(X|Y)}{P(X)}$ and $P(Y|A=0)$. This is just another decomposition of the unlabeled log-likelihood term in \cref{eq:e-step} up to a constant $P(A=0)$:
\begin{equation}
%\begin{aligned}
\frac{P(X|Y)}{P(X)} P(Y|A=0) 
\propto \frac{P(Y|X)}{P(Y)}P(Y) P(A=0|Y)
%\end{aligned}
\label{eq:equi-simpro}
\end{equation}
Instead of canceling out $P(Y)$, however, SimPro uses a logit adjustment loss \cite{logitadjustment} for the first term in \cref{eq:m-step}:
\vspace*{-.1in}
\begin{equation}
\begin{aligned}
- \sum_{i=1}^N \sum_{c=1}^C \gamma_i(c) \log &\Big\{P(Y=c|X=x_i,\textit{uniform},\theta) \\
&+ P(Y=c)\Big\}
\end{aligned}
\label{eq:simpro-la-loss}
\end{equation}
%\vspace*{-.1in}
As $P(Y=c)$ is unknown, they use its running estimate. The model is automatically logit adjusted to the uniform test distribution during training. In contrast, if the model is $P(Y|X)$ in \cref{eq:m-step}, we can apply post-hoc logit adjustment. As shown in \cite{logitadjustment}, the logit adjustment loss is often slightly better, and this is what we find experimentally as well. Other than this difference, we can recover the class distribution $P(Y|A)$ from the missingness mechanism $P(A|Y)$ and because $P(A)$ is known, so they are learned equivalently.

\subsection{Our 2-stage algorithm}
\label{subsec:2-stage}
% We have shown that we can generalize Fixmatch's pseudo labeling to the label shift case by applying the logit adjustment to the pseudo label in \cref{eq:e-step-posterior}. This adjustment term, the missingness mechanism $P(A|Y)$, can be learned at the same time by minimizing the second term in \cref{eq:m-step}. Since $P(Y|A)$ or equivalently $P(A|Y)$ is a finite-dimensional parameter, it can be much easier to estimate than the classification task $P(Y|X)$ and can be done first. Therefore, we propose to separate the estimation of the unlabeled class distribution from the classifier learning. In the first stage, we strive to achieve a good estimation of $P(Y|A=0)$. Then, we freeze this parameter and plug it into EM or SimPro, or any other approaches that also uses distribution alignment \cite{remixmatch}.

Figure~\ref{fig:algorithm} shows the overview of our algorithm. During training, we use the current model's predictions and adjust it to the unlabeled class distribution $P(Y|A=0)$. The quality our first-stage estimation of $P(Y|A=0)$ has a direct impact on the pseudo label accuracy, as highlighted in Theorem 3.1 of \cite{lsc}. Briefly, the error gap between the adjusted model and the Bayes-optimal model can be bounded by the sum of an error term induced by the model's performance on the training data and another error term induced by the quality of our unlabeled distribution estimation. Therefore, we should aim for the highest estimation quality we can get in the first stage. To this end, we present 3 possible estimators for the combined class distribution $P(Y)$, the outcome regression (OR) estimator, inverse probability weighted (IPW) estimator and the doubly robust (DR) estimator. The unlabeled class distribution $P(Y|A=0)$ can be recovered by noting that $P(Y) = \sum_a P(A=a) P(Y|A=a)$ and that $P(A)$ and the labeled class distribution $P(Y|A=1)$ is known. The OR estimator is simply the average of the model's predictions
% \vspace*{-.1in}
\begin{equation}
\Psi_{or}(c) = \frac{1}{N}\sum_{i=1}^N P(Y=c|X=x_i,\theta)
\end{equation}
where the summation takes both labeled and unlabeled data. 

Another estimator is the inverse probability weighted (IPW) estimator. Suppose that we have the ground truth missingness mechanism $P(A|Y)$, then we have the following identity:
\begin{equation}
P(Y=c) = \E_O\left[\frac{\one(A=1)}{P(A=1|Y)} \one(Y=c)\right]
\end{equation}
where $O$ is a random variable representing one observation from the combined dataset, which is complete ($O=(X,A=1,Y)$) if the datapoint is from the labeled set and missing $(X,A=0)$ if unlabeled set. The indicator $\one(A=1)$ means that we are not actually using ground truth labels from the unlabeled set, but up-weighting the existing labels from the labeled set by the missingness mechanism. Replacing expectation with sample average and $P(A=1|Y)$ with an estimation $P(A=1|Y,\theta)$, we get our IPW estimator of $P(Y)$, which depends on $\theta$
% \vspace*{-.15in}
\begin{equation}
\Psi_{ipw}(\theta)(c) = \frac{1}{N}\sum_{i=1}^N \frac{\one(a_i=1)}{P(A=1|Y=y_i,\theta)}\one(y_i=c)
\end{equation}


\noindent
\vspace*{-.3in}
\paragraph{Our doubly robust estimator}
It is worth noting that each estimator above (OR or IPW) uses only one part of the distribution, either $P(Y|X)$ or $P(A|Y)$. The DR estimator takes advantage of both of these quantities. It is
\vspace*{-.1in}
\begin{equation}
\label{eq:dr}
\begin{aligned}
&\Psi_{dr}(\theta)(c) = \frac{1}{N}\sum_{i=1}^N\Bigg[ P(Y=c|X=x_i,\theta) + \\
&\frac{\one(a_i=1)}{P(A=1|Y=y_i,\theta)}( \one(y_i=c) - P(Y=c|X=x_i,\theta) )\Bigg]
\end{aligned}
\end{equation}
$\Psi_{dr}(\theta)$ is called doubly-robust because, given either a correct $P(Y|X)$ or $P(A=1|Y)$, we will get an unbiased estimate of $P(Y)$. 
We need to learn both of these quantities from finite data which means their errors will propagate to the final estimation. However, this issue is addressed by the following optimality result.

\subsubsection{Theoretical guarantees for $\Psi_{dr}$}

We can show, under weak assumption on the quality of $\theta$, that $\Psi_{dr}$ has strong theoretical guarantees.
Let $o_p$ denote convergence in probability, define the $L_2(P)$ as $\|f\|_{L_2(P)} = (\int |f|^2 dP)^{1/2}$, where $P$ is the true distribution. We make the following assumption.
\begin{assumption}\label{assumption:4th-root-n}
Assume that both $P(Y|X,\theta)$ and $P(A=1|Y)$ converge at fourth-root-n rate i.e.
% \vspace*{-.1in}
\begin{equation}
\begin{aligned}
&\|P(Y|X,\theta) - P(Y|X)\|_{L_2(P)} = o_p(N^{-1/4})\\
&\|P(A=1|Y,\theta) - P(A=1|Y)\|_2 = o_p(N^{-1/4})
\end{aligned}
\end{equation} 
\end{assumption}
\noindent
\textbf{Justification:}
These assumptions (fourth-root-n rate of convergence) have been proven for neural networks \cite{riesznet}, which are consistent because of the universal approximation theorem, but tend to be biased because of regularization \cite{dml}.

We have the following optimality result:
\begin{theorem}  Under the assumption~\cref{assumption:4th-root-n} the DR estimator $\Psi_{dr}$ is asymptotically normal with 0-mean and the efficient influence function's variance:
% \vspace*{-.1in}
\begin{equation}
\sqrt{N}(\Psi_{dr}(\theta)(c) - P(Y=c)) \rightsquigarrow \mathcal{N}(0, \E[\phi(O)(c)^2])
\end{equation}
\label{theorem:dr}
\end{theorem}
The proof of theorem \cref{theorem:dr} is deferred to the supplemental material. This theorem states that $\Psi_{dr}$ is the most efficient regular estimator. 
Informally speaking, regularity means any other estimator that performs better than $\Psi_{dr}$ at one point, must do considerably worse at nearby points.%
% \footnote{Formally, an estimator $\Psi(\theta)$ of $\Psi(\theta_0)$ where $\theta_0$ is the truth is said to be regular if for every $h$
% \begin{equation}
% \sqrt{N} \left( \Psi(\theta) - \Psi\left( \theta_0 + \frac{h}{\sqrt{N}} \right) \right) \stackrel{\theta_0 + h / \sqrt{N}}{\rightsquigarrow} L_{\theta_0}
% \end{equation}
% where the convergence is in distribution under the law of  $\theta_0 + \frac{h}{\sqrt{N}}$, and $L$ is some asymptotic distribution that only depends on $\theta_0$ \cite{asymptoticstatistics}}


% As the efficient influence function $\phi$ has the smallest variance out of all regular and asymptotically linear (RAL) estimators of $P(Y)$, $\Psi_{dr}(\theta)$ is the most efficient RAL estimator. 
% }
To put this theorem into perspective, the sample mean $\frac{1}{n}\sum_i z_i$ is the most efficient estimator of the mean of a random variable $Z$, of which $z_i$s are unbiased samples. The OR estimator $\Psi_{or}$, which looks like a sample mean of $P(Y|X,\theta)$, is however potentially biased as $\theta$ is the model's approximation of the truth using finite data, and this bias slows the convergence of $\Psi_{or}$ if it does not go away quickly enough, for example if the first equation in assumption~\ref{assumption:4th-root-n} holds \cite{dml}. The same thing can happen to $P(A=1|Y,\theta)$. Thus, \cref{theorem:dr} shows that we can get an estimation quality as if we were using unbiased samples to estimate the mean. 

