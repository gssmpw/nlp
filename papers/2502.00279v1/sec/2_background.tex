\section{Background and Related Work}
\label{sec:background}

\textbf{Notation} We write the random variable $X \in \mathcal{X}$ for the feature(s) and  $Y \in \{1,\dots, C\}$ for the class among $C$ possible classes. We are given a labeled dataset $D_l = \{x_i,y_i\}_{i=1}^{N_l}$ and an unlabeled dataset $D_u = \{x_i\}_{i=N_l+1}^N$, where $x_i$ and $y_i$ are realizations of $X$ and $Y$. The training dataset is $D_t = D_l \cup D_u$. 
The auxiliary variable $A$ takes binary values and selects between the different class distributions $P(Y|A)$; let $A=1$ if the datapoint is in the labeled set and $A=0$ in the unlabeled set. 
Therefore $P(Y|A=0)$ is the class distribution the unlabeled set. The \textit{combined} class distribution $P(Y) = P(A=0)P(Y|A=0) + P(A=1)P(Y|A=1)$ is the class distribution of the combined dataset. For convenience, we also denote $P(Y|\textit{uniform})=1/C$ everywhere to be the uniform class distribution, noting that it is not another value of $A$. We assume that the class distribution of the test set is uniform throughout this paper.

\paragraph{Long-tailed Semi-supervised learning} is the intersection between long-tailed learning \cite{longtailedlearning} and semi-supervised learning \cite{chapelle2009semi}, and attempts to deal with two key real world problems: class distribution in the wild is often long-tailed with many classes having few samples; and the unlabeled data dwarfs the labeled data because of the advent of the web and the significant cost of large-scale manual labeling efforts. Pseudo labeling \cite{pseudolabeling, mixmatch, google-selftraining, temporalensembling} has become one of the prominent approaches in semi-supervised learning, and has been extended to the long-tailed case \cite{crest, abc}, although the unlabeled class distribution was assumed to be the same as the labeled class distribution \cite{remixmatch}. More recent work has tackled the unknown distribution case \cite{dcssl, rda, onnonrandommissinglabels, prg4ssl, acr, simpro, cpe, boat}.
% Pseudo labeling in the balanced case is also connected to semi-supervised EM \cite{entropyminimization}, which naturally raises the question whether a similar connection exists for the different and unknown unlabeled distribution case. 

\paragraph{(Balanced) Pseudo-labeling.}
Semi-supervised learning methods use a regularization loss on the unlabeled data in addition to the classification loss on the labeled data. A simple technique is to use the model's own predictions on the unlabeled data. Specifically, FixMatch \cite{fixmatch} keeps the maximum predictions that also fall above a certain confidence threshold and convert them into one-hot labels (operator $\delta$), which is called a pseudo label. For example, given a confidence threshold of $0.8$, a binary prediction $[0.1, 0.9]$ will be mapped to $[0, 1]$ while $[0.4, 0.6]$ to $[0, 0]$ under the operator $\delta$. FixMatch then minimizes the cross entropy loss between a strongly augmented version  and the pseudo label of a weakly augmented version  of the same unlabeled image:
\begin{equation}
L_u = - \sum_{i=N_l+1}^{N} \sum_{c=1}^C \delta (P(Y|\alpha(x_i)))_c \log P(Y=c|G(x_i))
\label{eq:fixmatch-unlabeled-loss}
\end{equation}
where $c$ is the class, $G$ is the strong augmentation, and $\alpha$ is the weak augmentation. FixMatch is simple and performant. However, it suffers when labeled and unlabeled class distributions are different, which label shift approach tries to address.

\paragraph{Label shift and logit-adjustment.} Label shift assumes that the probability of $X$ given $Y$ is unchanged:
\begin{equation}
P(X|Y,A) = P(X|Y)
\label{eq:label-shift}
\end{equation} 
i.e. feature $X$ is conditionally independent of the variable $A$. The posterior change in $P(Y|X,A)$ results from the difference between the class distributions i.e. $P(Y|A=0) \neq P(Y|A=1)$. 
If the class distributions are known, logit adjustment can be used to convert a classifier of one class distribution to another.
When label shift occurs between two datasets, classifiers performing well on one dataset may not perform well on the other.
For example, to adapt the labeled class distribution $P(Y|A=1)$ to the test class distribution $P(Y|\textit{uniform})$, we can use Bayes formula to get:
\begin{equation}
P(Y|X,\textit{uniform}) \propto P(Y|X,A=1) \frac{P(Y|\textit{uniform})}{P(Y|A=1)}
\label{eq:logit-adjustment}
\end{equation}
which is the basis of the post-hoc logit adjustment formula for long-tailed learning.

Label shift is the natural assumption in imbalanced / long-tailed learning where the target distribution is the uniform test distribution. Logit adjustment \cite{logitadjustment}, implicitly using this assumption, relies on the formula \cref{eq:logit-adjustment} to correct label shift in long-tailed data. When the test distribution is unknown, label shift adaptation methods exist that can estimate the unknown test distribution \cite{mlls, mllsishardtobeat, bbse, rlls} when given a good classifier of the source data. It is possible therefore to train on the labeled set and use a label shift adaptation method to estimate the unlabeled class distribution. This procedure is best suited to label shift test-time adaptation \cite{ttsla, minh-gpa} where the unlabeled data is not available during model training. In contrast, when additional unlabeled data is available, semi-supervised EM gives significantly better class distribution estimation.

\paragraph{Non-ignorable missingness.} This is a variant of missing data problems where the missing indicator $A$ can depend on both feature $X$ and outcome $Y$ \cite{rubin-missingdata}. 
The dependence on $Y$ distinguishes this variant from the standard ignorable missingness (missing at random) assumption \cite{tsiatis-missingdata}. The label shift assumption \cref{eq:label-shift} further assumes that only $Y$ causes $A$, and this assumption is sufficient to \textit{identify} the true data distribution, meaning that no two distributions can generate our missing data \cite{labelshift-nonignorable, arelabelsinformative}.


\paragraph{Doubly robust (DR) estimation} 
\label{subsec:related-work-dr}
This approach has roots in semi-parametric efficiency theory \cite{kennedy-dr, dml}. The most successful application of DR is the estimation of the average treatment effect in causal inference \cite{tsiatis-missingdata, pham2023stable}, which is an example of ignorable missingness. Recently doubly machine learning \cite{dml, riesznet} takes double robustness further by showing that powerful machine learning methods such as neural networks can be used to deal with high-dimensional and complex data while at the same time making valid inference about the target statistics. The applications of DR in modeling more complex data than traditionally studied in statistics have recently gained significant interest \cite{dragonnet, riesznet, zhang2023towards}. We contribute to this line of work, but furthermore shows that we can plug in this estimation to improve the final classification itself.

Our work builds and improves on \cite{simpro}. Specifically, we show in \cref{subsec:simpro} that it is a reparameterization of the semi-supervised EM algorithm in \cref{subsec:em}, and we use it as the training method for both stages of our algorithm.
Our work is also close to \cite{arelabelsinformative, onnonrandommissinglabels} who also note the connection to non-ignorable missingness and propose doubly robust estimation of the loss.
This loss remains consistent even when the pseudo labels are arbitrarily bad, in a similar spirit to \cite{schmutz-drloss, drst}, as long as the missingness mechanism is correct. Thus they try to safeguard against wrong un-adjusted labels. We on the other hand try to improve the label's quality via EM and adjustment by the doubly robust estimation of the unlabeled class distribution.
An important weaknesses of the doubly robust loss \cite{arelabelsinformative} is that it involves inverse-weighting \cite{cui-effective} which is prone to unstable training \cite{balanced-meta-softmax}. We provide more detail about the doubly-robust risk in \cref{subsec:dr-risk}, and experimentally compare with it in \cref{subsec:ablation-1}