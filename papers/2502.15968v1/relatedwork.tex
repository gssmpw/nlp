\section{Related Works}
\textbf{On-policy methods.}
On-policy algorithms aim at improving the policy performance monotonically between every update. The work~\cite{kakade2002approximately} developing Conservative Policy Iteration (CPI) for the first time theoretically introduced a policy improvement lower bound that can be approximated by using samples from the present policy. In this regard, trust-region policy optimization (TRPO)~\cite{schulman2015trust} and PPO have become quite popular baseline algorithms. TRPO solves a trust-region optimization problem to approximately obtain the policy improvement by imposing a Kullback-Leibler (KL) divergence constraint, which
% leverages the second-order information in the policy gradient but
requires solving a quadratic programming that may be compute-intensive. On the contrary, PPO achieves a similar objective by adopting a clipping mechanism to constrain the latest policy not to deviate far from the previous one during the update. Their satisfactory performance in different applications~\cite{hu2019towards,lele2020stock,zhang2022ppo,dutta2022survey,bahrpeyma2023application,nguyen2024modelling,zhang2020power} triggers considerable interest in better understanding these methods~\cite{jin2023stationary} and developing new policy optimization variants~\cite{huang2021neural}.
% In~\cite{engstrom2020implementation} and~\cite{andrychowicz2020matters}, substantial empirical analysis was performed for TRPO and PPO in terms of diverse implementation choices and tricks as well as code-level optimizations~\cite{liu2020towards}. Azizzadenesheli et al.~\cite{azizzadenesheli2018trust} also extended TRPO to partially observable Markov decision processes (POMDPs), while Lazic et al. investigated the optimization issues for TRPO in their work~\cite{lazic2021optimization}. 
% Recently, Huang et al.~\cite{huang2021neural} showed the neural PPO-Clip attained the global optimality in a Hinge loss perspective. 
% A few other works~\cite{sun2022you,chen2018adaptive,yao2021hinge,wang2023smooth,byun2020proximal} either studied the clipping mechanism or proposed novel variants to boost the vanilla PPO performance. 
% A more recent work~\cite{jin2023stationary} successfully showed the staionary point convergence for the popular PPO-Clip variant, which facilitates the theoretical understanding of PPO.
% how PPO works and paves the way for more efficient on-policy optimization algorithm design. 
Albeit numerous attempts have been made in the above works, the high sample complexity due to the on-policy behavior of PPO and its variants still obstructs efficient applications to real-world continuous control environments, which demands the connection with off-policy methods.

\textbf{Off-policy methods.} To address the high sample complexity issue in on-policy methods, a common approach is to reuse the samples generated by prior policies, which was devised in~\cite{hester2018deep,mnih2013playing}. Favored off-policy methods such as deep deterministic policy gradient (DDPG)~\cite{lillicrap2015continuous}, twin delayed DDPG (TD3)~\cite{fujimoto2018addressing} and soft actor-critic (SAC)~\cite{haarnoja2018soft} fulfilled this goal by employing a replay buffer to store historical data and sampling from it for computing the policy updates. As mentioned before, such approaches could cause data distribution drift due to the difference between the data distributions of current and prior policies. This work will include an implementation trick to address this issue to a certain extent. Kallus and Uehara developed a statistically efficient off-policy policy gradient (EOPPG) method~\cite{kallus2020statistically} and showed that it achieves an asymptotic lower bound that existing off-policy policy gradient approaches failed to attain. Other works such as nonparametric Bellman equation~\cite{tosatto2020nonparametric} and state distribution correction~\cite{kallus2020statistically} were also done with off-policy policy gradient. 
% Another line of work of interest in off-policy model-free reinforcement learning is value-based approaches such as deep Q-learning~\cite{mnih2013playing}, while we refer interested readers to excellent surveys~\cite{arulkumaran2017deep,wang2022deep} for more details. 

\textbf{Combination of on- and off-policy methods.} Making efficient use of on-policy and off-policy schemes is pivotal to designing better model-free reinforcement learning approaches. An early work merged them together to come up with the interpolated policy gradient~\cite{gu2017interpolated} for improving sample efficiency. Another work~\cite{fakoor2020p3o} developed Policy-on-off PPO to interleave off-policy updates with on-policy updates, which controlled the distance between the behavior and target policies without introducing any additional hyperparameters. Specifically, they utilized a complex gradient estimate to account for on-policy and off-policy behaviors, which may result in larger computational complexity in low-sample scenarios.
% To stabilize policy optimization,
% Touati et al.~\cite{touati2020stable} leveraged off-policy divergence regularization in TRPO and PPO to improve the ultimate performance. 
To compensate data inefficiency, Liang et al.~\cite{liang2021ptr} incorporated prioritized experience replay into PPO by proposing a truncated importance weight method to overcome the high variance and designing a policy improvement loss function for PPO under off-policy conditions. A more recent work~\cite{chen2023sufficiency} probed the insufficiency of PPO under an off-policy measure and explored in a much larger policy space to maximize the CPI objective.
% Other works~\cite{zheng2021variance,meng2023off,zhao2023policy,cheng2024proximal} have investigated the tradeoff between sample efficiency and data distribution drift. 
% Other works such as the scrutiny of variance reduction in PPO~\cite{zheng2021variance} with experience replay, Off-policy PPO~\cite{meng2023off}, sample adaptive reuse and dual-clipping~\cite{zhao2023policy} and PPO with off-policy advantage from prior policies~\cite{cheng2024proximal} have been developed to maintain the tradeoff between sample efficiency and data distribution drift. 
The most related work to ours is~\cite{queeney2021generalized}, where the authors proposed a generalized PPO with off-policy data from prior policies and derived a generalized policy improvement lower bound. They utilized directly the past trajectories right before the present one instead of a replay buffer, which still maintains a weakly on-policy behavior. However, their method may suffer from poor performance in sparse reward environments.