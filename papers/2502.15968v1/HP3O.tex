%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[para,online,flushleft]
{threeparttable}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{multirow}
\usepackage{graphicx}

\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage{subcaption}
\setlength{\abovedisplayskip}{0.2pt}
\setlength{\belowdisplayskip}{0.2pt}


%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

% \journal{Neural Networks}
\journal{IEEE}
\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Enhancing PPO with Trajectory-Aware Hybrid Policies}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[inst1,inst2]{Qisai Liu}
\author[inst4]{Zhanhong Jiang}
\author[inst1]{Hsin-Jung Yang}
\author[inst3]{Mahsa Khosravi}
\author[inst1,inst2]{Joshua R. Waite}
\author[inst1,inst2,inst4]{Soumik Sarkar}



\affiliation[inst1]{organization={Department of Mechanical Engineering},%Department and Organization
            addressline={Iowa State University}, 
            city={Ames},
            postcode={50011}, 
            state={Iowa},
            country={United States}}

\affiliation[inst2]{organization={Department of Computer Science},%Department and Organization
            addressline={Iowa State University}, 
            city={Ames},
            postcode={50011}, 
            state={Iowa},
            country={United States}}

\affiliation[inst3]{organization={Department of Industrial and Manufacturing Systems Engineering},%Department and Organization
            addressline={Iowa State University}, 
            city={Ames},
            postcode={50011}, 
            state={Iowa},
            country={United States}}

\affiliation[inst4]{organization={Translational AI Center},%Department and Organization
            addressline={Iowa State University}, 
            city={Ames},
            postcode={50011}, 
            state={Iowa},
            country={United States}}


%% Abstract
\begin{abstract}
%% Text of abstract
Proximal policy optimization (PPO) is one of the most popular state-of-the-art on-policy algorithms that has become a standard baseline in modern reinforcement learning with applications in numerous fields. Though it delivers stable performance with theoretical policy improvement guarantees, high variance, and high sample complexity still remain critical challenges in on-policy algorithms. To alleviate these issues, we propose Hybrid-Policy Proximal Policy Optimization (HP3O), which utilizes a trajectory replay buffer to make efficient use of trajectories generated by recent policies. Particularly, the buffer applies the "first in, first out" (FIFO) strategy so as to keep only the recent trajectories to attenuate the data distribution drift. A batch consisting of the trajectory with the best return and other randomly sampled ones from the buffer is used for updating the policy networks. The strategy helps the agent to improve its capability on top of the most recent best performance and in turn reduce variance empirically. We theoretically construct the policy improvement guarantees for the proposed algorithm. HP3O is validated and compared against several baseline algorithms using multiple continuous control environments. Our code is available \href{https://anonymous.4open.science/r/HP30-EB61/HP3O_train.py}{here}.
\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% \includegraphics[width=1\textwidth]{Figures/HP3O_schematic_diagram.pdf}
% \end{graphicalabstract}



%%Research highlights
% \begin{highlights}
%     \item We propose a Hybrid-Policy PPO (HP3O) to combine the advantageous features of on-policy and off-policy techniques to improve sample efficiency and reduce variance. 
%     % We also introduce another variant termed HP3O+ that leverages a new baseline to enhance the model performance. Please see Table~\ref{table:comparison} for a qualitative comparison between the proposed and existing methods.
%     % To further empirically boost the performance, we also develop prioritized trajectory replay for OPPO-BT. 
%     \item We theoretically construct the policy improvement lower bounds for HP3O and its variant HP3O+. 
%     % induces a value penalty term in the lower bound, which helps reduce the variance during training.
%     \item We perform extensive experiments to show the effectiveness of HP3O/HP3O+ across a few continuous control environments. 
%     % Empirical evidence demonstrates that our proposed algorithms are either comparable to or outperform on-policy baselines. Though off-policy techniques such as soft actor-critic (SAC) may still have better final returns for most tasks, our hybrid-policy algorithms have significantly more advantages in terms of run time complexity.
% \end{highlights}

%% Keywords
\begin{keyword}
Hybrid policy \sep proximal policy optimization \sep variance reduction \sep sample efficiency \sep best trajectory
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section

\section{Introduction}
Model-free reinforcement learning~\cite{liu2021policy} has demonstrated significant success in many different application areas, such as building energy systems~\cite{biemann2021experimental}, urban driving~\cite{toromanoff2020end,saxena2020driving}, radio networks~\cite{kaur2020energy}, robotics~\cite{polydoros2017survey}, and medical image analysis~\cite{hu2023reinforcement}. In particular, on-policy reinforcement learning approaches such as proximal policy optimization (PPO)~\cite{schulman2017proximal,chang2023learning} provide stable performance along with theoretical policy improvement guarantees that involve a lower bound~\cite{kakade2002approximately} on the expected performance loss which can be approximated using the generated samples from the current policy. These guarantees are theoretically quite attractive and mathematically elegant, but the requirement of on-policy data and the high variance nature demands significant data to be collected between every update, inevitably causing the issue of high sample complexity and the behavior of slow learning.

Off-policy algorithms~\cite{zanette2023realizability,prudencio2023survey}, on the other hand, alleviate some of these issues as they can leverage a replay buffer to store samples that enable more efficient policy updates by reusing these samples. While the off-policy approach leads to better sample efficiency, it causes another problem called data distribution drift~\cite{zhang2020reinforcement,lesort2021understanding}, and most studies~\cite{lillicrap2015continuous,dankwa2019twin} have just overlooked this issue. Furthermore, off-policy methods also suffer from high variance and even difficulty in convergence~\cite{lyu2020variance} due to the exploration in training. Mitigating
% Thus, various implementation tricks and extensive hyperparameter tuning are required to stabilize the learning process. 
this issue~\cite{bjorck2021high} still remains challenging due to the high variations of stored samples in the traditional replay buffer design. However, it has been receiving considerable attention in recent studies~\cite{liu2020improved,xu2019sample}. Numerous previous attempts~\cite{zhang2021convergence,xu2020improved,papini2018stochastic} took inspiration from supervised learning~\cite{wang2013variance,johnson2013accelerating} and specifically made adjustments to the estimation of policy gradients to achieve variance reduction. However, this involves auxiliary variables and complex estimation techniques, resulting in a more complicated learning process. Another simple strategy to attenuate high variance is to leverage the advantage function involving a baseline~\cite{jin2023stationary,mei2022role,wu2018variance}, which can be estimated by a parameterized model. Nevertheless, when the sampled data from the buffer has a large distribution drift, learning the parameterized model can be defective, triggering a poor advantage value. This naturally leads to the question:
\begin{center}
    \textit{Can we design a hybrid-policy algorithm by assimilating the low sample complexity from off-policy algorithms into on-policy PPO for variance reduction?}
\end{center}
\textbf{Contributions.} We provide an affirmative answer to the above question. In this work, we blend off-policy and on-policy approaches to balance the trade-off between sample efficiency and training stability. Specifically, we focus primarily on mitigating underlying issues of PPO by using a trajectory replay buffer. In contrast with traditional buffers that keep appending all generated experiences, we use a "first in, first out" (FIFO) strategy to keep only the recent trajectories to attenuate the data distribution drift. A batch consisting of the trajectory with the best return (a.k.a., best trajectory, $\tau^*$) and other randomly sampled ones from the buffer is used for updating the policy networks. This strategy helps the agent to improve its capability on top of the most recent `best performance' and in turn to also reduce variance. 
Additionally, we define a new baseline which is estimated from the best trajectory selected from the replay buffer. Such a baseline evaluates how much better the return is by selecting the present action than the most recent best one, which intuitively encourages the agent to further improve the performance. More technical detail will be discussed in Section~\ref{algorithm}.
% One popular variant of PPO called PPO-Clip~\cite{jin2023stationary,huang2023ppo} requires a value network to approximate the state value as the baseline, while for the new baseline in our study, it can be calculated much faster by using the best action trajectory. 
Specifically, our contributions are as follows.
\begin{itemize}
    \item We propose a novel variant of PPO, called Hybrid-Policy PPO (HP3O), that combines the advantageous features of on-policy and off-policy techniques to improve sample efficiency and reduce variance. We also introduce another variant termed HP3O+ that leverages a new baseline to enhance the model performance. Please see Table~\ref{table:comparison} for a qualitative comparison between the proposed and existing methods.
    % To further empirically boost the performance, we also develop prioritized trajectory replay for OPPO-BT. 
    \item We theoretically construct the policy improvement lower bounds for the proposed algorithms. HP3O provably shows a new lower bound where policies are not temporally correlated, while HP3O+ induces a value penalty term in the lower bound, which helps reduce the variance during training.
    \item We perform extensive experiments to show the effectiveness of HP3O/HP3O+ across a few continuous control environments. Empirical evidence demonstrates that our proposed algorithms are either comparable to or outperform on-policy baselines. Though off-policy techniques such as soft actor-critic (SAC) may still have better final returns for most tasks, our hybrid-policy algorithms have significantly more advantages in terms of run time complexity.
\end{itemize}

\begin{table}[htp]
% \captionsetup{skip=-5pt}
\caption{Qualitative comparison with PPO and its relevant variants}
\begin{center}
\begin{threeparttable}
\begin{tabular}{c c c c c}
    \toprule
    \textbf{Method} & \textbf{T.B.} & \textbf{On/off-policy} & \textbf{T.G.}\\ \midrule
      \texttt{PPO-Clip\cite{jin2023stationary}}   & \xmark                   &  \xmark   & \cmark\\
      \texttt{PTR-PPO\cite{liang2021ptr}}   & \cmark                      &  \cmark  &\xmark         \\ 
      \texttt{GEPPO\cite{queeney2021generalized}}   & \xmark                      &  \cmark&\cmark           \\
      \texttt{Policy-on-off PPO\cite{fakoor2020p3o}}   & \xmark                     &  \cmark&\xmark           \\
    \texttt{P3O\cite{chen2023sufficiency}}   & \xmark                     &  \xmark&\xmark           \\
      \texttt{Off-policy PPO\cite{meng2023off}}   & \xmark                     &  \cmark &\cmark           \\
      \hline
        \texttt{HP3O(+) (ours)}   & \cmark&   \cmark&\cmark\\
      \bottomrule
      
\end{tabular}
\begin{tablenotes}
T.B.: trajectory buffer; T.G.: theoretical guarantee.
\end{tablenotes}
\end{threeparttable}
\end{center}
\label{table:comparison}
\end{table}

\section{Related Works}
\textbf{On-policy methods.}
On-policy algorithms aim at improving the policy performance monotonically between every update. The work~\cite{kakade2002approximately} developing Conservative Policy Iteration (CPI) for the first time theoretically introduced a policy improvement lower bound that can be approximated by using samples from the present policy. In this regard, trust-region policy optimization (TRPO)~\cite{schulman2015trust} and PPO have become quite popular baseline algorithms. TRPO solves a trust-region optimization problem to approximately obtain the policy improvement by imposing a Kullback-Leibler (KL) divergence constraint, which
% leverages the second-order information in the policy gradient but
requires solving a quadratic programming that may be compute-intensive. On the contrary, PPO achieves a similar objective by adopting a clipping mechanism to constrain the latest policy not to deviate far from the previous one during the update. Their satisfactory performance in different applications~\cite{hu2019towards,lele2020stock,zhang2022ppo,dutta2022survey,bahrpeyma2023application,nguyen2024modelling,zhang2020power} triggers considerable interest in better understanding these methods~\cite{jin2023stationary} and developing new policy optimization variants~\cite{huang2021neural}.
% In~\cite{engstrom2020implementation} and~\cite{andrychowicz2020matters}, substantial empirical analysis was performed for TRPO and PPO in terms of diverse implementation choices and tricks as well as code-level optimizations~\cite{liu2020towards}. Azizzadenesheli et al.~\cite{azizzadenesheli2018trust} also extended TRPO to partially observable Markov decision processes (POMDPs), while Lazic et al. investigated the optimization issues for TRPO in their work~\cite{lazic2021optimization}. 
% Recently, Huang et al.~\cite{huang2021neural} showed the neural PPO-Clip attained the global optimality in a Hinge loss perspective. 
% A few other works~\cite{sun2022you,chen2018adaptive,yao2021hinge,wang2023smooth,byun2020proximal} either studied the clipping mechanism or proposed novel variants to boost the vanilla PPO performance. 
% A more recent work~\cite{jin2023stationary} successfully showed the staionary point convergence for the popular PPO-Clip variant, which facilitates the theoretical understanding of PPO.
% how PPO works and paves the way for more efficient on-policy optimization algorithm design. 
Albeit numerous attempts have been made in the above works, the high sample complexity due to the on-policy behavior of PPO and its variants still obstructs efficient applications to real-world continuous control environments, which demands the connection with off-policy methods.

\textbf{Off-policy methods.} To address the high sample complexity issue in on-policy methods, a common approach is to reuse the samples generated by prior policies, which was devised in~\cite{hester2018deep,mnih2013playing}. Favored off-policy methods such as deep deterministic policy gradient (DDPG)~\cite{lillicrap2015continuous}, twin delayed DDPG (TD3)~\cite{fujimoto2018addressing} and soft actor-critic (SAC)~\cite{haarnoja2018soft} fulfilled this goal by employing a replay buffer to store historical data and sampling from it for computing the policy updates. As mentioned before, such approaches could cause data distribution drift due to the difference between the data distributions of current and prior policies. This work will include an implementation trick to address this issue to a certain extent. Kallus and Uehara developed a statistically efficient off-policy policy gradient (EOPPG) method~\cite{kallus2020statistically} and showed that it achieves an asymptotic lower bound that existing off-policy policy gradient approaches failed to attain. Other works such as nonparametric Bellman equation~\cite{tosatto2020nonparametric} and state distribution correction~\cite{kallus2020statistically} were also done with off-policy policy gradient. 
% Another line of work of interest in off-policy model-free reinforcement learning is value-based approaches such as deep Q-learning~\cite{mnih2013playing}, while we refer interested readers to excellent surveys~\cite{arulkumaran2017deep,wang2022deep} for more details. 

\textbf{Combination of on- and off-policy methods.} Making efficient use of on-policy and off-policy schemes is pivotal to designing better model-free reinforcement learning approaches. An early work merged them together to come up with the interpolated policy gradient~\cite{gu2017interpolated} for improving sample efficiency. Another work~\cite{fakoor2020p3o} developed Policy-on-off PPO to interleave off-policy updates with on-policy updates, which controlled the distance between the behavior and target policies without introducing any additional hyperparameters. Specifically, they utilized a complex gradient estimate to account for on-policy and off-policy behaviors, which may result in larger computational complexity in low-sample scenarios.
% To stabilize policy optimization,
% Touati et al.~\cite{touati2020stable} leveraged off-policy divergence regularization in TRPO and PPO to improve the ultimate performance. 
To compensate data inefficiency, Liang et al.~\cite{liang2021ptr} incorporated prioritized experience replay into PPO by proposing a truncated importance weight method to overcome the high variance and designing a policy improvement loss function for PPO under off-policy conditions. A more recent work~\cite{chen2023sufficiency} probed the insufficiency of PPO under an off-policy measure and explored in a much larger policy space to maximize the CPI objective.
% Other works~\cite{zheng2021variance,meng2023off,zhao2023policy,cheng2024proximal} have investigated the tradeoff between sample efficiency and data distribution drift. 
% Other works such as the scrutiny of variance reduction in PPO~\cite{zheng2021variance} with experience replay, Off-policy PPO~\cite{meng2023off}, sample adaptive reuse and dual-clipping~\cite{zhao2023policy} and PPO with off-policy advantage from prior policies~\cite{cheng2024proximal} have been developed to maintain the tradeoff between sample efficiency and data distribution drift. 
The most related work to ours is~\cite{queeney2021generalized}, where the authors proposed a generalized PPO with off-policy data from prior policies and derived a generalized policy improvement lower bound. They utilized directly the past trajectories right before the present one instead of a replay buffer, which still maintains a weakly on-policy behavior. However, their method may suffer from poor performance in sparse reward environments. 


\section{Problem formulation and preliminary}
\textbf{Markov decision process.} In this context, we consider an infinite-horizon Markov Decision Process (MDP) with discounted reward defined by the tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, p, r, \rho_0, \gamma)$, where $\mathcal{S}$ indicates the set of states, $\mathcal{A}$ signifies the set of actions, $p:\mathcal{S}\times\mathcal{A}\to \mathcal{S}$ is the transition probability function, $r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the reward function, $\rho_0$ is the initial state distribution of environment, and $\gamma$ is the discount factor. 
In this study, the agent's policy is a stochastic mapping represented by $\pi:\mathcal{S}\to\mathcal{A}$. Reinforcement learning aims at choosing a policy that is able to maximize the expected discounted cumulative rewards $J(\pi)=\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^\infty\gamma^tr(s_t,a_t)]$, where $\tau\sim\pi$ indicates a trajectory sampled according to $s_0\sim\rho_0$, $a_t\sim\pi(\cdot|s_t)$, and $s_{t+1}\sim p(\cdot|s_t, a_t)$. We denote by $d^\pi(s)$ a normalized discounted state visitation distribution such that $d^\pi(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^t\mathbb{P}(s_t=s|\rho_0, \pi, p)$. Hence, the corresponding normalized discounted state-action visitation distribution can be expressed as $d^\pi(s,a)=d^\pi(s)\pi(s,a)$. Additionally, we define the state value function of the policy $\pi$ as $V^\pi(s)=\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^\infty]\gamma^tr(s_t,a_t)|s_0=s]$, the state-action value function, i.e., $Q$-function, as $Q^\pi(s,a)=\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^\infty]\gamma^tr(s_t,a_t)|s_0=s, a_0=a]$, and the critical advantage function as $A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$. 
% This intuitively evaluates how much better it is by taking action $a$ than the average.

\textbf{Policy improvement guarantee.} The foundation of numerous on-policy policy optimization algorithms is built upon a classic policy improvement lower bound originally established in~\cite{kakade2002approximately}. With different scenarios~\cite{schulman2015trust,achiam2017constrained,dai2021refined}, the lower bound was refined to reflect diverse policy improvements, which can be estimated by using the samples generated from the latest policy. For completeness, we present in Lemma~\ref{lemma_1} the policy improvement lower bound from~\cite{achiam2017constrained}. 
\begin{lemma}\label{lemma_1} (Corollary 1 in~\cite{achiam2017constrained})
    Suppose that the current time step is $k$ and that the corresponding policy is $\pi_k$. For any future policy $\pi$, the following relationship holds true:
    \begin{equation}\label{eq_1}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_k}}[\frac{\pi(a|s)}{\pi_k(a|s)}A^{\pi_k}(s,a)]-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{(s,a)\sim d^{\pi_k}}[\delta(\pi,\pi_k)(s)],
    \end{equation}
where $C^\pi_{\pi_k}=\textnormal{max}_{s\in\mathcal{S}}|\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]|$ and $\delta(\pi,\pi_k)(s)$ is the total variation distance between the distributions $\pi(\cdot|s)$ and $\pi_k(\cdot|s)$.
\end{lemma}
Lemma~\ref{lemma_1} implies that the policy improvement lower bound consists of the surrogate objective loss and the penalty term, which can be maximized by choosing a certain new policy $\pi_{k+1}$ to guarantee the policy improvement. However, directly maximizing such a lower bound could be computationally intractable if the next policy $\pi_{k+1}$ deviates far from the current one. Unless additional constraint is imposed such as a trust region in TRPO~\cite{schulman2015trust}, which unfortunately requires a complex second-order method to solve the optimization problem. Hence, PPO developed a simple yet effective heuristic for achieving this.

\textbf{Proximal policy optimization.} PPO has become a default baseline in a variety of applications, as mentioned above. It is favored because of its strong performance and simple implementation with sound theoretical motivation given by the policy improvement lower bound. Intuitively, PPO attempts to constrain the new policy close to the present one with a \textit{clipping} heuristic, which results in the most popular variant, PPO-clip~\cite{jin2023stationary}. Particularly, the following objective is solved at every policy update:
\begin{equation}\label{eq_2}
    \mathcal{L}^{clip}_k(\pi)=\mathbb{E}_{(s,a)\sim d^{\pi_k}}[\textnormal{min}(\frac{\pi(a|s)}{\pi_k(a|s)}A^{\pi_k}(s,a),\textnormal{clip}(\frac{\pi(a|s)}{\pi_k(a|s)},1-\epsilon,1+\epsilon)A^{\pi_k}(s,a))],
\end{equation}
where $\textnormal{clip}(a,b,c)=\textnormal{min}(\textnormal{max}(a,b),c)$. The clipping function plays a critical role in this objective as it consistently enforces the probability ratio between the current and next policies in a reasonable range between $[1-\epsilon, 1+\epsilon]$. The outer minimization in Eq.~\ref{eq_2} provides the lower bound guarantee for the surrogate loss in Eq.~\ref{eq_1}. In practice, one can set a small learning rate and a large number of time steps to generate sufficient samples to allow PPO to perform stably and approximate Eq.~\ref{eq_2}. However, due to its on-policy approach, high variance is a significant issue such that an extremely large number of samples may be required in some scenarios to make sure the empirical objective is able to precisely estimate the true objective in Eq.~\ref{eq_2}, which naturally causes the high sample complexity issue. This motivates us to leverage off-policy techniques to alleviate such an issue, while keeping the theoretical policy improvement.

\section{Hybrid-Policy PPO (HP3O)}\label{algorithm}
To achieve better sample efficiency of PPO, historical samples generated by previous policies are reused for policy updates, as done in off-policy algorithms. This inevitably results in a distribution drift between policies, which essentially disproves the policy improvement lower bound in Lemma~\ref{lemma_1}. In this context, to fix this issue, we will extend Lemma~\ref{lemma_1} to assimilate off-policy samples in a principled manner to derive a new policy improvement lower bound that works for our proposed algorithm, HP3O. HP3O (and its variant HP3O+), as shown in Figure.~\ref{fig:schematic_diagram}, takes a \textit{hybrid} approach that effectively synthesizes on-policy trajectory-wise policy updates and off-policy trajectory replay buffers. 
\begin{figure}[h]
\includegraphics[width=10cm]{Figures/HP3O_schematic_diagram.pdf}
\centering
\caption{Schematic diagram of HP3O/HP3O+: (left side) the trajectory replay buffer takes a "first in, first out" (FIFO) strategy to keep only recent trajectories; batch consisting of the trajectory with the best return ($\tau^*$) and other randomly sampled ones from the buffer are used for updating the actor/critic networks (\textit{off-policy} approach); (right side) model updating still follows the \textit{on-policy} PPO method, hence, \textit{hybrid-policy} PPO (HP3O); for HP3O+, $\tau^*$ is also used to update the advantage function}
\label{fig:schematic_diagram}
\end{figure}
% Simultaneously, one practical remedy to relieve the distribution shift is to have an updated trajectory replay buffer with a fixed size, iteratively appending a new trajectory and discarding the oldest one, which is consistent with the FIFO strategy. To further reduce the high variance, we leverage a new baseline approximated by using the best trajectory from the reply buffer. In the sequel, we first present the unified algorithmic framework of HP3O(+) and defer the detailed analysis to the next section.
\begin{algorithm}
\caption{HP3O(\textcolor{blue}{+})}
\label{alg:oppo-bat}
\begin{algorithmic}[1]
    \State \textbf{Input:} initializations of $\theta_0$, $\phi_0$, and trajectory replay buffer $R$, the number of episodes $K$, the number of time steps in each episode $T$, the number of epochs for updates $E$
    \For{$k=1,2,...,K$}
        \State Run policy $\pi_{\theta_k}$ to generate a trajectory $\tau=(s_0,a_0,r_1,s_1,...,s_{T-1},a_{T-1},r_T)$
        \State Append $\tau$ to $R$ and discard the oldest one $\tau^-$ \Comment{FIFO strategy}
        \State Sample a random minibatch $\mathcal{B}$ from the trajectory replay buffer $R$
        \State Select the best action trajectory $\tau^*_k$ from the trajectory replay buffer and add it to $\mathcal{B}$
        \For{each trajectory $j=1,2,...,|\mathcal{B}|$:}
            \For{$t=0,1,...,T-1$}
                \State $G_t^j=\sum_{l=t+1}^T\gamma^{l-t-1}r_l^j$
            \EndFor
        \EndFor
        % \State Calculate the ensemble average $\bar{G}_t=\frac{1}{|\mathcal{B}|}\sum_{j=1}^{|\mathcal{B}|}G_t^j$
        \State Compute advantage estimates $\hat{A}^{\pi_k}_t=G_t-V_\phi(s_t)$ \Comment{HP3O}
        \State \textcolor{blue}{Compute $V^{\tau^*_k}(s_t)$ using $\tau^*_k$ and advantage estimates $\hat{A}^{\pi_k}_t=G_t-V^{\tau^*_k}(s_t)$} \textcolor{blue}{\Comment{HP3O+}}
        \For{each epoch $e=1,2,...,E$}
            \State Compute the clipping loss Eq.~\ref{eq_2}
            \State Compute the mean square loss $\mathcal{L}^V(\phi)=-\frac{1}{T}\sum_{t=0}^{T-1}(G_t-V_\phi(s_t))^2$
            \State Update $\pi_{\theta_k}$ with $\nabla_{\theta}\mathcal{L}^{clip}(\theta)$ by Adam
            \State Update $V_{\phi_k}$ with $\nabla_{\phi}\mathcal{L}^V(\phi)$ by Adam 
        \EndFor
    \EndFor
    \State \Return $\pi_{\theta_K}$ and $V_{\phi_K}$
\end{algorithmic}
\end{algorithm}
Algorithm~\ref{alg:oppo-bat} shows the algorithm framework for HP3O and HP3O+ (blue line represents the only difference for HP3O+). We denote the actor and critic by $\theta\in\mathbb{R}^m$ and $\phi\in\mathbb{R}^n$ respectively such that the parameterized policy function is $\pi_\theta$ and the parameterized value function is $V_\phi=\mathbb{E}_{\tau_\sim\pi_\phi}[\sum_{l=t}^T\gamma^{l-t}r(s_l,a_l)|s_t]$. Denote by $\tau^*_k=\textnormal{argmax}_{\tau\in R}\sum_{t=0}^T\gamma^tr(s_t,a_t) $ the best action trajectory selected from the replay buffer $R$ at the current episode $k$.
%Line 4 in Algorithm~\ref{alg:oppo-bat} is rare in 

In most existing off-policy algorithms, the size of the replay buffer is fixed with a large number to ensure that a diverse set of experiences is captured. With this approach, though the random minibatch sampling allows the agent to learn from past experience, a large-size replay buffer may cause significant data distribution drifts. Additionally, a large replay buffer means that it takes more time for the buffer to fill up, especially in environments requiring extensive exploration. Hence, we apply the FIFO strategy and discard old trajectories empirically to attenuate the issue (Line 4 in Algorithm~\ref{alg:oppo-bat}). The recently proposed off-policy PPO~\cite{meng2023off} indeed uses off-policy data, but it does not employ a trajectory buffer as we do. In our approach, the trajectory buffer is an essential component because it allows us to store and process complete sequences of state-action pairs (trajectories) rather than isolated transitions. This will preserve the temporal coherence and enhance stability.
Line 5 is to sample from the trajectory replay buffer $R$, which is different from the reuse of $N$ samples generated from prior policies in~\cite{queeney2021generalized}, where the past immediate sample trajectories were used without random sampling. We note that a replay buffer in the proposed algorithm enhances the agent's performance by providing access to a more diverse set of experiences and highlighting the most impactful trajectories.
Line 6 signifies the core part of HP3O as the best action trajectory $\tau^*_k$ indicates the best return starting from state $s_t$ within the buffer. 
Line 7 through Line 12 calculate the rewards to go for each time step $t$ in each trajectory and obtain the total reward to go at each time step over all trajectories. One may wonder how to calculate the return $G_t$ if trajectories have varying lengths in some environments. In this work, we store different lengths of trajectories directly in the buffer and do not pad them. This approach preserves the natural variation in trajectory lengths that can occur in different environments. Although the length differ, we still compare the returns of these trajectories to identify the best one while ensuring the comparison remains consistent and fair. Particularly, line 13 is a key step in the proposed HP3O+. $V^{\tau^*_k}(s_t) = \mathbb{E}_{\tau^*_k\sim\pi_k}[\sum_{l=t}^T\gamma^{l-t}r(s_l,a_l)|s_t]$ induced by the current best action trajectory $\tau^*_k$ sets the best state value among all trajectories from $R$. $\hat{A}^{\pi_k}_t$ in Line 13 signifies how much better the return $G_t$ is by taking action $a_t$ than the best value we have obtained most recently. Intuitively, this "encourages" the agent to improve its performance in the next step on top of $V^{\tau^*_k}(s_t)$. While $V^{\tau^*_k}(s_t)$ can be theoretically calculated as above, in practice, to make sure that there always exists a best value for use, $V^{\tau^*_k}(s_t)$ is calculated by using a norm distance between the current trajectory and best trajectories to ensure $V^{\tau_k}$ has the best return since $s_t$. If the reward to go from $s_t$ in the best trajectory is lesser, the current trajectory is used to replace the best one for $V^{\tau^*_k}(s_t)$ calculation. Please see the Appendix for more details about the data structures of the proposed algorithms.
% Compared to the regular baseline in Line 13 attained by using the critic network, which gives us the average sense, the adoption of $\tau^*$ improves the performance. 
% However, this may bring about more excessive bias, we will technically discuss this in the next section when presenting theoretical analysis.
\begin{remark}
    We remark on the sampling method adopted in this work to obtain the trajectories apart from the best trajectory for update. We begin by randomly sampling a set of trajectories from our trajectory buffer. This set is specifically designed to include the best action trajectory, with the remaining trajectories selected randomly from the buffer. From the set of trajectories obtained by random sampling, we then apply uniform sampling. The resulting minibatch is used for training. This approach balances leveraging high-performing trajectories while maintaining exploration across the broader trajectory space, helping to reduce the risk of overfitting. However, we recognize that assigning a score to trajectories based on the loss function could offer additional benefits. Prioritizing trajectories~\cite{hou2017novel} that result in higher losses could help the agent focus on challenging experiences, potentially improving learning efficiency by addressing areas where the policy requires more refinement. This could also help in stabilizing training by emphasizing learning from mistakes, thereby potentially reducing the variance in policy updates. In fact, integrating a prioritized experience replay (PER) strategy could be a promising direction for future work. 
\end{remark}
% \begin{algorithm}
% \caption{OPPO-BAT+}
% \label{alg:oppo-bat+}
% \begin{algorithmic}[1]
%     \State \textbf{Input:} initializations of $\theta_0$, $\phi_0$, and trajectory replay buffer $R$, the number of episodes $K$, the number of time steps in each episode $T$, the number of epochs for updates $E$
%     \For{$k=1,2,...,K$}
%         \State Run policy $\pi_{\theta_k}$ to generate a trajectory $\tau=(s_0,a_0,r_1,s_1,...,s_{T-1},a_{T-1},r_T)$
%         \State Append $\tau$ to $R$ and discard the oldest one $\tau^-$
%         \State Sample a random minibatch $\mathcal{B}$ from the trajectory replay buffer $R$
%         \For{each trajectory $j=1,2,...,|\mathcal{B}|$:}
%             \For{$t=0,1,...,T-1$}
%                 \State $G_t^j=\sum_{l=t+1}^T\gamma^{l-t-1}r_l^j$
%             \EndFor
%         \EndFor
%         \State Calculate the ensemble average $\bar{G}_t=\frac{1}{|\mathcal{B}|}\sum_{j=1}^{|\mathcal{B}|}G_t^j$
%         \State Select the best action trajectory $\tau^*(s_t)$ from the trajectory replay buffer and calculate $V^{\tau^*}(s_t)$
%         \State Compute advantage estimates $\hat{A}^{\pi_k}_t=\bar{G}_t-V^{\tau^*}(s_t)$
%         \For{each epoch $e=1,2,...,E$}
%             \State Compute the clipping loss Eq.~\ref{eq_2}
%             \State Compute the mean square loss $\mathcal{L}^V(\phi)=-\frac{1}{T}\sum_{t=0}^{T-1}(\bar{G}_t-V_\phi(s_t))^2$
%             \State Update $\pi_{\theta_k}$ with $\nabla_{\theta}\mathcal{L}^{clip}(\theta)$ by Adam
%             \State Update $V_{\phi_k}$ with $\nabla_{\phi}\mathcal{L}^V(\phi)$ by Adam 
%         \EndFor
%     \EndFor
%     \State \Return $\pi_{\theta_K}$ and $V_{\phi_K}$
% \end{algorithmic}
% \end{algorithm}



\section{Theoretical analysis}
This section presents a theoretical analysis of the proposed HP3O and HP3O+. 
% A few existing works~\cite{achiam2017constrained,queeney2021generalized,zhao2023policy} have conducted preliminary studies on the theoretical guarantees, and the majority of them focused primarily on policy improvement lower bounds. 
We first derive a new policy improvement lower bound for HP3O and then present a different bound for HP3O+ to indicate the value penalty term. All proofs are deferred to the Appendix.
% We star with a key auxiliary technical lemma to characterize the relationship between two policies.
% \begin{lemma}(Lemma 6.1 in~\cite{kakade2002approximately})\label{lemma_2}
% For any policies $\hat{\pi}$ and $\pi$, we have 
% \begin{equation}
%     J(\hat{\pi}) - J(\pi)=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\hat{\pi}}}[\mathbb{E}_{a\sim\hat{\pi}(\cdot|s)}[A^{\pi}(s,a)]]
% \end{equation}
% \end{lemma}
% Lemma~\ref{lemma_2} signifies the cumulative return difference between two policies, $\pi$ and $\hat{\pi}$. 
% The next lemma is adapted from Lemma 3 in~\cite{achiam2017constrained}.
% \begin{lemma}\label{lemma_3}
% Consider any two policies $\hat{\pi}$ and $\pi$. Then the total variation distance between the state visitation distributions $d^{\hat{\pi}}$ and $d^\pi$ is bounded by
% \begin{equation}
%     \delta(d^\pi,d^{\hat{\pi}})\leq \frac{\gamma}{1-\gamma}\mathbb{E}_{s\sim d^{\hat{\pi}}}[\delta(\pi,\hat{\pi})(s)],
% \end{equation}
% where $\delta(\pi,\hat{\pi})(s)$ is defined in Lemma~\ref{lemma_1}.
% \end{lemma}
% The proof follows similarly from~\cite{achiam2017constrained}.
To incorporate prior policies in the policy improvement lower bound, we need to extend the conclusion in Lemma~\ref{lemma_1}, which quantifies the improvement for two consecutive policies. In~\cite{queeney2021generalized}, policies prior to the present policy $\pi_k$ in chronological order were used. However, in our study, this order has been broken due to the random sampling from the replay buffer, which motivates us to derive a relationship among the current, future, and prior policies independent of the chronological order. Before the main result, we first present an auxiliary technical lemma.
\begin{lemma}\label{lemma_4}
    Consider a current policy $\pi_k$, and any reference policy $\pi_r$. For any future policy $\pi$,
    \begin{equation}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_r}}[\frac{\pi(a|s)}{\pi_r(a|s)}A^{\pi_k}(s,a)]-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)],
    \end{equation}
where $C^\pi_{\pi_k}$ and $\delta(\pi,\pi_r)(s)$ are defined as in Lemma~\ref{lemma_1}.
\end{lemma}
\begin{remark}
Lemma~\ref{lemma_4} implies that now the visitation distribution, the probability ratio of the surrogate objective, and the maximum value of the total variation distance depend on the reference policy $\pi_r$, which essentially extends Lemma~\ref{lemma_1} to a more generalized case. However, the improvement is still for the two consecutive policies $\pi_k$ and $\pi$ as the advantage function in the surrogate objective and $C^\pi_{\pi_k}$ rely on the latest policy $\pi_k$. Lemma~\ref{lemma_4} does not necessarily require $\pi_r$ to be the last policy prior to $\pi_k$ as in~\cite{queeney2021generalized}, which paves the way for establishing the policy improvement for $|\mathcal{B}|$ prior policies sampled randomly from the replay buffer $R$. 
\end{remark}
% With this in hand, we are now ready to present the first main result in the following theorem.
\begin{theorem}\label{theorem_1}
    Consider prior policies $|\mathcal{B}|$ randomly sampled from the replay buffer $R$ with indices $i=0,1,...,|\mathcal{B}|-1$. For any distribution $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the $|\mathcal{B}|$ prior policies, and any future policy $\pi$ generated by HP3O in Algorithm~\ref{alg:oppo-bat}, the following relationship holds true
    \begin{equation}\label{eq_theo1}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{i\sim v}[\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}A^{\pi_k}(s,a)]]-\frac{\gamma C^\pi_{\pi_k}\epsilon}{(1-\gamma)^2},
    \end{equation}
where
% $\alpha=\textnormal{max}_{i\in v}\alpha_i, \alpha_i=\textnormal{max}_{s\sim d_{\pi_i}}\delta(\pi,\pi_i)(s), \forall i\in v$,
$C^\pi_{\pi_k}$ is defined as in Lemma~\ref{lemma_1}.
\end{theorem}
\begin{remark}\label{remark_3}
It is observed that the conclusion from Theorem~\ref{theorem_1} is similar to one of the main results in~\cite{queeney2021generalized}. The significant difference is that $\pi_i$ is not the same as $\pi_{k-i}$ in~\cite{queeney2021generalized}. It is technically attributed to Lemma~\ref{lemma_4}, where the reference policy $\pi_r$ may not have a close temporal relationship with $\pi_k$. Also, the advantage function has not been changed yet. Empirically speaking, for each minibatch $\mathcal{B}$, we have added the best trajectory in it, which essentially expedites the learning process. Additionally, Theorem~\ref{theorem_1} has an extra expectation operator over multiple trajectories on the first term of the right side in Eq.~\ref{eq_theo1}, leading to the smaller variance, compared to only one trajectory in Lemma~\ref{lemma_1}. We would also like to point out that Theorem~\ref{theorem_1} shows the policy improvement lower bound by sampling a mini-batch of trajectories associated with prior policies from the buffer, which is consistent with what has been done in Algorithm~\ref{alg:oppo-bat}. In HP3O+, we use it as a baseline to replace $V_\phi(s)$ and have surprisingly found that this leads to an extra term that penalizes the state value to reduce the variance. 
\end{remark}
We first define $\hat{A}^{\pi}(s,a)=Q^{\pi}(s,a)-V^{\pi^*}(s)$ and $G^\pi(s)=V^{\pi^*}(s)-V^{\pi}(s)$. It is immediately obtained that $A^\pi(s,a) = \hat{A}^\pi(s,a)+G^\pi(s)$. Hence, if we utilize the state value induced by the best trajectory at the moment as the baseline, there exists a \textit{value gap}  $G^\pi(s)$ between $A^\pi(s,a)$ and $\hat{A}^{\pi}(s,a)$. One may argue that the advantage $\hat{A}^{\pi}(s,a)$ is negative all the time, which implies the present action is not favorable such that the new policy should be changed to yield a lower probability for the current action and state. However, this is not always true as $V^{\pi^*}(s)$ is not the \textit{globally} optimal value, while it is approximately the optimal value up to the current time step over the last $|\mathcal{B}|$ episodes. The motivation behind $\hat{A}^{\pi}(s,a)$ is that the new baseline $V^{\pi^*}(s)$ becomes the driving force to facilitate the performance improvement between every update. We are now ready to state the policy improvement lower bound with the new baseline as follows.
\begin{lemma}\label{lemma_5}
    Consider a current policy $\pi_k$, and any reference policy $\pi_r$. For any future policy $\pi$,
\begin{equation}
\begin{split}
    J(\pi)-J(\pi_k)&\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_r}}[\frac{\pi(a|s)}{\pi_r(a|s)}\hat{A}^{\pi_k}(s,a)]-\frac{2\gamma \hat{C}^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)]\\&-\frac{2\gamma C^{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)],
\end{split}
\end{equation}
where $\hat{C}^\pi_{\pi_k}=\textnormal{max}_{s\in\mathcal{S}}|\mathbb{E}_{a\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]|$, $\delta(\pi,\pi_r)(s)$ is defined as in Lemma~\ref{lemma_1}, $C^{\pi_k}=\textnormal{max}_{s\in\mathcal{S}}|V^{\pi^*_k}(s)-V^{\pi_k}(s)|$. 
\end{lemma}
With Lemma~\ref{lemma_5} in hand, we have another main result in the following.
\begin{theorem}\label{theorem_2}
    Consider prior policies $|\mathcal{B}|$ randomly sampled from the replay buffer $R$ with indices $i=0,1,...,|\mathcal{B}|-1$. For any distribution $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the $|\mathcal{B}|$ prior policies, and any future policy $\pi$ generated by HP3O+ in Algorithm~\ref{alg:oppo-bat}, the following relationship holds true
    \begin{equation}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{i\sim v}[\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}\hat{A}^{\pi_k}(s,a)]]-\frac{\gamma \hat{C}^\pi_{\pi_k}\epsilon}{(1-\gamma)^2}-\frac{\gamma C^{\pi_k}\epsilon}{(1-\gamma)^2},
    \end{equation}
where
% $\alpha=\textnormal{max}_{i\in v}\alpha_i, \alpha_i=\textnormal{max}_{s\sim d_{\pi_i}}\delta(\pi,\pi_i)(s), \forall i\in v$, $\delta(\pi,\pi_i)(s)$ is defined as in Lemma~\ref{lemma_1},
$\hat{C}^\pi_{\pi_k}$ and $C^{\pi_k}$ are defined as in Lemma~\ref{lemma_5}.
\end{theorem}
\begin{remark}\label{remark_4}
Theorem~\ref{theorem_2} describes the policy improvement lower bound for HP3O+, which provides the theoretical guarantees when reusing trajectories generated by prior policies rigorously. The extra term on the right-hand side $\frac{\gamma C^{\pi_k}\epsilon}{(1-\gamma)^2}$ in the above inequality is not the penalty term between two policies, while it is a value gap between the current state value and the most recent best value. As $V^{\pi^*_k}(s)$ is time-varying, this acts as a "guide" to the current one $V^{\pi_k}$ not deviating too far away from $V^{\pi^*_k}(s)$. Equivalently, the term $\frac{\gamma C^{\pi_k}\epsilon}{(1-\gamma)^2}$ can be regarded as a regularization from the critic network, which assists in enhancing the overall agent performance and reducing the variance. We also include some technical discussion regarding whether our approach will cause overfitting and the adoption of the worst trajectories in~\ref{overfitting} and~\ref{worst_trajectories}.
\end{remark}
% \vspace{-0.2in}


\begin{remark}\label{remark_5}
The proposed HP3O algorithm and its variant have resorted to data randomly sampled from multiple policies in the training batch $\mathcal{B}$ that is prior to $\pi_k$ for the policy update. Thus, there exist $\textit{multiple}$ updates compared to the vanilla PPO, which only makes one policy update from $\pi_k$ to $\pi_{k+1}$. 
In this study, we aim to show how the off-policy sample reuse significantly affects the original sample efficiency PPO has. Though the direct sample complexity improvement analysis can be significantly beneficial to provide a solid theoretical foundation for the proposed algorithms, a thorough investigation of this aspect is out of the scope of this study. For instance, to arrive at an $\varepsilon$-optimality for policy gradient-based algorithms, a few works~\cite{zhong2024theoretical,zanette2021cautiously,dai2023refined,sherman2023improved} have revealed the exact complexity with respect to $\varepsilon$, but only for MDPs with linear function approximation. The exact sample complexity analysis for the off-policy PPO algorithm with nonlinear function approximation still remains extremely challenging and requires a substantial amount of non-trivial efforts. Therefore, in this paper, we instead disclose the impact of off-policy sample reuse on the tradeoff between sample efficiency and learning stability. Please see~\ref{sample_efficiency_analysis} for more details. Additionally, we also present a theoretical result in~\ref{hp3o_hp3o+} to reveal that HP3O+ increases updates in the total variational distance of the policy throughout training, given the same sample size, when it is compared to HP3O.
\end{remark}

% % \subsection{Stationary Point Convergence (Optional)}
% \vspace{-0.2in}


\section{Experiments}
The experimental evaluation aims to understand how the sample complexity and stability of our proposed algorithms compare with existing baseline on-policy and off-policy learning algorithms. Concretely, we conduct the comparison between our methods and prior approaches across challenging continuous control environments from the Gymnasium benchmark suite~\cite{brockman2016openai}. While easy control tasks can be solved by various algorithms, the more complex tasks are typically sample intensive with on-policy algorithms~\cite{schulman2017proximal}. Additionally, the high variance of the algorithms negatively impacts stability and convergence. Furthermore, though some off-policy algorithms enjoy high sample efficiency, the actual run time can be impractically large, which impedes its applications to real-world tasks. As our proposed hybrid-policy learning algorithms are developed on top of PPO, we mainly compare our methods to PPO, another popular on-policy method A2C~\cite{peng2018adversarial}, and three other relevant off-policy PPO approaches, including P3O~\cite{chen2023sufficiency} (a modification of PPO to leverage both on- and off-policy principles), GEPPO~\cite{queeney2021generalized}, and Off-policy PPO (abbreviated as OffPolicy)~\cite{meng2023off}. We acknowledge that SAC, a fully off-policy algorithm, may achieve comparatively higher returns in most of the continuous control problems at the expense of much longer training time and with careful hyperparameter tuning. Hence, we also compare with SAC in terms of variance reduction and run time complexity. 
%SAC having SAC for the variance reduction and run time complexity comparison. The reason is twofold. First, SAC has been recognized as one of the most popular and efficient off-policy algorithms. Second, our proposed methods are hybrid-policy learning algorithms developed on top of PPO and we think the direct comparison with SAC is not technically fair. 
As shown in Table~\ref{table:comparison}, there are other off-policy versions of PPO, such as Policy-on-off PPO~\cite{fakoor2020p3o}. However, the corresponding code base lacks a complete implementation to reproduce their results, which is evident in their code where the actor head for Mujoco environments is not implemented. Moreover, making the code functional for our purpose would require extensive effort, as it is built on MXNet, a deprecated open-source project. The above limitations have prevented us from performing head-to-head comparisons.
%Thus, we will not include these algorithms in the results. 
% More details about hyperparameter settings are deferred to the Appendix.
% \vspace{-0.1in}
\subsection{Comparative evaluation}

Figure~\ref{Mean_Variance_Comparison} shows the total average return during training for A2C, PPO, P3O, GEPPO, OffPolicy, HP3O, and HP3O+. Each experiment includes five different runs with various random seeds. The solid curves indicate the mean, while the shaded areas represent the standard deviation over the five runs.
% Notably, P3O was not implemented for the Lunar Lander environment due to a problematic dimension mismatch in the original code base. 
Clearly, the results show that, overall, both HP3O+ and HP3O are comparable to or outperform all baselines across diverse tasks with smaller variances, which supports our theoretical claims. For instance, in the HalfCheetah environment, our methods demonstrate a sharper average slope compared to the baseline, particularly in the later stages of training, where other baselines show a more flattened curve. This indicates that our method continues to learn effectively with fewer samples. In the Hopper environment, P3O performs slightly better than HP3O but at the cost of extremely large reward variance, indicating an unstable training process. However, HP3O+ significantly dominates in the latter phase with a much smaller variance. In the Swimmer environment, while A2C and P3O learn slowly and make almost no progress, HP3O and HP3O+ achieve the similarly highest reward with very low variance, as suggested by Remark~\ref{remark_3}. Notably, OffPolicy ranks second in terms of performance, but with the cost of extremely high variance. 
% In the Walker environment, OffPolicy also slightly outperforms both HP3O and HP3O+, while its learning reveals instability with relatively larger variance. 
Additionally, OffPolicy shows notably strong performance in the Walker environment. This is primarily attributed to the adoption of a new clipped surrogate that iteratively resorts to off-policy data to progress during training.
Generally, our proposed methods learn more stably than all baselines by dequeuing the buffer to suppress the instability caused by data distribution drift in most environments. 
Overall, HP3O+ excels HP3O in most environments, with also variance reduction particularly in the latter training phase.
As the learning trajectories are always around the best trajectory from the buffer. Essentially, the empirical evidence supports our theoretical results in Theorem~\ref{theorem_2} and Theorem~\ref{theorem_5}, which show that HP3O+ enables larger updates in the total variational distance of the policy, given the same number of changes to the policy.
% the value penalty term in the policy improvement lower bound from Theorem~\ref{theorem_2} regularizes the policy evaluation. 
Additional results are included in the Appendix, including Table~\ref{tab:mean_std_summary} to showcase rewards at or close to the converged stage.


\begin{figure}[h]
    \centering
    \includegraphics[width= 0.95\textwidth]{Figures/mean_variance/Mean_Variance_Comparison.png}
    \caption{Training curves (over 1M steps) on continuous control benchmarks. HP3O+ (black) performs consistently across all tasks and is comparable to or outperforming other baseline methods.}
    \label{Mean_Variance_Comparison}
\end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Halfcheetah_HP3O_vs_HP3O+.png}
% \centering
% \caption{HP3O+. HalfCheetah}
% \label{fig:half_cheetah_hp3o+}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Walker_HP3O_vs_HP3O+.png}
% \centering
% \caption{HP3O+. Walker}
% \label{fig:walker_hp3o+}
% \end{figure}


% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Comparison_HP3O+.png}
% \centering
% \label{fig:HP3O+}
% \caption{HP3O+.}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width= 0.95\textwidth]{Figures/mean_variance/Mean_Variance_Comparison_HP3O+.png}
%     \caption{Training curves on Walker and HalfCheetah environments. HP3O+ (black) performs with the smallest variance.}
%     \label{fig:HP3O+}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Halfcheetah_HP3O_vs_PPO.png}
% \centering
% \label{fig:half_cheetah}
% \caption{HP3O vs. PPO for half-cheetah.}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Hopper_HP3O_vs_PPO.png}
% \centering
% \label{fig:hopper}
% \caption{HP3O vs. PPO for Hopper.}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Swimmer_HP3O_vs_PPO.png}
% \centering
% \label{fig:swimmer}
% \caption{HP3O vs. PPO for swimmer.}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Lunarlander_HP3O_vs_PPO.png}
% \centering
% \label{fig:lander}
% \caption{HP3O vs. PPO for Lunar Lander.}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Walker_HP3O_vs_PPO.png}
% \centering
% \label{fig:walker}
% \caption{HP3O vs. PPO for Walker.}
% \end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width= 0.7\textwidth]{Figures/histogram/normalized_std_deviation.png}
%     \caption{Normalized Standard Deviation among different methods for various environments.}
%     \label{fig:SAC}
% \end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.57\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/histogram/normalized_std_deviation.png}
        \caption{Normalized Standard Deviation among different methods for various environments.}
        \label{fig:SAC}
    \end{subfigure}
    \hspace{0.01\textwidth} % Adjust this value to decrease the space between the plots
    \begin{subfigure}[b]{0.385\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/time_histogram_1m_steps.png}
        \caption{Runtime for HalfCheetah Environment among different methods}
        \label{fig:run_time}
    \end{subfigure}
    \caption{Comparison of Normalized Standard Deviation and Runtime for 1 million steps.}
    \label{fig:SAC_Comparison}
\end{figure}
% \vspace{-0.2in}
\subsection{Ablation study}
The experimental results in the previous section imply that algorithms based on the hybrid-policy approach can outperform the conventional on-policy methods on challenging control tasks. In this section, we further compare all policy optimization algorithms to SAC for variance reduction and run time complexity. We also inspect the robustness of the algorithms against variations of trajectories.  

% \textbf{Variance.} Figure~\ref{fig:SAC} shows the comparison of the relative standard deviation of the ultimate average return (at 1M steps) for different algorithms. It suggests that, on average, HP3O achieves the second lowest relative standard deviation (which is the ratio of standard deviation to the average reward over five runs at the last step).
% %, which strengthens the variance reduction effect in our proposed algorithms. 
% This implies that hybrid-policy algorithms have more advantages of regularizing the learning process to maintain the stability compared to typical on-policy algorithms.
% SAC attains the smallest relative standard deviation according to Figure~\ref{fig:SAC} (also on average, the maximum reward reported in the supplementary materials). This is not surprising since the maximum entropy principle can significantly help the meaningful exploration to achieve the highest return. However, this comes at the cost of run time complexity.


\textbf{Variance.} Figure~\ref{fig:SAC} shows the comparison of the relative standard deviation of the ultimate average return (at 1M steps) for different algorithms. It suggests that, on average, HP3O+ achieves the lowest relative standard deviation (which is the ratio of the standard deviation to the average reward over five runs at the last step). This implies that hybrid-policy algorithms have more advantages in regularizing the learning process to maintain stability compared to typical on-policy algorithms. Intuitively, as the policy and environment change over time, the use of replay buffers helps mitigate this issue by providing a more stationary training dataset. The buffer contains a mix of experiences collected under different policies, instead of the only current policy from PPO, which helps
in reducing the variance in updates. SAC attains a relatively small standard deviation according to Figure~\ref{fig:SAC} (also, on average, the maximum reward reported in the Appendix). This is not surprising since the maximum entropy principle can significantly help meaningful exploration to achieve the highest return. However, this comes at the cost of runtime complexity.

% Except for SAC, our proposed algorithm still outperforms baselines, which signifies the necessity of combining on-policy and off-policy manners together to design better RL algorithm.
\textbf{Run time complexity.} As shown in Figure~\ref{fig:run_time}, the run time for all algorithms is presented (all methods are implemented with the same hardware). Both GEPPO and SAC require much more run time to explore and then converge, which may impede its applications to solving real-world problems. P3O achieves the lowest run time complexity while performing worse than HP3O and HP3O+. However, our proposed approaches take approximately the same training time as PPO but with higher sample efficiency, as shown in Figure~\ref{Mean_Variance_Comparison}. Thus, HP3O/HP3O+ are able to achieve a desirable trade-off in practice between sample efficiency and computational time. These experiments used a local machine with an NVIDIA RTX 4090. Additional results regarding wall-clock time for diverse methods to reach a certain reward are included in~\ref{computational_efficiency}.
%computational complexity. 

\textbf{Robustness.} We also compute the \textit{explained variance}~\cite{lahuis2014explained} for all algorithms under consideration for evaluating robustness. Please check the~\ref{additional_results} for more details about this metric. Intuitively, it quantifies how good a model is to explain the variations in the data. Therefore, the higher the explained variance of a model, the more the model is able to explain the variations in trajectories. Essentially, the data in this work are trajectories produced by different policies, leading to a data distribution drift. 
%It may be more noticeable in the off-policy learning. 
Therefore, explained variance can, to some extent, be viewed as an indicator of how well an algorithm is robust against the data distribution drift.
Figure~\ref{fig:Explained Variance HalfCheetah} shows the explained variances for HP3O and PPO in the HalfCheetah environment for five different runs with different random seeds. HP3O has the highest explained variance over all runs suggesting that it is more robust against the variations of trajectories during learning. While for PPO, its explained variance can reach large negative values during training, which indicates the training instability when the trajectories vary significantly.
% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/explained_variance/explain_variance_Halfcheetah_HP3O_steps_720_to_980.png}
% \centering
% \label{fig:half_cheetah_ev_hp3o}
% \caption{Explained Variance for Half Cheetah for HP3O.}
% \end{figure}

% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/explained_variance/explain_variance_Halfcheetah_PPO_steps_1050_to_1900.png}
% \centering
% \label{fig:half_cheetah_ev_ppo}
% \caption{Explained Variance for Half Cheetah for PPO.}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/explained_variance/explain_variance_env_Halfcheetah_boxplot.png}
    \caption{Explained Variance for HalfCheetah for PPO and HP3O}
    \label{fig:Explained Variance HalfCheetah}
\end{figure}



% \begin{figure}[h]
% \includegraphics[width=11cm]{Figures/mean_variance/Mean_Variance_Comparison.png}
% \centering
% \label{fig:Mean_Variance_Comparison}
% \caption{Mean_Variance_Comparison}

% \end{figure}

\subsection{Limitations}
Though theoretical and empirical results have shown that the proposed HP3O outperforms the popular baseline PPO over diverse control tasks, some limitations need to be discussed for potential improvement in the future. First, HP3O/HP3O+ require more hyperparameter tuning for the trajectory replay buffer, which can impact model performance compared to PPO. It has been acknowledged that hyperparameter tuning is critical for reinforcement learning such that for the hardest benchmarks, the already narrow basins of effective hyperparameters may become prohibitively small for our proposed algorithms, leading to poor performance. Second, in sparse reward environments, dequeuing the trajectory replay buffer can result in insufficient learning. Unlike the traditional replay buffer, which stores all experiences, our design requires the buffer to discard old trajectories so that the potential data distribution drift can be alleviated. This may cause a problem that good trajectories may only be learned once. Thus, the tradeoff between data distribution drift and learning frequency for the buffer needs to be investigated more in future work. Finally, there remains substantial room for performance improvement for the proposed algorithms compared to SAC. Further work in algorithm design is required to ensure HP3O/HP3O+ is on par with SAC but with low variance. The current ones can be regarded as one of the first steps toward bridging the gap between on-policy and off-policy methods.

% The current ones can be regarded as one of the first steps towards filling the gap between on-policy and off-policy algorithms.


 
\section{Conclusion}
In this work, we presented a novel hybrid-policy reinforcement learning algorithm by incorporating a replay buffer into the popular PPO algorithm. Specifically, we utilized random sampling to reuse samples generated by the prior policies to improve the sample efficiency of PPO. We developed HP3O and theoretically derived its policy improvement lower bound. Subsequently, we designed a new advantage function in HP3O+ and presented a modified lower bound to provide theoretical guarantees. We investigated the stationary point convergence for HP3O and used several continuous control environments and baselines to showcase the superiority of the proposed algorithms. Additionally, we focused on variance reduction while maintaining high reward returns, encouraging the community to consider both high rewards and variance reduction. The theoretical claims of higher sample efficiency and variance reduction were empirically supported.

\newpage
\bibliographystyle{elsarticle-num} 
\bibliography{HP3O}

\appendix
\newpage
\section{Appendix}
In this section, we present additional analysis and experimental results as a supplement to the main contents. To conveniently refer to the theoretical results, we repeat the statements for all lemmas and theorems.
\subsection{Additional Theoretical Analysis}
\begin{lemma}(Lemma 6.1 in~\cite{kakade2002approximately})
For any policies $\hat{\pi}$ and $\pi$, we have 
\begin{equation}
    J(\hat{\pi}) - J(\pi)=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d_{\hat{\pi}}}[\mathbb{E}_{a\sim\hat{\pi}(\cdot|s)}[A^{\pi}(s,a)]]
\end{equation}
\end{lemma}
Lemma 4 signifies the cumulative return difference between two policies, $\pi$ and $\hat{\pi}$.

\begin{lemma}
Consider any two policies $\hat{\pi}$ and $\pi$. Then the total variation distance between the state visitation distributions $d^{\hat{\pi}}$ and $d^\pi$ is bounded by
\begin{equation}
    \delta(d^\pi,d^{\hat{\pi}})\leq \frac{\gamma}{1-\gamma}\mathbb{E}_{s\sim d^{\hat{\pi}}}[\delta(\pi,\hat{\pi})(s)],
\end{equation}
where $\delta(\pi,\hat{\pi})(s)$ is defined in Lemma~\ref{lemma_1}.
\end{lemma}
The proof follows similarly from~\cite{achiam2017constrained}. Next we present the proof for Lemma 2.
% To incorporate prior policies in the policy improvement lower bound, we need to extend the conclusion in Lemma~\ref{lemma_1}, which quantifies the improvement for two consecutive policies. In~\cite{queeney2021generalized}, policies prior to the current policy $\pi_k$ in a chronological order were used. However, in our study, this order has been broken due to the random sampling from the replay buffer, which motivates us to derive a relationship among the current, future, and prior policies independent of the chronological order. Before the main result, we first present an auxiliary technical lemma.

\textbf{Lemma 2:}     Consider a present policy $\pi_k$, and any reference policy $\pi_r$. We then have, for any future policy $\pi$,
    \begin{equation}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_r}}[\frac{\pi(a|s)}{\pi_r(a|s)}A^{\pi_k}(s,a)]-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)],
    \end{equation}
where $C^\pi_{\pi_k}$ and $\delta(\pi,\pi_r)(s)$ are defined as in Lemma~\ref{lemma_1}.
\begin{proof}
    The proof is similar to the proof of Lemma 7 in~\cite{queeney2021generalized}. We start from the equality in Lemma 4 by adding and subtracting the term
    \begin{equation}
        \frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]
    \end{equation}
\end{proof}
With this, we obtain the following relationship:
\begin{equation}\label{eq_11}
\begin{split}
    J(\pi)-J(\pi_k)&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]\\&+\frac{1}{1-\gamma}(\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]-\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]])\\&\geq \frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]\\&-\frac{1}{1-\gamma}|\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]-\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]|
\end{split}
\end{equation}
The last inequality follows from the Triangle inequality. Subsequently, we can bound the second term of the last inequality using Hlder's inequality:
\begin{equation}\label{eq_12}
\begin{split}
    \frac{1}{1-\gamma}|\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]&-\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]|\\&\leq \frac{1}{1-\gamma}\|d^\pi-d^{\pi_r}\|_1\|\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]\|_\infty,
\end{split}
\end{equation}
where $d^\pi$ and $d^{\pi_r}$ both signify the state visitation distributions. In light of the definition of total variation distance and Lemma~\ref{lemma_5}, the following relationship can be obtained accordingly
\begin{equation}\label{eq_13}
    \|d^\pi-d^{\pi_r}\|_1=2\delta(d^\pi,d^{\pi_r})\leq \frac{2\gamma}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)].
\end{equation}
Also note that
\begin{equation}\label{eq_14}
    \|\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]\|_\infty=\textnormal{max}|\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]|=C^\pi_{\pi_k}.
\end{equation}
Hence, substituting Eq.~\ref{eq_13} and Eq.~\ref{eq_14} into Eq.~\ref{eq_12} and combining Eq.~\ref{eq_11} yields the following inequality:
\begin{equation}
    J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)].
\end{equation}
Finally, without loss of generality, we assume that the support of $\pi$ is contained in the support of $\pi_r$ for all states, which is true for common policy representations used in policy optimization. We can rewrite the first term on the right hand side of the last inequality as 
\begin{equation}
    \frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]=\frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_r}}[\frac{\pi(a|s)}{\pi_r(a|s)}A^{\pi_k}(s,a)], 
\end{equation}
which leads to the desirable results.

\textbf{Theorem 1:} Consider prior policies $|\mathcal{B}|$ randomly sampled from the replay buffer $R$ with indices $i=0,1,...,|\mathcal{B}|-1$. For any distribution $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the $|\mathcal{B}|$ prior policies, and any future policy $\pi$ generated by HP3O in Algorithm~\ref{alg:oppo-bat}, the following relationship holds true
    \begin{equation}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{i\sim v}[\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}A^{\pi_k}(s,a)]]-\frac{\gamma C^\pi_{\pi_k}\epsilon}{(1-\gamma)^2},
    \end{equation}
where
% $\alpha=\textnormal{max}_{i\in v}\alpha_i, \alpha_i=\textnormal{max}_{s\sim d_{\pi_i}}\delta(\pi,\pi_i)(s), \forall i\in v$,
$C^\pi_{\pi_k}$ is defined as in Lemma~\ref{lemma_1}.
\begin{proof}
    Based on the definition of total variation distance, we have that
    \begin{equation}
        \mathbb{E}_{s\sim d^{\pi_k}}[\delta(\pi,\pi_k)(s)] = \mathbb{E}[\frac{1}{2}\int_{a\mathcal{A}}|\pi(a|s)-\pi_k(a|s)|\textnormal{d}a].
    \end{equation}
We still make the assumption that the support of $\pi$ is contained in the support of $\pi_k$ for all states, which is true for the common policy representations used in policy optimization. Then, by multiplying and dividing by $\pi_k(a|s)$, we can observe that
\begin{equation}\label{eq_19}
\begin{split}
    \mathbb{E}_{s\sim d^{\pi_k}}[\delta(\pi,\pi_k)(s)] &= \mathbb{E}[\frac{1}{2}\int_{a\mathcal{A}}\pi_k(a|s)|\frac{\pi(a|s)}{\pi_k(a|s)}-1|\textnormal{d}a]\\&=\frac{1}{2}\mathbb{E}_{(s,a)\sim d^{\pi_k}}[|\frac{\pi(a|s)}{\pi_k(a|s)}-1|]\leq \frac{\epsilon}{2}.
\end{split}
\end{equation}
The last inequality follows from the setup of PPO. With prior policies $\pi_i, i=0,1,2,...,|\mathcal{B}|-1$, we assume that the support of $\pi$ is contained in the support of $\pi_i$ for all states, which is true for common policy representations used in policy optimization. Based on Lemma~\ref{lemma_4}, we can obtain
\begin{equation}
    J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}A^{\pi_k}(s,a)]-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_i}}[\delta(\pi,\pi_i)(s)].
\end{equation}
Consider policy weights $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the policies in the minibatch $\mathcal{B}$. Thus, for any choice of distribution $v$, the convex combination determined by $v$ of the $|\mathcal{B}|$ lower bounds given by the last inequality yields the lower bound
\begin{equation}\label{eq_21}
\begin{split}
    J(\pi)-J(\pi_k)&\geq \frac{1}{1-\gamma}\mathbb{E}_{i\sim v}[\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}A^{\pi_k}(s,a)]]\\&-\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{i\sim v}[\mathbb{E}_{s\sim d^{\pi_i}}[\delta(\pi,\pi_i)(s)]].
\end{split}
\end{equation}
Combining Eq.~\ref{eq_19} and Eq.~\ref{eq_21}, with some mathematical manipulation, results in the desirable conclusion.
Now we're ready to prove Lemma 3.
\end{proof}
\textbf{Lemma 3:} Consider a present policy $\pi_k$, and any reference policy $\pi_r$. We then have, for any future policy $\pi$,
\begin{equation}
\begin{split}
    J(\pi)-J(\pi_k)&\geq \frac{1}{1-\gamma}\mathbb{E}_{(s,a)\sim d^{\pi_r}}[\frac{\pi(a|s)}{\pi_r(a|s)}\hat{A}^{\pi_k}(s,a)]\\&-\frac{2\gamma \hat{C}^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)]\\&-\frac{2\gamma C^{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)],
\end{split}
\end{equation}
where $\hat{C}^\pi_{\pi_k}=\textnormal{max}_{s\in\mathcal{S}}|\mathbb{E}_{a\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]|$, $\delta(\pi,\pi_r)(s)$ is defined as in Lemma~\ref{lemma_1}, $C^{\pi_k}=\textnormal{max}_{s\in\mathcal{S}}|V^{\pi^*_k}(s)-V^{\pi_k}(s)|$. 
\begin{proof}
    Due to Lemma~\ref{lemma_1}, we have
\begin{equation}
\begin{split}
    J(\pi)-J(\pi_k)&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]]\\&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[Q^{\pi_k}(s,a)-V^{\pi_k}(s)]]\\&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[Q^{\pi_k}(s,a)-V^{\pi_k^*}(s)+V^{\pi_k^*}(s)-V^{\pi_k}(s)]].
\end{split}
\end{equation}
Let $\hat{A}^{\pi_k}(s,a)=Q^{\pi_k}(s,a)-V^{\pi_k^*}(s)$ and $G^{\pi_k}(s)=V^{\pi_k^*}(s)-V^{\pi_k}(s)$ such that 
\begin{equation}
\begin{split}
    J(\pi)-J(\pi_k)&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]]+\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[G^{\pi_k}(s)]].
\end{split}
\end{equation}
Define $\|G^{\pi_k}(s)\|_\infty=\textnormal{max}_{s\in\mathcal{S}}|V^{\pi_k^*}(s)-V^{\pi_k}(s)|=C^{\pi_k}$. Follow similarly the proof from Lemma~\ref{lemma_4}, we can attain the relationship as follows:
\begin{equation}
    \begin{split}
        J(\pi)-J(\pi_k)&\geq \frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[\mathbb{E}_{a\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]]+\frac{1}{1-\gamma}\mathbb{E}_{s\sim d^{\pi_r}}[G^{\pi_k}(s)]\\&-\frac{2\gamma \hat{C}^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)]\\&-\frac{2\gamma C^{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_r}}[\delta(\pi,\pi_r)(s)].
    \end{split}
\end{equation}
\end{proof}
The fact that $\textnormal{min}_{s\in\mathcal{S}}|V^{\pi_k^*}(s)-V^{\pi_k}(s)|=0$ retains the desirable result.

\textbf{Theorem 2:}     Consider prior policies $|\mathcal{B}|$ randomly sampled from the replay buffer $R$ with indices $i=0,1,...,|\mathcal{B}|-1$. For any distribution $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the $|\mathcal{B}|$ prior policies, and any future policy $\pi$ generated by HP3O+ in Algorithm~\ref{alg:oppo-bat}, the following relationship holds true
    \begin{equation}\label{eq_26}
        J(\pi)-J(\pi_k)\geq \frac{1}{1-\gamma}\mathbb{E}_{i\sim v}[\mathbb{E}_{(s,a)\sim d^{\pi_i}}[\frac{\pi(a|s)}{\pi_i(a|s)}\hat{A}^{\pi_k}(s,a)]]-\frac{\gamma \hat{C}^\pi_{\pi_k}\epsilon}{(1-\gamma)^2}-\frac{\gamma C^{\pi_k}\epsilon}{(1-\gamma)^2},
    \end{equation}
where
% $\alpha=\textnormal{max}_{i\in v}\alpha_i, \alpha_i=\textnormal{max}_{s\sim d_{\pi_i}}\delta(\pi,\pi_i)(s), \forall i\in v$, $\delta(\pi,\pi_i)(s)$ is defined as in Lemma~\ref{lemma_1},
$\hat{C}^\pi_{\pi_k}$ and $C^{\pi_k}$ are defined as in Lemma~\ref{lemma_5}.
\begin{proof}
    Following the proof techniques in Theorem~\ref{theorem_1} and combining the conclusion from Lemma~\ref{lemma_5} obtains Eq.~\ref{eq_26}.
\end{proof}
\subsection{Risk of Overfitting?}\label{overfitting}
In our approach, each set of sampled trajectories includes the current best
action trajectory in the buffer, but we use a uniform distribution to sample mini-batch data points from all the trajectories rather than only focusing on the
best one. Additionally, the number of sampled trajectories is a tunable parameter that we adjust based on the specific environment. Therefore, we ensure that
the model is exposed to a diverse set of experiences, which also helps mitigate
the risk of overfitting.
Another important point is that our trajectory buffer operates on a FIFO (FirstIn-First-Out) basis. As newer trajectories are added to the buffer, the oldest ones
are replaced. This buffer maintains a dynamic structure where trajectories are
continually updated to reflect the most recent learning and also helps to reduce
distribution drift. We expect that these newer trajectories are more likely to be
better-performing as they are generated from the most current learned policy.
All these techniques are implemented in our buffer and help to balance exploration with prioritizing higher-performing trajectories while also reducing the
risk of overfitting.
\subsection{Incorporation of The Worst Trajectories}\label{worst_trajectories}
In our approach, we prioritize leveraging
higher-performing trajectories to optimize the agents learning efficiency and
to accelerate convergence toward optimal policies. This focus allows the agent
to reinforce successful behaviors more effectively.
However, we understand the concern regarding forgetting catastrophic behaviors, which could potentially lead to the agents catastrophic behaviors. In
practice, the FIFO buffer and uniform sampling from the sampled trajectories
make sure that a diverse range of experiences, including suboptimal or catastrophic behaviors, are preserved to some extent within the buffer. This diversity
helps the agent to maintain a broad understanding of the environment, including both successful and unsuccessful strategies.
Additionally, while we do not explicitly prioritize the worst trajectories, our
approach does not entirely discard them. By maintaining a diverse buffer, the
agent is still exposed to these behaviors, which can serve as alerting examples.
This exposure helps the agent learn to avoid repeating such catastrophic actions
without the need to focus on the worst trajectories explicitly. We believe this
balance allows the agent to focus on learning from successful strategies while
still retaining an understanding of less optimal behaviors, reducing the risk of
catastrophic forgetting.


\subsection{Sample Efficiency Analysis}\label{sample_efficiency_analysis}
In this section, we present the sample efficiency analysis for the proposed HP3O algorithm, compared to the vanilla PPO algorithm, which remains the most popular on-policy scheme so far. Though the analysis is conducted particularly for the comparison between PPO and HP3O, the techniques apply extensively to other on-policy policy-gradient-based algorithms whenever they satisfy the conservative policy iteration property~\cite{kakade2002approximately,achiam2017constrained} to have the policy improvement lower bounds.


In this study, we aim to show how the off-policy sample reuse significantly affects the original sample efficiency PPO has. We will not directly show the exact sample complexity of HP3O and the improvement on top of PPO. For instance, to arrive at an $\varepsilon$-optimality for policy gradient-based algorithms, a few works~\cite{zhong2024theoretical,zanette2021cautiously,dai2023refined,sherman2023improved} have revealed the exact complexity with respect to $\varepsilon$, but only for MDPs with linear function approximation. The exact sample complexity analysis for the on-policy PPO algorithm remains extremely challenging and requires a substantial amount of non-trivial effort. Thereby, in this paper, we disclose the impact of off-policy sample reuse on the tradeoff between sample efficiency and learning stability. 


To start with the comparison between PPO and HP3O, we denote by $\epsilon_H$ and $\epsilon_P$ the clipping parameters for HP3O and PPO. Such a clipping parameter indicates the worst-case expected performance loss of update at every time step. We next present a lemma that shows the relationship between $\epsilon_H$ and $\epsilon_P$.
\begin{lemma}\label{lemma_6}
    Consider prior policies $|\mathcal{B}|$ randomly sampled from the replay buffer $R$ with indices $i=0,1,...,|\mathcal{B}|-1$. For any distribution $v=[v_1,v_2,...,v_{|\mathcal{B}|}]$ over the $|\mathcal{B}|$ prior policies, both HP3O and PPO have the same worst-case expected performance loss at every update when the clipping parameters satisfy the following condition:
    \begin{equation}
        \epsilon_H = \frac{\epsilon_P}{\mathbb{E}_{i\sim v}[i+1]}.
    \end{equation}
\end{lemma}
\begin{proof}
Recall from PPO such that 
\begin{equation}\label{eq_28}
    \frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{s\sim d^{\pi_k}}[\delta(\pi,\pi_k)(s)]\leq \frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\frac{\epsilon_P}{2}.
\end{equation}
For HP3O, its penalty term in the policy improvement lower bound in Theorem~\ref{theorem_1} can be upper bounded by using the Triangle inequality. Therefore, we have the following relationship
\begin{equation}\label{eq_29}
\begin{split}
    &\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{i\sim v}[\mathbb{E}_{s\sim d^{\pi_i}}[\delta(\pi,\pi_i)(s)]]\\&\leq \frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{i\sim v}\bigg[\sum_{j=0}^i\mathbb{E}_{s\sim d^{\pi_{i}}}[\delta(\pi_{j+1},\pi_j)(s)]\bigg].
\end{split}    
\end{equation}
The last inequality holds if the prior policies are in a chronological order based on their histories. In practice, we do not set such an order for them, but due to the FIFO strategy we have leveraged, they can still be set in this for the sake of analysis. Since we still resort to the clipping mechanism in HP3O, each policy update approximately bounds each expected total variation distance $\mathbb{E}_{s\sim d^{\pi_{i}}}[\delta(\pi_{j+1},\pi_j)(s)]$ by $\frac{\epsilon_H}{2}$, which follows analogously from that in PPO. With this in hand, we are now ale to further bound Eq.~\ref{eq_29} in the following relationship
\begin{equation}\label{eq_30}
    \begin{split}
    &\frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{i\sim v}[\mathbb{E}_{s\sim d^{\pi_i}}[\delta(\pi,\pi_i)(s)]]\\&\leq \frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\mathbb{E}_{i\sim v}\bigg[\frac{\epsilon_H}{2}(i+1)\bigg]\\&\leq \frac{2\gamma C^\pi_{\pi_k}}{(1-\gamma)^2}\frac{\epsilon_H}{2}\mathbb{E}_{i\sim v}[i+1]
    \end{split}
\end{equation}
Comparing the bounds in Eq.~\ref{eq_28} and Eq.~\ref{eq_30} yields the desirable result.
\end{proof}
Lemma~\ref{lemma_6} technically shows us that if the two clipping parameters $\epsilon_H$ and $\epsilon_P$ satisfy the condition of $\epsilon_H = \frac{\epsilon_P}{\mathbb{E}_{i\sim v}[i+1]}$, the worst-case expected performance loss at each update remains roughly the same. This intuitively makes sense as HP3O leverages prior policies from the replay buffer to update the policy model, which requires it to perform smaller updates. A benefit from this is to make policy updates more frequently, thus schematically stabilizing policy learning. In what follows, we present more analysis about this tradeoff.


To ease the analysis, we assume that the policies in the training batch $\mathcal{B}$ are randomly sampled with uniform policy weights, i.e., $v_i=\frac{1}{|\mathcal{B}|}$, for $i=0,1,...,|\mathcal{B}|-1$, for collecting data to train the network models. However, more advanced techniques such as Prioritized Experience Replay (PER)~\cite{schaul2015prioritized} can be applied accordingly. In each episode, we also assume that for PPO, it requires $N=Mn$ samples for sufficiently training the critic and actor networks, where $M$ is the number of mini-batch and $n$ is the batch size. In this setting, PPO makes one episodic update upon the current policy $\pi_k$ by traversing $N$ samples generated by $\pi_k$. However, for HP3O, since there exist multiple policies prior to $\pi_k$, it is able to make $M$ updates sourced from different prior policies per $N$ samples collected from $\mathcal{B}$, as long as $|\mathcal{B}|\leq M$. Thus, we next show that HP3O is able to increase the change in the total variational distance of the policy throughout training, without sacrificing stability, when it is compared to PPO. 
\begin{theorem}\label{theorem_3}
    Suppose that $|\mathcal{B}|=M$ and that the policies in the training batch $\mathcal{B}$ are randomly sampled with uniform policy weights, i.e., $v_i=\frac{1}{|\mathcal{B}|}$, for $i=0,1,...,|\mathcal{B}|-1$. Then, HP3O has a larger frequency of change in total variation distance of the policy throughout training by a factor of $\frac{2M}{M+1}$ compared to PPO, while using the same number of samples for each update as PPO.
\end{theorem}
\begin{proof}
    Pertaining to Lemma~\ref{lemma_6} and the fact that $|\mathcal{B}|=M$, we have the following relationship:
    \begin{equation}
        \epsilon_H=\frac{\epsilon_P}{\frac{1}{M}\sum_{i=0}^{M-1}(i+1)}=\frac{2\epsilon_P}{M+1}.
    \end{equation}
    PPO makes one episodic policy update after $N$ samples are collected, say from $k$ to $k+1$, which yields a policy change of $\frac{\epsilon_P}{2}$ in terms of the total variation distance. While for HP3O, it resorts to data from prior policies to obtain $N$ samples and makes $M$ policy updates, as mentioned before. This results in the overall policy change of 
    \begin{equation}
        M\frac{\epsilon_H}{2}=\frac{2M}{M+1}\frac{\epsilon_P}{2}.
    \end{equation}
    Thus, HP3O has a larger frequency of changes in the total variation distance of the policy throughout training by a factor of $\frac{2M}{M+1}$ compared to PPO, with the same number of samples.
\end{proof}
By far, we have discussed the tradeoff between learning stability and sample size that biases toward learning stability when maintaining the same sample size as in PPO. Alternatively, we can perceive the problem from another perspective, in which HP3O needs to increase the sample size while maintaining the same change in total variation distance throughout training. A formal result is summarized as follows.
\begin{theorem}\label{theorem_4}
    Suppose that $|\mathcal{B}|=2M-1$ and that the policies in the training batch $\mathcal{B}$ are randomly sampled with uniform policy weights, i.e., $v_i=\frac{1}{|\mathcal{B}|}$, for $i=0,1,...,|\mathcal{B}|-1$. Thus, HP3O increases the sample size used for each policy update by a factor of $\frac{2M-1}{M}$ compared to PPO, simultaneously maintaining the same change in the total variation of distance of the policy throughout training as PPO.
\end{theorem}
\begin{proof}
    As $|\mathcal{B}|=2M-1$, HP3O uses $(2M-1)n$ to calculate each policy update from the prior to the new policy, compared to $Mn$ samples used in PPO. Hence, HP3O increases the sample size used for each policy update by a factor of $\frac{2M-1}{M}$ compared to PPO. Immediately, based on Lemma~\ref{lemma_6}, we can obtain
    \begin{equation}
        \epsilon_H=\frac{\epsilon_P}{\frac{1}{2M-1}\sum_{i=0}^{2M-2}(i+1)}=\frac{\epsilon_P}{M}.
    \end{equation}
    We have shown in Theorem~\ref{theorem_3} that PPO makes one policy update with $N$ samples collected, while HP3O makes $M$ policy updates with the same number of samples collected. We then have
    \begin{equation}
        \frac{M\epsilon_H}{2} = \frac{\epsilon_P}{2}.
    \end{equation}
    This implies that the overall change in total variation distance in HP3O is the same as in PPO.
\end{proof}
One implication from Theorem~\ref{theorem_3} and~\ref{theorem_4} is that HP3O with uniform policy weights enhances the tradeoff between learning stability and sample efficiency in the vanilla PPO when $|\mathcal{B}|$ is selected between $[M, 2M-1]$. This also motivates us to set the FIFO strategy as the selected training batch $\mathcal{B}$ cannot deviate too far away from the current policy. Otherwise, the negative impact of distribution drift could be extreme. 



\subsection{HP3O vs. HP3O+}\label{hp3o_hp3o+}
In the last subsection, we have shown that HP3O enables more frequent changes in the total variational distance of the policy throughout training, with the smaller updates. Though more changes in the total variational distance of the policy may help improve the sample efficiency, but in order to address the distribution drift, smaller updates are the resulting outcome, possibly slowing down the convergence. Hence, introducing the best trajectory $\pi^*$ in HP3O+ assists in mitigating this issue. Since it can increase the update, while maintaining the same number of changes as in HP3O. Such a behavior is empirically shown to enhance the model performance. We summarize the larger update in the total variational distance in a formal theoretical result as follows.
\begin{theorem}\label{theorem_5}
    Denote by $D_{TV}^H$ and $D_{TV}^{H+}$ the updates of total variational distance of the policies for HP3O and HP3O+, respectively, at the time step $k$. Then we have $D_{TV}^H\leq D_{TV}^{H+}$ for all $k$.
\end{theorem}
\begin{proof}
    In light of Theorem~\ref{theorem_1} and Theorem~\ref{theorem_2}, we know that $D_{TV}^H=\frac{\gamma C^\pi_{\pi_k}\epsilon}{(1-\gamma)^2}$ and $D_{TV}^{H+}=\frac{\gamma \hat{C}^\pi_{\pi_k}\epsilon}{(1-\gamma)^2}+\frac{\gamma C^{\pi_k}\epsilon}{(1-\gamma)^2}$. We next show the the latter is bounded below by the former. As $A^{\pi_k}(s,a) = \hat{A}^{\pi_k}(s,a)+G^{\pi_k}(s)$, $\hat{A}^{\pi_k}(s,a)=Q^{\pi_k}(s,a)-V^{\pi^*_k}(s)$, and $G^{\pi_k}(s)=V^{\pi^*_k}(s)-V^{\pi_k}(s)$, we have the following relationship
    \begin{equation}
        A^{\pi_k}(s,a) = \hat{A}^{\pi_k}(s,a)+V^{\pi^*_k}(s)-V^{\pi_k}(s)
    \end{equation}
Taking the expectation of the action $a~\sim\pi(\cdot|s)$ on both sides yields 
\begin{equation}
        \mathbb{E}_{a~\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)] = \mathbb{E}_{a~\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]+V^{\pi^*_k}(s)-V^{\pi_k}(s),
\end{equation}
which leads to
\begin{equation}
\begin{split}
    |\mathbb{E}_{a~\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]| &= |\mathbb{E}_{a~\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]+V^{\pi^*_k}(s)-V^{\pi_k}(s)|\\&\leq |\mathbb{E}_{a~\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]|+|V^{\pi^*_k}(s)-V^{\pi_k}(s)|.
\end{split}
\end{equation}
This last inequality is due to the Triangle inequality. Taking the maximum operator of the state $s\sim\mathcal{S}$ on both sides results in the following:
\begin{equation}
    \text{max}_{s\sim\mathcal{S}}|\mathbb{E}_{a~\sim\pi(\cdot|s)}[A^{\pi_k}(s,a)]|\leq \text{max}_{s\sim\mathcal{S}}|\mathbb{E}_{a~\sim\pi(\cdot|s)}[\hat{A}^{\pi_k}(s,a)]|+\text{max}_{s\sim\mathcal{S}}|V^{\pi^*_k}(s)-V^{\pi_k}(s)|.
\end{equation}
Multiplying both sides in the above inequality by $\frac{\gamma\epsilon}{(1-\gamma)^2}$ yields the desirable result.
\end{proof}
% \subsection{Analysis of Theorem 5 and the Lower Bound Comparison}\label{Theorem 5 Lower Bound}

Theorem~\ref{theorem_5} examines the conditions under which HP3O+ effectively balances exploration and stability. The primary focus is on analyzing the updates introduced by the total variation distance (TVD) for HP3O and HP3O+, rather than directly comparing the overall lower bound. As highlighted in~\ref{sample_efficiency_analysis} and aligned with the analysis in GEPPO, HP3O facilitates more frequent but smaller changes in the TVD of the policy throughout training compared to vanilla PPO. These frequent changes can enhance sample efficiency, but the smaller update size may also lead to slower convergence due to distribution drift.

To mitigate this, HP3O+ incorporates a best-trajectory mechanism that increases the magnitude of updates in TVD while maintaining the same frequency of changes as HP3O. Although this modification could theoretically yield a weaker lower bound, the policy improvement lower bound for HP3O+ may still be higher, given that the analysis is centered around TVD and does not capture all practical effects, such as exploration-exploitation dynamics, which significantly impact empirical performance.

The interplay between the terms \( -D^H_{TV} \) and \( -D^{H^+}_{TV} \) is influenced by the overall training configuration, particularly hyperparameter settings that regulate exploration dynamics. HP3O+ is structured to introduce a more controlled form of exploration, leading to improved empirical performance despite potential differences in theoretical bounds. Empirical results further indicate that HP3O+ benefits from larger TVD updates, helping to counteract the drawbacks of smaller, incremental changes in HP3O and leading to a faster convergence rate.

The balance between update magnitude and frequency plays a crucial role in optimizing exploration and exploitation. The empirical evidence suggests that the structured exploration introduced by HP3O+ often outweighs any theoretical disadvantages, resulting in overall performance improvements. To provide a more comprehensive perspective, the appendix includes additional discussion connecting these theoretical insights with the empirical results.

This analysis situates HP3O+ within the broader landscape of PPO-based methods, emphasizing its ability to maintain an effective trade-off between stability and exploration while achieving stronger empirical results.


% Regarding your concern about Theorem 5 and the lower bound comparison, we understand your point that the inequality $-D^H_{TV} \geq -D^{H^+}_{TV}$, alongside the apparent difference in the first term on the RHS of Eq. (4) versus Eq. (6), may suggest a potentially weaker bound for HP3O+. However, it is important to note that our goal with Theorem 5 is to demonstrate the conditions under which HP3O+ can effectively balance the trade-off between exploration and stability.
% In Theorem 5, our focus is primarily on analyzing the updates brought by the total variation distance (TVD) for HP3O and HP3O+, rather than comparing the overall lower bound. As noted in the revisions in Section A.4 and in line with the analysis presented in GEPPO, HP3O facilitates more frequent but smaller changes in the TVD of the policy throughout training compared to vanilla PPO. While these frequent changes in TVD can potentially enhance sample efficiency, the smaller update size may contribute to slower convergence due to distribution drift.

% To address this, HP3O+ introduces a best-trajectory mechanism to increase the magnitude of updates in the TVD, while maintaining the same frequency of changes as HP3O. We understand your concern that this approach could theoretically yield a weaker lower bound for HP3O+. However, it is important to emphasize that our analysis pertains only to TVD, meaning the policy improvement lower bound for HP3O+ could still be higher despite the differences in TVD. Additionally, we would like to clarify that our theoretical analysis does not capture all practical effects, such as exploration-exploitation dynamics, which often have a significant impact on empirical performance.

% In particular, the interplay between the terms $-D^H_{TV}$ and $-D^{H^+}_{TV}$ is influenced by the overall configuration of the training process hyperparameter settings, which controls the structure of exploration and other related dynamics. The HP3O+ mechanism is designed to introduce a more structured exploration, contributing to the improved empirical performance of HP3O+ even if, theoretically, the bounds appear to be looser. We argue that the empirical results illustrate that this added structure often outweighs any disadvantage introduced by the theoretical lower bound, thereby leading to a net benefit in practice.

% To provide further context, our empirical results indicate that the HP3O+ method's ability to introduce larger updates in TVD helps in overcoming the drawbacks of smaller, incremental changes seen in HP3O, ultimately leading to a faster convergence rate. The interplay between update magnitude and frequency is crucial in balancing exploration and exploitation, and we hypothesize that this mechanism contributes to the observed empirical advantage of HP3O+ over HP3O. We will ensure that the final revision includes a more detailed discussion in the appendix to better align the theoretical findings with the empirical observations.

% In summary, we have carefully evaluated our theoretical results with PPO as a standard, focusing on the magnitude and frequency of TVD updates. Our empirical findings support the hypothesis that HP3O+ strikes a beneficial balance between exploration and stability, ultimately resulting in stronger empirical performance despite the potential for a looser theoretical bound. We appreciate your comments, and we will contribute to making these connections clearer in the paper.




















\subsection{Training results for other environments}

The following plot in Figure~\ref{fig:Cartpole} presents the training curves obtained by training both the baseline algorithms and our policy. These results further support our claim in the main paper that our policy reduces variance while maintaining a high reward at the end.

%%%% CartPole 
% \begin{figure}[h]
%     \centering
%     \includegraphics[width= 0.75\textwidth]{Figures/mean_variance/Mean_Variance_Cartpole_HP3O_vs_PPO.png}
%     \caption{Training curves (over 100k steps) on the classical Cartpole environment.}
%     \label{fig:Cartpole}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.65\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/Mean_Variance_Cartpole_HP3O_vs_PPO.png}
        \caption{Training performance of HP3O and PPO on the Cartpole environment over 100k steps.}
        \label{fig:Cartpole}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.65\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/Mean_Variance_Humanoid_HP3O_vs_PPO.png}
        \caption{Training performance of HP3O and PPO on the Humanoid environment over 10 million steps.}
        \label{fig:humanoid}
    \end{subfigure}
    \caption{Comparison of HP3O and PPO training curves across different environments. (a) shows the performance on Cartpole, while (b) shows the performance on Humanoid.}
    \label{fig:mean_variance}
\end{figure}




% \subsection*{Additional Algorithm Framework}
% \textbf{Prioritized Trajectory Replay.} In off-policy algorithms with experience replay buffer, past experiences are sampled randomly with equal importance such as by the uniform sampling. While prioritized experience replay (PER)~\cite{schaul2015prioritized} found that some experiences should be put more significance and frequency for reuse. Following this technical idea, prioritized trajectory replay (PTR)~\cite{liang2021ptr} was developed accordingly for policy optimization algorithms to further mitigate the high variance issue. Similarly, we incorporate PTR into the proximal policy optimization approaches. Different from~\cite{liang2021ptr}, the PTR in our study uses the absolute value of the advantage at each time step for quantifying the priority, which is also significantly different from the temporal difference error in PER. With this, we additionally develop one variant of OPPO-BAT, termed OPPO-BAT-PTR. The algorithmic framework is presented in the Supplementary Materils due to the space limits. The adoption of PTR makes more effective use of past trajectories generated prior policies, which will be validated by the empirical evidence.
\subsection{Additional Experimental Results}\label{additional_results}
\textbf{Definition of explained variance.} The explained variance (EV) measures the proportion to which a mathematical model accounts for the variation of a given data set, which can be mathematically defined in the following:
\begin{equation}
    EV = 1-\frac{Var(y-\hat{y})}{Var(y)},
\end{equation}
where $y$ is the groundtruth and $\hat{y}$ is the prediction. EV values typically vary from 0 to 1. In some scenarios, the value may be a large negative number, which indicates a poor prediction of $y$.
Explained variance is a well-known metric in reinforcement learning, particularly for assessing the accuracy of value function predictions. In our experiment, explained variance was used to evaluate how well the value function predicts actual returns. The different runs correspond to separate training instances with different random seeds. The explained variance score is a risk metric that measures the dispersion of errors in a dataset. A score closer to 1.0 is better, as it indicates smaller squares of standard deviations of errors.

% References follow the acknowledgments in the camera-ready paper. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% Note that the Reference section does not count towards the page limit.
% \medskip
\subsection{Explained variance for other environments}\label{explained_variance}
Explained variance is a well-known metric in reinforcement learning, particularly for assessing the accuracy of value function predictions. In our experiment, explained variance was used to evaluate how well the value function predicts actual returns. The different runs correspond to separate training instances with different random seeds. 
% Denote by $Y$ the ground truth and $\tilde{Y}$ the prediction. The explained variance ($EV$) can be calculated by the following:
% \begin{equation}
%     EV = 1-\frac{Var[Y-\tilde{Y}]}{Var[Y]},
% \end{equation}
% where $Var[Y-\tilde{Y}]$ indicates the variance of difference $Y-\tilde{Y}$ and $Var[Y]$ indicates the variance of $Y$. 
% The explained variance score is a risk metric that measures the dispersion of errors in a dataset. A score closer to 1.0 is better, as it indicates smaller squares of standard deviations of errors.

% The following plots display the explained variance for the remaining environments we have trained. These plots provide additional evidence supporting our claim in the main paper that our approach generally achieves better performance in terms of explained variance.  Although the results may vary depending on the specific environment, they consistently follow our overall findings. 


%%%% Explained Variance for Hopper
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_HP3O_Hopper.png}
        \caption{Hopper with HP3O}
        \label{fig:explained_variance_hopper_hp3o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_PPO_Hopper.png}
        \caption{Hopper with PPO}
        \label{fig:explained_variance_hopper_ppo}
    \end{subfigure}
    \caption{Explained Variance for Hopper}
\end{figure}

%%%% Explained Variance for Walker
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_HP3O_Walker.png}
        \caption{Walker with HP3O}
        \label{fig:explained_variance_walker_hp3o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_PPO_Walker.png}
        \caption{Walker with PPO}
        \label{fig:explained_variance_walker_ppo}
    \end{subfigure}
    \caption{Explained Variance for Walker}
\end{figure}

%%%% Explained Variance for Swimmer
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_HP3O_Swimmer.png}
        \caption{Swimmer with HP3O}
        \label{fig:explained_variance_swimmer_hp3o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_PPO_Swimmer.png}
        \caption{Swimmer with PPO}
        \label{fig:explained_variance_swimmer_ppo}
    \end{subfigure}
    \caption{Explained Variance for Swimmer}
\end{figure}

%%%% Explained Variance for Cartpole
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_HP3O_Cartpole.png}
        \caption{Cartpole with HP3O}
        \label{fig:explained_variance_cartpole_hp3o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_PPO_Cartpole.png}
        \caption{Cartpole with PPO}
        \label{fig:explained_variance_cartpole_ppo}
    \end{subfigure}
    \caption{Explained Variance for Cartpole}
\end{figure}

%%%% Explained Variance for LunarLander
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_HP3O_Lunarlander.png}
        \caption{LunarLander with HP3O}
        \label{fig:explained_variance_lunarlander_hp3o}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/explained_variance/violin_plot/explain_variance_PPO_Lunarlander.png}
        \caption{LunarLander with PPO}
        \label{fig:explained_variance_lunarlander_ppo}
    \end{subfigure}
    \caption{Explained Variance for LunarLander}
\end{figure}



\subsection{SAC Training benchmarks}


The following plots showcase the benchmark training results obtained by using the SAC policy. In some environments, SAC shows a relatively large variance. A notable disadvantage of SAC is that it only works with continuous action spaces.

%%%% SAC Training Curves
\begin{figure}[h]
    \centering
    %%%% LunarLander SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Lunarlander_SAC.png}
        \caption{LunarLander}
        \label{fig:SAC_LunarLander}
    \end{subfigure}
    \hfill
    %%%% HalfCheetah SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_HalfCheetah_SAC.png}
        \caption{HalfCheetah}
        \label{fig:SAC_HalfCheetah}
    \end{subfigure}

    
    %%%% Hopper SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Hopper_SAC.png}
        \caption{Hopper}
        \label{fig:SAC_Hopper}
    \end{subfigure}
    \hfill
    %%%% InvertedPendulum SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Invertedpendulum_SAC.png}
        \caption{InvertedPendulum}
        \label{fig:SAC_InvertedPendulum}
    \end{subfigure}


    %%%% Walker SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Walker_SAC.png}
        \caption{Walker}
        \label{fig:SAC_Walker}
    \end{subfigure}
    \hfill
    %%%% Swimmer SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Swimmer_SAC.png}
        \caption{Swimmer}
        \label{fig:SAC_Swimmer}
    \end{subfigure}

    
    %%%% Humanoid SAC
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/mean_variance/SAC/Mean_Variance_Humanoid_SAC.png}
        \caption{Humanoid}
        \label{fig:SAC_Humanoid}
    \end{subfigure}
    \caption{Training curves for SAC across various environments. The solid curves indicate the mean, while the shaded areas represent the standard deviation over the five runs.}
    \label{fig:SAC_training_curves}
\end{figure}


\subsection{Experimental configuration}
Experiments were performed on a local machine with an Intel Core i7-14700 CPU, 128 GB of RAM, and NVIDIA RTX 4090 GPU. We provide detailed information on our algorithms' hyperparameters for all environments in our GitHub repository, which will be released once the paper is published.

\subsection{Data Storage for RL Training}
\begin{itemize}
  \item \textbf{Trajectory Buffer}: Stores complete trajectories $\tau$ as sequences of $(s_t, a_t, r_t, s_{t+1})$.
    \begin{itemize}
      \item \textbf{Data Types}: Arrays of states, actions, rewards, and next states.
      \item \textbf{Dimensions}:
        \begin{itemize}
          \item States $s_t$: Typically $\mathbb{R}^n$ where $n$ is the dimension of the state space.
          \item Actions $a_t$: Depends on the action space, usually $\mathbb{R}^m$ where $m$ is the dimension of the action space.
          \item Rewards $r_t$: Scalar values.
          \item Next states $s_{t+1}$: Same as states $s_t$.
        \end{itemize}
    \end{itemize}
\end{itemize}

% \resizebox{\textwidth}{!}{
% }


% \begin{table}[htbp]
% \caption{Summary of Mean and Standard Deviation of Rewards for Each Policy (in the form Mean $\pm$ Std).}
% \label{tab:mean_std_summary}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Environment} & \multicolumn{6}{c}{\textbf{Mean $\pm$ Std of Rewards}} \\n
% Policy & A2C & HP3O & HP3O+ & OFFPOLICY & P3O & PPO \\
% Environment &  &  &  &  &  &  \\
% \midrule
% cartPole & $162.75 \pm 89.84$ & $270.52 \pm 22.95$ & $\mathbf{487.21 \pm 1.11}$ & $291.24 \pm 38.25$ & $265.99 \pm 115.77$ & $349.84 \pm 40.26$ \\
% halfcheetah & $381.14 \pm 292.49$ & $2083.25 \pm 298.08$ & $\mathbf{2378.28 \pm 238.48}$ & $544.36 \pm 226.77$ & $881.99 \pm 407.94$ & $1797.84 \pm 468.75$ \\
% hopper & $118.85 \pm 50.22$ & $759.57 \pm 16.13$ & $\mathbf{941.28 \pm 74.91}$ & $869.67 \pm 109.16$ & $821.88 \pm 328.79$ & $846.05 \pm 89.11$ \\
% invertedPendulum & $173.30 \pm 161.71$ & $\mathbf{763.75 \pm 47.34}$ & $720.28 \pm 90.95$ & $10.90 \pm 3.08$ & $639.52 \pm 150.25$ & $233.76 \pm 213.24$ \\
% lunarLander & $-182.97 \pm 198.93$ & $65.20 \pm 20.64$ & $59.26 \pm 21.85$ & $64.63 \pm 75.99$ & $-615.96 \pm 256.03$ & $\mathbf{86.27 \pm 13.50}$ \\
% swimmer & $14.82 \pm 1.63$ & $\mathbf{252.59 \pm 12.63}$ & $249.13 \pm 13.06$ & $77.83 \pm 52.83$ & $29.23 \pm 16.33$ & $100.03 \pm 29.95$ \\
% walker & $148.45 \pm 70.68$ & $851.42 \pm 144.76$ & $736.88 \pm 92.12$ & $\mathbf{1522.20 \pm 246.11}$ & $1107.65 \pm 311.50$ & $708.39 \pm 142.92$ \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


% \begin{table}[htbp]
% \centering
% \caption{Summary of mean and standard deviation of rewards for each policy across diverse environments (in the form Mean $\pm$ Std) at or close to the converged stage. The bold one represents the best reward performance.}
% \label{tab:mean_std_summary}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Environment} & \textbf{A2C} & \texttt{HP3O} & \texttt{HP3O+} & \textbf{OffPolicy} & \textbf{P3O} & \textbf{PPO} \\ \midrule
% cartPole & $282.47 \pm 170.87$ & $498.25 \pm 2.51$ & $\mathbf{500.00 \pm 0.00}$ & $400.31 \pm 72.60$ & $295.38 \pm 113.44$ & $483.59 \pm 36.70$ \\ 
% halfcheetah & $334.16 \pm 302.14$ & $3523.20 \pm 565.39$ & $\mathbf{3967.47 \pm 244.80}$ & $738.11 \pm 274.89$ & $1251.17 \pm 517.84$ & $2276.87 \pm 902.20$ \\ 
% hopper & $120.31 \pm 48.29$ & $988.67 \pm 16.53$ & $\mathbf{1891.35 \pm 79.47}$ & $961.64 \pm 76.69$ & $1107.49 \pm 281.71$ & $946.90 \pm 64.86$ \\ 
% invertedPendulum & $71.26 \pm 76.20$ & $956.66 \pm 36.20$ & $\mathbf{1000.00 \pm 0.00}$ & $11.24 \pm 3.22$ & $810.77 \pm 46.26$ & $463.02 \pm 445.28$ \\ 
% lunarLander & $-69.13 \pm 105.52$ & $\mathbf{225.91 \pm 10.97}$ & $146.63 \pm 7.10$ & $114.68 \pm 106.51$ & $-624.72 \pm 292.51$ & $130.58 \pm 16.53$ \\ 
% swimmer & $12.99 \pm 6.25$ & $340.00 \pm 4.18$ & $\mathbf{343.40 \pm 1.41}$ & $157.73 \pm 107.07$ & $32.75 \pm 14.32$ & $131.98 \pm 38.76$ \\ 
% walker & $134.05 \pm 51.54$ & $1934.91 \pm 152.69$ & $1895.29 \pm 98.74$ & $\mathbf{2093.24 \pm 370.08}$ & $1777.91 \pm 465.01$ & $1150.36 \pm 97.18$ \\ 
% \bottomrule
% \end{tabular}
% }
% \end{table}



\subsection{Converged Reward}\label{converged_rewards}
In this subsection, we present the converged reward at the last time step. In some environments, the training curves do not fully converge, which may make it challenging to assess the ultimate performance. However, for consistency across all algorithms, we maintained the same number of training time steps for each experiment. This allows for a fair comparison of sample efficiency across different methods, even if the algorithms did not always fully converge within the given time frame. Additionally, in some environments, we do present converged training curves, demonstrating the capabilities of the algorithms.
In the reinforcement learning community, it is a common practice to show learning curves at a fixed number of steps for comparative analysis, even if full convergence is not always achieved. Notably, papers on SAC~\cite{haarnoja2018soft}, PPO~\cite{schulman2017proximal}, GEPPO~\cite{queeney2021generalized}, and Off-Policy PPO~\cite{meng2023off} follow similar practices, with many of the environments presented in these works employing non-converged curves to provide valuable insights into training dynamics and sample efficiency. Table~\ref{tab:mean_std_summary} shows the detailed converged reward performance of all different RL algorithms over different continuous tasks. In order to ensure a fair comparison between GEPPO and our method, we first analyzed the performance differences between the PPO baselines in our implementation vs. in the GEPPO repository. These discrepancies were primarily due to variations in implementation details (e.g., leveraging TensorFlow packages, early version of Mujoco environment), which significantly impacted the baseline performance. To address the discrepancies, we normalized the PPO baseline results to match our implementation. As a result, both baselines produced comparable outcomes. We then applied the same normalization factor to the GEPPO results, repeating this procedure for each environment to ensure fair and consistent comparisons across all settings.
\begin{table}[htbp]
\centering
\caption{Summary of mean and standard deviation of rewards for each policy across diverse environments (in the form Mean $\pm$ Std) at or close to the converged stage. The bold one represents the best reward performance.}
\label{tab:mean_std_summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Environment} & \textbf{A2C} & \textbf{GEPPO} & \textbf{HP3O} & \textbf{HP3O+} & \textbf{OffPolicy} & \textbf{P3O} & \textbf{PPO}\\ \midrule
CartPole & $282.47 \pm 170.87$ & $21.76 \pm 2.54$ & $498.25 \pm 2.51$ & $\mathbf{500.00 \pm 0.00}$ & $400.31 \pm 72.60$ & $295.38 \pm 113.44$ & $483.59 \pm 36.70$\\ 
Halfcheetah & $334.16 \pm 302.14$ & $2156.31 \pm 1024.06$ & $3523.20 \pm 565.39$ & $\mathbf{3967.47 \pm 244.80}$ & $738.11 \pm 274.89$ & $1251.17 \pm 517.84$ & $2276.87 \pm 902.20$ \\ 
Hopper & $120.31 \pm 48.29$ & $976.33 \pm 68.36$ & $988.67 \pm 16.53$ & $\mathbf{1891.35 \pm 79.47}$ & $961.64 \pm 76.69$ & $1107.49 \pm 281.71$ & $946.90 \pm 64.86$\\ 
InvertedPendulum & $71.26 \pm 76.20$ & $463.02 \pm 0.00$ & $956.66 \pm 36.20$ & $\mathbf{1000.00 \pm 0.00}$ & $11.24 \pm 3.22$ & $810.77 \pm 46.26$ & $463.02 \pm 445.28$\\ 
LunarLander & $-69.13 \pm 105.52$ & $136.07 \pm 12.33$ & $\mathbf{225.91 \pm 10.97}$ & $146.63 \pm 7.10$ & $114.68 \pm 106.51$ & $-624.72 \pm 292.51$ & $130.58 \pm 16.53$ \\ 
Swimmer & $12.99 \pm 6.25$ & $133.83 \pm 6.69$ & $340.00 \pm 4.18$ & $\mathbf{343.40 \pm 1.41}$ & $157.73 \pm 107.07$ & $32.75 \pm 14.32$ & $131.98 \pm 38.76$\\ 
Walker & $134.05 \pm 51.54$ & $1140.31 \pm 389.75$ & $1934.91 \pm 152.69$ & $1895.29 \pm 98.74$ & $\mathbf{2093.24 \pm 370.08}$ & $1777.91 \pm 465.01$ & $1150.36 \pm 97.18$ \\ 
\bottomrule
\end{tabular}
}
\end{table}
% \begin{table}[htbp]
% \centering
% \caption{Summary of mean and standard deviation of rewards for each policy across diverse environments (in the form Mean $\pm$ Std) at or close to the converged stage. The bold one represents the best reward performance.}
% \label{tab:mean_std_summary}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lcccccccc}
% \toprule
% \textbf{Environment} & \textbf{A2C} & \textbf{GEPPO} & \textbf{HP3O} & \textbf{HP3O+} & \textbf{OffPolicy} & \textbf{P3O} & \textbf{PPO} & \textbf{SAC} \\ \midrule
% cartPole & $282.47 \pm 170.87$ & $21.76 \pm 2.54$ & $498.25 \pm 2.51$ & $500.00 \pm 0.00$ & $400.31 \pm 72.60$ & $295.38 \pm 113.44$ & $483.59 \pm 36.70$ & - \\ 
% halfcheetah & $334.16 \pm 302.14$ & $2156.31 \pm 1024.06$ & $3523.20 \pm 565.39$ & $3967.47 \pm 244.80$ & $738.11 \pm 274.89$ & $1251.17 \pm 517.84$ & $2276.87 \pm 902.20$ & $10430.57 \pm 222.92$ \\ 
% hopper & $120.31 \pm 48.29$ & $976.33 \pm 68.36$ & $988.67 \pm 16.53$ & $1891.35 \pm 79.47$ & $961.64 \pm 76.69$ & $1107.49 \pm 281.71$ & $946.90 \pm 64.86$ & $3335.40 \pm 185.38$ \\ 
% invertedPendulum & $71.26 \pm 76.20$ & $463.02 \pm 0.00$ & $956.66 \pm 36.20$ & $1000.00 \pm 0.00$ & $11.24 \pm 3.22$ & $810.77 \pm 46.26$ & $463.02 \pm 445.28$ & $992.40 \pm 4.39$ \\ 
% lunarLander & $-69.13 \pm 105.52$ & $136.07 \pm 12.33$ & $225.91 \pm 10.97$ & $146.63 \pm 7.10$ & $114.68 \pm 106.51$ & $-624.72 \pm 292.51$ & $130.58 \pm 16.53$ & $277.42 \pm 9.38$ \\ 
% swimmer & $12.99 \pm 6.25$ & $133.83 \pm 6.69$ & $340.00 \pm 4.18$ & $343.40 \pm 1.41$ & $157.73 \pm 107.07$ & $32.75 \pm 14.32$ & $131.98 \pm 38.76$ & $17.04 \pm 0.23$ \\ 
% walker & $134.05 \pm 51.54$ & $1140.31 \pm 389.75$ & $1934.91 \pm 152.69$ & $1895.29 \pm 98.74$ & $2093.24 \pm 370.08$ & $1777.91 \pm 465.01$ & $1150.36 \pm 97.18$ & $4271.37 \pm 463.37$ \\ 
% \bottomrule
% \end{tabular}
% }
% \end{table}


\subsection{Computational Efficiency}\label{computational_efficiency}
To probe particularly the computational efficiency of diverse algorithms presented in this study, we compare them in the wall-clock time spent to reach a certain reward in the HalfCheetah environment. Due to the time limitation, we are unable to obtain results for all other environments, while including them in the final version. Such an investigation offers us useful insights about which methods are more practically feasible and deployable given the limited real-time budget. Figure~\ref{fig:time_to_reach_halfcheetah} shows the specific performance of wall-clock time cost for different approaches reaching the rewards of 1100 and 2100, respectively. One immediate observation from the results is that GEPPO requires significantly more time to converge compared to all other schemes. For a couple of algorithms, such as A2C and OffPolicy, the training progresses are pretty slow, eventually failing to achieve rewards of 1100 and 2100 in the HalfCheetah environment. Another implication of interest from the results is that at the beginning, PPO may progress faster, compared to both HP3O and HP3O+. However, due to its on-policy behavior, the sample inefficiency issue still affects the overall training progress. Different from that, both HP3O and HP3O+ make consistent progress throughout the training process and take minimal time to achieve certain rewards. Between them, HP3O+ has slightly better performance, which empirically validates our conclusion from Theorem~\ref{theorem_5}. The finding also complies with that in Figure~\ref{fig:run_time}.
%%%% Time Plot 
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Bar_Time_to_Reach_Reward_1100_Halfcheetah.png}
        \caption{Time to reach the reward of 1100 for HalfCheetah}
        \label{fig:Bar_Time_to_Reach_Reward_1100_Halfcheetah}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Bar_Time_to_Reach_Reward_2100_Halfcheetah.png}
        \caption{Time to reach the reward of 2100 for HalfCheetah}
        \label{fig:Bar_Time_to_Reach_Reward_2100_Halfcheetah}
    \end{subfigure}
    \caption{Time taken by algorithms to reach specific reward thresholds in the HalfCheetah environment.}
    \label{fig:time_to_reach_halfcheetah}
\end{figure}



\subsection{Impact of Buffer Size and Memory Usage}
In this subsection, we also discuss the impact of the buffer size on the model performance qualitatively. The other two aspects included here are the discussion on the sampling strategy and the trade-off in sparse reward settings.  
\begin{itemize}
    \item \textbf{Large Trajectory Buffer Size}: A larger trajectory buffer size allows us to store a greater number of diverse trajectories. This diversity can enhance generalization and reduce overfitting, as the agent learns from a broad range of experiences. However, in sparse reward settings, maintaining a large trajectory buffer may mean that the inclusion of outdated or less relevant trajectories could introduce instability and slow down learning, as the agent may be exposed to experiences that no longer align with its current policy.
    \item \textbf{Small Trajectory Buffer Size}: A smaller trajectory buffer retains fewer trajectories, which typically results in the agent learning from more recent experiences that are closely aligned with the current policy. This can improve stability, as updates are based on recent, relevant data. However, a smaller buffer can reduce the diversity of sampled experiences, leading to an increased risk of overfitting and limiting the agent's ability to effectively explore different parts of the environment.
    \item \textbf{Sampling Strategy}: Our sampling strategy also plays a critical role in managing the trade-off between stability and performance. By ensuring that the best return trajectory is always included in the sampled trajectories, we provide a strong guiding signal that improves policy performance. The sample rate, where we evenly sample from each selected trajectory, helps in maintaining a balance between exploration and exploitation, as well as in utilizing high-quality trajectories effectively.
    \item \textbf{Trade-off in Sparse Reward Settings}: In sparse reward environments, the need to maintain high-quality trajectories becomes even more crucial. A large trajectory buffer can help capture rare, valuable experiences, but the risk of dequeuing these valuable trajectories before they can contribute meaningfully to learning is higher. Ensuring that the best trajectory is always sampled helps mitigate this issue, but the buffer size still influences how effectively these rare rewards are retained and leveraged.
\end{itemize}
Regarding memory consumption, the total memory usage depends on the following:
\begin{enumerate}
    \item \textbf{Number of Trajectories in the Buffer (Buffer Size)}: We use a fixed-size replay buffer, which holds $N$ trajectories.
    \item \textbf{Average Length of Trajectories ($T$)}: Since trajectories have varying lengths, the memory usage will depend on the average trajectory length.
    \item \textbf{Dimensionality of State and Action Spaces}: Let $d_s$ be the dimension of the state, and $d_a$ be the dimension of the action. Each step in a trajectory stores both state and action information.
    \item \textbf{Data type size}: Denote by $n$ the specific data type size.
\end{enumerate}


The memory consumption ($M$) can be roughly estimated as:
\[
M = NTn(d_s + d_a)
\]
For example, if $N = 1000$ trajectories are stored, each with an average length of 200 steps, and assuming $d_s = 17$, $d_a = 6$, and using 32-bit floats (4 bytes), the memory requirement would be:
\[
M = 1000 \times 200 \times (17 + 6) \times 4 \text{ bytes} = 18,400,000 \text{ bytes} \approx 18.4 \text{ MB}
\]
This calculation provides an estimate, but the actual memory usage may vary depending on the environment and the specific implementation of the replay buffer.


\subsection{Evaluation Results }
%%%% Evaluation plot
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/evaluation_plot/Mean_Variance_Halfcheetah_HP3O_vs_PPO_eval.png}
        \caption{Evaluation plot for the HalfCheetah environment.}
        \label{fig:halfcheetah_eval}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/evaluation_plot/Mean_Variance_Hopper_HP3O_vs_PPO_eval.png}
        \caption{Evaluation plot for the Hopper environment. }
        \label{fig:hopper_eval}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/evaluation_plot/Mean_Variance_Walker_PPO_vs_A2C_eval.png}
        \caption{Evaluation plot for the Walker environment. }
        \label{fig:hopper_eval}
    \end{subfigure}
    \caption{Evaluation plots for the HalfCheetah, Hopper, and Walker environments during the evaluation stage every 5000 steps. Each experiment includes five different runs with various random seeds. The solid curves indicate the mean, while the shaded areas represent the standard deviation over the five runs.}
    \label{fig:evaluation_comparison}
\end{figure}


The evaluation results align with our training expectations. Overall, our presented models, HP3O and HP3O+, consistently outperform the baseline models across all environments, achieving higher rewards while maintaining relatively low variance. The PPO baseline performs well initially but tends to be less sample efficient and has relatively higher variance, whereas A2C struggles to reach comparable performance.


The results clearly demonstrate that our presented models, HP3O and HP3O+, are better equipped for these environments. This also verifies our claim from the training analysis. Both HP3O and HP3O+ combine improved sample efficiency and reduced variance, leading to more stable learning outcomes and higher cumulative rewards. These advantages enable our models to not only outperform the baselines but also maintain robustness and efficiency across diverse environments. Due to the time limitation, we are unable to obtain results for other environments with other methods, and will include additional results in the final version.




















%%

\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
