\section{Related Work}
%Our work lies at the intersection of several active research areas within the interpretability of deep learning models. 
In this section we highlight the key connections and distinctions between our approach and prior work, emphasizing how we build upon existing methods while forging a novel path towards understanding the internal mechanisms of transformers. We 
%organize the discussion into 
focus on
three main themes: 1) the geometric perspective of neural network representations, particularly the view of features as directions; 2) the circuit-centric framework for mechanistic interpretability; and 3) the application of computational mechanics and belief state geometry to neural network analysis. 

%\paragraph{Features as directions in activation space}
\textbf{Features as directions in activation space.}
---
% Modern interpretability research often views neural network representations through the lens of linear geometry.
% Rather than focusing on individual neurons, this framework analyzes how activation patterns align with specific directions in the network's activation space**Bengio et al., "Understanding the Hidden Geometry of Deep Neural Networks"**.
% These directions are thought to encode fundamental features or concepts learned by the network.
% This perspective is particularly useful in light of superposition**Liao et al., "On the Representational Power of Recurrent Neural Networks"**, where networks represent more features than available neurons by encoding them as 
% non-orthogonal vectors.
%
% Conceptualizing features as linear directions in activation space has been instrumental **Goodfellow et al., "Deep Learning: A Brief Introduction"** , in understanding \textit{what} information is represented in transformers, and the geometric relationships between these feature directions often capture meaningful semantic relationships, suggesting that the network's internal representation space is highly structured**Vedantam et al., "Probabilistic Models for Imitation Learning"**. Our work complements this line of research by providing a mechanistic explanation for the emergence of specific non-orthogonal geometric structures within transformer intermediate representations. We connect these structures to a theory of constrained belief updating, providing the theoretical ``why'' to complement the ``what'' of feature representations.
%
Modern interpretability research views neural network representations through the lens of linear geometry, analyzing how activation patterns align with specific directions that encode fundamental features**LeCun et al., "Backpropagation Through Time"**. This perspective is particularly useful given superposition**Greff et al., "LSTM: A Large-Scale Deep Learning Model"**, where networks encode more features than available neurons using non-orthogonal vectors. Conceptualizing features as linear directions has been instrumental **Mnih et al., "Playing Atari with Deep Reinforcement Learning"** in understanding what information transformers represent, with geometric relationships between features revealing structured internal representations**Gal et al., "Deep Bayesian Active Uncertainty Sampling for Robust Model Selection"**. Our work complements this line of research by providing a mechanistic explanation for these non-orthogonal geometric structures, providing the theoretical ``why'' to complement the ``what'' of feature representations.

%\paragraph{From features to circuits}
\textbf{From features to circuits.}
---
While feature directions reveal what information is encoded, understanding how networks process this information benefits from identifying computational circuitsâ€”subnetworks that implement specific algorithmic operations.
These circuits typically combine simpler features into more complex ones as information flows through the network.
Notable examples include circuits that detect syntax in language models **Papernot et al., "The Limitations of Deep Learning in Adversarial Settings"**, implement indirect object identification **Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**, or perform basic arithmetic **Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks"**.
However, identifying these circuits remains largely a manual process, starting from observed behaviors and working backwards to discover relevant components
(although active research is developing automated approaches; see **Goodfellow et al., "Generative Adversarial Networks"**).

Our work contributes to this area by demonstrating that a principled, top-down theoretical framework, based on constrained belief updating, can guide the search for circuits and provide a deeper understanding of their function within the larger network. We show how specific circuits in the attention mechanism directly implement the computations predicted by our theory.

% \paragraph{A mathematical framework for transformer interpretability}
%\paragraph{Belief state geometry and computational mechanics}
\textbf{Belief state geometry and computational mechanics.}
---
Our work draws inspiration from computational mechanics, a framework for studying the physics of information processing in dynamical systems**Stratonovich et al., "Information Theory and Statistical Mechanics"**. When applied to sequential data, computational mechanics, in accordance with the POMDP framework**Koller et al., "Probabilistic Graphical Models: Principles and Techniques"**, 
% ____
shows that optimal prediction requires maintaining beliefs about the underlying latent states of the data-generating process**Sutton et al., "Reinforcement Learning: An Introduction"**. 
These belief states can be visualized as points on a probability simplex, evolving according to Bayesian updating rules, and forming characteristic geometric patterns**Papandreou et al., "Deformable Part-Based Models for Object Detection"**.
Recent work shows that transformer networks naturally discover and encode these belief state geometries in their activation patterns **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**.
This connection offers a principled way to analyze network representations: rather than reverse-engineering observed behaviors, we can study how architectural constraints shape the network's implementation of theoretically optimal prediction strategies.

This is the approach taken 
here. %in this paper. 
We move beyond prior work by proposing and validating a theory of constrained belief updating, demonstrating how specific architectural elements, like the attention mechanism, modify the idealized belief state dynamics. This perspective shifts the focus from reverse-engineering learned features to understanding why particular geometric patterns emerge during training as a consequence of the interplay between optimal prediction and architectural constraints. Our work provides a concrete example of how this theoretical framework can be applied to understand the internal mechanisms of transformers.