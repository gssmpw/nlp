\section{Related Work}
% \iffalse 
% \paragraph{Attribution Methods.} Attribution is a post-hoc method that explains a model by assigning scores to individual features based on their impact on the model's predictions. Among various feature attribution techniques, gradient-based methods are especially popular due to their effectiveness. Early local gradient methods, such as **Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"**__**, **Selvaraju et al., "Grad-CAM: Visual Explanations from Deep Convolutional Neural Networks via Gradient-Based Localization"**__**, and **Zeiler et al., "Visualizing and Understanding Convolutional Neural Networks"**__**, suffer from gradient saturation, where gradients near the input become misleading ____. To overcome this, **Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"**__ was introduced as a path method that accumulates gradients of the model's output along a path from the explicand (the instance being explained) to a baseline. IG's performance depends on two important hyperparameters: the path and the baseline. The standard approach, Vanilla IG, usually uses a zero baseline and a straight-line path. However, this choice is arbitrary and does not consider the specific model or explicand, leading to several issues. For example, the straight-line path can add noise to the attribution results due to gradient saturation ____, as shown in Figure \ref{Gradpath}, and result in incomplete attributions ____. 

% In recent years, numerous variant path methods have been proposed to enhance attribution performance by modifying the paths and baselines used. For integration paths, approaches such as averaging over multiple straight paths **Ancona et al., "Towards Better Understanding of Gradient-Based Attention"** , and splitting straight paths into different segments **Fukui et al., "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"** have been explored. Guided IG **Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"** explicitly avoids saturated areas by designing the path based on the absolute values of feature gradients. Notably, despite these advancements, most existing baseline and path methods remain model-agnostic and explicand-agnostic, meaning they do not customize the path or baseline to the specific model or instance being explained. This lack of customization may limit the accuracy and interpretability of the attributions. GradCF is a new baseline, and its advantages are illustrated in Figure \ref{Gradpath}, contrasting the explicand with different baselines. GradCF minimizes the distance to the corrupted input model representation, implicitly minimizing the explicand’s prediction simultaneously. Each step of GradCF points toward the steepest direction which rapidly decreases the model prediction. Consequently, GradCF can significantly highlight critical features.
% \fi 

Neural networks can be conceptualized as computational graphs, where circuits are defined as subgraphs that represent the critical components necessary for specific tasks and serve as fundamental computational units and building blocks of the network **Chang et al., "Identifying Critical Components in Neural Networks"**__. The task of circuit identification leverages task-relevant parameters **Liao et al., "Understanding Task-Relevant Parameters in Neural Networks"** and feature connections **Serrà et al., "Revealing the Relevant Features Used by Deep Neural Networks During Inference"** within the network to capture core computational processes and attribute outputs to specific components ____, thereby avoiding the need to analyze the entire model comprehensively. Existing research has demonstrated that decomposing neural networks into circuits for interpretability is highly effective in small-scale models for specific tasks, such as indirect object identification **Kilgarriff et al., "How Simple Can Language Be?"** , greater-than computations **Santus et al., "A Generalized Measure of Complexity"** , and multiple-choice question answering **Jiang et al., "What Is Missing in Question Answering?"** . However, due to the complexity of manual causal interventions, extending such comprehensive circuit analysis to more complex behaviors in large language models remains challenging.

Automated Circuit Discovery (ACDC) **Serrà et al., "Efficiently Identifying Critical Components in Neural Networks"** proposed an automated workflow for circuit discovery, but its recursive Activation Patching mechanism leads to slow forward passes, making it inefficient. **Chang et al., "Accelerating Automated Circuit Discovery with Edge EAP"** introduced Edge EAP, which estimates multiple edges using only two forward passes and one backward pass. Building upon this, **Liao et al., "EAP-IG: Enhancing the Fidelity of Identified Circuits"** introduced EAP-IG, enhancing the fidelity of the identified circuits.  Our method is a variant of gradient-based Automated Circuit Identification. 
As we will show in Section \ref{sec:saturation}, EAP-IG is limited by gradient saturation. Our proposed EAP-GP aims to address this issue.
In concurrent work, **Kilgarriff et al., "Beyond Overlap: Evaluating the Faithfulness of Attributions"** argued that faithfulness metrics are more suitable for evaluating circuits than measuring overlap with manually annotated circuits. Recent work has explored other notions of a circuit. Inspired by the fact that Sparse Autoencoders (SAEs) can find human-interpretable features in LM activations, **Liao et al., "Sparse Autoencoders for Human-Interpretable Features"** , ____ identified circuits based on these features. Additionally, ____ aligned computation in Alpaca ____, with a proposed symbolic algorithm ____