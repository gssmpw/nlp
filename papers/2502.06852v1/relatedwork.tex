\section{Related Work}
% \iffalse 
% \paragraph{Attribution Methods.} Attribution is a post-hoc method that explains a model by assigning scores to individual features based on their impact on the model's predictions. Among various feature attribution techniques, gradient-based methods are especially popular due to their effectiveness. Early local gradient methods, such as Vanilla Gradient \citep{simonyan2014visualising}, Grad-CAM \citep{selvaraju2017gradcam}, and Guided Backpropagation \citep{springenberg2014striving}, suffer from gradient saturation, where gradients near the input become misleading \citep{shrikumar2017learning, dombrowski2019explanations}. To overcome this, Integrated Gradients (IG) \citep{sundararajan2017axiomatic} was introduced as a path method that accumulates gradients of the model's output along a path from the explicand (the instance being explained) to a baseline. IG's performance depends on two important hyperparameters: the path and the baseline. The standard approach, Vanilla IG, usually uses a zero baseline and a straight-line path. However, this choice is arbitrary and does not consider the specific model or explicand, leading to several issues. For example, the straight-line path can add noise to the attribution results due to gradient saturation \citep{miglani2020investigating}, as shown in Figure \ref{Gradpath}, and result in incomplete attributions \citep{erion2021improving, sturmfels2020visualizing}. 

% In recent years, numerous variant path methods have been proposed to enhance attribution performance by modifying the paths and baselines used. For integration paths, approaches such as averaging over multiple straight paths \citep{kapishnikov2019xrai, smilkov2017smoothgrad} , and splitting straight paths into different segments \citep{miglani2020investigating} have been explored. Guided IG \citep{kapishnikov2021guided} explicitly avoids saturated areas by designing the path based on the absolute values of feature gradients. Notably, despite these advancements, most existing baseline and path methods remain model-agnostic and explicand-agnostic, meaning they do not customize the path or baseline to the specific model or instance being explained. This lack of customization may limit the accuracy and interpretability of the attributions. GradCF is a new baseline, and its advantages are illustrated in Figure \ref{Gradpath}, contrasting the explicand with different baselines. GradCF minimizes the distance to the corrupted input model representation, implicitly minimizing the explicandâ€™s prediction simultaneously. Each step of GradCF points toward the steepest direction which rapidly decreases the model prediction. Consequently, GradCF can significantly highlight critical features.
% \fi 

Neural networks can be conceptualized as computational graphs, where circuits are defined as subgraphs that represent the critical components necessary for specific tasks and serve as fundamental computational units and building blocks of the network \citep{bereska2024mechanistic}. The task of circuit identification leverages task-relevant parameters \citep{bereska2024mechanistic} and feature connections \citep{he2024dictionary} within the network to capture core computational processes and attribute outputs to specific components \citep{miller2024transformer}, thereby avoiding the need to analyze the entire model comprehensively. Existing research has demonstrated that decomposing neural networks into circuits for interpretability is highly effective in small-scale models for specific tasks, such as indirect object identification \citep{wang2023interpretability}, greater-than computations \citep{hanna2024faith}, and multiple-choice question answering \citep{lieberum2023circuit}. However, due to the complexity of manual causal interventions, extending such comprehensive circuit analysis to more complex behaviors in large language models remains challenging.

Automated Circuit Discovery (ACDC) \citep{conmy2023automated} proposed an automated workflow for circuit discovery, but its recursive Activation Patching mechanism leads to slow forward passes, making it inefficient. \citet{syed2023attribution} introduced Edge EAP, which estimates multiple edges using only two forward passes and one backward pass. Building upon this, \citet{hanna2024faith} introduced EAP-IG, enhancing the fidelity of the identified circuits.  Our method is a variant of gradient-based Automated Circuit Identification. 
As we will show in Section \ref{sec:saturation}, EAP-IG is limited by gradient saturation. Our proposed EAP-GP aims to address this issue.
In concurrent work, \citet{hanna2024gpt2} argued that faithfulness metrics are more suitable for evaluating circuits than measuring overlap with manually annotated circuits. Recent work has explored other notions of a circuit. Inspired by the fact that Sparse Autoencoders (SAEs) can find human-interpretable features in LM activations, \citep{cunningham2023sparse}, \citet{marks2024sparse} identified circuits based on these features. Additionally, \citet{wu2024interpretability} aligned computation in Alpaca \citep{taori2023stanford} with a proposed symbolic algorithm \citep{geiger2024finding}.