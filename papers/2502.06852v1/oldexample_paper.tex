%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{multirow}
\usepackage{adjustbox}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{algorithm}
%\usepackage{algorithmicx}
\usepackage{algpseudocode}




% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

%Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% % If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[accepted]{icml2025}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%\addtolength{\intextsep}{-0.0cm}
%\setlength{\intextsep}{0.1cm} 
%\addtolength{\abovecaptionskip}{-0.0cm}
%\setlength{\abovecaptionskip}{0.7cm}
%\addtolength{\belowcaptionskip}{-0.0cm}
%\setlength{\belowcaptionskip}{0.7cm}
%\addtolength{\dbltextfloatsep}{-0.0cm}
%\setlength{\dbltextfloatsep}{0.1cm}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{xcolor}
\newcommand{\shu}[1]{\textcolor{purple}{#1}}
\newcommand{\di}[1]{\textcolor{blue}{#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}



\begin{document}

\twocolumn[
\icmltitle{EAP-GP: Mitigating Saturation
Effect in Gradient-based Automated Circuit Identification 
            }

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lin Zhang}{Kaust,PRADA,hit}
\icmlauthor{Wenshuo Dong}{Kaust,PRADA,UCPH}
\icmlauthor{Zhuoran Zhang}{pku}
\icmlauthor{Shu Yang}{Kaust,PRADA}
\icmlauthor{Lijie Hu}{Kaust,PRADA}
\icmlauthor{Ninghao Liu}{UGA}
\icmlauthor{Pan Zhou}{Hust}
\icmlauthor{Di Wang}{Kaust,PRADA}
\end{icmlauthorlist}

\icmlaffiliation{Kaust}{King Abdullah University of Science and Technology (KAUST)}
\icmlaffiliation{PRADA}{Provable Responsible AI and Data Analytics (PRADA) Lab}
\icmlaffiliation{hit}{Harbin Institute of Technology,Shenzhen}
\icmlaffiliation{UCPH}{University of Copenhagen}
\icmlaffiliation{pku}{Peking University}
\icmlaffiliation{UGA}{University of Georgia}
\icmlaffiliation{Hust}{Huazhong University of Science and Technology}

\icmlcorrespondingauthor{Di Wang}{di.wang@kaust.edu.sa}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{}
% \printAffiliationsAndNotice{\{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\vspace{-0.1in} 
Understanding the internal mechanisms of transformer-based language models remains challenging. Mechanistic interpretability based on circuit discovery aims to reverse engineer neural networks by analyzing their internal processes at the level of computational subgraphs. In this paper, we revisit existing gradient-based circuit identification methods and find that their performance is either affected by the zero-gradient problem or saturation effects, where edge attribution scores become insensitive to input changes, resulting in noisy and unreliable attribution evaluations for circuit components. To address the saturation effect, we propose Edge Attribution Patching with GradPath (EAP-GP), EAP-GP introduces an integration path, starting from the input and adaptively following the direction of the difference between the gradients of corrupted and clean inputs to avoid the saturated region. This approach enhances attribution reliability and improves the faithfulness of circuit identification. We evaluate EAP-GP on 6 datasets using GPT-2 Small, GPT-2 Medium, and GPT-2 XL. Experimental results demonstrate that EAP-GP outperforms existing methods in circuit faithfulness, achieving improvements up to 17.7\%. Comparisons with manually annotated ground-truth circuits demonstrate that EAP-GP achieves precision and recall comparable to or better than previous approaches, highlighting its effectiveness in identifying accurate circuits.
\end{abstract}

\vspace{-0.1in} 
\section{Introduction}
\vspace{-0.1in}  
In recent years, transformer-based language models \citep{vaswani2017attention} have achieved remarkable success \citep{devlin2019bert, achiam2023gpt4}, but their internal mechanisms remain unclear. Mechanistic interpretability \citep{olah2022mechanistic, nanda2023mechanistic} aims to precisely describe neural network computations, potentially in the form of pseudocode (also called reverse engineering), to better understand model behavior \citep{geva2020transformer, geiger2021causal, meng2022locating, zhang2024locate,hong2024dissecting,hu2024hopfieldian,cheng2024leveraging,yang2024makes}. Much research in mechanistic interpretability conceptualizes neural networks as computational graphs \citep{conmy2023automated, geiger2021causal}, where circuits are minimal subgraphs representing critical components for specific tasks and serving as fundamental building blocks of the model. Thus, identifying such circuits is crucial to understanding the inner workings of language models (LMs)~\cite {olah2020zoom, wang2023interpretability}. 

Prior research on circuit identification in LMs follows a straightforward methodology \citep{conmy2023automated, hanna2024faith}: employing causal interventions to identify components that contribute to specific behaviors, then formulating and testing hypotheses about the functions implemented by each component in the circuit. This approach has led to the development of frameworks that provide causal explanations for model outputs, such as predicting indirect objects \citep{conmy2023automated}, brain-inspired modular training \citep{nainani2024evaluating}, completing year-spans \citep{hanna2024gpt2}, and more \citep{lieberum2023circuit, tigges2023linear, prakash2024fine, merullo2024circuit}.

Recent work on circuit identification often aims to identify important components (e.g., attention heads, residual streams, or MLPs) and important edges, i.e., critical connections between components. Causal intervention-based circuit identification methods require a forward pass to test an edge’s importance by observing whether the relevant model behavior changes \citep{conmy2023automated}. However, as LMs contain an extremely large number of edges, %For example, GPT-2 Small \citep{radford2019language} has 32,491 edges. 
testing all edges requires significant computational resources, and as the model size increases, this challenge becomes even more severe. To address this issue, researchers have developed faster gradient-based automated circuit identification methods \citep{oneill2024sparse, marks2024sparse}. Edge Attribution Patching (EAP) \citep{syed2023attribution} attributes each edge’s importance using two forward and one backward pass instead. However, it can be affected by the zero-gradient problem, which may lead to incomplete attributions. EAP-IG \citep{hanna2024faith} incorporates Integrated Gradients (IG) into EAP to mitigate this, which produces more reliable attribution evaluations by computing the average gradients of corrupted inputs (activations) along a straight-line path from the original inputs to the baseline inputs (counterfactual input), resulting in more reliable importance (attribution) evaluation.

While EAP-IG can significantly improve the faithfulness of the discovered circuit, in this paper, we carefully revisit the approach and identify a critical issue called the saturation effect. This effect occurs when the corrupted input enters saturation regions, where the gradient becomes nearly zero (see Section \ref{sec:saturation} for details). As a result, the loss function becomes insensitive to further input variations, reducing the attribution's responsiveness to input variations. This leads to inaccurate and unfaithful edge attributions, ultimately reducing the faithfulness of circuit identification.


% While EAP-IG can significantly improve the faithfulness of the discovered circuit, in this paper, we carefully revisit the approach and find a critical issue called the saturation effect, which causes the metric to become insensitive to further input variations. EAP-IG computes the path integral of gradients by moving directly from the corrupted input \( x' \) to the original input \( x \) \citep{hanna2024faith}, considering gradients at intermediate points between the token embeddings of the original input \( z \) and the token embeddings of the corrupted input \( z' \). However, this method inevitably selects activations within saturation regions, where gradient changes slow down to near zero, as shown in Figure \ref{gradient1}. These activations significantly distort gradient estimates, leading to incorrect edge attributions for circuit components and ultimately reducing the faithfulness of circuit identification.


% \begin{table}[t]
% \centering
% \caption{Normalized faithfulness for Hypernymy Tasks}
% \begin{adjustbox}{width=0.48\textwidth}
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Task} & \textbf{Edges} & \textbf{EAP (\%)} & \textbf{EAP-IG (\%)} & \textbf{EAP-GP (\%)} & \textbf{EAP-IG-KL (\%)} & \textbf{EAP-GP-KL (\%)} \\
% \midrule
% \multirow{3}{*}{Hypernymy} 
%  & 500  & 35.52\% & 47.28\% & 54.94\% & 49.25\% & 55.24\% \\
%  & 800  & 48.57\% & 58.20\% & 69.70\% & 68.28\% & 75.53\% \\
%  & 1000 & 54.83\% & 63.78\% & 72.39\% & 75.17\% & 81.13\% \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \label{Normalized faithfulness}
% \end{table}

To address this issue, we propose Edge Attribution Patching with GradPath (EAP-GP), a novel method for mitigating saturation effects that can identify edges in circuits more accurately. Unlike previous methods, which are model-agnostic, 
EAP-GP constructs an integral path between the clean input and the baseline input in an adaptive and model-dependent way. Specifically, for the current corrupted input, EAP-GP gradually adjusts its next movement based on its difference in gradient to the baseline input rather than moving directly along a pre-fixed direction as in EAP-IG. Thus, intuitively, each step of EAP-IG follows the steepest direction to rapidly decrease the model’s prediction, effectively avoiding saturation regions and guiding a more efficient and faithful attribution evaluation.

We evaluate EAP-GP across 6 tasks and demonstrate that, at the same sparsity, it achieves improvements up to 17.7\% across individual datasets in terms of circuit faithfulness, outperforming previous gradient-based circuit identification methods. To further assess its performance, we extend our experiments to larger models, including GPT-2 Medium and GPT-2 XL, and find that EAP-GP maintains excellent performance. Finally, we compare the circuits identified by EAP-GP with manually annotated ground-truth circuits from prior research \citep{syed2023attribution}. The results show that EAP-GP achieves precision and recall comparable to or better than existing methods, further validating its reliability for circuit identification.

% Overall, our contributions are as follows:
% \begin{itemize}
%     \item We discover that existing gradient-based automatic circuit identification methods suffer from saturation effects, which lead to noisy attributions and misaligned integration paths.
%     \item We propose Edge Attribution Patching with GradPath (EAP-GP), a novel method that integrates gradients along the GradPath path for edge attribution. By aligning the integration direction with the gradient descent direction and avoiding saturated regions, it improves attribution stability and enhances the accuracy and faithfulness of circuit identification.
%     \item We demonstrate that EAP-GP outperforms existing methods in circuit faithfulness across various tasks and model sizes. Additionally, when evaluated against manually annotated circuits from prior studies, EAP-GP achieves precision and recall that are either superior to or on par with existing approaches, further validating its effectiveness in circuit identification.
% \end{itemize}

% We have made the code for this paper’s experiments available at \url{https://github.com/Linz-Z/EAP-GP}.

\section{Related Work}
% \iffalse 
% \paragraph{Attribution Methods.} Attribution is a post-hoc method that explains a model by assigning scores to individual features based on their impact on the model's predictions. Among various feature attribution techniques, gradient-based methods are especially popular due to their effectiveness. Early local gradient methods, such as Vanilla Gradient \citep{simonyan2014visualising}, Grad-CAM \citep{selvaraju2017gradcam}, and Guided Backpropagation \citep{springenberg2014striving}, suffer from gradient saturation, where gradients near the input become misleading \citep{shrikumar2017learning, dombrowski2019explanations}. To overcome this, Integrated Gradients (IG) \citep{sundararajan2017axiomatic} was introduced as a path method that accumulates gradients of the model's output along a path from the explicand (the instance being explained) to a baseline. IG's performance depends on two important hyperparameters: the path and the baseline. The standard approach, Vanilla IG, usually uses a zero baseline and a straight-line path. However, this choice is arbitrary and does not consider the specific model or explicand, leading to several issues. For example, the straight-line path can add noise to the attribution results due to gradient saturation \citep{miglani2020investigating}, as shown in Figure \ref{Gradpath}, and result in incomplete attributions \citep{erion2021improving, sturmfels2020visualizing}. 

% In recent years, numerous variant path methods have been proposed to enhance attribution performance by modifying the paths and baselines used. For integration paths, approaches such as averaging over multiple straight paths \citep{kapishnikov2019xrai, smilkov2017smoothgrad} , and splitting straight paths into different segments \citep{miglani2020investigating} have been explored. Guided IG \citep{kapishnikov2021guided} explicitly avoids saturated areas by designing the path based on the absolute values of feature gradients. Notably, despite these advancements, most existing baseline and path methods remain model-agnostic and explicand-agnostic, meaning they do not customize the path or baseline to the specific model or instance being explained. This lack of customization may limit the accuracy and interpretability of the attributions. GradCF is a new baseline, and its advantages are illustrated in Figure \ref{Gradpath}, contrasting the explicand with different baselines. GradCF minimizes the distance to the corrupted input model representation, implicitly minimizing the explicand’s prediction simultaneously. Each step of GradCF points toward the steepest direction which rapidly decreases the model prediction. Consequently, GradCF can significantly highlight critical features.
% \fi 

Neural networks can be conceptualized as computational graphs, where circuits are defined as subgraphs that represent the critical components necessary for specific tasks and serve as fundamental computational units and building blocks of the network \citep{bereska2024mechanistic}. The task of circuit identification leverages task-relevant parameters \citep{bereska2024mechanistic} and feature connections \citep{he2024dictionary} within the network to capture core computational processes and attribute outputs to specific components \citep{miller2024transformer}, thereby avoiding the need to analyze the entire model comprehensively. Existing research has demonstrated that decomposing neural networks into circuits for interpretability is highly effective in small-scale models for specific tasks, such as indirect object identification \citep{wang2023interpretability}, greater-than computations \citep{hanna2024faith}, and multiple-choice question answering \citep{lieberum2023circuit}. However, due to the complexity of manual causal interventions, extending such comprehensive circuit analysis to more complex behaviors in large language models remains challenging.

Automated Circuit Discovery (ACDC) \citep{conmy2023automated} proposed an automated workflow for circuit discovery, but its recursive Activation Patching mechanism leads to slow forward passes, making it inefficient. \citet{syed2023attribution} introduced Edge EAP, which estimates multiple edges using only two forward passes and one backward pass. Building upon this, \citet{hanna2024faith} introduced EAP-IG, enhancing the fidelity of the identified circuits.  Our method is a variant of gradient-based Automated Circuit Identification. 
As we will show in Section \ref{sec:saturation}, EAP-IG is limited by gradient saturation. Our proposed EAP-GP aims to address this issue.
In concurrent work, \citet{hanna2024gpt2} argued that faithfulness metrics are more suitable for evaluating circuits than measuring overlap with manually annotated circuits. Recent work has explored other notions of a circuit. Inspired by the fact that Sparse Autoencoders (SAEs) can find human-interpretable features in LM activations, \citep{cunningham2023sparse}, \citet{marks2024sparse} identified circuits based on these features. Additionally, \citet{wu2024interpretability} aligned computation in Alpaca \citep{taori2023stanford} with a proposed symbolic algorithm \citep{geiger2024finding}.

\section{Preliminaries}\label{Preliminaries}
To better illustrate our motivation and method, in this section we will revisit previous methods for circuit discovery.  

\noindent{\bf Integrated Gradients Method.}
Integrated Gradients (IG)~\citep{sundararajan2017axiomatic} is a gradient-based attribution method in explainable AI that aims to quantify how each input feature contributes to a deep neural network’s output. Generally, the idea is to evaluate how the model performance will be changed if we change the target feature of the input to the baseline input (or counterfactual input). 
It does so by estimating the accumulated gradients along a path from the baseline input to the target input. 
% IG is smooth, stable, and satisfies several important attribution properties. 
The performance of IG largely depends on two key hyperparameters: the path and the baseline. Specifically, consider an input $\mathbf{x} \in \mathbb{R}^n$, a path for integrating gradients is formally defined as $\gamma(\alpha)$ for $\alpha \in [0,1]$. This path is a sequence of points in $\mathbb{R}^n$ that transitions from the baseline $\mathbf{x}'$ to the target input $\mathbf{x}$, i.e., $\gamma(0) = \mathbf{x}'$ and $\gamma(1) = \mathbf{x}$.

Given a path $\gamma$ and a model $f : \mathbb{R}^n \to \mathbb{R}$, the integrated gradient for the $i$-th feature is computed by integrating the model’s gradient with respect to that feature along the path. Formally, following \citet{sundararajan2017axiomatic} we have 
\begin{equation}
\phi_i^{\text{Path}} = \int_{0}^{1} \frac{\partial f(\gamma(\alpha))}{\partial \gamma_i(\alpha)} \,\frac{\partial \gamma_i(\alpha)}{\partial \alpha} \, d\alpha,
\label{eq:path_integrated_gradient}
\end{equation}
where $\frac{\partial f(\gamma(\alpha))}{\partial \gamma_i(\alpha)}$ is the gradient of the model’s output with respect to the $i$-th feature at $\gamma(\alpha)$, and $\frac{\partial \gamma_i(\alpha)}{\partial \alpha}$ is the rate of change of that feature along the path. To simplify the computation, in practice, the simplest path is a straight line from $\mathbf{x}'$ to $\mathbf{x}$, given by
\begin{equation}
\gamma(\alpha) = \mathbf{x}' + \alpha \bigl(\mathbf{x} - \mathbf{x}'\bigr),
\quad \alpha \in [0,1].
\label{straight-line path}
\end{equation}
The choice of baseline $\mathbf{x}'$ is an active research topic. \citet{sturmfels2020visualizing} provide a thorough study of common baselines, including the zero vector ($\mathbf{x}' = \mathbf{0}$), the one vector ($\mathbf{x}' = \mathbf{1}$), and samples drawn from the training data distribution ($\mathbf{x}' \sim \mathcal{D}_{\text{train}}$).

However, directly computing the integral in Eq~\eqref{eq:path_integrated_gradient} is impractical. To address this computational challenge, a discrete sum approximation with $k$ points along the path is commonly used. For a straight-line path, IG is calculated as:
\begin{equation}
\phi_i^{\text{IG}} = (x_i - x'_i) \times \left( \frac{1}{k} \sum_{j=1}^{k} \frac{\partial f\left(\mathbf{x}' + \frac{j}{k} (\mathbf{x} - \mathbf{x}')\right)}{\partial x_i} \right).
\label{eq:integrated_gradients}
\end{equation}
where the index \( j \) corresponds to the \( j \)-th sampling point along the path from the baseline \( \mathbf{x}' \) to the input \( \mathbf{x} \), and the gradients are computed at each of these points.

\paragraph{Circuit discovery.}
Given a model \( G \), which can be represented as a computational subgraph, a circuit \( C \subset G \) is a subgraph, where it can be represented as a set of edges in the circuit~\cite{olah2020zoom}. 
\begin{definition}[Computational Graph~\cite{hanna2024faith}]
    A transformer LM $G$’s computational graph is a digraph describing the
computations it performs. It flows from the LM’s inputs to the unembedding that projects its
activations into vocabulary space. We define this digraph’s nodes to be the LM’s attention
heads and MLPs, though other levels of granularity, e.g., neurons, are possible. Edges
specify where a node’s output goes; a node $v$’s input is the sum of the outputs of all nodes $u$
with an edge to $v$. A circuit  $C$ is a subgraph of $G$ that connects the inputs to the logits.
\end{definition}

\begin{definition}[Circuit Discovery]
Given a full model $G$ and a subgraph $C$, for any pair of clean  and corrupted input (prompt) $z$ and $z'$, denote \( T \) as the task distribution, 
$E_G$ as the activations of $G$ with input $z$, 
$E_C(z, z')$ as the activations of the subgraph when \( z \) is input, with all edges in \( G \) not present in \( C \) overwritten by their activations on \( z' \). Moreover, denote $L(A)$ as a loss on the logits for activations $A$ (with input $z$), which is used to measure the performance of subgraphs. Formally, circuit discovery can be formulated as 
\begin{equation}\label{Circuit Function}
    \arg \min_C \mathbb{E}_{(z, z') \in T} |L(E_C(z, z')) - L(E_G(z))|.
\end{equation}
In practice, we always use logit difference or probability difference as the loss $L$.  
\end{definition}



Many studies identify circuits using activation patching~\cite{Vig2020GenderBias, geiger2021causal}, which evaluates the change by replacing a (clean) edge activation with a corrupted one during the
model’s forward pass. ACDC \cite{conmy2023automated} automatically checks whether for each edge such a change exceeds some threshold. However, causal interventions scale poorly because their iterative cost increases significantly as the model size grows.   EAP \cite{syed2023attribution} alleviates this issue.  It estimates edge importance (attribution) and selects the most important ones by computing the product of activation changes and input gradients.  Specifically, given an edge $e = (u, v)$ with clean and corrupted
activations $x_u$ and $x_u'$, we aim to approximate the change in loss $L$ (note that since $E_G$, $z$ and $x_u$ are clear in the text, we will 
denote $L(s)=L(E_G(z)-x_u+s)$ as the loss where we change the activation from $x_u$ to $s$), i.e., 
\begin{equation}\label{eap:loss}
    L(x_u)-L(x_u')
    \approx (x_u-x'_u)\frac{\partial L(x'_u)}{\partial x_v}. 
\end{equation}


 However, EAP may suffer from the zero-gradient problem, which may lead to inaccurate attributions. To address this issue, based on the similarities between the goals of feature attribution and circuit discovery,  EAP-IG \citep{hanna2024faith} incorporates IG, which averages gradients along a straight-line path $\gamma(\alpha)$ in \eqref{straight-line path} from original to corrupted activations. This method provides more stable and reliable attributions. Similar to \eqref{eq:integrated_gradients}, the IG score for  edge \( (u, v) \) is defined as:
\begin{equation}
\phi_{(u,v)}^{\text{IG}} = (x_u-x'_u)\times  \frac{1}{k} \sum_{j=1}^{k} \frac{\partial L\left(x'_u + \frac{k}{m}( x_u-x'_u)\right)}{\partial x_v}.
\label{eq:eap_ig}
\end{equation}
%where \( \frac{\partial L\left(x'_u + \frac{k}{m}(x'_u - x_u)\right)}{\partial x_v} \) represents the gradient of the loss function with respect to \( x_v \), where \( x_v \) is the activation at node \( v \) in the model.
This gradient is computed along an interpolated path from the original activation \( x_u \) to the corrupted activation \( x'_u \). The interpolation factor \( \frac{k}{m} \) determines the position along this path, ensuring that the gradient is averaged over multiple steps. This approach helps mitigate the zero-gradient problem, reducing the risk of incomplete attributions.


%%%%%%%%%%%%%%your original version%%%%%%%%%%%%%%%%%%%
\iffalse 
\paragraph{Circuit discovery.}
The goal of circuit discovery~\citep{olah2020zoom} is to identify a sparse subgraph that accurately describes the behavior of the full model on a particular task. Given a model \( G \), which can be represented as a computational graph, a circuit \( C \subset G \) is a subgraph consisting of a subset of edges that are critical for the task. Additionally, we define \( z \) as the token embedding activation of the original input (referred to as the original activation), and \( z' \) as the token embedding activation of the corrupted input (referred to as the corrupted activation).  

\begin{definition}[Circuit Function]  
For a full model \( G \) and a subgraph \( C \), let \( C(x, x') \) be the output of the subgraph when \( x \) is the original input, with all edges in \( G \) that are not present in \( C \) overwritten by their activations on \( x' \) (the corrupted input). The metric \( L \) often represents the loss function or logit difference. We evaluate \( C(x, x') \) by computing the metric difference:  

\begin{equation}  
|L(C(x, x')) - L(G(x))|  
\label{loss}  
\end{equation}  

which measures the discrepancy between the model's output and the subgraph’s predictions. Let \( T \) denote the task distribution. Formally, circuit discovery is defined as an optimization problem:  

\begin{equation}\label{Circuit Function}  
    \arg \min_C \mathbb{E}_{(x, x') \sim T} |L(C(x_i, x'_i)) - L(G(x_i))|  
\end{equation}  

\end{definition}  
Many studies use activation patching \cite{Vig2020GenderBias, geiger2021causal}, where an edge's activation is replaced with a corrupted version during the model’s forward pass. If this replacement significantly impacts the task-specific metric (exceeding a threshold \( \tau \)), the edge is included in the circuit. Methods like ACDC \cite{conmy2023automated} automate and accelerate this process. However, ACDC scale poorly because their iterative cost increases significantly as model size grows. EAP \cite{syed2023attribution} alleviates this issue.  It estimates edge importance (attribution) by computing the product of activation changes and  gradients.  Given an edge \( e = (u, v) \) with the original activation of node \( u \), denoted as \( z_u \), and the corrupted activation of node \( u \), denoted as \( z'_u \), EAP estimates the metric difference caused by corruption and attributes it to each edge as:
      
\begin{equation}
\phi_{(u,v)} = (z'_u - z_u)^\top \nabla_v L(G(z)).
\label{eq:eap}
\end{equation}
Here, \( \nabla_v L(x) \) represents the gradient of the loss function \( L \) with respect to the input of node \( v \). However, it can suffer from the zero-gradient problem, which may lead to inaccurate attributions. To address this issue,   EAP-IG \citep{hanna2024faith} incorporates IG, which averages gradients along a straight-line path $\gamma(\alpha)$ in \eqref{straight-line path} from original to corrupted activation. Similar to \eqref{eq:integrated_gradients}, the IG score for an edge \( (u, v) \) is defined as:
\begin{equation}
\phi_{(u,v)}^{\text{IG}} = (z'_u - z_u) \frac{1}{k} \sum_{j=1}^{k} \frac{\partial L\left(z' + \frac{j}{k}(z' - z)\right)}{\partial z_v}.
\label{eq:eap_ig}
\end{equation}

This gradient is computed along an interpolated path from the original activation \( z \) to the corrupted activation \( z' \). The interpolation factor \( \frac{j}{k} \) determines the position along this path, ensuring that the gradient is averaged over multiple steps. This approach helps mitigate the zero-gradient problem, reducing the risk of incomplete attributions.
\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Saturation Effects in Circuit Discovery}\label{sec:saturation}


% \di{The following should be the logic of this section:  (1) Mention what we are going to talk about in this section. (2) Menion what experiments we perform. (3) Analyzing the result and why the results could make unfaithful evaluations (4) Formally define the phenomenon. }

In this section, we revisit the above-mentioned gradient-based automatic circuit identification methods, EAP and EAP-IG. Both of them 
determine edge importance by computing the activation difference multiplied by the loss gradient. This product approximates the metric difference in \eqref{eap:loss} and is used to evaluate each edge’s contribution. 

%The two methods differ primarily in their choice of input for $\nabla_v L$: EAP directly utilizes the original activation, whereas EAP-IG averages the gradients computed at intermediate activations between the original activation $z$ and the corrupted activation $z'$. Furthermore, we compute the average gradient curves of the metric with respect to edge activations to analyze the influence of different input choices on the metric difference (Equations \ref{loss}) in EAP and EAP-IG.


% EAP-IG attributes the model's prediction to each edge in the network. In equation \ref{eq:eap_ig}, the attribution consists of two components: the activation value difference and the gradient integration. 

Specifically, we analyze a circuit edge \( (u,v) \) with its clean activation \( x_u \)  in the IOI dataset and examine how different input choices influence the gradient of the loss \( \frac{\partial L}{\partial x_v} \) in both EAP and EAP-IG. EAP in \eqref{eap:loss} evaluates an edge's importance by computing the product of the metric’s derivative at \( x_u \) and the change in the edge’s activation. However, as illustrated in Figure \ref{gradient1} (black point), this approach can be misleading: a nearly zero derivative at \( x_u \)   suggests that the edge has minimal influence and will not contribute to the attribution, even if the activation has a non-zero gradient at \( x_u' \) and the difference in activations is significant.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.4\textwidth]{Figure/gradient.pdf}
\vspace{-10pt}
\caption{Comparison of the gradient behavior of the loss along an integral path. EAP uses a single input \(  x_u \)(the original activation), while EAP-IG and EAP-GP utilize blended inputs along pre-fixed straight-line paths and gradient-based adjusted paths, respectively. The points on the dashed lines represent the intermediate perturbed inputs along each path.}
\label{gradient1}
\end{center}
\end{figure}

We also conduct experiments under the same setting for EAP-IG in \eqref{eq:eap_ig} with $k=5$.
%In practice, EAP-IG adopts a blended-input approach along a straight-line path to transition from the corrupted activation \( z' \) to the original activation \( z \), incorporating five intermediate activations  \cite{hanna2024faith} defined as: we also examined the impact of this blended-input path in EAP-IG on the metric.
As illustrated in Figure \ref{gradient1}, we find that the gradient remains close to zero when \( \frac{j}{k} \) falls within the ranges \([0,0.2]\) and \([0.8,1]\), where the IG score changes slowly. In contrast, slight variations in the score occur only within the range \( \frac{j}{k} \in [0.2,0.8] \). While this approach partially mitigates the zero-gradient issue in EAP, the nature of the chosen straight-line path inevitably leads it into regions where the gradient remains nearly zero (\([0,0.2]\) and \([0.8,1]\)). The perturbed activation inputs within these regions cause the loss function to become insensitive to further input variations, reducing the attribution's responsiveness to input perturbations. This results in inaccurate and unfaithful edge attribution evaluations and ultimately leads to unfaithful circuit identification. We provide the following definition for this saturation effect for gradient-based EAP methods. 

\begin{definition}[Saturation Effects and Regions]
For an edge \( (u,v) \) in a circuit discovery task, its saturation regions refer to segments of the integration path where the gradient of the loss, \( \frac{\partial L}{\partial x_v} \), remains close to zero, reducing the sensitivity of \( L \) to activation changes. Saturation effects occur when scores accumulate in these regions, reducing the attribution's responsiveness to activation variations. This distortion ultimately compromises the reliability of circuit analysis, leading to unfaithful circuit evaluations.
\label{Saturation Effects and Regions}
\end{definition}













% The presence of a saturated region indicates a misalignment \shu{do you mean `dissimilar directions`?} between the gradient and the integration path (otherwise, the model's prediction would drop sharply), leading to the Saturation Effects defined in \ref{Saturation Effects}. This misalignment suggests that the path is not aligned with the key features, resulting in incorrect attributions. \di{if you want to illustrate something, it will be better to use mathematical notations rather than words. No one know what you are talking about. IF you have expeirmenta results first, then it will be easier}

% In the definition \ref{Circuit Function}, the loss function $L$ should capture the discrepancy between the outputs of the full model and the circuit. EAP uses the logit difference as the loss function, but it has been shown to suffer from the zero-gradient problem, which results in incorrect attributions and a zero KL divergence \cite{hanna2024faith}. EAP-IG addresses this issue by using blended inputs to avoid the zero-gradient problem and can employ either the logit difference or the KL divergence between token predictions as the loss function \cite{hanna2024faith}.  \di{you should merge and mention it in the previous section} We test two types of loss functions and evaluate their average loss curves on the IOI dataset.

% As shown in Figure 2(a) \shu{ref your figure by  \texttt{ref}}, \( D \) increases significantly for \( \alpha \in [0, 0.70] \), where the model transitions from the original input to the corrupted input. Beyond \( \alpha = 0.70 \), \( D \) flattens and remains nearly constant as \( \alpha \) approaches 1, entering a saturated region. In this region, the feature gradients no longer contribute meaningfully to \( D \), making the straight-line path inefficient for capturing important features \di{need more discussions, why it is inefficient for capturing important features?} \di{there is no features in circuit!}. 

% The saturated region observed in Figure 2(a) \shu{ref formate} highlights a critical limitation of straight-line integration in EAP-IG: the integration path inevitably passes through areas where gradients are nearly zero. This misalignment between the feature gradients and the integration path reduces the effectiveness of EAP-IG in identifying important features \di{hard to unerstand}. Consequently, it fails to provide accurate attributions \shu{Are there any results to prove the decrease of accuracy} in these saturated areas, as the path does not align with the direction of significant feature contributions. This limitation underscores the need for a more adaptive integration path that avoids saturation.
% \begin{definition}[Saturation Effects \cite{miglani2020investigating, kapishnikov2021guided}] The straight-line path of Integrated Gradients (IG) can pass through saturated areas where the model output does not change significantly with respect to $\alpha$. In these regions, the feature gradient does not align with the integration path. This leads to the accumulation of noisy attributions, known as saturation effects. 
% \label{Saturation Effects}
% \end{definition}









\section{Mitigating Saturation Effect via EAP-GP}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.40\textwidth]{Figure/method.pdf}
\caption{Illustration of the straight-line path and the dynamically adjusted path used in EAP-GP. \textbf{GradPath} starts at the original input \( x_u \) and constructs a path in the direction of the steepest gradient descent toward the corrupted activation. The saturated area on the straight-line path is marked in red.}
\label{Gradpath1}
\end{center}

\end{figure}



In the previous section, we discussed how EAP suffers from the zero-gradient problem and how EAP-IG is affected by saturation effects. %We believe that an excellent gradient-based automatic circuit identification method should incorporate blended inputs, avoid saturation regions that cause saturation effects, and integrate information from both the input and the model.
Our previous experiments show that the main reason EAP-IG gets stuck in the saturation region is that the integration path is a direct line between the clean and baseline input, which is independent of the full model $G$. Such a model-agnostic way will unintentionally make the gradient nearly zero for some perturbed activations. Thus, we need to construct an integral path that depends on the model to avoid the saturation region.


To address these issues, we propose \textbf{Edge Attribution Patching with GradPath (EAP-GP)}. Unlike the pre-fixed straight-line paths used in EAP-IG, EAP-GP introduces GradPath, a dynamically adjusted path designed to integrate gradients more effectively and reduce saturation effects, as shown in Figure \ref{Gradpath1}. 
Specifically, given a target number of steps $k$, at step $j$, EAP-GP iteratively finds the best movement of the current perturbed input (activation), denoted as $\gamma^G(\frac{j}{k})$, starting from the original input $x_u$ (i.e., $\gamma^G(0)=x_u$) and ending at the baseline input $x'$ (i.e., $\gamma^G(1)=x_u'$). In detail, at the current  input $\gamma^G(\frac{j}{k})$, we aim to find the best direction toward $x_u'$, i.e., we aim to solve the following optimization problem locally. 
\begin{equation}\label{gradpath}
    \min_{\delta} \|G(\gamma^G(\frac{j}{k}) + \delta) - G(x_u')\|_2^2. 
\end{equation}
Here, for an activation $s$, $G(s)=G(E_G(z)-x_u+s)$ is  the output of the model $G$ with prompt $z$ and activations $E_G(z)-x_u+s$, where we change the activation from $x_u$ to $s$. Intuitively, such a task can make sure that we can move the current input $\gamma^G(\frac{j}{k})$ to make it closer to the baseline input. Thus, the gradient could not be too small to be nearly zero, making the path avoid the saturation region. 

Here, we use one step of gradient descent for \eqref{gradpath} and get the next perturbed input $\gamma^G(\frac{j+1}{k})$ as 
\begin{equation}
\begin{aligned} 
g_{j+1} &= \frac{\partial ||G(\gamma^G(\frac{j}{k})) - G(x_u') \|_2^2}{\partial \gamma^G(\frac{j}{k})},\\
\gamma^G(\frac{j+1}{k}) &= \gamma^G(\frac{j}{k}) - \frac{1}{W_j} \cdot  g_{j+1},
\end{aligned}
\label{eq:compact_gradpath}
\end{equation}
where \( W_j=\|g_{j+1}\|_2 \) is a normalization factor that dynamically adjusts the step size.
Our integrating path will be determined by these corrupted activations in $\gamma^G$. And we can approximate the integration by using the finite sum to get the EAP-GP score for  edge \((u, v)\): 

\begin{equation}
   \phi_i^{\text{GP}}= (x_u-x'_u) \times \frac{1}{k} \sum_{j=1}^{k} \frac{\partial L\left(\gamma^G(\frac{j}{k})\right)}{\partial x_v}.
    \label{eq:eap_gp}
\end{equation}


\begin{algorithm}[t]
\caption{Edge Attribution Patching with GradPath (EAP-GP) for edge $(u,v)$}
\label{alg:eap-gp}
\begin{algorithmic}[1]
\Require 
  $x_u, x_u'$: Original/Corrupted activations; 
  $k$: GradPath steps; 
  $C$: Circuit;
  $G$: Full model;
  $L(\cdot)$: Loss;
  $n$: Top edges to select.
\Ensure 
  Circuit $C$ and final intervention results.

\State \textbf{A. GradPath Construction}
\State $\gamma^G(0) \gets x_u$
\For{$j = 1 \to k$}
  \State $g_{j+1} \gets \nabla_{\gamma^G(\tfrac{j}{k})}\|G(\gamma^G(\tfrac{j}{k}))-G(x_u')\|^2_2$
  \State $\gamma^G\!\bigl(\tfrac{j+1}{k}\bigr) \gets \gamma^G\!\bigl(\tfrac{j}{k}\bigr) - \tfrac{1}{W_j}\,g_{j+1}$
\EndFor

\State \textbf{B. Edge Attribution (EAP-GP)}
\For{each edge $(u,v)$ in $G$}
  \State $\text{score}(u,v) \gets (x_u - x'_u)\, \times\frac{1}{k} \sum_{j=1}^{k} 
    \tfrac{\partial\,L(\gamma^G(\tfrac{j}{k}))}{\partial\,x_v}$
\EndFor

\State \textbf{C. Circuit Extraction}
\State Sort edges by $|\text{score}(u,v)|$; select top $n$ into $C$; prune isolated nodes

\State \textbf{D. Intervention \& Evaluation}
\For{each node $v$}
  \For{each edge $e=(u,v)$}
    \State $i_e \gets \mathbf{1}[\,e\in C\,]$
  \EndFor
  \State \textit{Input to $v$}: $\sum_{(u,v)\in E_v} \bigl[i_e \cdot x_u + (1 - i_e)\cdot x'_u\bigr]$
\EndFor
\State Evaluate the model output under this intervention

\State \Return $ C $
\end{algorithmic}
\end{algorithm}


% \di{the following is your writes. }


% GradPath iteratively optimizes the distance between two model representations using equation \ref{regulation} to identify the perturbed origin input most similar to the corrupted input, thus constructing the gradient interval path.
% \begin{equation}
%     \min_{\delta} \|G(x + \delta) - G(x')\|_2^2,
% \label{regulation}
% \end{equation}
% \di{make mathematical notations consistent} Specifically, GradPath is constructed in a direction similar to traditional straight-line path integration. The optimization begins at \( \gamma^G_{0} \) (the original input \( x \)), which also serves as the starting point of the straight-line path integration. However, GradPath proceeds along the gradient descent direction of the corrupted input. This is achieved by minimizing the model representation distance using equation \ref{regulation}, and iteratively searching for intermediate inputs most similar to the corrupted input to construct the path. GradPath is defined as a discrete function \( \gamma^G \) over a feasible set \( \{0, \frac{1}{k}, \ldots, \frac{k-1}{k}, 1 \} \), where \( 0 \leq j \leq k \), and \( j \in \mathbb{N} \). At each step, GradPath \( \gamma^G(\frac{j}{k}) \) transitions from \( j = 1 \) to \( j = k \), with all intermediate samples forming the GradPath. The process ends at \( \gamma^G(1) \) (the corrupted input \( x' \)). The update rule for GradPath is as follows: \di{hard to understand, need to rewrite}

% \begin{equation}
% \begin{aligned} 
% g_{j+1} &= \frac{\partial ||G(\gamma^G(\frac{j}{k})) - G(x') \|_2^2}{\partial \gamma^G(\frac{j}{k})},\\
% \gamma^G(\frac{j+1}{k}) &= \gamma^G(\frac{j}{k}) + \frac{1}{W_j} \cdot  g_{j+1}, \\
% \gamma^G_{0} &= x, \\
% \gamma^G_{1} &= x'.
% \end{aligned}
% \label{eq:compact_gradpath}
% \end{equation}

% where \( g_{j+1} \) represents the update magnitude at each iteration. 


After attributing each edge in the model, we follow the approach in \cite{hanna2024faith} and employ a greedy search strategy to iteratively evaluate the top-ranked edges iteratively, selecting the top $n$ edges (a hyperparameter) with the highest scores to form the circuit. Once the circuit is identified, we recursively prune nodes and edges with no parents or children, as they are redundant. We evaluate the circuit by applying the following intervention. Let \( v \) represent a node in the model’s computational graph, and let \( E_v \) denote the set of all incoming edges to \( v \). For each edge \( e \in E_v \), let \( i_e \) be a binary indicator, where \( i_e = 1 \) if the edge is part of the circuit and \( i_e = 0 \) otherwise. Without intervention, the input to \( v \) is:
\begin{equation}
\sum_{e = (u, v) \in E_v}x_u,
\end{equation}
which represents the sum of the outputs from all parent nodes of $v$. With intervention, the input to $v$ is modified as:
\begin{equation}
\sum_{e = (u, v) \in E_v} i_e \cdot x_u + (1 - i_e) \cdot x'_u.
\end{equation}
If all edges are in the circuit (\( i_e = 1 \) for all \( e \)), this intervention is equivalent to running the model on original inputs. Conversely, if no edges are in the circuit (\( i_e = 0 \) for all \( e \)), the intervention corresponds to running the model on corrupted inputs. The overall pseudocode of the EAP-GP algorithm is provided in Algorithm~\ref{alg:eap-gp}.



\begin{figure*}[t]

\begin{center}
\includegraphics[width=0.86\textwidth]{Figure/heatmap_all.pdf}
\vspace{-10pt}
\caption{Faithfulness of circuits obtained using EAP-GP across different edge sparsity levels and step counts for IOI and gender-bias tasks.}
\label{heatmap}
\end{center}

\end{figure*}








% \begin{figure*}[t]

% \begin{center}
% \includegraphics[width=\textwidth]{Figure/heatmap_all.pdf}
% \caption{ .}
% \label{PR}
% \end{center}

% \end{figure*}


\begin{table*}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l p{2cm} p{2cm} p{2cm} p{2cm} p{2cm}}
        \toprule
        \textbf{Method} & \textbf{Sparsity (\%)}  
        & \multicolumn{2}{c}{\textbf{IOI}} 
        & \multicolumn{2}{c}{\textbf{Greater-Than}} \\
        \cmidrule{3-4} \cmidrule{5-6}
        & & \textbf{NFS(\%)} $\uparrow$ &  \textbf{Time (s)} $\downarrow$ 
        & \textbf{NFS(\%)} $\uparrow$ &  \textbf{Time (s)} $\downarrow$ \\
        \midrule
        EAP & $ 97.5 \pm 0.01$ & 56.9  & \textbf{12.6}  & 96.3  & \textbf{11.7}\\
        EAP-IG & $ 97.5 \pm 0.01$ & 62.4  & 49.7 & 97.6  & 44.3 \\
        EAP-GP & $ 97.5 \pm 0.01$ & \textbf{80.1} & 232.5 & \textbf{99.8}  & 210.7 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different methods' performance based on 97.5\% sparsity. NFS represents the Normalized Faithfulness Score, and Times represents the computation time for different methods. A higher Normalized Faithfulness Score and shorter computation time indicate better performance.}
    \label{tab:comparison}
\end{table*}






\begin{figure*}[t]

\begin{center}
\includegraphics[width=0.94\textwidth]{Figure/scatter_plots_gpt2-small.pdf}
\caption{Comparison of circuit performance across different methods on GPT-2 Small. In all plots, a higher value indicates better performance. EAP-GP identifies circuits that outperform other methods across all six tasks. }
\label{performance}
\end{center}

\end{figure*}

\section{Experiments}
\subsection{Experimental Setup}
\noindent{\bf Dataset}
We evaluate model performance using six datasets: Indirect Object Identification (IOI), Subject-Verb Agreement (SVA), Gender-Bias, Capital–Country, Hypernymy, and Greater-Than \citep{hanna2024faith}. IOI tests the model's ability to identify indirect objects, while SVA assesses subject-verb agreement. Gender-Bias examines gender bias in language models, and Capital–Country evaluates the prediction of a country given its capital. Hypernymy focuses on identifying hypernyms (superordinate categories), and Greater-Than measures the model's ability to predict numbers that are greater than a specified value in a sentence. Table \ref{sample-table} provides representative task examples, and Appendix \ref{appendix:datasets} details the datasets and the loss functions used for circuit identification.

\noindent{\bf Baselines}
Since EAP-GP is a gradient-based method, we primarily compare it with previous gradient-based approaches, such as EAP  and EAP-IG. Both are outlined in Section~\ref{Preliminaries}.

\noindent{\bf Evaluation Metrics} 
A circuit is considered faithful to a model’s behavior on a task if all model edges outside the circuit can be corrupted while still preserving the model’s original outputs \citep{hanna2024faith}. Following the setting of \citet{hanna2024faith}, we evaluate circuit faithfulness across all tasks using the Normalized Faithfulness Score (NFS), which measures the similarity between the circuit's output and the full model's output:
\begin{equation}
\text{score} = \frac{\delta_{C(x, x')} - \delta_{-}}{\delta_{+} - \delta_{-}},
\end{equation}
where \( \delta_{+} \) and \( \delta_{-} \) represent the full model's performance on the original and corrupted inputs, respectively, and \( \delta_{C(x, x')} \) represents the circuit’s performance. The value of \( \delta_{C(x, x')} \) is measured using the task-specific Logit Difference or Probability Difference, with detailed explanations provided in the appendix\ref{appendix:datasets}. \( \delta_{+} \) and \( \delta_{-} \) for each dataset are provided in Table \ref{tab:baseline_performance}. Additionally, for the IOI and Greater-Than datasets, we compare the precision-recall (PR) performance curve of gradient-based methods with manually identified circuits from prior research \citep{syed2023attribution}.

% For IOI, Gender-Bias, and Capital-Country, we also use Logit Difference defined as:
% \begin{equation}
%     \log P(\text{correct}) - \log P(\text{misleading}),
% \end{equation}
% which measures the difference in log probabilities between the correct and misleading name/pronoun. For SVA, Greater-Than, and Hypernymy, we use Probability Difference, defined as:
% \begin{equation}
% \sum P(y_{\text{correct}}) - \sum P(y_{\text{incorrect}}),
% \end{equation}
% which compares the probability of the correct answer with the sum of the probabilities of incorrect answers.


\noindent {\bf Experimental Setup.}  All experiments are conducted on GPT-2 Small (117M), GPT-2 Medium (345M), and GPT-2 XL (1.5B), which contain 32,491, 231,877, and 2,235,025 edges, respectively. We set $k=5$ in EAP-GP. Following \citep{hanna2024faith}, we perform EAP-IG with hyperparameters set to \( k=5 \) steps. All experiments are conducted on an NVIDIA A40 GPU.













\begin{figure*}[t]

\begin{center}
\includegraphics[width=0.83\textwidth]{Figure/combined_PR4_nodes_edges.pdf}
\caption{Precision-recall curves for IOI (left) and Greater-Than (right) node / edge overlap}
\label{PR}
\end{center}

\end{figure*}

\subsection{Experimental Results}
This section compares the three methods based on our primary faithfulness metrics, with all experiments conducted on GPT-2 Small (117M). Additional experiments on GPT-2 Medium (345M) (see Figure \ref{medium-performance} and Table \ref{Table Medium faithfulness}) and GPT-2 XL (1.5B) (see Figure \ref{xl-performance} and Table \ref{Table XL faithfulness}), along with examples of circuits identified by EAP-GP (see Figure \ref{circuit}), are reported in Appendix \ref{More results}.

\noindent {\bf Circuit Faithfulness.} 
We compare the faithfulness of identified circuits for three gradient-based circuit identification methods (EAP, EAP-IG, and EAP-GP) across six tasks under edge sparsity levels ranging from 96.9\% to 99.9\%, as shown in Figure \ref{performance} and Table\ref{Table small faithfulness}.

In IOI, SVA, and Hypernymy, EAP-GP significantly outperforms the other methods. For example, in the IOI task, when sparsity is 97.5\%, EAP-GP achieves a score of 80.1\%, far exceeding EAP-IG (62.4\%) and EAP (56.9\%) (see Table \ref{tab:comparison}).. In IOI and SVA, the performance gap is small at low sparsity levels, but as sparsity increases, EAP-GP’s advantage becomes more pronounced. However, when edge sparsity ranges from 99.0\% to 99.9\%, EAP generates completely unfaithful circuits, with a regularized faithfulness score of 0. This is partly due to EAP producing many ``parentless heads'', which are subsequently pruned. As a result, EAP’s generated circuits are entirely pruned to an empty structure. In the Hypernymy task, this performance gap remains large across all sparsity levels, indicating that EAP-GP consistently outperforms both EAP and EAP-IG in this task.

For Greater-Than and Gender-Bias, the performance differences among the three methods are smaller, remaining relatively close across all sparsity levels. Although EAP-GP maintains a slight advantage, the performance gap is minimal. For example, in the Gender-Bias task, EAP-GP (92.68\%) only slightly outperforms EAP-IG (90.76\%). This is because these tasks primarily rely on direct feature mappings rather than complex reasoning models. The Gender-Bias task focuses on the direct association between professions and pronouns (e.g., banker $\rightarrow$ he), while the Greater-Than task involves simple sequential relations in numerical data (e.g., 1352 $\rightarrow$ 1353). These tasks are characterized by their reliance on explicit, localized features rather than requiring multi-step reasoning or complex relational identification, as seen in IOI and Hypernymy.

In the Country-Capital task, EAP performs poorly, failing to maintain faithful circuit structures as sparsity increases. EAP-IG performs better but remains slightly weaker than EAP-GP. Overall, EAP-GP demonstrates superior performance, outperforming both EAP and EAP-IG, as it achieves more effective edge attribution evaluation.

Additionally, we also test the runtime of the three methods on the IOI and Greater-Than datasets with 97.5\% edge sparsity (see Table \ref{tab:comparison}). EAP-GP is approximately five times slower than EAP-IG. However, this is not only due to the \( k \) forward and backward passes over the data but also because constructing the intermediate points of the integration path requires an additional forward and backward pass over the activations, further contributing to the overall runtime.


\noindent {\bf Comparison with Manual Circuit.}
We follow the approach of \citep{syed2023attribution} to check whether our method EAP-GP can identify the ground-truth circuits. Specifically, we compute the precision and recall of the EAP, EAP-IG, and EAP-GP circuits concerning the manually found circuits. Our results in Figure \ref{PR} indicate that EAP-GP performs well in retrieving nodes and edges. Specifically, for the IOI task, edge precision/recall and node precision/recall exhibit similar trends for all three methods. EAP-GP initially performs worse but improves slightly as recall increases when it is greater than 0.8 and 0.25 for node and edge, respectively. However, for IOI, this assessment may be somewhat flawed due to the ambiguous role of MLPs in the manually found IOI circuit, which is mentioned by \citep{hanna2024faith}. In the Greater-Than task, where the manually identified circuit also includes MLPs, we can easily see that EAP-GP outperforms both EAP and EAP-IG in both edge and node retrieval. It identifies circuits with higher precision and recall than EAP and EAP-IG. 

\noindent {\bf Effect on Number of Steps in EAP-GP.} Note that the 
number of steps is the only hyperparameter in EAP-GP. Here we perform an ablation study to see its effect on the performance of IOI and Gender-Bias tasks. We use their respective logit difference and probability difference as metrics to assess the circuit. Specifically, we run EAP-GP for \( k \) steps, where \( k \) ranges from 3 to 20. Notably, when \( k = 1 \), EAP-GP is equivalent to EAP, as illustrated in Figure \ref{gradient1}. When \( k = 2 \), the gradient used for evaluation is the average of the gradients of the clean and corrupted inputs.


We identify circuits at various edge sparsity levels, ranging from 96.9\% to 99.9\%. Our results (Figure \ref{heatmap}) indicate that, across all edge sparsity levels, only a few steps are sufficient to achieve high faithfulness. At \( k = 4 \) or \( 5 \) steps, EAP-GP already produces faithful circuits for both the IOI and Gender-Bias tasks.

We also observe that, in some cases, reducing sparsity (e.g., IOI from 97.5\% to 97.2\%) or increasing the number of steps (e.g., \( k > 5 \)) leads to a decline in the metric. This may be because the greedy and top-n edge selection strategies rely on absolute attribution scores to select edges, which may inadvertently include components that harm model performance, such as edges encoding noise or adversarial patterns. Furthermore, as the step count \( k \) increases, based on our method, the gradient norm will become smaller, resulting in reciprocal growth of the normalized step sizes $W_j^{-1}$. This amplifies high-frequency oscillations in the integration path near the corrupted input \( x' \), where gradient directions become unstable. Consequently, noise-dominated steps accumulate, diluting the accurate attribution signals and ultimately degrading the metric. 


\iffalse 
\begin{equation} 
\| g_j \|_2 \propto e^{-\lambda j}, \quad (\lambda > 0), 
\end{equation}
resulting in reciprocal growth of the normalized step sizes:
\begin{equation} 
W_j^{-1} = \frac{1}{\| g_j \|_2} \propto e^{\lambda j}. 
\end{equation}
\fi 


% \citet{sundararajan2017axiomatic} recommend using between 20 and 300 steps to approximate the integral within a 5\% error margin. We hypothesize possible explanations for this discrepancy. The commonly used baseline for integrated gradients is a zero baseline with a straight-line path, such as a black image in computer vision tasks. This choice is arbitrary and agnostic to both the model and the input. In contrast, the activations of clean and corrupted inputs in our tasks are likely to be much closer together, and our path contains information about both the input and the model. Consequently, fewer steps may be required in our case.

\section{Conclusion}
In this paper, we revisited gradient-based automatic circuit identification and identified the saturation effects and regions, which cause inaccurate edge attributions and unfaithful circuit identification. To address this, we proposed Edge Attribution Patching with GradPath (EAP-GP), which replaces EAP-IG’s fixed straight-line paths with GradPath. This dynamically adjusted path integrates gradients more effectively and mitigates saturation effects. Extensive experiments showed that EAP-GP enhances edge attribution accuracy and circuit faithfulness, outperforming previous methods.









% \section{Impact Statements}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.











































% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}

\newpage

\clearpage %+++++++


\appendix
\onecolumn
\section{Appendix}


\subsection{Datasets}
\label{appendix:datasets}

\textbf{Indirect Object Identification (IOI):} The IOI task (\citep{wang2023interpretability}) involves inputs such as: ``When Amy and Laura got a snack at the house, Laura decided to give it to''; where models are expected to predict ``Amy''. Predictions are evaluated using logit difference (logit diff), computed as the logit of ``Amy'' minus the logit of ``Laura''. Corrupted inputs replace the second occurrence of ``Laura'' with a third name (e.g., ``Nicholas''), making ``Laura'' and ``Amy'' roughly equiprobable. We generate a dataset using \citep{wang2023interpretability}'s dataset generator.

\textbf{Gender-Bias:} The Gender-Bias task is designed to examine gender bias in language models. It provides inputs such as ``The banker wished that'', where biased models tend to complete the sentence with ``he''. Bias is measured using logit difference (logit diff), computed as the logit of ``he'' minus the logit of ``she'', or vice versa if the profession is male-stereotyped. Corrupted inputs replace female-stereotyped professions with ``man'' and male-stereotyped professions with ``woman'', such as transforming ``The banker wished that'' into ``The woman wished that'', prompting the model to generate the opposite pronoun. This task originates from \cite{Vig2020GenderBias} and was later analyzed in a circuit-based context by \cite{Chintam2023GenderBias}.

\textbf{Capital–Country:} In the Capital-Country task, models receive inputs such as ``Port Vila, the capital of'', and are expected to output the corresponding country (Vanuatu). Corrupted instances replace the correct capital with another one, such as changing ``Port Vila, the capital of'' to ``Niamey, the capital of''. Performance is evaluated using the logit difference, defined as the logit of the correct country (Vanuatu) minus the logit of the corrupted country (Niger).

\textbf{Subject-Verb Agreement (SVA):} In the Subject-Verb Agreement (SVA) task, models are given sentences such as ``The pilot the assistant'' and must generate a verb that matches the subject’s number (e.g., ``is'' or ``has'' for pilot). In corrupted inputs, the subject’s number is modified, such as changing ``The pilot the assistant'' to ``The pilot the assistants'', causing the model to produce verbs with the opposite agreement. The model’s performance is evaluated using probability difference, defined as the probability assigned to verbs that agree with the subject minus the probability assigned to those that do not.

\textbf{Hypernymy:} In the Hypernymy task, models must predict a word’s hypernym (or superordinate category) given inputs such as ``, second cousins and other'', where the correct answer is ``relatives''. Corrupted inputs replace the target word with an instance from a different category, such as changing ``second cousins and other'' to ``robins and other''. Model performance is evaluated using probability difference, defined as the probability assigned to the correct hypernyms minus the probability assigned to incorrect ones. 

\textbf{Greater-Than:} In the Greater-Than task, models receive a sentence containing a chronological sequence of years as input and must predict the next year that follows the pattern. Given a clean input, such as ``The contract lasted from the year 1352 to the year 13'', the expected completion is the next valid number in the sequence. Corrupted inputs replace the last number with an incorrect continuation that disrupts the numerical pattern, such as ``The contract lasted from the year 1301 to the year 13''. Model performance is evaluated using probability difference (prob diff), defined as the probability assigned to the correct next number minus the probability assigned to the incorrect one (e.g., ``52'' in the example above).


Each dataset consists of both positive and negative examples. Positive examples require the model to utilize specific circuits to predict the correct next token, whereas negative examples are semantically similar but intentionally corrupted to ensure that no valid next token exists. This design enables us to distinguish attention heads involved in semantic processing from those responsible for circuit-specific computations. To ensure fair comparisons, the number of positive and negative examples in each dataset is kept consistent with \citep{hanna2024faith}. In this study, positive examples correspond to the original input, while negative examples represent the corrupted input. Table \ref{sample-table} provides an example of each task.
Overall, For IOI, Gender-Bias, and Capital-Country, we also use Logit Difference defined as:
\begin{equation}
    \log P(\text{correct}) - \log P(\text{misleading}),
\end{equation}
which measures the difference in log probabilities between the correct and misleading name/pronoun. For SVA, Greater-Than, and Hypernymy, we use Probability Difference, defined as:
\begin{equation}
\sum P(y_{\text{correct}}) - \sum P(y_{\text{incorrect}}),
\end{equation}
which compares the probability of the correct answer with the sum of the probabilities of incorrect answers.

\begin{table*}[t]

\label{sample-table}
\begin{center}
\begin{small}
\begin{sc}
\renewcommand{\arraystretch}{1.5} 
\begin{tabular}{p{4cm}p{5.5cm}p{2.5cm}p{3cm}}
\toprule
Task & Example Prompt & Output & Metric \\ 
\midrule
IOI & “When Amy and Laura got a snack at the house, Laura decided to give it to” \newline
“When Amy and Laura got a snack at the house, Nicholas decided to give it to” & “14235” \newline “16753” & Logit Difference \\ 

SVA  & “The song that the chef likes”\newline
“The songs that the chef likes” & “0” \newline “0” & Probability Difference \\ 

Gender-Bias & “The poet was promoted because”\newline
“The man was promoted because” & “673” \newline “339” & Logit Difference \\ 

Hypernymy & “, gemstones and other” \newline
“, vice presidents and other” & “[16840, 23685]” \newline “[7018, 7602]” & Probability Difference \\ 

Capital–Country & “Kabul, the capital of”\newline
“London, the capital of” & “8037” \newline “1578” & Logit Difference \\ 

Greater-Than & “The contract lasted from the year 1352 to the year 13”\newline
“The contract lasted from the year 1301 to the year 13” & “52” \newline “52” & Probability Difference \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\caption{Overview of Tasks and Metrics Used for Each Task. The table presents original and corrupted examples along with the expected output token IDs for six tasks: Indirect Object Identification (IOI), Subject-Verb Agreement (SVA), Gender Bias, Capital–Country, Hypernymy, and Greater-Than. The ``Example Prompt'' column provides representative original and corrupted inputs for each task. The ``Output'' column displays the expected output token ID, and the ``Metric'' column specifies the evaluation metric used for the corresponding task.}
\end{center}

\end{table*}





\begin{table}[h]
    \centering
    \begin{tabular}{l l r r}
        \toprule
        \textbf{Task} & \textbf{Metric} & \textbf{Clean Baseline} & \textbf{Corrupted Baseline} \\
        \midrule
        IOI & logit diff & 3.80 & 0.03 \\
        SVA & prob diff & 0.154 & -0.157 \\
        Gender-Bias & logit diff & 0.88 & -3.22 \\
        Hypernymy & prob diff & 23.43 & -3.482 \\
        Capital-Country & logit diff & 0.25 & -0.28 \\
        Greater-Than & prob diff & 0.814 & -0.456 \\
        \bottomrule
    \end{tabular}
    \caption{Original and Corrupted Baseline Performance of GPT-2 small across tasks, Clean Baseline refers to \( \delta_{+} \), while the Corrupted Baseline refers to \( \delta_{-} \).}
    \label{tab:baseline_performance}
\end{table}




\subsection{More results}
\label{More results}
In this section, we perform experiments on GPT-2 Medium (345M) (see Figure \ref{medium-performance}) and GPT-2 XL (1.5B) (see Figure \ref{xl-performance}) across six tasks and report the faithfulness of the identified circuits. Our results on both GPT-2 Medium and GPT-2 XL confirm our earlier findings. In both models, when there are discernible differences in the faithfulness of the identified circuits, EAP-GP outperforms both EAP and EAP-IG.

Additionally, we present example circuit diagrams of the circuits found by EAP-GP. However, these come with one caveat: the typical circuits we found still contained too many edges to be displayed in a reasonably sized figure. Therefore, we only provide visualizations for the Greater-Than task with 99.9\% sparsity (such as those reported in Figure \ref{circuit}).


\begin{table*}[h]
\centering
\small
\begin{adjustbox}{width=0.9\textwidth}
\begin{tabular}{lcccc}
\toprule
\textbf{Task} & \textbf{Edge Sparsity (\%)} & \textbf{EAP (\%)} & \textbf{EAP-IG (\%)} & \textbf{EAP-GP (\%)}\\
\midrule

\multirow{11}{*}{IOI}  
 & 99.30\%  & 0.03\%  & 25.53\% & 29.18\%\\
 & 99.00\%  & 0.05\%  & 42.51\% & 41.05\%\\
 & 98.70\%  & 35.41\% & 45.10\% & 44.76\%\\
 & 98.40\%  & 53.05\% & 49.68\% & 59.93\%\\
 & 98.10\%  & 50.21\% & 65.12\% & 65.23\%\\
 & 97.80\%  & 47.56\% & 66.18\% & 73.52\%\\
 & 97.50\%  & 56.94\% & 62.47\% & 80.08\%\\
\midrule

\multirow{11}{*}{SVA} 
 & 99.30\%  & 0.00\%  & 5.79\%  & 9.97\%\\
 & 99.00\%  & 0.00\%  & 13.83\% & 36.66\%\\
 & 98.70\%  & 21.54\% & 46.95\% & 69.77\%\\
 & 98.40\%  & 34.41\% & 72.03\% & 82.64\%\\
 & 98.10\%  & 47.59\% & 80.06\% & 91.32\%\\
 & 97.80\%  & 72.03\% & 84.57\% & 95.50\%\\
 & 97.50\%  & 75.88\% & 84.57\% & 96.14\%\\
\midrule

\multirow{11}{*}{Gender-Bias} 
 & 99.30\%  & 55.57\% & 70.06\% & 68.76\%\\
 & 99.00\%  & 80.00\% & 73.91\% & 78.91\%\\
 & 98.70\%  & 84.91\% & 85.00\% & 88.44\%\\
 & 98.40\%  & 79.61\% & 88.76\% & 90.10\%\\
 & 98.10\%  & 81.76\% & 90.47\% & 90.73\%\\
 & 97.80\%  & 89.54\% & 90.88\% & 91.17\%\\
 & 97.50\%  & 90.05\% & 90.95\% & 91.05\%\\
\midrule

\multirow{11}{*}{Country-Capital} 
 & 99.30\%  & -0.37\% & 52.89\% & 65.36\%\\
 & 99.00\%  & 13.22\% & 75.42\% & 82.50\%\\
 & 98.70\%  & 21.23\% & 82.31\% & 90.32\%\\
 & 98.40\%  & 30.35\% & 86.41\% & 91.62\%\\
 & 98.10\%  & 35.57\% & 90.50\% & 90.32\%\\
 & 97.80\%  & 57.54\% & 91.43\% & 91.81\%\\
 & 97.50\%  & 60.52\% & 91.62\% & 93.67\%\\
\midrule

\multirow{11}{*}{Hypernymy} 
 & 99.30\%  & 7.41\%  & 10.11\% & 14.01\%\\
 & 99.00\%  & 10.30\% & 17.26\% & 24.32\%\\
 & 98.70\%  & 17.11\% & 29.28\% & 38.27\%\\
 & 98.40\%  & 29.57\% & 38.57\% & 46.22\%\\
 & 98.10\%  & 35.51\% & 47.27\% & 54.93\%\\
 & 97.80\%  & 40.31\% & 53.07\% & 59.48\%\\
 & 97.50\%  & 45.58\% & 55.62\% & 67.24\%\\
\midrule

\multirow{11}{*}{Greater-Than} 

 & 99.30\%  & 70.24\% & 81.89\% & 85.83\%\\
 & 99.00\%  & 92.52\% & 95.35\% & 94.88\%\\
 & 98.70\%  & 94.41\% & 98.43\% & 97.95\%\\
 & 98.40\%  & 95.43\% & 98.03\% & 98.98\%\\
 & 98.10\%  & 95.12\% & 98.66\% & 99.69\%\\
 & 97.80\%  & 95.98\% & 100.00\% & 99.53\%\\
 & 97.50\%  & 96.38\% & 99.45\% & 98.98\%\\
 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Normalized faithfulness for Six Tasks in GPT2-Small}
\label{Table small faithfulness}
\end{table*}



\begin{figure*}[t]

\begin{center}
\includegraphics[width=\textwidth]{Figure/scatter_plots_gpt2-medium.pdf}
\caption{ Comparison of circuit performance across different methods on GPT-2 Medium. In all plots, a higher value indicates better performance. EAP-GP identifies circuits that outperform other methods across all six tasks. }
\label{medium-performance}
\end{center}

\end{figure*}



\begin{table*}[h]
\centering
\small
\begin{adjustbox}{width=0.9\textwidth}
\begin{tabular}{lcccc}
\toprule
\textbf{Task} & \textbf{Edge Sparsity (\%)} & \textbf{EAP (\%)} & \textbf{EAP-IG (\%)} & \textbf{EAP-GP (\%)}\\
\midrule

\multirow{9}{*}{IOI}  
 & 98.38\%  & 46.71\% & 60.03\% & 64.75\%\\
 & 97.98\%  & 57.24\% & 62.95\% & 70.04\%\\
 & 97.58\%  & 62.44\% & 69.54\% & 72.94\%\\
 & 97.19\%  & 65.75\% & 75.52\% & 76.83\%\\
 & 96.79\%  & 70.70\% & 77.67\% & 78.03\%\\
\midrule

\multirow{11}{*}{SVA} 
 & 99.17\%  & 82.11\% & 91.05\% & 93.37\%\\
 & 98.77\%  & 89.52\% & 94.33\% & 98.19\%\\
 & 98.38\%  & 94.56\% & 96.08\% & 98.81\%\\
 & 97.98\%  & 95.40\% & 97.09\% & 99.09\%\\
 & 97.58\%  & 96.16\% & 98.82\% & 98.40\%\\
 & 97.19\%  & 96.26\% & 99.00\% & 98.73\%\\
 & 96.79\%  & 96.62\% & 99.14\% & 98.89\%\\

\midrule

\multirow{11}{*}{Gender-Bias} 
 & 99.17\%  & 77.20\% & 80.17\% & 83.28\%\\
 & 98.77\%  & 77.63\% & 81.86\% & 85.46\%\\
 & 98.38\%  & 81.46\% & 86.47\% & 88.69\%\\
 & 97.98\%  & 85.61\% & 90.92\% & 90.74\%\\
 & 97.58\%  & 88.74\% & 91.42\% & 92.85\%\\
 & 97.19\%  & 90.87\% & 94.11\% & 95.52\%\\
 & 96.79\%  & 94.62\% & 98.30\% & 97.82\%\\
\midrule

\multirow{11}{*}{Country-Capital} 
 & 99.17\%  & 42.73\% & 71.34\% & 75.24\%\\
 & 98.77\%  & 54.78\% & 75.72\% & 78.88\%\\
 & 98.38\%  & 62.91\% & 78.74\% & 84.78\%\\
 & 97.98\%  & 71.47\% & 81.36\% & 85.25\%\\
 & 97.58\%  & 75.52\% & 84.19\% & 87.35\%\\
 & 97.19\%  & 79.23\% & 85.58\% & 87.76\%\\
 & 96.79\%  & 80.45\% & 86.16\% & 88.16\%\\

\midrule

\multirow{11}{*}{Hypernymy} 
 & 99.17\%  & 33.42\% & 42.16\% & 54.82\%\\
 & 98.77\%  & 41.45\% & 52.54\% & 64.08\%\\
 & 98.38\%  & 51.24\% & 59.53\% & 72.10\%\\
 & 97.98\%  & 56.07\% & 65.81\% & 76.61\%\\
 & 97.58\%  & 61.14\% & 69.87\% & 79.55\%\\
 & 97.19\%  & 64.88\% & 74.83\% & 82.08\%\\
 & 96.79\%  & 68.53\% & 77.98\% & 87.02\%\\

\midrule

\multirow{11}{*}{Greater-Than} 
 & 99.17\%  & 79.90\% & 83.23\% & 87.96\%\\
 & 98.77\%  & 82.99\% & 87.50\% & 91.78\%\\
 & 98.38\%  & 84.00\% & 89.47\% & 94.71\%\\
 & 97.98\%  & 89.84\% & 91.60\% & 95.15\%\\
 & 97.58\%  & 91.63\% & 93.96\% & 97.90\%\\
 & 97.19\%  & 97.64\% & 92.85\% & 99.19\%\\
 & 96.79\%  & 96.68\% & 95.76\% & 99.16\%\\
 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Normalized faithfulness for Six Tasks in GPT2-Medium}
\label{Table Medium faithfulness}
\end{table*}


\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{Figure/scatter_plots_gpt2-xl.pdf}
\caption{  Comparison of circuit performance across different methods on GPT-2 XL. In all plots, a higher value indicates better performance. EAP-GP identifies circuits that outperform other methods across all six tasks.}
\label{xl-performance}
\end{center}

\end{figure*}









\begin{table*}[h]
\centering
\small
\begin{adjustbox}{width=0.9\textwidth}
\begin{tabular}{lcccc}
\toprule
\textbf{Task} & \textbf{Edge Sparsity (\%)} & \textbf{EAP (\%)} & \textbf{EAP-IG (\%)} & \textbf{EAP-GP (\%)}\\
\midrule

\multirow{11}{*}{IOI}  
 & 99.19\%  & 55.63\% & 60.02\% & 61.43\%\\
 & 98.79\%  & 64.71\% & 69.69\% & 68.59\%\\
 & 98.39\%  & 73.07\% & 74.77\% & 74.51\%\\
 & 97.99\% & 78.55\% & 77.99\% & 80.24\%\\
 & 97.59\% & 82.27\% & 81.33\% & 82.97\%\\
 & 97.19\% & 83.91\% & 84.90\% & 86.08\%\\
 & 96.79\% & 85.20\% & 85.52\% & 88.14\%\\
\midrule

\multirow{11}{*}{SVA} 
 & 99.20\%  & 90.17\% & 93.69\% & 96.22\%\\
 & 98.80\%  & 94.28\% & 96.27\% & 98.00\%\\
 & 98.40\%  & 96.30\% & 97.73\% & 98.30\%\\
 & 98.00\%  & 97.10\% & 98.66\% & 98.25\%\\
 & 97.60\%  & 97.53\% & 98.97\% & 98.57\%\\
 & 97.20\%  & 98.01\% & 99.18\% & 98.75\%\\
 & 96.80\%  & 98.38\% & 99.39\% & 99.06\%\\
\midrule

\multirow{11}{*}{Gender-Bias} 
 & 98.80\%  & 98.26\% & 98.96\% & 99.58\%\\
 & 98.40\%  & 98.85\% & 99.36\% & 99.91\%\\
 & 98.00\%  & 99.25\% & 99.63\% & 100\%\\
 & 97.60\%  & 99.67\% & 100\% & 100\%\\
 & 97.20\%  & 99.67\% & 100\% & 100\%\\
 & 96.80\%  & 99.58\% & 100\% & 100\%\\
\midrule

\multirow{10}{*}{Country-Capital} 
 & 99.20\%  & 43.96\% & 74.92\% & 83.46\%\\
 & 98.80\%  & 64.10\% & 81.29\% & 86.78\%\\
 & 98.40\%  & 72.07\% & 85.23\% & 89.97\%\\
 & 98.00\%  & 78.09\% & 88.18\% & 91.28\%\\
 & 97.60\%  & 80.66\% & 89.28\% & 92.42\%\\
 & 97.20\%  & 85.00\% & 90.91\% & 93.49\%\\
 & 96.80\%  & 87.06\% & 92.03\% & 94.17\%\\

\midrule

\multirow{11}{*}{Hypernymy} 
 & 99.20\%  & 55.16\% & 60.54\% & 74.73\%\\
 & 98.80\%  & 68.55\% & 73.93\% & 84.02\%\\
 & 98.40\%  & 76.64\% & 82.15\% & 88.42\%\\
 & 98.00\%  & 82.60\% & 86.41\% & 91.24\%\\
 & 97.60\%  & 86.50\% & 88.67\% & 93.61\%\\
 & 97.20\%  & 88.79\% & 90.94\% & 95.23\%\\
 & 96.80\%  & 90.71\% & 92.79\% & 95.92\%\\
\midrule

\multirow{10}{*}{Greater-Than} 
 & 98.80\%  & 97.96\% & 98.15\% & 98.68\%\\
 & 98.40\%  & 98.30\% & 98.75\% & 98.91\%\\
 & 98.00\%  & 98.15\% & 98.74\% & 99.08\%\\
 & 97.60\%  & 98.47\% & 98.61\% & 99.11\%\\
 & 97.20\%  & 98.53\% & 98.75\% & 99.13\%\\
 & 96.80\%  & 99.31\% & 98.89\% & 99.45\%\\
 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Normalized faithfulness for Six Tasks in GPT2-XL}
\label{Table XL faithfulness}
\end{table*}
























\begin{figure*}[t]

\begin{center}
\includegraphics[width=\textwidth]{Figure/circuit_graph.pdf}
\caption{ A circuit for Greater-Than with 99.9\% sparsity, found by EAP-GP.}
\label{circuit}
\end{center}

\end{figure*}










\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
