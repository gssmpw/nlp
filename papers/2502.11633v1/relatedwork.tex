\section{Related Work}
With the advent of multimodal representation learning, the cross-modal text-molecule retrieval task has garnered significant attention. Existing methods can be broadly categorized into three groups: statistic-based, hashing-based, and deep learning-based approaches.

\subsection{Statistic-Based Methods}
Statistic-based methods aim to bridge the modality gap by analyzing feature correlations in subspace projections. Canonical correlation analysis \cite{hardoon2004canonical}, kernel canonical correlation analysis \cite{lai2000kernel}, and deep canonical correlation analysis \cite{andrew2013deep} are representative methods that maximize correlations between text and molecule embeddings in a shared latent space. While these methods demonstrated initial success, they often struggle with complex, high-dimensional data and fail to capture deep semantic relationships between different modalities.

\subsection{Hashing-Based Methods}
Hashing-based methods focus on mapping high-dimensional multimodal features into binary codes to enable efficient similarity search. Techniques such as spectral hashing \cite{weiss2008spectral}, self-taught hashing \cite{zhang2010self}, and iterative quantization \cite{gong2012iterative} project text and molecule representations into a shared Hamming space, where retrieval is accelerated by comparing binary codes. Notably, the discretization process may overlook nuanced semantic dependencies between textual concepts and molecular structural motifs, thereby limiting retrieval accuracy.

\begin{figure*}[!htbp]
\centering
\includegraphics[scale=0.48]{CLASS.pdf}
\caption{Overview of CLASS. All the molecules are represented by white spheres for H, red for O, gray for C, and yellow for P. Initially, the \textbf{multimodal encoder} (\textcircled{1}) encodes $z_i$ and $z_j$, and then inputs them into the \textbf{sample difficulty quantification} (\textcircled{2}) to calculate the similarity between samples, quantifying the difficulty of sample $z_i$ based on the number $\mathcal{N}_{i}$ of confusing samples. Thereafter, the \textbf{sample scheduler} (\textcircled{3}) based on a curriculum learning strategy introduces training samples via an easy-to-hard paradigm. Finally, the \textbf{adaptive intensity learning} (\textcircled{4}) dynamically adjusts the model's training intensity to control the global training process of the model.}
\vspace{-5pt}
\label{fig:overview}
\end{figure*}

\subsection{Deep Learning-Based Methods}
Deep learning pays attention to mapping the semantic representations of different modalities into a shared embedding space to facilitate similarity evaluation. Adversarial networks are widely employed in image-recipe retrieval tasks \cite{wang2019learning, li2021multi}. For instance, \citet{10063974} introduces adversarial training to align text and molecule representations, achieving robust modality alignment through triplet loss and a min-max game strategy. Another line of work involves the introduction of contrastive learning, with \citet{su2022molecularmultimodalfoundationmodel} deploying contrastive learning on molecular graphs and text embeddings. Additionally, \citet{10822800} leverages optimal transport for multi-grained alignment. \citet{10821722} decomposes molecules into hierarchical graphs and aligns them with text at multiple granularities using optimal transport, significantly improving retrieval precision. However, these methods overlook the aspect of enhancing training efficiency and varying difficulty across samples.