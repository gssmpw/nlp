\section{Related Works}
%\begin{itemize}
    %\item 
    \textbf{Multi-agent multi-armed bandit.} Numerous frameworks and algorithms have been proposed to solve various multi-agent multi-armed bandit problems. 
    %
    There are many prior works on different multi-agent multi-armed variants.
    %
    For example, 
    \cite{vial2021robust} considers the setting of including honest and malicious agents who recommend best-arm estimates and arbitrary arms, respectively. \cite{liu2020competing,zhangmatching,kong2023player} 
    add the feature that the agents may have collisions with each other when they are pulling the same arm in each time step. \cite{baek2021fair,yang2022distributed} consider the user or group's set of available arms as a subset of the complete arm set, and the regret for each user or group is based on the choices made within their own arm set.
    % arms have preferences over players. 
    Compared with our model, these existing works do not consider the case where an agent may refuse to share all her information with others, and even decide to withdraw if she is forced to share some data that she is not willing to share.
    
    % \junning{communication related works}
    Another strand of literature studies multi-agent multi-armed bandit problems with limited communication. For example, \cite{agarwal2021multi} considers a multi-agent multi-armed bandit framework with limited communication rounds and limits bits in each communication rounds; \cite{madhushani2021call} proposes ComEx, a novel and cost-effective communication protocol for cooperative bandits; in the work of \cite{shahrampour2017multi}, players can only exchange information locally to estimate the global reward confined to a network structure; and \cite{sankararaman2019social} proposes a pairwise asynchronous gossip-based protocol that only needs to exchange a limited number of bits to finish communication. 
%
Compared with these models, we are more interested in which data users are willing to share, rather than the form in which the data is shared. Therefore, we did not consider the adoption of more efficient sharing methods by users, and regard the communication costs as zero. %Hence, we considered the scenario where communication costs are zero.

     %\item 
     \textbf{Federated Learning.}
     % \junning{foundations of FL}
     Federated learning (FL), a decentralized machine learning approach, has gained significant attention in recent years. This emerging paradigm enables training of machine learning models on distributed data sources while preserving data privacy. Several related works have explored the foundations of federated learning. \cite{mcmahan2017communication} introduced the concept of federated learning and proposed a practical framework for training models on decentralized data, which enables collaborative model training across multiple devices. There are numerous foundational works dedicated to addressing the various challenges of federated learning and designing federated learning algorithms from multiple aspects, such as privacy preservation, robustness, efficiency, security, scalability, and performance \cite{kamp2019efficient, pillutla2022robust, mcmahan2017communication, jeong2018communication, sattler2019robust}.  Federated learning also expands to encompass a broad range of applications in healthcare \citep{kaissis2020secure}, manufacturing \citep{qu2020blockchained}, agriculture \citep{durrant2022role}, energy \citep{saputra2019energy}, and other fields. 
     
     There are also some prior works on applying FL in bandit problems. 
     For example, in \cite{shi2021federated,shi2021federated1}, an MAB framework of multiple heterogeneous agents and a global principal is proposed. In this framework, agents' local bandit models are not the same (i.e., different agents may have different expected rewards on the same arm) and the goal of the principal is to find the arm with the largest global mean.
     %
    \cite{zhu2021federated} proposes a framework where agents can only communicate their local data with neighbors in a connected graph.  They propose the FedUCB policy, in which the agents preserve differential privacy of their local data. 
    Compared with our model, \cite{shi2021federated,shi2021federated1} require that all agents must follow the global instructions unconditionally and send all their information to the principal. \cite{zhu2021federated} do not consider the case where agents could refuse to share their private information even with differential privacy. In our model, each agent can limit her shared information as her wish. 
    % \siwei{\citet{zhu2021federated} also need to send all the data?}
    %In this work, agents only need to send the partial information she is willing to share instead of the full information which contains private information.
    
    %\item 
    \textbf{Incentivized Learning.} 
    % \junning{Incentive}
    Since its proposal by \cite{frazier2014incentivizing, kremer2014implementing}, significant progress has been made in the field of incentivized learning in multi-armed bandit (MAB) problems. Specifically, there are two distinct lines of research in this area. 

    The first line of research \cite{kremer2014implementing, mansour2015bayesian, mansour2016bayesian, immorlica2018incentivizing} assumes that the principal has access to the complete history of actions and rewards, while the agents do not. In this setting, the principal can incentivize them to learn by leveraging proper information to them.
    
    The second line of research considers a publicly available history of actions and rewards, and the incentives are implemented through compensations. This concept was initially introduced by \cite{frazier2014incentivizing} and further generalized by \cite{han2015incentivizing} in Bayesian settings. In the non-Bayesian case,  \cite{wang2018multi} first studied this approach, and it has been recently extended by \cite{wang2021incentivizing}.
        
    
    % Many progresses have been made in incentivized learning in MAB instances. 
    % For example, \cite{frazier2014incentivizing, han2015incentivizing, wang2018multi, wang2021incentivizing} consider the case that the full history information is publicly available and the agents are motivated through compensations, and \cite{kremer2014implementing,mansour2015bayesian} assume that the agents cannot observe the full history information, so the principal can incentivize them to learn by leveraging proper information to them. 
    %

    %Compared with our results, 
    All the aforementioned works assume that every agent is myopic (i.e., they only do exploitation to maximize their short-term rewards), while we assume that every agent is considering her long-term rewards. 
    %
    Hence, the incentive mechanism in this paper can be very different from theirs. 
    %
  


   
    
    
%\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%