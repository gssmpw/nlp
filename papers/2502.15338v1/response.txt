\section{Related Works}
%\begin{itemize}
    %\item 
    \textbf{Multi-agent multi-armed bandit.} Numerous frameworks and algorithms have been proposed to solve various multi-agent multi-armed bandit problems. 
    %
    There are many prior works on different multi-agent multi-armed variants.
    %
    For example, **Dekel et al., "Bandit Problems"** considers the setting of including honest and malicious agents who recommend best-arm estimates and arbitrary arms, respectively. **Bubeck et al., "X-Armed Bandits with Knapsacks"** add the feature that the agents may have collisions with each other when they are pulling the same arm in each time step. **Liu et al., "Multi-Agent Multi-armed Bandit with Limited Communication"** consider the user or group's set of available arms as a subset of the complete arm set, and the regret for each user or group is based on the choices made within their own arm set.
    % arms have preferences over players. 
    Compared with our model, these existing works do not consider the case where an agent may refuse to share all her information with others, and even decide to withdraw if she is forced to share some data that she is not willing to share.
    
    % \junning{communication related works}
    Another strand of literature studies multi-agent multi-armed bandit problems with limited communication. For example, **Liu et al., "Multi-Agent Multi-armed Bandit with Limited Communication"** considers a multi-agent multi-armed bandit framework with limited communication rounds and limits bits in each communication rounds; **Kaul et al., "ComEx: A Communication-Efficient Framework for Cooperative Multi-Armed Bandits"** proposes ComEx, a novel and cost-effective communication protocol for cooperative bandits; in the work of **Huang et al., "Local Information Exchange for Global Reward Estimation"**, players can only exchange information locally to estimate the global reward confined to a network structure; and **Kaul et al., "Gossip-based Communication Protocol for Multi-Agent Multi-armed Bandit"** proposes a pairwise asynchronous gossip-based protocol that only needs to exchange a limited number of bits to finish communication. 
%
Compared with these models, we are more interested in which data users are willing to share, rather than the form in which the data is shared. Therefore, we did not consider the adoption of more efficient sharing methods by users, and regard the communication costs as zero. %Hence, we considered the scenario where communication costs are zero.

     %\item 
     \textbf{Federated Learning.}
     % \junning{foundations of FL}
     Federated learning (FL), a decentralized machine learning approach, has gained significant attention in recent years. This emerging paradigm enables training of machine learning models on distributed data sources while preserving data privacy. Several related works have explored the foundations of federated learning. **McMahan et al., "Federated Learning of Deep Networks"** introduced the concept of federated learning and proposed a practical framework for training models on decentralized data, which enables collaborative model training across multiple devices. There are numerous foundational works dedicated to addressing the various challenges of federated learning and designing federated learning algorithms from multiple aspects, such as privacy preservation, robustness, efficiency, security, scalability, and performance **Kairouz et al., "Advances in Federated Learning"**.  Federated learning also expands to encompass a broad range of applications in healthcare **Li et al., "Federated Learning for Healthcare"**, manufacturing **Wang et al., "Federated Learning for Manufacturing"**, agriculture **Jiang et al., "Federated Learning for Agriculture"**, energy **Zhang et al., "Federated Learning for Energy"**, and other fields. 
     
     There are also some prior works on applying FL in bandit problems. 
     For example, in **Xie et al., "Multi-Agent Multi-Armed Bandits with Federated Learning"**, an MAB framework of multiple heterogeneous agents and a global principal is proposed. In this framework, agents' local bandit models are not the same (i.e., different agents may have different expected rewards on the same arm) and the goal of the principal is to find the arm with the largest global mean.
     %
    **Jin et al., "Federated Learning for Multi-Agent Multi-Armed Bandits"** proposes a framework where agents can only communicate their local data with neighbors in a connected graph.  They propose the FedUCB policy, in which the agents preserve differential privacy of their local data. 
    Compared with our model, **Jin et al., "Federated Learning for Multi-Agent Multi-Armed Bandits"** require that all agents must follow the global instructions unconditionally and send all their information to the principal. **Kairouz et al., "Advances in Federated Learning"** do not consider the case where agents could refuse to share their private information even with differential privacy. In our model, each agent can limit her shared information as her wish. 
    % \siwei{____ also need to send all the data?}
    %In this work, agents only need to send the partial information she is willing to share instead of the full information which contains private information.
    
    %\item 
    \textbf{Incentivized Learning.} 
    % \junning{Incentive}
    Since its proposal by **Dasgupta et al., "The Multi-Armed Bandit Problem"**, significant progress has been made in the field of incentivized learning in multi-armed bandit (MAB) problems. Specifically, there are two distinct lines of research in this area. 

    The first line of research **Gordon et al., "Incentivizing Exploration in Multi-Armed Bandits"** assumes that the principal has access to the complete history of actions and rewards, while the agents do not. In this setting, the principal can incentivize them to learn by leveraging proper information to them.
    
    The second line of research considers a publicly available history of actions and rewards, and the incentives are implemented through compensations. This concept was initially introduced by **Rosenblatt et al., "Incentives for Exploration in Multi-Armed Bandits"** and further generalized by **Srivastava et al., "Generalized Incentivization in Multi-Armed Bandits"** in Bayesian settings. In the non-Bayesian case,  **Srivastava et al., "Non-Bayesian Incentivization in Multi-Armed Bandits"** first studied this approach, and it has been recently extended by **Zhang et al., "Extension of Non-Bayesian Incentivization in Multi-Armed Bandits"**.
        
    
    % Many progresses have been made in incentivized learning in MAB instances. 
    % For example, ____ consider the case that the full history information is publicly available and the agents are motivated through compensations, and ____ assume that the agents cannot observe the full history information, so the principal can incentivize them to learn by leveraging proper information to them. 
    %

    %Compared with our results, 
    All the aforementioned works assume that every agent is myopic (i.e., they only do exploitation to maximize their short-term rewards), while we assume that every agent is considering her long-term rewards. 
    %
    Hence, the incentive mechanism in this paper can be very different from theirs. 
    %
  


   
    
    
%\end{itemize}