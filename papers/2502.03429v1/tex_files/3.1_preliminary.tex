\vspace{-2em}
\section{Preliminary}
\label{sec:pre}

\paragraph{Structure of U-MLLMs}
We consider an \emph{autoregressive} U-MLLM that, given a textual prompt $x$, first converts it into a sequence of text tokens $\{x_1, \dots, x_{T_x}\}$ and then generates a sequence of \emph{image tokens} $\{z_1,\dots,z_{T_z}\}$, which an image decoder reconstructs into a final image $y$ (see~\autoref{fig:unified_model} for the pipeline). Let $\boldsymbol{\theta} = \{\boldsymbol{\theta}_{\mathrm{v}}, \boldsymbol{\theta}_{\mathrm{l}}\}$ denote the model parameters, where: \emph{(1)} $\boldsymbol{\theta}_{\mathrm{v}}$ is the \textbf{image tokenizer} (encoder-decoder) responsible for converting input images into discrete tokens and decoding tokens back into images. \emph{(2)}$ \boldsymbol{\theta}_{\mathrm{l}}$ is the \textbf{language model} (LM) that processes and generates token sequences (both text and image tokens) in a unified autoregressive way.

As shown in \autoref{fig:vision_tower},  the image encoder $E_{\boldsymbol{\theta}_{\mathrm{v}}}$ maps an image $y$ into latent embeddings $\textbf{e}$, then quantizes the embeddings into a discrete token sequence $\{z_1,\dots,z_{T_z}\}$. Conversely, the image decoder $D_{\boldsymbol{\theta}_{\mathrm{v}}}$ inverts this process:
\begin{align}
    \{z_1,\dots,z_{T_z}\} &= E_{\boldsymbol{\theta}_{\mathrm{v}}}(y), 
    \label{eq:enc}\\
    y &= D_{\boldsymbol{\theta}_{\mathrm{v}}}(z_1,\dots,z_{T_z}).
    \label{eq:dec}
\end{align}
Meanwhile, the LM $LM_{\boldsymbol{\theta}_{\mathrm{l}}}$ treats both text tokens and image tokens uniformly under a single next-token probability:
\begin{equation}
    P_{\boldsymbol{\theta}}(z_t \mid x,\, z_{<t})
    \;=\;
    LM_{\boldsymbol{\theta}_{\mathrm{l}}}(z_{t-1},\dots,z_1;\, x).
    \label{eq:lm}
\end{equation}
This design allows the U-MLLM to perform \emph{visual understanding} by mapping an image to a semantic space (via \cref{eq:enc}) and then interpreting those discrete tokens as inputs to the LM, and to perform \emph{image generation} by autoregressively sampling $\{z_t\}$ from \cref{eq:lm} and reconstructing an image from the resulting token sequence via \cref{eq:dec}.
\vspace{-1em}
\paragraph{Demographic Bias.}  
When conditioning on neutral generation prompt (no explicit gender or race specified), the model could exhibit \emph{demographic bias} by disproportionately generating output with a distribution skewing towards certain demographic groups (e.g., “male” or “female,” “Asian” or “Black”). ~\autoref{fig:fairness-mllms} shows such an example where a prompt \emph{``Please generate images of construction workers''} yield mostly images samples of one demographic group.

Formally, let $d \in \mathcal{D}$ where $D = \{d_1, \ldots, d_K\}$ is a set of demographic labels (e.g., \{male, female\} or \{Asian, Black, White, Indian\}). Given a neutral prompt \( x \), such as ``a portrait of a construction worker'', an unbiased model would generate a list of images \( \{y_1, y_2, \ldots\} \) with these demographic labels in a balanced distribution, for example, 50-50 for gender, or a uniform distribution for race. By contrast, we find that the latest U-MLLMs generate
\[
P_\theta(\mathcal{C}(y) = d_i \mid x) \gg P_\theta(\mathcal{C}(y) = d_j \mid x),
\]
where $d_i, d_j$ corresponds to ``male'',``female'' respectively(in this case) and $\mathcal{C}$ is a pre-trained image classifier from prior study \cite{shen2024finetuningtexttoimagediffusionmodels} that labels each image $y$ with a predicted attribute $\hat{d}_i$. This indicated a strong bias in gender preferences. Our goal is to mitigate this bias while preserving overall image fidelity.
\vspace{-1em}
\paragraph{Direct Preference Optimization} As an alternative to computational expensive RLHF \cite{ouyang2022traininglanguagemodelsfollow} methods that are required to train a reward model separately, DPO re-parameterizes reward model $r$ via a ratio between the policy $\pi_\theta$ and a reference policy $\pi_{\text{ref}}$ and eliminates the cost of training a reward function $r(x,y)$ explicitly \cite{rafailov2024direct}. Concretely, for a given pair $(x,y)$,
\begin{align}
r(x,y) 
&= \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)} 
+ \beta \log Z(x),
\label{eq:dpo_reward_restate}
\end{align}
where $Z(x)$ is a partition function and $\beta$ is a scaling factor\cite{rafailov2024direct}. By plugging $r$ into a Bradley-Terry model\cite{Bradley1952RankAO}, DPO formulates the preference probability for a winning response $y_w$ over a losing response $y_l$ as
\begin{equation}
p(y_w \succ y_l \mid x) 
\;=\;
\sigma \bigl( r(x,y_w) - r(x,y_l)\bigr),
\label{eq:bt-model}
\end{equation}
DPO aims to optimize the policy \emph{directly} by minimizing \autoref{eq:bt-model} and increasing the difference in models' preferences between $y_w$ and $y_l$ without a separate reward model.

To further reduce the overhead of the reference model, recent work explores \emph{reference-free} preference optimization~\cite{hong2024orpomonolithicpreferenceoptimization, meng2024simposimplepreferenceoptimization}. One representative method, ORPO~\cite{hong2024orpomonolithicpreferenceoptimization}, defines the \emph{odds} of generating a response $y$ given prompt $x$ as
\begin{equation}
    \text{odds}_\theta(y \mid x) 
    \;=\; 
    \frac{p_\theta(y \mid x)}{1 - p_\theta(y \mid x)}
\end{equation}

and further quantifies the odds ratio between two responses $y_{w}$ and $y_{l}$ via
\begin{equation}
    \text{OR}_\theta\bigl(y_{w}, y_{l}\bigr)
    \;=\;
    \frac{\text{odds}_\theta(y_{w} \mid x)}{\text{odds}_\theta(y_{l} \mid x)}.
\end{equation}
and finally, define a preference loss term as
\begin{equation}
    \mathcal{L}_{OR} = -\log \sigma \left( \log \frac{odds_\theta(y_w|x)}{odds_\theta(y_l|x)} \right) \label{eq:ratio} 
\end{equation}
By encouraging a large odds ratio $\text{OR}_\theta(y_{w}, y_{l})$, the model is pushed to \emph{prefer} the response $y_{w}$ over $y_{l}$ \emph{directly}, without relying on a separate  “reference” model. 
\vspace{-1em}
\section{Locating Bias}
As shown in~\autoref{fig:locating_bias}, to identify the biases that might emerge from \emph{where}, we analyze intermediate outputs - that is, the \emph{image tokens} (produced by the LM) and the \emph{image embeddings} (produced by the vision encoder) during the decoding step. We consider two main hypotheses regarding the origin of demographic bias as follows.
\vspace{-1em}
\subsection{Hypothesis I: The Bias is from Vision Encoder}
The vision encoder transforms input images into a high-dimensional embedding space, as illustrated in~\autoref{fig:vision_tower}. This encoder itself may be a source of bias within U-MLLMs. To test this hypothesis, we focus on auditing the \emph{vision encoder}, which is trained symmetrically alongside the decoder. By detecting the embedding output from the encoder, we aim to identify any inherent biases that could influence the overall fairness of the model.

\vspace{-1em}
\paragraph{Linear Probing on Image Embeddings}
\autoref{fig:locating_bias} illustrated the overall pipeline to audit vision encoder: first, we sample a balanced set of images denoted as ${y_i}$ from FairFace\cite{kärkkäinen2019fairfacefaceattributedataset} covering different demographic attributes (e.g., male, female). By feeding these images into the vision encoder, we obtained a set of image embeddings denoted as $\mathbf{E} = \{\mathbf{e}_i\}$. Each embedding $\mathbf{e_i}$ is labeled with the ground truth attribute $d_i \in \{ \text{male}, \text{female} \}$ according to its corresponding image input $y_i$. We then split $\mathbf{E}$ into training and testing subsets and train a \emph{linear} classifier $\ell(\cdot)$ to predict demographic labels from embeddings $e_i$. This gives us pairs $\bigl(y_i, \hat{d}_i, \mathbf{e}_i\bigr)$—i.e.\ the image, the predicted demographic label, and the corresponding image embedding. The precision, F1 score, recall and precision in the test set are all \emph{high}, indicating that the encoder’s latent space preserves explicit demographic distinctions:
\[
\widehat{d}_i \;=\; \ell(e_i) \quad \rightarrow \quad \mathrm{High\ Accuracy}.
\]
Since the decoder is trained to invert the encoder, it should \emph{faithfully} preserve these attributes. We also find that when we prompt the model with an attribute (e.g., ``Asian professor''), the final generated images are overwhelmingly associated with that attribute. This also implies that the \emph{decoder} consistently reflects whatever demographic semantics are either present in the tokens or passed by the encoder. Therefore, the chance that bias originated from the vision encoder is small. 
\vspace{-1em}
\subsection{Hypothesis II: The bias is from the language model}
Let $x_{\text{neutral}}$ be a neutral prompt (e.g., ``a photo of a professor''), and let $x_{\text{aug}}(d_i)$ denote an \emph{augmented} version of the same prompt specifying a particular demographic $d_i$ (e.g., ``a photo of a female professor''). Assume that for each $x$, we sample a list of images $\{y_1, \ldots, y_M\}$ and each image $y_i$ is associated with a sequence of image tokens generated from LM as $\mathbf{z}_i = (z_{i,1}, z_{i,2}, \ldots)$.

\vspace{0.5em}
\paragraph{Collecting Image Tokens.}
We record the \emph{intermediate} image tokens for (\emph{1}) The neutral prompt $x_{\text{neutral}}$  and (\emph{2}) The demographic-augmented prompts $x_{\text{aug}}(d_i)$ as described above. We then use a pre-trained image classifier $\mathcal{C}$ \cite{shen2024finetuningtexttoimagediffusionmodels} to label each final image $y_i$ with a predicted attribute $ \widehat{d}_i \;=\; \mathcal{C}(y_i)$. This yields pairs $\bigl(x_i, y_i, \hat{d}_i, \mathbf{z}_i\bigr)$—i.e.\ the prompt, the resulting image, the predicted demographic label, and the corresponding image token sequence.
\vspace{-1em}
\paragraph{Distribution Over Image Tokens.}
Let $p_{\theta}(z \mid x)$ denote the probability distribution over the image tokens $z$ given a prompt $x$. In practice, we approximate it by a finite sample:
\[
\widehat{p}_{\theta}(z \mid x)
\;=\;
\frac{1}{M}\,\sum_{i=1}^{M} \delta(\mathbf{z}_i),
\]
where $\delta(\cdot)$ is a Kronecker delta that counts the sampled sequences $\{\mathbf{z}_i\}$. This empirical approximation estimates the token distribution \( p_{\theta}(z \mid x) \) by averaging over \( M \) generated image token sequences, effectively capturing the model's likelihood of producing each token \( z \) given the prompt \( x \).

We then calculate a \emph{distribution distance}, e.g., Jensen-Shannon Divergence(JSD)\cite{Lin1991DivergenceMB}, between the neutral-prompt distribution $\widehat{p}_{\theta}(z \mid x_{\text{neutral}})$ and the augmented-prompt distribution $\widehat{p}_{\theta}(z \mid x_{\text{aug}}(d_i))$. Formally,
\[
D_{\mathrm{JS}}\!\Bigl(
\widehat{p}_{\theta}(z \mid x_{\text{neutral}})\;\|\;
\widehat{p}_{\theta}(z \mid x_{\text{aug}}(d_i))
\Bigr).
\]
where
\[
D_{\mathrm{JS}}(P\|Q) = \frac{1}{2}D_{\mathrm{KL}}(P\|M) + \frac{1}{2}D_{\mathrm{KL}}(Q\|M)
\]
and $M = \frac{1}{2}(P + Q)$ is the average distribution, with $D_{\mathrm{KL}}$ denoting the Kullback-Leibler divergence.

% \ch{\textbf{Results}}. 
We observe that when $d_i$ matches the \emph{predicted demographic label} $\hat{d}_i$ predicted from the final images, the JSD between the two distributions is significantly smaller. In other words, the token-level distribution for the neutral prompt $x_{\text{neutral}}$ is \emph{closest} to that for $x_{\text{aug}}(d_i)$ corresponding to the majority demographic group the model actually generated. 
\vspace{-1em}
\paragraph{Takeaway} This indicates that the \emph{language model} itself already introduces demographic preferences \emph{at the token level}. If the prompt ``firefighter'' leads to a token distribution similar to ``male firefighter,'' it suggests that the tokens $\mathbf{z}_i$ are implicitly biased toward that demographic, even \emph{before} any visual decoding procedure. Hence, the major source of demographic bias remains in the upstream processes: the autoregressive image token generation. In the following section, we describe the approach we developed to mitigate the debias in image generation by focusing on \emph{LM}.



