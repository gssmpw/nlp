
\begin{abstract}
Unified multimodal large language models (U-MLLMs) have demonstrated impressive performance in \emph{visual understanding} and \emph{generation} in an end-to-end pipeline. 
Compared with \emph{generation-only} models (e.g., Stable Diffusion), U-MLLMs may raise new questions about \emph{bias} in their outputs, which can be affected by their unified capabilities. This gap is particularly concerning given the under-explored risk of propagating harmful stereotypes.
% -- a risk in current U-MLLMs. 
In this paper, we benchmark the latest U-MLLMs and find that most of them exhibit significant demographic biases, such as gender and race bias. % The paper can be better with more kinds of biases. 
% For example, when prompted with ``\emph{generate an image of a construction worker},'' the models produce images labeled as male overwhelmingly. 
To better understand and mitigate this issue, we propose a \emph{locate-then-fix} strategy, where we \emph{audit} show how the individual model component is affected by bias.
% ---including the language model and the vision tokenizer---through techniques such as linear probing. 
Our analysis shows that bias originates primarily from the language model. 
% Second, inspired by direct preference optimization, 
More interestingly, we observe a ``partial alignment'' phenomenon in U-MLLMs, where understanding bias appears minimal, but generation bias remains substantial.
Thus, we propose a novel \emph{balanced preference loss} to balance the demographic distribution with synthetic data.
% by data synthesis. 
Experiments demonstrate that our approach reduces demographic bias while preserving semantic fidelity. 
% Finally, we examine the relationship between \emph{understanding bias} and \emph{generation bias} in U-MLLMs. We observe ``partial alignment'' in some models, where understanding bias appears minimal but generation bias remains substantial.  
We hope our finding underscores the need for more holistic interpretation and debiasing strategies of U-MLLMs %for both interpretation and generation 
in the future. 
\end{abstract}
