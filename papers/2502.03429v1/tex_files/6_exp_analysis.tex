\subsection{Discussion}
\begin{figure*}[ht]
    \centering
    \vspace{-0.2cm}
    \includegraphics[width=1.0\textwidth]{figs_tables/samples.pdf}
    \vspace{-0.8cm}
    \caption{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”. Images generated by: VILA-U (bottom-left), our method reduced bias in gender(bottom-right), race(top-left), gender-race(top-right). For each image, the first color-coded bar represents the predicted gender: \textcolor{blue}{male} or \textcolor{red}{female}. The second bar represents the predicted race: \textcolor{green}{While}, \textcolor{orange}{Asian}, Black, or \textcolor{brown}{Indian}. The height of each bar indicates the prediction confidence.}
    \label{fig:vilau-samples-compara}
    \vspace{-0.5cm}
\end{figure*}
\paragraph{Qualitive Samples}

In \autoref{fig:vilau-samples-compara}, we compare the VILA-U model with our method in different debiasing dimensions. Our approach evidently enables the model to generate more diverse samples. Our method effectively mitigates stereotypes and reduces the risk of perpetuating inequalities in automatically generated content.
\vspace{-1em}
\paragraph{Is there understanding bias in U-MLLMs?}
We examine \emph{visual understanding bias} to investigate bias in multimodal language models further. Specifically, we provide each U-MLLM with a prompt of the form \textit{``What is the gender of \{\texttt{occupation}\}? Answer with options male, female, unknown.''} while providing an input \emph{empty image} to ensure a fair comparison. We then sample 100 responses per prompt under a fixed sampling strategy. From these textual outputs, we extract the demographic attributes that each model most frequently associates with a given occupation. Then we use $RD$ metrics to calculate the bias.

\begin{figure}[ht]
    \centering
\includegraphics[width=0.48\textwidth]{figs_tables/compare_understanding_w_generation.pdf}
    \vspace{-1cm}
    \caption{Compare the understanding and generation bias.}
    \vspace{-0.8cm}
    \label{fig:compare_bias}
\end{figure}


Results in Figure~\ref{fig:compare_bias} indicate that most models exhibit a pronounced \emph{understanding bias}; they systematically infer one gender or demographic for an occupation even when provided no actual visual cues. Interestingly, we observe cases of \emph{partial alignment}. For example, while the Janus model shows a moderate understanding bias in its responses---often refusing to label an individual’s gender or race explicitly, it nonetheless exhibits a substantial generation bias when prompted to create images. In other words, the model’s \emph{understanding-level} alignment (refusal to answer demographic queries) does not carry over to its \emph{image-generation} behavior, which remains biased. This highlights that merely aligning a model’s visual understanding to minimize bias in VQA scenarios is insufficient if its generative capability still systematically favors particular demographics. 

\paragraph{Can debiasing understanding help debias generation?}
Prior work\cite{tong2024metamorphmultimodalunderstandinggeneration} suggests that enhancing a model’s capability for image understanding can indirectly improve its performance in image generation. Motivated by this, we use the diffusion-generated images and their demographic labels (derived from prompts) to train the model in a \emph{discriminative} fashion. The resulting model fails to reduce demographic bias in \emph{image generation}. In other words, the ability to classify or reason about demographic attributes in images does not carry over to balanced \emph{generation} of images with varied demographics. 

These findings imply that bias in \emph{understanding} (e.g., naming attributes) and bias in \emph{generation} (e.g., synthesizing certain demographics disproportionately) may arise from distinct internal mechanisms. Whereas understanding tasks rely on discriminative representations, generative tasks involve autoregressive sampling or likelihood maximization that can perpetuate different stereotypes. Therefore, improving one aspect of visual-linguistic alignment does not necessarily resolve the other. Designing more advanced methods that holistically address both \emph{comprehension} and \emph{production} biases is essential for fair multimodal generation.

\vspace{-1em}
\paragraph{Generalization to other models.}

Table~\ref{tab:tokenflow_comparison} shows the results of applying similar finetuning strategies to \textbf{TokenFlow}. While TokenFlow initially exhibits a high gender bias (0.84), simple interventions such as \emph{prompting engineering} reduce its bias to 0.69.  \emph{Finetuning} approach further lowers it to 0.55, at a minor cost in semantics scores. These findings suggest that finetuning can be extended beyond VILA-U to other U-MLLM toward fairer image generation.

% \paragraph{Generalization to non-templated prompts.}

% \paragraph{Generalization to multi-face image generation.}
