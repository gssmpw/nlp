\section{Related Work}

\paragraph{Multimodal Generative Models}

Unified multimodal large language models (U-MLLMs) have advanced the start-of-the-art by bridging visual understanding and conditional generation capabilities. 
Compared to early MLLMs, which focus purely on understanding, such as Llava series \cite{liu2024visual,liu2023improvedllava}, these more recent works, represented by VILA-U \cite{wu2024vila}, Show-o~\cite{xie2024showo}, MetaMorph~\cite{tong2024metamorphmultimodalunderstandinggeneration}, TokenFlow~\cite{qu2024tokenflow}, Emu3~\cite{wang2024emu3}, TokenFusion \cite{zhou2024transfusion}, Janus \cite{wu2024janus,ma2024janusflow}, and etc. \cite{bachmann20244m,le2024diffusiongenerate,li2024dual}, highlight their effectiveness in generating high-quality visuals conditioned on text prompts.
U-MLLMs usually employ autoregressive paradigms that may inherit or amplify the biases embedded in their training data.
Existing studies predominantly focus on performance improvement rather than understanding and addressing the more critical issues such as demographic fairness. 


\paragraph{Fairness in Image Generation}

The social risks of biased image generation, particularly gender and racial disparities, have been extensively documented \cite{kotek2023gender,li2024culturellm,li2024culturepark}. 
Prior efforts in diffusion-based models attempted to mitigate bias by re-balancing training data and incorporating fairness objectives \cite{kim2024training}. 
However, these approaches are not directly transferable to UMLLMs due to architectural differences and tokenization mechanisms. 
Few studies explore bias sources within unified models or their downstream effects on generated outputs, leaving a gap in understanding the interaction between text and image modalities. 

