 
\vspace{-1em}
\section{Experiment}

\subsection{Experimental Setup}
\label{subsec:exp_setup}

% In this section, we list the experimental setups. First, we describe the datasets used for training and evaluation, the U-MLLMs for comparison, and the overall metrics used for measuring bias and image quality. Our evaluation setup follows the same metrics and measurements as previous research\cite{shen2024finetuningtexttoimagediffusionmodels}, ensuring comparability across different models.

% \subsection{Datasets}

\paragraph{Models.}
In our study, we evaluate the generation bias with respect to gender and race for latest U-MLLMs that can both understand and generate visual content. The models we considered include VILA-U\cite{wu2024vila}, TokenFlow\cite{qu2024tokenflow}, Emu3\cite{wang2024emu3}, Janus\cite{wu2024janus}, Show-o\cite{xie2024showo}, Janus-Pro\cite{chen2025januspro}. Since VILA-U was the only model with available training code when we began this study, we fine-tuned it into various variants for comparison. For VILA-U, we compare its variants: \textbf{\emph{(1). }Prompt engineering}: original VILA-U model with a modified prompt to explicitly ask for diversity in image generation. \textbf{\emph{(2). }Understanding Finetuning(\textsf{I~$\rightarrow$~T})}: VILA-U finetuned on the dataset in \emph{image-to-text} fashion.  \textbf{\emph{(3). }Generation Finetuning(\textsf{T~$\rightarrow$~I})}: VILA-U finetuned on the same balanced data in \emph{text-to-image} fashion.\textbf{\emph{(4). }Balanced Preference Loss}: VILA-U finetuned using our proposed balanced preference optimization for visual generation. (Sec. \ref{sec:method}).
\vspace{-1.5em}
\paragraph{Evaluation Data.}
For evaluation, we collect a set of occupation-based generation prompts (e.g. ``nurse,'' ``data engineer,'' ``senator'') that have been used in previous studies\cite{shen2024finetuningtexttoimagediffusionmodels} to assess diffusion models' biases concerning gender and race. These prompts are 50 in size and are publicly available. We utilized these prompts to evaluate various U-MLLMs under consistent conditions: We generate $N = 160$ images for each test prompt and use an attribute classifier\cite{shen2024finetuningtexttoimagediffusionmodels} to determine the demographic attribute labels.
\vspace{-1.5em}

\paragraph{Finetuning Data.}
For training, we collected a separate set of occupation-based generation prompts\cite{shen2024finetuningtexttoimagediffusionmodels} (e.g., ``scientist,'' ``firefighter'') and generated \emph{synthetic} images using a diffusion model, as described in Section~\ref{sec:method}. Our dataset comprises 1,000 training prompts, resulting in 10,000 images per demographic group. We consider six demographic groups: male, female, White, Black, Asian, and Indian. Our training dataset consists of 10,000 samples, each containing a text prompt and six images from different demographic groups.
 
\textbf{Metrics and Evaluation} We follow prior study\cite{shen2024finetuningtexttoimagediffusionmodels}, the same \textbf{bias metrics} and \textbf{image quality metrics} are adapted:
\begin{itemize}[leftmargin=*]
    \vspace{-1em}
    \item \textbf{Bias Metrics}: Representation Disparity (RD) 
    measures how often each demographic attribute appears in generated outputs given a neutral prompt (e.g., ``CEO''). We leverage the classifier model provided by prior study to predict the attributes from generated images. RD is calculated as:
    \[
            \mathrm{bias}(\mathbf{P}) 
            \;=\; 
            \frac{1}{K(K-1)/2} \sum_{i,j \in [K] : i < j} \bigl|\mathrm{freq}(i) - \mathrm{freq}(j)\bigr|,
    \]
    where $\mathrm{freq}(i)$ is the frequency of demographic $d_i$ in the image generation samples. A lower RD indicates less biased and more balanced demographic representation.
    \vspace{-1em}
    \item \textbf{Image Quality Metrics}: we utilized a range of metrics for image quality.  \textbf{\emph{(1)}. CLIP Score}
    Measures alignment between generated image and textual prompt. Higher scores indicate better semantic matching.
\textbf{\emph{(2)}. CLIP IQA}:
    Image quality assessment using CLIP's visual-textual understanding. Evaluates aspects such as sharpness without reference images.
 \textbf{\emph{(3)}. Inception Score}:
    Metric used for evaluating quality and diversity of generated images. Higher scores indicate better quality.
\end{itemize}
\vspace{-1em}
\paragraph{Evaluation Protocol.}
First, we generate $N = 160$ images for each test prompt. Next, we apply a demographic classifier from a previous study \cite{shen2024finetuningtexttoimagediffusionmodels} to predict the labels of demographic attributes for each image. These labeled attributes are then used to compute the overall bias and semantic score based on the above metrics. 
\vspace{-1em}
\paragraph{Training Procedure.} \emph{Base Learning Rate}: We start with a fixed learning rate (for example, $1\times10^{-4}$) for fine-tuning. \emph{Batch Size}: Ranges from 8 to 32, depending on the setup. \emph{Number of Steps}: For the first stage, we fine-tune for up to 10 epochs,  checking bias and quality after the training.  For the second stage, the epoch is chosen to be 1 to 2. We use the LoRA\cite{hu2021loralowrankadaptationlarge} method for finetuning; the rank is 32 for all experiments. 

% \paragraph{Hardware Setup.}
% All experiments are conducted on machines with GPUs (e.g., NVIDIA A100). The training duration depends on the size of the finetuning dataset, typically ranging from a few hours to a day.

% \medskip
% In the next section, we present the experimental results and analysis of the bias mitigation achieved by various approaches, as well as comparisons with baseline models.
