\vspace{-3em}
\section{Introduction}
\label{sec:intro}
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.47\textwidth]{figs_tables/fairness_in_MLLMs.pdf}
    \vspace{-0.5cm}
    \caption{U-MLLMs are capable of generating images with high quality, but the generation lacks diversity. The model has a bias for some occupations. In this example, given the prompt ``construction worker'', the model generate most of images with demographic attribute as ``male'' and ``white''.}
    \label{fig:fairness-mllms}
    \vspace{-0.75cm}
\end{figure}


Multimodal Large language models (MLLMs) have demonstrated remarkable capabilities in visual understanding \cite{li2024llava, wang2024qwen2vlenhancingvisionlanguagemodels}. Recent research \cite{wu2024vila,  wu2024janus} has focused on extending MLLMs' capabilities to image generation settings, enabling them to produce textual content and \emph{visual} content. These \emph{unified} MLLMs (U-MLLMs), e.g., VILA-U \cite{wu2024vila}, present both visual understanding and generation capability, where they can not only understand the semantics in images but also generate images with high-quality conditioning on user prompts in natural language. However, these U-MLLMs with unified capabilities may inadvertently reproduce or amplify \emph{biases} at deployment, including gender and racial stereotypes embedded in their large-scale training data \cite{elazar2023s,chen2024catastrophic}. 

A common structure in U-MLLMs is an image \emph{tokenizer} that transforms images into discrete tokens via an \emph{encoder-decoder} framework \cite{esser2021taming}. Specifically, the vision encoder compresses the input image into latent embeddings and then quantizes them into discrete tokens. Afterward, a decoder reconstructs the image from these tokens. This discrete tokenization bridges textual and visual modalities by analogizing image tokens to words in a sentence, enabling a single autoregressive objective that unifies text and image generation. While this design has proven effective for quality and scalability, it also opens additional channels for bias from the tokenizer.


Existing work on debiasing in image generation has highlighted the social risks posed by skewed output distributions, and various methods have been proposed to reduce bias in image generation \cite{chuang2023debiasingvisionlanguagemodelsbiased,bansal2022texttoimagegenerativemodelsunderstand,wang2024conceptalgebrascorebasedtextcontrolled,gandikota2024unifiedconcepteditingdiffusion}. 
Nevertheless, many such methods are designed specifically for diffusion models, which leverage different principles for image generation \cite{shen2024finetuningtexttoimagediffusionmodels, gandikota2024unifiedconcepteditingdiffusion}. 
As U-MLLMs with autoregressive image generation capabilities become more and more prevalent, it is imperative to evaluate their bias in image generation and develop new methods to reduce bias in these token-based generation models. Moreover, it remains an open question whether the generation biases emerge more from the vision encoder, which generates image tokens, or from the language modeling component, which generates image tokens according to the given text prompt.


This paper investigates and mitigates demographic bias in U-MLLMs for \emph{text-to-image} generation. Specifically, we made the first step to study gender and race bias for U-MLLMs. We benchmark the latest U-MLLM on gender and race bias using datasets and metrics introduced by a recent study of image generation fairness \cite{shen2024finetuningtexttoimagediffusionmodels}. 
These models comprehensively include VILA-U \cite{wu2024vila}, Show-o \cite{xie2024showo}, Janus \cite{wu2024janus}, Janus-Pro \cite{chen2025januspro}, TokenFlow \cite{qu2024tokenflow}, Emu3 \cite{wang2024emu3}. 
Our results show that the latest UMLLMs exhibit notable gender and race biases in image generation (see an example in \autoref{fig:fairness-mllms}). Next, we conduct a detailed audit of the vision encoder/decoder and the language model component to localize these biases' source(s), where we find that the biases are mainly from the language model. Finally, we synthesize high-quality training data with a balanced demographic distribution and propose a novel balance preference loss to mitigate generation bias in U-MLLMs, inspired by recent research on direct preference optimization \cite{rafailov2024direct, hong2024orpomonolithicpreferenceoptimization}. 


Through extensive experiments with various U-MLLMs, we demonstrate that our approach significantly reduces biases (e.g., over-generation of certain genders or races) without sacrificing the quality of image generation output. For example, for the VILA-U model, our method reduces its gender bias by 71.91\% and increases the inception score by 12.2\%. In summary, our key contributions are:
\vspace{-1em}
\begin{itemize}[leftmargin=*]
    \item \textbf{Benchmarking Bias}: We benchmark the latest U-MLLMs on race and gender bias and find that they often display it with different degrees for image generation. Notably, Janus-Pro, the latest U-MLLM, witnessed the worst gender bias with a value of 0.90, compared with the stable diffusion model with a bias value of 0.67.
    \item \textbf{Localizing Bias}: We inspect different components in the VILA-U model (vision encoder, language model) by using methods such as linear probing in image embedding space to pinpoint potential sources of bias and find that the bias is mostly from the language model component.
    \item  \textbf{Mitigating Bias}: We used the diffusion model to synthesize training data with a balanced demographic distribution. We also introduce a balanced preference loss inspired by direct preference optimization, with the objective of balancing the likelihood of visual generation towards the different demographic groups. We empirically show that our approach yields substantial improvement in demographic fairness while preserving the quality of image generation, thus providing a practical framework for developing unified MLLMs with more fairness.
\end{itemize}