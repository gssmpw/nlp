\vspace{-1em}
\subsection{Experimental Results}
\label{sec:exp_results}

\input{figs_tables/tab_gender_bias}

Table~\ref{table:gender_race_bias} summarizes the results for generation bias evaluation on a range of U-MLLMs and especially the VILA-U model with a few debiasing strategies applied. We measure \emph{gender} bias, \emph{race} bias, and their intersection (\emph{G.$\times$R.}), together with semantics-preservation metrics. Lower bias scores indicate more fairness, while higher semantics scores indicate more fidelity in the generation.
\vspace{-1em}
\paragraph{Baseline Models.} As shown in Table~\ref{table:gender_race_bias}, Stable Diffusion exhibits moderate gender bias (0.67) and race bias (0.42). Among U-MLLMs, Janus-series (Janus, Janus-Pro) show higher overall bias in gender (0.87–0.90), as well as high race and G×R scores (0.43–0.48 and 0.23–0.24). Show-o and TokenFlow both witness slightly lower bias. Compared with other U-MLLMs, Emu3 stands out with low gender (0.83) and G×R (0.22) biases, along with good semantic scores. Finally, VILA-U exhibits strong text–image alignment (CLIP-S of 28.24) but has high gender (0.89) and intersectional (0.24) bias, highlighting the need for reducing bias in U-MLLMs.
\vspace{-1em}
\paragraph{Debiasing Gender.} Table~\ref{table:gender_race_bias} presents four methods to reduce gender bias in VILA-U. 
\emph{Prompt Engineering} witness moderate bias reduction (Gender = 0.56) while preserving high semantic alignment (CLIP-S = 28.51). 
\emph{Finetune (I~$\rightarrow$~T)} achieved the smallest reduction in gender bias (0.83) even though it witnessed a little increase in semantics (CLIP-S = 28.49). 
By contrast, \emph{Finetune (T~$\rightarrow$~I)} drastically lowers the gender bias to 0.27. However, it came with the side effect that image quality is lower (CLIP-IQA = 0.77). 
Finally, \emph{BPO} (Balanced Preference Optimization) further lowers gender bias (0.25) while preserving reasonable visual fidelity (Inception = 2.10), indicating that our approach is effective for mitigating stereotypical associations without compromising generation quality for U-MLLMs.
\vspace{-1em}
\paragraph{Debiasing Race.}
Table~\ref{table:gender_race_bias} shows the results for mitigating race bias in the VILA-U. 
\emph{Prompt Engineering} achieves a moderate bias reduction (Race = 0.49) while retaining a high level of semantic alignment (CLIP-S = 28.51). 
\emph{Finetune (I~$\rightarrow$~T)} yields Race = 0.44 and slightly lower semantic scores (CLIP-S = 28.14). 
By contrast, \emph{Finetune (T~$\rightarrow$~I)} significantly reduces the race bias to 0.23, albeit with some drop in semantic fidelity (Inception = 1.98). \emph{BPO} also lower the race bias a lot(0.26), and achieved good semantic fidelity (Inception 2.31) in the meantime.
\vspace{-1em}
\paragraph{Debiasing Intersectional Attributes (G$\times$R).}
As shown under \emph{Debias: G$\times$R} in Table~\ref{table:gender_race_bias}, we evaluate methods targeting \emph{both} gender and race simultaneously. 
\emph{Prompt Engineering} achieves a moderately low intersectional bias (G$\times$R = 0.18) while preserving acceptable semantic fidelity (CLIP-S = 28.09). 
\emph{Finetune (I~$\rightarrow$~T)} reduces intersectional bias to 0.23, accompanied by strong semantic alignment (CLIP-S = 28.34), whereas 
\emph{Finetune (T~$\rightarrow$~I)} further lowers G$\times$R to 0.17 but at the cost of slightly reduced image quality (Inception = 1.91). \emph{BPO} achieved the lowest bias score in gender-race intersection as 0.15. It also achieved high semantic fidelity with an Inception score of 2.06.
