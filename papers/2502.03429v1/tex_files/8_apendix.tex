\section{Structure of U-MLLM}
This section represents the structure of U-MLLM and visual tokenizer; all the content in this section is from prior study\cite{wu2024vila}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figs_tables/overview.pdf}
    \caption{\textbf{Overview of framework’s multi-modal training and inference process\cite{wu2024vila}} Visual inputs are converted into discrete tokens and merged with textual tokens to create a unified multi-modal token sequence. This sequence is used in next-token prediction process, which supports a unified training objective. During inference, output tokens are processed through either text detokenizer or vision tower decoder, generating multi-modal content outputs\cite{wu2024vila}.}
    \label{fig:unified_model}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/vision_tower.pdf}
    \caption{
    \looseness=-1
    \textbf{Overview of unified foundation vision tower\cite{wu2024vila}} Input images are processed by the vision encoder, where features are extracted and discretized using residual quantization. These discrete vision features are then utilized in two ways: they are fed into the vision decoder to reconstruct images and are used to perform text-image alignment. Throughout this process, both the reconstruction loss and contrastive loss are calculated to refine the vision tower, enabling it to generate discrete visual features that are aligned with text\cite{wu2024vila}.}
    \label{fig:vision_tower}
\end{figure}

\newpage
 
\section{Localizing Bias}
\label{subsec:localizing-bias}
\begin{figure*}[ht]
    \centering
    \vspace{-8pt}
    \includegraphics[width=0.93\textwidth]{figs_tables/locating_bias.pdf}
    % \vspace{-15pt}
    \caption{
    \looseness=-1
    \textbf{Detecting bias in LM(top), Vision encoder(bottom);}
    \label{fig:locating_bias}}
    \vspace{-10pt}
\end{figure*}


\section{BPO Algorithm}
\begin{algorithm}[ht]
\caption{Balanced Preference Optimization}
\label{alg:balanced_finetuning}
\textbf{Input}: U-MLLM with parameters $\theta_0$; SFT dataset $\mathcal{D}_{\text{SFT}} = \{(x_i, z_i)\}$ of prompts $x_i$ and image tokens $z_i$; Balanced dataset $\mathcal{D}_{\text{bal}} = \{(x_j, y_{j_1}, \ldots, y_{j_K})\}$, each $x_j$ with multiple demographic variants; Trade-off parameter $\lambda$, total training epochs $N_1, N_2$;
\textbf{Output}: Debiased model parameters $\theta$

\begin{algorithmic}[1]
\STATE \textbf{Stage 1: Supervised Finetuning}
\STATE \quad Initialize $\theta \leftarrow \theta_0$
\FOR{epoch $= 1$ to $N_1$}
    \STATE Sample a minibatch $\{(x_i, z_i)\}$ from $\mathcal{D}_{\text{SFT}}$
    \STATE $\mathcal{L}_{\mathrm{NLL}}(\theta)\;=\; -\sum_{(x_i,z_i)}\log P_\theta(z_i \mid x_i)$
    \STATE Update $\theta \leftarrow \theta - \eta \nabla_{\theta}\,\mathcal{L}_{\mathrm{NLL}}(\theta)$
\ENDFOR

\STATE \textbf{Stage 2: Balanced Preference Optimization}
\FOR{epoch $= 1$ to $N_2$}
    \STATE Sample a minibatch $\{(x_j, y_{j_1},\ldots,y_{j_K})\}$ from $\mathcal{D}_{\text{bal}}$
    \STATE \textbf{Balanced Preference Loss:} 
    \[
       \mathcal{L}_{\mathrm{bal}}(\theta)
       =
       \sum_{k\neq l}
        \mathcal{L}_{\mathrm{bal}}^{(d_{k-1},d_k)}(\theta).
    \]
    \STATE Update $\theta \leftarrow \theta - \eta \nabla_{\theta}\,\mathcal{L}_{\mathrm{bal}}(\theta)$
\ENDFOR
\STATE \textbf{Return} $\theta$
\end{algorithmic}
\end{algorithm}
\newpage
\section{Related Work(continued)}
\label{sec:related_work_continue}
\paragraph{Preference Optimization}  
Direct preference optimization (DPO) \cite{rafailov2024direct} has emerged as a promising technique to address biases in machine learning models, especially LLM. Since then, numerous new loss functions have been proposed~\cite{meng2024simposimplepreferenceoptimization, park2024disentanglinglengthqualitydirect, hong2024orpomonolithicpreferenceoptimization, ethayarajh2024ktomodelalignmentprospect, azar2023generaltheoreticalparadigmunderstand}.
Recent advances \cite{amini2024direct} integrate preference modeling into loss functions to guide models toward balanced outputs, which have rarely been explored for MLLMs.
Based on this, we introduce a novel balanced preference loss tailored for U-MLLMs.
By leveraging demographic attributes during training, the proposed method balances the likelihood of generating outputs across groups without compromising the image quality.


\section{Results for Debiasing TokenFlow}

\begin{table}[ht]
    \centering
    \small % Reduce font size
    \setlength{\tabcolsep}{2pt} % Reduce column spacing
    \vspace{-1.5em}
    \caption{Comparison of Gender Bias and Semantics Preservation Metrics for TokenFlow model.}
    \label{tab:tokenflow_comparison}
    \begin{tabular}{l|c|ccc}
        \toprule
        \textbf{Method} & \textbf{Gender Bias ↓} & \textbf{CLIP-S} & \textbf{CLIP-IQA} & \textbf{Inception} \\
        \midrule
        TokenFlow & 0.84 & 27.17 & 0.84 & 2.34 \\
        Prompting & 0.69 & 27.46 & 0.80 & 2.34 \\
        Finetune & 0.55 & 26.87 & 0.81 & 1.98 \\
        % BPO & 0.55 & 26.87 & 0.81 & 1.98 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Limitation}
\label{sec:limitation}
\paragraph{Limitation}
Several questions remain open despite the bias reduction achieved by our approach. First, our study primarily centered on \emph{overt} demographic categories (e.g., gender, race). Real-world scenarios may demand addressing \emph{intersectional} or \emph{nuanced} attributes (e.g., age, culture, or religion). Second, many models are not fully open-source, restricting the scope of our evaluations to two publicly available systems. Future research could broaden the range of tested models. Lastly, due to resource constraints, we did not explore alternative preference optimization objectives beyond our current framework. Building on our method to incorporate other debiasing approaches is a promising direction for future work. 


\newpage
\section{Samples from pretrained baseline models}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/prompt_3_vila_u.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by VILA-U.}
    \label{fig:vila-u}}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/prompt_3_janus.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by Janus}
    \label{fig:janus}}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/prompt_3_janus_pro.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by Janus-Pro}
    \label{fig:janus-pro}}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/promtp_3_showo.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by Show-o}
    \label{fig:show-o}}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/prompt_3_token_flow.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by TokenFlow}
    \label{fig:tokenflow}}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.93\textwidth]{figs_tables/prompt_3_emu3.pdf}
    \caption{
    \looseness=-1
    \textbf{Given the test prompt: “a photo of the face of a electrical and electronics repairer, a person”, images generated by Emu3}
    \label{fig:emu3}}
\end{figure}