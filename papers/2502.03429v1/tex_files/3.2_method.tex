\vspace{-1em}
\section{Method}
\label{sec:method}

To effectively reduce bias in U-MLLM, a training dataset that \emph{explicitly} includes diverse demographic attributes is of importance. In general, web-scale data under-represents certain groups or inherently skews toward certain stereotypes. By synthesizing balanced data—where each demographic group appears in similar proportion- we can give the model a stronger signal to reduce its biases during training. This can ensure that the U-MLLM is trained on a \emph{controlled distribution} counteracts the bias present in real-world data distribution.

\subsection{Training Data Generation}
\label{subsection: training data generation}

We leveraged diffusion model FLUX.1-dev\cite{flux2023} to synthesize our training data as follows:
\vspace{-1em}
\begin{enumerate}[leftmargin=1em]
    \item \textbf{Base Prompt Selection.}  
    We begin by collecting a set of \emph{base prompts} $x_{neutral}$ (for example, ``a portrait of a \{occupation\}''), where occupations are drawn from a publicly available dataset\cite{shen2024finetuningtexttoimagediffusionmodels}. 
    
    \item \textbf{Demographic Augmentation.}  
    For each base prompt $x_{neutral}$, we augment it with demographic attributes $d \in \{d_1, d_2, \dots\}$, resulting in \emph{augmented prompts} $x_{\text{aug}}(d_i)$. For example, starting with the base prompt ``a photo of a nurse'' we generate augmented prompts such as ``a photo of a male nurse'' and ``a photo of an Asian nurse.''  
    
    \item \textbf{Image Generation via Diffusion Model.}  
    For each $x_{\text{aug}}(d_i)$, we feed it into FLUX.1-dev\cite{flux2023} model to obatin a set of images $\{y_1, y_2, \dots\}$. This step ensures diversity in the visual representations of each demographic group.
    
    \item \textbf{Paired Dataset Creation.}  
    To create the dataset, we pair each generated set of images $y_k$ with its corresponding base prompt $x_{neutral}$. Before pairing, we use the ChatGPT-4o model to paraphrase the base prompt to introduce linguistic diversity while maintaining neutrality. For instance, the base prompt ``a photo of a nurse'' can be paraphrased as ``a photo of an individual in the nursing profession.'' The resulting synthetic dataset pairs each neutral prompt with a set of images, where each image corresponds to a specific demographic group:
    \[
        \mathcal{D}_{\mathrm{syn}} 
        \;=\; \bigl\{\,(x_{\mathrm{neutral}},\, y_{d_1},..., y_{d_K})\,\bigr\}.
    \]
\end{enumerate}

Because we explicitly injected demographic attributes into the prompts, $\mathcal{D}_{\mathrm{syn}}$ spans multiple genders, ethnicities, and roles—offering more balanced coverage than typical real-world data.

\subsection{Balanced Preference Loss}

\begin{figure*}[ht]
    \centering
    \vspace{-0.25cm}
    \includegraphics[width=0.8\textwidth]{figs_tables/balance_preference_optimization.pdf}
    \vspace{-0.5cm}
    \caption{The optimization objective is to minimize the deviation of preference between different demographic group.}
    \vspace{-0.5cm}
    \label{fig:bpo-loss}
\end{figure*}

% \paragraph{Autoregressive Likelihood.}  
% We denote the probability of generating a token sequence $z = \{z_1,\dots,z_T\}$ given prompt $x$ by:
% \begin{equation}
%     P_\theta(z \mid x) \;=\; \prod_{t=1}^{T} P_\theta\bigl(z_t \,\big|\; z_{<t},\, x\bigr).
%     \label{eq:autoregressive-prob}
% \end{equation}
% Typical training optimizes the negative log-likelihood:
% \begin{equation}
%     \mathcal{L}_{\mathrm{NLL}}(\theta) 
%     \;=\; 
%     - \sum_{(x,z)\,\in\,\mathcal{D}} \log P_\theta(z \mid x),
% \end{equation}
% where $\mathcal{D}$ is a dataset of (prompt, image) pairs, and $z$ is the discrete token sequence encoding the image generated by the image encoder.

% We now define a \emph{balanced preference loss} to encourage the model to generate image samples with evenly distributed demographic attributes. 


Existing preference-optimization approaches focus on \emph{aligning} the model’s response to \emph{user-specified} preferences (e.g., “prefer polite over rude text”). By contrast, in this study we seek \emph{balance} among demographic attributes---for instance, the model should equally generate \{female, male\} or \{Asian, Black, Indian, White\} under neutral prompts, rather than overwhelmingly generate one demographic group. This shift in perspective means we want the \textbf{absolute difference in preference} across demographics to be \emph{minimized}, rather than pushing a single demographic to be more or less likely in isolation. To encourage equal representation, as shown in \autoref{fig:bpo-loss}, we aim to penalize deviations by minimizing
\begin{equation}
\min ||p(y_{d_i} \succ y_{d_j} \mid x) -
p(y_{d_j} \succ y_{d_i} \mid x) ||
\end{equation}

In other words, if the model is more likely to generate $y_{d_1}$ (image with demographic label $d_1$) than $y_{d_j}$ (image with demographic label $d_j$), it will be penalized, which encourages balance of the output distribution, and vice versa.

\paragraph{Definition} Concretely, let $y_{d_i}, y_{d_j}$ be images from different demographic groups. We define a \emph{balanced odds} penalty:
\begin{equation}
  \mathcal{L}_{\mathrm{bal}}(\theta)
  \;=\;
  \log\left[ 1 + \left( 
    \sigma\!\Biggl(
      \log\text{OR}_\theta(y_{d_i}, y_{d_j}) 
    \Biggr) - \frac{1}{2} \right)^2 \right]
  \label{eq:bal_twogroups}
\end{equation}

Minimizing \eqref{eq:bal_twogroups} can lead to $p(y_{d_i} \succ y_{d_j} \mid x)$ closer to $p(y_{d_j} \succ y_{d_i} \mid x)$, thus encouraging the even distribution of generation among demographic attributes. 

\paragraph{Extension to Multiple Groups.}  
For race attributes, we have multiple demographic groups. Let's define $K$ to be the number of race categories. The auxiliary loss to penalize demographic odds of different attributes deviating from each other:
\begin{equation}
  \mathcal{L}_{\mathrm{bal}}(\theta)
  \;=\;
  \sum_{k=1}^{K}
  \mathcal{L}_{\mathrm{bal}}^{(d_{k},d_{(k+1) \% K})}(\theta).
  \label{eq:multigroup}
\end{equation}


\paragraph{Two-Stage Training} We adopt a straightforward two-stage procedure as illustrated in Algorithm  \ref{alg:balanced_finetuning}.
\begin{enumerate}[leftmargin=1em]
    \item \textbf{Supervised Finetuning (SFT).} 
    We begin by finetuning the U-MLLM on a supervised dataset of \{\emph{prompt}, \emph{image tokens}\} pairs to ensure high-quality image generation and semantic fidelity. This yields an SFT model $\pi_{\text{SFT}}$.
    \item \textbf{Balanced Preference Optimization.} 
    We then apply a reference-free odds-ratio penalty via Eq. \ref{eq:multigroup} to make the model equally prefer multiple demographic attributes.   
\end{enumerate}

Our approach \emph{directly} integrates demographic balance into the policy’s objective via~\autoref{eq:multigroup}. By including samples with multiple demographic attributes in the balanced dataset $\mathcal{D}_{\text{bal}}$, the model is enabled to generalize towards each demographic group. The balanced preference loss $\mathcal{L}_{\mathrm{bal}}$ then reduces the inter-difference among demographic groups for the model's preference. Overall, this two-stage method ensures the model maintains good generation quality (from Stage 1) while significantly reducing \emph{absolute preference} differences between demographic attributes (from Stage 2).

