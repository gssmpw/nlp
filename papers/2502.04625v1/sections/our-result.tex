\section{Reconstruction Results and Discussion} 
\label{sec:main-result}
We obtain our final reconstruction result by applying the method to all available data introduced in \S\ref{sec:dataset}.
Our reconstruction is based on individual characters, while existing results are based on phonological categories. 
A straightforward way to obtain category-centric result is averaging phonetic feature vectors of all characters belonging to the same phonological category. 
The nearest IPA phonemes to the averaged vectors can be directly used for comparison to previous manual results by philologists.
\footnote{A comprehensive summary of the result can be found at \url{https://github.com/LuoXiaoxi-cxq/Reconstruction-of-Middle-Chinese-via-Mixed-Integer-Optimization}.}

\subsection{Numerical Evaluation} \label{sec:eval-AMI}
%Characters within the same category have the same initial in their reconstruction, but not necessarily in ours. 
%Nevertheless 
The phonetic vectors resulted from our model should form clusters that align with phonological categories in \gy~to some extent.
Based on this assumption, we develop a clustering based method to evaluate the overall quality of a reconstruction result. 
We cluster the phonetic vectors with KMeans with a predefined number of clusters equal to 37\footnote{There are 38 categories in the manual categorial reconstruction of \gy. 
Our dataset excludes characters with the \ch{俟} category.}.
We then report the \textit{adjusted mutual information}  \citep[AMI;][]{AMI-2010}, an information-theoretic measure, between the automatic clustering and predefined phonological categories. 
%AMI measures the similarity between two clusterings. 
Given two clusterings $U$ and $V$, 
$$\mbox{AMI}(U,V)=\frac{\mbox{MI}(U, V) - \mathbb{E}[\mbox{MI}(U, V)]}{\mbox{avg}(\mbox{H}(U), \mbox{H}(V)) - \mathbb{E}[\mbox{MI}(U, V)]}$$
where $\mbox{H}$ is the Shannon entropy, $\mbox{MI}$ is the mutual information, and $\mathbb{E}$ is expectation.
Perfectly matched clusters yield an AMI of 1, while random cluster assignment yields 0.
The numbers of samples and clusters are not necessarily the same.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figure/AMI_our_vs_baseline.png}
    \caption{AMI values with different $\lambda$ compared to baseline results. 
    AMI values for seven individual dialects (BJ: Beijing, SZ: Suzhou, CS: Changsha, NC: Nanchang, MX: Meixian, GZ: Guangzhou, and XM: Xiamen) are presented, each representing one dialect group (see \S\ref{sec:dataset}).
    The single best dialect is Suzhou, with the highest AMI (0.7782). In feature-level majority vote, features are aggregated and voted individually. The KMeans algorithm is then applied to the voted feature vector, yielding an AMI of 0.6870.
    }
    \label{fig:AMI-our-baseline}
\end{figure}

Fig. \ref{fig:AMI-our-baseline} shows our results, along with two baselines (majority vote and single best dialect).
AMI is largely influenced by the value of $\lambda_{\text{fq}}$, indicating the effectiveness of \fq~in deriving categories. 


Since we are dealing with 20 dialects but only a single set of \fq~spellings, setting $\lambda_{\text{fq}}$ to 0.95 is a natural choice. 
However, since the information obtained from the dialects and \fq~lacks a common scale, it is difficult to make direct comparisons.
As a result, we cannot theoretically determine the optimal weighting for each information source, and the choice of $\lambda$ is therefore largely empirical. 

When $\lambda_{\text{fq}}$ is set to 0.95, our model achieves an AMI of 0.8148 and outperforms the baselines, indicating a high degree of similarity between the phonetic reconstruction by us and the manual phonological reconstruction by philologists. 
When $\lambda_{\text{fq}}$ is set to 0.99, the AMI increases to 0.8173, although the change is minimal.


In contrast to Table \ref{tab:synthesis-adjust-parameters}, where setting $\lambda_{\text{fq}}$ to 0.95 results in a decrease in equal rate, increasing $\lambda_{\text{fq}}$ actually improves AMI when applied to real data. This phenomenon reflects the difference in synthetic and real data, emphasizing the importance of adjusting $\lambda_{\text{fq}}$ based on specific situations.

It is worth noting that when $\lambda_{\text{fq}}$ is set to 0 (i.e., without using \fq), AMI drops to 0.5892. This further reflects the critical role of \fq~information when dealing with real data.



In some auxiliary experiments that are not reported in this paper, we also used hierarchical clustering to further study the impact of the number of clusters. Results show that the difference in AMI between hierarchical clustering and KMeans is within 0.05, regardless of the specific setting.

\subsection{Comparison to Existing Results} \label{sec:comparison}
Our model successfully reconstructs most categories with consensus among philologists, such as b\=ang 幫 p\=ang 滂 m\textipa{\'\i}ng 明 du\=an 端 t\`ou 透 n\textipa{\'\i} 泥\footnote{All the Chinese characters used in \S\ref{sec:comparison} are categorical labels representing initial categories in traditional Chinese phonology. For example, characters f\=ang 方, f\v u 府,  b\'o 博, b\textipa{\v\i} 彼, and many other characters are assumed to have the same initial in MC, and philologists use b\=ang 幫 to represent their common initials.}.
% For further details, see \citet{shen_2020} \S 1.4.}.
Similar to the computational operationalisation of the historical comparative approach to Indo-European languages \citep{list-etal-2022-new}, our study confirms the usefulness of computation in linguistic inquiry.  
%showing that our model at least has the basic ability.
The differences between different reconstruction results may provide new evidence for philologists and linguists to consider and therefore are useful too.
Such differences are mainly attributed to two factors.

%\paragraph{Lack of proper weights}
Different dialects changed in different directions. 
For example, it is generally believed that Wu dialects retained all the voiced stops, while most other dialects became devoiced \citep[pp.224--225]{huang2014handbook}.\footnote{For example, the character t\'ong 同 is believed to had a voiced initial d\textipa{\`\i}ng 定 in MC. In Wu dialect, its initial is [d], while in most other dialects, it is [t\textipa{\super{h}}].}
In phonology, devoicing refers to a sound change where a voiced consonant becomes voiceless due to the influence of its phonological environment. This process is common across many languages and is a part of Grimm's law. 
%In practical reconstruction, certain phenomena that only exists in one or two dialect groups can be more meaningful. 
Our current model, however, cannot differentiate in what aspects a dialect changed most and in what aspects it stayed constantly.
It treats different dialects with equal weight on different phenomena.
Consequently, a character's reconstructed initial tends to be closer with the phonetic value that is more commonly observed across various dialect pronunciations. 
For example, our model fails to reconstruct the `voiced' feature for categories that are assumed to be voiced by philologists, e.g. b\textipa{\`\i}ng 並 d\textipa{\`\i}ng 定 c\'ong 從 xi\'e 邪. 
%instead reconstructing them as voiceless, due to the loss of voicing as a distinctive feature in most dialects. 
Our model also has difficulty distinguishing the zh\textipa{\=\i} 知 zhu\=ang 莊 zh\=ang 章 groups, which have similar pronunciations in most modern Chinese dialects.

%\paragraph{Lack of information sources}
Our current model only contains the most basic information---\fq~spellings and modern Chinese varieties. 
Other types of information, including rhyme tables, e.g. Y\`unj\`ing 韻鏡, and seno-xenic\footnote{Sino-Xenic vocabularies are large-scale and systematic borrowings of the Chinese lexicon into the Japanese, Korean and Vietnamese languages. See \url{https://en.wikipedia.org/wiki/Sino-Xenic_vocabularies} for details.} pronunciations are not integrated into our model at present. 
Rhyme tables provide additional information about the voiced/voiceless feature of initials, which is crucial for philologists' manual reconstruction. 
It is another reason why our model fails to reconstruct voiced initials.

Because of the limitation in information sources, our model cannot provide definitive answers to some debatable problems, such as 
    whether categories n\'i 泥 and n\'iang 孃 are the same initial. 
It is generally believed that there is no distinction between the two categories in most modern Chinese varieties (\citealp[p.228]{Tseng-ni-niang}, \citealp[p.126]{lr-1956}).
Though \citet[pp.125--126]{lr-1956} and \citet[pp.98--101]{shaorongfen} have opposite opinions about this problem, they both used Sanskrit-Chinese pronunciations as the main evidence. 
However, in our model, with materials restricted to dialects, the reconstruction of n\'i 泥 and n\'iang 孃 appears similar.

\subsection{Extension}
In principle, our method can be generalized to other languages. However, in practice, our model requires phoneme-level alignment between each protoform's reflexes. 
For Chinese, this alignment occurs naturally, as each Chinese character typically corresponds to a morpheme, and morphemes are largely represented by single syllables that follow specific patterns, as described in \S\ref{sec:syllable-structure}.

Sound change is a central focus in linguistic research, and our model can engage with it in two ways.
First, incorporating common patterns of sound change as constraints into our model is a possible future direction. 
Second, by analyzing $F_{l}(X)-F_{\text{MC}}(X)$ in Eq. \ref{eq:objective}, we may identify potential sound changes in terms of distinctive features, such as devoicing.
