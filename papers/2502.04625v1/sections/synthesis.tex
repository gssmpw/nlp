\begin{table*}[thbp]
    \centering
    % Table generated by Excel2LaTeX from sheet 'authentic2'
    \scalebox{0.9}{
    \begin{tabular}{rrrrl}
    \toprule
    \multicolumn{1}{l}{\textbf{Example}} & \multicolumn{1}{l}{\textbf{I-feature $\tau(j)$}} & \multicolumn{1}{l}{\textbf{D-feature $j$}} & \multicolumn{1}{l}{\textbf{Valid Combinations}} & \textbf{Shortest Distance} \\
    \midrule
    \multicolumn{1}{l}{(1.048, 0.905)} & \multicolumn{1}{l}{sonority} & \multicolumn{1}{l}{delayed release} & \multicolumn{1}{l}{\textbf{(1, 1)}, (1, -1), (2/3/4/5, 0), (0, 0)} & 0.143 \\
    % \midrule
    \multicolumn{1}{l}{(0.946, -0.919)} & \multicolumn{1}{l}{labial} & \multicolumn{1}{l}{labiodental} & \multicolumn{1}{l}{(1, 1), \textbf{(1, -1)}, (-1, 0), (0, 0)} & 0.135 \\
    % \midrule
    \multicolumn{1}{l}{(0.499, 0.988)} & \multicolumn{1}{l}{coronal} & \multicolumn{1}{l}{anterior} & \multicolumn{1}{l}{\textbf{(1, 1)}, (1, -1), (-1, 0), (0, 0)} & 0.503 \\
    \multicolumn{1}{l}{(0.499, 0.499)} & \multicolumn{1}{l}{coronal} & \multicolumn{1}{l}{distributed} & \multicolumn{1}{l}{(1, 1), (1, -1), (-1, 0), \textbf{(0, 0)}} & 0.998 \\
    % \midrule
    \multicolumn{1}{l}{(0.992, 1.952)} & \multicolumn{1}{l}{dorsal} & \multicolumn{1}{l}{high} & \multicolumn{1}{l}{(-1, 0), (\textbf{1}, 1/\textbf{2}/3), (0, 0)} & 0.056 \\
    \multicolumn{1}{l}{(0.992, 2.889)} & \multicolumn{1}{l}{dorsal} & \multicolumn{1}{l}{front} & \multicolumn{1}{l}{(-1, 0), (\textbf{1}, 1/2/\textbf{3}), (0, 0)} & 0.119 \\
    \midrule
          &       &       & \textbf{Total Distance:} & \textbf{1.954} \\
    \bottomrule
    \end{tabular}%
    }
\caption{Demonstration of how to calculate the `total distance' from a reconstructed vector. The `Example' column contains all the ($\tau(j), j$) value pairs in a reconstructed vector. 
For each pair, we highlight the shortest $L_1$ distance between it and all valid combinations with bold font. 
`Total Distance' is the sum of all shortest distances over $j$.}
\label{tab:self-sound}
\end{table*}

\section{Validation Experiments on Synthetic Data}
\label{sec:synthesis}

%we hope to establish the reliability of our approach through three distinct aspects: synthetic data, held-out data, and comparison to manual categorical reconstruction. 
%However, it is important to note that our evaluation methods are still in need of further refinement and exploration.
Since language reconstruction lacks a definitive ground truth, it is challenging to discuss the `correctness' of any reconstruction result.
We validate our method on a wide range of synthetic datasets, which hopefully mirror diachronic phonetic change. 
Starting from a predefined consonant system, we create varieties of it by introducing systematic change and random noise. 
We then extract character--speller pairs to mimic the \fq~information.
In order to evaluate the effectiveness in reconstructing the predefined consonant system, we apply our model to the varieties as well as character--speller data.
The idea of experimentation with synthetic data has been utilized to simulate lexical semantic change (\citealp{rosenfeld-erk-2018-deep,shoemark-etal-2019-room}).

\subsection{Generating Synthetic Data} \label{sec:generate-synthesis}
Data synthesis is demonstrated in Figure \ref{fig:synthesis-flow}.
It consists of three steps, as explained as follows.

\paragraph{Step 1: Selecting/simulating a consonant system}
To generate the old stage initial system, we randomly sample an initial set $S_I=\{I_1, I_2, \dots, I_m\}$ from an IPA chart of consonants
\footnote{The IPA chart is based on Hayes's feature spreadsheet (\url{https://brucehayes.org/120a/index.htm\#features}). 
Diacritics [\textipa{\super h} \textipa{\super w} \textipa{\super j}] are additionally considered.
}
, namely $S_{\text{IPA}}$, with $m\in[35,40]$. For each initial $I_i (1 \leq i \leq m)$, we generate a set of characters $S_{C_i}=\{C_{i_1}, C_{i_2}, \dots, C_{i_{n_i}}\}$, with the number of characters $n_i$ falling within $[20,80]$. 

In addition to purely artificial consonant system, we also utilize 
%consonant systems in natural phonology (English, Germany, Mandarin) as well as 
modern English, German, Mandarin and reconstructed Latin\footnote{
See \url{https://en.wikipedia.org/wiki/English_phonology}, 
\url{https://en.wikipedia.org/wiki/Standard_German_phonology},
\url{https://en.wikipedia.org/wiki/Standard_Chinese_phonology}, and \url{https://en.wikipedia.org/wiki/Latin_phonology_and_orthography} respectively.}.

\paragraph{Step 2: Deriving character--speller pairs} 
Following the model of rhyme dictionaries, we assign an artificial \fq~ spelling to each character. 
Given that not all characters and their spellers share the same initial (\S\ref{sec:obj}), we introduce variability by randomly assigning a portion $p_{\text{fq}}$ of characters to have their upper spellers randomly selected from $S_I$.

% For a character $C_{ij}$ with initial $i$, its speller can be $C_{ij^{\prime}} (j^{\prime}\neq j)$ with probability $1-p_{\text{fq}}$, and be $C_{i^{\prime}k} (i^{\prime}\neq i)$ with probability $p_{\text{fq}}$.

\paragraph{Step 3: Generating variations} 
We generate 20 varieties based on $S_I$ and $S_{C_i}$s to simulate sound change, denoted as $S_I^v=\{I_1^v, I_2^v, \dots, I_m^v\}$ and $S_{C_i}^v=\{C_{i_1}^v, C_{i_2}^v, \dots, C_{i_{n_i}}^v\} (1\leq i \leq m, 1\leq v \leq 20)$.
We assume that most sound changes are regular, where the phonetic value of an initial influences all characters with that initial in a given variety. To simulate regular sound change, initial $I_i$ can change to any $I_i^v \in S_\text{IPA}$ in variety $v$ with probability $p_\text{dia}$.\\
Exceptions to regular change can occur due to various causes, e.g. lexical borrowing and grammatical analogy, and we model such irregular change by allowing the initial of character $C_{i_j}^v$ to change from $I_i^v$ to any consonant in $S_\text{IPA}$ with probability $p_\text{char}$.\\
Denote the $L_1$ distance between $I_a$ and $I_b$ (both in $S_\text{IPA}$) as $d_{I_a, I_b}$.

\begin{figure*}[t!]
  \centering
    \subfigure[$p_{\text{dia}}=0.3$]{
    \label{Fig:baseline-Latin-1}
    \includegraphics[width=0.49\linewidth]{figure/baseline_pdia_03.png}}
    \subfigure[$p_{\text{dia}}=0.5$]{
    \label{Fig:baseline-Latin-2}
    \includegraphics[width=0.49\linewidth]{figure/baseline_pdia_05.png}}
    \subfigure[$p_{\text{dia}}=0.7$]{
    \label{Fig:baseline-Latin-3}
    \includegraphics[width=0.49\linewidth]{figure/baseline_pdia_07.png}}
    \subfigure[$p_{\text{dia}}=0.9$]{
    \label{Fig:Fig:baseline-Latin-4}
    \includegraphics[width=0.49\linewidth]{figure/baseline_pdia_09.png}}
    \caption{Comparison between our results and baselines with synthetic data that starts from Latin consonant system, with respect to equal rate (ER) and average $L_1$ distance (avg $L_1$).}
  \label{Fig:baseline-Latin}
\end{figure*}

\subsection{Experimental Setup} \label{sec:synthesis-setup}
We use the Gurobi\footnote{\url{https://www.gurobi.com}} MIP solver for our empirical investigation.
We set \texttt{MIPGap} to 1e-4, and the \texttt{TimeLimit} to 8 hours as the maximal time for calculation.%\footnote{See \url{https://www.gurobi.com/documentation/current/refman/mipgap2.html} and \url{https://www.gurobi.com/documentation/current/refman/timelimit.html}} 
Notably, the obtained solutions usually are not optimal. 
Nevertheless, they are of relatively good quality to verify the reliability of our model.

% There are 5 hyperparameters: $p_{\text{fq}}$, $p_{\text{dia}}$, $p_{\text{char}}$, $\mu$, $\sigma$.

We consider the following three metrics to evaluate the goodness of a reconstruction result, when ground-truth is available.
The ground-truth consonant system of the experiments in this section is either stochastically sampled from IPA, the reconstructed Latin, or modern English, German and Mandarin.

\paragraph{Average $L_1$} Since we represent phonemes by vectors, a straightforward way to evaluate the goodness of reconstruction is to calculate the overall distance between reconstructed vectors and their corresponding ground-truth vectors. 
To this end, we report the average $L_1$ distance. % between reconstructed initials and their predefined values.

\paragraph{Equal rate} A more strict evaluation metric is to reward only when the reconstructed vector is extremely close to its ground-truth. 
Here, we consider a phoneme as successfully reconstructed only when the $L_1$ distance between the reconstructed vector and its predefined value is smaller than $10^{-4}$,  
Accordingly, we report  the proportion of successfully constructed initials as \textbf{equal rate}.


\paragraph{Soundness of phonetic feature vector} 
The reconstructed results should be valid phonemes that satisfy the constraints on D-features listed in \S\ref{sec:restriction}. 
Our features are continuous, and we thus propose to measure their deviation from the constraints rather than classifying them as strictly `valid' or `invalid'.
For each D-feature $j$ and its corresponding I-feature $\tau(j)$, we consider the shortest $L_1$ distance between our result and all the valid values of $(j, \tau(j))$.
Table \ref{tab:self-sound} serves as an example, listing all $(j, \tau(j))$ pairs and their valid combinations. 
% Each pair corresponds to a shortest distance, and the total distance can measure the 
We report \textbf{sound rate}---the proportion of characters with a total distance less than $10^{-4}$.

% \paragraph{Violation rate of \fq} To assess the extent to which \fq~information is utilized in the reconstructed vector, we consider a character-speller pair to violate \fq~information if the $L_1$ distance between them is larger than $10^{-4}$. The proportion of such violated pairs among all character-speller pairs is reported as the \textbf{Violation Rate of \fq}.


\subsection{Results and Analysis} \label{sec:synthesis-result-analysis}
%We report the experimental results on the synthesised character--speller pairs and varieties. 
Our main results are shown in Figure \ref{Fig:baseline-Latin}, where we compare our results with two baselines. Since the phenomenon of characters having different initials from their upper spellers is not common in real data, we set $p_{\text{fq}}=0.1$.

We report two versions of majority vote results as baseline: IPA-level and feature-level. Considering the randomness in generating consonant systems and their variations, we conduct the experiment three times and report the average for each setting. In the IPA-level majority vote, for each character, we select the most frequent IPA phoneme from all 20 dialects to reconstruct its initial. For feature-level voting, we choose the most frequent value for each feature of each character. 

The IPA-level majority vote achieves the highest equal rate when $p_{\text{fq}}$ and $p_{\text{char}}$ are small, but its equal rate declines rapidly as randomness increases. In contrast, the feature-level majority vote performs better under high $p_{\text{fq}}$ and $p_{\text{char}}$ settings. 
Compared with the baselines, our model significantly outperforms both in terms of equal rate and average $L_1$ distance across most settings, particularly when $p_{\text{fq}}$ and $p_{\text{char}}$ are large, which highlights the robustness of our model.

%Notably, the ratios of sound change we set are not unreasonably high. 
To estimate a proper value of the change rate $p_{\text{dia}}$ is challenging.
We use the following geometric method to estimate a lower bound of $p_{\text{dia}}$ from the ancestral form (MC) to modern dialects based on the differences among all modern dialects. 
First, we measure how different any two dialects are by calculating the percentage of characters with different initial pronunciation. For example, this proportion between Beijing and Guangzhou is 65.70\%. 
Intuitively, halving such a difference degree gives a lower bound though the estimation based on any single pair of dialects should be far from being tight. 
We then leverage the concept of high-dimensional sphere to integrate all possible pairs of dialects.
The key idea is as follows. 
Each dialect is viewed as a point in a high-dimensional space and the percentage of characters with different pronunciation of initials is viewed as the distance between the corresponding pair of dialects. 
It is easy to see that such a distance measurement satisfies the triangle inequality. 
%Although the `distance' defined here lacks mathematical foundation, we verified the triangle inequality for every triple of dialects and found that it holds for each case.
The radius of the minimal high-dimensional sphere that covers all dialects serves as a (loose) lower bound. 
Based on the data from \citeauthor{zihui}, 
we first convert the distance matrix into coordinates using the algorithm proposed by \citet{calculation-of-coordinates}, 
% yielding a 20-dimensional coordinate representation for each dialect. 
then apply the algorithm proposed by \citet{Smallest-Enclosing-Ball} to determine the radius of the minimal sphere, obtaining an empirical value of 0.4180. 
The maximum of such lower bound is 0.8844, when any two dialects have totally different pronunciation (in other words, the distance is 1).
The empirical result suggests to utilise a high ratio of sound change. 


% We have calculated the proportion of characters with different initials across all pairs of the 20 dialects. For example, the proportion of characters with different initials between Beijing and Jinan is 17.89\%, between Beijing and Yangzhou is 36.02\%, and between Beijing and Guangzhou is 65.70\%. We interpret this proportion as a `distance' between dialects, and our goal is to find the smallest radius of a sphere that covers all points. It is important to note that the `distance' lacks mathematical foundation, but we verified the triangle inequality for every triple of dialects and found that it holds for each case.

% Our approach consists of two steps. First, we convert the distance matrix into coordinates. We apply the algorithm proposed by \citet{calculation-of-coordinates} on the distance matrix, and obtain a 20-dimensional coordinate representation for each dialect. 
% Then, We utilize the algorithm proposed by \citet{Smallest-Enclosing-Ball} to find the smallest enclosing sphere of the 20 dialects, yielding a radius of 0.4180 in this scenario. 
% For comparison, we also compute the smallest radius inthe most extreme case, where the distance between any two dialects is set to 1, and the result is 0.8844.


Since the vectors derived from feature-level voting do not necessarily correspond to valid phonemes, we compare its sound rate with that of our model. Although our model maintains a high sound rate across all settings, we perform a two-proportion z-test to determine whether the difference between our model and the feature-level majority vote in terms of SR is statistically significant. The null hypothesis is that the SR of our method is equal to that of the baseline. The results are reported in Table \ref{tab:hypo-test}. If we test this hypothesis at a significance level of 95\%, for all settings except (0.5, 0.9), the statistic $z$
% $$z=\frac{|p_1-p_2|}{\sqrt{\hat{p}(1-\hat{p})(1/n_1+1/n_2)}}, \ \hat{p}=\frac{n_1 p_1+n_2 p_2}{n_1+n_2}$$
exceeds $z_{0.975}=1.96$, and we can reject the null hypothesis. For the (0.5, 0.9) setting, $z=1.7152$ is still larger than $z_{0.95}=1.65$. 
Therefore, we conclude that our model performs better than the baselines in terms of sound rate.

\begin{table}[htbp]
  \centering
  \scalebox{0.9}{
    \begin{tabular}{cccccc}
    \toprule
    \textbf{Setting} & \textbf{SR$_1$} & \textbf{$n_1$} & \textbf{SR$_2$} & \textbf{$n_2$} & \textbf{stat $z$} \\
    \midrule
    (.3, .3) & 1.0000  & 1078  & 0.9733  & 3138  & 5.4191  \\
    (.3, .5) & 0.9847  & 1110  & 0.9515  & 3290  & 4.8737  \\
    (.3, .7) & 0.9991  & 1107  & 0.9338  & 3436  & 8.6460  \\
    (.3, .9) & 0.9950  & 1001  & 0.9586  & 3160  & 5.6477  \\
    (.5, .3) & 0.9872  & 1173  & 0.9579  & 3228  & 4.7228  \\
    (.5, .5) & 0.9943  & 1047  & 0.9558  & 3109  & 5.9036  \\
    (.5, .7) & 0.9895  & 954   & 0.9422  & 3216  & 6.0635  \\
    (.5, .9) & 0.9826  & 1037  & 0.9731  & 3158  & 1.7152  \\
    (.7, .3) & 0.9829  & 994   & 0.9671  & 3108  & 2.5809  \\
    (.7, .5) & 0.9804  & 1172  & 0.9245  & 3174  & 6.8637  \\
    (.7, .7) & 0.9942  & 1030  & 0.9389  & 3165  & 7.2458  \\
    (.7, .9) & 0.9889  & 1169  & 0.9247  & 2985  & 8.0104  \\
    (.9, .3) & 0.9865  & 1040  & 0.9678  & 3190  & 3.1966  \\
    (.9, .5) & 0.9952  & 1048  & 0.9424  & 3179  & 7.1880  \\
    (.9, .7) & 0.9904  & 1038  & 0.9315  & 3272  & 7.2954  \\
    (.9, .9) & 0.9873  & 1004  & 0.9141  & 3339  & 8.0252  \\
    \bottomrule
    \end{tabular}%
    }
  \caption{Two-proportion z-test between our result and feature-level majority vote with respect to sound rate (SR). SR$_1$ and SR$_2$ represent the sound rates of our model and the baseline, respectively, while $n_1$ and $n_2$ indicate the sample sizes (number of characters) of our model and the baseline.}
  \label{tab:hypo-test}%
\end{table}%


% Moreover, in most settings, our result has a lower Violation rate of \fq~than baseline, demonstrating that \fq~information is better utilized by our model.
% We can find that our model is robust to different values of hyperparameters, which represents varying degrees of sound change. 
% Even when a large portion of characters' pronunciations are changed, our model can still successfully reconstruct most characters. 
% High equal rate greatly indicates the effectiveness of our model.

Though remarkable, we should exercise caution when interpreting the results. 
Naturally occuring sound changes display greater regularity in some cases but are much less regular in others.
% Natural sound change displays more patternisation in certain aspects while being more chaotic in others.

% \begin{table}[thbp] 
%     \centering
%     \scalebox{0.92}{
%     \begin{tabular}{ccccc}
%     \toprule
%     &\multicolumn{2}{c}{\bf Random system}&\multicolumn{2}{c}{\bf Latin}\\
%     \cmidrule(lr){2-3}\cmidrule(lr){4-5}
%     Settings & {ER(\%)} & {Avg. $L_1$} & {ER(\%)} & {Avg. $L_1$} \\
%     \midrule
%    (.1, .3, .2)   & 73.75 & 0.6953 & 86.83& 0.2763\\
%       (.1, .3, .3)   & 75.08 & 0.5722  &88.68  & 0.2253\\
%        (.1, .4, .2)   & 62.38 & 0.7518  & 86.79 & 0.2069\\
%         (.1, .4, .3)   & 59.37 & 0.8912  & 74.50 & 0.4456\\
%     \midrule
%     (.2, .3, .2)   & 77.02 & 0.4505  &86.47  & 0.1993\\
%     (.2, .3, .3)  & 70.15 & 0.5849  & 87.89 & 0.1995\\
%       (.2, .4, .2)   & 61.81 & 0.8298 & 87.09 & 0.2303\\
%         (.2, .4, .3)   & 77.93 & 0.4411  & 83.43 & 0.2830\\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{\label{tab:synthesis-basic-result}Results with synthetic data that starts from randomly sampled consonants and Latin respectively. The numbers in the `Settings' column correspond to ($p_{\text{fq}}, p_{\text{dia}}, p_{\text{char}}$) respectively. `ER' means equal rate.} 
% \end{table} 

%     &\multicolumn{2}{c}{\bf Random system}&\multicolumn{2}{c}{\bf Latin}\\
%     \cmidrule(lr){2-3}\cmidrule(lr){4-5}
%     Settings & {ER(\%)} & {Avg. $L_1$} & {ER(\%)} & {Avg. $L_1$} \\

\begin{table*}[thbp]
  \centering
    \begin{tabular}{cccccc}
    \toprule
     & \multicolumn{4}{c}{$k=1$}&\multicolumn{1}{c}{$k=3$}\\
    \cmidrule(lr){2-5} \cmidrule(lr){6-6} 
    \multicolumn{1}{c}{\textbf{Setting}} & $\lambda_{\text{fq}}=0$ & $\lambda_{\text{fq}}=0.5$ & $\lambda_{\text{fq}}=0.75$ & $\lambda_{\text{fq}}=0.95$ & \multicolumn{1}{l}{$\lambda_{\text{fq}}=0.5$} \\
    \midrule
    (0.1, 0.3, 0.3) &  96.85\%&  \textbf{98.61}\%& 98.89\% & 90.35\% & 98.70\% \\
    (0.1, 0.5, 0.5) & 75.45\% & \textbf{78.80}\% & 79.94\% & 68.77\% & 79.27\% \\
    (0.1, 0.7, 0.7) & 54.37\% & \textbf{57.48}\%& 59.13\% & 53.69\% & 57.67\% \\
    \bottomrule
    \end{tabular}%
    \caption{\label{tab:synthesis-adjust-parameters}Results on Latin consonant system with parameters adjusted. 
    The numbers in the `Settings' column correspond to ($p_{\text{fq}}, p_{\text{dia}}, p_{\text{char}}$) respectively. 
    % `Basic' represents $\lambda_{\text{fq}}=0.5$ and $k=1$. 
    $k$ represents the weight assigned to character and speller pairs with matching medials, as defined in \S\ref{sec:model-context}.} 
\end{table*}

\paragraph{Base Distance Function $f$} \label{form-of-function}
The general distance function $f$ (defined in \S\ref{sec:obj}) is also a hyperparameter. 
In Figure \ref{Fig:baseline-Latin}, it is set as $f(x_1,x_2)=|x_1-x_2|$. 
We did a number of auxiliary experiments with different $p$-norm distance functions. 
Even the quadratic function significantly increases the difficulty to the corresponding optimisation problems, without substantial improvement in performance. 
It seems that the only practical option for $f$ is $L_1$.
All following experiments are based on this choice.
%Here, We set $p$ percent of $f$ as $f_2$, while the rest remains $f_1$. The relationship between $p$ and performance is in Table \ref{table:efficiency-2661}\todo{wait for experiments}.
%
%We find that quadratic terms significantly increase the difficulty to the corresponding optimisation problems, without improvement in performance. 
%We assume that polynomial functions with a higher degree $d$ ($d >2$) as well as $d$-norm functions are even more computationally demanding. As a result, it seems that the only practical option for $f$ is $f(x_1,x_2)=|x_1-x_2|$.


\paragraph{Weight of \fq~($\lambda_{\text{fq}}$)} \label{sec:fq-weight}
We adjust the weight of terms related to \fq~in the objective function, i.e. $\lambda_{\text{fq}}$ in Eq. (\ref{eq:objective}). By default, $\lambda_{\text{fq}}$ is set to 0.5, and we explore its effect with respect to equal rate in Table \ref{tab:synthesis-adjust-parameters}. Setting $\lambda_{\text{fq}}$ to 0 (i.e., not using \fq~information) results in a decrease in the equal rate, while increasing it to 0.75 improves the equal rate. However, further increasing it to 0.95 leads to a decline. These experiments suggest that the choice of $\lambda_{\text{fq}}$ is empirical, and we will adjust it accordingly when working with real data (\S\ref{sec:eval-AMI}).

\paragraph{Results with English, German and Mandarin}
To evaluate the robustness, we experiment with synthetic data that starts from a consonant system in natural phonology. We choose modern standard English, German and Mandarin as representatives. 
We also present results from random consonant system for comparison.
The results are in Table \ref{tab:other-language}, under the setting of (0.1, 0.5, 0.3), with $\lambda_{\text{fq}}$ set to 0.5. The remarkable results reaffirm the reliability of our model.

It is worth noting that reconstructing natural consonant systems is much easier than artificial ones.

\begin{table}[htbp]
  \centering
\begin{tabular}{ccccc}
    \toprule
      & \textbf{Ger} & \textbf{Man} & \textbf{Eng} & \textbf{RND} \\
      \midrule
\textbf{ER}(\%) & 96.10 &  94.48   &  93.02  & 84.73 \\
\textbf{Avg. $L1$}\unboldmath{} & .1894 &  .0884    & .1519 & .2549 \\
    \bottomrule
\end{tabular}%
    \caption{\label{tab:other-language}Results with synthetic data that starts from German, Mandarin, English, and the random system.}
\end{table}%


\subsection{Modeling the Influence of Context} \label{sec:model-context}
The phonetic value of phonemes can be influenced by its context. 
For initials in Chinese syllables, the major influential context is the medial that follows. 
To integrate medial information in MC\footnote{The data is provided by Peking University.}
into our model, we adjust the weight of terms related to \fq~in the objective function. 
For each character and speller pair, i.e. $(X,X_u) \in S_{\text{fq}}$, we assign a weight of $k$ to $d(F_{\text{MC}}(X), F_{\text{MC}}(X_u))$ if they share the same medial, otherwise $1$. In the basic setting, $k=1$, and increasing it aims to improve the likelihood of pairs with matching medials sharing the same initial.

Setting $k$ to 3, we present representative results in Table \ref{tab:synthesis-adjust-parameters}. Changing $k$ from 1 to 3 has little influence on equal rate, indicating that medial information has already been well captured. 
Notably, our model is inherently conditional, 
as we always consider an initial in a particular character, where the medial, main vowel and coda are all fixed. 
%many initials only occur before medials that meet specific conditions, which likely explains the capture of medials.
When human scholars encounter overlapping heterogeneous information sources, decision-making becomes challenging, while our model provides a possible technique for such challenging issues.

The results demonstrate the flexibility of our model---heterogeneous information can be seamlessly integrated as constraints or terms in the objective function, and be integrated into our model conveniently. Furthermore, the flexibility to adjust weights of different terms allows fine-tuning according to specific requirements.

\begin{comment}
\begin{table}[th] 
  \centering
    \begin{tabular}{c|cccc}
    \hline
    \textbf{p} & 0 & 0.05 & 0.1   & 0.2  \\
    \hline
    \textbf{AMI} & 0.5917 & 0.5925 & 0.5919  & 0.5886\\
    \hline
    \textbf{time}(s) & 104.51 & 158.47  & 163.23  & 152.37 \\
    \hline
    \hline
    \textbf{p} & 0.3   & 0.4   & 0.5 & 0.6\\
    \hline
    \textbf{AMI} & 0.5870  &   0.5880  & 0.5888 & N/A \\
    \hline
    \textbf{time}(s)& 225.09  & 227.52  & 12268.53 & N/A \\
    \hline
    \end{tabular}%
    \caption{\label{table:efficiency-2661} Efficiency and accuracy wrt different $p$. 
    For each $p$, the experiment was ran for 3 times. The AMI of the three solutions have little difference, while the solving time may have a large variance due to the randomness in the solving algorithm. For example, when $p=0.5$, the solving times are 356.35s, 36005.28s, and 435.24s.}
\end{table}
\end{comment}