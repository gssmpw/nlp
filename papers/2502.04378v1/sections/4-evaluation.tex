\section{Evaluation}
\label{sec:eval}

In this section, we evaluate the performance of \approach and aim to answer the following research questions (RQs):

\noindent\textbf{RQ\textsubscript{1} (Validity).} Can DILLEMA generate valid and realistic test cases from existing data?

\noindent\textbf{RQ\textsubscript{2} (Testing Effectiveness).} Can the generated test cases identify weaknesses in state-of-the-art DL models?

\noindent\textbf{RQ\textsubscript{3} (Retraining).} Can the generated test cases be used to improve the robustness of the tested models?

\subsection{Experimental Setup}
\label{sec:eval:setup}
\noindent\textbf{Datasets.} We performed experiments using two datasets: ImageNet1K~\cite{DBLP:conf/cvpr/DengDSLL009} and SHIFT~\cite{DBLP:conf/cvpr/SunSPWGSTY22}. These datasets represent two different tasks, image classification, and semantic segmentation, allowing us to assess the flexibility and applicability of \approach in various scenarios. ImageNet1K is a large-scale dataset commonly used for image classification tasks and SHIFT is a synthetic dataset designed for evaluating autonomous driving systems under different conditions (e.g., weather changes, lighting conditions).

\noindent\textbf{Tested Models.} We used \approach to test several DL architectures.
For ImageNet1K, we evaluated classification models (that is, ResNet18, ResNet50, and ResNet152~\cite{DBLP:conf/cvpr/HeZRS16}) using pre-trained versions provided by PyTorch. For SHIFT, we tested a semantic segmentation model (i.e., DeepLabV3~\cite{DBLP:conf/eccv/ChenZPSA18} model with a ResNet50 backbone), which we custom-trained following the original authors' training procedure~\cite{DBLP:conf/eccv/ChenZPSA18}. The training of this model took approximately $24$ hours to complete.

\noindent\textbf{Evaluation Metrics.} We used accuracy to evaluate the quality of classification models (on ImageNet1K), and we used mean Intersection over Union (mIoU) to measure the ability to evaluate the quality of semantic segmentation models.

\noindent\textbf{\approach Configuration\footnote{
To support reproducibility, all our data, including the code of \approach, the results of the human survey, of the testing and retraining, are available in our replication package: \url{https://github.com/deib-polimi/dillema}.}.} We used BLIP2 6.7B~\cite{DBLP:conf/icml/0008LSH23} as the captioning model to generate context-aware descriptions, chosen for its ability to produce detailed, semantically rich captions. As LLM, we selected a 5-bit quantized LLaMA-2 13B~\cite{DBLP:journals/corr/abs-2307-09288} model to identify keywords, generate alternatives, and create counterfactual captions. We chose LLaMA-2 because it is open source and effective, and we opted for the 13B version with 5-bit quantization since it provided a balance between performance and resource efficiency given our computational and cost constraints.
Lastly, for image generation, we used ControlNet~\cite{DBLP:conf/iccv/ZhangRA23} with edge conditioning, a control-conditioned text-to-image diffusion model. ControlNet enabled us to introduce modifications to the images while maintaining the spatial structure of the original scene, ensuring that the relationships between objects and their surroundings remained consistent.
Although we chose these general-purpose models for compatibility with consumer hardware and reasonable runtime, other models with different capabilities could be used depending on specific needs.

\noindent\textbf{Prompt Template.} To guide the LLM effectively, we used a one-shot in-context learning approach~\cite{DBLP:conf/nips/Wei0SBIXCLZ22}, where each prompt included an example to help the model understand the request more accurately. The example illustrated the expected input and output formats. Each prompt was constructed to provide context and explicitly instruct the LLM on the required output format, which allowed for automated post-processing. If the LLM response failed to adhere to the specified output format and could not be automatically parsed, we repeated the request with a different random seed. This iterative process continued until a parsable response was obtained.

\noindent\textbf{Retraining Settings.}
For ImageNet1K, we re-trained the ResNet models using a batch size of $100$ and the SGD optimizer with an initial learning rate of $0.1$, a momentum of $0.9$ and a weight decay of $1 \times 10^{-4}$. The learning rate was decayed using the PyTorch StepLR scheduler with a step size of $30$ and a gamma of $0.1$, over $90$ epochs.
For SHIFT, we re-trained the DeepLabV3 model using the original settings provided by its authors. Specifically, the batch size was set to $12$, with training conducted over $200$ epochs using the Adam optimizer with a learning rate of $0.002$, betas set to $(0.9, 0.999)$, and epsilon set to $1 \times 10^{-8}$.

\noindent\textbf{Hardware and Software.} The experiments were carried out on an AWS virtual machine with an A10G NVIDIA GPU with 24GB of memory. Neural networks were designed using PyTorch 2.0.1, and accelerated using CUDA 11.8.
In general, the empirical evaluation required about $120$ GPU hours.
$96$ GPU-hours were spent on Imagenet1K ($125,000$ test cases), $24$ GPU hours were spent on SHIFT ($10,000$ test cases).

\subsection{RQ\textsubscript{1}. Validity}
\label{sec:experiments:validity}
This experiment aims to evaluate the realism and validity of the generated images, ensuring that they preserve the metamorphic relationship for both datasets and assessing how often hallucinations occur due to potential errors during the five steps of \approach. By validating the generated images end-to-end, we aim to identify instances where the pipeline produces incorrect or unrealistic results.
To achieve this, we conducted a human study using Amazon Mechanical Turk. Human evaluators were asked to verify if the generated images preserved the metamorphic relationship for both datasets.

In total, we obtained $2,500$ total responses. To ensure quality, we used control questions to filter unreliable answers. Responses failing these quality checks were discarded. To ensure experienced participants, the workers were selected based on an approval rate greater than $95\%$ and at least $50$ completed tasks. Each image was evaluated by five independent workers and the questions were discarded if consensus (agreement of at least $\frac{4}{5}$ participants) was not reached. In the end, only $2,380$ responses (out of $2,500$) were considered robust and good enough to answer the research question.


For ImageNet1K (\autoref{fig:rq1_validity_imagenet1k}), we used two types of questions and considered a transformation to be valid if our approach were able to correctly augment an existing image without modifying the label associated with it.
First, we performed a general evaluation on a randomly sampled set of $300$ augmented images from all generated cases to measure the overall validity.
Then, we proposed a focused evaluation of $100$ augmented images that the ResNet18 model misclassified, to check if the images were valid and interpretable by humans even when misclassified by the model.

\begin{figure}[h]
    \centering
    \includegraphics[width=.45\textwidth]{images/ImageNet-bar.pdf}
    \caption{Validity of the Generated Test Cases for Classification.}
    \label{fig:rq1_validity_imagenet1k}
\end{figure}

Our human study shows that human assessors achieved agreement on all images and $99.7\%$ of the augmented images were correctly classified by human assessors. Of the $300$ images, only $1$ image did not preserve the label associated with the original image.
For the set of images where the model (i.e., ResNet18) produced a misclassification, $82.7\%$ were still considered valid by human evaluators. This shows that while the test cases generated by \approach effectively induced misclassifications in the model, most of them could still be correctly classified by humans. This suggests that failures can often be attributed to bugs in the model rather than flaws in the image generation process, reinforcing the validity and utility of \approach for robust model testing.

For the SHIFT dataset (\autoref{fig:rq1_validity_shift}), we randomly selected $100$ augmented images. Among these, all depicted roads, $25$ included vehicles, and $15$ featured one or more pedestrians. Evaluators were tasked with verifying whether key elements critical for autonomous driving, such as roads, vehicles, and pedestrians, were consistently preserved through the transformations.  We checked these aspects since they are key elements that influence the behavior of an autonomous driving system.

\begin{figure}[h]
    \centering
    \includegraphics[width=.45\textwidth]{images/SHIFT-bar.pdf}
    \caption{Validity of the Generated Test Cases for Driving.}
    \label{fig:rq1_validity_shift}
\end{figure}

We observed the following validity rates: road preservation at $98.9\%$ ($100$ questions, $7$ were discarded due to lack of consensus), pedestrian preservation at $84.6\%$ ($15$ questions, $2$ discarded due to lack of consensus), and vehicle preservation at $100.0\%$ ($25$ questions, $1$ discarded due to lack of consensus). These results highlight that \approach can effectively maintain certain features, such as roads and vehicles, while being slightly less effective at preserving pedestrians.

\subsection{RQ\textsubscript{2}. Testing Effectiveness}

\begin{figure*}
\centering
    \begin{subfigure}[b]{.49\textwidth}
    \centering
\includegraphics[width=\textwidth]{images/conf_mat_shift.pdf}
    \caption{Accuracy on Original Test Suite.\label{fig:image1}}
    \end{subfigure}
    \begin{subfigure}[b]{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/conf_mat_shift_aug.pdf}
    \caption{Accuracy on \approach Augmented Test Suite.\label{fig:image2}}
    \end{subfigure}
\caption{Multi-class Confusion Matrix.}
\label{fig:rq2_effectiveness}
% \vspace{-3mm}
\end{figure*}

To evaluate the effectiveness of \approach, we evaluated its ability to detect weaknesses in state-of-the-art DL models using the generated test cases.

First, we performed experiments on ImageNet1K, focusing on identifying misclassification errors. For this purpose, we augmented $25$ images for each of the $1,000$ classes in the dataset. Each image was augmented five times to take advantage of the stochastic nature of diffusion models, which can generate different augmentations from the same input. The performance of the test suite generated by \approach was compared with the test set already available in the dataset.

\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \xspace\space\xspace\space  \textbf{Architecture}\xspace\space\xspace\space               & \xspace\space\xspace\space\xspace\space\xspace\space\textbf{Original Test Suite}\xspace\space\xspace\space\xspace\space\xspace\space                      & \xspace\space\xspace\space\textbf{\approach Test Suite}\xspace\space\xspace\space \\ \midrule
        \xspace\space\xspace\space  ResNet18\xspace\space\xspace\space & 5.26\% & 53.29\% \\
        \xspace\space\xspace\space  ResNet50\xspace\space\xspace\space & 2.55\% & 45.47\% \\
        \xspace\space\xspace\space  ResNet152\xspace\space\xspace\space & 1.47\% & 42.33\% \\ \bottomrule
        \end{tabular}
    \caption{Test Effectiveness.}
    \label{tab:rq2_effectiveness}
\end{table}

\autoref{tab:rq2_effectiveness} reports the performance of three ResNet variants in both test suites. The results reveal that, on average, $3.1\%$ of the original test suite was able to highlight misbehaviors, while $47.0\%$
of the test suite generated by \approach exposed faulty behaviors. However, it is important to note that, as discussed in \autoref{sec:experiments:validity}, not all of these detected misbehaviors may represent true failures. The human study confirmed that approximately $82.7\%$ of the misbehaviors detected by \approach were valid failures. Even after normalizing for this factor, the effectiveness of \approach remains significantly higher ($38.9\%$) than the original test set.

In addition, we analyzed how many augmentations per image led to model errors. Our findings indicate that for $33.29\%$ of the images, all augmentations resulted in misclassifications, whereas for $24.85\%$, none of the augmentations caused errors.

For the SHIFT dataset, we evaluated the DeepLabV3 model on the semantic segmentation task. The evaluation compared the augmented test set created by \approach with the original SHIFT test set. \autoref{fig:rq2_effectiveness} presents the normalized multi-class confusion matrix of the tested model on the original and augmented data. Rows represent the ground truth, columns represent the predicted class, and the diagonal indicates the percentage of correct predictions.

The results show that \approach successfully exposed interesting faulty behaviors. For example, in semantic classes where the model appeared robust in the original dataset, such as \textit{SideWalk} ($97\%$ correctly classified), the model showed significant vulnerability in the augmented dataset (only $38\%$). In more critical classes such as \textit{Road} and \textit{Vehicle}, we observed that the model maintained a relatively robust performance, with errors increasing by $9\%$ and $10\%$, respectively, as the accuracy decreased from $99\%$ and $97\%$ in the original dataset to $90\%$ and $87\%$ in the augmented dataset. However, for pedestrian recognition, the augmented dataset revealed a much higher vulnerability, with $34\%$ more misclassifications compared to the original dataset. This highlights the need to retrain the model with a stronger focus on identifying pedestrians to address this critical weakness.

These results highlight that \approach not only highlights hidden vulnerabilities in classes previously considered robust but also provides insights into critical performance degradations in safety-relevant semantic classes. In general, \approach effectively exposes model weaknesses in various scenarios.

\subsection{RQ\textsubscript{3}. Retraining Robustness}

To assess whether the test cases generated by \approach can improve the robustness, we conducted retraining experiments using the synthetically generated data. Retraining aimed to evaluate whether the incorporation of augmented test cases into the training process leads to improved performance on both original and augmented data.

For the ImageNet1K dataset, we retrained the ResNet18 model using a combined training set consisting of the original data and the augmented test cases generated by \approach. The model was re-trained for 90 epochs using the settings described in \textit{Retraining Settings}. The re-trained model showed a significant improvement in robustness, achieving a $52.27\%$ increase in accuracy in the augmented test cases and a $20.19\%$ improvement in the original test suite.

Concerning SHIFT, we achieved an improvement in mIoU across the original and augmented test sets. After retraining, mIoU in the original test suite improved from $85.32\%$ to $88.76\%$, while mIoU in the augmented dataset showed a more pronounced increase from $72.45\%$ to $80.32\%$.
Specifically, the retraining process revealed that while performance degradation on critical semantic classes like \textit{Road} and \textit{Vehicles} was minor, pedestrian recognition showed a significant recovery, increasing from $38\%$ to $62\%$. This improvement highlights the value of \approach in augmenting datasets to address vulnerabilities in safety-critical tasks.

These findings demonstrate that the generated test cases are highly effective in not only uncovering model vulnerabilities but also improving the robustness of DL models when incorporated into the retraining process.

\subsection{Threats to Validity}
\noindent\textbf{Internal Validity.}
Our pipeline relies on pre-trained models (captioning, LLM, diffusion) and random sampling of alternatives, which can introduce randomness and potential skew (e.g., consistently generating “red” vehicles). Another concern is the domain shift between real images and our synthesized outputs: models might perform worse simply because of unfamiliar synthetic characteristics rather than true weaknesses. However, our human study indicates that the vast majority of generated images retain labels recognizable to human evaluators, suggesting that they are semantically coherent rather than purely artificial or misleading. Thus, while some failures could stem from synthetic artifacts, the high human agreement on these images implies that many observed misclassifications reflect genuine model vulnerabilities rather than artifacts alone.

\noindent\textbf{External Validity.}
We tested \approach on classification and segmentation from distinct domains, but it may not generalize to specialized scenarios (e.g., medical imaging). Although each component (captioning, LLM, diffusion) seems broadly applicable, further testing on diverse datasets is required to confirm adaptability for industrial use and other vision tasks.

\noindent\textbf{Construct Validity.}
Our primary measure of success is whether the generated images preserve ground-truth labels and uncover vulnerabilities. While human assessments indicate that images remain valid, potential biases in LLM-generated alternatives (e.g., color choices) could distort conclusions. Additionally, the notion of validity is subjective; thus, future work should employ more rigorous metrics or automated checks to validate semantic consistency in generated test cases.