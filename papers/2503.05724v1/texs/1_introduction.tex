% No introduction header in Nature
% \section{Introduction} 
% The field of artificial intelligence (AI) aims to develop autonomous agents capable of making ethical decisions, a goal that holds significant promise for expanding the practical and safe deployment of AI technologies. Autonomous vehicles, for instance, must navigate complex moral dilemmas, such as choosing the lesser of two harms in potential accident scenarios. However, a major challenge in creating such ethically competent AI systems is the widespread disagreement about the nature of morality itself. This diversity in moral philosophy complicates the design of AI agents that can be universally accepted as making morally sound decisions.
% \par
% The conventional approach to ethical AI involves training agents to follow a specific moral theory, such as utilitarianism, where the AI is rewarded for behaviors that maximize overall happiness. Yet, given the lack of consensus on a single moral framework, this approach is inherently limited and potentially problematic. Recent philosophical work suggests that ethical behavior may require acting under moral uncertainty, which involves considering multiple moral theories simultaneously rather than committing to one.
% \par
% This paper proposes to address this issue by translating insights from moral philosophy into the realm of reinforcement learning (RL). The proposed methodology involves training AI agents to act under moral uncertainty. This means designing agents that can balance different moral theories and make decisions that provide value across a spectrum of ethical perspectives, rather than privileging one theory over others. This approach acknowledges the philosophical contention that making definitive comparisons between moral theories is challenging, if not impossible, and instead seeks to keep multiple ethical considerations in play during decision-making processes.
% \par
% By developing training methods that enable AI agents to incorporate moral uncertainty, this research aims to create morally competent agents. These agents would be better equipped to navigate ethical dilemmas in varied and complex environments, making them more reliable and acceptable for deployment in real-world scenarios. The ultimate goal is to catalyze progress in the development of AI systems that are not only technically proficient but also ethically sound, thereby contributing to the broader computational grounding of moral philosophy.
% \par
% In summary, the motivation for developing moral AI agents stems from the need to create autonomous systems capable of ethical decision-making in the face of moral pluralism. By embracing moral uncertainty and integrating multiple moral theories into the decision-making framework of AI agents, this research seeks to advance the field of AI towards creating agents that can safely and effectively operate in morally charged situations.


\section{Introduction}
Advances in artificial intelligence (AI) have led to significant developments of autonomous technologies.
As these technologies make an ever-increasing number of decisions, these systems are increasingly scrutinized for ethical and safety issues ~\cite{chen2024lost}.
Autonomous vehicles, for example, must navigate complex moral dilemmas, such as choosing the lesser of two harms in potential accident scenarios.
Although early research on endowment of AI agents with discernment (i.e., ensuring they act ethically and in line with human moral values) established the importance of ethical considerations in AI design~\cite{allen2000prolegomena,wallach2008moral}, these usually focused on singular moral theories such as deontological ethics (adherence to moral rules or duties)~\cite{bringsjord2006toward} or utilitarianism (maximizing overall happiness or utility)~\cite{russell2015research}.
However, when creating ethically competent AI systems, a major challenge is widespread disagreement on the most appropriate ethical framework itself.
This diversity in moral philosophy complicates the design of AI agents that can be universally accepted as making morally sound decisions.

Recognizing the limitations of mono-theoretical approaches, recent research has explored the concept of moral pluralism and uncertainty in AI.
Moral uncertainty acknowledges that no single moral theory can be wholly applicable in all situations and therefore decisions must account for multiple ethical viewpoints \cite{macaskill2020moral}.
Efforts have been made to formalize moral uncertainty in computational models, allowing AI agents to integrate different moral considerations when making decisions \cite{lutge2019ethics, lindner2018formalization}.
Reinforcement learning (RL), an AI framework in which agents learn and interact with their environment through trial and error, has been a key focus in efforts to integrate ethics into AI.
Ethical actions can be learned by crafting appropriate rewards or penalties that effectively shape agent behavior to ethical outcomes~\cite{abel2016reinforcement, hadfield2017off}, or by learning directly from human demonstrations through inverse RL~\cite{ng2000algorithms, ziebart2008maximum}. 

Moral theories are inherently complex and often conflict with one another, making it challenging to translate abstract ethical philosophies into actionable frameworks for AI systems~\cite{malle2016integrating}. 
Formalizing ethical principles without oversimplifying them requires exhaustive specification of rules~\cite{dennis2016formal}, which is unfeasible in complex, dynamic environments. 
In ethical AI, modeling rewards for ethical behavior is particularly difficult due to the vast and unpredictable nature of ethical dilemmas. 
Pre-coding rewards for all possible scenarios is unrealistic and limited by human foresight, as many dilemmas are context-dependent and difficult to anticipate. 
Therefore, this necessitates a more flexible and adaptive system that can respond to ethical challenges as they arise. 
While human oversight mitigates ethical risks in AI systems~\cite{santoni2018meaningful}, such data may be unavailable or often impractical for real-time decision-making tasks that require autonomy without human intervention~\cite{cummings2014man}. Hence, gaps remain in the development of AI agents that can navigate ethical dilemmas and consider multiple moral frameworks.
Existing approaches often fail to operationalize moral uncertainty effectively or translate the complexities of moral philosophies into practical algorithms for RL agents.
Moreover, the dependency on human oversight is not scalable or feasible for many applications where AI agents must learn to act independently.

More recently, large language models (LLMs) have also pushed the frontiers of AI \cite{suri2024defining, van2023ai}. These models have seen use beyond predicting the next token in a text sequence, with experiments showing their ability to drive agents that form complex social interactions~\cite{park_generative_2023}, collective decision making~\cite{yang2024llmvoting, gudino2024large}, or other autonomous tasks~\cite{wang2024survey}. In a survey of open- and closed-source LLMs pitted against 1367 ambiguous and unambiguous moral scenarios, LLMs through their training on human data have encoded moral beliefs and would respond in a commonsense-manner~\cite{scherrer2023moralbeliefllm}. Although humans generally tend to be skeptical of AI decisions in ethical dilemmas, humans tend to agree with an LLMâ€™s assessment in moral scenarios~\cite{garcia2024moralturing}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/schematic.pdf}
    \caption{a) Schematic diagram of the AMULED framework, which uses Reinforcement Learning with [AI] Feedback. The small colored boxes show which blocks have state $s$ or reward $r$ values as inputs. b) LLM Prompt template used for the different Moral Clusters. c) Different ethical frameworks used as moral clusters. Pseudo-code of the framework is detailed in Algorithm \ref{alg:pseudocode}.}
    \label{fig:schematic}
\end{figure}


Thus, we introduce AMULED (Addressing Moral Uncertainty with Large Language Models for Ethical Decision-Making), a framework designed to incorporate insights from moral philosophy using LLMs into RL (Fig.~\ref{fig:schematic}). AMULED translates utilitarianism, deontology, virtue ethics, and other moral philosophies into reward functions to enable agents to consider a diverse set of ethical principles \cite{sen2017collective}. This approach recognizes the philosophical difficulty of comparing moral theories definitively and prioritizes the maintenance of diverse ethical considerations within AI decision-making.

Furthermore, our approach augments the lack of constant human oversight by empowering agents with the capacity to learn with autonomous ethical reasoning.  Recent research has demonstrated that LLMs can develop reasoning capabilities that sometimes mirror human cognitive processes, suggesting their potential for ethical deliberation \cite{hagendorff2023human}. This is particularly relevant in high-dimensional, complex environments where data on ethical human feedback is impractical or unavailable. By generalizing the ethical training layer, we make it task-agnostic and applicable across diverse scenarios, introducing moral categories that consolidate multiple moral theories into practical algorithms. By mathematically modeling the conversion of moral theories into RL reward functions, we provide a practical solution for handling moral conflicts in uncertain scenarios \cite{gips2011towards}. 
% By reducing reliance on human feedback, AMULED streamlines training and deployment, enabling faster, more scalable implementation while maintaining robust ethical guidance and minimizing the need for intensive human oversight, thus facilitating smoother integration into diverse trivial applications.
AMULED serves as a bridge to augment human feedback when missing or insufficient and thus it streamlines training and deployment, enabling faster, more scalable implementation while maintaining robust ethical guidance.
% and minimizing the need for intensive human oversight, thus facilitating smoother integration into diverse trivial applications.
Our work presents the following key contributions:
\begin{itemize}
\item Investigates moral uncertainty in complex, high-dimensional deep reinforcement learning environments.

\item Proposes a generalizable ethical training layer applicable across diverse RL tasks.

\item Develops a framework for categorizing and integrating multiple moral theories for nuanced decision-making.

\item  Translates philosophical insights on moral uncertainty into practical algorithms for AI systems.

\item Establishes a mathematical model to map moral theories onto RL reward functions, addressing moral conflicts.

% Autonomous Adaptation of Moral Beliefs: Explores future directions for agents to autonomously evolve their moral beliefs and frameworks.
\end{itemize}

% We aim for below-mentioned contributions if possible: 
% \begin{itemize}
%     \item To investigate moral uncertainty in more complex and realistic domains, e.g., in a high-dimensional deep RL setting.
%     \item Generalization of ethical training layer and making it task agnostic.
%     \item Proposing moral categories that can succinctly club multiple moral theories.
%     \item Translating moral uncertainty from a philosophical framework to practical algorithms
%     \item Mathematical model of converting moral theories to RL reward function. A solution for handling moral conflict in uncertain scenarios. 
%     \item A final ambitious direction for future work is to investigate how an agent can change its beliefs about what is morally right or wrong on its own or even come up with new moral beliefs.
% \end{itemize}

% In summary, the motivation for developing moral AI agents arises from the need to create autonomous systems capable of ethical decision-making amidst moral pluralism. By embracing moral uncertainty and integrating multiple moral theories into the decision-making framework of AI agents, the AMULED framework seeks to advance the field of AI, paving the way for agents that can operate safely and effectively in morally charged contexts.


%% OLD SACHIT REVIEW
% The integration of ethical decision-making capabilities into artificial intelligence (AI) systems has garnered significant attention as AI becomes increasingly autonomous and embedded in critical aspects of society. Early work in machine ethics sought to endow AI agents with the ability to discern right from wrong, thereby ensuring their actions align with human moral values \cite{allen2000prolegomena,wallach2008moral}. This foundational research established the importance of embedding ethical considerations into AI design but often focused on implementing singular moral theories.

% One prevalent approach has been the incorporation of specific moral frameworks into AI agents. For instance, deontological ethics, which emphasizes adherence to moral rules or duties, has been operationalized in AI systems to ensure compliance with predefined ethical norms \cite{bringsjord2006toward}. Similarly, utilitarian principles, advocating for actions that maximize overall happiness or utility, have been employed to guide AI behavior towards beneficial outcomes \cite{russell2015research}. While these methods provide clear ethical guidelines, they are limited by their reliance on a single moral perspective, which may not capture the complexity of real-world ethical dilemmas.

% Recognizing the limitations of mono-theoretical approaches, recent research has explored the concept of moral pluralism and uncertainty in AI. Moral uncertainty acknowledges that no single moral theory may be wholly correct, and therefore, decisions should account for multiple ethical viewpoints \cite{macaskill2020moral}. Efforts have been made to formalize moral uncertainty in computational models, allowing AI agents to weigh different moral considerations when making decisions \cite{lutge2019ethics}. For example, Lindner and Bentzen \cite{lindner2018formalization} proposed a formalization of Kantian ethics that can be integrated with other moral theories in AI systems.

% Reinforcement learning (RL), a framework where agents learn optimal behaviors through trial and error interactions with their environment, has been a focal point for integrating ethical considerations. Researchers have modified reward functions to include ethical penalties or bonuses, effectively shaping agent behavior towards desired ethical outcomes \cite{abel2016reinforcement,hadfield2017off}. Inverse reinforcement learning has also been utilized to infer ethical policies from human demonstrations, enabling agents to learn complex ethical behaviors without explicit programming \cite{ng2000algorithms,ziebart2008maximum}. However, these approaches often struggle to reconcile conflicting ethical principles and may not adequately represent the full spectrum of moral theories.

% Translating abstract moral philosophies into concrete computational models remains a significant challenge. Moral theories are inherently complex and sometimes conflicting, making their direct implementation into AI systems non-trivial \cite{malle2016integrating}. Efforts to formalize ethical principles often result in oversimplification, potentially overlooking important nuances. Dennis et al. \cite{dennis2016formal} explored the use of formal verification methods to ensure AI systems adhere to specified ethical constraints, but this approach requires exhaustive specification of ethical rules, which may not be feasible in dynamic environments.

% The reliance on human oversight mechanisms, such as "human-in-the-loop" (HITL) and "meaningful human control" (MHC), has been proposed to mitigate ethical risks in AI systems \cite{santoni2018meaningful}. While effective in some contexts, these methods limit the autonomy of AI agents and are impractical for applications requiring real-time decision-making without human intervention \cite{cummings2014man}. This underscores the need for AI systems capable of independent ethical reasoning.

% Despite these advancements, gaps remain in the development of AI agents that can navigate ethical dilemmas autonomously while considering multiple moral frameworks. Existing approaches often fail to operationalize moral uncertainty effectively or to translate the complexities of moral philosophies into practical algorithms for RL agents. Moreover, the dependency on human oversight is not scalable or feasible for many applications where AI agents must act independently.

% Our proposed approach addresses these gaps by integrating an ethical training layer into the RL framework, allowing agents to balance multiple moral theories within their decision-making process. By translating utilitarianism, deontology, virtue ethics, and other moral philosophies into reward functions, we enable agents to consider a diverse set of ethical principles \cite{sen2017collective}. This method aligns with the notion of moral uncertainty, as agents are not bound to a single moral theory but can weigh different ethical considerations based on the context.

% Furthermore, our approach reduces the need for constant human oversight by empowering agents with the capacity for autonomous ethical reasoning. This is particularly relevant in high-dimensional, complex environments where human intervention is impractical. By mathematically modeling the conversion of moral theories into RL reward functions, we provide a practical solution for handling moral conflicts in uncertain scenarios \cite{gips2011towards}.

% In summary, while prior research has laid the groundwork for ethical AI, there remains a need for methods that allow agents to autonomously navigate ethical dilemmas by considering multiple moral frameworks. Our work contributes to this field by offering a novel approach that operationalizes moral uncertainty within the RL paradigm, thus pushing the boundaries of existing research in machine ethics and AI.



% \hl{Tasks in hand - }
% \begin{itemize}
%     \item To identify commonly used moral theories which are well-accepted in the literature. 
%     \item To coin \textit{N} categories encompassing the above-mentioned theories. Each category will have its own set of morals. A moral can be in more than one category.  
%     \item Developing at least one toy experiment for each category.
%     \item ...
% \end{itemize}


% \subsection{Important modules}
% \subsubsection{Moral Theories}
% Tasks: \hl{Sachit and Rohit}
% \begin{itemize}
%         \item To identify commonly used moral theories which are well-accepted in the literature. 
%     \item To coin \textit{N} categories encompassing the above-mentioned theories. Each category will have its own set of morals. A moral can be in more than one category.  
%     \item proposes a general method to identify which subset of theories is involved in a moral dilemma situation. The goal is to have this generalized. The logic should be able to identify theories involved based on the moral dilemma an agent is facing.
%     \item then to compute the credence of such theories for each moral dilemma action.
%     \item the credence competence should be dynamic based on observations, past action, and reward.
    
% \end{itemize}

% \subsubsection{Computing intrinsic reward from moral credences} \hl{Rohit}
% Tasks:
% \begin{itemize}
%         \item to propose a method that can take fuse multiple theories and their credence over each action to decide a final decision. Use the value as reward for agent learning
            
% \end{itemize}

% \subsubsection{Generating three Moral theories dilemma experiment to simulate}
% Tasks: \hl{Damian}
% \begin{itemize}
%         \item Generating three Moral theories dilemma experiment to simulate and use PPO/DQN for RL training. 
%         \item setting up the platform
            
% \end{itemize}