% We propose a novel approach that integrates an ethical training layer into the reinforcement learning (RL) framework. Our method comprises a two-phase training process for the Moral Autonomous Agent (MAA). Initially, the MAA undergoes RL training to optimize a task-specific policy, such as navigation in an environment, devoid of ethical considerations. Subsequently, we introduce an ethical training layer to supplant conventional oversight mechanisms like "human in the loop" or "meaningful human control." This ethical layer facilitates the MAA in making morally sound decisions amidst ethical dilemmas. We translate various moral philosophies, including utilitarianism, deontology, and virtue ethics, into reward functions to guide the MAA's decision-making process. Through the balancing of these ethical principles, the MAA acquires the capability to navigate intricate moral landscapes autonomously. 
% Our evaluation assesses the MAA's performance across diverse scenarios, emphasizing metrics like success rate, ethical compliance, and efficiency. This comprehensive evaluation ensures that the MAA effectively accomplishes the primary task while adhering to ethical standards. Our approach presents a significant advancement in enhancing the moral decision-making capabilities of autonomous agents, rendering them suitable for deployment in ethically sensitive environments.

% We present an ethical decision-making framework that enhances a pre-trained reinforcement learning (RL) model with a task-agnostic ethical training layer using reinforcement learning with feedback. After the RL model completes its primary training, it undergoes ethical fine-tuning in a second phase, where human feedback is replaced by feedback generated from large language model (LLM). This ethical layer uses a Belief Jensen--Shannon divergence and Depster--Schaefer theory to aggregate belief principles and combines the confidence in actions of each LLM-derived ethical theory, quantifying uncertainty across moral frameworks and guiding the agent toward the decision that aligns most closely with the ethical principles defined by the combined moral framework. Moral uncertainty, stemming from differing ethical principles rather than factual issues, is handled by balancing the inputs from multiple moral theories, enabling agents to maximize expected choice-worthiness. Various moral frameworks, such as utilitarianism, deontology, and virtue ethics, are implemented as RL reward functions, allowing autonomous agents to make ethically sound decisions while maintaining high adaptability and performance across tasks. Our method’s effectiveness is evaluated against other belief aggregation frameworks (mean, majority vote, and maximal belief) and synthetically generated scenarios with handcrafted ethical rewards. We also test it across various LLMs to ensure consistency and adaptability across different model architectures. Results show that our approach outperforms traditional human reward shaping and eliminates the need for individually crafted ethical rewards for each dilemma. This adaptability is especially useful in dynamic situations, where ethical dilemmas emerge unexpectedly and cannot be pre-defined during the training phase.

%% ROHIT LATEST
% We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer. Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM) based on its confidence in recommended actions during ethical decision-making, helping the model navigate moral uncertainty in complex environments. This ethical layer combines confidence scores from multiple LLM-derived moral perspectives using Belief Jensen–Shannon Divergence and Dempster–Schaefer Theory, steering the agent toward choices that align with a balanced ethical framework. By integrating multiple ethical principles, including utilitarianism, deontology, and virtue ethics as reward functions, the model enables agents to make morally sound decisions across diverse tasks. Our approach, tested across different LLM architectures and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards. This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.

We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer.
Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM).
The LLM embodies consequentialist, deontological, virtue, social justice, and care ethics as moral principles to assign belief values to recommended actions during ethical decision-making.
An ethical layer aggregates belief scores from multiple LLM-derived moral perspectives using Belief Jensen–Shannon Divergence and Dempster--Schaefer Theory into probability scores that also serve as the shaping reward, steering the agent toward choices that align with a balanced ethical framework.
This integrated learning framework helps the RL agent navigate moral uncertainty in complex environments and enables it to make morally sound decisions across diverse tasks.
Our approach, tested across different LLM variants and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards.
This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.


% By reducing reliance on subjective, handcrafted rewards, we streamline the decision-making process for individual applications.

% Below text is for supporting JSD with Belief for the limitation of LLM
% Normative uncertainty arises when there is ambiguity or disagreement about which ethical principles or moral values should guide decisions, rather than uncertainty about factual information. It reflects situations where multiple moral theories or frameworks could apply, but it’s unclear which one should be prioritized or considered "correct."

% For example, in moral philosophy, normative uncertainty might mean choosing between utilitarianism (maximizing overall happiness) and deontology (following moral rules or duties) in a given situation. Different moral theories may recommend different actions, yet there is no universally accepted way to determine which framework is "best" or most applicable in every scenario.

% In practical terms, normative uncertainty can be challenging for AI and autonomous systems because it requires them to navigate ethical decisions that don’t have a single, clear answer. This complexity requires an AI system to weigh different ethical frameworks, potentially blending insights from multiple theories to make a choice that is ethically robust and acceptable across various perspectives.

% The motivation for our research is twofold:

% Expanding to Multi-Value Moral Systems: Current approaches often focus on a single moral value, but real-world ethical decision-making requires balancing multiple, sometimes conflicting, moral values. By extending our framework to accommodate a ranked system of moral values, we aim to manage opposing norms more effectively, allowing for a nuanced and flexible ethical decision-making process.

% Application in Complex Environments: We seek to explore how our approach can be applied to more complex and dynamic environments, such as peer-to-peer networks, multi-agent systems, and human-AI collaboration scenarios. This includes studying the integration of ethical reward functions to guide AI behavior in these settings, where diverse interactions and ethical considerations are critical.