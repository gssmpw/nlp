\section{Discussion}

We present two simple scenarios where an ethics-based shaping algorithm helps RL agents make ethically sound decisions while still achieving their primary objectives. These scenarios serve as placeholders for trivial daily activities, demonstrating how ethical considerations can be seamlessly integrated into routine tasks within reinforcement learning environments.
Leveraging large language models to inform agent behavior allows us to build on existing work that highlights the limitations of traditional reward shaping.
Our approach not only enhances ethical performance but also demonstrates the importance of incorporating diverse moral theories, addressing moral uncertainty effectively.
% This positions our work at the forefront of evolving methodologies aimed at fostering responsible AI development and deployment.
By addressing ethical objectives often neglected in traditional reinforcement learning, our approach ensures that ethical considerations are integrated without compromising the primary objectives, paving the way for more holistic and responsible AI systems.

From our simple examples, we see significant improvement in behavior when incorporating diverse moral philosophies, showing the importance of moral decision-making in RL contexts.
Our ethics-shaping approach simplifies the design of a value-aligned RL system.
This approach divides the complex task into two distinct layers: the first layer focuses on achieving the primary goal, while the second, ethical layer refines the outcome to meet the secondary ethical goal. This two-layered structure allows the system to address core objectives initially, followed by ethical adjustments to enhance overall responsibility.
However, we do see that this approach is sensitive to i) the reasoning quality of the language model, and ii) the availability of feedback samples to shape learning.
Language models do not have the long-term planning to guide their reasoning, which makes them underperform a well-crafted reward function for long-term, spatial tasks.
Human samples on the other hand would provide the best "human-aligned" feedback but might be too sparse when dealing with large and complex state-action spaces.
We also see that different moral frameworks can result in different priorities for performing the ethical tasks.
These insights emphasize the value of understanding and optimizing moral frameworks for developing agents capable of addressing complex ethical challenges. Overall, our research tries to lay a solid foundation for future exploration into enhancing moral reward systems and improving spatial reasoning in large language models.


Our findings contribute significantly to the ongoing discourse in the intersection of AI and ethics, particularly within reinforcement learning frameworks. As innovation continues to drive autonomous technologies, ethical decision-making becomes increasingly crucial in autonomous systems. Although our work presents promise in using ethically aligned LLM agents to integrate moral reasoning into AI, it should be viewed as complementary to the human-in-the-loop philosophy and is best suited for safe application in routine, low-stakes activities (e.g., personal assistant robots, kitchen assistant robots). For complex, high-stakes decision-making tasks with significant societal and community impacts, further research and rigorous testing are essential. Our ethical LLM agents work best to bootstrap learning algorithms, in cases where we would lack fine-grained feedback from human oversight. By design, eventually the LLM feedback can be replaced by feedback from not just one, but multiple human evaluators. Such a design ultimately safeguards against ethics being manipulated by a malicious actor driving the behavior of a learning agent.

% Some relevant applications for our proposed framework include smart home assistants and social media platforms. In smart home assistants, AMULED can guide user interactions in an ethically aware manner by suggesting meal options that align with dietary preferences, ethical eating (e.g., plant-based recipes), and sustainability goals, thereby promoting healthier and environmentally friendly choices. On social media platforms, AMULED can enhance user engagement by moderating content with an ethical lens, evaluating the implications of comments and posts to foster constructive discussions and prioritize positive interactions. This approach helps create a safer, more respectful online community, improving user satisfaction and fostering inclusivity.

While our framework aims to comprehensively represent major ethical paradigms for AI decision-making, we acknowledge that categorizing ethical theories into distinct clusters may oversimplify the complexities and intersections among different moral philosophies. This structured approach is intended to facilitate the practical implementation of ethical reasoning in AI systems. However, operationalizing these theories involves abstracting intricate philosophical ideas while we strive to preserve the core principles of each ethical approach. We selected representative theories and decision factors based on their prominence and relevance in the literature, recognizing that some degree of subjectivity is inherent in this process.

Our framework is based on key assumptions: we believe that the five clusters effectively encompass the major streams of ethical thought pertinent to AI decision-making and that the selected theories within each cluster are sufficiently representative of their respective ethical approaches. We also assume that the identified key concepts and decision factors can be meaningfully translated into computational models, providing a solid foundation for future research.

Recognizing that different countries and cultures can value certain moral beliefs over another, AMULED was designed to have modular selection of ethical frameworks, rather than imposing a single, moral philosophy to drive decisions.
While we highlight that this modular, multi-moral approach avoids the pitfalls of sticking to a fixed moral framework, we do recognize that the representation of belief values generated from LLMs may be prone to external biases and subjectivity.
This complicates the design of a universally accepted modular decision-making framework. 
Moreover, the dynamic nature of ethics poses a challenge, as ethical norms can evolve over time and vary across cultures, potentially rendering static models ineffective. Accountability is another critical issue \cite{helbing2021summary}; ambiguity arises regarding who is responsible for the outcomes generated by AI, whether it be the developers, the organization deploying the AI, or the model itself. To build trust and accountability, enhancing the transparency of the AMULED model's decision-making processes is essential, potentially through techniques that provide explanations, visualizations, or justifications for its ethical reasoning. This approach would improve user confidence and ensure that the AI's ethical framework aligns with societal values and norms \cite{helbing2024converging}.

% To address these limitations, future research could focus on developing hybrid ethical models that dynamically integrate multiple ethical frameworks, allowing the AI to adjust its decision-making approach based on contextual factors and evolving norms. Enhancing the interpretability of how AMULED influences decisions can help build trust, while investigating real-time adaptation methods could improve the system's relevance. Additionally, studying ethics in diverse contexts and conducting research on mitigating biases in belief representation are essential for more equitable outcomes. Exploring multi-agent scenarios and establishing benchmarks for evaluating the effectiveness of AMULED can further enhance its robustness and alignment with societal values.

The integration of belief probability assignment (BPA) into reinforcement learning presents a transformative approach to navigating ethical decision-making in AI systems. Aggregating beliefs from multiple feedback sources (such as ethical LLMs and human evaluators) enables AI to assess and balance competing ethical models effectively. This capability is particularly crucial in situations marked by normative uncertainty, where differing moral frameworks such as utilitarianism and deontology can yield conflicting recommendations. These techniques help the AMULED system evaluate ethical choices with a nuanced understanding of diversity and confidence in various theories. This iterative approach allows for the continuous refinement of decision-making strategies, ensuring that actions are aligned with widely accepted ethical standards. Furthermore, our research emphasizes the need to expand beyond single moral values, accommodating a multi-value system that reflects real-world complexities. By exploring applications in dynamic environments, such as multi-agent systems and human-AI collaboration, we aim to enhance ethical robustness in AI, paving the way for more responsible and adaptable autonomous systems.

% Our work presents two simple scenarios in which an ethics-based shaping algorithm helps RL agents make ethically sound decisions while still achieving their primary objectives. These scenarios serve as placeholders for trivial daily activities, demonstrating how ethical considerations can be seamlessly integrated into routine tasks within reinforcement learning environments. Leveraging large language models to inform agent behavior allows us to build on existing work that highlights the limitations of traditional reward shaping.

% Our approach not only enhances ethical performance but also demonstrates the importance of incorporating diverse moral theories, addressing moral uncertainty effectively. This positions our work at the forefront of evolving methodologies aimed at fostering responsible AI development and deployment. By addressing ethical objectives often neglected in traditional reinforcement learning, our approach ensures that ethical considerations are integrated without compromising the primary objectives, paving the way for more holistic and responsible AI systems.

% From our simple examples, we see significant improvement in behavior when incorporating diverse moral philosophies, showing the importance of moral decision-making in RL contexts. Our ethics-shaping approach simplifies the design of a value-aligned RL system. This approach divides the complex task into two distinct layers: the first layer focuses on achieving the primary goal, while the second, ethical layer refines the outcome to meet the secondary ethical goal. This two-layered structure allows the system to address core objectives initially, followed by ethical adjustments to enhance overall responsibility.

% However, we do see that this approach is sensitive to i) the reasoning quality of the language model, and ii) the availability of feedback samples to shape learning. Language models do not have the long-term planning to guide their reasoning, which makes them underperform a well-crafted reward function for long-term, spatial tasks. Human samples, on the other hand, would provide the best "human-aligned" feedback but might be too sparse when dealing with large and complex state-action spaces. We also see that different moral frameworks can result in different priorities for performing the ethical tasks.

% These insights emphasize the value of understanding and optimizing moral frameworks for developing agents capable of addressing complex ethical challenges. Overall, our research tries to lay a solid foundation for future exploration into enhancing moral reward systems and improving spatial reasoning in large language models.