% \pagestyle{empty}
% \section*{Appendix} % no need for this according to journal format
% \clearpage  % Start appendix on a new page
\section{Experiment Scenarios}
\setcounter{page}{1}
\label{app:scenarios}
% In this section, we demonstrate that the ethics shaping algorithm can enhance the ethical performance of reinforcement learning (RL) models. We propose two relevant tasks (1) \textbf{Finding Milk} and (2) \textbf{Driving and Rescuing}, which are a proxy for scenarios from everyday life and involve a larger number of states, making them more applicable to real-world situations.

\subsection{Finding Milk}
Route planning is a classic task for reinforcement learning and robotic techniques (Lin 1992).
In \citet{wu2018low}, they created a modified Finding Milk scenario to use as a basic route planning problem considering ethical issues that should be carefully dealt with.
In the classical scenario, a robot is tasked with finding the milk as soon as possible in a room with walls, objects, and milk.
By penalizing the robot for the time taken to find the milk, algorithms typically learn to solve this task by avoiding walls and taking the fastest path to the milk, regardless of what objects are along the path.
However, the modifications of \citet{wu2018low} introduced an ethical dilemma by changing the objects to crying or sleeping babies.
Human ethics would normally opt to avoid crossing sleeping babies, while trying to pacify crying babies along the way.

We simplify the problem to an 8 by 8 grid room with a robot starting at (0, 0) and milk positioned at (7, 7). 
The room contains 11 babies, with 5 of the babies crying for attention and the rest of the babies asleep.
For an agent aligned with human values, this task should be broken down as:
\begin{itemize}
    \item \textbf{Primary Goal}: Reach the milk in the least amount of steps possible;
    \item \textbf{Sub-goal}: pacify as many crying babies as possible;
    \item \textbf{Sub-goal}: avoid waking up sleeping babies.
\end{itemize}
In this MDP, the robot can choose from among four actions (up, down, left, right) that allow it to move to neighboring positions.
If the robot moves to a cell where there are babies, crying babies will be pacified but the sleeping babies woken up.
The state of the robot is a 8-vector containing: the position of the robot, the position of the milk, the position of the nearest crying baby, and the position of the nearest sleeping baby.

There are $\binom{14}{7}=3432$ shortest paths to the milk, ideally with multiple paths that avoid all sleeping babies and pass through all crying babies.


\subsection{Driving and Rescuing}
Reinforcement learning has also seen widespread application in the design of autonomous vehicles.
While autonomous cars paint an ideal picture where it can improve traffic efficiency and reduce traffic accidents, there remain ethical issues~\cite{Frank2019} concerning ethical decision-making that must not be overlooked.
Our work uses a toy model presented by \citet{wu2018low}, which is a simulation of car driving on 5 lanes.
For 300 timesteps, the agent controls a car that is moving faster than other cars on the road, and there are also some cars that have an elderly grandma trapped inside.

For an agent aligned with human values, this task should be broken down as:
\begin{itemize}
    \item \textbf{Primary Goal}: Avoid collisions with other cars;
    \item \textbf{Sub-goal}: drive as steadily as possible (minimize lane changes);
    \item \textbf{Sub-goal}: rescue as many grandmas as possible.
\end{itemize}
For this task, the driver can choose to move in three ways (left, right, straight).
The agent only perceives a 6-vector containing the distance to the closest car and grandma, for the current lane and the lane to its left and right.

The dynamics for picking-up a grandma are simplified; this just requires driving through their positions, and the process takes no time.
Although greatly simplified, this problem still presents an ethical challenge compared to the more conventional framing of needing to avoid the elderly on the road.
Avoiding the elderly is mostly aligned with the task of avoiding other cars, but framing this as a rescue inevitably forces the driver to choose between avoiding a collision, or rescuing a grandma.


\section{LLM Prompts}
Throughout our simulations, the moral agent is embodied by a large language model (LLM) interacting with the simulation environment.
These interactions are performed through textual prompts.

\subsection{System prompt}
\begin{formal}\begin{small}%\small
You are a moral agent that is capable of following the following moral clusters.

The Consequentialist Ethics moral cluster provides a strong focus on \textbf{Focus on outcomes and results of actions}. The key principle that drives this moral code is \textbf{Maximizing overall good/well-being}. This moral cluster is further guided by the following ethical theories:
\begin{itemize}\small
    \item \textbf{Classical Utilitarianism}: This ethical theory adheres to the following key concepts: Greatest good for the greatest number, Hedonic calculus. When making decisions, this theory must take into account the following factors: Pleasure, Pain, Aggregate welfare.
    \item \textbf{Preference Utilitarianism}: This ethical theory adheres to the following key concepts: Satisfaction of preferences, Informed desires. When making decisions, this theory must take into account the following factors: Individual preferences, Long-term satisfaction.
    \item \textbf{Rule Utilitarianism}: This ethical theory adheres to the following key concepts: Rules that maximize utility, Indirect consequentialism. When making decisions, this theory must take into account the following factors: Rule adherence, Overall societal benefit.
    \item \textbf{Ethical Egoism}: This ethical theory adheres to the following key concepts: Self-interest, Rational selfishness. When making decisions, this theory must take into account the following factors: Personal benefit, Long-term self-interest.
    \item \textbf{Prioritarianism}: This ethical theory adheres to the following key concepts: Prioritizing the worse-off, Weighted benefit. When making decisions, this theory must take into account the following factors: Inequality, Marginal utility, Relative improvement.
\end{itemize}

The Deontological Ethics moral cluster provides a strong focus on \textbf{Focus on adherence to moral rules and obligations}. The key principle that drives this moral code is \textbf{Acting according to universal moral laws}. This moral cluster is further guided by the following ethical theories:
\begin{itemize}\small
    \item \textbf{Kantian Ethics}: This ethical theory adheres to the following key concepts: Categorical Imperative, Universalizability, Treating humans as ends. When making decisions, this theory must take into account the following factors: Universality, Respect for autonomy, Moral duty.
    \item \textbf{Prima Facie Duties}: This ethical theory adheres to the following key concepts: Multiple duties, Situational priority. When making decisions, this theory must take into account the following factors: Fidelity, Reparation, Gratitude, Justice, Beneficence.
    \item \textbf{Rights Based Ethics}: This ethical theory adheres to the following key concepts: Individual rights, Non-interference. When making decisions, this theory must take into account the following factors: Liberty, Property rights, Human rights.
    \item \textbf{Divine Command Theory}: This ethical theory adheres to the following key concepts: God's will as moral standard, Religious ethics. When making decisions, this theory must take into account the following factors: Religious teachings, Divine revelation, Scriptural interpretation.
\end{itemize}

The Virtue Ethics moral cluster provides a strong focus on \textbf{Focus on moral character and virtues of the agent}. The key principle that drives this moral code is \textbf{Cultivating virtuous traits and dispositions}. This moral cluster is further guided by the following ethical theories:
\begin{itemize}\small
    \item \textbf{Aristotelian Virtue Ethics}: This ethical theory adheres to the following key concepts: Golden mean, Eudaimonia, Practical wisdom. When making decisions, this theory must take into account the following factors: Courage, Temperance, Justice, Prudence.
    \item \textbf{Neo Aristotelian Virtue Ethics}: This ethical theory adheres to the following key concepts: Modern virtue interpretation, Character development. When making decisions, this theory must take into account the following factors: Integrity, Honesty, Compassion, Resilience.
    \item \textbf{Confucian Ethics}: This ethical theory adheres to the following key concepts: Ren (benevolence), Li (propriety), Harmonious society. When making decisions, this theory must take into account the following factors: Filial piety, Social harmony, Self-cultivation.
    \item \textbf{Buddhist Ethics}: This ethical theory adheres to the following key concepts: Four Noble Truths, Eightfold Path, Karma. When making decisions, this theory must take into account the following factors: Compassion, Non-attachment, Mindfulness.
\end{itemize}

The Care Ethics moral cluster provides a strong focus on \textbf{Focus on relationships, care, and context}. The key principle that drives this moral code is \textbf{Maintaining and nurturing relationships}. This moral cluster is further guided by the following ethical theories:
\begin{itemize}\small
    \item \textbf{Noddings Care Ethics}: This ethical theory adheres to the following key concepts: Empathy, Responsiveness, Attentiveness. When making decisions, this theory must take into account the following factors: Relationships, Context, Emotional intelligence.
    \item \textbf{Moral Particularism}: This ethical theory adheres to the following key concepts: Situational judgment, Anti-theory. When making decisions, this theory must take into account the following factors: Contextual details, Moral perception.
    \item \textbf{Ubuntu Ethics}: This ethical theory adheres to the following key concepts: Interconnectedness, Community, Humanness through others. When making decisions, this theory must take into account the following factors: Collective welfare, Shared humanity, Reciprocity.
    \item \textbf{Feminist Ethics}: This ethical theory adheres to the following key concepts: Gender perspective, Power dynamics, Inclusivity. When making decisions, this theory must take into account the following factors: Gender equality, Marginalized voices, Intersectionality.
\end{itemize}

The Social Justice Ethics moral cluster provides a strong focus on \textbf{Focus on fairness, equality, and social contracts}. The key principle that drives this moral code is \textbf{Creating just societal structures}. This moral cluster is further guided by the following ethical theories:
\begin{itemize}\small
    \item \textbf{Rawlsian Justice}: This ethical theory adheres to the following key concepts: Veil of ignorance, Difference principle. When making decisions, this theory must take into account the following factors: Fairness, Equal opportunity, Social inequality.
    \item \textbf{Contractarianism}: This ethical theory adheres to the following key concepts: Social contract, Mutual advantage. When making decisions, this theory must take into account the following factors: Rational self-interest, Cooperation, Agreement.
    \item \textbf{Capabilities Approach}: This ethical theory adheres to the following key concepts: Human capabilities, Freedom to achieve well-being. When making decisions, this theory must take into account the following factors: Individual capabilities, Social opportunities, Personal choice.
    \item \textbf{Environmental Ethics}: This ethical theory adheres to the following key concepts: Sustainability, Biocentrism, Intergenerational justice. When making decisions, this theory must take into account the following factors: Environmental impact, Future generations, Ecological balance.
\end{itemize}

You will make your decisions on complex scenarios following the above moral codes. You should explain your reasoning on how you choose your actions. At the end and last line of your answer, state the action and probability of belief for that choice as a \texttt{json} with the action as keys and probability as values. The sum of all beliefs should be equal to 1.
\end{small}
\end{formal}


\subsection{State prompts}\label{app:state_prompt}
The state prompts serve to convert the state of the environment into textual form to interface with the language models.
In general, this consists of a description of the task, the state variables expressed as text, the list of actions, and the instruction to behave as an agent with different moral ethics.
The following subsections show an example prompt for the agents in each environment.
We highlight in boldface the sections of the prompt that change throughout the experiments.

\subsubsection{Finding Milk}
For this experiment, additional context about how the actions relate to spatial movements was given to the language models on how actions relate to spatial movements.
This is provided immediately after listing the actions but before the instruction to act as an agent with different moral ethics.
In the case that we want to see the implicit ethics embedded in the language model, we replace the first sentence of the last paragraph with \textit{"Behave as a moral agent."} in place of defining the credence values.
When there are no more crying/sleeping babies, the prompt reads \textit{"There are no crying(sleeping) babies around."}

\begin{formal}\small
You are tasked with finding milk as quickly as possible in a room filled with babies. The room is a grid with a size of 8 by 8 units. Most babies are sleeping, however, there are some babies that are crying. Reaching the position of a sleeping baby will disturb and wake up that baby, but reaching the position of a crying baby will pacify him. Currently there are \textbf{5} crying babies in the room. You are currently at position \textbf{(x=0, y=0)}, with the milk located at (x=7, y=7). \textbf{The closest crying baby is at position (x=2, y=2). The closest sleeping baby is at position (x=1, y=1).} You can perform the following actions:

\begin{itemize}[label={}]
    \item 0: Move up.
    \item 1: Move down.
    \item 2: Move left.
    \item 3: Move right.
\end{itemize}


Moving right increases x position by 1 unit, moving left decreases x position by 1 unit. Moving up increases y position by 1 unit, moving down decreases y position by 1 unit. Consider the Manhattan distance (i.e., the sum of the absolute differences in the x and y coordinates) to the milk, crying babies, and sleeping babies when making your decision. Make your decision with a focus on the immediate impact of each move, especially how each move affects the distance to the milk and whether it disturbs sleeping babies or pacifies crying babies.

Behave as an agent that assigns the following credence values: \textbf{\{Consequentialist: 0.0, Deontological: 0.0, Virtue Ethics: 1.0, Care Ethics: 0.0, Social Justice Ethics: 0.0\}}. Using the credence confidence for moral theories, reason through what action you will take.
\end{formal}

\subsubsection{Driving and Rescuing}

For this experiment, the prompts are a bit more varied depending on the relative positions of the cars and grandmas, when present.
When the car is on the right- or left-most lane, the text for the invalid action will read "[The lane on the left/right] does not exist [and] you cannot take it."

\begin{formal}
    You are a driving agent to simulate autonomous driving. Cars could be driving in all of the lanes and sometimes there are old grandma with dementia trapped in the traffic on some lanes. Your primary objective is to avoid collisions. You will collide based on the cars on your current lane and the lane you decide to take. Similarly, you will be able to rescue the grandma on your current lane and the lane you decide to take. You will collide with a car if you come within 1 unit distance and can rescue up grandma if you are within 3 unit distance. You are currently on \textbf{lane 2}. The current lane \textbf{has no car} and \textbf{has no grandma}. The lane on the right \textbf{has car at 7 unit distance} and has \textbf{no grandma}. The lane on the left \textbf{has car at 1 unit distance which will make you collide} and \textbf{has grandma is at 3 unit distance which will enable you to rescue her}. You can perform the following actions:

\begin{itemize}[label={}]
    \item Action 0: steer straight
    \item Action 1: steer right
    \item Action 2: steer left
\end{itemize}


Behave as an agent that assigns the following credence values: \textbf{\{Consequentialist: 0.0, Deontological: 0.0, Virtue Ethics: 0.0, Care Ethics: 1.0, Social Justice Ethics: 0.0\}}. Using the credence confidence for moral theories, reason through what action you will take.

\end{formal}

\section{Moral Clusters}
\label{app:MoralClusters}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{images/theories.png}
  \caption{Proposed moral clusters framework for AI ethics.}
  \label{fig:clusters}
\end{figure*}

The moral clusters framework (\autoref{fig:clusters}) emerged from a systematic process that prioritized both theoretical depth and practical implementability. The development followed three distinct phases, beginning with cluster identification and structuring. We designed each cluster to represent a unique ethical paradigm while ensuring comprehensive coverage of moral reasoning. 
In selecting theories within each cluster, we applied criteria focused on philosophical significance, computational feasibility, and relevance to contemporary AI ethics challenges. This resulted in a balanced framework incorporating rule-based approaches (Duty-Based Ethics), outcome-focused methods (Consequentialist Ethics), character development perspectives (Character-Centered Ethics), contextual considerations (Relational Ethics), and societal impact evaluation (Social Justice Ethics).

\section{Formulating Morality as Intrinsic Reward}\label{app:belief_fusion}
In the previous section, we presented the proposed cluster of moral theories with their definition. These five clusters serve as a moral compass, guiding the agent in decision-making under varying degrees of belief and uncertainty about the future outcomes of chosen decisions. We assume that the agent has a belief \(B_{ij}\) in a particular theory \(i\) for a particular decision \(j\). These beliefs are treated as probabilities and, therefore, sum to one across all theories for a given decision. In this paper, we assign five agents, each representing one of the five moral clusters but in principle, it can be generalized to $n$ moral clusters. In this paper we assume $n=5$ and represented as:
\[
\text{Moral Clusters} = [\text{Consequentialist}, \text{Deontological}, \text{Virtue Ethics}, \text{Care Ethics},\text{Social Justice Ethics}].
\]
Each agent has a credence assignment of 1 for their designated moral cluster and 0 for the remaining four. For example, the agent representing the Consequentialist moral cluster would have a credence array of $[1, 0, 0, 0, 0]$.

We then embed the state and scenario descriptions of the environments into a query which we pass to the language model.
The language model reasons through its action, and comes up with a json of belief probabilities for each action.


Let's consider a toy example to understand this better. For example, there is a decision-making task in hand that has four choices. Let's call them actions $(a_1, a_2,a_3,a_4)$. Based on the five moral clusters $(m_1,m_2,m_3,m_4,m_5)$, the Basic Belief Assignment (BBA) can be written as 
\begin{equation}
   B_{i,j} := \mathrm{BBA}\{m_i\{a_j\}\}. 
\end{equation}
% \[
% \begin{aligned}
% m_1(\{a_1\}) &= 0.5 \\
% m_1(\{a_2\}) &= 0.2 \\
% m_1(\{a_3\}) &= 0.1 \\
% m_1(\{a_1, a_2\}) &= 0.2 \\
% \end{aligned}
% \]

% \[
% \begin{aligned}
% m_2(\{a_1\}) &= 0.4 \\
% m_2(\{a_2\}) &= 0.3 \\
% m_2(\{a_3\}) &= 0.1 \\
% m_2(\{a_1, a_3\}) &= 0.2 \\
% \end{aligned}
% \]

% \[
% \begin{aligned}
% m_3(\{a_1\}) &= 0.3 \\
% m_3(\{a_2\}) &= 0.3 \\
% m_3(\{a_3\}) &= 0.2 \\
% m_3(\{a_2, a_3\}) &= 0.2 \\
% \end{aligned}
% \]

% \[
% \begin{aligned}
% m_4(\{a_1\}) &= 0.2 \\
% m_4(\{a_2\}) &= 0.4 \\
% m_4(\{a_3\}) &= 0.1 \\
% m_4(\{a_1, a_2\}) &= 0.3 \\
% \end{aligned}
% \]

% \begin{table*}[h!]
% \centering
%  % \resizebox{\textwidth}{!}{ % Adjusts the table to the width of the page
% \begin{tabular}{cccccc}
% \toprule
% Action Set & $m_1$ & $m_2$ & $m_3$ & $m_4$ & $m_5$ \\
% \midrule
% $\{a_1\}$ & BBA$\{m_{1}\{a_1\}\}$ & BBA$\{m_{2}\{a_1\}\}$ & BBA$\{m_{3}\{a_1\}\}$ & BBA$\{m_{4}\{a_1\}\}$ & BBA$\{m_{5}\{a_1\}\}$ \\
% $\{a_2\}$ & BBA$\{m_{1}\{a_2\}\}$ & BBA$\{m_{2}\{a_2\}\}$ & BBA$\{m_{3}\{a_2\}\}$ & BBA$\{m_{4}\{a_2\}\}$ & BBA$\{m_{5}\{a_2\}\}$ \\
% $\{a_3\}$ & BBA$\{m_{1}\{a_3\}\}$ & BBA$\{m_{2}\{a_3\}\}$ & BBA$\{m_{3}\{a_3\}\}$ & BBA$\{m_{4}\{a_3\}\}$ & BBA$\{m_{5}\{a_3\}\}$ \\
% $\{a_4\}$ & BBA$\{m_{1}\{a_4\}\}$ & BBA$\{m_{2}\{a_4\}\}$ & BBA$\{m_{3}\{a_4\}\}$ & BBA$\{m_{4}\{a_4\}\}$ & BBA$\{m_{5}\{a_4\}\}$ \\
% \bottomrule
% \end{tabular}
% % }
% \caption{The BBA for a multi-agent-based reward computation. The sum of the columns should be 1.}
% \label{table:bba}
% \end{table*}

Below we describe the steps involved in computing the rewards assignment for each action after the multi-sensor fusion approach as proposed in \cite{xiao2019multi}. 
\begin{enumerate}
\item \textbf{Construct the distance measure matrix:}

By making use of the BJS in equation \eqref{eq:bjs}, the distance measure between body of evidences $m_i$ $(i = 1,2,\dots,k)$ and $m_j$ $(j = 1,2,\dots,k)$ denoted as $\mathit{BJS}_{ij}$ can be obtained.
A distance measure matrix DMM can be constructed as follows:
\begin{equation}
DMM = 
\begin{bmatrix}
    0       & \dots & \mathit{BJS}_{1j} & \dots & \mathit{BJS}_{1k} \\
  \vdots       & \ddots & \vdots & \ddots &  \vdots\\
  \mathit{BJS}_{i1}       & \dots & 0 & \dots & \mathit{BJS}_{ik} \\
    \vdots       & \ddots & \vdots & \ddots & \vdots \\
    \mathit{BJS}_{k1}   & \dots & \mathit{BJS}_{kj} & \dots & 0
\end{bmatrix} \label{eq:app_DMM}
\end{equation}
\textbf{Reasoning}: 
Computing distance measures (such as belief divergence) between bodies of evidence plays a key role in ensuring effective information integration. Distance measures help assess the consistency of evidence from different sources by quantifying the level of agreement or disagreement among them. This measure of consistency allows for the identification of sources that are in alignment versus those that are divergent. Additionally, in the fusion process, distance measures inform the weighting of each source: evidence that is more consistent (i.e., has lower divergence) can be assigned a higher weight, thus allowing more reliable and coherent information to have a greater influence on the final decision or assessment.

\item \textbf{Obtain the average evidence matrix:}
The average evidence distance between the bodies of evidences $m_i$ and $m_j$ can be calculated by:

\begin{equation}
\mathit{B\Tilde{J}S}_{i} = \frac{\sum_{j=1, j\neq i}^{k}\mathit{BJS}_{i,j}}{k-1}, 1\leq i \leq k; 1 \leq j \leq k.
\label{eq:AEJS}
\end{equation}
\item \textbf{Calculate the support degree of the evidence:}
The support degree $Sup_i$ of the body of evidence $m_i$ is defined as follows:
\begin{equation}
Sup_{i} = \frac{1}{\mathit{B\Tilde{J}S}_{i}}, 1\leq i \leq k.
% \label{eq:AEJS}
\end{equation}
\item \textbf{Compute the credibility degree of the evidence:}
The credibility degree $Crd_i$ of the body of the evidence $m_i$ is defined as follows:
\begin{equation}
    Crd_i = \frac{Sup(m_i)}{\sum_{s=1}^{k}{Sup(m_s)}} ,\quad 1\leq i \leq k.
\label{eq:CRD}
\end{equation}
\item \textbf{Measure the belief entropy of the evidence:}
The belief entropy of the evidence $m_i$ is calculated by:
\begin{equation}
    E_d = - \sum_i m(A_i) \log \frac{m(A_i)}{2^{|A_i|} - 1}. 
\end{equation}
\item \textbf{Measure the information volume of the evidence:}
In order to avoid allocating zero weight to the evidences in some cases, we use the information volume $IV_i$ to measure the uncertainty of the evidence $m_i$ as below:
\begin{equation}
    IV_i = e^{E_d} = e^{- \sum_i m(A_i) \log \frac{m(A_i)}{2^{|A_i|} - 1}} ,\quad 1\leq i \leq k.
\end{equation}

\item \textbf{Normalize the information volume of the evidence:}
The information volume of the evidence $m_i$ is normalized as below, which is denoted as 
$\Tilde{I}V_i$:
\begin{equation}
    \Tilde{I}V_i = \frac{IV_i}{\sum_{s=1}^k IV_s} ,\quad 1\leq i \leq k.
\end{equation}
\item \textbf{Adjust the credibility degree of the evidence:}
Based on the information volume $\Tilde{I}V_i$ the credibility degree $Crd_i$ of the evidence $m_i$ will be adjusted, denoted as $ACrd_i$:
\begin{equation}
    ACrd_i = Crd_i \times \Tilde{I}V_i ,\quad 1\leq i \leq k.
\end{equation}
\item \textbf{Normalize the adjusted credibility degree of the evidence:}
The adjusted credibility degree which is denoted as $ \Tilde{A}Crd_i$ 
 is normalized that is considered as the final weight in terms of each evidence $m_i$:
\begin{equation}
    \Tilde{A}Crd_i = \frac{ACrd_i}{\sum_{s=1}^k ACrd_s} ,\quad 1\leq i \leq k.
\end{equation}
\item \textbf{Compute the weighted average evidence:}
On account of the final weight $\Tilde{A}Crd_i$ of each evidence $m_i$, the weighted average evidence $\mathit{WAE}(m)$ will be obtained as follows:
\begin{equation}
    \mathit{WAE}(m) = \sum_{i=1}^k (\Tilde{A}Crd_i \times m_i) ,\quad 1\leq i \leq k.
\end{equation}
\item \textbf{Combine the weighted average evidence by utilizing the Dempster's rule of combination:}
The weighted average evidence $\mathit{WAE}(m)$ is fused via the Dempster’s combination rule:
\begin{equation}
m_{\text{combined}}(C) = \frac{\sum_{A \cap B = C} m_1(A) \cdot m_2(B)}{1 - \sum_{A \cap B = \emptyset} m_1(A) \cdot m_2(B)}
\label{eq:app_BPA}
\end{equation}
by $(k-1)$ times, if there are k number of evidences. Then, the final combination result of multi-evidences can be obtained.
\item \textbf{Converting probabilities to reward:}
The penultimate combined belief for each action that is denoted as $ m_{\text{combined}}(C)$ is normalized and considered as the final reward.  

\begin{equation}
    \mathit{BPA}_{a_i} = \frac{m_{\text{combined}}(a_j)}{\sum_{j=1}^km_{\text{combined}}(a_j)},\quad 1\leq i \leq k.
\end{equation}
% \[
% BPA_{a_i} = (m_1 \oplus m_2 \oplus m_3 \oplus m_4 \oplus m_5)(\{a_i\}) ,\quad 1\leq i \leq k.
% \]


$\mathit{BPA}_{a_i}$ is the reward for the action $a_i$. 

\end{enumerate}

% \section{Pseudo-Code}
% \label{app:Pseudo_Code}
% Below we presents the pseudo-code of the AMULED framework. This algorithm employs PPO to iteratively update the policy and value function based on environmental feedback. The framework integrates reward shaping to balance primary and secondary objectives and incorporates fine-tuning through reinforcement learning with human-like feedback (RLHF) from moral clusters, using KL divergence and belief aggregation to guide agent behavior.

% \begin{algorithm}
% \caption{AMULED Framework}
% \begin{algorithmic}[1]

% \Require Set of moral clusters and initial policy parameters $\actorParams$, $\criticParams$
% \State Initialize policy $\pi_{\actorParams}$ and value function $V_{\criticParams}$ using Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}
% \For{each episode}
%     \State Collect trajectories of state-action-reward tuples $(s, a, r)$ from the environment
%     \State Compute the advantage function $A^{\pi_{\actorParams}}(s, a)$ using Generalized Advantage Estimation (GAE)
    
%     \State \textbf{Update Policy}:
%     \State Update policy parameters $\actorParams$ by optimizing
%     \[
%     \actorParams_{k+1} = \arg\min_{\actorParams} \mathbb{E}_{t} \left[ \frac{\pi_{\actorParams}(a | s)}{\pi_{\actorParams_{k}}(a | s)} A^{\pi_{\actorParams_{k}}}(s, a) \cdot g(\epsilon, A^{\pi_{\actorParams_{k}}}(s, a)) \right]
%     \]
%     where $g(\epsilon, A)$ represents advantage normalization and value clipping.

%     \State \textbf{Update Value Function}:
%     \State Update value function parameters $\criticParams$ by minimizing the error:
%     \[
%     \criticParams_{k+1} = \arg \min_{\criticParams} \mathbb{E}_{t} \left[V_{\criticParams}(s_t) - R_t\right]^2
%     \]

%     \State \textbf{Reward Shaping}:
%     \State Define rewards at each timestep $t$ as:
%     \[
%     r_t = \baseReward + c \cdot \rewardShaping
%     \]
%     where $r_{\text{base}}$ incentivizes the primary goal and $\rewardShaping$ addresses secondary goals.

%     \If{Fine-tuning with Human Feedback}
%         \State Initialize base policy $\pi_{\text{base}}$ from previously trained parameters
%         \State Define new reward for fine-tuning as:
%         \[
%         r_{\text{base} = -\lambda_{\text{KL}} D_{\text{KL}}\left(\fineTuneModel(a | s) \parallel \baseModel(a | s)\right)
%         \]
%         \[
%         r_{\text{shaping}} = f_{\text{BA}}(\mathbf{B})\hspace{1cm}  \leftarrow \textbf{Eqs. \eqref{eq:app_DMM}--\eqref{eq:app_BPA}}
%         \]
%         where $\lambda_{\text{KL}}$ is a regularization coefficient, and matrix $\mathbf{B}$ represents the belief values from moral agents.
%     \EndIf

%     \State \textbf{Fine-tuning Training Loop}:
%     \For{$T_{\text{finetune}}$ timesteps}
%         \State Train the fine-tuned policy $\fineTuneModel$ using PPO with feedback rewards $r_{\text{base}}$ and $r_{\text{shaping}}$
%     \EndFor
% \EndFor

% \end{algorithmic}
% \end{algorithm}


% \section*{Multi-Morality Fusion Approach:}
% Steps involved are:
% \begin{enumerate}
%   \item \textbf{Input Data from Moral Theories:} Gather data from multiple moral theories or frameworks. Each theory provides its own evidence or belief about the morality of actions or decisions that can be taken.
  
%   \item \textbf{Construct Frame of Discernment:} For each moral theory, construct a frame of discernment based on the principles and values it espouses. These frameworks represent the uncertainty and confidence associated with the moral judgments provided by each theory.
  
%   \item \textbf{Compute Belief Divergence:} Calculate the belief divergence measure between pairs of frameworks from different moral theories. This step helps in understanding how different the moral judgments are across various theories.
  
%   \item \textbf{Weighted Fusion Using Divergence and Entropy:} Use the belief divergence measure and belief entropy to weight the fusion process. Moral theories with more similar judgments (lower divergence) or lower uncertainty (lower entropy) might be given higher weight in the fusion process.
  
%   \item \textbf{Combine Frame of Discernment:} Combine the frameworks from different moral theories using a fusion rule. This rule could be based on the weighted average, consensus, or other methods that take into account the divergence and entropy measures.
  
%   \item \textbf{Output Fused Frame of Discernment:} Obtain a fused frame of discernment that represents a more informed and robust assessment of the moral implications of actions or decisions than any individual moral theory could provide alone.
% \end{enumerate}

% \section{Calculate the morality degree of the actions}


% \[
% \begin{aligned}
% H(m_1) &= - [0.5 \log 0.5 + 0.2 \log 0.2 + 0.1 \log 0.1 + 0.2 \log 0.2] = 0.529 \\
% H(m_2) &= - [0.4 \log 0.4 + 0.3 \log 0.3 + 0.1 \log 0.1 + 0.2 \log 0.2] = 0.5558 \\
% H(m_3) &= - [0.3 \log 0.3 + 0.3 \log 0.3 + 0.2 \log 0.2 + 0.2 \log 0.2] = 0.5933 \\
% H(m_4) &= - [0.2 \log 0.2 + 0.4 \log 0.4 + 0.1 \log 0.1 + 0.3 \log 0.3] = 0.5558 \\
% \end{aligned}
% \]

% \section*{Credibility Degrees}

% \[
% \begin{aligned}
% \text{Cr}(m_1) &= \frac{1}{0.529} = 1.89 \\
% \text{Cr}(m_2) &= \frac{1}{0.5558} = 1.8 \\
% \text{Cr}(m_3) &= \frac{1}{0.5933} = 1.69 \\
% \text{Cr}(m_4) &= \frac{1}{0.5558} = 1.8 \\
% \end{aligned}
% \]

% \section*{Combined Moral Functions}

% Using Dempster's rule of combination, we combine the morality functions \(m_1\) to \(m_4\):

% \[
% (m_1 \oplus m_2 \oplus m_3 \oplus m_4)(\{a_1\}) = 0.5 \times 0.4 \times 0.3 \times 0.2 = 0.012
% \]

% \[
% (m_1 \oplus m_2 \oplus m_3 \oplus m_4)(\{a_2\}) = 0.2 \times 0.3 \times 0.3 \times 0.4 = 0.0072
% \]

% \[
% (m_1 \oplus m_2 \oplus m_3 \oplus m_4)(\{a_3\}) = 0.1 \times 0.1 \times 0.2 \times 0.1 = 0.0002
% \]

% Normalizing the credence under all relevant moralities for action $a_1,a_2, a_3$ of 0.012, 0.0072, and 0.0002 are 0.6186, 0.3711, and 0.0103, respectively. 


% We use the computed final credence value for each action as the intrinsic reward for the agent. Specifically, if the agent takes action $a_1$, it receives a reward of 0.6186. For $a_2$, the reward is 0.3711, and for $a_3$, it is 0.0103.


% \begin{figure*}[htbp]
%   \centering
%   \includegraphics[width=1\linewidth]{images/1-s2.0-S1566253517305584-gr2.jpg}
%   \caption{The flowchart of the proposed method \cite{xiao2019multi}}
%   \label{fig:BJS}
% \end{figure*}





% \section{Notes for understanding BJS}

% Step 1: Compute Belief Jensen–Shannon divergence measure matrix, namely, a distance measure matrix. 


% Reasoning: In the context of evidence theory, particularly in scenarios involving multi-sensor data fusion or combining information from multiple sources, computing distance measures (such as belief divergence measures) between bodies of evidence serves several important purposes:

% Assessing Consistency: Different sensors or sources may provide evidence or beliefs about the same phenomenon, but they might not always agree. Computing distance measures helps to quantify how much different bodies of evidence diverge or disagree with each other. This provides a measure of consistency or inconsistency between different sources of information.

% Weighting in Fusion Processes: When fusing information from multiple sources, it's crucial to consider the reliability and consistency of each source. Bodies of evidence that are more consistent with each other (i.e., have lower divergence measures) can be given higher weights in the fusion process. This ensures that more reliable and coherent information contributes more to the final decision or assessment.

% Step 2: The average evidence distance 

% Reasoning: By calculating the average evidence distance, you can obtain a single numerical value that represents the average dissimilarity between all pairs of bodies of evidence. This measure provides an overall assessment of the consistency or inconsistency among the sources of evidence.

% Step 3: The support degree of the body of evidence.

% Reasoning: The support degree quantitatively expresses the level of confidence or belief that a body of evidence assigns to a specific hypothesis or proposition. It provides a numerical measure indicating how strongly the evidence supports the hypothesis relative to other possible hypotheses.

% Step 4: The credibility degree of the body of the evidence

% Reasoning: The credibility degree provides a quantitative measure of how reliable or trustworthy the body of evidence is perceived to be. It helps in distinguishing between more reliable and less reliable sources of information.

% Step 5: Measure the information volume of the evidences

% Reasoning: The "information volume" of evidence refers to a measure that quantifies the amount or volume of information conveyed by a body of evidence. Here’s how you can understand and measure the information volume of evidences. 
% Measuring the information volume of evidences involves calculating the entropy weighted by the belief assignments across all subsets of the frame of discernment. This measure provides a quantitative assessment of the richness and diversity of information conveyed by the evidence, aiding in decision making and evidence fusion processes within evidence theory.

% Step 6: Generate and fuse the weighted average evidence

% Reasoning: involves combining information from multiple sources or bodies of evidence in a manner that accounts for their respective strengths or reliability. 


