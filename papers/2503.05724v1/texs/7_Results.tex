\begin{figure}[!tp]
    \centering
    \includegraphics[width=\textwidth]{images/milk_models.pdf}
    \caption{a) Metrics evaluating performance of the agent on the primary goal (left-most) and sub-goals. AMULED learns the ethical task to near-perfection, although a hand-crafted shaping reward performs the best for this environment. Error bands reflect 95\% confidence intervals of the mean. b) AMULED is compared to the performance of agents prompted to act like pure moral clusters, and a "moral agent". These values are measured from 50 episodes each. c) Illustration of one of the trajectories learned by AMULED.}
    \label{fig:milk_moral}
\end{figure}

\section{Results}
We study two pertinent tasks: (1) Finding Milk and (2) Driving and Rescuing, which have been used in studying ethical decision-making frameworks~\cite{wu2018low}. These tasks serve as proxies for real-world scenarios, encompassing a broader range of states and thereby demonstrating their applicability to everyday life situations. Fine-tuning the policy using feedback helps the RL agent incorporate ethical actions without deviating too much from the primary goal, effectively balancing ethics with operational efficiency. 

\subsection{Finding Milk}\label{sec:res_findmilk}

Figure~\ref{fig:milk_moral}a shows the learning curves of an agent trained with (moral or human) feedback.
In the \texttt{FindMilk} environment (Fig.~\ref{fig:milk_moral}c), the agent is tasked with reaching the location of the milk in the shortest possible time (primary task).
However, the agent finds crying and sleeping babies on their way to the milk.
The RL agents trained on the primary goal consistently learn to traverse the grid and find the milk in the shortest time possible. 
However, without informing an agent of the ethics of the problem, it will disregard any effects of meeting babies along its path.
Introducing additional rewards $r_{cry}=1$ and $r_{sleep}=-1$ for passing through babies would then help shape the agent's behavior to satisfy the moral sub-goals.
This works really well for the \texttt{FindMilk} environment, but (as we will show in Sec.~\nameref{sec:res_driving}), in practice it is not trivial to assign the relative values of rewards for more complex tasks.

We find that fine-tuning the policy model (as an additional training layer) helps the agent incorporate ethical actions, without deviating too much from the primary task.
We also see that using the outputs of a belief model (combining the beliefs of 5 moral clusters) works even better than the synthetically generated human actions.
However, for this environment, AMULED does not consistently find an ideal path to the milk.
This is because the LLM (GPT-4o-mini), although it presents logical arguments for its actions on the basis of moral theories, sometimes fails in its spatial reasoning: for example, even if it has identified a sleeping baby to the right and it has explicitly identified that this goes against its moral goals, it will still go right because that brings it closer to the milk (even if going up also brings it closer to the milk, without passing through the baby).
When we pass a similar prompt to GPT-4o (the best OpenAI model available at the time), the LLM is better able to reason out the spatial contexts.

Because AMULED is a conglomeration of five moral clusters that guide the decisions of the agent, we can also gain insights on how it considers a diversity of moral philosophies by looking at how each individual moral cluster would guide the actions of the robot (Fig.~\ref{fig:milk_moral}b).
For the sub-goal of avoiding crying babies, we see that the "ideal" behavior is captured by the care moral cluster, which then more strongly aligns with the actions of AMULED.
Another interesting thing to note is that, when we prompt the LLM with no explicit credence values (and thus just prompt it to behave as a "moral" agent), the agent passes through more sleeping babies and fewer crying babies.
These are strikingly different results compared to how the moral clusters would act in this task.



\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\textwidth]{images/drive_models.pdf}
    \caption{a) Metrics evaluating the performance of the agent on the primary goal (left-most) and sub-goals. AMULED manages the tradeoffs between its conflicting goals much better than the other baselines. Error bands reflect 95\% confidence intervals of the mean. b) Comparison of AMULED with the performance of agents prompted to act like pure moral clusters, and a "moral agent". These values are measured from 50 episodes each. c) Illustration of the \textbf{Driving and Rescuing} environment.}
    \label{fig:driving_moral}
\end{figure}

\subsection{Driving and Rescuing}
\label{sec:res_driving}

In the \texttt{Driving} environment, we train an agent to simulate autonomous driving and avoid collisions with other cars on a five-lane road (Fig.~\ref{fig:driving_moral}).
Besides other cars, the environment also has some grandmas trapped in traffic.
The agent will not factor "rescuing" grandmas (simplified as driving through the same lane as the grandma) in its decisions, unless given explicit rewards to shape its behavior.
One way of defining the shaping reward is $r_{shaping}=400r_{grandma} + 20(\mathrm{lane}_{t}==\mathrm{lane}_{t-1})$, where rescuing grandmas and staying on the lane are incentivized.
This task presents a challenge to the agent, as avoiding car collisions can conflict with the secondary goals, depending on the stochasticity of the environment.

Similar to the \texttt{FindMilk} scenario, we find that the base RL model can be trained to avoid collisions well.
Using both LLM and human feedback, the agent incurs more car collisions than the base RL agent.
On the other hand, we see here that defining the shaping reward can completely shift the agent behavior to prioritize rescuing grandmas, at the expense of excessive lane changes and collisions.
Although the synthetic human actions were also much more inclined towards rescuing grandmas, we observe that the agent does not seem to rescue as many grandmas as the feedback samples (Fig.~\ref{fig:driving_moral}a).
% By around episode 650, the RLHF approach rescues around 10 grandmas on average, but this behavior is "forgotten" by the RL agent.
The RLHF approach rescues around 10 grandmas on average while limiting the collisions to around 4.
In comparison, AMULED performs quite well at balancing the secondary goals (rescuing grandmas and remaining in its lane) against the primary goal.
Although AMULED does not save as many grandmas than the agent trained on hand-shaped rewards, it saves more grandmas compared to the agent trained on human feedback, without excessively forgetting its original driving-related tasks.

When compared with the performance of agents acting as pure moral theories, we see more variance in the sub-goal performance of each moral cluster.
Most strikingly, the consequentialist approach favors "inaction", which leads to staying more in lane and fewer rescues of grandmas than the other approaches.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\textwidth]{images/aggregation.pdf}
    \caption{Comparison of AMULED with the performance of agents trained with alternative belief aggregation functions. These values are measured from 50 episodes each.}
    \label{fig:aggregation}
\end{figure}

\subsection{Aggregation of moral beliefs}

Because we deal with a pluralistic moral framework, a key aspect of AMULED is an aggregation mechanism that combines beliefs from each moral cluster.
Taking inspiration from multi-sensor data fusion, AMULED employs a belief aggregation system that combines the beliefs of the five moral clusters to a resultant vector of values corresponding to each action.

For AMULED, we choose an aggregation method developed by \citet{xiao2019multi} to generate the combined belief probability assignment (BPA) for each action.
We take the BPA as the shaping rewards during learning with feedback.
In principle, this is not the only way to aggregate the belief values of the different moral clusters.
For comparison (Fig.~\ref{fig:aggregation}), we look at three other aggregation methods: i) \textbf{majority vote}: each moral cluster "votes" for the action with the highest belief; then, the action with the most votes is assigned an aggregated belief value of 1; ii) \textbf{maximum belief}: the aggregated belief value of an action is the highest belief value of all moral clusters; and iii) \textbf{weighted average}: the belief value of an action is the weighted average of beliefs across all other moral clusters. For simplicity, we set the weights to be 1 (i.e., the weighted average is the mean).

For the \texttt{FindMilk} scenario, we see that AMULED and the max aggregation method have the most similar results for both sub-goals.
On the other hand, AMULED is only similar to voting, and only for the "grandmas rescued" sub-goal.
Overall, AMULED mostly achieves the sub-goals best among the aggregation approaches, except for the "lane changes" sub-goal.
% MAYBE MOVE TO DISCUSSION
Certainly, the choice of aggregation function can have varying impact on the outcomes for different types of environments.



\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\textwidth]{images/llm_compare.pdf}
    \caption{Performance of AMULED using different LLMs as the moral agents. OpenAI's GPT-4o-mini is the LLM used to produce the results of the rest of the paper.}
    \label{fig:llm_compare}
\end{figure}
\subsection{Comparison with different LLMs}

The results of AMULED presented so far were obtained using OpenAI's GPT-4o-mini.
Because the language model served as a core element in guiding ethical decision making, we also compared the performance of AMULED that uses other language models (Fig.~\ref{fig:llm_compare}).
Although there is no official documentation on the exact size and architecture of GPT-4o-mini, some benchmarks put it on par with $\sim$70 Billion parameter models~\cite{livebench}.
We compared it against Mistral NeMo (12B) and LLaMa 3.1 (8B and 70B).
Overall, GPT-4o-mini performs the best for the primary and sub-goals.
The bigger LLaMa 3.1 70B also performs well for the \texttt{Driving} scenario and has a good average performance on the sub-goals of \texttt{FindMilk}, however, it fails to consistently achieve the primary goal for the \texttt{FindMilk} scenario.

% In general, one needs to balance the tradeoffs of 

