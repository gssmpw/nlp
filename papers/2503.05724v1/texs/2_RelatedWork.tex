\section{Related Work}
% \subsection{Ethical Considerations in Decision Theory}
% Rule-Based Approaches and Learning-Based Approaches. Discuss moral philisophy
% \subsection{LLMs and Their Role in Moral Reasoning}
% report LLM advancement and papers which uses for similar tasks
% \begin{itemize}
%     \item moral turning test~\cite{garcia2024moralturing}
%     \item moral beliefs encoded in llm~\cite{scherrer2023moralbeliefllm}
%     \item complex social decision making
%         \begin{itemize}
%             \item Simulacra (society of llm agents)~\cite{park_generative_2023}
%             \item LLM Augmented Democracy~\cite{gudino2024large}
%             \item LLM Voting~\cite{yang2024llmvoting}
%         \end{itemize}
% \end{itemize}
% \subsection{Reinforcement Learning in AI Ethics}
% justify why RL
% \subsection{State of the Arts}
% report similar papers

% The integration of ethical decision-making capabilities into artificial intelligence (AI) systems has garnered significant attention as AI becomes increasingly autonomous and embedded in critical aspects of society. Early work in machine ethics sought to endow AI agents with the ability to discern right from wrong, thereby ensuring their actions are aligned with human moral values \cite{allen2000prolegomena,wallach2008moral}. This foundational research established the importance of embedding ethical considerations into AI design but often focused on implementing singular moral theories.

% One prevalent approach has been the incorporation of specific moral frameworks into AI agents. For instance, deontological ethics, which emphasizes adherence to moral rules or duties, has been operationalized in AI systems to ensure compliance with predefined ethical norms \cite{bringsjord2006toward}. Similarly, utilitarian principles, advocating for actions that maximize overall happiness or utility, have been employed to guide AI behavior towards beneficial outcomes \cite{russell2015research}. While these methods provide clear ethical guidelines, they are limited by their reliance on a single moral perspective, which may not capture the complexity of real-world ethical dilemmas.

% Recognizing the limitations of mono-theoretical approaches, recent research has explored the concept of moral pluralism and uncertainty in AI. Moral uncertainty acknowledges that no single moral theory may be wholly correct, and therefore, decisions should account for multiple ethical viewpoints \cite{macaskill2020moral}. Efforts have been made to formalize moral uncertainty in computational models, allowing AI agents to weigh different moral considerations when making decisions \cite{lutge2019ethics}. For example, Lindner and Bentzen \cite{lindner2018formalization} proposed a formalization of Kantian ethics that can be integrated with other moral theories in AI systems.

% Reinforcement learning (RL), a framework where agents learn optimal behaviors through trial and error interactions with their environment, has been a focal point for integrating ethical considerations. Researchers have modified reward functions to include ethical penalties or bonuses, effectively shaping agent behavior towards desired ethical outcomes \cite{abel2016reinforcement,hadfield2017off}. Inverse reinforcement learning has also been utilized to infer ethical policies from human demonstrations, enabling agents to learn complex ethical behaviors without explicit programming \cite{ng2000algorithms,ziebart2008maximum}. However, these approaches often struggle to reconcile conflicting ethical principles and may not adequately represent the full spectrum of moral theories.

% Translating abstract moral philosophies into concrete computational models remains a significant challenge. Moral theories are inherently complex and sometimes conflicting, making their direct implementation into AI systems non-trivial \cite{malle2016integrating}. Efforts to formalize ethical principles often result in oversimplification, potentially overlooking important nuances. Dennis et al. \cite{dennis2016formal} explored the use of formal verification methods to ensure AI systems adhere to specified ethical constraints, but this approach requires exhaustive specification of ethical rules, which may not be feasible in dynamic environments.

% The reliance on human oversight mechanisms, such as "human-in-the-loop" (HITL) and "meaningful human control" (MHC), has been proposed to mitigate ethical risks in AI systems \cite{santoni2018meaningful}. While effective in some contexts, these methods limit the autonomy of AI agents and are impractical for applications requiring real-time decision-making without human intervention \cite{cummings2014man}. This underscores the need for AI systems capable of independent ethical reasoning.

% Despite these advancements, gaps remain in the development of AI agents that can navigate ethical dilemmas autonomously while considering multiple moral frameworks. Existing approaches often fail to operationalize moral uncertainty effectively or to translate the complexities of moral philosophies into practical algorithms for RL agents. Moreover, the dependency on human oversight is not scalable or feasible for many applications where AI agents must act independently.

% Our proposed approach addresses these gaps by integrating an ethical training layer into the RL framework, allowing agents to balance multiple moral theories within their decision-making process. By translating utilitarianism, deontology, virtue ethics, and other moral philosophies into reward functions, we enable agents to consider a diverse set of ethical principles \cite{sen2017collective}. This method aligns with the notion of moral uncertainty, as agents are not bound to a single moral theory but can weigh different ethical considerations based on the context.

% Furthermore, our approach reduces the need for constant human oversight by empowering agents with the capacity for autonomous ethical reasoning. This is particularly relevant in high-dimensional, complex environments where human intervention is impractical. By mathematically modeling the conversion of moral theories into RL reward functions, we provide a practical solution for handling moral conflicts in uncertain scenarios \cite{gips2011towards}.

% In summary, while prior research has laid the groundwork for ethical AI, there remains a need for methods that allow agents to autonomously navigate ethical dilemmas by considering multiple moral frameworks. Our work contributes to this field by offering a novel approach that operationalizes moral uncertainty within the RL paradigm, thus pushing the boundaries of existing research in machine ethics and AI.
The study of AI ethics has attracted significant research attention across various domains, ranging from decision theory to reinforcement learning and large language models. In this section, we present an overview of key contributions in these areas, categorizing them into four sub-domains: Ethical Considerations in Decision Theory, LLMs and Their Role in Moral Reasoning, Reinforcement Learning in AI Ethics, and the Current State of the Art. The following works provide a foundation for understanding how ethical frameworks are integrated into AI systems and highlight both the challenges and progress made toward more responsible and ethical AI decision-making.

\subsection{Ethical Considerations in Decision Theory}
Research in ethical decision theory for AI has explored both rule-based and learning-based approaches. Rule-based approaches often rely on moral philosophies such as deontology and utilitarianism. Deontological approaches emphasize adherence to moral rules or duties \cite{bringsjord2006toward}, while utilitarian frameworks guide decisions based on outcomes that maximize overall good \cite{russell2015research}. However, these single-theory approaches struggle with complex moral dilemmas. 
To address this, researchers have proposed models incorporating moral pluralism and uncertainty, allowing AI to weigh different ethical perspectives \cite{macaskill2020moral, lutge2019ethics}. Formalizations of Kantian ethics \cite{lindner2018formalization} and multi-theory frameworks \cite{sen2017collective} illustrate attempts to integrate multiple moral theories into decision-making models.

\subsection{LLMs and Their Role in Moral Reasoning}
LLMs have become prominent in exploring moral reasoning capabilities. Several studies have assessed LLMs' ability to encode and apply moral beliefs through their training on human datasets. For instance, the Moral Turing Test \cite{garcia2024moralturing} found that humans often agree with LLM moral judgments. Scherrer et al. \cite{scherrer2023moralbeliefllm} demonstrated that LLMs encode commonsense moral beliefs.
LLMs have also been used to simulate complex social interactions. Park et al. \cite{park_generative_2023} created Simulacra, a society of LLM agents demonstrating emergent social behaviors. In collective decision-making tasks, Gudino et al. \cite{gudino2024large} proposed LLM-Augmented Democracy, and Yang et al. \cite{yang2024llmvoting} introduced LLM Voting for consensus building. These studies highlight LLMs' potential in moral and ethical reasoning tasks. However, despite their potential benefits, LLMs also pose significant risks and ethical challenges. For instance, biases inherent in training data can lead to unfair treatment of marginalized groups, perpetuating societal inequalities \cite{jiao2024navigating}. Additionally, the ability of LLMs to generate convincing yet false content raises concerns about misinformation and its impact on public trust and democratic processes \cite{coeckelbergh2025llms}. The opaque nature of these models further complicates accountability, as determining responsibility for harmful outputs remains challenging \cite{jiao2024navigating}. Addressing these issues requires comprehensive strategies, including bias mitigation, transparency measures, and robust fact-checking frameworks \cite{xie2024fire}.

\subsection{Reinforcement Learning in AI Ethics}
RL has been a core method for teaching AI ethical behavior. By shaping reward functions, researchers have guided agents to learn ethical outcomes \cite{abel2016reinforcement, hadfield2017off}. Inverse reinforcement learning (IRL) has been used to derive ethical policies from human demonstrations \cite{ng2000algorithms, ziebart2008maximum}. Despite these advancements, RL faces challenges such as reward design complexity and the reconciliation of conflicting moral principles \cite{malle2016integrating, dennis2016formal}. 
Recent approaches attempt to incorporate moral pluralism within RL frameworks. For example, AMULED (Addressing Moral Uncertainty with LLMs for Ethical Decision-Making) \cite{sen2017collective} combines moral theories into reward functions. AMULED leverages LLMs to translate diverse ethical principles into reward signals, enabling agents to balance competing moral considerations in their learning processes.

\subsection{State of the Art}
Several recent works address ethical AI decision-making. Chen et al. \cite{chen2024lost} surveyed ethical challenges in autonomous decision-making, particularly in high-stakes domains like autonomous vehicles. Duan et al. \cite{duan2019artificial} and Mahajan et al. \cite{mahajan2024executioner} discussed ethical framework implementations in AI systems. Tolmeijer et al. \cite{tolmeijer2020implementations} reviewed the integration of moral theories into AI models. 
Additionally, LLM-driven approaches to AI ethics continue to gain traction, with studies such as Suri et al. \cite{suri2024defining} and Van et al. \cite{van2023ai} exploring LLMs' ability to navigate complex moral scenarios. These contributions collectively advance the field by integrating ethical reasoning into AI systems and highlighting gaps in moral uncertainty handling. Recent studies have also highlighted the challenges of integrating LLMs into ethical AI systems. For instance, biases in training data can lead to unfair outcomes, particularly for marginalized groups, as demonstrated by Schramowski et al. \cite{schramowski2022large}, who found that LLMs often encode human-like biases regarding moral judgments. Additionally, the opacity of LLMs complicates accountability, raising concerns about their use in high-stakes decision-making. Addressing these issues requires advancements in transparency measures and fairness constraints.

